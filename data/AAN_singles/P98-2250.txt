Tree-based Analysis of Simple Recurrent Network LearningIvetin StoianovDept.
Alfa-Informatica, Faculty of Arts, Groningen University, POBox 716, 9700 AS Groningen,The Netherlands.
Email:stoianov@let.rug.nl1 S imple  recur rent  networks  fo r  natura llanguage phonotaet les  analysis .In searching for a connectionist paradigm capable ofnatural anguage processing, many researchers haveexplored the Simple Recurrent Network (SRN) suchas Elman(1990), Cleermance(1993), Reilly(1995)and Lawrence(1996).
SRNs have a context layerthat keeps track of the past hidden neuronactivations and enables them to deal with sequentialdata.
The events in Natural Language span time soSRNs are needed to deal with them.Among the various levels of language proce-ssing" a phonological level can be distinguished.
ThePhonology deals with phonemes or graphem~ - thelatter in the case when one works with orthographicword representations.
The principles governing thecombinations of these symbols is called phonotactics(Laver'1994).
It is a good starting point forconnectionist language analysis because there arenot too many basic entities.
The number of thesymbols varies between 26 (for the Latingraphemes) and 50 "(for the phonemes).Recently.
some experiments consideringphonotactics modelling with SRNs have been carriedout by Stoianov(1997), Rodd(1997).
The neuralnetwork in Stoianov(1997) was trained to study thephonotactics of a large Dutch word corpus.
Thisproblem was implemented as an SRN learning task -to predict he symbol following the left context givento the input layer so far.
Words were applied to thenetwork, symbol by symbol, which in turn wereencoded orthogonally, that is, one node standing forone symbol (Fig.
1).
An extra symbol ( '#') was usedas a delimiter.
After the training, the networkresponded to the input with different neuronactivations at the output layer; The more active agiven output neuron is, the higher the probability isthat it is a successor.
The authors used a so-calledoptimal threshold method for establishing thethreshold which determines the possible successors.This method was based on examining the network"for Dutch, and up to at most 100 in other languages.response to a test corpus of words belonging to thetrained language and a random corpus, built up fromrandom strings.
Two error functions dependent on athreshold were computed, for the teat and therandom corpora, respectively.
The threshold atwhich both errors had minimal value was selected asan optimal threshold.
Using this approach, an SRN.trained to the phonotactics of a Dutch monosyllabiccorpus containing 4500 words, was reported todistinguish words from non-words with 7% error,Since the phonotactics of a given language isrepresented by the constraints allowing a givensequence to be a word or not, and the SRN managedto distinguish words from random strings withtolerable error, the authors claim that SRNs are ableto learn the phonotactics of Dutch language.SR1Fig.l.
SRN and mechanism of sequenceprocessing.
A character is provid~-I to the inputand the next one is used for training.
In turn, ithas to be predicted uring the test phase.In the present report, alternative evaluationprocedures are proposed.
The network evaluationmethods introduced are based on examining thenetwork response to each left context, available inthe training corpus.
An effective way to representand use the complete set of context strings is a tree-based data structure.
Therefore, these methods aretenlned tree-baaed analysis.
Two possibleapproaches are proposed for measuring the SRNresponse accuracy to each left context.
The In-st usesthe idea mentioned above of searching a thresholdthat distinguishes permitted successors fromimpossible ones.
An error as a function of the1502threshold is computed.
Its minimum valuecorresponds to the SRN learning error rate.
Thesecond approach computes the local proximitybetween the network response and a vectorcontaining the empirical symbol probabifities that agiven symbol would follow the current left context.Two measures are used: !,2 norm and normalisedvector multiplication.
The mean of these localproximities measures how close the networkresponses are to the desired responses.2 T ree-based  corpus  representat ion .There are diverse methods to represent a given set ofwords (corpus).
Lists is the simplest, but they arenot optimal with regard to the memory complexityand the time complexity of the operations workingwith the data.
A more effective method is the treo-based representation.
Each node in this tree has amaximum of 26 possible children (successors), if wework with orthographic word representations.
Theroot is empty, it does not represent a symbol.
It isthe beginning of a word.
The leaves do not havesuccessors and they always represent the end of aword.
A word can end sorr~where between the rootand the leaves as well.
This manner of corpusrepresentation, termed trie, is one of the mostcompact representations and is very effective fordifferent operations with words from the corpus.In addition to the symbol at each node, we cankeep additional information, for example thefrequency of a word, if this node is the end of aword.
Another useful piece of information is thefrequency of each node C, that is, the frequency ofeach left context.
It is computed recursively as asum of the frequencies of all successors and thefrequency of the word ending at this node, providedthat such a word exists.
These frequencies give us aninstant evaluation of the empirical distribution foreach successor.
In order to compute the successors'empirical distribution vector TO(.
), we have tonorrnelise the successors' frequencies with respect totheir sum.3 T ree-based  eva luat ion  of  SRN learn ing.During the training of a word, only one outputneuron is forced to be active in response to thecontext presented so far.
But usually, in the entirecorpus there are several successors following a givencontext.
Therefore, the training should result inoutput neurons, reproducing the successors'probability distn'bufion.
Following this reasoning,we can derive a test procedure that verifies whetherthe SRN output activations correspond tothese localdistributions.
Another approach related to thepractical implementation of a trained SRN is tosearch for a cue, giving an answer to the questionwhether given symbol can follow the contextprovirtea to the input layer so far.
As in the optimalthreshold method we can search for a threshold thatdistinguishes these neurons.The tree-based learning examination methodsare recursive procedures that process each tree node,performing an in-order (or depth-first) treetraversal.
This kind of traversal algorithms startfrom the root and process each sub-tree completely.At each node~ a comparison between the SRNsreaction to the input, and the empirical charactersdistribution is made.
Apart from this evaluation, theSRN state, that is, the context layer, has to be keptbefore moving to one of the sub-trees, in order for itto be reused after traversing this sub-tree.On the basis of above ideas, two methods fornetwork evaluation are performed at each tree nodec.
The first one computes an error function if(t)dependent on a threshold t. This fimction gives theerror rate for each threshold t. that is, the ratio oferroneous predictions given t. The values of if(t) arehigh for close to zero and close to one thresholds,since almost all neurons would permit thecorrespondent symbols to be successors in the firstcase, and would not allow any successor in thesecond case.
The minimum will occur somewhere inthe middle, where only a few neurons would have anactivation higher than this threshold.
The trainingadjusts the weights of the network so that onlyneurons corresponding to actual successors areactive.
The SRN evaluation is-based on the meanF(t) of these local error functions (Fig.2a).The second evaluation method computes theproximity D c ffi \[ N~(.)
,T'(.)
\[ between the networkresponse NC(.)
and the local empirical distributionsvector T?(.)
at each tree node.
The final evaluationof the SRN training is the mean D of D e for all treenodes.
Two measures are used to compute D ?.
Thefirst one is L~ norm (I):(t) 1~(.)
.~?.)
1~ = pvr'~.,~ (~c~)-'r%))'l '~1503The second is a vector nmltipfication, normali-sed with respect o the vector's length (cosine) (2):(2) I,=(veF(.
), ITC(.
)I) "I~'.M(I~CCi)TC(I))where M is the vector size, that is, the number ofpossible successors (e.g.
27) (see Fig.
2b).4 Results,Well-trained SRNs were examined with both theoptimal threshold method and the tree-basedapproaches.
A network with 30 hidden neuronspredicted about I 1% of the characters erroneously.The sarr~ network had mean ~ distance 0.056 andmean vector-multiplication proximity 0.851.
At thesame time, the optimal threshold method rated thelearning at 7% error.
Not surprisingly, the tree-based evaluations methods gave higher error rate -they do not examine the SRN response to non-existent left contexts, which in turn are used in theoptimal threshold method.Discuss ion and  conclus ions.Alternative valuation methods for SRN learning areproposed.
They examine the network response onlyto the training input data, which in turn isrepresented in a tree-based structure.
In contrast,previous methods examined trained SRNs with testand random corpora.
Both methods give a good ideaabout the learning attained.
Methods used previouslyestimate the SRN recognition capabilities, while themethods presented here evaluate how close thenetwork response is to the desired response - but forfamiliar input sequences.
The desired response iscbnsidered to be the successors' empiricalprobability distribution.
Hence, one of the methodsproposed compares the local empirical probabilities(a)10 .
.
.
.
.
.
.
.
.
.
.
.
.
.
- .
.
.
.
.
.
.
.
.
.
.
~ .
.
, 2  .
.
.
.
.
?
.
.
.
.
| .
.
.
.
?
.
.
.
.
: ; : : : ;0 " : -" : -'0 2 # 6 8 Tlw.e~ol d 12 14 16 18 20o .~0.40 .~0.
\ ]0o15O.Ito the network response.
The other approachsearches for a threshold that minimises theprediction error function.
The proposed methodshave been employed in the evaluation ofphonotactics learning, but they can be used invarious other tasks as well, wherever the data can beorganised hierarchically.
I hope, that the proposedanalysis will contribute to our understanding oflearning carried out in SRNs.References.Cleeremans, Axel (1993).
Mechanisms of ImplicitLearning.MIT Press.Elman, J.L (1990).
Finding structure in time.
CognitiveScience, 14, pp.179-211.Elman, J.L, et al (1996).
Rethinking Innates.
ABradford Book, The Mit Press.Haykin, Simon.
(1994).
Neural Networks, MacmillanCollege Publisher.Laver,John.
(1994).Principles of phonetics,Cambr.
U n Pr.Lawrence, S., ct al.
(1996).NL Gramatical Inference AComparison of RNN and ML Methods.
Con-nectionist, statistical and symbolic approaches tolearning for NLP, Spfinger-Verlag,pp.33-47Nerbonne, John, et al(1996).
Phonetic Distance betweenDutch Dialects.
In G.Dureux, W.Daelle-mans &S.Gillis(eds) Proc.of CLIN, pp.
185-202Reilly, Ronan G.(1995).Sandy Ideas and Coloured Days:Some Computational Implications of Embodiment.Art.
intellig.
Review,9: 305-322.,Kluver Ac.
PubI.,NL.Rodd, Jenifer.
(1997).
Recurrent Neural-NetworkLearning of Phonological Regula-rities in Turkish,ACL'97 Workshop: Computational Natural languagelearning, pp.
97-106.Stoianov, LP., John Nerbonne and Huub Bouma (1997).Modelling the phonotacti?
structure of naturallanguage words with Simple Recurrent Networks,Prac.
of 7-th CUN'97 (in press)BI  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
: .
.
.
.
: , .
,:1 !iii!i iii!
!iOlo o.1 o.2 0 .
\ ]  0.4 0.?
o.6 o.7 0.B o.9 1t i  Id:,elrll=e(b)Fig.2.
SRN evaluation by: (a.)
minim/sing the error function F(t).
(b.)
measuring the $RN matching to theempirical successor distributions.
The distributions of L~ distance and cosine are given (see the text).1504
