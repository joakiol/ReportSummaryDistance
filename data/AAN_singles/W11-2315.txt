Proceedings of the 2nd Workshop on Speech and Language Processing for Assistive Technologies, pages 137?147,Edinburgh, Scotland, UK, July 30, 2011. c?2011 Association for Computational LinguisticsTowards an on-demand Simple Portuguese WikipediaArnaldo Candido Junior Ann CopestakeInstitute of Mathematics and Computer Sciences Computer LaboratoryUniversity of S?o Paulo University of Cambridgearnaldoc at icmc.usp.br Ann.Copestake at cl.cam.ac.ukLucia Specia Sandra Maria Alu?sioResearch Group in Computational Linguistics Institute of Mathematics and Computer SciencesUniversity of Wolverhampton University of S?o Paulol.specia at wlv.ac.uk sandra at icmc.usp.brAbstractThe  Simple  English Wikipedia  provides  asimplified  version  of  Wikipedia's  Englisharticles  for  readers  with  special  needs.However,  there are fewer efforts  to makeinformation  in  Wikipedia  in  otherlanguages  accessible  to  a  large  audience.This work proposes the use of a syntacticsimplification  engine  with  high  precisionrules  to  automatically  generate  a  SimplePortuguese Wikipedia on demand, based onuser interactions with the main PortugueseWikipedia.
Our  estimates  indicated that  ahuman  can  simplify  about  28,000occurrences  of  analysed  patterns  permillion  words,  while  our  system  cancorrectly simplify 22,200 occurrences, withestimated f-measure 77.2%.1 IntroductionThe Simple English Wikipedia1 is an effort to makeinformation  in  Wikipedia2 accessible  for  lesscompetent  readers  of  English  by  using  simplewords and grammar.
Examples of  intended usersinclude  children  and  readers  with  special  needs,such as users with learning disabilities and learnersof English as a second language.Simple English (or Plain English), used in thisversion  of  Wikipedia,  is  a  result  from the  PlainEnglish movement that occurred in Britain and theUnited States in the late 1970?s as a reaction to theunclear language used in government and businessforms and documents.
Some recommendations onhow  to  write  and  organize  information  in  Plain1 http://simple.wikipedia.org/2 http://www.wikipedia.org/Language (the set of guidelines to write simplifiedtexts) are related to both syntax and lexical levels:use short sentences; avoid hidden verbs; use activevoice; use concrete, short, simple words.A number of resources, such as lists of commonwords3,  are available for the English language tohelp users write in Simple English.
These includelexical  resources  like  the  MRC  PsycholinguisticDatabase4 which  helps  identify  difficult  wordsusing  psycholinguistic  measures.
However,resources as such do not exist for Portuguese.
Anexception is a small list of simple words compiledas part  of  the  PorSimples project  (Aluisio et  al.,2008).Although  the  guidelines  from  the  PlainLanguage  can  in  principle  be  applied  for  manylanguages and text genres, for Portuguese there arevery  few  efforts  using  Plain  Language  to  makeinformation accessible to a large audience.
To thebest  of  our  knowledge,  the  solution  offered  byPortugues  Claro5 to  help  organizations  produceEuropean  Portuguese  (EP)  documents  in  simplelanguage is the only commercial option in such adirection.
For  Brazilian  Portuguese  (BP),  aBrazilian  Law (10098/2000)  tries  to  ensure  thatcontent  in  e-Gov sites  and  services  is  written  insimple  and  direct  language  in  order  to  removebarriers in communication and to ensure citizens'rights  to  information  and communication  access.However,  as  it  has  been  shown  in  Martins  andFilgueiras  (2007),  content  in  such  websites  stillneeds  considerable  rewriting  to  follow the  PlainLanguage guidelines.A few efforts from the research community haverecently  resulted  in  natural  language  processing3 http://simple.wiktionary.org/4 http://www2.let.vu.nl/resources/elw/resource/mrc.html5  http://www.portuguesclaro.pt/137systems to simplify and make Portuguese languageclearer.
ReEscreve (Barreiro and Cabral, 2009) is amulti-purpose  paraphraser that  helps  users  tosimplify their EP texts by reducing its ambiguity,number  of  words  and  complexity.
The  currentlinguistic phenomena paraphrased are support verbconstructions,  which  are  replaced  by  stylisticvariants.
In  the  case  of  BP,  the  lack  ofsimplification  systems  led  to  development  ofPorSimples project (Alu?sio and Gasperin, 2010).This  project  uses  simplification  in  differentlinguistic levels to provide simplified text to poorliteracy readers.For  English,  automatic  text  simplification  hasbeen  exploited  for  helping  readers  with  poorliteracy (Max, 2006) and readers with other specialneeds,  such  as  aphasic  people  (Devlin  andUnthank,  2006;  Carroll  et  al.
1999).
It  has  alsobeen used in bilingual education (Petersen, 2007)and  for  improving  the  accuracy  of  NaturalLanguage Processing (NLP) tasks (Klebanov et al,2004; Vickrey and Koller, 2008).Given the general scarcity of human resources tomanually simplify large content  repositories suchas Wikipedia, simplifying texts automatically canbe  the  only  feasible  option.
The  PortugueseWikipedia,  for  example,  is  the  tenth  largestWikipedia (as of May 2011), with 683,215 articlesand approximately 860,242 contributors6.In  this  paper  we  propose  a  new  rule-basedsyntactic simplification system to create a SimplePortuguese Wikipedia  on demand,  based on userinteractions with the main Portuguese Wikipedia.We use a simplification engine to change passiveinto active voice and to break down and change thesyntax of subordinate clauses.
We focus on theseoperations  because  they  are  more  difficult  toprocess  by  readers  with  learning  disabilities  ascompared  to  others  such  as  coordination  andcomplex noun phrases (Abedi et al, 2011; Jones etal.,  2006;  Chappell,  1985).
User  interaction withWikipedia can be performed by a system like theFacilita7 (Watanabe et al, 2009), a browser plug-indeveloped  in  the  PorSimples  project  to  allowautomatic adaptation (summarization and syntacticsimplification) of any web page in BP.This  paper  is  organized  as  follows.
Section  2presents related work on syntactic  simplification.6 http://meta.wikimedia.org/wiki/List_of_Wikipedias#Grand Total7 http://nilc.icmc.usp.br/porsimples/facilita/Section  3 presents the methodology to build andevaluate the simplification engine for BP.
Section 4presents  the  results  of  the  engine  evaluation.Section  5 presents  an  analysis  on  simplificationissues  and  discusses  possible  improvements.Section 6 contains some final remarks.2 Related workGiven the dependence of  syntactic  simplificationon  linguistic  information,  successful  approachesare  mostly  based  on  rule-based  systems.Approaches using operations learned from corpushave  not  shown  to  be  able  to  perform  complexoperations  such  the  splitting  of  sentences  withrelative clauses (Chandrasekar and Srinivas, 1997;Daelemans  et  al.,  2004;  Specia,  2010).
On  theother hand.
the use of machine learning techniquesto predict when to simplify a sentence, i.e.
learningthe properties of language that distinguish simplefrom normal  texts,  has  achieved relative  success(Napoles and Dredze, 2010).
Therefore, most workon syntactic simplification still relies on rule-basedsystems to simplify a set of syntactic constructions.This is also the approach we follow in this paper.In what follows we review some relevant and workon syntactic simplification.The seminal work of Chandrasekar and Srinivas(1997) investigated the induction of syntactic rulesfrom a corpus annotated with part-of-speech tagsaugmented  by  agreement  and  subcategorizationinformation.
They  extracted  syntacticcorrespondences  and  generated  rules  aiming  tospeed up parsing and improving its accuracy, butnot  working  on  naturally  occurring  texts.Daelemans et  al.
(2004)  compared both machinelearning  and  rule-based  approaches  for  theautomatic generation of TV subtitles for hearing-impaired  people.
In  their  machine  learningapproach,  a  simplification model  is  learned fromparallel  corpora  with  TV  programme  transcriptsand the associated subtitles.
Their method used amemory-based learner and features such as words,lemmas,  POS tags,  chunk tags,  relation tags  andproper  name  tags,  among  others  features  (30  intotal).
However, this approach did not perform aswell  as  the  authors  expected,  making errors  likeremoving sentence subjects or deleting a part of amulti-word  unit.
More  recently,  Specia  (2010)presented a new approach for text simplification,based  on  the  framework  of  Statistical  Machine138Translation.
Although the results are promising forlexical simplification, syntactic rewriting was notcaptured  by  the  model  to  address  long-distanceoperations,  since  syntactic  information  was  notincluded into the framework.Inui et al (2003) proposed a rule-based systemfor text simplification aimed at deaf people.
Usingabout  one  thousand  manually  created  rules,  theauthors  generate  several  paraphrases  for  eachsentence and train a classifier to select the simplerones.
Promising  results  were  obtained,  althoughdifferent  types  of  errors  on  the  paraphrasegeneration are encountered, such as problems withverb conjugation and regency.
Our work aims atmaking  Portuguese  Wikipedia  informationaccessible  to  a  large  audience  and  instead  ofgenerating  several  possible  outputs  we  generateonly one based on rules taken from a manual ofsimplification for BP.Siddharthan  (2006)  proposed  a  syntacticsimplification  architecture  that  relies  on  shallowparsing.
The general goal of the architecture is tomake texts more accessible to a broader audienceinstead of targeting any particular application.
Thesystem  simplifies  apposition,  relative  clauses,coordination  and  subordination.
Our  method,  onthe other hand, relies on deep parsing (Bick, 2000)and focuses  on  changing passive  to  active voiceand  changing  the  syntax  of  relative  clauses  andsubordinate sentences.Max  (2006)  applied  text  simplification  in  thewriting process by embedding the simplifier into aword  processor.
Although  this  system  ensuresaccurate  output,  it  requires  manual  choices.
Thesuggested simplifications are ranked by a score ofsyntactic  complexity  and  potential  change  ofmeaning.
The writer  then chooses their  preferredsimplification.
Our  method,  on  the  other  hand,offers the user only one simplification since it usesseveral  rules  to  better  capture  each  complexphenomenon.Inspired  by  Siddharthan  (2006),  Jonnalagaddaand  Gonzalez  (2009)  present  an  approach  tosyntactic  simplification  addressing  also  theproblem of accurately determining the grammaticalcorrectness  of  the  simplified  sentences.
Theypropose  the  combination  of  the  number  of  nulllinks  and  disjunct  cost  (the  level  ofinappropriateness,  caused  by  using  less  frequentrules in the linkage) from the cost vector returnedby a Link Grammar8 parser.
Their motivation is toimprove the performance of systems for extractingProtein-Protein  Interactions  automatically  frombiomedical  articles  by  automatically  simplifyingsentences.
Besides  treating  the  syntacticphenomena described in Siddharthan (2006), theyremove  describing  phrases  occurring  at  thebeginning  of  the  sentences,  like  ?These  resultssuggest that?
and ?As reported previously?.
Whilethey  focus  on  the  scientific  genre,  our  work  isfocused on the encyclopedic genre.In order to obtain a text easier to understand bychildren,  De  Belder  and  Moens  (2010)  use  theStanford parser9 to select the following phenomenato syntactically simplify the sentences: appositions,relative  clauses,  prefix  subordination  and  infixsubordination  and  coordination.
After  sentencesplitting, they try to apply the simplification rulesagain to both of the new sentences.
However, theyconclude that  with the set  of  simplification rulesused,  it  was  not  possible  to  reduce  the  readingdifficulty for children and foresee the use of othertechniques for this purpose, such as summarizationand elaborations for difficult words.3 Simplification engine3.1 Engine developmentThe  development  of  a  syntactic  simplificationengine  for  a  specific  task  and  audience  can  bedivided  into  five  distinct  phases:  (a)  targetaudience analysis; (b) review of complex syntacticphenomena for such an audience; (c) formulationof simplification guidelines; (d) refinement of rulesbased  on  evidence  from  corpora;  and  (e)programming and evaluation of rules.In this paper we focus on the last two phases.We  use  the  simplification  guidelines  from  thePorSimples  project,  but  these  are  based  ongrammar  studies  and  corpora  analysis  for  adifferent  text  genre  (news).
Therefore  additionalcorpora  evidence  proved  to  be  necessary.
Thisresulted  in  the  further  refinement  of  the  rules,covering  different  cases  for  each  syntacticphenomenon.The Simplification engine relies on the output ofthe  Palavras  Parser  (Bick,  2000)  to  performconstituent tree transformations (for example, tree8 http://www.abisource.com/projects/link-grammar/9 http://nlp.stanford.edu/software/lex-parser.shtml139splitting).
Each  node  of  a  sentence  tree  is  fed(breadth-first  order)  to  the  simplificationalgorithms,  which can simplify the node (and itssub-tree) or skip it when the node does not meetthe simplification prerequisites.
Breadth-first orderis chosen because several operations affect the rootof a (sub)tree, while none of them affect leaves.A development  corpus containing examples  ofcases analysed for each syntactic phenomenon isused  to  test  and  refine  the  rules.
The  currentversion of the corpus has 156 sentences extractedfrom news text.
The corpus includes negative andpositive  examples  for  each  rule.
Negativeexamples  should  not  be  simplified.
They  wereinserted  into  the  corpus  to  avoid  unnecessarysimplifications.
Each rule is first tested against itsown positive and negative examples.
This test  iscalled  local  test.
After reaching a good precisionon the local test, the rule is then tested against allthe  sentences  in  the  corpus,  global  test.
In  thecurrent corpus, the global test identified sentencescorrectly   simplified by  at  least  one  rule  (66%),sentences incorrectly simplified due to major errorsin  parsing/rules  (7%)  (ungrammatical  sentences)and  non-simplified  sentences  (27%).
The  lastincludes  mainly  negative  examples,  but  alsoincludes  sentences  not  selected  due  to  parsingerrors, sentences from cases not yet implemented,and sentences from cases ignored due to ambiguity.3.2 Passive voiceThe default case for dealing with passive voice inour simplification engine is illustrated by the pairof  original-simplified sentences  in  example10 (1).Sentences  belonging  to  this  case  have  a  non-pronominal subject and a passive agent.
Also, thepredicator has two verbs, the verb  to be followedby  a  verb  in  the  past  participle  tense.
Thesimplification consists  in  reordering the sentencecomponents,  turning  the  agent  into  subject(removing the  by preposition), turning the subjectinto direct  object and adjusting the predicator byremoving the verb to be and re-inflecting the mainverb.
The new tense of the main verb is the sameas  the  one  of  the  to  be  verb  and  its  number  isdefined according to the new subject.10 Literal translations from Portuguese result in somesentences appearing ungrammatical in English.O: As[The] transfer?ncias[transfers]foram[were:plural] feitas[made] pela[by the]empresa[company].
(1)S: A[The] empresa[company] fez[made:sing]as[the] transfer?ncias[transfers].Other correctly processed cases vary accordingthe  number  of  verbs  (three  or  four),  specialsubjects, and special agents.
For cases comprisingthree or four verbs, the simplification rule must re-inflect11 two verbs (2) (one of them should agreewith  the  subject  and  the  other  receives  its  tensefrom  the  verb  to  be).
There  are  two  cases  ofspecial subjects.
In the first case, a hidden subjectis turned into a pronominal direct object (3).
In thesecond  case,  a  pronominal  subject  must  betransformed to oblique case pronoun and then todirect  object.
Special  agents  also  represent  twocases.
In the first one, oblique case pronouns mustbe  transformed before  turning  the  agent  into  thesubject.
In the second case (4), a non-existent agentis turned into an undetermined subject (representedhere by ?they?
).O: A[The] porta[door] deveria[should] ter[have]sido[been] trancada[locked:fem] por[by] John.
(2)S: John deveria[should] ter[have]trancado[locked:masc] a[the] porta[door].O: [I] fui[was] encarregado[entrusted] por[by]minha[my] fam?lia[family].
(3)S: Minha[My] fam?lia[family]encarregou[entrusted] me[me].O: O[The] ladr?o[thief] foi[was] pego[caught].
(4)S: [They] pegaram[caught] o[the] ladr?o[thief].Two cases  are  not  processed because they arealready considered easy enough: the syndetic voiceand passive in non-root sentences.
In those cases,the  proposed  simplification  is  generally  lessunderstandable  than  the  original  sentence.Sentences  with  split  predicator  (as  in  ?thepolitician was very criticized by his electors?)
arenot  processed  for  the  time  being,  but  should  beincorporated in the pipeline in the future.Table  1 presents the algorithm used to processthe  default  case  rule  and  verb  case  rules.Simplification rules are applied against all nodes inconstituent tree, one node at a time, using breadth-first traversing.11 Some reinflections may not be visible on exampletranslation.140Step Description1 Validate these prerequisites or give up:1.1     Node must be root1.2     Predictor must have an inflection of auxiliaryverb to be1.3     Main verb has to be in past participle2 Transform subject into direct object3 Fix the predicator3.1 If main verb is finite then:main verb gets mode and tense from to bemain verb gets person according to agent3.2 Else:main verb gets mode and tense from verb to befinite verb gets person according to agent3.3 Remove verb to be4 Transform passive agent into a new subjectTable 1: Algorithm for default and verb cases3.3 SubordinationTypes of subordinate clauses are presented in Table2.
Two clauses are not processed: comparative andproportional.
Comparative  and  proportionalclauses will be addressed in future work.id Clause type Processedd Relative Restrictive ?e Relative  Non-restrictive ?f Reason ?g Comparativeh Concessive ?i Conditional ?j Result ?k Confirmative ?l Final Purpose ?m Time ?w ProportionalTable 2: Subordinate clausesSpecific  rules  are  used  for  groups  of  relatedsubordinate cases.
At least one of two operationscan  be  found  in  all  rules:  component  reorderingand sentence splitting.
Below, letter codes are usedto  describe  rules  involving  these  two  and  othercommon operations:A additional processingM splitting-order main-subordinateP Also processes non-clause phrases and/or non-finite clausesR component reorderingS splitting-order subordinate-mainc clone subject or turn object of a clause intosubject in another if it is necessaryd marker deletionm marker replacementv verb reinflection[na] not simplified due ambiguity[nf] not simplified, future case[np] not simplified due parsing problems2...8 covered cases (when more than one applies)Table  3 presents the marker information.
Theyare used to select sentences for simplification, andseveral  of  them  are  replaced  by  easier  markers.Cases  themselves  are  not  detailed since they aretoo  numerous  (more  than  40  distinct  cases).Operation  codes  used  for  each  marker  aredescribed in column ?Op?.
It is important to noticethat  multi-lexeme  markers  also  face  ambiguitiesdue to co-occurrence of its component lexemes12.The  list  does  not  cover  all  possible  cases,  sincethere  may  be  additional  cases  not  seen  in  thecorpus.
As relative clauses (d and e) require almostthe same processing, they are grouped together.Several  clauses  require  additional  processing.For  example,  some  conditional  clauses  requirenegating the main clause.
Other examples includenoun phrases replacing clause markers and clausereordering, both for relative clauses, as showed in(5).
The marker  cujo (whose) in the example canrefer to Northbridge or to the building.
Additionalprocessing  is  performed  to  try  to  solve  thisanaphora13,  mostly  using  number  agreementbetween the each possible co-referent and the mainverb in the subordinate clause.
The simplificationengine can give up in ambiguous cases (focusingon  precision)  or  elect  a  coreferent  (focusing  onrecall),  depending  on  the  number  of  possiblecoreferents  and  on  a  confidence  thresholdparameter, which was not used in this paper.O: Ele[He] deve[should] visitar[visit] o[the]pr?dio[building] em[in] Northbridgecujo[whose] desabamento[landslide]matou[killed] 16 pessoas[people].
(5)S: Ele[He] deve[should] visitar[visit] o[the]pr?dio[building] em[in] Northbridge.
O[The]desabamento[landslide] do[of  the]pr?dio[building] em[in] Northbridgematou[killed] 16 pessoas[people].12 For example, words ?de?, ?sorte?
and ?que?
can beadjacent  to each other without the meaning of ?de sorteque?
marker (?so that?
).13 We opted to solve this kind of anaphora instead of usingpronoun insertion in order to facilitate the reading of thetext.1413.4 Evaluation in the development corpusFigure 1 provides statistics from the of processingall  identified  cases  in  the  development  corpus.These statistics cover number of cases rather thanthe  number  of  sentences  containing  cases.
Thecases  ?incorrect  selection?
and  ?incorrectsimplification?
affect  precision  by  generatingungrammatical  sentences.
The  former  refers  tosentences  that  should  not  be  selected  for  thesimplification  process,  while  the  latter  refers  tosentences  correctly  selected  but  wronglysimplified.
There  are  three  categories  affectingrecall, classified according to their priority in thesimplification  engine.
Pending cases  areconsidered  to  be  representative,  with  higherpriority.
Possible cases  are  considered  to  beunrepresentative.
Having less priority, they can behandled in future versions of the engine.
Finally,Skipped cases  will  not  be  implemented,  mainlybecause  of  ambiguity,  but  also  due  to  lowrepresentativeness.
It  is  possible  to  observe  thatcategories  reducing  precision  (incorrect  selectionand simplification) represent a smaller number ofcases (5%) than categories reducing recall (45%).It  is  worth  noticing  that  our  approach  focus  onprecision  in  order  to  make  the  simplification  asautomatic  as  possible,  minimizing  the  need  forhuman interaction.Figure 1: Performance on the developmentcorpusThere are some important remarks regarding thedevelopment corpus used during the programmingphase.
First,  some  cases  are  not  representative,therefore  the  results  are  expected  to  varysignificantly in real texts.
Second, a few cases arenot orthogonal: i.e.,  there are sentences that can beclassified  in  more  than  one  case.
Third,  severalerrors  refer  to  sub-cases  of  cases  being  mostlycorrectly  processed,  which are  expected to occurless frequently.
Fourth, incorrect parsed sentenceswere not take in account in this phase.
Althoughthere may exist other cases not identified yet, it isplausible to estimate that only 5% of known casesare affecting the precision negatively.id Marker Op id Marker Op id Markers Opde que [that/which] 8MRAdv h se bem que [albeit] Mmv j tanto ?
que [so ?
that] [nf]de o qual [which]* 8MRAdv h ainda que [even if] 2Mm j tal ?
que [such ?
that] [nf]de como [as] [na] h mesmo que [even if] 2Mm j tamanho ?
que [so ?
that]* [nf]de onde [where] [nf] h nem que [even if] 2Mm k conforme [as/according] 3PRAcmde quando [when] [na] h por mais que [whatever] 2Mm k consoante [as/according] 3PRAcmde quem [who/whom] [nf] h mas [but] [np] k segundo [as/according] 3PRAcmde quanto [how much] [nf] i contanto que [provided that] 2Rmv k como [as] [na]de cujo [whose]* MAd i caso [case] 2Rmv l a fim de [in order to] 2PMcmde o que [what/which] Sd i se [if/whether] 2Rmv l a fim de que [in order that] 2PMcmf j?
que [since] Scm i a menos que [unless] 2RAmv l para que [so that] 2PMcmf porquanto [in view of] Scm i a n?o ser que [unless] 2RAmv l porque [because] [na]f uma vez que [since] Scm i exceto se [unless] 2RAmv m assim que [as soon as] 5PMAcvrf visto que [since] Scm i salvo se [unless] 2RAmv m depois de [after] 5PMAcvrf como [for] [na] i antes que [before] Rmv m depois que [after] 5PMAcvrf porque [because] [na] i sem que [without] Rmv m logo que [once] 5PMAcvrf posto que [since] [na] i desde que [since] RAmv m antes que [before] PSAcvrf visto como [seen as] [na] j de forma que [so] 5Mmv m apenas [only] [na]f pois que [since] [nf] j de modo que [so] 5Mmv m at?
que [until] [na]h apesar de que [although] Mmv j de sorte que [so that] 5Mmv m desde que [since] [na]h apesar que [despite] Mmv j tamanho que [so that]* 5Mmv m cada vez que [every time] [nf]h conquanto [although] Mmv j tal que [such that] 5Mmv m sempre que [whenever] [nf]h embora [albeit] Mmv j tanto que [so that] (1)* [na] m enquanto [while] [nf]h posto que [since] Mmv j tanto que [so that] (2) [na] m mal [just] [na]h por muito que [although] Mmv j t?o ?
que [so ?
that] [nf] m quando [when] [na]* gender and/or number variationTable 3: Marker processingcorrect 45%incorrectsimplification 4% incorrectselection 1%pending 17%possible 7%skipped 25%1424 Engine evaluation4.1 Evaluation patternsThe  evaluation  was  performed  on  a  sample  ofsentences  extracted  from Wikipedia's  texts  usinglexical patterns.
These patterns allows to filter thetexts,  extracting  only  relevant  sentences  forprecision and recall evaluation.
They were createdto  cover  both  positive  and  negative  sentences.They are applied before parsing or Part of Speech(PoS)  analysis.
For  passive  voice  detection,  thepattern is  defined as a sequence of  two or  morepossible verbs (no PoS in use) in which at least oneof them could be an inflection of verb to be.
Forsubordination detection, the pattern is equivalent tothe  discourse  markers  associated  with  eachsubordination type, as shown in Table 3.The  patterns  were  applied  against  featuredarticles  appearing  in  Wikipedia's  front  page  in2010 and 2011, including featured articles plannedto be featured, but not featured yet.
A maximum of30 sentences resulting from each pattern matchingwere then submitted to the simplification engine.Table 4 presents statistics from featured articles.texts 165sentences 83,656words 1,226,880applied patterns 57,735matched sentences 31,080Table 4: Wikipedia's featured articles (2010/2011)The number of applied patterns represents bothpatterns to be simplified (s-patterns) and patternsnot  to  be  simplified  (n-patterns).
N-patternsrepresent both non-processable patterns due to highambiguity (a-patterns) and pattern extraction falsenegatives.
We observed a few, but very frequent,ambiguous patterns introducing noise, particularlyse and  como.
In  fact,  these  two  markers  are  sonoisy that  we were not  be able  to  provide goodestimations  on  their  true  positives  distributiongiven the 30 sentences limit per pattern.
Similarlyto the number of applied patterns, the number ofmatched sentences correspond to both sentences tobe simplified and not to be simplified.Table  5 presents  additional  statistics  aboutcharacters,  words  and  sentences  calculated  in  asample of 32 articles where the 12 domains of thePortuguese Wikipedia are balanced.
The number ofautomatic simplified sentence is also presented.
InTable  5,  simple  words refers  to  percentage  ofwords  which  are  listed  on  our  simple  word  list,supposed to be common to youngsters,  extractedfrom the dictionary described in (Biderman, 2005),containing 5,900 entries.
Figure 2 presents clausedistribution per sentence in  the balanced sample.Zero  clauses refers  to  titles,  references,  figurelabels, and other pieces of text without a verb.
Weobserved  60%  of  multi-clause  sentences  in  thesample.characters per word 5.22words per sentence 21.17words per text 8,476simple words 75.52%sentences per text 400.34passive voice 15.11%total sentences 13,091simplified sentences 16,71%Table 5: Statistics from the balanced text sampleFigure 2: Clauses per sentence in the sample4.2 Simplification analysisWe manually analysed and annotated all sentencesin  our  samples.
These  samples  were  used  toestimate  several statistics, including the number ofpatterns  per  million  words,  the  system precisionand  recall  and  the  noise  rate.
We  opted  foranalysing  simplified  patterns  per  million  wordsinstead of  per simplified sentences.
First, becausean analysis based on sentences can be misleading,since  there  are  cases  of  long  coordinations  withmany patterns, as well as succinct sentences withno patterns.
Moreover,  one incorrectly  simplifiedmarker in a sentence could hide useful statistics ofcorrectly  simplified  patterns  and  even  of  otherincorrectly simplified patterns.The samples are composed by s-patterns and n-patterns  (including  a-patterns).
In  total  1,243patterns were annotated.
Table  6 presents patternestimates per million words.0 1 2 3 4 5 6 7 80,00000,05000,10000,15000,20000,25000,3000clausesdistribution[0-1]>7143Total patterns 70,834Human s-patterns 33,906Selection s-patterns 27,714Perfect parser s-patterns 23,969Obtained s-patterns 22,222Table 6: Patterns per million wordsTotal patterns refers to the expected occurrencesof  s-patterns  and  n-patterns  in  a  corpus  of  onemillion  words.
This  is  the  only  informationextracted from the full corpus, while the remainingfigures are estimates from the sample corpus.Human s-patterns is an estimate of the numberpatterns that a human could simplify in the corpus.Unlike  other  s-pattern  estimates,  a-patterns  areincluded, since a human can disambiguate them.
Inother words, this is the total of positive patterns.The estimate  does  not  include very rare  (samplesize equals to zero) or very noisy markers (patternspresenting 30 noisy sentences in its sample).Selection  s-patterns are  an  estimate  of  thenumber  of  patterns  correctly  selected  forsimplification,  regardless  of  whether  the  patternsimplification is correct or incorrect.
Precision andrecall derived from this measure (Table 7) considerincorrectly simplified patterns,  and do not includepatterns with parsing problems.
Its  purpose is  toevaluate how well the selection for simplificationis performed.
Rare or noisy patterns, whose humans-patterns  per  sample  is  lower  than  7,  are  notincluded.Perfect  parser  s-patterns is  an  estimate  verysimilar to selection s-patterns, but considering onlycorrectly  simplified  patterns.
As  in  selection  s-patterns,  incorrect  parsed  sentences  are  notincluded in calculations.
This is useful to analyseincorrect simplifications due to simplification ruleproblems, ignoring errors originating from parsing.Finally,  obtained  s-patterns refers  to  theestimate of correct simplified patterns,  similar toperfect  parser  s-patterns,  but  includingsimplification  problems  caused  by  parsing.
Thisestimate  represents  the  real  performance  to  beexpected from the system on Wikipedia's texts.It is important to note that the real numbers ofselection  s-patterns,  perfect  s-patterns  andobtained s-patterns  is expected to be bigger thanthe estimates,  since noisy and rare  pattern couldnot used be used in calculations (due the thresholdof  7  human  s-patterns  per  sample).
The  datapresented on Table 6 is calculated using estimatedlocal precisions for each pattern.
Table  7 presentsglobal  precision,  recall  and  f-measure  related  toselection,  perfect  parser  and  obtained s-patterns.The  real  values  of  the  estimates  are  expected tovariate up to +/- 2.48% .Measures Precision Recall F-measureSelection 99.05% 82.24% 89.86%Perfect parser 85.66% 82.24% 83.92%Obtained 79.42% 75.09% 77.20%Table 7: Global estimated measuresAlthough the precision of the selection seems tobe  impressive,  this  result  is  expected,  since  ourapproach  focus  on  the  processing  of  mostlyunambiguous  markers,  with  sufficient  syntacticinformation.
It is also due to the the threshold of 7human s-patterns  and the fact  that  a-patterns  arenot  included.
Due to  these two restrictions,  onlyapproximately 31.5% of unique patterns could beused for the calculations in Table  7.
Interestingly,these unique patterns correspond to 82.5% of thetotal estimated human s-patterns.
The majority ofthe 17.5%  remaining s-patterns refers to patternstoo  noisy  to  be  analysed  and  to  a-patterns  (notprocessed  due  ambiguity),  and  also  others  n-patterns which presented a low representativenessin  the  corpus.
The  results  indicate  goodperformance in rule formulation, covering the mostimportant (and non-ambiguous) markers, which isalso confirmed by the ratio between both selections-patterns  and  human  s-patterns  previouslypresented on Table 6.An  alternative  analysis,  including  a-patterns,lowers recall and f-measure, but not precision (ourfocus in this work).
In this case, recall drops from75.09% to  62.18%,  while  f-measure  drops  from77.20% to 70.18%.Figure 3: Pattern distributionFigure  3 presents  the  distribution  of  patternsaccording to their frequency per million words andtheir purity (1 - noisy rate).
This data is useful to2,0 20,0 200,0 2000,0 20000,0 200000,00,00000,20000,40000,60000,80001,00001,2000b-passivade-quel-a_fim_dej-tal_queJ-tanto_*_queFrequency PMWPurity144identify  most  frequent  patterns  (such  as  passivevoice in  b-passiva)  and patterns with medium tohigh  frequency,  which  are  easy  to  process  (notambiguous), such as l-a_fim_de.5 Issues on simplification qualityThis analysis aims at identifying factors affectingthe quality of simplifications considered as correct.Hence, factors affecting the overall simplified textquality  are  also  presented.
In  contrast,  thequantitative  analysis  presented  on  Section  4.2covered  the  ratio  between  incorrect  and  correctsimplifications.Three cases of clause disposition were identifiedas  important  factors  affecting  the  simplifiedsentence  readability.
These  cases  are  presentedusing  the  following  notation:  clauses  arerepresented  in  uppercase  letters;  clauseconcatenation represents coordination; parenthesesrepresent  subordination;  c1 and  c2 representclause/sentence  connectors  (including  markers);the  entailment  operator  (?)
represents  thesimplification rule transforming clauses.?
?A(B(c1 C)) ?
A(B).
c2 C?
: the vertical case.In this scenario it is more natural to read c2 asconnecting  C to the main clause  A, while  c1connects  C to  B,  as seen in (6).
This is stillacceptable for several sentences analysed, butwe  are  considering to  simplify only level  2clauses in the future, splitting C  from B onlyif another rule splits A and B first.?
?A(B)CD  ?
ACD.
c1 B?
:  the  horizontalcase.
In this scenario, c1 correctly connects Aand B, but long coordinations following A canimpact  negatively on text  reading,  since thetarget  audience  may  forget  about  A whenstarting  to  read  B.
In  this  scenario,coordination  compromise  subordinationsimplification,  showing  the  importance  ofsimplifying coordination as well, even thoughthey  are  considered  easier  to  read  thansubordination.?
Mixed  case:  this  scenario  combines  thepotential problems of horizontal and verticalcases.
It  may  occur  in  extremely  longsentences.Besides  clause  disposition  factors,  clauseinversions can also lead to  problems in sentencereadability.
In  our  current  system,  inversion  ismainly used to produce simplified sentences in thecause-effect  order  or  condition-action  order.Reordering, despite using more natural orders, cantransform  anaphors  into  cataphors.
A  goodanaphora resolution system would be necessary toavoid  this  issue.
Another  problem  is  movingsentence connectors as in ?A.
c1 BC.
?
A.
B. c2 c1C?,  while  ?A.
c1 B.  c2 C?
is  more  natural(maintaining c1 position).O: Ela[She] dissertou[talked] sobre[about]como[how] motivar[to motive] o[the]grupo[group] de_modo_que[so that] seu[their]desempenho[performance] melhore[improves] (6)S: [He/She] dissertou[talked] sobre[about]como[how] motivar[to motive] o[the]grupo[group].
Thus, seu[their]desempenho[performance] melhore[improves]We  have  observed  some  errors  in  sentenceparsing,  related  to  clause  attachment,  generatingtruncated ungrammatical text.
As a result, a badlysimplified key sentence can compromise the textreadability more than several  correctly simplifiedsentences  can  improve  it,  reinforcing  theimportance  of  precision  rather  than  recall  inautomated text simplification.Experienced  readers  analysed  the  simplifiedversions of the articles and considered them easierto read than the original ones in most cases, despitesimplification  errors.
Particularly,  the  readersconsidered  that  the  readability  would  improvesignificantly  if  cataphor  and horizontal  problemswere  addressed.
Evaluating  the  simplificationswith readers from the target audience is left  as afuture work, after  improvements in the identifiedissues.6 ConclusionsWe  have  presented  a  simplification  engine  toprocess texts from the Portuguese Wikipedia.
Ourquantitative  analysis  indicated  a  good  precision(79.42%),  and  reasonable  number  of  correctsimplifications  per  million  words  (22,222).Although our focus was on the encyclopedic genreevaluation,  the  proposed  system  can  be  used  inother genres as well.AcknowledgementsWe thank FAPESP (p.  2008/08963-4)  and CNPq(p. 201407/2010-8) for supporting this work.145ReferencesJ.
Abedi,  S.  Leon,  J.  Kao,  R.  Bayley,  N.  Ewers,  J.Herman and K. Mundhenk.
2011.
Accessible ReadingAssessments for Students with Disabilities: The Roleof  Cognitive,  Grammatical,  Lexical,  andTextual/Visual  Features.
CRESST  Report  785.National  Center  for  Research  on  Evaluation,Standards,  and  Student  Testing,  University  ofCalifornia, Los Angeles.S.
M.  Alu?sio,   C.  Gasperin.
2010.
Fostering  DigitalInclusion and Accessibility: The PorSimples projectfor Simplification of Portuguese Texts.
Proceedingsof  the  NAACL  HLT  2010  Young  InvestigatorsWorkshop  on  Computational  Approaches  toLanguages of the Americas.
: ACL, New York, USA.v.
1. p. 46-53.A.
Barreiro,  L.  M.  Cabral.
2009.
ReEscreve:  atranslator-friendly  multi-purpose  paraphrasingsoftware  tool.
The  Proceedings  of  the  WorkshopBeyond  Translation  Memories:  New  Tools  forTranslators,  The  Twelfth  Machine  TranslationSummit.
Ontario, Canada, pp.
1-8.E.
Bick.
2006.
The  parsing  system  ?Palavras?
:Automatic grammatical  analysis of  Portuguese in aconstraint  grammar  framework.
Thesis  (PhD).University of ?rhus, Aarhus, Denmark.M.
T.  C.  Biderman.
2005.
Dicion?rio  Ilustrado  dePortugu?s.
Editora ?tica.
1a.
ed.
S?o PauloJ.
Carroll, G. Minnen, D. Pearce, Y. Canning, S. Devlinand  J.  Tait.
1999.
Simplifying  Text  for  Language-Impaired  Readers,.
In  Proceedings  of  the  9thConference  of  the  European  Chapter  of  theAssociation for Computational  Linguistics  (EACL),269-270.R.
Chandrasekar  and  B.  Srinivas.
1997.
AutomaticInduction  of  Rules  for  Text  Simplification.Knowledge-Based Systems, 10, 183-190.G.
E. Chappell.
1985.
Description and assessment oflanguage disabilities of junior high school students.In:  Communication  skills  and  classroom  success:Assessment  of  language-learning  disabled  students.College- Hill Press,  San Diego,  pp.
207-239.W.
Daelemans,  A.  Hothker  and  E.  T.  K.  Sang.
2004.Automatic Sentence Simplification for Subtitling inDutch  and  English.
In:  Proceedings  of  the  4thConference on Language Resources and Evaluation,Lisbon, Portugal 1045-1048.J.
De Belder and M. Moens.
2010.
Text simplificationfor children.
Proceedings of the SIGIR Workshop onAccessible Search Systems, pp.19-26.S.
Devlin  and  G.  Unthank.
2006.
Helping  aphasicpeople process online information.
In: Proceedings ofthe  ACM  SIGACCESS  2006,  Conference  onComputers  and  Accessibility.
Portland,  Oregon,USA , 225-226.K.
Inui, A. Fujita, T. Takahashi, R. Iida and T. Iwakura.2003.
Text Simplification for Reading Assistance: AProject  Note.
In  the  Proceedings  of  the  SecondInternational Workshop on Paraphrasing, 9-16.S.
Jonnalagadda  and  G.  Gonzalez.
2009.
SentenceSimplification  Aids  Protein-Protein  InteractionExtraction.
Proceedings  of  the  3rd  InternationalSymposium on Languages in Biology and Medicine,Short  Papers,  pages  109-114,  Jeju  Island,  SouthKorea, 8-10 November 2009.F.
W.  Jones,  K.  Long  and  W.  M.  L.  Finlay.
2006.Assessing the reading comprehension of adults withlearning disabilities.
Journal of Intellectual DisabilityResearch, 50(6), 410-418.B.
Klebanov,  K.  Knight  and  D.  Marcu.
2004.
TextSimplification for Information-Seeking Applications.In:  On  the  Move  to  Meaningful  Internet  Systems.Volume  3290,  Springer-Verlag,  Berlin  HeidelbergNew York, 735-747.S.
Martins, L. Filgueiras.
2007.
M?todos de Avalia?
?ode Apreensibilidade das Informa?
?es Textuais:  umaAplica?
?o  em  S?tios  de  Governo  Eletr?nico.
Inproceeding  of  Latin  American  Conference  onHuman-Computer Interaction (CLIHC 2007).
Rio deJaneiro, Brazil.A.
Max.
2006.
Writing for Language-impaired Readers.In: Proceedings of Seventh International Conferenceon  Intelligent  Text  Processing  and  ComputationalLinguistics.
Mexico City, Mexico.
Berlin HeidelbergNew York, Springer-Verlag, 567-570.C.
Napoles  and  M.  Dredze.
2010.
Learning  simpleWikipedia:  a  cogitation  in  ascertaining  abecedarianlanguage.
In the  Proceedings of the NAACL HLT2010  Workshop  on  Computational  Linguistics  andWriting:  Writing  Processes  and  Authoring  Aids(CL&W '10), 42-50.S.
E.  Petersen.
2007.
Natural  Language  ProcessingTools  for  Reading  Level  Assessment  and  TextSimplification for  Bilingual  Education.
PhD thesis.University of Washington.A.
Siddharthan.
2006.
Syntactic simplification and textcohesion.
Research  on  Language  &  Computation,4(1):77-109.L.
Specia.
2010.
Translating  from  Complex  toSimplified  Sentences.
9th  International  Conference146on  Computational  Processing  of  the  PortugueseLanguage.
Lecture  Notes  in  Artificial  Intelligence,Vol.
6001, Springer, pp.
30-39.D.
Vickrey and D. Koller.
2008.
Sentence Simplificationfor Semantic Role Labelling.
In: Proceedings of theACL-HLT.
344-352.W.
M. Watanabe,  A. Candido Jr, V. R. Uzeda, R. P. M.Fortes,  T.  A.  S.  Pardo  and  S.  M.  Alu?sio.
2009.Facilita:  Reading  Assistance  for  Low-literacyReaders.
In:  ACM  International  Conference  onDesign of Communication (SIGDOC 2009), volume1, Bloomington, US,   29-36.147
