Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 39?47,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsSemi-supervised Graph-based Genre Classification for Web PagesNoushin Rezapour AsheghiSchool of ComputingUniversity of Leedsscs5nra@leeds.ac.ukKatja MarkertL3S Research CenterLeibniz Universit?at Hannoverand School of ComputingUniversity of Leedsmarkert@l3s.deSerge SharoffSchool of ModernLanguages and CulturesUniversity of Leedss.sharoff@leeds.ac.ukAbstractUntil now, it is still unclear which set offeatures produces the best result in au-tomatic genre classification on the web.Therefore, in the first set of experiments,we compared a wide range of content-based features which are extracted fromthe data appearing within the web pages.The results show that lexical features suchas word unigrams and character n-gramshave more discriminative power in genreclassification compared to features suchas part-of-speech n-grams and text statis-tics.
In a second set of experiments,with the aim of learning from the neigh-bouring web pages, we investigated theperformance of a semi-supervised graph-based model, which is a novel techniquein genre classification.
The results showthat our semi-supervised min-cut algo-rithm improves the overall genre classifi-cation accuracy.
However, it seems thatsome genre classes benefit more from thisgraph-based model than others.1 IntroductionIn Automatic Genre Identification (AGI), docu-ments are classified based on their genres ratherthan their topics or subjects.
Genre classes suchas editorial, interview, news and blog which arerecognizable by their distinct purposes, can be onany topic.
The most important application of AGIcould be in Information Retrieval.
If a user coulduse the search engine to retrieve web pages froma specific genre such as news articles, reviewsor blogs, search results could be more beneficial.With the aim of enhancing search engines, AGIhas attracted a lot of attention (see Section 2).In this paper, we investigate two important openquestions in AGI.
The first question is: what setof features produces the best result in genre clas-sification on the web?
The drawbacks of exist-ing genre-annotated web corpora (low inter-coderagreement; false correlations between topic andgenre classes) resulted in researchers?
doubt on theoutcomes of classification models based on thesecorpora (Sharoff et al., 2010).
Therefore, in orderto answer this question, we perform genre classi-fication with a wide range of features on a reli-able and source diverse genre-annotated web cor-pus.
The second question that we investigate inthis paper is: could we exploit the graph structureof the web to increase genre classification accu-racy?
With the aim of learning from the neigh-bouring web pages, we investigated the perfor-mance of a semi-supervised graph-based model,which is a novel technique in genre classification.The remainder of this paper is structured as fol-lows.
After reviewing related work in Section 2,we compare different supervised genre classifica-tion models based on various lexical, POS-basedand text statistics features in Section 3.
Section 4describes our semi-supervised graph-based classi-fication experiment, where we use the multi-classmin-cut algorithm as a novel technique in genreclassification.
Section 5 concludes the findingsand discusses future work.2 Related WorkThere has been a considerable body of researchin AGI.
In previous studies on automatic genreclassification of web pages, various types of fea-tures such as common words (Stamatatos et al.,2000), function words (Argamon et al., 1998),word unigrams (Freund et al., 2006), charactern-grams (Kanaris and Stamatatos, 2007), part-of-speech tags (Karlgren and Cutting, 1994) , part-of-speech trigrams (Argamon et al., 1998; San-tini, 2007), document statistics (e.g.
average sen-tence length, average word length and type/tokenratio) (Finn and Kushmerick, 2006; Kessler et39al., 1997), HTML tags (e.g.
(Santini, 2007))have been explored.
However, researchers con-ducted genre classification experiments with dif-ferent features on different corpora with differ-ent sets of genre labels.
As a result, it is dif-ficult to compare them.
This motivated Sharoffet al.
(2010) to examine a wide range of word-based, character-based and POS-based features onthe existing genre-annotated corpora.
They re-ported that word unigrams and character 4-gramsoutperform other features in genre classification.However, they concluded that the results cannotbe trusted because of two main reasons.
First,some of these collections exhibit low inter-coderagreement and any results based on unreliable datacould be misleading.
Second, the spurious cor-relation between topic and genre classes in someof these corpora was one of the reasons for someof the very impressive results reported by Sharoffet al.
(2010).
These good results were achievedby detecting topics rather than genres of individ-ual texts.
A similar point was made by Petrenzand Webber (2010) who examined the impact oftopic change on the performance of AGI systems.They showed that a shift in topic can have a mas-sive impact on genre classification models whichare based on lexical features such as word uni-grams or character n-grams.
Therefore, the ques-tion which set of features produces the best resultin automatic genre classification on the web is stillan open question.
In order to investigate this ques-tion, we perform genre classification with a widerange of features on a reliable and topically diversedataset.
Section 3.1 describes the dataset and theexperimental setup.Most of the current works in the field of AGIconcentrated on extracting features from the con-tent of the documents and classify them by em-ploying a standard supervised algorithm.
How-ever, on the web there are other sources ofinformation which can be utilized to improvegenre classification of web pages.
For instance,the web has a graph structure and web pagesare connected via hyper-links.
These connec-tions could be exploited to improve genre clas-sification.
Various graph-based classification al-gorithms have been proposed to improve topicclassification for web pages, such as the re-laxation labelling algorithm (Chakrabarti et al.,1998), iterative classification algorithm (Lu andGetoor, 2003), Markov logic networks (Craneand McDowell, 2012), random graph walk (Linand Cohen, 2010) and weighted-vote relationalneighbour algorithm (Macskassy and Provost,2007).
These classification algorithms which uti-lize hyper-link connections between web pagesto construct graphs, outperformed the classifierswhich are solely based on textual content of theweb pages for topic classification.
Such connecteddata presents opportunities for boosting the perfor-mance of genre classification too.Graph-based web page classification presentedin studies such as (Crane and McDowell, 2012;Lu and Getoor, 2003; Macskassy and Provost,2007) on the WebKB dataset (CRAVEN, 1998)could be considered as genre classification as op-posed to topic classification.
The WebKB datasetcontains web pages from four computer sciencedepartments categorised into seven classes: stu-dent, faculty, staff, department, course, projectand other.
However, this dataset is very specificto the academic domain with low coverage forthe web overall, whereas we examine graph-basedlearning for automatic genre classification of webpages on a much more general dataset with pop-ular genre classes such as news, blog and edito-rial.
Moreover, the graph-based algorithms usedon the WebKB dataset are all supervised and wereperformed on a very clean and noise free datasetwhich was achieved by removing the class other.Class other contains all the web pages which donot belong to any other predefined classes.
How-ever, our experiment is in a semi-supervised man-ner which is a much more realistic scenario on theweb, because it is highly unlikely that for eachweb page, we have genre labels for all its neigh-bouring web pages as well.
Therefore, we per-form our experiment on a very noisy dataset whereneighbouring web pages could belong to none ofour predefined genre classes.
Section 4 describesour semi-supervised graph-based classification ex-periment, where we use a multi-class min-cut al-gorithm as a novel technique in genre classifica-tion.3 Content-based Classification3.1 Dataset and Experimental SetupPetrenz and Webber (2010) and Sharoff etal.
(2010) emphasize that the impact of topic ongenre classification should be eliminated or con-trolled.
In order to avoid the influence of topic ongenre classification, some researchers (e.g.
(Sta-40Number of # of pages fromGenre the same website Fleiss?s ?web pages websites max min medPersonal Homepage (php) 304 288 9 1 1 0.858Company/ Business Homepage (com) 264 264 1 1 1 0.713Educational Organization Homepage (edu) 299 299 1 1 1 0.953Personal Blog /Diary (blog) 244 215 9 1 1 0.812Online Shop (shop) 292 209 23 1 1 0.830Instruction/ How to (instruction) 231 142 15 1 1 0.871Recipe 332 116 8 1 1 0.971News 330 127 12 1 1 0.801Editorial 310 69 11 1 3 0.877Conversational Forum (forum) 280 106 11 1 1 0.951Biography (bio) 242 190 15 1 1 0.905Frequently Asked Questions (faq) 201 140 8 1 1 0.915Review 266 179 15 1 1 0.880Story 184 24 38 1 7 0.953Interview 185 154 11 1 1 0.905Table 1: Statistics for each category illustrate source diversity and reliability of the corpus (Asheghi etal., 2014).
To save space, in this paper we use the abbreviation of genre labels which are specified afterthe genre names.matatos et al., 2000) and (Argamon et al., 1998))use only topic independent features such as com-mon words or function words in genre classifica-tion.
However, neither of these features are exclu-sive to genre classification.
Function words andcommon words are used in authorship classifica-tion (e.g.
(Argamon et al., 2007)) because they cancapture the style of the authors without being in-fluenced by the topics of the texts.
On the otherhand, word unigrams are a popular document rep-resentation in topic classification.
If we want thesemodels to capture the genre of documents withoutbeing influenced by their topics or the style of theirauthors, we must eliminate the influence of thesefactors on genre classification by keeping themconstant across the genre classes in the trainingdata.
That means all the documents in the train-ing set should be about the same topic and writtenby the same person.
However, constructing such adataset is practically impossible for genre classeson the web.
The other more practical solution tothis problem would be to collect data from varioustopics and sources in order to minimize the im-pact of these factors on genre classification.
Forthat reason, we (Asheghi et al., 2014) created aweb genre annotated corpus which is reliable (withFleiss?s kappa (Fleiss, 1971) equal to 0.874) andsource diverse.
We tried to reduce the influenceof topic, the writing style of the authors as well asthe design of the websites on genre classificationby collecting data from various sources and top-ics.
The corpus consists of 3964 web pages from2522 different websites, distributed across 15 gen-res (Table 1).Moreover, we prepared two versions of thecorpus: the original text and the main text cor-pora.
First, we converted web pages to plaintext by removing HTML markup using the Krd-Wrd tool (Steger and Stemle, 2009).
This re-sulted in the original text corpus which containsindividual web pages with all the textual elementspresent on them.
Moreover, in order to investigatethe influence of boilerplate parts (e.g.
advertise-ments, headers, footers, template materials, navi-gation menus and lists of links) of the web pageson genre classification, we removed the boilerplateparts and extracted the main text of each web pageusing the justext tool1.
This resulted in the cre-ation of the main text corpus.
This is the first timethat the performance of genre classification mod-els is compared on both the original and the maintext of the web pages.Since the outputs of the justext tool for 518 ofthe web pages were empty files, the main text cor-pus has fewer pages.
However, the main text cor-pus still has a balanced distribution with a rela-tively large number of web pages per category.
Ta-ble 2 compares the number of web pages in the twoversions of the corpus.
For all the experiments weuse this corpus via 10-fold cross-validation on theweb pages.
Also, in order to minimize the effectof factors such as topic, the writing style of the au-thors and the design of the websites even further,we ensured that all the web pages from the samewebsite are in the same fold.
Many, if not all of theprevious studies in automatic genre classificationon the web ignored this essential step when divid-ing the data into folds.
For machine learning, we1http://code.google.com/p/justext/41Number of web pages in corporaGenre Original text Main textphp 304 221com 264 190edu 299 191blog 244 242shop 292 221instruction 231 229recipe 332 243news 330 320editorial 310 307forum 280 251bio 242 242faq 201 160review 266 262story 184 184interview 185 183Table 2: Number of web pages in individual genreclasses in both original text and main text corpora.chose Support Vector Machines (SVM) becauseit has been shown by other researchers in AGI(e.g.
(Santini, 2007)) that SVM produces better orat least similar results compared to other machinelearning algorithms.
We used the one-versus-onemulti-class SVM implemented in Weka2with thedefault setting.
All the experiments are carried outon both the original text and the main text corpora.3.2 FeaturesIn order to compare the performance of differ-ent lexical and structural features used in previouswork, we reimplemented the following publishedapproaches to AGI: function words (Argamon etal., 1998), part-of-speech n-grams (Santini, 2007),word unigrams (Freund et al., 2006) and charac-ter 4-grams binary representation (Sharoff et al.,2010).
We also explored the discriminative powerof other features such as readability features (Pitlerand Nenkova, 2008), HTML tags3and named en-tity tags in genre classification (Table 3).
This isthe first time that some of these features such asaverage depth of syntax trees and entity coherencefeatures (Barzilay and Lapata, 2008) are used forgenre classification.
To set a base-line, we useda list of genre names (e.g.
news, editorial, in-terview, review) as features.
We used two differ-ent feature representations: binary and normalizedfrequency.
In the binary representation of a doc-ument, the value for each feature is either one orzero which represents the presence or the absenceof each feature respectively.
In the normalized fre-2http://www.cs.waikato.ac.nz/ml/weka/3http://www.w3schools.com/tags/ref byfunc.aspquency representation of a document, the value foreach feature is the frequency of that feature whichis normalized by the length of the document.For extracting lexical features, we tokenizedeach document using the Stanford tokenizer (in-cluded as part of the Stanford part of speech tag-ger (Toutanova et al., 2003)) and converted all thetokens to lower case.
For extracting POS tagsand named entity tags, we used the Stanford max-imum entropy tagger4and the Stanford NamedEntity Recognizer5respectively.
For extractingsome of the readability features such as averageparse tree height and average number of noun andverb phrases per sentences, we used the StanfordParser (Klein and Manning, 2003).
However, webpages must be cleaned before they can be fed toa parser, because parsers cannot handle tables andlist of links.
Therefore, we only used the maintext of each web page as an input to the parser.For web pages for which the justext tool producedempty files, we treated these features as missingvalues.
Moreover, we used the Brown CoherenceToolkit6to construct the entity grid for each webpage and computed the probability of each entitytransition type.
This tool needs the parsed versionof the text as an input.
Therefore, for web pagesfor which the justext tool produced empty files, wealso treated these features as missing values.3.3 Results and DiscussionTable 4 shows the result of the different featuresets listed in the previous section on both the orig-inal text and the main text corpora.
At first glance,we see that the results of genre classification onthe original text corpus are higher than the maintext corpus.
This shows that boiler plates containvaluable information which helps genre classifica-tion.Moreover, the results show that binary repre-sentation of word unigrams is the best performingfeature set when we use the whole text of the webpages.
However, on the main text corpus, charac-ter 4-grams outperform other features.
This con-firms the results reported in (Sharoff et al., 2010).The results also highlight that the performance ofPOS-based features are much less accurate thanthat of textual features such as word unigrams andcharacter n-grams.
The results also show that thecombination of word unigrams, text statistics and4http://nlp.stanford.edu/software/tagger.shtml5http://nlp.stanford.edu/software/CRF-NER.shtml6http://www.cs.brown.edu/ melsner/manual.html42Category FeaturesToken features number of tokens and number of typesnormalized frequency of punctuation marks and currency charactersNamed entity tags normalized frequency of tags: time, location, organization, person, money, dateaverage parse tree heightaverage sentence length and word lengthReadability features standard deviation of sentence length and of word lengthaverage number of syllables per wordtype/token ratioaverage number of noun phrases and verb phrases per sentenceentity coherence features (Barzilay and Lapata, 2008)HTML tags normalized frequency of tags for: sections / style, formatting, programming,visual features such as forms, images, lists and tablesTable 3: List of text statistics features explored in this paperpart of speech features resulted in improving genreclassification accuracy (compared to the accuracyachieved by word unigrams alone), for both origi-nal and main text corpora.
However, while the im-provement for the main text corpus is statisticallysignificant7, there is no significant difference be-tween these two models for the original corpus.Surprisingly, adding part of speech 3-grams to theword unigrams features decreased the genre clas-sification accuracy in both original and main textcorpora.
The reason could be that the model isover-fitted on the training data and as a result, itperforms poorly on the test data.
Therefore, com-bining various features will not always improvethe performance of the classification task.
More-over, for extracting POS-based features and someof the text statistics features we rely on tools suchas part-of-speech taggers and parsers whose per-formance varies for different genres.
Even the bestpart-of-speech taggers and parsers are error proneand cannot be trusted on new unseen genres.4 Graph-based ClassificationUntil now we extracted features only from the con-tent of the web pages.
However, other sourcesof information such as the connections and thelink patterns between the web pages could be ex-ploited to improve genre classification.
The under-lying assumption of this approach is that a page ismore likely to be connected to pages with the samegenre category.
For example, if the neighbouringweb pages of a particular web page are labelledas shop, it is more likely that this web page is ashop too, whereas, it is highly unlikely that it is anews or editorial.
This property (i.e.
entities withsimilar labels are more likely to be connected) isknown as homophily (Sen et al., 2008).
We hy-7McNemar test at the significance level of 5%pothesis that homophily exists for genre classesand it can help us to improve genre classifica-tion on the web.
In this paper, we use a semi-supervised graph-based algorithm namely, multi-class min-cut, which is a novel approach in genreclassification.
This algorithm, which is a collec-tive classification method, considers the class la-bels of all the web pages within a graph.4.1 Multi-class Min-cut: The Main IdeaThe Min-cut classification algorithm originallyproposed by Blum and Chawla (2001) is basedon the idea that linked entities have a tendencyto belong to the same class.
In other words, itis based on the homophily assumption.
There-fore, it should be able to improve genre classifica-tion on the web if our hypothesis holds.
However,this technique is a binary classification algorithm,whereas, we have a multi-class problem.
Unfor-tunately, multi-class min-cut is NP-hard and thereis no exact solution for it.
Nevertheless, Ganchevand Pereira (2007) proposed a multi-class exten-sion to Blum and Chawla (2001)?s min-cut algo-rithm by encoding a multi-class min-cut problemas an instance of metric labelling.
Kleinberg andTardos (1999; 2002) introduced metric labellingfor the first time.
The main idea of metric labellingfor web page classification can be described as fol-lows:Assume we have a weighted and undirectedgraph G = (V,E) where each vertex v ?
V is aweb page and the edges represent the hyper-linksbetween the web pages.
The task is to classifythese web pages into a set of labels L. This task canbe denoted as a function f : V ?
L. In order todo this labelling task in an optimal way, we need tominimize two different types of costs.
First, thereis a non-negative cost c(v, l) for assigning label l43Feature set Original text Main textgenre names bin 57.39 29.02genre name nf 38.29 14.16function words bin 65.71 55.57function words nf 74.95 66.86word unigrams bin 89.32 76.61word unigrams nf 85.21 74.91character 4-grams bin 87.96 78.88POS-3grams bin 73.18 61.23POS-3grams nf 70.28 57.83POS-2grams bin 64.10 54.91POS-2grams nf 68.94 60.76POS nf 60.14 54.64text statistics 55.47 59.17word unigrams bin + text statistics 89.48 78.09word uni-grams bin + text statistics + POS nf 89.63 78.24word uni-grams bin + POS 3-grams bin 88.14 75.59Table 4: Classification accuracy of different features in genre classification.
bin and nf refer to the use ofbinary and normalized frequency representation of the features respectively.to web page v. Second, if two web pages v1and v2are connected together with an edge e with weightwe, we need to pay a cost of we?
d(f(v1), f(v2))where d(., .)
denotes distance between the two la-bels.
A big distance value between labels indicatesless similarity between them.
Therefore, the totalcost of labelling task f is:(1)E(f) =?v?Vc(v, f(v)) +?e=(v1,v2)?Ewe?
d(f(v1), f(v2))Kleinberg and Tardos (1999; 2002) developedan algorithm for minimizing E(f).
However,their algorithm uses linear programming which isimpractical for large data (Boykov et al., 2001).In a separate study for metric labelling problems,Boykov et al.
(2001) have developed a multi-waymin-cut algorithm to minimize E(f).
This algo-rithm is very fast and can be applied to large-scaleproblems with good performance (Boykov et al.,2001).4.2 Selection of unlabelled dataA web page w has different kind of neighbours onthe web such as parents, children, siblings, grandparents and grand children which are mainly dif-ferentiated based on the distance to the target webpage as well as the direction of the links (Qi andDavison, 2009).
Since the identification of chil-dren of a web page (i.e.
web pages which haveCosine # of unlabelled Average # ofsimilarity web pages neighbours?
0 103,372 40.65?
0.1 98,824 39.08?
0.2 87,834 34.23?
0.3 70,602 26.46?
0.4 50,232 17.52?
0.5 28,437 8.62?
0.6 13,919 3.77?
0.7 7,241 1.86?
0.8 3,772 0.98?
0.9 1,732 0.44Table 5: Number of unlabelled web pages withdifferent cosine similarity thresholds.
The last col-umn shows the average number of neighbours perlabelled page.direct links from the target web page) is a straight-forward task as their URLs can be extracted fromthe HTML version of the target web page, in thisstudy, we explore the effect of the target webpages?
children on genre classification.
Therefore,in this experiment, by neighbouring web pages wemean the web pages?
children.
In order to collectthe neighbouring web pages, for every web page inthe data set, we extracted all its out-going URLsand downloaded them as unlabelled data.
How-ever, using all these neighbouring pages couldhurt the genre classification accuracy because webpages are noisy (e.g.
links to advertisements) andsome neighbours could have different genres thanthe target page.
In order to control the negative im-pact of such neighbours, we could preselect a sub-set of neighbours whose content are close enoughto the target page.
To implement this idea, we44computed the cosine similarity between the webpage w and its neighbouring web pages and useddifferent threshold to select the neighbours.
If u isa neighbour of w and?
?u and?
?w are the represen-tative feature vectors of these two web pages re-spectively, we could compute the cosine similaritybetween these two web pages using the followingformula:cos(?
?w ,?
?u ) =?
?w ???u??
?w ???
?u ?=?ni=1wi?
ui??ni=1(wi)2??
?ni=1(ui)2(2)where n is the number of the dimensions of thevectors and wiis the value of the ith dimensionof the vector?
?w .
Since the word unigrams bi-nary representation model yields the best result forcontent-based genre classification, we used thisrepresentation of web pages to construct their fea-ture vectors.
Table 5 shows the number of unla-belled data and the average number of neighboursper labelled web page for different cosine similar-ity thresholds.4.3 Formulation of Semi-supervisedMulti-class Min-cutsThe formulation of semi-supervised multi-classmin-cut for genre classification involves the fol-lowing steps:1.
We built the weighted and undirected graphG = (V,E) where vertices are the web pages(labelled and unlabelled) and the edges rep-resent the hyper-links between the web pagesand set the weights to 1.2.
For training nodes, set the cost of the correctlabel to zero and all other labels to a largeconstant.3.
For test nodes and unlabelled nodes, we setthe cost of each label using a supervised clas-sifier (SVM) using the following formula:c(w, l) = 1?
pl(w) (3)where c(w, l) is the cost of label l for webpage w and pl(w) is the probability of w be-longing to the label l which is computed by asupervised SVM using word unigrams binaryrepresentation of the web pages.4.
Set d(i, j), which denotes the distance be-tween two labels i and j, to 1 if i 6= j andzero otherwise.5.
Employ Boykov et al.
(2001) algorithm tofind the minimum total cost using multiwaymin-cut algorithm.4.4 Results and DiscussionWe divided the labelled data into 10 folds againensuring that all the web pages from the samewebsites are in the same fold.
We used 8 foldsfor training, one fold for validation and one foldfor testing.
We learnt the best cosine similar-ity threshold using validation data and then eval-uated it on the test data.
Tables 6 and 7 illus-trate the results of the multi-class min-cut algo-rithm and the content-based algorithm (both usingword unigrams as features) respectively.
The re-sults show that the multi-class min-cut algorithmsignificantly outperforms8the content-based clas-sifier for the cosine similarity equal or greater than0.8 which was chosen on the validation data.
Itmust be noted that the result of the multi-classmin-cut algorithm when we used all the neigh-bouring pages was much lower than the content-based algorithm due to noise.
The results alsoshows that some genre classes such as news, edito-rial, blog, interview and instruction benefited morethan other genre classes from the neighbouringweb pages.
Genre categories with improved re-sults are shown in bold in Table 6.
The homophilyproperty of these genre categories was the reasonbehind this improvement.
For example, the factthat a news article is more likely to be linked toother news articles, whereas, an editorial is morelikely to be linked to other editorials, helped us todifferentiate these two categories further.
On theother hand, we observe no improvement or evendecrease in F-measure for some genre categoriessuch as frequently asked questions, forums andcompany home pages.
Two reasons could havecontributed to these results.
First, the homophilyproperty might not exist for these categories.
Sec-ond, the homophily property holds for these cate-gories, however, in order to benefit from this prop-erty, we need to examine other neighbours of thetarget web pages such as parents, siblings, grandparents, grand children or even more distant neigh-8McNemar test at the significance level of 5%45class Recall Precision F1-measurephp 0.928 0.850 0.887forum 0.925 0.977 0.951review 0.895 0.832 0.862news 0.897 0.798 0.845com 0.897 0.891 0.894shop 0.860 0.965 0.910instruction 0.870 0.914 0.892recipe 0.994 0.991 0.993blog 0.889 0.879 0.884bio 0.905 0.948 0.926editorial 0.800 0.932 0.861faq 0.902 0.841 0.870edu 0.957 0.963 0.960story 0.902 0.943 0.922interview 0.870 0.809 0.839overall accuracy = 90.11%Table 6: Recall, Precision and F-measure for multi-class min-cut genre classification.class Recall Precision F1-measurephp 0.938 0.798 0.862forum 0.943 0.974 0.958review 0.872 0.859 0.866news 0.894 0.782 0.835com 0.920 0.874 0.897shop 0.849 0.950 0.897instruction 0.866 0.889 0.877recipe 0.988 0.988 0.988blog 0.865 0.841 0.853bio 0.884 0.926 0.905editorial 0.765 0.926 0.837faq 0.866 0.879 0.872edu 0.950 0.969 0.959story 0.864 0.941 0.901interview 0.827 0.785 0.805overall accuracy = 88.98%9Table 7: Recall, Precision and F-measure for content-basedgenre classification using word unigrams feature setbours.5 Conclusions and Future workIn the first set of experiments, we compareda diverse range of content-based features ingenre classification using a reliable and sourcediverse genre-annotated corpus.
The evaluationshows that lexical features outperformed allother features.
Source diversity of the corpusminimized the influence of topic, authorship andweb page design on genre classification.
In thesecond experiment, we significantly improved thegenre classification result using a semi-supervisedmin-cut algorithm by employing the children ofthe target web pages as unlabelled data.
Theresults of this method which takes advantage ofthe graph structure of the web shows that somegenre classes benefit more than others from theneighbouring web pages.
The homophily propertyof genre categories such as news, blogs and edi-torial was the reason behind the improvement ofgenre classification in this experiment.
In futurework, we would like to examine the effect of othertypes of neighbours on genre classification ofweb pages and experiment with other graph-basedalgorithms.ReferencesShlomo Argamon, Moshe Koppel, and Galit Avneri.1998.
Routing documents according to style.
In9Please note that in this experiment we had less trainingdata because we used 8 folds for training, one fold for valida-tion and one fold for testing.
As a result, the accuracy of wordunigrams is slightly lower than the result reported in Table 4.First international workshop on innovative informa-tion systems, pages 85?92.
Citeseer.Shlomo Argamon, Casey Whitelaw, Paul Chase, Sob-han Raj Hota, Navendu Garg, and Shlomo Levitan.2007.
Stylistic text classification using functionallexical features.
Journal of the American Societyfor Information Science and Technology, 58(6):802?822.Noushin Rezapour Asheghi, Serge Sharoff, and KatjaMarkert.
2014.
Designing and evaluating a reliablecorpus of web genres via crowd-sourcing.
In Pro-ceedings of the Ninth International Conference onLanguage Resources and Evaluation (LREC?14).Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Avrim Blum and Shuchi Chawla.
2001.
Learning fromlabeled and unlabeled data using graph mincuts.
InProceedings of the Eighteenth International Confer-ence on Machine Learning, pages 19?26.
MorganKaufmann Publishers Inc.Yuri Boykov, Olga Veksler, and Ramin Zabih.
2001.Fast approximate energy minimization via graphcuts.
Pattern Analysis and Machine Intelligence,IEEE Transactions on, 23(11):1222?1239.Soumen Chakrabarti, Byron Dom, and Piotr Indyk.1998.
Enhanced hypertext categorization using hy-perlinks.
In ACM SIGMOD Record, volume 27,pages 307?318.
ACM.Robert Crane and Luke McDowell.
2012.
Investigat-ing markov logic networks for collective classifica-tion.
In ICAART (1), pages 5?15.M CRAVEN.
1998.
Learning to extract symbolicknowledge from the world wide web.
In Proc.
of the15th National Conference on Artificial Intelligence(AAAI-98).46Aidan Finn and Nicholas Kushmerick.
2006.
Learningto classify documents according to genre.
Journalof the American Society for Information Science andTechnology, 57(11):1506?1518.J.L.
Fleiss.
1971.
Measuring nominal scale agree-ment among many raters.
Psychological Bulletin,76(5):378.L.
Freund, C.L.A.
Clarke, and E.G.
Toms.
2006.
To-wards genre classification for ir in the workplace.In Proceedings of the 1st international conferenceon Information interaction in context, pages 30?36.ACM.Kuzman Ganchev and Fernando Pereira.
2007.
Trans-ductive structured classification through constrainedmin-cuts.
TextGraphs-2: Graph-Based Algorithmsfor Natural Language Processing, page 37.Ioannis Kanaris and Efstathios Stamatatos.
2007.Webpage genre identification using variable-lengthcharacter n-grams.
In Tools with Artificial Intelli-gence, 2007.
ICTAI 2007.
19th IEEE InternationalConference on, volume 2, pages 3?10.
IEEE.J.
Karlgren and D. Cutting.
1994.
Recognizing textgenres with simple metrics using discriminant anal-ysis.
In Proceedings of the 15th conference on Com-putational linguistics-Volume 2, pages 1071?1075.B.
Kessler, G. Numberg, and H. Schutze.
1997.
Au-tomatic detection of text genre.
In Proceedings ofthe 35th Annual Meeting of the Association for Com-putational Linguistics and Eighth Conference of theEuropean Chapter of the Association for Computa-tional Linguistics, pages 32?38.Dan Klein and Christopher D Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting on Association for Computa-tional Linguistics-Volume 1, pages 423?430.
Asso-ciation for Computational Linguistics.Jon Kleinberg and Eva Tardos.
1999.
Approximationalgorithms for classification problems with pairwiserelationships: Metric labeling and markov randomfields.
In focs, page 14.
Published by the IEEE Com-puter Society.Jon Kleinberg and Eva Tardos.
2002.
Approximationalgorithms for classification problems with pairwiserelationships: Metric labeling and markov randomfields.
Journal of the ACM (JACM), 49(5):616?639.Frank Lin and William W Cohen.
2010.
Semi-supervised classification of network data using veryfew labels.
In Advances in Social Networks Analysisand Mining (ASONAM), 2010 International Confer-ence on, pages 192?199.
IEEE.Q.
Lu and L. Getoor.
2003.
Link-based classificationusing labeled and unlabeled data.
The Continuumfrom Labeled to Unlabeled Data in Machine Learn-ing & Data Mining, page 88.Sofus A Macskassy and Foster Provost.
2007.
Classifi-cation in networked data: A toolkit and a univariatecase study.
The Journal of Machine Learning Re-search, 8:935?983.P.
Petrenz and B. Webber.
2010.
Stable classificationof text genres.
Computational Linguistics, (EarlyAccess):1?9.Emily Pitler and Ani Nenkova.
2008.
Revisitingreadability: A unified framework for predicting textquality.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 186?195.Xiaoguang Qi and Brian D Davison.
2009.
Web pageclassification: Features and algorithms.
ACM Com-puting Surveys (CSUR), 41(2):12.Marina Santini.
2007.
Automatic identification ofgenre in web pages.
Ph.D. thesis, University ofBrighton.Prithviraj Sen, Galileo Namata, Mustafa Bilgic, LiseGetoor, Brian Galligher, and Tina Eliassi-Rad.2008.
Collective classification in network data.
AImagazine, 29(3):93.S.
Sharoff, Z. Wu, and K. Markert.
2010.
The web li-brary of babel: evaluating genre collections.
In Pro-ceedings of the Seventh Conference on InternationalLanguage Resources and Evaluation, pages 3063?3070.Efstathios Stamatatos, Nikos Fakotakis, and GeorgeKokkinakis.
2000.
Text genre detection using com-mon word frequencies.
In Proceedings of the 18thconference on Computational linguistics-Volume 2,pages 808?814.Johannes M. Steger and Egon W. Stemle.
2009.
Krd-Wrd ?
architecture for unified processing of webcontent.Kristina Toutanova, Dan Klein, Christopher D Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 173?180.47
