PARADISE: A Framework for Evaluating Spoken Dialogue AgentsMarilyn A. Walker, Diane J. Litman, Candace A. Kamm and Alicia AbellaAT&T Labs - -Research180 Park AvenueF lorham Park, NJ  07932-0971 USAwalker, d iane,cak,abel la@research.att .comAbstractThis paper presents PARADISE (PARAdigmfor Dialogue System Evaluation), a generalframework for evaluating spoken dialogueagents.
The framework decouples task require-ments from an agent's dialogue behaviors, up-ports comparisons among dialogue strategies,enables the calculation of performance oversubdialogues and whole dialogues, specifiesthe relative contribution of various factors toperformance, and makes it possible to compareagents performing different tasks by normaliz-ing for task complexity.1 IntroductionRecent advances in dialogue modeling, speech recogni-tion, and natural language processing have made it possi-ble to build spoken dialogue agents for a wide variety ofapplications, n Potential benefits of such agents includeremote or hands-free access, ease of use, naturalness,and greater efficiency of interaction.
However, acriticalobstacle to progress in this area is the lack of a generalframework for evaluating and comparing the performanceof different dialogue agents.One widely used approach to evaluation isbased on thenotion of a reference answer (Hirschman et al, 1990).
Anagent's responses to a query are compared with a prede-fined key of minimum and maximum reference answers;performance is the proportion of responses that match thekey.
This approach as many widely acknowledged lim-itations (Hirschman and Pao, 1993; Danieli et al, 1992;Bates and Ayuso, 1993), e.g., although there may be manypotential dialogue strategies for carrying out a task, thekey is tied to one particular dialogue strategy.In contrast, agents using different dialogue strategiescan be compared with measures uch as inappropri-ate utterance ratio, turn correction ratio, concept accu-racy, implicit recovery and transaction success (DanieliLWe use the term agent o emphasize the fact that we areevaluating a speaking entity that may have a personality.
Read-ers who wish to may substitute the word "system" wherever"agent" is used.and Gerbino, 1995; Hirschman and Pao, 1993; Po-lifroni et al, 1992; Simpson and Fraser, 1993; Shriberg,Wade, and Price, 1992).
Consider a comparison of twotrain timetable information agents (Danieli and Gerbino,1995), where Agent A in Dialogue I uses an explicit con-firmation strategy, while Agent B in Dialogue 2 uses animplicit confirmation strategy:(1) User: I want to go from Torino to Milano.Agent A: Do you want to go from Trento to Milano?Yes or No?User: No.
(2) User: I want to travel from Torino to Milano.Agent B: At which time do you want to leave fromMerano to Milano?User: No, I want to leave from Torino in the evening.Danieli and Gerbino found that Agent A had a highertransaction success rate and produced less inappropriateand repair utterances than Agent B, and thus concludedthat Agent A was more robust han Agent B.However, one limitation of both this approach and thereference answer approach is the inability to generalizeresults to other tasks and environments (Fraser, 1995).Such generalization requires the identification of factorsthat affect performance (Cohen, 1995; Sparck-Jones andGalliers, 1996).
For example, while Danieli and Gerbinofound that Agent A's dialogue strategy produced dia-logues that were approximately twice as long as AgentB's, they had no way of determining whether Agent A'shigher transaction success or Agent B's efficiency wasmore critical to performance.
In addition to agent factorssuch as dialogue strategy, task factors such as databasesize and environmental f ctors uch as background noisemay also be relevant predictors of performance.These approaches are also limited in that hey currentlydo not calculate performance over subdialogues a well aswhole dialogues, correlate performance with an externalvalidation criterion, or normalize performance for taskcomplexity.This paper describes PARADISE, a general frameworkfor evaluating spoken dialogue agents that addresses theselimitations.
PARADISE supports comparisons among di-alogue strategies by providing a task representation thatdecouples what an agent needs to achieve in terms of271I MAXIMIZE USER SATISFACTION\[lFigure 1: PARADISE's structure of objectives for spokendialogue performancethe task requirements from how the agent carries out thetask via dialogue.
PARADISE uses a decision-theoreticframework to specify the relative contribution of variousfactors to an agent's overall performance.
Performanceis modeled as a weighted function of a task-based suc-cess measure and dialogue-based cost measures, whereweights are computed by correlating user satisfactionwith performance.
Also, performance can be calculatedfor subdialogues as well as whole dialogues.
Since thegoal of this paper is to explain and illustrate the appli-cation of the PARADISE framework, for expository pur-poses, the paper uses simplified domains with hypothet-ical data throughout.
Section 2 describes PARADISE'sperformance model, and Section 3 discusses its general-ity, before concluding in Section 4.2 A Per fo rmance  Mode l  fo r  D ia loguePARADISE uses methods from decision theory (Keeneyand Raiffa, 1976; Doyle, 1992) to combine a disparateset of performance measures (i.e., user satisfaction, tasksuccess, and dialogue cost, all of which have been pre-viously noted in the literature) into a single performanceevaluation function.
The use of decision theory requires aspecification of both the objectives of the decision prob-lem and a set of measures (known as attributes in de-cision theory) for operationalizing the objectives.
ThePARADISE model is based on the structure of objectives(rectangles) shown in Figure 1.
The PARADISE modelposits that performance can be correlated with a mean-ingful external criterion such as usability, and thus thatthe overall goal of a spoken dialogue agent is to maxi-mize an objective related to usability.
User satisfactionratings (Kamm, 1995; Shriberg, Wade, and Price, 1992;Polifroni et al, 1992) have been frequently used in theliterature as an external indicator of the usability of a di-alogue agent.
The model further posits that two types offactors are potential relevant contributors to user satisfac-tion (namely task success and dialogue costs), and thattwo types of factors are potential relevant contributors tocosts (Walker, 1996).In addition to the use of decision theory to create thisobjective structure, other novel aspects of PARADISEinclude the use of the Kappa coefficient (Carletta, 1996;Siegel and Castellan, 1988) to operationalize task suc-cess, and the use of linear regression to quantify the rel-ative contribution of the success and cost factors to usersatisfaction.The remainder of this section explains the measures(ovals in Figure 1) used to operationalize the set of objec-tives, and the methodology for estimating a quantitativeperformance function that reflects the objective structure.Section 2.1 describes PARADISE's task representation,which is needed to calculate the task-based success mea-sure described in Section 2.2.
Section 2.3 describes thecost measures considered in PARADISE, which reflectboth the efficiency and the naturalness of an agent's dia-logue behaviors.
Section 2.4 describes the use of linearregression and user satisfaction to estimate the relativecontribution of the success and cost measures in a singleperformance function.
Finally, Section 2.5 explains howperformance can be calculated for subdialogues as wellas whole dialogues, while Section 2.6 summarizes themethod.2.1 Tasks as Attribute Value MatricesA general evaluation framework requires atask represen-tation that decouples what an agent and user accomplishfrom how the task is accomplished using dialogue strate-gies.
We propose that an attribute value matrix (AVM)can represent many dialogue tasks.
This consists of theinformation that must be exchanged between the agentand the user during the dialogue, represented as a set ofordered pairs of attributes and their possible values.
2As a first illustrative xample, consider asimplificationof the train timetable domain of Dialogues 1 and 2, wherethe timetable only contains information about rush-hourtrains between four cities, as shown in Table 1.
This AVMconsists of four attributes (abbreviations for each attributename are also shown).
3 In Table 1, these attribute-valuepairs are annotated with the direction of information flowto represent who acquires the information, although thisinformation is not used for evaluation.
During the dia-logue the agent must acquire from the user the values ofDC, AC, and DR, while the user must acquire DT.Performance evaluation for an agent requires a corpusof dialogues between users and the agent, in which usersexecute a set of scenarios.
Each scenario execution has2For infinite sets of values, actual values found in the exper-imental data constitute the required finite set.3The AVM serves as an evaluation mechanism only.
We arenot claiming that AVMs determine an agent's behavior or serveas an utterance's semantic representation.272attribute possible values information flowdepart-city (DC)arrival-city (AC)depart-range (DR)depart-time (DT)Milano, Roma.
Torino, TrentoMilano, Roma, Torino, Trentomorning,evening6am,8am,6pm,Spmto agentto agentto agentto userTable 1: Attribute value matrix, simplified train timetabledomainA 1 : Hello, This is Train Enquiry Service.
DC,AC,DR,DTPlease speak after the tone.
DC,AC,DR,DTWhich information do you need?
DC,AC,DR,DTU 1: I want o go from Torino to Milano.
DC,ACA2: Do you want o go from Trento to Milano?
DC,ACYes or No?
DC,ACU2: No.
DC,ACA3: Do you want o leave from Trento?
DCYes or No?
DCU3: No.
DCA4: Where do you want o leave from?
DCU4: I want to leave from Torino.
DCA5: Do you want o leave from Torino?
DCYes or No?
DCU5: Yes.
DCA6: Do you want o go to Milano?
ACU6: Yes.
ACA7: At which time do you want o leave?
DRU7: I want o travel in the evening.
DRA8: Do you want o leave between 6 and 9 p.m.?
DRYes or No?
DRU8: Yes.
DRA9: There is a train leaving at 8:00 p.m. DTFigure 2: Agent A dialogue interaction (Danieli andGerbino, 1995)a corresponding AVM instantiation indicating the taskinformation requirements for the scenario, where eachattribute is paired with the attribute value obtained viathe dialogue.For example, assume that a scenario requires the userto find a train from Torino to Milano that leaves in theevening, as in the longer versions of Dialogues 1and 2 inFigures 2 and 3.4 Table 2 contains an AVM correspondingto a "key" for this scenario.
All dialogues resulting fromexecution of this scenario in which the agent and theuser correctly convey all attribute values (as in Figures2 and 3) would have the same AVM as the scenario keyin Table 2.
The AVMs of the remaining dialogues woulddiffer from the key by at least one value.
Thus, eventhough the dialogue strategies in Figures 2 and 3 areradically different, the AVM task representation for thesedialogues i identical and the performance of the systemfor the same task can thus be assessed on the basis of theAVM representation.2.2 Measuring Task SuccessSuccess at the task for a whole dialogue (or subdia-logue) is measured by how well the agent and user achievethe information requirements ofthe task by the end of the4These dialogues have been slightly modified from (Danieliand Gerbino, 1995).
The attribute names at the end of eachutterance will be explained below.B I : Hello, This is Train Enquiry Service.
DC,AC,DILDTPlease speak after the tone.
DC,AC,DR,DTWhich information do you need?
DC,AC,DR,DTU 1 : I want o travel from Torino to Milano.
DC,ACB2: At which time do you want o leavefrom Merano to Milano?
DC,AC,DRU2: No, I want o leave from Torinoin the evening.
DC, DRB3: Do you want o leave from Torinobetween 6and 11 p.m?
DC, DRPlees~ answer Yes or No.
DC, DRU3: Yes.
DC, DRB4: A train leaves at 8 p.m. DTFigure 3: Agent B dialogue interaction (Danieli andGerbino, 1995)attribute actual valuedepart-city Torinoarrival-city Milanodepart-range eveningdepart-time 8pmTable 2: Attribute value matrix instantiation, scenariokey for Dialogues 1and 2dialogue (or subdialogue).
This section explains howPARADISE uses the Kappa coefficient (Carletta, 1996;Siegel and Castellan, 1988) to operationalize the task-based success measure in Figure 1.The Kappa coefficient, ~, is calculated from a confu-sion matrix that summarizes how well an agent achievesthe information requirements ofa particular task for a setof dialogues instantiating a set of scenarios,  For exam-ple, Tables 3 and 4 show two hypothetical confusion ma-trices that could have been generated in an evaluation of100 complete dialogues with each of two train timetableagents A and B (perhaps using the confirmation strategiesillustrated in Figures 2 and 3, respectively), 6 The valuesin the matrix cells are based on comparisons between thedialogue and scenario key AVMs.
Whenever an attributevalue in a dialogue (i.e., data) AVM matches the value inits scenario key, the number in the appropriate diagonalcell of the matrix (boldface for clarity) is incrementedby 1.
The off diagonal cells represent misunderstand-ings that are not corrected in the dialogue.
Note thatdepending on the strategy that a spoken dialogue agentuses, confusions across attributes are possible, e.g., "Mi-lano " could be confused with "morning."
The effect ofmisunderstandings that are corrected uring the courseof the dialogue are reflected in the costs associated withthe dialogue, as will be discussed below.The first matrix summarizes how the 100 AVMs rep-resenting each dialogue with Agent A compare with theAVMs representing the relevant scenario keys, while the5Confusion matrices can be constructed to summarize theresult of dialogues for any subset of the scenarios, attributes,users or dialogues.~The distributions in the tables were roughly based on per-formance r sults in (Danieli and Gerbino, 1995).273DATAvlv2v3v4v5v6v7v8v9vlOvi iv12v13vl4sumKEYDEPART.CITY ARRIVAL-CTrY DEPART-RANGE DEPART-TIMEvl v2 v3 v4 v5 v6 v7 v8 v9 vl0 v i i  v12 v13 v1422 1 3294 16 4 I1 1 5 11 13 20222 1 1 20 51 1 2 8 1545 105 40o IB I~ 15 25 25 30 20 50 5020 2I 19 2 42 182 6 3 2125 25 25 25Table 3: Confusion matrix, Agent ADEPART-CITYDATA vl v2 v3 v4v!
16 1v2 1 20 1v3  5 1 9 4v4 1 2 6 6v5 4v6 1 6v7 5 2v8 1 3 3v9 2vl0v i iv12v13v14sum 30 30 25 15ARR2VAL-CITYv5 v6 v7 v8432 4215191 1 151 2 925 25 30DEPART-RANGEv9 vl03 2232 341139 106 3520 5O 50DEPAK'F-TIMEI / E20 5 5 410 5 55 5 10 55 5 1125 25 25 25Table 4: Confusion matrix, Agent Bsecond matrix summarizes the information exchange withAgent B. Labels vl to v4 in each matrix represent thepossible values of depart-city shown in Table 1; v5 tov8 are for arrival-city, etc.
Columns represent the key,specifying which information values the agent and userwere supposed to communicate o one another given aparticular scenario.
(The equivalent column sums in bothtables reflects that users of both agents were assumed tohave performed the same scenarios).
Rows represent thedata collected from the dialogue corpus, reflecting whatattribute values were actually communicated between theagent and the user.Given a confusion matrix M, success at achieving theinformation requirements of the task is measured with theKappa coefficient (Carletta, 1996; Siegel and Castellan,1988):P(A) - P(E)K- -1 - P (E )P(A) is the proportion of times that the AVMs for theactual set of dialogues agree with the AVMs for the sce-nario keys, and P(E) is the proportion of times that theAVMs for the dialogues and the keys are expected to agreeby chance.
7 When there is no agreement o her than thatwhich would be expected by chance, ~ = 0.
When there istotal agreement, ~= 1. n is superior to other measures ofsuccess uch as transaction success (Danieli and Gerbino,1995), concept accuracy (Simpson and Fraser, 1993), andpercent agreement (Gale, Church, and Yarowsky, 1992)because n takes into account the inherent complexity ofthe task by correcting for chance expected agreement.Thus ~ provides a basis for comparisons across agentsthat are performing different tasks.When the prior distribution of the categories i  un-known, P(E), the expected chance agreement betweenthe data and the key, can be estimated from the distri-bution of the values in the keys.
This can be calculatedfrom confusion matrix M, since the columns representthe values in the keys.
In particular:r~P(E) = ~j--,ft_i ~2L .~,  T ,i=l7~ has  been used  to measure  pa i rw ise  agreement  amongcoders  mak ing  category  judgments  (Car letta,  1996;  Kr ippen-dof f ,  1980;  S iegel  and  Caste l lan,  1988).
Thus ,  the observeduser /agent  in teract ions  are mode led  as a coder ,  and  the idea lin teract ions  as an  expert  coder.274where ti is the sum of the frequencies in column i of M,and T is the sum of the frequencies in M (tl + ?
?
?
+ tn).P(A), the actual agreement between the data and thekey, is always computed from the confusion matrix M:P(A)  - ~'~i~=l M( i ,  i)TGiven the confusion matrices in Tables 3 and 4, P(E)= 0.079 for both agents,  For Agent A, P(A) = 0.795and  = 0.777, while for Agent B, P(A) = 0.59 and a =0.555, suggesting that Agent A is more successful thanB in achieving the task goals.2.3 Measuring Dialogue CostsAs shown in Figure 1, performance is also a function of acombination of cost measures.
Intuitively, cost measuresshould be calculated on the basis of any user or agentdialogue behaviors that should be minimized.
A widerange of cost measures have been used in previous work;these include pure efficiency measures such as the num-ber of turns or elapsed time to complete the task (Abella,Brown, and Buntschuh, 1996; Hirschman et al, 1990;Smith and Gordon, 1997; Walker, 1996), as well as mea-sures of qualitative phenomena such as inappropriate orrepair utterances (Danieli and Gerbino, 1995; Hirschmanand Pao, 1993; Simpson and Fraser, 1993).PARADISE represents each cost measure as a functionci that can be applied to any (sub)dialogue.
First, considerthe simplest case of calculating efficiency measures overa whole dialogue.
For example, let cl be the total numberof utterances.
For the whole dialogue D1 in Figure 2,el(D1) is 23 utterances.
For the whole dialogue D2 inFigure 3, cl (D2) is 10 utterances.To calculate costs over subdialogues and for some ofthe qualitative measures, it is necessary to be able to spec-ify which information goals each utterance contributesto.
PARADISE uses its AVM representation to link theinformation goals of the task to any arbitrary dialoguebehavior, by tagging the dialogue with the attributes forthe task.
9 This makes it possible to evaluate any potentialdialogue strategies for achieving the task, as well as toevaluate dialogue strategies that operate at the level ofdialogue subtasks (subdialogues).Consider the longer versions of Dialogues 1 and 2 inFigures 2 and 3.
Each utterance in Figures 2 and 3 hasbeen tagged using one or more of the attribute abbrevia-tions in Table 1, according to the subtask(s) the utterancecontributes to.
As a convention of this type of tagging,SUsing a single confusion matrix for all attributes as inTables 3 and 4 inflates n when there are few cross-attributeconfusions by making P(E) smaller.
In some cases it mightbe desirable to calculate ~; first for identification of attributesand then for values within attributes, or to average ~ for eachattribute to produce an overall t?
for the task.9This tagging can be hand generated, or system generatedand hand corrected.
Preliminary studies indicate that reliabilityfor human tagging is higher for AVM attribute tagging thanfor other types of discourse segment tagging (Passonneau andLitman, 1997; Hirschberg and Nakatani, 1996).~:E.AC, DR, D~:A I .
.A9SEG~cr: S3 S~Ml~Cr: S4G0~: I?
GOALS: ACo'rr~cES: A3...u5 0TI/~ES: A6...U6Figure 4: Task-defined iscourse structure of Agent Adialogue interactionutterances that contribute to the success of the whole dia-logue, such as greetings, are tagged with all the attributes.Since the structure of a dialogue reflects the structure ofthe task (Carberry, 1989; Grosz and Sidner, 1986; Litmanand Allen, 1990), the tagging of a dialogue by the AVMattributes can be used to generate a hierarchical discoursestructure such as that shown in Figure 4 for Dialogue1 (Figure 2).
For example, segment (subdialogue) $2in Figure 4 is about both depart-city (DC) and arrival-city (AC).
It contains egments $3 and $4 within it, andconsists of utterances U1 .
.
.
U6.Tagging by AVM attributes is required to calculatecosts over subdialogues, ince for any subdialogue, taskattributes define the subdialogue.
For subdialogue $4in Figure 4, which is about the attribute arrival-city andconsists of utterances A6 and U6, ct(S4) is 2.Tagging by AVM attributes i also required to calculatethe cost of some of the qualitative measures, such asnumber of repair utterances.
(Note that to calculate suchcosts, each utterance in the corpus of dialogues must alsobe tagged with respect o the qualitative phenomenon iquestion, e.g.
whether the utterance is a repair, l?)
Forexample, let c2 be the number of repair utterances.
Therepair utterances in Figure 2 are A3 through U6, thusc2(D1) is 10 utterances and c2($4) is 2 utterances.
Therepair utterance in Figure 3 is U2, but note that accordingto the AVM task tagging, U2 simultaneously addressesthe information goals for depart-range.
In general, ifan utterance U contributes to the information goals of Ndifferent attributes, each attribute accounts for 1/N of anycosts derivable from U.
Thus, c2(D2) is .5.Given a set of ci, it is necessary to combine the dif-mPrevious work has shown that this can be done with highreliability (Hirschman and Pao, 1993).275ferent cost measures in order to determine their relativecontribution to performance.
The next section explainshow to combine ~ with a set of ci to yield an overallperformance measure.2.4 Estimating a Performance FunctionGiven the definition of success and costs above and themodel in Figure 1, performance for any (sub)dialogue Dis defined as follows: itnPerformance = (o~ ?
.N'(t~)) - ~ wi * .N'(ci)i=1Here ~ is a weight on ~, the cost functions ci are weightedby wi, and At" is a Z score normalization function (Cohen,1995).The normalization function is used to overcome theproblem that the values of ci are not on the same scale asx, and that the cost measures ci may also be calculatedover widely varying scales (e.g.
response delay couldbe measured using seconds while, in the example, costswere calculated in terms of number of utterances).
Thisproblem is easily solved by normalizing each factor x toits Z score:N' (x )  =O'.
:t:where ~r= is the standard eviation for x.user agent US ~ el (#utt) e2 (#rep)1 A 1 1 46 302 A 2 1 50 303 A 2 I 52 304 A 3 1 40 205 A 4 1 23 106 A 2 1 50 367 A 1 0.46 75 308 A 1 0.19 60 309 B 6 I 8 010 B 5 1 15 111 B 6 I 10 0.512 B 5 1 20 313 B 1 0.L9 45 1814 B 1 0.46 50 2215 B 2 0.19 34 1816 B 2 0.46 40 18Mean(A) A 2 0.83 49.5 27Mean(B) B 3.5 0.66 27.8 10,1Mean NA 2.75 0.75 38,6 18,5Table 5: Hypothetical performance data from users ofAgents A and BTo illustrate the method for estimating'a performancefunction, we will use a subset of the data from Tables 3and 4, shown in Table 5.
Table 5 represents the resultstZWe assume an additive performance (utility) function be-cause it appears that n and the various cost factors ci are util-ity independent and additive independent (Keeney and Raiffa,1976).
It is possible however that user satisfaction data col-lected in future experiments (or other data such as willingnessto pay or use) would indicate otherwise.
If so, continuing use ofan additive function might require a transformation fthe data,a reworking of the model shown in Figure 1, or the inclusion ofinteraction terms in the model (Cohen, 1995).from a hypothetical experiment inwhich eight users wererandomly assigned to communicate with Agent A andeight users were randomly assigned to communicate withAgent B.
Table 5 shows user satisfaction (US) ratings(discussed below), ~, number of utterances (#utt) andnumber of repair utterances (#rep) for each of these users.Users 5 and 11 correspond to the dialogues in Figures2 and 3 respectively.
To normalize ct for user 5, wedetermine that ~ is 38.6 and crc~ is 18.9.
Thus, .N'(cl) is-0.83.
Similarly A/'(cl) for user 11 is -1.51.To estimate the performance function, the weightsand wi must be solved for.
Recall that he claim implicit inFigure 1 was that the relative contribution of task successand dialogue costs to performance should be calculated byconsidering their contribution to user satisfaction.
Usersatisfaction is typically calculated with surveys that askusers to specify the degree to which they agree with oneor more statements about he behavior or the performanceof the system.
A single user satisfaction measure can becalculated from a single question, or as the mean of aset of ratings.
The hypothetical user satisfaction ratingsshown in Table 5 range from a high of 6 to a low of 1.Given a set of dialogues for which user satisfaction(US), ~ and the set of ci have been collected experimen-tally, the weights ~ and wi  can be solved for using multi-ple linear regression.
Multiple linear regression producesa set of coefficients (weights) describing the relative con-tribution of each predictor factor in accounting for thevariance in a predicted factor.
In this case, on the basisof the model in Figure 1, US is treated as the predictedfactor.
Normalization of the predictor factors (~ and ci)to their Z scores guarantees that the relative magnitudeof the coefficients directly indicates the relative contribu-tion of each factor.
Regression on the Table 5 data forboth sets of users tests which factors ~, #utt, #rep moststrongly predicts US.In this illustrative xample, the results of the regressionwith all factors included shows that only ~ and #rep aresignificant (p < .02).
In order to develop a performancefunction estimate that includes only significant factorsand eliminates redundancies, a second regression includ-ing only significant factors must then be done.
In thiscase, a second regression yields the predictive quation:Performance = .40.N'(~) - .78.N'(c2)i.e., c~ is .40 and w2 is .78.
The results also show ~ issignificant at p < .0003, #rep significant at p < .0001,and the combination of ~ and #rep account for 92% ofthe variance in US, the external validation criterion.
Thefactor #utt was not a significant predictor of performance,in part because #utt and #rep are highly redundant.
(Thecorrelation between #utt and #rep is 0.91).Given these predictions about he relative contributionof different factors to performance, it is then possibleto return to the problem first introduced in Section 1:given potentially conflicting performance criteria such asrobustness and efficiency, how can the performance ofAgent A and Agent B be compared?
Given values forand wi, performance can be calculated for both agents276using the equation above.
The mean performance of Ais -.44 and the mean performance of B is .44, suggestingthat Agent B may perform better than Agent A overall.The evaluator must then however test these perfor-mance differences for statistical significance.
In this case,a t test shows that differences are only significant at the p< .07 level, indicating a trend only.
In this case, an eval-uation over a larger subset of the user population wouldprobably show significant differences.2.5 Application to SubdialoguesSince both ~ and ei can be calculated over subdialogues,performance can also be calculated at the subdialoguelevel by using the values for c~ and wi  as solved for above.This assumes that the factors that are predictive of globalperformance, based on US, generalize as predictors oflocal performance, i.e.
within subdialogues defined bysubtasks, as defined by the attribute tagging.
12Consider calculating the performance of the dialoguestrategies used by train timetable Agents A and B, overthe subdialogues that repair the value of depart-city.
Seg-ment $3 (Figure 4) is an example of such a subdialoguewith Agent A.
As in the initial estimation of a perfor-mance function, our analysis requires experimental data,namely aset of values for ~ and el, and the application ofthe Z score normalization function to this data.
However,the values for ~ and ci are now calculated at the subdia-Iogue rather than the whole dialogue level.
In addition,only data from comparable strategies can be used to cal-culate the mean and standard eviation for normalization.Informally, acomparable strategy isone which applies inthe same state and has the same effects.For example, to calculate ~ for Agent A over the sub-dialogues that repair depart-city, P(A) and P(E) are com-puted using only the subpart of Table 3 concerned withdepart-city.
For Agent A, P(A) = .78, P(E) = .265, and= .70.
Then, this value of~ is normalized using data fromcomparable subdialogues with both Agent A and AgentB.
Based on the data in Tables 3 and 4, the mean ~ is .515and ~r is .261, so that.M(~c) for Agent A is .71.To calculate c2 for Agent A, assume that the averagenumber of repair utterances for Agent A's subdialoguesthat repair depart-city is 6, that the mean over all compa-rable repair subdialogues is 4, and the standard eviationis 2.79.
Then A/'(cz) is .72.Let Agent A's repair dialogue strategy for subdialoguesrepairing depart-city be RA and Agent B's repair strat-egy for depart-city be RB.
Then using the performanceequation above, predicted performance for RA is:Performance(Ra) = .40 ?
.71 -- .78 ?
.72 = --0.28For Agent B, using the appropriate subpart of Table4 to calculate ~, assuming that the average number ofdepart-city repair utterances i 1.38, and using similar12This assumption has a sound basis in theories of dialoguestructure (Carberry, 1989; Grosz and Sidner, 1986; Litman andAllen, 1990), but should be tested empirically.calculations, yieldsPerformance(RB) = .40.
-.71 - .78 ?
- .94 = 0.45Thus the results of these xperiments predict hat whenan agent needs to choose between the repair strategy thatAgent B uses and the repair strategy that Agent A usesfor repairing depart-city, it should use Agent B's strategyRB, since the performance(RB) is predicted to be greaterthan the performance(Ra).Note that he ability to calculate performance over sub-dialogues allows us to conduct experiments hat simulta-neously test multiple dialogue strategies.
For example,suppose Agents A and B had different strategies for pre-senting the value of depart-time (in addition to differentconfirmation strategies).
Without he ability to calculateperformance over subdialogues, it would be impossibleto test the effect of the different presentation strategiesindependently of the different confirmation strategies.2.6 SummaryWe have presented the PARADISE framework, and haveused it to evaluate two hypothetical dialogue agents in asimplified train timetable task domain.
We used PAR-ADISE to derive a performance function for this task, byestimating the relative contribution of a set of potentialpredictors to user satisfaction.
The PARADISE method-ology consists of the following steps:?
definition of a task and a set of scenarios;?
specification of the AVM task representation;?
experiments with alternate dialogue agents for thetask;?
calculation of user satisfaction using surveys;?
calculation of task success using ~;?
calculation of dialogue cost using efficiency andqualitative measures;?
estimation of a performance function using linearregression and values for user satisfaction, K anddialogue costs;?
comparison with other agents/tasks to determinewhich factors generalize;?
refinement of the performance model.Note that all of these steps are required to developthe performance function.
However once the weightsin the performance function have been solved for, usersatisfaction ratings no longer need to be collected.
In-stead, predictions about user satisfaction can be made onthe basis of the predictor variables, as illustrated in theapplication of PARADISE to subdialogues.Given the current state of knowledge, it is important toemphasize that researchers should be cautious about gen-eralizing a derived performance function to other agents.or tasks.
Performance function estimation should be doneiteratively over many different tasks and dialogue strate-gies to see which factors generalize.
In this way, thefield can make progress on identifying the relationshipbetween various factors and can move towards more pre-dictive models of spoken dialogue agent performance.2773 Genera l i tyIn the previous section we used PARADISE to evalu-ate two confirmation strategies, using as examples fairlysimple information access dialogues in the train timetabledomain.
In this section we demonstrate that PARADISEis applicable to a range of tasks, domains, and dialogues,by presenting AVMs for two tasks involving more thaninformation access, and showing how additional dialoguephenomena can be tagged using AVM attributes.depart-city (DC)arrival-city (AC)depart-range (DR)depart-time (DT)request-type (R'r)possible values information flowMilano, Roma, Torino, Trento to agentMilano, Roma, Torino, Trento to agentmorning,evening to agent6am,Sam,6pm,8pm to userreserve, purchase to agent ITable 6: Attribute value matrix, train timetable domainwith requestsFirst, consider an extension of the train timetable task,where an agent can handle requests to reserve a seat orpurchase a ticket.
This task could be represented usingthe AVM in Table 6 (an extension of Table 1), wherethe agent must now acquire the value of the attributerequest-type, in order to know what to do with the otherinformation it has acquired.U 1: I want to go from Torino to Roma DC,ACC 1: Approximately what time of day would you like to travel?
DRU2: What are the options?
DRC2: Morning or evening.
DRU3: Are those departure times?
DRC3: Yes.
DRU4: I'd like to leave in the morning.
DRC4: Train 702 leaves Torino Porto at 8 a.m. DTU5: Please reserve me a seat on that train.
RTFigure 5: Hypothetical Agent C dialogue interactionFigure 5 presents a hypothetical dialogue in this ex-tended task domain, and illustrates user utterance typesand an agent dialogue strategy that are very different fromthose in Figures 2 and 3.
First, Agent C in Figure 5 usesa "no confirmation" dialogue strategy, in contrast to theexplicit and implicit confirmation strategies used in Fig-ures 2 and 3.
Second, Figure 5 illustrates new types ofuser  utterances that do not directly further the informa-tional goals of the task.
In U2, the user asks the agenta wh-question about the DR attribute itself, rather thanproviding information about that attribute's value.
SinceU2 satisfies a knowledge precondition related to answer-ing C l, U2 contributes to the DR goal and is taggedas such.
In U3, the user similarly asks a yes-no questionthat addresses asubgoal related to answering C1.
Finally,U5 illustrates a user request for an agent action, and istagged with the RT attribute.
The value of RT in the AVMinstantiation for the dialogue would be "reserve"Second, consider the very different domain and taskof diagnosing a fault and repairing a circuit (Smith andGordon, 1997).
Figure 6 presents one dialogue from thisdomain.I.
C: This is the circuit fix it shop.How may I help you?2.
U: I want to fix a circuit.3.
C: What is the ID of the circuit?4.
U: Rsl l  1.5.
C: I am familiar with that circuit.The LED is supposed to be displayingalternately flashing one and seven.
CB6.
U: The LED is off.
RB7.
C: What is the switch at when the LED is off?.
RB8, U: The switch is down.
RB9.
C: Put the switch up.
RB10.
U: Okay.
RB11.
C: Put the knob to one zero.
RB12.
U: Okay.
RB13.
C: What is the LED displaying?
RB14.
U: LED is displaying only a flashing seven.
RB15.
C: Is there a wire between connector eight fourand connector nine nine?
FT16.
U: No.
FT17.
C: Add a wire between connector eight fourand connector nine nine.
FC18.
U: Done.
FC19.
C: What is the LED displaying?
T20.
U: Alternately flashing one and seven.
T2 I.
C: Is the one on the LED displaying fora longer period of time?
T22.
U: No.
T23.
C: Is the seven on the LED displayingfor a longer period of time?
T24.
U: No.
T25.
C: Put the knob to zero.
T26.
U: Okay.
T27.
C: What is the LED displaying?
T28.
U: Alternately displaying one and seven.
T29.
C: Is the one on the LED displayingfor a longer period of time?
T30.
U: Yes.
T31.
C: Put the switch down.
T32.
U: The switch is down.
T33.
C: What is the LED displaying?
T34.
U: Nothing.
T35.
C: The circuit is working correctly.
Good-bye.
TID,CB,RB,FT, FC,TID,CB,RB,FT, FC,TID,CB,RB,FT, FC,TIDIDCBFigure 6: A circuit domain dialogue (Smith and Gordon,1997), with AVM taggingSmith and Gordon collected 144 dialogues for this task,in which agent initiative was varied by using differentdialogue strategies, and tagged each dialogue accordingto the following subtask structure: 13?
Introduction (I)--establish the purpose of the task.
Assessment (A)--establish the current behavior?
Diagnosis (D)---establish the cause for the errantbehavior?
Repair (R)---establish that the correction for the er-rant behavior has been made?
Test (T)---establish that the behavior is now correctOur informational analysis of this task results in the AVMshown in Table 7.
Note that the attributes are almostidentical to Smith and Gordon's list of subtasks.
Circuit-ID corresponds to Introduction, Correct-Circuit-Behaviorand Current-Circuit-Behavior correspond to Assessment,t3They report a ~ of.82 for reliability of their tagging scheme.278Fault-Type corresponds to Diagnosis, Fault-Correctioncorresponds toRepair, and Test corresponds toTest.
Theattribute names emphasize information exchange, whilethe subtask names emphasize function.attribute possible valuesCircuit-ID (ID) RSI 11, RS112 ....Correct-Circuit-Behavior (CB) Flash- 1-7, Flash- 1 ....Current-Circuit-Behavior (RB) Flash-7Fault-Type (P-'q') MissingWire84-99, MissingWire88-99 ....Fault-Correction (FC) yes, noTest (T) yes, noTable 7: Attribute value matrix, circuit domainFigure 6 is tagged with the attributes from Table 7.Smith and Gordon's tagging of this dialogue accordingto their subtask representation was as follows: turns 1-4 were I, turns 5-14 were A, turns 15-16 were D, turns17-18 were R, and turns 19-35 were T. Note that thereare only two differences between the dialogue structuresyielded by the two tagging schemes.
First, in our scheme(Figure 6), the greetings (turns 1 and 2) are tagged withall the attributes.
Second, Smith and Gordon's ingletag A corresponds totwo attribute tags in Table 7, whichin our scheme defines an extra level of structure withinassessment subdialogues.4 DiscussionThis paper presented the PARADISE framework for eval-uating spoken dialogue agents.
PARADISE is a gen-eral framework for evaluating spoken dialogue agentsthat integrates and enhances previous work.
PARADISEsupports comparisons among dialogue strategies with atask representation that decouples what an agent needsto achieve in terms of the task requirements from howthe agent carries out the task via dialogue.
Furthermore,this task representation supports the calculation of perfor-mance over subdialogues a well as whole dialogues.
Inaddition, because PARADISE's success measure normal-izes for task complexity, it provides abasis for comparingagents performing different tasks.The PARADISE performance measure is a function ofboth task success (~) and dialogue costs (ci), and hasa number of advantages.
First, it allows us to evaluateperformance atany level of a dialogue, since n and cican be calculated for any dialogue subtask.
Since per-formance can be measured over any subtask, and sincedialogue strategies can range over subdialogues or thewhole dialogue, we can associate performance with indi-vidual dialogue strategies.
Second, because our successmeasure n takes into account the complexity of the task,comparisons can be made across dialogue tasks.
Third,~; allows us to measure partial success at achieving thetask.
Fourth, performance can combine both objectiveand subjective cost measures, and specifies how to eval-uate the relative contributions of those costs factors tooverall performance.
Finally, to our knowledge, we arethe first to propose using user satisfaction to determineweights on factors related to performance.In addition, this approach is broadly integrative, in-corporating aspects of transaction success, concept accu-racy, multiple cost measures, and user satisfaction.
In ourframework, transaction success is reflected in ~;, corre-sponding to dialogues with a P(A) of 1.
Our performancemeasure also captures information similar to concept ac-curacy, where low concept accuracy scores translate intoeither higher costs for acquiring information from theuser, or lower ~ scores.One limitation of the PARADISE approach is that thetask-based success measure does not reflect that somesolutions might be better than others.
For example, in thetrain timetable domain, we might like our task-based suc-cess measure to give higher atings to agents that suggestexpress over local trains, or that provide helpful infor-mation that was not explicitly requested, especially sincethe better solutions might occur in dialogues with highercosts.
It might be possible to address this limitationby using the interval scaled data version of n (Krippen-dorf, 1980).
Another possibility is to simply substitut*.a domain-specific task-based success measure in the per-formance model for n.The evaluation model presented here has many applica-tions in apoken dialogue processing.
We believe that theframework is also applicable to other dialogue modal-ities, and to human-human task-oriented dialogues.
Inaddition, while there are many proposals in the litera-ture for algorithms for dialogue strategies that are co-operative, collaborative or helpful to the user (Webberand Joshi, 1982; Pollack, Hirschberg, and Webber, 1982;Joshi, Webber, and Weischedel, 1984; Chu-Carrol andCarberry, 1995), very few of these strategies have beenevaluated as to whether they improve any measurable as-pect of a dialogue interaction.
As we have demonstratedhere, any dialogue strategy can be evaluated, so it shouldbe possible to show that a cooperative r sponse, or othercooperative strategy, actually improves task performanceby reducing costs or increasing task success.
We hopethat this framework will be broadly applied in future di-alogue research.5 AcknowledgmentsWe would like to thank James Allen, Jennifer Chu-Carroll, Morena Danieli, Wieland Eckert, Giuseppe DiFabbrizio, Don Hindle, Julia Hirschberg, Shri Narayanan,Jay Wilpon, Steve Whittaker and three anonymous re-views for helpful discussion and comments on earlierversions of this paper.ReferencesAbella, Alicia, Michael K Brown, and Bruce Buntschuh.1996.
Development principles for dialog-based inter-faces.
In ECAI-96 Spoken Dialog Processing Work-shop, Budapest, Hungary.279Bates, Madeleine and Damaris Ayuso.
1993.
A proposalfor incremental dialogue valuation.
In Proceedings ofthe DARPA Speech and NL Workshop, ages 319-322.Carberry, S. 1989.
Plan recognition and its use in un-derstanding dialogue.
In A. Kobsa and W. Wahlster,editors, User Models in Dialogue Systems.
SpringerVerlag, Berlin, pages 133-162.Carletta, Jean C. 1996.
Assessing the reliabilityof subjective codings.
Computational Linguistics,22(2):249-254.Chu-Carrol, Jennifer and Sandra Carberry.
1995.
Re-sponse generation i collaborative n gotiation.
InPro-ceedings of the Conference of the 33rd Annual Meet-ing of the Association for Computational Linguistics,pages 136-143.Cohen, Paul.
R. 1995.
Empirical Methods for ArtificialIntelligence.
MIT Press, Boston.Danieli, M., W. Eckert, N. Fraser, N. Gilbert, M. Guy-omard, P. Heisterkam p, M. Kharoune, J. Magadur,S.
McGlashan, D. Sadek, J. Siroux, and N. Youd.1992.
Dialogue manager design evaluation.
TechnicalReport Project Esprit 2218 SUNDIAL, WP6000-D3.Danieli, Morena and Elisabetta Gerbino.
1995.
Metricsfor evaluating dialogue strategies ina spoken languagesystem.
In Proceedings of the 1995 AAAI Spring Sym-posium on Empirical Methods in Discourse Interpre-tation and Generation, pages 34-39.Doyle, Jon.
1992.
Rationality and its roles in reasoning.Computational Intelligence, 8(2):376--409.Fraser, Norman M. 1995.
Quality standards for spokendialogue systems: areport on progress in EAGLES.
InESCA Workshop on Spoken Dialogue Systems Vigso,Denmark, pages 157-160.Gale, William, Ken W. Church, and David Yarowsky.1992.
Estimating upper and lower bounds on the per-formance of word-sense disambiguation programs.
InProc.
of3Oth ACL, pages 249-256, Newark, Delaware.Grosz, Barbara J. and Candace L. Sidner.
1986.
Atten-tions, intentions and the structure of discourse.
Com-putational Linguistics, 12:175-204.Hirschberg, Julia and Christine Nakatani.
1996.
Aprosodic analysis of discourse segments in direction-giving monologues.
In 34th Annual Meeting of theAssociation for Computational Linguistics, pages 286--293.Hirschman, Lynette, Deborah A. Dahl, Donald P. McKay,Lewis M. Norton, and Marcia C. Linebarger.
1990.Beyond class A: A proposal for automatic evaluationof discourse.
In Proceedings ofthe Speech and NaturalLanguage Workshop, ages 109-113.Hirschman, Lynette and Christine Pao.
1993.
The costof errors in a spoken language system.
In Proceedingsof the Third European Conference on Speech Commu-nication and Technology, pages 1419-1422.Joshi, Aravind K., Bonnie L. Webber, and Ralph M.Weischedel.
1984.
Preventing false inferences.
InCOLING84: Proc.
lOth International Conference onComputational Linguistics., pages 134-138.Kamm, Candace.
1995.
User interfaces for voice appli-cations.
In David Roe and Jay Wilpon, editors, VoiceCommunication between Humans and Machines.
Na-tional Academy Press, pages 422--442.Keeney, Ralph and Howard Raiffa.
1976.
Decisions withMultiple Objectives: Preferences and Value Tradeoffs.John Wiley and Sons.Krippendorf, Klaus.
1980.
Content Analysis: An Intro-duction to its Methodology.
Sage Publications, Bev-erly Hills, Ca.Litman, Diane and James Allen.
1990.
Recognizing andrelating discourse intentions and task-oriented plans.In Philip Cohen, Jerry Morgan, and Martha Pollack,editors, Intentions in Communication.
MIT Press.Passonneau, Rebecca J. and Diane Litman.
1997.
Dis-course segmentation byhuman and automated means.Computational Linguistics, 23(1).Polifroni, Joseph, Lynette Hirschman, Stephanie Seneff,and Victor Zue.
1992.
Experiments in evaluating in-teractive spoken language systems.
In Proceedings ofthe DARPA Speech and NL Workshop, ages 28-33.Pollack, Martha, Julia Hirschberg, and Bonnie Webber.1982.
User participation i the reasoning process ofexpert systems.
In Proceedings First National Confer-ence on Artificial Intelligence, pages pp.
358-361.Shriberg, Elizabeth, Elizabeth Wade, and Patti Price.1992.
Human-machine problem solving using spo-ken language systems (SLS): Factors affecting perfor-mance and user satisfaction.
In Proceedings of theDARPA Speech and NL Workshop, ages 49-54.Siegel, Sidney and N. J. Castellan.
1988.
NonparametricStatistics for the Behavioral Sciences.
McGraw Hill.Simpson, A. and N. A. Fraser.
1993.
Black box andglass box evaluation of the SUNDIAL system.
In Pro-ceedings of the Third European Conference on SpeechCommunication a d Technology, pages 1423-1426.Smith, Ronnie W. and Steven A. Gordon.
1997.
Effectsof variable initiative on linguistic behavior in human-computer spoken natural language dialog.
Computa-tional Linguistics, 23(1).Sparck-Jones, Karen and Julia R. Galliers.
1996.
Evalu-ating Natural Language Processing Systems.
Springer.Walker, Marilyn A.
1996.
The Effect of Resource Limitsand Task Complexity on Collaborative Planning in Di-alogue.
Artificial Intelligence Journal, 85(1-2): 181-243.Webber, Bonnie and Aravind Joshi.
1982.
Taking theinitiative in natural language database interaction: Jus-tifying why.
In Coling 82, pages 413--419.280
