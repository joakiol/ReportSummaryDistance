Us ing  Focus  to Generate Complex  and Simple SentencesMarcia A. Derr Kathleen R. McKeownAT&T Bell LaboratoriesMurray Hill, NJ 07974 USAandDepartment of Computer ScienceColumbia UniversityDepartment of Computer ScienceColumbia UniversityNew York, NY 10027 USAAbstractOne problem for the generation of natural anguagetext is determining when to use a sequence of simplesentences and when a single complex one is moreappropriate.
In this paper, we show how focus of attentionis one factor that influences this decision and describe itsimplementation in a system that generates explanations fora student advisor expert system.
The implementation usestests on functional information such as focus of attentionwithin the Prolog definite clause grammar formalism todetermine when to use complex sentences, resulting in anefficient generator that has the same benefits as afunctional grammar system.1.
IntroductionTwo problems in natural anguage generation aredeciding what to say and how to say it.
This paperaddresses issues in the second of these tasks, that ofsurface generation.
Given a semantic representation fwhat to say, a surface generator must construct anappropriate surface structure taking into consideration awide variety of alternatives.
When a generator is used toproduce text and not just single sentences, one decision itmust make is whether to use a sequence of simplesentences or a single complex one.
We show how focus ofattention can be used as the basis on which this decisioncan be made.A second goal of this paper is to introduce aformalism for surface generation that uses aspects of Kay'sfunctional grammar (Kay, 1979) within a Prolog definiteclause grammar (Pereira and Warren, 1980).
Thisformalism was used to implement a surface generator thatmakes choices about sentence complexity based on shifts infocus of attention.
The implementation was done as partof an explanation facility for a student advisor expertsystem being developed at Columbia University.2.
Language Generation ModelIn our model of natural language generation, weassume that the task of generating a response can bedivided into two stages: determining the semantic ontentof the response and choosing a surface structure I. Onecomponent makes decisions about which information toinclude in the response and passes this information to asurface generator.
For example, an expert systemexplanation facility may select part of the goal tree, aparticular goal and its antecedent subgoals, to explain abehavior of the system.
In the advisor system, the outputof this component consists of one or more logicalpropositions where each proposition consists of a predicaterelating a group of arguments.
The output includesfunctional information, such as focus, and some syntacticfeatures, such as number and tense, for convenience.Other information, such as the relationships betweenpropositions, is implicit in the organizational structure ofthe output.The output of the semantic omponent is passed onto another component, he surface generator.
The job ofgenerator is to use whatever syntactic and lexicalinformation is needed to translate the logical propositionsinto English.
The generator must be able to make choicesconcerning various alternatives, uch as whether to useactive or passive voice, or when to pronominalize.
Whilewe have found the explanation facility for the advisorsystem to be a valuable testbed for the surface generator,the generator is an independent module that can betransported to other domains by changing only thevocabulary.3.
Choosing Surface StructureGiven a set of propositions, one decision a surfacegenerator must make is whether to produce a simplesentence for each proposition or whether to combinepropositions to form complex sentences.
As an example,consider propositions 1 and 2 below.
These may beexpressed as two simple sentences (sequence l) or as onesentence containing a subordinate clause (sentence 2).The sentences in 1 and 2 also show that a generationsystem should be able to choose between definite andindefinite reference and decide when to pronominalize.Another decision is what syntactic structure to use, such as1.
In order to concentrate on the task of surface generation, these twostages are totally separate inour system, but we doN't dispute thevalue of interaction between the two (Appclt, 1983).319whether to use the active or the passive voice.
Thus,proposition l may be expressed as any of the sentencesshown in 3-5.proposition 1:1.2.3.4.5.predicate=giveprotagonist~Johngoal '~ bookbeneficiary~MaryJohn gave Mary a book.Mary needed the book.proposition 2:predicate-needprotagonist -Marygoal = bookJohn gave Mary a book that she needed.John gave Mary a book.Mary was given a book by John.A book was given to Mary by John.Given that there are multiple ways to express thesame underlying message, how does a text generatorchoose a surface structure that is appropriate?
What aresome mechanisms for guiding the various choices?Previous research as identified focus of attention as onechoice mechanism.
McKeown (1982) demonstrated howfocus can be used to select sentence voice, and todetermine whether pronominalization is called for.
In thispaper, we will show how focus can also be used as thebasis on which to combine propositions.3.1 Linguistic BackgroundGrosz (1977) distinguished between two types offocus: global and immediate.
Immediate focus refers tohow a speaker's center of attention shifts or remainsconstant over two consecutive s ntences, while global focusdescribes the effect of a speaker's center of attentionthroughout a sequence of discourse utterances onsucceeding utterances.
In this paper, when we refer to"focus of attention," we are referring to immediate focus.Phenomena reflecting immediate focus in text havebeen studied by several linguists.
Terminology anddefinitions for these vary widely; some of the names thathave emerged include topic/comment, given/new, andtheme/rheme.
These linguistic concepts describedistinctions between functional roles elements play in asentence.
In brief, they can be defined as follows:?
Topic: Constituents in the sentence thatrepresent what the speaker is talking about.Comment labels constituents that representwhat s/he has to say about that topic (seeSgall, Hajicova, and Benesova, 1973; Lyons,1968; Reinhart, 1981).?
Given: Information that is assumed by thespeaker to be derivable from context wherecontext may mean either the precedingdiscourse or shared world knowledge.
Newlabels information that cannot be derived(see Halliday, 1967; Prince, 1979; andChafe, 1976).?
Theme: The Prague School of linguists (seeFirbas, 1966; Firbas, 1974) define the themeof a sentence as elements providing commonground for the conversants.
Rheme refers toelements that function in conveying theinformation to be imparted.
In sentencescontaining elements that are contextuallydependent, the contextually dependentelements always function as theme.
Thus,the Prague School version is close to thegiven/new distinction with the exception thata sentence always contains a theme, while itneed not always contain given information 2.What is important here is that each of theseconcepts, at one time or another, has been associated withthe selection of various syntactic structures.
For example,it has been suggested that new information and rhemeusually occur toward the end of a sentence (e.g., Halliday,1967; Lyons, 1968; Sgall et al, 1973; Firbas, 1974).
Toplace this information in its proper position in thesentence, structures other than the unmarked activesentence may be required (for example, the passive).Structures such as it-extraposition, there-insertion,3 topicalization, and left-dislocation have been shown tofunction in the introduction of new information intodiscourse (Sidner, 1979; Prince, 1979), often with theassumption that it will be talked about for a period of time(Joshi and Weinstein, 1981).
Pronominalization is anotherlinguistic device associated with these distinctions (seeAkmajian, 1973; Sidner, 1979).One major difference between linguistic conceptsand immediate focus is that focusing describes an activeprocess on the part of speaker and listener.
However, thespeaker's immediate focus influences the surfacing of eachof the linguistic concepts in the text.
It influences topic(and Halliday's theme) in that it specifies what thespeaker is focusing on (i.e., talking about) now.
But italso influences given information in that immediate focusis linked to something that has been mentioned in theprevious utterance and thus, is already present in thereader's consciousness.
Since immediate focus isintimately related to the linguistic definitions of functionalinformation, the influence of functional information on thesurface structure of the sentence can be extended toimmediate focus as well.2.
Halliday also discusses theme (Halliday, 1967), but he definestheme as that which the speaker is talking about now, as opposedto given, that which the speaker was talking about.
Thus, hisnotion of theme is closer to the concept of topic/commentarticulation.
Furthermore, Halliday always ascribes the termtheme to the element occurring first in the sentence.3.
Some xamples ofthese constructions are:I.
It was Sam who left the door open.
(it-extraposition)2.
There are 3 blocks on the table.
(there-insertion)3.
Sam, I like him.
(left-dislocation)4.
Sam I like.
(topicalization)3203.2 Focus and Complex SentencesWhile previous research in both linguistics andcomputer science has identified focus as a basis forchoosing sentence voice and for deciding when topronominalize, its influence on selecting complex sentencestructure over several simple sentences has for the mostpart gone unnoticed.
If a speaker wants to focus on asingle concept over a sequence of utterances, /he mayneed to present information about a second concept.
Insuch a case, a temporary digression must be made to thesecond concept, but the speaker will immediately continueto focus on the first.
To signal that s/he is not shiftingfocus, the speaker can use subordinate sentence structurein describing the second concept.Suppose that, in the previous example, focus is onJohn in proposition 1and book in proposition 2.
If a thirdproposition follows with focus returning to John, then thesurface generator can signal that the shift to book is onlytemporary by combining the two propositions usingsubordination as in sentence 2.
A textual sequenceillustrating this possibility is shown in 6 below.
On theother hand, if the third proposition continues to focus onbook, then it is more appropriate to generate the first andsecond propositions as two separate sentences as insentence 1 above.
It may even be possible to combine thesecond and third propositions using coordination as in thetextual sequence shown in 7 below.6.
John gave Mary a book that she needed.He had seen it in the Columbia bookstore.7.
John gave Mary a book.Mary needed the book and had been planningon buying it herself.Argument identity can also serve with focus as abasis for combining propositions as shown in the examplebelow.
In proposition 3, the values of predicate,protagonist, and focus match the values of thecorresponding arguments in proposition 4.
Thus, the twopropositions can be joined by deleting the protagonist andpredicate of the second proposition and using conjunctionto combine the two goals as in sentence 8.
Note that ifthe focus arguments were different, the propositions couldnot be combined on this basis.
Propositions 5 and 6, withmatching values for focus can also be combined by usingcoordination and deleting the focused protagonist in thesecond proposition (sentence 9).proposition 3:predicate = buyprotagonist - Johngoal -" bookfocus - Johnproposition 5:predicate -- readprotagonist = Marygoal = bookfocus = Maryproposition 4:predicate = buyprotagonist -- Johngoal -- cassettefocus "~ Johnproposition 6:predicate - playprotagonist = Marygoal = cassettefocus = Mary8.
John bought a book and a cassette.9.
Mary read the book and played the cassette.4.
A Formalism for Surface GenerationIn this section we discuss the Prolog definite clausegrammar (DCG) formalism (Pereira and Warren, 1980)and how it can be used for language generation, as well asrecognition.
We then review the functional grammarformalism (Kay, 1979) that has been used in othergeneration systems (e.g., McKeown, 1982; Appelt, 1983).Finally, we describe how aspects of a functional grammarcan be encoded in a DCG to produce a generator with thebest features of both formalisms.4.1 Definite Clause GrammarsThe DCG formalism (Pereira and Warren, 1980) isbased on a method for for expressing rammar ules asclauses of first-order predicate logic (Colmerauer, 1978;Kowalski, 1980).
As discussed by Pereira and Warren,DCGs extend context-free grammars in several ways thatmake them suitable for describing and analyzing naturallanguage.
DCGs allow nonterminals to have argumentsthat can be used to hold the string being analyzed, buildand pass structures that represent the parse tree, and carryand test contextual information (such as number orperson).
DCGs also allow extra conditions (enclosed withinbrackets '{' and '}') to be included in the rules, providinganother mechanism for encoding tests.
A simple sentencegrammar is shown in Figure 1.Viewed as a set of grammar rules, a DCG functionsas a declarative description of a language.
Viewed as a setof logic clauses, it functions as an executable program foranalyzing strings of the language.
In particular, a DCGcan be executed by Prolog, a logic programming languagethat implements an efficient resolution proof procedureusing a depth-first search strategy with backtracking and amatching algorithm based on unification (Robinson, 1965).To analyze a sentence, the sentence is encoded as anargument to a Prolog goal.
Prolog attempts to prove thisgoal by matching it against he set of grammar clauses.
Ifthe proof succeeds, the sentence is valid and a secondargument is instantiated to the parse tree structure.
Arecognition goal and its resulting parse tree are shown inFigure 1.
More extensive examples can be found inPereira and Warren (1980).321sentence(s(N P,VP)) - ->n phrase(N P,N um),v phrase(VP,N urn).n__phrase (np (Noun),N urn) -- >noun(Noun,Num).noun(n(Root,Num),Num) -->\[the\], \[Word\], {is_noun(Root,Word,Num)}.v__phrase(vp(Verb,NP),Num) -->verb(Verb,Num), n__phrase(N P,N2).verb (v (Root, N u m ,Tense), N u m) -- >\[Word\], {is verb (Root,Word,N urn,Tense) }.is_noun (student,student,singular).is_noun (student,students,plural).is_noun (answer,answer,singular).is_noun (answer,answers,plural).is__verb (give,gives,singular,pres)is_ver b (give,give,plural,pres).is_verb(give,gave, ,past).Recognition Goal:sentence (T,\[the,student,gave,the,answer\],\[\]).Result:T = s(np(n (student,singular)),vp(v (give,singular,past),np(n (answer,singular))))Generation Goal:sentence (s(np (n (student,singular)),vp(v (give,singular,past),np(n (answer,singular)))),S,\[l).Result:S = \[the,student,gave,the,answer\]Figure I.
Language recognition and generation using aDCGWhile Pereira and Warren concentrate ondescribing the DCG formalism for language recognitionthey also note its use for language generation, which issimilar to its use for language recognition.
The maindifference is in the specification of input in the goalarguments.
In recognition, the input argument specifies asurface string that is analyzed and returned as a parse treestructure in another argument.
In generation, the inputgoal argument specifies a deep structure and a resultingsurface string is returned in the second argument.
Thoughnot always practical, grammar rules can be designed towork in both directions (as were the rules in Figure 1).
Ageneration goal and the sentence it produces are shown inFigure I.4.2 Functional GrammarsAnother formalism that has been used in previous.generation systems (McKeown, 1982; Appelt, 1983) is thefunctional grammar formalism (Kay, 1979) 4 .
In afunctional, grammar, functional information such as focusand protagonist are treated in the same manner assyntactic and grammatical information such as subject andNP.
By using functional information, input to thegenerator is simplified as it need not completely specify allthe syntactic details.
Instead, tests on functionalinformation, that select between alternative surfacestructures, can be encoded in the grammar to arrive at thecomplete syntactic structure from which the string isgenerated.
This formalism is consistent with theassumption that is part of our generation model: that onegeneration component produces a semantic specificationthat feeds into another component for selecting the finalsurface structure.In the functional grammar formalism, both theunderlying message and the grammar are specified asfunctional descriptions, lists of attribute-value pairs, thatare unified 5 to produce a single complete surface structuredescription.
The text is then derived by linearizing thecomplete surface structure description.
As an example,consider the proposition encoded as a functionaldescription below.
When unified with a sentence grammarthat contains tests on focus to determine voice and orderconstituents, sentence 12 is generated.
If FOCUS were<GOAL>,  instead, sentence 13 would result.CAT ~ SPRED = \[LEX = give\]TENSE = PASTPROT ---- \[LEX = student\]GOAL = \[LEX = answer\]BENEF = NONEFOCUS = <PROT>12.
The student gave the answer.13.
The answer was given by the student.Previous implementations of functional grammarshave been concerned with the efficiency of the functionalgrammar unification algorithm.
Straightforwardimplementations of the algorithm have proved too time-consuming (McKeown, 1982) and efforts have been madeto alter the algorithm to improve efficiency (Appelt, 1983).Efficiency continues to be a problem and a functionalgrammar generator that can be used practically has as yetto be developed.4.3 Combining the FormalismsWe have implemented a surface generator based onboth the DCG formalism and the functional grammar4.
Functional grammar has also been referred to as unificationgrammar (Appett, 1983).5.
The functional grammar unification operation is similar to setunion.
A description of the algorithm is given in Appelt (1983).It is not to be confused with the unification process used inresolution theorem proving, though a similarity has been noted byPereira nd Warren (1983).322formalism.
The result is a generator with the best featuresof both grammars: simplification of input by usingfunctional information and efficiency of execution throughProlog.
Functional information, supplied as part of thegeneration goal's input argument, is used by the grammarrules to select an appropriate surface structure.
The extraconditions and context arguments allowed by the DCGformalism provide the mechanism for testing functionalinformation.Figure 2 shows a proposition encoded as the inputargument o a DCG goal.
The proposition specifies, inorder, a predicate, protagonist, goal, beneficiary, and focus.In this example, the focus argument is the same as theprotagonist.
While the proposition also includes tense andnumber information, less syntactic information is specifiedcompared to the input argument of the generation goal inFigure 1.
In particular, no information regardingconstituent order is specified.
Also shown in Figure 2 aresome DCG rules for choosing syntactic structure based onfocus.
The rules test for number agreement and tense, aswell.
The sentence rule selects the focused argument asthe subject noun phrase.
The vp rule determines thatfocus is on the protagonist, selects active voice, and putsthe goal into a noun phrase followed by the beneficiary ina to prepositional phrase.
Thus, the order of constituentsin the generated sentence is not explicitly stated in theinput goal, but is determined during the generationprocess.
The sentence that results from the givenproposition is shown at the bottom of Figure 2.Generation Goal:sentence(prop(pred (give, past),arg(student, singular),arg(answer, singular),arg(nil, __),arg(student, singular)),S,\[\]).Rules:sentence (prop(Pred,Prot,Goal,Bene, Foc)) -- >np(Foc),vp(Pred,Prot,Goal,Bene,Foc).vp (pred (Verb,Tense),Prot,Goal,Bene,Prot) -- >{gemum(Prot,Num)},verb (Verb,Tense,Num,active),np(Goal),pp(to,Bene).Result:S m \[the,student,gave,the,answer\]Figure 2.
DCG rules that use focus to select syntacticstructure5.
Surface Generator ImplementationA surface generator, with mechanisms for selectingsurface structure and, in particular, combiningpropositions, was implemented as part of an explanationfacility for a student advisor expert system which isimplemented in Prolog.
One component of the advisorsystem, the planner, determines a student's chedule ofcourses for a particular semester.
An explanation of theresults of the planning process can be derived from a traceof the Prolog goals that were invoked during planning(Davis and Lenat, 1982).
Each element of the trace is aproposition that corresponds to a goal.
The propositionsare organized hierarchically, with propositions toward thetop of the hierarchy corresponding to higher level goals.Relationships between propositions are implicit in thisorganization.
For example, satisfying a higher level goal isconditional on satisfying its subgoals.
This provides a richtestbed on which to experiment with techniques forcombining propositions.
Because the expert system doesnot yet automatically generate a trace of its execution, thepropositions that served as input to the surface generatorwere hand-encoded from the results of several systemexecutions.
In the current implementation, the grammar islimited to handling input propositions tructured as a listof antecedents ( ubgoals) followed by a single consequence(goal).A grammar for automatically generatingexplanations was implemented using the formalismdescribed in the previous section.
The grammar encodesseveral tests for combining propositions.
Based ontemporary focus shift, it forms complex sentences usingsubordination.
Based on focus and argument identities ituses coordination and identity deletion to combinepropositions.
The grammar also includes tests on focus fordetermining active/passive sentence voice, but does notcurrently pronominalize on the basis of focus.The generator determines that subordination isnecessary by checking whether focus shifts over a sequenceof three propositions.
A simplified example of a DCG rulefocshi f t ,  that tests for this is shown in Figure 3.
Theleft-hand-side of this rule contains three input propositionsand an output proposition.
Each proposition has fivearguments: verb, protagonist, goal, beneficiary, and focus.If the first proposition focuses on Focl and mentions anunfocused argument Goall, and if the second propositionspecifies Goall as its focus, 6 but in the third propositionthe focus returns to Focl, then the first and secondpropositions can be combined using subordination.
Thecombined propositions are returned as a single propositionin the fourth argument; the third proposition is returned,unchanged, in the third argument.
Both can be tested forfurther combination with other propositions.
A sampletext produced using this rule is shown in 14 below.6.
The right-hand-side of the rule contains a test to check that thefocus of the second proposition is different from the focus of thefirst.323foc shift (prop (Verbl, Protl, Goall, Benl, Focl),prop (Verb2, Prot2, Goal2, Ben2, Goall ),prop (Verb3, Prot3, Goal3, Ben3, Focl),prop (Verbl, Protl,np(Goall, prop(Verb2, Prot2, Goal2, Ben2, Goall )),Benl, Focl)){Goall \~= Focl }.14.
Assembly Language has a prerequisite that was taken.Assembly Language does not conflict.Figure 3.
Combining propositions using subordinationOther tests for combining propositions look foridentities among the arguments of propositions.
Simplifiedexamples of these rules are id del and focdel in Figure 4.According to id_del, if the -first and second propositiondiffer only by the arguments Goall and Goal2o thesearguments are combined into one Goal and returned in thethird proposition.
The result is a single sentencecontaining a noun phrase conjunction as sentence 15illustrates.
The other rule, foc_del, specifies that if twopropositions have the same focus, Foc, and in the secondproposition, the focus specifies the protagonist, then thetwo propositions can form a coordinate sentence, deletingthe focused protagonist of the second proposition.
Insteadof returning a proposition, foc_del in its right-hand-side,invokes rules for generating a compound sentence.
Sampletext generated by this rule is shown in 16.id del (prop (Verb, Prot, Goall, Ben, Foe),prop (Verb, Prot, Goal2, Ben, Foe),prop (Verb, Prot, Goal, Ben, Foe)){Goall \="  Goal2, append (Goall, Goal2, Goal)}.foc del (prop (Verbl, Protl, Goall, Benl, Foe),prop (Verb2, Prot2, Goal2, Ben2, Foe))sentence (prop (Verb I, Prot I, Goal 1, Ben 1, Foc) ),\[andl,verb_phrase (Verb2, Prot2, Goal2, Ben2, Foe).15.
Analysis of Algorithms requires Data Structuresand Discrete Math.16.
Introduction toComputer Programming does nothave prerequisites and does not conflict.Figure 4.
Combining propositions using coordination andidentity deletionThe generator uses of the organization of the inputto show causal connectives.
Recall that the input to thegenerator is a set of propositions divided into a list ofantecedents and a single consequence that was derived bythe expert system.
The generator can identify theconsequence for the reader by using a causal connective.An explanation for why a particular course was notscheduled is shown in 17.
The antecedents are presentedin the first part of the explanation; the consequence,introduced by therefore, follows.17.
Modeling and Analysis of Operating Systems requiresFundamental A gorithms, Computability and FormalLanguages, and Probability.Fundamental A gorithms and Computability andFormal Languages were taken.Probability was not taken.Therefore, Modeling and Analysis of OperatingSystems was not added.6.
Related Work in GenerationThere are two basic classes of related work ingeneration.
The first class of systems makes use offunctional information in constructing the surface structureof the text and has relatively little to say about how andwhen to produce complex sentences.
The second class ofwork has addressed the problem of producing complexsentences but does not incorporate functional informationas part of this decision making process.Of the systems which make use of functionalinformation, three (Kay, 1979; McKeown, 1982; Appelt,1983) have already been mentioned.
Kay's work providesthe basis for McKeown's and Appelt's and emphasizes thedevelopment of a formalism and grammar for generationthat allows for the use of functional information.
BothMcKeown and Appelt make direct use of Kay's formalism,with McKeown's emphasis being on the influence of focusinformation on syntax and Appelt's emphasis being on thedevelopment of a facility that allows interaction betweenthe grammar and an underlying planning component.Nigel (Mann, 1983) is a fourth system that makesuse of functional information and is based on systemicgrammar (Hudson, 1974).
A systemic grammar containschoice points that query the environment to decide betweenalternatives (the environment may include functional,discourse, semantic, or contextual information).
Mann'semphasis, so far, has been on the development of thesystem, on the development of a large linguisticallyjustified grammar, and on the influence of underlyingsemantics on choices.
The influence of functionalinformation on syntactic hoice as well as the generation ofcomplex propositions are issues he has not yet addressedwithin the systemic grammar framework.Of previous ystems that are able to combine simpleclauses to produce complex sentences, Davey's (1978) isprobably the most sophisticated.
Davey's ystem is able torecognize underlying semantic and rhetorical relationsbetween propositions to combine phrases using textualconnectives, also an important basis for combining.
propositions.
His emphasis is on the identification ofcontrastive relations that could be specified by connectivessuch as although, but, or however.
While Davey uses asystemic grammar in his generator, he does not exploit the324influence of functional information on generating complexsentences.Several other systems also touch on the generationof complex sentences although it is not their main focus.MUMBLE (McDonald, 1983) can produce complexsentences if directed to do so.
It is capable of ignoringthese directions when it is syntactically inappropriate toproduce complex sentences, but it can not decide when tocombine propositions.
KDS (Mann, 1981) uses heuristicsthat sometimes dictate that a complex sentence isappropriate, but the heuristics are not based on generallinguistic principles.
Ana (Kukich, 1983) can alsocombine propositions, although, like Davey, the decision isbased on rhetorical relations rather than functionalinformation.In sum, those systems that are capable ofgenerating complex sentences tend to rely on rhetorical,semantic, or syntactic information to make their decisions.Those systems that make use of functional informationhave not investigated the general problem of choosingbetween complex and simple sentences.7.
Future DirectionsThe current implementation can be extended in avariety of ways to produce better connected text.Additional research is required to determine how and whento use other textual connectives for combining propositions.For example, the second and third sentences of 17 mightbe better expressed as 18.18.
Although Fundamental Algorithms andComputability and Formal Languages were taken,Probability was not taken.The question of how to organize propositions andhow to design the grammar to handle various organizationsdeserves further attention.
In the current implementation,the grammar is limited to handling input propositionsstructured as a list of antecedents and a singleconsequence.
If propositions were organized in trees ratherthan lists, as in more complex explanations, the use ofadditional connectives would be necessary.The grammar can also be extended to include testsfor other kinds of surface choice such as definite/indefinitereference, pronominalization, and lexical choice.
As thegrammar grows larger and more complex, the task ofspecifying rules becomes unwieldy.
Further work isneeded to devise a method for automatically generatingDCG rules.8.
ConclusionsWe have shown how focus of attention can be usedas the basis for a language generator to decide when tocombine propositions.
By encoding tests on functionalinformation within the DCG formalism, we haveimplemented an efficient generator that has the samebenefits as a functional grammar: input is simplified andsurface structure can be determined based on constituents'function within the sentence.
In addition to producingnatural language explanations for the student advisorapplication, this formalism provides a useful research toolfor experimenting with techniques for automatic textgeneration.
We plan to use it to investigate additionalcriteria for determining surface choice.AcknowledgmentsWe would like to thank Mitch Marcus for hiscomments on an earlier version of this paper.ReferencesAkmajian, A.
(1973), "The role of focus in theinterpretation of anaphoric expressions," In Andersonand Kiparsky (Ed.
), Festschrifffor Morris Halle, Holt,Rinehart, and Winston, New York, NY, 1973.Appelt, Douglas E. (1983), "Telegram: a grammarformalism for language planning," Proceedings of the21st Annual Meeting of the Association forComputational Linguistics, 74-78, !
983.Chafe, W. L. (1976), "'Givenness, contrastiveness,definiteness, ubjects, topics, and points of view," In Li,C.
N.
(Ed.
), Subject and Topic, Academic Press, NewYork, 1976.Colmerauer, A.
(1978), "Metamorphosis grammars," InBole, L.
(Ed.
), Natural Language Communication withComputers, Springer, Berlin, 1978.Davey, Anthony.
(1978), Discourse Production,Edinburgh University Press, 1978.Davis, Randall and Lenat, Douglas B.
(1982),Knowledge-Based Systems in Artificial Intelligence,McGraw-Hill, New York, 1982.Firbas, J.
(1966), "On defining the theme in functionalsentence analysis," Travaux Linguistiques de Prague !,University of Alabama Press, 1966.Firbas, J.
(1974), "Some aspects of the Czechoslovakapproach to problems of functional sentenceperspective," Papers on Functional SentencePerspective, Academia, Prague, 1974.Grosz, B. J .
(1977), The representation a d use of focusin dialogue understanding.
Technical note 151,Stanford Research Institute, Menlo Park, CA, 1977.Halliday, M. A. K. (1967), "Notes on transitivity andtheme in English," Journal of Linguistics, 3 1967.Hudson, R.A. (1974), "Systemic generative grammar,"Linguistics, 139, 5-42, 1974.Joshi, A. and Weinstein, S. (1981), "Control of inference:role of some aspects of discourse structure - centering,"Proceedings of the 7th International Joint Cot~erenceon Artificial Intelligence, 198 I.Kay, Martin.
(1979), "Functional grammar," Proceedingsof the 5th Annual Meeting of the Berkeley LinguisticSociety, 1979.Kowalski, R .A .
(1980), Logic for Problem Solving,North Holland, New York, NY, 1980.Kukich, Karen.
(1983), "Design of a knowledge-basedreport generator," Proceedings of the 21st AnnualMeeting of the Association for Computational325Linguistics, 145-150, 1983.Lyons, J, (1968), Introduction to Theoretical Linguistics,Cambridge University Press, London, 1968.Mann, W.A.
and Moore, J.A.
(1981), "Computergeneration of multiparagraph English text," AmericanJournal of Computational Linguistics, 7 (1), 17-29,1981.Mann, William C. (1983), "An overview of the Nigel textgeneration grammar," Proceedings of the 21st AnnualMeeting of the Association for ComputationalLinguistics, 79-84, 1983.McDonald, David D. (1983), "Description directedcontrol: its implications for natural languagegeneration," Computers and Mathematics withApplications, 9 (I), I 11-129, 1983.McKeown, Kathleen R. (1982), Generating NaturalLanguage Text in Response to Questions aboutDatabase Structure, Ph.D. dissertation, University ofPennsylvania, 1982.Pereira, Fernando C. N. and Warren, David H. D.(1980), "'Definite clause grammars for languageanalysis--a survey of the formalism and a comparisonwith augmented transition networks," ArtilqcialIntelligence, 13, 231-278, 1980.Pereira, Fernando C. N. and Warren.
David H. D.(1983), "'Parsing as deduction," Proceedings of the21st Annual Meeting o\[ the Association forComputational Linguistics.
137-144, 1983.Prince, E. (1979), "On the given/new distinction," CLS,15, 1979.Reinhart, T. (1981), "Pragmatics and linguistics: ananalysis of sentence topics," Philosophica, 1981.Robinson, J.
A.
(1965), "A machine-oriented logic basedon the resolution principle," Journal of the ACM, 12(I), 23-41, 1965.Sgall, P., Hajicova.
E., and Benesova, E. (1973), Focusand Generative Semantics, Scriptor Verlag, DemocraticRepublic of Germany, 1973.Sidner, C. L. (1979), Towards a Computation Theory ofDefinite Anaphora Comprehension in EnglishDiscourse, Ph.D. dissertation, MIT, Cambridge, MA,1979.326
