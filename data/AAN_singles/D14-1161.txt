Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1522?1531,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsWord Semantic Representations using Bayesian Probabilistic TensorFactorizationJingwei Zhang and Jeremy SalwenColumbia UniversityComputer ScienceNew York, NY 10027, USA{jz2541,jas2312}@columbia.eduMichael Glass and Alfio GliozzoIBM T.J. Waston ResearchYorktown Heights, NY 10598, USA{mrglass,gliozzo}@us.ibm.comAbstractMany forms of word relatedness have beendeveloped, providing different perspec-tives on word similarity.
We introducea Bayesian probabilistic tensor factoriza-tion model for synthesizing a single wordvector representation and per-perspectivelinear transformations from any numberof word similarity matrices.
The result-ing word vectors, when combined with theper-perspective linear transformation, ap-proximately recreate while also regulariz-ing and generalizing, each word similarityperspective.Our method can combine manually cre-ated semantic resources with neural wordembeddings to separate synonyms andantonyms, and is capable of generaliz-ing to words outside the vocabulary ofany particular perspective.
We evaluatedthe word embeddings with GRE antonymquestions, the result achieves the state-of-the-art performance.1 IntroductionIn recent years, vector space models (VSMs)have been proved successful in solving variousNLP tasks including named entity recognition,part-of-speech tagging, parsing, semantic role-labeling and answering synonym or analogy ques-tions (Turney et al., 2010; Collobert et al., 2011).Also, VSMs are reported performing well ontasks involving the measurement of word related-ness (Turney et al., 2010).
Many existing worksare distributional models, based on the Distribu-tional Hypothesis, that words occurring in simi-lar contexts tend to have similar meanings (Har-ris, 1954).
The limitation is that word vectors de-veloped from distributional models cannot revealword relatedness if its information does not lie inword distributions.
For instance, they are believedto have difficulty distinguishing antonyms fromsynonyms, because the distribution of antonymouswords are close, since the context of antonymouswords are always similar to each other (Moham-mad et al., 2013).
Although some research claimsthat in certain conditions there do exist differ-ences between the contexts of different antony-mous words (Scheible et al., 2013), the differencesare subtle enough that it can hardly be detected bysuch language models, especially for rare words.Another important class of lexical resource forword relatedness is a lexicon, such as Word-Net (Miller, 1995) or Roget?s Thesaurus (Kipfer,2009).
Manually producing or extending lexi-cons is much more labor intensive than generat-ing VSM word vectors using a corpus.
Thus, lex-icons are sparse with missing words and multi-word terms as well as missing relationships be-tween words.
Considering the synonym / antonymperspective as an example, WordNet answers lessthan 40% percent of the the GRE antonym ques-tions provided by Mohammad et al.
(2008) di-rectly.
Moreover, binary entries in lexicons do notindicate the degree of relatedness, such as the de-gree of lexical contrast between happy and sad orhappy and depressed.
The lack of such informa-tion makes it less fruitful when adopted in NLPapplications.In this work, we propose a Bayesian tensor fac-torization model (BPTF) for synthesizing a com-posite word vector representation by combiningmultiple different sources of word relatedness.The input is a set of word by word matrices, whichmay be sparse, providing a number indicating thepresence or degree of relatedness.
We treat wordrelatedness matrices from different perspectives asslices, forming a word relatedness tensor.
Then thecomposite word vectors can be efficiently obtainedby performing BPTF.
Furthermore, given any twowords and any trained relatedness perspective, we1522can create or recreate the pair-wise word related-ness with regularization via per-perspective lineartransformation.This method allows one set of word vectors torepresent word relatednesses from many differentperspectives (e.g.
LSA for topic relatedness / cor-pus occurrences, ISA relation and YAGO type) Itis able to bring the advantages from both word re-latedness calculated by distributional models, andmanually created lexicons, since the former havemuch more vocabulary coverage and many varia-tions, while the latter covers word relatedness thatis hard to detect by distributional models.
We canuse information from distributional perspectives tocreate (if does not exist) or re-create (with regular-ization) word relatedness from the lexicon?s per-spective.We evaluate our model on distinguishing syn-onyms and antonyms.
There are a number of re-lated works (Lin and Zhao, 2003; Turney, 2008;Mohammad et al., 2008; Mohammad et al., 2013;Yih et al., 2012; Chang et al., 2013).
A number ofsophisticated methods have been applied, produc-ing competitive results using diverse approaches.We use the GRE antonym questions (Mohammadet al., 2008) as a benchmark, and answer thesequestions by finding the most contrasting choiceaccording to the created or recreated synonym /antonym word relatedness.
The result achievesstate-of-the-art performance.The rest of this paper is organized as fol-lows.
Section 2 describes the related work ofword vector representations, the BPTF model andantonymy detection.
Section 3 presents our BPTFmodel and the sampling method.
Section 4 showsthe experimental evaluation and results with Sec-tion 5 providing conclusion and future work.2 Related Work2.1 Word Vector RepresentationsVector space models of semantics have a long his-tory as part of NLP technologies.
One widely-used method is deriving word vectors using la-tent semantic analysis (LSA) (Deerwester et al.,1990), for measuring word similarities.
This pro-vides a topic based perspective on word simi-larity.
In recent years, neural word embeddingshave proved very effective in improving variousNLP tasks (e.g.
part-of-speech tagging, chunking,named entity recognition and semantic role label-ing) (Collobert et al., 2011).
The proposed neuralmodels have a large number of variations, such asfeed-forward networks (Bengio et al., 2003), hi-erarchical models (Mnih and Hinton, 2008), re-current neural networks (Mikolov, 2012), and re-cursive neural networks (Socher et al., 2011).Mikolov et al.
(2013) reported their vector-spaceword representation is able to reveal linguisticregularities and composite semantics using sim-ple vector addition and subtraction.
For example,?King?Man+Woman?
results in a vector veryclose to ?Queen?.
Luong et al.
(2013) proposeda recursive neural networks model incorporatingmorphological structure, and has better perfor-mance for rare words.Some non-VSM models1also generate wordvector representations.
Yih et al.
(2012) apply po-larity inducing latent semantic analysis (PILSA)to a thesaurus to derive the embedding of words.They treat each entry of a thesaurus as a docu-ment giving synonyms positive term counts, andantonyms negative term counts, and preform LSAon the signed TF-IDF matrix In this way, syn-onyms will have cosine similarities close to oneand antonyms close to minus one.Chang et al.
(2013) further introduced Multi-Relational LSA (MRLSA), as as extension ofLSA, that performs Tucker decomposition over athree-way tensor consisting of multiple relations(document-term like matrix) between words asslices, to capture lexical semantics.
The purposesof MRLSA and our model are similar, but the dif-ferent factorization techniques offer different ad-vantages.
In MRLSA, the k-th slice of tensor Wis approximated byW:,:,k?
X:,:,k= US:,:,kVT,where U and V are both for the same word listbut are not guaranteed (or necessarily desired) tobe the same.
Thus, this model has the ability tocapture asymmetric relations, but this flexibility isa detriment for symmetric relatedness.
In order toexpand word relatedness coverage, MRLSA needsto choose a pivot slice (e.g.
the synonym slice),thus there always must existence such a slice, andthe model performance depends on the quality ofthis pivot slice.
Also, while non-completeness isa pervasive issue in manually created lexicons,MRLSA is not flexible enough to treat the un-known entries as missing.
Instead it just sets them1As defined by Turney et al.
(2010), VSM must be derivedfrom event frequencies.1523to zero at the beginning and uses the pivot sliceto re-calculate them.
In contrast, our method ofBPTF is well suited to symmetric relations withmany unknown relatedness entries.2.2 BPTF ModelSalakhutdinov and Mnih (2008) introduced aBayesian Probabilistic Matrix Factorization(BPMF) model as a collaborative filtering algo-rithm.
Xiong et al.
(2010) proposed a BayesianProbabilistic Tensor Factorization (BPTF) modelwhich further extended the original model toincorporate temporal factors.
They modeled latentfeature vector for users and items, both can betrained efficiently using Markov chain MonteCarlo methods, and they obtained competitiveresults when applying their models on real-worldrecommendation data sets.2.3 Antonomy DetectionThere are a number of previous works in detect-ing antonymy.
Lin and Zhao (2003) identifiesantonyms by looking for pre-identified phrases incorpus datasets.
Turney (2008) proposed a su-pervised classification method for handling analo-gies, then apply it to antonyms by transformingantonym pairs into analogy relations.
Mohammadet al.
(Mohammad et al., 2008; Mohammad etal., 2013) proposed empirical approaches consid-ering corpus co-occurrence statistics and the struc-ture of a published thesaurus.
Based on the as-sumption that the strongly related words of twowords in a contrasting pair are also often antony-mous, they use affix patterns (e.g.
?un-?, ?in-?
and?im-?)
and a thesaurus as seed sets to add con-trast links between word categories.
Their bestperformance is achieved by further manually an-notating contrasting adjacent categories.
This ap-proach relies on the Contrast Hypothesis, whichwill increase false positives even with a carefullydesigned methodology.
Furthermore, while thisapproach can expand contrast relationships in alexicon, out-of-vocabulary words still pose a sub-stancial challenge.Yih et al.
(2012) and Chang et al.
(2013) alsoapplied their vectors on antonymy detection, andYih et al.
achieves the state-of-the-art performancein answering GRE antonym questions.
In additionto the word vectors generated from PILSA, theyuse morphology and k-nearest neighbors from dis-tributional word vector spaces to derive the em-beddings for out-of-vocabulary words.
The latteris problematic since both synonyms and antonymsare distributionally similar.
Their approach is twostage: polarity inducing LSA from a manuallycreated thesaurus, then falling back to morphol-ogy and distributional similarity when the lexiconlacks coverage.
In contrast, we focus on fusingthe information from thesauruses and automati-cally induced word relatedness measures duringthe word vector space creation.
Then predictionis done in a single stage, from the latent vectorscapturing all word relatedness perspectives and theappropriate per-perspective transformation vector.3 Methods3.1 The Bayesian Probabilistic TensorFactorization ModelOur model is a variation of the BPMF model(Salakhutdinov and Mnih, 2008), and is similarto the temporal BPTF model (Xiong et al., 2010).To model word relatedness from multiple perspec-tives, we denote the relatedness between word iand word j from perspective k as Rkij.
Then wecan organize these similarities to form a three-waytensor R ?
RN?N?K.Table 1 shows an example, the first slice of thetensor is a N ?
N matrix consists of 1/-1 corre-sponding to the synonym/antonym entries in theRoget?s thesaurus, and the second slice is aN?Nmatrix consists of the cosine similarity from neuralword embeddings created by Luong et al.
(2013),where N is the number of words in the vocabu-lary.
Note that in our model the entries missingin Table 1a do not necessarily need to be treatedas zero.
Here we use the indicator variable Ikijto denote if the entry Rkijexists (Ikij= 1) or not(Ikij= 0).
If K = 1, the BPTF model becomes toBPMF.
Hence the key difference between BPTFand BPMF is that the former combines multi-ple complementary word relatedness perspectives,while the later only smooths and generalizes overone.We assume the relatedness Rkijto be Gaussian,and can be expressed as the inner-product of threeD-dimensional latent vectors:Rkij|Vi, Vj, Pk?
N (< Vi, Vj, Pk>,?
?1),where< ?, ?, ?
> is a generalization of dot product:< Vi, Vj, Pk>?D?d=1V(d)iV(d)jP(d)k,1524happy joyful lucky sad depressedhappy 1 1 -1 -1joyful 1 -1lucky 1 -1sad -1 -1 -1 1depressed -1 1(a) The first slice: synonym & antonym relatednesshappy joyful lucky sad depressedhappy .03 .61 .65 .13joyful .03 .25 .18 .23lucky .61 .25 .56 .31sad .65 .18 .56 -.01depressed .13 .23 .31 -.01(b) The second slice: distributional similarityTable 1: Word Relatedness Tensorand ?
is the precision, the reciprocal of the vari-ance.
Viand Vjare the latent vectors of word i andword j, and Pkis the latent vector for perspectivek.We follow a Bayesian approach, adding Gaus-sian priors to the variables:Vi?
N (?V,??1V),Pi?
N (?P,?
?1P),where ?Vand ?Pare D dimensional vectors and?Vand ?Pare D-by-D precision matrices.Furthermore, we model the prior distribution ofhyper-parameters as conjugate priors (followingthe model by (Xiong et al., 2010)):p(?)
=W(?|?W0, ?0),p(?V,?V) = N (?V|?0, (?0?V)?1)W(?V|W0, ?0),p(?P,?P) = N (?P|?0, (?0?P)?1)W(?P|W0, ?0),where W(W0, ?0) is the Wishart distribution ofdegree of freedom ?
and a D-by-D scale matrixW , and?W0is a 1-by-1 scale matrix for ?.
Thegraphical model is shown in Figure 1 (with ?0setto 1).
After choosing the hyper-priors, the only re-maining parameter to tune is the dimension of thelatent vectors.Due to the existence of prior distributions, ourmodel can capture the correlation between dif-ferent perspectives during the factorization stage,then create or re-create word relatedness using thiscorrelation for regularization and generalization.This advantage is especially useful when such cor-relation is too subtle to be captured by other meth-ods.
On the other hand, if perspectives (let?s say kand l) are actually unrelated, our model can handleit as well by making Pkand Plorthogonal to eachother.3.2 InferenceTo avoid calculating intractable distributions, weuse a numerical method to approximate the re-sults.
Here we use the Gibbs sampling algorithmRkijPk?P?P?0W0, ?0?ViVj?V?VW0, ?0?0?
?
?
?
?
??
?
?k = 1, ..., KIki,j= 1i 6= ji, j = 1, ..., NFigure 1: The graphical model for BPTF.to perform the Markov chain Monte Carlo method.When sampling a block of parameters, all otherparameters are fixed, and this procedure is re-peated many times until convergence.
The sam-pling algorithm is shown in Algorithm 1.With conjugate priors, and assuming Iki,i=0, ?i, k (we do not consider a word?s relatednessto itself), the posterior distributions for each blockof parameters are:p(?|R,V,P) =W(?W0?, ??0?)
(1)Where:??
?0= ??0+2?k=1N?i,j=1Ikij,(?W?0)?1=?W?10+2?k=1N?i,j=1Ikij(Rkij?
< Vi, Vj, Pk>)21525p(?V,?V|V) = N (?V|?
?0, (?
?0?V)?1)W(?V|W?0, ??0)(2)Where:?
?0=?0?0+N?V?0+N, ?
?0= ?0+N, ?
?0= ?0+N,(W?0)?1= W?10+N?S +?0N?0+N(?0?
?V )(?0?
?V )T,?V =1NN?i=1Vi,?S =1NN?i=1(Vi?
?V )(Vi?
?V )Tp(?P,?P|P) = N (?P|?
?0, (?
?0?P)?1)W(?P|W?0, ?
?0)(3)Which has the same form as p(?V,?V|V).p(Vi|R,V?i,P, ?V,?V, ?)
= N (?
?i, (?
?i)?1) (4)Where:?
?i= (?
?i)?1(?V?V+ ?2?k=1N?j=1IkijRkijQjk),?
?i= ?V+ ?2?k=1N?j=1IkijQjkQTjk,Qjk= VjPkis the element-wise product.p(Pi|R,V,P?i, ?P,?P, ?)
= N (?
?i, (?
?i)?1) (5)Where:?
?k= (?
?k)?1(?P?P+ ?N?i,j=1IkijRkijXij),?
?k= ?P+ ?N?i,j=1IkijXijXTij,Xij= ViVjThe influence each perspective k has on the la-tent word vectors is roughly propotional to thenumber of non-empty entries nk=?i,jIki,j.
Ifone wants to adjust the weight of each slices, thiscan easily achieved by adjusting (e.g.
down sam-pling) the number of entries of each slice sampledat each iteration.3.2.1 Out-of-Vocabulary wordsIt often occurs that some of the perspectives havegreater word coverage than the others.
For ex-ample, hand-labeled word relatedness usually hasmuch less coverage than automatically acquiredsimilarities.
Of course, it is typically for the hand-labeled perspectives that the generalization is mostAlgorithm 1 Gibbs Sampling for BPTFInitialize the parameters.repeatSample the hyper-parameters ?, ?V, ?V, ?P,?P(Equation 1, 2, 3)for i = 1 to N doSample Vi(Equation 4)end forfor k = 1 to 2 doSample Pk(Equation 5)end foruntil convergencedesired.
In this situation, our model can generalizeword relatedness for the sparse perspective.
Forexample, assume perspective k has larger vocabu-lary coverageNk, while perspective l has a smallercoverage Nl.There are two options for using the high vocab-ulary word relation matrix to generalize over theperspective with lower coverage.
The most directway simply considers the larger vocabulary in theBPTF R ?
RNk?Nk?Kdirectly.
A more efficientmethod trains on a tensor using the smaller vocab-ulary R ?
RNl?Nl?K, then samples the Nk?Nlword vectors using Equation 4.3.3 PredictionsWith MCMC method, we can approximate theword relatedness distribution easily by averagingover a number of samples (instead of calculatingintractable marginal distribution):p(?Rkij|R) ?1MM?m=1p(?Rkij|Vmi, Vmj, Pmk, ?m),wherem indicate parameters sampled from differ-ent sampling iterations.3.4 ScalabilityThe time complexity of training our model isroughly O(n?D2), where n is the number of ob-served entries in the tensor.
If one is only inter-ested in creating and re-creating word relatednessof one single slice rather than synthesizing wordvectors, then entries in other slices can be down-sampled at every iteration to reduce the trainingtime.
In our model, the vector length D is notsensitive and does not necessarily need to be verylong.
Xiong et al.
(2010) reported in their collab-orative filtering experiment D = 10 usually givessatisfactory performance.15264 Experimental EvaluationIn this section, we evaluate our model by answer-ing antonym questions.
This task is especiallysuitable for evaluating our model since the perfor-mance of straight-forward look-up from the the-sauruses we considered is poor.
There are two ma-jor limitations:1.
The thesaurus usually only contains antonyminformation for word pairs with a strong con-trast.2.
The vocabulary of the antonym entries in thethesaurus is limited, and does not containmany words in the antonym questions.On the other hand, distributional similarities canbe trained from large corpora and hence have alarge coverage for words.
This implies that we cantreat the thesaurus data as the first slice, and thedistributional similarities as the second slice, thenuse our model to create / recreate word relatednesson the first slice to answer antonym questions.4.1 The GRE Antonym QuestionsThere are several publicly available test datasetsto measure the correctness of our word embed-dings.
In order to be able to compare with pre-vious works, we follow the widely-used GRE testdataset provided by (Mohammad et al., 2008),which has a development set (consisting of 162questions) and a test set (consisting of 950 ques-tions).
The GRE test is a good benchmark becausethe words are relatively rare (19% of the words inMohammad?s test are not in the top 50,000 mostfrequent words from Google Books (Goldberg andOrwant, 2013)), thus it is hard to lookup answersfrom a thesaurus directly with high recall.
Belowis an example of the GRE antonym question:adulterate: a. renounce b. forbidc.
purify d. criticize e. correctThe goal is to choose the most opposite word fromthe target, here the correct answer is purify.4.2 Data ResourcesIn our tensor model, the first slice (k = 1) con-sists of synonyms and antonyms from public the-sauruses, and the second slice (k = 2) consists ofcosine similarities from neural word embeddings(example in Table 1)4.2.1 ThesaurusTwo popular thesauruses used in other research arethe Macquarie Thesaurus and the Encarta The-saurus.
Unfortunately, their electronic versionsare not publicly available.
In this work we use twoalternatives:WordNet Words in WordNet (version 3.0) aregrouped into sense-disambiguated synonym sets(synsets), and synsets have links between eachother to express conceptual relations.
Previ-ous works reported very different look-up perfor-mance using WordNet (Mohammad et al., 2008;Yih et al., 2012), we consider this differenceas different understanding of the WordNet struc-ture.
By extending ?indirect antonyms?
defined inWordNet to nouns, verbs and adverbs that similarwords share the antonyms,we achieve a look-upperformance close to Yih et al.
(2012).
Using thisinterpretation of WordNet synonym and antonymstructure we obtain a thesaurus containing 54,239single-token words.
Antonym entries are presentfor 21,319 of them with 16.5 words per entry onaverage, and 52,750 of them have synonym entrieswith 11.7 words per entry on average.Roget?s Only considering single-token words,the Roget?s Thesaurus (Kipfer, 2009) contains47,282 words.
Antonym entries are present for8,802 of them with 4.2 words per entry on av-erage, and 22,575 of them have synonym entrieswith 20.7 words per entry on average.
Althoughthe Roget?s Thesaurus has a less coverage on bothvocabulary and antonym pairs, it has better look-up precision in the GRE antonym questions.4.2.2 Distributional SimilaritiesWe use cosine similarity of the morphRNN wordrepresentations2provided by Luong et al.
(2013)as a distributional word relatedness perspective.They used morphological structure in training re-cursive neural networks and the learned mod-els outperform previous works on word similaritytasks, especially a task focused on rare words.
Thevector space models were initialized from exist-ing word embeddings trained on Wikipedia.
Weuse word embeddings adapted from Collobert etal.
(2011).
This advantage complements the weak-ness of the thesaurus perspective ?
that it has lesscoverage on rare words.
The word vector data con-tains 138,218 words, and it covers 86.9% of thewords in the GRE antonym questions.
Combiningthe two perspectives, we can cover 99.8% of the1527Dev.
Set Test SetPrec.
Rec.
F1Prec.
Rec.
F1WordNet lookup 0.40 0.40 0.40 0.42 0.41 0.42WordNet PILSA 0.63 0.62 0.62 0.60 0.60 0.60WordNet MRLSA 0.66 0.65 0.65 0.61 0.59 0.60Encarta lookup 0.65 0.61 0.63 0.61 0.56 0.59Encarta PILSA 0.86 0.81 0.84 0.81 0.74 0.77Encarta MRLSA 0.87 0.82 0.84 0.82 0.74 0.78Encarta PILSA + S2Net + Emebed 0.88 0.87 0.87 0.81 0.80 0.81W&E MRLSA 0.88 0.85 0.87 0.81 0.77 0.79WordNet lookup* 0.93 0.32 0.48 0.95 0.33 0.49WordNet lookup 0.48 0.44 0.46 0.46 0.43 0.44WordNet BPTF 0.63 0.63 0.63 0.63 0.62 0.62Roget lookup* 1.00 0.35 0.52 0.99 0.31 0.47Roget lookup 0.61 0.44 0.51 0.55 0.39 0.45Roget BPTF 0.80 0.80 0.80 0.76 0.75 0.76W&R lookup* 1.00 0.48 0.64 0.98 0.45 0.62W&R lookup 0.62 0.54 0.58 0.59 0.51 0.55W&R BPMF 0.59 0.59 0.59 0.52 0.52 0.52W&R BPTF 0.88 0.88 0.88 0.82 0.82 0.82Table 2: Development and test results on the GRE antonym questions.
*Note: to allow comparison, inlook-up we follow the approach used by (Yih et al., 2012): randomly guess an answer if the target wordis in the vocabulary while none of the choices are.
Asterisk indicates the look-up results without randomguessing.GRE antonym question words.
Further using mor-phology information from WordNet, the coverageachieves 99.9%.4.3 TestsTo answer the GRE questions, we calculateR1ijforword pair (i, j), where i is the target word and jis one of the question?s candidates.
The candidatewith the smallest similarity is then the predictedanswer.
If a target word is missing in the vocabu-lary, that question will not be answered, while if achoice is missing, that choice will be ignored.We first train on a tensor from a subset consist-ing of words with antonym entries, then add allother words using the out-of-vocabulary methoddescribed in Section 3.
During each iteration, ze-ros are randomly added into the first slice to keepthe model from overfitting.
In the meantime, thesecond slice entries is randomly downsampled tomatch the number of non-empty entries in the firstslice.
This ensures each perspective has approxi-mately equal influence on the latent word vectors.We sample the parameters iteratively, andchoose the burn-in period and vector length D ac-cording to the development set.
We choose thevector length D = 40, the burn-in period startingfrom the 30thiterations, then averaging the relat-edness over 200 runs.
The hyper-priors used are?0= 0, ?0= ?
?0= D, ?0= 1 and W0=?W0= I(not tuned).
Note that Yih et al.
(2012) use a vec-tor length of 300, which means our embeddingssave considerable storage space and running time.Our model usually takes less than 30 minutes tomeet the convergence criteria (on a machine withan Intel Xeon E3-1230V2 @ 3.3GHz CPU ).
Incontrast, the MRLSA requires about 3 hours fortensor decomposition (Chang et al., 2013).4.4 ResultsThe results are summarized in Table 2.
We list theresults of previous works (Yih et al., 2012; Changet al., 2013) at the top of the table, where thebest performance is achieved by PILSA on Encartawith further discriminative training and embed-ding.
For comparison, we adopt the standard firstused by (Mohammad et al., 2008), where preci-sion is the number of questions answered correctly2http://www-nlp.stanford.edu/ lmthang/morphoNLM/152820 40 60 80 100 120 140Number of Iterations0.00.51.01.52.02.5RMSEBPMFBPTFFigure 2: Convergence curves of BPMF and BPTFin training the W&R dataset.
MAE is the meanabsolute error over the synonym & antonym slicein the training tensor.divided by the number of questions answered.
Re-call is the number of questions answered correctlydivided by the total number of questions.
BPMF(Bayesian Probabilistic Matrix Factorization) re-sult is derived by only keeping the synonym &antonym slice in our BPTF model.By using Roget?s and WordNet together, ourmethod increases the baseline look-up recall from51% to 82% on the test set, while Yih?s methodincreases the recall of Encarta from 56% to 80%.This state-of-the-art performance is achieved withthe help of a neural network for fine tuning andmultiple schemes of out-of-vocabulary embed-ding, while our method has inherent and straight-forward ?out-of-vocabulary embedding?.
WhileMRLSA, which has this character as well, onlyhas a recall 77% when combining WordNet andEncarta together.WordNet records less antonym relations fornouns, verbs and adverbs, while the GRE antonymquestions has a large coverage of them.
Al-though by extending these antonym relations us-ing the ?indirect antonym?
concept achieves betterlook-up performance than Roget?s, in contrast, theBPTF performance is actually much lower.
Thisimplies Roget?s has better recording of antonymrelations.
Mohammad et al.
(2008) reproted a 23%F-score look-up performance of WordNet whichsupport this claim as well.
Combining WordNetand Roget?s together can improve the look-up per-formance further to 59% precision and 51% recall(still not as good as Encarta look-up).Notably, if we strictly follow our BPTF ap-proach but only use the synonym & antonym slice(i.e.
a matrix factorization model instead of ten-sor factorization model), this single-slice modelBPMF has performance that is only slightly bet-ter than look-up.
Meanwhile Figure 1 shows theconvergence curves of BPMF and BPTF.
BPMFactually has lower MAE after convergence.
Suchbehavior is caused by overfitting of BPMF on thetraining data.
While known entries were recreatedwell, empty entries were not filled correctly.
Onthe other hand, note that although our BPTF modelhas a higher MAE, it has much better performancein answering the GRE antonym questions.
We in-terpret this as the regularization and generalizationeffect from other slice(s).
Instead of focusing onone-slice training data, our model fills the missingentries with the help of inter-slice relations.We also experimented with a linear metriclearning method over the generated word vectors(to learn a metric matrix A to measure the wordrelatedness via VTiAVj) using L-BFGS.
By op-timizing the mean square error on the synonym& antonym slice, we can reduce 8% of the meansquare error on a held out test set, and improvethe F-score by roughly 0.5% (of a single iteration).Although this method doesn?t give a significantimprovement, it is general and has the potentialto boost the performance in other scenarios.5 ConclusionIn this work, we propose a method to map wordsinto a metric space automatically using thesaurusdata, previous vector space models, or other wordrelatedness matrices as input, which is capableof handling out-of-vocabulary words of any par-ticular perspective.
This allows us to derive therelatedness of any given word pair and any per-spective by the embedded word vectors with per-perspective linear transformation.
We evaluatedthe word embeddings with GRE antonym ques-tions, and the result achieves the state-of-the-artperformance.For future works, we will extend the model andits applications in three main directions.
First, inthis model we only use a three-way tensor withtwo slices, while more relations may be able toadd into it directly.
Possible additional perspec-tive slices include LSA for topic relatedness, andcorpus occurrences in engineered or induced se-mantic patterns.Second, we will apply the method to other tasksthat require completing a word relatedness matrix.We evaluated the performance of our model on1529creating / recreating one perspective of word re-latedness: antonymy.
Perhaps using vectors gen-erated from many kinds of perspectives would im-prove the performance on other NLP tasks, suchas term matching employed by textual entailmentand machine translation metrics.Third, if our model does learn the relation be-tween semantic similarities and distributional sim-ilarities, there may be fruitful information con-tained in the vectors Viand Pkthat can be ex-plored.
One straight-forward idea is that the dotproduct of perspective vectors Pk?
Plshould be ameasurement of correlation between perspectives.Also, a straightforward adaptation of our modelhas the potential ability to capture asymmet-ric word relatedness as well, by using a per-perspective matrix instead of vector for the asym-metric slices (i.e.
use VTiAkVjinstead of?Dd=1V(d)iP(d)kV(d)jfor calculating word related-ness, where Akis a square matrix).AcknowledgmentsWe thank Christopher Kedzie for assisting theSemantic Technologies in IBM Watson seminarcourse in which this work has been carried out,and Kai-Wei Chang for giving detailed explana-tion of the evaluation method in his work.ReferencesYoshua Bengio, R?ejean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Re-search, 3:1137?1155.Kai-Wei Chang, Wen-tau Yih, and Christopher Meek.2013.
Multi-relational latent semantic analysis.
InEMNLP.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural language processing (almost) fromscratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Scott C. Deerwester, Susan T. Dumais, George W. Fur-nas, Thomas K. Landauer, and Richard A. Harsh-man.
1990.
Indexing by latent semantic analysis.JASIS, 41(6):391?407.Yoav Goldberg and Jon Orwant.
2013.
A dataset ofsyntactic-ngrams over time from a very large cor-pus of english books.
In Second Joint Conferenceon Lexical and Computational Semantics (* SEM),volume 1, pages 241?247.Zellig Harris.
1954.
Distributional structure.
Word,10(23):146?162.Barbara Ann Kipfer.
2009.
Roget?s 21st Century The-saurus, Third Edition.
Philip Lief Group.Dekang Lin and Shaojun Zhao.
2003.
Identifying syn-onyms among distributionally similar words.
In InProceedings of IJCAI-03, page 14921493.Minh-Thang Luong, Richard Socher, and Christo-pher D. Manning.
2013.
Better word representa-tions with recursive neural networks for morphol-ogy.
In CoNLL, Sofia, Bulgaria.Tomas Mikolov, Wen tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In HLT-NAACL, pages 746?751.
The Association for Computational Linguistics.Tom Mikolov.
2012.
Statistical language modelsbased on neural networks.
Ph.D. thesis, Ph.
D. the-sis, Brno University of Technology.George A. Miller.
1995.
Wordnet: A lexical databasefor english.
Commun.
ACM, 38(11):39?41, Novem-ber.Andriy Mnih and Geoffrey E. Hinton.
2008.
A scal-able hierarchical distributed language model.
InNIPS, pages 1081?1088.Saif Mohammad, Bonnie Dorr, and Graeme Hirst.2008.
Computing word-pair antonymy.
In EMNLP,pages 982?991.
Association for Computational Lin-guistics.Saif Mohammad, Bonnie Dorr, Graeme Hirst, and Pe-ter Turney.
2013.
Computing lexical contrast.
Com-putational Linguistics, 39(3):555?590.Ruslan Salakhutdinov and Andriy Mnih.
2008.Bayesian probabilistic matrix factorization usingmarkov chain monte carlo.
In ICML, pages 880?887.
ACM.Silke Scheible, Sabine Schulte im Walde, and SylviaSpringorum.
2013.
Uncovering distributional dif-ferences between synonyms and antonyms in a wordspace model.
International Joint Conference onNatural Language Processing, pages 489?497.Richard Socher, Cliff C. Lin, Andrew Y. Ng, andChristopher D. Manning.
2011.
Parsing naturalscenes and natural language with recursive neuralnetworks.
In Proceedings of the 28th InternationalConference on Machine Learning (ICML-11), pages129?136.Peter D. Turney, Patrick Pantel, et al.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of artificial intelligence research,37(1):141?188.Peter D. Turney.
2008.
A uniform approach to analo-gies, synonyms, antonyms, and associations.
Col-ing, pages 905?912, August.1530Liang Xiong, Xi Chen, Tzu-Kuo Huang, Jeff G.Schneider, and Jaime G. Carbonell.
2010.
Tempo-ral collaborative filtering with bayesian probabilis-tic tensor factorization.
In SDM, volume 10, pages211?222.
SIAM.Wen-tau Yih, Geoffrey Zweig, and John C. Platt.2012.
Polarity inducing latent semantic analysis.
InEMNLP-CoNLL, pages 1212?1222.
Association forComputational Linguistics.1531
