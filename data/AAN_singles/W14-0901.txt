Proceedings of the 3rd Workshop on Computational Linguistics for Literature (CLfL) @ EACL 2014, pages 1?10,Gothenburg, Sweden, April 27, 2014.c?2014 Association for Computational LinguisticsGenerating Music from LiteratureHannah DavisNew York Universityhannah.davis@nyu.eduSaif M. MohammadNational Research Council Canadasaif.mohammad@nrc-cnrc.gc.caAbstractWe present a system, TransProse, that au-tomatically generates musical pieces fromtext.
TransProse uses known relations be-tween elements of music such as tempoand scale, and the emotions they evoke.Further, it uses a novel mechanism to de-termine sequences of notes that capture theemotional activity in text.
The work hasapplications in information visualization,in creating audio-visual e-books, and indeveloping music apps.1 IntroductionMusic and literature have an intertwined past.
Itis believed that they originated together (Brown,1970), but in time, the two have developed intoseparate art forms that continue to influence eachother.1Music, just as prose, drama, and poetry, isoften used to tell stories.2Opera and ballet tell sto-ries through music and words, but even instrumen-tal music, which is devoid of words, can have apowerful narrative form (Hatten, 1991).
Mahler?sand Beethoven?s symphonies, for example, are re-garded as particularly good examples of narrativeand evocative music (Micznik, 2001).In this paper, for the first time, we present amethod to automatically generate music from liter-ature.
Specifically, we focus on novels and gener-ate music that captures the change in the distribu-tion of emotion words.
We list below some of thebenefits in pursuing this general line of research:?
Creating audio-visual e-books that generatemusic when certain pages are opened?musicthat accentuates the mood conveyed by thetext in those pages.1The term music comes from muses?the nine Greek god-desses of inspiration for literature, science, and arts.2Music is especially close to poetry as songs often tend tobe poems set to music.?
Mapping pieces of literature to musicalpieces according to compatibility of the flowof emotions in text with the audio character-istics of the musical piece.?
Finding songs that capture the emotions indifferent parts of a novel.
This could be use-ful, for example, to allow an app to find andplay songs that are compatible with the moodof the chapter being read.?
Generating music for movie scripts.?
Appropriate music can add to good visualiza-tions to communicate information effectively,quickly, and artfully.Example 1: A tweet stream that is accom-panied by music that captures the aggregatedsentiment towards an entity.Example 2: Displaying the world map whereclicking on a particular region plays musicthat captures the emotions of the tweets ema-nating from there.Given a novel (in an electronically readableform), our system, which we call TransProse, gen-erates simple piano pieces whose notes are depen-dent on the emotion words in the text.
The chal-lenge in composing new music, just as in creat-ing a new story, is the infinite number of choicesand possibilities.
We present a number of map-ping rules to determine various elements of music,such as tempo, major/minor key, etc.
according tothe emotion word density in the text.
We intro-duce a novel method to determine the sequenceof notes (sequences of pitch and duration pairs)to be played as per the change in emotion worddensity in the text.
We also list some guidelineswe followed to make the sequence of notes soundlike music as opposed to a cacophonous cascadeof sounds.Certainly, there is no one right way of capturingthe emotions in text through music, and there is noone right way to produce good music.
Generating1compelling music is an art, and TransProse can beimproved in a number of ways (we list several ad-vancements in the Future Work section).
Our goalwith this project is to present initial ideas in anarea that has not been explored before.This paper does not assume any prior knowl-edge of music theory.
Section 2 presents all theterminology and ideas from music theory neededto understand this paper.
We present related workin Section 3.
Sections 4, 5, 6, and 7 describe oursystem.
In Sections 8 and 9, we present an analysisof the music generated by our system for variouspopular novels.
Finally in Section 10 we presentlimitations and future work.2 MusicIn physical terms, music is a series of possiblyoverlapping sounds, often intended to be pleasingto the listener.
Sound is what our ears perceivewhen there is a mechanical oscillation of pressurein some medium such as air or water.
Thus, differ-ent sounds are associated with different frequen-cies.
In music, a particular frequency is referredto as pitch.
A note has two aspects: pitch andrelative duration.3Examples of relative durationare whole-note, half-note, quarter-note, etc.
Eachsuccessive element in this list is of half the dura-tion as the preceding element.
Consider the exam-ple notes: 400Hz?quarter-note and 760Hz?whole-note.
The first note is the sound correspondingto 400Hz, whereas the second note is the soundcorresponding to 760Hz.
Also, the first note is tobe played for one fourth the duration the secondnote is played.
It is worth repeating at this pointthat note and whole-note do not refer to the sameconcept?the former is a combination of pitch andrelative duration, whereas whole-note (and oth-ers such as quarter-note and half-note) are used tospecify the relative duration of a note.
Notes aredefined in terms of relative duration to allow forthe same melody to be played quickly or slowly.A series of notes can be grouped into a mea-sure (also called bar).
Melody (also called tune) isa sequence of measures (and therefore a sequenceof notes) that creates the musical piece itself.
Forexample, a melody could be defined as 620Hz?half-note, 1200Hz-whole-note, 840Hz-half-note,3Confusingly, note is also commonly used to refer to pitchalone.
To avoid misunderstanding, we will not use note in thatsense in this paper.
However, some statements, such as playthat pitch may seem odd to those familiar with music, whomay be more used to play that note.660Hz?quarter-note, and so on.
There can be onemelody (for example, in the song Mary Had ALittle Lamb) or multiple melodies; they can lastthroughout the piece or appear in specific sections.A challenge for TransProse is to generate appro-priate sequences of notes, given the infinite possi-bilities of pitch, duration, and order of the notes.Tempo is the speed at which the piece shouldbe played.
It is usually indicated by the num-ber of beats per minute.
A beat is a basic unitof time.
A quarter-note is often used as onebeat.
In which case, the tempo can be under-stood simply as the number of quarter-notes perminute.
Consider an example.
Let?s assume itis decided that the example melody specified inthe earlier paragraph is to be played at a tempo of120 quarter-notes per minute.
The total number ofquarter-notes in the initial sequence (620Hz?half-note, 1200Hz?whole-note, 840Hz?half-note, and660Hz?quarter-note) is 2 + 4 + 2 + 1 = 9.
Thus theinitial sequence must be played in 9/120 minutes,or 4.5 seconds.The time signature of a piece indicates twothings: a) how many beats are in a measure, andb) which note duration represents one beat.
It iswritten as one number stacked on another num-ber.
The upper number is the number of beats permeasure, and the lower number is the note dura-tion that represents one beat.
For example, a timesignature of68would mean there are six beats permeasure, and an eighth note represents one beat.One of the most common time signatures is44, andit is referred to as common time.Sounds associated with frequencies that aremultiples or factors of one another (for exam-ple, 440Hz, 880Hz, 1760Hz, etc) are perceivedby the human ear as being consonant and pleas-ing.
This is because the pressure waves associ-ated with these sounds have overlapping peaks andtroughs.
Sets of such frequencies or pitches formpitch classes.
The intervals between successivepitches in a pitch class are called octaves.
On amodern 88-key piano, the keys are laid out in in-creasing order of pitch.
Every successive 12 keyspertain to an octave.
(Thus there are keys pertain-ing to 7 octaves and four additional keys pertain-ing to the eighth octave.)
Further, each of the 12keys split the octave such that the difference in fre-quency between successive keys in an octave is thesame.
Thus the corresponding keys in each octaveform a pitch class.
For example, the keys at posi-2tion 1, 13, 25, 37, and so on, form a pitch class.Similarly keys at position 2, 14, 26, 38, and so on,form another pitch class.
The pitch classes on apiano are given names C, C#, D, D#, E, F, F#, G,G#, A, A#, B.
(The # is pronounced sharp).
Thesame names can also be used to refer to a partic-ular key in an octave.
(In an octave, there existsonly one C, only one D#, and so on.)
The octavesare often referred to by a number.
On a standardpiano, the octaves in increasing order are 0, 1, 2,and so on.
C2 refers to the key in octave 2 that isin the pitch class C.4The difference in frequency between successivepiano keys is called a semitone or Half-Tone (Halffor short).
The interval between two keys sep-arated by exactly one key is called Whole-Tone(Whole for short).
Thus, the interval betweenC and C# is half, whereas the interval betweenC and D is whole.
A scale is any sequence ofpitches ordered by frequency.
A major scale isa sequence of pitches obtained by applying theascending pattern: Whole?Whole?Half?Whole?Whole?Whole?Half.
For example, if one startswith C, then the corresponding C major scale con-sists of C, D (frequency of C + Whole interval), E(frequency of D + Whole interval), F (frequencyof E + Half interval), G, A, B, C. Major scales canbegin with any pitch (not just C), and that pitchis called the base pitch.
A major key is the set ofpitches corresponding to the major scale.
Playingin the key of C major means that one is primarilyplaying the keys (pitches) from the correspondingscale, C major scale (although not necessarily in aparticular order).Minor scales are series of pitches obtainedby applying the ascending pattern: Whole-Half?Whole?Whole?Half?Whole?Whole.
Thus, C mi-nor is C, D, D#, F, G, G#, A#, C. A minor keyis the set of pitches corresponding to the minorscale.
Playing in major keys generally createslighter sounding pieces, whereas playing in minorkeys creates darker sounding pieces.Consonance is how pleasant or stable one per-ceives two pitches played simultaneously (or oneafter the other).
There are many theories on whatmakes two pitches consonant, some of which are4The frequencies of piano keys at a given position acrossoctaves is in log scale.
For example, frequencies of C1,C2,.
.
.
, and so on are in log scale.
The perception of sound(frequency) in the human ear is also roughly logarithmic.Also, the frequency 440Hz (mentioned above) is A4 and itis the customary tuning standard for musical pitch.culturally dependent.
The most common notion(attributed to Pythagoras) is that the simpler theratio between the two frequencies, the more con-sonant they are (Roederer, 2008; Tenney, 1988).Given a particular scale, some have argued thatthe order of the pitches in decreasing consonanceis as follows: 1st, 5th, 3rd, 6th, 2nd, 4th, and 7th(Perricone, 2000).
Thus for the C major?C (thebase pitch, or 1st), D (2nd) , E (3rd), F (4th), G(5th) , A (6th) , B (7th)?the order of the pitchesin decreasing consonance is?C, G, E, A, D, F, B.Similarly, for C minor?C (the base pitch, or 1st),D (2nd), D# (3rd), F (4th), G (5th), G# (6th), A#(7th)?the order of pitches in decreasing conso-nance is?C, G, D#, G#, D, F, A#.
We will usethese orders in TransProse to generate more dis-cordant and unstable pitches to reflect higher emo-tion word densities in the novels.3 Related WorkThis work is related to automatic sentiment andemotion analysis of text (computational linguis-tics), the generation of music (music theory), aswell as the perception of music (psychology).Sentiment analysis techniques aim to determinethe evaluative nature of text?positive, negative,or neutral.
They have been applied to many dif-ferent kinds of texts including customer reviews(Pang and Lee, 2008), newspaper headlines (Bel-legarda, 2010), emails (Liu et al., 2003; Moham-mad and Yang, 2011), blogs (Genereux and Evans,2006; Mihalcea and Liu, 2006), and tweets (Pakand Paroubek, 2010; Agarwal et al., 2011; Thel-wall et al., 2011; Brody and Diakopoulos, 2011;Aisopos et al., 2012; Bakliwal et al., 2012).
Sur-veys by Pang and Lee (2008) and Liu and Zhang(2012) give a summary of many of these ap-proaches.
Emotion analysis and affective comput-ing involve the detection of emotions such as joy,anger, sadness, and anticipation in text.
A num-ber of approaches for emotion analysis have beenproposed in recent years (Boucouvalas, 2002; Zheand Boucouvalas, 2002; Aman and Szpakowicz,2007; Neviarouskaya et al., 2009; Kim et al.,2009; Bollen et al., 2009; Tumasjan et al., 2010).Text-to-speech synthesis employs emotion detec-tion to produce speech consistent with the emo-tions in the text (Iida et al., 2000; Pierre-Yves,2003; Schr?oder, 2009).
See surveys by Picard(2000) and Tao and Tan (2005) for a broader re-view of the research in this area.3Some prior empirical sentiment analysis workfocuses specifically on literary texts.
Alm andSproat (2005) analyzed twenty two BrothersGrimm fairy tales to show that fairy tales oftenbegan with a neutral sentence and ended with ahappy sentence.
Mohammad (2012) visualized theemotion word densities in novels and fairy tales.Volkova et al.
(2010) study human annotation ofemotions in fairy tales.
However, there exists nowork connecting automatic detection of sentimentwith the automatic generation of music.Methods for both sentiment and emotion analy-sis often rely on lexicons of words associated withvarious affect categories such as positive and neg-ative sentiment, and emotions such as joy, sad-ness, fear, and anger.
The WordNet Affect Lexi-con (WAL) (Strapparava and Valitutti, 2004) has afew hundred words annotated with associations toa number of affect categories including the six Ek-man emotions (joy, sadness, anger, fear, disgust,and surprise).5The NRC Emotion Lexicon, com-piled by Mohammad and Turney (2010; 2013),has annotations for about 14000 words with eightemotions (six of Ekman, trust, and anticipation).6We use this lexicon in our project.Automatic or semi-automatic generation of mu-sic through computer algorithms was first popular-ized by Brian Eno (who coined the term generativemusic) and David Cope (Cope, 1996).
Lerdahl andJackendoff (1983) authored a seminal book on thegenerative theory of music.
Their work greatly in-fluenced future work in automatic generation ofmusic such as that of Collins (2008) and Biles(1994).
However, these pieces did not attempt toexplicitly capture emotions.Dowling and Harwood (1986) showed that vastamounts of information are processed when listen-ing to music, and that the most expressive qualitythat one perceives is emotion.
The communicationof emotions in non-verbal utterances and in musicshow how emotions in music have an evolutionarybasis (Rousseau, 2009; Spencer, 1857; Juslin andLaukka, 2003).
There are many known associa-tions between music and emotions:?
Loudness: Loud music is associated with in-tensity, power, and anger, whereas soft musicis associated with sadness or fear (Gabriels-son and Lindstr?om, 2001).5http://wndomains.fbk.eu/wnaffect.html6http://www.purl.org/net/NRCemotionlexicon?
Melody: A sequence of consonant notes isassociated with joy and calm, whereas a se-quence of disconsonant notes is associatedwith excitement, anger, or unpleasantness(Gabrielsson and Lindstr?om, 2001).?
Major and Minor Keys: Major keys are as-sociated with happiness, whereas minor keysare associated with sadness (Hunter et al.,2010; Hunter et al., 2008; Ali and Peynirci-olu, 2010; Gabrielsson and Lindstr?om, 2001;Webster and Weir, 2005).?
Tempo: Fast tempo is associated with hap-piness or excitement (Hunter et al., 2010;Hunter et al., 2008; Ali and Peynirciolu,2010; Gabrielsson and Lindstr?om, 2001;Webster and Weir, 2005).Studies have shown that even though many ofthe associations mentioned above are largely uni-versal, one?s own culture also influences the per-ception of music (Morrison and Demorest, 2009;Balkwill and Thompson, 1999).4 Our System: TransProseOur system, which we call TransProse, generatesmusic according to the use of emotion words ina given novel.
It does so in three steps: First, itanalyzes the input text and generates an emotionprofile.
The emotion profile is simply a collectionof various statistics about the presence of emotionwords in the text.
Second, based on the emotionprofile of the text, the system generates values fortempo, scale, octave, notes, and the sequence ofnotes for multiple melodies.
Finally, these valuesare provided to JFugue, an open-source Java APIfor programming music, that generates the appro-priate audio file.
In the sections ahead, we de-scribe the three steps in more detail.5 Calculating Emotion Word DensitiesGiven a novel in electronic form, we use theNRC Emotion Lexicon (Mohammad and Turney,2010; Mohammad and Turney, 2013) to identifythe number of words in each chapter that are asso-ciated with an affect category.
We generate countsfor eight emotions (anticipation, anger, joy, fear,disgust, sadness, surprise, and trust) as well asfor positive and negative sentiment.
We partitionthe novel into four sections representing the begin-ning, early middle, late middle, and end.
Each sec-tion is further partitioned into four sub-sections.4The number of sections, the number of subsec-tions per section, and the number of notes gener-ated for each of the subsections together determinethe total number of notes generated for the novel.Even though we set the number of sections andnumber of sub-sections to four each, these settingscan be varied, especially for significantly longer orshorter pieces of text.For each section and for each sub-section the ra-tio of emotion words to the total number of wordsis calculated.
We will refer to this ratio as the over-all emotions density.
We also calculate densitiesof particular emotions, for example, the joy den-sity, anger density, etc.
As described in the sectionahead, the emotion densities are used to generatesequences of notes for each of the subsections.6 Generating Music SpecificationsEach of the pieces presented in this paper arefor the piano with three simultaneous, but differ-ent, melodies coming together to form the musicalpiece.
Two melodies sounded too thin (simple),and four or more melodies sounded less cohesive.6.1 Major and Minor KeysMajor keys generally create a more positive atmo-sphere in musical pieces, whereas minor keys tendto produce pieces with more negative undertones(Hunter et al., 2010; Hunter et al., 2008; Ali andPeynirciolu, 2010; Gabrielsson and Lindstr?om,2001; Webster and Weir, 2005).
No consensushas been reached on whether particular keys them-selves (for example, A minor vs E minor) evokedifferent emotions, and if so, what emotions areevoked by which keys.
For this reason, the pro-totype of Transprose does not consider differentkeys; the chosen key for the produced musicalpieces is limited to either C major or C minor.
(Cmajor was chosen because it is a popular choicewhen teaching people music.
It is simple becauseit does not have any sharps.
C minor was chosenas it is the minor counterpart of C major.
)Whether the piece is major or minor is de-termined by the ratio of the number of positivewords to the number of negative words in the en-tire novel.
If the ratio is higher than 1, C majoris used, that is, only pitches pertaining to C majorare played.
If the ratio is 1 or lower, C minor isused.Experimenting with keys other than C majorand C minor is of interest for future work.
Further-more, the eventual intent is to include mid-piecekey changes for added effect.
For example, chang-ing the key from C major to A minor when the plotsuddenly turns sad.
The process of changing keyis called modulation.
Certain transitions such asmoving from C major to A minor are commonlyused and musically interesting.6.2 MelodiesWe use three melodies to capture the change inemotion word usage in the text.
The notes inone melody are based on the overall emotion worddensity (the emotion words associated with any ofthe eight emotions in the NRC Emotion Lexicon).We will refer to this melody, which is intended tocapture the overarching emotional movement, asmelody o or Mo(the ?o?
stands for overall emo-tion).
The notes in the two other melodies, melodye1 (Me1) and melody e2 (Me2), are determinedby the most prevalent and second most prevalentemotions in the text, respectively.
Precisely howthe notes are determined is described in the nextsub-section, but first we describe how the octavesof the notes is determined.The octave of melody o is proportional to thedifference between the joy and sadness densitiesof the novel.
We will refer to this difference by JS.We calculated the lowest density difference (JSmin)and highest JS score (JSmax) in a collection of nov-els.
For a novel with density difference, JS, thescore is linearly mapped to octave 4, 5, or 6 of astandard piano, as per the formula shown below:Oct(Mo) = 4 + r((JS?
JSmin) ?
(6?
4)JSmax?
JSmin) (1)The function r rounds the expression to the closestinteger.
Thus scores closer to JSminare mappedto octave 4, scores closer to JSmaxare mapped tooctave 6, and those in the middle are mapped tooctave 5.The octave of Me1is calculated as follows:Oct(Me1) =??????????
?Oct(Mo) + 1, if e1 is joy or trustOct(Mo)?
1, if e1 is anger, fear,sadness, or disgustOct(Mo), otherwise(2)That is, Me1is set to:?
an octave higher than the octave of Moif e1is a positive emotion,5?
an octave lower than the octave of Moif e1isa negative emotion,?
the same octave as that of Moif e1is surpriseor anticipation.Recall that higher octaves evoke a sense of positiv-ity, whereas lower octaves evoke a sense of nega-tivity.
The octave of Me2is calculated exactly asthat of Me1, except that it is based on the secondmost prevalent emotion (and not the most preva-lent emotion) in the text.6.3 Structure and NotesAs mentioned earlier, TransProse generates threemelodies that together make the musical piece fora novel.
The method for generating each melodyis the same, with the exception that the threemelodies (Mo, Me1, and Me2) are based on theoverall emotion density, predominant emotion?sdensity, and second most dominant emotion?s den-sity, respectively.
We describe below the methodcommon for each melody, and use emotion worddensity as a stand in for the appropriate density.Each melody is made up of four sections, repre-senting four sections of the novel (the beginning,early middle, late middle, and end).In turn, eachsection is represented by four measures.
Thus eachmeasure corresponds to a quarter of a section (asub-section).
A measure, as defined earlier, is aseries of notes.
The number of notes, the pitch ofeach note, and the relative duration of each noteare determined such that they reflect the emotionword densities in the corresponding part of thenovel.Number of Notes: In our implementation, wedecided to contain the possible note durationsto whole notes, half notes, quarter notes, eighthnotes, and sixteenth notes.
A relatively high emo-tion density is represented by many notes, whereasa relatively low emotion density is represented byfewer notes.
We first split the interval between themaximum and minimum emotion density for thenovel into five equal parts (five being the num-ber of note duration choices ?
whole, half, quar-ter, eighth, or sixteenth).
Emotion densities thatfall in the lowest interval are mapped to a singlewhole note.
Emotion densities in the next intervalare mapped to two half-notes.
The next intervalis mapped to four quarter-notes.
And so on, untilthe densities in the last interval are mapped to six-teen sixteenth-notes (1/16th).
The result is shorternotes during periods of higher emotional activity(with shorter notes making the piece sound moreactive), and longer notes during periods of loweremotional activity.Pitch: If the number of notes for a measureis n, then the corresponding sub-section is parti-tioned into n equal parts and the pitch for eachnote is based on the emotion density of the cor-responding sub-section.
Lower emotion densitiesare mapped to more consonant pitches in the key(C major or C minor), whereas higher emotiondensities are mapped to less consonant pitches inthe same scale.
For example, if the melody is inthe key of C major, then the lowest to highest emo-tion densities are mapped linearly to the pitches C,G, E, A, D, F, B.
Thus, a low emotion value wouldcreate a pitch that is more consonant and a highemotion value would create a pitch that is moredissonant (more interesting and unusual).Repetition: Once the four measures of a sectionare played, the same four measures are repeatedin order to create a more structured and melodicfeeling.
Without the repetition, the piece soundsless cohesive.6.4 TempoWe use a44time signature (common time) be-cause it is one of the most popular time signatures.Thus each measure (sub-section) has 4 beats.
Wedetermined tempo (beats per minute) by first de-termining how active the target novel is.
Eachof the eight basic emotions is assigned to be ei-ther active, passive, or neutral.
In TransProse, thetempo is proportional to the activity score, whichwe define to be the difference between the aver-age density of the active emotions (anger and joy)and the average density of the passive emotions(sadness).
The other five emotions (anticipation,disgust, fear, surprise, and trust) were consideredambiguous or neutral, and did not influence thetempo.We subjectively identified upper and lowerbounds for the possible tempo values to be 180 and40 beats/minute, respectively.
We determined ac-tivity scores for a collection of novels, and identi-fied the highest activity score (Actmax) and the low-est activity score (Actmin).
For a novel whose ac-tivity score was Act, we determined tempo as perthe formula shown below:tempo = 40 +(Act?
Actmin) ?
(180?
40)Actmax?
Actmin(3)Thus, high activity scores were represented by6tempo values closer to 180 and lower activityscores were represented by tempo values closer to40.
The lowest activity score in our collection oftexts, Actmin, was -0.002 whereas the highest ac-tivity score, Actmax, was 0.017.7 Converting Specifications to MusicJFugue is an open-source Java API that helps cre-ate generative music.7It allows the user to easilyexperiment with different notes, instruments, oc-taves, note durations, etc within a Java program.JFugue requires a line of specifically-formattedtext that describes the melodies in order to playthem.
The initial portion of the string of JFugue to-kens for the novel Peter Pan is shown below.
Thestring conveys the overall information of the pieceas well as the first eight measures (or one section)for each of the three melodies (or voices).KCmaj X[VOLUME]=16383 V0 T180A6/0.25 D6/0.125 F6/0.25 B6/0.25B6/0.125 B6/0.25 B6/0.25...K stands for key and Cmaj stands for C major.This indicates that the rest of the piece will be inthe key of C major.
The second token controls thevolume, which in this example is at the loudestvalue (16383).
V0 stands for the first melody (orvoice).
The tokens with the letter T indicate thetempo, which in the case of this example is 180beats per minute.The tokens that follow indicate the notes of themelody.
The letter is the pitch class of the note,and the number immediately following it is theoctave.
The number following the slash char-acter indicates the duration of the note.
(0.125is an eighth-note (1/8th), 0.25 is a quarter note,0.5 is a half note, and 1.0 is a whole note.)
Weused JFugue to convert the specifications of themelodies into music.
JFugue saves the pieces asa midi files, which we converted to MP3 format.88 Case StudiesWe created musical pieces for several popularnovels through TransProse.
These pieces areavailable at: http://transprose.weebly.com/final-pieces.html.
Since these novels are7http://www.jfugue.org8The MP3 format uses a lossy data compression, but theresulting files are significantly smaller in size.
Further, awider array of music players support the MP3 format.likely to have been read by many people, the read-ers can compare their understanding of the storywith the music generated by TransProse.
Table 1presents details of some of these novels.8.1 Overall ToneTransProse captures the overall positive or nega-tive tone of the novel by assigning an either majoror minor key to the piece.
Peter Pan and Anneof Green Gables, novels with overall happy anduplifting moods, created pieces in the major key.On the other hand, novels such as Heart of Dark-ness, A Clockwork Orange, and The Road, withdark themes, created pieces in the minor key.
Theeffect of this is pieces that from the start have amood that aligns with the basic mood of the novelthey are based on.8.2 Overall Happiness and SadnessThe densities of happiness and sadness in a novelare represented in the baseline octave of a piece.This representation instantly conveys whether thenovel has a markedly happy or sad mood.
Theoverall high happiness densities in Peter Pan andAnne of Green Gables create pieces in an octaveabove the average, resulting in higher tones and alighter overall mood.
Similarly, the overall highsadness densities in The Road and Heart of Dark-ness result in pieces an octave lower than the aver-age, and a darker overall tone to the music.
Nov-els, such as A Clockwork Orange, and The LittlePrince, where happiness and sadness are not dra-matically higher or lower than the average novelremain at the average octave, allowing for the cre-ation of a more nuanced piece.8.3 Activeness of the NovelNovels with lots of active emotion words, suchas Peter Pan, Anne of Green Gables, Lord of theFlies, and A Clockwork Orange, generate fast-paced pieces with tempos over 170 beats perminute.
On the other hand, The Road, which hasrelatively few active emotion words is rather slow(a tempo of 42 beats per minute).8.4 Primary EmotionsThe top two emotions of a novel inform two of thethree melodies in a piece (Me1and Me2).
Recallthat if the melody is based on a positive emotion, itwill be an octave higher than the octave of Mo, andif it is based on a negative emotion, it will be an oc-tave lower.
For novels where the top two emotions7Table 1: Emotion and audio features of a few popular novels that were processed by TransProse.
Themusical pieces are available at: http://transprose.weebly.com/final-pieces.html.Book Title Emotion 1 Emotion 2 Octave Tempo Pos/Neg Key Activity Joy-SadA Clockwork Orange Fear Sadness 5 171 Negative C Minor 0.009 -0.0007Alice in Wonderland Trust Fear 5 150 Positive C Major 0.007 -0.0002Anne of Green Gables Joy Trust 6 180 Positive C Major 0.010 0.0080Heart of Darkness Fear Sadness 4 122 Negative C Minor 0.005 -0.0060Little Prince, The Trust Joy 5 133 Positive C Major 0.006 0.0028Lord of The Flies Fear Sadness 4 151 Negative C Minor 0.008 -0.0053Peter Pan Trust Joy 6 180 Positive C Major 0.010 0.0040Road, The Sadness Fear 4 42 Negative C Minor -0.002 -0.0080To Kill a Mockingbird Trust Fear 5 132 Positive C Major 0.006 -0.0013are both positive, such as Anne of Green Gables(trust and joy), the pieces sound especially lightand joyful.
For novels where the top two emotionsare both negative, such as The Road (sadness andfear), the pieces sound especially dark.8.5 Emotional ActivityUnlike the overall pace of the novel, individualsegments of activity were also identified in thepieces through the number and duration of notes(with more and shorter notes indicating higheremotion densities).
This can be especially heardin the third section of A Clockwork Orange, the fi-nal section of The Adventures of Sherlock Holmes,the second section of To Kill a Mockingbird, andthe final section of Lord of the Flies.
In A Clock-work Orange, the main portion of the piece ischaotic and eventful, likely as the main charac-ters cause havoc; at the end of the novel (as themain character undergoes therapy) the piece dra-matically changes and becomes structured.
Sim-ilarly, in Heart of Darkness, the piece starts outonly playing a few notes; as the tension in thenovel builds, the number of notes increases andtheir durations decrease.9 Comparing Alternative ChoicesWe examine choices made in TransProse by com-paring musical pieces generated with different al-ternatives.
These audio clips are available here:http://transprose.weebly.com/clips.html.Pieces with two melodies (based on overallemotion density and the predominant emotion?sdensity) and pieces based on four melodies (basedon the top three emotions and the overall emotiondensity) were generated and uploaded in the clipswebpage.
Observe that with only two melodies,the pieces tend to sound thin, whereas with fourmelodies the pieces sound less cohesive and some-times chaotic.
The effect of increasing and de-creasing the total number of sections and sub-sections is also presented.
Additionally, the web-page displays pieces with tempos and octaves be-yond the limits chosen in TransProse.
We alsoshow other variations such as pieces for relativelypositive novels generated in C minor (instead ofC major).
These alternatives are not necessarilyincorrect, but they tend to often be less effective.10 Limitations and Future workWe presented a system, TransProse, that gener-ates music according to the use of emotion wordsin a given piece of text.
A number of avenuesfor future work exist such as exploring the use ofmid-piece key changes and intentional harmonyand discord between the melodies.
We will fur-ther explore ways to capture activity in music.
Forexample, an automatically generated activity lex-icon (built using the method proposed by Turneyand Littman (2003)) can be used to identify por-tions of text where the characters are relativelyactive (fighting, dancing, conspiring, etc) and ar-eas where they are relatively passive (calm, inca-pacitated, sad, etc).
One can even capture non-emotional features of the text in music.
For ex-ample, recurring characters or locations in a novelcould be indicated by recurring motifs.
We willconduct human evaluations asking people to judgevarious aspects of the generated music such as thequality of music and the amount and type of emo-tion evoked by the music.
We will also evaluatethe impact of textual features such as the length ofthe novel and the style of writing on the generatedmusic.
Work on capturing note models (analogousto language models) from existing pieces of musicand using them to improve the music generated byTransProse seems especially promising.8References[Agarwal et al.2011] Apoorv Agarwal, Boyi Xie, IliaVovsha, Owen Rambow, and Rebecca Passonneau.2011.
Sentiment analysis of twitter data.
In Pro-ceedings of the Workshop on Languages in SocialMedia, LSM ?11, pages 30?38, Portland, Oregon.
[Aisopos et al.2012] Fotis Aisopos, George Papadakis,Konstantinos Tserpes, and Theodora Varvarigou.2012.
Textual and contextual patterns for sentimentanalysis over microblogs.
In Proceedings of the21st International Conference on World Wide WebCompanion, WWW ?12 Companion, pages 453?454, New York, NY, USA.
[Ali and Peynirciolu2010] S Omar Ali and Zehra FPeynirciolu.
2010.
Intensity of emotions con-veyed and elicited by familiar and unfamiliar mu-sic.
Music Perception: An Interdisciplinary Journal,27(3):177?182.
[Alm and Sproat2005] Cecilia O. Alm and RichardSproat, 2005.
Emotional sequencing and develop-ment in fairy tales, pages 668?674.
Springer.
[Aman and Szpakowicz2007] Saima Aman and StanSzpakowicz.
2007.
Identifying expressions of emo-tion in text.
In Vclav Matou?sek and Pavel Mautner,editors, Text, Speech and Dialogue, volume 4629 ofLecture Notes in Computer Science, pages 196?205.Springer Berlin / Heidelberg.
[Bakliwal et al.2012] Akshat Bakliwal, Piyush Arora,Senthil Madhappan, Nikhil Kapre, Mukesh Singh,and Vasudeva Varma.
2012.
Mining sentimentsfrom tweets.
In Proceedings of the 3rd Workshop onComputational Approaches to Subjectivity and Sen-timent Analysis, WASSA ?12, pages 11?18, Jeju, Re-public of Korea.
[Balkwill and Thompson1999] Laura-Lee Balkwill andWilliam Forde Thompson.
1999.
A cross-culturalinvestigation of the perception of emotion in music:Psychophysical and cultural cues.
Music perception,pages 43?64.
[Bellegarda2010] Jerome Bellegarda.
2010.
Emotionanalysis using latent affective folding and embed-ding.
In Proceedings of the NAACL-HLT 2010Workshop on Computational Approaches to Analy-sis and Generation of Emotion in Text, Los Angeles,California.
[Biles1994] John Biles.
1994.
Genjam: A genetic al-gorithm for generating jazz solos.
pages 131?137.
[Bollen et al.2009] Johan Bollen, Alberto Pepe, andHuina Mao.
2009.
Modeling public mood and emo-tion: Twitter sentiment and socio-economic phe-nomena.
CoRR.
[Boucouvalas2002] Anthony C. Boucouvalas.
2002.Real time text-to-emotion engine for expressive in-ternet communication.
Emerging Communication:Studies on New Technologies and Practices in Com-munication, 5:305?318.
[Brody and Diakopoulos2011] Samuel Brodyand Nicholas Diakopoulos.
2011.Cooooooooooooooollllllllllllll!!!!!!!!!!!!!!
: us-ing word lengthening to detect sentiment inmicroblogs.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 562?570, Stroudsburg, PA,USA.
Association for Computational Linguistics.
[Brown1970] Calvin S Brown.
1970.
The relations be-tween music and literature as a field of study.
Com-parative Literature, 22(2):97?107.
[Collins2008] Nick Collins.
2008.
The analysisof generative music programs.
Organised Sound,13(3):237?248.
[Cope1996] David Cope.
1996.
Experiments in musi-cal intelligence, volume 12.
AR Editions Madison,WI.
[Dowling and Harwood1986] W Jay Dowling andDane L Harwood.
1986.
Music cognition, volume19986.
Academic Press New York.
[Gabrielsson and Lindstr?om2001] Alf Gabrielsson andErik Lindstr?om.
2001.
The influence of musicalstructure on emotional expression.
[Genereux and Evans2006] Michel Genereux andRoger P. Evans.
2006.
Distinguishing affectivestates in weblogs.
In Proceedings of the AAAISpring Symposium on Computational Approachesto Analysing Weblogs, pages 27?29, Stanford,California.
[Hatten1991] Robert Hatten.
1991.
On narrativity inmusic: expressive genres and levels of discourse inbeethoven.
[Hunter et al.2008] Patrick G Hunter, E Glenn Schel-lenberg, and Ulrich Schimmack.
2008.
Mixedaffective responses to music with conflicting cues.Cognition & Emotion, 22(2):327?352.
[Hunter et al.2010] Patrick G Hunter, E Glenn Schel-lenberg, and Ulrich Schimmack.
2010.
Feelingsand perceptions of happiness and sadness inducedby music: Similarities, differences, and mixed emo-tions.
Psychology of Aesthetics, Creativity, and theArts, 4(1):47.
[Iida et al.2000] Akemi Iida, Nick Campbell, SoichiroIga, Fumito Higuchi, and Michiaki Yasumura.
2000.A speech synthesis system with emotion for assist-ing communication.
In ISCA Tutorial and ResearchWorkshop (ITRW) on Speech and Emotion.
[Juslin and Laukka2003] Patrik N Juslin and PetriLaukka.
2003.
Communication of emotions invocal expression and music performance: Differ-ent channels, same code?
Psychological bulletin,129(5):770.
[Kim et al.2009] Elsa Kim, Sam Gilbert, Michael J. Ed-wards, and Erhardt Graeff.
2009.
Detecting sadnessin 140 characters: Sentiment analysis of mourningmichael jackson on twitter.
[Lerdahl and Jackendoff1983] Fred Lerdahl and Ray SJackendoff.
1983.
A generative theory of tonal mu-sic.
MIT press.
[Liu and Zhang2012] Bing Liu and Lei Zhang.
2012.A survey of opinion mining and sentiment analysis.In Charu C. Aggarwal and ChengXiang Zhai, edi-tors, Mining Text Data, pages 415?463.
Springer.9[Liu et al.2003] Hugo Liu, Henry Lieberman, and TedSelker.
2003.
A model of textual affect sensing us-ing real-world knowledge.
In Proceedings of the 8thInternational Conference on Intelligent User Inter-faces, pages 125?132, New York, NY.
ACM.
[Micznik2001] Vera Micznik.
2001.
Music and nar-rative revisited: degrees of narrativity in beethovenand mahler.
Journal of the Royal Musical Associa-tion, 126(2):193?249.
[Mihalcea and Liu2006] Rada Mihalcea and Hugo Liu.2006.
A corpus-based approach to finding happi-ness.
In Proceedings of the AAAI Spring Sympo-sium on Computational Approaches to AnalysingWeblogs, pages 139?144.
AAAI Press.
[Mohammad and Turney2010] Saif M. Mohammad andPeter D. Turney.
2010.
Emotions evoked by com-mon words and phrases: Using Mechanical Turkto create an emotion lexicon.
In Proceedings ofthe NAACL-HLT Workshop on Computational Ap-proaches to Analysis and Generation of Emotion inText, LA, California.
[Mohammad and Turney2013] Saif M. Mohammad andPeter D. Turney.
2013.
Crowdsourcing a word-emotion association lexicon.
29(3):436?465.
[Mohammad and Yang2011] Saif M. Mohammad andTony (Wenda) Yang.
2011.
Tracking sentiment inmail: How genders differ on emotional axes.
In Pro-ceedings of the ACL Workshop on ComputationalApproaches to Subjectivity and Sentiment Analysis,WASSA ?11, Portland, OR, USA.
[Mohammad2012] Saif M. Mohammad.
2012.
Fromonce upon a time to happily ever after: Trackingemotions in mail and books.
Decision Support Sys-tems, 53(4):730?741.
[Morrison and Demorest2009] Steven J Morrison andSteven M Demorest.
2009.
Cultural constraints onmusic perception and cognition.
Progress in brainresearch, 178:67?77.
[Neviarouskaya et al.2009] Alena Neviarouskaya, Hel-mut Prendinger, and Mitsuru Ishizuka.
2009.
Com-positionality principle in recognition of fine-grainedemotions from text.
In Proceedings of the Proceed-ings of the Third International Conference on We-blogs and Social Media (ICWSM-09), pages 278?281, San Jose, California.
[Pak and Paroubek2010] Alexander Pak and PatrickParoubek.
2010.
Twitter as a corpus for senti-ment analysis and opinion mining.
In Proceed-ings of the 7th Conference on International Lan-guage Resources and Evaluation, LREC ?10, Val-letta, Malta, May.
European Language ResourcesAssociation (ELRA).
[Pang and Lee2008] Bo Pang and Lillian Lee.
2008.Opinion mining and sentiment analysis.
Founda-tions and Trends in IR, 2(1?2):1?135.
[Perricone2000] J. Perricone.
2000.
Melody in Song-writing: Tools and Techniques for Writing HitSongs.
Berklee guide.
Berklee Press.
[Picard2000] Rosalind W Picard.
2000.
Affective com-puting.
MIT press.
[Pierre-Yves2003] Oudeyer Pierre-Yves.
2003.
Theproduction and recognition of emotions in speech:features and algorithms.
International Journal ofHuman-Computer Studies, 59(1):157?183.
[Roederer2008] Juan G Roederer.
2008.
The physicsand psychophysics of music: an introduction.Springer Publishing Company, Incorporated.
[Rousseau2009] Jean-Jacques Rousseau.
2009.
Essayon the origin of languages and writings related tomusic, volume 7.
UPNE.
[Schr?oder2009] Marc Schr?oder.
2009.
Expressivespeech synthesis: Past, present, and possible futures.In Affective information processing, pages 111?126.Springer.
[Spencer1857] Herbert Spencer.
1857.
The origin andfunction of music.
Frasers Magazine, 56:396?408.
[Strapparava and Valitutti2004] Carlo Strapparava andAlessandro Valitutti.
2004.
WordNet-Affect: AnAffective Extension of WordNet.
In Proceedings ofthe 4th International Conference on Language Re-sources and Evaluation (LREC-2004), pages 1083?1086, Lisbon, Portugal.
[Tao and Tan2005] Jianhua Tao and Tieniu Tan.
2005.Affective computing: A review.
In Affective com-puting and intelligent interaction, pages 981?995.Springer.
[Tenney1988] James Tenney.
1988.
A history of conso-nance and dissonance.
Excelsior Music PublishingCompany New York.
[Thelwall et al.2011] Mike Thelwall, Kevan Buckley,and Georgios Paltoglou.
2011.
Sentiment in Twitterevents.
Journal of the American Society for Infor-mation Science and Technology, 62(2):406?418.
[Tumasjan et al.2010] Andranik Tumasjan, Timm OSprenger, Philipp G Sandner, and Isabell M Welpe.2010.
Predicting elections with twitter : What 140characters reveal about political sentiment.
WordJournal Of The International Linguistic Association,pages 178?185.
[Turney and Littman2003] Peter Turney and Michael LLittman.
2003.
Measuring praise and criticism:Inference of semantic orientation from association.ACM Transactions on Information Systems, 21(4).
[Volkova et al.2010] Ekaterina P Volkova, Betty JMohler, Detmar Meurers, Dale Gerdemann, andHeinrich H B?ulthoff.
2010.
Emotional perception offairy tales: Achieving agreement in emotion annota-tion of text.
In Proceedings of the NAACL HLT 2010Workshop on Computational Approaches to Analysisand Generation of Emotion in Text, pages 98?106.Association for Computational Linguistics.
[Webster and Weir2005] Gregory D Webster andCatherine G Weir.
2005.
Emotional responses tomusic: Interactive effects of mode, texture, andtempo.
Motivation and Emotion, 29(1):19?39.
[Zhe and Boucouvalas2002] Xu Zhe and A Boucou-valas, 2002.
Text-to-Emotion Engine for Real TimeInternet CommunicationText-to-Emotion Engine forReal Time Internet Communication, pages 164?168.10
