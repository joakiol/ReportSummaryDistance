Automatically Inducing Ontologies from CorporaInderjeet ManiDepartment of LinguisticsGeorgetown University, ICC 45237th and O Sts, NWWashington, DC 20057, USAim5@georgetown.eduKen Samuel, Kris Concepcion andDavid VogelThe MITRE Corporation7515 Colshire DriveMcLean, VA 22102, USA{samuel, kjc9, dvogel}@mitre.orgAbstractThe emergence of vast quantities of on-lineinformation has raised the importance of methodsfor automatic cataloguing of information in avariety of domains, including electronic commerceand bioinformatics.
Ontologies can play a criticalrole in such cataloguing.
In this paper, we describea system that automatically induces an ontologyfrom any large on-line text collection in a specificdomain.
The ontology that is induced consists ofdomain concepts, related by kind-of and part-oflinks.
To achieve domain-independence, we use acombination of relatively shallow methods alongwith any available repositories of applicablebackground knowledge.
We describe ourevaluation experiences using these methods, andprovide examples of induced structures.1 IntroductionThe emergence of vast quantities of on-lineinformation has raised the importance of methodsfor automatic cataloguing of information in avariety of domains, including electronic commerceand bioinformatics.
Ontologies1 can play a criticalrole in such cataloguing.
In bioinformatics, forexample, there is growing recognition thatcommon ontologies, e.g., the Gene Ontology2, arecritical to interoperation and integration ofbiological data, including both structured data asfound in protein databases, as well as unstructureddata, as found in on-line biomedical literature.Constructing an ontology is an extremelylaborious effort.
Even with some reuse of ?core?knowledge from an Upper Model (Cohen et al1999), the task of creating an ontology for aparticular domain and task has a high cost,incurred for each new domain.
Tools that couldautomate, or semi-automate, the construction of1 This research was supported by the National ScienceFoundation (ITR-0205470).2 www.geneontology.orgontologies for different domains coulddramatically reduce the knowledge creation cost.One approach to developing such tools is to relyon information implicit in collections of on-linetext in a particular domain.
If it were possible toautomatically extract terms and their semanticrelations from the text corpus, the ontologydeveloper could build on that knowledge, revisingit, as needed, etc.
This would be more cost-effective than having a human develop theontology from scratch.Our approach is inspired by research on topic-focused multi-document summarization of largetext collections, where there is a need tocharacterize the collection content succinctly in ahierarchy of topic terms and their relationships.Current approaches to multi-documentsummarization combine linguistic analysis, corpusstatistics, and the use of background semanticknowledge from generic thesauri such as WordNetto infer semantic information about a person.
Inextending such approaches to ontology induction,the hypothesis is that similar hybrid approachescan be used to identify technical terms in adomain-specific corpus and infer semanticrelationships among them.In this paper, we describe a system thatautomatically induces an ontology from any largeon-line text collection in a specific domain, tosupport cataloguing in information access and dataintegration tasks.
The induced ontology consists ofdomain concepts related by kind-of and part-oflinks, but does not include more specializedrelations or axioms.
The structure of the ontologyis a directed acyclic graph (DAG).
To achievedomain-independence, we use a combination ofrelatively shallow methods along with existingrepositories of applicable background knowledge.These are described in Section 2.
In Section 3, wealso introduce a new metric Relation Precision forevaluating induced ontologies in comparison withreference ontologies.
We have applied our systemto produce ontologies in numerous domains:CompuTerm 2004  -  3rd International Workshop on Computational Terminology 47Figure 1: System ArchitectureIRSPublication 17285k10 285n1ReutersCorpus9k219,024 19,043n2Total 294 19,024 19,328Table 1: Distribution of ?income tax?
in domain and background corpora(i) newswire from the TREC collection (ii)taxation information from the IRS (Publication 17,from (IRS 2001)), (iii) epidemiological newsgroupmessages from the Program for MonitoringEmerging Diseases (PROMED) from theFederation of American Scientists3, (iv) the text ofa book by the first author called AutomaticSummarization, and (v) MEDLINE biomedicalabstracts retrieved from the National Library ofMedicine?s PubMed system4.
In the latter domain,we have begun building a large ontology using theontology induction methods along with post-editing by domain experts in molecular biology atGeorgetown University 5 .
This ontology, calledPRONTO, involves hundreds of thousands ofprotein names found in MEDLINE abstracts and inUNIPROT, the world?s largest protein database6.
Itis therefore infeasible to construct PRONTO byhand from scratch.
PRONTO is also much largerthan other ontologies in the biology area; forexample, the Gene Ontology is rather high-level,and contains (as of March 2004) only about 17,000terms.3 www.fas.org/promed/4www4.ncbi.nlm.nih.gov/PubMed/5 complingone.georgetown.edu/~prot/6pir.georgetown.edu2 Approach2.1 System ArchitectureAn overall architecture for domain-independentontology induction is shown in Figure 1.
Thedocuments are preprocessed to separate outheaders.
Next, terms are extracted using finite-statesyntactic parsing and scored to discover domain-relevant terms.
The subsequent processing inferssemantic relations between pairs of terms using the?weak?
knowledge sources run in the orderdescribed below.
Evidence from multipleknowledge sources is then combined to infer theresulting relations.
The resulting ontologies arewritten out in a standard XML-based format (e.g.,XOL, RDF, OWL), for use in various informationaccess applications.While the ontology induction procedure does notinvolve human labor, except for writing thepreprocessing and term tokenization program forspecialized technical domains, the human may editthe resulting ontology for use in a givenapplication.
An ontology editor has beendeveloped, discussed briefly in Section 3.1.2.2 Term DiscoveryThe system takes a collection of documents in asubject area, and identifies terms characteristic ofthe domain.
In a given domain such asCompuTerm 2004  -  3rd International Workshop on Computational Terminology48bioinformatics, specialized term tokenization (intosingle- and multi-word terms) is required.
Theprotein names can be long, e.g.,?steroid/thyroid/retinoic nuclear hormone receptorhomolog nhr-35?, and involve specialized patterns.In constructing PRONTO, we have used a proteinname tagger based on an ensemble of statisticalclassifiers to tag protein names in collections ofMEDLINE abstracts (Anon 2004).
Thus, in such adomain, a specialized tagger replaces thecomponents in the dotted box in Figure 1.In other domains, we adopt a generic term-discovery approach.
Here the text is tagged forpart-of-speech, and single- and multi-word termsconsisting of minimal NPs are extracted usingfinite-state parsing with CASS (Abney 1996).
Allpunctuation except for hyphens are removed fromthe terms, which are then lower-cased.
Each wordin each term is stemmed, with statistics (see below)being gathered for each stemmed term.
Multi-wordterms are clustered so that open, closed andhyphenated compounds are treated as equivalent,with the most frequent term in the collection beingused as the cluster representative.The terms are scored for domain-relevance basedon the assumption that if a term occurssignificantly more in a domain corpus than in amore diffuse background corpus, then the term isclearly domain relevant.As an illustration, in Table 1 we compare thenumber of documents containing the term ?incometax?
(or ?income taxes?)
in a long (2.18 Mb) IRSpublication, Publication 17, from an IRS web site(IRS 2001) compared to a larger (27.63 Mb subsetof the) Reuters 21578 news corpus7.
One wouldexpect that ?income tax?
is much more acharacteristic of the IRS publication, and this isborne out by the document frequencies in the table.We use the log likelihood ratio (LLR) (Dunning1993) given by-2log2(Ho(p;k1,n1,k2,n2)/Ha(p1,p2;n1,k1,n2,k2))LLR measures the extent to which ahypothesized model of the distribution of cellcounts, Ha, differs from the null hypothesis, Ho(namely, that the percentage of documentscontaining this term is the same in both corpora).We used a binomial model for Ho and Ha8.2.3 Relationship DiscoveryThe main innovation in our approach is to fusetogether information from multiple knowledge7 In Publication 17, each ?chapter?
is a document.8From Table 1, p=294/19238=.015, p1=285/285=1.0,p2=9/19043=4.72, k1=285, n1=285, k2=9, n2=19043.sources as evidence for particular semanticrelationships between terms.
To infer semanticrelations such as kind-of and part-of, the systemuses a bottom-up data-driven approach using acombination of evidence from shallow methods.2.3.1 Subphrase RelationsThese are based on the presence of commonsyntactic heads, and allow us to infer, for example,that ?p68 protein?
is a kind-of ?protein?.
Likewise,in the TREC domain, subphrase analysis tells usthat ?electric car?
is a kind of ?car?, and in the IRSdomain, that ?federal income tax?
is a kind of?income tax?.2.3.2 Existing Ontology RelationsThese are obtained from a thesaurus.
Forexample, the Gene Ontology can be used to inferthat ?ATP-dependent RNA helicase?
is a kind of?RNA-helicase?.
Likewise, in the TREC domain,using WordNet tells us that ?tailpipe?
is part of?automobile?, and in the IRS domain, that ?spouse?is a kind of ?person?.
Synonyms are also mergedtogether at this stage.2.3.3 Contextual Subsumption RelationsWe also infer hierarchical relations betweenterms, by top-down clustering using a context-based subsumption (CBS) algorithm.
Thealgorithm uses a probabilistic measure of setcovering to find subsumption relations.
For eachterm in the corpus, we note the set of contexts inwhich the term appears.
Term1 is said to subsumeterm2 when the conditional probability of term1appearing in a context given the presence of term2,i.e., P(term1|term2), is greater than some threshold.CBS is based on the algorithm of (Lawrie et al2001), which used a greedy approximation of theDomination Set Problem for graphs to discoversubsumption relations among terms.
Unlike theirwork, we did not seek to minimize the set ofcovering terms; therefore, a subsumed term mayhave multiple parents.
The conditional probabilitythreshold (0.8) we use to determine subsumption ismuch higher than in their approach.
We alsorestrict the height of the hierarchies we build tothree tiers.
Tightening these latter two constraintsappears to notably improve the quality of oursubsumption relations.The largest corpus against which CBS has run isthe ProMed corpus where, considering eachparagraph a distinct context, there were 117,690contexts in the 11,198 documents.
Here is anexample from ProMed of a transitive relation thatspans three tiers: ?mosquito?
is a hypernym of?mosquito pool?, and ?mosquito?
is also ahypernym of ?standing water?.CompuTerm 2004  -  3rd International Workshop on Computational Terminology 492.3.4 Explicit Patterns RelationsThis knowledge source infers specific relationsbetween terms based on characteristic cue-phraseswhich relate them.
For example, the cue-phrase?such as?
(Hearst 1992) (Caraballo 1999) suggest akind-of relation, e.g., ?a ligand such astriethylphosphine?
tells us that ?triethylphosphene?is a kind of ?ligand?.
Likewise, in the TRECdomain, ?air toxics such as benzene?
can suggestthat ?benzene?
is a kind of ?air toxic?.
However,since such cue-phrase patterns tend to be sparse inoccurrence, we do not use them in the evaluationsdescribed below.2.3.5 Domain-Specific Knowledge SourcesAlthough our approach is domain-independent, itis possible to factor in domain knowledge sourcesfor a given domain.
For example, in biology, ?ase?is usually a suffix indicating an enzyme.Postmodifying PPs (found using a CASS grammar)can also be useful in some domains, as shown in?tax on investment income of child?
in Figure 2.We have so far, however, not investigated otherdomain-specific knowledge sources.2.4 Evidence CombinationThe main point about these and other knowledgesources is that each may provide only partialinformation.
Combining these knowledge sourcestogether, we expect, will lead to superiorperformance compared to just any one of them.Not only do inferences from different knowledgesources support each other, but they are alsocombined to produce new inferences by transitivityrelations.
For example, since phrase analysis tellsus that ?pyridine metabolism?
is a kind-of?metabolism?, and Gene Ontology tells us that?metabolism?
is a kind-of ?biological process?, itfollows that ?pyridine metabolism?
is a kind-of?biological process?.
The evidence combination, inaddition to computing transitive closure of theserelations, also detects inconsistencies, querying theuser to resolve them when detected.3 Evaluation3.1 Informal AssessmentSubphrase Relations is a relatively high-precision knowledge source compared to theothers, producing many linked chains.
Itsperformance can be improved by flagging andexcluding proper names and idioms from its input(e..g, so that ?palm pilot?
doesn?t show up as akind-of ?pilot?).
However, a chain of such relationscan be interrupted by terms that aren?t lexicallysimilar, but that are nevertheless in a kind-ofrelation.
Some of these gaps are filled bytransitivity relations involving other knowledgesources, especially Existing Ontologies, which isespecially useful in filling gaps in some of theupper levels of the ontology.
While ContextualSubsumption is good at  discovering associationsbetween ?leaves?
in the DAG and other concepts,the method cannot reliably infer the label of therelation.
For example, in the IRS domain, weobtain ?divorce?
as more general than ?decree ofdivorce?
and ?separate maintenance?, but we don?tknow the nature of the relations.
ContextualSubsumption-inferred links are directed edges withlabel ?unknown?.Overall, the ontologies produced are noisy andrequire human correction, and the methods canproduce many fragments that need to be linked byhand.
While the system can detect cycles that needresolution by the human, these rarely ariseFigure 2: An IRS Ontology viewed in the Ontology EditorCompuTerm 2004  -  3rd International Workshop on Computational Terminology50Term TargetDFBack-groundDFLLR IG MI DF TF TF *IDFelectric 80 61 99.9 99.9 81.3 99.9 99.9 27.8car 77 56 99.6 99.3 81.5 99.8 99.9 79.4battery 54 16 99.0 98.2 86.9 98.7 99.9 94.9emission 15 0 96.5 96.8 99.2 79.1 96.6 64.8year 58 505 67.9 67.6 25.0 99.2 99.7 65.7informal 10 29 66.2 66.3 0.2 48.6 99.7 99.2record 8 138 15.2 15.7 4.4 50.2 99.9 99.9osha 1 0 0.0 0.0 0.0 0.0 99.9 0.0Table 2: Comparing Topic 230 Term Percentile RankingsFor a flavor of the kind of results we get, seeFigure 2, which displays an ontology inducedwithout any human intervention from IRSPublication 17.
Here the DAG is displayed as atree.
The immediate children of  ?person?, a nodehigh in the ontology, is shown in the left part of thewindow.
Selecting ?child?
brings up its kinds aswell as some other children linked by ?unknown?label via Contextual Subsumption, e.g., ?full-timestudent?.
A list of orphaned terms that aren?trelated to any others are shown on the far right.The terms with checkboxes are those that occur inthe corpus; the others are those that are foundexclusively by Existing Ontology Relations.Checking a term allows it to be inspected in itsoccurrence context in the corpus.
The editor comeswith a variety of tools to help integrate ontologyfragments.3.2 Human Evaluation3.2.1 Term ScoringTo evaluate term scoring, we used a corpus ofnews articles about automobiles that consisted of85 documents relevant to the TREC Topic 230query: ?Is the automobile industry making anhonest effort to develop and produce an electric-powered automobile??
In Table 2, we providesome examples of how the LLR term scoringstatistic performed with respect to five others onselected unigrams in the Topic 230 domain: termfrequency, document frequency, term frequencytimes inverse document frequency (TF*IDF),pointwise mutual information (MI), andinformation gain (IG).
Terms in bold are ones wejudged important in the Topic 230 domain, theothers are deemed unimportant.
The numbers arepercentile rankings.
LLR and IG do equally well,outperforming the others.We carried out other comparisons for two otherdomains.
In the income-tax domain, a hand-builtterm list from the IRS contained 82 terms whichoccurred in IRS Publication 17, of which thesystem discovered 77 (94% recall).
In the ProMeddomain, a pre-existing hand-built taxonomyproduced by a bioterrorism analyst had 1048 termswhich occurred in the ProMed message corpus, ofwhich 607 were discovered by the system (58%recall).
However, the hand-built taxonomy, whichwas built without consulting a corpus, wasn?t afull-fledged ontology, for example, there was nolabel for the parent-child relation.3.2.2 Term RelationshipsWe also carried out an evaluation experiment todetermine if the relations being discovered by themachine were in keeping with human judgments.We focused here on an evaluation of pairs ofknowledge sources.
Our experiment examined thecase where the system discovered a kind-ofrelation.
Here each subject was first asked to readfour newspaper articles from the TREC topic-230sub-collection.
The articles were then keptaccessible to the subject in a browser window forthe subject to consult if needed in answeringsubsequent questions.
The subject was asked tojudge, based on the documents read, whether termX was a kind of term Y, term Y was a kind of termX, or neither; e.g., ?Is acid a kind of pollutant, or ispollutant a kind of acid, or neither??.
The subjecthad one of three mutually exclusive choices; thefirst two choices were presented in randomizedorder.The subjects were 16 native speakers of Englishunconnected with the project.
Each subject wasgiven ten questions to answer in each of theexperiments.
For each set of ten questions, fivewere chosen at random from pairs of terms relatedCompuTerm 2004  -  3rd International Workshop on Computational Terminology 51by (immediate) kind-of relations.
The remainingfive questions were chosen at random from pairs ofterms between which the system found no relationwhatsoever.HumanSystem kind-of(A, B)not kind-of(A,B)kind-of(A, B) 56 18not kind-of(A,B)674Table 3: Is X a kind-of Y?We first discuss inter-subject agreement.
Threesubjects given the same relation to judge agreed75% of the time, leading to a Kappa score of 0.72,indicating a good level of agreement.
This meansthat subjects were able to reliably make judgmentsas to whether A is a kind of B in some document.The results for the 16 subjects are shown inTable 3.
When the system is compared to thehuman as ground truth, this gives a Precision of.90, a Recall of .75, and an F-measure of .82.
Thisperformance is also significantly better thanrandom assignment: with chi-square=74.29, with p< 0.0019.
The substantial effect sizes of the chi-square indicates a very solid result.
There were 62decisions involving Subphrase Relations (with 44True Positives and 18 False Negatives), and 10decisions involving WordNet (with 12 TruePositives).
This shows that there is solid agreementbetween the human subjects and the system on thekind-of relations.
However, these 154 decisionsinvolved only four newspaper articles, so clearlymore data would be helpful.3.3 Automatic EvaluationWhile evaluation by humans is valuable, it isexpensive to carry out, and this expense must beincurred each time one wants to do an evaluation.Automatic comparison of a machine-generatedontology against reference ontologies constructedby humans, e.g., (Zhang et al 1996) (Sekine et al1999) (Daude et al 2001), is therefore desirable,provided suitable reference ontologies areavailable.
In this evaluation, the human-generatedtaxonomy for ProMed described in Section 3.2.1was used as a reference ontology, with itsunlabeled parent-child relation treated as a kind-oflink.
However, the human ?ontology?
was createdwithout looking at a corpus, and was developed foruse with a different set of goals in mind.
Althoughthis involves comparing ?apples?
and ?oranges?, acomparison is nevertheless illustrative, and can inaddition be useful when comparing mutipleontologies created under similar conditions.Figure 3:  Automatically Induced Fragmentfrom ProMedTo set aside the problem of differences interminology involved in the comparison, wedecided to restrict our attention to the set of termsTH (of cardinality 3025) in the human ontology(H), and have our system induce relations betweenthem using the ProMed corpus.
Relations wereinduced automatically in the machine ontology (M)for just 761 of those terms, yielding a set TH1.
Thestructure of  TH1 is shown in a fragment in Figure3.
Here A is a kind-of B if it is printed under Bwithout a label; A is a part-of B if it is printedunder B with a ?p?
label.We then automatically computed, for each pairof terms t1 and t2 in TH1 that were linked distance 1apart in M, the distance between those terms in H.Likewise, we also computed, for each pair of termst1 and t2 in TH1 distance 1 apart in H, the distancebetween those terms in M.The results of this comparison are as follows.The number of relations where the two ontologiesagree exactly is 63 (i.e., the terms are distance 1apart in both ontologies).
Since, given a set ofterms, there are many different ways to constructan ontology, this is encouraging.The number of relations that our system foundwhich were ?missed?, i.e., more than distance 1away, in H is 1203.
Given the previous experimentwhere the human subjects agreed with the system'srelations, these 1203 relations are likely to containmany that the human probably missed.
Forexample, the relations in the machine ontologybetween ?eye?
and ?farsightedness?, and ?medicine?9  The chi-square for Subphrase Relations is 61.68,and the chi-square for WordNet is 56.73, with p < 0.001in all cases.CompuTerm 2004  -  3rd International Workshop on Computational Terminology524 Related Work and ?chiropractic medicine?
are missed by H. Thishighlights a problem with human-generatedontologies: substantial errors of omission.
The existing approaches to ontology induction include those that start from structured data,merging ontologies or database schemas (Doan etal.
2002).
Other approaches use natural languagedata, sometimes just by analyzing the corpus(Sanderson and Croft 1999), (Caraballo 1999) orby learning to expand WordNet with clusters ofterms from a corpus, e.g., (Girju et al 2003).Information extraction approaches that inferlabeled relations either require substantial hand-created linguistic or domain knowledge, e.g.,(Craven and Kumlien 1999) (Hull and Gomez1993), or require human-annotated training datawith relation information for each domain (Cravenet al 1998).The number of relations in H that our systemmissed (relations that were more than distance 1away in the system ontology), is 3493.
However,of these 3493 relations, 2955 involved at least 1term that was not included in M, leaving 538relations that we could calculate the distance for inM.
These 538 relations in H include relationsbetween ?acid indigestion medicine?
and ?maalox?,and ?alternative medicine?
and ?acupuncture?
(amajority of the misses involved relations between adisease and the name of a specific drug for it,which aren?t part-of or kind-of relations).00.10.20.30.40.50.60.70.80.911 2 3 4 5 6 7 8 9DRelation-Prec.(H,M,D)Relation-Prec.
(M,H,D)Many, though not all, domain-independentapproaches (Evans et al 1991) (Grefenstette 1997)have restricted themselves to discovering term-associations, rather than labeled relations.
Anotable exception is (Sanderson and Croft 1995),which (unlike our approach) assumes the existenceof a query that was used to originally retrieve thedocuments (so that terms can be extracted from thequery and then expanded to generate additionalterms for the ontology).
Their approach also isrestricted to one method to discover relations,while we use several.Our approach is complementary to approachesaimed at automatically enhancing existingresources for a particular domain, e.g.
(Moldovanet al 2000).
Finally, the prior methods, while theyoften carry out evaluation, lack standard criteria forontology evaluation.
Although ontology evaluationremains challenging, we have discussed severalevaluation methods in this paper.Figure 4: Relation PrecisionThese observations lead to a metric forcomparing one ontology with another one servingas a reference ontology.
Given two ontologies Aand B, define Relation Precision (A, B, D) as theproportion of the distance 1 relations in A that areat most a distance D apart in B.
This measure canbe plotted for different values of D. In Figure 4, weshow the Relation Precision(H, M, D), andRelation Precision(M, H, D), for our machineontology M and human ontology H. Both curvesshow Relation Precision(H, M, D) growing fasterthan Relation Precision(M, H, D), with 70% of thearea being below the former curve and 54% beingbelow the latter curve.
The graph shows that while22% of distance 1 relations in M are at most 3apart in H (but keep in mind the errors of omissionin H), 40% of distance 1 relations in H are at most3 apart in M10.5 ConclusionThe evidence combination described above isbased on transitivity and union.
Since the aboveevaluations, we have been experimenting with anad hoc weighted evidence combination scheme,based on each knowledge source expressing astrength for a posited relation.
In future, we willalso investigate using an initial seed ontology toprovide a better ?backbone?
for induction, and thenusing a spreading activation method to activatenodes related by existing knowledge sources toseed nodes.
Corpus statistics can be used to weightthe links.
For example, based on (Caraballo 1999),each parent of a leaf node could be viewed as acluster label for its children, with the weight of aparent-child link being determined based on howstrongly the child is associated with the cluster.10 The mean distance in H between terms that aredistance 1 apart in M is 5.17, with a standard deviationof 2.12.
The mean distance in M between terms whichare distance 1 apart in H is 3.85, with a standarddeviation of 1.69.The ontology induction methods described herecan allow for considerable savings in time inCompuTerm 2004  -  3rd International Workshop on Computational Terminology 53constructing ontologies.
The evaluations we havecarried out are suggestive, but many issues remainopen.
There are many unanswered questions abouthuman-created reference ontologies, including lackof inter-annotator agreement studies.
Indeed,experience shows that without guidelines forontology construction, humans are prone to comeup with very different ontologies for a domain.Comparing a machine-induced ontology against anideal human reference ontology, were one to beavailable, is also fraught with problems.
Ourexperience with using an implementation of the(Daude et al 2001) constraint relaxation algorithmfor ontology comparison suggests that much workis needed on distance metrics which are not over-sensitive to small differences in structure.Our interest, therefore, is focused more towardsan extrinsic evaluation.
PRONTO, which is due tobe released in 2004, offers the opportunity tomeasure costs of ontology induction and post-editing on a large-scale problem of value to thebiology community.
We also plan to measure theeffectiveness of PRONTO in query expansion forinformation access to MEDLINE and proteindatabases.
Finally, we will investigate moresophisticated evidence combination methods, andcompare against other automatic methods forontology induction.The ontology induction tools are available forfree distribution for research purposes.ReferencesAbney, S. 1996.
Partial parsing Via Finite-StateCascades.
Proceedings of the ESSLLI '96 RobustParsing Workshop.Caraballo, S. A.
1999.
Automatic Construction of ahypernym-labeled noun hierarchy from text.
InProceedings of the 37th Annual Meeting of theAssociation for Computational Linguistics(ACL'1999), 120-122.Cohen, P. R., Chaudhri, V., Pease, A. and Schrag,R.
1999.
Does Prior Knowledge Facilitate theDevelopment of Knowledge-based Systems?
TheSixteenth National Conference on ArtificialIntelligence (AAAI-99).Craven, M. and Kumlien, J.
1999.
Constructingbiological knowledge bases by extractinginformation from text sources.
Proc Int ConfIntell Syst Mol Biol., 77-86.Craven, M., DiPasquo, D., Freitag, D., McCallum,A., Mitchell, T., Nigam, K., and Slattery, S..1998.
Learning to Extract Symbolic Knowledgefrom the World Wide Web.
Proceedings ofAAAI-98, 509-516.Daude, J., Padro, L. and Rigau, G. 2001 AComplete WN1.5 to WN1.6 Mapping.
NAACL-2001 Workshop on WordNet and Other LexicalResources: Applications, Extension, andCustomization, 83-88.Doan, A.,  Madhavan, J. , Domings, P. and Halevy,A.
2002.
Learning to Map between Ontologieson the Semantic Web.
WWW?2002.Dunning, T. 1993.
Accurate Methods for theStatistics of Surprise and Coincidence,?Computational Linguistics, 19(1):61-74, 1993.Girju, R.,  Badulescu, A., and Moldovan, D. 2003.Learning Semantic Constraints for the AutomaticDiscovery of Part-Whole Relations.
Proceedingsof HLT?2003, Edmonton.Grefenstette, G. 1997.
Explorations in AutomaticThesaurus Discovery.
Kluwer InternationalSeries in Engineering and Computer Science,Vol 278.Hearst, M. 1992.
Automatic Acquisition ofHyponyms from Large Text Corpora.Proceedings of the fourteenth InternationalConference on Computational Linguistics,Nantes, France, July 1992.Hull, R. and Gomez, F. 1993.
Inferring HeuristicClassification Hierarchies from NaturalLanguage Input.
Telematics and Informatics,9(3/4), pp.
265-281.IRS (Internal Revenue Service).
2001.
Tax Guide2001.
Publication 17. http://www.irs.gov/pub/irs-pdf/p17.pdfLawrie, D., Croft, W. B., and Rosenberg, A.
2001.Finding topic words for hierarchicalsummarization.
24th ACM Intl.
Conf.
onResearch and Development in InformationRetrieval, 349-357, 2001.Miller, G. (1995).
WordNet: A Lexical Databasefor English.
Communications Of the AssociationFor Computing Machinery (CACM) 38, 39-41.Sanderson, M. and Croft, B.
1995.
Derivingconcept hierarchies from text.
Proceedings of the22nd Annual Internationaql ACM SIGIRConference on Research and Development inInformation Retrieval, 160-170.Sekine, S., Sudo, K. and Ogino, T. 1999.
StatisticalMatching of Two Ontologies.
Proceedings ofACL SIGLEX99 Workshop: StandardizingLexical Resources.Zhang, K., Wang, J. T. L. and Shasha, D.  1996.On the Editing Distance between UndirectedAcyclic Graphs and Related Problems.International Journal of Foundations ofComputer Science 7, 43-58.CompuTerm 2004  -  3rd International Workshop on Computational Terminology54
