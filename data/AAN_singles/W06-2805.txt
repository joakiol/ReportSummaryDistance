Learning to Recognize Blogs: A Preliminary ExplorationErik Elgersma and Maarten de RijkeISLA, University of AmsterdamKruislaan 403, 1098SJ Amsterdam, The Netherlandserik@elgersma.net, mdr@science.uva.nlAbstractWe present results of our experimentswith the application of machine learningon binary blog classification, i.e.
deter-mining whether a given web page is ablog page.
We have gathered a corpus inexcess of half a million blog or blog-likepages and pre-classified them using asimple baseline.
We investigate whichalgorithms attain the best results for ourclassification problem and experimentwith resampling techniques, with the aimof utilising our large dataset to improveupon our baseline.
We show that the ap-plication of off-the-shelf machine learn-ing technology to perform binary blogclassification offers substantial improve-ment over our baseline.
Further gains cansometimes be achieved using resamplingtechniques, but these improvements arerelatively small compared to the initialgain.1 IntroductionIn recent years, weblogs (online journals inwhich the owner posts entries on a regular basis)have not only rapidly become popular as a newand easily accessible publishing tool for themasses, but its content is becoming ever morevaluable as a ?window to the world,?
an exten-sive medium brimming with subjective contentthat can be mined and analysed to discover whatpeople are talking about and why.
In recent yearsthe volume of blogs is estimated to have doubledapproximately every six months.
Technorati1report that about 11% of internet users are blogreaders and that about 70 thousand new blogs arecreated daily.
Popular blogosphere (the completecollection of all blogs) analysis tools estimate theblogosphere to contain anywhere between 201and 24 million2 blogs at time of writing.
Giventhis growing popularity and size, research onblogs and the blogosphere is also increasing.
Alarge amount of this research is being done onthe content provided by the blogosphere and thenature of this content, like for example (Mishneand de Rijke, 2005), or the structure of the blo-gosphere (Adar et al, 2004).In this paper, however, we address the task ofbinary blog classification: given a (web) docu-ment, is this a blog or not?
Our aim is to basethis classification mostly on blog characteristicsrather than content.
We will by no means ignorecontent but it should not become a crucial part ofthe classification process.Reliable blog classification is an importanttask in the blogosphere as it allows researchers,ping feeds (used to broadcast blog updates),trend analysis tools and many others to separatereal blog content from blog-like content such asbulletin boards, newsgroups or trade markets.
Itis a task that so far has proved difficult as can bewitnessed by checking any of the major blog up-date feeds such as weblogs.com3 or blo.gs.4 Bothwill at any given time list content that clearly isnot a blog.
In this paper we will explore blogclassification using machine learning to improveblog detection and experiment with severalmethods to try and further improve the percent-age of instances classified correctly.The main research question we address in thispaper is exploratory in nature:- How hard is binary blog classification?Put more specifically,1 Intelliseek?s BlogPulse, http://www.blogpulse.com2 Technorati, http://www.technorati.com3 Weblogs.com, http://www.weblogs.com4 Blo.gs, http://blo.gs24- What is the performance of basic off-the-shelf machine learning algorithms on thistask?and- Can the performance of these methods beimproved using resampling methods suchas bootstrapping and co-training?An important complicating factor is the lack oflabeled data.
It is widely accepted that given asufficient amount of training data, most machinelearning algorithms will achieve similar per-formance levels.
For our experiments, we willhave a very limited amount of training materialavailable.
Therefore, we expect to see substantialdifferences between algorithms.In this paper we will first discuss related workin the following section, before describing theexperiments in detail and reporting on the results.Finally, we will draw conclusions based on theexperiments and the results.2 Related WorkBlog classification is still very much in its in-fancy and to date no directly related work hasbeen published as far as we are aware.
There is,however, work related to several aspects of ourexperiments.Nanno et al (2004) describe a system forgathering a large collection of weblogs, not onlythose published using one of the many well-known authoring tools but also the hand-writtenvariety.
A very much comparable system wasdeveloped and used for these experiments.Members of the BlogPulse team also describeblog crawling and corpus creation in some detail(Glance et al, 2004), but their system is aimedmore at gathering updates and following activeblogs rather than gathering as many blogs in theirentirety, as our system is set up to do.As to the resampling methods used in this pa-per?bootstrapping and co-training?, Jones etal.
(1999) describe the application of bootstrap-ping to text learning tasks and report very goodresults applying this method to these tasks.
Eventhough text learning is a very different genre,their results provide hope that the application ofthis method may also prove useful for our blogclassification problem.Blum and Mitchell (1998) describe the use ofseparate weak indicators to label unlabeled in-stances as ?probably positive?
to further train alearning algorithm and gathered results that sug-gested that their method has the potential for im-proving results on many practical learning prob-lems.
Indeed their example of web-page classifi-cation is in many ways very similar to our binaryblog classification problem.
In these experimentshowever we will use a different kind of indica-tors on the unlabeled data, namely the predic-tions of several different types of algorithms.3 Binary blog classificationIn our first experiment, we attempted binaryblog classification (?is this a blog or not??)
usinga small manually annotated dataset and a largevariety of algorithms.
The aim of this experimentwas to discover what the performance of readilyavailable, off-the-shelf algorithms is given thistask.We used a broad spectrum of learners imple-mented in the well-known Weka machine learn-ing toolkit (Witten and Frank, 2005).3.1 DatasetFor our later resampling experiments, a largeamount of data was gathered, as will be ex-plained further on in this paper.
To create a data-set for this experiment, 201 blog / blog-likepages were randomly selected from the collec-tion, processed into Weka?s arff format andmanually annotated.
These instances were thenexcluded from the rest of the collection.
Thisyielded a small but reliable dataset, which wehoped would be sufficient for this task.3.2 Attribute selectionAll pages were processed into instances de-scribed by a variety of attributes.
For binary blogclassification to succeed, we had to find a largenumber of characteristics with which to accu-rately describe the data.
This was done by manu-ally browsing the HTML source code of severalblogs as well as some simple intuition.
Theseattributes range from ?number of posts?
and?post length?
to checking for characteristicphrases such as ?Comments?
or ?Archives?
orchecking for the use of style sheets.
Interestingattributes are the ?firstLine?
/ ?lastLine?
attrib-utes, which calculate a score depending on thenumber of tokens found in those lines, whichfrequently occur in those lines in verified blogposts.
The ?contentType?
attribute does some-thing very similar, but based on the completeclean text of a page rather than particular lines inposts.
It counts how many of the 100 most fre-quent tokens in clean text versions of actualblogs, are found in a page and returns a true25value if more than 60% of these are found, inwhich case the page is probably a blog.
The ?fre-quent terms?-lists for these attributes were gen-erated using a manually verified list gatheredfrom a general purpose dataset used for earlierexperiments.
A ?host?-attribute is also used,which we binarised into a large number of binaryhost name attributes as most machine learningalgorithms cannot cope with string attributes.
Forthis purpose we took the 30 most common hostsin our dataset, which included Livejournal,5Xanga,6 20six,7 etc., but also a number of hoststhat are obviously not blog sites (but host manypages that resemble blogs).
Negative indicatorson common hosts that don?t serve blogs are justas valuable to the machine learner as the positiveindicators of common blog hosts.
Last but notleast a binary attribute was added that acts as aclass label for the instance.
This process left uswith the following 46 attributes:Attribute TypenrOfPosts numericavgPostLength numericminPostLength numericmaxPostLength numericfirstLine numericlastLine numericcontainsBlog numericcontainsMetaTag binarycontentType binarycontainsComment binarycontainsPostedBy binarycontainsRSS binarycontainsArchives binarycontainsPreviousPosts binaryStyleSheetsUsed binarylivejournal.com binarymsn.com binarywretch.cc binaryxanga.com binarydiaryland.com binaryabazy.com binary20six.fr binaryresearch101-411.com binarysearch-now700.com binarysearch-now999.com binarysearch-now600.com binary20six.co.uk binaryresearch-bot.com binary5 http://www.livejournal.com6 http://www.xanga.com7 http://www.20six.comblogsearchonline.com binarygoogane.com binarytypepad.com binaryfindbestnow.com binarymyblog.de binaryquick-blog.com binaryfindhererightnow.com binaryfindfreenow.com binarywebsearch010.com binarytwoday.net binarywebsearch013.com binarytracetotal.info binarykotobabooks.com binarycocolog-nifty.com binary20six.de binaryis-here-online.com binary4moreadvice.info binaryblog binaryTable 1: Attributes selected for our experiments.3.3 Experimental setupFor this experiment, we trained a wide range oflearners using the manually annotated data andtested using ten-fold cross-validation.
We thencompared the results to a baseline.This baseline is based mostly on simple heu-ristics, and is an extended version of the WWW-Blog-Identify8 perl module that is freely avail-able online.
First of all, a URL check is donewhich looks for a large number of the well-known blog hosts as an indicator.
Should thisfail, a search is done for metatags which indicatethe use of well-known blog creation tools such asNucleus,9 Greymatter,10 Movable Type11 etc.Should this also fail, an actual content search isdone for other indicators such as particular iconsblog creation tools leave on pages (?created us-ing?
.gif?
etc).
Next, the module checks for anRSS feed, and as a very last resort checks thenumber of times the term ?blog?
is used on thepage as an indicator.In earlier research, our version of the modulewas manually tested by a small group of indi-viduals and found to have an accuracy of roughly80% which means it is very useful as a target toaim for with our machine learning algorithmsand a good baseline.8 http://search.cpan.org/~mceglows/WWW-Blog-Identify-0.06/Identify.pm9 http://nucleuscms.org10 http://www.noahgrey.com/greysoft/11 http://www.movabletype.org/263.4 Results: single classifiersFigure 1: Chart showing the percentage cor-rect predictions for each algorithm tested.It is clear that all algorithms bar ZeroR performwell, most topping 90%.
ZeroR achieves nomore than 73%, and is the only algorithm thatactually performs worse than our baseline.
Thebest algorithm for this task, and on this dataset, isclearly the support vector-based algorithm SMO,which scores 94.75%.
These scores can be con-sidered excellent for a classification task, and thewide success across the range of algorithmsshows that our attribute selection has been a suc-cess.
The attributes clearly describe the datawell.Full results of this experiment can be found inAppendix A.4 ResamplingNow we turn to the second of our research ques-tions: to what extent can resampling methodshelp create better blog classifiers.As reported earlier, the blogosphere todaycontains millions of blogs and therefore poten-tially plenty of data for our classifier.
However,this data is all unlabeled.
Furthermore, we have adistinct lack of reliably labeled data.
Resamplingmay provide us with a solution to this problemand allow us to reliably label the data from ourunlabeled data source and further improve uponthe results gained using our very small manuallyannotated dataset.For these experiments we selected two resam-pling methods.
The first is ordinary bootstrap-ping, which we chose because it is the simplestway of relabeling unlabeled data on the basis of amachine learning model.
Additionally, we chosea modified form of co-training, as co-training isalso a well-known resampling method, whichwas easily adaptable to our problem and seem-ingly offered a good approach.4.1 Data setTo gather a large data set containing both blogsand non-blogs, a crawler was developed that in-cluded a blog detection module based on the heu-ristics in our baseline module mentioned earlier.After downloading a page judged likely to be ablog by the module on the basis of its URL, sev-eral additional checks were done by the blog de-tection module based on several other character-istics, most importantly the presence of date-entry combinations.
Pages judged to be a blogand those judged not to be even though the URLlooked promising, were consequently storedseparately.
Blogs were stored in html, clean textand single entry (text) formats.
For non-blogsonly the html was stored to conserve space whilestill allowing the documents to be fully analysedpost-crawling.Using this system, 227.380 blog- and 285.337non-blog pages (often several pages were gath-ered from the same blog, so the actual number ofblogs gathered is significantly lower) were gath-ered in the period from July 7 until November 3,2005.
This amounts to roughly 30Gb of HTMLand text, and includes blogs from all the well-known blog sites as well as personal hand-written blogs and in many different languages.The blog detection module in the crawler wasused purely for the purpose of filtering out URLsand webpages that bear no resemblence to ablog.
By performing this pre-classification, wewere able to gather a dataset containing onlyblogs and pages that in appearance closely re-semble blogs so that our dataset contained bothpositive examples and useful negative examples.This approach should force the machine learnerto make a clear distinction between blogs andnon-blogs.
However, even though this data waspre-classified by our baseline, we treat it as unla-beled data in our experiments and make no fur-ther use of this pre-classification whatsoever.For our resampling experiments, we randomlydivided the large dataset into small subsets con-taining 1000 instances, one for each iteration.This figure ensures that the training set grows ata reasonable rate at every iteration while prevent-ing the training set from becoming too large tooquickly which would mean a lot of unlabeled27instances being labeled on the basis of very fewlabeled instances and the model building processwould take too long after only a few iterations.For training and test data we turned back toour manually annotated dataset used previously.Of this set, 100 instances were used for the initialtraining and the remaining 101 for testing.4.2 Experimental setup: bootstrappingGenerally, bootstrapping is an iterative processwhere at every iteration unlabeled data is labeledusing predictions made by the learner modelbased on the previously available training set(Jones et al, 1999).
These newly labeled in-stances are then added to the training set and thewhole process repeats.
Our expectation was thatthe increase in available training instances shouldimprove the algorithm?s accuracy, especially asit proved quite accurate to begin with so the al-gorihm?s predictions should prove quite reliable.For this experiment we used the best performingalgorithm from Section 3, the SMO support-vector based algorithm.
The bootstrappingmethod is applied to this problem as follows:- Initialisation: use the training set contain-ing 100 manually annotated instances topredict the labels of the first subset of1000 unlabeled instances.- Iterations: Label the unlabeled instancesaccording to the algorithm?s predictionand add these instances to the previoustraining set to form a new training set.Build a new model based on the new train-ing set and use it to predict the labels ofthe next subset.4.3 Results: bootstrappingWe now present the results of our experimentusing normal bootstrapping.
After every itera-tion, the model built by the learner was tested onour manually annotated test set.Iteration Nr.
oftraininginstancesCorrectly /incorrectlyclassified(%)Precision(yes/no)Recall(yes/no)init 100 95.05 / 4.95 0.957 /0.9490.846 /0.9871 1100 94.06 / 5.94 0.955 /0.9370.808 /0.9872 2100 94.06 / 5.94 0.955 /0.9370.808 /0.9873 3100 94.06 / 5.94 0.955 /0.9370.808 /0.9874 4100 94.06 / 5.94 0.955 /0 9370.808 /0 9870.937 0.9875 5100 94.06 / 5.94 0.955 /0.9370.808 /0.9876 6100 94.06 / 5.94 0.955 /0.9370.808 /0.9877 7100 94.06 / 5.94 0.955 /0.9370.808 /0.9878 8100 93.07 / 6.93 0.952 /0.9250.769 /0.9879 9100 93.07 / 6.93 0.952 /0.9250.769 /0.98710 10100 93.07 / 6.93 0.952 /0.9250.769 /0.98711 - 42 11100 ?4210092.08 / 7.92 0.95 /0.9140.731 /0.987Table 2: Overview of results using normal boot-strapping.After 36 iterations, the experiment was halted asthere was clearly no more gain to be expectedfrom any further iterations.
Clearly, ordinarybootstrapping does not offer any advantages forour binary blog classification problem.
Also, theavailability of larger amounts of training in-stances does nothing to improve results as theresults are best using only the very small trainingset.Generally, both precision and recall slowlydecrease as the training set grows, showing thatclassifier accuracy as a whole declines.
However,recall of instances with class label ?no?
(non-blogs) remains constant throughout.
Clearly theclassifier is able to easily detect non-blog pageson the basis of the attributes provided, and isthwarted only by a small number of outliers.
Thiscan be explained by the fact that the learner rec-ognizes non-blogs mostly on the basis of the firstfew attributes having zero values (nrOfPosts,minPostLength, maxPostLength etc.).
The out-liers consistently missed by the classifier areprobably blog-like pages in which date-entrycombinations have been found but which never-theless have been manually classified as non-blogs.
Examples of this are calendar pages com-monly associated with blogs (but which do notcontain blog content), or MSN Space pages onwhich the user is using the photo album buthasn?t started a blog yet.
In this case the page isrecognized as a blog, but contains no blog con-tent and is therefore manually labeled a non-blog.4.4 Experimental setup: co-trainingAs mentioned in Section 2, we will use the pre-dictions of several of the most successful learn-ing algorithms from Section 3 as our indicators28in this experiment.
The goal of our co-trainingexperiment is to take unanimous predictionsfrom the three best performing algorithms fromSection 3, and use those predictions, which weassume to have a very high degree of confidence,to bootstrap the training set.
We will then test tosee if it offers an improvement over the SMOalgorithm by itself.
By unanimous predictions wemean the predictions of those instances, onwhich all the algorithms agree unanimously afterthey have been allowed to predict labels usingtheir respective models.As instances for which the predictions areunanimous can be reasoned to have a very highlevel of confidence, the predictions for those in-stances are almost certainly correct.
Thereforewe expect this method to offer substantial im-provements over any single algorithm as it po-tentially yields a very large number of correctlylabeled instances for the learner to train on.Figure 2: Visual representation of our implemen-tation of the co-training method.We chose to adapt the co-training idea in thisfashion as we believe it to be a good way of radi-cally reducing the fuzziness of potential predic-tions and a way to gain a very high degree ofconfidence in the labels attached to previouslyunlabeled data.
Should the algorithms disagreeon a large number of instances there would stillnot be a problem as we have a very large pool ofunlabeled instances (133.000, we only used partof our corpus for our experiments as our datasetwas so large that there was no need to use all thedata available).
The potential maximum of 133iterations should prove quite sufficient even ifthe growth of the training set per iteration provesto be very small.The algorithms we chose for this experimentwere SMO (support vector), J48 (decision tree, aC4.5 implementation) and Jrip (rule based).
Wechose not to use nearest neighbour algorithms forthis experiment even though they performed wellindividually as we feared it would prove a lesssuccessful approach given the large training setsizes.
Indeed, an earlier experiment done duringour blog classification research showed the per-formance of near neighbour algorithms bottomedout very quickly so no real improvement can beexpected from those algorithms given largertraining sets and given the unanimous nature ofthis method of co-training it may spoil any gainthat might otherwise be achieved.The process started with the manually anno-tated training set and used the predictions fromthe three algorithms, for unlabeled instances theyagree unanimously on, to label those instances.Those instances were subsequently added to thetrainingset and using this new trainingset, anumber of the instances in another unlabeled set(1000 instances per set) were to be labeled(again, only those instances on which the algo-rithms agree unanimously).
Once again, thoseinstances are added to the training set and so onand so forth for as many iterations as possible.4.5 Results: co-trainingWe now turn to the results of our experimentusing our unanimous co-training method de-scribed above.
The experiment was halted after30 iterations, as Weka ran out of memory.
Theexperiment was not re-run with altered memorysettings as it was clear that no more gain was tobe expected by doing so.
Again, testing aftereach iteration was performed by building amodel using the SMO support-vector learningalgorithm and testing classifier accuracy on themanually annotated test set.Iteration Nr.
oftrainingin-stancesCorrectly/incorectlyclassified(%)Precision(yes/no)Recall(yes/no)init  100 95.05 / 4.95 0.957 /0.9490.846 /0.9871 1000 94.06 / 5.94 0.955 /0.9370.808 /0.9872 1903 93.07 / 6.93 0.952 /0.9250.769 /0.9873 2798 95.05 / 4.95 0.957 /0.9490.846 /0.9874 3696 95.05 / 4.95 0.957 /0.9490.846 /0.9875 4566 95.05 / 4.95 0.957 /0.9490.846 /0.9876 5458 96.04 / 3.96 0.958 /0.9610.885 /0.9877 6351 96.04 / 3.96 0.958 /0.9610.885 /0.9878 7235 95.05 / 4.95 0.957 /0.9490.846 /0.9879 8149 95.05 / 4.95 0.957 /0.9490.846 /0.9872910 9041 95.05 / 4.95 0.957 /0.9490.846 /0.98711 9929 95.05 / 4.95 0.957 /0.9490.846 /0.98712 10810 95.05 / 4.95 0.957 /0.9490.846 /0.98713 - 43 11684 -3851094.06 / 5.94 0.955 /0.9370.808 /0.987Table 3: Overview of results using our unani-mous co-training method.Even though the ?steps?
in test percentagesshown represent only one more blog being clas-sified correctly (or incorrectly), the classifierdoes perform better than it did using only themanually annotated training set at some stages ofthe experiment.
This means that gains in classi-fier accuracy can be achieved by using thismethod of co-training on this problem.
Also theclassifier generally performs better than in ourbootstrapping experiment, which shows that theinstances unanimously agreed on by all threealgorithms are certainly more reliable than thepredictions of even the best algorithm by itself,as predicted.Clearly this method offers potential for an im-provement even though the SMO algorithm wasalready very accurate in our first binary blogclassification experiment.5 DiscussionAs the title suggests, these experiments are of apreliminary and exploratory nature.
The highaccuracy achieved by almost all algorithms inour binary classification experiment show thatour attribute set clearly defines the subject well.However, these results must be viewed with anair of caution as they were obtained using a smallsubset and as such the data may not represent thenature of the complete dataset well.
Indeed, howstable are the results obtained?Later experiments using a (disjoin, but) largermanually annotated dataset containing 700 in-stances show that the results obtained here areoptimistic.
The extremely diverse nature of theblogosphere means that describing an entiredataset using a relatively small subset is verydifficult and as such both the performance andranking of off-the-shelf machine learning algo-rithms will vary among different datasets.
Off-the-shelf algorithms do however still perform farbetter than our baseline and the best performingalgorithms still achieve accuracy rates in excessof 90%.Two aspects of our attribute set that need to beworked on in future are date detection and con-tent checks.
Outliers are almost always caused bythe date detection algorithm not detecting certaindate formats, and pages containing date-entrycombinations but no real blog content.
Therefore,although it is possible to perform binary blogclassification based purely on the particular char-acteristics of blog pages with high accuracy, con-tent checks are invaluable.
The rise of blogspam,which cannot be separated from real blogs on thebasis of page characteristics at all, further em-phasises this.
We have already developed adocument frequency profile and replaced thecontentType attribute used in these experiments,to extend the content-based attributes in ourdataset and hopefully improve blog recognition.6 ConclusionOur experiments have shown that binary blogclassification can be performed successfully ifthe right attributes are chosen to describe thedata, even if the classifier is forced to rely on asmall number of training instances.
Almost allbasic off-the-shelf machine learning algorithmsperform well given this task, but support vectorbased algorithms performed best in this experi-ment.
Notable was that the best algorithms ofeach type achieved almost the same accuracy, allover 90% and the difference is never larger thana few percent even though they approach theproblem in completely different manners.The performance of these algorithms can beimproved by using resampling methods, but notall resampling methods achieve gains and thosethat do gain very little.
The extremely high suc-cess rates of the plain algorithms means thatthere is very little room for improvement, espe-cially as the classification errors are almost al-ways caused by outliers that none of the algo-rithms manage to classify correctly.The results of later experiments with largernumbers of manually annotated instances showthat a lot of work remains to be done and thatalthough this paper shows that the application ofmachine learning to this problem offers substan-tial improvements over our baseline, this prob-lem is still far from solved.Future work will include further analysis ofthe results obtained using larger manually anno-tated subsets as well as a detailed analysis of thecontributions of the different features in the fea-ture set described in Section 3.30AcknowledgementsThe authors wish to thank Gilad Mishne for hisinput and valuable comments during these ex-periments and the writing of this piece.Maarten de Rijke was supported by grantsfrom the Netherlands Organization for ScientificResearch (NWO) under project numbers017.001.190, 220-80-001, 264-70-050,  354-20-005, 612-13-001, 612.000.106, 612.000.207,612.066.302, 612.069.006, 640.001.501, and640.002.501.ReferencesG.
Mishne, M. de Rijke.
2006.
Capturing GlobalMood Levels using Blog Posts, In: AAAI 2006Spring Symposium on Computational Approachesto Analysing Weblogs (AAAI-CAAW 2006)E. Adar, L. Zhang, L. Adamic, R. Lukose.
2004.
Im-plicit Structure and the Dynamics of Blogspace, In:Workshop on the Weblogging Ecosystem, WWWConference, 2004T.
Nanno, Y. Suzuki, T. Fujiki, M. Okumura.
2004.Automatic Collection and Monitoring of JapaneseWeblogs, In: Proceeding of WWW2004: the 13thinternational World Wide Web conference, NewYork, NY, USA, 2004.
ACM Press.N.
Glance, M. Hurst, T. Tomokiyo.
2004.
BlogPulse:Automated Trend Discovery for Weblogs, In: Pro-ceeding of WWW2004: the 13th international WorldWide Web conference, New York, NY, USA, 2004.ACM Press.R.
Jones, A. McCallum, K. Nigam, and E. Riloff.1999.
Bootstrapping for Text Learning Tasks, In:IJCAI-99 Workshop on Text Mining: Foundations,Techniques and Applications, p52-63.A.
Blum, T. Mitchell.
1998.
Combining Labeled andUnlabeled Data with Co-Training, In: Proceedingsof the 1998 Conference on Computational Learn-ing Theory.I.
Witten, E. Frank.
2005.
Data Mining: Practicalmachine learning tools and techniques, 2nd Edi-tion, Morgan Kaufmann, San Francisco, 2005.Appendix A.
Full results of our binaryblog classification experimentAlgorithm Type Percentagecorrect predic-tionsNa?ve Bayes Bayes 90.07Na?ve Bayes Simple Bayes 89.64SMO SupportVector94.75IB1 Instancebased93.00KStar Instancebased93.30LWL Instancebased91.25BayesNet Bayes 90.08DecisionStump Tree 91.25J48 Tree 93.29ZeroR Rule-based 73.00DecisionTable Rule-based 92.55OneR Rule-based 87.60ConjunctiveRule Rule-based 88.75NNGe Rule-based 93.73PART Rule-based 91.67Ridor Rule-based 91.26JRip Rule-based 93.7331
