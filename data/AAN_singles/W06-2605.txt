Discourse Parsing: Learning FOL Rules based on Rich Verb SemanticRepresentations to automatically label Rhetorical RelationsRajen SubbaComputer ScienceUniversity of IllinoisChicago, IL, USArsubba@cs.uic.eduBarbara Di EugenioComputer ScienceUniversity of IllinoisChicago, IL, USAbdieugen@cs.uic.eduSu Nam KimDepartment of CSSEUniversity of MelbourneCarlton, VIC, Australiasnkim@csse.unimelb.edu.auAbstractWe report on our work to build a dis-course parser (SemDP) that uses seman-tic features of sentences.
We use an In-ductive Logic Programming (ILP) Systemto exploit rich verb semantics of clausesto induce rules for discourse parsing.
Wedemonstrate that ILP can be used to learnfrom highly structured natural languagedata and that the performance of a dis-course parsing model that only uses se-mantic information is comparable to thatof the state of the art syntactic discourseparsers.1 IntroductionThe availability of corpora annotated with syntac-tic information have facilitated the use of prob-abilistic models on tasks such as syntactic pars-ing.
Current state of the art syntactic parsersreach accuracies between 86% and 90%, as mea-sured by different types of precision and recall(for more details see (Collins, 2003)).
Recentsemantic (Kingsbury and Palmer, 2002) and dis-course (Carlson et al, 2003) annotation projectsare paving the way for developments in seman-tic and discourse parsing as well.
However unlikesyntactic parsing, significant development in dis-course parsing remains at large.Previous work on discourse parsing ((Soricutand Marcu, 2003) and (Forbes et al, 2001))have focused on syntactic and lexical featuresonly.
However, discourse relations connectclauses/sentences, hence, descriptions of eventsand states.
It makes linguistic sense that thesemantics of the two clauses ?generally builtaround the semantics of the verbs, composed withthat of their arguments?
affects the discourse re-lation(s) connecting the clauses.
This may beeven more evident in our instructional domain,where relations derived from planning such asPrecondition-Act may relate clauses.Of course, since semantic information is hardto come by, it is not surprising that previous workon discourse parsing did not use it, or only usedshallow word level ontological semantics as spec-ified in WordNet (Polanyi et al, 2004).
But whenrich sentence level semantics is available, it makessense to experiment with it for discourse parsing.A second major difficulty with using such richverb semantic information, is that it is rep-resented using complex data structures.
Tradi-tional Machine Learning methods cannot han-dle highly structured data such as First Or-der Logic (FOL), a representation that is suit-ably used to represent sentence level seman-tics.
Such FOL representations cannot be reducedto a vector of attribute/value pairs as the rela-tions/interdependencies that exist among the pred-icates would be lost.Inductive Logic Programming (ILP) can learnstructured descriptions since it learns FOL de-scriptions.
In this paper, we present our first stepsusing ILP to learn semantic descriptions of dis-course relations.
Also of relevance to the topic ofthis workshop, is that discourse structure is inher-ently highly structured, since discourse structureis generally described in hierarchical terms: ba-sic units of analysis, generally clauses, are relatedby discourse relations, resulting in more complexunits, which in turn can be related via discourse re-lations.
At the moment, we do not yet address theproblem of parsing at higher levels of discourse.We intend to build on the work we present in thispaper to achieve that goal.The task of discourse parsing can be di-vided into two disjoint sub-problems ((Soricut andMarcu, 2003) and (Polanyi et al, 2004)).
The twosub-problems are automatic identification of seg-ment boundaries and the labeling of rhetorical re-lations.
Though we consider the problem of auto-matic segmentation to be an important part in dis-course parsing, we have focused entirely on thelatter problem of automatically labeling rhetorical33Figure 1: SemDP System Architecture (Discourse Parser)relations only.
Our approach uses rich verb seman-tics1 of elementary discourse units (EDUs)2 basedon VerbNet(Kipper et al, 2000) as backgroundknowledge and manually annotated rhetorical re-lations as training examples.
It is trained on a lotfewer examples than the state of the art syntax-based discourse parser (Soricut and Marcu, 2003).Nevertheless, it achieves a comparable level ofperformance with an F-Score of 60.24.
Figure 1shows a block diagram of SemDP?s system archi-tecture.
Segmentation, annotation of rhetorical re-lations and parsing constitute the data collectionphase of the system.
Learning is accomplishedusing an ILP based system, Progol (Muggleton,1995).
As can be seen in Figure 1, Progol takesas input both rich verb semantic information ofpairs of EDUs and the rhetorical relations betweenthem.
The goal was to learn rules using the se-mantic information from pairs of EDUs as in Ex-ample 1:(1) EDU1: ?Sometimes, you can add a liquid to the waterEDU2: ?to hasten the process?relation(EDU1,EDU2,?Act:goal?
).to automatically label unseen examples with thecorrect rhetorical relation.The rest of the paper is organized as follows.Section 2 describes our data collection methodol-ogy.
In section 3, Progol, the ILP system that we1The semantic information we used is composed of Verb-Net semantic predicates that capture event semantics as wellas thematic roles.2EDUs are minimal discourse units produced as a resultof discourse segmentation.used to induce rules for discourse parsing is de-tailed.
Evaluation results are presented in section4 followed by the conclusion in section 5.2 Data CollectionThe lack of corpora annotated with both rhetoricalrelations as well as sentence level semantic rep-resentation led us to create our own corpus.
Re-sources such as (Kingsbury and Palmer, 2002) and(Carlson et al, 2003) have been developed man-ually.
Since such efforts are time consuming andcostly, we decided to semi-automatically build ourannotated corpus.
We used an existing corpus ofinstructional text that is about 9MB in size and ismade up entirely of written English instructions.The two largest components are home repair man-uals (5Mb) and cooking recipes (1.7Mb).
3Segmentation.
The segmentation of the corpuswas done manually by a human coder.
Our seg-mentation rules are based on those defined in(Mann and Thompson, 1988).
For example, (asshown in Example 2) we segment sentences inwhich a conjunction is used with a clause at theconjunction site.
(2) You can copy files (//) as well as cut messages.
(//) is the segmentation marker.
Sentences aresegmented into EDUs.
Not all the segmentation3It was collected opportunistically off the internet andfrom other sources, and originally assembled at the Informa-tion Technology Research Institute, University of Brighton.34rules from (Mann and Thompson, 1988) are im-ported into our coding scheme.
For example, wedo not segment relative clauses.
In total, our seg-mentation resulted in 10,084 EDUs.
The seg-mented EDUs were then annotated with rhetoricalrelations by the human coder4 and also forwardedto the parser as they had to be annotated with se-mantic information.2.1 Parsing of Verb SemanticsWe integrated LCFLEX (Rose?
and Lavie, 2000),a robust left-corner parser, with VerbNet (Kipperet al, 2000) and CoreLex (Buitelaar, 1998).
Ourinterest in decompositional theories of lexical se-mantics led us to base our semantic representationon VerbNet.VerbNet operationalizes Levin?s work and ac-counts for 4962 distinct verbs classified into 237main classes.
Moreover, VerbNet?s strong syntac-tic components allow it to be easily coupled with aparser in order to automatically generate a seman-tically annotated corpus.To provide semantics for nouns, we useCoreLex (Buitelaar, 1998), in turn based on thegenerative lexicon(Pustejovsky, 1991).
CoreLexdefines basic types such as art (artifact) or com(communication).
Nouns that share the same bun-dle of basic types are grouped in the same System-atic Polysemous Class (SPC).
The resulting 126SPCs cover about 40,000 nouns.We modified and augmented LCFLEX?s exist-ing lexicon to incorporate VerbNet and CoreLex.The lexicon is based on COMLEX (Grishman etal., 1994).
Verb and noun entries in the lexiconcontain a link to a semantic type defined in the on-tology.
VerbNet classes (including subclasses andframes) and CoreLex SPCs are realized as types inthe ontology.
The deep syntactic roles are mappedto the thematic roles, which are defined as vari-ables in the ontology types.
For more details onthe parser see (Terenzi and Di Eugenio, 2003).Each of the 10,084 EDUs was parsed using theparser.
The parser generates both a syntactic treeand the associated semantic representation ?
forthe purpose of this paper, we only focus on thelatter.
Figure 2 shows the semantic representationgenerated for EDU1 from Example 1, ?sometimes,you can add a liquid to the water?.The semantic representation in Figure 2 is part4Double annotation and segmentation is currently beingdone to assess inter-annotator agreement using kappa.
(*SEM*((AGENT YOU)(VERBCLASS ((VNCLASS MIX-22.1-2))) (EVENT +)(EVENT0((END((ARG1 (LIQUID))(FRAME *TOGETHER) (ARG0 PHYSICAL)(ARG2 (WATER)))))))(EVENTSEM((FRAME *CAUSE) (ARG1 E) (ARG0 (YOU)))))(PATIENT1 LIQUID)(PATIENT2 WATER)(ROOT-VERB ADD))Figure 2: Parser Output (Semantic Information)of the F-Structure produced by the parser.
Theverb add is parsed for a transitive frame with a PPmodifier that belongs to the verb class ?MIX-22.1-2?.
The sentence contains two PATIENTs, namelyliquid and water.
you is identified as the AGENTby the parser.
*TOGETHER and *CAUSE are theprimitive semantic predicates used by VerbNet.Verb Semantics in VerbNet are defined as eventsthat are decomposed into stages, namely start, end,during and result.
The semantic representation inFigure 2 states that there is an event EVENT0 inwhich the two PATIENTs are together at the end.An independent evaluation on a set of 200 sen-tences from our instructional corpus was con-ducted.
5 It was able to generate complete parsesfor 72.2% and partial parses for 10.9% of the verbframes that we expected it to parse, given the re-sources.
The parser cannot parse those sentences(or EDUs) that contain a verb that is not cov-ered by VerbNet.
This coverage issue, coupledwith parser errors, exacerbates the problem of datasparseness.
This is further worsened by the factthat we require both the EDUs in a relation setto be parsed for the Machine Learning part of ourwork.
Addressing data sparseness is an issue leftfor future work.2.2 Annotation of Rhetorical RelationsThe annotation of rhetorical relations was donemanually by a human coder.
Our coding schemebuilds on Relational Discourse Analysis (RDA)(Moser and Moore, 1995), to which we made mi-5The parser evaluation was not based on EDUs but ratheron unsegmented sentences.
A sentence contained one ormore EDUs.35nor modifications; in turn, as far as discourse rela-tions are concerned, RDA was inspired by Rhetor-ical Structure Theory (RST) (Mann and Thomp-son, 1988).Rhetorical relations were categorized as infor-mational, elaborational, temporal and others.
In-formational relations describe how contents intwo relata are related in the domain.
These re-lations are further subdivided into two groups;causality and similarity.
The former group con-sists of relations between an action and other ac-tions or between actions and their conditions oreffects.
Relations like ?act:goal?, ?criterion:act?fall under this group.
The latter group con-sists of relations between two EDUs accordingto some notion of similarity such as ?restate-ment?
and ?contrast1:contrast2?.
Elaborationalrelations are interpropositional relations in whicha proposition(s) provides detail relating to someaspect of another proposition (Mann and Thomp-son, 1988).
Relations like ?general:specific?
and?circumstance:situation?
belong to this category.Temporal relations like ?before:after?
capture timedifferences between two EDUs.
Lastly, the cate-gory others includes relations not covered by theprevious three categories such as ?joint?
and ?inde-terminate?.Based on the modified coding scheme manual,we segmented and annotated our instructional cor-pus using the augmented RST tool from (Marcu etal., 1999).
The RST tool was modified to incor-porate our relation set.
Since we were only inter-ested in rhetorical relations that spanned betweentwo adjacent EDUs 6, we obtained 3115 sets ofpotential relations from the set of all relations thatwe could use as training and testing data.The parser was able to provide complete parsesfor both EDUs in 908 of the 3115 relation sets.These constitute the training and test set for Pro-gol.The semantic representation for the EDUs alongwith the manually annotated rhetorical relationswere further processed (as shown in Figure 4) andused by Progol as input.3 The Inductive Logic ProgrammingFrameworkWe chose to use Progol, an Inductive Logic Pro-gramming system (ILP), to learn rules based on6At the moment, we are concerned with learning relationsbetween two EDUs at the base level of a Discourse Parse Tree(DPT) and not at higher levels of the hierarchy.the data we collected.
ILP is an area of researchat the intersection of Machine Learning (ML) andLogic Programming.
The general problem speci-fication in ILP is given by the following property:B ?H |= E (3)Given the background knowledge B and the ex-amples E, ILP systems find the simplest consistenthypothesis H, such that B and H entails E.While most of the work in NLP that involveslearning has used more traditional ML paradigmslike decision-tree algorithms and SVMs, we didnot find them suitable for our data which is rep-resented as Horn clauses.
The requirement of us-ing a ML system that could handle first order logicdata led us to explore ILP based systems of whichwe found Progol most appropriate.Progol combines Inverse Entailment withgeneral-to-specific search through a refinementgraph.
A most specific clause is derived usingmode declarations along with Inverse Entailment.All clauses that subsume the most specific clauseform the hypothesis space.
An A*-like searchis used to search for the most probable theorythrough the hypothesis space.
Progol allows arbi-trary programs as background knowledge and ar-bitrary definite clauses as examples.3.1 Learning from positive data onlyOne of the features we found appealing about Pro-gol, besides being able to handle first order logicdata, is that it can learn from positive examplesalone.Learning in natural language is a universal hu-man process based on positive data only.
How-ever, the usual traditional learning models do notwork well without negative examples.
On theother hand, negative examples are not easy to ob-tain.
Moreover, we found learning from positivedata only to be a natural way to model the task ofdiscourse parsing.To make the learning from positive data onlyfeasible, Progol uses a Bayesian framework.
Pro-gol learns logic programs with an arbitrarily lowexpected error using only positive data.
Of course,we could have synthetically labeled examples ofrelation sets (pairs of EDUs), that did not belongto a particular relation, as negative examples.
Weplan to explore this approach in the future.A key issue in learning from positive dataonly using a Bayesian framework is the abilityto learn complex logic programs.
Without any36negative examples, the simplest rule or logicprogram, which in our case would be a singledefinite clause, would be assigned the highestscore as it captures the most number of examples.In order to handle this problem, Progol?s scoringfunction exercises a trade-off between the size ofthe function and the generality of the hypothesis.The score for a given hypothesis is calculatedaccording to formula 4.ln p(H | E) = m ln( 1g(H))?sz(H)+dm (4)sz(H) and g(H) computes the size of the hy-pothesis and the its generality respectively.
Thesize of a hypothesis is measured as the numberof atoms in the hypothesis whereas generality ismeasured by the number of positive examples thehypothesis covers.
m is the number of examplescovered by the hypothesis and dm is a normaliz-ing constant.
The function ln p(H|E) decreaseswith increases in sz(H) and g(H).
As the numberof examples covered (m) grow, the requirementson g(H) become even stricter.
This property fa-cilitates the ability to learn more complex rulesas they are supported by more positive examples.For more information on Progol and the computa-tion of Bayes?
posterior estimation, please refer to(Muggleton, 1995).3.2 Discourse Parsing with ProgolWe model the problem of assigning the correctrhetorical relation as a classification task withinthe ILP framework.
The rich verb semantic repre-sentation of pairs of EDUs, as shown in Figure 3 7,form the background knowledge and the manuallyannotated rhetorical relations between the pairs ofEDUs, as shown in Figure 4, serve as the positiveexamples in our learning framework.
The num-bers in the definite clauses are ids used to identifythe EDUs.Progol constructs logic programs based on thebackground knowledge and the examples in Fig-ures 3 and 4.
Mode declarations in the Progol in-put file determines which clause to be used as thehead (i.e.
modeh) and which ones to be used inthe body (i.e.
modeb) of the hypotheses.
Figure 5shows an abridged set of our mode declarations.7The output from the parser was further processed intodefinite clauses....agent(97,you).together(97,event0,end,physical,liquid,water).cause(97,you,e).patient1(97,liquid).patient2(97,water).theme(98,process).rushed(98,event0,during,process).cause(98,AGENT98,e)....Figure 3: Background Knowledge for Example 1...relation(18,19,?Act:goal?).relation(97,98,?Act:goal?).relation(1279,1280,?Step1:step2?).relation(1300,1301,?Step1:step2?).relation(1310,1311,?Step1:step2?).relation(412,413,?Before:after?).relation(441,442,?Before:after?
)....Figure 4: Positive ExamplesOur mode declarations dictate that the predicaterelation be used as the head and the other pred-icates (has possession, transfer and visible) formthe body of the hypotheses.
?*?
indicates that thenumber of hypotheses to learn for a given relationis unlimited.
?+?
and ?-?
signs indicate variableswithin the predicates of which the former is an in-put variable and the latter an output variable.
?#?is used to denote a constant.
Each argument of thepredicate is a type, whether a constant or a vari-able.
Types are defined as a single definite clause.Our goal is to learn rules where the LHS of therule contains the relation that we wish to learn and:- modeh(*,relation(+edu,+edu,#relationtype))?
:- modeb(*,has possession(+edu,#event,#eventstage,+verbarg,+verbarg))?
:- modeb(*,has possession(+edu,#event,#eventstage,+verbarg,-verbarg))?
:- modeb(*,transfer(+edu,#event,#eventstage,-verbarg))?
:- modeb(*,visible(+edu,#event,#eventstage,+verbarg))?
:- modeb(*,together(+edu,#event,#eventstage,+verbarg,+verbarg,+verbarg))?
:- modeb(*,rushed(+edu,#event,#eventstage,+verbarg))?Figure 5: Mode Declarations37RULE1:relation(EDU1,EDU2,?Act:goal?)
:-degradation material integrity(EDU1,event0,result,C),allow(EDU2,event0,during,C,D).RULE2:relation(EDU1,EDU2,?Act:goal?)
:-cause(EDU1,C,D),together(EDU1,event0,end,E,F,G),cause(EDU2,C,D).RULE3:relation(EDU1,EDU2,?Step1:step2?)
:-together(EDU2,event0,end,C,D,E),has possession(EDU1,event0,during,C,F).RULE4:relation(EDU1,EDU2,?Before:after?)
:-motion(EDU1,event0,during,C),location(EDU2,event0,start,C,D).RULE6:relation(EDU1,EDU2,?Act:goal?)
:-motion(EDU1,event0,during,C).Figure 6: Rules Learnedthe RHS is a CNF of the semantic predicates de-fined in VerbNet with their arguments.
Given theamount of training data we have, the nature of thedata itself and the Bayesian framework used, Pro-gol learns simple rules that contain just one or twoclauses on the RHS.
6 of the 68 rules that Progolmanages to learn are shown in Figure 6.
RULE4states that there is a theme in motion during theevent in EDU A (which is the first EDU) and thatthe theme is located in location D at the start ofthe event in EDU B (the second EDU).
RULE2 islearned from pairs of EDUs such as in Example1.
The simple rules in Figure 6 may not readilyappeal to our intuitive notion of what such rulesshould include.
It is not clear at this point as tohow elaborate these rules should be, in order tocorrectly identify the relation in question.
Oneof the reasons why more complex rules are notlearned by Progol is that there aren?t enough train-ing examples.
As we add more training data in thefuture, we will see if rules that are more elaboratethan the ones in Figure 6 are learned .4 Evaluation of the Discourse ParserTable 1 shows the sets of relations for which wemanaged to obtain semantic representations (i.e.for both the EDUs).Relations like Preparation:act did not yield anyRelation Total Train TestSet SetStep1:step2: 232 188 44Joint: 190Goal:act: 170 147 23General:specific: 77Criterion:act: 53 46 7Before:after: 53 42 11Act:side-effect: 38Co-temp1:co-temp2: 22Cause:effect: 19Prescribe-act:wrong-act: 14Obstacle:situation: 11Reason:act: 9Restatement: 6Contrast1:contrast2: 6Circumstance:situation: 3Act:constraint: 2Criterion:wrong-act: 2Set:member: 1Act:justification: 0Comparison: 0Preparation:act: 0Object:attribute: 0Part:whole: 0Same-unit: 0Indeterminate: 0908 423 85Table 1: Relation Set Count (Total Counts include ex-amples that yielded semantic representations for both EDUs)examples that could potentially be used.
For anumber of relations, the total number of exampleswe could use were less than 50.
For the time being,we decided to use only those relation sets that hadmore than 50 examples.
In addition, we chose notto use Joint and General:specific relations.
Theywill be included in the future.
Hence, our trainingand testing data consisted of the following four re-lations: Goal:act, Step1:step2, Criterion:act andBefore:after.
The total number of examples weused was 508 of which 423 were used for trainingand 85 were used for testing.Table 2, Table 3 and Table 4 show the resultsfrom running the system on our test data.
A totalof 85 positive examples were used for testing thesystem.Table 2 evaluates our SemDP system against abaseline.
Our baseline is the majority function,which performs at a 51.7 F-Score.
SemDP outper-forms the baseline by almost 10 percentage points38Discourse Precision Recall F-ScoreParserSemDP 61.7 58.8 60.24Baseline* 51.7 51.7 51.7Table 2: Evaluation vs Baseline (* our baseline isthe majority function)Relation Precision Recall F-ScoreGoal:act 31.57 26.08 28.57Step1:step2 75 75 75Before:after 54.5 54.5 54.5Criterion:act 71.4 71.4 71.4Total 61.7 58.8 60.24Table 3: Test Results for SemDPwith an F-Score of 60.24.
To the best of ourknowledge, we are also not aware of any work thatuses rich semantic information for discourse pars-ing.
(Polanyi et al, 2004) do not provide any eval-uation results at all.
(Soricut and Marcu, 2003) re-port that their SynDP parser achieved up to 63.8 F-Score on human-segmented test data.
Our result of60.24 F-Score shows that a Discourse Parser basedpurely on semantics can perform as well.
How-ever, since the corpus, the size of training data andthe set of rhetorical relations we have used differfrom (Soricut and Marcu, 2003), a direct compar-ison cannot be made.Table 3 breaks down the results in detail foreach of the four rhetorical relations we tested on.Since we are learning from positive data only andthe rules we learn depend heavily on the amountof training data we have, we expected the systemto be more accurate with the relations that havemore training examples.
As expected, SemDP didvery well in labeling Step1:step2 relations.
Sur-prisingly though, it did not perform as well withGoal:act, even though it had the second highestnumber of training examples (147 in total).
In fact,SemDP misclassified more positive test examplesfor Goal:act than Before:after or Criterion:act, re-lations which had almost one third the number ofRelation Goal:act Step1:step2 Before:after Criterion:actGoal:act 6 8 5 0Step1:step2 6 33 5 0Before:after 0 4 6 1Criterion:act 0 0 2 5Table 4: Confusion Matrix for SemDP Test Resulttraining examples.
Overall SemDP achieved a pre-cision of 61.7 and a Recall of 58.8.In order to find out how the positive test exam-ples were misclassified, we investigated the dis-tribution of the relations classified by SemDP.
Ta-ble 4 is the confusion matrix that highlights thisissue.
A majority of the actual Goal:act relationsare incorrectly classified as Step1:step1 and Be-fore:after.
Likewise, most of the misclassificationof actual Step1:step1 seems to labeled as Goal:actor Before:after.
Such misclassification occurs be-cause the simple rules learned by SemDP are notable to accurately distinguish cases where positiveexamples of two different relations share similarsemantic predicates.
Moreover, since we are learn-ing using positive examples only, it is possible thata positive example may satisfy two or more rulesfor different relations.
In such cases, the rule thathas the highest score (as calculated by formula 4)is used to label the unseen example.5 Conclusions and Future WorkWe have shown that it is possible to learn First Or-der Logic rules from complex semantic data us-ing an ILP based methodology.
These rules canbe used to automatically label rhetorical relations.Moreover, our results show that a Discourse Parserthat uses only semantic information can performas well as the state of the art Discourse Parsersbased on syntactic and lexical information.Future work will involve the use of syntactic in-formation as well.
We also plan to run a more thor-ough evaluation on the complete set of relationsthat we have used in our coding scheme.
It is alsoimportant that the manual segmentation and an-notation of rhetorical relations be subject to inter-annotator agreement.
A second human annotatoris currently annotating a sample of the annotatedcorpus.
Upon completion, the annotated corpuswill be checked for reliability.Data sparseness is a well known problem inMa-chine Learning.
Like most paradigms, our learn-ing model is also affected by it.
We also plan toexplore techniques to deal with this issue.39Lastly, we have not tackled the problem of dis-course parsing at higher levels of the DPT and seg-mentation in this paper.
Our ultimate goal is tobuild a Discourse Parser that will automaticallysegment a full text as well as annotate it withrhetorical relations at every level of the DPT usingsemantic as well as syntactic information.
Muchwork needs to be done but we are excited to seewhat the aforesaid future work will yield.AcknowledgmentsThis work is supported by award 0133123 from the NationalScience Foundation.
Thanks to C.P.
Rose?
for LCFLEX, M.Palmer and K. Kipper for VerbNet, C. Buitelaar for CoreLex,and Stephen Muggleton for Progol.ReferencesPaul Buitelaar.
1998.
CoreLex: Systematic Polysemyand Underspecification.
Ph.D. thesis, Computer Science,Brandeis University, February.Lynn Carlson, Daniel Marcu, and Mary Ellen Okurowski.2003.
Building a discourse-tagged corpus in the frame-work of Rhetorical Structure Theory.
In Current Direc-tions in Discourse and Dialogue, pp.
85-112, Jan van Kup-pevelt and Ronnie Smith eds., Kluwer Academic Publish-ers.Michael Collins.
2003.
Head-driven statistical methods fornatural language parsing.
Computational Linguistics, 29.Katherine Forbes, Eleni Miltsakaki, Rashmi Prasad, AnoopSarkar, Aravind Joshi and Bonnie Webber.
2001.
D-LTAG System - Discourse Parsing with a Lexicalized TreeAdjoining Grammar.
Information Stucture, DiscourseStructure and Discourse Semantics, ESSLLI, 2001.Ralph Grishman, Catherine Macleod, and Adam Meyers.1994.
COMLEX syntax: Building a computational lex-icon.
In COLING 94, Proceedings of the 15th Interna-tional Conference on Computational Linguistics, pages472?477, Kyoto, Japan, August.Paul Kingsbury and Martha Palmer.
2000.
From Treebankto Propbank.
In Third International Conference on Lan-guage Resources and Evaluation, LREC-02, Las Palmas,Canary Islands, Spain, May 28 - June 3, 2002.Karin Kipper, Hoa Trang Dang, and Martha Palmer.
2000.Class-based construction of a verb lexicon.
In AAAI-2000,Proceedings of the Seventeenth National Conference onArtificial Intelligence, Austin, TX.Beth Levin and Malka Rappaport Hovav.
1992.
Wiping theslate clean: a lexical semantic exploration.
In Beth Levinand Steven Pinker, editors, Lexical and Conceptual Se-mantics, Special Issue of Cognition: International Journalof Cognitive Science.
Blackwell Publishers.William C. Mann and Sandra Thompson.
1988.
RhetoricalStructure Theory: toward a Functional Theory of Text Or-ganization.
Text, 8(3):243?281.Daniel Marcu and Abdessamad Echihabi.
2002.
An unsuper-vised approach to recognizing discourse relations.
In Pro-ceedings of the 40th Annual Meeting of the Association forComputational Linguistics (ACL-2002), Philadelphia, PA,July.Daniel Marcu, Magdalena Romera and Estibaliz Amorrortu.1999.
Experiments in Constructing a Corpus of DiscourseTrees: Problems, Annotation Choices, Issues.
In TheWorkshop on Levels of Representation in Discourse, pages71-78, Edinburgh, Scotland, July.M.
G. Moser, and J. D. Moore.
1995.
Using DiscourseAnalysis and Automatic Text Generation to Study Dis-course Cue Usage.
In AAAI Spring Symposium on Empir-ical Methods in Discourse Interpretation and Generation,1995.Stephen H. Muggleton.
1995.
Inverse Entailment and Pro-gol.
In New Generation Computing Journal, Vol.
13, pp.245-286, 1995.Martha Palmer, Daniel Gildea and, Paul Kingsbury.
2005.The Proposition Bank: An Annotated Corpus of SemanticRoles.
Computational Linguistics, 31(1):71?105.Livia Polanyi, Christopher Culy, Martin H. van den Berg,Gian Lorenzo Thione, and David Ahn.
2004.
Senten-tial Structure and Discourse Parsing.
Proceedings of theACL2004 Workshop on Discourse Annotation, Barcelona,Spain, July 25, 2004.James Pustejovsky.
1991.
The generative lexicon.
Computa-tional Linguistics, 17(4):409?441.Carolyn Penstein Rose?
and Alon Lavie.
2000.
Balancing ro-bustness and efficiency in unification-augmented context-free parsers for large practical applications.
In Jean-Clause Junqua and Gertjan van Noord, editors, Robustnessin Language and Speech Technology.
Kluwer AcademicPress.Radu Soricut and Daniel Marcu.
2003.
Sentence Level Dis-course Parsing using Syntactic and Lexical Information.In Proceedings of the Human Language Technology andNorth American Assiciation for Computational Linguis-tics Conference (HLT/NAACL-2003), Edmonton, Canada,May-June.Elena Terenzi and Barbara Di Eugenio.
2003.
Building lex-ical semantic representations for natural language instruc-tions.
In HLT-NAACL03, 2003 Human Language Tech-nology Conference, pages 100?102, Edmonton, Canada,May.
(Short Paper).40
