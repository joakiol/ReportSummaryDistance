Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 6?10,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsGood Seed Makes a Good Crop:Accelerating Active Learning Using Language ModelingDmitriy DligachDepartment of Computer ScienceUniversity of Colorado at BoulderDmitriy.Dligach@colorado.eduMartha PalmerDepartment of LinguisticsUniversity of Colorado at BoulderMartha.Palmer@colorado.eduAbstractActive Learning (AL) is typically initializedwith a small seed of examples selected ran-domly.
However, when the distribution ofclasses in the data is skewed, some classesmay be missed, resulting in a slow learningprogress.
Our contribution is twofold: (1) weshow that an unsupervised language modelingbased technique is effective in selecting rareclass examples, and (2) we use this techniquefor seeding AL and demonstrate that it leadsto a higher learning rate.
The evaluation isconducted in the context of word sense disam-biguation.1 IntroductionActive learning (AL) (Settles, 2009) has become apopular research field due to its potential benefits: itcan lead to drastic reductions in the amount of anno-tation that is necessary for training a highly accuratestatistical classifier.
Unlike in a random samplingapproach, where unlabeled data is selected for anno-tation randomly, AL delegates the selection of un-labeled data to the classifier.
In a typical AL setup,a classifier is trained on a small sample of the data(usually selected randomly), known as the seed ex-amples.
The classifier is subsequently applied to apool of unlabeled data with the purpose of selectingadditional examples that the classifier views as infor-mative.
The selected data is annotated and the cycleis repeated, allowing the learner to quickly refine thedecision boundary between the classes.Unfortunately, AL is susceptible to a shortcom-ing known as the missed cluster effect (Schu?tze etal., 2006) and its special case called the missed classeffect (Tomanek et al, 2009).
The missed cluster ef-fect is a consequence of the fact that seed examplesinfluence the direction the learner takes in its ex-ploration of the instance space.
Whenever the seeddoes not contain the examples of a certain clusterthat is representative of a group of examples in thedata, the learner may become overconfident aboutthe class membership of this cluster (particularly if itlies far from the decision boundary).
As a result, thelearner spends a lot of time exploring one region ofthe instance space at the expense of missing another.This problem can become especially severe, whenthe class distribution in the data is skewed: a ran-domly selected seed may not adequately representall the classes or even miss certain classes altogether.Consider a binary classification task where rare classexamples constitute 5% of the data (a frequent sce-nario in e.g.
word sense disambiguation).
If 10examples are chosen randomly for seeding AL, theprobability that none of the rare class examples willmake it to the seed is 60% 1.
Thus, there is a highprobability that AL would stall, selecting only theexamples of the predominant class over the courseof many iterations.
At the same time, if we had away to ensure that examples of the rare class werepresent in the seed, AL would be able to select theexamples of both classes, efficiently clarifying thedecision boundary and ultimately producing an ac-curate classifier.Tomanek et al (2009) simulated these scenariosusing manually constructed seed sets.
They demon-strated that seeding AL with a data set that is artifi-cially enriched with rare class examples indeed leadsto a higher learning rate comparing to randomly1Calculated using Binomial distribution6sampled and predominant class enriched seeds.
Inthis paper, we propose a simple automatic approachfor selecting the seeds that are rich in the examplesof the rare class.
We then demonstrate that this ap-proach to seed selection accelerates AL.
Finally, weanalyze the mechanism of this acceleration.2 ApproachLanguage Model (LM) Sampling is a simple unsu-pervised technique for selecting unlabeled data thatis enriched with rare class examples.
LM samplinginvolves training a LM on a corpus of unlabeled can-didate examples and selecting the examples with lowLM probability.
Dligach and Palmer (2009) usedthis technique in the context of word sense disam-biguation and showed that rare sense examples tendto concentrate among the examples with low prob-ability.
Unfortunately these authors provided a lim-ited evaluation of this technique: they looked at itseffectiveness only at a single selection size.
We pro-vide a more convincing evaluation in which the ef-fectiveness of this approach is examined for all sizesof the selected data.Seed Selection for AL is typically done ran-domly.
However, for datasets with a skewed dis-tribution of classes, rare class examples may endup being underrepresented.
We propose to use LMsampling for seed selection, which captures moreexamples of rare classes than random selection, thusleading to a faster learning progress.3 Evaluation3.1 DataFor our evaluation, we needed a dataset that ischaracterized by a skewed class distribution.
Thisphenomenon is pervasive in word sense data.
Alarge word sense annotated corpus has recentlybeen released by the OntoNotes (Hovy et al, 2006;Weischedel et al, 2009) project.
For clarity of eval-uation, we identify a set of verbs that satisfy threecriteria: (1) the number of senses is two, (2) thenumber of annotated examples is at least 100, (3) theproportion of the rare sense is at most 20%.
The fol-lowing 25 verbs satisfy these criteria: account, add,admit, allow, announce, approve, compare, demand,exist, expand, expect, explain, focus, include, invest,issue, point, promote, protect, receive, remain, re-place, strengthen, wait, wonder.
The average num-ber of examples for these verbs is 232.
In supervisedword sense disambiguation, a single model per wordis typically trained and that is the approach we take.Thus, we conduct our evaluation using 25 differentdata sets.
We report the averages across these 25data sets.
In our evaluation, we use a state-of-the-art word sense disambiguation system (Dligach andPalmer, 2008), that utilizes rich linguistic features tocapture the contexts of ambiguous words.3.2 Rare Sense RetrievalThe success of our approach to seeding AL hingeson the ability of LM sampling to discover rare classexamples better than random sampling.
In this ex-periment, we demonstrate that LM sampling outper-forms random sampling for every selection size.
Foreach verb we conduct an experiment in which weselect the instances of this verb using both methods.We measure the recall of the rare sense, which wecalculate as the ratio of the number of selected raresense examples to the total number of rare sense ex-amples for this verb.We train a LM (Stolcke, 2002) on the corporafrom which OntoNotes data originates: the WallStreet Journal, English Broadcast News, EnglishConversation, and the Brown corpus.
For each verb,we compute the LM probability for each instance ofthis verb and sort the instances by probability.
Inthe course of the experiment, we select one examplewith the smallest probability and move it to the setof selected examples.
We then measure the recall ofthe rare sense for the selected examples.
We con-tinue in this fashion until all the examples have beenselected.
We use random sampling as a baseline,which is obtained by continuously selecting a singleexample randomly.
We continue until all the exam-ples have been selected.
At the end of the exper-iment, we have produced two recall curves, whichmeasure the recall of the rare sense retrieval for thisverb at various sizes of selected data.
Due to thelack of space, we do not show the plots that displaythese curves for individual verbs.
Instead, in Figure1 we display the curves that are averaged across allverbs.
At every selection size, LM sampling resultsin a higher recall of the rare sense.
The average dif-ference across all selection sizes is 11%.7Figure 1: Average recall of rare sense retrieval for LMand random sampling by relative size of training set3.3 Classic and Selectively Seeded ALIn this experiment, we seed AL using LM samplingand compare how this selectively seeded AL per-forms in comparison with classic (randomly-seeded)AL.
Our experimental setup is typical for an activelearning study.
We split the set of annotated exam-ples for a verb into 90% and 10% parts.
The 90%part is used as a pool of unlabeled data.
The 10%part is used as a test set.
We begin classic AL byrandomly selecting 10% of the examples from thepool to use as seeds.
We train a maximum entropymodel (Le, 2004) using these seeds.
We then repeat-edly apply the model to the remaining examples inthe pool: on each iteration of AL, we draw a sin-gle most informative example from the pool.
Theinformativeness is estimated using prediction mar-gin (Schein and Ungar, 2007), which is computed as|P (c1|x) ?
P (c2|x)|, where c1 and c2 are the twomost probable classes of example x according to themodel.
The selected example is moved to the train-ing set.
On each iteration, we also keep track of howaccurately the current model classifies the held outtest set.In parallel, we conduct a selectively seeded ALexperiment that is identical to the classic one butwith one crucial difference: instead of selecting theseed examples randomly, we select them using LMsampling by identifying 10% of the examples fromthe pool with the smallest LM probability.
We alsoproduce a random sampling curve to be used as abaseline.
At the end of this experiment we have ob-tained three learning curves: for classic AL, for se-lectively seeded AL, and for the random samplingbaseline.
The final learning curves for each verb areproduced by averaging the learning curves from tendifferent trials.Figure 2 presents the average accuracy of selec-tively seeded AL (top curve), classic AL (middlecurve) and the random sampling baseline (bottomcurve) at various fractions of the total size of thetraining set.
The size of zero corresponds to a train-ing set consisting only of the seed examples.
Thesize of one corresponds to a training set consistingof all the examples in the pool labeled.
The accuracyat a given size was averaged across all 25 verbs.It is clear that LM-seeded AL accelerates learn-ing: it reaches the same performance as classic ALwith less training data.
LM-seeded AL also reachesa higher classification accuracy (if stopped at itspeak).
We will analyze this somewhat surprising be-havior in the next section.
The difference betweenthe classic and LM-seeded curves is statistically sig-nificant (p = 0.0174) 2.Figure 2: Randomly and LM-seeded AL.
Random sam-pling baseline is also shown.3.4 Why LM Seeding Produces Better ResultsFor random sampling, the system achieves its bestaccuracy, 94.4%, when the entire pool of unlabeledexamples is labeled.
The goal of a typical AL studyis to demonstrate that the same accuracy can be2We compute the average area under the curve for each typeof AL and use Wilcoxon signed rank test to test whether thedifference between the averages is significant.8achieved with less labeled data.
For example, in ourcase, classic AL reaches the best random samplingaccuracy with only about 5% of the data.
However,it is interesting to notice that LM-seeded AL actuallyreaches a higher accuracy, 95%, during early stagesof learning (at 15% of the total training set size).
Webelieve this phenomenon takes place due to overfit-ting the predominant class: as the model receivesnew data (and therefore more and more examples ofthe predominant class), it begins to mislabel moreand more examples of the rare class.
A similar ideahas been expressed in literature (Weiss, 1995; Kubatand Matwin, 1997; Japkowicz, 2001; Weiss, 2004;Chen et al, 2006), however it has never been veri-fied in the context of AL.To verify our hypothesis, we conduct an experi-ment.
The experimental setup is the same as in sec-tion 3.3.
However, instead of measuring the accu-racy on the test set, we resort to different metricsthat reflect how accurately the classifier labels the in-stances of the rare class in the held out test set.
Thesemetrics are the recall and precision for the rare class.Recall is the ratio of the correctly labeled examplesof the rare class and the total number of instances ofthe rare class.
Precision is the ratio of the correctlylabeled examples of the rare class and the number ofinstances labeled as that class.
Results are in Figures3 and 4.Figure 3: Rare sense classification recallObserve that for LM-seeded AL, the recall peaksat first and begins to decline later.
Thus the clas-sifier makes progressively more errors on the rareclass as more labeled examples are being received.Figure 4: Rare sense classification precisionThis is consistent with our hypothesis that the clas-sifier overfits the predominant class.
When all thedata is labeled, the recall decreases from about 13%to only 7%, an almost 50% drop.
The reason thatthe system achieved a higher level of recall at first isdue to the fact that AL was seeded with LM selecteddata, which has a higher content of rare classes (aswe demonstrated in the first experiment).
The avail-ability of the extra examples of the rare class allowsthe classifier to label the instances of this class inthe test set more accurately, which in turn boosts theoverall accuracy.4 Conclusion and Future WorkWe introduced a novel approach to seeding AL, inwhich the seeds are selected from the examples withlow LM probability.
This approach selects more rareclass examples than random sampling, resulting inmore rapid learning and, more importantly, leadingto a classifier that performs better on rare class ex-amples.
As a consequence of this, the overall classi-fication accuracy is higher than that for classic AL.Our plans for future work include improving ourLM by incorporating syntactic information such asPOS tags.
This should result in better performanceon the rare classes, which is currently still low.We also plan to experiment with other unsupervisedtechniques, such as clustering and outlier detection,that can lead to better retrieval of rare classes.
Fi-nally, we plan to investigate the applicability of ourapproach to a multi-class scenario.9AcknowledgementsWe gratefully acknowledge the support of the Na-tional Science Foundation Grant NSF-0715078,Consistent Criteria for Word Sense Disambiguation,and the GALE program of the Defense AdvancedResearch Projects Agency, Contract No.
HR0011-06-C-0022, a subcontract from the BBN-AGILETeam.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the authors and do not necessarily reflectthe views of the National Science Foundation.ReferencesJinying Chen, Andrew Schein, Lyle Ungar, and MarthaPalmer.
2006.
An empirical study of the behaviorof active learning for word sense disambiguation.
InProceedings of the main conference on Human Lan-guage Technology Conference of the North AmericanChapter of the Association of Computational Linguis-tics, pages 120?127, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Dmitriy Dligach and Martha Palmer.
2008.
Novel se-mantic features for verb sense disambiguation.
InHLT ?08: Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics onHuman Language Technologies, pages 29?32, Morris-town, NJ, USA.
Association for Computational Lin-guistics.Dmitriy Dligach and Martha.
Palmer.
2009.
Using lan-guage modeling to select useful annotation data.
InProceedings of Human Language Technologies: The2009 Annual Conference of the North American Chap-ter of the Association for Computational Linguistics,Companion Volume: Student Research Workshop andDoctoral Consortium, pages 25?30.
Association forComputational Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% solution.
In NAACL ?06: Proceedings ofthe Human Language Technology Conference of theNAACL, Companion Volume: Short Papers on XX,pages 57?60, Morristown, NJ, USA.
Association forComputational Linguistics.Nathalie Japkowicz.
2001.
Concept-learning in the pres-ence of between-class and within-class imbalances.
InAI ?01: Proceedings of the 14th Biennial Conferenceof the Canadian Society on Computational Studiesof Intelligence, pages 67?77, London, UK.
Springer-Verlag.M.
Kubat and S. Matwin.
1997.
Addressing the curse ofimbalanced training sets: one-sided selection.
In Pro-ceedings of the Fourteenth International Conferenceon Machine Learning, pages 179?186.
Citeseer.Zhang Le, 2004.
Maximum Entropy Modeling Toolkit forPython and C++.A.I.
Schein and L.H.
Ungar.
2007.
Active learning forlogistic regression: an evaluation.
Machine Learning,68(3):235?265.H.
Schu?tze, E. Velipasaoglu, and J.O.
Pedersen.
2006.Performance thresholding in practical text classifica-tion.
In Proceedings of the 15th ACM internationalconference on Information and knowledge manage-ment, pages 662?671.
ACM.Burr Settles.
2009.
Active learning literature survey.
InComputer Sciences Technical Report 1648 Universityof Wisconsin-Madison.Andreas Stolcke.
2002.
Srilm - an extensible languagemodeling toolkit.
In International Conference on Spo-ken Language Processing, Denver, Colorado., pages901?904.Katrin Tomanek, Florian Laws, Udo Hahn, and HinrichSchu?tze.
2009.
On proper unit selection in activelearning: co-selection effects for named entity recog-nition.
In HLT ?09: Proceedings of the NAACL HLT2009 Workshop on Active Learning for Natural Lan-guage Processing, pages 9?17, Morristown, NJ, USA.Association for Computational Linguistics.R.
Weischedel, E. Hovy, M. Marcus, M. Palmer,R Belvin, S Pradan, L. Ramshaw, and N. Xue, 2009.OntoNotes: A Large Training Corpus for EnhancedProcessing, chapter in Global Automatic LanguageExploitation, pages 54?63.
Springer Verglag.G.M.
Weiss.
1995.
Learning with rare cases and smalldisjuncts.
In Proceedings of the Twelfth InternationalConference on Machine Learning, pages 558?565.Citeseer.G.M.
Weiss.
2004.
Mining with rarity: a unifyingframework.
ACM SIGKDD Explorations Newsletter,6(1):7?19.10
