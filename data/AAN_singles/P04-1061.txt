Corpus-Based Induction of Syntactic Structure:Models of Dependency and ConstituencyDan KleinComputer Science DepartmentStanford UniversityStanford, CA 94305-9040klein@cs.stanford.eduChristopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040manning@cs.stanford.eduAbstractWe present a generative model for the unsupervisedlearning of dependency structures.
We also describethe multiplicative combination of this dependency modelwith a model of linear constituency.
The product modeloutperforms both components on their respective evalu-ation metrics, giving the best published figures for un-supervised dependency parsing and unsupervised con-stituency parsing.
We also demonstrate that the com-bined model works and is robust cross-linguistically, be-ing able to exploit either attachment or distributional reg-ularities that are salient in the data.1 IntroductionThe task of statistically inducing hierarchical syn-tactic structure over unannotated sentences of nat-ural language has received a great deal of atten-tion (Carroll and Charniak, 1992; Pereira and Sch-abes, 1992; Brill, 1993; Stolcke and Omohundro,1994).
Researchers have explored this problem fora variety of reasons: to argue empirically againstthe poverty of the stimulus (Clark, 2001), to useinduction systems as a first stage in constructinglarge treebanks (van Zaanen, 2000), to build betterlanguage models (Baker, 1979; Chen, 1995), andto examine cognitive issues in language learning(Solan et al, 2003).
An important distinction shouldbe drawn between work primarily interested in theweak generative capacity of models, where model-ing hierarchical structure is only useful insofar as itleads to improved models over observed structures(Baker, 1979; Chen, 1995), and work interested inthe strong generative capacity of models, where theunobserved structure itself is evaluated (van Zaa-nen, 2000; Clark, 2001; Klein and Manning, 2002).This paper falls into the latter category; we will beinducing models of linguistic constituency and de-pendency with the goal of recovering linguisticallyplausible structures.
We make no claims as to thecognitive plausibility of the induction mechanismswe present here; however, the ability of these sys-tems to recover substantial linguistic patterns fromsurface yields alone does speak to the strength ofsupport for these patterns in the data, and hence un-dermines arguments based on ?the poverty of thestimulus?
(Chomsky, 1965).2 Unsupervised Dependency ParsingMost recent progress in unsupervised parsing hascome from tree or phrase-structure grammar basedmodels (Clark, 2001; Klein and Manning, 2002),but there are compelling reasons to reconsider un-supervised dependency parsing.
First, most state-of-the-art supervised parsers make use of specific lexi-cal information in addition to word-class level infor-mation ?
perhaps lexical information could be a use-ful source of information for unsupervised methods.Second, a central motivation for using tree struc-tures in computational linguistics is to enable theextraction of dependencies ?
function-argument andmodification structures ?
and it might be more ad-vantageous to induce such structures directly.
Third,as we show below, for languages such as Chinese,which have few function words, and for which thedefinition of lexical categories is much less clear,dependency structures may be easier to detect.2.1 Representation and EvaluationAn example dependency representation of a shortsentence is shown in figure 1(a), where, follow-ing the traditional dependency grammar notation,the regent or head of a dependency is marked withthe tail of the dependency arrow, and the dependentis marked with the arrowhead (Mel?c?uk, 1988).
Itwill be important in what follows to see that sucha representation is isomorphic (in terms of stronggenerative capacity) to a restricted form of phrasestructure grammar, where the set of terminals andnonterminals is identical, and every rule is of theform X ?
X Y or X ?
Y X (Miller, 1999), givingthe isomorphic representation of figure 1(a) shownin figure 1(b).1 Depending on the model, part-of-1Strictly, such phrase structure trees are isomorphic not toflat dependency structures, but to specific derivations of thoseNNFactoryNNSpayrollsVBDfellINinNNSeptemberROOTVBDNNSNNFactoryNNSpayrollsVBDVBDfellININinNNSeptemberSNPNNFactoryNNSpayrollsVPVBDfellPPINinNNSeptember(a) Classical Dependency Structure (b) Dependency Structure as CF Tree (c) CFG StructureFigure 1: Three kinds of parse structures.speech categories may be included in the depen-dency representation, as shown here, or dependen-cies may be directly between words.
Below, we willassume an additonal reserved nonterminal ROOT,whose sole dependent is the head of the sentence.This simplifies the notation, math, and the evalua-tion metric.A dependency analysis will always consist of ex-actly as many dependencies as there are words in thesentence.
For example, in the dependency structureof figure 1(b), the dependencies are {(ROOT, fell),(fell, payrolls), (fell, in), (in, September), (payrolls,Factory)}.
The quality of a hypothesized depen-dency structure can hence be evaluated by accuracyas compared to a gold-standard dependency struc-ture, by reporting the percentage of dependenciesshared between the two analyses.In the next section, we discuss several models ofdependency structure, and throughout this paper wereport the accuracy of various methods at recover-ing gold-standard dependency parses from variouscorpora, detailed here.
WSJ is the entire Penn En-glish Treebank WSJ portion.
WSJ10 is the subsetof sentences which contained 10 words or less afterthe removal of punctuation.
CTB10 is the sentencesof the same length from the Penn Chinese treebank(v3).
NEGRA10 is the same, for the German NE-GRA corpus, based on the supplied conversion ofthe NEGRA corpus into Penn treebank format.
Inmost of the present experiments, the provided parts-of-speech were used as the input alphabet, thoughwe also present limited experimentation with syn-thetic parts-of-speech.It is important to note that the Penn treebanks donot include dependency annotations; however, theautomatic dependency rules from (Collins, 1999)are sufficiently accurate to be a good benchmarkfor unsupervised systems for the time being (thoughsee below for specific issues).
Similar head-findingrules were used for Chinese experiments.
The NE-GRA corpus, however, does supply hand-annotateddependency structures.structures which specify orders of attachment among multipledependents which share a common head.?
?
?
?
?
ROOTFigure 2: Dependency graph with skeleton chosen, butwords not populated.Where possible, we report an accuracy figure forboth directed and undirected dependencies.
Report-ing undirected numbers has two advantages: first, itfacilitates comparison with earlier work, and, moreimportantly, it allows one to partially obscure theeffects of alternate analyses, such as the system-atic choice between a modal and a main verb forthe head of a sentence (in either case, the two verbswould be linked, but the direction would vary).2.2 Dependency ModelsThe dependency induction task has received rela-tively little attention; the best known work is Car-roll and Charniak (1992), Yuret (1998), and Paskin(2002).
All systems that we are aware of operate un-der the assumption that the probability of a depen-dency structure is the product of the scores of thedependencies (attachments) in that structure.
De-pendencies are seen as ordered (head, dependent)pairs of words, but the score of a dependency canoptionally condition on other characteristics of thestructure, most often the direction of the depen-dency (whether the arrow points left or right).Some notation before we present specific mod-els: a dependency d is a pair ?h, a?
of a head andargument, which are words in a sentence s, in a cor-pus S. For uniformity of notation with section 4,words in s are specified as size-one spans of s: forexample the first word would be 0s1.
A dependencystructure D over a sentence is a set of dependencies(arcs) which form a planar, acyclic graph rooted atthe special symbol ROOT, and in which each wordin s appears as an argument exactly once.
For a de-pendency structure D, there is an associated graphG which represents the number of words and arrowsbetween them, without specifying the words them-selves (see figure 2).
A graph G and sentence s to-gether thus determine a dependency structure.
TheModel Dir.
Undir.English (WSJ)Paskin 01 39.7RANDOM 41.7Charniak and Carroll 92-inspired 44.7ADJACENT 53.2DMV 54.4English (WSJ10)RANDOM 30.1 45.6ADJACENT 33.6 56.7DMV 43.2 63.7German (NEGRA10)RANDOM 21.8 41.5ADJACENT 32.6 51.2DMV 36.3 55.8Chinese (CTB10)RANDOM 35.9 47.3ADJACENT 30.2 47.3DMV 42.5 54.2Figure 3: Parsing performance (directed and undirecteddependency accuracy) of various dependency models onvarious treebanks, along with baselines.dependency structure is the object generated by allof the models that follow; the steps in the deriva-tions vary from model to model.Existing generative dependency models intendedfor unsupervised learning have chosen to first gen-erate a word-free graph G, then populate the sen-tence s conditioned on G. For instance, the model ofPaskin (2002), which is broadly similar to the semi-probabilistic model in Yuret (1998), first chooses agraph G uniformly at random (such as figure 2),then fills in the words, starting with a fixed rootsymbol (assumed to be at the rightmost end), andworking down G until an entire dependency struc-ture D is filled in (figure 1a).
The correspondingprobabilistic model isP(D) = P(s, G)= P(G)P(s|G)= P(G)?
(i, j,dir)?GP(i?1si | j?1s j , dir) .In Paskin (2002), the distribution P(G) is fixed to beuniform, so the only model parameters are the con-ditional multinomial distributions P(a|h, dir) thatencode which head words take which other wordsas arguments.
The parameters for left and right ar-guments of a single head are completely indepen-dent, while the parameters for first and subsequentarguments in the same direction are identified.In those experiments, the model above wastrained on over 30M words of raw newswire, usingEM in an entirely unsupervised fashion, and at greatcomputational cost.
However, as shown in figure 3,the resulting parser predicted dependencies at be-low chance level (measured by choosing a randomdependency structure).
This below-random perfor-mance seems to be because the model links wordpairs which have high mutual information (suchas occurrences of congress and bill) regardless ofwhether they are plausibly syntactically related.
Inpractice, high mutual information between words isoften stronger between two topically similar nounsthan between, say, a preposition and its object.One might hope that the problem with this modelis that the actual lexical items are too semanti-cally charged to represent workable units of syn-tactic structure.
If one were to apply the Paskin(2002) model to dependency structures parameter-ized simply on the word-classes, the result wouldbe isomorphic to the ?dependency PCFG?
modelsdescribed in Carroll and Charniak (1992).
In thesemodels, Carroll and Charniak considered PCFGswith precisely the productions (discussed above)that make them isomorphic to dependency gram-mars, with the terminal alphabet being simply parts-of-speech.
Here, the rule probabilities are equiva-lent to P(Y|X, right) and P(Y|X, left) respectively.2The actual experiments in Carroll and Charniak(1992) do not report accuracies that we can compareto, but they suggest that the learned grammars wereof extremely poor quality.
With hindsight, however,the main issue in their experiments appears to be nottheir model, but that they randomly initialized theproduction (attachment) probabilities.
As a result,their learned grammars were of very poor qualityand had high variance.
However, one nice propertyof their structural constraint, which all dependencymodels share, is that the symbols in the grammar arenot symmetric.
Even with a grammar in which theproductions are initially uniform, a symbol X canonly possibly have non-zero posterior likelihoodover spans which contain a matching terminal X.Therefore, one can start with uniform rewrites andlet the interaction between the data and the modelstructure break the initial symmetry.
If one recaststheir experiments in this way, they achieve an accu-racy of 44.7% on the Penn treebank, which is higherthan choosing a random dependency structure, butlower than simply linking all adjacent words into aleft-headed (and right-branching) structure (53.2%).A huge limitation of both of the above models isthat they are incapable of encoding even first-ordervalence facts.
For example, the latter model learnsthat nouns to the left of the verb (usually subjects)2There is another, subtle distinction: in the Paskin work,a canonical ordering of multiple attachments was fixed, whilein the Carroll and Charniak work all attachment orders are con-sidered, giving a numerical bias towards structures where headstake more than one argument.ihjdaekhidaejhekheihjheSTOPihejdheSTOP(a) (b) (c) (d)Figure 4: Dependency configurations in a lexicalized tree: (a) right attachment, (b) left attachment, (c) right stop, (d)left stop.
h and a are head and argument words, respectively, while i , j , and k are positions between words.attach to the verb.
But then, given a NOUN NOUNVERB sequence, both nouns will attach to the verb?
there is no way that the model can learn that verbshave exactly one subject.
We now turn to an im-proved dependency model that addresses this prob-lem.3 An Improved Dependency ModelThe dependency models discussed above are dis-tinct from dependency models used inside high-performance supervised probabilistic parsers in sev-eral ways.
First, in supervised models, a head out-ward process is modeled (Eisner, 1996; Collins,1999).
In such processes, heads generate a sequenceof arguments outward to the left or right, condition-ing on not only the identity of the head and direc-tion of the attachment, but also on some notion ofdistance or valence.
Moreover, in a head-outwardmodel, it is natural to model stop steps, where thefinal argument on each side of a head is always thespecial symbol STOP.
Models like Paskin (2002)avoid modeling STOP by generating the graph skele-ton G first, uniformly at random, then populatingthe words of s conditioned on G. Previous work(Collins, 1999) has stressed the importance of in-cluding termination probabilities, which allows thegraph structure to be generated jointly with the ter-minal words, precisely because it does allow themodeling of required dependents.We propose a simple head-outward dependencymodel over word classes which includes a modelof valence, which we call DMV (for dependencymodel with valence).
We begin at the ROOT.
In thestandard way, each head generates a series of non-STOP arguments to one side, then a STOP argumentto that side, then non-STOP arguments to the otherside, then a second STOP.For example, in the dependency structure in fig-ure 1, we first generate a single child of ROOT, herefell.
Then we recurse to the subtree under fell.
Thissubtree begins with generating the right argumentin.
We then recurse to the subtree under in (gener-ating September to the right, a right STOP, and a leftSTOP).
Since there are no more right arguments af-ter in, its right STOP is generated, and the processmoves on to the left arguments of fell.In this process, there are two kinds of deriva-tion events, whose local probability factors consti-tute the model?s parameters.
First, there is the de-cision at any point whether to terminate (generateSTOP) or not: PSTOP(STOP|h, dir, ad j).
This is a bi-nary decision conditioned on three things: the headh, the direction (generating to the left or right ofthe head), and the adjacency (whether or not an ar-gument has been generated yet in the current di-rection, a binary variable).
The stopping decisionis estimated directly, with no smoothing.
If a stopis generated, no more arguments are generated forthe current head to the current side.
If the currenthead?s argument generation does not stop, anotherargument is chosen using: PCHOOSE(a|h, dir).
Here,the argument is picked conditionally on the iden-tity of the head (which, recall, is a word class) andthe direction.
This term, also, is not smoothed inany way.
Adjacency has no effect on the identityof the argument, only on the likelihood of termina-tion.
After an argument is generated, its subtree inthe dependency structure is recursively generated.Formally, for a dependency structure D, leteach word h have left dependents depsD(h, l)and right dependents depsD(h, r).
The follow-ing recursion defines the probability of the frag-ment D(h) of the dependency tree rooted at h:P(D(h)) =?dir?
{l,r}?a?depsD(h,dir)PSTOP(?STOP|h, dir, ad j)PCHOOSE(a|h, dir)P(D(a))PSTOP(STOP|h, dir, ad j)One can view a structure generated by this deriva-tional process as a ?lexicalized?
tree composed ofthe local binary and unary context-free configura-tions shown in figure 4.3 Each configuration equiv-alently represents either a head-outward derivationstep or a context-free rewrite rule.
There are foursuch configurations.
Figure 4(a) shows a head h3It is lexicalized in the sense that the labels in the tree arederived from terminal symbols, but in our experiments the ter-minals were word classes, not individual lexical items.taking a right argument a.
The tree headed by hcontains h itself, possibly some right arguments ofh, but no left arguments of h (they attach after allthe right arguments).
The tree headed by a containsa itself, along with all of its left and right children.Figure 4(b) shows a head h taking a left argument a?
the tree headed by h must have already generatedits right stop to do so.
Figure 4(c) and figure 4(d)show the sealing operations, where STOP derivationsteps are generated.
The left and right marks onnode labels represent left and right STOPs that havebeen generated.4The basic inside-outside algorithm (Baker, 1979)can be used for re-estimation.
For each sentences ?
S, it gives us cs(x : i, j), the expected frac-tion of parses of s with a node labeled x extend-ing from position i to position j .
The model canbe re-estimated from these counts.
For example, tore-estimate an entry of PSTOP(STOP|h, left, non-adj)according to a current model 2, we calculate twoquantities.5 The first is the (expected) number oftrees headed by he whose rightmost edge i is strictlyleft of h. The second is the number of trees headedby dhe with rightmost edge i strictly left of h. Theratio is the MLE of that local probability factor:PSTOP(STOP|h, left, non-adj) =?s?S?i<loc(h)?k c(he : i, k)?s?S?i<loc(h)?k c(dhe : i, k)This can be intuitively thought of as the relativenumber of times a tree headed by h had alreadytaken at least one argument to the left, had an op-portunity to take another, but didn?t.6Initialization is important to the success of anylocal search procedure.
We chose to initialize EMnot with an initial model, but with an initial guessat posterior distributions over dependency structures(completions).
For the first-round, we constructeda somewhat ad-hoc ?harmonic?
completion whereall non-ROOT words took the same number of ar-guments, and each took other words as argumentsin inverse proportion to (a constant plus) the dis-tance between them.
The ROOT always had a single4Note that the asymmetry of the attachment rules enforcesthe right-before-left attachment convention.
This is harmlessand arbitrary as far as dependency evaluations go, but imposesan x-bar-like structure on the constituency assertions made bythis model.
This bias/constraint is dealt with in section 5.5To simplify notation, we assume each word h occurs atmost one time in a given sentence, between indexes loc(h) andloc(h) + 1).6As a final note, in addition to enforcing the right-argument-first convention, we constrained ROOT to have at most a singledependent, by a similar device.argument and took each word with equal probabil-ity.
This structure had two advantages: first, whentesting multiple models, it is easier to start them alloff in a common way by beginning with an M-step,and, second, it allowed us to point the model in thevague general direction of what linguistic depen-dency structures should look like.On the WSJ10 corpus, the DMV model recov-ers a substantial fraction of the broad dependencytrends: 43.2% of guessed directed dependencieswere correct (63.7% ignoring direction).
To ourknowledge, this is the first published result to breakthe adjacent-word heuristic (at 33.6% for this cor-pus).
Verbs are the sentence heads, prepositionstake following noun phrases as arguments, adverbsattach to verbs, and so on.
The most common sourceof discrepancy between the test dependencies andthe model?s guesses is a result of the model system-atically choosing determiners as the heads of nounphrases, while the test trees have the rightmost nounas the head.
The model?s choice is supported bya good deal of linguistic research (Abney, 1987),and is sufficiently systematic that we also report thescores where the NP headship rule is changed to per-colate determiners when present.
On this adjustedmetric, the score jumps hugely to 55.7% directed(and 67.9% undirected).This model also works on German and Chinese atabove-baseline levels (55.8% and 54.2% undirected,respectively), with no modifications whatsoever.
InGerman, the largest source of errors is also thesystematic postulation of determiner-headed noun-phrases.
In Chinese, the primary mismatch is thatsubjects are considered to be the heads of sentencesrather than verbs.This dependency induction model is reasonablysuccessful.
However, our intuition is still that themodel can be improved by paying more attentionto syntactic constituency.
To this end, after brieflyrecapping the model of Klein and Manning (2002),we present a combined model that exploits depen-dencies and constituencies.
As we will see, thiscombined model finds correct dependencies moresuccessfully than the model above, and finds con-stituents more successfully than the model of Kleinand Manning (2002).4 Distributional Constituency InductionIn linear distributional clustering, items (e.g., wordsor word sequences) are represented by characteristicdistributions over their linear contexts (e.g., multi-nomial models over the preceding and followingwords, see figure 5).
These context distributionsare then clustered in some way, often using standardSpan Label Constituent Context?0,5?
S NN NNS VBD IN NN  ?
?0,2?
NP NN NNS  ?
VBD?2,5?
VP VBD IN NN NNS ?
?3,5?
PP IN NN VBD ?
?0,1?
NN NN  ?
NNS?1,2?
NNS NNS NN ?
VBD?2,3?
VBD VBD NNS ?
IN?3,4?
IN IN VBD ?
NN?4,5?
NN NNS IN ?
(a) (b)Figure 5: The CCM model?s generative process for thesentence in figure 1.
(a) A binary tree-equivalent brack-eting is chosen at random.
(b) Each span generates itsyield and context (empty spans not shown here).
Deriva-tions which are not coherent are given mass zero.data clustering methods.
In the most common case,the items are words, and one uses distributions overadjacent words to induce word classes.
Previouswork has shown that even this quite simple repre-sentation allows the induction of quite high qualityword classes, largely corresponding to traditionalparts of speech (Finch, 1993; Schu?tze, 1995; Clark,2000).
A typical pattern would be that stocks andtreasuries both frequently occur before the wordsfell and rose, and might therefore be put into thesame class.Clark (2001) and Klein and Manning (2002)show that this approach can be successfully usedfor discovering syntactic constituents as well.
How-ever, as one might expect, it is easier to clusterword sequences (or word class sequences) than totell how to put them together into trees.
In par-ticular, if one is given all contiguous subsequences(subspans) from a corpus of sentences, most natu-ral clusters will not represent valid constituents (tothe extent that constituency of a non-situated se-quence is even a well-formed notion).
For exam-ple, it is easy enough to discover that DET N andDET ADJ N are similar and that V PREP DET andV PREP DET ADJ are similar, but it is much lessclear how to discover that the former pair are gen-erally constituents while the latter pair are generallynot.
In Klein and Manning (2002), we proposed aconstituent-context model (CCM) which solves thisproblem by building constituency decisions directlyinto the distributional model, by earmarking a sin-gle cluster d for non-constituents.
During the cal-culation of cluster assignments, only a non-crossingsubset of the observed word sequences can be as-signed to other, constituent clusters.
This integratedapproach is empirically successful.The CCM works as follows.
Sentences are givenas sequences s of word classes (parts-of-speech orotherwise).
One imagines each sentence as a listof the O(n2) index pairs ?i, j?, each followed bythe corresponding subspan is j and linear contexti?1si ?
j s j+1 (see figure 5).
The model generatesall constituent-context pairs, span by span.The first stage is to choose a bracketing B forthe sentence, which is a maximal non-crossing sub-set of the spans (equivalent to a binary tree).
Inthe basic model, P(B) is uniform over binary trees.Then, for each ?i, j?, the subspan and context pair(i s j , i?1si ?
j s j+1) is generated via a class-conditional independence model:P(s, B) = P(B)?
?i, j ?P(is j |bi j )P(i?1si ?
j s j+1|bi j )That is, all spans guess their sequences and contextsgiven only a constituency decision b.7This is a model P(s, B) over hidden bracketingsand observed sentences, and it is estimated via EMto maximize the sentence likelihoods P(s) over thetraining corpus.
Figure 6 shows the accuracy of theCCM model not only on English but for the Chineseand German corpora discussed above.8 Results arereported at convergence; for the English case, F1is monotonic during training, while for the others,there is an earlier peak.Also shown is an upper bound (the target trees arenot all binary and so any all-binary system will over-propose constituents).
Klein and Manning (2002)gives comparative numbers showing that the basicCCM outperforms other recent systems on the ATIScorpus (which many other constituency inductionsystems have reported on).
While absolute numbersare hard to compare across corpora, all the systemscompared to in Klein and Manning (2002) parsedbelow a right-branching baseline, while the CCM issubstantially above it.5 A Combined ModelThe two models described above have some com-mon ground.
Both can be seen as models over lexi-calized trees composed of the configurations in fig-ure 4.
For the DMV, it is already a model over thesestructures.
At the ?attachment?
rewrite for the CCM7As is typical of distributional clustering, positions in thecorpus can get generated multiple times.
Since derivationsneed not be consistent, the entire model is mass deficient whenviewed as a model over sentences.8In Klein and Manning (2002), we reported results usingunlabeled bracketing statistics which gave no credit for brack-ets which spanned the entire sentence (raising the scores) butmacro-averaged over sentences (lowering the scores).
Thenumbers here hew more closely to the standard methods usedfor evaluating supervised parsers, by being micro-averaged andincluding full-span brackets.
However, the scores are, overall,approximately the same.in (a/b), we assign the quantity:P(isk |true)P(i?1si ?
ksk+1|true)P(isk |false)P(i?1si ?
ksk+1|false)which is the odds ratio of generating the subse-quence and context for span ?i, k?
as a constituentas opposed to a non-constituent.
If we multiply alltrees?
attachment scores by?
?i, j ?
P(is j |false)P(i?1si ?
j s j+1|false)the denominators of the odds ratios cancel, and weare left with each tree being assigned the probabilityit would have received under the CCM.9In this way, both models can be seen as generat-ing either constituency or dependency structures.
Ofcourse, the CCM will generate fairly random depen-dency structures (constrained only by bracketings).Getting constituency structures from the DMV isalso problematic, because the choice of which sideto first attach arguments on has ramifications onconstituency ?
it forces x-bar-like structures ?
eventhough it is an arbitrary convention as far as depen-dency evaluations are concerned.
For example, ifwe attach right arguments first, then a verb with aleft subject and a right object will attach the ob-ject first, giving traditional VPs, while the other at-tachment order gives subject-verb groups.
To avoidthis bias, we alter the DMV in the following ways.When using the dependency model alone, we alloweach word to have even probability for either gener-ation order (but in each actual head derivation, onlyone order occurs).
When using the models together,better performance was obtained by releasing theone-side-attaching-first requirement entirely.In figure 6, we give the behavior of the CCM con-stituency model and the DMV dependency modelon both constituency and dependency induction.Unsurprisingly, their strengths are complementary.The CCM is better at recovering constituency, andthe dependency model is better at recovering depen-dency structures.
It is reasonable to hope that a com-bination model might exhibit the best of both.
In thesupervised parsing domain, for example, scoring alexicalized tree with the product of a simple lexicaldependency model and a PCFG model can outper-form each factor on its respective metric (Klein andManning, 2003).9This scoring function as described is not a generativemodel over lexicalized trees, because it has no generation stepat which nodes?
lexical heads are chosen.
This can be correctedby multiplying in a ?head choice?
factor of 1/(k ?
j) at each fi-nal ?sealing?
configuration (d).
In practice, this correction fac-tor was harmful for the model combination, since it duplicateda strength of the dependency model, badly.Model UP UR UF1 Dir UndirEnglish (WSJ10 ?
7422 Sentences)LBRANCH/RHEAD 25.6 32.6 28.7 33.6 56.7RANDOM 31.0 39.4 34.7 30.1 45.6RBRANCH/LHEAD 55.1 70.0 61.7 24.0 55.9DMV 46.6 59.2 52.1 43.2 62.7CCM 64.2 81.6 71.9 23.8 43.3DMV+CCM (POS) 69.3 88.0 77.6 47.5 64.5DMV+CCM (DISTR.)
65.2 82.8 72.9 42.3 60.4UBOUND 78.8 100.0 88.1 100.0 100.0German (NEGRA10 ?
2175 Sentences)LBRANCH/RHEAD 27.4 48.8 35.1 32.6 51.2RANDOM 27.9 49.6 35.7 21.8 41.5RBRANCH/LHEAD 33.8 60.1 43.3 21.0 49.9DMV 38.4 69.5 49.5 40.0 57.8CCM 48.1 85.5 61.6 25.5 44.9DMV+CCM 49.6 89.7 63.9 50.6 64.7UBOUND 56.3 100.0 72.1 100.0 100.0Chinese (CTB10 ?
2437 Sentences)LBRANCH/RHEAD 26.3 48.8 34.2 30.2 43.9RANDOM 27.3 50.7 35.5 35.9 47.3RBRANCH/LHEAD 29.0 53.9 37.8 14.2 41.5DMV 35.9 66.7 46.7 42.5 54.2CCM 34.6 64.3 45.0 23.8 40.5DMV+CCM 33.3 62.0 43.3 55.2 60.3UBOUND 53.9 100.0 70.1 100.0 100.0Figure 6: Parsing performance of the combined modelon various treebanks, along with baselines.In the combined model, we score each tree withthe product of the probabilities from the individ-ual models above.
We use the inside-outside algo-rithm to sum over all lexicalized trees, similar to thesituation in section 3.
The tree configurations areshown in figure 4.
For each configuration, the rele-vant scores from each model are multiplied together.For example, consider figure 4(a).
From the CCMwe must generate isk as a constituent and its cor-responding context.
From the dependency model,we pay the cost of h taking a as a right argument(PCHOOSE), as well as the cost of choosing not tostop (PSTOP).
We then running the inside-outside al-gorithm over this product model.
For the results,we can extract the sufficient statistics needed to re-estimate both individual models.10The models in combination were intitialized inthe same way as when they were run individually.Sufficient statistics were separately taken off theseindividual completions.
From then on, the resultingmodels were used together during re-estimation.Figure 6 summarizes the results.
The combinedmodel beats the CCM on English F1: 77.6 vs. 71.9.The figure also shows the combination model?sscore when using word classes which were inducedentirely automatically, using the simplest distribu-tional clustering method of Schu?tze (1995).
Theseclasses show some degradation, e.g.
72.9 F1, but it10The product, like the CCM itself, is mass-deficient.is worth noting that these totally unsupervised num-bers are better than the performance of the CCMmodel of Klein and Manning (2002) running offof Penn treebank word classes.
Again, if we mod-ify the gold standard so as to make determiners thehead of NPs, then this model with distributional tagsscores 50.6% on directed and 64.8% on undirecteddependency accuracy.On the German data, the combination again out-performs each factor alone, though while the com-bination was most helpful at boosting constituencyquality for English, for German it provided a largerboost to the dependency structures.
Finally, onthe Chinese data, the combination did substantiallyboost dependency accuracy over either single factor,but actually suffered a small drop in constituency.11Overall, the combination is able to combine the in-dividual factors in an effective way.6 ConclusionWe have presented a successful new dependency-based model for the unsupervised induction of syn-tactic structure, which picks up the key ideas thathave made dependency models successful in super-vised statistical parsing work.
We proceeded toshow that it works cross-linguistically.
We thendemonstrated how this model could be combinedwith the previous best constituent-induction modelto produce a combination which, in general, sub-stantially outperforms either individual model, oneither metric.
A key reason that these models are ca-pable of recovering structure more accurately thanprevious work is that they minimize the amount ofhidden structure that must be induced.
In particu-lar, neither model attempts to learn intermediate, re-cursive categories with no direct connection to sur-face statistics.
Our results here are just on the un-grounded induction of syntactic structure.
Nonethe-less, we see the investigation of what patterns canbe recovered from corpora as important, both from acomputational perspective and from a philosophicalone.
It demonstrates that the broad constituent anddependency structure of a language can be recov-ered quite successfully (individually or, more effec-tively, jointly) from a very modest amount of train-ing data.7 AcknowledgementsThis work was supported by a Microsoft Gradu-ate Research Fellowship to the first author and by11This seems to be partially due to the large number of un-analyzed fragments in the Chinese gold standard, which leavea very large fraction of the posited bracketings completely un-judged.the Advanced Research and Development Activity(ARDA)?s Advanced Question Answering for Intel-ligence (AQUAINT) Program.
This work also ben-efited from an enormous amount of useful feedback,from many audiences and individuals.ReferencesStephen P. Abney.
1987.
The English Noun Phrase in its SententialAspect.
Ph.D. thesis, MIT.James K. Baker.
1979.
Trainable grammars for speech recognition.
InD.
H. Klatt and J. J. Wolf, editors, Speech Communication Papersfor the 97th Meeting of the Acoustical Society of America, pages547?550.Eric Brill.
1993.
Automatic grammar induction and parsing free text:A transformation-based approach.
In ACL 31, pages 259?265.Glenn Carroll and Eugene Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from corpora.
In CarlWeir, Stephen Abney, Ralph Grishman, and Ralph Weischedel, edi-tors, Working Notes of the Workshop Statistically-Based NLP Tech-niques, pages 1?13.
AAAI Press, Menlo Park, CA.Stanley F. Chen.
1995.
Bayesian grammar induction for languagemodeling.
In ACL 33, pages 228?235.Noam Chomsky.
1965.
Aspects of the Theory of Syntax.
MIT Press,Cambridge, MA.Alexander Clark.
2000.
Inducing syntactic categories by context distri-bution clustering.
In The Fourth Conference on Natural LanguageLearning.Alexander Clark.
2001.
Unsupervised induction of stochastic context-free grammars using distributional clustering.
In The Fifth Confer-ence on Natural Language Learning.Michael Collins.
1999.
Head-Driven Statistical Models for NaturalLanguage Parsing.
Ph.D. thesis, University of Pennsylvania.Jason Eisner.
1996.
Three new probabilistic models for dependencyparsing: An exploration.
In COLING 16, pages 340?345.Steven Paul Finch.
1993.
Finding Structure in Language.
Ph.D. thesis,University of Edinburgh.Dan Klein and Christopher D. Manning.
2002.
A generativeconstituent-context model for improved grammar induction.
In ACL40, pages 128?135.Dan Klein and Christopher D. Manning.
2003.
Fast exact inferencewith a factored model for natural language parsing.
In SuzannaBecker, Sebastian Thrun, and Klaus Obermayer, editors, Advancesin Neural Information Processing Systems 15, Cambridge, MA.MIT Press.Igor Aleksandrovich Mel?
c?uk.
1988.
Dependency Syntax: theory andpractice.
State University of New York Press, Albany, NY.Philip H. Miller.
1999.
Strong Generative Capacity.
CSLI Publications,Stanford, CA.Mark A. Paskin.
2002.
Grammatical bigrams.
In T. G. Dietterich,S.
Becker, and Z. Ghahramani, editors, Advances in Neural Infor-mation Processing Systems 14, Cambridge, MA.
MIT Press.Fernando Pereira and Yves Schabes.
1992.
Inside-outside reestimationfrom partially bracketed corpora.
In ACL 30, pages 128?135.Hinrich Schu?tze.
1995.
Distributional part-of-speech tagging.
In EACL7, pages 141?148.Zach Solan, Eytan Ruppin, David Horn, and Shimon Edelman.
2003.Automatic acquisition and efficient representation of syntacticstructures.
In Suzanna Becker, Sebastian Thrun, and Klaus Ober-mayer, editors, Advances in Neural Information Processing Systems15, Cambridge, MA.
MIT Press.Andreas Stolcke and Stephen M. Omohundro.
1994.
Inducing proba-bilistic grammars by Bayesian model merging.
In Grammatical In-ference and Applications: Proceedings of the Second InternationalColloquium on Grammatical Inference.
Springer Verlag.Menno van Zaanen.
2000.
ABL: Alignment-based learning.
In COL-ING 18, pages 961?967.Deniz Yuret.
1998.
Discovery of Linguistic Relations Using LexicalAttraction.
Ph.D. thesis, MIT.
