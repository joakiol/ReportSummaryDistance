THE COMPUTATIONAL DIFF ICULTY OF ID /LPPARSINGG.
Edward Barton, Jr.M.I.T.
Artificial Intelligence Laboratory545 Technology SquareCaanbridge, MA 02139ABSTRACT.\lodern linguistic theory attributes urface complexityto interacting snbsystems of constraints.
\["or instance, theID LP gr,'unmar formalism separates constraintson immediate dominance from those on linear order.5hieber's (t983) ID/I .P parsing algorithm shows how touse ID and LP constraints directly in language process-ing, without expandiqg them into an intcrmrdiate "objectgammar."
However, Shieber's purported O(:,Gi 2 .n ~) run-time bound underestimates the tlillicnlty of ID/LP parsing.ID/LP parsing is actually NP-complete, anti the worst-caseruntime of Shieber's algorithm is actually exponential ingrammar size.
The growth of parser data structures causesthe difficulty.
So)tie ct)mputational nd linguistic implica-tions follow: in particular, it is important to note thatdespite its poteutial for combinatorial explosion, Shieber'salgorithm remains better thau the alternative of parsingan expanded object gr~anmar.INTRODUCTIONRecent linguistic theories derive surface complexityfr~ml modular subsystems of constraints; Chotusky (1981:5)proposes separate theories of bounding, government,O-marking, and so forth, while G,'xzdar and \['ullum's GPSGfi)rmalism (Shieber.
1983:2ff) use- immediate-donfinance?\[D) rules, linear-precedence (l,P) constraints, and,netarules.
When modular ctmstraints ,xre involved, rulesystems that multiply out their surface effects are largeand clumsy (see Barton.
1984a).
"\['he expanded context-free %bjeet grammar" that nmltiplies out tile constraintsin a typical (,PSG system would contain trillions of rules(Silieber, 1983:1).5bicher (198:1) thus leads it: a welconte direction by,.hw.ving how (D,\[.P grammars can be parsed "directly,"wit hour the combinatorially explosive step of nmltiplyingmtt the effects of the \[D and LP constraints.
Shieber's?
dqorithm applies ID and LP constraints one step at a;ime.
;,s needed, ttowever, some doubts about computa-tion;d complexity remain.
~hieber (198.3:15) argates thathis algorithm is identical to Earley's in time complexity,but this result seems almost too much to hope for.
Anll)/f.I ) grammar G can be much smalhr th;m an equiva-lent context-free gr,'umnar G'; for example, if Gt containsonly the rule ,5 ~to  abcde, the corresponding G~t contains5!
~- 120 rules.
If Shieber's algorithm has the same timecomplexity ~ Earley's.
this brevity of exprd~slon comesfree (up to a constant).
5hieber ~ays little to ;dlay possibledoubts:W,, will t.,r proq,nt a rigor..s (h.lllOtlstr'~..li?
)t) of  I'llnPc'(,mpt,'xlty.
I.,t ~t .I.,.
id b~, ch..tr fr.m tiw oh,.
',, rc.lationh,.t w,',.n ) he l,rt,,,vtlt(',l ;tl~,nt hm ;rod E.rt<.y'~ t hat the( '+,t.ph'xity Is )h;d of  Earh.y'> ;tig,)rltl~tlt \[II t.l.+ worst,'.:+,,.
wh,,re tl.. I.I" rnh'.
;dw:ty:.
+p,'('ffy ;t tllli(llll" ordor-t;,~ l'-r t!+(.
ri~i~t-imlld :~+'(' ,,l'<,v<.
"y ID rtih., the l)i'('r~'tlte~l;d.,;,with;'.
r,,,In.
"v., t.+ E , t rh 'y ,  ; t l~t)r l lh l l l  ~qin,.+, ~ivon)h,' .
.
:ramm.tr.
vht .
rkm~ : I .
.
L I )  rnh.:.
; t;Lk(..., Cl)ll+'r+liillt t ime.rh,,.
th in.
c,)IJHd,,'.
":it y , , I  i t .
.
pre>ented :d~.
, r l th t .
i..., ideo-tw;d t() E.ri(.y'+ .
That  i:.. it ts ( i t  (,' '2 .'t:;).
wht .
ro :(';:t> )1., qzt' ,,f thv gramt,~ar im,ml,vr ,,f \[D ruh'.~) and ni.
~ tilt' h'ngth <)f the input.
(:i,If)Among other questions, it is nnclear why a +ituation ofmaximal constraint shouhl represent the worst case.
Mtrd-real constraint may mean that there are more possibilitiesto consider..q.h;eber's algorithm does have a time advantage overthe nse of garley's algorithm on the expanded CF'G.
butit blows up in tile worst case; tile el;din of (9 (G"  .
r(~)time complexity is nustaken.
A reduction of the vertex-cover l>rt)blenl shows that ID/LP parsing is actually NI )-comph.te: hence ti,is bh)wup arises from the inherent diffi-culty of ID,'LP parsing ratlter than a defect in $hieber's al-gorithm (unless g' = A2) .
Tile following ~ections explainaud discuss this result.
LP constraints are neglected be-cause it is the ID r.les that make parsing dilficultAtte)~tion focuses on unordered contest-free 9rammar~(I ~('F(;s; essentially, l l ) / l ,P  gram,oars aans LIt).
A UCFGrule ;s like a standard C\[:G rule except that when use(t m aderivati,,n, it may have the symbols ,)f its ex\[~ansio l t  writ-ten in any order.SHIEBER'S  ALG OI I ITHMShiel)er generalizes Earley's algorithm by generalizingthe dotted-rule representation that Earley uses to trackprogress thro,gh rule expansions.
A UCIrG rule differsfrom a CFG rule only in that its right-hand side is un-ordered; hence successive accumulation of set elements re-places linear ad-.mcement through a sequence.
Obviousinterpretations follow for the operations that the Earleypar.,er performs on dotted rules: X - .
{}.
{A, B,C} is a78typical initial state for a dotted UCFG rule;X --  {A ,B ,C} .
{}  is a t~'pical completed state;Z ---.
{W}.
{a ,X ,Y}  predicts terminal a and nontermi-nail X,Y ;  and X -- {A} .
{B,C ,C}  should be advancedto X - .
{A ,C} .
{B,C}  after the predicted C is located, tExcept for these changes, Shieber's algorithm is identicalto Earley's.As Shieber hoped, direct parsing is better than usingEarley's algorithm on an expanded gr,-mlmar.
If Shieber'sparser is used to parse abcde according to Ct, the statesets of the parser remain small.
The first state set con-tains only iS - -  {}.
{a,b,c,d,e},O I, the second state setcontains only \[S - -  {a}.
{b,c,d,e},O i, ,'rod so forth.
Thestate sets grow lnuch larger if the Earley parser is used toparse the string according to G' t with its 120 rules.
Afterthe first terminal a has been processed, the second state setof the Earley parser contain, .1!
- 2.t stales spelling out allpossible orders in which the renmiaing symbols {b,e,d,e}may appear: ;S ~ a.bcde,O!, ;S - ,  ,,.ccdb.
Oi and so on.Shieber's parser should be faster, since both parsers workby successively processing all of tile states in tile state sets.Similar examples how that tile 5hieber parser can have,-m arbitrarily large advantage over the tlse of the Earleyparser on tile object gr,'unmar.Shieber's parser does not always enjoy such a large ad-vantage; in fact it can blow tip in the presence of ambiguity.Derive G~.
by modifying Gt in two ways.
First, introducedummy categories A. tl, C ,D ,E  so that A ~ a and soforth, with S -+ ABCDE.
Second, !et z be ambiguouslyin any of the categories A, B,C,  D ,E  so that the rule forA becomes A ~ a ~, z and so on.
What happens whenthe string zzzza  is parsed according to G~.?
After the firstthree occurrences of z, the state set of the parser will reflectthe possibility that any three of the phrases A,/3, C, D, Emight have been seen ,'rod any two of then| might remain tobe parsed.
There will be (~) = t0 states reflecting progressthrough the rule expanding S; iS ~ {A, B ,C}.
{D,E},0 \ ]will be in the state set, a.s w i l l ' S  ~ {A,C ,E} .
{B,D},O I ,etc.
There will also be 15 states reflecting the completionand prediction of phrases.
In cases like this, $hieber's al-gorithm enumerates all of the combinations of k elementstaken i at a tin|e, where k is the rule length and i is thenumber of elements already processed.
Thus it can becombinatorially explosive.
Note, however, that Shieber'salgorithm is still better than parsing the object grammar.With the Earley parser, the state set would reflect the samepossibilities, but encoded in a less concise representation.In place ot the state involving S ~ {A, 13, C} .
{D,E} ,for instance, there would be 3!.
2!
= 12 states involvingS ~ ABC.DE,  S ~ 13CA.ED, and so forth.
2 his|endIFor mor~.
dl.rail~ ~-e Bar ton  (198,1bi ~ ld  Shi,.hPr (1983}.
Shieber'.~rel,re~,ent;ttion ,lilfers in .~mle ways f rom tilt.
reprr.
'~,nlatiol l  de...a'ribt.,\[ lit.re, wit|ell W~.~ ,h.veh}ped illth'pt, ndeut ly  by tilt, author .The dilft,r,.nces tuft.
i~ellPrldly iut.~.
'~eutiid, but  .~ee |tote 2.l ln  eontrP....t?
tit t|lt.
rr|)rrr4.ntztl.ion .ilht..4tr;tled here.
:?,}:ieber'.., rt.|v.
?P.~Wl'llt+l+liOll H '?ll;Idl|y .~ulfl.r.~ to POIlI(" eXtt'tlt flOlll Tilt + Y.;tllle |lf\[lil-of a total of 25 states, the Earley state set would contain135 = 12 ?
10 -+- 15 states.With G~., the parser could not be sure of the categorialidentities of the phrases parsed, but at least it was certainof the number ,'tad eztent of the phrases.
The situation getsworse if there is uncertainty in those areas ~ well.
DeriveG3 by replacing every z in G,.
with the empty string e sothat ,an A, for instance, can be either a or nothing.
Beforeany input has been read, state set S, in $hieber's parsermust reflect the possibility that the correct parse may in-clude any of the 2 ~ = 32 possible subsets of A, B, C, D, ~'empty initial constituents.
For example, So must in-clude \[..q - -  {A, \]3,C, D, E}.
{},0i because the input mightturn out to be the null string.
Similarly, S. must include:S ~ {A,C,  El.
{~3, Dt,O~ because the input might be bdor db.
Counting all possible subsets in addition to otherstates having to do with predictions, con|pie|ions, and theparser start symbol that some it||p\[ententatioas introduce,there will be .14 states in ?,.
(There are 3:~8 states ill thecorresponding state when the object gra, atuar G~ is used.
)| low call :Shieber's algorithm be exponeatial in grant-Inar size despite its similarity to Earh:y's algorithm,which is polynontiM in gratnln~tr size7 The answer is thatShieber's algorithm involves a leech larger bouad on thenumber of states in a state set.
Since the Eariey parsersuccessively processes all of the states in each state set(Earley, 1970:97), an explosion in the size of the state setskills any small runtime bound.Consider the Earley parser.
Resulting from each ruleX ~ At .
.
.
.
4~ in a gram|oar G,, there are only k - t pos-sible dotted rules.
The number of possible dotted rulesis thus bounded by the au~'uber of synibois that it takesto write G, down, i.e.
by :G,, t. Since an Eariey statejust pairs a dotted rule with an interword position rangingfront 0 to the length n of the input string, there are onlyO('~C~; ?
n) possible states: hence no state set may containmore than O(Ga i 'n )  (distinct) states.
By an argumentdue to Eartey, this limit allows an O(:G~: .
n z) bound tobe placed on Earley-parser runti,ne.
In contrast, the statesets of Shieber's parser may grow t|tuch larger relative togr~nmar size.
A rule X ~ At .
.
.
A~ in a UCFG G~ yieldsnot k + I ordinary dotted rules, but but 2 ~ possible dot-ted UCFC rules tracking accumulation of set elements.
\[nthe worst ca.,e the gr,'uutttar contains only one rule and kis on the order of G,,:: hence a bound on the mt,nber ofpossible dotted UCFG rules is not given by O(G,,.
), butby 0(2 el, ).
(Recall tile exponential blowup illustrated forgranmmr /5:.)
The parser someti,,tes blows up becausethere are exponentially more possible ways to to progressthrough an :reordered rule expansion than an through anordered one.
in ID/LP parsing, the emits|  case occursl em.
.qh ivher  {1083:10} um.~ ,~t ordered seqt .
,nre  in.~tead of a mld-tim.t hvfore tilt.
dot: ?ou.~equently.
in plltco of the ..,tate invo|v ingS ~ {A.B.(:}.
{D.E}, Sltiei,er wouhJ have t i l t ,  :E = 6 ~t;ttt..~ itl-vtdving S -- ~t.
{D. E}, where o~ range* over l|te six pernlutlxtion8 ofABC.77ae  ebI \["'d el I ,, e2.
/e3Figure 1: This graph illustrates a trivial inst,ance of thevertex cover problem.
The set {c,d} is a vertex cover ofsize 2.when the LP constraints force a unique ordering for ev-ery rule expansion.
Given sufficiently strong constraints,Shieber's parser reduces to Earley's as Shieber thought,but strong constraint represents the best case computa-tionally rather than the worst caze.NP-COMPLETENESSThe worst-case time complexity of Shieber's algorithmis exponential in grammar size rather than quadratic ,'mShieber (1983:15} believed, l)id Shieber choose a poor al-gorithm, or is ID/LP parsing inherently difficult?
In fact,the simpler problem of recoyn~zzn 9 sentences according to aUCFG is NP-complete.
Thus, unless P = 3/P, no ID/LPparsing algorithm can always run in trine polynomial inthe combined size of grammar and input.
The proof is areduction of the vertex cover problem (Garey and John-son, 1979:,16), which involves finding a small set of verticesin a graph such that every edge of the graph has an end-point in the set.
Figure 1 gives a trivial example.To make the parser decide whether the graph in Fig-ure I has a vertex cover of size 2, take the vertex names a,b, c, and d as the alphabet.
Take Ht through H4 as specialsymbols, one per edge; also take U and D as dummy sym-bols.
Next, encode the edges of the graph: for instance,edge el runs from a to c, so include the rules itll ---, a andHt ~ c. Rules for the dummy symbols are also needed.Dummy symbol D will be used to soak up excess inputsymbols, so D ~ a through D ~ d should be rules.Dummy symbol U will also soak up excess input symbols,but U will be allowed to match only when there are fouroccurrences in a row of the same symbol {one occurrencefor each edge).
Take U ~ aaaa, U --.
bbbb, and U --.
cccc,and U ---, dddd as the rules expanding U.Now, what does it take for the graph to have a vertexcover of size k = 2?
One way to get a vertex cover is to gothrough the list of edges and underline one endpoint of eachedge.
If the vertex cover is to be of size 2, the nmlerliningmust be done in such a way that only two distinct verticesaxe ever touched in the process.
Alternatively, since thereaxe 4 vertices in all, the vertex cover will be of size 2 if thereare 4 - 2 = 2 vertices left untouched in the underlining.This method of finding a vertex cover can be translatedSTART -~ Hi  tI2H3H4UU DDDDHl -.-.aI cH2--*bleH3 --.
c l ,~H,..-.bl~U ---, aaaa !
bbbb t cccc I ddddD~alb lc ldFigure 2: For k = 2, the construction described in the texttransforms the vertex-cover problem of Figure 1 into thisUCFG.
A parse exists for the string aaaabbbbecccdddd iffthe graph in the previous figure has a vertex cover of size<2.into an initial rule for the UCFG, ,as follows:START - .
Hi I I2H~I I4UUDDDDEach //-symbol will match one of the endpoints of thecorresponding edge, each /.r-symbol will correspond to avertex that was left untouclted by the H-matching, andthe D-symbols are just for bookkeeping.
(Note that this isthe only ~ule in the construction that makes essential useof the unordered nat , re of rule right-hand sides.}
Figure 2shows the complete gr,'unmar that encodes the vertex-coverproblem ,,f Figure I.To make all of this work properly, takea = aaaabbbbccccddddas the input string to be parsed.
(For every vertex name z,include in a a contiguous run of occurrences of z, one foreach edge in the graph.)
The gramnlar encodes the under-lining procedure by requiring each //-symbol to match oneof its endpoints in a.
Since the expansion of the STARTrx, le is unordered, ,an H-symbol can match anywhere in a,hence can match any vertex name (subject to interferencefrom previously matched rules).
Furthermore, since thereis one occurrence of each vertex name for every edge, it'simpossible to run out of vertex-name occurrences.
Thegrammar will allow either endpoint of an edge to be "un-derlined" - -  that is, included in the vertex cover - -  so theparser must figure out which vertex cover to select.
How-ever, the gr,-mtmar also requires two occurrences of U tomatch.
U can only match four contiguous identical inputsymbols that have not been matched in any other way;thus if the parser chooses too iarge a vertex cover, the U-symbols will not match and the parse will fail.
The propernumber of D-symbols equals the length of the input string,minus t|,e number of edges in the graph (to ~count  for the//,-matches), minus k times the number of edges (to ac-count for the U-matches): in this case, 16 - 4 - (2 ?
4) = 4,as illustrated in the START rule.The result of this construction is that in order to decidewhether a is in the language generated by the UCFG, the78STARTU U Ht //2 H3 D //4 D D DA/ IIIIIIIIa a a a b b b b c c c c d d d dFigure 3: The grammar of Figure 2, which encodes thevertex-cover problem of Figure I, generates the stringa = aaaabbbbccccddddaccording to this parse tree.
Thevertex cover {c,d} can be read off from the parse tree a~the set of elements domi,~ated by //-symbols.parser nmst search for a vertex cover of size 2 or less.
3 Ifa parse exists, an appropriate vertex cover can be read offfrom beneath the //-symbols in the parse tree; conversely,if an appropriate vertex cover exists, it shows how to con-struct a parse.
Figure 3 shows the parse tree that encodes asolution to the vertex-cover problem of Figure 1.
The con-struction thus reduces Vertex Cover to UCFG recognition,and since the c,~nstruction can be carried out in polyno-mial time, it follows that UCFG recognition and the moregeneral ta.sk of ID/LP parsing nmst be computationallydifficult.
For a more detailed treatment of the reduction,see Barton (1984b).IMPL ICAT IONSThe reduction of Vertex Cover shows that the \[D/LPparsing problem is YP-complete; unless P = ~/P, its timecomplexity is not bounded by ,'my polynomial in the size'ofthe grammar and input.
Ilence complexity analysis mustbe done carefully: despite sintilarity to Earley's algorithm,Shieber's algorithm does not have complexity O(IG\[ 2. n3),but can sometimes undergo exponential growth of its in-ternal structures.
Other computational ,and linguistic on-sequences alzo follow.Although Shieber's parser sometimes blows up, it re-mains better than the alternative of ,~arsing an expanded"object ~arnmar."
The NP-completeness result shows thatthe general c~e of ID/LP parsing is inherently difficult;hence it is not surprising that Shieber's ID/LP parser some-times suffers from co,nbinatorial explosion.
It is more im-portant o note that parsing with the expanded CFG blowsup in ea~v c~es.
It should not be h~d to parse the lan-~lf the v#rtex er, ver i.~ t, ma l le r  tllall expected, the D-.~y,nbo~ willup the extra eonti~mun trm that could have been matrhed I~'more (f-symbols.guage that consists of aH permutations of the string abode,but in so doing, the Earley parser can use 24 states or moreto encode what the Shieber parser encodes in only one (re-call Gl).
Tile significant fact is not that the Shieber parsercan blow up; it is that the use of the object grammar blowsup unnecessarily.The construction that reduces the Vertex Cover prob-lem to ID/LP P,xrsing involves a grammar and input stringthat both depend on the problem instance; hence it leavesit open that a clever programmer ,night concentrate mostof the contputational dilliculty of ID/LF' parsing into anofll_ine grammar-precompilation stage independent of theinput - -  under optimistic hopes, perhaps reducing the timerequired for parsing ;m input (after precompilation) to apolynomial function of grammar size and inpt,t length.Shieber's algorithm has no precompilation step, ~ so thepresent complexity results apply with full force; ,'my pos-sible precompilation phase remains hyl~othetical.
More-over, it is not clear that a clever preco,npilation step iseven possible.
For example, i fn  enters into the true com-plexity of ID/LI ~ parsing ,~ a factor multiplying an expo-nential, ,an inpnt-indepemtent precompilation phase can-not help enough to make the parsing phase always run inpolynomial time.
On a related note,.~uppo,e the precom-pilation step is conversiol, to CF(.
; farm ?md the runtimealgorithm is the Earley parser.
Ahhough the precompila-tion step does a potentially exponenti;d amount of work inproducing G' from G, another expoaential factor shows upat runtime because G' in the complexity bound G'2n~is exponentially larger than the original G'.The NP-completeness result would be strengthened ifthe reduction used the same grammar for all vertex-coverproblems, for it woold follow that precompilation couldnot bring runtime down to polynomial time.
However,unless ,~ = & P, there can be no such reduction.
Sincegr.
'Jannlar size would not count as a parameter of a fixed-gramm~tr \[D/LP parsing problem, the l,se of the Earleyparser on the object gr,-ulzmar would already constitute apolynomial-time algorithm for solving it.
(See the nextsection for discussion.
)The Vertex Cover reduction also helps pin down thecomputational power of UCFGs.
As G, ,'tad G' t illus-trated, a UCFG (or an ID/LP gr,'uumar) is sometimestnttch smaller than an equivalent CFG.
The NP-complete-ness result illuminat,_'s this property in three ways.
First,th'e reduction shows that enough brevity is gained so thatan instance of any problem in .~ .~ can be stated in a UCFGthat is only polyno,nially larger than the original probleminstance.
In contrast, the current polynomial-time r duc-tion could not be carried out with a CFG instead of aUCFG, since the necessity of spelling out all the orders inwhich symbols lltight appear couhl make the CFG expo-nentially larger than the instance.
Second, the reductionshows that this brevity of expression is not free.
CFG'Shieber {1983:15 n. 6) mentmn.~ a possible precompilation step.
butit i~ concerned ~,,,itlt he, \[,P r~'hLrum rather tha.
'* tlt~r ID rtth.-~.79recognition can be solved in cubic time or less, but unlessP = .~'P, general UCFG recognition cannot be solved inpolynomial time.
Third, the reduction shows that onlyone essential use of the power to permute rule expansionsis necessary to make the parsing problem NP-comphte,though the rule in question may need to be arbitrarilylong.Finally, the ID/LP parsing problem illustrates howweakness of constraint c,-m make a problem computation-ally difficult.
One might perhaps think that weakconstraints would make a problem emier since weak con-straints ound easy to verify, but it often takes ~trong con-straints to reduce the number of possibilities that an algo-rithm nmst consider.
In the present case, the removal ofconstraints on constituent order causes the dependence ofthe runt|me bound on gr,'unmar size to grow from IGI ~ toTG',.The key factors that cause difficuhy in ID/LP parsingare familiar to linguistic theory.
GB-theory amt GPSGboth permit the existence of constituents that are emptyon the surface, and thus in principle they both allow thekind of pathology illustrated by G~, subject to ,-uueliora-tion by additional constraints.
Similarly, every currenttheory acknowledges lexical ambiguity, a key ingredient ofthe vertex-cover reduction.
Though the reduction illumi-nates the power of certain u,echanisms and formal devices,the direct intplications of the NP-completeness result forgrammatical theory are few.The reduction does expose the weakness of attemptsto link context-free generative power directly to efficientparsability.
Consider, for inst,'mce, Gazdar's (1981:155)claim that the use of a formalism with only context-freepower can help explain the rapidity of human sentenceprocessing:Suppose ... that the permitted class of genera-live gl'anllllal'S constituted ,t s,b~ct -f t.h~Jsc phrasestructure gramni;trs c;qmblc only of generating con-text-free lung||ages.
Such ;t move w, mld have twoiz,lportant uetathcoretical conseqoences, one hav-ing to do with lear,mbility, the other with process-ability .
.
.
We wen|hi have the beginnings of an ex-plan:tti~:u for the obvious, but larg~.ly ignored, factthltI hll:llD.ns process the ~ttterance~ they hear veryrapidly.
.
"~cnll+llCe+ c f;t co;O.exl-frec I;tngu;tge areI+r,val>ly l;ar~;tl~h: in ;t l.illn'~ that i>~ i>r,,l>ot'tionitl tothe ct,bc ,,f the lezlgl h of the ~entenee or less.As previously remarked, the use of Earley's algorithm onthe expanded object grantmar constitutes a parsing methodfor the ILxed-grammar (D/LP parsing problem that is in-deed no worse than cubic in sentence length.
However, themost important, aspect of this possibility is that it is devoidof practical significance.
The object ~,'mmtar could con-tain trillions of rules in practical cases (Shieber, 1983:4).If IG'~, z. n ~ complexity is too slow, then it rentains too slowwhen !G'I: is regarded as a constant.
Thus it is impossi-ble to sustain this particular argument for the advantagesof such formalisms ,as GPSG over other linguistic theo-ries; instead, GPSG and other modern theories seem tobe (very roughly) in the same boat with respect to com-plexity.
In such a situation, the linguistic merits of varioustheories are more important han complexity results.
(SeeBerwick (1982), Berwick and Weinberg (1984), aJad Ris-tad (1985) for further discussion.
)The reduction does not rule out the use of formalismsthat decouple ID and LP constraints; note that Shieber'sdirect parsing algorithm wins out over the use of the objectgrammar.
However, if we assume that natural languages,xre efficiently parsable (EP), then computational difFicul-ties in parsing a formalism do indicate that the formalismitself fl~ils to capture whatever constraints are responsiblefor making natural languages EP.
If the linquistically rel.evant ID/LP grammars are EP but the general ID/LPgramu,ars ~e not, there must be additional factors thatguarantee, say, a certain amount of constraint from the LPretationJ (Constraints beyond the bare ID, LP formalismare reqt, ired on linguistic grounds ,as well.)
The subsetprtnciple ,ff language acqoisition (cf.
\[h, rwick and We|n-berg, 198.1:233) wouht lead the language learner to initiallyhypothesize strong order constraints, to be weakened onlyin response to positive evidence.llowever, there are other potential ways to guaranteethat languages will be EP.
It is possible that the principlesof grammatical theory permit lunge,ages that are not EPin the worst c,'tse, just as ~,'uumatical theory allows sen-tences that are deeply center-embedded (Miller and Chom-sky, 1963}.
Difficuh languages or sentences still wouhl notturn up in general use, precisely because they wot, ht be dif-ficult to process.
~The factors making languages EP wouldnot be part of grammatical theory because they wouldrepresent extragrammatical f ctors, i.e.
the resource lim-itations of the language-processing mechanisms.
In thesame way, the limitations of language-acquisition mech-anisms might make hard-to-parse lunge, ages maccesstbleto the langamge le,'u'ner in spite of satisfying ~ammat ica lconstraints.
However, these "easy explanations" are nottenable without a detailed account of processing mecha-nisms; correct oredictions are necessary about which con-structions will be easy to parse.ACKNOWLEDGEMENTSThis report describes research done at the ArtificialIntelligence Laboratory of the Ma.ssachusetts Institute of~|a the (;B-fr~unework of Chom.-ky (1981).
for in~tance, the ,~yn-tactic expre~..,ion f unnrdered 0-grids at tire X level i'~ constrainedby tile principlv.~ of C.'~e th~ry, gndocentrieity is anotlmr .~ignifi-cant constraint.
See aL~o Berwick's ( 1982} discu.-,,-,ion f constraintsthat could be pl;wed ml another gr;unmatie',d form,'dism -- lexic,'d-fimetional grammar - to avoid a smfil.
'u" intr,'u'tability result.nit is often anordotally remarked that lain|rouges that allow relativelyfre~ word order '.end to m',tke heavy u.-.e of infh~'tions.
A rich iattec-timln.l system can .-upply parsing constraints that make up for thehack of ordering e.,strai,*s: thu~ tile situation we do not find is thecomputationa/ly dill|cult cnse ~ff weak cmmcraint.80Technology.
Support for the Laboratory's artificial intel-ligence research as been provided in part by the Ad-vanced Research Projects Agency of the Department ofDefense under Office of Naval Research contract N00014-80-C-0505.
During a portion of this research the author'sgraduate studies were supported by the Fannie and JohnHertz Foundation.
Useful guidance and commentary dur-ing this research were provided by Bob Berwick, MichaelSipser, and Joyce Friedman.REFERENCESBarton, E. (1984a).
"Towed a Principle-Based Parser,"A.I.
Menlo No.
788, M.I.T.
Artificial Intelligence Lab-oratory, Cambridge, Mass.Barton, E. (198,1b).
"On the Complexity of ID/LP Pars-ing," A.I.
Menlo No.
812, M.I.T.
Artificial IntelligenceLaboratory, Cambridge, Mass.Berwick, R. (1982).
"Computational Comphxity andLexical-Functional Grammar," American Journal ofCompu:ational Linguistica 8.3-4:97-109.Berwick, R., and A. Wcinberg (1984).
The GrammaticalBasi~ of Linguistic Performance.
Cambridge, Mass.:M.I.T.
Press.Chomsky, N. (1981).
Lecture8 on Government and Bind.ing.
Dordrecht, tolland: Foris Publications.Earley, J.
(1970).
"An EfFicient Context-Free Parsing Al-gorithm," Comm.
ACM 13.2:94-102.Gaxey, M., and D. Johnson (1979).
Computer~ and In-tractability.
San Francisco: W. H. Freeman and Co.Gazdar, Gerald (1981).
"Unbounded Dependencies andCoordinate Structure," Linguistic Inquiry 12.2:155-184.Miller, G., and N. Chomsky (1963).
"Finitary Models ofLanguage Users."
in R. D. Luce, R. R. Bush, and E.Galanter, eds., Handbook of Mathematical Psychology,vol.
II, 419-492.
New York: John Wiley and Sons, Inc.Ristad, E. (1985).
"GPSG-Recognition is NP-Ilard," A.I.Memo No.
837, M.I.T.
Artificial Intelligence Labora-tory, Cambridge, M,xss., forthcoming.Shieber, S. (1983).
"Direct Parsing of !D/LP Grammars.
"Technical Report 291R, SRI International, Menlo Park,California.
Also appears in Lingui~tic~ and Philosophy7:2.81
