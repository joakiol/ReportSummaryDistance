Introduction to the CoNLL-2004 Shared Task:Semantic Role LabelingXavier Carreras and Llu?
?s Ma`rquezTALP Research CentreTechnical University of Catalonia (UPC){carreras,lluism}@lsi.upc.esAbstractIn this paper we describe the CoNLL-2004shared task: semantic role labeling.
We intro-duce the specification and goal of the task, de-scribe the data sets and evaluation methods, andpresent a general overview of the systems thathave contributed to the task, providing compar-ative description.1 IntroductionIn recent years there has been an increasing interest insemantic parsing of natural language, which is becominga key issue in Information Extraction, Question Answer-ing, Summarization, and, in general, in all NLP applica-tions requiring some kind of semantic interpretation.The shared task of CoNLL-2004 1 concerns the recog-nition of semantic roles, for the English language.
Wewill refer to it as Semantic Role Labeling (SRL).
Given asentence, the task consists of analyzing the propositionsexpressed by some target verbs of the sentence.
In par-ticular, for each target verb all the constituents in the sen-tence which fill a semantic role of the verb have to beextracted (see Figure 1 for a detailed example).
Typicalsemantic arguments include Agent, Patient, Instrument,etc.
and also adjuncts such as Locative, Temporal, Man-ner, Cause, etc.Most existing systems for automatic semantic role la-beling make use of a full syntactic parse of the sentencein order to define argument boundaries and to extract rel-evant information for training classifiers to disambiguatebetween role labels.
Thus, the task has been usually ap-proached as a two phase procedure consisting of recogni-tion and labeling of arguments.1CoNLL-2004 Shared Task web page ?withdata, software and systems?
outputs available?
athttp://cnts.uia.ac.be/conll2004/roles .Regarding the learning component of the systems,we find pure probabilistic models (Gildea and Juraf-sky, 2002; Gildea and Palmer, 2002; Gildea and Hock-enmaier, 2003), Maximum Entropy (Fleischman et al,2003), generative models (Thompson et al, 2003), De-cision Trees (Surdeanu et al, 2003; Chen and Ram-bow, 2003), and Support Vector Machines (Hacioglu andWard, 2003; Pradhan et al, 2003a; Pradhan et al, 2003b).There have also been some attempts at relaxing the ne-cessity of using syntactic information derived from fullparse trees.
For instance, in (Pradhan et al, 2003a; Ha-cioglu and Ward, 2003), a SVM-based SRL system isdevised which performs an IOB sequence tagging usingonly shallow syntactic information at the level of phrasechunks.Nowadays, there exist two main English corpora withsemantic annotations from which to train SRL systems:PropBank (Palmer et al, 2004) and FrameNet (Fillmoreet al, 2001).
In the CoNLL-2004 shared task we concen-trate on the PropBank corpus, which is the Penn Treebankcorpus enriched with predicate?argument structures.
Itaddresses predicates expressed by verbs and labels corearguments with consecutive numbers (A0 to A5), try-ing to maintain coherence along different predicates.
Anumber of adjuncts, derived from the Treebank functionaltags, are also included in PropBank annotations.To date, the best results reported on the PropBank cor-respond to a F1 measure slightly over 83, when usingthe gold standard parse trees from Penn Treebank as themain source of information (Pradhan et al, 2003b).
Thisperformance drops to 77 when a real parser is used in-stead.
Comparatively, the best SRL system based solelyon shallow syntactic information (Pradhan et al, 2003a)performs more than 15 points below.
Although these re-sults are not directly comparable to the ones obtained inthe CoNLL-2004 shared task (different datasets, differ-ent version of PropBank, etc.)
they give an idea about thestate-of-the art results on the task.The challenge for CoNLL-2004 shared task is to comeup with machine learning strategies which address theSRL problem on the basis of only partial syntactic in-formation, avoiding the use of full parsers and externallexico-semantic knowledge bases.
The annotations pro-vided for the development of systems include, apart fromthe argument boundaries and role labels, the levels of pro-cessing treated in the previous editions of the CoNLLshared task, i.e., words, PoS tags, base chunks, clauses,and named entities.The rest of the paper is organized as follows.
Section2 describes the general setting of the task.
Section 3 pro-vides a detailed description of training, development andtest data.
Participant systems are described and comparedin section 4.
In particular, information about learningtechniques, SRL strategies, and feature development isprovided, together with performance results on the devel-opment and test sets.
Finally, section 5 concludes.2 Task DescriptionThe goal of the task is to develop a machine learning sys-tem to recognize arguments of verbs in a sentence, andlabel them with their semantic role.
A verb and its set ofarguments form a proposition in the sentence, and typi-cally, a sentence will contain a number of propositions.There are two properties that characterize the structureof the arguments in a proposition.
First, arguments do notoverlap, and are organized sequentially.
Second, an argu-ment may appear split into a number of non-contiguousphrases.
For instance, in the sentence ?
[A1 The apple],said John, [C?A1 is on the table]?, the utterance argument(labeled with type A1) appears split into two phrases.Thus, there is a set of non-overlapping arguments la-beled with semantic roles associated with each proposi-tion.
The set of arguments of a proposition can be seen asa chunking of the sentence, in which chunks are parts ofthe semantic roles of the proposition predicate.In practice, number of target verbs are marked in a sen-tence, each governing one proposition.
A system has torecognize and label the arguments of each target verb.2.1 Methodological SettingTraining and development data are provided to build thelearning system.
Apart from the correct output, both datasets contain the correct input, as well as predictions of theinput made by state-of-the-art processors.
The trainingset is used for training systems, whereas the developmentset is used to tune parameters of the learning systems andselect the best model.Systems have to be developed strictly with the dataprovided, which consists of input and output data and theofficial external resources (described below).
Since thecorrect annotations for the input data are provided, a sys-tem is allowed either to be trained to predict the inputpart, or to make use of an external tool developed strictlywithin this setting, such as previous CoNLL shared tasksystems.2.2 EvaluationEvaluation is performed on a separate test set, which in-cludes only predicted input data.
A system is evaluatedwith respect to precision, recall and the F1 measure.
Pre-cision (p) is the proportion of arguments predicted by asystem which are correct.
Recall (r) is the proportion ofcorrect arguments which are predicted by a system.
Fi-nally, the F1 measure computes the harmonic mean ofprecision and recall, and is the final measure to com-pare the performance of systems.
It is formulated as:F?=1 = 2pr/(p + r).For an argument to be correctly recognized, the wordsspanning the argument as well as its semantic role haveto be correct.
2As an exceptional case, the verb argument of eachproposition is excluded from the evaluation.
This argu-ment is the lexicalization of the predicate of the proposi-tion.
Most of the time, the verb corresponds to the targetverb of the proposition, which is provided as input, andonly in few cases the verb participant spans more wordsthan the target verb.Except for non-trivial cases, this situation makes theverb fairly easy to identify and, since there is one verbwith each proposition, evaluating its recognition over-estimates the overall performance of a system.
For thisreason, the verb argument is excluded from evaluation.3 DataThe data consists of six sections of the Wall Street Jour-nal part of the Penn Treebank (Marcus et al, 1993), andfollows the setting of past editions of the CoNLL sharedtask: training set (sections 15-18), development set (sec-tion 20) and test set (section 21).
We first describe anno-tations related to argument structure.
Then, we describethe preprocessing of input data.
Finally, we describe theformat of the data sets.3.1 PropBankThe Proposition Bank (PropBank) (Palmer et al, 2004)annotates the Penn Treebank with verb argument struc-ture.
The semantic roles covered by PropBank are thefollowing:?
Numbered arguments (A0?A5, AA): Argumentsdefining verb-specific roles.
Their semantics de-pends on the verb and the verb usage in a sentence,or verb sense.
In general, A0 stands for the agent2The srl-eval.pl program is the official program toevaluate the performance of a system.
It is available at theShared Task web page.and A1 corresponds to the patient or theme of theproposition, and these two are the most frequentroles.
However, no consistent generalization can bemade across different verbs or different senses of thesame verb.
PropBank takes the definition of verbsenses from VerbNet, and for each verb and eachsense defines the set of possible roles for that verbusage, called the roleset.
The definition of rolesetsis provided in the PropBank Frames files, which ismade available for the shared task as an official re-source to develop systems.?
Adjuncts (AM-): General arguments that any verbmay take optionally.
There are 13 types of adjuncts:AM-ADV : general-purpose AM-MOD : modal verbAM-CAU : cause AM-NEG : negation markerAM-DIR : direction AM-PNC : purposeAM-DIS : discourse marker AM-PRD : predicationAM-EXT : extent AM-REC : reciprocalAM-LOC : location AM-TMP : temporalAM-MNR : manner?
References (R-): Arguments representing argu-ments realized in other parts of the sentence.
Therole of a reference is the same as the role of the ref-erenced argument.
The label is an R- tag prefixed tothe label of the referent, e.g.
R-A1.?
Verbs (V): Participant realizing the verb of theproposition, with exactly one verb for each one.We used the February 2004 release of PropBank.
Mostpredicative verbs were annotated, although not all ofthem (for example, most of the occurrences of the verb?to have?
and ?to be?
were not annotated).
We appliedprocedures to check consistency of propositions, lookingfor overlapping arguments, and incorrect semantic rolelabels.
Also, co-referenced arguments were annotated asa single item in PropBank, and we automatically distin-guished between the referent and the reference with sim-ple rules matching pronominal expressions, which weretagged as R arguments.
A total number of 68 proposi-tions were not compliant with our procedures, and werefiltered out from the CoNLL data sets.
The predicate-argument annotations, thus, are not necessarily completein a sentence.
Table 1 provides counts of the number ofsentences, annotated propositions, distinct verbs and ar-guments in the three data sets.3.2 PreprocessingIn this section we describe the pipeline of processors tocompute the annotations which form the input part ofthe data: part-of-speech (PoS) tags, chunks, clauses andnamed entities.
The preprocessors correspond to the fol-lowing state-of-the-art systems for each level of annota-tion:Training Devel.
TestSentences 8,936 2,012 1,671Tokens 211,727 47,377 40,039Propositions 19,098 4,305 3,627Distinct Verbs 1,838 978 855All Arguments 50,182 11,121 9,598A0 12,709 2,875 2,579A1 18,046 4,064 3,429A2 4,223 954 714A3 784 149 150A4 626 147 50A5 14 4 2AA 5 0 0AM-ADV 1,727 352 307AM-CAU 283 53 49AM-DIR 231 60 50AM-DIS 1,077 204 213AM-EXT 152 49 14AM-LOC 1,279 230 228AM-MNR 1,337 334 255AM-MOD 1,753 389 337AM-NEG 687 131 127AM-PNC 446 100 85AM-PRD 10 3 3AM-REC 2 1 0AM-TMP 3,567 759 747R-A0 738 162 159R-A1 360 74 70R-A2 49 17 9R-A3 8 0 1R-AA 1 0 0R-AM-ADV 1 0 0R-AM-LOC 27 4 4R-AM-MNR 4 0 1R-AM-PNC 1 0 1R-AM-TMP 35 6 14Table 1: Counts on the three data sets.?
PoS tagger: (Gime?nez and Ma`rquez, 2003), basedon Support Vector Machines, and trained on PennTreebank sections 0?18.?
Chunker and Clause Recognizer: (Carreras andMa`rquez, 2003), based on Voted Perceptrons, andfollowing the CoNLL settings of 2000 and 2001tasks (Tjong Kim Sang and Buchholz, 2000; TjongKim Sang and De?jean, 2001).
These two processorsform a coherent partial syntax of a sentence, that is,chunks and clauses form a tree.?
Named entities with (Chieu and Ng, 2003), basedon Maximum-Entropy classifiers, and following theCoNLL-2003 task setting (Tjong Kim Sang andDe Meulder, 2003).Precision Recall F1/Acc.PoS Dev.
(acc.)
?
?
96.88PoS Test (acc.)
?
?
96.70Chunking Dev.
94.28% 93.65% 93.96Chunking Test 93.80% 92.93% 93.36Clauses Dev.
90.51% 86.12% 88.26Clauses Test 88.73% 82.92% 85.73Named Entities 88.12% 88.51% 88.31Table 2: Results of the preprocessing modules on the de-velopment and test sets.
Named Entity figures are basedon the CoNLL-2003 test set.Such processors were ran in a pipeline, from PoS tags,to chunks, clauses and finally named entities.
Table 2summarizes the performance of the processors on the de-velopment and test sections.
These figures differ from theoriginal results in the original due to a better quality of theinput information in our runs.
The figures of the namedentity extractor are based on the corpus of the CoNLL-2003 shared task, since gold annotations of named enti-ties were not available for the current corpus.3.3 FormatFigure 1 shows an example of a fully-annotated sentence.Annotations of a sentence are given using a flat represen-tation in columns, separated by spaces.
Each column en-codes an annotation by associating a tag with every word.For each sentence, the following columns are provided:1.
Words.2.
Part of Speech tags.3.
Chunks in IOB2 format.4.
Clauses in Start-End format.5.
Named Entities in IOB2 format.6.
Target verbs, marking n predicative verbs.
Thiscolumn, provided as input, specifies the governingverbs of the propositions to be analyzed.
Each targetverb is in the base form.
Occasionally this columndoes not mark any verb (i.e., n may be 0).7.
For each of the n target verbs, a column in Start-Endformat specifying the arguments of the proposition.These columns are the output of a system, that is,the ones to be predicted, and are not available forthe test set.IOB2 format.
Represents chunks which do not overlapnor embed.
Words outside a chunk receive the tag O. Forwords forming a chunk of type k, the first word receivesthe B-k tag (Begin), and the remaining words receive thetag I-k (Inside).Start-End format.
Represents non-overlappingphrases (clauses or arguments) which may be embed-ded3 inside one another.
Each tag indicates whethera clause starts or ends at that word and is of the formSTART*END.
The START part is a concatenation of (kparentheses, each representing that a phrase of type kstarts at that word.
The END part is a concatenation ofk) parentheses, each representing that a phrase of typek ends at that word.
For example, the * tag representsa word with no starts and ends; the (A0*A0) tagrepresents a word constituting an A0 argument; and the(S(S*S) tag represents a word which constitutes abase clause (labeled S) and starts another higher-levelclause.
Finally, the concatenation of all tags constitutesa well-formed bracketing.
For the particular case of splitarguments, of type k, the first part appears as a phrasewith label k, and the remaining as phrases with labelC-k (continuation prefix).
See examples of annotationsat columns 4th, 7th and 8th of Figure 1.4 Participating SystemsTen systems have participated in the CoNLL-2004 sharedtask.
They approached the task in several ways, using dif-ferent learning components and labeling strategies.
Thefollowing subsections briefly summarize the most impor-tant properties of each system and provide a qualitativecomparison between them, together with a quantitativeevaluation on the development and test sets.4.1 Learning techniquesUp to six different learning algorithms have been ap-plied in the CoNLL-2004 shared task.
None of themis new with respect to the past editions.
Two teamsused the Maximum Entropy (ME) statistical framework(Baldewein et al, 2004; Lim et al, 2004).
Two teamsused Brill?s Transformation-based Error-driven Learning(TBL) (Higgins, 2004; Williams et al, 2004).
Two othergroups applied Memory-Based Learning (MBL) (van denBosch et al, 2004; Kouchnir, 2004).
The remaining fourteams employed vector-based linear classifiers of differ-ent types: Hacioglu et al (2004) and Park et al (2004)used Support Vector Machines (SVM) with polyno-mial kernels, Carreras et al (2004) used Voted Percep-trons (VP) also with polynomial kernels, and finally,Punyakanok et al (2004) used SNoW, a Winnow-basednetwork of linear separators.
Additionally, the team ofBaldewein et al (2004) used a EM?based clustering al-gorithm for feature development (see section 4.3).As a main difference with respect to past editions, lesseffort has been put into combining different learning al-gorithms and outputs.
Instead, the main effort of partici-pants went into developing useful SRL strategies and intothe development of features (see sections 4.2 and 4.3).As an exception, van den Bosch et al (2004) applied a3Arguments in data do not embed, though format allows so.The DT B-NP (S* O - (A0* *San NNP I-NP * B-ORG - * *Francisco NNP I-NP * I-ORG - * *Examiner NNP I-NP * I-ORG - *A0) *issued VBD B-VP * O issue (V*V) *a DT B-NP * O - (A1* (A1*special JJ I-NP * O - * *edition NN I-NP * O - *A1) *A1)around IN B-PP * O - (AM-TMP* *noon NN B-NP * O - *AM-TMP) *yesterday NN B-NP * O - (AM-TMP*AM-TMP) *that WDT B-NP (S* O - (C-A1* (R-A1*R-A1)was VBD B-VP (S* O - * *filled VBN I-VP * O fill * (V*V)entirely RB B-ADVP * O - * (AM-MNR*AM-MNR)with IN B-PP * O - * *earthquake NN B-NP * O - * (A2*news NN I-NP * O - * *and CC I-NP * O - * *information NN I-NP *S)S) O - *C-A1) *A2).
.
O *S) O - * *Figure 1: An example of an annotated sentence, in columns.
Input consists of words (1st), PoS tags (2nd), base chunks(3rd), clauses (4th) and named entities (5th).
The 6th column marks target verbs, and their propositions are found inremaining columns.
According to the PropBank Frames, for issue (7th), the A0 annotates the issuer, and the A1 thething issued, which appears split into two parts.
For fill (8th), A1 is the the destination, and A2 the theme.voting strategy to derive the final sequence tagging asa voted combination of three overlapping n-gram outputsequences.
The same team also applied a meta-learningstep, by using iterative classifier stacking, for correctingsystematic errors committed by the low?level classifiers.This work is also worth mentioning because of the exten-sive work done on parameter tuning and feature selection.4.2 SRL approachesSRL is a complex task which has to be decomposed intoa number of simpler decisions and tagging schemes inorder to be addressed by learning techniques.One first issue is the annotation of the different propo-sitions of a sentence.
Most of the groups treated theannotation of semantic roles for each verb predicate asan independent problem.
An exception is the system ofCarreras et al (2004), which performs the annotation ofall propositions simultaneously.
As a consequence, theformer teams treat the problem as the recognition of se-quential structures (a.k.a.
chunking), while the latter di-rectly derives a hierarchical structure formed by the argu-ments of all propositions.
Table 3 summarizes the mainproperties of each system regarding the SRL strategy im-plemented.
This property corresponds to the first column.Regarding the labeling strategy, we can distinguish atleast three different strategies.
The first one consists ofperforming role identification directly by a IOB-type se-quence tagging.
The second approach consists of divid-ing the problem into two independent phases: recogni-tion, in which the arguments are recognized, and label-ing, in which the already recognized arguments are as-signed role labels.
The third approach also proceeds intwo phases: filtering, in which a set of argument can-didates are decided and labeling, in which the set ofoptimal arguments is derived from the proposed can-didates.
As a variant of the first two-phase strategy,van den Bosch et al (2004) first perform a direct classi-fication of chunks into argument labels, and then decidethe actual arguments in a post-process by joining previ-ously classified argument fragments.
All this informationis summarized in the second column of Table 3.An implication of implementing the two-phase strat-egy is the ability to work with argument candidates inthe second phase, allowing to develop feature patterns forcomplete arguments.
Regarding the first phase, the recog-nition of candidate arguments is performed by meansof a IOB or open?close tagging using classifiers, eitherargument?independent, or specialized by argument type.It is also worth noting that all participant systems per-formed learning of predicate-independent classifiers in-stead of specializing by the verb predicate.
Informationabout verb predicates is captured through features andsome global restrictions.Another important issue is the granularity at whichthe sentence elements are processed.
It has become veryclear that a good election for this problem is phrase-by-phrase processing (P-by-P, using the notation introducedby Hacioglu et al (2004)) instead of word-by-word (W-by-W).
The motivation is twofold: (1) phrase boundariesare almost always consistent with argument boundaries;(2) P-by-P processing is computationally less expensiveand allows to explore a relatively larger context.
Most ofthe groups performed a P-by-P processing, but admittinga processing by words within the target verb chunks.
Thesystem by Baldewein et al (2004) works with a bit moregeneral elements called ?chunk sequences?, extracted ina preprocess using heuristic rules.
This information ispresented in the third column of Table 3.Information regarding clauses has proven to be veryuseful, as can be seen in section 4.3.
All systems capturedsome kind of clause information through feature codifica-tion.
However, some of the systems restrict the search forarguments only to the immediate clause (Park et al, 2004;Williams et al, 2004) and others use the clause hierarchyto guide the exploration of the sentence (Lim et al, 2004;Carreras et al, 2004).Very relevant to the SRL strategy is the availability ofglobal sentential information when decisions are taken.Almost all of the systems try to capture some global levelinformation by collecting features describing the targetpredicate and its context, the ?syntactic path?
from theelement under consideration to the predicate, etc.
(seesection 4.3).
But only some of them include a globaloptimization procedure at sentence level in the labelingstrategy.
The systems working with Maximum EntropyModels (Baldewein et al, 2004; Lim et al, 2004) usebeam search to find taggings that maximize the prob-ability of the output sequence.
Carreras et al (2004)and Punyakanok et al (2004) also define a global scor-ing function to maximize.
At this point, the system ofPunyakanok et al (2004) deserves special consideration,since it formally implements a set of structural and lin-guistic constraints directly in the global cost function tomaximize.
These constraints act as a filter for valid out-put sequences and ensure coherence of the output.
Au-thors refer to this part of the system as the inferencelayer and they implement it using integer linear program-ming.
The iterative classifier stacking mechanism usedby van den Bosch et al (2004) also tries to alleviate theproblem of locality of the low-level classifiers.
This in-formation is found in the fourth column of Table 3.Finally, some systems use some kind of postprocess-ing to ensure coherence of the final labeling, correct somesystematic errors, or to treat some types of adjunctive ar-guments.
In most of the cases, this postprocess is per-formed on the basis of simple ad-hoc rules.
This infor-mation is included in the last column of Table 3.4.3 FeaturesWith a very few exceptions all the participant systemshave used all levels of linguistic information provided inthe training data sets, that is, words, PoS and chunk la-bels, clauses, and named entities.It is worth mentioning that the general type of featuresprop.
lab.
gran.
glob.
posthacioglu s t P-by-P no nopunyakanok s fl W-by-W yes nocarreras j fl P-by-P yes nolim s t P-by-P yes nopark s rc P-by-P no yeshiggins s t W-by-W no yesvan den bosch s cj P-by-P part.
yeskouchnir s rc P-by-P no yesbaldewein s rc P-by-P yes nowilliams s t mixed no noTable 3: Main properties of the SRL strategies imple-mented by the ten participant teams (sorted by perfor-mance on the test set).
?prop.?
stands for the treatment ofall propositions of a sentence; possible values are: s (sep-arate) and j (joint).
?lab.?
stands for labeling strategy;possible values are: t (one step tagging), rc (recognition+ classification), fl (filtering + labeling), cj (classifica-tion + joining).
?gran.?
stands for granularity; ?glob.
?stands for global optimization.
?post?
stands for post-processing.derived from the basic information are strongly inspiredby previous works on the SRL task (Gildea and Jurafsky,2002; Surdeanu et al, 2003; Pradhan et al, 2003a).
Manysystems used the same kind of ideas but implementedin different ways, since the particular learning strategiesused (see section 4.2) impose different constraints on thetype of information available or the way of expressing it.As a general idea, we can divide the features into fourtypes: (1) basic features, evaluating some kind of localinformation on the context of the word or constituent be-ing treated; (2) Features characterizing the internal struc-ture of a candidate argument; (3) Features describingproperties of the target verb predicate; (4) Features thatcapture the relations between the verb predicate and theconstituent under consideration.All systems used some kind of basic features.
Roughlyspeaking, they consist of words, PoS tags, chunks, clauselabels, and named entities extracted from a window-based context.
These values can be considered withor without the relative position with respect to the el-ement under consideration, and some n-grams of themcan also be computed.
If the granularity of the sys-tem is at phrase level then typically a representativehead word of the phrase is used as lexical information.As an exception to the general approach, the system ofWilliams et al (2004) does not make use of word forms.The rest of the features are more interesting since theyare task dependent, and deserve special attention.
Table4 summarizes the type of features exploited by systems.To represent an argument itself, few attributes are ofgeneral usage.
Some systems count the length of it,with different granularities.
Others make use of heuris-tics to derive its syntactic type.
There are systems thatextract a structured representation of the argument, ei-ther homogeneous (capturing different sequences of headwords, PoS tags, chunks or clauses), or heterogeneous(combining all elements, based on the syntactic hierar-chy).
A few systems have captured the existence ofneighboring arguments, previously identified in the pro-cess.
Interestingly, the system of Lim et al (2004) rep-resents the context of an argument relative to the syntac-tic hierarchy by means of relative constituent sequencesand syntactic levels.
Concerning lexicalization of theargument, most of the techniques rely on head wordrules based on Collins?, or content word rules as inSurdeanu et al (2003).
Only Carreras et al (2004) de-cide to use a bag-of-words model, apart from heuristic-based lexicalization.Regarding the target verb, the voice feature of the verbis generally used, in addition to basic features capturingthe form and PoS tag of the verb.
Some systems capturedstatistics on frequent argument patterns for each predi-cate.
Also, systems represented the elements in the prox-imity of the target verb, inspired by local subcategoriza-tion patterns of a predicate.As for features related to a constituent-predicate pair,all systems use the simple feature describing the relativeposition between them, and to a lesser degree, the dis-tance and the difference in clausal levels.
Again, there isa general tendency to describe the structured path fromthe argument to the verb.
Its design goes from sim-ple homogeneous sequences of head words or chunks, tomore sophisticated paths combining chunks and clauses,and capturing hierarchical properties.
The system ofPark et al (2004) also tracks the number of different syn-tactic elements found between the pair.
Remarkably, thesystem of Baldewein et al (2004) uses an EM clusteringtechnique to derive features representing the affinity of anargument and a predicate.On top of basic feature extraction, all teams work-ing with SVM and VP used polynomial kernels of de-gree 2.
Similar in expressiveness, the system designedby Punyakanok et al (2004) expanded the feature spacewith all pairs of basic features.4.4 EvaluationA baseline rate was computed for the task.
It was pro-duced by a system developed by Erik Tjong Kim Sang,from the University of Antwerp, Belgium.
The base-line processor finds semantic roles based on the followingseven rules:?
Tag target verb and successive particles as V.?
Tag not and n?t in target verb chunk as AM-NEG.?
Tag modal verbs in target verb chunk as AM-MOD.?
Tag first NP before target verb as A0.?
Tag first NP after target verb as A1.?
Tag that, which and who before target verb asR-A0.?
Switch A0 and A1, and R-A0 and R-A1 if the targetverb is part of a passive VP chunk.
A VP chunk isconsidered in passive voice if it contains a form ofto be and the verb does not end in ing.Table 5 presents the overall results obtained by theten participating systems, on the development and testsets.
The best performance was obtained by the SVM-based IOB tagger of (Hacioglu et al, 2004), which al-most reached the performance of 70 in F1 on the test.The seven best systems obtained F1 scores in the rangeof 60-70, and only three systems scored below that.Comparing the results across development and test cor-pora, most systems experienced a decrease in perfor-mance between 1.5 and 3 points.
As in previous editionsof the shared task, we attribute this behavior to a greaterdifficulty of the test set instead of an overfitting effect.Interestingly, the three systems performing below 60 inthe development set did not experienced this decrease.
Infact (Williams et al, 2004) and (Baldewein et al, 2004)even improved the results on the test set.Table 6 details the performance of systems for the A0-A4 arguments, on the test set.
Consistently, the best per-forming system of the task also outperforms all other sys-tems on these semantic roles.5 ConclusionWe have described the CoNLL-2004 shared task on se-mantic role labeling.
The task was based on the Prop-Bank corpus, and the challenge was to come up with ma-chine learning techniques to recognize and label semanticroles on the basis of partial syntactic structure.
Ten sys-tems have participated to the task, contributing with a va-riety of standard or novel learning architectures.
The bestsystem, presented by the most experienced group on thetask (Hacioglu et al, 2004), achieved a moderate perfor-mance of 69.49 at the F1 measure.
It is based on a SVMtagging system, performing IOB decisions on the chunksof the sentence, and exploiting a wide variety of featuresbased on partial syntax.Most of the systems advance the state-of-the-art on se-mantic role labeling on the basis of partial syntax.
How-ever, state-of-the-art systems working with full syntaxstill perform substantially better, although far from a de-sired behavior for real-task application.
Two questionsremain open: which syntactic structures are needed as in-put for the task, and what other sources of information arerequired to obtain a real-world, accurate performance.As a future line, a more thorough experimental eval-uation is required to see which are the components thatsy ne al at as aw an vv vs vf vc rp di pa exhacioglu + + + ?
?
+ ?
+ + ?
+ + + + +punyakanok + + + + + + ?
+ ?
+ + + ?
+ +carreras + ?
?
?
+ + ?
+ ?
?
?
+ ?
+ +lim + ?
?
?
?
+ + + ?
?
?
+ ?
+ ?park + ?
?
?
?
?
?
+ ?
?
+ + + + +higgins + + ?
?
?
?
+ + ?
?
?
+ + + ?van den bosch + + ?
?
?
?
?
+ + ?
?
+ + ?
?kouchnir + ?
+ ?
+ + ?
+ ?
+ ?
+ + ?
?baldewein + + + + + + ?
+ + ?
?
+ + ?
?williams + + ?
?
?
?
?
?
?
?
?
+ ?
?
?Table 4: Main feature types used by the 10 participating systems in the CoNLL-2004 shared task, sorted by perfor-mance on the test set.
?sy?
: use of partial syntax (all levels); ?ne?
: use of named entities; ?al?
: argument length; ?at?
:argument type; ?as?
: argument internal structure; ?aw?
: head-word lexicalization of arguments; ?an?
: neighboringarguments; ?vv?
: verb voice; ?vs?
: verb statistics; ?vf?
: verb features derived from PropBank frames; ?vc?
: verb localcontext; ?rp?
: relative position; ?di?
: distance (horizontal or in the hierarchy); ?pa?
: path; ?ex?
: feature expansion.most contributed to the performance of systems.AcknowledgementsAuthors would like to thank the following people andinstitutions.
The PropBank team, and specially MarthaPalmer and Scott Cotton, for making the corpus available.The CoNLL-2004 board for fruitful discussions and sug-gestions.
In particular, Erik Tjong Kim Sang for usefulcomments from his valuable experience, and for makingthe baseline SRL processor available.
Llu?
?s Padro?
andMihai Surdeanu, Grzegorz Chrupa?a, and Hwee Tou Ngfor helping us in the reviewing process and the prepara-tion of this document.
Finally, the teams contributing toshared task, for their great interest in participating.This work has been partially funded by the EuropeanCommission (Meaning, IST-2001-34460) and the Span-ish Research Department (Aliado, TIC2002-04447-C02).Xavier Carreras is supported by a pre-doctoral grant fromthe Catalan Research Department.ReferencesUlrike Baldewein, Katrin Erk, Sebastian Pado?, and DetlefPrescher.
2004.
Semantic role labeling with chunksequences.
In Proceedings of CoNLL-2004.Xavier Carreras and Llu?
?s Ma`rquez.
2003.
Phrase recog-nition by filtering and ranking with perceptrons.
InProceedings of RANLP-2003, Borovets, Bulgaria.Xavier Carreras, Llu?
?s Ma`rquez, and Grzegorz Chrupa?a.2004.
Hierarchical recognition of propositional argu-ments with perceptrons.
In Proceedings of CoNLL-2004.John Chen and Owen Rambow.
2003.
Use of deep lin-guistic features for the recognition and labeling of se-mantic arguments.
In Proceedings of EMNLP-2003,Sapporo, Japan.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.
InProceedings of CoNLL-2003, Edmonton, Canada.Charles J. Fillmore, Charles Wooters, and Collin F.Baker.
2001.
Building a large lexical databank whichprovides deep semantics.
In Proceedings of the Pa-cific Asian Conference on Language, Informa tion andComputation, Hong Kong, China.Michael Fleischman, Namhee Kwon, and Eduard Hovy.2003.
Maximum entropy models for framenet clas-sification.
In Proceedings of EMNLP-2003, Sapporo,Japan.Daniel Gildea and Julia Hockenmaier.
2003.
Identifyingsemantic roles using combinatory categorial grammar.In Proceedings of EMNLP-2003, Sapporo, Japan.Daniel Gildea and Daniel Jurafsky.
2002.
Automatic la-beling of semantic roles.
Computational Linguistics,28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
In Pro-ceedings of ACL 2002, Philadelphia, USA.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2003.
Fast and accu-rate part-of-speech tagging: The svm approach revis-ited.
In Proceedings of RANLP-2003, Borovets, Bul-garia.Kadri Hacioglu and Wayne Ward.
2003.
Target word de-tection and semantic role chunking using support vec-tor machines.
In Proceedings of HLT-NAACL 2003,Edmonton, Canada.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, James H.Martin, and Daniel Jurafsky.
2004.
Semantic role la-beling by tagging syntactic chunks.
In Proceedings ofCoNLL-2004.Derrick Higgins.
2004.
A transformation-based ap-proach to argument labeling.
In Proceedings ofCoNLL-2004.development Precision Recall F1hacioglu 74.18% 69.43% 71.72punyakanok 71.96% 64.93% 68.26carreras 73.40% 63.70% 68.21lim 69.78% 62.57% 65.97park 67.27% 64.36% 65.78higgins 65.59% 60.16% 62.76van den bosch 69.06% 57.84% 62.95kouchnir 44.93% 63.12% 52.50baldewein 64.90% 41.61% 50.71williams 53.37% 32.43% 40.35baseline 50.63% 30.30% 37.91test Precision Recall F1hacioglu 72.43% 66.77% 69.49punyakanok 70.07% 63.07% 66.39carreras 71.81% 61.11% 66.03lim 68.42% 61.47% 64.76park 65.63% 62.43% 63.99higgins 64.17% 57.52% 60.66van den bosch 67.12% 54.46% 60.13kouchnir 56.86% 49.95% 53.18baldewein 65.73% 42.60% 51.70williams 58.08% 34.75% 43.48baseline 54.60% 31.39% 39.87Table 5: Overall precision, recall and F1 rates obtained bythe ten participating systems in the CoNLL-2004 sharedtask on the development and test sets.Beata Kouchnir.
2004.
A memory-based approach forsemantic role labeling.
In Proceedings of CoNLL-2004.Joon-Ho Lim, Young-Sook Hwang, So-Young Park, andHae-Chang Rim.
2004.
Semantic role labeling usingmaximum entropy model.
In Proceedings of CoNLL-2004.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of English: the Penn Treebank.
ComputationalLinguistics, 19.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2004.
The proposition bank: An annotated corpus ofsemantic roles.
Computational Linguistics.
Submit-ted.Kyung-Mi Park, Young-Sook Hwang, and Hae-ChangRim.
2004.
Two-phase semantic role labelingbased on support vector machines.
In Proceedings ofCoNLL-2004.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Jurafsky.2003a.
Support vector learning for semantic argumentclassification.
Technical Report TR-CSLR-2003-03,Center for Spoken Language Research, University ofColorado.A0 A1 A2 A3 A4hacioglu 81.37 71.63 49.33 51.11 66.67punyakanok 79.38 68.16 46.69 34.04 65.22carreras 79.05 66.96 43.28 31.22 62.07lim 77.42 66.00 49.07 41.77 54.55park 76.38 66.14 46.57 42.32 51.76higgins 70.67 62.72 45.52 40.00 39.64van den bosch 74.95 60.83 40.41 37.44 62.37kouchnir 65.49 54.48 30.95 19.71 36.07baldewein 66.76 53.37 37.60 22.89 27.69williams 56.24 49.05 00.00 00.00 00.00baseline 57.65 34.19 00.00 00.00 00.00Table 6: F1 scores on the most frequent core argumenttypes obtained by the ten participating systems in theCoNLL-2004 shared task on the test set.
Systems sortedby overall performance on the test set.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H.Martin, and Daniel Jurafsky.
2003b.
Semantic roleparsing: Adding semantic structure to unstructuredtext.
In Proceedings of the International Conferenceon Data Mining (ICDM-2003), Melbourne, USA.Vasin Punyakanok, Dan Roth, Wen-Tau Yih, Dav Zimak,and Yuancheng Tu.
2004.
Semantic role labeling viageneralized inference over classifiers.
In Proceedingsof CoNLL-2004.Mihai Surdeanu, Sanda Harabagiu, John Williams, andPaul Aarseth.
2003.
Using predicate-argument struc-tures for information extraction.
In Proceedings ofACL 2003, Sapporo, Japan.Cynthia A. Thompson, Roger Levy, and Christopher D.Manning.
2003.
A generative model for semanticrole labeling.
In Proceedings of ECML?03, Dubrovnik,Croatia.E.
F. Tjong Kim Sang and S. Buchholz.
2000.
Intro-duction to the CoNLL-2000 shared task: Chunking.In Proceedings of the 4th Conference on Natural Lan-guage Learning, CoNLL-2000.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof CoNLL-2003.Erik F. Tjong Kim Sang and Herve?
De?jean.
2001.
Intro-duction to the CoNLL-2001 shared task: Clause identi-fication.
In Proceedings of the 5th Conference on Nat-ural Language Learning, CoNLL-2001.Antal van den Bosch, Sander Canisius, Walter Daele-mans, Iris Hendrickx, and Erik Tjong Kim Sang.2004.
Memory-based semantic role labeling: Optimiz-ing features, algorithm, and output.
In Proceedings ofCoNLL-2004.Ken Williams, Christopher Dozier, and Andrew McCul-loh.
2004.
Learning transformation rules for semanticrole labeling.
In Proceedings of CoNLL-2004.
