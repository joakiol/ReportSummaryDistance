Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1298?1307,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsRelation Classification via Multi-Level Attention CNNsLinlin Wang1?, Zhu Cao1?, Gerard de Melo2, Zhiyuan Liu3?1Institute for Interdisciplinary Information Sciences, Tsinghua University, Beijing, China2Department of Computer Science, Rutgers University, Piscataway, NJ, USA3State Key Laboratory of Intelligent Technology and Systems,Tsinghua National Laboratory for Information Science and Technology,Department of Computer Science and Technology, Tsinghua University, Beijing, China{ll-wang13,cao-z13}@mails.tsinghua.edu.cn, gdm@demelo.orgAbstractRelation classification is a crucial ingredi-ent in numerous information extraction sys-tems seeking to mine structured facts fromtext.
We propose a novel convolutionalneural network architecture for this task,relying on two levels of attention in orderto better discern patterns in heterogeneouscontexts.
This architecture enables end-to-end learning from task-specific labeleddata, forgoing the need for external knowl-edge such as explicit dependency structures.Experiments show that our model outper-forms previous state-of-the-art methods, in-cluding those relying on much richer formsof prior knowledge.1 IntroductionRelation classification is the task of identifying thesemantic relation holding between two nominal en-tities in text.
It is a crucial component in naturallanguage processing systems that need to mine ex-plicit facts from text, e.g.
for various informationextraction applications as well as for question an-swering and knowledge base completion (Tandonet al, 2011; Chen et al, 2015).
For instance, giventhe example input?Fizzy [drinks] and meat cause heart disease and [diabetes].
?with annotated target entity mentions e1= ?drinks?and e2= ?diabetes?, the goal would be to automati-cally recognize that this sentence expresses a cause-effect relationship between e1and e2, for whichwe use the notation Cause-Effect(e1,e2).
Accuraterelation classification facilitates precise sentenceinterpretations, discourse processing, and higher-level NLP tasks (Hendrickx et al, 2010).
Thus,?Equal contribution.
?Corresponding author.
Email: liuzy@tsinghua.edu.cnrelation classification has attracted considerable at-tention from researchers over the course of the pastdecades (Zhang, 2004; Qian et al, 2009; Rink andHarabagiu, 2010).In the example given above, the verb corre-sponds quite closely to the desired target relation.However, in the wild, we encounter a multitudeof different ways of expressing the same kind ofrelationship.
This challenging variability can belexical, syntactic, or even pragmatic in nature.
Aneffective solution needs to be able to account foruseful semantic and syntactic features not only forthe meanings of the target entities at the lexicallevel, but also for their immediate context and forthe overall sentence structure.Thus, it is not surprising that numerous feature-and kernel-based approaches have been proposed,many of which rely on a full-fledged NLP stack,including POS tagging, morphological analysis, de-pendency parsing, and occasionally semantic anal-ysis, as well as on knowledge resources to capturelexical and semantic features (Kambhatla, 2004;Zhou et al, 2005; Suchanek et al, 2006; Qian etal., 2008; Mooney and Bunescu, 2005; Bunescuand Mooney, 2005).
In recent years, we have seen amove towards deep architectures that are capable oflearning relevant representations and features with-out extensive manual feature engineering or useof external resources.
A number of convolutionalneural network (CNN), recurrent neural network(RNN), and other neural architectures have beenproposed for relation classification (Zeng et al,2014; dos Santos et al, 2015; Xu et al, 2015b).Still, these models often fail to identify criticalcues, and many of them still require an externaldependency parser.We propose a novel CNN architecture that ad-dresses some of the shortcomings of previous ap-proaches.
Our key contributions are as follows:1.
Our CNN architecture relies on a novel multi-1298level attention mechanism to capture bothentity-specific attention (primary attention atthe input level, with respect to the target en-tities) and relation-specific pooling attention(secondary attention with respect to the targetrelations).
This allows it to detect more subtlecues despite the heterogeneous structure ofthe input sentences, enabling it to automati-cally learn which parts are relevant for a givenclassification.2.
We introduce a novel pair-wise margin-basedobjective function that proves superior to stan-dard loss functions.3.
We obtain the new state-of-the-art results forrelation classification with an F1 score of88.0% on the SemEval 2010 Task 8 dataset,outperforming methods relying on signifi-cantly richer prior knowledge.2 Related WorkApart from a few unsupervised clustering meth-ods (Hasegawa et al, 2004; Chen et al, 2005),the majority of work on relation classification hasbeen supervised, typically cast as a standard multi-class or multi-label classification task.
Traditionalfeature-based methods rely on a set of featurescomputed from the output of an explicit linguis-tic preprocessing step (Kambhatla, 2004; Zhou etal., 2005; Boschee et al, 2005; Suchanek et al,2006; Chan and Roth, 2010; Nguyen and Grish-man, 2014), while kernel-based methods make useof convolution tree kernels (Qian et al, 2008), sub-sequence kernels (Mooney and Bunescu, 2005),or dependency tree kernels (Bunescu and Mooney,2005).
These methods thus all depend either oncarefully handcrafted features, often chosen on atrial-and-error basis, or on elaborately designedkernels, which in turn are often derived from otherpre-trained NLP tools or lexical and semantic re-sources.
Although such approaches can benefitfrom the external NLP tools to discover the dis-crete structure of a sentence, syntactic parsing iserror-prone and relying on its success may alsoimpede performance (Bach and Badaskar, 2007).Further downsides include their limited lexical gen-eralization abilities for unseen words and their lackof robustness when applied to new domains, genres,or languages.In recent years, deep neural networks haveshown promising results.
The Recursive Matrix-Vector Model (MV-RNN) by Socher et al (2012)sought to capture the compositional aspects of thesentence semantics by exploiting syntactic trees.Zeng et al (2014) proposed a deep convolutionalneural network with softmax classification, extract-ing lexical and sentence level features.
However,these approaches still depend on additional featuresfrom lexical resources and NLP toolkits.
Yu et al(2014) proposed the Factor-based CompositionalEmbedding Model, which uses syntactic depen-dency trees together with sentence-level embed-dings.
In addition to dos Santos et al (2015), whoproposed the Ranking CNN (CR-CNN) model witha class embedding matrix, Miwa and Bansal (2016)similarly observed that LSTM-based RNNs are out-performed by models using CNNs, due to limitedlinguistic structure captured in the network archi-tecture.
Some more elaborate variants have beenproposed to address this, including bidirectionalLSTMs (Zhang et al, 2015), deep recurrent neuralnetworks (Xu et al, 2016), and bidirectional tree-structured LSTM-RNNs (Miwa and Bansal, 2016).Several recent works also reintroduce a dependencytree-based design, e.g., RNNs operating on syntac-tic trees (Hashimoto et al, 2013), shortest depen-dency path-based CNNs (Xu et al, 2015a), andthe SDP-LSTM model (Xu et al, 2015b).
Finally,Nguyen and Grishman (2015) train both CNNs andRNNs and variously aggregate their outputs usingvoting, stacking, or log-linear modeling (Nguyenand Grishman, 2015).
Although these recent mod-els achieve solid results, ideally, we would want asimple yet effective architecture that does not re-quire dependency parsing or training multiple mod-els.
Our experiments in Section 4 demonstrate thatwe can indeed achieve this, while also obtainingsubstantial improvements in terms of the obtainedF1 scores.3 The Proposed ModelGiven a sentence S with a labeled pair of entitymentions e1and e2(as in our example from Sec-tion 1), relation classification is the task of identify-ing the semantic relation holding between e1and e2among a set of candidate relation types (Hendrickxet al, 2010).
Since the only input is a raw sen-tence with two marked mentions, it is non-trivial toobtain all the lexical, semantic and syntactic cuesnecessary to make an accurate prediction.To this end, we propose a novel multi-levelattention-based convolution neural network model.A schematic overview of our architecture is given1299Notation Definition Notation DefinitionwMiFinal word emb.
ziContext emb.WfConv.
weight BfConv.
biaswONetwork output WLRelation emb.AjInput att.
ApPooling att.G Correlation matrixTable 1: Overview of main notation.in Figure 1.
The input sentence is first encodedusing word vector representations, exploiting thecontext and a positional encoding to better capturethe word order.
A primary attention mechanism,based on diagonal matrices is used to capture therelevance of words with respect to the target en-tities.
To the resulting output matrix, one thenapplies a convolution operation in order to capturecontextual information such as relevant n-grams,followed by max-pooling.
A secondary attentionpooling layer is used to determine the most usefulconvolved features for relation classification fromthe output based on an attention pooling matrix.The remainder of this section will provide furtherdetails about this architecture.
Table 1 provides anoverview of the notation we will use for this.
Thefinal output is given by a new objective function,described below.3.1 Classification ObjectiveWe begin with top-down design considerations forthe relation classification architecture.
For a givensentence S, our network will ultimately outputsome wO.
For every output relation y ?
Y , weassume there is a corresponding output embeddingWLy, which will automatically be learnt by the net-work (dos Santos et al, 2015).We propose a novel distance function ??
(S) thatmeasures the proximity of the predicted networkoutput wOto a candidate relation y as follows.??
(S, y) =????wO|wO|?WLy????
(1)using the L2norm (note that WLyare already nor-malized).
Based on this distance function, we de-sign a margin-based pairwise loss function L asL =[??
(S, y) + (1?
??
(S, y??
))]+ ???
?2=[1 +????wO|wO|?WLy?????????wO|wO|?WLy??????
]+ ???
?2, (2)where 1 is the margin, ?
is a parameter, ??
(S, y)is the distance between the predicted label embed-dingWLand the ground truth label y and ??
(S, y??
)refers to the distance between wOand a selectedincorrect relation label y??.
The latter is chosenas the one with the highest score among all incor-rect classes (Weston et al, 2011; dos Santos et al,2015), i.e.y?
?= argmaxy??Y,y?6=y?
(S, y?).
(3)This margin-based objective has the advantageof a strong interpretability and effectiveness com-pared with empirical loss functions such as theranking loss function in the CR-CNN approach bydos Santos et al (2015).
Based on a distance func-tion motived by word analogies (Mikolov et al,2013b), we minimize the gap between predictedoutputs and ground-truth labels, while maximizingthe distance with the selected incorrect class.
Byminimizing this pairwise loss function iteratively(see Section 3.5), ??
(S, y) are encouraged to de-crease, while ??
(S, y??)
increase.3.2 Input RepresentationGiven a sentence S = (w1, w2, ..., wn) withmarked entity mentions e1(=wp) and e2(=wt),(p, t ?
[1, n], p 6= t), we first transform everyword into a real-valued vector to provide lexical-semantic features.
Given a word embedding matrixWVof dimensionality dw?
|V | , where V is theinput vocabulary and dwis the word vector dimen-sionality (a hyper-parameter), we map every witoa column vector wdi?
Rdw.To additionally capture information about therelationship to the target entities, we incorporateword position embeddings (WPE) to reflect the rel-ative distances between the i-th word to the twomarked entity mentions (Zeng et al, 2014; dosSantos et al, 2015).
For the given sentence inFig.
1, the relative distances of word ?and?
to en-tity e1?drinks?
and e2?diabetes?
are ?1 and 6,respectively.
Every relative distance is mappedto a randomly initialized position vector in Rdp,where dpis a hyper-parameter.
For a given wordi, we obtain two position vectors wpi,1and wpi,2,with regard to entities e1and e2, respectively.The overall word embedding for the i-th word iswMi= [(wdi)?, (wpi,1)?, (wpi,2)?
]?.Using a sliding window of size k centeredaround the i-th word, we encode k successive1300Pooling att.
matrixfizzyInput S with marked entities: Fizzy [drinks]     and meat cause heart disease and [diabetes]     .Inputatt.matrixInputatt.matrixdrinksandmeatcauseheartdiseaseanddiabetesfizzydrinksandmeatcauseheartdiseaseanddiabetesEntitydiabetesLookup table&              ,Lookup tablexxConvolution layer outputConvolutionwithkernelWindowoperationMaxxxxEntitydrinksAttention basedconvolution inputFigure 1: Schematic overview of our Multi-Level Attention Convolutional Neural Networkswords into a vector zi?
R(dw+2dp)kto incorpo-rate contextual information aszi= [(wMi?
(k?1)/2)?, ..., (wMi+(k?1)/2)?]?
(4)An extra padding token is repeated multiple timesfor well-definedness at the beginning and end ofthe input.3.3 Input Attention MechanismWhile position-based encodings are useful, we con-jecture that they do not suffice to fully capture therelationships of specific words with the target en-tities and the influence that they may bear on thetarget relations of interest.
We design our model soas to automatically identify the parts of the inputsentence that are relevant for relation classification.Attention mechanisms have successfully beenapplied to sequence-to-sequence learning taskssuch as machine translation (Bahdanau et al, 2015;Meng et al, 2015) and abstractive sentence sum-marization (Rush et al, 2015), as well as to taskssuch as modeling sentence pairs (Yin et al, 2015)and question answering (Santos et al, 2016).
Todate, these mechanisms have generally been usedto allow for an alignment of the input and outputsequence, e.g.
the source and target sentence in ma-chine translation, or for an alignment between twoinput sentences as in sentence similarity scoringand question answering.fizzy drinks      and  meat cause heart  disease  and  diabetesEntity 1: drinksEntity 2: diabetesLookuptableGiven Text S:...........................Word&PositionembeddingWindowoperationDiagonalInput att.
matrix(S,drinks)DiagonalInput att.
matrix(S,diabetes)............Figure 2: Input and Primary AttentionIn our work, we apply the idea of modeling atten-tion to a rather different kind of scenario involvingheterogeneous objects, namely a sentence and twoentities.
With this, we seek to give our model thecapability to determine which parts of the sentenceare most influential with respect to the two enti-ties of interest.
Consider that in a long sentencewith multiple clauses, perhaps only a single verbor noun might stand in a relevant relationship witha given target entity.As depicted in Fig.
2, the input representationlayer is used in conjunction with diagonal attentionmatrices and convolutional input composition.Contextual Relevance Matrices.
Consider theexample in Fig.
1, where the non-entity word?cause?
is of particular significance in determiningthe relation.
Fortunately, we can exploit the fact1301that there is a salient connection between the words?cause?
and ?diabetes?
also in terms of corpus cooc-currences.
We introduce two diagonal attentionmatrices Ajwith values Aji,i= f(ej, wi) to char-acterize the strength of contextual correlations andconnections between entity mention ejand wordwi.
The scoring function f is computed as the in-ner product between the respective embeddings ofword wiand entity ej, and is parametrized into thenetwork and updated during the training process.Given the Ajmatrices, we define?ji=exp(Aji,i)?ni?=1exp(Aji?,i?
), (5)to quantify the relative degree of relevance of the i-th word with respect to the j-th entity (j ?
{1, 2}).Input Attention Composition.
Next, we takethe two relevance factors ?1iand ?2iand modeltheir joint impact for recognizing the relation viasimple averaging asri= zi?1i+ ?2i2.
(6)Apart from this default choice, we also evaluatetwo additional variants.
The first (Variant-1) con-catenates the word vectors asri= [(zi?1i)?, (zi?2i)?
]?, (7)to obtain an information-enriched input attentioncomponent for this specific word, which containsthe relation relevance to both entity 1 and entity 2.The second variant (Variant-2) interprets rela-tions as mappings between two entities, and com-bines the two entity-specific weights asri= zi?1i?
?2i2, (8)to capture the relation between them.Based on these ri, the final output of theinput attention component is the matrix R =[r1, r2, .
.
.
, rn], where n is the sentence length.3.4 Convolutional Max-Pooling withSecondary AttentionAfter this operation, we apply convolutional max-pooling with another secondary attention model toextract more abstract higher-level features from theprevious layer?s output matrix R.Convolution Layer.
A convolutional layer may,for instance, learn to recognize short phrases suchas trigrams.
Given our newly generated inputattention-based representation R, we accordinglyapply a filter of size dcas a weight matrix Wfofsize dc?
k(dw+ 2dp).
Then we add a linear biasBf, followed by a non-linear hyperbolic tangenttransformation to represent features as follows:R?= tanh(WfR+Bf).
(9)Attention-Based Pooling.
Instead of regularpooling, we rely on an attention-based poolingstrategy to determine the importance of individualwindows in R?, as encoded by the convolutionalkernel.
Some of these windows could representmeaningful n-grams in the input.
The goal here isto select those parts of R?that are relevant withrespect to our objective from Section 3.1, whichessentially calls for a relation encoding process,while neglecting sentence parts that are irrelevantfor this process.We proceed by first creating a correlation mod-eling matrix G that captures pertinent connectionsbetween the convolved context windows from thesentence and the relation class embedding WLin-troduced earlier in Section 3.1:G = R?
?U WL, (10)where U is a weighting matrix learnt by the net-work.Then we adopt a softmax function to deal withthis correlation modeling matrix G to obtain anattention pooling matrix ApasApi,j=exp(Gi,j)?ni?=1exp(Gi?,j), (11)where Gi,jis the (i, j)-th entry of G and Api,jis the(i, j)-th entry of Ap.Finally, we multiply this attention pooling matrixwith the convolved output R?to highlight impor-tant individual phrase-level components, and applya max operation to select the most salient one (Yinet al, 2015; Santos et al, 2016) for a given dimen-sion of the output.
More precisely, we obtain theoutput representation wOas follows in Eq.
(12):wOi= maxj(R?Ap)i,j, (12)where wOiis the i-th entry of wOand (R?Ap)i,jisthe (i, j)-th entry of R?Ap.13023.5 Training ProcedureWe rely on stochastic gradient descent (SGD) to up-date the parameters with respect to the loss functionin Eq.
(2) as follows:?
?= ?
+ ?d(?|S|i=1[??
(Si, y) + (1?
??
(Si, y?
?i))])d?+ ?1d(?||?||2)d?
(13)where ?
and ?1are learning rates, and incorporat-ing the ?
parameter from Eq.
(2).4 Experiments4.1 Experimental SetupDataset and Metric.
We conduct our exper-iments on the commonly used SemEval-2010Task 8 dataset (Hendrickx et al, 2010), whichcontains 10,717 sentences for nine types of an-notated relations, together with an additional?Other?
type.
The nine types are: Cause-Effect,Component-Whole, Content-Container, Entity-Destination, Entity-Origin, Instrument-Agency,Member-Collection, Message-Topic, and Product-Producer, while the relation type ?Other?
indicatesthat the relation expressed in the sentence is notamong the nine types.
However, for each of theaforementioned relation types, the two entities canalso appear in inverse order, which implies thatthe sentence needs to be regarded as expressinga different relation, namely the respective inverseone.
For example, Cause-Effect(e1,e2) and Cause-Effect(e2,e1) can be considered two distinct rela-tions, so the total number |Y| of relation types is19.
The SemEval-2010 Task 8 dataset consists of atraining set of 8,000 examples, and a test set withthe remaining examples.
We evaluate the modelsusing the official scorer in terms of the Macro-F1score over the nine relation pairs (excluding Other).Settings.
We use the word2vec skip-gram model(Mikolov et al, 2013a) to learn initial word rep-resentations on Wikipedia.
Other matrices are ini-tialized with random values following a Gaussiandistribution.
We apply a cross-validation procedureon the training data to select suitable hyperparam-eters.
The choices generated by this process aregiven in Table 2.4.2 Experimental ResultsTable 3 provides a detailed comparison of ourMulti-Level Attention CNN model with previousParameter Parameter Name ValuedpWord Pos.
Emb.
Size 25dcConv.
Size 1000k Word Window Size 3?
Learning rate 0.03?1Learning rate 0.0001Table 2: Hyperparameters.approaches.
We observe that our novel attention-based architecture achieves new state-of-the-art re-sults on this relation classification dataset.
Att-Input-CNN relies only on the primal attention atthe input level, performing standard max-poolingafter the convolution layer to generate the networkoutput wO, in which the new objective functionis utilized.
With Att-Input-CNN, we achieve anF1-score of 87.5%, thus already outperforming notonly the original winner of the SemEval task, anSVM-based approach (82.2%), but also the well-known CR-CNN model (84.1%) with a relativeimprovement of 4.04%, and the newly releasedDRNNs (85.8%) with a relative improvement of2.0%, although the latter approach depends on theStanford parser to obtain dependency parse infor-mation.
Our full dual attention model Att-Pooling-CNN achieves an even more favorable F1-score of88%.Table 4 provides the experimental results for thetwo variants of the model given by Eqs.
(7) and (8)in Section 3.3.
Our main model outperforms theother variants on this dataset, although the variantsmay still prove useful when applied to other tasks.To better quantify the contribution of the differentcomponents of our model, we also conduct an ab-lation study evaluating several simplified models.The first simplification is to use our model withoutthe input attention mechanism but with the poolingattention layer.
The second removes both atten-tion mechanisms.
The third removes both formsof attention and additionally uses a regular objec-tive function based on the inner product s = r ?
wfor a sentence representation r and relation classembedding w. We observe that all three of ourcomponents lead to noticeable improvements overthese baselines.4.3 Detailed AnalysisPrimary Attention.
To inspect the inner work-ings of our model, we considered the primary at-tention matrices of our multi-level attention model1303Classifier F1Manually Engineered MethodsSVM (Rink and Harabagiu, 2010) 82.2Dependency MethodsRNN (Socher et al, 2012) 77.6MVRNN (Socher et al, 2012) 82.4FCM (Yu et al, 2014) 83.0Hybrid FCM (Yu et al, 2014) 83.4SDP-LSTM (Xu et al, 2015b) 83.7DRNNs (Xu et al, 2016) 85.8SPTree (Miwa and Bansal, 2016) 84.5End-To-End MethodsCNN+ Softmax (Zeng et al, 2014) 82.7CR-CNN (dos Santos et al, 2015) 84.1DepNN (Liu et al, 2015) 83.6depLCNN+NS (Xu et al, 2015a) 85.6STACK-FORWARD?
83.4VOTE-BIDIRECT?
84.1VOTE-BACKWARD?
84.1Our ArchitecturesAtt-Input-CNN 87.5Att-Pooling-CNN 88.0Table 3: Comparison with results published in theliterature, where ???
refers to models from Nguyenand Grishman (2015).for the following randomly selected sentence fromthe test set:The disgusting scene was retaliation againsther brother Philip who rents the [room]e1insidethis apartment [house]e2on Lombard street.Fig.
3 plots the word-level attention values forthe input attention layer to act as an example, us-ing the calculated attention values for every indi-vidual word in the sentence.
We find the word?inside?
was assigned the highest attention value,while words such as ?room?
and ?house?
alsoare deemed important.
This appears sensible inlight of the ground-truth labeling as a Component-Whole(e1,e2) relationship.
Additionally, we ob-serve that words such as ?this?, which are ratherirrelevant with respect to the target relationship,indeed have significantly lower attention scores.Most Significant Features for Relations.
Ta-ble 5 lists the top-ranked trigrams for each relationclass y in terms of their contribution to the scorefor determining the relation classification.
Recallthe definition of ??
(x, y) in Eq.
(1).
In the network,we trace back the trigram that contributed most toClassifier F1Att-Input-CNN (Main) 87.5Att-Input-CNN (Variant-1) 87.2Att-Input-CNN (Variant-2) 87.3Att-Pooling-CNN (regular) 88.0?
w/o input attention 86.6?
w/o any attention 86.1?
w/o any attention, w/o ?-objective 84.1Table 4: Comparison between the main model andvariants as well as simplified models.thedisgustingscene wasretaliationagainstherbrotherphilip who rents the roominside thisapartmenthouse onlombardstreet0.00.10.20.30.40.50.60.7Figure 3: Input Attention Visualization.
The valueof the y-coordinate is computed as 100 ?
(Ati?mini?
{1,...,n}Ati), where Atistands for the overallattention weight assigned to the word i.the correct classification in terms of ??
(Si, y) foreach sentence Si.
We then rank all such trigrams inthe sentences in the test set according to their totalcontribution and list the top-ranked trigrams.
?InTable 5, we see that these are indeed very informa-tive for deducing the relation.
For example, the toptrigram for Cause-Effect(e2,e1) is ?are caused by?,which strongly implies that the first entity is an ef-fect caused by the latter.
Similarly, the top trigramfor Entity-Origin(e1,e2) is ?from the e2?, whichsuggests that e2could be an original location, atwhich entity e1may have been located.Error Analysis.
Further, we examined some ofthe misclassifications produced by our model.
Thefollowing is a typical example of a wrongly classi-fied sentence:?For Entity-Destination(e2,e1), there was only one occur-rence in the test set.1304Relation (e1, e2) (e2, e1)e1caused a, caused a e2, e2caused by, e2from e1,Cause-Effect e1resulted in, the cause of, is caused by, are caused by,had caused the, poverty cause e2was caused by, been caused byComponent-Whole e1of the, of a e2, of the e2, with its e2, e1consists of,in the e2, part of the e1has a, e1comprises e2Content-Container in a e2, was hidden in, e1with e2, filled with e2,inside a e2, was contained in e1contained a, full of e2,Entity-Destination e1into the, e1into a, had thrown intowas put inside, in a e2Entity-Origin from this e2, is derived from, e1e2is, the e1e2,from the e2, away from the for e1e2, the source ofInstrument-Agency for the e2, is used by, by a e2, e1use e2, with a e2,with the e2, a e1e2by using e2Member-Collection of the e2, in the e2, a e1of, e1of various,a member of, from the e2e1of e2, the e1ofMessage-Topic on the e2, e1asserts the, the e1of, described in the,e1points out, e1is the the topic for, in the e2Product-Producer e1made by, made by e2, has constructed a, came up with,from the e2, by the e2has drawn up, e1who createdTable 5: Most representative trigrams for different relations.A [film]e1revolves around a [cadaver]e2whoseems to bring misfortune on those who comein contact with it.This sentence is wrongly classified as belongingto the ?Other?
category, while the ground-truth la-bel is Message-Topic(e1,e2).
The phrase ?revolvesaround?
does not appear in the training data, andmoreover is used metaphorically, rather than in itsoriginal sense of turning around, making it difficultfor the model to recognize the semantic connection.Another common issue stems from sentences ofthe form ?.
.
.
e1e2.
.
.
?, such as the following ones:The size of a [tree]e1[crown]e2is strongly .
.
.Organic [sesame]e1[oil]e2has an .
.
.Before heading down the [phone]e1[operator]e2career .
.
.These belong to three different relation classes,Component-Whole(e2,e1), Entity-Origin(e2,e1),and Instrument-Agency(e1,e2), respectively, whichare only implicit in the text, and the context is notparticularly helpful.
More informative word em-beddings could conceivably help in such cases.Convergence.
Finally, we examine the conver-gence behavior of our two main methods.
We plotthe performance of each iteration in the Att-Input-CNN and Att-Pooling-CNN models in Fig.
4.
Itcan be seen that Att-Input-CNN quite smoothlyconverges to its final F1 score, while for the Att-Pooling-CNN model, which includes an additionalattention layer, the joint effect of these two atten-tion layer induces stronger back-propagation ef-fects.
On the one hand, this leads to a seesawphenomenon in the result curve, but on the otherhand it enables us to obtain better-suited modelswith slightly higher F1 scores.0 5 10 15 20 25 300.40.450.50.550.60.650.70.750.80.850.9IterationF1 scoreAtt?Pooling?CNNAtt?Input?CNNFigure 4: Training Progress of Att-Input-CNN andAtt-Pooling-CNN across iterations.5 ConclusionWe have presented a CNN architecture with a novelobjective and a new form of attention mechanismthat is applied at two different levels.
Our resultsshow that this simple but effective model is able tooutperform previous work relying on substantiallyricher prior knowledge in the form of structuredmodels and NLP resources.
We expect this sortof architecture to be of interest also beyond thespecific task of relation classification, which weintend to explore in future work.1305AcknowledgmentsThe research at IIIS is supported by China 973Program Grants 2011CBA00300, 2011CBA00301,and NSFC Grants 61033001, 61361136003,61550110504.
Prof. Liu is supported by the China973 Program Grant 2014CB340501 and NSFCGrants 61572273 and 61532010.ReferencesNguyen Bach and Sameer Badaskar.
2007.A review of relation extraction.
Online athttp://www.cs.cmu.edu/%7Enbach/papers/A-survey-on-Relation-Extraction.pdf.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proceedings ofthe International Conference on Learning Represen-tations (ICLR).Elizabeth Boschee, Ralph Weischedel, and Alex Zama-nian.
2005.
Automatic information extraction.
InProceedings of the 2005 International Conferenceon Intelligence Analysis, McLean, VA, pages 2?4.Razvan C Bunescu and Raymond J Mooney.
2005.A shortest path dependency kernel for relation ex-traction.
In Proceedings of the Conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, pages 724?731.Association for Computational Linguistics.Yee Seng Chan and Dan Roth.
2010.
Exploiting back-ground knowledge for relation extraction.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, pages 152?160.
Associ-ation for Computational Linguistics.Jinxiu Chen, Donghong Ji, Chew Lim Tan, andZhengyu Niu.
2005.
Unsupervised feature selectionfor relation extraction.
In Proceedings of IJCNLP.Jiaqiang Chen, Niket Tandon, and Gerard de Melo.2015.
Neural word representations from large-scalecommonsense knowledge.
In Proceedings of WI2015.C?cero Nogueira dos Santos, Bing Xiang, and BowenZhou.
2015.
Classifying relations by ranking withconvolutional neural networks.
In Proceedings ofthe 53rd Annual Meeting of the Association forComputational Linguistics and the 7th InternationalJoint Conference on Natural Language Processing,volume 1, pages 626?634.Takaaki Hasegawa, Satoshi Sekine, and Ralph Grish-man.
2004.
Discovering relations among namedentities from large corpora.
In Proceedings of the42nd Annual Meeting of the Association for Compu-tational Linguistics, page 415.
Association for Com-putational Linguistics.Kazuma Hashimoto, Makoto Miwa, Yoshimasa Tsu-ruoka, and Takashi Chikayama.
2013.
Simple cus-tomization of recursive neural networks for seman-tic relation classification.
In Proceedings of the2013 Conference on Empirical Methods in Natu-ral Language Processing, pages 1372?1376, Seattle,Washington, USA, October.
Association for Compu-tational Linguistics.Iris Hendrickx, Su Nam Kim, Zornitsa Kozareva,Preslav Nakov, Diarmuid?O S?eaghdha, SebastianPad?o, Marco Pennacchiotti, Lorenza Romano, andStan Szpakowicz.
2010.
SemEval-2010 task 8:Multi-way classification of semantic relations be-tween pairs of nominals.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 33?38.
Association for Computational Lin-guistics.Nanda Kambhatla.
2004.
Combining lexical, syntactic,and semantic features with maximum entropy mod-els for extracting relations.
In Proceedings of the42nd Annual Meeting of the Association for Compu-tation Linguistics, page 22.
Association for Compu-tational Linguistics.Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou,and Houfeng Wang.
2015.
A dependency-basedneural network for relation classification.
In Pro-ceedings of the 53rd Annual Meeting of the Associ-ation for Computational Linguistics and the 7th In-ternational Joint Conference on Natural LanguageProcessing, pages 285?290.Fandong Meng, Zhengdong Lu, Mingxuan Wang,Hang Li, Wenbin Jiang, and Qun Liu.
2015.
Encod-ing source language with convolutional neural net-work for machine translation.
In Proceedings of the53rd Annual Meeting of the Association for Compu-tational Linguistics and the 7the International JointConference on Natural Language Processing, pages20?30.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013a.
Efficient estimation of word represen-tations in vector space.
In ICLR Workshop.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013b.
Linguistic regularities in continuous spaceword representations.
In HLT-NAACL, pages 746?751.Makoto Miwa and Mohit Bansal.
2016.
End-to-endrelation extraction using LSTMs on sequences andtree structures.
arXiv preprint arXiv:1601.00770.Raymond J Mooney and Razvan C Bunescu.
2005.Subsequence kernels for relation extraction.
In Ad-vances in Neural Information Processing Systems,pages 171?178.Thien Huu Nguyen and Ralph Grishman.
2014.
Em-ploying word representations and regularization fordomain adaptation of relation extraction.
In Pro-ceedings of the 52nd Annual Meeting of the Associ-ation for Computational Linguistics (Short Papers),pages 68?74.1306Thien Huu Nguyen and Ralph Grishman.
2015.Combining neural networks and log-linear mod-els to improve relation extraction.
arXiv preprintarXiv:1511.05926.Longhua Qian, Guodong Zhou, Fang Kong, QiaomingZhu, and Peide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction.
In Proceedings of the 22nd InternationalConference on Computational Linguistics, volume 1,pages 697?704.
Association for Computational Lin-guistics.Longhua Qian, Guodong Zhou, Fang Kong, andQiaoming Zhu.
2009.
Semi-supervised learningfor semantic relation classification using stratifiedsampling strategy.
In Proceedings of the 2009 Con-ference on Empirical Methods in Natural LanguageProcessing.Bryan Rink and Sanda Harabagiu.
2010.
Utd: Clas-sifying semantic relations by combining lexical andsemantic resources.
In Proceedings of the 5th Inter-national Workshop on Semantic Evaluation, pages256?259.
Association for Computational Linguis-tics.Alexander M Rush, Sumit Chopra, and Jason Weston.2015.
A neural attention model for abstractive sen-tence summarization.
In Proceedings of EMNLP2015.Cicero dos Santos, Ming Tan, Bing Xiang, and BowenZhou.
2016.
Attentive pooling networks.
arXivpreprint arXiv:1602.03609.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.Fabian M Suchanek, Georgiana Ifrim, and GerhardWeikum.
2006.
Combining linguistic and statis-tical analysis to extract relations from web docu-ments.
In Proceedings of the 12th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, pages 712?717.
ACM.Niket Tandon, Gerard de Melo, and Gerhard Weikum.2011.
Deriving a Web-scale common sense factdatabase.
In Proceedings of the Twenty-fifth AAAIConference on Artificial Intelligence (AAAI 2011),pages 152?157, Palo Alto, CA, USA.
AAAI Press.Jason Weston, Samy Bengio, and Nicolas Usunier.2011.
Wsabie: Scaling up to large vocabulary im-age annotation.
In Proceedings of IJCAI, volume 11,pages 2764?2770.Kun Xu, Yansong Feng, Songfang Huang, andDongyan Zhao.
2015a.
Semantic relation classifica-tion via convolutional neural networks with simplenegative sampling.
Proceedings of EMNLP 2015.Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng,and Zhi Jin.
2015b.
Classifying relations via longshort term memory networks along shortest depen-dency paths.
In Proceedings of Conference on Em-pirical Methods in Natural Language Processing (toappear).Yan Xu, Ran Jia, Lili Mou, Ge Li, Yunchuan Chen,Yangyang Lu, and Zhi Jin.
2016.
Improved re-lation classification by deep recurrent neural net-works with data augmentation.
arXiv preprintarXiv:1601.03651.Wenpeng Yin, Hinrich Sch?utze, Bing Xiang, andBowen Zhou.
2015.
ABCNN: attention-basedconvolutional neural network for modeling sentencepairs.
arXiv preprint arXiv:1512.05193.Mo Yu, Matthew Gormley, and Mark Dredze.
2014.Factor-based compositional embedding models.
InNIPS Workshop on Learning Semantics.Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou,Jun Zhao, et al 2014.
Relation classification viaconvolutional deep neural network.
In COLING,pages 2335?2344.Shu Zhang, Dequan Zheng, Xinchen Hu, and MingYang.
2015.
Bidirectional long short-term memorynetworks for relation classification.
In Proceedingsof the 29th Pacific Asia Conference on Language, In-formation and Computation pages, pages 73?78.Zhu Zhang.
2004.
Weakly-supervised relation classifi-cation for information extraction.
In In Proceedingsof the Thirteenth ACM International Conference onInformation and Knowledge Management.Guodong Zhou, Su Jian, Zhang Jie, and Zhang Min.2005.
Exploring various knowledge in relation ex-traction.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics,pages 427?434.
Association for Computational Lin-guistics.1307
