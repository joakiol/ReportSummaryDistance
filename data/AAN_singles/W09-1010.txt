Proceedings of the EACL 2009 Workshop on Computational Linguistic Aspects of Grammatical Inference, pages 66?74,Athens, Greece, 30 March 2009. c?2009 Association for Computational LinguisticsComparing learners for Boolean partitions:implications for morphological paradigms ?Katya PertsovaUniversity of North Carolina,Chapel Hillpertsova@email.unc.eduAbstractIn this paper, I show that a problem oflearning a morphological paradigm is sim-ilar to a problem of learning a partitionof the space of Boolean functions.
I de-scribe several learners that solve this prob-lem in different ways, and compare theirbasic properties.1 IntroductionLately, there has been a lot of work on acquir-ing paradigms as part of the word-segmentationproblem (Zeman, 2007; Goldsmith, 2001; Snoveret al, 2002).
However, the problem of learningthe distribution of affixes within paradigms as afunction of their semantic (or syntactic) features ismuch less explored to my knowledge.
This prob-lem can be described as follows: suppose that thesegmentation has already been established.
Canwe now predict what affixes should appear inwhat contexts, where by a ?context?
I mean some-thing quite general: some specification of seman-tic (and/or syntactic) features of the utterance.
Forexample, one might say that the nominal suffix -z in English (as in apple-z) occurs in contexts thatinvolve plural or possesive nouns whose stems endin a voiced segment.In this paper, I show that the problem of learn-ing the distribution of morphemes in contextsspecified over some finite number of featuresis roughly equivalent to the problem of learn-ing Boolean partitions of DNF formulas.
Giventhis insight, one can easily extend standard DNF-learners to morphological paradigm learners.
Ishow how this can be done on an example ofthe classical k-DNF learner (Valiant, 1984).
Thisinsight also allows us to bridge the paradigm-learning problem with other similar problems in?This paper ows a great deal to the input from Ed Stabler.As usual, all the errors and shortcomings are entirely mine.the domain of cognitive science for which DNF?shave been used, e.g., concept learning.
I also de-scribe two other learners proposed specifically forlearning morphological paradigms.
The first ofthese learners, proposed by me, was designed tocapture certain empirical facts about syncretismand free variation in typological data (Pertsova,2007).
The second learner, proposed by DavidAdger, was designed as a possible explanation ofanother empirical fact - uneven frequencies of freevariants in paradigms (Adger, 2006).In the last section, I compare the learners onsome simple examples and comment on their mer-its and the key differences among the algorithms.I also draw connections to other work, and discussdirections for further empirical tests of these pro-posals.2 The problemConsider a problem of learning the distributionof inflectional morphemes as a function of someset of features.
Using featural representations, wecan represent morpheme distributions in terms ofa formula.
The DNF formulas are commonly usedfor such algebraic representation.
For instance,given the nominal suffix -z mentioned in the in-troduction, we can assign to it the following rep-resentation: [(noun; +voiced]stem; +plural) ?
(noun; +voiced]stem; +possesive)].
Presum-ably, features like [plural] or [+voiced] or ]stem(end of the stem) are accessible to the learners?cognitive system, and can be exploited duringthe learning process for the purpose of ?ground-ing?
the distribution of morphemes.1 This wayof looking at things is similar to how some re-searchers conceive of concept-learning or word-1Assuming an a priori given universal feature set, theproblem of feature discovery is a subproblem of learningmorpheme distributions.
This is because learning what fea-ture condition the distribution is the same as learning whatfeatures (from the universal set) are relevant and should bepaid attention to.66learning (Siskind, 1996; Feldman, 2000; Nosofskyet al, 1994).However, one prominent distinction that setsinflectional morphemes apart from words is thatthey occur in paradigms, semantic spaces defin-ing a relatively small set of possible distinctions.In the absence of free variation, one can say thatthe affixes define a partition of this semantic spaceinto disjoint blocks, in which each block is asso-ciated with a unique form.
Consider for instancea present tense paradigm of the verb ?to be?
instandard English represented below as a partitionof the set of environments over the following fea-tures: class (with values masc, fem, both (masc& fem),inanim,), number (with values +sg and?sg), and person (with values 1st, 2nd, 3rd).2am 1st.
person; fem; +sg.1st.
person; masc; +sg.are 2nd.
person; fem; +sg.2nd.
person; masc; +sg.2nd.
person; fem; ?sg.2nd.
person; masc; ?sg.2nd.
person; both; ?sg.1st.
person; fem; ?sg.1st.
person; masc; ?sg.1st.
person; both; ?sg.3rd.
person; masc; ?sg3rd.
person; fem; ?sg3rd.
person; both; ?sg3rd.
person; inanim; ?sgis 3rd person; masc; +sg3rd person; fem; +sg3rd person; inanim; +sgEach block in the above partition can be rep-resented as a mapping between the phonologicalform of the morpheme (a morph) and a DNF for-mula.
A single morph will be typically mapped toa DNF containing a single conjunction of features(called a monomial).
When a morph is mappedto a disjunction of monomials (as the morph [-z]discussed above), we think of such a morph asa homonym (having more than one ?meaning?
).Thus, one way of defining the learning problem isin terms of learning a partition of a set of DNF?s.2These particular features and their values are chosen justfor illustration.
There might be a much better way to repre-sent the distinctions encoded by the pronouns.
Also noticethat the feature values are not fully independent: some com-binations are logically ruled out (e.g.
speakers and listenersare usually animate entities).Alternatively, we could say that the learner hasto learn a partition of Boolean functions associatedwith each morph (a Boolean function for a morphm maps the contexts in which m occurs to true,and all other contexts to false).However, when paradigms contain free varia-tion, the divisions created by the morphs no longerdefine a partition since a single context may be as-sociated with more than one morph.
(Free vari-ation is attested in world?s languages, althoughit is rather marginal (Kroch, 1994).)
In case aparadigm contains free variation, it is still possibleto represent it as a partition by doing the follow-ing:(1) Take a singleton partition of morph-meaning pairs (m, r) and merge any cellsthat have the same meaning r. Then mergethose blocks that are associated with thesame set of morphs.Below is an example of how we can use this trickto partition a paradigm with free-variation.
Thedata comes from the past tense forms of ?to be?
inBuckie English.was 1st.
person; fem; +sg.1st.
person; masc; +sg.3rd person; masc; +sg3rd person; fem; +sg3rd person; inanim; +sgwas/were 2nd.
person; fem; +sg.2nd.
person; masc; +sg.2nd.
person; fem; ?sg.2nd.
person; masc; ?sg.2nd.
person; both; ?sg.1st.
person; fem; ?sg.1st.
person; masc; ?sg.1st.
person; both; ?sg.were 3rd.
person; masc; ?sg3rd.
person; fem; ?sg3rd.
person; both; ?sg3rd.
person; inanim; ?sgIn general, then, the problem of learning thedistribution of morphs within a single inflectionalparadigm is equivalent to learning a Boolean par-tition.In what follows, I consider and compare severallearners for learning Boolean partitions.
Some ofthese learners are extensions of learners proposedin the literature for learning DNFs.
Other learners67were explicitly proposed for learning morphologi-cal paradigms.We should keep in mind that all these learnersare idealizations and are not realistic if only be-cause they are batch-learners.
However, becausethey are relatively simple to state and to under-stand, they allow a deeper understanding of whatproperties of the data drive generalization.2.1 Some definitionsAssume a finite set of morphs, ?, and a finite setof features F .
It would be convenient to think ofmorphs as chunks of phonological material cor-responding to the pronounced morphemes.3 Ev-ery feature f ?
F is associated with some setof values Vf that includes a value [?
], unspec-ified.
Let S be the space of all possible com-plete assignments over F (an assignment is a set{fi ?
Vf |?fi ?
F}).
We will call those assign-ments that do not include any unspecified featuresenvironments.
Let the set S?
?
S correspond tothe set of environments.It should be easy to see that the set S forms aBoolean lattice with the following relation amongthe assignments, ?R: for any two assignments a1and a2, a1 ?R a2 iff the value of every feature fiin a1 is identical to the value of fi in a2, unless fiis unspecified in a2.
The top element of the latticeis an assignment in which all features are unspec-ified, and the bottom is the contradiction.
Everyelement of the lattice is a monomial correspondingto the conjunction of the specified feature values.An example lattice for two binary features is givenin Figure 1.Figure 1: A lattice for 2 binary featuresA language L consists of pairs from ?
?
S?.That is, the learner is exposed to morphs in differ-ent environments.3However, we could also conceive of morphs as functionsspecifying what transformations apply to the stem withoutmuch change to the formalism.One way of stating the learning problem is tosay that the learner has to learn a grammar for thetarget language L (we would then have to spec-ify what this grammar should look like).
Anotherway is to say that the learner has to learn the lan-guage mapping itself.
We can do the latter by us-ing Boolean functions to represent the mapping ofeach morph to a set of environments.
Dependingon how we state the learning problem, we mightget different results.
For instance, it?s known thatsome subsets of DNF?s are not learnable, whilethe Boolean functions corresponding to them arelearnable (Valiant, 1984).
Since I will use Booleanfunctions for some of the learners below, I intro-duce the following notation.
Let B be the set ofBoolean functions mapping elements of S?
to trueor false.
For convenience, we say that bm corre-sponds to a Boolean function that maps a set of en-vironments to true when they are associated withm in L, and to false otherwise.3 Learning Algorithms3.1 Learner 1: an extension of the Valiantk-DNF learnerAn observation that a morphological paradigm canbe represented as a partition of environments inwhich each block corresponds to a mapping be-tween a morph and a DNF, allows us to easily con-vert standard DNF learning algorithms that relyon positive and negative examples into paradigm-learning algorithms that rely on positive examplesonly.
We can do that by iteratively applying anyDNF learning algorithm treating instances of in-put pairs like (m, e) as positive examples for mand as negative examples for all other morphs.Below, I show how this can be done by ex-tending a k-DNF4 learner of (Valiant, 1984) to aparadigm-learner.
To handle cases of free varia-tion we need to keep track of what morphs occurin exactly the same environments.
We can do thisby defining the partition ?
on the input followingthe recipe in (1) (substituting environments for thevariable r).The original learner learns from negative exam-ples alone.
It initializes the hypothesis to the dis-junction of all possible conjunctions of length atmost k, and subtracts from this hypothesis mono-mials that are consistent with the negative ex-amples.
We will do the same thing for each4k-DNF formula is a formula with at most k feature val-ues in each conjunct.68morph using positive examples only (as describedabove), and forgoing subtraction in a cases of free-variation.
The modified learner is given below.The following additional notation is used: Lex isthe lexicon or a hypothesis.
The formula D is adisjunction of all possible conjunctions of lengthat most k. We say that two assignments are con-sistentwith each other if they agree on all specifiedfeatures.
Following standard notation, we assumethat the learner is exposed to some text T that con-sists of an infinite sequence of (possibly) repeatingelements from L. tj is a finite subsequence of thefirst j elements from T .
L(tj) is the set of ele-ments in tj .Learner 1 (input: tj)1. set Lex := {?m,D?| ?
?m, e?
?L(tj)}2.
For each ?m, e?
?
L(tj), for eachm?
s.t.
??
block bl ?
?
of L(tj),?m, e?
?
bl and ?m?, e?
?
bl:replace ?m?, f?
in Lex by ?m?, f ?
?where f ?
is the result of removingevery monomial consistent with e.This learner initially assumes that every morphcan be used everywhere.
Then, when it hears onemorph in a given environment, it assumes that noother morph can be heard in exactly that environ-ment unless it already knows that this environmentpermits free variation (this is established in thepartition ?
).4 Learner 2:The next learner is an elaboration on the previouslearner.
It differs from it in only one respect: in-stead of initializing lexical representations of ev-ery morph to be a disjunction of all possible mono-mials of length at most k, we initialize it to be thedisjunction of all and only those monomials thatare consistent with some environment paired withthe morph in the language.
This learner is simi-lar to the DNF learners that do something on bothpositive and negative examples (see (Kushilevitzand Roth, 1996; Blum, 1992)).So, for every morph m used in the language, wedefine a disjunction of monomials Dm that can bederived as follows.
(i) Let Em be the enumerationof all environments in which m occurs in L (ii)let Mi correspond to a set of all subsets of featurevalues in ei, ei ?
E (iii) let Dm be?M , where aset s ?M iff s ?Mi, for some i.Learner 2 can now be stated as a learner thatis identical to Learner 1 except for the initial set-ting of Lex.
Now, Lex will be set to Lex :={?m,Dm?| ?
?m, e?
?
L(ti)}.Because this learner does not require enumer-ation of all possible monomials, but just thosethat are consistent with the positive data, it canhandle ?polynomially explainable?
subclass ofDNF?s (for more on this see (Kushilevitz and Roth,1996)).5 Learner 3: a learner biased towardsmonomial and elsewhere distributionsNext, I present a batch version of a learner I pro-posed based on certain typological observationsand linguists?
insights about blocking.
The typo-logical observations come from a sample of verbalagreement paradigms (Pertsova, 2007) and per-sonal pronoun paradigms (Cysouw, 2003) show-ing that majority of paradigms have either ?mono-mial?
or ?elsewhere?
distribution (defined below).Roughly speaking, a morph has a monomial dis-tribution if it can be described with a single mono-mial.
A morph has an elsewhere distribution ifthis distribution can be viewed as a complementof distributions of other monomial or elsewhere-morphs.
To define these terms more precisely Ineed to introduce some additional notation.
Let?ex be the intersection of all environments inwhich morph x occurs (i.e., these are the invariantfeatures of x).
This set corresponds to a least up-per bound of the environments associated with x inthe lattice ?S,?R?, call it lubx.
Then, let the min-imal monomial function for a morph x, denotedmmx, be a Boolean function that maps an envi-ronment to true if it is consistent with lubx andto false otherwise.
As usual, an extension of aBoolean function, ext(b) is the set of all assign-ments that b maps to true.
(2) Monomial distributionA morph x has a monomial distribution iffbx ?
mmx.The above definition states that a morph has amonomial distribution if its invariant features pickout just those environments that are associatedwith this morph in the language.
More concretely,if a monomial morph always co-occurs with thefeature +singular, it will appear in all singular en-69vironments in the language.
(3) Elsewhere distributionA morph x has an elsewhere distributioniff bx ?
mmx ?
(mmx1 ?mmx2 ?
.
.
.
?
(mmxn)) for all xi 6= x in ?.The definition above amounts to saying that amorph has an elsewhere distribution if the envi-ronments in which it occurs are in the extensionof its minimal monomial function minus the min-imal monomial functions of all other morphs.
Anexample of a lexical item with an elsewhere distri-bution is the present tense form are of the verb ?tobe?, shown below.Table 1: The present tense of ?to be?
in Englishsg.
pl1p.
am are2p.
are are3p.
is areElsewhere morphemes are often described inlinguistic accounts by appealing to the notion ofblocking.
For instance, the lexical representationof are is said to be unspecified for both personand number, and is said to be ?blocked?
by twoother forms: am and is.
My hypothesis is thatthe reason why such non-monotonic analyses ap-pear so natural to linguists is the same reason forwhy monomial and elsewhere distributions are ty-pologically common: namely, the learners (and,apparently, the analysts) are prone to generalizethe distribution of morphs to minimal monomi-als first, and later correct any overgeneralizationsthat might arise by using default reasoning, i.e.
bypositing exceptions that override the general rule.Of course, the above strategy alone is not sufficientto capture distributions that are neither monomial,nor elsewhere (I call such distributions ?overlap-ping?, cf.
the suffixes -en and -t in the Germanparadigm in Table 2), which might also explainwhy such paradigms are typologically rare.Table 2: Present tense of some regular verbs inGermansg.
pl1p.
-e -en2p.
-st -t3p.
-t -enThe original learner I proposed is an incre-mental learner that calculates grammars similarto those proposed by linguists, namely grammarsconsisting of a lexicon and a filtering ?blocking?component.
The version presented here is a sim-pler batch learner that learns a partition of Booleanfunctions instead.5 Nevertheless, the main proper-ties of the original learner are preserved: specifi-cally, a bias towards monomial and elsewhere dis-tributions.To determine what kind of distribution a morphhas, I define a relation C. A morph m stands in arelation C to another morph m?
if ?
?m, e?
?
L,such that lubm?
is consistent with e. In otherwords, mCm?
if m occurs in any environmentconsistent with the invariant features of m?.
LetC+ be a transitive closure of C.Learner 3 (input: tj)1.
Let S(tj) be the set of pairs in tj containingmonomial- or elsewhere-distribution morphs.That is, ?m, e?
?
S(tj) iff ??m?
such thatmC+m?
and m?C+m.2.
Let O(tj) = tj ?
S(tj) (the set of all otherpairs).3.
A pair ?m, e?
?
S is a least element of Siff ??
?m?, e??
?
(S ?
{?m, e?})
such thatm?C+m.4.
Given a hypothesis Lex, and for any expres-sion ?m, e?
?
Lex: let rem((m, e), Lex) =(m, (mmm ?
{b|?m?, b?
?
Lex}))61. set S := S(tj) and Lex := ?2.
While S 6= ?
: remove a least xfrom S and set Lex := Lex ?rem(x, Lex)3.
Set Lex := Lex ?O(tj).This learner initially assumes that the lexicon isempty.
Then it proceeds adding Boolean functionscorresponding to minimal monomials for morphsthat are in the set S(tj) (i.e., morphs that have ei-ther monomial or elsewhere distributions).
This5I thank Ed Stabler for relating this batch learner to me(p.c.
).6For any two Boolean functions b, b?
: b?b?
is the functionthat maps e to 1 iff e ?
ext(b) and e 6?
ext(b?).
Similarly,b + b?
is the function that maps e to 1 iff e ?
ext(b) ande ?
ext(b?
).70is done in a particular order, namely in the or-der in which the morphs can be said to blockeach other.
The remaining text is learned by rote-memorization.
Although this learner is more com-plex than the previous two learners, it generalizesfast when applied to paradigms with monomialand elsewhere distributions.5.1 Learner 4: a learner biased towardsshorter formulasNext, I discuss a learner for morphologicalparadigms, proposed by another linguist, DavidAdger.
Adger describes his learner informallyshowing how it would work on a few examples.Below, I formalize his proposal in terms of learn-ing Boolean partitions.
The general strategy ofthis learner is to consider simplest monomials first(those with the fewer number of specified features)and see how much data they can unambiguouslyand non-redundantly account for.
If a monomialis consistent with several morphs in the text - it isdiscarded unless the morphs in question are in freevariation.
This simple strategy is reiterated for thenext set of most simple monomials, etc.Learner 4 (input tj)1.
Let Mi be the set of all monomials over Fwith i specified features.2.
Let Bi be the set of Boolean functions fromenvironments to truth values correspondingto Mi in the following way: for each mono-mial mn ?
Mi the corresponding Booleanfunction b is such that b(e) = 1 if e is anenvironment consistent with mn; otherwiseb(e) = 0.3.
Uniqueness check:For a Boolean function b, morph m, and texttj let unique(b,m, tj) = 1 iff ext(bm) ?ext(b) and ??
?m?, e?
?
L(tj), s.t.
e ?ext(b) and e 6?
ext(bm).1. set Lex := ??
?
and i := 0;2. while Lex does not correspond toL(tj) AND i ?
|F | do:for each b ?
Bi, for each m, s.t.?
?m, e?
?
L(tj):?
if unique(b,m, tj) = 1 thenreplace ?m, f?
with ?m, f + b?in Lexi?
i + 1This learner considers all monomials in the or-der of their simplicity (determined by the num-ber of specified features), and if the monomial inquestion is consistent with environments associ-ated with a unique morph then these environmentsare added to the extension of the Boolean functionfor that morph.
As a result, this learner will con-verge faster on paradigms in which morphs can bedescribed with disjunctions of shorter monomialssince such monomials are considered first.6 Comparison6.1 Basic propertiesFirst, consider some of the basic properties of thelearners presented here.
For this purpose, we willassume that we can apply these learners in an iter-ative fashion to larger and larger batches of data.We say that a learner is consistent if and only if,given a text tj , it always converges on the gram-mar generating all the data seen in tj (Oshersonet al, 1986).
A learner is monotonic if and onlyif for every text t and every point j < k, the hy-pothesis the learner converges on at tj is a subsetof the hypothesis at tk (or for learners that learnby elimination: the hypothesis at tj is a supersetof the hypothesis at tk).
And, finally, a learner isgeneralizing if and only if for some tj it convergeson a hypothesis that makes a prediction beyond theelements of tj .The table below classifies the four learners ac-cording to the above properties.Learner consist.
monoton.
generalizingLearner 1 yes yes yesLearner 2 yes yes yesLearner 3 yes no yesLearner 4 yes yes yesAll learners considered here are generalizingand consistent, but they differ with respect tomonotonicity.
Learner 3 is non-monotonic whilethe remaining learners are monotonic.
Whilemonotonicity is a nice computational property,some aspects of human language acquisition aresuggestive of a non-monotonic learning strategy,e.g.
the presence of overgeneralization errors andtheir subsequent corrections by children(Marcus etal., 1992).
Thus, the fact that Learner 3 is non-monotonic might speak in its favor.716.2 IllustrationTo demonstrate how the learners work, considerthis simple example.
Suppose we are learning thefollowing distribution of morphs A and B over 2binary features.
(4) Example 1+f1 ?f1+f2 A B?f2 B BSuppose further that the text t3 is:A +f1;+f2B ?f1;+f2B +f1;?f2Learner 1 generalizes right away by assumingthat every morph can appear in every environmentwhich leads to massive overgeneralizations.
Theseovergeneralizations are eventually eliminated asmore data is discovered.
For instance, after pro-cessing the first pair in the text above, the learner?learns?
that B does not occur in any environ-ment consistent with (+f1;+f2) since it has justseen A in that environment.
After processing t3,Learner 1 has the following hypothesis:A (+f1;+f2) ?
(?f1;?f2)B (?f1) ?
(?f2)That is, after seeing t3, Learner 2 correctly pre-dicts the distribution of morphs in environmentsthat it has seen, but it still predicts that both Aand B should occur in the not-yet-observed en-vironment, (?f1;?f2).
This learner can some-times converge before seeing all data-points, es-pecially if the input includes a lot of free varia-tion.
If fact, if in the above example A and B werein free variation in all environments, Learner 1would have converged right away on its initial set-ting of the lexicon.
However, in paradigms with nofree variation convergence is typically slow sincethe learner follows a very conservative strategy oflearning by elimination.Unlike Learner 1, Learner 2 will converge afterseeing t3.
This is because this learner?s initial hy-pothesis is more restricted.
Namely, the initial hy-pothesis for A includes disjunction of only thosemonomials that are consistent with (+f1;+f2).Hence, A is never overgeneralized to (?f1;?f2).Like Learner 1, Learner 2 also learns by elimina-tion, however, on top of that it also restricts its ini-tial hypothesis which leads to faster convergence.Let?s now consider the behavior of learner 3 onexample 1.
Recall that this learner first computesminimal monomials of all morphs, and checksin they have monomial or elsewhere distributions(this is done via the relation C+).
In this case, Ahas a monomial distribution, and B has an else-where distribution.
Therefore, the learner firstcomputes the Boolean function forAwhose exten-sion is simply (+f1;+f2); and then the Booleanfunction for B, whose extension includes environ-ments consistent with (*;*) minus those consistentwith (+f1;+f2), which yields the following hy-pothesis:ext(bA) [+f1;+f2]ext(bB) [?f1;+f2][+f1;?f2][?f1;?f2]That is, Learner 3 generalizes and converges onthe right language after seeing text t3.Learner 4 also converges at this point.
Thislearner first considers how much data can be un-ambiguously accounted for with the most minimalmonomial (*;*).
Since both A and B occur in en-vironments consistent with this monomial, noth-ing is added to the lexicon.
On the next round,it considers all monomials with one specified fea-ture.
2 such monomials, (?f1) and (?f2), areconsistent only with B, and so we predict B to ap-pear in the not-yet-seen environment (?f1;?f2).Thus, the hypothesis that Learner 4 arrives at is thesame as the hypothesis Learners 3 arrives at afterseeing t3.6.3 DifferencesWhile the last three learners perform similarly onthe simple example above, there are significantdifferences between them.
These differences be-come apparent when we consider larger paradigmswith homonymy and free variation.First, let?s look at an example that involves amore elaborate homonymy than example 1.
Con-sider, for instance, the following text.
(5) Example 2A [+f1;+f2;+f3]A [+f1;?f2;?f3]A [+f1;+f2;?f3]A [?f1;+f2;+f3]B [?f1;?f2;?f3]72Given this text, all three learners will differ intheir predictions with respect to the environ-ment (?f1;+f2;?f3).
Learner 2 will pre-dict both A and B to occur in this environmentsince not enough monomials will be removedfrom representations of A or B to rule out ei-ther morph from occurring in (?f1;+f2;?f3).Learner 3 will predict A to appear in all envi-ronments that haven?t been seen yet, including(?f1;+f2;?f3).
This is because in the cur-rent text the minimal monomial for A is (?
; ?
; ?
)and A has an elsewhere distribution.
On theother hand, Learner 4 predicts B to occur in(?f1;+f2;?f3).
This is because the exten-sion of the Boolean function for B includesany environments consistent with (?f1;?f3) or(?f1;?f2) since these are the simplest monomi-als that uniquely pick out B.Thus, the three learners follow very differentgeneralization routes.
Overall, Learner 2 is morecautious and slower to generalize.
It predicts freevariation in all environments for which not enoughdata has been seen to converge on a single morph.Learner 3 is unique in preferring monomial andelsewhere distributions.
For instance, in the aboveexample it treats A as a ?default?
morph.
Learner4 is unique in its preference for morphs describ-able with disjunction of simpler monomials.
Be-cause of this preference, it will sometimes gener-alize even after seeing just one instance of a morph(since several simple monomials can be consistentwith this instance alone).One way to test what the human learners doin a situation like the one above is to use artifi-cial grammar learning experiments.
Such experi-ments have been used for learning individual con-cepts over features like shape, color, texture, etc.Some work on concept learning suggests that it issubjectively easier to learn concepts describablewith shorter formulas (Feldman, 2000; Feldman,2004).
Other recent work challenges this idea (La-fond et al, 2007), showing that people don?t al-ways converge on the most minimal representa-tion, but instead go for the more simple and gen-eral representation and learn exceptions to it (thisapproach is more in line with Learner 3).Some initial results from my pilot experimentson learning partitions of concept spaces (using ab-stract shapes, rather than language stimuli) alsosuggest that people find paradigms with else-where distributions easier to learn than the oneswith overlapping distributions (like the Germanparadigms in 2).
However, I also found a bias to-ward paradigms with the fewer number of relevantfeatures.
This bias is consistent with Learner 4since this learner tries to assume the smallest num-ber of relevant features possible.
Thus, both learn-ers have their merits.Another area in which the considered learn-ers make somewhat different predictions has todo with free variation.
While I can?t discussthis at length due to space constraints, let mecomment that any batch learner can easily de-tect free-variation before generalizing, which isexactly what most of the above learners do (ex-cept Learner 3, but it can also be changed to dothe same thing).
However, since free variationis rather marginal in morphological paradigms,it is possible that it would be rather problem-atic.
In fact, free variation is more problematic ifwe switch from the batch learners to incrementallearners.7 Directions for further researchThere are of course many other learners one couldconsider for learning paradigms, including ap-proaches quite different in spirit from the onesconsidered here.
In particular, some recently pop-ular approaches conceive of learning as matchingprobabilities of the observed data (e.g., Bayesianlearning).
Comparing such approaches with thealgorithmic ones is difficult since the criteria forsuccess are defined so differently, but it wouldstill be interesting to see whether the kinds ofprior assumptions needed for a Bayesian modelto match human performance would have some-thing in common with properties that the learn-ers considered here relied on.
These propertiesinclude the disjoint nature of paradigm cells, theprevalence of monomial and elsewhere morphs,and the economy considerations.
Other empiricalwork that might help to differentiate Boolean par-tition learners (besides typological and experimen-tal work already mentioned) includes finding rele-vant language acquisition data, and examining (ormodeling) language change (assuming that learn-ing biases influence language change).ReferencesDavid Adger.
2006.
Combinatorial variation.
Journalof Linguistics, 42:503?530.73Avrim Blum.
1992.
Learning Boolean functions in aninfinite attribute space.
Machine Learning, 9:373?386.Michael Cysouw.
2003.
The Paradigmatic Structureof Person Marking.
Oxford University Press, NY.Jacob Feldman.
2000.
Minimization of complexity inhuman concept learning.
Nature, 407:630?633.Jacob Feldman.
2004.
How surprising is a simple pat-tern?
Quantifying ?Eureka!?.
Cognition, 93:199?224.John Goldsmith.
2001.
Unsupervised learning of amorphology of a natural language.
ComputationalLinguistics, 27:153?198.Anthony Kroch.
1994.
Morphosyntactic variation.
InKatharine Beals et al, editor, Papers from the 30thregional meeting of the Chicago Linguistics Soci-ety: Parasession on variation and linguistic theory.Chicago Linguistics Society, Chicago.Eyal Kushilevitz and Dan Roth.
1996.
On learning vi-sual concepts and DNF formulae.
Machine Learn-ing, 24:65?85.Daniel Lafond, Yves Lacouture, and Guy Mineau.2007.
Complexity minimization in rule-based cat-egory learning: revising the catalog of boolean con-cepts and evidence for non-minimal rules.
Journalof Mathematical Psychology, 51:57?74.Gary Marcus, Steven Pinker, Michael Ullman,Michelle Hollander, T. John Rosen, and Fei Xu.1992.
Overregularization in language acquisition.Monographs of the Society for Research in ChildDevelopment, 57(4).
Includes commentary byHarold Clahsen.Robert M. Nosofsky, Thomas J. Palmeri, and S.C.McKinley.
1994.
Rule-plus-exception modelof classification learning.
Psychological Review,101:53?79.Daniel Osherson, Scott Weinstein, and Michael Stob.1986.
Systems that Learn.
MIT Press, Cambridge,Massachusetts.Katya Pertsova.
2007.
Learning Form-Meaning Map-pings in the Presence of Homonymy.
Ph.D. thesis,University of California, Los Angeles.Jeffrey Mark Siskind.
1996.
A computational studyof cross-situational techniques for learning word-to-meaning mappings.
Cognition, 61(1-2):1?38, Oct-Nov.Matthew G. Snover, Gaja E. Jarosz, and Michael R.Brent.
2002.
Unsupervised learning of morphologyusing a novel directed search algorithm: taking thefirst step.
In Proceedings of the ACL-02 workshopon Morphological and phonological learning, pages11?20, Morristown, NJ, USA.
Association for Com-putational Linguistics.Leslie G. Valiant.
1984.
A theory of the learnable.CACM, 17(11):1134?1142.Daniel Zeman.
2007.
Unsupervised acquiring of mor-phological paradigms from tokenized text.
In Work-ing Notes for the Cross Language Evaluation Forum,Budapest.
Madarsko.
Workshop.74
