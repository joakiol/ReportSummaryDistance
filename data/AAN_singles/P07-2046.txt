Proceedings of the ACL 2007 Demo and Poster Sessions, pages 181?184,Prague, June 2007. c?2007 Association for Computational LinguisticsBoosting Statistical Machine Translation by Lemmatization and LinearInterpolationRuiqiang Zhang1,2 and Eiichiro Sumita1,21National Institute of Information and Communications Technology2ATR Spoken Language Communication Research Laboratories2-2-2 Hikaridai, Seiika-cho, Soraku-gun, Kyoto, 619-0288, Japan{ruiqiang.zhang,eiichiro.sumita}@atr.jpAbstractData sparseness is one of the factors that de-grade statistical machine translation (SMT).Existing work has shown that using morpho-syntactic information is an effective solu-tion to data sparseness.
However, fewer ef-forts have been made for Chinese-to-EnglishSMT with using English morpho-syntacticanalysis.
We found that while English isa language with less inflection, using En-glish lemmas in training can significantlyimprove the quality of word alignment thatleads to yield better translation performance.We carried out comprehensive experimentson multiple training data of varied sizes toprove this.
We also proposed a new effec-tive linear interpolation method to integratemultiple homologous features of translationmodels.1 IntroductionRaw parallel data need to be preprocessed in themodern phrase-based SMT before they are alignedby alignment algorithms, one of which is the well-known tool, GIZA++ (Och and Ney, 2003), fortraining IBM models (1-4).
Morphological analy-sis (MA) is used in data preprocessing, by which thesurface words of the raw data are converted into anew format.
This new format can be lemmas, stems,parts-of-speech and morphemes or mixes of these.One benefit of using MA is to ease data sparsenessthat can reduce the translation quality significantly,especially for tasks with small amounts of trainingdata.Some published work has shown that apply-ing morphological analysis improved the quality ofSMT (Lee, 2004; Goldwater and McClosky, 2005).We found that all this earlier work involved exper-iments conducted on translations from highly in-flected languages, such as Czech, Arabic, and Span-ish, to English.
These researchers also provided de-tailed descriptions of the effects of foreign languagemorpho-syntactic analysis but presented no specificresults to show the effect of English morphologi-cal analysis.
To the best of our knowledge, therehave been no papers related to English morpholog-ical analysis for Chinese-to-English (CE) transla-tions even though the CE translation has been themain track for many evaluation campaigns includ-ing NIST MT, IWSLT and TC-STAR, where onlysimple tokenization or lower-case capitalization hasbeen applied to English preprocessing.
One possi-ble reason why English morphological analysis hasbeen neglected may be that English is less inflectedto the extent that MA may not be effective.
How-ever, we found this assumption should not be taken-for-granted.We studied what effect English lemmatization hadon CE translation.
Lemmatization is shallow mor-phological analysis, which uses a lexical entry to re-place inflected words.
For example, the three words,doing, did and done, are replaced by one word, do.They are all mapped to the same Chinese transla-tions.
As a result, it eases the problem with sparsedata, and retains word meanings unchanged.
It isnot impossible to improve word alignment by usingEnglish lemmatization.We determined what effect lemmatization had inexperiments using data from the BTEC (Paul, 2006)CSTAR track.
We collected a relatively large cor-pus of more than 678,000 sentences.
We conductedcomprehensive evaluations and used multiple trans-181lation metrics to evaluate the results.
We found thatour approach of using lemmatization improved boththe word alignment and the quality of SMT witha small amounts of training data, and, while muchwork indicates that MA is useless in training largeamounts of data (Lee, 2004), our intensive exper-iments proved that the chance to get a better MTquality using lemmatization is higher than that with-out it for large amounts of training data.On the basis of successful use of lemmatizationtranslation, we propose a new linear interpolationmethod by which we integrate the homologous fea-tures of translation models of the lemmatization andnon-lemmatization system.
We found the integratedmodel improved all the components?
performance inthe translation.2 Moses training for system withlemmatization and withoutWe used Moses to carry out the expriments.
Mosesis the state of the art decoder for SMT.
It is an ex-tension of Pharaoh (Koehn et al, 2003), and sup-ports factor training and decoding.
Our idea canbe easily implemented by Moses.
We feed MosesEnglish words with two factors: surface word andlemma.
The only difference in training with lemma-tization from that without is the alignment factor.The former uses Chinese surface words and Englishlemmas as the alignment factor, but the latter usesChinese surface words and English surface words.Therefore, the lemmatized English is only used inword alignment.
All the other options of Moses aresame for both the lemmatization translation and non-lemmatization translation.We use the tool created by (Minnen et al, 2001) tocomplete the morphological analysis of English.
Wehad to make an English part-of-speech (POS) tag-ger that is compatible with the CLAWS-5 tagset touse this tool.
We use our in-house tagset and En-glish tagged corpus to train a statistical POS taggerby using the maximum entropy principle.
Our tagsetcontains over 200 POS tags, most of which are con-sistent to the CLAWS-5.
The tagger achieved 93.7%accuracy for our test set.We use the default features defined by Pharaohin the phrase-based log-linear models i.e., a targetlanguage model, five translation models, and onedistance-based distortion model.
The weighting pa-rameters of these features were optimized in termsof BLEU by the approach of minimum error ratetraining (Och, 2003).The data for training and test are from theIWSLT06 CSTAR track that uses the Basic TravelExpression Corpus (BTEC).
The BTEC corpus arerelatively larger corpus for travel domain.
We use678,748 Chinese/English parallel sentences as thetraining data in the experiments.
The number ofwords are about 3.9M and 4.4M for Chinese and En-glish respectively.
The number of unique words forEnglish is 28,709 before lemmatization and 24,635after lemmatization.
A 15%-20% reduction in vo-cabulary is obtained by the lemmatization.
The testdata are the one used in IWSLT06 evaluation.
Itcontains 500 Chinese sentences.
The test data ofIWSLT05 are the development data for tuning theweighting parameters.
Multiple references are usedfor computing the automatic metrics.3 Experiments3.1 Regular testThe purpose of the regular tests is to find what ef-fect lemmatization has as the amount of trainingdata increases.
We used the data from the IWSLT06CSTAR track.
We started with 50,000 (50 K) ofdata, and gradually added more training data froma 678 K corpus to this.
We applied the methodsin Section 2 to train the non-lemmatized translationand lemmatized translation systems.
The results arelisted in Table 1.
We use the alignment error rate(AER) to measure the alignment performance, andthe two popular automatic metric, BLEU1 and ME-TEOR2 to evaluate the translations.
To measure theword alignment, we manually aligned 100 parallelsentences from the BTEC as the reference file.
Weuse the ?sure?
links and the ?possible?
links to de-note the alignments.
As shown in Table 1, we foundour approach improved word alignment uniformlyfrom small amounts to large amounts of trainingdata.
The maximal AER reduction is up to 27.4%for the 600K.
However, we found some mixed trans-lation results in terms of BLEU.
The lemmatized1http://domino.watson.ibm.com/library/CyberDig.nsf (key-word=RC22176)2http://www.cs.cmu.edu/?alavie/METEOR182Table 1: Translation results as increasing amount of trainingdata in IWSLT06 CSTAR trackSystem AER BLEU METEOR50K nonlem 0.217 0.158 0.427lemma 0.199 0.167 0.431100K nonlem 0.178 0.182 0.457lemma 0.177 0.188 0.463300K nonlem 0.150 0.223 0.501lemma 0.132 0.217 0.505400K nonlem 0.136 0.231 0.509lemma 0.102 0.224 0.507500K nonlem 0.119 0.235 0.519lemma 0.104 0.241 0.522600K nonlem 0.095 0.238 0.535lemma 0.069 0.248 0.536Table 2: Statistical significance test in terms of BLEU:sys1=non-lemma, sys2=lemmaData size Diff(sys1-sys2)50K -0.092 [-0.0176,-0.0012]100K -0.006 [-0.0155,0.0039]300K 0.0057 [-0.0046,0.0161]400K 0.0074 [-0.0023,0.0174]500K -0.0054 [-0.0139,0.0035]600K -0.0103 [-0.0201,-0.0006]translations did not outperform the non-lemmatizedones uniformly.
They did for small amounts of data,i.e., 50 K and 100 K, and for large amounts, 500 Kand 600 K. However, they failed for 300 K and 400K.The translations were under the statistical signif-icance test by using the bootStrap scripts3.
The re-sults giving the medians and confidence intervals areshown in Table 2, where the numbers indicate themedian, the lower and higher boundary at 95% con-fidence interval.
we found the lemma systems wereconfidently better than the nonlem systems for the50K and 600K, but didn?t for other data sizes.This experiments proved that our proposed ap-proach improved the qualities of word alignmentsthat lead to the translation improvement for the 50K,100K, 500K and 600K.
In particular, our resultsrevealed large amounts of data of 500 K and 6003http://projectile.is.cs.cmu.edu/research/public/tools/bootStrap/tutorial.htmTable 3: Competitive scores (BLEU) for non-lemmatization andlemmatization using randomly extracted corporaSystem 100K 300K 400K 600K totallemma 10/11 5.5/11 6.5/11 5/7 27/40nonlem 1/11 5.5/11 4.5/11 2/7 13/40K was improved by the lemmatization while it hasbeen found impossible in most published results.However, data of 300 K and 400 K worsen trans-lations achieved by the lemmatization4.
In what fol-lows, we discuss a method of random sampling ofcreating multiple corpora of varied sizes to see ro-bustness of our approach and re-investigate the re-sults of the 300K and 400K.3.2 Random sampling testIn this section, we use a method of random extrac-tion to generate new multiple training data for eachcorpus of one definite size.
The new data are ex-tracted from the whole corpus of 678 K randomly.We generate ten new corpora for 100 K, 300 K,and 400 K data and six new corpora for the 678 Kdata.
Thus, we create eleven and seven corpora ofvaried sizes if the corpora in the last experimentsare counted.
We use the same method as in Sec-tion 2 for each generated corpus to construct sys-tems to compare non-lemmatization and lemmati-zation.
The systems are evaluated again using thesame test data.
The results are listed in Table 3and Figure 1.
Table 3 shows the ?scoreboard?
ofnon-lemmatized and lemmatized results in terms ofBLEU.
If its score for the lemma system is higherthan that for the nonlem system, the former earnsone point; if equal, each earns 0.5; otherwise, thenonlem earns one point.
As we can see from the ta-ble, the results for the lemma system are better thanthose for the nonlem system for the 100K in 10 ofthe total 11 corpora.
Of the total 40 random corpora,the lemma systems outperform the nonlem systemsin 27 times.By analyzing the results from Tables 1 and 3, wecan arrive at some conclusions.
The lemma systemsoutperform the nonlem for training corpora less than4while the results was not confident by statistical signifi-cance test, the medians of 300K and 400K were lowered bythe lemmatization1830.160.25 NL-600KL-600KNL-400KL-400KNL-300KL-300KNL-100KL-100K11109876543210.1690.1780.1870.1960.2050.2140.2230.2320.241BLEUNumber of randomly extracted corporaFigure 1: Bleu scores for randomly extracted corpora100 K. The BLEU score favors the lemma systemoverwhelmingly for this size.
When the amount oftraining data is increased up to 600 K, the lemmastill beat the nonlem system in most tests while thenumber of success by the nonlem system increases.This random test, as a complement to the last ex-periment, reveals that the lemma either performs thesame or better than the nonlem system for trainingdata of any size.
Therefore, the lemma system isslightly better than the nonlem in general.Figure 1 illustrates the BLEU scores for the?lemma(L)?
and ?nonlem(NL)?
systems for ran-domly extracted corpora.
A higher number of pointsis obtained by the lemma system than the nonlem foreach corpus.4 Effect of linear interpolation of featuresWe generated translation models for lemmatizationtranslation and non-lemmatization translation.
Wefound some features of the translation models couldbe added linearly.
For example, phrase translationmodel p(e| f ) can be calculated as,p(e| f ) = ?1 pl(e| f ) + ?2 pnl(e| f )where pl(e| f ) and pnl(e| f ) is the phrase translationmodels corresponding to the lemmatization systemand non-lemma system.
?1 + ?2 = 1.
?s can beobtained by maximizing likelihood or BLEU scoresof a development data.
But we used the same val-ues for all the ?.
p(e| f ) is the phrase translationmodel after linear interpolation.
Besides the phrasetranslation model, we used this approach to integrateTable 4: Effect of linear interpolationlemma nonlemma interpolationopen track 0.1938 0.1993 0.2054the three other features: phrase inverse probability,lexical probability, and lexical inverse probability.We tested this integration using the open track ofIWSLT 2006, a small task track.
The BLEU scoresare shown in Table 4.
An improvement over both ofthe systems were observed.5 ConclusionsWe proposed a new approach of using lemmatiza-tion and linear interpolation of homologous featuresin SMT.
The principal idea is to use lemmatized En-glish for the word alignment.
Our approach wasproved effective for the BTEC Chinese to Englishtranslation.
It is significant in particular that wehave target language, English, as the lemmatized ob-ject because it is less usual in SMT.
Nevertheless,we found our approach significantly improved wordalignment and qualities of translations.ReferencesSharon Goldwater and David McClosky.
2005.
Im-proving statistical MT through morphological analy-sis.
In Proceedings of HLT/EMNLP, pages 676?683,Vancouver, British Columbia, Canada, October.Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In HLT-NAACL2003: Main Proceedings, pages 127?133.Young-Suk Lee.
2004.
Morphological analysis for statis-tical machine translation.
In HLT-NAACL 2004: ShortPapers, pages 57?60, Boston, Massachusetts, USA.Guido Minnen, John Carroll, and Darren Pearce.
2001.Applied morphological processing of english.
NaturalLanguage Engineering, 7(3):207?223.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In ACL 2003, pages160?167.Michael Paul.
2006.
Overview of the IWSLT 2006 Eval-uation Campaign.
In Proc.
of the IWSLT, pages 1?15,Kyoto, Japan.184
