Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358?366,Beijing, August 2010A Large Scale Ranker-Based Systemfor Search Query Spelling CorrectionJianfeng GaoMicrosoft Research, Redmondjfgao@microsoft.comXiaolong LiMicrosoft Corporationxiaolong.li@microsoft.comDaniel MicolMicrosoft Corporationdanielmi@microsoft.comChris QuirkMicrosoft Research, Redmondchrisq@microsoft.comXu SunUniversity of Tokyoxusun@mist.i.u-tokyo.ac.jpAbstractThis paper makes three significant extensions to anoisy channel speller designed for standard writ-ten text to target the challenging domain of searchqueries.
First, the noisy channel model is sub-sumed by a more general ranker, which allows avariety of features to be easily incorporated.
Se-cond, a distributed infrastructure is proposed fortraining and applying Web scale n-gram languagemodels.
Third, a new phrase-based error model ispresented.
This model places a probability distri-bution over transformations between multi-wordphrases, and is estimated using large amounts ofquery-correction pairs derived from search logs.Experiments show that each of these extensionsleads to significant improvements over the state-of-the-art baseline methods.1 IntroductionSearch queries present a particular challenge fortraditional spelling correction methods.
Newsearch queries emerge constantly.
As a result,many queries contain valid search terms, such asproper nouns and names, which are not well es-tablished in the language.
Therefore, recent re-search has focused on the use of Web corporaand search logs, rather than human-compiled lex-icons, to infer knowledge about spellings andword usages in search queries (e.g., Whitelaw etal., 2009; Cucerzan and Brill, 2004).The spelling correction problem is typicallyformulated under the framework of the noisychannel model.
Given an input query, we want to find the best spelling correc-tion           among all candidates:(1)Applying Bayes' Rule, we have(2)where the error model        models the trans-formation probability from C to Q, and the lan-guage model (LM)      models the likelihoodthat C is a correctly spelled query.This paper extends a noisy channel speller de-signed for regular text to search queries in threeways: using a ranker (Section 3), using Web scaleLMs (Section 4), and using phrase-based errormodels (Section 5).First of all, we propose a ranker-based spellerthat covers the noisy channel model as a specialcase.
Given an input query, the system first gen-erates a short list of candidate corrections usingthe noisy channel model.
Then a feature vector iscomputed for each query and candidate correc-tion pair.
Finally, a ranker maps the feature vec-tor to a real-valued score, indicating the likeli-hood that this candidate is a desirable correction.We will demonstrate that ranking provides a flex-ible modeling framework for incorporating awide variety of features that would be difficult tomodel under the noisy channel framework.Second, we explore the use of Web scale LMsfor query spelling correction.
While traditionalLM research focuses on how to make the model?smarter?
via how to better estimate the probabil-ity of unseen words (Chen and Goodman, 1999);and how to model the grammatical structure oflanguage (e.g., Charniak, 2001), recent studiesshow that significant improvements can beachieved using ?stupid?
n-gram models trainedon very large corpora (e.g., Brants et al, 2007).We adopt the latter strategy in this study.
We pre-sent a distributed infrastructure to efficiently trainand apply Web scale LMs.
In addition, we ob-serve that search queries are composed in a lan-guage style different from that of regular text.
Wethus train multiple LMs using different texts as-sociated with Web corpora and search queries.Third, we propose a phrase-based error modelthat captures the probability of transforming one358multi-term phrase into another multi-term phrase.Compared to traditional error models that accountfor transformation probabilities between singlecharacters or substrings (e.g., Kernighan et al,1990; Brill and Moore, 2000), the phrase-basederror model is more effective in that it capturesinter-term dependencies crucial for correctingreal-word errors, prevalent in search queries.
Wealso present a novel method of extracting largeamounts of query-correction pairs from searchlogs.
These pairs, implicitly judged by millions ofusers, are used for training the error models.Experiments show that each of the extensionsleads to significant improvements over its base-line methods that were state-of-the-art until thiswork, and that the combined method yields a sys-tem which outperforms the noisy channel spellerby a large margin: a 6.3% increase in accuracy ona human-labeled query set.2 Related WorkPrior research on spelling correction for regulartext can be grouped into two categories: correct-ing non-word errors and real-word errors.
Theformer focuses on the development of error mod-els based on different edit distance functions (e.g.,Kucich, 1992; Kernighan et al, 1990; Brill andMoore, 2000; Toutanova and Moore, 2002).
Brilland Moore?s substring-based error model, con-sidered to be state-of-the-art among these models,acts as the baseline against which we compareour models.
On the other hand, real-word spellingcorrection tries to detect incorrect usages of avalid word based on its context, such as "peace"and "piece" in the context "a _ of cake".
N-gramLMs and na?ve Bayes classifiers are commonlyused models (e.g., Golding and Roth, 1996;Mangu and Brill, 1997; Church et al, 2007).While almost all of the spellers mentionedabove are based on a pre-defined dictionary (ei-ther a lexicon against which the edit distance iscomputed, or a set of real-word confusion pairs),recent research on query spelling correction fo-cuses on exploiting noisy Web corpora and querylogs to infer knowledge about spellings and wordusag in queries (Cucerzan and Brill 2004; Ahmadand Kondrak, 2005; Li et al, 2006; Whitelaw etal., 2009).
Like those spellers designed for regu-lar text, most of these query spelling systems arealso based on the noisy channel framework.3 A Ranker-Based SpellerThe noisy channel model of Equation (2) doesnot have the flexibility to incorporate a wide va-riety of features useful for spelling correction,e.g., whether a candidate appears as a Wikipediadocument title.
We thus generalize the speller toa ranker-based system.
Let f be a feature vectorof a query and candidate correction pair (Q, C).The ranker maps f to a real value y that indicateshow likely C is a desired correction.
For example,a linear ranker maps f to y with a weight vector wsuch as      , where w is optimized for accu-racy on human-labeled       pairs.
Since thelogarithms of the LM and error model probabili-ties can be included as features, the ranker coversthe noisy channel model as a special case.For efficiency, our speller operates in two dis-tinct stages: candidate generation and re-ranking.In candidate generation, an input query is firsttokenized into a sequence of terms.
For each termq, we consult a lexicon to identify a list ofspelling suggestions c whose edit distance from qis lower than some threshold.
Our lexicon con-tains around 430,000 high frequency query uni-gram and bigrams collected from 1 year of querylogs.
These suggestions are stored in a lattice.We then use a decoder to identify the 20-bestcandidates from the lattice according to Equation(2), where the LM is a backoff bigram modeltrained on 1 year of query logs, and the errormodel is approximated by weighted edit distance:(3)The decoder uses a standard two-pass algorithm.The first pass uses the Viterbi algorithm to findthe best C according to the model of Equations(2) and (3).
The second pass uses the A-star al-gorithm to find the 20-best corrections, using theViterbi scores computed at each state in the firstpass as heuristics.The core component in the second stage is aranker, which re-ranks the 20-best candidate cor-rections using a set of features extracted from.
If the top C after re-ranking is differentfrom Q, C is proposed as the correction.
We use96 features in this study.
In addition to the twofeatures derived from the noisy channel model,the rest of the features can be grouped into thefollowing 5 categories.1.
Surface-form similarity features, whichcheck whether C and Q differ in certain patterns,359e.g., whether C is transformed from Q by addingan apostrophe, or by adding a stop word at thebeginning or end of Q.2.
Phonetic-form similarity features, whichcheck whether the edit distance between the met-aphones (Philips, 1990) of a query term and itscorrection candidate is below some thresholds.3.
Entity features, which check whether theoriginal query is likely to be a proper noun basedon an in-house named entity recognizer.4.
Dictionary features, which check whethera query term or a candidate correction are in oneor more human-compiled dictionaries, such as theextracted Wiki, MSDN, and ODP dictionaries.5.
Frequency features, which check whetherthe frequency of a query term or a candidate cor-rection is above certain thresholds in differentdatasets, such as query logs and Web documents.4 Web Scale Language ModelsAn n-gram LM assigns a probability to a wordstringaccording to?
(  |)?
(  |)(4)where the approximation is based on a Markovassumption that each word depends only upon theimmediately preceding n-1 words.
In a speller,the log of n-gram LM probabilities of an originalquery and its candidate corrections are used asfeatures in the ranker.While recent research reports the benefits oflarge LMs trained on Web corpora on a variety ofapplications (e.g.
Zhang et al, 2006; Brants et al,2007), it is also clear that search queries are com-posed in a language style different from that ofthe body or title of a Web document.
Thus, in thisstudy we developed a set of large LMs from dif-ferent text streams of Web documents and querylogs.
Below, we first describe the n-gram LMcollection used in this study, and then present adistributed n-gram LM platform based on whichthese LMs are built and served for the speller.4.1 Web Scale Language ModelsTable 1 summarizes the data sets and Web scalen-gram LMs used in this study.
The collection isbuilt from high quality English Web documentscontaining trillions of tokens, served by a popularcommercial search engine.
The collection con-sists of several data sets built from different Websources, including the different text fields fromthe Web documents (i.e., body, title, and anchortexts) and search query logs.
The raw texts ex-tracted from these different sources were pre-processed in the following manner: texts are to-kenized based on white-space and upper case let-ters are converted to lower case.
Numbers areretained, and no stemming/inflection is per-formed.
The n-gram LMs are word-based backoffmodels, where the n-gram probabilities are esti-mated using Maximum Likelihood Estimationwith smoothing.
Specifically, for a trigram mod-el, the smoothed probability is computed as(5){(             )where      is the count of the n-gram in the train-ing corpus and   is a normalization factor.is a discount function for smoothing.
We usemodified absolute discounting (Gao et al, 2001),whose parameters can be efficiently estimatedand performance converges to that of more elabo-rate state-of-the-art techniques like Kneser-Neysmoothing in large data (Nguyen et al 2007).4.2 Distributed N-gram LM PlatformThe platform is developed on a distributed com-puting system designed for storing and analyzingmassive data sets, running on large clusters con-sisting of hundreds of commodity servers con-nected via high-bandwidth network.We use the SCOPE (Structured ComputationsOptimized for Parallel Execution) programmingmodel (Chaiken et al, 2008) to train the Webscale n-gram LMs shown in Table 1.
The SCOPEscripting language resembles SQL which manyprogrammers are familiar with.
It also supportsDataset Body Anchor Title QueryTotal tokens 1.3T 11.0B 257.2B 28.1BUnigrams 1.2B 60.3M 150M 251.5MBigrams 11.7B 464.1M 1.1B 1.3BTrigrams 60.0B 1.4B 3.1B 3.1B4-grams 148.5B 2.3B 5.1B 4.6BSize on disk# 12.8TB 183GB 395GB 393GB# N-gram entries as well as other model parameters arestored.Table 1: Statistics of the Web n-gram LMs collection (countcutoff = 0 for all models).
These models will be accessible atMicrosoft (2010).360C# expressions so that users can easily plug-incustomized C# classes.
SCOPE supports writinga program using a series of simple data transfor-mations so that users can simply write a script toprocess data in a serial manner without wonder-ing how to achieve parallelism while the SCOPEcompiler and optimizer are responsible for trans-lating the script into an efficient, parallel execu-tion plan.
We illustrate the usage of SCOPE forbuilding LMs using the following example ofcounting 5-grams from the body text of EnglishWeb pages.
The flowchart is shown in Figure 1.The program is written in SCOPE as a step-by- step of computation, where a command takesthe output of the previous command as its input.ParsedDoc=SELECT docId, TokenizedDocFROM @?/shares/?/EN_Body.txt?USING DefaultTextExtractor;NGram=PROCESS ParsedDocPRODUCE NGram, NGcountUSING NGramCountProcessor(-streamTokenizedDoc -order 5 ?bufferSize20000000);NGramCount=REDUCE NGramON NGramPRODUCE NGram, NGcountUSING NGramCountReducer;OUTPUT TO @?Body-5-gram-count.txt?
;The first SCOPE command is a SELECTstatement that extracts parsed Wed body text.
Thesecond command uses a build-in Processor(NGramCountProcessor) to map the parsed doc-uments into separate n-grams together with theircounts.
It generates a local hash at each node(i.e., a core in a multi-core server) to store the (n-gram, count) pairs.
The third command (RE-DUCE) aggregates counts from different nodesaccording to the key (n-gram string).
The finalcommand (OUTPUT) writes out the resulting to adata file.The smoothing method can be implementedsimilarly by the customized smoothing Proces-sor/Reducer.
They can be imported from the ex-isting C# codes (e.g., developed for building LMsin a single machine) with minor changes.It is straightforward to apply the built LMs forthe ranker in the speller.
The n-gram platformprovides a DLL for n-gram batch lookup.
In theserver, an n-gram LM is stored in the form ofmultiple lists of key-value pairs, where the key isthe hash of an n-gram string and the value is ei-ther the n-gram probability or backoff parameter.5 Phrase-Based Error ModelsThe goal of an error model is to transform a cor-rectly spelled query C into a misspelled query Q.Rather than replacing single words in isolation,the phrase-based error model replaces sequencesof words with sequences of words, thus incorpo-rating contextual information.
The training pro-cedure closely follows Sun et al (2010).
For in-stance, we might learn that ?theme part?
can bereplaced by ?theme park?
with relatively highprobability, even though ?part?
is not a mis-spelled word.
We use this generative story: firstthe correctly spelled query C is broken into Knon-empty word sequences c1, ?, ck, then each isreplaced with a new non-empty word sequenceq1, ?, qk, finally these phrases are permuted andconcatenated to form the misspelled Q.
Here, cand q denote consecutive sequences of words.To formalize this generative process, let S de-note the segmentation of C into K phrases c1?cK,and let T denote the K replacement phrasesq1?qK ?
we refer to these (ci, qi) pairs as bi-phrases.
Finally, let M denote a permutation of Kelements representing the final reordering step.Figure 2 demonstrates the generative procedure.Next let us place a probability distributionover rewrite pairs.
Let B(C, Q) denote the set of S,T, M triples that transform C into Q.
Assuming auniform probability over segmentations, thephrase-based probability can be defined as:RecursiveReducerNode 1 Node 2 Node N?...
?...OutputWeb PagesParsingCountingLocalHashTokenizeWeb PagesParsingCountingLocalHashTokenizeWeb PagesParsingCountingLocalHashTokenizeFigure 1.
Distributed 5-gram counting.C: ?disney theme park?
correct queryS: [?disney?, ?theme park?]
segmentationT: [?disnee?, ?theme part?]
translationM: (1 ?
2, 2?
1) permutationQ: ?theme part disnee?
misspelled queryFigure 2: Example demonstrating the generative procedurebehind the phrase-based error model.361?
(6)As is common practice in SMT, we use the max-imum approximation to the sum:(7)5.1 Forced AlignmentsAlthough we have defined a generative model fortransforming queries, our goal is not to proposenew queries, but rather to provide scores overexisting Q and C pairs that will act as features forthe ranker.
Furthermore, the word-level align-ments between Q and C can most often be identi-fied with little ambiguity.
Thus we restrict ourattention to those phrase transformations con-sistent with a good word-level alignment.Let J be the length of Q, L be the length of C,and A = a1?aJ  be a hidden variable representingthe word alignment between them.
Each ai takeson a value ranging from 1 to L indicating its cor-responding word position in C, or 0 if the ithword in Q is unaligned.
The cost of assigning kto ai is equal to the Levenshtein edit distance(Levenshtein, 1966) between the ith word in Qand the kth word in C, and the cost of assigning 0to ai is equal to the length of the ith word in Q.The least cost alignment A* between Q and C iscomputed efficiently using the A-star algorithm.When scoring a given candidate pair, we fur-ther restrict our attention to those S, T, M triplesthat are consistent with the word alignment,which we denote as B(C, Q, A*).
Here, consisten-cy requires that if two words are aligned in A*,then they must appear in the same bi-phrase (ci,qi).
Once the word alignment is fixed, the finalpermutation is uniquely determined, so we cansafely discard that factor.
Thus we have:(8)For the sole remaining factor P(T|C, S), wemake the assumption that a segmented query T =q1?
qK is generated from left to right by trans-forming each phrase c1?cK independently:?, (9)where          is a phrase transformation prob-ability, the estimation of which will be describedin Section 5.2.To find the maximum probability assignmentefficiently, we use a dynamic programming ap-proach, similar to the monotone decoding algo-rithm described in Och (2002).5.2 Training the Error ModelGiven a set of (Q, C) pairs as training data, wefollow a method commonly used in SMT (Ochand Ney, 2004) to extract bi- phrases and esti-mate their replacement probabilities.
A detaileddescription is discussed in Sun et al (2010).We now describe how (Q, C) pairs are gener-ated automatically from massive query reformu-lation sessions of a commercial Web browser.A query reformulation session contains a listof URLs that record user behaviors that relate tothe query reformulation functions, provided by aWeb search engine.
For example, most commer-cial search engines offer the "did you mean"function, suggesting a possible alternate interpre-tation or spelling of a user-issued query.
Figure 3shows a sample of the query reformulation ses-sions that record the "did you mean" sessionsfrom three of the most popular search engines.These sessions encode the same user behavior: Auser first queries for "harrypotter sheme part",Google:http://www.google.com/search?hl=en&source=hp&q=harrypotter+sheme+part&aq=f&oq=&aqi=http://www.google.com/search?hl=en&ei=rnNAS8-oKsWe_AaB2eHlCA&sa=X&oi=spell&resnum=0&ct=result&cd=1&ved=0CA4QBSgA&q=harry+potter+theme+park&spell=1Yahoo:http://search.yahoo.com/search;_ylt=A0geu6ywckBL_XIBSDtXNyoA?p=harrypotter+sheme+part&fr2=sb-top&fr=yfp-t-701&sao=1http://search.yahoo.com/search?ei=UTF-8&fr=yfp-t-701&p=harry+potter+theme+park&SpellState=n-2672070758_q-tsI55N6srhZa.qORA0MuawAAAA%40%40&fr2=sp-topBing:http://www.bing.com/search?q=harrypotter+sheme+part&form=QBRE&qs=nhttp://www.bing.com/search?q=harry+potter+theme+park&FORM=SSREFigure 3.
A sample of query reformulation sessions from 3popular search engines.
These sessions show that a user firstissues the query "harrypotter sheme part", and then clicks onthe resulting spell suggestion "harry potter theme park".362and then clicks on the resulting spelling sugges-tion "harry potter theme park".
We can "reverse-engineer" the parameters from the URLs of thesesessions, and deduce how each search engine en-codes both a query and the fact that a user arrivedat a URL by clicking on the spelling suggestionof the query ?
an strong indication that thespelling suggestion is desired.
In this study, from1 year of sessions, we extracted ~120 millionpairs.
We found the data set very clean becausethese spelling corrections are actually clicked,and thus judged implicitly, by many users.In addition to the "did you mean" functionali-ty, recently some search engines have introducedtwo new spelling suggestion functions.
One is the"auto-correction" function, where the search en-gine is confident enough to automatically applythe spelling correction to the query and execute itto produce search results.
The other is the "splitpane" result page, where one half portion of thesearch results are produced using the originalquery, while the other half, usually visually sepa-rate portion of results, are produced using theauto-corrected query.In neither of these functions does the user everreceive an opportunity to approve or disapproveof the correction.
Since our extraction approachfocuses on user-approved spelling suggestions,we ignore the query reformulation sessions re-cording either of the two functions.
Although bydoing so we could miss some basic, obviousspelling corrections, our experiments show thatthe negative impact on error model training isnegligible.
One possible reason is that our base-line system, which does not use any error modellearned from the session data, is already able tocorrect these basic, obvious spelling mistakes.Thus, including these data for training is unlikelyto bring any further improvement.We found that the error models trained usingthe data directly extracted from the query refor-mulation sessions suffer from the problem of un-derestimating the self-transformation probabilityof a query P(Q2=Q1|Q1), because we only includ-ed in the training data the pairs where the query isdifferent from the correction.
To deal with thisproblem, we augmented the training data by in-cluding correctly spelled queries, i.e., the pairs(Q1, Q2) where Q1 = Q2.
First, we extracted a setof queries from the sessions where no spell sug-gestion is presented or clicked on.
Second, weremoved from the set those queries that were rec-ognized as being auto-corrected by a search en-gine.
We do so by running a sanity check of thequeries against our baseline noisy channelspeller, which will be described in Section 6.
Ifthe system consider a query misspelled, we as-sumed it an obvious misspelling, and removed it.The remaining queries were assumed to be cor-rectly spelled and were added to the training data.6 ExperimentsWe perform the evaluation using a manually an-notated data set containing 24,172 queries sam-pled from one year?s query logs from a commer-cial search engine.
The spelling of each query ismanually corrected by four independent annota-tors.
The average length of queries in the datasets is 2.7 words.
We divided the data set intonon-overlapped training and test data sets.
Thetraining data contain 8,515       pairs, amongwhich 1,743 queries are misspelled (i.e.
).The test data contain 15,657       pairs, amongwhich 2,960 queries are misspelled.The speller systems we developed in thisstudy are evaluated using the following metrics.?
Accuracy: The number of correct outputsgenerated by the system divided by the totalnumber of queries in the test set.?
Precision: The number of correct spellingcorrections for misspelled queries generatedby the system divided by the total number ofcorrections generated by the system.?
Recall: The number of correct spelling cor-rections for misspelled queries generated bythe system divided by the total number ofmisspelled queries in the test set.We also perform a significance test, a t-testwith a significance level of 0.05.In our experiments, all the speller systems areranker-based.
Unless otherwise stated, the rankeris a two-layer neural net with 5 hidden nodes.The free parameters of the neural net are trainedto optimize accuracy on the training data usingthe back propagation algorithm (Burges et al,2005) .6.1 System ResultsTable 1 summarizes the main results of differentspelling systems.
Row 1 is the baseline spellerwhere the noisy channel model of Equations (2)363and (3) is used.
The error model is based on theweighted edit distance function and the LM is abackoff bigram model trained on 1 year of querylogs, with count cutoff 30.
Row 2 is the spellerusing a linear ranker to incorporate all rankingfeatures described in Section 3.
The weights ofthe linear ranker are optimized using the Aver-aged Perceptron algorithm (Freund and Schapire,1999).
Row 3 is the speller where a nonlinearranker (i.e., 2-layer neural net) is trained atop thefeatures.
Rows 4, 5 and 6 are systems that incor-porate the additional features derived from thephrase-based error model (PBEM) described inSection 5 and the four Web scale LMs (WLMs)listed in Table 1.The results show that (1) the ranker is a veryflexible modeling framework where a variety offine-grained features can be easily incorporated,and a ranker-based speller outperforms signifi-cantly (p < 0.01) the traditional system based onthe noisy channel model (Row 2 vs. Row 1); (2)the speller accuracy can be further improved byusing more sophisticated rankers and learningalgorithms (Row 3 vs. Row 2); (3) both WLMsand PBEM bring significant improvements(Rows 4 and 5 vs. Row 3); and (4) interestingly,the gains from WLMs and PBEM are additiveand the combined leads to a significantly betterspeller (Row 6 vs.
Rows 4 and 5) than that ofusing either of them individually.In what follows, we investigate in detail howthe WLMs and PBEM trained on massive Webcontent and search logs improve the accuracy ofthe speller system.
We will compare our modelswith the state-of-the-art models proposed previ-ously.
From now on, the system listed in Row 3of Table 1 will be used as baseline.6.2 Language ModelsThe quality of n-gram LMs depends on the orderof the model, the size of the training data, andhow well the training data match the test data.Figure 4 illustrates the perplexity results of thefour LMs trained on different data sources testedon a random sample of 733,147 queries.
The re-sults show that (1) higher order LMs producelower perplexities, especially when moving be-yond unigram models; (2) as expected, the queryLMs are most predictive for the test queries,though they are from independent query logsnapshots; (3) although the body LMs are trainedon much larger amounts of data than the title andanchor LMs, the former lead to much higher per-plexity values, indicating that both title and an-chor texts are quantitatively much more similar toqueries than body texts.Table 2 summarizes the spelling results usingdifferent LMs.
For comparison, we also built a 4-gram LM using the Google 1T web 5-gram cor-pus (Brants and Franz, 2006).
This model is re-ferred to as the G1T model, and is trained usingthe ?stupid backoff?
smoothing method (Brants etal., 2007).
Due to the high count cutoff appliedby the Google corpus (i.e., n-grams must appearat least 40 times to be included in the corpus), wefound the G1T model results to a higher OOVrate (i.e., 6.5%) on our test data than that of the 4Web scale LMs (i.e., less than 1%).The results in Table 2 are more or less con-sistent with the perplexity results: the query LMis the best performer; there is no significant dif-ference among the body, title and anchor LMsthough the body LM is trained on a much largeramount of data; and all the 4 Web scale LMs out-perform the G1T model substantially due to thesignificantly lower OOV rates.6.3 Error ModelsThis section compares the phrase-based errormodel (PBEM) described in Section 5, with oneof the state-of-the-art error models, proposed byBrill and Moore (2000), henceforth referred to as# System Accuracy Precision Recall1 Noisy channel 85.3 72.1 35.92 Linear ranker 88.0 74.0 42.83 Nonlinear ranker 89.0 74.1 49.64 3 + PBEM 90.7 78.7 58.25 3 + WLMs 90.4 75.1 58.76 3 + PBEM + WLMs  91.6 79.1 63.9Table 1.
Summary of spelling correction results.Figure 4.
Perplexity results on test queries, using n-gram LMs with different orders, derived from differ-ent data sources.364the B&M model.
B&M is a substring error mod-el.
It estimates        as?
(10)where R is a partitioning of correction term c intoadjacent substrings, and T is a partitioning ofquery term q, such that |T|=|R|.
The partitions arethus in one-to-one alignment.
To train the B&Mmodel, we extracted 1 billion term-correctionpairs       from the set of 120 million query-correction pairs      , derived from the searchlogs as described in Section 5.2.Table 3 summarizes the comparison results.Rows 1 and 2 are our ranker-based baseline sys-tems with and without the error model (EM) fea-ture.
The error model is based on weighted editdistance of Eq.
(3), where the weights are learnedon some manually annotated word-correctionpairs (which is not used in this study).
Rows 3and 4 are the B&M models using different maxi-mum substring lengths, specified by L. L=1 re-duces B&M to the weighted edit distance modelin Row 2.
Rows 5 and 6 are PBEMs with differ-ent maximum phrase lengths.
L=1 reduces PBEMto a word-based error model.
The results showthe benefits of capturing context information inerror models.
In particular, the significant im-provements resulting from PBEM demonstratethat the dependencies between words are farmore effective than that between characters(within a word) for spelling correction.
This islargely due to the fact that there are many real-word spelling errors in search queries.
We alsonotice that PBEM is a more powerful model  than# # of word pairs Accuracy Precision Recall1 Baseline w/o EM 88.55 71.95 46.972 1M 89.15 73.71 50.743 10M 89.22 74.11 50.924 100M 89.20 73.60 51.065 1B 89.21 73.72 50.99Table 4.
The performance of B&M error model (L=3) as afunction of the size of training data (# of word pairs).# # of (Q, C) pairs Accuracy Precision Recall1 Baseline w/o EM 88.55 71.95 46.972 5M 89.59 77.01 52.343 15M 90.23 77.87 56.674 45M 90.45 78.56 57.025 120M 90.70 78.49 58.12Table 5.
The performance of PBEM (L=3) as a function ofthe size of training data (# of (Q, C) pairs).B&M in that it can benefit more from increasing-ly larger training data.
As shown in Tables 4 and5, whilst the performance of B&M saturatesquickly with the increase of training data, the per-formance of PBEM does not appear to havepeaked ?
further improvements are likely given alarger data set.7 Conclusions and Future WorkThis paper explores the use of massive Web cor-pora and search logs for improving a ranker-based search query speller.
We show significantimprovements over a noisy channel speller usingfine-grained features, Web scale LMs, and aphrase-based error model that captures intern-word dependencies.
There are several techniqueswe are exploring to make further improvements.First, since a query speller is developed for im-proving the Web search results, it is natural to usefeatures from search results in ranking, as studiedin Chen et al (2007).
The challenge is efficiency.Second, in addition to query reformulation ses-sions, we are exploring other search logs fromwhich we might extract more       pairs for er-ror model training.
One promising data source isclickthrough data (e.g., Agichtein et al 2006;Gao et al, 2009).
For instance, we might try tolearn a transformation from the title or anchortext of a document to the query that led to a clickon that document.
Finally, the phrase-based errormodel is inspired by phrase-based SMT systems.We are introducing more SMT techniques suchas alignment and translation rule exaction.
In abroad sense, spelling correction can be viewed asa monolingual MT problem where we translatebad English queries into good ones.# System Accuracy Precision Recall1 Baseline 89.0 74.1 49.62 1+ query 4-gram 90.1 75.6 56.33 1 + body 4-gram 89.9 75.7 54.44 1 + title 4-gram 89.8 75.4 54.75 1 + anchor 4-gram 89.9 75.1 55.66 1 + G1T 4-gram 89.4 75.1 51.5Table 2.
Spelling correction results using different LMstrained on different data sources.# System Accuracy Precision Recall1 Baseline w/o EM 88.6 72.0 47.02 Baseline 89.0 74.1 49.63 1 + B&M, L=1 89.0 73.3 50.14 1 + B&M, L=3 89.2 73.7 51.05 1 + PBEM, L=1 90.1 76.7 55.66 1 + PBEM, L=3 90.7 78.5 58.1Table 3.
Spelling correction results using different errormodels.365AcknowledgmentsThe authors would like to thank Andreas Bode,Mei Li, Chenyu Yan and Kuansan Wang for thevery helpful discussions and collaboration.
Thework was done when Xu Sun was visiting Mi-crosoft Research Redmond.ReferencesAgichtein, E., Brill, E. and Dumais, S. 2006.
Improv-ing web search ranking by incorporating user be-havior information.
In SIGIR, pp.
19-26.Ahmad, F., and Kondrak, G. 2005.
Learning a spellingerror model from search query logs.
In HLT-EMNLP, pp.
955-962.Brants, T., and Franz, A.
2006.
Web 1T 5-gram corpusversion 1.1.
Technical report, Google Research.Brants, T., Popat, A. C., Xu, P., Och, F. J., and Dean, J.2007.
Large language models in machine translation.In EMNLP-CoNLL, pp.
858 - 867.Brill, E., and Moore, R. C. 2000.
An improved errormodel for noisy channel spelling correction.
In ACL,pp.
286-293.Burges, C., Shaked, T., Renshaw, E., Lazier, A.,Deeds, M., Hamilton, and Hullender, G. 2005.Learning to rank using gradient descent.
In ICML,pp.
89-96.Chaiken, R., Jenkins, B., Larson, P., Ramsey, B.,Shakib, D., Weaver, S., and Zhou, J.
2008.
SCOPE:easy and efficient parallel processing f massive datasets.
In Proceedings of the VLDB Endowment, pp.1265-1276.Charniak, E. 2001.
Immediate-head parsing for lan-guage models.
In ACL/EACL, pp.
124-131.Chen, S. F., and Goodman, J.
1999.
An empiricalstudy of smoothing techniques for language model-ing.
Computer Speech and Language, 13(10):359-394.Chen, Q., Li, M., and Zhou, M. 2007.
Improving que-ry spelling correction using web search results.
InEMNLP-CoNLL, pp.
181-189.Church, K., Hard, T., and Gao, J.
2007.
Compressingtrigram language models with Golomb coding.
InEMNLP-CoNLL, pp.
199-207.Cucerzan, S., and Brill, E. 2004.
Spelling correction asan iterative process that exploits the collectiveknowledge of web users.
In EMNLP, pp.
293-300.Freund, Y. and Schapire, R. E. 1999.
Large marginclassification using the perceptron algorithm.
InMachine Learning, 37(3): 277-296.Gao, J., Goodman, J., and Miao, J.
2001.
The use ofclustering techniques for language modeling -application to Asian languages.
Computational Lin-guistics and Chinese Language Processing,6(1):27?60, 2001.Gao, J., Yuan, W., Li, X., Deng, K., and Nie, J-Y.2009.
Smoothing clickthrough data for web searchranking.
In SIGIR, pp.
355-362.Golding, A. R., and Roth, D. 1996.
Applying winnowto context-sensitive spelling correction.
In ICML, pp.182-190.Joachims, T. 2002.
Optimizing search engines usingclickthrough data.
In SIGKDD, pp.
133-142.Kernighan, M. D., Church, K. W., and Gale, W. A.1990.
A spelling correction program based on anoisy channel model.
In COLING, pp.
205-210.Koehn, P., Och, F., and Marcu, D. 2003.
Statisticalphrase-based translation.
In HLT/NAACL, pp.
127-133.Kucich, K. 1992.
Techniques for automaticallycorrecting words in text.
ACM Computing Surveys,24(4):377-439.Levenshtein, V. I.
1966.
Binary codes capable of cor-recting deletions, insertions and reversals.
SovietPhysics Doklady, 10(8):707-710.Li, M., Zhu, M., Zhang, Y., and Zhou, M. 2006.
Ex-ploring distributional similarity based models forquery spelling correction.
In ACL, pp.
1025-1032.Mangu, L., and Brill, E. 1997.
Automatic rule acquisi-tion for spelling correction.
In ICML, pp.
187-194.Microsoft Microsoft web n-gram services.
2010.http://research.microsoft.com/web-ngramNguyen, P., Gao, J., and Mahajan, M. 2007.
MSRLM:a scalable language modeling toolkit.
Technical re-port TR-2007-144, Microsoft Research.Och, F. 2002.
Statistical machine translation: fromsingle-word models to alignment templates.
PhDthesis, RWTH Aachen.Och, F., and Ney, H. 2004.
The alignment templateapproach to statistical machine translation.Computational Linguistics, 30(4): 417-449.Philips, L. 1990.
Hanging on the metaphone.
Comput-er Language Magazine, 7(12):38-44.Sun, X., Gao, J., Micol, D., and Quirk, C. 2010.Learning phrase-based spelling error models fromclickthrough data.
In ACL.Toutanova, K., and Moore, R. 2002.
Pronunciationmodeling for improved spelling correction.
In ACL,pp.
144-151.Whitelaw, C., Hutchinson, B., Chung, G. Y., and Ellis,G.
2009.
Using the web for language independentspellchecking and autocorrection.
In EMNLP, pp.890-899.Zhang, Y., Hildebrand, Al.
S., and Vogel, S. 2006.Distributed language modeling for n-best list re-ranking.
In EMNLP, pp.
216-233.366
