Functional Unification Grammar RevisitedKathleen R. McKeown and Cecile L. ParisDepartment ofComputer Science450 Computer ScienceColumbia UniversityNew York, N.Y. 10027MCKEOWN@CS.COLUMBIA.EDUCECIL~@CS.COLUMBIA.EDUAbstractIn this paper, we show that one benefit of FUG, theability to state global conslralnts on choice separately fromsyntactic rules, is difficult in generation systems based onaugmented context free grammars (e.g., Def'mite ClauseCn'anmm~).
They require that such constraints be expressedlocally as part of syntactic rules and therefore, duplicated inthe grammar.
Finally, we discuss a reimplementation oflUg  that achieves the similar levels of efficiency asRubinoff's adaptation of MUMBLE,  a detcrministclanguage generator.1 I n t roduct ionInefficiency of functional unification grammar (FUG,\[5\]) has prompted some effort to show that the same benefitsoffered by FUG can be achieved in other formalisms moreefficiently \[3; 14; 15; 16\].
In this paper, we show that onebenefit of FUG, the ability to conciselyl state globalconstraints on choice in generation, is difficult in otherformalhms in which we have written generation systems.
Inparticular, we show that a global constraint can be statedseparately from syntactic rules in FUG, while in generationsystems based on augmented context free ~g~nunars (e.g.,Definite Clause Cn'amma~ (DCG, \[13\])) such consWaintsmust be expressed locally as part of syntactic rules andthe~=for?, duplicated in the grammar.
Finally, we discuss areimplementation f lUG in TAILOR \[11; 12\] that achievesthe si.m/l~r leveLs of efficiency as Rubinoff's adaptation\[16\] of MUMBLE \[7\], a deterministc language generator.1.1 Statement of  ConstraintsLanguage generation can be viewed primarily as aproblem of choice, requiring decisions about which syntacticstructures best express intent.
As a result, much research inlanguage generanon has focused on identi~ing conswaintson choice, and it is important to be able to represent theseconstraints clearly and efficiently.
In this paper, we comparethe representation of constraints in FUG with theirrepn:sentation i a DCG generation system \[3\].
We areinterested in representing functional constraints on syntacticsWacture where syntax does not fully restrict expression; thatis, conswaints other than those coming from syntax.
Welook at the representation of two specific constraints onsyntactic choice: focus of attention on the choice of sentencevoice and focus of attention on the choice of simple versuscomplex sentences.We claim that, in a lUG, these constraints can bestated separately from rules dictating syntactic structure, thusleading to simplicity of the granunar since the constraintsonly need to be stated once.
This is possible in FUG becauseof unification and the ability to build constituent structure inthe grammar.
In contrast, in a DCG, constraints must bestated as part of the individual grammar rules, resulting induplication of a constraint for each syntactic rule to which itapplies.1.2 Passive/Active ConstraintFocus of attention can determine whether the passiveor active voice should be used in a sentence \[8\].
Theconstraint dictates that focused information should appear assurface subject in the sentence.
In FUG, this can berepresented by one pattern indicating that focus should occurf'u'st in the sentence as shown in Figu~ 1.
This panern wouldoccur in the sentence category of the grammar, since focus isa sentence constituent.
This constraint is represented aspartof an alternative so that other syntactic constraints canoverride it (e.g., if the goal were in focus but the verb couldnot be pmsivized, ~ constraint would not apply and anactive sentence would be generated).
The structure of activeor passive would be indicated in the verb group as shown inFigure 2.1 The correct choice of active or passive is madethrough unification of the patterns: active voice is selected ifthe focus is on the protagonist (focus unifies with pro:) andpassive if focus is on the goal or beneficiary Orocus unifieswith goal or beheld.
This representation has two desirableproperties: the constraint can be stated simply and theconstruction of the resulting choice b expr=ssed separatelyfrom the constraint.
(a l t  ( (pat te rn  ( focus  .
.
. )
) ) )Figure 1: Constraint on Passive/Active in FUGIn the DCG, the unification of argument variablesmeans a single rule can state that focus should occur first inthe sentence.
However, the rules specifying construction ofthe passive and active verb phrases must now depend onwhich role (protagonist, goal, or beneficiary) is in focus.This requires three separate rules, one of which will bechosen depending on which of the three other case roles isthe same as the value for focus.
The DCG v..presentation thusmixes information from the conswaint, focus of attention,with the passive/active construction, duplicating it over threetThis figure shows only the m'dm, of comtitmmu foe active and passivevoice m~l does no?
include odwr details of the co~au'ucdon.97(alt( (voice act ive)(pattern (prot verb  goal) ) )( (voice pass ive}(alt( (pat tern  (goal verb1  verb2  by-pp) ) )( (pat tern(benef verb l  verb2  by -pp)}})})Figure 2: Passive/Active Construction i FUGrules.The sentence rule is shown in Figure 3 and the threeother rules are presented in Figure 4.
The constituents of theproposition are represented as variables of a clause.
InFigure 4, the arguments, in order, are verb (V), protagonist(PR), goal (G), beneficiary (B), and focus.
The argumentswith the same variable name must be equal.
Hence, in theFigure, focus of the clause must be equal to the protagonist(PR).sentence (clause(Verb, Prot, Goal, Benef, Focus} )~>np l i s t  (Focus},verb_phrase  (Verb, Prot,  Goal,  Benef ,  Focus)  .Figure 3: Passive/Active Constraint in DCG1.3 Focus Shift ConstraintThis constraint, identified and formalized by Derr andMcKeown \[3\], constrains simple and complex sentencegeneration.
Any generation system that generates texts andnot just sentences must determine when to generate asequence of simple sentences and when to combine simplesentences to form a more complex sentence.
Derr andMcKcown noted that when a speaker wants to focus on asingle concept over a sequence of sentences, additionalinformation may need to be presented about some otherconcept.
In such a case, the speaker will make a temporarydigression to the other concept, but will immediatelycontinue to focus on the first.
To signal that focus does notshift, the speaker can use subordinate sentence structurewhen presenting additional information.The focus constraint can be stated formally as follows:assume input of three propositions, PI, P2, and P3 with/* V = Verb; PR = Prot; G ~ Goal;B = Beneficiary; last argument - focus */?
verb_phrase (pred (V, NEG, T, AUX}, PR, G, B, PR)-->verb (V, NEG, T, AUX, N, active),np l i s t  (G),pp (to, B).verb_phrase (pred (V, NEG, T, AUX), PR, G, B, G)-->verb (V, NEG, T, AUX, N, passive),pp (to, B),pp (by, PR).verbphrase  (pred (V, NEG, T, AUX), PR, G, B, B)-->verb (V, NEG, T, AUX, N, passive),npl ist (G),pp (by, PR).Figure 4: Passive/Active Construction i DCGarguments indicating focus F1, F2, and F3.
2 The constraintstates that if F1 = F3, Fl does not equal F2 and F2 is aconstituent of PI, the generator should produce a complexsentence consisting of PI, as main sentence with P2subordinated to it through P2's focus, followed by a secondsentence consisting of P3.
In FUG, this constraint can bestated in three parts, separately from other syntactic rulesthat will apply:I.
Test that focus remains the same from PI toP3.2.
Test that focus changes from PI to P2 and thatthe focus of I'2 is some constituent of PI.3.
If focus does shift, form a new constituent, acomplex sentence formed from PI and P2, andorder it to occur before P3 in the output (orderis specified by patterns in FUG).Figure 5 presents the constraint, while Figure 6 shows theconstruction of the complex sentence from P1 and P2.Unification and paths simplify the representation of theconstraint.
Paths, indicated by angle brackets (<>), allow thegrammar to point to the value of other constituents.
Pathsand unification are used in conjunction in Part 1 of Figure 5to state that the value of focus of P1 should unify with the2In the systems we are describing, input is specified in a case frameformalism, with each pmpositioa indicating protagonist (prot), goal,beneficiary (benef), verb, and focus.
In these systems, iexical choice ismade before ntering the grammar, thus each of these arguments includesthe word to be used in the sentence.98(alt% Is focus the same in P1 and P3?1.
((PI ((focus <^ P3 focus>)))% Does not apply if focus% stays the same2.
(alt (((PI ((focus <^ P2 focus>))))( % Focus shifts; Check that  P2% focus is a const i tuent  of% PI.
(alt(((PI ((prot <^ P2 focus>))))((PI ((goal <a P2 focus>))))((P1 ((benef<^ P2 focus>))))))% Form new const i tuent  from P1% and P2 and order before  P3.3.
(pattern (PIP2subord P3) )(P3 (cat s) )% New const i tuent  is of category% subordinate.
(PIPRsubord% Place P2 focus into% subord inate as it wi l l% be head of re lat ive clause.
(same <^ P2 focus>)(cat subordinate) ) ) ) ) )Figure 5: Focus Shift Constraifit n FUGvalue of focus of P3 (i.e., these two values should be equal).
3Unification also allows for structure to be built in thegrammar and added to the input.
In Part 3, a new constituentP1P2subord is built.
The full structure will result fromunifying P1P2aubord with the category subordinate, inwhich the syntactic structure is represented.
The grammarfor this category is shown in Figure 6.
It constructs a relativeclause 4 from P2 and attaches it to the constituent in P1 towhich focus shifts in 1:'2.
Figure 7 shows the form of inputrequixed for this constraint and the output that would beproduced.3A path is used to expect he focus of P3.
An atuibute value pair suchas (focus <P3 focus>) determines the value for focus by searching for anamibute P3 in the list of am'ibutes (or Functional Description if'D)) inwhichfocus occurs.
The value of P3'sfocua isthen copied in as the valueof focus.
In order to refer to attributes at any level in the m~e formed bythe nestsd set of FDs, the formalism includes an up-arrow (^).
Forexample, given the attribum value pair (attrl <^ am'2 attt3>), the up-arrow indica,,'s that he system should look for attr2 in the FD containingthe FD ofattrl.
Since P3 occurs in the FD containing PI, an up-arrow isused to specify that he system should look for the attribute P3 in the FDcontaining PI (i.e., one level up).
More up-arrows can be used if the fastattribute in the path occurs in an even higher level FD.4The entire grammar for relative clauses is not shown.
In particular, itwould have to add a relative pronoun to the input.
( (cat subordinate)% Wi l l  cons ist  of one compound sentence(pattern (s))(s ((cat s)))% Place contents  of P1 in s.(s <^^ PI>)% Add the subord inate as a% re lat ive c lause modi fy ing  SAME.
( s ^ me% P lace the new subord inate made from% P2 af ter  head.
((pattern (... head newsubord  ...))% Form new subord inate c lause(newsubord% It's a re lat ive clause.
(cat s-bar)(head <^ head>)% Al l  other  const i tuents  in% newsubord  come from P2.
(same ( (newsubord <^ ^ P2>)% Uni fy  same with appropr ia te% const i tuent  of P1 to at tach% re lat ive c lause(s((alt (((prot <^ same>))( (goal <^ same>))( (banef  <^ same>)  ) ) ) ) )Figure 6: Forming the Subordinate Clause in FUGIn the DCG formalism, the constraint is dividedbetween a rule and a test on the rule.
The rule dictates focusremain the same from P1 to P3 and that P2's focus be aconstituent of P1, while the test states that P2's focus mustnot equal P l 's .
Second, because the DCG is essentially acontext free formalism, a duplication of rules for threedifferent cases of the construction is required, depending onwhether focus in P2 shifts to protagonist, goal or beneficiaryof PI.
Figure g shows the three rules needed.
Each ruletakes as input three clauses (the first three clauses listed) andproduces as output a clause (the last listed) that combines P1and P2.
The test for the equality of loci in Pl and P3 is donethrough PROLOG unification of variables.
As in theprevious DCG example, arguments with the same variablename must be equal.
Hence, in the first rule, focus of thethird clause (FI) must be equal to focus of the first clause(also FI).
The shift in focus from P1 to P2 is specified as acondition (in curly brackets {}).
The condition in the firstrule of Figure 8 states that the focus of the second clause(PR l) must not be the same as the focus of the fast clauseif:l).Note that the rules shown in Figure 8 representprimarily the constraint (i.e., the equivalent of Figure 5).99INPUT:( (P l  ( (prot ((head girl)))(goal ((head cat)))(verb-group ((verb .
.
.
.
.
pet)))(focus <prot>))))(P2 (prot ((head =ms cat))(goal ((head ~ mouse))(verb-group ((verb .ms caught)))(focus <prot>))))(P3 ((prot ((head ~-  girl)))(goal ((head ~m happy)))(verb-group ((verb ~ be)))(focus <prot>)))))OUTPUT - The girl pet the cat that caughtthe mouse.
The girl was happy.Figure 7: Input and Output for FUGThe building of structure, dictating how to construct herelative clause from P2 is not shown, although these rules doshow where to attach the relative clause.
Second, note thatthe conswaint must be duplicated for each case where focuscan shift (i.e., whether it shifts to pint, goal or beneficiary).1.4 Compar isons  Wi th  Other  Generat ion  SystemGrammarsThe DCG's duplication of rules and constraints in theexamples given above results because of the mechanismsprovided in DCG for representing conswaints.
Constraintson consdtuent ordering and structure are usually expressed inthe context free portion of the granmmr;, that is, in the leftand fight hand sides of rules.
Constraints on when thecontext free rules should apply are usually expressed as testson the rules.
For generation, such constraints includepragmatic onstraints on free syntactic hoice as well as anycontext sensitive constraints.
When pragmatic onstraintsapply to more than one ordering constraint on constituents,this necessarily means that the constraints must be duplicatedover the rules to which they apply.
Since DCG allows forsome constraints to be represented through the unification ofvariables, this can reduce the amount of duplicationsomewhat.FUG allows pragmatic onstraints to be represented asmeta-rules which are applied to syntactic rules expressingordering constraints through the process of unification.
Thisis similar to Chomsky's \[2\] use of movement and focus rulesto transform the output of context free rules in order to avoidrule duplication.
It may be possible to factor out constraintsand represent them as recta-rules in a DCG, but this wouldinvolve a non-standard implementation of the DCG (forexample, compilation of the DCG to another grammarformalism which is capable of representing constraints asmeta-rules)./* Focus of P2 is protagonist of PI (PR1)Example: the cat was petted by the girlthat brought it.
the cat purred */foc_shift (clause (VI, PR1, GI, B1, FI),clause (V2, PR2, G2, B2, PRI) ,clause (V3, PR3, G3, B3, F1),clause (Vl,\[np (PRI, clause (V2, PR2, G2, B2, PRI) ) \],GI, BI, FI) )/* Test: focus shifts from P1 to P2 */(~ I  \-~ FI}/* Focus of P2 is goal of P1 (GI)Example: the girl pet the cat thatcaught the mouse, the girl was happy */foc shift (clause (Vl, PRI, GI, BI, FI),Iclause (V2, PR2, G2, B2, GI),clause (V3, PR3, G3, B3, FI) ,clause (Vl, PRI,\[np (GI, clause (V2, PR2, G2, B2, GI) ) \],~i,Fl) )/* Test: focus shifts from P1 to P2 */{GI \~m FI}/* Focus of P2 is Beneficiary of P1 (BI)Example: the mouse was given to the catthat was hungry, the mouse was nothappy */foc shift (clause (Vl, PRI, G1, B1, FI),~ause  (V2, PR2, G2, B2, BI) ,clause (V3, PR3, G3, B3, FI),clause (VI, PRI, GI,\[np (B1, clause (V2, PR2, G2, B2, BI) ) \],r l )  )/* Test: focus shifts from P1 to P2 */(~I V-= r l}Figure 8: Focus Shift Constraint in DCGOther grammar formalisms that express constraintsthrough tests on rules also have the same problem with ruleduplication, sometimes even more severely.
The use of asimple augmented context free grammar for generation, asimplemented for example in a bottom-up parser or anaugmented transition network, will require even moreduplication of constraints because it is lacking the unificationof variables that the DCG includes.
For example, in abottom-up generator implemented for word algebra problemgeneration by Ment \[10\], constraints on wording of theproblem are expressed as tests on context free rules andnatural anguage output is generated through actions on therules.
Since Ment controls the linguistic difficulty of thegenerated word algebra problem as well as the algebraicdifficulty, his constraints determine when to generate100particular syntactic constructions that increase wordingdifficulty.
In the bottom-up generator, one such instructionalconsuaint must be duplicated over six different syntacticrules, while in FUG it could be expressed as a singleconstraint.
Ment's work points to interesting waysinstructional constraints interact as well, further complicatingthe problem of clearly representing constraints.In systemic grammars, such as N IGEL  \[6\], each choicepoint in the grmm'nar is represented as a system.
The choicemade by a single system often determines how choice ismade by other systems, and this causes an interdependenceamong the systems.
The grammar of English thus forms ahierarchy of systems where each branch point is a choice.For example, in the part of the grammar devoted to clauses,one of the Rrst branch points in the grammar woulddetermine the voice of the sentence to be generated.Depending on the choice for sentcmce voice, other choicesfor ovcrali sentence structure would be made.
Constraints onchoice arc expressed as LISP functions called choosers ateach branch point in the grammar.
Typically a differentchooser is written for each system of the grammar.
Choosersinvoke functions called inquiry operators to make testsdetermining choice.
Inquiry operators are the primitivefunctions representing constraints and are not duplicated inthe grammar.
Calls to inquiry operators from differentchoosers, however, may be duplicated.
Since choosers areassociated with individual syntactic choices, duplications ofcalls is in some ways similar to duplication in augmentedcontext free grammars.
On the other hand, since choice isgiven an explicit representation and is captured in a singletype of rule called a system, representation of constraints ismade clearer.
This is in contrast to a DCG where constraintscan be distributed over the grammar, sometimes representedin tests on rules and sometimes represented in the rule itself.The systcmic's grammar use of features and functionalcategories as opposed to purely syntactic categories isanother way in which it, like FUG, avoids duplication ofrules.It is unclear from published reports how constraints arerepresented in MUMBLE \[7\].
Rubinoff\[16\] states thatconstraints are local in MUMBLE, and thus we suspect thatthey would have to be duplicated, but this can only beverified by inspection of the actual grammar.2 Improved Eff ic iencyOur implementation f FUG is a reworked version ofthe tactical component for TEXT \[9\] and is implemented inPSL on an IBM 4381 as the tactical component for theTAILOR system \[11; 12\].
TAILOR's FOG took 2 minutesand 10 seconds of real time to process the 57 sentences fromthe appendix of TEXT examples in \[9\] (or 117 seconds ofCPU time).
This is an average of 2.3 seconds real time persentence, while TEXT's FUG took, in some cases, 5 minutesper sentence.
5 This compares quite favorably withRubinoff's adaptation \[16\] of MUMBLE\[7\] for TEXT'sstrategic omponent.
Rubinoff's MUMBLE could processall 57 sentences in the appendix of TEXT examples in 5minutes, yielding an average of 5 seconds per sentence.SWe use real times for our comparisons i  ordea to make an analogywith Rubinoff \[16\], who also used real times.Thus our new implementation results in yet a better speed-up(130 times faster) than Rubinoff's claimed 60 fold speed-upof the TEXT tactical component.Note, however, that Rubinoff's comparison is not at alla fair one.
First, Rubinoff's comparisons were done in realtimes which are dependent on machine loads for time-sharing machines such as the VAX-780, while Symbolicsreal time is essentially the same as CPU time since it is asingle user workstation.
Average CPU time per sentence inTEXT is 125 seconds.
6 This makes Rubinoff's system only25 times faster than TEXT.
Second, his system runs on aSymbolics 3600 in Zctalisp, while the original TEXT tacticalcomponent ran in Franzlisp on a VAX 780.
Using Gabriel'sbenchmarks \[4\] for Boyer's theorem proving unificationbased program, which ran at 166.30 seconds in Franzlisp ona Vax 780 and at 14.92 seconds in Symbolics 3600Commonl.isp, we see that switching machines alone yields a11 fold speed-up.
This means Rubinoff's system is actuallyonly 2.3 times faslcr than TEXT.Of course, this means our computation of a 130 foldspeed-up in the new implementation is also exaggeratedsince it was computed using real time on a faster machinetoo.
Gabriel's benchmarks arc not available for PSL  on theIBM 4381, 7 but we are able to make a fair comparison of thetwo implementations since we have both the old and newversions of FUG running in PSL on the IBM.
Using CPUtimes, the new version proves to be 3.5 times faster than theold tactical component, eRegardless of the actual amount of spc~-up achieved,our new version of FUG is able to achieve similar speeds toMUMBLE on the same input, despite the fact that FUG usesa non-deterministic algorithm and MUMBLE uses adeterministic approach.
Second, regardless of comparisonsbetween systems, an average of 2.3 seconds real time persentence is quite acceptable for a practical generationsystem.We were able to achieve the speed-up in our newversion of FUG by making relatively simple changes in theunification algorithm.
The fast change involvedimmediately selecting the correct category for unificationfrom the grammar whenever possible.
Since the grammar isrepresented asa llst of possible syntactic ategories, the firststage in unification involves selecting the correct category tounify with the input.
On fast invoking the unifier, thismeans selecting the sentence level category and on unifyingeach constituent of the input with the grammar, this meansselecting the category of the constituem.
In the oldgrammar, each category was unified successively until thecorrect one was found.
In the current implementation, weretrieve the correct category immediately and begin?
'rhis was computed using TEXT's appendix where CPU time is givenin units corresponding to 1/60 second.
"/Gabriel's benchmarks are available only for much larger IBM,mainfranzs.SThe new version took 117 CPU seconds to process all sentences, or 2CPU seconds per sentence, while the old version took 410 CPU secondsto process all sentences, or 7 CPU seconds per sentence.101unification directly with the correct category.
Althoughunification would fail immediately in the old version,directly retrieving the category saves a number of recursivecalls.Unification with the lexicon uses the same technique inthe new version.
The correct lexicai item is directly retrievedfrom the grammar for unification, rather than unifying witheach entry, in the lexicon successively.Another change involved the generation of only onesentence for a given input.
Although the grammar is oftencapable of generating more than one possible sentence for itsinput 9, in practice, only one output sentence is desired.
In theold version of the unifier, all possible output sentences weregenerated and one was selected.
In the new version, only onesuccessful sentence is actually generated.Finally, other minor changes were made to avoidrecursive calls that would result in failure.
Our point inenumerating these changes i  to show that they arc extremelysimple.
Considerably more speed-up is likely possible iffurther implementation were done.
In fact, we recentlyreceived from ISI a version of the FUG unifier which wascompletely rewritten from our original code by Jay Myers.
Itgenerates about 6 sentences per seconds on the average inSymbolics Commonlisp.
Both of these implementationsdemonstrate hat unification for FUG can be done efficiently.3 Conc lus ionsWe have shown how constraints on generation can berepresented separately from representation of syntacticstructure in FUG.
Such an ability is attractive because itmeans that the constraint can be stated once in the grammarand can be applied to a number of different syntactic rules.In contrast, m augmented context free based generationsystems, constraints must be stated locally as part ofindividual syntactic rules to which they apply.
As a result'constraints must be duplicated.
Since a main focus inlanguage generation research as been to identify constraintson choice, the ability to represent constraints clearly andefficiently is an important one.Representing constraints eparately is only useful forglobal constraints, of course.
Some constraints in languagegeneration are necessarily local and must be represented inFUG as they would in augmented context free basedsystems: as part of the syntactic structures to which theyapply.
Furthermore, information for some constraints maybe more easily represented outside of the grammar.
In suchcases, using a function caLl to other components of thesystem, as is done in NIGEL, is more appropriate.
In fact,this ability was implemented as part of a FUG inTELEGRAM \[I\].
But for global constraints for whichinformation is available in the grammar, FUG has anadvantage over other systems.Our reimplementation f FUG has demonstrated thatefficiency is not as problematic as was previously believed.Our version of FUG, running in PSL on an IBM 4381, runs9Often the surface sentences gen~ated are the same, but the syntacticstructure built in producing the sentence differs.faster than Rubinoff's version of MUMBLE in Symbolics3600 Zetalisp for the same set of input sentences.Furthermore, we have shown that we were able to achieve aslightly better speed-up over TEXT's old tactical componentthan Rubinoff's MUMBLE using a comparison that takesinto account different machines.
Given that FUG canproduce sentences in time comparable to a deterministicgenerator, efficiency should no longer be an issue whenevaluating FUG as a generation system.AcknowledgementsThe research reported in this paper was partiallysupported by DARPA grant N00039-84-C-0165, by ONRgrant N00014-82-K-0256 and by NSF grant IST-84-51438.We would like to thank Bill Mann for making a portion ofNIGEL's grammar available to us for comparisons.Re ferences\[1\] Appelt' D. E.T~T .~GRAM: A Gra.tm'nar Formalism for LanguagePlanning.In Proceedings of the Eigth National Conference onArtificial Intelligence, pages 595 - 9.
Karlsruhe,West Germany, August, 1983.\[2\] Chomsky, N.Essays on Form and Interpretation.North-Holland Publishing Co., Amsterdam, TheNetherlands, 1977.\[3\] Deft, M.A.
and McKeown, K. R.Using Focus to Generate Complex and SimpleSentences.In Proceedings of the \]Oth International Conferenceon Computational Linguistics, pages 501-4.Stanford, Ca., July, 1984.\[4\] Gabriel, R. P.Performance and Evaluation of Lisp Systems.MIT Press, Cambridge, Mass., 1985.Kay, Martin.Functional Grammar.In Proceedings of the 5th meeting of the BerkeleyLinguistics Society.
Berkeley Linguistics Society,1979.\[6\] Mann, W.C. and Matthiessen, C.NIGEL: A Systemic Grammar for Text Generation.Technical Report ISI/RR-85-105, InformationSciences Institute, February, 1983.4676 Admiralty Way, Marina del Rey, California90292-6695.\[7\] McDonald, D. D.Natural Language Production as a Process ofDecision Making under Constraint.PhD thesis, MIT, Cambridge, Mass, 1980.McKeown, K. R.Focus Constraints on Language Generation.In Proceedings of the Eight International Conferenceon Artificial Intelligence.
Karlsruhe, Germany,August, 1983.,.
\[51.\[8\]102\[9\] McKeown, K.R.Text Generation: Using Discourse Strategies andFocus Constraints to Generate Natural LanguageText.Cambridge University Press, Cambridge, England,1985.\[10\] Ment~ J.From Equations to Words.
Language Generationand Constraints in the Instruction of AlgebraWord Problems.Technical Report, Computer Science Depamnent,Columbia University, New York, New York,10027, 1987.\[11\] Paris, C. L.Description Strategies for Naive and Expert Users.In Proceedings of the 23rd Annual Meeting of theAssociation for Computational Linguistics.Chicago, 1985.\[12\] Paris, C. L.Tailoring Object Descriptions to the User's Level ofExpertise.Paper presented atthe International Workshop onUser Modelling, Maria Laach, West Germany.August, 1986\[13\] Pereira, F.C.N.
and Warren, D.H.D.Definite Clause Grammars for Language Analysis -A Survey of the Formalism and a Comparisonwith Augmented Transition Network.Artificial Intelligence :231- 278, 1980.\[14\] Ritchie, G.The Computational Complexity of SentenceDerivation in Functional Unification Grammar.In Proceedings of COLING '86.
Association forComputational Linguistics, Bonn, West Germany,August, 1986.\[15\] Ritchie, G.Personal Communication.\[ 16\] Rubinoff, R.Adapting MUMBLE: Experience with NaturalLanguage Generation.In Proceedings of the Fifth Annual Conference onArtificial Intelligence.
American Association ofArtificial Intelligence, 1986.103
