Recent  Progress on the VOYAGER SystemVictor Zue, James Glass, David Goodine,Hong Leung, Michael McCandless, Michael Phillips,Joseph Polifroni, and Stephanie SeneffRoom NE43-601Spoken Language Systems GroupLaboratory for Computer ScienceMassachusetts Institute of TechnologyCambridge, MA 02139IntroductionThe VOYAGER speech recognition system, which was de-scribed in some detail at the last DARPA meeting \[9\], isan urban exploration system which provides the user withhelp in locating various sites in the area of Cambridge, Mas-sachusetts.
The system has a limited database of objectssuch as banks, restaurants, and post offices and can provideinformation about these objects (e.g., phone numbers, type ofcuisine served) as well as providing navigational ssistance be-tween them.
VOYAGER accepts both spoken and typed inputand responds in the form of text, graphics, and synthesizedspeech.
Since the last meeting, we have made developmentsto VOYAGER that have had an impact on the usability of thesystem.In this paper, we will describe these developments andreport on evaluation results after these changes were incor-porated into the system.
Two key developments to VOYAGERare a tighter integration of the speech and natural anguagecomponents and a pipelined hardware implementation lead-ing to a speed-up in processing time from approximately 12times real time to approximately 5 times real time.
We alsodiscuss here a number of incremental improvements in theword-pair grammar, pronunciation etworks, and the back-end capabilities.SR/NL IntegrationIn our initial implementation f VOYAGER, the integrationof speech and natural anguage components was accomplishedby obtaining the best word sequence from the recognizer andpassing that word sequence to the natural anguage system.Modifying the speech recognition component to produce alist of the top scoring word sequences provides a convenientmeans for increasing the level of integration of the speechrecognition and natural anguage components \[2\].
In this way,the natural anguage system can be run successively on eachof the word sequences to find the highest scoring sequencethat passes the natural anguage constraints.Two-stage N-Best searchPreviously, to produce the top scoring word sequence, ourspeech recognition system used Viterbi search \[4,10\].
This al-gorithm provides an efficient search for the top word sequencebut does not directly provide the top N word sequences.
Oth-ers have chosen to modify this search by keeping track of thetop N word sequences at each point in the search \[2\].
We alsouse a modification of Viterbi search to produce the top Nword sequences.
In our algorithm, we first use Viterbi searchto compute the best partial paths both arriving and leavingeach lexical node at each point in time.
The algorithm thensuccessively extracts the next best complete path by search-ing through the precomputed matrix of partial paths to findthe highest scoring path that has not yet been extracted.To extract he N highest scoring paths from the precom-puted matrix of partial paths, this two-stage N-Best searchutilizes the fact that each new path must either contain a newnode-pair (a given lexical node at a given point in time) ormust be some combination of portions of the paths foundso far.
So, the search must keep track of the best pathpassing through each node-pair (which is the sum of thescores of the best arriving and leaving paths computed bythe Viterbi search) and must also keep track of all combina-tions of the complete paths found so far.
The next highestscoring path can be found by taking the highest scoring patheither through a new node-pair or from some combination ofprevious paths.The computation of the partial paths either arriving orleaving each lexical node at each point in time is the same asneeded for the forward Viterbi search for the top scoring wordsequence.
Therefore, the total computation eeded for thisalgorithm is two times the Viterbi search plus the amountof computation we need to extract the paths from the pre-computed matrix.
We have measured the computation timeand memory use of our implementation of this algorithm asa function of the number of sentence hypotheses.
This re-source use is plotted as the open symbols in Figure 1.
Thisexperiment was performed on 495 utterances with a test setword-pair perplexity of 73 and a vocabulary size of 350 words.This algorithm is somewhat different from the frame- syn-chronous algorithm described previously \[2\], and has a num-ber of advantages and disadvantages.
An important advan-tage for VOYAGER is that we do not have to choose N beforeperforming the search.
In the system, we are able to checkeach word string as it is produced by the recognizer and tellthe system to quit as soon as one of the sentences passes thenatural anguage constraints.
Also, at least in our segmentbased system, this algorithm is quite efficient.
This efficiencyadvantage may not hold for frame-based systems.
As de-scribed above, it is necessary to keep track of pointers for thepartial paths for the entire node-pair matrix.
This is not a206large problem in our system, since the nodes are at a segmentlevel rather than at a frame level.
Furthermore, we neededto keep track of these pointers for the forward pass in theViterbi search anyway, and so the memory requirements onlyincrease by a factor of two.
A disadvantage of this approach,at least when implemented on a per utterance-basis a de-scribed, is that more than two-thirds of the search cannotbe started until the end of the utterance is reached.
There-fore, this part of the processing cannot be pipelined with theincoming speech.A* searchPassing the top N word sequences to the natural anguagesystem is an improvement over passing only the single bestscoring sequen6e, but our goal is to make better use of thenatural anguage constraints at an early stage of the search.The A* search algorithm can Provide a flexible mechanism formaking use of natural anguage constraints because it keeps astack of partial paths that are extended based on an evalua-tion function.
Non-probabilistic natural anguage constraintscan be used to prune partial hypotheses either before theyare put on the stack or before they are extended.
Predictioncapability of the natural anguage system can be used to pro-pose ways of extending partial paths.
Finally, probabilitiesof partial paths provided by the natural anguage system canbe incorporated into the evaluation function.The A* search evaluation function is defined asif(p) = g(p) + h*(p),where f*(p) is the estimated score of the best path con-taining partial path p, g(p) is the score for the match fromthe beginning of the utterance to the end of the partial pathp, and h*(p) is an estimate of the best scoring extension ofthe partial path p to the end of the utterance \[1\].
This searchis admissible if h*(p) is an upper bound on the actual bestscoring extension of partial path p to the end.To efficiently apply A* search to spoken language sys-tems, it is important o have as tight a bound as possible forha(p) since a looser bound results in increased computation.We can use Viterbi search to compute this upper bound bysearching back from the end of the utterance to find the bestscore to the end for each lexical node at each point in time.If the constraints we use in the Viterbi search to compute thebest score to the end are a subset of the full natural anguageconstraints, this estimate of the best score to the end is guar-anteed to be an upper bound on best score to the end giventhe full constraints.The A* search allows a large amount of flexibility in whento apply the natural anguage constraints.
For example, wecan wait until we have entire sentence hypotheses before ap-plying the full natural anguage constraints.
This turns theA* search into an N-best algorithm \[3\] and allows us to com-pare it directly to the other N-best algorithms.
We computedprocessing time and memory use for our implementation ofthis algorithm and plotted it in Figure 1.
For the top 1 wordsequence, this algorithm requires about the same amount ofresources as our implementation of Viterbi search and theICUL6ooII4 .~E 2I O I  Two-Stage N-Best CPU TimeTwo-Stage N-Best Memory UsageA* CPU Time ~ i ,2~ ?
~: 3'0N!00F igure  1: This figure compares the CPU andMemory usage of the A* N-Best search with theTwo-Stage N-Best algorithm as a function of N. Allquantities are relative to the resource use of our im-plementation of Viterbi search for the top scoringword sequence.amount of resources increases approximately inearly with Nat least for small N.We have begun to perform experiments odetermine whichnatural language constraints to apply at an earlier stage ofthe A* search.
There is a tradeoff between the cost of apply-ing the constraint and the amount of other computation thatis saved by the application of the constraint.
Since we areable to apply word-pair constraints at a very small cost (byprecompiling them into the lexical network), we have beenapplying word-pair constraints at the lowest levels in all ofthese experiments.Word pair constraintsIn our initial implementation f VOYAGER, the search wasconstrained by a word-pair language model obtained irectlyfrom the.
training utterances.
This word-pair language modelhad a perplexity of 22 and a coverage of 65%.
However, thisword-pair language model was obtained without considera-tion of the constraints from TINA and, therefore, did notmatch the capabilities of the full system.
Utterances thatTINA could accept as well-formed were sometimes rejected bythe word-pair language model.Now that we are moving towards tighter integration ofthe speech and natural anguage components, we are not sodependent on the constraints of a simple language model.However, if it is possible to automatically extract the localconstraints of the natural anguage system, we can save com-putation by making use of them.
Even in a tightly integratedspeech and natural anguage system, it is possible to compilethese constraints directly into a lexical network.
The overallaccuracy will not suffer as long as we can guarantee that the207constraints of the local language model are a subset of thefull constraints.A useful facility for deriving inexpensive recognizer con-straints from a natural anguage system would be a mecha-nism to extract an exhaustive word-pair language model auto-matically from the parent grammar.
To this end, we exploreda number of procedures to discover all legitimate two wordsequences allowed by TINA.
We assessed the resulting lan-guage models by measuring coverage and perplexity on ourdesignated evelopment set of about 500 sentences.The simplest approach is to exhaustively generate all ter-minal -pairs directly from the context-free rules, without ap-plying any other semantic or syntactic onstraints.
We triedthis approach, and, as expected, it gave 100% coverage on thetest set, but with a very high perplexity (~ 200).
In an at-tempt to reduce the perplexity, we tried some permutationsof this method.
We first discarded any rules that did notshow up in our set of 3000 training sentences.
This resultedin a loss of coverage on 10% of the test sentences, o this ideawas abandoned.
A second, more conservative, idea was toallow the disappearance of trace nodes only within those rulecontexts that showed up in the training set.
This resultedin a slight reduction in perplexity to 190, and the coverageremained at 100%.The other approach we tried was to make use of TINA'Sgeneration capability to generate sentences at random, andthen use the resulting terminal pairs to update the word-pair language model.
This approach has the disadvantagethat it can never be guaranteed that TINA's language modelis exhaustively covered.
However, it permits the incorpora-tion of local syntactic and semantic onstraints.
We decidedto discard semantic match requirements in the trace mecha-nism, so that a sentence such as "(What restaurant)i is it (ti)from MIT to Harvard Square?"
would be accepted.
We didaway with the trace mechanism in generation since these longdistance constraints are generally invisible to the word-pairlanguage model.
This was necessary because, when seman-tic matches are required, generation usually picks the wrongpath and aborts on constraint failure.
As a consequence,paths with traces are rarely visited by the generator and maynot show up in our word-pair language model.This method was quite successful.
TINA can generate100,000 sentences in an overnight run, and the resulting word-pair language model had a perplexity of only 73 with a singlemissed word-pair in the test set.
We therefore decided to in-corporate this word-pair language model into the recognizer.Increased CoverageAs we have described previously \[9\], the command gener-ation component translates the natural anguage parse to afunctional form that is evaluated by the system.
This compo-nent has been made more flexible, in part due to our experi-ence with developing an ATIS system \[6\].
We have extendedthe capabilities of the back-end functions to handle more com-plex manipulations.
Some of these changes were motivatedby an examination of our training data.
In other cases, wewere interested in knowing if our framework could handle ma-nipulations commonly used in other database query systems.For this reason we included conjunction and negation, eventhough they are rarely used by subjects (except by those witha natural anguage processing background!).
As a result ofthese modifications, the system is now capable of handlingqueries such as "Show me the Chinese or Japanese restau-rants that are not in Central Square," or "Do you know ofany other restaurants near the main library?
"Pronunciation NetworksPronunciation etworks and their expansion rules weremodified as a result of the increased amount of training data.An effort was made to modify both the networks and the rulesas consistently and minimally as possible.
The VOYAGER dic-tionary was periodically reviewed to insure that pronuncia-tions were consistent in terms of both segmentals and themarking of stressed and unstressed syllables.
When phoneti-cally labelling the VOYAGER corpus, unusual or new pronun-ciations were noted by the labelers, who conferred on pho-netic transcriptions.
New pronunciations were entered intothe dictionary or added to the lexical rules when it was feltthat the phenomena they represented were sufficiently gener-alizable to the corpus as a whole.
Aberrant pronunciationsor mispronunciations were not included.Current ImplementationIn the initial implementation of VOYAGER, the system ranon a Sun 4/280 using a Macintosh II with four DSP32Cs as afront-end.
That system was not pipelined and took approx-imately 12 times real time before the top-choice utteranceappeared.
Since that time we have developed a pipelined im-plementation of VOYAGER on a new set of hardware as illus-trated in Figure 2.
We are using four signal processing boardsmade by Valley Enterprises, each of which has four DSP32C's.Each processor has 128Kbytes of memory and operates inde-pendently of the others (in the board configuration that wehave been using).
Communication with the host is throughthe VME bus of the host.
The host may read from any loca-tion of any of the DSP32C's memory while the DSP  processoris running.
The host may simultaneously write to any com-bination of the four DSP32C's memories.
For speech inputand playback, we are using an A/D D/A made by AtlantaSignal Processing Inc.
This has a high speed serial interfacewhich connects to the serial port of one of the DSP32Cs.
Weare currently using a Sun4/330 with 24Mbytes of memory asa host.
We are running the natural anguage and responsegeneration components on a separate Sparcstation.
Theseparts of the system are written in Lisp; they have fairly largememory requirements and would slow down the processing ifrun simultaneously on the same host as the speech recogni-tion system.
Also, our Sun4/330 has no display.
The entiresystem could easily run on a single host with more memoryplus a display.It has been straightforward to divide the processing forVOYAGER's front-end \[9\] into subsets which can each be per-formed in real-time by a single DSP32C and which do notrequire excessive amounts of intercommunication.
The au-ditory model can be broken up by frequency channel and208EthemetISu.u..4/~O \[ SPARCeS~t, ion I?
Data Capture ?
Auditory Modelling * Natural Language?
Phonetic Recognition * Response Generation?
Lex:\[cal AccessF igure  2: This figure shows the current hardwareconfiguration of the VOYAGER, system.With further optimization of DSP code, we believe thatthe processing through phonetic lassification will run in realtime in the present hardware configuration.
When combinedwith lexical access, the entire system will run in approxi-mately 3 times real time on a Sun4/330 and in approximately2 times real time on a Sun 4/490.Eva luat ionsAt the October 1989 DARPA meeting, we presented anumber of evaluations of our initial version of VOYAGER \[8\]and we have used the same test set to measure the effects ofthe changes made since that time.
To measure the effects ofmultiple sentence hypotheses, we allowed the system evalu-ated in \[8\] to produce the top N word sequences rather thanthe highest scoring word sequence.
Its performance is plot-ted as a function of N in Figure 3.
For each utterance, wetherefore the current representation could be run on up to 40different processors.
The dendrogram computation is difficultto divide among processors, but fortunately it runs in underreal time on a single DSP32C.
The computation of acousticmeasurements and phonetic classification is done on a seg-mental basis and could be broken up by segment if necessary.We have implemented each processor-sized subset of thecomputation for the DSP32C with a circular input and outputbuffer.
Each of these processes monitors the input and outputbuffers, and runs as long as the input buffer is not empty andthe output buffer is not full.
The host keeps larger circularbuffers for each of the intermediate r presentations aud fillsthe input buffers and empties the output buffers of the DSPprocessors as the data become available.
We have used thesame general mechanism for each part of the system, allow-ing us to easily change the various parts of the system as newalgorithms are developed.
All parts of the system before nat-ural language processing are written in C with the exceptionof a small number of hand-optimized DSP32C functions.The lexical access component is using a reversed versionof the A* N-Best search as described above and in \[3\].
So,rather than using Viterbi search to compute the best com-pletion of partial paths and A* search to search forward, weuse Viterbi search to find the best path from the beginningof any partial path and use A* search to find the best pathfrom the end.
This allows us to pipeline the Viterbi searchwith the incoming speech.We are still in the process of optimSzing the code on theDSP32C's, so we are not sure what the final configuration willbe, but we are currently using one processor for data capture,one processor for input normalization, eight processors for theauditory model, two processors for some additional represen-tations, one processor for the dendrogram, one processor foracoustic measurements, and two processors for phonetic las-sification.
The current implementation computes these partsof the system in 2.3 times real time.
When we combine lex-ical access on the same host the total processing time forVOYAGER is 5 times real time to completion.1009080 ?
'70 ~60"50"~ 40~~, 30 ~20100-----4=!---- (b)(c)(d)(e).
.
.
.
.
.
.
.
!10N100F igure  3: This figure shows the overall performanceon the test set as a function of the number of wordstrings produced by the speech recognition compo-nent.
Curve (d) shows the percentage of utteranceswhere the correct word string is found.
Curve (c)shows the percentage where the correct responseis generated (see text for definition of "correct").Curve (b) shows the percentage of utterances whereVOYAGER produces any response.
The horizontalline (e) shows the percentage of utterances where aresponse would have been produced if the correctword string had been found by the speech recogni-tion component.
Finally, curve (a) shows the per-centage of utterances where either a response wasproduced from the top N word sequences from therecognition, or a response would have been producedgiven the correct word string.took the highest scoring word string accepted by the natu-ral language component of VOYAGER.
The lower curve showsthe percentage of these strings that are identical (after ex-panding contractions such as "what's") to the orthographic209transcription of the utterance?
The next curve shows thepercentage that produce the same action in VOYAGER as theaction produced by the correct word string; these are the ut-terances that are "correct" at a functional level.
The nextcurve shows the percentage of utterances that produced anyresponse from VOYAGER.
The difference between curve (c)and curve (b) indicates the number of incorrect responses(with "incorrect" meaning that the utterance produces a dif-ferent response from the one that would have been producedwith the correct word string).
The remaining utterances, in-dicated by the area above curve (b), produce an "I'm sorry,I didn't understand you" response from VOYAGER.
Of theseremaining utterances, we found the number that would haveproduced a response if the system was given the correct wordstring.
This is plotted as the difference between curves (b)and (a).
The horizontal line (e) shows the percentage of ut-terances that produce an action given the correct word string.The difference between curves (a) and the horizontal line isthe percentage of utterances that produce a response fromVOYAGER when given the speech input but do not produce aresponse given the correct word string.
These responses werejudged either correct or incorrect by the system designers.There are a number of things to learn from this figure.If we search deeper (either by increasing N or by incorporat-ing the natural anguage constraints earlier in the search), westill increase the number of utterances that produce a correctresponse but at the expense of producing more incorrect re-sponses.
The difference between curves (a) and (b) shows thenumber of utterances that will produce a response if we canonly find the correct word string with the search.
So, thisdifference is the most that we can hope to gain by increasingthe depth of the search (although this is not quite true sinceit is possible to find a word string that parses and producesthe correct response ven if the correct word string does notparse).The previous results were computed using the perplexity22 word pair grammar.
As discussed previously, we haveproduced a word pair grammar with perplexity 73 that bettermatches the constraints of the natural anguage system.
Acomparison of these two sets of constraints can be seen inFigure 4.
In this figure, we have plotted the upper threecurves of Figure 3 for both the perplexity 22 grammar andthe perplexity 73 grammar.
It can be seen that while theperplexity 73 grammar has slightly lower performance, thisdegradation decreases as N increases above 10.
We wouldhope that even with less constraint in the speech recognitioncomponent, he performance will be better than the tighterconstraints as we search deeper.
This should be true sincethe constraints match the natural anguage constraints muchbetter.Summary/Future PlansThe evaluations show that compared to passing only thetop scoring word string to the natural anguage system, theperformance of the overall system is much improved by in-creasing the degree of integration of the speech recognitionand natural anguage systems.
However, the evaluations alsoshow that there is not much to be gained in our system byI0090-80-70~60-40"30'20'IO--- PERP=73 (b)at.
PERP=73 (c)+ PERP=-22 (a)PERIX=-22 (b)PERP=22 (c).
.
.
.
.
.
.
10  .
.
.
.
.
.
.N100F igure  4: This figure shows the difference in per-formance for two different sets of speech recognitionconstraints.
The curves are the same as the upperthree curves in Figure 3 for perplexity=22 and per-plexity=73.increasing the depth of the search (either by increasing Nin an N-Best search or by integrating the natural languageconstraints at an earlier stage of the search) since this willincrease the number of incorrect responses faster than in-creasing the number of correct responses.
What is neededare new sources of information for the search.
Fortunately,our natural anguage system is capable of providing probabil-ities that we have not yet utilized.
These probabilities havebeen shown to reduce the perplexity by at least a factor ofthree \[9\] and therefore should allow an increase in the depthof the search with a smaller number of incorrect responses.We may also gain some performance by incorporatingsome form of explicit rejection criterion.
Currently we re-ject an utterance based on the number of word strings thatfail to produce a response (by choosing an upper bound onN in the N-Best search).
If we used a more explicit rejectioncriterion (by taking into account he scores of the top N wordstrings for example) we may be able to decrease the ratio ofincorrect response to correct responses.There have been a number of developments in the speechrecognition components that we intend to incorporate intothe VOYAGER system.
These are discussed in more detailin \[7\].We would like to begin exploring dynamic adaptation ofthe natural anguage constraints.
For example, we would liketo increase the objects in VOYAGER's database to a muchmore complete set.
In our current implementation, this wouldincrease the perplexity of the speech recognition and resultin poor performance.
However, if we limit the vocabularybased on the discourse history, it is likely that we can makelarge increases in the size of the VOYAGER domain without210significant increases in perplexity.Since we are interested in improving performance in theinteractive use of the system, we have implemented a mecha-nism for automatically generating tasks for the user to solvewith the help of the system [5].
This has allowed us to be-gin testing the system in a goal-directed mode and compareresults obtained in such a mode to results obtained on datacollected in a simulation mode.AcknowledgementsWe would like to thank Dave Goddeau and Kirk John-son for their help with the modifications made to VOYAGERdescribed above.References[1] Barr, A., E. Feigenbaum, and P. Cohen, The Handbookof Artificial Intelligence, 3 vols., William KaufmanPublishers, Los Altos, CA, 1981.
[2] Chow, Y, and R. Schwartz, "The N-Best Algorithm:An Efficient Procedure for Finding Top N SentenceHypotheses", Proc.
DARPA Speech and NaturalLanguage Workshop, pp.
199-202, October, 1989.
[3] Soong, F., and E. Huang, "A Tree-Trellis Based FastSearch for Finding the N-best Sentence Hypotheses inContinuous Speech Recognition", these proceedings.
[4] Viterbi, A., "Error Bounds for Convolutional Codesand an Asymptotically Optimal Decoding Algorithm",IEEE Trans.
Inform.
Theory Yol.
IT-13, pp.
260-269,April, 1967.
[5] Whitney, D. Building a Paradigm to Elicit a Dialogwith a Spoken Language System, Bachelor Thesis, MITDepartment of Electrical Engineering and ComputerScience, Cambridge, MA, 1990.
[6] Zue, V., J.
Glass, D. Goodine, H. Leung, M. Phillips, J.Polifroni, and S. Seneff, "Preliminary ATISDevelopment a MIT", these proceedings.
[7] Zue, V., J.
Glass, D. Goodine, H. Leung, M. Phillips, J.Polifroni, and S. Seneff, "Recent Progress on theSUMMIT System", these proceedings.
[8] Zue, V., N. Daly, J.
Glass, D. Goodine, H. Leung, M.Phillips, J. Polifroni, S. Seneff, and M. Soclof, "TheCollection and Preliminary Analysis of a SpontaneousSpeech Database", Proc.
DARPA Speech and NaturalLanguage Workshop, pp.
126-134, October, 1989.
[9] Zue, V., J.
Glass, D. Goodine, H. Leung, M. Phillips, J.Polifroni, and S. Seneff, "The VOYAGER SpeechUnderstanding System: A Progress Report", Proc.DARPA Speech and Natural Language Workshop, pp.51-59, October, 1989.
[10] Zue, V., J.
Glass, M. Phillips, and S. Seneff, "The MITSUMMIT Speech Recognition System: A ProgressReport," Proc.
of DARPA Speech and NaturalLanguage Workshop, February, 1989.211
