Keyword Extraction using Term-Domain Interdependence forDictation of Radio NewsYosh imi  Suzuk i  Fumiyo  Fukumoto  Yosh ih i ro  Sek iguch iDept.
of Computer Science and Media EngineeringYamanashi University4-3-11 Takeda, Kofu 400 Japan{ysuzuki@suwa, fukumot o@skyo, sokiguti?saiko}, osi.
yamanashi, ac.
jpAbst ractIn this paper, we propose keyword extractionmethod for dictation of radio news which con-sists of several domains.
In our method, news-paper articles which are automatically classifiedinto suitable domains are used in order to calcu-late feature vectors.
The feature vectors howsterm-domain terdependence and are used forselecting a suitable domain of each part of ra-dio news.
Keywords are extracted by using theselected omain.
The results of keyword extrac-tion experiments showed that our methods arerobust and effective for dictation of radio news.1 In t roduct ionRecently, many speech recognition systemsare designed for various tasks.
However, mostof them are restricted to certain tasks, for ex-ample, a tourist information and a hamburgershop.
Speech recognition systems for the taskwhich consists of various domains eems to berequired for some tasks, e.g.
a closed captionsystem for TV and a transcription system ofpublic proceedings.
In order to recognize spokendiscourse which has several domains, the speechrecognition system has to have large vocabu-lary.
Therefore, it is necessary to limit wordsearch space using linguistic restricts, e.g.
do-main identification.There have been many studies of do-main identification which used term weight-ing (J.McDonough et al, 1994; Yokoi et al,1997).
McDonough proposed a topic identifi-cation method on switch board corpus.
He re-ported that the result was best when the num-ber of words in keyword dictionary was about800.
In his method, duration of discourses ofswitch board corpora is rather long and thereare many keywords in the discourse.
However,for a short discourse, there are few keywordsin a short discourse.
Yokoi also proposed atopic identification method using co-occurrenceof words for topic identification (Yokoi et al,1997).
He classified each dictated sentence ofnews into 8 topics.
In TV or Radio news, how-ever, it is difficult to segment each sentence au-tomatically.
Sekine proposed a method for se-lecting a suitable sentence from sentences whichwere extracted by a speech recognition systemusing statistical language model (Sekine, 1996).However, if the statistical model is used for ex-traction of sentence candidates, we will obtainhigher recognition accuracy.Some initial studies of transcription ofbroad-cast news proceed (Bakis et al, 1997).
Howeverthere are some remaining problems, e.g.
speak-ing styles and domain identification.We conducted omain identification and key-word extraction experiment (Suzuki et al,1997) for radio news.
In the experiment,we classified radio news into 5 domains (i.e.accident, economy, international, politics andsports).
The problems which we faced with are;1.
Classification of newspaper articles intosuitable domains could not be performedautomatically.2.
Many incorrect keywords are extracted, be-cause the number of domains was few.In this paper, we propose a method for key-word extraction using term-domain terdepen-dence in order to cope with these two problems.The results of the experiments demonstratedthe effectiveness of our method.2 An  overv iew of  our  methodFigure 1 shows an overview of our method.Our method consists of two procedures.
In theprocedure of term-domain terdependence cal-culation, the system calculates feature vectors1272of term-domain terdependence using an ency-clopedia of current erm and newspaper articles.In the procedure of keyword extraction in radionews, firstly, the system divides radio news intosegments according to the length of pauses.
Wecall the segments units.
The domain which hasthe largest similarity between the unit of newsand the feature vector of each domain is selectedas domain of the unit.
Finally, the system ex-tracts keywords in each unit using the featurevector of selected omain which is selected bydomain identification.Explanations of~ ~ Radio Newsn en,~pediaJ Lar~icle~=i"~' j Q: : : : : : : : : : : : : : lFeature vectorscaVe)D1D7... \ [~D141Feature vectors(FeaVa) ,~...D1 \ [~ Domain identificationD7 "0".
.
.
ID3 ID7 D18 1?\ [~  Keyword ExtractionD141 * :~I President \]I \[ {, Democratic partyJlKeyword extraction Calculation of term-domaininterdependenceFigure 1: An overview of our method3 Ca lcu la t ing  feature  vectorsIn the procedure of term-domain terdepen-dence calculation, We calculate likelihood of ap-pearance of each noun in each domain.
Figure 2shows how to calculate feature vectors of term-domain interdependence.In our previous experiments, we used 5 do-mains which were sorted manually and calcu-lated 5 feature vectors for classifying domains ofeach unit of radio news and for extracting key-words.
Our previous ystem could not extractsome keywords because of many noisy keywords.In our method, newspaper articles and units ofradio news are classified into many domains.
Ateach domain, a feature vector is calculated byan encyclopedia of current erms and newspaperarticles.3.1" Sort ing newspaper  articlesaccording to the i r  domainsFirstly, all sentences in the encyclopedia areanalyzed morpheme by Chasen (Matsumoto etAn encyclopedia of current erms 141domains 10,236 explanations)?ISorting explanations \]Q Newspaper articlesabout 110,000 articles.,/?\[Separa~articles I\[ Extra~:~nouns IIE~rac~'~ n?unsl i Calculating f requ~ vectors (FreqVa) IICalculating frequency vectors (FreqVe)l._1 Calculating similarity I.
~ '~ between FeaVe and FreqVa ICalculating X values of | ' _r-LJ each noun on domains J X,,7I I Sorting articles into domains.
V .
I laccording tosimitarity I~41 feature vectors (FeaVe)~ - .
- I  .
:~Calculating x:values ofeach noun on doma ns?041 feature vectors (FeaVa)~Figure 2: Calculating feature vectorsal., 1997) and nouns which frequently appearare extracted.
A feature vector is calculated byfrequency of each noun at each domain.
Wecall the feature vector FeaVe.
Each elementof FeaVe is a X 2 value (Suzuki et al, 1997).Then, nouns are extracted from newspaper ar-ticles by a morphological nalysis ystem (Mat-sumoto et al, 1997), and frequency of each nounare counted.
Next, similarity between FeaVe ofeach domain and each newspaper article are cal-culated by using formula (1).
Finally, a suitabledomain of each newspaper article are selected byusing formula (2).Sirn(i,j) = FeaVej.
FreqVai (1)Dornainl = arg max Sim(i, j) (2)I~j~Nwhere i means a newspaper article and j meansa domain.
(.)
means operation of inner vector.3.2 Term-domain  in terdependencerepresented  by feature  vectorsFirstly, at each newspaper articles, less than5 domains whose similarities between each ar-ticle and each domain are large are selected.Then, at each selected omain, the frequencyvector is modified according to similarity valueand frequency of each noun in the article.
Forexample, If an article whose selected omainsare "political party" and "election", and simi-larity between the article and "political party"1273and similarity between the article and "elec-tion" are 100 and 60 respectively, each fre-quency vector is calculated by formula (3) andformula (4).100 FreqVm = FreqV~ + FreqVal x 1-~ (3)60 freqV~l = FreqV~z + freqVai x 1-~ (4)where i means a newspaper article.Then, we calculate feature vectors FeaVa us-ing FreqV using the method mentioned in ourprevious paper (Suzuki et al, 1997).
Each el-ement of feature vectors hows X 2 value of thedomain and wordk.
All wordk (1 < k < M :Mmeans the number of elements of a feature vec-tor) are put into the keyword ictionary.4 Keyword  ext ract ionInput news stories are represented byphoneme lattice.
There are no marks for wordboundaries in input news stories.
Phoneme lat-tices are segmented by pauses which are longerthan 0.5 second in recorded radio news.
Thesystem selects a domain of each unit which isa segmented phoneme lattice.
At each frame ofphoneme lattice, the system selects maximum20 words from keyword dictionary.4.1 Simi lar i ty between a domain  andan unitWe define the words whose X 2 values inthe feature vector of domainj are large as key-words of the domainj.
In an unit of radionews about "political party", there are manykeywords of "political party" and the X 2 valueof keywords in the feature vector of "political2 party" is large.
Therefore, sum of Xw,pol l t ica lpartytends to be large (w : a word in the unit).
In ourmethod, the system selects a word path whose2 is maximized in the word lattice sum of Xkjat domaini.
The similarity between unit/ anddomainj is calculated by formula (5).Sim(i, j) = max Sim'(i, j)all paths= max np(wordk) x Xk,15)all pathsIn formula (5), wordk is a word in theword lattice, and each selected word does notshare any frames with any other selected words.np(wordk) is the number of phonemes of wordk.2 Xk,j is x2value of wordk for domainj.The system selects a word path whoseSiml(i,j) is the largest among all word pathsfor domainj.Figure 3 shows the method of calculating sim-ilarity between unit/ and domainD1.
The sys-tem selects a word path whose Sim~(uniti, D1)is larger than those of any other word paths.phoneme lattice of uni~andidatesi -Si~unit.
DI ) =max(3.2x3+ 0.5x6,3,2x3+ 4.3x4+ 0.7?
2,3.2x3+ 4.3x4+ 4.3x3,1.2 x 3+ 0.3 x 4,--.
)Figure 3: Calculating similarity between unit/and D14.2 Domain  ident i f icat ion and keywordextractionIn the domain identification process, the sys-tem identifies each unit to a domain by formula(5).
If Sim(i,j) is larger than similarities be-tween an unit and any other domains, domainjseems to be the domain of unit~.
The system se-lects the domain which is the largest of all sim-ilarities in N of domains as the domain of theunit (formula (6)) .
The words in the selectedword path for selected omain are selected askeywords of the unit.Domaini = arg max Sim(i,j) (6)X<j<N "5 Exper iments5.1 Test dataThe test data we have used is a radio newswhich is selected from NHK 6 o'clock radio newsin August and September of 1995.
Some newsstories are hard to be classified into one do-main in radio news by human.
For evalua-tion of domain identification experiments, we1274selected news stories which two persons classi-fied into the same domains are selected.
Theunits which were used as test data are seg-mented by pauses which are longer than 0.5second.
We selected 50 units of radio news forthe experiments.
The 50 units consisted of 10units of each domain.
We used two kinds of testdata.
One is described with correct phonemesequence.
The other is written in phoneme lat-tice which is obtained by a phoneme recognitionsystem (Suzuki et al, 1993).
In each frame ofphoneme lattice, the number of phoneme candi-dates did not exceed 3.
The following equationsshow the results of phoneme recognition.the number of correct phonemes inphoneme latticethe number of uttered phonemesthe number of correct phonemes inphoneme latticephoneme segments in phoneme lattice= 95.6%= 81.2%5.2 Training dataIn order to classify newspaper articles intosmall domain, we used an encyclopedia of cur-rent terms "Chiezo"(Yamamoto, 1995).
In theencyclopedia, there are 141 domains in 9 largedomains.
There are 10,236 head-words andthose explanations in the encyclopedia.
In or-der to calculate feature vectors of domains, allexplanations in the encyclopedia are performedmorphological nalysis by Chasen (Matsumotoet al, 1997).
9,805 nouns which appeared morethan 5 times in the same domains were selectedand a feature vector of each domain was cal-culated.
Using 141 feature vectors which werecalculated in the encyclopedia, we identified o-mains of newspaper articles.
We identified o-mains of 110,000 articles of newspaper for cal-culating feature vectors automatically.
We se-lected 61,727 nouns which appeared at least 5times in the newspaper articles of same domainsand calculated 141 feature vectors.5.3 Domain  identi f icat ion exper imentThe system selects uitable domain of eachunit for keyword extraction.
Table I showsthe results of domain identification.
We con-ducted domain identification experiments u ingtwo kinds of input data, i.e.
correct phonemesequence and phoneme lattice and two kinds ofdomains, i.e.
141 domains and 9 large domains.We also compared the results and the result us-ing previous method (Suzuki et al, 1997).
Forcomparison, we selected 5 domains which areused by previous method in our method.
Inprevious method, we used a keyword ictionarywhich has 4,212 words.Table 1: The result of domain identificationnumber of Correct Phonememethod domains phoneme latticeour 141 62% 40%method 9 78% 54%5 90% 82%previous 5 86% 78%method5.4 Keyword  ext ract ion  exper imentWe have conducted keyword extraction ex-periment using the method with 141 featurevectors (our method), 5 feature vectors (pre-vious method) and without domain identifica-tion.
Table 2 shows recall and precision whichare shown in formula (7), and formula (8), re-spectively, when the input data was phonemelattice.the number of correct words inrecall = MSKPthe number of selected words in (7)MSKPthe number of correct wordsprecision = in MSKPthe number of correct nouns (8)in the unitMSKP : the most suitable keyword path for se-lected domain6 D iscuss ion6.1 Sort ing newspaper  art ic lesaccording to the i r  domainsFor using X 2 values in feature vectors, wehave good result of domain identification ofnewspaper articles.
Even if the newspaper ar-ticles which are classified into several domains,the suitable domains are selected correctly.6.2 Domain  ident i f i cat ion  o f  radio newsTable I shows that when we used 141 kinds ofdomains and phoneme lattice, 40% of units wereidentified as the most suitable domains by our1275Table 2: Recall and precision of keyword extrac-tionMethod R/Pour method R(141 domains) Pprevious method R(5 domains) Pwithout DI R. (1 domain) PCorrectphonemePhonemelattice88.5% 48.9%69.0% 38.1%80.0%63.1%77.0%60.1%24.0%33.0%12.2%9.5%R: recall P: precision Dh domain identificationmethod and shows that when we used 9 kindsof domains and phoneme lattice, 54% of unitsare identified as the most suitable domains byour method.
When the number of domains was5, the results using our method are better thanour previous experiment.
The reason is that weuse small domains.
Using small domains, thenumber of words whose X 2 values of a certaindomain are high is smaller than when large do-mains are used.For further improvement of domain identifi-cation, it is necessary to use larger newspapercorpus in order to calculate feature vectors pre-cisely and have to improve phoneme recogni-tion.6.3 Keyword  ext ract ion  o f  rad io  newsWhen we used our method to phoneme lat-tice, recall was 48.9% and precision was 38.1%.We compared the result with the result of ourprevious experiment (Suzuki et al, 1997).
Theresult of our method is better than the our pre-vious result.
The reason is that we used do-mains which are precisely classified, and we canlimit keyword search space.
However ecall was48.9% using our method.
It shows that about50% of selected keywords were incorrect words,because the system tries to find keywords forall parts of the units.
In order to raise recallvalue, the system has to use co-occurrence be-tween keywords in the most suitable keywordpath.7 ConclusionsIn this paper, we proposed keyword extrac-tion in radio news using term-domain interde-pendence.
In our method, we could obtainsorted large corpus according to domains forkeyword extraction automatically.
Using ourmethod, the number of incorrect keywords inextracted words was smaller than the previousmethod.In future, we will study how to select correctwords from extracted keywords in order to ap-ply our method for dictation of radio news.8 AcknowledgmentsThe authors would like to thank MainichiShimbun for permission to use newspaper arti-cles on CD-Mainichi Shimbun 1994 and 1995,Asahi Shimbun for permission to use the dataof the encyclopedia of current terms "Chiezo1996" and Japan Broadcasting Corporation(NHK) for permission to use radio news.
Theauthors would also like to thank the anonymousreviewers for their valuable comments.Re ferencesBaimo Bakis, Scott Chen, Ponani Gopalakrishnan,Ramesh Gopinath, Stephane Maes, and LazarosPllymenakos.
1997.
Transcription of broadcastnews - system robustness i sues and adaptationtechniques.
In Proc.
ICASSP'97, pages 711-714.J.McDonough, K.Ng, P.Jeanrenaud, H.Gish, andJ.R.Rohlicek.
1994.
Approaches to topic identifi-cation on the switchboard corpus.
In Proc.
IEEEICASSP'94, volume 1, pages 385-388.Yuji Matsumoto, Akira Kitauchi, Tatuo Yamashita,Osamu Imaichi, and Tomoaki Imamura, 1997.Japanese Morphological Analysis System ChaSenManual.
Matsumoto Lab.
Nara Institute of Sci-ence and Technology.Satoshi.
Sekine.
1996.
Modeling topic coherence forspeech recognition.
In Proc.
COLING 96, pages913-918.Yoshimi Suzuki, Chieko Furuichi, and Satoshi Imai.1993.
Spoken japanese sentence recognition us-ing dependency relationship with systematicalsemantic ategory.
Trans.
of IEICE J76 D-II,11:2264-2273.
(in Japanese).Yoshimi Suzuki, Fumiyo Fukumoto, and YoshihiroSekiguchi.
1997.
Keyword extraction of radionews using term weighting for speech recognition.In NLPRS97, pages 301-306.Shin Yamamoto, editor.
1995.
The Asahi Encyclo-pedia of Current Terms 'Chiezo'.
Asahi Shimbun.Kentaro Yokoi, Tatsuya Kawahara, and ShujiDoshita.
1997.
Topic identification of newsspeech using word cooccurrence statistics.
InTechnical Report of IEICE SP96-I05, pages 71-78.
(in Japanese).1276
