Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 128?137,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsNatural Language Comprehension with the EpiReaderAdam Trischleradam.trischlerZheng Yejeff.yeXingdi Yuaneric.yuanPhilip Bachmanphil.bachmanAlessandro Sordonialessandro.sordoniKaheer Sulemank.suleman@maluuba.comMaluuba ResearchMontr?al, Qu?bec, CanadaAbstractWe present EpiReader, a novel model for ma-chine comprehension of text.
Machine com-prehension of unstructured, real-world text is amajor research goal for natural language pro-cessing.
Current tests of machine comprehen-sion pose questions whose answers can be in-ferred from some supporting text, and evaluatea model?s response to the questions.
EpiReaderis an end-to-end neural model comprising twocomponents: the first component proposes asmall set of candidate answers after compar-ing a question to its supporting text, and thesecond component formulates hypotheses us-ing the proposed candidates and the question,then reranks the hypotheses based on their esti-mated concordance with the supporting text.We present experiments demonstrating thatEpiReader sets a new state-of-the-art on theCNN and Children?s Book Test benchmarks,outperforming previous neural models by a sig-nificant margin.1 IntroductionWhen humans reason about the world, we tend to for-mulate a variety of hypotheses and counterfactuals,then test them in turn by physical or thought exper-iments.
The philosopher Epicurus first formalizedthis idea in his Principle of Multiple Explanations: ifseveral theories are consistent with the observed data,retain them all until more data is observed.
In this pa-per, we argue that the same principle can be appliedto machine comprehension of natural language.
Wepropose a deep neural comprehension model, trainedend-to-end, that we call EpiReader.Comprehension of natural language by machines,at a near-human level, is a prerequisite for an ex-tremely broad class of useful applications of artificialintelligence.
Indeed, most human knowledge is col-lected in the natural language of text.
Machine com-prehension (MC) has therefore garnered significantattention from the machine learning research commu-nity.
Machine comprehension is typically evaluatedby posing a set of questions based on a supportingtext passage, then scoring a system?s answers to thosequestions.
Such tests are objectively gradable andmay assess a range of abilities, from basic understand-ing to causal reasoning to inference (Richardson etal., 2013).In the past year, two large-scale MC datasets havebeen released: the CNN/Daily Mail corpus, consist-ing of news articles from those outlets (Hermannet al, 2015), and the Children?s Book Test (CBT),consisting of short excerpts from books availablethrough Project Gutenberg (Hill et al, 2016).
Thesize of these datasets (on the order of 105 distinctquestions) makes them amenable to data-intensivedeep learning techniques.
Both corpora use Cloze-style questions (Taylor, 1953), which are formulatedby replacing a word or phrase in a given sentencewith a placeholder token.
The task is then to find theanswer that ?fills in the blank?.In tandem with these corpora, a host of neu-ral machine comprehension models has been devel-oped (Weston et al, 2015b; Hermann et al, 2015;Hill et al, 2016; Kadlec et al, 2016; Chen et al,2016).
We compare EpiReader to these earlier mod-els through training and evaluation on the CNN and128CBT datasets.1EpiReader factors into two components.
The firstcomponent extracts a small set of potential answersbased on a shallow comparison of the question withits supporting text; we call this the Extractor.
The sec-ond component reranks the proposed answers basedon deeper semantic comparisons with the text; wecall this the Reasoner.
We can summarize this pro-cess as Extract ?
Hypothesize ?
Test2.
The se-mantic comparisons implemented by the Reasonerare based on the concept of recognizing textual en-tailment (RTE) (Dagan et al, 2006), also known asnatural language inference.
This process is computa-tionally demanding.
Thus, the Extractor serves theimportant function of filtering a large set of poten-tial answers down to a small, tractable set of likelycandidates for more thorough testing.
The two-stageprocess is an analogue of structured prediction cas-cades (Weiss and Taskar, 2010), wherein a sequenceof increasingly complex models progressively filtersthe output space in order to trade off between modelcomplexity and limited computational resources.
Wedemonstrate that this cascade-like framework is appli-cable to machine comprehension and can be trainedend-to-end with stochastic gradient descent.The Extractor follows the form of a pointer net-work (Vinyals et al, 2015), and uses a differentiableattention mechanism to indicate words in the textthat potentially answer the question.
This approachwas used (on its own) for question answering withthe Attention Sum Reader (Kadlec et al, 2016).
TheExtractor outputs a small set of answer candidatesalong with their estimated probabilities of correct-ness.
The Reasoner forms hypotheses by insertingthe candidate answers into the question, then esti-mates the concordance of each hypothesis with eachsentence in the supporting text.
We use these esti-mates as a measure of the evidence for a hypothesis,and aggregate evidence over all sentences.
In theend, we combine the Reasoner?s evidence with theExtractor?s probability estimates to produce a finalranking of the answer candidates.1The CNN and Daily Mail datasets were released togetherand have the same form.
The Daily Mail dataset is signifi-cantly larger; therefore, models consistently score higher whentrained/tested on it.2The Extractor performs extraction, while the Reasoner bothhypothesizes and tests.This paper is organized as follows.
In Section 2we formally define the problem to be solved and givesome background on the datasets used in our tests.In Section 3 we describe EpiReader, focusing on itstwo components and how they combine.
Section 4discusses related work, and Section 5 details ourexperimental results and analysis.
We conclude inSection 6.2 Problem definition, notation, datasetsEpiReader?s task is to answer a Cloze-style questionby reading and comprehending a supporting passageof text.
The training and evaluation data consist oftuples (Q, T , a?, A), where Q is the question (a se-quence of words {q1, ...q|Q|}), T is the text (a se-quence of words {t1, ..., t|T |}), A is a set of possibleanswers {a1, ..., a|A|}, and a?
?
A is the correct an-swer.
All words come from a vocabulary V , andA ?
T .
In each question, there is a placeholdertoken indicating the missing word to be filled in.2.1 DatasetsCNN This corpus is built using articles scrapedfrom the CNN website.
The articles themselves formthe text passages, and questions are generated syn-thetically from short summary statements that ac-company each article.
These summary points are(presumably) written by human authors.
Each ques-tion is created by replacing a named entity in a sum-mary point with a placeholder token.
All namedentities in the articles and questions are replaced withanonymized tokens that are shuffled for each (Q, T )pair.
This forces the model to rely only on the text,rather than learning world knowledge about the en-tities during training.
The CNN corpus (henceforthCNN) was presented by Hermann et al (2015).Children?s Book Test This corpus is constructedsimilarly to CNN, but from children?s books avail-able through Project Gutenberg.
Rather than articles,the text passages come from book excerpts of 20sentences.
Since no summaries are provided, a ques-tion is generated by replacing a single word in thenext (i.e.
21st) sentence.
The corpus distinguishesquestions based on the type of word that is replaced:named entity, common noun, verb, or preposition.Like Kadlec et al (2016), we focus only on the firsttwo classes since Hill et al (2016) showed that stan-129dard LSTM language models already achieve human-level performance on the latter two.
Unlike in theCNN corpora, named entities are not anonymizedand shuffled in the Children?s Book Test (CBT).
CBTwas presented by Hill et al (2016).The different methods of construction for ques-tions in each corpus mean that CNN and CBT assessdifferent aspects of comprehension.
The summarypoints of CNN are a condensed paraphrasing of infor-mation in the text; thus, determining the correct an-swer relies mostly on recognizing textual entailment.On the other hand, CBT is about story prediction.
Itis a comprehension task insofar as comprehension islikely necessary for story prediction, but comprehen-sion alone may not be sufficient.
Indeed, there aresome CBT questions that are unanswerable given thepreceding context.3 EpiReader3.1 Overview and intuitionEpiReader explicitly leverages the observation thatthe answer to a question is often a word or phrasefrom the related text passage.
This condition holdsfor the CNN and CBT datasets.
EpiReader?s firstmodule, the Extractor, can thus select a small set ofcandidate answers by pointing to their locations inthe supporting passage.
This mechanism is detailedin Section 3.2, and was used previously by the At-tention Sum Reader (Kadlec et al, 2016).
Pointingto candidate answers removes the need to apply asoftmax over the entire vocabulary as in Weston et al(2015b), which is computationally more costly anduses less-direct information about the context of apredicted answer in the supporting text.EpiReader?s second module, the Reasoner, beginsby formulating hypotheses using the extracted answercandidates.
It generates each hypothesis by replacingthe placeholder token in the question with an answercandidate.
Cloze-style questions are ideally-suitedto this process, because inserting the correct answerat the placeholder location produces a well-formed,grammatical statement.
Thus, the correct hypothesiswill ?make sense?
to a language model.The Reasoner then tests each hypothesis individu-ally.
It compares a hypothesis to the text, split intosentences, to measure textual entailment, and then ag-gregates entailment over all sentences.
This compu-tation uses a pair of convolutional encoder networksfollowed by a recurrent neural network.
The convo-lutional encoders generate abstract representations ofthe hypothesis and each text sentence; the recurrentnetwork estimates and aggregates entailment.
Thisis described formally in Section 3.3.
The end-to-end EpiReader model, combining the Extractor andReasoner modules, is depicted in Figure 1.Throughout our model, words will be representedwith trainable embeddings (Bengio et al, 2000).
Werepresent these embeddings using a matrix W ?RD?|V |, where D is the embedding dimension and|V | is the vocabulary size.3.2 The ExtractorThe Extractor is a Pointer Network (Vinyals et al,2015).
It uses a pair of bidirectional recurrent neuralnetworks, f(?T ,T) and g(?Q,Q), to encode the textpassage and the question.
?T represents the param-eters of the text encoder, and T ?
RD?N is a ma-trix representation of the text (comprising N words),whose columns are individual word embeddings ti.Likewise, ?Q represents the parameters of the ques-tion encoder, and Q ?
RD?NQ is a matrix represen-tation of the question (comprisingNQ words), whosecolumns are individual word embeddings qj .We use a recurrent neural network with gated recur-rent units (GRU) (Bahdanau et al, 2015) to scan overthe columns (i.e.
word embeddings) of the input ma-trix.
We selected the GRU because it is computation-ally simpler than Long Short-Term Memory (Hochre-iter and Schmidhuber, 1997), while still avoidingthe problem of vanishing/exploding gradients oftenencountered when training recurrent networks.The GRU?s hidden state gives a representation ofthe ith word conditioned on preceding words.
Toinclude context from proceeding words, we run asecond GRU over T in the reverse direction.
Werefer to the combination as a biGRU.
At each step thebiGRU outputs two d-dimensional encoding vectors,one for the forward direction and one for the back-ward direction.
We concatenate these to yield a vectorf(ti) ?
R2d.
The question biGRU is similar, but weform a single-vector representation of the questionby concatenating the final forward state with the finalbackward state, which we denote g(Q) ?
R2d.As in Kadlec et al (2016), we model the probabil-ity that the ith word in text T answers question Q130}{{ }biGRUEmbeddingsX was Sam?s best friendComparisonTop candidates/probabilitiesPassageQuestion ErnieJamesElmo?
?biGRUEmbeddingsWord encodingsQuestion encodingSam and James played all day.?
?It was a beautiful morning.They lived happily ever after.p1p2pkConvolutional NetworkJames was Sam?s best friendHypothesesGRUTop candidates}{ ErnieJamesElmoSam and James played all day.It was a beautiful morning.They lived happily ever after.?
?EmbeddingsPassageEmbeddingsHypothesis encodingSentence encodings?
?j1nsConvolutional NetworkekEntailment?kScoreExtractorReasonerWord-match scores1jns?Figure 1: The complete EpiReader framework.
The Extractor is above, the Reasoner below.
Propagating the Extractor?s probabilityestimates forward and combining them with the Reasoner?s entailment estimates renders the model end-to-end differentiable.usingsi ?
exp(f(ti) ?
g(Q)), (1)which takes the inner product of the text and questionrepresentations followed by a softmax.
In many casesunique words repeat in a text.
Therefore, we computethe total probability that word w is the correct answerusing a sum:P (w | T ,Q) =?i: ti=wsi.
(2)This probability is evaluated for each unique word inT .
Finally, the Extractor outputs the set {p1, ..., pK}of the K highest word probabilities from 2, alongwith the corresponding set of K most probable an-swer words {a?1, ..., a?K}.3.3 The ReasonerThe indicial selection involved in gathering{a?1, ..., a?K}, which is equivalent to a K-bestargmax, is not a continuous function of its inputs.To construct an end-to-end differentiable model, webypass this by propagating the probability estimatesof the Extractor directly through the Reasoner.The Reasoner begins by inserting the answer can-didates, which are single words or phrases, into thequestion sequence Q at the placeholder location.This forms K hypotheses {H1, ...,HK}.
At this131point, we consider each hypothesis to have proba-bility p(Hk) ?
pk, as estimated by the Extractor.The Reasoner updates and refines this estimate.The hypotheses represent new information in somesense?they are statements we have constructed, al-beit from words already present in the question andtext passage.
The Reasoner estimates entailment be-tween the statements Hk and the passage T .
Wedenote these estimates using ek = F (Hk, T ), withF to be defined.
We start by reorganizing T intoa sequence of Ns sentences: T = {t1, .
.
.
, tN} ?
{S1, .
.
.
,SNs}, where Si is a sequence of words.For each hypothesis and each sentence of thetext, Reasoner input consists of two matrices: Si ?RD?|Si|, whose columns are the embedding vectorsfor each word of sentence Si, and Hk ?
RD?|Hk|,whose columns are the embedding vectors for eachword in the hypothesisHk.
The embedding vectorsthemselves come from matrix W, as before.These matrices feed into a convolutional architec-ture based on that of Severyn and Moschitti (2016).The architecture first augments Si with matrix M ?R2?|Si|.
The first row of M contains the inner prod-uct of each word embedding in the sentence with thecandidate answer embedding, and the second rowcontains the maximum inner product of each sen-tence word embedding with any word embedding inthe question.
These word-matching features wereinspired by similar approaches in Wang and Jiang(2016) and Trischler et al (2016), where they wereshown to improve entailment estimates.The augmented Si is then convolved with a bankof filters FS ?
R(D+2)?m, while Hk is convolvedwith filters FH ?
RD?m, where m is the convolu-tional filter width.
We add a bias term and apply anonlinearity (we use a ReLU) following the convo-lution.
Maxpooling over the sequences then yieldstwo vectors: the representation of the text sentence,rSi ?
RNF , and the representation of the hypothesis,rHk ?
RNF , where NF is the number of filters.We then compute a scalar similarity score betweenthese vector representations using the bilinear form?
= rTSiRrHk , (3)where R ?
RNF?NF is a matrix of trainable parame-ters.
We then concatenate the similarity score withthe sentence and hypothesis representations to get avector, xik = [?
; rSi ; rHk ]T .
There are more pow-erful models of textual entailment that could havebeen used in place of this convolutional architecture.We adopted the approach of Severyn and Moschitti(2016) for computational efficiency.The resulting sequence of Ns vectors feeds intoyet another GRU for synthesis, of hidden dimensiondS .
Intuitively, it is often the case that evidencefor a particular hypothesis is distributed over severalsentences.
For instance, if we hypothesize that thefootball is in the park, perhaps it is because one sen-tence tells us that Sam picked up the football and alater one tells us that Sam ran to the park.3 The Rea-soner synthesizes distributed information by runninga GRU network over xik, where i indexes sentencesand represents the step dimension.4 The final hiddenstate of the GRU is fed through a fully-connectedlayer, yielding a single scalar yk.
This value repre-sents the collected evidence forHk based on the text.In practice, the Reasoner processes all K hypothesesin parallel and the estimated entailment of each isnormalized by a softmax, ek ?
exp(yk).As pointed out in Kadlec et al (2016), it is astrength of the pointer framework that it does notblend the representations that are being attended.Contrast this with typical attention mechanismswhere such a blended representation is used down-stream to make similarity comparisons with, e.g.,output vectors.Differentiable attention mechanisms (as in Bah-danau et al (2015), for example) typically blend in-ternal representations together through a weightedsum, then use this ?blend?
downstream for similaritycomparisons.
The pointer framework does not resortto this blending; Kadlec et al (2016) explain that thisis an advantage, since in comprehension tasks thegoal is to select the correct answer among semanti-cally similar candidates and more exact matching isnecessary.
The reranking function performed by theReasoner entails this advantage, by examining theseparate hypotheses individually without blending.3This example is characteristic of the bAbI dataset (Westonet al, 2015a).4Note a benefit of forming the hypothesis: it renders bidirec-tional aggregation unnecessary, since knowing both the questionand the putative answer "closes the loop" the same way that abidirectional encoding would.1323.4 Combining componentsFinally, we combine the evidence from the Reasonerwith the probability from the Extractor.
We com-pute the output probability of each hypothesis, pik,according to the productpik ?
ekpk, (4)whereby the evidence of the Reasoner can be inter-preted as a correction to the Extractor probabilities,applied as an additive shift in log-space.
We experi-mented with other combinations of the Extractor andReasoner, but we found the multiplicative approachto yield the best performance.After combining results from the Extractor andReasoner to get the probabilities pik described inEq.
4, we optimize the parameters of the fullEpiReader to minimize a cost comprising two terms,LE and LR.
The first term is a standard negative log-likelihood objective, which encourages the Extractorto rate the correct answer above other answers.
Thisis the same loss term used in Kadlec et al (2016).
Itis given by:LE = E(Q,T ,a?,A)[?
logP (a?
| T ,Q)] , (5)where P (a?
| T ,Q) is as defined in Eq.
2, and a?denotes the true answer.
The second term is a margin-based loss on the end-to-end probabilities pik.
Wedefine pi?
as the probability pik corresponding to thetrue answer word a?.
This term is given by:LR = E(Q,T ,a?,A)??
?a?i?{a?1,...,a?K}\a?[?
?
pi?
+ pia?i ]+??
,(6)where ?
is a margin hyperparameter, {a?1, ..., a?K}is the set of K answers proposed by the Extractor,and [x]+ indicates truncating x to be non-negative.Intuitively, this loss says that we want the end-to-endprobability pi?
for the correct answer to be at least ?larger than the probability pia?i for any other answerproposed by the Extractor.
During training, the cor-rect answer is occasionally missed by the Extractor,especially in early epochs.
We counter this issue byforcing the correct answer into the top K set whiletraining.
When evaluating the model on validationand test examples we rely fully on the top K answersproposed by the Extractor.To get the final loss term LER, minus `2 regular-ization terms on the model parameters, we take aweighted combination of LE and LR:LER = LE + ?LR, (7)where ?
is a hyperparameter for weighting the rela-tive contribution of the Extractor and Reasoner losses.In practice, we found that ?
should be fairly large(e.g., 10 < ?
< 100).
Empirically, we observedthat the output probabilities from the Extractor of-ten peak and saturate the first softmax; hence, theExtractor term can come to dominate the Reasonerterm without the weight ?
(we discuss the Extractor?spropensity to overfit in Section 5).4 Related WorkThe Impatient and Attentive Reader models wereproposed by Hermann et al (2015).
The AttentiveReader applies bidirectional recurrent encoders to thequestion and supporting text.
It then uses the atten-tion mechanism described in Bahdanau et al (2015)to compute a fixed-length representation of the textbased on a weighted sum of the text encoder?s output,guided by comparing the question representation toeach location in the text.
Finally, a joint representa-tion of the question and supporting text is formed bypassing their separate representations through a feed-forward MLP and an answer is selected by comparingthe MLP output to a representation of each possibleanswer.
The Impatient Reader operates similarly, butcomputes attention over the text after processing eachconsecutive word of the question.
The two modelsachieved similar performance on the CNN and DailyMail datasets.Memory Networks were first proposed by Westonet al (2015b) and later applied to machine compre-hension by Hill et al (2016).
This model buildsfixed-length representations of the question and ofwindows of text surrounding each candidate answer,then uses a weighted-sum attention mechanism tocombine the window representations.
As in the previ-ous Readers, the combined window representation isthen compared with each possible answer to form aprediction about the best answer.
What distinguishesMemory Networks is how they construct the ques-tion and text window representations.
Rather thana recurrent network, they use a specially-designed,trainable transformation of the word embeddings.133Most of the details for the very recent AS Readerare provided in the description of our Extractor mod-ule in Section 3.2, so we do not summarize it furtherhere.
This model (Kadlec et al, 2016) set the previ-ous state-of-the-art on the CBT dataset.During the write-up of this paper, another very re-cent model came to our attention.
Chen et al (2016)propose using a bilinear term instead of a tanh layerto compute the attention between question and pas-sage words, and also uses the attended word encod-ings for direct, pointer-style prediction as in Kadlecet al (2016).
This model set the previous state-of-the-art on the CNN dataset.
However, this model usedembedding vectors pretrained on a large external cor-pus (Pennington et al, 2014).EpiReader borrows ideas from other models aswell.
The Reasoner?s convolutional architecture isbased on Severyn and Moschitti (2016) and Blunsomet al (2014).
Our use of word-level matching was in-spired by the Parallel-Hierarchical model of Trischleret al (2016) and the natural language inference modelof Wang and Jiang (2016).
Finally, the idea of formu-lating and testing hypotheses for question-answeringwas used to great effect in IBM?s DeepQA systemfor Jeopardy!
(Ferrucci et al, 2010) (although thatwas a more traditional information retrieval pipelinerather than an end-to-end neural model), and alsoresembles the framework of structured predictioncascades (Weiss and Taskar, 2010).5 Evaluation5.1 Implementation and training detailsTo train our model we used stochastic gradient de-scent with the ADAM optimizer (Kingma and Ba,2015), with an initial learning rate of 0.001.
Theword embeddings were initialized randomly, draw-ing from the uniform distribution over [?0.05, 0.05).We used batches of 32 examples, and early stoppingwith a patience of 2 epochs.
Our model was imple-mented in Theano (Bergstra et al, 2010) using theKeras framework (Chollet, 2015).The results presented below for EpiReader wereobtained by searching over a small grid of hyperpa-rameter settings.
We selected the model that, on eachdataset, maximized accuracy on the validation set,then evaluated it on the test set.
We record the bestsettings for each dataset in Table 1.
As has beenTable 1: Hyperparameter settings for best EpiReaders.
D isthe embedding dimension, d is the hidden dimension in theExtractor GRUs, K is the number of candidates to consider, mis the filter width, NF is the number of filters, and dS is thehidden dimension in the Reasoner GRU.HyperparametersDataset D d K m NF dSCBT-NE 300 128 5 3 16 32CBT-CN 300 128 5 3 32 32CNN 384 256 10 3 32 32done previously, we train separate models on CBT?snamed entity (CBT-NE) and common noun (CBT-CN) splits.
All our models used `2-regularizationat 0.001, ?
= 50, and ?
= 0.04.
We did not usedropout but plan to investigate its effect in the future.Hill et al (2016) and Kadlec et al (2016) also presentresults for ensembles of their models.
Time did notpermit us to generate an ensemble of EpiReaders onthe CNN dataset so we omit those measures; how-ever, EpiReader ensembles (of seven models) demon-strated improved performance on the CBT dataset.5.2 ResultsIn Table 5.2, we compare the performance ofEpiReader against that of several baselines, on thevalidation and test sets of the CBT and CNN corpora.We measure EpiReader performance at the outputof both the Extractor and the Reasoner.
EpiReaderachieves state-of-the-art performance across theboard for both datasets.
On CNN, we score 2.2%higher on test than the best previous model of Chenet al (2016).
Interestingly, an analysis of the CNNdataset by Chen et al (2016) suggests that approxi-mately 25% of the test examples contain coreferenceerrors or questions which are ?ambiguous/hard?
evenfor a human analyst.
If this estimate is accurate, thenEpiReader, achieving an absolute test accuracy of74.0%, is operating close to expected human perfor-mance.
On the other hand, ambiguity is unlikely tobe distributed evenly over entities, so a good modelshould be able to perform at better-than-chance levelseven on questions where the correct answer is uncer-tain.
If, on the 25% of ?noisy?
questions, the modelcan shift its hit rate from, e.g., 1/10 to 1/3, then thereis still a fair amount of performance to gain.134CBT-NE CBT-CNModel valid test valid testHumans (context + query) 1 - 81.6 - 81.6LSTMs (context + query) 1 51.2 41.8 62.6 56.0MemNNs 1 70.4 66.6 64.2 63.0AS Reader 2 73.8 68.6 68.8 63.4EpiReader Extractor 73.2 69.4 69.9 66.7EpiReader 75.3 69.7 71.5 67.4AS Reader (ensemble) 2 74.5 70.6 71.1 68.9EpiReader (ensemble) 76.6 71.8 73.6 70.6CNNModel valid testDeep LSTM Reader 3 55.0 57.0Attentive Reader 3 61.6 63.0Impatient Reader 3 61.8 63.8MemNNs 1 63.4 66.8AS Reader 2 68.6 69.5Stanford AR 4 72.4 72.4EpiReader Extractor 71.8 72.0EpiReader 73.4 74.0Table 2: Model comparison on the CBT and CNN datasets.
Results marked with 1 are from Hill et al (2016), with 2 are fromKadlec et al (2016), with 3 are from Hermann et al (2015), and with 4 are from Chen et al (2016).Ablated component Validation accuracy (%)- 71.5Word-match scores 70.3Bilinear similarity 70.0Reasoner 68.7Convolutional encoders 71.0Table 3: Ablation study on CBT-CN validation set.On CBT-CN our single model scores 4.0% higherthan the previous best of the AS Reader.
The improve-ment on CBT-NE is more modest at 1.1%.
Lookingmore closely at our CBT-NE results, we found thatthe validation and test accuracies had relatively highvariance even in late epochs of training.
We discov-ered that many of the validation and test questionswere asked about the same named entity, which mayexplain this issue.5.3 AnalysisWe measure the contribution of several componentsof the Reasoner by ablating them.
Results on thevalidation set of CBT-CN are presented in Table 3.The word-match scores (cosine similarities stored inthe first two rows of matrix M, see Section 3.3) makea contribution of 1.2% to the validation performance,indicating that they are useful.
Similarly, the bilinearsimilarity score ?
, which is passed to the final GRUnetwork, contributes 1.5%.Removing the Reasoner altogether reduces ourmodel to the AS Reader, whose results we havereproduced to within negligible difference.
Asidefrom achieving state-of-the-art results at its final out-put, the EpiReader framework gives a boost to itsExtractor component through the joint training pro-cess.
This can be seen by referring back to Table 5.2,wherein we also provide accuracy scores evaluatedat the output of the Extractor.
These are all higherthan the analogous scores reported for the AS Reader.Based on our own work with that model, we foundit to overfit the training set rapidly and significantly,achieving training accuracy scores upwards of 98%after only 2 epochs.
We suspect that the Reasonermodule had a regularizing effect on the Extractor, butleave the confirmation for future work.Although not exactly an ablation, we also triedbypassing the Reasoner?s convolutional encoders al-together, along with the word-match scores and thebilinear similarity.
This was done as follows: fromthe Extractor, we pass to the Reasoner?s final GRU (i)the bidirectional hidden representation of the ques-tion; (ii) the bidirectional hidden representations ofthe end of each story sentence (recall that the Rea-soner operates on sentence representations).
Thus,we reuse (parts of) the original biGRU encodings.This cuts down on the number of model parametersand on the length of the graph through which gra-dients must flow, potentially providing a strongerlearning signal to the initial encoders.
We found thatthis change yielded a relatively small reduction in per-formance on CBT-CN, perhaps for the reasons justdiscussed?only 0.5%, as given in the final line of135Mr.
Blacksnake grinned and started after him, not very fast because he knew that he wouldn't have to run very fast to catch old Mr. Toad, and he thought the exercise would do him good.?
?Still, the green meadows wouldn't be quite the same without old Mr. Toad.
I should miss him if anything happened to him.
I suppose it would be partly my fault, too, for if I hadn't pulled over that piece of bark, he probably would have stayed there the rest of the day and been safe.
?QUESTION: ?Maybe he won't meet Mr. XXXXX,?
said a little voice inside of Jimmy.EXTRACTOR: Toad REASONER: Blacksnake1.18.21.19.20.Figure 2: An abridged example from CBT-NE demonstratingcorrective reranking by the Reasoner.Table 3.
This suggests that competitive performancemay be achieved with other, simpler architectures forthe Reasoner?s entailment system and this will be thesubject of future research.An analysis by Kadlec et al (2016) indicates thatthe trained AS Reader includes the correct answeramong its five most probable candidates on approxi-mately 95% of test examples for both datasets.
Weverified that our Extractor achieved a similar rate,and of course this is vital for performance of the fullsystem, since the Reasoner cannot recover when thecorrect answer is not among its inputs.Our results show that the Reasoner often correctserroneous answers from the Extractor.
Figure 2 givesan example of this correction.
In the text passage,from CBT-NE, Mr. Blacksnake is pursuing Mr. Toad,presumably to eat him.
The dialogue in the questionsentence refers to both: Mr. Toad is its subject, re-ferred to by the pronoun ?he?, and Mr. Blacksnake isits object.
In the preceding sentences, it is clear (toa human) that Jimmy is worried about Mr. Toad andhis potential encounter with Mr. Blacksnake.
TheExtractor, however, points most strongly to ?Toad?,possibly because he has been referred to most re-cently.
The Reasoner corrects this error and selects?Blacksnake?
as the answer.
This relies on a deeperunderstanding of the text.
The named entity can, inthis case, be inferred through an alternation of theentities most recently referred to.
This kind alterna-tion is typical of dialogues, when two actors interactin turns.
The Reasoner can capture this behaviorbecause it examines sentences in sequence.6 ConclusionWe presented the novel EpiReader framework formachine comprehension and evaluated it on twolarge, complex datasets: CNN and CBT.
Our modelachieves state-of-the-art results on these corpora, out-performing all previous approaches.
In future work,we plan to test our framework with alternative modelsfor natural language inference (e.g., Wang and Jiang(2016)), and explore the effect of pretraining such amodel specifically on an inference task.As a general framework that consists in a two-stage cascade, EpiReader can be implemented using avariety of mechanisms in the Extractor and Reasonerstages.
We have demonstrated that this cascade-likeframework is applicable to machine comprehensionand can be trained end-to-end.
As more powerfulmachine comprehension models inevitably emerge,it may be straightforward to boost their performanceusing EpiReader?s structure.References[Bahdanau et al2015] Dzmitry Bahdanau, KyunghyunCho, and Yoshua Bengio.
2015.
Neural machine trans-lation by jointly learning to align and translate.
ICLR.
[Bengio et al2000] Yoshua Bengio, R?jean Ducharme,and Pascal Vincent.
2000.
A neural probabilistic lan-guage model.
In Advances in Neural Information Pro-cessing Systems, pages 932?938.
[Bergstra et al2010] J. Bergstra, O. Breuleux, F. Bastien,P.
Lamblin, R. Pascanu, G. Desjardins, J. Turian,D.
Warde-Farley, and Y. Bengio.
2010.
Theano: aCPU and GPU math expression compiler.
In In Proc.of SciPy.
[Blunsom et al2014] Phil Blunsom, Edward Grefenstette,and Nal Kalchbrenner.
2014.
A convolutional neuralnetwork for modelling sentences.
[Chen et al2016] Danqi Chen, Jason Bolton, and Christo-pher D. Manning.
2016.
A thorough examination ofthe cnn / daily mail reading comprehension task.
InAssociation for Computational Linguistics (ACL).
[Chollet2015] Fran?ois Chollet.
2015. keras.
https://github.com/fchollet/keras.
[Dagan et al2006] Ido Dagan, Oren Glickman, andBernardo Magnini.
2006.
The pascal recognising tex-tual entailment challenge.
In Machine learning chal-lenges.
evaluating predictive uncertainty, visual ob-ject classification, and recognising textual entailment,pages 177?190.
Springer.136[Ferrucci et al2010] David Ferrucci, Eric Brown, JenniferChu-Carroll, James Fan, David Gondek, Aditya AKalyanpur, Adam Lally, J William Murdock, Eric Ny-berg, John Prager, et al 2010.
Building watson: Anoverview of the deepqa project.
AI magazine, 31(3):59?79.
[Hermann et al2015] Karl Moritz Hermann, Tomas Ko-cisky, Edward Grefenstette, Lasse Espeholt, Will Kay,Mustafa Suleyman, and Phil Blunsom.
2015.
Teachingmachines to read and comprehend.
In Advances in Neu-ral Information Processing Systems, pages 1684?1692.
[Hill et al2016] Felix Hill, Antoine Bordes, Sumit Chopra,and Jason Weston.
2016.
The goldilocks principle:Reading children?s books with explicit memory repre-sentations.
ICLR.
[Hochreiter and Schmidhuber1997] Sepp Hochreiter andJ?rgen Schmidhuber.
1997.
Long short-term memory.Neural computation, 9(8):1735?1780.
[Kadlec et al2016] Rudolf Kadlec, Martin Schmid, On-drej Bajgar, and Jan Kleindienst.
2016.
Text under-standing with the attention sum reader network.
arXivpreprint arXiv:1603.01547.
[Kingma and Ba2015] Diederik Kingma and Jimmy Ba.2015.
Adam: A method for stochastic optimization.ICLR.
[Pennington et al2014] Jeffrey Pennington, RichardSocher, and Christopher D Manning.
2014.
GloVe:Global vectors for word representation.
Proc.
EMNLP,12.
[Richardson et al2013] Matthew Richardson, Christo-pher JC Burges, and Erin Renshaw.
2013.
Mctest:A challenge dataset for the open-domain machine com-prehension of text.
In EMNLP, volume 1, page 2.
[Severyn and Moschitti2016] Aliaksei Severyn andAlessandro Moschitti.
2016.
Modeling relational in-formation in question-answer pairs with convolutionalneural networks.
arXiv preprint arXiv:1604.01178.
[Taylor1953] Wilson L Taylor.
1953.
Cloze procedure:a new tool for measuring readability.
Journalism andMass Communication Quarterly, 30.
[Trischler et al2016] Adam Trischler, Zheng Ye, XingdiYuan, Jing He, Philip Bachman, and Kaheer Suleman.2016.
A parallel-hierarchical model for machine com-prehension on sparse data.
In Proceedings of the 54thAnnual Meeting of the Association for ComputationalLinguistics.
[Vinyals et al2015] Oriol Vinyals, Meire Fortunato, andNavdeep Jaitly.
2015.
Pointer networks.
In Advancesin Neural Information Processing Systems, pages 2674?2682.
[Wang and Jiang2016] Shuohang Wang and Jing Jiang.2016.
Learning natural language inference with lstm.NAACL.
[Weiss and Taskar2010] David J Weiss and BenjaminTaskar.
2010.
Structured prediction cascades.
In AIS-TATS, pages 916?923.
[Weston et al2015a] Jason Weston, Antoine Bordes,Sumit Chopra, and Tomas Mikolov.
2015a.
Towardsai-complete question answering: A set of prerequisitetoy tasks.
arXiv preprint arXiv:1502.05698.
[Weston et al2015b] Jason Weston, Sumit Chopra, andAntoine Bordes.
2015b.
Memory networks.
ICLR.137
