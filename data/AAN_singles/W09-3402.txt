Proceedings of the 7th Workshop on Asian Language Resources, ACL-IJCNLP 2009, pages 9?16,Suntec, Singapore, 6-7 August 2009. c?2009 ACL and AFNLPAn Empirical Study of Vietnamese Noun Phrase Chunking withDiscriminative Sequence ModelsLe Minh NguyenSchool of Information Science, JAISTnguyenml@jaist.ac.jpHuong Thao Nguyen and Phuong Thai NguyenCollege of Technology, VNU{thaonth, thainp}@vnu.edu.vnTu Bao Ho and Akira ShimazuJapan Advanced Institute of Science and Technology{bao,shimazu}@jaist.ac.jpAbstractThis paper presents an empirical workfor Vietnamese NP chunking task.
Weshow how to build an annotation corpus ofNP chunking and how discriminative se-quence models are trained using the cor-pus.
Experiment results using 5 fold crossvalidation test show that discriminative se-quence learning are well suitable for Viet-namese chunking.
In addition, by em-pirical experiments we show that the partof speech information contribute signifi-cantly to the performance of there learningmodels.1 IntroductionMany Natural Language Processing applications(i.e machine translation) require syntactic infor-mation and tools for syntactic analysis.
However,these linguistic resources are only available forsome languages(i.e English, Japanese, Chines).
Inthe case of Vietnamese, currently most researchershave focused on word segmentation and part ofspeech tagging.
For example, Nghiem et al(Nghiem, Dinh, Nguyen, 2008) has developed aVietnamese POS tagging.
Tu (Tu, Phan, Nguyen,Ha, 2006) (Nguyen, Romary, Rossignol, Vu,2006)(Dien, Thuy, 2006) have developed Viet-namese word segmentation.The processing of building tools and annotateddata for other fundamental tasks such as chunk-ing and syntactic parsing are currently developed.This can be viewed as a bottleneck for develop-ing NLP applications that require a deeper under-standing of the language.
The requirement of de-veloping such tools motives us to develop a Viet-namese chunking tool.
For this goal, we havebeen looking for an annotation corpus for conduct-ing a Vietnamese chunking using machine learn-ing methods.
Unfortunately, at the moment, thereis still no common standard annotated corpus forevaluation and comparison regarding Vietnamesechunking.In this paper, we aim at discussing on howwe can build annotated data for Vietnamese textchunking and how to apply discriminative se-quence learning for Vietnamese text chunking.
Wechoose discriminative sequence models for Viet-namese text chunking because they have shownvery suitable methods for several languages(i.eEnglish, Japanese, Chinese) (Sha and Pereira,2005)(Chen, Zhang, and Ishihara, 2006) (Kudoand Matsumoto, 2001).
These presentative dis-criminative models which we choose for conduct-ing empirical experiments including: ConditionalRandom Fields (Lafferty, McCallum, and Pereira,2001), Support Vector Machine (Vapnik, 1995)and Online Prediction (Crammer et al 2006).
Inother words, because Noun Phrase chunks appearmost frequently in sentences.
So, in this paperwe focus mainly on empirical experiments for thetasks of Vietnamese NP chunking.We plan to answer several major questions byusing empirical experiments as follows.?
Whether or not the discriminative learningmodels are suitable for Vietnamese chunkingproblem??
We want to know the difference of SVM,Online Learning, and Conditional RandomFields for Vietnamese chunking task.?
Which features are suitable for discriminativelearning models and how they contribute tothe performance of Vietnamese text chunk-ing?The rest of this paper is organized as follows:Section 2 describes Vietnamese text chunking withdiscriminative sequence learning models.
Section3 shows experimental results and Section 4 dis-9cusses the advantage of our method and describesfuture work.2 Vietnamese NP Chunking withDiscriminative Sequence LearningNoun Phrase chunking is considered as the taskof grouping a consecutive sequence of words intoa NP chunk lablel.
For example: ?
[NP Anh Ay(He)] [VP thich(likes)] [NP mot chiec oto(a car)]?Before describing NP chunking tasks, wesummarize the characteristic of Vietnamese lan-guage and the background of Conditional Ran-dom Fields, Support Vector Machine, and OnlineLearning.
Then, we present how to build the an-notated corpus for the NP chunking task.2.1 The characteristic of Vietnamese WordsVietnamese syllables are elementary units thathave one way of pronunciation.
In documents,they are usually delimited by white-space.
Be-ing the elementary units, Vietnamese syllables arenot undivided elements but a structure.
Generally,each Vietnamese syllable has all five parts: firstconsonant, secondary vowel, main vowel, last con-sonant and a tone mark.
For instance, the sylla-ble tu.n (week) has a tone mark (grave accent), afirst consonant (t), a secondary vowel (u), a mainvowel () and a last consonant (n).
However, exceptfor main vowel that is required for all syllables,the other parts may be not present in some cases.For example, the syllable anh (brother) has no tonemark, no secondary vowel and no first consonant.In other case, the syllable hoa (flower) has a sec-ondary vowel (o) but no last consonant.Words in Vietnamese are made of one or moresyllables which are combined in different ways.Based on the way of constructing words from syl-lables, we can classify them into three categories:single words, complex words and reduplicativewords (Mai,Vu, Hoang, 1997).The past of speechs (Pos) of each word in Viet-namese are mainly sketched as follows.A Noun Phrase (NP) in Vietnamese consists ofthree main parts as follows: the noun center, theprefix part, and the post fix part.
The prefix andpostfix are used to support the meaning of the NP.For example in the NP ?ba sinh vien nay?, thenoun center is ?sinh vien?, and the prefix is ?ba(three)?, the postfix is ?nay?.Vietnamese Tag Equivalent to English TagCC Coordinating conjunction)CD Cardinal number)DT Determiner)V VerbP PrepositionA AdjectiveLS List item markerMD ModalN NounTable 1: Part of Speeches in Vietnamese2.2 The CorpusWe have collected more than 9,000 sentences fromseveral web-sites through the internet.
After that,we then applied the segmentation tool (Tu, Phan,Nguyen, Ha, 2006) to segment each sentencesinto a sequence of tokens.
Each sequence oftokens are then represented using the format ofCONLL 2000.
The details are sketched as follows.Each line in the annotated data consists ofthree columns: the token (a word or a punc-tuation mark), the part-of-speech tag of the to-ken, and the phrase type label (label for short)of the token.
The label of each token indicateswhether the token is outside a phrase (O), startsa phrase (B-?PhraseType?
), or continues a phrase(I-?PhraseType?
).In order to save time for building annotateddata, we made a set of simple rules for automat-ically generating the chunking data as follows.
Ifa word is not a ?noun?, ?adjective?, or ?article?
itshould be assigned the label ?O?.
The consecu-tive words are NP if they is one of type as follows:?noun noun?
; ?article noun?, ?article noun adjec-tive?.
After generating such as data, we ask anexpert about Vietnamese linguistic to correct thedata.
Finally, we got more than 9,000 sentenceswhich are annotated with NP chunking labels.Figure 1 shows an example of the Vietnamesechunking corpus.2.3 Discriminative Sequence LearningIn this section, we briefly introduce three dis-criminative sequence learning models for chunk-ing problems.2.3.1 Conditional Random FieldsConditional Random Fields (CRFs) (Lafferty,McCallum, and Pereira, 2001) are undirectedgraphical models used to calculate the conditional10Figure 1: An Example of the Vietnamese chunk-ing corpusprobability of values on designated output nodes,given values assigned to other designated inputnodes for data sequences.
CRFs make a first-orderMarkov independence assumption among outputnodes, and thus correspond to finite state machine(FSMs).Let o = (o1, o2, .
.
.
, oT ) be some observed in-put data sequence, such as a sequence of words ina text (values on T input nodes of the graphicalmodel).
Let S be a finite set of FSM states, each isassociated with a label l such as a clause start po-sition.
Let s = (s1, s2, .
.
.
, sT ) be some sequencesof states (values on T output nodes).
CRFs de-fine the conditional probability of a state sequencegiven an input sequence to beP?
(s|o) = 1Zo exp( T?t=1F (s, o, t))(1)where Zo =?s exp(?Tt=1 F (s, o, t))is a nor-malization factor over all state sequences.
We de-note ?
to be the Kronecker-?.
Let F (s, o, t) be thesum of CRFs features at time position t:?i?ifi(st?1, st, t) +?j?jgj(o, st, t) (2)where fi(st?1, st, t) = ?
(st?1, l?)?
(st, l) is atransition feature function which represents se-quential dependencies by combining the label l?of the previous state st?1 and the label l of thecurrent state st, such as the previous label l?
=AV (adverb) and the current label l = JJ (adjec-tive).
gj(o, st, t) = ?
(st, l)xk(o, t) is a per-statefeature function which combines the label l of cur-rent state st and a context predicate, i.e., the binaryfunction xk(o, t) that captures a particular prop-erty of the observation sequence o at time positiont.
For instance, the current label is JJ and the cur-rent word is ?conditional?.Training CRFs is commonly performed by max-imizing the likelihood function with respect tothe training data using advanced convex optimiza-tion techniques like L-BFGS.
Recently, there areseveral works apply Stochastic Gradient Descent(SGD) for training CRFs models.
SGD has beenhistorically associated with back-propagation al-gorithms in multilayer neural networks.And inference in CRFs, i.e., searching the mostlikely output label sequence of an input observa-tion sequence, can be done using Viterbi algo-rithm.2.3.2 Support Vector MachinesSupport vector machine (SVM)(Vapnik, 1995)is a technique of machine learning based on sta-tistical learning theory.
The main idea behindthis method can be summarized as follows.
Sup-pose that we are given l training examples (xi, yi),(1 ?
i ?
l), where xi is a feature vector in n di-mensional feature space, and yi is the class label{-1, +1 } of xi.SVM finds a hyperplane w.x+b = 0 which cor-rectly separates training examples and has maxi-mum margin which is the distance between twohyperplanes w ?
x + b ?
1 and w ?
x + b ?
?1.Finally, the optimal hyperplane is formulated asfollows:f(x) = sign( l?1?iyiK(xi, x) + b)(3)where ?i is the Lagrange multiple, and K(x?, x??
)is called a kernel function, which calculates sim-ilarity between two arguments x?
and x??.
For in-stance, the Polynomial kernel function is formu-lated as follows:K(x?, x??)
= (x?
?
x??
)p (4)SVMs estimate the label of an unknown examplex whether the sign of f(x) is positive or not.Basically, SVMs are binary classifier, thus wemust extend SVMs to multi-class classifier in or-11der to classify three or more classes.
The pair-wise classifier is one of the most popular meth-ods to extend the binary classification task to thatof K classes.
Though, we leave the details to(Kudo and Matsumoto, 2001), the idea of pairwiseclassification is to build K.(K-1)/2 classifiers con-sidering all pairs of classes, and final decision isgiven by their weighted voting.
The implementa-tion of Vietnamese text chunking is based on Yam-cha (V0.33)1.2.3.3 Online Passive-Aggressive LearningOnline Passive-Aggressive Learning (PA) wasproposed by Crammer (Crammer et al 2006) asan alternative learning algorithm to the maximizemargin algorithm.
The Perceptron style for nat-ural language processing problems as initially pro-posed by (Collins, 2002) can provide to state ofthe art results on various domains including textsegmentation, syntactic parsing, and dependencyparsing.
The main drawback of the Perceptronstyle algorithm is that it does not have a mech-anism for attaining the maximize margin of thetraining data.
It may be difficult to obtain highaccuracy in dealing with hard learning data.
Theonline algorithm for chunking parsing in whichwe can attain the maximize margin of the trainingdata without using an optimization technique.
Itis thus much faster and easier to implement.
Thedetails of PA algorithm for chunking parsing arepresented as follows.Assume that we are given a set of sentencesxi and their chunks yi where i = 1, ..., n. Letthe feature mapping between a sentence x anda sequence of chunk labels y be: ?
(x, y) =?1(x, y),?2(x, y), ...,?d(x, y) where each fea-ture mapping ?j maps (x, y) to a real value.
Weassume that each feature ?
(x, y) is associated witha weight value.
The goal of PA learning for chunk-ing parsing is to obtain a parameter w that min-imizes the hinge-loss function and the margin oflearning data.Algorithm 1 shows briefly the Online Learningfor chunking problem.
The detail about this al-gorithm can be referred to the work of (Crammeret al 2006).
In Line 7, the argmax value is com-puted by using the Viterbi algorithm which is sim-ilar to the one described in (Collins, 2002).
Algo-rithm 1 is terminated after T round.1Yamcha is available athttp://chasen.org/ taku/software/yamcha/Input: S = (xi; yi), i = 1, 2, ..., n in which1xi is the sentence and yi is a sequence ofchunksAggressive parameter C2Output: the model3Initialize: w1 = (0, 0, ..., 0)4for t=1, 2... do5Receive an sentence xt6Predict y?t = argmaxy?Y (wt.?
(xt, yt))7Suffer loss: lt =wt.?
(xt, y?t )?
wt.?
(xt, yt) +??
(yt, y?t )Set:?t = lt||?
(xt,y?t )??
(xt,yt)||28Update:9wt+1 = wt + ?t(?
(xt, yt)?
?
(xt, y?t ))end10Algorithm 1: The Passive-Aggressive algo-rithm for NP chunking.2.3.4 Feature SetFeature set is designed through features templatewhich is shown in Table 2.
All edge features obeythe first-order Markov dependency that the label(l) of the current state depends on the label (l?
)of the previous state (e.g., ?l = I-NP?
and ?l?
=B-NP?).
Each observation feature expresses howmuch influence a statistic (x(o, i)) observed sur-rounding the current position i has on the label(l) of the current state.
A statistic captures a par-ticular property of the observation sequence.
Forinstance, the observation feature ?l = I-NP?
and?word?1 is the?
indicates that the label of the cur-rent state should be I-NP (i.e., continue a nounphrase) if the previous word is the.
Table 2 de-scribes both edge and observation feature tem-plates.
Statistics for observation features are iden-tities of words, POS tags surrounding the currentposition, such as words and POS tags at ?2, ?1,1, 2.We also employ 2-order conjunctions of the cur-rent word with the previous (w?1w0) or the nextword (w0w1), and 2-order and 3-order conjunc-tions of two or three consecutive POS tags withinthe current window to make use of the mutual de-pendencies among singleton properties.
With thefeature templates shown in Table 2 and the featurerare threshold of 1 (i.e., only features with occur-rence frequency larger than 1 are included into thediscriminative models)12Edge feature templatesCurrent state: si Previous state: si?1l l?Observation feature templatesCurrent state: si Statistic (or context predicate) templates: x(o, i)l w?2; w?1; w0; w1; w2; w?1w0; w0w1;t?2; t?1; t0; t1; t2;t?2t?1; t?1t0; t0t1; t1t2; t?2t?1t0;t?1t0t1; t0t1t2Table 2: Feature templates for phrase chunking3 Experimental ResultsWe evaluate the performance of using several se-quence learning models for the Vietnamese NPchunking problem.
The data of more than 9,000sentences is evaluated using an empirical experi-ment with 5 fold cross validation test.
It meanswe used 1,800 and 7,200 sentences for testingand training the discriminative sequence learningmodels, respectively.
Note that the evaluationmethod is used the same as CONLL2000 did.
Weused Precision, Recall, and F-Measure in whichPrecision measures how many chunks found bythe algorithm are correct and the recall is per-centage of chunks defined in the corpus that werefound by the chunking program.Precision = #correct?chunk#numberofchunksRecall = #correct?chunks#numerofchunksinthecorpusF?measure =2?
Precision?
RecallPrecision + RecallTo compute the scores in our experiments, weutilized the evaluation tool (conlleval.pl) which isavailable in CONLL 2000 (Sang and Buchholz,2000, ).Figure 2 shows the precision scores of threemethods using 5 Folds cross validation test.
Itreports that the CRF-LBFGS attain the highestscore.
The SVMs and CRF-SGD are comparableto CRF-LBFGS.
The Online Learning achievedthe lowest score.Figure 3 shows the recall scores of three CRFs-LBFGS, CRFs-SGD, SVM, and Online Learning.The results show that CRFs-SGD achieved thehighest score while the Online Learning obtainedthe lowest score in comparison with others.Figure 4 and Figure 5 show the F-measure andaccuracy scores using 5 Folds Cross-validationFigure 2: Precision results in 5 Fold cross valida-tion testTest.
Similar to these results of Precision and Re-call, CRFs-LBFGS was superior to the other oneswhile the Online Learning method obtained thelowest result.Table 3 shows the comparison of three discrim-inative learning methods for Vietnamese NounPhrase chunking.
We compared the three se-quence learning methods including: CRFs usingthe LBFGS method, CRFs with SGD, and On-line Learning.
Experiment results show that theCRFs-LBFGS is the best in comparison with oth-ers.
However, the computational times when train-ing the data is slower than either SGD or OnlineLearning.
The SGD is faster than CRF-LBFS ap-proximately 6 times.
The SVM model obtained acomparable results with CRFs models and it wassuperior to Online Learning.
It yields results thatwere 0.712% than Online Learning.
However, theSVM?s training process take slower than CRFsand Online Learning.
According to our empiricalinvestigation, it takes approximately slower thanCRF-SGF, CRF-LBFGS as well as Online Learn-ing.13Figure 3: Recall result in 5 Fold cross validationtestFigure 4: The F-measure results of 5 Folds Cross-validation TestNote that we used FlexCRFs (Phan, Nguyen,Tu , 2005) for Conditional Random Fields us-ing LBFGS, and for Stochastic Gradient Descent(SGD) we used SGD1.3 which is developed byLeon Bottou 2.Methods Precision Recall F1CRF-LBGS 80.85 81.034 80.86CRF-SGD 80.74 80.66 80.58Online-PA 80.034 80.13 79.89SVM 80.412 80.982 80.638Table 3: Vietnamese Noun Phrase chunking per-formance using Discriminative Sequence Learn-ing (CRFs, SVM, Online-PA)In order to investigate which features are ma-jor effect on the discriminative learning models forVietnamese Chunking problems, we conduct threeexperiments as follows.2http://leon.bottou.org/projects/sgdFigure 5: The accuracy scores of four methodswith 5 Folds Cross-validation Test?
Cross validation test for three modes withoutconsidering the edge features?
Cross validation test for three models withoutusing POS features?
Cross validation test for three models withoutusing lexical features?
Cross validation test for three models withoutusing ?edge features template?
featuresNote that the computational time of trainingSVMs model is slow, so we skip considering fea-ture selection for SVMs.
We only consider featureselection for CRFs and Online Learning.Feature Set LBFGS SGD OnlineFull-Features 80.86 80.58 79.89Without-Edge 80.91 78.66 80.13Without-Pos 62.264 62.626 59.572Without-Lex 77.204 77.712 75.576Table 4: Vietnamese Noun Phrase chunking per-formance using Discriminative Sequence Learn-ing (CRFs, Online-PA)Table 4 shows that the Edge features have animpact to the CRF-SGD model while it do notaffect to the performance of CRFs-LBFGS andOnline-PA learning.
Table 4 also indicates thatthe POS features are severed as important featuresregarding to the performance of all discrimina-tive sequence learning models.
As we can see,if one do not use POS features the F1-score ofeach model is decreased more than 20%.
We alsoremark that the lexical features contribute an im-portant role to the performance of Vietnamese text14Figure 6: F-measures of three methods with different feature setchunking.
If we do not use lexical features theF1-score of each model is decreased till approxi-mately 3%.
In conclusion, the POS features signif-icantly effect on the performance of the discrimi-native sequence models.
This is similar to the noteof (Chen, Zhang, and Ishihara, 2006).Figure 6 reports the F-Measures of using dif-ferent feature set for each discriminative models.Note that WPos, WLex, and WEdge mean withoutusing Pos features, without using lexical features,and without using edge features, respectively.
Aswe can see, the CRF-LBFGs always achieved thebest scores in comparison with the other ones andthe Online Learning achieved the lowest scores.4 ConclusionsIn this paper, we report an investigation of devel-oping a Vietnamese Chunking tool.
We have con-structed an annotation corpus of more than 9,000sentences and exploiting discriminative learningmodels for the NP chunking task.
Experimen-tal results using 5 Folds cross-validation test haveshowed that the discriminative models are wellsuitable for Vietnamese phrase chunking.
Con-ditional random fields show a better performancein comparison with other methods.
The part ofspeech features are known as the most influencefeatures regarding to the performances of discrim-inative models on Vietnamese phrases chunking.What our contribution is expected to be usefulfor the development of Vietnamese Natural Lan-guage Processing.
Our results and corpus can besevered as a very good baseline for Natural Lan-guage Processing community to develop the Viet-namese chunking task.There are still room for improving the perfor-mance of Vietnamese chunking models.
For ex-ample, more attention on features selection is nec-essary.
We would like to solve this in future work.AcknowledgmentsThe constructive comments and helpful sugges-tions from three anonymous reviewers are greatlyappreciated.
This paper is supported by JAISTGrant for Research Associates and a part from anational project named Building Basic Resourcesand Tools for Vietnamese Language and SpeechProcessing, KC01.01/06-10.ReferencesM.
Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP 2002.K.
Crammer et al 2006.
Online Passive-AggressiveAlgorithm.
Journal of Machine Learning Research,2006W.
Chen, Y. Zhang, and H. Ishihara 2006.
An em-pirical study of Chinese chunking.
In ProceedingsCOLING/ACL 2006Dinh Dien, Vu Thuy 2006.
A maximum entropyapproach for vietnamese word segmentation.
InProceedings of the IEEE - International Conferenceon Computing and Telecommunication Technolo-gies RIVF 2006: 248-253J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In the proceed-15ings of International Conference on Machine Learn-ing (ICML), pp.282-289, 2001N.C.
Mai, D.N.
Vu, T.P.
Hoang.
1997.
Foundationsof linguistics and Vietnamese.
Education Publisher(1997) 142.
152Thi Minh Huyen Nguyen, Laurent Romary, MathiasRossignol, Xuan Luong Vu.
2006.
A lexiconfor Vietnamese language processing.
Language Re-seourse Evaluation (2006) 40:291-309.Minh Nghiem, Dien Dinh, Mai Nguyen.
2008.
Im-proving Vietnamese POS tagging by integrating arich feature set and Support Vector Machines.
InProceedings of the IEEE - International Conferenceon Computing and Telecommunication Technolo-gies RIVF 2008: 128?133.X.H.
Phan, M.L.
Nguyen, C.T.
Nguyen.
Flex-CRFs: Flexible Conditional Random Field Toolkit.http://flexcrfs.sourceforge.net, 2005T.
Kudo and Y. Matsumoto.
2001.
Chunking withSupport Vector Machines.
The Second Meeting ofthe North American Chapter of the Association forComputational Linguistics (2001)F. Sha and F. Pereira.
2005.
Shallow Parsing withConditional Random Fields.
Proceedings of HLT-NAACL 2003 213-220 (2003)C.T.
Nguyen, T.K.
Nguyen, X.H.
Phan, L.M.
Viet-namese Word Segmentation with CRFs and SVMs:An Investigation.
2006.
The 20th Pacific Asia Con-ference on Language, Information, and Computation(PACLIC), 1-3 November, 2006, Wuhan, ChinaTjong Kim Sang and Sabine Buchholz.
2000.
Intro-duction to the CoNLL-2000 Shared Task: Chunk-ing.
Proceedings of CoNLL-2000 , Lisbon, Portugal,2000.V.
Vapnik.
1995.
The Natural of Statistical LearningTheory.
New York: Springer-Verlag, 1995.16
