Zock/Rapp/Huang (eds.
): Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon, pages 64?67,Dublin, Ireland, August 23, 2014.Predicting sense convergence with distributional semantics: anapplication to the CogALex-IV 2014 shared taskLaurianne SitbonSchool of Electrical Engineering andComputer ScienceQueensland University of TechnologyBrisbane, Australialaurianne.sitbon@qut.edu.auLance De VineSchool of Electrical Engineering andComputer ScienceQueensland University of TechnologyBrisbane, Australial.devine@student.qut.edu.auAbstractThis paper presents our system to address the CogALex-IV 2014 shared task of identifying asingle word most semantically related to a group of 5 words (queries).
Our system uses animplementation of a neural language model and identifies the answer word by finding the mostsemantically similar word representation to the sum of the query representations.
It is a fullyunsupervised system which learns on around 20% of the UkWaC corpus.
It correctly identifies85 exact correct targets out of 2,000 queries, 285 approximate targets in lists of 5 suggestions.1 IntroductionHow humans draw associations between words or concepts has been the object of many studies by psy-chologists, and for many years computer scientists have attempted to model this human mental lexicon bymeans of symbolic methods (Enguix et al., 2014) or statistical models (Baroni and Lenci, 2013).
Thesemodels and methods have in turn been used to improve natural language processing systems (Lewis andSteedman, 2013), search technologies (Deerwester et al., 1990) and have since been evaluated in the viewof supporting such systems more than helping users directly.
The Shared Task CogALex-IV 2014 aims toevaluate how these models can support a user with deficiencies in their lexical access.
The task is set asone of retrieving one target word when being presented with 5 cue (associated) words.
After submissionsof all systems, the organisers revealed that the cue words were the 5 words most often associated withthe target words.
They have been collected from a large number of users who were presented with thetarget word and invited to produce one associate.
In this paper we present our preliminary investigationsto address the task with a neural net language model learning representations for words on the UkWaCcorpus (M. Baroni and Zanchetta, 2009).
We propose a strict evaluation (accuracy of finding the targetword) as well as a retrieval based evaluation that we believe is closer to the aim of helping user find theirwords.2 Approach and methodology2.1 Neural Net Language ModelIn 2003 Bengio et.
al.
(Bengio et al., 2003) introduced a neural net based method for language modellingthat learns simultaneously 1) a distributed representation for each word and 2) the probability functionfor word sequences, expressed in terms of the distributed representations.
Generalization to unseen wordsequences is obtained because such sequences receive a high probability if they are composed of wordsthat are similar to words from an already seen sequence.
An outcome of this approach is the learningof ?word embeddings?, which are vectors representing the meanings of words relative to other words(via a mechanism akin to word distribution).
For this task, we used our own implementation of thecontinuous Skip-Gram neural language model introduced by (Mikolov et al., 2013).
We refer to thismodel hereafter as skip-gram.
The implementation is similar to the word2vec software package.
NeitherThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footer areadded by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/64sub-sampling nor negative sampling were used.
A small context term window radius of size two and avector dimensionality of 128 were used.
We use the cosine between the word embedding representations(vectors) to estimate the similarity between the words in the evaluation task.
The parameters were nottuned for the task and so it is probable that further improvements can be made.2.2 Combined similarityOnce semantic vectors are created with skip-gram it allows us to measure the distances between wordsand retrieve the words most similar to another word, or those with a vector most similar to any vector,such as the sum of several word vectors.In the CogALex-IV shared task, we are provided with 5 words (cues) associated to a target word to befound.
If we consider that these words are effectively a unique semantic context for the word to be found,then it makes sense to add their vectors and find the unique word most similar.
This approach is of courseinspired by vectorial models of information retrieval and adopted widely when testing distributionalmodels for more than single words (see for example (Deerwester et al., 1990)).However we found that this strategy has limitations, because in the case of some polysemous words,some of the cues were from radically different contexts, and therefore summing up the vector did notnecessarily make sense.
For such situations, it makes more sense to find the lists of words most relatedto each of the cues, and then combine these lists.
To do this we first selected 10 candidate targets foreach cue, which are the 10 words with a representation most similar to the cue, according to the cosinebetween their words embeddings and that of the cue.
We then ranked the words according to their numberof occurrences in the 5 lists.
We did not consider the distance as measured by cosine similarity (the actualvalue) because while cosine is a good measure to rank terms by similarity, we do not believe that thisleads to an absolute estimate for actual semantic similarity.
Additionally, we chose not to assign weightsto the terms depending on which cue they were associated to as there was no reason to believe that thecues were ordered in any way (that is, by manual inspection, we did not find that cues early in the listswere most likely to lead to the target than cues later in the list were).
The results were not as good then asthose with the summed vectors.
We then adopted a third strategy, which was to consider the sum of thecues as a 6th cue when generating the lists of candidates, but also to decide on priority when selecting aunique target in case there are several candidates ranked first with an equal number of occurrences in the6 lists.
On the training set, this allowed us to find 92 correct answers for the 2,000 cases.court law judge judges courts SUMcourts laws judges appellants court courtsheriff legislation pettiti judge rackets courtstribunal jurisprudence court defendants magistrates judgeprosecution statutes sheriff respondents badminton judgesjudge statute prosecutor panellists sharia sheriffjusticiary litigation dredd jury squash lawjudicature antiunion jury organizers tribunals magistratesconsistory sharia coroner complainants prosecutors prosecutionleet criminology appellant winners proceedings prosecutorsmagistrates arbitration defendant plaintiffs parliaments prosecutorprosecutor llm magistrate magistrates rulings tribunalcontactfulhambaronsregulation complainant appellant law consistoryappeal courts magistrates senatus prosecution rulingspalace penal appeal chairmen leagues judicatureFigure 1: Example of lists of 14 most similar words for the 5 cues ?court law judge judges courts?
andtheir sum vectorFigure 1 shows an example where the cues are ?court law judge judges courts?
and the target was65magistrates.
We present for each cue as well as for the sum of all cues the lists of 14 most similar terms.In gray the words that were cues or plural of a cue were ignored.
In bold we show how ?sheriff?
wouldhave been picked if we considered the sum only, while when considering the individual sets of similarterms in addition to the sum we could find that magistrates was a more likely target.2.3 Training corpusThe corpus we used for learning the word representations is the UKWaC corpus (M. Baroni andZanchetta, 2009).
This is the corpus suggested by the organisers of the CogALex-IV 2014 Shared task,and contains web pages from the UK domain.
We pre-processed the corpus by a) lower-casing all terms,b) replacing contractions with their expansion, eg.
?it?s?
becomes ?it is?, c) removing all punctuationand d) replacing all digits with the single character ?7?.
The Skip-gram model that we used is able toscale to the whole UKWaC corpus (approximately 2 billion terms) but because of time constraints weselected only the first 20% of the corpus, and then processed the remainder of the corpus, adding to ourcorpus subset all sentences containing words that were present in the training set but not in the initial20% subset.
This was to ensure that representations for all the words in the training set could be learned.3 Results3.1 Shared task evaluationThe evaluation proposed in the shared task is the exact accuracy of a single proposed target for eachquery composed of 5 words.
There were 2,000 queries in each of the training and test set, and the resultsare expressed in total number.
All our results according to this metric are situated between 4% and 5%.We have included in table 1 the results according to the task metric, on the training and on the test set,for both the sum vectors and the combination of results from a sum and the individual words.
The latteris the one that was submitted to the shared task.Method Train TestSum 81 75Combination 84 85Table 1: Accuracy of the methods on the training and on the test corpus3.2 Retrieval evaluationWe now consider a task where a system would support a user in finding a word that they describe usingthe 5 associations.
In such a tip-of-the-tongue context, users would immediately recognise the word theyare looking for when presented in a list and also if it is presented with a different inflection (ie.
?run?instead of ?running?).
Therefore, if presented a list of words containing the target word or variationof the target word, the outcome of such a system would be considered successful.
While it would beimpractical to consider very long lists in a usability context, we have measured the accuracy for lists of2, 3, 4 and 5 words, with a measure of 1 where the word (or at least one of its inflections) is in the listand 0 otherwise.
In other words, the accuracy is the number of target words that appear as is or as aninflection in suggestion lists of varying sizes.The results presented on Figure 2 show that taking inflections into account leads to only marginal im-provements, but more importantly considering additional targets (as a list) can really improve outcomesfor the users, with almost 13% of targets being retrieved in lists of 5 suggestions with the combinedapproach.4 Discussion and conclusionThe accuracy of our approach, even when considering lists of 5 suggestions and inflections of words,show that results are still very low if one would consider a usable assistance system for users with lexicalaccess issues.
This is consistent with previous findings on a similar task in French (Sitbon et al., 2008).6650?100?150?200?250?300?1?
2?
3?
4?
5?train-??sum?train-??comb?50?100?150?200?250?300?1?
2?
3?
4?
5?test-??sum?test-?
?comb?Figure 2: Total number of targets on the left, or inflections of target on the right, out of 2,000, found inlists of 1 to 5 results, in the training set and in the test setThis work suggested that a combination of resources encoding various types of semantic relations wouldbe best, along with user models.
CogALex-IV task was not based on associations drawn by a single user,but rather by majority associations drawn by many users, so this would not apply to the task specifically.However we believe that including definitional associations such as that drawn from an ESA model onthe Wikipedia would be a way to dramatically improve the accuracy, at least when considering lists ofresults.
Additionally it would be interesting to inspect a number of variables to weigh the contributionof each cue (depending on their specificity for example).
In this paper we found that adding the vectorsrepresenting each word let to better results than only considering the words individually.
This modeof combination is one of many proposed by (Mitchell and Lapata, 2010) and in future work we willexperiment with alternative combination models.
Finally, an area for future work would be to considercleaning up the dataset so as to avoid effects such as several cues being inflections of one another (i.e..?courts?
and ?court?)
or even the target being an inflection of one of the cues, as we have observed in theCogALex-IV dataset.ReferencesMarco Baroni and Alessandro Lenci.
2013.
Distributional memory: A general framework for corpus-basedsemantics.
Computational Linguistics, 36(4):673?721.Y.
Bengio, R. Ducharme, P. Vincent, and C. Jauvin.
2003.
A neural probabilistic language model.
Journal ofMachine Learning Research.Scott C. Deerwester, Susan T Dumais, Thomas K. Landauer, George W. Furnas, and Richard A. Harshman.
1990.Indexing by latent semantic analysis.
JASIS, 41(6):391?407.Gemma Bel Enguix, Reinhard Rapp, and Michael Zock.
2014.
A graph-based approach for computing free wordassociations.
In Proceedings of LREC 2014.Mike Lewis and Mark Steedman.
2013.
Combined distributional and logical semantics.
TACL, 1:179?192.A.
Ferraresi M. Baroni, S. Bernardini and E. Zanchetta.
2009.
The wacky wide web: A collection of very largelinguistically processed web-crawled corpora.
Language Resources and Evaluation, 43(3):209?226.T.
Mikolov, K. Chen, G. Corrado, and J.
Dean.
2013.
Efficient estimation of word representations in vector space.Proceedings of Workshop at ICLR.J.
Mitchell and M. Lapata.
2010.
Composition in distributional models of semantics.
Cognitive Science,(34):1388?1429.L.
Sitbon, P. Bellot, and P. Blache.
2008.
Evaluation of lexical resources and semantic networks on a corpus ofmental associations.
In Proceedings of the Language Ressources and Evaluation Conference.67
