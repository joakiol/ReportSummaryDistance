Proceedings of ACL-08: HLT, pages 398?406,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUsing adaptor grammars to identify synergiesin the unsupervised acquisition of linguistic structureMark JohnsonBrown UniversityMark Johnson@Brown.eduAbstractAdaptor grammars (Johnson et al, 2007b) area non-parametric Bayesian extension of Prob-abilistic Context-Free Grammars (PCFGs)which in effect learn the probabilities of en-tire subtrees.
In practice, this means that anadaptor grammar learns the structures usefulfor generating the training data as well astheir probabilities.
We present several differ-ent adaptor grammars that learn to segmentphonemic input into words by modeling dif-ferent linguistic properties of the input.
Oneof the advantages of a grammar-based frame-work is that it is easy to combine grammars,and we use this ability to compare models thatcapture different kinds of linguistic structure.We show that incorporating both unsupervisedsyllabification and collocation-finding into theadaptor grammar significantly improves un-supervised word-segmentation accuracy overthat achieved by adaptor grammars that modelonly one of these linguistic phenomena.1 IntroductionHow humans acquire language is arguably the cen-tral issue in the scientific study of language.
Hu-man language is richly structured, but it is still hotlydebated as to whether this structure can be learnt,or whether it must be innately specified.
Compu-tational linguistics can contribute to this debate byidentifying which aspects of language can poten-tially be learnt from the input available to a child.Here we try to identify linguistic properties thatconvey information useful for learning to segmentstreams of phonemes into words.
We show that si-multaneously learning syllable structure and collo-cations improves word segmentation accuracy com-pared to models that learn these independently.
Thissuggests that there might be a synergistic interactionin learning several aspects of linguistic structure si-multaneously, as compared to learning each kind oflinguistic structure independently.Because learning collocations and word-initialsyllable onset clusters requires the learner to be ableto identify word boundaries, it might seem that weface a chicken-and-egg problem here.
One of the im-portant properties of the adaptor grammar inferenceprocedure is that it gives us a way of learning theseinteracting linguistic structures simultaneously.Adaptor grammars are also interesting becausethey can be viewed as directly inferring linguisticstructure.
Most well-known machine-learning andstatistical inference procedures are parameter esti-mation procedures, i.e., the procedure is designed tofind the values of a finite vector of parameters.
Stan-dard methods for learning linguistic structure typi-cally try to reduce structure learning to parameterestimation, say, by using an iterative generate-and-prune procedure in which each iteration consists ofa rule generation step that proposes new rules ac-cording to some scheme, a parameter estimation stepthat estimates the utility of these rules, and pruningstep that removes low utility rules.
For example, theBayesian unsupervised PCFG estimation proceduredevised by Stolcke (1994) uses a model-mergingprocedure to propose new sets of PCFG rules anda Bayesian version of the EM procedure to estimatetheir weights.398Recently, methods have been developed in thestatistical community for Bayesian inference ofincreasingly sophisticated non-parametric models.(?Non-parametric?
here means that the models arenot characterized by a finite vector of parameters,so the complexity of the model can vary dependingon the data it describes).
Adaptor grammars are aframework for specifying a wide range of such mod-els for grammatical inference.
They can be viewedas a nonparametric extension of PCFGs.Informally, there seem to be at least two natu-ral ways to construct non-parametric extensions of aPCFG.
First, we can construct an infinite number ofmore specialized PCFGs by splitting or refining thePCFG?s nonterminals into increasingly finer states;this leads to the iPCFG or ?infinite PCFG?
(Liang etal., 2007).
Second, we can generalize over arbitrarysubtrees rather than local trees in much the way donein DOP or tree substitution grammar (Bod, 1998;Joshi, 2003), which leads to adaptor grammars.Informally, the units of generalization of adap-tor grammars are entire subtrees, rather than justlocal trees, as in PCFGs.
Just as in tree substitu-tion grammars, each of these subtrees behaves asa new context-free rule that expands the subtree?sroot node to its leaves, but unlike a tree substitu-tion grammar, in which the subtrees are specifiedin advance, in an adaptor grammar the subtrees, aswell as their probabilities, are learnt from the train-ing data.
In order to make parsing and inferencetractable we require the leaves of these subtrees tobe terminals, as explained in section 2.
Thus adaptorgrammars are simple models of structure learning,where the subtrees that constitute the units of gen-eralization are in effect new context-free rules learntduring the inference process.
(In fact, the inferenceprocedure for adaptor grammars described in John-son et al (2007b) relies on a PCFG approximationthat contains a rule for each subtree generalizationin the adaptor grammar).This paper applies adaptor grammars to word seg-mentation and morphological acquisition.
Linguis-tically, these exhibit considerable cross-linguisticvariation, and so are likely to be learned by humanlearners.
It?s also plausible that semantics and con-textual information is less important for their acqui-sition than, say, syntax.2 From PCFGs to Adaptor GrammarsThis section introduces adaptor grammars as an ex-tension of PCFGs; for a more detailed exposition seeJohnson et al (2007b).
Formally, an adaptor gram-mar is a PCFG in which a subset M of the nonter-minals are adapted.
An adaptor grammar generatesthe same set of trees as the CFG with the same rules,but instead of defining a fixed probability distribu-tion over these trees as a PCFG does, it defines adistribution over distributions over trees.
An adaptorgrammar can be viewed as a kind of PCFG in whicheach subtree of each adapted nonterminal A ?M isa potential rule, with its own probability, so an adap-tor grammar is nonparametric if there are infinitelymany possible adapted subtrees.
(An adaptor gram-mar can thus be viewed as a tree substitution gram-mar with infinitely many initial trees).
But any finiteset of sample parses for any finite corpus can only in-volve a finite number of such subtrees, so the corre-sponding PCFG approximation only involves a finitenumber of rules, which permits us to build MCMCsamplers for adaptor grammars.A PCFG can be viewed as a set of recursively-defined mixture distributions GA over trees, one foreach nonterminal and terminal in the grammar.
If Ais a terminal then GA is the distribution that puts allof its mass on the unit tree (i.e., tree consisting of asingle node) labeled A.
If A is a nonterminal thenGA is the distribution over trees with root labeled Athat satisfies:GA =?A?B1...Bn?RA?A?B1...BnTDA(GB1 , .
.
.
, GBn)where RA is the set of rules expanding A,?A?B1,...,Bn is the PCFG ?probability?
parame-ter associated with the rule A ?
B1 .
.
.
Bn andTDA(GB1 , .
.
.
, GBn) is the distribution over treeswith root label A satisfying:TDA(G1, .
.
.
, Gn)( XXAt1 tn.
.
.
)=n?i=1Gi(ti).That is, TDA(G1, .
.
.
, Gn) is the distribution overtrees whose root node is labeled A and each subtreeti is generated independently from the distributionGi.
This independence assumption is what makesa PCFG ?context-free?
(i.e., each subtree is inde-pendent given its label).
Adaptor grammars relax399this independence assumption by in effect learningthe probability of the subtrees rooted in a specifiedsubset M of the nonterminals known as the adaptednonterminals.Adaptor grammars achieve this by associatingeach adapted nonterminal A ?
M with a DirichletProcess (DP).
A DP is a function of a base distri-bution H and a concentration parameter ?, and itreturns a distribution over distributions DP(?,H).There are several different ways to define DPs; oneof the most useful is the characterization of the con-ditional or sampling distribution of a draw fromDP(?,H) in terms of the Polya urn or ChineseRestaurant Process (Teh et al, 2006).
The Polya urninitially contains ?H(x) balls of color x.
We samplea distribution from DP(?,H) by repeatedly drawinga ball at random from the urn and then returning itplus an additional ball of the same color to the urn.In an adaptor grammar there is one DP for eachadapted nonterminal A ?
M , whose base distribu-tion HA is the distribution over trees defined usingA?s PCFG rules.
This DP ?adapts?
A?s PCFG distri-bution by moving mass from the infrequently to thefrequently occuring subtrees.
An adaptor grammarassociates a distribution GA that satisfies the follow-ing constraints with each nonterminal A:GA ?
DP(?A,HA) if A ?MGA = HA if A 6?MHA =?A?B1...Bn?RA?A?B1...BnTDA(GB1 , .
.
.
, GBn)Unlike a PCFG, an adaptor grammar does not definea single distribution over trees; rather, each set ofdraws from the DPs defines a different distribution.In the adaptor grammars used in this paper there isno recursion amongst adapted nonterminals (i.e., anadapted nonterminal never expands to itself); it iscurrently unknown whether there are tree distribu-tions that satisfy the adaptor grammar constraints forrecursive adaptor grammars.Inference for an adaptor grammar involves findingthe rule probabilities ?
and the adapted distributionsover trees G. We put Dirichlet priors over the ruleprobabilities, i.e.
:?A ?
DIR(?A)where ?A is the vector of probabilities for the rulesexpanding the nonterminal A and ?A are the corre-sponding Dirichlet parameters.The applications described below require unsu-pervised estimation, i.e., the training data consistsof terminal strings alone.
Johnson et al (2007b)describe an MCMC procedure for inferring theadapted tree distributions GA, and Johnson et al(2007a) describe a Bayesian inference procedure forthe PCFG rule parameters ?
using a Metropolis-Hastings MCMC procedure; implementations areavailable from the author?s web site.Informally, the inference procedure proceeds asfollows.
We initialize the sampler by randomly as-signing each string in the training corpus a randomtree generated by the grammar.
Then we randomlyselect a string to resample, and sample a parse of thatstring with a PCFG approximation to the adaptorgrammar.
This PCFG contains a production for eachadapted subtree in the parses of the other strings inthe training corpus.
A final accept-reject step cor-rects for the difference in the probability of the sam-pled tree under the adaptor grammar and the PCFGapproximation.3 Word segmentation with adaptorgrammarsWe now turn to linguistic applications of adap-tor grammars, specifically, to models of unsu-pervised word segmentation.
We follow previ-ous work in using the Brent corpus consists of9790 transcribed utterances (33,399 words) of child-directed speech from the Bernstein-Ratner corpus(Bernstein-Ratner, 1987) in the CHILDES database(MacWhinney and Snow, 1985).
The utteranceshave been converted to a phonemic representationusing a phonemic dictionary, so that each occur-rence of a word has the same phonemic transcrip-tion.
Utterance boundaries are given in the input tothe system; other word boundaries are not.
We eval-uated the f-score of the recovered word constituents(Goldwater et al, 2006b).
Using the adaptor gram-mar software available on the author?s web site, sam-plers were run for 10,000 epochs (passes throughthe training data).
We scored the parses assignedto the training data at the end of sampling, and forthe last two epochs we annealed at temperature 0.5(i.e., squared the probability) during sampling in or-4001 10 100 1000U word 0.55 0.55 0.55 0.53U morph 0.46 0.46 0.42 0.36U syll 0.52 0.51 0.49 0.46C word 0.53 0.64 0.74 0.76C morph 0.56 0.63 0.73 0.63C syll 0.77 0.77 0.78 0.74Table 1: Word segmentation f-score results for all mod-els, as a function of DP concentration parameter ?.
?U?indicates unigram-based grammars, while ?C?
indicatescollocation-based grammars.Sentence ?
Word+Word ?
Phoneme+Figure 1: The unigram word adaptor grammar, whichuses a unigram model to generate a sequence of words,where each word is a sequence of phonemes.
Adaptednonterminals are underlined.der to concentrate mass on high probability parses.In all experiments below we set ?
= 1, which corre-sponds to a uniform prior on PCFG rule probabilities?.
We tied the Dirichlet Process concentration pa-rameters ?, and performed runs with ?
= 1, 10, 100and 1000; apart from this, no attempt was made tooptimize the hyperparameters.
Table 1 summarizesthe word segmentation f-scores for all models de-scribed in this paper.3.1 Unigram word adaptor grammarJohnson et al (2007a) presented an adaptor gram-mar that defines a unigram model of word segmen-tation and showed that it performs as well as theunigram DP word segmentation model presented by(Goldwater et al, 2006a).
The adaptor grammar thatencodes a unigram word segmentation model shownin Figure 1.In this grammar and the grammars below, under-lining indicates an adapted nonterminal.
Phonemeis a nonterminal that expands to each of the 50 dis-tinct phonemes present in the Brent corpus.
Thisgrammar defines a Sentence to consist of a sequenceof Words, where a Word consists of a sequence ofPhonemes.
The category Word is adapted, whichmeans that the grammar learns the words that oc-cur in the training corpus.
We present our adap-Sentence ?
WordsWords ?
WordWords ?
Word WordsWord ?
PhonemesPhonemes ?
PhonemePhonemes ?
Phoneme PhonemesFigure 2: The unigram word adaptor grammar of Fig-ure 1 where regular expressions are expanded using newunadapted right-branching nonterminals.SentenceWordy u w a n tWordt uWords i D 6Wordb U kFigure 3: A parse of the phonemic representation of ?youwant to see the book?
produced by unigram word adap-tor grammar of Figure 1.
Only nonterminal nodes la-beled with adapted nonterminals and the start symbol areshown.tor grammars using regular expressions for clarity,but since our implementation does not handle reg-ular expressions in rules, in the grammars actuallyused by the program they are expanded using newnon-adapted nonterminals that rewrite in a uniformright-branching manner.
That is, the adaptor gram-mar used by the program is shown in Figure 2.The unigram word adaptor grammar generatesparses such as the one shown in Figure 3.
With ?
=1 and ?
= 10 we obtained a word segmentation f-score of 0.55.
Depending on the run, between 1, 100and 1, 400 subtrees (i.e., new rules) were found forWord.
As reported in Goldwater et al (2006a) andGoldwater et al (2007), a unigram word segmen-tation model tends to undersegment and misanalysecollocations as individual words.
This is presumablybecause the unigram model has no way to capturedependencies between words in collocations exceptto make the collocation into a single word.3.2 Unigram morphology adaptor grammarThis section investigates whether learning mor-phology together with word segmentation improvesword segmentation accuracy.
Johnson et al (2007a)presented an adaptor grammar for segmenting verbsinto stems and suffixes that implements the DP-401Sentence ?
Word+Word ?
Stem (Suffix)Stem ?
Phoneme+Suffix ?
Phoneme+Figure 4: The unigram morphology adaptor grammar,which generates each Sentence as a sequence of Words,and each Word as a Stem optionally followed by a Suffix.Parentheses indicate optional constituents.SentenceWordStemw a nSuffix6WordStemk l o zSuffixI tSentenceWordStemy uSuffixh & vWordStemt uWordStemt E lSuffixm iFigure 5: Parses of ?wanna close it?
and ?you have to tellme?
produced by the unigram morphology grammar ofFigure 4.
The first parse was chosen because it demon-strates how the grammar is intended to analyse ?wanna?into a Stem and Suffix, while the second parse shows howthe grammar tends to use Stem and Suffix to capture col-locations.based unsupervised morphological analysis modelpresented by Goldwater et al (2006b).
Here wecombine that adaptor grammar with the unigramword segmentation grammar to produce the adap-tor grammar shown in Figure 4, which is designedto simultaneously learn both word segmentation andmorphology.Parentheses indicate optional constituents in theserules, so this grammar says that a Sentence consistsof a sequence of Words, and each Word consists of aStem followed by an optional Suffix.
The categoriesWord, Stem and Suffix are adapted, which meansthat the grammar learns the Words, Stems and Suf-fixes that occur in the training corpus.
Technicallythis grammar implements a Hierarchical DirichletProcess (HDP) (Teh et al, 2006) because the basedistribution for the Word DP is itself constructedfrom the Stem and Suffix distributions, which arethemselves generated by DPs.This grammar recovers words with an f-score ofonly 0.46 with ?
= 1 or ?
= 10, which is consid-erably less accurate than the unigram model of sec-tion 3.1.
Typical parses are shown in Figure 5.
Theunigram morphology grammar tends to misanalyseeven longer collocations as words than the unigramword grammar does.
Inspecting the parses showsthat rather than capturing morphological structure,the Stem and Suffix categories typically expand towords themselves, so the Word category expands toa collocation.
It may be possible to correct this by?tuning?
the grammar?s hyperparameters, but we didnot attempt this here.These results are not too surprising, since the kindof regular stem-suffix morphology that this grammarcan capture is not common in the Brent corpus.
Itis possible that a more sophisticated model of mor-phology, or even a careful tuning of the Bayesianprior parameters ?
and ?, would produce better re-sults.3.3 Unigram syllable adaptor grammarPCFG estimation procedures have been used tomodel the supervised and unsupervised acquisitionof syllable structure (Mu?ller, 2001; Mu?ller, 2002);and the best performance in unsupervised acquisi-tion is obtained using a grammar that encodes lin-guistically detailed properties of syllables whoserules are inferred using a fairly complex algorithm(Goldwater and Johnson, 2005).
While that workstudied the acquisition of syllable structure from iso-lated words, here we investigate whether learningsyllable structure together with word segmentationimproves word segmentation accuracy.
Modelingsyllable structure is a natural application of adaptorgrammars, since the grammar can learn the possibleonset and coda clusters, rather than requiring themto be stipulated in the grammar.In the unigram syllable adaptor grammar shownin Figure 7, Consonant expands to any consonantand Vowel expands to any vowel.
This gram-mar defines a Word to consist of up to three Syl-lables, where each Syllable consists of an Onsetand a Rhyme and a Rhyme consists of a Nucleusand a Coda.
Following Goldwater and Johnson(2005), the grammar differentiates between OnsetI,which expands to word-initial onsets, and Onset,402SentenceWordOnsetIWNucleusACodaFt sWordOnsetIDNucleusICodaFsFigure 6: A parse of ?what?s this?
produced by theunigram syllable adaptor grammar of Figure 7.
(Onlyadapted non-root nonterminals are shown in the parse).which expands to non-word-initial onsets, and be-tween CodaF, which expands to word-final codas,and Coda, which expands to non-word-final codas.Note that we do not need to distinguish specific posi-tions within the Onset and Coda clusters as Goldwa-ter and Johnson (2005) did, since the adaptor gram-mar learns these clusters directly.
Just like the un-igram morphology grammar, the unigram syllablegrammar also defines a HDP because the base dis-tribution for Word is defined in terms of the Onsetand Rhyme distributions.The unigram syllable grammar achieves a wordsegmentation f-score of 0.52 at ?
= 1, which is alsolower than the unigram word grammar achieves.
In-spection of the parses shows that the unigram sylla-ble grammar also tends to misanalyse long colloca-tions as Words.
Specifically, it seems to misanalysefunction words as associated with the content wordsnext to them, perhaps because function words tendto have simpler initial and final clusters.We cannot compare our syllabification accuracywith Goldwater?s and others?
previous work becausethat work used different, supervised training dataand phonological representations based on Britishrather than American pronunciation.3.4 Collocation word adaptor grammarGoldwater et al (2006a) showed that modeling de-pendencies between adjacent words dramaticallyimproves word segmentation accuracy.
It is notpossible to write an adaptor grammar that directlyimplements Goldwater?s bigram word segmentationmodel because an adaptor grammar has one DP peradapted nonterminal (so the number of DPs is fixedin advance) while Goldwater?s bigram model hasone DP per word type, and the number of wordtypes is not known in advance.
However it is pos-Sentence ?
Word+Word ?
SyllableIFWord ?
SyllableI SyllableFWord ?
SyllableI Syllable SyllableFSyllable ?
(Onset) RhymeSyllableI ?
(OnsetI) RhymeSyllableF ?
(Onset) RhymeFSyllableIF ?
(OnsetI) RhymeFRhyme ?
Nucleus (Coda)RhymeF ?
Nucleus (CodaF)Onset ?
Consonant+OnsetI ?
Consonant+Coda ?
Consonant+CodaF ?
Consonant+Nucleus ?
Vowel+Figure 7: The unigram syllable adaptor grammar, whichgenerates each word as a sequence of up to three Sylla-bles.
Word-initial Onsets and word-final Codas are distin-guished using the suffixes ?I?
and ?F?
respectively; theseare propagated through the grammar to ensure that theseappear in the correct positions.Sentence ?
Colloc+Colloc ?
Word+Word ?
Phoneme+Figure 8: The collocation word adaptor grammar, whichgenerates a Sentence as sequence of Colloc(ations), eachof which consists of a sequence of Words.sible for an adaptor grammar to generate a sentenceas a sequence of collocations, each of which con-sists of a sequence of words.
These collocations givethe grammar a way to model dependencies betweenwords.With the DP concentration parameters ?
= 1000we obtained a f-score of 0.76, which is approxi-mately the same as the results reported by Goldwa-ter et al (2006a) and Goldwater et al (2007).
Thissuggests that the collocation word adaptor grammarcan capture inter-word dependencies similar to thosethat improve the performance of Goldwater?s bigramsegmentation model.3.5 Collocation morphology adaptor grammarOne of the advantages of working within a gram-matical framework is that it is often easy to combine403SentenceCollocWordy uWordw a n tWordt uCollocWords iWordD 6Wordb U kFigure 9: A parse of ?you want to see the book?
producedby the collocation word adaptor grammar of Figure 8.Sentence ?
Colloc+Colloc ?
Word+Word ?
Stem (Suffix)Stem ?
Phoneme+Suffix ?
Phoneme+Figure 10: The collocation morphology adaptor gram-mar, which generates each Sentence as a sequence of Col-loc(ations), each Colloc as a sequence of Words, and eachWord as a Stem optionally followed by a Suffix.different grammar fragments into a single grammar.In this section we combine the collocation aspectof the previous grammar with the morphology com-ponent of the grammar presented in section 3.2 toproduce a grammar that generates Sentences as se-quences of Colloc(ations), where each Colloc con-sists of a sequence of Words, and each Word consistsof a Stem followed by an optional Suffix, as shownin Figure 10.This grammar achieves a word segmentation f-score of 0.73 at ?
= 100, which is much better thanthe unigram morphology grammar of section 3.2,but not as good as the collocation word grammar ofthe previous section.
Inspecting the parses showsSentenceCollocWordStemy uWordStemh & vSuffixt uCollocWordStemt E lSuffixm iFigure 11: A parse of the phonemic representation of?you have to tell me?
using the collocation morphologyadaptor grammar of Figure 10.SentenceCollocWordOnsetIhNucleus&CodaFvCollocWordNucleus6WordOnsetId rNucleusICodaFN kFigure 12: A parse of ?have a drink?
produced by the col-location syllable adaptor grammar.
(Only adapted non-root nonterminals are shown in the parse).that while the ability to directly model collocationsreduces the number of collocations misanalysed aswords, function words still tend to be misanalysed asmorphemes of two-word collocations.
In fact, someof the misanalyses have a certain plausibility to them(e.g., ?to?
is often analysed as the suffix of verbssuch as ?have?, ?want?
and ?like?, while ?me?
is of-ten analysed as a suffix of verbs such as ?show?
and?tell?
), but they lower the word f-score considerably.3.6 Collocation syllable adaptor grammarThe collocation syllable adaptor grammar is thesame as the unigram syllable adaptor grammar ofFigure 7, except that the first production is replacedwith the following pair of productions.Sentence ?
Colloc+Colloc ?
Word+This grammar generates a Sentence as a sequence ofColloc(ations), each of which is composed of a se-quence of Words, each of which in turn is composedof a sequence of Syll(ables).This grammar achieves a word segmentation f-score of 0.78 at ?
= 100, which is the highest f-score of any of the grammars investigated in this pa-per, including the collocation word grammar, whichmodels collocations but not syllables.
To confirmthat the difference is significant, we ran a Wilcoxontest to compare the f-scores obtained from 8 runs ofthe collocation syllable grammar with ?
= 100 andthe collocation word grammar with ?
= 1000, andfound that the difference is significant at p = 0.006.4 Conclusion and future workThis paper has shown how adaptor grammars canbe used to study a variety of different linguistic hy-404potheses about the interaction of morphology andsyllable structure with word segmentation.
Techni-cally, adaptor grammars are a way of specifying avariety of Hierarchical Dirichlet Processes (HDPs)that can spread their support over an unboundednumber of distinct subtrees, giving them the abil-ity to learn which subtrees are most useful for de-scribing the training corpus.
Thus adaptor gram-mars move beyond simple parameter estimation andprovide a principled approach to the Bayesian es-timation of at least some types of linguistic struc-ture.
Because of this, less linguistic structure needsto be ?built in?
to an adaptor grammar compared to acomparable PCFG.
For example, the adaptor gram-mars for syllable structure presented in sections 3.3and 3.6 learn more information about syllable onsetsand codas than the PCFGs presented in Goldwaterand Johnson (2005).We used adaptor grammars to study the effectsof modeling morphological structure, syllabificationand collocations on the accuracy of a standard unsu-pervised word segmentation task.
We showed howadaptor grammars can implement a previously in-vestigated model of unsupervised word segmenta-tion, the unigram word segmentation model.
Wethen investigated adaptor grammars that incorpo-rate one additional kind of information, and foundthat modeling collocations provides the greatest im-provement in word segmentation accuracy, result-ing in a model that seems to capture many of thesame interword dependencies as the bigram modelof Goldwater et al (2006b).We then investigated grammars that combinethese kinds of information.
There does not seemto be a straight forward way to design an adaptorgrammar that models both morphology and sylla-ble structure, as morpheme boundaries typically donot align with syllable boundaries.
However, weshowed that an adaptor grammar that models col-locations and syllable structure performs word seg-mentation more accurately than an adaptor grammarthat models either collocations or syllable structurealone.
This is not surprising, since syllable onsetsand codas that occur word-peripherally are typicallydifferent to those that appear word-internally, andour results suggest that by tracking these onsets andcodas, it is possible to learn more accurate word seg-mentation.There are a number of interesting directions forfuture work.
In this paper all of the hyperparame-ters ?A were tied and varied simultaneously, but itis desirable to learn these from data as well.
Justbefore the camera-ready version of this paper wasdue we developed a method for estimating the hyper-parameters by putting a vague Gamma hyper-prioron each ?A and sampled using Metropolis-Hastingswith a sequence of increasingly narrow Gamma pro-posal distributions, producing results for each modelthat are as good or better than the best ones reportedin Table 1.The adaptor grammars presented here barelyscratch the surface of the linguistically interestingmodels that can be expressed as Hierarchical Dirich-let Processes.
The models of morphology presentedhere are particularly naive?they only capture reg-ular concatenative morphology consisting of oneparadigm class?which may partially explain whywe obtained such poor results using morphologyadaptor grammars.
It?s straight forward to designan adaptor grammar that can capture a finite numberof concatenative paradigm classes (Goldwater et al,2006b; Johnson et al, 2007a).
We?d like to learn thenumber of paradigm classes from the data, but do-ing this would probably require extending adaptorgrammars to incorporate the kind of adaptive state-splitting found in the iHMM and iPCFG (Liang etal., 2007).
There is no principled reason why thiscould not be done, i.e., why one could not design anHDP framework that simultaneously learns both thefragments (as in an adaptor grammar) and the states(as in an iHMM or iPCFG).However, inference with these more complexmodels will probably itself become more complex.The MCMC sampler of Johnson et al (2007a) usedhere is satifactory for small and medium-sized prob-lems, but it would be very useful to have more ef-ficient inference procedures.
It may be possible toadapt efficient split-merge samplers (Jain and Neal,2007) and Variational Bayes methods (Teh et al,2008) for DPs to adaptor grammars and other lin-guistic applications of HDPs.AcknowledgmentsThis research was funded by NSF awards 0544127and 0631667.405ReferencesN.
Bernstein-Ratner.
1987.
The phonology of parent-child speech.
In K. Nelson and A. van Kleeck, editors,Children?s Language, volume 6.
Erlbaum, Hillsdale,NJ.Rens Bod.
1998.
Beyond grammar: an experience-basedtheory of language.
CSLI Publications, Stanford, Cal-ifornia.Sharon Goldwater and Mark Johnson.
2005.
Repre-sentational bias in unsupervised learning of syllablestructure.
In Proceedings of the Ninth Conference onComputational Natural Language Learning (CoNLL-2005), pages 112?119, Ann Arbor, Michigan, June.Association for Computational Linguistics.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006a.
Contextual dependencies in unsupervisedword segmentation.
In Proceedings of the 21st In-ternational Conference on Computational Linguisticsand 44th Annual Meeting of the Association for Com-putational Linguistics, pages 673?680, Sydney, Aus-tralia, July.
Association for Computational Linguistics.Sharon Goldwater, Tom Griffiths, and Mark Johnson.2006b.
Interpolating between types and tokensby estimating power-law generators.
In Y. Weiss,B.
Scho?lkopf, and J. Platt, editors, Advances in NeuralInformation Processing Systems 18, pages 459?466,Cambridge, MA.
MIT Press.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2007.
Distributional cues to word boundaries:Context is important.
In David Bamman, TatianaMagnitskaia, and Colleen Zaller, editors, Proceedingsof the 31st Annual Boston University Conference onLanguage Development, pages 239?250, Somerville,MA.
Cascadilla Press.Sonia Jain and Radford M. Neal.
2007.
Splitting andmerging components of a nonconjugate dirichlet pro-cess mixture model.
Bayesian Analysis, 2(3):445?472.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007a.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Human Language Technologies2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics;Proceedings of the Main Conference, pages 139?146,Rochester, New York, April.
Association for Compu-tational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007b.
Adaptor Grammars: A frameworkfor specifying compositional nonparametric Bayesianmodels.
In B. Scho?lkopf, J. Platt, and T. Hoffman, ed-itors, Advances in Neural Information Processing Sys-tems 19, pages 641?648.
MIT Press, Cambridge, MA.Aravind Joshi.
2003.
Tree adjoining grammars.
In Rus-lan Mikkov, editor, The Oxford Handbook of Compu-tational Linguistics, pages 483?501.
Oxford Univer-sity Press, Oxford, England.Percy Liang, Slav Petrov, Michael Jordan, and Dan Klein.2007.
The infinite PCFG using hierarchical Dirichletprocesses.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 688?697.Brian MacWhinney and Catherine Snow.
1985.
Thechild language data exchange system.
Journal of ChildLanguage, 12:271?296.Karin Mu?ller.
2001.
Automatic detection of syllableboundaries combining the advantages of treebank andbracketed corpora training.
In Proceedings of the 39thAnnual Meeting of the Association for ComputationalLinguistics.Karin Mu?ller.
2002.
Probabilistic context-free grammarsfor phonology.
In Proceedings of the 6th Workshopof the ACL Special Interest Group in ComputationalPhonology (SIGPHON), pages 70?80, Philadelphia.Andreas Stolcke.
1994.
Bayesian Learning of Proba-bilistic Language Models.
Ph.D. thesis, University ofCalifornia, Berkeley.Y.
W. Teh, M. Jordan, M. Beal, and D. Blei.
2006.
Hier-archical Dirichlet processes.
Journal of the AmericanStatistical Association, 101:1566?1581.Yee Whye Teh, Kenichi Kurihara, and Max Welling.2008.
Collapsed variational inference for hdp.
In J.C.Platt, D. Koller, Y.
Singer, and S. Roweis, editors, Ad-vances in Neural Information Processing Systems 20.MIT Press, Cambridge, MA.406
