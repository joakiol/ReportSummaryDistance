Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 145?152Manchester, August 2008Mind the Gap: Dangers of Divorcing Evaluations of Summary Contentfrom Linguistic QualityJohn M. ConroyIDA/Center for Computing SciencesBowie, Maryland, USAconroy@super.orgHoa Trang DangInformation Access DivisionNational Institute of Standards and TechnologyGaithersburg, Maryland, USAhoa.dang@nist.govAbstractIn this paper, we analyze the state of cur-rent human and automatic evaluation oftopic-focused summarization in the Docu-ment Understanding Conference main taskfor 2005-2007.
The analyses show thatwhile ROUGE has very strong correlationwith responsiveness for both human andautomatic summaries, there is a signifi-cant gap in responsiveness between hu-mans and systems which is not accountedfor by the ROUGE metrics.
In additionto teasing out gaps in the current auto-matic evaluation, we propose a methodto maximize the strength of current auto-matic evaluations by using the method ofcanonical correlation.
We apply this newevaluation method, which we call ROSE(ROUGE Optimal Summarization Evalua-tion), to find the optimal linear combina-tion of ROUGE scores to maximize corre-lation with human responsiveness.1 IntroductionROUGE (Lin, 2004) and its linguistically-motivated descendent, Basic Elements (BE) (Hovyet al, 2005), evaluate a summary by computing itsoverlap with a set of model (human) summaries;ROUGE considers lexical n-grams as the unitfor comparing the overlap between summaries,while Basic Elements uses larger units of com-parison based on the output of syntactic parsers.The ROUGE/BE toolkit has become the standardautomatic method for evaluating the content ofc?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.machine-generated summaries, but the correlationof these automatic scores with human evaluationmetrics has not always been consistent.In this paper, we analyze the state of currenthuman and automatic evaluation of topic-focusedsummarization.
Using the results of the DocumentUnderstanding Conference main task for 2005-2007 we explore the correlation between variantsof ROUGE and the human metrics of responsive-ness and linguistic quality.
The analyses exposea number of challenges and several surprising re-sults.
In particular, while ROUGE has very strongcorrelation with responsiveness for both humanand system summaries, there is a significant gapin responsiveness between humans and systemswhich is not accounted for by the ROUGE metrics.One cause of the gap is that many automatic sum-marizers truncate the last sentence of their sum-mary, which shows significant reduction in the re-sponsiveness score but does not result in a statis-tically significant drop in ROUGE scores.
In ad-dition to teasing out gaps in the current automaticevaluation, we propose a method to maximize thestrength of current automatic evaluations by us-ing the method of canonical correlation.
We applythis new evaluation method, which we call ROSE(ROUGE Optimal Summarization Evaluation), tofind the optimal linear combination of ROUGEmetrics to maximize correlation with human re-sponsiveness.2 DUC 2005-2007 Task and EvaluationThe main task for DUC 2005-2007 was a com-plex question-focused summarization task that re-quired summarizers to piece together informationfrom multiple documents to answer a question orset of questions as posed in a DUC topic state-ment.
The topic statement was a request for infor-145mation that could not be met by just stating a name,date, quantity, etc.
The summarization task was thesame for both human and automatic summarizers:Given a topic statement and a set of 25-50 rele-vant newswire documents, the summarization taskwas to create from the documents a brief, well-organized, fluent summary that answered the needfor information expressed in the topic statement.The summary could be no longer than 250 words.Summaries over the size limit were truncated, andno bonus was given for creating a shorter sum-mary.NIST Assessors developed the DUC topics usedas test data.
There were 50 DUC topics each yearin 2005-2006, and 45 topics in DUC 2007.
Eachyear, 10 NIST assessors produced a total of 4 hu-man summaries for each of the topics.
The asses-sor who developed a particular topic always wroteone of the 4 summaries for that topic.NIST manually assessed each summary for bothcontent and readability.
Readability was assessedusing a set of linguistic quality questions; sum-mary content was assessed using the pseudo-extrinsic measure of content responsiveness.All summaries for a given topic were judged bya single assessor who was usually the same as thetopic developer.
In all cases, the assessor was oneof the summarizers for the topic.
Assessors firstjudged each summary for a topic for readability,assigning a separate score for each of 5 linguis-tic qualities; each summary for the topic was thenjudged for content responsiveness.
Each of thesemanual evaluations was based on a five-point scale(1=very poor, 5=very good), resulting in 6 scoresfor each summary.2.1 Evaluation of ReadabilityThe readability of the summaries was assessed us-ing five linguistic quality questions which mea-sured qualities of the summary that did not involvecomparison with a reference summary or DUCtopic.
The linguistic qualities measured were Q1:Grammaticality, Q2: Non-redundancy, Q3: Refer-ential clarity, Q4: Focus, and Q5: Structure andcoherence.2.2 Evaluation of ContentNIST performed manual pseudo-extrinsic evalu-ation of peer summaries in the form of assess-ment of responsiveness.
Responsiveness differsfrom other measures of summary content such asSEE coverage (Lin and Hovy, 2002) and Pyramidscores (Nenkova and Passonneau, 2004) in that itdoes not compare a peer summary against a set ofknown human summaries.
Rather, the assessor isgiven a list of randomly ordered, unlabeled sum-maries (both human and system-generated) for atopic, and must assign a responsiveness score toeach summary (after having read all the summariesfirst).In DUC 2005-2007, NIST assessors assigneda content responsiveness score to each summary;content responsiveness indicated the amount of in-formation in the summary that helped to satisfy theinformation need expressed in the topic statement.For content responsiveness, the linguistic qualityof the summary was to play a role in the assess-ment only insofar as it interfered with the expres-sion of information and reduced the amount of in-formation that was conveyed.In DUC 2006, assessors assigned an additionaloverall responsiveness score, which was based onboth information content and readability.
Asses-sors judged overall responsiveness only after judg-ing all their topics for readability and content re-sponsiveness; however, they were not given di-rect access to these previously assigned scores, butwere told to give their ?gut?
reaction to the overallresponsiveness of each summary.The content responsiveness score provides acoarse manual measure of information coverage;overall responsiveness reflects a combination ofreadability and content.
Content responsivenesswas largely responsible for determining how as-sessors perceived the overall quality of a sum-mary, but readability also played an important role.While poor readability could downgrade the over-all responsiveness of a summary that had verygood content responsiveness, very good readabil-ity could sometimes bolster the overall responsive-ness score of a less information-laden summary(Dang, 2006).
Attempts at greater readability in2006 paid off among the peers with the best over-all responsiveness scores.
However, the automaticpeers generally had poor readability, and the aver-age overall responsiveness for each peer was gen-erally much lower than its average content respon-siveness.In addition to the human assessment of respon-siveness, NIST computed three ?official?
auto-matic scores using ROUGE and Basic Elements:ROUGE-2, ROUGE-SU4, and ROUGE-BE recall.For the BE evaluation, summaries were parsed146withMinipar (Lin, 2005), and BE-F were extractedand matched using the Head-Modifier criterion.Jackknifing was used for each [peer, topic] pairso that human and automatic peers could be com-pared.3 An Analysis of the MetricsFigure 1 shows the average scores for each sum-marizer for DUC 2005, 2006, and 2007.
Foreach year we report the Pearson correlation coeffi-cient for ROUGE-2, ROUGE-SU4, and ROUGE-BE (denoted ?R2, ?SU4and ?BE), against contentresponsiveness.
This correlation is computed in-cluding just the systems as the human summariz-ers are clearly distributed differently.1To high-light the trend in the correlation we fit the systemsdata using robust linear regression.
This line couldbe used to extrapolate the system performance ifROUGE scores were to increase.As seen in Figure 1, while both the manualand the automatic ROUGE scores of the humansummarizers remained relatively constant over theyears, the systems made significant progress intheir automatic scores, with the top systems per-forming within statistical confidence of the humansummarizers in the ROUGE metrics as reported byConroy et al (2007).
While the content respon-siveness scores of the systems also increased as agroup over the years, all systems performed signif-icantly worse than humans in content responsive-ness as measured by Tukey?s honestly significantdifference criterion (Conroy et al, 2007).
Thus,there is not only a gap in performance betweenhumans and systems on this task as measuredmanually by content responsiveness, but there isalso a ?metric gap?
in using any single variant ofROUGE to predict content responsiveness.
Thismetric gap becomes more pronounced as systemperformance improves to the point where ROUGEis unable to distinguish between systems and hu-mans.We turn next to an analysis of sources of the per-formance gap and ?metric gap?.
Responsivenessis a subjective measure, and because NIST usesthe same humans both to generate abstracts and toevaluate the abstracts, there is the possibility thathumans may give high scores to their own abstract1One system each year in 2005-2007 had formatting prob-lems in their summaries which resulted in abnormally lowROUGE-BE scores.
While these systems are included in thescatter plot, they are not included in the correlation coefficientcomputation.Figure 1: Scatter plot of average manual con-tent responsiveness vs. automatic ROUGE scores(ROUGE-BE, ROUGE-2, and ROUGE-SU4) forhumans (filled points) and systems (unfilledpoints), for DUC 2005-2007.147just because it not surprisingly ?says what theywould say.?
To test this hypothesis we performedone-side Student T?test, testing if the group of?Self-Assessed?
abstracts had significantly higherresponsiveness for DUC 2005-2007.
Indeed, asTable 1 shows in each year and for both contentand overall responsiveness, humans gave signifi-cantly higher scores to their own abstracts than theother human abstracts.
This bias adds to the gapin content responsiveness between the human andautomatic summarizers.
Fortunately, this effect isdampened by the fact that NIST used 10 asses-sors and on average a human got to assess theirown abstract only 25% of the time.
It is notewor-thy to add that at the Multi-lingual SummarizationEvaluation of 2006, the human assessors were notthe abstractors.
This and other factors, notably aneasier task, lead to there being no gap in perfor-mance between the human and the top scoring sys-tem (Schlesinger et al, 2008).Table 1: Mean responsiveness assessment by hu-mans for their own (Self) vs. Other abstracts.Data Self Other SignifDUC 2005 Content 4.88 4.61 0.00277DUC 2006 Content 4.96 4.68 0.00052DUC 2006 Overall 4.94 4.67 0.00326DUC 2007 Content 4.87 4.65 0.01931We next examine the correlation between re-sponsiveness and each of the five (manual) met-rics for linguistic quality.
We divide the correlationinto three groups: Human (the group of 10 humansummarizers), Systems (the automatic systems en-tered into DUC), and Combined (the union of thesetwo groups).
Table 2 gives the Pearson correlationcoefficient and the p?value of statistical signifi-cance between content responsiveness and each ofthe five linguistic quality questions for DUC 2005-2007.
For DUC 2005 there is no significant corre-lation between the average score of a human or au-tomatic summarizer on linguistic questions and thecontent responsiveness score.
The fact that there isa significant correlation in the ?Combined?
case isprimarily due to the fact that the human summa-rizers scored higher as a group than the systems inthe content metric as well as the linguistic metrics.In DUC 2006 and 2007, the linguistic questionwhich rewards summaries for not having redun-dancy (Q2) has a significant negative correlationwith content responsiveness in the group of sys-tems.
This negative correlation is due largely to thefact that a number of low scoring systems (includ-ing the baseline) have no significant redundancy.Rarely does any system have sentences which arenear duplicates.
However, many systems, eventhose with relatively high responsiveness scores,still suffer from clause level redundancy, much ofit in the form of noun phrases for which a humansummarizer would employ pronouns.Table 3 gives additional correlations betweenoverall responsiveness and the linguistic questionsfor DUC 2006.
We contrast the correlations forDUC 2006 in Table 2 vs. those in Table 3.
Not sur-prisingly, overall responsiveness, which intention-ally penalizes summaries for linguistic problems,does correlate more strongly with the linguisticquestions than content responsiveness.
Also, wenote that the DUC 2007 correlations for contentresponsiveness appear more like those for DUC2006 overall responsiveness than the correspond-ing correlations for DUC 2006 content responsive-ness.
NIST did not have sufficient time in 2007to perform an overall responsiveness evaluation.We hypothesize that the assessors, many of whomworked on DUC 2006, may have inadvertentlytaken linguistic quality into account more in 2007than in 2006 for the content responsiveness, sinceonly one measure was done in 2007.Finally, it was hypothesized at the DUC 2006workshop2that the human assessors penalize sys-tems in content responsiveness which end with asentence fragment, more than could be accountedfor by the missing content of the sentence frag-ment.
We tested the hypothesis by comparingthe average grammaticality (Q1), content respon-siveness, and ROUGE scores of the 15 systemsin DUC 2007 that ended their summaries with acomplete sentence, against the 17 systems whosesummaries ended with a sentence fragment.
Ta-ble 4 gives a summary of the results.
As measuredby a Student T?test, systems that ended theirsummaries with a complete sentence had signif-icantly higher content responsiveness scores thanthose that did not; however, there was no signifi-cant difference in ROUGE scores.
The table listsROUGE-2 as an example; these results are consis-tent with both ROUGE-BE and ROUGE-SU4.Because linguistic quality clearly influencescontent responsiveness, automatic methods ofevaluating summary content that try to maximize2Lucy Vanderwende, personal communication148Table 2: Correlation and p-values between Content Responsiveness and Linguistic Quality Questions,DUC 2005-2007Year Group Q1 Q2 Q3 Q4 Q5Grammar Non-redund.
Refer.
Clarity Focus Structure/Coherence2005 Humans -0.10( 0.78) 0.03( 0.94) 0.06( 0.87) 0.23( 0.53) 0.31( 0.39)2005 Systems -0.05( 0.79) 0.15( 0.42) 0.19( 0.29) 0.30( 0.10) 0.08( 0.66)2005 Combined 0.72( 0.00) 0.75( 0.00) 0.87( 0.00) 0.90( 0.00) 0.91( 0.00)2006 Humans 0.26( 0.47) 0.15( 0.69) 0.04( 0.91) 0.64( 0.05) 0.40( 0.26)2006 Systems 0.33( 0.05) -0.38( 0.03) 0.27( 0.11) 0.41( 0.01) 0.16( 0.35)2006 Combined 0.74( 0.00) 0.68( 0.00) 0.86( 0.00) 0.87( 0.00) 0.89( 0.00)2007 Humans 0.80( 0.01) 0.73( 0.02) 0.24( 0.51) 0.57( 0.09) 0.47( 0.17)2007 Systems 0.60( 0.00) -0.43( 0.01) 0.59( 0.00) 0.71( 0.00) 0.49( 0.00)2007 Combined 0.77( 0.00) 0.72( 0.00) 0.85( 0.00) 0.92( 0.00) 0.90( 0.00)Table 3: Correlation and p-values between Overall Responsiveness and Linguistic Quality Questions,DUC 2006Group Q1 Q2 Q3 Q4 Q5Grammar Non-redund.
Refer.
Clarity Focus Structure/CoherenceHumans 0.60( 0.06) 0.27( 0.45) 0.39( 0.26) 0.74( 0.01) 0.82( 0.00)Systems 0.49( 0.00) -0.23( 0.19) 0.55( 0.00) 0.64( 0.00) 0.49( 0.00)Combined 0.77( 0.00) 0.72( 0.00) 0.89( 0.00) 0.89( 0.00) 0.93( 0.00)Table 4: Average scores of DUC 2007 systemsending with a complete sentence vs. those endingwith a fragment.Metric Sentence Fragment SignifGrammaticality 3.88 3.24 0.011Content Resp.
2.79 2.46 0.021ROUGE-2 0.098 0.092 0.408correlation with content responsiveness should at-tempt to include some measures of linguistic qual-ity.
We hypothesize that different variants ofROUGE may capture different qualities of a sum-mary; for example, ROUGE-1 may be a good in-dicator of the relevance of summary content, butROUGE variants that take into account larger con-texts may capture linguistic qualities of the sum-mary.
Hence, a combination of scores (includ-ing measures of linguistic quality) would be a bet-ter predictor of ?content?
responsiveness.3In the3An additional weakness in the automatic metrics, whichwe do not attempt to address in our current work, is their in-ability to adequately handle the generalizations that are oftenmade in model summaries (Dang, 2006), which are abstrac-tive as opposed to the extractive summaries of most systems.next section, we present a new evaluation metricthat finds a linear combination of ROUGE met-rics which, in general, has stronger correlationwith content responsiveness than any of the cur-rent ROUGE metrics.4 ROSE: Un Melange de ROUGEsWe developed an automatic content evaluationmodel which combines multiple ROUGE scoresusing canonical correlation (Hotelling, 1935).Canonical correlation finds the linear combinationof ROUGE scores that has maximum correlationwith human responsiveness on a given data set.As this family of models is a ?blend?
of ROUGEscores we call this metric ROSE, for ROUGE Op-timal Summarization Evaluation.
We first applycanonical correlation for each year of DUC usinga Monte Carlo method.
We then report on pre-liminary experiments that use ROSE models fromone year to predict content responsiveness in sub-sequent years.4.1 Blending ROUGE Scoring with aCanonical Correlation ModelSuppose we are given a set of ROUGE scores andthe corresponding content responsiveness scores.149We let aij, for i = 1, ...,m and j = 1, .., n, be theROUGE score of type j for the summarizer i, andbithe human content evaluation metric.
Canonicalcorrelation finds an n?long vector x such thatx = argmax ?
(n?j=1aijxj, bi), (1)where ?
(x, y) is the Pearson correlation betweenx and y.
A similar approach has been used by Liuand Gildea (2007) in the application of machinetranslation metrics, where they use a gradient opti-mization method to solve the maximization prob-lem.Canonical correlation actually solves a moregeneral correlation optimization problem, wherethe goal is to find two linear combinations of vari-ables to maximize the correlation between twosub-spaces.
In the application of document sum-marization, we may wish to consider a matrix B ofhuman evaluation metrics where bijis the j?th hu-man evaluation for the i?th summarizer.
We couldinclude, for example, content and overall respon-siveness or linguistic questions.
Here we solve for(x, y) in the equation below:(x, y) = argmax ?(n?j=1aijxj,k?j=1bijyj).
(2)This maximization procedure can be solved via ageneralized eigenvalue problem, which we com-puted in Matlab using a routine distributed byBorga (2000).
For the case studied here, as givenin Equation (1), the generalized eigenvalue reducesto a linear least squares problem.To find strong canonical correlations we decidedto explore a large space of metrics.
To this end, weincluded in our optimization 7 ROUGE automaticmetrics: ROUGE-1,2,3,4,L,SU4, and BE to pre-dict content responsiveness and (for DUC 2006)overall responsiveness.
As our analyses of the pre-vious section indicated for DUC 2006 and 2007there was a significant correlation between the lin-guistic questions and content responsiveness.
Weadd questions 1 and 4 to our canonical correla-tion model to see to what extent these questionscould improve the correlation with content respon-siveness.
While the linguistic questions evaluationscores are manually generated we combine themwith the automatic methods of ROUGE in an at-tempt see to what extent these non-content scorescan better model both content and overall respon-siveness.
Thus, in all we consider 9 variables topredict responsiveness.
In order to perform anevaluation that would avoid over-fitting the datawe used a Monte Carlo method of resampling toevaluate which of the 29?
1 = 511 combinationsof variables (canonical variates) to include in themodel.4In each experiment of the Monte Carlo methodwe randomly held back 1/4 of the data (human andsystem summarizers) for testing and used 3/4 ofthe data to build a canonical variate model.
Wefound 4000 random samples sufficient to achieveaccuracy within at least 2 digits.
For each ofthe canonical variate models, 4000 trials are per-formed and then the computed model is appliedto the held-back portion of the data and its Pear-son correlation and p-value is reported.
These4000 correlations (and p-values) are then used toestimate the median correlation for a canonicalvariate.
The median is computed from the sub-set of 4000 experiments with statistically signif-icant correlations on the testing data (95% con-fidence, a p-value less then 0.05).
The canoni-cal variate with the highest estimated median cor-relation is then compared with the best perform-ing ROUGE method.
We compare the best of504=511-7 canonical variates with the best of the7 ROUGE variants by using the Mann-Whitney U-test, which tests for equal medians.The procedure is then repeated using only thesystems to find the ROSE model that gives the bestprediction for just machine summarizers.Table 5 gives the results of the Monte Carlo ex-periments.
In each case the best canonical variateand the estimated median correlation are reportedover the set of ROUGE scores and the ROUGEscores in union with the linguistic questions.
Asthese results are based on 4000 trials they are morereliable than the simple correlation analysis doneusing the three official DUC automatic metrics,ROUGE-2, SU4, and BE.
We note, in particular,that occasionally ROUGE-1 and ROUGE-L werefound to be the best predictor even when linguisticquestions were allowed in the model.
Not surpris-ingly, the human evaluation of overall responsive-ness was harder to predict and the optimal variantsincluded both linguistic questions 1 and 4.The ROSE models give the best combinations4We also removed one system each year that had a poorROUGE-BE score due to formatting problems.150Table 5: Monte Carlo Results for Canonical Correlation Model.
A * by a variant indicates that it differssignificantly from the best single ROUGE correlation with a p-value of 10?7or less as measured by aMann Whitney U-test.Year Metric Summarizer Best ROUGE Corr.
ROSEROUGECorr.
ROSE(ROUGE,Q)Corr.2005 Content All BE 0.976 R1,R2,R4,SU4,BE* 0.981 R1,R2,R3,RL,SU4,BE,Q4* 0.9862005 Content Systems R2 0.939 R1,R2,RL 0.940 R2,RL,SU4,Q4 0.9412006 Content All RL 0.928 R1,R2,R3,R4* 0.942 RL,Q1* 0.9602006 Content Systems R1 0.900 R1 0.900 R1 0.9002007 Content All BE 0.937 R1,R4,RL,BE 0.940 BE,Q4* 0.9662007 Content Systems R3 0.906 RL,BE* 0.915 R1,RL,BE,Q1* 0.9292006 Overall All BE 0.893 R1,R2,R3,R4* 0.913 R3,R4,Q1,Q4* 0.9462006 Overall Systems RL 0.854 RL 0.854 R1,R3,SU4,Q1,Q4* 0.894of ROUGE scores to give maximum correlationwith the human judgement of content or overallresponsiveness.
The ROSE models based on justROUGE for the automatic summarizers are an ap-propriate method to use to compare systems thatdid not compete in DUC with those that did.4.2 Applying ROSE across the YearsTo further evaluate the generality of the ROSEmodel we apply DUC 2005 canonical correlationmodels to DUC 2006 and DUC 2007, and simi-larly apply the DUC 2006 model to the DUC 2007data.
In these experiments we measure the stabil-ity of a ROSE model from one year to the next.
(Note, we have also computed a model based onthe combined data of DUC 2005 and DUC 2006for use with DUC 2007 and these results are com-parable to those presented.)
Here, for simplicity,we restrict the ROSE model to use only the ?offi-cial?
ROUGE metrics to build a model based on agiven year and then evaluate that model on a subse-quent year.
Table 6 gives results for ROSE modelsconstructed from only ROUGE-2, ROUGE-SU4,ROUGE-BE, and content responsiveness to createthe ROSE model for each year; results are alsogiven for ROSE models (ROSE+Q1,4) which alsoincludes the linguistic questions on grammaticality(Q1) and focus (Q4).The ROSE models built from only ROUGEscores had mixed results, sometimes performingworse than a single ROUGE score (e.g., the ROSEmodel trained on DUC 2005 and evaluated onDUC 2006), but in other cases performing as wellas or better than single ROUGE scores.
Thesepreliminary results with ROSE illustrate the diffi-culty in finding a single canonical variate that canbe used from year to year to build ROSE mod-els based on previous years?
data.
We hypothesizethat the task is made more difficult due to humanschanging their criteria for judging content respon-siveness over the years.On the other hand, ROSE+Q1,4models that in-cluded the linguistic questions Q1 and Q4 alwaysyielded the best correlation with content respon-siveness both for the systems and for the group ofcombined systems and human summarizers.5 ConclusionsWe analyzed the results of the topic-focused sum-marization task using the data from DUC 2005-2007.
Our main concern was to expose causes ofthe gap that currently exists between automatic andhuman evaluation of summary content.
As the au-tomatic ROUGE scores of system summaries ap-proaches that of human summaries, the disparitybetween automatic and manual measures of sum-mary content becomes a more important concern.We find that there is a slight bias in the human eval-uation: humans give their own summaries signifi-cantly higher scores.
Furthermore, the responsive-ness metric appears to be time varying, i.e., the hu-mans changed their standards for judging respon-siveness over the years, making it difficult to useautomatic scores from one year to predict respon-siveness in another year.Assessors naturally tend toward taking linguis-tic quality into account when assessing summaries.The instructions for assessing content responsive-ness implicitly acknowledges this; what is surpris-ing is the extent to which linguistic quality doesinfluence content responsiveness.
In particular, wedemonstrated that content responsiveness in DUC2006 and 2007 correlated with the linguistic qual-ity questions of grammar (Q1) and focus (Q4),and that systems were significantly penalized incontent responsiveness when their summary ended151Table 6: Correlation and p-values between content responsiveness and various metrics for each ?Test?year of DUC.
ROSE models were constructed using DUC data from ?Train?
year and evaluated on datafrom ?Test?
year.Train/Test Summarizer R2 SU4 BE Q1 Q4 ROSE ROSE+Q1,42005/2006 Humans 0.64(0.05) 0.69(0.03) 0.57(0.09) 0.26(0.47) 0.64(0.05) 0.59 (0.07) 0.61(0.06)2005/2006 Systems 0.83(0.00) 0.85(0.00) 0.85(0.00) 0.33(0.06) 0.41(0.02) 0.83 (0.00) 0.85(0.00)2005/2006 All 0.90(0.00) 0.88(0.00) 0.90(0.00) 0.74(0.00) 0.87(0.00) 0.90 (0.00) 0.93(0.00)2005/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.53 (0.12) 0.57(0.09)2005/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.90 (0.00) 0.92(0.00)2005/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92 (0.00) 0.94(0.00)2006/2007 Humans 0.41(0.24) 0.26(0.47) 0.55(0.10) 0.80(0.01) 0.57(0.09) 0.52(0.12) 0.67(0.03)2006/2007 Systems 0.88(0.00) 0.84(0.00) 0.89(0.00) 0.56(0.00) 0.68(0.00) 0.89 (0.00) 0.90(0.00)2006/2007 All 0.91(0.00) 0.88(0.00) 0.92(0.00) 0.77(0.00) 0.92(0.00) 0.92( 0.00) 0.96(0.00)with a sentence fragment even though the auto-matic content measures did not show a statisti-cally significant difference.
The influence of lin-guistic quality on ?content?
responsiveness con-tributes to the evaluation gap that we see betweenROUGE/BE and this coarse human measure ofsummary content.Automatic methods of evaluating summary con-tent that try to maximize correlation with contentresponsiveness should therefore attempt to includesome measures of linguistic quality.
We found thata blending of ROUGE scores using canonical cor-relation gave higher correlations with content andoverall responsiveness.
When the linguistic ques-tions Q1 and Q4 were added to the ROSE model,correlations of up to 0.96 were observed.
This re-sult leads to a natural question: What automaticmethods could be used to approximate the linguis-tic questions?
The work of Barzilay and Lapata(2005) on local coherence might be a possible can-didate for estimating focus (Q4), while an auto-matic parser could be run on the summaries andthe induced score could be used as a surrogate forgrammaticality (Q1).ReferencesBarzilay, Regina and Mirella Lapata.
2005.
Modelinglocal coherence: An entity-based approach.
In Pro-ceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL?05), pages141?148, Ann Arbor, Michigan, June.
Associationfor Computational Linguistics.Borga, Magnus.
2000.
Matlab function cca().Conroy, John M., Judith D. Schlesinger, and Dianne P.O?Leary.
2007.
CLASSY 2007 at DUC 2007.
InProceedings of the Seventh Document Understand-ing Conference (DUC), Rochester, New York.Dang, Hoa Trang.
2006.
Overview of DUC 2006.
InProceedings of the Sixth Document UnderstandingConference (DUC), New York City, New York.Hotelling, H. 1935.
The most predictable criterion.Journal of Educational Psychology, 26:139?142.Hovy, Eduard, Chin-Yew Lin, and Liang Zhou.
2005.Evaluating duc 2005 using Basic Elements.
In Pro-ceedings of the Fifth Document Understanding Con-ference (DUC), Vancouver, Canada.Lin, Chin-Yew and Eduard Hovy.
2002.
Manual andautomatic evaluation of summaries.
In Proceedingsof the ACL-02 Workshop on Automatic Summariza-tion, Philadelphia, PA.Lin, Chin-Yew.
2004.
ROUGE: A package forautomatic evaluation of summaries.
In Proceed-ings of the ACL-04 Workshop: Text SummarizationBranches Out, pages 74?81, Barcelona, Spain.Lin, Dekang.
2005.
A dependency-base method forevaluating broad-coverage parsers.
In Proceedingsof the Nineteenth International Joint Conference onArtificial Intelligence (IJCAI), Edinburgh, Scotland.Liu, Ding and Daniel Gildea.
2007.
Source-languagefeatures and maximum correlation training for ma-chine translation evaluation.
In Proceedings of the2007 Meeting of the North American chapter of theAssociation for Computational Linguistics (NAACL-07).Nenkova, Ani and Rebecca Passonneau.
2004.
Evalu-ating content selection in summarization: The pyra-mid method.
In Proceedings of the Human Lan-guage Technology Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, pages 145?152, Boston, MA.Schlesinger, Judith D., Dianne P. O?Leary, and John M.Conroy.
2008.
Arabic/English multi-documentsummarization with CLASSY?the past and the fu-ture.
In Conference on Intelligent Text Processingand Computational Linguistics 2008.
Lecture Notesin Computer Science, Springer-Verlag.
to appear.152
