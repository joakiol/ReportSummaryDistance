Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 277?285,Beijing, August 2010Entity Disambiguation for Knowledge Base Population?Mark Dredze and ?Paul McNamee and ?Delip Rao and ?Adam Gerber and ?Tim Finin?Human Language Technology Center of Excellence, Center for Language and Speech ProcessingJohns Hopkins University?University of Maryland ?
Baltimore Countymdredze,mcnamee,delip,adam.gerber@jhu.edu, finin@umbc.eduAbstractThe integration of facts derived from information extractionsystems into existing knowledge bases requires a system todisambiguate entity mentions in the text.
This is challeng-ing due to issues such as non-uniform variations in entitynames, mention ambiguity, and entities absent from a knowl-edge base.
We present a state of the art system for entity dis-ambiguation that not only addresses these challenges but alsoscales to knowledge bases with several million entries usingvery little resources.
Further, our approach achieves perfor-mance of up to 95% on entities mentioned from newswireand 80% on a public test set that was designed to includechallenging queries.1 IntroductionThe ability to identify entities like people, orga-nizations and geographic locations (Tjong KimSang and De Meulder, 2003), extract their at-tributes (Pasca, 2008), and identify entity rela-tions (Banko and Etzioni, 2008) is useful for sev-eral applications in natural language processingand knowledge acquisition tasks like populatingstructured knowledge bases (KB).However, inserting extracted knowledge into aKB is fraught with challenges arising from nat-ural language ambiguity, textual inconsistencies,and lack of world knowledge.
To the discern-ing human eye, the ?Bush?
in ?Mr.
Bush leftfor the Zurich environment summit in Air ForceOne.?
is clearly the US president.
Further con-text may reveal it to be the 43rd president, GeorgeW.
Bush, and not the 41st president, George H.W.
Bush.
The ability to disambiguate a polyse-mous entity mention or infer that two orthograph-ically different mentions are the same entity iscrucial in updating an entity?s KB record.
Thistask has been variously called entity disambigua-tion, record linkage, or entity linking.
When per-formed without a KB, entity disambiguation iscalled coreference resolution: entity mentions ei-ther within the same document or across multi-ple documents are clustered together, where eachcluster corresponds to a single real world entity.The emergence of large scale publicly avail-able KBs like Wikipedia and DBPedia has spurredan interest in linking textual entity references totheir entries in these public KBs.
Bunescu andPasca (2006) and Cucerzan (2007) presented im-portant pioneering work in this area, but sufferfrom several limitations including Wikipedia spe-cific dependencies, scale, and the assumption ofa KB entry for each entity.
In this work we in-troduce an entity disambiguation system for link-ing entities to corresponding Wikipedia pages de-signed for open domains, where a large percent-age of entities will not be linkable.
Further, ourmethod and some of our features readily general-ize to other curated KB.
We adopt a supervisedapproach, where each of the possible entities con-tained within Wikipedia are scored for a match tothe query entity.
We also describe techniques todeal with large knowledge bases, like Wikipedia,which contain millions of entries.
Furthermore,our system learns when to withhold a link whenan entity has no matching KB entry, a task thathas largely been neglected in prior research incross-document entity coreference.
Our systemproduces high quality predictions compared withrecent work on this task.2 Related WorkThe information extraction oeuvre has a gamut ofrelation extraction methods for entities like per-sons, organizations, and locations, which can beclassified as open- or closed-domain dependingon the restrictions on extractable relations (Bankoand Etzioni, 2008).
Closed domain systems ex-tract a fixed set of relations while in open-domainsystems, the number and type of relations are un-bounded.
Extracted relations still require process-ing before they can populate a KB with facts:namely, entity linking and disambiguation.277Motivated by ambiguity in personal namesearch, Mann and Yarowsky (2003) disambiguateperson names using biographic facts, like birthyear, occupation and affiliation.
When presentin text, biographic facts extracted using regularexpressions help disambiguation.
More recently,the Web People Search Task (Artiles et al, 2008)clustered web pages for entity disambiguation.The related task of cross document corefer-ence resolution has been addressed by severalresearchers starting from Bagga and Baldwin(1998).
Poesio et al (2008) built a cross documentcoreference system using features from encyclo-pedic sources like Wikipedia.
However, success-ful coreference resolution is insufficient for cor-rect entity linking, as the coreference chain muststill be correctly mapped to the proper KB entry.Previous work by Bunescu and Pasca (2006)and Cucerzan (2007) aims to link entity men-tions to their corresponding topic pages inWikipedia but the authors differ in their ap-proaches.
Cucerzan uses heuristic rules andWikipedia disambiguation markup to derive map-pings from surface forms of entities to theirWikipedia entries.
For each entity in Wikipedia,a context vector is derived as a prototype for theentity and these vectors are compared (via dot-product) with the context vectors of unknown en-tity mentions.
His work assumes that all entitieshave a corresponding Wikipedia entry, but this as-sumption fails for a significant number of entitiesin news articles and even more for other genres,like blogs.
Bunescu and Pasca on the other handsuggest a simple method to handle entities not inWikipedia by learning a threshold to decide if theentity is not in Wikipedia.
Both works mentionedrely on Wikipedia-specific annotations, such ascategory hierarchies and disambiguation links.We just recently became aware of a systemfielded by Li et al at the TAC-KBP 2009 eval-uation (2009).
Their approach bears a numberof similarities to ours; both systems create candi-date sets and then rank possibilities using differinglearning methods, but the principal difference is inour approach to NIL prediction.
Where we simplyconsider absence (i.e., the NIL candidate) as an-other entry to rank, and select the top-ranked op-tion, they use a separate binary classifier to decidewhether their top prediction is correct, or whetherNIL should be output.
We believe relying on fea-tures that are designed to inform whether absenceis correct is the better alternative.3 Entity LinkingWe define entity linking as matching a textual en-tity mention, possibly identified by a named en-tity recognizer, to a KB entry, such as a Wikipediapage that is a canonical entry for that entity.
Anentity linking query is a request to link a textualentity mention in a given document to an entry ina KB.
The system can either return a matching en-try or NIL to indicate there is no matching entry.In this work we focus on linking organizations,geo-political entities and persons to a Wikipediaderived KB.3.1 Key IssuesThere are 3 challenges to entity linking:Name Variations.
An entity often has multiplemention forms, including abbreviations (BostonSymphony Orchestra vs. BSO), shortened forms(Osama Bin Laden vs. Bin Laden), alternatespellings (Osama vs. Ussamah vs. Oussama),and aliases (Osama Bin Laden vs. Sheikh Al-Mujahid).
Entity linking must find an entry de-spite changes in the mention string.Entity Ambiguity.
A single mention, likeSpringfield, can match multiple KB entries, asmany entity names, like people and organizations,tend to be polysemous.Absence.
Processing large text collections vir-tually guarantees that many entities will not ap-pear in the KB (NIL), even for large KBs.The combination of these challenges makesentity linking especially challenging.
Consideran example of ?William Clinton.?
Most read-ers will immediately think of the 42nd US pres-ident.
However, the only two William Clintons inWikipedia are ?William de Clinton?
the 1st Earlof Huntingdon, and ?William Henry Clinton?
theBritish general.
The page for the 42nd US pres-ident is actually ?Bill Clinton?.
An entity link-ing system must decide if either of the WilliamClintons are correct, even though neither are ex-act matches.
If the system determines neither278matches, should it return NIL or the variant ?BillClinton??
If variants are acceptable, then perhaps?Clinton, Iowa?
or ?DeWitt Clinton?
should beacceptable answers?3.2 ContributionsWe address these entity linking challenges.Robust Candidate Selection.
Our system isflexible enough to find name variants but suffi-ciently restrictive to produce a manageable can-didate list despite a large-scale KB.Features for Entity Disambiguation.
We de-veloped a rich and extensible set of features basedon the entity mention, the source document, andthe KB entry.
We use a machine learning rankerto score each candidate.Learning NILs.
We modify the ranker to learnNIL predictions, which obviates hand tuning andimportantly, admits use of additional features thatare indicative of NIL.Our contributions differ from previous efforts(Bunescu and Pasca, 2006; Cucerzan, 2007) inseveral important ways.
First, previous efforts de-pend on Wikipedia markup for significant perfor-mance gains.
We make no such assumptions, al-though we show that optional Wikipedia featureslead to a slight improvement.
Second, Cucerzandoes not handle NILs while Bunescu and Pascaaddress them by learning a threshold.
Our ap-proach learns to predict NIL in a more generaland direct way.
Third, we develop a rich fea-ture set for entity linking that can work with anyKB.
Finally, we apply a novel finite state machinemethod for learning name variations.
1The remaining sections describe the candidateselection system, features and ranking, and ournovel approach learning NILs, followed by anempirical evaluation.4 Candidate Selection for Name VariantsThe first system component addresses the chal-lenge of name variants.
As the KB contains a largenumber of entries (818,000 entities, of which 35%are PER, ORG or GPE), we require an efficient se-lection of the relevant candidates for a query.Previous approaches used Wikipedia markupfor filtering ?
only using the top-k page categories1http://www.clsp.jhu.edu/ markus/fstrain(Bunescu and Pasca, 2006) ?
which is limited toWikipedia and does not work for general KBs.We consider a KB independent approach to selec-tion that also allows for tuning candidate set size.This involves a linear pass over KB entry names(Wikipedia page titles): a naive implementationtook two minutes per query.
The following sec-tion reduces this to under two seconds per query.For a given query, the system selects KB entriesusing the following approach:?
Titles that are exact matches for the mention.?
Titles that are wholly contained in or containthe mention (e.g., Nationwide and Nationwide In-surance).?
The first letters of the entity mention match theKB entry title (e.g., OA and Olympic Airlines).?
The title matches a known alias for the entity(aliases described in Section 5.2).?
The title has a strong string similarity scorewith the entity mention.
We include several mea-sures of string similarity, including: characterDice score > 0.9, skip bigram Dice score > 0.6,and Hamming distance <= 2.We did not optimize the thresholds for stringsimilarity, but these could obviously be tuned tominimize the candidate sets and maximize recall.All of the above features are general for anyKB.
However, since our evaluation used a KBderived from Wikipedia, we included a fewWikipedia specific features.
We added an entry ifits Wikipedia page appeared in the top 20 Googleresults for a query.On the training dataset (Section 7) the selectionsystem attained a recall of 98.8% and producedcandidate lists that were three to four orders ofmagnitude smaller than the KB.
Some recall er-rors were due to inexact acronyms: ABC (ArabBanking; ?Corporation?
is missing), ASG (AbuSayyaf; ?Group?
is missing), and PCF (FrenchCommunist Party; French reverses the order of thepre-nominal adjectives).
We also missed Interna-tional Police (Interpol) and Becks (David Beck-ham; Mr. Beckham and his wife are collectivelyreferred to as ?Posh and Becks?
).2794.1 Scaling Candidate SelectionOur previously described candidate selection re-lied on a linear pass over the KB, but we seekmore efficient methods.
We observed that theabove non-string similarity filters can be pre-computed and stored in an index, and that the skipbigram Dice score can be computed by indexingthe skip bigrams for each KB title.
We omittedthe other string similarity scores, and collectivelythese changes enable us to avoid a linear pass overthe KB.
Finally we obtained speedups by servingthe KB concurrently2.
Recall was nearly identicalto the full system described above: only two morequeries failed.
Additionally, more than 95% ofthe processing time was consumed by Dice scorecomputation, which was only required to cor-rectly retrieve less than 4% of the training queries.Omitting the Dice computation yielded results ina few milliseconds.
A related approach is that ofcanopies for scaling clustering for large amountsof bibliographic citations (McCallum et al, 2000).In contrast, our setting focuses on alignment vs.clustering mentions, for which overlapping parti-tioning approaches like canopies are applicable.5 Entity Linking as RankingWe select a single correct candidate for a queryusing a supervised machine learning ranker.
Werepresent each query by a D dimensional vectorx, where x ?
RD, and we aim to select a sin-gle KB entry y, where y ?
Y , a set of possibleKB entries for this query produced by the selec-tion system above, which ensures that Y is small.The ith query is given by the pair {xi, yi}, wherewe assume at most one correct KB entry.To evaluate each candidate KB entry in Y wecreate feature functions of the form f(x, y), de-pendent on both the example x (document and en-tity mention) and the KB entry y.
The featuresaddress name variants and entity disambiguation.We take a maximum margin approach to learn-ing: the correct KB entry y should receive ahigher score than all other possible KB entriesy?
?
Y, y?
6= y plus some margin ?.
This learning2Our Python implementation with indexing features andfour threads achieved up to 80?
speedup compared to naiveimplementation.constraint is equivalent to the ranking SVM algo-rithm of Joachims (2002), where we define an or-dered pair constraint for each of the incorrect KBentries y?
and the correct entry y.
Training sets pa-rameters such that score(y) ?
score(y?)
+ ?.
Weused the library SVMrank to solve this optimiza-tion problem.3 We used a linear kernel, set theslack parameter C as 0.01 times the number oftraining examples, and take the loss function asthe total number of swapped pairs summed overall training examples.
While previous work useda custom kernel, we found a linear kernel just aseffective with our features.
This has the advan-tage of efficiency in both training and prediction 4?
important considerations in a system meant toscale to millions of KB entries.5.1 Features for Entity Disambiguation200 atomic features represent x based on eachcandidate query/KB pair.
Since we used a lin-ear kernel, we explicitly combined certain fea-tures (e.g., acroynym-match AND known-alias) tomodel correlations.
This included combining eachfeature with the predicted type of the entity, al-lowing the algorithm to learn prediction functionsspecific to each entity type.
With feature combina-tions, the total number of features grew to 26,569.The next sections provide an overview; for a de-tailed list see McNamee et al (2009).5.2 Features for Name VariantsVariation in entity name has long been recog-nized as a bane for information extraction sys-tems.
Poor handling of entity name variants re-sults in low recall.
We describe several featuresranging from simple string match to finite statetransducer matching.String Equality.
If the query name and KB en-try name are identical, this is a strong indication ofa match, and in our KB entry names are distinct.However, similar or identical entry names thatrefer to distinct entities are often qualified withparenthetical expressions or short clauses.
Asan example, ?London, Kentucky?
is distinguished3www.cs.cornell.edu/people/tj/svm_light/svm_rank.html4Bunescu and Pasca (2006) report learning tens of thou-sands of support vectors with their ?taxonomy?
kernel whilea linear kernel represents all support vectors with a singleweight vector, enabling faster training and prediction.280from ?London, Ontario?, ?London, Arkansas?,?London (novel)?, and ?London?.
Therefore,other string equality features were used, such aswhether names are equivalent after some transfor-mation.
For example, ?Baltimore?
and ?BaltimoreCity?
are exact matches after removing a commonGPE word like city; ?University of Vermont?
and?University of VT?
match if VT is expanded.Approximate String Matching.
Many entitymentions will not match full names exactly.
Weadded features for character Dice, skip bigramDice, and left and right Hamming distance scores.Features were set based on quantized scores.These were useful for detecting minor spellingvariations or mistakes.
Features were also added ifthe query was wholly contained in the entry name,or vice-versa, which was useful for handling ellip-sis (e.g., ?United States Department of Agricul-ture?
vs. ?Department of Agriculture?).
We alsoincluded the ratio of the recursive longest com-mon subsequence (Christen, 2006) to the shorterof the mention or entry name, which is effective athandling some deletions or word reorderings (e.g.,?Li Gong?
and ?Gong Li?).
Finally, we checkedwhether all of the letters of the query are found inthe same order in the entry name (e.g., ?Univ Wis-consin?
would match ?University of Wisconsin?).Acronyms.
Features for acronyms, using dic-tionaries and partial character matches, enablematches between ?MIT?
and ?Madras Institute ofTechnology?
or ?Ministry of Industry and Trade.?Aliases.
Many aliases or nicknames are non-trivial to guess.
For example JAVA is thestock symbol for Sun Microsystems, and ?Gin-ger Spice?
is a stage name of Geri Halliwell.
Areasonable way to do this is to employ a dictio-nary and alias lists that are commonly availablefor many domains5.FST Name Matching.
Another measure of sur-face similarity between a query and a candidatewas computed by training finite-state transducerssimilar to those described in Dreyer et al (2008).These transducers assign a score to any string pairby summing over all alignments and scoring all5We used multiple lists, including class-specific lists (i.e.,for PER, ORG, and GPE) lists extracted from Freebase (Bol-lacker et al, 2008) and Wikipedia redirects.
PER, ORG, andGPE are the commonly used terms for entity types for peo-ple, organizations and geo-political regions respectively.contained character n-grams; we used n-grams oflength 3 and less.
The scores are combined using aglobal log-linear model.
Since different spellingsof a name may vary considerably in length (e.g.,J Miller vs. Jennifer Miller) we eliminated thelimit on consecutive insertions used in previousapplications.65.3 Wikipedia FeaturesMost of our features do not depend on Wikipediamarkup, but it is reasonable to include featuresfrom KB properties.
Our feature ablation studyshows that dropping these features causes a smallbut statistically significant performance drop.WikiGraph statistics.
We added features de-rived from the Wikipedia graph structure for anentry, like indegree of a node, outdegree of a node,and Wikipedia page length in bytes.
These statis-tics favor common entity mentions over rare ones.Wikitology.
KB entries can be indexed with hu-man or machine generated metadata consisting ofkeywords or categories in a domain-appropriatetaxonomy.
Using a system called Wikitology,Syed et al (2008) investigated use of ontologyterms obtained from the explicit category systemin Wikipedia as well as relationships induced fromthe hyperlink graph between related Wikipediapages.
Following this approach we computed top-ranked categories for the query documents andused this information as features.
If none of thecandidate KB entries had corresponding highly-ranked Wikitology pages, we used this as a NILfeature (Section 6.1).5.4 PopularityAlthough it may be an unsafe bias to give prefer-ence to common entities, we find it helpful to pro-vide estimates of entity popularity to our rankeras others have done (Fader et al, 2009).
Apartfrom the graph-theoretic features derived from theWikipedia graph, we used Google?s PageRank toby adding features indicating the rank of the KBentry?s corresponding Wikipedia page in a Googlequery for the target entity mention.6Without such a limit, the objective function may divergefor certain parameters of the model; we detect such cases andlearn to avoid them during training.2815.5 Document FeaturesThe mention document and text associated with aKB entry contain context for resolving ambiguity.Entity Mentions.
Some features were based onpresence of names in the text: whether the queryappeared in the KB text and the entry name in thedocument.
Additionally, we used a named-entitytagger and relation finder, SERIF (Boschee et al,2005), identified name and nominal mentions thatwere deemed co-referent with the entity mentionin the document, and tested whether these nounswere present in the KB text.
Without the NE anal-ysis, accuracy on non-NIL entities dropped 4.5%.KB Facts.
KB nodes contain infobox attributes(or facts); we tested whether the fact text waspresent in the query document, both locally to amention, or anywhere in the text.
Although thesefacts were derived from Wikipedia infoboxes,they could be obtained from other sources as well.Document Similarity We measured similaritybetween the query document and the KB text intwo ways: cosine similarity with TF/IDF weight-ing (Salton and McGill, 1983); and using the Dicecoefficient over bags of words.
IDF values wereapproximated using counts from the Google 5-gram dataset as by Klein and Nelson (2008).Entity Types.
Since the KB contained typesfor entries, we used these as features as well asthe predicted NE type for the entity mention inthe document text.
Additionally, since only asmall number of KB entries had PER, ORG, orGPE types, we also inferred types from Infoboxclass information to attain 87% coverage in theKB.
This was helpful for discouraging selectionof eponymous entries named after famous enti-ties (e.g., the former U.S. president vs. ?John F.Kennedy International Airport?
).5.6 Feature CombinationsTo take into account feature dependencies we cre-ated combination features by taking the cross-product of a small set of diverse features.
Theattributes used as combination features includedentity type; a popularity based on Google?s rank-ings; document comparison using TF/IDF; cov-erage of co-referential nouns in the KB nodetext; and name similarity.
The combinations werecascaded to allow arbitrary feature conjunctions.Thus it is possible to end up with a feature kbtype-is-ORG AND high-TFIDF-score AND low-name-similarity.
The combined features increased thenumber of features from roughly 200 to 26,000.6 Predicting NIL MentionsSo far we have assumed that each example has acorrect KB entry; however, when run over a largecorpus, such as news articles, we expect a signifi-cant number of entities will not appear in the KB.Hence it will be useful to predict NILs.We learn when to predict NIL using the SVMranker by augmenting Y to include NIL, whichthen has a single feature unique to NIL answers.It can be shown that (modulo slack variables) thisis equivalent to learning a single threshold ?
forNIL predictions as in Bunescu and Pasca (2006).Incorporating NIL into the ranker has severaladvantages.
First, the ranker can set the thresh-old optimally without hand tuning.
Second, sincethe SVM scores are relative within a single exam-ple and cannot be compared across examples, set-ting a single threshold is difficult.
Third, a thresh-old sets a uniform standard across all examples,whereas in practice we may have reasons to favora NIL prediction in a given example.
We designfeatures for NIL prediction that cannot be cap-tured in a single parameter.6.1 NIL FeaturesIntegrating NIL prediction into learning meanswe can define arbitrary features indicative of NILpredictions in the feature vector corresponding toNIL.
For example, if many candidates have goodname matches, it is likely that one of them is cor-rect.
Conversely, if no candidate has high entry-text/article similarity, or overlap between factsand the article text, it is likely that the entity isabsent from the KB.
We included several features,such as a) the max, mean, and difference betweenmax and mean for 7 atomic features for all KBcandidates considered, b) whether any of the can-didate entries have matching names (exact andfuzzy string matching), c) whether any KB en-try was a top Wikitology match, and d) if the topGoogle match was not a candidate.282Micro-Averaged Macro-AveragedBest Median All Features Best Features Best Median All Features Best FeaturesAll 0.8217 0.7108 0.7984 0.7941 0.7704 0.6861 0.7695 0.7704non-NIL 0.7725 0.6352 0.7063 0.6639 0.6696 0.5335 0.6097 0.5593NIL 0.8919 0.7891 0.8677 0.8919 0.8789 0.7446 0.8464 0.8721Table 1: Micro and macro-averaged accuracy for TAC-KBP data compared to best and median reported performance.Results are shown for all features as well as removing a small number of features using feature selection on development data.7 EvaluationWe evaluated our system on two datasets: theText Analysis Conference (TAC) track on Knowl-edge Base Population (TAC-KBP) (McNamee andDang, 2009) and the newswire data used byCucerzan (2007) (Microsoft News Data).Since our approach relies on supervised learn-ing, we begin by constructing our own trainingcorpus.7 We highlighted 1496 named entity men-tions in news documents (from the TAC-KBP doc-ument collection) and linked these to entries ina KB derived from Wikipedia infoboxes.
8 Weadded to this collection 119 sample queries fromthe TAC-KBP data.
The total of 1615 training ex-amples included 539 (33.4%) PER, 618 (38.3%)ORG, and 458 (28.4%) GPE entity mentions.
Ofthe training examples, 80.5% were found in theKB, matching 300 unique entities.
This set has ahigher number of NIL entities than did Bunescuand Pasca (2006) (10%) but lower than the TAC-KBP test set (43%).All system development was done using a train(908 examples) and development (707 examples)split.
The TAC-KBP and Microsoft News datasets were held out for final tests.
A model trainedon all 1615 examples was used for experiments.7.1 TAC-KBP 2009 ExperimentsThe KB is derived from English Wikipedia pagesthat contained an infobox.
Entries contain basicdescriptions (article text) and attributes.
The TAC-KBP query set contains 3904 entity mentions for560 distinct entities; entity type was only providedfor evaluation.
The majority of queries were fororganizations (69%).
Most queries were missingfrom the KB (57%).
77% of the distinct GPEsin the queries were present in the KB, but for7Data available from www.dredze.com8http://en.wikipedia.org/wiki/Help:InfoboxPERs and ORGs these percentages were signifi-cantly lower, 19% and 30% respectively.Table 1 shows results on TAC-KBP data us-ing all of our features as well a subset of featuresbased on feature selection experiments on devel-opment data.
We include scores for both micro-averaged accuracy ?
averaged over all queries?
and macro-averaged accuracy ?
averaged overeach unique entity ?
as well as the best and me-dian reported results for these data (McNameeand Dang, 2009).
We obtained the best reportedresults for macro-averaged accuracy, as well asthe best results for NIL detection with micro-averaged accuracy, which shows the advantage ofour approach to learning NIL.
See McNamee etal.
(2009) for additional experiments.The candidate selection phase obtained a re-call of 98.6%, similar to that of development data.Missed candidates included Iron Lady, whichrefers metaphorically to Yulia Tymoshenko, PCC,the Spanish-origin acronym for the Cuban Com-munist Party, and Queen City, a former nicknamefor the city of Seattle, Washington.
The system re-turned a mean of 76 candidates per query, but themedian was 15 and the maximum 2772 (Texas).
Inabout 10% of cases there were four or fewer can-didates and in 10% of cases there were more than100 candidate KB nodes.
We observed that ORGswere more difficult, due to the greater variationand complexity in their naming, and that they canbe named after persons or locations.7.2 Feature EffectivenessWe performed two feature analyses on the TAC-KBP data: an additive study ?
starting from asmall baseline feature set used in candidate selec-tion we add feature groups and measure perfor-mance changes (omitting feature combinations),and an ablative study ?
starting from all features,remove a feature group and measure performance.283Class All non-NIL NILBaseline 0.7264 0.4621 0.9251Acronyms 0.7316 0.4860 0.9161NE Analysis 0.7661 0.7181 0.8022Google 0.7597 0.7421 0.7730Doc/KB Text Similarity 0.7313 0.6699 0.7775Wikitology 0.7318 0.4549 0.9399All 0.7984 0.7063 0.8677Table 2: Additive analysis: micro-averaged accuracy.Table 2 shows the most significant features inthe feature addition experiments.
The baselineincludes only features based on string similarityor aliases and is not effective at finding correctentries and strongly favors NIL predictions.
In-clusion of features based on analysis of named-entities, popularity measures (e.g., Google rank-ings), and text comparisons provided the largestgains.
The overall changes are fairly small,roughly ?1%; however changes in non-NIL pre-cision are larger.The ablation study showed considerable redun-dancy across feature groupings.
In several cases,performance could have been slightly improvedby removing features.
Removing all feature com-binations would have improved overall perfor-mance to 81.05% by gaining on non-NIL for asmall decline on NIL detection.7.3 Experiments on Microsoft News DataWe downloaded the evaluation data used inCucerzan (2007)9: 20 news stories from MSNBCwith 642 entity mentions manually linked toWikipedia and another 113 mentions not havingany corresponding link to Wikipedia.10 A sig-nificant percentage of queries were not of typePER, ORG, or GPE (e.g., ?Christmas?).
SERIFassigned entity types and we removed 297 queriesnot recognized as entities (counts in Table 3).We learned a new model on the training dataabove using a reduced feature set to increasespeed.11 Using our fast candidate selection sys-tem, we resolved each query in 1.98 seconds (me-dian).
Query processing time was proportional to9http://research.microsoft.com/en-us/um/people/silviu/WebAssistant/TestData/10One of the MSNBC news articles is no longer availableso we used 759 total entities.11We removed Google, FST and conjunction featureswhich reduced system accuracy but increased performance.Num.
Queries AccuracyTotal Nil All non-NIL NILNIL 452 187 0.4137 0.0 1.0GPE 132 20 0.9696 1.00 0.8000ORG 115 45 0.8348 0.7286 1.00PER 205 122 0.9951 0.9880 1.00All 452 187 0.9469 0.9245 0.9786Cucerzan (2007) 0.914 - -Table 3: Micro-average results for Microsoft data.the number of candidates considered.
We selecteda median of 13 candidates for PER, 12 for ORGand 102 for GPE.
Accuracy results are in Table3.
The high results reported for this dataset overTAC-KBP is primarily because we perform verywell in predicting popular and rare entries ?
bothof which are common in newswire text.One issue with our KB was that it was derivedfrom infoboxes in Wikipedia?s Oct 2008 versionwhich has both new entities, 12 and is missing en-tities.13 Therefore, we manually confirmed NILanswers and new answers for queries marked asNIL in the data.
While an exact comparison is notpossible (as described above), our results (94.7%)appear to be at least on par with Cucerzan?s sys-tem (91.4% overall accuracy).With the strong re-sults on TAC-KBP, we believe that this is strongconfirmation of the effectiveness of our approach.8 ConclusionWe presented a state of the art system to disam-biguate entity mentions in text and link them toa knowledge base.
Unlike previous approaches,our approach readily ports to KBs other thanWikipedia.
We described several important chal-lenges in the entity linking task including han-dling variations in entity names, ambiguity in en-tity mentions, and missing entities in the KB, andwe showed how to each of these can be addressed.We described a comprehensive feature set to ac-complish this task in a supervised setting.
Impor-tantly, our method discriminately learns when notto link with high accuracy.
To spur further re-search in these areas we are releasing our entitylinking system.122008 vs. 2006 version used in Cucerzan (2007) Wecould not get the 2006 version from the author or the Internet.13Since our KB was derived from infoboxes, entities nothaving an infobox were left out.284ReferencesJavier Artiles, Satoshi Sekine, and Julio Gonzalo.2008.
Web people search: results of the first evalu-ation and the plan for the second.
In WWW.Amit Bagga and Breck Baldwin.
1998.
Entity-based cross-document coreferencing using the vec-tor space model.
In Conference on ComputationalLinguistics (COLING).Michele Banko and Oren Etzioni.
2008.
The tradeoffsbetween open and traditional relation extraction.
InAssociation for Computational Linguistics.K.
Bollacker, C. Evans, P. Paritosh, T. Sturge, andJ.
Taylor.
2008.
Freebase: a collaboratively cre-ated graph database for structuring human knowl-edge.
In SIGMOD Management of Data.E.
Boschee, R. Weischedel, and A. Zamanian.
2005.Automatic information extraction.
In Conferenceon Intelligence Analysis.Razvan C. Bunescu and Marius Pasca.
2006.
Usingencyclopedic knowledge for named entity disam-biguation.
In European Chapter of the Assocationfor Computational Linguistics (EACL).Peter Christen.
2006.
A comparison of personal namematching: Techniques and practical issues.
Techni-cal Report TR-CS-06-02, Australian National Uni-versity.Silviu Cucerzan.
2007.
Large-scale named entitydisambiguation based on wikipedia data.
In Em-pirical Methods in Natural Language Processing(EMNLP).Markus Dreyer, Jason Smith, and Jason Eisner.
2008.Latent-variable modeling of string transductionswith finite-state methods.
In Empirical Methods inNatural Language Processing (EMNLP).Anthony Fader, Stephen Soderland, and Oren Etzioni.2009.
Scaling Wikipedia-based named entity dis-ambiguation to arbitrary web text.
In WikiAI09Workshop at IJCAI 2009.Thorsten Joachims.
2002.
Optimizing search enginesusing clickthrough data.
In Knowledge Discoveryand Data Mining (KDD).Martin Klein and Michael L. Nelson.
2008.
A com-parison of techniques for estimating IDF values togenerate lexical signatures for the web.
In Work-shop on Web Information and Data Management(WIDM).Fangtao Li, Zhicheng Zhang, Fan Bu, Yang Tang,Xiaoyan Zhu, and Minlie Huang.
2009.
THUQUANTA at TAC 2009 KBP and RTE track.
In TextAnalysis Conference (TAC).Gideon S. Mann and David Yarowsky.
2003.
Unsuper-vised personal name disambiguation.
In Conferenceon Natural Language Learning (CONLL).Andrew McCallum, Kamal Nigam, and Lyle Ungar.2000.
Efficient clustering of high-dimensional datasets with application to reference matching.
InKnowledge Discovery and Data Mining (KDD).Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the TAC 2009 knowledge base population track.In Text Analysis Conference (TAC).Paul McNamee, Mark Dredze, Adam Gerber, NikeshGarera, Tim Finin, James Mayfield, Christine Pi-atko, Delip Rao, David Yarowsky, and MarkusDreyer.
2009.
HLTCOE approaches to knowledgebase population at TAC 2009.
In Text Analysis Con-ference (TAC).Marius Pasca.
2008.
Turning web text and searchqueries into factual knowledge: hierarchical classattribute extraction.
In National Conference on Ar-tificial Intelligence (AAAI).Massimo Poesio, David Day, Ron Artstein, Jason Dun-can, Vladimir Eidelman, Claudio Giuliano, RobHall, Janet Hitzeman, Alan Jern, Mijail Kabadjov,Stanley Yong, Wai Keong, Gideon Mann, Alessan-dro Moschitti, Simone Ponzetto, Jason Smith, JosefSteinberger, Michael Strube, Jian Su, Yannick Ver-sley, Xiaofeng Yang, and Michael Wick.
2008.
Ex-ploiting lexical and encyclopedic resources for en-tity disambiguation: Final report.
Technical report,JHU CLSP 2007 Summer Workshop.Gerard Salton and Michael McGill.
1983.
Introduc-tion to Modern Information Retrieval.
McGraw-Hill Book Company.Erik Tjong Kim Sang and Fien De Meulder.
2003.
In-troduction to the conll-2003 shared task: Language-independent named entity recognition.
In Confer-ence on Natural Language Learning (CONLL).Zareen Syed, Tim Finin, and Anupam Joshi.
2008.Wikipedia as an ontology for describing documents.In Proceedings of the Second International Confer-ence on Weblogs and Social Media.
AAAI Press.285
