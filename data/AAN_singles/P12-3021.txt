Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 121?126,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsBuilding trainable taggers in a web-based, UIMA-supported NLPworkbenchRafal Rak, BalaKrishna Kolluru and Sophia AnaniadouNational Centre for Text MiningSchool of Computer Science, University of ManchesterManchester Interdisciplinary Biocentre131 Princess St, M1 7DN, Manchester, UK{rafal.rak,balakrishna.kolluru,sophia.ananiadou}@manchester.ac.ukAbstractArgo is a web-based NLP and text miningworkbench with a convenient graphical userinterface for designing and executing process-ing workflows of various complexity.
Theworkbench is intended for specialists and non-technical audiences alike, and provides theever expanding library of analytics compliantwith the Unstructured Information Manage-ment Architecture, a widely adopted interop-erability framework.
We explore the flexibil-ity of this framework by demonstrating work-flows involving three processing componentscapable of performing self-contained machinelearning-based tagging.
The three componentsare responsible for the three distinct tasks of 1)generating observations or features, 2) train-ing a statistical model based on the generatedfeatures, and 3) tagging unlabelled data withthe model.
The learning and tagging compo-nents are based on an implementation of con-ditional random fields (CRF); whereas the fea-ture generation component is an analytic ca-pable of extending basic token information toa comprehensive set of features.
Users de-fine the features of their choice directly fromArgo?s graphical interface, without resortingto programming (a commonly used approachto feature engineering).
The experimental re-sults performed on two tagging tasks, chunk-ing and named entity recognition, showed thata tagger with a generic set of features builtin Argo is capable of competing with task-specific solutions.1 IntroductionThe applications of automatic recognition of cate-gories, or tagging, in natural language processing(NLP), range from part of speech tagging to chunk-ing to named entity recognition and complex scien-tific discourse analyses.
Currently, there is a varietyof tools capable of performing these tasks.
A com-monly used approach involves the use of machinelearning to first build a statistical model based on amanually or semi-automatically tagged sample dataand then to tag new data using this model.
Sincethe machine learning algorithms for building mod-els are well established, the challenge shifted to fea-ture engineering, i.e., developing task-specific fea-tures that form the basis of these statistical models.This task is usually accomplished programmaticallywhich pose an obstacle to a non-technically inclinedaudience.
We alleviate this problem by demonstrat-ing Argo1, a web-based platform that allows the userto build NLP and other text analysis workflows viaa graphical user interface (GUI) available in a webbrowser.
The system is equipped with an ever grow-ing library of text processing components rangingfrom low-level syntactic analysers to semantic an-notators.
It also allows for including user-interactivecomponents, such as an annotation editor, into oth-erwise fully automatic workflows.
The interoper-ability of processing components is ensured in Argoby adopting Unstructured Information ManagementArchitecture (UIMA) (Ferrucci and Lally, 2004) asthe system?s framework.
In this work we explore thecapabilities of this framework to support machine1http://nactem.ac.uk/Argo121learning components for tagging textual content.In the following section we present related work.Section 3 provides background information on Argoand its relationship to UIMA.
The details of the threemachine learning components are discussed in Sec-tion 4.
Section 5 provides evaluation, whereas Sec-tion 6 concludes the paper.2 Related workLanguage processing tools with machine learningcapabilities for tagging textual content have beendistributed by various groups in form of either stan-dalone applications or application programming in-terfaces (API).
Packages such as Lingpipe2, Mal-let3, Stanford NLP tools4 and OpenNLP5 have beenextensively used by the NLP and text mining com-munities (Kolluru et al, 2011; Corbett and Murray-Rust, 2006).
However, such tools inherently imposeinconveniences on users, such as a lack of GUI, of-ten arduous manual installation procedures, profi-ciency in programming or familiarity with the de-tails of machine learning algorithms.These limitations are overcome by GUI-equipped,workflow-supporting platforms that often directlyuse the solutions provided by the former tools.
Thenotable examples of such platforms designed specif-ically for NLP and text mining tasks are GATE(Cunningham et al, 2002), a suite of text process-ing and annotation tools, and U-Compare (Kano etal., 2010), a standalone application supporting theUIMA framework that formed the inspiration forArgo.Although the GUI platforms provide machinelearning solutions, these are usually limited to us-ing pre-trained models and providing a rich set offeatures for training requires resorting to program-ming.
Argo, on the other hand, allows the users totrain their own models with either a generic set offeatures or customisable features without having towrite a single line of code.
This capability is pro-vided in Argo entirely through its GUI.2http://alias-i.com/lingpipe3http://mallet.cs.umass.edu4http://nlp.stanford.edu/software/index.shtml5http://opennlp.apache.orgFigure 1: Screen capture of Argo?s web-based inter-face.3 Argo and UIMAArgo?s main user interface consists of three panelsas shown in Figure 1.
The left-hand panel includesuser-owned or shared storable objects; the middlepanel is a drawing space for constructing workflowsand the right-hand panel displays context-dependentinformation.
The storable objects are categorisedinto workflows, represented as block diagrams ofinterconnected processing components, documentsthat represent the user?s space intended for upload-ing resources and saving processing results, and ex-ecutions that provide past and live workflow exe-cution details and access points to user-interactivecomponents should such be present in a workflow.Component interoperability in Argo is ensured byUIMA which defines common structures and inter-faces.
A typical UIMA processing pipeline consistsof a collection reader, a set of analysis engines and aconsumer.
The role of a collection reader is to fetcha resource (e.g., a text document) and deposit it ina common annotation structure, or CAS, as the sub-ject of annotation.
Analysis engines then process thesubject of annotation stored in the CAS and populatethe CAS with their respective annotations.
The con-sumer?s role is to transform some or all of the an-notations and/or the subject of annotation from theCAS and serialise it into some storable format.Readers, analysers and consumers are representedgraphically in Argo as blocks with incoming only,incoming and outgoing, and outgoing only ports, re-spectively, visible in the middle of Figure 1.122(a) Training (b) TaggingFigure 2: Two generic workflows demonstratingthe use of the Feature Generator component for (a)training and (b) tagging.4 Machine learning components in ArgoIn order to ensure flexibility in building workflows,we split the machine learning capability into threedistinct processing components, namely feature gen-erator, model trainer and tagger.
The trainer andthe tagger are intrinsic machine learning compo-nents, whereas the feature generator is a convenientand customisable processing component capable ofbuilding a feature space for a user-defined domain.From UIMA?s perspective, the feature generatorand the tagger are both analysis engines whose pur-pose is to analyse the incoming CASes and en-rich them with additional annotations; whereas thetrainer is a consumer that transforms the informationstored in CASes into a statistical model.A typical use of the three components is shownin Figure 2.
The three components are repre-sented as the Feature Generator, CRF++ Trainer andCRF++ Tagger blocks.
Figure 2a shows a pro-cess of building a statistical model supported bya document reader, common, well-established pre-processing components (in this case, to establishboundaries of sentences and tokens), and the previ-ously mentioned editor for manually creating anno-tations6.
The manual annotations serve to generatetags/labels which are used in the training process to-gether with the features produced by Feature Gener-ator.
The trained model is then used in the workflowshown in Figure 2b to tag new resources.
Althoughthe tagging workflow automatically recognises thelabels of interest (based on the model supplied inCRF++ Tagger), in practice, the labels need furthercorrection, hence the use of Annotation Editor afterthe tagger.4.1 Training and taggingAt present, our implementation of the training andtagging components is based on the conditional ran-dom fields (CRF) (Lafferty et al, 2001).
Our choiceis dictated by the fact that CRF models are currentlyone of the best models for tagging and efficient algo-rithms to compute marginal probabilities and n-bestsequences are freely available.We used the CRF++ implementation7 andwrapped it into two UIMA-compatible components,CRF++ Trainer and CRF++ Tagger.
The trainerdeals with the optimisation of feature parameters,whereas word observations are produced by FeatureGenerator, as described in the following section.4.2 From annotations to featuresThe Feature Generator component is an intermedi-ary between annotations stored in CASes and thetraining component.
This component is customis-able via the component?s settings panel, parts ofwhich are shown in Figure 3.
The panel allows theuser to 1) identify the stream of tokens8 (Figure 3a),2) identify the stream of token sequences (usually6The preprocessing and manual annotation componentscould be replaced with CAS Reader, a component capable ofsupplying the workflow with a previously annotated set of doc-uments.7http://code.google.com/p/crfpp/8The definition of token depends on the selected UIMA an-notation type.
It may range from a simple span of text to acomplex lexical or semantic structure.123(a) Selecting a token annotation type(b) Defining featuresFigure 3: Feature Generator settings panel allowsthe user to (a) select labels for machine learning and(b) define features.sentences), and 3) define features or token observa-tions (Figure 3b).Each feature definition consists of a name, a tokenfield, an optional list of token field transformations,and an optional set of context windows.
The nameis only for the user?s convenience of identifying in-dividual feature definitions.
The token field is theprimary subject of transformations (if any) and it isone of the data fields of the selected token annota-tion type.
For instance, the token annotation typemay define data fields such as part of speech, chunk,or lemma.
By default, the system selects ?coveredtext?, i.e., the span of text covered by an annotation,since this data field is available for any annotation.If no transformation is declared, the string rep-Figure 4: UML diagram of transformation typesresentation of the token field?s value ultimately be-comes the value of the generated feature.
If theuser declares one or more transformations then theseare applied on the token field?s value in sequence,i.e., an outcome of the preceding transformation be-comes an input of the following one.
Figure 4 showsthe various transformations currently available in thesystem.Context windows allow for enriching the currenttoken?s feature set by introducing observations fromsurrounding tokens as n-grams.
For example, theselected feature definition in Figure 3b, ?surface hassymbols?, declares the covered text as the feature?sbasis and defines two transformations and two con-text windows.
The two transformations will firsttransform the covered text to a collapsed shape (e.g.,?NF-kappa?
will become ?A#a?)
and then produce?Y?
or ?N?
depending on whether the collapsedshape matches the simple regular expression ?#?
(e.g., ?A#a?
will become ?Y?).
The two context win-dows define six unigrams and four bigrams, whichwill ultimately result in this single feature defini-tion?s producing ten observations for training.5 EvaluationWe show the performance of taggers trained withtwo distinct sets of features, basic and extended.The basic set of features uses token fields such asthe covered text and the part of speech without anytransformations or context n-grams.
The extendedset makes the full use of Feature Generator?s settingsand enriches the basic set with various transforma-tions and context n-grams.
The transformations in-124Dataset Setup P R FCoNLL Best 94.29 94.01 94.13L2 IOBES 92.20 93.43 92.81L2 IOB 92.14 93.27 92.70L1 IOBES 91.95 93.17 92.55L1 IOB 91.83 93.11 92.46Baseline 72.58 82.14 77.07BioNLP/ Best 76.00 69.40 72.6NLPBA L1 IOBES 66.22 65.06 65.63L2 IOB 66.06 64.87 65.46L1 IOB 66.05 64.61 65.32L2 IOBES 65.77 64.79 65.28Baseline 52.60 43.60 47.70Table 1: Performance of various setups (L1 vs L2,and IOB vs IOBES) on the chunking and NER tasks.The setups are ordered by F-score.Dataset Setup P R FCoNLL Basic 73.80 84.50 78.78Extended 92.20 93.43 92.81BioNLP/ Basic 37.06 48.13 41.88NLPBA Extended 66.22 65.06 65.63Table 2: Comparison of setups with basic and ex-tended features for the chunking and NER tasks.clude surface shape, length, prefixes, suffixes, andthe presence of various combinations of letters, dig-its and symbols.
The context n-grams include uni-grams for all feature definitions and bigrams for se-lected ones.
Figure 3b shows a sample of the actualextended set.We use two datasets, one prepared for the CoNLL2000 shared task (Tjong et al, 2000) and anotherprepared for the BioNLP/NLPBA 2004 shared task(Kim et al, 2004).
They represent two differenttagging tasks, chunking and named entity recog-nition, respectively.
The CoNLL 2000 chunkingdataset involves 10 labels and comes pre-tokenisedwith 211,727 tokens in the training set and 47,377tokens in the test set.
The dataset alo provides part-of-speech tags for each token.
The BioNLP/NLPBA2004 named entity recognition dataset involves fivebiology-related labels and consists of 472,006 and96,780 tokens in the training and testing sets, re-spectively.
Contrary to the former dataset, there isno other information supporting the tokens in theBioNLP/NLPBA dataset.
To compensate for it weautomatically generated part of speech and chunk la-bels for each token.The chosen datasets/tasks are by no means anexhaustive set of representative comparative-setupdatasets available.
Our goal is not to claim the su-periority of our approach over the solutions reportedin the respective shared tasks.
Instead, we aim toshow that our generic setup is comparable to thosetask-tuned solutions.We further explore the options of both FeatureGenerator and CRF++ Trainer by manipulating la-belling formats (IOB vs IOBES (Kudo and Mat-sumoto, 2001)) for the former and parameter esti-mation algorithms (L2- vs L1-norm regularisation)for the latter.
Ultimately, there are 32 setups as theresult of the combinations of the two feature sets, thetwo datasets, the two labelling formats and the twoestimation algorithms.5.1 ResultsTable 1 shows the precision, recall and f-scores ofour extended-feature setups against each other aswell as with reference to the best and baseline solu-tions as reported in the respective shared tasks.
Thegap to the best performing solution for the chunkingtask is about 1.3% points in F-score, ahead of thebaseline by 15.7% points.
Respectively for the NERtask, our best setup stands behind the best reportedsolution by about 7% points, ahead of the baselineby about 18% points.
In both instances our solutionwould be placed in the middle of the reported rank-ings, which is a promising result, especially that oursetups are based solely on the tokens?
surface form,part of speech, and (in the case of the NER task)chunk.
In contrast, the best solutions for the NERtask involve the use of dictionaries and advancedanalyses such as acronym resolution.The tested combinations of the labelling formatsand parameter estimation algorithms showed to beinconclusive, with a difference between the best andworst setups of only 0.35% points for both tasks.The advantage of using the extended set of fea-tures over the basic set is clearly illustrated in Table2.
The performance of the basic set on the chunkingdataset is only at the level of the baseline, whereasfor the NER task it falls nearly 6% points behind the125Dataset Setup L2 L1CoNLL Extended IOB 555 187Basic IOB 134 70Extended IOBES 528 209Basic IOBES 139 72BioNLP/ Extended IOB 865 179NLPBA Basic IOB 226 72Extended IOBES 860 201Basic IOBES 217 79Table 3: Number of iterations needed for the optimi-sation algorithm to converge.baseline (which comes as no surprise given that thebaseline system is a string match of entities found inthe training set).Table 3 shows the number of iterations9 neededfor the optimisation algorithm of the trainer to con-verge.
The advantage of the L1 regularisation isapparent with nearly two to five times less itera-tions needed when compared to the L2 regularisa-tion.
Given the close F-scores achieved by the twofamily of setups, the L1 regularisation becomes aclear winner in our experimentation setup.6 ConclusionsArgo?s strength is manifested by its online avail-ability, an intuitive graphical user interface availablefrom a web browser, convenience in building evenmost complex text processing workflows, and theavailability of trainable machine learning compo-nents.
The Feature Generator component, customis-able entirely through a GUI, provides the flexibilityneeded to extend the basic set of features withoutresorting to programming.
The experiment resultsshowed that an extended, yet generic, set of featurescan be taken to competitive levels in terms of effec-tiveness.7 AcknowledgementsThis work was partially supported by Biotechnol-ogy and Biological Sciences Research Council (BB-9We do not report detailed CPU times due to experimentingon resource-shared machines.
Such a setup makes direct side-by-side comparisons largely skewed.
As a reference we notethat the workflows completed in 15 minutes to about 11 hoursdepending on a feature space size and machine load.SRC BB/G53025X/1 From Text to Pathways) andKorea Institute of Science and Technology Informa-tion (KISTI Text Mining and Pathways).ReferencesP.
Corbett and P. Murray-Rust.
2006.
High-throughputidentification of chemistry in life science texts.
CompLife, pages 107?118.
LNBI 4216.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
GATE: A framework and graphi-cal development environment for robust NLP tools andapplications.
In Proc.
of the 40th Anniversary Meetingof the Association for Computational Linguistics.D.
Ferrucci and A. Lally.
2004.
UIMA: An Architec-tural Approach to Unstructured Information Process-ing in the Corporate Research Environment.
NaturalLanguage Engineering, 10(3-4):327?348.Y.
Kano, R. Dorado, L. McCrochon, S. Ananiadou, andJ.
Tsujii.
2010.
U-Compare: An integrated languageresource evaluation platform including a comprehen-sive UIMA resource library.
In Proc.
of the SeventhInternational Conference on Language Resources andEvaluation (LREC 2010), pages 428?434.J.-D. Kim, T. Ohta, Y. Tsuruoka, Y. Tateisi, and N. Col-lier.
2004.
Introduction to the bio-entity recogni-tion task at jnlpba.
In Proc.
of the InternationalJoint Workshop on Natural Language Processing inBiomedicine and its Applications, JNLPBA ?04, pages70?75, Geneva, Switzerland.
Association for Compu-tational Linguistics.B.
Kolluru, S. Nakjang, R. P. Hirt, A. Wipat, and S. Ana-niadou.
2011.
Automatic extraction of microorgan-isms and their habitats from free text using text min-ing workflows.
Journal of Integrative Bioinformatics,8(2):184.T.
Kudo and Y. Matsumoto.
2001.
Chunking with sup-port vector machines.
In Proc.
of the second meetingof the North American Chapter of the Association forComputational Linguistics on Language technologies,NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
Asso-ciation for Computational Linguistics.J.
Lafferty, A. Mccallum, and F. Pereira.
2001.
Condi-tional Random Fields: Probabilistic Models for Seg-menting and Labeling Sequence Data.
In Proc.
18thInternational Conf.
on Machine Learning, pages 282?289.
Morgan Kaufmann, San Francisco, CA.K.
S. Tjong, F. Erik, and S. Buchholz.
2000.
Introduc-tion to the CoNLL-2000 shared task: chunking.
InProc.
of the 2nd workshop on Learning language inlogic and the 4th Conference on Computational nat-ural language learning, pages 127?132, Morristown,NJ, USA.
Association for Computational Linguistics.126
