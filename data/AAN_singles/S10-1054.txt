Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 242?247,Uppsala, Sweden, 15-16 July 2010.c?2010 Association for Computational LinguisticsUBA: Using Automatic Translation and Wikipedia forCross-Lingual Lexical SubstitutionPierpaolo BasileDept.
of Computer ScienceUniversity of Bari ?Aldo Moro?Via E. Orabona, 470125 Bari (ITALY)basilepp@di.uniba.itGiovanni SemeraroDept.
of Computer ScienceUniversity of Bari ?Aldo Moro?Via E. Orabona, 470125 Bari (ITALY)semeraro@di.uniba.itAbstractThis paper presents the participation of theUniversity of Bari (UBA) at the SemEval-2010 Cross-Lingual Lexical SubstitutionTask.
The goal of the task is to substi-tute a word in a language Ls, which oc-curs in a particular context, by provid-ing the best synonyms in a different lan-guage Ltwhich fit in that context.
Thistask has a strict relation with the task ofautomatic machine translation, but thereare some differences: Cross-lingual lexi-cal substitution targets one word at a timeand the main goal is to find as many goodtranslations as possible for the given tar-get word.
Moreover, there are some con-nections with Word Sense Disambiguation(WSD) algorithms.
Indeed, understand-ing the meaning of the target word is nec-essary to find the best substitutions.
Animportant aspect of this kind of task isthe possibility of finding synonyms with-out using a particular sense inventory or aspecific parallel corpus, thus allowing theparticipation of unsupervised approaches.UBA proposes two systems: the former isbased on an automatic translation systemwhich exploits Google Translator, the lat-ter is based on a parallel corpus approachwhich relies on Wikipedia in order to findthe best substitutions.1 IntroductionThe goal of the Cross-Lingual Lexical Substitu-tion (CLLS) task is to substitute a word in a lan-guage Ls, which occurs in a particular context,by providing the best substitutions in a differentlanguage Lt.
In SemEval-2010 the source lan-guage Lsis English, while the target language Ltis Spanish.
Clearly, this task is related to LexicalSubstitution (LS) (McCarthy and Navigli, 2007)which consists in selecting an alternative word fora given one in a particular context by preservingits meaning.
The main difference between the LStask and the CLLS one is that in LS source andtarget languages are the same.
CLLS is not a easytask since neither a list of candidate words nor aspecific parallel corpus are supplied by the orga-nizers.
However, this opens the possibility of us-ing several knowledge sources, instead of a singleone fixed by the task organizers.
Therefore, thesystem must identify a set of candidate words inLtand then select only those words which fit thecontext.
From another point of view, the cross-lingual nature of the task allows to exploit auto-matic machine translation methods, hence the goalis to find as many good translations as possible forthe given target word.
A thorough description ofthe task can be found in (Mihalcea et al, 2010;Sinha et al, 2009).To easily understand the task, an example fol-lows.
Consider the sentence:During the siege, George Robertson hadappointed Shuja-ul-Mulk , who was a brightboy only 12 years old and the youngestsurviving son of Aman-ul-Mulk, as the rulerof Chitral.In the previous sentence the target word is?bright?.
Taking into account the meaning ofthe word ?bright?
in this particular context, thebest substitutions in Spanish are: ?inteligente?,?brillante?
and ?listo?.We propose two systems to tackle the problemof CLLS: the first is based on an automatic trans-lation system which exploits the API of GoogleTranslator1, the second is based on a parallel cor-pus approach which relies on Wikipedia.
In par-ticular, in the second approach we use a struc-tured version of Wikipedia called DBpedia (Bizer1http://code.google.com/p/google-api-translate-java/242et al, 2009).
Both systems adopt several lexicalresources to select the list of possible substitutionsfor a given word.
Specifically, we use three differ-ent dictionaries: Google Dictionary, Babylon Dic-tionary and Spanishdict.
Then, we combine thedictionaries into a single one, as described in Sec-tion 2.1.The paper is organized as follows: Section 2 de-scribes the strategy we adopted to tackle the CLLStask, while results of an experimental session wecarried out in order to evaluate the proposed ap-proaches are presented in Section 3.
Conclusionsare discussed in Section 4.2 MethodologyGenerally speaking, the problem of CLLS can becoped with a strategy which consists of two steps,as suggested in (Sinha et al, 2009):?
candidate collection: in this step several re-sources are queried to retrieve a list of po-tential translation candidates for each targetword and part of speech;?
candidate selection: this step concerns theranking of potential candidates, which are themost suitable ones for each instance, by usinginformation about the context.Regarding the candidate collection, we exploitthree dictionaries: Google Dictionary, BabylonDictionary and Spanishdict.
Each dictionary ismodeled using a strategy described in Section 2.1.We use the same approach to model each dictio-nary in order to make easy both the inclusion of fu-ture dictionaries and the integration with the can-didate selection step.Candidate selection is performed in two dif-ferent ways.
The first one relies on the auto-matic translation of the sentence in which the tar-get word occurs, in order to find the best substitu-tions.
The second method uses a parallel corpusbuilt on DBpedia to discover the number of doc-uments in which the target word is translated byone of the potential translation candidates.
Detailsabout both methods are reported in Section 2.22.1 Candidate collectionThis section describes the method adopted to re-trieve the list of potential translation candidates foreach target word and part of speech.Our strategy combines several bi-lingual dictio-naries and builds a single list of candidates foreach target word.
The involved dictionaries meetthe following requirements:1. the source language Lsmust be English andthe target one Ltmust be Spanish;2. each dictionary must provide informationabout the part of speech;3. the dictionary must be freely available.Moreover, each candidate has a score sijcom-puted by taking into account its rank in the listof possible translations supplied by the i ?
thdictionary.
Formally, let us denote by D ={d1, d2, .
.
.
, dn} the set of n dictionaries and byLi= {c1, c2, .
.
.
, cmi} the list of potential candi-dates provided by di.
The score sijis computedby the following equation:sij= 1?jmij ?
{1, 2, .
.
.
,mi} (1)Since each list Lihas a different size, we adopta score normalization strategy based on Z-scoreto merge the lists in a unique one.
Z-score nor-malizes the scores according to the average ?
andstandard deviation ?.
Given the list of scoresL = {s1, s2, .
.
.
, sn}, ?
and ?
are computed onL and the normalized score is defined as:si=si?
??
(2)Then, all the lists Liare merged in a single listM .
The list M contains all the potential candi-dates belonging to all the dictionaries with the re-lated score.
If a candidate occurs in more than onedictionary, only the occurrence with the maximumscore is chosen.At the end of the candidate collection step thelist M of potential translation candidates for eachtarget word is computed.
It is important to pointout that the list M is sorted and supplies an initialrank, which can be then modified by the candidateselection step.2.2 Candidate selectionWhile the candidate collection step is common tothe two proposed systems, the problem of candi-date selection is faced by using different strategiesin the two systems.243The first system, called unibaTranslate,uses a method based on google-api-translate-java2.The main idea behind unibaTranslate is tolook for a potential candidate in the translation ofthe target sentence.
Sometimes, no potential can-didates occur into the translation.
When this hap-pens the system uses some heuristics to discover apossible translation.For example, given the target word ?raw?
andthe potential candidates M ={puro, crudo, sin re-finar, de baja calidad, agrietado, al natural, bozal,asado, frito and bruto}, the two possible scenariosare:1. a potential candidate occurs into the transla-tion:?
Sen: The raw honesty of that basiccrudeness makes you feel stronger in away.?
Ses: La cruda honestidad de esacrudeza de base que te hace sentir masfuerte en un camino.2.
no potential candidates occur into the trans-lation, but a correct translation of the targetword is provided:?
Sen: Many institutional investors arenow deciding that they are getting a rawdeal from the company boards of Aus-tralia.?
Ses: Muchos inversores institucionalesestan ahora decidiendo que estan recibi-endo un trato injusto de los directoriosde las empresas de Australia.In detail, the strategy can be split in severalsteps:1.
Retrieve the list M of potential translationcandidates using the method described inSection 2.1.2.
Translate the target sentence Senfrom En-glish to Spanish, using the google-api-translate-java, which results into the sentenceSes.3.
Enrich M by adding multiword expressions.To implement this step, the two bigramswhich contain the target word and the onlytrigram in which the target word is the 2ndterm are taken into to account.2http://code.google.com/p/google-api-translate-java/Coming back to the first sentence in the previ-ous example, the following n-grams are built:?the raw?, ?raw honesty?
and ?the raw hon-esty?.
For each n-gram, candidate transla-tions are looked for using Google Dictionary.If translations are found, they are added toMwith an initial score equal to 0.4.
Fix a window W3of n words to the right andto the left of the target word, and perform thefollowing steps:(a) for each candidate ckin M , try to findckin W .
If ckoccurs in W , then add 2to the score of ckin M ;(b) if no exact match is found in the previ-ous step, perform a new search by com-paring ckwith the words in W usingthe Levenshtein distance4(Levenshtein,1966).
If the Levenshtein distance isgreater than 0.8, then add 2 to the scoreof ckin M .5.
If no exact/partial match is found in the pre-vious steps, probably the target word is trans-lated with a word which does not belong toM .
To overcome this problem, we implementa strategy able to discover a possible transla-tion in Seswhich is not in M .
This approachinvolves three steps:(a) for each word wiin Sen, a list of poten-tial translations Piis retrieved;(b) if a word in Piis found in Ses, the wordis removed from Ses5;(c) at this point, Sescontains a list R ofwords with no candidate translations.
Ascore is assigned to those words by tak-ing into account their position in Seswith respect to the position of the targetword in Sen, using the following equa-tion:1?|posc?
post|Lmax(3)where poscis the translation candidateposition in Ses, postis the target wordposition in Senand Lmaxis the maxi-mum length between the length of Senand Ses.3The window W is the same for both Senand Ses.4A normalized Levenshtein distance is adopted to obtaina value in [0, 1].5A partial match based on normalized Levenshtein dis-tance is implemented.244Moreover, the words not semanti-cally related to the potential candidates(found using Spanish WordNet6) are re-moved fromR.
In detail, for each candi-date in M a list of semantically relatedwords in Spanish WordNet7is retrievedwhich results in a set WN of relatedwords.
Words in R but not in WN areremoved from R. In the final step, thelist R is sorted and the first word in R isadded toM assigning a score equal to 2.6.
In the last step, the list M is sorted.
The out-put of this process is the ranked list of poten-tial candidates.It is important to underline that both SenandSesare tokenized, part-of-speech tagged and lem-matized.
Lemmatization plays a key role in thematching step, while part-of-speech tagging isneeded to query both the dictionaries and theSpanish WordNet.
We adopt META (Basile et al,2008) and FreeLing (Atserias et al, 2006) to per-form text processing for English and Spanish re-spectively.The second proposed system, calledunibaWiki, is based on the idea of automati-cally building a parallel corpus from Wikipedia.We use a structured version of Wikipedia calledDBpedia (Bizer et al, 2009).
The main idea be-hind DBpedia is to extract structured informationfrom Wikipedia and then to make this informationavailable.
The main goal is to have access easilyto the large amount of information in Wikipedia.DBpedia opens new and interesting ways to useWikipedia in NLP applications.In CLLS task, we use the extended abstracts ofEnglish and Spanish provided by DBpedia.
Foreach extended abstract in Spanish which has thecorresponding extended abstract in English, webuild a document composed by two fields: the for-mer contains the English text (texten) and the lat-ter contains the Spanish text (textes).
We adoptLucene8as storage and retrieval engine to makethe documents access fast and easy.The idea behind unibaWiki is to count, foreach potential candidate, the number of docu-ments in which the target word occurs in textenand the potential candidate occurs in textes.
A6http://www.lsi.upc.edu/?nlp/projectes/ewn.html7The semantic relations of hyperonymy, hyponymy and?similar to?
are exploited.8http://lucene.apache.org/score equal to the number of retrieved documentsis assigned, then the candidates are sorted accord-ing to that score.Given the list M of potential candidates and thetarget word t, for each ck?
M we perform thefollowing query:texten: t AND textes: ckwhere the field name is followed by a colon andby the term you are looking for.It is important to underline here that multiwordexpressions require a specific kind of query.
Foreach multiword expression we adopt the Phrase-Query which is able to retrieve documents thatcontain a specific sequence of words instead of asingle keyword.2.3 ImplementationTo implement the candidate collection step we de-veloped a Java application able to retrieve infor-mation from dictionaries.
For each dictionary, adifferent strategy has been adopted.
In particular:1.
Google Dictionary: Google Dictionary web-site is queried by using the HTTP protocoland the answer page is parsed;2.
Spanishdict: the same strategy adopted forGoogle Dictionary is used for the Spanishdictwebsite9;3.
Babylon Dictionary: the original file avail-able from the Babylon website10is convertedto obtain a plain text file by using the Unixutility dictconv.
After that, an applicationqueries the text file in an efficient way bymeans of a hash map.Both candidate selection systems are developedin Java.
Regarding the unibaWiki system, weadopt Lucene to index DBpedia abstracts.
Theoutput of Lucene is an index of about 680 Mbytes,277,685 documents and about 1,500,000 terms.3 EvaluationThe goal of the evaluation is to measure the sys-tems?
ability to find correct Spanish substitutionsfor a given word.
The dataset supplied by the or-ganizers contains 1,000 instances in XML format.9http://www.spanishdict.com/10www.babylon.com245Moreover, the organizers provide trial data com-posed by 300 instances to help the participantsduring the development of their systems.The systems are evaluated using two scoringtypes: best scores the best guessed substitution,while out-of-ten (oot) scores the best 10 guessedsubstitutions.
For each scoring type, precision (P)and recall (R) are computed.
Mode precision (P-mode) and mode recall (R-mode) calculate preci-sion and recall against the substitution chosen bythe majority of the annotators (if there is a ma-jority), respectively.
Details about evaluation andscoring types are provided in the task guidelines(McCarthy et al, 2009).Results of the evaluation using trial data arereported in Table 1 and Table 2.
Our systemsare tagged as UBA-T and UBA-W, which de-note unibaTranslate and unibaWiki, re-spectively.
Systems marked as BL-1 and BL-2are the two baselines provided by the organiz-ers.
The baselines use Spanishdict dictionary toretrieve candidates.
The system BL-1 ranks thecandidates according to the order returned on theonline query page, while the BL-2 rank is based oncandidate frequencies in the Spanish Wikipedia.Table 1: best results (trial data)System P R P-mode R-ModeBL-1 24.50 24.50 51.80 51.80BL-2 14.10 14.10 28.38 28.38UBA-T 26.39 26.39 59.01 59.01UBA-W 22.18 22.18 48.65 48.65Table 2: oot results (trial data)System P R P-mode R-ModeBL-1 38.58 38.58 71.62 71.62BL-2 37.83 37.83 68.02 68.02UBA-T 44.16 44.16 78.38 78.38UBA-W 45.15 45.15 72.52 72.52Results obtained using trial data show that oursystems are able to overcome the baselines.
Onlythe best score achieved by UBA-W is below BL-1.Moreover, our strategy based on Wikipedia (UBA-W) works better than the one proposed by the or-ganizers (BL-2).Results of the evaluation using test data are re-ported in Table 3 and Table 4, which include all theparticipants.
Results show that UBA-T obtains thehighest recall using best scoring strategy.
More-over, both systems UBA-T and UBA-W achievethe highest R-mode and P-mode using oot scoringstrategy.
It is worthwhile to point out that the pres-ence of duplicates affect recall (R) and precision(P), but not R-mode and P-mode.
For this reasonsome systems, such as SWAT-E, obtain very highrecall (R) and low R-mode using oot scoring.
Du-plicates are not produced by our systems, but weperformed an a posteriori experiment in which du-plicates are allowed.
In that experiment, the firstcandidate provided by UBA-T has been duplicatedten times in the results.
Using that strategy, UBA-Tachieves a recall (and precision) equal to 271.51.This experiment proves that also our system is ableto obtain the highest recall when duplicates are al-lowed into the results.
Moreover, it is important tounderline here that we do not know how other par-ticipants generate duplicates in their results.
Weadopted a trivial strategy to introduce duplicates.Table 3: best results (test data)System P R P-mode R-ModeBL-1 24.34 24.34 50.34 50.34BL-2 15.09 15.09 29.22 29.22UBA-T 27.15 27.15 57.20 57.20UBA-W 19.68 19.68 39.09 39.09USPWLV 26.81 26.81 58.85 58.85Colslm 27.59 25.99 59.16 56.24WLVUSP 25.27 25.27 52.81 52.81SWAT-E 21.46 21.46 43.21 43.21UvT-v 21.09 21.09 43.76 43.76CU-SMT 21.62 20.56 45.01 44.58UvT-g 19.59 19.59 41.02 41.02SWAT-S 18.87 18.87 36.63 36.63ColEur 19.47 18.15 40.03 37.72IRST-1 22.16 15.38 45.95 33.47IRSTbs 22.51 13.21 45.27 28.26TYO 8.62 8.39 15.31 14.95Table 4: oot results (test data)System P R P-mode R-ModeBL-1 44.04 44.04 73.53 73.53BL-2 42.65 42.65 71.60 71.60UBA-T 47.99 47.99 81.07 81.07UBA-W 52.75 52.75 83.54 83.54USPWLV 47.60 47.60 79.84 79.84Colslm 46.61 43.91 69.41 65.98WLVUSP 48.48 48.48 77.91 77.91SWAT-E 174.59 174.59 66.94 66.94UvT-v 58.91 58.91 62.96 62.96UvT-g 55.29 55.29 73.94 73.94SWAT-S 97.98 97.98 79.01 79.01ColEur 44.77 41.72 71.47 67.35IRST-1 33.14 31.48 58.30 55.42IRSTbs 29.74 8.33 64.44 19.89TYO 35.46 34.54 59.16 58.02FCC-LS 23.90 23.90 31.96 31.96Finally, Table 5 reports some statistics aboutUBA-T and the number of times (N) the candi-246date translation is taken from Spanish WordNet(Spanish WN) or multiword expressions (Multi-word exp.).
The number of instances in whichthe candidate is a correct substitution is reportedin column C. Analyzing the results we note thatmost errors are due to part-of-speech tagging.
Forexample, given the following sentence:Sen: You will still be responsible for the shippingand handling fees, and for the cost of return-ing the merchandise.Ses: Usted seguira siendo responsable de los gas-tos de envio y manipulacion y, para los gastosde devolucion de la mercancia.where the target word is the verb return.
In thiscase the verb is used as noun and the algorithmsuggests correctly devolucion (noun) as substitu-tion instead of devolver (verb).
The gold standardprovided by the organizers contains devolver assubstitution and there is no match between devolu-cion and devolver during the scoring.Table 5: UBA-T statistics.Strategy N CSpanish WN 34 11Multiword exp.
21 114 ConclusionsWe described our participation at SemEval-2Cross-Lingual Lexical Substitution Task, propos-ing two systems called UBA-T and UBA-W. Thefirst relies on Google Translator, the secondis based on DBpedia, a structured version ofWikipedia.
Moreover, we exploited several dictio-naries to retrieve the list of candidate substitutions.UBA-T achieves the highest recall among allthe participants to the task.
Moreover, the resultsproved that the method based on Google Transla-tor is more effective than the one based on DBpe-dia.AcknowledgmentsThis research was partially funded by RegionePuglia under the contract POR PUGLIA 2007-2013 - Asse I Linea 1.1 Azione 1.1.2 - Bando?Aiuti agli Investimenti in Ricerca per le PMI?
-Fondo per le Agevolazioni alla Ricerca, project ti-tle: ?Natural Browsing?.ReferencesJordi Atserias, Bernardino Casas, Elisabet Comelles,Meritxell Gonz?alez, Llu?
?s Padr?o, and Muntsa Padr?o.2006.
FreeLing 1.3: Syntactic and semantic servicesin an open-source NLP library.
In Proceedings ofthe 5th International Conference on Language Re-sources and Evaluation (LREC06), pages 48?55.Pierpaolo Basile, Marco de Gemmis, Anna Lisa Gen-tile, Leo Iaquinta, Pasquale Lops, and Giovanni Se-meraro.
2008.
META - MultilanguagE Text Ana-lyzer.
In Proceedings of the Language and SpeechTechnnology Conference - LangTech 2008, Rome,Italy, February 28-29, pages 137?140.Christian Bizer, Jens Lehmann, Georgi Kobilarov,S?oren Auer, Christian Becker, Richard Cyganiak,and Sebastian Hellmann.
2009.
DBpedia-A crys-tallization point for the Web of Data.
Web Seman-tics: Science, Services and Agents on the World WideWeb, Issue 7:154?165.Vladimir Iosifovich Levenshtein.
1966.
Binary codescapable of correcting deletions, insertions, and re-versals.
In Soviet Physics-Doklady, volume 10.Diana McCarthy and Roberto Navigli.
2007.SemEval-2007 Task 10: English Lexical Sub-stitution Task.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 48?53, Prague, Czech Re-public, June.
Association for Computational Lin-guistics.Diana McCarthy, Rada Sinha, and Ravi Mihal-cea.
2009.
Cross Lingual Lexical Sub-stitution.
http://lit.csci.unt.edu/DOCS/task2clls-documentation.pdf.Rada Mihalcea, Ravi Sinha, and Diana McCarthy.2010.
SemEval-2010 Task 2: Cross-LingualLexical Substitution.
In Proceedings of the 5thInternational Workshop on Semantic Evaluations(SemEval-2010).
Association for ComputationalLinguistics.Ravi Sinha, Diana McCarthy, and Rada Mihalcea.2009.
SemEval-2010 Task 2: cross-lingual lexicalsubstitution.
In SEW ?09: Proceedings of the Work-shop on Semantic Evaluations: Recent Achieve-ments and Future Directions, pages 76?81, Morris-town, NJ, USA.
Association for Computational Lin-guistics.247
