Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 244?249,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsWhen and why are log-linear models self-normalizing?Jacob Andreas and Dan KleinComputer Science DivisionUniversity of California, Berkeley{jda,klein}@cs.berkeley.eduAbstractSeveral techniques have recently been pro-posed for training ?self-normalized?
discrimi-native models.
These attempt to find parametersettings for which unnormalized model scoresapproximate the true label probability.
How-ever, the theoretical properties of such tech-niques (and of self-normalization generally)have not been investigated.
This paper exam-ines the conditions under which we can ex-pect self-normalization to work.
We character-ize a general class of distributions that admitself-normalization, and prove generalizationbounds for procedures that minimize empiri-cal normalizer variance.
Motivated by theseresults, we describe a novel variant of an estab-lished procedure for training self-normalizedmodels.
The new procedure avoids computingnormalizers for most training examples, anddecreases training time by as much as factor often while preserving model quality.1 IntroductionThis paper investigates the theoretical properties oflog-linear models trained to make their unnormalizedscores approximately sum to one.Recent years have seen a resurgence of interest inlog-linear approaches to language modeling.
Thisincludes both conventional log-linear models (Rosen-feld, 1994; Biadsy et al, 2014) and neural networkswith a log-linear output layer (Bengio et al, 2006).On a variety of tasks, these LMs have produced sub-stantial gains over conventional generative modelsbased on counting n-grams.
Successes include ma-chine translation (Devlin et al, 2014) and speechrecognition (Graves et al, 2013).
However, log-linearLMs come at a significant cost for computational ef-ficiency.
In order to output a well-formed probabilitydistribution over words, such models must typicallycalculate a normalizing constant whose computa-tional cost grows linearly in the size of the vocab-ulary.Fortunately, many applications of LMs remainwell-behaved even if LM scores do not actually cor-respond to probability distributions.
For example,if a machine translation decoder uses output from apre-trained LM as a feature inside a larger model, itsuffices to have all output scores on approximatelythe same scale, even if these do not sum to one forevery LM context.
There has thus been considerableresearch interest around training procedures capa-ble of ensuring that unnormalized outputs for everycontext are ?close?
to a probability distribution.
Weare aware of at least two such techniques: noise-contrastive estimation (NCE) (Vaswani et al, 2013;Gutmann and Hyv?arinen, 2010) and explicit penal-ization of the log-normalizer (Devlin et al, 2014).Both approaches have advantages and disadvantages.NCE allows fast training by dispensing with the needto ever compute a normalizer.
Explicit penalizationrequires full normalizers to be computed during train-ing but parameterizes the relative importance of thelikelihood and the ?sum-to-one?
constraint, allowingsystem designers to tune the objective for optimalperformance.While both NCE and explicit penalization are ob-served to work in practice, their theoretical propertieshave not been investigated.
It is a classical result thatempirical minimization of classification error yieldsmodels whose predictions generalize well.
This pa-per instead investigates a notion of normalizationerror, and attempts to understand the conditions un-der which unnormalized model scores are a reliablesurrogate for probabilities.
While language model-ing serves as a motivation and running example, ourresults apply to any log-linear model, and may be ofgeneral use for efficient classification and decoding.Our goals are twofold: primarily, to provide intu-ition about how self-normalization works, and why itbehaves as observed; secondarily, to back these intu-itions with formal guarantees, both about classes ofnormalizable distributions and parameter estimationprocedures.
The paper is built around two questions:244When can self-normalization work?for which dis-tributions do good parameter settings exist?
Andwhy should self-normalization work?how does vari-ance of the normalizer on held-out data relate to vari-ance of the normalizer during training?
Analysisof these questions suggests an improvement to thetraining procedure described by Devlin et al, andwe conclude with empirical results demonstratingthat our new procedure can reduce training time forself-normalized models by an order of magnitude.2 PreliminariesConsider a log-linear model of the formp(y|x;?)
=exp{?>yx}?y?exp{?>y?x}(1)We can think of this as a function from a context x to aprobability distribution over decisions yi, where eachdecision is parameterized by a weight vector ?y.1Forconcreteness, consider a language modeling problemin which we are trying to predict the next word afterthe context the ostrich.
Here x is a vector of fea-tures on the context (e.g.
x = {1-2=the ostrich,1=the, 2=ostrich, .
.
.
}), and y ranges over thefull vocabulary (e.g.
y1= the, y2= runs, .
.
.
).Our analysis will focus on the standard log-linearcase, though later in the paper we will also relatethese results to neural networks.
We are specificallyconcerned with the behavior of the normalizer orpartition functionZ(x;?
)def=?yexp{?>yx} (2)and in particular with choices of ?
for whichZ(x;?)
?
1 for most x.To formalize the questions in the title of this paper,we introduce the following definitions:Definition 1.
A log-linear model p(y|x,?)
is nor-malized with respect to a set X if for every x ?
X ,Z(x;?)
= 1.
In this case we call X normalizableand ?
normalizing.Now we can state our questions precisely: Whatdistributions are normalizable?
Given data points1An alternative, equivalent formulation has a single weightvector and a feature function from contexts and decisions ontofeature vectors.Figure 1: A normalizable set, the solutions [x, y] toZ([x, y]; {[?1, 1], [?1,?2]}) = 1.
The set forms asmooth one-dimensional manifold bounded on either sideby the hyperplanes normal to [?1, 1] and [?1,?2].from a normalizableX , how do we find a normalizing?
?In sections 3 and 4, we do not analyze whether thesetting of ?
corresponds to a good classifier?only agood normalizer.
In practice we require both goodnormalization and good classification; in section 5we provide empirical evidence that both are achiev-able.Some notation: Weight vectors ?
(and feature vec-tors x) are d-dimensional.
There are k output classes,so the total number of parameters in ?
is kd.
|| ?
||pis the `pvector norm, and || ?
||?specifically is themax norm.3 When should self-normalization work?In this section, we characterize a large class ofdatasets (i.e.
distributions p(y|x)) that are normal-izable either exactly, or approximately in terms oftheir marginal distribution over contexts p(x).
Webegin by noting simple features of Equation 2: it isconvex in x, so in particular its level sets enclose con-vex regions, and are manifolds of lower dimensionthan the embedding space.As our definition of normalizability requires theexistence of a normalizing ?, it makes sense to beginby fixing ?
and considering contexts x for which itis normalizing.Observation.
Solutions x to Z(x;?)
= 1, if anyexist, lie on the boundary of a convex region in Rd.245This follows immediately from the definition of aconvex function, but provides a concrete example ofa set for which ?
is normalizing: the solution set ofZ(x;?)
= 1 has a simple geometric interpretation asa particular kind of smooth surface.
An example isdepicted in Figure 1.We cannot expect real datasets to be this well be-haved, so seems reasonable to ask whether ?good-enough?
self-normalization is possible for datasets(i.e.
distributions p(x)) which are only close to someexactly normalizable distribution.Definition 2.
A context distribution p(x) is D-closeto a set X ifEp[infx?
?X||X ?
x?||?
]= D (3)Definition 3.
A context distribution p(x) is ?-approximately normalizable if Ep| logZ(X;?
)| ?
?.Theorem 1.
Suppose p(x) is D-close to {x :Z(x;?)
= 1}, and each ||?i||??
B.
Then p(x)is dBD-approximately normalizable.Proof sketch.2Represent each X as X?+ X?,where X?solves the optimization problem in Equa-tion 3.
Then it is possible to bound the normalizer bylog exp {??>X?
}, where??
maximizes the magnitudeof the inner product with X?over ?.In keeping with intuition, data distributions thatare close to normalizable sets are themselves approx-imately normalizable on the same scale.34 Why should self-normalization work?So far we have given a picture of what approxi-mately normalizable distributions look like, but noth-ing about how to find normalizing ?
from trainingdata in practice.
In this section we prove that any pro-cedure that causes training contexts to approximatelynormalize will also have log-normalizers close tozero in unseen contexts.
As noted in the introduction,this does not follow immediately from correspond-ing results for classification with log-linear models.While the two problems are related (it would be quitesurprising to have uniform convergence for classifi-cation but not normalization), we nonetheless have a2Full proofs of all results may be found in the Appendix.3Here (and throughout) it is straightforward to replace quan-tities of the form dB with B by working in `2instead of `?.different function class and a different loss, and neednew analysis.Theorem 2.
Consider a sample (X1, X2, .
.
.
), withall ||X||??
R, and ?
with each ||?i||??
B. Ad-ditionally define?L =1n?i| logZ(Xi)| and L =E| logZ(X)|.
Then with probability 1?
?,|?L ?
L| ?
2?dk(log dBR+ log n) + log1?2n+2n(4)Proof sketch.
Empirical process theory providesstandard bounds of the form of Equation 4 (Kakade,2011) in terms of the size of a cover of the functionclass under consideration (here Z(?;?)).
In particu-lar, given some ?, we must construct a finite set of?Z(?
; ?)
such that some?Z is everywhere a distanceof at most ?
from every Z.
To provide this cover,it suffices to provide a cover??
for ?.
If the??
arespaced at intervals of length D, the size of the coveris (B/D)kd, from which the given bound follows.This result applies uniformly across choices of ?regardless of the training procedure used?in partic-ular, ?
can be found with NCE, explicit penalization,or the variant described in the next section.As hoped, sample complexity grows as the numberof features, and not the number of contexts.
In partic-ular, skip-gram models that treat context words inde-pendently will have sample efficiency multiplicative,rather than exponential, in the size of the condition-ing context.
Moreover, if some features are correlated(so that data points lie in a subspace smaller than ddimensions), similar techniques can be used to provethat sample requirements depend only on this effec-tive dimension, and not the true feature vector size.We emphasize again that this result says nothingabout the quality of the self-normalized model (e.g.the likelihood it assigns to held-out data).
We de-fer a theoretical treatment of that question to futurework.
In the following section, however, we provideexperimental evidence that self-normalization doesnot significantly degrade model quality.5 ApplicationsAs noted in the introduction, previous approachesto learning approximately self-normalizing distribu-tions have either relied on explicitly computing the246normalizer for each training example, or at least keep-ing track of an estimate of the normalizer for eachtraining example.Our results here suggest that it should be possi-ble to obtain approximate self-normalizing behaviorwithout any representation of the normalizer on sometraining examples?as long as a sufficiently largefraction of training examples are normalized, thenwe have some guarantee that with high probabilitythe normalizer will be close to one on the remainingtraining examples as well.
Thus an unnormalizedlikelihood objective, coupled with a penalty term thatlooks at only a small number of normalizers, mightnonetheless produce a good model.
This suggests thefollowing:l(?)
=?i?>yixi+???h?H(logZ(xh;?
))2(5)where the parameter ?
controls the relative impor-tance of the self-normalizing constraint, H is theset of indices to which the constraint should be ap-plied, and ?
controls the size of H , with |H| = dn?e.Unlike the objective used by Devlin et al (2014)most examples are never normalized during training.Our approach combines the best properties of thetwo techniques for self-normalization previously dis-cussed: like NCE, it does not require computation ofthe normalizer on all training examples, but like ex-plicit penalization it allows fine-grained control overthe tradeoff between the likelihood and the quality ofthe approximation to the normalizer.We evaluate the usefulness of this objective witha set of small language modeling experiments.
Wetrain a log-linear LM with features similar to Biadsyet al (2014) on a small prefix of the Europarl cor-pus of approximately 10M words.4We optimize theobjective in Equation 5 using Adagrad (Duchi et al,2011).
The normalized set H is chosen randomly foreach new minibatch.
We evaluate using two metrics:BLEU on a downstream machine translation task, andnormalization risk R, the average magnitude of thelog-normalizer on held-out data.
We measure the re-sponse of our training to changes in ?
and ?.
Resultsare shown in Table 1 and Table 2.4This prefix was chosen to give the fully-normalized modeltime to finish training, allowing a complete comparison.
Dueto the limited LM training data, these translation results are farfrom state-of-the-art.Normalized fraction (?
)0 0.001 0.01 0.1 1Rtrain22.0 1.7 1.5 1.5 1.5Rtest21.6 1.7 1.5 1.5 1.5BLEU 1.5 19.1 19.2 20.0 20.0Table 1: Result of varying normalized fraction ?, with?
= 1.
When no normalization is applied, the model?s be-havior is pathological, but when normalizing only a smallfraction of the training set, performance on the down-stream translation task remains good.Normalization strength (?)?
0.01 0.1 1 10Rtrain20.4 9.7 1.5 0.5Rtest20.1 9.7 1.5 0.5BLEU 1.5 2.6 20.0 16.9Table 2: Result of varying normalization parameter ?,with ?
= 0.1.
Normalization either too weak or too strongresults in poor performance on the translation task, em-phasizing the importance of training procedures with atunable normalization parameter.Table 1 shows that with small enough ?, normal-ization risk grows quite large.
Table 2 shows thatforcing the risk closer to zero is not necessarily desir-able for a downstream machine translation task.
Ascan be seen, no noticeable performance penalty isincurred when normalizing only a tenth of the train-ing set.
Performance gains are considerable: setting?
= 0.1, we observe a roughly tenfold speedup over?
= 1.On this corpus, the original training procedure ofDevlin et al with ?
= 0.1 gives a BLEU score of20.1 and Rtestof 2.7.
Training time is equivalentto choosing ?
= 1, and larger values of ?
resultin decreased BLEU, while smaller values result insignificantly increased normalizer risk.
Thus we seethat we can achieve smaller normalizer variance andan order-of-magnitude decrease in training time witha loss of only 0.1 BLEU.6 Relation to neural networksOur discussion has focused on log-linear models.While these can be thought of as a class of single-layer neural networks, in practice much of the de-mand for fast training and querying of log-linear LMs247comes from deeper networks.
All of the proof tech-niques used in this paper can be combined straight-forwardly with existing tools for covering the out-put spaces of neural networks (Anthony and Bartlett,2009).
If optimization of the self-normalizing portionof the objective is deferred to a post-processing stepafter standard (likelihood) training, and restrictedto parameters in the output layers, then Theorem 2applies exactly.7 ConclusionWe have provided both qualitative and formal charac-terizations of ?self-normalizing?
log-linear models,including what we believe to be the first theoreticalguarantees for self-normalizing training procedures.Motivated by these results, we have described a novelobjective for training self-normalized log-linear mod-els, and demonstrated that this objective achievessignificant performance improvements without a de-crease in the quality of the models learned.A Quality of the approximationProof of Theorem 1.
Using the definitions of X?,X?and??
given in the proof sketch for Theorem 1,E| log(?exp{?>iX})|= E| log(?exp{?>i(X?+X?)})|?
E| log(exp{??>X?}?exp{?>iX?})|?
E| log(exp{??>X?})|?
dDBB Generalization errorLemma 3.
For any ?1, ?2with ||?1,i?
?2,i||?
?Ddef= ?/dR for all i,|| logZ(x;?1)| ?
| logZ(x;?2)|| ?
?
(6)Proof.|| logZ(x;?1)| ?
| logZ(x;?2)||?
| logZ(x;?1)?
logZ(x;?2)|?
logZ(x;?1)Z(x;?2)(w.l.o.g.
)= log?iexp{(?1i?
?2i)>x}exp{?>2ix}?iexp{?>2ix}?
dDR+ logZ(x;?2)Z(x;?2)= ?Corollary 4.
The set of partition functions Z ={Z(?;?)
: ||?||??
B ??
?
?}
can be coveredon on the `?ball of radius R by a grid of??
withdistance D. The size of this cover is|?Z| =(BD)dk=(dBR?
)dk(7)Proof of Theorem 2.
From a standard discretizationlemma (Kakade, 2011) and Corollary 4, we immedi-ately have that with probabilty 1?
?,supZ?Z|?L ?
L| ??
inf?2?dk(log dBR?
log?)
+ log1?2n+ 2?Taking ?
= 1/n,?
2?dk(log dBR+ log n) + log1?2n+2nAcknowledgementsThe authors would like to thank Peter Bartlett, RobertNishihara and Maxim Rabinovich for useful discus-sions.
This work was partially supported by BBNunder DARPA contract HR0011-12-C-0014.
Thefirst author is supported by a National Science Foun-dation Graduate Fellowship.ReferencesMartin Anthony and Peter Bartlett.
2009.
Neural net-work learning: theoretical foundations.
CambridgeUniversity Press.Yoshua Bengio, Holger Schwenk, Jean-S?ebastien Sen?ecal,Fr?ederic Morin, and Jean-Luc Gauvain.
2006.
Neu-ral probabilistic language models.
In Innovations inMachine Learning, pages 137?186.
Springer.248Fadi Biadsy, Keith Hall, Pedro Moreno, and Brian Roark.2014.
Backoff inspired features for maximum entropylanguage models.
In Proceedings of the Conference ofthe International Speech Communication Association.Jacob Devlin, Rabih Zbib, Zhongqiang Huang, ThomasLamar, Richard Schwartz, and John Makhoul.
2014.Fast and robust neural network joint models for statisti-cal machine translation.
In Proceedings of the AnnualMeeting of the Association for Computational Linguis-tics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learning andstochastic optimization.
Journal of Machine LearningResearch, 12:2121?2159.Alex Graves, Navdeep Jaitly, and Abdel-rahman Mo-hamed.
2013.
Hybrid speech recognition with deepbidirectional LSTM.
In IEEE Workshop on AutomaticSpeech Recognition and Understanding, pages 273?278.Michael Gutmann and Aapo Hyv?arinen.
2010.
Noise-contrastive estimation: A new estimation principle forunnormalized statistical models.
In Proceedings of theInternational Conference on Artificial Intelligence andStatistics, pages 297?304.Sham Kakade.
2011.
Uniform and empirical cov-ering numbers.
http://stat.wharton.upenn.edu/?skakade/courses/stat928/lectures/lecture16.pdf.Ronald Rosenfeld.
1994.
Adaptive statistical languagemodeling: a maximum entropy approach.
Ph.D. thesis.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scale neurallanguage models improves translation.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing.249
