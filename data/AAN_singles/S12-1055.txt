First Joint Conference on Lexical and Computational Semantics (*SEM), pages 413?418,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsUTD: Determining Relational Similarity Using Lexical PatternsBryan Rink and Sanda HarabagiuUniversity of Texas at DallasP.O.
Box 830688; MS EC31Richardson, TX, 75083-0688, USA{bryan,sanda}@hlt.utdallas.eduAbstractIn this paper we present our approach forassigning degrees of relational similarity topairs of words in the SemEval-2012 Task 2.To measure relational similarity we employedlexical patterns that can match against wordpairs within a large corpus of 12 million docu-ments.
Patterns are weighted by obtaining sta-tistically estimated lower bounds on their pre-cision for extracting word pairs from a givenrelation.
Finally, word pairs are ranked basedon a model predicting the probability that theybelong to the relation of interest.
This ap-proach achieved the best results on the Se-mEval 2012 Task 2, obtaining a Spearman cor-relation of 0.229 and an accuracy on reproduc-ing human answers to MaxDiff questions of39.4%.1 IntroductionConsiderable prior research has examined and elab-orated upon a wide variety of semantic relationsbetween concepts along with techniques for auto-matically discovering pairs of concepts for whicha relation holds (Bejar et al, 1991; Stephens andChen, 1996; Rosario and Hearst, 2004; Khoo andNa, 2006; Girju et al, 2009).
However, most pre-vious work has considered membership assignmentfor a semantic relation as a binary property.
In thispaper we discuss an approach which assigns a de-gree of membership to a pair of concepts for a givenrelation.
For example, for the semantic relationCLASS-INCLUSION (Taxonomic), the concept pairsweapon:spear and bird:robin are stronger membersConsider the following word pairs: millionaire:money,author:copyright, robin:nest.
These X:Y pairs share arelation ?X R Y?.
Now consider the following wordpairs:(1) teacher:students(2) farmer:crops(3) homeowner:door(4) shrubs:rootsWhich of the numbered word pairs is the MOST illus-trative example of the same relation ?X R Y?
?Which of the above numbered word pairs is theLEAST illustrative example of the same relation ?XR Y?
?Figure 1: Example Phase 2 MaxDiff question for the re-lation 2h PART-WHOLE: Creature:Possession.of the relationship than hair:brown, because brownmay describe many things other than hair, and brownis also used much less frequently as a noun than thewords in the first two word pairs.
Task 2 of Se-mEval 2012 (Jurgens et al, 2012) was designed toevaluate the effectiveness of automatic approachesfor determining the similarity of a pair of conceptsto a specific semantic relation.
The task focused on79 semantic relations from Bejar et al (1991) whichbroadly fall into the ten categories enumerated in Ta-ble 1.The data for the task was collected in two phasesusing Amazon Mechanical Turk 1.
During Phase1, Turkers were asked to provide pairs of wordswhich fit a relation template, such as ?X pos-sesses/owns/has Y?.
Turkers provided word pairssuch as expert:experience, mall:shops, letters:words,and doctor:degree.
A total of 3,218 word pairs1http://www.mturk.com/mturk/413Category Example word pairs RelationsCLASS-INCLUSION flower:tulip, weapon:knife, clothing:shirt, queen:Elizabeth 5PART-WHOLE car:engine, fleet:ship, mile:yard, kickoff:football 10SIMILAR car:auto, stream:river, eating:gluttony, colt:horse 8CONTRAST alive:dead, old:young, east:west, happy:morbid 8ATTRIBUTE beggar:poor, malleable:molded, soldier:fight, exercise:vigorous 8NON-ATTRIBUTE sound:inaudible, exemplary:criticized, war:tranquility, dull:cunning 8CASE RELATIONS tailor:suit, farmer:tractor, teach:student, king:crown 8CAUSE-PURPOSE joke:laughter, fatigue:sleep, gasoline:car, assassin:death 8SPACE-TIME bookshelf:books, coast:ocean, infancy:cradle, rivet:girder 9REFERENCE smile:friendliness, person:portrait, recipe:cake, astronomy:stars 6Table 1: The ten categories of semantic relations used in SemEval 2012 Task 2.
Each word pair has been taken from adifferent subcategory of each major category.across 79 relations were provided by Turkers inPhase 1.
Some of these word pairs are naturallymore representative of the relationship than others.Therefore, in the second phase, each word pair waspresented to a different set of Turkers for rankingin the form of MaxDiff (Louviere and Woodworth,1991) questions.
Figure 1 shows an example MaxD-iff question for the relation 2h PART-WHOLE: Crea-ture:Possession (?X possesses/owns/has Y?).
In eachMaxDiff question, Turkers were simply asked to se-lect the word pair which was the most illustrativeof the relation and the word pair which was theleast illustrative of the relation.
For the example inFigure 1, most Turkers chose either shrubs:roots orfarmer:crops as the most illustrative of the Crea-ture:Possession relation, and homeowner:door asthe least illustrative.
When Turkers select a pair ofwords they are performing a semantic inference thatwe wanted to also perform in a computational man-ner.
In this paper we present a method for automat-ically ranking word pairs according to their related-ness to a given semantic relation.2 Approach for Determining RelationalSimilarityIn the vein of previous methods for determining re-lational similarity (Turney, 2011; Turney, 2008a;Turney, 2008b; Turney, 2005), we propose two ap-proaches using patterns generated from the contextsin which the word pairs occur.
Our corpus consistsof 8.4 million documents from Gigaword (Parkerand Consortium, 2009) and over 4 million articlesfrom Wikipedia.
For each word pair, <W1>, <W2>provided by Turkers in Phase 1, as well as the threerelation examples, we collected all contexts whichmatched the schema:?
[0 or more non-content words] <W1> [0 to 7words] <W2> [0 or more non-content words]?We also include those contexts where W1 and W2are swapped.
The window size of seven words wasdetermined based on experiments on the training setof ten relations provided by the task organizers.
Forthe non-content words, we considered closed classwords such as determiners (the, who, every), prepo-sitions (in, on, instead of), and conjunctions (and,but).
Members of these classes were collected fromtheir corresponding Wikipedia pages.
Below weprovide a sample of the 7,022 contexts found for theword pair love:hate:?they <W1> to <W2> it?
?<W1> and <W2> the most .
by?
?between <W1> & <W2>?
?<W1> you then i <W2> you and?We restrict the context before and after the word pairto non-content words in order to match longer con-texts without introducing exponential growth in thenumber of patterns and the consequential sparsityproblems.
These contexts are directly used as pat-terns.
To generate additional patterns we have onemethod for shortening contexts and two methods forgenerating patterns from contexts.Any contexts which contain words before <W1>or after <W1> are used to create additional shortercontexts by successively removing leading and trail-ing words.
For example, the context ?as much<W1> in the <W2> as his?
for the word pairmoney:bank would generate the following shortenedcontexts:?much <W1> in the <W2> as his?
?<W1> in the <W2> as his?414?as much <W1> in the <W2>?
as?as much <W1> in the <W2>?
?much <W1> in the <W2> as?
?<W1> in the <W2> as?
?<W1> in the <W2>?These shortened contexts are used, along with theoriginal context, to generate patterns.The first pattern generation method replaces eachword between<W1> and<W2> with a wildcard ([?
]+ means one or more non-space characters).
For ex-ample:?as much <W1> [?
]+ the <W2> as?
?as much <W1> in [?
]+ <W2> as?The second pattern generation technique allows fora single word to be matched in the context betweenthe arguments <W1> and <W2>, along with arbi-trary matching of other tokens in the context.
Forexample, the context for red:stop ?the <W1> flag isflagged to indicate a <W2>?
will generate new pat-terns such as:?the <W1>.
* flag .*<W2>?
?the <W1>.
* is .*<W2>?
?the <W1>.
* flagged .*<W2>?
?the <W1>.
* indicate .
*<W2>?After all patterns have been generated, they are usedby our two approaches to assign relational similarityscores to word pairs.2.1 UTD-NB ApproachThe first of our two approaches, UTD-NB, assignsweights to patterns which are then used to assignsimilarity scores to word pairs.
The approach beginsby obtaining all word pairs associated with a rela-tion.
Each relation is associated with a target set (T )of word pairs from two sources: (i) the three or fourexample word pairs provided for each relation, and(ii) the word pairs provided by Turkers in Phase 1.We collect all of the contexts for those word pairs togenerate patterns.
The UTD-NB approach assumesthat the word pairs provided by Turkers, while noisy,can be used to characterize the relation.
As an exam-ple, consider these word pairs provided by Turkersfor the relation 8a (Cause:Effect) illness:discomfort,fire:burns, accident:damage.
A pattern which ex-tracts these word pairs is: ?<W1> that caused [?
]+<W2>?.
This pattern is unlikely to match the con-texts of word pairs from other relations.
Therefore,we use the statistics about how many target wordFigure 2: Probabilistic model for the word pairs extractedby patterns, for a single relation.pairs a pattern extracts versus how many non-targetpairs a pattern extracts to assign a weight to the pat-tern.
A pattern which matches many of the wordpairs from the target relation and few (or none) of theword pairs from other relations is likely to be a goodindicator of that relation.
For example, the patternP1 for the relation 8a (Cause:Effect): ?the <W1>.
*caused .
*<W2> to his?
matches only three wordpairs: explosion:damage, accident:damage, and in-jury:pain, all of them belonging to the target rela-tion.
Conversely, the pattern P2: ?<W1>.
* caus-ing .
*<W2> but?
matches five words pairs.
How-ever, only three of them belong to the target relation:hit:injury, explosion:damage, germs:sickness.
Theremaining two: city:people, action:alarm belong toother relations: .We use the number of target word pairs extracted,x, and the total number of word pairs extracted, n,to calculate ?
: the probability that a word pair ex-tracted by the pattern will belong to the target re-lation.
The maximum likelihood estimate for ?
isxn , however for small values of x this estimate hasa high variance and can significantly overestimatethe true value.
Therefore, we used the Wilson in-terval score for determining a lower bound on ?
ata 99.9% confidence level.
This gives the pattern P1above with x = 3 and n = 3 a lower bound on ?of 21.7% and P2 with x = 3 and n = 5 a lowerbound on ?
of 16.6%.
We use this lower bound asthe pattern?s weight.
These pattern weights are thencombined to score each word pair for the target rela-tion.We model the word pairs extracted by the patternsas a generative process shown in Figure 2.
Each pat-tern, p, is associated with with a precision, ?
, whichis the probability that a word pair extracted by thatpattern is a member of the target relation.
The ob-415served word pairs extracted by a pattern are denotedby w. Our model assumes that a word pair extractedby a pattern may be drawn from one of two distinctdistributions over word pairs: a distribution for thetarget relation ~t, and a background distribution overword pairs ~b.
The generation of a word pair be-gins with a binary variable x drawn from a Bernoullidistribution parametrized by ?
(the pattern?s preci-sion), which represents whether a word pair is gen-erated according to a relation specific distribution, ora background distribution.
More explicitly, if x = 1,then a word pair w is generated by the target relationdistribution ~t, and if x = 0, a word pair is generatedby the background distribution~b.We may not yet perform any meaningful infer-ence because no evidence has been observed to cor-rectly infer whether the target distribution or thebackground distribution generated w. Therefore weuse the pattern weights derived above (based on thelower bounds on the pattern precisions) as that pat-tern?s value of ?
.
For estimating the distributions~t and ~b, we assume that x is 1 (w is generated by~t) if and only if ?
?
0.1 and the word pair w be-longs to the target set of word pairs T .
This thresh-old on ?
has a filtering effect on the patterns, andthose patterns below the threshold are treated as non-indicative of the relation.
These assumptions allowus to estimate the parameters for ~t and~b:P (w|~t) ={#(w,h)#(h) if w ?
T0 if w 6?
T(1)P (w|~b) =#(w,?h) + #(w, h)1w 6?T?u #(u,?h) + #(u, h)1u6?T(2)where #(w, h) is the number of times w was ex-tracted by a high precision pattern (?
?
10%), and#(h) is the number of word pairs extracted by a highprecision pattern.The only remaining hidden variable in the modelis x which we can now estimate using the inferreddistributions for the other variables.
We chose to usethe probability of x for a word pair w as the scoreby which we rank the word pairs.
Furthermore, weuse only the probability of x for the highest rankingpattern p which extracted w:P (x = 1|p, w) =P (x = 1, w|p)P (w|p)(3)where P (x = 1, w|p) = ?p ?
~t(w) and P (w|p) =P (x = 1, w|p) + P (x = 0, w|p)This method of scoring word pairs accounts forhow common a word pair is overall.
For examplefor the relation 4c (CONTRAST: Reverse), the wordpair white:black occurs very commonly in both highprecision patterns and low precision patterns (thosemore likely associated with other relations).
There-fore even though the word pair shares its highestranking pattern with the pair eat:fast, white:black re-ceives a score of 0.019 while eat:fast receives a scoreof 0.216 because ~t(white : black) = 0.006 and~b(white : black) = 0.104, while ~t(eat : fast) =0.0016 and ~b(eat : fast) = 0.0018.
However,if a pattern with 100% precision were to extractwhite:black, the pair would appropriately receive ascore of 1.0 despite being much more common in thebackground distribution.
This is motivated by ourassumption that such a pattern can only extract wordpairs which truly belong to the relation.
Anothermotivation for scoring word pairs by their highestranking pattern is that it does not depend on anyassumption of independence between the patternswhich extract the pairs.
For example, the pattern?<W1> , not <W2> .
?
extracts largely the sameword pairs as ?<W1> [?
]+ not<W2> .?
and thus itsmatches should not be taken as additional evidenceabout the word pairs.2.2 UTD-SVM ApproachOur second approach uses an SVM-rank (Joachims,2006) model to rank the word pairs.
Each word pairfrom a target relation is represented as a binary fea-ture vector indicating which patterns extracted theword pair.
We train the SVM-rank classifier by as-signing all word pairs from the target relation rank 2,and all word pairs from other relations with rank 1.The SVM model is then trained and used to classifythe word pairs from the target relation.
Even thoughthe model is used to classify the same word pairs itwas trained on, it still provides higher scores to wordpairs more likely to belong to the target relation.
Wedirectly rank the word pairs using these scores.3 DiscussionThe organizers of SemEval 2012 Task 2 viewed re-lational similarity in two different ways.
The first416Word pair % Most illustrative -% Least illustrative?freezing:warm?
56.0?earsplitting:quiet?
36.0?evil:angelic?
18.0?ancient:modern?
12.0?disastrous:peaceful?
6.0?ecstatic:disgruntled?
2.0?disgusting:tasty?
0.0?beautiful:plain?
-2.0?dirty:sterile?
-4.0?wrinkled:smooth?
-6.0?sweet:sour?
-20.0?disgruntled:ecstatic?
-32.0?white:gray?
-54.0Table 2: A sample of the 41 word pairs provided byAmazon Mechanical Turk participants for the relation 4f(CONTRAST: Asymmetric Contrary - X and Y are at op-posite ends of the same scale).
The word pairs are rankedby how illustrative of the relation participants found eachpair to be.view was that of solving a MaxDiff problem, ques-tion in which participants are shown a list of fourword pairs and asked to select the most and leastillustrative pairs.
The second view of relation simi-larity considers the task of assigning scores to a ac-cording to their similarity to the relation of interest.The first column of Table 2 provides an example ofword pairs that Amazon Turkers said belonged to the4f: CONTRAST: Asymmetric Contrary relation inPhase 1, ranked according to how well other Turk-ers felt they represented the relation.
The score inthe second column is calculated as the percentage ofhow often Turkers rated a word pair as the most il-lustrative and how often Turkers rated the word pairas the least illustrative.Both of our approaches for determining relationsimilarity assign scores directly to the word pairscollected in Phase 1, with the goal of ranking thewords in the same order that was induced from theresponses by Amazon Mechanical Turkers.3.1 Evaluation MeasuresSemEval-2012 Task 2 had two official evaluationmetrics.
The first directly measured the accuracyof automatically choosing the most and least illus-trative word pairs among a set of four word pairstaken from responses during Phase 1.
The accuracyof choosing the most illustrative word pair and theTeam-Algorithm Spearman MaxDiffUTD-NB 0.229 39.4UTD-SVM 0.116 34.7Duluth-V0 0.050 32.4Duluth-V1 0.039 31.5Duluth-V2 0.038 31.1BUAP 0.014 31.7Random 0.018 31.2Table 3: Results for all systems participating in SemEval2012 Task 2 on relational similarity, including a randombaseline.accuracy of choosing the least illustrative word pairwere calculated separately and averaged to producethe MaxDiff accuracy.The second evaluation metric measured the corre-lation between an automatic ranking of word pairsfor a relation and a ranking induced by the Turkers?responses to the MaxDiff questions.
The word pairswere given scores equal to the percentage of timesthey were chosen by Turkers as the most illustra-tive example for a relation minus the percentage oftimes they were chosen as the least illustrative.
Sys-tems were then evaluated according to their Spear-man rank correlation with the ranking of word pairsinduced by that score.
Spearman correlations rangefrom -1 for a negative correlation to 1.0 for a perfectcorrelation.3.2 ResultsTable 3 shows the results for the six systems whichparticipated in SemEval-2012 Task 2, along with theresults for a baseline which ranks each word pairrandomly.
Our two approaches achieved the best re-sults on both evaluation metrics.
Our UTD-NB ap-proach achieves much better performance than ourUTD-SVM approach, likely due to the unconven-tional use of the SVM to classify its own trainingdata.
That said, the results are still significantlyhigher than those of other participants.
This maybe attributed to our incorporation of better patternsor our use of a large corpus.
It might also be a con-sequence of our approaches considering all of thetesting word pairs simultaneously.Table 4 shows the results for each of the ten cat-egories of relations.
The best results are achievedon SPACE-TIME relations, while the lowest perfor-mance is on the NON-ATTRIBUTE relations.
NON-417Category Rndm BUAP UTD UMDNB V01 CLASS-INCLUSION 0.057 0.064 0.233 0.0452 PART-WHOLE 0.012 0.066 0.252 -0.0613 SIMILAR 0.026 -0.036 0.214 0.1834 CONTRAST -0.049 0.000 0.206 0.1425 ATTRIBUTE 0.037 -0.095 0.158 0.0446 NON-ATTRIBUTE -0.070 0.009 0.098 0.0797 CASE RELATIONS 0.090 -0.037 0.241 -0.0118 CAUSE-PURPOSE -0.011 0.114 0.183 0.0219 SPACE-TIME 0.013 0.035 0.375 0.05510 REFERENCE 0.142 -0.001 0.346 0.028Table 4: Spearman correlation results for the best systemfrom each team, across all ten categories of relations.ATTRIBUTE relations associate objects and actionswith an atypical attribute (harmony:discordant, im-mortal:death, recluse:socialize).
Because the pairsof words associated with these relation are not typ-ically associated together, our approach likely per-forms poorly on these relations because our ap-proach is based on finding the pairs of words to-gether in a large corpus.An interesting consequence of the 10% precisionthreshold used in the UTD-NB approach is that 24relations had no patterns exceeding the thresholdand therefore produced zeroes as scores for all wordpairs.
However, word pairs which never occurredwithin seven tokens of each other in our corpus re-ceived a negative score and were ranked lower.
Suchrankings tend to produce Spearman scores around0.0.
Our lowest Spearman score was -0.068, whileother teams had low scores of -0.344 and -0.266,both occurring on relations for which UTD-NB pro-duced no positive word pair scores.
There are twolessons to be learned from this result: (i) the UTD-NB approach does a good job of recognizing whenit cannot rank word pairs, and (ii) such relations arelikely difficult and worth further investigation.4 ConclusionWe described the UTD approaches to determiningrelation similarity using lexical patterns from a largecorpus.
Combined with a probabilistic model forword pair extraction by those patterns, we were ableto achieve the highest performance at the SemEval2012 Task 2.
Our results showed the approachsignificantly outperformed a model which used anSVM-rank model used to classify its own trainingset.
The approach also performed well across a widerange of relation types and argument classes whichincluded nouns, adjectives, verbs, and adverbs.
Thisimplies that the approaches presented in this pa-per could be successfully applied to other domainswhich involve semantic relations.ReferencesIsaac I. Bejar, Roger Chaffin, and Susan E. Embretson.1991.
Cognitive and psychometric analysis of analog-ical problem solving.
Recent research in psychology.Springer-Verlag Publishing.Roxana Girju, Preslav Nakov, Vivi Nastase, Stan Sz-pakowicz, Peter Turney, and Deniz Yuret.
2009.Classification of semantic relations between nominals.Language Resources and Evaluation, 43(2):105?121.Thorsten Joachims.
2006.
Training linear SVMs in lineartime.
In Proceedings of the 12th ACM SIGKDD inter-national conference KDD ?06, page 217, New York,New York, USA, August.
ACM Press.David A. Jurgens, Saif M. Mohammad, Peter D. Turney,and Keith J. Holyoak.
2012.
SemEval-2012 Task 2:Measuring Degrees of Relational Similarity.
In Pro-ceedings of the 6th International Workshop on Seman-tic Evaluation (SemEval 2012).Christopher S G Khoo and Jin-cheon Na.
2006.
Seman-tic relations in information science.
Annual Review ofInformation Science and Technology, 40(1):157?228.Jordan J Louviere and G G Woodworth.
1991.
Best-worst scaling: A model for the largest difference judg-ments.
Technical report, University of Alberta.Robert Parker and Linguistic Data Consortium.
2009.English gigaword fourth edition.
Linguistic Data Con-sortium.Barbara Rosario and Marti A. Hearst.
2004.
Classifyingsemantic relations in bioscience texts.
In Proceedingsof the AACL ?04, pages 430?es, July.Larry M. Stephens and Yufeng F. Chen.
1996.
Principlesfor organizing semantic relations in large knowledgebases.
IEEE Transactions on Knowledge and DataEngineering, 8(3):492?496, June.Peter D. Turney.
2005.
Measuring Semantic Similarityby Latent Relational Analysis.
In International JointConference On Artificial Intelligence, volume 19.Peter D. Turney.
2008a.
A Uniform Approach to Analo-gies, Synonyms, Antonyms, and Associations.
In Pro-ceedings of COLING ?08, August.Peter D. Turney.
2008b.
The Latent Relation MappingEngine: Algorithm and Experiments.
Journal of Arti-ficial Intelligence Research, 33:615?655.Peter D. Turney.
2011.
Analogy perception appliedto seven tests of word comprehension.
Journal ofExperimental & Theoretical Artificial Intelligence,23(3):343?362, July.418
