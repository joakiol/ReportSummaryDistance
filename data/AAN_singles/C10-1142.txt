Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1263?1271,Beijing, August 2010Estimating Linear Models for Compositional Distributional SemanticsFabio Massimo Zanzotto1(1) Department of Computer ScienceUniversity of Rome ?Tor Vergata?zanzotto@info.uniroma2.itIoannis KorkontzelosDepartment of Computer ScienceUniversity of Yorkjohnkork@cs.york.ac.ukFrancesca Fallucchi1,2(2) Universita` Telematica?G.
Marconi?f.fallucchi@unimarconi.itSuresh ManandharDepartment of Computer ScienceUniversity of Yorksuresh@cs.york.ac.ukAbstractIn distributional semantics studies, thereis a growing attention in compositionallydetermining the distributional meaning ofword sequences.
Yet, compositional dis-tributional models depend on a large setof parameters that have not been explored.In this paper we propose a novel approachto estimate parameters for a class of com-positional distributional models: the addi-tive models.
Our approach leverages ontwo main ideas.
Firstly, a novel idea forextracting compositional distributional se-mantics examples.
Secondly, an estima-tion method based on regression modelsfor multiple dependent variables.
Experi-ments demonstrate that our approach out-performs existing methods for determin-ing a good model for compositional dis-tributional semantics.1 IntroductionLexical distributional semantics has been largelyused to model word meaning in many fields ascomputational linguistics (McCarthy and Carroll,2003; Manning et al, 2008), linguistics (Harris,1964), corpus linguistics (Firth, 1957), and cogni-tive research (Miller and Charles, 1991).
The fun-damental hypothesis is the distributional hypoth-esis (DH): ?similar words share similar contexts?
(Harris, 1964).
Recently, this hypothesis has beenoperationally defined in many ways in the fields ofphysicology, computational linguistics, and infor-mation retrieval (Li et al, 2000; Pado and Lapata,2007; Deerwester et al, 1990).Given the successful application to words, dis-tributional semantics has been extended to wordsequences.
This has happened in two ways: (1)via the reformulation of DH for specific word se-quences (Lin and Pantel, 2001); and (2) via thedefinition of compositional distributional seman-tics (CDS) models (Mitchell and Lapata, 2008;Jones and Mewhort, 2007).
These are two differ-ent ways of addressing the problem.Lin and Pantel (2001) propose the pattern dis-tributional hypothesis that extends the distribu-tional hypothesis for specific patterns, i.e.
wordsequences representing partial verb phrases.
Dis-tributional meaning for these patterns is deriveddirectly by looking to their occurrences in a cor-pus.
Due to data sparsity, patterns of differentlength appear with very different frequencies inthe corpus, affecting their statistics detrimentally.On the other hand, compositional distributionalsemantics (CDS) propose to obtain distributionalmeaning for sequences by composing the vectorsof the words in the sequences (Mitchell and Lap-ata, 2008; Jones and Mewhort, 2007).
This ap-proach is fairly interesting as the distributionalmeaning of sequences of different length is ob-tained by composing distributional vectors of sin-gle words.
Yet, many of these approaches have alarge number of parameters that cannot be easilyestimated.In this paper we propose a novel approach to es-1263timate parameters for additive compositional dis-tributional semantics models.
Our approach lever-ages on two main ideas.
Firstly, a novel way forextracting compositional distributional semanticsexamples and counter-examples.
Secondly, an es-timation model that exploits these examples anddetermines an equation system that represents aregression problem with multiple dependent vari-ables.
We propose a method to estimate a solu-tion of this equation system based on the Moore-Penrose pseudo-inverse matrices (Penrose, 1955).The rest of the paper is organised as follows:Firstly, we shortly review existing compositionaldistributional semantics (CDS) models (Sec.
2).Then we describe our model for estimating CDSmodels parameters (Sec.
3).
In succession, weintroduce a way to extract compositional dis-tributional semantics examples from dictionaries(Sec.
4).
Then, we discuss the experimental set upand the results of our linear CDS model with es-timated parameters with respect to existing CDSmodels (Sec.
5).2 Models for compositionaldistributional semantics (CDS)A CDS model is a function  that computes thedistributional vector of a sequence of words s bycombining the distributional vectors of its com-ponent words w1 .
.
.wn.
Let(s) be the distribu-tional vector describing s and ~wi the distributionalvectors describing its component word wi.
Then,the CDS model can be written as:(s) = (w1 .
.
.wn) = ~w1  .
.
.
~wn (1)This generic model has been fairly studied andmany different functions have been proposed andtested.Mitchell and Lapata (2008) propose the fol-lowing general CDS model for 2-word sequencess = xy:(s) = (xy) = f(~x, ~y,R,K) (2)where ~x and ~y are respectively the distributionalvectors of x and y, R is the particular syntacticand/or semantic relation connecting x and y, and,K represents the amount of background knowl-edge that the vector composition process takesvector dimensionsbetweengap processsocialtwocontact < 11, 0, 3, 0, 11 >x: close < 27, 3, 2, 5, 24 >y: interaction < 23, 0, 3, 8, 4 >Table 1: Example of distributionalfrequency vectors for the triple t =( ~contact, ~close, ~interaction)into account.
Two specialisations of the gen-eral CDS model are proposed: the basic additivemodel and the basic multiplicative model.The basic additive model (BAM) is written as:(s) = ?~x+ ?~y (3)where ?
and ?
are two scalar parameters.
Thesimplistic parametrisation is ?
= ?
= 1.
Forexample, given the vectors ~x and ~y of Table 1,BAM (s) =< 50, 3, 5, 13, 28 >.The basic multiplicative model (BMM) is writ-ten as:si = xiyi (4)where si, xi, and yi are the i-th dimensions ofthe vectors (s), ~x, and ~y, respectively.
Forthe example of Table 1, BMM (s) =< 621, 0,6, 40, 96 >.Erk and Pado?
(2008) look at the problem in adifferent way.
Let the general distributional mean-ing of the word w be ~w.
Their model computes adifferent vector ~ws that represents the specific dis-tributional meaning of w with respect to s, i.e.
:~ws = (w, s) (5)In general, this operator gives different vectors foreach word wi in the sequence s, i.e.
(wi, s) 6=(wj , s) if i 6= j.
It also gives different vectorsfor a word wi appearing in different sequences skand sl, i.e.
(wi, sk) 6= (wi, sl) if k 6= l.The model of Erk and Pado?
(2008) was de-signed to disambiguate the distributional mean-ing of a word w in the context of the sequences.
However, substituting the word w with the se-mantic head h of s, allows to compute the distri-butional meaning of sequence s as shaped by the1264word that is governing the sequence (c.f.
Pollardand Sag (1994)).
For example, the distributionalmeaning of the word sequence eats mice is gov-erned by the verb eats.
Following this model, thedistributional vector (s) can be written as:(s) ?
(h, s) (6)The function (h, s) explicitly uses the re-lation R and the knowledge K of the generalequation 2, being based on the notion of selec-tional preferences.
We exploit the model for se-quences of two words s=xy where the two wordsare related with an oriented syntactic relation r(e.g.
r=adj modifier).
For making the syntac-tic relation explicit, we indicate the sequence as:s = x r??
y.Given a word w, the model has to keep trackof its selectional preferences.
Consequently, eachword w is represented with a triple:(~w,Rw, R?1w ) (7)where ~w is the distributional vector of the word w,Rw is the set of the vectors representing the directselectional preferences of the word w, and R?1w isthe set of the vectors representing the indirect se-lectional preferences of the word w. Given a set ofsyntactic relationsR, the set Rw and R?1w containrespectively a selectional preference vectorRw(r)and Rw(r)?1 for each r ?
R. Selectional prefer-ences are computed as in Erk (2007).
If x is thesemantic head of sequence s, then the model canbe written as:(s) = (x, x r??
y) = ~xRy(r) (8)Otherwise, if y is the semantic head:(s) = (y, x r??
y) = ~y R?1x (r) (9)is in both cases realised using BAM or BMM.We will call these models: basic additive modelwith selectional preferences (BAM-SP) and basicmultiplicative model with selectional preferences(BMM-SP).Both Mitchell and Lapata (2008) and Erk andPado?
(2008) experimented with few empiricallyestimated parameters.
Thus, the general additiveCDS model has not been adequately explored.3 Estimating Additive CompositionalSemantics Models from DataThe generic additive model sums the vectors ~xand ~y in a new vector ~z:(s) = ~z = A~x+B~y (10)where A and B are two square matrices captur-ing the relation R and the background knowledgeK of equation 2.
Writing matrices A and B byhand is impossible because of their large size.
Es-timating these matrices is neither a simple classi-fication learning problem nor a simple regressionproblem.
It is a regression problem with multipledependent variables.
In this section, we proposeour model to solve this regression problem usinga set of training examples E.The set of training examples E contains triplesof vectors (~z, ~x, ~y).
~x and ~y are the two distribu-tional vectors of the words x and y.
~z is the ex-pected distributional vector of the composition of~x and ~y.
Note that for an ideal perfectly perform-ing CDS model we can write ~z = (xy).
How-ever, in general the expected vector ~z is not guar-anteed to be equal to the composed one (xy).Figure 1 reports an example of these triples, i.e.,t = ( ~contact, ~close, ~interaction), with the re-lated distributional vectors.
The construction ofE is discussed in section 4.In the rest of the section, we describe how theregression problem with multiple dependent vari-ables can be solved with a linear equation systemand we give a possible solution of this equationsystem.
In the experimental section, we refer toour model as the estimated additive model (EAM).3.1 Setting the linear equation systemThe matrices A and B of equation 10 can bejoined in a single matrix:~z =(A B)(~x~y)(11)For the triple t of table 1, equation 11 is:~contact =(A B)(~close~interaction)(12)1265and it can be rewritten as:??????1103011?????
?=(A5?5 B5?5)???????????2732524230384???????????
(13)Focusing on matrix (AB), we can transpose thematrices as follows:~zT =((A B)(~x~y))T=(~xT ~yT)(ATBT)(14)Matrix (~xT ~yT ) is known and matrix(ATBT)isto be estimated.Equation 14 is the prototype of our final equa-tion system.
The larger the matrix (AB) to beestimated, the more equations like 14 are needed.Given set E that contains n triples (~z, ~x, ~y), wecan write the following system of equations:?????~zT1~zT2...~zTn?????
=?????
(~xT1 ~yT1)(~xT2 ~yT2)...(~xTn ~yTn)?????
(ATBT)(15)The vectors derived from the triples can be seen astwo matrices of n rows, Z and (XY ) related to ~zTiand (~xTi ~yTi), respectively.
The overall equationsystem is then the following:Z =(X Y)(ATBT)(16)This equation system represents the constraintsthat matrices A and B have to satisfy in order tobe a possible linear CDS model that can at leastdescribe seen examples.
We will hereafter call?
=(A B) and Q = (X Y ).
The systemin equation 16 can be simplified as:Z = Q?T (17)As Q is a rectangular and singular matrix, it isnot invertible and the system in equation 16 hasno solutions.
It is possible to use the principleof Least Square Estimation for computing an ap-proximation solution.
The idea is to compute thesolution ??
that minimises the residual norm, i.e.:?
?T = arg min?T?Q?T ?
Z?2 (18)One solution for this problem is the Moore-Penrose pseudoinverse Q+ (Penrose, 1955) thatgives the following final equation:?
?T = Q+Z (19)In the next section, we discuss how the Moore-Penrose pseudoinverse is obtained using singularvalue decomposition (SVD).3.2 Computing the pseudo-inverse matrixThe pseudo-inverse matrix can provide an approx-imated solution even if the equation system has nosolutions.
We here compute the Moore-Penrosepseudoinverse using singular value decomposi-tion (SVD) that is widely used in computationallinguistics and information retrieval for reducingspaces (Deerwester et al, 1990).Moore-Penrose pseudoinverse (Penrose, 1955)is computed in the following way.
Let the originalmatrix Q have n rows and m columns and be ofrank r. The SVD decomposition of the originalmatrix Q is Q = U?V T where ?
is a square di-agonal matrix of dimension r. Then, the pseudo-inverse matrix that minimises the equation 18 is:Q+ = V ?+UT (20)where the diagonal matrix ?+ is the r ?
r trans-posed matrix of ?
having as diagonal elements thereciprocals of the singular values 1?1 , 1?2 , ..., 1?r of?.Using SVD to compute the pseudo-inverse ma-trix allows for different approximations (Fallucchiand Zanzotto, 2009).
The algorithm for comput-ing the singular value decomposition is iterative(Golub and Kahan, 1965).
Firstly derived dimen-sions have higher singular value.
Then, dimensionk is more informative than dimension k?
> k. Wecan consider different values for k to obtain differ-ent SVD for the approximations Q+k of the origi-nal matrix Q+ in equation 20), i.e.
:Q+k = Vn?k?+k?kUTk?m (21)1266where Q+k is a matrix n by m obtained consider-ing the first k singular values.4 Building positive and negativeexamplesAs explained in the previous section, estimatingCDS models, needs a set of triples E, similar totriple t of table 1.
This set E should contain pos-itive examples in the form of triples (~zi, ~xi, ~yi).Examples are positive in the sense that ~zi =(xy) for an ideal CDS.
There are no availablesets to contain such triples, with the exception ofthe set used in Mitchell and Lapata (2008) whichis designed only for testing purposes.
It containssimilar and dissimilar pairs of sequences (s1,s2)where each sequence is a verb-noun pair (vi,ni).From the positive part of this set, we can only de-rive quadruples where (v1n1) ?
(v2n2) butwe cannot derive the ideal resulting vector of thecomposition (vini).
Sets used to test multi-word expression (MWE) detection models (e.g.,(Schone and Jurafsky, 2001; Nicholson and Bald-win, 2008; Kim and Baldwin, 2008; Cook etal., 2008; Villavicencio, 2003; Korkontzelos andManandhar, 2009)) are again not useful as con-taining only valid MWE that cannot be used todetermine the set of training triples needed here.As a result, we need a novel idea to build setsof triples to train CDS models.
We can leverageon knowledge stored in dictionaries.
In the rest ofthe section, we describe how we build the positiveexample set E and a control negative example setNE.
Elements of the two sets are pairs (t,s) wheret is a target word s is a sequence of words.
t is theword that represent the distributional meaning ofs in the case ofE.
Contrarily, t is totally unrelatedto the distributional meaning of s inNE.
The setsE and NE can be used both for training and fortesting.
In the testing phase, we can use these setsto determine whether a CDS model is good or notand to compare different CDS models.4.1 Building Positive Examples usingDictionariesDictionaries as natural repositories of equivalentexpressions can be used to extract positive exam-ples for training and testing CDS models.
Thebasic idea is the following: dictionary entries aredeclarations of equivalence.
Words or, occasion-ally, multi-word expressions t are declared to besemantically similar to their definition sequencess.
This happens at least for some sense of thedefined words.
We can then observe that t ?
s.For example, we report some sample definitionsof contact and high life:target word (t) definition sequence (s)contact close interactionhigh life excessive spendingIn the first case, a word, i.e.
contact, is semanti-cally similar to a two-word expression, i.e.
closeinteraction.
In the second case, two two-word ex-pressions are semantically similar.Then, the pairs (t, s) can be used to modelpositive cases of compositional distributional se-mantics as we know that the word sequence sis compositional and it describes the meaning ofthe word t. The distributional meaning ~t of t isthe expected distributional meaning of s. Conse-quently, the vector ~t is what the CDS model (s)should compositionally obtain from the vectors ofthe components ~s1 .
.
.
~sm of s. This way of ex-tracting similar expressions has some interestingproperties:First property Defined words t are generallysingle words.
Thus, we can extract stable andmeaningful distributional vectors for these wordsand then compare them to the distributional vec-tors composed by CDS model.
This is an impor-tant property as we cannot compare directly thedistributional vector ~s of a word sequence s andthe vector (s) obtained by composing its com-ponents.
As the word sequence s grows in length,the reliability of the vector ~s decreases since thesequence s becomes rarer.Second property Definitions s have a large va-riety of different syntactic structures ranging fromsimple structures as Adjective-Noun to more com-plex ones.
This gives the possibility to train andtest CDS models that take into account syntax.Table 2 represents the distribution of the morefrequent syntactic structures in the definitions ofWordNet1 (Miller, 1995).1Definitions were extracted from WordNet 3.0 and wereparsed with the Charniak parser (Charniak, 2000)1267Freq.
Structure2635 (FRAG (PP (IN) (NP (DT) (JJ) (NN))))833 (NP (DT) (JJ) (NN))811 (NP (NNS))645 (NP (NNP))623 (S (VP (VB) (ADVP (RB))))610 (NP (JJ) (NN))595 (NP (NP (DT) (NN)) (PP (IN) (NP (NN))))478 (NP (NP (DT) (NN)) (PP (IN) (NP (NNP))))451 (FRAG (PP (IN) (NP (NN))))419 (FRAG (RB) (ADJP (JJ)))375 (S (VP (VB) (PP (IN) (NP (DT) (NN)))))363 (S (VP (VB) (PP (IN) (NP (NN)))))342 (NP (NP (DT) (NN)) (PP (IN) (NP (DT) (NN))))341 (NP (DT) (JJ) (JJ) (NN))330 (ADJP (RB) (JJ))307 (NP (JJ) (NNS))244 (NP (DT) (NN) (NN))241 (S (NP (NN)) (NP (NP (NNS)) (PP (IN) (NP (DT) (NNP)))))239 (NP (NP (DT) (JJ) (NN)) (PP (IN) (NP (DT) (NN))))Table 2: Top 20 syntactic structures of WordNetdefinitions4.2 Extracting Negative Examples fromWord EtymologyIn order to devise complete training and testingsets for CDS models, we need to find a sensibleway to extract negative examples.
An option is torandomly generate totally unrelated triples for thenegative examples set, NE.
In this case, due todata sparseness NE would mostly contain triples(~z, ~x, ~y) where it is expected that ~z 6= (xy).
Yet,these can be too generic and too loosely related tobe interesting cases.Instead we attempt to extract sets of negativepairs (t,s) comparable with the one used for build-ing the training set E. The target word t shouldbe a single word and s should be a sequence ofwords.
The latter should be a sequence of wordsrelated by construction to t but the meaning of tand s should be unrelated.The idea is the following: many words are et-ymologically derived from very old or ancientwords.
These words represent a collocation whichis in general not related to the meaning of thetarget word.
For example, the word philosophyderives from two Greek words philos (beloved)and sophia (wisdom).
However, the use of theword philosophy in not related to the collocationbeloved wisdom.
This word has lost its origi-nal compositional meaning.
The following tableshows some more etymologically complex wordsalong with the compositionally unrelated colloca-tions:target word compositionally unrelated seq.municipal receive dutyoctopus eight footAs the examples suggest, we are able to build aset NE with features similar to the features ofN .
In particular, each target word is paired witha related word sequence derived from its etymol-ogy.
These etymologically complex words are un-related to the corresponding compositional collo-cations.
To derive a set NE with the above char-acteristics we can use dictionaries containing ety-mological information as Wiktionary2.5 Experimental evaluationIn the previous sections, we presented the esti-mated additive model (EAM): our approach to es-timate the parameters of a generic additive modelfor CDS.
In this section, we experiment with thismodel to determine whether it performs betterthan existing models: the basic additive model(BAM), the basic multiplicative model (BMM),the basic additive model with selectional pref-erences (BAM-SP), and the basic multiplicativemodel with selectional preferences (BMM-SP)(c.f.
Sec.
2).
In succession, we explore whetherour estimated additive model (EAM) is better thanany possible BAM obtained with parameter ad-justment.
In the rest of the section, we firstly givethe experimental setup and then we discuss the ex-periments and the results.5.1 Experimental setupOur experiments aim to compare compositionaldistributional semantic (CDS) models  with re-spect to their ability of detecting statistically sig-nificant difference between sets E and NE.
Inparticular, the average similarity sim(~z,(xy))for (~z, ~x, ~y) ?
E should be significantly differentfrom sim(~z,(xy)) for (~z, ~x, ~y) ?
NE.
In thissection, we describe the chosen similarity mea-sure sim, statistical significance testing and con-struction details for the training and testing set.Cosine similarity was used to compare the con-text vector ~z representing the target word z withthe composed vector (xy) representing the con-text vector of sequence x y. Cosine similarity be-2http://www.wiktionary.org1268tween two vectors ~x and ~y of the same dimensionis defined as:sim(~x, ~y) = ~x ?
~y?~x?
?~y?
(22)where ?
is the dot product and ?~a?
is the magni-tude of vector ~a computed the Euclidean norm.To evaluate whether a CDS model distinguishespositive examples E from negative examplesNE, we test if the distribution of similaritiessim(~z,(xy)) for (~z, ~x, ~y) ?
E is statisticallydifferent from the distribution of the same simi-larities for (~z, ~x, ~y) ?
NE.
For this purpose, weused Student?s t-test for two independent samplesof different sizes.
t-test assumes that the two dis-tributions are Gaussian and determines the prob-ability that they are similar, i.e., derive from thesame underlying distribution.
Low probabilitiesindicate that the distributions are highly dissimilarand that the corresponding CDS model performswell, as it detects statistically different similaritiesfor the positive set E and the negative set NE.Based on the null hypothesis that the means ofthe two samples are equal, ?1 = ?2, Student?s t-test takes into account the sizes N , means M andvariances s2 of the two samples to compute thefollowing value:t = (M1 ?M2) ?1?2(s21 + s22)df ?Nh(23)where df = N1 + N2 ?
2 stands for the degreesof freedom and Nh = 2(N?11 + N?12 )?1 is theharmonic mean of the sample sizes.
Given thestatistic t and the degrees of freedom df , we cancompute the corresponding p-value, i.e., the prob-ability that the two samples derive from the samedistribution.
The null hypothesis can be rejected ifthe p-value is below the chosen threshold of statis-tical significance (usually 0.1, 0.05 or 0.01), oth-erwise it is accepted.
In our case, rejecting thenull hypothesis means that the similarity values ofinstances of E are significantly different from in-stances of NE, and that the corresponding CDSmodel perform well.
p-value can be used as a per-formance ranking function for CDS models.We constructed two sets of instances: (a) aset containing Adjective-Noun or Noun-Noun se-NN set VN setBAM 0.05690 0.50753BMM 0.20262 0.37523BAM-SP 0.42574 0.01710BMM-SP <1.00E-10 0.23552EAM (k=20) 0.00431 0.00453Table 3: Probability of confusing E and NE withdifferent CDS modelsquences (NN set); and (b) a set containing Verb-Noun sequences (VN set).
Capturing differentsyntactic relations, these two sets can support thatour results are independent from the syntactic re-lation between the words of each sequence.
Foreach set, we used WordNet for extracting positiveexamples E and Wiktionary for extracting nega-tive examples NE as described in Section 4.
Weobtained the following sets: (a) NN consists of1065 word-sequence pairs from WordNet defini-tions and 377 pairs extracted from Wiktionary;and (b) VN consists of 161 word-sequence pairsfrom WordNet definitions and 111 pairs extractedfrom Wiktionary.
We have then divided these twosets in two parts of 50% each, for training andtesting.
Instances of the training part of E havebeen used to estimate matricesA andB for modelEAM , while the testing parts have been used fortesting all models.
Frequency vectors for all sin-gle words occurring in the above pairs were con-structed from the British National Corpus usingsentences as contextual windows and words asfeatures.
The resulting space has 689191 features.5.2 Results and AnalysisThe first set of experiments compares EAM withother existing CDS models: BAM, BMM, BAM-SP, and BMM-SP.
Results are shown in Table 3.The table reports the p-value, i.e., the probabilityof confusing the positive set E and the negativeset NE for all models.
Lower probabilities char-acterise better models.
Probabilities below 0.05indicate that the model detects a statistically sig-nificant difference between setsE andNE.
EAMhas been computed with k = 20 different dimen-sions for the pseudo-inverse matrix.
The two basicadditive models (BAM and BAM-SP) have beencomputed for ?
= ?
= 1.1269NN set V N setFigure 1: p-values of BAM with different values for parameter ?
(where ?
= 1 ?
?)
and of EAM fordifferent approximations of the SVD pseudo-inverse matrix (k)The first observation is that EAM models sig-nificantly separate positive from negative exam-ples for both sets.
This is not the case for anyof the other models.
Only, the selectional prefer-ences based models in two cases have this prop-erty, but this cannot be generalised: BAM-SP onthe VN set and BMM-SP on the NN set.
In gen-eral, these models do not offer the possibility ofseparating positive from negative examples.In the second set of experiments, we attempt toinvestigate whether simple parameter adjustmentof BAM can perform better than EAM.
Results areshown in figure 1.
Plots show the basic additivemodel (BAM) with different values for parameter?
(where ?
= 1??)
and EAM computed for dif-ferent approximations of the SVD pseudo-inversematrix (i.e., with different k).
The x-axis of theplots represents parameter ?
and the y-axis repre-sents the probability of confusing the positive setE and the negative setNE.
The representation fo-cuses on the performance ofBAM with respect todifferent ?
values.
The performance of EAM fordifferent k values is represented with horizontallines.
Probabilities of different models are directlycomparable.
Line SS represents the threshold ofstatistical significance; the value below which thedetected difference between the E and NE setsbecomes statistically significant.Experimental results show some interestingfacts: While BAM for ?
> 0 perform better thanEAM computed with k = 1 in the NN set, theydo not perform better in the VN set.
EAM withk = 1 has 1 degree of freedom corresponding to1 parameter, the same as BAM.
The parameter ofEAM is tuned on the training set, in contrast to?, the parameter of BAM.
Increasing the numberof considered dimensions, k of EAM, estimatedmodels outperform BAM for all values of param-eter ?.
Moreover, EAM detect a statistically sig-nificant difference between theE and theNE setsfor k ?
10 and k = 20 for the NN set and theVN set set, respectively.
Simple parametrisationof a BAM does not outperform the proposed esti-mated additive model.6 ConclusionsIn this paper, we presented an innovative methodto estimate linear compositional distributional se-mantics models.
The core of our approach con-sists on two parts: (1) providing a method to es-timate the regression problem with multiple de-pendent variables and (2) providing a training setderived from dictionary definitions.
Experimentsshowed that our model is highly competitive withrespect to state-of-the-art models for composi-tional distributional semantics.ReferencesCharniak, Eugene.
2000.
A maximum-entropy-inspired parser.
In proceedings of the 1st NAACL,pages 132?139, Seattle, Washington.Cook, Paul, Afsaneh Fazly, and Suzanne Stevenson.2008.
The VNC-Tokens Dataset.
In proceedingsof the LREC Workshop: Towards a Shared Task forMultiword Expressions (MWE 2008), Marrakech,Morocco.1270Deerwester, Scott C., Susan T. Dumais, Thomas K.Landauer, George W. Furnas, and Richard A. Harsh-man.
1990.
Indexing by latent semantic analysis.Journal of the American Society of Information Sci-ence, 41(6):391?407.Erk, Katrin and Sebastian Pado?.
2008.
A structuredvector space model for word meaning in context.
Inproceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 897?906.
Association for Computational Linguistics.Erk, Katrin.
2007.
A simple, similarity-based modelfor selectional preferences.
In proceedings of ACL.Association for Computer Linguistics.Fallucchi, Francesca and Fabio Massimo Zanzotto.2009.
SVD feature selection for probabilistic tax-onomy learning.
In proceedings of the Workshop onGeometrical Models of Natural Language Seman-tics, pages 66?73.
Association for ComputationalLinguistics, Athens, Greece.Firth, John R. 1957.
Papers in Linguistics.
OxfordUniversity Press, London.Golub, Gene and William Kahan.
1965.
Calculat-ing the singular values and pseudo-inverse of a ma-trix.
Journal of the Society for Industrial and Ap-plied Mathematics, Series B: Numerical Analysis,2(2):205?224.Harris, Zellig.
1964.
Distributional structure.
In Katz,Jerrold J. and Jerry A. Fodor, editors, The Philos-ophy of Linguistics, New York.
Oxford UniversityPress.Jones, Michael N. and Douglas J. K. Mewhort.
2007.Representing word meaning and order informationin a composite holographic lexicon.
PsychologicalReview, 114:1?37.Kim, Su N. and Timothy Baldwin.
2008.
Standard-ised evaluation of english noun compound inter-pretation.
In proceedings of the LREC Workshop:Towards a Shared Task for Multiword Expressions(MWE 2008), pages 39?42, Marrakech, Morocco.Korkontzelos, Ioannis and Suresh Manandhar.
2009.Detecting compositionality in multi-word expres-sions.
In proceedings of ACL-IJCNLP 2009, Sin-gapore.Li, Ping, Curt Burgess, and Kevin Lund.
2000.
Theacquisition of word meaning through global lexicalco-occurrences.
In proceedings of the 31st ChildLanguage Research Forum.Lin, Dekang and Patrick Pantel.
2001.
DIRT-discovery of inference rules from text.
In Proceed-ings of the ACM Conference on Knowledge Discov-ery and Data Mining (KDD-01).
San Francisco, CA.Manning, Christopher D., Prabhakar Raghavan, andHinrich Schu?tze.
2008.
Introduction to InformationRetrieval.
Cambridge University Press, Cambridge,UK.McCarthy, Diana and John Carroll.
2003.
Disam-biguating nouns, verbs, and adjectives using auto-matically acquired selectional preferences.
Compu-tational Linguistics, 29(4):639?654.Miller, George A. and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, VI:1?28.Miller, George A.
1995.
WordNet: A lexicaldatabase for English.
Communications of the ACM,38(11):39?41.Mitchell, Jeff and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio.Association for Computational Linguistics.Nicholson, Jeremy and Timothy Baldwin.
2008.
Inter-preting compound nominalisations.
In proceedingsof the LREC Workshop: Towards a Shared Task forMultiword Expressions (MWE 2008), pages 43?45,Marrakech, Morocco.Pado, Sebastian and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Penrose, Roger.
1955.
A generalized inverse for ma-trices.
In Proceedings of Cambridge PhilosophicalSociety.Pollard, Carl J. and Ivan A.
Sag.
1994.
Head-drivenPhrase Structured Grammar.
Chicago CSLI, Stan-ford.Schone, Patrick and Daniel Jurafsky.
2001.
Isknowledge-free induction of multiword unit dictio-nary headwords a solved problem?
In Lee, Lil-lian and Donna Harman, editors, proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 100?108.Villavicencio, Aline.
2003.
Verb-particle construc-tions and lexical resources.
In proceedings ofthe ACL 2003 workshop on Multiword expressions,pages 57?64, Morristown, NJ, USA.
Association forComputational Linguistics.1271
