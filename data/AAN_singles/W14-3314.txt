Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 136?141,Baltimore, Maryland USA, June 26?27, 2014.c?2014 Association for Computational LinguisticsThe DCU-ICTCAS MT system at WMT 2014 on German-EnglishTranslation TaskLiangyou Li?, Xiaofeng Wu?, Santiago Cort?es Va?
?llo?Jun Xie?, Andy Way?, Qun Liu??
?CNGL Centre for Global Intelligent Content, School of ComputingDublin City University, Dublin 9, Ireland?Key Laboratory of Intelligent Information Processing, Institute of Computing TechnologyChinese Academy of Sciences, Beijing, China{liangyouli,xiaofengwu,scortes,away,qliu}@computing.dcu.iexiejun@ict.ac.cnAbstractThis paper describes the DCU submis-sion to WMT 2014 on German-Englishtranslation task.
Our system uses phrase-based translation model with several pop-ular techniques, including LexicalizedReordering Model, Operation SequenceModel and Language Model interpolation.Our final submission is the result of sys-tem combination on several systems whichhave different pre-processing and align-ments.1 IntroductionOn the German-English translation task of WMT2014, we submitted a system which is built withMoses phrase-based model (Koehn et al., 2007).For system training, we use all providedGerman-English parallel data, and conducted sev-eral pre-processing steps to clean the data.
In ad-dition, in order to improve the translation quali-ty, we adopted some popular techniques, includ-ing three Lexicalized Reordering Models (Axel-rod et al., 2005; Galley and Manning, 2008), a 9-gram Operation Sequence Model (Durrani et al.,2011) and Language Model interpolation on sev-eral datasets.
And then we use system combina-tion on several systems with different settings toproduce the final outputs.Our phrase-based systems are tuned with k-bestMIRA (Cherry and Foster, 2012) on developmentset.
We set the maximum iteration to be 25.The Language Models in our systems aretrained with SRILM (Stolcke, 2002).
We trainedCorpus Filtered Out (%)Bilingual 7.17Monolingual (English) 1.05Table 1: Results of language detection: percentageof filtered out sentencesa 5-gram model with Kneser-Ney discounting(Chen and Goodman, 1996).In the next sections, we will describe our systemin detail.
In section 2, we will explain our pre-processing steps on corpus.
Then in section 3, wewill describe some techniques we have tried forthis task and the experiment results.
In section 4,our final configuration for submitted system willbe presented.
And we conclude in the last section.2 Pre-processingWe use all the training data for German-Englishtranslation, including Europarl, News Commen-tary and Common Crawl.
The first thing we no-ticed is that some Non-German and Non-Englishsentences are included in our training data.
So weapply Language Detection (Shuyo, 2010) for bothmonolingual and bilingual corpora.
For mono-lingual data (only including English sentences inour task), we filter out sentences which are detect-ed as other language with probability more than0.999995.
And for bilingual data, A sentencepair is filtered out if the language detector detect-s a different language with probability more than0.999995 on either the source or the target.
Thefiltering results are given in Table 1.In our experiment, German compound word-s are splitted based on frequency (Koehn and136Knight, 2003).
In addition, for both monolingualand bilingual data, we apply tokenization, nor-malizing punctuation and truecasing using Mosesscripts.
For parallel training data, we also filter outsentence pairs containing more than 80 tokens oneither side and sentence pairs whose length ratiobetween source and target side is larger than 3.3 TechniquesIn our preliminary experiments, we take newstest2013 as our test data and newstest 2008-2012 asour development data.
In total, we have morethan 10,000 sentences for tuning.
The tuning stepwould be very time-consuming if we use them al-l.
So in this section, we use Feature Decay Al-gorithm (FDA) (Bic?ici and Yuret, 2014) to select2000 sentences as our development set.
Table 2shows that system performance does not increasewith larger tuning set and the system using only2K sentences selected by FDA is better than thebaseline tuned with all the development data.In this section, alignment model is trainedby MGIZA++ (Gao and Vogel, 2008) withgrow-diag-final-and heuristic function.And other settings are mostly default values inMoses.3.1 Lexicalized Reordering ModelGerman and English have different word orderwhich brings a challenge in German-English ma-chine translation.
In our system, we adopt threeLexicalized Reordering Models (LRMs) for ad-dressing this problem.
They are word-based LRM(wLRM), phrase-based LRM (pLRM) and hierar-chal LRM (hLRM).These three models have different effect on thetranslation.
Word-based and phrase-based LRMsare focus on local reordering phenomenon, whilehierarchical LRM could be applied into longer re-ordering problem.
Figure 1 shows the differences(Galley and Manning, 2008).
And Table 3 showseffectiveness of different LRMs.In our system based on Moses, weuse wbe-msd-bidirectional-fe,phrase-msd-bidirectional-fe andhier-mslr-bidirectional-fe to specifythese three LRMs.
From Table 2, we could seethat LRMs significantly improves the translation.Figure 1: Occurrence of a swap according tothe three orientation models: word-based, phrase-based, and hierarchical.
Black squares represen-t word alignments, and gray squares represen-t blocks identified by phrase-extract.
In (a), blockbi= (ei, fai) is recognized as a swap according toall three models.
In (b), biis not recognized as aswap by the word-based model.
In (c), biis rec-ognized as a swap only by the hierarchical model.
(Galley and Manning, 2008)3.2 Operation Sequence ModelThe Operation Sequence Model (OSM) (Durraniet al., 2011) explains the translation procedure asa linear sequence of operations which generatessource and target sentences in parallel.
Durraniet al.
(2011) defined four translation operations:Generate(X,Y), Continue Source Concept, Gener-ate Source Only (X) and Generate Identical, aswell as three reordering operations: Insert Gap,Jump Back(W) and Jump Forward.
These oper-ations are described as follows.?
Generate(X,Y) make the words in Y and thefirst word in X added to target and sourcestring respectively.?
Continue Source Concept adds the word inthe queue from Generate(X,Y) to the sourcestring.?
Generate Source Only (X) puts X in thesource string at the current position.?
Generate Identical generates the same wordfor both sides.?
Insert Gap inserts a gap in the source side forfuture use.?
Jump Back (W) makes the position for trans-lation be the Wth closest gap to the currentposition.?
Jump Forward moves the position to the in-dex after the right-most source word.137Systems Tuning Set newstest 2013Baseline ?
24.1+FDA ?
24.2+LRMs 24.0 25.4+OSM 24.4 26.2+LM Interpolation 24.6 26.4+Factored Model ?
25.9+Sparse Feature 25.6 25.9+TM Combination 24.1 25.4+OSM Interpolation 24.4 26.0Table 2: Preliminary results on tuning set and test set (newstest 2013).
All scores on test set are case-sensitive BLEU[%] scores.
And scores on tuning set are case-insensitive BLEU[%] directly from tuningresult.
Baseline uses all the data from newstest 2008-2012 for tuning.Systems Tuning Set (uncased) newstest 2013Baseline+FDA ?
24.2+wLRM 23.8 25.1+pLRM 23.9 25.2+hLRM 24.0 25.4+pLRM 23.8 25.1+hLRM 23.7 25.2Table 3: System BLEU[%] scores when different LRMs are adopted.The probability of an operation sequence O =(o1o2?
?
?
oJ) is:p(O) =J?j=1p(oj|oj?n+1?
?
?
oj?1) (1)where n indicates the number of previous opera-tions used.In this paper we train a 9-gram OSM on train-ing data and integrate this model directly into log-linear framework (OSM is now available to usein Moses).
Our experiment shows OSM improvesour system by about 0.8 BLEU (see Table 2).3.3 Language Model InterpolationIn our baseline, Language Model (LM) is trainedon all the monolingual data provided.
In this sec-tion, we try to build a large language model by in-cluding data from English Gigaword fifth edition(only taking partial data with size of 1.6G), En-glish side of UN corpus and English side of 109French-English corpus.
Instead of training a s-ingle model on all data, we interpolate languagemodels trained on each subset (monolingual dataprovided is splitted into three parts: News 2007-2013, Europarl and News Commentary) by tuningweights to minimize perplexity of language modelmeasured on the target side of development set.In our experiment, after interpolation, the lan-guage model doesn?t get a much lower perplexity,but it slightly improves the system, as shown inTable 2.3.4 Other TriesIn addition to the techniques mentioned above, wealso try some other approaches.
Unfortunately al-l of these methods described in this section arenon-effective in our experiments.
The results areshown in Table 2.?
Factored Model (Koehn and Hoang, 2007):We tried to integrate a target POS factoredmodel into our system with a 9-gram POSlanguage model to address the problem ofword selection and word order.
But ex-periment doesn?t show improvement.
TheEnglish POS is from Stanford POS Tagger(Toutanova et al., 2003).?
Translation Model Combination: In this ex-periment, we try to use the method of (Sen-nrich, 2012) to combine phrase tables or re-ordering tables from different subsets of data138to minimize perplexity measured on develop-ment set.
We try to split the training data intwo ways.
One is according to data source,resulting in three subsets: Europarl, NewsCommentary and Common Crawl.
Anotherone is to use data selection.
We use FDA toselect 200K sentence pairs as in-domain dataand the rest as out-domain data.
Unfortunate-ly both experiments failed.
In Table 2, we on-ly report results of phrase table combinationon FDA-based data sets.?
OSM Interpolation: Since OSM in our sys-tem could be taken as a special languagemodel, we try to use the idea of interpolationsimilar with language model to make OSMadapted to some data.
Training data are s-plitted into two subsets with FDA.
We train9-gram OSM on each subsets and interpolatethem according to OSM trained on the devel-opment set.?
Sparse Features: For each source phrase,there is usually more than one correspondingtranslation option.
Each different translationmay be optimal in different contexts.
Thusin our systems, similar to (He et al., 2008)which proposed a Maximum Entropy-basedrule selection for the hierarchical phrase-based model, features which describe thecontext of phrases, are designed to select theright translation.
But different with (He etal., 2008), we use sparse features to mod-el the context.
And instead of using syn-tactic POS, we adopt independent POS-likefeatures: cluster ID of word.
In our experi-ment mkcls was used to cluster words into 50groups.
And all features are generalized tocluster ID.4 SubmissionBased on our preliminary experiments in the sec-tion above, we use LRMs, OSM and LM inter-polation in our final system for newstest 2014.But as we find that Language Models trained onUN corpus and 109French-English corpus havea very high perplexity and in order to speed upthe translation by reducing the model size, in thissection, we interpolate only three language model-s from monolingual data provided, English Giga-word fifth edition and target side of training data.In addition, we also try some different methods forfinal submission.
And the results are shown in Ta-ble 4.?
Development Set Selection: Instead of usingFDA which is dependent on test set, we usethe method of (Nadejde et al., 2013) to se-lect tuning set from newstest 2008-2013 forthe final system.
We only keep 2K sentenceswhich have more than 30 words and higherBLEU score.
The experiment result is shownin Table 4 ( The system is indicated as Base-line).?
Pre-processing: In our preliminary exper-iments, sentences are tokenized withoutchanging hyphen.
Thus we build another sys-tem where all the hyphens are tokenized ag-gressively.?
SyMGIZA++: Better alignment could lead tobetter translation.
So we carry out some ex-periments on SyMGIZA++ aligner (Junczys-Dowmunt and Sza, 2012), which modifies theoriginal IBM/GIZA++ word alignment mod-els to allow to update the symmetrized mod-els between chosen iterations of the originaltraining algorithms.
Experiment shows thisnew alignment improves translation quality.?
Multi-alignment Selection: We also try to usemulti-alignment selection (Tu et al., 2012)to generate a ?better?
alignment from threealignmens: MGIZA++ with function grow-diag-final-and, SyMGIZA++ with functiongrow-diag-final-and and fast alignment (Dy-er et al., 2013).
Although this method showcomparable or better result on developmentset, it fails on test set.Since we build a few systems with differentsetting on Moses phrase-based model, a straight-forward thinking is to obtain the better transla-tion from several different translation systems.
Sowe use system combination (Heafield and Lavie,2010) on the 1-best outputs of three systems (in-dicated with?in table 4).
And this results in ourbest system so far, as shown in Table 4.
In our finalsubmission, this result is taken as primary.5 ConclusionThis paper describes our submitted system toWMT 2014 in detail.
This system is based on139Systems Tuning Set newstest 2014Baseline?34.2 25.6+SyMGIZA++?34.3 26.0+Multi-Alignment Selection 34.4 25.6+Hyphen-Splitted 33.9 25.9+SyMGIZA++?34.0 26.0+Multi-Alignment Selection 34.0 25.7System Combination ?
26.5Table 4: Experiment results on newstest 2014.
We report case-sensitive BLEU[%] score on test set andcase-insensitive BLEU[%] on tuning set which is directly from tuning result.
Baseline is the phrase-basedsystem with LRMs, OSM and LM interpolation on smaller datasets, tuned with selected development set.Systems indicated with?are used for system combination.Moses phrase-based model, and integrates Lexi-calized Reordering Models, Operation SequenceModel and Language Model interpolation.
Al-so system combination is used on several system-s which have different pre-processing and align-ment.AcknowledgmentsThis work is supported by EC Marie-Curie initialtraining Network EXPERT (EXPloiting Empiri-cal appRoaches to Translation) project (http://expert-itn.eu).
Thanks to Johannes Lev-eling for his help on German compound splitting.And thanks to Jia Xu and Jian Zhang for their ad-vice and help on this paper and experiments.ReferencesAmittai Axelrod, Ra Birch Mayne, Chris Callison-burch, Miles Osborne, and David Talbot.
2005.
Ed-inburgh system description for the 2005 iwslt speechtranslation evaluation.
In Proceedings of the Inter-national Workshop on Spoken Language Translation(IWSLT).Ergun Bic?ici and Deniz Yuret.
2014.
Optimizing in-stance selection for statistical machine translationwith feature decay algorithms.
IEEE/ACM Transac-tions On Audio, Speech, and Language Processing(TASLP).Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th Annual Meet-ing on Association for Computational Linguistics,ACL ?96, pages 310?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Colin Cherry and George Foster.
2012.
Batch tuningstrategies for statistical machine translation.
In Pro-ceedings of the 2012 Conference of the North Amer-ican Chapter of the Association for ComputationalLinguistics: Human Language Technologies, NAA-CL HLT ?12, pages 427?436, Stroudsburg, PA, US-A.
Association for Computational Linguistics.Nadir Durrani, Helmut Schmid, and Alexander Fraser.2011.
A joint sequence translation model with in-tegrated reordering.
In Proceedings of the 49th An-nual Meeting of the Association for ComputationalLinguistics: Human Language Technologies - Vol-ume 1, HLT ?11, pages 1045?1054, Stroudsburg, PA,USA.
Association for Computational Linguistics.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameteriza-tion of ibm model 2.
In Proceedings of NAACL.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Process-ing, pages 848?856, Honolulu, Hawaii, October.
As-sociation for Computational Linguistics.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, SETQA-NLP ?08, pages 49?57, Stroudsburg, PA, USA.
Association for Compu-tational Linguistics.Zhongjun He, Qun Liu, and Shouxun Lin.
2008.
Im-proving statistical machine translation using lexical-ized rule selection.
In Proceedings of the 22nd In-ternational Conference on Computational Linguis-tics (Coling 2008), pages 321?328, Manchester, UK,August.
Coling 2008 Organizing Committee.Kenneth Heafield and Alon Lavie.
2010.
Combiningmachine translation output with open source: TheCarnegie Mellon multi-engine machine translationscheme.
The Prague Bulletin of Mathematical Lin-guistics, 93:27?36, January.Marcin Junczys-Dowmunt and Arkadiusz Sza.
2012.Symgiza++: Symmetrized word alignment modelsfor statistical machine translation.
In Pascal Bouvry,MieczysawA.
Kopotek, Franck Leprvost, Magorza-ta Marciniak, Agnieszka Mykowiecka, and Henryk140Rybiski, editors, Security and Intelligent Informa-tion Systems, volume 7053 of Lecture Notes in Com-puter Science, pages 379?390.
Springer Berlin Hei-delberg.Philipp Koehn and Hieu Hoang.
2007.
Factored trans-lation models.
In Proceedings of the 2007 JointConference on Empirical Methods in Natural Lan-guage Processing and Computational Natural Lan-guage Learning (EMNLP-CoNLL), pages 868?876.Philipp Koehn and Kevin Knight.
2003.
Empiricalmethods for compound splitting.
In Proceedings ofthe Tenth Conference on European Chapter of theAssociation for Computational Linguistics - Volume1, EACL ?03, pages 187?193, Stroudsburg, PA, US-A.
Association for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertol-di, Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Maria Nadejde, Philip Williams, and Philipp Koehn.2013.
Edinburgh?s syntax-based machine transla-tion systems.
In Proceedings of the Eighth Work-shop on Statistical Machine Translation, pages 170?176, Sofia, Bulgaria, August.
Association for Com-putational Linguistics.Rico Sennrich.
2012.
Perplexity minimization fortranslation model domain adaptation in statisticalmachine translation.
In Proceedings of the 13thConference of the European Chapter of the Associa-tion for Computational Linguistics, pages 539?549,Avignon, France, April.
Association for Computa-tional Linguistics.Nakatani Shuyo.
2010.
Language detection library forjava.Andreas Stolcke.
2002.
Srilm ?
an extensible languagemodeling toolkit.
In Proceedings of the Internation-al Conference Spoken Language Processing, pages901?904, Denver, CO.Kristina Toutanova, Dan Klein, Christopher D. Man-ning, and Yoram Singer.
2003.
Feature-rich part-of-speech tagging with a cyclic dependency network.In Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology- Volume 1, NAACL ?03, pages 173?180, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Zhaopeng Tu, Yang Liu, Yifan He, Josef van Genabith,Qun Liu, and Shouxun Lin.
2012.
Combining mul-tiple alignments to improve machine translation.
InCOLING (Posters), pages 1249?1260.141
