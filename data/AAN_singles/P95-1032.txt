A Pattern Matching Method for Finding Noun and Proper NounTranslations from Noisy Parallel CorporaPascale FungComputer  Science Depar tmentCo lumbia  Univers i tyNew York, NY 10027pascale?cs, columbia, eduAbstractWe present a pattern matching method forcompiling a bilingual exicon of nouns andproper nouns from unaligned, noisy paral-lel texts of Asian/Indo-European languagepairs.
Tagging information of one lan-guage is used.
Word frequency and posi-tion information for high and low frequencywords are represented in two different vec-tor forms for pattern matching.
New an-chor point finding and noise eliminationtechniques are introduced.
We obtaineda 73.1% precision.
We also show how theresults can be used in the compilation ofdomain-specific noun phrases.1 Bilingual lexicon compilationwithout  sentence  a l ignmentAutomatically compiling a bilingual lexicon of nounsand proper nouns can contribute significantly tobreaking the bottleneck in machine translation andmachine-aided translation systems.
Domain-specificterms are hard to translate because they often donot appear in dictionaries.
Since most of these termsare nouns, proper nouns or noun phrases, compilinga bilingual lexicon of these word groups is an impor-tant first step.We have been studying robust lexicon compilationmethods which do not rely on sentence alignment.Existing lexicon compilation methods (Kupiec 1993;Smadja & McKeown 1994; Kumano & Hirakawa1994; Dagan et al 1993; Wu & Xia 1994) all attemptto extract pairs of words or compounds that aretranslations of each other from previously sentence-aligned, parallel texts.
However, sentence align-ment (Brown et al 1991; Kay & RSscheisen 1993;Gale & Church 1993; Church 1993; Chen 1993;Wu 1994) is not always practical when corpora haveunclear sentence boundaries or with noisy text seg-ments present in only one language.Our proposed algorithm for bilingual exicon ac-quisition bootstraps off of corpus alignment proce-dures we developed earlier (Fung & Church 1994;Fung & McKeown 1994).
Those procedures at-tempted to align texts by finding matching wordpairs and have demonstrated their effectiveness forChinese/English and Japanese/English.
The mainfocus then was accurate alignment, but the proce-dure produced a small number of word translationsas a by-product.
In contrast, our new algorithm per-forms a minimal alignment, o facilitate compiling amuch larger bilingual exicon.The paradigm for Fung ~: Church (1994); Fung& McKeown (1994) is based on two main steps -find a small bilingual primary lexicon, use the textsegments which contain some of the word pairs inthe lexicon as anchor points for alignment, align thetext, and compute a better secondary lexicon fromthese partially aligned texts.
This paradigm can beseen as analogous to the Estimation-Maximizationstep in Brown el al.
(1991); Dagan el al.
(1993); Wu& Xia (1994).For a noisy corpus without sentence boundaries,the primary lexicon accuracy depends on the robust-ness of the algorithm for finding word translationsgiven no a priori information.
The reliability of theanchor points will determine the accuracy of the sec-ondary lexicon.
We also want an algorithm thatbypasses a long, tedious entence or text alignmentstep.2 A lgor i thm overv iewWe treat the bilingual exicon compilation problemas a pattern matching problem - each word sharessome common features with its counterpart in thetranslated text.
We try to find the best repre-sentations of these features and the best ways tomatch them.
We ran the algorithm on a small Chi-nese/English parallel corpus of approximately 5760unique English words.The outline of the algorithm is as follows:1.
Tag the English hal f  of the paral lel  text.In the first stage of the algorithm, only En-glish words which are tagged as nouns or propernouns are used to match words in the Chinesetext.2362.
Compute  the  pos i t iona l  d i f ference vectorof  each word.
Each of these nouns or propernouns is converted from their positions in thetext into a vector.3.
Match  pairs  of  pos i t iona l  d i f ference vec-tors~ giv ing scores.
All vectors from Englishand Chinese are matched against each other byDynamic Time Warping (DTW).4.
Select  a pr imary  lex icon us ing the  scores.A threshold is applied to the DTW score of eachpair, selecting the most correlated pairs as thefirst bilingual exicon.5.
F ind  anchor  po in ts  us ing the  pr imary  lex-icon.
The algorithm reconstructs the DTWpaths of these positional vector pairs, giving usa set of word position points which are filteredto yield anchor points.
These anchor points areused for compiling a secondary lexicon.6.
Compute  a pos i t ion  b inary  vector  foreach word  us ing the  anchor  points .
The re-maining nouns and proper nouns in English andall words in Chinese are represented in a non-linear segment binary vector form from their po-sitions in the text.7.
Match  b inary  vectors  to  y ie ld  a secondarylexicon.
These vectors are matched againsteach other by mutual information.
A confidencescore is used to threshold these pairs.
We ob-tain the secondary bilingual lexicon from thisstage.In Section 3, we describe the first four stages inour algorithm, cumulating in a primary lexicon.
Sec-tion 4 describes the next anchor point finding stage.Section 5 contains the procedure for compiling thesecondary lexicon.3 F ind ing  h igh  f requency  b i l ingua lword  pa i rsWhen the sentence alignments for the corpus are un-known, standard techniques for extracting bilinguallexicons cannot apply.
To make matters worse, thecorpus might contain chunks of texts which appearin one language but not in its translation 1, suggest-ing a discontinuous mapping between some paralleltexts.We have previously shown that using a vector rep-resentation of the frequency and positional informa-tion of a high frequency word was an effective way tomatch it to its translation (Fung & McKeown 1994).Dynamic Time Warping, a pattern recognition tech-nique, was proposed as a good way to match these1This was found to be the case in the Japanese trans-lation of the AWK manual (Church et al 1993).
TheJapanese AWK was also found to contain different pro-gramming examples from the English version.vectors.
In our new algorithm, we use a similar po-sitional difference vector representation and DTWmatching techniques.
However, we improve on thematching efficiency by installing tagging and statis-tical filters.
In addition, we not only obtain a scorefrom the DTW matching between pairs of words,but we also reconstruct the DTW paths to get thepoints of the best paths as anchor points for use inlater stages.3.1 Tagging to ident i fy  nounsSince the positional difference vector representationrelies on the fact that words which are similar inmeaning appear fairly consistently in a parallel text,this representation is best for nouns or proper nounsbecause these are the kind of words which have con-sistent ranslations over the entire text.As ultimately we will be interested in findingdomain-specific terms, we can concentrate our ef-fort on those words which are nouns or proper nounsfirst.
For this purpose, we tagged the English part ofthe corpus by a modified POS tagger, and apply ouralgorithm to find the translations for words whichare tagged as nouns, plural nouns or proper nounsonly.
This produced a more useful list of lexicon andagain improved the speed of our program.3.2 Pos i t iona l  d i f ference vectorsAccording to our previous findings (Fung& McK-eown 1994), a word and its translated counterpartusually have some correspondence in their frequencyand positions although this correspondence mightnot be linear.
Given the position vector of a wordp\[i\] where the values of this vector are the positionsat which this word occurs in the corpus, one cancompute a positional difference vector V\[i- 1\] whereVi i -  1\] = p\[i\]- p\[ i -  1\].
dim(V) is the dimensionof the vector which corresponds to the occurrencecount of the word.For example, if positional difference vectors for theword Governor and its translation in Chinese .~are plotted against their positions in the text, theygive characteristic signals such as shown in Figure 1.The two vectors have different dimensions becausethey occur with different frequencies.
Note that thetwo signals are shifted and warped versions of eachother with some minor noise.3.3 Match ing  pos i t iona l  d i f ference vectorsThe positional vectors have different lengths whichcomplicates the matching process.
Dynamic TimeWarping was found to be a good way to match wordvectors of shifted or warped forms (Fung & McK-eown 1994).
However, our previous algorithm onlyused the DTW score for finding the most correlatedword pairs.
Our new algorithm takes it one step fur-ther by backtracking to reconstruct the DTW pathsand then automatically choosing the best points onthese DTW paths as anchor points.23716G00140Q01200010000800O6OOO4O0O200O050 1OO ~ 150 200 250word pos~ M text"govemor.ch.vec.diff" - -T40001000030080QO20O050 100 150 200word positiorl in text?
govem~.en.vec.diff" - -250Figure 1: Positional difference signals showing similarity between Governor in English and ChineseFor a given pair of vectors V1, V2, we attemptto discover which point in V1 corresponds to whichpoint in V2 .
I f  the two were not scaled, then po-sition i in V1 would correspond to position j in V2where j / i  is a constant.
If we plot V1 against V2,we can get a diagonal ine with slope j/i.
If theyoccurred the same number of times, then every po-sition i in V1 would correspond to one and only oneposition j in V2.
For non-identical vectors, DTWtraces the correspondences between all points in V1and V2 (with no penalty for deletions or insertions).Our DTW algorithm with path reconstruction is asfollows:?
In i t ia l i zat ionwhere~oz(1,1) = ((1,1)?pl(i, 1) = ?
(i, 1) + ~o(i - 1, 1\])toz(1,j) = f f (1 , j )+~o(1 , j -a )9~(a, b) = minimum cost of movingfrom a to b((c,d) = IVl\[c\]- V2\[aq\[for i = 1 ,2 , .
.
.
,Nj = 1 ,2 , .
.
.
,Mg = dim(V1)M = dim(V2)?
Recurs ion~on+l (i, m) min \[~(l, m) + ~o.
(i,/)\]1</<3for nand m= argmin \ [~( / ,  m) + ~n(i, 1)\]1<1<3= 1 ,2 , .
.
.
,N -2= 1 ,2 , .
.
.
,M?
Terminat ion~ON(i, j) = min ~oN-1 (i,/)\] 1</<3\[ I (1 , rt2) +(N(j) = argmin\[~(l,m) + ~oN-x(i,j)\]1_</_<3?
Pa th  reconst ruct ionIn our algorithm, we reconstruct the DTW pathand obtain the points on the path for later use.The DTW path for Governor/~d~,~ is as shownin Figure 2.optimal path - (i, i l , i 2 , .
.
.
, im-2 , j )where in = ~n+l ( in+l ) ,n -- N -  1 ,N-  2 , .
.
.
,1with iN = jWe thresholded the bilingual word pairs obtainedfrom above stages in the algorithm and stored themore reliable pairs as our primary bilingual exicon.3.4 Stat i s t i ca l  f i l tersIf we have to exhaustively match all nouns andproper nouns against all Chinese words, the match-ing will be very expensive since it involves comput-ing all possible paths between two vectors, and thenbacktracking to find the optimal path, and doing thisfor all English/Chinese word pairs in the texts.
Thecomplexity of DTW is @(NM) and the complexityof the matching is O(I JNM) where I is the numberof nouns and proper nouns in the English text, J isthe number of unique words in the Chinese text, Nis the occurrence count of one English word and Mthe occurrence count of one Chinese word.We previously used some frequency difference con-straints and starting point constraints (Fung &McKeown 1994).
Those constraints limited the238W5000001001~pathf| i i i i100otm ~ 300~o 40o00o 50000oFigure 2: Dynamic Time Warping path for Governor in English and Chinesenumber of the pairs of vectors to be compared byDTW.
For example, low frequency words are notconsidered since their positional difference vectorswould not contain much information.
We also ap-ply these constraints in our experiments.
However,there is still many pairs of words left to be compared.To improve the computation speed, we constrainthe vector pairs further by looking at the Euclideandistance g of their means and standard deviations:E = ~/iml - m2) 2 + (~1 - ~2)~If their Euclidean distance is higher than a cer-tain threshold, we filter the pair out and do not useDTW matching on them.
This process eliminatedmost word pairs.
Note that this Euclidean distancefunction helps to filter out word pairs which are verydifferent from each other, but it is not discriminativeenough to pick out the best translation of a word.So for word pairs whose Euclidean distance is belowthe threshold, we still need to use DTW matchingto find the best translation.
However, this Euclideandistance filtering greatly improved the speed of thisstage of bilingual exicon compilation.4 F ind ing  anchor  po in ts  ande l iminat ing  no iseSince the primary lexicon after thresholding is rela-tively small, we would like to compute a secondarylexicon including some words which were not foundby DTW.
At stage 5 of our algorithm, we try tofind anchor points on the DTW paths which dividethe texts into multiple aligned segments for compil-ing the secondary lexicon.
We believe these anchorpoints are more reliable than those obtained by trac-ing all the words in the texts.For every word pair from this lexicon, we had ob-tained a DTW score and a DTW path.
If we plot thepoints on the DTW paths of all word pairs from thelexicon, we get a graph as in the left hand side of Fig-ure 3.
Each point (i, j )  on this graph is on the DTWpath(vl, v2) where vl is from English words in thelexicon and v2 is from the Chinese words in the lexi-con.
The union effect of all these DTW paths showsa salient line approximating the diagonal.
This linecan be thought of the text alignment path.
Its de-parture from the diagonal illustrates that the textsof this corpus are not identical nor linearly aligned.Since the lexicon we computed was not perfect,we get some noise in this graph.
Previous align-ment methods we used such as Church (1993); Fung& Church (1994); Fung & McKeown (1994) wouldbin the anchor points into continuous blocks for arough alignment.
This would have a smoothing ef-fect.
However, we later found that these blocks ofanchor points are not precise enough for our Chi-nese/English corpus.
We found that it is more ad-vantageous to increase the overall reliability of an-chor points by keeping the highly reliable points anddiscarding the rest.From all the points on the union of the DTWpaths, we filter out the points by the following con-ditions: If the point (i, j)  satisfies(slope constraint) j / i  > 600 * N\[0\](window size constraint) i >= 25 -t- iprevious(continuity constraint) j >= Jpreviou,(offset constraini) j - -  jp rev ious  > 500then the point (i, j)  is noise and is discarded.After filtering, we get points such as shown in theright hand side of Figure 3.
There are 388 highly re-liable anchor points.
They divide the texts into 388segments.
The total length of the texts is around100000, so each segment has an average window sizeof 257 words which is considerably onger than a sen-tence length; thus this is a much rougher alignmentthan sentence alignment, but nonetheless we still geta bilingual exicon out of it.239IO00(X)90OO08O000700006O00O5O000400003O00O2C00010OOO0, , , , v~ece  "a I.dlw.pos" ?~o e?
$ ,t , ,~ J "O '~*?o * % ?
?
?
* ,~*  r ' *  *4' *~o ,~4!Pt  s?
- - ? '
? "
~ " ~.4R " ?
.
oe.
.5 , , ,=:~.
~-?
?
,?
".
,~" t .
?e  .20000 40000 600(\]0 80000 100000 120000100000 vI90ooo i-80000 k7o~oo 6OOO0 F500OO F ~?e ee~o3OOOO F1o000 F ?
. '
f ,0- -  ~ = i i0 10000 20000 30000 40000 50000d' ; v"finered.dtw,pos" e?
?,7.I t l I66000 70000 80000 90000 100000Figure 3: DTW path reconstruction output and the anchor points obtained after filteringThe constants in the above conditions are cho-sen roughly in proportion to the corpus size so thatthe filtered picture looks close to a clean, diagonalline.
This ensures that our development s age is stillunsupervised.
We would like to emphasize that ifthey were chosen by looking at the lexicon outputas would be in a supervised training scenario, thenone should evaluate the output on an independenttest corpus.Note that if one chunk of noisy data appeared intext1 but not in text2, this part would be segmentedbetween two anchor points (i, j)  and (u, v).
We knowpoint i is matched to point j ,  and point u to pointv, the texts between these two points are matchedbut we do not make any assumption about how thissegment of texts are matched.
In the extreme casewhere i -- u, we know that the text between j andv is noise.
We have at this point a segment-alignedparallel corpus with noise elimination.5 F ind ing  low f requency  b i l ingua lword  pa i rsMany nouns and proper nouns were not translated inthe previous tages of our algorithm.
They were notin the first lexicon because their frequencies were toolow to be well represented by positional differencevectors.5.1 Non- l inear  segment  b inary  vectorsIn stage 6, we represent the positional and frequencyinformation of low frequency words by a binary vec-tor for fast matching.The 388 anchor points (95,10), (139,131), .
.
.
,(98809, 93251) divide the two texts into 388 non-linear segments.
Textl  is segmented by the points(95,139, .
.
.
,  98586, 98809) and text2 is segmentedby the points (10,131, .
.
.
,  90957, 93251).For the nouns we are interested in finding thetranslations for, we again look at the positionvectors.
For example, the word prosperity oc-curred seven times in the English text.
Its posi-tion vector is (2178, 5322,.. .
,86521,95341) .
Weconvert this position vector into a binary vectorV1 of 388 dimensions where VI\[i\] = 1 if pros-perity occured within the ith segment, VI\[i\] --0 otherwise.
For prosperity, VI\[i\] -- 1 wherei = 20, 27, 41, 47,193,321,360.
The Chinese trans-lation for prosperity is ~!
.
Its posit ion vec-tor is (1955,5050,... ,88048).
Its binary vector isV2\[i\] = 1 where i = 14, 29, 41, 47,193,275,321,360.We can see that these two vectors hare five segmentsin common.We compute the segment vector for all Englishnouns and proper nouns not found in the first lex-icon and whose frequency is above two.
Words oc-curring only once are extremely hard to translatealthough our algorithm was able to find some pairswhich occurred only once.5.2 "B inary  vector  cor re la t ion  measureTo match these binary vectors V1 with their coun-terparts in Chinese V2, we use a mutual informationscore m.Pr(V1, V2)m = log2 Pr(Vl )  Pr(V2)freq(Vl\[i\] = 1) Pr(V1) --Lfreq(V2\[i\] = 1) Pr(V2) =Lfreq(Vl\[i\] -- V2\[i\] - 1) Pr(VI ,V2)  =Lwhere L = dim(V1) = dim(V2)240If prosperity and ~ occurred in the same eightsegments, their mutual information score would be5.6.
If they never occur in the same segments, theirm would be negative infinity.
Here, for prosperity/~~,  m = 5.077 which shows that these two words areindeed highly correlated.The t-score was used as a confidence measure.
Wekeep pairs of words if their t > 1.65 wheret ~ Pr(Y l ,  Y2) - Pr(V1) Pr(Y2)For prosperity/~.~\]~, t = 2.33 which shows thattheir correlation is reliable.6 Resu l tsThe English half of the corpus has 5760 unique wordscontaining 2779 nouns and proper nouns.
Mostof these words occurred only once.
We carriedout two sets of evaluations, first counting only thebest matched pairs, then counting top three Chinesetranslations for an English word.
The top N candi-date evaluation is useful because in a machine-aidedtranslation system, we could propose a list of up to,say, ten candidate translations to help the transla-tor.
We obtained the evaluations of three humanjudges (El-E3).
Evaluator E1 is a native Cantonesespeaker, E2 a Mandarin speaker, and E3 a speaker ofboth languages.
The results are shown in Figure 6.The average accuracy for all evaluators for bothsets is 73.1%.
This is a considerable improvementfrom our previous algorithm (Fung & McKeown1994) which found only 32 pairs of single word trans-lation.
Our program also runs much faster thanother lexicon-based alignment methods.We found that many of the mistaken transla-tions resulted from insufficient data suggesting thatwe should use a larger size corpus in our futurework.
Tagging errors also caused some translationmistakes.
English words with multiple senses alsotend to be wrongly translated at least in part (e.g.,means).
There is no difference between capital let-ters and small letters in Chinese, and no differencebetween singular and plural forms of the same term.This also led to some error in the vector represen-tation.
The evaluators' knowledge of the languageand familiarity with the domain also influenced theresults.Apart from single Word to single word transla-tion such as Governor /~ and prosperity/~i~fl?~,we also found many single word translations whichshow potential towards being translated as com-pound domain-specific terms such as follows:?
f ind ing Ch inese  words:  Chinese texts do nothave word boundaries uch as space in English,therefore our text was tokenized into words by astatistical Chinese tokenizer (Fung & Wu 1994).Tokenizer error caused some Chinese charactersto be not grouped together as one word.
Ourprogram located some of these words.
For ex-ample, Green was aligned to ,~j~,/~ and -~ whichsuggests that ,~ j~ could be a single Chineseword.
It indeed is the name for Green Paper -a government document.?
compound noun t rans la t ions :  carbon couldbe translated as \]i~, and monoxide as ~ .
Ifcarbon monoxide were translated separately, wewould get ~ --~K4h .
However, our algorithmfound both carbon and monoxide to be mostlikely translated to the single Chinese word - -~4h~ which is the correct translation for carbonmonoxide.The words Legislative and Council were bothmatched to ~-?r~ and similarly we can de-duce that Legislative Council is a compoundnoun/collocation.
The interesting fact here is,Council is also matched to ~J.
So we can deducethat ~-'r_~j should be a single Chinese word cor-responding to Legislative Council.?
s lang: Some word pairs seem unlikely to betranslations of each other, such as collusion andits first three candidates ~( i t  pull), ~t~(cat), F~(tail).
Actually pulling the cat's tail is Can-tonese slang for collusion.The word gweilo is not a conventional Englishword and cannot be found in any dictionarybut it appeared eleven times in the text.
Itwas matched to the Cantonese characters ~,  ~,~ ,  and ~ which separately mean vulgar/folk,name/litle, ghost and male.
~ meansthe colloquial term gweilo.
Gweilo in Cantoneseis actually an idiom referring to a male west-erner that originally had pejorative implica-tions.
This word reflects a certain cultural con-text and cannot be simply replaced by a wordto word translation.?
co l locat ions:  Some word pairs such as projectsand ~(houses)  are not direct translations.However, they are found to be constituentwords of collocations - the Housing Projects (bythe Hong Kong Government).Both Cross andHarbour are translated to 'd~Yff.
(sea bottom), andthen to Pi~:i(tunnel), not a very literal transla-tion.
Yet, the correct translation for ~ J -~ l l~is indeed the Cross Harbor Tunnel and not theSea Bottom Tunnel.The words Hong and Kong are both translatedinto ~i4~, indicating Hong Kong is a compoundname.Basic and Law are both matched to ~:~2~, sowe know the correct translation for ~2g~ isBasic Law which is a compound noun.?
p roper  names  In Hong Kong, there is aspecific system for the transliteration of Chi-nese family names into English.
Our algo-241lexiconsprimary(l)secondary(l)total(l)primary(3)secondary(3)total(3)total word pairs128533661128533661correct pairs accuracyE1 E2 E3 E1 E2 E3101 107 90 78.9% 83.6% 70.3%352 388 382 66.0% 72.8% 71.7%453 495 472 68.5% 74.9% 71.4%112 101 99 87.5% 78.9% 77.3%401 368 398 75.2% 69.0% 74.7%513 469 497 77.6% 71.0% 75.2%Figure 4: Bilingual exicon compilation resultsrithm found a handful of these such as Fung/~g,Wong/~, Poon/~, Hui/ iam/CY?, Tam/--~, etc.7 ConclusionOur algorithm bypasses the sentence alignment stepto find a bilingual lexicon of nouns and proper nouns.Its output shows promise for compilation of domain-specific, technical and regional compounds terms.
Ithas shown effectiveness in computing such a lexiconfrom texts with no sentence boundary informationand with noise; fine-grain sentence alignment is notnecessary for lexicon compilation as long as we havehighly reliable anchor points.
Compared to otherword alignment algorithms, it does not need a pri-ori information.
Since EM-based word alignmentalgorithms using random initialization can fall intolocal maxima, our output can also be used to pro-vide a better initializing basis for EM methods.
Ithas also shown promise for finding noun phrases inEnglish and Chinese, as well as finding new Chinesewords which were not tokenized by a Chinese wordtokenizer.
We are currently working on identifyingfull noun phrases and compound words from noisyparallel corpora with statistical and linguistic infor-mation.Re ferencesBROWN, P., J. LAI, L: R. MERCER.
1991.
Aligningsentences in parallel corpora.
In Proceedings ofthe 29th Annual Conference of the Associationfor Computational Linguistics.CHEN, STANLEY.
1993.
Aligning sentences in bilin-gual corpora using lexical information.
In Pro-ceedings of the 31st Annual Conference of theAssociation for Computational Linguistics, 9-16, Columbus, Ohio.CHURCH, K., I. DAGAN, W. GALE, P. FUNG,J.
HELFMAN, ~ B. SATISH.
1993.
Aligning par-allel texts: Do methods developed for English-French generalize to Asian languages?
In Pro-ceedings of Pacific Asia Conference on Formaland Computational Linguistics.CHURCH, KENNETH.
1993.
Char_align: A programfor aligning parallel texts at the character level.In Proceedings of the 31st Annual Conference ofthe Association for Computational Linguistics,1-8, Columbus, Ohio.DAGAN, IDO,  KENNETH W. CHURCH, ~:;WILLIAM A. GALE.
1993.
Robust bilingualword alignment for machine aided translation.In Proceedings of the Workshop on Very LargeCorpora: Academic and Industrial Perspectives,1-8, Columbus, Ohio.FUNG, PASCALE & KENNETH CHURCH.
1994.
Kvec:A new approach for aligning parallel texts.
InProceedings of COLING 94, 1096-1102, Kyoto,Japan.FUNG, PASCALE & KATHLEEN McKEOWN.
1994.Aligning noisy parallel corpora across languagegroups: Word pair feature matching by dy-namic time warping.
In Proceedings of theFirst Conference of the Association for MachineTranslation in the Americas, 81-88, Columbia,Maryland.FUNC, PASCALE & DEKAI WU.
1994.
Statisticalaugmentation of a Chinese machine-readabledictionary.
In Proceedings of the 2nd AnnualWorkshop on Very Large Corpora, 69-85, Ky-oto, Japan.GALE, WILLIAM A.
& KENNETH W. CHURCH.1993.
A program for aligning sentences inbilingual corpora.
Computational Linguistics,19(1):75-102.KAY, MARTIN ~; MARTIN ROSCHEISEN.
1993.
Text-Translation alignment.
Computational Linguis-tics, 19(1):121-142.KUMANO, AKIRA ~ HIDEKI HIRAKAWA.
1994.Building an mt dictionary from parallel textsbased on linguistic and statistical information.In Proceedings of the 15th International Con-ference on Computational Linguistics COLING94, 76-81, Kyoto, Japan.KUPIEC, JULIAN.
1993.
An algorithm for findingnoun phrase correspondences in bilingual cor-pora.
In Proceedings of the 31st Annual Confer-ence of the Association for Computational Lin-guistics, 17-22, Columbus, Ohio.SMADJA, FRANK & KATHLEEN McKEOWN.
1994.Translating collocations for use in bilingual lex-icons.
In Proceedings of the ARPA Human242Language Technology Workshop 94, Plainsboro,New Jersey.Wu, DEKAI.
1994.
Aligning a parallel English-Chinese corpus tatistically with lexical criteria.In Proceedings of the 32nd Annual Conferenceof the Association for Computational Linguis-tics, 80-87, Las Cruces, New Mexico.Wu, DEKAI L; XUANYIN XIh.
1994.
Learningan English-Chinese l xicon from a parallel cor-pus.
In Proceedings of the First Conference ofthe Association for Machine Translation in theAmericas, 206-213, Columbia, Maryland.243
