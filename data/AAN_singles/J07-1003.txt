Word-Level Confidence Estimation forMachine TranslationNicola Ueffing?RWTH Aachen UniversityHermann Ney?RWTH Aachen UniversityThis article introduces and evaluates several different word-level confidence measures for ma-chine translation.
These measures provide a method for labeling each word in an automaticallygenerated translation as correct or incorrect.
All approaches to confidence estimation presentedhere are based on word posterior probabilities.
Different concepts of word posterior probabilitiesas well as different ways of calculating them will be introduced and compared.
They can bedivided into two categories: System-based methods that explore knowledge provided by thetranslation system that generated the translations, and direct methods that are independentof the translation system.
The system-based techniques make use of system output, such asword graphs or N-best lists.
The word posterior probability is determined by summing theprobabilities of the sentences in the translation hypothesis space that contains the target word.The direct confidence measures take other knowledge sources, such as word or phrase lexica,into account.
They can be applied to output from nonstatistical machine translation systemsas well.Experimental assessment of the different confidence measures on various translation tasksand in several language pairs will be presented.
Moreover, the application of confidence measuresfor rescoring of translation hypotheses will be investigated.1.
IntroductionThe work presented in this article deals with confidence estimation for machine trans-lation (MT).
Because sentences generated by a machine translation system are oftenincorrect but may contain correct substrings, a method for identifying these correctsubstrings and finding possible errors is desirable.
For this purpose, each word inthe generated target sentence is assigned a value expressing the confidence that itis correct.Confidence measures have been extensively studied for speech recognition.
Onlyrecently have researchers started to investigate confidence measures for machine trans-lation (Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003; Blatz et al2004; Quirk 2004).
In this article, we will develop a sound theoretical framework for?
Now at National Research Council Canada, Interactive Language Technologies Group, Gatineau, Que?becJ8P 3G5, Canada.
E-mail: nicola.ueffing@nrc.gc.ca.?
Lehrstuhl fu?r Informatik VI, Computer Science Department, D-52056 Aachen, Germany.
E-mail:ney@cs.rwth-aachen.de.Submission received: 7 March 2006; revised submission received: 30 September 2006; accepted for publication:3 October 2006.?
2007 Association for Computational LinguisticsComputational Linguistics Volume 33, Number 1calculating and evaluating word confidence measures.
Possible applications of confi-dence measures include: marking words with low confidence as potential errors for post-editing improving translation prediction accuracy in TransType-style interactivemachine translation (Gandrabur and Foster 2003; Ueffing and Ney 2005a) combining output from different machine translation systems: Hypotheseswith low confidence can be discarded before selecting one of the systemtranslations (Akiba et al 2004), or the word confidence scores can be usedin the generation of new hypotheses from the output of different systems(Jayaraman and Lavie 2005), or the sentence confidence value can beemployed for reranking (Blatz et al 2003).The article is organized as follows: In Section 2, we briefly review the statisticalapproach to machine translation.
The phrase-based translation system, which serves asthe basis for one of the direct confidence measures, will be presented.
Section 3 gives anoverview of related work on confidence estimation for machine translation.
Moreover,word posterior probabilities will be introduced, and we will explain how they can beused as word-level confidence measures.
In Section 4, we describe so-called system-based methods for confidence estimation, which make use of the output of a statisticalmachine translation system, such as word graphs or N-best lists.
In Section 5, we presentconfidence measures based on direct models.
The combination of several confidencemeasures into one is described in Section 6.
Experimental evaluation and comparisonof the different confidence measures is provided in Section 7.
Section 8 deals with therescoring of translation hypotheses using confidence measures.
The article concludes inSection 9.2.
Statistical Machine Translation2.1 GeneralIn statistical machine translation (SMT), the translation is modeled as a decision process:Given a source string f J1 = f1 .
.
.
fj .
.
.
fJ, we seek the target string eI1 = e1 .
.
.
ei .
.
.
eI withmaximal posterior probability:e?I?1 = argmaxI,eI1{Pr(eI1 | fJ1 )}= argmaxI,eI1{Pr( f J1 | eI1) ?
Pr(eI1)}(1)Through this decomposition of the probability, we obtain two knowledge sources:the translation model Pr( f J1 | eI1) and the language model Pr(eI1).
Both can be mod-eled independently of each other.
The translation model is responsible for linking thesource string f J1 and the target string eI1.
It captures the semantics of the sentence.The target language model captures the well-formedness of the syntax in the targetlanguage.Nowadays, most state-of-the-art SMT systems are based on bilingual phrases (Och,Tillmann, and Ney 1999; Koehn, Och, and Marcu 2003; Tillmann 2003; Bertoldi et al2004; Vogel et al 2004; Zens and Ney 2004; Chiang 2005).
A more detailed description of10Ueffing and Ney Word-Level Confidence Estimation for MTa phrase-based approach to statistical machine translation will be given in the followingsection.2.2 Review of the Phrase-Based Translation SystemFor the confidence measures which will be introduced in Section 5.1, we use a state-of-the-art phrase-based translation approach as described in Zens and Ney (2004).The key elements of this translation approach are bilingual phrases.
Note that thesephrases are sequences of words in the two languages and not necessarily phrases inthe linguistic sense.
The bilingual phrases are extracted from a word-aligned bilingualtraining corpus.In this translation approach, the posterior probability Pr(eI1 | fJ1 ) is modeled directlyusing a weighted log-linear combination of a language model, a phrase translationmodel, and a word-based lexicon model.
The translation models are used for bothdirections: p( f | e) and p(e | f ).
Additionally, a word penalty and a phrase penalty areapplied.
With the exception of the language model, all models can be considered aswithin-phrase models as they depend only on a single phrase pair, but not on the contextoutside the phrase.In the following, we will present the generation criterion for the phrase-based trans-lation approach.
This will be done for a monotone search in order to keep the equationssimple.
The extension to the non-monotone case is straightforward.
Let ( jK0 , iK0 ) be asegmentation of the source sentence into phrases, where jk?1 < jk and ik?1 < ik fork = 1, .
.
.
, K. The corresponding (bilingual) phrase pairs are denoted as( f?k, e?k) = ( fjkjk?1+1, eikik?1+1), k = 1, .
.
.
, KAssume a trigram language model.
The phrase-based approach to SMT is then ex-pressed by the following equation:e?I?1 = argmaxjK0 ,iK0 ,I,eI1{ I?i=1[c1 ?
p(ei | ei?1i?2)?1]?K?k=1[c2 ?
p( f?k | e?k)?2 ?
p(e?k | f?k)?3 ?
(2)?jk?j=jk?1+1p( fj | e?k)?4 ?ik?i=ik?1+1p(ei | f?k)?5]}where p( f?k | e?k) and p(e?k | f?k) are the phrase lexicon models in both translation directions.The phrase translation probabilities are computed as a log-linear interpolation of therelative frequencies and the IBM model 1 probability.
The single word?based lexiconmodels are denoted as p( fj | e?k) and p(ei | f?k), respectively.
p( fj | e?k) is defined as theIBM model 1 probability of fj over the whole phrase e?k, and p(ei | f?k) is the inversemodel, respectively.
c1 is the so-called word penalty, and c2 is the phrase penalty,assigning constant costs to each target language word/phrase.
The language modelis a trigram model with modified Kneser?Ney discounting and interpolation (Stolcke2002).
The search determines the target sentence and segmentation that maximize theobjective function.11Computational Linguistics Volume 33, Number 1As Equation (2) shows, the sub-models are combined via weighted log-linear in-terpolation.
The model scaling factors ?1, .
.
.
, ?5 and the word and phrase penaltiesare optimized with respect to some evaluation criterion (Och 2003) such as BLEUscore.The phrase-based translation model will be needed later to define the differentconfidence measures.
We therefore introduce the following notation: Let QPM( f?k, e?k) bethe score of the phrase pair, which consists of the phrase penalty c2, the phrase lexiconscores, and the two word lexicon model scores (see Equation (2)):QPM( f?k, e?k) := c2 ?
p( f?k | e?k)?2 ?
p(e?k | f?k)?3 ?jk?j=jk?1+1p( fj | e?k)?4 ?ik?i=ik?1+1p(ei | f?k)?5 (3)3.
Confidence Measures for MT3.1 Related WorkIn many areas of natural language processing, confidence measures have scarcelybeen investigated.
The exception is automatic speech recognition, where an exten-sive amount of research on the topic exists.
Confidence measures are widely used inthis area?for example, in dialogue systems and in unsupervised training.
Recently,researchers have started to investigate confidence measures for machine translation(Blatz et al 2003, 2004; Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003;Quirk 2004; Sanchis 2004).
This section gives an overview of confidence estimation formachine translation on the word level as well as the sentence level and discusses itsapplications.The first work that studied confidence estimation for statistical machine transla-tion was Gandrabur and Foster (2003).
Their confidence measures consist of a com-bination of different features in a neural network.
The confidence is estimated for asequence of up to four words in an interactive machine translation environment.
Theprobability of being a correct extension of a given sentence prefix is computed for thisword sequence.
The authors report significant improvement in quality of the predictedtranslations.In 2003, a team at the yearly summer workshop at the Center for Language andSpeech Processing (CLSP) at Johns Hopkins University, Baltimore, MD, developedconfidence measures for machine translation.
The combination of several confidencefeatures using neural networks and a naive Bayes classifier was investigated.
Theworkshop team studied confidence estimation on the word level as well as on the sen-tence level, though the focus was on the sentence level.
The features applied includednew features as well as those that had previously been developed by team members(Gandrabur and Foster 2003; Ueffing, Macherey, and Ney 2003).
Among them were alsosome of the word posterior probabilities, which will be presented here.
Additionally,heuristic and semantic features were studied.
For a description of the features andresults, see Blatz et al (2003, 2004).Following the work of the summer workshop team, Quirk (2004) presented aninvestigation of different approaches to sentence-level confidence estimation.
A set offeatures is computed for each sentence generated by an MT system, and these featuresare combined using several different methods: modified linear regression, neural nets,support vector machines, and decision trees.
Many of the sentence features are similar to12Ueffing and Ney Word-Level Confidence Estimation for MTthose presented in Blatz et al (2003); the others are specific to the underlying MT systemthat generated the translations.
Quirk (2004) also investigated the use of manuallytagged data for training the confidence measures.
The author found that using a smallamount of manually labeled training data yields better performance than using largequantities of automatically labeled data.Akiba et al (2004) reported the application of confidence measures to the selectionof output on N-best lists produced by different MT systems.
Word-level confidence mea-sures, namely the rank-weighted sum as described in Section 4.1 (and first introducedin Ueffing, Macherey and Ney [2003]), are used to discard low-quality system outputbefore selecting a translation from the various MT systems.Zens and Ney (2006) presented an extension of the word posterior probabili-ties presented in this article: Posterior probabilities are calculated not only on theword level, but also for n-grams, and are successfully applied to the rescoring of MThypotheses.3.2 Word Posterior ProbabilitiesThe confidence of a target word can be expressed by its posterior probability, that is, theprobability of the word occurring in the target sentence, given the source sentence.
Wordposterior probabilities are the basis of all approaches to confidence estimation presentedhere.
The following explains how they can be determined.
The different methods can beclassified into two categories: system-based methods, which make use of system outputsuch as word graphs or N-best lists; and direct methods, which use external knowledgesources such as statistical word or phrase lexica.The system-based approaches derive the word posterior probability from the sen-tence posterior.
The posterior probability of a sentence eI1 can be approximated by thejoint probability p( f J1, eI1), which the statistical machine translation system assigns toa generated translation.
The sentence probabilities employed in the search (see Equa-tion (1)) are not normalized, which does not affect the result of the search.
But for use inconfidence estimation, they need to be normalized in order to obtain a probability distri-bution over all target sentences (see Equation (6)).
From the sentence posterior probabil-ities, the word posterior probabilities can be calculated by summing up the probabilitiesof all sentences containing the target word.
For an exact quantification of word posteriorprobabilities, we need to consider the following problem: How can we define a criterionfor the occurrence of a word in a sentence?
The answer to this question is not at all triv-ial.
Due to ambiguities, the word position in the sentence is not fixed.
Sentences can havedifferent numbers of words because of deletions and insertions.
Additionally, the wordscan be reordered in different ways during the translation process.
The posterior proba-bility of a target word e can depend on its occurrence in position i of the target sentence,for example, or on the number of times the word is contained in the sentence.
Thus, sev-eral different definitions of posterior probabilities will be introduced and investigated inthe following discussion.
The basic concept of calculating the posterior probability willbe explained for the target word e occurring in a fixed position i of the sentence.
This isa rather strict and simple criterion; it will be used here mainly to illustrate the idea.
Sec-tion 4 will describe several different concepts of word posterior probabilities that relaxthis condition.Let p( f J1, eI1) be the joint probability of source sentence fJ1 and target sentence eI1.Here, this is approximated by the probability that an SMT system assigns to thesentence pair (see Section 2).
The word posterior probability of e occurring in position13Computational Linguistics Volume 33, Number 1i is calculated as the normalized sum of probabilities of all sentences containing e inexactly this position:pi(e | f J1 ) =pi(e, fJ1 )?e?pi(e?, fJ1 )(4)where pi(e, fJ1 ) =?I,eI1?
(ei, e) ?
p( f J1, eI1) (5)Here ?
(?, ?)
is the Kronecker function.
The normalization term in Equation (4) is?e?pi(e?, fJ1 ) =?I,eI1p( f J1, eI1) = p( fJ1 ) (6)This definition of word posterior probabilities raises the question of how to calculate thesums over the target sentences in Equations (5) and (6).
This problem can be solved byapproximating the summation space via a word graph or an N-best list.
The summationis then performed explicitly over all sentences given in this restricted space.
In the caseof an N-best list, this is straightforward because the sentences are already listed.
On aword graph, the forward?backward algorithm can be applied to carry out the summa-tion efficiently.
In these system-based approaches, the calculation depends on the outputof the SMT system that generated the translations.
The sentence probabilities summedin Equations (5) and (6) are the scores assigned by the underlying SMT system.
Thesummation space is restricted to those hypotheses that are assigned a high probabilityby the SMT system, and the others are not considered.The second approach to the calculation of word posterior probabilities is summa-tion using direct models such as IBM model 1 or a phrase-based translation model.These methods do not consider the whole target sentence.
The summation of prob-abilities is carried out over single words or phrases without context.
These model-based word posterior probabilities are independent of the system generating thetranslations.
They do not require the MT system to assign a probability to the translationhypothesis.
Thus, they can also be used for confidence estimation on hypotheses froma non-statistical MT system or if only the single best translations without any scoresare given.3.3 Word Confidence MeasuresThe idea behind word-level confidence estimation is to be able to detect possible errorsin the output of a machine translation system.
Using confidence measures, individ-ual words can be labeled as either correct or incorrect.
This additional informationcan be used in, for example, interactive TransType-style machine translation systems(Gandrabur and Foster 2003; Ueffing and Ney 2005a).Two problems have to be solved in order to compute confidence measures.
First,suitable confidence features have to be computed.
Second, a binary classifier has to bedefined, which decides whether a word is correct or not.
The word posterior proba-bilities introduced in Section 3.2 can be interpreted as the probability of a word beingcorrect.
That is, the probability can directly be used as a confidence measure.
For this14Ueffing and Ney Word-Level Confidence Estimation for MTpurpose, it is compared to a threshold t. All words that have confidence above thisthreshold are tagged as correct, and all others are tagged as incorrect, translations.
Thus,the binary classifier is defined asclass(e) =??
?correct if p(e | f J1 ) ?
tincorrect otherwise(7)The threshold t is optimized on a distinct development set beforehand.The question of how the correctness of a word in MT output is determined is not atall an easy one.
We will address this issue in Section 7.2.4.
System-Based Confidence MeasuresIn this section, we will present confidence measures that are calculated over N-bestlists or word graphs generated by an SMT system.
Several different models for theoccurrence of a target word in a sentence will be defined and experimentally evaluated.These are the models that proved most promising from a theoretical viewpoint and inthe experimental evaluation: Target word e occurs in position i of the target sentence (see Section 4.1).The calculation of word posterior probabilities over word graphs andN-best lists is explained in detail for this concept. The word is considered if it occurs in a window around the position:i ?
t, t ?
N, for some position i (see Section 4.2). The Levenshtein alignments between the hypothesis under considerationand all other possible translations are determined.
The target word e (insome position i) is taken into account if it is Levenshtein-aligned to itself(see Section 4.3). e is contained in the sentence at least n times, n ?
N (see Section 4.4).Section 4.5 will treat the issue of scaling the probabilities that the SMT system assignsto the translation hypothesis.4.1 Approach Based on the Fixed Target PositionIn this approach, the word posterior probability is determined for word e occurring intarget position i as shown in Equation (4).
This variant requires the word to occur exactlyin the given position i.
Hence, a probability distribution over the pairs (e, i) of targetwords e and positions i in the target sentence is obtained.
This type of word posteriorprobability was first introduced in Ueffing, Macherey, and Ney (2003).The concept of word posterior probabilities based on the fixed target position allowsfor easy calculation over word graphs and N-best lists.
However, this concept is ratherrestrictive.
In practice, the target position of a word varies between different translationalternatives.
The method presented here is a starting point for more flexible approachesthat perform summation over a window of target positions.In the following, we will show how the word posterior probabilities based on fixedtarget positions are calculated over word graphs and over N-best lists.15Computational Linguistics Volume 33, Number 1Calculation using word graphs.
A word graph represents the most promising hypothesesgenerated by the translation system (Ueffing, Och, and Ney 2002; Zens and Ney 2005).It has the advantage of being a compact representation of the translation hypothesisspace, which allows for efficient calculation of word posterior probabilities.
A wordgraph is a directed acyclic graph G = (V, E) with vertex set V and edge set E. It hasone designated root node n0 ?
V, representing the beginning of the sentence.
Eachpath through the word graph represents a translation candidate.
The nodes of thegraph contain information such as the set of covered source positions and the languagemodel history.
Two hypotheses can be recombined if their information is identical.Recombination is carried out during decoding to accelerate the search process.
If twohypotheses represent the same information with respect to translation and languagemodels, they will be assigned the same probabilities by these models in the future.Therefore, the outcome of the search is not altered, but the processing time can besignificantly decreased if only the more promising of the two hypotheses is consideredfor further expansion.
If no recombination were carried out, the word graph would havethe structure of a tree.The edges in the word graph are annotated with target words.
Additionally, theycontain weights representing the part of the probability that is assigned to each particu-lar word as part of the target hypothesis.
When multiplying the scores along a path, theprobability of the corresponding hypothesis is obtained.
The sentence position of a wordrefers to the path length in the word graph: Consider an edge (n, n?)
that is annotatedwith word e. If a path leading from source node n0 into n has i edges, then e willbe the (i + 1)-th word in the corresponding sentence.
Note that due to recombinationthis position is not unambiguous.
If two hypotheses of different lengths i and i?
arerecombined in node n, then e will be in position i + 1 in the one resulting sentence, andin i?
+ 1 in the other sentence.For an example of a word graph, see Figure 1.
The source sentence is Wir ko?nnendas machen!, and the reference translation is We can do that!.
The leftmost nodeis the root node n0.
The other nodes represent different states with respect to the setof covered source positions and language model history.
In this example, a trigramFigure 1Example of forward?backward calculation on a word graph.
The posterior probability of canin the second position is obtained by multiplying the total probability of all incoming paths(dashed lines) and outgoing paths (dotted), separately for the two edges, and summing theproducts.16Ueffing and Ney Word-Level Confidence Estimation for MTlanguage model is applied, that is, all paths leading into a node share the last two words.The translation alternatives contained in this word graph represent different reorderingsof the words in the sentence: The monotone translation that do as well as the correctlyreordered sequence do that occur.
Note that in order to limit the size of the graph andkeep the presentation simple, an example was chosen where all target sentences havethe same length.The posterior probabilities of word e in position i can be computed by summing upthe probabilities of all paths in the graph that contain an edge annotated with e in posi-tion i of the target sentence.
This summation is performed efficiently using the forward?backward algorithm (Jurafsky and Martin 2000).
This algorithm also determines thetotal probability mass that is needed for normalization, as shown in Equation (6).
Inthe following, we will present the exact equations for a word graph generated by thephrase-based translation system described in Section 2.2.
In such a word graph, thefirst word of a target phrase is assigned the score for the whole phrase.
That is, whentranslating a source phrase f?k by a target phrase e?k = eik?1+1 .
.
.
eik , the full contributionof all sub-models for this phrase is included for the first word eik?1+1.
All followingwords eik?1+2 .
.
.
eik are assigned probability 1.The forward?backward algorithm works as follows: Let QPM( f?k, e?k) be the phrasemodel score of a phrase pair as defined in Equation (3) in Section 2.2.
In order to keepthe notation simple, we assume a bigram language model.
The extension to higher-orderlanguage models is straightforward.
The forward probability ?i(ei ; e?k, f?k) of word ei isthe probability of reaching word ei from the sentence start, where ei occurs in positioni of the sentence.
It depends on the phrase pair ( f?k, e?k) for which ei ?
e?k.
Because thefull score of this phrase pair is included at the first word eik?1+1, two cases have tobe distinguished in the calculation: Either ei is the first word in the phrase, that is,i = ik?1 + 1, or ik?1 + 1 < i ?
ik.
The forward probability can be determined by sum-ming the probabilities of all partial hypotheses of length i ?
1.
This allows for recursivecalculation in ascending order of i.
We obtain the following formula:?i(ei ; e?k, f?k) ==????????????
?QPM( f?k, e?k) ?ik?i?=i+1c1 ?
p(ei?
| ei?
?1)?1 ?
?e?k?1p(ei | eik?1 )?1?
?f?k?1?i?1(eik?1 ; e?k?1, f?k?1) if i = ik?1 + 1?i?1(ei?1 ; e?k, f?k) if ik?1 + 1 < i ?
ikThe backward probability ?i(ei ; e?k, f?k) expresses the probability of completing a sen-tence from the current word on.
It can be determined recursively in descending orderof i.
Again, we distinguish two cases:?i(ei ; e?k, f?k) ==?????????????
?e?k+1ik+1?i?=ic1 ?
p(ei?
| ei?
?1)?1 ?
?f?k+1QPM( f?k+1, e?k+1) ?
?i+1(ei+1 ; e?k+1, f?k+1)if i = ik?i+1(ei+1 ; e?k, f?k) if ik?1 < i < ik17Computational Linguistics Volume 33, Number 1Using the forward?backward algorithm, the word posterior probability of word e inposition i is determined by combining the forward and backward probabilities of allhypotheses containing e in this position.
We carry out a summation over all correspond-ing phrase pairs ( f?k, e?k).
This yieldspi(e, fJ1 ) =?e?k?f?k?i(e ; e?k, f?k) ?
?i(e ; e?k, f?k) (8)To obtain a posterior probability, a normalization (as shown in Equation (4)) has tobe performed.
The normalization term p( f J1 ) :=?e?pi(e?, fJ1 ) corresponds to the probabilitymass contained in the word graph and can be calculated by summing the backwardprobabilities of all words that occur in the first sentence position:p( f J1 ) =?e?1=e1...ei1?f?1?1(e1; e?1, f?1)Figure 1 illustrates the forward?backward algorithm.
Assume the word posteriorprobability of the word can appearing in the second position of the target sentence isto be calculated.
There are two edges in the graph that contain this word in the desiredtarget position.
Thus, the probabilities of the paths leading through these edges haveto be summed.
The forward probabilities are the probabilities of the incoming edges,shown by dashed lines.
The backward probabilities are those of the paths marked bydotted lines.
They are combined (separately for each edge) and then summed to obtainthe word posterior probability of can in position 2.Calculation using N-best lists.
An N-best list contains the n most promising translationhypotheses generated by the statistical machine translation system.
The N-best listis extracted from a word graph.
The hypotheses are sorted by their probability indescending order.
This representation allows for easy computation of the sum given inEquation (5).
Furthermore, the calculation of more complex variants of word posteriorprobabilities, such as the approach based on Levenshtein alignment (see Section 4.3),is feasible.Let en,Inn,1 , n = 1, .
.
.
, N, be the target hypotheses in the N-best list.
The word pos-terior probabilities presented in Equation (4) are calculated by summing the sentenceprobabilities of all sentences containing target word e in target position i.
The sentenceprobability p( f J1, en,Inn,1 ) is given in the N-best list.
The word posterior probability is thendetermined aspi(e | f J1 ) =N?n=1?
(en,i, e) ?
p( f J1, en,Inn,1 )?e?N?n=1?
(en,i, e?)
?
p( f J1, en,Inn,1 )The normalization term in the denominator equals the probability mass contained inthe N-best list.18Ueffing and Ney Word-Level Confidence Estimation for MTInstead of the sum of probabilities, one can also determine the relative frequency orthe rank-weighted frequency of a word as follows: The relative frequency of e occurringin target position i in the N-best list is computed ashi(e | f J1 ) := 1NN?n=1?
(en,i, e) (9)The rank-weighted frequency is determined asri(e | f J1 ) := 2N(N + 1)N?n=1?
(en,i, e) ?
(N + 1 ?
n) (10)Here, the inverted ranks N + 1 ?
n are summed up because an occurrence of the wordin a hypothesis near the top of the list will score better than one in the lower ranks.This value is normalized by the sum of all ranks in the list.
Note that the values inEquations (9) and (10) could also be calculated over N-best lists that do not contain thesentence probability.4.2 Approach Based on a Window over Target PositionsOne way of accounting for slight variations in the target position i of word e is the intro-duction of a window i ?
t, t ?
N, around position i.
The word confidence is determinedas the sum of the word posterior probabilities calculated for the positions within thiswindow.
This leads topi,t(e | f J1 ) =i+t?k=i?tpk(e | f J1 ) (11)The window can easily be integrated both into the N-best list and the word graph?basedimplementation: The target position-dependent word posterior probabilities are calcu-lated as stated in Equation (4), and the summation over the positions in the window isperformed in an additional step.4.3 Approach Based on the Levenshtein AlignmentAnother way of accounting for variations in the target position of a word is to performthe Levenshtein alignment (Levenshtein 1966) between sentence eI1 under considerationand the other possible target sentences.
The summation in Equation (5) is then per-formed over all sentences containing e in a position Levenshtein-aligned to i (Ueffing,Macherey, and Ney 2003).The implementation of this summation over N-best lists is straightforward: TheLevenshtein alignment is performed between the hypothesis eI1 and every sentenceen,Inn,1 contained in the N-best list individually, and then the summation is carried out.For word graphs, no efficient way of determining the Levenshtein alignments andthe resulting word posterior probabilities is known.19Computational Linguistics Volume 33, Number 1Let L(eI1, en,Inn,1 ) be the Levenshtein alignment between sentences eI1 and en,Inn,1 , andLi(eI1, en,Inn,1 ) that of word e in position i in eI1.
Consider the following example: Cal-culating the Levenshtein alignment between the sentences eI1 =?A B C D E?
anden,Inn,1 =?B C G E F?
yieldsL(eI1, en,Inn,1 ) = ?
?
B C G E?
?Using this representation, the word posterior probability of word e occurring in aposition Levenshtein-aligned to i is given byplev(e | f J1, eI1,L) =plev(e, fJ1, eI1,L)?e?plev(e?, fJ1, eI1,L)(12)where plev(e, fJ1, eI1,L) =N?n=1?
(e,Li(eI1, en,Inn,1 )) ?
p( fJ1, en,Inn,1 )The probability depends on all target words in the hypothesis eI1 under consideration,because the Levenshtein alignment of the whole sentence, L(eI1, en,Inn,1 ), is determined.This concept of word posterior probabilities is inspired by the error measure WordError Rate (WER).
It can be shown that the word posterior probabilities form a partof the Bayes risk for WER: Formulating the loss function and deriving the risk yields aminimization criterion consisting of the word posterior probabilities defined previously,one term representing the sentence length, and one for the deletion operations in theLevenshtein alignment.
For more details, see Ueffing and Ney (2004) and Ueffing (2006).4.4 Count-Based ApproachInspired by Bayes risk for Position-independent Word Error Rate (PER), the wordposterior probability can be defined by taking the counts of the words in the generatedsentence into account (Ueffing and Ney 2004).
The probability of target word e occurringin the sentence n times is determined aspe(n| f J1 ) =pe(n, fJ1 )nmax?n?=0pe(n?
, f J1 )(13)where pe(n, fJ1 ) =?I,eI1?
(ne, n) ?
p( f J1, eI1)Here, ne is the count of word e in sentence eI1, and nmax is the maximal count that isobserved.
This term does not depend on the actual word sequence, but only on thecounts of the target words.
Let nE1 be the counts of all target words 1, .
.
.
, E in sentenceeI1.
Analogously, n?E1 denotes the count sequence for sentence e?I?1.
In practice, many ofthese counts will be zero, of course.
The posterior probabilities can then be expressedby the distribution over the count sequences:pe(n, fJ1 ) =?nE1?
(ne, n) ?
p(nE1 , fJ1 )20Ueffing and Ney Word-Level Confidence Estimation for MTwhere the distribution over the count sequences is determined by summing up theprobabilities of all sentences with these counts:p(nE1 , fJ1 ) =?I?,e?I?1?
(n?E1 , nE1 ) ?
p( fJ1, eI1)Using this concept, the target position of the word is not taken into account, but the firstoccurrence of a word in the sentence will obtain a word posterior probability differentfrom that of the second occurrence.In Ueffing (2006), it is shown that the posterior risk for PER comprises one termrelated to the count-based word posterior probabilities defined here and one termrelated to the posterior probability of the sentence length.
We can thus expect the count-based word posterior probabilities to perform especially well if the word correctnessis defined on the basis of PER.
The experimental results presented in Section 7.4 willconfirm this assumption.The summation in Equation (13) can be performed over N-best lists (analogouslyto the word posterior probability variants described so far), but it cannot efficientlybe determined over the word graph.
The problem is that the number of occurrencesof a word on the whole path is needed.
Because the word graph stores only localinformation, this count cannot be determined efficiently.
The normalization term inEquation (13) corresponds to the total probability mass contained in the N-best list,because the case n?
= 0 is also included.4.5 Scaling the ProbabilitiesDuring the translation process, the different sub-models (such as the language modeland the lexicon model) are weighted differently.
These weights or scaling factors canbe optimized with respect to some evaluation criterion (Och 2003).
Nevertheless, thisoptimization determines only the relation between the different models, and not theabsolute values of the scaling factors.
The absolute values are not needed for thetranslation process, because the search is performed using the maximum approximation(see Equations (1) and (2)).
In contrast to this, the actual values of the weights make adifference for confidence estimation, because the summation over the sentence proba-bilities is performed.
To account for this and to find the optimal values of the scalingfactors, a global weight ?
is introduced, which scales the sentence probability.
Theword posterior probability based on the fixed position i, for example, is then calculatedaccording topi(e | f J1 ) =?I,eI1?
(ei, e) ?
p?
( f J1, eI1)?e??I,eI1?
(ei, e?)
?
p?
( f J1, eI1)(14)When determining the system-based word posterior probabilities, this scaling factor isoptimized with respect to some metric for confidence estimation on a development setdistinct from the test set.21Computational Linguistics Volume 33, Number 15.
Confidence Measures Based on Direct ModelsIn the following, confidence measures based on direct models will be described.
Theseapproaches model the word posterior probability directly instead of summing the prob-abilities of sentences containing the target word.
Confidence measures based on IBMmodel 1 and phrase-based translation models were developed and will be presentedhere.
They make use of knowledge sources such as statistical word or phrase lexica forestimating the word confidence.
Unlike the system-based word posterior probabilitiespresented so far, these confidence measures are completely independent of the targetsentence position in which the word e occurs.
They determine the confidence of e beingcontained anywhere in the sentence.5.1 Direct Approach to Confidence Estimation Using PhrasesThe statistical models presented in Section 2.2 can be used to estimate the confidence oftarget words as first described in Ueffing and Ney (2005b).
In contrast to the approachespresented in Section 4, the direct phrase-based confidence measures do not use thecontext information at the sentence level, but only at the phrase level.For a given source sentence f J1 and a target word e, we want to determine a sortof marginal probability Q(e, f J1 ).
Therefore, we extract all source phrases fj+sj that occurin the given source sentence f J1.
For these source phrases, we find the possible transla-tions ei+ti in the bilingual phrase lexicon.
The confidence of target word e is then calcu-lated by summing over all phrase pairs ( f j+sj , ei+ti ) where the target part ei+ti contains e.Let QPM( f?k, e?k) be the phrase model score of a phrase pair as defined in Equation (3)in Section 2.2.
Analogously, we define QLM(ei+ti ) as the language model score of thetarget phrase together with the word penalty c1 for each word in the phrase, that is,QLM(ei+ti ) :=i+t?i?=ic1 ?
p(ei?
| ei??1i?
?2)?1 (15)Note that this is the within-phrase language model probability, which does not includethe context of the phrase.
The language model probability at the phrase boundary isapproximated by a unigram and bigram.The (unnormalized) confidence of target word e is then determined by summingthe product of the language model and the phrase model score of all phrase pairscontaining e:Q(e, f J1 ) :=J?j=1min{smax,J?j}?s=0?ei+ti?
(e , ei+ti ) ?
QLM(ei+ti ) ?
QPM( fj+sj , ei+ti ) (16)where s ?
smax and t are source and target phrase lengths, smax being the maximal sourcephrase length.
?
(e, ei+ti ) denotes an extension of the Kronecker delta:?
(a, A) ={1 if a ?
A0 otherwise22Ueffing and Ney Word-Level Confidence Estimation for MTThe value calculated in Equation (16) is not normalized.
In order to obtain a probability,this value is divided by the sum over the (unnormalized) confidence values of all targetwords:pphr(e | f J1 ) =Q(e, f J1 )?e?Q(e?, f J1 )(17)As shown in Equations (3) and (15), the different sub-models of the phrase-basedtranslation approach are combined in a log-linear manner.
The weights ?1, .
.
.
, ?5 andthe penalties c1, c2 are optimized in the translation process with respect to some eval-uation criterion such as WER or BLEU.
This is done using the Downhill Simplexalgorithm (Press et al 2002).
The resulting values of the weights express the relationbetween the sub-models, but not their absolute values.
They are usually normalized sothat they sum to 1.
For use in confidence estimation, two different aspects thus have tobe considered: The relation of the sub-models that is optimal for translation qualityis not necessarily optimal for classification performance.
Therefore, thesub-model scaling factors are optimized with respect to some confidenceevaluation measure (see Section 7.3).
The direct phrase-based confidencemeasures provide a framework for optimizing the sub-model weightsefficiently.
The optimization is performed analogously to the procedurefor machine translation: The confidence values are determined for allwords in the development corpus.
Then, classification is carried out asdescribed in Section 3.3, and the result is evaluated.
The weights arethen modified and the confidence estimation is repeated, until optimalclassification performance on the development set is achieved.
Again,the Downhill Simplex algorithm is used for optimization. For MT, only the relation between the different sub-models, but not theactual values of the scaling factors, are important.
Confidence measures,however, also depend on these actual values.
In MT, the sub-model scalingfactors are normalized such that they sum to 1.
For the use in confidenceestimation, the value of this sum,?
:=5?i=1?i + c1 + c2is also optimized.
This ?
is analogous to the global scaling factor for thesystem-based confidence measures introduced in Section 4.5.5.2 Confidence Measure Based on IBM Model 1Another type of confidence measure that does not rely on system output and is thusapplicable to any kind of machine translation system is the IBM model 1?based confi-dence measure that was introduced in Blatz et al (2003).
We modified this confidencemeasure because we found that the average lexicon probability used there is dominated23Computational Linguistics Volume 33, Number 1by the maximum.
Therefore, we determine the maximal translation probability of thetarget word e over the source sentence words:pibm1(e| f J1 ) = maxj=0,...,J p(e| fj) (18)where f0 is the ?empty?
source word (Brown et al 1993).
The probabilities p(e| fj) areword-based lexicon probabilities.Investigations of the use of the IBM model 1 for word-level confidence estimationshowed promising results (Blatz et al 2003, 2004).
Thus, we apply this method hereand compare it to the other types of confidence measures.
Ueffing and Ney (2005a)report on the use of this IBM model 1?based confidence measure in a TransType-styleinteractive MT system.
The work presented there shows that even this relatively simpleconfidence measure yields a significant gain in the quality of the predictions proposedby the interactive system.6.
Combination of Confidence MeasuresIn related work in MT as well as in speech recognition, the combination of numerousconfidence features has been suggested (Gandrabur and Foster 2003; Blatz et al 2004;Quirk 2004; Sanchis 2004).
Among the methods used for combination are multi-layerartificial neural networks, naive Bayes classifiers, and modified linear regression.Because the combination of several confidence measures proved successful, thedifferent word posterior probabilities proposed here were combined with each other.The combination was performed in a log-linear manner.
Let pm(e | f J1, .
.
.)
, m = 1, .
.
.
, M,be the word posterior probabilities of e determined using different approaches.
Theword confidence resulting from their combination is calculated asc(e) = exp{?M?m=1?m ?
log pm(e | f J1, .
.
.
)}The interpolation weights ?m are optimized with respect to some confidence evaluationmetric on the development corpus using the Downhill Simplex algorithm (Press et al2002).
With this approach, the confidence error rates were reduced over the best singleconfidence measure consistently on all corpora we examined.
The experimental resultswill be presented in Section 7.4.
This section also contains details on which confidencemeasures were combined.However, the focus of this work is on word posterior probabilities as stand-aloneconfidence measures.
It was shown that they are the best single features for confidenceestimation (Blatz et al 2004).
Moreover, they are closely related to Bayes risk, whichyields a sound theoretical foundation (Ueffing and Ney 2004).7.
Experiments7.1 Experimental SettingThe experiments were performed on three translation tasks in different language pairs.The corpora were compiled in the EU projects TransType2 (TransType2 2005) andTC-STAR (TC-STAR 2005), and for the NIST MT evaluation campaign (NIST 2004).
The24Ueffing and Ney Word-Level Confidence Estimation for MTTransType2 corpora consist of technical manuals for Xerox devices such as printers.They are available in three different language pairs.
This domain is very specializedwith respect to terminology and style.
The corpus statistics are given in Table 1.
TheTC-STAR corpus consists of proceedings of the European Parliament.
It is a spokenlanguage translation corpus containing the verbatim transcriptions of the speeches inthe European Parliament Plenary Sessions (EPPS).
The domain is basically unrestrictedbecause a wide range of different topics is covered in the sessions.
The translationdirection is from Spanish into English.
For corpus statistics, see Table 2.
The NIST corpuswas compiled for the yearly MT evaluation campaign carried out since 2001.
Chinesenews articles are translated into English.
Similarly to the EPPS data, the domain isbasically unrestricted, because a wide range of different topics is covered.
However,the vocabulary size and the training corpus are much larger than in the EPPS collection,as the corpus statistics presented in Table 3 show.
Additionally to the bilingual data, amonolingual English corpus consisting of 636M running words was used for languagemodel training.
The SMT systems that generated the translations for which confidenceestimation was performed were trained on these corpora.
The same holds for theprobability models that were used to estimate the word confidences.We translated the development and test corpora using several different MT systemsfor testing the confidence measures: The phrase-based translation system described in Section 2.2 (denoted asPBT in the tables); a large part of the results will be presented for outputof this system.Table 1Statistics of the training, development, and test corpora for the TransType2 task.French English Spanish English German EnglishTRAIN Sentences 53,046 55,761 49,376Running words 680,796 628,329 752,606 665,399 537,464 589,531Vocabulary 15,632 13,816 11,050 7,956 23,845 13,223DEV Sentences 994 1,012 964Running words 11,674 10,903 15,957 14,278 10,462 10,642TEST Sentences 984 1,125 996Running words 11,709 11,177 10,106 8,370 11,704 12,298Table 2Statistics of the training, development, and test corpora for the TC-STAR EPPS Spanish?Englishtask.
Both development and test corpus are provided with two English references.Spanish EnglishTRAIN Sentences 1,652,174Running words 32,554,077 31,147,901Vocabulary 124,192 80,125DEV Sentences 2,643Running words 20,289 40,396TEST Sentences 1,073Running words 18,896 37,74225Computational Linguistics Volume 33, Number 1Table 3Statistics of the training, development, and test corpora for the NIST Chinese?English task.
Bothdevelopment and test corpus are provided with four English references.Chinese EnglishTRAIN Sentences 7MRunning words 199M 213MVocabulary 223K 351KDictionary entries 82KDEV (2002 evaluation set) Sentences 878Running words 25K 105KTEST (2004 evaluation set) Sentences 1,788Running words 52K 239K The alignment template system (Och and Ney 2004) (denoted as AT in thetables), which is also a state-of-the art phrase-based translation system. The Systran version available at http://babelfish.altavista.com/tr inJune 2005.
These hypotheses were used to investigate whether the directconfidence measures perform well on translations generated by astructurally different system.The translation quality on the TransType2 task in terms of WER, PER, BLEU score(Papineni et al 2002), and NIST score (NIST 2002) is given in Table 4.
We see thatthe best results are obtained on Spanish to English translation, followed by French toEnglish and German to English.
The reason that Systran generates translations of muchlower quality than the SMT systems is due to the fact that the technical manuals are veryspecific in terminology.
The SMT systems were trained on similar corpora so that theyare familiar with the terminology.
The table additionally shows the translation qualityachieved by the system PBT on the NIST test set.Table 4Translation quality of different MT systems on the TransType2 and the NIST test corpora.Task Language pair System WER[%] PER[%] BLEU[%] NISTTransType2 F ?
E PBT 54.9 43.4 31.3 6.62AT 54.8 43.7 31.5 6.64Systran 81.5 71.7 12.5 4.23S ?
E PBT 26.1 17.5 66.9 8.98AT 29.6 20.1 63.4 8.80Systran 78.0 62.3 23.4 4.77G ?
E PBT 61.6 49.6 25.7 5.72AT 62.7 49.8 26.6 5.92Systran 79.2 66.4 12.0 4.09NIST C ?
E PBT 61.8 42.9 31.1 8.47C = Chinese; E = English; F = French; G = German; S = Spanish.26Ueffing and Ney Word-Level Confidence Estimation for MTOn the EPPS task from TC-STAR, the confidence measures were tested on outputfrom the phrase-based translation system.
The hypotheses are generated by the versionof the system that participated in the TC-STAR evaluation round in March 2005 andthat was ranked first there.
The translation quality can be seen in Table 12 later inthis article.7.2 Word Error MeasuresIn order to evaluate the classifier built from the confidence measures as described inSection 3.3, reference tags are needed that define the true class of each word.
In machinetranslation, it is not intuitively clear how to determine the correctness of a word.Therefore, a number of different measures for identifying the reference classes for singlewords in a translation hypothesis were implemented (Ueffing 2006).
They are inspiredby different translation evaluation measures like WER and PER.
All of them comparethe translation hypothesis to one or?if available?several references to determine theword errors.
In this article, we will present results for the following error measures: WER: A word is counted as correct if it is Levenshtein-aligned to itself inone of the references. PER: A word is tagged as correct if it occurs in one of the references.
Thenumber of occurrences per word is taken into account, but the position ofthe word in the sentence is completely disregarded.Both word error measures exist in two variants: First, each translation hypothesis iscompared to the pool of all references (in case there exist different reference translationsfor the development and test corpus).
Second, the reference with minimum distance tothe hypothesis according to the translation evaluation measure under consideration isdetermined.
The true classes of the words are then defined with respect to this nearestreference.
For example, if the PER metric is applied, the pooled variant labels all thosewords as correct that occur in any of the references (with this count).
The second variantconsiders as correct only those words that are contained in the nearest reference (withthis count).
The latter corresponds to the procedure used for m-WER and m-PER in MTevaluation (Nie?en et al 2000).Table 5 shows the percentage of words that are labeled as correct according to thedifferent error measures on the development and test corpora of the EPPS task.
It canbe seen that WER is the stricter error measure: It considers fewer words as correctthan PER does.
A comparison of the pooled and the nearest reference shows that thepooling yields a significant increase in the number of words labeled as correct.
Notethat the figures in the table do not directly correspond to the translation error rates forthe system output.
They are calculated only for the words contained in the generatedTable 5Ratio of correct words (%) in the EPPS Spanish ?
English development and test corpora,according to different word error measures.Error Measure WER PERpooled nearest pooled nearestDEV 78.6 72.9 81.5 77.4TEST 76.5 69.8 81.5 76.527Computational Linguistics Volume 33, Number 1translation hypotheses and do not take deleted words into account.
Moreover, theyare normalized by the hypothesis lengths.
If WER and PER are applied as translationevaluation measures (on the sentence level), deletions are counted as well, and thenumber of errors is divided by the number of reference words.7.3 Evaluation MetricsAfter computing the confidence measure, each generated word is tagged as eithercorrect or incorrect, depending on whether its confidence exceeds the tagging thresh-old that was optimized on the development set beforehand.
The performance of theconfidence measures is evaluated using the following three measures: Classification or Confidence Error Rate (CER): This is defined as thenumber of incorrect tags divided by the total number of generated wordsin the translated sentence.
The baseline CER is determined by assigningthe most frequent class (in the whole development or test corpus) to allwords.
Assume that the correct classes of the words are defined on thebasis of WER.
If the overall WER on the considered development ortest corpus is below 50%, the baseline CER is calculated by tagging allwords as correct.
The baseline CER then corresponds to the numberof substitutions and insertions, divided by the number of generatedwords.
The CER strongly depends on the tagging threshold.
Therefore,the tagging threshold is adjusted beforehand (to minimize CER) on adevelopment corpus distinct from the test set.
Moreover, we will presentsignificance bounds for the baseline CER.
They were determined usingthe bootstrap estimation method described in Bisani and Ney (2004).1 Receiver Operating Characteristic (ROC) curve (Duda, Hart, and Stork2001):2 The ROC curve plots the correct rejection rate versus the correctacceptance rate for different values of the tagging threshold.
The correctrejection rate is the number of incorrectly translated words that weretagged as false, divided by the total number of incorrectly translatedwords.
The correct acceptance rate is the ratio of correctly translated wordsthat were tagged as correct.
These two rates depend on each other: If oneof them is restricted by a lower bound, the other one cannot be restricted.The further the ROC curve lies away from the diagonal (and away fromthe point of origin), the better the performance of the confidence measure.Unlike the CER, the ROC curve is independent of the prior probability ofthe two classes correct and incorrect.
This means that ROC curves fromdifferent data sets can be compared directly. Integral of the ROC curve (IROC): ROC curves provide for a qualitativeanalysis of classifier performance; a related quantitative metric is IROC,defined as the area under a ROC curve.
The IROC takes on values in [0, 1],with 0.5 corresponding to a random separation of correct and incorrectwords, 1.0 corresponding to a perfect separation, and 0.0 the opposite.1 The tool described in this paper is freely available from http://www-i6.informatik.rwth-aachen.de/web/Software/.2 A variant of the ROC curve is the Detection Error Tradeoff (DET) curve which plots the false rejection rateversus the false acceptance rate.28Ueffing and Ney Word-Level Confidence Estimation for MT7.4 Experimental ResultsTransType2 task.
Table 6 compares the classification performance of several confidencemeasures on the TransType2 French?English task.
The CER and the IROC values aregiven for WER- and PER-based classification.
Note that lower CER and higher IROCvalues express better performance.
It is interesting to see that, in most of the cases, thetendencies are consistent for the two evaluation metrics: Lower CER is accompanied byhigher IROC.In general, one can see that the very simple approach that sums over sentencesin the N-best list or word graph considering the fixed target position of the wordclearly performs worst.
This is to be expected, and the method was included only forcomparison.
It can be considered as a simple baseline method.
The other system-basedmeasures discriminate significantly better in both settings.The system-based confidence measures show much better discriminative powerthan the direct IBM model 1.
The N-best list based measure with Levenshtein alignmentand the word posterior probabilities calculated over word graphs using a windowperform similarly well.
For WER-based classification, they are outperformed only bythe direct phrase?based approach, which achieves the best CER and IROC values.It is interesting to compare the two methods that were applied to both wordgraphs and N-best lists: the approach based on the fixed target position and the onesumming over a window of positions.
In both cases, the word graph?based calculationis slightly superior to that based on 10,000-best lists.
However, the difference in CER isnot significant.The count-based method working on N-best lists is clearly the best confidencemeasure for PER-based classification.
This result was to be expected because the count-based word posterior probability was derived from the Bayes risk for PER (Ueffingand Ney 2004).
Even if its CER does not differ much from that of the direct phrase-based measure, there exists a clear predominance in terms of IROC.
The IBM-1?basedconfidence measure performs rather poorly compared to the other methods.
This is notsurprising because the IBM model 1 is a very simple model.Table 6Classification performance in terms of CER (%) and IROC (%) for different confidence measures.TransType2 French ?
English test set.
References based on WER and PER, confidence measuresoptimized accordingly.
Hypotheses from the phrase-based system.Model WER PERCER IROC CER IROCbaseline 42.2 ?
34.2 ?99% confidence interval ?2.3 ?
?2.0 ?10,000-best lists, fixed position 39.7 66.2 33.3 66.2Levenshtein 31.3 72.6 28.1 74.8window ?3 31.6 70.7 28.3 73.4count-based 31.9 71.6 27.0 76.5word graphs, fixed position 38.6 70.5 33.1 67.6window ?3 31.1 72.4 27.3 75.4IBM-1 (max.)
39.2 67.0 31.5 71.0direct phrase-based 30.6 74.4 27.4 73.729Computational Linguistics Volume 33, Number 1The comparison of the IROC values for WER- and PER-based classification showsthat PER is easier to learn than WER: The IROC values for PER are higher for mostconfidence measures.
This is consistent with the results obtained in the CLSP workshop(Blatz et al 2003).
The classifiers investigated there also show better discriminativepower for reference classes based on PER than for WER.To further illustrate the classification performance of the different confidence mea-sures, the ROC curves for some of them are given in Figure 2.
In each, the diagonalline refers to random classification of words as correct and incorrect.
The left curveshows the results for WER-based classification, and the right one for PER, respectively.The N-best list-based method considering the fixed target position is again given forcomparison.
One can see that the IBM-1?based confidence measure is clearly better thanthis baseline for PER, but not for WER.
The curves for the direct phrase-based model andthe best N-best list-based method lie relatively close to each other.
These two confidencemeasures clearly dominate all others.Because the direct phrase-based confidence measures perform so well on the outputof the phrase-based translation system, we were interested in finding out whether this isdue to the fact that the translation system and the confidence measure explore the samestatistical models.
Therefore, the system-independent confidence measures (i.e., thosebased on IBM model 1 and the direct phrase-based method) were tested on output fromdifferent machine translation systems, including Systran as a non-statistical MT system.The experimental results are shown in Table 7.
They can be summarized as follows: In all settings, both measures distinctly decrease the CER compared to thebaseline.
In one case (Spanish to English, Systran), the achieved CER is asmuch as 60% lower than the baseline CER. On French to English and German to English, all improvements aresignificant at the 1% level.
On Spanish to English, which is the languagepair yielding by far the lowest baseline CER, only the phrase-basedmeasure achieves a reduction at this level of significance. In all but one case, the direct phrase-based approach outperforms theIBM-1?based method significantly.
This tendency is consistent for bothCER and IROC.
The relative difference in CER is up to 20%.Figure 2ROC curves for different confidence measures.
TransType2 French ?
English test set.
Referencesbased on WER (left) and PER (right).
Hypotheses from the phrase-based system.30Ueffing and Ney Word-Level Confidence Estimation for MTTable 7Classification performance in terms of CER (%) and IROC (%) for different system-independentconfidence measures.
TransType2 test sets.
Reference based on WER.
Hypotheses from differentMT systems.Task Model AT PBT SystranCER IROC CER IROC CER IROCF ?
E baseline 42.5 ?
42.2 ?
32.8 ?99% confidence interval ?2.3 ?
?2.3 ?
?1.7 ?IBM-1 (max.)
34.1 68.3 35.6 66.9 26.0 81.3direct phrase-based 30.2 73.0 30.6 74.4 22.7 83.2S ?
E baseline 20.8 ?
19.2 ?
43.7 ?99% confidence interval ?1.9 ?
?2.0 ?
?1.5 ?90% confidence interval ?1.2 ?
?1.3 ?
?1.0 ?IBM-1 (max.)
20.0 66.8 18.3 73.2 21.7 85.5direct phrase-based 17.5 76.0 16.4 77.0 17.3 87.5G ?
E baseline 49.2 ?
48.4 ?
37.4 ?99% confidence interval ?2.2 ?
?2.4 ?
?1.4 ?IBM-1 (max.)
32.7 73.3 32.8 72.2 23.6 80.7direct phrase-based 27.6 79.1 26.4 80.3 24.3 81.4 On the German to English Systran hypotheses, both confidence measuresdiscriminate similarly well.
In terms of CER, the IBM model 1 is slightlybetter, whereas the phrase-based method achieves the highest IROC value.EPPS task.
Further experiments comparing the classification performance of the differ-ent confidence measures were carried out on the EPPS data task, which is structurallydifferent from the Xerox task.
The EPPS collection consists of speeches given in theplenary sessions of the European Parliament, translated from Spanish into English.
TheEPPS task is more challenging than the Xerox manuals because the domain is almostunrestricted and the translation has to cope with effects of spontaneous speech.
The goalof these experiments is to find out whether the confidence measures perform equallywell on this challenging task as on the Xerox task.
The development and test set of theEPPS data are provided with two references each.
This makes it possible to compare thetwo ways of handling multiple references: As explained in Section 7.2, the true class of aword can be determined either with respect to the pooled references or to the referencewith minimal distance.Table 8 presents the CER and IROC values for different confidence measures on theEPPS task.
The classification with respect to m-WER and m-PER (i.e., considering onlythe nearest reference) as word error measures was investigated.
The confidence mea-sures based on the fixed position were not calculated because the previous experimentsshowed that they perform significantly worse than the other measures.
It can be seen inthe table that the word posterior probabilities derived from the Bayes risk for the worderror measures perform best: The Levenshtein-based confidence measure discriminatesbest for m-WER and the count-based approach for m-PER.
They are clearly superior toall other confidence measures, especially in terms of IROC.
For WER-based classifica-tion, the word graph-based method performs similarly well to the Levenshtein-basedmeasure in terms of CER, but significantly worse if IROC is considered.The results achieved by the direct phrase-based approach on this task are not asgood as on the Xerox data.
The reason for this is that the domain of the EPPS collection31Computational Linguistics Volume 33, Number 1Table 8Classification performance in terms of CER (%) and IROC (%) for different confidence measures.EPPS Spanish ?
English test set.
Reference based on m-WER and m-PER, confidence measuresoptimized accordingly.
Hypotheses from the phrase-based system.Model m-WER m-PERCER IROC CER IROCbaseline 30.2 ?
23.5 ?99% confidence interval ?1.2 ?
?1.0 ?15,000-best lists, Levenshtein 25.7 75.4 21.6 74.2window ?3 26.7 69.9 21.4 71.8count-based 27.6 71.4 21.2 78.3word graphs, window ?3 25.6 72.1 21.9 73.2IBM-1 (max.)
27.7 68.7 21.5 72.5direct phrase-based 26.8 67.5 21.2 70.9is almost unrestricted.
We found in a data analysis that the phrase models do notcapture the data as well as they do in the Xerox domain (Ueffing 2006).
Nevertheless, form-PER?based classification, the direct phrase-based measures achieve the same reduc-tion in CER over the baseline as the system-based method using count information.Because the direct phrase-based confidence measures completely disregard the targetposition of the word, they are better suited for PER-based classification than for WER.As is to be expected, the IBM model 1?based confidence measure performs betterfor reference tags defined by m-PER than for m-WER.
However, it is among the methodswith the worst discriminative power in both cases.In general, the improvements over the CER baseline are not as high on theseEPPS data as on the TransType2 corpora.
The relative gain in CER is 15% for the bestconfidence measure.
But because the test corpora are large?with 20,000 running wordsthey are about twice as big as the TransType2 test sets?all achieved improvementsare significant at the 1% level.
The IROC values are comparable to those achieved onTransType2 data.
The fact that the IROC is independent of the baseline error allowsfor the conclusion that the confidence measures are well-suited for this challengingtranslation task as well.Figure 3ROC curves for different confidence measures.
EPPS Spanish ?
English test set.
Referencesbased on m-WER (left) and m-PER (right).
Hypotheses from the phrase-based system.32Ueffing and Ney Word-Level Confidence Estimation for MTThe ROC curves shown in Figure 3 further illustrate the classification performanceof the different measures.
The left curve shows the results for m-WER?based classifi-cation, and the right one for m-PER.
One can see that for m-WER, the IBM-1?basedand the direct phrase-based confidence measures perform very similarly.
There is noclear difference between these two approaches and the one calculated over a windowof target positions.
The discriminative power of the direct model is higher for a lowercorrect acceptance ratio, whereas the system-based measure performs better for a highcorrect acceptance ratio.
The Levenshtein-based word posterior probabilities are clearlysuperior to all other approaches.
The ROC curve lies beyond the others over the wholerange.
For PER, the classifier based on word counts dominates all other confidencemeasures.
The three other methods show relatively similar performance.For all results presented so far, the reference tags were determined by comparingeach hypothesis to the most similar reference.
As mentioned in Section 7.2, it is alsopossible to pool the references instead.
Table 9 presents an assessment of the discrimi-native power of different confidence measures for these reference tags.
The conclusionsfrom these results are the same as for those in Table 8: The Levenshtein-based methodperforms best for WER, and the count-based one for PER.
All reported improvements inCER are significant at the 1% level.
The IROC values for the pooled error measures arehigher than for m-WER and m-PER for all confidence measures.
Obviously, this methodof error counting is easier to assess using confidence measures.
The differences in CERare not as large here as in Table 8.
However, the IROC values provide a clear indicationof the differences in quality between the classifiers.NIST task.
The third translation task that was used for the evaluation of the confidencemeasures proposed in this article is part of the NIST MT evaluation campaign.
The taskhere is the translation of news articles from Chinese into English.
As with the EPPS data,the domain is basically unrestricted.The experimental results are presented in Table 10.
The confidence measuresthat perform best on the two other tasks were evaluated on the NIST data.
Theresults support those achieved on the EPPS collection.
All confidence measures reduceCER over the baseline with significance at the 1% level.
For reference tags definedby m-WER, the confidence measure using Levenshtein alignment over N-best listsTable 9Classification performance in terms of CER (%) and IROC (%) for different confidence measures.EPPS Spanish ?
English test set.
Reference based on pooled WER and PER, confidencemeasures optimized accordingly.
Hypotheses from the phrase-based system.Model pooled WER pooled PERCER IROC CER IROCbaseline 23.5 ?
18.5 ?99% confidence interval ?1.1 ?
?1.0 ?15,000-best lists, Levenshtein 21.3 77.5 17.0 76.4window ?3 21.8 71.5 17.1 73.2count-based 21.8 73.7 16.7 80.6word graphs, window ?3 21.8 73.0 18.1 74.1IBM-1 (max.)
21.7 70.2 16.9 74.3direct phrase-based 21.3 69.5 16.8 69.333Computational Linguistics Volume 33, Number 1Table 10Classification performance in terms of CER (%) and IROC (%) for different confidence measures.NIST04 Chinese ?
English test set.
Reference based on m-WER and m-PER, confidencemeasures optimized accordingly.
Hypotheses from the phrase-based system.model m-WER m-PERCER IROC CER IROCbaseline 46.2 ?
32.7 ?99% confidence interval ?1.0 ?
?0.6 ?10,000-best lists, Levenshtein 37.2 67.4 30.5 67.5window ?3 39.2 64.4 30.5 67.0count-based 37.0 66.0 28.4 72.0IBM-1 (max) 42.9 58.0 31.9 59.9direct phrase-based 37.3 66.7 27.1 71.8performs best.
Especially in terms of IROC, this method is clearly superior to theother confidence measures.
The count-based method achieves a CER that is 0.2% lower,which is not significant.
For classification with respect to m-PER, there are two meth-ods that outperform the others: the count-based confidence measure calculated overN-best lists and the direct phrase-based approach.
They achieve CER and IROC valuesthat differ significantly from those of the other measures.
However, neither of the twoapproaches is clearly superior to the other: The direct phrase-based confidence measureachieves a lower CER of 27.1%, whereas the count-based confidence measure calculatedover N-best lists achieves a slightly higher IROC value.
The confidence measure basedon IBM model 1 shows by far the worst discriminative power for both m-WER- andm-PER-based classification.
The CER obtained with this method is significantly higherthan those of all other measures.Combination of features.
Because feature combination yields good results in the exper-iments reported in related work such as Blatz et al (2003), we performed similarexperiments.
The confidence measures investigated here were combined log-linearlyas described in Section 6.
The resulting confidence measures were evaluated on allthree translation tasks.
The three single word posterior probabilities that perform bestin each setting were used in the combination.
For the confidence estimation with respectto reference tags defined by m-WER, these are: the system-based word posterior probabilities based on Levenshteinalignment the system-based word posterior probabilities performing windowingover target positions the direct phrase-based methodIf the reference tags are determined by m-PER, the features used differ slightly, depend-ing on the corpus.
The measures that are combined are three of the following: the system-based word posterior probabilities based on the word count the system-based word posterior probabilities performing windowingover target positions34Ueffing and Ney Word-Level Confidence Estimation for MT the confidence measure based on IBM model 1 the direct phrase-based methodThe experimental results for the combined confidence measures are presented inTable 11.
They show that the resulting confidence measure outperforms the best singlemethod.
The improvement in CER is up to 1.8% in absolute terms.
In terms of IROC,the gain is up to 4.4 points.
This is in the same range as the improvements achieved inthe CLSP summer workshop (Blatz et al 2003).
However, there is one case in which theIROC decreases, namely the m-PER?based classification on EPPS Spanish to English.This can be explained by the fact that the combination was optimized with respectto CER.
In order to avoid this type of inconsistency, the optimization could be performedconsidering a combination of CER and IROC as criterion.8.
Rescoring8.1 ApproachThis section reports on the use of word posterior probabilities for rescoring of N-bestlists.
The rescoring is performed as follows: For every hypothesis in the N-best list, theconfidence of each word in the sentence is calculated.
These word posterior probabilitiesare multiplied to obtain a score for the whole sentence.
This sentence score is thenused as an additional model for N-best list rescoring.
It serves as an indicator of theoverall quality of the generated hypothesis.
Additionally, the minimal word posteriorprobability over the sentence is determined.
This can be seen as an indicator of whetherthe hypothesis contains words that are likely to be incorrect.
These new models arecombined with the existing models (such as the score assigned by the underlying SMTsystem and additional language model scores) in a log-linear manner.
The scalingfactors of all models are optimized on the development corpus using the DownhillSimplex algorithm.
This combination using the optimized factors is then applied andevaluated on the test set.Table 11Classification performance in terms of CER (%) and IROC (%) for a log-linear combination ofword posterior probabilities.
Test sets from all three tasks.
References based on m-WER andm-PER.
Hypotheses from the phrase-based system.Reference tag m-WER m-PERTask confidence measure CER IROC CER IROCTransType2 F ?
E baseline 42.2 ?
34.2 ?best single 30.6 74.4 27.0 76.5combination of 3 29.5 75.5 25.4 78.4EPPS S ?
E baseline 30.2 ?
23.5 ?best single 25.7 75.4 21.2 78.3combination of 3 25.7 76.1 20.1 76.9NIST C ?
E baseline 46.2 ?
32.7 ?best single 37.3 67.2 27.4 71.3combination of 3 35.5 68.0 25.8 75.735Computational Linguistics Volume 33, Number 18.2 Experimental ResultsRescoring was carried out on EPPS data using the direct phrase-based confidencemeasures.
Within the project TC-STAR, an MT evaluation campaign was performedin March 2005 to compare the research systems of the consortium members (Ney et al2005).
Different conditions concerning the input data were defined.
In the following,rescoring results on the verbatim transcriptions will be presented.
The translationsthat RWTH submitted to this evaluation were generated by the phrase-based translationsystem described in Section 2.2.
N-best lists were generated for development and testcorpus, with a maximum length of 20,000 and 15,000, respectively.
These were thenrescored with an IBM model 1, a 4-gram language model, and a deletion model basedon IBM-1.
The weights for all these models and for the sentence probability assignedby the SMT system were optimized with respect to BLEU score on the developmentcorpus.
For a detailed description of the system, see Vilar et al (2005).
This system wasranked first in the evaluation round according to all evaluation criteria (Ney et al 2005).Two different sets of rescoring experiments were performed.
They differ only intheir starting points: The first one starts from the baseline system without rescoring.The sub-model weights of this system were optimized with respect to BLEU on thedevelopment set, but no additional models were used for rescoring the N-best list.This experiment was performed to analyze the maximum improvement that can beachieved through rescoring with confidence measures.
The second experiment startsfrom the system that has already been rescored with the three different models men-tioned above.
This is the system that was used in the TC-STAR evaluation campaign,and that was ranked first there.
In this setting, it can be seen whether the rescoringwith confidence measures manages to improve upon the best available system as well.Furthermore, it is possible to analyze whether the gains from all rescoring models areadditive.The results are shown in Table 12.
The upper block evaluates the translation qualitywithout considering case, and the second one contains the case-sensitive evaluation.These different figures are presented here in order to separate the effect of the transla-tion and the true-casing process.
The translation system was trained on a lower-casedcorpus, and the true-casing is performed as an additional post-processing step.Table 12Translation quality for rescoring with confidence measures.
EPPS Spanish ?
English test set.Optimized for BLEU.case?
System WER (%) PER (%) BLEU (%) NISTno baseline 40.9 30.4 45.5 9.83+ direct phrase-based confidence measure 40.8 29.9 46.5 9.93+IBM-1+LM+deletion model 40.6 29.5 46.6 9.99+direct phrase-based confidence measure 40.4 29.4 47.2 10.04yes baseline 42.5 32.2 45.1 9.67+ direct phrase-based confidence measure 42.7 32.0 45.6 9.68+IBM-1+LM+deletion model 42.5 31.7 45.9 9.75+direct phrase-based confidence measure 42.4 31.6 46.2 9.78second best translation system 43.9 33.4 44.1 9.4736Ueffing and Ney Word-Level Confidence Estimation for MTLet us first consider the case-insensitive results.
The baseline is the single bestoutput of the translation system.
This system can be improved through rescoring withconfidence measures by 1 BLEU point.
This is only 0.1 BLEU points less than thegain achieved from rescoring with the three other models.
The system from the secondsetup (rescored with IBM model 1, the language and the deletion model) improves theBLEU score by 1.1 points over the baseline.
Another 0.6 BLEU points can be gainedthrough additional rescoring with the direct phrase-based confidence measures.
Theimprovement is consistent across all four automatic evaluation criteria.
Naturally, thegain in BLEU score is higher than for the other measures, because the system wasoptimized with respect to BLEU.In the TC-STAR evaluation campaign, case information was taken into account.The corresponding results are presented in the second block of the table.
The overalltranslation quality is lower if case is considered.
For all models applied here, the gainachieved through rescoring is not as big as in the case-insensitive evaluation.
If only theconfidence measures are used for rescoring, the BLEU score is increased by 0.5 points.The NIST score and the error measures change only slightly.
However, when all fourrescoring models are applied, the system is significantly improved.
The models usedin the TC-STAR evaluation yield an increase of 0.8 BLEU points.
The word posteriorprobabilities add another 0.3 points to this.
This change is rather small, but comparableto the contribution of each single rescoring model used in the evaluation campaign.For comparison, the translation quality of the second best system in this campaign isreported in the last row of the table.
The difference in BLEU score between the RWTHsystem and the second best can be significantly improved through rescoring.9.
ConclusionIn this work, we set up a probabilistic framework for the computation of word posteriorprobabilities for machine translation.
Within this framework, different concepts of wordposterior probabilities were defined and analyzed.
Several approaches to the calculationof word posterior probabilities were investigated and compared: system-based methodsthat explore information provided by the SMT system that generated the translations,and direct model-based methods that make use of statistical (translation) models.The use of word posterior probabilities as confidence measures was studied, in-cluding their application in a rescoring scenario.
The proposed confidence measureswere systematically evaluated on different translation tasks and different languagepairs.
On all corpora, the best methods developed here reduce the confidence error ratesignificantly (at the 1% level).
The direct confidence measures were also successfullyapplied to output from a non-statistical MT system.The results of the experiments can be summarized as follows: The performance of the confidence measures depends heavily on theword error measure that defines the reference tags.
Naturally, theword posterior probabilities derived from Bayes risk for this word errormeasure discriminate best.
For WER, this is the approach based on theLevenshtein alignment, and for PER this is the method that considersthe counts of the words. The direct phrase-based confidence measures perform very well on therestricted domain of the TransType2 corpora consisting of technicalmanuals.
There, they outperform all other measures.
However, this is37Computational Linguistics Volume 33, Number 1not the case for data from domains that are basically unrestricted,such as the EPPS and NIST corpora.
There, the system-based measuresdiscriminate better for reference tags given by WER.
For PER-basedconfidence estimation, the direct phrase-based confidence measure andthe count-based confidence measure calculated over N-best lists showthe best performance. The confidence measures based on IBM model 1 normally perform worsethan the system-based or direct phrase-based methods.
The reason for thisis that the IBM model 1 is a very simple model that does not consider thecontext of a target word at all. The combination of several different word posterior probabilities into oneconfidence measure yields better confidence estimation performance thanthe best single feature.
However, the word posterior probabilitiesproposed here proved to be strong stand-alone features (see alsoexperiments reported in Blatz et al [2003]). Rescoring with confidence measures was shown to improve translationquality.
The SMT system investigated here was the one that was rankedfirst in the TC-STAR evaluation campaign in March 2005.
It wasconsistently improved through rescoring with confidence measures.AcknowledgmentsThis work was partly funded by theEuropean Union under the RTD projectTransType2 (IST?2001?32091), and under theintegrated project TC-STAR?Technology andCorpora for Speech to Speech Translation(IST-2002-FP6-506738).
Nicola Ueffing wouldlike to thank her former and currentcolleagues at RWTH Aachen University andthe National Research Council Canada andeverybody from the ?CE for SMT?
workshopteam for their feedback and support, and theanonymous reviewers for their helpfulcomments on earlier versions of this article.ReferencesAkiba, Yasuhiro, Eiichiro Sumita, HiromiNakaiwa, Seiichi Yamamoto, andHiroshi G. Okuno.
2004.
Using a mixtureof N-best lists from multiple MT systemsin rank-sum-based confidence measurefor MT outputs.
In Proceedings of COLING?04: The 20th International Conference onComputational Linguistics, pages 322?328,Geneva, Switzerland.Bertoldi, Nicola, Roldano Cattoni, MauroCettolo, and Marcello Federico.
2004.
TheITC-irst statistical machine translationsystem for IWSLT-2004.
In Proceedingsof the International Workshop on SpokenLanguage Translation (IWSLT), pages 51?58,Kyoto, Japan.Bisani, Maximilian and Hermann Ney.2004.
Bootstrap estimates for confidenceintervals in ASR performance evaluation.In Proceedings of the IEEE InternationalConference on Acoustics, Speech, and SignalProcessing (ICASSP), pages 409?412,Montreal, Canada.Blatz, John, Erin Fitzgerald, GeorgeFoster, Simona Gandrabur, CyrilGoutte, Alex Kulesza, AlbertoSanchis, and Nicola Ueffing.
2003.Confidence estimation formachine translation.
Final report,JHU/CLSP Summer Workshop.http://www.clsp.jhu.edu/ws2003/groups/estimate/.Blatz, John, Erin Fitzgerald, George Foster,Simona Gandrabur, Cyril Goutte,Alex Kulesza, Alberto Sanchis, andNicola Ueffing.
2004.
Confidenceestimation for machine translation.In Proceedings of COLING ?04: The 20thInternational Conference on ComputationalLinguistics, pages 315?321, Geneva,Switzerland.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and Robert L.Mercer.
1993.
The mathematics ofstatistical machine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Chiang, David.
2005.
A hierarchicalphrase-based model for statisticalmachine translation.
In Proceedings of38Ueffing and Ney Word-Level Confidence Estimation for MTthe 43rd Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 263?270, Ann Arbor, MI.Duda, Richard O., Peter E. Hart, andDavid G. Stork.
2001.
Pattern Classificationand Scene Analysis.
John Wiley & Sons,New York.Gandrabur, Simona and George Foster.
2003.Confidence estimation for text prediction.In Proceedings of the Conference on NaturalLanguage Learning (CoNLL), pages 95?102,Edmonton, Canada.Jayaraman, Shyamsundar and Alon Lavie.2005.
Multi-engine machine translationguided by explicit word matching.
InProceedings of the 10th Annual Conferenceof the European Association for MachineTranslation (EAMT), pages 143?152,Budapest, Hungary.Jurafsky, Daniel and James H. Martin.
2000.Speech and Language Processing.
PrenticeHall, Upper Saddle River, NJ.Koehn, Philipp, Franz J. Och, and DanielMarcu.
2003.
Statistical phrase-basedtranslation.
In Proceedings of theHuman Language Technology Conference(HLT/NAACL), pages 127?133, Edmonton,Canada.Levenshtein, Vladimir I.
1966.
Binary codescapable of correcting deletions, insertionsand reversals.
Soviet Physics Doklady,10(8):707?710.Ney, Hermann, Volker Steinbiss, RichardZens, Evgeny Matusov, J. Gonzalez,Young-Suk Lee, Salim Roukos, MarcelloFederico, Muntsin Kolss, and RafaelBanchs.
2005.
TC-STAR deliverableno.
D5: SLT progress report.
Technicalreport, Integrated project TC-STAR(IST-2002-FP6-506738) funded by theEuropean Commission.http://www.tc-star.org/.Nie?en, Sonja, Franz J. Och, Gregor Leusch,and Hermann Ney.
2000.
An evaluationtool for machine translation: Fastevaluation for MT research.
In Proceedingsof the Second International Conference onLanguage Resources and Evaluation (LREC),pages 39?45, Athens, Greece.NIST.
2002.
Automatic evaluation ofmachine translation quality usingN-gram co-occurrence statistics.http://nist.gov/speech/tests/mt/.NIST.
2004.
Machine translation evaluationChinese?English.
http://nist.gov/speech/tests/mt/.Och, Franz J.
2003.
Minimum error ratetraining in statistical machinetranslation.
In Proceedings of the41st Annual Meeting of the Association forComputational Linguistics (ACL), pages160?167, Sapporo, Japan.Och, Franz J. and Hermann Ney.
2004.
Thealignment template approach to statisticalmachine translation.
ComputationalLinguistics, 30(4):417?449.Och, Franz J., Christoph Tillmann, andHermann Ney.
1999.
Improved alignmentmodels for statistical machine translation.In Proceedings of the Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora(EMNLP/VLC-99), pages 20?28, Universityof Maryland, College Park, MD.Papineni, Kishore, Salim Roukos, ToddWard, and Wei-Jing Zhu.
2002.
BLEU:A method for automatic evaluation ofmachine translation.
In Proceedings ofthe 40th Annual Meeting of the Associationfor Computational Linguistics (ACL),pages 311?318, Philadelphia, PA.Press, William H., Saul A. Teukolsky,William T. Vetterling, and Brian P.Flannery.
2002.
Numerical Recipes inC++.
Cambridge University Press,Cambridge, UK.Quirk, Chris.
2004.
Training a sentence-levelmachine translation confidence metric.In Proceedings of the Fourth InternationalConference on Language Resources andEvaluation (LREC), pages 825?828,Lisbon, Portugal.Sanchis, Alberto.
2004.
Estimacio?n y aplicacio?nde medidas de confianza en reconocimientoautoma?tico del habla.
Ph.D. thesis,Departamento de Sistemas Informa?ticos yComputacio?n, Universidad Polite?cnica deValencia, Valencia, Spain.Stolcke, Andreas.
2002.
SRILM?anextensible language modeling toolkit.In Proceedings of the International Conferenceon Spoken Language Processing (ICSLP),volume 2, pages 901?904, Denver, CO.TC-STAR.
2005.
TC-STAR?Technologyand corpora for speech to speechtranslation.
Integrated project TC-STAR(IST-2002-FP6-506738) funded by theEuropean Commission.http://www.tc-star.org/.Tillmann, Christoph.
2003.
A projectionextension algorithm for statisticalmachine translation.
In Proceedings ofthe Conference on Empirical Methods forNatural Language Processing (EMNLP),pages 1?8, Sapporo, Japan.TransType2.
2005.
TransType2?Computerassisted translation.
RTD projectTransType2 (IST?2001?32091) funded39Computational Linguistics Volume 33, Number 1by the European Commission.http://tt2.atosorigin.es/.Ueffing, Nicola.
2006.
Word ConfidenceMeasures for Machine Translation.
Ph.D.thesis, Computer Science Department,RWTH Aachen University, Aachen,Germany.Ueffing, Nicola, Klaus Macherey, andHermann Ney.
2003.
Confidencemeasures for statistical machinetranslation.
In Proceedings of theMT Summit IX, pages 394?401,New Orleans, LA.Ueffing, Nicola and Hermann Ney.2004.
Bayes decision rule andconfidence measures for statisticalmachine translation.
In Proceedings ofEsTAL?Espan?a for Natural LanguageProcessing, pages 70?81, Alicante, Spain.Lecture Notes in Computer Science,Springer Verlag.Ueffing, Nicola and Hermann Ney.
2005a.Application of word-level confidencemeasures in interactive statisticalmachine translation.
In Proceedingsof the 10th Annual Conference of theEuropean Association for MachineTranslation (EAMT), pages 262?270,Budapest, Hungary.Ueffing, Nicola and Hermann Ney.
2005b.Word-level confidence estimation formachine translation using phrase-basedtranslation models.
In Proceedingsof the Human Language TechnologyConference (HLT/EMNLP), pages 763?770,Vancouver, Canada.Ueffing, Nicola, Franz J. Och, andHermann Ney.
2002.
Generation ofword graphs in statistical machinetranslation.
In Proceedings of the Conferenceon Empirical Methods for Natural LanguageProcessing (EMNLP), pages 156?163,Philadelphia, PA.Vilar, David, Evgeny Matusov, Sas?a Hasan,Richard Zens, and Hermann Ney.
2005.Statistical machine translation of Europeanparliamentary speeches.
In Proceedings ofthe MT Summit X, pages 259?266, Phuket,Thailand.Vogel, Stephan, Sanjika Hewavitharana,Muntsin Kolss, and Alex Waibel.
2004.The ISL statistical translation system forspoken language translation.
In Proceedingsof the International Workshop on SpokenLanguage Translation (IWSLT), pages 65?72,Kyoto, Japan.Zens, Richard and Hermann Ney.
2004.Improvements in phrase-basedstatistical machine translation.
InProceedings of the Human LanguageTechnology Conference (HLT/NAACL),pages 257?264, Boston, MA.Zens, Richard and Hermann Ney.
2005.Word graphs for statistical machinetranslation.
In 43rd Annual Meeting of theAssociation for Computational Linguistics:Proceedings of the Workshop on Building andUsing Parallel Texts: Data-Driven MachineTranslation and Beyond, pages 191?198,Ann Arbor, MI.Zens, Richard and Hermann Ney.
2006.N-gram posterior probabilities forstatistical machine translation.
InHuman Language TechnologyConference of the North American Chapterof the Association for ComputationalLinguistics (HLT/NAACL): Proceedingsof the Workshop on Statistical MachineTranslation, pages 72?77,New York, NY.40
