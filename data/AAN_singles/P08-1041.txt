Proceedings of ACL-08: HLT, pages 353?361,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSummarizing Emails with Conversational Cohesion and SubjectivityGiuseppe Carenini, Raymond T. Ng and Xiaodong ZhouDepartment of Computer ScienceUniversity of British ColumbiaVancouver, BC, Canada{carenini, rng, xdzhou}@cs.ubc.caAbstractIn this paper, we study the problem of sum-marizing email conversations.
We first builda sentence quotation graph that captures theconversation structure among emails.
Weadopt three cohesion measures: clue words,semantic similarity and cosine similarity asthe weight of the edges.
Second, we usetwo graph-based summarization approaches,Generalized ClueWordSummarizer and Page-Rank, to extract sentences as summaries.Third, we propose a summarization approachbased on subjective opinions and integrate itwith the graph-based ones.
The empiricalevaluation shows that the basic clue wordshave the highest accuracy among the three co-hesion measures.
Moreover, subjective wordscan significantly improve accuracy.1 IntroductionWith the ever increasing popularity of emails, it isvery common nowadays that people discuss spe-cific issues, events or tasks among a group of peo-ple by emails(Fisher and Moody, 2002).
Those dis-cussions can be viewed as conversations via emailsand are valuable for the user as a personal infor-mation repository(Ducheneaut and Bellotti, 2001).In this paper, we study the problem of summariz-ing email conversations.
Solutions to this problemcan help users access the information embedded inemails more effectively.
For instance, 10 minutesbefore a meeting, a user may want to quickly gothrough a previous discussion via emails that is go-ing to be discussed soon.
In that case, rather thanreading each individual email one by one, it wouldbe preferable to read a concise summary of the pre-vious discussion with the major information summa-rized.
Email summarization is also helpful for mo-bile email users on a small screen.Summarizing email conversations is challengingdue to the characteristics of emails, especially theconversational nature.
Most of the existing meth-ods dealing with email conversations use the emailthread to represent the email conversation struc-ture, which is not accurate in many cases (Yeh andHarnly, 2006).
Meanwhile, most existing emailsummarization approaches use quantitative featuresto describe the conversation structure, e.g., numberof recipients and responses, and apply some generalmulti-document summarization methods to extractsome sentences as the summary (Rambow et al,2004) (Wan and McKeown, 2004).
Although suchmethods consider the conversation structure some-how, they simplify the conversation structure intoseveral features and do not fully utilize it into thesummarization process.In contrast, in this paper, we propose new summa-rization approaches by sentence extraction, whichrely on a fine-grain representation of the conversa-tion structure.
We first build a sentence quotationgraph by content analysis.
This graph not only cap-tures the conversation structure more accurately, es-pecially for selective quotations, but it also repre-sents the conversation structure at the finer granular-ity of sentences.
As a second contribution of this pa-per, we study several ways to measure the cohesionbetween parent and child sentences in the quotationgraph: clue words (re-occurring words in the reply)353(Carenini et al, 2007), semantic similarity and co-sine similarity.
Hence, we can directly evaluate theimportance of each sentence in terms of its cohesionwith related ones in the graph.
The extractive sum-marization problem can be viewed as a node rankingproblem.
We apply two summarization algorithms,Generalized ClueWordSummarizer and Page-Rankto rank nodes in the sentence quotation graph andto select the corresponding most highly ranked sen-tences as the summary.Subjective opinions are often critical in many con-versations.
As a third contribution of this paper, westudy how to make use of the subjective opinionsexpressed in emails to support the summarizationtask.
We integrate our best cohesion measure to-gether with the subjective opinions.
Our empiricalevaluations show that subjective words and phrasescan significantly improve email summarization.To summarize, this paper is organized as follows.In Section 2, we discuss related work.
After buildinga sentence quotation graph to represent the conver-sation structure in Section 3, we apply two summa-rization methods in Section 4.
In Section 5, we studysummarization approaches with subjective opinions.Section 6 presents the empirical evaluation of ourmethods.
We conclude this paper and propose fu-ture work in Section 7.2 Related WorkRambow et al proposed a sentence extraction sum-marization approach for email threads (Rambow etal., 2004).
They described each sentence in an emailconversations by a set of features and used machinelearning to classify whether or not a sentence shouldbe included into the summary.
Their experimentsshowed that features about emails and the emailthread could significantly improve the accuracy ofsummarization.Wan et al proposed a summarization approachfor decision-making email discussions (Wan andMcKeown, 2004).
They extracted the issue and re-sponse sentences from an email thread as a sum-mary.
Similar to the issue-response relationship,Shrestha et al(Shrestha and McKeown, 2004) pro-posed methods to identify the question-answer pairsfrom an email thread.
Once again, their resultsshowed that including features about the emailthread could greatly improve the accuracy.
Simi-lar results were obtained by Corston-Oliver et alThey studied how to identify ?action?
sentencesin email messages and use those sentences as asummary(Corston-Oliver et al, 2004).
All these ap-proaches used the email thread as a coarse represen-tation of the underlying conversation structure.In our recent study (Carenini et al, 2007), webuilt a fragment quotation graph to represent anemail conversation and developed a ClueWordSum-marizer (CWS) based on the concept of clue words.Our experiments showed that CWS had a higheraccuracy than the email summarization approachin (Rambow et al, 2004) and the generic multi-document summarization approach MEAD (Radevet al, 2004).
Though effective, the CWS methodstill suffers from the following four substantial limi-tations.
First, we used a fragment quotation graph torepresent the conversation, which has a coarser gran-ularity than the sentence level.
For email summa-rization by sentence extraction, the fragment granu-larity may be inadequate.
Second, we only adoptedone cohesion measure (clue words that are based onstemming), and did not consider more sophisticatedones such as semantically similar words.
Third, wedid not consider subjective opinions.
Finally, we didnot compared CWS to other possible graph-basedapproaches as we propose in this paper.Other than for email summarization, other docu-ment summarization methods have adopted graph-ranking algorithms for summarization, e.g., (Wan etal., 2007), (Mihalcea and Tarau, 2004) and (Erkanand Radev, 2004).
Those methods built a completegraph for all sentences in one or multiple documentsand measure the similarity between every pair ofsentences.
Graph-ranking algorithms, e.g., Page-Rank (Brin and Page, 1998), are then applied to rankthose sentences.
Our method is different from them.First, instead of using the complete graph, we buildthe graph based on the conversation structure.
Sec-ond, we try various ways to compute the similarityamong sentences and the ranking of the sentences.Several studies in the NLP literature have ex-plored the reoccurrence of similar words within onedocument due to text cohesion.
The idea has beenformalized in the construct of lexical chains (Barzi-lay and Elhadad, 1997).
While our approach andlexical chains both rely on lexical cohesion, they are354quite different with respect to the kind of linkagesconsidered.
Lexical chain is only based on similar-ities between lexical items in contiguous sentences.In contrast, in our approach, the linkage is based onthe existing conversation structure.
In our approach,the ?chain?
is not only ?lexical?
but also ?conversa-tional?, and typically spans over several emails.3 Extracting Conversations from MultipleEmailsIn this section, we first review how to build a frag-ment quotation graph through an example.
Then weextend this structure into a sentence quotation graph,which can allow us to capture the conversational re-lationship at the level of sentences.3.1 Building the Fragment Quotation Graphb> aE2c> b> > aE3 E4de> c> > b> > > aE5gh> > d> f> > eE6> gi> hjaE1(a) Conversation involving 6 Emailsba cedfhg ij(b) Fragment Quotation GraphFigure 1: A Real ExampleFigure 1(a) shows a real example of a conversa-tion from a benchmark data set involving 6 emails.For the ease of representation, we do not show theoriginal content but abbreviate them as a sequenceof fragments.
In the first step, all new and quotedfragments are identified.
For instance, email E3 isdecomposed into 3 fragments: new fragment c andquoted fragments b, which in turn quoted a. E4is decomposed into de, c, b and a.
Then, in thesecond step, to identify distinct fragments (nodes),fragments are compared with each other and over-laps are identified.
Fragments are split if necessary(e.g., fragment gh in E5 is split into g and h whenmatched with E6), and duplicates are removed.
Atthe end, 10 distinct fragments a, .
.
.
, j give rise to10 nodes in the graph shown in Figure 1(b).As the third step, we create edges, which repre-sent the replying relationship among fragments.
Ingeneral, it is difficult to determine whether one frag-ment is actually replying to another fragment.
Weassume that any new fragment is a potential reply toneighboring quotations ?
quoted fragments immedi-ately preceding or following it.
Let us consider E6in Figure 1(a).
there are two edges from node i to gand h, while there is only a single edge from j to h.For E3, there are the edges (c, b) and (c, a).
Becauseof the edge (b, a), the edge (c, a) is not included inFigure 1(b).
Figure 1(b) shows the fragment quota-tion graph of the conversation shown in Figure 1(a)with all the redundant edges removed.
In contrast,if threading is done at the coarse granularity of en-tire emails, as adopted in many studies, the thread-ing would be a simple chain from E6 to E5, E5 toE4 and so on.
Fragment f reflects a special and im-portant phenomenon, where the original email of aquotation does not exist in the user?s folder.
We callthis as the hidden email problem.
This problem andits influence on email summarization were studiedin (Carenini et al, 2005) and (Carenini et al, 2007).3.2 Building the Sentence Quotation GraphA fragment quotation graph can only represent theconversation in the fragment granularity.
We no-tice that some sentences in a fragment are more rel-evant to the conversation than the remaining ones.The fragment quotation graph is not capable of rep-resenting this difference.
Hence, in the following,we describe how to build a sentence quotation graphfrom the fragment quotation graph and introduceseveral ways to give weight to the edges.In a sentence quotation graph GS, each node rep-resents a distinct sentence in the email conversation,and each edge (u, v) represents the replying rela-tionship between node u and v. The algorithm tocreate the sentence quotation graph contains the fol-lowing 3 steps: create nodes, create edges and assignweight to edges.
In the following, we first illustratehow to create nodes and edges.
In Section 3.3, wediscuss different ways to assign weight to edges.Given a fragment quotation graph GF , we firstsplit each fragment into a set of sentences.
For eachsentence, we create a node in the sentence quotationgraph GS.
In this way, each sentence in the emailconversation is represented by a distinct node in GS.As the second step, we create the edges in GS.The edges in GS are based on the edges in GF355Pks1 s2 snP1C1 Ck(a) Fragment Quotation Graph(b) Sentence Quotation GraphF:Cts1, s2,...,sn... ...P1C1Pk...
...Figure 2: Create the Sentence Quotation Graph from theFragment Quotation Graphbecause the edges in GF already reflect the reply-ing relationship among fragments.
For each edge(u, v) ?
GF , we create edges from each sentenceof u to each sentence of v in the sentence quotationgraph GS.
This is illustrated in Figure 2.Note that when each distinct sentence in an emailconversation is represented as one node in the sen-tence quotation graph, the extractive email sum-marization problem is transformed into a standardnode ranking problem within the sentence quotationgraph.
Hence, general node ranking algorithms, e.g.,Page-Rank, can be used for email summarization aswell.3.3 Measuring the Cohesion BetweenSentencesAfter creating the nodes and edges in the sentencequotation graph, a key technical question is how tomeasure the degree that two sentences are related toeach other, e.g., a sentence su is replying to or be-ing replied by sv.
In this paper, we use text cohe-sion between two sentences su and sv to make thisassessment and assign this as the weight of the cor-responding edge (su, sv).
We explore three typesof cohesion measures: (1) clue words that are basedon stems, (2) semantic distance based on WordNetand (3) cosine similarity that is based on the wordTFIDF vector.
In the following, we discuss thesethree methods separately in detail.3.3.1 Clue WordsClue words were originally defined as re-occurring words with the same stem between twoadjacent fragments in the fragment quotation graph.In this section, we re-define clue words based on thesentence quotation graph as follows.
A clue word ina sentence S is a non-stop word that also appears(modulo stemming) in a parent or a child node (sen-tence) of S in the sentence quotation graph.The frequency of clue words in the two sentencesmeasures their cohesion as described in Equation 1.weight(su, sv) =?wi?sufreq(wi, sv) (1)3.3.2 Semantic Similarity Based on WordNetOther than stems, when people reply to previousmessages they may also choose some semanticallyrelated words, such as synonyms and antonyms, e.g.,?talk?
vs. ?discuss?.
Based on this observation, wepropose to use semantic similarity to measure thecohesion between two sentences.
We use the well-known lexical database WordNet to get the seman-tic similarity of two words.
Specifically, we use thepackage by (Pedersen et al, 2004), which includesseveral methods to compute the semantic similarity.Among those methods, we choose ?lesk?
and ?jcn?,which are considered two of the best methods in (Ju-rafsky and Martin, 2008).
Similar to the clue words,we measure the semantic similarity of two sentencesby the total semantic similarity of the words in bothsentences.
This is described in the following equa-tion.weight(su, sv) =?wi?su?wj?sv?
(wi, wj), (2)3.3.3 Cosine SimilarityCosine similarity is a popular metric to computethe similarity of two text units.
To do so, each sen-tence is represented as a word vector of TFIDF val-ues.
Hence, the cosine similarity of two sentencessu and sv is then computed as?
?su ???sv||?
?su ||?||?
?sv || .3564 Summarization Based on the SentenceQuotation GraphHaving built the sentence quotation graph with dif-ferent measures of cohesion, in this section, we de-velop two summarization approaches.
One is thegeneralization of the CWS algorithm in (Careniniet al, 2007) and one is the well-known Page-Rank algorithm.
Both algorithms compute a score,SentScore(s), for each sentence (node) s, which isused to select the top-k% sentences as the summary.4.1 Generalized ClueWordSummarizerGiven the sentence quotation graph, since the weightof an edge (s, t) represents the extent that s is relatedto t, a natural assumption is that the more relevant asentence (node) s is to its parents and children, themore important s is.
Based on this assumption, wecompute the weight of a node s by summing up theweight of all the outgoing and incoming edges of s.This is described in the following equation.SentScore(s) =?
(s,t)?GSweight(s, t) +?
(p,s)?GSweight(p, s)(3)The weight of an edge (s, t) can be any of thethree metrics described in the previous section.
Par-ticularly, when the weight of the edge is based onclue words as in Equation 1, this method is equiva-lent to Algorithm CWS in (Carenini et al, 2007).
Inthe rest of this paper, let CWS denote the General-ized ClueWordSummarizer when the edge weight isbased on clue words, and let CWS-Cosine and CWS-Semantic denote the summarizer when the edgeweight is cosine similarity and semantic similarityrespectively.
Semantic can be either ?lesk?
or ?jcn?.4.2 Page-Rank-based SummarizationThe Generalized ClueWordSummarizer only con-siders the weight of the edges without consideringthe importance (weight) of the nodes.
This mightbe incorrect in some cases.
For example, a sentencereplied by an important sentence should get some ofits importance.
This intuition is similar to the oneinspiring the well-known Page-Rank algorithm.
Thetraditional Page-Rank algorithm only considers theoutgoing edges.
In email conversations, what wewant to measure is the cohesion between sentencesno matter which one is being replied to.
Hence, weneed to consider both incoming and outgoing edgesand the corresponding sentences.Given the sentence quotation graph Gs, the Page-Rank-based algorithm is described in Equation 4.PR(s) is the Page-Rank score of a node (sentence)s. d is the dumping factor, which is initialized to0.85 as suggested in the Page-Rank algorithm.
Inthis way, the rank of a sentence is evaluated globallybased on the graph.5 Summarization with SubjectiveOpinionsOther than the conversation structure, the measuresof cohesion and the graph-based summarizationmethods we have proposed, the importance of a sen-tence in emails can be captured from other aspects.In many applications, it has been shown that sen-tences with subjective meanings are paid more at-tention than factual ones(Pang and Lee, 2004)(Esuliand Sebastiani, 2006).
We evaluate whether this isalso the case in emails, especially when the conver-sation is about decision making, giving advice, pro-viding feedbacks, etc.A large amount of work has been done on deter-mining the level of subjectivity of text (Shanahanet al, 2005).
In this paper we follow a very sim-ple approach that, if successful, could be extendedin future work.
More specifically, in order to as-sess the degree of subjectivity of a sentence s, wecount the frequency of words and phrases in s thatare likely to bear subjective opinions.
The assump-tion is that the more subjective words s contains, themore likely that s is an important sentence for thepurpose of email summarization.
Let SubjScore(s)denote the number of words with a subjective mean-ing.
Equation 5 illustrates how SubjScore(s) is com-puted.
SubjList is a list of words and phrases thatindicate subjective opinions.SubjScore(s) =?wi?SubjList,wi?sfreq(wi) (5)The SubjScore(s) alone can be used to evaluatethe importance of a sentence.
In addition, we cancombine SubjScore with any of the sentence scoresbased on the sentence quotation graph.
In this paper,we use a simple approach by adding them up as thefinal sentence score.357PR(s) = (1 ?
d) + d ?
?si?child(s)weight(s, si) ?
PR(si) +?sj?parent(s)weight(sj , s) ?
PR(sj)?si?child(s)weight(s, si) +?sj?parent(s)weight(sj , s)(4)As to the subjective words and phrases, weconsider the following two lists generated by re-searchers in this area.?
OpFind: The list of subjective words in (Wil-son et al, 2005).
The major source of this list isfrom (Riloff and Wiebe, 2003) with additionalwords from other sources.
This list contains8,220 words or phrases in total.?
OpBear: The list of opinion bearing wordsin (Kim and Hovy, 2005).
This list contains27,193 words or phrases in total.6 Empirical Evaluation6.1 Dataset SetupThere are no publicly available annotated corpora totest email summarization techniques.
So, the firststep in our evaluation was to develop our own cor-pus.
We use the Enron email dataset, which is thelargest public email dataset.
In the 10 largest in-box folders in the Enron dataset, there are 296 emailconversations.
Since we are studying summarizingemail conversations, we required that each selectedconversation contained at least 4 emails.
In total, 39conversations satisfied this requirement.
We use theMEAD package to segment the text into 1,394 sen-tences (Radev et al, 2004).We recruited 50 human summarizers to reviewthose 39 selected email conversations.
Each emailconversation was reviewed by 5 different humansummarizers.
For each given email conversation,human summarizers were asked to generate a sum-mary by directly selecting important sentences fromthe original emails in that conversation.
We askedthe human summarizers to select 30% of the totalsentences in their summaries.Moreover, human summarizers were asked toclassify each selected sentence as either essentialor optional.
The essential sentences are crucial tothe email conversation and have to be extracted inany case.
The optional sentences are not critical butare useful to help readers understand the email con-versation if the given summary length permits.
Byclassifying essential and optional sentences, we candistinguish the core information from the support-ing ones and find the most convincing sentences thatmost human summarizers agree on.As essential sentences are more important thanthe optional ones, we give more weight to the es-sential selections.
We compute a GSV alue for eachsentence to evaluate its importance according to thehuman summarizers?
selections.
The score is de-signed as follows: for each sentence s, one essen-tial selection has a score of 3, one optional selec-tion has a score of 1.
Thus, the GSValue of a sen-tence ranges from 0 to 15 (5 human summarizers x3).
The GSValue of 8 corresponds to 2 essential and2 optional selections.
If a sentence has a GSValueno less than 8, we take it as an overall essential sen-tence.
In the 39 conversations, we have about 12%overall essential sentences.6.2 Evaluation MetricsEvaluation of summarization is believed to be a dif-ficult problem in general.
In this paper, we use twometrics to measure the accuracy of a system gener-ated summary.
One is sentence pyramid precision,and the other is ROUGE recall.
As to the statisticalsignificance, we use the 2-tail pairwise student t-testin all the experiments to compare two specific meth-ods.
We also use ANOVA to compare three or moreapproaches together.The sentence pyramid precision is a relative pre-cision based on the GSValue.
Since this idea isborrowed from the pyramid metric by Nenkova etal.
(Nenkova et al, 2007), we call it the sentencepyramid precision.
In this paper, we simplify it asthe pyramid precision.
As we have discussed above,with the reviewers?
selections, we get a GSValue foreach sentence, which ranges from 0 to 15.
Withthis GSValue, we rank all sentences in a descendantorder.
We also group all sentences with the sameGSValue together as one tier Ti, where i is the corre-358sponding GSValue; i is called the level of the tier Ti.In this way, we organize all sentences into a pyra-mid: a sequence of tiers with a descendant order oflevels.
With the pyramid of sentences, the accuracyof a summary is evaluated over the best summary wecan achieve under the same summary length.
Thebest summary of k sentences are the top k sentencesin terms of GSValue.Other than the sentence pyramid precision, wealso adopt the ROUGE recall to evaluate the gen-erated summary with a finer granularity than sen-tences, e.g., n-gram and longest common subse-quence.
Unlike the pyramid method which givesmore weight to sentences with a higher GSValue,ROUGE is not sensitive to the difference betweenessential and optional selections (it considers all sen-tences in one summary equally).
Directly applyingROUGE may not be accurate in our experiments.Hence, we use the overall essential sentences as thegold standard summary for each conversation, i.e.,sentences in tiers no lower than T8.
In this way,the ROUGE metric measures the similarity of a sys-tem generated summary to a gold standard summarythat is considered important by most human sum-marizers.
Specifically, we choose ROUGE-2 andROUGE-L as the evaluation metric.6.3 Evaluating the Weight of EdgesIn Section 3.3, we developed three ways to com-pute the weight of an edge in the sentence quotationgraph, i.e., clue words, semantic similarity based onWordNet and cosine similarity.
In this section, wecompare them together to see which one is the best.It is well-known that the accuracy of the summariza-tion method is affected by the length of the sum-mary.
In the following experiments, we choose thesummary length as 10%, 12%, 15%, 20% and 30%of the total sentences and use the aggregated averageaccuracy to evaluate different algorithms.Table 1 shows the aggregated pyramid preci-sion over all five summary lengths of CWS, CWS-Cosine, two semantic similarities, i.e., CWS-leskand CWS-jcn.
We first use ANOVA to compare thefour methods.
For the pyramid precision, the F ratiois 50, and the p-value is 2.1E-29.
This shows that thefour methods are significantly different in the aver-age accuracy.
In Table 1, by comparing CWS withthe other methods, we can see that CWS obtains theCWS CWS-Cosine CWS-lesk CWS-jcnPyramid 0.60 0.39 0.57 0.57p-value <0.0001 0.02 0.005ROUGE-2 0.46 0.31 0.39 0.35p-value <0.0001 <0.001 <0.001ROUGE-L 0.54 0.43 0.49 0.45p-value <0.0001 <0.001 <0.001Table 1: Generalized CWS with Different Edge Weightshighest precision (0.60).
The widely used cosinesimilarity does not perform well.
Its precision (0.39)is about half of the precision of CWS with a p-valueless than 0.0001.
This clearly shows that CWS issignificantly better than CWS-Cosine.
Meanwhile,both semantic similarities have lower accuracy thanCWS, and the differences are also statistically sig-nificant even with the conservative Bonferroni ad-justment (i.e., the p-values in Table 1 are multipliedby three).The above experiments show that the widely usedcosine similarity and the more sophisticated seman-tic similarity in WordNet are less accurate than thebasic CWS in the summarization framework.
This isan interesting result and can be viewed at least fromthe following two aspects.
First, clue words, thoughstraight forward, are good at capturing the impor-tant sentences within an email conversation.
Thehigher accuracy of CWS may suggest that peopletend to use the same words to communicate in emailconversations.
Some related words in the previousemails are adopted exactly or in another similar for-mat (modulo stemming).
This is different from otherdocuments such as newspaper articles and formal re-ports.
In those cases, the authors are usually profes-sional in writing and choose their words carefully,even intentionally avoid repeating the same wordsto gain some diversity.
However, for email conver-sation summarization, this does not appear to be thecase.Moreover, in the previous discussion we only con-sidered the accuracy in precision without consider-ing the runtime issue.
In order to have an idea ofthe runtime of the two methods, we did the follow-ing comparison.
We randomly picked 1000 pairs ofwords from the 20 conversations and compute theirsemantic distance in ?jcn?.
It takes about 0.056 sec-onds to get the semantic similarity for one pair on the359average.
In contrast, when the weight of edges arecomputed based on clue words, the average runtimeto compute the SentScore for all sentences in a con-versation is only 0.05 seconds, which is even a littleless than the time to compute the semantic similar-ity of one pair of words.
In other words, when CWShas generated the summary of one conversation, wecan only get the semantic distance between one pairof words.
Note that for each edge in the sentencequotation graph, we need to compute the distancefor every pair of words in each sentence.
Hence, theempirical results do not support the use of semanticsimilarity.
In addition, we do not discuss the runtimeperformance of CWS-cosine here because of its ex-tremely low accuracy.6.4 Comparing Page-Rank and CWSTable 2 compares Page-Rank and CWS under differ-ent edge weights.
We compare Page-Rank only withCWS because CWS is better than the other Gener-alized CWS methods as shown in the previous sec-tion.
This table shows that Page-Rank has a loweraccuracy than that of CWS and the difference is sig-nificant in all four cases.
Moreover, when we com-pare Table 1 and 2 together, we can find that, foreach kind of edge weight, Page-Rank has a loweraccuracy than the corresponding Generalized CWS.Note that Page-Rank computes a node?s rank basedon all the nodes and edges in the graph.
In contrast,CWS only considers the similarity between neigh-boring nodes.
The experimental result indicates thatfor email conversation, the local similarity based onclue words is more consistent with the human sum-marizers?
selections.6.5 Evaluating Subjective OpinionsTable 3 shows the result of using subjective opinionsdescribed in Section 5.
The first 3 columns in this ta-ble are pyramid precision of CWS and using 2 listsof subjective words and phrases alone.
We can seethat by using subjective words alone, the precision ofeach subjective list is lower than that of CWS.
How-ever, when we integrate CWS and subjective wordstogether, as shown in the remaining 2 columns, theprecisions get improved consistently for both lists.The increase in precision is at least 0.04 with statisti-cal significance.
A natural question to ask is whetherclue words and subjective words overlap much.
OurCWS PR-Clue PR-Cosine PR-lesk PR-jcnPyramid 0.60 0.51 0.37 0.54 0.50p-value < 0.0001 < 0.0001 < 0.0001 < 0.0001ROUGE-2 0.46 0.4 0.26 0.36 0.39p-value 0.05 < 0.0001 0.001 0.02ROUGE-L 0.54 0.49 0.36 0.44 0.48p-value 0.06 < 0.0001 0.0005 0.02Table 2: Compare Page-Rank with CWSCWS OpFind OpBear CWS+OpFind CWS+OpBearPyramid 0.60 0.52 0.59 0.65 0.64p-value 0.0003 0.8 <0.0001 0.0007ROUGE-2 0.46 0.37 0.44 0.50 0.49p-value 0.0004 0.5 0.004 0.06ROUGE-L 0.54 0.48 0.56 0.60 0.59p-value 0.01 0.6 0.0002 0.002Table 3: Accuracy of Using Subjective Opinionsanalysis shows that the overlap is minimal.
For thelist of OpFind, the overlapped words are about 8%of clue words and 4% of OpFind that appear in theconversations.
This result clearly shows that cluewords and subjective words capture the importanceof sentences from different angles and can be usedtogether to gain a better accuracy.7 ConclusionsWe study how to summarize email conversationsbased on the conversational cohesion and the sub-jective opinions.
We first create a sentence quota-tion graph to represent the conversation structure onthe sentence level.
We adopt three cohesion metrics,clue words, semantic similarity and cosine similar-ity, to measure the weight of the edges.
The Gener-alized ClueWordSummarizer and Page-Rank are ap-plied to this graph to produce summaries.
Moreover,we study how to include subjective opinions to helpidentify important sentences for summarization.The empirical evaluation shows the following twodiscoveries: (1) The basic CWS (based on cluewords) obtains a higher accuracy and a better run-time performance than the other cohesion measures.It also has a significant higher accuracy than thePage-Rank algorithm.
(2) By integrating clue wordsand subjective words (phrases), the accuracy ofCWS is improved significantly.
This reveals an in-teresting phenomenon and will be further studied.ReferencesRegina Barzilay and Michael Elhadad.
1997.
Using lex-ical chains for text summarization.
In Proceedings of360the Intelligent Scalable Text Summarization Workshop(ISTS?97), ACL, Madrid, Spain.Sergey Brin and Lawrence Page.
1998.
The anatomyof a large-scale hypertextual web search engine.
InProceedings of the seventh international conference onWorld Wide Web, pages 107?117.Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.2005.
Scalable discovery of hidden emails from largefolders.
In ACM SIGKDD?05, pages 544?549.Giuseppe Carenini, Raymond T. Ng, and Xiaodong Zhou.2007.
Summarizing email conversations with cluewords.
In WWW ?07: Proceedings of the 16th interna-tional conference on World Wide Web, pages 91?100.Simon Corston-Oliver, Eric K. Ringger, Michael Gamon,and Richard Campbell.
2004.
Integration of emailand task lists.
In First conference on email and anti-Spam(CEAS), Mountain View, California, USA, July30-31.Nicolas Ducheneaut and Victoria Bellotti.
2001.
E-mailas habitat: an exploration of embedded personal infor-mation management.
Interactions, 8(5):30?38.Gu?nes Erkan and Dragomir R. Radev.
2004.
Lexrank:graph-based lexical centrality as salience in text sum-marization.
Journal of Artificial Intelligence Re-search(JAIR), 22:457?479.Andrea Esuli and Fabrizio Sebastiani.
2006.
Sentiword-net: A publicly available lexical resource for opinionmining.
In Proceedings of the International Confer-ence on Language Resources and Evaluation, May 24-26.Danyel Fisher and Paul Moody.
2002.
Studies of au-tomated collection of email records.
In University ofIrvine ISR Technical Report UCI-ISR-02-4.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing: An Introduction to NaturalLanguage Processing, Computational Linguistics, andSpeech Recognition (Second Edition).
Prentice-Hall.Soo-Min Kim and Eduard Hovy.
2005.
Automatic de-tection of opinion bearing words and sentences.
InProceedings of the Second International Joint Con-ference on Natural Language Processing: CompanionVolume, Jeju Island, Republic of Korea, October 11-13.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringingorder into texts.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP 2004), July.Ani Nenkova, Rebecca Passonneau, and Kathleen McK-eown.
2007.
The pyramid method: incorporating hu-man content selection variation in summarization eval-uation.
ACM Transaction on Speech and LanguageProcessing, 4(2):4.Bo Pang and Lillian Lee.
2004.
A sentimental educa-tion: sentiment analysis using subjectivity summariza-tion based on minimum cuts.
In ACL ?04: Proceedingsof the 42nd Annual Meeting on Association for Com-putational Linguistics, pages 271?278.Ted Pedersen, Siddharth Patwardhan, and Jason Miche-lizzi.
2004.
Wordnet::similarity - measuring the relat-edness of concepts.
In Proceedings of Fifth AnnualMeeting of the North American Chapter of the As-sociation for Computational Linguistics (NAACL-04),pages 38?41, May 3-5.Dragomir R. Radev, Hongyan Jing, Malgorzata Stys?, andDaniel Tam.
2004.
Centroid-based summarizationof multiple documents.
Information Processing andManagement, 40(6):919?938, November.Owen Rambow, Lokesh Shrestha, John Chen, andChirsty Lauridsen.
2004.
Summarizing email threads.In HLT/NAACL, May 2?7.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP 2003), pages 105?112.James G. Shanahan, Yan Qu, and Janyce Wiebe.
2005.Computing Attitude and Affect in Text: Theoryand Applications (The Information Retrieval Series).Springer-Verlag New York, Inc.Lokesh Shrestha and Kathleen McKeown.
2004.
Detec-tion of question-answer pairs in email conversations.In Proceedings of COLING?04, pages 889?895, Au-gust 23?27.Stephen Wan and Kathleen McKeown.
2004.
Generat-ing overview summaries of ongoing email thread dis-cussions.
In Proceedings of COLING?04, the 20th In-ternational Conference on Computational Linguistics,August 23?27.Xiaojun Wan, Jianwu Yang, and Jianguo Xiao.
2007.
To-wards an iterative reinforcement approach for simulta-neous document summarization and keyword extrac-tion.
In Proceedings of the 45th Annual Meeting of theAssociation of Computational Linguistics, pages 552?559, Prague, Czech Republic, June.Theresa Wilson, Paul Hoffmann, Swapna Somasun-daran, Jason Kessler, Janyce Wiebe, Yejin Choi, ClaireCardie, Ellen Riloff, and Siddharth Patwardhan.
2005.Opinionfinder: a system for subjectivity analysis.
InProceedings of HLT/EMNLP on Interactive Demon-strations, pages 34?35.Jen-Yuan Yeh and Aaron Harnly.
2006.
Email threadreassembly using similarity matching.
In Third Con-ference on Email and Anti-Spam (CEAS), July 27 - 28.361
