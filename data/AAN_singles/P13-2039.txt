Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 217?221,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsTopicSpam: a Topic-Model-Based Approach for Spam DetectionJiwei Li , Claire CardieSchool of Computer ScienceCornell UniversityIthaca, NY, 14853jl3226@cornell.educardie@cs.cornell.eduSujian LiLaboratory of Computational LinguisticsPeking UniversityBejing, P.R.China, 150001lisujian@pku.edu.cnAbstractProduct reviews are now widely used byindividuals and organizations for decisionmaking (Litvin et al, 2008; Jansen, 2010).And because of the profits at stake, peo-ple have been known to try to game thesystem by writing fake reviews to promotetarget products.
As a result, the task of de-ceptive review detection has been gainingincreasing attention.
In this paper, we pro-pose a generative LDA-based topic mod-eling approach for fake review detection.Our model can aptly detect the subtle dif-ferences between deceptive reviews andtruthful ones and achieves about 95% ac-curacy on review spam datasets, outper-forming existing baselines by a large mar-gin.1 IntroductionConsumers rely increasingly on user-generatedonline reviews to make purchase decisions.
Pos-itive opinions can result in significant financialgains.
This gives rise to deceptive opinion spam(Ott et al, 2011; Jindal et al, 2008), fake reviewswritten to sound authentic and deliberately mis-lead readers.
Previous research has shown thathumans have difficulty distinguishing fake fromtruthful reviews, operating for the most part atchance (Ott et al, 2011).
Consider, for example,the following two hotel reviews.
One is truthfuland the other is deceptive1:1.
My husband and I stayed for two nights at the HiltonChicago.
We were very pleased with the accommoda-tions and enjoyed the service every minute of it!
Thebedrooms are immaculate, and the linens are very soft.We also appreciated the free wifi, as we could stay intouch with friends while staying in Chicago.
The bath-room was quite spacious, and I loved the smell of theshampoo they provided.
Their service was amazing,1The first example is a deceptive review.and we absolutely loved the beautiful indoor pool.
Iwould recommend staying here to anyone.2.
We stayed at the Sheraton by Navy Pier the first week-end of November.
The view from both rooms was spec-tacular (as you can tell from the picture attached).
Theyalso left a plate of cookies and treats in the kids roomupon check-in made us all feel very special.
The hotelis central to both Navy Pier and Michigan Ave. so wewalked, trolleyed, and cabbed all around the area.
Weate the breakfast buffet on both mornings and thoughtit was pretty good.
The eggs were a little runny.
Oursix year old ate free and our two eleven year old were$14 (instead of the adult $20).
The rooms were clean,the concierge and reception staff were both friendlyand helpful...we will definitely visit this Sheraton againwhen we stay in Chicago next time.Because of the difficulty of recognizing deceptiveopinions, there has been a widespread and growinginterest in developing automatic, usually learning-based methods to help users identify deceptive re-views (Ott et al, 2011; Jindal et al, 2008; Jindalet al, 2010; Li et al, 2011; Lim et al, 2011; Wanget al, 2011).The state-of-the-art approach treats the task ofspam detection as a text categorization prob-lem and was first introduced by Jindal and Liu(2009) who trained a supervised classifier to dis-tinguish duplicated reviews (assumed deceptive)from original ones (assumed truthful).
Since then,many supervised approaches have been proposedfor spam detection.
Ott et al (2011) employedstandard word and part-of-speech (POS) n-gramfeatures for supervised learning and built a gold?standard opinion dataset of 800 reviews.
Lim etal.
(2010) proposed the inclusion of user behavior-based features and found that behavior abnormali-ties of reviewers could predict spammers, withoutusing any textual features.
Li et al (2011) care-fully explored review-related features based oncontent and sentiment, training a semi-supervisedclassifier for opinion spam detection.
However,the disadvantages of standard supervised learningmethods are obvious.
First, they do not gener-ally provide readers with a clear probabilistic pre-217diction of how likely a review is to be deceptivevs.
truthful.
Furthermore, identifying features thatprovide direct evidence against deceptive reviewsis always a hard problem.LDA topic models (Blei et al, 2003) havewidely been used for their ability to model latenttopics in document collection.
In LDA, each docu-ment is presented as a mixture distribution of top-ics and each topic is presented as a mixture distri-bution of words.
Researchers also integrated dif-ferent levels of information into LDA topic mod-els to model the specific knowledge that they areinterested in, such as user-specific information(Rosen-zvi et al, 2006), document-specific infor-mation (Li et al, 2010) and time-specific infor-mation (Diao et al, 2012).
Ramage et al (2009)developed a Labeled LDA model to define a one-to-one correspondence between LDA latent topicsand tags.
Chemudugunta et al (2008) illustratedthat by considering background information anddocument-specific information, we can largely im-prove the performance of topic modeling.In this paper, we propose a Bayesian approachcalled TopicSpam for deceptive review detection.Our approach, which is a variation of LatentDirichlet Allocation (LDA) (Blei et al, 2003),aims to detect the subtle differences between thetopic-word distributions of deceptive reviews vs.truthful ones.
In addition, our model can givea clear probabilistic prediction on how likely areview should be treated as deceptive or truth-ful.
Performance is tested on dataset from Ott etal.
(2011) that contains 800 reviews of 20 Chicagohotels.
Our model achieves more than 94% accu-racy on that dataset.2 TopicSpamWe are presented with four subsets of ho-tel reviews, M = {Mi}i=4i=1, representingdeceptive train, truthful train, deceptive testand truthful test data, respectively.
Each re-view r is comprised of a number of words r ={wt}t=nrt=1 .
Input for the TopicSpam algorithm isthe datasets M ; output is the label (deceptive,truthful) for each review inM3 andM4.
V denotesvocabulary size.2.1 Details of TopicSpamIn TopicSpam, each document is modeled as abag of words, which are assumed to be gener-ated from a mixture of latent topics.
Each wordis associated with a latent variable that specifiesFigure 1: Graphical Model for TopicSpamthe topic from which it is generated.
Words in adocument are assumed to be conditionally inde-pendent given the hidden topics.
A general back-ground distribution ?B and hotel-specific distri-butions ?Hj (j = 1, ..., 20) are first introducedto capture the background information and hotel-specific information.
To capture the differencebetween deceptive reviews and truthful reviews,TopicSpam also learns a deceptive topic distribu-tion ?D and truthful topic distribution ?T .
Thegenerative model of TopicSpam is shown as fol-lows:?
For a training review in r1j ?
M1, words areoriginated from one of the three different top-ics: ?B , ?Hj and ?D.?
For a training review in r2j ?
M2, words areoriginated from one of the three different top-ics: ?B , ?Hj and ?T .?
For a test review in rmj ?
Mm,m = 3, 4,words are originated from one of the four dif-ferent topics: ?B , ?Hj ?D and ?T .The generation process of TopicSpam is shownin Figure 1 and the corresponding graphicalmodel is illustrated in Figure 2.
We use?
= (?G, ?Hi , ?D, ?T ) to represent the asym-metric priors for topic-word distribution genera-tion.
In our experiments, we set ?G = 0.1,and ?Hi = ?D = ?T = 0.01.
The intu-ition for the asymmetric priors is that there shouldbe more words assigned to the background topic.?
= [?B, ?Hi , ?D, ?T ] denotes the priors forthe document-level topic distribution in the LDAmodel.
We set ?B = 2 and ?T = ?D = ?Hi = 1,reflecting the intuition that more words in eachdocument should cover the background topic.2.2 InferenceWe adopt the collapsed Gibbs sampling strategy toinfer the latent parameters in TopicSpam.
In Gibbs2181.
sample ?G ?
Dir(?G)2. sample ?D ?
Dir(?D)3. sample ?T ?
Dir(?T )4. for each hotel j ?
[1, N ]: sample ?Hj ?
?H5.
for each review rif i=1: sample ?r ?
Dir(?B, ?Hj , ?D)if i=2: sample ?r ?
Dir(?B, ?Hj , ?T )if i=3: sample ?r ?
Dir(?B, ?Hj , ?D, ?T )if i=4: sample ?r ?
Dir(?B, ?Hj , ?D, ?T )for each word w in Rsample z ?
?r sample w ?
?zFigure 2: Generative Model for TopicSpamsampling, for each word w in review r, we needto calculate P (zw|w, z?w, ?, ?)
in each iteration,where z?w denotes the topic assignments for allwords except that of the current word zw.P (zw = m|z?w, i, j, ?, ?
)Nmr + ?m?m?
(Nm?r + ??m)?
Ewm + ?m?Vw?
Ewm + V ?m(1)where Nmr denotes the number of times that topicm appears in current review r and Ewm denotes thenumber of times that word w is assigned to topicm.
After each sampling iteration, the latent pa-rameters can be estimated using the following for-mulas:?mr =Nmr + ?m?m?
(Nm?r + ?m)?
(w)m =Ewm + ?m?w?
Ew?m + V ?m(2)2.3 Labeling the Test DataFor each review r in the test data, let NDr denotethe number of words generated from the decep-tive topic and NTr , the number of words generatedfrom the truthful topic.
The decision for whether areview is deceptive or truthful is made as follows:?
if NDr > NTr , r is deceptive.?
if NDr < NTr , r is truthful.?
if NDr = NTr , it is hard to decide.Let P(D) denote the probability that r is deceptiveand P(T) denote the probability that r is truthful.P (D) = NDrNDr +NTrP (T ) = NTrNDr +NTr(3)3 Experiments3.1 System DescriptionOur experiments are conducted on the datasetfrom Ott et al(2011), which contains reviews ofthe 20 most popular hotels on TripAdvisor in theChicago areas.
There are 20 truthful and 20 decep-tive reviews for each of the chosen hotels (800 re-views total).
Deceptive reviews are gathered usingAmazon Mechanical Turk2.
In our experiments,we adopt the same 5-fold cross-validation strat-egy as in Ott et al, using the same data partitions.Words are stemmed using PorterStemmer3.3.2 BaselinesWe employ a number of techniques as baselines:TopicTD: A topic-modeling approach that onlyconsiders two topics: deceptive and truthful.Words in deceptive train are all generated fromthe deceptive topic and words in truthful trainare generated from the truthful topic.
Test docu-ments are presented with a mixture of the decep-tive and truthful topics.TopicTDB: A topic-modeling approach thatonly considers background, deceptive and truthfulinformation.SVM-Unigram: Using SVMlight(Joachims,1999) to train linear SVM models on unigram fea-tures.SVM-Bigram: Using SVMlight(Joachims,1999) to train linear SVM models on bigram fea-tures.SVM-Unigram-Removal1: In SVM-Unigram-Removal, we first train TopicSpam.
Then wordsgenerated from hotel-specific topics are removed.We use the remaining words as features in SVM-light.SVM-Unigram-Removal2: Same as SVM-Unigram-removal-1 but removing all backgroundwords and hotel-specific words.Experimental results are shown in Table 14.As we can see, the accuracy of TopicSpam is0.948, outperforming TopicTD by 6.4%.
This il-lustrates the effectiveness of modeling backgroundand hotel-specific information for the opinionspam detection problem.
We also see that Top-icSpam slightly outperforms TopicTDB, which2https://www.mturk.com/mturk/.3http://tartarus.org/martin/PorterStemmer/4Reviews with NDr = NTr are regarded as incorrectlyclassified by TopicSpam.219Approach Accuracy T-P T-R T-F D-P D-R D-FTopicSpam 0.948 0.954 0.942 0.944 0.941 0.952 0.946TopicTD 0.888 0.901 0.878 0.889 0.875 0.897 0.886TopicTDB 0.931 0.938 0.926 0.932 0.925 0.937 0.930SVM-Unigram 0.884 0.899 0.865 0.882 0.870 0.903 0.886SVM-Bigram 0.896 0.901 0.890 0.896 0.891 0.903 0.897SVM-Unigram-Removal1 0.895 0.906 0.889 0.898 0.887 0.907 0.898SVM-Unigram-Removal2 0.822 0.852 0.806 0.829 0.793 0.840 0.817Table 1: Performance for different approaches based on nested 5-fold cross-validation experiments.neglects hotel-specific information.
By check-ing the results of Gibbs sampling, we find thatthis is because only a small number of wordsare generated by the hotel-specific topics.
Top-icTD and SVM-Unigram get comparative accu-racy rates.
This can be explained by the factthat both models use unigram frequency as fea-tures for the classifier or topic distribution train-ing.
SVM-Unigram-Removal1 is also slightlybetter than SVM-Unigram.
In SVM-Unigram-removal1, hotel-specific words are removed forclassifier training.
So the first-step LDA modelcan be viewed as a feature selection process for theSVM, giving rise to better results.
We can also seethat the performance of SVM-Unigram-removal2is worse than other baselines.
This can be ex-plained as follows: for example, word ?my?
haslarge probability to be generated from the back-ground topic.
However it can also be generated bydeceptive topic occasionaly but can hardly be gen-erated from the truthful topic.
So the removal ofthese words results in the loss of useful informa-tion, and leads to low accuracy rate.Our topic-modeling approach uses word fre-quency as features and does not involve any fea-ture selection process.
Here we present the re-sults of the sample reviews from Section 1.
Stopwords are labeled in black, background topics (B)in blue, hotel specific topics (H) in orange, de-ceptive topics (D) in red and truthful topic (T) ingreen.1.
My husband and I stayed for two nights at the HiltonChicago.
We were very pleased with the accommoda-tions and enjoyed the service every minute of it!
Thebedrooms are immaculate,and the linens are very soft.We also appreciated the free wifi, as we could stay intouch with friends while staying in Chicago.
The bath-room was quite spacious, and I loved the smell of theshampoo they provided not like most hotel shampoos.Their service was amazing,and we absolutely loved thebeautiful indoor pool.
I would recommend staying hereto anyone.
[B,H,D,T]=[41,6,10,1] p(D)=0.909 P(T)=0.0912.
We stayed at the Sheraton by Navy Pier the first week-end of November.
The view from both rooms was spec-tacular (as you can tell from the picture attached).
Theyalso left a plate of cookies and treats in the kids roomupon check-in made us all feel very special.
The ho-tel is central to both Navy Pier and Michigan Ave. sowe walked, trolleyed, and cabbed all around the area.We ate the breakfast buffet both mornings and thoughtit was pretty good.
The eggs were a little runny.
Oursix year old ate free and our two eleven year old were$14 ( instead of the adult $20) The rooms were clean,the concierge and reception staff were both friendlyand helpful...we will definitely visit this Sheraton againwhen we?re in Chicago next time.
[B,H,D,T]=[80,15,3,18] p(D)=0.143 P(T)=0.857background deceptive truthful Hiltonhotel hotel room Hiltonstay my ) palmerwe chicago ( millenniumroom will but lockwood!
room $ parkChicago very bathroom lobbymy visit location linegreat husband night valetI city walk shampoovery experience park dogOmni Amalfi Sheraton JamesOmni Amalfi tower Jamespool breakfast Sheraton serviceplasma view pool spasundeck floor river barchocolate bathroom lake upgradeindoor cocktail navy primehouserequest morning indoor designpillow wine shower overlooksuitable great kid romanticarea room theater homeTable 2: Top words in different topics from Topic-Spam4 ConclusionIn this paper, we propose a novel topic modelfor deceptive opinion spam detection.
Our modelachieves an accuracy of 94.8%, demonstrating itseffectiveness on the task.5 AcknowledgementsWe thank Myle Ott for his insightful comments and sugges-tions.
This work was supported in part by NSF Grant BCS-0904822, a DARPA Deft grant, and by a gift from Google.220ReferencesDavid Blei, Andrew Ng and Micheal Jordan.
LatentDirichlet alocation.
2003.
In Journal of MachineLearning Research.Carlos Castillo, Debora Donato, Luca Becchetti, PaoloBoldi, Stefano Leonardi Massimo Santini, and Se-bastiano Vigna.
A reference collection for webspam.
In Proceedings of annual international ACMSIGIR conference on Research and development ininformation retrieval, 2006.Chaltanya Chemudugunta, Padhraic Smyth and MarkSteyers.
Modeling General and Specific Aspects ofDocuments with a Probabilistic Topic Model.. InAdvances in Neural Information Processing Systems19: Proceedings of the 2006 Conference.Paul-Alexandru Chirita, Jorg Diederich, and WolfgangNejdl.
MailRank: using ranking for spam detection.In Proceedings of ACM international conference onInformation and knowledge management.
2005.Harris Drucke, Donghui Wu, and Vladimir Vapnik.2002.
Support vector machines for spam categoriza-tion.
In Neural Networks.Qiming Diao, Jing Jiang, Feida Zhu and Ee-Peng Lim.In Proceeding of the 50th Annual Meeting of the As-sociation for Computational Linguistics.
2012Thorsten Joachims.
1999.
Making large-scale supportvector machine learning practical.
In Advances inkernel methods.Jack Jansen.
2010.
Online product research.
In Pew In-ternet and American Life Project Report.Nitin Jindal, and Bing Liu.
Opinion spam and analysis.2008.
In Proceedings of the international conferenceon Web search and web data miningNitin Jindal, Bing Liu, and Ee-Peng Lim.
FindingUnusual Review Patterns Using Unexpected Rules.2010.
In Proceedings of the 19th ACM internationalconference on Information and knowledge manage-mentPranam Kolari, Akshay Java, Tim Finin, Tim Oates andAnupam Joshi.
Detecting Spam Blogs: A MachineLearning Approach.
In Proceedings of Associationfor the Advancement of Artificial Intelligence.
2006.Peng Li, Jing Jiang and Yinglin Wang.
2010.
Gener-ating templates of entity summaries with an entity-aspect model and pattern mining.
In Proceedings ofthe 48th Annual Meeting of the Association for Com-putational Linguistics.Fangtao Li, Minlie Huang, Yi Yang, and Xiaoyan Zhu.Learning to identify review Spam.
2011.
In Proceed-ings of the Twenty-Second international joint confer-ence on Artificial Intelligence.Ee-Peng Lim, Viet-An Nguyen, Nitin Jindal, Bing Liu,and Hady Wirawan Lauw.
Detecting Product Re-view Spammers Using Rating Behavior.
2010.
InProceedings of the 19th ACM international confer-ence on Information and knowledge management.Stephen Litvina, Ronald Goldsmithb and Bing Pana.2008.
Electronic word-of-mouth in hospitalityand tourism management.
Tourism management,29(3):458468.Juan Martinez-Romo and Lourdes Araujo.
Web SpamIdentification Through Language Model Analysis.In AIRWeb.
2009.Arjun Mukherjee, Bing Liu and Natalie Glance.
Spot-ting Fake Reviewer Groups in Consumer Reviews.In Proceedings of the 18th international conferenceon World wide web, 2012.Alexandros Ntoulas, Marc Najork, Mark Manasse andDennis Fetterly.
Detecting Spam Web Pages throughContent Analysis.
In Proceedings of internationalconference on World Wide Web 2006Myle Ott, Yejin Choi, Claire Cardie and Jeffrey Han-cock.
Finding deceptive opinion spam by any stretchof the imagination.
2011.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics: Human Language TechnologiesBo Pang and Lillian Lee.
Opinion mining and senti-ment analysis.
In Found.
Trends Inf.
Retr.Daniel Ramage, David Hall, Ramesh Nallapati andChristopher D. Manning.
Labeled LDA: a super-vised topic model for credit attribution in multi-labeled corpora.
2009.
In Proceedings of the 2009Conference on Empirical Methods in Natural Lan-guage Processing 2009.Michal Rosen-zvi, Thomas Griffith, Mark Steyvers andPadhraic Smyth.
The author-topic model for authorsand documents.
In Proceedings of the 20th confer-ence on Uncertainty in artificial intelligence.Guan Wang, Sihong Xie, Bing Liu and Philip Yu.
Re-view Graph based Online Store Review SpammerDetection.
2011.
In Proceedings of 11th Interna-tional Conference of Data Mining.Baoning Wu, Vinay Goel and Brian Davison.
TopicalTrustRank: using topicality to combat Web spam.In Proceedings of international conference on WorldWide Web 2006 .Kyang Yoo and Ulrike Gretzel.
2009.
Compari-son of Deceptive and Truthful Travel Reviews.InInformation and Communication Technologies inTourism 2009.221
