Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1173?1181,Beijing, August 2010A Character-Based Joint Model for Chinese Word SegmentationKun Wang and Chengqing ZongNational Laboratory of Pattern RecognitionInstitute of Automation, Chinese Academy of Science{kunwang,cqzong}@nlpr.ia.ac.cnKeh-Yih SuBehavior Design CorporationKysu@bdc.com.twAbstractThe character-based tagging approachis a dominant technique for Chineseword segmentation, and both discrimi-native and generative models can beadopted in that framework.
However,generative and discriminative charac-ter-based approaches are significantlydifferent and complement each other.A simple joint model combining thecharacter-based generative model andthe discriminative one is thus proposedin this paper to take advantage of bothapproaches.
Experiments on the Sec-ond SIGHAN Bakeoff show that thisjoint approach achieves 21% relativeerror reduction over the discriminativemodel and 14% over the generative one.In addition, closed tests also show thatthe proposed joint model outperformsall the existing approaches reported inthe literature and achieves the best F-score in four out of five corpora.1 IntroductionChinese word segmentation (CWS) plays animportant role in most Chinese NLP applica-tions such as machine translation, informationretrieval and question answering.
Many statis-tical methods for CWS have been proposed inthe last two decades, which can be classified aseither word-based or character-based.
Theword-based approach regards the word as thebasic unit, and the desired segmentation resultis the best word sequence found by the searchprocess.
On the other hand, the character-basedapproach treats the word segmentation task asa character tagging problem.
The final segmen-tation result is thus indirectly generated ac-cording to the tag assigned to each associatedcharacter.
Since the vocabulary size of possiblecharacter-tag-pairs is limited, the character-based models can tolerate out-of-vocabulary(OOV) words and have become the dominanttechnique for CWS in recent years.On the other hand, statistical approaches canalso be classified as either adopting a genera-tive model or adopting a discriminative model.The generative model learns the joint probabil-ity of the given input and its associated labelsequence, while the discriminative modellearns the posterior probability directly.
Gen-erative models often do not perform well be-cause they make strong independence assump-tions between features and labels.
However,(Toutanova, 2006) shows that generative mod-els can also achieve very similar or better per-formance than the corresponding discrimina-tive models if they have a structure that avoidsunrealistic independence assumptions.In terms of the above dimensions, methodsfor CWS can be classified as:1) The word-based generative model (Gao etal., 2003; Zhang et al, 2003), which is a well-known approach and has been used in manysuccessful applications;2) The word-based discriminative model(Zhang and Clark, 2007), which generatesword candidates with both word and characterfeatures and is the only word-based model thatadopts the discriminative approach?3) The character-based discriminative model(Xue, 2003; Peng et al, 2004; Tseng et al,2005; Jiang et al, 2008), which has becomethe dominant method as it is robust on OOVwords and is capable of handling a range ofdifferent features, and it has been adopted inmany previous works;11734) The character-based generative model(Wang et al, 2009), which adopts a character-tag-pair-based n-gram model and achievescomparable results with the popular character-based discriminative model.In general, character-based models are muchmore robust on OOV words than word-basedapproaches do, as the vocabulary size of char-acters is a closed set (versus the open set ofthat of words).
Furthermore, among thosecharacter-based approaches, the generativemodel and the discriminative one complementeach other in handling in-vocabulary (IV)words and OOV words.
Therefore, a character-based joint model is proposed to combine them.This proposed joint approach has achievedgood balance between IV word recognitionand OOV word identification.
The experimentsof closed tests on the second SIGHAN Bakeoff(Emerson, 2005) show that the joint modelsignificantly outperforms the baseline modelsof both generative and discriminative ap-proaches.
Moreover, statistical significancetests also show that the joint model is signifi-cantly better than all those state-of-the-art sys-tems reported in the literature and achieves thebest F-score in four of the five corpora tested.2 Character-Based Models for CWSThe goal of CWS is to find the correspondingword sequence for a given character sequence.Character-based model is to find out the corre-sponding tags for given character sequence.2.1 Character-Based Discriminative ModelThe character-based discriminative model(Xue, 2003) treats segmentation as a taggingproblem, which assigns a corresponding tag toeach character.
The model is formulated as:11 1 1 1 21 1( ) ( , ) (n nn n k n kk kk kP t c P t t c P t c?
?= == ??
?
2 )k +        (1)Where tk is a member of {Begin, Middle, End,Single} (abbreviated as B, M, E and S fromnow on) to indicate the corresponding positionof character ck in its associated word.
For ex-ample, the word ????
(Beijing City)?
willbe assigned with the corresponding tags as: ?
?/B (North) ?/M (Capital) ?/E (City)?.Since this tagging approach treats charactersas basic units, the vocabulary size of thosepossible character-tag-pairs is limited.
There-fore, this method is robust to OOV words andcould possess a high recall of OOV words(ROOV).
Although the dependency between ad-jacent tags/labels can be addressed, the de-pendency between adjacent characters within aword cannot be directly modeled under thisframework.
Lower recall of IV words (RIV) isthus usually accompanied (Wang et al, 2009).In this work, the character-based discrimina-tive model is implemented by adopting the fea-ture templates given by (Ng and Low, 2004),but excluding those ones that are forbidden bythe closed test regulation of SIGHAN (e.g.,Pu(C0): whether C0 is a punctuation).
Thosefeature templates adopted are listed below:11 1( ) ( 2, 1,0,1, 2);( ) ( 2, 1,0,1);( )nn na C nb C C nc C C+?= ?
?= ?
?For example, when we consider the thirdcharacter ???
in the sequence ??????
?,template (a) results in the features as following:C-2=?, C-1=?, C0=?, C1=?, C2=?, and tem-plate (b) generates the features as: C-2C-1=??,C-1C0=?
?, C0C1=?
?, C1C2=?
?, and tem-plate (c) gives the feature C-1C1=?
?.2.2 Character-Based Generative ModelTo incorporate the dependency between adja-cent characters in the character-based approach,(Wang et al, 2009) proposes a character-basedgenerative model.
In this approach, word wi isfirst replaced with its corresponding sequenceof [character, tag] (denoted as [c, t]), where tagis the same as that adopted in the above char-acter-based discriminative model.
With thisrepresentation, this model can be expressed as:1 1 1 11 1 1 1( ) ([ , ] )( [ , ] ) ([ , ] ) ( )m n n nn n n nP w c P c t cP c c t P c t P c?= ?
(2)Since 1 1( [ , ] ) 1n nP c c t ?
and  is the same forvarious candidates, only should beconsidered.
It can be further simplified withMarkov Chain assumption as:1( )nP c([ ,P c 1] )nt111([ , ] ) ([ , ] [ , ] ).nni i kiP c t P c t c t ??=??
i                     (3)Compared with the character-based dis-criminative model, this generative model keepsthe capability to handle OOV words because italso regards the character as basic unit.
In ad-dition, the dependency between adjacent1174?
Gold and Discriminative Tag: M Generative Trigram Tag: ETag probability:  B/0.0333 E/0.2236 M/0.7401 S/0.0030FeatureTag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1B -1.4375 0.1572 0.0800 0.2282 0.7709 0.2741 0.0000 0.0000 -0.6718 0.0000E 1.3558 0.1910 0.7229 -1.2696 -0.5970 0.0049 0.0921 0.0000 0.8049 0.0000M 1.1071 -0.5527 -0.3174 2.9422 0.4636 -0.1708 0.0000 0.0000 -0.9700 0.0000S -1.0254 0.2046 -0.4856 -1.9008 -0.6375 0.0000 0.0000 0.0000 0.8368 0.0000?
Gold and Discriminative Tag: E Generative Trigram Tag: STag probability:  B/0.0009 E/0.8138 M/0.0012 S/0.1841FeatureTag C-2 C-1 C0 C1 C2 C-2C-1 C-1C0 C0C1 C1C2 C-1C1B 0.3586 0.4175 0.0000 -0.7207 0.4626 0.0085 0.0000 0.0000 0.0000 0.0000E 0.3666 0.0687 4.5381 2.8300 -0.0846 0.0000 0.0000 -1.0279 0.6127 0.0000M -0.5657 -0.4330 1.8847 0.0000 -0.0918 0.0000 0.0000 0.0000 0.0000 0.0000S -0.1595 -0.0532 2.7360 1.8223 -0.2862 -0.0024 0.0000 1.0494 0.7113 0.0000Table 1: The corresponding lambda weight of features for ?????
in the sentence ?[?]
[?]
[?]
[???][?]
[?]
[?]
[?]?.
In the Feature column and Tag row, the value is the corresponding lambda weight forthe feature and tag under ME framework.
The meanings of those features are explained in Section 2.1.characters is now directly modeled.
This willgive sharper preference when the history ofassignment is given.
Therefore, this approachnot only holds robust IV performance but alsoachieves comparable results with the discrimi-native model.
However, the OOV performanceof this approach is still lower than that of thediscriminative model (see in Table 5), whichwould be discussed in the next section.3 Problems with the Character-BasedGenerative ModelThe character-based generative model canhandle the dependency between adjacent char-acters and thus performs well on IV words.However, this generative trigram model is de-rived under the second order Markov Chainassumption.
Future character context (i.e., C1and C2) is thus not utilized in the model whenthe tag of the current character (i.e., t0) is de-termined.
Nevertheless, the future contextwould help to select the correct tag when theassociated trigram has not been observed in thetraining-set, which is just the case for thoseOOV words.
In contrast, the discriminativeone could get help from the future context inthis case.
The example given in the next para-graph clearly shows the above situation.At the sentence ??
(that) ?
(place) ?
(of) ???
(street sleeper) ?
(only) ?
(have) ?(some)?
(person) (There are only some street sleepersin that place)?
in the CITYU corpus, ?
?/B?/M?/E(street sleeper)?
is observed to be anOOV word, while ??
/B?
/E(sleep on thestreet)?
is an IV word, where the associated tagof each character is given after the slash sym-bol.
The character-based generative modelwrongly splits ?????
into two words ??/B?/E?
and ?
?/S (person)?, as the associatedtrigram for ?????
is not seen in the trainingset.
However, the discriminative model givesthe correct result for ??/M?
and the dominantfeatures come from its future context ???
and???.
Similarly, the future context ???
helpsto give the correct tag to ??/E?.
Table 1 givesthe corresponding lambda feature weights (un-der the Maximum Entropy (ME) (Ratnaparkhi,1998) framework) for ?????
in the dis-criminative model.
It shows that in the columnof ?C1?
below ??
?, the lambda value associ-ated with the correct tag ?M?
is 2.9422, whichis the highest value in that column and is fargreater than that of the wrong tag ?E?
(i.e., -1.2696) assigned by the generative model.Which indicates that the future feature ?C1?
isthe most useful feature for tagging ??
?.The above example shows the character-based generative model fails to handle someOOV words such as ?????
because this ap-proach cannot utilize future context when it isindeed required.
However, the future contextfor the generative model scanning from left toright is just its past context when it scans fromright to left.
It is thus expected that this kind of1175errors will be fixed if we let the model scansfrom both directions, and then combine theirresults.
Unfortunately, it is observed that thesetwo scanning modes share over 90% of theirerrors.
For example, in CITYU corpus, theleft-to-right scan generates 1,958 wrong wordsand the right-to-left scan results 1,947 ones,while 1,795 of them are the same.
Similar be-havior can also be observed on other corpora.To find out what are the problems, 10 errorsthat are similar to ?????
are selected to ex-amine.
Among those errors, only one of themis fixed, and ?????
still cannot be correctlysegmented.
Having analyzed the scores of themodel scanning from both directions, we foundthat the original scores (from left-to-right scan)at the stages ???
and ???
indeed get better ifthe model scans from right-to-left.
However,the score at the stage ???
deteriorates becausethe useful feature ???
(a past non-adjacentcharacter for ???
when scans form right-to-left) still cannot be utilized when the past con-text ????
as a whole is unseen, when the re-lated probabilities are estimated via modifiedKneser-Ney smoothing (Chen and Goodman,1998) technique.Two scanning modes seem not complement-ing each other, which is out of our original ex-pectation.
However, we found that the charac-ter-based generative model and the discrimina-tive one complement each other much morethan the two scanning modes do.
It is observedthat these two approaches share less than 50%of their errors.
For example, in CITYU corpus,the generative approach generates 1,958 wrongwords and the discriminative one results 2,338ones, while only 835 of them are the same.The statistics of the remaining errors re-sulted from the generative model and the dis-criminative model is shown in Table 2.
Asshown in the table, it can be seen that the gen-erative model and the discriminative modelcomplement each other on handling IV wordsand OOV words (In the ?IV Errors?
column,the number of ?G+D-?
is much more than the?G-D+?, while the behavior is reversed in the?OOV Errors?
column).4 Proposed Joint ModelSince the performance of both IV words andOOV words are important for real applications,IV Errors OOV ErrorsG+D- G-D+ G-D- G+D- G-D+ G-D-12,027 4,723 7,481 2,384 6,139 3,975Table 2: Statistics for remaining errors of the char-acter-based generative model and the discriminativeone on the second SIGHAN Bakeoff (?G+D-?
inthe ?IV Errors?
column means that the generativemodel segments the IV words correctly but the dis-criminative one gives wrong results.
The meaningsof other abbreviations are similar with this one.
).we need to combine the strength from bothmodels.
Among various combining methods,log-linear interpolation combination is a sim-ple but effective one (Bishop, 2006).
Therefore,the following character-based joint model isproposed, and a parameter ?
is used to weightthe generative model in a cross-validation set.1222( ) log( ([ , ] [ , ] ))(1 ) log( ( ))kk kkk kScore t P c t c tP t c???
?+?= ?+ ?
?k(4)Where tk indicates the corresponding positionof character ck, and (0.0 1.0)?
??
?
is theweight for the generative model.
Score(tk) willbe used during searching the best sequence.
Itcan be seen that these two models are inte-grated naturally as both are character-based.Generally speaking, if the ?G(or D)+?
has astrong preference on the desired candidate, butthe ?D(or G)-?
has a weak preference on itstop-1 incorrect candidate, then this combiningmethod would correct most ?G+D- (also  G-D+)?
errors.
On the other hand, the advantageof combining two models would vanish if the?G(or D)+?
has a weak preference while the?D(or G)-?
has a strong preference over theirtop-1 candidates.
In our observation, these twomodels meet this requirement quite well.5 Weigh Various Features DifferentlyFor a given observation, intuitively eachfeature should be trained only once under theME framework and its associated weight willbe automatically learned from the training cor-pus.
However, when we repeat the work of(Jiang et al, 2008), which reports to achievethe state-of-art performance in the data-setsthat we adopt, it has been found that some fea-tures (e.g., C0) are unnoticeably trained severaltimes in their model (which are implicitly gen-erated from different feature templates used inthe paper).
For example, the feature C0 actually1176Corpus Abbrev.
Encoding Training Size(Words/Type)Test Size(Words/Type) OOV RateAcademia Sinica (Taipei) AS Unicode/Big5 5.45M/141K 122K/19K 0.046City University of Hong Kong CITYU Unicode/Big5 1.46M/69K 41K/9K 0.074Microsoft Research (Beijing) MSR Unicode/CP936 2.37M/88K 107K/13K 0.026PKU(ucvt.)
Unicode/CP936 1.1M/55K 104K/13K 0.058 Peking UniversityPKU(cvt.)
Unicode/CP936 1.1M/55K 104K/13K 0.035Table 3: Corpus statistics for the second SIGHAN Bakeoffappears twice, which is generated from twodifferent templates Cn (with n=0, generates C0)and [C0Cn] (used in (Jiang et al, 2008), withn=0, generates [C0C0]).
The meanings of fea-tures are illustrated in Section 2.1.
Those re-petitive features also include [C-1C0] and[C0C1], which implicitly appear thrice.
And itis surprising to discover that its better perform-ance is mainly due to this implicit feature repe-tition but the authors do not point out this fact.As all the features adopted in (Jiang et al,2008) possess binary values, if a binary featureis repeated n times, then it should behave like areal-valued feature with its value to be ?n?, atleast in principle.
Inspired by the above dis-covery, accordingly, we convert all the binary-value features into their corresponding real-valued features.
After having transformed bi-nary features into their corresponding real-valued ones, the original discriminative modelis re-trained under the ME framework.This new implementation, which would benamed as the character-based discriminative-plus model, just weights various features dif-ferently before conducting ME training.
Af-terwards, it is further combined with the gen-erative trigram model, and is called the charac-ter-based joint-plus model.6 ExperimentsThe corpora provided by the second SIGHANBakeoff (Emerson, 2005) were used in our ex-periments.
The statistics of those corpora areshown in Table 3.Note that the PKU corpus is a little differentfrom others.
In the training set, Arabic num-bers and English characters are in full-widthform occupying two bytes.
However, in thetesting set, these characters are in half-widthform occupying only one byte.
Most research-ers in the SIGHAN Bakeoff competition per-formed a conversion before segmentation(Xiong et al, 2009).
In this work, we conductthe tests on both unconverted (ucvt.)
case andconverted (cvt.)
case.
After the conversion, theOOV rate of converted corpus is obviouslylower than that of unconverted corpus.To fairly compare the proposed approachwith previous works, we only conduct closedtests1.
The metrics Precision (P), Recall (R),F-score (F) (F=2PR/(P+R)), Recall of OOV(ROOV) and Recall of IV (RIV) are used toevaluate the results.6.1 Character-Based Generative Modeland Discriminative ModelAs shown in (Wang et al, 2009), the character-based generative trigram model significantlyexceeds its related bigram model and performsthe same as its 4-gram model.
Therefore,  SRILanguage Modeling  Toolkit2 (Stolcke, 2002)is used to train the trigram model with modi-fied Kneser-Ney smoothing (Chen and Good-man, 1998).
Afterwards, a beam search de-coder is applied to find out the best sequence.For the character-based discriminativemodel, the ME Package3 given by Zhang Le isused to conduct the experiments.
Training wasdone with Gaussian prior 1.0 and 300, 150 it-erations for AS and other corpora respectively.Table 5 gives the segmentation results of boththe character-based generative model and thediscriminative model.
From the results, it canbe seen that the generative model achievescomparable results with the discriminative oneand they outperform each other on differentcorpus.
However, the generative model ex-ceeds the discriminative one on RIV (0.973 vs.0.956) but loses on ROOV (0.511 vs. 0.680).
Itillustrates that they complement each other.1 According to the second Sighan Bakeoff regulation, theclosed test could only use the training data directly pro-vided.
Any other data or information is forbidden, includ-ing the knowledge of characters set, punctuation set, etc.2 http://www.speech.sri.com/projects/srilm/3 http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html1177Joint model performance on Development sets0.93000.94000.95000.96000.97000.98000.99000.000.100.200.300.400.500.600.700.800.901.00alphaF-scoreASCITYUMSRPKUFigure 1: Development sets performance of Charac-ter-based joint model.Corpus Set Words  OOV Num OOV RateDevelopment 17,243 445 0.026  ASTesting 122,610 5,308/5,311 0.043/0.043Development 17,324 355 0.020 MSRTesting 106,873 2,829/2,833 0.026/0.027Development 12,075 537 0.044 CITYUTesting 40,936 3,028/3,034 0.074/0.074Development 13,576 532 0.039Testing (ucvt.)
104,372 6,006/6,054 0.058/0.058PKUTesting (cvt.)
104,372 3,611/3,661 0.035/0.035Table 4: Corpus statistics for Development sets andTesting sets.
A ?/?
separates the OOV number (orOOV rate) with respect to the original training setsand the new training sets.6.2 Character-Based Joint ModelFor the character-based joint model, a devel-opment set is required to obtain the weight ?for its associated generative model.
A smallportion of each original training corpus is thusextracted as the development set and the re-maining data is regarded as the new training-set, which is used to train two new parameter-sets for both generative and discriminativemodels associated.The last 2,000, 600, 400, and 300 sentencesfor AS, MSR, CITYU, and PKU are extractedfrom the original training corpora as their cor-responding development sets.
The statistics fornew data sets are shown in Table 4.
It can beseen that the variation of the OOV rate couldbe hardly noticed.
The F-scores of the jointmodel, versus different ?
, evaluated on fourdevelopment sets are shown in Figure 1.
It canbe seen that the curves are not sharp but flatnear the top, which indicates that the character-based joint model is not sensitive to the ?value selected.
From those curves, the bestsuitable ?
for AS, CITYU, MSR and PKU arefound to be 0.30, 0.60, 0.60 and 0.60, respec-Corpus Model R P F ROOV RIVtively.
Those alpha values will then be adoptedto conduct the experiments on the testing sets.G 0.958 0.938 0.948 0.518 0.978D 0 0.946 0  0.967.955 .951 0.707D-Plus 0.960 0.948 0.954 0.680 0.973J 0.962 0.950 0.956 0.679 0.975ASJ-Plus 0.963 0.949 0.956 0.652 0.977G 0.951 0.937 0.944 0.609 0.978D 0.941 0.944 0.942 0.708 0.959D-Plus 0.951 0.952 0.952 0.720 0.970J 0.957 0.951 0.954 0.691 0.979CITYUJ-Plus 0.959 0.952 0.956 0.700 0.980G 0.974 0.967 0.970 0.561 0.985D 0.957 0.962 0.960 0.719 0.964D-Plus 0.965 0.967 0.966 0.675 0.973J 0.974 0.971 0.972 0.659 0.983MSRJ-Plus 0.975 0.970 0.972 0.632 0.984G 0.929 0.933 0.931 0.435 0.959D 0.922 0.941 0.932 0.620 0.941D-Plus 0.934 0.949 0.941 0.649 0.951J 0.935 0.946 0.941 0.561 0.958PKU(ucvt.
)J-Plus 0.937 0.947 0.942 0.556 0.960G 0.952 0.951 0.952 0.503 0.968D 0.940 0.951 0.946 0.685 0.949D-Plus 0.949 0.958 0.953 0.674 0.958J 0.954 0.958 0.956 0.616 0.966PKU(cvt.
)J-Plus 0.955 0.958 0.957 0.610 0.967G 0.953 0.946 0.950 0.511 0.973D 0.944 0.950 0.947 0.680 0.956D-Plus 0.952 0.955 0.953 0.676 0.965J 0.957 0.955 0.956 0.633 0.971OverallJ-Plus 0.958 0.955 0.957 0.621 0.973Table 5: ent ebased m n t Gificantly outperforms both the character-baSegmodels oation r sults of various character-he second SI HAN Bakeoff, thegenerative trigram model (G), the discriminativemodel (D), the discriminative-plus model (D-Plus),the joint model (J) and the joint-plus model (J-Plus).As shown in Table 5, the joint model sig-nsed generative model and the discriminativeone in F-score on all the testing corpora.
Com-pared with the generative approach, the jointmodel increases the overall ROOV from 0.510 to0.633, with the cost of slightly degrading theoverall RIV from 0.973 to 0.971.
This showsthat the joint model holds the advantage of thegenerative model on IV words.
Compared withthe discriminative model, the proposed jointmodel improves the overall RIV from 0.956 to0.971, with the cost of degrading the overallROOV from 0.680 to 0.633.
It clearly shows thatthe joint model achieves a good balance be-tween IV words and OOV words and achievesthe best F-scores obtained so far (21% relativeerror reduction over the discriminative modeland 14% over the generative model).11786.3 Weigh Various Features DifferentlyInspired by (Jiang et al, 2008), we set the real-dAlthough Table 5 has shown that the proposedall thevalue of C0 to be 2.0, the value of C-1C0 anC0C1 to be 3.0, and the values of all other fea-tures to be 1.0 for the character-based dis-criminative-plus model.
Although it seems rea-sonable to weight those closely relevant fea-tures more (C0 should be the most relevant fea-ture for assigning tag t0), both implementationsseem to be equal if their correspondinglambda-values are also updated accordingly.However, Table 5 shows that this new dis-criminative-plus implementation (D-Plus) sig-nificantly outperforms the original one (overallF-score is raised from 0.947 to 0.953) whenboth of them adopt real-valued features.
It isnot clear how this change makes the difference.Similar improvements can be observed withtwo other ME packages.
One anonymous re-viewer pointed out that the duplicated featuresshould not make difference if there is no regu-larization.
However, we found that the dupli-cated features would improve the performancewhether we give Gaussian penalty or not.Afterwards, this new implementation andthe generative trigram model are further com-bined (named as the joint-plus model).
Table 5shows that this joint-plus model also achievesbetter results compared with the discrimina-tive-plus model, which illustrates that our jointapproach is an effective and robust method forCWS.
However, compared with the originaljoint model, the new joint-plus approach doesnot show much improvement, regardless of thesignificant improvement made by the discrimi-native-plus model, as the additional benefitgenerated by the discriminative-plus model hasalready covered by the generative approach(Among the 6,965 error words corrected by thediscriminative-plus model, 6,292 (90%) ofthem are covered by the generative model).7 Statistical Significance Testsjoint (joint-plus) model outperformsbaselines mentioned above, we want to knowif the difference is statistically significantenough to make such a claim.
Since there isonly one testing set for each training corpus,the bootstrapping technique (Zhang et al, 2004)is adopted to conduct the tests: Giving anModelsA BAS CITYU MSR PKU (ucvt.)PKU(cvt.
)G D <  ~ >  ~ >D-Plus G >  >  <  >  >D-Plus D >  >  >  >  >J G >  >  >  >  >J D >  >  >  >  >J-Plus G >  >  >  >  >J-Plus D-Plus >  >  >  ~ >J-Plus J ~ >  ~ >  >Table 6 atistic sign anc est F- ev er-b d m ls.f T0) willbe generated by repeatedly re-sampling dataeas-the dis-he confi-the pro-poe-ngd.tegoryincludes (Asahara et al, 2005) (denoted as: St al ific e t of scoramong arious charact ase odetesting-set T0, additional M-1 new testing-setsT0,?,TM-1 (each with the same size ofrom T0.
Then, we will have a total of Mtesting-sets (M=2000 in our experiments).7.1 Comparisons with BaselinesWe then follow (Zhang et al, 2004) to mure the 95% confidence interval forcrepancy between two models.
If tdence interval does not include the origin point,we then claim that system A is significantlydifferent from system B.
Table 6 gives the re-sults of significant tests among various modelsmentioned above.
In this table, ?>?
means thatsystem A is significantly better than B, whereas ?<?
denotes that system A is significantlyworse than B, and ?~?
indicates that these twosystems are not significantly different.As shown in Table 6, the proposed jointmodel is significantly better than the two base-line models on all corpora.
Similarly,sed joint-plus model also significantly out-performs the generative model and the dis-criminative-plus model on all corpora excepton the PKU(ucvt.).
The comparison shows thatthe proposed joint (also joint-plus) model in-deed exceeds each of its component models.7.2 Comparisons with Previous WorksThe above comparison mainly shows the supriority of the proposed joint model amothose approaches that have been implementeHowever, it would be interesting to know if thejoint (and joint-plus) model also outperformsthose previous state-of-the-art systems.The systems that performed best for at leastone corpus in the second SIGHAN Bakeoff arefirst selected for comparison.
This ca1179A-sets.
In-stthsahara05) and (Tseng et al, 2005) 4(Tseng05).
(Asahara et al, 2005) achieves thebest result in the AS corpus, and (Tseng et al,2005) performs best in the remaining threecorpora.
Besides, those systems that are re-ported to exceed the above two systems arealso selected.
This category includes (Zhang etal., 2006) (Zhang06), (Zhang and Clark, 2007)(Z&C07) and (Jiang et al, 2008) (Jiang08).They are briefly summarized as follows.
(Zhang et al, 2006) is based on sub-word tag-ging and uses a confidence measure method tocombine the sub-word CRF (Lafferty et al,2001) and rule-based models.
(Zhang andClark, 2007) uses perceptron (Collins, 2002) togenerate word candidates with both word andcharacter features.
Last, (Jiang et al, 2008)5adds repeated features implicitly based on (Ngand Low, 2004).
All of the above models, ex-cept (Zhang and Clark, 2007), adopt the char-acter-based discriminative approach.All the results of the systems mentionedabove are shown in Table 7.
Since the systemsare not re-implemented, we cannot generatepaired samples from those M testingead, we calculate the 95% confidence inter-val of the joint (also joint-plus) model.
After-wards, those systems can be compared withour proposed models.
If the F-score of systemB does not fall within the 95% confidence in-terval of system A (joint or joint-plus), thenthey are statistically significantly different.Table 8 gives the results of significant testsfor those systems mentioned in this section.
Itshows that both our joint-plus model and jointmodel exceed (or are comparable to) almost alle state-of-the-art systems across all corpora,except (Zhang and Clark, 2007) at PKU(ucvt.
).In that special case, (Zhang and Clark, 2007)4 We are not sure whether (Asahara et al, 2005) and(Tseng et al, 2005) performed a conversion before seg-mentation in PKU corpus.
In this paper, we followedprevious works, which cited and compared with them.5 The data for (Jiang et al, 2008) given at Table 7 aredifferent from what were reported at their paper.
In thecommunication with the authors, it is found that the scriptfor evaluating performance, provided by the SIGHANBakeoff, does not work correctly in their platform.
Afterthe problem is fixed, the re-evaluated real performancesreported here deteriorate from their original version.Please see the announcement in Jiang?s homepage(http://mtgroup.ict.ac.cn/~jiangwenbin/papers/error_correction.pdf).CorpusParticipants AS CITYU MSRPKU(ucvt.)PKU(cvt.
)Asahara05 0.952 0.941 0.958 N/A 0.941Tseng05 0.947 0.943 0.964 N/A 0.950Zhang06 0.951 0.951 0.971 N/A 0.951Z&C07 0.946 0.951 0.972 0.945 N/AJiang08 0.953 0.948 0.966 0.937 N/AOur Joint 0.956 0.954 0.972 0.941 0.956Our Joint-Plus 0.956 0.956 0.972 0.942 0.957Table 7: Compari r  p uthe-art sysons of F-sco e with revio sstate-of- stems.SystemsA BAS CITYU MSR (ucvt.)PKU(cvt.
)PKUAsahara05 > > > N/A >Tseng05 > > > N/A >Zhang06 > ~ ~ N/A >Z&C07 > > ~ < N/AJJiang08 > > > > N/AAsahara05 > > > N/A >Tseng05 > > > N/A >Zhang06 > > ~ N/A >Z&C07 > > ~ < N/AJ-PlusJiang08 ~ > > > N/ATable al s ific e te of rf-the  syst s.outpe he jo -plu model by .3%and 0.5%, re-ne,e two models complementdling IV words and OOVe-nomenon.8: Statistic ign anc st  F-score foprevious state-o -art emrforms t int s  0  onF- score (0.4% for the joint model).
However,our joint-plus model exceeds it more over ASand CITYU corpora by 1.0%spectively (1.0% and 0.3% for the joint model).Thus, it is fair to say that both our joint modeland joint-plus model are superior to the state-of-the-art systems reported in the literature.8 ConclusionFrom the error analysis of the character-basedgenerative model and the discriminative owe found that theseach other on hanwords.
To take advantage of these two ap-proaches, a joint model is thus proposed tocombine them.
Experiments on the SecondSIGHAN Bakeoff show that the joint modelachieves 21% error reduction over the dis-criminative model (14% over the generativemodel).
Moreover, closed tests on the secondSIGHAN Bakeoff corpora show that this jointmodel significantly outperforms all the state-of-the-art systems reported in the literature.Last, it is found that weighting various fea-tures differently would give better result.
How-ever, further study is required to find out thetrue reason for this strange but interesting ph1180A Generic-Beam-Search code ando Ms. Nanyan Kuo foreric-Beam-Search code.moptimumentation.
In Proceedings ofGHAN Workshop on Chinese Lan-StThJiaWJoHwFuction using conditional random fields.AdMNLP, pagesHuld Word SegmenterKu d Keh-Yih Su, 2009.YiscriminativeNissing, 8 (1).
pagesHuSecondRu2006.
Subword-based Tagging for Con-Yiscores: How much im-Yuf ACL, pages 840-847,cknowledgementThe authors extend sincere thanks to WenbingJiang for his helps with our experiments.
Also,we thank Behavior Design Corporation forusing theirshow special thanks ther helps with the GenThe research work has been partially fundedby the Natural Science Foundation of Chinaunder Grant No.
60975053, 90820303 and60736014, the National Key Technology R&DProgram under Grant No.
2006BAH03B02,and also the Hi-Tech Research and Develop-ent Program (?863?
Program) of China underGrant No.
2006AA010108-4 as well.ReferencesMasayuki Asahara, Kenta Fukuoka, Ai Azuma,Chooi-Ling Goh, Yotaro Watanabe, Yuji Ma-tsumoto and Takashi Tsuzuki, 2005.
Combina-tion of machine learning methods forChinese word segmthe Fourth SIguage Processing, pages 134?137, Jeju, Korea.Christopher M. Bishop, 2006.
Pattern recognitionand machine learning.
New York: Springeranley F. Chen and Joshua Goodman, 1998.
Anempirical study of smoothing techniques for lan-guage modeling.
Technical Report TR-10-98,Harvard University Center for Research inComputing Technology.Michael Collins, 2002.
Discriminative trainingmethods for hidden markov models: theory andexperiments with perceptron algorithms.
In Pro-ceedings of EMNLP, pages 1-8, Philadelphia.omas Emerson, 2005.
The second internationalChinese word segmentation bakeoff.
In Proceed-ings of the Fourth SIGHAN Workshop on Chi-nese Language Processing, pages 123-133.nfeng Gao, Mu Li and Chang-Ning Huang, 2003.Improved Source-Channel Models for ChineseWord Segmentation.
In Proceedings of ACL,pages 272-279.enbin Jiang, Liang Huang, Qun Liu and YajuanLu, 2008.
A Cascaded Linear Model for JointChinese Word Segmentation and Part-of-SpeechTagging.
In Proceedings of ACL, pages 897-904.hn Lafferty, Andrew McCallum and FernandoPereira, 2001.
Conditional Random Fields: Prob-abilistic Models for Segmenting and LabelingSequence Data.
In Proceedings of ICML, pages282-289.ee Tou Ng and Jin Kiat Low, 2004.
Chinesepart-of-speech tagging: one-at-a-time or all-at-once?
word-based or character-based.
In Pro-ceedings of EMNLP, pages 277-284.chun Peng, Fangfang Feng and AndrewMcCallum, 2004.
Chinese segmentation and newword deteIn Proceedings of COLING, pages 562?568.wait Ratnaparkhi, 1998.
Maximum entropymodels for natural language ambiguity resolu-tion.
University of Pennsylvania.Andreas Stolcke, 2002.
SRILM-an extensible lan-guage modeling toolkit.
In Proceedings of theInternational Conference on Spoken LanguageProcessing, pages 311-318.Kristina Toutanova, 2006.
Competitive generativemodels with structure learning for NLP classifi-cation tasks.
In Proceedings of E576-584, Sydney, Australia.ihsin Tseng, Pichuan Chang, Galen Andrew,Daniel Jurafsky and Christopher Manning, 2005.A Conditional Random Fiefor Sighan Bakeoff 2005.
In Proceedings of theFourth SIGHAN Workshop on Chinese Lan-guage Processing, pages 168-171.n Wang, Chengqing Zong anWhich is more suitable for Chinese word seg-mentation, the generative model or the discrimi-native one?
In Proceedings of PACLIC, pages827-834, Hong Kong, China.ng Xiong, Jie Zhu, Hao Huang and Haihua Xu,2009.
Minimum tag error for ditraining of conditional random fields.
Informa-tion Sciences, 179 (1-2).
pages 169-179.anwen Xue, 2003.
Chinese Word Segmentationas Character Tagging.
Computational Linguisticsand Chinese Language Proce29-48.aping Zhang, Hongkui Yu, Deyi Xiong and QunLiu, 2003.
HHMM-based Chinese lexical ana-lyzer ICTCLAS.
In Proceedings of theSIGHAN Workshop on Chinese Language Proc-essing, pages 184?187.iqiang Zhang, Genichiro Kikui and EiichiroSumita,fidence-dependent Chinese Word Segmentation.In Proceedings of the COLING/ACL, pages 961-968, Sydney, Australia.ng Zhang, Stephan Vogel and Alex Waibel, 2004.Interpreting BLEU/NISTprovement do we need to have a better system.In Proceedings of LREC, pages 2051?2054.e Zhang and Stephen Clark, 2007.
Chinese Seg-mentation with a Word-Based Perceptron Algo-rithm.
In Proceedings oPrague, Czech Republic.1181
