Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 1918?1927, Dublin, Ireland, August 23-29 2014.Class-Based Language Modeling forTranslating into Morphologically Rich LanguagesArianna Bisazza and Christof MonzInformatics Institute, University of AmsterdamScience Park 904, 1098 XH Amsterdam, The Netherlands{a.bisazza,c.monz}@uva.nlAbstractClass-based language modeling (LM) is a long-studied and effective approach to overcome datasparsity in the context of n-gram model training.
In statistical machine translation (SMT), differ-ent forms of class-based LMs have been shown to improve baseline translation quality when usedin combination with standard word-level LMs but no published work has systematically com-pared different kinds of classes, model forms and LM combination methods in a unified SMTsetting.
This paper aims to fill these gaps by focusing on the challenging problem of translatinginto Russian, a language with rich inflectional morphology and complex agreement phenomena.We conduct our evaluation in a large-data scenario and report statistically significant BLEU im-provements of up to 0.6 points when using a refined variant of the class-based model originallyproposed by Brown et al.
(1992).1 IntroductionClass-based n-gram modeling is an effective approach to overcome data sparsity in language model (LM)training.
By grouping words with similar distributional behavior into equivalence classes, class-basedLMs have less parameters to train and can make predictions based on longer histories.
This makes themparticularly attractive in situations where n-gram coverage is low due to shortage of training data or tospecific properties of the language at hand.While translation into English has drawn most of the research effort in statistical machine translation(SMT) so far, there is now a growing interest in translating into languages that are more challengingfor standard n-gram modeling techniques.
Notably, morphologically rich languages are characterized byhigh type/token ratios (T/T) that reflect in high out-of-vocabulary word rates and frequent backing-off tolow order n-gram estimates, even when large amounts of training data are used.
These problems havebeen long studied in the field of speech recognition but much less in SMT, although the target LM is acore component of all state-of-the-art SMT frameworks.Partly inspired by successful research in the field of speech recognition, various forms of class-basedLMs have been shown to improve the quality of SMT when used in combination with standard word-level LMs.
These approaches, however, have mostly focused on English (Uszkoreit and Brants, 2008;Dyer et al., 2011; Monz, 2011; Hassan et al., 2007; Birch et al., 2007) with only recent exceptions(Green and DeNero, 2012; Ammar et al., 2013; Wuebker et al., 2013; Durrani et al., 2014).
Moreover,there is no published work that systematically evaluates different kinds of classes, model forms and LMcombination methods in a unified SMT setting.
On the contrary, most of the existing literature on LMcombination uses mixtures of multiple word-level LMs for domain adaptation purposes.This paper aims to fill these gaps by applying various class-based LM techniques to the challengingproblem of translating into a morphologically rich language.
In particular we focus on English-Russian,a language pair for which a fair amount of both parallel data and monolingual data has been provided bythe Workshop of Machine Translation (Bojar et al., 2013).
Russian is characterized by a rich inflectionalmorphology, with a particularly complex nominal declension (six core cases, three genders and twoThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1918number categories).
This results in complex agreement phenomena and an extremely rich vocabulary.Indeed, by examining our training data (see Section 4), we find the Russian T/T ratio to be almost twotimes higher than the English one.Given this task, we make a number of contributions leading to a better understanding of ways to utilizeclass-based language models for translating into morphologically rich languages.
We conduct a compar-ative evaluation of different target LMs along the following axes: (1) Classes: data-driven versus shallowmorphology-based; (2) Model forms: simple class sequence (stream-based) versus original class-based(Brown et al., 1992); and (3) Combination frameworks: model-level log-linear combination versus word-level linear interpolation.
When comparing the different model forms we pay particular attention to therole word emission probabilities play in class-based models, which turns out to be a significant factorfor translating into morphologically rich languages.
In this context we also evaluate for the first time aspecific form of class-based LM called fullibm (Goodman, 2001) within statistical MT.2 Class-based language modelsAs introduced by (Brown et al., 1992), the idea of class-based n-gram language modeling is to groupwords with similar distributional behavior into equivalence classes.
The word transition probability isthen decomposed into a class transition probability and a word emission probability:Pclass(wi|wi?1i?n+1) = p0(C(wi)|C(wi?1i?n+1)) ?
p1(wi|C(wi)) (1)This results in models that are more compact and more robust to data sparsity.
Often, in the context ofSMT, the word emission probability is dropped and only the class sequence is modeled.
In this work, werefer to this model form as stream-based n-gram LM:1Pstream(wi|wi?1i?n+1) = p0(C(wi)|C(wi?1i?n+1)) (2)Stream-based LMs are used, for instance, in factored SMT (Koehn et al., 2007), and in general manyof the ?class-based LMs?
mentioned in the SMT literature are actually of the latter form (2) (Dyer etal., 2011; Green and DeNero, 2012; Ammar et al., 2013; Chahuneau et al., 2013; Wuebker et al., 2013;Durrani et al., 2014).
One exception is the work of Uszkoreit and Brants (2008), who incorporate wordemission probabilities in their class-based LM used as an additional feature function in the log-linearcombination (cf.
Section 3.1).
Interestingly, we are not aware of work that compares actual class-basedLMs and stream-based LMs with respect to SMT quality.While class-based LMs are known to be effective at counteracting data sparsity issues due to richvocabularies, it is worth noting that they adhere to the fundamental constraints of n-gram modeling.Thus, grammatical agreement may be improved by a class-based LM approach only within a limitedcontext window.
Previous work that attempted to overcome this limitation includes (i) syntactic LMs forn-best reranking (Hasan et al., 2006; Carter and Monz, 2011) or integrated into decoding with significantengineering challenges (Galley and Manning, 2009; Schwartz et al., 2011) and (ii) unification-basedconstraints applied to a syntax-based SMT framework (Williams and Koehn, 2011).We will now describe different kinds of word-to-class mapping functions used by class-based LMs.These can be completely data-driven or based on different sorts of linguistic or orthographic features.2.1 Data-driven classesThe most popular form of class-based LMs was introduced by (Brown et al., 1992).
In this approach, thecorpus vocabulary is partitioned into a preset number of clusters by directly maximizing the likelihoodof a training corpus.
No linguistic or orthographic features are taken into account while training theclasses.2Later work has focused on decreasing the large computational cost of the exchange algorithmproposed by Brown et al.
(1992), either with a distributed algorithm (Uszkoreit and Brants, 2008) or byusing a whole-context distributional vector space model (Sch?utze and Walsh, 2011).
In this paper we usethe standard SRILM implementation of Brown clustering.1Not to be confused with the incrementally trainable stream-based LMs of Levenberg and Osborne (2009).2Och (1999) extends a similar approach to bilingual clustering with the aim of generalizing the applicability of translationrules in an alignment template SMT framework.19192.2 Linguistic classesLinguistic knowledge is another way to establish word equivalence classes.
Common examples includelemma, part of speech and morphology-based classes, each of which can capture different aspects ofthe word sequence, such as the relative order of syntactic constituents or grammatical agreement.
Has-san et al.
(2007) and Birch et al.
(2007) went as far as scoring n-grams of Combinatorial CategorialGrammar supertags.
When using linguistic classes, one has to deal with the fact that the same word canbelong to different classes when used in different contexts.
Solutions to this problem include taggingthe target word sequence as it is generated (Koehn et al., 2007; Birch et al., 2007; Green and DeNero,2012), choosing the most probable class sequence for each phrase pair (Monz, 2011) or?even morelightweight?choosing the most probable class for each word (Bisazza and Federico, 2012).Alternatively, simpler deterministic class mappings can be derived by using shallow linguistic knowl-edge, such as suffixes or orthographic features.
The former can be obtained with a rule-based stemmer(as in this work), or, even more simply, by selecting the ?
most common word suffixes in a trainingcorpus and then mapping each word to its longest matching suffix (M?uller et al., 2012).
Orthographicfeatures may include capitalization information or the presence of digits, punctuation or other specialcharacters (M?uller et al., 2012).2.3 Hybrid surface/class modelsM?uller et al.
(2012) obtain the best perplexity reduction when excluding frequent words from the classmapping.
That is, each word with more than ?
occurrences in the training corpus is assigned to a singletonclass with word emission probability equal to 1.
The frequency threshold ?
is determined with a gridsearch on a monolingual held-out set.
Optimal values for perplexities are shown to vary considerablyamong languages.
In this work we follow this setup closely.It is worth noting that Bisazza and Federico (2012) have applied a similar idea to the problem ofstyle adaptation: they train a hybrid POS/word n-gram LM on an in-domain corpus and use it as anadditional SMT feature function with the goal of counterbalancing the bias towards the style of the largeout-of-domain data.
The idea of modeling sequences of mixed granularity (word/subword) was earlierintroduced to speech recognition by Yazgan and Sarac?lar (2004).The most extensive comparison of distributional, morphological and hybrid classes that we are awareof is the work by M?uller et al.
(2012), but that does not include any SMT evaluation.
Looking at perplex-ity results over a large number of European language pairs (not including Russian), M?uller et al.
(2012)conclude that a hybrid suffix/word class-based LM simply built on frequency-based suffixes performs aswell as a model trained on much more expensive distributional classes.
Motivated by this finding, weevaluate these two kinds of classes in the context of SMT into a morphologically rich language.2.4 Fullibm language modelAs outlined above, the class-based LMs generally used in SMT are in fact stream-based models in thesense that they only estimate the probability of the class sequence (see Equation 2).
However, the clas-sic form of class-based LM (Brown et al., 1992) also includes a class-to-word emission probabilityp1(wi|C(wi)) whose utility has not been properly assessed in the context of SMT.Besides, we observe that a variety of class-based LM variants have been studied in speech recognitionbut not in SMT.
In particular, Goodman (2001) presents a generalization of the standard class-based formwhere the word emission is also conditioned on the class history rather than on the current class alone.The resulting model is called fullibm:Pfullibm(wi|wi?1i?n+1) = p0(C(wi)|C(wi?1i?n+1)) ?
p1(wi|C(wii?n+1)) (3)We expect this model to yield more refined, context-sensitive word emission distributions which mayresult in better target LM probabilities for our SMT system.19203 SMT combining frameworkClass-based LMs are rarely used in isolation, but are rather combined with standard word-level models.There exist at least two ways to combine multiple LMs into a log-linear SMT decoder: (i) as separatefeature functions in the global log-linear combination or (ii) as components of a linear mixture countingas a single feature function in the global combination.3.1 Log-linear combinationThe standard log-linear approach to SMT allows for the combination of m arbitrary model components(or feature functions), each weighted by a corresponding weight ?m:p(x|h) =?mpm(x|h)?m(4)In typical SMT settings, pm(x|h) are phrase- or word-level translation probabilities, reordering prob-abilities, and so on.
Treating the new LM as an additional feature function has the advantage that itsweight can be directly optimized for SMT quality together with all other feature weights, using standardparameter tuning techniques (Och, 2003; Hopkins and May, 2011).3.2 Linear interpolationThe other widely used combining framework is linear interpolation or mixture model:p(x|h) =?q?qpq(x|h) (5)More specifically, word LMs are usually interpolated as a word-level weighted average of the n-gramprobabilities:pmixLM(e) =n?i=1(?q?qpq(ei|hi))(6)The drawback of this approach is that the linear interpolation weights, or lambdas, cannot be set withstandard SMT tuning techniques.
Instead, interpolation weights are typically determined by maximizingthe likelihood of a held-out monolingual data set, but this does not always outperform simple uniformweighting in terms of translation quality.3Despite the lambda optimization issue, linear interpolation with uniform or maximum-likelihoodweights has been shown to work better for SMT than log-linear combination when combining regu-lar word n-gram LMs (Foster and Kuhn, 2007).
However, to the best of our knowledge, the linearinterpolation of word- and class-based LMs has never been tested in SMT.In their intrinsic evaluation, M?uller et al.
(2012) show that linear mixing with hybrid class/surfacemodels of various kinds consistently decrease the perplexity of a Kneser-Ney smoothed word-level LM,with relative improvements ranging between 3% (English) and 11% (Finnish).
All their models areinterpolated with class-specific lambda weights, according to the following formula:Pmix(wi|wi?1i?n+1) = ?C(wi?1)?
Pclass(wi|wi?1i?n+1) + (1?
?C(wi?1)) ?
Pword(wi|wi?1i?n+1) (7)where Pwordcorresponds to the standard n-gram model using the lexical forms.
Equation 7 can be seenas a generalization of the simple interpolation ?Pclass+ (1 ?
?
)Pwordused by Brown et al.
(1992).The class-specific lambdas are estimated by a deleted interpolation algorithm (Bahl et al., 1991).
In ourexperiments, we test both generic and class-specific lambda interpolation for SMT.3Foster and Kuhn (2007) also tried more sophisticated techniques to set interpolation weights but did not obtain significantimprovements.1921Corpus Lang.
#Sent.
#Tok.
T/Tparal.trainEN1.9M48.9M .0107RU 45.9M .0204Wiki dict.EN/RU 508K ?
?mono.trainRU 21.0M 390M .0068newstest12EN3K 64K ?newstest133K 56K ?Table 1: Training and test data statistics: number of sentences, number of tokens and type/token ratio(T/T).
All numbers refer to tokenized, lowercased data.4 EvaluationWe perform a series of experiments to compare the effectiveness for SMT of various class mappingfunctions, different model forms, and different LM combining frameworks.The task, organized by the Workshop of Machine Translation (WMT, Bojar et al.
(2013)), consistsof translating a set of news stories from English to Russian.
As shown in Table 1, the available dataincludes a fairly large parallel training corpus (1.9M sentences) from various sources, a set of Wikipediaparallel headlines shared by CMU,4and a larger monolingual corpus for model training (21M sentences).By measuring the type/token ratios of the two sides of a parallel corpus, we can estimate the differencein morphological complexity between two languages: as shown in Table 1, the Russian T/T is almosttwo times higher than the English one (.0204 vs .0107) in the WMT13 parallel training data.
As isusually the case, much more data is available for LM training.
Nevertheless we report a rather highout-of-vocabulary word rate on the devsets?
reference translations (2.28%).4.1 BaselineOur baseline is an in-house phrase-based (Koehn et al., 2003) statistical machine translation system verysimilar to Moses (Koehn et al., 2007).
All system runs use hierarchical lexicalized reordering (Gal-ley and Manning, 2008; Cherry et al., 2012), distinguishing between monotone, swap, and discontinuousreordering, all with respect to left-to-right and right-to-left decoding.
Other features include linear distor-tion, bidirectional lexical weighting (Koehn et al., 2003), word and phrase penalties, and finally a word-level 5-gram target language model trained on all available monolingual data with modified Kneser-Neysmoothing (Chen and Goodman, 1999).
The distortion limit is set to 6 and for each source phrase the top30 translation candidates are considered.The feature weights for all approaches were tuned by using pairwise ranking optimization (Hopkinsand May, 2011) on newstest12.
During tuning, 14 PRO parameter estimation runs are performed in paral-lel on different samples of the n-best list after each decoder iteration.
The weights of the individual PROruns are then averaged and passed on to the next decoding iteration.
Performing weight estimation inde-pendently for a number of samples corrects for some of the instability that can be caused by individualsamples.4.2 Language modelsThe additional LMs are trained with Witten-Bell smoothing (Witten and Bell, 1991), which is a commonchoice for class-based LM training as Kneser-Ney smoothing cannot be used for computing discountfactors when the count-of-counts are zero.
The main series of experiments employ 5-gram models, butwe also evaluate the usefulness of increasing the order to 7-gram (see Table 3).5Data-driven clusters are learned with the standard Brown clustering algorithm, which greedily maxi-mizes the log likelihood of a class bigram model on the training data.
Following Ammar et al.
(2013),we set the number of data-driven clusters to 600.
In preliminary experiments we also tested a 256-clustersetting, but 600 yielded better BLEU scores.
For time reasons, we train the clusters on a subset of the4http://www.statmt.org/wmt13/wiki-titles.ru-en.tar.gz5For this second series of experiments we use the feature weights tuned for the corresponding 5-gram LMs.1922LM type smoothing vocab.
PPwords Kneser-Ney 2.7M 270Brown clusters Witten-Bell 600 588suffixes Witten-Bell 968 2455suffix/word hybrid (?=5000) Witten-Bell 8530 460Linear interp.PPgeneric ?
class-spec.?
?swords + clusters 225 224words + suffixes 266 265words + hybrid 243 247Table 2: Intrinsic evaluation of various types of LMs and their linear interpolations.
Perplexity (PP) iscomputed on a separate held-out set of 5K Russian sentences.
All models are 5-grams.monolingual data including all the parallel data (news commentary) and the large commoncrawl corpusfor a total of 1M sentences (22M tokens).
We then map all monolingual data to the learned clusters anduse that to train all our cluster-based LMs.For the suffix-based class LMs we closely follow the setup of M?uller et al.
(2012) with the onlydifference that we use the Russian Snowball stemmer6to segment the vocabulary instead of frequency-based suffixes.
The suffix threshold ?
(see Section 2.3) is determined by minimizing perplexity on aseparate held-out set (5K sentences): ?=5000 is the optimal setting among {2000, 5000, 10000, 20000}.7The same held-out set is used to estimate both the generic and the class-specific lambdas for the linearinterpolation experiments.Table 2 presents an overview of the LMs used in our experiments.
We can see on the left side thatall class-based LMs have notably higher perplexities compared to the word-level, with the fully suffix-based LM performing worst by far.
Nevertheless, all class-based models yield a decrease in perplexitywhen they are interpolated with the word-level model (right side).
The best improvement is achievedby the data-driven classes (225 versus 270, that is -17%), but the result of the hybrid LM is also quitesuccessful (-10%) and much in line with the improvements reported by M?uller et al.
(2012) on otherSlavic languages.
Because the fully suffix-based LM yields only a modest reduction, we do not to includeit in the SMT evaluation.
The right side of Table 2 also shows that using class-specific interpolationweights is not significantly better, and sometimes is even worse than using only one generic ?, at leastfrom the point of view of perplexity.
Since weight estimation for linear interpolation is still an openproblem for SMT, we decide nevertheless to compare these two interpolation methods in our translationexperiments (see Table 4).4.3 SMT resultsTable 3 shows the results for English to Russian translation using log-linear combination with Brownclusters and the hybrid suffix/word classes.
Translation quality is measured by case-insensitive BLEU(Papineni et al., 2002) on newstest13 using one reference translation.
The relative improvements of thedifferent class-based LM runs are with respect to the baseline which uses a word-based LM only andachieves comparable results to the state-of-the-art.
We use approximate randomization (Noreen, 1989)to test for statistically significant differences between runs (Riezler and Maxwell, 2005).We can see from Table 2(a) that using a stream-based LM as an additional feature, which is log-linearlyinterpolated with the other decoder features during parameter estimation, leads to small but statisticallysignificant improvements.
The results also indicate that using a higher n-gram class model (7-gram)does not yield additional improvements over a 5-gram class model, which is in contrast with the resultsreported by Wuebker et al.
(2013) on a French-German task.Since the stream-based models ignore word emission probabilities, one would expect further improve-ments from the theoretically more correct class-based model which include word emission probabilities(see Equation 1).
Somewhat surprisingly, this is not the case.
On the contrary, both 5- and 7-gramclass-based models perform slightly worse than the stream-based models.
We suspect that this is due tothe limited context used to estimate the emission probabilities in the original Brown class-based mod-els.
To verify this we compared this to the fullibm model (Equation 3) which conditions word emission6http://snowball.tartarus.org/algorithms/russian/stemmer.html7Our training corpus is considerably larger than those used by M?uller et al.
(2012), therefore we search among higher values.1923(a) Brown clusters (600)surface stemAdditional LM BLEU ?
BLEU ??
none [baseline] 18.8 ?
24.7 ??
5g stream-based 19.1 +0.3?24.8 +0.17g stream-based 19.1 +0.3?24.9 +0.2?
5g class-based 18.9 +0.1 24.6 ?0.17g class-based 18.8 ?0.0 24.7 ?0.05g fullibm 19.4 +0.6?25.0 +0.3?7g fullibm 19.3 +0.5?25.0 +0.3?
(b) Suffixes/words, ?
= 5000surface stemAdditional LM BLEU ?
BLEU ??
none [baseline] 18.8 ?
24.7 ??
5g stream-based 18.9 +0.1 24.6 ?0.17g stream-based 18.9 +0.1 24.6 ?0.1?
5g class-based 19.0 +0.2?24.8 +0.17g class-based 19.1 +0.3?24.7 ?0.05g fullibm 19.1 +0.3?24.8 +0.17g fullibm 19.2 +0.4?24.9 +0.2?Table 3: SMT translation quality on newstest13 when using different kinds of class-based language mod-els as additional features in the log-linear combination.
The settings used for weight tuning are markedwith ?.
Statistically significant differences wrt the baseline are marked with?at the p ?
.01 level and?at the p ?
.05 level.probabilities on the entire n-gram class history of length n ?
1.
The fullibm class-based models yieldthe biggest statistically significant improvements over the baseline and also compare favorably to thestream-based and original class-based models.
Similarly to stream- and class-based models we do notobserve a difference in performance between 5- and 7-gram models for fullibm.Table 2(b) shows the results obtained by the shallow morphology-based classes inspired by M?ulleret al.
(2012).
This form of classes is easy to implement in many languages and computationally muchcheaper than the Brown clusters.
Although less than the data-driven class models, the hybrid suffix/wordmodels also appear to improve translation quality.
We can see that fullibm again yields the highestimprovements, but we can also observe more consistent trends where longer n-grams help and class-based models are preferable to stream-based models without emission probabilities.When translating into a morphologically rich language, such as Russian, the role of the target lan-guage model is two-fold.
On the one hand, it helps choose the correct meaning from the available phrasetranslation candidates, on the other hand, it helps choose the correct surface realization of the trans-lation candidate that agrees grammatically with the previous target context.
For morphologically richlanguages the second aspect plays a considerably larger role than for morphologically poor languages.To disentangle these two roles of the language model we also evaluated the different language modelswith respect to stem-based information only, stripping off any inflectional information using the Snow-ball stemmer.
These results are also reported in Table 3 and in general exhibit the same trend as thesurface-based BLEU scores.
Again, fullibm performs best, and the original class-based LMs do not leadto any improvements over the baseline.
As a general observation, we find that the surface-level gainsare most of the time larger than the stem-level ones, which suggests that the additional LMs are mainlyimproving the choice of word inflections.All systems compared in Table 3 use a class language model as an additional feature, which is log-linearly interpolated with the other decoder features.
Alternatively, the word- and the class-based lan-(a) Brown clusters (600)surface stemAdditional LM BLEU ?
BLEU ??
none [baseline] 18.8 ?
24.7 ??
5g class, log-linear comb.
18.9 +0.1 24.6 ?0.1?
5g class, linear (global ?)
18.5 ?0.3 24.4 ?0.35g class, linear (class ?
?s) 18.6 ?0.2 24.5 ?0.2(b) Suffixes/words, ?
= 5000surface stemAdditional LM BLEU ?
BLEU ??
none [baseline] 18.8 ?
24.7 ??
5g class, log-linear comb.
19.0 +0.2?24.8 +0.1?
5g class, linear (global ?)
18.9 +0.1 24.8 +0.15g class, linear (class ?
?s) 18.6 ?0.1 24.6 ?0.1Table 4: SMT translation quality on newstest13 when using different LM combining frameworks: ad-ditional feature in the log-linear combination or linear interpolation with perplexity-tuned weights (oneglobal lambda or class-specific lambdas).1924guage models may be linearly interpolated with weights determined by maximizing the likelihood of aheld-out monolingual data set (see Section 3.2).
While linear interpolation often outperforms log-linearinterpolation for combining language models for domain adaptation (Foster and Kuhn, 2007), this doesnot seem to be the case for language models for morphologically rich target languages.
The resultspresented in Table 4 consistently show that linear interpolation under-performs log-linear combinationunder all conditions.
Even using class-specific interpolation weights as suggested by M?uller et al.
(2012)did not lead to any further improvements.5 ConclusionWe have presented the first systematic comparison of different forms of class-based LMs and differentclass LM combination methods in the context of SMT into a morphologically rich language.First of all, our results have shown that careful modeling of class-to-word emission probabilities?often omitted from the models used in SMT?is actually important for improving translation quality.In particular, we have achieved best results when using a refined variant of the original class-basedLM, called fullibm, which had never been tested for SMT but only for speech recognition (Goodman,2001).
Secondly, we have found that a rather simple LM based on shallow morphology-based classescan get close, in terms of BLEU, to the performance of more computationally expensive data-drivenclasses.
Although the reported improvements are modest, they are statistically significant and obtainedin a competitive large-data scenario against a state-of-the-art baseline.On the downside, and somewhat in contrast with previous findings in domain adaptation, we haveobserved that linear interpolation of word- and class-based LMs with perplexity-tuned weights performsworse than the log-linear combination of models with model-level weights globally tuned for translationquality.
This result was confirmed also when using class-specific lambdas as suggested by M?uller et al.
(2012).Indeed, modeling morphologically rich languages remains a challenging problem for SMT but, withour evaluation, we have contributed to assess how far existing language modeling techniques may goin this direction.
Natural extensions of this work include combining multiple LMs based on different,and possibly complementary, kinds of classes such as data-driven and suffix-based, or using supervisedmorphological analyzers instead of a simple stemmer.
In a broader perspective, we believe that future re-search should question the fundamental constraints of n-gram modeling and develop innovative modelingtechniques that conform to the specific requirements of translating into morphologically rich languages.AcknowledgmentsThis research was funded in part by the Netherlands Organisation for Scientific Research (NWO) underproject numbers 639.022.213 and 612.001.218.
We kindly thank Thomas M?uller for providing code andsupport for the weight optimization of linearly interpolated models.ReferencesWaleed Ammar, Victor Chahuneau, Michael Denkowski, Greg Hanneman, Wang Ling, Austin Matthews, KentonMurray, Nicola Segall, Alon Lavie, and Chris Dyer.
2013.
The CMU machine translation systems at WMT2013: Syntax, synthetic translation options, and pseudo-references.
In Proceedings of the Eighth Workshop onStatistical Machine Translation, pages 70?77, Sofia, Bulgaria, August.
Association for Computational Linguis-tics.Lalit R. Bahl, Peter F. Brown, Peter V. de Souza, Robert L. Mercer, and David Nahamoo.
1991.
A fast algorithmfor deleted interpolation.
In Eurospeech.
ISCA.Alexandra Birch, Miles Osborne, and Philipp Koehn.
2007.
CCG supertags in factored statistical machine trans-lation.
In Proceedings of the Second Workshop on Statistical Machine Translation, pages 9?16, Prague, CzechRepublic, June.
Association for Computational Linguistics.Arianna Bisazza and Marcello Federico.
2012.
Cutting the long tail: Hybrid language models for translation styleadaptation.
In Proceedings of the 13th Conference of the European Chapter of the Association for Computa-tional Linguistics, pages 439?448, Avignon, France, April.
Association for Computational Linguistics.1925Ond?rej Bojar, Christian Buck, Chris Callison-Burch, Christian Federmann, Barry Haddow, Philipp Koehn, ChristofMonz, Matt Post, Radu Soricut, and Lucia Specia.
2013.
Findings of the 2013 Workshop on Statistical MachineTranslation.
In Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 1?44, Sofia,Bulgaria, August.
Association for Computational Linguistics.Peter F. Brown, Peter V. deSouza, Robert L. Mercer, Vincent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-basedn-gram models of natural language.
Computational Linguistics, 18(4):467?479.Simon Carter and Christof Monz.
2011.
Syntactic discriminative language model rerankers for statistical machinetranslation.
Machine Translation, 25(4):317?339.Victor Chahuneau, Eva Schlinger, Noah A. Smith, and Chris Dyer.
2013.
Translating into morphologically richlanguages with synthetic phrases.
In Proceedings of the 2013 Conference on Empirical Methods in NaturalLanguage Processing, pages 1677?1687, Seattle, Washington, USA, October.
Association for ComputationalLinguistics.Stanley F. Chen and Joshua Goodman.
1999.
An empirical study of smoothing techniques for language modeling.Computer Speech and Language, 4(13):359?393.Colin Cherry, Robert C. Moore, and Chris Quirk.
2012.
On hierarchical re-ordering and permutation parsingfor phrase-based decoding.
In Proceedings of the Seventh Workshop on Statistical Machine Translation, pages200?209, Montr?eal, Canada, June.
Association for Computational Linguistics.Nadir Durrani, Philipp Koehn, Helmut Schmid, and Alexander Fraser.
2014.
Investigating the usefulness of gen-eralized word representations in SMT.
In Proceedings of the 25th International Conference on ComputationalLinguistics (COLING), Dublin, Ireland, August.Chris Dyer, Kevin Gimpel, Jonathan H. Clark, and Noah A. Smith.
2011.
The CMU-ARK German-EnglishTranslation System.
In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 337?343,Edinburgh, Scotland, July.
Association for Computational Linguistics.George Foster and Roland Kuhn.
2007.
Mixture-model adaptation for SMT.
In Proceedings of the SecondWorkshop on Statistical Machine Translation, pages 128?135, Prague, Czech Republic, June.
Association forComputational Linguistics.Michel Galley and Christopher D. Manning.
2008.
A simple and effective hierarchical phrase reordering model.In EMNLP ?08: Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages848?856, Morristown, NJ, USA.
Association for Computational Linguistics.Michel Galley and Christopher D. Manning.
2009.
Quadratic-time dependency parsing for machine translation.In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International JointConference on Natural Language Processing of the AFNLP, pages 773?781, Suntec, Singapore, August.
Asso-ciation for Computational Linguistics.Joshua T. Goodman.
2001.
A bit of progress in language modeling.
Computer Speech & Language, 15(4):403?434.Spence Green and John DeNero.
2012.
A class-based agreement model for generating accurately inflected trans-lations.
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, ACL ?12,pages 146?155, Stroudsburg, PA, USA.
Association for Computational Linguistics.Sa?sa Hasan, Oliver Bender, and Hermann Ney.
2006.
Reranking translation hypotheses using structural prop-erties.
In Proceedings of the EACL?06 Workshop on Learning Structured Information in Natural LanguageApplications, pages 41?48, Trento, Italy, April.Hany Hassan, Khalil Sima?an, and Andy Way.
2007.
Supertagged phrase-based statistical machine translation.In Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 288?295,Prague, Czech Republic, June.
Association for Computational Linguistics.Mark Hopkins and Jonathan May.
2011.
Tuning as ranking.
In Proceedings of 2011 Conference on EmpiricalMethods in Natural Language Processing (EMNLP?11).Philipp Koehn, Franz Josef Och, and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedings ofHLT-NAACL 2003, pages 127?133, Edmonton, Canada.1926Philipp Koehn, Hieu Hoang, Alexandra Birch, Chris Callison-Burch, Marcello Federico, Nicola Bertoldi, BrookeCowan, Wade Shen, Christine Moran, Richard Zens, Chris Dyer, Ondrej Bojar, Alexandra Constantin, and EvanHerbst.
2007.
Moses: Open Source Toolkit for Statistical Machine Translation.
In Proceedings of the 45thAnnual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demoand Poster Sessions, pages 177?180, Prague, Czech Republic.
Association for Computational Linguistics.Abby Levenberg and Miles Osborne.
2009.
Stream-based randomised language models for smt.
In Proceedingsof the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 2 - Volume 2, EMNLP?09, pages 756?764, Stroudsburg, PA, USA.
Association for Computational Linguistics.Christof Monz.
2011.
Statistical Machine Translation with Local Language Models.
In Proceedings of the 2011Conference on Empirical Methods in Natural Language Processing, pages 869?879, Edinburgh, Scotland, UK.,July.
Association for Computational Linguistics.Thomas M?uller, Hinrich Sch?utze, and Helmut Schmid.
2012.
A comparative investigation of morphologicallanguage modeling for the languages of the European Union.
In Proceedings of the 2012 Conference of theNorth American Chapter of the Association for Computational Linguistics: Human Language Technologies,pages 386?395, Montr?eal, Canada, June.
Association for Computational Linguistics.Eric W. Noreen.
1989.
Computer Intensive Methods for Testing Hypotheses.
An Introduction.
Wiley-Interscience.Franz Josef Och.
1999.
An efficient method for determining bilingual word classes.
In Proceedings of the 9thConference of the European Chapter of the Association for Computational Linguistics (EACL), pages 71?76.Franz Josef Och.
2003.
Minimum error rate training in statistical machine translation.
In Erhard Hinrichs and DanRoth, editors, Proceedings of the 41st Annual Meeting of the Association for Computational Linguistics, pages160?167.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic evalua-tion of machine translation.
In Proceedings of the 40th Annual Meeting of the Association of ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA.Stefan Riezler and John T. Maxwell.
2005.
On some pitfalls in automatic evaluation and significance testingfor MT.
In Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for MachineTranslation and/or Summarization, pages 57?64, Ann Arbor, Michigan, June.
Association for ComputationalLinguistics.Hinrich Sch?utze and Michael Walsh.
2011.
Half-context language models.
Comput.
Linguist., 37(4):843?865,December.Lane Schwartz, Chris Callison-Burch, William Schuler, and Stephen Wu.
2011.
Incremental syntactic languagemodels for phrase-based translation.
In Proceedings of the 49th Annual Meeting of the Association for Computa-tional Linguistics: Human Language Technologies, pages 620?631, Portland, Oregon, USA, June.
Associationfor Computational Linguistics.Jakob Uszkoreit and Thorsten Brants.
2008.
Distributed word clustering for large scale class-based languagemodeling in machine translation.
In Proceedings of ACL-08: HLT, pages 755?762, Columbus, Ohio, June.Association for Computational Linguistics.Philip Williams and Philipp Koehn.
2011.
Agreement constraints for statistical machine translation into german.In Proceedings of the Sixth Workshop on Statistical Machine Translation, pages 217?226, Edinburgh, Scotland,July.
Association for Computational Linguistics.Ian H. Witten and Timothy C. Bell.
1991.
The zero-frequency problem: Estimating the probabilities of novelevents in adaptive text compression.
IEEE Trans.
Inform.
Theory, IT-37(4):1085?1094.Joern Wuebker, Stephan Peitz, Felix Rietig, and Hermann Ney.
2013.
Improving statistical machine translationwith word class models.
In Proceedings of the 2013 Conference on Empirical Methods in Natural LanguageProcessing, pages 1377?1381, Seattle, Washington, USA, October.
Association for Computational Linguistics.Ali Yazgan and Murat Sarac?lar.
2004.
Hybrid language models for out of vocabulary word detection in largevocabulary conversational speech recognition.
In Proceedings of ICASSP, volume 1, pages I ?
745?8 vol.1,may.1927
