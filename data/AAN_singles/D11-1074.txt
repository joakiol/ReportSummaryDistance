Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 804?813,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsLinking Entities to a Knowledge Base with Query ExpansionSwapna GottipatiSchool of Information SystemsSingapore Management UniversitySingaporeswapnag.2010@smu.edu.sgJing JiangSchool of Information SystemsSingapore Management UniversitySingaporejingjiang@smu.edu.sgAbstractIn this paper we present a novel approachto entity linking based on a statistical lan-guage model-based information retrieval withquery expansion.
We use both local con-texts and global world knowledge to expandquery language models.
We place a strongemphasis on named entities in the local con-texts and explore a positional language modelto weigh them differently based on their dis-tances to the query.
Our experiments onthe TAC-KBP 2010 data show that incor-porating such contextual information indeedaids in disambiguating the named entities andconsistently improves the entity linking per-formance.
Compared with the official re-sults from KBP 2010 participants, our systemshows competitive performance.1 IntroductionWhen people read news articles, Web pages andother documents online, they may encounter namedentities which they are not familiar with and there-fore would like to look them up in an encyclope-dia.
It would be very useful if these entities could beautomatically linked to their corresponding encyclo-pedic entries.
This task of linking mentions of enti-ties within specific contexts to their correspondingentries in an existing knowledge base is called en-tity linking and has been proposed and studied in theKnowledge Base Population (KBP) track of the TextAnalysis Conference (TAC) (McNamee and Dang,2009).
Besides improving an online surfer?s brows-ing experience, entity linking also has potential us-age in many other applications such as normalizingentity mentions for information extraction.The major challenge of entity linking is to resolvename ambiguities.
There are generally two types ofambiguities: (1) Polysemy: This type of ambigu-ities refers to the case when more than one entityshares the same name.
E.g.
George Bush may re-fer to the 41st President of the U.S., the 43rd Presi-dent of the U.S., or any other individual who has thesame name.
Clearly polysemous names cause diffi-culties for entity linking.
(2) Synonymy: This typeof ambiguities refers to the case when more thanone name variation refers to the same entity.
E.g.Metro-Goldwyn-Mayer Inc. is often abbreviated asMGM.
Synonymy affects entity linking when the en-tity mention in the document uses a name variationnot covered in the entity?s knowledge base entry.Intuitively, to disambiguate a polysemous entityname, we should make use of the context in whichthe name occurs, and to address synonymy, exter-nal world knowledge is usually needed to expandacronyms or find other name variations.
Indeedboth strategies have been explored in existing litera-ture (Zhang et al, 2010; Dredze et al, 2010; Zhenget al, 2010).
However, most existing work usessupervised learning approaches that require carefulfeature engineering and a large amount of trainingdata.
In this paper, we take a simpler unsupervisedapproach using statistical language model-based in-formation retrieval.
We use the KL-divergence re-trieval model (Zhai and Lafferty, 2001) and ex-pand the query language models by considering boththe local contexts within the query documents andglobal world knowledge obtained from the Web.804Symbol DescriptionQ QueryDQ Query documentNQ Query name stringE KB entity nodeNE KB entity name stringDE KB entity disambiguation textSQ Set of alternate query name stringsN l,iQ Local alternative name stringsNgQ Global alternative name stringsEQ Candidate KB entries for Q?Q Query Language Model?LQ KB entry language model using local context from DQ?GQ KB entry language model using global knowledge?L+GQ KB entry language model using local context and global knowledge?NE KB entry language model with named entities only?NE+DE KB entry language model with named entities and disambiguation textTable 1: NotationWe evaluate our retrieval method with query ex-pansion on the 2010 TAC-KBP data set.
We find thatour expanded query language models can indeedimprove the performance significantly, demonstrat-ing the effectiveness of our principled and yet sim-ple techniques.
Comparison with the official resultsfrom KBP participants also shows that our system iscompetitive.
In particular, when no disambiguationtext from the knowledge base is used, our system canachieve an overall 85.2% accuracy and 9.3% relativeimprovement over the best performance reported inKBP 2010.2 Task Definition and System OverviewFollowing TAC-KBP (Ji et al, 2010), we define theentity linking task as follows.
First, we assumethe existence of a Knowledge Base (KB) of enti-ties.
Each KB entry E represents a unique entityand has three fields: (1) a name string NE , whichcan be regarded as the official name of the entity,(2) an entity type TE , which is one of {PER, ORG,GPE, UNKNOWN}, and (3) some disambiguationtext DE .
Given a query Q which consists of a queryname string NQ and a query document DQ wherethe name occurs, the task is to return a single KBentry to which the query name string refers or Nil ifthere is no such KB entry.It is fairly natural to address entity linking byranking the KB entries given a query.
In this sectionwe present an overview of our system, which con-sists of two major stages: a candidate selection stageto identify a set of candidate KB entries throughname matching, and a ranking stage to link the queryentity to the most likely KB entry.
In both stages,we consider the query?s local context in the querydocument and world knowledge obtained from theWeb.
It is important to note that the selection stageis based on string matching where the order of theword matters.
It is different from the ranking stagewhere a probabilistic retrieval model based on bag-of-word representation is used.
Our preliminary ex-periments demonstrate that without the first candi-date selection stage the linking process results in lowperformance.2.1 Selecting Candidate KB EntriesThe first stage of our system aims to filter out irrel-evant KB entries and select only a set of candidatesthat are potentially the correct match to the query.Intuitively, we determine whether two entities arethe same by comparing their name strings.
We there-fore need to compare the query name stringNQ withthe name string NE of each KB entry.
However,because of the name ambiguity problem, we cannotexpect the correct KB entry to always have exactlythe same name string as the query.
To address thisproblem, we use a set of alternative name strings ex-panded from NQ and select KB entries whose name805strings match at least one of them.
These alterna-tive name strings come from two sources: the querydocument DQ and the Web.First, we observe that some useful alternativename strings come from the query document.
Forexample, a PER query name string may contain onlya person?s last name but the query document con-tains the person?s full name, which is clearly a lessambiguous name string to use.
Similarly, a GPEquery name string may contain only the name of acity or town but the query document contains thestate or province, which also helps disambiguate thequery entity.
Based on this observation, we do thefollowing.
Given query Q, let SQ denote the set ofalternative query name strings.
Initially SQ containsonly NQ.
We then use an off-the-shelf NER taggerto identify named entities from the query documentDQ.
For PER and ORG queries, we select namedentities in DQ that contain NQ as a substring.
ForGPE queries, we select named entities that are of thetype GPE, and we then combine each of them withNQ.
We denote these alternative name strings as{N l,iQ }KQi=1, where l indicates that these name stringscome locally fromDQ andKQ is the total number ofsuch name strings.
{N l,iQ } are added to SQ.
Figure1 and Figure 2 show two example queries togetherwith their SQ.Sometimes alternative name strings have to comefrom external knowledge.
For example, one of thequeries we have contains the name string ?AMPAS,?and the query document also uses only this acronymto refer to this entity.
But the full name of the entity,?Academy of Motion Pictures Arts and Sciences,?
isneeded in order to locate the correct KB entry.
Totackle this problem, we leverage Wikipedia to findthe most likely official name.
Given query namestring NQ, we check whether the following link ex-ists: http://en.wikipedia.org/NQ.
If NQis an abbreviation, Wikipedia will redirect the linkto the Wikipedia page of the corresponding entitywith its official name.
So if the link exists, we usethe title of the Wikipedia page as another alternativename string for NQ.
We refer to this name string asNgQ to indicate that it is a global name variant.
NgQ isalso added to SQ.
Figure 2 shows such an example.For each name stringN in SQ, we find KB entrieswhose name strings match N .
We take the union ofQuery name string (NQ): MobileQuery document (DQ): The site is near Mount Ver-non in the Calvert community on the Tombigbee River,some 25 miles (40 kilometers) north of Mobile.
It?s ona river route to the Gulf of Mexico and near Mobile?srails and interstates.
Along with tax breaks and $400million (euro297 million) in financial incentives, Al-abama offered a site with a route to a Brazil plant thatwill provide slabs for processing in Mobile.Alternative Query Strings (SQ):from local context: Mobile, Mobile Mount Vernon,Mobile Calvert, Mobile River, Mobile Mexico, MobileAlabama, Mobile BrazilFigure 1: An example GPE query from TAC 2010.Query name string (NQ): CoppolaQuery document (DQ): I had no idea of all thesesemi-obscure connections, felicia!
Alex Greenwaldand Claire Oswalt aren?t names I?m at all familiarwith, but Jason Schwartzman I?ve heard of.
Isn?t heSophia Coppola?s cousin?
I think I once saw a pic-ture of him sometime agoAlternative Query Strings (SQ):from local context: Coppola, Sophia Coppola, SofiaCoppolafrom world knowledge(Wikipedia): Sofia CoppolaFigure 2: An example PER query from TAC 2010.these sets of KB entries and refer to it as EQ.
Theseare the candidate KB entries for query Q.2.2 Ranking KB EntriesGiven the candidate KB entries EQ, we need todecide which one of them is the correct match.We adopt the widely-used KL-divergence retrievalmodel, a statistical language model-based retrievalmethod proposed by Lafferty and Zhai (2001).Given a KB entry E and query Q, we score E basedon the KL-divergence defined below:s(E,Q) = ?Div(?Q?
?E) = ?
?w?Vp(w|?Q) logp(w|?Q)p(w|?E).
(1)Here ?Q and ?E are the query language model andthe KB entry language model, respectively.
A lan-guage model here is a multinomial distribution overwords (i.e.
a unigram language model).
V is thevocabulary and w is a single word.To estimate ?E , we follow the standard maxi-mum likelihood estimation with Dirichlet smooth-806ing (Zhai and Lafferty, 2004):p(w|?E) =c(w,E) + ?p(w|?C)|E| + ?
, (2)where c(w,E) is the count of w in E, |E| is thenumber of words in E, ?C is a background lan-guage model estimated from the whole KB, and ?is the Dirichlet prior.
Recall that E contains NE , TEand DE .
We consider using either NE only or bothNE and DE to obtain c(w,E) and |E|.
We referto the former estimated ?E as ?NE and the latter as?NE+DE .To estimate ?Q, typically we can use the empiricalquery word distribution:p(w|?Q) =c(w,NQ)|NQ|, (3)where c(w,NQ) is the count of w in NQ and |NQ|is the length of NQ.
We call this model the originalquery language model.After ranking the candidate KB entries in EQ us-ing Equation (1), we perform entity linking as fol-lows.
First, using an NER tagger, we determine theentity type of the query name string NQ.
Let TQ de-note this entity type.
We then pick the top-rankedKB entry whose score is higher than a threshold ?and whose TE is the same as TQ.
The system linksthe query entity to this KB entry.
If no such entryexists, the system returns Nil.3 Query ExpansionWe have shown in Section 2.1 that using the origi-nal query name string NQ itself may not be enoughto obtain the correct KB entry, and additional wordsfrom both the query document and external knowl-edge can be useful.
However, in the KB entry se-lection stage, these additional words are only usedto enlarge the set of candidate KB entries; they havenot been used to rank KB entries.
In this section, wediscuss how to expand the query language model ?Qwith these additional words in a principled way inorder to rank KB entries based on how likely theymatch the query entity.3.1 Using Local ContextsLet us look at the example from Figure 2 again.During the KB entry ranking stage, if we use ?Qestimated from NQ, which contains only the word?Coppola,?
the retrieval function is unlikely to rankthe correct KB entry on the top.
But if we includethe contextual word ?Sophia?
from the query doc-ument when estimating the query language model,KL-divergence retrieval model is likely to rank thecorrect KB entry on the top.
This idea of usingcontextual words to expand the query is very sim-ilar to (pseudo) relevance feedback in informationretrieval.
We can treat the query document DQ asour only feedback document.Many different (pseudo) relevance feedbackmethods have been proposed.
Here we apply therelevance model (Lavrenko and Croft, 2001), whichhas been shown to be effective and robust in a re-cent comparative study (Lv and Zhai, 2009).
Wefirst briefly review the relevance model.
Given a setof (pseudo) relevant documents Dr, where for eachD ?
Dr there is a document language model ?D,we can estimate a feedback language model ?fbQ asfollows:p(w|?fbQ) ??D?Drp(w|?D)p(?D)p(Q|?D).
(4)For our problem, since we have only a single feed-back document DQ, the equation above can be sim-plified.
In fact, in this case the feedback languagemodel is the same as the document language modelof the only feedback document, i.e.
?DQ .We then linearly interpolate the feedback lan-guage model with the original query language modelto form an expanded query language model:p(w|?LQ) = ?p(w|?Q) + (1 ?
?
)p(w|?DQ), (5)where ?
is a parameter between 0 and 1, to controlthe amount of feedback.
The larger ?
is, the less werely on the local context.
L indicates that the queryexpansion comes from local context.
This ?LQ canthen replace ?Q in Equation (1) to rank KB entries.Special Treatment of Named EntitiesUsually the document language model ?DQ is es-timated using the entire text from DQ.
For entitylinking, we suspect that named entities surroundingthe query name string in DQ are particularly usefulfor disambiguation and thus should be emphasizedover other words.
This can be done by weighting807NE and non-NE words differently.
In the extremecase, we can use only NEs to estimate the documentlanguage model ?DQ as follows:p(w|?DQ) =1KQKQ?i=1c(w,N l,iQ )|N l,iQ |, (6)where {N l,iQ } are defined in Section 2.Positional ModelAnother observation is that words closer to thequery name string in the query document are likelyto be more important than words farther away.
Intu-itively, we can use the distance between a word andthe query name string to help weigh the word.
Herewe apply a recently proposed positional pseudo rel-evance feedback method (Lv and Zhai, 2010).
Thedocument language model ?DQ now has the follow-ing form:p(w|?DQ) =1KQKQ?i=1f(pi, q) ?c(w,N l,iQ )|N l,iQ |, (7)where pi and q are the absolute positions of N l,iQand NQ in DQ.
The function f is Gaussian functiondefined as follows:f(p, q) = 1?2pi?2exp(?(p?
q)22?2).
(8)where variance ?
controls the spread of the curve.3.2 Using Global World KnowledgeSimilar to the way we incorporate words from DQinto the query language model, we can also con-struct a feedback language model using the mostlikely official name of the query entity obtained fromWikipedia.
Specifically, we definep(w|?NgQ) =c(w,NgQ)|NgQ|.
(9)We can then linearly interpolate ?NgQ with the orig-inal query language model ?Q to form an expandedquery language model ?GQ:p(w|?GQ) = ?p(w|?Q) + (1 ?
?)p(w|?NgQ).
(10)Here G indicates that the query expansion comesfrom global world knowledge.Entity Type %Nil %non-NilGPE 32.8% 67.2 %ORG 59.5% 40.5 %PER 71.7% 28.3 %Table 2: Percentages of Nil and non-Nil queries.3.3 Combining Local Context and WorldKnowledgeWe can further combine the two kinds of additionalwords into the query language model as follows:p(w|?L+GQ ) = ?p(w|?Q) + (1 ?
?
)(?p(w|?DQ)+(1 ?
?)p(w|?NgQ)).
(11)Note that here we have two parameters ?
and ?
tocontrol the amount of contributions from the localcontext and from global world knowledge.4 Experiments4.1 Experimental SetupData Set: We evaluate our system on the TAC-KBP2010 data set (Ji et al, 2010).
The knowledge basewas constructed from Wikipedia with 818,741 en-tries.
The data set contains 2250 queries and querydocuments come from news wire and Web pages.Around 45% of the queries have non-Nil entries inthe KB.
Some statistics of the queries are shown inTable 2.Tools: In our experiments, to extract named entitieswithinDQ and to determine TQ, we use the StanfordNER tagger1.
An example output of the NER taggeris shown below:<PERSON>Hugh Jackman<PERSON> isJacked!
!This piece of text comes from a query documentwhere the query name string is ?Jackman.?
We cansee that the NER tagger can help locate the full nameof the person.We use the Lemur/Indri2 search engine for re-trieval.
It implements the KL-divergence retrievalmodel as well as many other useful functionalities.Evaluation Metric: We adopt the Micro-averagedaccuracy metric, which is the mean accuracy overall queries.
It was used in TAC-KBP 2010 (Ji et1http://nlp.stanford.edu/software/CRF-NER.shtml2http://www.lemurproject.org/indri.php808al., 2010) as the official metric to evaluate the per-formance of entity linking.
This metric is simplydefined as the percentage of queries that have beencorrectly linked.Methods to Compare: Recall that our system con-sists of a KB entry selection stage and a KB entryranking stage.
At the selection stage, a set SQ ofalternative name strings are used to select candidateKB entries.
We first define a few settings where dif-ferent alternative name string sets are used to selectcandidate KB entries:?
Q represents the baseline setting which usesonly the original query name string NQ to se-lect candidate KB entries.?
Q+L represents the setting where alternativename strings obtained from the query docu-ment DQ are combined with NQ to select can-didate KB entries.?
Q+G represents the setting where the alterna-tive name string obtained from Wikipedia iscombined with NQ to select candidate KB en-tries.?
Q+L+G represents the setting as we describedin Section 2.1, that is, alternative name stringsfrom both DQ and Wikipedia are used togetherwith NQ to select candidate KB entries.After selecting candidate KB entries, in the KBentry ranking stage, we have four options for thequery language model and two options for the KBentry language model.
For the query languagemodel, we have (1) ?Q, the original query languagemodel, (2) ?LQ, an expanded query language modelusing local context from DQ, (3) ?GQ, an expandedquery language model using global world knowl-edge, and (4) ?L+GQ , an expanded query languagemodel using both local context and global worldknowledge.
For the KB entry language model, wecan choose whether or not to use the KB disam-biguation text DE and obtain ?NE and ?NE+DE , re-spectively.4.2 Results and DiscussionFirst, we compare the performance of KB entry se-lection stage for all four settings on non-Nil queries.The performance measure recall is defined asrecall ={1, if E that refers to Q, exists in EQ0, otherwiseThe recall statistics in Table 3 shows that, Q+L+Ghas the highest recall of the KB candidate entries.Method Recall(%)Q 67.1Q+L 89.7Q+G 94.9Q+L+G 98.2Table 3: Comparing the effect of candidate entry selec-tion using different methods - KB entry selection stagerecall.Before examining the effect of query expansionin ranking, we now compare the effect of using dif-ferent sets of alternative query name strings in thecandidate KB entry selection stage.
For this set ofexperiments, we fix the query language model to ?Qand the KB entry language model to ?NE in the rank-ing stage.Table 4 shows the performance of all the settingsin terms of micro-averaged accuracy.
The resultsshown in Tables 4, 5 and 6 are based on the opti-mum parameter settings.
We can see that in termsof the overall performance, both Q+L and Q+G givebetter performance than Q with a 7.7% and a 9.9%relative improvement, respectively.
Q+L+G givesthe best performance with a 12.8% relative improve-ment over Q.
If we further zoom into the results, wesee that for ORG and PER queries, when no correctKB entry exists (i.e.
the Nil case), the performanceof Q, Q+L, Q+G and Q+L+G is very close, indicat-ing that the additional alternative query name stringsdo not help.
It shows that the alternative query namestrings are most useful for queries that do have theircorrect entries in the KB.We now further analyze the impact of the ex-panded query language models ?LQ, ?GQ and ?L+GQ .We first analyze the results without using the KBdisambiguation text, i.e.
using ?NE .
Table 5 showsthe comparison between ?Q and other expandedquery language models in terms of micro-averagedaccuracy.
The results reveal that the expanded querylanguage models can indeed improve the overall per-formance (the both Nil and non-Nil case) under allsettings.
This shows the effectiveness of using theprincipled query expansion technique coupled withKL-divergence retrieval model to rank KB entries.809All Nil Non-NilMethod ALL GPE ORG PER GPE ORG PER GPE ORG PERQ 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789Q+L 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399Q+G 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291Q+L+G 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338Table 4: Comparing the performance of using different sets of query name strings for candidate KB entry selection.
?Q and ?NE are used in KB entry ranking.All Nil Non-NilMethod QueryModel ALL GPE ORG PER GPE ORG PER GPE ORG PERQ+L ?Q 0.7449 0.7156 0.6533 0.8655 0.9472 0.9888 0.9944 0.6024 0.1612 0.5399?LQ 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493Q+G ?Q 0.7604 0.7009 0.6893 0.8908 0.9431 0.9888 0.9944 0.5825 0.2500 0.6291?GQ 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653Q+L+G ?Q 0.7800 0.7583 0.6893 0.8921 0.9431 0.9888 0.9944 0.6680 0.2500 0.6338?L+GQ 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357Table 5: Comparison between the performance of ?Q and expanded query language models in terms of micro averageaccuracy.
?NE was used in ranking.On the other hand, again we observe that the ef-fects on the Nil and the non-Nil queries are differ-ent.
While in Table 4 the alternative name stringsdo not affect the performance much for Nil queries,now the expanded query language models actuallyhurt the performance for Nil queries.
It is not sur-prising to see this result.
When we expand the querylanguage model, we can possibly introduce noise,especially when we use the external knowledge ob-tained from Wikipedia, which largely depends onwhat Wikipedia considers to be the most popularofficial name of a query name string.
With noisyterms in the expanded query language model we in-crease the chance to link the query to a KB entrywhich is not the correct match.
The challenge is thatwe do not know when additional terms in the ex-panded query language model are noise and whenthey are not, because for non-Nil queries we do ob-serve a substantial amount of improvement broughtby query expansion, especially with external worldknowledge.
We will further investigate this researchquestion in the future.We now further study the impact of using the KBdisambiguation text associated with each entry to es-timate the KB entry language model used in the KL-divergence ranking function.
The results are shownin Table 6 for all the methods on ?NE vs. ?NE+DEusing the expanded query language models.
We cansee that for all methods the impact of using the KBdisambiguation text is very minimal and is observedonly for GPE and ORG queries.
Table 7 shows anexample of the KL-divergence scores for a query,Mobile whose context is previously shown in theFigure 1.
Without the KB disambiguation text boththe KB entry Mobile Alabama and the entry MobileRiver are given the same score, resulting in inaccu-rate linking in the ?NE case.
But with ?NE+DE , Mo-bile Alabama was scored higher, resulting in an ac-curate linking.
However, we observe that such casesare very rare in the TAC 2010 query list and thus theoverall improvement observed is minimal.KB Entry KB Name w/o text w/ textE0583976 Mobile Alabama -6.28514 -6.3839E0183287 Mobile River -6.28514 -6.69372Table 7: The KL-divergence scores of KB entities for thequery Mobile.Finally, we compare our performance with thehighest scores from TAC-KBP 2010 as shown in theTable 8.
It is important to note that the highest TACresults shown in the table under each setting are notnecessarily obtained by the same team.
We can seethat our overall performance when KB text is used iscompetitive compared with the highest TAC score,and is substantially higher than the TAC score whenKB text is not used.
Lehmann et al (2010) achievedhighest TAC scores.
They used a variety of evidencefrom Wikipedia like disambiguation pages, anchors,expanded acronyms and redirects to build a rich fea-ture set.
But as we discussed, building a rich fea-810All Nil Non-NilMethod KB Text ALL GPE ORG PER GPE ORG PER GPE ORG PERQ ?NE 0.6916 0.5714 0.6533 0.8495 0.8618 0.9888 0.9963 0.4294 0.1612 0.4789?NE+DE 0.6888 0.5607 0.6533 0.8495 0.8618 0.9888 0.9963 0.4135 0.1612 0.4789Q+L ?NE 0.7689 0.7850 0.6533 0.8682 0.9309 0.9888 0.9944 0.7137 0.1612 0.5493?NE+DE 0.7707 0.7904 0.6533 0.8682 0.9390 0.9888 0.9944 0.7177 0.1612 0.5493Q+G ?NE 0.8160 0.7423 0.7867 0.9188 0.9106 0.9372 0.9796 0.6600 0.5658 0.7653?NE+DE 0.8222 0.7450 0.7827 0.9387 0.8902 0.9372 0.9814 0.6740 0.5559 0.8310Q+L+G ?NE 0.8516 0.8278 0.7867 0.9401 0.8821 0.9372 0.9814 0.8012 0.5658 0.8357?NE+DE 0.8524 0.8291 0.7880 0.9401 0.8740 0.9372 0.9814 0.8072 0.5691 0.8357Table 6: Comparing the performance using KB text and without using KB text for all methods using expanded querymodels in terms of micro average accuracy on 2250 queries.
?NE+DE represents method using KB text and ?NErepresents methods without using KB text.ture set is an expensive task.
Their overall accu-racy is 1.5% higher than our model.
Table 8 showsthat the performance of ORG entities is lower whencompared with the TAC results when we used KBtext.
In our analysis, we observed that, even thoughsome entities like AMPAS are linked correctly, theentities like CCC (Consolidated Contractors Com-pany) failed due to ambiguity in the title.
Here, wemay benefit by leveraging more global knowledge,i.e, we should expand the NgQ with Wikipedia globalcontext entities together with the title to fully benefitfrom global knowledge.
In particular, when KB textis not used, our system outperforms the highest TACresults for all three types of queries.From the analysis by Ji et al (2010), overall theparticipating teams generally performed the best onPER queries and the worst on GPE queries.
With oursystem, we can achieve good performance on GPEqueries.KB Text Usage Type Our System TAC Highest?NE+DEAll 0.8524 0.8680GPE 0.8291 0.7957ORG 0.7880 0.8520PER 0.9401 0.9601?NEAll 0.8516 0.7791GPE 0.8278 0.7076ORG 0.7867 0.7333PER 0.9401 0.9001Table 8: Comparison of the best configuration of our sys-tem (Q+L+Gwith ?L+GQ ) with the TAC-KBP 2010 resultsin terms of micro-averaged accuracy.
?NE+DE representsthe method using KB disambiguation text and ?NE repre-sents the method without using KB disambiguation text.4.3 Parameter SensitivityIn all our experiments, we set the Dirichlet prior ?
to2500 following previous studies.
For the threshold ?we empirically set it to -12.0 in all the experimentsbased on preliminary results.
Recall that all the ex-panded query language models also have a controlparameters ?.
The local context-based models ?LQand ?L+GQ have an additional parameter ?
whichcontrols the proximity weighing.
The ?L+GQ modelhas another additional parameter ?
that controls thebalance between local context and world knowledge.In this subsection, we study the sensitivity of theseparameters.
We plot the sensitivity graphs for all themethods that involve ?
(?
set to 0.5) in Figure 3.
Aswe can see, all the curves appear to be stable and?=0.4 appears to work well.0.70.750.80.850.90.950  0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8MicroAveragedAccuracyalphaQ+LQ+GQ+L+GFigure 3: Sensitivity of ?
in regard to micro-averagedaccuracy.Similarly, we set ?=0.4 and examine how ?
af-fects micro averaged accuracy.
We plot the sensi-tivity curve for ?
for the Q+L+G setting with ?L+GQin Figure 5.
As we can see, the best performanceis achieved when ?=0.5.
This implies that the local8110.760.770.780.790.80.810.820.830.840.850.860  0.2  0.4  0.6  0.8  1MicroAveragedAccuracybetaQ+L+GFigure 4: Sensitivity of ?
in regard to micro-averagedaccuracy.context and the global world knowledge are weighedequally for aiding disambiguation and improving theentity linking performance.0.70.750.80.850.90.9540  60  80  100  120MicroAveragedAccuracysigmaQ+LQ+L+GFigure 5: Sensitivity of ?
with respect to micro-averagedaccuracy.Furthermore, we systematically test a fixed set of?
values from 25 to 125 with an intervals of 25 andexamine how ?
affects micro averaged accuracy.
Weset ?=0.4 and ?=0.5, which is the best parametersetting as discussed above.
We plot the sensitivitycurves for the parameter ?
for methods that utilizethe local context, i.e.
?LQ and ?L+GQ , in Figure 5.
Weobserve that all the curves are stable and 75 <= ?<= 100 appears to work well.
We set ?=100 for allour experiments.
Moreover, after 100, the graph be-comes stable, which indicates that proximity has lessimpact on the method from this point on.
This im-plies that an equal weighing scheme actually wouldwork the same for these experiments.
Part of thereason may be that by using only named entities inthe context rather than all words, we have effectivelypicked the most useful contextual terms.
Therefore,positional feedback models do have exhibit muchbenefit for our problem.5 Related WorkBunescu and Pasca (2006) and Cucerzan (2007) ex-plored the entity linking task using Vector SpaceModels for ranking.
They took a classification ap-proach together with the novel idea of exploitingWikipedia knowledge.
In their pioneering work,they used Wikipedia?s category information for en-tity disambiguation.
They show that using differ-ent background knowledge, we can find efficient ap-proaches for disambiguation.
In their work, theytook an assumption that every entity has a KB en-try and thus the NIL entries are not handled.Similar to other researchers, Zhang et al (2010)took an approach of classification and used a two-stage approach for entity liking.
They proposed asupervised model with SVM ranking to filter out thecandidates and deal with disambiguation effectively.For entity diambiguation they used the contextualcomparisons between the Wikipedia article and theKB article.
However, their work ignores the possi-bilities of acronyms in the entities.
Also, the am-biguous geo-political names are not handled in theirwork.Dredze et al (2010) took the approach that largenumber of entities will be unlinkable, as there isa probability that the relevant KB entry is unavail-able.
Their algorithm for learning NIL has shownvery good results.
But their proposal for handlingthe alias name or stage name via multiple lists is notscalable.
Unlike their approach, we use the globalknowledge to handle the stage names and thus thisgives an optimized solution to handle alias names.Similarly, for acronyms we use the global knowl-edge that aids unabbreviating and thus entity dis-ambiguation.
Similar to other approaches, Zhenget al (2010) took a learning to rank approach andcompared list-wise rank model to the pair-wise rankmodel.
They achieved good results on the list-wiseranking approach.
They handled acronyms and dis-ambiguity through wiki redirect pages and the an-chor texts which is similar to others ideas.Challenges in supervised learning includes care-ful feature selection.
The features can be selected inad hoc manner - similarity based or semantic based.Also machine learning approach induces challengesof handling heterogenous cases.
Unlike their ma-chine learning approach which requires careful fea-812ture engineering and heterogenous training data, ourmethod is simple as we use simple similarity mea-sures.
At the same time, we propose a statisticallanguage modeling approach to the linking prob-lem.
Many researchers have proposed efficient ideasin their works.
We integrated some of their ideaslike world knowledge with our new techniques toachieve efficient entity linking accuracy.6 ConclusionsIn this paper we proposed a novel approach to entitylinking based on statistical language model-basedinformation retrieval with query expansion using thelocal context from the query document as well asworld knowledge from the Web.
Our model is a sim-ple unsupervised one that follows principled exist-ing information retrieval techniques.
And yet it per-forms the entity linking task effectively comparedwith the best performance achieved in the TAC-KBP2010 evaluation.Currently our model does not exploit worldknowledge from the Web completely.
World knowl-edge, especially obtained from Wikipedia, hasshown to be useful in previous studies.
As our futurework, we plan to explore how to further incorporatesuch world knowledge into our model in a principledway.ReferencesRazvan Bunescu and Marius Pasca.
2006.
Using ency-clopedic knowledge for named entity disambiguation.In Proceesings of the 11th Conference of the EuropeanChapter of the Association for Computational Linguis-tics (EACL-06), pages 9?16, Trento, Italy.Silviu Cucerzan.
2007.
Large-scale named entity dis-ambiguation based on Wikipedia data.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 708?716.Mark Dredze, Paul McNamee, Delip Rao, Adam Ger-ber, and Tim Finin.
2010.
Entity disambiguationfor knowledge base population.
In Proceedings of the23rd International Conference on Computational Lin-guistics, pages 277?285.Heng Ji, Ralph Grishman, Hoa Trang Dang, Kira Grif-fitt, and Joe Ellis.
2010.
Overview of the TAC 2010knowledge base population track.
In Proceedings ofthe Third Text Analysis Conference.John Lafferty and ChengXiang Zhai.
2001.
Documentlanguage models, query models, and risk minimiza-tion for information retrieval.
In Proceedings of the24th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 111?119.Victor Lavrenko and W. Bruce Croft.
2001.
Rele-vance based language models.
In Proceedings of the24th Annual International ACM SIGIR Conference onResearch and Development in Information Retrieval,pages 120?127.John Lehmann, Sean Monahan, Luke Nezda, ArnoldJung, and Ying Shi.
2010.
Lcc approaches to knowl-edge base population at tac 2010.
In Proceedings TAC2010 Workshop.
TAC 2010.Yuanhua Lv and ChengXiang Zhai.
2009.
A compar-ative study of methods for estimating query languagemodels with pseudo feedback.
In Proceeding of the18th ACM Conference on Information and KnowledgeManagement, pages 1895?1898.Yuanhua Lv and ChengXiang Zhai.
2010.
Positional rel-evance model for pseudo-relevance feedback.
In Pro-ceeding of the 33rd Annual International ACM SIGIRConference on Research and Development in Informa-tion Retrieval, pages 579?586.Paul McNamee and Hoa Trang Dang.
2009.
Overviewof the TAC 2009 knowledge base population track.
InProceedings of the Second Text Analysis Conference.ChengXiang Zhai and John Lafferty.
2001.
Model-basedfeedback in the language modeling approach to infor-mation retrieval.
In Proceedings of the 10th Inter-national Conference on Information and KnowledgeManagement, pages 403?410.Chengxiang Zhai and John Lafferty.
2004.
A study ofsmoothing methods for language models applied to in-formation retrieval.
ACM Transactions on InformationSystems, 22(2):179?214, April.Wei Zhang, Jian Su, Chew Lim Tan, andWen TingWang.2010.
Entity linking leveraging automatically gener-ated annotation.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics (Col-ing 2010), pages 1290?1298.Zhicheng Zheng, Fangtao Li, Minlie Huang, and XiaoyanZhu.
2010.
Learning to link entities with knowledgebase.
In Human Language Technologies: The 2010Annual Conference of the North American Chapter ofthe Association for Computational Linguistics, pages483?491.813
