UNIVERSITY OF MASSACHUSETTS:MUC-3 TEST RESULTS AND ANALYSISWendy Lehnert, Claire Cardie, David Fisher, Ellen Riloff, Robert William sDepartment of Computer and Information Scienc eUniversity of Massachusett sAmherst, MA 01003lehnert@cs .umass.eduTEST RESULTSWe believe that the score reports we obtained for TST2 provide an accurate assessment of ou rsystem's capabilities insofar as they are consistent with the results of our own internal tests conductednear the end of phase 2 .
.
The required TST2 score reports indicate that our system achieved the highestcombined scores for recall (51%) and precision (62%) as well as the highest recall score of all the MUC-3 systems under the official MATCHED/MISSING scoring profile .We ran one optional test in addition to the required test for TST2 .
The optional run differs from th erequired run in only one respect, an alteration to our consolidation module .
The consolidation modulecontains all procedures that translate parser output into target template instantiations .
The complet econsolidation module includes a case-based reasoning (CBR) component that makes predictions abou tthe target output based on a portion of the development corpus .
For our optional run, we executed amodified version of consolidation that does not include this CBR component .
We predicted that th eabsence of the CBR component would pull recall down but push precision up (looking atMATCHED/MISSING only) .
This trade-off prediction was confirmed by the required and optionalTST2 score reports .
(Please consult Appendix F for our required and optional test score summaries) .The source of our recall/precision trade-off can be found by examining the actual, spurious andmissing counts for template-ids .
When we run with CBR, we generate 215 actual templates as opposedto 137 actual templates without CBR.
Most of these extra templates are spurious (64), but some ar ecorrect (14) .
The extra CBR templates increase our recall by reducing the number of missing templatesfrom 16 to 6, while lowering our precision by raising the number of spurious templates from 44 to 108 .
Thenet effect of the CBR component on TST2 is a 4% gain in recall and a 3% loss of precision .All of our system development and testing took place on a Texas Instruments Explorer II workstationrunning Common Lisp with 8 megabytes of RAM .
It took about 1 .5 hours to process the TST2 texts(without traces) .
No effort had been made to optimize run-time efficiency .
Shortly after the final TST2evaluation we found a way to reduce runtimes by about 40% .SYSTEM DEVELOPMENTAlmost all of our MUC-3 effort has been knowledge engineering in one form or another .
We canfurther categorize this effort in terms of (1) dictionary construction, and (2) discourse analysis .Dictionary construction received somewhat more attention than discourse analysis, with both relyin gheavily on examples from the development corpus .
Overall, we estimate that roughly 30-40% of th edevelopment corpus was analyzed for the purposes of either dictionary construction or discours eanalysis by the end of phase 2 .116Because we are working with a domain-specific dictionary, we construct our lexicon on the basis o fexamples in the development corpus .
Virtually all of our dictionary construction is done by hand .
Weexamine texts from the corpus in order to identify critical verbs and nouns that organize informationrelevant to the domain.
Then we create syntactic and semantic predictions based on these instance swith the expectation that similar linguistic constructs will be encountered in other texts as well .
Ourdictionary is effective only to the extent that we can extrapolate well on the basis of the example swe've seen .Our TST2 dictionary contained 5407 words and 856 proper names (mostly locations and terroris torganizations) .
1102 dictionary entries were associated with semantic features, and 286 entriesoperated as concept node triggers (CIRCUS cannot produce any output unless it encounters at least on econcept node trigger) .
131 verbs and 125 nouns functioned as concept node triggers .
Our semantic featurehierarchy contained 66 semantic features.
Although CIRCUS operates without a syntactic sentenc egrammar, it did exploit syntactic knowledge in the form of 84 syntactic prediction patterns, with 12 o fthese doing most of the work .
CIRCUS also accessed 11 control kernels for handling embedded claus econstructions [1] .Our version of discourse analysis took place during consolidation, when output from the CIRCU Ssentence analyzer was examined and organized into target template instantiations .
This translationfrom CIRCUS output to MUC-3 templates was handled by a rule base containing 139 rules .Consolidation errors could effectively destroy perfectly good output at the level of sentence analysis, s oour overall performance was really only as good as our consolidation component .
One of our ongoingproblems was in trying to evaluate the performance of CIRCUS and the performance of consolidationindependently.
We never did manage to tease the two apart, but we are confident that both component swould benefit from additional knowledge engineering .Serious consolidation development could not really get underway until we had a large number o ftexts to examine along with internal scoring runs based on the development corpus .
Although ourearliest opportunity for this was November, dictionary deficiencies delayed substantial progress o nconsolidation until February or March .
It was impossible to know how well consolidation was operatinguntil CIRCUS could provide consolidation with enough input to give it a fighting chance.
Theconsolidation rule base was generated by hand and modified upon inspection, with rapid growth takin gplace during phase 2 .
The number of consolidation rules almost doubled between TST1 and TST2 .We estimate that our time spent (measured in person/years) on technical development for MUC- 3was distributed as follows:alterations to CIRCUScase-based discourse analysi scorpus developmentdictionary constructionrule-based discourse analysistest runs & other misc .
technical2 .25 person/yearsThis estimate assumes that our graduate research assistants were working 30 hrs/wk on MUC-3 ,although it is notoriously difficult to estimate graduate student labor .
General alterations to CIRCUSincluded morphological analysis, conjunction handling, noun phrase recognition, embedded claus ehandling, and machinery for some special constructions like appositives .
These alterations to CIRCU Sand the CBR component are all domain-independent enhancements .
All other effort should becategorized as domain-specific ..35.15.25.75.50.25117DOMAIN-INDEPENDENT ADVANCESPrior to MUC-3, we had no experience with consolidation-style processing, so consolidatio nprovided us with many opportunities to explore new problem areas .
For example, we can locat epronominal referents both within sentences and across sentence boundaries 73% of the time (based on a nanalysis of pronouns in the relevant texts of the development corpus and TST1) .
However, theseheuristics are limited to four pronouns and there are only 130 instances of these pronouns in the text sanalyzed.
We examined the role of pronoun resolution with internal test runs, and came to th econclusion that this particular problem has little impact on overall recall or precision.A more compelling innovation for consolidation was first proposed in March, when we began toexperiment with the CBR component .
The CBR component allows our system to augment its fina ltemplate output based on known correlations between CIRCUS output and target template encodingsfound in the development corpus .
It performs this analysis using a case base of 254 template patternsdrawn from the 100 TST1 texts along with 283 development corpus texts .Case-based consolidation generates additional templates that might have been missed o rdismissed during the rule-based analysis, and thereby reduces the number of missing templates .
Becaus ethe CBR component effectively operates to counterbalance omissions made by rule-based consolidation ,we expect that the gain in recall due to CBR will eventually diminish as the system becomes mor ecomprehensive in its domain coverage .
Even so, the prospects for applying CBR techniques in NLP ar eopen-ended, and deserve further attention.
This preliminary attempt to bring CBR into naturallanguage processing is one of two original advances made during the course of our work on MUC-3 .The other significant advance was made very early on while we were assessing the robustness o fthe CIRCUS sentence analyzer and making some final adjustments to CIRCUS .
We were generallyconcerned about scaling up with respect to complex syntax, and thinking about ways that CIRCUSmight approach syntactically complex sentences in a principled manner .
At that time we discovered aformalism for embedded clause analysis, Lexically Indexed Control Kernels (aka LICKs) .
LICKsdescribe syntactic and semantic interactions within CIRCUS as it interprets embedded clauses .
Thisformalism makes it relatively easy to see how CIRCUS handles an embedded clause, and has made i tpossible for us to talk about this aspect of CIRCUS much more effectively.
In fact, a paper was writte nduring MUC-3 relating embedded clause analysis by CIRCUS to experimental results i npsycholinguistics [1] .
In that paper we argue that CIRCUS provides a cognitively plausible approachto complex syntax.UP AGAINST THE WALL : ARE WE THERE YET?The major limiting factor in our TST2 performance was time.
We are confident that significantimprovements in recall could be made if we had more time to do more knowledge engineering .
We wouldalso predict higher precision scores although our precision percentages have grown at a much slowe rrate than our recall percentages, based on a comparison of official test scores for TST1 and TST2 .We tend to think of our system in three major pieces: (1) the CIRCUS sentence analyzer, (2) rule-based consolidation, and (3) case-based consolidation .
Because the CBR component is truly optional, th eprimary responsibilities fall on CIRCUS and rule-based consolidation .
We know that both of thesecomponents make mistakes, but we have not been able to separate them well enough to say which one i sthe weaker one .
As with all knowledge-based systems, an assessment of these components is alsoconfounded by the fact that we are working with incomplete knowledge bases.
Both the dictionary andthe consolidation rule base incorporate domain knowledge, and we have thus far analyzed less than50% of the MUC-3 development corpus in our knowledge engineering efforts.118As one might expect, our best internal test runs are those that include texts we have analyzed fo rthe purposes of constructing our dictionary and consolidation rules.
For example, on May 13 we ran th eTST2 version of our system on TST1, and posted recall-precision scores of 66-68 running with CBR, an d62-73 running without CBR (for MATCHED/MISSING) .
It is heartening to contrast this with our phas e1 test results for TST1 which were 28-59 (no CBR component was available for phase 1 testing) .
Roughly20% of the TST1 texts were analyzed between February and May, so the substantial improvement i nboth recall and precision on TST1 can be only partially attributed to knowledge engineering based onthe TST1 texts.
A complete analysis of all the TST1 texts would provide us with a better estimate of aperformance ceiling that is not confounded by inadequate knowledge engineering .As far as future system development goes, we cannot conclude at this time that any one of our syste mcomponents requires redesign or major alterations .
We would like to exploit more of the corpus for thesake of knowledge engineering to get a better sense of what we can do when incomplete knowledge is no ta factor.
Only then can we hope to isloate limitations that need to be addressed by innovations insystem design .One limitation that applies more to our system development than our system itself, is the hand -coding of dictionary definitions and consolidation rules .
It would be highly advantageous for us t oautomate certain aspects of this or at least design an intelligent interface to speed the work of ourknowledge engineers.
We did manage to use the CBR component as a tool to direct us to useful textsduring dictionary construction, and this application of the CBR component was both very welcome an dvery effective (albeit rather late in the MUC-3 timetable) .
In any event, we would clearly benefit fro mintelligent interfaces or more ambitious machine learning strategies to help facilitate the knowledgeengineering effort that is so central to our whole approach .To sum up, we are confident that the performance of CIRCUS has not yet reached its limit .Unfortunately, it is not possible to say anything about where our ultimate upper bound lies .
We hope t opursue this question by participating in future performance evaluations .ACKNOWLEDGEMENTSOur participation in MUC-3 was supported by a DARPA contract administered by Meridia nAggregates Co., Contract No .
MDA903-89-C-0041, the Office of Naval Research, under a Universit yResearch Initiative Grant, Contract No .
N00014-86-K-0764 and NSF Presidential Young Investigator sAward NSFIST-8351863 .BIBLIOGRAPHY[1] Cardie, C., and Lehnert, W., "A cognitively plausible approach to understanding complex syntax,"Proceedings of the Ninth National Conference on Artificial Intelligence .
Anaheim, CA.
1991 .119
