TAILORING LEXICAL CHOICE TO THE USER'S VOCABULARYIN MULTIMEDIA EXPLANATION GENERATIONKathleen McKeownJacques RobinMichael TanenblattDepartment ofComputer Science450 Computer Science BuildingColumbia UniversityNew York, N.Y. 10027{ kathy,robin,tanenbla} @cs.columbia.eduABSTRACTIn this paper, we discuss the different strategies u ed in COMET(COordinated Multimedia Explanation Testbed) for selectingwords with which the user is familiar.
When pictures cannot beused to disambiguate a word or phrase, COMET has fourstrategies for avoiding unknown words.
We give examples foreach of these strategies and show how they are implemented inCOMET.1.
IntroductionA language generation system should select wordsthat its user knows.
While this would seem to involvesimply selecting a known word instead of an un-known word (as is done, for example, in \[1\]), in manycases it requires entirely rephrasing the rest of thesentence.
For example, in our domain of equipmentmaintenance and repair, if the user does not know theword "polarity," a sentence like "Check thepolarity."
will be rephrased as "Make sure the pluson the batte~,lines up with the plus on the batterycompartment.
Even when alternative words can beused-instead of an unknown word (e.g., a descriptiveexpression can be used instead of an object name),the alternative phrase may interact with other parts ofthe sentence which then need to be reworded as well.In this paper, we discuss the different strategies usedin COMET for selecting words with which the user isfamiliar.
Since COMET integrates text and picturesin a single explanation 1, unknown words are fre-quently disambiguated through accompan, ying.
pic-tures.
For example, when the accompanying pictureclearly shows the object and its location, COMETwill use the most common object name even if theuser is unfamiliar with the name 2.
When pictures can-not be used to disambiguate a word-or  phrase,COMET has four strategms for avoiding unknownwords:1.
Selecting an alternative word or phrase(e.g., generating "some number" in-stead of "arbitrary number' ')2.
Rephrasing by providing conceptualdefinitions (e.g., generating "Make surethe plus on the battery lines up with theplus on the battery compartment."
in-stead of "Check the polarity")3.
Rephrasing by generating descriptivereferring expressions (e.g., generating"the cable that runs to the KY57" in-stead of "the COMSEC cable' ')4.
Using past discourse to construct areferring expression (e.g., generating"Test the cable you just removed."
in-stead of "Test the COMSEC cable."
ifthe user had previously been instructedto remove this cable.
)In the following sections, we first t?rov!de an over-view of lexical choice in COMET, snowing how andwhere it occurs in the overall system.
Each of thestrategies is then described in turn, prefaced by abrief discussion of disambiguation of unknown termsthrough pictures.
Finally, we compare our work withprevious work in the area.1See \[2\] for a system overview and \[3, 4\] for details on mediacoordination in COMET.2This is similar to Appelt's \[5\] integration of language andphysical ctions for generating referring expressions.226iText GeneratorContent PlannerLogical Form Il.
IAnnotated Logical Form/JLexical \]ChooserText )Lay tionI Graphics Generator \]Multimedia ExplanationFigure 1: COMET System Architecture2.
Lexical Choice and ArchitectureCOMET's architecture is shown in Figure 1.
Onreceiving a request for an explanation via a menu in-terface, the content planner uses schemas \[6\] todetermine which information should be included inthe explanation from the underlying knowledgesources.
The explanation content, represented as ahierarchy of logical forms (LFs) \[7\] is passed to themedia coordinator \[3, 8\], which adds annotations in-dicating which portions are to be produced by thetext generator and which by the graphics generator\[9\].The Lexical Chooser is part of the text generator \[7\].Typically, it selects a word or phrase for each seman-tic concept in the input LF (i.e., the semantic on-straints on word choice).
In terms of coverage, theimplementation can select words for 148 differentsemantic concepts using 253 mapping rules, thusyielding on average slightly less than two alternativeword choices per concept (there are many conceptswhich are mapped to a single word, while others havemore than two alternatives).
The lexicon contains 159open class words.In this paper, we show how the user model and pastdiscourse (pragmatic constraints) also influence wordchoice.
But these are not the only constraints onword choice.
Syntactic form of the sentence and lex-ical constraints are other demonstrated\[10, 11\] influences on lexical choice.
For example,once the verb has been chosen, syntactic onstraintson its arguments (e.g., whether the object is a clause,227Load the frequency in channel one.
Step 3 of 4Step 1:Set the FCTN knob to LD.Figure 2: Accompanying Picture Clarifies Referentan adj, or np) will influence what words are chosen torealize the semantic oncept hat fill these arguments.Conversely, if one of the verb roles can only be real-ized as a noun phrase, for example, and not as othersyntactic categories, this restricts which verb isselected.
Lexical constraints on word choice arisefrom the use of collocations [12].
For example, a verblike "stand" takes the preposition "on"  for its loca-tion role, while the verb "turn" takes the preposition"onto."
Lexical choice is thus influenced by a widevariety of constraints which interact in many ways.Since syntactic and lexical constraints are only avail-able within the text generator, lexical choice isdelayed until this point.
Thus COMET waits until avariety of semantic, pragmatic, syntactic and lexicalconstraints are accumulated before selecting words.This means that COMET can use syntactic and lex-ical constraints on word choice in conjunction withsemantic and graphical constraints provided as input,plus the new.
pragmatic constraints we present.
Pre-vious work addressing pragmatic onstraints on wordusage folded lexical choice into the content planner(e.g., [13], [1]).
This was possible since the workfocused primarily on lexical side effects of contentdetermination (e.g., what property to include in a ref-erence as opposed to what linguistic form to use for aproperty).
Such approaches do not allow a system totake syntactic and lexical constraints on word choiceinto account.On receiving the hierarchy of logical forms, the Lex-ical Chooser determines the overall grammatical formof each sentence based on the semantic structure ofthe LFs (e.g., conditional sentences are generated forprecondition-action structures) and selects the wordsand phrases realizing semantic oncepts of the LF.
ItaSSeS a specification of the sentence's grammaticalrm and open-class words to the general purposesurface sentence generator FUF [14, 15, 16].
TheLexical Chooser uses a rewriting system itself im-plemented on top of FUF.
Its lexicon consists of abase of rules, where each rule rewrites a given set ofsemantic features into a corresponding set of lexicaland syntactic features.
Thus, each lexicon entry as-sociates a semantic concept with words that can beused to realize it.
Additional constraints from the usermodel, past discourse, and the underlying knowledgebase determine which of the alternative words orphrases hould be selected.
3 The user model indicatesboth the reading level of the current user 4, any in-dividual words that COMET knows the user does notunderstand, and any wording preferences (e.g., theuser knows abbreviations, the user is familiar withmilitary terminology).
We make no claims aboutwhich of these forms of user models is easier to ac-quire, but simply show how to use them when avail-able.If none of the alternative wordings for a given seman-tic concept of the LF are known to the user and the3When these constraints come from knowledge sources exter-nal to FUF, the Lexical Chooser uses FUF extensions to accesssuch knowledge through the use of coroutines [ 17].4We currently use two levels for a poor and good reader.
At thebeginning of the session, the reading level is either preset orCOMET can ask the user.228Install the new holding battery.
Step 2 of  6Remove the old holding battery, shown in the cutaway view.Figure 3: Use of Cross References: Remove the holding battery, shown in the cutaway viewaccompanying illustration cannot disambiguate thesewords, COMET reinvokes the content planner toreplan portions of the sentence content or to includeadditional semantic information.
Thus, COMET's ar-chitecture interleaves lexical choice and content plan-ning in order to account for a wide variety of mter-acting constraints on word choice.3.
Multimedia DisambiguationAn accompanying picture often makes clear what thereferent o f  a referring expression is.
If the user isunfamiliar with a term, the accompanying picturemight define it.
For example, Figure 2 shows onestep of an explanation generated by COMET forloading frequency into the radio.
The text refers to a"FCTN knob ' and the accompanying picture clearlysingles out the knob on the front panel of the radio[4].
COMET can also generate an explicit referenceto the illustration itself (called a cross reference).
Forexample, the cross reference shown in Figure 3 isgenerated if the user does not understand the term"holding battery".
In this case, the Lexical Chooser,on determining that "holding battery" is an un-familiar term, reinvokes the content planner whichfinds that no accompanying illustration is currentlyplanned and invokes graphics to generate an accom-panying illustration that depicts the holdin~ batteryand its location.
For full details on cross reierencingin COMET see [ 18].4.
Selecting a Familiar Word/phraseWhenever possible, COMET simply selects afamiliar word over an unknown word from the list ofalternatives in the lexicon.
Figure 4 shows some9uaired sentences that COMET generates which ii-strate alternative wordings.
The first italicizedphrase is generated if the user's vocabulary level isabove a certain reading level or if a word is not ex-plicitly listed in the user model as unknown.
Sincethe lexicon maintains a simple association betweenthe semantic concept and alternative phrasings,COMET selects the first alternative which the usermodel indicates is familiar to the user.
For example,Figure 5 shows that for any concept under the con-cept c-disconnect in the knowledge base taxonomy,COMET will use the word "disconnect" if the user'svocabulary level is high and the word "remove"otherwise.
COMET also checks whether the userknows abbreviations and if so, will use a referringexpression such as "FCTN knob" as shown mFigure 2.
If not, COMET uses the full name ("func-tion knob").
If COMET has no information about heuser, it generates the abbreviation and relies on theaccompanying illustration to clarify the referent.1.
Screw the new manpack antenna onto the RTand tighten until the manpack antenna issnug/tight.2.
Disconnect/Remove the COMSEC cablefrom the KY57 audio connector.3.
This will cause the display to show anarbitrary/some number.Figure 4: COMET-Generated Word Substitutions229(; semantic key((concept #(under c-disconnect))); rea l izat ion((process((cat verb-group) ; wil l  be a verb(alt(; if level h igh select "disconnect"((CONTROL (OK-Lex-UM 'c-disconnect high))(lex "disconnect")); else select "remove"((lex "remove"))))))))Figure 5: Lexicon Entry for Disconnect Concept5.
Rephrasing through ReplanningSelecting an alternative wording for a semantic on-cept is not always possible since none of the alter-natives may be known by the user.
Instead, COMETcan describe concepts at a more detailed semanticlevel of abstraction by retrieving additional defini-tional information from the knowledge base and itcan create referring descriptions when object namesare not known, by retrieving object attributes.5.1.
Retrieving alternative concept definitionsSometimes the original text uses a word or phrasethat abstracts the details of a concept o allow genera-tion of a very concise expression.
If unfamiliar withthe word or phrase, the user will be unable to inferthe specifics needed to perform the task.
Alternativewordings require choosing a less abstract level ofsemantic decomposition atwhich to describe the con-cept.
In these cases, COMET's lexical chooser ein-vokes the content planner to retrieve a finer graineddefinition of the concept from the knowledge base.For example, this strategy is used for rephrasing therequest "Check the polarity" which COMET issueswhen providing instructions for installing a new hold-ing battery.
More detailed semantics of checking thepolarity are stored as different okens of the conceptc-polarity in the knowledge base.
5 For example, inFigure 6 polarity is represented as the ecjuivalence be-tween the two plusses on two batteries ?.
Now, if theplan calls for checking polarity, it can be representedIn terms of a checking action on the equivalence ofthese two plusses (i.e., that they line up).
If the useris unfamiliar with the word "polarity," an alternatedecomposition will be retrieved and replace thephenomenon role filler in the original LF (Figure 7).Figure 8 shows the alternative LF with a newphenomenon role (the remainder of the LF is un-changed).
The resulting rephrased sentence is"Make sure that the plus on the battery lines up withthe plus on the battery compartment.
.
"Lines up'is selected in the lexicon for the equivalence relatlonbased on the semantics of its roles (i.e., that they areboth plusses on the batteries).
Here semantic selec-tional restrictions on the.roles control lexical choiceof the verb.Since the object of the new sentence is an embeddedsentence, COMET can use either the verb "check"or the collocation "make sure" as the verb realizingthe mental process concept c-check.
Note that, whilethese two verbs are listed as alternatives in the lex-icon for c-cheek, "make sure" cannot be used in theoriginal sentence due to a syntactic onstraint: its ob-ject cannot be an NP as one cannot say "Make surethe polar i ty.
.
This is an example of interaction be-tween syntactic and pragmatic onstraints.
Since syn-tax does not constrain the choice of verb in themodified sentence, COMET arbitrarily selects "makesure' .The lexicon entry containing these two verbs isshown below in Figure 9.
Note that the entry is in-dexed by the semantic concept c-check.
There aretwo alternative verbs, only one of which is com-patible with a clause as phenomenon role (ultimatelythe object).
When the phenomenon is an NP, bothverbs are valid and one is randomly selected.
; Instance def in i t ions for po la r i ty(tellm (polarity polar ity- l )(polarity polar ity-2)); More detai l  for one instance: po la r i ty  is; represented as two p lusses which  should; be equivalent.
The roles of the ec/uative; re lat ion are ident i f ied and ident i f ier:about polar i ty-2( identif ied plus-l)( identif ier plus-2)); one is located on the bat tery(:about plus- I  (on-loc battery- l)); one is located on the bat tery  compartment(:about plus-2 (on-loc bc-l))))Figure 6: Knowledge base tokens for polarity5The more detailed efinition is stored with e-polarity and notwith c-check since in our domain checking is carried out onmany different objects, while few actions are carried out onpolarity.6The equative relations has two roles, identified and identifier.Since they are included here, the equative relation (i.e., that hetwo plusses "line up") is inferred to hold.230(Concept C-Check) ; "check"(Process-Type Mental)(Roles((Phenomenon((Concept C-Polar i ty))))))  ; "the polarity"Figure 7: Logical Form for Original Sentence(Concept C-Check) ; "make sure that"(Process-Type Mental)(Roles((Phenomenon((Concept C-Polarity)(Process-Type Equative) ; "lines up with"(Roles(( Identi f ied((Concept C-Plus) ; "the plus"(Roles((On-Loc ; "on the battery"((Concept C-Battery)))))))( Identif ier((Concept C-Plus) ; "the plus"(Roles((On-Loc; "on  the battery compartment"((Concept C-BC))))))))))))))Figure 8: Logical Form of Rephrased Sentence; semantic key((concept #(under c-check)); rea l izat ion(cat verb-group) ; wi l l  be a verb(alt( ; if phenomenon real ized by NP((roles((phenomenon ((cat #((under np))))); then always choose "to check"(lex "check")); if phenomenon real ized by clause((roles((phenomenon ((cat #((under clause) )))); then randomly p ick  "to check" or; "to make sure"(lex ((Ralt ("check .... make sure")))) )))Figure 9: Lexicon Entry for Check Concept5.2.
Generating New Referential DescriptionsIf the user does not know an object name, the content~ lanner is reinvoked to generate object attributes to uild a referential description.
Although our selec-tion algorithm is not as sophisticated as others\[19, 5, 13\] because we do not use a detailed model ofuser beliefs, we address a new issue: the interactionbetween the new description and other parts of theoriginal sentence which may require rephrasing.
Twotypes of object attributes are used in a referring ex-pression in COMET: object subpart relations andatial relations to other objects in the accompanyingstration.
COMET selects the relations thatuniquely identify the object.For example, suppose COMET's Lexical Chooser isprovided with the LF for sentence 1, Figure 10, butthe user does not know the term "COMSEC."
In-stead of generating sentence 1, COMET generatessentence 2.
To do this, COMET first selects a uniquerelation between the cable and a known object.
In thiscase, it selects the connects patial relation betweenthe Radio Transmitter (RT) and the KY57, since thiscable is the only one that connects the radio and theKY57.
Selecting this relation for the description andsubstituting it for 'the COMSEC cable wouldresult in sentence 3, Fig.
10.
However, COMET notesthe redundant references to the audio connector andremoves one from the cable modifier by selecting theverb "runs to" instead which only requires one rolein the generated sentence.
This would result in thesentence 4, Fig.
10.
In this sentence, the attachmentof the prepositional phrase "from the KY57 audioconnector is ambiguous.
COMET detects this am-biguity when it removes the first from-location; sincethe two from-locations would have occurred side byside and both previous verbs of the sentence take it asa modifier, the generator must clarify that it is thefrom-location o f  the earlier verb "disconnect" andnot "run to."
To remove ambiguity, COMET sur-rounds the modifier of the cable by commas in sen-tence 2, Fig.
107.Descriptions Generated by COMET:I.
"Disconnect the COMSEC cable from theKY57 audio connector."2.
"Disconnect the cable, which runs to the RT,from the KY57 audio connector.
"Descriptions Avoided by COMET:3.
"Disconnect the cable that connects the RTto the KY57 audio connector f om the KY57audio connector."4.
"Disconnect the cable that runs to the RTfrom the KY57 audio connector.
"Figure 10: Generated Object Description7Another possible way to avoid ambiguity would be togenerate wo sentences such as "Find the cable that runs from theRT to the KY57 audio connector.
Disconnect the cable from theaudio connector."2316.
Using Past DiscourseFor subsequent reference, the presence of a discursivecontext allows for a wider variety of strategies to getaround gaps in the user's vocabulary.
COMET takesadvantage of this fact by maintaining a discourse his-tory, The content planner records all descriptionsinto the discourse history, creating one record for thedescription as a whole and a separate record for eachof its roles.
The entry for the description has fourfields:?
The name of the concept.?
The description used in the reference.?
The action in which the referringdescription plays a role.?
The list of roles that the description fillsin that action (e.g., "COMSEC cable" isthe medium of the action "discon-nect").For each subsequent reference, the concept name isused as the access key and the three other fields areupdated; they thus always contain the information onthe last reference.
By looking up information in thediscourse history, the content planner is able to con-struct object descriptions in terms of the last action itwas involved in.Sentences generated if the user knows "COMSEC"1.
"Disconnect the COMSEC cable from theKY57 audio-connector."2.
"Plug in the handset to the KY57 audio-connector.'
'3.
"Test the COMSEC cable.
"Sentences generated ifnot:4.
"Disconnect the cable, which runs tothe RT, from the KY57 audio connector."5.
"Plug in the handset to the KY57 audioconnector."6.
"Test the cable that you just disconnected.
"Figure 11: Use of Previous DiscourseAs an example, consider the explanations COMETenerates when instructing the user how to diagnosess of side tone.
When the user has no vocabularygaps, COMET .generates entences 1-3, Figure 1 l.When the user is unfamiliar with the term "COM-SEC," sentences 4-6 are generated instead.
HereCOMET uses past discourse to produce a descriptivereference for the second reference to the COMSECcable.As in the previous examples, the gap is detectedwhen the Lexical Chooser checks the  user model.Since there is no alternative phrase for "COMSEC"in the lexicon, COMET calls the content planner toreplan the reference.
Since it is not the first referenceto the cable, COMET uses the discourse history toplan a modifying description.
A reference to the cablets discovered in the history (its entry is shown inFigure 12) and the action in this entry is selected asthe modifier to build a referring expression.
8 The roleof the cable was medium and thus, COMET cangenerate the modifier as a relative clause.
The LF forthis referring expression is shown in Figure 13.
ThisLF is sent back to the lexical chooser, which selectsthe words for the concepts within it, and continueswith generation where it left off.
On third and fourthreference to the same concept, COMET uses itsanaphoric reference facility to generate ither a barehead (e.g., "cable")  or a pronoun (e.g., "it' ').
(; The concept name:((Concept C-Comsec-Cable)); The init ial  generated descr ipt ion:; inc luded where connected to and from.
((Concept C-Cable)(Roles ((To-Loc ((Concept C-RT)))(From-Loc ((Concept C-KY57)))))); The role it p lays in the action:((Roles Medium)); The act ion itself: ' 'd isconnect the cable'' .
((Process-Type Material)(Concept C-Disconnect); Rest of act ion descr ipt ion; in d iscourse h is tory)) ; but not shown hereFigure 12: Entry for COMSEC Cablein the Discourse History7.
Conclusions and Related WorkCOMET performs everal lexical choice tasks.
It canchoose between alternative words or phrases for anypart of speech.
When generating a request o performan action, it chooses a level of detail in the conceptdescription appropriate to the user.
When generatingboth initial and subsequent referring expressions, itselects a set of distinguishing properties of thereferent and chooses words to express the selected8There is a limit to how far back COMET looks in the dis-course to construct a new referring expression: the discoursehistory is cleared after each menu request for a new explanation.232((Concept C-Cable)(Roles((Latest-Participation((Process-Type Material)(Concept C-Disconnect)(Roles((Agent ((Concept C-User)))(Medium((Concept {^5 Concept}))))))))))Figure 13: "the cable you just disconnected"properties, Finally, for subsequent references,COMET can use previous discourse to avoid un-known words.COMET is thus using constraints from the usermodel, the accompanying illustration, and past dis-course in addition to traditional constraints fromsemantics, yntax, and other word choices.
Althoughother generation systems take into account some ofthese constraints, COMET is the first attempt to in-tegrate such a variety of constraints and  lexicalchoice strategies in a single system.
In addition, be-cause COMET is a multimedia system, it can use theaccompanying illustrations advantageously for dis-ambiguation.WIP \[20\] can also generate cross references but doesnot rely.
on a user model for either cross referenceeneratlon or lexical choice.
EPICURE \[19\], KAMP5\], and FN \[13\] tailor references based on situation,but they do not constrain this choice based on theuser's lexical knowledge.
EPICURE uses the user'sdomain knowledge, KAMP mutual beliefs about thedomain, and FN the user's domain knowledge in con-junction with rules on implicatures.
They focus onthe selection of appropriate properties to distinguishan object in generating references but do not choosebetween alternative wordings for the selectedproperties.
None of these systems reword actiondescriptions or use past discourse to avoid terms theuser does not know.
While Bateman and Paris' sys-tem \[21\] uses different dialects depending on whichclass of users it is addressing through register map-pings, in COMET different erms can be mixed andmatched epending on the individual user model.AcknowledgementsResearch on language generation in COMET hasbeen supported in part by Defense AdvancedResearch Projects Agency Contract N00039-84-C-0165, National Science Foundation GrantsIRT-84-51438 and GER-90-2406, New York StateCenter for Advanced Technology ContractsNYSSTF-CAT(90)-053, (91)-053, and(92)-053, andOffice of Naval Research Contracts N00014-82-K-0256 and N00014-89-J-1782.
COMET's develop-ment is an ongoing group effort and has benefitedfrom the contributions o f  Michael Elhadad (FUF),Doree Seligmann (graphics generator), AndreaDanyluk (diagnostic rule base), Yumiko Fukumoto(media coordinator), Jong Lim (static knowledge baseand content planner), Christine Lombardi (mediacoordinator), Jacques Robin (lexical chooser), JamesShaw (anaphoric reference facility), MichaelTanenblatt (knowledge base, content planner),Michelle Baker, Cl i ff  Beshers, David Fox, LauraGabbe, Frank Smadja, and Tony Weida.REFERENCES1.........10.Swartout, W.R., "XPLAIN: a system for creatingand explaining expert consulting systems",Artificial lntelligence, Vol.
21, No.
3, 1983, pp.285-325.Feiner, S. and K.R.
McKeown, "Generating Coor-dinated Multimedia Explanations", Proceedings ofthe IEEE Conference on AI Applications, SantaBarbara, CA., March 1990.Feiner, S. and K.R.
McKeown, "Coordinating Textand Graphics in Explanation Generation",Proceedings of the National Conference on Artifi-cial Intelligence, Boston, Mass., August 1990.Feiner, S. and McKeown, K.R., "Automating theGeneration of Coordinated Multimedia Explana-tions", IEEE Computer, Vol.
24, No.10, October 1991, pp.
33-41.Appelt, D.E., Planning English Sentences,Cambridge University Press, Cambridge, England,1985.McKeown, K.R., Text Generation: Using Dis-course Strategies and Focus Constraints toGenerate Natural Language Text, CambridgeUniversity Press, Cambridge, England, 1985.McKeown, K.R., Elhadad, M., Fukumoto, Y., Lim,J., Lombardi, C., Robin, J., and Smadja, F., "Lan-guage Generation in COMET", in CurrentResearch in Language Generation, Mellish, C.,Dale, R., and Zock, M., eds., Academic Press, Lon-don, 1990.Elhadad, M., Seligmann, D., Feiner, S., andMcKeown, K., "A Common Intention DescriptionLanguage for Interactive Multi-media Systems", ANew Generation of Intelligent Interfaces: Proceed-ings of lJCAl89 Workshop on Intelligent Interfaces,Detroit, MI, August 22 1989, pp.
46-52.Seligmann, D.D., and Feiner, S., "SpecifyingComposite Illustrations with CommunicativeGoals", Proc.
ACM Symposium on User InterfaceSoftware and Technology, Williamsburg, VA,November 13-15 1989, pp.
1-9.McDonald, D.D, "On the place of words in thegeneration process", in Natural Language Genera-23311.12.13.14.15.16.tion in Artificial Intelligence and ComputationalLinguistics, Paris, C., Swartout, W. and Mann.W.C., eds., Kluwer Academic Publishers, 1991.Danlos, L., The Linguistic Basis of TextGeneration, Cambridge University Press,Cambridge, England, 1987.Smadja, F. and K.R.
McKeown, "AutomaticallyExtracting and Representing Collocations for Lan-guage Generation", Proceedings of the 28th An-nual Meeting of the Association for ComputationalLinguistics, Pittsburgh, Pa., June 1990, pp.
252-9.Reiter, E.B., Generating appropriate natural lan-guage object description, PhD dissertation, Centerfor research in computing technology, HarvardUniversity, 1990.Elhadad, M., "The FUF Functional Unifier: User'sManual", Tech.
report, Columbia University, 1988.Elhadad, M., "Types in Functional UnificationGrammars", Proceedings of the 28th meeting ofthe Association for Computational Linguistics,Pittsburgh, Pa, June 1990.Elhadad, M., Using argumentation to control lex-ical choice: a unification-based implementation,PhD dissertation, Computer Science Department,Columbia University, 1993.17.
Elhadad, M. and Robin, J., "Controlling ContentRealization with Functional Unification Gram-mars", in Aspects of Automated Natural LanguageGeneration, Dale, R. and Hovy, H. and Roesner,D.
and Stock, O., ed., Springier Verlag, 1992, pp.89-104.18.
McKeown, K. R., Feiner, S.K., Robin, J., Selig-mann, D., and Tanenblatt, M., "Generating CrossReferences for Multimedia Explanations",Proceedings ofAAAI-92, AAAI, July 1992.19.
Dale, R., Generating Referring Expressions, ACL-MIT Press Series in Natural Language Processing,Cambridge, Ma., 1992.20.
Wahlster, W., Andre, E., Hecking, M., and T. Rist,"WIP: Knowledge-based Presentation of Infor-mation", Tech.
report WIP-1, German ResearchCenter for Artificial Intelligence, May 1989.21.
Bateman, J.A.
and Paris, C.L., "Phrasing a text interms the user can understand", Proceedings of thellth International Joint Conference on ArtificialIntelligence, Detroit, MI, 1989, pp.
1511-1517.234
