Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1618?1628,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsUnsupervised Prediction of Acceptability JudgementsJey Han Lau, Alexander Clark, and Shalom Lappinjeyhan.lau@gmail.com, alexsclark@gmail.com, shalom.lappin@kcl.ac.ukKing?s College LondonAbstractIn this paper we present the task of un-supervised prediction of speakers?
accept-ability judgements.
We use a test setgenerated from the British National Cor-pus (BNC) containing both grammaticalsentences and sentences containing a va-riety of syntactic infelicities introducedby round trip machine translation.
Thisset was annotated for acceptability judge-ments through crowd sourcing.
We traineda variety of unsupervised language mod-els on the original BNC, and tested themto see the extent to which they could pre-dict mean speakers?
judgements on the testset.
To map probability to acceptability,we experimented with several normalisa-tion functions to neutralise the effects ofsentence length and word frequencies.
Wefound encouraging results with the unsu-pervised models predicting acceptabilityacross two different datasets.
Our method-ology is highly portable to other domainsand languages, and the approach has po-tential implications for the representationand the acquisition of linguistic knowl-edge.1 IntroductionLanguage modelling involves predicting the prob-ability of a sentence.
Given a trained model, wecan infer the quantitative likelihood that a sentenceoccurs.
Acceptability, on the other hand, indicatesthe extent to which a sentence is permissible or ac-ceptable to native speakers of the language.
Whileacceptability is affected by frequency and exhibitsgradience (Keller, 2001; Sprouse, 2007; Lau et al,2014), there is limited research on the relationshipbetween acceptability and probability.
In this pa-per, we consider the the task of unsupervised pre-diction of acceptability.Speakers have robust intuitions about accept-ability, and acceptability has been consistentlyrated on various scales (Sprouse and Almeida,2012).
The acceptability of a sentence appears tobe relatively unaffected by its length (within cer-tain bounds), or the frequency of its words, prop-erties that we have confirmed experimentally.
Bycontrast sentence probability does depend on thesefactors.
To filter the effects of sentence length andword frequency, we devise normalising functionsto map the probability of a sentence (inferred byour unsupervised language models) to an accept-ability score.Keller (2001) and Lau et al (2014) present ev-idence that acceptability exhibits gradience.
Ac-cordingly, we treat acceptability as a continuousvariable here.
We train a variety of unsuper-vised models for the acceptability prediction task,and we assess the performance of these modelsby measuring the correlation between their nor-malised acceptability scores and the mean crowd-sourced acceptability judgements on a set of testsentences.There are a number of NLP tasks to which ourwork can be fruitfully applied.
It can be usedto evaluate the fluency of the output for machinetranslation and other language generation systems.It could also contribute to automatic essay scoring,and to second language tutorial systems.There are several reasons to favour unsuper-vised models.
From an engineering perspective,unsupervised models offer greater portability toother domains and languages.
Our methodologytakes only unannotated text as input.
Extending1618our methodology to other domains/languages istherefore straightforward, as it requires only a rawtraining corpus in that domain/language.Our work may also have significant implica-tions for the cognitive foundations of the repre-sentation and acquisition of linguistic knowledge.The unannotated training corpora of our languagemodels are impoverished input in comparison tothe data available to humans language learners,who learn from a variety of data sources (vi-sual and auditory cues, interaction with adults andpeers in a non-linguistic environment, etc).
If anunsupervised language model can reliably predicthuman acceptability judgements, then it providesa benchmark of what humans could, in principle,achieve with the same learning algorithm.Success in this task raises interesting questionsabout the nature of grammatical knowledge.
If ac-ceptability judgments can be accurately modeledthrough these techniques, then it seems unneces-sary to posit an underlying binary model of syn-tax which enumerates all and only the set of well-formed sentences.
Instead it is reasonable to sug-gest that humans represent linguistic knowledgeas a probabilistic, rather than as a binary system.Probability distributions provide a natural expla-nation of the gradience that characterises accept-ability judgements.
Gradience is intrinsic to prob-ability distributions, and to the acceptability scoresthat we derive from these distributions.While our results raise important questions con-cerning the nature of syntactic representation andof language acquisition, we leave them open forfurther research.
We refrain from making strongclaims on cognitive issues here.
Clearly addi-tional psychological evidence is required to moti-vate substantive conclusions on these issues, evenif our results suggest them.Our focus in this paper is on the task of predict-ing speakers?
acceptability judgements throughunsupervised language models.
We take this to bea problem in natural language processing, whosesolution has useful applications in language tech-nology.
All models described in this paper are im-plemented in an open source toolkit.1We describe our dataset in Section 2, whichconsists of crowd sourced acceptability judg-ments applied to sentences with errors introducedthrough round trip machine translation.
We de-1This toolkit can be accessed at https://github.com/jhlau/acceptability_prediction.scribe the models and their results in Section 3.
InSection 4 we present results with a different cor-pus based on English Wikipedia.
The new datasetshows that our observations generalise to anotherdomain.
We compare our methodology to a super-vised system in the acceptability prediction task inSection 5.
We look more closely at the influenceof sentence length and lexical frequency in Sec-tion 6, and we show that the normalising functionssucceed in neutralising these effects.
Finally, wediscuss the implications of our results, and drawconclusions from them in Section 7 and Section 8.2 Dataset and MethodologyFor our experiments, we require a collection ofsentences that exhibit varying degrees of gram-matical well-formedness.
We use the dataset thatwe discuss in Lau et al (2014).
We translatedBritish National Corpus (BNC Consortium, 2007)English sentences to four other languages ?
Nor-wegian, Spanish, Japanese and Chinese ?
and thenback to English using Google Translate.
To collecthuman judgements of acceptability for the sen-tences, we used Amazon Mechanical Turk.
A totalof 2,500 sentences were annotated.Three modes of presentation were used for rat-ing a sentence: (1) binary with two options (un-natural vs. natural); (2) four options (extremelyunnatural, somewhat unnatural, somewhat naturaland extremely natural); and (3) a sliding scale withtwo extremes (extremely unnatural and extremelynatural).
To aggregate the ratings over multiplespeakers for each sentence, we computed the arith-metic mean.
As there is a high correlation of meanratings among different modes of presentation, wetake the judgements for the four-option mode ofpresentation as the gold-standard for our experi-ments.To predict the ratings of the 2,500 test sen-tences, we trained several probabilistic models onthe BNC, and then used the trained models to inferthe probabilities of the test sentences.
Models aretrained on the written portion of the BNC, whichhas approximately 100 million words (henceforthreferred to as BNC-100M).2We used only thewords, and no forms of annotation information inthe BNC, as input to training.We first experiment with simple lexicalN -grammodels, and then move to Bayesian and neural2We removed sentences with less than 8 words, as well asthe 2,500 test sentences, from the training data.1619network models, increasing the complexity of themodels to better capture word dependencies.To translate probability into acceptabilityscores, we compute several acceptability mea-sures extracted from the model parameters.
Theacceptability measures are variants of the sen-tence?s log probability, devised to normalise sen-tence length and low frequency words.
Thesemeasures are summarised in Table 1.
For eachmeasure (including LogProb as a baseline) wecompute its Pearson correlation coefficient withthe gold standard sentence mean rating to evalu-ate its effectiveness in predicting acceptability.We tokenised the training data (BNC-100M) andthe test sentences using OpenNLP, and we con-verted all words to lower case.
To address out ofvocabulary (OOV) words that are seen in the testsentences but not in the training data, we adoptthe Berkeley Parser approach, where we replacelow frequency or OOV words using the UNK sig-nature.
We capture additional surface characteris-tics of the original word by attaching features atthe end of the signature (e.g.
the OOV word 1949would be replaced by the signature UNK-NUM).33 Unsupervised Models3.1 Lexical N -gram ModelLexical N -gram models were variously exploredin tasks related to acceptability estimation (Heil-man et al, 2014; Clark et al, 2013; Pauls andKlein, 2012).
We use an N -gram model withKneser-Ney interpolation (Goodman, 2001), andwe train bigram, trigram, and 4-gram models onBNC-100M.
The trained models are then used tocompute the acceptability measures of the test sen-tences.The results are detailed in Table 2 (columns:?2-gram?, ?3-gram?
and ?4-gram?
).4In generalacross all models, the Norm LP (Div) and SLORmeasures consistently produce the best correla-tions.We see a significant improvement when the con-text window is increased from 2-gram to 3-gram,but not so from 3-gram to 4-gram (2-gram best:0.34; 3-gram best: 0.42; 4-gram best: 0.42).
Thisresult implies that increasing the context window3Low frequency words are defined as words occurring lessthan 4 times in the BNC training data.
A total of 15 featuresare used for the UNK signature.4We do not present model perplexity values in the results,as we did not find any correlation between perplexity and taskperformance.Acc.
Measure EquationLogProb logPm(?
)Mean LPlogPm(?
)|?|Norm LP (Div) ?logPm(?)logPu(?
)Norm LP (Sub) logPm(?)?
logPu(?)SLORlogPm(?)?
logPu(?
)|?|Table 1: Acceptability measures for predicting theacceptability of a sentence.
Notations: SLOR isthe syntactic log-odds ratio, introduced by Paulsand Klein (2012); ?
is the sentence (|?| is the sen-tence length); Pm(?)
is the probability of the sen-tence given by the model; Pu(?)
is the unigramprobability of the sentence.
Note that the negativesign in Norm LP (Div) is given to reverse the signchange introduced by the division of log unigramprobabilities.of the lexical N -gram model does not correspondto a better representation of grammatical structure(insofar as the size of the dataset is fixed), and amore sophisticated model is necessary.3.2 Bayesian HMMSeeing that local context is insufficient for pre-dicting acceptability, we explore various Bayesianmodels that incorporate richer latent structures.We chose a Bayesian implementation because its?rich gets richer?
dynamics tends to work well forlanguages (Goldwater and Griffiths, 2007; Gold-water et al, 2009; Newman et al, 2012; Lau et al,2012).Lexical N -grams model the generation of aword based on its preceding words.
We introducea layer of latent variables on top of the words,which can be interpreted as the word classes,and we model the transitions between the latentvariables and observed words using Markov pro-cesses.
In this model we first generate a (latent)word class based on its preceding word classes,and we then generate the word based on its wordclass.
Figure 1(b) illustrates the structure ofa second order Hidden Markov model (HMM).1620wi?2wi?1wiwi+1(a) Lexical 3-gramsi?2si?1sisi+1wi?2wi?1wiwi+1(b) Bayesian HMM (2nd Order)ti?2ti?1titi+1si?2si?1sisi+1wi?2wi?1wiwi+1(c) Two-Tier BHMMFigure 1: A comparison of word structures for 3-gram, BHMM and Two-Tier BHMM.
w = observedwords; s = tier-1 latent states (?word classes?
); t = tier-2 latent states (?phrase classes?
).For comparison, the structure of a lexical 3-grammodel is given in Figure 1(a).Goldwater and Griffiths (2007) propose aBayesian approach for learning the HMM struc-ture.
The authors found that their BayesianHMM (BHMM) significantly outperforms aHMM trained with Maximum Likelihood Estima-tion in unsupervised part-of-speech tagging.
Weadopt the methodology of Goldwater and Griffiths(2007), and train a 2nd order BHMM for our task,using collapsed Gibbs sampling for inference.BHMM has two sets of multinomials: the statetransition multinomials and the word emissionmultinomials.
To generalise the state transitionprobabilities for start probabilities, we use dummywords/states for empty preceding words/states.BHMM has 3 parameters: (1) S, the number oflatent states; (2) ?, the Dirichlet hyper-parameterfor the state transition multinomials; and (3) ?,the Dirichlet hyper-parameter for the word emis-sion multinomials.
We assume symmetric Dirich-let priors for the hyper-parameters, and optimisethe 3 parameters based on test perplexity using agreedy search approach, i.e.
we optimise locallyfor one parameter at each stage, while keeping thedefault or previously optimised values for otherparameters.5For the optimisation step models aretrained using 10% of the full BNC (BNC-10M) for2,000 iterations.6Using the optimised parameters, we trainBHMM on BNC-100M for 10,000 iterations.
Fortest inference, we run the Gibbs sampler usingthe trained model for 5,000 iterations, and take 50samples from the final 500 iterations (with a lagof 10 iterations).
In each of the samples, we com-pute the test probabilities and acceptability mea-5When optimising for a parameter, we experimented with4?6 values of various orders of magnitudes.6The optimised parameters are: S = 100, ?
= 1.0 and?
= 0.01.sures using the MAP estimated states.7The finalprobabilities are computed as a harmonic mean ofprobabilities over the 50 samples.We summarise the correlation results in Table 2(column: ?BHMM?).
Compared to the N -grammodels, we see an improvement in the correlation,indicating that the introduction of a layer of (la-tent) word classes produces a better structure formodelling acceptability.3.3 LDAHMM and LDATo better understand the role of semantics inacceptability, we experimented with LDAHMM(Griffiths et al, 2004), a model that combines syn-tactic and semantic dependencies between words.The generative method of LDAHMM to gener-ate a word in a document is to first decide whetherto generate a syntactic state or a semantic state forthe word.
For the former, follow the HMM processto generate a state, and generate the word basedon the chosen state.
For the latter, follow the LDA(Blei et al, 2003) process to generate a topic basedon the document?s topic mixture, and generate theword based on the chosen topic.We use a second order HMM for the HMM partand Collapsed Gibbs sampling for performing in-ference.
LDAHMM has 4 sets of multinomials:the HMM multinomials (state transition and wordemission) and the LDA multinomials (document-topic and topic-word).LDAHMM has 6 parameters to optimise: (1)K the number of topics; (2) S the numberof syntactic states (semantic state has only 1state, designated as state 0); (3) ?, the Dirichlethyper-parameter for document-topic multinomi-als; (4) ?, the Dirichlet hyper-parameter for topic-word multinomials; (5) ?, the Dirichlet hyper-7As computing full probabilities gave little difference inthe final outcome, we adopted the computationally more effi-cient MAP approach.1621Measure 2-gram 3-gram 4-gram BHMM LDA LDAHMM 2T Chunker RNNLM PCFG*LogProb 0.24 0.30 0.32 0.25 0.09 0.21 0.26 0.32 0.32 0.21Mean LP 0.26 0.35 0.37 0.26 0.14 0.19 0.31 0.42 0.39 0.18Norm LP (Div) 0.33 0.42 0.42 0.44 0.05 0.33 0.50 0.43 0.53 0.26Norm LP (Sub) 0.12 0.20 0.23 0.33 0.01 0.19 0.46 0.14 0.31 0.22SLOR 0.34 0.41 0.41 0.45 0.03 0.33 0.50 0.42 0.53 0.25Table 2: Pearson?s r of acceptability measure and mean sentence rating for all experimented models in BNC.
Boldfaceindicates the best performing measure.
Note that PCFG is a supervised model unlike the others.parameter for state transition multinomials; and(6) ?, the Dirichlet hyper-parameter for wordemission multinomials.
We follow the same ap-proach as with BHMM for optimising, training,and testing the model.8Note that as LDAHMMoperates with documents, the training data is par-titioned into documents, and each test sentence istreated as a document.The results are summarised in Table 2 (column:?LDAHMM?).
The result shows that LDAHMMunderperforms in comparison to BHMM, indicat-ing that the incorporation of LDA did not improvethe model.
To understand the impact of LDAalone, we repeat the experiments using LDA andfind that it performs very poorly.
Results are sum-marised in Table 2 (column: LDA).
We suspectthat the low performance of LDA and LDAHMMis due to the small context window of the test doc-uments.
The LDA part is unable to generate anymeaningful topic mixtures, as there is insufficientcontext.3.4 Two-Tier BHMMBHMM uses (latent) word classes to drive wordgeneration.
Exploring a richer structure, we intro-duce another layer of latent variables on top of theword classes.
This second layer can be interpretedas phrase classes.
The idea behind this model isto use these phrase classes to drive word class andword generation.
An illustration of its word struc-ture is given in Figure 1(c).We use collapsed Gibbs sampling for perform-ing inference.
We sample the tier-1 state s and tier-2 state t separately, and the sampling equations aregiven as follows:8The final optimised parameters are: K = 100, S = 80,?
= 0.1, ?
= 0.0001, ?
= 0.1, and ?
= 0.01.P (ti|t?i, s,w, ?, ?, ?)
?#(ti?1, si?1, ti) + ?#(ti?1, si?1) + T?
?#(ti, si?1, si) + ?#(ti, si?1) + S?
?#(ti, si, ti+1) + ?#(ti, si) + T?
;P (si|s?i, t,w, ?, ?, ?)
?#(ti, si?1, si) + ?#(ti, si?1) + S?
?#(ti, si, ti+1) + ?#(ti, si) + T?
?#(ti+1, si, si+1) + ?#(ti+1, si) + S?
?#(si, wi) + ?#(si) +W?where si, tiare the tier-1 and tier-2 state indices;s, t, w are the assignments for all tier-1 states,tier-2 states and words, respectively (subscript?imeans the current assignment is excluded); ?, ?and ?
are the Dirichlet hyper-parameters; S =number of tier-1 states; T = number of tier-2states;W = vocabulary size; and #(x, [y], [z]) arethe multinomial counts.We follow the same process for optimising,training, and testing the model, and we summarisethe results in Table 2 (column: ?2T?
).9We see animproved correlation relative to BHMM (BHMMbest: 0.45, Two-Tier BHMM best: 0.50).
In factit has the best performance of all models thus far.This is encouraging, as it implies that the introduc-tion of the phrase layer produces a more optimalstructure for representing acceptability.3.5 Bayesian ChunkerGoldwater et al (2009) propose a Bayesian ap-proach to segment words in speech streams.
New-man et al (2012) extend the approach to segmentphrases ?
i.e.
multiword units ?
in sentences, andthey apply it to the task of index term identificationand keyphrase extraction.The core machinery of the methodology isdriven by the Dirichlet Process, where segments(words in Goldwater et al (2009) or phrases inNewman et al (2012)) are retrieved from a cache,9The optimised parameters: S = 100, T = 60, ?
= 1.0,?
= 1.0, ?
= 0.01.1622or newly generated.
Using Gibbs sampling for in-ference, the sampler considers one boundary pointat a time, and computes the probability of two hy-potheses: H0, for not generating a boundary; andH1, for generating a boundary.Borrowing the notation of Newman et al(2012), given p#is the probability of generating asegment boundary, at the boundary point betweenwords wxand wy, the probability of the hypothe-ses is computed as follows:P (H0|H?)
=n(wxy) + ?P0(wxy)n+ ?
;P (H1|H?)
=n(wx) + ?P0(wx)n+ ?
?n(wy) + ?P0(wy)n+ 1 + ?where H?is all of the structure shared by bothhypotheses; wxyis a multiword unit consisting ofwxand wy; n is the number of multiword tokens;?
is the concentration parameter of the Dirichletprocess; n(w) is the count of multiword w; andP0(w) is the probability of generating a novel w.i.e.
P0(wxy) = p#(1?
p#)P (wx)P (wy).We extend their methodology to segment wordclasses to do unsupervised chunking, motivatedby the idea that a well-formed sentence containspredictable patterns of word class chunks.
Weextend the sampling process to incorporate tran-sitions between chunks.
Given the word classes?cwcxcycz?, at the boundary point between wordclass cxand cy, the hypothesis H0to not gener-ate a boundary (therefore producing a single chunkcxy), and the hypothesis H1to generate a bound-ary (therefore producing two chunks cxand cy),are computed as follows:P (H0|H?)
=#(cw, cxy) + ?(n(cxy)+?P0(cxy)n+?
)#(cw) +m?
?#(cxy, cz) + ?(n(cz)+?P0(cz)n+?
)#(cxy) +m?
;P (H1|H?)
=#(cw, cx) + ?(n(cx)+?P0(cx)n+?
)#(cw) +m?
?#(cx, cy) + ?(n(cy)+?P0(cy)n+?
)#(cx) +m?
?#(cy, cz) + ?(n(cz)+?P0(cz)n+?
)#(cy) +m?where m = number of chunk types; n = num-ber of chunk tokens; ?
is the Dirichlet hyper-parameter for the chunk transition multinomials;and #(x, [y]) is the count for the chunk transitionmultinomials.As the model takes word classes as input, weuse the word classes induced by two-tier BHMM.We follow the same process for optimising, train-ing and testing the model.10The results are sum-marised in Table 2 (column: ?Chunker?).
Themodel produces a moderate correlation, perform-ing on par with the lexical 4-gram model.3.6 Recurrent Neural Network LanguageModelIn recent years, we have seen a resurgence in theuse of neural networks for deep machine learn-ing and NLP.
Rather than designing structures orhandcrafting features that seem intuitive for a task,deep learning introduces an entirely general ar-chitecture for machine learning.
It has yieldedsome impressive results for NLP tasks: automaticspeech recognition, parsing, part of speech tag-ging, and named entity recognition, to name a few(Seide et al, 2011; Mikolov et al, 2011a; Col-lobert et al, 2011; Chen and Manning, 2014).We experiment with a recurrent neural net-work language model (RNNLM: (Elman, 1998;Mikolov, 2012)) for our task.
We choose thismodel because it has an internal state that keepstrack of previously observed sequences, which iswell suited for natural language problems.
Totrain the model, we use stochastic gradient descentcombined with back propagation through time.RNNLM is optimised to reduce the error in pre-dicting the following word, based on the currentword and its history (represented in a compresseddimension in the size of the hidden layer).
Fulldetails of RNNLM can be found in the originalpapers (Mikolov et al, 2011b; Mikolov, 2012).11We experimented with some of the parametersof RNNLM using BNC-10M and found that mostparameters have an intuitive setting.
Its perfor-mance largely depends on the number of neuronsin the hidden layer.
Mikolov (2012) introduced avariant of RNNLM that does joint learning witha Maximum Entropy model which learns directconnections of N -gram features.
We found thatalthough there are advantages to using the MEmodel, the benefits disappear as we increase thenumber of neurons in the hidden layer.
We sawoptimal performance at 600 neurons, without us-ing the ME model.
All our results are based10The optimised parameters are: ?
= 0.1, ?
= 0.001,p#= 0.5.11We use Mikolov?s implementation of RNNLM for ourexperiment: http://rnnlm.org/.1623Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLMLogProb 0.31 0.36 0.38 0.32 0.33 0.35 0.42 0.44Mean LP 0.28 0.36 0.37 0.28 0.28 0.35 0.45 0.46Norm LP (Div) 0.34 0.41 0.41 0.44 0.42 0.49 0.43 0.55Norm LP (Sub) 0.11 0.20 0.22 0.32 0.32 0.44 0.14 0.33SLOR 0.35 0.41 0.41 0.46 0.44 0.50 0.41 0.57Table 3: Pearson?s r of acceptability measure and mean sentence rating for all experimented models in ENWIKI.
Boldfaceindicates the best performing measure.on the original RNNLM with 600 neurons in thehidden layer, trained on BNC-100M (Table 2 col-umn: ?RNNLM?
).12We see that RNNLM per-forms very well.
It outperforms the other models,achieving a correlation of 0.53.3.7 PCFG Parser (Supervised)Although we are interested in unsupervised mod-els, for purposes of comparison we experimentedwith a constituent PCFG parser for our task.We use the Stanford Parser (Klein and Man-ning, 2003a; Klein and Manning, 2003b), andtested both the unlexicalised and lexicalised PCFGparser with the supplied model.
To compute thelog probability of test sentences, we experimentedwith both top-1 and top-1K best parses.We found that the unlexicalised variant givesbetter performance, but we saw little differencebetween using the top-1 and the top-1K best parsesfor computing log probability.
In Table 2 (col-umn: ?PCFG?
), we report results for the unlexi-calised variant based on the top-1 best parse.
Thesupervised PCFG parser performs poorly.
This isnot surprising, given that the parser is trained ona different domain.13Moreover, the log probabil-ity scores are not true probabilities, but arbitraryvalues used for ranking the parse trees.4 English WikipediaFor the BNC domain we saw that SLOR and NormLP (Div) give the best acceptability measures,and that BHMM, two-tier BHMM and RNNLMare the best performing models.
These findingsare limited to a particular dataset.
To better un-derstand if these observations generalise to an-other domain, we developed an English Wikipediadataset (ENWIKI), following the same process de-scribed in Lau et al (2014) to generate test sen-12Other parameter values of RNNLM: number of classes= 550; bptt = 4; bptt-block = 100.13The Stanford English model is trained on the parse treehand annotated WSJ (section 1?21), Genia, and a few otherdatasets.tences through round-trip machine translation ,andto collect annotations via Mechanical Turk.14Asbefore, we follow the same procedures describedin Section 3 to optimise, train, and test all models(excluding LDA and PCFG).
The Pearson corre-lations with mean AMT annotations are presentedin Table 3.We identify similar trends in ENWIKI: Norm LP(Div) and SLOR are the best acceptability mea-sures, and we see improvements when we usea richer structure in the language model (two-tier BHMM>BHMM>N -grams).
Interestingly,LDAHMM performs much better in this domain(possibly due to increased coherence in the docu-ment structure of ENWIKI).
RNNLM has the bestperformance of all models, surpassing two-tierBHMM by a substantial margin.
Overall, the cor-relation values are very similar across the two do-mains, indicating that the models and the accept-ability measures are robust.5 Comparison with a Supervised SystemAlthough not a focus of this paper, supervisedlearning can further improve the correlation per-formance of our models.
The acceptability mea-sures can be combined in a supervised context.
Weexperimented with this approach in a support vec-tor regression model (with an RBF kernel).
Weachieved a correlation performance of 0.64 in BNCand of 0.69 in ENWIKI.15Heilman et al (2014) propose a system for pre-dicting acceptability.
They built a dataset con-sisting of sentences from essays written by non-native speakers for an ESL test.
Acceptabilityratings were judged by the authors, and throughcrowdsourcing (henceforth we refer to this anno-tated data set as the GUG data set).
They applied14Both the BNC and the English Wikipedia datasets areavailable at http://www.dcs.kcl.ac.uk/staff/lappin/smog/?page=research.15We use only the unsupervised models, excluding the su-pervised PCFG parser.
The models are trained and tested us-ing 10-fold cross validation.1624a 4-category ordinal scale for rating the sentences.To predict sentence acceptability, they employ alinear regression model that draws features fromspelling errors, anN -gram model, precision gram-mar parsers, and the Stanford PCFG parser.To better understand the performance of oursystem compared to other acceptability predictionsystems, we evaluated our methodology againstthat of Heilman et al (2014) on the GUG dataset.We preprocessed the GUG dataset minimally.
Weremoved 15 short sentences that have less than 5words, lowercased all words, and tokenised thesentences using OpenNLP.
This yields 2255 sen-tences for the training and development subset,and 749 sentences for the test set.
Using the out-put ?
i.e.
the acceptability measures ?
of our un-supervised models (trained on BNC) as features,we trained an SVR model using GUG training anddevelopment subsets to predict acceptability rat-ings on GUG test sentences.
We applied the de-fault SVR parameters, and so it was not necessaryto use the development subset separately to opti-mise the parameters.
For evaluation we computedthe correlation of the predicted ratings and meanhuman ratings.We present a comparison of results in Table 4.We first tested the unsupervised models, with thebest correlation of 0.472 produced by the lexical4-gram model using the Norm LP (Div) measure.Combining the models in SVR, we achieve a cor-relation of 0.603.Heilman et al (2014) note that spelling is oneof the important features in their regression model,as the dataset often contains spelling mistakes.
Weborrowed this feature, computed as the proportionof misspelled words, and incorporated into ourmodel.
It produced a significant improvement inthe correlation (0.636), a performance almost onpar with that of Heilman et al (2014).16Our results demonstrate the robustness andportability of our system in a new domain.
OurSVR model requires significantly less supervisionthan that of Heilman et al (2014), which relies onprecision and constituent parsers.
Moreover, ourmethodology provides a completely unsupervisedalternative that requires only raw text for training.16We use PyEnchant for spellcheck: http://pythonhosted.org/pyenchant/.
Note thatwe also tried adding the spelling feature to our originalBNC derived dataset, but it yielded no improvement in thecorrelation.
This is not surprising, given that it contains fewspelling errors.System Pearson?s rHeilman et al (2014) 0.644Unsupervised Best 0.472SVR: All Models 0.603SVR: All Models+Spell 0.636Table 4: A comparison of results of our system and Heil-man et al (2014) on GUG.6 Influence of Sentence Length andLexical FrequencyOur primary motivation in doing this research hasbeen to use acceptability predictions to explorewhether acceptability can be represented throughprobability information.
Unlike probability, ac-ceptability is generally not influenced by sentencelength or low frequency words.The acceptability measures we apply normalisesentence length and word frequency.
To evaluatetheir effectiveness, we computed two correlationsin the BNC domain: (1) acceptability measure vs.sentence length (Table 5); and (2) acceptabilitymeasure vs. sentence minimum word frequency(Table 6).17For comparison we additionally computed thecorrelation of these factors with human ratings.The correlations are: +0.13 with sentence length;and +0.07 with minimum word frequency.
Theseobservations confirm the view that acceptability isnot affected by these two factors.Table 5 shows that although LogProb yields astrong negative correlation with sentence length,Mean LP, Norm LP (Div) and SLOR all producelow correlations.
The only exception is Norm LP(Sub), which still has a significant correlation withsentence length.In Table 6 we see some degree of correlation inLogProb with the minimum word frequency, but itis relatively small.
In general, SLOR is the scoringfunction that most effectively normalises word fre-quency, producing low correlation for most mod-els.
Norm LP (Div) also does very well, for allmodels except N -grams.7 DiscussionIn principle, the upper bound of the correlation be-tween our models?
predicted acceptability valuesand mean human ratings is 1.0.
But no individualhuman annotator will match mean judgements per-fectly.
It is more plausible to measure our models?17We use BNC-100M for computing word frequency.1625Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLMLogProb ?0.89 ?0.80 ?0.84 ?0.85 ?0.86 ?0.86 ?0.83 ?0.86Mean LP ?0.16 ?0.08 ?0.18 +0.03 +0.05 ?0.02 ?0.01 +0.08Norm LP (Div) ?0.15 ?0.07 ?0.17 +0.10 +0.15 ?0.00 ?0.00 +0.14Norm LP (Sub) +0.69 +0.63 +0.54 +0.46 +0.54 +0.11 +0.70 +0.62SLOR ?0.07 +0.04 ?0.03 +0.12 +0.17 +0.01 ?0.00 +0.17Table 5: Pearson?s r of acceptability measure and sentence length for all models in BNC.
For comparison the correlationwith human ratings is +0.13.Measure 2-gram 3-gram 4-gram BHMM LDAHMM 2T Chunker RNNLMLogProb +0.27 +0.27 +0.27 +0.27 +0.27 +0.27 +0.19 +0.28Mean LP +0.30 +0.28 +0.27 +0.29 +0.28 +0.29 +0.08 +0.26Norm LP (Div) +0.24 +0.23 +0.21 +0.11 +0.06 +0.12 +0.06 +0.11Norm LP (Sub) ?0.04 ?0.03 ?0.03 ?0.03 ?0.09 +0.05 ?0.13 ?0.08SLOR +0.16 +0.14 +0.12 +0.06 ?0.00 +0.10 +0.04 +0.03Table 6: Pearson?s r of acceptability measure and sentence minimum word frequency for all models in BNC.
The correlationwith the human ratings is +0.07.rate of success against an estimated level of indi-vidual human performance.
We do this by mim-icking an arbitrary speaker, and testing the corre-lation of this construct?s judgements with the meanscores of the annotators.We simulate such an individual human judge byrandomly selecting a single annotator rating foreach sentence, and computing the Pearson corre-lation between these judgements and the mean rat-ings for the rest of the annotators (one vs the rest)in our test sets.
We ran this experiment 50 timesfor each test set to reduce sample variation, pro-ducing a mean correlation of 0.67 for BNC and0.74 for ENWIKI.
For comparison, the best unsu-pervised model (RNNLM) achieves a correlationof 0.53 in BNC and 0.57 in ENWIKI (Section 3).The supervised model (SVR) produces a correla-tion of 0.64 in BNC and 0.69 in ENWIKI (Section 5).Although there is still room for improvement forthe unsupervised methodology, it is encouragingto note that the supervised variant predicts accept-ability at a level that approaches estimated humanperformance.To test the robustness of our methodologyacross languages, we are currently developingdatasets in other languages, based on Wikipedia.Our preliminary results show similar performanceto that which we report here for ENWIKI, suggestingthat these results hold across languages.8 ConclusionWe developed a methodology for using unsuper-vised language models to predict human accept-ability judgements.
We experimented with a va-riety of unsupervised models.
To map proba-bility to acceptability we proposed a set of ac-ceptability measures to normalise sentence lengthand lexical frequency.
We achieved encourag-ing results across two datasets constructed throughround trip machine translation, and the methodol-ogy is highly portable to other domains and lan-guages.
This research has potential implicationsfor our understanding of human language acquisi-tion and the way in which linguistic knowledge isrepresented.AcknowledgementsThe research reported here was done as part of the StatisticsalModels of Grammar (SMOG) project at King?s College Lon-don (www.dcs.kcl.ac.uk/staff/lappin/smog/),funded by grant ES/J022969/1 from the Economic and So-cial Research Council of the UK.We are grateful to Douglas Saddy and Garry Smith at theCentre for Integrative Neuroscience and Neurodynamics atthe University of Reading for generously giving us access totheir computing cluster, and for much helpful technical sup-port.
We thank J. David Lappin for invaluable assistance inorganising our AMT HITS.
We presented part of the workdiscussed here to CL/NLP, cognitive science, and machinelearning colloquia at Chalmers University of Technology,University of Gothenburg, University of Sheffield, Universityof Edinburgh, The Weizmann Institute of Science, Universityof Toronto, MIT, and the ILLC at the University of Amster-dam.
We very much appreciate the comments and criticismsthat we received from these audiences, which have guided usin our research.1626ReferencesD.
Blei, A. Ng, and M. Jordan.
2003.
Latent Dirichletallocation.
Journal of Machine Learning Research,3:993?1022.BNC Consortium.
2007.
The British National Corpus,version 3 (BNC XML Edition).
Distributed by Ox-ford University Computing Services on behalf of theBNC Consortium.D.
Chen and C. Manning.
2014.
A fast and accu-rate dependency parser using neural networks.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2014), pages 740?750, Doha, Qatar.Alexander Clark, Gianluca Giorgolo, and Shalom Lap-pin.
2013.
Statistical representation of grammat-icality judgements: The limits of n-gram models.In Proceedings of the ACL Workshop on CognitiveModelling and Computational Linguistics, Sofia,Bulgaria.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.J.
Elman.
1998.
Generalization, simple recurrent net-works, and the emergence of structure.
In M. Gerns-bacher and S. Derry, editors, Proceedings of the 20thAnnual Conference of the Cognitive Science Society.Lawrence Erlbaum Associates, Mahway, NJ.Sharon Goldwater and Tom Griffiths.
2007.
A fullyBayesian approach to unsupervised part-of-speechtagging.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL 2007), pages 744?751, Prague, Czech Repub-lic.S.
Goldwater, T. Griffiths, and M. Johnson.
2009.
ABayesian framework for word segmentation: Ex-ploring the effects of context.
Cognition, 112:21?54.J.T.
Goodman.
2001.
A bit of progress in lan-guage modeling.
Computer Speech & Language,15(4):403?434.Thomas L. Griffiths, Mark Steyvers, David M. Blei,and Joshua B. Tenenbaum.
2004.
Integrating top-ics and syntax.
In Advances in Neural InformationProcessing Systems 17, pages 537?544.
Vancouver,Canada.Michael Heilman, Aoife Cahill, Nitin Madnani,Melissa Lopez, Matthew Mulholland, and JoelTetreault.
2014.
Predicting grammaticality on anordinal scale.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics (ACL 2014), Volume 2: Short Papers, pages174?180, Baltimore, Maryland.Frank Keller.
2001.
Gradience in Grammar: Exper-imental and Computational Aspects of Degrees ofGrammaticality.
Ph.D. thesis, The University of Ed-inburgh.D.
Klein and C. Manning.
2003a.
Accurate unlex-icalized parsing.
In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics (ACL 2003), pages 423?430, Sapporo,Japan.D.
Klein and C. Manning.
2003b.
Fast exact inferencewith a factored model for natural language parsing.In Advances in Neural Information Processing Sys-tems 15 (NIPS-03), pages 3?10, Whistler, Canada.J.H.
Lau, P. Cook, D. McCarthy, D. Newman, andT.
Baldwin.
2012.
Word sense induction for novelsense detection.
In Proceedings of the 13th Con-ference of the EACL (EACL 2012), pages 591?601,Avignon, France.J.H.
Lau, A. Clark, and S. Lappin.
2014.
Measuringgradience in speakers?
grammaticality judgements.In Proceedings of the 36th Annual Conference of theCognitive Science Society, pages 821?826, QuebecCity, Canada.T.
Mikolov, A. Deoras, S. Kombrink, L. Burget, andJ.`Eernock?y.
2011a.
Empirical evaluation and com-bination of advanced language modeling techniques.In Proceedings of the 12th Annual Conference ofthe International Speech Communication Associa-tion (INTERSPEECH 2011), pages 605?608, Flo-rence, Italy.T.
Mikolov, S. Kombrink, A. Deoras, L. Burget, andJ.`Eernock?y.
2011b.
Rnnlm - recurrent neural net-work language modeling toolkit.
In IEEE AutomaticSpeech Recognition and Understanding Workshop,Hawaii, US.T.
Mikolov.
2012.
Statistical Language Models basedon Neural Networks.
Ph.D. thesis, Brno Universityof Technology.David Newman, Nagendra Koilada, Jey Han Lau, andTimothy Baldwin.
2012.
Bayesian text segmenta-tion for index term identification and keyphrase ex-traction.
In Proceedings of the 24th InternationalConference on Computational Linguistics (COLING2012), pages 2077?2092, Mumbai, India.A.
Pauls and D. Klein.
2012.
Large-scale syntacticlanguage modeling with treelets.
In Proceedings ofthe 50th Annual Meeting of the Association for Com-putational Linguistics, pages 959?968.
Jeju, Korea.F.
Seide, G. Li, and D. Yu.
2011.
Conversationalspeech transcription using context-dependent deepneural networks.
In Proceedings of the 12th AnnualConference of the International Speech Communica-tion Association (INTERSPEECH 2011), Florence,Italy.1627J.
Sprouse and D. Almeida.
2012.
Assessing the relia-bility of textbook data in syntax: Adger?s core syn-tax.
Journal of Linguistics, 48(3):609?652.J.
Sprouse.
2007.
Continuous acceptability, categor-ical grammaticality, and experimental syntax.
Bi-olinguistics, 1:123?134.1628
