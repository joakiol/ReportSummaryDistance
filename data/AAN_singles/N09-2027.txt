Proceedings of NAACL HLT 2009: Short Papers, pages 105?108,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsThe Importance of Sub-Utterance Prosody in Predicting Level of CertaintyHeather Pon-BarrySchool of Engineering and Applied SciencesHarvard UniversityCambridge, MA 02138, USAponbarry@eecs.harvard.eduStuart ShieberSchool of Engineering and Applied SciencesHarvard UniversityCambridge, MA 02138, USAshieber@seas.harvard.eduAbstractWe present an experiment aimed at under-standing how to optimally use acoustic andprosodic information to predict a speaker?slevel of certainty.
With a corpus of utteranceswhere we can isolate a single word or phrasethat is responsible for the speaker?s level ofcertainty we use different sets of sub-utteranceprosodic features to train models for predict-ing an utterance?s perceived level of certainty.Our results suggest that using prosodic fea-tures of the word or phrase responsible for thelevel of certainty and of its surrounding con-text improves the prediction accuracy withoutincreasing the total number of features whencompared to using only features taken fromthe utterance as a whole.1 IntroductionProsody is a fundamental part of human-to-humanspoken communication; it can affect the syntac-tic and semantic interpretation of an utterance(Hirschberg, 2003) and it can be used by speakersto convey their emotional state.
In recent years, re-searchers have found prosodic features to be usefulin automatically detecting emotions such as annoy-ance and frustration (Ang et al, 2002) and in dis-tinguishing positive from negative emotional states(Lee and Narayanan, 2005).In this paper, we address the problem of predict-ing the perceived level of certainty of a spoken ut-terance.
Specifically, we have a corpus of utter-ances where it is possible to isolate a single wordor phrase responsible for the speaker?s level of cer-tainty.
With this corpus we investigate whether us-ing prosodic features of the word or phrase causinguncertainty and of its surrounding context improvesthe prediction accuracy when compared to using fea-tures taken only from the utterance as a whole.This work goes beyond existing research by look-ing at the predictive power of prosodic features ex-tracted from salient sub-utterance segments.
Pre-vious work on uncertainty has examined the pre-dictive power of utterance- and intonational phrase-level prosodic features (Liscombe et al, 2005) aswell as the relative strengths of correlations betweenlevel of certainty and sub-utterance prosodic fea-tures (Pon-Barry, 2008).
Our results suggest thatwe can do a better job at predicting an utterance?sperceived level of certainty by using prosodic fea-tures extracted from the whole utterance plus onesextracted from salient pieces of the utterance, with-out increasing the total number of features, than byusing only features from the whole utterance.This work is relevant to spoken language applica-tions in which the system knows specific words orphrases that are likely to cause uncertainty.
For ex-ample, this would occur in a tutorial dialogue systemwhen the speaker answers a direct question (Pon-Barry et al, 2006; Forbes-Riley et al, 2008), or inlanguage (foreign or ESL) learning systems and lit-eracy systems (Alwan et al, 2007) when new vocab-ulary is being introduced.2 Previous WorkResearchers have examined certainty in spoken lan-guage using data from tutorial dialogue systems(Liscombe et al, 2005) and data from an uncertaintycorpus (Pon-Barry, 2008).Liscombe et al (2005) trained a decision tree105classifier on utterance-level and intonational phrase-level prosodic features to distinguish between cer-tain, uncertain, and neutral utterances.
Theyachieved 76% accuracy, compared to a 66% accu-racy baseline (choosing the most common class).We have collected a corpus of utterances spokenunder varying levels of certainty (Pon-Barry, 2008).The utterances were elicited by giving adult nativeEnglish speakers a written sentence containing oneor more gaps, then displaying multiple options forfilling in the gaps and telling the speakers to readthe sentence aloud with the gaps filled in accordingto domain-specific criteria.
We elicited utterancesin two domains: (1) using public transportation inBoston, and (2) choosing vocabulary words to com-plete a sentence.
An example is shown below.Q: How can I get from Harvard to the Silver Line?A: Take the red line toa.
South Stationb.
Downtown CrossingThe term ?context?
refers to the fixed part of the re-sponse (?Take the red line to ?, in this exam-ple) and the term ?target word?
refers to the word orphrase chosen to fill in the gap.The corpus contains 600 utterances from 20speakers.
Each utterance was annotated for levelof certainty, on a 5-point scale, by five humanjudges who listened to the utterances out of context.The average inter-annotator agreement (Kappa) was0.45.
We refer to the average of the five ratings asthe ?perceived level of certainty?
(the quantity we at-tempt to predict in this paper).We computed correlations between perceivedlevel of certainty and prosodic features extractedfrom the whole utterance, the context, and the tar-get word.
Pauses preceding the target word wereconsidered part of the target word; all segmenta-tion was done manually.
Because the speakers hadunlimited time to read over the context before see-ing the target words, the target word is consideredto be the source of the speaker?s confidence or un-certainty; it corresponds to the decision that thespeaker had to make.
Our correlation results sug-gest that while some prosodic cues to level of cer-tainty were strongest in the whole utterance, otherswere strongest in the context or the target word.
Inthis paper, we extend this past work by testing theprediction accuracy of models trained on differentsubsets of these prosodic features.3 Prediction ExperimentsIn our experiments we used 480 of the 600 utter-ances in the corpus, those which contained exactlyone gap.
(Some had two or three gaps.)
We ex-tracted the following 20 prosodic feature-types fromeach whole utterance, context, and target word (a to-tal of 60 features) using WaveSurfer1 and Praat2.Pitch: minf0, maxf0, meanf0, stdevf0, rangef0, rel-ative position minf0, relative position maxf0,absolute slope (Hz), absolute slope (semitones)Intensity: minRMS, maxRMS, meanRMS, stdev-RMS, relative position minRMS, relative posi-tion maxRMSTemporal: total silence, percent silence, total dura-tion, speaking duration, speaking rateThese features are comparable to those used in Lis-combe et al?s (2005) prediction experiments.
Thepitch and intensity features were represented asz-scores normalized by speaker; the temporal fea-tures were not normalized.Next, we created a ?combination?
set of 20 fea-tures based on our correlation results.
Figure 1 il-lustrates how the combination set was created: foreach prosodic feature-type (each row in the table) wechose either the whole utterance feature, the contextfeature, or the target word feature, whichever onehad the strongest correlation with perceived level ofcertainty.
The selected features (highlighted in Fig-ure 1) are listed below.Whole Utterance: total silence, total duration,speaking duration, relative position maxf0, rel-ative position maxRMS, absolute slope (Hz),absolute slope (semitones)Context: minf0, maxf0, meanf0, stdevf0, rangef0,minRMS, maxRMS, meanRMS, relative posi-tion minRMSTarget Word: percent silence, speaking rate, rela-tive position minf0, stdevRMS1http://www.speech.kth.se/wavesurfer/2http://www.fon.hum.uva.nl/praat/106Feature-type Whole Utterance Context Target Wordmin f0 0.107 0.119 0.041max f0 ?0.073 ?0.153 ?0.045mean f0 0.033 0.070 ?0.004stdev f0 ?0.035 ?0.047 ?0.043range f0 ?0.128 ?0.211 ?0.075rel.
position min f0 0.042 0.022 0.046rel.
position max f0 0.015 0.008 0.001abs.
slope f0 (Hz) 0.275 0.180 0.191abs.
slope f0 (Semi) 0.160 0.147 0.002min RMS 0.101 0.172 0.027max RMS ?0.091 ?0.110 ?0.034mean RMS ?0.012 0.039 ?0.031stdev RMS ?0.002 ?0.003 ?0.019rel.
position min RMS 0.101 0.172 0.027rel.
position max RMS ?0.039 ?0.028 ?0.007total silence ?0.643 ?0.507 ?0.495percent silence ?0.455 ?0.225 ?0.532total duration ?0.592 ?0.502 ?0.590speaking duration ?0.430 ?0.390 ?0.386speaking rate 0.090 0.014 0.1361Figure 1: The Combination feature set (highlighted in ta-ble) was produced by selecting either the whole utterancefeature, the context feature, or the target word featurefor each prosodic feature-type, whichever one was moststrongly correlated with perceived level of certainty.To compare the prediction accuracies of differentsubsets of features, we fit five linear regression mod-els to the feature sets.
The five subsets are: (A)whole utterance features only, (B) target word fea-tures only, (C) context features only, (D) all fea-tures, and (E) the combination feature set.
We di-vided the data into 20 folds (one fold per speaker)and performed a 20-fold cross-validation for eachset of features.
Each experiment fits a model us-ing data from 19 speakers and tests on the remain-ing speaker.
Thus, when we test our models, we aretesting the ability to classify utterances of an unseenspeaker.Table 1 shows the accuracies of the modelstrained on the five subsets of features.
The num-bers reported are averages of the 20 cross-validationaccuracies.
We report results for two cases: 5 pre-diction classes and 3 prediction classes.
We firstcomputed the prediction accuracy over five classes(the regression output was rounded to the nearestinteger).
Next, in order to compare our results tothose of Liscombe et al (2005), we recoded the5-class results into 3-class results, following Pon-Barry (2008), in the way that maximized inter-annotator agreement.
The naive baseline numbersare the accuracies that would be achieved by alwayschoosing the most common class.4 DiscussionAssuming that the target word is responsible for thespeaker?s level of certainty, it is not surprising thatthe target word feature set (B) yields higher accura-cies than the context feature set (C).
It is also not sur-prising that the set of all features (D) yields higheraccuracies than sets (A), (B), and (C).The key comparison to notice is that the combi-nation feature set (E), with only 20 features, yieldshigher average accuracies than the utterance fea-ture set (A): a difference of 6.42% for 5 classesand 5.83% for 3 classes.
This suggests that using acombination of features from the context and targetword in addition to features from the whole utter-ance leads to better prediction of the perceived levelof certainty than using features from only the wholeutterance.One might argue that these differences are justdue to noise.
To address this issue, we comparedthe prediction accuracies of sets (A) and (E) per fold.This is illustrated in Figure 2.
Each fold in our cross-validation corresponds to a different speaker, so thefolds are not identically distributed and we do notexpect each fold to yield the same prediction accu-racy.
That means that we should compare predic-tions of the two feature sets within folds rather thanbetween folds.
Figure 2 shows the correlations be-tween the predicted and perceived levels of certaintyfor the models trained on sets (A) and (E).
The com-bination set (E) predictions were more strongly cor-related than whole utterance set (A) predictions in16 out of 20 folds.
This result supports our claimthat using a combination of features from the con-text and target word in addition to features from thewhole utterance leads to better prediction of level ofcertainty.Our best prediction accuracy for the 3 class case,74.79%, was slightly lower than the accuracy re-ported by Liscombe et al (2005), 76.42%.
However,our difference from the naive baseline was 18.54%where Liscombe et al?s was 10.42%.
Liscombe etal.
randomly divided their data into training and testsets, so it is unclear whether they tested on seen orunseen speakers.
Further, they ran one experimentrather than a cross-validation, so their reported ac-curacy may not be indicative of the entire data set.We also trained support vector models on thesesubsets of features.
The main result was the same:107Table 1: Average prediction accuracies for the linear regression models trained on five subsets of prosodic features.The models trained on the Combination feature set and the All feature set perform better than the other three modelsin both the 3- and 5-class settings.Feature Set Num Features Accuracy (5 classes) Accuracy (3 classes)Naive Baseline N/A 31.46% 56.25%(A) Utterance 20 39.00% 68.96%(B) Target Word 20 43.13% 68.96%(C) Context 20 37.71% 67.50%(D) All 60 48.54% 74.58%(E) Combination 20 45.42% 74.79%Fold UTT COMBI13 0.60742805 0.74174205 1 0.13431410 0.71345083 0.84506209 1 0.131611272 0.65645441 0.7745844 1 0.1181299920 0.59862684 0.69875998 1 0.1001331419 0.61302941 0.70460363 1 0.091574223 0.67823016 0.75606366 1 0.07783355 0.54426476 0.61711862 1 0.072853864 0.74102672 0.81252066 1 0.0714939417 0.71910042 0.77176522 1 0.05266486 0.78220835 0.82806993 1 0.0458615818 0.66737245 0.71009756 1 0.0427251115 0.66996149 0.70962379 1 0.039662319 0.63477603 0.66365739 1 0.028881371 0.7359401 0.7631083 1 0.027168217 0.71645922 0.73071498 1 0.0142557512 0.78824649 0.79491313 1 0.0066666411 0.39557157 0.39381644 0 -0.0017551316 0.62168851 0.61984036 0 -0.0018481514 0.6971148 0.67762751 0 -0.019487298 0.82685033 0.80103581 0 -0.025814521600.20.40.60.810 2 4 6 8 10 12 14 16 18 20FoldCorrelation Coeff (R) CombinationUtteranceFigure 2: Correlations with perceived level of certaintyper fold for the Combination (O) and the Utterance (X)feature set predictions, sorted by the size of the difference.In 16 of the 20 experiments, the correlation coefficientsfor the Combination feature set are greater than those ofthe Utterance feature set.the set of all features (D) and the combination set(E) had better prediction accuracies than the utter-ance feature set (A).
In addition, the combination set(E) had the best prediction accuracies (of all models)in both the 3- and 5-class settings.
The raw accura-cies were approximately 5% lower than those of thelinear regression models.5 Conclusion and Future WorkThe results of our experiments suggest a better pre-dictive model of level of certainty for systems wherewords or phrases likely to cause uncertainty areknown ahead of time.
Without increasing the totalnumber of features, combining select prosodic fea-tures from the target word, the surrounding contextand the whole utterance leads to better prediction oflevel of certainty than using features from the wholeutterance only.
In the near future, we plan to exper-iment with prediction models of the speaker?s self-reported level of certainty.AcknowledgmentsThis work was supported by a National Defense Sci-ence and Engineering Graduate Fellowship.ReferencesAbeer Alwan, Yijian Bai, Matthew Black, et al 2007.
Asystem for technology based assessment of languageand literacy in young children: the role of multiple in-formation sources.
Proc.
of IEEE International Work-shop on Multimedia Signal Processing, pp.
26?30,Chania, Greece.Jeremy Ang, Rajdip Dhillon, Ashley Krupski, et al2002.
Prosody-based automatic detection of annoy-ance and frustration in human-computer dialog.
Proc.of ICSLP 2002, pp.
2037?2040, Denver, CO.Kate Forbes-Riley, Diane Litman, and Mihai Rotaru.2008.
Responding to student uncertainty during com-puter tutoring: a preliminary evaluation.
Proc.
of the9th International Conference on Intelligent TutoringSystems, Montreal, Canada.Julia Hirschberg.
2003.
Intonation and pragmatics.
InL.
Horn and G. Ward (ed.
), Handbook of Pragmatics,Blackwell.Chul Min Lee and Shrikanth Narayanan.
2005.
Towardsdetecting emotions in spoken dialogs.
IEEE Transac-tions on Speech and Audio Processing, 13(2):293?303.Jackson Liscombe, Julia Hirschberg, and Jennifer Ven-ditti.
2005.
Detecting certainness in spoken tutorialdialogues.
Proceedings of Eurospeech 2005, Lisbon,Portugal.Heather Pon-Barry, Karl Schultz, Elizabeth Bratt, BradyClark, and Stanley Peters.
2006.
Responding to stu-dent uncertainty in spoken tutorial dialogue systems.International Journal of Artificial Intelligence in Edu-cation 16:171-194.Heather Pon-Barry.
2008.
Prosodic manifestations ofconfidence and uncertainty in spoken language.
Proc.of Interspeech 2008, pp.
74?77, Brisbane, Australia.108
