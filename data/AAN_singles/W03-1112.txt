AnyQ: Answer Set based Information Retrieval SystemHyo-Jung Oh, Myung-Gil Jang Moon-Soo ChangElectronics and Telecommunications Department of SoftwareResearch Institute (ETRI) Seokyeong UniversityDaejeon, Korea Seoul, Korea{ohj, mgjang}@etri.re.kr cosmos@skuniv.ac.kr1.
IntroductionThe goal of Information Retrieval (IR) is findinganswer suited to user question from massivedocument collections with satisfied response time.With the exponential growth of information on theWeb, user is expecting to find answer more fast withless effort.
Current IR systems especially focus onimproving precision the result rather than recall.
Anotable trend in IR is to provide more accurate,immediately usable information as in QuestionAnswering systems(Q/A) [1] or in some systemsusing pre-constructed question/answer documentpairs [2, 3], known ?answer set driven?
system.While traditional search engine uses term indexing,i.e.
tf*idf, answer approaches use syntactic, semanticand pragmatic knowledge provided expert, i.e.WordNet[4].
Another difference comes from the factthat answer approach returns ?answer set?
distilledinformation need of user as retrieval result, not justdocument appeared query terms.The TREC Q/A track [1, 5, 6] which hasmotivated much of the recent work in the fieldfocuses on fact-based, short-answer question type, e.g.
?Who is Barbara Jordan??
or ?What is Mardi Gras?.The Q/A runs find an actual answer in TRECcollection, rather than a ranked list of documents, inresponse to a question.
On the other hand, userqueries in answer set driven system, likeAskJeeves[2], are more implicit and conceptual.These system was developed targeting the Web [7, 8],is larger than the TREC Q/A document collection.Whereas the user gives incomplete query to system,they need not only answers but related information.Sometimes the user even has uncertainty whatexactly they need.
For example, the user query just?Paris?
is answered by gathering informationincluding Paris city guide, photographs of Paris, andso on.
To catch information need of user, thesesystem have pre-defined query pattern and preparedcorrect answers belonging to each question.
Since itis still considered difficult, if not impossible, tocapture semantics and pragmatics of sentences in userqueries and documents, such systems requireknowledge bases built manually so that a certain levelof quality can be guaranteed.
Needless to say, thisknowledge base construction process is labor-intensive, typically requiring significant andcontinuous human efforts [9].This paper rests on the both directions: a new typeof IR and its operational experience.
Our system,named ?AnyQ?1, attempts to provide high qualityanswer documents to user queries by maintaining aknowledge base consisting of expected queries andcorresponding answer document.
We defined thesemantic category of the answer as attributes and the1 http://anyq.etri.re.kr in koreanAbstractThe accuracy of IR result continues to grow onimportance as exponential growth of WWW, andit is therefore increasingly important thatappropriate retrieval technologies be developedfor the web.
We explore a new type of IR,?answer set based IR?, and its operationalexperience.
Our proposed approach attempts toprovide high quality answer documents to user bymaintaining a knowledge base with expectedqueries and corresponding answer document.
Wewill elaborate on our architecture and theexperimental results.Keywords: answer set driven IR, attribute-based classification, automatic knowledge baseconstruction,.Figure 1.
System architecture of Answer Set based IRdocuments associated with each attributes as answerset.
In order to reduce the cost of manuallyconstructing and maintaining answer sets, we havedevised a new method of automating the answerdocument selection process by using the automatictext categorization, reported ABC(Attribute-BasedClassification)[10].The rest of the paper is organized as follows.Section 2 presents overviews of our answer set drivenretrieval system and knowledge base.
In Section 3and 4 elaborates on answer set construction and itsretrieval process.
Section 5 details experiment resultsfor our method.
After discussing the limitations ofour approach in Section 6, we conclude bysummarizing our contributions and describing futureworks.2.
Answer Set based IR System2.1.
System OverviewSeveral approaches to find answer usinginformative knowledge from expert were reported [1,2, 3].
Most recent research proposed a new method ofcapturing the semantics of the question and thenpresents the document as answer, named ?answer setdriven IR.
The goal of these systems is to explorehow does map user question into answer documentthat might be contain pertinent information.
In thesesystems, it is crucial to devise a method to construct ahigh-quality knowledge base.
In our system, we takea hybrid approach of using a human-generatedconcept hierarchy and automatic classificationtechniques to make it more feasible to build anoperational system.Our system analyzes a user query to extractconcept and attribute terms that can be matchedagainst the knowledge base where a set of answerdocuments can be found.
As such, it has three parts:answer set construction, answer set search, answerpresentation, as illustrated in Figure 1.
The answerset construction part, which is seemed indexing partin traditional IR system, employees both manual andautomatic methods to build the knowledge base.
Theanswer set search part processes a natural languagequery, extracts concepts and attributes, and mapsthem to the knowledge base so that the answerdocuments associated with the <concept, attribute>pairs can be retrieve.
In the answer presentation part,the search result is presented with highlightedparagraphs considered to contain the answer to thequery.docum entsAnswer Set ConstructionM anual CollectionAuto ConstructionW ith ABC*Query Analys isQuery ?
ASM atchingAnswer Set SearchingDocum ent Analys isAnswer PresentationAnsw er E xtractionAnsw erInfo rm ationNatural Langu ageQueryKnow ledge BaseConceptNetw orkAttributeAnsw er Set*ABC: A ttribute-Based Classif icationFigure 2.
Concept, Attribute, and DocumentsGroup of concepts in concept network hierarchy and Distribution of attributes for conceptsTable 1.
Distribution of attributes for concepts in an equivalence class(?-relation)attributesconceptsDefinitionPolicyPayConsultationcaseProblemsKindsPurposeCurrentsituationRegulationsMeritsCalculationmethodsNegotiation#attributesIncentives O O O O O O O O O O 10Hourly wage O O O O O O 6Basic salary O O O O 4Service allowance O O O O O O O O 8Ability allowance O O O O O O O 7Bonus O O O O O O O O 82.2.
Knowledge BaseOur knowledge base consists of three parts: aconcept network, attributes associated with eachconcept, and answer set belonging to each <concept,attribute> pair.The concept network contains about 50,000conceptual word2 as in WordNet [11] with 6 lexico-semantic relations that are used to form a synsethierarchy.
By using the concept network, as alreadywell-know in the WordNet-related research [1, 4, 6],a semantic processing of questions becomes possible.The information mined from concept network guidesprocess of bridging inference between the query andthe expected answer.
Finding a place, i.e.
conceptnode, in the network for a query can be construed asunderstanding the meaning of the query.
Attribute setan intermediary as connecting concept network with2 Include 14,700 conceptual word in economy domainanswer documents.
The answer-set driven retrievalsystem maps a user query into one or more conceptsand further down to one or more attributes whereassociated documents can be picked up as the answerset.
Attributes play the role in subcategorizing thedocuments belonging to the concept node.
A set ofattributes chosen for a particular concept specifiesvarious aspects often mentioned in the documentsbearing the concept and serves as an intermediarybetween a concept and high-precision answerdocuments.
It should be noted that attributes are notinherently associated with a concept, but found in thedocuments addressing the concept.
For instance, as inFigure 2, the concept node for ?angel investment?would have attributes, ?definition?, ?strategy?,?prospects?
and ?merits?, that are aspects orcharacteristics of ?angel investment?
often mentionedin relevant documents.Figure 2 and table 1 represent groups of conceptsfrom different levels of the concept network and thedistribution of attributes, showing that someFinancial ActivitiesSavingsInvestmentTransactionAngelInvestmentDistributiveInvestment Investment ofForeignersHedge TradeCyberStock TradeOff-boardTransactionForeignCurrency DepositFixed-PeriodDepositJointInvestmentConcept NetworkAttributesAnswer SetDefinition Merits ProspectsStrategyDefinition Merits Problemsattributes are shared by some concepts while othersare unique to a concept.
The fact that some attributesare shared by all or most of the concepts belonging toa higher level concept allow us to assume that relatedconcepts share the same set of attributes.
Anotherassumption we employ is that because of theobservation that attributes tend to be found in theneighborhoods of some concept, the training data fora particular attribute under a given concept can beused for the same attribute under another concept.With these assumptions, we devised a method ofminimizing the training data construction effortsrequired for attribute-based classification, which isessential to select documents to be associated with<concept, attribute> pairs.
In order to re-use thetraining data constructed for a particular <concept,attribute> pair, we define ?-relation between twoconcepts.
Two concepts are said to have an ?-relation when the sets of associated attributes aresufficiently similar to each other.
A later sectiondescribes how this relation is used for the knowledgebase construction process.3.
Answer Set ConstructionAttributes are defined to exist for conceptscorresponding to categories in subject-basedclassification.
Documents classified to a concept areconsidered to possess one or more attributes thatreveal some characteristics or aspects of a document.Considering attributes as a different type ofcategories, we can define an attribute-based classifierfor documents.
While we employ the same learning-based and rule-based classification techniques forattribute-based classification, we underscore the wayit is used for automatic knowledge-base construction,together with traditional subject-based classification.It works on reducing human efforts dramatically toknowledge base construction.Our attribute-based classification method [10],at least as it is now, is no different from traditionaltext classification methods in that it uses trainingdocuments.
However, the task of knowledge baseconstruction for the answer set based retrieval systemcalls for unusual requirements.
Whereas categoriesare pretty much fixed in traditional classificationsystems, the number of attributes (i.e.
categories) fora given concept may change in our context.
Anotherdifference comes from the fact that it is not easy tohave a sufficient number of training documents foreach category since there are so many <concept,attribute> pairs that correspond to categories.In order to address the issues mentioned above,we decided to add two additional steps to theordinary statistical classification:- use of pattern rules in conjunction with theusual learning-based classification- selective use of words that may not bespecific for attributes- use of ?-relationWhile the first and second were chosen toimprove precision of the classifier, the third wasdevised to widen the coverage of <concept, attribute>pairs for which training documents are provided.
Inother words, the use of ?-relation allow us to re-use aclassifier learned from a set of training documentsbelonging to a concept for the same attribute classunder a different concept.To improve upon accuracy of our attribute-basedclassifier, we have employed both rule-based andlearning-based approaches.
Unlink the case ofsubject-based classification, attribute classboundaries are sometimes hard to detect if onlywords are used as features.
As such, we decided touse patterns of word sequences, not just single words.We have defined rules from train documents, whichexpress the characteristics of a given attribute class.Rules may include single words, phrases, sentences,or even paragraphs.
The pattern rules3 are used tocomplement the errors made by the machine learningmethod, and further it is reused in query processing.We take the approach of a hybrid system combiningrule-based classification and learning-basedclassification, with different weights are applied todifferent attributes.
Besides we detect that someterms are not good at discriminate attribute since theyare too specific to concept, whereas these terms arehelpful to classify in concept.
Therefore we eliminatethe terms that concept-dependent word whichfrequently appearing in a certain concept area.Another challenging problem in an operationalsetting is to define useful attributes to each of theconcept nodes and collect training documents foreach attribute under a concept node.
It would be tooexpensive and time-consuming to collect a sufficientnumber of training documents for all the classesrepresented by <concept, attribute> pairs.
Currentlythe number of classes is more than 250,0004.
We needa method by which we can assign an attribute to anew document without separate training documentsfor that particular <concept, attribute> pair.
Ourapproach to this problem is to use a special kind ofrelation, named ?-relation, defined over the concepts3 Currently, we define 83 kind of attributes and 1,938 attributepattern rule.4 More precisely, 14700*18, the number of concept nodes timesthe average number of attribute number of attributes under eachconcept.Figure 3.
Retrieval Processin the network.
The main idea is to build a classifierfor only one concept among the many belonging toan equivalence class based on the ?-relation, and useit for other concepts.
That is, we only need trainingdocuments for the representative class.
Table 1 showsa distribution of attributes among the concept in ?-relation.
Once the attributes are identified for the firsttwo concepts, ?Incentives?
and ?Hourly wage?, andtraining documents are selected for them, all theattributes except for the last one, ?negotiation?, canbe considered having training documents.
If wedefine attributes up to the third concept, ?Basicsalary?, all the attributes are covered.
The classifierlearned from a single set of documents belonging tothe concept would have the capability of classifyingdocuments to the attribute classes belonging to otherconcepts if the concepts are all ?-related.We first construct a training document set5 onlyfor a single concept node representing all those nodeswith the same attributes, using meta-search engineand document clustering.
So we can build a classifierfor those attributes that manifest themselves in thetraining set.
If some documents fail to be assigned toan attribute, they are assigned to one of the remainingattributes.
If a concept node other than therepresentative node in the equivalence class needs a.new attribute, we simply look for training documentsfor that attribute only.
This kind of incrementalprocess is based on our assumption that althoughattributes are associated with individual conceptnodes, they share the common characteristicsregardless of their parent concept nodes.
Undoubtedly,however, this assumption does not always hold.5 It is only 3% of total amount of training set we needed.4.
Retrieval Process4.1.
Answer Set SearchThe main task of answer set search process iscapturing a <concept, attribute> pair from naturallanguage query, and mapping them to the knowledgebase so that the answer documents can be retrieve.User query is represented as natural language so thatimply semantic information need, not just single term.The query processing distinguishes between the mainand additional terms from query.
The former coveythe essence of the query, reflected <concept,attribute> pairs.
The latter help to convey themeaning of the query but can be omitted withoutchanging the essence of the meaning.
The secondaryterms are useful clue for extracting answer sentencein answer document.
Predefined patterns are alsoimportant for query processing.
As noted earlier, wedefined attribute pattern rules for improving accuracyof attribute-based classification.
Then we rebuildthese patterns as query-attribute pattern6, expectingappeared pattern in interrogative form.
Queryprocessing consists of following part: 1) linguisticanalyzing, 2) concept focusing, and 3) determiningattribute, as illustrated in Figure 3.Given a query, ?what is the problem of angelinvestment?
?, we analyze the sentence structure, suchas conjunction structure and parallel phrases.
Wesegment complex query into simple sentence.
Wedistinguish the main terms in query by matching thelongest term in concept network for focusing concept.To determine attribute of query we first plainly look6 It was extended 2,170 query-attribute pattern.
?Knowledge BaseConceptNetworkAttributeAnswer SetQuery Linguistic analyzingConcept focusingDetermining attributeMain concept<concept, attribute> pairanswer setHighlighted paragraphScoring sentenceSelecting candidateadditional query termsexpanded conceptsExpanded answer setMain attributeUser!Table 2.
Result of Answer Set ConstructionAttribute setsAll attributes(no ?-relationsused)Pre-selectedattributes(with ?-relation)Precision .5025 .6020Recall .4662 .6696F-score .4835 .6358(+31.4%)Time 4 1Table 3.
Result of Answer Set RetrievalAS based IR Web IRTotal Top 5 Top 5 Top 10Precision 0.584 0.769 0.291 0.2864Recall 0.391 0.655 0.291 0.315F-score 0.468 0.797 0.291 0.3HighlightingMRR 0.78for the term in attribute synset, which included titleattribute representing all those synonym, i.e.?Problems?
is title of set {Warning, danger, abuse,damage}.
If not, we classify the question into one of83 categories, each of mapped to a particular set ofquery-attribute pattern.
Our example query map<angel investment, problem> pair.After extracting appropriate <concept, attribute>pair, query expansion is generated, connecting withrelated concept in concept network.
This expansion isbased on the assessment of similarity betweendistances of concept network.
The main advantage ofconnecting related concept is that the user can betraverse concept network through semantic path.Thus continuous search feedback can be possible.Expanded query map to knowledge base so that thedocuments corresponding the <concept, attribute>pairs can be retrieve as answer set.
The results wereranked using attributed-based classification score inanswer set construction processing.4.2.
Result Presentation: HighlightingAnswer SentenceFinding answer to a natural language questioninvolves not only knowing what the answer is butalso where the answer is.
The answer set thatproduced initial searching step is considered toinclude the candidate answer sentence.
For detectinganswer sentence, we extract all the possible <concept,attribute> pairs each sentence in answer document.The sentence was not include query pairs was discardso that we can get candidate sentences where answeris appeared.
Similarly query expansion, candidatesentences calculate score of match additional querywords, which is generated in query processing.Highest scoring sentence was highlighted includingits former and latter sentence.
Right side box inFigure 3 shows our retrieval process.5.
ExperimentsWhereas traditional Q/A and IR system havecompetition conference, like TREC, so that they canstart with standard retrieval test collection, to explorehow useful the proposed approach, we evaluateperformance of answer document and candidateanswer sentence.
Another difference comes from thefact that result units for these systems are different.That is Q/A system returns exactly relevant answer(50 byte or 250 byte), while IR system returnsdocument scored by ranking mechanism.
Our systemreturns ?answer set?
distilled semantic knowledge asretrieval result5.1.
Automatic Answer Set ConstructionBefore evaluating our retrieval system, we wereinterested in knowing how effective and efficient theproposed knowledge base construction method.
Wetested the attribute-based classification for automaticconstruction method with 4,599 documents, 120concepts, and 83 attributes.
For performancecomparisons, we used the standard precision, recall,and F-score [12].
Table 2 shows that the scores forusing ?-relation are higher than that of not using therelation.
We gain a 31.4% increase in F-score and400% in speed by using the knowledge.
The potentialadvantage of using the ?-relation is the ability tominimize the efforts not only required training setcothchaugr5.opA83doav43anpequnstruction but also new answer set construction.
One other hand, a disadvantage is that it has a lessance to assign new attributes.
The result ourtomatic answer set construction was to establish aound work for further experiments.2.
Performance of Answer set retrievalFor our experimental evaluations we constructederational system in Web, named ?AnyQ?.
OurnyQ system currently consists of 14,700 concepts,unique attributes, and more than 1.8 million webcuments in the economy domain for Korean.
Theerage number of document under each concept is.4, the average number of answer document is 25,d the average number of attribute is 18.
To measurerformance of retrieving answer set, we build 110ery-relevant answer set, judged by 4 assessor.
Ourassessors team with 2 people.
For performancecomparisons, we used the P, R, F-score and MRR[5]for highlighted sentence.
All retrieval runs arecompletely automatic, starting with queries, retrieveanswer documents, and finally generating a rankedlist of 5 candidate answer sentence.We build traditional Web IR system on the samedocument set for baseline system.
The Web IRsystem uses 2-poisson model for term indexing andvector space model for document retrieving.
Table 3summarized the effectiveness of our initial searchstep, answer set search.
As expected, we obtainedbetter results progressively as answer set basedapproach.
The accuracy of Web IR become highertop10(0.31) to top5(0.291) when we determine morenumber of documents retrieved.
By contrast, ASbased IR has improvement both precision(0.769) andrecall(0.655) when we assess less number ofdocuments on top ranked.
Even when all documentswas considered(0.468) is higher than Web IR top10(0.3).
It comes from the fact that Web IR retrievesmassive documents appeared term query.
But ASbased IR handled prepared answer set.
That is, ASbased IR tend to set highly relevant documents on topresult.
In other words, answer set based approach canbe easier for user to find information they need withless effort.To evaluate highlighted paragraphs, we generatea ranked list of 5 candidate answer sentencesconsidered to contain the answer to the query.
Thescore is 0.78 MRR.
As mentioned before, our result isnot the same type as TREC answer.
But we can saythat highlighted sentences are helpful to satisfy userinformation need.We further realized that the query pattern asattribute was not sufficient for finding answer.Moreover, Korean has characteristic, variousvariation of same pattern, its duplicate over theattributes.
It brings the fact that query processing hasambiguity.
Another weakness of our system is thatthe accuracy of retrieval depends on knowledge basegranularity.
That is, the effectiveness of attribute-based classification influences whole process of ourapproach.Unfortunately, Our experience cannot comparewith other commercial system since there is nostandard test collection.
By the way AskJeeves waspublished their accuracy of retrieval is over30~40%[7], however, this is not absolute contrast.6.
ConclusionThe accuracy of IR result continues to grow onimportance as exponential growth of WWW, and it istherefore increasingly important that appropriateretrieval technologies be developed for the web.
Wehave introduced a new type of IR, ?Answer Set basedIR?, attempts to provide high quality answerdocuments to user queries.In the context of answer set-driven text retrieval,it is crucial to capture semantics and pragmatics ofsentences in user queries and documents.
In our case,we defined the semantic category of the answer asattributes, the documents associated with eachattributes as answer set.
We attempted to providemore accurate answers by attaching attributes toindividual concepts in concept network.
In order toconstruct knowledge bases, a certain level of qualityis guaranteed, we developed a new method forattributed-based classifier(ABC) and built attributepattern for improving accuracy of ABC and queryprocessing both.
In retrieval, we process a naturallanguage query, extract concepts and attributes, andmap them to the knowledge base so that the answerdocuments associated with the <concept, attribute>pairs can be retrieve.Our proposed IR ranked highly relevantdocument on top result, thus it helps reducing humanefforts dramatically to find answer.
By establishedoperational system, named ?AnyQ?, our experimentshowed realistic possibility of our approachsystematically.While our experiments were designed carefully,and comparisons made thoroughly, it has limitations.Our current work depends on the domain of theconcept network.
It is not clear how the proposedmethod can be extended to other domains.
Ourassumption, reflecting semantics in sentence to<concept, attribute> pairs, needs to be tested further.More fundamentally, we need a certain amount ofmanual work to initially construct the knowledgebase such as the concept hierarchy and the initialtraining documents.
We will have to see how theinitial manual process influences the latter processesand what kind of performance degradation occurswhen smaller efforts are used for the initialconstruction.7.
Reference[1] Dan Moldovan, Sanda Harabagiu, et al ?LCCTool for Question Answering?, Proc of TextRetrieval Conference (TREC-11), November,2002.
[2] Ask Jeevestm,http://www.jeevessolutions.com/technology/[3] M. G. Jang, H. J. Oh, M. S. Chang, et al?Semantic Based Information Retrieval?, KoreaInformation Science Society review, 19(10):7-18,October 2001.
[4] Marius Pasca and Sanda M. Harabagiu, ?TheInformative Role of WordNet in Open-DomainQuestion Answering?, Proc of the NAACL 2001Workshop on WordNet and Other LexicalResource, pp 138-143, CMU, Pittsburge PA,June 2001[5] Ellen M. Voorhees, ?Overview of the TREC 2000Question Answering Track?, Proc of TextRetrieval Conference (TREC-11), November,2002[6] Eduard Hovy, Ulf Hermjakob, and Chin-Yew Lin,?The Use of External Knowledge in Factoid QA?,Proc of Text Retrieval Conference (TREC-10),November, 2001[7] Cody C. T. Kwok, Oren Etzioni, and Daniel S.Weld, ?Scaling Question Answering to theWeb?, , Proc.
of the 10th annual internationalACM WWW10, pp.
150-161, 2001[8] Susan Dumais, Michele Banko, Eric Brill, JimmyLin, and Andrew Ng, ?Web Question Answering:Is More Always Better?
?, Proc.
of the 25thannual international ACM SIGIR ?2002, pp.
291-298, Tampere, Finland, 2002[9] Andrew McCallum, Kamal Nigam, et al, ?AMachine Learning Approach to BuildingDomain-Specific Search Engines?, Proc.
of the16th IJCAI Conference, pp 662-667, 1999[10] Hyo-Jung Oh, Moon-Su Chang, Myung-Gil Jang,and Sung-Hyon Myaeng, ?Integrating Attribute-Based Classification for Answer SetConstruction?, Proc.
of the 25th annualinternational ACM SIGIR ?2002 2nd workshopon Operational Text Classification, Tampere,Finland, 2002[11] Christiane Fellbaum, ?WordNet : An ElectronicLexical Database?, The MIT press, 1998[12] Ricardo Baeza-Yates, Berthier Ribeiro-Neto,?Modern Information Retrieval?, Yiming Yangand Xin Liu, ?A Re-examination Of TextCategorization Methods?, pp.
73~98, Addition-Wesley Published, ACM press New York, 1999.
