Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 181?184,New York, June 2006. c?2006 Association for Computational LinguisticsImproved Affinity Graph Based Multi-Document SummarizationXiaojun Wan, Jianwu YangInstitute of Computer Science and Technology, Peking UniversityBeijing 100871, China{wanxiaojun, yangjianwu}@icst.pku.edu.cnAbstractThis paper describes an affinity graphbased approach to multi-document sum-marization.
We incorporate a diffusionprocess to acquire semantic relationshipsbetween sentences, and then compute in-formation richness of sentences by agraph rank algorithm on differentiated in-tra-document links and inter-documentlinks between sentences.
A greedy algo-rithm is employed to impose diversitypenalty on sentences and the sentenceswith both high information richness andhigh information novelty are chosen intothe summary.
Experimental results ontask 2 of DUC 2002 and task 2 of DUC2004 demonstrate that the proposed ap-proach outperforms existing state-of-the-art systems.1 IntroductionAutomated multi-document summarization hasdrawn much attention in recent years.
Multi-document summary is usually used to provide con-cise topic description about a cluster of documentsand facilitate the users to browse the documentcluster.
A particular challenge for multi-documentsummarization is that the information stored indifferent documents inevitably overlaps with eachother, and hence we need effective summarizationmethods to merge information stored in differentdocuments, and if possible, contrast their differ-ences.A variety of multi-document summarizationmethods have been developed recently.
In thisstudy, we focus on extractive summarization,which involves assigning saliency scores to someunits (e.g.
sentences, paragraphs) of the documentsand extracting tKe sentences with highest scores.MEAD is an implementation of the centroid-basedmethod (Radev et al, 2004) that scores sentencesbased on sentence-level and inter-sentence features,including cluster centroids, position, TF*IDF, etc.NeATS (Lin and Hovy, 2002) selects importantcontent using Ventence position, term frequency,topic signature and term clustering, and then usesMMR (Goldstein et al, 1999) to remove redun-dancy.
XDoX (Hardy et al, 1998) identifies themost salient themes within the set by passage clus-tering and then composes an extraction summary,which reflects these main themes.
Harabagiu andLacatusu (2005) investigate different topic repre-sentations and extraction methods.Graph-based methods have been proposed torank sentences or passages.
Websumm (Mani andBloedorn, 2000) uses a graph-connectivity modeland operates under the assumption that nodeswhich are connected to many other nodes are likelyto carry salient information.
LexPageRank (Erkanand Radev, 2004) is an approach for computingsentence importance based on the concept of ei-genvector centrality.
Mihalcea and Tarau (2005)also propose similar algorithms based on PageR-ank and HITS to compute sentence importance fordocument summarization.In this study, we extend the above graph-basedworks by proposing an integrated framework forconsidering both information richness and infor-mation novelty of a sentence based on sentenceaffinity graph.
First, a diffusion process is imposedon sentence affinity graph in order to make the af-finity graph reflect true semantic relationships be-tween sentences.
Second, intra-document links andinter-document links between sentences are differ-entiated to attach more importance to inter-document links for sentence information richnesscomputation.
Lastly, a diversity penalty process isimposed on sentences to penalize redundant sen-tences.
Experiments on DUC 2002 and DUC 2004data are performed and we obtain encouraging re-sults and conclusions.1812 The Affinity Graph Based ApproachThe proposed affinity graph based summarizationmethod consists of three steps: (1) an affinity graphis built to reflect the semantic relationship betweensentences in the document set; (2) informationrichness of each sentence is computed based on theaffinity graph; (3) based on the affinity graph andthe information richness scores, diversity penalty isimposed to sentences and the affinity rank scorefor each sentence is obtained to reflect both infor-mation richness and information novelty of thesentence.
The sentences with high affinity rankscores are chosen to produce the summary.2.1 Affinity Graph BuildingGiven a sentence collection S={si | 1?i?n}, the af-finity weight aff(si, sj) between a sentence pair of siand sj  is calculated using the cosine measure.
Theweight associated with term t is calculated with thetft*isft formula, where tft is the frequency of term tin the corresponding sentence and isft is the inversesentence frequency of term t, i.e.
1+log(N/nt),where N is the total number of sentences and nt isthe number of sentences containing term t.  If sen-tences are considered as nodes, the sentence collec-tion can be modeled as an undirected graph bygenerating the link between two sentences if theiraffinity weight exceeds 0, i.e.
an undirected linkbetween si and sj (i?j) with affinity weight aff(si,sj)is constructed if aff(si,sj)>0; otherwise no link isconstructed.
Thus, we construct an undirectedgraph G reflecting the semantic relationship be-tween sentences by their content similarity.
Thegraph is called as Affinity Graph.
We use an adja-cency (affinity) matrix M to describe the affinitygraph with each entry corresponding to the weightof a link in the graph.
M = (Mi,j)n?nis defined asfollows:)s,s(affM jij,i  (1)Then M is normalized to make the sum of eachrow equal to 1.
Note that we use the same notationto denote a matrix and its normalized matrix.However, the affinity weight between two sen-tences in the affinity graph is currently computedsimply based on their own content similarity andignore the affinity diffusion process on the graph.Other than the direct link between two sentences,the possible paths with more than two steps be-tween the sentences in the graph also convey moreor less semantic relationship.
In order to acquirethe implicit semantic relationship between sen-tences, we apply a diffusion process Kandola etal., 2002 on the graph to obtain a more appropri-ate affinity matrix.
Though the number of possiblepaths between any two given nodes can grow ex-ponentially, recent spectral graph theory (Kondorand Lafferty, 2002) shows that it is possible tocompute the affinity between any two given nodesefficiently without examining all possible paths.The diffusion process on the graph is as follows:t1t1t~ MM-?f  J(2)where ?
(0<?<1) is the decay factor set to 0.9.is the t-th power of the initial affinity matrixand the entry in it is given bytMM?
?
?ju,iun}{1,...,u1t1u,utjit1t1MM""",(3)that is the sum of the products of the weights overall paths of length t that start at node i and finish atnode j in the graph on the examples.
If the entriessatisfy that they are all positive and for each nodethe sum of the connections is 1, we can view theentry as the probability that a random walk begin-ning at node i reaches node j after t steps.
The ma-trix M is normalized to make the sum of each rowequal to 1. t is limited to 5 in this study.~2.2 Information Richness ComputationThe computation of information richness of sen-tences is based on the following three intuitions: 1)the more neighbors a sentence has, the more in-formative it is; 2) the more informative a sen-tence?s neighbors are, the more informative it is; 3)the more heavily a sentence is linked with otherinformative sentences, the more informative it is.Based on the above intuitions, the informationrichness score InfoRich(si) for a sentence si can bededuced from those of all other sentences linkedwith it and it can be formulated in a recursive formas follows:?z?
?ijalli,jji n)d1(M~)s(InfoRichd)s(InfoRich (4)And the matrix form is:en)d1(~d T &&&  OO M (5)182where 1ni )]s(InfoRich[ u O&is the eigenvector of.
is a unit vector with all elements equalingto 1. d is the damping factor set to 0.85.T~M e&Note that given a link between a sentence pair ofsi and sj, if si and sj comes from the same document,the link is an intra-document link; and if si and sjcomes from different documents, the link is an in-ter-document link.
We believe that inter-documentlinks are more important than intra-document linksfor information richness computationDifferentweights are assigned to intra-document links andinter-document links respectively, and the new af-finity matrix is:interintra~~?
MMM ED  (6)where intra~M is the affinity matrix containing onlythe intra-document links (the entries of inter-document links are set to 0) and inter~M is the affin-ity matrix containing only the inter-document links(the entries of intra-document links are set to 0).
?,?
are weighting parameters and we let 0?
?, ?
?1.7he matrix is normalized and now the matrix  isreplaced by  in Equations (4) and (5).M~M?2.3 Diversity Penalty ImpositionBased on the affinity graph and obtained informa-tion richness scores, a greedy algorithm is appliedto impose the diversity penalty and compute thefinal affinity rank scores of sentences as follows:1.
Initialize two sets A=?, B={si | i=1,2,?,n}, andeach sentence?s affinity rank score is initialized toits information richness score, i.e.
ARScore(si) =InfoRich(si), i=1,2,?n.2.
Sort the sentences in B by their current affinity rankscores in descending order.3.
Suppose si is the highest ranked sentence, i.e.
thefirst sentence in the ranked list.
Move sentence sifrom B to A, and then a diversity penalty is im-posed to the affinity rank score of each sentencelinked with si as follows:For each sentence sj  in B, we have)InfoRich(sM~?
)ARScore(s)ARScore(s iij,jj ?
? (7)where ?>0 is the penalty degree factor.
The larger?
is, the greater penalty is imposed to the affinityrank score.
If ?=0, no diversity penalty is imposedat all.4.
Go to step 2 and iterate until B= ?
or the iterationcount reaches a predefined maximum number.After the affinity rank scores are obtained for allsentences, the sentences with highest affinity rankscores are chosen to produce the summary accord-ing to the summary length limit.3 Experiments and ResultsWe compare our system with top 3 performingsystems and two baseline systems on task 2 ofDUC 2002 and task 4 of DUC 2004 respectively.ROUGE (Lin and Hovy, 2003) metrics is used forevaluation1 and we mainly concern about ROUGE-1.
The parameters of our system are tuned on DUC2001 as follows: ?=7, ?=0.3 and ?=1.We can see from the tables that our system out-performs the top performing systems and baselinesystems on both DUC 2002 and DUC 2004 tasksover all three metrics.
The performance improve-ment achieved by our system results from threefactors: diversity penalty imposition, intra-document and inter-document link differentiationand diffusion process incorporation.
The ROUGE-1 contributions of the above three factors are0.02200, 0.00268 and 0.00043 respectively.System ROUGE-1 ROUGE-2 ROUGE-WOur System 0.38125 0.08196 0.12390S26 0.35151 0.07642 0.11448S19 0.34504 0.07936 0.11332S28 0.34355 0.07521 0.10956Coverage Baseline 0.32894 0.07148 0.10847Lead Baseline 0.28684 0.05283 0.09525Table 1.
System comparison on task 2 of DUC 2002System ROUGE-1 ROUGE-2 ROUGE-WOur System 0.41102 0.09738 0.12560S65 0.38232 0.09219 0.11528S104 0.37436 0.08544 0.11305S35 0.37427 0.08364 0.11561Coverage Baseline 0.34882 0.07189 0.10622Lead Baseline 0.32420 0.06409 0.09905Table 2.
System comparison on task 2 of DUC 2004Figures 1-4 show the influence of the parametersin our system.
Note that ?
: ?
denotes the real val-ues ?
and ?
are set to.
?w/ diffusion?
is the systemwith the diffusion process (our system) and  ?w/odiffusion?
is the system without the diffusion proc-1 We use ROUGEeval-1.4.2 with ?-l?
or ?-b?
option for trun-cating longer summaries, and ?-m?
option for word stemming.183ess.
The observations demonstrate that ?w/ diffu-sion?
performs better than ?w/o diffusion?
for mostparameter settings.
Meanwhile, ?w/ diffusion?
ismore robust than ?w/o diffusion?
because theROUGE-1 value of ?w/ diffusion?
changes lesswhen the parameter values vary.
Note that in Fig-ures 3 and 4 the performance decreases sharplywith the decrease of the weight ?
of inter-document links and it is the worst case when inter-document links are not taken into account (i.e.
?
:?=1:0), while if intra-document links are not takeninto account (i.e.
?
:?=0:1), the performance is stillgood, which demonstrates the great importance ofinter-document links.        528*(ZRGLIIXVLRQZGLIIXVLRQZFigure 1.
Penalty factor tuning on task 2 of DUC 2002               528*(ZRGLIIXVLRQZGLIIXVLRQZFigure 2.
Penalty factor tuning on task 2 of DUC 2004528*(ZRGLIIXVLRQZGLIIXVLRQD EFigure3.
Intra- & Inter-document link weight tuning ontask 2 of DUC 2002528*(ZRGLIIXVLRQZGLIIXVLRQD EFigure 4.
Intra- & Inter-document link weight tuning ontask 2 of DUC 2004ReferencesG.
Erkan and D. Radev.
LexPageRank: prestige in multi-document text summarization.
In Proceedings ofEMNLP?04J.
Goldstein, M. Kantrowitz, V. Mittal, and J. Carbonell.Summarizing Text Documents: Sentence Selection andEvaluation Metrics.
Proceedings of SIGIR-99.S.
Harabagiu and F. Lacatusu.
Topic themes for multi-document summarization.
In Proceedings of SIGIR?05,Salvador, Brazil, 202-209, 2005.H.
Hardy, N. Shimizu, T. Strzalkowski, L. Ting, G. B. Wise,and X. Zhang.
Cross-document summarization by con-cept classification.
In Proceedings of SIGIR?02, Tampere,Finland, 2002.J.
Kandola, J. Shawe-Taylor, N. Cristianini.
Learning semanticsimilarity.
In Proceedings of NIPS?2002.K.
Knight and D. Marcu.
Summarization beyond sentenceextraction: a probabilistic approach to sentence compres-sion, Artificial Intelligence, 139(1), 2002.R.
I. Kondor and J. Lafferty.
Diffusion kernels on graphs andother discrete structures.
In Proceedings of ICML?02.C.-Y.
Lin and E.H. Hovy.
Automatic Evaluation of Summa-ries Using N-gram Co-occurrence Statistics.
In Proceed-ings of HLT-NAACL 2003.C.-Y.
Lin and E.H. Hovy.
From Single to Multi-documentSummarization: A Prototype System and its Evaluation.In Proceedings of ACL-02.I.
Mani and E. Bloedorn.
Summarizing Similarities and Dif-ferences Among Related Documents.
Information Re-trieval, 1(1), 2000.R.
Mihalcea and P. Tarau.
A language independent algorithmfor single and multiple document summarization.
In Pro-ceedings of IJCNLP?05.D.
R. Radev, H. Y. Jing, M. Stys and D. Tam.
Centroid-basedsummarization of multiple documents.
Information Proc-essing and Management, 40: 919-938, 2004.184
