Polynomial Time and Space Shift-Reduce Parsingof Arbitrary Context-free Grammars.
*Yves SchabesDept .
of Computer  & In fo rmat ion  ScienceUnivers i ty  of Pennsy lvan iaPh i ladelphia ,  PA 19104-6389, USAe-mail: schabes~l inc .c i s .upenn.eduAbst ractWe introduce an algorithm for designing a predictiveleft to right shift-reduce non-deterministic push-downmachine corresponding to an arbitrary unrestrictedcontext-free grammar and an algorithm for efficientlydriving this machine in pseudo-parallel.
The perfor-mance of the resulting parser is formally proven to besuperior to Earley's parser (1970).The technique mployed consists in constructingbefore run-time a parsing table that encodes a non-deterministic machine in the which the predictive be-havior has been compiled out.
At run time, the ma-chine is driven in pseudo-parallel with the help of achart.The recognizer behaves in the worst case inO(IGI2n3)-time and O(IGIn2)-space.
However inpractice it is always superior to Earley's parser sincethe prediction steps have been compiled before run-time.Finally, we explain how other more efficient vari-ants of the basic parser can be obtained by deter-minizing portionsof the basic non-deterministic push-down machine while still using the same pseudo-parallel driver.1 In t roduct ionPredictive bottom-up arsers (Earley, 1968; Earley,1970; Graham et al, 1980) are often used for naturallanguage processing because of their superior averageperformance compared to purely bottom-up arsers*We are extremely indebted to Fernando Pereira and StuartShleber for providing valuable technical comments during dis-cussions about earlier versio/m of this algorithm.
We are alsograteful to Aravind Joehi for his support of this research.
Wealso thank Robert Frank.
All remaining errors are the author'sresponsibility alone.
This research wa~ partially funded byARO grant DAAL03-89-C0031PRI and DARPA grant N00014-90-J-1863.such as CKY-style parsers (Kasami, 1965; Younger,1967).
Their practical superiority is mainly obtainedbecause of the top-down filtering accomplished by thepredictive component of the parser.
Compiling outas much as possible this predictive component beforerun-time will result in a more efficient parser so longas the worst case behavior is not deteriorated.Approaches in this direction have been investigated(Earley, 1968; Lang, 1974; Tomita, 1985; Tomita,1987), however none of them is satisfying, either be-cause the worst case complexity is deteriorated (worsethan Earley's parser) or because the technique is notgeneral.
Furthermore, none of these approaches havebeen formally proven to have a behavior superior towell known parsers uch as Earley's parser.Earley himself (\[1968\] pages 69-89) proposed to pre-compile the state sets generated by his algorithm tomake it as efficient as LR(k) parsers (Knuth, 1965)when used on LR(k) grammars by precomputing allpossible states ets that the parser could create.
How-ever, some context-free grammars, including mostlikely most natural language grammars, cannot becompiled using his technique and the problem ofknowing if a grammar can be compiled with this tech-nique is undecidable (Earley \[1968\], page 99).Lang (1974) proposed a technique for evaluatingin pseudo-parallel non-deterministic push down au-tomata.
Although this technique achieves a worstcase complexity of O(n3)-time with respect to thelength of input, it requires that at most two symbolsare popped from the stack in a single move.
When thetechnique is used for shift-reduce parsing, this con-straint requires that the context-free grammar is inChomsky normal form (CNF).
As far as the grammarsize is concerned, an exponential worst case behavioris reached when used with the characteristic LR(0)106machine.
1Tomita (1985; 1987) proposed to extend LR(0)parsers to non-deterministic context-free grammarsby explicitly using a graph structured stack whichrepresents he pseudo-parallel evaluation of the movesof a non-deterministic LR(0) push-down automaton.Tomita's encoding of the non-deterministic push-down automaton suffers from an exponential timeand space worst case complexity with respect o theinput length and also with respect o the grammarsize (Johnson \[1989\] and also page 72 in Tomita\[1985\]).
Although Tomita reports experimental datathat seem to show that the parser behaves in practicebetter than Earley's parser (which is proven to takein the worst case O(\[G\[2n3)-time), the duplication ofthe same experiments shows no conclusive outcome.Modifications to Tomita's algorithm have been pro-posed in order to alleviate the exponential complex-ity with respect o the input length (Kipps, 1989) but,according to Kipps, the modified algorithm does notlead to a practical parser.
Furthermore, the algorithmis doomed to behave in the worst case in exponentialtime with respect o the grammar size for some am-biguous grammars and inputs (Johnson, 1989).
2 Sofar, there is no formal proof showing that the Tomita'sparser can be superior for some grammars and in-puts to Earley's parser, and its worst case complexityseems to contradict he experimental data.As explained, the previous attempts to compilethe predictive component are not general and achievea worst case complexity (with respect to the gram-mar size and the input length) worse than standardparsers.The methodology we follow in order to compile thepredictive component of Earley's parser is to definea predictive bottom-up pushdown machine equiva-lent to the given grammar which we drive in pseudo-parallel.
Following Johnson's (1989) argument, anyparsing algorithm based on the LR(0) characteris-tic machine is doomed to behave in exponential timewith respect o the grammar size for some ambigu-ous grammars and inputs.
This is a result of the factthat the number of states of an LR(0) characteristicmachine can be exponential and that there are somegrammars and inputs for which an exponential num-ber of states must be reached (See Johnson \[1989\] forexamples of such grammars and inputs).
One musttherefore design a different pushdown machine which1 The same arguraent for the exponential graramar size com-plexity of Tomita's parser (Johnson, 1989) holds for Lang'stechnique.2 This problem is particularly acute for natural language pro-cessing since in this context he input length is typically small(10-20 words) and the granunar size very large (hundreds orthousands of rules and symbols).can be driven efficiently in pseudo-parallel.We construct a non-deterministic predictive push-down machine given an arbitrary context-free gram-mar whose number of states is proportional to the sizeof the grammar.
Then at run time, we efficiently drivethis machine in pseudo-parallel.
Even if all the statesof the machine are reached for some grammars andinputs, a polynomial complexity will still be obtainedsince the number of states is bounded by the gram-mar size.
We therefore introduce a shift-reduce driverfor this machine in which all of the predictive compo-nent has been compiled in the finite state control ofthe machine.
The technique makes no requirement onthe form of the context-free grammar and it behavesin the worst case as well as Earley's parser (Earley,1970).
The push-down machine is built before run-time and it is encoded as parsing tables in the whichthe predictive behavior has been compiled out.In the worst case, the recognizer behaves in thesame O(\[Gl2nS)-time and O(\[G\[n2)-space as Earley'sparser.
However in practice it is always superiorto Earley's parser since the prediction steps havebeen eliminated before run-time.
We show that theitems produced in the chart correspond to equiva-lence classes on the items produced for the same inputby Earley's parser.
This mapping formally shows itspractical superior behavior.
3Finally, we explain how other more efficient vari-ants of the basic parser can be obtained by deter-minizing portions of the basic non-deterministic push-down machine while still using the same pseudo-parallel driver.2 The  ParserThe parser we propose handles any context-free gram-mar; the grammar can be ambiguous and need not bein any normal form.
The parser is a predictive shift-reduce bottom-up arser that uses compiled top downprediction information in the form of tables.
Beforerun-time, a non-deterministic push down automa-ton (NPDA) is constructed from a given context-freegrammar.
The parsing tables encode the finite statecontrol and the moves of the NPDA.
At run-time,the NPDA is then driven in pseudo-parallel with thehelp of a chart.
We show the construction of a basicmachine which will be driven non-deterministically.In the following, the input string is w -- a l .
.
.anand the context-free grammar being considered isG = (~, NT ,  P, S), where ~ is the set of terminal3The characteristic LR(0) machine is the result of deter-minizing the n~acldne we introduce.
Since this procedure in-troduce xponentially more states, the LR(0) machine can beexponentially large.107symbols, NT the set of non-terminal symbols, P aset of production rules, S the start symbol.
We willneed to refer to the subsequence of the input stringw = az.
.
.aN from position i to j ,  w\]i,j\], which wedefine as follows:f ai+l ... aj , if i < j w\]i,~\] I, ?
, i f i>_ jWe explain the data-structures u ed by the parser,the moves of the parser, and how the parsing tablesare constructed for the basic NPDA.
Then, we studythe formal characteristics of the parser.The parser uses two moves: shift and reduce.
As instandard shift-reduce parsers, shift moves recognizenew terminal symbols and reduce moves perform therecognition of an entire context-free rule.
However inthe parser we propose, shift and reduce moves behavedifferently on rules whose recognition has just started(i.e.
rules that have been predicted) than on rulesof which some portion has been recognized.
This be-havior enables the parser to efficiently perform reducemoves when ambiguity arises.2.1 Data -St ructures  and the Moves  ofthe ParserThe parser collects items into a set called the chart,C.
Each item encodes a well formed substring of theinput.
The parser proceeds until no more items canbe added to the chart C.An item is defined as a triple (s,i , j l ,  where s is astate in the control of the NPDA, i and j are indicesreferring to positions in the input string (i, j E \[0, n\]).In an item (s,i, j), j corresponds to the currentposition in the input string and i is a position in theinput which will facilitate the reduce move.A dotted rule of a context-free grammar G is definedas a production of G associated with a dot at someposition of the right hand side: A ~ a ?/~ withA --~ afl E P.We distinguish two kinds of dotted rules.
Kerneldotted rules, which are of the form A ~ a ?
fl with anon empty, and non-kernel dotted rules, which havethe dot at the left most position in the right handside (A --~ ?1~).
As we will see, non-kernel dottedrules correspond to the predictive component of theparser.We will later see each state s of the NPDA corre-sponds to a set of dotted rules for the grammar G.The set of all possible states in the control of theNPDA is written S. Section 2.2 explains how thestates are constructed.The algorithm maintains the following property(which guarantees its soundness)4: if an item (s, i,j)is in the chart C then for all dotted rules A ~ aofl E sthe following is satisfied:(i) if a E (E U NT) +, then B7 E (NT U ~)* suchthat S~w\]o, i \]A 7 and a=:=~w\]~d\];(ii) if a is the empty string, then B 7 E (NT O ~)*such that S=~w\]0./\]A 7.The parser uses three tables to determine whichmove(s) to perform: an action table, ACTION, andtwo goto tables, the kernel goto table, GOTOk, andthe non-kernel goto table, GOTOnk.The goto tables are accessed by a state and a non-terminal symbol.
They each contain a set of states:GOTO~(s,X) = {r},GOTOnk(s,X) = {r'} withr, rt,s E S ,X  E NT.
The use of these tables is ex-plained below.The action table is accessed by a state and a ter-minal symbol.
It  contains a set of actions.
Givenan item, (s, i,j), the possible actions are determinedby the content of ACTION(s,  aj+x) where aj+l is thej + 1 th input token.
The possible actions containedin ACTION(s,  aj+l)  are the following:?
KERNEL SHIFT s t, (ksh(s t) for short), for s t ES.
A new token is recognized in a kernel dottedrule A --* a ?
aft and a push move is performed.The item (s I, i , j  + 1) is added to the chart, sinceaa  spans in this case w\]i,j+l\].?
NON-KERNEL SHIFT  s t, (nksh(s I) for short),for s t E S. A new token is recognized in a non-kernel dotted rule of the form A --* ?aft.
Theitem (s ' , j , j  + 1) is is added to the chart, since aspans in this case wl j j+x \]?
REDUCE X ---.
fl, (red(X ---* fl) for short), forX --* ~ E P. The context-free rule X --*/~ hasbeen totally recognized.
The rule spans the sub-string ai+z .. .aj .
For all items in the chart of theform (s ~, k, i), perform the following two steps:- for all r l  E GOTOk(s',X), it adds the item(ra, k,j) to the chart.
In this case, a dottedrule of the form A ~ a ?
Xf l  is combinedwith X --* fl?
to form A ---* aX  ?/~; since aspans w\]k,i\] and X spans wli,j\], aX  spansw\]k,j\].- for all r2 E GOTOnk(s t, X),  it adds the item(r2,i,j) to the chart.
In this case, a dot-ted rule of the form A ~ ?
Xf~ is combinedwith X --~ fl?
to form A ~ X ?/~; in thiscase X spans w\]idl-4This property holds for all machines derived from the basicNPDA.108The recognizer follows:begin  (* recognizer *)Input:al * ?
?
anACTIONGOTO~GOTOnkstart E ,9.~ C ,q(* input string *)(* action table *)(* kernel goto table *)(* non-kernel goto table *)(* start state *)(* set of final states *)Output:acceptance or rejection of the inputstring.Initialization: C := {(start, O, 0)}Perform the following three operations until nomore items can be added to the chart C:(1) KERNEL SHIFT: if (s,i,j) 6 C andif ksh(s') 6 ACTION(s, aj+I), then(s', i, j + 1) is added to C.(2) NON-KERNEL SHIFT: if (s,i,j) e Cand if nksh(s') E ACTION(s, aj+I), then(s ' , j , j+ 1) is added to C.(3) REDUCE: if (s, i, j) E C, then for allX --~ j3 s.t.
red(X ~ ~) 6 ACTION(s, aj+t)and for all (s', k, i) E C, perform the follow-ing:?
for all r l  6 GOTO~(s',X), (rl,k,j) isadded to C;?
for all r2 E GOTOnk(s',X), (r~,i,j) isadded to C.If {(s, O, n) I (s, O, n) 6 C and s e .r} .# #then return acceptanceotherwise return rejection.end (* recognizer *)In the above algorithm, non-determinism arisesfrom multiple entries in ACTION(s, a) and also fromthe fact that GOTOk(s,X)and GOTOnk(s,X)con-tain a set of states.2 .2  Const ruct ion  o f  the  Pars ing  Tab lesWe shall give an LR(0)-like method for constructingthe parsing tables corresponding to the basic NPDA.Several other methods (such as LR(k)-like, SLR(k)-like) can also be used for constructing the parsingtables and are described in (Schabes, 1991).To construct the LR(0)-like finite state controlfor the basic non-deterministic push-down automatonthat the parser simulates, we define three functions,closure, gotok and gotonk.If s is a state, then closure(s) is the state con-structed from s by the two rules:(i) Initially, every dotted rule in s is added toclosure(s);(ii) If A --* a ?
B/~ is in closure(s) and B --* 7 is aproduction, then add the dotted rule B --* e7 toclosure(s) (if it is not already there).
This ruleis applied until no more new dotted rules can beadded to closure(s).If s is a state and if X is a non-terminal or terminalsymbol, gotok(s,X) and gotonk(s,X) are the set ofstates defined as follows:gotok(s, X )  ={c losure({A ?
A - *  ?
XZ  e sand a E (Z3 U NT) + }gotonk ( s, X ) ={closure({A X .,8))1 A ?
s}The goto functions we define differ from the one de-fined for the LR(0) construction i  two ways: first wehave distinguished transitions on symbols from ker-nel items and non-kernel items; second, each statein goto~(s,X) and gOtOn~(S,X) contains exactly onekernel item whereas for the LR(0) construction theymay contain more than one.We are now ready to compute the set of states ,9defining the finite state control of the parser.The SET OF STATES CONSTRUCTION is con-structed as follows:procedure states(G)beginS := {closure({S --, .~ I S - *  a e P})}repeatfor each state s in 8for each X E r~ u NT terminalfor  each r E gotok(s,X) Ugoton~(s, X)add r to Sunt i l  no more states can be added to 8endPARSING TABLES.
Now we construct the LR(0)parsing tables ACT ION,  GOTOk and GOTOnk fromthe finite state control constructed above.
Given acontext-free grammar G, we construct ~q, the set ofstates for G with the procedure given above.
We con-struct the action table ACT ION and the goto tablesusing the following algorithm.begin (CONSTRUCTION OF THE PARSING TABLES)Input: A context-free grammarG = (Y,, NT, P, S).Output: The parsing tables ACTION, GOTOkand GOTOnk for G, the start state start andthe set of final states ~'.109Step 1.
Construct 8 = {so, .
.
.
,  sin}, the set of statesfor G.Step 2.
The parsing actions for state si are deter-mined for all terminal symbols a E ~ as follows:(i) for all r e gotok(si,a), add ksh(r) toACTION(si,  a);(ii) for all r E goto, k(si,a), add nksh(r) to toACTION(si,  a);(iii) if A --* a* is in si, then add red(A--* a)to ACTION(si,  a) for all terminal symbol aand for the end marker $.Step 4.
The kernel and non-kernel goto tables forstate si are determined for all non-terminal sym-bols X as follows:(i) VX E NT, GOTO~(si,X) := gotok(si,X)(ii) VX E NT,GOTOnk(si, X) :-- gotonk(si, X)Step 3.
The start state of the parser isstart := ciosure({S --* .a I S --~ a ~_ P})Step 4.
The set of final states of the parser isY := {s e SI3 S--* a 6 P s.t.
S--.
a .
E s}end (CONSTRUCTION OF THE PARSING TABLES)Appendix A gives an example of a parsing table.3 Complex i tyThe recognizer equires in the worst case O(\[GIn2)-space and O(\[G\[2na)-time; n is the length of the inputstring, \]GI is the size of the grammar computed asthe sum of the lengths of the right hand side of eachproductions:\[GI = E \[a I , where la\] is the length of a.A-*a EPOne of the objectives for the design of the non-deterministic machine was to make sure that it wasnot possible to reach an exponential number of states,a property without which the machine is doomed tohave exponential complexity (Johnson, 1989).
Firstwe observe that the number of states of the finitestate control of the non-deterministic machine thatwe constructed in Section 2.2 is proportional to thesize of the grammar, IG\[.
By construction, each state(except for the start state) contains exactly one ker-nel dotted rule.
Therefore, the number of states isbounded by the maximum number of kernel rules ofthe form A --* ao/~ (with a non empty), and is O(IGI).We conclude that the algorithm requires in the worstcase O(IGIn~)-space since the maximum number ofitems (8, i, j)  in the chart is proportional to IGIn 2.A close look at the moves of the parser eveals thatthe reduce move is the most complex one since it in-volves a pair of states (s, i,j) and (s', k , j / .
This movecan be instantiated at most O(IGI2nS)-time sincei , j ,k E \[0, n\] and there are in the worst case O(IGI ~)pairs of states involved in this move.
5 The parsertherefore behaves in the worst case in O(IGI2nS)-time.One should however note that in order to bound theworst case complexity as stated above, arrays similarto the one needed for Earley's parser must be used toimplement efficiently the shift and reduce moves.
6As for Earley's parser, it can also be shown that thealgorithm requires in the worst case O(IGI2n2)-timefor unambiguous context-free grammars and behavesin linear time on a large class of grammars.4 Retr ieving a ParseThe algorithm that we described in Section 2 is a rec-ognizer.
However, if we include pointers from an itemto the other items (to a pair of items for the reducemoves or to an item for the shift moves) which causedit to be placed in the chart, the recognizer can bemodified to record all parse trees of the input string.The representation is similar to a shared forest.The worst case time complexity of the parser is thesame as for the recognizer (O(\[GI2n3)-time) but, asfor Earley's parser, the worst case space complexityincreases to O(\[G\[2n 3) because of the additional book-keeping.5 Correctness and Comparisonwith Earley's ParserWe derive the correctness of the parser by showinghow it can be mapped to Earley's parser.
In the pro-cess, we will also be able to show why this parser canbe more efficient han Earley's parser.
The detailedproofs are given in (Schabes, 1991).We are also interested in formally characterizingthe differences in performance between the parserwe propose and Earley's parser.
We show that theparser behaves in the worst scenario as well as Ear-ley's parser by mapping it into Earley's parser.
Theparser behaves better than Earley's parser because ithas eliminated the prediction step which takes in theworst case O(\]GIn)-time for Earley's parser.
There-fore, in the most favorable scenario, the parser weSKerael shift and non-kernel shift moves require both atmost O(IGIn 2 )-time.6Due to the lack of space, the details of the implementationare not given in this paper but they are given in (Schabes,1991).110propose will require O(IGln) less time than Earley'sparser.For a given context-free grammar G and an inputstring al .
- .an,  let C be the set of items produced bythe parser and CearZey be the set of items producedby Earley's parser.
Earley's parser (Earley, 1970)produces items of the form (A ---* a * ~, i, j) whereA --* a ?
~ is a single dotted rule and not a set ofdotted rules.The following lemma shows how one can map theitems that the parser produces to the items that Ear-ley's parser produces for the same grammar and in-put:Lemma 1 If Cs, i , j )  E C then we have:(i) for all kernel dotted rules A ~ a ?
~ E s, wehave C A ~ ct ?
~, i, j) E CearIey(ii) and for all non-kernel dotted rules A ---, *j3 Es, we have C A ~ ?~, j, j) E CearaevThe proof of the above lemma is by induction onthe number of items added to the chart C.This shows that an item is mapped into a set ofitems produced by Earley's parser.By construction, in a given state s E S, non-kerneldotted rules have been introduced before run-time bythe closure of kernel dotted rules.
It follows that Ear-ley's parser can require O(IGln) more space since allEarley's items of the form C A ~ ?a,  i, i) (i E \[0, n\])are not stored separately from the kernel dotted rulewhich introduced them.Conversely, each kernel item in the chart created byEarley's parser can be put into correspondence withan item created by the parser we propose.Lemma 2 If CA --* a ?
fl, i , j )  E CearZev and if (~ # e,then C s, i , j )  e C where s = closure({A ~ a ?
fl}).The proof of the above lemma is by induction onthe number of kernel items added to the chart createdby Earley's parser.The correctness of the parser follows from Lemma 1and its completeness from Lemma 2 since it is wellknown that the items created by Earley's parser arecharacterized asfollows (see, for example, page 323 inAho and Ullman \[1973\] for a proof of this invariant):Lemma 3 The item C A --.
a ?
fl, i, j) E Cearteyif and only if, ST E (VNT U VT)* such thatS"~W\]o, i \ ]XT and X==c, FA=~w\] i j \ ]A .The parser we propose is therefore more efficientthan Earley's parser since it has compiled out predic-tion before run time.
How much more efficient it is,depends on how prolific the prediction is and thereforeon the nature of the grammar and the input string.6 Opt imizat ionsThe parser can be easily extended to incorporate stan-dard optimization techniques proposed for predictiveparsers.The closure operation which defines how a stateis constructed already optimizes the parser on chainderivations in a manner very similar to the tech-niques originally proposed by Graham eta\] .
(1980)and later also used by Leiss (1990).In addition, the closure operation can be designedto optimize the processing of non-terminal symbolsthat derive the empty string in manner very simi-lar to the one proposed by Graham et al (1980) andLeiss (1990).
The idea is to perform the reductionof symbols that derive the empty string at compila-tion time, i.e.
include this type of reduction in thedefinition of closure by adding (iii):If s is a state, then closure(s) is now the state con-structed from s by the three rules:(i) Initially, every dotted rule in s is added toclosure(s);(ii) i fA~ a .B f l i s inc losure(s )  andB ~ 7 isa production, then add the dotted rule B ~ ?
7to closure(s) (if it is not already there);(iii) i fA ~ a .B~ is in closure(s) and i fB=~ e, thenadd the dotted rule A ~ aB ?
~ to closure(s)(if it is not already there).Rules (ii) and (iii) are applied until no more newdotted rules can be added to closure(s).The rest of the parser remains as before.7 Var iants  on  the  bas ic  ma-ch ineIn the previous section we have constructed a ma-chine whose number of states is in the worst caseproportional to the size of the grammar.
This re-quirement is essential to guarantee that the complex-ity of the resulting parser with respect o the gram-mar size is not exponential or worse than O(IGI2)-time as other well known parsers.
However, we mayuse some non-determinism in the machine to guaran-tee this property.
The non-determinism of the ma-chine is not a problem since we have shown how thenon-deterministic machine can be efficiently driven inpseudo-parallel (in O(\[G\[2n3)-time).We can now ask the question of whether it is pos-sible to determinize the finite state control of the ma-chine while still being able to bound the complexityof the parser to O(\[Gl2n3)-time.
Johnson (1989) ex-hibits grammars for which the full determinization111of the finite state control (the LR(0) construction)leads to a parser with exponential complexity, becausethe finite state control has an exponential number ofstates and also because there are some input stringfor which an exponential number of states will bereached.
However, there are also cases where the fulldetermin~ation either will not increase the numberof states or will not lead to a parser with exponentialcomplexity because there are no input that require toreach an exponential number of states.
We are cur-rently studying the classes of grammars for which thisis the case.One can also try to determinize portions of the fi-nite state automaton from which the control is derivedwhile making sure that the number of states does notbecome larger than O(IGI).All these variants of the basic parser obtained bydeterminizing portions of the basic non-deterministicpush-down machine can be driven in pseudo-parallelby the same pseudo-parallel driver that we previouslydefined.
These variants lead to a set of more efficientmachines ince the non-determinism is decreased.8 Conclus ionWe have introduced a shift-reduce parser for unre-stricted context-free grammars based on the construc-tion of a non-deterministic machine and we have for-mally proven its superior performance compared toEarley's parser.The technique which we employed consists of con-structing before run-time a parsing table that encodesa non-deterministic machine in the which the predic-tive behavior has been compiled out.
At run time, themachine is driven in pseudo-parallel with the help achart.By defining two kinds of shift moves (on kernel dot-ted rules and on non-kernel dotted rules) and twokinds of reduce moves (on kernel and non-kernel dot-ted rules), we have been able to efficiently evaluate inpseudo-parallel the non-deterministic push down ma-chine constructed for the given context-free grammar.The same worst case complexity as Earley's rec-ognizer is achieved: O(IGl2na)-time and O(IG\]n2) -space.
However, in practice, it is superior to Earley'sparser since all the prediction steps and some of thecompletion steps have been compiled before run-time.The parser can be modified to simulate other typesof machines (such LR(k)-like or SLR-like automata).It can also be extended to handle unification basedgrammars using a similar method as that employedby Shieber (1985) for extending Earley's algorithm.Furthermore, the algorithm can be tuned to a par-ticular grammar and therefore be made more effi-cient by carefully determinizing portions of the non-deterministic machine while making sure that thenumber of states in not increased.
These variantslead to more efficient parsers than the one based onthe basic non-deterministic push-down machine.
Fur-thermore, the same pseudo-parallel driver can be usedfor all these machines.We have adapted the technique presented in thispaper to other grammatical formalism such as tree-adjoining grammars (Schabes, 1991).Bibl iographyA.
V. Aho and J. D. Ullman.
1973.
Theory of Pars-ing, Translation and Compiling.
Vol I: Parsing.Prentice-Hall, Englewood Cliffs, NJ.Jay C. Earley.
1968.
An Efficient Context-Free Pars-ing Algorithm.
Ph.D. thesis, Carnegie-Mellon Uni-versity, Pittsburgh, PA.Jay C. Earley.
1970.
An efficient context-free parsingalgorithm.
Commun.
ACM, 13(2):94-102.S.L.
Graham, M.A.
Harrison, and W.L.
Ruzzo.
1980.An improved context-free r cognizer.
ACM Trans-actions on Programming Languages and Systems,2(3):415-462, July.Mark Johnson.
1989.
The computational complex-ity of Tomlta's algorithm.
In Proceedings of theInternational Workshop on Parsing Technologies,Pittsburgh, August.T.
Kasami.
1965.
An efficient recognition and syn-tax algorithm for context-free languages.
TechnicalReport AF-CRL-65-758, Air Force Cambridge Re-search Laboratory, Bedford, MA.James R. Kipps.
1989.
Analysis of Tomita's al-gorithm for general context-free parsing.
In Pro-ceedings of the International Workshop on ParsingTechnologies, Pittsburgh, August.D.
E. Knuth.
1965.
On the translation of languagesfrom left to right.
Information and Control, 8:607-639.Bernard Lang.
1974.
Deterministic tech-niques for efficient non-deterministic parsers.
InJacques Loeckx, editor, Automata, Languagesand Programming, 2nd Colloquium, University ofSaarbr~cken.
Lecture Notes in Computer Science,Springer Verlag.112Hans Leiss.
1990.
On Kilbury's modification of Ear-ley's algorithm.
ACM Transactions on Program-ming Languages and Systems, 12(4):610-640, Oc-tober.Yves Schabes.
1991.
Polynomial time and spaceshift-reduce parsing of context-free grammars andof tree-adjoining grammars.
In preparation.tteOStuart M. Shieber.
1985.
Using restriction to ex- 1tend parsing algorithms for complex-feature-based 2formalisms.
In 23 rd Meeting of the Association 3 4 for Computational Linguistics (ACL '85), Chicago, sJuly.Masaru Tomita.
1985.
Efficient Parsing for NaturalLanguage, A Fast Algorithm for Practical Systems.Kluwer Academic Publishers.Masaru Tomita.
1987.
An efficient augmented-context-free parsing algorithm.
ComputationalLinguistics, 13:31-46.D.
H. Younger.
1967.
Recognition and parsing ofcontext-free languages in time n 3.
Information andControl, 10(2):189-208.A An ExampleWe give an example that illustrates how the recog-nizer works.
The grammar used for the example gen-erates the language L = {a(ba)nln >_ O} and is in-finitely ambiguous:S- - .SbSS~SS --, aThe set of states and the goto function are shownin Figure 1.
In Figure 1, the set of states is{0, 1, 2, 3, 4, 5}.
We have marked with a sharp sign (~)transitions on a non-kernel dotted rule.
If an arc from51 to 52 is labeled by a non-sharped symbol X, thens2 is in gotot(Sl,X).
If an arc from sl to 52 is labeledby a sharped symbol X~, then 52 is in gotont(Sx, X).1 4$~"(S-~ S'b$)sCL" ~rS--~ Sb'S~- -  TLi .Sb , -.> --~*S /IS-~ S , --#-a J--> SbS- )Figure 1: Example of set of states and goto function.The parsing table corresponding to this grammaris given in Figure 2.ACTION.k,h(3)red(S--*S)red(S~a)nksh(3)red(S--*SbS)I b I $ksh(4),~d(S--.S) ,~a(S-.S),~d(S--.,) , ,d(s-~,)red(S -~  5bS)  red(S. - -*~SbS')GOTOkS{5)Figure 2: An LR(0) parsing table for L ={a(ba)" I n ~ 0}.
The start state is 0, the set offinal states is {2, 3, 5}.
$ stands for the end marker ofthe input string.The input string given to the recognizer is: ababa$($ is the end marker).
The chart is shown in Fig-ure 3.
In Figure 3, an arc labeled by s from positioni to position j denotes the item (s, i,j).
The input isaccepted since the final states 2 and 5 span the en-tire string ((2, 0, 5) E C and (5, 0, 5) E C).
Notice thatthere are multiple arcs subsuming the same substring.aababaababababaitems in the chart(0, O, 0 I(3,0,1) (2,10,1) (1,0,1)14,0,2)(3' 2' 3) (2' 0' 3) (2' 2 l, 3)(1,0, 3)(1,2, 3)15,0,3)(4, O, 4 ) (4 ,  2, 4)(3,4,5) (2,0,5) (2,2,5)(2,4,5) (1,0,5) (1,2,5)(1,4,5) (5,0,5)(5,2,5)Figure 3: Chart created ~r the inputoal b2a3b4ah$.OoT0nkS I{1,2){1,2}113
