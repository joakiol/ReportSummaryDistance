Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1467?1478,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsClosing the Loop: Fast, Interactive Semi-Supervised AnnotationWith Queries on Features and InstancesBurr SettlesMachine Learning DepartmentCarnegie Mellon UniversityPittsburgh, PA 15213 USAbsettles@cs.cmu.eduAbstractThis paper describes DUALIST, an activelearning annotation paradigm which solicitsand learns from labels on both features (e.g.,words) and instances (e.g., documents).
Wepresent a novel semi-supervised training al-gorithm developed for this setting, which is(1) fast enough to support real-time interac-tive speeds, and (2) at least as accurate as pre-existing methods for learning with mixed fea-ture and instance labels.
Human annotators inuser studies were able to produce near-state-of-the-art classifiers?on several corpora in avariety of application domains?with only afew minutes of effort.1 IntroductionIn active learning, a classifier participates in its owntraining process by posing queries, such as request-ing labels for documents in a text classification task.The goal is to maximize the accuracy of the trainedsystem in the most economically efficient way.
Thisparadigm is well-motivated for natural language ap-plications, where unlabeled data may be readilyavailable (e.g., text on the Internet), but the anno-tation process can be slow and expensive.Nearly all previous work in active learning, how-ever, has focused on selecting queries from thelearner?s perspective.
For example, experiments areoften run in simulation rather than with user stud-ies, and results are routinely evaluated in terms oftraining set size rather than human annotation timeor labor costs (which are more reasonable measuresof labeling effort).
Many state-of-the-art algorithmsare also too slow to run or too tedious to implementto be useful for real-time interaction with human an-notators, and few analyses have taken these factorsinto account.
Furthermore, there is very little workon actively soliciting domain knowledge from hu-mans (e.g., information about features) and incorpo-rating this into the learning process.While selecting good queries is clearly important,if our goal is to reduce actual annotation effort thesehuman factors must be taken into account.
In thiswork, we propose a new interactive annotation inter-face which addresses some of these issues; in partic-ular it has the ability to pose queries on both features(e.g., words) and instances (e.g., documents).
Wepresent a novel semi-supervised learning algorithmthat is fast, flexible, and accurate enough to supportthese interface design constraints interactively.2 DUALIST: Utility for Active Learningwith Instances and Semantic TermsFigure 1 shows a screenshot of the DUALIST an-notation tool, which is freely available as an open-source software project1.
On the left panel, usersare presented with unlabeled documents: in this caseUsenet messages that belong to one of two sports-related topics: baseball and hockey.
Users may labeldocuments by clicking on the class buttons listed be-low each text.
In cases of extreme ambiguity, usersmay ignore a document by clicking the ?X?
to re-move it from the pool of possible queries.On the right panel, users are given a list of fea-ture queries organized into columns by class label.1http://code.google.com/p/dualist/1467Figure 1: A screenshot of DUALIST.The rationale for these columns is that they shouldreduce cognitive load (i.e., once a user is in the base-ball mindset, s/he can simply go down the list, label-ing features in context: ?plate,?
?pitcher,?
?bases,?etc.).
Within each column, words are sorted by howinformative they are the to classifier, and users mayclick on words to label them.
Each column also con-tains a text box, where users may ?inject?
domainknowledge by typing in arbitrary words (whetherthey appear in any of the columns or not).
The listof previously labeled words appears at the bottom ofeach list (highlighted), and can be unlabeled at anytime, if users later feel they made any errors.Finally, a large submit button is located at the topof the screen, which users must click to re-train theclassifier and receive a new set of queries.
The learn-ing algorithm is actually fast enough to do this au-tomatically after each labeling action.
However, wefound such a dynamically changing interface to befrustrating for users (e.g., words they wanted to la-bel would move or disappear).2.1 A Generative Model for Learning fromFeature and Instance LabelsFor the underlying model in this system, we usemultinomial na?
?ve Bayes (MNB) since it is sim-ple, fast, and known to work well for several nat-ural language applications?text classification inparticular?despite its simplistic and often violatedindependence assumptions (McCallum and Nigam,1998; Rennie et al, 2003).MNB models the distribution of features as amultinomial: documents are sequences of words,with the ?na??ve?
assumption that words in eachposition are generated independently.
Each docu-ment is treated as a mixture of classes, which havetheir own multinomial distributions over words.
Letthe model be parameterized by the vector ?, with?j = P (yj) denoting the probability of class yj , and?jk = P (fk|yj) denoting the probability of generat-ing word fk given class yj .
Note that for class pri-ors?j ?j = 1, and for per-class word multinomials?k ?jk = 1.
The likelihood of document x beinggenerated by class yj is given by:P?
(x|yj) = P (|x|)?k(?jk)fk(x),where fk(x) is the frequency count of word fk indocument x.
If we assume P (|x|) is distributed in-dependently of class, and since document length |x|is fixed, we can drop the first term for classificationpurposes.
Then, we can use Bayes?
rule to calculatethe posterior probability under the model of a label,given the input document for classification:P?
(yj |x) =P?(yj)P?(x|yj)P?
(x)= ?j?k(?jk)fk(x)Z(x) ,(1)where Z(x) is shorthand for a normalization con-stant, summing over all possible class labels.The task of training such a classifier involves es-timating the parameters in ?, given a set of labeledinstances L = {?x(l), y(l)?}Ll=1.
To do this, we usea Dirichlet prior and take the expectation of eachparameter with respect to the posterior, which is asimple way to estimate a multinomial (Heckerman,1995).
In other words, we count the fraction of timesthe word fk occurs in the labeled set among doc-uments of class yj , and the prior adds mjk ?hallu-cinated?
occurrences for a smoothed version of themaximum likelihood estimate:?jk =mjk +?i P (yj |x(i))fk(x(i))Z(fk).
(2)Here, mjk is the prior for word fk under class yj ,P (yj |x(i)) ?
{0, 1} indicates the true labeling of theith document in the training set, and Z(fk) is a nor-malization constant summing over all words in thevocabulary.
Typically, a uniform prior is used, suchas the Laplacian (a value of 1 for all mjk).
Class pa-rameters ?j are estimated a similar way, by counting1468the fraction of documents that are labeled with thatclass, subject to a prior mj .
This prior is importantin the event that no documents are yet labeled withyj , which can be quite common early on in the activelearning process.Recall that our scenario lets human annotatorsprovide not only document labels, but feature labelsas well.
To make use of this additional information,we assume that labeling the word fk with a class yjincreases the probability P (fk|yj) of the word ap-pearing in documents of that class.
The natural in-terpretation of this under our model is to increase theprior mjk for the corresponding multinomial.
To dothis we introduce a new parameter ?, and define theelements of the Dirichlet prior as follows:mjk ={1 + ?
if fk is labeled with yj ,1 otherwise.This approach is extremely flexible, and offers threeparticular advantages over the previous ?poolingmultinomials?
approach for incorporating feature la-bels into MNB (Melville et al, 2009).
The poolingmultinomials algorithm averages together two setsof ?jk parameters: one that is estimated from labeleddata, and another derived from feature labels underthe assumption of a boolean output variable (treatinglabeled features are ?polarizing?
factors).
Therefore,pooling multinomials can only be applied to binaryclassification tasks, while our method works equallywell for problems with multiple classes.
The secondadvantage is that feature labels need not be mutu-ally exclusive, so the word ?score?
could be labeledwith both baseball and hockey, if necessary (e.g.,if the task also includes several non-sports labels).Finally, our framework allows users to conceivablyprovide feature-specific priors ?jk to, for example,imply that the word ?inning?
is a stronger indicatorfor baseball than the word ?score?
(which is a moregeneral sports term).
However, we leave this aspectfor future work and employ the fixed-?
approach asdescribed above in this study.2.2 Exploiting Unlabeled DataIn addition to document and feature labels, we usu-ally have access to a large unlabeled corpus.
In fact,these texts form the pool of possible instance queriesin active learning.
We can take advantage of this ad-ditional data in generative models like MNB by em-ploying the Expectation-Maximization (EM) algo-rithm.
Combining EM with pool-based active learn-ing was previously studied in the context of instancelabeling (McCallum and Nigam, 1998), and we ex-tend the method to our interactive scenario, whichsupports feature labeling as well.First, we estimate initial parameters ??
as in Sec-tion 2.1, but using only the priors (and no instances).Then, we apply the induced classifier on the unla-beled pool U = {x(u)}Uu=1 (Eq.
1).
This is the ?E?step of EM.
Next we re-estimate feature multino-mials ?jk, using both labeled instances from L andprobabilistically-labeled instances from U (Eq.
2).In other words, P (yj |x) ?
{0, 1} for x ?
L, andP (yj |x) = P??
(yj |x) for x ?
U .
We also weightthe data in U by a factor of 0.1, so as not to over-whelm the training signal coming from true instancelabels in L. Class parameters ?j are re-estimated inthe analogous fashion.
This is the ?M?
step.For speed and interactivity, we actually stop train-ing after this first iteration.
When feature labels areavailable, we found that EM generally converges infour to 10 iterations, requiring more training timebut rarely improving accuracy (the largest gains con-sistently come in the first iteration).
Also, we ignorelabeled data in the initial estimation of ??
because Lis too small early in active learning to yield good re-sults with EM.
Perhaps this can be improved by us-ing an ensemble (McCallum and Nigam, 1998), butthat comes at further computational expense.
Fea-ture labels, on the other hand, seem generally morereliable for probabilistically labeling U .2.3 Selecting Instance and Feature QueriesThe final algorithmic component to our system isthe selection of informative queries (i.e., unlabeledwords and documents) to present to the annotator.Querying instances is the traditional mode of ac-tive learning, and is well-studied in the literature;see Settles (2009) for a review.
In this work we useentropy-based uncertainty sampling, which ranks allinstances in U by the posterior class entropy underthe modelH?
(Y |x) = ?
?j P?
(yj |x) logP?
(yj |x),and asks the user to label the top D unlabeled doc-uments.
This simple heuristic is an approximationto querying the instance with the maximum infor-mation gain (since the class entropy, once labeled,is zero), under the assumption that each x is repre-1469sentative of the underlying natural data distribution.Moreover, it is extremely fast to compute, which isimportant for our interactive environment.Querying features, though, is a newer idea withsignificantly less research behind it.
Previous workhas either assumed that (1) features are not assignedto classes, but instead flagged for ?relevance?
to thetask (Godbole et al, 2004; Raghavan et al, 2006),or (2) feature queries are posed just like instancequeries: a word is presented to the annotator, whomust choose among the labels (Druck et al, 2009;Attenberg et al, 2010).
Recall from Figure 1 thatwe want to organize feature queries into columns byclass label.
This means our active learner must pro-duce queries that are class-specific.To select these feature queries, we first rank ele-ments in the vocabulary by information gain (IG):IG(fk) =?Ik?jP (Ik, yj) logP (Ik, yj)P (Ik)P (yj),where Ik ?
{0, 1} is a variable indicating the pres-ence or absence of a feature.
This is essentiallythe common feature-selection method for identify-ing the most salient features in text classification(Sebastiani, 2002).
However, we use both L andprobabilistically-labeled instances from U to com-pute IG(fk), to better reflect what the model be-lieves it has learned.
To organize queries intoclasses, we take the top V unlabeled features andpose fk for the class yj with which it occurs mostfrequently, as well as any other class with which itoccurs at least 75% as often.
Intuitively, this ap-proach (1) queries features that the model believesare most informative, and (2) automatically identi-fies classes that seem most correlated.
To our knowl-edge, DUALIST is the first active learning environ-ment with both of these properties.3 ExperimentsWe conduct four sets of experiments to evaluate ourapproach.
The first two are ?offline?
experiments,designed to better understand (1) how our trainingalgorithm compares to existing methods for feature-label learning, and (2) the effects of tuning the ?parameter.
The other experiments are user studiesdesigned to empirically gauge how well human an-notators make use of DUALIST in practice.We use a variety of benchmark corpora in the fol-lowing evaluations.
Reuters (Rose et al, 2002) isa collection of news articles organized into topics,such as acquisitions, corn, earnings, etc.
As in pre-vious work (Raghavan et al, 2006) we use the 10most frequent topics, but further process the cor-pus by removing ambiguous documents (i.e., thatbelong to multiple topics) so that all articles havea unique label, resulting in a corpus of 9,002 arti-cles.
WebKB (Craven et al, 1998) consists of 4,199university web pages of four types: course, faculty,project, and student.
20 Newsgroups (Lang, 1995)is a set of 18,828 Usenet messages from 20 differentonline discussion groups.
For certain experiments(such as the one shown in Figure 1), we also usetopical subsets.
Movie Reviews (Pang et al, 2002)is a set of 2,000 online movie reviews categorized aspositive or negative in sentiment.
All data sets wereprocessed using lowercased unigram features, withpunctuation and common stop-words removed.3.1 Comparison of Learning AlgorithmsAn important question is how well our learning al-gorithm, ?MNB/Priors,?
performs relative to exist-ing baseline methods for learning with labeled fea-tures.
We compare against two such approachesfrom the literature.
?MaxEnt/GE?
is a maximum en-tropy classifier trained using generalized expectation(GE) criteria (Druck et al, 2008), which are con-straints used in training discriminative linear mod-els.
For labeled features, these take the form of ex-pected ?reference distributions?
conditioned on thepresence of the feature (e.g., 95% of documents con-taining the word ?inning?
should be labeled base-ball).
For each constraint, a term is added to theobjective function to encourage parameter settingsthat yield predictions conforming to the referencedistribution on unlabeled instances.
?MNB/Pool?
isna?
?ve Bayes trained using the pooling multinomialsapproach (Melville et al, 2009) mentioned in Sec-tion 2.1.
We also expand upon MNB/Pool using anEM variant to make it semi-supervised.We use the implementation of GE training fromthe open-source MALLET toolkit2, and implementboth MNB variants in the same data-processingpipeline.
Because the GE implementation available2http://mallet.cs.umass.edu1470Corpus MaxEnt/GE MNB/Pool Pool+EM1 MNB/Priors Priors+EM1Reuters 82.8 (22.9) ?
?
?
?
83.7 (?0.1) 86.6 (0.3)WebKB 22.2 (4.9) ?
?
?
?
67.5 (?0.1) 67.8 (0.1)20 Newsgroups 49.7 (326.6) ?
?
?
?
50.1 (0.2) 70.7 (6.9)Science 86.9 (5.7) ?
?
?
?
71.4 (?0.1) 92.8 (0.1)Autos/Motorcycles 90.8 (0.8) 90.1 (?0.1) 97.5 (?0.1) 89.9 (?0.1) 97.6 (?0.1)Baseball/Hockey 49.9 (0.8) 90.7 (?0.1) 96.7 (?0.1) 90.5 (?0.1) 96.9 (?0.1)Mac/PC 50.5 (0.6) 86.7 (?0.1) 91.2 (?0.1) 86.6 (?0.1) 90.2 (?0.1)Movie Reviews 68.8 (1.8) 68.0 (?0.1) 73.4 (0.1) 67.7 (?0.1) 72.0 (0.1)Table 1: Accuracies and training times for different feature-label learning algorithms on benchmark corpora.
Classi-fication accuracy is reported for each model, using only the top 10 oracle-ranked features per label (and no labeledinstances) for training.
The best model for each corpus is highlighted in bold.
Training time (in seconds) is shown inparentheses on the right side of each column.
All results are averaged across 10 folds using cross-validation.to us only supports labeled features (and not labeledinstances as well), we limit the MNB methods tofeatures for a fair comparison.
To obtain feature la-bels in this experiment, we simulate a ?feature or-acle?
as in previous work (Druck et al, 2008; At-tenberg et al, 2010), which is essentially the queryselection algorithm from Section 2.3, but using com-plete labeled data to compute IG(fk).
We con-servatively use only the top 10 features per class,which is meant to resemble a handful of very salientfeatures that a human might brainstorm to jump-start the learning process.
We experiment withEM1 (one-step EM) variants of both MNB/Pooland MNB/Priors, and set ?
= 50 for the latter(see Section 3.2 for details on tuning this parame-ter).
Results are averaged over 10 folds using cross-validation, and all experiments are conducted on asingle 2.53GHz processor machine.Results are shown in Table 1.
As expected, addingone iteration of EM for semi-supervised training im-proves the accuracy of both MNB methods across alldata sets.
These improvements come without signif-icant overhead in terms of time: training still rou-tinely finishes in a fraction of a second per fold.MNB/Pool and MNB/Priors, where they can becompared, perform virtually the same as each otherwith or without EM, in terms of accuracy and speedalike.
However, MNB/Pool is only applicable to bi-nary classification problems.
As explained in Sec-tion 2.1, MNB/Priors is more flexible, and prefer-able for a more general-use interactive annotationtool like DUALIST.The semi-supervised MNB methods are also con-sistently more accurate than GE training?and areabout 40 times faster as well.
The gains ofPriors+EM1 over MaxEnt/GE are statistically sig-nificant in all cases but two: Autos/Motorcycles andMovie Reviews3.
MNB is superior when using any-where from five to 20 oracle-ranked features perclass, but as the number of feature labels increasesbeyond 30, GE is often more accurate (results notshown).
If we think of MaxEnt/GE as a discrim-inative analog of MNB/Priors+EM, this is consis-tent with what is known about labeled set size in su-pervised learning for generative/discriminative pairs(Ng and Jordan, 2002).
However, the time complex-ity of GE training increases sharply with each newlabeled feature, since it adds a new constraint to theobjective function whose gradient must be computedusing all the unlabeled data.
In short, GE train-ing is too slow and too inaccurate early in the ac-tive learning process (where labels are more scarce)to be appropriate for our scenario.
Thus, we selectMNB/Priors to power the DUALIST interface.3.2 Tuning the Parameter ?A second question is how sensitive the accuracy ofMNB/Priors is to the parameter ?.
To study this,we ran experiments varying ?
from from one to 212,using different combinations of labeled instancesand/or features (again using the simulated oracle and10-fold cross-validation).3Paired 2-tailed t-test, p < 0.05, correcting for multiple testsusing the Bonferroni method.14710.720.740.760.780.80.820.840.860.880.91  10  100  100010 feat10 feat, 100 inst100 feat100 feat, 100 inst0.620.640.660.680.70.720.741  10  100  100010 feat10 feat, 100 inst100 feat100 feat, 100 inst0.90.910.920.930.940.950.961  10  100  100010 feat10 feat, 100 inst100 feat100 feat, 100 inst0.620.640.660.680.70.720.740.760.780.81  10  100  100010 feat10 feat, 100 inst100 feat100 feat, 100 instalpha alphaReuters WebKBScience Movie ReviewsFigure 2: The effects of varying ?
on accuracy for fourcorpora, using differing amounts of training data (labeledfeatures and/or instances).
For clarity, vertical axes arescaled differently for each data set, and horizontal axesare plotted on a logarithmic scale.
Classifier performanceremains generally stable across data sets for ?
< 100.Figure 2 plots these results for four of the corpora.The first thing to note is that in all cases, accuracy isrelatively stable for ?
< 100, so tuning this valueseems not to be a significant concern; we chose 50for all other experiments in this paper.
A second ob-servation is that, for all but the Reuters corpus, label-ing 90 additional features improves accuracy muchmore than labeling 100 documents.
This is encour-aging, since labeling features (e.g., words) is knownto be generally faster and easier for humans than la-beling entire instances (e.g., documents).For Reuters, however, the additional feature la-bels appear harmful.
The anomaly can be explainedin part by previous work with this corpus, whichfound that a few expertly-chosen keywords canoutperform machine learning methods (Cohen andSinger, 1996), or that aggressive feature selection?i.e., using only three or four features per class?helps tremendously (Moulinier, 1996).
Corpora likeReuters may naturally lend themselves to feature se-lection, which is (in some sense) what happens whenlabeling features.
The simulated oracle here wasforced to label 100 features, some with very lowinformation gain (e.g., ?south?
for acquisitions, or?proven?
for gold); we would not expect humans an-notators to provide such misleading information.
In-stead, we hypothesize that in practice there may be alimited set of features with high enough informationcontent for humans to feel confident labeling, afterwhich they switch their attention to labeling instancequeries instead.
This further indicates that the user-guided flexibility of annotation in DUALIST is anappropriate design choice.3.3 User ExperimentsTo evaluate our system in practice, we conducteda series of user experiments.
This is in contrast tomost previous work, which simulates active learningby using known document labels and feature labelsfrom a simulated oracle (which can be flawed, as wesaw in the previous section).
We argue that this is animportant contribution, as it gives us a better senseof how well the approach actually works in practice.It also allows us to analyze behavioral results, whichin turn may help inform future protocols for humaninteraction in active learning.DUALIST is implemented as a web-based appli-cation in Java and was deployed online.
We usedthree different configurations: active dual (as in Fig-ure 1, implementing everything from Section 2), ac-tive instance (instance queries only, no features), anda passive instance baseline (instances only, but se-lected at random).
We also began by randomly se-lecting instances in the active configurations, untilevery class has at least one labeled instance or onelabeled feature.
D = 2 documents and V = 100 fea-tures were selected for each round of active learning.We recruited five members of our research groupto label three data sets using each configuration, inan order of their choosing.
Users were first allowedto spend a minute or two familiarizing themselveswith DUALIST, but received no training regardingthe interface or data sets.
All experiments used afixed 90% train, 10% test split which was consistentacross all users, and annotators were not allowed tosee the accuracy of the classifier they were train-ing at any time.
Each annotation action was times-tamped and logged for analysis, and each experi-ment automatically terminated after six minutes.Figure 3 shows learning curves, in terms of accu-racy vs. annotation time, for each trial in the userstudy.
The first thing to note is that the active1472WebKB Science Movie Reviewsannotation time (sec) annotation time (sec) annotation time (sec)user1user2user3user4user500.10.20.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.20.40.60.810  60  120  180  240  300  360active dualactive instpassive inst0.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.10.20.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.20.40.60.810  60  120  180  240  300  360active dualactive instpassive inst0.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.10.20.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.20.40.60.810  60  120  180  240  300  360active dualactive instpassive inst0.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.10.20.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.20.40.60.810  60  120  180  240  300  360active dualactive instpassive inst0.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.10.20.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive inst00.20.40.60.810  60  120  180  240  300  360active dualactive instpassive inst0.30.40.50.60.70.80.90  60  120  180  240  300  360active dualactive instpassive instFigure 3: User experiments involving human annotators for text classification.
Each row plots accuracy vs. timelearning curves for a particular user (under all three experimental conditions) for each of the three corpora (onecolumn per data set).
For clarity, vertical axes are scaled differently for each corpus, but held constant across all users.The thin dashed lines at the top of each plot represents the idealized fully-supervised accuracy.
Horizontal axes showlabeling cost in terms of actual elapsed annotation time (in seconds).1473dual configuration yields consistently better learn-ing curves than either active or passive learning withinstances alone, often getting within 90% of fully-supervised accuracy (in under six minutes).
Theonly two exceptions make interesting (and differ-ent) case studies.
User 4 only provided four la-beled features in the Movie Review corpus, whichpartially explains the similarity in performance tothe instance-only cases.
Moreover, these weremanually-added features, i.e., he never answeredany of the classifier?s feature queries, thus deprivingthe learner of the information it requested.
User 5,on the other hand, never manually added featuresand only answered queries.
With the WebKB cor-pus, however, he apparently found feature queriesfor the course label to be easier than the otherclasses, and 71% of all his feature labels camefrom that class (sometimes noisily, e.g., ?instructor?might also indicate faculty pages).
This imbalanceultimately biased the learner toward the course la-bel, which led to classification errors.
These patho-logical cases represent potential pitfalls that couldbe alleviated with additional user studies and train-ing.
However, we note that the active dual interfaceis not particularly worse in these cases, it is simplynot significantly better, as in the other 13 trials.Feature queries were less costly than instances,which is consistent with findings in previous work(Raghavan et al, 2006; Druck et al, 2009).
Theleast expensive actions in these experiments werelabeling (mean 3.2 seconds) and unlabeling (1.8s)features, while manually adding new features tookonly slightly longer (5.9s).
The most expensive ac-tions were labeling (10.8s) and ignoring (9.9s) in-stance queries.
Interestingly, we observed that thehuman annotators spent most of the first three min-utes performing feature-labeling actions ( ), andswitched to more instance-labeling activity for thefinal three minutes ( ).
As hypothesized in Sec-tion 3.2, it seems that the active learner is exhaustingthe most salient feature queries early on, and usersbegin to focus on more interpretable instance queriesover time.
However, more study (and longer annota-tion periods) are warranted to better understand thisphenomenon, which may suggest additional user in-terface design improvements.We also saw surprising trends in annotation qual-ity.
In active settings, users made an average of oneinstance-labeling error per trial (relative to the gold-standard labels), but in the passive case this rose to1.6, suggesting they are more accurate on the activequeries.
However, they also explicitly ignored moreinstances in the active dual condition (7.7) than ei-ther active instance (5.9) or passive (2.5), indicatingthat they find these queries more ambiguous.
Thisseems reasonable, since these are the instances theclassifier is least certain about.
But if we look atthe time users spent on these actions, they are muchfaster to label/ignore (9.7s/7.5s) in the active dualscenario than in the active instance (10.0s/10.7s) orpassive (12.3s/15.4s) cases, which means they arebeing more efficient.
The differences in time be-tween dual and passive are statistically significant4.3.4 Additional Use CasesHere we discuss the application of DUALIST to afew other natural language processing tasks.
Thissection is not meant to show its superiority relativeto other methods, but rather to demonstrate the flex-ibility and potential of our approach in a variety ofproblems in human language technology.3.4.1 Word Sense DisambiguationWord Sense Disambiguation (WSD) is the prob-lem of determining which meaning of a word is be-ing used in a particular context (e.g., ?hard?
in thesense of a challenging task vs. a marble floor).
Weasked a user to employ DUALIST for 10 minutesfor each of three benchmark WSD corpora (Moham-mad and Pedersen, 2004): Hard (3 senses), Line(6 senses), and Serve (4 senses).
Each instance rep-resents a sentence using the ambiguous word, andfeatures are lowercased unigram and bigram termsfrom the surrounding context in the sentence.
Thelearned models?
prediction accuracies (on the sen-tences not labeled by the user) were: 83.0%, 78.4%,and 78.7% for Hard, Line, and Serve (respectively),which appears to be comparable to recent supervisedlearning results in the WSD literature on these datasets.
However, our results were achieved in less than10 minutes of effort each, by labeling an average of76 sentences and 32 words or phrases per task (com-pared to the thousands of labeled training sentencesused in previous work).4Kolmogorov-Smirnov test, p < 0.01.14743.4.2 Information ExtractionDUALIST is also well-suited to a kind of large-scale information extraction known as semanticclass learning: given a set of semantic categoriesand a very large unlabeled text corpus, learn to pop-ulate a knowledge base with words or phrases thatbelong to each class (Riloff and Jones, 1999; Carl-son et al, 2010).
For this task, we first processed500 million English Web pages from the ClueWeb09corpus (Callan and Hoy, 2009) by using a shallowparser.
Then we represented noun phrases (e.g., ?AlGore,?
?World Trade Organization,?
?upholstery?
)as instances, using a vector of their co-occurrenceswith heuristic contextual patterns (e.g., ?visit to X?or ?X?s mission?)
as well as a few orthographic pat-terns (e.g., capitalization, head nouns, affixes) asfeatures.
We filtered out instances or contexts thatoccurred fewer than 200 times in the corpus, result-ing in 49,923 noun phrases and 87,760 features.We then had a user annotate phrases and patternsinto five semantic classes using DUALIST: person,location, organization, date/time, and other (thebackground or null class).
The user began by insert-ing simple hyponym patterns (Hearst, 1992) for theircorresponding classes (e.g., ?people such as X?
forperson, or ?organizations like X?
for organization)and proceeded from there for 20 minutes.
Sincethere was no gold-standard for evaluation, we ran-domly sampled 300 predicted extractions for eachof the four non-null classes, and hired human eval-uators using the Amazon Mechanical Turk service5to estimate precision.
Each instance was assignedto three evaluators, using majority vote to score forcorrectness.Table 2 shows the estimated precision, total ex-tracted instances, and the number of user-labeledfeatures and instances for each class.
While thereis room for improvement (published results for thiskind of task are often above 80% precision), it isworth noting that in this experiment the user did notprovide any initial ?seed examples?
for each class,which is fairly common in semantic class learning.In practice, such additional seeding should help, asthe active learner acquired 115 labeled instances forthe null class, but fewer than a dozen for each non-null class (in the first 20 minutes).5http://www.mturk.comClass Prec.
# Ext.
# Feat.
# Inst.person 74.7 6,478 37 6location 76.3 5,307 47 5organization 59.7 4,613 51 7date/time 85.7 494 51 12other ?
32,882 13 115Table 2: Summary of results using DUALIST for web-scale information extraction.3.4.3 Twitter Filtering and Sentiment AnalysisThere is growing interest in language analysisfor online social media services such as Twitter6(Petrovic?
et al, 2010; Ritter et al, 2010), which al-lows users to broadcast short messages limited to140 characters.
Two basic but interesting tasks inthis domain are (1) language filtering and (2) sen-timent classification, both of which are difficult be-cause of the extreme brevity and informal use of lan-guage in the messages.Even though Twitter attempts to provide languagemetadata for its ?tweets,?
English is the default set-ting for most users, so about 35% of English-taggedtweets are actually in a different language.
Further-more, the length constraints encourage acronyms,emphatic misspellings, and orthographic shortcutseven among English-speaking users, so many tweetsin English actually contain no proper English words(e.g., ?OMG ur sooo gr8!!
#luvya?).
This mayrender existing lexicon-based language filters?andpossibly character n-gram filters?ineffective.To quickly build an English-language filter forTwitter, we sampled 150,000 tweets from the Twit-ter Streaming API and asked an annotator spend 10minutes with DUALIST labeling English and non-English messages and features.
Features were rep-resented as unigrams and bigrams without any stop-word filtering, plus a few Twitter-specific featuressuch as emoticons (text-based representations of fa-cial expressions such as :) or :( used to convey feel-ing or tone), the presence of anonymized usernames(preceded by ?@?)
or URL links, and hashtags (com-pound words preceded by ?#?
and used to label mes-sages, e.g., ?#loveit?).
Following the same method-ology as Section 3.4.2, we evaluated 300 randompredictions using the Mechanical Turk service.
The6http://twitter.com1475estimated accuracy of the trained language filter was85.2% (inter-annotator agreement among the evalu-ators was 94.3%).We then took the 97,813 tweets predicted to be inEnglish and used them as the corpus for a sentimentclassifier, which attempts to predict the mood con-veyed by the author of a piece of text (Liu, 2010).Using the same feature representation as the lan-guage filter, the annotator spent 20 minutes withDUALIST, labeling tweets and features into threemood classes: positive, negative, and neutral.
Theannotator began by labeling emoticons, by whichthe active learner was able to uncover some interest-ing domain-specific salient terms, e.g., ?cant wait?and ?#win?
for positive tweets or ?#tiredofthat?
fornegative tweets.
Using a 300-instance MechanicalTurk evaluation, the estimated accuracy of the sen-timent classifier was 65.9% (inter-annotator agree-ment among the evaluators was 77.4%).4 Discussion and Future WorkWe have presented DUALIST, a new type of dual-strategy annotation interface for semi-supervised ac-tive learning.
To support this dual-query interface,we developed a novel, fast, and practical semi-supervised learning algorithm, and demonstratedhow users can employ it to rapidly develop use-ful natural language systems for a variety of tasks.For several of these applications, the interactively-trained systems are able to achieve 90% of state-of-the-art performance after only a few minutes oflabeling effort on the part of a human annotator.By releasing DUALIST as an open-source tool, wehope to facilitate language annotation projects andencourage more user experiments in active learning.This represents one of the first studies of an ac-tive learning system designed to compliment thestrengths of both learner and annotator.
Future di-rections along these lines include user studies of effi-cient annotation behaviors, which in turn might leadto new types of queries or improvements to the userinterface design.
An obvious extension in the naturallanguage domain is to go beyond classification tasksand query domain knowledge for structured predic-tion in this way.
Another interesting potential appli-cation is human-driven active feature induction andengineering, after Della Pietra et al (1997).From a machine learning perspective, there is anopen empirical question of how useful the labelsgathered by DUALIST?s internal na?
?ve Bayes modelmight be in later training machine learning systemswith different inductive biases (e.g., MaxEnt modelsor decision trees), since the data are not IID.
So far,attempts to ?reuse?
active learning data have yieldedmixed results (Lewis and Catlett, 1994; Baldridgeand Osborne, 2004).
Practically speaking, DUAL-IST is designed to run on a single machine, andsupports a few hundred thousand instances and fea-tures at interactive speeds on modern hardware.
Dis-tributed data storage (Chang et al, 2008) and paral-lelized learning algorithms (Chu et al, 2007) mayhelp scale this approach into the millions.Finally, modifying the learning algorithm to bettercope with violated independence assumptions maybe necessary for interesting language applicationsbeyond those presented here.
TAN-Trees (Fried-man et al, 1997), for example, might be able to ac-complish this while retaining speed and interactiv-ity.
Alternatively, one could imagine online stochas-tic learning algorithms for discriminatively-trainedclassifiers, which are semi-supervised and can ex-ploit feature labels.
To our knowledge, such flexi-ble and efficient learning algorithms do not currentlyexist, but they could be easily incorporated into theDUALIST framework in the future.AcknowledgmentsThanks to members of Carnegie Mellon?s ?Read theWeb?
research project for helpful discussions andparticipation in the user studies.
This work is sup-ported in part by DARPA (under contracts FA8750-08-1-0009 and AF8750-09-C-0179), the NationalScience Foundation (IIS-0968487), and Google.ReferencesJ.
Attenberg, P. Melville, and F. Provost.
2010.
A uni-fied approach to active dual supervision for labelingfeatures and examples.
In Proceedings of the Euro-pean Conference on Machine Learning and Principlesand Practice of Knowledge Discovery in Databases(ECML PKDD).
Springer.J.
Baldridge and M. Osborne.
2004.
Active learning andthe total cost of annotation.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 9?16.
ACL Press.1476J.
Callan and M. Hoy.
2009.
The clueweb09 dataset.http://lemurproject.org/clueweb09/.A.
Carlson, J. Betteridge, B. Kisiel, B.
Settles, E.R.
Hr-uschka Jr., and T.M.
Mitchell.
2010.
Toward an ar-chitecture for never-ending language learning.
In Pro-ceedings of the Conference on Artificial Intelligence(AAAI), pages 1306?1313.
AAAI Press.F.
Chang, J.
Dean, S. Ghemawat, W.C. Hsieh, D.A.
Wal-lach, M. Burrows, T. Chandra, A. Fikes, and R.E.
Gru-ber.
2008.
Bigtable: A distributed storage system forstructured data.
ACM Transactions on Computer Sys-tems, 26(2):1?26.C.T.
Chu, S.K.
Kim, Y.A.
Lin, Y. Yu, G. Bradski, A.Y.Ng, and K. Olukotun.
2007.
Map-reduce for machinelearning on multicore.
In B. Scho?lkopf, J. Platt, andT.
Hoffman, editors, Advances in Neural InformationProcessing Systems, volume 19, pages 281?288.
MITPress.W.
Cohen and Y.
Singer.
1996.
Context-sensitive learn-ing methods for text categorization.
In Proceedings ofthe ACM SIGIR Conference on Research and Develop-ment in Information Retrieval, pages 307?315.
ACMPress.M.
Craven, D. DiPasquo, D. Freitag, A. McCallum,T.
Mitchell, K. Nigam, and S. Slattery.
1998.
Learn-ing to extract symbolic knowledge from the worldwide web.
In Proceedings of the National Confer-ence on Artificial Intelligence (AAAI), pages 509?516.AAAI Press.S.
Della Pietra, V. Della Pietra, and J. Lafferty.
1997.Inducing features of random fields.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,19(4):380?393.G.
Druck, G. Mann, and A. McCallum.
2008.
Learn-ing from labeled features using generalized expecta-tion criteria.
In Proceedings of the ACM SIGIR Con-ference on Research and Development in InformationRetrieval, pages 595?602.
ACM Press.G.
Druck, B.
Settles, and A. McCallum.
2009.
Ac-tive learning by labeling features.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP), pages 81?90.
ACL Press.N.
Friedman, D. Geiger, and M. Goldszmidt.
1997.Bayesian network classifiers.
Machine learning,29(2):131?163.S.
Godbole, A. Harpale, S. Sarawagi, and S. Chakrabarti.2004.
Document classification through interactive su-pervision of document and term labels.
In Proceed-ings of the Conference on Principles and Practice ofKnowledge Discovery in Databases (PKDD), pages185?196.
Springer.M.A.
Hearst.
1992.
Automatic acquisition of hyponymsfrom large text corpora.
In Proceedings of the Confer-ence on Computational Linguistics (COLING), pages539?545.
ACL.D.
Heckerman.
1995.
A tutorial on learning withbayesian networks.
Technical Report MSR-TR-95-06,Microsoft Research.K.
Lang.
1995.
Newsweeder: Learning to filter net-news.
In Proceedings of the International Conferenceon Machine Learning (ICML), pages 331?339.
Mor-gan Kaufmann.D.
Lewis and J. Catlett.
1994.
Heterogeneous un-certainty sampling for supervised learning.
In Pro-ceedings of the International Conference on MachineLearning (ICML), pages 148?156.
Morgan Kaufmann.B.
Liu.
2010.
Sentiment analysis and subjectivity.
InN.
Indurkhya and F.J. Damerau, editors, Handbook ofNatural Language Processing,.
CRC Press.A.
McCallum and K. Nigam.
1998.
Employing EMin pool-based active learning for text classification.In Proceedings of the International Conference onMachine Learning (ICML), pages 359?367.
MorganKaufmann.P.
Melville, W. Gryc, and R.D.
Lawrence.
2009.
Sen-timent analysis of blogs by combining lexical knowl-edge with text classification.
In Proceedings of the In-ternational Conference on Knowledge Discovery andData Mining (KDD), pages 1275?1284.
ACM Press.S.
Mohammad and T. Pedersen.
2004.
Combining lex-ical and syntactic features for supervised word sensedisambiguation.
In Hwee Tou Ng and Ellen Riloff,editors, Proceedings of the Conference on Compu-tational Natural Language Learning (CoNLL), pages25?32.
ACL Press.I.
Moulinier.
1996.
A framework for comparing text cat-egorization approaches.
In Proceedings of the AAAISymposium on Machine Learning in Information Ac-cess.
AAAI Press.A.Y.
Ng and M. Jordan.
2002.
On discriminative vs.generative classifiers: A comparison of logistic regres-sion and naive bayes.
In Advances in Neural Infor-mation Processing Systems (NIPS), volume 14, pages841?848.
MIT Press.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup: Sentiment classification using machine learningtechniques.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), pages 79?86.
ACL Press.S.
Petrovic?, M. Osborne, and V. Lavrenko.
2010.Streaming first story detection with application toTwitter.
In Proceedings of the North American Asso-ciation for Computational Linguistics (NAACL), pages181?189.
ACL Press.H.
Raghavan, O. Madani, and R. Jones.
2006.
Activelearning with feedback on both features and instances.Journal of Machine Learning Research, 7:1655?1686.1477J.D.
Rennie, L. Shih, J. Teevan, and D. Karger.
2003.Tackling the poor assumptions of naive bayes text clas-sifiers.
In Proceedings of the International Conferenceon Machine Learning (ICML), pages 285?295.
Mor-gan Kaufmann.E.
Riloff and R. Jones.
1999.
Learning dictionaries forinformation extraction by multi-level bootstrapping.In Proceedings of the Conference on Artificial Intel-ligence (AAAI), pages 474?479.
AAAI Press.A.
Ritter, C. Cherry, and B. Dolan.
2010.
Unsupervisedmodeling of Twitter conversations.
In Proceedingsof the North American Association for ComputationalLinguistics (NAACL), pages 172?180.
ACL Press.T.
Rose, M. Stevenson, and M. Whitehead.
2002.
TheReuters corpus vol.
1 - from yesterday?s news to to-morrow?s language resources.
In Proceedings of theConference on Language Resources and Evaluation(LREC), pages 29?31.F.
Sebastiani.
2002.
Machine learning in automated textcategorization.
ACM Computing Surveys, 34(1):1?47.B.
Settles.
2009.
Active learning literature survey.
Com-puter Sciences Technical Report 1648, University ofWisconsin?Madison.1478
