Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1481?1490,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsGraph parsing with s-graph grammarsJonas Groschwitz and Alexander Koller and Christoph TeichmannDepartment of LinguisticsUniversity of Potsdamfirstname.lastname@uni-potsdam.deAbstractA key problem in semantic parsing withgraph-based semantic representations isgraph parsing, i.e.
computing all pos-sible analyses of a given graph accord-ing to a grammar.
This problem arisesin training synchronous string-to-graphgrammars, and when generating stringsfrom them.
We present two algorithms forgraph parsing (bottom-up and top-down)with s-graph grammars.
On the relatedproblem of graph parsing with hyperedgereplacement grammars, our implementa-tions outperform the best previous systemby several orders of magnitude.1 IntroductionThe recent years have seen an increased interestin semantic parsing, the problem of deriving a se-mantic representation for natural-language expres-sions with data-driven methods.
With the recentavailability of graph-based meaning banks (Ba-narescu et al, 2013; Oepen et al, 2014), muchwork has focused on computing graph-based se-mantic representations from strings (Jones et al,2012; Flanigan et al, 2014; Martins and Almeida,2014).One major approach to graph-based semanticparsing is to learn an explicit synchronous gram-mar which relates strings with graphs.
One canthen apply methods from statistical parsing toparse the string and read off the graph.
Chiang etal.
(2013) and Quernheim and Knight (2012) rep-resent this mapping of a (latent) syntactic struc-ture to a graph with a grammar formalism calledhyperedge replacement grammar (HRG; (Dreweset al, 1997)).
As an alternative to HRG, Koller(2015) introduced s-graph grammars and showedthat they support linguistically reasonable gram-mars for graph-based semantics construction.One problem that is only partially understoodin the context of semantic parsing with explicitgrammars is graph parsing, i.e.
the computationof the possible analyses the grammar assigns toan input graph (as opposed to string).
This prob-lem arises whenever one tries to generate a stringfrom a graph (e.g., on the generation side of an MTsystem), but also in the context of extracting andtraining a synchronous grammar, e.g.
in EM train-ing.
The state of the art is defined by the bottom-up graph parsing algorithm for HRG by Chiang etal.
(2013), implemented in the Bolinas tool (An-dreas et al, 2013).We present two graph parsing algorithms (top-down and bottom-up) for s-graph grammars.
S-graph grammars are equivalent to HRGs, but em-ploy a more fine-grained perspective on graph-combining operations.
This simplifies the parsingalgorithms, and facilitates reasoning about them.Our bottom-up algorithm is similar to Chiang etal.
?s, and derives the same asymptotic number ofrule instances.
The top-down algorithm is novel,and achieves the same asymptotic runtime as thebottom-up algorithm by reasoning about the bi-connected components of the graph.
Our eval-uation on the ?Little Prince?
graph-bank showsthat our implementations of both algorithms out-perform Bolinas by several orders of magnitude.Furthermore, the top-down algorithm can be morememory-efficient in practice.2 Related workThe AMR-Bank (Banarescu et al, 2013) annotatessentences with abstract meaning representations(AMRs), like the one shown in Fig.
1(a).
Theseare graphs that represent the predicate-argumentstructure of a sentence; notably, phenomena suchas control are represented by reentrancies in thegraph.
Another major graph-bank is the SemEval-2014 shared task on semantic dependency parsingdataset (Oepen et al, 2014).1481(a)AAA(b)ROS(f)ROS(c)RS(d)OS(e)ROSarg2arg1arg1arg2arg1arg1arg2arg1arg1 arg1arg2arg1arg1wantsleepboywantsleepboywantboysleep sleepwantsleepboyFigure 1: AMR (a) for ?The boy wants to sleep?, and s-graphs.
We call (b) SGwantand (c) SGsleep.The primary grammar formalism currently inuse for synchronous graph grammars is hyper-edge replacement grammar (HRG) (Drewes et al,1997), which we sketch in Section 4.3.
An alterna-tive is offered by Koller (2015), who introduced s-graph grammars and showed that they lend them-selves to manually written grammars for semanticconstruction.
In this paper, we show the equiv-alence of HRG and s-graph grammars and workout graph parsing for s-graph grammars.The first polynomial graph parsing algorithmfor HRGs on graphs with limited connectivity waspresented by Lautemann (1988).
Lautemann?soriginal algorithm is a top-down parser, which ispresented at a rather abstract level that does notdirectly support implementation or detailed com-plexity analysis.
We extend Lautemann?s workby showing how new parse items can be repre-sented and constructed efficiently.
Finally, Chianget al (2013) presented a bottom-up graph parserfor HRGs, in which the representation and con-struction of items was worked out for the first time.It produces O((n ?
3d)k+1) instances of the rulesin a parsing schema, where n is the number ofnodes of the graph, d is the maximum degree ofany node, and k is a quantity called the tree-widthof the grammar.3 An algebra of graphsWe start by introducing the exact type of graphsthat our grammars and parsers manipulate, and bydeveloping some theory.Throughout this paper, we define a graph G =(V,E) as a directed graph with edge labels fromsome label alphabet L. The graph consists of afinite set V of nodes and a finite setE ?
V ?V ?Lof edges e = (u, v, l), where u and v are the nodesconnected by e, and l ?
L is the edge label.
Wesay that e is incident to both u and v, and call thenumber of edges incident to a node its degree.
Wewrite ue?
v if either e = (u, v, l) or e = (v, u, l)for some l; we drop the e if the identity of the edgeis irrelevant.
Edges with u = v are called loops;we use them here to encode node labels.
Given agraph G, we write n = |V |, m = |E|, and d forthe maximum degree of any node in V .If f : A  B and g : A  B are partial func-tions, we let the partial function f ?
g be definedif for all a ?
A with both f(a) and g(a) defined,we have f(a) = g(a).
We then let (f ?
g)(a) bef(a) if f(a) is defined; g(a) if g(a) is defined; andundefined otherwise.3.1 The HR algebra of graphs with sourcesOur grammars describe how to build graphs fromsmaller pieces.
They do this by accessing nodes(called source nodes) which are assigned ?publicnames?.
We define an s-graph (Courcelle and En-gelfriet, 2012) as a pair SG = (G,?)
of a graphG and a source assignment, i.e.
a partial, injectivefunction ?
: S  V that maps some source namesfrom a finite set S to the nodes of G. We call thenodes in ?
(S) the source nodes or sources of SG;all other nodes are internal nodes.
If ?
is definedon the source name ?, we call ?(?)
the ?-sourceof SG.
Throughout, we let s = |S|.Examples of s-graphs are given in Fig.
1.
Weuse numbers as node names and lowercase stringsfor edge names (except in the concrete graphs ofFig.
1, where the edges are marked with edge la-bels instead).
Source nodes are drawn in black,with source names drawn on the inside.
Fig.
1(b)shows an s-graph SGwantwith three nodes andfour edges.
The three nodes are marked as the R-,S-, and O-source, respectively.
Likewise, the s-graph SGsleepin (c) has two nodes (one of whichis an R-source and the other an S-source) and twoedges.We can now apply operations to these graphs.First, we can rename the R-source of (c) to an O-source.
The result, denoted SGd= SGsleep[R ?O], is shown in (d).
Next, we can merge SGdwith SGwant.
This copies the edges and nodesof SGdand SGwantinto a new s-graph; but cru-cially, for every source name ?
the two s-graphshave in common, the ?-sources of the graphs arefused into a single node (and become a ?-source ofthe result).
We write || for the merge operation;1482thus we obtain SGe= SGd|| SGwant, shownin (e).
Finally, we can forget source names.
Thegraph SGf= fS(fO(SGe)), in which we forgot Sand O, is shown in (f).
We refer to Courcelle andEngelfriet (2012) for technical details.1We can take the set of all s-graphs, together withthese operations, as an algebra of s-graphs.
In ad-dition to the binary merge operation and the unaryoperations for forget and rename, we fix some fi-nite set of atomic s-graphs and take them as con-stants of the algebra which evaluate to themselves.Following Courcelle and Engelfriet, we call thisalgebra the HR algebra.
We can evaluate any term?
consisting of these operation symbols into an s-graph J?K as usual.
For instance, the followingterm encodes the merge, forget, and rename oper-ations from the example above, and evaluates tothe s-graph in Fig.
1(f).
(1) fS(fO(SGwant|| SGsleep[R?
O]))The set of s-graphs that can be represented asthe value J?K of some term ?
over the HR alge-bra depends on the source set S and on the con-stants.
For simplicity, we assume here that wehave a constant for each s-graph consisting of asingle labeled edge (or loop), and that the valuesof all other constants can be expressed by combin-ing these using merge, rename, and forget.3.2 S-componentsA central question in graph parsing is how somes-graph that is a subgraph of a larger s-graph SG(a sub-s-graph) can be represented as the mergeof two smaller sub-s-graphs of SG.
In general,SG1|| SG2is defined for any two s-graphs SG1and SG2.
However, if we see SG1and SG2assubgraphs of SG, SG1|| SG2may no longer bea subgraph of SG.
For instance, we cannot mergethe s-graphs (b) and (c) in Fig.
2 as part of thegraph (a): The startpoints of the edges a and d areboth A-sources and would thus become the samenode (unlike in (a)), and furthermore the edge dwould have to be duplicated.
In graph parsing, wealready know the identity of all nodes and edgesin sub-s-graphs (as nodes and edges in SG), andmust thus pay attention that merge operations donot accidentally fuse or duplicate them.
In partic-1Note that the rename operation of Courcelle and En-gelfriet (2012) allows for swapping source assignments andmaking multiple renames in one step.
We simplify the pre-sentation here, but all of our techniques extend easily.
{d}2{a,b,c}3{e}{f} {g} {h}(a)(b) (c)(d) (e)A1 A 2A 4B3C 6B5A1 A 2A 4B3A 2B1A 4A1 A 4dA2A 4eB3A5fA5h A 6A5gA4gadbcdabcdefghabcdfehg{d}{a,b,c,f,e}{a,b,c,d}{e}{f}Figure 2: (a) An s-graph with (b,c) some sub-s-graphs, (d) its BCCs, and (e) its block-cutpointgraph.ular, two sub-s-graphs cannot be merged if theyhave edges in common.We call a sub-s-graph SG1of SG extensible ifthere is another sub-s-graph SG2of SG such thatSG1|| SG2contains the same edges as SG.
Anexample of a sub-s-graph that is not extensible isthe sub-s-graph (b) of the s-graph in (a) in Fig.
2.Because sources can only be renamed or forgottenby the algebra operations, but never introduced,we can never attach the missing edge a: this canonly happen when 1 and 2 are sources.
As a gen-eral rule, a sub-s-graph can only be extensible ifit contains all edges that are adjacent to all of itsinternal nodes in SG.
Obviously, a graph parserneed only concern itself with sub-s-graphs that areextensible.We can further clarify the structure of extensiblesub-s-graphs by looking at the s-components of agraph.
Let U ?
V be some set of nodes.
Thisset splits the edges of G into equivalence classesthat are separated by U .
We say that two edgese, f ?
E are equivalent with respect toU , e ?Uf ,if there is a sequence v1e?
v2?
.
.
.
vk?1f?
vkwith v2, .
.
.
, vk?1/?
U , i.e.
if we can reach ffrom an endpoint of ewithout visiting a node in U .We call the equivalence classes of E with respectto ?Uthe s-components of G and denote the s-component that contains an edge e with [e].
InFig.
2(a), the edges a and f are equivalent withrespect to U = {4, 5}, but a and h are not.
The s-components are [a] = {a, b, c, d, e, f}, [g] = {g},and [h] = {h}.It can be shown that for any s-graph SG =1483(G,?
), a sub-s-graph SH with source nodes Uis extensible iff its edge set is the union of a setof s-components of G with respect to U .
We letan s-component representation C = (C, ?)
in thes-graph SG = (G,??)
consist of a source assign-ment ?
: S  V and a set C of s-components ofG with respect to the set VSC= ?
(S) ?
V ofsource nodes of ?.
Then we can represent everyextensible sub-s-graph SH = (H,?)
of SG bythe s-component representation C = (C, ?)
whereC is the set of s-components of which SH con-sists.
Conversely, we write T (C) for the uniqueextensible sub-s-graph of SG represented by thes-component representation C.The utility of s-component representations de-rives from the fact that merge can be evaluated onthese representations alone, as follows.Lemma 1.
Let C = (C, ?
), C1= (C1, ?1), C2=(C2, ?2) be s-component representations in the s-graph SG.
Then T (C) = T (C1) || T (C2) iff C =C1unionmultiC2(i.e., disjoint union) and ?1?
?2is defined,injective, and equal to ?.3.3 Boundary representationsIf there is no C such that all conditions of Lemma 1are satisfied, then T (C1) || T (C2) is not defined.In order to check this efficiently in the bottom-upparser, it will be useful to represent s-componentsexplicitly via their boundary.Consider an s-component representation C =(C, ?)
in SG and let E be the set of all edges thatare adjacent to a source node in VSCand containedin an s-component in C. Then we let the bound-ary representation (BR) ?
of C in the s-graph SGbe the pair ?
= (E, ?).
That is, ?
represents thes-components through the in-boundary edges, i.e.those edges inside the s-components (and thus thesub-s-graph) which are adjacent to a source.
TheBR ?
specifies C uniquely if the base graph SGis connected, so we write T (?)
for T (C) and VS?for VSC.In Fig.
2(a), the bold sub-s-graph is representedby ?
= ?
{d, e, f, g}, {A:4,B:5}?, indicating thatit contains the A-source 4 and the B-source 5; andfurther, that the edge set of the sub-s-graph is [d]?
[e] ?
[f ] ?
[g] = {a, b, c, d, e, f, g}.
The edge h(which is also incident to 5) is not specified, andtherefore not in the sub-s-graph.The following lemma can be shown about com-puting merge on boundary representations.
Intu-itively, the conditions (b) and (c) guarantee thatthe component sets are disjoint; the lemma thenfollows from Lemma 1.Lemma 2.
Let SG be an s-graph, and let ?1=(E1, ?1), ?2= (E2, ?2) be two boundary repre-sentations in SG.
Then T (?1) || T (?2) is definedwithin SG iff the following conditions hold:(a) ?1?
?2is defined and injective;(b) the two BRs have no in-boundary edges incommon, i.e.
E1?
E2= ?
;(c) for every source node v of ?1, the last edgeon the path in SG from v to the closest sourcenode of ?2is not an in-boundary edge of ?2,and vice versa.Furthermore, if these conditions hold, we haveT (?1|| ?2) = T (?1) || T (?2), where we define?1|| ?2= (E1?
E2, ?1?
?2).4 S-graph grammarsWe are now ready to define s-graph grammars,which describe languages of s-graphs.
We alsointroduce graph parsing and relate s-graph gram-mars to HRGs.4.1 Grammars for languages of s-graphsWe use interpreted regular tree grammars (IRTGs;Koller and Kuhlmann (2011)) to describe lan-guages of s-graphs.
IRTGs are a very generalmechanism for describing languages over and re-lations between arbitrary algebras.
They sepa-rate conceptually the generation of a grammaticalderivation from its interpretation as a string, tree,graph, or some other object.Consider, as an example, the tiny grammar inFig.
3; see Koller (2015) for linguistically mean-ingful grammars.
The left column consists of aregular tree grammar G (RTG; see e.g.
Comon etal.
(2008)) with two rules.
This RTG describes aregular language L(G) of derivation trees (in gen-eral, it may be infinite).
In the example, we canderive S ?
r1(VP) ?
r1(r2), therefore we havet = r1(r2) ?
L(G).We then use a tree homomorphism h to rewritethe derivation trees into terms over an algebra; inthis case the HR algebra.
In the example, the val-ues h(r1) and h(r2) are specified in the second col-umn of Fig.
3.
We compute h(t) by substitutingthe variable x1in h(r1) with h(r2).
The term h(t)is thus the one shown in (1).
It evaluates to thes-graph SGfin Fig.
1(f).1484Rule of RTG G homomorphism hS?
r1(VP) fS(fO(SGwant|| x1[R?
O]))VP?
r2SGsleepFigure 3: An example s-graph grammar.In general, the IRTG G = (G, h,A) generatesthe language L(G) = {Jh(t)K | t ?
L(G)}, whereJ?K is evaluation in the algebra A.
Thus, in theexample, we have L(G) = {SGf}.In this paper, we focus on IRTGs that describelanguages L(G) ?
A of objects in an algebra;specifically, of s-graphs in the HR algebra.
How-ever, IRTGs extend naturally to a synchronousgrammar formalism by adding more homomor-phisms and algebras.
For instance, the grammarsin Koller (2015) map each derivation tree simulta-neously to a string and an s-graph, and thereforedescribe a binary relation between strings and s-graphs.
We call IRTGs where at least one algebrais the HR algebra, s-graph grammars.4.2 Parsing with s-graph grammarsIn this paper, we are concerned with the pars-ing problem of s-graph grammars.
In the contextof IRTGs, parsing means that we are looking forthose derivation trees t that are (a) grammaticallycorrect, i.e.
t ?
L(G), and (b) match some giveninput object a, i.e.
h(t) evaluates to a in the al-gebra.
Because the set P of such derivation treesmay be large or infinite, we aim to compute anRTG Gasuch that L(Ga) = P .
This RTG playsthe role of a parse chart, which represents the pos-sible derivation trees compactly.In order to compute Ga, we need to solve twoproblems.
First, we need to determine all the pos-sible ways in which a can be represented by terms?
over the algebra A.
This is familiar from stringparsing, where a CKY parse chart spells out allthe ways in which larger substrings can be decom-posed into smaller parts by concatenation.
Sec-ond, we need to identify all those derivation treest ?
L(G) that map to such a decomposition ?
,i.e.
for which h(t) evaluates to a.
In string pars-ing, this corresponds to retaining only such de-compositions into substrings that are justified bythe grammar rules.While any parsing algorithm must address bothof these issues, they are usually conflated, in thatparse items combine information about the de-composition of a (such as a string span) with infor-mation about grammaticality (such as nonterminalsymbols).
In IRTG parsing, we take a different,more generic approach.
We assume that the setD of all decompositions ?
, i.e.
of all terms ?
thatevaluate to a in the algebra, can be representedas the language D = L(Da) of a decompositiongrammar Da.
Dais an RTG over the signature ofthe algebra.
Crucially, Daonly depends on the al-gebra and a itself, and not on G or h, because Dcontains all terms that evaluate to a and not justthose that are licensed by the grammar.
However,we can compute GafromDaefficiently by exploit-ing the closure of regular tree languages under in-tersection and inverse homomorphism; see Kollerand Kuhlmann (2011) for details.In practice, this means that whenever we wantto apply IRTGs to a new algebra (as, in this pa-per, to the HR algebra), we can obtain a parsingalgorithm by specifying how to compute decom-position grammars over this algebra.
This is thetopic of Section 5.4.3 Relationship to HRGWe close our exposition of s-graph grammars byrelating them to HRGs.
It is known that the graphlanguages that can be described with s-graphgrammars are the same as the HRG languages(Courcelle and Engelfriet, 2012, Prop.
4.27).
Herewe establish a more precise equivalence result, sowe can compare our asymptotic runtimes directlyto those of HRG parsers.An HRG rule, such as the one shown in Fig.
4,rewrites a nonterminal symbol into a graph.
Theexample rule constructs a graph for the nontermi-nal S by combining the graph Grin the middle(with nodes 1, 2, 3 and edges e, f ) with graphsGXand GYthat are recursively derived from the non-terminals X and Y .
The combination happens bymerging the external nodes of GXand GYwithnodes of Gr: the squiggly lines indicate that theexternal node I of GXshould be 1, and the ex-ternal node II should be 2.
Similarly the externalnodes of GYare unified with 1 and 3.
Finally, theexternal nodes I and II of the HRG rule for S itself,shaded gray, are 1 and 3.The fundamental idea of the HRG-to-IRTGtranslation is to encode external nodes as sources,and to use rename and merge to unify the nodes ofthe different graphs.
In the example, we might saythat the external nodes of GXand GYare repre-sented using the source names I and II, and extendGrto an s-graph by saying that the nodes 1, 2, and14853 are its I-source, III-source, and II-source respec-tively.
This results in the expression(2) fIII(?I?e?
?III?
|| x1[II?
III]|| ?I?f?
?II?
|| x2)where we write ??I?e?
?III??
for the s-graph con-sisting of the edge e, with node 1 as I-source and2 as III-source.However, this requires the use of three sourcenames (I, II, and III).
The following encoding ofthe rule uses the sources more economically:(3) fII(?I?e?
?II?
|| x1) || ?I?f?
?II?
|| x2This term uses only two source names.
It forgetsII as soon as we are finished with the node 2, andfrees the name up for reuse for 3.
The completeencoding of the HRG rule consists of the RTG ruleS?
r(X,Y) with h(r) = (3).In the general case, one can ?read off?
possibleterm encodings of a HRG rule from its tree decom-positions; see Chiang et al (2013) or Def.
2.80 ofCourcelle and Engelfriet (2012) for details.
A treedecomposition is a tree, each of whose nodes pi islabeled with a subset Vpiof the nodes in the HRGrule.
We can construct a term encoding from a treedecomposition bottom-up.
Leaves map to vari-ables or constants; binary nodes introduce mergeoperations; and we use rename and forget oper-ations to ensure that the subterm for the node pievaluates to an s-graph in which exactly the nodesin Vpiare source nodes.2In the example, we obtain(3) from the tree decomposition in Fig.
4 like this.The tree-width k of an HRG rule is measuredby finding the tree decomposition of the rule forwhich the node sets have the lowest maximum sizes and setting k = s?
1.
It is a crucial measure be-cause Chiang et al?s parsing algorithm is exponen-tial in k. The translation we just sketched uses ssource names.
Thus we see that a HRG with rulesof tree-width ?
k can be encoded into an s-graphgrammar with k + 1 source names.
(The converseis also true.
)5 Graph parsing with s-graph grammarsNow we show how to compute decompositiongrammars for the s-graph algebra.
As we ex-plained in Section 4.2, we can then obtain a com-plete parser for s-graph grammars through genericmethods.2This uses the swap operations mentioned in Footnote 1.I1II3A2X YS?GrfeIIIIIIe : 1,2 f : 1,3X : 1,21,2Y : 1,31,31,3Figure 4: An HRG rule (left) with one of its treedecompositions (right).Given an s-graph SG, the language of the de-composition grammar DSGis the set of all termsover the HR algebra that evaluate to SG.
For ex-ample, the decomposition grammar for the graphSG in Fig.
1(a) contains ?
among many others ?the following two rules:(4) SG?
fR(SGf)(5) SGe?
|| (SGb, SGd),where SGf, SGe, SGb, and SGdare the graphsfrom Fig.
1 (see Section 3.1).
In other words,DSGkeeps track of sub-s-graphs in the nonterminals,and the rules spell out how ?larger?
sub-s-graphscan be constructed from ?smaller?
sub-s-graphsusing the operations of the HR algebra.
The al-gorithms below represent sub-s-graphs compactlyusing s-component and boundary representations.Because the decomposition grammars in the s-graph algebra can be very large (see Section 6),we will not usually compute the entire decompo-sition grammar explicitly.
Instead, it is sufficientto maintain a lazy representation of DSG, whichallows us to answer queries to the decompositiongrammar efficiently.
During parsing, such querieswill be generated by the generic part of the pars-ing algorithm.
Specifically, we will show how toanswer the following types of query:?
Top-down: given an s-component represen-tation C of some s-graph and an algebraoperation o, enumerate all the rules C ?o(C1, .
.
.
, Ck) inDSG.
This asks how a largersub-s-graph can be derived from other sub-s-graphs using the operation o.
In the exampleabove, a query for SG and fR(?)
should yield,among others, the rule in (4).?
Bottom-up: given boundary representations?1, .
.
.
, ?kand an algebra operation o, enu-merate all the rules ?
?
o(?1, .
.
.
, ?k) inDSG.
This asks how smaller sub-s-graphscan be combined into a bigger one using the1486forget rename mergebottom-up O(d + s) O(s) O(ds)top-down O(ds) O(s) O(ds)I = # rules O(ns2ds) O(ns2ds) O(ns3ds)Table 1: Amortized per-rule runtimes T for thedifferent rule types.operation o.
In the example above, a mergequery for SGband SGdshould yield the rulein (5).
Unlike in the top-down case, everybottom-up query returns at most one rule.The runtime of the complete parsing algorithmis bounded by the number I of different queriesto DSGthat we receive, multiplied by the per-rule runtime T that we need to answer each query.The factor I is analogous to the number of ruleinstances in schema-based parsing (Shieber et al,1995).
The factor T is often ignored in the anal-ysis of parsing algorithms, because in parsingschemata for strings, we typically have T = O(1).This need not be the case for graph parsers.
In theHRG parsing schema of Chiang et al (2013), wehave I = O(nk+13d(k+1)), where k is the tree-width of the HRG.
In addition, each of their ruleinstances takes time T = O(d(k + 1)) to actuallycalculate the new item.Below, we show how we can efficiently answerboth bottom-up and top-down queries toDSG.
Ev-ery s-graph grammar has an equivalent normalform where every constant describes an s-graphwith a single edge.
Assuming that the grammaris in this normal form, queries of the form ?
?
g(resp.
C ?
g), where g is a constant of the HR-algebra, are trivial and we will not consider themfurther.
Table 1 summarizes our results.5.1 Bottom-up decompositionForget and rename.
Given a boundary repre-sentation ?
?= (E?, ??
), answering the bottom-upforget query ?
?
fA(??)
amounts to verifying thatall edges incident to ??
(A) are in-boundary in ?
?,since otherwise the result would not be extensible.This takes time O(d).
We then let ?
= (E, ?
),where ?
is like ?
?but undefined on A, and E is theset of edges in E?that are still incident to a sourcein ?.
Computing ?
thus takes time O(d+ s).The rename operation works similarly, but sincethe edge set remains unmodified, the per-rule run-time is O(s).A BR is fully determined by specifying the nodeand in-boundary edges for each source name, sothere are at most O((n2d)s)different BRs.
Sincethe result of a forget or rename rule is determinedby the child ?
?, this is an upper bound for the num-ber I of rule instances of forget or rename.Merge.
Now consider the bottom-up mergequery for the boundary representations ?1and ?2.As we saw in Section 3.3, T (?1) || T (?2) is notalways defined.
But if it is, we can answer thequery with the rule (?1|| ?2)?
|| (?1, ?2), with?1|| ?2defined as in Section 3.3.
Computing thisBR takes time O(ds).We can check whether T (?1) || T (?2) is de-fined by going through the conditions of Lemma 2.The only nontrivial condition is (c).
In order tocheck it efficiently, we precompute a data struc-ture which contains, for any two nodes u, v ?
V ,the length k of the shortest undirected path u =v1?
.
.
.e?
vk= v and the last edge e on thispath.
This can be done in time O(n3) using theFloyd-Warshall algorithm.
Checking (c) for everysource pair then takes time O(s2) per rule, but be-cause sources that are common to both ?1and ?2automatically satisfy (c) due to (a), one can showthat the total runtime of checking (c) for all mergerules of DSG is O(ns3dss).Observe finally that there are I = O(ns3ds)instances of the merge rule, because each of theO(ds) edges that are incident to a source node canbe either in ?1, in ?2, or in neither.
Thereforethe runtime for checking (c) amortizes to O(s) perrule.
The Floyd-Warshall step amortizes to O(1)per rule for s ?
3; for s ?
2 the node table canbe computed in amortized O(1) using more spe-cialized algorithms.
This yields a total amortizedper-rule runtime T for bottom-up merge ofO(ds).5.2 Top-down decompositionFor the top-down queries, we specify sub-s-graphsin terms of their s-component representations.
Thenumber I of instances of each rule type is the sameas in the bottom-up case because of the one-to-one correspondence of s-component and bound-ary representations.
We focus on merge and forgetqueries; rename is as above.Merge.
Given an s-component representationC = (C, ?
), a top-down merge query asks us toenumerate the rules C ?
|| (C1, C2) such thatT (C1) || T (C2) = T (C).
By Lemma 1, wecan do this by using every distribution of the s-components in C over C1and C2and restricting ?1487accordingly.
This brings the per-rule time of top-down merge to O(ds), the maximum number ofs-components in C.Block-cutpoint graphs.
The challenging queryto answer top-down is forget.
We will first de-scribe the problem and introduce a data structurethat supports efficient top-down forget queries.Consider top-down forget queries onthe sub-s-graph SG1drawn in bold inFig.
2(a); its s-component representation is?
{[a], [g]}, {A:4,B:5}?.
A top-down forget mightpromote the node 3 to a C-source, yielding asub-s-graph SG2(that is, fC(SG2) is the orig-inal s-graph SG1).
In SG2, a, e, and f areno longer equivalent; its s-component repre-sentation is ?
{[a], [e], [f ], [g]}, {A:4,B:5,C:3}?.Thus promoting 3 to a source splits the originals-component into smaller parts.By contrast, the same top-down forget mightalso promote the node 1 to a C-source, yield-ing a sub-s-graph SG3; fC(SG3) is also SG1.However, all edges in [a] are still equiva-lent in SG3; its s-component representation is?
{[a], [g]}, {A:4,B:5,C:1}?.An algorithm for top-down forget must be ableto determine whether promotion of a node splitsan s-component or not.
To do this, let G be the in-put graph.
We create an undirected auxiliary graphGUfrom G and a set U of (source) nodes.
GUcontains all nodes in V \U , and for each edge ethat is incident to a node u ?
U , it contains a node(u, e).
Furthermore, GUcontains undirected ver-sions of all edges inG; if an edge e ?
E is incidentto a node u ?
U , it becomes incident to (u, e) inGUinstead.
The auxiliary graph G{4,5}for ourexample graph is shown in Fig.
2(d).Two edges are connected in GUif and only ifthey are equivalent with respect to U in G. There-fore, promotion of u splits s-components iff u is acutpoint in GU, i.e.
a node whose removal discon-nects the graph.
Cutpoints can be characterizedas those nodes that belong to multiple biconnectedcomponents (BCCs) of GU, i.e.
the maximal sub-graphs such that any node can be removed withoutdisconnecting a graph segment.
In Fig.
2(d), theBCCs are indicated by the dotted boxes.
Observethat 3 is a cutpoint and 1 is not.For any given U , we can represent the structureof the BCCs of GUin its block-cutpoint graph.This is a bipartite graph whose nodes are the cut-points and BCCs of GU, and a BCC is connectedto all of its cutpoints; see Fig.
2(e) for the block-cutpoint graph of the example.
Block-cutpointgraphs are always forests, with the individual treesrepresenting the s-components of G. Promotinga cutpoint u splits the s-component into smallerparts, each corresponding to an incident edge of u.We annotate each edge with that part.Forget.
We can now answer a top-down forgetquery C ?
fA(C?)
efficiently from the block-cutpoint graph for the sources of C = (C, ?).
Weiterate over all components c ?
C, and then overall internal nodes u of c. If u is not a cutpoint,we simply let C?= (C?, ??)
by making u an A-source and letting C?= C. Otherwise, we alsoremove c from C and add the new s-componentson the edges adjacent to u in the block-cutpointgraph.
The query returns rules for all C?that canbe constructed like this.The per-rule runtime of top-down forget isO(ds), the time needed to compute C?in the cut-point case.
We furthermore precompute the block-cutpoint graphs for the input graph with respect toall sets U ?
V of nodes with |U | ?
s ?
1.
Foreach U , we can compute the block-cutpoint graphand annotate its edges in time O(nd2s).
Thus thetotal time for the precomputation is O(ns?
d2s),which amortizes to O(1) per rule.6 EvaluationWe evaluate the performance of our algorithms onthe ?Little Prince?
AMR-Bank version 1.4, avail-able from amr.isi.edu.
This graph-bank con-sists of 1562 sentences manually annotated withAMRs.
We implemented our algorithms in Javaas part of the Alto parser for IRTGs (Alto Devel-opers, 2015), and compared them to the BolinasHRG parser (Andreas et al, 2013).
We measuredruntimes using Java 8 (for Alto) and Pypy 2.5.0(for Bolinas) on an Intel Xeon E7-8857 CPU at 3GHz, after warming up the JIT compilers.As there are no freely available grammars forthis dataset, we created our own for the evalua-tion, using Bayesian grammar induction roughlyalong the lines of Cohn et al (2010).
We pro-vide the grammars as supplementary material.Around 64% of the AMRs in the graph-bank havetreewidth 1 and can thus be parsed using s = 2source names.
98% have treewidth 1 or 2, corre-sponding to s = 3 source names.
All experimentsevaluated parser times on the same AMRs fromwhich the grammar was sampled.1488Top-down versus bottom-up.
Fig.
5 comparesthe performance of the top-down and the bottom-up algorithm, on a grammar with three sourcenames sampled from all 1261 graphs with up to10 nodes.
Each point in the figure is the geometricmean of runtimes for all graphs with a given num-ber of nodes; note the log-scale.
We aborted thetop-down parser after its runtimes grew too large.We observe that the bottom-up algorithm out-performs the top-down algorithm, and yields prac-tical runtimes even for nontrivial graphs.
One pos-sible explanation for the difference is that the top-down algorithm spends more time analyzing un-grammatical s-graphs, particularly subgraphs thatare not connected.Comparison to Bolinas.
We also compare ourimplementations to Bolinas.
Because Bolinas ismuch slower than Alto, we restrict ourselves totwo source names (= treewidth 1) and sampled thegrammar from 30 randomly chosen AMRs each ofsize 2 to 8, plus the 21 AMRs of size one.Fig.
6 shows the runtimes.
Our parsers aregenerally much faster than in Fig.
5, due to thedecreased number of sources and grammar size.They are also both much faster than Bolinas.
Mea-suring the total time for parsing all 231 AMRs,our bottom-up algorithm outperforms Bolinas by afactor of 6722.
The top-down algorithm is slower,but still outperforms Bolinas by a factor of 340.Further analysis.
In practice, memory use canbe a serious issue.
For example, the decomposi-tion grammar for s=3 for AMR #194 in the corpushas over 300 million rules.
However, many usesof decomposition grammars, such as sampling forgrammar induction, can be phrased purely in termsof top-down queries.
The top-down algorithm cananswer these without computing the entire gram-mar, alleviating the memory problem.Finally, we analyzed the asymptotic runtimes inTable 1 in terms of the maximum number d ?
s ofin-boundary edges.
However, the top-down parserdoes not manipulate individual edges, but entires-components.
The maximum number Dsof s-components into which a set of s sources can splita graph is called the s-separability of G by Laute-mann (1990).
We can analyze the runtime of thetop-down parser more carefully as O(ns3Dsds);as the dotted line in Fig.
5 shows, this predictsthe runtime well.
Interestingly, Dsis much lowerin practice than its theoretical maximum.
In the???????
??
?
?O(n3 ?
3D3 ?
3 ?
d)Bottom?upTop?down1 2 3 4 5 6 7 8 9 10110100100001e+06Node count[ms]Figure 5: Runtimes of our parsers with s = 3.????
?
??
??
Bottom?upTop?downBolinas1 2 3 4 5 6 7 811010010001e+05Node count[ms]Figure 6: Runtimes of our parsers and Bolinaswith s = 2.?Little Prince?
AMR-Bank, the mean ofD3is 6.0,whereas the mean of 3 ?
d is 12.7.
Thus exploit-ing the s-component structure of the graph can im-prove parsing times.7 ConclusionWe presented two new graph parsing algorithmsfor s-graph grammars.
These were framed interms of top-down and bottom-up queries to a de-composition grammar for the HR algebra.
Ourimplementations outperform Bolinas, the previ-ously best system, by several orders of magnitude.We have made them available as part of the Altoparser.A challenge for grammar-based semantic pars-ing is grammar induction from data.
We will ex-plore this problem in future work.
Furthermore,we will investigate methods for speeding up graphparsing further, e.g.
with different heuristics.Acknowledgments.
We thank the anonymousreviewers for their comments, and Daniel Bauerfor his help with Bolinas.
We received valuablefeedback at the 2015 Dagstuhl seminar on graphgrammars and the 2014 Johns Hopkins workshopin Prague.
This work was supported by the DFGgrant KO 2916/2-1.1489ReferencesAlto Developers.
2015.
Alto: algebraic languagetoolkit for parsing and decoding with IRTGs.
Avail-able at https://bitbucket.org/tclup/alto.Jacob Andreas, Daniel Bauer, Karl Moritz Hermann,Bevan Jones, Kevin Knight, and David Chiang.2013.
Bolinas graph processing package.
Availableat http://www.isi.edu/publications/licensed-sw/bolinas/.
Downloaded in Jan-uary 2015.Laura Banarescu, Claire Bonial, Shu Cai, MadalinaGeorgescu, Kira Griffitt, Ulf Hermjakob, KevinKnight, Philipp Koehn, Martha Palmer, and NathanSchneider.
2013.
Abstract meaning representationfor sembanking.
In Proceedings of the 7th Lin-guistic Annotation Workshop & Interoperability withDiscourse, pages 178?186.David Chiang, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, Bevan Jones, and KevinKnight.
2013.
Parsing graphs with hyperedgereplacement grammars.
In Proceedings of the 51stAnnual Meeting of the Association for Computa-tional Linguistics, pages 924?932.Trevor Cohn, Phil Blunsom, and Sharon Goldwater.2010.
Inducing tree-substitution grammars.
Journalof Machine Learning Research (JMLR), 11:3053?3096.Hubert Comon, Max Dauchet, R?emi Gilleron, Flo-rent Jacquemard, Denis Lugiez, Christof L?oding,Sophie Tison, and Marc Tommasi.
2008.
Treeautomata techniques and applications.
http://tata.gforge.inria.fr/.Bruno Courcelle and Joost Engelfriet.
2012.
GraphStructure and Monadic Second-Order Logic, vol-ume 138 of Encyclopedia of Mathematics and itsApplications.
Cambridge University Press.Frank Drewes, Hans-J?org Kreowski, and Annegret Ha-bel.
1997.
Hyperedge replacement graph gram-mars.
In Grzegorz Rozenberg, editor, Handbook ofGraph Grammars and Computing by Graph Trans-formation, pages 95?162.
World Scientific Publish-ing Co., Inc.Jeffrey Flanigan, Sam Thomson, Jamie Carbonell,Chris Dyer, and Noah A. Smith.
2014.
A discrim-inative graph-based parser for the abstract meaningrepresentation.
In Proceedings of the 52nd AnnualMeeting of the Association for Computational Lin-guistics, pages 1426?1436, Baltimore, Maryland.Bevan K. Jones, Jacob Andreas, Daniel Bauer,Karl Moritz Hermann, and Kevin Knight.
2012.
Se-mantics ?
Based machine translation with hyperedgereplacement grammars.
In Proceedings of COLING2012: Technical Papers, pages 1359?1376.Alexander Koller and Marco Kuhlmann.
2011.
A gen-eralized view on parsing and translation.
In Pro-ceedings of the 12th International Conference onParsing Technologies, pages 2?13.Alexander Koller.
2015.
Semantics construction withgraph grammars.
In Proceedings of the 11th Inter-national Conference on Computational Semantics(IWCS), pages 228?238.Clemens Lautemann.
1988.
Decomposition trees:Structured graph representation and efficient algo-rithms.
In Max Dauchet and Maurice Nivat, editors,13th Colloquium on Trees in Algebra and Program-ming, volume 299 of Lecture Notes in Computer Sci-ence, pages 28?39.
Springer Berlin Heidelberg.Clemens Lautemann.
1990.
The complexity ofgraph languages generated by hyperedge replace-ment.
Acta Informatica, 27:399?421.Andr?e F. T. Martins and Mariana S. C. Almeida.
2014.Priberam: A turbo semantic parser with second or-der features.
In Proceedings of the 8th InternationalWorkshop on Semantic Evaluation (SemEval 2014),pages 471?476.Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,Daniel Zeman, Dan Flickinger, Jan Hajic, AngelinaIvanova, and Yi Zhang.
2014.
SemEval 2014 Task8: Broad-coverage semantic dependency parsing.
InProceedings of the 8th International Workshop onSemantic Evaluation (SemEval 2014), pages 63?72.Daniel Quernheim and Kevin Knight.
2012.
DAG-GER: A toolkit for automata on directed acyclicgraphs.
In Proceedings of the 10th InternationalWorkshop on Finite State Methods and Natural Lan-guage Processing, pages 40?44.Stuart M. Shieber, Yves Schabes, and Fernando C. N.Pereira.
1995.
Principles and implementation ofdeductive parsing.
Journal of Logic Programming,24(1?2):3?36.1490
