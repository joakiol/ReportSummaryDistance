Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 163?171,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsUsing Mostly Native Data to Correct Errors in Learners' Writing:A Meta-Classifier ApproachMichael GamonMicrosoft ResearchOne Microsoft WayRedmond, WA 98052mgamon@microsoft.comAbstractWe present results from a range of experi-ments on article and preposition error correc-tion for non-native speakers of English.
Wefirst compare a language model and error-specific classifiers (all trained on large Eng-lish corpora) with respect to their performancein error detection and correction.
We thencombine the language model and the classifi-ers in a meta-classification approach by com-bining evidence from the classifiers and thelanguage model as input features to the meta-classifier.
The meta-classifier in turn is trainedon error-annotated learner data, optimizing theerror detection and correction performance onthis domain.
The meta-classification approachresults in substantial gains over the classifier-only and language-model-only scenario.
Sincethe meta-classifier requires error-annotateddata for training, we investigate how muchtraining data is needed to improve results overthe baseline of not using a meta-classifier.
Allevaluations are conducted on a large error-annotated corpus of learner English.1 IntroductionResearch on the automatic correction of grammati-cal errors has undergone a renaissance in the pastdecade.
This is, at least in part, based on the recog-nition that non-native speakers of English nowoutnumber native speakers by 2:1 in some esti-mates, so any tool in this domain could be of tre-mendous value.
While earlier work in both nativeand non-native error correction was focused on theconstruction of grammars and analysis systems todetect and correct specific errors (see Heift andSchulze, 2005 for a detailed overview), more re-cent approaches have been based on data-drivenmethods.The majority of the data-driven methods use aclassification technique to determine whether aword is used appropriately in its context, continu-ing the tradition established for contextual spellingcorrection by Golding (1995) and Golding andRoth (1996).
The words investigated are typicallyarticles and prepositions.
They have two distinctadvantages as the subject matter for investigation:They are a closed class and they comprise a sub-stantial proportion of learners?
errors.
The investi-gation of preposition corrections can even benarrowed further: amongst the more than 150 Eng-lish prepositions, the usage of the ten most fre-quent prepositions accounts for 82% of prepositionerrors in the 20 million word Cambridge Universi-ty Press Learners?
Corpus.
Learning correct articleuse is most difficult for native speakers of an L1that does not overtly mark definiteness and indefi-niteness as English does.
Prepositions, on the oth-er hand, pose difficulties for language learnersfrom all L1 backgrounds (Dalgish, 1995; Bitcheneret al, 2005).Contextual classification methods represent thecontext of a preposition or article as a feature vec-tor gleaned from a window of a few words aroundthe preposition/article.
Different systems typicallyvary along three dimensions: choice of features,choice of classifier, and choice of training data.Features range from words and morphological in-formation (Knight and Chander, 1994) to the inclu-sion of part-of-speech tags (Minnen et al, 2000;Han et al, 2004, 2006; Chodorow et al, 2007;Gamon et al, 2008, 2009; Izumi et al, 2003, 2004;Tetrault and Chodorow, 2008) to features based onlinguistic analysis and on WordNet (Lee, 2004;DeFelice and Pulman, 2007, 2008).
Knight andChander (1994) and Gamon et al (2008) used de-cision tree classifiers but, in general, maximumentropy classifiers have become the classification163algorithm of choice.
Training data are normallydrawn from sizeable corpora of native English text(British National Corpus for DeFelice and Pulman(2007, 2008), Wall Street Journal in Knight andChander (1994), a mix of Reuters and Encarta inGamon et al (2008, 2009).
In order to partiallyaddress the problem of domain mismatch betweenlearners?
writing and the news-heavy data sets of-ten used in data-driven NLP applications, Han etal.
(2004, 2006) use 31.5 million words from theMetaMetrics corpus, a diverse corpus of fiction,non-fiction and textbooks categorized by readinglevel.In addition to the classification approach to errordetection, there is a line of research - going back toat least Atwell (1987) - that uses language models.The idea here is to detect errors in areas where thelanguage model score is suspiciously low.
Atwell(1987) uses a part-of-speech tag language model todetect errors, Chodorow and Leacock (2000) usemutual information and chi square statistics toidentify unlikely function word and part-of-speechtag sequences, Turner and Charniak (2007) employa language model based on a generative statisticalparser, and Stehouwer and van Zaanen (2009) in-vestigate a diverse set of language models withdifferent backoff strategies to determine whichchoice, from a set of confusable words, is mostlikely in a given context.
Gamon et al (2008,2009) use a combination of error-specific classifi-ers and a large generic language model with hand-tuned heuristics for combining their scores to max-imize precision.
Finally, Yi et al (2008) and Her-met et al (2008) use n-gram counts from the webas a language model approximation to identifylikely errors and correction candidates.2 Our ApproachWe combine evidence from the two kinds of data-driven models that have been used for error detec-tion and correction (error-specific classifiers and alanguage model) through a meta-classifier.
We usethe term primary models for both the initial error-specific classifiers and a large generic languagemodel.
The meta-classifier takes the output of theprimary models (language model scores and classprobabilities) as input.
Using a meta-classifier forensemble learning has been proven effective formany machine learning problems (see e.g.
Diette-rich 1997), especially when the combined modelsare sufficiently different to make distinct kinds oferrors.
The meta-classification approach also hasan advantage in terms of data requirements: Ourprimary models are trained on large sets of widelyavailable well-formed English text.
The meta-classifier, in contrast, is trained on a smaller set oferror-annotated learner data.
This allows us to ad-dress the problem of domain mismatch: We canleverage large well-formed data sets that are sub-stantially different from real-life learner languagefor the primary models, and then fine-tune the out-put to learner English using a much smaller set ofexpensive and hard-to-come-by annotated learnerwriting.For the purpose of this paper, we restrict our-selves to article and preposition errors.
The ques-tions we address are:1.
How effective is the meta-classification ap-proach compared to either a classifier or a lan-guage model alone?2.
How much error-annotated data are sufficientto produce positive results above the baselineof using either a language model or a classifieralone?Our evaluation is conducted on a large data setof error-annotated learner data.3 Experimental Design3.1 Primary ModelsOur error-specific primary models are maximumentropy classifiers (Rathnaparki 1997) for articlesand for prepositions.
Features include contextualfeatures from a window of six tokens to the rightand left, such as lexical features (word), part-of-speech tags, and a handful of ?custom features?,for example lexical head of governing VP or go-verned NP (as determined by part-of-speech-tagbased heuristics).
For both articles and preposi-tions, we employ two classifiers: the first deter-mines the probability that a preposition/article ispresent in a given context (presence classifier), thesecond classifier determines the probability that aspecific article or preposition is chosen (choiceclassifier).
A training event for the presence clas-sifier is any noun phrase boundary that is a poten-tial location for a preposition or article.
Whether alocation is an NP boundary and a potential site foran article/preposition is determined by a simpleheuristic based on part-of-speech tags.164The candidates for article choice are the anda/an, and the choice for prepositions is limited totwelve very frequent prepositions (in, at, on, for,since, with, to, by, about, from, of, as) which ac-count for 86.2 % of preposition errors in our learn-er data.
At prediction time, the presence and choiceclassifiers produce a list of potential changes inpreposition/article usage for the given context.Since the application of our system consists ofsuggesting corrections to a user, we do not consid-er identity operations where the suggested wordchoice equals the actual word choice.
For a poten-tial preposition/article location where there is nopreposition/article, each of the candidates is consi-dered for an insertion operation.
For a potentiallocation that contains a preposition/article, thepossible operations include deletion of the existingtoken or substitution with another preposi-tion/article from the candidate set.
Training datafor the classifiers is a mix of primarily well-formeddata sources: There are about 2.5 million sen-tences, distributed roughly equally across Reutersnewswire, Encarta encyclopedia, UN proceedings,Europarl and web-scraped general domain data1.From the total set of candidate operations (substi-tutions, insertions, and deletions) that each combi-nation of presence and choice classifier producesfor prepositions, we consider only the top threehighest-scoring operations2.Our language model is trained on the Gigawordcorpus (Linguistic Data Consortium, 2003) andutilizes 7-grams with absolute discount smoothing(Gao, Goodman, and Miao, 2001; Nguyen, Gao,and Mahajan, 2007).
Each suggested revision fromthe preposition/article classifiers (top three for pre-positions, all revisions from the article classifiers)are scored by the language model: for each revi-sion, the language model score of the original andthe suggested rewrite is recorded, as is the lan-guage model entropy (defined as the languagemodel probability of the sentence, normalized bysentence length).1 We are not able to train the error-specific classifiers on alarger data set like the one we use for the language model.Note that the 2.5 million sentences used in the classifier train-ing already produce 16.5 million training vectors.2 This increases runtime performance because fewer calls needto be made to the language model which resides on a server.
Inaddition, we noticed that overall precision is increased by notconsidering the less likely suggestions by the classifier.3.2 Meta-ClassifierFor the meta-classifier we chose to use a decisiontree, trained with the WinMine toolkit (Chickering2002).
The motivation for this choice is that deci-sion trees are well-suited for continuously valuedfeatures and for non-linear decision surfaces.
Anobvious alternative would be to use a support vec-tor machine with non-linear kernels, a route thatwe have not explored yet.
The feature set for themeta-classifier consists of the following scoresfrom the primary models, including some arithmet-ic combinations of scores:Ratio and delta of Log LM score of the origi-nal word choice and the suggested revision (2features)Ratio and delta of the LM entropy for origi-nal and suggested revision (2 features).Products of the above ratios/deltas and clas-sifier choice/presence probabilitiesType of operation: deletion, insertion, substi-tution (3 features)P(presence) (1 feature)For each preposition/article choice:P(choice): 13 features for prepositions (12prepositions and other for a preposition notin that set), 2 for articlesOriginal token: none (for insertion) or theoriginal preposition/article (13 features forprepositions, 2 for articles)Suggested token: none (for deletion) or thesuggested preposition/article (13 features forprepositions, 2 for articles)The total number of features is 63 for preposi-tions and 36 for articles.The meta-classifier is trained by collecting sug-gested corrections from the primary models on theerror annotated data.
The error-annotation providesthe binary class label, i.e.
whether the suggestedrevision is correct or incorrect.
If the suggestedrevision matches an annotated correction, it countsas correct, if it does not match it counts as incor-rect.
To give an example, the top three prepositionoperations for the position before this test in thesentence I rely to this test are:Change_to_onDelete_toChange_to_ofThe class label in this example is "suggestioncorrect", assuming that the change of preposition is165annotated in the data.
The operation Change_to_onin this example has the following feature values forthe basic classifier and LM scores:classifier P(choice): 0.755classifier P(presence): 0.826LM logP(original): -17.373LM logP(rewrite): -14.184An example of a path through the decision treemeta-classifier for prepositions is:LMLogDelta is Not < -8.59  andLMLogDelta is Not < -3.7  andProductRewriteLogRatioConf is Not < -0.00115  andLMLogDelta is Not < -1.58  andProductOrigEntropyRatioChoiceConf is Not < -0.00443  andchoice_prob is Not < 0.206  andOriginal_of is 0  andchoice_prob is Not < 0.329  andto_prob is < 0.108  andSuggested_on is 1  andOriginal_in is 0  andchoice_prob is Not < 0.497  andchoice_prob is Not < 0.647  andpresence_prob is Not < 0.553The leaf node at the end of this path has a 0.21probability of changing ?to?
to ?on?
being a cor-rect rewrite suggestion.The features selected by the decision trees rangeacross all of the features discussed above.
For boththe article and preposition meta-classifiers, theranking of features by importance (as measured byhow close to the root the decision tree uses the fea-ture) follows the order in which features are listed.3.3 DataIn contrast to the training data for the primarymodels, the meta-classifier is trained on error-annotated data from the Cambridge UniversityPress Learners?
Corpus (CLC).
The version ofCLC that we have licensed currently contains atotal of 20 million words from learner English es-says written as part of one of Cambridge?s EnglishLanguage Proficiency Tests (ESOL) ?
at all profi-ciency levels.
The essays are annotated for errortype, erroneous span and suggested correction.We first perform a random split of the essays in-to 70% training, 20% test and 10% for parametertuning.
Next, we create error-specific training, tun-ing and test sets by performing a number of clean-up steps on the data.
First, we correct all errors thatwere flagged as being spelling errors, since wepresume that the user will perform a spelling checkon the data before proceeding to grammaticalproofing.
Spelling errors that were flagged as mor-phology errors were left alone.
By the same token,we corrected confused words that are covered byMS Word.
We then revised British English spel-ling to American English spelling conventions.
Inaddition, we eliminated all annotations for non-pertinent errors (i.e., non-preposition/article errors,or errors that do not involve any of the targetedprepositions), but we maintained the original (er-roneous) text for these.
This makes our task hardersince we will have to learn how to make predic-tions in text containing multiple errors, but it alsois a more realistic scenario given real learner writ-ing.
Finally, we eliminated sentences containingnested errors and immediately adjacent errorswhen they involve pertinent (preposition/article)errors.
For example, an annotated error "take a pic-ture" with the correction "take pictures" is anno-tated as two consecutive errors: "delete a" and"rewrite picture as pictures".
Since the error in-volves operations on both the article and the noun,which our article correction module is not designedto cover, we eliminated the sentence from the data.
(This last step eliminated 31% of the sentencesannotated with preposition errors and 29% or thesentences annotated with article errors.)
Sentencesthat were flagged for a replacement error but con-tained no replacement were also eliminated fromthe data.The final training, tuning and test set sizes are asfollows (note that for prepositions we had to re-duce the size of the training set by an additional20% in order to avoid memory limitations of ourdecision tree tools).Prepositions:train: 584,485 sentences, 68,806 prep errorstuning: 105,166 sentences, 9918 prep errorstest:  208,724 sentences, 19,706 prep errorsArticles:train: 737,091 sentences, 58,356 article errorstuning: 106,052 sentences, 8341 article errorstest: 210,577 sentences, 16,742 article errorsThis mix is strongly biased towards ?correct?usage.
After all, there are many more correct usesof articles and prepositions in the CLC data thanincorrect ones.
Again, this is likely to make ourtask harder, but more realistic, since both at train-166ing and test time we are working with the errordistribution that is observed in learner data.3.4 EvaluationTo evaluate, we run our meta-classifier system onthe preposition and article test sets described inabove and calculate precision and recall.
Precisionand recall for the overall system are controlled bythresholding the meta-classifier class probability.As a point of comparison, we also evaluate the per-formance of the primary models (the error-specificclassifier and the language model) in isolation.Precision and recall for the error-specific classifieris controlled by thresholding class probability.
Tocontrol the precision-recall tradeoff for the lan-guage model, we calculate the difference betweenthe log probabilities of the original user input andthe suggested correction.
We then vary that differ-ence across all observed values in small incre-ments, which affects precision and recall: thehigher the difference, the fewer instances we find,but the higher the reliability of these instances is.This evaluation differs from many of the evalua-tions reported in the error detection/correction lite-rature in several respects.
First, the test set is abroad random sample across all proficiency levelsin the CLC data.
Second, it is far larger than anysets that have been so far to report results of prepo-sition/article correction on learner data.
Finally, weare only considering cases in which the systemsuggests a correction.
In other words, we do notcount as correct instances where the system's pre-diction matches a correct preposition/article.This evaluation scheme, however, ignores oneaspect of a real user scenario.
Of all the suggestedchanges that are counted as wrong in our evalua-tion because they do not match an annotated error,some may in fact be innocuous or even helpful fora real user.
Such a situation can arise for a varietyof reasons: In some cases, there are legitimate al-ternative ways to correct an error.
In other cases,the classifier has identified the location of an erroralthough that error is of a different kind (which canbe beneficial because it causes the user to make acorrection - see Leacock et al, 2009).
Gamon et al(2009), for example manually evaluate prepositionsuggestions as belonging to one of three catego-ries: (a) properly correcting an existing error, (b)offering a suggestion that neither improves nordegrades the user sentence, (c) offering a sugges-tion that would degrade the user input.
Obviously,(c) is a more serious error than (b).
Similarly, Te-trault and Chodorow (2008) annotate their test setwith preposition choices that are valid alternatives.We do not have similar information in the CLCdata, but we can perform a manual analysis of arandom subset of test data to estimate an "upperbound" for our precision/recall curve.
Our annota-tor manually categorized each suggested correctioninto one of seven categories.Details of the distribution of suggested correc-tions into the seven categories are shown in Table1.Category preps.
articlesCorrects a CLC error 32.87% 33.34%Corrects an  error thatwas not annotated as be-ing that error type in CLC 11.67% 12.16%Corrects a CLC error, butuses an alternative cor-rection 3.62% 2.26%Original and suggestedcorrection are equallygood 9.60% 11.30%Error correctly detected,but the correction iswrong 8.73% 5.03%Identifies an error site,but the actual error is nota preposition error 19.17% 12.64%Introduces an error 14.65% 23.26%Table 1: Manual analysis of suggested corrections onCLC data.This analysis involves costly manual evaluation,so we only performed it at one point of the preci-sion/recall curve (our current runtime system set-ting).
The sample size was 6,000 sentences forprepositions and 5981 sentences for articles (halfof the sentences were flagged as containing at leastone article/preposition error while the other halfwere not).
On this manual evaluation, we achieve32.87% precision if we count all flags that do notperfectly match a CLC annotation as a false posi-tive.
Only counting the last category (introductionof an error) as a false positive, precision is at85.34%.
Similarly, for articles, the manual estima-tion arrives at 76.74% precision, where pure CLCannotation matching gives us 33.34%.1674 ResultsFigure 1 and Figure 2 show the evaluation resultsof the meta-classifier for prepositions and articles,compared to the performance of the error-specificclassifier and language model alone.
For both pre-positions and articles, the first notable observationis that the language model outperforms the clas-sifier by a large margin.
This came as a surprise tous, given the recent prevalence of classificationapproaches in this area of research and the fact thatour classifiers produce state-of-the art performancewhen compared to other systems, on well-formeddata.
Second, the combination of scores from theclassifier and language model through a meta-classifier clearly outperforms either one of them inisolation.
This result, again, is consistent acrossprepositions and articles.We had previously used a hand-tuned scorecombination instead of a meta-classifier.
We alsoestablished that this heuristic performs worse thanthe language model for prepositions, and just aboutat the same level as the language model for ar-ticles.
Note, though, that the manual tuning wasperformed to optimize performance against a dif-ferent data set (the Chinese Learners of EnglishCorpus: CLEC), so the latter point is not reallycomparable and hence is not included in the charts.Figure 1: Precision and recall for prepositions.Figure 2: Precision and recall for articles.We now turn to the question of the requiredamount of annotated training data for the meta-classifier.
CLC is commercially available, but it isobvious that for many researchers such a corpuswill be too expensive and they will have to createor license their own error-annotated corpus.
Thusthe question of whether one could use less anno-tated data to train a meta-classifier and still achievereasonable results becomes important.
Figure 3 andFigure 4 show results obtained by using decreasingamounts of training data.
The dotted line shows thelanguage model baseline.
Any result below thelanguage model performance shows that the train-ing data is insufficient to warrant the use of a meta-classifier.
In these experiments there is a clear dif-ference between prepositions and articles.
We canreduce the amount of training data for prepositionsto 10% of the original data and still outperform thelanguage model baseline.
10% of the data corres-ponds to 6,800 annotated preposition errors and58,400 sentences.
When we reduce the trainingdata to 1% of the original amount (680 annotatederrors, 5,800 sentences) we clearly see degradedresults compared to the language model.
With ar-ticles, the system is much less data-hungry.
Reduc-ing the training data to 1% (580 annotated errors,7,400 sentences) still outperforms the languagemodel alone.
This result can most likely be ex-plained by the different complexity of the preposi-tion and article tasks.
Article operations includeonly six distinct operations: deletion of the, dele-tion of a/an, insertion of the, insertion of a/an,change of the to a/an, and change of a/an to the.For the twelve prepositions that we work with, the00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6PrecisionRecallPrepositionsLM only classifier onlylearned thresholds00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallArticlesLearned thresholds classifier onlyLM only168total number of insertions, deletions and substitu-tions that require sufficient training events andmight need different score combinations is 168,making the problem much harder.Figure 3: Using different amounts of annotated trainingdata for the preposition meta-classifier.Figure 4: Using different amounts of annotated trainingdata for the article meta-classifier.To find out if it is possible to reduce the re-quired amount of annotated preposition errors for asystem that still covers more than one third of thepreposition errors, we ran the same learning curveexperiments but now only taking the four mostfrequent prepositions into account: to, of, in, for.
Inthe CLC, these four prepositions account for39.8% of preposition error flags.
As in the previousexperiments, however, we found that we are notable to outperform the baseline by using just 1% ofannotated data.5 Error AnalysisWe have conducted a failure analysis on exampleswhere the system produces a blatantly bad sugges-tion in order to see whether this decision could beattributed to the error-specific classifier or to thelanguage model, or both, and what the underlyingcause is.
This preliminary analysis highlights twocommon causes for bad flags.
One is that of fre-quent lower order n-grams that dominate the lan-guage model score.
Consider the CLEC sentence Iget to know the world outside the campus by news-paper and television.
The system suggests deletingby.
The cause of this bad decision is that the bi-gram campus newspaper is extremely likely,trumping all other n-grams, and  leading to a highprobability for the suggested string compared tothe original: Log (P(original)) = -26.2 and Log(P(suggestion)) = -22.4.
This strong imbalance ofthe language model score causes the meta-classifier to assign a relatively high probability tothis being a correct revision, even though the error-specific classifier is on the right track and gives arelatively high probability for the presence of apreposition and the choice of by.
A similar exam-ple, but for substitution, occurs in They give dis-counts to their workers on books.
Here the bigramin books has a very high probability and the systemincorrectly suggests replacing on with in.
An ex-ample for insertion is seen in Please send me theletter back writing what happened.
Here, the bi-gram back to causes the bad suggestion of insertingto after back.
Since the language model is general-ly more accurate than the error-specific classifier,the meta-classifier tends to trust its score more thanthat of the classifier.
As a result we see this kind oferror quite frequently.Another common error class is the opposite situ-ation: the language model is on the right track, butthe classifier makes a wrong assessment.
ConsiderWhatever direction my leg fought to stretch, withthe suggested insertion of on before my leg.
HereLog (P(original)) = -31.5 and Log (P(suggestion))= -32.1, a slight preference for the original string.The error-specific classifier, however, assigns aprobability of 0.65 for a preposition to be present,and 0.80 for that preposition to be on.
The contex-tual features that are important in that decision are:the insertion site is between a pronoun and a noun,it is relatively close to the beginning of the sen-tence, and the head of the NP my leg has a possible00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6PrecisionRecallPrepositions100% training data LM only10% training data 1% training data00.10.20.30.40.50.60.70.80.910 0.2 0.4 0.6 0.8 1PrecisionRecallArticles100% training data 10% training dataLanguage model alone 1% training data169mass noun sense.
An example involving deletion isin Someone came to sort of it.
While the languagemodel assigns a high probability for deleting of,the error-specific classifier does not.
Similarly, forsubstitution, in Your experience is very interestingfor our company, the language model suggestssubstituting for with to while the classifier givesthe substitution a very low probability.As can be seen from the learner sentences citedabove, often, even though the sentences are gram-matical, they are not idiomatic, which can confuseall of the classifiers.6 Conclusion and Future WorkWe have addressed two questions in this paper:1.
How effective is a meta-classification ap-proach that combines language modeling anderror-specific classification to the detectionand correction of preposition and article errorsby non-native speakers?2.
How much error-annotated data is sufficient toproduce positive results using that approach?We have shown that a meta-classifier approachoutperforms using a language model or a classifieralone.
An interesting side result is that the lan-guage model solidly outperforms the contextualclassifier for both article and preposition correc-tion, contrary to current practice in the field.
Train-ing data requirements for the meta-classifier varysignificantly between article and preposition errordetection.
The article meta-classifier can be trainedwith as few as 600 annotated errors, but the prepo-sition meta-classifier requires more annotated databy an order of magnitude.
Still, the overall amountof expensive error-annotated data is relativelysmall, and the meta-classification approach makesit possible to leverage large amounts of well-formed text in the primary models, tuning to thenon-native domain in the meta-classifier.We believe that the logical next step is to com-bine more primary models in the meta-classifier.Candidates for additional primary models include(1) more classifiers trained either on different datasets or with a different classification algorithm, and(2) more language models, such as skip models orpart-of-speech n-gram language models.AcknowledgmentsWe thank Claudia Leacock from the Butler HillGroup for detailed error analysis and the anonym-ous reviewers for helpful and constructive feed-back.ReferencesEric Steven Atwell.
1987.
How to detect grammaticalerrors in a text without parsing it.
In Proceedings ofthe 3rd EACL (pp 38 ?
45).
Copenhagen.John Bitchener, Stuart Young, and Denise Cameron.2005.
The effect of different types of corrective feed-back on ESL student writing.
Journal of Second Lan-guage Writing, 14(3), 191-205.David Maxwell Chickering.
2002.
The WinMine Tool-kit.
Microsoft Technical Report 2002-103.
Redmond.Martin Chodorow, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involving pre-positions.
In Proceedings of the Fourth ACL-SIGSEM Workshop on Prepositions (pp.
25-30).
Pra-gue.Gerard M. Dalgish.
1985.
Computer-assisted ESL re-search and courseware development.
Computers andComposition, 2(4), 45-62.Rachele De Felice and Stephen G. Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the Fourth ACL-SIGSEM Workshopon Prepositions (pp.
45-50).
Prague.Rachele De Felice and Stephen Pulman.
2008.
A clas-sifier-based approach to preposition and determinererror correction in L2 English.
In Proceedings ofCOLING.
Manchester, UK.Thomas G. Dietterich.
1997.
Machine learning research:Four current directions.
AI Magazine, 18(4), 97-136.Ted Dunning.
1993.
Accurate Methods for the Statisticsof Surprise and Coincidence.
Computational Linguis-tics, 19, 61-74.Michael Gamon, Claudia Leacock, Chris Brockett, Wil-liam B. Dolan, Jianfeng Gao, Dmitriy Belenko, andAlexandre Klementiev,.
2009.
Using statistical tech-niques and web search to correct ESL errors.CALICO Journal, 26(3).Michael Gamon, Jianfeng Gao, Chris Brockett, Alexan-der Klementiev, William Dolan, Dmitriy Belenko,and Lucy Vanderwende.
2008.
Using contextualspeller techniques and language modeling for ESLerror correction.
In Proceedings of IJCNLP, Hydera-bad, India.Jianfeng Gao, Joshua Goodman, and Jiangbo Miao.2001.
The use of clustering techniques for languagemodeling?Application to Asian languages.
Compu-170tational Linguistics and Chinese LanguageProcessing, 6(1), 27-60.Andrew Golding.
1995.
A Bayesian Hybrid for ContextSensitive Spelling Correction.
In Proceedings of the3rd Workshop on Very Large Corpora (pp.
39?53).Cambridge, USA.Andrew R. Golding and Dan Roth.
1996.
ApplyingWinnow to context-sensitive spelling correction.
InProceedings of the Int.
Conference on MachineLearning  (pp 182 ?190).Na-Rae Han, Martin Chodorow, and Claudia Leacock.2004.
Detecting errors in English article usage with amaximum entropy classifier trained on a large, di-verse corpus.
In Proceedings of the 4th InternationalConference on Language Resources and Evaluation.Lisbon.Na-Rae Han, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineering,12(2), 115-129.Trude Heift and Mathias Schulze.
2007.
Errors andIntelligence in Computer-Assisted Language Learn-ing: Parsers and Pedagogues.
New York & London:Routledge.Matthieu Hermet, Alain D?silets, and Stan Szpakowicz.2008.
Using the web as a linguistic resource to auto-matically correct lexico-yyntactic errors.
In Proceed-ings of the 6th Conference on Language Resourcesand Evaluation (LREC), (pp.
874 - 878).Emi Izumi, Kiyotaka Uchimoto, Toyomi Saiga, Thep-chai Supnithi and Hitoshi Isahara.
2003.
Automaticerror detection in the Japanese learners' English spo-ken data.
In Proceedings of the 41st Annual Meetingof the Association for Computational Linguistics(pp.
145-148).Emi Izumi, Kiyotaka Uchimoto and Hitoshi Isahara.2004.
SST speech corpus of Japanese learners' Eng-lish and automatic detection of learners' errors.
InProceedings of the 4th International Conference onLanguage Resources and Evaluation (LREC), (Vol 4,pp.
31-48).Kevin Knight and Ishwar Chander,.
1994.
Automaticpostediting of documents.
In Proceedings of the  12thNational Conference on Artificial Intelligence (pp.779-784).
Seattle: Morgan Kaufmann.Claudia Leacock, Michael Gamon, and Chris Brockett.2009.
User Input and Interactions on Microsoft ESLAssistant.
In Proceedings of the Fourth Workshop onInnovative Use of NLP for Building Educational Ap-plications (pp.
73-81).John Lee.
2004.
Automatic article restoration.
In Pro-ceedings of the Human Language Technology Confe-rence of the North American Chapter of theAssociation for Computational Linguistics,  (pp.
31-36).
Boston.Guido Minnen, Francis Bond, and Anne Copestake.2000.
Memory-based learning for article generation.In Proceedings of the Fourth Conference on Compu-tational Natural Language Learning and of theSecond Learning Language in Logic Workshop (pp.43-48).
Lisbon.Patrick Nguyen, Jianfeng Gao, and Milind Mahajan.2007.
MSRLM: A scalable language modeling tool-kit.
Microsoft Technical Report 2007-144.
Redmond.Adwait Ratnaparkhi.
1997.
A simple introduction tomaximum entropy models for natural languageprocessing.
Technical Report IRCS Report 97-98, In-stitute for Research in Cognitive Science, Universityof Pennsylvania.Herman Stehouwer and Menno van Zaanen.
2009.
Lan-guage models for contextual error detection and cor-rection.
In Proceedings of the EACL 2009 Workshopon Computational Linguistic Aspects of GrammaticalInference ( pp.
41-48).
Athens.Joel Tetreault and Martin Chodorow.
2008a.
The upsand downs of preposition error detection in ESL.
InProceedings of COLING.
Manchester, UK.Joel Tetreault and Martin Chodorow.
2008b.
Nativejudgments of non-native usage: Experiments in pre-position error detection.
In Proceedings of the Work-shop on Human Judgments in ComputationalLinguistics, 22nd International Conference on Com-putational Linguistics  (pp 43-48).
Manchester, UK.Jenine Turner and Eugene Charniak.
2007.
Languagemodeling for determiner selection.
In Human Lan-guage Technologies 2007: NAACL; Companion Vo-lume, Short Papers (pp.
177-180).
Rochester, NY.Wikipedia.
English Language.http://en.wikipedia.org/wiki/English_languageXing Yi, Jianfeng Gao, and Bill Dolan.
2008.
A web-based English proofing system for English as asecond language users.
In Proceedings of the ThirdInternational Joint Conference on Natural LanguageProcessing (IJCNLP).
Hyderabad, India.171
