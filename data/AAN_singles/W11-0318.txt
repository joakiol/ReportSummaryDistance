Proceedings of the Fifteenth Conference on Computational Natural Language Learning, pages 154?162,Portland, Oregon, USA, 23?24 June 2011. c?2011 Association for Computational LinguisticsUsing the Mutual k-Nearest Neighbor Graphsfor Semi-supervised Classification of Natural Language DataKohei Ozaki and Masashi Shimbo and Mamoru Komachi and Yuji MatsumotoNara Institute of Science and Technology8916-5 Takayama, Ikoma, Nara 630-0192, Japan{kohei-o,shimbo,komachi,matsu}@is.naist.jpAbstractThe first step in graph-based semi-supervisedclassification is to construct a graph from in-put data.
While the k-nearest neighbor graphshave been the de facto standard method ofgraph construction, this paper advocates usingthe less well-known mutual k-nearest neigh-bor graphs for high-dimensional natural lan-guage data.
To compare the performanceof these two graph construction methods, werun semi-supervised classification methods onboth graphs in word sense disambiguation anddocument classification tasks.
The experi-mental results show that the mutual k-nearestneighbor graphs, if combined with maximumspanning trees, consistently outperform the k-nearest neighbor graphs.
We attribute betterperformance of the mutual k-nearest neigh-bor graph to its being more resistive to mak-ing hub vertices.
The mutual k-nearest neigh-bor graphs also perform equally well or evenbetter in comparison to the state-of-the-artb-matching graph construction, despite theirlower computational complexity.1 IntroductionSemi-supervised classification try to take advan-tage of a large amount of unlabeled data in addi-tion to a small amount of labeled data, in order toachieve good classification accuracy while reducingthe cost of manually annotating data.
In particular,graph-based techniques for semi-supervised classi-fication (Zhou et al, 2004; Zhu et al, 2003; Cal-lut et al, 2008; Wang et al, 2008) are recognizedas a promising approach.
Some of these techniqueshave been successfully applied for NLP tasks: wordsense disambiguation (Alexandrescu and Kirchhoff,2007; Niu et al, 2005), sentiment analysis (Gold-berg and Zhu, 2006), and statistical machine trans-lation (Alexandrescu and Kirchhoff, 2009), to namebut a few.However, the focus of these studies is how to as-sign accurate labels to vertices in a given graph.
Bycontrast, there has not been much work on how sucha graph should be built, and graph construction re-mains ?more of an art than a science?
(Zhu, 2005).Yet, it is an essential step for graph-based semi-supervised classification and (unsupervised) cluster-ing, and the input graph affects the quality of finalclassification/clustering results.Both for semi-supervised classification and forclustering, the k-nearest neighbor (k-NN) graphconstruction has been used almost exclusively in theliterature.
However, k-NN graphs often producehubs, or vertices with extremely high degree (i.e.,the number of edges incident to a vertex).
This ten-dency is obvious especially if the original data ishigh-dimensional?a characteristic typical of natu-ral language data.
In a later section, we demonstratethat such hub vertices indeed deteriorate the accu-racy of semi-supervised classification.While not in the context of graph construction,Radovanovic?
et al (2010) made an insightful obser-vation into the nature of hubs in high-dimensionalspace; in their context, a hub is a sample close tomany other samples in the (high-dimensional) sam-ple space.
They state that such hubs inherentlyemerge in high-dimensional data as a side effect ofthe ?curse of dimensionality,?
and argue that this is a154reason nearest neighbor classification does not workwell in high-dimensional space.Their observation is insightful for graph con-struction as well.
Most of the graph-based semi-supervised classification methods work by gradu-ally propagating label information of a vertex to-wards neighboring vertices in a graph, but the neigh-borhood structure in the graph is basically deter-mined by the proximity of data in the original high-dimensional sample space.
Hence, it is very likelythat a hub in the sample space also makes a hubin the k-NN graph, since k-NN graph constructiongreedily connects a pair of vertices if the sample cor-responding to one vertex is among the k closest sam-ples of the other sample in the original space.
It istherefore desirable to have an efficient graph con-struction method for high-dimensional data that canproduce a graph with reduced hub effects.To this end, we propose to use the mutual k-nearest neighbor graphs (mutual k-NN graphs),a less well-known variant of the standard k-NNgraphs.
All vertices in a mutual k-NN graph havea degree upper-bounded by k, which is not usuallythe case with standard k-NN graphs.
This propertyhelps not to produce vertices with extremely highdegree (hub vertices) in the graph.
A mutual k-NNgraph is easy to build, at a time complexity identicalto that of the k-NN graph construction.We first evaluated the quality of the graphs apartfrom specific classification algorithms using the ?-edge ratio of graphs.
Our experimental results showthat the mutual k-NN graphs have a smaller num-ber of edges connecting vertices with different la-bels than the k-NN graphs, thus reducing the possi-bility of wrong label information to be propagated.We also compare the classification accuracy of twostandard semi-supervised classification algorithmson the mutual k-NN graphs and the k-NN graphs.The results show that the mutual k-NN graphs con-sistently outperorm the k-NN graphs.
Moreover, themutual k-NN graphs achieve equally well or bet-ter classification accuracy than the state-of-the-artgraph construction method called b-matching (Je-bara et al, 2009), while taking much less time toconstruct.2 Problem Statement2.1 Semi-supervised ClassificationThe problem of semi-supervised classification canbe stated as follows.
We are given a set of n ex-amples, X = {x1, .
.
.
,xn}, but only the labelsof the first l examples are at hand; the remainingu = n ?
l examples are unlabeled examples.
LetS = {1, .
.
.
, c} be the set of possible labels, andyi ?
S the label of xi, for i = 1, .
.
.
, n. Sincewe only know the labels of the first l examples, wedo not have access to yl+1, .
.
.
, yn.
For later conve-nience, further let y = (y1, .
.
.
, yn).The goal of a semi-supervised classification al-gorithm is to predict the hidden labels yl+1, .
.
.
, ynof u unlabeled examples xl+1, .
.
.
,xn, giventhese unlabeled examples and l labeled data(x1, y1), .
.
.
, (xl, yl).
A measure of similarity be-tween examples is also provided to the algorithm.Stated differently, the classifier has access to an all-pair similarity matrix W ?
of size n ?
n, with its(i, j)-element W ?ij holding the similarity of exam-ples xi and xj .
It is assumed that W ?
is a symmetricmatrix, and the more similar two examples are (withrespect to the similarity measure), more likely theyare to have the same label.
This last assumption isthe premise of many semi-supervised classificationalgorithms and is often called the cluster assumption(Zhou et al, 2004).2.2 Graph-based Semi-supervisedClassificationGraph-based approaches to semi-supervised classi-fication are applicable if examples X are graph ver-tices.
Otherwise, X must first be converted into agraph.
This latter case is the focus of this paper.That is, we are interested in how to construct a graphfrom the examples, so that the subsequent classifica-tion works well.Let G denote the graph constructed from the ex-amples.
Naturally, G has n vertices, since verticesare identified with examples.
Instead of graph G it-self, let us consider its real-valued (weighted) adja-cency matrix W , of size n ?
n. The task of graphconstruction then reduces to computing W from all-pairs similarity matrix W ?.The simplest way to compute W from W ?
is tolet W = W ?, which boils down to using a dense,155complete graph G with the unmodified all-pairs sim-ilarity as its edge weights.
However, it has been ob-served that a sparseW not only save time needed forclassification, but also results in better classificationaccuracy1 than the full similarity matrix W ?
(Zhu,2008).
Thus, we are concerned with how to sparsifyW ?
to obtain a sparseW ; i.e., the strategy of zeroingout some elements of W ?.Let the set of binary values be B = {0, 1}.
A spar-sification strategy can be represented by a binary-valued matrix P ?
Bn?n, where Pij = 1 if W ?ijmust be retained as Wij , and Pij = 0 if Wij = 0.Then, the weighted adjacency matrix W of G isgiven by Wij = PijW ?ij .
The n ?
n matrices Wand P are symmetric, reflecting the fact that mostgraph-based algorithms require the input graph to beundirected.3 k-Nearest Neighbor Graphs and theEffect of HubsThe standard approach to making a sparse graph G(or equivalently, matrix W ) is to construct a k-NNgraph from the data (Szummer and Jaakkola, 2002;Niu et al, 2005; Goldberg and Zhu, 2006).3.1 The k-Nearest Neighbor GraphsThe k-NN graph is a weighted undirected graph con-necting each vertex to its k-nearest neighbors in theoriginal sample space.
Building a k-NN graph is atwo step process.
First we solve the following opti-mization problem.maxP?
?Bn?n?i,jP?ijW?ij (1)s.t.
?jP?ij = k, P?ii = 0, ?i, j ?
{1, .
.
.
, n}Note that we are trying to find P?
, and not P .
Thisis an easy problem and we can solve it by greedilyassigning P?ij = 1 only if W ?ij is among the top kelements in the ith row of W ?
(in terms of the mag-nitude of the elements).
After P?
is determined, welet Pij = max(P?ij , P?ji).
Thus P is a symmetricmatrix, i.e., Pij = Pji for all i and j, while P?
may1See also the experimental results of Section 6.3.2 in whichthe full similarity matrix W ?
is used as the baseline.d 1 2 ?
3 total# of vertices 1610 1947 164 3721original 65.9 65.7 69.8 66.0hub-removed 66.6 66.0 69.8 66.4Table 1: Classification accuracy of vertices around hubsin a k-NN graph, before (?original?)
and after (?hub-removed?)
hubs are removed.
The value d represents theshortest distance (number of hops) from a vertex to itsnearest hub vertex in the graph.not.
Finally, weighted adjacency matrix W is deter-mined byWij = PijW ?ij .
MatrixW is also symmet-ric since P and W ?
are symmetric.This process is equivalent to retaining all edgesfrom each vertex to its k-nearest neighbor vertices,and then making all edges undirected.Note the above symmetrization step is necessarybecause the k-nearest neighbor relation is not sym-metric; even if a vertex vi is a k-nearest neighbor ofanother vertex vj , vj may or may not be a k-nearestneighbor of vi.
Thus, symmetrizing P and W asabove makes the graph irregular; i.e., the degree ofsome vertices may be larger than k, which opens thepossibility of hubs to emerge.3.2 Effect of Hubs on ClassificationIn this section, we demonstrate that hubs in k-NNgraphs are indeed harmful to semi-supervised clas-sification as we claimed earlier.
To this end, weeliminate such high degree vertices from the graph,and compare the classification accuracy of other ver-tices before and after the elimination.
For this pre-liminary experiment, we used the ?line?
dataset ofa word sense disambiguation task (Leacock et al,1993).
For details of the dataset and the task, seeSection 6.In this experiment, we randomly selected 10 per-cent of examples as labeled examples.
The remain-ing 90 percent makes the set of unlabeled examples,and the goal is to predict the label (word sense) ofthese unlabeled examples.We first built a k-NN graph (with k = 3)from the dataset, and ran Gaussian Random Fields(GRF) (Zhu et al, 2003), one of the most widely-used graph-based semi-supervised classification al-gorithms.
Then we removed vertices with degree156greater than or equal to 30 from the k-NN graph,and ran GRF again on this ?hub-removed?
graph.Table 1 shows the classification accuracy of GRFon the two graphs.
The table shows both the over-all classification accuracy, and the classification ac-curacy on the subsets of vertices, stratified by theirdistance d from the nearest hub vertices (which wereeliminated in the ?hub-removed?
graph).
Obvi-ously, overall classification accuracy has improvedafter hub removal.
Also notice that the increase inthe classification accuracy on the vertices nearest tohubs (d = 1, 2).
These results suggest that the pres-ence of hubs in the graph is deteriorating classifica-tion accuracy.4 Mutual k-Nearest Neighbor Graphs forSemi-supervised ClassificationAs demonstrated in Section 3.2, removing hub ver-tices in k-NN graphs is an easy way of improv-ing the accuracy of semi-supervised classification.However, this method adds another parameter to thegraph construction method, namely, the threshold onthe degree of vertices to be removed.
The methodalso does not tell us how to assign labels to the re-moved (hub) vertices.
Hence, it is more desirableto have a graph construction method which has onlyone parameter just like the k-NN graphs, but is at thesame time less prone to produce hub vertices.In this section, we propose to use mutual k-NNgraphs for this purpose.4.1 Mutual k-Nearest Neighbor GraphsThe mutual k-NN graph is not a new concept andit has been used sometimes in clustering.
Even inclustering, however, they are not at all as popular asthe ordinary k-NN graphs.
A mutual k-NN graphis defined as a graph in which there is an edge be-tween vertices vi and vj if each of them belongs tothe k-nearest neighbors (in terms of the original sim-ilarity metric W ) of the other vertex.
By contrast, ak-NN graph has an edge between vertices vi and vjif one of them belongs to the k-nearest neighbors ofthe other.
Hence, the mutual k-NN graph is a sub-graph of the k-NN graph computed from the samedata with the same value of k. The mutual k-NNgraph first optimizes the same formula as (1), but inmutual k-NN graphs, the binary-valued symmetricmatrix P is defined as Pij = min(P?ij , P?ji).
Sincemutual k-NN graph construction guarantees that allvertices in the resulting graph have degree at mostk, it is less likely to produce extremely high degreevertices in comparison with k-NN graphs, providedthat the value of k is kept adequately small.4.2 Fixing Weak ConnectivityBecause the mutual k-NN graph construction ismore selective of edges than the standard k-NNgraphs, the resulting graphs often contain manysmall disconnected components.
Disconnectedcomponents are not much of a problem for clus-tering (since its objective is to divide a graph intodiscrete components eventually), but can be a prob-lem for semi-supervised classification algorithms; ifa connected component does not contain a labelednode, the algorithms cannot reliably predict the la-bels of the vertices in the component; recall thatthese algorithms infer labels by propagating label in-formation along edges in the graph.As a simple method for overcoming this problem,we combine the mutual k-NN graph and the max-imum spanning tree.
To be precise, the minimumnumber of edges from the maximum spanning treeare added to the mutual k-NN graph to ensure thatonly one connected component exists in a graph.4.3 Computational EfficiencyUsing a Fibonacci heap-based implementation(Fredman and Tarjan, 1987), one can constructthe standard k-NN graph in (amortized) O(n2 +kn log n) time.
A mutual k-NN graph can also beconstructed in the same time complexity as the k-NN graphs.
The procedure below transforms a stan-dard k-NN graph into a mutual k-NN graph.
It usesFibonacci heaps once again and assumes that the in-put k-NN graph is represented as an adjacency ma-trix in sparse matrix representation.1.
Each vertex is associated with its own heap.For each edge e connecting vertices u and v,insert e to the heaps associated with u and v.2.
Fetch maximum weighted edges from eachheap k times, keeping globally the record ofthe number of times each edge is fetched.
No-tice that an edge can be fetched at most twice,157once at an end vertex of the edge and once atthe other end.3.
A mutual k-NN graph can be constructed byonly keeping edges fetched twice in the previ-ous step.The complexity of this procedure is O(kn).
Hencethe overall complexity of building a mutual k-NNgraph is dominated by the time needed to buildthe standard k-NN graph input to the system; i.e.,O(n2 + kn log n).If we call the above procedure on an approximatek-NN graph which can be computed more efficiently(Beygelzimer et al, 2006; Chen et al, 2009; Ramet al, 2010; Tabei et al, 2010), it yields an ap-proximate mutual k-NN graphs.
In this case, theoverall complexity is identical to that of the ap-proximate k-NN graph construction algorithm, sincethese approximate algorithms have a complexity atleast O(kn).5 Related Work5.1 b-Matching GraphsRecently, Jebara et al (2009) proposed a newgraph construction method called b-matching.
A b-matching graph is a b-regular graph, meaning thatevery vertex has the degree b uniformly.
It can be ob-tained by solving the following optimization prob-lem.maxP?Bn?n?ijPijW?ijs.t.
?jPij = b, ?i ?
{1, .
.
.
, n} (2)Pii = 0, ?i ?
{1, .
.
.
, n} (3)Pij = Pji, ?i, j ?
{1, .
.
.
, n} (4)After P is computed, the weighted adjacency matrixW is determined by Wij = PijW ?ij The constraint(4) makes the binary matrix P symmetric, and (3) isto ignore self-similarity (loops).
Also, the constraint(2) ensures that the graph is regular.
Note that k-NNgraphs are in general not regular.
The regularity re-quirement of the b-matching graphs can be regardedas an effort to avoid the hubness phenomenon dis-cussed by Radovanovic?
et al (2010).Figure 1: Two extreme cases of ?-edge ratio.
Vertexshapes (and colors) denote the class labels.
The ?-edgeratio of the graph on the left is 1, meaning that all edgesconnect vertices with different labels.
The ?-edge ratioof the one on the right is 0, because all edges connectvertices of the same class.Jebara et al (2009) reported that b-matchinggraphs achieve semi-supervised classification accu-racy higher than k-NN graphs.
However, with-out approximation, building a b-matching graphis prohibitive in terms of computational complex-ity.
Huang and Jebara (2007) developed a fast im-plementation based on belief propagation, but theguaranteed running time of the implementation isO(bn3), which is still not practical for large scalegraphs.
Notice that the k-NN graphs and mutual k-NN graphs can be constructed with much smallertime complexity, as we mentioned in Section 4.3.In Section exp, we empirically compare the per-formance of mutual k-NN graphs with that of b-matching graphs.5.2 Mutual Nearest Neighbor in ClusteringIn the clustering context, mutual k-NN graphs havebeen theoretically analyzed by Maier et al (2009)with Random Geometric Graph Theory.
Their studysuggests that if one is interested in identifying themost significant clusters only, the mutual k-NNgraphs give a better clustering result.
However, it isnot clear what their results imply in semi-supervisedclassification settings.6 ExperimentsWe compare the k-NN, mutual k-NN, and b-matching graphs in word sense disambiguation anddocument classification tasks.
All of these tasks aremulti-class classification problems.6.1 DatasetsWe used two word sense disambiguation datasets inour experiment: ?interest?
and ?line.?
The ?inter-est?
data is originally taken from the POS-tagged158interest datasetnumber of edges (x 103)phi?edge ratio0.100.150.200.25lllllllllllllllllllllllllllll0 5 10 15 20 25 30 35l bMGkNNGMkNNGline datasetnumber of edges (x 103)phi?edge ratio0.150.200.250.300.35llllllllllllllllllllll0 10 20 30 40l bMGkNNGMkNNGReuters datasetnumber of edges (x 103)phi?edge ratio0.100.120.140.160.180.200.220.24llllllllllllllllllllllllllll0 10 20 30 40 50l bMGkNNGMkNNG20 newsgroups datasetnumber of edges (x 103)phi?edge ratio0.150.200.250.300.350.400 50 100 150 200 250kNNGMkNNGFigure 2: ?-edge ratios of the k-NN graph, mutual k-NN graph, and b-matching graphs.
The ?-edge ratio of a graphis a measure of how much the cluster assumption is violated; hence, smaller the ?-edge ratio, the better.
The plot forb-matching graph is missing for the 20 newsgroups dataset, because its construction did not finish after one week forthis dataset.dataset examples features labelsinterest 2,368 3,689 6line 4,146 8,009 6Reuters 4,028 17,143 420 newsgroups 19,928 62,061 20Table 2: Datasets used in experiments.portion of the Wall Street Journal Corpus.
Each in-stance of the polysemous word ?interest?
has beentagged with one of the six senses in Longman Dic-tionary of Contemporary English.
The details of thedataset are described in Bruce and Wiebe (1994).The ?line?
data is originally used in numerous com-parative studies of word sense disambiguation.
Eachinstance of the word ?line?
has been tagged with oneof the six senses on the WordNet thesaurus.
Furtherdetails can be found in the Leacock et al (1993).Following Niu et al (2005), we used the followingcontext features in the word sense disambiguationtasks: part-of-speech of neighboring words, singlewords in the surrounding context, and local colloca-tion.
Details of these context features can be foundin Lee and Ng (2002).The Reuters dataset is extracted from RCV1-v2/LYRL2004, a text categorization test collection(Lewis et al, 2004).
In the same manner as Cram-mer et al (2009), we produced the classificationdataset by selecting approximately 4,000 documentsfrom 4 general topics (corporate, economic, gov-ernment and markets) at random.
The features de-scribed in Lewis et al (2004) are used with thisdataset.The 20 newsgroups dataset is a popular datasetfrequently used for document classification andclustering.
The dataset consists of approximately20,000 messages on newsgroups and is originallydistributed by Lang (1995).
Each message is as-signed one of the 20 possible labels indicating whichnewsgroup it has been posted to, and represented asbinary bag-of-words features as described in Rennie(2001).Table 2 summarizes the characteristics of thedatasets used in our experiments.6.2 Experimental SetupOur focus in this paper is a semi-supervised classi-fication setting in which the dataset contains a smallamount of labeled examples and a large amount ofunlabeled examples.
To simulate such settings, wecreate 10 sets of labeled examples, with each setconsisting of randomly selected l examples from theoriginal dataset, where l is 10 percent of the totalnumber of examples.
For each set, the remaining90 percent constitute the unlabeled examples whoselabels must be inferred.After we build a graph from the data using oneof the graph construction methods discussed earlier,a graph-based semi-supervised classification algo-rithm must be run on the resulting graph to infer la-bels to the unlabeled examples (vertices).
We usetwo most frequently used classification algorithms:Gaussian Random Fields (GRF) (Zhu et al, 2003)and the Local/Global Consistency algorithm (LGC)(Zhou et al, 2004).
Averaged classification accuracyis used as the evaluation metric.
For all datasets, co-159interest dataset (GRF)number of edges (x 103)averagedaccuracy0.790.800.810.820.83llllllllllllllllllllllllll0 5 10 15 20 25 30 35l bMGkNNGMkNNGinterest dataset (LGC)number of edges (x 103)averagedaccuracy0.790.800.810.820.83lllllllllllllllllllllllllll0 5 10 15 20 25 30 35l bMGkNNGMkNNGline dataset (GRF)number of edges (x 103)averagedaccuracy0.620.640.660.680.70llllllllllllllllllll0 10 20 30 40l bMGkNNGMkNNGline dataset (LGC)number of edges (x 103)averagedaccuracy0.620.640.660.680.70llllllllllllllllllll0 10 20 30 40l bMGkNNGMkNNGFigure 3: Averaged classification accuracies for k-NN graphs, b-matching graphs and mutual k-NN graphs (+ maxi-mum spanning trees) in the interest and line datasets.sine similarity is used as the similarity measure be-tween examples.In ?interest?
and ?line?
datasets, we compare theperformance of the graph construction methods overthe broad range of their parameters; i.e., b in b-matching graphs and k in (mutual) k-NN graphs.In Reuters and the 20 newsgroups datasets, 2-foldcross validation is used to determine the hyperpa-rameters (k and b) of the graph construction meth-ods; i.e., we split the labeled data into two folds, andused one fold for training and the other for develop-ment, and then switch the folds in order to find theoptimal hyperparameter among k, b ?
{2, .
.
.
, 50}.The smoothing parameter ?
of LGC is fixed at ?
=0.9.6.3 Results6.3.1 Comparison of ?-Edge RatioWe first compared the ?-edge ratios of k-NNgraphs, mutual k-NN graphs, and b-matching graphsto evaluate the quality of the graphs apart from spe-cific classification algorithms.For this purpose, we define the ?-edge ratio as theyardstick to measure the quality of a graph.
Here, a?-edge of a labeled graph (G,y) is any edge (vi, vj)for which yi 6= yj (Cesa-Bianchi et al, 2010), andwe define the ?-edge ratio of a graph as the numberof ?-edges divided by the total number of edges inthe graph.
Since most graph-based semi-supervisedclassification methods propagate label informationalong edges, edges connecting vertices with differ-ent labels may lead to misclassification.
Hence, agraph with a smaller ?-edge ratio is more desirable.Figure 1 illustrates two toy graphs with extreme val-ues of ?-edge ratio.Figure 2 shows the plots of ?-edge ratios of thecompared graph construction methods when the val-ues of parameters k (for k-NN and mutual k-NNgraphs) and b (for b-matching graphs) are varied.
Inthese plots, the y-axes denote the ?-edge ratio of theconstructed graphs.
The x-axes denote the numberof edges in the constructed graphs, and not the val-ues of parameters k or b, because setting parametersb and k to an equal value does not achieve the samelevel of sparsity (number of edges) in the resultinggraphs.As mentioned earlier, the smaller the ?-edge ra-tio, the more desirable.
As the figure shows, mu-tual k-NN graphs achieve smaller ?-edge ratio thanother graphs if they are compared at the same levelof graph sparsity.The plot for b-matching graph is missing for the20 newsgroups data, because we were unable tocomplete its construction in one week2.
Meanwhile,a k-NN graph and a mutual k-NN graph for the samedataset can be constructed in less than 15 minutes onthe same computer.6.3.2 Classification ResultsFigure 3 shows the classification accuracy of GRFand LGC on the different types of graphs con-structed for the interest and line datasets.
As in Fig-ure 2, the x-axes represent the sparsity of the con-structed graphs measured by the number of edges inthe graph, which can change as the hyperparameter(b or k) of the compared graph construction methods2All experiments were run on a machine with 2.3 GHz AMDOpteron 8356 processors and 256 GB RAM.160kNN graph b-matching graph mutual kNN graphdataset alorithm Dense MST original +MST original +MST original +MSTReuters GRF 43.65 72.74 81.70 80.89 84.04 84.04 85.01 84.72Reuters LGC 43.66 71.78 82.60 82.60 84.42 84.42 84.81 84.8520 newsgroups GRF 10.18 66.96 75.47 75.47 ??
??
76.31 76.4620 newsgroups LGC 14.51 65.82 75.19 75.19 ??
??
75.27 75.41Table 3: Document classification accuracies for k-NN graphs, b-matching graphs, and mutual k-NN graphs.
The col-umn for ?Dense?
is the result for the graph with the original similarity matrix W ?
as the adjacency matrix; i.e., withoutusing any graph construction (sparsification) methods.
The column for ?MST?
is the result the for the maximum span-ning tree.
b-matching graph construction did not complete after one week on the 20 newsgroups data, and hence noresults are shown.vs.
kNNG vs. bMGdataset (algo) orig +MST orig +MSTReuters (GRF)   > ?Reuters (LGC)   ?
?20 newsgroups (GRF)   ??
?
?20 newsgroups (LGC) ?
> ??
?
?Table 4: One-sided paired t-test results of averaged ac-curacies between using mutual k-NN graphs and othergraphs.
?
?, ?>?, and ???
correspond to p-value <0.01, (0.01, 0.05], and > 0.05 respectively.are varied.As shown in the figure, the combination of mu-tual k-NN graphs and the maximum spanning treesachieves better accuracy than other graph construc-tion methods in most cases, when they are com-pared at the same levels of graph sparsity (numberof edges).Table 3 summarizes the classification accuracy onthe document classification datasets.
As a baseline,the table also shows the results (?Dense?)
on thedense complete graph with the original all-pairs sim-ilarity matrix W ?
as the adjacency matrix (i.e., nograph sparsification), as well as the results for us-ing the maximum spanning tree alone as the graphconstruction method.In all cases, mutual k-NN graphs achieve betterclassification accuracy than other graphs.Table 4 reports the one-sided paired t-test resultsof averaged accuracies with k-NN graphs and b-matching graphs against our proposed approach, thecombination of mutual k-NN graphs and maximumspanning trees.
From Table 4, we see that mutualk-NN graphs perform significantly better than k-NN graphs.
On the other hand, theere is no signifi-cant difference in the accuracy of the mutual k-NNgraphs and b-matching graphs.
However, mutualk-NN graphs achieves the same level of accuracywith b-matching graphs, at much less computationtime and are applicable to large datasets.
As men-tioned earlier, mutual k-NN graphs can be computedwith less than 15 minutes in the 20 newsgroups data,while b-matching graphs cannot be computed in oneweek.7 ConclusionIn this paper, we have proposed to use mutual k-NN graphs instead of the standard k-NN graphs forgraph-based semi-supervised learning.
In mutual k-NN graphs, all vertices have degree upper boundedby k. We have demonstrated that this type ofgraph construction alleviates the hub effects statedin Radovanovic?
et al (2010), which also makes thegraph more consistent with the cluster assumption.In addition, we have shown that the weak connectiv-ity of mutual k-NN graphs is not a serious problemif we augment the graph with maximum spanningtrees.
Experimental results on various natural lan-guage processing datasets show that mutual k-NNgraphs lead to higher classification accuracy than thestandard k-NN graphs, when two popular label in-ference methods are run on these graphs.ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2007.
Data-driven graph construction for semi-supervised graph-based learning in NLP.
In Proc.
of HLT-NAACL.161Andrei Alexandrescu and Katrin Kirchhoff.
2009.Graph-based learning for statistical machine transla-tion.
In Proc.
of NAACL-HLT.Alina Beygelzimer, Sham Kakade, and John Langford.2006.
Cover trees for nearest neighbor.
In Proc.
ofICML.Rebecca Bruce and Janyce Wiebe.
1994.
Word-sensedisambiguation using decomposable models.
In Proc.of ACL.Je?ro?me Callut, Kevin Franc?oisse, Marco Saerens, andPierre Dupont.
2008.
Semi-supervised classificationfrom discriminative random walks.
In Proc.
of ECML-PKDD.Nicolo Cesa-Bianchi, Claudio Gentile, Fabio Vitale, andGiovanni Zappella.
2010.
Random spanning trees andthe prediction of weighted graphs.
In Proc.
of ICML.Jie Chen, Haw-ren Fang, and Yousef Saad.
2009.
Fastapproximate kNN graph construction for high dimen-sional data via recursive lanczos bisection.
Journal ofMachine Learning Research, 10.Koby Crammer, Mark Dredze, and Alex Kulesza.
2009.Multi-class confidence weighted algorithms.
In Proc.of EMNLP.Michael L. Fredman and Robert Endre Tarjan.
1987.
Fi-bonacci heaps and their uses in improved network op-timization algorithms.
J. ACM, 34:596?615, July.Andrew B. Goldberg and Xiaojin Zhu.
2006.
Seeingstars when there aren?t many stars: graph-based semi-supervised learning for sentiment categorization.
InProc.
of TextGraphs Workshop on HLT-NAACL.Bert Huang and Tony Jebara.
2007.
Loopy belief prop-agation for bipartite maximum weight b-matching.
InProc.
of AISTATS.Tony Jebara, Jun Wang, and Shih-Fu Chang.
2009.Graph construction and b-matching for semi-supervised learning.
In Proc.
of ICML.Ken Lang.
1995.
Newsweeder: Learning to filter net-news.
In Proc.
of ICML.Claudia Leacock, Geoffrey Towell, and Ellen Voorhees.1993.
Corpus-based statistical sense resolution.
InProc.
of ARPA Workshop on HLT.Yoong Keok Lee and Hwee Tou Ng.
2002.
An empir-ical evaluation of knowledge sources and learning al-gorithms for word sense disambiguation.
In Proc.
ofEMNLP.David D. Lewis, Yiming Yang, Tony G. Rose, Fan Li,G.
Dietterich, and Fan Li.
2004.
RCV1: A new bench-mark collection for text categorization research.
Jour-nal of Machine Learning Research, 5.Markus Maier, Matthias Hein, and Ulrike von Luxburg.2009.
Optimal construction of k-nearest-neighborgraphs for identifying noisy clusters.
Journal of Theo-retical Computer Science, 410.Zheng-Yu Niu, Dong-Hong Ji, and Chew Lim Tan.
2005.Word sense disambiguation using label propagationbased semi-supervised learning.
In Proc.
of ACL.Milos?
Radovanovic?, Alexandros Nanopoulos, and Mir-jana Ivanovic?.
2010.
Hub in space: popular nearestneighbors in high-dimensional data.
Journal of Ma-chine Learning Research, 11.Parikshit Ram, Dongryeol Lee, William March, andAlexander Gray.
2010.
Linear-time algorithms forpairwise statistical problems.
In Proc.
of NIPS.Jason D. M. Rennie.
2001.
Improving multi-class textclassification with naive bayes.
Master?s thesis, Mas-sachusetts Institute of Technology.
AITR-2001-004.Martin Szummer and Tommi Jaakkola.
2002.
Partiallylabeled classification with markov random walks.
InProc.
of NIPS.Yasuo Tabei, Takeaki Uno, Masashi Sugiyama, and KojiTsuda.
2010.
Single versus multiple sorting in allpairs similarity search.
In Proc.
of ACML.Jun Wang, Tony Jebara, and Shih-Fu.
Chang.
2008.Graph transduction via alternating minimization.
InProc.
of ICML.Dengyong Zhou, Olivier Bousquet, Thomas Navin Lal,Jason Weston, and Bernhard Scho?lkopf.
2004.
Learn-ing with local and global consistency.
In Proc.
ofNIPS.Xiaojin Zhu, Zoubin Ghahramani, and John D. Lafferty.2003.
Semi-supervised learning using gaussian fieldsand harmonic functions.
In Proc.
of ICML.Xiaojin Zhu.
2005.
Semi-Supervised Learning withGraphs.
Ph.D. thesis, Carnegie Mellon University.CMU-LTI-05-192.Xiaojin Zhu.
2008.
Semi-supervised learning literaturesurvey.
Technical Report 1530, Computer Sciences,University of Wisconsin-Madison.162
