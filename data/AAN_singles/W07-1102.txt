Proceedings of the Workshop on A Broader Perspective on Multiword Expressions, pages 9?16,Prague, June 2007. c?2007 Association for Computational LinguisticsDistinguishing Subtypes of Multiword Expressions UsingLinguistically-Motivated Statistical MeasuresAfsaneh FazlyDepartment of Computer ScienceUniversity of TorontoToronto, Canadaafsaneh@cs.toronto.eduSuzanne StevensonDepartment of Computer ScienceUniversity of TorontoToronto, Canadasuzanne@cs.toronto.eduAbstractWe identify several classes of multiword ex-pressions that each require a different encod-ing in a (computational) lexicon, as well asa different treatment within a computationalsystem.
We examine linguistic propertiespertaining to the degree of semantic idiosyn-crasy of these classes of expressions.
Ac-cordingly, we propose statistical measures toquantify each property, and use the measuresto automatically distinguish the classes.1 MotivationMultiword expressions (MWEs) are widely used inwritten language as well as in colloquial speech.
AnMWE is composed of two or more words that to-gether form a single unit of meaning, e.g., frying pan,take a stroll, and kick the bucket.
Most MWEs behavelike any phrase composed of multiple words, e.g.,their components may be separated, as in She took arelaxing stroll along the beach.
Nonetheless, MWEsare distinct from multiword phrases because they in-volve some degree of semantic idiosyncrasy, i.e., theoverall meaning of an MWE diverges from the com-bined contribution of its constituent parts.
Because oftheir frequency and their peculiar behaviour, MWEspose a great challenge to the creation of natural lan-guage processing (NLP) systems (Sag et al, 2002).NLP applications, such as semantic parsing and ma-chine translation should not only identify MWEs, butalso should know how to treat them when they areencountered.Semantic idiosyncrasy is a matter of degree (Nun-berg et al, 1994).
The idiom shoot the breeze islargely idiosyncratic, because its meaning (?to chat?
)does not have much to do with the meaning of shootor breeze.
MWEs such as give a try (?try?)
and makea decision (?decide?)
are semantically less idiosyn-cratic (more predictable).
These are MWEs becausethe overall meaning of the expression diverges fromthe combined meanings of the constituents.
Nonethe-less, there is some degree of predictability in theirmeanings that makes them distinct from idioms.
Inthese, the complement of the verb (here, a noun) de-termines the primary meaning of the overall expres-sion.
This class of expressions is referred to as lightverb constructions (LVCs) in the linguistics literature(Miyamoto, 2000; Butt, 2003).Clearly, a computational system should distinguishidioms and LVCs, both from each other, and fromsimilar-on-the-surface (literal) phrases such as shootthe bird and give a present.
Idioms are largely id-iosyncratic; a computational lexicographer thus maydecide to list idioms such as shoot the breeze in a lex-icon along with their idiomatic meanings.
In contrast,the meaning of MWEs such as make a decision canbe largely predicted, given that they are LVCs.
Ta-ble 1 shows the different underlying semantic struc-ture of a sentence containing an idiom (shoot thebreeze) and a sentence containing an LVC (give atry).
As can be seen, such MWEs should also betreated differently when translated into another lan-guage.
Note that in contrast to a literal combination,such as shoot the bird, for idioms and LVCs, the num-ber of arguments expressed syntactically may differfrom the number of the semantic participants.Many NLP applications also need to distinguishanother group of MWEs that are less idiosyncratic9Class English sentence Semantic representation French translationLiteral Jill and Tim shot the bird.
(event/SHOOT Jill et Tim ont abattu l?oiseau.
:agent (?Jill ?
Tim?)
Jill and Tim shot down the bird.
:theme (?bird?
))Abstract Jill makes a living singing in pubs.
(event/EARN-MONEY Jill gagne sa vie en chantant dans des bars.
:agent (?Jill?))
Jill makes a living by singing in the pubs.LVC Jill gave the lasagna a try.
(event/TRY Jill a essaye?
le lasagne.
:agent (?Jill?)
Jill tried the lasagna.
:theme (?lasagna?
))Idiom Jill and Tim shot the breeze.
(event/CHAT Jill et Tim ont bavarde?.
:agent (?Jill ?
Tim?))
Jill and Tim chatted.Table 1: Sample English MWEs and their translation in French.than idioms and LVCs, but more so than literal com-binations.
Examples include give confidence andmake a living.
These are idiosyncratic because themeaning of the verb is a metaphorical (abstract)extension of its basic physical semantics.
More-over, they often take on certain connotations be-yond the compositional combination of their con-stituent meanings.
They thus exhibit behaviour of-ten attributed to collocations, e.g., they appear withgreater frequency than semantically similar combina-tions.
For example, searching on Google, we foundmuch higher frequency for give confidence comparedto grant confidence.
As can be seen in Table 1, an ab-stract combination such as make a living, althoughlargely compositional, may not translate word-for-word.
Rather, it should be translated taking into ac-count that the verb has a metaphorical meaning, dif-ferent from its basic semantics.Here, we focus on a particular class of EnglishMWEs that are formed from the combination of averb with a noun in its direct object position, re-ferred to as verb+noun combinations.
Specifically,we provide a framework for identifying members ofthe following semantic classes of verb+noun combi-nations: (i) literal phrases (LIT), (ii) abstract combi-nations (ABS), (iii) light verb constructions (LVC),and (iv) idiomatic combinations (IDM).
Section 2elaborates on the linguistic properties related to thedifferences in the degree of semantic idiosyncrasyobserved in members of the above four classes.
InSection 3, we propose statistical measures for quan-tifying each of these properties, and use them as fea-tures for type classification of verb+noun combina-tions.
Section 4 and Section 5 present an evaluationof our proposed measures.
Section 6 discusses therelated studies, and Section 7 concludes the paper.2 Semantic Idiosyncrasy: LinguisticPropertiesLinguists and lexicographers often attribute certaincharacteristics to semantically idiosyncratic expres-sions.
Some of the widely-known properties are in-stitutionalization, lexicosyntactic fixedness, and non-compositionality (Cowie, 1981; Gibbs and Nayak,1989; Moon, 1998).
The following paragraphs elab-orate on each property, as well as on its relevance tothe identification of the classes under study.Institutionalization is the process through which acombination of words becomes recognized and ac-cepted as a semantic unit involving some degree ofsemantic idiosyncrasy.
IDMs, LVCs, and ABS com-binations are institutionalized to some extent.Lexicosyntactic fixedness refers to some degree oflexical and syntactic restrictiveness in a semanticallyidiosyncratic expression.
An expression is lexicallyfixed if the substitution of a semantically similarword for any of its constituents does not preserve itsoriginal meaning (e.g., compare spill the beans andspread the beans).
In contrast to LIT and ABS com-binations, IDMs and LVCs are expected to exhibitlexical fixedness to some extent.An expression is syntactically fixed if it cannot un-dergo syntactic variations and at the same time retainits original semantic interpretation.
IDMs and LVCsare known to show strong preferences for the syn-tactic patterns they appear in (Cacciari and Tabossi,1993; Brinton and Akimoto, 1999).
E.g., compare10Joe gave a groan with ?A groan was given by Joe,and Tim kicked the bucket with *Tim kicked the buck-ets (in the idiom reading).
Nonetheless, the type anddegree of syntactic fixedness in LVCs and IDMs aredifferent.
For example, most LVCs prefer the patternin which the noun is introduced by the indefinite arti-cle a (as in give a try and make a decision), whereasthis is not the case with IDMs (e.g., shoot the breezeand kick the bucket).
IDMs and LVCs may also ex-hibit preferences with respect to adjectival modifica-tion of their noun constituent.
LVCs are expected toappear both with and without an adjectival modifier,as in give a (loud) groan and make a (wise) decision.IDMs, on the other hand, mostly appear either withan adjective, as in keep an open mind (cf.
?keep amind), or without, as in shoot the breeze (cf.
?shootthe fun breeze).Non-compositionality refers to the situation wherethe meaning of a word combination deviates fromthe meaning emerging from a word-by-word inter-pretation of it.
IDMs are largely non-compositional,whereas LVCs are semi-compositional since theirmeaning can be mainly predicted from the noun con-stituent.
ABS and LIT combinations are expected tobe largely compositional.None of the above-mentioned properties are suffi-cient criteria by themselves for determining whichsemantic class a given verb+noun combination be-longs to.
Moreover, semantic properties of the con-stituents of a combination are also known to be rele-vant for determining its class (Uchiyama et al, 2005).Verbs may exhibit strong preferences for appearingin MWEs from a particular class, e.g., give, take andmake commonly form LVCs.
The semantic categoryof the noun is also relevant to the type of MWE, e.g.,the noun constituent of an LVC is often a predicativeone.
We hypothesize that if we look at evidence fromall these different sources, we will find members ofthe same class to be reasonably similar, and membersof different classes to be notably different.3 Statistical Measures of SemanticIdiosyncrasyThis section introduces measures for quantifying theproperties of idiosyncratic MWEs, mentioned in theprevious section.
The measures will be used as fea-tures in a classification task (see Sections 4?5).3.1 Measuring InstitutionalizationCorpus-based approaches often assess the degree ofinstitutionalization of an expression by the frequencywith which it occurs.
Raw frequencies drawn froma corpus are not reliable on their own, hence asso-ciation measures such as pointwise mutual informa-tion (PMI) are also used in many NLP applications(Church et al, 1991).
PMI of a verb+noun combina-tion ?v , n?
is defined as:PMI (v , n) .= log P (v , n)P (v)P (n)?
log f (?, ?
)f (v , n)f (v , ?)
f (?, n) (1)where all frequency counts are calculated oververb?object pairs in a corpus.
We use both frequencyand PMI of a verb+noun combination to measure itsdegree of institutionalization.
We refer to this groupof measures as INST.3.2 Measuring FixednessTo measure fixedness, we use statistical measures oflexical, syntactic, and overall fixedness that we havedeveloped in a previous study (Fazly and Stevenson,2006), as well as some new measures we introducehere.
The following paragraphs give a brief descrip-tion of each.Fixednesslex quantifies the degree of lexical fixed-ness of the target combination, ?v ,n?, by compar-ing its strength of association (measured by PMI)with those of its lexical variants.
Like Lin (1999),we generate lexical variants of the target automati-cally by replacing either the verb or the noun con-stituent by a semantically similar word from theautomatically-built thesaurus of Lin (1998).
We thenuse a standard statistic, the z -score, to calculateFixednesslex:Fixednesslex(v , n) .=PMI(v , n) ?
PMIstd (2)where PMI is the mean and std the standard devia-tion over the PMI of the target and all its variants.Fixednesssyn quantifies the degree of syntacticfixedness of the target combination, by comparingits behaviour in text with the behaviour of a typicalverb?object, both defined as probability distributionsover a predefined set of patterns.
We use a stan-dard information-theoretic measure, relative entropy,11v det:NULL nsg v det:NULL nplv det:a/an nsgv det:the nsg v det:the nplv det:DEM nsg v det:DEM nplv det:POSS nsg v det:POSS nplv det:OTHER nsg,pl det:ANY nsg,pl be vpassiveTable 2: Patterns for syntactic fixedness measure.to calculate the divergence between the two distribu-tions as follows:Fixednesssyn (v , n).= D(P(pt |v ,n) ||P(pt))=?ptk?PP(ptk | v , n) logP(ptk | v , n)P(ptk )(3)where P is the set of patterns (shown in Table 2)known to be relevant to syntactic fixedness in LVCsand IDMs.
P(pt | v , n) represents the syntactic be-haviour of the target, and P(pt) represents the typicalsyntactic behaviour over all verb?object pairs.Fixednesssyn does not show which syntactic pat-tern the target prefers the most.
We thus use an addi-tional measure, Patterndom, to determine the domi-nant pattern for the target:Patterndom(v , n) .= argmaxptk?Pf (v , n, ptk ) (4)In addition to the individual measures of fixedness,we use Fixednessoverall, which quantifies the degreeof overall fixedness of the target:Fixednessoverall (v , n).= ?
Fixednesssyn (v , n)+ (1 ?
?)
Fixednesslex (v , n) (5)where ?
weights the relative contribution of lexi-cal and syntactic fixedness in predicting semantic id-iosyncrasy.Fixednessadj quantifies the degree of fixednessof the target combination with respect to adjectivalmodification of the noun constituent.
It is similar tothe syntactic fixedness measure, except here there areonly two patterns that mark the presence or absenceof an adjectival modifier preceding the noun:Fixednessadj(v , n) .= D(P(ai |v ,n) ||P(ai )) (6)where ai ?
{present, absent}.
Fixednessadj doesnot determine which pattern of modification the tar-get combination prefers most.
We thus add anothermeasure?the odds of modification?to capture this:Oddsadj(v , n) .=P(ai = present|v ,n)P(ai = absent|v ,n)(7)Overall, we use six measures related to fixedness;we refer to the group as FIXD.3.3 Measuring CompositionalityCompositionality of an expression is often approxi-mated by comparing the ?context?
of the expressionwith the contexts of its constituents.
We measurethe degree of compositionality of a target verb+nouncombination, t =?v ,n?, in a similar fashion.We take the context of the target (t) and each of itsconstituents (v and n) to be a vector of the frequencyof nouns cooccurring with it within a window of ?5words.
We then measure the ?similarity?
between thetarget and each of its constituents, Simdist (t , v) andSimdist (t , n), using the cosine measure.1Recall that an LVC can be roughly paraphrased bya verb that is morphologically related to its noun con-stituent, e.g., to make a decision nearly means to de-cide.
For each target t , we thus add a third measure,Simdist (t , rv), where rv is a verb morphologicallyrelated to the noun constituent of t , and is automati-cally extracted from WordNet (Fellbaum, 1998).2We use abbreviation COMP to refer to the group ofmeasures related to compositionality.3.4 The ConstituentsRecall that semantic properties of the constituents ofa verb+noun combination are expected to be relevantto its semantic class.
We thus add two simple fea-ture groups: (i) the verb itself (VERB); and (ii) thesemantic category of the noun according to WordNet(NSEM).
We take the semantic category of a noun tobe the ancestor of its first sense in the hypernym hier-archy of WordNet 2.1, cut at the level of the children1Our preliminary experiments on development data from Fa-zly and Stevenson (2006) revealed that the cosine measure and awindow size of ?5 words resulted in the best performance.2If no such verb exists, Simdist (t , rv) is set to zero.
If morethan one verb exist, we choose the one that is identical to thenoun or the one that is shorter in length.12of ENTITY (which will include PHYSICAL ENTITYand ABSTRACT ENTITY).34 Experimental Setup4.1 Corpus and Experimental ExpressionsWe use the British National Corpus (BNC),4 auto-matically parsed using the Collins parser (Collins,1999), and further processed with TGrep2.5 Weselect our potential experimental expressions frompairs of verb and direct object that have a minimumfrequency of 25 in the BNC and that involve oneof a predefined list of basic (transitive) verbs.
Ba-sic verbs, which in their literal uses refer to states oracts central to human experience (e.g., give and put),commonly form MWEs in combination with their di-rect object argument (Cowie et al, 1983).
We use 12such verbs ranked highly according to the number ofdifferent nouns they appear with in the BNC.
Hereare the verbs in alphabetical order:bring, find, get, give, hold, keep, lose, make, put, see, set, takeTo guarantee that the final set of expressions con-tains pairs from all four classes, we pseudo-randomlyselect them from the initial list of pairs extracted fromthe BNC as explained above.
To ensure the inclusionof IDMs, we consult two idioms dictionaries (Cowieet al, 1983; Seaton and Macaulay, 2002).
To en-sure we include LVCs, we select pairs in which thenoun has a morphologically related verb accordingto WordNet.
We also select pairs whose noun is notmorphologically related to any verb to ensure the in-clusion of LIT combinations.This selection process resulted in 632 pairs, re-duced to 563 after annotation (see Section 4.2 fordetails on annotation).
Out of these, 148 are LIT,196 are ABS, 102 are LVC, and 117 are IDM.
Werandomly choose 102 pairs from each class as ourfinal experimental expressions.
We then pseudo-randomly divide these into training (TRAIN), devel-opment (DEV), and test (TEST) data sets, so that eachset has an equal number of pairs from each class.
Inaddition, we ensure that pairs with the same verb thatbelong to the same class are divided equally amongthe three sets.
Our final TRAIN, DEV, and TEST sets3Experiments on development data show that looking at allsenses of a noun degrades performance.4http://www.natcorp.ox.ac.uk.5http://tedlab.mit.edu/?dr/Tgrep2.contain 240, 84, and 84 pairs, respectively.4.2 Human JudgmentsWe asked four native speakers of English with suf-ficient linguistic background to annotate our exper-imental expressions.
The annotation task was ex-pected to be time-consuming, hence it was not feasi-ble for all the judges to annotate all the expressions.Instead, we asked one judge to be our primary anno-tator, PA henceforth.
(PA is an author of this paper,but the other three judges are not.
)First, PA annotated all the 632 expressions selectedas described in Section 4.1, and removed 69 of themthat could be potential sources of disagreement forvarious reasons (e.g., if an expression was unfamil-iar or was likely to be part of a larger phrase).
Next,we divided the remaining 563 pairs into three equal-sized sets, and gave each set to one of the otherjudges to annotate.
The judges were given a com-prehensive guide for the task, in which the classeswere defined solely in terms of their semantic prop-erties.
Since expressions were annotated out of con-text (type-based), we asked the judges to annotate thepredominant meaning of each expression.We use the annotations of PA as our gold standardfor evaluation, but use the annotations of the othersto measure inter-annotator agreement.
The observedagreement (po) between PA and each of the otherthree annotators are 79.8%, 72.2%, and 67%, respec-tively.
The kappa (?)
scores are .72, .62, and .56.The reasonably high agreement scores confirm thatthe classes are coherent and linguistically plausible.4.3 Classification Strategy and FeaturesWe use the decision tree induction system C5.0 asour machine learning software, and the measures pro-posed in Section 3 as features in our classification ex-periments.6 We explore the relevance of each featuregroup in the overall classification, as well as in iden-tifying members of each individual class.5 Experimental ResultsWe performed experiments on DEV to find featuresmost relevant for classification.
These experiments6Experiments on DEV using a Support Vector Machine algo-rithm produced poorer results; we thus do not report them.13revealed that removing Simdist (t , v) resulted in bet-ter performance.
This is not surprising given that ba-sic verbs are highly polysemous, and hence the distri-butional context of a basic verb may not correspondto any particular sense of it.
We thus remove thisfeature (from COMP) in experiments on TEST.
Re-sults presented here are on the TEST set; those on theDEV set have similar trends.
Here, we first look at theoverall performance of classification in Section 5.1.Section 5.2 presents the results of classification forthe individual classes.5.1 Overall Classification PerformanceTable 3 presents the results of classification?interms of average accuracy (%Acc) and relative er-ror reduction (%RER)?for the individual featuregroups, as well as for all groups combined.
The base-line (chance) accuracy is 25% since we have fourequal-sized classes in TEST.
As can be seen, INSTfeatures yield the lowest overall accuracy, around36%, with a relative error reduction of only 14%over the baseline.
This shows that institutionaliza-tion, although relevant, is not sufficient for distin-guishing among different levels of semantic idiosyn-crasy.
Interestingly, FIXD features achieve the high-est accuracy, 50%, with a relative error reduction of33%, showing that fixedness is a salient aspect of se-mantic idiosyncrasy.
COMP features achieve reason-ably good accuracy, around 40%, though still notablylower than the accuracy of FIXD features.
This is es-pecially interesting since much previous research hasfocused solely on the non-compositionality of MWEsto identify them (McCarthy et al, 2003; Baldwin etal., 2003; Bannard et al, 2003).
Our results confirmthe relevance of this property, while at the same timerevealing its insufficiency.
Interestingly, features re-lated to the semantic properties of the constituents,VERB and NSEM, overall perform comparably to thecompositionality features.
However, a closer look attheir performance on the individual classes (see Sec-tion 5.2) reveals that, unlike COMP, they are mainlygood at identifying items from certain classes.
Ashypothesized, we achieve the highest performance,an accuracy of 58% and a relative error reduction of44%, when we combine all features.Table 4 displays classification performance, whenwe use all the feature groups except one.
These re-sults are more or less consistent with those in Ta-Only the features in group %Acc (%RER)INST 35.7 (14.3)FIXD 50 (33.3)COMP 40.5 (20.7)VERB 42.9 (23.9)NSEM 39.3 (19.1)ALL 58.3 (44.4)Table 3: Accuracy (%Acc) and relative error reduction(%RER) over TEST pairs, for the individual feature groups, andfor all features combined.All features except those in group %Acc (%RER)INST 53.6 (38.1)FIXD 47.6 (30.1)COMP 56 (41.3)VERB 48.8 (31.7)NSEM 46.4 (28.5)ALL 58.3 (44.4)Table 4: Accuracy (%Acc) and relative error reduction(%RER) over TEST pairs, removing one feature group at a time.ble 3 above, except some differences which we dis-cuss below.
Removing FIXD features results in adrastic decrease in performance (10.7%), while theremoval of INST and COMP features cause muchsmaller drops in performance (4.7% and 2.3%, re-spectively).
Here again, we can see that features re-lated to the semantics of the verb and the noun aresalient features.
Removing either of these resultsin a substantial decrease in performance?9.5% and11.9%, respectively?which is comparable to the de-crease resulting from removing FIXD features.
Thisis an interesting observation, since VERB and NSEMfeatures, on their own, do not perform nearly as wellas FIXD features.
It is thus necessary to futher in-vestigate the performance of these groups on largerdata sets with more variability in the verb and nounconstituents of the expressions.5.2 Performance on Individual ClassesWe now look at the performance of the featuregroups, both separately and combined, on the indi-vidual classes.
For each combination of class andfeature group, the F -measures of classification aregiven in Table 5, with the two highest F -measuresfor each class shown in boldface.7 These resultsshow that the combination of all feature groups yieldsthe best or the second-best performance on all fourclasses.
(In fact, in only one case is the performance7Our F -measure gives equal weights to precision and recall.14Only the features in groupClass INST FIXD COMP VERB NSEM ALLLIT .48 .42 .51 .54 .57 .60ABS .40 .32 .17 .27 .49 .46LVC .21 .58 .47 .55 - .68IDM .33 .67 .42 0 - .56Table 5: F -measures on TEST pairs, for individual featuregroups and all features combined.ANNOTATOR1 ANNOTATOR2 ANNOTATOR3Class %po ?
%po ?
%po ?LIT 93.6 .83 88.3 .67 91.4 .78ABS 83 .63 76.6 .46 78 .52LVC 91 .71 83 .54 87.7 .61IDM 92 .73 87.2 .63 87.2 .59Table 6: Per-class observed agreement and kappa score be-tween PA and each of the three annotators.of ALL features notably smaller than the best perfor-mance achieved by a single feature group.
)Looking at the performance of ALL features, wecan see that we get reasonably high F -measure forall classes, except for ABS.
The relatively low valuesof po and ?
on this class, as shown in Table 6, suggestthat this class was also the hardest to annotate.
It ispossible that members of this class share propertieswith other classes.
The extremely poor performanceof the COMP features on ABS also reflects that per-haps members of this class are not coherent in termsof their degree of compositionality (e.g, compare giveconfidence and make a living).
In the future, we needto incorporate more coherent membership criteria forthis class into our annotation procedure.According to Table 5, the most relevant featuregroup for identifying members of the LIT and ABSclasses is NSEM.
This is expected since NSEM is a bi-nary feature determining whether the noun is a PHYS-ICAL ENTITY or an ABSTRACT ENTITY.8 Amongother feature groups, INST features also perform rea-sonably well on both these classes.
The most relevantfeature group for LVC and IDM is FIXD.
(Note thatfor IDM, the performance of this group is notablyhigher than ALL).
On the other hand, INST featureshave a very poor performance on these classes, rein-forcing that IDMs and LVCs may not necessarily ap-pear with significantly high frequency of occurrencein a given corpus.
Fixedness features thus prove to be8Since this is a binary feature, it can only distinguish twoclasses.
In the future, we need to include more semantic classes.particularly important for the identification of highlyidiosyncratic MWEs, such as LVCs and IDMs.6 Related WorkMuch recent work on classifying MWEs focuses ondetermining different levels of compositionality inverb+particle combinations using a measure of distri-butional similarity (McCarthy et al, 2003; Baldwinet al, 2003; Bannard et al, 2003).
Another group ofresearch attempts to classify a particular MWE sub-type, such as verb-particle constructions (VPCs) orLVCs, according to some fine-grained semantic crite-ria (Wanner, 2004; Uchiyama et al, 2005; Cook andStevenson, 2006).
Here, we distinguish subtypes ofMWEs that are defined according to coarse-graineddistinctions in their degree of semantic idiosyncrasy.Wermter and Hahn (2004) recognize the impor-tance of distinguishing MWE subtypes that are sim-ilar to our four classes, but only focus on separat-ing MWEs as one single class from literal combina-tions.
For this, they use a measure that draws on thelimited modifiability of MWEs, in addition to theirexpected high frequency.
Krenn and Evert (2001)attempt to separate German idioms, LVCs, and lit-eral phrases (of the form verb+prepositional phrase).They treat LVCs and idioms as institutionalized ex-pressions, and use frequency and several associationmeasures, such as PMI, for the task.
The main goalof their work is to find which association measuresare particularly suited for identifying which of theseclasses.
Here, we look at properties of MWEs otherthan their institutionalization (the latter we quantifyusing an association measure).The work most similar to ours is that of Venkata-pathy and Joshi (2005).
They propose a minimally-supervised classification schema that incorporates avariety of features to group verb+noun combinationsaccording to their level of compositionality.
Theirwork has the advantage of requiring only a smallamount of manually-labeled training data.
However,their classes are defined on the basis of composition-ality only.
Here, we consider classes that are linguis-tically salient, and moreover need special treatmentwithin a computational system.
Our work is also dif-ferent in that it brings in a new group of features, thefixedness measures, which prove to be very effectivein identifying particular classes of MWEs.157 ConclusionsWe have provided an analysis of the important char-acteristics pertaining to the semantic idiosyncrasy ofMWEs.
We have also elaborated on the relation-ship between these properties and four linguistically-motivated classes of verb+noun combinations, fallingon a continuum from less to more semantically id-iosyncratic.
On the basis of such analysis, wehave developed statistical, corpus-based measuresthat quantify each of these properties.
Our resultsconfirm that these measures are effective in type clas-sification of the MWEs under study.
Our class-based results look into the interaction between themeasures (each capturing a property of MWEs) andthe classes (which are defined in terms of seman-tic idiosyncrasy).
Based on this, we can see whichmeasures?or properties they relate to?are most orleast relevant for identifying each particular class ofverb+noun combinations.
We are currently expand-ing this work to investigate the use of similar mea-sures in token classification of verb+noun combina-tions in context.AcknowledgementsWe thank Eric Joanis for providing us with NP-head extractionsoftware.
We thank Saif Mohammad for the CooccurrenceMa-trix and the DistributionalDistance packages.ReferencesTimothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical model of mul-tiword expression decomposability.
In Proc.
of ACL-SIGLEX Wkshp.
on Multiword Expressions, 89?96.Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In Proc.
of ACL-SIGLEX Wkshp.
on Multi-word Expressions, 65?72.Laurel J. Brinton and Minoji Akimoto, eds.
1999.
Col-locational and Idiomatic Aspects of Composite Predi-cates in the History of English.
John Benjamins.Miriam Butt.
2003.
The light verb jungle.
Workshop onMulti-Verb Constructions.Cristina Cacciari and Patrizia Tabossi, eds.
1993.
Idioms:Processing, Structure, and Interpretation.
LawrenceErlbaum Associates, Publishers.Kenneth Church, William Gale, Patrick Hanks, and Don-ald Hindle.
1991.
Using statistics in lexical analysis.In Uri Zernik, editor, Lexical Acquisition: ExploitingOn-Line Resources to Build a Lexicon, 115?164.Michael Collins.
1999.
Head-Driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, UPenn.Paul Cook and Suzanne Stevenson.
2006.
Classifyingparticle semantics in English verb-particle construc-tions.
In Proc.
of COLING-ACL?06 Wkshp.
on Multi-word Expressions, 45?53.Anthony P. Cowie, Ronald Mackin, and Isabel R. McCaig.1983.
Oxford Dictionary of Current Idiomatic English,volume 2.
OUP.Anthony P. Cowie.
1981.
The treatment of collocationsand idioms in learner?s dictionaries.
Applied Linguis-tics, II(3):223?235.Afsaneh Fazly and Suzanne Stevenson.
2006.
Automat-ically constructing a lexicon of verb phrase idiomaticcombinations.
In Proc.
of EACL?06, 337?344.Christiane Fellbaum, editor.
1998.
WordNet, An Elec-tronic Lexical Database.
MIT Press.Raymond W., Jr. Gibbs and Nandini P. Nayak.
1989.
Psy-chololinguistic studies on the syntactic behaviour of id-ioms.
Cognitive Psychology, 21:100?138.Brigitte Krenn and Stefan Evert.
2001.
Can we do bet-ter than frequency?
A case study on extracting PP-verbcollocations.
In Proc.
of ACL?01 Wkshp.
on Colloca-tions, 39?46.Dekang Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proc.
of COLING-ACL?98, 768?774.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proc.
of ACL?99, 317?324.Diana McCarthy, Bill Keller, and John Carroll.
2003.Detecting a continuum of compositionality in phrasalverbs.
In Proc.
of ACL-SIGLEX Wkshp.
on MultiwordExpressions, 73?80.Tadao Miyamoto.
2000.
The Light Verb Constructionin Japanese: the Role of the Verbal Noun.
John Ben-jamins.Rosamund Moon.
1998.
Fixed Expressions and Idioms inEnglish: A Corpus-Based Approach.
Oxford Univer-sity Press.Geoffrey Nunberg, Ivan A.
Sag, and Thomas Wasow.1994.
Idioms.
Language, 70(3):491?538.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expres-sions: A pain in the neck for NLP.
In Proc.
of CI-CLing?02, 1?15.Maggie Seaton and Alison Macaulay, eds.
2002.
CollinsCOBUILD Idioms Dictionary.
HarperCollins.Kiyoko Uchiyama, Timothy Baldwin, and Shun Ishizaki.2005.
Disambiguating Japanese compound verbs.Computer Speech and Language, 19:497?512.Sriram Venkatapathy and Aravind Joshi.
2005.
Measur-ing the relative compositionality of verb-noun (V-N)collocations by integrating features.
In Proc.
of HLT-EMNLP?05, 899?906.Leo Wanner.
2004.
Towards automatic fine-grained se-mantic classification of verb-noun collocations.
Natu-ral Language Engineering, 10(2):95?143.Joachim Wermter and Udo Hahn.
2004.
Collocation ex-traction based on modifiability statistics.
In Proc.
ofCOLING?04, 980?986.16
