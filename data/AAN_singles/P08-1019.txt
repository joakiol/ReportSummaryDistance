Proceedings of ACL-08: HLT, pages 156?164,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSearching Questions by Identifying Question Topic and Question FocusHuizhong Duan1, Yunbo Cao1,2, Chin-Yew Lin2 and Yong Yu11Shanghai Jiao Tong University,Shanghai, China, 200240{summer, yyu}@apex.sjtu.edu.cn2Microsoft Research Asia,Beijing, China, 100080{yunbo.cao, cyl}@microsoft.comAbstractThis paper is concerned with the problem ofquestion search.
In question search, given aquestion as query, we are to return questionssemantically equivalent or close to the queriedquestion.
In this paper, we propose to conductquestion search by identifying question topicand question focus.
More specifically, we firstsummarize questions in a data structure con-sisting of question topic and question focus.Then we model question topic and questionfocus in a language modeling framework forsearch.
We also propose to use the MDL-based tree cut model for identifying questiontopic and question focus automatically.
Expe-rimental results indicate that our approach ofidentifying question topic and question focusfor search significantly outperforms the base-line methods such as Vector Space Model(VSM) and Language Model for InformationRetrieval (LMIR).1 IntroductionOver the past few years, online services have beenbuilding up very large archives of questions andtheir answers, for example, traditional FAQ servic-es and emerging community-based Q&A services(e.g., Yahoo!
Answers1 , Live QnA2, and BaiduZhidao3).To make use of the large archives of questionsand their answers, it is critical to have functionalityfacilitating users to search previous answers.
Typi-cally, such functionality is achieved by first re-trieving questions expected to have the sameanswers as a queried question and then returningthe related answers to users.
For example, givenquestion Q1 in Table 1, question Q2 can be re-1 http://answers.yahoo.com2 http://qna.live.com3 http://zhidao.baidu.comturned and its answer will then be used to answerQ1 because the answer of Q2 is expected to par-tially satisfy the queried question Q1.
This is whatwe called question search.
In question search, re-turned questions are semantically equivalent orclose to the queried question.Query:Q1: Any cool clubs in Berlin or Hamburg?Expected:Q2: What are the best/most fun clubs in Berlin?Not Expected:Q3: Any nice hotels in Berlin or Hamburg?Q4: How long does it take to Hamburg from Berlin?Q5: Cheap hotels in Berlin?Table 1.
An Example on Question SearchMany methods have been investigated for tack-ling the problem of question search.
For example,Jeon et al have compared the uses of four differentretrieval methods, i.e.
vector space model, Okapi,language model, and translation-based model,within the setting of question search (Jeon et al,2005b).
However, all the existing methods treatquestions just as plain texts (without consideringquestion structure).
For example, obviously, Q2can be considered semantically closer to Q1 thanQ3-Q5 although all questions (Q2-Q5) are relatedto Q1.
The existing methods are not able to tell thedifference between question Q2 and questions Q3,Q4, and Q5 in terms of their relevance to questionQ1.
We will clarify this in the following.In this paper, we propose to conduct questionsearch by identifying question topic and questionfocus.The question topic usually represents the majorcontext/constraint of a question (e.g., Berlin, Ham-burg) which characterizes users?
interests.
In con-trast, question focus (e.g., cool club, cheap hotel)presents certain aspect (or descriptive features) ofthe question topic.
For the aim of retrieving seman-tically equivalent (or close) questions, we need to156assure that returned questions are related to thequeried question with respect to both question top-ic and question focus.
For example, in Table 1, Q2preserves certain useful information of Q1 in theaspects of both question topic (Berlin) and ques-tion focus (fun club) although it loses some usefulinformation in question topic (Hamburg).
In con-trast, questions Q3-Q5 are not related to Q1 inquestion focus (although being related in questiontopic, e.g.
Hamburg, Berlin), which makes themunsuitable as the results of question search.We also propose to use the MDL-based (Mini-mum Description Length) tree cut model for auto-matically identifying question topic and questionfocus.
Given a question as query, a structure calledquestion tree is constructed over the question col-lection including the queried question and all therelated questions, and then the MDL principle isapplied to find a cut of the question tree specifyingthe question topic and the question focus of eachquestion.In a summary, we summarize questions in a datastructure consisting of question topic and questionfocus.
On the basis of this, we then propose tomodel question topic and question focus in a lan-guage modeling framework for search.
To the bestof our knowledge, none of the existing studies ad-dressed question search by modeling both questiontopic and question focus.We empirically conduct the question search withquestions about ?travel?
and ?computers & internet?.Both kinds of questions are from Yahoo!
Answers.Experimental results show that our approach cansignificantly improve traditional methods (e.g.VSM, LMIR) in retrieving relevant questions.The rest of the paper is organized as follow.
InSection 2, we present our approach to questionsearch which is based on identifying question topicand question focus.
In Section 3, we empiricallyverify the effectiveness of our approach to questionsearch.
In Section 4, we employ a translation-basedretrieval framework for extending our approach tofix the issue called ?lexical chasm?.
Section 5 sur-veys the related work.
Section 6 concludes the pa-per by summarizing our work and discussing thefuture directions.2 Our Approach to Question SearchOur approach to question search consists of twosteps: (a) summarize questions in a data structureconsisting of question topic and question focus; (b)model question topic and question focus in a lan-guage modeling framework for search.In the step (a), we employ the MDL-based (Min-imum Description Length) tree cut model for au-tomatically identifying question topic and questionfocus.
Thus, this section will begin with a briefreview of the MDL-based tree cut model and thenfollow that by an explanation of steps (a) and (b).2.1 The MDL-based tree cut modelFormally, a tree cut model ?
(Li and Abe, 1998)can be represented by a pair consisting of a tree cut?, and a probability parameter vector ?
of the samelength, that is,?
?
?
?, ??
(1)where ?
and ?
are?
?
??
?, ?
?, .
.
???,?
?
?????
?, ????
?, ?
, ??????
(2)where ?
?, ?
?, ???
?are classes determined by a cutin the tree and ?
?????
?
1????
.
A ?cut?
in a tree isany set of nodes in the tree that defines a partitionof all the nodes, viewing each node as representingthe set of child nodes as well as itself.
For example,the cut indicated by the dash line in Figure 1 cor-responds to three classes:??
?, ????,???
?, ???
?, and???
?, ??
?, ??
?, ???
?.Figure 1.
An Example on the Tree Cut ModelA straightforward way for determining a cut of atree is to collapse the nodes of less frequency intotheir parent nodes.
However, the method is tooheuristic for it relies much on manually tuned fre-quency threshold.
In our practice, we turn to use atheoretically well-motivated method based on theMDL principle.
MDL is a principle of data com-pression and statistical estimation from informa-tion theory (Rissanen, 1978).Given a sample ?
and a tree cut ?, we employMLE to estimate the parameters of the correspond-ing tree cut model ??
?
?
?, ???
, where ??
denotesthe estimated parameters.According to the MDL principle, the descriptionlength (Li and Abe, 1998)  ???
?, ??
of the tree cutmodel ??
and the sample ??
is the sum of the model?????
???
??????
???
???
??
?157description length ???
?, the parameter descriptionlength ????|??
, and the data description length??
?|?, ??
?, i.e.???
?, ??
?
????
?
???????
?
??
?|?, ???
(3)The model description length ????
is a subjec-tive quantity which depends on the coding schemeemployed.
Here, we simply assume that each treecut model is equally likely a priori.The parameter description length ????|??
is cal-culated as???????
?
???
log |?|  (4)where |?|  denotes the sample size and ?
denotesthe number of free parameters in the tree cut model,i.e.
?
equals the number of nodes in ?
minus one.The data description length ??
?|?, ???
is calcu-lated as????
?, ???
?
??
???????????
(5)where?????
?
?|?|?
???
?|?|(6)where ?
is the class that ?
belongs to and ???
?denotes the total frequency of instances in class ?in the sample ?.With the description length defined as (3), wewish to select a tree cut model with the minimumdescription length and output it as the result.
Notethat the model description length ????
can be ig-nored because it is the same for all tree cut models.The MDL-based tree cut model was originallyintroduced for handling the problem of generaliz-ing case frames using a thesaurus (Li and Abe,1998).
To the best of our knowledge, no existingwork utilizes it for question search.
This may bepartially because of the unavailability of the re-sources (e.g., thesaurus) which can be used forembodying the questions in a tree structure.
In Sec-tion 2.2, we will introduce a tree structure calledquestion tree for representing questions.2.2 Identifying question topic and questionfocusIn principle, it is possible to identify question topicand question focus of a question by only parsingthe question itself (for example, utilizing a syntac-tic parser).
However, such a method requires accu-rate parsing results which cannot be obtained fromthe noisy data from online services.Instead, we propose using the MDL-based treecut model which identifies question topics andquestion foci for a set of questions together.
Morespecifically, the method consists of two phases:1) Constructing a question tree: represent thequeried question and all the related questionsin a tree structure called question tree;2) Determining a tree cut: apply the MDL prin-ciple to the question tree, which yields the cutspecifying question topic and question focus.2.2.1 Constructing a question treeIn the following, with a series of definitions, wewill describe how a question tree is constructedfrom a collection of questions.Let?s begin with explaining the representation ofa question.
A straightforward method is torepresent a question as a bag-of-words (possiblyignoring stop words).
However, this method cannotdiscern ?the hotels in Paris?
from ?the Paris hotel?.Thus, we turn to use the linguistic units carrying onmore semantic information.
Specifically, we makeuse of two kinds of units: BaseNP (Base NounPhrase) and WH-ngram.
A BaseNP is defined as asimple and non-recursive noun phrase (Cao and Li,2002).
A WH-ngram is an ngram beginning withWH-words.
The WH-words that we consider in-clude ?when?, ?what?, ?where?, ?which?, and ?how?.We refer to these two kinds of units as ?topicterms?.
With ?topic terms?, we represent a questionas a topic chain and a set of questions as a questiontree.Definition 1 (Topic Profile) The topic profile??
of a topic term ?
in a categorized question col-lection is a probability distribution of categories????|??????
where ?
is a set of categories.???|??
?
???????,???
???????,?????
(7)where ??????
?, ??
is the frequency of the topicterm ?
within category ?
.
Clearly, wehave??
???|?????
?
1.By ?categorized questions?, we refer to the ques-tions that are organized in a tree of taxonomy.
Forexample, at Yahoo!
Answers, the question ?Howdo I install my wireless router?
is categorized as?Computers & Internet ?
Computer Networking?.Actually, we can find categorized questions at oth-er online services such as FAQ sites, too.Definition 2 (Specificity) The specificity ????
?ofa topic term ??
is the inverse of the entropy of thetopic profile???.
More specifically,????
?
1 ???
???|??
log ???|?????
?
???
(8)158where ?
is a smoothing parameter used to copewith the topic terms whose entropy is 0.
In our ex-periments, the value of ?
was set 0.001.We use the term specificity to denote how spe-cific a topic term is in characterizing informationneeds of users who post questions.
A topic term ofhigh specificity (e.g., Hamburg, Berlin) usuallyspecifies the question topic corresponding to themain context of a question because it tends to oc-cur only in a few categories.
A topic term of lowspecificity is usually used to represent the questionfocus (e.g., cool club, where to see) which is rela-tively volatile and might occur in many categories.Definition 3 (Topic Chain) A topic chain ??
ofa question ?
is a sequence of ordered topic terms??
?
??
?
?
?
??
such that1) ??
is included in 1  ,?
?
?
?
?
;2) ?????
?
????
?,  1 ?
?
?
?
?
?.For example, the topic chain of ?any cool clubsin Berlin or Hamburg??
is ?Hamburg ?
Berlin ?cool?club?
because the specificities for ?Hamburg?,?Berlin?, and ?cool club?
are 0.99, 0.62, and 0.36.Definition 4 (Question Tree) A question tree ofa question set ?
?
????????
is a prefix tree builtover the topic chains ??
?
?????????
of the questionset ?.
Clearly, if a question set contains only onequestion, its question tree will be exactly same asthe topic chain of the question.Note that the root node of a question tree is as-sociated with empty string as the definition of pre-fix tree requires (Fredkin, 1960).Figure 2.
An Example of a Question TreeGiven the topic chains with respect to the ques-tions in Table 1 as follow,?
Q1: Hamburg ?
Berlin ?
cool?club??
Q2: Berlin ?
fun?club??
Q3: Hamburg ?
Berlin ?
nice?hotel??
Q4: Hamburg ?
Berlin ?
how?long?does?it?take??
Q5: Berlin ?
cheap?hotel?we can have the question tree presented in Figure 2.2.2.2 Determining the tree cutAccording to the definition of a topic chain, thetopic terms in a topic chain of a question are or-dered by their specificity values.
Thus, a cut of atopic chain naturally separates the topic terms oflow specificity (representing question focus) fromthe topic terms of high specificity (representingquestion topic).
Given a topic chain of a questionconsisting of ?
topic terms, there exist (?
?
1?possible cuts.
The question is: which cut is the best?We propose using the MDL-based tree cut mod-el for the search of the best cut in a topic chain.Instead of dealing with each topic chain individual-ly, the proposed method handles a set of questionstogether.
Specifically, given a queried question, weconstruct a question tree consisting of both thequeried question and the related questions, andthen apply the MDL principle to select the best cutof the question tree.
For example, in Figure 2, wehope to get the cut indicated by the dashed line.The topic terms on the left of the dashed linerepresent the question topic and those on the rightof the dashed line represent the question focus.Note that the tree cut yields a cut for each individ-ual topic chain (each path) within the question treeaccordingly.A cut of a topic chain ???
of a question q sepa-rates the topic chain in two parts: HEAD and TAIL.HEAD (denoted as ?????)
is the subsequence ofthe original topic chain ???
before the cut.
TAIL(denoted as ?????)
is the subsequence of ???
afterthe cut.
Thus,???
?
?????
?
?????.
For instance,given the tree cut specified in Figure 2, for the top-ic chain of Q1 ?Hamburg ?
Berlin ?
cool?club?,the HEAD and TAIL are ?Hamburg ?
Berlin?and ?cool?club?
respectively.2.3 Modeling question topic and question fo-cus for searchWe employ the framework of language modeling(for information retrieval) to develop our approachto question search.In the language modeling approach to informa-tion retrieval, the relevance of a targeted question??
to a queried question ?
is given by the probabili-ty ???|???
of generating the queried question ?Q1: Any cool clubs in Berlin or Hamburg?Q2: What are the most/best fun clubs in Berlin?Q3: Any nice hotels in Berlin or Hamburg?Q4: How long does it take to Hamburg from Berlin?Q5: Cheap hotels in Berlin?ROOTHamburgBerlinBerlincheap hotelfun clubcool clubnice hotelhow long does it take159from the language model formed by the targetedquestion ??.
The targeted question ??
is from a col-lection ?
of questions.Following the framework, we propose a mixturemodel for modeling question structure (namely,question topic and question focus) within theprocess of searching questions:???|???
?
?
?
??????|?????????????
?1 ?
???
?
??????|??????
(9)In the mixture model, it is assumed that theprocess of generating question topics and theprocess of generating question foci are independentfrom each other.In traditional language modeling, a single multi-nomial model ???|???
over terms is estimated foreach targeted question ??
.
In our case, two multi-nomial models ??????????
and ??????????
need tobe estimated for each targeted question ?
?.If unigram document language models are used,the equation (9) can then be re-written as,???|???
?
?
?
?
?????????????????,????????
?
?1 ?
???
?
?
?????????????????,????????
(10)where ??????
?, ??
is the frequency of ?
within ?.To avoid zero probabilities and estimate moreaccurate language models, the HEAD and TAIL ofquestions are smoothed using background collec-tion,??????????
?
?
?
???????????????????????????????????
?1 ?
??
?
????|??(11)??????????
?
?
?
????????????????????????????????????
?1 ?
??
?
????|??
(12)where ????|??????
, ????|??????
, and ????|??
are theMLE  estimators with respect to the HEAD of ?
?,the TAIL of ?
?, and the collection ?.3 Experimental ResultsWe have conducted experiments to verify the ef-fectiveness of our approach to question search.Particularly, we have investigated the use of identi-fying question topic and question focus for search.3.1 Dataset and evaluation measuresWe made use of the questions obtained from Ya-hoo!
Answers for the evaluation.
More specifically,we utilized the resolved questions under two of thetop-level categories at Yahoo!
Answers, namely?travel?
and ?computers & internet?.
The questionsinclude 314,616 items from the ?travel?
categoryand 210,785 items from the ?computers & internet?category.
Each resolved question consists of threefields: ?title?, ?description?, and ?answers?.
Forsearch we use only the ?title?
field.
It is assumedthat the titles of the questions already provideenough semantic information for understandingusers?
information needs.We developed two test sets, one for the category?travel?
denoted as ?TRL-TST?, and the other for?computers & internet?
denoted as ?CI-TST?.
Inorder to create the test sets, we randomly selected200 questions for each category.To obtain the ground-truth of question search,we employed the Vector Space Model (VSM) (Sal-ton et al, 1975) to retrieve the top 20 results andobtained manual judgments.
The top 20 resultsdon?t include the queried question itself.
Given areturned result by VSM, an assessor is asked tolabel it with ?relevant?
or ?irrelevant?.
If a returnedresult is considered semantically equivalent (orclose) to the queried question, the assessor willlabel it as ?relevant?
; otherwise, the assessor willlabel it as ?irrelevant?.
Two assessors were in-volved in the manual judgments.
Each of them wasasked to label 100 questions from ?TRL-TST?
and100 from ?CI-TST?.
In the process of manuallyjudging questions, the assessors were presentedonly the titles of the questions (for both the queriedquestions and the returned questions).
Table 2 pro-vides the statistics on the final test set.# Queries # Returned # RelevantTRL-TST 200 4,000 256CI-TST 200 4,000 510Table 2.
Statistics on the Test DataWe utilized two baseline methods for demon-strating the effectiveness of our approach, theVSM and the LMIR (language modeling methodfor information retrieval) (Ponte and Croft, 1998).We made use of three measures for evaluatingthe results of question search methods.
They areMAP, R-precision, and MRR.3.2 Searching questions about ?travel?In the experiments, we made use of the questionsabout ?travel?
to test the performance of our ap-proach to question search.
More specifically, weused the 200 queries in the test set ?TRL-TST?
tosearch for ?relevant?
questions from the 314,616160questions categorized as ?travel?.
Note that only thequestions occurring in the test set can be evaluated.We made use of the taxonomy of questions pro-vided at Yahoo!
Answers for the calculation ofspecificity of topic terms.
The taxonomy is orga-nized in a tree structure.
In the following experi-ments, we only utilized as the categories ofquestions the leaf nodes of the taxonomy tree (re-garding ?travel?
), which includes 355 categories.We randomly divided the test queries into fiveeven subsets and conducted 5-fold cross-validationexperiments.
In each trial, we tuned the parameters?, ?, and ?
in the equation (10)-(12) with four ofthe five subsets and then applied it to one remain-ing subset.
The experimental results reported be-low are those averaged over the five trials.Methods MAP R-Precision  MRRVSM 0.198 0.138 0.228LMIR 0.203 0.154 0.248LMIR-CUT 0.236 0.192 0.279Table 3.
Searching Questions about ?Travel?In Table 3, our approach denoted by LMIR-CUT is implemented exactly as equation (10).Neither VSM nor LMIR uses the data structurecomposed of question topic and question focus.From Table 3, we see that our approach outper-forms the baseline approaches VSM and LMIR interms of all the measures.
We conducted a signi-ficance test (t-test) on the improvements of ourapproach over VSM and LMIR.
The result indi-cates that the improvements are statistically signif-icant (p-value < 0.05) in terms of all the evaluationmeasures.Figure 3.
Balancing between Question Topic and Ques-tion FocusIn equation (9), we use the parameter ?
to bal-ance the contribution of question topic and the con-tribution of question focus.
Figure 3 illustrates howinfluential the value of ?
is on the performance ofquestion search in terms of MRR.
The result wasobtained with the 200 queries directly, instead of5-fold cross-validation.
From Figure 3, we see thatour approach performs best when ?
is around 0.7.That is, our approach tends to emphasize questiontopic more than question focus.We also examined the correctness of questiontopics and question foci of the 200 queried ques-tions.
The question topics and question foci wereobtained with the MDL-based tree cut model au-tomatically.
In the result, 69 questions have incor-rect question topics or question foci.
Furtheranalysis shows that the errors came from two cate-gories: (a) 59 questions have only the HEAD parts(that is, none of the topic terms fall within theTAIL part), and (b) 10 have incorrect orders oftopic terms because the specificities of topic termswere estimated inaccurately.
For questions onlyhaving the HEAD parts, our approach (equation (9))reduces to traditional language modeling approach.Thus, even when the errors of category (a) occur,our approach can still work not worse than the tra-ditional language modeling approach.
This alsoexplains why our approach performs best when ?
isaround 0.7.
The error category (a) pushes our mod-el to emphasize more in question topic.Methods ResultsVSM1.
How cold does it usually get in Charlotte,NC during winters?2.
How long and cold are the winters inRochester, NY?3.
How cold is it in Alaska?LMIR1.
How cold is it in Alaska?2.
How cold does it get really in Toronto inthe winter?3.
How cold does the Mojave Desert get inthe winter?LMIR-CUT1.
How cold is it in Alaska?2.
How cold is Alaska in March and out-door activities?3.
How cold does it get in Nova Scotia in thewinter?Table 4.
Search Results for?How cold does it get in winters in Alaska?
?Table 4 provides the TOP-3 search results whichare given by VSM, LMIR, and LMIR-CUT (ourapproach) respectively.
The questions in bold arelabeled as ?relevant?
in the evaluation set.
The que-ried question seeks for the ?weather?
informationabout ?Alaska?.
Both VSM and LMIR rank certain0.050.10.150.20.250.30 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MRR?161?irrelevant?
questions higher than ?relevant?
ques-tions.
The ?irrelevant?
questions are not about?Alaska?
although they are about ?weather?.
Thereason is that neither VSM nor PVSM is aware thatthe query consists of the two aspects ?weather?
(how cold, winter) and ?Alaska?.
In contrast, ourapproach assures that both aspects are matched.Note that the HEAD part of the topic chain of thequeried question given by our approach is ?Alaska?and the TAIL part is ?winter ?
how?cold?.3.3 Searching questions about ?computers &internet?In the experiments, we made use of the questionsabout ?computers & internet?
to test the perfor-mance of our proposed approach to question search.More specifically, we used the 200 queries in thetest set ?CI-TST??
to search for ?relevant?
questionsfrom the 210,785 questions categorized as ?com-puters & internet?.
For the calculation of specificityof topic terms, we utilized as the categories ofquestions the leaf nodes of the taxonomy tree re-garding ?computers & Internet?, which include 23categories.We conducted 5-fold cross-validation for the pa-rameter tuning.
The experimental results reportedin Table 5 are averaged over the five trials.Methods MAP R-Precision  MRRVSM 0.236 0.175 0.289LMIR 0.248 0.191 0.304LMIR-CUT 0.279 0.230 0.341Table 5.
Searching Questions about ?Computers & In-ternet?Again, we see that our approach outperforms thebaseline approaches VSM and LMIR in terms ofall the measures.
We conducted a significance test(t-test) on the improvements of our approach overVSM and LMIR.
The result indicates that the im-provements are statistically significant (p-value <0.05) in terms of all the evaluation measures.We also conducted the experiment similar tothat in Figure 3.
Figure 4 provides the result.
Thetrend is consistent with that in Figure 3.We examined the correctness of (automaticallyidentified) question topics and question foci of the200 queried questions, too.
In the result, 65 ques-tions have incorrect question topics or questionfoci.
Among them, 47 fall in the error category (a)and 18 in the error category (b).
The distribution oferrors is also similar to that in Section 3.2, whichalso justifies the trend presented in Figure 4.Figure 4.
Balancing between Question Topic and Ques-tion Focus4 Using Translation ProbabilityIn the setting of question search, besides the topicwhat we address in the previous sections, anotherresearch topic is to fix lexical chasm between ques-tions.Sometimes, two questions that have the samemeaning use very different wording.
For example,the questions ?where to stay in Hamburg??
and?the best hotel in Hamburg??
have almost the samemeaning but are lexically different in question fo-cus (where to stay vs. best hotel).
This is the so-called ?lexical chasm?.Jeon and Bruce (2007) proposed a mixture mod-el for fixing the lexical chasm between questions.The model is a combination of the language mod-eling approach (for information retrieval) andtranslation-based approach (for information re-trieval).
Our idea of modeling question structurefor search can naturally extend to Jeon et al?smodel.
More specifically, by using translationprobabilities, we can rewrite equation (11) and (12)as follow:??????????
?
??
?
??????????????
?
?
????|???
?
?????????????????????
?1 ?
??
?
???
?
????|??(13)??????????
?
??
?
??????????????
?
?
????|???
?
?????????????????????
?1 ?
??
?
???
?
????|??
(14)where ????|???
denotes the probability that topicterm ?
is the translation of ??.
In our experiments,to estimate the probability ????|??
?, we used thecollections of question titles and question descrip-tions as the parallel corpus and the IBM model 1(Brown et al, 1993) as the alignment model.0.150.20.250.30.350.40 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1MRR?162Usually, users reiterate or paraphrase their ques-tions (already described in question titles) in ques-tion descriptions.We utilized the new model elaborated by equa-tion (13) and (14) for searching questions about?travel?
and ?computers & internet?.
The new mod-el is denoted as ?SMT-CUT?.
Table 6 provides theevaluation results.
The evaluation was conductedwith exactly the same setting as in Section 3.
FromTable 6, we see that the performance of our ap-proach can be further boosted by using translationprobability.Data Methods MAP R-Precision MRRTRL-TSTLMIR-CUT 0.236 0.192 0.279SMT-CUT 0.266 0.225 0.308CI-TSTLMIR-CUT 0.279 0.230 0.341SMT-CUT 0.282 0.236 0.337Table 6.
Using Translation Probability5 Related WorkThe major focus of previous research efforts onquestion search is to tackle the lexical chasm prob-lem between questions.The research of question search is first con-ducted using FAQ data.
FAQ Finder (Burke et al,1997) heuristically combines statistical similaritiesand semantic similarities between questions to rankFAQs.
Conventional vector space models are usedto calculate the statistical similarity and WordNet(Fellbaum, 1998) is used to estimate the semanticsimilarity.
Sneiders (2002) proposed templatebased FAQ retrieval systems.
Lai et al (2002) pro-posed an approach to automatically mine FAQsfrom the Web.
Jijkoun and Rijke (2005) used su-pervised learning methods to extend heuristic ex-traction of Q/A pairs from FAQ pages, and treatedQ/A pair retrieval as a fielded search task.Harabagiu et al (2005) used a Question AnswerDatabase (known as QUAB) to support interactivequestion answering.
They compared seven differ-ent similarity metrics for selecting related ques-tions from QUAB and found that the concept-based metric performed best.Recently, the research of question search hasbeen further extended to the community-basedQ&A data.
For example, Jeon et al (Jeon et al,2005a; Jeon et al, 2005b) compared four differentretrieval methods, i.e.
vector space model, Okapi,language model (LM), and translation-based model,for automatically fixing the lexical chasm betweenquestions of question search.
They found that thetranslation-based model performed best.However, all the existing methods treat ques-tions just as plain texts (without considering ques-tion structure).
In this paper, we proposed toconduct question search by identifying questiontopic and question focus.
To the best of our know-ledge, none of the existing studies addressed ques-tion search by modeling both question topic andquestion focus.Question answering (e.g., Pasca and Harabagiu,2001; Echihabi and Marcu, 2003; Voorhees, 2004;Metzler and Croft, 2005) relates to question search.Question answering automatically extracts shortanswers for a relatively limited class of questiontypes from document collections.
In contrast to that,question search retrieves answers for an unlimitedrange of questions by focusing on finding semanti-cally similar questions in an archive.6 Conclusions and Future WorkIn this paper, we have proposed an approach toquestion search which models question topic andquestion focus in a language modeling framework.The contribution of this paper can be summa-rized in 4-fold: (1) A data structure consisting ofquestion topic and question focus was proposed forsummarizing questions; (2) The MDL-based treecut model was employed to identify question topicand question focus automatically; (3) A new formof language modeling using question topic andquestion focus was developed for question search;(4) Extensive experiments have been conducted toevaluate the proposed approach using a large col-lection of real questions obtained from Yahoo!
An-swers.Though we only utilize data from community-based question answering service in our experi-ments, we could also use categorized questionsfrom forum sites and FAQ sites.
Thus, as futurework, we will try to investigate the use of the pro-posed approach for other kinds of web services.AcknowledgementWe would like to thank Xinying Song, Shasha Li,and Shilin Ding for their efforts on developing theevaluation data.
We would also like to thank Ste-phan H. Stiller for his proof-reading of the paper.163ReferencesA.
Echihabi and D. Marcu.
2003.
A Noisy-Channel Ap-proach to Question Answering.
In Proc.
of ACL?03.C.
Fellbaum.
1998.
WordNet: An electronic lexical da-tabase.
MIT Press.D.
Metzler and W. B. Croft.
2005.
Analysis of statisticalquestion classification for fact-based questions.
In-formation Retrieval, 8(3), pages 481-504.E.
Fredkin.
1960.
Trie memory.
Communications of theACM, D. 3(9):490-499.E.
M. Voorhees.
2004.
Overview of the TREC 2004question answering track.
In Proc.
of TREC?04.E.
Sneiders.
2002.
Automated question answering usingquestion templates that cover the conceptual modelof the database.
In Proc.
of the 6th InternationalConference on Applications of Natural Language toInformation Systems, pages 235-239.G.
Salton, A. Wong, and C. S. Yang 1975.
A vectorspace model for automatic indexing.
Communica-tions of the ACM, vol.
18, nr.
11, pages 613-620.H.
Li and N. Abe.
1998.
Generalizing case frames us-ing a thesaurus and the MDL principle.
Computa-tional Linguistics, 24(2), pages 217-244.J.
Jeon and W.B.
Croft.
2007.
Learning translation-based language models using Q&A archives.
Tech-nical report, University of Massachusetts.J.
Jeon, W. B. Croft, and J. Lee.
2005a.
Finding seman-tically similar questions based on their answers.
InProc.
of SIGIR?05.J.
Jeon, W. B. Croft, and J. Lee.
2005b.
Finding similarquestions in large question and answer archives.
InProc.
of CIKM ?05, pages 84-90.J.
Rissanen.
1978.
Modeling by shortest data description.Automatica, vol.
14,  pages.
465-471J.M.
Ponte, W.B.
Croft.
1998.
A language modelingapproach to information retrieval.
In Proc.
ofSIGIR?98.M.
A. Pasca and S. M. Harabagiu.
2001.
High perfor-mance question/answering.
In Proc.
of SIGIR?01,pages 366-374.P.
F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: parameter estimation.
ComputationalLinguistics, 19(2):263-311.R.
D. Burke, K. J. Hammond, V. A. Kulyukin, S. L.Lytinen, N. Tomuro, and S. Schoenberg.
1997.
Ques-tion answering from frequently asked question files:Experiences with the FAQ finder system.
Technicalreport, University of Chicago.S.
Harabagiu, A. Hickl, J. Lehmann and D. Moldovan.2005.
Experiments with Interactive Question-Answering.
In Proc.
of ACL?05.V.
Jijkoun, M. D. Rijke.
2005.
Retrieving Answers fromFrequently Asked Questions Pages on the Web.
InProc.
of CIKM?05.Y.
Cao and H. Li.
2002.
Base noun phrase translationusing web data and the EM algorithm.
In Proc.
ofCOLING?02.Y.-S. Lai, K.-A.
Fung, and C.-H. Wu.
2002.
Faq miningvia list detection.
In Proc.
of the Workshop on Multi-lingual Summarization and Question Answering,2002.164
