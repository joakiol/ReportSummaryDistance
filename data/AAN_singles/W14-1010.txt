Proceedings of the 3rd Workshop on Hybrid Approaches to Translation (HyTra) @ EACL 2014, pages 58?66,Gothenburg, Sweden, April 27, 2014.c?2014 Association for Computational LinguisticsImproving the precision of automatically constructed human-orientedtranslation dictionariesAlexandra AntonovaYandex16, Leo Tolstoy St., Moscow, Russiaantonova@yandex-team.ruAlexey MisyurevYandex16, Leo Tolstoy St., Moscow, Russiamisyurev@yandex-team.ruAbstractIn this paper we address the problem ofautomatic acquisition of a human-orientedtranslation dictionary from a large-scaleparallel corpus.
The initial translationequivalents can be extracted with the helpof the techniques and tools developed forthe phrase-table construction in statisticalmachine translation.
The acquired transla-tion equivalents usually provide good lexi-con coverage, but they also contain a largeamount of noise.
We propose a super-vised learning algorithm for the detectionof noisy translations, which takes into ac-count the context and syntax features, av-eraged over the sentences in which a givenphrase pair occurred.
Across nine Euro-pean language pairs the number of seri-ous translation errors is reduced by 43.2%,compared to a baseline which uses onlyphrase-level statistics.1 IntroductionThe automatic acquisition of translation equiva-lents from parallel texts has been extensively stud-ied since the 1990s.
At the beginning, the acquiredbilingual lexicons had much poorer quality ascompared to the human-built translation dictionar-ies.
The limited size of available parallel corporaoften resulted in small coverage and the imper-fections of alignment methods introduced a con-siderable amount of noisy translations.
However,the automatimacally acquired lexicons served asinternal resources for statistical machine trans-lation (SMT) (Brown et al., 1993), informationretrieval (IR) (McEvan et al., 2002; Velupillai,2008), or computer-assisted lexicography (Atkins,1994; Hartmann, 1994).The current progress in search of web-basedparallel documents (Resnik, 2003; Smith, 2013)makes it possible to automatically construct large-scale bilingual lexicons.
These lexicons can al-ready compare in coverage to the traditional trans-lation dictionaries.
Hence a new interesting pos-sibility arises - to produce automatically acquiredhuman-oriented translation dictionaries, that havea practical application.
A machine translation sys-tem can output an automatically generated dictio-nary entry in response to the short queries.
Thepercentage of short queries can be quite large, andthe system benefits from showing several possibletranslations instead of a single result of machinetranslation (Figure 1).Figure 1: Examples of dictionary entries in twoonline statistical machine translation systems.The initial translation equivalents for a bilin-gual lexicon can be extracted with the help ofthe techniques and tools developed for the phrase-table construction in SMT.
The widely used wordalignment and phrase extraction algorithms are de-scribed in Brown et.al (1993) and Och (2004).Though an SMT phrase-table actually consists oftranslation equivalents, it may differ substantiallyfrom a traditional dictionary (Table 1).58Human-oriented dic-tionarySMT phrase-tableLemmatized entriesare preferred.Words and phrases inall forms are accept-able.Only linguisticallymotivated phrases areacceptable.Any multiword phraseis acceptable.Precision is important.Any noise is undesir-able.Having lots of low-probability noise isacceptable, since it isgenerally overriddenby better translations.Table 1: Differences between a human-orienteddictionary and an SMT phrase-table.While the problems of lemmatization and se-lection of linguistically motivated phrases can beaddressed by applying appropriate morphologicaland syntactic tools, the problem of noise reduc-tion is essential for the dictionary quality.
The cur-rent progress in the automatic acquisition of simi-lar Web documents in different languages (Resnik,2003; Smith, 2013) allows to collect large-scalecorpora.
But the automatically found documentscan be non-parallel, or contain spam, machinetranslation, language recognition mistakes, badlyparsed HTML-markup.
The noisy parallel sen-tences can be the source of lots of noisy transla-tions ?
unrelated, misspelled, or belonging to adifferent language.
For example, non-parallel sen-tencesThe apartment is at a height of 36floors!
(English)La plage est `a 1 minute envoiture.
(French: The beach is 1minute by car.
)may produce a wrong translation ?apartment -plage?.
Or, automatically translated sentencesThe figures in the foreground and back-ground play off each other well.
(En-glish)Les chiffres du premier plan et jouerhors de l?autre bien.
(French: The dig-its of the foreground and play out of theother well.
)may produce a wrong phrase translation ?figuresin the foreground - chiffres du premier plan?.An intuitive approach would be to apply noisefiltering to the corpus, not to the lexicon.
Onecould discard those sentences that deviate toomuch from the expected behavior.
For example,sentences that have many unknown words and fewsymmetrically aligned words are unlikely to be re-ally parallel.
However, natural language demon-strates a great variability.
A single sentence paircan deviate strongly from the expected behavior,and still contain some good translations.
On theother hand, many noisy translations can still pen-etrate the lexicon, and further noise detection isnecessary.In a bilingual lexicon we want not just to lowerthe probabilities of noisy translations, but to re-move them completely.
This can be regarded as abinary classification task ?
the phrase pairs are tobe classified into good and noisy ones.Different types of information can be com-bined in a feature vector.
We take advantage ofthe phrase-level features, such as co-occurrencecounts or translation probabilities, and also pro-pose a number of sentence-level context features.To calculate the sentence-level features for a givenphrase-pair, we average the characteristics of allthe sentences where it occurs.We test the proposed algorithm experimentally,by constructing the bilingual lexicons for nine lan-guage pairs.
The manually annotated samplesof phrase pairs serve as the data for training su-pervised classifiers.
The experiment shows thatthe use of the sentence-level features increasesthe classification accuracy, compared to a baselinewhich uses only phrase frequencies and translationprobabilities.
We compare the accuracy of differ-ent classifiers and evaluate the importance of dif-ferent features.The rest of the paper is organized as follows.
InSection 2 we outline the related work.
Section 3describes our approach to the noise reduction in abilingual lexicon and discusses the proposed fea-tures.
We describe our experiments on trainingclassifiers in Section 4.
Section 5 concludes thepaper.2 Previous workThe methods of extracting a bilingual lexicon fromparallel texts as a part of the alignment processare discussed in Brown (1993), Melamed (1996),Tufis?
and Barbu (2001).
Melamed (1996) pro-poses a method of noise reduction that allows59to re-estimate and filter out indirect word asso-ciations.
However, he works with a carefullyprepared Hansards parallel corpus and the noisecomes only from the imperfections of statisticalmodeling.Sahlgren (2004) proposes a co-occurrence-based approach, representing words as high-dimensional random index vectors.
The vectorsof translation equivalents are expected to havehigh correlation.
Yet, he notes that low-frequencywords do not produce reliable statistics for thismethod.The methods of bilingual lexicon extractionfrom comparable texts (Rapp, 1995; Fung, 1998;Otero, 2007) also deal with the problem of noisereduction.
However, the precision/recall ratio of alexicon extracted from comparable corpus is gen-erally lower.
For the purpose of building a human-oriented dictionary, the parallel texts may providethe larger coverage and better quality of the trans-lation equivalents.The noise reduction task is addressed by someof the SMT phrase-table pruning techniques.
Themost straightforward approach is thresholding onthe translation probability (Koehn et al., 2003).Moore (2004) proposes the log-likelihood ratioand Fisher?s exact test to re-estimate word asso-ciation strength.
Johnson et al.
(2007) appliesFisher?s exact test to dramatically reduce the num-ber of phrase pairs in the phrase-table.
They getrid of phrases that appear as alignment artifacts orare unlikely to occur again.
The implementationof their algorithm requires a special index of allparallel corpus in order to enable a quick look-upfor a given phrase pair.
Eck et al.
(2007) assessesthe phrase pairs based on the actual usage statisticswhen translating a large amount of text.
Entropy-based criteria are proposed in Ling et al.
(2012),Zens et al.
(2012).Automatically acquired bilingual lexicons arecapable to reflect many word meanings and trans-lation patterns, which are often not obvious evento the professional lexicographers (Sharoff, 2004).Their content can also be updated regularly to in-corporate more parallel texts and capture the trans-lations of new words and expressions.
Thus, themethods allowing to improve the quality of au-tomatic bilingual lexicons are of practical impor-tance.3 Noise detection featuresWe treat the noise recognition task as a binaryclassification problem.
A set of nonlexical con-text features is designed to be sensitive to differ-ent types of noise in the parallel corpus.
We ex-pect that the combination of these features withthe phrase-level features based on co-occurrencestatistics can improve the accuracy of the classifi-cation and the overall quality of a bilingual lexi-con.3.1 Context feature extraction algorithmThe procedure of getting the context featuresis outlined in Algorithm 1.
Unlike Johnson etal.
(2007) we do not rely on any pre-constructedindex of the parallel sentences, because it requiresa lot of RAM on large corpora.
Instead we re-run the phrase extraction algorithm of the Mosestoolkit (Koehn et al., 2007) and update the con-text features at the moment when a phrase pair t isfound.Algorithm 1 Calculate context features for all lex-icon entriesRequire: Parallel corpus ?
C; {word-alignedsentences}Require: Bilingual lexicon ?
D; {this is aphrase-table, derived from C and modified asdescibed in 4.1}Ensure: V = {v?
(d): d ?
D}; {resulting fea-tures}for all d ?
D dov?(d)?
0;n(d)?
0;for all s ?
C doT ?
PhraseExtraction(s);{Moses func-tion}for all t ?
T doif t ?
D thenv?(t)?
v?
(t) + SentFeats(s); {Alg.
2}n(t)?
n(t) + 1;for all d ?
D dov?
(d) ?
v?
(d)/(1 + n(d)); {average,+1 smoothing}return V3.2 Sentence-level featuresThe phrase extraction algorithms do not preservethe information about the sentences in which agiven phrase pair occurred, assuming that all thesentences are equally good.
As a result, the60phrase-level statistics is insufficient in case of anoisy corpus.The sentence-level features are designed topartly restore the information which is lost dur-ing the phrase extraction process.
We try to es-timate the general characteristics of the whole setof parallel sentences where a given phrase pair oc-curred.
The proposed sentence-level features relyon the different sources of information, which arediscussed in 3.2.1, 3.2.2 and 3.2.3.
Table 2 pro-vides illustrating examples of noisy phrase pairsand sample sentences.3.2.1 Word-alignment annotationWe use the intersection of direct and reverseGiza++ (Och and Ney, 2004) alignments as aheuristic rule to find words reliably aligned to eachother.
The alignment information gives rise to sev-eral sentence-level features:?
UnsafeAlign - percentage of words that arenot symmetrically aligned to each other.?
UnsafeJump - average distance betweenthe translations of subsequent input words.?
UnsafeDigAlign percentage of unequaldigits among the symmetrically alignedwords.The UnsafeAlign and UnsafeJump values canvary in different sentences.
However, their beingtoo large on the whole set of sentences where agiven phrase pair occurred possibly indicates somesystematic noise.The translations of digits are not included to thedictionary by themselves.
But if a pair of digits iswrongly aligned, then its nearest context may alsobe aligned wrongly.3.2.2 One-side morphological and syntacticannotationThe target side of our parallel sentences has beenprocessed by a rule-based parser.
The syntax givesrise to:?
UnsafeStruct - percentage of words havingno dependence on any other word in the parsetree.The morphological annotation participates in:?
OOV - percentage of out-of-vocabularywords in the sentence.The low parse tree connectivity may indicate thatthe sentence is ungrammatical or produced by apoor-quality machine translation system.
Sen-tences containing many out-of-vocabulary wordsprobably do not belong to the given language.
Wecompute out-of-vocabulary words according to anexternal vocabulary, which is embedded in taggingand parsing tools.
However, instead one can use acollection of unigrams filtered by some frequencythreshold..gratuit ?
internet access, Slem= 215Sample sentence:Petit d?ejeuner continental de luxe gratuitBusiness center with free wireless Internet ac-cessUnsafeAlign = 0.387`a ?
you, Slem= 586La plainte `a transmettreYou should submit your complaintUnsafeJump = 1.75juin ?
May, Slem= 35Membre depuis: 17 juin 2011Member since: 01 May 2012UnsafeAlignDig = 0.08le ?
Fr, Slem= 24Edvaldo et le p`ere AntenoreEdvaldo and Fr AntenoreOOV = 0.117Paris ?
England, Slem= 54TERTIALIS (Paris, Paris)(England)Punct = 0.117Table 2: Examples of noisy French-English trans-lations to which different sentence-level featuresmay be sensitive.
Slem?
is the number ofsentences where a lemmatized phrase pair co-occurred.
Sample sentences are provided.3.2.3 Surface textThe surface word tokens can be used for:?
Punct - percentage of non-word/punctuationtokens in the sentence.?
Uniqueness - the percentage of unique uni-grams in both source and target language sen-tences.Sentences with lots of punctuation can be un-natural or contain enumeration.
Large enumera-tion lists are often not exactly parallel and can be61aligned incorrectly, because punctuation tokens,like many commas, are easily mapped to eachother.
The low Uniqueness possibly indicatesthat the sentences containing a given phrase pairare similar to each other.
This can lead to overes-timated translation probabilities.Algorithm 2 Get features of one sentence pair(SentFeats)Require: sentsrc= (w1, ..., wm);Require: sentdst= (w1, ..., wn);Require: Alignment matrix ?
Mm,n: x ?
{0, 1}; {intersection of two Giza++ align-ments}Require: oov = (x1, ..., xn), x ?
{0, 1};{xi= 1 ??
sentdst[i] is out-of-vocabulary}Require: pnt = (x1, ..., xn), x ?
{0, 1};{xi= 1 ??
sentdst[i] is punctuation}Require: nohead = (x1, ..., xn), x ?
{0, 1};{xi= 1 ??
sentdst[i] is not dependenton any other word in the parse}Ensure: v?
= (v1, ..., v7); {features}v?
?
0;v2?1n?x?noheadx; {UnsafeStruct}Let A be the set of pairs of indices of symmet-rically aligned words, ordered by the source in-dices:A?
{(i, j) |M(i, j) = 1};v3?
1?|A|m+n; {UnsafeAlign}for all (i, j) ?
A doif words with indices i, j are unequal digitsthenv4?
v4+ 1;v4?v4|A|; {UnsafeAlignDig}v5?1|A|?(i,j)?Aji?
ji?1; {UnsafeJump}v6?1n?x?oovx; {OOV }v7?1n?x?pntx; {Punct}return v?3.3 Phrase-level statisticsMultiple phrase-level features can be derived fromthe occurrence and co-occurrence counts, that arecalculated during the phrase extraction procedureas described in Koehn et.
al (2003).?
C(f), C(e), C(e, f) ?
surface phrase occur-rence counts.?
Clem(f), Clem(e), Clem(e, f) ?
same forlemmatized phrases.?
S(e, f), Slem(e, f) ?
the number of sen-tences, in which the surface (or lemmatized)phrases co-occurred.?
P (e|f), P (f |e) ?
translation probabilitiesof surface phrases.?
Plem(e|f), Plem(f |e) ?
translation proba-bilities of lemmatized phrases.Some of these features are highly correlated, andit is hard to tell in advance which subset leads tobetter performance.4 ExperimentWe conducted experiments on nine languagepairs: German-English, German-Russian, French-English, French-Russian, Italian-English, Italian-Russian, Spanish-English, Spanish-Russian andEnglish-Russian.
The parallel corpora consistedof the sentence-aligned documents automaticallycollected from multilingual web-sites.We implemented the procedure of bilingual lex-icon construction and the algorithm calculating thesentence-level features (Section 3).The annotated phrase pair samples, one foreach language pair, provided positive and nega-tive examples for training a supervised classifier.We compared the accuracy of several classifierstrained on different feature sets.
The importanceof different features was evaluated .4.1 Bilingual lexicon creationWe used Giza++ for word alignment and Mosestoolkit for phrase extraction procedure.
The fol-lowing automatic annotation had been provided.The source side of the parallel corpora had beenprocessed by a part-of-speech tagger, and eachword had been assigned a lemma based on its tag.The target side of the parallel corpora, which wasalways either English or Russian, was processedby a rule-based dependency parser, which alsosupplied morphological annotations and lemmas.In the case of English-Russian corpus, the sourceside had also been processed by the parser.62The extracted English phrases were restrictedto at most 3 words, provided that they were con-nected in the dependency tree.
The same restric-tions were imposed on the Russian phrases.
Theextracted phrases for all other languages were re-stricted to single words to avoid the ungrammati-cal multiword expressions.Each extracted phrase pair was assigned a lem-matized key consisting of lemmas of all wordsin it.
The co-occurrence counts were summedover all phrase pairs sharing the same key, giv-ing the aggregate count Clem(e, f).
Then a sin-gle pair was chosen to serve as a best substitutefor a lemmatized lexicon entry.
The choice wasmade heuristically, based on the morphological at-tributes and co-occurrence counts.As a preliminary lexicon cleanup we removedthe phrase pairs which contained punctuation sym-bols or digits on either side.
We also removed thepairs that co-occurred only once in the corpus.
Anexample of differences between the size of originalphrase table and the size of bilingual lexicon af-ter lemmatization and preliminary cleanup is rep-resented in Table 3.Millions of phrase pairsfr-en fr-ruInitial 1-3 phrase-table 16.4 30.8After lemmatization 7.9 6.4After preliminary cleanup 1.6 0.8Table 3: The number of phrase pairs on differentstages of French-English and French-Russian dic-tionary creation.
Phrase pairs in the initial phrasetable are restricted to at most 1 source word and atmost 3 target words.4.2 Experimental dataFor the experiment we selected random1transla-tion equivalents from the nine translation lexicons,to which no further noise reduction had been ap-plied.
The resulting translation equivalents wereassessed by human experts.
The annotation taskwas to determine how well a phrase pair fits for ahuman-oriented translation dictionary.
The anno-tators classified each translation according to thefollowing gradation:Class 0 ?
difficult to assess.1Random was used proportionally to the square root ofjoint frequency, in order to balance rare and frequent phrasepairs in the sample.Class 1 ?
totally wrong or noisy (e.g.misspelled);Class 2 ?
incorrect or incomplete trans-lation;Class 3 ?
not a mistake, but unneces-sary translation;Class 4 ?
good, but not vital;Class 5 ?
vital translation (must bepresent in human-built dictionary);The pairs annotated as 0 usually representedthe translations of unfamiliar words, abbreviationsand the like.
Such phrases were excluded fromtraining and testing.
We didn?t use ?acceptable,but unnecessary?
translation pairs either, becausethey do not influence the quality of the lexicon.We treated as negative the phrase pairs that wereannotated as 1 or 2.
Analogously, the positive ex-amples had to belong to 4 or 5 class.
The annota-tion statistics is given in Table 4.Language Size %Negative %Positiveit-ru 2340 56.6 28.7it-en 2366 59.9 21.4es-ru 2388 55.5 27.2es-en 2384 69.0 24.0de-ru 2397 50.3 37.6de-en 2438 72.1 24.5fr-ru 2461 44.5 31.2fr-en 2325 57.0 24.4en-ru 2346 27.8 33.2Table 4: Statistics of the annotated data: the num-ber of annotated phrase pairs, the percentage ofnegative and positive examples.4.3 Training settingThe experiments were run with two different fea-ture sets:?
Baseline ?
features based on co-occurrencecounts.?
Full ?
baseline and sentence-level features.We had to choose a subset of co-occurrence-basedfeatures experimentally (see, Section 3.3).
Thebest subset for our data consisted of three features:log(Slem), log(P (e|f)), log(P (f |e)).
In the fullfeature set we combined the baseline features andthe sentence-level features calculated as describedin Algorithm 2.63We considered three metrics related to the im-provement of the lexicon quality:?
Err ?
the percentage of prediction errors;?
Err-1 ?
the percentage of class 1 exampleswhich were classified as positive.?
F1 ?
the harmonic mean of precision and re-call w.r.t.
the positive and negative examples;We used the standard packages of the R pro-gramming language, to train and tune differ-ent classifiers: random forest (RF), support vec-tor machines (SVM), logistic regression (GLM),Naive Bayes classifier, neural networks, k-NearestNeighbors and some of the combinations of thesemethods with SVD.
To assess the predictive accu-racy we used repeated random sub-sampling val-idation.
In each of 40 iterations, a 10% test setwas randomly chosen from the dataset, the modelwas trained on the rest of the data, and then tested.The resulting accuracy was averaged over the iter-ations.Classifier Full feature set Base feature set%Err %Err-1 %Err %Err-1RF 19.80 8.31 24.00 14.62SVM 19.63 9.36 23.49 12.91GLM 22.74 6.35 25.23 7.30Table 5: Percentage of prediction errors of dif-ferent classifiers, averaged over the nine languagepairs.The results of RF, SVM and GLM are reportedin Table 5.
Though the composition of differentclassifiers could perform slightly better, it wouldrequire an individual tuning for each languagepair.
For clearness, we use a single classifier (RF)for the rest of the experiments.The experiment showed that training on the fullfeature set reduced the total amount of predictionerrors by 17.5%, compared to the baseline setting.The number of false positives among the class 1examples reduced by 43%.
It is also important thatbetter results were obtained on each of the ninelanguage pairs, not only on average.
In Table 6the baseline results are shown in brackets and onecan see that F1 diminishes in the baseline setting,while the percentage of errors goes up.
The classi-fication accuracy depends on the size of the train-ing set (Table 7).Lang %Err %Err-1 F1de-en 18.0 (+3.6) 4.0 (+5.2) .562 (-.050)de-ru 25.7 (+4.0) 13.5 (+6.7) .672 (-.040)es-en 16.4 (+3.8) 3.2 (+4.0) .610 (-.059)es-ru 20.6 (+4.7) 8.3 (+6.0) .643 (-.064)fr-en 20.5 (+1.5) 6.0 (+5.8) .603 (-.031)fr-ru 21.4 (+6.1) 15.5 (+10.8) .704 (-.070)it-en 15.2 (+3.3) 3.5 (+2.9) .663 (-.059)it-ru 19.6 (+5.5) 9.4 (+6.7) .670 (-.071)en-ru 20.8 (+5.6) 11.5 (+8.8) .797 (-.048)Table 6: Classification quality of the classifiertrained on all features, compared to the baselinetrained only on phrase-level features.
The relativechange of the baseline values is given in brackets.Examples 1700 680 272 108 43Accuracy .803 .794 .780 .757 .709Table 7: Classification accuracy w.r.t different sizeof training set averaged over eight language pairs.We measured the impact of different features,as described in Breiman (2001), with the help ofthe standard function of the R library ?random-Forest?
(Table 8).
The three baseline featureswere ranked as most important, followed by Un-safeAlign, OOV, UnsafeJump and others.Feature Importancelog(Slem) 35.679log(P (e|f)) 33.9729log(P (f |e)) 28.8637UnsafeAlign 24.3705OOV 22.8306UnsafeJump 20.1108Punct 15.4501UnsafeStruct 15.1157Uniqueness 13.5049UnsafeDigAlign 12.915Table 8: Feature importance measured bythe mean decrease of classification accu-racy (Breiman, 2001).
The value is averaged overthe nine language pairs.We explored the dependence of the predictionaccuracy on the co-occurrence frequency of aphrase pair for the classifiers trained on the fullfeature set and on the baseline feature set.
The re-sults for German-English and French-English lan-64guage pairs are shown in Figure 2.
The accu-racy function was smoothed with cubic smooth-ing spline.
The differences in the distribution ofclassification errors between language pairs sug-gest that the nature of the noise can vary for dif-ferent corpora.
The general U shape of the curvesin Figure 2 is partly due to the fact that there aremany true negatives in the low-frequency area, andmany true positives in the high-frequency area.0 500 1000 1500 20000.60.70.80.9rfsvmglmrf?bsvm?bglm?b0 500 1000 15000.700.750.800.850.900.95rfsvmglmrf?bsvm?bglm?bFigure 2: Prediction accuracy of different classi-fiers w.r.t.
the phrase pairs sorted by the ascend-ing co-occurrence count.
The upper plot relatesto the German-English pair, the bottom relates toFrench-English pair.
The labels rf, svm, glm re-fer to the classifiers trained on the full feature set;rf-b, svm-b, glm-b refer to the baseline setting.Table 9 reports the top English translations ofthe French word ?connexion?
before the noise re-duction and shows which variants were recognizedas positive and negative by the RF classifier.English C(e, f) p(f |e) p(e|f) RFconnection 58018 0.689 0.374 +wireless 32630 0.450 0.211 -free 31775 0.113 0.205 -wifi 16272 0.382 0.105 -login 4910 0.443 0.032 +connectivity 394 0.055 0.003 +logon 290 0.185 0.002 +access 276 0.001 0.002 -link 148 0.001 0.001 -Table 9: English translations of the French word?connexion?.
C(e, f) is the co-occurrence count,p(f |e), p(e|f) are the translation probabilities oflemmatized pairs.
The last column shows the clas-sification result.5 ConclusionThe main contributions of this paper are the fol-lowing.
We address the problem of noise reduc-tion in automatic construction of human-orientedtranslation dictionary.
We introduce an approachto increase the precision of automatically acquiredbilingual lexicon, which allows to mitigate thenegative impact of a noisy corpus.
Our noisereduction method relies on the supervised learn-ing on a small set of annotated translation pairs.In addition to the phrase-level statistics, such asco-occurrence counts and translation probabilities,we propose a set of non-lexical context featuresbased on the analysis of sentences in which aphrase pair occurred.
The experiment demon-strates a substantial improvement in the accuracyof the detection of noisy translations, compared toa baseline which uses only phrase-level statistics.We have shown that the proposed noise de-tection method is applicable to various languagepairs.
The alignment-based features can be easilyobtained for any parallel corpus, even if other toolsdo not exist.
We hope that our noise detection ap-proach can also be adapted for SMT phrase-tables,if the initial parallel sentences are still available.ReferencesB.
T. Sue Atkins.
1994.
A corpus-based dictionary.In Oxford-Hachette French Dictionary, Introductionxix-xxxii.
Oxford: Oxford University Press.Leo Breiman.
2001.
Random Forests.
Machine Learn-ing 45 5-32.65Peter F. Brown, Stephen A. Della Pietra, Vincent J.Della Pietra and Robert L. Mercer.
1993.
TheMathematics of Statistical Machine Translation:Parameter estimation.
Computational Linguistics,19(2):263?312, June.Matthias Eck, Stephan Vogel, and Alex Waibel.
2007.Translation model pruning via usage statistics forstatistical machine translation.
In Human LanguageTechnologies 2007: The Conference of the NAACL;Companion Volume, Short Papers, pages 21?24,Rochester, New York, April.
Association for Com-putational LinguisticsPascale Fung.
1998.
A Statistical View on BilingualLexicon Extraction from Parallel Corpora to Non-parallel Corpora.
Parallel Text Processing: Align-ment and Use of Translation Corpora.
Kluwer Aca-demic PublishersHartmann, R.R.K.
1994.
The use of parallel text cor-pora in the generation of translation equivalents forbilingual lexicography.
In W. Martin, et al.
(Eds.
),Euralex 1994 Proceedings (pp.
291-297).
Amster-dam: Vrije Universiteit.Howard Johnson, Joel Martin, George Foster, andRoland Kuhn 2007.
Improving translation qualityby discarding most of the phrasetable.
In Proceed-ings of EMNLP-CoNLL, ACL, Prague, Czech Re-public, pages 967-975.Philipp Koehn, Franz Josef Och, and Daniel Marcu2003.
Statistical phrase-based translation.
In Pro-ceedings of HLT-NAACL 2003, pages 127?133.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ond?rej Bojar, AlexandraConstantin, and Evan Herbst 2007.
Moses: Opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180, Prague, Czech RepublicAkira Kumano and Hideki Hirakawa.
1994.
Build-ing An MT Dictionary From Parallel Texts BasedOn Linguistic And Statistical Information.
COLING1994: 76-81Wang Ling, Jo?ao Grac?a, Isabel Trancoso and AlanBlack 2012.
Entropy-based Pruning for Phrase-based Machine Translation.
In Proceedings ofEMNLP-CoNLL, Association for ComputationalLinguistics, Jeju Island, Korea, pp.
972-983C.
J.
A. McEwan, I. Ounis, and I. Ruthven.
2002.Building bilingual dictionaries from parallel webdocuments.
In Proceedings of the 24th BCS-IRSGEuropean Colloquium on IR Research, pp.
303-323.Springer-Verlag.I.
Dan Melamed.
1996.
Automatic constructionof clean broad-coverage translation lexicons.
InProceedings of the 2nd Conference of the Associa-tion for Machine Translation in the Americas, pages125?134, Montreal, CanadaI.
Dan Melamed.
2000.
Models of TranslationalEquivalence among Words.
Computational Linguis-tics 26(2), 221-249, June.Robert C. Moore.
2004.
On Log-Likelihood-Ratiosand the Significance of Rare Events.
In Proceed-ings of the 2004 Conference on Empirical Methodsin Natural Language Processing, Barcelona, Spain.Franz Josef Och and Hermann Ney.
2000.
ImprovedStatistical Alignment Models.
Proceedings of the38th Annual Meeting of the ACL, pp.
440-447,Hongkong, China.Franz Josef Och and Hermann Ney.
2004.
TheAlignment Template Approach to Statistical Ma-chine Translation.
Computational Linguistics, vol.30 (2004), pp.
417-449.Pablo Gamallo Otero.
2007.
Learning bilingual lexi-cons from comparable English and Spanish corpora.Proceedings of MT Summit XI, pages 191?198.Reinhard Rapp.
1995.
Identifying word translations innon-parallel texts.
In Proceedings of the ACL 33,320-322.Resnik, Philip and Noah A. Smith.
2003.
The web asa parallel corpus.. Computational Linguistics, 29,pp.349?380Magnus Sahlgren.
2004.
Automatic Bilingual Lexi-con Acquisition Using Random Indexing.
Journalof Natural Language Engineering, Special Issue onParallel Texts, 11.Serge Sharoff.
2004.
Harnessing the lawless: usingcomparable corpora to find translation equivalents.Journal of Applied Linguistics 1(3), 333-350.Jason Smith, Herve Saint-Amand, Magdalena Pla-mada, Philipp Koehn, Chris Callison-Burch andAdam Lopez.
2013.
Dirt Cheap Web-Scale Par-allel Text from the Common Crawl.
To appear inProceedings of ACL 2013.Dan Tufis?
and Ana-Maria Barbu.
2001.
Computa-tional Bilingual Lexicography: Automatic Extrac-tion of Translation Dictionaries.
In InternationalJournal on Science and Technology of Informa-tion, Romanian Academy, ISSN 1453-8245, 4/3-4,pp.325-352Velupillai, Sumithra, Martin Hassel, and HerculesDalianis.
2008.
?Automatic Dictionary Construc-tion and Identification of Parallel Text Pairs.
In Pro-ceedings of the International Symposium on UsingCorpora in Contrastive and Translation Studies (UC-CTS).Richard Zens, Daisy Stanton and Peng Xu.
2012.A Systematic Comparison of Phrase Table PruningTechniques.
In Proceedings of EMNLP-CoNLL,ACL, Jeju Island, Korea, pp.
972-983.66
