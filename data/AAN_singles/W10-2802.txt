Proceedings of the 2010 Workshop on GEometrical Models of Natural Language Semantics, ACL 2010, pages 7?16,Uppsala, Sweden, 16 July 2010. c?2010 Association for Computational LinguisticsManifold Learning for the Semi-Supervised Inductionof FrameNet Predicates: An Empirical InvestigationDanilo Croce and Daniele Previtali{croce,previtali}@info.uniroma2.itDepartment of Computer Science, Systems and ProductionUniversity of Roma, Tor VergataAbstractThis work focuses on the empirical inves-tigation of distributional models for theautomatic acquisition of frame inspiredpredicate words.
While several seman-tic spaces, both word-based and syntax-based, are employed, the impact of ge-ometric representation based on dimen-sionality reduction techniques is inves-tigated.
Data statistics are accordinglystudied along two orthogonal perspectives:Latent Semantic Analysis exploits globalproperties while Locality Preserving Pro-jection emphasizes the role of local reg-ularities.
This latter is employed by em-bedding prior FrameNet-derived knowl-edge in the corresponding non-euclideantransformation.
The empirical investiga-tion here reported sheds some light on therole played by these spaces as complexkernels for supervised (i.e.
Support VectorMachine) algorithms: their use configures,as a novel way to semi-supervised lexicallearning, a highly appealing research di-rection for knowledge rich scenarios likeFrameNet-based semantic parsing.1 IntroductionAutomatic Semantic Role Labeling (SRL) is anatural language processing (NLP) technique thatmaps sentences to semantic representations andidentifies the semantic roles conveyed by senten-tial constituents (Gildea and Jurafsky, 2002).
Sev-eral NLP applications have exploited this kind ofsemantic representation ranging from InformationExtraction (Surdeanu et al, 2003; Moschitti et al,2003)) to Question Answering (Shen and Lapata,2007), Paraphrase Identification (Pado and Erk,2005), and the modeling of Textual Entailment re-lations (Tatu and Moldovan, 2005).
Large scaleannotated resources have been used by Seman-tic Role Labeling methods: they are commonlydeveloped using a supervised learning paradigmwhere a classifier learns to predict role labelsbased on features extracted from annotated train-ing data.
One prominent resource has been de-veloped under the Berkeley FrameNet project asa semantic lexicon for the core vocabulary of En-glish, according to the so-called frame seman-tic model (Fillmore, 1985).
Here, a frame is aconceptual structure modeling a prototypical sit-uation, evoked in texts through the occurrence ofits lexical units (LU) that linguistically expressesthe situation of the frame.
Lexical units of thesame frame share semantic arguments.
For ex-ample, the frame KILLING has lexical units suchas assassin, assassinate, blood-bath, fatal, mur-derer, kill or suicide that share semantic argumentssuch as KILLER, INSTRUMENT, CAUSE, VICTIM.The current FrameNet release contains about 700frames and 10,000 LUs.
A corpus of 150,000 an-notated examples sentences, from the British Na-tional Corpus (BNC), is also part of FrameNet.Despite the size of this resource, it is un-der development and hence incomplete: severalframes are not represented by evoking words andthe number of annotated sentences is unbalancedacross frames.
It is one of the main reason for theperformance drop of supervised SRL systems inout-of-domain scenarios (Baker et al, 2007) (Jo-hansson and Nugues, 2008).
The limited cover-age of FrameNet corpus is even more noticeablefor the LUs dictionary: it only contains 10,000lexical units, far less than the 210,000 entries inWordNet 3.0.
For example, the lexical unit crown,according to the annotations, evokes the ACCOU-TREMENT frame.
It refers to a particular sense:according to WordNet, it is ?an ornamental jew-eled headdress signifying sovereignty?.
Accord-ing to the same lexical resource, this LU has 12lexical senses and the first one (i.e.
?The Crown7(or the reigning monarch) as the symbol of thepower and authority of a monarchy?)
could evokeother frames, like LEADERSHIP.
In (Pennacchiottiet al, 2008) and (De Cao et al, 2008), the prob-lem of LU automatic induction has been treatedin a semi-supervised fashion.
First, LUs are mod-eled by exploiting the distributional analysis of anunannotated corpus and the lexical information ofWordNet.
These representations were used in or-der to find out frames potentially evoked by novelwords in order to extend the FrameNet dictionarylimiting the effort of manual annotations.In this work the distributional model of LUsis further developed.
As in (Pennacchiotti et al,2008), several word spaces (Pado and Lapata,2007) are investigated in order to find the mostsuitable representation of the properties whichcharacterize a frame.
Two dimensionality reduc-tion techniques are applied here in this context.Latent Semantic Analysis (Landauer and Dumais,1997) uses the Singular Value Decomposition tofind the best subspace approximation of the orig-inal word space, in the sense of minimizing theglobal reconstruction error projecting data alongthe directions of maximal variance.
Locality Pre-serving Projection (He and Niyogi, 2003) is alinear approximation of the nonlinear LaplacianEigenmap algorithm: its locality preserving prop-erties allows to add a set of constraints forcingLUs that belong to the same frame to be near inthe resulting space after the transformation.
LSAperforms a global analysis of a corpus capturingrelations between LUs and removing the noise in-troduced by spurious directions.
However it risksto ignore lexical senses poorly represented into thecorpus.
In (De Cao et al, 2008) external knowl-edge about LUs is provided by their lexical sensesfrom a lexical resource (e.g WordNet).
In thiswork, prior knowledge about the target problem isdirectly embedded into the space through the LPPtransformation, by exploiting locality constraints.Then a Support Vector Machine is employed toprovide a robust acquisition of lexical units com-bining global information provided by LSA andthe local information provided by LPP into a com-plex kernel function.In Section 2 related work is presented.
In Sec-tions 3 the investigated distributional model ofLUs is presented as well as the dimensionality re-duction techniques.
Then, in Section 4 the exper-imental investigation and comparative evaluationsare reported.
Finally, in Section 5 we draw finalconclusions and outline future work.2 Related WorkAs defined in (Pennacchiotti et al, 2008), LU in-duction is the task of assigning a generic lexicalunit not yet present in the FrameNet database (theso-called unknown LU) to the correct frame(s).The number of possible classes (i.e.
frames) andthe multiple assignment problem make it a chal-lenging task.
LU induction has been integratedat SemEval-2007 as part of the Frame Seman-tic Structure Extraction shared task (Baker et al,2007), where systems are requested to assign thecorrect frame to a given LU, even when the LU isnot yet present in FrameNet.
Several approachesshow low coverage (Johansson and Nugues, 2007)or low accuracy, like (Burchardt et al, 2005).
Thistask is presented in (Pennacchiotti et al, 2008) and(De Cao et al, 2008), where two different mod-els which combine distributional and paradigmatic(i.e.
lexical) information have been discussed.
Thedistributional model is used to select a list of framesuggested by the corpus?
evidences and then theplausible lexical senses of the unknown LU areused to re-rank proposed frames.In order to exploit prior information providedby the frame theory, the idea underlying is that se-mantic knowledge can be embedded from exter-nal sources (i.e the FrameNet database) into thedistributional model of unannotated corpora.
In(Basu et al, 2006) a limited prior knowledge is ex-ploited in several clustering tasks, in term of pair-wise constraints (i.e., pairs of instances labeledas belonging to same or different clusters).
Sev-eral existing algorithms enhance clustering qual-ity by applying supervision in the form of con-straints.
These algorithms typically utilize thepairwise constraints to either modify the clusteringobjective function or to learn the clustering distor-tion measure.
The approach discussed in (Basu etal., 2006) employs Hidden Markov Random Fields(HMRFs) as a probabilistic generative model forsemi-supervised clustering, providing a principledframework for incorporating constraint-based su-pervision into prototype-based clustering.Another possible approach is to directly embedthe prior-knowledge into data representations.
Themain idea is to employ effective and efficient algo-rithms for constructing nonlinear low-dimensionalmanifolds from sample data points embedded8in high-dimensional spaces.
Several algorithmsare defined, including Isometric feature mapping(ISOMAP) (Tenenbaum et al, 2000), Locally Lin-ear Embedding (LLE) (Roweis and Saul, 2000),Local Tangent Space alignment (LTSA) (Zhangand Zha, 2004) and Locality Preserving Projec-tion (LPP) (He and Niyogi, 2003) and they havebeen successfully applied in several computer vi-sion and pattern recognition problems.
In (Yanget al, 2006) it is demonstrated that basic nonlineardimensionality reduction algorithms, such as LLE,ISOMAP, and LTSA, can be modified by takinginto account prior information on exact mappingof certain data points.
The sensitivity analysisof these algorithms shows that prior informationimproves stability of the solution.
In (Goldbergand Elhadad, 2009), a strategy to incorporate lexi-cal features into classification models is proposed.Another possible approach is the strategy pursuedin recent works on deep learning techniques toNLP tasks.
In (Collobert and Weston, 2008) aunified architecture for NLP that learns featuresrelevant to the tasks at hand given very limitedprior knowledge is presented.
It embodies theidea that a multitask learning architecture coupledwith semi-supervised learning can be effectivelyapplied even to complex linguistic tasks such asSemantic Role Labeling.
In particular, (Collobertand Weston, 2008) proposes an embedding of lex-ical information using Wikipedia as source, andexploits the resulting language model for the mul-titask learning process.
The extensive use of unla-beled texts allows to achieve a significant level oflexical generalization in order to better capitalizeon the smaller annotated data sets.3 Geometrical Embeddings as models ofFrame SemanticsThe aim of this distributional approach is to modelframes in semantic spaces where words are repre-sented from the distributional analysis of their co-occurrences over a corpus.
Semantic spaces arewidely used in NLP for representing the meaningof words or other lexical entities.
They have beensuccessfully applied in several tasks, such as in-formation retrieval (Salton et al, 1975) and har-vesting thesauri (Lin, 1998).
The fundamental in-tuition is that the meaning of a word can be de-scribed by the set of textual contexts in which itappears (Distributional Hypothesis as described in(Harris, 1964)), and that words with similar vec-tors are semantically related.
Contexts are wordsappearing together with a LU: such a space mod-els a generic notion of semantic relatedness, i.e.two LUs spatially close in the space are likely tobe either in paradigmatic or syntagmatic relationas in (Sahlgren, 2006).
Here, LUs delimit sub-spaces modeling the prototypical semantic of thecorresponding evoked frames and novel LUs canbe induced by exploiting their projections.Since a semantic space supports the languagein use from the corpus statistics in an unsuper-vised fashion, vectors representing LUs can becharacterized by different distributions.
For exam-ple, LUs of the frame KILLING, such as blood-bath, crucify or fratricide, are statistically infe-rior in a corpus if compared to a wide-spanningterm as kill.
Moreover other ambiguous LUs, asliquidate or terminate, could appear in sentencesevoking different frames.
These problems of data-sparseness and distribution noise can be over-come by applying space transformation techniquesaugmenting the space expressiveness in model-ing frame semantics.
Semantic space models veryelegantly map words in vector spaces (there areas many dimensions as words in the dictionary)and LUs collections into distributions of data-points.
Every distribution implicitly expresses twoorthogonal facets: global properties, as the occur-rence scores computed for terms across the entirecollection (irrespectively from their word sensesor evoking situation) and local regularities, for ex-ample the existence of subsets of terms that tend tobe used every time a frame manifests.
These alsotend to be closer in the space and should be closerin the transformed space too.
Another importantaspect that a transformation could account is exter-nal semantic information.
In the new space, priorknowledge can be exploited to gather a more regu-lar LUs representation and a clearer separation be-tween subspaces representing different frame se-mantics.In the following sections the investigated dis-tributional model of LUs will be discussed.
Asmany criteria can be adopted to define a LU con-text, one of the goals of this investigation is to finda co-occurrence model that better captures the no-tion of frames, as described in Section 3.1.
Then,two dimensionality reduction techniques, exploit-ing semantic space distributions to improve framesrepresentation, are discussed.
In Section 3.2 therole of global properties of data statistics will be9investigated through the Latent Semantic Analy-sis while in Section 3.3 the Locality PreservingProjection algorithm will be discussed in order tocombine prior knowledge about frames with localregularities of LUs obtained from text.3.1 Choosing the spaceDifferent types of context define spaces with dif-ferent semantic properties.
Such spaces model ageneric notion of semantic relatedness.
Two LUsclose in the space are likely to be related by sometype of generic semantic relation, either paradig-matic (e.g.
synonymy, hyperonymy, antonymy)or syntagmatic (e.g.
meronymy, conceptual andphrasal association), as observed in (Sahlgren,2006).
The target of this work is the construc-tion of a space able to capture the properties whichcharacterize a frame, assuming those LUs in thesame frame tend to be either co-occurring or sub-stitutional words (e.g.
murder/kill).
Two tradi-tional word-based co-occurrence models capturethe above property:Word-based space: Contexts are words, aslemmas, appearing in a n-window of the LU.The window width n is a parameter that allowsthe space to capture different aspects of a frame:higher values risk to introduce noise, since a framecould not cover an entire sentence, while lowervalues lead to sparse representations.Syntax-based space: Contexts words are en-riched through information about syntactic rela-tions (e.g.
X-VSubj-killer where X is the LU), asdescribed in (Pado and Lapata, 2007).
Two LUsclose in the space are likely to be in a paradig-matic relation, i.e.
to be close in an IS-A hierarchy(Budanitsky and Hirst, 2006; Lin, 1998).
Indeed,as contexts are syntactic relations, targets with thesame part of speech are much closer than targetsof different types.3.2 Latent Semantic AnalysisLatent Semantic Analysis (LSA) is an algorithmpresented in (Furnas et al, 1988) afterwards dif-fused by Landauer (Landauer and Dumais, 1997):it can be seen as a variant of the Principal Compo-nent Analysis idea.
LSA aims to find the best sub-space approximation to the original word space,in the sense of minimizing the global reconstruc-tion error projecting data along the directions ofmaximal variance.
It captures term (semantic)dependencies by applying a matrix decomposi-tion process called Singular Value Decomposition(SVD).
The original term-by-term matrix M istransformed into the product of three new matri-ces: U , S, and V so that M = USV T .
MatrixM is approximated by Ml = UlSlV Tl in whichonly the first l columns of U and V are used, andonly the first l greatest singular values are consid-ered.
This approximation supplies a way to projectterm vectors into the l-dimensional space usingYterms = UlS1/2l .
Notice that the SVD processaccounts for the eigenvectors of the entire originaldistribution (matrix M ).
LSA is thus an exampleof a decomposition process strongly dependent ona global property.
The original statistical informa-tion aboutM is captured by the new l-dimensionalspace which preserves the global structure whileremoving low-variant dimensions, i.e.
distribu-tion noise.
These newly derived features may bethought of as artificial concepts, each one repre-senting an emerging meaning component as a lin-ear combination of many different words (i.e.
con-texts).
Such contextual usages can be used insteadof the words to represent texts.
This technique hastwo main advantages.
First, the overall computa-tional cost of the model is reduced, as similaritiesare computed on a space with much fewer dimen-sions.
Secondly, it allows to capture second-orderrelations among LUs, thus improving the qualityof the similarity measure.3.3 The Locality Preserving ProjectionMethodAn alternative to LSA, much tighter to local prop-erties of data, is the Locality Preserving Projection(LPP ), a linear approximation of the non-linearLaplacian Eigenmap algorithm introduced in (Heand Niyogi, 2003).
LPP is a linear dimensional-ity reduction method whose goal is, given a set ofLUs x1, x2, .., xm in Rn, to find a transformationmatrix A that maps these m points into a set ofpoints y1, y2, .., ym in Rk (k  n).
LPP achievesthis result through a cascade of processing stepsdescribed hereafter.Construction of an Adjacency graph.
Let Gdenote a graph with m nodes.
Nodes i and j havegot a weighted connection if vectors xi and xj areclose, according to an arbitrary measure of simi-larity.
There are many ways to build an adjacencygraph.
The cosine graph with cosine weightingscheme is explored: given two vectors xi and xj ,the weight wij between them is set bywij = max{0,cos(xi, xj)?
?|cos(xi, xj)?
?
|?
cos(xi, xj)} (1)10where a cosine threshold ?
is necessary.
The ad-jacency graph can be represented by using a sym-metricm?m adjacency matrix, namedW , whoseelement Wij contains the weight between nodes iand j.
The method of constructing an adjacencygraph outlined above is correct if the data actuallylie on a low dimensional manifold.
Once such anadjacency graph is obtained, LPP will try to opti-mally preserve it in choosing projections.Solve an Eigenmap problem.
Compute theeigenvectors and eigenvalues for the generalizedeigenvector problem:XLXT a = ?XDXT awhere X is a n?m matrix whose columns are theoriginal m vectors in Rn, D is a diagonal m ?mmatrix whose entries are column (or row) sums ofW , Dii =?jWij and L = D ?W is the Lapla-cian matrix.
The solution of this problem is theset of eigenvectors a0, a1, .., an?1, ordered accord-ing to their eigenvalues ?0 < ?1 < .. < ?n?1.LPP projection matrix A is obtained by selectingthe k eigenvectors corresponding to the k smallesteigenvalues: therefore it is a n ?
k matrix whosecolumns are the selected n-dimensional k eigen-vectors.
Final projection of original vectors intoRk can be linearly performed by Y = ATX .
Thistransformation provides a valid kernel that can beefficiently embedded into a classifier.Embedding predicate knowledge throughLPPs.
While LSA finds a projection, according tothe global properties of the space, LPP tries to pre-serve the local structures of the data.
LPP exploitsthe adjacency graph in order to represent neigh-borhood information.
It computes a transforma-tion matrix which maps data points into a lower di-mensional subspace.
As the construction of an ad-jacency graph G can be based on any principle, itsdefinition could account on some external infor-mation reflecting prior knowledge available aboutthe task.In this work, prior knowledge about LUs is em-bedded by exploiting their membership to framedictionaries, thus removing from the graph all con-nections between LUs xi and xj that do not evokethe same prototypical situation.
More formallyEquation 1 can be rewritten more formally as:wij = max{0,cos(xi, xj)?
?|cos(xi, xj)?
?
|?
cos(xi, xj) ?
?
(i, j)}where?
(i, j) ={1 iff ?F s.t.
LUi ?
F ?
LUj ?
F0 otherwiseso the resulting manifold keeps close all LUsevoking the same frame.
Since the number of con-nections could introduce too many constraints tothe Eigenmap problem, a threshold is introducedto avoid the space collapse: for each LU, onlythe most-similar c connections are selected.
Theadoption of the proper a priori knowledge aboutthe target task can be thus seen as a promising re-search direction.4 Empirical AnalysisIn this section the empirical evaluation of distribu-tional models applied to the task of inducing LUsis presented.
Different spaces obtained throughthe dimensionality reduction techniques imply dif-ferent kernel functions used to independently traindifferent SVMs.
Our aim is to investigate the im-pact of these kernels in capturing both the framesand LUs?
properties, as well as the effectivenessof their possible combination.The problem of LUs?
induction is here treatedas a multi-classification problem, where each LUis considered as a positive or negative instance of aframe.
We use Support Vector Machines (SVMs),(Joachims, 1999) a maximum-margin classifierthat realizes a linear discriminative model.
In caseof not linearly separable examples, convolutionfunctions ?(?)
can be used in order to transformthe initial feature space into another one, where ahyperplane that separates the data with the widestmargin can be found.
Here new similarity mea-sures, the kernel functions, can be defined throughthe dot-product K(oi, oj) = ??
(oi) ?
?(oj)?
overthe new representation.
In this way, kernel func-tions KLSA and KLPP can be induced throughthe dimensionality reduction techniques ?LSA and?LPP respectively, as described in sections 3.2and 3.3.
Kernel methods are advantageous be-cause the combination of of kernel functions canbe integrated into the SVM as they are still kernels.Consequently, the kernel combination ?KLSA +?KLPP linearly combines the global propertiescaptured by LSA and the locality constraints im-posed by the LPP transformation.
Here, parame-ters ?
and ?
weight the combination of the twokernels.
The evoking frame for a novel LU isthe one whose corresponding SVM has the high-est (possibly negative) margin, according to a one-11train tune test overallmax 107 35 34 176avg 28 8 8 44total 2466 722 723 3911Table 1: Number of LU examples for each data setfrom the 100 framesvs-all scheme.
In order to evaluate the quality ofthe presented models, accuracy is measured as thepercentage of LUs that are correctly re-assigned totheir original (gold-standard) frame.
As the sys-tem can suggest more than one frame, differentaccuracy levels can be obtained.
A LU is cor-rectly assigned if its correct frame (according toFrameNet) belongs to the set of the best b pro-posals by the system (i.e.
the first b scores fromthe underlying SVMs).
Assigning different val-ues to b, we obtained different levels of accuracyas the percentage of LUs that is correctly assignedamong the first b proposals, as shown in Table 3.4.1 Experimental SetupThe adopted gold standard is a subset of theFrameNet database and it consists of the most 100represented frames in term of annotated examplesand LUs.
As the number of example is extremelyunbalanced across frames1, the LUs dictionary ofeach selected frame contains at least 10 LUs.
It isa reasonable amount of information for the SVMstraining and it is still a representative data set, be-ing composed of 3,911 LUs, i.e.
the 55% of theentire dictionary2 of 7,230 evoking words.
Allword spaces are derived from the British NationalCorpus (BNC), which is underlying FrameNet andconsisting of about 100 million words for English.Each selected frame is represented into the BNCby at least 362 annotated sentences, as the lackof a reasonable number of examples hardly pro-duces a good distributional model of LUs.
Eachframe?s list of LUs is split into train (60%), tuning(20%) and test set (20%) and LUs having Part-of-speech different from verb, noun or adjective areremoved.
In Table 1 the number of LUs for eachset, as well as the maximum and the average num-ber per frame, are summarized.Four different approaches for the Word Space1For example the SELF MOTION frame counts 6,248 ex-amples while 119 frames are represented by less than 10 ex-amples2The entire database contains 10,228 LUs and the numberof evoking word is 7,230, without taking in account multipleframe assignments.construction are used.
The first two correspond toa Word-Based space, the last to a Syntax-Based,as described in section 3.1:Window-n (Wn): contextual features correspondto the set of the 20,000 most frequent lemmatizedwords in the BNC.
The association measure be-tween LUs and contexts is the Point-wise Mu-tual Information (PMI).
Valid contexts for LUs arefixed to a n-window.
Hereafter two window widthvalues will be investigated: Window5 (W5) andWindow10 (W10).Sentence (Sent): contextual features are the sameabove, but the valid contexts are extended to theentire sentence length.SyntaxBased (SyntB): contextual features havebeen computed according to the ?dependency-based?
vector space discussed3 in (Pado and La-pata, 2007).
Observable contexts here are made ofsyntactically-typed co-occurrences within depen-dency graphs built from the entire set of BNC sen-tences.
The most frequent 20,000 basic features,i.e.
(syntactic relation,lemma) pairs, have beenemployed as contextual features corresponding toPMI scores.
Syntactic relations are extracted usingthe Minipar parser.Word space models thus focus on the LUs of theselected 100 frames and the dimensionality havebeen reduced by applying LSA and LPP at a newsize of l = 100.
Any prior knowledge informa-tion is provided to the tuning and test sets duringthe LPP transformation: the construction of thereduced feature space takes in account only LUsfrom the train set while remaining predicates arerepresented through the LPP linear projection.
Inthese experiments the cosine threshold ?
and themaximum number of constraints c are estimatedover the tuning set and the best parametrizationsare shown in Table 2.
The adopted implementa-tion of SVM is SVM-Light-TK 4.4.2 ResultsIn these experiments the impact of the lexicalknowledge gathered by different word-spaces isevaluated over the LU induction task.
Moreover,the improvements achieved through LSA and LPPis measured.
SVM classifiers are trained over thesemantic spaces produced through the dimension-3The Minimal context provided by the De-pendency Vectors tool is used.
It is available athttp://www.nlpado.de/?sebastian/dv.html4SVM-Light-TK is available at the urlhttp://disi.unitn.it/?moschitt/Tree-Kernel.htm12?/?
?
c1.0/0.0 .9/.1 .8/.2 .7/.3 .6/.4 .5/.5 .4/.6 .3/.7 .2/.8 .1/.9 0.0/1.0W5 0.668 0.669 0.672 0.673 0.669 0.662 0.649 0.632 0.612 0.570 0.033 0.55 5W10 0.615 0.619 0.618 0.612 0.604 0.597 0.580 0.575 0.565 0.528 0.048 0.65 3Sent 0.557 0.567 0.580 0.584 0.574 0.564 0.561 0.545 0.523 0.496 0.048 0.80 5SyntB 0.654 0.664 0.662 0.652 0.651 0.647 0.649 0.634 0.627 0.592 0.056 0.40 3Table 2: Accuracy at different combination weights of kernel ?KLSA + ?KLPP (specific baseline is0.043)b-1 b-2 b-3 b-4 b-5 b-6 b-7 b-8 b-9 b-10 ?/?W5orig 0,563 0,685 0,733 0,770 0,801 0,835 0,841 0,854 0,868 0,879 -W10orig 0,510 0,634 0,707 0,776 0,810 0,830 0,841 0,857 0,865 0,875 -Sentorig 0,479 0,618 0,680 0,734 0,764 0,793 0,813 0,837 0,845 0,852 -SyntBorig 0,585 0,741 0,803 0,840 0,866 0,874 0,886 0,903 0,907 0,913 -W5LSA+LPP 0.673 0.781 0.831 0.865 0.881 0.891 0.906 0.912 0.926 0.938 0.7/0.3W10LSA+LPP 0.619 0.739 0.786 0.818 0.849 0.865 0.878 0.888 0.901 0.909 0.9/0.1SentLSA+LPP 0.584 0.705 0.766 0.798 0.825 0.835 0.848 0.864 0.876 0.889 0.7/0.3SyntBLSA+LPP 0.664 0.791 0.840 0.864 0.878 0.893 0.901 0.903 0.907 0.911 0.9/0.1Table 3: Accuracy of original word-space models (orig) and semantic space models (LSA+LPP) onbest-k proposed framesality reduction transformations.
Representationsof both semantic spaces are linearly combined as?KLSA + ?KLPP , where kernel weights ?
and?
are estimated over the tuning set.
Both ker-nels are used even without a combination: a ra-tio ?
= 1.0/?
= 0.0 denotes the LSA kernelalone, while ?
= 0.0/?
= 1.0 the LPP kernel.
Ta-ble 2 shows best results, obtained through a RBFkernel.
The Window5 model achieves the high-est accuracy, i.e.
67% of correct classification,where a baseline of 4.3% is estimated assigningLUs to the most likely frame in the training set (i.e.the one containing the highest number of LUs).Wider windows achieve lower classification accu-racy confirming that most of lexical informationtied to a frame is near the LU.
The Syntactic-basedword space does not outperform the accuracy of aword-based space.
The combination of both ker-nels has always provided the best outcome and theLSA space seems to be more accurate and expres-sive respect to the LPP one, as shown in Figure1.
In particular LPP alone is extremely unstable,suggesting that constraints imposed by the priorknowledge are orthogonal with respect to the cor-pus statistics.Further experiments are carried out using theoriginal co-occurrence space models, to assess im-provements due to LSA and LPP kernel.
In thelatter investigation linear kernel achieved best re-sults as confirmed in (Bengio et al, 2005), wherethe sensitivity to the curse of dimensionality ofa large class of modern learning algorithms (e.g.0,400,450,500,550,600,650,701.0/0.00.9/0.10.8/0.20.7/0.30.6/0.40.5/0.50.4/0.60.3/0.70.2/0.80.1/0.9?LSA / ?LPP weightsWindow5Window10SentenceSyntaxBasedFigure 1: Accuracy at different combinationweights of kernel ?KLSA + ?KLPPSVM) based on local kernels (e.g.
RBF) is ar-gued.
As shown in Table 3, the performance dropof original (orig) models against the best kernelcombination of LSA and LPP are significant,i.e.
?
10%, showing how the latent semanticspaces better capture properties of frames, avoid-ing data-sparseness, dimensionality problem andlow-regularities of data-distribution.Moreover, Table 3 shows how the accuracy levellargely increases when more than one frame isconsidered: at a level b = 3, i.e.
the novelLU is correctly classified if one of the originalframes is comprised in the list (of three frames)proposed by the system, accuracy is 0.84 (i.e theSyntaxBased model), while at b = 10 accuracy is13LU (# WNsyns) frame 1 frame 2 frame 3 Correct framesboil.v (5) FOOD FLUIDIC MOTION CONTAINERS CAUSE HARMclap.v (7) SOUNDS MAKE NOISE COMMUNICATION NOISE BODY MOVEMENTcrown.n (12) LEADERSHIP ACCOUTREMENTS PLACING ACCOUTREMENTSOBSERVABLE BODYPARTSschool.n (7) EDUCATION TEACHING BUILDINGS LOCALE BY USEEDUCATION TEACHINGLOCALE BY USEAGGREGATEthreat.n (4) HOSTILE ENCOUNTER IMPACT COMMITMENT COMMITMENTtragedy.n (2) TEXT KILLING EMOTION DIRECTED TEXTTable 4: Proposed 3 frames for each LU (ordered by SVM scores) and correct frames provided by theFrameNet dictionary.
In parenthesis the number of different WordNet lexical senses for each LU.nearly 0.94 (i.e Window5).
It is high enough tosupport tasks such as the semi-automatic creationof new FrameNets.
An error analysis indicates thatmany misclassifications are induced by a lack inthe frame annotations, especially those concern-ing polysemic LUs5.
Table 4 reports the analysisof a LU subset where the first 3 frames proposedfor each evoking word are shown, ranked by themargin of the SMVs.
The last column contains theframes evoked by LUs, according to the FrameNetdictionary, and the frame names in bold suggesttheir correct classification.
Some LUs, like threat(characterized by 4 lexical senses) seem to be mis-classified: in this case the FrameNet annotationregards a specific sense that evokes the COMMIT-MENT frame (e.g.
?There was a real threat thatshe might have to resign?)
without taking in ac-count other senses like WordNet?s ?menace, threat(something that is a source of danger)?
that couldevoke the HOSTILE ENCOUNTER frame.
In othercases proposed frames seem to enrich the LUs dic-tionary, like BUILDINGS, here evoked by school.5 ConclusionsThe core purpose of this was to present an em-pirical investigation of the impact of different dis-tributional models on the lexical unit inductiontask.
The employed word-spaces, based on dif-ferent co-occurrence models (either context andsyntax-driven), are used as vector models of theLU semantics.
On these spaces, two dimensional-ity reduction techniques have been applied.
LatentSemantic Analysis (LSA) exploits global proper-ties of data distributions and results in a globalmodel for lexical semantics.
On the other hand,the Locality Preserving Projection (LPP) method,that exploits regularities in the neighborhood of5According to WordNet, in our dataset an average of 3.6lexical senses for each LU is estimated.each lexical predicate, is also employed in a semi-supervised manner: local constraints expressingprior knowledge on frames are defined in the ad-jacency graph.
The resulting embedding is there-fore expected to determine a new space where re-gions for LU of a given frame can be more eas-ily discovered.
Experiments have been run usingthe resulting spaces for task dependent kernels ina SVM learning setting.
The application of theFrameNet KB on the 100 best represented framesshowed that a combined use of the global and lo-cal models made available by LSA and LPP, re-spectively, achieves the best results, as the 67.3%of LUs recovers the same frames of the annotateddictionary.
This is a significant improvement withrespect to previous results achieved by the puredistributional model reported in (Pennacchiotti etal., 2008).Future work is required to increase the levelof constraints made available from the semi-supervised setting of LPP: syntactic informa-tion, as well as role-related evidence, can beboth accommodated by the adjacency constraintsimposed for LPP.
This constitutes a significantarea of research towards a comprehensive semi-supervised model of frame semantics, entirelybased on manifold learning methods, of which thisstudy on LSA and LPP is just a starting point.Acknowledgement We want to acknowledgeProf.
Roberto Basili because this work would notexist without his ideas, inspiration and invaluablesupport.ReferencesCollin Baker, Michael Ellsworth, and Katrin Erk.2007.
Semeval-2007 task 19: Frame semantic struc-ture extraction.
In Proceedings of SemEval-2007,14pages 99?104, Prague, Czech Republic, June.
Asso-ciation for Computational Linguistics.Sugato Basu, Mikhail Bilenko, Arindam Banerjee,and Raymond Mooney.
2006.
Probabilistic semi-supervised clustering with constraints.
In Semi-Supervised Learning, pages 73?102.
MIT Press.Yoshua Bengio, Olivier Delalleau, and Nicolas LeRoux.
2005.
The curse of dimensionality for lo-cal kernel machines.
Technical report, Departementd?Informatique et Recherche Operationnelle.Alexander Budanitsky and Graeme Hirst.
2006.
Eval-uating WordNet-based measures of semantic dis-tance.
Computational Linguistics, 32(1):13?47.Aljoscha Burchardt, Katrin Erk, and Anette Frank.2005.
A WordNet Detour to FrameNet.
InSprachtechnologie, mobile Kommunikation und lin-guistische Resourcen, volume 8 of Computer Stud-ies in Language and Speech.
Peter Lang, Frank-furt/Main.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: deepneural networks with multitask learning.
In In Pro-ceedings of ICML ?08, pages 160?167, New York,NY, USA.
ACM.Diego De Cao, Danilo Croce, Marco Pennacchiotti,and Roberto Basili.
2008.
Combining word senseand usage for modeling frame semantics.
In In Pro-ceedings of STEP 2008, Venice, Italy.Charles J. Fillmore.
1985.
Frames and the semantics ofunderstanding.
Quaderni di Semantica, 4(2):222?254.G.
W. Furnas, S. Deerwester, S. T. Dumais, T. K. Lan-dauer, R. A. Harshman, L. A. Streeter, and K. E.Lochbaum.
1988.
Information retrieval using a sin-gular value decomposition model of latent semanticstructure.
In Proc.
of SIGIR ?88, New York, USA.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Yoav Goldberg and Michael Elhadad.
2009.
On therole of lexical features in sequence labeling.
In InProceedings of EMNLP ?09, pages 1142?1151, Sin-gapore.Zellig Harris.
1964.
Distributional structure.
In Jer-rold J. Katz and Jerry A. Fodor, editors, The Philos-ophy of Linguistics, New York.
Oxford UniversityPress.Xiaofei He and Partha Niyogi.
2003.
Locality preserv-ing projections.
In Proceedings of NIPS03, Vancou-ver, Canada.T.
Joachims.
1999.
Making large-Scale SVM LearningPractical.
MIT Press, Cambridge, MA.Richard Johansson and Pierre Nugues.
2007.
UsingWordNet to extend FrameNet coverage.
In Proceed-ings of the Workshop on Building Frame-semanticResources for Scandinavian and Baltic Languages,at NODALIDA, Tartu, Estonia, May 24.Richard Johansson and Pierre Nugues.
2008.
Theeffect of syntactic representation on semantic rolelabeling.
In Proceedings of COLING, Manchester,UK, August 18-22.Tom Landauer and Sue Dumais.
1997.
A solution toplato?s problem: The latent semantic analysis the-ory of acquisition, induction and representation ofknowledge.
Psychological Review, 104.Dekang Lin.
1998.
Automatic retrieval and clusteringof similar word.
In Proceedings of COLING-ACL,Montreal, Canada.Alessandro Moschitti, Paul Morarescu, and Sanda M.Harabagiu.
2003.
Open domain information ex-traction via automatic semantic labeling.
In FLAIRSConference, pages 397?401.Sebastian Pado and Katrin Erk.
2005.
To cause ornot to cause: Cross-lingual semantic matching forparaphrase modelling.
In Proceedings of the Cross-Language Knowledge Induction Workshop, Cluj-Napoca, Romania.Sebastian Pado and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Marco Pennacchiotti, Diego De Cao, Roberto Basili,Danilo Croce, and Michael Roth.
2008.
Automaticinduction of framenet lexical units.
In Proceedingsof The Empirical Methods in Natural Language Pro-cessing (EMNLP 2008) Waikiki, Honolulu, Hawaii.S.T.
Roweis and L.K.
Saul.
2000.
Nonlinear dimen-sionality reduction by locally linear embedding.
Sci-ence, 290(5500):2323?2326.Magnus Sahlgren.
2006.
The Word-Space Model.Ph.D.
thesis, Stockholm University.G.
Salton, A. Wong, and C. Yang.
1975.
A vectorspace model for automatic indexing.
Communica-tions of the ACM, 18:613A?
?`620.Dan Shen and Mirella Lapata.
2007.
Using semanticroles to improve question answering.
In Proceed-ings of EMNLP-CoNLL, pages 12?21, Prague.Mihai Surdeanu, , Mihai Surdeanu, A Harabagiu, JohnWilliams, and Paul Aarseth.
2003.
Using predicate-argument structures for information extraction.
InIn Proceedings of ACL 2003.Marta Tatu and Dan I. Moldovan.
2005.
A seman-tic approach to recognizing textual entailment.
InHLT/EMNLP.15J.
B. Tenenbaum, V. Silva, and J. C. Langford.
2000.A Global Geometric Framework for Nonlinear Di-mensionality Reduction.
Science, 290(5500):2319?2323.Xin Yang, Haoying Fu, Hongyuan Zha, and Jesse Bar-low.
2006.
Semi-supervised nonlinear dimension-ality reduction.
In 23rd International Conferenceon Machine learning, pages 1065?1072, New York,NY, USA.
ACM Press.Zhenyue Zhang and Hongyuan Zha.
2004.
Princi-pal manifolds and nonlinear dimensionality reduc-tion via tangent space alignment.
SIAM J. ScientificComputing, 26(1):313?338.16
