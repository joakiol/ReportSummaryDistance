Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 28?34,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsStanford?s Multi-Pass Sieve Coreference Resolution System at theCoNLL-2011 Shared TaskHeeyoung Lee, Yves Peirsman, Angel Chang, Nathanael Chambers,Mihai Surdeanu, Dan JurafskyStanford NLP GroupStanford University, Stanford, CA 94305{heeyoung,peirsman,angelx,natec,mihais,jurafsky}@stanford.eduAbstractThis paper details the coreference resolutionsystem submitted by Stanford at the CoNLL-2011 shared task.
Our system is a collectionof deterministic coreference resolution mod-els that incorporate lexical, syntactic, seman-tic, and discourse information.
All these mod-els use global document-level information bysharing mention attributes, such as gender andnumber, across mentions in the same cluster.We participated in both the open and closedtracks and submitted results using both pre-dicted and gold mentions.
Our system wasranked first in both tracks, with a score of 57.8in the closed track and 58.3 in the open track.1 IntroductionThis paper describes the coreference resolution sys-tem used by Stanford at the CoNLL-2011 sharedtask (Pradhan et al, 2011).
Our system extendsthe multi-pass sieve system of Raghunathan etal.
(2010), which applies tiers of deterministic coref-erence models one at a time from highest to lowestprecision.
Each tier builds on the entity clusters con-structed by previous models in the sieve, guarantee-ing that stronger features are given precedence overweaker ones.
Furthermore, this model propagatesglobal information by sharing attributes (e.g., genderand number) across mentions in the same cluster.We made three considerable extensions to theRaghunathan et al (2010) model.
First, we addedfive additional sieves, the majority of which addressthe semantic similarity between mentions, e.g., us-ing WordNet distance, and shallow discourse under-standing, e.g., linking speakers to compatible pro-nouns.
Second, we incorporated a mention detectionsieve at the beginning of the processing flow.
Thissieve filters our syntactic constituents unlikely to bementions using a simple set of rules on top of thesyntactic analysis of text.
And lastly, we added apost-processing step, which guarantees that the out-put of our system is compatible with the shared taskand OntoNotes specifications (Hovy et al, 2006;Pradhan et al, 2007).Using this system, we participated in both theclosed1 and open2 tracks, using both predicted andgold mentions.
Using predicted mentions, our sys-tem had an overall score of 57.8 in the closed trackand 58.3 in the open track.
These were the top scoresin both tracks.
Using gold mentions, our systemscored 60.7 in the closed track in 61.4 in the opentrack.We describe the architecture of our entire systemin Section 2.
In Section 3 we show the results of sev-eral experiments, which compare the impact of thevarious features in our system, and analyze the per-formance drop as we switch from gold mentions andannotations (named entity mentions and parse trees)to predicted information.
We also report in this sec-tion our official results in the testing partition.1Only the provided data can be used, i.e., WordNet and gen-der gazetteer.2Any external knowledge source can be used.
We usedadditional animacy, gender, demonym, and country and statesgazetteers.282 System ArchitectureOur system consists of three main stages: mentiondetection, followed by coreference resolution, andfinally, post-processing.
In the first stage, mentionsare extracted and relevant information about men-tions, e.g., gender and number, is prepared for thenext step.
The second stage implements the ac-tual coreference resolution of the identified men-tions.
Sieves in this stage are sorted from highestto lowest precision.
For example, the first sieve (i.e.,highest precision) requires an exact string match be-tween a mention and its antecedent, whereas thelast one (i.e., lowest precision) implements pronom-inal coreference resolution.
Post-processing is per-formed to adjust our output to the task specific con-straints, e.g., removing singletons.It is important to note that the first system stage,i.e., the mention detection sieve, favors recall heav-ily, whereas the second stage, which includes the ac-tual coreference resolution sieves, is precision ori-ented.
Our results show that this design lead tostate-of-the-art performance despite the simplicityof the individual components.
This strategy hasbeen successfully used before for information ex-traction, e.g., in the BioNLP 2009 event extractionshared task (Kim et al, 2009), several of the top sys-tems had a first high-recall component to identifyevent anchors, followed by high-precision classi-fiers, which identified event arguments and removedunlikely event candidates (Bjo?rne et al, 2009).
Inthe coreference resolution space, several works haveshown that applying a list of rules from highest tolowest precision is beneficial for coreference reso-lution (Baldwin, 1997; Raghunathan el al., 2010).However, we believe we are the first to show that thishigh-recall/high-precision strategy yields competi-tive results for the complete task of coreference res-olution, i.e., including mention detection and bothnominal and pronominal coreference.2.1 Mention Detection SieveIn our particular setup, the recall of the mention de-tection component is more important than its preci-sion, because any missed mentions are guaranteedto affect the final score, but spurious mentions maynot impact the overall score if they are left as sin-gletons, which are discarded by our post-processingstep.
Therefore, our mention detection algorithm fo-cuses on attaining high recall rather than high preci-sion.
We achieve our goal based on the list of sievessorted by recall (from highest to lowest).
Each sieveuses syntactic parse trees, identified named entitymentions, and a few manually written patterns basedon heuristics and OntoNotes specifications (Hovy etal., 2006; Pradhan et al, 2007).
In the first andhighest recall sieve, we mark all noun phrase (NP),possessive pronoun, and named entity mentions ineach sentence as candidate mentions.
In the follow-ing sieves, we remove from this set al mentions thatmatch any of the exclusion rules below:1.
We remove a mention if a larger mention withthe same head word exists, e.g., we remove Thefive insurance companies in The five insurancecompanies approved to be established this time.2.
We discard numeric entities such as percents,money, cardinals, and quantities, e.g., 9%,$10, 000, Tens of thousands, 100 miles.3.
We remove mentions with partitive or quanti-fier expressions, e.g., a total of 177 projects.4.
We remove pleonastic it pronouns, detected us-ing a set of known expressions, e.g., It is possi-ble that.5.
We discard adjectival forms of nations, e.g.,American.6.
We remove stop words in a predetermined listof 8 words, e.g., there, ltd., hmm.Note that the above rules extract both mentions inappositive and copulative relations, e.g., [[YongkangZhou] , the general manager] or [Mr. Savoca] hadbeen [a consultant.
.
.
].
These relations are not an-notated in the OntoNotes corpus, e.g., in the text[[Yongkang Zhou] , the general manager], only thelarger mention is annotated.
However, appositiveand copulative relations provide useful (and highlyprecise) information to our coreference sieves.
Forthis reason, we keep these mentions as candidates,and remove them later during post-processing.2.2 Mention ProcessingOnce mentions are extracted, we sort them by sen-tence number, and left-to-right breadth-first traversal29order in syntactic trees in the same sentence (Hobbs,1977).
We select for resolution only the first men-tions in each cluster,3 for two reasons: (a) the firstmention tends to be better defined (Fox, 1993),which provides a richer environment for feature ex-traction; and (b) it has fewer antecedent candidates,which means fewer opportunities to make a mis-take.
For example, given the following ordered listof mentions, {m11, m22, m23, m34, m15, m26}, wherethe subscript indicates textual order and the super-script indicates cluster id, our model will attemptto resolve only m22 and m34.
Furthermore, we dis-card first mentions that start with indefinite pronouns(e.g., some, other) or indefinite articles (e.g., a, an)if they have no antecedents that have the exact samestring extents.For each selected mention mi, all previous men-tions mi?1, .
.
.
, m1 become antecedent candidates.All sieves traverse the candidate list until they finda coreferent antecedent according to their criteriaor reach the end of the list.
Crucially, when com-paring two mentions, our approach uses informa-tion from the entire clusters that contain these men-tions instead of using just information local to thecorresponding mentions.
Specifically, mentions ina cluster share their attributes (e.g., number, gen-der, animacy) between them so coreference decisionare better informed.
For example, if a cluster con-tains two mentions: a group of students, which issingular, and five students, which is plural,the number attribute of the entire cluster becomessingular or plural, which allows it to matchother mentions that are both singular and plural.Please see (Raghunathan et al, 2010) for more de-tails.2.3 Coreference Resolution Sieves2.3.1 Core SystemThe core of our coreference resolution system isan incremental extension of the system described inRaghunathan et al (2010).
Our core model includestwo new sieves that address nominal mentions andare inserted based on their precision in a held-outcorpus (see Table 1 for the complete list of sievesdeployed in our system).
Since these two sieves use3We initialize the clusters as singletons and grow them pro-gressively in each sieve.Ordered sieves1.
Mention Detection Sieve2.
Discourse Processing Sieve3.
Exact String Match Sieve4.
Relaxed String Match Sieve5.
Precise Constructs Sieve (e.g., appositives)6-8.
Strict Head Matching Sieves A-C9.
Proper Head Word Match Sieve10.
Alias Sieve11.
Relaxed Head Matching Sieve12.
Lexical Chain Sieve13.
Pronouns SieveTable 1: The sieves in our system; sieves new to this pa-per are in bold.simple lexical constraints without semantic informa-tion, we consider them part of the baseline model.Relaxed String Match: This sieve considers twonominal mentions as coreferent if the strings ob-tained by dropping the text following their headwords are identical, e.g., [Clinton] and [Clinton,whose term ends in January].Proper Head Word Match: This sieve marks twomentions headed by proper nouns as coreferent ifthey have the same head word and satisfy the fol-lowing constraints:Not i-within-i - same as Raghunathan et al (2010).No location mismatches - the modifiers of two men-tions cannot contain different location named entities,other proper nouns, or spatial modifiers.
For example,[Lebanon] and [southern Lebanon] are not coreferent.No numeric mismatches - the second mention cannothave a number that does not appear in the antecedent, e.g.,[people] and [around 200 people] are not coreferent.In addition to the above, a few more rules areadded to get better performance for predicted men-tions.Pronoun distance - sentence distance between a pronounand its antecedent cannot be larger than 3.Bare plurals - bare plurals are generic and cannot have acoreferent antecedent.2.3.2 Semantic-Similarity SievesWe first extend the above system with twonew sieves that exploit semantics from WordNet,Wikipedia infoboxes, and Freebase records, drawingon previous coreference work using these databases(Ng & Cardie, 2002; Daume?
& Marcu, 2005;Ponzetto & Strube, 2006; Ng, 2007; Yang & Su,302007; Bengston & Roth, 2008; Huang et al, 2009;inter alia).
Since the input to a sieve is a collection ofmention clusters built by the previous (more precise)sieves, we need to link mention clusters (rather thanindividual mentions) to records in these three knowl-edge bases.
The following steps generate a query forthese resources from a mention cluster.First, we select the most representative mentionin a cluster by preferring mentions headed by propernouns to mentions headed by common nouns, andnominal mentions to pronominal ones.
In case ofties, we select the longer string.
For example, themention selected from the cluster {President GeorgeW.
Bush, president, he} is President George W.Bush.
Second, if this mention returns nothing fromthe knowledge bases, we implement the followingquery relaxation algorithm: (a) remove the text fol-lowing the mention head word; (b) select the lowestnoun phrase (NP) in the parse tree that includes themention head word; (c) use the longest proper noun(NNP*) sequence that ends with the head word; (d)select the head word.
For example, the query pres-ident Bill Clinton, whose term ends in January issuccessively changed to president Bill Clinton, thenBill Clinton, and finally Clinton.
If multiple recordsare returned, we keep the top two for Wikipedia andFreebase, and all synsets for WordNet.Alias SieveThis sieve addresses name aliases, which are de-tected as follows.
Two mentions headed by propernouns are marked as aliases (and stored in the sameentity cluster) if they appear in the same Wikipediainfobox or Freebase record in either the ?name?
or?alias?
field, or they appear in the same synset inWordNet.
As an example, this sieve correctly de-tects America Online and AOL as aliases.
We alsotested the utility of Wikipedia categories, but foundlittle gain over morpho-syntactic features.Lexical Chain SieveThis sieve marks two nominal mentions as coref-erent if they are linked by a WordNet lexical chainthat traverses hypernymy or synonymy relations.
Weuse all synsets for each mention, but restrict it tomentions that are at most three sentences apart, andlexical chains of length at most four.
This sieve cor-rectly links Britain with country, and plane with air-craft.To increase the precision of the above two sieves,we use additional constraints before two mentionscan match: attribute agreement (number, gender, an-imacy, named entity labels), no i-within-i, no loca-tion or numeric mismatches (as in Section 2.3.1),and we do not use the abstract entity synset in Word-Net, except in chains that include ?organization?.2.3.3 Discourse Processing SieveThis sieve matches speakers to compatible pro-nouns, using shallow discourse understanding tohandle quotations and conversation transcripts.
Al-though more complex discourse constraints havebeen proposed, it has been difficult to show improve-ments (Tetreault & Allen, 2003; 2004).We begin by identifying speakers within text.
Innon-conversational text, we use a simple heuristicthat searches for the subjects of reporting verbs (e.g.,say) in the same sentence or neighboring sentencesto a quotation.
In conversational text, speaker infor-mation is provided in the dataset.The extracted speakers then allow us to imple-ment the following sieve heuristics:?
?I?s4 assigned to the same speaker are coreferent.?
?you?s with the same speaker are coreferent.?
The speaker and ?I?s in her text are coreferent.For example, I, my, and she in the following sen-tence are coreferent: ?
[I] voted for [Nader] because[he] was most aligned with [my] values,?
[she] said.In addition to the above sieve, we impose speakerconstraints on decisions made by subsequent sieves:?
The speaker and a mention which is not ?I?
in thespeaker?s utterance cannot be coreferent.?
Two ?I?s (or two ?you?s, or two ?we?s) assigned todifferent speakers cannot be coreferent.?
Two different person pronouns by the same speakercannot be coreferent.?
Nominal mentions cannot be coreferent with ?I?,?you?, or ?we?
in the same turn or quotation.?
In conversations, ?you?
can corefer only with theprevious speaker.For example, [my] and [he] are not coreferent in theabove example (third constraint).4We define ?I?
as ?I?, ?my?, ?me?, or ?mine?, ?we?
as firstperson plural pronouns, and ?you?
as second person pronouns.31Annotations Coref R P F1Gold Before 92.8 37.7 53.6Gold After 75.1 70.1 72.6Not gold Before 87.9 35.6 50.7Not gold After 71.7 68.4 70.0Table 2: Performance of the mention detection compo-nent, before and after coreference resolution, with bothgold and actual linguistic annotations.2.4 Post ProcessingTo guarantee that the output of our system matchesthe shared task requirements and the OntoNotesannotation specification, we implement two post-processing steps:?
We discard singleton clusters.?
We discard the mention that appears later intext in appositive and copulative relations.
Forexample, in the text [[Yongkang Zhou] , thegeneral manager] or [Mr. Savoca] had been[a consultant.
.
.
], the mentions Yongkang Zhouand a consultant.
.
.
are removed in this stage.3 Results and DiscussionTable 2 shows the performance of our mention de-tection algorithm.
We show results before and aftercoreference resolution and post-processing (whensingleton mentions are removed).
We also list re-sults with gold and predicted linguistic annotations(i.e., syntactic parses and named entity recognition).The table shows that the recall of our approach is92.8% (if gold annotations are used) or 87.9% (withpredicted annotations).
In both cases, precision islow because our algorithm generates many spuriousmentions due to its local nature.
However, as the ta-ble indicates, many of these mentions are removedduring post-processing, because they are assignedto singleton clusters during coreference resolution.The two main causes for our recall errors are lackof recognition of event mentions (e.g., verbal men-tions such as growing) and parsing errors.
Parsingerrors often introduce incorrect mention boundaries,which yield both recall and precision errors.
Forexample, our system generates the predicted men-tion, the working meeting of the ?863 Program?
to-day, for the gold mention the working meeting of the?863 Program?.
Due to this boundary mismatch,all mentions found to be coreferent with this pre-dicted mention are counted as precision errors, andall mentions in the same coreference cluster with thegold mention are counted as recall errors.Table 3 lists the results of our end-to-end systemon the development partition.
?External Resources?,which were used only in the open track, includes: (a)a hand-built list of genders of first names that we cre-ated, incorporating frequent names from census listsand other sources, (b) an animacy list (Ji and Lin,2009), (c) a country and state gazetteer, and (d) a de-monym list.
?Discourse?
stands for the sieve intro-duced in Section 2.3.3.
?Semantics?
stands for thesieves presented in Section 2.3.2.
The table showsthat the discourse sieve yields an improvement ofalmost 2 points to the overall score (row 1 versus3), and external resources contribute 0.5 points.
Onthe other hand, the semantic sieves do not help (row3 versus 4).
The latter result contradicts our initialexperiments, where we measured a minor improve-ment when these sieves were enabled and gold men-tions were used.
Our hypothesis is that, when pre-dicted mentions are used, the semantic sieves aremore likely to link spurious mentions to existingclusters, thus introducing precision errors.
This sug-gests that a different tuning of the sieve parametersis required for the predicted mention scenario.
Forthis reason, we did not use the semantic sieves forour submission.
Hence, rows 2 and 3 in the tableshow the performance of our official submission inthe development set, in the closed and open tracksrespectively.The last three rows in Table 3 give insight on theimpact of gold information.
This analysis indicatesthat using gold linguistic annotation yields an im-provement of only 2 points.
This implies that thequality of current linguistic processors is sufficientfor the task of coreference resolution.
On the otherhand, using gold mentions raises the overall score by15 points.
This clearly indicates that pipeline archi-tectures where mentions are identified first are inad-equate for this task, and that coreference resolutionmight benefit from the joint modeling of mentionsand coreference chains.Finally, Table 4 lists our results on the held-outtesting partition.
Note that in this dataset, the goldmentions included singletons and generic mentions32Components MUC B3 CEAFE BLANCER D S GA GM R P F1 R P F1 R P F1 R P F1 avg F1?58.8 56.5 57.6 68.0 68.7 68.4 44.8 47.1 45.9 68.8 73.5 70.9 57.3?59.1 57.5 58.3 69.2 71.0 70.1 46.5 48.1 47.3 72.2 78.1 74.8 58.6?
?60.1 59.5 59.8 69.5 71.9 70.7 46.5 47.1 46.8 73.8 78.6 76.0 59.1?
?
?60.3 58.5 59.4 69.9 71.1 70.5 45.6 47.3 46.4 73.9 78.2 75.8 58.8?
?
?63.8 61.5 62.7 71.4 72.3 71.9 47.1 49.5 48.3 75.6 79.6 77.5 61.0?
?
?73.6 90.0 81.0 69.8 89.2 78.3 79.4 52.5 63.2 79.1 89.2 83.2 74.2?
?
?
?74.0 90.1 81.3 70.2 89.3 78.6 79.7 53.1 63.7 79.5 89.6 83.6 74.5Table 3: Comparison between various configurations of our system.
ER, D, S stand for External Resources, Discourse,and Semantics sieves.
GA and GM stand for Gold Annotations, and Gold Mentions.
The top part of the table showsresults using only predicted annotations and mentions, whereas the bottom part shows results of experiments with goldinformation.
Avg F1 is the arithmetic mean of MUC, B3, and CEAFE.
We used the development partition for theseexperiments.MUC B3 CEAFE BLANCTrack Gold Mention Boundaries R P F1 R P F1 R P F1 R P F1 avg F1Close Not Gold 61.8 57.5 59.6 68.4 68.2 68.3 43.4 47.8 45.5 70.6 76.2 73.0 57.8Open Not Gold 62.8 59.3 61.0 68.9 69.0 68.9 43.3 46.8 45.0 71.9 76.6 74.0 58.3Close Gold 65.9 62.1 63.9 69.5 70.6 70.0 46.3 50.5 48.3 72.0 78.6 74.8 60.7Open Gold 66.9 63.9 65.4 70.1 71.5 70.8 46.3 49.6 47.9 73.4 79.0 75.8 61.4Table 4: Results on the official test set.as well, whereas in development (lines 6 and 7 in Ta-ble 3), gold mentions included only mentions part ofan actual coreference chain.
This explains the largedifference between, say, line 6 in Table 3 and line 4in Table 4.Our scores are comparable to previously reportedstate-of-the-art results for coreference resolutionwith predicted mentions.
For example, Haghighiand Klein (2010) compare four state-of-the-art sys-tems on three different corpora and report B3 scoresbetween 63 and 77 points.
While the corpora usedin (Haghighi and Klein, 2010) are different from theone in this shared task, our result of 68 B3 suggeststhat our system?s performance is competitive.
In thistask, our submissions in both the open and the closedtrack obtained the highest scores.4 ConclusionIn this work we showed how a competitive end-to-end coreference resolution system can be built usingonly deterministic models (or sieves).
Our approachstarts with a high-recall mention detection compo-nent, which identifies mentions using only syntacticinformation and named entity boundaries, followedby a battery of high-precision deterministic corefer-ence sieves, applied one at a time from highest tolowest precision.
These models incorporate lexical,syntactic, semantic, and discourse information, andhave access to document-level information (i.e., weshare mention attributes across clusters as they arebuilt).
For this shared task, we extended our ex-isting system with new sieves that model shallowdiscourse (i.e., speaker identification) and seman-tics (lexical chains and alias detection).
Our resultsdemonstrate that, despite their simplicity, determin-istic models for coreference resolution obtain com-petitive results, e.g., we obtained the highest scoresin both the closed and open tracks (57.8 and 58.3respectively).
The code used for this shared task ispublicly released.5AcknowledgmentsWe thank the shared task organizers for their effort.This material is based upon work supported bythe Air Force Research Laboratory (AFRL) underprime contract no.
FA8750-09-C-0181.
Any opin-ions, findings, and conclusion or recommendationsexpressed in this material are those of the authorsand do not necessarily reflect the view of the AirForce Research Laboratory (AFRL).5See http://nlp.stanford.edu/software/dcoref.shtml for the standalone coreference resolutionsystem and http://nlp.stanford.edu/software/corenlp.shtml for Stanford?s suite of natural languageprocessing tools, which includes this coreference resolutionsystem.33ReferencesB.
Baldwin.
1997.
CogNIAC: high precision corefer-ence with limited knowledge and linguistic resources.In Proceedings of a Workshop on Operational Factorsin Practical, Robust Anaphora Resolution for Unre-stricted Texts.E.
Bengston & D. Roth.
2008.
Understanding the valueof features for coreference resolution.
In EMNLP.Jari Bjo?rne, Juho Heimonen, Filip Ginter, Antti Airola,Tapio Pahikkala, and Tapio Salakoski.
2009.
Ex-tracting Complex Biological Events with Rich Graph-Based Feature Sets.
Proceedings of the Workshop onBioNLP: Shared Task.H.
Daume?
III and D. Marcu.
2005.
A large-scale ex-ploration of effective global features for a joint entitydetection and tracking model.
In EMNLP-HLT.B.
A.
Fox 1993.
Discourse structure and anaphora:written and conversational English.
Cambridge Uni-versity Press.A.
Haghighi and D. Klein.
2010.
Coreference resolutionin a modular, entity-centered model.
In Proc.
of HLT-NAACL.E.
Hovy, M. Marcus, M. Palmer, L. Ramshaw, and R.Weischedel 2006.
OntoNotes: The 90% Solution.
InHLT/NAACL.Z.
Huang, G. Zeng, W. Xu, and A. Celikyilmaz 2009.Accurate semantic class classifier for coreference res-olution.
In EMNLP.J.R.
Hobbs.
1977.
Resolving pronoun references.
Lin-gua.H.
Ji and D. Lin.
2009.
Gender and animacy knowl-edge discovery from web-scale n-grams for unsuper-vised person mention detection.
In PACLIC.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof the BioNLP?09 Shared Task on Event Extrac-tion.
Proceedings of the NAACL-HLT 2009 Work-shop on Natural Language Processing in Biomedicine(BioNLP?09).V.
Ng 2007.
Semantic Class Induction and CoreferenceResolution.
In ACL.V.
Ng and C. Cardie.
2002.
Improving Machine Learn-ing Approaches to Coreference Resolution.
in ACL2002S.
Ponzetto and M. Strube.
2006.
Exploiting semanticrole labeling, Wordnet and Wikipedia for coreferenceresolution.
Proceedings of NAACL.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted Coreference: Indentifying Entities and Eventsin OntoNotes.
In Proceedings of the IEEE Interna-tional Conference on Semantic Computing (ICSC).Sameer Pradhan, Lance Ramshaw, Mitchell Marcus,Martha Palmer, Ralph Weischedel, and Nianwen Xue.2011.
CoNLL-2011 Shared Task: Modeling Unre-stricted Coreference in OntoNotes.
In Proceedingsof the Fifteenth Conference on Computational NaturalLanguage Learning (CoNLL 2011).K.
Raghunathan, H. Lee, S. Rangarajan, N. Chambers,M.
Surdeanu, D. Jurafsky, and C. Manning 2010.A Multi-Pass Sieve for Coreference Resolution.
InEMNLP.J.
Tetreault and J. Allen.
2003.
An Empirical Evalua-tion of Pronoun Resolution and Clausal Structure.
InProceedings of the 2003 International Symposium onReference Resolution.J.
Tetreault and J. Allen.
2004.
Dialogue Structure andPronoun Resolution.
In DAARC.X.
Yang and J. Su.
2007.
Coreference Resolution Us-ing Semantic Relatedness Information from Automat-ically Discovered Patterns.
In ACL.34
