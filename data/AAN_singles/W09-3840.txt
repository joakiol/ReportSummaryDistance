Proceedings of the 11th International Conference on Parsing Technologies (IWPT), pages 242?253,Paris, October 2009. c?2009 Association for Computational LinguisticsDependency Constraints for Lexical DisambiguationGuillaume BonfanteLORIA INPLguillaume.bonfante@loria.frBruno GuillaumeLORIA INRIAbruno.guillaume@loria.frMathieu MoreyLORIA Nancy-Universite?mathieu.morey@loria.frAbstractWe propose a generic method to per-form lexical disambiguation in lexicalizedgrammatical formalisms.
It relies on de-pendency constraints between words.
Thesoundness of the method is due to invariantproperties of the parsing in a given gram-mar that can be computed statically fromthe grammar.1 IntroductionIn this work, we propose a method of lexical dis-ambiguation based on the notion of dependencies.In modern linguistics, Lucien Tesnie`re devel-oped a formal and sophisticated theory with de-pendencies (Tesnie`re, 1959).
Nowadays, manycurrent grammatical formalisms rely more or lessexplicitly on the notion of dependencies betweenwords.
The most straightforward examples areformalisms in the Dependency Grammars familybut it is also true of the phrase structure based for-malisms which consider that words introduce in-complete syntactic structures which must be com-pleted by other words.
This idea is at the core ofCategorial Grammars (CG) (Lambek, 1958) andall its trends such as Abstract Categorial Gram-mars (ACG) (de Groote, 2001) or CombinatoryCategorial Grammars (CCG) (Steedman, 2000),being mostly encoded in their type system.
De-pendencies in CG were studied in (Moortgat andMorrill, 1991) and for CCG in (Clark et al,2002; Koller and Kuhlmann, 2009).
Other for-malisms can be viewed as modeling and usingdependencies, such as Tree Adjoining Grammars(TAG) (Joshi, 1987) with their substitution and ad-junction operations.
Dependencies for TAG werestudied in (Joshi and Rambow, 2003).
More re-cently, Marchand et al (2009) showed that it isalso possible to extract a dependency structurefrom a syntactic analysis in Interaction Grammars(IG) (Guillaume and Perrier, 2008).Another much more recent concept of polaritycan be used in grammatical formalisms to expressthat words introduce incomplete syntactic struc-tures.
IG directly use polarities to describe thesestructures but it is also possible to use polaritiesin other formalisms in order to make explicit themore or less implicit notion of incomplete struc-tures: for instance, in CG (Lamarche, 2008) or inTAG (Kahane, 2006; Bonfante et al, 2004; Gar-dent and Kow, 2005).
On this regard, Marchandet al (2009) exhibited a direct link between polar-ities and dependencies.
This encourages us to saythat in many respects dependencies and polaritiesare two sides of the same coin.The aim of this paper is to show that dependen-cies can be used to express constraints on the tag-gings of a sentence and hence these dependencyconstraints can be used to partially disambiguatethe words of a sentence.
We will see that, in prac-tice, using the link between dependencies and po-larities, these dependency constraints can be com-puted directly from polarized structures.Exploiting the dependencies encoded in lexicalentries to perform disambiguation is the intuitionbehind supertagging (Bangalore and Joshi, 1999),a method introduced for LTAG and successfullyapplied since then to CCG (Clark and Curran,2004) and HPSG (Ninomiya et al, 2006).
Theseapproaches select the most likely lexical entry (en-tries) for each word, based on Hidden MarkovModels or Maximum Entropy Models.
Like thework done by Boullier (2003), our method is notbased on statistics nor heuristics, but on a neces-sary condition of the deep parsing.
Consequently,we accept to have more than one lexical taggingfor a sentence, as long as we can ensure to havethe good ones (when they exist!).
This property isparticulary useful to ensure that the deep parsingwill not fail because of an error at the disambigua-tion step.In wide-coverage lexicalized grammars, a word242typically has about 10 corresponding lexical de-scriptions, which implies that for a short sentenceof 10 words, we get 1010 possible taggings.
It isnot reasonable to treat them individually.
To avoidthis, it is convenient to use an automaton to rep-resent the set of all paths.
This automaton has lin-ear size with regard to the initial lexical ambiguity.The idea of using automata is not new.
In partic-ular, methods based on Hidden Markov Models(HMM) use such a technique for part-of-speechtagging (Kupiec, 1992; Merialdo, 1994).
Usingautomata, we benefit from dynamic programmingprocedures, and consequently from an exponentialtemporal and space speed up.2 Abstract Grammatical FrameworkOur filtering method is applicable to any lexical-ized grammatical formalism which exhibits somebasic properties.
In this section we establish theseproperties and define from them the notion of Ab-stract Grammatical Framework (AGF).Formally, an Abstract Grammatical Frame-work is an n-tuple (V, S,G,anc,F,p,dep)where:?
V is the vocabulary: a finite set of words ofthe modeled natural language;?
S is the set of syntactic structures used bythe formalism;?
G ?
S is the grammar: the finite set of initialsyntactic structures; a finite list [t1, .
.
.
, tn] ofelements of G is called a lexical tagging;?
anc : G ?
V maps initial syntactic struc-tures to their anchors;?
F ?
S is the set of final syntactic structuresthat the parsing process builds (for instancetrees);?
p is the parsing function from lexical tag-gings to finite subsets of F;?
dep is the dependency function which mapsa couple composed of a lexical tagging and afinal syntactic structure to dependency struc-tures.Note that the anc function implies that thegrammar is lexicalized: each initial structure in Gis associated to an element of V. Note also that noparticular property is required on the dependencystructures that are obtained with the dep function,they can be non-projective for instance.We call lexicon the function (written `) from Vto subsets of G defined by:`(w) = {t ?
G | anc(t) = w}.We will say that a lexical tagging L =[t1, .
.
.
, tn] is a lexical tagging of the sentence[anc(t1), .
.
.
,anc(tn)].The final structures in p (L) ?
F are called theparsing solutions of L.Henceforth, in our examples, we will considerthe ambiguous French sentence (1).
(1) ?La belle ferme la porte?Example 1 We consider the following toy AGF,suited for parsing our sentence:?
V = { ?la?, ?belle?, ?ferme?, ?porte?
};?
the grammar G is given in the table be-low: each ?
corresponds to an element inG, written with the category and the Frenchword as subscript.
For instance, the Frenchword ?porte?
can be either a common noun(?door?)
or a transitive verb (?hangs?
);hence G contains the 2 elements CNporte andTrVporte .la belle ferme porteDet ?LAdj ?
?RAdj ?
?CN ?
?
?
?Clit ?TrV ?
?IntrV ?In our example, categories stand for, respec-tively: determiner, left adjective, right adjec-tive, common noun, clitic pronoun, transitiveverb and intransitive verb.With respect to our lexicon, for sentence (1),there are 3 ?
3 ?
5 ?
3 ?
2 = 270 lexical tag-gings.The parsing function p is such that 3 lexicaltaggings have one solution and the 267 remainingones have no solution; we do not need to precisethe final structures, so we only give the Englishtranslation as the result of the parsing function:243?
p([Detla , CNbelle , TrVferme , Detla , CNporte ]) ={?The nice girl closes the door?}?
p([Detla , LAdjbelle , CNferme , Clitla , TrVporte ]) ={?The nice farm hangs it?}?
p([Detla , CNbelle , RAdjferme , Clitla , TrVporte ]) ={?The firm nice girl hangs it?
}3 The Companionship PrincipleWe have stated in the previous section the frame-work and the definitions required to describe ourprinciple.3.1 Potential CompanionWe say that u ?
G is a companion of t ?
G ifanc(t) and anc(u) are linked by a dependencyin dep(L,m) for some lexical tagging L whichcontains t and u and some m ?
p(L).
The subsetof elements of G that are companions of t is calledthe potential companion set of t.The Companionship Principle says that if a lex-ical tagging contains some t but no potential com-panion of t, then it can be removed.In what follows, we will generalize a bit thisidea in two ways.
First, the same t can be impliedin more than one kind of dependency and hence itcan have several different companion sets with re-spect to the different kinds of dependencies.
Sec-ondly, it can be the case that some companion thas to be on the right (resp.
on the left) to fulfill itsduty.
These generalizations are done through thenotion of atomic constraints defined below.3.2 Atomic constraintsWe say that a pair (L,R) of subsets of G is anatomic constraint for an initial structure t ?
Gif for each lexical tagging L = [t1, .
.
.
, tn] suchthat p(L) 6= ?
and t = ti for some i then:?
either there is some j < i such that tj ?
L,?
or there is some j > i such that tj ?
R.In other words, (L,R) lists the potential com-panions of t, respectively on the left and on theright.A system of constraints for a grammar G is afunction C which associates a finite set of atomicconstraints to each element of G.The Companionship Principle is an immedi-ate consequence of the definition of atomic con-straints.
It can be stated as the necessary condi-tion:The Companionship PrincipleIf a lexical tagging [t1, .
.
.
, tn] has a solutionthen for all i and for all atomic constraints(L,R) ?
C(ti)?
{t1, .
.
.
, ti?1} ?
L 6= ??
or {ti+1, .
.
.
, tn} ?
R 6= ?.Example 2 Often, constraints can be expressedindependently of the anchors.
In our example, weuse the category to refer to the subset of G of struc-tures defined with this category: LAdj for instancerefers to the subset {LAdjbelle , LAdjferme}.We complete the example of the previous sectionwith the following constraints1:?
t ?
CN?
(Det, ?)
?
C(t)?
t ?
LAdj?
(?, CN) ?
C(t)?
t ?
RAdj?
(CN, ?)
?
C(t)?
t ?
Det?
(?, CN) ?
C(t)?
t ?
Det?
(TrV, TrV ?
IntrV) ?
C(t)?
t ?
TrV?
(Clit, Det) ?
C(t)?
t ?
TrV?
(Det, ?)
?
C(t)?
t ?
IntrV?
(Det, ?)
?
C(t)?
t ?
Clit?
(?, TrV) ?
C(t)The two constraints ?
and ?
for instance ex-press that every determiner is implied in two de-pendencies.
First, it must find a common noun onits right to build a noun phrase.
Second, the nounphrase has to be used in a verbal construction.Now, let us consider the lexical tagging:[Detla , LAdjbelle , TrVferme , Clitla , CNporte ] andthe constraint ?
(a clitic is waiting for a transitiveverb on its right).
This constraint is not fulfilledby the tagging so this tagging has no solution.3.3 The ?Companionship Principle?languageActually, a lexical tagging is an element of theformal language G?
and we can consider the fol-lowing three languages.
First, G?
itself.
Second,the set C ?
G?
corresponds to the lexical tag-gings which can be parsed.
The aim of lexicaldisambiguation is then to exhibit for each sen-tence [w1, .
.
.
, wn] all the lexical taggings that arewithin C. Third, the Companionship Principle de-fines the language P of lexical taggings which ver-ify this Principle.
P squeezes between the two lat-1These constraints are relative to our toy grammar and arenot linguistically valid in a larger context.244ter sets C ?
P ?
G?.
Remarkably, the languageP can be described as a regular language.
Since Cis presumably not a regular language (at least fornatural languages!
), P is a better regular approxi-mation than the trivial G?.Let us consider one lexical entry t and an atomicconstraint (L,R) ?
C(t).
Then, the set of lexicaltaggings verifying this constraint can be describedasLt:(L,R) = {(({L)?t({R)?
)where { denotes the complement of a set.Since P is defined as the lexical taggings verify-ing all constraints, P is a regular language definedby :P = ?
(L,R)?C(t)Lt:(L,R)From the Companionship Principle, we derivea lexical disambiguation Principle which simplytests tagging candidates with P .
Notice that P canbe statically computed (at least, in theory) fromthe grammar itself.Example 3 For instance, for our example gram-mar, this automaton is given in the figure 1 wherec=Clit, n=CN, d=Det, i=IntrV, l=LAdj, r=RAdjand t=TrV.A rough approximation of the size of the au-tomaton corresponding to P can be easily com-puted.
Since each automaton Lt:(L,R) has 4 states,P has at most 4m states where m is the num-ber of atomic constraints.
For instance, the gram-mar used in the experiments contains more thanone atomic constraint for each lexical entry, andm > |G| > 106.
Computing P by brute-force isthen intractable.4 Implementation of the CompanionshipPrinciple with automataIn this section we show how to use the Compan-ionship Principle for disambiguation.
Actually, wepropose two implementations based on the princi-ple, an exact one and an approximate one.
Thelatter is really fast and can be used as a first stepbefore applying the first one.4.1 Automaton to represent sets of lexicaltaggingsThe number of lexical taggings grows exponen-tially with the length of sentences.
To avoid that,we represent sets of lexical taggings as the sets ofpaths of some acyclic automata where transitionsare labeled by elements of G .
We call such anautomaton a lexical taggings automaton (LTA).Generally speaking, such automata save a lot ofspace.
For instance, given a sentence [w1, .
.
.
, wn]the number of lexical taggings to consider at thebeginning of the parsing process is ?1?i?n|`(wi)|.This set of taggings can be efficiently representedas the set of paths of the automaton with n + 1states s0, .
.
.
, sn and with a transition from si?1to si with the label t for each t ?
`(wi).
Thisautomaton has?1?i?n |`(wi)| transitions.Example 4 With the data of the previous exam-ples, we have the initial automaton:0 1DetCNClit2LAdjRAdjCN3TrVIntrVLAdjRAdjCN4DetCNClit5CNTrVTo improve readability, only the categories aregiven on the edges, while the French words can beinferred from the position in the automaton.4.2 Exact Companionship Principle (ECP)Suppose we have a LTA A for a sentence[w1, .
.
.
, wn].
For each transition t and for eachatomic constraint in (L,R) ?
C(t), we constructan automaton At,L,R in the following way.Each state s of At,L,R is labeled with a triplecomposed of a state of the automaton A andtwo booleans.
The intended meaning of the firstboolean is to say that each path reaching thisstate passes through the transition t. The secondboolean means that the atomic constraint (L,R) isnecessarily fulfilled.The initial state is labeled (s0, F, F) where s0 isthe initial state of A and other states are labeled asfollows: if s u??
s?
in A then, in At,L,R, we have:1.
(s, F, b) u??
(s?, T, b) if u = t2.
(s, F, b) u??
(s?, F, T) if u ?
L3.
(s, F, b) u??
(s?, F, b) if u /?
L4.
(s, T, b) u??
(s?, T, T) if u ?
R5.
(s, T, b) u??
(s?, T, b) if u /?
Rwhere b ?
{T, F}.245DetCNliLAdjRjTrVtINniLDAdjRjTjrt LLANjirAdjRjTrtLANjirAtjirLeNANjirLdtLCLlRtNiAtjdjRjTrLVANjirANjijTrdRtR N AdjijTrtAdjijRjTjrNDtRANjdjijTjrtRANjdjijTrtRNAdjirtRAtjNjdjirLIRAdjijrLntNdtANjir ARjTrRAtjNjdjijTrRtANjdjijrNAtjdjirRLNAtjdjijRjTrNtAdjijrRRtANjdjirNtAdjijRjTjrFigure 1: The P language for GIt is then routine to show that, for each state la-beled (s, b1, b2):?
b1 is T iff all paths from the initial state to scontain the transition t;?
b2 is T iff for all paths p reaching this state,either there is some u ?
L or p goes throught and there is some u ?
R. In other words,the constraint is fulfilled.In conclusion, a path ending with (sf , T, F) withsf a final state of A is built with transitions 1, 3and 5 only and hence contains t but no transitionable to fulfill the constraint.
The final states are:?
(sf , F, b): each path ending here does notcontain the edge t and thus the constraintdoes not apply here,?
(sf , T, T) each path ending here contains theedge t but it contains also either a transition2 or 4, so the constraint is fulfilled by thesepaths.The size of these automata is easily bounded by4n where n is the size of A.
Using a slightly moreintricated presentation, we built automata of size2n.Example 5 We give below the automaton A forthe atomic constraint ?
(an intransitive verb iswaiting for a determiner on its left):DetetCeteNliLCetetAdAjRLTeteNrVInVInAdTetetrVInVInAdeteNrVInVInAdNeNeNLetetrVInVInAdNeNetLeteNliLAdAjRLetetliLAdAjRLeNeNliLAdAjRLeNetliLAdAjRLeteNAdNetetAdNeNeNAdNeNetAdNThe dotted part of the graph corresponds to thepart of the automaton that can be safely removed.After minimization, we finally obtain:DetCNeliLiAdNjRTrVITrViLjlRTrVITrViLnRTrVITrViLNRTrVITrViLtCNiLiAdN iLThis automaton contains 234 paths (36 lexicaltaggings are removed by this constraint).For each transition t of the lexical taggings au-tomaton and for each constraint (L,R) ?
C(t),we construct the atomic constraint automatonAt,L,R.
The intersection of these automata rep-resents all the possible lexical taggings of the sen-tence which respect the Companionship Principle.246That is, we output :ACP =?1?i?n, t?A;(L,R)?C(t)At,L,RIt can be shown that the automaton is the sameas the one obtained by intersection with the au-tomaton of the language defined in 3.3:ACP = A ?
PExample 6 In our example, the intersection ofthe 9 automata built for the atomic constraints isgiven below:D etCNliLAdljRTrjR TVInTNnjRLAdInTANn jNtCNjRjNjR InjRThis automaton has 8 paths: there are 8 lexicaltaggings which fulfill every constraint.4.3 Approximation: the QuickCompanionship Principle (QCP)The issue with the previous algorithm is that it in-volves a large number of automata (actuallyO(n))where n is the size of the input sentence.
Eachof these automata has size O(n).
The theoreti-cal complexity of the intersection is then O(nn).Sometimes, we face the exponential.
So, letus provide an algorithm which approximates thePrinciple.
The idea is to consider at the same timeall the paths that contain some transition.We consider a LTA A.
We write ?A the prece-dence relation on transitions in an automaton A.We define lA(t) = {u ?
G, u ?A t} and rA(t) ={u ?
G, t ?A u}.For each transition s t??
s?
and each constraint(L,R) ?
C(t), if lA(t) ?
L = ?
and rA(t) ?
R =?, then none of the lexical taggings which use thetransition t has a solution and the transition t canbe safely removed from the automaton.This can be computed by a double-for loop: foreach atomic constraint of each transition, verifythat either the left context or the right context ofthe transition contains some structure to solve theconstraint.
Observe that the cost of this algorithmis O(n2), where n is the size of the input automa-ton.Note that one must iterate this algorithm until afixpoint is reached.
Indeed, removing a transitionwhich serves as a potential companion breaks theverification.
Nevertheless, since for each step be-fore the fixpoint is reached, we remove at least onetransition, we iterate the double-for at most O(n)times.
The complexity of the whole algorithm isthenO(n3).
In practice, we have observed that thecomplexity is closer to O(n2): only 2 or 3 loopsare enough to reach the fixpoint.Example 7 If we apply the QCP to the automatonof Example 4, in the first step, only the transition0 CN??
1 is removed by applying the atomic con-straint ?.
In the next step, the transition 1 RAdj????
2is removed by applying the atomic constraint ?.The fixpoint is reached and the output automaton(with 120 paths) is:0 1DetCNlt 2iLAdCj 3iLAdRLAdCjTrVIntrV4DetCjCNlt 5CjTrV5 The Generalized CompanionshipPrincipleIn practice, of course, we have to face the problemof the computation of the constraints.
In a largecoverage grammar, the size of G is too big to com-pute all the constraints in advance.
However, aswe have seen in example 2 we can identify sub-sets of G that have the same constraints; the sameway, we can use these subsets to give a more con-cise presentation of the L and R sets of the atomicconstraints.
This motivates us to define a General-ized Principle which is stated on a quotient set ofG.5.1 Generalized atomic constraintsLet U be a set of subsets of G that are a partitionof G. For t ?
G, we write t the subset of U whichcontains t.We say that a pair (L,R) of subsets of U is ageneralized atomic constraint for u ?
U if foreach lexical tagging L = [t1, .
.
.
, tn] such thatp(L) 6= ?
and u = ti for some i then:?
either there is some j < i such that tj ?
L,?
or there is some j > i such that tj ?
R.A system of generalized constraints for a par-titionU of a grammar G is a function Cwhich asso-247ciates a finite set of generalized atomic constraintsto each element of U.5.2 The Generalized PrincipleThe Generalized Companionship Principle is thenan immediate consequence of the previous defini-tion and can be stated as the necessary condition:The Generalized Companionship PrincipleIf a lexical tagging [t1, .
.
.
, tn] has a solutionthen for all i and for all generalized atomic con-straints (L,R) ?
C(ti)?
{t1, .
.
.
, ti?1} ?
L 6= ??
or {ti+1, .
.
.
, tn} ?
R 6= ?.Example 8 The constraints given in example 2are in fact generalized atomic constraints on theset (recall that we write LAdj for the 2 elementsset {LAdjbelle , LAdjferme}):U = {Det, LAdj, RAdj, CN, Clit, TrV, IntrV}.Then the constraints are expressed on |U| = 7 el-ements and not on |G| = 13.A generalized atomic constraint on U can, ofcourse, be expressed as a set of atomic constraintson G: let u ?
U and t ?
G such that t = u(L,R) ?
C(u) =?(?L?LL,?R?RR)?
C(t)5.3 Implementation of lexicalized grammarsIn implementations of large coverage linguistic re-sources, it is very common to have, first, the de-scription of the set of ?different?
structures neededto describe the modeled natural language and thenan anchoring mechanism that explains how wordsof the lexicon are linked to these structures.
Wecall unanchored grammar the set U of differ-ent structures (not yet related to words) that areneeded to describe the grammar.
In this context,the lexicon is split in two parts:?
a function ` from V to subsets of U,?
an anchoring function ?
which builds thegrammar elements from a word w ?
V andan unanchored structure u ?
`(w); we sup-pose that ?
verifies that anc(?
(w, u)) = w.In applications, we suppose that U, ` and ?
aregiven.
In this context, we define the grammar asthe codomain of the anchoring function:G = ?w?V,u?`(w)?
(w, u)Now, we can define generalized constraints onthe unanchored grammar, which are independentof the lexicon and can be computed statically for agiven unanchored grammar.6 Application to Interaction GrammarsIn this section, we apply the Companionship Prin-ciple to the Interaction Grammars formalism.
Wefirst give a short and simplified description of IGand an example to illustrate them at work; we re-fer the reader to (Guillaume and Perrier, 2008) fora complete and detailed presentation.6.1 Interaction GrammarsWe illustrate some of the important features onthe French sentence (2).
In this sentence, ?la?is an object clitic pronoun which is placed beforethe verb whereas the canonical place for the (non-clitic) object is on the right of the verb.
(2) ?Jean la demande.?
[John asks for it]The set F of final structures, used as output ofthe parsing process, contains ordered trees calledparse trees (PT).
An example of a PT for the sen-tence (2) is given in Figure 2.
A PT for a sentencecontains the words of the sentence or the emptyword  in its leaves (the left-right order of the treeleaves follows the left-right order of words in theinput sentence).
The internal nodes of a PT repre-sent the constituents of the sentence.
The morpho-syntactic properties of these constituents are de-scribed with feature structures (only the categoryis shown in the figure).As IG use the Model-Theoretic Syntax (MTS)framework, a PT is defined as the model of a setof constraints.
Constraints are defined at the wordlevel: words are associated to a set of constraintsformally described as a polarized tree descrip-tion (PTD).
A PTD is a set of nodes provided withrelations between these nodes.
The three PTDsused to build the model above are given in Fig-ure 3.
The relations used in the PTDs are: imme-diate dominance (lines) and immediate sisterhood(arrows).
Nodes represent syntactic constituents248A2-A3=SB1-B3=NP C2-C3=V D2-D3=NPJean E2=Cl F2-F3=V ?la demandeFigure 2: The PT of sentence (2)and relations express structural dependencies be-tween these constituents.Moreover, nodes carry a polarity: the set of po-larities is {+,?,=,?}.
A + (resp.?)
polarityrepresents an available (resp.
needed) resource, a?
polarity describes a node which is unsaturated.Each + must be associated to exactly one ?
(andvice versa) and each ?
must be associated to atleast another polarity.A2-3=SB1NPCVDJCVe aC-3=nCEJl FCVe ?l1PdEDAdm3= JdEe adm3=FdEeB1NBFigure 3: PTDs for the sentence (2)Now, we define a PT to be a model of a set ofPTDs if there is a surjective function I from nodesof the PTDs to nodes of the PT such that:?
relations in the PTDs are realized in the PT:if M is a daughter (resp.
immediate sister)of N in some PTD then I(M) is a daughter(resp.
immediate sister) of I(N);?
each node N in the PT is saturated: thecomposition of the polarities of the nodes inI?1(N) with the associative and commuta-tive rule given in Table 4 is =;?
the feature structure of a node N in the PT isthe unification of the feature structures of thenodes in I?1(N).One of the strong points of IG is the flexibilitygiven by the MTS approach: PTDs can be partiallysuperposed to produce the final tree (whereas su-perposition is limited in usual CG or in TAG forinstance).
In our example, the four grey nodesin the PTD which contains ?la?
are superposedto the four grey nodes in the PTD which contains?demande?
to produce the four grey nodes in themodel.?
?
+ =?
?
?
+ =?
?
=+ + == =Figure 4: Polarity compositionIn order to give an idea of the full IG system,we briefly give here the main differences betweenour presentation and the full system.?
Dominance relations can be underspecified:for instance a PTD can impose a node to be anancestor of another one without constrainingthe length of the path in the model.
This ismainly used to model unbounded extraction.?
Sisterhood relations can also be underspeci-fied: when the order on subconstituents is nottotal, it can be modeled without using severalPTDs.?
Polarities are attached to features rather thannodes: it sometimes gives more freedomto the grammar writer when the same con-stituent plays a role in different constructions.?
Feature values can be shared between severalnodes: once again, this is a way to factorizethe unanchored grammar.The application of the Companionship Princi-ple is described on the reduced IG but it canbe straightforwardly extended to full IG withunessential technical details.Following the notation given in 5.3, an IG ismade of:?
A finite set V of words;?
A finite set U of unanchored PTDs (withoutany word attached to them);?
A lexicon function ` from V to subsets of U.249When t ?
`(w), we can construct the anchoredPTD ?
(w, u).
Technically, in each unanchoredPTD u, a place is marked to be the anchor, i.e.to be replaced by the word during the anchoringprocess.
Moreover, the anchoring process can alsobe used to refine some features.
The fact thatthe feature can be refined gives more flexibilityand more compactness to the unanchored gram-mar construction.
In the French IG grammar, thesame unanchored PTD can be used for masculineor feminine common nouns and the gender is spec-ified during the anchoring to produce distinct an-chored PTDs for masculine and feminine nouns.
Gis defined by:G = ?w?V,u?`(w)?
(w, u)The parsing solutions of a lexical tagging is theset of PTs that are models of the list of PTDs de-scribed by the lexical tagging:p(L) = {m ?
F | m is a model of L}With the definitions of this section, an IG is aspecial case of AGF as defined in section 2.6.2 Companionship Principle for IGIn order to apply the Companionship Principle, wehave to explain how the generalized atomic con-straints are built for a given grammar.
One wayis to look at dependency structures but in IG po-larities are built in and then we can read the de-pendency information we need directly on polari-ties.
A requirement to build a model is the satura-tion of all the polarities.
This requirement can beexpressed using atomic constraints.
Each time aPTD contains an unsaturated polarity +, ?
or ?,we have to find some other compatible dual po-larity somewhere else in the grammar to saturateit.From the general MTS definition of IG above,we can define a step by step process to build mod-els of a lexical tagging.
The idea is to build in-crementally the interpretation function I with theatomic operation of node merging.
In this atomicoperation, we choose two nodes and make the hy-pothesis that they have the same image through Iand hence that they can be identified.Now, suppose that the unanchored PTD u con-tains some unsaturated polarity p. We can use theatomic operation of node merging to test if theunanchored PTD u?
can be used to saturate the po-larity p. Let L (resp R) be the set of PTDs thatcan be used on the left (resp.
on the right) of uto saturate p, then (L,R) is a generalized atomicconstraint in C(u).7 Companionship Principle for otherformalismsAs we said in the introduction, many current gram-matical formalisms can more or less directly beused to generate dependency structures and henceare candidate for disambiguation with the Com-panionship Principle.
With IG, we have seen thatdependencies are strongly related to polarities: de-pendency constraints in IG are built with the polar-ity system.We give below two short examples of polarityuse to define atomic constraints on TAG and onCG.
We use, as for IG, the polarity view of depen-dencies to describe how the constraints are built.7.1 Tree Adjoining GrammarsFeature-based Tree Adjoining Grammars (here-after FTAG) (Joshi, 1987) are a unification basedversion of Tree Adjoining Grammars.
An FTAGconsists of a set of elementary trees and of twotree composition operations: substitution and ad-junction.
There are two kinds of trees: auxiliaryand initial.
Substitution inserts a tree t with rootr onto a leaf node l of another tree t?
under thecondition that l is marked as a place for substitu-tion and l and r have compatible feature structures.Adjunction inserts an auxiliary tree t into a tree t?by splitting a node n of t?
under the condition thatthe feature structures of the root and foot nodes oft are compatible with the top and bottom ones ofn.Getting the generalized atomic constraints andthe model building procedure for lexical taggingis extremely similar to what was previously de-scribed for IG if we extend the polarization pro-cedure which was described in (Gardent and Kow,2005) to do polarity based filtering in FTAG.
Theidea is that for each initial tree t, its root of cate-gory C is marked as having the polarity +C, andits substitution nodes of category S are marked ashaving the polarity ?S.
A first constraint set con-tains trees t?
whose root is polarized +S and suchthat feature structures are unifiable.
A second con-straint set contains trees t??
which have a leaf thatis polarized ?C.
We can extend this procedure to250auxiliary trees: each auxiliary tree t of category Aneeds to be inserted in a node of category A of an-other tree t?.
This gives us a constraint in the spiritof the ?
polarity in IG: C(t) contains all the treest?
in which t could be inserted2.7.2 Categorial GrammarsIn their type system, Categorial Grammars en-code linearity constraints and dependencies be-tween constituents.
For example, a transitive verbis typed NP\S/NP , meaning that it waits for asubject NP on its left and an object NP on itsright.
This type can be straightforwardly decom-posed as two ?NP and one +S polarities.
Thenagain, getting the generalized atomic constraintsis immediate and in the same spirit as what wasdescribed for IG.7.3 Non-lexicalized formalismsThe lexicalization condition stated in section 2excludes non-lexicalized formalisms like LFG orHPSG.
Nothing actually prevents our methodfrom being applied to these, but adding non-lexicalized combinators requires to complexify theformal account of the method.
Adapting ourmethod to HPSG would result in a generaliza-tion and unification of some of the techniques de-scribed in (Kiefer et al, 1999).8 Experimental results8.1 SetupThe experiments are performed using a French IGgrammar on a set of 31 000 sentences taken fromthe newspaper Le Monde.The French grammar we consider (Perrier,2007) contains |U| = 2 088 unanchored trees.It covers 88% of the grammatical sentences andrejects 85% of the ungrammatical ones on theTSNLP (Lehmann et al, 1996) corpus.The constraints have been computed on theunanchored grammar as explained in section 5:each tree contains several polarities and thereforeseveral atomic constraints.
Overall, the grammarcontains 20 627 atomic constraints.
It takes 2 daysto compute the set of constraints and the resultscan be stored in a constraints file of 10MB.
Ofcourse, an atomic constraint is more interestingwhen the sizes of L and R are small.
In our gram-mar, 50% of the constraints set (either R or L)2Note that in the adjunction case, the constraint is not ori-ented and then L= R.contain at most 40 elements and 80% of these setscontain at most 200 elements over 2 088.We give in figure 5 the number of sentences ofeach length in the corpus we consider.0?500?1000?1500?2000?2500?3000?3500?4000?4500?5000?6?
7?
8?
9?
10?
11?
12?
13?
14?
15?
16?
17?
18?
19?number?of?sentences?sentence?length?
(number?of?words)?Figure 5: number of sentences of each length8.2 ResultsTwo preliminary comments need to be made onthe treatment of the results.First, as we observed above, the number nof lexical taggings is a priori exponential in thelength of the sentence.
We thus consider its log.Moreover, because we use a raw corpus, somesentences are considered as ungrammatical by thegrammar; in this case it may happen that the dis-ambiguation method removes all taggings.
In or-der to avoid undefined values when n = 0, we infact consider log10(1 + n).Second, as expected, the ECP method is moretime consuming and for some sentences the timeand/or memory required is problematic.
To be ableto apply the ECP to a large number of sentences,we have used it after another filtering methodbased on polarities and described in (Bonfante etal., 2004).Thus, for each sentence we have computed 3different filters, each one finer than the previous:?
QCP the Quick Companionship Principle;?
QCP+POL QCP followed by a filtering tech-nique based on polarity counting;?
QCP+POL+ECP the Exact CompanionshipPrinciple applied to the previous filter.Figure 6 displays the mean computation timefor each length: it confirms that the ECP is moretime consuming and goes up to 5s for our long sen-tences.251Finally, we report the number of lexical tag-gings that each method returns.
Figure 7 displaysthe mean value of log10(1 + n) where n is eitherthe initial number of lexical taggings or the num-ber of lexical taggings left by the filter.0.01?0.1?1?10?6?
7?
8?
9?
10?
11?
12?
13?
14?
15?
16?
17?
18?
19??es?(in?s)?sentence?length?
(number?of?words)?QCP?QCP+POL?QCP+POL+ECP?Figure 6: mean execution time (in s)0?2?4?6?8?10?12?14?16?18?6?
7?
8?
9?
10?
11?
12?
13?
14?
15?
16?
17?
18?
19?Log?(1+n)?sentence?length?
(number?of?words)?QCP?QCP+POL?QCP+POL+ECP?Ini?l?Figure 7: number of taggings (initial and after the3 disambiguation methods)We can observe that the slope of the lines cor-responds to the mean word ambiguity: if themean ambiguity is a then the number of taggingsfor a sentence of length n is about an and thenlog(an) = n ?
log(a).
As a consequence, the meanambiguity can be read as 10s where s is the slopein the last figure.An illustration is given in figure 8 which ex-hibits the mean word ambiguity for sentences oflength 16.init QCP QCP+POL QCP+POL+ECP6.13 3.41 1.93 1.41Figure 8: mean word ambiguity for sentences oflength 169 ConclusionWe have presented a disambiguation methodbased on dependency constraints which allows tofilter out many wrong lexical taggings before en-tering the deep parsing.
As this method relies onthe computation of static constraints on the lin-guistic data and not on a statistical model, we canbe sure that we will never remove any correct lex-ical tagging.
Moreover, we manage to apply ourmethods to an interesting set of data and prove thatit is efficient for a large coverage grammar and notonly for a toy grammar.These results are also an encouragement to de-velop further this kind of disambiguation methods.In the near future, we would like to explore someimprovements.First, we have seen that our principle cannot becomputed on the whole grammar and that in its im-plementation we consider unanchored structures.We would like to explore the possibility of com-puting finer constraints (relative to the full gram-mar) on the fly for each sentence.
We believe thatthis can eliminate some more taggings before en-tering the deep parsing.Concerning the ECP, as we have seen, there is akind of interplay between the efficiency of the fil-tering and the time of the computation.
We wouldlike to explore the possibility to define some in-termediate way between QCP and ECP either byusing approximate automata or using the ECP butonly on a subset of elements where it is known tobe efficient.Another challenging method we would like toinvestigate is to use the Companionship Principlenot only as a disambiguation method but as a guidefor the deep parsing.
Actually, we have observedfor at least 20% of the words that dependencies arecompletely determined by the filtering methods.
Ifdeep parsing can be adapted to use this observation(this is the case for IG), this can be of great help.Finally, we can improve the filtering using bothworlds: the Companionship Principle and the po-larity counting method.
Two different constraintscannot be fulfilled by the same potential compan-ion: this may allow to discover some more lexicaltaggings that can be safely removed.AcknowledgmentsWe would like to thank the anonymous reviewersfor their helpful comments and suggestions.252ReferencesSrinivas Bangalore and Aravind K. Joshi.
1999.
Su-pertagging: an approach to almost parsing.
Comput.Linguist., 25(2):237?265.G.
Bonfante, B. Guillaume, and G. Perrier.
2004.Polarization and abstraction of grammatical for-malisms as methods for lexical disambiguation.
InCoLing 2004, pages 303?309, Gene`ve, Switzerland.P.
Boullier.
2003.
Supertagging : A non-statisticalparsing-based approach.
In Pro- ceedings of the8th International Workshop on Parsing Technologies(IWPT 03), pages 55?65, Nancy, France.Stephen Clark and James R. Curran.
2004.
The impor-tance of supertagging for wide-coverage CCG pars-ing.
In COLING ?04: Proceedings of the 20th in-ternational conference on Computational Linguis-tics, page 282, Morristown, NJ, USA.
Associationfor Computational Linguistics.S.
Clark, J. Hockenmaier, and M. Steedman.
2002.Building Deep Dependency Structures with a Wide-Coverage CCG Parser.
In Proceedings of ACL?02,pages 327?334, Philadephia, PA.Ph.
de Groote.
2001.
Towards abstract categorialgrammars.
In Association for Computational Lin-guistics, 39th Annual Meeting and 10th Conferenceof the European Chapter, Proceedings of the Confer-ence, pages 148?155.C.
Gardent and E. Kow.
2005.
Generating and se-lecting grammatical paraphrases.
Proceedings of theENLG, Aug.B.
Guillaume and G. Perrier.
2008.
Interaction Gram-mars.
Research Report RR-6621, INRIA.A.
Joshi and O. Rambow.
2003.
A Formalism for De-pendency Grammar Based on Tree Adjoining Gram-mar.
In Proceedings of the Conference on Meaning-Text Theory.A.
Joshi.
1987.
An Introduction to Tree AdjoiningGrammars.
Mathematics of Language.S.
Kahane.
2006.
Polarized unification grammar.
InProceedings of Coling-ACL?02, Sydney.Bernd Kiefer, Hans-Ulrich Krieger, John Carroll, andRob Malouf.
1999.
A bag of useful techniques forefficient and robust parsing.
In Proceedings of the37th annual meeting of the Association for Compu-tational Linguistics on Computational Linguistics,pages 473?480, Morristown, NJ, USA.
Associationfor Computational Linguistics.A.
Koller and M. Kuhlmann.
2009.
Dependencytrees and the strong generative capacity of CCG.
InEACL?
2009, Athens, Greece.J.
Kupiec.
1992.
Robust Part-of-Speech Tagging Us-ing a Hidden Markov Model.
Computer Speech andLanguage, 6(3):225?242.F.
Lamarche.
2008.
Proof Nets for Intuitionistic LinearLogic: Essential Nets.
Technical report, INRIA.J.
Lambek.
1958.
The mathematics of sentence struc-ture.
American mathematical monthly, pages 154?170.S.
Lehmann, S. Oepen, S. Regnier-Prost, K. Netter,V.
Lux, J. Klein, K. Falkedal, F. Fouvry, D. Esti-val, E. Dauphin, H. Compagnion, J. Baur, L. Balkan,and D. Arnold.
1996.
TSNLP: Test Suites for Nat-ural Language Processing.
In Proceedings of the16th conference on Computational linguistics, pages711?716.J.
Marchand, B. Guillaume, and G. Perrier.
2009.Analyse en de?pendances a` l?aide des grammairesd?interaction.
In Actes de TALN 09, Senlis, France.B.
Merialdo.
1994.
Tagging English Text with a Prob-abilistic Model.
Computational linguistics, 20:155?157.M.
Moortgat and G. Morrill.
1991.
Heads and phrases.Type calculus for dependency and constituent struc-ture.
In Journal of Language, Logic and Informa-tion.Takashi Ninomiya, Takuya Matsuzaki, Yoshimasa Tsu-ruoka, Yusuke Miyao, and Jun?ichi Tsujii.
2006.Extremely lexicalized models for accurate and fastHPSG parsing.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 155?163, Sydney, Australia, July.Association for Computational Linguistics.G.
Perrier.
2007.
A French Interaction Grammar.
InRANLP 2007, pages 463?467, Borovets Bulgarie.M.
Steedman.
2000.
The Syntactic Process.
MITPress.L.
Tesnie`re.
1959.
E?le?ments de syntaxe structurale.Klinksieck.253
