Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 233?243,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsAligning context-based statistical models of languagewith brain activity during readingLeila Wehbe1,2, Ashish Vaswani3, Kevin Knight3and Tom Mitchell1,21Machine Learning Department, Carnegie Mellon University, Pittsburgh, PA2Center for the Neural Basis of Computation, Carnegie Mellon University, Pittsburgh, PA3Information Sciences Institute, University of Southern California, Los Angeles, CAlwehbe@cs.cmu.edu, vaswani@usc.edu, knight@isi.edu, tom.mitchell@cs.cmu.eduAbstractMany statistical models for natural language pro-cessing exist, including context-based neural net-works that (1) model the previously seen contextas a latent feature vector, (2) integrate successivewords into the context using some learned represen-tation (embedding), and (3) compute output proba-bilities for incoming words given the context.
Onthe other hand, brain imaging studies have sug-gested that during reading, the brain (a) continu-ously builds a context from the successive wordsand every time it encounters a word it (b) fetches itsproperties from memory and (c) integrates it withthe previous context with a degree of effort that isinversely proportional to how probable the word is.This hints to a parallelism between the neural net-works and the brain in modeling context (1 and a),representing the incoming words (2 and b) and in-tegrating it (3 and c).
We explore this parallelism tobetter understand the brain processes and the neu-ral networks representations.
We study the align-ment between the latent vectors used by neural net-works and brain activity observed via Magnetoen-cephalography (MEG) when subjects read a story.For that purpose we apply the neural network to thesame text the subjects are reading, and explore theability of these three vector representations to pre-dict the observed word-by-word brain activity.Our novel results show that: before a new word iis read, brain activity is well predicted by the neuralnetwork latent representation of context and the pre-dictability decreases as the brain integrates the wordand changes its own representation of context.
Sec-ondly, the neural network embedding of word i canpredict the MEG activity when word i is presentedto the subject, revealing that it is correlated with thebrain?s own representation of word i.
Moreover, weobtain that the activity is predicted in different re-gions of the brain with varying delay.
The delay isconsistent with the placement of each region on theprocessing pathway that starts in the visual cortexand moves to higher level regions.
Finally, we showthat the output probability computed by the neuralnetworks agrees with the brain?s own assessment ofthe probability of word i, as it can be used to predictthe brain activity after the word i?s properties havebeen fetched from memory and the brain is in theprocess of integrating it into the context.1 IntroductionNatural language processing has recently seen asurge in increasingly complex models that achieveimpressive goals.
Models like deep neural net-works and vector space models have become pop-ular to solve diverse tasks like sentiment analy-sis and machine translation.
Because of the com-plexity of these models, it is not always clear howto assess and compare their performances as theymight be useful for one task and not the other.It is also not easy to interpret their very high-dimensional and mostly unsupervised representa-tions.
The brain is another computational systemthat processes language.
Since we can record brainactivity using neuroimaging, we propose a new di-rection that promises to improve our understand-ing of both how the brain is processing languageand of what the neural networks are modeling byaligning the brain data with the neural networksrepresentations.In this paper we study the representations of twokinds of neural networks that are built to predictthe incoming word: recurrent and finite contextmodels.
The first model is the Recurrent NeuralNetwork Language Model (Mikolov et al., 2011)which uses the entire history of words to modelcontext.
The second is the Neural ProbabilisticLanguage Model (NPLM) which uses limited con-text constrained to the recent words (3 grams or 5grams).
We trained these models on a large HarryPotter fan fiction corpus and we then used them topredict the words of chapter 9 of Harry Potter andthe Sorcerer?s Stone (Rowling, 2012).
In paral-lel, we ran an MEG experiment in which 3 subjectread the words of chapter 9 one by one while theirbrain activity was recorded.
We then looked forthe alignment between the word-by-word vectorsproduced by the neural networks and the word-by-word neural activity recorded by MEG.Our neural networks have 3 key constituents:a hidden layer that summarizes the history of theprevious words ; an embeddings vector that sum-marizes the (constant) properties of a given wordand finally the output probability of a word given233Reading comprehension is reflected in the subsequent acti-vation of the left superior temporal cortex at 200?600 ms(Halgren et al., 2002; Helenius et al., 1998; Pylkka?nenet al., 2002, 2006; Pylkka?nen and Marantz, 2003; Simoset al., 1997).
This sustained activation differentiatesbetween words and nonwords (Salmelin et al., 1996; Wil-son et al., 2005; Wydell et al., 2003).
Apart from lexical-se-mantic aspects it also seems to be sensitive to phonologicalmanipulation (Wydell et al., 2003).As discussed above, in speech perception activation isconcentrated to a rather small area in the brain and wehave to rely on time information to dissociate between dif-ferent processes.
Here, the different processes are separableboth in timing and location.
Because of that, one mightthink that it is easier to characterize language-related pro-cesses in the visual than auditory modality.
However, herethe difficulties appear at another level.
In reading, activa-tion is detected bilaterally in the occipital cortex, alongthe temporal lobes, in the parietal cortex and, in vocalizedreading, also in the frontal lobes, at various times withrespect to stimulus onset.
Interindividual variability furthercomplicates the picture, resulting in practically excessiveamounts of temporal and spatial information.
The areasand time windows depicted in Fig.
5, with specific rolesin reading, form a limited subset of all active areasobserved during reading.
In order to perform proper func-tional localization one needs to vary the stimuli and taskssystematically, in a parametric fashion.
Let us now consid-er how one may extract activation reflecting pre-lexical let-ter-string analysis and lexical-semantic processing.3.2.
Pre-lexical analysisIn order to tease apart early pre-lexical processes inreading, Tarkiainen and colleagues (Tarkiainen et al.,1999) used words, syllables, and single letters, imbeddedin a noisy background, at four different noise levels(Fig.
6).
For control, the sequences also contained symbolstrings.
One sequence was composed of plain noise stimuli.The stimuli were thus varied along two major dimensions:the amount of features to process increased with noise andwith the number of items, letters or symbols.
On the otherhand, word-likeness was highest for clearly visible completewords and lowest for symbols and noise.At the level of the brain, as illustrated in Fig.
7, the datashowed a clear dissociation between two processes withinthe first 200 ms: visual feature analysis occurred at about100 ms after stimulus presentation, with the active areasaround the occipital midline, along the ventral stream.
Inthese areas, the signal increased with increasing noise andwith the number of items in the string, similarly for lettersand symbols.
Only 50 ms later, at about 150 ms, the leftinferior occipitotemporal cortex showed letter-string spe-cific activation.
This signal increased with the visibility ofthe letter strings.
It was strongest for words, weaker for syl-lables, and still weaker for single letters.
Crucially, the acti-vation was significantly stronger for letter than symbolstrings of equal length.Bilateral occipitotemporal activation at about 200 mspost-stimulus is consistently reported in MEG studies ofreading (Cornelissen et al., 2003b; Pammer et al., 2004; Sal-melin et al., 1996, 2000b) but, interestingly, functionalspecificity for letter-strings is found most systematicallyin the left hemisphere.
The MEG data on letter-string spe-cific activation are in good agreement with intracranialrecordings, both with respect to timing and location andthe pre-lexical nature of the activation (Nobre et al., 1994).3.3.
Lexical-semantic analysisTo identify cortical dynamics of reading comprehension,Helenius and colleagues (Helenius et al., 1998) employed aVisual featureanalysisNon-specific Words =Nonwords NonwordsLetter-stringanalysisTime (ms)0 400 800 0 400 800 0 400 800Lexical-semanticanalysisFig.
5.
Cortical dynamics of silent reading.
Dots represent centres of active cortical patches collected from individual subjects.
The curves display themean time course of activation in the depicted source areas.
Visual feature analysis in the occipital cortex (!100 ms) is stimulus non-specific.
The stimuluscontent starts to matter by !150 ms when activation reflecting letter-string analysis is observed in the left occipitotemporal cortex.
Subsequent activationof the left superior temporal cortex at !200?600 ms reflects lexical-semantic analysis and, probably, also phonological analysis.
Modified from Salmelinet al.
(2000a).!LippincottWilliams&Wilkins2000R.
Salmelin / Clinical Neurophysiology xxx (2006) xxx?xxx 5ARTICLE IN PRESSPlease cite this article as: Riitta Salmelin, Clinical neurophysiology of language: The MEG approach, Clinical Neurophysiology(2006), doi:10.1016/j.clinph.2006.07.316Figure 1: Cortical dynamics of silent reading.
This figureis adapted from (Salmelin, 2007).
Dots represent projectedsources of activity in the visual cortex (left brain sketch) andthe temporal cortex (right brain sketch).
The curves displaythe m an time course of activation in the depicted ource ar-eas for different conditions.
The initial visual feature anal-ysis in the visual cortex at ?100 ms is non-specific to lan-guage.
Comparing responses to letter strings and other vi-sual stimuli rev als that letter string nalysis ccurs around150 ms.
Finally comparing the responses to words and non-words (made-up words) reveals lexical-se antic analysis inthe temporal cortex at ?200-500ms.the context.
We set out to find the brain analogsof these model constituents using an MEG decod-ing task.
We compare the different models andh ir represen ations n term of how well theycan be used to decode the word being read fromMEG data.
We obtain correspondences betweenthe models and the brain data that are consistentwith model of language processing in whichbrain activity encodes story context, and whereeach new word generates additional brain activity,flowing generally from visual processing areas tomore high level areas, culminating in an updatedstory cont xt, and reflecting a overall m gnitudeof neural effort influenced by the probability ofthat new word given the previous context.1.1 Neural processes involved in readingHumans read with an average speed of 3 wordsper second.
Reading requires us to perceive in-coming words and gradually integrate them intoa representation of the meaning.
As words areread, it takes 100ms for the visual input to reachthe visual cortex.
50ms later, the visual input isprocessed as letter strings in a specialized regionof the left visual cortex (Salmelin, 2007).
Be-tween 200-500ms, the word?s semantic propertiesare processed (see Fig.
1).
Less is understoodabout the cortical dynamics of word integration, asmultiple theories exist (Friederici, 2002; Hagoort,2003).Magnetoencephalography (MEG) is a brain-imaging tool that is well suited for studying lan-guage.
MEG records the change in the magneticfield on the surface of the head that is caused bya large set of aligned neurons that are changingtheir firing patterns in synchrony in response toa stimulus.
Because of the nature of the signal,MEG recordings are directly related to neural ac-tivity and have no latency.
They are sampled ata high frequency (typically 1kHz) that is ideal fortracking the fast dynamics of language processing.In this work, we are interested in the mecha-nism of human text understanding as the meaningof ncoming words is fetched from memory andintegrated with the context.
Interestingly, this isanalogous to neural network models of languagethat are used to predict the incoming word.
Themental representation of the previous context isanalogous to the latent layer of the neural networkwhich summarizes the relevant context before see-ing the word.
The representation of the meaningof a word is analogous to the embedding that theneural network learns in training and then uses.Finally, one common hypotheses is that the brainintegrates the word with inversely proportional ef-fort to how predictable the word is (Frank et al.,2013).
There is a well studied response known asthe N400 that is an increase of the activity in themporal cortex that has been recently shown to begraded by the amount of surprisal of the incomingword given the context (Frank et al., 2013).
This isanalogous to the output probability of the incom-ing word from the neural network.Fig.
2 shows a hypothetical activity in an MEGsensor as a subject reads a story in our experi-ment, in which words are presented one at a timefor 500ms each.
We conjecture that the activity intime window a, i.e.
before word i is understood, ismostly related to the previous context before see-ing word i.
We also conjecture that the activity intime window b is related to understanding word iand integrating it into the context, leading to a newrepresentation of context in window c.Using three types of features from neural net-works (hidden layer context representation, outputprobabilities and word embeddings) from threedifferent models of language (one recurrent modeland two finite context models), we therefore set topredict the activity in the brain in different timewindows.
We want to align the brain data with thevarious model constituents to understand whereand when different types of processes are com-puted in the brain, and simultaneously, we want to234Harry'Harry'had'had'never'never'embedding(iC1)' embedding(i)' embedding(i+1)'context(iC1)'context(iC2)' context(i)'out.'prob.
(iC1)' out.'prob.
(i)' out.'prob.
(i+1)'Studying'the'construc<on'of'meaning'3'word i+1 word i-1 word i0.5's'0.5's' 0.5's'a bcLeila'Wehbe''?''Harry'''''''had''''''''never'?
''Figure 2: [Top] Sketch of the updates of a neural networkreading chapter 9 after it has been trained.
Every word cor-responds to a fixed embedding vector (magenta).
A contextvector (blue) is computed before the word is seen given theprevious words.
Given the context vector, the probability ofevery word can be computed (symbolized by the histogramin green).
We only use the output probability of the actualword (red circle).
[Bottom] Hypothetical activity in an MEGsensor when the subject reads the corresponding words.
Thetime periods approximated as a, b and c can be tested for in-formation content relating to: the context of the story beforeseeing word i (modeled by the context vector at i), the repre-sentation of the properties of word i (the embedding of wordi) and the integration of word i into the context (the outputprobability of word i).
The periods drawn here are only aconjecture on the timings of such cognitive events.use the brain data to shed light on what the neuralnetwork vectors are representing.Related workDecoding cognitive states from brain data is arecent field that has been growing in popularity.Most decoding studies that study language usefunctional Magnetic Resonance Imaging (fMRI),while some studies use MEG.
MEG?s high tempo-ral resolution makes it invaluable for looking at thedynamics of language understanding.
(Sudre etal., 2012) decode from MEG the word a subject isreading.
The authors estimate from the MEG datathe semantic features of the word and use these asan intermediate step to decode what the word is.This is in principle similar to the classification ap-proach we follow, as we will also use the featurevectors as an intermediate step for word classifica-tion.
However the experimental paradigm in (Su-dre et al., 2012) is to present to the subjects sin-gle isolated words and to find how the brain rep-resents their semantic features; whereas we have amuch more complex and ?naturalistic?
experimentin which the subjects read a non-artificial passageof text, and we look at processes that exceed in-dividual word processing: the construction of themeanings of the successive words and the predic-tion/integration of incoming words.In (Frank et al., 2013), the amount of surprisalthat a word has given its context is used to pre-dict the intensity of the N400 response describedpreviously.
This is the closest study we could findto our approach.
This study was concerned withanalyzing the brain processes related only to sur-prisal while we propose a more integral accountof the processes in the brain.
The study also didn?taddress the major contribution we propose here,which is to shed light on the inner constituents oflanguage models using brain imaging.1.2 Recurrent and finite context neuralnetworksSimilar to standard language models, neural lan-guage models also learn probability distributionsover words given their previous context.
However,unlike standard language models, words are rep-resented as real-valued vectors in a high dimen-sional space.
These word vectors, referred to asword embeddings, can be different for input andoutput words, and are learned from training data.Thus, although at training and test time, the in-put and output to the neural language models areone-hot representation of words, it is their em-beddings that are used to compute word proba-bility distributions.
After training the embeddingvectors are fixed and it is these vectors that wewill use later on to predict MEG data.
To predictMEG data, we will also use the latent vector rep-resentations of context that these neural networksproduce, as well as the probability of the currentword given the context.
In this section, we willdescribe how recurrent neural network languagemodels and feedforward neural probabilistic lan-guage models compute word probabilities.
In theinterest of space, we keep this description brief,and for details, the reader is requested to refer tothe original papers describing these models.235w(t) s(t?
1)y(t) c(t)hiddens(t)outputP (wt+1| s(t))D WD?XFigure 3: Recurrent neural network language model.Recurrent Neural Network Language ModelUnlike standard feedforward neural languagemodels that only look at a fixed number of pastwords, recurrent neural network language modelsuse all the previous history from position 1 to t?1to predict the next word.
This is typically achievedby feedback connections, where the hidden layeractivations used for predicting the word in posi-tion t ?
1 are fed back into the network to com-pute the hidden layer activations for predicting thenext word.
The hidden layer thus stores the historyof all previous words.
We use the RNNLM archi-tecture as described in Mikolov (2012), shown inFigure 3.
The input to the RNNLM at position tare the one-hot representation of the current word,w(t), and the activations from the hidden layer atposition t ?
1, s(t ?
1).
The output of the hiddenlayer at position t?
1 iss(t) = ?
(Dw(t) + Ws(t?
1)) ,where D is the matrix of input word embeddings,W is a matrix that transforms the activations fromthe hidden layer in position t ?
1, and ?
is asigmoid function, defined as ?
(x) =11+exp(?x),that is applied elementwise.
We need to computethe probability of the next word w(t + 1) giventhe hidden state s(t).
For fast estimation of out-put word probabilities, Mikolov (2012) divides thecomputation into two stages: First, the probabilitydistribution over word classes is computed, afterwhich the probability distribution over the subsetof words belonging to the class are computed.
Theclass probability of a particular class with indexmat position t is computed as:P (cm(t) | s(t)) =exp (s(t)Xvm)?Cc=1(exp (s(t)Xvc)),where X is a matrix of class embeddings and vmis a one-hot vector representing the class with in-dex m. The normalization constant is computedu1u2inputwordsinputembeddingshiddenh1hiddenh2outputP (w | u)D?MC1C2DFigure 4: Neural probabilistic language modelover all classes C. Each class specifies a subsetV?of words, potentially smaller than the entire vo-cabulary V .
The probability of an output word l atposition t + 1 given that its class is m is definedas:P (yl(t+ 1) | cm(t), s(t)) =exp (s(t)D?vl)?V?k=1(exp (s(t)D?vk)),where D?is a matrix of output word embeddingsand vlis a one hot vector representing the wordwith index l. The probability of the word w(t+1)given its class cican now be computed as:P (w(t+ 1) | s(t)) =P (w(t+ 1) | ci, s(t))P (ci| s(t)).Neural Probabilistic Language ModelWe use the feedforward neural probabilistic lan-guage model architecture of Vaswani et al.
(2013),as shown in Figure 4.
Each context u comprisesa sequence of words uj(1 ?
j ?
n ?
1) repre-sented as one-hot vectors, which are fed as inputto the neural network.
At the output layer, the neu-ral network computes the probability P (w | u) foreach word w, as follows.The output of the first hidden layer h1ish1= ??
?n?1?j=1CjDuj+ b1?
?,where D is a matrix of input word embeddingswhich is shared across all positions, the Cjare thecontext matrices for each word in u, b1is a vec-tor of biases with the same dimension as h1, and ?is applied elementwise.
Vaswani et al.
(2013) userectified linear units (Nair and Hinton, 2010) for236the hidden layers h1and h2, which use the activa-tion function ?
(x) = max(0, x).The output of the second layer h2ish2= ?
(Mh1+ b2) ,where M is a weight matrix between h1and h2and b2is a vector of biases for h2.
The probabil-ity of the output word is computed at the outputsoftmax layer as:P (w | u) =exp(vwD?h2+ bTvw)?Vw?=1exp (vw?D?h2+ bTvw?
),where D?is the matrix of output word embed-dings, b is a vector of biases for every output wordand vwits the one hot representation of the wordw in the vocabulary.2 MethodsWe describe in this section our approach.
In sum-mary, we trained the neural network models ona Harry Potter fan fiction database.
We then ranthese models on chapter 9 of Harry Potter and theSorcerer?s Stone (Rowling, 2012) and computedthe context and embedding vectors and the outputprobability for each word.
In parallel, 3 subjectsread the same chapter in an MEG scanner.
Webuild models that predict the MEG data for eachword as a function of the different neural networkconstituents.
We then test these models with aclassification task that we explain below.
We de-tect correspondences between the neural networkcomponents and the brain processes that under-lie reading in the following fashion.
If using aneural network vector (e.g.
the RNNLM embed-ding vector) allows us to classify significantly bet-ter than chance in a given region of the brain ata given time (e.g.
the visual cortex at time 100-200ms), then we can hypothesize a relationshipbetween that neural network constituent and thetime/location of the analogous brain process.2.1 Training the Neural NetworksWe used the freely available training tools pro-vided by Mikolov (2012)1and Vaswani et al.
(2013)2to train our RNNLM and NPLM modelsused in our brain data classification experiments.Our training data comprised around 67.5 million1http://rnnlm.org/2http://nlg.isi.edu/software/nplmwords for training and 100 thousand words for val-idation from the Harry Potter fan fiction database(http://harrypotterfanfiction.com).
We restrictedthe vocabulary to the top 100 thousand wordswhich covered all but 4 words from Chapter 9 ofHarry Potter and the Sorcerer?s Stone.For the RNNLM, we trained models with differ-ent hidden layers and learning rates and found theRNNLM with 250 hidden units to perform best onthe validation set.
We extracted our word embed-dings from the input matrix D (Figure 3).
We usedthe default settings for all other hyper parameters.We trained 3-gram and 5-gram NPLMs with150 dimensional word embeddings and experi-mented with different number of units for the firsthidden layer (h1in Figure 4), and different learn-ing rates.
For both the 3-gram and 5-gram mod-els, we found 750 hidden units to perform the beston the validation set and chose those models forour final experiments.
We used the output wordembeddings D?in our experiments.
We visuallyinspected the nearest neighbors in the 150 dimen-sional word embedding space for some words anddidn?t find the neighbors from D?or D to be dis-tinctly better than each other.
We leave the com-parison of input and output embeddings on brainactivity prediction for future work.2.2 MEG paradigmWe recorded MEG data for three subjects (2 fe-males and one male) while they read chapter 9of Harry Potter and the Sorcerer?s Stone (Rowl-ing, 2012).
The participants were native Englishspeakers and right handed.
They were chosen tobe familiar with the material: we made sure theyhad read the Harry Potter books or seen the moviesseries and were familiar with the characters andthe story.
All the participants signed the consentform, which was approved by the University ofPittsburgh Institutional Review Board, and werecompensated for their participation.The words of the story were presented in rapidserial visual format (Buchweitz et al., 2009):words were presented one by one at the centerof the screen for 0.5 seconds each.
The text wasshown in 4 experimental blocks of ?11 minutes.In total, 5176 words were presented.
Chapter 9was presented in its entirety without modificationsand each subject read the chapter only once.One can think of an MEG machine as a largehelmet, with sensors located on the helmet that237record the magnetic activity.
Our MEG recordingswere acquired on an Elekta Neuromag device atthe University of Pittsburgh Medical Center Pres-byterian Hospital.
This machine has 306 sensorsdistributed into 102 locations on the surface of thesubject?s head.
Each location groups 3 sensors ortwo types: one magnometer that records the in-tensity of the magnetic field and two planar gra-diometers that record the change in the magneticfield along two orthogonal planes3.Our sampling frequency was 1kHz.
For prepro-cessing, we used Signal Space Separation method(SSS, (Taulu et al., 2004)), followed by its tempo-ral extension (tSSS, (Taulu and Simola, 2006)).For each subject, the experiment data consiststherefore of a 306 dimensional time series oflength?45 minutes.
We averaged the signal in ev-ery sensor into 100ms non-overlapping time bins.Since words were presented for 500ms each, wetherefore obtain for every word p = 306 ?
5 val-ues corresponding to 306 vectors of 5 points.2.3 Decoding experimentTo find which parts of brain activity are related tothe neural network constituents (e.g.
the RNNLMcontext vector), we run a prediction and classifica-tion experiment in a 10-fold cross validated fash-ion.
At every fold, we train a linear model to pre-dict MEG data as a function of one of the featuresets, using 90% of the data.
On the remaining 10%of the data, we run a classification experiment.MEG data is very noisy.
Therefore, classify-ing single word waveforms yields a low accuracy,peaking at 60%, which might lead to false nega-tives when looking for correspondences betweenneural network features and brain data.
To revealinformative features, one can boost signal by ei-ther having several repetitions of the stimuli in theexperiment and then averaging (Sudre et al., 2012)or by combining the words into larger chunks (We-hbe et al., 2014).
We chose the latter because theformer sacrifices word and feature diversity.At testing, we therefore repeat the following300 times.
Two sets of words are chosen ran-domly from the test fold.
To form the first set, 20words are sampled without replacement from thetest sample (unseen by the classifier).
To form thesecond set, the kthword is chosen randomly fromall words in the test fold having the same length as3In this paper, we treat these three different sensors asthree different dimensions without further exploiting theirphysical properties.the kthword of the first set.
Since every fold ofthe data was used 9 times in the training phase andonce in the testing phase, and since we use a highnumber of randomized comparisons, this averagesout biases in the accuracy estimation.
Classifyingsets of 20 words improves the classification accu-racy greatly while lowering its variance and makesit dissociable from chance performance.
We com-pare only between words of equal length, to mini-mize the effect of the low level visual features onthe classification accuracy.After averaging out the results of multiple folds,we end up with average accuracies that reveal howrelated one of the models?
constituents (e.g.
theRNNLM context vector) is to brain data.2.3.1 Annotation of the stimulus textWe have 9 sets of annotations for the words of theexperiment.
Each set j can be described as a ma-trix Fjin which each row i corresponds to the vec-tor of annotations of word i.
Our annotations cor-respond to the 3 model constituents for each of the3 models: the hidden layer representation beforeword i, the output probability of word i and thelearned embeddings for word i.2.3.2 ClassificationIn order to align the brain processes and the differ-ent constituents of the different models, we use aclassification task.
The task is to classify the worda subject is reading out of two possible choicesfrom its MEG recording.
The classifier uses onetype of feature in an intermediate classificationstep.
For example, the classifier learns to predictthe MEG activity for any setting of the RNNLMhidden layer.
Given an unseen MEG recording foran unknown word i and two possible story wordsi?and i??
(one of which being the true word i), theclassifier predicts the MEG activity when readingi?and i?
?from their hidden layer vectors.
It thenassigns the label i?or i?
?to the word recording idepending on which prediction is the closest to therecording.
The following are the detailed steps ofthis complex classification task.
However, for therest of the paper the most useful point to keep inmind is that the main purpose of the classificationis to find a correspondence between the brain dataand a given feature set j.1.
Normalize the columns of M (zero mean,standard deviation = 1).
Pick feature set Fjand normalize its columns to a minimum of 0and a maximum of 1.2382.
Divide the data into 10 folds, for each fold b:(a) Isolate Mband Fbjas test data.
The re-mainder M?band F?bjwill be used fortraining4.
(b) Subtract the mean of the columns ofM?bfrom Mband M?band the meanof the columns of F?bjfrom Fbjand F?bj(c) Use ridge regression to solveM?b= F?bj?
?tjby tuning the ?
parameter to every oneof the p output dimensions indepen-dently.
?
is chosen via generalized crossvalidation (Golub et al., 1979).
(d) Perform a binary classification.
Samplefrom the set of words in b a set c of 20words.
Then sample from b another setof 20 words such that the kthword in cand d have the same number of letters.For every sample (c,d):i. predict the MEG data for c and d as:Pc= Fcj?
?bjand Pd= Fdj?
?bjii.
assign to Mcthe label c or d depend-ing on which of Pcor Pdis closest(Euclidean distance).iii.
assign to Mdthe label c or d de-pending on which of Pcor Pdisclosest (Euclidean distance).3.
Compute the average accuracy.2.3.3 Restricting the analysis spatially: asearchlight equivalentWe adapt the searchlight method (Kriegeskorte etal., 2006) to MEG.
The searchlight is a discoveryprocedure used in fMRI in which a cube is slidover the brain and an analysis is performed in eachlocation separately.
It allows to find regions in thebrain where a specific phenomenon is occurring.In the MEG sensor space, for every one of the 102sensor locations `, we assign a group of sensors g`.For every location `, we identify the locations thatimmediately surround it in any direction (Anterior,Right Anterior, Right etc...) when looking at the2D flat representation of the location of the sensorsin the MEG helmet (see Fig.
9 for an illustration ofthe 2D helmet).
g`therefore contains the 3 sensorsat location ` and at the neighboring locations.
Themaximum number of sensors in a group is 3 ?
9.4The rows from M?band F?bjthat correspond to the fivewords before or after the test set are ignored in order to makethe test set independent.The locations at the edge of the helmet have fewersensors because of the missing neighbor locations.2.3.4 Restricting the analysis temporallyInstead of using the entire time course of the word,we can use only one of the corresponding 100mstime windows.
Obtaining a high classification ac-curacy using one of the time windows and featureset j means that the analogous type of informationis encoded at that time.2.3.5 Classification accuracy by time andregionThe above steps compute whole brain accuracy us-ing all the time series.
In order to perform a moreprecise spatio-temporal analysis, one can use onlyone time windowm and one location ` for the clas-sification.
This can answer the question of whenand where different information is represented bybrain activity.
For every location, we will use onlythe columns corresponding to the time pointm forthe sensors belonging to the group g`.
Step (d) ofthe classification procedure is changed as such:(d) Perform a binary classification.
Sample fromthe set of words in b a set c of 20 words.
Thensample from b another set of 20 words suchthat the kthword in c and d have the samenumber of letters.
For every sample (c,d), andfor every setting of {m, `}:i. predict the MEG data for c and d as:Pc{m,`}= Fcj?
?bj,{m,`}andPd{m,`}= Fdj?
?bj,{m,`}ii.
assign to Mc{m,`}the label c or d depend-ing on which of Pc{m,`}or Pd{m,`}is clos-est (Euclidean distance).iii.
assign to Md{m,`}the label c or d depend-ing on which of Pc{m,`}or Pd{m,`}is clos-est (Euclidean distance).2.3.6 Statistical significance testingWe determine the distribution for chance perfor-mance empirically.
Because the successive wordsamples in our MEG and feature matrices are notindependent and identically distributed, we breakthe relationship between the MEG and feature ma-trices by shifting the feature matrices by large de-lays (e.g.
2000 to 2500 words) and we repeatthe classification using the delayed matrices.
Thissimulates chance performance more fairly than apermutation test because it keeps the time struc-ture of the matrices.
It was used in (Wehbe et al.,2392014) and inspired by (Chwialkowski and Gret-ton, 2014).
For every {m, `} setting we can there-fore compute a standardized z-value by subtract-ing the mean of the shifted classifications and di-viding by the standard deviation.
We then com-pute the p-value for the true classification accu-racy being due to chance.
Since the three p-valuesfor the three subjects for a given {m, `} are inde-pendent, we combine them using Fisher?s methodfor independent test statistics (Fisher, 1925).
Thestatistics we obtain for every {m, `} are depen-dent because they comprise nearby time and spacewindows.
We control the false discovery rate us-ing (Benjamini and Yekutieli, 2001) to adjust forthe testing at multiple locations and time windows.This method doesn?t assume any kind of indepen-dence or positive dependence.3 ResultsWe present in Fig.
5 the accuracy using all the timewindows and sensors.
In Fig.
6 we present theclassification accuracy when running the classifi-cation at every time window exclusively.
In Fig.
9we present the accuracy when running the classifi-cation using different time windows and groups ofsensors centered at every one of the 102 locations.It is important to lay down some conventionsto understand the complex results in these plots.To recap, we are trying to find parallels betweenmodel constituents and brain processes.
We use:?
a subset of the data (for example the timewindow 0-100ms and all the sensors)?
one type of feature (for example the hiddencontext layer from the NPLM 3g model)and we obtain a classification accuracy A.
If Ais low, there is probably no relationship betweenthe feature set and the subset of data.
If A is high,it hints to an association between the subset of dataand the mental process that is analogous to the fea-ture set.
For example, when using all the sensorsand time window 0-100ms, along with the NPLM3g hidden layer, we obtain an accuracy of 0.70(higher than chance with p < 10?14, see Fig.
6).Since the NPLM 3g hidden layer summarizes thecontext of the story before seeing word i, this sug-gests that the brain is still processing the contextof the story before word i between 0-100ms.Fig.
6 shows the accuracy for different typesof features when using all of the time points andall the sensors to classify a word.
We can seeNPLM 3g NPLM 5g RNNLM0.60.81classification accuracyhidden layer output probability embeddingshidden layer output probability embeddings0.60.81classification accuracyNPLM 3g NPLM 5g RNNLMFigure 5: Average accuracy using all time windows andsensors, grouped by model (top) and type of feature (bot-tom).
All accuracies are significantly higher than chance(p < 10?8).0 200 4000.50.60.7NPLM 3g0 200 4000.50.60.7NPLM 5g0 200 4000.50.60.7RNNLMhidden layeroutput probabilityembeddingsFigure 6: Average accuracy in different time windowswhen using different types of features as input to the clas-sifier, for different models.
Accuracy is plotted in the centerof the respective time window.
Points marked with a circleare significantly higher than chance accuracy for the givenfeature set and time window after correction.similar classification accuracies for the three typesof models, with RNNLM ahead for the hiddenlayer and embeddings and behind for the outputprobability features.
The hidden layer featuresare the most powerful for classification.
Betweenthe three types of features, the hidden layer fea-tures are the best at capturing the information con-tained in the brain data, suggesting that most ofthe brain activity is encoding the previous context.The embedding features are the second best.
Fi-nally the output probability have the smallest ac-curacies.
This makes sense considering that theycapture much less information than the other twohigh dimensional descriptive vectors, as they donot represent the complex properties of the words,only a numerical assessment of their likelihood.Fig.
6 shows the accuracy when using differentwindows of time exclusively, for the 100ms time240windows starting at 0, 100 .
.
.
400ms after wordpresentation.
We can see that using the embed-ding vector becomes increasingly more useful forclassification until 300-400ms, and then its perfor-mance starts decreasing.
This results aligns withthe following hypothesis: the word is being per-ceived and understood by the brain gradually afterits presentation, and therefore the brain represen-tation of the word becomes gradually similar to theneural network representation of the word (i.e.
theembedding vector).
The output probability featureaccuracy peaks at a later time than the embeddingsaccuracy.
Obtaining a higher than chance accu-racy at time window m using the output probabil-ity as input to the classifier suggests strongly thatthe brain is integrating the word at time windowm, because it is responding differently for pre-dictable and unpredictable words5.
The integra-tion step happens after the perception step, whichis probably why the output probability curves peaklater than the embeddings curves.
?500 0 500 10000.50.60.70.8Hidden LayerNPLM 3GNPLM 5GRNNLMFigure 7: Average accuracy in time for the different hiddenlayers.
The analysis is extended to the time windows beforeand after the word is presented, the input feature is restrictedto be the hidden layer before the central word is seen.
Thefirst vertical bar indicates the onset of the word, the secondone indicates the end of its presentation.
?1000 0 10000.60.8 Subject 1?1000 0 10000.60.8 Subject 2?1000 0 10000.60.8 Subject 3hidden layer output probability embeddingsFigure 8: Accuracy in time when using the RNNLM fea-tures for each of the three subjects.To understand the time dynamics of the hiddenlayer accuracy we need to see a larger time scalethan the word itself.
The hidden layer captures the5the fact that we can classify accurately during windows300-400ms indicates that the classifier is taking advantage ofthe N400 response discussed in the introductioncontext before word i is seen.
Therefore it seemsreasonable that the hidden layer is not only relatedto the activity when the word is on the screen, butalso related to the activity before the word is pre-sented, which is the time when the brain is inte-grating the previous words to build that context.On the other hand, as the word i and subsequentwords are integrated, the context starts divergingfrom the context of word i (computed before see-ing word i).
We therefore ran the same analysisas before, but this time we also included the timewindows before and after word i in the analysis,while maintaining the hidden layer vector to be thecontext before word i is seen.
We see the behav-ior we predicted in the results: the context beforeseeing word i becomes gradually more useful forclassification until word i is seen, and then it grad-ually decreases until it is no longer useful sincethe context has changed.
We observe the RNNLMhidden layer has a higher classification accuracythan the finite context NPLMs.
This might be dueto the fact that the RNNLM has a more completerepresentation of context that captures more of theproperties of the previous words.To show the consistency of the results, we plotas illustration the three curves we obtain for eachsubject for the RNNLM (Fig.
8).
The patternsseem very consistent indicating the phenomena wedescribed can be detected at the subject level.We now move on to the spatial decompositionof the analysis.
When the visual input enters thebrain, it first reaches the visual cortex at the backof the head, and then moves anteriorly towards theleft and right temporal cortices and eventually thefrontal cortex.
As it flows through these areas, itis processed to higher levels of interpretations.
InFig.
9, we plot the accuracy for different regionsof the brain and different time windows for theRNNLM features.
To make the plots simpler wemultiplied by zero the accuracies which were notsignificantly higher than chance.
We expand a fewcharacteristic plots.
We see that in the back of thehead the embedding features have an accuracy thatseems to peak very early on.
As we move forwardin the brain towards the left and right temporal cor-tices, we see the embeddings accuracy peaking ata later time, reflecting the delay it takes for the in-formation to reach this part of the brain.
The out-put probability start being useful for classificationafter the embeddings, and specifically in the lefttemporal cortex which is the cite where the N400241Back%hidden layeroutput probabilityembeddings 0 5000.50.60.7time (s)accuracyLe(% Right%hidden layeroutput probabilityembeddings 0 5000.50.60.7time (s)accuracyhidden layeroutput probabilityembeddings 0 5000.50.60.7time (s)accuracyhidden layeroutput probabilityembeddings0 5000.50.60.7time (s)accuracyhidden layeroutput probabilityembeddings0 5000.50.60.7time (s)accuracyhidden layeroutput probabilityembeddings0 5000.50.60.7time (s)accuracyFigure 9: Average accuracy in time and space on the MEG helmet when using the RNNLM features.
For each of the 102locations the average accuracy for the group of sensors centered at that location is plotted versus time.
The axes are definedin the rightmost, empty plot.
Three plots have been magnified to show the increasing delay in high accuracy when using theembeddings feature, reflecting the delay in processing the incoming word as information travels through the brain.
A sensormap is provided in the lower right corner: visual cortex = cyan, temporal = red, frontal = dark green.is reported in the literature.
Finally, as we reachthe frontal cortex, we see that the embeddings fea-tures have an even later accuracy peak.4 Conclusion and contributionsNovel brain data exploration We present herea novel and revealing approach to shed light onthe brain processes involved in reading.
This is adeparture from the classical approach of control-ling for a few variables in the text (e.g.
showinga sentence with an expected target word versus anunexpected one).
While we cannot make clear cutcausal claims because we did not control for ourvariables, we are able to explore the data muchmore and offer a much richer interpretation thanis possible with artificially constrained stimuli.Comparing two models of language Addingbrain data into the equation allowed us to com-pare the performance of the models and to identifya slight advantage for the RNNLM in capturingthe text contents.
Numerical comparison is how-ever a secondary contribution of our approach.
Weshowed that it might be possible to use brain datato understand, interpret and illustrate what exactlyis being encoded by the obscure vectors that neuralnetworks compute, by drawing parallels betweenthe models constituents and brain processes.Anecdotally, in the process of running the ex-periments, we noticed that the accuracy for thehidden layer of the RNNLM was peaking in thetime window corresponding to word i?2, and thatit was decreasing during word i ?
1.
Since thiswas against our expectations, we went back andlooked at the code and found that it was indeedreturning a delayed value and corrected the fea-tures.
We therefore used the brain data in order tocorrect a mis-specification in our neural networkmodel.
This hints if not proves the potential of ourapproach for assessing language models.Future Work The work described here is ourfirst attempt along the promising endeavor ofmatching complex computational models of lan-guage with brain processes using brain recordings.We plan to extend our efforts by (1) collecting datafrom more subjects and using various types of textand (2) make the brain data help us with trainingbetter statistical language models by using it to de-termine whether the models are expressive enoughor have reached a sufficient degree of convergence.AcknowledgementsThis research was supported in part by NICHDgrant 5R01HD07328-02.
We thank Nicole Rafidifor help with data acquisition.242ReferencesYoav Benjamini and Daniel Yekutieli.
2001.
The con-trol of the false discovery rate in multiple testing un-der dependency.
Annals of statistics, pages 1165?1188.Augusto Buchweitz, Robert A Mason, L?eda Tomitch,and Marcel Adam Just.
2009.
Brain activationfor reading and listening comprehension: An fMRIstudy of modality effects and individual differencesin language comprehension.
Psychology & neuro-science, 2(2):111?123.Kacper Chwialkowski and Arthur Gretton.
2014.A kernel independence test for random processes.arXiv preprint arXiv:1402.4501.Ronald Aylmer Fisher.
1925.
Statistical methods forresearch workers.
Genesis Publishing Pvt Ltd.Stefan L Frank, Leun J Otten, Giulia Galli, andGabriella Vigliocco.
2013.
Word surprisal predictsN400 amplitude during reading.
In Proceedings ofthe 51st annual meeting of the Association for Com-putational Linguistics, pages 878?883.Angela D Friederici.
2002.
Towards a neural basisof auditory sentence processing.
Trends in cognitivesciences, 6(2):78?84.Gene H Golub, Michael Heath, and Grace Wahba.1979.
Generalized cross-validation as a method forchoosing a good ridge parameter.
Technometrics,21(2):215?223.Peter Hagoort.
2003.
How the brain solves the bindingproblem for language: a neurocomputational modelof syntactic processing.
Neuroimage, 20:S18?S29.Nikolaus Kriegeskorte, Rainer Goebel, and Peter Ban-dettini.
2006.
Information-based functional brainmapping.
Proceedings of the National Academyof Sciences of the United States of America,103(10):3863?3868.Tomas Mikolov, Stefan Kombrink, Anoop Deoras,Lukar Burget, and J Cernocky.
2011.
RNNLM-recurrent neural network language modeling toolkit.In Proc.
of the 2011 ASRU Workshop, pages 196?201.Tomas Mikolov.
2012.
Statistical Language ModelsBased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.Vinod Nair and Geoffrey E. Hinton.
2010.
Rectifiedlinear units improve restricted Boltzmann machines.In Proceedings of ICML, pages 807?814.Joanne K. Rowling.
2012.
Harry Potter and the Sor-cerer?s Stone.
Harry Potter US.
Pottermore Limited.Riitta Salmelin.
2007.
Clinical neurophysiology oflanguage: the MEG approach.
Clinical Neurophysi-ology, 118(2):237?254.Gustavo Sudre, Dean Pomerleau, Mark Palatucci, LeilaWehbe, Alona Fyshe, Riitta Salmelin, and TomMitchell.
2012.
Tracking neural coding of percep-tual and semantic features of concrete nouns.
Neu-roImage, 62(1):451?463.Samu Taulu and Juha Simola.
2006.
Spatiotem-poral signal space separation method for rejectingnearby interference in MEG measurements.
Physicsin medicine and biology, 51(7):1759.Samu Taulu, Matti Kajola, and Juha Simola.
2004.Suppression of interference and artifacts by the sig-nal space separation method.
Brain topography,16(4):269?275.Ashish Vaswani, Yinggong Zhao, Victoria Fossum, andDavid Chiang.
2013.
Decoding with large-scaleneural language models improves translation.Leila Wehbe, Brian Murphy, Partha Talukdar, AlonaFyshe, Aaditya Ramdas, and Tom Mitchell.
2014.Simultaneously uncovering the patterns of brain re-gions involved in different story reading subpro-cesses.
in press.243
