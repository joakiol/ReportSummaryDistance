Proceedings of ACL-08: HLT, pages 148?155,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsSelecting Query Term Alterations for Web Search by Exploiting QueryContextsGuihong Cao Stephen Robertson Jian-Yun NieDept.
of Computer Science andOperations ResearchMicrosoft Research atCambridgeDept.
of Computer Science andOperations ResearchUniversity of Montreal, Canada Cambridge, UK University of Montreal, Canadacaogui@iro.umontreal.ca ser@microsoft.com nie@iro.umontreal.caAbstractQuery expansion by word alterations (alterna-tive forms of a word) is often used in Websearch to replace word stemming.
This allowsusers to specify particular word forms in aquery.
However, if many alterations areadded, query traffic will be greatly increased.In this paper, we propose methods to selectonly a few useful word alterations for queryexpansion.
The selection is made according tothe appropriateness of the alteration to thequery context (using a bigram languagemodel), or according to its expected impacton the retrieval effectiveness (using a regres-sion model).
Our experiments on two TRECcollections will show that both methods onlyselect a few expansion terms, but the retrievaleffectiveness can be improved significantly.1 IntroductionWord stemming is a basic NLP technique used inmost of Information Retrieval (IR) systems.
Ittransforms words into their root forms so as to in-crease the chance to match similar words/termsthat are morphological variants.
For example, withstemming, ?controlling?
can match ?controlled?because both have the same root ?control?.
Moststemmers, such as the Porter stemmer (Porter,1980) and Krovetz stemmer (Krovetz, 1993), dealwith stemming by stripping word suffixes accord-ing to a set of morphological rules.
Rule-based ap-proaches are intuitive and easy to implement.However, while in general, most words can bestemmed correctly; there is often erroneous stem-ming that unifies unrelated words.
For instance,?jobs?
is stemmed to ?job?
in both ?find jobs inApple?
and ?Steve Jobs at Apple?.
This is particu-larly problematic in Web search, where users oftenuse special or new words in their queries.
A stan-dard stemmer such as Porter?s will wrongly stemthem.To better determine stemming rules, Xu andCroft (1998) propose a selective stemming methodbased on corpus analysis.
They refine the Porterstemmer by means of word clustering: words arefirst clustered according to their co-occurrences inthe text collection.
Only word variants belongingto the same cluster will be conflated.Despite this improvement, the basic idea ofword stemming is to transform words in both doc-uments and queries to a standard form.
Once this isdone, there is no means for users to require a spe-cific word form in a query ?
the word form will beautomatically transformed, otherwise, it will notmatch documents.
This approach does not seem tobe appropriate for Web search, where users oftenspecify particular word forms in their queries.
Anexample of this is a quoted query such as ?SteveJobs?, or ?US Policy?.
If documents are stemmed,many pages about job offerings or US police maybe returned (?policy?
conflates with ?police?
inPorter stemmer).
Another drawback of stemming isthat it usually enhances recall, but may hurt preci-sion (Kraaij and Pohlmann, 1996).
However, gen-eral Web search is basically a precision-orientedtask.One alternative approach to word stemming is todo query expansion at query time.
The originalquery terms are expanded by their related formshaving the same root.
All expansions can be com-bined by the Boolean operator ?OR?.
For example,148the query ?controlling acid rain?
can be expandedto ?
(control OR controlling OR controller OR con-trolled OR controls) (acid OR acidic OR acidify)(rain OR raining OR rained OR rains)?.
We willcall each such expansion term an alteration to theoriginal query term.
Once a set of possible altera-tions is determined, the simplest approach to per-form expansion is to add all possible alterations.We call this approach Naive Expansion.
One caneasily show that stemming at indexing time isequivalent to Naive Expansion at retrieval time.This approach has been adopted by most commer-cial search engines (Peng et al, 2007).
However,the expansion approaches proposed previously canhave several serious problems: First, they usuallydo not consider expansion ambiguity ?
each queryterm is usually expanded independently.
However,some expansion terms may not be appropriate.
Thecase of ?Steve Jobs?
is one such example, forwhich the word ?job?
can be proposed as an ex-pansion term.
Second, as each query term mayhave several alterations, the na?ve approach usingall the alterations will create a very long query.
Asa consequence, query traffic (the time required forthe evaluation of a query) is greatly increased.Query traffic is a critical problem, as each searchengine serves millions of users at the same time.
Itis important to limit the query traffic as much aspossible.In practice, we can observe that some word al-terations are irrelevant and undesirable (as in the?Steve Jobs?
case), and some other alterations havelittle impact on the retrieval effectiveness (for ex-ample, if we expand a word by a rarely used wordform).
In this study, we will address these twoproblems.
Our goal is to select only appropriateword alterations to be used in query expansion.This is done for two purposes: On the one hand,we want to limit query traffic as much as possiblewhen query expansion is performed.
On the otherhand, we also want to remove irrelevant expansionterms so that fewer irrelevant documents will beretrieved, thereby improve the retrieval effective-ness.To deal with the two problems we mentionedabove, we will propose two methods to select al-terations.
In the first method, we make use of thequery context to select only the alterations that fitthe query.
The query context is modeled by a bi-gram language model.
To reduce query traffic, weselect only one alteration for each query term,which is the most coherent with the bigram model.We call this model Bigram Expansion.
Despite thefact that this method adds far fewer expansionterms than the na?ve expansion, our experimentswill show that we can achieve comparable or evenbetter retrieval effectiveness.Both the Naive Expansion and the Bigram Ex-pansion determine word alterations solely accord-ing to general knowledge about the language(bigram model or morphological rules), and noconsideration about the possible effect of the ex-pansion term is made.
In practice, some alterationswill have virtually no impact on retrieval effec-tiveness.
They can be ignored.
Therefore, in oursecond method, we will try to predict whether analteration will have some positive impact on re-trieval effectiveness.
Only the alterations with pos-itive impact will be retained.
In this paper, we willuse a regression model to predict the impact onretrieval effectiveness.
Compared to the bigramexpansion method, the regression method results ineven fewer alterations, but experiments show thatthe retrieval effectiveness is even better.Experiments will be conducted on two TRECcollections, Gov2 data for Web Track andTREC6&7&8 for ad-hoc retrieval.
The resultsshow that the two methods we propose both out-perform the original queries significantly with lessthan two alterations per query on average.
Com-pared to the Naive Expansion method, the two me-thods can perform at least equally well, whilequery traffic is dramatically reduced.In the following section, we provide a brief re-view of related work.
Section 3 shows how to gen-erate alteration candidates using a similar approachto Xu and Croft?s corpus analysis (1998).
In sec-tion 4 and 5, we describe the Bigram Expansionmethod and Regression method respectively.
Sec-tion 6 presents some experiments on TRECbenchmarks to evaluate our methods.
Section 7concludes this paper and suggests some avenuesfor future work.2 Related WorkMany stemmers have been implemented and usedas standard processing in IR.
Among them, thePorter stemmer (Porter, 1980) is the most widelyused.
It strips term suffixes step-by-step accordingto a set of morphological rules.
However, the Por-ter stemmer sometimes wrongly transforms a terminto an unrelated root.
For example, it will unify149?news?
and ?new?, ?execute?
and ?executive?.
Onthe other hand, it may miss some conflations, suchas ?mice?
and ?mouse?, ?europe?
and ?european?.Krovetz (1993) developed another stemmer, whichuses a machine-readable dictionary, to improve thePorter stemmer.
It avoids some of the Porterstemmer?s wrong stripping, but does not produceconsistent improvement in IR experiments.Both stemmers use generic rules for English tostrip each word in isolation.
In practice, the re-quired stemming may vary from one text collectionto another.
Therefore, attempts have been made touse corpus analysis to improve existing rule-basedstemmers.
Xu and Croft (1998) create equivalenceclusters of words which are morphologically simi-lar and occur in similar contexts.As we stated earlier, the stemming-based IR ap-proaches are not well suited to Web search.
Queryexpansion has been used as an alternative (Peng etal.
2007).
To limit the number of expansion terms,and thus the query traffic, Peng et al only use al-terations for some of the query words: They seg-ment each query into phrases and only the headword in each phrase is expanded.
The assumptionsare: 1)Queries issued in Web search often consistof noun phrases.
2) Only the head word in the nounphrase varies in form and needs to be expanded.However, both assumptions may be questionable.Their experiments did not show that the two as-sumptions hold.Stemming is related to query expansion or queryreformulation (Jones et al, 2006; Anick, 2003; Xuand Croft, 1996), although the latter is not limitedto word variants.
If the expansion terms used arethose that are variant forms of a word, then queryexpansion can produce the same effect as wordstemming.
However, if we add all possible wordalterations, query expansion/reformulation will runthe risk of adding many unrelated terms to theoriginal query, which may result in both heavytraffic and topic drift.
Therefore, we need a way toselect the most appropriate expansion terms.
In(Peng et al 2007), a bigram language model isused to determine the alteration of the head wordthat best fits the query.
In this paper, one of theproposed methods will also use a bigram languagemodel of the query to determine the appropriatealteration candidates.
However, in our approach,alterations are not limited to head words.
In addi-tion, we will also propose a supervised learningmethod to predict if an alteration will have a posi-tive impact on retrieval effectiveness.
To ourknowledge, no previous method uses the same ap-proach.In the following sections, we will describe ourapproach, which consists of two steps: the genera-tion of alteration candidates, and the selection ofappropriate alterations for a query.
The first step isquery-independent using corpus analysis, while thesecond step is query-dependent.
The selected wordalterations will be OR-ed with the original querywords.3  Generating Alteration CandidatesOur method to generate alteration candidates canbe described as follows.
First, we do word cluster-ing using a Porter stemmer.
All words in the vo-cabulary sharing the same root form are groupedtogether.
Then we do corpus analysis to filter outthe words which are clustered incorrectly, accord-ing to word distributional similarity, following (Xuand Croft, 1998; Lin 1998).
The rationale behindthis is that words sharing the same meaning tend tooccur in the same contexts.The context of each word in the vocabulary isrepresented by a vector containing the frequenciesof the context words which co-occur with the wordwithin a predefined window in a training corpus.The window size is set empirically at 3 words andthe training corpus is about 1/10 of the GOV2 cor-pus (see section 5 for details about the collection).Similarity is measured by the cosine distance be-tween two vectors.
For each word, we select atmost 5 similar words as alteration candidates.In the next sections, we will further consider waysto select appropriate alterations according to thequery.4 Bigram Expansion Model for AlterationSelectionIn this section, we try to select the most suitablealterations according to the query context.
Thequery context is modeled by a bigram languagemodel as in (Peng et al 2007).Given a query described by a sequence ofwords, we consider each of the query word as rep-resenting a concept ci.
In addition to the givenword form, ci can also be expressed by other alter-native forms.
However, the appropriate alterationsdo not only depend on the original word of ci, butalso on other query words or their alterations.150Figure 1: Considering all Combinations to Calculate thePlausibility of AlterationsAccordingly, a confidence weight is determinedfor each alteration candidate.
For example, in thequery ?Steve Jobs at Apple?, the alteration ?job?
of?jobs?
should have a low confidence; while in thequery ?finding jobs in Apple?, it should have ahigh confidence.One way to measure the confidence of an altera-tion is the plausibility of its appearing in the query.Since each concept may be expressed by severalalterations, we consider all the alterations of con-text concepts when calculating the plausibility of agiven word.
Suppose we have the query ?control-ling acid rain?.
The second concept has two altera-tions - ?acidify?
and ?acidic?.
For each of thealterations, our method will consider all the com-binations with other words, as illustrated in figure1, where each combination is shown as a path.More precisely, for a query of n words (or theircorresponding concepts), let ei,j?ci, j=1,2,?,|ci| bethe alterations of concept ci.
Then we have:??
?
?
?== = =?
=+?
?++=||1, ,,,2,1||1,1||1,2||1,1||1,1),...,,...,,(.........)(2111221111nn niiiiicjn jnjijjcjcjcjicjiijeeeePeP(1)In equation 1,ni jnjijj eeee ,,,2,1 ,...,,...,, 21 is a pathpassing through ei,j.
For simplicity, we abbreviate itas e1e2?ei?en.
In this work, we used bigram lan-guage model to calculate the probability of eachpath.
Then we have:?=?=nk kknieePePeeeeP2 1121)|()(),...,,...,,(               (2)P(ek|ek-1) is estimated with a back-off bigram lan-guage model (Goodman, 2001).
In the experimentswith TREC6&7&8, we train the model with alltext collections; while in the experiments withGov2 data, we only used about 1/10 of the GOV2data to train the bigram model because the wholeGov2 collection is too large.Directly calculating P(eij) by summing the prob-abilities of all paths passing through eij is an NPproblem (Rabiner, 1989), and is intractable if thequery is long.
Therefore, we use the forward-backward algorithm (Bishop, 2006) to calculateP(eij) in a more efficient way.
After calculatingP(eij) for each ci, we select one alteration whichhas the highest probability.
We limit the number ofadditional alterations to 1 in order to limit querytraffic.
Our experiments will show that this is oftensufficient.5 Regression Model for Alteration Selec-tionNone of the previous selection methods considershow well an alteration would perform in retrieval.The Bigram Expansion model assumes that thequery replaced with better alterations should havea higher likelihood.
This approach belongs to thefamily of unsupervised learning.
In this section, weintroduce a method belonging to supervised learn-ing family.
This method develops a regressionmodel from a set of training data, and it is capableof predicting the expected change in performancewhen the original query is augmented by this al-teration.
The performance change is measured bythe difference in the Mean Average Precision(MAP) between the augmented and the originalquery.
The training instances are defined by theoriginal query string, an original query term underconsideration and one alteration to the query term.A set of features will be used, which will be de-fined later in this section.5.1 Linear Regression ModelThe goal of the regression model is to predict theperformance change when a query term is aug-mented with an alteration.
There are several re-gression models, ranging from the simplest linearregression model to non-linear alternatives, such asa neural network (Duda et al, 2001), a RegressionSVM (Bishop, 2006).
For simplicity, we use linearregression model here.
We denote an instance inthe feature space as X, and the weights of featuresare denoted as W. Then the linear regression modelis defined as:f(X)=WTX                                                             (3)where WT is the transpose of W. However, we willhave a technical problem if we set the target valueto the performance change directly: The range ofcontrollingcontrolcontrolledcontrolleracidifyacidicrainrainsraining151values of f(X) is ),( +???
, while the range of per-formance change is [-1,1].
The two value ranges donot match.
This inconsistency may result in severeproblems when the scales of feature values varydramatically (Duda et al, 2001).
To solve thisproblem, we do a simple transformation on the per-formance change.
Let the change be ]1,1[?
?y , thenthe transformed performance change is:]1,1[11log)( ?
?+?++= yyyy???
(4)where ?
is a very small positive real number (set tobe 1e-37 in the experiments), which acts as asmoothing factor.
The value of )(y?
can be an arbi-trary real number.
)(y?
is a monotonic functiondefined in the range of [-1,1].
Moreover, the fixedpoint of )(y?
is 0, i.e., yy =)(?
when y=0.
Thisproperty is nice; it means that the expansion bringspositive improvement if and only if f(X)>0, whichmakes it easy to determine which alteration is bet-ter.We train the regression model by minimizingthe mean square error.
Suppose there are traininginstances X1,X2,?,Xm, and the corresponding per-formance change is yi, i=1,2,?,m.
We calculatethe mean square error with the following equation:?=?=mi iiT yXWWerr12))(()( ?
(5)Then the optimal weight is defined as:?=?==mi iiTWWyXWWerrW12*))((minarg)(minarg?
(6)Because err(W) is a convex function of W, it hasa global minimum and obtains its minimum whenthe gradient is zero (Bazaraa et al, 2006).
Then wehave:0))(()(1**=?=??
?=miTiiiT XyXWWWerr ?So,  ?
?===miTiimiTiiT XyXXW11* )(?In fact,  ?=miTii XX1  is a square matrix, we denoteit as XXT.
Then we have: [ ]?=?=mi iiT XyXXW11* )()( ?
(7)The matrix  XXT is an ll ?
square matrix, where lis the number of features.
In our experiments, weonly use three features.
Therefore the optimalweights can be calculated efficiently even we havea large number of training instances.5.2 Constructing Training DataAs a supervised learning method, the regressionmodel is trained with a set of training data.
Weillustrate here the procedure to generate traininginstances with an example.Given a query ?controlling acid rain?, we obtainthe MAP of the original query at first.
Then weaugment the query with an alteration to the originalterm (one term at a time) at each time.
We retainthe MAP of the augmented query and compare itwith the original query to obtain the performancechange.
For this query, we expand ?controlling?
by?control?
and get an augmented query ?
(control-ling OR control) acid rain?.
We can obtain the dif-ference between the MAP of the augmented queryand that of the original query.
By doing this, wecan generate a series of training instances consist-ing of the original query string, the original queryterm under consideration, its alteration and the per-formance change, for example:<controlling acid rain, controlling, control,  0.05>Note that we use MAP to measure performance,but we could well use other metrics such as NDCG(Peng et al, 2007) or P@N (precision at top-Ndocuments).5.3 Features Used for Regression ModelThree features are used.
The first feature reflects towhat degree an alteration is coherent with the otherterms.
For example, for the query ?controlling acidrain?, the coherence of the alteration ?acidic?
ismeasured by the logarithm of its co-occurrencewith the other query terms within a predefinedwindow (90 words) in the corpus.
That is:log(count(controlling?acidic?rain|window)+0.5)where ???
means there may be some words be-tween two query terms.
Word order is ignored.The second feature is an extension to point-wisemutual information (Rijsbergen, 1979), defined asfollows:????????)()()()|......
(lograinPacidicPgcontrollinPwindowrainacidicgcontrollinPwhere P(controlling?acidic?rain|window) is theco-occurrence probability of the trigram containingacidic within a predefined window (50 words).P(controlling), p(acidic), P(rain) are probabilitiesof the three words in the collection.
The threewords are defined as: the term under consideration,the first term to the left of that term, and the firstterm to the right.
If a query contains less than 3152terms or the term under consideration is the begin-ning/ending term in the query, we will set theprobability of the missed term/terms to be 1.Therefore, it becomes point-wise mutual informa-tion when the query contains only two terms.
Infact, this feature is supplemental to the first feature.When the query is very long and the first featurealways obtains a value of log(0.5), so it does nothave any discriminative ability.
On the other hand,the second feature helps because it can capturesome co-occurrence information no matter howlong the query is.The last feature is the bias, whose value is al-ways set to be 1.0.The regression model is trained in a leave-one-out cross-validation manner on three collections;each of them is used in turn as a test collectionwhile the two others are used for training.
Foreach incoming query, the regression model pre-dicts the expected performance change when onealteration is used.
For each query term, we onlyselect the alteration with the largest positive per-formance change.
If none of its alterations producea positive performance change, we do not expandthe query term.
This selection is therefore morerestrictive than the Bigram Expansion Model.Nevertheless, our experiments show that it im-proves retrieval effectiveness further.6 Experiments6.1 Experiment SettingsIn this section, our aim is to evaluate the two con-text-sensitive word alteration selection methods.The ideal evaluation corpus should be composed ofsome Web data.
Unfortunately, such data are notpublicly available and the results also could not becompared with other published results.
Therefore,we use two TREC collections.
The first one is thead-hoc retrieval test collections used forTREC6&7& 8.
This collection is relative small andhomogeneous.
The second one is the Gov2 data.
Itis obtained by crawling the entire .gov domain andhas been used for three TREC Terabyte tracks(TREC2004-2006).
Table 1 shows some statisticsof the two collections.
For each collection, we use150 queries.
Since the Regression model needssome data for training, we divided the queries intothree parts, each containing 50 queries.
We thenuse leave-one-out cross-validation.
The evaluationmetrics shown below are the average value of theName Description Size(GB)#Doc QueryTREC6&7&8TREC disk4&5,Newpapers1.7 500,447 301-450Gov2 2004 crawl of entire.gov domain427 25,205,179 701-850Table1: Overview of Test Collectionsthree-fold cross-validation.
Because the queries inWeb are usually very short, we use only the titlefield of each query.To correspond to Web search practice, bothdocuments and queries are not stemmed.
We donot filter the stop words either.Two main metrics are used: the Mean AveragePrecision (MAP) for the top 1000 documents tomeasure retrieval effectiveness, and the number ofterms in the query to reflect query traffic.
In addi-tion, we also provide precision for the top 30 doc-uments (P@30) to show the impact on top rankeddocuments.
We also conducted t-tests to determinewhether the improvement is statistically significant.The Indri 2.5 search engine (Strohman et al,2004) is used as our basic retrieval system.
It pro-vides for a rich query language allowing disjunc-tive combinations of words in queries.6.2 Experimental ResultsThe first baseline method we compare with onlyuses the original query, which is named Original.In addition to this, we also compare with the fol-lowing methods:Na?ve Exp: The Na?ve expansion model expandseach query term with all terms in the vocabu-lary sharing the same root with it.
This model isequivalent to the traditional stemming method.UMASS: This is the result reported in (Metzler et al,2006) using Porter stemming for both documentand query terms.
This reflects a state-of-the-artresult using Porter stemming.Similarity: We select the alterations (at most 5)with the highest similarity to the original term.This is the method described in section 3.The two methods we propose in this paper are thefollowing ones:Bigram Exp: the alteration is chosen by a BigramExpansion model.Regression: the alteration is chosen by a Regres-sion model.153Model P@30 #term MAP Imp.Original 0.4701 158 0.2440 ----UMASS ------- ------- 0.2666 9.26Na?ve Exp 0.4714 1345 0.2653 8.73Similarity 0.4900 303 0.2689 10.20*Bigram Exp 0.5007 303 0.2751 12.75**Regression 0.5054 237 0.2773 13.65**Table 2: Results of Query 701-750 Over Gov2 DataModel P@30 #term MAP Imp.Original 0.4907 158 0.2738 ----UMASS ------- ------- 0.3251 18.73Naive Exp 0.5213 1167 0.3224 17.75**Similarity 0.5140 290 0.3043 11.14**Bigram Exp.
0.5153 290 0.3107 13.47**Regression 0.5140 256 0.3144 14.82**Table 3: Results of Query 751-800 over Gov2 DataModel P@30 #term MAP Imp.Original 0.4710 154 0.2887 ----UMASS ------- ------- 0.2996 3.78Na?ve Exp 0.4633 1225 0.2999 3.87Similarity 0.4710 288 0.2976 3.08Bigram Exp 0.4730 288 0.3137 8.66**Regression 0.4748 237 0.3118 8.00*Table 4: Results of Query 801-850 over Gov2 DataModel P@30 #term MAP Imp.Original 0.2673 137 0.1669 ----Na?ve Exp 0.3053 783 0.2146 28.57**Similarity 0.3007 255 0.2020 21.03**Bigram Exp 0.3033 255 0.2091 25.28**Regression 0.3113 224 0.2161 29.48**Table 5: Results of Query 301-350 over TREC6&7&8Model P@30 #term MAP Imp.Original 0.2820 126 0.1639 -----Naive Exp 0.2787 736 0.1665 1.59Similarity 0.2867 244 0.1650 0.67Bigram Exp.
0.2800 244 0.1641 0.12Regression 0.2867 214 0.1664 1.53Table 6: Results of Query 351-400 over TREC6&7&8Model P@30 #term MAP Imp.Original 0.2833 124 0.1759 -----Na?ve Exp 0.3167 685 0.2138 21.55**Similarity 0.3080 240 0.2066 17.45**Bigram Exp 0.3133 240 0.2080 18.25**Regression 0.3220 187 0.2144 21.88**Table7: Results of Query 401-450 over TREC6&7&8Tables 2, 3, 4 show the results of Gov2 datawhile table 5, 6, 7 show the results of theTREC6&7&8 collection.
In the tables, the * markindicates that the improvement over the originalmodel is statistically significant with p-value<0.05,and ** means the p-values<0.01.From the tables, we see that both word stem-ming (UMASS) and expansion with word altera-tions can improve MAP for all six tasks.
In mostcases (except in table 4 and 6), it also improve theprecision of top ranked documents.
This shows theusefulness of word stemming or word alterationexpansion for IR.We can make several additional observations:1).
Stemming Vs Expansion.
UMASS uses docu-ment and query stemming while Naive Exp usesexpansion by word alteration.
We stated that bothapproaches are equivalent.
The equivalence isconfirmed by our experiment results: for all Gov2collections, these approaches perform equiva-lently.2).
The Similarity model performs very well.
Com-pared with the Na?ve Expansion model, it pro-duces quite similar retrieval effectiveness, whilethe query traffic is dramatically reduced.
Thisapproach is similar to the work of Xu and Croft(1998), and can be considered as another state-of-the-art result.3).
In comparison, the Bigram Expansion modelperforms better than the Similarity model.
Thisshows that it is useful to consider query contextin selecting word alterations.4).
The Regression model performs the best of allthe models.
Compared with the Original query, itadds fewer than 2 alterations for each query onaverage (since each group has 50 queries); never-theless we obtained improvements on all the sixcollections.
Moreover, the improvements on fivecollections are statistically significant.
It also per-forms slightly better than the Similarity and Bi-gram Expansion methods, but with feweralterations.
This shows that the supervised learn-ing approach, if used in the correct way, is supe-rior to an unsupervised approach.
Anotheradvantage over the two other models is that theRegression model can reduce the number of al-terations further.
Because the Regression modelselects alterations according to their expectedimprovement, the improvement of the alterationsto one query term can be compared with that ofthe alterations to other query terms.
Therefore,we can select at most one optimal alteration forthe whole query.
However, with the Similarity orBigram Expansion models, the selection value,either similarity or query likelihood, cannot be154compared across the query terms.
As a conse-quence, more alterations need to be selected,leading to heavier query traffic.7 ConclusionTraditional IR approaches stem terms in both doc-uments and queries.
This approach is appropriatefor general purpose IR, but is ill-suited for the spe-cific retrieval needs in Web search such as quotedqueries or queries with a specific word form thatshould not be stemmed.
The current practice inWeb search is not to stem words in index, but ra-ther to perform a form of expansion using wordalteration.However, a na?ve expansion will result in manyalterations and this will increase the query traffic.This paper has proposed two alternative methodsto select precise alterations by considering thequery context.
We seek to produce similar or betterimprovements in retrieval effectiveness, while lim-iting the query traffic.In the first method proposed ?
the Bigram Ex-pansion model, query context is modeled by a bi-gram language model.
For each query term, theselected alteration is the one which maximizes thequery likelihood.
In the second method - Regres-sion model, we fit a regression model to calculatethe expected improvement when the original queryis expanded by an alteration.
Only the alterationthat is expected to yield the largest improvement toretrieval effectiveness is added.The proposed methods were evaluated on twoTREC benchmarks: the ad-hoc retrieval test collec-tion for TREC6&7&8 and the Gov2 data.
Our ex-perimental results show that both proposedmethods perform significantly better than the orig-inal queries.
Compared with traditional wordstemming or the na?ve expansion approach, ourmethods can not only  improve retrieval effective-ness, but also greatly reduce the query traffic.This work shows that query expansion withword alterations is a reasonable alternative to wordstemming.
It is possible to limit the query traffic bya query-dependent selection of word alterations.Our work shows that both unsupervised and super-vised learning can be used to perform alterationselection.Our methods can be further improved in severalaspects.
For example, we could integrate other fea-tures in the regression model, and use other non-linear regression models, such as Bayesian regres-sion models (e.g.
Gaussian Process regression)(Rasmussen and Williams, 2006).
The additionaladvantage of these models is that we can not onlyobtain the expected improvement in retrieval effec-tiveness for an alteration, but also the probabilityof obtaining an improvement (i.e.
the robustness ofthe alteration).Finally, it would be interesting to test the ap-proaches using real Web data.ReferencesAnick, P. (2003) Using Terminological Feedback forWeb Search Refinement: a Log-based Study.
InSIGIR, pp.
88-95.Bazaraa, M., Sherali, H., and Shett, C. (2006).
Nonlin-ear Programming, Theory and Algorithms.
JohnWiley & Sons Inc.Bishop, C. (2006).
Pattern Recognition and MachineLearning.
Springer.Duda, R.,  Hart, P.,  and Stork, D. (2001).
Pattern Clas-sification, John Wiley & Sons, Inc.Goodman, J.
(2001).
A Bit of Progress in LanguageModeling.
Technical report.Jones, R., Rey, B., Madani, O., and Greiner, W. (2006).Generating Query Substitutions.
In WWW2006, pp.387-396Kraaij, W. and Pohlmann, R. (1996) Viewing Stemmingas Recall Enhancement.
Proc.
SIGIR, pp.
40-48.Krovetz, R. (1993).
Viewing Morphology as an Infer-ence Process.
Proc.
ACM SIGIR, pp.
191-202.Lin, D. (1998).
Automatic Retrieval and Clustering ofSimilar Words.
In COLING-ACL, pp.
768-774.Metzler, D., Strohman, T. and Croft, B.
(2006).
IndriTREC Notebook 2006: Lessons learned from ThreeTerabyte Tracks.
In the Proceedings of TREC 2006.Peng, F., Ahmed, N., Li, X., and Lu, Y.
(2007).
ContextSensitive Stemming for Web Search.
Proc.
ACMSIGIR, pp.
639-636 .Porter, M. (1980) An Algorithm for Suffix Stripping.Program, 14(3): 130-137.Rabiner, L. (1989).
A Tutorial on Hidden Markov Mod-els and Selected Applications in Speech Recognition.In Proceedings of IEEE Vol.
77(2), pp.
257-286.Rijsbergen, V. (1979).
Information Retrieval.
Butter-worths, second version.Strohman, T., Metzler, D. and Turtle, H., and Croft, B.(2004).
Indri: A Language Model-based Search En-gine for Complex Queries.
In Proceedings of the In-ternational conference on Intelligence Analysis.Xu, J. and Croft, B.
(1996).
Query Expansion UsingLocal and Global Document Analysis.
Proc.
ACMSIGIR, pp.
4-11.Xu, J. and Croft, B.
(1998).
Corpus-based StemmingUsing Co-occurrence of Word Variants.
ACMTOIS, 16(1): 61-81.155
