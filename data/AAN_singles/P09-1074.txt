Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 656?664,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPConundrums in Noun Phrase Coreference Resolution:Making Sense of the State-of-the-ArtVeselin StoyanovCornell UniversityIthaca, NYves@cs.cornell.eduNathan GilbertUniversity of UtahSalt Lake City, UTngilbert@cs.utah.eduClaire CardieCornell UniversityIthaca, NYcardie@cs.cornell.eduEllen RiloffUniversity of UtahSalt Lake City, UTriloff@cs.utah.eduAbstractWe aim to shed light on the state-of-the-art in NPcoreference resolution by teasing apart the differ-ences in the MUC and ACE task definitions, the as-sumptions made in evaluation methodologies, andinherent differences in text corpora.
First, we exam-ine three subproblems that play a role in coreferenceresolution: named entity recognition, anaphoric-ity determination, and coreference element detec-tion.
We measure the impact of each subproblem oncoreference resolution and confirm that certain as-sumptions regarding these subproblems in the eval-uation methodology can dramatically simplify theoverall task.
Second, we measure the performanceof a state-of-the-art coreference resolver on severalclasses of anaphora and use these results to developa quantitative measure for estimating coreferenceresolution performance on new data sets.1 IntroductionAs is common for many natural language process-ing problems, the state-of-the-art in noun phrase(NP) coreference resolution is typically quantifiedbased on system performance on manually anno-tated text corpora.
In spite of the availability ofseveral benchmark data sets (e.g.
MUC-6 (1995),ACE NIST (2004)) and their use in many formalevaluations, as a field we can make surprisinglyfew conclusive statements about the state-of-the-art in NP coreference resolution.In particular, it remains difficult to assess the ef-fectiveness of different coreference resolution ap-proaches, even in relative terms.
For example, the91.5 F-measure reported by McCallum and Well-ner (2004) was produced by a system using perfectinformation for several linguistic subproblems.
Incontrast, the 71.3 F-measure reported by Yang etal.
(2003) represents a fully automatic end-to-endresolver.
It is impossible to assess which approachtruly performs best because of the dramaticallydifferent assumptions of each evaluation.Results vary widely across data sets.
Corefer-ence resolution scores range from 85-90% on theACE 2004 and 2005 data sets to a much lower 60-70% on the MUC 6 and 7 data sets (e.g.
Soon et al(2001) and Yang et al (2003)).
What accounts forthese differences?
Are they due to properties ofthe documents or domains?
Or do differences inthe coreference task definitions account for the dif-ferences in performance?
Given a new text collec-tion and domain, what level of performance shouldwe expect?We have little understanding of which aspectsof the coreference resolution problem are handledwell or poorly by state-of-the-art systems.
Ex-cept for some fairly general statements, for exam-ple that proper names are easier to resolve thanpronouns, which are easier than common nouns,there has been little analysis of which aspects ofthe problem have achieved success and which re-main elusive.The goal of this paper is to take initial steps to-ward making sense of the disparate performanceresults reported for NP coreference resolution.
Forour investigations, we employ a state-of-the-artclassification-based NP coreference resolver andfocus on the widely used MUC and ACE corefer-ence resolution data sets.We hypothesize that performance variationwithin and across coreference resolvers is, at leastin part, a function of (1) the (sometimes unstated)assumptions in evaluation methodologies, and (2)the relative difficulty of the benchmark text cor-pora.
With these in mind, Section 3 first examinesthree subproblems that play an important role incoreference resolution: named entity recognition,anaphoricity determination, and coreference ele-ment detection.
We quantitatively measure the im-pact of each of these subproblems on coreferenceresolution performance as a whole.
Our resultssuggest that the availability of accurate detectorsfor anaphoricity or coreference elements couldsubstantially improve the performance of state-of-the-art resolvers, while improvements to namedentity recognition likely offer little gains.
Our re-sults also confirm that the assumptions adopted in656MUC ACERelative Pronouns no yesGerunds no yesNested non-NP nouns yes noNested NEs no GPE & LOC premodSemantic Types all 7 classes onlySingletons no yesTable 1: Coreference Definition Differences for MUC andACE.
(GPE refers to geo-political entities.
)some evaluations dramatically simplify the resolu-tion task, rendering it an unrealistic surrogate forthe original problem.In Section 4, we quantify the difficulty of atext corpus with respect to coreference resolutionby analyzing performance on different resolutionclasses.
Our goals are twofold: to measure thelevel of performance of state-of-the-art corefer-ence resolvers on different types of anaphora, andto develop a quantitative measure for estimatingcoreference resolution performance on new datasets.
We introduce a coreference performance pre-diction (CPP) measure and show that it accuratelypredicts the performance of our coreference re-solver.
As a side effect of our research, we pro-vide a new set of much-needed benchmark resultsfor coreference resolution under common sets offully-specified evaluation assumptions.2 Coreference Task DefinitionsThis paper studies the six most commonly usedcoreference resolution data sets.
Two of those arefrom the MUC conferences (MUC-6, 1995; MUC-7, 1997) and four are from the Automatic Con-tent Evaluation (ACE) Program (NIST, 2004).
Inthis section, we outline the differences between theMUC and ACE coreference resolution tasks, anddefine terminology for the rest of the paper.Noun phrase coreference resolution is the pro-cess of determining whether two noun phrases(NPs) refer to the same real-world entity or con-cept.
It is related to anaphora resolution: a NP issaid to be anaphoric if it depends on another NPfor interpretation.
Consider the following:John Hall is the new CEO.
He starts on Monday.Here, he is anaphoric because it depends on its an-tecedent, John Hall, for interpretation.
The twoNPs also corefer because each refers to the sameperson, JOHN HALL.As discussed in depth elsewhere (e.g.
vanDeemter and Kibble (2000)), the notions of coref-erence and anaphora are difficult to define pre-cisely and to operationalize consistently.
Further-more, the connections between them are extremelycomplex and go beyond the scope of this paper.Given these complexities, it is not surprising thatthe annotation instructions for the MUC and ACEdata sets reflect different interpretations and sim-plifications of the general coreference relation.
Weoutline some of these differences below.Syntactic Types.
To avoid ambiguity, we willuse the term coreference element (CE) to referto the set of linguistic expressions that participatein the coreference relation, as defined for each ofthe MUC and ACE tasks.1 At times, it will be im-portant to distinguish between the CEs that are in-cluded in the gold standard ?
the annotated CEs?
from those that are generated by the corefer-ence resolution system ?
the extracted CEs.At a high level, both the MUC and ACE eval-uations define CEs as nouns, pronouns, and nounphrases.
However, the MUC definition excludes(1) ?nested?
named entities (NEs) (e.g.
?Amer-ica?
in ?Bank of America?
), (2) relative pronouns,and (3) gerunds, but allows (4) nested nouns (e.g.?union?
in ?union members?).
The ACE defini-tion, on the other hand, includes relative pronounsand gerunds, excludes all nested nouns that are notthemselves NPs, and allows premodifier NE men-tions of geo-political entities and locations, suchas ?Russian?
in ?Russian politicians?.Semantic Types.
ACE restricts CEs to entitiesthat belong to one of seven semantic classes: per-son, organization, geo-political entity, location, fa-cility, vehicle, and weapon.
MUC has no semanticrestrictions.Singletons.
The MUC data sets include annota-tions only for CEs that are coreferent with at leastone other CE.
ACE, on the other hand, permits?singleton?
CEs, which are not coreferent withany other CE in the document.These substantial differences in the task defini-tions (summarized in Table 1) make it extremelydifficult to compare performance across the MUCand ACE data sets.
In the next section, we take acloser look at the coreference resolution task, ana-lyzing the impact of various subtasks irrespectiveof the data set differences.1We define the term CE to be roughly equivalent to (a)the notion of markable in the MUC coreference resolutiondefinition and (b) the structures that can be mentions in thedescriptions of ACE.6573 Coreference Subtask AnalysisCoreference resolution is a complex task thatrequires solving numerous non-trivial subtaskssuch as syntactic analysis, semantic class tagging,pleonastic pronoun identification and antecedentidentification to name a few.
This section exam-ines the role of three such subtasks ?
named en-tity recognition, anaphoricity determination, andcoreference element detection ?
in the perfor-mance of an end-to-end coreference resolutionsystem.
First, however, we describe the corefer-ence resolver that we use for our study.3.1 The RECONCILEACL09 CoreferenceResolverWe use the RECONCILE coreference resolutionplatform (Stoyanov et al, 2009) to configure acoreference resolver that performs comparably tostate-of-the-art systems (when evaluated on theMUC and ACE data sets under comparable as-sumptions).
This system is a classification-basedcoreference resolver, modeled after the systems ofNg and Cardie (2002b) and Bengtson and Roth(2008).
First it classifies pairs of CEs as coreferentor not coreferent, pairing each identified CE withall preceding CEs.
The CEs are then clusteredinto coreference chains2 based on the pairwise de-cisions.
RECONCILE has a pipeline architecturewith four main steps: preprocessing, feature ex-traction, classification, and clustering.
We willrefer to the specific configuration of RECONCILEused for this paper as RECONCILEACL09.Preprocessing.
The RECONCILEACL09 prepro-cessor applies a series of language analysis tools(mostly publicly available software packages) tothe source texts.
The OpenNLP toolkit (Baldridge,J., 2005) performs tokenization, sentence splitting,and part-of-speech tagging.
The Berkeley parser(Petrov and Klein, 2007) generates phrase struc-ture parse trees, and the de Marneffe et al (2006)system produces dependency relations.
We em-ploy the Stanford CRF-based Named Entity Rec-ognizer (Finkel et al, 2004) for named entitytagging.
With these preprocessing components,RECONCILEACL09 uses heuristics to correctly ex-tract approximately 90% of the annotated CEs forthe MUC and ACE data sets.Feature Set.
To achieve roughly state-of-the-art performance, RECONCILEACL09 employs a2A coreference chain refers to the set of CEs that refer toa particular entity.dataset docs CEs chains CEs/ch tr/tst splitMUC6 60 4232 960 4.4 30/30 (st)MUC7 50 4297 1081 3.9 30/20 (st)ACE-2 159 2630 1148 2.3 130/29 (st)ACE03 105 3106 1340 2.3 74/31ACE04 128 3037 1332 2.3 90/38ACE05 81 1991 775 2.6 57/24Table 2: Dataset characteristics including the number ofdocuments, annotated CEs, coreference chains, annotatedCEs per chain (average), and number of documents in thetrain/test split.
We use st to indicate a standard train/test split.fairly comprehensive set of 61 features introducedin previous coreference resolution systems (seeBengtson and Roth (2008)).
We briefly summarizethe features here and refer the reader to Stoyanovet al (2009) for more details.Lexical (9): String-based comparisons of the twoCEs, such as exact string matching and head nounmatching.Proximity (5): Sentence and paragraph-basedmeasures of the distance between two CEs.Grammatical (28): A wide variety of syntacticproperties of the CEs, either individually or as apair.
These features are based on part-of-speechtags, parse trees, or dependency relations.
For ex-ample: one feature indicates whether both CEs aresyntactic subjects; another indicates whether theCEs are in an appositive construction.Semantic (19): Capture semantic informationabout one or both NPs such as tests for gender andanimacy, semantic compatibility based on Word-Net, and semantic comparisons of NE types.Classification and Clustering.
We configureRECONCILEACL09 to use the Averaged Percep-tron learning algorithm (Freund and Schapire,1999) and to employ single-link clustering (i.e.transitive closure) to generate the final partition-ing.33.2 Baseline System ResultsOur experiments rely on the MUC and ACE cor-pora.
For ACE, we use only the newswire portionbecause it is closest in composition to the MUCcorpora.
Statistics for each of the data sets areshown in Table 2.
When available, we use thestandard test/train split.
Otherwise, we randomlysplit the data into a training and test set followinga 70/30 ratio.3In trial runs, we investigated alternative classificationand clustering models (e.g.
C4.5 decision trees and SVMs;best-first clustering).
The results were comparable.658Scoring Algorithms.
We evaluate using twocommon scoring algorithms4 ?
MUC and B3.The MUC scoring algorithm (Vilain et al, 1995)computes the F1 score (harmonic mean) of preci-sion and recall based on the identifcation of uniquecoreference links.
We use the official MUC scorerimplementation for the two MUC corpora and anequivalent implementation for ACE.The B3 algorithm (Bagga and Baldwin, 1998)computes a precision and recall score for each CE:precision(ce) = |Rce ?Kce|/|Rce|recall(ce) = |Rce ?Kce|/|Kce|,where Rce is the coreference chain to which ce isassigned in the response (i.e.
the system-generatedoutput) and Kce is the coreference chain that con-tains ce in the key (i.e.
the gold standard).
Pre-cision and recall for a set of documents are com-puted as the mean over all CEs in the documentsand the F1 score of precision and recall is reported.B3 Complications.
Unlike the MUC score,which counts links between CEs, B3 presumesthat the gold standard and the system response areclusterings over the same set of CEs.
This, ofcourse, is not the case when the system automat-ically identifies the CEs, so the scoring algorithmrequires a mapping between extracted and anno-tated CEs.
We will use the term twin(ce) to referto the unique annotated/extracted CE to which theextracted/annotated CE is matched.
We say thata CE is twinless (has no twin) if no correspondingCE is identified.
A twinless extracted CE signalsthat the resolver extracted a spurious CE, while anannotated CE is twinless when the resolver fails toextract it.Unfortunately, it is unclear how the B3 scoreshould be computed for twinless CEs.
Bengtsonand Roth (2008) simply discard twinless CEs, butthis solution is likely too lenient ?
it doles no pun-ishment for mistakes on twinless annotated or ex-tracted CEs and it would be tricked, for example,by a system that extracts only the CEs about whichit is most confident.We propose two different ways to deal withtwinless CEs for B3.
One option, B3all, retainsall twinless extracted CEs.
It computes the preci-4We also experimented with the CEAF score (Luo, 2005),but excluded it due to difficulties dealing with the extracted,rather than annotated, CEs.
CEAF assigns a zero score toeach twinless extracted CE and weights all coreference chainsequally, irrespective of their size.
As a result, runs with ex-tracted CEs exhibit very low CEAF precision, leading to un-reliable scores.sion as above when ce has a twin, and computesthe precision as 1/|Rce| if ce is twinless.
(Simi-larly, recall(ce) = 1/|Kce| if ce is twinless.
)The second option, B30, discards twinlessextracted CEs, but penalizes recall by settingrecall(ce) = 0 for all twinless annotated CEs.Thus, B30 presumes that all twinless extractedCEs are spurious.Results.
Table 3, box 1 shows the performanceof RECONCILEACL09 using a default (0.5) coref-erence classifier threshold.
The MUC score ishighest for the MUC6 data set, while the four ACEdata sets show much higher B3 scores as com-pared to the two MUC data sets.
The latter occursbecause the ACE data sets include singletons.The classification threshold, however, can begainfully employed to control the trade-off be-tween precision and recall.
This has not tradi-tionally been done in learning-based coreferenceresolution research ?
possibly because there isnot much training data available to sacrifice as avalidation set.
Nonetheless, we hypothesized thatestimating a threshold from just the training datamight be effective.
Our results (BASELINE boxin Table 3) indicate that this indeed works well.5With the exception of MUC6, results on all datasets and for all scoring algorithms improve; more-over, the scores approach those for runs using anoptimal threshold (box 3) for the experiment as de-termined by using the test set.
In all remaining ex-periments, we learn the threshold from the trainingset as in the BASELINE system.Below, we resume our investigation of the roleof three coreference resolution subtasks and mea-sure the impact of each on overall performance.3.3 Named EntitiesPrevious work has shown that resolving corefer-ence between proper names is relatively easy (e.g.Kameyama (1997)) because string matching func-tions specialized to the type of proper name (e.g.person vs. location) are quite accurate.
Thus, wewould expect a coreference resolution system todepend critically on its Named Entity (NE) extrac-tor.
On the other hand, state-of-the-art NE taggersare already quite good, so improving this compo-nent may not provide much additional gain.To study the influence of NE recognition,we replace the system-generated NEs of5All experiments sample uniformly from 1000 thresholdvalues.659ReconcileACL09 MUC6 MUC7 ACE-2 ACE03 ACE04 ACE051.
DEFAULT THRESHOLD (0.5)MUC 70.40 58.20 65.76 66.73 56.75 64.30B3all 69.91 62.88 77.25 77.56 73.03 72.82B30 68.55 62.80 76.59 77.27 72.99 72.432.
BASELINEMUC 68.50 62.80 65.99 67.87 62.03 67.41= THRESHOLD ESTIMATIONB3all 70.88 65.86 78.29 79.39 76.50 73.71B30 68.43 64.57 76.63 77.88 75.41 72.473.
OPTIMAL THRESHOLDMUC 71.20 62.90 66.83 68.35 62.11 67.41B3all 72.31 66.52 78.50 79.41 76.53 74.25B30 69.49 64.64 76.83 78.27 75.51 72.944.
BASELINE withMUC 69.90 - 66.37 70.35 62.88 67.72perfect NEsB3all 72.31 - 78.06 80.22 77.01 73.92B30 67.91 - 76.55 78.35 75.22 72.905.
BASELINE withMUC 85.80* 81.10* 76.39 79.68 76.18 79.42perfect CEsB3all 76.14 75.88 78.65 80.58 77.79 76.49B30 76.14 75.88 78.65 80.58 77.79 76.496.
BASELINE withMUC 82.20* 71.90* 86.63 85.58 83.33 82.84anaphoric CEsB3all 72.52 69.26 80.29 79.71 76.05 74.33B30 72.52 69.26 80.29 79.71 76.05 74.33Table 3: Impact of Three Subtasks on Coreference Resolution Performance.
A score marked with a * indicates that a 0.5threshold was used because threshold selection from the training data resulted in an extreme version of the system, i.e.
one thatplaces all CEs into a single coreference chain.RECONCILEACL09 with gold-standard NEsand retrain the coreference classifier.
Resultsfor each of the data sets are shown in box 4 ofTable 3.
(No gold standard NEs are available forMUC7.)
Comparison to the BASELINE system(box 2) shows that using gold standard NEsleads to improvements on all data sets with theexception of ACE2 and ACE05, on which perfor-mance is virtually unchanged.
The improvementstend to be small, however, between 0.5 to 3performance points.
We attribute this to twofactors.
First, as noted above, although far fromperfect, NE taggers generally perform reasonablywell.
Second, only 20 to 25% of the coreferenceelement resolutions required for these data setsinvolve a proper name (see Section 4).Conclusion #1: Improving the performance of NE tag-gers is not likely to have a large impact on the performanceof state-of-the-art coreference resolution systems.3.4 Coreference Element DetectionWe expect CE detection to be an important sub-problem for an end-to-end coreference system.Results for a system that assumes perfect CEsare shown in box 5 of Table 3.
For these runs,RECONCILEACL09 uses only the annotated CEsfor both training and testing.
Using perfect CEssolves a large part of the coreference resolutiontask: the annotated CEs divulge anaphoricity in-formation, perfect NP boundaries, and perfect in-formation regarding the coreference relation de-fined for the data set.We see that focusing attention on all and onlythe annotated CEs leads to (often substantial) im-provements in performance on all metrics overall data sets, especially when measured using theMUC score.Conclusion #2: Improving the ability of coreference re-solvers to identify coreference elements would likely improvethe state-of-the-art immensely ?
by 10-20 points in MUC F1score and from 2-12 F1 points for B3.This finding explains previously published re-sults that exhibit striking variability when run withannotated CEs vs. system-extracted CEs.
On theMUC6 data set, for example, the best publishedMUC score using extracted CEs is approximately71 (Yang et al, 2003), while multiple systemshave produced MUC scores of approximately 85when using annotated CEs (e.g.
Luo et al (2004),McCallum and Wellner (2004)).We argue that providing a resolver with the an-notated CEs is a rather unrealistic evaluation: de-termining whether an NP is part of an annotatedcoreference chain is precisely the job of a corefer-ence resolver!Conclusion #3: Assuming the availability of CEs unre-alistically simplifies the coreference resolution task.3.5 Anaphoricity DeterminationFinally, several coreference systems have suc-cessfully incorporated anaphoricity determination660modules (e.g.
Ng and Cardie (2002a) and Beanand Riloff (2004)).
The goal of the module is todetermine whether or not an NP is anaphoric.
Forexample, pleonastic pronouns (e.g.
it is raining)are special cases that do not require coreferenceresolution.Unfortunately, neither the MUC nor the ACEdata sets include anaphoricity information for allNPs.
Rather, they encode anaphoricity informa-tion implicitly for annotated CEs: a CE is consid-ered anaphoric if is not a singleton.6To study the utility of anaphoricity informa-tion, we train and test only on the ?anaphoric?
ex-tracted CEs, i.e.
the extracted CEs that have anannotated twin that is not a singleton.
Note thatfor the MUC datasets all extracted CEs that havetwins are considered anaphoric.Results for this experiment (box 6 in Table 3)are similar to the previous experiment using per-fect CEs: we observe big improvements across theboard.
This should not be surprising since the ex-perimental setting is quite close to that for perfectCEs: this experiment also presumes knowledgeof when a CE is part of an annotated coreferencechain.
Nevertheless, we see that anaphoricity info-mation is important.
First, good anaphoricity iden-tification should reduce the set of extracted CEsmaking it closer to the set of annotated CEs.
Sec-ond, further improvements in MUC score for theACE data sets over the runs using perfect CEs (box5) reveal that accurately determining anaphoric-ity can lead to substantial improvements in MUCscore.
ACE data includes annotations for single-ton CEs, so knowling whether an annotated CE isanaphoric divulges additional information.Conclusion #4: An accurate anaphoricity determina-tion component can lead to substantial improvement in coref-erence resolution performance.4 Resolution ComplexityDifferent types of anaphora that have to be han-dled by coreference resolution systems exhibit dif-ferent properties.
In linguistic theory, bindingmechanisms vary for different kinds of syntacticconstituents and structures.
And in practice, em-pirical results have confirmed intuitions that differ-ent types of anaphora benefit from different clas-sifier features and exhibit varying degrees of diffi-culty (Kameyama, 1997).
However, performance6Also, the first element of a coreference chain is usuallynon-anaphoric, but we do not consider that issue here.evaluations rarely include analysis of where state-of-the-art coreference resolvers perform best andworst, aside from general conclusions.In this section, we analyze the behavior ofour coreference resolver on different types ofanaphoric expressions with two goals in mind.First, we want to deduce the strengths and weak-nesses of state-of-the-art systems to help directfuture research.
Second, we aim to understandwhy current coreference resolvers behave so in-consistently across data sets.
Our hypothesis isthat the distribution of different types of anaphoricexpressions in a corpus is a major factor for coref-erence resolution performance.
Our experimentsconfirm this hypothesis and we use our empiricalresults to create a coreference performance predic-tion (CPP) measure that successfully estimates theexpected level of performance on novel data sets.4.1 Resolution ClassesWe study the resolution complexity of a text cor-pus by defining resolution classes.
Resolutionclasses partition the set of anaphoric CEs accord-ing to properties of the anaphor and (in somecases) the antecedent.
Previous work has stud-ied performance differences between pronominalanaphora, proper names, and common nouns, butwe aim to dig deeper into subclasses of each ofthese groups.
In particular, we distinguish be-tween proper and common nouns that can be re-solved via string matching, versus those that haveno antecedent with a matching string.
Intuitively,we expect that it is easier to resolve the casesthat involve string matching.
Similarly, we par-tition pronominal anaphora into several subcate-gories that we expect may behave differently.
Wedefine the following nine resolution classes:Proper Names: Three resolution classes coverCEs that are named entities (e.g.
the PER-SON, LOCATION, ORGANIZATION and DATEclasses for MUC and ACE) and have a prior ref-erent7 in the text.
These three classes are distin-guished by the type of antecedent that can be re-solved against the proper name.
(1) PN-e: a proper name is assigned to this exact string matchclass if there is at least one preceding CE in its gold standardcoreference chain that exactly matches it.
(2) PN-p: a proper name is assigned to this partial stringmatch class if there is at least one preceding CE in its goldstandard chain that has some content words in common.
(3) PN-n: a proper name is assigned to this no string match7We make a rough, but rarely inaccurate, assumption thatthere are no cataphoric expressions in the data.661MUC6 MUC7 ACE2 ACE03 ACE04 ACE05 Avg# % scr # % scr # % scr # % scr # % scr # % scr % scrPN-e 273 17 .87 249 19 .79 346 24 .94 435 25 .93 267 16 .88 373 31 .92 22 .89PN-p 157 10 .68 79 6 .59 116 8 .86 178 10 .87 194 11 .71 125 10 .71 9 .74PN-n 18 1 .18 18 1 .28 85 6 .19 79 4 .15 66 4 .21 89 7 .27 4 .21CN-e 292 18 .82 276 21 .65 84 6 .40 186 11 .68 165 10 .68 134 11 .79 13 .67CN-p 229 14 .53 239 18 .49 147 10 .26 168 10 .24 147 9 .40 147 12 .43 12 .39CN-n 194 12 .27 148 11 .15 152 10 .50 148 8 .90 266 16 .32 121 10 .20 11 .181+2Pr 48 3 .70 65 5 .66 122 8 .73 76 4 .73 158 9 .77 51 4 .61 6 .70G3Pr 160 10 .73 50 4 .79 181 12 .83 237 13 .82 246 14 .84 69 60 .81 10 .80U3Pr 175 11 .49 142 11 .49 163 11 .45 122 7 .48 153 9 .49 91 7 .49 9 .48Table 4: Frequencies and scores for each resolution class.class if no preceding CE in its gold standard chain has anycontent words in common with it.Common NPs: Three analogous string matchclasses cover CEs that have a common noun as ahead: (4) CN-e (5) CN-p (6) CN-n.Pronouns: Three classes cover pronouns:(7) 1+2Pr: The anaphor is a 1st or 2nd person pronoun.
(8) G3Pr: The anaphor is a gendered 3rd person pronoun(e.g.
?she?, ?him?).
(9) U3Pr: The anaphor is an ungendered 3rd person pro-noun.As noted above, resolution classes are defined forannotated CEs.
We use the twin relationship tomatch extracted CEs to annotated CEs and to eval-uate performance on each resolution class.4.2 Scoring Resolution ClassesTo score each resolution class separately, we de-fine a new variant of the MUC scorer.
We computea MUC-RC score (for MUC Resolution Class) forclass C as follows: we assume that all CEs that donot belong to class C are resolved correctly by tak-ing the correct clustering for them from the goldstandard.
Starting with this correct partial cluster-ing, we run our classifier on all ordered pairs ofCEs for which the second CE is of class C, es-sentially asking our coreference resolver to deter-mine whether each member of class C is corefer-ent with each of its preceding CEs.
We then countthe number of unique correct/incorrect links thatthe system introduced on top of the correct par-tial clustering and compute precision, recall, andF1 score.
This scoring function directly measuresthe impact of each resolution class on the overallMUC score.4.3 ResultsTable 4 shows the results of our resolution classanalysis on the test portions of the six data sets.The # columns show the frequency counts for eachresolution class, and the % columns show the dis-tributions of the classes in each corpus (i.e.
17%MUC6 MUC7 ACE2 ACE03 ACE04 ACE050.92 0.95 0.91 0.98 0.97 0.96Table 5: Correlations of resolution class scores with respectto the average.of all resolutions in the MUC6 corpus were in thePN-e class).
The scr columns show the MUC-RC score for each resolution class.
The right-handside of Table 4 shows the average distribution andscores across all data sets.These scores confirm our expectations about therelative difficulty of different types of resolutions.For example, it appears that proper names are eas-ier to resolve than common nouns; gendered pro-nouns are easier than 1st and 2nd person pronouns,which, in turn, are easier than ungendered 3rd per-son pronouns.
Similarly, our intuition is confirmedthat many CEs can be accurately resolved based onexact string matching, whereas resolving againstantecedents that do not have overlapping strings ismuch more difficult.
The average scores in Table 4show that performance varies dramatically acrossthe resolution classes, but, on the surface, appearsto be relatively consistent across data sets.None of the data sets performs exactly the same,of course, so we statistically analyze whether thebehavior of each resolution class is similar acrossthe data sets.
For each data set, we compute thecorrelation between the vector of MUC-RC scoresover the resolution classes and the average vec-tor of MUC-RC scores for the remaining five datasets.
Table 5 contains the results, which show highcorrelations (over .90) for all six data sets.
Theseresults indicate that the relative performance of theresolution classes is consistent across corpora.4.4 Coreference Performance PredictionNext, we hypothesize that the distribution of res-olution classes in a corpus explains (at least par-tially) why performance varies so much from cor-662MUC6 MUC7 ACE2 ACE03 ACE04 ACE05P 0.59 0.59 0.62 0.65 0.59 0.62O 0.67 0.61 0.66 0.68 0.62 0.67Table 6: Predicted (P) vs Observed (O) scores.pus to corpus.
To explore this issue, we create aCoreference Performance Prediction (CPP) mea-sure to predict the performance on new data sets.The CPP measure uses the empirical performanceof each resolution class observed on previous datasets and forms a predicton based on the make-upof resolution classes in a new corpus.
The distribu-tion of resolution classes for a new corpus can beeasily determined because the classes can be rec-ognized superficially by looking only at the stringsthat represent each NP.We compute the CPP score for each of our sixdata sets based on the average resolution class per-formance measured on the other five data sets.The predicted score for each class is computed asa weighted sum of the observed scores for eachresolution class (i.e.
the mean for the class mea-sured on the other five data sets) weighted by theproportion of CEs that belong to the class.
Thepredicted scores are shown in Table 6 and com-pared with the MUC scores that are produced byRECONCILEACL09.8Our results show that the CPP measure is agood predictor of coreference resolution perfor-mance on unseen data sets, with the exceptionof one outlier ?
the MUC6 data set.
In fact,the correlation between predicted and observedscores is 0.731 for all data sets and 0.913 exclud-ing MUC6.
RECONCILEACL09?s performance onMUC6 is better than predicted due to the higherthan average scores for the common noun classes.We attribute this to the fact that MUC6 includesannotations for nested nouns, which almost al-ways fall in the CN-e and CN-p classes.
In ad-dition, many of the features were first created forthe MUC6 data set, so the feature extractors arelikely more accurate than for other data sets.Overall, results indicate that coreference perfor-mance is substantially influenced by the mix ofresolution classes found in the data set.
Our CPPmeasure can be used to produce a good estimateof the level of performance on a new corpus.8Observed scores for MUC6 and 7 differ slightly from Ta-ble 3 because this part of the work did not use the OPTIONALfield of the key, employed by the official MUC scorer.5 Related WorkThe bulk of the relevant related work is describedin earlier sections, as appropriate.
This paper stud-ies complexity issues for NP coreference resolu-tion using a ?good?, i.e.
near state-of-the-art, sys-tem.
For state-of-the-art performance on the MUCdata sets see, e.g.
Yang et al (2003); for state-of-the-art performance on the ACE data sets see, e.g.Bengtson and Roth (2008) and Luo (2007).
Whileother researchers have evaluated NP coreferenceresolvers with respect to pronouns vs. propernouns vs. common nouns (Ng and Cardie, 2002b),our analysis focuses on measuring the complexityof data sets, predicting the performance of coref-erence systems on new data sets, and quantify-ing the effect of coreference system subcompo-nents on overall performance.
In the related areaof anaphora resolution, researchers have studiedthe influence of subsystems on the overall per-formance (Mitkov, 2002) as well as defined andevaluated performance on different classes of pro-nouns (e.g.
Mitkov (2002) and Byron (2001)).However, due to the significant differences in taskdefinition, available datasets, and evaluation met-rics, their conclusions are not directly applicableto the full coreference task.Previous work has developed methods to predictsystem performance on NLP tasks given data setcharacteristics, e.g.
Birch et al (2008) does this formachine translation.
Our work looks for the firsttime at predicting the performance of NP corefer-ence resolvers.6 ConclusionsWe examine the state-of-the-art in NP coreferenceresolution.
We show the relative impact of perfectNE recognition, perfect anaphoricity informationfor coreference elements, and knowledge of alland only the annotated CEs.
We also measure theperformance of state-of-the-art resolvers on sev-eral classes of anaphora and use these results todevelop a measure that can accurately estimate aresolver?s performance on new data sets.Acknowledgments.
We gratefully acknowledgetechnical contributions from David Buttler andDavid Hysom in creating the Reconcile corefer-ence resolution platform.
This research was sup-ported in part by the Department of HomelandSecurity under ONR Grant N0014-07-1-0152 andLawrence Livermore National Laboratory subcon-tract B573245.663ReferencesA.
Bagga and B. Baldwin.
1998.
Algorithms for Scor-ing Coreference Chains.
In In Linguistic Corefer-ence Workshop at LREC 1998.Baldridge, J.
2005.
The OpenNLP project.http://opennlp.sourceforge.net/.D.
Bean and E. Riloff.
2004.
Unsupervised Learn-ing of Contextual Role Knowledge for CoreferenceResolution.
In Proceedings of the Annual Meetingof the North American Chapter of the Associationfor Computational Linguistics (HLT/NAACL 2004).Eric Bengtson and Dan Roth.
2008.
Understandingthe Value of Features for Coreference Resolution.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages294?303.
Association for Computational Linguis-tics.Alexandra Birch, Miles Osborne, and Philipp Koehn.2008.
Predicting Success in Machine Translation.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages745?754.
Association for Computational Linguis-tics.Donna Byron.
2001.
The Uncommon Denomina-tor: A Proposal for Consistent Reporting of Pro-noun Resolution Results.
Computational Linguis-tics, 27(4):569?578.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating TypedDependency Parses from Phrase Structure Parses.
InLREC.J.
Finkel, S. Dingare, H. Nguyen, M. Nissim, andC.
Manning.
2004.
Exploiting Context for Biomed-ical Entity Recognition: From Syntax to the Web.
InJoint Workshop on Natural Language Processing inBiomedicine and its Applications at COLING 2004.Yoav Freund and Robert E. Schapire.
1999.
LargeMargin Classification Using the Perceptron Algo-rithm.
In Machine Learning, pages 277?296.Megumi Kameyama.
1997.
Recognizing ReferentialLinks: An Information Extraction Perspective.
InWorkshop On Operational Factors In Practical Ro-bust Anaphora Resolution For Unrestricted Texts.Xiaoqiang Luo, Abe Ittycheriah, Hongyan Jing, NandaKambhatla, and Salim Roukos.
2004.
AMention-Synchronous Coreference Resolution Al-gorithm Based on the Bell Tree.
In Proceedingsof the 42nd Annual Meeting of the Association forComputational Linguistics.X.
Luo.
2005.
On Coreference Resolution Perfor-mance Metrics.
In Proceedings of the 2005 HumanLanguage Technology Conference / Conference onEmpirical Methods in Natural Language Process-ing.Xiaoqiang Luo.
2007.
Coreference or Not: A TwinModel for Coreference Resolution.
In Proceedingsof the Annual Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT/NAACL 2007).A.
McCallum and B. Wellner.
2004.
Conditional Mod-els of Identity Uncertainty with Application to NounCoreference.
In 18th Annual Conference on NeuralInformation Processing Systems.Ruslan Mitkov.
2002.
Anaphora Resolution.
Long-man, London.MUC-6.
1995.
Coreference Task Definition.
In Pro-ceedings of the Sixth Message Understanding Con-ference (MUC-6), pages 335?344.MUC-7.
1997.
Coreference Task Definition.
InProceedings of the Seventh Message UnderstandingConference (MUC-7).V.
Ng and C. Cardie.
2002a.
Identifying Anaphoricand Non-Anaphoric Noun Phrases to ImproveCoreference Resolution.
In Proceedings of the 19thInternational Conference on Computational Lin-guistics (COLING 2002).V.
Ng and C. Cardie.
2002b.
Improving MachineLearning Approaches to Coreference Resolution.
InProceedings of the 40th Annual Meeting of the As-sociation for Computational Linguistics.NIST.
2004.
The ACE Evaluation Plan.S.
Petrov and D. Klein.
2007.
Improved Inference forUnlexicalized Parsing.
In Proceedings of the AnnualMeeting of the North American Chapter of the Asso-ciation for Computational Linguistics (HLT/NAACL2007).W.
Soon, H. Ng, and D. Lim.
2001.
A MachineLearning Approach to Coreference of Noun Phrases.Computational Linguistics, 27(4):521?541.Veselin Stoyanov, Nathan Gilbert, Claire Cardie, EllenRiloff, David Buttler, and David Hysom.
2009.Reconcile: A Coreference Resolution Research Plat-form.
Computer Science Technical Report, CornellUniversity, Ithaca, NY.Kees van Deemter and Rodger Kibble.
2000.
OnCoreferring: Coreference in MUC and RelatedAnnotation Schemes.
Computational Linguistics,26(4):629?637.M.
Vilain, J. Burger, J. Aberdeen, D. Connolly, andL.
Hirschman.
1995.
A Model-Theoretic Corefer-ence Scoring Theme.
In Proceedings of the SixthMessage Understanding Conference (MUC-6).Xiaofeng Yang, Guodong Zhou, Jian Su, andChew Lim Tan.
2003.
Coreference Resolution Us-ing Competition Learning Approach.
In ACL ?03:Proceedings of the 41st Annual Meeting on Associa-tion for Computational Linguistics, pages 176?183.664
