Proceedings of the NAACL HLT 2010: Demonstration Session, pages 33?36,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsInterpretation of Partial Utterances in Virtual Human Dialogue SystemsKenji Sagae and David DeVault and David R. TraumInstitute for Creative TechnologiesUniversity of Southern CaliforniaMarina del Rey, CA 90292, USA{sagae,devault,traum}@ict.usc.eduAbstractDialogue systems typically follow a rigid paceof interaction where the system waits until theuser has finished speaking before producinga response.
Interpreting user utterances be-fore they are completed allows a system todisplay more sophisticated conversational be-havior, such as rapid turn-taking and appropri-ate use of backchannels and interruptions.
Wedemonstrate a natural language understandingapproach for partial utterances, and its use in avirtual human dialogue system that can oftencomplete a user?s utterances in real time.1 IntroductionIn a typical spoken dialogue system pipeline, theresults of automatic speech recognition (ASR) foreach user utterance are sent to modules that per-form natural language understanding (NLU) and di-alogue management only after the utterance is com-plete.
This results in a rigid and often unnatural pac-ing where the system must wait until the user stopsspeaking before trying to understand and react touser input.
To achieve more flexible turn-taking withhuman users, for whom turn-taking and feedback atthe sub-utterance level is natural, the system needsthe ability to start interpretation of user utterancesbefore they are completed.We demonstrate an implementation of techniqueswe have developed for partial utterance understand-ing in virtual human dialogue systems (Sagae et al,2009; DeVault et al, 2009) with the goal of equip-ping these systems with sophisticated conversationalbehavior, such as interruptions and non-verbal feed-back.
Our demonstration highlights the understand-ing of utterances before they are finished.
It alsoincludes an utterance completion capability, where avirtual human can make a strategic decision to dis-play its understanding of an unfinished user utter-ance by completing the utterance itself.The work we demonstrate here is part of a grow-ing research area in which new technical approachesto incremental utterance processing are being de-veloped (e.g.
Schuler et al (2009), Kruijff et al(2007)), new possible metrics for evaluating the per-formance of incremental processing are being pro-posed (e.g.
Schlangen et al (2009)), and the ad-vantages for dialogue system performance and us-ability are starting to be empirically quantified (e.g.Skantze and Schlangen (2009), Aist et al (2007)).2 NLU for partial utterancesIn previous work (Sagae et al, 2009), we presentedan approach for prediction of semantic content frompartial speech recognition hypotheses, looking atlength of the speech hypothesis as a general indi-cator of semantic accuracy in understanding.
Insubsequent work (DeVault et al, 2009), we incor-porated additional features of real-time incremen-tal interpretation to develop a more nuanced predic-tion model that can accurately identify moments ofmaximal understanding within individual spoken ut-terances.
This research was conducted in the con-text of the SASO-EN virtual human dialogue sys-tem (Traum et al, 2008), using a corpus of approxi-mately 4,500 utterances from user sessions.
The cor-pus includes a recording of each original utterance, a33?????????
?mood : declarativesem :???????
?type : eventagent : captain?
kirkevent : delivertheme : power ?
generatormodal :[possibility : can]speech?
act :[type : offer]?????????????????
?Figure 1: AVM utterance representation.manual transcription, and a gold-standard semanticframe, allowing us to develop and evaluate a data-driven NLU approach.2.1 NLU in SASO-EN Virtual HumansOur NLU module for the SASO-EN system,mxNLU (Sagae et al, 2009), is based on maxi-mum entropy classification (Berger et al, 1996) ,where we treat entire individual semantic frames asclasses, and extract input features from ASR.
TheNLU output representation is an attribute-value ma-trix (AVM), where the attributes and values repre-sent semantic information that is linked to a domain-specific ontology and task model (Figure 1).
TheAVMs are linearized, using a path-value notation, asseen in the NLU input-output example below:?
Utterance (speech): we are prepared to giveyou guys generators for electricity downtown?
ASR (NLU input): we up apparently give youguys generators for a letter city don town?
Frame (NLU output):<s>.mood declarative<s>.sem.type event<s>.sem.agent captain-kirk<s>.sem.event deliver<s>.sem.theme power-generator<s>.sem.modal.possibility can<s>.sem.speechact.type offerWhen mxNLU is trained on complete ASR out-put for approximately 3,500 utterances, and testedon a separate set of 350 complete ASR utterances,the F-score of attribute-value pairs produced by theNLU is 0.76 (0.78 precision and 0.74 recall).
Thesefigures reflect the use of ASR at run-time, and mosterrors are caused by incorrect speech recognition.2.2 NLU with partial ASR results (Sagae et al,2009)To interpret utterances before they are complete,we use partial recognition hypotheses produced byASR every 200 milliseconds while the user is speak-ing.
To process these partial utterances produced byASR, we train length-specific models for mxNLU.These models are trained using the partial ASR re-sults we obtain by running ASR on the audio corre-sponding to the utterances in the training data.
TheNLU task is then to predict the meaning of the en-tire utterance based only on a (noisy) prefix of theutterance.
On average, the accuracy of mxNLU on asix-word prefix of an utterance (0.74 F-score) is al-most as the same as the accuracy of mxNLU on en-tire utterances.
Approximately half of the utterancesin our corpus contain more than six words, creatinginteresting opportunities for conversational behaviorthat would be impossible under a model where eachutterance must be completed before it is interpreted.2.3 Detecting points of maximalunderstanding (DeVault et al, 2009)Although length-specific NLU models produce ac-curate results on average, more effective use of theinterpretation provided by these models might beachieved if we could automatically gauge their per-formance on individual utterances at run-time.
Tothat end, we have developed an approach (DeVault etal., 2009) that aims to detect those strategic points intime, as specific utterances are occurring, when thesystem reaches maximal understanding of the utter-ance, in the sense that its interpretation will not sig-nificantly improve during the rest of the utterance.Figure 2 illustrates the incremental output ofmxNLU as a user asks, elder do you agree to movethe clinic downtown?
Our ASR processes capturedaudio in 200ms chunks.
The figure shows the par-tial ASR results after the ASR has processed each200ms of audio, along with the F-score achieved bymxNLU on each of these partials.
Note that the NLUF-score fluctuates somewhat as the ASR revises itsincremental hypotheses about the user utterance, butgenerally increases over time.For the purpose of initiating an overlapping re-sponse to a user utterance such as this one, the agentneeds to be able (in the right circumstances) to make34Utterance time (ms)NLU F?score(empty)(empty)allelderelder doyouelder toyoudelder doyouagreeelder doyouagree toelder doyouagree to movetheelder doyouagree to movetheelder doyouagree to movetheclinic toelder doyouagree to movetheclinic downelder doyouagree to movetheclinic downtownelder doyouagree to movetheclinic downtown200 400 600 800 10001200140016001800200022002400260028000.00.10.20.30.40.50.60.70.80.9Partial ASR resultFigure 2: Incremental interpretation of a user utterance.an assessment that it has already understood the ut-terance ?well enough?, based on the partial ASR re-sults that are currently available.
We have imple-mented a specific approach to this assessment whichviews an utterance as understood ?well enough?
ifthe agent would not understand the utterance anybetter than it currently does even if it were to waitfor the user to finish their utterance (and for the ASRto finish interpreting the complete utterance).Concretely, Figure 2 shows that after the entire2800ms utterance has been processed by the ASR,mxNLU achieves an F-score of 0.91.
However, infact, mxNLU already achieves this maximal F-scoreat the moment it interprets the partial ASR result el-der do you agree to move the at 1800ms.
The agenttherefore could, in principle, initiate an overlappingresponse at 1800ms without sacrificing any accuracyin its understanding of the user?s utterance.Of course the agent does not automatically realizethat it has achieved a maximal F-score at 1800ms.To enable the agent to make this assessment, wehave trained a classifier, which we call MAXF, thatcan be invoked for any specific partial ASR result,and which uses various features of the ASR resultand the current mxNLU output to estimate whetherthe NLU F-score for the current partial ASR resultis at least as high as the mxNLU F-score would be ifthe agent were to wait for the entire utterance.To facilitate training of a MAXF classifier, weidentified a range of potentially useful features thatthe agent could use at run-time to assess its confi-dence in mxNLU?s output for a given partial ASRresult.
These features include: the number of par-tial results that have been received from the ASR;the length (in words) of the current partial ASRresult; the entropy in the probability distributionmxNLU assigns to alternative output frames (lowerentropy corresponds to a more focused distribution);the probability mxNLU assigns to the most probableoutput frame; and the most probable output frame.Based on these features, we trained a decision treeto make the binary prediction that MAXF is TRUEor FALSE for each partial ASR result.
DeVault et al(2009) include a detailed evaluation and discussionof the classifier.
To briefly summarize our results,the precision/recall/F-score of the trained MAXFmodel are 0.88/0.52/0.65 respectively.
The high pre-cision means that 88% of the time that the modelpredicts that F-score is maximized at a specific par-tial, it really is.
Our demonstration, which we out-line in the next section, highlights the utility of ahigh-precision MAXF classifier in making the deci-sion whether to complete a user?s utterance.3 Demo script outlineWe have implemented the approach for partial utter-ance understanding described above in the SASO-EN system (Traum et al, 2008), a virtual humandialogue system with speech input and output (Fig-ure 3), allowing us to demonstrate both partial utter-ance understanding and some of the specific behav-iors made possible by this capability.
We divide thisdemonstration in two parts: visualization of NLUfor partial utterances and user utterance completion.35Figure 3: SASO-EN: Dr. Perez and Elder al-Hassan.Partial ASR result Predicted completionwe can provide transportation to move the patient therethe market is not safethere are supplies where we are goingTable 1: Examples of user utterance completions.3.1 Visualization of NLU for partial utterancesBecause the demonstration depends on usage of thesystem within the domain for which it was designed,the demo operator provides a brief description of thesystem, task and domain.
The demo operator (ora volunteer user) then speaks normally to the sys-tem, while a separate window visualizes the sys-tem?s evolving understanding.
This display is up-dated every 200 milliseconds, allowing attendees tosee partial utterance understanding in action.
Forease of comprehension, the display will summarizethe NLU state using an English paraphrase of thepredicted meaning (rather than displaying the struc-tured frame that is the actual output of NLU).
Thedisplay will also visualize the TRUE or FALSE stateof the MAXF classifier, highlighting the moment thesystem thinks it reaches maximal understanding.3.2 User utterance completionThe demo operator (or volunteer user) starts to speakand pauses briefly in mid-utterance, at which point,if possible, one of the virtual humans jumps in andcompletes the utterance (DeVault et al, 2009).
Ta-ble 1 includes a few examples of the many utterancesthat can be completed by the virtual humans.4 ConclusionInterpretation of partial utterances, combined witha way to predict points of maximal understanding,opens exciting possibilities for more natural conver-sational behavior in virtual humans.
This demon-stration showcases the NLU approach and a sampleapplication of the basic techniques.AcknowledgmentsThe work described here has been sponsored by theU.S.
Army Research, Development, and Engineer-ing Command (RDECOM).
Statements and opin-ions expressed do not necessarily reflect the positionor the policy of the United States Government, andno official endorsement should be inferred.ReferencesG.
Aist, J. Allen, E. Campana, C. G. Gallo, S. Stoness,M.
Swift, and M. K. Tanenhaus.
2007.
Incrementaldialogue system faster than and preferred to its non-incremental counterpart.
In Proc.
of the 29th AnnualConference of the Cognitive Science Society.A.
Berger, S. Della Pietra, and V. Della Pietra.
1996.
Amaximum entropy approach to natural language pro-cessing.
Computational Linguistics, 22(1):39?71.D.
DeVault, K. Sagae, and D. Traum.
2009.
Can I finish?Learning when to respond to incremental interpreta-tion results in interactive dialogue.
In Proc.
SIGDIAL.G.
J. Kruijff, P. Lison, T. Benjamin, H. Jacobsson, andN.
Hawes.
2007.
Incremental, multi-level processingfor comprehending situated dialogue in human-robotinteraction.
In Proc.
LangRo?2007.K.
Sagae, G. Christian, D. DeVault, and D. R. Traum.2009.
Towards natural language understanding of par-tial speech recognition results in dialogue systems.
InShort Paper Proceedings of NAACL HLT.D.
Schlangen, T. Baumann, and M. Atterer.
2009.
In-cremental reference resolution: The task, metrics forevaluation, and a Bayesian filtering model that is sen-sitive to disfluencies.
In Proc.
SIGDIAL, page 30?37.W.
Schuler, S. Wu, and L. Schwartz.
2009.
A frame-work for fast incremental interpretation during speechdecoding.
Computational Linguistics, 35(3):313?343.G.
Skantze and D. Schlangen.
2009.
Incremental dia-logue processing in a micro-domain.
In Proc.
EACL.D.
Traum, S. Marsella, J. Gratch, J. Lee, and A. Hartholt.2008.
Multi-party, multi-issue, multi-strategy negoti-ation for multi-modal virtual agents.
In Proc.
of theEighth International Conference on Intelligent VirtualAgents.36
