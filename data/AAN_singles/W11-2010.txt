Proceedings of the SIGDIAL 2011: the 12th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 68?77,Portland, Oregon, June 17-18, 2011. c?2011 Association for Computational LinguisticsGiving instructions in virtual environments by corpus based selectionLuciana BenottiPLN Group, FAMAFNational University of Co?rdobaCo?rdoba, Argentinaluciana.benotti@gmail.comAlexandre DenisTALARIS team, LORIA/CNRSLorraine.
Campus scientifique, BP 239Vandoeuvre-le`s-Nancy, Francealexandre.denis@loria.frAbstractInstruction giving can be used in severalapplications, ranging from trainers in sim-ulated worlds to non player characters forvirtual games.
In this paper we present anovel algorithm for rapidly prototyping virtualinstruction-giving agents from human-humancorpora without manual annotation.
Automat-ically prototyping full-fledged dialogue sys-tems from corpora is far from being a realitynowadays.
Our approach is restricted in thatonly the virtual instructor can perform speechacts while the user responses are limited tophysical actions in the virtual worlds.We have defined an algorithm that, given atask-based corpus situated in a virtual world,which contains human instructor?s speech actsand the user?s responses as physical actions,generates a virtual instructor that robustlyhelps a user achieve a given task in the vir-tual world.
We explain how this algorithmcan be used for generating a virtual instructorfor a game-like, task-oriented virtual world.We evaluate the virtual instructor with humanusers using task-oriented as well as user satis-faction metrics.
We compare our results withboth human and rule-based virtual instructorshand-coded for the same task.1 IntroductionVirtual human characters constitute a promisingcontribution to many fields, including simulation,training and interactive games (Kenny et al, 2007;Jan et al, 2009).
The ability to communicate usingnatural language is important for believable and ef-fective virtual humans.
Such ability has to be goodenough to engage the trainee or the gamer in the ac-tivity.
Nowadays, most conversational systems oper-ate on a dialogue-act level and require extensive an-notation efforts in order to be fit for their task (Rieserand Lemon, 2010).
Semantic annotation and ruleauthoring have long been known as bottlenecks fordeveloping conversational systems for new domains.In this paper, we present a novel algorithm forgenerating virtual instructors from automatically an-notated human-human corpora.
Our algorithm,when given a task-based corpus situated in a virtualworld, generates an instructor that robustly helps auser achieve a given task in the virtual world of thecorpus.
There are two main approaches toward au-tomatically producing dialogue utterances.
One isthe selection approach, in which the task is to pickthe appropriate output from a corpus of possible out-puts.
The other is the generation approach, in whichthe output is dynamically assembled using somecomposition procedure, e.g.
grammar rules.
The se-lection approach to generation has only been usedin conversational systems that are not task-orientedsuch as negotiating agents (Gandhe and Traum,2007a), question answering characters (Kenny et al,2007), and virtual patients (Leuski et al, 2006).
Tothe best of our knowledge, our algorithm is the firstone proposed for doing corpus based generation andinteraction management for task-oriented systems.The advantages of corpus based generation aremany.
To start with, it affords the use of complexand human-like sentences without detailed analysis.Moreover, the system may easily use recorded au-dio clips rather than speech synthesis and recordedvideo for animating virtual humans.
Finally, no68rule writing by a dialogue expert or manual an-notations is needed.
The disadvantage of corpusbased generation is that the resulting dialogue maynot be fully coherent.
For non-task oriented sys-tems, dialogue management through corpus basedmethods has shown coherence related problems.Shawar and Atwell (2003; 2005) present a methodfor learning pattern matching rules from corpora inorder to obtain the dialogue manager for a chat-bot.
Gandhe and Traum (2007b) investigate severaldialogue models for negotiating virtual agents thatare trained on an unannotated human-human corpus.Both approaches report that the dialogues obtainedby these methods are still to be improved becausethe lack of dialogue history management results inincoherences.
Since in task-based systems, the di-alogue history is restricted by the structure of thetask, the absence of dialogue history management isalleviated by tracking the current state of the task.In the next section we introduce the corpora usedin this paper.
Section 3 presents the two phases ofour algorithm, namely automatic annotation and di-alogue management through selection.
In Section 4we present a fragment of an interaction with a virtualinstructor generated using the corpus and the algo-rithm introduced in the previous sections.
We evalu-ate the virtual instructor in interactions with humansubjects using objective as well as subjective met-rics.
We present the results of the evaluation in Sec-tion 5.
We compare our results with both humanand rule-based virtual instructors hand-coded for thesame task.
Finally, Section 7 discusses the weak-nesses of the approach for developing instructiongiving agents, as well as its advantages and draw-backs with respect to hand-coded systems.
In thislast section we also discuss improvements on our al-gorithms designed as a result of our error analysis.2 The GIVE corpusThe Challenge on Generating Instructions in Vir-tual Environments (GIVE; Koller et al (2010)) isa shared task in which Natural Language Gener-ation systems must generate real-time instructionsthat guide a user in a virtual world.
In this paper,we use the GIVE-2 Corpus (Gargett et al, 2010), afreely available corpus of human instruction givingin virtual environments.
We use the English part ofthe corpus which consists of 63 American Englishwritten discourses in which one subject guided an-other in a treasure hunting task in 3 different 3Dworlds.The task setup involved pairs of human partners,each of whom played one of two different roles.
The?direction follower?
(DF) moved about in the vir-tual world with the goal of completing a treasurehunting task, but had no knowledge of the map ofthe world or the specific behavior of objects withinthat world (such as, which buttons to press to opendoors).
The other partner acted as the ?directiongiver?
(DG), who was given complete knowledge ofthe world and had to give instructions to the DF toguide him/her to accomplish the task.The GIVE-2 corpus is a multi-modal corpuswhich consists of all the instructions uttered by theDG, and all the object manipulations done by the DFwith the corresponding timestamp.
Furthermore, theDF?s position and orientation is logged every 200milliseconds, making it possible to extract informa-tion about his/her movements.3 The unsupervised conversational modelOur algorithm consists of two phases: an annotationphase and a selection phase.
The annotation phaseis performed only once and consists of automaticallyassociating the DG instruction to the DF reaction.The selection phase is performed every time the vir-tual instructor generates an instruction and consistsof picking out from the annotated corpus the mostappropriate instruction at a given point.3.1 The automatic annotationThe basic idea of the annotation is straightforward:associate each utterance with its corresponding re-action.
We assume that a reaction captures the se-mantics of its associated instruction.
Defining re-action involves two subtle issues, namely boundarydetermination and discretization.
We discuss theseissues in turn and then give a formal definition ofreaction.We define the boundaries of a reaction as follows.A reaction Rk to an instruction Uk begins right af-ter the instruction Uk is uttered and ends right beforethe next instruction Uk+1 is uttered.
In the follow-ing example, instruction 1 corresponds to the reac-69tion ?2, 3, 4?, instruction 5 corresponds to ?6?, andinstruction 7 to ?8?.DG(1): hit the red you see in the far roomDF(2): [enters the far room]DF(3): [pushes the red button]DF(4): [turns right]DG(5): hit far side greenDF(6): [moves next to the wrong green]DG(7): noDF(8): [moves to the right green and pushes it]As the example shows, our definition of bound-aries is not always semantically correct.
For in-stance, it can be argued that it includes too muchbecause 4 is not strictly part of the semantics of 1.Furthermore, misinterpreted instructions (as 5) andcorrections (e.g., 7) result in clearly inappropriateinstruction-reaction associations.
Since we want toavoid any manual annotation, we decided to use thisnaive definition of boundaries anyway.
We discussin Section 5 the impact that inappropriate associa-tions have on the performance of a virtual instructor.The second issue that we address here is dis-cretization of the reaction.
It is well known that thereis not a unique way to discretize an action into sub-actions.
For example, we could decompose action 2into ?enter the room?
or into ?get close to the doorand pass the door?.
Our algorithm is not dependenton a particular discretization.
However, the samediscretization mechanism used for annotation has tobe used during selection, for the dialogue managerto work properly.
For selection (i.e., in order to de-cide what to say next) any virtual instructor needsto have a planner and a planning problem: i.e., aspecification of how the virtual world works (i.e.,the actions), a way to represent the states of the vir-tual world (i.e., the state representation) and a wayto represent the objective of the task (i.e., the goal).Therefore, we decided to use them in order to dis-cretize the reaction.For the virtual instructor we present in Section 4we used the planner LazyFF and the planning prob-lem provided with the GIVE Framework.
Theplanner LazyFF is a reimplementation (in Java) ofthe classical artificial intelligence planner FF (Hoff-mann and Nebel, 2001).
The GIVE framework (Gar-gett et al, 2010) provides a standard PDDL (Hsu etal., 2006) planning problem which formalizes howthe GIVE virtual worlds work.
Both the LazzyFFplanner and the GIVE planning problem are freelyavailable on the web1.Now we are ready to define reaction formally.
LetSk be the state of the virtual world when uttering in-struction Uk, Sk+1 be the state of the world when ut-tering the next utterance Uk+1 and Acts be the rep-resentation of the virtual world actions.
The reactionto Uk is defined as the sequence of actions returnedby the planner with Sk as the initial state, Sk+1 asthe goal state and Acts as the actions.Given this reaction definition, the annotation ofthe corpus then consists of automatically associat-ing each utterance to its (discretized) reaction.
Thesimple algorithm that implements this annotation isshown in Figure 1.
Moreover, we provide a fragmentof the resulting annotated corpus in Appendix A.1: Acts?
world possible actions2: for all utterance Uk in the corpus do3: Sk ?
world state at Uk4: Sk+1 ?
world state at Uk+15: Uk.Reaction?
plan(Sk, Sk+1, Acts)6: end forFigure 1: Annotation algorithm3.2 Selecting what to say nextIn this section we describe how the selection phase isperformed every time the virtual instructor generatesan instruction.The instruction selection algorithm, displayed inFigure 2, consists in finding in the corpus the set ofcandidate utterances C for the current task plan P(P is the sequence of actions that needs to be exe-cuted in the current state of the virtual world in or-der to complete the task).
We define C = {U ?Corpus | P starts with U.Reaction}.
In other words,an utterance U belongs to C if the first actions of thecurrent plan P exactly match the reaction associatedto the utterance U .
All the utterances that pass thistest are considered paraphrases and hence suitable inthe current context.Whenever the plan P changes, as a result of theactions of the DF, we call the selection algorithm inorder to regenerate the set of candidate utterances C.1http://www.give-challenge.org/701: C ?
?2: Plan?
current task plan3: for all utterance U in the corpus do4: if Plan starts with U.Reaction then5: C ?
C ?
{U}6: end if7: end for8: return CFigure 2: Selection algorithmWhile the plan P doesn?t change, because theDF is staying still, the virtual instructor offers al-ternative paraphrases of the intended instruction.Each paraphrase is selected by picking an utterancefrom C and verbalizing it, at fixed time intervals(every 3 seconds).
The order in which utterancesare selected depends on the length of the utterancereaction (in terms of number of actions), startingfrom the longest ones.
Hence, in general, instruc-tions such as ?go back again to the room with thelamp?
are uttered before instructions such as ?gostraight?, because the reaction of the former utter-ance is longer than the reaction of the later.It is important to notice that the discretizationused for annotation and selection directly impactsthe behavior of the virtual instructor.
It is crucialthen to find an appropriate granularity of the dis-cretization.
If the granularity is too coarse, manyinstructions in the corpus will have an empty reac-tion.
For instance, in the absence of the representa-tion of the user orientation in the planning domain(as is the case for the virtual instructor we evaluatein Section 5), instructions like ?turn left?
and ?turnright?
will have empty reactions making them indis-tinguishable during selection.
However, if the gran-ularity is too fine the user may get into situationsthat do not occur in the corpus, causing the selec-tion algorithm to return an empty set of candidateutterances.
It is the responsibility of the virtual in-structor developer to find a granularity sufficient tocapture the diversity of the instructions he wants todistinguish during selection.4 A virtual instructor for a virtual worldWe implemented an English virtual instructor forone of the worlds used in the corpus collection wepresented in Section 2.
The English fragment of thecorpus that we used has 21 interactions and a totalof 1136 instructions.
Games consisted on averageof 54.2 instructions from the human DG, and tookabout 543 seconds on average for the human DF tocomplete the task.On Figures 4 to 7 we show an excerpt of an in-teraction between the system and a user.
The fig-ures show a 2D map from top view and the 3D in-game view.
In Figure 4, the user, represented by ablue character, has just entered the upper left room.He has to push the button close to the chair.
Thefirst candidate utterance selected is ?red closest tothe chair in front of you?.
Notice that the referringexpression uniquely identifies the target object us-ing the spatial proximity of the target to the chair.This referring expression is generated without anyreasoning on the target distractors, just by consid-ering the current state of the task plan and the userposition.Figure 4: ?red closest to the chair in front of you?After receiving the instruction the user gets closerto the button as shown in Figure 5.
As a result of thenew user position, a new task plan exists, the set ofcandidate utterances is recalculated and the systemselects a new utterance, namely ?the closet one?.The generation of the ellipsis of the button or thechair is a direct consequence of the utterances nor-mally said in the corpus at this stage of the task plan(that is, when the user is about to manipulate this ob-ject).
From the point of view of referring expressionalgorithms, the referring expression may not be op-timal because it is over-specified (a pronoun would71L goyes leftstraight now go backgo back out now go back outclosest the door down the passagego back to the hallway nowin to the shade roomgo back out of the room out the way you came inexit the way you entered ok now go out the same doorback to the room with the lamp go back to the door you came inGo through the opening on the left okay now go back to the original roomokay now go back to where you came from ok go back again to the room with the lampnow i ned u to go back to the original room Go through the opening on the left with the yellow wall paperFigure 3: All candidate selected utterances when exiting the room in Figure 7Figure 5: ?the closet one?Figure 6: ?good?be preferred as in ?click it?
), Furthermore, the in-struction contains a spelling error (?closet?
insteadFigure 7: ?go back to the room with the lamp?of ?closest?).
In spite of this non optimality, the in-struction led our user to execute the intended reac-tion, namely pushing the button.Right after the user clicks on the button (Figure 6),the system selects an utterance corresponding to thenew task plan.
The player position stayed the sameso the only change in the plan is that the button nolonger needs to be pushed.
In this task state, DGsusually give acknowledgements and this is then whatour selection algorithm selects: ?good?.After receiving the acknowledgement, the userturns around and walks forward, and the next ac-tion in the plan is to leave the room (Figure 7).
Thesystem selects the utterance ?go back to the roomwith the lamp?
which refers to the previous interac-tion.
Again, the system keeps no representation ofthe past actions of the user, but such utterances arethe ones that are found at this stage of the task plan.72We show in Figure 3 all candidate utterances se-lected when exiting the room in Figure 7.
That is,for our system purposes, all the utterances in the fig-ure are paraphrases of the one that is actually utteredin Figure 7.
As we explained in Section 3.2, theutterance with the longest reaction is selected first(?go back to the room with the lamp?
), the secondutterance with the longest reaction is selected sec-ond (?ok go back again to the room with the lamp?
),and so on.
As you can observe in Figure 3 the ut-terances in the candidate set can range from tele-graphic style like ?L?
to complex sentences like ?Gothrough the opening on the left with the yellow wallpaper?.
Several kinds of instructions are displayed,acknowledgements such as ?yes?, pure moving in-structions like ?left?
or ?straight?, instructions thatrefer to the local previous history such as ?go backout the room?
or ?ok now go out the same door?
andinstructions that refer back to the global history suchas ?okay now go back to the original room?.Due to the lack of orientation consideration in oursystem, some orientation dependent utterances areinappropriate in this particular context.
For instance,?left?
is incorrect given that the player does not haveto turn left but go straight in order to go throughthe correct door.
However, most of the instructions,even if quite different among themselves, could havebeen successfully used in the context of Figure 7.5 Evaluation and error analysisIn this section we present the results of the evalu-ation we carried out on the virtual instructor pre-sented in Section 4 which was generated using thedialogue model algorithm introduced in Section 3.We collected data from 13 subjects.
The partici-pants were mostly graduate students; 7 female and6 male.
They were not English native speakers butrated their English skills as near-native or very good.The evaluation contains both objective measureswhich we discuss in Section 5.1 and subjective mea-sures which we discuss in Section 5.2.5.1 Objective metricsThe objective metrics we extracted from the logs ofinteraction are summarized in Table 1.
The tablecompares our results with both human instructorsand the three rule-based virtual instructors that weretop rated in the GIVE-2 Challenge.
Their results cor-respond to those published in (Koller et al, 2010)which were collected not in a laboratory but con-necting the systems to users over the Internet.
Thesehand-coded systems are called NA, NM and Saar.We refer to our system as OUR.Human NA Saar NM OURTask success 100% 47% 40% 30% 70%Canceled 0% 24% n/a 35% 7%Lost 0% 29% n/a 35% 23%Time (sec) 543 344 467 435 692Mouse actions 12 17 17 18 14Utterances 53 224 244 244 194Table 1: Results for the objective metricsIn the table we show the percentage of games thatusers completed successfully with the different in-structors.
Unsuccessful games can be either can-celed or lost.
We also measured the average timeuntil task completion, and the average number of ut-terances users received from each system.
To ensurecomparability, we only counted successfully com-pleted games.In terms of task success, our system performs bet-ter than all hand-coded systems.
We duly notice that,for the GIVE Challenge in particular (and proba-bly for human evaluations in general) the successrates in the laboratory tend to be higher than the suc-cess rate online (this is also the case for completiontimes) (Koller et al, 2009).
Koller et al justify thisdifference by stating that the laboratory subject isbeing discouraged from canceling a frustrating taskwhile the online user is not.
However, it is also pos-sible that people canceled less because they foundthe interaction more natural and engaging as sug-gested by the results of the subjective metrics (seenext section).In any case, our results are preliminary given theamount of subjects that we tested, but they are in-deed encouraging.
In particular, our system helpedusers to identify better the objects that they neededto manipulate in the virtual world, as shown by thelow number of mouse actions required to completethe task (a high number indicates that the user musthave manipulated wrong objects).
This correlateswith the subjective evaluation of referring expres-sion quality (see next section).73We performed a detailed analysis of the instruc-tions uttered by our system that were unsuccessful,that is, all the instructions that did not cause the in-tended reaction as annotated in the corpus.
From the2081 instructions uttered in total (adding all the ut-terances of the 13 interactions), 1304 (63%) of themwere successful and 777 (37%) were unsuccessful.Given the limitations of the annotation discussedin Section 3.1 (wrong annotation of correction utter-ances and no representation of user orientation) weclassified the unsuccessful utterances using lexicalcues into 1) correction like ?no?
or ?wrong?, 2) ori-entation instruction such as ?left?
or ?straight?, and3) other.
We found that 25% of the unsuccessful ut-terances are of type 1, 40% are type 2, 34% are type3 (1% corresponds to the default utterance ?go?
thatour system utters when the set of candidate utter-ances is empty).
In Section 7 we propose an im-proved virtual instructor designed as a result of thiserror analysis.5.2 Subjective metricsThe subjective measures were obtained from re-sponses to the GIVE-2 questionnaire that was pre-sented to users after each game.
It asked users to ratedifferent statements about the system using a contin-uous slider.
The slider position was translated to anumber between -100 and 100.
As done in GIVE-2, for negative statements, we report the reversedscores, so that in Tables 2 and 3 greater numbersindicates that the system is better (for example, Q14shows that OUR system is less robotic than the rest).In this section we compare our results with the sys-tems NA, Saar and NM as we did in Section 5.1, wecannot compare against human instructors becausethese subjective metrics were not collected in (Gar-gett et al, 2010).The GIVE-2 Challenge questionnaire includestwenty-two subjective metrics.
Metrics Q1 to Q13and Q22 assess the effectiveness and reliability ofinstructions.
For almost all of these metrics we gotsimilar or slightly lower results than those obtainedby the three hand-coded systems, except for threemetrics which we show in Table 2.
We suspect thatthe low results obtained for Q5 and Q22 relate tothe unsuccessful utterances identified and discussedin Section 5.1 (for instance, corrections were some-times contradictory causing confusion and resultingin subjects ignoring them as they advanced in the in-teraction).
The high unexpected result in Q6, thatis indirectly assessing the quality of referring ex-pressions, demonstrates the efficiency of the refer-ring process despite the fact that nothing in the algo-rithms is dedicated to reference.
This good result isprobably correlated with the low number of mouseactions mentioned in Section 5.1.NA Saar NM OURQ5: I was confused about which direction to go in29 5 9 -12Q6: I had no difficulty with identifying the objects thesystem described for me18 20 13 40Q22: I felt I could trust the system?s instructions37 21 23 0Table 2: Results for the significantly different subjectivemeasures assessing the effectiveness of the instructions(the greater the number, the better the system)Metrics Q14 to Q20 are intended to assess the nat-uralness of the instructions, as well as the immer-sion and engagement of the interaction.
As Table 3shows, in spite of the unsuccessful utterances, oursystem is rated as more natural and more engaging(in general) than the best systems that competed inthe GIVE-2 Challenge.NA Saar NM OURQ14: The system?s instructions sounded robotic-4 5 -1 28Q15: The system?s instructions were repetitive-31 -26 -28 -8Q16: I really wanted to find that trophy-11 -7 -8 7Q17: I lost track of time while solving the task-16 -11 -18 16Q18: I enjoyed solving the task-8 -5 -4 4Q19: Interacting with the system was really annoying8 -2 -2 4Q20: I would recommend this game to a friend-30 -25 -24 -28Table 3: Results for the subjective measures assessingthe naturalness and engagement of the instructions (thegreater the number, the better the system)746 Portability to other virtual environmentsThe hand-coded systems, which we compared to, donot need a corpus in a particular GIVE virtual worldin order to generate instructions for any GIVE vir-tual world, while our system cannot do without suchcorpus.
These hand-coded systems are designed towork on different GIVE virtual worlds without theneed of training data, hence their algorithms aremore complex (e.g.
they include domain indepen-dent algorithms for generation of referring expres-sions) and take a longer time to develop.Our algorithm is independent of any particularvirtual world.
In fact, it can be ported to any otherinstruction giving task (where the DF has to per-form a physical task) with the same effort than re-quired to port it to a new GIVE world.
This is nottrue for the hand-coded GIVE systems.
The inputsof our algorithm are an off-the-shelf planner, a for-mal planning problem representation of the task anda human-human corpus collected on the very sametask the system aims to instruct.
It is important tonotice that any virtual instructor, in order to give in-structions that are both causally appropriate at thepoint of the task and relevant for the goal cannot dowithout such planning problem representation.
Fur-thermore, it is quite a normal practice nowadays tocollect a human-human corpus on the target task do-main.
It is reasonable, then, to assume that all theinputs of our algorithm are already available whendeveloping the virtual instructor, which was indeedthe case for the GIVE framework.Another advantage of our approach is that vir-tual instructor can be generated by developers with-out any knowledge of generation of natural languagetechniques.
Furthermore, the actual implementationof our algorithms is extremely simple as shown inFigures 1 and 2.
This makes our approach promisingfor application areas such as games and simulationtraining.7 Future work and conclusionsIn this paper we presented a novel algorithm forautomatically prototyping virtual instructors fromhuman-human corpora without manual annotation.Using our algorithms and the GIVE corpus we havegenerated a virtual instructor for a game-like vir-tual environment.
A video of our virtual instruc-tor is available in http://cs.famaf.unc.edu.ar/?luciana/give-OUR.
We obtained encouraging re-sults in the evaluation with human users that we didon the virtual instructor.
In our evaluation, our sys-tem outperforms rule-based virtual instructors hand-coded for the same task both in terms of objectiveand subjective metrics.
We plan to participate in theGIVE Challenge 20112 in order to get more evalua-tion data from online users and to evaluate our algo-rithms on multiple worlds.The algorithms we presented solely rely on theplan to define what constitutes the context of utter-ing.
It may be interesting though to make use ofother kinds of features.
For instance, in order to inte-grate spatial orientation and differentiate ?turn left?and ?turn right?, the orientation can be either addedto the planning domain or treated as a context fea-ture.
While it may be possible to add orientationin the planning domain of GIVE, it is not straight-forward to include the diversity of possible featuresin the same formalization, like modeling the globaldiscourse history or corrections.
Thus we plan to in-vestigate the possibility of considering the context ofan utterance as a set of features, including plan, ori-entation, discourse history and so forth, in order toextend the algorithms presented in terms of contextbuilding and feature matching operations.In the near future we plan to build a new versionof the system that improves based on the error anal-ysis that we did.
For instance, we plan to take ori-entation into account during selection.
As a resultof these extensions however we may need to enlargethe corpus we used so as not to increase the numberof situations in which the system does not find any-thing to say.
Finally, if we could identify correctionsautomatically, as suggested in (Raux and Nakano,2010), we could get an increase in performance, be-cause we would be able to treat them as correctionsand not as instructions as we do now.In sum, this paper presents the first existing al-gorithm for fully-automatically prototyping task-oriented virtual agents from corpora.
The generatedagents are able to effectively and naturally help auser complete a task in a virtual world by givingher/him instructions.2http://www.give-challenge.org/research75ReferencesSudeep Gandhe and David Traum.
2007a.
Creating spo-ken dialogue characters from corpora without annota-tions.
In Proceedings of 8th Conference in the AnnualSeries of Interspeech Events, pages 2201?2204, Bel-gium.Sudeep Gandhe and David Traum.
2007b.
Firststeps toward dialogue modelling from an un-annotatedhuman-human corpus.
In IJCAI Workshop on Knowl-edge and Reasoning in Practical Dialogue Systems,Hyderabad, India.Andrew Gargett, Konstantina Garoufi, Alexander Koller,and Kristina Striegnitz.
2010.
The GIVE-2 corpusof giving instructions in virtual environments.
In Pro-ceedings of the 7th International Conference on Lan-guage Resources and Evaluation (LREC), Malta.Jo?rg Hoffmann and Bernhard Nebel.
2001.
The FF plan-ning system: Fast plan generation through heuristicsearch.
JAIR, 14:253?302.Chih-Wei Hsu, Benjamin W. Wah, Ruoyun Huang,and Yixin Chen.
2006.
New features in SGPlanfor handling soft constraints and goal preferences inPDDL3.0.
In Proceedings of ICAPS.Dusan Jan, Antonio Roque, Anton Leuski, Jacki Morie,and David Traum.
2009.
A virtual tour guide for vir-tual worlds.
In Proceedings of the 9th InternationalConference on Intelligent Virtual Agents, IVA ?09,pages 372?378, Berlin, Heidelberg.
Springer-Verlag.Patrick Kenny, Thomas D. Parsons, Jonathan Gratch, An-ton Leuski, and Albert A. Rizzo.
2007.
Virtual pa-tients for clinical therapist skills training.
In Proceed-ings of the 7th international conference on IntelligentVirtual Agents, IVA ?07, pages 197?210, Berlin, Hei-delberg.
Springer-Verlag.Alexander Koller, Kristina Striegnitz, Donna Byron, Jus-tine Cassell, Robert Dale, Sara Dalzel-Job, JohannaMoore, and Jon Oberlander.
2009.
Validating theweb-based evaluation of nlg systems.
In Proceedingsof ACL-IJCNLP 2009 (Short Papers), Singapore.Alexander Koller, Kristina Striegnitz, Andrew Gargett,Donna Byron, Justine Cassell, Robert Dale, JohannaMoore, and Jon Oberlander.
2010.
Report on the sec-ond NLG challenge on generating instructions in vir-tual environments (GIVE-2).
In Proceedings of the In-ternational Natural Language Generation Conference(INLG), Dublin.Anton Leuski, Ronakkumar Patel, David Traum, andBrandon Kennedy.
2006.
Building effective questionanswering characters.
In Proceedings of the 7th SIG-dial Workshop on Discourse and Dialogue, SigDIAL?06, pages 18?27, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Antoine Raux and Mikio Nakano.
2010.
The dynam-ics of action corrections in situated interaction.
InProceedings of the SIGDIAL 2010 Conference, pages165?174, Tokyo, Japan, September.
Association forComputational Linguistics.Verena Rieser and Oliver Lemon.
2010.
Learning hu-man multimodal dialogue strategies.
Natural Lan-guage Engineering, 16:3?23.Bayan Abu Shawar and Eric Atwell.
2003.
Using di-alogue corpora to retrain a chatbot system.
In Pro-ceedings of the Corpus Linguistics Conference, pages681?690, United Kingdom.Bayan Abu Shawar and Eric Atwell.
2005.
Using cor-pora in machine-learning chatbot systems.
volume 10,pages 489?516.76A Automatically annotated fragment of the GIVE corpusUtterance: make a left and exit the roomReaction: ?move(b2-room-1-9,room-1-9), move(room-1-9,room-1-8), move(room-1-8,room-1-7),move(room-1-7,room-1-6), move(room-1-6,room-1-3), move(room-1-3,room-1-4),move(room-1-4,room-1-5), move(room-1-5,d3-room-1-5)?Utterance: go forward and turn 90 degreesReaction: ?move(d3-room-1-5,d3-room-2), move(d3-room-2,room-2)?Utterance: go into the room on the rightReaction: ?move(room-2,d6-room-2), move(d6-room-2,a2-d6-room-3), move(a2-d6-room-3,room-3)?Utterance: push the green button to the left of the red buttonReaction: ?move(room-3,b6-room-3), manipulate-stateless(b6), move(b6-room-3,room-3)?Utterance: go into the room on your rightReaction: ?move(room-3,d11-room-3), move(d11-room-3,d11-room-7), move(d11-room-7,room-7)?Utterance: turn 90 degrees right and push the red buttonReaction: ?move(room-7,b11-room-7), manipulate(b11), move(b11-room-7,room-7)?Utterance: on your right, push the yellow buttonReaction: ?move(room-7,b10-room-7), manipulate-stateless(b10), move(b10-room-7,room-7)?Utterance: turn 180 degrees and push the red button next to the plantReaction: ?move(room-7,b12-room-7), manipulate-stateless(b12), move(b12-room-7,room-7)?Utterance: turn 180 degrees and push the blue button in the middle of the yellow and blue buttonReaction: ?move(room-7,b8-b9-room-7), manipulate-stateless(b9), move(b8-b9-room-7,room-7)?Utterance: turn 90 degrees leftReaction: ?
?Utterance: go into the room on the rightReaction: ?move(room-7,d10-room-7), move(d10-room-7,d10-room-6), move(d10-room-6,room-6)?Utterance: turn right and proceed down the roomReaction: ?
?Utterance: push the red button next to the blue button on your rightReaction: ?move(room-6,b13-b14-room-6), manipulate(b14), move(b13-b14-room-6,room-6)?Utterance: turn left 120 degrees leftReaction: ?
?Utterance: and walk through the hallReaction: ?move(room-6,d9-room-6), move(d9-room-6,d9-room-5), move(d9-room-5,room-5)?77
