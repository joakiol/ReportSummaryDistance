Utterance Segmentation Using Combined ApproachBased on Bi-directional N-gram and Maximum EntropyDing LiuNational Laboratory of Pattern RecognitionInstitute of AutomationChinese Academy of SciencesBeijing 100080, China.dliu@nlpr.ia.ac.cnChengqing ZongNational Laboratory of Pattern RecognitionInstitute of AutomationChinese Academy of SciencesBeijing 100080, China.cqzong@nlpr.ia.ac.cnAbstractThis paper proposes a new approach tosegmentation of utterances into sentencesusing a new linguistic model based uponMaximum-entropy-weighted Bi-directional N-grams.
The usual N-gramalgorithm searches for sentence bounda-ries in a text from left to right only.
Thusa candidate sentence boundary in the textis evaluated mainly with respect to its leftcontext, without fully considering its rightcontext.
Using this approach, utterancesare often divided into incomplete sen-tences or fragments.
In order to make useof both the right and left contexts of can-didate sentence boundaries, we propose anew linguistic modeling approach basedon Maximum-entropy-weighted Bi-directional N-grams.
Experimental resultsindicate that the new approach signifi-cantly outperforms the usual N-gram al-gorithm for segmenting both Chinese andEnglish utterances.1 IntroductionDue to the improvement of speech recognitiontechnology, spoken language user interfaces, spo-ken dialogue systems, and speech translation sys-tems are no longer only laboratory dreams.Roughly speaking, such systems have the structureshown in Figure 1.Figure 1.
System with speech input.In these systems, the language analysis moduletakes the output of speech recognition as its input,representing the current utterance exactly as pro-nounced, without any punctuation symbols mark-ing the boundaries of sentences.
Here is anexample: ????????
9 ???????????????
913???
.
(this way pleaseplease take this elevator to the ninth floor the floorattendant will meet you at your elevator entrancethere and show you to room 913.)
As the exampleshows, it will be difficult for a text analysis moduleto parse the input if the utterance is not segmented.Further, the output utterance from the speech rec-ognizer usually contains wrongly recognizedwords or noise words.
Thus it is crucial to segmentthe utterance before further language processing.We believe that accurate segmentation can greatlyimprove the performance of language analysismodules.Stevenson et al have demonstrated the difficul-ties of text segmentation through an experiment inwhich six people, educated to at least the Bache-lor?s degree level, were required to segment intosentences broadcast transcripts from which allpunctuation symbols had been removed.
The ex-perimental results show that humans do not alwaysagree on the insertion of punctuation symbols, andthat their segmentation performance is not verygood (Stevenson and Gaizauskas, 2000).
Thus it isa great challenge for computers to perform the taskOutput (textor speech) Languageanalysis andgenerationSpeechrecognitionInput speechautomatically.
To solve this problem, many meth-ods have been proposed, which can be roughlyclassified into two categories.
One approach isbased on simple acoustic criteria, such as non-speech intervals (e.g.
pauses), pitch and energy.We can call this approach acoustic segmentation.The other approach, which can be called linguisticsegmentation, is based on linguistic clues, includ-ing lexical knowledge, syntactic structure, seman-tic information etc.
Acoustic segmentation can notalways work well, because utterance boundaries donot always correspond to acoustic criteria.
For ex-ample: ??<pause>??<pause>?????????<pause>??<pause>?????.
Sincethe simple acoustic criteria are inadequate, linguis-tic clues play an indispensable role in utterancesegmentation, and many methods relying on themhave been proposed.This paper proposes a new approach to linguis-tic segmentation using a Maximum-entropy-weighted Bi-directional N-gram-based algorithm(MEBN).
To evaluate the performance of MEBN,we conducted experiments in both Chinese andEnglish.
All the results show that MEBN outper-forms the normal N-gram algorithm.
The remain-der of this paper will focus on description of ournew approach for linguistic segmentation.
In Sec-tion 2, some related work on utterance segmenta-tion is briefly reviewed, and our motivations aredescribed.
Section 3 describes MEBN in detail.The experimental results are presented in Section 4.Finally, Section 5 gives our conclusion.2 Related Work and Our Motivations2.1 Related WorkStolcke et al (1998, 1996) proposed an approachto detection of sentence boundaries and disfluencylocations in speech transcribed by an automaticrecognizer, based on a combination of prosodiccues modeled by decision trees and N-gram lan-guage models.
Their N-gram language model ismainly based on part of speech, and retains somewords which are particularly relevant to segmenta-tion.
Of course, most part-of-speech taggers re-quire sentence boundaries to be pre-determined; soto require the use of part-of-speech information inutterance segmentation would risk circularity.
Cet-tolo et al?s (1998) approach to sentence boundarydetection is somewhat similar to Stolcke et al?s.They applied word-based N-gram language modelsto utterance segmentation, and then combinedthem with prosodic models.
Compared with N-gram language models, their combined modelsachieved an improvement of 0.5% and 2.3% inprecision and recall respectively.Beeferman et al (1998) used the CYBERPUNCsystem to add intra-sentence punctuation (espe-cially commas) to the output of an automaticspeech recognition (ASR) system.
They claim that,since commas are the most frequently used punc-tuation symbols, their correct insertion is by far themost helpful addition for making texts legible.CYBERPUNC augmented a standard trigramspeech recognition model with lexical informationconcerning commas, and achieved a precision of75.6% and a recall of 65.6% when testing on 2,317sentences from the Wall Street Journal.Gotoh et al (1998) applied a simple non-speechinterval model to detect sentence boundaries inEnglish broadcast speech transcripts.
They com-pared their results with those of N-gram languagemodels and found theirs far superior.
However,broadcast speech transcripts are not really spokenlanguage, but something more like spoken writtenlanguage.
Further, radio broadcasters speak for-mally, so that their reading pauses match sentenceboundaries quite well.
It is thus understandable thatthe simple non-speech interval model outperformsthe N-gram language model under these conditions;but segmentation of natural utterances is quite dif-ferent.Zong et al (2003) proposed an approach to ut-terance segmentation aiming at improving the per-formance of spoken language translation (SLT)systems.
Their method is based on rules which areoriented toward key word detection, templatematching, and syntactic analysis.
Since this ap-proach is intended to facilitate translation of Chi-nese-to-English SLT systems, it rewrites longsentences as several simple units.
Once again,these results cannot be regarded as general-purposeutterance segmentation.
Furuse et al (1998) simi-larly propose an input-splitting method for translat-ing spoken language which includes many long orill-formed expressions.
The method splits an inputinto well-balanced translation units, using a seman-tic dictionary.Ramaswamy et al (1998) applied a maximumentropy approach to the detection of commandboundaries in a conversational natural languageuser interface.
They considered as their featureswords and their distances to potential boundaries.They posited 400 feature functions, and trainedtheir weights using 3000 commands.
The systemthen achieved a precision of 98.2% in a test set of1900 commands.
However, command sentencesfor conversational natural language user interfacescontain much smaller vocabularies and simplerstructures than the sentences of natural spoken lan-guage.
In any case, this method has been veryhelpful to us in designing our own approach to ut-terance segmentation.There are several additional approaches which arenot designed for utterance segmentation but whichcan nevertheless provide useful ideas.
For example,Reynar et al (1997) proposed an approach to thedisambiguation of punctuation marks.
They con-sidered only the first word to the left and right ofany potential sentence boundary, and claimed thatexamining wider context was not beneficial.
Thefeatures they considered included the candidate?sprefix and suffix; the presence of particular charac-ters in the prefix or suffix; whether the candidatewas honorific (e.g.
Mr., Dr.); and whether the can-didate was a corporate designator (e.g.
Corp.).
Thesystem was tested on the Brown Corpus, andachieved a precision of 98.8%.
Elsewhere, Nakanoet al (1999) proposed a method for incrementallyunderstanding user utterances whose semanticboundaries were unknown.
The method operatedby incrementally finding plausible sequences ofutterances that play crucial roles in the task execu-tion of dialogues, and by utilizing beam search todeal with the ambiguity of boundaries and withsyntactic and semantic ambiguities.
Though themethod does not require utterance segmentationbefore discourse processing, it employs specialrule tables for discontinuation of significant utter-ance boundaries.
Such rule tables are not easy tomaintain, and experimental results have demon-strated only that the method outperformed themethod assuming pauses to be semantic boundaries.2.2 Our motivationsThough numerous methods for utterance segmen-tation have been proposed, many problems remainunsolved.One remaining problem relates to the languagemodel.
The N-gram model evaluates candidatesentence boundaries mainly according to their leftcontext, and has achieved reasonably good results,but it can?t take into account the distant right con-text to the candidate.
This is the reason that N-gram methods often wrongly divide some longsentences into halves or multiple segments.
Forexample:????????.
The N-gram methodis likely to insert a boundary mark between ??
?and ??
?, which corresponds to our everyday im-pression that, if reading from the left and notconsidering several more words to the right of thecurrent word, we will probably consider ??????
as a whole sentence.
However, we find that, ifwe search the sentence boundaries from right toleft, such errors can be effectively avoided.
In thepresent example, we won?t consider ?????
?as a whole sentence, and the search will be contin-ued until the word ???
is encountered.
Accord-ingly, in order to avoid segmentation errors madeby the normal N-gram method, we propose a re-verse N-gram segmentation method (RN) whichdoes seek sentence boundaries from right to left.Further, we simply integrate the two N-grammethods and propose a bi-directional N-grammethod (BN), which takes into account both theleft and the right context of a candidate segmenta-tion site.
Since the relative usefulness or signifi-cance of the two N-gram methods variesdepending on the context, we propose a method ofweighting them appropriately, using parametersgenerated by a maximum entropy method whichtakes as its features information about words in thecontext.
This is our Maximum-Entropy-WeightedBi-directional N-gram-based segmentation method.We hope MEBN can retain the correct segmentsdiscovered by the usual N-gram algorithm, yet ef-fectively skip the wrong segments.3 Maximum-Entropy-Weighted Bi-directional N-gram-based SegmentationMethod3.1 Normal N-gram Algorithm (NN) for Ut-terance SegmentationAssuming that mWWW ...21 (where m is a naturalnumber) is a word sequence, we consider it as an norder Markov chain, in which the word)1( miWi ??
is predicted by the n-1 words to itsleft.
Here is the corresponding formula:)...|()...|( 11121 ?+??
= iniiii WWWPWWWWPFrom this conditional probability formula for aword, we can derive the probability of a word se-quence iWWW ...21 :)...|()...()...( 12112121 ??
?= iiii WWWWPWWWPWWWPIntegrating the two formulas above, we get:)...|()...()...( 1112121 ?+??
?= iniiii WWWPWWWPWWWPLet us use SB to indicate a sentence boundaryand add it to the word sequence.
The value of)...( 121 +ii SBWWWWP  and )...( 121 +iiWWWWP willdetermine whether a specific word)1( miWi ??
is the final word of a sentence.
Wesay iW  is the final word of a sentence if and onlyif )...( 121 +ii SBWWWWP > )...( 121 +iiWWWWP .Taking the trigram as our example and consid-ering the two cases where Wi-1 is and is not thefinal word of a sentence, )...( 121 +ii SBWWWWPand )...( 121 +iiWWWWP  is computed respectivelyby the following two formulas:)|()...()|()...()...()|()|()...()|()|()...()...(1112112112111121121121iiiiiiiiiiiiiiiiiiiiiiWWWPWWWWPSBWWPSBWWWPWWWWPSBWWPWWSBPWWWWPSBWWPSBWSBPSBWWWPSBWWWWP?+?+++??++?+?=??+?
?=In the normal N-gram method, the above iterativeformulas are computed to search the sentenceboundaries from 1W  to mW .3.2 Reverse N-gram Algorithm (RN) for Ut-terance SegmentationIn the reverse N-gram segmentation method, wetake the word sequence mWWW ...21  as a reverseMarkov chain in which )1( miWi ??
is predictedby the n-1 words to its right.
That is:)...|()...|( 1111 +?++?
= iniiimmi WWWPWWWWPAs in the N-gram algorithm, we compute theoccurring probability of word sequencemWWW ...21  using the formula:)...|()...()...( 11111 +?+??
?= immiimmimm WWWWPWWWPWWWPThen the iterative computation formula is:)...|()...()...( 11111 +?++??
?= iniiimmimm WWWPWWWPWWWPBy adding SB to the word sequence, we say iWis the final word of a sentence if and only if)...( 11 iimm SBWWWWP +?
> )...( 11 iimm WWWWP +?
.Similar to NN, )...( 11 iimm SBWWWWP +?
and)...( 11 iimm WWWWP +?
are computed as follows inthe trigram:)|()...()|()...()...()|()|()...()|()|()...()...(1212111111112121111111++++?++?+?+++++?+++?+??+?=??+?
?=iiiiimmiiimmiimmiiiiiimmiiiimmiimmWWWPWWWWPSBWWPSBWWWPWWWWPSBWWPWWSBPWWWWPSBWWPSBWSBPSBWWWPSBWWWWPIn contrast to the normal N-gram segmentationmethod, we compute the above iterative formulasto seek sentence boundaries from mW  to 1W .3.3 Bi-directional N-gram Algorithm for Ut-terance SegmentationFrom the iterative formulas of the normal N-gramalgorithm and the reverse N-gram algorithm, wecan see that the normal N-gram method recognizesa candidate sentence boundary location mainlyaccording to its left context, while the reverse N-gram method mainly depends on its right context.Theoretically at least, it is reasonable to supposethat, if we synthetically consider both the left andthe right context by integrating the NN and the RN,the overall segmentation accuracy will be im-proved.Considering the word sequence mWWW ...21 , thecandidate sites for sentence boundaries may befound between 1W  and 2W , between 2W  and3W , ?, or between 1?mW and mW .
The number ofcandidate sites is thus m-1.
We number those m-1candidate sites 1, 2 ?
m-1 in succession, and weuse )(iPis )11( ???
mi  and)(iPno )11( ???
mi  respectively to indicate theprobability that the current site i really is, or is not,a sentence boundary.
Thus, to compute the wordsequence segmentation, we must compute )(iPisand )(iPno  for each of the m-1 candidate sites.
Inthe bi-directional BN, we compute )(iPis  and)(iPno  by combining the NN results and RN re-sults.
The combination is described by the follow-ing formulas:)()()()()()(______iPiPiPiPiPiPRNnoNNnoBNnoRNisNNisBNis?=?=where )(_ iP NNis , )(_ iP NNno  denote the probabili-ties calculated by NN which correspond to)...( 121 +ii SBWWWWP  and )...( 121 +iiWWWWP insection 3.1 respectively and )(_ iP RNis , )(_ iP RNnodenote the probabilities calculated by RN whichcorrespond to )...( 11 iimm SBWWWWP +?
and)...( 11 iimm WWWWP +?
in section 3.2 respectively.We say there exits a sentence boundary at site i)11( ???
mi if and only if )()( __ iPiP BNnoBNis > .3.4 Maximum Entropy Approach for Utter-ance SegmentationIn this section, we explain our maximum-entropy-based model for utterance segmentation.
That is,we estimate the joint probability distribution of thecandidate sites and their surrounding words.
Sincewe consider information concerning the lexicalcontext to be useful, we define the feature func-tions for our maximum method as follows:???
===elsebScefixincludeifcbf jj 0)0&&)),((Pr(1),(10???
===elsebScefixincludeifcbf jj 0)1&&)),((Pr(1),(11???
===elsebScSuffixincludeifcbf jj 0)0&&)),(((1),(20???
===elsebScSuffixincludeifcbf jj 0)1&&)),(((1),(21Sj denotes a sequence of one or more wordswhich we can call the Matching String.
(Note thatSj may contain the sentence boundary mark ?SB?.
)The candidate c?s state is denoted by b, where b=1indicates that c is a sentence boundary and b=0indicates that it is not a boundary.
Prefix(c) de-notes all the word sequences ending with c (that is,c's left context plus c) and Suffix(c) denotes all theword sequences beginning with c (in other words,c plus its right context).
For example: in the utter-ance: ?<c1>?<c2>?<c3>?<c4>?<c5>?,??
?, ???
?,  and ?????
are c3?s Prefix, while???
, ???
?and ?????
are c3?s Suffix.
Thevalue of function )),((Pr jScefixinclude  is truewhen word sequence Sj is one of c?s Prefixes, andthe value of function )),(( jScSuffixinclude  istrue when Sj is one of c?s Suffixes.Corresponding to the four feature functions),(10 cbf j , ),(11 cbf j , ),(20 cbf j , ),(21 cbf j  are thefour parameters 10j?
, 11j?
, 20j?
, 21j?
.
Thus thejoint probability distribution of the candidate sitesand their surrounding contexts is given by:)(),(1),(21),(20),(11),(1021201110?
= ??
?= kj cbfjcbfjcbfjcbfj jjjjbcP ????
?where k is the total number of the Matching Stringsand ?
is a parameter set to make P(c,1) and P(c,0)sum to 1.
The unknown parameters10j?
, 11j?
, 20j?
, 21j?
are chosen to maximize thelikelihood of the training data using the General-ized Iterative Scaling (Darroch and Ratcliff, 1972)algorithm.
In the maximum entropy approach, wesay that a candidate site is a sentence boundary ifand only if P(c, 1) > P(c, 0).
(At this point, we cananticipate a technical problem with the maximumapproach to utterance segmentation.
When aMatching String contains SB, we cannot knowwhether it belongs to the Prefixes or Suffixes ofthe candidate site until the left and right contexts ofthe candidate site have been segmented.
Thus if thesegmentation proceeds from left to right, the lexi-cal information in the right context of the currentcandidate site will always remain uncertain.
Like-wise, if it proceeds from right to left, the informa-tion in the left context of the current candidate siteremains uncertain.
The next subsection will de-scribe a pragmatic solution to this problem.
)3.5 Maximum-Entropy-Weighted Bi-directional N-gram Algorithm for Utter-ance SegmentationIn the bi-directional N-gram based algorithm, wehave considered the left-to-right N-gram algorithmand the right-to-left algorithm as having the samesignificance.
Actually, however, they should beassigned differing weights, depending on the lexi-cal contexts.
The combination formulas are as fol-lows:)()()()()()()()()()(________iPCWiPCWiPiPCWiPCWiPRNnoinorNNnoinonnoRNisiisrNNisiisnis???=??
?=)(_ iisn CW , )(_ inon CW , )(_ iisr CW , )(_ inor CWare the functions of the context surrounding candi-date site i which denotes the weights of)(_ iP NNis , )(_ iP NNno , )(_ iP RNis  and )(_ iP RNno  re-spectively.
Assuming that the weights of )(_ iP NNisand )(_ iP NNno  depend upon the context to the leftof the candidate site, and that the weights of)(_ iP RNis  and )(_ iP RNno  depend on the context tothe right of the candidate site, the weight functionscan be rewritten as:)(_ iisn LeftCW , )(_ inon LeftCW , )(_ iisr RightCW ,)(_ inor RightCW .
It is reasonable to assume that asthe joint probability ),( SBiLeftCP i =  rises,)(_ iP NNis  will increase in significance.
(The jointprobability in question is the probability of the cur-rent candidate?s left context, taken together withthe probability that the candidate is a sentenceboundary.)
Therefore the value of )(_ iisn LeftCWis given by ),()(_ SBiLeftCPLeftCW iiisn == .Similarly we can give the formulas for comput-ing )(_ inon LeftCW , )(_ iisr RightCW , and)(_ inor RightCW  as follows:)!,()(_ SBiLeftCPLeftCW iinon ==),()(_ SBiRightCPRightCW iiisr ==)!,()(_ SBiRightCPRightCW iinor ==We can easily get the values of),( SBiLeftCP i = , )!,( SBiLeftCP i = ,),( SBiRightCP i = , and )!,( SBiRightCP i =using the method described in the maximum en-tropy approach section.
For example:?
=== kj ifji jSBiLeftCP 1 ),1(1111),( ???
=== kj ifji jSBiLeftCP 1 ),0(1010)!,( ?
?As mentioned in last subsection, we need seg-mented contexts for maximum entropy approach.Since the maximum entropy parameters for MEBNalgorithm are used as modifying NN and RN, wejust estimate the joint probability of the candidateand its surrounding contexts based upon the seg-ments by NN and RN.
Using NLeftCi indicate theleft context to the candidate i which has been seg-mented by NN algorithm and RRightCi indicate theright context to i which has been segmented by RN,the combination probability computing formulasfor MEBN are as follows:)()!,()()!,()()(),()(),()(______iPSBiRRightCPiPSBiNLeftCPiPiPSBiRRightCPiPSBiNLeftCPiPRNnoiNNnoiMEBNnoRNisiNNisiMEBNis?=??==?=?
?==We evaluate site i as a sentence boundary if andonly if )()( __ iPiP MEBNnoMEBNis > .4 Experiment4.1 Model TrainingOur models are trained on both Chinese and Eng-lish corpora, which cover the domains of hotel res-ervation, flight booking, traffic information,sightseeing, daily life and so on.
We replaced thefull stops with ?SB?
and removed all other punc-tuation marks in the training corpora.
Since in mostactual systems part of speech information cannotbe accessed before determining the sentenceboundaries, we use Chinese characters and Englishwords without POS tags as the units of our N-grammodels.
Trigram and reverse trigram probabilitiesare estimated based on the processed training cor-pus by using Modified Kneser-Ney Smoothing(Chen and Goodman, 1998).
As to the maximumentropy model, the Matching Strings are chosen asall the word sequences occurring in the trainingcorpus whose length is no more than 3 words.
Theunknown parameters corresponding to the featurefunctions are generated based on the training cor-pus using the Generalized Iterative Scaling algo-rithm.
Table 1 gives an overview of the trainingcorpus.Corpus SIZE SB Num-berAverage Lengthof SentenceChinese 4.02MB 148967 8 Chinese charac-tersEnglish 4.49MB 149311 6 wordsTable 1.
Overview of the Training Corpus.4.2 Testing ResultsWe test our methods using open corpora which arealso limited to the domains mentioned above.
Allpunctuation marks are removed from the test cor-pora.
An overview of the test corpus appears intable 2.Corpus SIZE SBNumberAverage Lengthof SentenceChinese 412KB 12032 10 Chinese char-actersEnglish 391KB 10518 7 wordsTable 2.
Overview of the Testing Corpus.We have implemented four segmentation algo-rithms using NN, RN, BN and MEBN respectively.If we use ?RightNum?
to denote the number ofright segmentations, ?WrongNum?
denote thenumber of wrong segmentations, and ?TotalNum?to denote the number of segmentations in theoriginal testing corpus, the precision (P) can becomputed using the formulaP=RightNum/(RightNum+WrongNum), the recall(R) is computed as R=RightNum/TotalNum, andthe F-Score is computed as F-Score =RPRP+?
?2.The testing results are described in Table 3 andTable 4.Methods Total NumRightNumWrongNumPreci-sion  Recall F-ScoreNN 12032 10167 2638 79.4% 84.5% 81.9%RN 12032 10396 2615 79.9% 86.4% 83.0%BN 12032 10528 2249 82.4% 87.5% 84.9%MEBN 12032 10348 1587 86.7% 86.0% 86.3%Table 3.
Experimental Results for Chinese Utter-ance Segmentation.Methods Total NumRightNumWrongNumPreci-sion  Recall F-ScoreNN 10518 8730 3164 73.4% 83.0% 77.9%RN 10518 9014 3351 72.9% 85.7% 78.8%BN 10518 9056 3019 75.0% 86.1% 80.2%MEBN 10518 8929 2403 78.8% 84.9% 81.7%Table 4.
Experimental Results for English Utter-ance Segmentation.From the result tables it is clear that RN, BN, andMEBN all outperforms the normal N-gram algo-rithm in the F-score for both Chinese and Englishutterance segmentation.
MEBN achieved the bestperformance which improves the precision by7.3% and the recall by 1.5% in the Chinese ex-periment, and improves the precision by 5.4% andthe recall by 1.9% in the English experiment.4.3 Result analysisMEBN was proposed in order to maintain the cor-rect segments of the normal N-gram algorithmwhile skipping the wrong segments.
In order to seewhether our original intention has been realized,we compared the segments as determined by RNwith those determined by NN, compare the seg-ments found by BN with those of NN and thencompare the segments found by MEBN with thoseof NN.
For RN, BN and MEBN, suppose TN de-notes the number of total segmentations, CON de-notes the number of correct segmentationsoverlapping with those found by NN; SWN de-notes the number of wrong NN segmentationswhich were skipped; WNON denotes the numberof wrong segmentations not overlapping with thoseof NN; and CNON denotes the number of segmen-tations which were correct but did not overlap withthose of NN.
The statistical results are listed inTable 5 and Table 6.Methods TN CON SWN WNON CNONRN 13011 9525 1098 1077 870BN 12777 9906 753 355 622MEBN 11935 9646 1274 223 678Table 5.
Chinese Utterance Segmentation ResultsComparison.Methods TN CON SWN WNON CNONRN 12365 8223 1077 1271 792BN 12075 8565 640 488 491MEBN 11332 8370 1247 486 559Table 6.
English Utterance Segmentation ResultsComparison.Focusing upon the Chinese results, we can see thatRN skips 1098 incorrect segments found by NN,and has 9525 correct segments in common withthose of NN.
It verifies our supposition that RNcan effectively avoid some errors made by NN.But because at the same time RN brings in 1077new errors, RN doesn?t improve much in precision.BN skips 753 incorrect segments and brings in 355new segmentation errors; has 9906 correct seg-ments in common with those of NN and brings in622 new correct segments.
So by equally integrat-ing NN and RN, BN on one hand finds more cor-rect segments, on the other hand brings in lesswrong segments than NN.
But in skipping incor-rect segments by NN, BN still performs worse thanRN, showing that it only exerts the error skippingability of RN to some extent.
As for MEBN, itskips 1274 incorrect segments and at the same timebrings in only 223 new incorrect segments.
Addi-tionally it maintains 9646 correct segments in com-mon with those of NN and brings in 678 newcorrect segments.
In recall MEBN performs a littleworse than BN, but in precision it achieves a muchbetter performance than BN, showing that modi-fied by the maximum entropy weights, MEBNmakes use of the error skipping ability of RN moreeffectively.
Further, in skipping wrong segmentsby NN, MEBN even outperforms RN, which indi-cates the weights we set on NN and RN not onlyact as modifying parameters, but also have directbeneficial affection on utterance segmentation.5 ConclusionThis paper proposes a reverse N-gram algorithm, abi-directional N-gram algorithm and a Maximum-entropy-weighted Bi-directional N-gram algorithmfor utterance segmentation.
The experimental re-sults for both Chinese and English utterance seg-mentation show that MEBN significantlyoutperforms the usual N-gram algorithm.
This isbecause MEBN takes into account both the left andright contexts of candidate sites: it integrates theleft-to-right N-gram algorithm and the right-to-leftN-gram algorithm with appropriate weights, usingclues on the sites?
lexical context, as modeled bymaximum entropy.AcknowledgementsThis work is sponsored by the Natural SciencesFoundation of China under grant No.60175012, aswell as supported by the National Key Fundamen-tal Research Program (the 973 Program) of Chinaunder the grant G1998030504.The authors are very grateful to Dr. Mark Selig-man for his very useful suggestions and his verycareful proofreading.ReferencesBeeferman D., A. Berger, and J. Lafferty.
1998.CYBERPUNC: A lightweight punctuation annotationsystem for speech.
In Proceedings of the IEEE Inter-national Conference on Acoustics, Speech and SignalProcessing, Seattle, WA.
pp.
689-692.Beeferman D., A. Berger, and J. Lafferty.
1999.
Statisti-cal models for text segmentation.
Machine Learning34, pp 177-210.Berger A., S. Della Pietra, and V. Della Pietra.
1996.
AMaximum Entropy Approach to Natural LanguageProcessing.
Computational Linguistics, 22(1), pp.
39-71.Cettolo M. and D. Falavigna.
1998.
AutomaticDetection of Semantic Boundaries Based on Acousticand Lexical Knowledge.
ICSLP 1998, pp.
1551-1554.Chen S. F. and J. Goodman.
1998.
An empirical studyof smoothing techniques for language modeling.
Tech-nical Report TR-10-98, Center for Research in Com-puting Technology, Harvard University.
pp.243-255.Darroch J. N. and D. Ratcliff.
1972.
Generalized Itera-tive Scaling for Log-Linear Models.
The Annals ofMathematical Statistics, 43(5), pp.
1470-1480.Furuse O., S. Yamada, and K. Yamamoto.
1998.
Split-ting Long or Ill-formed Input for Robust Spoken-language Translation.
COLING-ACL 1998, pp.
421-427.Gotoh Y. and S. Renals.
2000.
Sentence Boundary De-tection in Broadcast Speech Transcripts.
In Proc.
In-ternational Workshop on Automatic SpeechRecognition, pp.
228-235.Nakano M., N. Miyazaki, J. Hirasawa, K. Dohsaka, andT.
Kawabata.
1999.
Understanding Unsegmented UserUtterances in Real-Time Spoken Dialogue Systems.Proceedings of the 37th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-99), Col-lege Park, MD, USA, pp.
200-207.Ramaswamy N. G. and J. Kleindienst.
1998.
AutomaticIdentification of Command Boundaries in a Conversa-tional Natural Language User Interface.
ICSLP 1998.pp.
401-404.Reynar J. and A. Ratnaparkhi.
1997.
A maximum en-tropy approach to identifying sentence boundaries.
InProceedings of the 5th Conference on Applications ofNatural Language Processing (ANLP), WashingtonDC, pp.
16-19.Seligman M. 2000.
Nine Issues in Speech Translation.In Machine Translation, 15, pp.
149-185.Stevenson M. and R. Gaizauskas.
2000.
Experiments onsentence boundary detection.
In Proceedings of theSixth Conference on Applied Natural Language Proc-essing and the First Conference of the North AmericanChapter of the Association for Computational Linguis-tics, pp.
24-30.Stolcke A. and E. Shriberg.
1996.
Automatic linguisticsegmentation of conversational speech.
Proc.
Intl.Conf.
on Spoken Language Processing, Philadelphia,PA, vol.
2, pp.
1005-1008.Stolcke A., E. Shriberg, R. Bates, M. Ostendorf, D.Hakkani, M. Plauche, G. Tur, and Y. Lu.
1998.
Auto-matic Detection of Sentence Boundaries and Disfluen-cies based on Recognized Words.
Proc.
Intl.
Conf.
onSpoken Language Processing, Sydney, Australia, vol.5, pp.
2247-2250.Zong, C. and F. Ren.
2003.
Chinese Utterance Segmen-tation in Spoken Language translation.
In Proceedingsof the 4th international conference on intelligent textprocessing and Computational Linguistics (CICLing),Mexico, Feb 16-22. pp.
516-525.Zhou Y.
2001.
Utterance Segmentation Based on Deci-sion Tree.
Proceedings of the 6th National joint Con-ference on Computational Linguistics, Taiyuan, China,pp.
246-252.
