Proceedings of the 12th Conference of the European Chapter of the ACL, pages 442?450,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsOptimization in Coreference Resolution Is Not Needed:A Nearly-Optimal Algorithm with Intensional ConstraintsManfred Klenner & E?tienne AilloudComputational LinguisticsZurich University, Switzerland{klenner, ailloud}@cl.uzh.chAbstractWe show how global constraints such as transitiv-ity can be treated intensionally in a Zero-One Inte-ger Linear Programming (ILP) framework which isgeared to find the optimal and coherent partition ofcoreference sets given a number of candidate pairsand their weights delivered by a pairwise classifier(used as reliable clustering seed pairs).
In order tofind out whether ILP optimization, which is NP-complete, actually is the best we can do, we com-pared the first consistent solution generated by ouradaptation of an efficient Zero-One algorithm withthe optimal solution.
The first consistent solution,which often can be found very fast, is already asgood as the optimal solution; optimization is thusnot needed.1 IntroductionOne of the main advantages of Integer Linear Pro-gramming (ILP) applied to NLP problems is thatprescriptive linguistic knowledge can be used topose global restrictions on the set of desirable so-lutions.
ILP tries to find an optimal solution whileadhering to the global constraints.
One of thecentral global constraints in the field of corefer-ence resolution evolves from the interplay of intra-sentential binding constraints and the transitivityof the anaphoric relation.
Consider the followingsentence taken from the Internet: ?He told him thathe deeply admired him?.
?He?
and ?him?
are ex-clusive (i.e.
they could never be coreferent) withintheir clauses (the main and the subordinate clause,respectively).
A pairwise classifier could learn thisgiven appropriate features or, alternatively, bind-ing constraints could act as a hard filter preventingsuch pairs from being generated at all.
But in ei-ther case, since pairwise classification is trappedin its local perspective, nothing can prevent theclassifier to resolve the ?he?
and ?him?
from thesubordinate clause in two independently carriedout steps to the same antecedent from the mainclause.
It is transitivity that prohibits such an as-signment: if two elements are both coreferent toa common third element, then the two are (transi-tively given) coreferent as well.
If they are knownto be exclusive, such an assignment is disallowed.But transitivity is beyond the scope of pairwiseclassification?it is a global phenomena.
The so-lution is to take ILP as a clustering device, wherethe probabilities of the pairwise classifier are in-terpreted as weights and transitivity and other re-strictions are acting as global constraints.Unfortunately, in an ILP program every con-straint has to be extensionalized (i.e.
all instantia-tions of the constraint are to be generated).
Cap-turing transitivity for e.g.
150 noun phrases (about30 sentences) already produces 1,500,000 equa-tions (cf.
Section 4).
Solving such ILP programsis far too slow for real applications (let alne itsbrute force character).A closer look at existing ILP approaches to NLPreveals that they are of a special kind, namelyZero-One ILP with unweighted constraints.
Al-though still NP-complete there exist a number ofalgorithms such as the Balas algorithm (Balas,1965) that efficiently explore the search space andreduce thereby run time complexity in the mean.We have adapted Balas?
algorithm to the specialneeds of coreference resolution.
First and fore-most, this results in an optimization algorithm thattreats global constraints intensionally, i.e.
thatgenerates instantiations of a constraint only on de-mand.
Thus, transitivity can be captured for eventhe longest texts.
But more important, we foundout empirically that ?full optimization?
is not re-ally needed.
The first consistent solution, whichoften can be found very fast, is already as good?in terms of F-measure values?as the optimal so-lution.
This is good news, since it reduces runtimeand at same time maintains the empirical results.We first introduce Zero-One ILP, discuss ourbaseline model and give an ILP formalization ofcoreference resolution.
Then we go into the de-tails of our Balas adaptation and provide empiri-cal evidence for our central claim?that optimiza-tion search can already be stopped (without qual-442ity loss) when the first consistent solution has beenfound.2 Zero-One Integer LinearProgramming (ILP)The algorithm in (Balas, 1965) solves Zero-One Integer Linear Programming (ILP), where aweighted linear function (the objective function)of binary variables F(x1, .
.
.
,xn) = w1x1 + .
.
.
+wnxn is to be minimized under the regiment oflinear inequalities a1x1 + .
.
.+ anxn ?
A.1 Unlikeits real-valued counterpart, Zero-One ILP is NP-complete (cf., say, (Papadimitriou and Steiglitz,1998)), but branch-and-bound algorithms with ef-ficient heuristics exist, as the Balas Algorithm:Balas (1965) proposes an approach where the ob-jective function?s addends are sorted according tothe magnitude of the weights: 0 ?
w1 ?
.
.
.
?wn.
This preliminary ordering induces the follow-ing functioning principles for the algorithm (see(Chinneck, 2004, Chap.
13) for more details):1.
It seeks to minimize F , so that a solution withas few 1s as possible is preferred.2.
If, during exploration of solutions, con-straints force an xi to be set to 1, then it shouldbear as small an index as possible.The Balas algorithm follows a depth-first searchwhile checking feasibility (i.e., through the con-straints) of the branches partially explored: Uponbranching, the algorithm bounds the cost of set-ting the current variable xN to 1 by the costs ac-cumulated so far: w1x1 + .
.
.+ wN?1xN?1 + wN isnow the lowest cost this branch may yield.
If,on the contrary, xN is set to 0, a violated ?-constraint may only be satisfied via an xi set to 1(i > N), so the cheapest change to ameliorate thepartial solution is to set the right-next variable to1: w1x1 + .
.
.+ wN?1xN?1 + wN+1 would be thecheapest through this branch.If setting all weights past the branching variableto 0 yields a cheaper solution than the so far mini-mal solution obtained, then it is worthwile explor-ing this branch, and the algorithms goes on to thenext weighted variable, until it reaches a feasiblesolution; otherwise it backtracks to the last unex-plored branching.
The complexity thus remainsexponential in the worst case, but the initial order-ing of weights is a clever guide.1Maximization and coping with?-constraints are also ac-cessible via simple transformations.3 Our Baseline ModelThe memory-based learner TiMBL (Daelemanset al, 2004) is used as a (pairwise) classifier.TiMBL stores all training examples, learns fea-ture weights and classifies test instances accord-ing to the majority class of the k-nearest (i.e.
mostsimilar) neighbors.
We have experimented withvarious features; Table 1 lists the set we have fi-nally used (Soon et al (2001) and Ng and Cardie(2002) more thoroughly discuss different featuresand their benefits):- distance in sentences and markables- part of speech of the head of the markables- the grammatical functions- parallelism of grammatical functions- do the heads match or not- where is the pronoun (if any): left or right- word form if POS is pronoun- salience of the non-pronominal phrases- semantic class of noun phrase headsTable 1: Features for Pairwise ClassificationAs a gold standard the Tu?Ba-D/Z (Telljohannet al, 2005; Naumann, 2006) coreference corpusis used.
The Tu?Ba is a treebank (1,100 Germannewspaper texts, 25,000 sentences) augmentedwith coreference annotations2.
In total, there are13,818 anaphoric, 1,031 cataphoric and 12,752coreferential relations.
There are 3,295 relativepronouns, 8,929 personal pronouns, 2,987 reflex-ive pronouns, and 3,921 possessive pronouns.There are some rather long texts in the Tu?Bacorpus.
Which pair generation algorithm is rea-sonable?
Should we pair every markable (evenfrom the beginning of the text) with every othersucceeding markable?
This is linguistically im-plausible.
Pronouns are acting as a kind of localvariables.
A ?he?
at the beginning of a text anda second distant ?he?
at the end of the text hardlytend to corefer, except if there is a long chain ofcoreference ?renewals?
that lead somehow fromthe first ?he?
to the second ?he?.
But the plain ?he?-?he?
pair does not reliably indicate coreference.A smaller window seems to be appropriate.
Wehave experimented with various window sizes andfound that a size of 3 sentences worked best.Candidate pairs are generated only within that2Recently, a new version of the Tu?Ba was released with35,000 sentences with coreference annotations.443window, which is moved sentence-wise over thewhole text.4 Our Constraint-Based ModelThe output of the TiMBL classifier is the inputto the optimization step, it provides the set ofvariables and their weights.
In order to utilizeTiMBL?s classification results as weights in a min-imization task, we have defined a measure calledclassification costs (see Fig.
1).wi j =| negi j || negi j ?
posi j |Figure 1: Score for Classification Costs| negi j | (| posi j |) denotes the number of instancessimilar (according to TiMBL?s metric) to ?i, j?
thatare negative (positive) examples.
If no negative in-stances are found, a safe positive classification de-cision is proposed at zero cost.
Accordingly, thecost of a decision without any positive instances ishigh, namely one.
If both sets are non-empty, theratio of the negative instances to the total of all in-stances is taken.
For example, if TiMBL finds 10positive and 5 negative examples similar to the yetunclassified new example ?i, j?
the cost of a posi-tive classification is 5/15 while a negative classifi-cation costs 10/15.We introduce our model in an ILP style.
In sec-tion 6 we discuss our Balas adaptation which al-lows us to define constraints intensionally.The objective function is:min : ?
?i, j??
O0.5wi j ?
ci j +(1?wi j) ?
c ji (1)O0.5 is the set of pairs ?i, j?
that have receiveda weight ?
0.5 according to our weight function(see Fig.
1).
Any binary variable ci j combines theith markable (of the text) with the jth markable(i < j) within a fixed window3.c ji represents the (complementary) decision thati and j are not coreferent.
The weight of thisdecision is (1?wi j).
Please note that every op-timization model of coreference resolution mustinclude both variables4.
Otherwise optimization3As already discussed, the window is realized as part ofthe vector generation component, so O0.5 automatically onlycaptures pairs within the window.4Even if an anaphoricity classifier is used.would completely ignore the classification deci-sions of the pairwise classifier (i.e., that?
0.5 sug-gests coreference).
For example, the choice notto set ci j = 1 at costs wi j ?
0.5 must be sanc-tioned by instantiating its inverse variable c ji = 1and adding (1?
wi j) to the objective function?svalue.
Otherwise minimization would turn?inthe worst case?everything to be non-coreferent,while maximization would preferentially set ev-erything to be actually coreferent (as long as noconstraints are violated, of course).5The first constraint then is:ci j + c ji = 1, ?
?i, j?
?
O0.5 (2)A pair ?i, j?
is either coreferent or not.Transitivity is captured by (see (Finkel andManning, 2008) for an alternative but equivalentformalization):ci j + c jk ?
cik +1, ?i, j,k (i < j < k)cik + c jk ?
ci j +1, ?i, j,k (i < j < k)ci j + cik ?
c jk +1, ?i, j,k (i < j < k)(3)In order to take full advantage of ILP?s reason-ing capacities, three equations are needed giventhree markables.
The extensionalization of tran-sitivity thus produces n!3!(n?3)!
?
3 equations for nmarkables.
Note that transitivity?as a globalconstraint?ought to spread over the whole can-didate set, not just within in the window.Transitivity without further constraints is point-less.6 What we really can gain from transitivity isconsistency at the linguistic level, namely (glob-ally) adhering to exclusiveness constraints (cf.
theexample in the introduction).
We have defined twopredicates that replace the traditional c-command(which requires full syntactical analysis) and ap-proximate it: clause bound and np bound.Two mentions are clause-bound if they occur inthe same subclause, none of them being a reflex-ive or a possessive pronoun, and they do not forman apposition.
There are only 16 cases in our dataset where this predicate produces false negatives(e.g.
in clauses with predicative verbs: ?Hei is stillprime ministeri?).
We currently regard this short-coming as noise.5The need for optimization or other numerical preferencemechanisms originates from the fact that coreference reso-lution is underconstrained?due to the lack of a deeper textunderstanding.6Although it might lead to a reordering of coreference setsby better ?balancing the weights?.444Two markables that are clause-bound (in thesense defined above) are exclusive, i.e.ci j = 0, ?i, j (clause bound(i, j)).
(4)A possessive pronoun is exclusive to all markablesin the noun phrase it is contained in (e.g.
ci j = 0given a noun phrase ?
[heri manager j]?
), but mightget coindexed with markables outside of such a lo-cal context (?Annei talks to heri manager?).
Wedefine a predicate np bound that is true of twomarkables, if they occur in the same noun phrase.In general, two markables that np-bind each otherare exclusive:ci j = 0, ?i, j (np bound(i, j)) (5)5 Representing ILP ConstraintsIntensionallyExisting ILP-based approaches to NLP (e.g.
(Pun-yakanok et al, 2004; Althaus et al, 2004;Marciniak and Strube, 2005)) belong to theclass of Zero-One ILP: only binary variables areneeded.
This has been seldom remarked (butsee (Althaus et al, 2004)) and generic (out-of-the-box) ILP implementations are used.
More-over, these models form a very restricted variant ofZero-One ILP: the constraints come without anyweights.
The reason for this lies in the logical na-ture of NLP constraints.
For example in the case ofcoreference, we have the following types of con-straints:1. exclusivity of two instantiations (e.g.
eithercoreferent or not, equation 2)2. dependencies among three instantiations(transitivity: if two are coreferent then so thethird, equation 3)3. the prohibition of pair instantiation (bindingconstraints, equations 4 and 5)4. enforcement of at least one instantiation of amarkable in some pair (equation 6 below).We call the last type of constraints ?boundness en-forcement constraints?.
Only two classes of pro-nouns strictly belong to this class: relative (POSlabel ?PRELS?)
and possessive pronouns (POSlabel ?PPOSAT?)7.
The corresponding ILP con-straint is, e.g.
for possessive pronouns:?ici j ?
1, ?
j s.t.
pos( j) =?PPOSAT?
(6)7In rare cases, even reflexive pronouns are (correctly)used non-anaphorically, and, more surprisingly, 15% of thepersonal pronouns in the Tu?Ba are used non-anaphorically.Note that boundness enforcement constraints leadto exponential time in the worst case.
Given thatsuch a constraint holds on a pair with the highestcosts of all pairs (thus being the last element ofthe Balas ordered list with n elements): in order toprove whether it can be bound (set to one), 2n (bi-nary) variable flips need to be checked in the worstcase.
All other constraints can be satisfied by set-ting some ci j = 0 (i.e.
non-coreferent) which doesnot affect already taken or (any) yet to be takenassignments.
Although exponential in the worstcase, the integration of constraint (6) has sloweddown CPU time only slightly in our experiments.A closer look at these constraints reveals thatmost of them can be treated intensionally in anefficient manner.
This is a big advantage, sincenow transitivity can be captured even for long texts(which is infeasible for most generic ILP models).To intensionally capture transitivity, we onlyneed to explicitly maintain the evolving corefer-ence sets.
If a new markable is about to enter aset (e.g.
if it is related to another markable that isalready member of the set) it is verified that it iscompatible with all members of the set.A markable i is compatible with a coreferenceset if, for all members j of the set, ?i, j?
doesnot violate binding constraints, agrees morpholog-ically and semantically.
Morphological agreementdepends on the POS tags of a pair.
Two personalpronouns must agree in person, number and gen-der.
In German, a possessive pronoun must onlyagree in person with its antecedent.
Two nounsmight even have different grammatical gender, sono morphological agreement is checked here.Checking binding for the clause bound con-straint is simple: each markable has a subclause IDattached (extracted from the Tu?Ba).
If two mark-ables (except reflexive or possessive pronouns)share an ID they are exclusive.
Possessive pro-nouns must not be np-bound.
All members of thenoun phrase containing the possessive pronoun areexclusive to it.Note that such a representation of constraints isintensional since we need not enumerate all exclu-sive pairs as an ILP approach would have to.
Wesimply check (on demand) the identity of IDs.There is also no need to explicitly maintain con-straint (2), either, which states that a pair is eithercoreferent or not.
In the case that a pair cannot beset to 1 (representing coreference), it is set to 0;i.e.
ci j and c ji are represented by the same index445position p of a Balas solution v (cf.
Section 6); noextensional modelling is necessary.Although our special-purpose Balas adaptationno longer constitutes a general framework that canbe fed with each and every Zero-One ILP formal-ization around, the algorithm is simple enough tojustify this.
Even if one uses an ILP translator suchas Zimpl8, writing a program for a concrete ILPproblem quickly becomes comparably complex.6 A Variant of the Balas AlgorithmOur algorithm proceeds as follows: we generatethe first consistent solution according to the Balasalgorithm (Balas-First, henceforth).
The result is avector v of dimension n, where n is the size of O0.5.The dimensions take binary values: a value 1 atposition p represents the decision that the pth pairci j from the (Balas-ordered) objective function iscoreferent (0 indicates non-coreference).
One mi-nor difference to the original Balas algorithm isthat the primary choice of our algorithm is to set avariable to 1, not to 0?thus favoring coreference.However, in our case, 1 is the cheapest solution(with cost wi j ?
0.5).
Setting a variable to zerohas cost 1?wi j which is more expensive in anycase.
But aside from this assignment convention,the principal idea is preserved, namely that the as-signment is guided by lowest cost decisions.The search for less expensive solutions is donea bit differently from the original.
The Balas algo-rithm takes profit from weighted constraints.
Asdiscussed in Section 5, constraints in existing ILPmodels for NLP are unweighted.
Another differ-ence is that in the case of coreference resolutionboth decisions have costs: setting a variable to 1(wi j) and setting it to 0 (1?wi j).
This is the key toour cost function that guides the search.Let us first make some properties of the searchspace explicit.
First of all, given no constraintswere violated, the optimal solution would be theone with all pairs from O0.5 set to 1 (since any 0would add a suboptimal weight, namely 1?wi j).Now we can see that any less expensive solutionthan Balas-First must be longer than Balas-First,where the length (1-length, henceforth) of a Balassolution is defined as the number of dimensionswith value 1.
A shorter solution would turn at leasta single 1 into 0, which leads to a higher objectivefunction value.8http://zimpl.zib.de/Any solution with the same 1-length is more ex-pensive since it requires swapping a 1 to 0 at oneposition and a 0 to 1 at a farther position.
The per-mutation of 1/0s from Balas-First is induced bythe weights and the constraints.
A 0 at position qis forced by (a constraint together with) some (ormore) 1 at position p (p < q).
Thus, we can onlyswap a 0 to 1 if we swap at least one preceding 1to 0.
The costs of swapping a preceding 1 to 0 arehigher than the gain from swapping the 0 to 1 (asa consequence of the Balas ordering).
So no solu-tion with the same 1-length can be less expensivethan Balas-First.We then have to search for solutions with higher1-length.
In Section 7 we will argue that this actu-ally goes in the wrong direction.Any longer solution must swap?for every 1swapped to 0?at least two 0s to 1.
Otherwise thecosts are higher than the gain.
We can utilize thisfor a reduction of the search space.Let p be a position index of Balas-First (v),where the value of the dimension at p is 1 andthere exist at least two 0s with position indicesq > p.Consider v = ?1,0,1,1,0,0?.
Positions 1, 3and 4 are such positions (identifying the follow-ing parts of v resp.
: ?1,0,1,1,0,0?,?1,1,0,0?
and?1,0,0?
).We define a projection c(p) that returns theweight wi j of the pth pair ci j from the Balas or-dering.
v(p) is the value of dimension p in v (0 or1).
The cost of swapping 1 at position p to 0 is thedifference between the cost of c ji (1?
c(p)) andci j (c(p)): costs(p) = 1?2 ?
c(p).We define the potential gain pg(p) of swappinga 1 at position p to 0 and every succeeding 0 to 1by:pg(p) = costs(p)?
?q>p s.t.
v(q)=01?2 ?
c(q) (7)For example, let v = ?1,0,1,1,0,0?, p = 4,c(4) = 0.2 and (the two 0s) c(5) = 0.3, c(6) =0.35.
costs(4) = 1?0.4 = 0.6 and pg(4) = 0.6?
(0.4+0.3) =?0.1.
Even if all 0s (after position 4)can be swapped to 1, the objective function valueis lower that before, namely by 0.1.
Thus, we neednot consider this branch.In general, each time a 0 is turned into 1, thepotential gain is preserved, but if we have to turnanother 1 to 0 (due to a constraint), or if a 0 cannotbe swapped to 1, the potential gain is decremented446by a certain cost factor.
If the potential gain isexhausted that way, we can stop searching.7 Is Optimization Really Needed?Empirical EvidenceThe first observation we made when running ouralgorithm was that in more than 90% of all cases,Balas-First already constitutes the optimal solu-tion.
That is, the time-consuming search for a lessexpensive solution ended without further success.As discussed in Section 6, any less expensivesolution must be longer (1-length) than Balas-First.
But can longer solutions be better (in termsof F-measure scores) than shorter ones?
Theymight: if the 1-length re-assignment of variablesremoves as much false positives as possible andraises instead as much of the true positives as canbe found in O0.5.
Such a solution might have a bet-ter F-measure score.
But what about its objectivefunction value?
Is it less expensive than Balas-First?We have designed an experiment with all (true)coreferent pairs from O0.5 (as indicated by the goldstandard) set to 1.
Note that this is just anotherkind of constraints: the enforcement of corefer-ence (this time extensionally given).The result was surprising: The objective func-tion values that our algorithm finds under theseconstraints were in any case higher than Balas-First without that constraint.Fig.
2 illustrates this schematically (Fig.
4 be-low justifies the curve?s shape).
The curve rep-Figure 2: The best solution is ?less optimal?resents a function mapping objective values to F-measure scores.
Note that it is not monotoni-cally decreasing (from lower objective values tohigher ones)?as one would expect (less expensive= higher F-measure).
The vertical line labelled bidentifies Balas-First.
Starting with Balas-First,optimization searches to the left, i.e.
searchingfor smaller objective function values.
The hori-zontal line labelled m shows the local maximumof that search region (the arrow from left points toit).
But unfortunately, the global maximum (thearrow from right), i.e.
the 1-length solution withall (true) coreferent pairs set to 1, lies to the right-hand side of Balas-First.This indicates that, in our experimental con-ditions, optimization efforts can never reach theglobal maximum, but it also indicates that search-ing for less expensive solutions nevertheless mightlead (at least) to a local maximum.
However, ifit is true that the goal function is not monotonic,there is no guarantee that the optimal solution ac-tually constitutes the local maximum, i.e.
the bestsolution in terms of F-measure scores.Unfortunately, we cannot prove mathematicallyany hypotheses about the optimal values and theirbehavior.
However, we can compare the opti-mal value?s F-measure scores to the Balas-FirstF-measure scores empirically.
Two experimentswere designed to explore this.
In the first exper-iment, we computed for each text the differencebetween the F-measure value of the optimal so-lution and the F-measure value of Balas-First.
Itis positive if the optimal solution has higher F-measure score than Balas-First and negative oth-erwise.
This was done for each text (99) that hasmore than one objective function value (remem-ber that in more than 90% of texts Balas-First wasalready the optimal solution).Fig.
3 shows the results.
The horizontal line isFigure 3: Balas-First or Optimal Solutionseparating gain from loss.
Points above it indicatethat the optimal solution has a better F-measurescore, points below indicate a loss in percentage447(for readability, we have drawn a curve).
Takingthe mean of loss and gain across all texts, we foundthat the optimal solution shows no significant F-measure difference with the Balas-First solution:the optimal solution even slightly worsens the F-measure compared to Balas-First by ?0.086%.The second experiment was meant to explorethe curve shape of the goal function that mapsan objective function value to a F-measure value.This is shown in Fig.
4.
The values of that func-tion are empirically given, i.e.
they are producedby our algorithm.
The x-axis shows the mean ofthe nth objective function value better than Balas-First.
The y-value of the nth x-value thus marks theeffect (positive or negative) in F-measure scoreswhile proceeding to find the optimal solution.
Ascan be seen from the figure, the function (at leastempirically) is rather erratic.
In other words,searching for the optimal solution beyond Balas-First does not seem to lead reliably (and monoton-ically) to better F-measure values.Figure 4: 1st Compared to Balas-nth ValueIn the next section, we show that Balas-First asthe first optimization step actually is a significantimprovement over the classifier output.
So we arenot saying that we should dispense with optimiza-tion efforts completely.8 Does Balas-First help?
EmpiricalEvidenceBesides the empirical fact that Balas-First slightlyoutperforms the optimal solution, we must demon-strate that Balas-First actually improves the base-line.
Our experiments are based on a five-foldcross-validation setting (1100 texts from the Tu?Bacoreference corpus).
Each experiment was carriedout in two variants.
One where all markables havebeen taken as input?an application-oriented set-ting, and one where only markables that representtrue mentions have been taken (cf.
(Luo et al,2004; Ponzetto and Strube, 2006) for other ap-proaches with an evaluation based on true men-tions only).
The assumption is that if only truementions are considered, the effects of a modelcan be better measured.We have used the Entity-Constrained Measure(ECM), introduced in (Luo et al, 2004; Luo,2005).
As argued in (Klenner and Ailloud, 2008),it is more appropriate to evaluate the quality ofcoreference sets than the MUC score.9To obtain the baseline, we merged all pairs thatTiMBL classified as coreferent into coreferencesets.
Table 2 shows the results.all mentions true mentionsTimbl B-First Timbl B-FirstF 61.83 64.27 71.47 78.90P 66.52 72.05 73.81 84.10R 57.76 58.00 69.28 74.31Table 2: Balas-First (B-First) vs. BaselineIn the ?all mentions setting?, 2.4% F-measure im-provement was achieved, with ?true mentions?
it is7.43%.
These improvements clearly demonstratethat Balas-First is superior to the results based onthe classifier output.But is the specific order proposed by the Balasalgorithm itself useful?
Since we have dispensedwith ?full optimization?, why not dispense with theBalas ordering as well?
Since the ordering of thepairs does not affect the rest of our algorithm wehave been able to compare the Balas order to themore natural linear order.
Note that all constraintsare applied in the linear variant as well, so the onlydifference is the ordering.
Linear ordering overpairs is established by sorting according to the in-dex of the first pair element (the i from ci j).all mentions true mentionslinear B-First linear B-FirstF 62.83 64.27 76.08 78.90P 70.39 72.05 81.40 84.10R 56.73 58.00 71.41 74.31Table 3: Balas Order vs.
Linear OrderOur experiments (cf.
Table 3) indicate that the9Various authors have remarked on the shortcomings ofthe MUC evaluation scheme (Bagga and Baldwin, 1998; Luo,2005; Nicolae and Nicolae, 2006).448Balas ordering does affect the empirical results.The F-measure improvement is 1.44% (?all men-tions?)
and 2.82% (?true mentions?
).The search for Balas-First remains, in general,NP-complete.
However, constraint models with-out boundness enforcement constraints (cf.
Sec-tion 5) pose no computational burden, they can besolved in quadratic time.
In the presence of bound-ness enforcement constraints, exponential time isrequired in the worst case.
In our experiments,boundness enforcement constraints have proved tobe unproblematic.
Most of the time, the classi-fier has assigned low costs to candidate pairs con-taining a relative or a possessive pronoun, whichmeans that they get instantiated rather soon (al-though this is not guaranteed).9 Related WorkThe focus of our paper lies on the evaluation of thebenefits optimization could have for coreferenceresolution.
Accordingly, we restrict our discus-sion to methodologically related approaches (i.e.ILP approaches).
Readers interested in other workon anaphora resolution for German on the basisof the Tu?Ba coreference corpus should consider(Hinrichs et al, 2005) (pronominal anaphora) and(Versley, 2006) (nominal anaphora).Common to all ILP approaches (incl.
ours)is that they apply ILP on the output of pairwisemachine-learning.
Denis and Baldridge (2007;2008) have an ILP model to jointly determineanaphoricity and coreference, but take neithertransitivity nor exclusivity into account.
So nocomplexity problems arise in their approach.
Themodel from (Finkel and Manning, 2008) utilizestransitivity, but not exclusivity.
The benefits oftransitivity are thus restricted to an optimal bal-ancing of the weights (e.g.
given two positivelyclassified pairs, the transitively given third pairin some cases is negative, ILP globally resolvesthese cases to the optimal solution).
The authorsdo not mention complexity problems with exten-sionalizing transitivity.
Klenner (2007) utilizesboth transitivity and exclusivity.
To overcome theoverhead of transitivity extensionalization, he pro-poses a fixed transitivity window.
This, however,is bound to produce transitivity gaps, so the bene-fits of complete transitivity propagation are lost.Another attempt to overcome the problem ofcomplexity with ILP models is described in(Riedel and Clarke, 2006) (dependency parsing).Here an incremental?or better, cascaded?ILPmodel is proposed, where at each cascade onlythose constraints are added that have been vio-lated in the preceding one.
The search stops withthe first consistent solution (as we suggest in thepresent paper).
However, it is difficult to quantifythe number of cascades needed to come to it andmoreover, the full ILP machinery is being used (soagain, constraints need to be extensionalized).To the best of our knowledge, our work is thefirst that studies the proper utility of ILP optimiza-tion for NLP, while offering an intensional alter-native to ILP constraints.10 Conclusion and Future WorkIn this paper, we have argued that ILP for NLPreduces to Zero-One ILP with unweighted con-straints.
We have proposed such a Zero-One ILPmodel that combines exclusivity, transitivity andboundness enforcement constraints in an inten-sional model driven by best-first inference.We furthermore claim and empirically demon-strate for the domain of coreference resolution thatNLP approaches can take advantage from that newperspective.
The pitfall of ILP, namely the needto extensionalize each and every constraint, canbe avoided.
The solution is an easy to carry outreimplementation of a Zero-One algorithm suchas Balas?, where (most) constraints can be treatedintensionally.
Moreover, we have found empiri-cal evidence that ?full optimization?
is not needed.The first found consistent solution is as good as theoptimal one.
Depending on the constraint modelthis can reduce the costs from exponential time topolynomial time.Optimization efforts, however, are not superflu-ous, as we have showed.
The first consistent so-lution found with our Balas reimplementation im-proves the baseline significantly.
Also, the Balasordering itself has proven superior over other or-ders, e.g.
linear order.In the future, we will experiment with morecomplex constraint models in the area of corefer-ence resolution.
But we will also consider otherdomains in order to find out whether our resultsactually are widely applicable.Acknowledgement The work described hereinis partly funded by the Swiss National ScienceFoundation (grant 105211-118108).
We wouldlike to thank the anonymous reviewers for theirhelpful comments.449ReferencesE.
Althaus, N. Karamanis, and A. Koller.
2004.
Com-puting locally coherent discourses.
In Proc.
of theACL.A.
Bagga and B. Baldwin.
1998.
Algorithms for scor-ing coreference chains.
In Proceedings of the Lin-guistic Coreference Workshop at The First Interna-tional Conference on Language Resources and Eval-uation (LREC98), pages 563?566.E.
Balas.
1965.
An additive algorithm for solving lin-ear programs with zero-one variables.
OperationsResearch, 13(4):517?546.J.W.
Chinneck.
2004.
Practical optimization:a gentle introduction.
Electronic document:http://www.sce.carleton.ca/faculty/chinneck/po.html.W.
Daelemans, J. Zavrel, K. van der Sloot, andA.
van den Bosch.
2004.
TiMBL: Tilburg Memory-Based Learner.P.
Denis and J. Baldridge.
2007.
Joint determinationof anaphoricity and coreference resolution using in-teger programming.
Proceedings of NAACL HLT,pages 236?243.P.
Denis and J. Baldridge.
2008.
Specialized modelsand ranking for coreference resolution.
In Proceed-ings of the Empirical Methods in Natural LanguageProcessing (EMNLP 2008), Hawaii, USA.
To ap-pear.J.R.
Finkel and C.D.
Manning.
2008.
Enforcing tran-sitivity in coreference resolution.
Association forComputational Linguistics.E.
Hinrichs, K. Filippova, and H. Wunsch.
2005.
Adata-driven approach to pronominal anaphora reso-lution in German.
In Proc.
of RANLP ?05.M.
Klenner and E?.
Ailloud.
2008.
Enhancing coref-erence clustering.
In C. Johansson, editor, Proc.
ofthe Second Workshop on Anaphora Resolution (WARII), volume 2 of NEALT Proceedings Series, pages31?40, Bergen, Norway.M.
Klenner.
2007.
Enforcing consistency on corefer-ence sets.
In Recent Advances in Natural LanguageProcessing (RANLP), pages 323?328, September.X.
Luo, A. Ittycheriah, H. Jing, N. Kambhatla, andS.
Roukos.
2004.
A mention-synchronous coref-erence resolution algorithm based on the Bell tree.Proceedings of the 42nd Annual Meeting on Associ-ation for Computational Linguistics.X.
Luo.
2005.
On coreference resolution perfor-mance metrics.
In Proceedings of the conference onHuman Language Technology and Empirical Meth-ods in Natural Language Processing, pages 25?32.Association for Computational Linguistics Morris-town, NJ, USA.T.
Marciniak and M. Strube.
2005.
Beyond thepipeline: Discrete optimization in NLP.
In Proc.
ofthe CoNLL.K.
Naumann.
2006.
Manual for the annotation ofindocument referential relations.
Electronic doc-ument: http://www.sfs.uni-tuebingen.de/de_tuebadz.shtml.V.
Ng and C. Cardie.
2002.
Improving machine learn-ing approaches to coreference resolution.
In Proc.of the ACL.C.
Nicolae and G. Nicolae.
2006.
Best Cut: A graphalgorithm for coreference resolution.
In Proceed-ings of the 2006 Conference on Empirical Methodsin Natural Language Processing, pages 275?283.Association for Computational Linguistics.C.H.
Papadimitriou and K. Steiglitz.
1998.
Combi-natorial Optimization: Algorithms and Complexity.Dover Publications.S.P.
Ponzetto and M. Strube.
2006.
Exploiting seman-tic role labeling, WordNet and Wikipedia for coref-erence resolution.
In Proc.
of HLT-NAACL, vol-ume 6, pages 192?199.V.
Punyakanok, D. Roth, W. Yih, and D. Zimak.
2004.Semantic role labeling via integer linear program-ming inference.
In Proc.
of the COLING.S.
Riedel and J. Clarke.
2006.
Incremental integerlinear programming for non-projective dependencyparsing.
In Proc.
of the EMNLP.W.M.
Soon, H.T.
Ng, and D.C.Y.
Lim.
2001.
Amachine learning approach to coreference resolu-tion of noun phrases.
Computational Linguistics,27(4):521?544.H.
Telljohann, E.W.
Hinrichs, S. Ku?bler, and H. Zins-meister.
2005.
Stylebook for the Tu?bingentreebank of written German (Tu?Ba-D/Z).
Semi-nar fur Sprachwissenschaft, Universita?t Tu?bingen,Tu?bingen, Germany.Y.
Versley.
2006.
A constraint-based approach to nounphrase coreference resolution in German newspa-per text.
In Konferenz zur Verarbeitung Natu?rlicherSprache (KONVENS).450
