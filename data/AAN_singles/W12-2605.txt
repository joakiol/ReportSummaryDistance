Proceedings of the Workshop on Evaluation Metrics and System Comparison for Automatic Summarization, pages 36?43,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsThe Heterogeneity Principle in Evaluation Measures for AutomaticSummarization?Enrique Amigo?
Julio Gonzalo Felisa VerdejoUNED, Madrid{enrique,julio,felisa}@lsi.uned.esAbstractThe development of summarization systemsrequires reliable similarity (evaluation) mea-sures that compare system outputs with hu-man references.
A reliable measure shouldhave correspondence with human judgements.However, the reliability of measures dependson the test collection in which the measureis meta-evaluated; for this reason, it has notyet been possible to reliably establish whichare the best evaluation measures for automaticsummarization.
In this paper, we proposean unsupervised method called Heterogeneity-Based Ranking (HBR) that combines summa-rization evaluation measures without requiringhuman assessments.
Our empirical results in-dicate that HBR achieves a similar correspon-dence with human assessments than the bestsingle measure for every observed corpus.
Inaddition, HBR results are more robust acrosstopics than single measures.1 IntroductionIn general, automatic evaluation metrics for summa-rization are similarity measures that compare systemoutputs with human references.
The typical develop-ment cycle of a summarization system begins withselecting the most predictive metric.
For this, evalu-ation metrics are compared to each other in terms?This work has been partially funded by the Madrid gov-ernment, grant MA2VICMR (S-2009/TIC- 1542), the SpanishGovernment, grant Holopedia (TIN2010-21128-C02-01) andthe European Community?s Seventh Framework Programme(FP7/ 2007-2013) under grant agreement nr.
288024 (LiMo-SINe project).of correlation with human judgements.
The sec-ond step consists of tuning the summarization sys-tem (typically in several iterations) in order to maxi-mize the scores according to the selected evaluationmeasure.There is a wide set of available measures beyondthe standard ROUGE: for instance, those comparingbasic linguistic elements (Hovy et al, 2005), depen-dency triples (Owczarzak, 2009) or convolution ker-nels (Hirao et al, 2005) which reported some relia-bility improvement with respect to ROUGE in termsof correlation with human judgements.
However,in practice ROUGE is still the preferred metric ofchoice.
The main reason is that the superiority of ameasure with respect to other is not easy to demon-strate: the variability of results across corpora, ref-erence judgements (Pyramid vs responsiveness) andcorrelation criteria (system vs. summary level) issubstantial.
In the absence of a clear quality crite-rion, the de-facto standard is usually the most rea-sonable choice.In this paper we rethink the development cy-cle of summarization systems.
Given that the bestmeasure changes across evaluation scenarios, wepropose using multiple automatic evaluation mea-sures, together with an unsupervised method to com-bine measures called Heterogeneity Based Rank-ing (HBR).
This method is grounded on the gen-eral Heterogeneity property proposed in (Amigo?
etal., 2011), which states that the more a measureset is heterogeneous, the more a score increase ac-cording to all the measures simultaneously is reli-able.
In brief, the HBR method consists of com-puting the heterogeneity of measures for which a36system-produced summary improves each of the restof summaries in comparison.Our empirical results indicate that HBR achievesa similar correspondence with human assessmentsthan the best single measure for every observed cor-pus.
In addition, HBR results are more robust acrosstopics than single measures.2 DefinitionsWe consider here the definition of similarity mea-sure proposed in (Amigo?
et al, 2011):Being ?
the universe of system outputs (sum-maries) s and gold-standards (human references) g,we assume that a similarity measure is a functionx : ?2 ??
< such that there exists a decompo-sition function f : ?
??
{e1..en} (e.g., wordsor other linguistic units or relationships) satisfyingthe following constraints; (i) maximum similarity isachieved only when the summary decomposition re-sembles exactly the gold standard; (ii) adding oneelement from the gold standard increases the simi-larity; and (iii) removing one element that does notappear in the gold standard also increases the simi-larity.
Formally:f(s) = f(g)??
x(s, g) = 1(f(s) = f(s?)
?
{eg ?
f(g) \ f(s)}) =?x(s, g) > x(s?, g)(f(s) = f(s?)?
{e?g ?
f(s) \ f(g)}) =?x(s, g) > x(s?, g)This definition excludes random functions, or theinverse of any similarity function (e.g.
1f(s) ).
Itcovers, however, any overlapping or precision/recallmeasure over words, n-grams, syntactic structures orany kind of semantic unit.
In the rest of the paper,given that the gold standard g in summary evaluationis usually fixed, we will simplify the notation sayingthat x(s, g) ?
x(s).We consider also the definition of heterogeneityof a measure set proposed in (Amigo?
et al, 2011):The heterogeneity H(X ) of a set of measures X isdefined as, given two summaries s and s?
such thatg 6= s 6= s?
6= g (g is the reference text), the proba-bility that there exists two measures that contradicteach other.H(X ) ?Ps,s?
6=g(?x, x?
?
X/x(s) > x(s?)
?
x?
(s) < x?(s?
))3 ProposalThe proposal in this paper is grounded on the hetero-geneity property of evaluation measures introducedin (Amigo?
et al, 2011).
This property establishesa relationship between heterogeneity and reliabilityof measures.
However, this work does not provideany method to evaluate and rank summaries given aset of available automatic evaluation measures.
Wenow reformulate the heterogeneity property in orderto define a method to combine measures and ranksystems.3.1 Heterogeneity Property ReformulationThe heterogeneity property of evaluation measuresintroduced in (Amigo?
et al, 2011) states that, as-suming that measures are based on similarity to hu-man references, the real quality difference betweentwo texts is lower bounded by the heterogeneity ofmeasures that corroborate the quality increase.
Wereformulate this property in the following way:Given a set of automatic evaluation measuresbased on similarity to human references, the prob-ability of a quality increase in summaries is corre-lated with the heterogeneity of the set of measuresthat corroborate this increase:P (Q(s) ?
Q(s?))
?
H({x|x(s) ?
x(s?
)})where Q(s) is the quality of the summary s accord-ing to human assessments.
In addition, the proba-bility is maximal if the heterogeneity is maximal:H({x|x(s) ?
x(s?)})
= 1?
P (Q(s) ?
Q(s?))
= 1The first part is derived from the fact thatincreasing heterogeneity requires additional di-verse measures corroborating the similarity increase(H({x|x(s) ?
x(s?)}))).
The correlation is the re-sult of assuming that a similarity increase accord-ing to any aspect is always a positive evidence oftrue similarity to human references.
In other words,37a positive match between the automatic summaryand the human references, according to any feature,should never be a negative evidence of quality.As for the second part, if the heterogeneity of ameasure set X is maximal, then the condition ofthe heterogeneity definition (?x, x?
?
X .x(s) >x(s?)
?
x?
(s) < x?(s?))
holds for any pair of sum-maries that are different from the human references.Given that all measures in X corroborate the simi-larity increase (X = {x|x(s) ?
x(s?
)}), the hetero-geneity condition does not hold.
Then, at least oneof the evaluated summaries is not different from thehuman reference and we can ensure that P (Q(s) ?Q(s?))
= 1.3.2 The Heterogeneity Based RankingThe main goal in summarization evaluation is rank-ing systems according to their quality.
This can beseen as estimating, for each system-produced sum-mary s, the average probability of being ?better?than other summaries:Rank(s) = Avgs?
(P (Q(s) ?
Q(s?
)))Applying the reformulated heterogeneity propertywe can estimate this as:HBRX (s) = Avgs?
(H({x|x(s) ?
x(s?
)}))We refer to this ranking function as the Heterogene-ity Based Ranking (HBR).
It satisfies three crucialproperties for a measure combining function.
Notethat, assuming that any similarity measure over hu-man references represents a positive evidence ofquality, the measure combining function must beat least robust with respect to redundant or randommeasures:1.
HBR is independent from measure scales andit does not require relative weighting schemesbetween measures.
Formally, being f any strictgrowing function:HBRx1..xn(s) = HBRx1..f(xn)(s)2.
HBR is not sensitive to redundant measures:HBRx1..xn(s) = HBRx1..xn,xn(s)3.
Given a large enough set of similarityinstances, HBR is not sensitive to non-informative measures.
In other words, beingxr a random function such that P (xr(s) >xr(s?))
= 12 , then:HBRx1..xn(s) ?
HBRx1..xn,xr(s)The first two properties are trivially satisfied: the?
operator in H and the score comparisons are not af-fected by redundant measures nor their scale proper-ties.
Regarding the third property, the Heterogeneityof a set of measures plus a random function xr is:H(X ?
{xr}) ?Ps,s?
(?x, x?
?
X?
{xr}|x(s) > x(s?)?x?
(s) < x?(s?))
=H(X ) + (1?H(X )) ?12=H(X ) + 12That is, the Heterogeneity grows proportionallywhen including a random function.
Assuming thatthe random function corroborates the similarity in-crease in a half of cases, the result is a proportionalrelationship between HBR and HBR with the addi-tional measure.
Note that we need to assume a largeenough amount of data to avoid random effects.4 Experimental Setting4.1 Test BedWe have used the AS test collections used in theDUC 2005 and DUC 2006 evaluation campaigns1(Dang, 2005; Dang, 2006).
The task was to gener-ate a question focused summary of 250 words from aset of 25-50 documents to a complex question.
Sum-maries were evaluated according to several criteria.Here, we will consider the responsiveness judge-ments, in which the quality score was an integer be-tween 1 and 5.
See Table 1 for a brief numericaldescription of these test beds.In order to check the measure combining method,we have employed standard variants of ROUGE(Lin, 2004), including the reversed precision versionfor each variant 2.
We have considered also the F1http://duc.nist.gov/2Note that the original ROUGE measures are oriented to re-call38DUC 2005 DUC 2006#human-references 3-4 3-4#systems 32 35#system-outputs-assessed 32 35#system-outputs 50 50#outputs-assessed per-system 50 50Table 1: Test collections from 2005 and 2006 DUC evaluation campaigns used in our experiments.measure between recall and precision oriented mea-sures.
Finally, our measure set includes also BE orBasic Elements (Hovy et al, 2006).4.2 Meta-evaluation criterionThe traditional way of meta-evaluating measuresconsists of computing the Pearson correlation be-tween measure scores and quality human assess-ments.
But the main goal of automatic evaluationmetrics is not exactly to predict the real quality ofsystems; rather than this, their core mission is de-tecting system outputs that improve the baseline sys-tem in each development cycle.
Therefore, the issueis to what extent a quality increase between two sys-tem outputs is reflected by the output ranking pro-duced by the measure.According to this perspective, we propose meta-evaluating measures in terms of an extended versionof AUC (Area Under the Curve).
AUC can be seenas the probability of observing a score increase whenobserving a real quality increase between two sys-tem outputs (Fawcett, 2006).AUC(x) = P (x(s) > x(s?
)|Q(s) > Q(s?
))In order to customize this measure to our scenario,two special cases must be handled:(i) For cases in which both summaries obtain thesame value, we assume that the measure rewardseach instance with equal probability.
That is, ifx(s) = x(s?
),P (x(s) > x(s?
)|Q(s) > Q(s?))
= 12 .
(ii) Given that in the AS evaluation scenarios thereare multiple quality levels, we still apply the sameprobabilistic AUC definition, considering pairs ofsummaries in which one of them achieves morequality than the other according to human assessors.Figure 1: Correlation between probability of quality in-crease and Heterogeneity of measures that corroboratethe increase5 Experiments5.1 Measure Heterogeneity vs. QualityIncreaseWe hypothesize that the probability of a real similar-ity increase to human references (as stated by humanassessments) is directly related to the heterogeneityof the set of measures that confirm such increase.
Inorder to verify whether this principle holds in prac-tice, we need to measure the correlation betweenboth variables.
Therefore, we compute, for each pairof summaries in the same topic the heterogeneity ofthe set of measures that corroborate a score increasebetween both:H({x ?
X |x(s) ?
x(s?
)})The Heterogeneity has been estimated by countingcases over 10,000 samples (pairs of summaries) inboth corpora.Then, we have sorted each pair ?s, s??
accordingto its related heterogeneity.
We have divided the re-sulting rank in 100 intervals of the same size.
For39Figure 2: AUC comparison between HBR and single measures in DUC 2005 and DUC 2006 corpora.each interval, we have computed the average hetero-geneity of the set and the probability of real qualityincrease (P (Q(s) ?
Q(s?
))).Figure 1 displays the results.
Note that the directrelation between both variables is clear: a key forpredicting a real quality increase is how heteroge-neous is the set of measures corroborating it.5.2 HBR vs.
Single MeasuresIn the following experiment, we compute HBR andwe compare the resulting AUC with that of singlemeasures.
The heterogeneity of measures is esti-mated over samples in both corpora (DUC 2005 andDUC 2006), and HBR ranking is computed to ranksummaries for each topic.
For the meta-evaluation,the AUC probability is computed over summarypairs from the same topic.Figure 2 shows the resulting AUC values of sin-gle measures and HBR.
The black bar represents theHBR approach.
The light grey bars are ROUGEmeasures oriented to precision.
The dark grey barsinclude ROUGE variants oriented to recall and F,and the measure BE.
As the Figure shows, recall-based measures achieve in general higher AUC val-ues than precision-oriented measures.
The HBRmeasure combination appears near the top.
It is im-proved by some measures such as ROUGE SU4 R,although the difference is not statistically significant(p = 0.36 for a t-test between ROUGE SU4 R andHBR, for instance).
HBR improves the 10 worstsingle measures with statistical significance (p <0.025).5.3 RobustnessThe next question is why using HBR instead of the?best?
measure (ROUGE-SU4-R in this case).
Aswe mentioned, the reliability of measures can varyacross scenarios.
For instance, in DUC scenariosmost systems are extractive, and exploit the maxi-mum size allowed in the evaluation campaign guide-lines.
Therefore, the precision over long n-grams isnot crucial, given that the grammaticality of sum-maries is ensured.
In this scenario the recall overwords or short n-grams over human references is aclear signal of quality.
But we can not ensure thatthese characteristics will be kept in other corpora, oreven when evaluating new kind of summarizers withthe same corpora.Our hypothesis is that, given that HBR resemblesthe best measure without using human assessments,it should have a more stable performance in situa-tions where the best measure changes.In order to check empirically this assertion, wehave investigated the lower bound performance ofmeasures in our test collections.
First, we haveranked measures for each topic according to theirAUC values; Then, we have computed, for everymeasure, its rank regarding the rest of measures(scaled from 0 to 1).
Finally, we average each mea-sure across the 10% of topics in which the measure40Figure 3: Average rank of measures over the 10% of topics with lowest results for the measure.gets the worst ranks.
Figure 3 shows the results: theworst performance of HBR across topics is betterthan the worst performance of any single measure.This confirms that the combination of measures us-ing HBR is indeed more robust than any measure inisolation.5.4 Consistent vs.
Inconsistent TopicsThe Heterogeneity property is grounded on the as-sumption that any similarity criteria represents apositive evidence of similarity to human references.In general, we can assert that this assumption holdsover a large enough random set of texts.
However,depending on the distribution of summaries in thecorpus, this assumption may not always hold.
Forinstance, we can assume that, given all possible sum-maries, improving the word precision with respect tothe gold standard can never be a negative evidenceof quality.
However, for a certain topic, it could hap-pen that the worst summaries are also the shortest,and have high precision and low recall.
In this case,precision-based similarity could be correlated withnegative quality.
Let us refer to these as inconsis-tent topics vs. consistent topics.
In terms of AUC,a measure represents a negative evidence of qualitywhen AUC is lower than 0.5.
Our test collectionscontain 100 topics, out of which 25 are inconsis-tent (i.e., at least one measure achieves AUC valueslower than 0.5) and 75 are consistent with respect toour measure set (all measures achieve AUC valueshigher than 0.5).Figure ??
illustrates the AUC achieved by mea-sures when inconsistent topics are excluded.
As withthe full set of topics, recall-based measures achievehigher AUC values than precision-based measures;but, in this case, HBR appears at the top of the rank-ing.
This result illustrates that (i) HBR behaves par-ticularly well when our assumptions on similaritymeasures hold in the corpus; and that (ii) in prac-tice, there may be topics for which our assumptionsdo not hold.6 ConclusionsIn this paper, we have confirmed that the heterogene-ity of a set of summary evaluation measures is cor-related with the probability of finding a real qualityimprovement when all measures corroborate it.
TheHBR measure combination method is based on thisprinciple, which is grounded on the assumption thatany similarity increase with respect to human refer-ences is a positive signal of quality.Our empirical results indicate that the Hetero-geneity Based Ranking achieves a reliability simi-lar to the best single measure in the set.
In addi-41Figure 4: AUC comparison between HBR and single measures in corpora DUC2005 and DUC 2006 over topics inwhich all measures achieve AUC bigger than 0.5.tion, HBR results are more robust across topics thansingle measures.
Our experiments also suggest thatHBR behaves particularly well when the assump-tions of the heterogeneity property holds in the cor-pus.
These assumptions are conditioned by the dis-tribution of summaries in the corpus (in particular,on the amount and variability of the summaries thatare compared with human references), and in prac-tice 25% of the topics in our test collections do notsatisfy them for our set of measures.The HBR (Heterogeneity Based Ranking) methodproposed in this paper does not represent the ?bestautomatic evaluation measure?.
Rather than this, itpromotes the development of new measures.
WhatHBR does is solving ?or at least palliating?
the prob-lem of reliability variance of measures across testbeds.
According to our analysis, our practical rec-ommendations for system refinement are:1.
Compile an heterogenous set of measures, cov-ering multiple linguistic aspects (such as n-gram precision, recall, basic linguistic struc-tures, etc.).2.
Considering the summarization scenario, dis-card measures that might not always representa positive evidence of quality.
For instance,if very short summaries are allowed (e.g.
oneword) and they are very frequent in the set ofsystem outputs to be compared to each other,precision oriented measures may violate HBRassumptions.3.
Evaluate automatically your new summariza-tion approach within this corpus according tothe HBR method.Our priority for future work is now developinga reference benchmark containing an heterogenousset of summaries, human references and measuressatisfying the heterogeneity assumptions and cover-ing multiple summarization scenarios where differ-ent measures play different roles.The HBR software is available athttp://nlp.uned.es/?enrique/ReferencesEnrique Amigo?, Julio Gonzalo, Jesus Gimenez, and Fe-lisa Verdejo.
2011.
Corroborating text evaluation re-sults with heterogeneous measures.
In Proceedings ofthe 2011 Conference on Empirical Methods in Natu-ral Language Processing, pages 455?466, Edinburgh,Scotland, UK., July.
Association for ComputationalLinguistics.Hoa Trang Dang.
2005.
Overview of DUC 2005.
In Pro-ceedings of the 2005 Document Understanding Work-shop.Hoa Trang Dang.
2006.
Overview of DUC 2006.
In Pro-ceedings of the 2006 Document Understanding Work-shop.42Tom Fawcett.
2006.
An introduction to roc analysis.Pattern Recogn.
Lett., 27:861?874, June.Tsutomu Hirao, Manabu Okumura, and Hideki Isozaki.2005.
Kernel-based approach for automatic evaluationof natural language generation technologies: Applica-tion to automatic summarization.
In Proceedings ofHuman Language Technology Conference and Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 145?152, Vancouver, British Columbia,Canada, October.
Association for Computational Lin-guistics.Eduard Hovy, Chin-Yew Lin, and Liang Zhou.
2005.Evaluating DUC 2005 using Basic Elements.
Proceed-ings of Document Understanding Conference (DUC).Vancouver, B.C., Canada.Eduard Hovy, Chin-Yew Lin, Liang Zhou, and JunichiFukumoto.
2006.
Automated Summarization Evalu-ation with Basic Elements.
In Proceedings of the 5thInternational Conference on Language Resources andEvaluation (LREC), pages 899?902.Chin-Yew Lin.
2004.
Rouge: A Package for Auto-matic Evaluation of Summaries.
In Marie-FrancineMoens and Stan Szpakowicz, editors, Text Summariza-tion Branches Out: Proceedings of the ACL-04 Work-shop, pages 74?81, Barcelona, Spain, July.
Associa-tion for Computational Linguistics.Karolina Owczarzak.
2009.
Depeval(summ):dependency-based evaluation for automatic sum-maries.
In ACL-IJCNLP ?09: Proceedings of the JointConference of the 47th Annual Meeting of the ACLand the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP: Volume 1, pages190?198, Morristown, NJ, USA.
Association for Com-putational Linguistics.43
