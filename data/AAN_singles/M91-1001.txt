OVERVIEW OF THE THIRD MESSAGE UNDERSTANDINGEVALUATION AND CONFERENCEBeth M. SundheimNaval Ocean Systems CenterCode 444Decision Support and AI Technology Branc hSan Diego, CA 92152-5000sundheim@nosc .mi lINTRODUCTIONThe Naval Ocean Systems Center (NOSC) has conducted the third in a series o fevaluations of English text analysis systems .
These evaluations are intended toadvance our understanding of the merits of current text analysis techniques, a sapplied to the performance of a realistic information extraction task.
The latestone is also intended to provide insight into information retrieval technolog y(document retrieval and categorization) used instead of or in concert wit hlanguage understanding technology .The inputs to the analysis/extraction proces sconsist of naturally-occurring texts that were obtained in the form of electroni cmessages .The outputs of the process are a set of templates or semantic frame sresembling the contents of a partially formatted database .The premise on which these evaluations are based is that task-oriented test senable straightforward comparisons among systems and provide usefu lquantitative data on the state of the art in text understanding .
The tests aredesigned to treat the systems under evaluation as black boxes and to point u psystem performance on discrete aspects of the task as well as on the task overall .These quantitative data can be interpreted in light of information known abouteach system's text analysis techniques in order to yield qualitative insights into th erelative validity of those techniques as applied to the general problem ofinformation extraction .The process of conducting these evaluations has presented great opportunitie sfor examining and improving on the evaluation methodology itself.
Although stil lfar from perfect, the MUC-3 evaluation was markedly better than the previous one ,especially with respect to the way scoring was done and the degree to which thetest set was representative of the training set .
Much of the credit for improvemen tgoes to the evaluation participants themselves, who have been actively involved i nnearly every aspect of the evaluation .
The previous MUC, known as MUCK-II (thenaming convention has since been stripped down), proved that systems existe dthat could do a reasonable job of extracting data from ill-formed paragraph-lengt htexts in a narrow domain (naval messages about encounters with hostile forces )and that measuring performance on such a task was a feasible and viable thing t odo .However, the usage of a very small test set (just 5 texts) and an extremel yunsophisticated scoring procedure combined to make it inadvisable to publicize th e3results .
(Results obtained in experiments conducted on one MUCK-II system afte rthe evaluation was completed are discussed in [1] .
)The MUC-3 evaluation was significantly broader in scope than previous ones inmost respects, including text characteristics, task specifications, performancemeasures, and range of text understanding and information extraction techniques .MUC-3 presented a significantly more challenging task than MUCK-II, which washeld in June of 1989 .The results show that MUC-3 was not an unreasonabl echallenge to 1991 technologies .
The means used to measure performance haveevolved far enough that we no longer hesitate to present the system scores, andwork on the evaluation methodology is planned that will take the next step t odetermine the statistical significance of the results .In another effort to determine their significance, some work has already bee nundertaken by Hirschman [2] to measure the difference in complexity of MUC-lik eevaluation tasks so that the results can be used to quantify progress in the field oftext understanding .
This objective, however, brings up another critical area o fimprovement for future evaluations, namely refining the evaluation methodolog yin such a way as to better isolate the systems' text analysis capabilities from thei rdata extraction capabilities.
This will be done, since the MUC- 3 corpus and task ar esufficiently challenging that .they can be used again (with a new test set) in afuture evaluation .That evaluation will seek to examine more closely the tex tanalysis capabilities of the systems, to measure improvements in performance b yMUC-3 systems, and to establish performance baselines for any new systems .This paper covers most of the basics of the MUC-3 evaluation, which wer epresented during a tutorial session and in an overview presentation at the start o fthe regular sessions .
This paper is also an overview of the conferenc eproceedings, which includes papers contributed by the sites that participated i nthe evaluation and by individuals who were involved in the evaluation in othe rways.
Parts I, II, and III of the proceedings are organized in the order in whic hthe sessions were held, but the ordering of papers within Parts II and III i salphabetical by site and does not necessarily correspond with the order in whic hthe presentations were made during the conference.The proceedings als oincludes a number of appendices containing materials pertinent to the evaluation .OVERVIEW OF MUC- 3The planning for MUC-3 began while MUCK-II was still in progress, wit hsuggestions from MUCK-II participants for improvements .
A MUC-3 programcommittee was formed from among those MUCK-II participants who provide dsignificant feedback on the MUCK-II effort.
The MUC-3 program committeeincluded Laura Blumer Balcom (Advanced Decision Systems), Ralph Grishman (Ne wYork University), Jerry Hobbs (SRI International), Lisa Rau (General Electric), an dCarl Weir (Unisys Center for Advanced Information Technology) .
Since one of thesuggestions for MUC-3 was to add an element of document filtering to the task o fdata extraction, David Lewis (then at the University of Massachusetts and now atthe University of Chicago) was invited to join the committee as a representative o fthe information retrieval community .NOSC began looking for a suitable corpus - in late 1989 and obtained assistanc efrom other government agencies to acquire it during the summer of 1990 .
At that4time, a call for participation was sent to academic, industrial, and commercia lorganizations in the United States that were known to be engaged in system designor development in the area of text analysis or information retrieval .
Participationon the part of many of the respondents was contingent upon receiving outsid efinancial support ; approximately two-thirds of the sites were awarded financia lsupport by the Defense Advanced Research Projects Agency (DARPA) .
Theseawards were modest, some sites having requested funds only to pay travel expensesand others having requested funds to cover up to half of the total cost ofparticipating.
The total cost was typically estimated to be approximately equivalen tto one person-year of effort .The evaluation was officially launched in October, 1990, with a three-mont hphase dedicated to compiling the "answer key" templates for the texts in th etraining set (see next section), refining the task definition, and developing theinitial MUC-3 version of the data extraction systems .
These systems underwent adry-run test in February, 1991, after which a meeting was held to discuss th eresults and hammer out some of the remaining evaluation issues .
Twelve sitesparticipated in the dry run.
One site dropped out after the dry run (TRW), and fou rnew sites entered, three of which had already been involved to some extent (BB NSystems and Technologies, McDonnell Douglas Electronic Systems Company, an dSynchronetics, Inc .)
and one that had not (Hughes Research Laboratories) .The second phase began in mid-February and, while system developmen tcontinued at each of the participating sites, updates were made to the scorin gprogram, the task definition, and the answer key templates for the training set .Final testing was carried out in May, 1991, concluding with the Third Messag eUnderstanding Conference (MUC-3), which was attended by representatives of th eparticipating sites and interested government organizations .
During theconference, the evaluation participants decided that the test results should b evalidated by having the system-generated templates rescored by a single party .Two of the participants were selected to work as a team to carry out this task, an dthe results of their effort are the official test scores presented in this volume .Pure and hybrid systems based on a wide range of text interpretatio ntechniques (e .g ., statistical, key-word, template-driven, pattern-matching, in -depth natural language processing) were represented in the MUC-3 evaluation .The fifteen sites that completed the evaluation are Advanced Decision Systems(Mountain View, CA), BBN Systems and Technologies (Cambridge, MA), Genera lElectric (Schenectady, NY), General Telephone and Electronics (Mountain View ,CA), Intelligent Text Processing, Inc .
(Santa Monica, CA), Hughes Researc hLaboratories (Malibu, CA), Language Systems, Inc .
(Woodland Hills, CA), McDonnel lDouglas Electronic Systems (Santa Ana, CA), New York University (New York City ,NY), PRC, Inc .
(McLean, VA), SRI International (Menlo Park, CA), Synchronetics ,Inc .
together with the University of Maryland (Baltimore, MD), Unisys Center fo rAdvanced Information Technology (Paoli, PA), the University of Massachusett s(Amherst, MA), and the University of Nebraska (Lincoln, NE) in association wit hthe University of Southwest Louisiana (Lafayette, LA) .Parts II and III of thisvolume include papers by each of these sites .
In addition, an experimenta lprototype of a probabilistic text categorization system was developed by Davi dLewis, who is now at the University of Chicago, and was tested along with the othe rsystems.
That work is described in a paper in Part IV .5CORPUS AND TASKThe corpus was formed via a keyword query'.
to an electronic databasecontaining articles in message format from open sources worldwide .
These article shad been gathered, translated (if necessary), edited, and disseminated by th eForeign Broadcast Information Service (FBIS) of the U .S .
Government .
A trainingset of 1300 texts was identified, and additional texts were set aside for use as tes tdata 2 .
The message headers were used to create or augment a dateline and the tex ttype information appearing at the front of the article ; the original messag eheaders and routing information were removed .
The layout was modified slightlyto improve readability (e .g., by double-spacing between paragraphs), an dproblems that arose with certain characters when the data was downloaded wererectified (e.g., square brackets were missing and had to be reinserted) .
The body ofthe text was modified minimally and with the sole purpose of eliminating som eidiosyncratic features that were well beyond the scope of interest of MUC-3 3 .The corpus presents realistic challenges in terms of overall size (over 2 .
5megabytes), length of the individual articles (approximately a half-page each o naverage), variety of text types (newspaper articles, TV and radio news, speech andinterview transcripts, rebel communiques, etc .
), range of linguistic phenomenarepresented (both well-formed and ill-formed), and open-endedness of th evocabulary (especially with respect to proper nouns) .
The texts used in MUCK-Iand MUCK-II originated as teletype messages and thus were all upper case ; theMUC-3 texts are also all upper case, but only as a consequence of downloading fromthe source database, where the texts appear in mixed upper and lower case .The task was to extract information on terrorist incidents (incident type, date ,location, perpetrator, target, instrument, outcome, etc .
)from the relevant texts ina blind test on 100 previously unseen texts .Approximately half the articles wereirrelevant to the task as defined .In some cases the terrorism keywords in th equery used to form the corpus (see footnote) were used in irrelevant senses, e .g .
,"explosion" in the phrase "social explosion" .In other cases, an entity of one of the'.
The query specified a hit as a message containing both a country/nationality name (e .g .
,Honduras or Honduran) for one of the nine countries of interest (Argentina, Bolivia, Chile ,Colombia, Ecuador, El Salvador, Guatemala, Honduras, Peru) and some inflectional form of acommon word associated with terrorist acts (abduct, abduction, ambush, arson, assassinate ,assassination, assault, blow [up], bomb, bombing, explode, explosion, hijack, hijacking, kidnap ,kidnapping, kill, killing, murder, rob, shoot, shooting, steal, terrorist) .
Some of the articles i nthe MUC-3 corpus may no longer satisfy this query, since the message headers (including th esubject line) were removed after the retrieval was done .2 Over 300 articles were set aside from the overall corpus to be used as test data .
Thecomposition of the test sets was intentionally controlled with respect to the frequency wit hwhich incidents concerning any given country are represented; otherwise, the selection wa sdone simply by taking every nth article about that country .3 For example, transcriptions of radio and TV broadcasts sometimes contained sentences i nwhich words were enclosed in parentheses to indicate that the transcriber could not be certai nof them, e .g., "They are trying to implicate the (Ochaski Company) with narcoterrorism ."
(Thisquote is from article number PA1807130691 of the Latin America volume of the Foreig nBroadcast Information Service Daily Reports .)
In cases such as this, where the text i sparenthetical in form but not in function, the parentheses were deleted .6nine countries of interest -- the second necessary condition for a hit -- wa smentioned, but the entity did not play a significant role in the terrorist incident .Other articles were irrelevant for reasons that were harder to formulate .Forexample, some articles concerned common criminal activity or guerrilla warfare(or other military conflict) .Rules were developed to challenge the systems t odiscriminate among various kinds of violent acts and to generate templates onl yfor those that would be of interest to a terrorism news analyst .
The real-lifescenario also required that only timely, substantive information be extracted ; thus ,rules were formulated that defined relevance in terms of whether the news wa srecent and whether it at least mentioned who/what the target was .Otherrelevance criteria were developed as well, again with the intent of simulating areal-life task .
The relevance criteria are described in the first part of appendix A ,which is the principal documentation of the MUC-3 task .
Appendix D contains somerepresentative samples of relevant and irrelevant articles .It can be seen that the relevance criteria are extensive and would sometimes b edifficult to state, let alne implement .
It was learned that greater allowance sneeded to made for the fact that this was an evaluation task and not a real-life one .Systems that generated generally correct internal data structures for a relevan tincident, only to filter out that data structure by making a single mistake on one o fthe relevance criteria, were penalized for having missed the incident entirel yrather than being penalized for getting just one aspect of the incident descriptio nwrong.
Some allowance was made in the answer key for the fact that incidents o rfacts about incidents might be of questionable relevance, given the vagueness o fsome texts and gaps in the statement of the relevance criteria ; the templatenotation allowed for optionality, and systems were not penalized if they failed t ogenerate an optional template or an optional filler in a required template .If an article was determined to be relevant, there was then the task o fdetermining how many distinct relevant incidents were being reported.
Theinformation on these incidents had to be correctly disentangled and represented i nseparate templates .The extracted information was to be represented in th etemplate in one of several ways, according to the data format requirements of eachslot.
(See appendix A .)
Some slot fills were required to be categories from apredefined set of possibilities called a "set list" (e .g., for the various types o fterrorist incidents such as BOMBING, ATTEMPTED BOMBING, BOMB THREAT) ;others were required to be canonicalized forms (e.g ., for dates) or numbers ; stil lothers were to be in the form of strings (e .g., for person names) .A relatively simple article and corresponding answer key template from th edry-run test set (labeled TST1) are shown in Figures 1 and 2 .
Note that the text i nFigure 1 is all upper case, that the dateline includes the source of the articl e("Inravision Television Cadena 1") and that the article is a news report by JorgeAlonso Sierra Valencia.
In Figure 2, the left-hand column contains the slot labels ,and the right-hand column contains the correct answers as defined by NOSC .Slashes mark alternative correct responses (systems are to generate just one of thepossibilities), an asterisk marks slots that are inapplicable to the incident typ ebeing reported, a hyphen marks a slot for which the text provides no fill, and acolon introduces the cross-reference portion of a fill (except for slot 16, where th ecolon is used as a separator between more general and more specific place names) .More information on the template notation can be found in appendix A, andfurther examples of texts and templates can be found in appendices D and E .7TST 1-MUC 3-008 0BOGOTA, 3 APR 90 (INRAVISION TELEVISION CADENA 1) -- [REPORT] [JORGE ALONS OSIERRA VALENCIA] [TEXT] LIBERAL SENATOR FEDERICO ESTRADA VELEZ WA SKIDNAPPED ON 3 APRIL AT THE CORNER OF 60TH AND 48TH STREETS IN WESTER NMEDELLIN, ONLY 100 METERS FROM A METROPOLITAN POLICE CAI [IMMEDIATEA1"1ENTION CENTER] .
THE ANTIOQUTA DEPARTMENT LIBERAL PARTY LEADER HADLEFT HIS HOUSE WITHOUT ANY BODYGUARDS ONLY MINUTES EARLIER .
AS HE WAITEDFOR THE TRAFFIC LIGHT TO CHANGE, THREE HEAVILY ARMED MEN FORCED HIM TO GE TOUT OF HIS CAR AND GET INTO A BLUE RENAULT .HOURS LATER, THROUGH ANONYMOUS TELEPHONE CALLS TO THE METROPOLITA NPOLICE AND TO THE MEDIA, THE EXTRADITABLES CLAIMED RESPONSIBILITY FOR TH EKIDNAPPING .
IN THE CALLS, THEY ANNOUNCED THAT THEY WILL RELEASE TH ESENATOR WITH A NEW MESSAGE FOR THE NATIONAL GOVERNMENT .LAST WEEK, FEDERICO ESTRADA VELEZ HAD REJECTED TALKS BETWEEN TH EGOVERNMENT AND THE DRUG TRAFFICKERS .Figure 1 .
Article from MUC-3 Corpus 4Figure 2.
Answer Key Template4This article has serial number PA0404072690 in the Latin America volume of the FBIS Dail yReports, which are the secondary source for all the texts in the MUC-3 corpus .0 .
MESSAGE ID1.
TEMPLATE ID2.
DATE OF INCIDENT3.
TYPE OF INCIDENT4.
CATEGORY OF INCIDENT5.
PERPETRATOR: ID OF INDIV(S)6.
PERPETRATOR : ID OF ORG(S )7.
PERPETRATOR : CONFIDENCE8.
PHYSICAL TARGET : ID(S )9.
PHYSICAL TARGET : TOTAL NUM10.
PHYSICAL TARGET : TYPE(S)11.
HUMAN TARGET: ID(S)12.
HUMAN TARGET: TOTAL NUM13.
HUMAN TARGET: TYPE(S)14.
TARGET: FOREIGN NATION(S )15.
INSTRUMENT : TYPE(S)16.
LOCATION OF INCIDENT17.
EFFECT ON PHYSICAL TARGET(S )18.
EFFECT ON HUMAN TARGET(S)TST1-MUC3-008 0103 APR 90KIDNAPPINGTERRORIST ACT"THREE HEAVILY ARMED MEN ""THE EXTRADITABLES" / "EXTRADITABLES "CLAIMED OR ADMITTED : "THE EXTRADITABLES" /"EXTRADITABLES "***"FEDERICO ESTRADA VELEZ" ("LIBERAL SENATOR" /"ANTIOQUTA DEPARTMENT LIBERAL PARTY LEADER "/ "SENATOR" / "LIBERAL PARTY LEADER" / "PARTYLEADER" )1GOVERNMENT OFFICIAL / POLITICAL FIGURE :"FEDERICO ESTRADA VELEZ"*COLOMBIA: MEDELLIN (CITY)*8The participants collectively created the answer key for the training set, eachsite manually filling in templates for partially overlapping subset of the texts .This task was carried out at the start of the evaluation ; it therefore providedparticipants with good training on the task requirements and provided NOSC wit hgood early feedback .
Generating and cross-checking the templates required aninvestment of at least two person-weeks of effort per site .
These answer keys wereupdated a number of times to reduce errors and to maintain currency wit hchanging template fill specifications .
In addition to generating answer ke ytemplates, sites were also responsible for compiling a list of the place names tha tappeared in their set of texts ; NOSC then merged these lists to create the set lists fo rthe TARGET: FOREIGN NATION slot and LOCATION OF INCIDENT slot .MEASURES OF PERFORMANC EAll systems were evaluated on the basis of performance on the informatio nextraction task in a blind test at the end of each phase of 'he evaluation .
It wasexpected that the degree of success achieved by the different techniques in Maywould depend on such factors as whether the number of possible slot fillers wa ssmall, finite, or open-ended and whether the slot could typically be filled by fairl ystraightforward extraction or not .
System characteristics such as amount o fdomain coverage, degree of robustness, and general ability to make proper use o finformation found in novel input were also expected to be major factors .
The dry -run test results were not assumed to provide a good basis for estimatin gperformance on the final test in May, but the expectation was that most, if not all ,of the systems that participated in the dry run would show dramatic improvement sin performance .The test results show that some of these expectations were born eout, while others were not or were less significant than expected .A semi-automated scoring program was developed under contract for MUC-3 t oenable the calculation of the various measures of performance.
It was distributedto participants early on during the evaluation and proved invaluable in providin gthem with the performance feedback necessary to prioritize and reprioritize thei rdevelopment efforts as they went along .The scoring program can be set up t oscore all the templates that the system generates or to score subsets oftemplates/slots .User interaction is required only to determine whether amismatch between the system-generated templates and the answer key template sshould be judged completely or partially correct .
(A partially correct filler for slo t11 in Figure 2 might be "VELEZ"("LEADER"), and a partially correct filler fo rslot 16 would be simply COLOMBIA .
)An extensive set of interactive scorin gguidelines was developed to standardize the interactive scoring .These guideline sare contained in appendix C .
The scoring program maintains a log of interaction sthat can be used in later scoring runs and augmented by the user as the system i supdated and the system-generated templates change .The two primary measures of performance were completeness (recall) an daccuracy (precision) .
There were two additional measures, one to isolate theamount of spurious data generated (overgeneration) and the other to determin ethe rate of incorrect generation as a function of the number of opportunities t oincorrectly generate (fallout) .The labels "recall," "precision," and "fallout" wereborrowed from the field of information retrieval, but the definitions of those term shad to be substantially modified to suit the template-generation task .Theovergeneration metric has no correlate in the information retrieval field, i .e ., a9MUC-3 system can generate indefinitely more data than is actually called for, butan information retrieval system cannot retrieve more than the total number ofitems (e .g., documents) that are actually present in the corpus .Fallout can be calculated only for those slots whose fillers form a closed set .Scores for the other three measures were calculated for the test set overall, wit hbreakdowns by template slot .Figure 3 presents somewhat simplified definitions .MEASUREII DEFINITIONRECALL #correctfillsgenerated#fillsinkeyPRECISION #correctfillsgenerated#fillsgeneratedOVERGENERATION #spuriousfillsgenerated#fillsgenerate dFALLOUT #incorrect+spuriousgenerate d#possibleincorrectfillsFigure 3 .
MUC-3 Scoring Metric sThe most significant thing that this table does not show is that precision and recal lare actually calculated on the basis of points -- the term "correct" includes syste mresponses that matched the key exactly (earning 1 point each) and syste mresponses that were judged to be a good partial match (earning .5 point each) .
Itshould also be noted that overgeneration is not only a measure in its own right bu tis also a component of precision, where it acts as a penalty by contributing to thedenominator .Overgeneration also figures in fallout by contributing to thenumerator.
Further information on the MUC-3 evaluation metrics and scorin gmethods, including information on three different ways penalties for missing andspurious data were assigned, can be found elsewhere in this volume in the pape ron evaluation metrics by Nancy Chinchor [3] .TEST PROCEDUREFinal testing was done on a test set of 100 previously unseen texts that wererepresentative of the corpus as a whole.
Participants were asked to copy the tes tpackage electronically to their own sites when they were ready to begin testing .Appendix B contains a copy of the test procedure .
The testing had to be conductedand the results submitted within a week of the date when the test package was mad eavailable for electronic transfer .
Each site submitted their system-generatedtemplates, the outputs of the scoring program (score reports and the interactiv escoring history file), and a trace of the system's processing (whatever type of trac ethe system normally produces that could serve to help validate the system' soutputs) .Initial scoring was done at the individual sites, with someone designate das interactive scorer who preferably had not been part of the system developmen tteam .
After the conference, the system-generated templates for all sites werelabeled anonymously and rescored by two volunteers in order to ensure that th eofficial scores were obtained as consistently as possible .The system at each site was to be frozen before the test package wa stransferred ; no updates were permitted to the system until testing and scorin g1 0were completed .Furthermore, no backing up was permitted during testing in th eevent of a system error .In such a situation, processing was to be aborted an drestarted with the next text .A few sites encountered unforeseen system problem sthat were easily pinpointed and fixed.
They reported unofficial, revised test result sat the conference that were generally similar to the official test results and do no talter the overall picture of the current state of the art .The basic test called for systems to be set up to generate templates tha tproduced the "maximum tradeoff" between recall and precision, i .e., templates thatachieved scores as high as possible and as similar as possible on both recall an dprecision .
This was the normal mode of operation for most systems and for man ywas the only mode of operation that the developers had tried .
Those sites that coul doffer alternative tradeoffs were invited to do so, provided they notified NOSC i nadvance of the particular setups they intended to test on .In addition to the scores obtained for these metrics on the basic template -generation task, scores were obtained of system performance on the linguisticphenomenon of apposition, as measured by the template fills generated by th esystems in particular sets of instances .
That is, sentences exemplifying appositionwere marked for separate scoring if successful handling of the phenomenonseemed to be required in order to fill one or more template slots correctly for tha tsentence .
This test was conducted as an experiment and is described in the paperby Nancy Chinchor on linguistic phenomena testing [4] .TEST RESULTS AND DISCUSSIO NThe summary score reports produced for the tested systems by the scorin gprogram are found in appendix F ; scatter plots for selected portions of the final tes tresults are shown in appendix G .
Most of the figures in appendix G plot recal lversus precision; a couple plot recall vs overgeneration, since the generation o fspurious data is a significant element of precision with respect to a templat egeneration task .The plots facilitate consideration of que ions such as th efollowing :* On which aspect of the task (slot in the template) were the systems as agroup most successful ?
* How well did the systems handle time expressions (DATE OF INCIDEN Tslot)?
* How did the front-running systems on the overall measures differ wit hrespect to individual slot performance?
* To what extent do the different ways of computing the score s(Matched/Missing, Matched Only, All Templates, and Set Fills Only) change th epicture?
* To what extent was generation of spurious data taking place ?
* To what extent did the individual systems' recall and precision represen ttradeoffs?11Not included in the appendices are the detailed score reports produced by thescoring program for each of the system-generated templates .
These reports permitconsideration of other interesting questions such as how systems performed fro mone terrorist incident type to another and how they performed when a messag econtained more than one relevant incident report .
It is also possible to use the mtogether with the corresponding texts to answer questions such as how wel lsystems handled newspaper articles versus TV and radio news reports and how wel lthey handled incident reports that were spread out over a paragraph or acrossparagraphs rather than being completely described in a single sentence .The appendices also do not include the results of a minor study of huma nperformance on the MUC-3 final test .
This study was conducted using two MUC-3evaluators as subjects and measuring their performance individually compared t othe official answer key, which was created by merging and correcting thei rindividual draft keys.
The evaluator with the lower scores for Matched/Missinghad 87% recall, 91% precision, and 5% overgeneration .
Needless to say, since thes esubjects were responsible for preparing the official answer key, thei rperformance on the draft keys was undoubtedly higher than could be expecte deven from other highly trained persons .
Another reason they are higher tha nwould be obtained in a different study is that the two evaluators prepared the draf tkeys in two stages and reconciled most of the differences that arose in the firs tstage before starting the second stage .
In the first stage, the evaluators identifiedwhich articles were relevant, how many templates would be generated for th erelevant ones, and which incident types would be represented in each of th etemplates .
In the second stage, the evaluators filled in the templates, with th eassistance of an interactive software tool that provides some integrity checking ,automatic fill-in, etc .The plots in appendix G present an interesting picture of the MUC-3 results as awhole, but the significance of the numbers for each of the tested systems needs t obe assessed on the basis of a careful reading of the papers in this volume that wer esubmitted by each of the sites .
To facilitate interpretation of the test results, th esites were asked to focus on the test scores and the evaluation experience in th efirst of those papers and to elaborate in their second paper on how the system -- a sit was actually implemented for MUC-3 -- works in general and how it is designed t ohandle the kinds of phenomena found in the MUC-3 corpus .The level of, effort that could be afforded by each of the sites varie dconsiderably, as did the maturity of the systems at the start of the evaluation .
Al lsites were operating under time constraints imposed by the evaluation schedule.In addition, the evaluation demands were a consequence of the intricacies of th etask and of general corpus characteristics such - as the following :* The texts that are relevant to the MUC-3 task (comprising approximatel y50% of the total corpus) are likely to contain more than one relevant incident .
* The information on a relevant incident may be dispersed throughout thetext and may be intertwined with accounts of other (relevant or irrelevant )incidents .
* The corpus includes a mixture of material (newspaper articles, TV news ,speeches, interviews, propaganda, etc .)
with varying text structures and styles .12The scoring program produces four sets of overall scores, three of which arebased on different means of assessing penalties for missing and spurious data .These sets of scores appear in the rows at the bottom of the score reports .
The setcalled Matched/Missing is a compromise between Matched Only (which is mor elenient than Matched/Missing) and All Templates (more stringent) and is used a sthe official one for reporting purposes .
Figure G1 is based on the Matched/Missin gmethod of assessing penalties .
The fourth method does the scoring only for thoseslots that require set fills, i .e., fills that come from predefined sets of categories .Figure G4 is based on that method of scoring .
The various methods are describe dmore fully in [3] .The remainder of this section is a discussion of just a few of the figures i nappendix G. (The data points in appendix G are labeled with abbreviated names o fthe 15 sites, and optional test runs are marked with the site ' s name and an "0 "extension .)
Figure GI gives the most general picture of the results of MUC-3 fina ltesting .It shows that precision always exceeds recall and that the systems withrelatively high recall are also the ones that have relatively high precision .
Thelatter fact inspires an optimistic attitude toward the promise of at least some of th etechniques employed by today's systems -- further efforts to enhance existin gtechniques and extend the systems' domain coverage may lead to significantl yimproved performance on both measures .
However, since all systems show bette rprecision than recall, it appears that it will be a bigger challenge to obtain ver yhigh recall than it will be to achieve higher precision at recall levels that ar esimilar to those achievable today .
This observation hold true even for Figure G 2(Matched Only), where recall is substantially greater for most systems compared toG1 .
5The distribution of data points tentatively supports at least one genera lobservation about the technologies underlying today's systems : those systems tha tuse purely stochastic techniques or handcrafted pattern-matching technique swere not able to achieve the same level of performance for MUC-3 as some of thesystems that used parsing techniques .
The "non-parsing" systems are ADS, HU ,MDC, UNI, UNL, UNL-01, and UNL-02, and the "parsing" systems are BBN, BBN-O, GE ,GTE, ITP, LSI, NYU, NYU-01, NYU-02, PRC, SRI, SYN, UMA, and UMA-O .Further support for this observation can be found in Figure G4, where thescores are computed for all slots requiring set fills, and in Figure G9, which show sthe scores for just one of those set-fill slots, the TYPE OF INCIDENT .
In thesecases, one might expect the non-parsing systems to compare more favorably wit hthe parsing systems, since the fill options are restricted to a fairly small ,predefined set of possibilities .
6 However, none of the non-parsing systems appearsat the leading edge in Figure G4, and the only non-parsing system in the cluster a tthe leading edge in Figure G9 is ADS (which shares a data point with NYU-02) ,5 Recall is greater in G2 because Matched Only differs from Matched/Missing in that the "tota lpossible," i .e ., the recall denominator, does not include penalties for missing templates .6The results in G4 are somewhat contaminated due to the fact that some of the set-fill slot srequire that the fillers be cross-referenced to fillers of string-fill slots (see, for example, th efillers of slots 7, 11, and 13 in Figure 2 earlier in this paper) .
The scoring of the set-fill slotsis affected by these cross-reference tags .
However, the TYPE OF INCIDENT results (G9) ar enot contaminated in this way.13although a few non-parsing systems have extremely high precision scores (UNI ,UNL, UNL-01, and UNL-02) .On the other hand, there is quite a range in performance even among th esystems in the parsing group, all of which had to cope with having limite dcoverage of the domain .
One thing that is apparent from the sites' systemdescriptions (see Part III of this proceedings) is that the ones on the leading edg ein Figure G1 have the ability to make good use of partial sentence parses whe ncomplete parses cannot be obtained .
Level of effort is also an indicator o fperformance success, though not a completely reliable one : GE, NYU, and UMass al lreported investing more than one person-year of effort in MUC-3, but severa lother sites with lower overall performance also reported just under or over on eperson-year of effort .It must be said that there were some extremely immature systems in the non -parsing group and the parsing group alike, so any general conclusions must b etaken as tentative and should certainly not be used to form opinions about th erelative validity of isolated techniques employed by the individual systems in eac hgroup.
It could be that the relatively low-performing systems use extremel yeffective techniques that, if supplemented by other known techniques o rsupported by more extensive domain coverage, would put the system well out i nfront .Neither should one assume that the systems at the leading edge are simila rkinds of systems .In fact, those systems have quite different architectures an dhave varying sizes of lexicons, kinds of parsers and semantic interpreters, etc .Figures G7 through G24 show how system performance varied from one slot t oanother .
Figures G7, G9, and G17 are useful as examples of the way spurious datageneration combines with incorrect data generation to affect the precision score sin different kinds of slots .
Figure G7 is for the TEMPLATE ID slot .
The fillers ofthis slot are arbitrary numbers that uniquely identify the templates for a give nmessage .
The scoring program disregards the actual values and finds the bes tmatch between the system-generated templates and the answer key templates for agiven message based on the degree of match in fillers of other slots in th etemplate.
Since there is no such thing as an incorrect template ID, only a spuriou sor missing template ID, and since missing data plays no role at all in computin gprecision, the only penalty to precision for the TEMPLATE ID slot is due tospurious data generation .
In contrast to the TEMPLATE ID slot, the TYPE OFINCIDENT slot (Figure G9) shows no influence of spurious data on precision at all .This is because the TYPE OF INCIDENT slot permits only one filler.
The HUMA NTARGET : ID(S) slot (Figure G17) can be filled with indefinitely many fillers an dthus shows the impact of both incorrect and spurious data on precision .Four sites submitted results for the optional test runs that were alluded to in th eprevious section -- BBN Systems and Technologies (BBN-O), New York University(NYU-01 and NYU-02), the University of Massachusetts (UMA-O), and th eUniversity of Nebraska/University of Southwestern Louisiana (UNL-01 and UNL -02) .These sites conducted radically different experiments to generate templatesmore conservatively .
The BBN-O experiment largely involved doing a narrowersearch in the text for the template-filling information ; the NYU-01 and NYU-02experiments involved throwing out templates in which certain key slots wer eeither unfilled or were filled with information that indicated an irrelevan tincident with good probability ; the UMA-O experiment bypassed a case-base dreasoning component of the system; and the UNL-01 and UNL-02 experiment s14involved the usage of different thresholds in their connectionist framework .Theexperiments resulted in predicted differences in the Matched/Missing score scompared to the basic test .In almost all cases the experiments had the overal leffect of lowering recall ; in all cases they lowered overgeneration and therebyraised precision .
Figure G7 shows the marked difference the experiments made i nspurious template generation ; Figure G1 shows the much smaller difference theymade in overall recall and precision .CONCLUSIONSThe MUC-3 evaluation established a solid set of performance benchmarks fo rsystems with diverse approaches to text analysis and information extraction .
TheMUC-3 task was extremely challenging, and the results show what can be done withtoday's technologies after only a modest domain- and task-specific developmen teffort (on the order of one person-year) .
On a task this difficult, the systems tha tcluster at the leading edge were able to generate in the neighborhood of 40-50% o fthe expected data and to do it with 55-65% accuracy .
Breakdowns of performanceby slot show that performance was best on identifying the type of incident -- 70 -80% recall (completeness) and 80-85% precision (accuracy) were achieved, an dprecision figures in the 90-100% range were possible with some sacrifice in recall .All of the MUC-3 system developers are optimistic about the prospects fo rseeing steady improvements in system performance for the foreseeable future .This feeling is based variously on such evidence as the amount of improvemen tachieved between the dry-run test and the final test, the slope of improvementrecorded on internal tests conducted at intervals during development, and thedevelopers' own awareness of significant components of the system that they hadnot had time to adapt to the MUC-3 task .
The final test results are consistent withthe claim that most systems, if not all, may well be still on a steep slope o fimprovement .
However, they also show that performance on recall is not as goo das performance on precision, and they lend support to the possibility that thi sdiscrepancy will persist .It appears that systems cannot be built today that ar ecapable of obtaining high overall recall, even at the expense of outrageously hig hovergeneration .Systems can, however, be built that will do a good job a tpotentially useful subtasks such as identifying terrorist incidents of various kinds .The results give at least a tentative indication that systems incorporatin grobust parsing techniques show more long-term promise of high performancethan non-parsing systems .
However, there are great differences in techniquesamong the systems in the parsing and non-parsing groups and even among thos erobust parsing systems that did the best in maximizing recall and precision an dminimizing the tradeoff between them .
Further variety was evident in theoptional test runs conducted by some of the sites .
Those runs show promise for th edevelopment of systems that can be "tuned" in various ways to generate data mor eaggressively or more conservatively, yielding tradeoffs between recall an dprecision that respond to differences in emphasis in real-life applications .Some conclusions can be drawn regarding the evaluation setup itself that wil linfluence future work.
First, the evaluation corpus and task were sufficientl ychallenging that they can be used again in a future evaluation (with a refined tas kdefinition and a new test set) .Second, the information extraction task need smodification in order to focus as much as possible on language processing15capabilities separate from information extraction capabilities, and new ideas fo rdesigning tests related to specific linguistic phenomena are needed .
Finally, morework is needed to ensure that the statistical significance of the results is known ,and a serious study of human performance on the task is needed in order to defin econcrete performance goals for the systems .ACKNOWLEDGEMENTSThis work was funded by DARPA under ARPA order 6359 .
The author i sindebted to all the evaluation participants, whose collaboration on MUC-3 deserve sthe highest praise .
The author would especially like to thank those individualswho served in special capacities and contributed extra time and energy to ensur ethe success of the evaluation and the publication of the proceedings, among who mare Laura Blumer Balcom, Nancy Chinchor, Ralph Grishman, Pete Halverson ,Lynette Hirschman, Jerry Hobbs, Cheryl Kariya, George Krupka, David Lewis, Lis aRau, Eric Scott, John Sterling, Charles Wayne, and Carl Weir.REFERENCES[1] Grishman, R ., and Sterling, J ., Preference Semantics for MessageUnderstanding, in Proceedings of the Speech and Natural Language Workshop ,October, 1989, Morgan Kaufmann, pp .
71-74.
[2] Hirschman, L ., Comparing MUCK-II and MUC-3 :Assessing the Difficulty o fDifferent Tasks (in this volume) .
[3] Chinchor, N., MUC-3 Evaluation Metrics (in this volume) .
[4] Chinchor, N., MUC-3 Linguistic Phenomena Test Experiment (in this volume) .16
