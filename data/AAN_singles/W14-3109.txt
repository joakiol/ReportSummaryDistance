Proceedings of the Workshop on Interactive Language Learning, Visualization, and Interfaces, pages 59?62,Baltimore, Maryland, USA, June 27, 2014. c?2014 Association for Computational LinguisticsDesign of an Active Learning System with Human Correction for Content Analysis  Nancy McCracken School of Information Studies Syracuse University, USA njmccrac@syr.eduJasy Liew Suet Yan School of Information Studies Syracuse University, USA jliewsue@syr.eduKevin Crowston National Science Foundation Syracuse University, USA crowston@syr.edu    AbstractOur research investigation focuses on the role of humans in supplying corrected examples in active learning cycles, an important aspect of deploying active learning in practice.
In this paper, we dis-cuss sampling strategies and sampling sizes in set-ting up an active learning system for human ex-periments in the task of content analysis, which involves labeling concepts in large volumes of text.
The cost of conducting comprehensive hu-man subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high.
To reduce those costs, we first applied an active learning simulation approach to test the ef-fect of different sampling strategies and sampling sizes on machine learning (ML) performance in order to select a smaller set of parameters to be evaluated in human subject studies.
1 Introduction Social scientists often use content analysis to understand the practices of groups by analyzing texts such as transcripts of interpersonal commu-nication.
Content analysis is the process of iden-tifying and labeling conceptually significant fea-tures in text, referred to as ?coding?
(Miles and Huberman, 1994).
For example, researchers studying leadership might look for evidence of behaviors such as ?suggesting or recommending?
or ?inclusive reference?
expressed in email mes-sages.
However, analyzing text is very labor-intensive, as the text must be read and under-stood by a human.
Consequently, important re-search questions in the qualitative social sciences may not be addressed because there is too much data for humans to analyze in a reasonable time.
A few researchers have tried automatic tech-niques on content analysis problems.
For exam-ple, Crowston et al.
(2012) manually developed a classifier to identify codes related to group maintenance behavior in free/libre open source software (FLOSS) teams.
Others have applied machine-learning (ML) techniques.
For example, Ishita et al.
(2010) used ML to automaticallyclassify sections of text within documents on ten human values taken from the Schwartz?s Value Inventory.
Broadwell et al.
(2012) developed models to classify sociolinguistic behaviors to infer social roles (e.g., leadership).
On the best performing codes, these approaches achieve ac-curacies from 60?80%, showing the potential of automatic qualitative content analysis.
However, these studies all limited their reports to a subset of codes used by the social scientists, due in part to the need for a large volume of training data.
The state-of-the-art ML approaches for con-tent analysis require researchers to obtain a large amount of annotated data upfront, which is often costly or impractical.
An active learning ap-proach which uses human correction during the steps of active learning could potentially help produce a large amount of annotated data while minimizing the cost of human annotation effort.
Unlike other text annotation tasks, the code an-notation for content analysis requires significant cognitive effort, which may limit, or even nulli-fy, the benefits of active learning.
We are building an active machine learning system to semi-automate the process of content analysis, and are planning to study the human role in such machine learning systems.Manually codedocuments inATLAS.tiExport goldstandard data intoXMLHuman Annotation Machine AnnotationLearn modelApply model toadditionaldocumentsHuman CorrectionManually correctmodel codingSave correctedcoding as silver dataFigure 1.
Active learning for semi-automatic content analysis.
As illustrated in Figure 1, the system design in-corporates building a classifier from an initial set of hand-coded examples and iteratively improv-59ing the model by having human annotators cor-rect new examples identified by the system  Little is yet known about the optimal number of machine annotations to be presented to human annotators for correction, and how the sample sizes of machine annotations affect ML perfor-mance.
Also, existing active learning sampling strategies to pick out the most ?beneficial?
ex-amples for human correction to be used in the next round of ML training have not been tested in the context of social science data, where con-cept codes may be multi-dimensional or hierar-chical, and the problem may be multi-label (one phrase or sentence in the annotated text has mul-tiple labels).
Also, concept codes tend to be very sparse in the text, resulting in a classification problem that has both imbalance?the non-annotated pieces of text (negative examples) tend to be far more frequent that annotated text?and rarity, where there may not be enough examples of some codes to achieve a good classifier.
The cost of conducting comprehensive human subject studies to experimentally determine the effects of sampling sizes and sampling sizes is high.
Therefore, we first applied an active learn-ing simulation approach to test the effect of dif-ferent sampling strategies and sampling sizes on machine learning (ML) performance.
This allows the human subject studies to involve a smaller set of parameters to be evaluated.
2 Related Work For active learning in our system, we are using what is sometimes called pool-based active learning, where a large number of unlabeled ex-amples are available to be the pool of the next samples.
This type of active learning has been well explored for text categorization tasks (Lewis and Gale, 1994; Tong and Koller 2000; Schohn and Cohn 2000).
This approach often uses the method of uncertainty sampling to pick new samples from the pool, both with probability models to give the ?uncertainty?
(Lewis and Gale, 1994) and with SVM models, where the margin numbers give the ?uncertainty?
(Tong and Koller 2000; Schohn and Cohn 2000).
While much of the research focus has been on the sam-pling method, some has also focused on the size of the sample, e.g.
in Schohn and Cohn (2000), sample sizes of 4, 8, 16, and 32 were used, where the result was that smaller sizes gave a steeper learning curve with a greater classification cost, and the authors settled on a sample size of 8.
Foradditional active learning references, see the Set-tles (2009) survey of active learning literature.
This type of active learning has also been used in the context of human correction.
One such system is described in Mandel et al.
(2006), using active learning for music retrieval, where users were presented with up to 6 examples of songs to label.
Another is the DUALLIST system described in Settles (2011) and Settles and Zhu (2012) where human experiments were carried out for text classification and other tasks.
While most active learning experiments focus on reduc-ing the number of examples to achieve an accu-rate model, there has been some effort to model the reduction of the cost of human time in anno-tation, where the human time is non-uniform per example.
Both the systems in Culotta and McCallum (2005) and in Clancy et al.
(2012) for the task of named entity extraction, modeled hu-man cost in the context of sequential information extraction tasks.
However, one difference be-tween these systems and ours is that all of the tasks studied in these systems did not require annotators to have extensive training to annotate complex concept codes.
3 Problem We worked with a pilot project in which researchers are studying leadership in open source software groups by analyzing open source developer emails.
After a year of part-time annotation by two annotators, the researchers developed a codebook that provides a definition and examples for 35 codes.
The coders achieved an inter-annotator agreement (kappa) of about 80%, and annotated about 400 email threads, consisting of about 3700 sentences.
We used these coded messages as the ?gold standard?
data for our study.
However, only 15 codes had more than 25 instances in the gold standard set.
The most common code (?Explanation/Rationale/ Background?)
occurred only 319 times.
In our pilot correction experiments, annota-tors tried correcting samples of sizes ranging from about 50 to about 400.
Anecdotal evidence indicates that annotators liked to annotate sample sizes of about 100 in order to achieve good focus on a particular code definition at one time, but without getting stressed with too many examples.
Part of the required focus is that annotators need to refresh their memory on any particular code at the start of annotation, so switching frequently between different codes is cognitively taxing.
This desired sample size contrasts with prior ac-60tive learning systems that employ much smaller sample sizes, in the range of 1 to 20.
We are currently in the process of setting up the human experiments to test our main research question of achieving an accurate model for con-tent analysis using a minimum of human effort.
In this paper, we discuss two questions for active learning in order to have annotators cor-rect an acceptable number of machine annota-tions that are most likely to increase the perfor-mance of the ML model in each iteration.
These are:  how do different sample sizes and different sampling strategies of machine annotations pre-sented to human annotators for correction in each round affect ML performance?
4 Active Learning Simulation Setup In a similar strategy to that of Clancy et al.
(2012), we carried out a preliminary investiga-tion by conducting an active learning simulation on our gold standard data.
The simulation starts with a small initial sample, and uses active learn-ing where we ?correct?
the sample labels by tak-ing labels from the gold standard corpus.
For our simulation experiments, we separated the gold standard data randomly into a training set of 90% of the examples, 3298 sentences, and a test set of 10%, 366 sentences.
In the experimental setup, we used a version of libSVM that was modified to produce num-bers of distance to the margin of the SVM classi-fication.
We implemented the multi-label classi-fication by classifying each label separately where some sentences have the selected label and all others were counted as ?negative?
labels.
We used svm weights to handle the problem of imbalance in the negative examples.
After exper-imentation with different combinations of fea-tures, we used a set of features that was best overall for the codes: unigram tokens lowercased and filtered by stop words, bigrams, orthographic features from capitalization, the token count, and the role of the sender of the email.
For an initial sample, we randomly chose 3 positive and 3 negative examples from the de-velopment set to be the initial training set used for all experimental runs.
We carried out experi-ments with a number of sample sizes, b, ranging over 5, 10, 20, 40, 50, 60, 80 and 100 instances.
For experiments on methods used to select correction examples, we have chosen to experi-ment with sampling methods similar to those found in Lewis and Gale (1994) and Lewis (1995) using a random sampling method, wherea new sample is chosen randomly from the re-maining examples in the development set, a rele-vance sampling method, where a new sample is chosen as the b number of most likely labeled candidates in the development set with the larg-est distance from the margin of the SVM classi-fication, and an uncertainty sampling method, where a new sample is chosen as the b number of candidates in the region of uncertainty on either side of the margin of the SVM classification.
5 Preliminary Results In this simulation experiment, the pool size is quite small (3664 examples) compared to the large amount of unlabeled data that is normally available for active learning, and would be avail-able for our system under actual use.
We tested the active learning simulation on 8 codes.
There was no clear winning sampling strategy out of the 3 we used in the simulation experiment but random sampling (5 out of 8 codes) appeared to be the one that most often produced the highest F?2 score in the shortest number of iterations.
Figure 2 shows the F?2 score for each sampling strategy based on code ?Opinion/Preference?
using sample sizes 5 and 100 respectively.
As for sampling sizes, we did not observe a large difference in the evolution of the F?2 score between the various sample sizes, and the learn-ing curves in Figure 2, shown for the sample siz-es of 5 and 100, are typical.
This means that we should be able to use larger sample sizes for hu-man subject studies to achieve the same im-provements in performance as with the smaller sample sizes, and can carry out the experiments to relate the cost of human annotation with in-creases in performance.Figure 2.
Active ML performance for code Opinion/Preference.616 Conclusion and Future Work Our findings are inconclusive as we have yet to run the active learning simulations on all the codes.
However, preliminary results are directing us towards using larger sample sizes and then experimenting with random and uncertainty sampling in the human subject studies.
From our experiments with the different codes, we found the performance on less fre-quent codes to be problematic as it is difficult for the active learning system to identify potential positive examples to improve the models.
While the system performance may improve to handle such sparse cases, it may be better to modify the codebook instead.
We plan to give the user feed-back on the performance of the codes at each iteration of the active learning and support modi-fications to the codebook, for example, the user may wish to drop some codes or collapse them according to some hierarchy.
After all, if a code is not found in the text, it is hard to argue for its theoretical importance.
We are currently completing the design of the parameters of the active learning process for the human correction experiments on our pilot project with the codes about leadership in open source software groups.
We will also be testing and undergoing further development of the user interface for the annotators.
Our next step will be to test the system on other projects with other researchers.
We hope to gain more insight into what types of coding schemes and codes are easier to learn than oth-ers, and to be able to guide social scientists into developing coding schemes that are not only based on the social science theory but also useful in practice to develop an accurate classifier for very large amounts of digital text.
Acknowledgements:  This material is based upon work supported by the National Science Foundation under Grant No.
IIS-1111107.
Kevin Crowston is supported by the National Science Foundation.
Any opin-ions, findings, and conclusions or recommenda-tions expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.
The authors gratefully acknowledge helpful suggestions by the reviewers.Reference Broadwell, G. A., Stromer-Galley, J., Strzalkowski, T., Shaikh, S., Taylor, S., Liu, T., Boz, U., Elia, A., Jiao, L., & Webb, N. (2013).
Modeling sociocul-tural phenomena in discourse.
Natural Language Engineering, 19(02), 213?257.
Clancy, S., Bayer, S. and Kozierok, R. (2012)  ?Ac-tive Learning with a Human In The Loop,?
Mitre Corporation.
Crowston, K., Allen, E. E., & Heckman, R. (2012).
Using natural language processing technology for qualitative data analysis.
International Journal of Social Research Methodology, 15(6), 523?543.
Culotta, A. and McCallum, A.
(2005) ?Reducing La-beling Effort for Structured Prediction Tasks.?
Ishita, E., Oard, D. W., Fleischmann, K. R., Cheng, A.-S., & Templeton, T. C. (2010).
Investigating multi-label classification for human values.
Pro-ceedings of the American Society for Information Science and Technology, 47(1), 1?4.
Miles, M. B., & Huberman, A. M. (1994).
Qualitative data analysis: An expanded sourcebook.
Sage Pub-lications.
Lewis, D. D., & Gale, W. A.
(1994).
A sequential algorithm for training text classifiers.
In Proceed-ings of the 17th annual international ACM SIGIR conference on Research and development in infor-mation retrieval (pp.
3-12).
Lewis, D. D. (1995).
A sequential algorithm for train-ing text classifiers: Corrigendum and additional da-ta.
In ACM SIGIR Forum (Vol.
29, No.
2, pp.
13-19).
Mandel, M. I., Poliner, G. E., & Ellis, D. P. (2006).
Support vector machine active learning for music retrieval.
Multimedia systems, 12(1), 3-13.
Schohn, G., & Cohn, D. (2000).
Less is more: Active learning with support vector machines.
In Interna-tional Conference on Machine Learning (pp.
839-846).
Settles, B.
(2010).
Active learning literature survey.
University of Wisconsin, Madison, 52, 55-66.
Settles, B.
(2011).
Closing the loop: Fast, interactive semi-supervised annotation with queries on fea-tures and instances.
In Proceedings of the Confer-ence on Empirical Methods in Natural Language Processing (pp.
1467-1478).
Settles, B., & Zhu, X.
(2012).
Behavioral factors in interactive training of text classifiers.
In Proceed-ings of the 2012 Conference of the North American Chapter of the Association for Computational Lin-guistics: Human Language Technologies (pp.
563-567).
Tong, S., & Koller, D. (2002).
Support vector ma-chine active learning with applications to text clas-sification.
The Journal of Machine Learning Re-search, 2, 45-66.62
