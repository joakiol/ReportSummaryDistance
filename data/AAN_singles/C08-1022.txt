Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 169?176Manchester, August 2008A classifier-based approach to preposition and determiner errorcorrection in L2 EnglishRachele De Felice and Stephen G. PulmanOxford University Computing LaboratoryWolfson Building, Parks Road, Oxford OX1 3QD, UK{rachele.defelice|stephen.pulman}@comlab.ox.ac.ukAbstractIn this paper, we present an approach to theautomatic identification and correction ofpreposition and determiner errors in non-native (L2) English writing.
We show thatmodels of use for these parts of speechcan be learned with an accuracy of 70.06%and 92.15% respectively on L1 text, andpresent first results in an error detectiontask for L2 writing.1 IntroductionThe field of research in natural language process-ing (NLP) applications for L2 language is con-stantly growing.
This is largely driven by the ex-panding population of L2 English speakers, whosevarying levels of ability may require different typesof NLP tools from those designed primarily fornative speakers of the language.
These includeapplications for use by the individual and withininstructional contexts.
Among the key tools areerror-checking applications, focusing particularlyon areas which learners find the most challenging.Prepositions and determiners are known to be oneof the most frequent sources of error for L2 En-glish speakers, a finding supported by our analysisof a small error-tagged corpus we created (deter-miners 17% of errors, prepositions 12%).
There-fore, in developing a system for automatic errordetection in L2 writing, it seems desirable to focuson these problematic, and very common, parts ofspeech (POS).This paper gives a brief overview of the prob-lems posed by these POS and of related work.
Wec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.then present our proposed approach on both L1 andL2 data and discuss the results obtained so far.2 The problem2.1 PrepositionsPrepositions are challenging for learners becausethey can appear to have an idiosyncratic behaviourwhich does not follow any predictable pattern evenacross nearly identical contexts.
For example, wesay I study in Boston but I study at MIT; or He isindependent of his parents, but dependent on hisson.
As it is hard even for L1 speakers to articulatethe reasons for these differences, it is not surpris-ing that learners find it difficult to master preposi-tions.2.2 DeterminersDeterminers pose a somewhat different problemfrom prepositions as, unlike them, their choice ismore dependent on the wider discourse contextthan on individual lexical items.
The relation be-tween a noun and a determiner is less strict thanthat between a verb or noun and a preposition, themain factor in determiner choice being the specificproperties of the noun?s context.
For example, wecan say boys like sport or the boys like sport, de-pending on whether we are making a general state-ment about all boys or referring to a specific group.Equally, both she ate an apple and she ate the ap-ple are grammatically well-formed sentences, butonly one may be appropriate in a given context, de-pending on whether the apple has been mentionedpreviously.
Therefore, here, too, it is very hard tocome up with clear-cut rules predicting every pos-sible kind of occurrence.1693 Related workAlthough in the past there has been some researchon determiner choice in L1 for applications such asgeneration and machine translation output, work todate on automatic error detection in L2 writing hasbeen fairly limited.
Izumi et al (2004) train a max-imum entropy classifier to recognise various er-rors using contextual features.
They report resultsfor different error types (e.g.
omission - precision75.7%, recall 45.67%; replacement - P 31.17%,R 8%), but there is no break-down of results byindividual POS.
Han et al (2006) use a maxi-mum entropy classifier to detect determiner errors,achieving 83% accuracy.
Chodorow et al (2007)present an approach to preposition error detectionwhich also uses a model based on a maximum en-tropy classifier trained on a set of contextual fea-tures, together with a rule-based filter.
They report80% precision and 30% recall.
Finally, Gamon etal.
(2008) use a complex system including a deci-sion tree and a language model for both preposi-tion and determiner errors, while Yi et al (2008)propose a web count-based system to correct de-terminer errors (P 62%, R 41%).The work presented here displays some similar-ities to the papers mentioned above in its use of amaximum entropy classifier and a set of features.However, our feature set is more linguistically so-phisticated in that it relies on a full syntactic anal-ysis of the data.
It includes some semantic compo-nents which we believe play a role in correct classassignment.4 Contextual models for prepositions anddeterminers4.1 Feature setThe approach proposed in this paper is based onthe belief that although it is difficult to formulatehard and fast rules for correct preposition and de-terminer usage, there is enough underlying regu-larity of characteristic syntactic and semantic con-texts to be able to predict usage to an acceptabledegree of accuracy.
We use a corpus of grammat-ically correct English to train a maximum entropyclassifier on examples of correct usage.
The classi-fier can therefore learn to associate a given prepo-sition or determiner to particular contexts, and re-liably predict a class when presented with a novelinstance of a context for one or the other.The L1 source we use is the British NationalHead noun ?apple?Number singularNoun type countNamed entity?
noWordNet category food, plantPrep modification?
yes, ?on?Object of Prep?
noAdj modification?
yes, ?juicy?Adj grade superlativePOS ?3 VV, DT, JJS, IN, DT, NNTable 1: Determiner feature set for Pick the juiciestapple on the tree.POS modified verbLexical item modified ?drive?WordNet Category motionSubcat frame pp toPOS of object nounObject lexical item ?London?Named entity?
yes, type = locationPOS ?3 NNP, VBD, NNPGrammatical relation iobjTable 2: Preposition feature set for John drove toLondon.Corpus (BNC) as we believe this offers a represen-tative sample of different text types.
We representtraining and testing items as vectors of values forlinguistically motivated contextual features.
Ourfeature vectors include 18 feature categories fordeterminers and 13 for prepositions; the main onesare illustrated in Table 1 and Table 2 respectively.Further determiner features note whether the nounis modified by a predeterminer, possessive, nu-meral, and/or a relative clause, and whether it ispart of a ?there is.
.
.
?
phrase.
Additional preposi-tion features refer to the grade of any adjectives oradverbs modified (base, comparative, superlative)and to whether the items modified are modified bymore than one PP1.In De Felice and Pulman (2007), we describedsome of the preprocessing required and offeredsome motivation for this approach.
As for ourchoice of features, we aim to capture all the ele-ments of a sentence which we believe to have aneffect on preposition and determiner choice, andwhich can be easily extracted automatically - thisis a key consideration as all the features derivedrely on automatic processing of the text.
Grammat-ical relations refer to RASP-style grammatical re-lations between heads and complements in whichthe preposition occurs (see e.g.
(Briscoe et al,1A full discussion of each feature, including motivationfor its inclusion and an assessment of its contribution to themodel, is found in De Felice (forthcoming).170Author AccuracyBaseline 26.94%Gamon et al 08 64.93%Chodorow et al 07 69.00%Our model 70.06%Table 3: Classifier performance on L1 prepositions2006)).
Semantic word type information is takenfrom WordNet lexicographer classes, 40 broad se-mantic categories which all nouns and verbs inWordNet belong to2(e.g.
?verb of motion?, ?noundenoting food?
), while the POStags are from thePenn Treebank tagset - we note the POS of threewords either side of the target word3.
For eachoccurrence of a preposition or determiner in thecorpus, we obtain a feature vector consisting ofthe preposition or determiner and its context, de-scribed in terms of the features noted above.5 Acquiring the models5.1 PrepositionsAt the moment, we restrict our analysis to the ninemost frequent prepositions in the data: at, by, for,from, in, of, on, to, and with, to ensure a sufficientamount of data for training.
This gives a trainingdataset comprising 8,898,359 instances.
We usea standard maximum entropy classifier4and donot omit any features, although we plan to experi-ment with different feature combinations to deter-mine if, and how, this would impact the classifier?sperformance.
Before testing our model on learnerdata, it is important to ascertain that it can correctlyassociate prepositions to a given context in gram-matical, well-edited data.
We therefore tested themodel on a section of the BNC not used in train-ing, section J.
Our best result to date is 70.06%accuracy (test set size: 536,193).
Table 3 relatesour results to others reported in the literature oncomparable tasks.
The baseline refers to alwayschoosing the most frequent option, namely of.We can see that our model?s performance com-pares favourably to the best results in the literature,although direct comparisons are hard to draw sincedifferent groups train and test on different preposi-tion sets and on different types of data (British vs.American English, BNC vs. news reports, and so2No word sense disambiguation was performed at thisstage.3In NPs with a null determiner, the target is the head noun.4Developed by James Curran.Proportion of training data Precision Recallof 27.83% (2,501,327) 74.28% 90.47%to 20.64% (1,855,304) 85.99% 81.73%in 17.68% (1,589,718) 60.15% 67.60%for 8.01% (720,369) 55.47% 43.78%on 6.54% (587,871) 58.52% 45.81%with 6.03% (541,696) 58.13% 46.33%at 4.72% (424,539) 57.44% 52.12%by 4.69% (421,430) 63.83% 56.51%from 3.86% (347,105) 59.20% 32.07%Table 4: L1 results - individual prepositionson).
Furthermore, it should be noted that Gamonet al report more than one figure in their results,as there are two components to their model: onedetermining whether a preposition is needed, andthe other deciding what the preposition should be.The figure reported here refers to the latter task,as it is the most similar to the one we are evalu-ating.
Additionally, Chodorow et al also discusssome modifications to their model which can in-crease accuracy; the result noted here is the onemore directly comparable to our own approach.5.1.1 Further discussionTo fully assess the model?s performance on the L1data, it is important to consider factors such as per-formance on individual prepositions, the relation-ship between training dataset size and accuracy,and the kinds of errors made by the model.Table 4 shows the classifier?s performance on in-dividual prepositions together with the size of theirtraining datasets.
At first glance, a clear correlationappears between the amount of data seen in train-ing and precision and recall, as evidenced for ex-ample by of or to, for which the classifier achievesa very high score.
In other cases, however, the cor-relation is not so clear-cut.
For example by hasone of the smallest data sets in training but higherscores than many of the other prepositions, whilefor is notable for the opposite reason, namely hav-ing a large dataset but some of the lowest scores.The absence of a definite relation betweendataset size and performance suggests that theremight be a cline of ?learnability?
for these prepo-sitions: different prepositions?
contexts may bemore or less uniquely identifiable, or they mayhave more or fewer senses, leading to less confu-sion for the classifier.
One simple way of verify-ing the latter case is by looking at the number ofsenses assigned to the prepositions by a resource171Target prep Confused withat by for from in of on to withat xx 4.65% 10.82% 2.95% 36.83% 19.46% 9.17% 10.28% 5.85%by 6.54% xx 8.50% 2.58% 41.38% 19.44% 5.41% 10.04% 6.10%for 8.19% 3.93% xx 1.91% 25.67% 36.12% 5.60% 11.29% 7.28%from 6.19% 4.14% 6.72% xx 26.98% 26.74% 7.70% 16.45% 5.07%in 7.16% 9.28% 10.68% 3.01% xx 43.40% 10.92% 8.96% 6.59%of 3.95% 2.00% 18.81% 3.36% 40.21% xx 9.46% 14.77% 7.43%on 5.49% 3.85% 8.66% 2.29% 32.88% 27.92% xx 12.20% 6.71%to 9.77% 3.82% 11.49% 3.71% 24.86% 27.95% 9.43% xx 8.95%with 3.66% 4.43% 12.06% 2.24% 28.08% 26.63% 6.81% 16.10% xxTable 5: Confusion matrix for L1 data - prepositionssuch as the Oxford English Dictionary.
However,we find no good correlation between the two as thepreposition with the most senses is of (16), andthat with the fewest is from (1), thus negating theidea that fewer senses make a preposition easierto learn.
The reason may therefore be found else-where, e.g.
in the lexical properties of the contexts.A good picture of the model?s errors can behad by looking at the confusion matrix in Table 5,which reports, for each preposition, what the clas-sifier?s incorrect decision was.
Analysis of theseerrors may establish whether they are related to thedataset size issue noted above, or have a more lin-guistically grounded explanation.From the table, the frequency effect appears evi-dent: in almost every case, the three most frequentwrong choices are the three most frequent prepo-sitions, to, of, and in, although interestingly not inthat order, in usually being the first choice.
Con-versely, the less frequent prepositions are less of-ten suggested as the classifier?s choice.
This effectprecludes the possibility at the moment of draw-ing any linguistic conclusions.
These may only begleaned by looking at the errors for the three morefrequent prepositions.
We see for example thatthere seems to be a strong relation between of andfor, the cause of which is not immediately clear:perhaps they both often occur within noun phrases(e.g.
book of recipes, book for recipes).
More pre-dictable is the confusion between to and from, andbetween locative prepositions such as to and at, al-though the effect is less strong for other potentiallyconfusable pairs such as in and at or on.Table 6 gives some examples of instances wherethe classifier?s chosen preposition differs from thatfound in the original text.
In most cases, the clas-sifier?s suggestion is also grammatically correct,Classifier choice Correct phrasedemands of the sector demands for.
.
.condition for development condition of.
.
.travel to speed travel at.
.
.look at the USA look to.
.
.Table 6: Examples of classifier errors on preposi-tion L1 taskAuthor AccuracyBaseline 59.83%Han et al 06 83.00%Gamon et al 08 86.07%Turner and Charniak 07 86.74%Our model 92.15%Table 7: Classifier performance - L1 determinersbut the overall meaning of the phrases changessomewhat.
For example, while the demands ofthe sector are usually made by the sector itself,the demands for the sector suggest that someoneelse may be making them.
These are subtle dif-ferences which it may be impossible to capturewithout a more sophisticated understanding of thewider context.The example with travel, on the other hand,yields an ungrammatical result.
We assume thatthe classifier has acquired a very strong link be-tween the lexical item travel and the preposition tothat directs it towards this choice (cf.
also the ex-ample of look at/to).
This suggests that individuallexical items play an important role in prepositionchoice along with other more general syntactic andsemantic properties of the context.172%of training data Prec.
Recalla 9.61% (388,476) 70.52% 53.50%the 29.19% (1,180,435) 85.17% 91.51%null 61.20% (2,475,014) 98.63% 98.79%Table 8: L1 results - individual determiners5.2 DeterminersFor the determiner task, we also consider only thethree most frequent cases (a, the, null), whichgives us a training dataset consisting of 4,043,925instances.
We achieve accuracy of 92.15% on theL1 data (test set size: 305,264), as shown in Ta-ble 7.
Again, the baseline refers to the most fre-quent class, null.The best reported results to date on determinerselection are those in Turner and Charniak (2007).Our model outperforms their n-gram languagemodel approach by over 5%.
Since the two ap-proaches are not tested on the same data this com-parison is not conclusive, but we are optimistic thatthere is a real difference in accuracy since the typeof texts used are not dissimilar.
As in the case ofthe prepositions, it is interesting to see whether thishigh performance is equally distributed across thethree classes; this information is reported in Ta-ble 8.
Here we can see that there is a very strongcorrelation between amount of data seen in train-ing and precision and recall.
The indefinite arti-cle?s lower ?learnability?, and its lower frequencyappears not to be peculiar to our data, as it is alsofound by Gamon et al among others.The disparity in training is a reflection of the dis-tribution of determiners in the English language.Perhaps if this imbalance were addressed, themodel would more confidently learn contexts ofuse for a, too, which would be desirable in view ofusing this information for error correction.
On theother hand, this would create a distorted represen-tation of the composition of English, which maynot be what we want in a statistical model of lan-guage.
We plan to experiment with smaller scale,more similar datasets to ascertain whether the issueis one of training size or of inherent difficulty inlearning about the indefinite article?s occurrence.In looking at the confusion matrix for determin-ers (Table 9), it is interesting to note that for theclassifier?s mistakes involving a or the, the erro-neous choice is in the almost always the other de-terminer rather than the null case.
This suggeststhat the frequency effect is not so strong as to over-Target det Confused witha the nulla xx 92.92% 7.08%the 80.66% xx 19.34%null 14.51% 85.49% xxTable 9: Confusion matrix for L1 determinersride any true linguistic information the model hasacquired, otherwise the predominant choice wouldalways be the null case.
On the contrary, these re-sults show that the model is indeed capable of dis-tinguishing between contexts which require a de-terminer and those which do not, but requires fur-ther fine tuning to perform better in knowing whichof the two determiner options to choose.
Perhapsthe introduction of a discourse dimension mightassist in this respect.
We plan to experiment withsome simple heuristics: for example, given a se-quence ?Determiner Noun?, has the noun appearedin the preceding few sentences?
If so, we mightexpect the to be the correct choice rather than a.6 Testing the model6.1 Working with L2 textTo evaluate the model?s performance on learnerdata, we use a subsection of the CambridgeLearner Corpus (CLC)5.
We envisage our model toeventually be of assistance to learners in analysingtheir writing and identifying instances of preposi-tion or determiner usage which do not correspondto what it has been trained to expect; the moreprobable instance would be suggested as a moreappropriate alternative.
In using NLP tools andtechniques which have been developed with andfor L1 language, a loss of performance on L2 datais to be expected.
These methods usually expectgrammatically well-formed input; learner text isoften ungrammatical, misspelled, and different incontent and structure from typical L1 resourcessuch as the WSJ and the BNC.6.2 PrepositionsFor the preposition task, we extract 2523 instancesof preposition use from the CLC (1282 correct,1241 incorrect) and ask the classifier to mark them5The CLC is a computerised database of contemporarywritten learner English (currently over 25m words).
It wasdeveloped jointly by Cambridge ESOL and Cambridge Uni-versity Press.
The Cambridge Error Coding System has beendeveloped and applied manually to the data by CambridgeUniversity Press.173Instance type AccuracyCorrect 66.7%Incorrect 70%Table 10: Accuracy on L2 data - prepositions.
Ac-curacy on incorrect instances refers to the classifiersuccessfully identifying the preposition in the textas not appropriate for that context.as correct or incorrect.
The results from this taskare presented in Table 10.
These first results sug-gest that the model is fairly robust: the accuracyrate on the correct data, for example, is not muchlower than that on the L1 data.
In an applicationdesigned to assist learners, it is important to aimto reduce the rate of false alarms - cases where theoriginal is correct, but the model flags an error - toa minimum, so it is positive that this result is com-paratively high.
Accuracy on error identification isat first glance even more encouraging.
However, ifwe look at the suggestions the model makes to re-place the erroneous preposition, we find that theseare correct only 51.5% of the time, greatly reduc-ing its usefulness.6.2.1 Further discussionA first analysis of the classifier?s decisions and itserrors points to various factors which could be im-pairing its performance.
Spelling mistakes in theinput are one of the most immediate ones.
For ex-ample, in the sentence I?m Franch, responsable onthe computer services, the classifier is not able tosuggest a correct alternative to the erroneous on:since it does not recognise the adjective as a mis-spelling of responsible, it loses the information as-sociated with this lexical feature, which could po-tentially determine the preposition choice.A more complex problem arises when poorgrammar in the input misleads the parser so thatthe information it gives for a sentence is incor-rect, especially as regards PP attachment.
In thisexample, I wold like following equipment to myspeech: computer, modem socket and microphone,the missing the leads the parser to treat followingas a verb, and believes it to be the verb to which thepreposition is attached.
It therefore suggests fromas a correction, which is a reasonable choice giventhe frequency of phrases such as to follow from.However, this was not what the PP was meantto modify: impaired performance from the parsercould be a significant negative factor in the model?sperformance.
It would be interesting to test themodel on texts written by students of different lev-els of proficiency, as their grammar may be moreerror-free and more likely to be parsed correctly.Alternatively, we could modify the parser so as toskip cases where it requires several attempts beforeproducing a parse, as these more challenging casescould be indicative of very poorly structured sen-tences in which misused prepositions are depen-dent on more complex errors.A different kind of problem impacting our accu-racy scores derives from those instances where theclassifier selects a preposition which can be cor-rect in the given context, but is not the correct onein that particular case.
In the example I receiveda beautiful present at my birthday, the classifieridentifies the presence of the error, and suggeststhe grammatically and pragmatically appropriatecorrection for.
The corpus annotators, however,indicate on as the correct choice.
Since we usetheir annotations as the benchmark against whichto evaluate the model, this instance is counted asthe classifier being wrong because it disagrees withthe annotators.
A better indication of the model?sperformance may be to independently judge its de-cisions, to avoid being subject to the annotators?bias.
Finally, we are beginning to look at the rela-tions between preposition errors and other types oferror such as verb choice, and how these are anno-tated in the data.An overview of the classifier?s error patterns forthe data in this task shows that they are largely sim-ilar to those observed in the L1 data.
This sug-gests that the gap in performance between L1 andL2 is due more to the challenges posed by learnertext than by inherent shortcomings in the model,and therefore that the key to better performanceis likely to lie in overcoming these problems.
Infuture work we plan to use L2 data where someof the spelling errors and non-preposition or deter-miner errors have been corrected so that we cansee which of the other errors are worth focussingon first.6.3 DeterminersOur work on determiner error correction is still inthe early stages.
We follow a similar procedure tothe prepositions task, selecting a number of bothcorrect and incorrect instances.
On the former (setsize 2000) accuracy is comparable to that on L1data: 92.2%.
The danger of false alarms, then, ap-pears not to be as significant as for the prepositions174task.
On the incorrect instances (set size ca.
1200),however, accuracy is less than 10%.Preliminary error analysis shows that the modelis successful at identifying cases of misused deter-miner, e.g.
a for the or vice versa, doing so in overtwo-thirds of cases.
However, by far the most fre-quent error type for determiners is not confusionbetween indefinite and definite article, but omittingan article where one is needed.
At the moment, themodel detects very few of these errors, no doubt in-fluenced by the preponderance of null cases seenin training.
Furthermore, some of the issues raisedearlier in discussing the application of NLP toolsto L2 language hold for this task, too.In addition to those, though, in this task morethan for prepositions we believe that differences intext type between the training texts - the BNC -and the testing material - learner essays - has a sig-nificant negative effect on the model.
In this task,the lexical items play a crucial role in class assign-ment.
If the noun in question has not been seen intraining, the classifier may be unable to make aninformed choice.
Although the BNC comprises awide variety of texts, there may not be a sufficientnumber covering topics typical of learner essays,such as ?business letters?
or ?postcards to penpals?.Also, the BNC was created with material from al-most 20 years ago, and learners writing in contem-porary English may use lexical items which are notvery frequently seen in the BNC.
A clear exam-ple of this discrepancy is the noun internet, whichrequires the definite article in English, but not inseveral other languages, leading to countless sen-tences such as I saw it in internet, I booked it oninternet, and so on.
This is one of the errors themodel never detects: a fact which is not surpris-ing when we consider that this noun occurs onlyfour times in the whole of the training data.
It maybe therefore necessary to consider using alternativesources of training data to overcome this problemand improve the classifier?s performance.7 Comparison to human learnersIn developing this model, our first aim was not tocreate something which learns like a human, butsomething that works in the best and most effi-cient possible way.
However, it is interesting tosee whether human learners and classifiers displaysimilar patterns of errors in preposition choice.This information has twofold value: as well as be-ing of pedagogical assistance to instructors of En-glish L2, were the classifier to display student-likeerror patterns, insights into ?error triggers?
couldbe derived from the L2 pedagogical literature toimprove the classifier.
The analysis of the typesof errors made by human learners yields some in-sights which might be worthy of further investi-gation.
A clear one is the confusion between thethree locative and temporal prepositions at, in, andon (typical sentence: The training programme willstart at the 1st August).
This type of error is madeoften by both learners and the model on both typesof data, suggesting that perhaps further attentionto features might be necessary to improve discrim-ination between these three prepositions.There are also interesting divergences.
For ex-ample, a common source of confusion in learnersis between by and from, as in I like it becauseit?s from my favourite band.
However, this confu-sion is not very frequent in the model, a differencewhich could be explained either by the fact that,as noted above, performance on from is very lowand so the classifier is unlikely to suggest it, or thatin training the contexts seen for by are sufficientlydistinctive that the classifier is not misled like thelearners.Finally, a surprising difference comes fromlooking at what to is confused with.
The modeloften suggests at where to would be correct.
Thisis perhaps not entirely unusual as both can occurwith locative complements (one can go to a placeor be at a place) and this similarity could be con-fusing the classifier.
Learners, however, althoughthey do make this kind of mistake, are much morehampered by the confusion between for and to, asin She was helpful for me or This is interestingfor you.
In other words, for learners it seems thatthe abstract use of this preposition, its benefactivesense, is much more problematic than the spatialsense.
We can hypothesise that the classifier is lessdistracted by these cases because the effect of thelexical features is stronger.A more detailed discussion of the issues arisingfrom the comparison of confusion pairs cannot behad here.
However, in noting both divergences andsimilarities between the two learners, human andmachine, we may be able to derive useful insightsinto the way the learning processes operate, andwhat factors could be more or less important forthem.1758 Conclusions and future directionsThis paper discussed a contextual feature basedapproach to the automatic acquisition of modelsof use for prepositions and determiners, whichachieve an accuracy of 70.06% and 92.15% re-spectively, and showed how it can be applied to anerror correction task for L2 writing, with promis-ing early results.
There are several directions thatcan be pursued to improve accuracy on both typesof data.
The classifier can be further fine-tuned toacquire more reliable models of use for the twoPOS.
We can also experiment with its confidencethresholds, for example allowing it to make an-other suggestion when its confidence in its firstchoice is low.
Furthermore, issues relating to theuse of NLP tools with L2 data must be addressed,such as factoring out spelling or other errors in thedata, and perhaps training on text types which aremore similar to the CLC.
In the longer term, wealso envisage mining the information implicit inour training data to create a lexical resource de-scribing the statistical tendencies observed.AcknowledgementsWe wish to thank Stephen Clark and Laura Rimell for stim-ulating discussions and the anonymous reviewers for theirhelpful comments.
We acknowledge Cambridge UniversityPress?s assistance in accessing the Cambridge Learner Corpusdata.
Rachele De Felice was supported by an AHRC scholar-ship for the duration of her studies.ReferencesBriscoe, Ted, John Carroll, and Rebecca Watson.2006.
The second release of the RASP system.
InCOLING-ACL 06 Demo Session.Chodorow, Martin, Joel Tetreault, and Na-Rae Han.2007.
Detection of grammatical errors involv-ing prepositions.
In Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions.De Felice, Rachele and Stephen Pulman.
2007.
Au-tomatically acquiring models of preposition use.
InProceedings of the 4th ACL-SIGSEM Workshop onPrepositions.De Felice, Rachele.
forthcoming.
Recognising prepo-sition and determiner errors in learner English.Ph.D.
thesis, Oxford University Computing Labora-tory.Gamon, M., J. Gao, C. Brockett, A. Klementiev,W.
Dolan, D. Belenko, and L. Vanderwende.
2008.Using contextual speller techniques and languagemodeling for ESL error correction.
In Proceedingsof IJCNLP.Han, Na-Rae, Martin Chodorow, and Claudia Leacock.2006.
Detecting errors in English article usage bynon-native speakers.
Natural Language Engineer-ing, 12(1):115?129.Izumi, Emi, Kiyotaka Uchimoto, and Hitoshi Isahara.2004.
SST speech corpus of Japanese learners?English and automatic detection of learners?
errors.ICAME, 28:31?48.Turner, Jenine and Eugen Charniak.
2007.
Languagemodeling for determiner selection.
In NAACL-HLTCompanion volume.Yi, Xing, Jianfeng Gao, and William Dolan.
2008.
Aweb-based English proofing system for ESL users.In Proceedings of IJCNLP.176
