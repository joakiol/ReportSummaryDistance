Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 63?72,Dublin, Ireland, August 23-24, 2014.SemEval 2014 Task 8:Broad-Coverage Semantic Dependency ParsingStephan Oepen?
?, Marco Kuhlmann?, Yusuke Miyao?, Daniel Zeman?,Dan Flickinger?, Jan Haji?c?, Angelina Ivanova?, and Yi Zhang?
?University of Oslo, Department of Informatics?Potsdam University, Department of Linguistics?Link?ping University, Department of Computer and Information Science?National Institute of Informatics, Tokyo?Charles University in Prague, Faculty of Mathematics and Physics, Institute of Formal and Applied Linguistics?Stanford University, Center for the Study of Language and Information?Nuance Communications Aachen GmbHsdp-organizers@emmtee.netAbstractTask 8 at SemEval 2014 defines Broad-Coverage Semantic Dependency Pars-ing (SDP) as the problem of recoveringsentence-internal predicate?argument rela-tionships for all content words, i.e.
the se-mantic structure constituting the relationalcore of sentence meaning.
In this taskdescription, we position the problem incomparison to other sub-tasks in compu-tational language analysis, introduce the se-mantic dependency target representationsused, reflect on high-level commonalitiesand differences between these representa-tions, and summarize the task setup, partic-ipating systems, and main results.1 Background and MotivationSyntactic dependency parsing has seen great ad-vances in the past decade, in part owing to rela-tively broad consensus on target representations,and in part reflecting the successful execution of aseries of shared tasks at the annual Conference forNatural Language Learning (CoNLL; Buchholz &Marsi, 2006; Nivre et al., 2007; inter alios).
Fromthis very active research area accurate and efficientsyntactic parsers have developed for a wide rangeof natural languages.
However, the predominantdata structure in dependency parsing to date aretrees, in the formal sense that every node in the de-pendency graph is reachable from a distinguishedroot node by exactly one directed path.This work is licenced under a Creative Commons At-tribution 4.0 International License.
Page numbers and theproceedings footer are added by the organizers: http://creativecommons.org/licenses/by/4.0/.Unfortunately, tree-oriented parsers are ill-suitedfor producing meaning representations, i.e.
mov-ing from the analysis of grammatical structure tosentence semantics.
Even if syntactic parsing ar-guably can be limited to tree structures, this is notthe case in semantic analysis, where a node willoften be the argument of multiple predicates (i.e.have more than one incoming arc), and it will oftenbe desirable to leave nodes corresponding to se-mantically vacuous word classes unattached (withno incoming arcs).Thus, Task 8 at SemEval 2014, Broad-CoverageSemantic Dependency Parsing (SDP 2014),1seeksto stimulate the dependency parsing communityto move towards more general graph processing,to thus enable a more direct analysis of Who didWhat to Whom?
For English, there exist severalindependent annotations of sentence meaning overthe venerable Wall Street Journal (WSJ) text of thePenn Treebank (PTB; Marcus et al., 1993).
Theseresources constitute parallel semantic annotationsover the same common text, but to date they havenot been related to each other and, in fact, havehardly been applied for training and testing of data-driven parsers.
In this task, we have used threedifferent such target representations for bi-lexicalsemantic dependencies, as demonstrated in Figure 1below for the WSJ sentence:(1) A similar technique is almost impossible to apply toother crops, such as cotton, soybeans, and rice.Semantically, technique arguably is dependent onthe determiner (the quantificational locus), the mod-ifier similar, and the predicate apply.
Conversely,the predicative copula, infinitival to, and the vac-1See http://alt.qcri.org/semeval2014/task8/ for further technical details, information on how toobtain the data, and official results.63A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .A1 A2(a) Partial semantic dependencies in PropBank and NomBank.A similar technique is almost impossible to apply to other crops, such as cotton, soybeans and rice.topARG2 ARG3 ARG1ARG2mwe _and_cARG1ARG1BVARG1 implicit_conjARG1(b) DELPH-IN Minimal Recursion Semantics?derived bi-lexical dependencies (DM).A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and ricetopARG1ARG2ARG1ARG2ARG2ARG1ARG1 ARG1 ARG1ARG1ARG1ARG2ARG1ARG2ARG1ARG2ARG1 ARG1 ARG1 ARG2(c) Enju Predicate?Argument Structures (PAS).A similar technique is almost impossible to apply to other crops , such as cotton , soybeans and rice .RSTRPATEXTPATACTRSTRADDRADDRADDRADDRAPPS.mAPPS.mCONJ.mCONJ.m CONJ.mtop(d) Parts of the tectogrammatical layer of the Prague Czech-English Dependency Treebank (PCEDT).Figure 1: Sample semantic dependency graphs for Example (1).uous preposition marking the deep object of ap-ply can be argued to not have a semantic contri-bution of their own.
Besides calling for node re-entrancies and partial connectivity, semantic depen-dency graphs may also exhibit higher degrees ofnon-projectivity than is typical of syntactic depen-dency trees.In addition to its relation to syntactic dependencyparsing, the task also has some overlap with Se-mantic Role Labeling (SRL; Gildea & Jurafsky,2002).
In much previous work, however, targetrepresentations typically draw on resources likePropBank and NomBank (Palmer et al., 2005; Mey-ers et al., 2004), which are limited to argumentidentification and labeling for verbal and nominalpredicates.
A plethora of semantic phenomena?for example negation and other scopal embedding,comparatives, possessives, various types of modi-fication, and even conjunction?typically remainunanalyzed in SRL.
Thus, its target representationsare partial to a degree that can prohibit seman-tic downstream processing, for example inference-based techniques.
In contrast, we require parsersto identify all semantic dependencies, i.e.
computea representation that integrates all content words inone structure.
Another difference to common inter-pretations of SRL is that the SDP 2014 task defini-tion does not encompass predicate disambiguation,a design decision in part owed to our goal to focuson parsing-oriented, i.e.
structural, analysis, and inpart to lacking consensus on sense inventories forall content words.Finally, a third closely related area of much cur-rent interest is often dubbed ?semantic parsing?,which Kate and Wong (2010) define as ?the task ofmapping natural language sentences into completeformal meaning representations which a computercan execute for some domain-specific application.
?In contrast to most work in this tradition, our SDPtarget representations aim to be task- and domain-independent, though at least part of this general-ity comes at the expense of ?completeness?
in theabove sense; i.e.
there are aspects of sentence mean-ing that arguably remain implicit.2 Target RepresentationsWe use three distinct target representations for se-mantic dependencies.
As is evident in our run-ning example (Figure 1), showing what are calledthe DM, PAS, and PCEDT semantic dependencies,there are contentful differences among these anno-tations, and there is of course not one obvious (oreven objective) truth.
In the following paragraphs,64we provide some background on the ?pedigree?
andlinguistic characterization of these representations.DM: DELPH-IN MRS-Derived Bi-Lexical De-pendencies These semantic dependency graphsoriginate in a manual re-annotation of Sections 00?21 of the WSJ Corpus with syntactico-semanticanalyses derived from the LinGO English Re-source Grammar (ERG; Flickinger, 2000).
Amongother layers of linguistic annotation, this resource?dubbed DeepBank by Flickinger et al.
(2012)?includes underspecified logical-form meaning rep-resentations in the framework of Minimal Recur-sion Semantics (MRS; Copestake et al., 2005).Our DM target representations are derived througha two-step ?lossy?
conversion of MRSs, first tovariable-free Elementary Dependency Structures(EDS; Oepen & L?nning, 2006), then to ?pure?bi-lexical form?projecting some construction se-mantics onto word-to-word dependencies (Ivanovaet al., 2012).
In preparing our gold-standardDM graphs from DeepBank, the same conversionpipeline was used as in the system submission ofMiyao et al.
(2014).
For this target representa-tion, top nodes designate the highest-scoping (non-quantifier) predicate in the graph, e.g.
the (scopal)degree adverb almost in Figure 1.2PAS: Enju Predicate-Argument StructuresThe Enju parsing system is an HPSG-based parserfor English.3The grammar and the disambigua-tion model of this parser are derived from the EnjuHPSG treebank, which is automatically convertedfrom the phrase structure and predicate?argumentstructure annotations of the PTB.
The PAS dataset is extracted from the WSJ portion of the EnjuHPSG treebank.
While the Enju treebank is an-notated with full HPSG-style structures, only itspredicate?argument structures are converted intothe SDP data format for use in this task.
Topnodes in this representation denote semantic heads.Again, the system description of Miyao et al.
(2014)provides more technical detail on the conversion.PCEDT: Prague Tectogrammatical Bi-LexicalDependencies The Prague Czech-English De-pendency Treebank (PCEDT; Haji?c et al., 2012)4is a set of parallel dependency trees over the WSJ2Note, however, that non-scopal adverbs act as mere in-tersective modifiers, e.g.
loudly is a predicate in DM, but themain verb provides the top node in structures like Abramssang loudly.3See http://kmcs.nii.ac.jp/enju/.4See http://ufal.mff.cuni.cz/pcedt2.0/.id form lemma pos top pred arg1 arg2#202000021 Ms. Ms. NNP ?
+ _ _2 Haag Haag NNP ?
?
compound ARG13 plays play VBZ + + _ _4 Elianti Elianti NNP ?
?
_ ARG25 .
.
.
?
?
_ _Table 1: Tabular SDP data format (showing DM).texts from the PTB, and their Czech translations.Similarly to other treebanks in the Prague family,there are two layers of syntactic annotation: an-alytical (a-trees) and tectogrammatical (t-trees).PCEDT bi-lexical dependencies in this task havebeen extracted from the t-trees.
The specifics ofthe PCEDT representations are best observed in theprocedure that converts the original PCEDT data tothe SDP data format; see Miyao et al.
(2014).
Topnodes are derived from t-tree roots; i.e.
they mostlycorrespond to main verbs.
In case of coordinateclauses, there are multiple top nodes per sentence.3 Graph RepresentationThe SDP target representations can be character-ized as labeled, directed graphs.
Formally, a se-mantic dependency graph for a sentence x =x1, .
.
.
, xnis a structure G = (V,E, `V, `E) whereV = {1, .
.
.
, n} is a set of nodes (which are inone-to-one correspondence with the tokens of thesentence); E ?
V ?
V is a set of edges; and `Vand `Eare mappings that assign labels (from somefinite alphabet) to nodes and edges, respectively.More specifically for this task, the label `V(i) of anode i is a tuple consisting of four components: itsword form, lemma, part of speech, and a Booleanflag indicating whether the corresponding tokenrepresents a top predicate for the specific sentence.The label `E(i?
j) of an edge i?
j is a seman-tic relation that holds between i and j.
The exactdefinition of what constitutes a top node and whatsemantic relations are available differs among ourthree target representations, but note that top nodescan have incoming edges.All data provided for the task uses a column-based file format (dubbed the SDP data format)similar to the one of the 2009 CoNLL Shared Task(Haji?c et al., 2009).
As in that task, we assume gold-standard sentence and token segmentation.
Forease of reference, each sentence is prefixed by aline with just a unique identifier, using the scheme2SSDDIII, with a constant leading 2, two-digit sec-tion code, two-digit document code (within each65section), and three-digit item number (within eachdocument).
For example, identifier 20200002 de-notes the second sentence in the first file of PTBSection 02, the classic Ms. Haag plays Elianti.
Theannotation of this sentence is shown in Table 1.With one exception, our fields (i.e.
columns inthe tab-separated matrix) are a subset of the CoNLL2009 inventory: (1) id, (2) form, (3) lemma, and(4) pos characterize the current token, with tokenidentifiers starting from 1 within each sentence.
Be-sides the lemma and part-of-speech information, inthe closed track of our task, there is no explicitanalysis of syntax.
Across the three target represen-tations in the task, fields (1) and (2) are aligned anduniform, i.e.
all representations annotate exactlythe same text.
On the other hand, fields (3) and (4)are representation-specific, i.e.
there are differentconventions for lemmatization, and part-of-speechassignments can vary (but all representations usethe same PTB inventory of PoS tags).The bi-lexical semantic dependency graph overtokens is represented by two or more columns start-ing with the obligatory, binary-valued fields (5)top and (6) pred.
A positive value in the topcolumn indicates that the node corresponding tothis token is a top node (see Section 2 below).
Thepred column is a simplification of the correspond-ing field in earlier tasks, indicating whether or notthis token represents a predicate, i.e.
a node withoutgoing dependency edges.
With these minor dif-ferences to the CoNLL tradition, our file format canrepresent general, directed graphs, with designatedtop nodes.
For example, there can be singletonnodes not connected to other parts of the graph,and in principle there can be multiple tops, or anon-predicate top node.To designate predicate?argument relations, thereare as many additional columns as there are pred-icates in the graph (i.e.
tokens marked + in thepred column); these additional columns are called(7) arg1, (8) arg2, etc.
These colums containargument roles relative to the i-th predicate, i.e.
anon-empty value in column arg1 indicates thatthe current token is an argument of the (linearly)first predicate in the sentence.
In this format, graphreentrancies will lead to a token receiving argumentroles for multiple predicates (i.e.
non-empty argivalues in the same row).
All tokens of the same sen-tence must always have all argument columns filledin, even on non-predicate words; in other words,all lines making up one block of tokens will havethe same number n of fields, but n can differ acrossDM PAS PCEDT(1) # labels 51 42 68(2) % singletons 22.62 4.49 35.79(3) # edge density 0.96 1.02 0.99(4) %gtrees 2.35 1.30 56.58(5) %gprojective 3.05 1.71 53.29(6) %gfragmented 6.71 0.23 0.56(7) %nreentrancies 27.35 29.40 9.27(8) %gtopless 0.28 0.02 0.00(9) # top nodes 0.9972 0.9998 1.1237(10) %nnon-top roots 44.71 55.92 4.36Table 2: Contrastive high-level graph statistics.sentences, depending on the count of graph nodes.4 Data SetsAll three target representations are annotations ofthe same text, Sections 00?21 of the WSJ Cor-pus.
For this task, we have synchronized theseresources at the sentence and tokenization levelsand excluded from the SDP 2014 training and test-ing data any sentences for which (a) one or more ofthe treebanks lacked a gold-standard analysis; (b) aone-to-one alignment of tokens could not be estab-lished across all three representations; or (c) at leastone of the graphs was cyclic.
Of the 43,746 sen-tences in these 22 first sections of WSJ text, Deep-Bank lacks analyses for close to 15%, and the EnjuTreebank has gaps for a little more than four per-cent.
Some 500 sentences show tokenization mis-matches, most owing to DeepBank correcting PTBidiosyncrasies like ?G.m.b, H.?, ?S.p, A.?, and?U.S., .
?, and introducing a few new ones (Fareset al., 2013).
Finally, 232 of the graphs obtainedthrough the above conversions were cyclic.
In total,we were left with 34,004 sentences (or 745,543tokens) as training data (Sections 00?20), and 1348testing sentences (29,808 tokens), from Section 21.Quantitative Comparison As a first attempt atcontrasting our three target representations, Table 2shows some high-level statistics of the graphs com-prising the training data.5In terms of distinctions5These statistics are obtained using the ?official?
SDPtoolkit.
We refer to nodes that have neither incoming noroutgoing edges and are not marked as top nodes as singletons;these nodes are ignored in subsequent statistics, e.g.
whendetermining the proportion of edges per node (3) or the per-centages of rooted trees (4) and fragmented graphs (6).
Thenotation ?%n?
denotes (non-singleton) node percentages, and?%g?
percentages over all graphs.
We consider a root node any(non-singleton) node that has no incoming edges; reentrantnodes have at least two incoming edges.
Following Sagae andTsujii (2008), we consider a graph projective when there areno crossing edges (in a left-to-right rendering of nodes) and noroots are ?covered?, i.e.
for any root j there is no edge i?
k66Directed UndirectedDM PAS PCEDT DM PAS PCEDTDM ?
.6425 .2612 ?
.6719 .5675PAS .6688 ?
.2963 .6993 ?
.5490PCEDT .2636 .2963 ?
.5743 .5630 ?Table 3: Pairwise F1similarities, including punctu-ation (upper right diagonals) or not (lower left).drawn in dependency labels (1), there are clear dif-ferences between the representations, with PCEDTappearing linguistically most fine-grained, and PASshowing the smallest label inventory.
Unattachedsingleton nodes (2) in our setup correspond totokens analyzed as semantically vacuous, which(as seen in Figure 1) include most punctuationmarks in PCEDT and DM, but not PAS.
Further-more, PCEDT (unlike the other two) analyzes somehigh-frequency determiners as semantically vacu-ous.
Conversely, PAS on average has more edgesper (non-singleton) nodes than the other two (3),which likely reflects its approach to the analysis offunctional words (see below).Judging from both the percentage of actual trees(4), the proportions of projective graphs (5), and theproportions of reentrant nodes (7), PCEDT is muchmore ?tree-oriented?
than the other two, which atleast in part reflects its approach to the analysisof modifiers and determiners (again, see below).We view the small percentages of graphs withoutat least one top node (8) and of graphs with atleast two non-singleton components that are notinterconnected (6) as tentative indicators of generalwell-formedness.
Intuitively, there should alwaysbe a ?top?
predicate, and the whole graph should?hang together?.
Only DM exhibits non-trivial (ifsmall) degrees of topless and fragmented graphs,and these may indicate imperfections in the Deep-Bank annotations or room for improvement in theconversion from full MRSs to bi-lexical dependen-cies, but possibly also exceptions to our intuitionsabout semantic dependency graphs.Finally, in Table 3 we seek to quantify pairwisestructural similarity between the three representa-tions in terms of unlabeled dependency F1(dubbedUF in Section 5 below).
We provide four variantsof this metric, (a) taking into account the direc-tionality of edges or not and (b) including edgesinvolving punctuation marks or not.
On this view,DM and PAS are structurally much closer to eachother than either of the two is to PCEDT, even moresuch that i < j < k.so when discarding punctuation.
While relaxingthe comparison to ignore edge directionality alsoincreases similarity scores for this pair, the effectis much more pronounced when comparing eitherto PCEDT.
This suggests that directionality of se-mantic dependencies is a major source of diversionbetween DM and PAS on the one hand, and PCEDTon the other hand.Linguistic Comparison Among other aspects,Ivanova et al.
(2012) categorize a range of syntac-tic and semantic dependency annotation schemesaccording to the role that functional elements take.In Figure 1 and the discussion of Table 2 above, wealready observed that PAS differs from the otherrepresentations in integrating into the graph aux-iliaries, the infinitival marker, the case-markingpreposition introducing the argument of apply (to),and most punctuation marks;6while these (andother functional elements, e.g.
complementizers)are analyzed as semantically vacuous in DM andPCEDT, they function as predicates in PAS, thoughdo not always serve as ?local?
top nodes (i.e.
the se-mantic head of the corresponding sub-graph): Forexample, the infinitival marker in Figure 1 takes theverb as its argument, but the ?upstairs?
predicateimpossible links directly to the verb, rather than tothe infinitival marker as an intermediate.At the same time, DM and PAS pattern alikein their approach to modifiers, e.g.
attributive ad-jectives, adverbs, and prepositional phrases.
Un-like in PCEDT (or common syntactic dependencyschemes), these are analyzed as semantic predi-cates and, thus, contribute to higher degrees ofnode reentrancy and non-top (structural) roots.Roughly the same holds for determiners, but hereour PCEDT projection of Prague tectogrammaticaltrees onto bi-lexical dependencies leaves ?vanilla?articles (like a and the) as singleton nodes.The analysis of coordination is distinct in thethree representations, as also evident in Figure 1.By design, DM opts for what is often calledthe Mel?
?cukian analysis of coordinate structures(Mel?
?cuk, 1988), with a chain of dependenciesrooted at the first conjunct (which is thus consid-ered the head, ?standing in?
for the structure atlarge); in the DM approach, coordinating conjunc-tions are not integrated with the graph but rathercontribute different types of dependencies.
In PAS,the final coordinating conjunction is the head of the6In all formats, punctuation marks like dashes, colons, andsometimes commas can be contentful, i.e.
at times occur asboth predicates, arguments, and top nodes.67employee stock investment planscompound compound compoundemployee stock investment plansARG1ARG1ARG1employee stock investment plansACTPAT REGFigure 2: Analysis of nominal compounding in DM, PAS, and PCEDT, respectively .structure and each coordinating conjunction (or in-tervening punctuation mark that acts like one) is atwo-place predicate, taking left and right conjunctsas its arguments.
Conversely, in PCEDT the lastcoordinating conjunction takes all conjuncts as itsarguments (in case there is no overt conjunction, apunctuation mark is used instead); additional con-junctions or punctuation marks are not connectedto the graph.7A linguistic difference between our representa-tions that highlights variable granularities of anal-ysis and, relatedly, diverging views on the scopeof the problem can be observed in Figure 2.
Muchnoun phrase?internal structure is not made explicitin the PTB, and the Enju Treebank from whichour PAS representation derives predates the brack-eting work of Vadas and Curran (2007).
In thefour-way nominal compounding example of Fig-ure 2, thus, PAS arrives at a strictly left-branchingtree, and there is no attempt at interpreting seman-tic roles among the members of the compound ei-ther; PCEDT, on the other hand, annotates both theactual compound-internal bracketing and the as-signment of roles, e.g.
making stock the PAT(ient)of investment.
In this spirit, the PCEDT annota-tions could be directly paraphrased along the linesof plans by employees for investment in stocks.
Ina middle position between the other two, DM dis-ambiguates the bracketing but, by design, merelyassigns an underspecified, construction-specific de-pendency type; its compound dependency, then,is to be interpreted as the most general type of de-pendency that can hold between the elements ofthis construction (i.e.
to a first approximation eitheran argument role or a relation parallel to a prepo-sition, as in the above paraphrase).
The DM andPCEDT annotations of this specific example hap-pen to diverge in their bracketing decisions, wherethe DM analysis corresponds to [...] investmentsin stock for employees, i.e.
grouping the concept7As detailed by Miyao et al.
(2014), individual con-juncts can be (and usually are) arguments of other predicates,whereas the topmost conjunction only has incoming edges innested coordinate structures.
Similarly, a ?shared?
modifier ofthe coordinate structure as a whole would take as its argumentthe local top node of the coordination in DM or PAS (i.e.
thefirst conjunct or final conjunction, respectively), whereas itwould depend as an argument on all conjuncts in PCEDT.employee stock (in contrast to ?common stock?
).Without context and expert knowledge, these de-cisions are hard to call, and indeed there has beenmuch previous work seeking to identify and anno-tate the relations that hold between members of anominal compound (see Nakov, 2013, for a recentoverview).
To what degree the bracketing and roledisambiguation in this example are determined bythe linguistic signal (rather than by context andworld knowledge, say) can be debated, and thus theobserved differences among our representations inthis example relate to the classic contrast between?sentence?
(or ?conventional?)
meaning, on the onehand, and ?speaker?
(or ?occasion?)
meaning, onthe other hand (Quine, 1960; Grice, 1968).
Inturn, we acknowledge different plausible points ofview about which level of semantic representationshould be the target representation for data-drivenparsing (i.e.
structural analysis guided by the gram-matical system), and which refinements like theabove could be construed as part of a subsequenttask of interpretation.5 Task SetupTraining data for the task, providing all columns inthe file format sketched in Section 3 above, togetherwith a first version of the SDP toolkit?includinggraph input, basic statistics, and scoring?werereleased to candidate participants in early Decem-ber 2013.
In mid-January, a minor update to thetraining data and optional syntactic ?companion?analyses (see below) were provided, and in earlyFebruary the description and evaluation of a sim-ple baseline system (using tree approximations andthe parser of Bohnet, 2010).
Towards the end ofMarch, an input-only version of the test data wasreleased, with just columns (1) to (4) pre-filled; par-ticipants then had one week to run their systems onthese inputs, fill in columns (5), (6), and upwards,and submit their results (from up to two differentruns) for scoring.
Upon completion of the testingphase, we have shared the gold-standard test data,official scores, and system results for all submis-sions with participants and are currently preparingall data for general release through the LinguisticData Consortium.68DM PAS PCEDTLF LP LR LF LM LP LR LF LM LP LR LF LMPeking 85.91 90.27 88.54 89.40 26.71 93.44 90.69 92.04 38.13 78.75 73.96 76.28 11.05Priberam 85.24 88.82 87.35 88.08 22.40 91.95 89.92 90.93 32.64 78.80 74.70 76.70 09.42Copenhagen-80.77 84.78 84.04 84.41 20.33 87.69 88.37 88.03 10.16 71.15 68.65 69.88 08.01Malm?Potsdam 77.34 79.36 79.34 79.35 07.57 88.15 81.60 84.75 06.53 69.68 66.25 67.92 05.19Alpage 76.76 79.42 77.24 78.32 09.72 85.65 82.71 84.16 17.95 70.53 65.28 67.81 06.82Link?ping 72.20 78.54 78.05 78.29 06.08 76.16 75.55 75.85 01.19 60.66 64.35 62.45 04.01DM PAS PCEDTLF LP LR LF LM LP LR LF LM LP LR LF LMPriberam 86.27 90.23 88.11 89.16 26.85 92.56 90.97 91.76 37.83 80.14 75.79 77.90 10.68CMU 82.42 84.46 83.48 83.97 08.75 90.78 88.51 89.63 26.04 76.81 70.72 73.64 07.12Turku 80.49 80.94 82.14 81.53 08.23 87.33 87.76 87.54 17.21 72.42 72.37 72.40 06.82Potsdam 78.60 81.32 80.91 81.11 09.05 89.41 82.61 85.88 07.49 70.35 67.33 68.80 05.42Alpage 78.54 83.46 79.55 81.46 10.76 87.23 82.82 84.97 15.43 70.98 67.51 69.20 06.60In-House 75.89 92.58 92.34 92.46 48.07 92.09 92.02 92.06 43.84 40.89 45.67 43.15 00.30Table 4: Results of the closed (top) and open tracks (bottom).
For each system, the second column (LF)indicates the averaged LF score across all target representations), which was used to rank the systems.Evaluation Systems participating in the taskwere evaluated based on the accuracy with whichthey can produce semantic dependency graphs forpreviously unseen text, measured relative to thegold-standard testing data.
The key measures forthis evaluation were labeled and unlabeled preci-sion and recall with respect to predicted dependen-cies (predicate?role?argument triples) and labeledand unlabeled exact match with respect to completegraphs.
In both contexts, identification of the topnode(s) of a graph was considered as the identifi-cation of additional, ?virtual?
dependencies froman artificial root node (at position 0).
Below weabbreviate these metrics as (a) labeled precision,recall, and F1: LP, LR, LF; (b) unlabeled precision,recall, and F1: UP, UR, UF; and (c) labeled andunlabeled exact match: LM, UM.The ?official?
ranking of participating systems, inboth the closed and the open tracks, is determinedbased on the arithmetic mean of the labeled depen-dency F1scores (i.e.
the geometric mean of labeledprecision and labeled recall) on the three target rep-resentations (DM, PAS, and PCEDT).
Thus, to beconsidered for the final ranking, a system had tosubmit semantic dependencies for all three targetrepresentations.Closed vs. Open Tracks The task was sub-divided into a closed track and an open track, wheresystems in the closed track could only be trainedon the gold-standard semantic dependencies dis-tributed for the task.
Systems in the open track, onthe other hand, could use additional resources, suchas a syntactic parser, for example?provided thatthey make sure to not use any tools or resourcesthat encompass knowledge of the gold-standardsyntactic or semantic analyses of the SDP 2014test data, i.e.
were directly or indirectly trained orotherwise derived from WSJ Section 21.This restriction implies that typical off-the-shelfsyntactic parsers had to be re-trained, as many data-driven parsers for English include this section ofthe PTB in their default training data.
To simplifyparticipation in the open track, the organizers pre-pared ready-to-use ?companion?
syntactic analyses,sentence- and token-aligned to the SDP data, intwo formats, viz.
PTB-style phrase structure treesobtained from the parser of Petrov et al.
(2006) andStanford Basic syntactic dependencies (de Marn-effe et al., 2006) produced by the parser of Bohnetand Nivre (2012).6 Submissions and ResultsFrom 36 teams who had registered for the task,test runs were submitted for nine systems.
Eachteam submitted one or two test runs per track.
Intotal, there were ten runs submitted to the closedtrack and nine runs to the open track.
Three teamssubmitted to both the closed and the open track.The main results are summarized and ranked inTable 4.
The ranking is based on the average LFscore across all three target representations, whichis given in the LF column.
In cases where a teamsubmitted two runs to a track, only the highest-ranked score is included in the table.69Team Track Approach ResourcesLink?ping C extension of Eisner?s algorithm for DAGs, edge-factoredstructured perceptron?Potsdam C & O graph-to-tree transformation, Mate companionPriberam C & O model with second-order features, decoding with dual decom-position, MIRAcompanionTurku O cascade of SVM classifiers (dependency recognition, labelclassification, top recognition)companion,syntactic n-grams,word2vecAlpage C & O transition-based parsing for DAGs, logistic regression, struc-tured perceptroncompanion,Brown clustersPeking C transition-based parsing for DAGs, graph-to-tree transforma-tion, parser ensemble?CMU O edge classification by logistic regression, edge-factored struc-tured SVMcompanionCopenhagen-Malm?
C graph-to-tree transformation, Mate ?In-House O existing parsers developed by the organizers grammarsTable 5: Overview of submitted systems, high-level approaches, and additional resources used (if any).In the closed track, the average LF scores acrosstarget representations range from 85.91 to 72.20.Comparing the results for different target represen-tations, the average LF scores across systems are85.96 for PAS, 82.97 for DM, and 70.17 for PCEDT.The scores for labeled exact match show a muchlarger variation across both target representationsand systems.8In the open track, we see very similar trends.The average LF scores across target representationsrange from 86.27 to 75.89 and the correspondingscores across systems are 88.64 for PAS, 84.95for DM, and 67.52 for PCEDT.
While these scoresare consistently higher than in the closed track,the differences are small.
In fact, for each of thethree teams that submitted to both tracks (Alpage,Potsdam, and Priberam) improvements due to theuse of additional resources in the open track do notexceed two points LF.7 Overview of ApproachesTable 5 shows a summary of the systems that sub-mitted final results.
Most of the systems tooka strategy to use some algorithm to process (re-stricted types of) graph structures, and apply ma-chine learning like structured perceptrons.
Themethods for processing graph structures are clas-sified into three types.
One is to transform graphsinto trees in the preprocessing stage, and apply con-ventional dependency parsing systems (e.g.
Mate;Bohnet, 2010) to the converted trees.
Some sys-tems simply output the result of dependency pars-ing (which means they inherently lose some depen-8Please see the task web page at the address indicatedabove for full labeled and unlabeled scores.dencies), while the others apply post-processingto recover non-tree structures.
The second strat-egy is to use a parsing algorithm that can directlygenerate graph structures (in the spirit of Sagae &Tsujii, 2008; Titov et al., 2009).
In many casessuch algorithms generate restricted types of graphstructures, but these restrictions appear feasible forour target representations.
The last approach ismore machine learning?oriented; they apply classi-fiers or scoring methods (e.g.
edge-factored scores),and find the highest-scoring structures by some de-coding method.It is difficult to tell which approach is the best;actually, the top three systems in the closed andopen tracks selected very different approaches.
Apossible conclusion is that exploiting existing sys-tems or techniques for dependency parsing wassuccessful; for example, Peking built an ensembleof existing transition-based and graph-based depen-dency parsers, and Priberam extended an existingdependency parser.
As we indicated in the task de-scription, a novel feature of this task is that we haveto compute graph structures, and cannot assumewell-known properties like projectivity and lack ofreentrancies.
However, many of the participantsfound that our representations are mostly tree-like,and this fact motivated them to apply methods thathave been well studied in the field of syntactic de-pendency parsing.Finally, we observe that three teams participatedin both the closed and open tracks, and all of themreported that adding external resources improvedaccuracy by a little more than one point.
Systemswith (only) open submissions extensively use syn-tactic features (e.g.
dependency paths) from exter-nal resources, and they are shown effective even70with simple machine learning models.
Pre-existing,tree-oriented dependency parsers are relatively ef-fective, especially when combined with graph-to-tree transformation.
Comparing across our threetarget representations, system scores show a ten-dency PAS> DM> PCEDT, which can be taken asa tentative indicator of relative levels of ?parsabil-ity?.
As suggested in Section 4, this variation mostlikely correlates at least in part with diverging de-sign decisions, e.g.
the inclusion of relatively localand deterministic dependencies involving functionwords in PAS, or the decision to annotate contex-tually determined speaker meaning (rather than?mere?
sentence meaning) in at least some construc-tions in PCEDT.8 Conclusions and OutlookWe have described the motivation, design, and out-comes of the SDP 2014 task on semantic depen-dency parsing, i.e.
retrieving bi-lexical predicate?argument relations between all content wordswithin an English sentence.
We have converted toa common format three existing annotations (DM,PAS, and PCEDT) over the same text and have putthis to use for the first time in training and testingdata-driven semantic dependency parsers.
Buildingon strong community interest already to date andour belief that graph-oriented dependency parsingwill further gain importance in the years to come,we are preparing a similar (slightly modified) taskfor SemEval 2015.
Candidate modifications andextensions will include cross-domain testing andevaluation at the level of ?complete?
predications(in contrast to more lenient per-dependency F1usedthis year).
As optional new sub-tasks, we plan onoffering cross-linguistic variation and predicate (i.e.semantic frame) disambiguation for at least some ofthe target representations.
To further probe the roleof syntax in the recovery of semantic dependencyrelations, we will make available to participantsa wider selection of syntactic analyses, as well asadd a third (idealized) ?gold?
track, where syntacticdependencies are provided directly from availablesyntactic annotations of the underlying treebanks.AcknowledgementsWe are grateful to ?eljko Agi?c and Bernd Bohnetfor consultation and assistance in preparing ourbaseline and companion parses, to the LinguisticData Consortium (LDC) for support in distributingthe SDP data to participants, as well as to Emily M.Bender and two anonymous reviewers for feedbackon this manuscript.
Data preparation was supportedthrough access to the ABEL high-performance com-puting facilities at the University of Oslo, and weacknowledge the Scientific Computing staff at UiO,the Norwegian Metacenter for Computational Sci-ence, and the Norwegian tax payers.
Part of thiswork has been supported by the infrastructural fund-ing by the Ministry of Education, Youth and Sportsof the Czech Republic (CEP ID LM2010013).ReferencesBohnet, B.
(2010).
Top accuracy and fast depen-dency parsing is not a contradiction.
In Proceedingsof the 23rd International Conference on Computa-tional Linguistics (p. 89 ?
97).
Beijing, China.Bohnet, B., & Nivre, J.
(2012).
A transition-basedsystem for joint part-of-speech tagging and labelednon-projective dependency parsing.
In Proceedingsof the 2012 Joint Conference on Empirical Methodsin Natural Language Processing and Conference onNatural Language Learning (p. 1455 ?
1465).
JejuIsland, Korea.Buchholz, S., & Marsi, E. (2006).
CoNLL-X sharedtask on multilingual dependency parsing.
In Pro-ceedings of the 10th Conference on Natural Lan-guage Learning (p. 149 ?
164).
New York, NY,USA.Copestake, A., Flickinger, D., Pollard, C., & Sag, I.
A.(2005).
Minimal Recursion Semantics.
An introduc-tion.
Research on Language and Computation, 3(4),281 ?
332.de Marneffe, M.-C., MacCartney, B., & Manning, C. D.(2006).
Generating typed dependency parses fromphrase structure parses.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (p. 449 ?
454).
Genoa, Italy.Fares, M., Oepen, S., & Zhang, Y.
(2013).
Machinelearning for high-quality tokenization.
Replicatingvariable tokenization schemes.
In Computational lin-guistics and intelligent text processing (p. 231 ?
244).Springer.Flickinger, D. (2000).
On building a more efficientgrammar by exploiting types.
Natural Language En-gineering, 6 (1), 15 ?
28.Flickinger, D., Zhang, Y., & Kordoni, V. (2012).
Deep-Bank.
A dynamically annotated treebank of the WallStreet Journal.
In Proceedings of the 11th Interna-tional Workshop on Treebanks and Linguistic Theo-ries (p. 85 ?
96).
Lisbon, Portugal: Edi?
?es Colibri.Gildea, D., & Jurafsky, D. (2002).
Automatic labelingof semantic roles.
Computational Linguistics, 28,71245 ?
288.Grice, H. P. (1968).
Utterer?s meaning, sentence-meaning, and word-meaning.
Foundations of Lan-guage, 4(3), 225 ?
242.Haji?c, J., Ciaramita, M., Johansson, R., Kawahara, D.,Mart?, M. A., M?rquez, L., .
.
.
Zhang, Y.
(2009).The CoNLL-2009 Shared Task.
syntactic and seman-tic dependencies in multiple languages.
In Proceed-ings of the 13th Conference on Natural LanguageLearning (p. 1 ?
18).
Boulder, CO, USA.Haji?c, J., Haji?cov?, E., Panevov?, J., Sgall, P., Bojar,O., Cinkov?, S., .
.
.
?abokrtsk?, Z.
(2012).
An-nouncing Prague Czech-English Dependency Tree-bank 2.0.
In Proceedings of the 8th InternationalConference on Language Resources and Evaluation(p. 3153 ?
3160).
Istanbul, Turkey.Ivanova, A., Oepen, S., ?vrelid, L., & Flickinger, D.(2012).
Who did what to whom?
A contrastive studyof syntacto-semantic dependencies.
In Proceedingsof the Sixth Linguistic Annotation Workshop (p. 2 ?11).
Jeju, Republic of Korea.Kate, R. J., & Wong, Y. W. (2010).
Semantic pars-ing.
The task, the state of the art and the future.
InTutorial abstracts of the 20th Meeting of the Associ-ation for Computational Linguistics (p. 6).
Uppsala,Sweden.Marcus, M., Santorini, B., & Marcinkiewicz, M.
A.(1993).
Building a large annotated corpora of En-glish: The Penn Treebank.
Computational Linguis-tics, 19, 313 ?
330.Mel?
?cuk, I.
(1988).
Dependency syntax.
Theory andpractice.
Albany, NY, USA: SUNY Press.Meyers, A., Reeves, R., Macleod, C., Szekely, R.,Zielinska, V., Young, B., & Grishman, R. (2004).Annotating noun argument structure for NomBank.In Proceedings of the 4th International Conferenceon Language Resources and Evaluation (p. 803 ?806).
Lisbon, Portugal.Miyao, Y., Oepen, S., & Zeman, D. (2014).
In-house:An ensemble of pre-existing off-the-shelf parsers.
InProceedings of the 8th International Workshop onSemantic Evaluation.
Dublin, Ireland.Nakov, P. (2013).
On the interpretation of noun com-pounds: Syntax, semantics, and entailment.
NaturalLanguage Engineering, 19(3), 291 ?
330.Nivre, J., Hall, J., K?bler, S., McDonald, R., Nilsson,J., Riedel, S., & Yuret, D. (2007).
The CoNLL 2007shared task on dependency parsing.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and Conference onNatural Language Learning (p. 915 ?
932).
Prague,Czech Republic.Oepen, S., & L?nning, J. T. (2006).
Discriminant-based MRS banking.
In Proceedings of the 5th In-ternational Conference on Language Resources andEvaluation (p. 1250 ?
1255).
Genoa, Italy.Palmer, M., Gildea, D., & Kingsbury, P. (2005).
TheProposition Bank.
A corpus annotated with semanticroles.
Computational Linguistics, 31(1), 71 ?
106.Petrov, S., Barrett, L., Thibaux, R., & Klein, D. (2006).Learning accurate, compact, and interpretable treeannotation.
In Proceedings of the 21st InternationalConference on Computational Linguistics and the44th Meeting of the Association for ComputationalLinguistics (p. 433 ?
440).
Sydney, Australia.Quine, W. V. O.
(1960).
Word and object.
Cambridge,MA, USA: MIT press.Sagae, K., & Tsujii, J.
(2008).
Shift-reduce depen-dency DAG parsing.
In Proceedings of the 22ndInternational Conference on Computational Linguis-tics (p. 753 ?
760).
Manchester, UK.Titov, I., Henderson, J., Merlo, P., & Musillo, G.(2009).
Online graph planarisation for synchronousparsing of semantic and syntactic dependencies.
InProceedings of the 21st International Joint Confer-ence on Artifical Intelligence (p. 1562 ?
1567).Vadas, D., & Curran, J.
(2007).
Adding Noun PhraseStructure to the Penn Treebank.
In Proceedings ofthe 45th Meeting of the Association for Computa-tional Linguistics (p. 240 ?
247).
Prague, Czech Re-public.72
