Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 414?421,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsA Description of Tunable Machine Translation Evaluation Systems inWMT13 Metrics TaskAaron L.-F. Hanhanlifengaaron@gmail.comDerek F. Wongderekfw@umac.moLidia S. Chaolidiasc@umac.moYi Lumb25435@umac.moLiangye Hewutianshui0515@gmail.comYiming Wangmb25433@umac.moJiaji Zhoumb25473@uamc.moNatural Language Processing & Portuguese-Chinese Machine Translation LaboratoryDepartment of Computer and Information ScienceUniversity of Macau, Macau S.A.R.
ChinaAbstractThis paper is to describe our machine transla-tion evaluation systems used for participationin the WMT13 shared Metrics Task.
In theMetrics task, we submitted two automatic MTevaluation systems nLEPOR_baseline andLEPOR_v3.1.
nLEPOR_baseline is an n-grambased language independent MT evaluationmetric employing the factors of modified sen-tence length penalty, position difference penal-ty, n-gram precision and n-gram recall.nLEPOR_baseline measures the similarity ofthe system output translations and the refer-ence translations only on word sequences.LEPOR_v3.1 is a new version of LEPOR met-ric using the mathematical harmonic mean togroup the factors and employing some linguis-tic features, such as the part-of-speech infor-mation.
The evaluation results of WMT13show LEPOR_v3.1 yields the highest average-score 0.86 with human judgments at system-level using Pearson correlation criterion onEnglish-to-other (FR, DE, ES, CS, RU) lan-guage pairs.1 IntroductionMachine translation has a long history since the1950s (Weaver, 1955) and gains a fast develop-ment in the recent years because of the higherlevel of computer technology.
For instances, Och(2003) presents Minimum Error Rate Training(MERT) method for log-linear statistical ma-chine translation models to achieve better trans-lation quality; Menezes et al(2006) introduce asyntactically informed phrasal SMT system forEnglish-to-Spanish translation using a phrasetranslation model, which is based on global reor-dering and the dependency tree; Su et al(2009)use the Thematic Role Templates model to im-prove the translation; Costa-juss?
et al(2012)develop the phrase-based SMT system for Chi-nese-Spanish translation using a pivot language.With the rapid development of Machine Transla-tion (MT), the evaluation of MT has become achallenge in front of researchers.
However, theMT evaluation is not an easy task due to the factof the diversity of the languages, especially forthe evaluation between distant languages (Eng-lish, Russia, Japanese, etc.
).2 Related worksThe earliest human assessment methods for ma-chine translation include the intelligibility andfidelity used around 1960s (Carroll, 1966), andthe adequacy (similar as fidelity), fluency andcomprehension (improved intelligibility) (Whiteet al 1994).
Because of the expensive cost ofmanual evaluations, the automatic evaluationmetrics and systems appear recently.The early automatic evaluation metrics in-clude the word error rate WER (Su et al 1992)and position independent word error rate PER(Tillmann et al 1997) that are based on the Le-venshtein distance.
Several promotions for theMT and MT evaluation literatures include theACL?s annual workshop on statistical machinetranslation WMT (Koehn and Monz, 2006; Calli-son-Burch et al 2012), NIST open machinetranslation (OpenMT) Evaluation series (Li,2005) and the international workshop of spokenlanguage translation IWSLT, which is also orga-nized annually from 2004 (Eck and Hori, 2005;414Paul, 2008, 2009; Paul, et al 2010; Federico etal., 2011).BLEU (Papineni et al 2002) is one of thecommonly used evaluation metrics that is de-signed to calculate the document level precisions.NIST (Doddington, 2002) metric is proposedbased on BLEU but with the information weightsadded to the n-gram approaches.
TER (Snover etal., 2006) is another well-known MT evaluationmetric that is designed to calculate the amount ofwork needed to correct the hypothesis translationaccording to the reference translations.
TER in-cludes the edit categories such as insertion, dele-tion, substitution of single words and the shifts ofword chunks.
Other related works include theMETEOR (Banerjee and Lavie, 2005) that usessemantic matching (word stem, synonym, andparaphrase), and (Wong and Kit, 2008), (Popovic,2012), and (Chen et al 2012) that introduces theword order factors, etc.
The traditional evalua-tion metrics tend to perform well on the languagepairs with English as the target language.
Thispaper will introduce the evaluation models thatcan also perform well on the language pairs thatwith English as source language.3 Description of Systems3.1 Sub FactorsFirstly, we introduce the sub factor of modifiedlength penalty inspired by BLEU metric.
{(1)In the formula,    means sentence lengthpenalty that is designed for both the shorter orlonger translated sentence (hypothesis translation)as compared to the reference sentence.
Parame-ters   and   represent the length of candidatesentence and reference sentence respectively.Secondly, let?s see the factors of n-gram pre-cision and n-gram recall.
(2)(3)The variable                represents thenumber of matched n-gram chunks between hy-pothesis sentence and reference sentence.
The n-gram precision and n-gram recall are firstly cal-culated on sentence-level instead of corpus-levelthat is used in BLEU (  ).
Then we define theweighted n-gram harmonic mean of precisionand recall (WNHPR).(?
((4)Thirdly, it is the n-gram based position differ-ence penalty (NPosPenal).
This factor is de-signed to achieve the penalty for the differentorder of successfully matched words in referencesentence and hypothesis sentence.
The alignmentdirection is from the hypothesis sentence to thereference sentence.
It employs the  -gram meth-od into the matching period, which means thatthe potential matched word will be assignedhigher priority if it also has nearby matching.The nearest matching will be accepted as a back-up choice if there are both nearby matching orthere is no other matched word around the poten-tial pairs.(5)?
(6)(7)The variable           means the length ofthe hypothesis sentence; the variablesand           represent the posi-tion number of matched words in hypothesis sen-tence and reference sentence respectively.3.2 Linguistic FeaturesThe linguistic features could be easily employedinto our evaluation models.
In the submitted ex-periment results of WMT Metrics Task, we usedthe part of speech information of the words inquestion.
In grammar, a part of speech, which isalso called a word class, a lexical class, or a lexi-cal category, is a linguistic category of lexicalitems.
It is generally defined by the syntactic ormorphological behavior of the lexical item inquestion.
The POS information utilized in ourmetric LEPOR_v3.1, an enhanced version ofLEPOR (Han et al 2012), is extracted using theBerkeley parser (Petrov et al 2006) for English,German, and French languages, using COM-POST Czech morphology tagger (Collins, 2002)for Czech language, and using TreeTagger(Schmid, 1994) for Spanish and Russian lan-guages respectively.415Ratioother-to-English English-to-otherCZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FRHPR:LP:NPP(word) 7:2:1 3:2:1 7:2:1 3:2:1 7:2:1 1:3:7 3:2:1 3:2:1HPR:LP:NPP(POS) NA 3:2:1 NA 3:2:1 7:2:1 7:2:1 NA 3:2:1(      1:9 9:1 1:9 9:1 9:1 9:1 9:1 9:1(     NA 9:1 NA 9:1 9:1 9:1 NA 9:1NA 1:9 NA 9:1 1:9 1:9 NA 9:1Table 1.
The tuned weight values in LEPOR_v3.1 systemSystemCorrelation Score with Human Judgmentother-to-English English-to-other Meanscore CZ-EN DE-EN ES-EN FR-EN EN-CZ EN-DE EN-ES EN-FRLEPOR_v3.1 0.93 0.86 0.88 0.92 0.83 0.82 0.85 0.83 0.87nLEPOR_baseline 0.95 0.61 0.96 0.88 0.68 0.35 0.89 0.83 0.77METEOR 0.91 0.71 0.88 0.93 0.65 0.30 0.74 0.85 0.75BLEU 0.88 0.48 0.90 0.85 0.65 0.44 0.87 0.86 0.74TER 0.83 0.33 0.89 0.77 0.50 0.12 0.81 0.84 0.64Table 2.
The performances of nLEPOR_baseline and LEPOR_v3.1 systems on WMT11 corpora3.3 The nLEPOR_baseline SystemThe nLEPOR_baseline system utilizes the simpleproduct value of the factors: modified lengthpenalty, n-gram position difference penalty, andweighted n-gram harmonic mean of precisionand recall.
(8)The system level score is the arithmeticalmean of the sentence level evaluation scores.
Inthe experiments of Metrics Task using thenLEPOR_baseline system, we assign N=1 in thefactor WNHPR, i.e.
weighted unigram harmonicmean of precision and recall.3.4 The LEPOR_v3.1 SystemThe system of LEPOR_v3.1 (also called ashLEPOR) combines the sub factors usingweighted mathematical harmonic mean insteadof the simple product value.
(9)Furthermore, this system takes into accountthe linguistic features, such as the POS of thewords.
Firstly, we calculate the hLEPOR scoreon surface words            (the closeness ofthe hypothesis translation and the referencetranslation).
Then, we calculate the hLEPORscore on the extracted POS sequences(the closeness of the correspondingPOS tags between hypothesis sentence and refer-ence sentence).
The final score             isthe combination of the two sub-scoresand          .
((10)4 Evaluation MethodIn the MT evaluation task, the Spearman rankcorrelation coefficient method is usually used bythe authoritative ACL WMT to evaluate the cor-relation of different MT evaluation metrics.
Sowe use the Spearman rank correlation coefficientto evaluate the performances ofnLEPOR_baseline and LEPOR_v3.1 in systemlevel correlation with human judgments.
Whenthere are no ties,   is calculated using:?
((11)The variable    is the difference value be-tween the ranks for         and   is the numberof systems.
We also offer the Pearson correlationcoefficient information as below.
Given a sampleof paired data (X, Y) as (      ,         , thePearson correlation coefficient is:?
(      (??
(??
(     )(12)416where    and    specify the mean of discreterandom variable X and Y respectively.DirectionsEN-FREN-DEEN-ESEN-CSEN-RUAvLEPOR_v3.1 .91 .94 .91 .76 .77 .86nLEPOR_baseline .92 .92 .90 .82 .68 .85SIMP-BLEU_RECALL.95 .93 .90 .82 .63 .84SIMP-BLEU_PREC.94 .90 .89 .82 .65 .84NIST-mteval-inter.91 .83 .84 .79 .68 .81Meteor .91 .88 .88 .82 .55 .81BLEU-mteval-inter.89 .84 .88 .81 .61 .80BLEU-moses .90 .82 .88 .80 .62 .80BLEU-mteval .90 .82 .87 .80 .62 .80CDER-moses .91 .82 .88 .74 .63 .80NIST-mteval .91 .79 .83 .78 .68 .79PER-moses .88 .65 .88 .76 .62 .76TER-moses .91 .73 .78 .70 .61 .75WER-moses .92 .69 .77 .70 .61 .74TerrorCat .94 .96 .95 na na .95SEMPOS na na na .72 na .72ACTa .81 -.47 na na na .17ACTa5+6 .81 -.47 na na na .17Table 3.
System-level Pearson correlation scoreson WMT13 English-to-other language pairs5 Experiments5.1 TrainingIn the training stage, we used the officially re-leased data of past WMT series.
There is no Rus-sian language in the past WMT shared tasks.
Sowe trained our systems on the other eight lan-guage pairs including English to other (French,German, Spanish, Czech) and the inverse transla-tion direction.
In order to avoid the overfittingproblem, we used the WMT11 corpora as train-ing data to train the parameter weights in order toachieve a higher correlation with human judg-ments at system-level evaluations.
For thenLEPOR_baseline system, the tuned values ofand   are 9 and 1 respectively for all languagepairs except for (   ,    ) for CS-EN lan-guage pair.
For the LEPOR_v3.1 system, thetuned values of weights are shown in Table 1.The evaluation scores of the training results onWMT11 corpora are shown in Table 2.
The de-signed methods have shown promising correla-tion scores with human judgments at system lev-el, 0.87 and 0.77 respectively fornLEPOR_baseline and LEPOR_v3.1 of the meanscore on eight language pairs.
As compared toMETEOR, BLEU and TER, we have achievedhigher correlation scores with human judgments.5.2 TestingIn the WMT13 shared Metrics Task, we alsosubmitted our system performances on English-to-Russian and Russian-to-English languagepairs.
However, since the Russian language didnot appear in the past WMT shared tasks, weassigned the default parameter weights to Rus-sian language for the submitted two systems.
Theofficially released results on WMT13 corpora areshown in Table 3, Table 4 and Table 5 respec-tively for system-level and segment-level per-formance on English-to-other language pairs.DirectionsEN-FREN-DEEN-ESEN-CSEN-RUAvSIMP-BLEU_RECALL.92 .93 .83 .87 .71 .85LEPOR_v3.1 .90 .9 .84 .75 .85 .85NIST-mteval-inter.93 .85 .80 .90 .77 .85CDER-moses .92 .87 .86 .89 .70 .85nLEPOR_baseline .92 .90 .85 .82 .73 .84NIST-mteval .91 .83 .78 .92 .72 .83SIMP-BLEU_PREC.91 .88 .78 .88 .70 .83Meteor .92 .88 .78 .94 .57 .82BLEU-mteval-inter.92 .83 .76 .90 .66 .81BLEU-mteval .89 .79 .76 .90 .63 .79TER-moses .91 .85 .75 .86 .54 .78BLEU-moses .90 .79 .76 .90 .57 .78WER-moses .91 .83 .71 .86 .55 .77PER-moses .87 .69 .77 .80 .59 .74TerrorCat .93 .95 .91 na na .93SEMPOS na na na .70 na .70ACTa5+6 .81 -.53 na na na .14ACTa .81 -.53 na na na .14Table 4.
System-level Spearman rank correlationscores on WMT13 English-to-other languagepairsTable 3 shows LEPOR_v3.1 andnLEPOR_baseline yield the highest and the sec-ond highest average Pearson correlation score0.86 and 0.85 respectively with human judg-ments at system-level on five English-to-otherlanguage pairs.
LEPOR_v3.1 and417nLEPOR_baseline also yield the highest Pearsoncorrelation score on English-to-Russian (0.77)and English-to-Czech (0.82) language pairs re-spectively.
The testing results of LEPOR_v3.1and nLEPOR_baseline show better correlationscores as compared to METEOR (0.81), BLEU(0.80) and TER-moses (0.75) on English-to-otherlanguage pairs, which is similar with the trainingresults.On the other hand, using the Spearman rankcorrelation coefficient, SIMPBLEU_RECALLyields the highest correlation score 0.85 withhuman judgments.
Our metric LEPOR_v3.1 alsoyields the highest Spearman correlation score onEnglish-to-Russian (0.85) language pair, whichis similar with the result using Pearson correla-tion and shows its robust performance on thislanguage pair.DirectionsEN-FREN-DEEN-ESEN-CSEN-RU AvSIMP-BLEU_RECALL.16 .09 .23 .06 .12 .13Meteor .15 .05 .18 .06 .11 .11SIMP-BLEU_PREC.14 .07 .19 .06 .09 .11sentBLEU-moses .13 .05 .17 .05 .09 .10LEPOR_v3.1 .13 .06 .18 .02 .11 .10nLEPOR_baseline .12 .05 .16 .05 .10 .10dfki_logregNorm-411na na .14 na na .14TerrorCat .12 .07 .19 na na .13dfki_logregNormSoft-431na na .03 na na .03Table 5.
Segment-level Kendall?s tau correlationscores on WMT13 English-to-other languagepairsHowever, we find a problem in the Spearmanrank correlation method.
For instance, let twoevaluation metrics MA and MB with their evalu-ation scores   ??????
and????
??
respectively reflectingthree MT systems??
.
Before the calculation of cor-relation with human judgments, they will beconverted into   ??????
?
and   ????
??
?with the same rank sequence using Spearmanrank method; thus, the two evaluation systemswill get the same correlation with human judg-ments whatever are the values of human judg-ments.
But the two metrics reflect different re-sults indeed: MA gives the outstanding score(0.95) to M1 system and puts very low scores(0.50 and 0.45) on the other two systems M2 andM3 while MB thinks the three MT systems havesimilar performances (scores from 0.74 to 0.77).This information is lost using the Spearman rankcorrelation methodology.The segment-level performance ofLEPOR_v3.1 is moderate with the average Ken-dall?s tau correlation score 0.10 on five English-to-other language pairs, which is due to the factthat we trained our metrics at system-level in thisshared metrics task.
Lastly, the officially releasedresults on WMT13 other-to-English languagepairs are shown in Table 6, Table 7 and Table 8respectively for system-level and segment-levelperformance.DirectionsFR-ENDE-ENES-ENCS-ENRU-EN AvMeteor .98 .96 .97 .99 .84 .95SEMPOS .95 .95 .96 .99 .82 .93Depref-align .97 .97 .97 .98 .74 .93Depref-exact .97 .97 .96 .98 .73 .92SIMP-BLEU_RECALL.97 .97 .96 .94 .78 .92UMEANT .96 .97 .99 .97 .66 .91MEANT .96 .96 .99 .96 .63 .90CDER-moses .96 .91 .95 .90 .66 .88SIMP-BLEU_PREC.95 .92 .95 .91 .61 .87LEPOR_v3.1 .96 .96 .90 .81 .71 .87nLEPOR_baseline .96 .94 .94 .80 .69 .87BLEU-mteval-inter.95 .92 .94 .90 .61 .86NIST-mteval-inter .94 .91 .93 .84 .66 .86BLEU-moses .94 .91 .94 .89 .60 .86BLEU-mteval .95 .90 .94 .88 .60 .85NIST-mteval .94 .90 .93 .84 .65 .85TER-moses .93 .87 .91 .77 .52 .80WER-moses .93 .84 .89 .76 .50 .78PER-moses .84 .88 .87 .74 .45 .76TerrorCat .98 .98 .97 na na .98Table 6.
System-level Pearson correlation scoreson WMT13 other-to-English language pairsMETEOR yields the highest average correla-tion scores 0.95 and 0.94 respectively usingPearson and Spearman rank correlation methodson other-to-English language pairs.
The averageperformance of nLEPOR_baseline is a little bet-ter than LEPOR_v3.1 on the five language pairsof other-to-English even though it is also moder-ate as compared to other metrics.
However, using418the Pearson correlation method,nLEPOR_baseline yields the average correlationscore 0.87 which already wins the BLEU (0.86)and TER (0.80) as shown in Table 6.DirectionsFR-ENDE-ENES-ENCS-ENRU-EN AvMeteor .98 .96 .98 .96 .81 .94Depref-align .99 .97 .97 .96 .79 .94UMEANT .99 .95 .96 .97 .79 .93MEANT .97 .93 .94 .97 .78 .92Depref-exact .98 .96 .94 .94 .76 .92SEMPOS .94 .92 .93 .95 .83 .91SIMP-BLEU_RECALL.98 .94 .92 .91 .81 .91BLEU-mteval-inter.99 .90 .90 .94 .72 .89BLEU-mteval .99 .89 .89 .94 .69 .88BLEU-moses .99 .90 .88 .94 .67 .88CDER-moses .99 .88 .89 .93 .69 .87SIMP-BLEU_PREC.99 .85 .83 .92 .72 .86nLEPOR_baseline .95 .95 .83 .85 .72 .86LEPOR_v3.1 .95 .93 .75 0.8 .79 .84NIST-mteval .95 .88 .77 .89 .66 .83NIST-mteval-inter .95 .88 .76 .88 .68 .83TER-moses .95 .83 .83 0.8 0.60.80WER-moses .95 .67 .80 .75 .61 .76PER-moses .85 .86 .36 .70 .67 .69TerrorCat .98 .96 .97 na na .97Table 7.
System-level Spearman rank correlationscores on WMT13 other-to-English languagepairsOnce again, our metrics perform moderate atsegment-level on other-to-English language pairsdue to the fact that they are trained at system-level.
We notice that some of the evaluation met-rics do not submit the results on all the languagepairs; however, their performance on submittedlanguage pair is sometimes very good, such asthe dfki_logregFSS-33 metric with a segment-level correlation score 0.27 on German-to-English language pair.6 ConclusionThis paper describes our participation in theWMT13 Metrics Task.
We submitted two sys-tems nLEPOR_baseline and LEPOR_v3.1.
Bothof the two systems are trained and tested usingthe officially released data.
LEPOR_v3.1 yieldsthe highest Pearson correlation average-score0.86 with human judgments on five English-to-other language pairs, and nLEPOR_baselineyields better performance than LEPOR_v3.1 onother-to-English language pairs.
Furthermore,LEPOR_v3.1 shows robust system-level perfor-mance on English-to-Russian language pair, andnLEPOR_baseline shows best system-level per-formance on English-to-Czech language pair us-ing Pearson correlation criterion.
As compared tonLEPOR_baseline, the experiment results ofLEPOR_v3.1 also show that the proper use oflinguistic information can increase the perfor-mance of the evaluation systems.DirectionsFR-ENDE-ENES-ENCS-ENRU-EN AvSIMP-BLEU_RECALL.19 .32 .28 .26 .23 .26Meteor .18 .29 .24 .27 .24 .24Depref-align .16 .27 .23 .23 .20 .22Depref-exact .17 .26 .23 .23 .19 .22SIMP-BLEU_PREC.15 .24 .21 .21 .17 .20nLEPOR_baseline .15 .24 .20 .18 .17 .19sentBLEU-moses .15 .22 .20 .20 .17 .19LEPOR_v3.1 .15 .22 .16 .19 .18 .18UMEANT .10 .17 .14 .16 .11 .14MEANT .10 .16 .14 .16 .11 .14dfki_logregFSS-33na .27 na na na .27dfki_logregFSS-24na .27 na na na .27TerrorCat .16 .30 .23 na na .23Table 8.
Segment-level Kendall?s tau correlationscores on WMT13 other-to-English languagepairsAcknowledgmentsThe authors wish to thank the anonymous re-viewers for many helpful comments.
The authorsare grateful to the Science and Technology De-velopment Fund of Macau and the ResearchCommittee of the University of Macau for thefunding support for our research, under the refer-ence No.
017/2009/A and RG060/09-10S/CS/FST.ReferencesBanerjee, Satanjeev and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
InProceedings of the 43th Annual Meeting of the419Association of Computational Linguistics(ACL- 05), pages 65?72, Ann Arbor, US, June.Association of Computational Linguistics.Callison-Burch, Chris, Philipp Koehn, Christof Monz,and Omar F. Zaidan.
2011.
Findings of the 2011Workshop on Statistical Machine Translation.In Proceedings of the Sixth Workshop on Sta-tistical Machine Translation (WMT '11).
Asso-ciation for Computational Linguistics, Stroudsburg,PA, USA, 22-64.Callison-Burch, Chris, Philipp Koehn, Christof Monz,Matt Post, Radu Soricut, and Lucia Specia.
2012.Findings of the 2012Workshop on Statistical Ma-chine Translation.
In Proceedings of the SeventhWorkshop on Statistical Machine Translation,pages 10?51, Montreal, Canada.
Association forComputational Linguistics.Carroll, John B.
1966.
An Experiment in Evaluatingthe Quality of Translations, Mechanical Transla-tion and Computational Linguistics, vol.9,nos.3 and 4, September and December 1966, page55-66, Graduate School of Education, HarvardUniversity.Chen, Boxing, Roland Kuhn and Samuel Larkin.
2012.PORT: a Precision-Order-Recall MT EvaluationMetric for Tuning, Proceedings of the 50th An-nual Meeting of the Association for Computa-tional Linguistics, pages 930?939, Jeju, Republicof Korea, 8-14 July 2012.Collins, Michael.
2002.
Discriminative TrainingMethods for Hidden Markov Models: Theory andExperiments with Perceptron Algorithms.
In Pro-ceedings of the ACL-02 conference on Empiri-cal methods in natural language processing,Volume 10 (EMNLP 02), pages 1-8.
Associationfor Computational Linguistics, Stroudsburg, PA,USA.Costa-juss?, Marta R., Carlos A. Henr?quez and Ra-fael E. Banchs.
2012.
Evaluating Indirect Strategiesfor Chinese-Spanish Statistical Machine Transla-tion.
Journal of artificial intelligence research,Volume 45, pages 761-780.Doddington, George.
2002.
Automatic evaluation ofmachine translation quality using n-gram co-occurrence statistics.
In Proceedings of the sec-ond international conference on Human Lan-guage Technology Research (HLT '02).
MorganKaufmann Publishers Inc., San Francisco, CA,USA, 138-145.Eck, Matthias and Chiori Hori.
2005.
Overview of theIWSLT 2005 Evaluation Campaign.
Proceedingsof IWSLT 2005.Federico, Marcello, Luisa Bentivogli, Michael Paul,and Sebastian Stiiker.
2011.
Overview of theIWSLT 2011 Evaluation Campaign.
In Proceed-ings of IWSLT 2011, 11-27.Han, Aaron Li-Feng, Derek F. Wong and Lidia S.Chao.
2012.
LEPOR: A Robust Evaluation Metricfor Machine Translation with Augmented Factors.Proceedings of the 24th International Confer-ence on Computational Linguistics (COLING2012: Posters), Mumbai, India.Koehn, Philipp and Christof Monz.
2006.
Manual andAutomatic Evaluation of Machine Translation be-tween European Languages.
Proceedings of theACLWorkshop on Statistical Machine Trans-lation, pages 102?121, New York City.Li, A.
(2005).
Results of the 2005 NIST machinetranslation evaluation.
In Machine TranslationWorkshop.Menezes, Arul, Kristina Toutanova and Chris Quirk.2006.
Microsoft Research Treelet Translation Sys-tem: NAACL 2006 Europarl Evaluation, Proceed-ings of the ACLWorkshop on Statistical Ma-chine Translation, pages 158-161, New YorkCity.Och, Franz Josef.
2003.
Minimum Error Rate Train-ing for Statistical Machine Translation.
In Pro-ceedings of the 41st Annual Meeting of the As-sociation for Computational Linguistics (ACL-2003).
pp.
160-167.Papineni, Kishore, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a method for automat-ic evaluation of machine translation.In Proceedings of the 40th Annual Meeting onAssociation for Computational Linguis-tics (ACL '02).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 311-318.Paul, Michael.
2008.
Overview of the IWSLT 2008Evaluation Campaign.
Proceeding of IWLST2008, Hawaii, USA.Paul, Michael.
2009.
Overview of the IWSLT 2009Evaluation Campaign.
In Proc.
of IWSLT 2009,Tokyo, Japan, pp.
1?18.Paul, Michael, Marcello Federico and SebastianStiiker.
2010.
Overview of the IWSLT 2010 Eval-uation Campaign, Proceedings of the 7th Inter-national Workshop on Spoken LanguageTranslation, Paris, December 2nd and 3rd, 2010,page 1-25.Petrov, Slav, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and in-terpretable tree annotation.
In Proceedings of the21st International Conference on Computa-tional Linguistics and the 44th annual meetingof the Association for Computational Linguis-420tics (ACL-44).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 433-440.Popovic, Maja.
2012.
Class error rates for evaluationof machine translation output.
Proceedings of the7th Workshop on Statistical Machine Transla-tion, pages 71?75, Canada.Schmid, Helmut.
1994.
Probabilistic Part-of-SpeechTagging Using Decision Trees.
In Proceedings ofInternational Conference on New Methods inLanguage Processing, Manchester, UK.Snover, Matthew, Bonnie J. Dorr, Richard Schwartz,Linnea Micciulla, and John Makhoul.
2006.
AStudy of Translation Edit Rate with Targeted Hu-man Annotation.
In Proceedings of the 7th Con-ference of the Association for Machine Trans-lation in the Americas (AMTA-06), pages 223?231, USA.
Association for Machine Translation inthe Americas.Su, Hung-Yu and Chung-Hsien Wu.
2009.
ImprovingStructural Statistical Machine Translation for SignLanguage With Small Corpus Using ThematicRole Templates as Translation Memory, IEEETRANSACTIONS ON AUDIO, SPEECH, ANDLANGUAGE PROCESSING, VOL.
17, NO.
7.Su, Keh-Yih, Wu Ming-Wen and Chang Jing-Shin.1992.
A New Quantitative Quality Measure forMachine Translation Systems.
In Proceedings ofthe 14th International Conference on Compu-tational Linguistics, pages 433?439, Nantes,France.Tillmann, Christoph, Stephan Vogel, Hermann Ney,Arkaitz Zubiaga, and Hassan Sawaf.
1997.
Accel-erated DP Based Search For Statistical Translation.In Proceedings of the 5th European Confer-ence on Speech Communication and Technol-ogy (EUROSPEECH-97).Weaver, Warren.
1955.
Translation.
In William Lockeand A. Donald Booth, editors, Machine Transla-tion of Languages: Fourteen Essays.
JohnWiley & Sons, New York, pages 15?23.White, John S., Theresa O?Connell, and FrancisO?Mara.
1994.
The ARPA MT evaluation method-ologies: Evolution, lessons, and future approaches.In Proceedings of the Conference of the Asso-ciation for Machine Translation in the Ameri-cas (AMTA 1994).
pp193-205.Wong, Billy and Chunyu Kit.
2008.
Word choice andword position for automatic MT evaluation.
InWorkshop: MetricsMATR of the Association forMachine Translation in the Americas (AMTA),short paper, Waikiki, Hawai?I, USA.
Associationfor Machine Translation in the Americas.421
