Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 133?136,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Statistical Machine Translation of Euparl Data by using Bilingual N-gramsRafael E. Banchs Josep M. Crego Adria` de GispertDepartment of Signal Theory and CommunicationsUniversitat Polite`cnica de Catalunya, Barcelona 08034, Spain{rbanchs,jmcrego,agispert,lambert,canton}@gps.tsc.upc.eduPatrik Lambert Jose?
B. Marin?oAbstractThis work discusses translation results forthe four Euparl data sets which were madeavailable for the shared task ?Exploit-ing Parallel Texts for Statistical MachineTranslation?.
All results presented weregenerated by using a statistical machinetranslation system which implements alog-linear combination of feature func-tions along with a bilingual n-gram trans-lation model.1 IntroductionDuring the last decade, statistical machine transla-tion (SMT) systems have evolved from the orig-inal word-based approach (Brown et al, 1993)into phrase-based translation systems (Koehn et al,2003).
Similarly, the noisy channel approach hasbeen expanded to a more general maximum entropyapproach in which a log-linear combination of mul-tiple models is implemented (Och and Ney, 2002).The SMT approach used in this work implementsa log-linear combination of feature functions alongwith a translation model which is based on bilingualn-grams.
This translation model was developed byde Gispert and Marin?o (2002), and it differs from thewell known phrase-based translation model in twobasic issues: first, training data is monotonously seg-mented into bilingual units; and second, the modelconsiders n-gram probabilities instead of relativefrequencies.
This model is described in section 2.Translation results from the four source languagesmade available for the shared task (es: Spanish, fr:French, de: German, and fi: Finnish) into English(en) are presented and discussed.The paper is structured as follows.
Section 2 de-scribes the bilingual n-gram translation model.
Sec-tion 3 presents a brief overview of the whole SMTprocedure.
Section 4 presents and discusses theshared task results and other interesting experimen-tation.
Finally, section 5 presents some conclusionsand further work.2 Bilingual N-gram Translation ModelAs already mentioned, the translation model usedhere is based on bilingual n-grams.
It actually con-stitutes a language model of bilingual units whichare referred to as tuples (de Gispert and Marin?o,2002).
This model approximates the joint probabil-ity between source and target languages by using 3-grams as it is described in the following equation:p(T, S) ?N?n=1p((t, s)n|(t, s)n?2, (t, s)n?1) (1)where t refers to target, s to source and (t, s)n to thenth tuple of a given bilingual sentence pair.Tuples are extracted from a word-to-word alignedcorpus according to the following two constraints:first, tuple extraction should produce a monotonicsegmentation of bilingual sentence pairs; and sec-ond, the produced segmentation is maximal in thesense that no smaller tuples can be extracted with-out violating the previous constraint (Crego et al,2004).
According to this, tuple extraction provides aunique segmentation for a given bilingual sentencepair alignment.
Figure 1 illustrates this idea with asimple example.133We would like to achieve perfect translationsNULL quisieramos lograr traducciones perfectast1 t2 t3 t4Figure 1: Example of tuple extraction from analigned sentence pair.Two important issues regarding this translationmodel must be mentioned.
First, when extractingtuples, some words always appear embedded into tu-ples containing two or more words, so no translationprobability for an independent occurrence of suchwords exists.
To overcome this problem, the tuple3-gram model is enhanced by incorporating 1-gramtranslation probabilities for all the embedded words(de Gispert et al, 2004).Second, some words linked to NULL end up pro-ducing tuples with NULL source sides.
This cannotbe allowed since no NULL is expected to occur in atranslation input.
This problem is solved by prepro-cessing alignments before tuple extraction such thatany target word that is linked to NULL is attachedto either its precedent or its following word.3 SMT Procedure DescriptionThis section describes the procedure followed forpreprocessing the data, training the models and op-timizing the translation system parameters.3.1 Preprocessing and AlignmentThe Euparl data provided for this shared task (Eu-parl, 2003) was preprocessed for eliminating all sen-tence pairs with a word ratio larger than 2.4.
As aresult of this preprocessing, the number of sentencesin each training set was slightly reduced.
However,no significant reduction was produced.In the case of French, a re-tokenizing procedurewas performed in which all apostrophes appearingalone were attached to their corresponding words.For example, pairs of tokens such as l ?
and qu ?were reduced to single tokens such as l?
and qu?.Once the training data was preprocessed, a word-to-word alignment was performed in both direc-tions, source-to-target and target-to-source, by us-ing GIZA++ (Och and Ney, 2000).
As an approxi-mation to the most probable alignment, the Viterbialignment was considered.
Then, the intersectionand union of alignment sets in both directions werecomputed for each training set.3.2 Feature Function ComputationThe considered translation system implements a to-tal of five feature functions.
The first of these mod-els is the tuple 3-gram model, which was already de-scribed in section 2.
Tuples for the translation modelwere extracted from the union set of alignments asshown in Figure 1.
Once tuples had been extracted,the tuple vocabulary was pruned by using histogrampruning.
The same pruning parameter, which wasactually estimated for Spanish-English, was used forthe other three language pairs.
After pruning, thetuple 3-gram model was trained by using the SRILanguage Modeling toolkit (Stolcke, 2002).
Finally,the obtained model was enhanced by incorporating1-gram probabilities for the embedded word tuples,which were extracted from the intersection set ofalignments.Table 1 presents the total number of runningwords, distinct tokens and tuples, for each of the fourtraining data sets.Table 1: Total number of running words, distinct to-kens and tuples in training.source running distinct tuplelanguage words tokens vocabularySpanish 15670801 113570 1288770French 14844465 78408 1173424German 15207550 204949 1391425Finnish 11228947 389223 1496417The second feature function considered was a tar-get language model.
This feature actually consistedof a word 3-gram model, which was trained from thetarget side of the bilingual corpus by using the SRILanguage Modeling toolkit.The third feature function was given by a wordpenalty model.
This function introduces a sentencelength penalization in order to compensate the sys-134tem preference for short output sentences.
Morespecifically, the penalization factor was given by thetotal number of words contained in the translationhypothesis.Finally, the fourth and fifth feature functions cor-responded to two lexicon models based on IBMModel 1 lexical parameters p(t|s) (Brown et al,1993).
These lexicon models were calculated foreach tuple according to the following equation:plexicon((t, s)n) =1(I + 1)JJ?j=1I?i=0p(tin|sjn) (2)where sjn and tin are the jth and ith words in thesource and target sides of tuple (t, s)n, being J andI the corresponding total number words in each sideof it.The forward lexicon model uses IBM Model 1 pa-rameters obtained from source-to-target algnments,while the backward lexicon model uses parametersobtained from target-to-source alignments.3.3 Decoding and OptimizationThe search engine for this translation system wasdeveloped by Crego et al (2005).
It implementsa beam-search strategy based on dynamic program-ming and takes into account all the five feature func-tions described above simultaneously.
It also allowsfor three different pruning methods: threshold prun-ing, histogram pruning, and hypothesis recombina-tion.
For all the results presented in this work thedecoder?s monotonic search modality was used.An optimization tool, which is based on a simplexmethod (Press et al, 2002), was developed and usedfor computing log-linear weights for each of the fea-ture functions described above.
This algorithm ad-justs the log-linear weights so that BLEU (Papineniet al, 2002) is maximized over a given developmentset.
One optimization for each language pair wasperformed by using the 2000-sentence developmentsets made available for the shared task.4 Shared Task ResultsTable 2 presents the BLEU scores obtained for theshared task test data.
Each test set consisted of 2000sentences.
The computed BLEU scores were caseinsensitive and used one translation reference.Table 2: BLEU scores (shared task test sets).es - en fr - en de - en fi - en0.3007 0.3020 0.2426 0.2031As can be seen from Table 2 the best ranked trans-lations were those obtained for French, followed bySpanish, German and Finnish.
A big difference isobserved between the best and the worst results.Differences can be observed from translation out-puts too.
Consider, for example, the following seg-ments taken from one of the test sentences:es-en: We know very well that the present Treaties are notenough and that , in the future , it will be necessary to developa structure better and different for the European Union...fr-en: We know very well that the Treaties in their currentare not enough and that it will be necessary for the future todevelop a structure more effective and different for the Union...de-en: We very much aware that the relevant treaties areinadequate and , in future to another , more efficient structurefor the European Union that must be developed...fi-en: We know full well that the current Treaties are notsufficient and that , in the future , it is necessary to develop theUnion better and a different structure...It is evident from these translation outputs thattranslation quality decreases when moving fromSpanish and French to German and Finnish.
Adetailed observation of translation outputs revealsthat there are basically two problems related to thisdegradation in quality.
The first has to do with re-ordering, which seems to be affecting Finnish and,specially, German translations.The second problem has to do with vocabulary.
Itis well known that large vocabularies produce datasparseness problems (Koehn, 2002).
As can be con-firmed from Tables 1 and 2, translation quality de-creases as vocabulary size increases.
However, it isnot clear yet, in which degree such degradation isdue to monotonic decoding and/or vocabulary size.Finally, we also evaluated how much the full fea-ture function system differs from the baseline tu-ple 3-gram model alone.
In this way, BLEU scoreswere computed for translation outputs obtained forthe baseline system and the full system.
Since theEnglish reference for the test set was not available,we computed translations and BLEU scores over de-135velopment sets.
Table 3 presents the results for boththe full system and the baseline.1Table 3: Baseline- and full-system BLEU scores(computed over development sets).language pair baseline fulles - en 0.2588 0.3004fr - en 0.2547 0.2938de - en 0.1844 0.2350fi - en 0.1526 0.1989From Table 3, it is evident that the four additionalfeature functions produce important improvementsin translation quality.5 Conclusions and Further WorkAs can be concluded from the presented results, per-formance of the translation system used is much bet-ter for French and Spanish than for German andFinnish.
As some results suggest, reordering andvocabulary size are the most important problems re-lated to the low translation quality achieved for Ger-man and Finnish.It is also evident that the bilingual n-gram modelused requires the additional feature functions to pro-duce better translations.
However, more experimen-tation is required in order to fully understand eachindividual feature?s influence on the overall log-linear model performance.6 AcknowledgmentsThis work has been funded by the European Unionunder the integrated project TC-STAR - Technologyand Corpora for Speech to Speech Translation -(IST-2002-FP6-506738, http://www.tc-star.org).The authors also want to thank Jose?
A. R. Fonol-losa and Marta Ruiz Costa-jussa` for their participa-tion in discussions related to this work.ReferencesPeter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
?The mathemat-1Differently from BLEU scores presented in Table 2, whichare case insensitive, BLEU scores presented in Table 3 are casesensitive.ics of statistical machine translation: parameter esti-mation?.
Computational Linguistics, 19(2):263?311.Josep M. Crego, Jose?
B. Marin?o, and Adria` de Gispert.2004.
?Finite-state-based and phrase-based statisticalmachine translation?.
Proc.
of the 8th Int.
Conf.
onSpoken Language Processing, :37?40, October.Josep M. Crego, Jose?
B. Marin?o, and Adria` de Gispert.2005.
?A Ngram-based Statistical Machine Transla-tion Decoder?.
Submitted to INTERSPEECH 2005.Adria` de Gispert, and Jose?
B. Marin?o.
2002.
?Using X-grams for speech-to-speech translation?.
Proc.
of the7th Int.
Conf.
on Spoken Language Processing.Adria` de Gispert, Jose?
B. Marin?o, and Josep M. Crego.2004.
?TALP: Xgram-based spoken language transla-tion system?.
Proc.
of the Int.
Workshop on SpokenLanguage Translation, :85?90.
Kyoto, Japan, October.EUPARL: European Parliament Proceedings ParallelCorpus 1996-2003.
Available on-line at: http://people.csail.mit.edu/people/koehn/publications/europarl/Philipp Koehn.
2002.
?Europarl: A Multilingual Cor-pus for Evaluation of Machine Translation?.
Avail-able on-line at: http://people.csail.mit.edu/people/koehn/publications/europarl/Philipp Koehn, Franz J. Och, and Daniel Marcu.
2003.?Statistical phrase-based translation?.
Proc.
of the2003 Meeting of the North American chapter of theACL, Edmonton, Alberta.Franz J. Och and Hermann Ney.
2000.
?Improved statis-tical alignment models?.
Proc.
of the 38th Ann.
Meet-ing of the ACL, Hong Kong, China, October.Franz J. Och and Hermann Ney.
2002.
?Discriminativetraining and maximum entropy models for statisticalmachine translation?.
Proc.
of the 40th Ann.
Meetingof the ACL, :295?302, Philadelphia, PA, July.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
?Bleu: a method for automatic eval-uation of machine translation?.
Proc.
of the 40th Ann.Conf.
of the ACL, Philadelphia, PA, July.William H. Press, Saul A. Teukolsky, William T. Vetter-ling, and Brian P. Flannery.
2002.
Numerical Recipesin C++: the Art of Scientific Computing, CambridgeUniversity Press.Andreas Stolcke.
2002.
?SRLIM: an extensible languagemodeling toolkit?.
Proc.
of the Int.
Conf.
on SpokenLanguage Processing :901?904, Denver, CO, Septem-ber.
Available on line at: http://www.speech.sri.com/projects/srilm/136
