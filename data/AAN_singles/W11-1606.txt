Workshop on Monolingual Text-To-Text Generation, pages 43?53,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 43?53,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsTowards Strict Sentence Intersection: Decoding and Evaluation StrategiesKapil Thadani and Kathleen McKeownDepartment of Computer ScienceColumbia UniversityNew York, NY 10027, USA{kapil,kathy}@cs.columbia.eduAbstractWe examine the task of strict sentence inter-section: a variant of sentence fusion in whichthe output must only contain the informa-tion present in all input sentences and nothingmore.
Our proposed approach involves align-ment and generalization over the input sen-tences to produce a generation lattice; we thencompare a standard search-based approach fordecoding an intersection from this lattice to aninteger linear program that preserves alignedcontent while minimizing the disfluency ininterleaving text segments.
In addition, weintroduce novel evaluation strategies for in-tersection problems that employ entailment-style judgments for determining the validityof system-generated intersections.
Our experi-ments show that the proposed models producevalid intersections a majority of the time andthat the segmented decoder yields advantagesover the search-based approach.1 IntroductionIn recent years, there has been growing interestin text-to-text generation problems which transformtext according to specifications.
Tasks such as sen-tence compression, which strives to retain the mostsalient content of an input sentence, and sentence fu-sion, which attempts to combine the important con-tent in related sentences, are useful components fortackling larger natural language problems such asabstractive summarization of documents.
Systemsfor these types of text-to-text problems are typicallyevaluated on the informativeness of the output textas judged by human annotators.A natural aspect of most text generation systemsis that a given input can map to a range of lexi-cally diverse outputs.
However, text-to-text tasksdefined with vague criteria such as the preserva-tion of the ?important?
information in text can alsopermit outputs that are semantically distinct.
Thiscan make evaluation difficult; for instance, system-generated sentences may differ (partially or com-pletely) in informational content from referencehuman-annotated text.
This phenomenon has beennoted and discussed in the task of pairwise sentencefusion (Daume?
III and Marcu, 2004) and also in sen-tence compression (McDonald, 2006).
Some exam-ples are listed in Table 1.In this work, we examine the task of sentence in-tersection: a variant of sentence fusion that does notpermit semantic variation in the output.
A strict1 in-tersection system is expected to produce a fused sen-tence that contains all the information common to itsinput sentences and avoid information that is in justone of the inputs.
In other words, a valid intersectionshould only contain information that is substantiatedby all input sentences.
The set-theoretic notions ofintersection (along with union) have been employedto describe variants of sentence fusion tasks in previ-ous work (Marsi and Krahmer, 2005; Krahmer et al,2008) but, to our knowledge, this work is the first toexplicitly tackle and evaluate the strict intersectiontask.We focus on the case of unsupervised pairwisesentence intersection and propose a strategy to yield1We use the term strict to make explicit the distinction fromtraditional fusion systems, which generally aim at notions ofintersection but are not formally evaluated with respect to it.43(a) Fusion example fromDaume?
III and Marcu (2004)(i) After years of pursuing separate and conflicting paths, AT&T and Digital Equip-ment Corp. agreed in June to settle their computer-to-PBX differences.
(ii) The two will jointly develop an applications interface that can be shared bycomputers and PBXs of any stripe.Human fusion #1 AT&T and Digital Equipment Corp. agreed in June to settle their computer-to-PBX differences and develop an applications interface that can be shared by anycomputer or PBX.Human fusion #2 After years of pursuing different paths, AT&T and Digital agreed to jointly developan applications interface that can be shared by computers and PBXs of any stripe.
(b) Compression examplefrom McDonald (2006)TapeWare , which supports DOS and NetWare 286 , is a value-added process thatlets you directly connect the QA150-EXAT to a file server and issue a commandfrom any workstation to back up the serverHuman compression #1 TapeWare supports DOS and NetWare 286Human compression #2 TapeWare lets you connect the QA150-EXAT to a file server(hypothesized)Table 1: Examples of text-to-text generation problems with multiple valid human-generated outputs that differ signif-icantly in semantic content.
Italicized text is used to indicate fragments that are semantically identical.valid intersections that follows the basic frameworkof previous unsupervised fusion systems (Barzilayand McKeown, 2005; Filippova and Strube, 2008b).In our approach, the input sentences are first alignedusing a modified version of a recent phrase-basedalignment approach (MacCartney et al, 2008).
Weassume the alignments that are produced define as-pects of the input that must appear in the output fu-sion and consider decoding strategies to recover in-tersections that preserve these alignments.
In addi-tion to a search-based decoding strategy, we proposea constrained integer linear programming (ILP) for-mulation that attempts to decode the most fluent sen-tence covering all these aspects while minimizingthe size and disfluency of interleaving text.
This is afairly general model which can also be extended toother alignment-based tasks such as pairwise unionand difference.As this is a substantially more constrained taskthan generic sentence fusion, we also present anovel evaluation approach that avoids out-of-contextsalience judgments.
We make use of a recently-released corpus of fusion candidates (McKeown etal., 2010) and propose a crowdsourced entailment-style evaluation to determine the validity of gener-ated intersections, as well as the grammaticality ofthe sentences produced.
Additionally, automatedmachine translation (MT) metrics are explored toquantify the amount of information missing fromvalid intersections.
Our decoding strategies showpromise under these experiments and we discuss po-tential directions for improving intersection perfor-mance.2 Related WorkThe distinction between intersection and union oftext was introduced in the context of sentence fu-sion (Krahmer et al, 2008; Marsi and Krahmer,2005) in order to distinguish between traditional fu-sion strategies that attempted to include only com-mon content and fusions that attempted to includeall non-redundant content from the input.
We fo-cus here on strict sentence intersection, explicitlyincorporating a constraint that requires that a pro-duced fusion must not contain information that isnot present in all input sentences.
This distin-guishes our approach from traditional sentence fu-sion approaches (Jing and McKeown, 2000; Barzi-lay and McKeown, 2005; Filippova and Strube,2008b) which generally attempt to retain commoninformation but are typically evaluated in an abstrac-tive summarization context in which additional in-formation in the fusion output does not negativelyimpact judgments.This task is also related to the field of sentencecompression which has received much attention inrecent years (Turner and Charniak, 2005; McDon-ald, 2006; Clarke and Lapata, 2008; Filippova andStrube, 2008a; Cohn and Lapata, 2009; Marsi et al,2010).
Intersections can be viewed as guided com-44pressions in which the redundancy of informationcontent across input sentences in a multidocumentsetting is assumed to directly indicate its salience,thereby consigning it to the output.Additionally, in this work, we frequently con-sider the sentence intersection task from the per-spective of textual entailment (cf.
?5.1).
The textualentailment task involves automatically determiningwhether a given hypothesis can be inferred from atextual premise (Dagan et al, 2005; Bar-Haim et al,2006).
Automatic construction of positive and neg-ative entailment examples has been explored in thepast (Bensley and Hickl, 2008) to provide trainingdata for entailment systems; however the produc-tion of text that is simultaneously entailed by two(or more) sentences is a far more constrained anddifficult challenge.ILP has been used extensively for text-to-text gen-eration problems in recent years (Clarke and Lapata,2008; Filippova and Strube, 2008b; Woodsend et al,2010), including techniques which incorporate syn-tax directly into the decoding to imporove the flu-ency of the resulting text.
In this paper, we focus ongenerating valid intersections and do not incorporatesyntactic and semantic constraints into our ILP mod-els; these are areas we intend to explore in the future.3 The Intersection TaskThe need for strict variants of fusion is motivatedby considerations of evaluation and utility in text-to-text generation tasks.
Without explicit constraints onthe semantic content of valid output, the operationaldefinition of fusion can encompass the full spectrumfrom sentence intersection to sentence union.
Thismakes the comparison of different fusion systemsdependent on task-based utility2.
In addition, inter-section comprises an interesting problem in its ownright.
It necessitates the use of generalization overphrases in order to convey only the content of theinput sentences when different wording is used andtherefore involves more than just word deletion.The analogy to set-theoretic intersection in thistask implies an underlying consideration of eachsentence as a set of informational concepts, sim-2For instance, systems may trade off conciseness againstgrammaticality, or informational content with degree of supportacross the input sentences.ilar to previous work in summarization and re-dundancy (Filatova and Hatzivassiloglou, 2004;Thadani and McKeown, 2008).
While we don?tcommit to any semantic representation for such el-ements of information, we can nevertheless attemptto identify repeated information using well-studiednatural language analysis techniques such as align-ment and paraphrase recognition, and furthermoreisolate this information through text-to-text genera-tion techniques.Consider, for example, the first sentence pair fromthe examples in Table 2.
A valid intersection forthese sentences must not contain any informationthat is not substantiated by both of them, so a fu-sion that mentions ?Mr Litvinenko?s poisoning?,?Britain?
or ?Sunday?
would not satisfy this crite-rion.
In other words, a valid intersection must neces-sarily be textually entailed by every input sentence.Following this, we can interpret the sentence inter-section task as one that requires the generation offluent text that is mutually entailed by all input sen-tences3.
We use this perspective in developing anevaluation technique for strict intersection in ?5.1.A major distinguishing factor between this workand previous work on fusion is that simply addingor deleting words in a sentence is not adequate; inmany cases, intersections require additional wordsor phrases to be introduced in order to general-ize over related but non-interchangeable alignedterms (such as ?go?
and ?expand?).
Additionally,we must attempt to avoid introducing additionalcontent-bearing text in the output while simultane-ously striving to maintain the fluency of text.3.1 DatasetA corpus of sentence fusion instances was recentlymade available by McKeown et al (2010), consist-ing of 297 sentence pairs taken from newswire clus-ters and manually judged as being good candidatesfor fusion.
Each sentence pair is accompanied byhuman-produced intersections and unions collectedvia Amazon?s Mechanical Turk service4.
McKeownet al (2010) noted that union responses are mostlyvalid but intersections are frequently incorrect and3From this perspective, the complementary task of sentenceunion involves the generation of fluent text that entails all theinput sentences.4http://www.mturk.com451 (i) Home Secretary John Reid said Sunday the inquiry would go wherever ?the police take it.?
(ii) It comes as Home Secretary John Reid said the inquiry into Mr Litvinenko?s poisoning would expand beyondBritain.2 (i) Traces of polonium have been found on the planes on which they are believed to have travelled betweenLondon and Moscow.
(ii) Small traces of radioactive substances had been found on the planes.3 (i) Prosecutors allege that the accuser, who appeared in the program, was molested after the show aired.
(ii) Prosecutors allege that the boy, a cancer survivor, was molested twice after the program aired.Table 2: Example sentence pairs from the McKeown et al (2010) corpus.
Table 3 contains the corresponding system-generated intersections for these sentence pairs.hypothesized that the task is more confusing foruntrained annotators.
A similar phenomenon wasnoted by Krahmer et al (2008): while demonstrat-ing that query-based human fusions exhibited lessvariation than generic fusions, it was also observedthat intersections varied more than unions.Due to the absence of adequate training data forintersection, our approach to the task is unsuper-vised, similar to previous work in fusion (Barzilayand McKeown, 2005; Filippova and Strube, 2008b)and sentence compression (Clarke and Lapata, 2008;Filippova and Strube, 2008a).
Additionally, we fo-cus on the case of pairwise sentence intersection andassume that the common information between theinput sentence pair can be represented within a sin-gle output sentence.
As a result, although the McK-eown et al (2010) corpus cannot be used for trainingan intersection model, we can make use of the sen-tence pairs it contains for evaluation.4 Models for intersectionOur proposed strategies for sentence intersection in-volve phrase-based alignment, intermediate general-ization steps that build a generation lattice and tech-niques for decoding an output sentence, as describedbelow.4.1 Phrase-based alignmentThe alignment phase is a major component of anyintersection system as it is used to uncover thecommon segments in the input that must be pre-served in the output.
We make use of an adapta-tion of the supervised MANLI phrase-based align-ment technique originally developed for textual en-tailment systems (MacCartney et al, 2008); ourimplementation replaces approximate search-baseddecoding with exact ILP-based alignment decod-ing and incorporates syntactic constraints to pro-duce more precise alignments (Thadani and McKe-own, 2011).
The aligner is trained on a corpus ofhuman-generated alignment annotations producedby Microsoft Research (Brockett, 2007) for infer-ence problems from the second Recognizing Tex-tual Entailment (RTE2) challenge (Bar-Haim et al,2006).Entailment problems are inherently asymmetricbecause premise text is generally larger than hypoth-esis text; however, this does not apply to our inter-section problems and consequently our MANLI im-plementation drops asymmetric indicator features.The absence of these features impacts alignmentperformance on RTE2 data but our reimplementa-tion performs comparably to the original model un-der the alignment evaluation from MacCartney et al(2008).4.2 Ontology-based generalizationAn aligned phrase pair produced by the previousstep does not necessarily indicate that the phrasesare equivalent but merely that they are similar inthe given sentence context (such as ?accuser?
and?boy?
in the third example from Table 2).
We needto generalize over these phrases as they are not inter-changeable from the perspective of the intersectiontask.
We consider an alignment as containing threetypes of aligned phrases:1.
Identical phrases or paraphrases: Either ofthese may appear in the output2.
Entailed phrases: Only the entailed phrasemust appear in a valid intersection3.
Instances of a general concept: The commonconcept must be lexicalized in the output46Although generalization of words within stan-dalone sentences is usually hampered by word senseambiguity, our approach is less likely to encounterthis problem because we can generalize simultane-ously over phrases which have already been alignedusing additional information (such as their neighbor-ing context), thus avoiding generalizations that donot fit the alignment.For our experiments, we make use of the Wordnetontology (Miller, 1995) to find the hypernyms com-mon to every aligned pair of non-identical phrases,and only attempt to detect entailments which arecomprised of specific instances that entail generalconcepts.
This approach can be augmented by theuse of entailment corpora and distributional cluster-ing which we intend to explore in future work.
Wealso use the lexical resource CatVar (Habash andDorr, 2003) to try to generate morphological vari-ants of aligned words that enable them to be inter-changed without creating disfluencies.4.3 Pragmatic abstractionOur strategy assumes that aligned text must be pre-served in output intersections whereas unalignedtext must be minimized.
However, unaligned textcannot simply be dropped as it may contain vitalportions for generating fluent text.
In addition, un-aligned phrases can be caused by paraphrased ormetaphorical text that the aligner is not capable ofidentifying.
For example, the phrases ?polonium?and ?radioactive substances?
in the second sentencepair from Table 2 fail to align with each other.On the other hand, retaining unaligned text fromone of the input sentences for the sake of fluencyis likely to introduce information that is not sup-ported by the other input sentence.
We thereforeneed to abstract away as much content from the un-aligned portions of the text as possible.
For thispurpose, we generate a large number of potentialcompressions and abstractions for every unalignedspan that occurs between two consecutive alignedphrases in each sentence.
These compressions andabstractions, referred to as interleaving paths, be-tween pairs of aligned phrases essentially constructa lattice over the input sentences that encodes all po-tential intersection outputs.Generation of interleaving paths is accomplishedthrough the application of rules on the dependencyparse structure over unaligned text spans from a sin-gle sentence (as well as spans that occur before thefirst aligned phrase and after the last aligned phrasein each sentence).
Interleaving paths are generatedby applying rules that:1.
Drop insignificant dependent words and un-aligned prepositional phrases2.
Replace content-bearing verbs with tense-adjusted generic variants such as ?did some-thing?
and ?happened?, with an exception forstatement verbs3.
Replace nouns with generic words such as?someone?
or ?something?, using Wordnet todetermine which generic variant fits a noun4.
Suggest connective text fragments such as?something about?
to cover long spans andclause boundariesOur abstraction rules are relatively simple but canoften generate reasonable interleaving paths.
In gen-eral, we note that shorter abstractions are less likelyto include glaring grammatical errors because longunaligned spans are often indicative of problematicalignments that either incorrectly relate unconnectedterms or fail to recognize paraphrases.4.4 Decoding strategiesAfter sentence alignment, generalization overaligned phrases and the construction of interleav-ing paths, we are left with a lattice that encodespotential intersections of the input sentence.
Fig-ure 1 describes the general structure of this lattice.Every alignment link encompasses a set of alignedphrases.
Phrases may be identical or generaliza-tions, in which case they can appear in the contextof either sentence, or they may be sentence-specific(for example, verbs with different tenses or nominal-izations like ?nominated?
and ?nominations?).
Ad-ditionally, the abstraction phase generates interleav-ing paths from unaligned spans between all pairs ofalignment links.
These paths are generated from in-dividual sentences and can only be used to connectphrases that appear in the context of those sentences.Our task now reduces to recovering a well-formedintersection from this lattice.
We make use of a lan-guage model (LM) to judge fluency and propose twotechniques to decode high-scoring text from the lat-tice: a simple beam-search technique and an ILP47Alignment link kmmmmPhrasesfrom S1SharedphrasesPhrasesfrom S2Alignment link lPhrasesfrom S1SharedphrasesPhrasesfrom S2Figure 1: The general structure of one segment of thealignment lattice, illustrating the potential interleavingpaths between aligned phrases.
Solid lines indicate pathsderived from sentence 1 and dashed lines indicate pathsderived from sentence 2strategy that leverages our initial assumption that allaligned phrases must appear in the output.4.4.1 Beam searchSearch-based decoding is often employed in phrase-based MT systems (Och and Ney, 2003) and isimplemented in the Moses toolkit5; similar ap-proaches have also been used in text-to-text gener-ation tasks (Barzilay and McKeown, 2005; Soricutand Marcu, 2006).
This technique attempts to findthe highest-scoring sentence string under the LM byunwrapping and searching through a lattice.
Sincethe dynamic programming search could require anexponential number of search states, a fixed-widthbeam can be used to control the number of searchstates being actively considered at each step.In order to decode an intersection problem, wefirst pick a beam size B and initialize the list of can-didate search states with the first interleaving pathsin each sentence.
At every iteration, we consider theB candidates with the highest normalized scores un-der the LM and remove them from the candidate list.Each candidate is then advanced, i.e., all alignedphrases and interleaving paths following it are ex-amined, scored and added to the candidate list.
Wecontinue searching in this manner until B candidateshave covered all aligned phrases; the highest scoringcandidate is then retrieved as the target intersection.4.4.2 Segmented decodingWhile beam search is a viable strategy for decodingintersections, its performance is contingent on the5http://www.statmt.org/moses/beam size parameter and it is not guaranteed to re-turn the highest scoring sentence under the LM.
Forinstance, if a potential intersection starts with un-usual text, it is unlikely to be explored by the search-based approach even if it is the optimal solution tothe decoding problem.
To address this, we also pro-pose an alternative decoding problem that can beformulated as the optimization of a linear objectivefunction with linear constraints.
This can then besolved exactly by well-studied algorithms using off-the-shelf ILP solvers6.This decoding problem does not look for thehighest scoring sentence under the LM; instead, itattempts to find the set of interleaving paths andaligned phrases that are most locally coherent7 un-der the LM.
Good phrase-path combinations that oc-cur towards the tail end of an intersection can thusbe put on even footing with the combinations thatappear in the beginning.
Although the two problemsconsider different objective functions, they are bothengaged in the same overall goal: that of recoveringa fluent sentence from the lattice.We first define boolean indicator variables aki ?Ak for every aligned phrase in each aligned link Akpresent in the intersection problem I.
We also in-troduce indicator variables pklij for every possible in-terleaving path between aligned phrases aki and alj .The linear objective for I that maximizes the localcoherence of all phrases can be expressed asf = max?Ak,Al?I|Ak|?i=0|Al|?j=0pklij ?
score(pklij )where score(pklij ) is the normalized LM score of thefragment of text representing aki pklij alj .
In otherwords, the score for each interleaving path is cal-culated by appending it and the two phrases it con-nects into a single fragment of text and determiningthe score of that fragment under an LM8.6We use LPsolve: http://lpsolve.sourceforge.net/7As noted by Clarke and Lapata (2008), normalizing LMscores cannot be easily accomplished with linear constraintsand we do not have training data to devise appropriate word-insertion penalties as used in MT.8If the fragment of text is smaller than the LM size, weconsider additional sentence context around the aligned phrasesrather than backing off to a smaller LM size to avoid a bias to-wards short but ungrammatical interleaving paths.48We now introduce linear constraints to keep theproblem well-formed.
First, we add a restrictionto ensure that only one phrase from each alignmentlink is present in the solution.
?aki ?Akaki = 1 ?Ak ?
IWe can also ensure that interleaving paths are only inthe solution when the aligned phrases that they con-nect together are themselves present using the fol-lowing set of constraints.aki ?|Ak|?i=0pk?i?
= 1 ?aki ?
Ak, Ak ?
Ialj ?|Al|?j=0p?l?j = 1 ?alj ?
Al, Al ?
Ipklij ?
aki <= 0 ?i, j, k, lpklij ?
alj <= 0 ?i, j, k, lAs we don?t restrict the structure of the lattice in anyway and allow crossing alignment links, the programas defined thus far is capable of generating cyclicand fragmented solutions.
To combat this, we adddummy start and end phrase variables and introduceadditional single commodity flow constraints (Mag-nanti and Wolsey, 1994) adapted from Martins etal.
(2009) over the interleaving paths to guaranteethat the output will only involve a linear sequence ofaligned phrases and paths.5 EvaluationWe now turn to the design of experiments for thestrict sentence intersection task and discuss the per-formance of the proposed models using the corpusprovided by McKeown et al (2010).
We use a beamsize of 50 for the beam search decoder and a 4-gramLM for all experiments.
Dependency parsing is ac-complished with MICA, a TAG-based parser (Ban-galore et al, 2009).
Our primary considerationsfor studying system-generated fusions are validity(whether the output contains only the informationcommon to each sentence), coverage (whether theoutput contains all the common information in theinput sentences) and the fluency of the output.5.1 Evaluating Validity and FluencyEvaluating the validity of an intersection involvesdetermining whether it contains only the informa-tion contained in each sentence and nothing else.
Inorder to do this, we make use of the interpretation ofvalid intersections as being mutually entailed by theinput sentences.
It follows that the task of judgingthe validity of an intersection can simply be decom-posed into two tasks that judge whether the intersec-tion is entailed by each input sentence.We make use of Amazon?s Mechanical Turk(AMT) platform to have humans evaluate the in-tersections produced.
Crowdsourcing annotationsand judgments in this manner has been shown to becheap and effective for natural language tasks (Snowet al, 2008) and has recently been employed in sim-ilar entailment-detection tasks (Negri and Mehdad,2010; Buzek et al, 2010).
Since we only seek judg-ments on produced intersections and avoid present-ing both input sentences to users, we do not antic-ipate the noisiness that was noted by McKeown etal.
(2010) when asking AMT users to generate in-tersections.Each entailment task is framed as a multiplechoice question.
An AMT user is shown just oneinput sentence (the premise in entailment terminol-ogy) along with a potential intersection (the hypoth-esis) and is required to respond to whether there isany new or different information in the latter that isnot in the former.
They can respond on a 3-pointscale (yes/no/maybe) where maybe is clarified to in-clude ambiguous rewording in the intersection.
Fora given intersection instance, the responses9 usingeach input sentence as the premise are averaged sep-arately and then combined10 to give a measure ofhow well the intersection is entailed by both sen-tences.A second question allows the user to specify thegrammaticality of the intersection on a 4-point scale.As this measure doesn?t depend on the input sen-tence presented to the AMT user, all scores providedare simply averaged per intersection.9Each instance is presented to 6 AMT users, 3 per premise.Responses were automatically filtered for spam and removingthe largest outlier from each per-premise or per-intersectiongroup did not yield a notable change in relative performance.10We use the harmonic mean for combination, but the resultsare largely similar when using an arithmetic mean.49Intersection output Fluency ValidityAligned words (i) Home Secretary John Reid said the inquiry would go.
0.667 0.800(ii) Home Secretary John Reid said the inquiry would expand.
0.778Beam search Home Secretary John Reid said something about the inquiry would movewherever ?the something take it?.0.389 0.667Segmented decoder Home Secretary John Reid said the inquiry would change.
0.944 0.909Aligned words (i) Traces of have been found on the planes.
0.445 1.000(ii) traces of had been found on the planes.
0.556Beam search Small traces of some things have been found on the planes.
0.611 0.909Segmented decoder Small traces of had been found on the planes.
0.500 0.741Aligned words (i) Prosecutors allege that the accuser the program was molested after aired.
0.167 0.800(ii) Prosecutors allege that the boy was molested after the program aired.
1.000Beam search Prosecutors allege that the being, who did something in the program, wasmolested after something about aired.0.400 0.909Segmented decoder Prosecutors allege that the organism, who did something, was molestedafter the program aired.0.667 0.857Table 3: Intersections produced for the examples introduced in Table 2 along with judgments from AMT users.Validity Fluency Har.
MeanOther sentence 0.188 0.945 0.314Aligned words 0.863 0.563?
0.682?Beam search 0.729 0.450 0.557Segmented decoder 0.812?
0.504 0.622Oracle combination 0.813?
0.575?
0.674?Table 4: Results of the AMT evaluation described in ?5.1.Statistically insignificant differences within columns areindicated with ?
; all other entries are significantly distinctat p ?
0.05.5.2 Results of AMT evaluationTable 4 contains the results from this evaluationover the McKeown et al (2010) corpus11 and Ta-ble 3 shows the system-produced intersections cor-responding to the examples from ?3.
We report nor-malized scores of validity and fluency for ease ofcomparison, as well as their unweighted harmonicmean as a crude measure of combined human judg-ment.
In addition to the beam search and segmenteddecoders, we report the performance of two upper-bound systems that present artificial hypothesis sen-tences to AMT users.
Other sentence is simply thesentence that is not the current premise from the sen-tence pair; although this is rarely an appropriate in-tersection in the data, it is useful as a measure ofhow well humans judge grammaticality and infor-11The first 20 sentence pairs of the corpus were examinedwhen devising abstraction rules and are therefore excluded fromthese results.mation content.
Aligned words is the aligned subsetof the premise sentence; this is quite likely to be con-sidered a valid entailment by AMT users as no newwords are introduced.
Although the latter also scoressurprisingly well on fluency, we must note that thisis not an actual intersection solution: the alignedwords displayed to AMT users for a given intersec-tion instance are different depending on which inputsentence is displayed as the premise.Turning to the systems under study, we observethat the ILP-based segmented decoder produces textthat is judged more fluent on average than the beamsearch decoder.
In order to judge the degree of over-lap between the two systems, we also report theperformance of a pseudo-hybrid oracle combinationsystem which assumes the presence of an oracle thatruns both decoders and always chooses the outputintersection that is more grammatical.
The improvedperformance illustrates that each decoder has its ad-vantages and that a real hybrid system might yieldimprovements over either approach.5.3 Evaluating CoverageWhile validity experiments test whether the pro-posed intersections contain extraneous or unsup-ported information, we also need to check whetherthe intersections contain all the information that isshared between the input sentences.
This cannot befactored into a task that involves only one input sen-tence and therefore cannot be easily accomplished50BLEU NISTAligned words 0.682 11.10Beam search 0.726 10.53Segmented decoder 0.818 11.56Table 5: Results of the automated evaluation for coverageof intersections described in ?5.3.without annotators who understand the concept ofintersection.We instead attempt to utilize the high-qualityhuman-generated union dataset from McKeown etal.
(2010) in evaluating the coverage of our inter-section systems.
Using the simple absorption lawA ?
(A ?
B) = A, we assume that the coverageof intersection systems can be judged by how wellthey can recover an input sentence from human-generated unions.
The resulting outputs are com-pared to the original input sentences in an MT-style evaluation under two commonly-used metrics:BLEU (Papineni et al, 2002) and NIST (Dodding-ton, 2002).The results of this automated evaluation areshown in Table 5.
The aligned words system herealways considers words from the union sentence andcan therefore be seen as a baseline system.
We ob-serve that the segmented decoder produces outputthat is judged most similar to the input sentencesunder BLEU, which measures n-gram overlap, al-though results under NIST (which gives additionalweight to rarer n-grams) are less conclusive.6 DiscussionThe experimental results indicate that the two sys-tems we describe, particularly the segmented de-coder, do a reasonable job of finding valid intersec-tions with good coverage; however, producing fluentoutput remains a challenge.
Analysis of the inter-sections produced leads us to note that the qualityof interleaving paths is the prime obstacle to im-proving intersection output (cf.
Table 3): produc-ing syntactically-valid textual abstractions to con-nect text is a challenge that is not met by our sim-ple rule-based approach.
Furthermore, we noticethat the quality of alignment also factors in to thisproblem: systems that miss phrases which shouldbe aligned or systems that mistakenly align farawayfragments both cause spans of unaligned text thatmust be then abstracted over.We hypothesize that these issues could be tackledwith the use of joint models: a system that alignsas it decodes could reduce the need for abstrac-tion over long unaligned spans, although care wouldhave to be taken to ensure that coverage is main-tained.
Additionally, richer lexical resources suchas wider-coverage ontologies (Snow et al, 2006)and entailment/paraphrase dictionaries could aid inimproving coverage.
Finally, previous work in fu-sion (Filippova and Strube, 2008b; Filippova andStrube, 2009) has noted that models based on syntaxoutperform techniques that rely solely on LM scoresto determine fluency, and strict intersection appearsto be well-suited for further exploration in this vein.7 ConclusionWe have examined the text-to-text generation task ofstrict sentence intersection, which restricts semanticvariation in the output and necessarily invokes theproblems of generalization and abstraction in addi-tion to the usual challenge of producing fluent text.We tackle the task as lattice decoding and discusstwo decoding strategies for producing valid intersec-tions.
In addition, we assume that strict intersec-tion tasks are best considered as problems of mu-tual entailment generation and describe evaluationstrategies for this task that make use of both humanjudgments as well as automated metrics run over arelated corpus.
Experimental results indicate thatthese systems are fairly effective at generating validintersections and that our novel segmented decoderstrategy outperforms the traditional beam search ap-proach.
Although fluency remains a challenge, wehypothesize that the use of joint models, syntac-tic constraints and lexical resources could bring im-provements.AcknowledgmentsThe authors are grateful to the anonymous reviewersfor their helpful feedback.
This material is based onresearch supported in part by the U.S. National Sci-ence Foundation (NSF) under IIS-05-34871.
Anyopinions, findings and conclusions or recommenda-tions expressed in this material are those of the au-thors and do not necessarily reflect the views of theNSF.51ReferencesSrinivas Bangalore, Pierre Boullier, Alexis Nasr, OwenRambow, and Beno?
?t Sagot.
2009.
MICA: a prob-abilistic dependency parser based on tree insertiongrammars.
In Proceedings of HLT-NAACL: Short Pa-pers, pages 185?188.Roy Bar-Haim, Ido Dagan, Bill Dolan, Lisa Ferro, DaniloGiampiccolo, Bernardo Magnini, and Idan Szpektor.2006.
The second PASCAL Recognising Textual En-tailment challenge.
In Proceedings of the SecondPASCAL Challenges Workshop on Recognising Tex-tual Entailment.Regina Barzilay and Kathleen R. McKeown.
2005.
Sen-tence fusion for multidocument news summarization.Computational Linguistics, 31(3):297?328.Jeremy Bensley and Andrew Hickl.
2008.
Unsupervisedresource creation for textual inference applications.
InProceedings of LREC.Chris Brockett.
2007.
Aligning the 2006 RTE cor-pus.
Technical Report MSR-TR-2007-77, MicrosoftResearch.Olivia Buzek, Philip Resnik, and Benjamin B. Bederson.2010.
Error driven paraphrase annotation using me-chanical turk.
In Proceedings of the NAACL HLT 2010Workshop on Creating Speech and Language Datawith Amazon?s Mechanical Turk, pages 217?221.James Clarke and Mirella Lapata.
2008.
Global infer-ence for sentence compression: an integer linear pro-gramming approach.
Journal of Artifical IntelligenceResearch, 31:399?429, March.Trevor Cohn and Mirella Lapata.
2009.
Sentence com-pression as tree transduction.
Journal of Artificial In-telligence Research, 34:637?674.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailment chal-lenge.
In Proceedings of the PASCAL ChallengesWorkshop on Recognising Textual Entailment.Hal Daume?
III and Daniel Marcu.
2004.
Generic sen-tence fusion is an ill-defined summarization task.
InProceedings of the ACL Text Summarization BranchesOut Workshop, pages 96?103.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrencestatistics.
In Proceedings of HLT, pages 138?145.Elena Filatova and Vasileios Hatzivassiloglou.
2004.A formal model for information selection in multi-sentence text extraction.
In Proceedings of COLING,page 397.Katja Filippova and Michael Strube.
2008a.
Dependencytree based sentence compression.
In Proceedings ofthe Fifth International Natural Language GenerationConference, INLG ?08, pages 25?32.Katja Filippova and Michael Strube.
2008b.
Sentence fu-sion via dependency graph compression.
In Proceed-ings of EMNLP, pages 177?185.Katja Filippova and Michael Strube.
2009.
Treelinearization in English: improving language modelbased approaches.
In Proceedings of NAACL, pages225?228.Nizar Habash and Bonnie Dorr.
2003.
A categorial vari-ation database for English.
In Proceedings of NAACL,NAACL ?03, pages 17?23.Hongyan Jing and Kathleen R. McKeown.
2000.
Cutand paste based text summarization.
In Proceedingsof NAACL, pages 178?185.Emiel Krahmer, Erwin Marsi, and Paul van Pelt.
2008.Query-based sentence fusion is better defined andleads to more preferred results than generic sentencefusion.
In Proceedings of ACL, pages 193?196.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment modelfor natural language inference.
In Proceedings ofEMNLP, pages 802?811.Thomas L. Magnanti and Laurence A. Wolsey.
1994.Optimal trees.
In Technical Report 290-94,Massechusetts Institute of Technology, Operations Re-search Center.Erwin Marsi and Emiel Krahmer.
2005.
Explorationsin sentence fusion.
In Proceedings of the EuropeanWorkshop on Natural Language Generation, pages109?117.Erwin Marsi, Emiel Krahmer, Iris Hendrickx, and Wal-ter Daelemans.
2010.
On the limits of sentencecompression by deletion.
In Emiel Krahmer andMarie?t Theune, editors, Empirical Methods in NaturalLanguage Generation, pages 45?66.
Springer-Verlag,Berlin, Heidelberg.Andre?
F. T. Martins, Noah A. Smith, and Eric P. Xing.2009.
Concise integer linear programming formula-tions for dependency parsing.
In Proceedings of ACL-IJCNLP, pages 342?350.Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceedingsof EACL, pages 297?304.Kathleen McKeown, Sara Rosenthal, Kapil Thadani, andColeman Moore.
2010.
Time-efficient creation of anaccurate sentence fusion corpus.
In Proceedings ofNAACL-HLT.George A. Miller.
1995.
Wordnet: a lexical databasefor english.
Communications of the ACM, 38:39?41,November.Matteo Negri and Yashar Mehdad.
2010.
Creating abi-lingual entailment corpus through translations withmechanical turk: $100 for a 10-day rush.
In Proceed-ings of the NAACL HLT 2010 Workshop on Creating52Speech and Language Data with Amazon?s Mechani-cal Turk, pages 212?216.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29:19?51, March.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic evalu-ation of machine translation.
In Proceedings of ACL,ACL ?02, pages 311?318, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Rion Snow, Daniel Jurafsky, and Andrew Y. Ng.
2006.Semantic taxonomy induction from heterogenous evi-dence.
In Proceedings of ACL, pages 801?808.Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of EMNLP, pages 254?263.Radu Soricut and Daniel Marcu.
2006.
Stochastic lan-guage generation using widl-expressions and its appli-cation in machine translation and summarization.
InProceedings of ACL, pages 1105?1112.Kapil Thadani and Kathleen McKeown.
2008.
A frame-work for identifying textual redundancy.
In Proceed-ings of COLING, pages 873?880.Kapil Thadani and Kathleen McKeown.
2011.
Optimaland syntactically-informed decoding for monolingualphrase-based alignment.
In Proceedings of ACL.Jenine Turner and Eugene Charniak.
2005.
Supervisedand unsupervised learning for sentence compression.In Proceedings of ACL, pages 290?297.Kristian Woodsend, Yansong Feng, and Mirella Lapata.2010.
Title generation with quasi-synchronous gram-mar.
In Proceedings of EMNLP, EMNLP ?10, pages513?523.53
