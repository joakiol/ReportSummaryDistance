Optimizing disambiguation in SwahiliArvi HURSKAINENInstitute for Asian and African StudiesBox 59FI-00014 University of HelsinkiArvi.Hurskainen@helsinki.fiAbstractIt is argued in this paper that an optimalsolution to disambiguation is a combination oflinguistically motivated rules and resolutionbased on probability or heuristic rules.
Bydisambiguation is here meant ambiguityresolution on all levels of language analysis,including morphology and semantics.
Thediscussion is based on Swahili, for which acomprehensive analysis system has beendeveloped by using two-level description inmorphology and constraint grammarformalism in disambiguation.
Particularattention is paid to optimising the use ofdifferent solutions for achieving maximalprecision with minimal rule writing.1 IntroductionIn ambiguity resolution of natural language,both explicit linguistic information andprobability calculation have been used as basicapproaches.
In early experiments usually onlyone strategy was applied, so that ambiguityresolution was performed either with the help oflinguistic rules, or through probabilitycalculation.
Advanced approaches make use ofboth strategies, and they differ mainly in whatkind of role each of these two methods has in thesystem (Wilks and Stevenson 1998, Stevensonand Wilks 2001).
Sources of structured data, suchas the WordNet (Miller 1990; Resnik 1998b;Banerjee and Pedersen 2002), have also beenmade use of.It is commonly known that the morecomprehensive the description of language is, themore ambiguous is the interpretation ofindividual words1.
Ambiguity occurs betweenword classes, between variously inflected word-forms, and above all, between various meaningsof a word.
A fairly large number2 of words indifferent word categories have more than oneclearly distinguished meaning.
Semanticdisambiguation tends to be the hardest part of thedisambiguation process, largely because of thefact that in semantics there are fewdistinguishable categories that could be used forgeneralising disambiguation rules.
Below I shalldescribe a method where the use of linguisticrules and probability has been optimised withminimal loss of linguistic precision.Morphological description is carried out in theframework of two-level formalism3.
After havingbeen under development for 19 years(Hurskainen 1992, 1996), the parser of Swahilihas now reached a phase where the recall as wellas the precision4 is close to 100% in unrestrictedstandard Swahili text.1 By word is here meant any string of characters,excluding punctuation marks and diacritics.
Also,multi-word concepts, if they are handled as singleentities, are considered words.2 I do not consider it meaningful to presentstatistical details of ambiguity, because, whensemantic glosses are included, the borderline betweenreal ambiguity and such ambiguity as is foundbetween synonyms and near-synonyms is vague.3 The development environment for designing themorphological parser was provided by Lingsoft andKimmo Koskenniemi (1983).4 The criterion of precision in morphologicalanalysis is considered fulfilled if one of the readingsof a word is correct in the context concerned, and allother readings are grammatically correct analyses insome other context.Disambiguation rules, as well as the rules forsyntactic mapping (not discussed here) and foridentifying idioms, were written within theframework of constraint grammar by using theCG-2 parser5.
In other words, morphologicaldisambiguation and semantic disambiguationwere implemented within a single rule system.This was possible because the CG-2 parser treatsall strings in the analysis result, including glossesin English, as tags that can be made use of in rulewriting (Tapanainen 1996: 6).6The properties of the CG-2 parser include thefollowing:(a) With a rule one may either select or removea reading from a cohort7.
(b) The application of a rule can be constrainedin several ways by making use of the occurrenceor absence of features.
Reference to the positionof the constraining feature can be precisely madeforwards and backwards within the sentence.
(c) The identification of constraining featurescan be made relational by more than one phase ofscanning, whereby after finding one feature,scanning may be continued again in eitherdirection.
By default, scanning terminates at asentence boundary, but its termination can alsobe defined elsewhere.
(d) Rule conditions can be expressed eitherdirectly with concrete tags or indirectly by usingset names.
The latter facility simplifies rulewriting, especially of general rules.
(e) The possibility of concatenating tag sets aswell as concrete tags decreases considerably theneed of defining tag sets.
(f) The application of rule order can be definedby placing the rules into sections, so that themore general and reliable rules come first andother rules later in the order of decreasingreliability.
This also makes it possible to writeheuristic rules within the same rule system.5 The environment for writing and testingdisambiguation rules was provided by Connexor andPasi Tapanainen (1996).6 In disambiguation, the precision criterion isconsidered fulfilled if the reading chosen in thatcontext is correct.
In two independent tests with recentnews texts of 5,000 words each, the precision was99.8% and 99.9%.7 A cohort is a word-form plus all its morphologicalinterpretations.
(g) Mapping rules, which are the standard rulesfor syntactic mapping, also include a possibilityof adding a new reading as well as of replacingthe reading of a line.
The latter facility isdemonstrated below when discussing idioms.2 Maximal morphological and semanticdescription as preconditionThe basic strategy in processing is that themorphological description is as full and detailedas possible.
Each string in text is interpreted andall possible interpretations of each string aremade explicit.
The maximal recall and precisionare achieved by updating the dictionary fromtime to time with the help of the changing targetlanguage8.
As a result of analysis there is a textwhere every string has at least one interpretationand no legitimate interpretation is excluded.Example (1) illustrates the point.
(1)Kiboko"kiboko"  N 7/8-SG  { fat person } HUM"kiboko"  N 7/8-SG  { whip , strip of hippo hide }"kiboko"  N 7/8-SG  { hippo , hippopotamus } AN"kiboko"  N 7/8-SG{ beautiful/attractive/outstanding thing }"kiboko"  N 7/8-SG  { ornamental stitch }"boko"  ADV ADV:ki  9/10-SG  { gourd fordrinking water or local brew }"boko"  ADV ADV:ki  9/10-PL  { gourd fordrinking water or local brew }aishiye"ishi"  V 1/2-SG3-SP VFIN { live , reside , stay }SV AR GEN-REL 1/2-SGkwenye"kwenye" PREP { in , at }"enye" PRON 15-SG { which has }"enye" PRON 17-SG { place which has }maziwa8 By target language I mean the kind of text, forwhich the application is intended.
It is hardly possibleto maintain a dictionary that is optimal for handling alltypes of domain-specific texts.
Although the large sizeof the dictionary would not be a problem, it would bedifficult to handle e.g.
such words that in one type oftext are individual lexemes but in another domain arepart of multi-word concepts that should be treated asone unit.
In addition to new words, misspellings alsocause problems.
Some commonly occurringmisspellings and non-standard spellings can beencoded into the dictionary and thus give the word aprecise interpretation.
"ziwa"  N 5a/6-PL  { lake }"ziwa"  N 5a/6-PL  { breast }"maziwa" N 6-PL  { milk }amekula"la"  V 1/2-SG3-SP VFIN PERF:me 1/2-SG2-OBJOBJ { eat } SV SVO MONOSLB"la"  V 1/2-SG3-SP VFIN PERF:me 15-SG-OBJOBJ { eat } SV SVO MONOSLB"la"  V 1/2-SG3-SP VFIN PERF:me 17-SG-OBJOBJ { eat } SV SVO MONOSLB"la"  V 1/2-SG3-SP VFIN PERF:me INFMARK{ eat } SV SVO MONOSLBnyanya"nyanya"  N 5a/6-SG  { tomato }"nyanya"  N 9/10-SG  { tomato }"nyanya"  N 9/10-SG  { grandmother } HUM"nyanya"  N 9/10-PL  { tomato }"nyanya"  N 9/10-PL  { grandmother } HUM"nyanya"  N 9/6-SG  { grandmother }  HUM.$Without disambiguation, the followinginterpretations are possible:(a) A fat person, who lives in lakes, has eatentomatoes.
(b) A fat person, who lives in lakes,has eaten grandmothers.
(c) A fat person, wholives in breasts, has eaten tomatoes.
(d) A fatperson, who lives in breasts, has eatengrandmothers.
(e) A fat person, who lives inmilk, has eaten tomatoes.
(f) A fat person, wholives in milk, has eaten grandmothers.
(g) Ahippo, which lives in lakes, has eaten tomatoes.
(h) A hippo, which lives in lakes, has eatengrandmothers.
(i) A hippo, which lives in breasts,has eaten tomatoes.
(j) A hippo, which lives inbreasts, has eaten grandmothers.
(k) A hippo,which lives in milk, has eaten tomatoes.
(l) Ahippo, which lives in milk, has eatengrandmothers.The situation would be even worse if "aishiye"with relative marker (GEN-REL 1/2-SG) weremissing.
It requires that the preceding referent beanimate and thus excludes inanimate alternatives.The subject prefix in the main verb "amekula"also refers to an animate subject.
But because itcan also stand without an overt subject, this clueis not reliable.When we look for the possible subject in thesentence, we seem to have three candidates.
"Kiboko" certainly is one of them, because it is anoun and some of its readings agree9 with the9 In this case agreement means something otherthan morphological agreement.
The noun belongs tosubject prefix of the main verb.
In regard to itsposition, "ziwani" would also suit, but it is ruledout because it has a locative suffix.
Finally, noovert subject would be necessary, whereby thephrase preceding the main verb would be anobject dislocated to the left and the sentencewould mean, "The grandmother has eaten thehippo/fat person who lives in thelakes/breasts/milk".3 Disambiguation with linguistic rulesFrom the analysed sentence we can see thatpart of the ambiguity is easy to resolve withrules.
For example, "kiboko" cannot be anadverbial form (ADV:ki) of "boko" (= in themanner of a gourd), because it is the referent ofthe following relative verb "aishiye", which forits part requires that the referent has to beanimate.
Therefore, the interpretation "whip" andmore rare meanings, "beautiful thing" and"ornamental stitch", are also ruled out.
So we areleft with two animate meanings, "fat person" and"hippo", for which there are no reliable tagsavailable for writing disambiguation rules.One of the three interpretations of  "kwenye"can be removed (15-SG), because no infinitiveprecedes it.
The word "maziwa" with threeinterpretations has no grammatical criteria fordisambiguation.The interpretations with object marker (OBJ)of "amekula" (has eaten you) can be removed onthe basis of the following noun (withoutlocative), which is reliably the real object.For "nyanya" there are no reliable criteria fordisambiguation.
Because it is in object positionand without qualifications, no clues fordisambiguation can be found among agreementmarkers.After applying linguistic disambiguationrules10, we have an analysis as in (2).
(2)Kiboko"kiboko"  N 7/8-SG  { fat person } HUM"kiboko"  N 7/8-SG  { hippo , hippopotamus }Class 7 (7/8-SG) and the subject prefix of the verb toClass 1 (1/2-SG3-SP), but the semantic principle, i.e.animacy, overrides the formal criterion.10 Because of space restrictions, those rules are notreproduced here.AN ARaishiye"ishi"  V 1/2-SG3-SP VFIN { live ,  reside , stay }SV GEN-REL 1/2-SGkwenye"kwenye" PREP { in , at }"enye" PRON 17-SG { place which has }maziwa"ziwa"  N 5a/6-PL  { lake }"ziwa"  N 5a/6-PL  { breast }"maziwa" N 6-PL  { milk }amekula"la"  V 1/2-SG3-SP VFIN PERF:me INFMARK{ eat } SV SVO MONOSLBnyanya"nyanya"  N 5a/6-SG  { tomato }"nyanya"  N 9/10-SG  { tomato }"nyanya"  N 9/10-SG  { grandmother } HUM"nyanya"  N 9/10-PL  { tomato }"nyanya"  N 9/10-PL  { grandmother } HUM"nyanya"  N 9/6-SG  { grandmother }  HUM.$4 Disambiguation with context-sensitivesemantic rulesNow follows the hard part of disambiguation,because no reliable linguistic rules can bewritten.
The easiest case is "kwenye", becausethe two interpretations represent different phasesof the grammaticalization process, and thesemantic difference between them is marginal.The preposition "kwenye" is in fact formally alocative (17-SG) form of the relative word "enye"(which has).For "Kiboko" we can make use of the commonknowledge that fat persons do not normally livein lakes, or in breasts, or in milk.
Therefore, arule based on the co-occurrence of "kiboko" and"maziwa"11 with appropriate meanings can bewritten.The word "maziwa" is even more difficult todisambiguate.
The word "kiboko" in the sense ofhippo can easily co-occur with all three meaningsof "maziwa".
Here we have to rely onprobability12.11 A set of words referring to places where a hipporesides can be defined and used in the rule.12 It is possible to write also a context-sensitiverule, where use is made of the fact that rhinos can livein lakes but not in breasts or milk, but such a ruleeasily becomes too specific.The word "nyanya" in object position is almostimpossible to disambiguate elegantly.
Thesubject of eating can be one or more tomatoes, aswell as one or more grandmothers.
It is not rareat all that hippos devour people, although there isno proof that they would be particularly fond ofgrandmothers.
Nobody has heard fat men eatinggrandmothers, but those do not come intoquestion in any case, because they do not live inlakes.If we assume that hippos hardly eatgrandmothers we can remove the reading, whichhas the tag "grandmother".
We are still left withsingular and plural alternatives of tomato.
Hereplural would be more natural, because tomatoesare here treated as a mass rather than asindividual fruits.When context-sensitive semantic rules andheuristic rules are applied, the reading is asshown in (3).
(3)Kiboko"kiboko"  N 7/8-SG  { hippo , hippopotamus } ANaishiye"ishi"  V 1/2-SG3-SP VFIN { live , reside , stay }SV GEN-REL 1/2-SGkwenye"kwenye" PREP { in , at }maziwa"ziwa"  N 5a/6-PL  { lake }amekula"la"  V 1/2-SG3-SP VFIN PERF:me INFMARK{ eat } SV SVO MONOSLBnyanya"nyanya"  N 9/10-PL  { tomato }.$5 Problem of semantic generalisationAlthough the possibilities for generalisation insemantics are limited, in noun class languagesrelevant semantic clusters can be found.
Eventhough classes in Swahili are only in exceptionalcases semantically 'pure', the class membershipoften provides sufficient information fordisambiguation, either by direct selection or,more often, by exclusion of a reading.The grades of animacy  (e.g.
human, animal,vegetation) are an example of useful semanticgroupings, which can be used in generalisingdisambiguation.
Another useful feature, actuallybelonging to syntax, is the division of verbs intocategories according to their argument structure(e.g.
SV, SVO, SVOO)Neural networks have been used successfullyfor identifying clusters of co-occurrence of wordsand their accompanying tags (Veronis and Ide1990; Sussna 1993; Resnik 1998a).
Researchresults, carried out with the Self-Organizing Map(Kohonen 1995) on semantic clustering of verbsand their arguments in Swahili, are verypromising, and useful generalizations have beenfound (Ng'ang'a 2003).13 These findings can beencoded into the morphological parser and usedin writing semantic disambiguation rules.6 When means for rule writing failIt sometimes happens that linguisticdisambiguation rules cannot be written.Particularly problematic is the noun of the Class9/10 in object position without qualifiers, manyof which would help in disambiguation.
In thisnoun class there are no features in nouns fordetermining whether the word is in singular orplural14.
The detailed survey of about 11,000occurrences of class 9/10 nouns in object positionshows, however, that 97% of them areunambiguously in singular.
Among the remaining3%, 2% can be either in singular or plural, andonly one percent are such cases where the noun isclearly in plural.
These 2% are typically countnouns, which sometimes can be disambiguated,if, for example, they are members in a list ofnouns.
Nouns in such lists tend to be either insingular or in plural, and often at least one listmember belongs to one of the other noun classes,where singular and plural are distinguished.The solution for the nouns of the class 9/10 inobject position is, therefore, that for the rareplural cases, disambiguation rules are written,while singular is the default interpretation.13 The likelihood of co-occurrence can beestablished between word pairs, or clusters, and alsobetween words and tags attached to them.
Therefore,the full range of information in an analysed corpus canbe utilized in establishing relationships.14 Singular and plural are identical in this class, andit is the biggest class of the language, consisting ofabout 39% of all nouns.7 Treatment of multi-word conceptsand idiomsIn computational description of a language,multi-word concepts and idioms can be treated asone unit, because in both cases the meaning isbased on more than one string in text.
If a multi-word concept consists of a collocation or nounphrase, it can be encoded in the tokenizer (4) andthe morphological lexicon (5).
Suchconstructions have two forms (SG and PL) at themost.
(4) bwana  shamba > bwana_shambajumba la makumbusho > jumba_la_makumbushomajumba ya makumbusho >majumba_ya_makumbusho(5) bwana_shamba"bwana_shamba" N 9/6-SG { agricultural adviser}HUMjumba_la_makumbusho"jumba_la_makumbusho" N 5/6-SG { museum }majumba_ya_makumbusho"majumba_ya_makumbusho" N 5/6-PL{ museums}If the concept has a non-finite verb as part ofthe construction, as is often the case in idioms,the constructions cannot be handled on thesurface level.
It is possible to handle them withdisambiguation rules.
Example (6), which is anidiom, shows how each of its constituent parts isinterpreted in isolation.
(6)alipiga"piga"  V 1/2-SG3-SP VFIN PAST  { hit , beat }SVOkonde"konde"  N 5/6-SG { cultivated land , fist}la"la"  GEN-CON 5/6-SG { of }nyuma"nyuma"  ADV { behind }With the help of disambiguation rules, theidiom can be identified, although the verb "piga"may have several surface forms, includingextended forms.
The solution adopted here is thefollowing:As a first step we identify the constituent partsof the idiom and describe its structure by a tag, asis shown in (7).
The angle brackets (<>>) showthat the idiom contains the current word as wellas the preceding word and two following words.Also the meaning of the idiom ("to bribe") isattached to this word.
(7)alipiga"piga"  V 1/2-SG3-SP VFIN PAST { hit , beat }SVOkonde"konde"  <>>IDIOM { to bribe }la"la"  GEN-CON 5/6-SG { of }nyuma"nyuma"  ADV { behind }Then we mark each of the other constituentparts of the idiom and show their relative locationin the structure by using angle brackets, as shownin (8).
For example, "nyuma" is the lastconstituent and all three words before it are partof the idiom.
Original glosses of other constituentparts are removed.
The verb retains itsmorphological tags, and a special tag (IDIOM-V)is added to show that it is part of the idiom.
(8)alipiga"piga"  V 1/2-SG3-SP VFIN PAST SVO  IDIOM-Vkonde"konde" <>>IDIOM { to bribe }la"la" IDIOM<<>nyuma"nyuma" IDIOM<<<8 Making use of default interpretationAlthough it would be possible to writedisambiguation rules for practically all such caseswhere sufficient features for rule writing areavailable, it is sometimes impractical, especiallyin selecting the right semantic interpretation.
Thiscan be implemented in more than one way, forexample by constructing the morphologicalanalyser so that the alternative semantic analysesare in frequency order (9).
(9)taa"taa"  N 9/10-SG  { lamp , lantern } AR"taa"  N 9/10-SG  { discipline , obedience }"taa"  N 9/10-SG  { large flat fish , skate } AN"taa"  N 9/10-PL  { lamp , lantern } AR"taa"  N 9/10-PL  { discipline , obedience }"taa"  N 9/10-PL  { large flat fish , skate } ANThe word "taa" gets three semanticinterpretations, each in singular and plural.
Themost obvious gloss (lamp, lantern) is the first inorder, and if no rule has chosen any of the otheralternatives, this one is chosen as the defaultcase.
The choice of other alternatives iscontrolled by rules as far as possible.
Forexample, the animate reading can often be chosenwith congruence rules.9 DiscussionThe disambiguation of a language is a processwhere the cooperation of linguistic rules andprobability should be optimised.
It was shownabove briefly that different disambiguationoperations should be cascaded so that the mostreliable disambiguation is carried out first and theleast reliable cases last.
Multi-word concepts canbe handled so that such constructions that do nothave inflecting constituent parts are treated aspart of morphology, and those with inflectingparts, especially idioms, are handled withdisambiguation rules.
We have also seen thatlinguistic rules should precede rules based onprobability.
It is also possible to simplify thewriting of semantic rules by constructing themorphological parser so that semantic readingscome in order of frequency, whereby the mostfrequent interpretation is considered a defaultcase, and only other interpretations need rules.The experiments with the SOM algorithmindicate that it is possible to find significantrelationships between adjacent words on the onehand and between words and tags on the other.Such information can then be encoded in themorphological dictionary and used ingeneralising disambiguation rules.
Ambiguityresolution can be enhanced further byconstructing explicit dependencies betweenconstituent parts of a sentence  (J?rvinen andTapanainen 1997; Tapanainen and J?rvinen1997; Tapanainen 1999) or by making use of aparse tree bank of the type of WordNet (Hirst andOnge 1998).10 AcknowledgementsThanks go to Lingsoft and KimmoKoskenniemi for allowing me to use the Two-Level Compiler for handling morphologicalanalysis and to Connexor and Pasi Tapanainenfor prividing access to CG-2 for writingdisambiguation rules.ReferencesBanerjee, S. and T. Pedersen, 2002.
An adaptedLesk algorithm for word sense disambiguationusing WordNet.
In: Proceedings of the ThirdInternational Conference on Intelligent TextProcessing and Computational Linguistics,Mexico City, pp.
136-145.Fellbaum, C.
(Ed.)
1998.
WordNet: An electroniclexical database.
MIT Press.Hirst, G. and D. St. Onge 1998.
Lexical chains asrepresentations of context for the detection andcorrection of malapropisms.
In: C.
Fellbaum(Ed.
), WordNet: An electronic lexical database.MIT Press, pp.
305-332.Hurskainen A.
1992.
A Two-Level ComputerFormalism for the Analysis of BantuMorphology.
An Application to Swahili.Nordic Journal of African Studies 1(1): 87-122.Hurskainen A.
1996.
Disambiguation ofmorphological analysis in Bantu languages.
In:Proceedings of COLING-96, pp.
568-573.J?rvinen, T. and P. Tapanainen, 1997.
ADependency Parser for English.
TechnicalReports, No.
TR-1.
Department of GeneralLinguistics, University of Helsinki.Kohonen, T. 1995.
Self-Organizing Maps.
Berlin:Springer.Koskenniemi, K. 1983.
Two-level morphology: Ageneral computational model for word-formrecognition and production.
PublicationsNo.11.
Department of General Linguistics,University of Helsinki.Miller, G. 1990.
Wordnet: An On-line LexicalDatabase.
International Journal ofLexicography 3(4): 235-312.Ng'ang'a, J.
2003.
Semantic Analysis of KiswahiliWords Using the Self Organizing Map.
NordicJournal of African Studies, 12(3): 407-425.Resnik, P. 1998a.
Semantic similarity in ataxonomy: An information-based measure andits application to problems of ambiguity innatural language.
Journal of ArtificialIntelligence Research 11:95-130.Resnik, P. 1998b.
WordNet and class-basedprobabilities.
In: Fellbaum (Ed.
), WordNet: Anelectronic lexical database.
MIT Press, pp.239-263.Stevenson, M. and Y. Wilks, 2001.
Theinteraction of knowledge sources in word sensedisambiguation.
Computational Linguistics27(3): 321-349.Sussna, M. 1993.
Word sense disambiguation forfree-text indexing using a massive semanticnetwork.
In: Proceedings of the SecondInternational Conference on Information andKnowledge Management, pp.
67-74.Tapanainen, P. 1999.
Parsing in two frameworks:finite-state and functional dependencygrammar.
Ph.D. thesis, Department of GeneralLinguistics, University of Helsinki.Tapanainen, P. 1996.
The Constraint GrammarParser CG-2.
Publications No.
27.
Departmentof General Linguistics, University of Helsinki.Tapanainen, P. and T. J?rvinen, 1997.
A non-projective dependency parser.
ANLP'97,Washington, pp.
64-71.Veronis, J. and N. Ide, 1990.
Word sensedisambiguation with very large neuralnetworks extracted from machine readabledictionaries.
In: Proceedings of the 13thInternational Conference on ComputationalLinguistics, Helsinki, pp.
389-394.Wilks, Y. and M. Stevenson, 1998.
Word sensedisambiguation using optimised combinationsof knowledge sources.
In: Proceedings of the17th International Conference onComputational Linguistics and the 36th AnnualMeeting of the Association for ComputationalLinguistics, pp.
1398-1402.
