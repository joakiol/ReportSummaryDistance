Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 157?160,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsExploiting N-best Hypotheses for SMT Self-EnhancementBoxing Chen, Min Zhang, Aiti Aw and Haizhou LiDepartment of Human Language TechnologyInstitute for Infocomm Research21 Heng Mui Keng Terrace, 119613, Singapore{bxchen, mzhang, aaiti, hli}@i2r.a-star.edu.sgAbstractWord and n-gram posterior probabilities esti-mated on N-best hypotheses have been used toimprove the performance of statistical ma-chine translation (SMT) in a rescoring frame-work.
In this paper, we extend the idea toestimate the posterior probabilities on N-besthypotheses for translation phrase-pairs, targetlanguage n-grams, and source word re-orderings.
The SMT system is self-enhancedwith the posterior knowledge learned from N-best hypotheses in a re-decoding framework.Experiments on NIST Chinese-to-English taskshow performance improvements for all thestrategies.
Moreover, the combination of thethree strategies achieves further improvementsand outperforms the baseline by 0.67 BLEUscore on NIST-2003 set, and 0.64 on NIST-2005 set, respectively.1 IntroductionState-of-the-art Statistical Machine Translation(SMT) systems usually adopt a two-pass searchstrategy.
In the first pass, a decoding algorithm isapplied to generate an N-best list of translationhypotheses; while in the second pass, the finaltranslation is selected by rescoring and re-rankingthe N-best hypotheses through additional featurefunctions.
In this framework, the N-best hypothe-ses serve as the candidates for the final translationselection in the second pass.These N-best hypotheses can also provide usefulfeedback to the MT system as the first decodinghas discarded many undesirable translation candi-dates.
Thus, the knowledge captured in the N-besthypotheses, such as posterior probabilities forwords, n-grams, phrase-pairs, and source word re-orderings, etc.
is more compatible with the sourcesentences and thus could potentially be used toimprove the translation performance.Word posterior probabilities estimated from theN-best hypotheses have been widely used for con-fidence measure in automatic speech recognition(Wessel, 2002) and have also been adopted intomachine translation.
Blatz et al (2003) and Uef-fing et al (2003) used word posterior probabilitiesto estimate the confidence of machine translation.Chen et al (2005), Zens and Ney (2006) reportedperformance improvements by computing target n-grams posterior probabilities estimated on the N-best hypotheses in a rescoring framework.
Trans-ductive learning method (Ueffing et al, 2007)which repeatedly re-trains the generated source-target N-best hypotheses with the original trainingdata again showed translation performance im-provement and demonstrated that the translationmodel can be reinforced from N-best hypotheses.In this paper, we further exploit the potential ofthe N-best hypotheses and propose severalschemes to derive the posterior knowledge fromthe N-best hypotheses, in an effort to enhance thelanguage model, translation model, and sourceword reordering under a re-decoding framework ofany phrase-based SMT system.2 Self-Enhancement with PosteriorKnowledgeThe self-enhancement system structure is shown inFigure 1.
Our baseline system is set up usingMoses (Koehn et al, 2007), a state-of-the-artphrase-base SMT open source package.
In the fol-lowings, we detail the approaches to exploiting thethree different kinds of posterior knowledge,namely, language model, translation model andword reordering.1572.1 Language ModelWe consider self-enhancement of language modelas a language model adaptation problem similar to(Nakajima et al, 2002).
The original monolingualtarget training data is regarded as general-domaindata while the test data as a domain-specific data.Obviously, the real domain-specific target data(test data) is unavailable for training.
In this work,the N-best hypotheses of the test set are used as aquasi-corpus to train a language model.
This newlanguage model trained on the quasi-corpus is thenused together with the language model trained onthe general-domain data (original training data) toproduce a new list of N-best hypotheses under ourself-enhancement framework.
The feature functionof the language model 1 1( , )J ILMh f e  is a mixturemodel of the two language models as in Equation 1.1 1 1 1 2 1( , ) ( ) ( )J I I ILM TLM QLMh f e h e h e?
?= +      (1)where 1Jf is the source language words string,1Ie is  the target language words string, TLM is thelanguage model trained on target training data, andQLM is on the quasi-corpus of N-best hypotheses.The mixture model exploits multiple languagemodels with weights 1?
and 2?
being optimizedtogether with other feature functions.
The proce-dure for self-enhancement of the language model isas follows.1.
Run decoding and extract N-best hypotheses.2.
Train a new language model (QLM) on the N-best hypotheses.3.
Optimize the weights of the decoder which usesboth original LM (TLM) and the new LM(QLM).4.
Repeat step 1-3 for a fixed number of iterations.2.2 Translation ModelIn general, we can safely assume that for a givensource input, phrase-pairs that appeared in the N-best hypotheses are better than those that did not.We call the former ?good phrase-pairs?
and thelater ?bad phrase-pairs?
for the given source input.Hypothetically, we can reinforce the translationmodel by appending the ?good phrase-pairs?
to theoriginal phrase table and changing the probabilityspace of the translation model, as phrase-basedtranslation probabilities are estimated using rela-tive frequencies.
The new direct phrase-basedtranslation probabilities are computed as follows:( , ) ( , )( | )( ) ( )train nbesttrain nbestN f e N f ep e fN f N f+= +% %% %%% % %       (2)where f%  is the source language phrase, e%  is  thetarget language phrase, (.
)trainN is the frequenciesobserved in the training data, and (.
)nbestN  is thefrequencies observed in the N-best hypotheses.
Forthose phrase-pairs that did not appear in the N-besthypotheses list (?bad phrase-pairs?
), ( , )nbestN f e% %equals 0, but the marginal count of f%  is increasedby ( )nbestN f% , in this way the phrase-based transla-tion probabilities of ?bad phrase-pairs?
degradedwhen compared with the corresponding probabili-ties in the original translation model, and that of?good phrase-pairs?
increased, hence improve thetranslation model.The procedure for translation model self-enhancement can be summarized as follows.1.
Run decoding and extract N-best hypotheses.2.
Extract ?good phrase-pairs?
according to thehypotheses?
phrase-alignment information andappend them to the original phrase table to gen-erate a new phrase table.3.
Score the new phrase table to create a newtranslation model.4.
Optimize the weights of the decoder with theabove new translation model.5.
Repeat step 1-4 for a fixed number of iterations.2.3 Word ReorderingSome previous work (Costa-juss?
and Fonollosa,2006; Li et al, 2007) have shown that reordering asource sentence to match the word order in its cor-Figure 1: Self-enhancement system structure, whereTM is translation model, LM is language model, andRM is reordering model.158responding target sentence can produce bettertranslations for a phrase-based SMT system.
Webring this idea forward to our word reordering self-enhancement framework, which similarly trans-lates a source sentence (S) to target sentence (T) intwo stages: S S T??
?
, where S ?
is the reor-dered source sentence.The phrase-alignment information in each hy-pothesis indicates the word reordering for sourcesentence.
We select the word reordering with thehighest posterior probability as the best word reor-dering for a given source sentence.
Word re-orderings from different phrase segmentation butwith same word surface order are merged.
Theposterior probabilities of the word re-orderings arecomputed as in Equation 3.11 1( )( | )JJ JhypN rp r fN=                        (3)where 1( )JN r  is the count of word reordering 1Jr ,and hypN  is the number of N-best hypotheses.The words of the source sentence are then reor-dered according to their indices in the best selectedword reordering 1Jr .
The procedure for self-enhancement of word reordering is as follows.1.
Run decoding and extract N-best hypotheses.2.
Select the best word re-orderings according tothe phrase-alignment information.3.
Reorder the source sentences according to theselected word reordering.4.
Optimize the weights of the decoder with thereordered source sentences.5.
Repeat step 1-4 for a fixed number of iterations.3 Experiments and ResultsExperiments on Chinese-to-English NIST transla-tion tasks were carried out on the FBIS1 corpus.We used NIST 2002 MT evaluation test set as ourdevelopment set, and the NIST 2003, 2005 test setsas our test sets as shown in Table 1.We determine the number of iteration empiri-cally by setting it to 10.
We then observe theBLEU score on the development set for each itera-tion.
The iteration number which achieved the bestBLEU score on development set is selected as theiteration number of iterations for the test set.1 LDC2003E14#Running words Data set typeChinese Englishparallel 7.0M 8.9M trainmonolingual - 61.5MNIST 02 dev 23.2K 108.6KNIST 03 test 25.8K 116.5KNIST 05 test 30.5K 141.9KTable 1: Statistics of training, dev and test sets.
Evalua-tion sets of NIST campaigns include 4 references: totalnumbers of running words are provided in the table.System #iter.
NIST 02 NIST 03 NIST 05Base - 27.67 26.68 24.82TM 4 27.87 26.95 25.05LM 6 27.96 27.06 25.07WR 6 27.99 27.04 25.11Comb 7 28.45 27.35 25.46Table 2: BLEU% scores of five systems: decoder (Base),self-enhancement on translation model (TM), languagemodel (LM), word reordering (WR) and the combina-tion of TM, LM and WR (Comb).Further experiments also suggested that, in thisexperiment scenario, setting the size of N-best listto 3,000 arrives at the greatest performance im-provements.
Our evaluation metric is BLEU (Pap-ineni et al, 2002).
The translation performance isreported in Table 2, where the column ?#iter.?
re-fers to the iteration number where the systemachieved the best BLEU score on development set.Compared with the baseline (?Base?
in Table 2),all three self-enhancement methods (?TM?, ?LM?,and ?WR?
in Table 2) consistently improved theperformance.
In general, absolute gains of 0.23-0.38 BLEU score were obtained for each methodon two test sets.
While comparing the performanceamong all three methods, we can see that theyachieved very similar improvement.
Combiningthe three methods showed further gains in BLEUscore.
Totally, the combined system outperformedthe baseline by 0.67 BLEU score on NIST?03, and0.64 on NIST?05 test set, respectively.4 DiscussionAs posterior knowledge applied in our models areposterior probabilities, the main difference be-tween our work and all previous work is the use ofknowledge source, where we derive knowledgefrom the N-best hypotheses generated from previ-ous iteration.159Comparing the work of (Nakajima et al, 2002),there is a slight difference between the two models.Nakajima et al used only 1-best hypothesis, whilewe use N-best hypotheses of test set as the quasi-corpus to train the language model.In the work of  (Costa-juss?
and Fonollosa, 2006;Li et al, 2007) which similarly translates a sourcesentence (S) to target sentence (T) in two stages:S S T??
?
, they derive S ?
from training data;while we obtain S ?
based on the occurrence fre-quency, i.e.
posterior probability of each sourceword reordering in the N-best hypotheses list.An alternative solution for enhancing the trans-lation model is through self-training (Ueffing,2006; Ueffing et al, 2007) which re-trains thesource-target N-best hypotheses together with theoriginal training data, and thus differs from ours inthe way of new phrase pairs extraction.
We onlysupplement those phrase-pairs appeared in the N-best hypotheses to the original phrase table.
Fur-ther experiment showed that improvement ob-tained by self-training method is not as consistenton both development and test sets as that by ourmethod.
One possible reason is that in self-training,the entire translation model is adjusted with theaddition of new phrase-pairs extracted from thesource-target N-best hypotheses, and hence theeffect is less predictable.5 ConclusionsTo take advantage of the N-best hypotheses, weproposed schemes in a re-decoding framework andmade use of the posterior knowledge learned fromthe N-best hypotheses to improve a phrase-basedSMT system.
The posterior knowledge includeposterior probabilities for target n-grams, transla-tion phrase-pairs and source word re-orderings,which in turn improve the language model, transla-tion model, and word reordering respectively.Experiments were based on the state-of-the-artphrase-based decoder and carried out on NISTChinese-to-English task.
It has been shown that allthree methods improved the performance.
More-over, the combination of all three strategies outper-forms each individual method and significantlyoutperforms the baseline.
We demonstrated thatthe SMT system can be self-enhanced by exploit-ing useful feedback from the N-best hypotheseswhich are generated by itself.ReferencesJ.
Blatz, E. Fitzgerald, G. Foster, S. Gandrabur, C.Goutte, A. Kulesza, A. Sanchis, and N. Ueffing.
2003.Confidence estimation for machine translation.
Finalreport, JHU/CLSP Summer Workshop.B.
Chen, R. Cattoni, N. Bertoldi, M. Cettolo and M.Federico.
2005.
The ITC-irst SMT System forIWSLT-2005.
In Proceeding of IWSLT-2005, pp.98-104, Pittsburgh, USA, October.M.
R.
Costa-juss?, J.
A. R. Fonollosa.
2006.
StatisticalMachine Reordering.
In Proceeding of EMNLP 2006.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch, M.Federico, N. Bertoldi, B. Cowan, W. Shen, C. Moran,R.
Zens, C. Dyer, O. Bojar, A. Constantin and E.Herbst.
2007.
Moses: Open Source Toolkit for Statis-tical Machine Translation.
In Proceedings of ACL-2007, pp.
177-180, Prague, Czech Republic.C.-H. Li, M. Li, D. Zhang, M. Li, M. Zhou and Y. Guan.2007.
A Probabilistic Approach to Syntax-based Re-ordering for Statistical Machine Translation.
In Pro-ceedings of ACL-2007.
Prague, Czech Republic.H.
Nakajima, H. Yamamoto, T. Watanabe.
2002.
Lan-guage model adaptation with additional text gener-ated by machine translation.
In Proceedings ofCOLING-2002.
Volume 1, Pages: 1-7.
Taipei.K.
Papineni, S. Roukos, T. Ward, and W.J.
Zhu, 2002.BLEU: a method for automatic evaluation of ma-chine translation.
In Proceeding of ACL-2002, pp.311-318.N.
Ueffing.
2006.
Using Monolingual Source-LanguageData to Improve MT Performance.
In Proceedings ofIWSLT 2006.
Kyoto, Japan.
November 27-28.N.
Ueffing, K. Macherey, and H. Ney.
2003.
Confi-dence Measures for Statistical Machine Translation.In Proceeding of MT Summit IX, pages 394?401,New Orleans, LA, September.N.
Ueffing, G. Haffari, A. Sarkar.
2007.
Transductivelearning for statistical machine translation.
In Pro-ceedings of ACL-2007, Prague.F.
Wessel.
2002.
Word Posterior Probabilities for LargeVocabulary Continuous Speech Recognition.
Ph.D.thesis, RWTH Aachen University.
Aachen, Germany,January.R.
Zens and H. Ney.
2006.
N-gram Posterior Probabili-ties for Statistical Machine Translation.
In Proceed-ings of the HLT-NAACL Workshop on SMT, pp.
72-77, NY.160
