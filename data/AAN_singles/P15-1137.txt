Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 1416?1426,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsLearning Anaphoricity and Antecedent Ranking Featuresfor Coreference ResolutionSam Wiseman1Alexander M. Rush1,21School of Engineering and Applied SciencesHarvard UniversityCambridge, MA, USA{swiseman,srush,shieber}@seas.harvard.eduStuart M. Shieber1Jason Weston22Facebook AI ResearchNew York, NY, USAjase@fb.comAbstractWe introduce a simple, non-linearmention-ranking model for coreferenceresolution that attempts to learn distinctfeature representations for anaphoricitydetection and antecedent ranking, whichwe encourage by pre-training on a pairof corresponding subtasks.
Although weuse only simple, unconjoined features, themodel is able to learn useful representa-tions, and we report the best overall scoreon the CoNLL 2012 English test set todate.1 IntroductionOne of the major challenges associated with re-solving coreference is that in typical documentsthe number of mentions (syntactic units capableof referring or being referred to) that are non-anaphoric ?
that is, that are not coreferent withany previous mention ?
far exceeds the numberof mentions that are anaphoric (Kummerfeld andKlein, 2013; Durrett and Klein, 2013).This preponderance of non-anaphoric mentionsmakes coreference resolution challenging, partlybecause many basic coreference features, such asthose looking at head, number, or gender matchfail to distinguish between truly coreferent pairsand the large number of matching but nonethe-less non-coreferent pairs.
Indeed, several au-thors have noted that it is difficult to obtain goodperformance on the coreference task using sim-ple features (Lee et al, 2011; Fernandes et al,2012; Durrett and Klein, 2013; Kummerfeld andKlein, 2013; Bj?orkelund and Kuhn, 2014) and, asa result, state-of-the-art systems tend to use lin-ear models with complicated feature conjunctionschemes in order to capture more fine-grained in-teractions.
While this approach has shown suc-cess, it is not obvious which additional featureconjunctions will lead to improved performance,which is problematic as systems attempt to scalewith new data and features.In this work, we propose a data-drivenmodel for coreference that does not require pre-specifying any feature relationships.
Inspired byrecent work in learning representations for nat-ural language tasks (Collobert et al, 2011), weexplore neural network models which take onlyraw, unconjoined features as input, and attempt tolearn intermediate representations automatically.In particular, the model we describe attempts tocreate independent feature representations usefulfor both detecting the anaphoricity of a mention(that is, whether or not a mention is anaphoric) andranking the potential antecedents of an anaphoricmention.
Adequately capturing anaphoricity in-formation has long been thought to be an impor-tant aspect of the coreference task (see Ng (2004)and Section 7), since a strong non-anaphoric sig-nal might, for instance, discourage the erroneousprediction of an antecedent for a non-anaphoricmention even in the presence of a misleading headmatch.We furthermore attempt to encourage the learn-ing of the desired feature representations by pre-training the model?s weights on two correspond-ing subtasks, namely, anaphoricity detection andantecedent ranking of known anaphoric mentions.Overall our best model has an absolute gain ofalmost 2 points in CoNLL score over a similarbut linear mention-ranking model on the CoNLL2012 English test set (Pradhan et al, 2012), andof over 1.5 points over the state-of-the-art coref-erence system.
Moreover, unlike current state-of-the-art systems, our model does only local infer-ence, and is therefore significantly simpler.1.1 Problem SettingWe consider here the mention-ranking (or?mention-synchronous?)
approach to coreference1416resolution (Denis and Baldridge, 2008; Bengtsonand Roth, 2008; Rahman and Ng, 2009), whichhas been adopted by several recent coreferencesystems (Durrett and Klein, 2013; Chang et al,2013).
Such systems aim to identify whether amention is coreferent with an antecedent mention,or whether it is instead non-anaphoric (the firstmention in the document referring to a particularentity).
This is accomplished by assigning a scoreto the mention?s potential antecedents as well asto the possibility that it is non-anaphoric, andthen predicting the greatest scoring option.
Wefurthermore assume the more realistic ?systemmention?
setting, where it is not known a prioriwhich mentions in a document participate incoreference clusters, and so (all) mentions mustbe automatically extracted, typically with the aidof automatically detected parse trees.Formally, we denote the set of automatically de-tected mentions in a document by X .
For a men-tion x?X , let A(x) denote the set of mentionsappearing before x; we refer to this set as x?s po-tential antecedents.
Additionally let the symbol denote the empty antecedent, to which we willview x as referring when x is non-anaphoric.1De-noting the set A(x) ?
{} by Y(x), a mention-ranking model defines a scoring function s(x, y) :X ?
Y ?
R, and predicts the antecedent of x tobe y?= arg maxy?Y(x)s(x, y).It is common to be quite liberal when extractingmentions, taking, essentially, every noun phrase orpronoun to be a candidate mention, so as not toprematurely discard those that might be coreferent(Lee et al, 2011; Fernandes et al, 2012; Changet al, 2012; Durrett and Klein, 2013).
For in-stance, the Berkeley Coreference System (hereinBCS) (Durrett and Klein, 2013), which we usefor mention extraction in our experiments, recov-ers approximately 96.4% of the truly anaphoricmentions in the CoNLL 2012 training set, withan almost 3.5:1 ratio of non-anaphoric mentionsto anaphoric mentions among the extracted men-tions.2 Mention Ranking ModelsThe structural simplicity of the mention-rankingframework puts much of the burden on the scor-ing function s(x, y).
We begin by consider-ing mention-ranking systems using linear scoring1We make this stipulation for modeling convenience; it isnot intended to reflect any linguistic fact.functions.
In the next section, we will extend thesemodels to operate over learned non-linear repre-sentations.Linear mention-ranking models generally uti-lize the following scoring functionslin(x, y) , wT?
(x, y) ,where ?
:X ?Y?Rdis a pairwise feature func-tion defined on a mention and a potential an-tecedent, and w is a learned parameter vector.To add additional flexibility to the model, lin-ear mention ranking models may duplicate indi-vidual features in ?, with one version being usedwhen predicting an antecedent for x, and anotherwhen predicting that x is non-anaphoric (Durrettand Klein, 2013).
Such a scheme effectively givesrise to the following piecewise scoring functionslin+(x, y) ,{uT[?a(x)?p(x,y)]if y 6= vT?a(x) if y =  ,where ?a: X ?
Rdais a feature function definedon a mention and its context, ?p: X ?
Y ?
Rdpis a pairwise feature function defined on a mentionand a potential antecedent, and parameters u andv replacew.
Above, we have made an explicit dis-tinction between pairwise features (?p) and thosestrictly on x and its context (?a), and moreover as-sumed that our features need not examine potentialantecedents when predicting y= .We refer to the basic, unconjoined features usedfor ?aand ?pas raw features.
Figure 2 showstwo versions of these features, a base set BASICand an extended set BASIC+.
The BASIC set arethe raw features used in BCS, and BASIC+ in-cludes additional raw features used in other recentcoreference sytems.
For instance, BASIC+ addi-tionally includes features suggested by Recasenset al (2013) to be useful for anaphoricity, suchas the number of a mention, its named entity sta-tus, and its animacy, as well as number and gen-der information.
We additionally include bilexi-cal head features, which are used in many well-performing systems (for instance, that of Fernan-des et al (2012)).2.1 Problems with Raw FeaturesMany authors have observed that, taken individu-ally, raw features tend to not be particularly pre-dictive for the coreference task.
We examinethis phenomenon empirically in Figure 1.
These1417Figure 1: Two histograms illustrating the predictive abilityof raw (unconjoined) features per feature occurrence: (top)mention-context features from ?aas independent predictorsof anaphoricity (y 6= ), and (bottom) antecedent-mentionfeatures from ?pas independent predictors of coreferentmentions.
Very few raw features are strong indicators of ei-ther anaphoricity or an antecedent match.
Data taken fromthe CoNLL development set.graphs show that the vast majority of individualfeatures do not give a strong positive signal eitherof anaphoricity or for an antecedent match.To address this issue, state-of-the-art mention-ranking systems often rely on manual or otherwiseinduced conjunction schemes to capture specificfeature interactions.
Durrett and Klein (2013),for instance, conjoin all raw features in ?awiththe type of the mention x, and all raw features in?pwith the types of the current mention and an-tecedent.
For these purposes, the type of a mentionis either ?nominal?, ?proper?, or a canonicaliza-tion of the pronoun if it is a pronominal mention.Fernandes et al (2012) and Bj?orkelund and Kuhn(2014) use an automatic but complicated schemeto induce conjunctions by first extracting featuretemplates from a separately trained decision tree,and then doing greedy forward selection amongthe templates.
These conjunctions add some non-linearity to the scoring function while still main-taining a tractable, though large, feature set.3 Learning Features for RankingAs an alternative to the aforementioned featureconjunction schemes, we consider learning featurerepresentations in order to better capture relevantaspects of the task.
Representation learning af-fords the model more flexibility in exploiting fea-ture interactions, although it can make the under-lying training problem more difficult.Mention Features (?a)Feature Value SetMention Head VMention First Word VMention Last Word VWord Preceding Mention VWord Following Mention V# Words in Mention {1, 2, .
.
.
}Mention Synt.
Ancestry see BCS (2013)Mention Type T+ Mention Governor V+ Mention Sentence Index {1, 2, .
.
.
}+ Mention Entity Type NER tags+ Mention Number {sing.,plur.,unk}+ Mention Animacy {an.,inan.,unk}+ Mention Gender {m,f,neut.,unk}+ Mention Person {1,2,3,unk}Pairwise Features (?p)Feature Value SetBASIC features on Mention see aboveBASIC features on Antecedent see aboveMentions between Ment., Ante.
{0. .
.
10}Sentences between Ment., Ante.
{0. .
.
10}i-within-i {T,F}Same Speaker {T,F}Document Type {Conv.,Art.
}Ante., Ment.
String Match {T,F}Ante.
contains Ment.
{T,F}Ment.
contains Ante.
{T,F}Ante.
contains Ment.
Head {T,F}Mention contains Ante.
Head {T,F}Ante., Ment.
Head Match {T,F}Ante., Ment.
Synt.
Ancestries see above+ BASIC+ features on Ment.
see above+ BASIC+ features on Ante.
see above+ Ante., Ment.
Numbers see above+ Ante., Ment.
Genders see above+ Ante., Ment.
Persons see above+ Ante., Ment., Entity Types see above+ Ante., Ment.
Heads see above+ Ante., Ment.
Types see aboveFigure 2: Features used for ?a(x) and ?p(x, y).
The ?+?indicates a feature is in BASIC+ feature set.
V denotes thetraining vocabulary, and T denotes the set of mention types,viz., {nominal,proper} ?
{canonical pronouns}, as definedin BCS.
Conv.
and Art.
abbreviate conversation and article(resp.).
Lexicalized features occurring fewer than 20 timesin the training set back off to part-of-speech; bilexical headsoccurring fewer than 10 times back off to an indicator feature.Animacy information is taken from a list and rules used in theStanford Coreference system (Lee et al, 2013).3.1 ModelWe use a neural network to define our model asan extension to the mention-ranking model intro-duced in Section 2.
We consider in particular thescoring function:s(x, y) ,{uTg([ha(x)hp(x,y)]) + u0if y 6= vTha(x) + v0if y =  ,1418where haand hpare feature representations, non-linear functions of the features ?aand ?p(respec-tively), and g is a function of these representa-tions.
In particular, we defineha(x) , tanh(Wa?a(x) + ba)hp(x, y) , tanh(Wp?p(x, y) + bp) ,and we take g to either be the identity func-tion, in which case the above model is analo-gous to slin+but defined over non-linear fea-ture representations, or to be an additional hiddenlayer: g([ha(x)hp(x,y)]) = tanh(W[ha(x)hp(x,y)]+ b).For ease of exposition, we will refer to these twosettings of g as g1and g2(respectively) in whatfollows.
As we will see below, both settings leadto comparable performance, but to a different errordistribution.In either case, by defining the functions haandhp, we allow the model to learn representationsof the input features ?aand ?p.
The benefit ofthe added non-linearities is that, in theory, it is nolonger necessary to explicitly specify feature con-junctions, since the model may learn them auto-matically if necessary.
Accordingly, for this modelwe use only ?aand ?pconsisting of the raw fea-tures in Figure 2 without conjunctions.
Any inter-action between these features must be learned bythe feature representations hpand ha.3.2 TrainingWe can directly train our model using back-propagation.
To specify the training problem, wefirst define notation for the training objective.Define the set C(x) to contain just the mentionsin A(x) that are coreferent with x.
We then defineC?
(x) ={C(x) if x is anaphoric{} otherwise .Finally, let y`n= arg maxy?C?
(xn)s(xn, y) be thehighest scoring correct antecedent of xn, whichmay be .
(Thus, following recent work (Yu andJoachims, 2009; Fernandes et al, 2012; Chang etal., 2013; Durrett and Klein, 2013), we view eachmention as having a ?latent antecedent?.2) Wetrain to minimize the regularized, slack-rescaled,2Note that this renders the objectives of even models witha linear scoring function non-convex.latent-variable loss3given by:L(?)
=N?n=1maxy??Y(xn)?
(xn, y?
)(1 + s(xn, y?
)?s(xn, y`n))+ ?||?||1,where ?
is a mistake-specific cost function,which is 0 when y?
?C?(xn).
Above, weuse ?
to refer to the full set of parameters{W ,u,v,Wa,Wp, ba, bp}.For experiments, we define ?
to take on differ-ent costs for the three kinds of mistakes possiblein a coreference task, as follows:?
(x, y?)
={?1if y?
6=  ?
 ?
C?
(x)?2if y?
=  ?
 6?
C?
(x)?3if y?
6=  ?
y?
6?
C?
(x) .The ?idetermine the trade-off between these mis-takes (and thus precision and recall).
Adopting theterminology of BCS, we refer to these mistakesas ?false link?
(FL), ?false new?
(FN), and ?wronglink?
(WL), respectively.4 Representations from SubtasksWhile we could train our full model directly, it isknown to be difficult to train high performing non-convex neural-network models from a random ini-tialization (Erhan et al, 2010).
In order to over-come the problems associated with training fromthis setting, and to learn feature representationsuseful for the full coreference task, we pretrainsubparts of the model on the subtasks targetingthe desired feature representations.
We then trainthe entire model on the full coreference task (fromthe pre-trained initializations).
As we will see,the pre-training scheme outlined below helps themodel achieve improved performance.The proposed pre-training scheme involveslearning the parameters associated with haand hpusing two natural subtasks: anaphoricity detectionand antecedent ranking.
In particular, we (1) trainhaon the task of predicting whether a particularmention is anaphoric or not, and (2) train hponthe task of predicting the antecedent of mentionsknown to be anaphoric.4.1 Anaphoricity DetectionFor the first subtask we attempt to predict whethera mention is anaphoric or not based only on its3Previous work divides between log-loss and margin loss.We use the latter because gradient updates (within backprop)for the non-probabilistic objectives only involve terms relat-ing to y?
and y`n, and are therefore faster.1419Feat.
(Conj.)
ModelAnaphoric AnteP R F1Acc.BASIC (N) Lin.
74.15 74.20 74.18 69.10BASIC (Y) Lin.
73.98 75.04 74.51 79.76BASIC (N) NN 75.30 75.36 75.33 81.65BASIC+ (N) Lin.
74.14 74.71 74.43 74.02BASIC+ (Y) Lin.
74.24 75.39 74.81 80.44BASIC+ (N) NN 75.84 76.02 75.93 82.86Table 1: Performance of the two subtasks on the CoNLL 2012development set by feature set and model type.
?Conj.?
indi-cates whether conjunctions are used.
The linear anaphoricsystem is an SVM (LibLinear implementation (Fan et al,2008)), and the linear antecedent system is a linear modelwith the margin-based objective.local context.4Anaphoricity detection in vari-ous forms has been used as an initial step in sev-eral coreference systems (Ng and Cardie, 2002;Bengtson and Roth, 2008; Rahman and Ng, 2009;Bj?orkelund and Farkas, 2012), and the relatedquestion of whether a mention can be determinedto be a singleton or not has been explored recentlyby Recasens et al (2013), Ma et al (2014), andothers.5Formally, let tn?
{?1, 1} indicate whether  ?C?
(xn) or not (respectively).
That is, tn= 1 if andonly if xnis anaphoric.
Define the subtask scoringfunction sa: X ?
R assa(x) , vaTha(x) + ?0,where the vector vaand the bias ?0are specific tothis subtask and are discarded after pre-training.We train this model to minimize the followingslack-rescaled objectiveLa(?a) =N?n=1?a(tn)[1?
tnsa(xn)]++ ?||?a||1,where ?ais a class-specific cost used to help en-courage anaphoric decisions given the imbalanceddata set, and ?a= {va,Wa, ba} are the parame-ters of the subtask.4.2 Antecedent RankingFor the second subtask, antecedent ranking, wepredict the antecedent for mentions known a pri-ori to be anaphoric.
This subtask is inspired by4While performance on this subtask can in fact be im-proved further by looking at previous mentions, featureslearned in this way led to inferior performance on the fulltask.5Note that singleton detection is slightly different fromanaphoricity detection, since a mention can be non-anaphoricbut not a singleton if it is the first mention in a cluster.Figure 3: Visualization of the representation matrix Wp.A subset of the raw features were manually grouped intofive classes indicating: full lexical match [F], head match[H], mention/sentence distance [D] (near versus far), gen-der/number match [G], and type [P] (pronoun versus other).The heat map illustrates 10-columns of Wpas a weightedcombination of these classes, roughly illustrating the com-bination of raw features required for this dimension of therepresentation.the ?gold mention?
version of the coreference task.Systems designed for this task are forced to handlemany fewer non-anaphoric mentions and can oftensuccessfully utilize richer feature representations.The setup for this task is similar to the fullcoreference problem, except that we discard anymention xnsuch that  ?
C?(xn).
Thus, we definethe pairwise scoring function sp: X ?
Y ?
R assp(x, y) , upThp(x, y) + ?0.As before, upand ?0are discarded after train-ing for this subtask, but we keep the rest of theparameters.
For training, we use an analogouslatent-variable loss function to that used for thefull coreference task, except we replace C?withC, and the cost ?
(x, y?)
is always 1 (when it isnonzero).4.3 Subtask PerformanceAs a preliminary experiment, we train models forthese two subtasks using both the BASIC and BA-SIC+ raw features.
Table 1 shows the results.
Forthe first subtask, experiments look at the preci-sion, recall, and F1score of predicting anaphoricmentions on the CoNLL 2012 development set.As a baseline we use an L1-regularized SVMimplemented using LibLinear (Fan et al, 2008),both using raw features and using features con-joined according to the BCS scheme.
For the sec-ond subtask, experiments look at the accuracy ofthe model in predicting the correct antecedent onknown anaphoric mentions.
As a baseline we usea linear mention ranking model, with and without1420conjunctions, trained using the same margin-basedloss.In both subtasks, the neural network modelperforms quite well, significantly better than theunconjoined baselines and better than the modeltrained with manually conjoined features.
We pro-vide a visual representation of the antecedent rank-ing features learned in Figure 3.
While the im-proved subtask performance does not imply betterperformance on the full coreference task, it showsthat model can learn useful feature representationswith only raw input features.5 Coreference ExperimentsOur experiments examine performance as com-pared with other coreference systems, as well asthe effect of features, pre-training, and model ar-chitecture.
We also perform a qualitative compar-ison of our model with the analogous linear modelon some challenging non-anaphoric cases.5.1 MethodsAll experiments use the CoNLL 2012 Englishdataset (Pradhan et al, 2012), which is based onthe OntoNotes corpus (Hovy et al, 2006).
Thedata set contains 3,493 documents consisting of1.6 million words.
We use the standard experi-mental split with the training set containing 2,802documents and 156K annotated mentions, the de-velopment set containing 343 documents and 19Kannotated mentions, and the test set containing348 documents and 20K annotated mentions.
Forall experiments, we use BCS (Durrett and Klein,2013) to extract system mentions and to computesome of the features.For training, we minimize the loss describedabove using the composite mirror descent Ada-Grad update (Duchi et al, 2011) with docu-ment sized mini-batches.6We tuned the Ada-Grad learning rate and regularization parametersusing a grid search over possible learning rates?
?
{0.001, 0.002, 0.01, 0.02, 0.1, 0.2} and overregularization parameters ??
{10?6, .
.
.
, 10?1}.For the full coreference task, we use a differ-ent learning rate for the pre-trained weights andfor the second-layer weights, using ?1= 0.1 and?2= 0.001, respectively, and ?= 10?6.
When ini-tializing weight-matrices that were not pre-trained6In preliminary experiments we also used Nesterov?s ac-celerated gradient (Nesterov, 1983), but found AdaGrad toperform better.we used the sparse initialization technique pro-posed by Sutskever et al (2013).
For all experi-ments we use the cost-weights ?
= ?0.5, 1.2, 1?in defining ?.For the anaphoricity representations the ma-trix dimensions used are Wa?R128?da, and forthe pairwise representations the matrix dimensionsused are Wp?R700?dp.
In the g2model, theouter matrix dimensions are W ?R128?
(dp+da).With the BASIC+ features, dpand dacome outto be slightly less than 106and 104, respectively,with bilexical head features accounting for the vastmajority of dp.7We tuned all hyper-parameters (aswell as those of baseline systems) on the develop-ment set.We use the CoNLL 2012 scoring script v8.018(Pradhan et al, 2014; Luo et al, 2014), whichscores based on 3 metrics, including MUC (Vilainet al, 1995), CEAFe(Luo, 2005), and B3(Baggaand Baldwin, 1998), as well as the CoNLL score,which is the arithmetic mean of the 3 metrics.Code implementing our models is availableat https://github.com/swiseman/nn_coref.
The system trains in time comparable tothat of linear systems, mainly because we use onlyraw features and sparse margin-based gradient up-dates.5.2 ResultsOur main results are shown in Table 2.
This tablecompares the performance of our system with theperformance reported by several other state-of-theart systems on the CoNLL 2012 English corefer-ence test set.
Our full models achieve the best F1score across two of the three metrics and have thebest aggregate (CoNLL) score, with an improve-ment of over 1.5 points over the best reported re-sult, and of almost 2 points over the best mention-ranking system.
Our F1improvements on all threemetrics are significant (p < 0.05 under the boot-strap resample test (Koehn, 2004)) as comparedwith both Bj?orkelund and Kuhn (2014), and Dur-rett and Klein (2014), the two most recent, state-of-the-art systems.Since our full models use some additional rawfeatures (although an order of magnitude fewertotal features than the comparable conjunction-7Note that the BCS conjunction scheme, for instance, ap-plied to our raw features gives a dpand dathat are over anorder of magnitude larger.8http://conll.github.io/reference-coreference-scorers/1421SystemMUC B3CEAFeP R F1P R F1P R F1CoNLLBCS (2013) 74.89 67.17 70.82 64.26 53.09 58.14 58.12 52.67 55.27 61.41Prune&Score (2014) 81.03 66.16 72.84 66.90 51.10 57.94 68.75 44.34 53.91 61.56B&K (2014) 74.30 67.46 70.72 62.71 54.96 58.58 59.40 52.27 55.61 61.63D&K (2014) 72.73 69.98 71.33 61.18 56.60 58.80 56.20 54.31 55.24 61.79This work (g2) 76.96 68.10 72.26 66.90 54.12 59.84 59.02 53.34 56.03 62.71This work (g1) 76.23 69.31 72.60 66.07 55.83 60.52 59.41 54.88 57.05 63.39Table 2: Results on CoNLL 2012 English test set.
We compare against recent state-of-the-art systems, including (in order)Durrett and Klein (2013), Ma et al (2014), Bj?orkelund and Kuhn (2014), and Durrett and Klein (2014) (rescored with the v8.01scorer).
F1gains are significant (p < 0.05 under the bootstrap resample test (Koehn, 2004)) compared with both B&K andD&K for all metrics.Model Features MUC B3CEAFeCoNLLLin.BASIC70.44 59.10 55.57 61.71NN (g2) 71.59 60.56 57.45 63.20NN (g1) 71.86 60.90 57.90 63.55Lin.BASIC+70.92 60.05 56.39 62.45NN (g2) 72.68 61.70 58.32 64.23NN (g1) 72.74 61.77 58.63 64.38Table 3: F1performance comparison between state-of-the-artlinear mention-ranking model (Durrett and Klein, 2013) andour full models on CoNLL 2012 development set for differentfeature sets.based linear model), we are interested in what partof the improvement in performance comes fromfeatures rather than modeling power.
Table 3 com-pares the full model to BCS, a system effectivelyusing the slin+scoring function together with amanual conjunction scheme, on both BASIC andBASIC+ features.
While our models outperformBCS in both cases, we see that as we add morefeatures (as in the BASIC+ set), the performancegap between our model and the linear system be-comes even more pronounced.We may also wonder whether the architecturerepresented by our scoring function, where the in-termediate representations haand hpare sepa-rated in the first layer, is necessary for these re-sults.
We accordingly compare with the fullyconnected versions of these two models (whichare equivalent to 1 and 2 layer multi-layer per-ceptrons) using the BASIC+ features in Table 4.9There, we also evaluate the effect of pre-trainingon these models by comparing with the results oftraining from a random initialization.
We see thatwhile even randomly initialized models are capa-ble of excellent performance, pre-training is bene-ficial, especially for g1.9We also experimented with bilinear models both withand without non-linearities; these were also inferior.Model MUC B3CEAFeCoNLLFully Conn. 1 Layer 71.80 60.93 57.51 63.41Fully Conn. 2 Layer 71.77 60.84 57.05 63.22g1+ RI 71.92 61.06 57.59 63.52g1+ PT 72.74 61.77 58.63 64.38g2+ RI 72.31 61.79 58.06 64.05g2+ PT 72.68 61.70 58.32 64.23Table 4: Comparison of performance (in F1score) of vari-ous models on CoNLL 2012 development set using BASIC+features.
?PT?
and ?RI?
refer to pretraining and random ini-tialization respectively.
?Fully Conn.?
refers to baseline fullyconnected networks.
See text for further model descriptions.6 DiscussionWe attempt to gain insight into our model?s er-rors using using two different error breakdowns.In Table 5 we show the errors as reported by theanalysis tool of Kummerfeld and Klein (2013).
InTable 6 we show a more fine-grained breakdowninspired by a similar analysis in Durrett and Klein(2013).
In the latter table, we categorize the er-rors made by our system on the CoNLL 2012 de-velopment data in terms of (1) whether or not themention has a head match with a previously oc-curring mention in the document, unless it is apronominal mention, which we treat separately,(2) in terms of the status of the mention in thegold clustering, namely, singleton, first-in-cluster,or anaphoric, and (3) in terms of the type of errormade (which, as discussed in Section 3, are one ofFL, FN, and WL).We note that the two models have slightly dif-ferent error profiles, with g1being slightly betterat recall and g2being slightly better at precision.Indeed, we see from Table 6 that the two mod-els make a comparable number of total errors (g1makes only 17 fewer errors overall).
The increasedprecision of the g2model is presumably due to thesecond layer around haand hpin g2allowing forantecedent evidence to interact with anaphoricity1422Error Type BCS NN (g1) NN (g2)Conflated Entities 1603 1434 1371Extra Mention 0651 0568 0529Extra Entity 0655 0623 0561Divided Entity 1989 1837 1835Missing Mention 1004 0997 1005Missing Entity 1070 1026 1114Table 5: Absolute error counts from the coreference analysistool of Kummerfeld and Klein (2013).
The upper set roughlycorresponds to the precision and the lower to the recall of thecoreference clusters produced by the model.NN (g1)Singleton 1stin clust.
AnaphoricFL # FL # FN + WL #HM 817 08.2K 147 0.8K 700 + 318 4.7KNo HM 086 19.8K 041 2.4K 677 + 59 1.0KPron.
948 02.6K 257 0.5K 434 + 875 7.3KNN (g2)Singleton 1stin clust.
AnaphoricFL # FL # FN + WL #HM 770 08.2K 130 0.8K 803 + 306 4.7KNo HM 073 19.8K 039 2.4K 699 + 52 1.0KPron.
896 02.6K 249 0.5K 456 + 869 7.3KTable 6: Errors made by NN (g1) (top) and NN (g2) (bottom)on CoNLL 2012 English development data.
Rows correspondto (1) mentions with a (previous) head match (HM), that is,mentions x such thatA(x) contains another mention with thesame head word, (2) with no previous head match (no HM),and (3) to pronominal mentions, respectively.
The 3 columngroups correspond to singleton, first-in-cluster, and anaphoricmentions (resp.
), as determined by the gold clustering, withthe number and type of errors on the left and the total numberof mentions in the category (#) on the right.evidence in a more complicated way.
Ultimately,however, coreference systems operating over sys-tem mentions are already biased toward precision,and so the increased precision of g2is not as help-ful as the increased recall of g1in the final CoNLLscore.In further analysis we found that many of thecorrect predictions made by the g2model notmade by g1and the linear model involve predict-ing non-anaphoric even in the presence of highlymisleading antecedent features like head-match.Figure 4 shows some examples of mentions withprevious head matches that the linear system pre-dicted as anaphoric and that our system correctlyidentifies as non-anaphoric.We illustrate how the features in Figure 2 mightbe useful in such cases by considering the firstexample in Figure 4.
There, a comma follows?the Nika TV company?
in the text (and is pickedup by the ?word following?
feature), perhaps in-dicating an appositive, which makes anaphoric-ity unlikely.
The model can also learn that theNon-Anaphoric (x) Spurious Antecedent (y)the Nika TV company an independent companyLexus sales GM ?s domestic car salesThe storage area the harbor areathe Budapest location Radio Free Europe ?s new locationthe synagogue the synagogue too or somethingthe equity market The junk markettheir silver coin one silver cointhe international school The Hong Kong elementary schoolthe 1970s the early 1970sthe 2003 season the 2001 seasonFigure 4: Example mentions x that were correctly markednon-anaphoric by g2, but incorrectly marked anaphoric withy as an antecedent by the BASIC+ linear model.
These ex-amples highlight the difficult case where there is a spurioushead-match between non-coreferent pairs.
See text for fur-ther details.?company-company?
head match is often mislead-ing, and, in general, distance features may alsorule out head matches.
Note that while these fea-tures on their own may be more or less correlatedwith a mention being non-anaphoric, the modellearns to combine them in a predictive way.6.1 Further Improving Coreference SystemsTable 6 also gives a sense of where coreferencesystems such as ours need to improve.
It isfirst important to note that the case of resolvingan anaphoric mention that has no previous headmatches (e.g., identifying that ?the team?
and ?theNew York Giants?
are coreferent), which is of-ten taken to be one of the major challenges fac-ing coreference systems because it presumablyrequires semantic information, is not the largestsource of errors.
In fact, we see from Table 6(second row, third column in both sub-tables) thatwhile these cases do indeed account for a substan-tial percentage of errors, we make hundreds moreerrors predicting singleton pronominal mentionsto be anaphoric (in the case of g1) and on incor-rectly linking anaphoric pronominal mentions (inthe case of g2).
Further analysis indicates thatthese errors are almost entirely related to incor-rectly linking pleonastic pronouns, such as ?it?
or?you,?
and that moreover the incorrectly predictedantecedent for these pleonastic pronouns is almostalways (another instance of) the same pronoun.That these pleonastic cases are so problematicis interesting when considered against the back-drop of the inference strategies typically employedby coreference systems, which we briefly men-tion here but discuss more fully in the next sec-tion.
Currently, coreference systems divide be-1423tween those using ?local?
models, which chooseantecedents for potentially anaphoric mentions in-dependently of each other, and ?non-local?
mod-els, which make predictions that take into ac-count predictions made for previous mentions, andperhaps even attempt to jointly predict all men-tions in a document.
While our model is en-tirely local, other recent high performing sys-tems, such as that of Bj?orkelund and Kuhn (2014),are not.
One might suspect, then, that ?non-local?
inference might allow us to capture the factthat, for instance, a cluster of coreferent mentionsshould generally not consist solely of pronouns,and thereby avoid predicting (identical) pronomi-nal antecedents for pleonastic pronouns.As it turns out, however, almost 30% of theanaphoric pronominal mentions in the CoNLL de-velopment data participate in pronoun-only clus-ters (primarily in the context of broadcast or tele-phone conversations), which suggests that such a?non-local?
rule may not be particularly useful,though further experiments are required.
It is alsoworth noting that a suitably modified loss functionmay also be able to prevent excessive pronoun-pronoun linking, even in a local model.7 Related WorkThere is a voluminous literature on machine learn-ing approaches to coreference resolution, effec-tively beginning with Soon et al (2001).
The re-cent introduction of the CoNLL datasets (Pradhanet al, 2012) has spurred research that takes ad-vantage of more fine-grained features and richermodels (Bj?orkelund and Farkas, 2012; Chang etal., 2012; Durrett and Klein, 2013; Chang et al,2013; Bj?orkelund and Kuhn, 2014; Ma et al,2014).
Of these approaches, our model is relatedto the mention-ranking approaches (Bengtson andRoth, 2008; Denis and Baldridge, 2008; Rahmanand Ng, 2009; Durrett and Klein, 2013; Changet al, 2013), as opposed to those that focus onnon-local, structured prediction (McCallum andWellner, 2003; Culotta et al, 2006; Haghighi andKlein, 2010; Fernandes et al, 2012; Stoyanov andEisner, 2012; Bj?orkelund and Farkas, 2012; Wicket al, 2012; Bj?orkelund and Kuhn, 2014; Durrettand Klein, 2014).In motivation, our work is most similar to that ofNg (2004), who notes that anaphoricity informa-tion is useful within the broader coreference task,and who accordingly attempts to ?globally?
opti-mize performance based on this information, aswell as that of Denis et al (2007), who do jointdecoding of anaphoricity and coreference predic-tions using ILP.
Both of these works are taken tocontrast with the more popular approach of do-ing an initial non-anaphoric pruning step (Ng andCardie, 2002; Rahman and Ng, 2009; Recasens etal., 2013; Lee et al, 2013).
In contrast, we jointlylearn non-linear functions of anaphoricity and an-tecedent features, rather than tune a threshold,or jointly decode based on independently trainedclassifiers (as in Denis et al (2007)).
In a simi-lar vein, several authors have also proposed usingthe output of an anaphoricity classifier as a featurein a downstream coreference system (Ng, 2004;Bengtson and Roth, 2008).
In our framework we(re)learn features jointly with the full task, aftera pre-training scheme that targets anaphoricity aswell antecedent representations.There has also been some work on automat-ically inducing feature conjunctions for use incoreference systems (Fernandes et al, 2012; Las-salle and Denis, 2013), though the approach wepresent here is somewhat simpler, and unlike thatof Lassalle and Denis (2013) is designed for useon system rather than gold mentions.There has been much interest recently in us-ing neural networks for classic natural languagetasks such as tagging and semantic role labelingCollobert et al (2011), sentiment analysis (Socheret al, 2011; Socher et al, 2012), prepositionalphrase attachment (Belinkov et al, 2014) amongothers.
These systems often use some form of pre-training for initialization, often word-embeddingslearned from external tasks.
However, there hasbeen little work of this form for coreference reso-lution.8 ConclusionWe have presented a simple, local model ca-pable of learning feature representations usefulfor coreference-related subtasks, and of therebyachieving state-of-the-art performance.
Becauseour approach automatically learns intermediaterepresentations given raw features, directions forfurther research might alternately explore includ-ing additional (perhaps semantic) raw features,as well as developing loss functions that furtherdiscourage learning representations that allow forcommon errors (such as those involving pleonasticpronouns).1424ReferencesAmit Bagga and Breck Baldwin.
1998.
Algorithmsfor Scoring Coreference Chains.
In The first in-ternational conference on language resources andevaluation workshop on linguistics coreference, vol-ume 1, pages 563?566.
Citeseer.Yonatan Belinkov, Tao Lei, Regina Barzilay, and AmirGloberson.
2014.
Exploring Compositional Archi-tectures and Word Vector Representations for Prepo-sitional Phrase Attachment.
Transactions of the As-sociation for Computational Linguistics, 2:561?572.Eric Bengtson and Dan Roth.
2008.
Understandingthe Value of Features for Coreference Resolution.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages294?303.
ACL.Anders Bj?orkelund and Rich?ard Farkas.
2012.
Data-driven Multilingual Coreference Resolution usingResolver Stacking.
In Joint Conference on EMNLPand CoNLL-Shared Task, pages 49?55.
ACL.Anders Bj?orkelund and Jonas Kuhn.
2014.
Learn-ing structured perceptrons for coreference Resolu-tion with Latent Antecedents and Non-local Fea-tures.
ACL, Baltimore, MD, USA, June.Kai-Wei Chang, Rajhans Samdani, Alla Rozovskaya,Mark Sammons, and Dan Roth.
2012.
Illinois-coref: The UI System in the CoNLL-2012 SharedTask.
In Joint Conference on EMNLP and CoNLL-Shared Task, pages 113?117.
Association for Com-putational Linguistics.Kai-Wei Chang, Rajhans Samdani, and Dan Roth.2013.
A Constrained Latent Variable Model forCoreference Resolution.
In Proceedings of the 2013Conference on Empirical Methods in Natural Lan-guage Processing, pages 601?612.Ronan Collobert, Jason Weston, L?eon Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.2011.
Natural Language Processing (almost) fromScratch.
The Journal of Machine Learning Re-search, 12:2493?2537.Aron Culotta, Michael Wick, Robert Hall, and AndrewMcCallum.
2006.
First-order Probabilistic Modelsfor Coreference Resolution.
NAACL-HLT.Pascal Denis and Jason Baldridge.
2008.
SpecializedModels and Ranking for Coreference Resolution.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages660?669.
ACL.Pascal Denis, Jason Baldridge, et al 2007.
Joint De-termination of Anaphoricity and Coreference Reso-lution using Integer Programming.
In HLT-NAACL,pages 236?243.
Citeseer.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive Subgradient Methods for Online Learningand Stochastic Optimization.
The Journal of Ma-chine Learning Research, 12:2121?2159.Greg Durrett and Dan Klein.
2013.
Easy Victories andUphill Battles in Coreference Resolution.
In Pro-ceedings of the 2013 Conference on Empirical Meth-ods in Natural Language Processing, pages 1971?1982.Greg Durrett and Dan Klein.
2014.
A Joint Model forEntity Analysis: Coreference, Typing, and Linking.Transactions of the Association for ComputationalLinguistics, 2:477?490.Dumitru Erhan, Yoshua Bengio, Aaron Courville,Pierre-Antoine Manzagol, Pascal Vincent, and SamyBengio.
2010.
Why does unsupervised pre-traininghelp deep learning?
The Journal of Machine Learn-ing Research, 11:625?660.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: ALibrary for Large Linear Classification.
The Journalof Machine Learning Research, 9:1871?1874.Eraldo Rezende Fernandes, C?
?cero Nogueira Dos San-tos, and Ruy Luiz Milidi?u.
2012.
Latent StructurePerceptron with Feature Induction for UnrestrictedCoreference Resolution.
In Joint Conference onEMNLP and CoNLL-Shared Task, pages 41?48.
As-sociation for Computational Linguistics.Aria Haghighi and Dan Klein.
2010.
CoreferenceResolution in a Modular, Entity-centered Model.
InThe 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 385?393.
Association for Computa-tional Linguistics.Eduard Hovy, Mitchell Marcus, Martha Palmer, LanceRamshaw, and Ralph Weischedel.
2006.
Ontonotes:the 90% Solution.
In Proceedings of the human lan-guage technology conference of the NAACL, Com-panion Volume: Short Papers, pages 57?60.
Associ-ation for Computational Linguistics.Philipp Koehn.
2004.
Statistical Significance Tests forMachine Translation Evaluation.
In Proceedings ofthe 2004 Conference on Empirical Methods in Natu-ral Language Processing, pages 388?395.
Citeseer.Jonathan K. Kummerfeld and Dan Klein.
2013.
Error-driven Analysis of Challenges in Coreference Reso-lution.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, Seattle, WA, USA, October.Emmanuel Lassalle and Pascal Denis.
2013.
Improv-ing Pairwise Coreference Models through FeatureSpace Hierarchy Learning.
In ACL 2013-Annualmeeting of the Association for Computational Lin-guistics.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2011.
Stanford?s Multi-pass Sieve Corefer-ence Resolution System at the CoNLL-2011 Shared1425Task.
In Proceedings of the Fifteenth Conference onComputational Natural Language Learning: SharedTask, pages 28?34.
Association for ComputationalLinguistics.Heeyoung Lee, Angel Chang, Yves Peirsman,Nathanael Chambers, Mihai Surdeanu, and Dan Ju-rafsky.
2013.
Deterministic Coreference Res-olution based on Entity-centric, Precision-rankedRules.
Computational Linguistics, 39(4):885?916.Xiaoqiang Luo, Sameer Pradhan, Marta Recasens, andEduard Hovy.
2014.
An Extension of BLANC toSystem Mentions.
Proceedings of ACL, Baltimore,Maryland, June.Xiaoqiang Luo.
2005.
On Coreference ResolutionPerformance Metrics.
In Proceedings of the confer-ence on Human Language Technology and Empiri-cal Methods in Natural Language Processing, pages25?32.
Association for Computational Linguistics.Chao Ma, Janardhan Rao Doppa, J Walker Orr,Prashanth Mannem, Xiaoli Fern, Tom Dietterich,and Prasad Tadepalli.
2014.
Prune-and-score:Learning for Greedy Coreference Resolution.
InProceedings of the 2014 Conference on EmpiricalMethods in Natural Language Processing.Andrew McCallum and Ben Wellner.
2003.
TowardConditional Models of Identity Uncertainty withApplication to Proper Noun Coreference.
Advancesin Neural Information Processing Systems 17.Yurii Nesterov.
1983.
A Method of Solving a ConvexProgramming Problem with Convergence Rate O(1/k2).
In Soviet Mathematics Doklady, volume 27,pages 372?376.Vincent Ng and Claire Cardie.
2002.
IdentifyingAnaphoric and Non-anaphoric Noun Phrases to Im-prove Coreference Resolution.
In Proceedings ofthe 19th international conference on Computationallinguistics-Volume 1, pages 1?7.
Association forComputational Linguistics.Vincent Ng.
2004.
Learning Noun Phrase Anaphoric-ity to Improve Coreference Resolution: Issues inRepresentation and Optimization.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 151.
Association for Com-putational Linguistics.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue,Olga Uryupina, and Yuchen Zhang.
2012.
Conll-2012 Shared Task: Modeling Multilingual Unre-stricted Coreference in OntoNotes.
In Joint Con-ference on EMNLP and CoNLL-Shared Task, pages1?40.
ACL.Sameer Pradhan, Xiaoqiang Luo, Marta Recasens, Ed-uard Hovy, Vincent Ng, and Michael Strube.
2014.Scoring Coreference Partitions of Predicted Men-tions: A Reference Implementation.
In Proceedingsof the Association for Computational Linguistics.Altaf Rahman and Vincent Ng.
2009.
SupervisedModels for Coreference Resolution.
In Proceed-ings of the 2009 Conference on Empirical Methodsin Natural Language Processing: Volume 2-Volume2, pages 968?977.
ACL.Marta Recasens, Marie-Catherine de Marneffe, andChristopher Potts.
2013.
The Life and Death of Dis-course Entities: Identifying Singleton Mentions.
InHLT-NAACL, pages 627?633.Richard Socher, Jeffrey Pennington, Eric H Huang,Andrew Y Ng, and Christopher D Manning.
2011.Semi-supervised Recursive Autoencoders for Pre-dicting Sentiment Distributions.
In Proceedings ofthe 2011 Conference on Empirical Methods in Nat-ural Language Processing, pages 151?161.
ACL.Richard Socher, Brody Huval, Christopher D Manning,and Andrew Y Ng.
2012.
Semantic Compositional-ity through Recursive Matrix-vector Spaces.
In Pro-ceedings of the 2012 Joint Conference on Empiri-cal Methods in Natural Language Processing andComputational Natural Language Learning, pages1201?1211.
ACL.Wee Meng Soon, Hwee Tou Ng, and DanielChung Yong Lim.
2001.
A Machine Learning Ap-proach to Coreference Resolution of Noun Phrases.Computational Linguistics, 27(4):521?544.Veselin Stoyanov and Jason Eisner.
2012.
Easy-firstCoreference Resolution.
In COLING, pages 2519?2534.
Citeseer.Ilya Sutskever, James Martens, George Dahl, and Geof-frey Hinton.
2013.
On the Importance of Initializa-tion and Momentum in Deep Learning.
In Proceed-ings of the 30th International Conference on Ma-chine Learning, pages 1139?1147.Marc Vilain, John Burger, John Aberdeen, Dennis Con-nolly, and Lynette Hirschman.
1995.
A Model-theoretic Coreference Scoring Scheme.
In Proceed-ings of the 6th conference on Message Understand-ing, pages 45?52.
ACL.Michael Wick, Sameer Singh, and Andrew McCallum.2012.
A Discriminative Hierarchical Model for FastCoreference at Large Scale.
In Proceedings of the50th Annual Meeting of the Association for Compu-tational Linguistics: Long Papers-Volume 1, pages379?388.
Association for Computational Linguis-tics.Chun-Nam John Yu and Thorsten Joachims.
2009.Learning Structural SVMs with Latent Variables.
InProceedings of the 26th Annual International Con-ference on Machine Learning, pages 1169?1176.ACM.1426
