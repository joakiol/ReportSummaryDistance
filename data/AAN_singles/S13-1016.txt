Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 1: Proceedings of the Main Conferenceand the Shared Task, pages 119?123, Atlanta, Georgia, June 13-14, 2013. c?2013 Association for Computational LinguisticsBUT-TYPED: Using domain knowledge for computing typed similarityLubomir OtrusinaBrno University of TechnologyFaculty of Information TechnologyIT4Innovations Centre of ExcellenceBozetechova 2, 612 66 BrnoCzech Republiciotrusina@fit.vutbr.czPavel SmrzBrno University of TechnologyFaculty of Information TechnologyIT4Innovations Centre of ExcellenceBozetechova 2, 612 66 BrnoCzech Republicsmrz@fit.vutbr.czAbstractThis paper deals with knowledge-based textprocessing which aims at an intuitive notionof textual similarity.
Entities and relations rel-evant for a particular domain are identified anddisambiguated by means of semi-supervisedmachine learning techniques and resulting an-notations are applied for computing typed-similarity of individual texts.The work described in this paper particularlyshows effects of the mentioned processes inthe context of the *SEM 2013 pilot task ontyped-similarity, a part of the Semantic Tex-tual Similarity shared task.
The goal is toevaluate the degree of semantic similarity be-tween semi-structured records.
As the evalu-ation dataset has been taken from Europeana?
a collection of records on European culturalheritage objects ?
we focus on computing a se-mantic distance on field author which has thehighest potential to benefit from the domainknowledge.Specific features that are employed in our sys-tem BUT-TYPED are briefly introduced to-gether with a discussion on their efficient ac-quisition.
Support Vector Regression is thenused to combine the features and to provide afinal similarity score.
The system ranked thirdon the attribute author among 15 submittedruns in the typed-similarity task.1 IntroductionThe goal of the pilot typed-similarity task lied inmeasuring a degree of semantic similarity betweensemi-structured records.
The data came from theEuropeana digital library1 collecting millions ofrecords on paintings, books, films, and other mu-seum and archival objects that have been digitizedthroughout Europe.
More than 2,000 cultural andscientific institutions across Europe have contributedto Europeana.
There are many metadata fields at-tached to each item in the library, but only fieldstitle, subject, description, creator, date and sourcewere used in the task.Having this collection, it is natural to expect thatdomain knowledge on relevant cultural heritage en-tities and their inter-relations will help to measuresemantic closeness between particular items.
Whenfocusing on similarities in a particular field (a se-mantic type) that clearly covers a domain-specificaspect (such as field author/creator in our case), thesignificance of the domain knowledge should be thehighest.Intuitively, the semantic similarity among authorsof two artworks corresponds to strengths of linksthat can be identified among the two (groups of)authors.
As the gold standard for the task resultedfrom a Mechanical Turk experiment (Paolacci et al2010), it could be expected that close fields corre-spond to authors that are well known to representthe same style, worked in the same time or the sameart branch (e. g., Gabrie?l Metsu and Johannes Ver-meer), come from the same region (often guessedfrom the names), dealt with related topics (not nec-essarily in the artwork described by the record inquestion), etc.
In addition to necessary evaluation ofthe intersection and the union of two author fields(leading naturally to the Jaccard similarity coeffi-1http://www.europeana.eu/119cient on normalized name records ?
see below), itis therefore crucial to integrate means measuring theabove-mentioned semantic links between identifiedauthors.Unfortunately, there is a lot of noise in the dataused in the task.
Since Europeana does not preciselydefine meaning and purpose of each particular fieldin the database, many mistakes come directly fromthe unmanaged importing process realized by par-ticipating institutions.
Fields often mix content ofvarious semantic nature and, occasionally, they arecompletely misinterpreted (e. g., field creator standsfor the author, but, in many cases, it contains onlythe institution the data comes from).
Moreover, thedata in records is rather sparse ?
many fields are leftempty even though the information to be filled in isincluded in original museum records (e. g., the au-thor of an artwork is known but not entered).The low quality of underlying data can be alsoresponsible for results reported in related studies.For example, Aletras et al(2012) evaluate semanticsimilarity between semi-structured items from Euro-peana.
They use several measures including a sim-ple normalized textual overlap, the extended Leskmeasure, the cosine similarity, a Wikipedia-basedmodel and the LDA (Latent Dirichlet Allocation).The study, restricted to fields title, subject and de-scription, shows that the best score is obtained bythe normalized overlap applied only to the title field.Any other combination of the fields decreased theperformance.
Similarly, sophisticated methods didnot bring any improvement.The particular gold standard (training/test data)used in the typed-similarity task is also problematic.For example, it provides estimates of location-basedsimilarity even though it makes no sense for partic-ular two records ?
no field mentions a location andit cannot be inferred from other parts).
A through-out analysis of the task data showed that creator isthe only field we could reasonably use in our exper-iments (although many issues discussed in previousparagraphs apply for the field as well).
That is whywe focus on similarities between author fields in thisstudy.While a plenty of measures for computing tex-tual similarity have been proposed (Lin, 1998; Lan-dauer et al 1998; Sahlgren, 2005; Gabrilovich andMarkovitch, 2007) and there is an active researchin the fields of Textual Entailment (Negri et al2012), Paraphrase Identification (Lintean and Rus,2010) and, recently, the Semantic Textual Similar-ity (Agirre et al 2012), the semi-structured recordsimilarity is a relatively new area of research.
Eventhough we focus on a particular domain-specificfield in this study, our work builds on previous re-sults (Croce et al 2012; Annesi et al 2012) topre-compute semantic closeness of authors based onavailable biographies and other related texts.The rest of the paper is organized as follows: Thenext section introduces the key domain-knowledgeprocessing step of our system which aims at recog-nizing and disambiguating entities relevant for thecultural heritage domain.
The realized system andits results are described in Section 3.
Finally, Sec-tion 4 briefly summarizes the achievements.2 Entity Recognition and DisambiguationA fundamental step in processing text in particu-lar fields lies in identifying named entities relevantfor similarity measuring.
There is a need for anamed entity recognition tool (NER) which identi-fies names and classifies referred entities into pre-defined categories.
We take advantage of such atool developed by our team within the DECIPHERproject2.The DECIPHER NER is able to recognize artistsrelevant for the cultural heritage domain and, formost of them, to identify the branch of the arts theywere primarily focused on (such as painter, sculp-tors, etc.).
It also recognizes names of artworks,genres, art periods and movements and geograph-ical features.
In total, there are 1,880,985 recog-nizable entities from the art domain and more than3,000,000 place names.
Cultural-heritage entitiescome from various sources; the most productiveones are given in Table 1.
The list of place namesis populated from the Geo-Names database3.The tool takes lists of entities and constructs a fi-nite state automaton to scan and annotate input texts.It is extremely fast (50,000 words per second) andhas a relatively small memory footprint (less than90 MB for all the data).Additional information attached to entities is2http://decipher-research.eu/3http://www.geonames.org/120Source # of entitiesFreebase4 1,288,192Getty ULAN5 528,921VADS6 31,587Arthermitage7 4,259Artcyclopedia8 3,966Table 1: Number of art-related entities from varioussourcesstored in the automaton too.
A normalized form of aname and its semantic type is returned for each en-tity.
Normalized forms enable identifying equivalententities expressed differently in texts, e. g., Gabrie?lMetsu refers to the same person as Gabriel Metsu,US can stand for the United States (of America), etc.Type-specific information is also stored.
It includesa detailed type (e. g., architect, sculptor, etc.
), na-tionality, relevant periods or movements, and yearsof birth and death for authors.
Types of geographicalfeatures (city, river), coordinates and the GeoNamesdatabase identifiers are stored for locations.The tool is also able to disambiguate entitiesbased on a textual context in which they appeared.Semantic types and simple rules preferring longermatches provide a primary means for this.
For ex-ample, a text containing Bobigny ?
Pablo Picasso,refers probably to a station of the Paris Metro anddoes not necessarily deal with the famous Spanishartist.
A higher level of disambiguation takes formof classification engines constructed for every am-biguous name from Wikipedia.
A set of most spe-cific terms characterizing each particular entity witha shared name is stored together with an entity iden-tifier and used for disambiguation during the textprocessing phase.
Disambiguation of geographicalnames is performed in a similar manner.3 System Description and ResultsTo compute semantic similarity of two non-emptyauthor fields, normalized textual content is com-pared by an exact match first.
As there is no unifiedform defined for author names entered to the field,the next step applies the NER tool discussed in theprevious section to the field text and tries to identifyall mentioned entities.
Table 2 shows examples oftexts from author fields and their respective annota-tions (in the typewriter font).Dates and places of birth and death as well as fewspecific keywords are put together and used in thefollowing processing separately.
To correctly anno-tate expressions that most probably refer to names ofpeople not covered by the DECIPHER NER tool, weemploy the Stanford NER9 that is trained to identifynames based on typical textual contexts.The final similarity score for a pair of author fieldsis computed by means of the SVR combining spe-cific features characterizing various aspects of thesimilarity.
Simple Jaccard coefficient on recognizedperson names, normalized word overlap of the re-maining text and its edit distance (to deal with typos)are used as basic features.Places of births and deaths, author?s nationality(e. g., Irish painter) and places of work (active inSpain and France) provide data to estimate location-based similarity of authors.
Coordinates of each lo-cation are used to compute an average location forthe author field.
The distance between the averagecoordinates is then applied as a feature.
Since typesof locations (city, state, etc.)
are also available, thenumber of unique location types for each item andthe overlap between corresponding sets are also em-ployed as features.Explicitly mentioned dates as well as informationprovided by the DECIPHER NER are compared too.The time-similarity feature takes into account timeoverlap of the dates and time distance of an earlierand a later event.Other features reflect an overlap between visualart branches represented by artists in question (Pho-tographer, Architect, etc.
), an overlap between theirstyles, genres and all other information availablefrom external sources.
We also employ a matrix ofartistic influences that has been derived from a largecollection of domain texts by means of relation ex-traction methods.Finally, general relatedness of artists is pre-computed from the above-mentioned collection bymeans of Random Indexing (RI), Explicit Seman-tic Analysis (ESA) and Latent Dirichlet Allocation(LDA) methods, stored in sparse matrices and en-tered as a final set of features to the SVR process.The system is implemented in Python and takes9http://nlp.stanford.edu/software/CRF-NER.shtml121Eginton, Francis; West, Benjamin<author name="Francis Eginton" url="http://www.freebase.com/m/0by1w5n">Eginton, Francis</author>; <author name="Benjamin West"url="http://www.freebase.com/m/01z6r6">West, Benjamin</author>Yossef Zaritsky Israeli, born Ukraine, 1891-1985<author name="Joseph Zaritsky" url="http://www.freebase.com/m/0bh71xw"nationality="Israel" place of birth="Ukraine" date of birth="1891"date of death="1985">Yossef Zaritsky Israeli, born Ukraine,1891-1985</author>Man Ray (Emmanuel Radnitzky) 1890, Philadelphia ?
1976, Paris<author name="Man Ray" alternate name="Emmanuel Radnitzky"url="http://www.freebase.com/m/0gskj" date of birth="1890"place of birth="Philadelphia" date of death="1976" place of death="Paris">Man Ray (Emmanuel Radnitzky) 1890, Philadelphia - 1976, Paris</author>Table 2: Examples of texts in the author field and their annotationsadvantage of several existing modules such as gen-sim10 for RI, ESA and other text-representationmethods, numpy11 for Support Vector Regression(SVR) with RBF kernels, PyVowpal12 for an effi-cient implementation of the LDA, and nltk13 for gen-eral text pre-processing.The resulting system was trained and tested on thedata provided by the task organizers.
The train andtest sets consisted each of 750 pairs of cultural her-itage records from Europeana along with the goldstandard for the training set.
The BUT-TYPED sys-tem reached score 0.7592 in the author field (cross-validated results, Pearson correlation) on the train-ing set where 80 % were used for training whereas20 % for testing.
The score for the field on the test-ing set was 0.7468, while the baseline was 0.4278.4 ConclusionsDespite issues related to the low quality of thegold standard data, the attention paid to the sim-ilarity computation on the chosen field showed tobear fruit.
The realized system ranked third among14 others in the criterion we focused on.
Domainknowledge proved to significantly help in measuringsemantic closeness between authors and the resultscorrespond to an intuitive understanding of the sim-10http://radimrehurek.com/gensim/11http://www.numpy.org/12https://github.com/shilad/PyVowpal13http://nltk.org/ilarity between artists.AcknowledgmentsThis work was partially supported by the EC?sSeventh Framework Programme (FP7/2007-2013) under grant agreement No.
270001,and by the Centrum excellence IT4Innovations(ED1.1.00/02.0070).ReferencesAgirre, E., Diab, M., Cer, D., and Gonzalez-Agirre,A.
(2012).
Semeval-2012 task 6: A pilot on se-mantic textual similarity.
In Proceedings of theFirst Joint Conference on Lexical and Computa-tional Semantics-Volume 1: Proceedings of themain conference and the shared task, and Vol-ume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 385?393.
Association for Computational Linguistics.Aletras, N., Stevenson, M., and Clough, P. (2012).Computing similarity between items in a digitallibrary of cultural heritage.
Journal on Computingand Cultural Heritage (JOCCH), 5(4):16.Annesi, P., Storch, V., and Basili, R. (2012).
Spaceprojections as distributional models for seman-tic composition.
In Computational Linguisticsand Intelligent Text Processing, pages 323?335.Springer.122Croce, D., Annesi, P., Storch, V., and Basili, R.(2012).
Unitor: combining semantic text similar-ity functions through sv regression.
In Proceed-ings of the First Joint Conference on Lexical andComputational Semantics-Volume 1: Proceedingsof the main conference and the shared task, andVolume 2: Proceedings of the Sixth InternationalWorkshop on Semantic Evaluation, pages 597?602.
Association for Computational Linguistics.Gabrilovich, E. and Markovitch, S. (2007).
Comput-ing semantic relatedness using wikipedia-basedexplicit semantic analysis.
In Proceedings of the20th International Joint Conference on ArtificialIntelligence, pages 6?12.Landauer, T., Foltz, P., and Laham, D. (1998).
An in-troduction to latent semantic analysis.
Discourseprocesses, 25(2):259?284.Lin, D. (1998).
An information-theoretic definitionof similarity.
In Proceedings of the 15th Inter-national Conference on Machine Learning, vol-ume 1, pages 296?304.
Citeseer.Lintean, M. C. and Rus, V. (2010).
Paraphrase iden-tification using weighted dependencies and wordsemantics.
Informatica: An International Journalof Computing and Informatics, 34(1):19?28.Negri, M., Marchetti, A., Mehdad, Y., Bentivogli,L., and Giampiccolo, D. (2012).
semeval-2012task 8: Cross-lingual textual entailment for con-tent synchronization.
In Proceedings of the FirstJoint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2:Proceedings of the Sixth International Workshopon Semantic Evaluation, pages 399?407.
Associ-ation for Computational Linguistics.Paolacci, G., Chandler, J., and Ipeirotis, P. (2010).Running experiments on amazon mechanicalturk.
Judgment and Decision Making, 5(5):411?419.Sahlgren, M. (2005).
An introduction to random in-dexing.
In Methods and Applications of Seman-tic Indexing Workshop at the 7th InternationalConference on Terminology and Knowledge En-gineering, TKE 2005.
Citeseer.123
