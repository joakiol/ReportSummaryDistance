Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 139?144,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsA Graphical Interface for MT Evaluation and Error AnalysisMeritxell Gonza`lez and Jesu?s Gime?nez and Llu?
?s Ma`rquezTALP Research CenterUniversitat Polite`cnica de Catalunya{mgonzalez,jgimenez,lluism}@lsi.upc.eduAbstractError analysis in machine translation is a nec-essary step in order to investigate the strengthsand weaknesses of the MT systems under de-velopment and allow fair comparisons amongthem.
This work presents an application thatshows how a set of heterogeneous automaticmetrics can be used to evaluate a test bed ofautomatic translations.
To do so, we haveset up an online graphical interface for theASIYA toolkit, a rich repository of evaluationmeasures working at different linguistic lev-els.
The current implementation of the inter-face shows constituency and dependency treesas well as shallow syntactic and semantic an-notations, and word alignments.
The intelli-gent visualization of the linguistic structuresused by the metrics, as well as a set of navi-gational functionalities, may lead towards ad-vanced methods for automatic error analysis.1 IntroductionEvaluation methods are a key ingredient in the de-velopment cycle of machine translation (MT) sys-tems.
As illustrated in Figure 1, they are used toidentify and analyze the system weak points (erroranalysis), to introduce new improvements and adjustthe internal system parameters (system refinement),and to measure the system performance in compari-son to other systems or previous versions of the samesystem (evaluation).We focus here on the processes involved in theerror analysis stage in which MT developers need tounderstand the output of their systems and to assessthe improvements introduced.Automatic detection and classification of the er-rors produced by MT systems is a challenging prob-lem.
The cause of such errors may depend not onlyon the translation paradigm adopted, but also on thelanguage pairs, the availability of enough linguisticresources and the performance of the linguistic pro-cessors, among others.
Several past research worksstudied and defined fine-grained typologies of trans-lation errors according to various criteria (Vilar etal., 2006; Popovic?
et al, 2006; Kirchhoff et al,2007), which helped manual annotation and humananalysis of the systems during the MT developmentcycle.
Recently, the task has received increasing at-tention towards the automatic detection, classifica-tion and analysis of these errors, and new tools havebeen made available to the community.
Examplesof such tools are AMEANA (Kholy and Habash,2011), which focuses on morphologically rich lan-guages, and Hjerson (Popovic?, 2011), which ad-dresses automatic error classification at lexical level.In this work we present an online graphical inter-face to access ASIYA, an existing software designedto evaluate automatic translations using an heteroge-neous set of metrics and meta-metrics.
The primarygoal of the online interface is to allow MT develop-ers to upload their test beds, obtain a large set of met-ric scores and then, detect and analyze the errors oftheir systems using just their Internet browsers.
Ad-ditionally, the graphical interface of the toolkit mayhelp developers to better understand the strengthsand weaknesses of the existing evaluation measuresand to support the development of further improve-ments or even totally new evaluation metrics.
Thisinformation can be gathered both from the experi-139Figure 1: MT systems development cycleence of ASIYA?s developers and also from the statis-tics given through the interface to the ASIYA?s users.In the following, Section 2 gives a generaloverview of the ASIYA toolkit.
Section 3 describesthe variety of information gathered during the eval-uation process, and Section 4 provides details on thegraphical interface developed to display this infor-mation.
Finally, Section 5 overviews recent work re-lated to MT error analysis, and Section 6 concludesand reports some ongoing and future work.2 The ASIYA ToolkitASIYA is an open toolkit designed to assist devel-opers of both MT systems and evaluation measuresby offering a rich set of metrics and meta-metricsfor assessing MT quality (Gime?nez and Ma`rquez,2010a).
Although automatic MT evaluation is stillfar from manual evaluation, it is indeed necessaryto avoid the bottleneck introduced by a fully man-ual evaluation in the system development cycle.
Re-cently, there has been empirical and theoretical justi-fication that a combination of several metrics scoringdifferent aspects of translation quality should corre-late better with humans than just a single automaticmetric (Amigo?
et al, 2011; Gime?nez and Ma`rquez,2010b).ASIYA offers more than 500 metric variants forMT evaluation, including the latest versions of themost popular measures.
These metrics rely on dif-ferent similarity principles (such as precision, recalland overlap) and operate at different linguistic layers(from lexical to syntactic and semantic).
A generalclassification based on the similarity type is givenbelow along with a brief summary of the informa-tion they use and the names of a few examples1.Lexical similarity: n-gram similarity and edit dis-tance based on word forms (e.g., PER, TER,WER, BLEU, NIST, GTM, METEOR).Syntactic similarity: based on part-of-speech tags,base phrase chunks, and dependency and con-stituency trees (e.g., SP-Overlap-POS, SP-Overlap-Chunk, DP-HWCM, CP-STM).Semantic similarity: based on named entities, se-mantic roles and discourse representation (e.g.,NE-Overlap, SR-Overlap, DRS-Overlap).Such heterogeneous set of metrics allow the userto analyze diverse aspects of translation quality atsystem, document and sentence levels.
As discussedin (Gime?nez and Ma`rquez, 2008), the widely usedlexical-based measures should be considered care-fully at sentence level, as they tend to penalize trans-lations using different lexical selection.
The combi-nation with complex metrics, more focused on ad-equacy aspects of the translation (e.g., taking intoaccount also semantic information), should help re-ducing this problem.3 The Metric-dependent InformationASIYA operates over a fixed set of translation testcases, i.e., a source text, a set of candidate trans-lations and a set of manually produced referencetranslations.
To run ASIYA the user must providea test case and select the preferred set of metrics(it may depend on the evaluation purpose).
Then,ASIYA outputs complete tables of score values forall the possible combination of metrics, systems,documents and segments.
This kind of results isvaluable for rapid evaluation and ranking of trans-lations and systems.
However, it is unfriendly forMT developers that need to manually analyze andcompare specific aspects of their systems.During the evaluation process, ASIYA generatesa number of intermediate analysis containing par-tial work outs of the evaluation measures.
Thesedata constitute a priceless source for analysis pur-poses since a close examination of their content al-lows for analyzing the particular characteristics that1A more detailed description of the metric set and its imple-mentation can be found in (Gime?nez and Ma`rquez, 2010b).140Reference The remote control of the Wiihelps to diagnose an infantileocular disease .Ol scoreCandidate 1 The Wii Remote to help diag-nose childhood eye disease .717 = 0.41Candidate 2 The control of the Wii helpsto diagnose an ocular infantiledisease .1314 = 0.93Table 1: The reference sentence, two candidatetranslation examples and the Ol scores calculationdifferentiate the score values obtained by each can-didate translation.Next, we review the type of information used byeach family of measures according to their classifi-cation, and how this information can be used for MTerror analysis purposes.Lexical information.
There are several variants un-der this family.
For instance, lexical overlap (Ol)is an F-measure based metric, which computes sim-ilarity roughly using the Jaccard coefficient.
First,the sets of all lexical items that are found in the ref-erence and the candidate sentences are considered.Then, Ol is computed as the cardinality of their in-tersection divided by the cardinality of their union.The example in Table 1 shows the counts used to cal-culate Ol between the reference and two candidatetranslations (boldface and underline indicate non-matched items in candidate 1 and 2, respectively).Similarly, metrics in another category measure theedit distance of a translation, i.e., the number ofword insertions, deletions and substitutions that areneeded to convert a candidate translation into a ref-erence.
From the algorithms used to calculate thesemetrics, these words can be identified in the set ofsentences and marked for further processing.
Onanother front, metrics as BLEU or NIST computea weighted average of matching n-grams.
An inter-esting information that can be obtained from thesemetrics are the weights assigned to each individualmatching n-gram.
Variations of all of these mea-sures include looking at stems, synonyms and para-phrases, instead of the actual words in the sentences.This information can be obtained from the imple-mentation of the metrics and presented to the userthrough the graphical interface.Syntactic information.
ASIYA considers three lev-els of syntactic information: shallow, constituentand dependency parsing.
The shallow parsing an-notations, that are obtained from the linguistic pro-cessors, consist of word level part-of-speech, lem-mas and chunk Begin-Inside-Outside labels.
Use-ful figures such as the matching rate of a given(sub)category of items are the base of a group ofmetrics (i.e., the ratio of prepositions between areference and a candidate).
In addition, depen-dency and constituency parse trees allow for captur-ing other aspects of the translations.
For instance,DP-HCWM is a specific subset of the dependencymeasures that consists of retrieving and matching allthe head-word chains (or the ones of a given length)from the dependency trees.
Similarly, CP-STM, asubset of the constituency parsing family of mea-sures, consists of computing the lexical overlap ac-cording to the phrase constituent of a given type.Then, for error analysis purposes, parse trees com-bine the grammatical relations and the grammati-cal categories of the words in the sentence and dis-play the information they contain.
Figure 2 and 3show, respectively, several annotation levels of thesentences in the example and the constituency trees.Semantic information.
ASIYA distinguishes alsothree levels of semantic information: named enti-ties, semantic roles and discourse representations.The former are post-processed similarly to the lex-ical annotations discussed above; and the semanticpredicate-argument trees are post-processed and dis-played in a similar manner to the syntactic trees.Instead, the purpose of the discourse representationanalysis is to evaluate candidate translations at doc-ument level.
In the nested discourse structures wecould identify the lexical choices for each discoursesub-type.
Presenting this information to the user re-mains as an important part of the future work.4 The Graphical InterfaceThis section presents the web application that makespossible a graphical visualization and interactive ac-cess to ASIYA.
The purpose of the interface istwofold.
First, it has been designed to facilitate theuse of the ASIYA toolkit for rapid evaluation of testbeds.
And second, we aim at aiding the analysis ofthe errors produced by the MT systems by creating141Figure 2: PoS, chunk and named entity annota-tions on the source, reference and two translationhypothesesFigure 3: Constituency trees for the reference andsecond translation candidatea significant visualization of the information relatedto the evaluation metrics.The online interface consists of a simple web formto supply the data required to run ASIYA, and then,it offers several views that display the results infriendly and flexible ways such as interactive scoretables, graphical parsing trees in SVG format andinteractive sentences holding the linguistic annota-tions captured during the computation of the met-rics, as described in Section 3.4.1 Online MT evaluationASIYA allows to compute scores at three granular-ity levels: system (entire test corpus), document andsentence (or segment).
The online application ob-tains the measures for all the metrics and levels andgenerates an interactive table of scores displayingthe values for all the measures.
Table organiza-Figure 4: The bar charts plot to compare the metricscores for several systemstion can swap among the three levels of granularity,and it can also be transposed with respect to sys-tem and metric information (transposing rows andcolumns).
When the metric basis table is shown, theuser can select one or more metric columns in or-der to re-rank the rows accordingly.
Moreover, thesource, reference and candidate translation are dis-played along with metric scores.
The combination ofall these functionalities makes it easy to know whichare the highest/lowest-scored sentences in a test set.We have also integrated a graphical library2 togenerate real-time interactive plots to show the met-ric scores graphically.
The current version of the in-terface shows interactive bar charts, where differentmetrics and systems can be combined in the sameplot.
An example is shown in Figure 4.4.2 Graphically-aided Error Analysis andDiagnosisHuman analysis is crucial in the development cy-cle because humans have the capability to spot er-rors and analyze them subjectively, in relation to theunderlying system that is being examined and thescores obtained.
Our purpose, as mentioned previ-ously, is to generate a graphical representation ofthe information related to the source and the trans-lations, enabling a visual analysis of the errors.
Wehave focused on the linguistic measures at the syn-tactic and semantic level, since they are more robustthan lexical metrics when comparing systems basedon different paradigms.
On the one hand, one ofthe views of the interface allows a user to navigateand inspect the segments of the test set.
This viewhighlights the elements in the sentences that match a2http://www.highcharts.com/142given criteria based on the various linguistic annota-tions aforementioned (e.g., PoS prepositions).
Theinterface integrates also the mechanisms to uploadword-by-word alignments between the source andany of the candidates.
The alignments are also vi-sualized along with the rest of the annotations, andthey can be also used to calculate artificial annota-tions projected from the source in such test beds forwhich there is no linguistic processors available.
Onthe other hand, the web application includes a libraryfor SVG graph generation in order to create the de-pendency and the constituent trees dynamically (asshown in Figure 3).4.3 Accessing the DemoThe online interface is fully functional and accessi-ble at http://nlp.lsi.upc.edu/asiya/.
Al-though the ASIYA toolkit is not difficult to install,some specific technical skills are still needed in or-der to set up all its capabilities (i.e., external com-ponents and resources such as linguistic processorsand dictionaries).
Instead, the online application re-quires only an up to date browser.
The website in-cludes a tarball with sample input data and a videorecording, which demonstrates the main functional-ities of the interface and how to use it.The current web-based interface allows the userto upload up to five candidate translation files, fivereference files and one source file (maximum size of200K each, which is enough for test bed of about1K sentences).
Alternatively, the command basedversion of ASIYA can be used to intensively evaluatea large set of data.5 Related WorkIn the literature, we can find detailed typologies ofthe errors produced by MT systems (Vilar et al,2006; Farru?s et al, 2011; Kirchhoff et al, 2007) andgraphical interfaces for human classification and an-notation of these errors, such as BLAST (Stymne,2011).
They represent a framework to study theperformance of MT systems and develop further re-finements.
However, they are defined for a specificpair of languages or domain and they are difficultto generalize.
For instance, the study described in(Kirchhoff et al, 2007) focus on measures relying onthe characterization of the input documents (source,genre, style, dialect).
In contrast, Farru?s et al (2011)classify the errors that arise during Spanish-Catalantranslation at several levels: orthographic, morpho-logical, lexical, semantic and syntactic errors.Works towards the automatic identification andclassification of errors have been conducted very re-cently.
Examples of these are (Fishel et al, 2011),which focus on the detection and classification ofcommon lexical errors and misplaced words usinga specialized alignment algorithm; and (Popovic?and Ney, 2011), which addresses the classifica-tion of inflectional errors, word reordering, missingwords, extra words and incorrect lexical choices us-ing a combination of WER, PER, RPER and HPERscores.
The AMEANA tool (Kholy and Habash,2011) uses alignments to produce detailed morpho-logical error diagnosis and generates statistics at dif-ferent linguistic levels.
To the best of our knowl-edge, the existing approaches to automatic errorclassification are centered on the lexical, morpho-logical and shallow syntactic aspects of the transla-tion, i.e., word deletion, insertion and substitution,wrong inflections, wrong lexical choice and part-of-speech.
In contrast, we introduce additional lin-guistic information, such as dependency and con-stituent parsing trees, discourse structures and se-mantic roles.
Also, there exist very few tools de-voted to visualize the errors produced by the MTsystems.
Here, instead of dealing with the automaticclassification of errors, we deal with the automaticselection and visualization of the information usedby the evaluation measures.6 Conclusions and Future WorkThe main goal of the ASIYA toolkit is to cover theevaluation needs of researchers during the develop-ment cycle of their systems.
ASIYA generates anumber of linguistic analyses over both the candi-date and the reference translations.
However, thecurrent command-line interface returns the resultsonly in text mode and does not allow for fully ex-ploiting this linguistic information.
We present agraphical interface showing a visual representationof such data for monitoring the MT development cy-cle.
We believe that it would be very helpful for car-rying out tasks such as error analysis, system com-parison and graphical representations.143The application described here is the first releaseof a web interface to access ASIYA online.
Sofar, it includes the mechanisms to analyze 4 out of10 categories of metrics: shallow parsing, depen-dency parsing, constituent parsing and named en-tities.
Nonetheless, we aim at developing the sys-tem until we cover all the metric categories currentlyavailable in ASIYA.Regarding the analysis of the sentences, we haveconducted a small experiment to show the ability ofthe interface to use word level alignments betweenthe source and the target sentences.
In the near fu-ture, we will include the mechanisms to upload alsophrase level alignments.
This functionality will alsogive the chance to develop a new family of evalua-tion metrics based on these alignments.Regarding the interactive aspects of the interface,the grammatical graphs are dynamically generatedin SVG format, which proffers a wide range of inter-active functionalities.
However their interactivity isstill limited.
Further development towards improvedinteraction would provide a more advanced manip-ulation of the content, e.g., selection, expansion andcollapse of branches.Concerning the usability of the interface, we willadd an alternative form for text input, which will re-quire users to input the source, reference and candi-date translation directly without formatting them infiles, saving a lot of effort when users need to ana-lyze the translation results of one single sentence.Finally, in order to improve error analysis capa-bilities, we will endow the application with a searchengine able to filter the results according to varieduser defined criteria.
The main goal is to providethe mechanisms to select a case set where, for in-stance, all the sentences are scored above (or below)a threshold for a given metric (or a subset of them).AcknowledgmentsThis research has been partially funded by the Span-ish Ministry of Education and Science (OpenMT-2, TIN2009-14675-C03) and the European Commu-nity?s Seventh Framework Programme under grantagreement numbers 247762 (FAUST project, FP7-ICT-2009- 4-247762) and 247914 (MOLTO project,FP7-ICT-2009-4- 247914).ReferencesEnrique Amigo?, Julio Gonzalo, Jesu?s Gime?nez, and Fe-lisa Verdejo.
2011.
Corroborating text evaluation re-sults with heterogeneous measures.
In Proc.
of theEMNLP, Edinburgh, UK, pages 455?466.Mireia Farru?s, Marta R. Costa-Jussa`, Jose?
B. Marin?o,Marc Poch, Adolfo Herna?ndez, Carlos Henr?
?quez, andJose?
A. Fonollosa.
2011.
Overcoming Statistical Ma-chine Translation Limitations: Error Analysis and Pro-posed Solutions for the Catalan?Spanish LanguagePair.
LREC, 45(2):181?208.Mark Fishel, Ondr?ej Bojar, Daniel Zeman, and Jan Berka.2011.
Automatic Translation Error Analysis.
In Proc.of the 14th TSD, volume LNAI 3658.
Springer Verlag.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2008.
Towards Het-erogeneous Automatic MT Error Analysis.
In Proc.
ofLREC, Marrakech, Morocco.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010a.
Asiya:An Open Toolkit for Automatic Machine Translation(Meta-)Evaluation.
The Prague Bulletin of Mathemat-ical Linguistics, (94):77?86.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2010b.
LinguisticMeasures for Automatic Machine Translation Evalua-tion.
Machine Translation, 24(3?4):77?86.Ahmed El Kholy and Nizar Habash.
2011.
AutomaticError Analysis for Morphologically Rich Languages.In Proc.
of the MT Summit XIII, Xiamen, China, pages225?232.Katrin Kirchhoff, Owen Rambow, Nizar Habash, andMona Diab.
2007.
Semi-Automatic Error Analysis forLarge-Scale Statistical Machine Translation Systems.In Proc.
of the MT Summit XI, Copenhagen, Denmark.Maja Popovic?
and Hermann Ney.
2011.
Towards Auto-matic Error Analysis of Machine Translation Output.Computational Linguistics, 37(4):657?688.Maja Popovic?, Hermann Ney, Adria` de Gispert, Jose?
B.Marin?o, Deepa Gupta, Marcello Federico, Patrik Lam-bert, and Rafael Banchs.
2006.
Morpho-SyntacticInformation for Automatic Error Analysis of Statisti-cal Machine Translation Output.
In Proc.
of the SMTWorkshop, pages 1?6, New York City, USA.
ACL.Maja Popovic?.
2011.
Hjerson: An Open Source Toolfor Automatic Error Classification of Machine Trans-lation Output.
The Prague Bulletin of MathematicalLinguistics, 96:59?68.Sara Stymne.
2011.
Blast: a Tool for Error Analysis ofMachine Translation Output.
In Proc.
of the 49th ACL,HLT, Systems Demonstrations, pages 56?61.David Vilar, Jia Xu, Luis Fernando D?Haro, and Her-mann Ney.
2006.
Error Analysis of Machine Trans-lation Output.
In Proc.
of the LREC, pages 697?702,Genoa, Italy.144
