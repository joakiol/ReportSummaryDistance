Automatic Stochastic Tagging ofNatural Language TextsEvangelos Dermatas ?University of PatrasGeorge Kokkinakis*University of PatrasFive language and tagset independent stochastic taggers, handling morphological and contextualinformation, are presented and tested in corpora of seven European languages (Dutch, English,French, German, Greek, Italian and Spanish), using two sets of grammatical tags; a small setcontaining the eleven main grammatical c asses and a large set of grammatical categories commonto all languages.
The unknown words are tagged using an experimentally proven stochastichypothesis that links the stochastic behavior of the unknown words with that of the less probableknown words.
A fully automatic training and tagging program has been implemented onan IBMPC-compatible 80386-based computer.
Measurements oferror rate, time response, and memoryrequirements have shown that the taggers" performance is satisfactory, even though a smalltraining text is available.
The error rate is improved when new texts are used to update thestochastic model parameters.1.
IntroductionIn the natural language processing community, there has been a growing awareness ofthe key importance that lexical and corpora resources, especially annotated corpora,have to play, both in the advancement of research in this area and in the develop-ment of relevant products.
In order to reduce the huge cost of manually creating suchcorpora, the development of automatic taggers is of paramount importance.
In thisrespect, the ability of a tagger to handle both known and unknown words, to improveits performance by training, and to achieve a high rate of correctly tagged words, isthe criterion for assessing its usability in practical cases.Several taggers based on rules, stochastic models, neural networks, and hybridsystems have already been presented for Part-of-speech (POS) tagging.
Rule-basedtaggers (Brill 1992; Elenius 1990; Jacobs and Zernik 1988; Karlsson 1990; Karlsson etal.
1991; Voutilainen, Heikkila, and Antitila 1992; Voutilainen and Tapanainen 1993)use POS-dependent constraints defined by experienced linguists.
A small error ratehas been achieved by such systems when a restricted, application-dependent POS setis used; e.g., an error rate of 2-6 percent has been reported by Marcus, Santorini, andMarcinkiewicz (1993) using the Penn Treebank corpus.
Nevertheless, if a large POS setis specified, the number of rules increases ignificantly and rule definition becomeshighly costly and cumbersome.Stochastic taggers use both contextual and morphological information, and themodel parameters are usually defined or updated automatically from tagged texts(Cerf-Danon and E1-Beze 1991; Church 1988; Cutting et al 1992; Dermatas and Kokki-nakis 1988, 1990, 1993, 1994; Garside, Leech, and Sampson 1987; Kupiec 1992; Maltese* Department of Electrical Engineering, Wire Communications Laboratory (WCL), University of Patras,265 00 Patras, Greece.
E-mail: dermatas@wcl.ee.upatras.gr.
(~) 1995 Association for Computational LinguisticsComputational Linguistics Volume 21, Number 2and Mancini 1991; Meteer, Schwartz, and Weischedel 1991; Merialdo 1991; Pelillo,Moro, and Refice 1992; Weischedel et al 1993; Wothke et al 1993).
These taggers arepreferred when tagged texts are available for training, and large tagsets and multilin-gual applications are involved.
In the case where additionally raw untagged text isavailable, the Maximum Likelihood training can be used to reestimate the parametersof HMM taggers (Merialdo 1994).Connectionist models have been used successfully for lexical acquisition (Eineborgand Gamback 1993; Elenius 1990; Elenius and Carlson 1989; Nakamura et al 1990).Correct classification rates up to 96.4 percent have been achieved in the latter case bytesting on the Teleman Swedish corpus.
On the other hand, a time-consuming trainingprocess has been reported.Recently, several solutions to the problem of tagging unknown words have beenpresented (Charniak et al 1993; Meteer, Schwartz, and Weischedel 1991).
Hypothesesfor unknown words, both stochastic (Dermatas and Kokkinakis 1993, 1994; Malteseand Mancini 1991; Weischedel et al 1993), and connectionist (Eineborg and Gamback1993; Elenius 1990) have been applied to unlimited vocabulary taggers.
In taggers thatare based on hidden Markov models (HMM), parameters of the unknown words areestimated by taking into account morphological information from the last part of theword (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991).
Accurate tagging ofseven European languages has been achieved in the first case (error rates of 3-13 per-cent for a detailed POS set), but an enormous amount of training text is requiredfor the estimation of the parameters for unknown words.
Similar results have beenreported by Maltese and Mancini (1991) for the Italian language.
Weischedel et al(1993) have used four categories of word morphology, such as inflectional endings,derivational endings, hyphenation, and capitalization.
For the case in which only arestricted training text is available, a simple, language- and tagset-independent HMMtagger has been presented by Dermatas and Kokkinakis (1993), where the HMM pa-rameters for the unknown words are estimated by assuming that the POS probabilitydistribution of the unknown words and the POS probability distribution of the lessprobable words in the small training text are identical.In this paper, five natural anguage stochastic taggers that are able to predict POSof unknown words are presented and tested following the process of developing anno-tated corpora (the most recently fully tagged and corrected text is used to update themodel parameters).
Three stochastic optimization criteria and seven European lan-guages (Dutch, English, French, German, Greek, Italian and Spanish) and two POSsets are used in the tests.
The set of main grammatical classes and an extended setof detailed grammatical categories i the same in all languages.
The testing materialconsists of newspaper texts with 60,000-180,000 words for each language and an En-glish EEC-law text with 110,000 words.
This material was assembled and annotatedin the framework of the ESPRIT-291/860 project "Linguistic Analysis of the EuropeanLanguages."
In addition, we present ransformations of the taggers' calculations to afixed-point arithmetic system, which are useful for machines without floating-pointhardware.The taggers handle both lexical and tag transition information, and without per-forming morphological nalysis can be used to annotate corpora when small trainingtexts are available.
Thus, they are preferred when a new language or a new tagsetis used.
When the training text is adequate to estimate the tagger parameters, moreefficient stochastic taggers (Dermatas and Kokkinakis 1994; Maltese and Mancini 1991;Weischedel et al 1993) and training methods can be implemented (Merialdo 1994).The structure of this paper is as follows: in Section 2 the stochastic tagging modelsare presented in detail.
In Section 3 the influence of the training text errors and the138Dermatas and Kokkinakis Stochastic Taggingsources of stochastic tagger errors are discussed, followed, in Section 4, by a short pre-sentation of the implementation.
In Section 5, statistical measurements on the corporaand a short description of the taggers' performance is given.
Detailed experimentalresults are included in Appendices A and B.2.
Stochastic Tagging ModelsA stochastic optimal sequence of tags T, to be assigned to the words of a sentenceW, can be expressed as a function of both lexical P(W \[ T) and language model P(T)probabilities using Bayes' rule:To -- argmaxP(T \[ W) = argmax P(W \[ T) ?
P(T) = argmaxP(W \[ T) ?
P(T) (1) P(W) T T TSeveral assumptions and approximations on the probabilities P(W \[ T) and P(T)lead to good comprises concerning memory and computational complexity.2.1 Hidden Markov Model (HMM) ApproachThe tagging process can be modeled by an HMM by assuming that each hidden tagstate produces a word in the sentence, each word wi is uncorrelated with neighboringwords and their tags, and each tag is probabilistic dependent on the N previous tagsonly.2.1.1 Most probable tag sequence (HMM-TS).
The optimal tag sequence for a givenobservation sequence of words is given by the following equation:N M MZ~ HMM-TS) -- a rgmaxP(h)H P(ti \[ ti-1 .
.
.
.
.
h) H P(ti \] t i -1 , .
.
.
,  ti-N) H P(wi \[ ti)tl,...,tM i=2 i=N+I i=1(2)where M is the number of words in the sentence W.The optimal solution is estimated by the well-known Viterbi algorithm.
The first-(Rabiner 1989) and second- (He 1988) order Viterbi algorithms have been presentedelsewhere.
Recently, Tao (1992) described the Viterbi algorithm for generalized HMMs.2.1.2 Most probable tags (HMM-T).
The optimal criterion is to choose the tags thatare most likely to be computed independently at each word event:To HMM-T) = {tio, tio -----argmaxP(ti\[W)},tii = 1,M (3)The optimum tag tio is estimated using the probabilities of the forward-backwardalgorithm (Rabiner 1989):rio -- argmax P(ti, W) = argmax P(ti, wl,.
?., wi)P(wi+l,..., WM \[ ti) (4)ti tiThe probabilities in equation 4 are estimated recursively for the first- (Rabiner1989) and second-order HMM (Watson and Chung 1992).The main difference between the optimization criteria in 2.1.1 and that in 2.1.2results from the definition of the expected correct tagging rate; the HMM-TS modelmaximizes the correctly tagged sentences, while the HMM-T model maximizes thecorrectly tagged words.139Computational Linguistics Volume 21, Number 22.1.3 Stochastic hypothesis for the unknown words.
When a new text is processed,some words are unknown to the tagger lexicon (i.e.
they are not included in the trainingtext).
In this case, in order to use the forward-backward and the Viterbi algorithm wemust estimate the unknown word's conditional probabilities P(w I t).
Methods for theestimation of these probabilities have already been proposed (e.g.
the use of wordendings morphology).
Nevertheless, these methods fail if only a small training text isavailable because of the huge number of events not occurring in this text, such as pairsof tags and word endings.
To address the above problem we have approximated theconditional probabilities of the unknown word tags by the conditional probabilities ofthe less probable word tags, i.e.
tags of the words occurring only once.
In the followingwe demonstrate experimentally that this approximation is valid and independent ofthe training text size.Figures 1 and 2 show the probability distributions of the tags in the training text(known words) and that of the words occurring only once in this text for the Englishand French language, respectively.
Furthermore, the tags' probability distribution ofthe words that are not included in the training text and are characterized as unknownwords is shown.
This distribution is measured in a different open testing text, i.e.a text that may include both known and unknown words.
The measurements werecarried out on newspaper text and split into two parts of the same size--the trainingand the open testing text.
Each part contained 90,000 words for the English text and50,000 words for the French text.
In this experiment, a tagset comprising the maingrammatical categories was used: Verb (Vet), Noun (Nou), Adjective (Adj), Adverb(Adv), Pronoun (Pro), Preposition (Pre), Article/Determiner (A-D), Conjunction (Con),Particle (Par), Interjection (Int), Miscellaneous (Mis; i.e., tags that cannot be classifiedin the previous categories).This experiment has two significant results:a.
The probability distribution of the tags of unknown words is significantly differentfrom the distribution for known words, while it is very close to the probabilitydistribution of the tags of the less probable known words both in the Englishand French text.b.
A number of closed and functional grammatical c asses has very low probabilityfor both unknown and words occurring only once, e.g., the tags article,determiner, conjunction, pronoun, miscellaneous in English text, andarticle, determiner, conjunction, pronoun, interjection and miscellaneousin French text.In the English text, verbs, adjectives and conjunctions are more frequent han inthe French text.
On the other hand, prepositions in the French text have a 0.05 greaterprobability, which is also the most significant difference between the distributions ofthe two languages.
Prepositions in the words occurring only once and in unknownwords are minimal in the English text, while in the French text one out of ten unknownwords is a preposition.
The text coverage by prepositions i 11.2 percent for the Englishand 16.2 percent for the French corpus.
This difference increases ignificantly in thelexicon coverage: 0.47 percent for the English and 1.54 percent for the French lexicon.In Figures 3 and 4, the results of chi-square tests that measure the differencebetween the probability distribution of the tags of the less probable words and thatof the unknown words are shown.
Various sizes of training text and two sets ofgrammatical categories, the main set (11 classes) and an extended set (described indetail in Section 5) were used.140Dermatas and Kokkinakis Stochastic Tagging0,70,60,5.~ 0,4J~o 0,3Q.0,20,10 , .Nou IntKnown wordsUnknown wordsoccurring only Wordselm uB am B I I  imVer Mis Pre A-D Adj Pro Adv Con ParGrammatical classFigure 1Distribution of the main grammatical classes of the known and unknown words and thewords occurring only once in English text.0,70,60,5>,0,4o 0,30,20,1m 0 , .Nou Int?
Known wordsUnknown words?
Words occurring onlyonceVer Mis Pre A-D Adj Pro Adv Con ParGrammatical classFigure 2Distribution of the main grammatical classes of the known and unknown words and thewords occurring only once in French text.141Computational Linguistics Volume 21, Number 2Specifically, the grammatically abeled text of 180,000 word entries of the Englishlanguage was separated into two parts: the training text, where the tag probabilitiesdistribution of the less probable words was estimated, and the open testing text, wherethe tag probabilities distribution of the unknown words was measured.
Multiple chi-square experiments were carried out by transferring successively a portion of 30,000words from the open testing text to the training text and by modifying the wordoccurrence threshold from 1 to 15 in order to determine the experimentally optimalthreshold.
Words having an occurrence below or equal to this threshold in the trainingtext are counted as less probable words.
The results of the tests shown in Figures 3 and4 include threshold values up to 15 because the difference between the distributionsfor values greater than 15 increases ignificantly.As shown in the above figures, the close relation between the tested probabil-ity distributions is evident for all sizes of training and testing text.
Furthermore, weobserve that:a.b.C.d.e.The chi-square distance between the tag probability distributions iminimized for low values of the word occurrence threshold.
In the tagsetof main grammatical classes, this distance is minimized for thresholdvalues less than three, four, or five, depending on the training text size.In the extended set of grammatical classes the distance is minimized inall cases for the threshold value one; i.e., when only the words occurringonce in the training text are regarded as less probable words.In the English text the chi-square distance between the tag.
probabilitydistributions i  minimized for 120,000 words training text for the set ofmain grammatical classes and for 60,000 words for the extended set.
Thesame results are measured in the French text.There is no significant variation in the chi-square test results foradditional training text.The closed and functional grammatical classes can be estimatedautomatically as the less probable grammatical classes of the lessprobable words in the tagged text.
(The manual definition process istime-consuming when a set of detailed grammatical classes is used).The probability distribution of some grammatical classes of the unknownwords changes ignificantly when the size of the training text isincreased.
These changes can be measured in the training text from thetags' distribution of the less probable words.Similar results have been achieved by testing the Dutch, German, Greek, Italian,and Spanish texts, both with the tagset of the main grammatical categories and withthe common extended set of grammatical categories.Based on the above we can complete both optimization criteria of the HMM for-mulation, given in 2.1.1 and 2.1.2, by calculating the conditional probability of theunknown word tags using Bayes' rule:P(Unknown word \[ ti) = P(ti\[ Unknown word)P(Unknown word) P(ti)P(ti \] Less probable word)P(Unknown word) (5)- -  P(ti)142Dermatas and Kokkinakis Stochastic TaggingTagset of Main Grammatical Classes0,035 - -~  30K .~I ~B l~ 60K0,03 ~k~ )< 120K0,0250,020,0150,011 2 3 4 5 6 7 8 9 10 11 12 13 14 15Word Occurrence ThresholdFigure 3Chi-square test for the main grammatical classes' distribution of the unknown and the lessprobable words in the English text for various training text sizes.Extended tagset of Grammatical classes?
30K0,14 t ~60K .0,13 .1~ "\[ ~.
90K ~0,12 / X 120K0,110,10,090,080,070,060,050,041 2 3 4 5 6 7 8 9 10 11 12 13 14 15Word Occurrence ThresholdFigure 4Chi-square test for the distribution of the grammatical tags of the unknown words and theless probable words in the English text, for the extended tagset of grammatical classes andvarious training text sizes.143Computational Linguistics Volume 21, Number 2The probability P(Unknown word) is approximated in open testing texts by mea-suring the unknown word frequency.
Therefore the model parameters are adapted each timean open testing text is being tagged.
The probability P(t I Less probable word) and thetags probability P(t) are measured in the training text.
Finally, each tag-conditionalprobability of the unknown word tags is normalized:L~_, P(wj \[ ti) + P(Unknown word I ti) = 1,j= lVi = 1, T (6)where L is the number of the known words and T is the number of tags.2.2 Tagging without Lexical ProbabilitiesWhen the corresponding lexical probabilities p(w I t) are not available in the dictionarythat specifies the possible tags for each word, a simple tagger can be implemented byassuming that each word wi in a sentence is uncorrelated with the assigned tag ti; e.g.,p(wi l ti) = p(wi).In this case the most probable tag sequence, according to equation 2, is given by:N MT~MLM) = argmaxP(h)1-IP(ti \[t i -b .
.
.
,h )  1-I P(ti \[ t i-1,.. .
,t i-N)tl,...,tM i=2 i=N+I(7)which is a Nth-order Markovian chain for the language model (MLM).Taggers based on MLM require the training process to store each tag assigned toevery lexicon entry and to define the unknown word tagset.2.2.1 Stochastic hypothesis for the unknown words.
The unknown word tagset isdefined by the selection of the most probable tags that have been assigned to the lessprobable words of the training text.
In this way the unknown words' ambiguity isdecreased significantly.
The word occurrence threshold used to define the less prob-able words and a tag probability threshold used to isolate the less probable tags areestimated experimentally.Extensive experiments have shown insignificant differences in the tagging errorrate when alternative word occurrence thresholds have been tested.
The best resultsare obtained when values less than 10 are used.
In this paper the word occurrencethreshold has been set to one in all experiments.3.
Tagger Errors3.1 Errors in the Training TextTaggers based on the HMM technique compensate for some serious training problemsinherent in the MLM approach.
The most important one is the presence of errors inthe training text.
This situation appears when uncorrected tags or analysts' mistakesremain in the text used to estimate the stochastic model parameters.
These errorsgenerate tag assignments that are not valid.
In MLM taggers these tags are equallyweighted to the correct ones.
In contrast, in HMM taggers invalid assignments arebiased by the very low value of the corresponding conditional probability of the tags(the wrong tag rarely appears in the specific word environment), which decreases theoverall probability for incorrect ag assignments.144Dermatas and Kokkinakis Stochastic TaggingAnother important issue concerns the HMM ability to handle lexicon information,e.g., to find how frequently the tags have been assigned to each lexicon entry.
In somelanguages, taggers based on HMMs almost reduce the prediction error to the halfcompared to the MLM approach.3.2 Tagger prediction errorsGenerally, tagger errors can be classified into three categories:a.b.C.Errors due to inadequate training data.
When the model parameters areestimated from a limited amount of training data, tagging errors appearbecause of unknown or inaccurately estimated conditional probabilities.Various interpolation techniques have been proposed for the estimationof the model parameters for unseen events or to smooth the modelparameters (Church and Gale 1991; Essen and Steinbiss 1992; Jardinoand Adda 1993; Katz 1987; McInnes 1992).Errors due to the syntactical or grammatical style of the testing text.
This type oferror appears when the testing text has a style unknown to the model(i.e., a style used in the open testing text, not included in the trainingtext).
It can be reduced by using multiple models that have beenpreviously trained in different ext styles.Errors due to insufficient model hypotheses.
In this case the model hypothesesare not satisfied; e.g., there are strong intra-tag relations in distancesgreater than the model order, idiomatic expressions, language dependentexceptions, etc.
A general solution to the variable length and depth ofdependency for HMM has been already proposed (Tao 1992), but has notbeen implemented in taggers.4.
ImplementationIn this section we present echniques to speed up the tagging process and avoid un-derflow or overflow phenomena during the estimation of the optimum solution.
Thesetechniques do not increase the prediction error rate or have only minimal influenceon it, as proven in the experiments.Two modules consume the majority of the tagger computational time.
The firstmodule extracts from the model parameters the intra-tag and the word-tag conditionalprobabilities requested by the second module, which computes the optimum solutionby multiplying the corresponding conditional probabilities.
Binary search maximizesthe searching speed of the first module, while the following three transformationtechniques decrease the computing time of the second module, avoid underflow oroverflow phenomena, nd use the faster and low-cost fixed-point arithmetic system.4.1 Logarithmic TransformationThe stochastic solutions described by equations 2 and 7 are computed by multiplyingseveral conditional probabilities.
The floating-point multiplications of these probabili-ties are transformed into an equal number of floating-point additions, by computingthe logarithm of the optimum criterion probability.
This technique solves the under-flow problem which arises when many small probabilities are multiplied, and accel-erates the tagger esponse time.145Computational Linguistics Volume 21, Number 24.2 Fixed-Point TransformationThe fixed-point transformation converts the floating-point logarithmic additions intoan equal number of fixed-point additions.
It is realized by the following quantizationprocess:\ [ /max (ln(Pmin)-ln(Px))\] (8) Ix ---- Round Mw ln(Pmin)where: Px is a conditional probability, Pmin  is the minimum conditional probability inthe model parameter set,/max is the maximum integer of the fixed-point arithmeticsystem, Mw is the maximum number of words in a sentence and Round\[.\] is a quan-tization function mapping real numbers into the nearest integer.After the logarithmic and the fixed-point ransformation, equations 2 and 7 be-come:NI (HMM-Ts) -- argmaxI(tl) + ~_,I(ti lt i_l,...,h)tl,...,tM i=2M M+ ~_, I(ti \] ti-1,...,ti-N) + ~I (w i  I ti) (9)i=N+l  i=1N MI~ MLM) = argmaxI(tl) + ~__I(ti I ti_, .... ,tl) + ~ I(ti I ti_,,...,ti-N) (10)tl ..... tM i=2 i=N+IThe quantization function approximates the computations, producing theoretically dif-fering solutions.
In practice the prediction error differences measured for all languages,taggers, and tagsets were less than 0.02 percent.4.3 ScalingThe solution obtained by the forward-backward algorithm cannot be logarithmicallytransformed because of the presence of summations.
It is well known that for HMMsthe forward and backward probabilities tend exponentially tozero.
The scaling processintroduced in this case multiplies the forward and backward probabilities by a scalingfactor at selective word events in order to keep the computations within the floating-point dynamic range of the computer (Rabiner 1989).4.4 Hardware--SoftwareThe taggers have been realized under MS-DOS using a 32-bit C compiler.
The lexiconsize is limited by the available RAM.
A mean value of 35 bytes per word is allocated.The tagger speed exceeds the rate of 500 word/sec in a 80386 (33MHz) for all languagesand tagsets in text with known words.
A maximum memory requirement of 930Kbhas been measured in the experiments described in this paper.A set of symbols and keywords (a sentence separators et) and the maximumlength of a sentence are the only manually defined parameters when the HMM taggersare applied.In the MLM taggers, the word occurrence threshold that isolates the less probablewords and the tag probability threshold used to reject he less probable tags from theunknown words tagset are the manually defined parameters.The training process has been designed to estimate or update the model param-eters from fully tagged text without any manual intervention.
Therefore, frequencymeasurements are defined or updated as model parameters instead of conditional146Dermatas and Kokkinakis Stochastic TaggingTable 1Size of the corpora.Text Dutch English French German Greek Italian SpanishNewspaper 110,000 180,000 100,000 100,000 120,000 160,000 60,000EEC-Law -- 110,000 .
.
.
.
.Table 2ESPRIT 291/860: Project partners.Country PartnerEnglandFranceGermanyGreeceItalyItalyNetherlandsSpainAcorn Computers LimitedCentre National de la Recherche Scientifique (CNRS), LIMSI DivisionRuhr - Universitaet Bochum, Lehrstuhl fur Allgemeine Elektrotechnikund AkustikUniversity of Patras, Wire Communications Laboratory (WCL), Speech andLanguage GroupIng.
C. Olivetti & C., S.p.A.Centro Studi Applicazioni in Tecnologie Avanzate - CSATAKatholieke Universiteit Nijmegen, Dienst A-FaculteitenUniversidad National de Educacion aDistancia (UNED), Madridprobabilities that are computed afterwards by using the corresponding relative fre-quencies.5.
Performance of the Systems5.1 TaggersFive taggers have been realized and tested using bi-POS and tri-POS transition prob-abilities.
Specifically, the first- and the second-order MLM (MLM1 and MLM2, re-spectively), the first- and the second-order HMM of the most probable tag sequencecriterion (HMM-TS1 and HMM-TS2, respectively), and the first-order HMM of themost probable tag criterion (HMM-T1) have been realized.5.2 CorporaThe tagger performance has been measured in extensive xperiments carried out oncorpora of seven languages, English, Dutch, German, French, Greek, Italian and Span-ish, annotated according to detailed grammatical categories.
In Table 1, the type andthe size of these corpora is shown.
They are part of corpora selected in the frameworkof the ESPRIT-I project 291/860: "Linguistic Analysis of the European Languages"(1985-1989) by the project partners (Table 2) and annotated by using semi-automatictaggers.
Manual correction was performed by experienced, native analysts for eachlanguage separately.
In all languages the entries were tagged as they appeared in thetext.
In the German corpus, for example, where multiple words are concatenated, thewords were not separated.5.3 TagsetsTwo sets of grammatical tags were isolated from a unified set of grammatical categoriesdefined in the ESPRIT I project 291/860 (ESPRIT-860, Internal report, 1986):147Computational Linguistics Volume 21, Number 2Table 3Extended set of grammatical categories.Main grammatical categories Detailed grammatical informationAdjective, Noun, PronounAdverbArticle, Determiner, PrepositionVerbRegular base comparative superlative interrogative p rsonnumber caseRegular base comparative superlative interrogativePerson number caseTense voice mood person umber caseTable 4Number of grammatical tags.Text Dutch English French German Greek Italian SpanishMain set 9 News: 10, Law: 10 10 11 11 10 10Extended set 50 News: 43, Low: 36 14 116 443 121 121Table 5Word ambiguity in the newspaper corpus.Tagset English Dutch German French Greek Italian SpanishMain set 1.336 1.111 1.3 1.69 1.209 1.62 1.197Extended set 1.417 1.291 1.878 1.705 1.855 1.729 1.25a.b.A common tagset of 11 main grammatical categories for each language,as described in 2.1.3.An extended set including common categorization f the grammaticalinformation for all languages, as shown in Table 3.
In some languages anumber of grammatical categories i not applicable.
The depth ofgrammatical nalysis and the grammatical structure of each languageproduce a different number of POS tags.
In Table 4 the number of POStags used for each language and each set of grammatical categories ishown.5.4 Corpus AmbiguityThe corpus ambiguity was measured by the mean number of possible tags for eachword of the corpus for both sets of grammatical tags (Table 5).
The most ambiguoustexts are the French, Italian, and English in the tagset of main grammatical c asses andthe German, Greek, Italian, and French in the extended set of grammatical categories.In Figure 5 the percent occurrence of unknown words in an open testing text of10,000 words is shown versus the size of the training text.The Italian and Greek corpora have the greatest number of unknown words fol-lowed by the Spanish corpus (for the available results with restricted training text).Taking into account he word ambiguity in the training text (Table 5), the occur-rence of unknown words in the open testing text (Figure 5), and the hypothesis thatthe unknown word tagset and the application tagset are the same, the ambiguity ofthe open testing corpus for both sets of grammatical categories was computed for a50,000-word training corpus (Table 6).148Dermatas and Kokkinakis Stochastic Tagging3525%15Dutch-B-Engl ish\ --A- French--X- German~ -~-Greek~(~ -O-  ItalianI I I I I I I I I I I I I I2 3 4 5 6 7 8 9 10 11 12 13 14 15Size of training text ('1 OK words)Figure 5Percentage of unknown words in open testing text of 10,000 words for various izes of thetraining text.Table 6Corpus ambiguity in newspaper open testing text.Tagset English Dutch German French Greek Italian SpanishMain set 8.75 7.83 9.9 9.19 9.32 8.5 8.5Extended set 37.03 42.78 103.07 12.8 367.25 99.86 100.69For the set of main grammatical c asses the ambiguity of the open testing corpusis more or less the same for all languages, varying from a minimum of 7.83 tags perword in the Dutch text to a maximum of 9.32 in the Greek corpus.
For the extendedset of grammatical categories three types of corpora can be distinguished:a.
The most ambiguous i the corpus of the Greek language, because of thegreat number of grammatical tags (443) and the strong presence ofunknown words in the open testing text.b.
In the German, Spanish, and Italian texts the same ambiguity ismeasured.c.
The least ambiguous are the Dutch and French texts.Taking into account he previous results, it is important to note that the great dif-ferences between languages in text ambiguity, in the presence of unknown words andin the statistics of the grammatical categories, e.g.
the different occurrence of preposi-tions in English and French corpora, prevent a direct comparison of languages fromthe taggers' error rate.
Apart from a few obvious observations given in Section 5.7,such a comparison would require a detailed examination of the corpora and the tag-gers' errors by experienced linguists.
Therefore, the prediction error rates presented in149Computational Linguistics Volume 21, Number 2Table 7Lexicon size for 100,000-word training text.Language Dutch English French German Greek ItalianLexicon size 13,700 12,200 13,500 8,900 17,400 15,300this paper should be regarded only as indication of the probabilistic taggers' efficiencyin each separate language when small training texts are available.5.5 ExperimentsThe corpora were divided into 10,000-word entries.
All parts except he last one wereused to create (initially) and update the model parameters successively.
The last partwas tagged each time after the model parameters were updated, giving results of thetagger performance on open testing text.
The influence of the application tagset on thetagger performance was measured by testing the two totally different agsets describedin Section 5.3.The experimental process was repeated for each language, tagset and tagger.Thus a total number of 2 (tagsets) ?
5 (taggers) ~ \[7 (languages) + 1 (Test on EnglishEEC-law text)\] = 80 experiments was carried out.5.6 Tagger Speed and Memory RequirementsIn Figures 6 and 7 the tagger speed and the memory requirements after the last mem-ory adaptation process are presented for all taggers and languages, and for the ex-tended tagset.The Greek and Italian corpora have a great number of lexical entries (differentword forms) for the same amount of 100,000-word training text, as shown in Table 7.As a result these taggers require more memory (Figure 7).
In contrast, the small sizeof the German lexicon decreases the required memory.Tagger speed is closely related to the corpus ambiguity (Table 6).
The ambiguityof the Greek corpus is more than three times greater than the next one, the Germancorpus.The significant influence of the training text size on tagger speed is proven bycomparing the experimental results in the English corpus (newspaper and EEC-Law).When the taggers are trained using the 170,000 words of the English newspaper corpus,a greater number of lexicon entries and a greater number of transition probabilities(Figure 7) is measured than in the case of the EEC-law corpus (100K words trainingtext).
The model becomes more complex, but tagger speed is slightly higher because ofthe greater size of the training text, which reduces the presence of unknown words inthe testing text.
Generally, tagger speed increases when the training text is increased.5.7 Tagger Error RateThe actual tagger error rates for all experiments are given in Appendices A and B. Inthis section we present a discussion of these error rates.The error rate depends trongly on the test text and language, and the type andsize of the tagset.
The worst results have been obtained for the Greek language becauseof its significantly greater ambiguity, the number of tags (requiring significantly greatertraining text), and its freer syntax.In the main category of tagset experiments, the model parameters for the MLMsystems are estimated accurately when the training text exceeds 50,000-90,000 words,150Ou)01000100108007000,1Dut\/Tiii  $1 $2 HMM-T1I I I I I I IEng Eng- Fre Get Gre Ira SpaLawLanguageFigure 6Tagger speed after the last adaptation process for the extended set of grammatical categories.600==500400Gre Ita Spa3002OODut--4P-- MLM1MLM2,L HMM-TS1X HMM-TS2Eng Eng Fre GerLawLanguageDermatas and Kokkinakis Stochastic TaggingFigure 7Tagger memory requirements for the extended set of grammatical categories.151Computational Linguistics Volume 21, Number 245?
352515Dutch-B -  EnglishFrench-X -  German-~-  Greek-~ I ta l ian~--  Span ishI I I I I J I \] I I \] I I I \[ I I2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18Size of training text ('1 OK words)Figure 8Unknown word error rate for the HMM-TS2 tagger and the set of main grammaticalcategories.in contrast to the extended tagset experiments, where a greater-size training text forthe German, Greek, and Spanish languages i required.
This phenomenon becomesstronger in taggers based on the HMM where the accuracy of the P(w J t) estimation isproportional to the word and the tag frequency of occurrence in the training text.
Thus,for all tagsets and languages a larger training text is required in order to minimize theerror rate.The taggers based on the HMM reduce the prediction error almost to half incomparison to the same order taggers based on MLM.
Strong dependencies on thelanguage and the estimation accuracy of the model parameters influence this reduction.The alternative HMM solutions give trivial performance differences, confirming recentresults obtained in the Treebank corpus by using an HMM tagger (Merialdo 1991).Concerning the performance of the taggers in unknown words, we present in Fig-ure 8 as an example the HMM-TS2 error rate for the tagset of the main grammaticalcategories, which is also the worst case for this set of grammatical categories.
Gener-ally the error rate decreases when the training text is increased.
The stochastic modelis successful for only half of the unknown words for the Italian text and for approx-imately two out of three unknown words for the English text.
In all other languagesthe HMM-TS2 tagger gives the correct solution for three out of four unknown words.Similar results are achieved when the extended set of grammatical categories itested.
In this case the unknown word error rate increases about 10-20 percent forall the languages except he Greek language.
In the Greek text the error rate reachesapproximately 65percent when 100,000-word text is used to define the parameters ofthe HMM.The unknown words, which initially cover about 25-35 percent of the text, arereduced to 8-15 percent when all the available text is used as training data.
In the ma-jority of the experiments, the tagger error rate decreases when new text updates the152Dermatas and Kokkinakis Stochastic Taggingmodel parameters.
Trivial differences of the tagger learning rates between languagesand tagsets how the efficiency of the training method in estimating the model transi-tion probabilities for the tested languages and the validity of the stochastic hypothesisfor the unknown words.6.
Conclus ionIn this paper five automatic, stochastic taggers that are able to tag unknown wordshave been presented.
The taggers have been tested in newspaper corpora of sevenEuropean languages and an EEC-law text of the English language using two sets ofgrammatical categories.
When new training text updates the model parameters, thetagging error rate changes as expected: in text with unknown words a lower error rateis measured, proving the efficiency of the relative frequencies learning method andthe validity of the hypothesis for the unknown words' stochastic behavior.153Computational Linguistics Volume 21, Number 2Appendix A: Tests in the Main Grammatical Categories SetError rate (%) Pred ic t ion  Error  for the  Dutch languageMain  grammat ica l  c lasses  14131211109-8-7-6543?
e .
_ .
- - - - - -13-_  " ?
- - .
."
"O .
,- .
e  .
.
.
.
.
.
oTagging system, MLM1- - -  MLM2..... HMM-TS1.
.
.
.
HMM-TS2..... HMM-T1Size of the training text (* 10 Kwords)Error rate (%)19181716-15-14-13-12-11-10-9-8 ~7-Prediction Error for the English languageMain grammatical classesI, .
.
.
,', ~ , ~  ~ -0.~ ~ ~ _  , ~ ,~xo , \o.. '~" ,8 - - .~O _ , _ Q6-5 -i ~, ~ ,~ 5 6 -;' 8 o 1'o 1'11', 1'31'41'51'61~Size of the training text ('10 Kwords)Tagging systemMLM1MLM2HMM-TS1H M M -TS 2HMM-T1154Dermatas  and  Kokk inak is  S tochast ic  Tagg ingError rate (%)11Prediction Error for the English EEC-law text109 -8 -7 -65432Main grammatical classes.
.
.
.
e ?
-~- .
.
.
.
~ - .
.
.
.
-=' o -  .
.
.
.
.
- .o~ _oi 2 3 4 6 6 ~ 6 9 f0Size of the training text ( '10  Kwords)Tagg ing  system, MLM1MLM2.. .
.
.
HMM-TS1.
.
.
.
HMM-TS2.
.
.
.
.
HMM-T1Prediction Error for the French language Error rate (%)16-15-14-13-12-11109 -8 -7 -6 -5 -i =4 i 2 3 4Size of the training text ('10 Kwords)Main grammatical classes.
.
.
.
.
\ \ .
.
.
.- .
.~\\x,:,Tagging system, MLM1.
.
.
.
MLM2.. .
.
.
HMM-TS1.
.
.
.
.
HMM-TS2.
.
.
.
.
HMM-T1155Computational Linguistics Volume 21, Number 2Error rate (%)16-15-14-13-1211109- 827-6 -s:4-3-2-1Prediction Error for the German languageMain grammatical classes\"NN\  N\~-"  xSize of the training text ('10 Kwords)Tagging system, MLM1MLM2..... HMM-TS1.
.
.
.
HMM-TS2.. .
.
.
HMM-T1Error rate (%)24-Prediction Error for the Greek language222018161412108642Main grammatical classes-~ .0.. L. : ' ,~_  .
.
.
.
.
@~.
.
.
.
.
o ~  _ _I 2 3 4 5 6 7 8 9 1'0 11Size of the training text ('10 Kwords)Tagging system, MLM1- - -  MLM2..... HMM-TS1.
.
.
.
HMM-TS2--~-- HMM-T1156Dermatas and Kokkinakis Stochastic TaggingError rate (%) Prediction Error for the Italian languageMain grammatical classes\o~.,\\ \38-3634-32-30-28-26-24-22-20-18161412lo i 2 3 ;, s 6 -; 8 9 1'o 1'1 1'2 1'3 ;4 l'sSize of the training text ( '10 Kwords)Tagging system, MLM1- - -  MLM2.. .
.
.
HMM-TS1.
.
.
.
H M M -TS2.
.
.
.
.
HMM-T1Error rate (%)18-17-16-15-14.13-12-10Prediction Error for the Spanish languageMain grammatical classes Tagging system, MLM1- - -  M LM2.. .
.
.
HMM-TS1- ~--- HMM-TS2- -~-- -HMM-T1\ \o ~ "  .
~ ,.
.
.
.
.
.
oSize of the training text ( '10 Kwords)157Computational Linguistics Volume 21, Number 2Appendix B: Tests in the Extended Grammatical Categories SetError rate (%) Predict ion Error for the Dutch languageExtended grammatica l  c lasses  191817-16-15-14-13-12-11-10-9-8-7-6, , ,  \Tagging system, MLM1MLM2..... HMM-TS1.. .
.
HMM-TS2--~--.
HMM-T1Size of the training text ('10 Kwords)Error rate (%)22212019-18-17-16-15-14-13-12Predict ion Error for the Engl ish languageExtended grammat ica l  set10I?
i"' i~ ' ,~ - ~ \~""~ ~ .
.
.
.
.?
~, ~ ~" '0  "~?
>~.~.
"~~: ' .~-~.- -e .
- .
.e .
_ .
.
(3  _ .
_oSize of the training text ('10 Kwords)Tagging systemMLM1MLM2HMM-TS1HMM-TS2HMM-T1158Dermatas and Kokkinakis Stochastic TaggingError rate (%)13Prediction Error for the English EEC-law text12111098i7~6-5 -4 -2Extended grammatical classesC,,x. "
--...?
%.. "- ~ ~-----..~?
, \?
.
.
.
~ '~.i ~, 3 ;.
s 6 -~ 8 9 1'oSize of the training text ('10 Kwords)Tagging system, MLM1.
.
.
.
MLM2..... HMM-TS1-~ HMM-TS2... .
.
HMM-T1Error rate (%)16Prediction Error for the French languageExtended grammatical classes151413121110987654~++ ~_~Size of the training text ('10 Kwords)Tagging system, MLM1- - -  MLM2..... HMM-TS1.
.
.
.
H M M -TS2--~--.
HMMoT1159Computational Linguistics Volume 21, Number 2Error rate (%)2826242220-18-16-14-12-10-8Prediction Error for the German languageExtended grammatical classes' \  '%,, " , ' , \  \.
.
.
.
.
- _ v : ~ - ~ v = .
_ _ , = ~  ~?
.
.
.
.
.
.
.
.
.
--:,=~=" .... ---:a- - r - - -  - i .
.
.
.
.
.
.
.
.
r .
.
.
.
.
.
r .
.
.
.
.
T .
.
.
.
.
.  '
-  -1 2 3 4 5 6 7 8 9Size of the training text ('10 Kwords)Tagging system, MLM1- - -  MLM2..... HMM-TS1.
.
.
.
HMM-TS2.
.
.
.
.
HMM-T1Error rate (%):t35-30-25-2015Prediction Error for the Greek languageExtended grammatical classesR4,, ,?
"x \  " \" " ,3 -  .e~ e- O "- -43 - -Size of the training text ('10 Kwords)Tagging systemMLM1MLM2HMM-TS1HMM-TS2HMM-T1160Dermatas and Kokkinakis Stochastic TaggingError rate (%)45-Prediction Error for the Italian language403530252015105Extended grammatical set~,  i i i?
"~<,x~h 3 4 6 6 7 8 9 10 11 12 la 14 isSize of the training text ('10 Kwords)Tagging system, MLM1- - -  MLM2..... HMM-TS1.
.
.
.
HMM-TS2.
.
.
.
.
HMM-T1Error rate (%)26-Prediction Error for the Spanish language2422-20-18-16-14-12108Extended grammatical classeso\\.
.
"N  .
.
.
.
.. .
.
.
.  "
" = oh a , sSize of the training text ('10 Kwords)Tagging system, MLM1- - -  MLM2..... HMM-TS1.
.
.
.
HMM-TS2.
.
.
.
.
HMM-T1161Computational Linguistics Volume 21, Number 2ReferencesBrill, E. (1992).
"A simple rule-based part ofspeech tagger."
In Proceedings, ThirdConference on Applied Natural LanguageProcessing.
Trento, Italy, 152-155.Cerf-Danon, H., and EI-Beze, M.
(1991).
"Three different probabilistic languagemodels: Comparison and combination.
"In Proceedings, International Conference onAcoustics Speech and Signal Processing,297-300.Charniak, E.; Hendrickson, C.; Jacobson, N.;and Perkowitz, M. (1993).
"Equations forpart-of-speech tagging."
In Proceedings,National Conference on Artificial Intelligence.Church, K. (1988).
"A stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondConference on Applied Natural LanguageProcessing.
Austin, Texas, 136-143.Church, K., and Gale, W. (1991).
"Acomparison of the enhanced Good-Turingand deleted estimation methods forestimating probabilities of Englishbigrams."
Computer Speech and Language 5,19-24.Cutting, D.; Kupiec, J.; Pederson, J.; andSibun, P. (1992).
"A practicalpart-of-speech tagger."
In Proceedings,Third Conference on Applied NaturalLanguage Processing.
Trento, Italy, 133-140.Dermatas, E., and Kokkinakis, G.
(1988).
"Semi automatic labelling of Greek texts.
"In Proceedings, Seventh FASE SymposiumSPEECH "88.
Edinburgh, 239-245.Dermatas, E., and Kokkinakis, G. (1993).
"Asystem for automatic text labelling."
InProceedings, Eurospeech-90.
Paris, 382-385.Dermatas, E., and Kokkinakis, G. (1993).
"Afast multilingual probabilistic tagger."
InProceedings, Eurospeech-93.
Berlin,1323-1326 (presented also in theEurospeech-93 exhibition).Dermatas, E., and Kokkinakis, G. (1994).
"Amultilingual unlimited vocabularystochastic tagger."
In Advanced SpeechApplications--European Commission ESPRIT(1), edited by K. Varghese, S. Pfleger, andJ.
Lefevre, 98-106.
Springer-Verlag.Eineborg, M., and Gamback, B.
(1993).
"Back-propagation based lexicalacquisition experiments."
In Proceedings,NeuroNimes: Neural Networks and theirIndustrial & Cognitive Applications.
Nimes,169-178.Elenius, K.
(1990).
"Comparing aconnectionist and rule based model forassignment parts-of-speech."
InProceedings, International Conference onAcoustics, Speech and Signal Processing,597-600.Elenius, K., and Carlson, R.
(1989).
"Assigning parts-of-speech of words fromtheir orthography using a connectionistmodel."
In Proceedings, European Conferenceon Speech Communication a d Technology.Paris, 534-537.Partners of ESPRIT-291/860 (1986).
"Unification of the word classes of theESPRIT Project 860."
BU-WKL-0376.Internal Report.Essen, U., and Steinbiss, V.
(1992).
"Cooccurrence smoothing for statisticallanguage modelling."
In Proceedings,International Conference on Acoustics, Speechand Signal Processing, 161-164.Garside, R.; Leech, G.; and Sampson, G.(1987).
The Computational Analysis ofEnglish: A Corpus-Based Approach.Longman.He, Y.
(1988).
"Extended Viterbi algorithmfor second order hidden Markovprocess."
In Proceedings, InternationalConference on Acoustics, Speech and SignalProcessing, 718-720.Jacobs, P., and Zernik, U.
(1988).
"Acquiringlexical knowledge from text: A casestudy."
In Proceedings, Seventh NationalConference on Artificial Intelligence.
SaintPaul, Minnesota, 739-744.Jardino, M., and Adda, G.
(1993).
"Automatic word classification usingsimulated annealing."
In Proceedings,International Conference on Acoustics, Speechand Signal Processing, 41-44.Karlsson, E (1990).
"Constraint grammar asa framework for parsing running text."
InProceedings, Thirteenth InternationalConference on Computational Linguistics.Helsinki, Finland, 168-173.Karlsson, F.; Voutilainen, A.; Anttila, A.; andHeikkila, J.
(1991).
"Constraint grammar:A language-independent sys em forparsing unrestricted text, with anapplication to English."
In Workshop Notesfrom the Ninth National Conference onArtificial Intelligence.
Anaheim, California.Katz, S. (1987).
"Estimation of probabilitiesfrom sparse data for the language modelcomponent of a speech recognizer."
IEEETrans.
on Acoustics, Speech, and LanguageProcessing, 35(3), 400-401.Kupiec, J.
(1992).
"Robust part-of-speechtagging using a Hidden Markov Model.
"Computer Speech & Language, 6(3), 225-242.Maltese, G., and Mancini, F. (1991).
"Atechnique to automatically assignparts-of-speech to words taking into162Dermatas and Kokkinakis Stochastic Taggingaccount word-ending informationthrough a probabilistic model."
InProceedings, Eurospeech-91, 753-756.Marcus, M.; Santorini, B.; andMarcinkiewicz, M. (1993).
"Building alarge annotated corpus of English: ThePenn Treebank."
Computational Linguistics,19(2), 315-330.McInnes, E (1992).
"An enhancedinterpolation technique forcontext-specific probability estimation ispeech and language modelling."
InProceedings, International Conference onSpoken Language Processing, 1491-1494.Merialdo, B.
(1991).
"Tagging text with aprobabilistic model."
In InternationalConference on Acoustics, Speech and SignalProcessing, 809-812.Merialdo, B.
(1994).
"Tagging English textwith a probabilistic model."
ComputationalLinguistics, 20(2), 155-171.Meteer, M.; Schwartz, R.; and Weischedel, R.(1991).
"Empirical studies in part ofspeech labelling."
In Proceedings, FourthDARPA Workshop on Speech and NaturalLanguage.
Morgan Kaufman.Nakamura, M.; Maruyama, K.; Kawabata,T.
; and Shikano, K. (1990).
"Neuralnetwork approach to word categoryprediction for English texts."
InProceedings, Thirteenth InternationalConference on Computational Linguistics.Helinski, Finland, 213-218.Pelillo, W.; Moro, E; and Refice, M.
(1992).
"Probabilistic prediction ofparts-of-speech from spelling usingdecision trees."
In Proceedings,International Conference on Spoken LanguageProcessing, 1343-1346.Rabiner, L. (1989).
"A tutorial on HiddenMarkov Models and selected applicationsin speech recognition."
In Proceedings,IEEE 77(2), 257-285.Tao, C. (1992).
"A generalisation f discreteHidden Markov Model and of Viterbialgorithm."
Pattern Recognition, 25(11),1381-1397.Voutilainen, A., and Tapanainen, P.
(1993).
"Ambiguity resolution in a reductionisticparser."
In Proceedings, Sixth Conference ofthe European Chapter of the Association forComputational Linguistics.
Utrecht,Netherlands, 394-403.Voutilainen, A.; Heikkila, J.; and Antitila, A.(1992).
"Constraint grammar of English.
"Publication 21, Department of GeneralLinguistics, University of Helinski,Helinski, Finland.Watson, B., and Chung Tsoi, A.
(1992).
"Second order Hidden Markov Modelsfor speech recognition."
In Proceedings,Fourth Australian International Conference onSpeech Science and Technology, 146-151.Weischedel, R.; Meteer, M.; Schwartz, R.;Ramshaw, L.; and Palmucci, J.
(1993).
"Coping with ambiguity and unknownwords through probabilistic models.
"Computational Linguistics, 19(2), 359-382.Wothke, K.; Weck-Ulm, I.; Heinecke, J.;Mertineit, O.; and Pachunke, T.
(1993).
"Statistically based automatic tagging ofGerman text corpora withparts-of-speech--some experiments."TR75.93.02-IBM.
IBM Germany,Heidelberg Scientific Center.163
