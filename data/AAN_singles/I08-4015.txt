The Character-based CRF Segmenter of MSRA&NEUfor the 4th BakeoffZhenxing Wang1,2, Changning Huang2 and Jingbo Zhu11 Institute of Computer Software and Theory, Northeastern University,Shenyang, China, 1100042 Microsoft Research Asia, 49, Zhichun Road,Haidian District, Beijing, China, 100080zxwang@ics.neu.edu.cnv-cnh@microsoft.comzhujingbo@mail.neu.edu.cnAbstractThis paper describes the Chinese WordSegmenter for the fourth InternationalChinese Language Processing Bakeoff.Base on Conditional Random Field (CRF)model, a basic segmenter is designed as aproblem of character-based tagging.
Tofurther improve the performance of oursegmenter, we employ a word-based ap-proach to increase the in-vocabulary (IV)word recall and a post-processing to in-crease the out-of-vocabulary (OOV) wordrecall.
We participate in the word segmen-tation closed test on all five corpora andour system achieved four second best andone the fifth in all the five corpora.1 IntroductionSince Chinese Word Segmentation was firstlytreated as a character-based tagging task in (Xueand Converse, 2002), this method has been widelyaccepted and further developed by researchers(Peng et al, 2004), (Tseng et al, 2005), (Low etal., 2005), (Zhao et al, 2006).
Thus, as a powerfulsequence tagging model, CRF became the domi-nant method in the Bakeoff 2006 (Levow, 2006).In this paper, we improve basic segmenter un-der the CRF work frame in two aspects, namelyIV and OOV identification respectively.
We usethe result from word-based segmentation to revisethe CRF output so that we gain a higher IV wordrecall.
For the OOV part a post-processing rule isproposed to find those OOV words which arewrongly segmented into several fractions.
Oursystem performs well in the Fourth Bakeoff,achieving four second best and on the fifth in allthe five corpora.
In the following of this paper, wedescribe our method in more detail.The rest of this paper is organized as follows.In Section 2, we first give a brief review to thebasic CRF tagging approach and then we proposeour methods to improve IV and OOV performancerespectively.
In Section 3 we give the experimentresults on the fourth Bakeoff corpora to show thatour method is effective to improve the perfor-mance of the segmenter.
In Section 4, we con-clude our work.2 Our Word Segmentation SystemIn this section, we describe our system in moredetail.
Our system includes three modules: a basicCRF tagger, a word-base segmenter to improvethe IV recall and a post-processing rule toimprove the OOV recall.
In the following of thissection, we introduce these three modulesrespectively.2.1 Basic CRF taggerSequence tagging approach treat Word Segmenta-tion task as a labeling problem.
Every character ininput sentences will be given a label which indi-cates whether this character is a word boundary.Our basic CRF1 tagger is almost the same as thesystem described in (Zhao et al, 2006) except weadd a feature to incorporate word information,which is learned from training corpus.1 CRF tagger in this paper  is implemented by CRF++which is downloaded from http://crfpp.sourceforge.net/98Sixth SIGHAN Workshop on Chinese Language ProcessingType Feature FunctionUnigram C-1, C0, C1 Previous, current and next characterBigram C-1 C0, C0 C1 Two adjacent characterJump C-1 C1 Previous character and next characterWord Flag F0 F1 Whether adjacent characters form an IV wordTable 1 Feature templates used for CRF in our systemUnder the CRF tagging scheme, each characterin one sentence will be given a label by CRFmodel to indicate which position this characteroccupies in a word.
In our system, CRF tag set isproposed to distinguish different positions in themulti-character words when the word length isless than 6, namely 6-tag set {B, B2, B3, M, E,O}.
Here, Tag B and E stand for the first and thelast position in a multi-character word, respective-ly.
S stands up a single-character word.
B2 and B3stand for the second and the third position in amulti-character word, whose length is larger thantwo-character or three-character.
M stands for thefourth or more rear position in a multi-characterword, whose length is larger than four-character.We add a new feature, which also used inmaximum entropy model for word segmentationtask by (Low et al, 2005), to the feature templatesfor CRF model while keep the other features sameas (Zhao et al, 2006).
The feature templates aredefined in table 1.
In the feature template, only theWord Flag feature needs an explanation.
The bi-nary function F0 = 1 if and only if C-1 C0  form a IVword, else F0 = 0 and F1 = 1 if and only if C0 C1form a IV word, else F1 = 0.2.2 Word based segmenter and revise rulesFor the word-based word segmentation, we collectdictionary from training corpus first.
Instead ofMaximum Match, trigram language model 2trained on training corpus is employed for disam-biguation.
During the disambiguation procedure, abeam search decoder is used to seek the mostpossible segmentation.
For detail, the decoderreads characters from the input sentence one at atime, and generates candidate segmentations in-crementally.
At each stage, the next incoming cha-racter is combined with an existing candidate intwo different ways to generate new candidates: itis either appended to the last word in the candidate,or taken as the start of a new word.
This methodguarantees exhaustive generation of possible seg-2 Language model used in this paper is SLRIM down-loaded from http://www.speech.sri.com/projects/srilm/mentations for any input sentence.
However, theexponential time and space of the length of theinput sentence are needed for such a search and itis always intractable in practice.
Thus, we use thetrigram language model to select top B (B is aconstant predefined before search and in our expe-riment 3 is used) best candidates with highestprobability at each stage so that the search algo-rithm can work in practice.
Finally, when thewhole sentence has been read, the best candidatewith the highest probability will be selected as thesegmentation result.After we get word-based segmentation result,we use it to revise the CRF tagging result similarto (Zhang et al, 2006).
Since word-based segmen-tation result also corresponds to a tag sequenceaccording to the 6-tag set, we now have two tagsfor each character, word-based tag (WT) and CRFtag (CT).
Which tag will be kept as the final resultdepends on Marginal Probability (MP) of the CT.Here, we give a short explanation about whatis the MP of the CT.
Suppose there is a sentenceMcccC ...10?
, where ic  is the character this sen-tence containing.
CRF model gives this sentence aoptimal tag sequenceMtttT ...10?
, where it is thetag foric .
If tti ?
and },,,,,{ 32 SEMBBBt ?
,the MP ofit is defined as:??
??
?TttTi CTPCTPtt i )|()|()(MP ,Here, )|( CTP is the conditional probability giv-en by CRF model.
For more detail about how tocalculate this conditional probability, please referto (Lafferty et al, 2001).Assume that the tag assigned to the currentcharacter is CT by CRF and WT by word-basedsegmenter respectively.
The rules under which werevise CRF result with word-based result is that ifMP(CT) of a character is less than a predefinedthreshold and WT is not ?S?, the WT of this cha-racter will be kept as the final result, else the CTof the character will be kept as the final result.99Sixth SIGHAN Workshop on Chinese Language ProcessingThe restriction that WT should not be ?S?
isreasonable because word-based segmentation isincapable to recognize the OOV word and alwayssegments OOV word into single characters.
Be-sides CRF model is better at dealing with OOVword than our word-based segmentation.
WhenWT is ?S?
it is possible that current word is anOOV word and segmented into single characterwrongly by the word-based segmenter, so the CTof the character should be kept under such situa-tion.
For more detail about this analysis pleaserefer to (Wang et al, 2008).2.3 Post-processing ruleThe rules we described in last subsection is help-ful to improve the IV word recall and now we in-troduce our post-processing rule to improve theOOV recall.Our post-processing rule is designed to dealwith one typical type of OOV errors, namely anOOV word wrongly segmented into several parts.In practice many OOV errors belong to such type.The rule is quite simple.
When we read a sen-tence from the result we get by the last step, wealso kept the last N sentences in memory, in oursystem we set N equals to 20.
We do this becauseadjacent sentences are always relevant and somenamed entity likely occurs repeatedly in these sen-tences.
Then, we scan these sentences to find alln-grams (n from 2 to 7) and count their occur-rence.
If certain n-gram appears more than a thre-shold and this n-gram never appears in trainingcorpus, the n-gram will be selected as a word can-didate.
Then, we filter these word candidates ac-cording to the context entropy (Luo and Song,2004).
Assume w  is a word candidate appearsn times in the current sentence and last N sen-tences and },...,,{ 10 laaa??
is the set of left sidecharacters of w .
Left Context Entropy (LCE) canbe defined as:???
?ia ii waCnwaCnwLCE ),(log),(1)(Here, ),( waC i is the count of concurrence ofia and w .
For the Right Context Entropy, the de-finition is the same except change left into right.Now, we define Context Entropy (CE) of a wordcandidate w as ))(),(min( wRCEwLCE .
Theword candidates with CE larger than a predefinedthreshold will be bind as a whole word in test cor-pus no matter what tag sequence the segmentergiving it.
If a shorter n-gram is contained in alonger n-gram and both of them satisfy the abovecondition, the shorter n-gram will be overlookedand the longer n-gram is bind as a whole word.3 Evaluation of Our SystemOn the corpora of the Fourth Bakeoff, we evaluateour system.
We carry out our evaluation on theclosed tracks.
It means that we do not use any ad-ditional knowledge beyond the training corpus.The thresholds set for MP and CE on each corpusare tuned on left-out data of training corpus bycross validation.
To analyze our methods on IVand OOV words, we use a detailed evaluation me-tric than Bakeoff 2006 (Levow, 2006) which in-cludes Foov and Fiv.
Our results are shown in Ta-ble 2.
In Table 2, the row ?Basic Model?
meansthe results produced by our basic CRF tagger, therow ?+IV?
means the results produced by thecombination of CRF tagger and word-based seg-menter and the row ?+IV+OOV?
means the resultwe get by executing post-processing rule on thecombination results.
The F measure of the basicCRF tagger alone in the Table 2 is within the topthree in the closed tests except Cityu.
Performanceon Cityu corpus is not so good because the incon-sistencies existing in Cityu training and test corpo-ra.
In the training corpus the quotation marks are?
?while in test corpus quotation marks are?
?,which never apper in the training corpus.
As areult, a lot of errors were caused by quotationmarks.
For example, the following four character???
?were combined as a one word in ourresult and fragment???
?was tagged as twowords??
and ??.
Because CRF tagger nevermet ?
and ?
in training corpus so the taggergave the most common tags, namely B and E tothe quotation marks, which cause segmentationerrors not only on quotation marks themselves butalso on the characters adjacent to them.
Weremove these inconsistencies munually and gotthe F measure 0.5 percentage higer than the rusultin table 2.
This result is within the top three in theclosed tests.
On all the five corpora, our ?+IV?module can increase the Fiv and our ?+OOV?module can increase Foov respectively.
However,these improvements are not significant.100Sixth SIGHAN Workshop on Chinese Language ProcessingCorpus Method R P F ROOV POOV FOOV RIV PIV FIVCKIPBasic Model 0.946 0.923 0.940 0.651 0.719 0.683 0.969 0.948 0.958+ IV 0.949 0.935 0.942 0.647 0.741 0.691 0.973 0.948 0.960+ IV + OOV 0.950 0.936 0.943 0.656 0.748 0.699 0.973 0.949 0.961CityUBasic Model 0.944 0.934 0.939 0.654 0.721 0.686 0.970 0.951 0.960+ IV 0.946 0.936 0.941 0.655 0.738 0.694 0.972 0.951 0.962+ IV + OOV 0.949 0.937 0.943 0.678 0.759 0.716 0.973 0.951 0.962CTBBasic Model 0.953 0.951 0.952 0.703 0.727 0.715 0.967 0.964 0.965+ IV 0.954 0.952 0.953 0.697 0.747 0.721 0.969 0.963 0.966+ IV + OOV 0.954 0.953 0.953 0.703 0.749 0.725 0.969 0.964 0.966NCCBasic Model 0.940 0.928 0.934 0.438 0.580 0.499 0.965 0.940 0.952+ IV 0.944 0.930 0.936 0.434 0.603 0.504 0.969 0.941 0.955+ IV + OOV 0.945 0.932 0.939 0.450 0.620 0.522 0.970 0.943 0.956SXUBasic Model 0.960 0.953 0.956 0.636 0.674 0.654 0.977 0.967 0.972+ IV 0.962 0.955 0.958 0.637 0.696 0.665 0.980 0.967 0.973+ IV + OOV 0.962 0.955 0.959 0.645 0.702 0.673 0.979 0.968 0.974Table 2 performance each step of our system achieves4 Conclusions and Future WorkIn this paper, we propose a three-stage strategy inChinese Word Segmentation.
Based on the resultsproduced by basic CRF, our word-based segmen-tation module and post-processing module aredesigned to improve IV and OOV performancerespectively.
The results above show that our sys-tem achieves the state-of-the-art performance.Since only the CRF tagger is good enough as weshown in our experiment, in the future work wewill pay effort on the semi-supervised learning forCRF model in order to mining more useful infor-mation from training and test corpus for CRF tag-ger.ReferencesJohn Lafferty, Andrew McCallum, and Fernando Perei-ra.
2001.
Conditional random fields: probabilisticmodels for segmenting and labeling sequence data.In Proceedings of ICML-2001, pages 591?598.Gina-Anne Levow.
2006.
The Third International Chi-nese Language Processing Bakeoff: Word Segmen-tation and Named Entity Recognition.
In  Proceed-ings of the Fifth SIGHAN Workshop on ChineseLanguage Processing , pages 108-117, Sydney: Ju-ly.Jin Kiat Low, Hwee Tou Ng, and Wenyuan Guo.
2005.A Maximum Entropy Approach to Chinese WordSegmentation.
In Proceedings of the Fourth SIGHANWorkshop on Chinese Language Processing, pages 161-164, Jeju Island, Korea.Zhiyong Luo, Rou Song, 2004.
?An integrated methodfor Chinese unknown word extraction?, In Proceed-ings of Third SIGHAN Workshop on Chinese Lan-guage Processing, pages 148-154.
Barcelona, Spain.Fuchun Peng, Fangfang Feng, and Andrew McCallum.2004.
Chinese segmentation and new word detectionusing conditional random fields.
In COLING 2004,pages 562?568.
Geneva, Switzerland.Huihsin Tseng, Pichuan Chang et al 2005.
A ConditionalRandom Field Word Segmenter for SIGHAN Bakeoff2005.
In Proceedings of the Fourth SIGHAN Workshopon Chinese Language Processing, pages 168-171, JejuIsland, Korea.Zhenxing Wang, Changning Huang and Jingbo Zhu.2008.
Which Performs Better on In-VocabularyWord Segmentation: Based on Word or Character?In Proceeding of the Sixth Sighan Workshop onChinese Language Processing.
To be published.Neinwen Xue and Susan P. Converse.
2002.
Combin-ing Classifiers for Chinese Word Segmentation.
InProceedings of the First SIGHAN Workshop onChinese Language Processing, pages 63-70, Taipei,Taiwan.Ruiqiang Zhang, Genichiro Kikui and Eiichiro Sumita.2006.
Subword-based Tagging by Conditional Ran-dom Fields for Chinese Word Segmentation.
In Pro-ceedings of the Human Language Technology Con-ference of the NAACL, Companion volume, pages193-196.
New York, USA.Hai Zhao, Changning Huang et al 2006.
Effective TagSet Selection in Chinese Word Segmentation viaConditional Random Field Modeling.
In Proceed-ings of PACLIC-20.
pages 87-94.
Wuhan, China,Novemeber.101Sixth SIGHAN Workshop on Chinese Language Processing
