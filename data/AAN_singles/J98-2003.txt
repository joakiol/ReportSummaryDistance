Contextual Grammars as GenerativeModels of Natural LanguagesSo lomon Marcus*University of BucharestGheorghe  P~un*Institute of Mathematics of the Roma-nian AcademyCar los  Mar t in -V ide  tRovira i Virgili UniversityThe paper discusses some classes of contextual grammars--mainly those with "maximal use ofselectors"--giving some arguments that these grammars can be considered a good model fornatural anguage syntax.A contextual grammar produces a language starting from a finite set of words and iterativelyadding contexts to the currently generated words, according to a selection procedure: ach contexthas associated with it a selector, a set of words; the context is adjoined to any occurrence of such aselector in the word to be derived.
In grammars with maximal use of selectors, a context is adjoinedonly to selectors for which no superword is a selector.
Maximality can be defined either locallyor globally (with respect to all selectors in the grammar).
The obtained families of languages areincomparable with that of Chomsky context-free languages (and with other families of languagesthat contain linear languages and that are not "too large"; see Section 5) and have a series ofproperties supporting the assertion that these grammars are a possible adequate model for thesyntax of natural anguages.
They are able to straightforwardly describe all the usual restrictionsappearing in natural (and artificial) languages, which lead to the non-context-freeness of theselanguages: reduplication, crossed ependencies, and multiple agreements; however, there arecenter-embedded constructions that cannot be covered by these grammars.While these assertions concern only the weak generative capacity of contextual grammars,some ideas are also proposed for associating a structure to the generated words, in the form of atree, or of a dependence r lation (as considered in descriptive linguistics and also similar to thatin link grammars).1.
IntroductionContextual grammars  were introduced by Marcus (1969), as "intrinsic grammars,"without auxiliary symbols, based only on the fundamental  linguistic operation of in-serting words in given phrases, according to certain contextual dependencies.
Moreprecisely, contextual grammars  include contexts (pairs of words), associated with* University of Bucharest, Faculty of Mathematics, Str.
Academiei 14, 70109 Bucharest, Romania.
E-mail:solomon@imar.rot Research Group on Mathematical Linguistics and Language Engineering (GRLMC), Rovira i VirgiliUniversity, P1.
Imperial Thrraco 1, 43005 Tarragona, Spain.
E-maih cmv@astor.urv.esInstitute of Mathematics ofthe Romanian Academy, P.O.
Box 1-764, 70700 Bucharest, Romania.
E-mail:gpaun@imanro.
Research supported by the Academy of Finland, project 11281, and Spanish Secretariade Estado de Universidades  Investigaci6n, grant SAB 95-0357.
@ 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 2selectors (sets of words); a context can be adjoined to any associated word-selector.
Inthis way, starting from a finite set of words, we can generate a language.This operation of iterated selective insertion of words is related to the basic com-binatorics on words, as well as to the basic operations in rewriting systems of anytype.
Indeed, contextual grammars, in the many variants considered in the literature,were investigated mainly from a mathematical point of view; see P~iun (1982, 1985,1994), P~un, Rozenberg and Salomaa (1994), and their references.
A complete sourceof information is the monograph P~iun (1997).
A few applications of contextual gram-mars were developed in connection with action theory (P~un 1979), with the study oftheatrical works (P~iun 1976), and with computer program evolution (B~lanescu andGheorghe 1987), but up to now no attempt has been made to check the relevanceof contextual grammars in the very field where they were motivated: linguistics, thestudy of natural anguages.
A sort of a posteriori explanation is given: the variantsof contextual grammars investigated so far are not powerful enough, hence they arenot interesting enough; what they can do, a regular or a context-free grammar cando as well.
However, a recently introduced class of contextual grammars seems to bequite appealing from this point of view: the grammars with a maximal use of selectors(Martin-Vide t al.
1995).
In these grammars, a context is adjoined to a word-selectorif this selector is the largest on that place (no other word containing it as a propersubword can be a selector).
Speaking strictly from a formal language theory pointof view, the behavior of these grammars i  not spectacular: the family of generatedlanguages i incomparable with the family of context-free languages, incomparablewith many other families of contextual languages, and (strictly) included in the fam-ily of context-sensitive languages, properties rather common in the area of contextualgrammars.This type of grammar has a surprising property, however, important from a lin-guistic point of view: all of the three basic features of natural (and artificial) languagesthat lead to their non-context-freeness (reduplication, crossed ependencies, and mul-tiple agreements) can be covered by such grammars (and no other class of contextualgrammars can do the same).
Technically, the above mentioned non-context-free fea-tures lead to formal anguages of the forms {xcx I x E {a, b}*} (duplicated words of ar-bitrary length), {anbmcnd m I n, m > 1} (two crossed ependencies), and {anbnc" I n > 1}(\[at least\] three correlated positions).
All of them are non-context-free languages andall of them can be generated in a surprisingly simple way by contextual grammarswith selectors used in the maximal mode.Examples of natural anguage constructions based on reduplication were found,for instance, by Culy (1985), and Radzinski (1990), whereas crossed ependencies weredemonstrated for Swiss German by Shieber (1985); see also Partee, ter Meulen andWall (1990) or a number of contributions toSavitch et al (1987).
Multiple agreementswere identified early on in programming languages (see, for example, Floyd \[1962\]),and certain constructions having such characteristics an also be found in naturallanguages.
We shall give some arguments in Section 4.Some remarks are in order here.
Although we mainly deal with the syntax of nat-ural languages, we sometimes also mention artificial languages, mainly programminglanguages.
Without entering into details outside the scope of our paper, 1we adopt thestandpoint that natural and artificial anguages have many common features (Man-1 A word of warning: When we invoke statements concerning various topics, some of which have beendebated for a long time, we do not necessarily argue for these statements and we do not consider theadequacy ofcontextual grammars a either proved or disproved by them.
We simply mention aconnection between a linguistic fact and a feature of our grammars.246Marcus, Martin-Vide, and P~un Contextual Grammarsaster Ramer 1993).
For instance, we consider these languages infinite and organized onsuccessive l vels of grammaticality, whose number is unlimited in principle, althoughpractically only a finite number of such levels can be approached.
In Marcus (1981-83), where an effective analysis of contextual ambiguity in English, French, Romanian,and Hungarian is proposed, practical difficulties imposed a limitation to two levelsof grammaticality for English (one level excluding compound words, the other levelallowing the building of compound words) and Hungarian, but six levels for the anal-ysis of French verbs.
The reason for this situation is the "open" character of naturallanguages, making it impossible to formulate anecessary and sufficient condition for asentence to be well-formed.
As is pointed out by Hockett (1970), the set of well-formedstrings in a natural anguage should be both finite and infinite, a requirement that isimpossible to fulfill in the framework of classical set theory; for a related discussion,see Savitch (1991, 1993).
This can also be related to Chomsky's claim that a basic prob-lem in linguistics is to find a grammar able to generate all and only the well-formedstrings in a natural language.
Chomsky's claim presupposes that natural languageshave the status of formal languages, but not everyone agrees with this notion.
Evenfor programming languages, many authors reject the idea that well-formed stringsconstitute a formal language; see, for instance, the various articles in the collectivevolume Olivetti (1970), as well as Marcus (1979).Returning to constructions specific to natural anguages, we have found the sur-prising fact that the language {ancbmcbmca n I n, m > 1} cannot be generated by contex-tual grammars with a maximal global use of selectors.
Observe the center-embeddedstructure of this language and the fact that it is an "easy" linear language.
As ManasterRamer (1994, 4) points out, "the Chomsky hierarchy is in fact highly misleading .. .
.suggesting as it does, for example, that center-embedded structures (including mirror-images) are simpler (since they are context-free) than cross-serial structures (includingreduplications).
Yet we know that natural languages abound in reduplications butabhor mirror-images (Rounds, Manaster Ramer, and Friedman 1987) and it also ap-pears that, other things being equal, cross-serial structures are easier to process thancenter-embedded ones.
"This point brings to mind Chomsky's arguments (1964, 120-25) that center-embedded constructions can be handled by the grammar (the description of com-petence), but not by the performance system.
Here competence itself is not able tocover the center-embedded construction.
However, we have to mention the fact thatother similar constructions can be covered by contextual grammars (with or withoutmaximal use of selectors).
This is the case with {wc mi(w) I w c {a, b}*}, where mi(w)is the mirror image of w. Also the language {w mi(w) I w E {a,b}*} can be gener-ated when the maximal use of selectors is considered, but not without involving thisfeature.The difference between these last two languages suggests another point supportingthe adequacy of contextual grammars: from the Chomsky hierarchy point of view,there is no difference between these languages; rather, their grammars are similar.This is not the case in the contextual grammar framework, and this also correspondsto our intuition: having a marker (the central c here) is helpful, it is significantly easierto process a language when certain positions of its sentences are specified.
(Furtherillustrations of this point can be found in Section 4.)
We conclude that contextualgrammars with a maximal use of selectors eem adequate from these points of viewfor modeling natural anguages.
22 We do not claim and we do not intend to prove (because we cannot) hat a contextual grammar with247Computational Linguistics Volume 24, Number 2In the architecture and the functioning of a contextual grammar one can notetwo contradictory basic ingredients.
On the one hand, because we use adjoining, notrewrit ing (moreover, we do not use nonterminal symbols), the strings are always in-creased.
At every step, we preserve all previously introduced symbols and we addnew ones.
This looks quite limiting for the power of these grammars.
On the otherhand, in contextual grammars there is a clear context-sensing capability, the contextsare adjoined to their selectors and depend on them.
Context-sensitivity is in generala powerful property.
Context-sensitivity plus erasing produces everything.
In manycases in formal language theory, this combination leads to characterizations of recur-sively enumerable languages.
Such a result has been proved by Ehrenfeucht, P~un,and Rozenberg (1997) for contextual grammars with unrestricted use of selectors.
Inthe last section of this paper, we prove that this is also true for the case of maximaluse of selectors.
Specifically, we prove that every recursively enumerable language,L, can be written in the form L ~- hl(h~-l(L')), where hi,h2 are morphisms and L' is alanguage generated by a contextual grammar with maximal use of selectors.
The proofuses the same construction as in Ehrenfeucht, P~un, and Rozenberg (1997), adaptedto our class of grammars.
The effect of hi, h~ -I can also be achieved by a sequentialtransducer (with finite memory), hence we may state the theorem in the form: everyrecursively enumerable language is a sequential translation of a contextual language(generated with maximal use of selectors).
As a consequence, we find that our gram-mars can generate languages outside any family of languages that is strictly includedin the family of recursively enumerable languages and is closed under direct and in-verse morphisms or under finite sequential transducers.
Important families in formallanguage theory have these properties: the family of context-free languages, severalfamilies in the regulated rewriting area (see Dassow and P~un \[1989\]), including in-dexed languages and programmed languages.
Together with the fact that the language{ a"cbmcbmcan I n, m > 1} mentioned above is linear, we get the incomparability of ourfamilies with many families in the Chomsky hierarchy or in its refinements.This relates to another statement of Manaster Ramer's (1994, 4): "The question asposed by Chomsky \[about the place of natural anguages in a hierarchy of generativedevices\] seems to suggest hat the class of natural anguages will be found somewherein the Chomsky hierarchy.
Yet this need not be the case, and probably is not.
It is en-tirely possible, for example, that a realistic theory of natural anguages would definea class of languages which is incommensurate with the Chomsky types, e.g., a fewregular languages, a few non-regular context-free languages, a few non-context-freecontext-sensitive languages, and so on.
Indeed, it has been pointed out .. .
that, if finitelanguages are to be excluded from linguistic theory as Chomsky himself has alwayscontended, then the class of natural languages will necessarily be a non-Chomskyclass, since all the Chomsky classes do contain finite languages."
Maybe contextualgrammars (with maximal use of selectors) are one example of such a realistic possi-bility.The discussion above has concerned the weak generative capacity of contextualmaximal use of selectors i the best model for natural language syntax, that these grammars candescribe all types of constructions in natural languages or in other languages, orthat, for instance, wecan describe in a satisfactory manner the syntax of English.
Maybe even other classes of contextualgrammars have to be imagined, which will be better than the existing ones.
Further efforts hould bemade to clarify the relevance ofcontextual grammars of various types for the study of naturallanguages.
For instance, we can report no practical experience in writing a contextual grammar for afragment of a natural language.
In short, our goal is to acquaint the reader with contextual grammarsand to convince him or her that these grammars deserve further investigation---of a mathematical and,more importantly, ofa linguistic type.248Marcus, Martin-Vide, and P~un Contextual Grammarsgrammars (with maximal use of selectors).
Recently (see Martin-Vide and P~un \[1998\]),some attempts were made to introduce a structure into the strings generated by con-textual grammars.
An easy way to do so is to associate a tree to a derivation (just adda pair of parentheses toeach context, then build a tree in the usual way: when readinga left parenthesis add a new edge, when reading a right parenthesis go back along thecurrent edge, etc.)
or a graph describing a dependence r lation similar to those dis-cussed in descriptive linguistics (see Chapter VI of Marcus \[1967\]) or in link grammars(Sleator and Temperley 1991; Grinberg, Lafferty, and Sleator 1995).
We briefly presentthese possibilities here, although the linguistic relevance of the obtained structures istill being researched.Let us also mention that, by definition, contextual grammars are (fully) lexicalized(in accordance with many current trends in formal syntax) and that their languageshave the bounded growth property.In view of all these results and properties, we believe that contextual grammarsare an attractive model for natural anguage syntax, completing (but not necessarilycompeting with) the existing models, and that they deserve further investigation.2.
DefinitionsIn this section, we introduce the classes of grammars we shall investigate in this paper.As usual, given an alphabet V (which we also call vocabulary), we denote by V* theset of all words (equivalently: strings) over V, including the empty one, which isdenoted by A.
The set of all nonempty words over V, hence V* - {A}, is denotedby V +.
The length of x c V* is denoted by Ix\[ and its mirror image (also called thereversal) by mi(x).
The families of finite, regular, linear, context-free, context-sensitive,and recursively enumerable languages are denoted by FIN, REG, LIN, CF, CS, RE,respectively.
For the elements of formal language theory we use, we refer to Harrison(1978), Rozenberg and Salomaa (1997), and Salomaa (1973).
3A contextual grammar (with choice) is a construct:G = (V ,A , (S1 ,  C1) , .
.
.
, (Sn ,  Cn)), n > 1,where V is an alphabet, A is a finite language over V, $1 .
.
.
.
.
Sn are languages over V,and C1 .
.
.
.
.
Cn are finite subsets of V* x V*.The elements of A are called axioms (starting words), the sets Si are called selectors,and the elements of sets Ci, written in the form (u, v), are called contexts.
The pairs(Si, Ci) are also called productions.
The intuition behind this construction is that thecontexts in Ci may be adjoined to words-in the associated set Si.
Formally, we definethe direct derivation relation on V* as follows:X ::=:=-kin yiff x = XlX2X3, y =Ill, lX2VX3, where x2 E Si,(u,v) E Ci, for some i, 1 < i < n.Denoting by ~Tn the reflexive and transitive closure of the relation ==->'in, thelanguage generated by G is:Lin(G) -~ {z E V* I w ===>'i~ z, for some w E A}.3 As general mathematical notations, we use: C (inclusion, not necessarily proper), C (proper inclusion),C ("is an element of"), 0 (the empty set), 2 x (the family of all subsets of the set X).249Computational Linguistics Volume 24, Number 2Consequently, Lin (G) contains all words of A, as well as all words that can be obtainedfrom them by adjoining finitely many contexts, according to the selection imposed bythe pairing (Si, Ci).Remark 1The previous definition of a contextual grammar is called modular.
Sometimes, it isuseful to present a contextual grammar in the so-called functional form, that is, as aconstruct G = (V ,A ,C ,~) ,  where V and A are as above, C is a finite set of contextsover V, and ~: V* ~ 2 c associates sets of contexts from C to strings in V*.
Then wewrite x ~ in  y iff x = xlx2x3, y = XlUX2VX3, for some (u,v) E ~(x2), Xl,X2, X3 E V*.It is easy to see that starting from a contextual grammar in the modular pre-sentation, G = (V,A, ($1, C1), .
.
.
,  (Sn, C,)), we can consider its functional counterpartG' = (V ,A ,C ,~) ,  with:nC = U ci,i=1~o(x) = {(u,v) I (u,v) ~ Ci, x ~ Si,1 < i < n}, x E V*.Conversely, from a grammar given as G = (V,A, C, ~) with:C --~ {(U l ,V l  ) .
.
.
.
.
(Un, Vn)},we can pass, for instance, to G' = (V,A, ($1, C1), .
.
.
,  (S,,C,)),  taking, for each i, 1 <i<n:ci  = {(ui, vi)},and Si the set of strings in V* to which the context (ui, vi) can be adjoined, that is:si = {x E V* I (u~,vi) E ~(x)}.The two grammars G and G' are clearly equivalent in both cases.Thus, in the proofs below we shall use that presentation of a contextual grammarwhich is more appropriate (economical) for that case.Remark 2The derivation relation defined above has been denoted by ~ in  in order to distin-guish it from the external derivation defined for G, where the context is adjoined atthe ends of the derived word: x ==~'ex y iff y = uxv for (u, v) E Ci, x E Si, for somei, 1 < i < n. In Marcus (1969), only the external derivation is considered, for gram-mars presented in the functional form, without restrictions on the selection mapping.Contextual grammars with internal derivation were introduced in P~un and Nguyen(1980).We do not investigate the external derivation here.Two natural variants of the relation ~ in  defined above were considered byMart/n-Vide et al (1995):x ::=-~Mt Yiff x = XlX2X3, y = XlUX2VX3, for X2 E Si, (u,v) E Ci, for some 1 < i < n, and there are!
!
!
no  X~, x2,' X 3!
E V* such that x = xlx2x3, x~ E Si, and Ix~l _< IXll, Ix~I <_ I/B1, Ix~l > Ix2\];x ~MR y250Marcus, Martin-Vide, and P~un Contextual Grammarsiff x = XlX2X3, y = XlUX2VX3, for x2 E Si, (u,v) E Ci, for some 1 < i < n, and thereI !
/ V* !
!
!
!
are no x 1, x 2, x 3 E such that x = XlX2X3, x 2 E Sj, for some 1 < j _< n, and I/~1 _<J i l l ,  IX3J ~ Ix3l, IX2I > IX2I.We say that ~Ml  is a derivation in the maximal local mode (the word-selector x2is maximal in Si) and ~Mx is a derivation in the maximal global mode (the word-selector x2 is maximal with respect o all selectors $1 .
.
.
.
.
Sn).For ol E {MI, Mg}, we denote:L~(G) = {z E V* I w ~ z, for some w E A}.?
If in a grammar G = (V,A,(S1,C1),...,(SH, Cn)), all selectors $1 .
.
.
.
.
Sn are lan-guages in a given family F, then we say that G is a contextual grammar with F choice(or with F selection).
The families of languages L~(G), for G a contextual grammarwith F choice, are denoted by CL~(F), where oL E {in, MI, Mg}.
Here we consider F oneof the families FIN, REG only.
(It is natural to deal with selectors that are as simpleas possible, otherwise the grammar is no longer of "practical" interest.
Still, for thecase of regular selectors we have here a sort of two-level grammar, because in order tocompletely describe a contextual grammar, we also need a grammatical description forthe selector languages.
However, using a selector Si means deciding the membershipof a substring of the current string with respect o Si; when Si is a regular language,this question can be solved in real time, using the simplest ype of recognizers: a de-terministic finite automaton.
Derivations where the selectors are used in the minimalmode (no subword of a word-selector can be a selector) are introduced by Martin-Videet al (1995); we do not discuss this variant here.3.
Generative CapacityFirst, we recall some results from previous papers devoted to contextual grammars ofthe basic type or with maximal use of selectors, then we prove new results about thepower of the latter classes of grammars.The relations between families of contextual languages, defined above, and be-tween these families and families in the Chomsky hierarchy, pictured in the diagramin Figure 1, were proved by Mart/n-Vide et al (1995).
An arrow from a family F1 toa family F2 indicates the strict inclusion F1 C F2; the dotted arrow indicates an inclu-sion not known to be proper.
Families not related by a path in this diagram are notnecessarily incomparable.
The families CLMg(REG) and CF are incomparable with allfamilies CL~(F), o~ E {in, Ml},F E {FIN, REG}; CF is incomparable with CLMx(REG),too.Here are three languages used by Martin-Vide et al (1995) in order to prove someof these strict inclusions and incomparabilities (we will need these languages later):L1 = {anbmanb m \] n,m _> 1} E CLc~(REG) - CLfl(FIN), o~,fl E {in, MI, Mg},L 2 = {anb\] n > 1} U {anb" I n > 1} E CLMg(FIN) - CL~(REG), a E {in, Ml},L3 = {x mi(x) Ix E (a,b}*} E CL~(REG) - CLin(REG), c~ E {MI, Mg}.Note that languages L2 and L 3 a re  linear, but L1 is not context-free.
In P~un (1985), itis proved that CLin (FIN) - CF ~ O.Here is a grammar generating the language L2 in the Mg mode:G= ({a,b},{ab, a2b2},({ab},{(a,&)}),({a2b2in ~ 1},{(a,b)})).251Computational Linguistics Volume 24, Number 2CSCLin (REG) CLMI(REG)TCLMi(FIN)CLin (FIN)CLMg(REG)CF CLMg(FIN)REGFINFigure 1Relations between families of contextual languages and families in the Chomsky hierarchy.Indeed, for any word a"b with n > 1 only the first context can be used, and for anyword anb n with n > 2 only the second context can be used (here, the use of selectorsin the maximal global mode is essential, in order to prevent he adjoining of the firstcontext o words of the form anb n, n > 2, in such a way as to destroy the equality of thenumber of a and of b occurrences).
However, L2 ~ CLMi(REG) U CLin(REG).
Assumethe contrary and take G' = ({a, b},A, ($1, C1), .
.
.
,  (Sn, Cn)) such that L,~(G') = L2, ol E{in, Ml}.In order to generate all strings anb, n > 1, we need a context (ai, aJ), with i+ j  > 1,either associated with aSb for s > 0 (then j = 0), or with a k, k > O.
For oz = in, thecontradiction is clear: strings a"'b n with n ~ > n can be produced in both cases.Assume that G' is used in the maximal ocal mode.
In order to generate the stringsanb ", n > 1, we also need a context (a k, bk), k > 1.
This context cannot be applied to astring to which (ai, a j) above can be applied (from arab we get am+kbk+l, which is notin L2).
Therefore (a k, b k) and (a i, aJ) can be used independently (these contexts belongto sets Cs and Ct in G', respectively, with 1 < s, t < n, s # t).
This implies that (a t, a 0can be applied to a string aqbq with large enough q, again producing strings that arenot in L2.The relationships between the family CLMx(FIN) and other families CL~(F), oz E{in, Ml},F E {FIN, REG}, as well as between CLMg(FIN) and CF, are not settled byMartin-Vide et al (1995).
We solve most of these problems here.We start with two results having a linguistic relevance.
The first one points outa surprising limitation of contextual grammars with global maximal use of selectors:there are center-embedded structures that cannot be generated by such grammars even252Marcus, Martfn-Vide, and Paiun Contextual Grammarswhen regular selectors are used.
Specifically, let us consider the language:L4 = { an cbm cbm ca" In, m > 1}.Note that this is a linear language in Chomsky's sense and that it belongs to thefamilies CLin(FIN) and CLMr(FIN).
For the grammar:G= ({a,b,c},{acbcbca},({bcb},{(b,b)}),({acbcbca},{(a,a)})),we have Lin(G) = L; the context (a,a) cannot be used after using the context (b, b),hence the grammar G can first generate any word of the form ancbcbca n, n > 1, thenany word ancbmcbmca n, n, m > 1; each selector consists of one word only, hence the localmaximal use of selectors imposes no restriction, Lin (G) = LMt(G).In contrast o these observations, we have the following result:Theorem 1The language L4 is not in the family CLMg(REG).ProofAssume that L4 = LMg(G) for some grammar G = ({a,b,c}, A, (S1,C~),..., (Sk, Ck)).
Inorder to generate strings ancbmcbmca n with arbitrarily large n and m we need:contexts (a i, a i) associated with selectors of the form aPcbrcbrca q, for somep,q>O,r> l,contexts (bJ, bJ) associated with selectors of the form bScb t, for somes, t > 1 (if one of s, t is zero, then we can introduce occurrences of b infront of the first occurrence of c or after the third occurrence of c instrings ancbmcbmca n with large enough m).If in a derivation we use an a-context, hen no b-context can be used at a subsequentstep: either the a-context is still applicable or an a-context with a larger selector isapplicable, while the central subword cbmcbmc has not been changed; the b-contextsuse proper subwords of cbmcbmc, hence they are not allowed in the Mg mode.Therefore, the derivations in G start by a phase:w ==;~'~,Ig anlcbmcbmcanl,where only b-contexts are used, then (possibly) continue by a phase:anlcbmcbmca nl =_=~g ancbmcbmca n,where only a-contexts are used (and the subword cbmcbmc is not modified).For a given n > 1, denote:M(n) = {x E Lug(G) I w ==~g x by using only b-contexts,w c A ,w = ancbmcbmca n, for some m > 1}.Let:no = max{n I M(n) is infinite}.253Computational Linguistics Volume 24, Number 2All strings in M(no) are of the form an?cbmcbmca "o, m _> 1.
Denote:M'(n0) = {w E LMx(G) I w ==-~Mg x by using a b-context, x c M(n0)}.Because M(n0) is infinite and we use a finite set of contexts, the set Mr(no) is alsoinfinite.
Because ach w E M(no) is derived using a b-context, it follows that no a-context can be applied to w, otherwise the derivation is not done in the Mg mode.However, for each m such that an?cbmcbmca"?
E M'(n0), all strings z = ancbmcbmca ", n >1, are in L4.
Let us denote their set with M"(n0).
In order to generate such stringswith arbitrarily large n, we have to use a-contexts.
Because such a context (ai, a i) E~(aPcbmcb'nca q) cannot be applied to a"?cbmcbmca "o, it follows that at least one of therelations p > no, q > no holds.
We have seen that a-contexts can be used only afterb-contexts.
Therefore, the strings in M"(n0) must be generated starting from axiomsan'cbmlcbm'ca "1 with nl > no.
By the choice of no, such axioms are able to generate onlyfinitely many strings of the form anlcbmcbmca n~.The set M'(n0) is infinite, the set ofaxioms is finite, hence M'(n0) cannot be covered by strings generated in this way, acontradiction.
The equality L4 ~- LMx(G) is not possible, L4 ~ CLMg(REG).Note that the type of selectors plays no role in the previous argument, henceL4 ~ CLMg(F), for any family F of languages.The fact that L4 E CL~(FIN) - CLMx(REG), for o~ c {in, Ml}, should be contrastedwith the fact that L3 E CL~(REG) - CLin(REG), for o~ E {MI, Mg}: there are center-embedded constructions that cannot be handled by grammars with global maximal useof selectors, but the "total mirror language" can be generated when using a maximalrestriction and not in the free case.On the other hand, the family CLMg(FIN) goes surprisingly far in the Chomskyhierarchy.
The result will be stressed, indirectly, in Section 6, but we prefer to alsogive an example of a language that belongs to the family CLMg(FIN) and looks quitecomplex.
Together with the previous theorem, this example settles the relationshipsbetween the family CF and families CL~ (F), o~ E {Mg, MI}, F c {FIN, REG}.Theorem 2The family CLMx(FIN) contains non-context-free languages.ProofConsider the grammar:G=({a,b},{aab},({b},{(a,~),(b,a)}),({abb},{(bb, a) ),({ ababb},{(&,&)})).Let us examine the intersection of the language LMg(G) with the regular language:R = (bba)+a +.The family CF is closed under intersection with regular languages; if LMg(G) E CF,then Lag(G) N R E CF.
However, this does not hold, because, as we shall prove below,we obtain:LMx(G) M R = {(bba)2"a n I n > 2}.This is not a context-free language.Indeed, examine the derivations in G, with a global maximal use of selectors,starting from aab and leading to words of the form (bba)"a m, n, m > 1 (such words areelements of R).254Marcus, Martin-Vide, and Pgun Contextual GrammarsAs an illustration of the arguments that follow, let us consider a short example:take n = 3.
We have to proceed as follows:aaab_ba ==-~ MR~Mg~Mg~Mgaabbabbaa~Mg ab_babbaabbaabbabbaabbaabbaa==~Mg bbabbabbabbaaabb___aabbabbabbabbaabbabbaaa--~Ms bbabbabbabbabbabbaabbaaabbabbabbabbabbabbabbabbaaaa=(bba)23a 3.
(The selector used at each step is underlined.
)Let us now examine a derivation of a general form.
The first context of the firstproduction, (a, A), can be adjoined to occurrences of the symbol b only when theseoccurrences do not have a symbol a to their right-hand side; in such a case, the Selectorba is present, which is larger than b, thus preventing the use of the first production.Thus, from aab we can produce any word of the form anb, n > 2.To such a word we can adjoin the context (b,a), thus obtaining a"bba.
From nowon, the selector b can never be used: the symbols b in pairs of b occurrences will not beseparated, and there can never be four adjacent occurrences of b.
(This could happenonly when two symbols b are already present and two further ones are introducedby the context (bb, a); this means that we would have started from a word xbbabbx',but with such a word we are not allowed to use the selector abb, because the longerselector babb is present.)
Therefore, the right occurrence of b in each pair bb is followedby an occurrence of a, and thus the use of the selector b is forbidden by the selectorba in the last production, whereas for the left b in a pair bb we cannot use the selectorb, because the selector abb is present.
Thus, from a word of the form anbba we haveto obtain a word in R using only the second production of the grammar.
This meansthat every occurrence of a will go to the right, using this production.
Crossing a pairbb, each occurrence of a introduces one more pair bb, as well as one more a. Hence,each use of the production doubles the number of occurrences of the pair bb.
Sincewe eventually get a word starting with bb, this means that one pair bb has crossedall occurrences of a; at every step, one further a is introduced.
One copy of a remainsin triples bba, the other must migrate to the suffix of the word.
Consequently, theobtained string is of the form (bba)ma p, where m is the number of times of using theproduction ({abb}, ((bb, a)}) minus one, and p is the number of initial occurrences ofa, that is p = n. This implies that the occurrence of a immediately to the left-hand sideof the initial pair bb has crossed one pair bb (doubling it), the next one has crossedtwo pairs bb (doubling them), and so on until the leftmost occurrence of a, the n-thone.
In total, we have n doublings, because we started from anbba.
This means that mabove is equal to 2 n, that is the obtained word is of the form (bba)2"a n.This completesthe proof.Corollary 1The families CF, CLMs (FIN) are incomparable.ProofTheorem 1 shows that CF - CLMg(FIN) ~ O, whereas from Theorem 2 we know thatCLMx(FIN) - CF # O.Returning to the diagram in Figure 1, we now know that any two families notlinked by a path in this diagram are incomparable, except the pairs (CLi,(REG),CLMt(FIN)), (CLin(REG), CLMI(REG)), and (REG, CLMg(FIN)), (REG, CLMs(REG)).255Computational Linguistics Volume 24, Number 2For the convenience of the reader, we list all pairs (F1, F2) of families of contextuallanguages from this diagram that are not known in be included in each other, specify-ing in each case the known incomparability arguments in the form L, (F1, F2), L t, withthe following meaning: L E F1 - F2, L' E F2 - F1.
When L or L' is not specified, it meansthat no such language is known.
The languages L1, L2, L3, L4 used are those mentionedabove:L4, (CLin(FIN),CLMR(FIN)), L2L4, (CLin(FIN),CLMR(REG)), L2L1, (CLin(REG), CLMj(FIN))(CLin (REG), CLMI (REG)), L3L4, (CLi,(REG),CLMR(FIN)), L2L4, (CLin(REG),CLMg(REG)), L2L4, (CLMi(FIN),CLM~(FIN)), L2L4, (CLMt(FIN),CLMx(REG)), L2L4, (CLMi(REG),CLMg(FIN)), L2L4, (CLMt(REG),CLMR(REG)), L2.Both families CLi,(FIN) and CLMg(FIN) contain non-context-free languages, but thereare linear languages not in CLin(REG), CLMi(REG) (for example: L2) or in CLMg(REG)(for example: L4).
We conjecture that REG C_ CLMg(FIN), however, the constructionfrom the proof of the inclusion REG c CLin (FIN) from Ehrenfeucht, P~un, and Rozen-berg (1997) cannot be directly modified for the Mg case.4.
On the Linguistic Relevance of Contextual Grammars with Maximal Use ofSelectorsWith regard to their linguistic foundations, contextual grammars are closely relatedto American distributional linguistics, the potential of which they try to exploit.
Letus quote some words of Manaster Ramer (1994, 4): "It is my contention that, untilthe early 1960's, the situation, as revealed by a close mathematical nalysis of theunderlying issues, was this: (a) there was no basis for concluding that 'in principle'natural anguages were anything but context-sensitive (and it should have been clearthat nothing was likely to change that result), (b) it was clear that phrase structure wasinadequate in terms of its descriptive devices, and (c) it should have been clear (sinceit had been admitted in print) that phrase structure left out some of the descriptivedevices of immediate constituent analysis.
The right thing to have done would havebeen to pursue a more accurate formalization of immediate constituent analysis, anda more detailed analysis of just how much context-sensitivity was really required fornatural anguages.
"The generative process in a contextual grammar is based on two dual linguisticoperations, which are among the most important in both natural and artificial lan-guages: insertion of a string in a given context and adding a context o a given string.Descriptive distributional linguistics developed in the U.S.A. in the 1940s and 1950sis entirely based on these ideas.
To some extent, a similar idea is behind some as-pects of Chomsky grammars; for instance, the difference between a context-free anda context-sensitive rule is that a certain substitution, generally valid in a context-freegrammar, becomes possible only in a given context as soon as the grammar is nolonger context-free, but context-sensitive.Any derivation in a contextual grammar is a finite sequence of such operations,starting from an initial finite stock of strings, simple enough to be considered primitivewell-formed strings (axioms).Given a language L over the alphabet V, each context (u, v) over V selects a set of256Marcus, Martin-Vide, and PSun Contextual Grammarsstrings x such that uxv c L. We say in this case that x is accepted by (u, v) in L or that(u, v) accepts x in L. Any set C of contexts over V selects the set X of those strings thatare accepted in L by any context in C. Obviously, X is here maximal, because it is theset of all strings with the relevant property.
The dual phenomenon is the following:each string x over V selects the set C(x) of those contexts that accept x in L. To anyset E of strings over V we associate the set of contexts accepting in L any string in E.In short, given a language L, each set of contexts (strings) selects, with respect o L,a set of strings (contexts); in other words, each language over V determines a preciseinterplay of strings and contexts over V.A natural question can be raised: could we now follow an inverse itinerary, bystarting from a finite stock A of strings (over V) simple enough to be consideredprimitive well-formed strings (axioms), and by considering a finite set of couples(Si, Ci), 1 < i < n, where Si is a set of strings, while Ci is a finite set of contexts, to askwhat is (are) the language(s) with respect o which Ci selects Si, 1 < i < n?
The ideaof a contextual grammar, in its various forms, is born from the attempt to answer thisquestion.
A series of details about this topic can be found in Marcus (1997).Let us consider again the three non-context-free constructions in natural anguagesmentioned in the introduction.
The (non-)context-freeness of natural and programminglanguages has been investigated since the early sixties (Bar-Hillel and Shamir \[1964\];Floyd \[1962\], among others).
While for Algol 60 and for all advanced programminglanguages, the question has been settled from the very beginning--these languagesare not context-free--a long debate was necessary concerning natural anguages.
Weshall use information about this question from Gazdar and Pullum (1985); the readermight also consult Pullum (1985, 1986, 1987) and Pullum and Gazdar (1982).The general technique in approaching this problem is the same for both program-ming and natural languages.
Look for special constructions that seem, intuitively,to require a non-context-free competence.
In order to extract hem from the studiedlanguage, use an intersection with a regular language.
Because CF is closed underintersection with regular sets, if the result is not context-free, then we have a proofthat the initial language is not context-free.The basic constructions of this type are duplication of arbitrarily long subwords,dependencies (agreements) between crossed pairs of subwords, and dependencies act-ing on (at least) three correlated subwords.
The basic features of programming lan-guages requiring dependencies are the necessity of declaring identifiers and names ofprocedures, and of defining labels.In natural anguages, such replications and dependencies can appear either at thelevel of the vocabulary or at the level of the sentences in a given language.
The questionis not simple, because it might not be clear what is grammatical and what is notgrammatical with respect o a natural anguage.
However, there are now convincingexamples of non-context-free constructions in many languages.
At the level of thevocabulary, the case of Bambara, a language from the Mande family in Africa (Culy1985) is illustrative: compound words of the form string-of-words-o-string-of-words arepossible in this language.
The corresponding formal language consists of words of theform xcx, for x an arbitrarily long word over an alphabet not containing the symbol c(this symbol corresponds to the separator o in the Bambara construction).
Because wecan always codify words using two symbols, we work here with the language:M1 = {xcx l x E {a,b}*}.Another non-context-free construction has been found in a dialect of German spo-ken around Zurich, Switzerland (Shieber 1985; Pullum 1985), which allows construc-257Computational Linguistics Volume 24, Number 2tions of the form NP~ NP~ V~ n' V~', where NPa are accusative noun phrases, NPd aredative noun phrases, Va are accusative-demanding verbs, Vd are dative-demandingverbs, and the numbers match up, that is m = m', n = nq This leads to languages ofthe form:M2 = {ambncmd n \] n,m ~_ 1}:Both of these constructions can be easily found in programming languages, too.The proof of Floyd (1962) that Algol 60 is not context-free l ads to a language of M1type.
Intersecting any Algol-like language with a regular language consisting of stringsof the form:begin; real x; .
.
.
,  go to label1; .
.
.
.
y := 1; ... label2 : ... ; endwe force the equalities x = y, label1 = label2, hence a language like M2 is obtained.If, however, we intersect an Algol-like language with the regular set of strings ofthe form:begin; real x; y := z; endthen we force the equalities x = y = z, which can be translated into a language of theform:M3 = {a"b"c n \] n > 1}.Concerning a natural anguage version of this form, Manaster Ramer (1993, 12) says:"The interaction of two different constructions (coordination and serial-verb forma-tion) gives rise to patterns essentially of the form anbnc" (and, more generally, (a"b) m)in Dutch and German, but there is no indication that any one construction i  anylanguage has this property."
Also according to Manaster Ramer (1994, 21), "Columnarstructures like anbnc n, anb"cnd ", etc.
(for all positive n) seem not to exist by themselvesas constructions but do appear as compositions of two constructions (in particular, theserial verb construction of German or the cross-serial construction of Dutch togetherwith coordination of the verb clusters) ... in these terms, natural anguages possessan important property different from the usual formal anguages.
Namely, in naturallanguages individual constructions often have forms which no natural language, takenas a whole, can have.
Thus, reduplication is common (probably universal), but thereis no natural language which is made up, in its entirety, of reduplications.
"Counterparts of these much-used examples of non-context-free languages can beidentified in other areas, such as the semiotics of folklore (Marcus 1978).None of the languages M1, M2, M3 is context-free, and this is an easy exercise inany formal language textbook.
Moreover, M1 and M3 belong to no family CLin(F),for arbitrary F (even more general than FIN and REG).
The argument is similar in allcases: in the free mode of using selectors, one cannot sense the place where the contextmust be added without producing a parasitic word.
Take, for instance, the case of M3.If, in order to introduce arbitrarily many occurrences of a, we use a context (a i, bici),i > 1, associated with words of the form aJbk, j,k ~_ O, then aJ+k+lbJ+k+lcJ+k+l ==:~inak+laiaJbkbicibJ+lcJ+k+l is a correct derivation, but the word produced is not in M3.A similar parasitic word is obtained if we use a context of the form (aib i, ci), i > 1,associated with bJck, j,k ~_ 1, and for contexts of the form (aibJ, bkcl),i = j + k = l > 1,associated with words bP, p > O.
At least one such context is necessary, hence nogrammar can generate M3 in the free mode without producing parasitic strings.However, all three languages mentioned above can be generated by using the selectors in258Marcus, MartLn-Vide, and PSun Contextual GrammarsTable 1Languages generated by various contextual grammars.CLi.
(FIN) CLin(REG) CLMi(FIN) CLMt(REG ) CLMg(FIN) CLMg(REG)M1 No No No Yes No YesM~ No No No No No ?M2 No Yes No ~s  No ~sM3 No No No ~s  No YesM4 ~s  ~S ~S ~S ~S ~SM~ No No No ~s  No Yesthe maximal mode, both in the local and the global way.
Here are grammars proving thisassertion:G1 = ({a, b, c}, {c}, ({c}{a, b}*, {(a, a), (b, b)})),G2 = ( {a, b, c, d}, {abcd}, (ab+c, { (a, c) }), (bc+d, { (b, d) })),G3 = ({a, b, c}, {abc}, (b +, {(a, bc)})).The reader can easily check that LMi(Gi) = LMg(Gi) = Mi, i = 1,2,3.
Notice howsimple these grammars are, even compared with regulated context-free grammars(Dassow and P~iun 1989), which, in some sense, are specially designed for handlingsuch languages.What is significant here is that all of these languages, hence all of the subjacentsyntactic restrictions, can be handled by contextual grammars with both a local anda global maximal use of selectors, although--as we have seen--the overall generativepower of such grammars is not "too large": there are context-free languages (evenlinear ones: remember the language in Theorem 1) that they cannot generate.
On theother hand, the power of these grammars is not "too small."
Theorem 2 from Section 3and Theorems 3, and 4 from Section 6 explain the meaning of this statement.At the beginning of Section 3, we mentioned that M2 E CLin (REG).
This also fol-lows from grammar G2, for which we have LiR(G2) = LMg(G2) = LMi(G2): the twoselectors are disjoint and their elements are "marked strings," bounded by fixed sym-bols, hence no selector string is the subword of another selector string.
The maximalityfeature is, however, essential for G1 and G3, because, as we have mentioned before,the languages M1 and M3 cannot be generated by contextual grammars working inthe in mode.Consider now the "unmarked" variant of the language M1 above, that is:M~ = {xx Ix E {a,b}*},as well as the marked and unmarked mirror image languages:M4 = {XC mi(x) IX E {a,b}*},M~ = {x mi(x) l x E {a,b}*}.For reference, we indicate the possibility of generating these languages by contextualgrammars of various types in Table 1.Proofs of the assertions represented in Table 1 can be found in Martin-Vide et al(1995), some of them were mentioned above, or can be easily found by the reader.
For259Computational Linguistics Volume 24, Number 2the sake of the completeness, some hints for the proofs not discussed here are givenin the appendix.It is worth emphasizing the clear difference between marked and unmarked lan-guages: the former are easier to handle than the latter.
There is also a clear differencebetween contextual grammars and Chomsky grammars, with respect o the languageslisted above.
For instance, M4 and M~ are of the same complexity when they are gener-ated by Chomsky grammars (both of them are linear and can be generated by almostidentical grammars); in the framework of contextual grammars, M4 and M~ are sig-nificantly different.
This also holds for M1 and M~.
The case of contextual grammarsis closer to our intuition, because the existence of a marker makes it very easy tocheck the property defining the strings in our languages (knowing the "center," wecan directly check the relation between the two halves of the strings).It is known that the language M3 mentioned above cannot be generated by a treeadjoining grammar (TAG) in the pure form introduced by Joshi, Levy, and Takahashi(1975), but CF c TAL, where TAL is the family of languages generated by TAGswithout additional features (see also Section 21.2 in Partee, ter Meulen, and Wall\[1990\]).
In view of the languages L2 and L3 in Section 3, which are context-free but notin CLMg(REG), or CLMt(REG), respectively, it follows that TAL is incomparable witheach of the families CLMt(REG), and CLMg(REG).
However, TAGs with constraints (forinstance, with null adjoining contraints; see, for example, Joshi \[1987\] and referencestherein) can generate all languages M1, M2, and M3; hence, a proper superfamily ofTAL is obtained.
The relationships between such enlarged TAL families and familiesof contextual languages are not settled yet.An important question in this framework is whether or not the languages in thefamilies CLo,(REG), a E {MI, Mg}, are mildly context-sensitive.
It is obvious that, bydefinition, contextual languages have the bounded growth property: the set of contextsis finite, passing from one string to another means adjoining of a context from a finiteset, and all generated strings belong to the language.
However, we do not knowwhether or not the languages in families CL,~(REG), ~ E {MI, Mg} are parsable inpolynomial time.In general, the parsing of languages generated by contextual grammars (of anytype, not only with maximal use of selectors) is a research area still open.
There areseveral attempts to define contextual automata (see, for example, P~un \[1982\], Jan~ar etal.
\[1996\], and Miquel-Verg6s \[1997\]).
Some of them characterize a number of familiesof contextual languages, and some of them recognize families that do not correspond toclasses of contextual grammars.
However, no systematic study of parsing complexityhas been done, even for basic classes of contextual grammars.
(Of course, becausein contextual grammars we do not have erasing operations but only adjoining, wealways generate context-sensitive languages, hence membership is decidable.
)The only complexity results known at the moment concern external contextualgrammars with regular (even context-free) selectors, and a variant of internal contex-tual grammars with regular selectors used in a "localized" manner: the selector usedat any derivation step should "touch" the context used at the previous tep.
Ilie (1997a,1997b) proved that the parsing of the languages generated by such grammars can bedone in polynomial time.Let us close this section with the observation that contextual grammars haveanother property much discussed recently: they are lexicalized (we might say "fullylexicalized'), as each of their productions (pair selector-context) consists of terminalsymbols only.260Marcus, Martfn-Vide, and P~un Contextual Grammars5.
Attempts to Associate a Structure to Contextual LanguagesIn this section, we investigate further the adequacy of contextual grammars for de-scribing the syntax of natural anguages.One of the features of context-free grammars and of other grammars based oncontext-free core rules (TAGs included) most useful for linguistics is the fact that aderivation can be described by a tree defining a structure of the generated sentence.On this basis, the difference between weak generative capacity and strong generativecapacity was introduced: the former refers to the set of sentences that a grammarproduces, while the latter refers to the set of pairs composed by a sentence and itsphrase-structure tr e.Only very recently (see Martin-Vide and P~un \[1998\]) some possibilities for intro-ducing a structure to the words generated by contextual grammars were considered.We present here some ideas from Martin-Vide and P~un (1998), without entering intodetails; research is still in progress.
We only want to show that various natural solu-tions exist for structuring contextual languages.
For instance, a tree can be associatedto a derivation in a contextual grammar, as we describe below.Consider the parentheses \[ and \] and denote by B their set.
A string w E (V U B)*,where V is an alphabet, is said to be minimally Dyck covered if:..w can be reduced to A by using reduction rules of the form \[x\] ~ ,~, forXE V+;if w = wl\]w2\[w3, with  Wl ,W 3 E (VU B)* and W 2 E W*, then w2 = )~.We denote by MDC(V) the language of all minimally Dyck covered strings overthe alphabet V.To any string x E MDC(V) we can associate a tree T(x) with labeled edges in thefollowing way:?
draw a dot representing the root of the tree; the tree will be representedwith the root up and the leaves down;?
scan x from the left to the right and grow 7-(x) according to the followingtwo rules:?
for each maximal substring \[w of x, for w E V* (hence after w we findeither \[ or \]), we draw a new edge, starting from the current point of thepartially constructed ~-(x), marked with w on its left side, and placed tothe right of the currently constructed tree;?
for each maximal w\], w E V*, not scanned yet (hence, either before w wefind \], or w = ,~ and to the left of \] we have a substring \[z for some z E V*already scanned), we climb the current edge, writing w on its right side.Here is a simple example.
The tree corresponding to the string:x = \[a\[ab\] \[ab\[ab\[c\]b\]b\]a\] \[a\](which is clearly in MDC({a, b, c})) is presented in Figure 2.
The nodes are numberedin the order of producing them (1 is the root).A bracketed contextual grammar is a construct:c = (V,A,  (S , ,C , )  .
.
.
.
.261Computational Linguistics Volume 24, Number 2abCcbAFigure 2Tree corresponding to the string x = \[a\[ab\]\[ab\[ab\[c\]b\]b\]a\]\[a\].where V is an alphabet, A is a finite subset of MDC(V), S ic  MDC(V), and Ci are finitesubsets of V" x V* - (A, A), for all 1 < i < n; in turn, n > 1.For x,y E MDC(V) we define:x ==Fin Yiff x = XlX2X3, y = xl\[ux2v\]x3, where xl,x3 E (Vl3B)*,x2 E MDC(V), and x2 E Si, (u,v) ECi, for some 1 < i < n. (Clearly, if x E MDC(V) and x ~ i ,  y, then y E MDC(V), hencethe definition above is consistent.
)The string language generated by a bracketed contextual grammar G = (V,A,($1,C1) .
.
.
.
.
(Sn, C,)) is defined by:L(G) = {pry(z) I w ==>~ z, for some w E A},where pry(z) denotes the projection of z E (V U B)* on V, that is the string in V*obtained by removing \[ and \] from z.We can also associate to G the bracketed language BL(G) defined by:BL(G) -= {(prv(z),T(z)) I w ==~ z, for some w E A}.Note the fact that each string in L(G) is paired with a tree in BL(G); however,the string should be read on the edges of this tree, not on leaf nodes as in the caseof derivation trees of context-free grammars.
The linguistic significance of such a treeis not yet clear to us, hence we do not insist on this idea (the ambiguity of contex-tual grammars and languages can be defined in this framework, but how the treeilluminates the grammatical structure of a sentence remains to be clarified).Another idea considered by Martfn-Vide and P~un (1998), closer to linguistics,is to introduce a dependence relation on the set of symbols appearing in axioms,262Marcus, Martfn-Vide, and P~un Contextual Grammarsselectors, and contexts of a contextual grammar.
We present some of the details of thisidea informally below.Consider an alphabet V and a string x c V*.
We denote:M(x) = {1,2 .
.
.
.
.
Ixl}and we write x = x(1)x(2)...x(n), for n = Ixl, x(i) E V, 1 < i < n. Any antireflexiverelation on M(x) is called a dependence r lation on x.
Let px be such a relation (antire-flexivity means i Px i for no value of i).
The pair (x, Px) is called a structured string.
Ifi px j, then we say that x(j) depends on x(i).
Let us denote by p~+ the transitive closureof px.
If i px + j, then we say that x(j) is subordinate to x(i).
A structured string (x, px)can be represented in a graphical form by writing the elements x(1), .
.
.
,  x(n) of x ina row and drawing above them arcs (x(i),x(j)) for i px j.
A structured string (X, px) iscalled a simple string of center x(io) if the graph associated to it as described aboveis a tree with the root marked with x(io) (the center corresponds to the predicativeelement of a sentence).The notion of a structured string is well-known in linguistics: see, for example,Chapter VI of Marcus (1967).
A related notion has been recently considered, that ofa link grammar: see Sleator and Temperley (1991), or Grinberg, Lafferty, and Sleator(1995).
In a link grammar, the elements of a sentence are correctly related in a linkage,according to a pairing of left and right connectors given for each word in the dictionary,providing that the obtained ependence r lation has several properties: the associatedgraph is connected, planar, etc.
Because we do not investigate here the possibility ofproducing correct linkages, in the sense of Sleator and Temperley (1991), by usingcontextual grammars (such results appear in Martin-Vide and P~un \[1998\]), we do notformally define the notion of a link grammar.For a structured string (X, px), x E V +, and a substring y of x, we denote by pxlythe restriction of px to y, defined in the natural way (we remove the symbols of x notappearing in y and we collect the remaining pairs of px).Now, a structured contextual grammar is a construct:G = (V,A,P),where V is an alphabet, A is a finite set of structured strings over V, and P is a finiteset of triples of the form Ix, (u, v);puxvl, with x E V +, (u, v) c V* x V*, and p,xv adependence r lation over uxv such that p,xv\[x = O.The elements of A are called axioms, the triples in P are called productions; in aproduction Ix, (u, v); puxvl, the string x is the selector, (u, v) is the context and puxv is arelation defining the structure of uxv; note that no dependence is considered betweenthe elements of x.
(Thus, we consider here only grammars with finite selectors.
)The derivation relation is defined (only for structured strings) as follows: for(x, px), (y, py), x, y E V +, we write:(x, px) =~c (y, py)iff x = XlX2X3, y = XlUX2VX3, for xl, x3 C V* and (X2, (U, P); Pux2vl E P, such that pylx, x~x3 =Px, and Py\]ux2v = PUXRV.
In words, the string x is enlarged with the context (u, v) andthe structure of x is extended according to the dependencies imposed by pux2v; dueto the restriction Pux2vlx2 = 0, the dependencies in x are not modified when adjoiningu, v. The elements of x2 can be linked to elements of Xl, x3, but the elements of u, vparticipate only in dependencies with elements of the selector string x2.263Computational Linguistics Volume 24, Number 2a c ba a c b ba a a c b b bFigure 3First three strings generated by G1.The string language generated by G isiL(G) = {w E V* \[ (x, Px) ~h (w, Pw), for some (X, px) c A}.The language of structured strings generated by a grammar G as above is:SL(G) = {(W, pw) \[ (X, px) ~ (W, pw), for some (x, px) c A}.Let us examine two examples.
For the grammar:G1 = ({a, b, c}, {(acb, {(2,1), (1, 3)})}, {/c, (a, b); {(2,1), (1, 3)}/} ),we obtain:L(G1) = {ancb n In _> 1},SL(G1) = {(a"cbn,{(n+l , i ) , ( i ,2n+2- i ) l l  < i<n}) \ ]n> l}.The first three strings generated by G1 are represented in Figure 3.The structured strings generated by G1 are simple strings with center c; the struc-ture graph is not planar if we preserve the order of elements of strings when writingthem in a row as above.For the grammar:G2 = ({a, b, c}, {(acb, {(2,1), (2, 3)})}, {/c, (a, b); {(2,1), (2, 3)})}),we obtain:n(G2) = {a"cb" In > 1},SL(G2) = {(ancbn, {(n + l,i),(n + l,2n + 2 -  i) l l < i< n}) l n > 1}.264Marcus, Martfn-Vide, and P~un Contextual Grammarsa c ba a c b ba a a c b b bFigure 4First three strings generated by G2.One sees that G1 and G2 are weakly equivalent, they generate the same string language,but they are not strongly equivalent, he structures of the same strings generated byG1 and G2 are not identical.
For instance, the first three strings generated by G2 are asshown in Figure 4.We again obtain simple strings with center c, but the graphs describing the struc-ture of the strings in the representation above are planar.These examples uggest that classes of structured contextual grammars should beconsidered on the basis of a classification of the graphs associated to their generatedstrings.
Thus, a grammar G = (V,A,P) is said to be connected, simple, or planar ifthe graphs associated to the relation describing the structure of the strings generatedby G is connected, a tree, or planar (when the string is written on a horizontal line,as before), respectively.
Moreover, we can use these properties as restrictions on thegrammar, selecting from the languages L(G), and SL(G) only the (structured) stringswhose structure graph has the properties mentioned above.
Of course, many othervariants can be defined; for instance, we can consider the various types of projectivity(progressive, regressive, strong, and so on), as investigated in Chapter VI of Marcus(1967).The above definitions of bracketed and structured contextual grammars can beextended in an obvious way to grammars with maximal use of selectors.
Some resultsin this area can be found in Martin-Vide and P~iun (1998), but a lot of questionsremain to be clarified.
The main problem is to find the most useful and natural typeof structured contextual grammars for describing the structure of natural anguagesyntactic onstructions.265Computational Linguistics Volume 24, Number 26.
Representations of Recursively Enumerable LanguagesCompleting the study on (weak) generative power of contextual grammars from Sec-tion 3, we now give a result proving the eccentric position of families of contextuallanguages with regard to the Chomsky hierarchy.Ehrenfeucht, P~un, and Rozenberg (1997) prove that each recursively enumerablelanguage L can be written in the form L ~- hi (h~-!
(L')), for L' E CLin (FIN) and hi, h2 twomorphisms.
In view of the inclusion CLin(FIN) C_ CLMt(FIN), this result is valid also forL' E CLM~(FIN) and L' E CLM~(REG).
Because CLin(FIN) - CLMx(REG) -~ 0, the resultof Ehrenfeucht, P~un, and Rozenberg (1997) is not directly valid for L t E CLMx(REG)or L ~ E CLMg(FIN).
However, the result of Ehrenfeucht, P~un, and Rozenberg (1997)can also be extended to these cases.
Because we shall use it below, we outline here theconstruction of Ehrenfeucht, P~un, and Rozenberg (1997).Take L C_ T*, L E RE, and a type-0 Chomsky grammar Go = (N, T, S, P) for L.Consider the new symbols \[,\], t-, and construct he contextual grammar G with thealphabet:V = N U T U {\[,\],1-},the starting string S, and the following productions:?2.3.
({u}, {(\[, Iv)}), for u ~ v E P,({ol\[u\]}, {0-, ol)}), for o~ E N U T, u --~ v E P,({o~-fl},{(t-, o~)}), for a, fl E N UT.Consider also the set:R= {\[u\] I u ---~ v E P} U {~ o~ I oL E NUT}.For each string w E R, consider a new symbol, bw; denote by D = {bw I w E R} theirset.
We define the coding hi : (D U T)* ~ T* by:hl(bw)=&,wER, hl(a)=a, aET,as well as the morphism h2 : (D U T)* ~ V* by:h2(bw)=w, w ER, h2(a)=a, aET.One obtains the equality L = hl(h~-l(Lin(G))).The idea is the following: h~ -1 is defined on (R U T)*, hence all derivations in Gthat do not produce words in (R U T)* will be "lost"; thus, h~ -1 acts like an intersectionwith the regular language (R U T)*, plus the conversion of each string w E R into theassociated symbol bw.
In order to obtain a string in (R U T)*, a derivation in G mustfollow a derivation in Go, in the sense that each rule u ~ v E P is simulated by aproduction of type 1, ({u}, ({(\[, \]v)}), thus replacing u with \[u\]v. The parentheses \[,\] "kill" the word u.
Productions of types 2 and 3 allow "living" symbols o~ to go tothe right, across "dead" symbols; also b is a "killer," specifically, of the symbol placedimmediately to its right.
The requirement that a word in (R U T)* must eventually bereached imposes the use of productions of type 1 for living u only, and the use ofproductions of types 2 and 3 for living c~ and dead u and fl, respectively.
After usingthese rules, u is dead, v is living (type 1), the first o~ is dead, the new one is living266Marcus, Martfn-Vide, and P~fun Contextual Grammars(types 2 and 3).
This ensures that the obtained word contains only dead symbols andkillers in words of R and living terminal symbols.
By means of hi, h~ -1, only the livingterminals remain.Note that the construction of Ehrenfeucht, Paun, and Rozenberg (1997) does notwork directly for the global maximal case: the grammar Go can contain, for instance,two rules u --, v, u I ~ v t with u a proper subword of u'; the first rule cannot besimulated in G when u' is present, because we are forced to use the maximal selector,u ~ in this case.
However, the proof can be modified to cover the case of global maximalselectors as well.Theorem 3Every language L E RE can be written in the form L = hl(h~-l(L')), where L' ECLMg(FIN) and hi, h2 are two morphisms.ProofTake L C_ T*, L E RE, and take a type-0 Chomsky grammar Go = (N, T, S, P) for L inthe Kuroda normal form, that is containing rules of the forms:1.
X~YZ,  X~a,  X~&,  forX, Y, ZEN,  aET ,2.
XY-~ ZU, for X, Y,Z, U E N.(Context-free rules and non-context-free rules, respectively, all of them with left-handand right-hand members of length at most two.
)Take a new symbol, c ~ T, and construct the Chomsky grammar G1 = (NU {S'}, TU{c}, S', P'), where:P' = {s' Sc} uU {XY- - ,ZUtXY~ZUEP,  X,Y,Z, UEN}UU {Xc~ --~ xo~ I X ~ x is a rule of type 1 in P and c~ E N U r U {c}}.It is easy to see that L(G1) = L(Go){C}.Now start the procedure of Ehrenfeucht, Phun, and Rozenberg (1997) from thegrammar G1, constructing the contextual grammar G exactly as in Ehrenfeucht, P~iun,and Rozenberg (1997) and extending the morphisms hi, h2 by:hi(c) = A,h2(c) = c.Because all rules in P~, excepting SI ~ Sc, which is used only once, have left-handmembers of the same length, the maximal restriction of using the associated selectorshas no effect.
Concerning selectors u and of\[u\], appearing in productions of type 1 andtype 2, respectively, the first selector for u is already dead (as is the case of the secondselector), so its use is illegal; it leads to nonsuccessful derivations.
The symbol c ispreserved by h~ -1 and it is erased by hi.
Consequently, with the details of the proofin Ehrenfeucht, P~iun, and Rozenberg (1997), we obtain L = hl(h~-l(LMg(G))), whichcompletes the proof.Corollary 2Every L E RE can be written in the form L = g(L'), where L' E CLMg(FIN) and g is ageneralized sequential transducer.267Computational Linguistics Volume 24, Number 2ProofA sequential transducer can simulate at the same time both the work of hi and of h~ -1.\[\]These results have a rather InterestIng consequence.Theorem 4Every family F of languages uch that L INc  F C RE that is closed under direct andinverse morphisms is incomparable with each family CL~(F'), for o~ E {MI, Mg} andF' E {FIN, REG}.ProofConsider a family F with the properties mentioned above.
Because L INc  F andLIN - CL~(REG) ~ O for both o~ E {MI, Mg}, we get F - CL~(F') ~ 0 for o~,F' as above.Let us now prove that also the assertion CL~(F') - F ~ 0 holds, for o~ and F' as above.Assume the contrary, that is, CL~(F') G F. The closure of CL~(F I) under direct andinverse morphisms hould be included in the closure of F under direct and inversemorphisms.
From Theorem 3 we know that the closure of CL~(F ~) under direct andinverse morphisms is equal to RE.
From the closure properties of F, the closure of Funder direct and inverse morphisms is equal to F. This implies RE C F, contradictingthe strictness of the inclusion F c RE from the theorem statement.
\[\]Important families in formal language theory that fulfill the conditions in Theorem4 are: (1) languages generated by programmed grammars without appearance check-ing but possibly using h-rules introduced by Rosenkrantz (1969) (they are equivalent tomany other grammars with context-free core rules applied in a regulated manner: seeDassow and P~un \[1989\]; (2) indexed languages (Aho 1968); (3) ETOL languages (gen-erated by extended tabled interactionless LIndenmayer systems; ETOL is the largestfamily in this area--see Rozenberg and Salomaa \[1980\]; and (4) other subfamilies ofETOL (for instance, EOL).
Each of the families CL~(FIN),e~ E {in, MI, Mg}, contains(context-sensitive) languages outside these families.
Therefore, the families CL~ (FIN)occupy a quite eccentric position in the Chomsky hierarchy (Figure 5).7.
Summary and Final RemarksIn this paper, we have continued the investigation of contextual grammars with (globalor local) maximal use of selectors, recently introduced by Martfn-Vide et al (1995).
Wehave mainly borne in mind issues concerning the adequacy of these grammars as analternative model (with respect o Chomsky grammars) for the syntax of natural an-guages, because "the arguments against he adequacy of phrase structure grammar (asdefined by Chomsky) are absolutely incontrovertible (although they also apply to fullcontext-sensitive grammars and to unrestricted grammars), that is, the constructions ofnatural anguages cannot be described In an adequate way using the descriptive mech-anisms of such grammars .. .
.
Bizarre though it may sound .
.
.
.
Bloomfield's theory ofconstructions i probably the best point of departure for future work on the subject"(Manaster Ramer 1994, 20).
We need to keep in mInd, as Manaster Ramer (1994) pointsout, that "the kinds of mathematical models we are used to are, of course, largely de-rived from Chomsky's early work on phrase structure, and this in turn represents ...the formalization of a terribly diminished, impoverished, and even caricatured ideaof immediate constituent analysis, created by Leonard Bloomfield" (p. 22).268Marcus, Martfn-Vide, and P~un Contextual Grammars#fCF CSCL~(FIN)~JJFigure 5Position of the families CL~ (FIN) in the Chomsky hierarchy.Our essential arguments have been the following:...4..The families of contextual languages are incomparable with some basicfamilies in the Chomsky hierarchy (with LIN and CF) or in refinementsof this hierarchy (programmed languages, indexed languages, languagesgenerated by various classes of Lindenmayer systems).
Some pieces ofevidence indicate that perhaps natural anguages occupy a similarincommensurate position with regard to Chomsky's classification.Contextual grammars with global maximal use of selectors cannotgenerate all languages based on center-embedded constructions, asChomsky linear grammars (and TAGs) do.
Such constructions seem notto be very frequent in natural anguages.Contextual grammars with (global or local) maximal use of selectors cangenerate, in a very easy way, the three basic non-context-fleeconstructions in natural anguages: reduplication, crossed dependencies,multiple agreements.Contextual grammars are sensitive to using markers, languages of theform {wcw\[w E {a,b}*} and {wc mi(w) I w E {a,b}*} are handled moreeasily (i.e., by classes of grammars with simpler features) than{ww Iw E {a,b}*} and {w mi(w) \[ w E {a,b}*}.
This again corresponds toour intuition, but it does not fit the Chomsky hierarchy.By definition, contextual grammars are "fully" lexicalized (they use onlyterminal symbols), and their languages have the bounded growthproperty, which is specific to natural anguages (and one of the main269Computational Linguistics Volume 24, Number 2..ideas behind the notion of mild context-sensitivity, see Joshi \[1985\]): eachword generated by a contextual grammar---excepting the axioms--isobtained by adjoining a context from a finite set.If we intend our model to convey some cognitive meaning, we must saythat the simple operation of adjoining might be closer than rewriting tothe way our brain may work when building a sentence.
It is hard toimagine our brain using auxiliary intermediate sentences of anonterminal type.
Instead, it looks more natura l ,  in  the proper sense ofthe word, to start with a collection of well-formed sentences, maybeacquired from experience, and to produce new well-formed ones byadding further words, in pairs that can observe dependencies andagreements, and in accordance with specified selectors, which can ensurethe preservation of grammaticality.
Of course, this is only a speculation,but it also fits with the general idea of "natural computation": forexample, nature seems not to use the rewriting operation in the area ofgenetics, where recombination (crossing over) of chromosomes is thebasic evolutionary operation (together with nondeterministic insertionand deletion operations, which, again, are not rewriting) and where no"nonterminal symbol" is used.
Further discussion of this topic can befound in Martin-Vide (1997).A structure for the words generated by a contextual grammar can beintroduced in various ways.
By parenthesizing the contexts, we get atree.
Considering dependence r lations on symbols appearing in axioms,contexts, and selectors, we can obtain structured strings of a type wellinvestigated in descriptive linguistics and very similar to thephrase-linkage structures produced by a link grammar.A number of the previous points need further investigation.
There are also severaltopics that are important from a linguistic point of view and that are still poorlyinvestigated for contextual grammars.
The main one concerns the parsing algorithmsand their complexity.
Polynomial parsing algorithms were found for a few variantsof contextual grammars, which is encouraging, but the problem is still open for thevariants discussed in this paper.
-The main aim of this paper was to call the reader's attention to contextual gram-mars, to prove that they deserve further research efforts, especially in terms of theirlinguistic adequacy and relevance.
It is our (optimistic) belief that such efforts will berewarded.Appendix: Proofs of Some Assertions Represented in Table 1ProofAssume that M1 E CL~ (F/N), o~ E {MI ,  Mg},  take a grammar G for M1, and consider astring of the form:zi = aba2b2 .
.
.
aibi caba2b2 .
.
.
aib i,for a large-enough integer i.
In order to produce such a string, we need a derivation:W ~ WlW2W3 =:::::'~c~ WlUW2VW3 ~ Zi"It is obvious that IW21 depends on i, and so cannot be bounded; therefore G cannothave finite selectors only.
\[\]270Marcus, Martfn-Vide, and P~un Contextual GrammarsProofAssume that M~ E CLin (F), for F E {FIN, REG}.
Take G = ({a, b}, A, ($1, C , ) , .
.
.
,  (Sn, Cn))such that Li,(G) = M~.
There is at least one context (u,v) in G with uv # A; all thestrings in M~ are of even length, so luvl must be even.
Take x in the selector of (u, v)and consider the strings xaixa i, i > luvl.
Then uxvaixa i E Lin(G), so uxvaixa i = yy, fory E {a, b}*.
This implies:uxvai-luv\]/2 = aluvl/2xa i,that is, uxv = zd , j  > 1.
In the same way, starting from xbixb i we get that uxv = z'bk, k _>1, a contradiction.
As in point 1, we obtain that M~ ~ CL~ (FIN), c~ E {MI, Mg}.
\[\]ProofAssume that M~ = LMt(G), for any G = ({a,b},A,(S1,C1), .
.
.
,  (Sn, Cn)) with regularselectors.
Let us denote:~(X) = {(u,v) l (u,v) E Ci, x E Si, l < i < n}, x E {a,b}*,~-l((u,v)) = {x E {a,b}* I (u,v) E ~(x)}, (u,v) E Ci, 1 < i < n.All strings ambamb, m > 1, are in M~.
Take such a string with arbitrarily large m. If thereis a derivation step aq =:::=~M!
ambamb, then there is a context (u, v) = (a 6 ba 6, a6b) E ~(aP),for p < q.
As m = /2 + p +/3, it follows that p is arbitrarily large.
The set qa-l((u,v))is regular (it is the union of a finite number of regular sets), so it contains an infinitenumber of strings of the form a s (we apply a pumping lemma to a m in ~-l((u,v))).Therefore, (u, v) must be used for a maximal selector of the form a t. In this way, astring aJ'baJ2ba j3 can be produced, with bounded jl,j3 and arbitrarily large j2.
Such astring is not in M~, a contradiction.
Therefore, in the derivation of ambamb there existsan arbitrary number of derivation steps of the form:aSbaSb ==-~Mi aS+kbaS+kb,with k _> 1 and aPba q E ~-l((ak, ak)).
Consider now a string:w = d'ba'2bdlbat2b,with arbitrarily large il,/2.
Each such string is in M~.
If aPba q above or any other selectorof the form arba r' from ~-l((ak, ak)) is maximal in w, then we shall produce a stringwhich is not in M~.
On the other hand, flPbfl q is a subword of w, so the selectorsincluded in ~-l((ak, ak)) must contain a string that is a strict superword of aPba q, inorder to prevent he generation of a parasitic word.
Such a superword can only beof the forms aJ'bai2baJ 2 or ahbailba j2.
In both cases, the middle subword, bai2b or bai'b,respectively, is arbitrarily long.
As elements of a regular language, such strings havepumping properties.
Let us consider the case of ba6b (the second one is similar).
Thismeans that all the strings of the form:Z = a h bai2+rhbaJ2,for r > 1 and all h > 0, are in ~-l((u,v)).
Take such a string z with h being largeenough to have:i2 + rh > jl + j2.Consider the string:w ~- ai2+rh-J2bai2+rhba j2.271Computational Linguistics Volume 24, Number 2Because:i2 + rh = (/2 + rh - j2) + j2,we have w E M~.
Because:i2 -4- rh - j2 > jl,the context (a k, a k) is applicable to w. That is:W ::==~MI ai2+rh-j2+kbai2+rhbaJ2+k.The string obtained is not in M~, a contradiction.
In conclusion, M~ cannot be inCLMI(REG).
The previous argument does not hold for the global maximal derivation,so the relation M~ E CLMg(REG) remains open.
\[\]ProofFor the grammar G = ({a, b, c}, {c}, ({c}, { (a, a), (b, b) })), we have L~ (G) = M4 for all o~.\[\]ProofThe fact that M~ ~ CLin(REG) is already proved in P~iun (1982).
As for M1, one caneasily prove that M~ ~ CL~ (FIN), o~ C {MI, Mg}.
On the other hand, for the grammarG = ({a, b}, {,~}, ({a, b}*, {(a,a), (b, b)})),we have LMI(G) = LMg(G) = M~.
Hence M~ c CL~(REG),c~ E {MI, Mg}.
\[\]AcknowledgmentsThe authors are much indebted to threeanonymous reviewers, who very carefullyread a previous draft of the manuscript, andwho proposed a number of modificationsthat have improved both the readability andthe content of this paper.ReferencesAho, Alfred V. 1968.
Indexed grammars.
AnExtension of context-free grammars.Journal of the ACM, 15:647---671.Bar-Hillel, Yehoshua nd Eliyahu Shamir.1964.
Finite state languages: Formalrepresentations and adequacy problems.In Yehoshua Bar-Hillel, editor, Languageand Information.
Addison-Wesley, Reading,MA, pages 87-98.Ba"l~inescu, Tudor and Marian Gheorghe.1987.
Program tracing and languages ofaction.
Revue roumaine delinguistique--Cahiers delinguistiquethdorique t appliqude, 32:167-170.Chomsky, Noam.
1964.
On the Notion "Ruleof Grammar".
In Janet A. Fodor andJerrold J. Katz, editors, The Structure ofLanguage: Readings in the Philosophy ofLanguage.
Prentice Hall, Englewood Cliffs,N.J., pages 50-118.Culy, Christopher.
1985.
The complexity ofthe vocabulary of Bambara.
Linguistics andPhilosophy, 8:345-351.Dassow, Jtirgen and Gheorghe P~un.
1989.Regulated Rewriting in Formal LanguageTheory.
Springer, Berlin.Ehrenfeucht, Andrzej, Gheorghe P~iun, andGrzegorz Rozenberg.
1997.
Onrepresenting recursively enumerablelanguages by internal contextuallanguages.
Theoretical Computer Science.
Toappear.Floyd, R. W. 1962.
On the non-existence of aphrase-structure grammar for Algol-60.Communications of the ACM, 5:483-484.Gazdar, Gerald and Geoffrey K. Pullum.1985.
Computationally relevant propertiesof natural languages and their grammars.New Generation Computing, 3:273-306.Grinberg, Dennis, John Lafferty, and DanielSleator.
1995.
A robust parsing algorithmfor link grammars.
In Proceedings oftheFourth International Workshop on ParsingTechnologies, pages 111-125,Prague/Karlovy Vary.Harrison, Michael.
1978.
Introduction toFormal Language Theory.
Addison-Wesley,Reading, MA.Hockett, Charles.
1970.
The State of the Art.Mouton, The Hague.272Marcus, Martfn-Vide, and P~un Contextual GrammarsIlie, Lucian.
1997a.
On computationalcomplexity of contextual languages.Theoretical Computer Science.
To appear.Ilie, Lucian.
1997b.
The computationalcomplexity of Marcus contextuallanguages.
Submitted.Jan~ar, Petr, Franti~ek Mr~z, Martin Pl~tek,Martin Proch~izka, and J6rg Vogel.
1996.Restarting automata, Marcus grammarsand context-free languages.
In Ji~rgenDassow, Grzegorz Rozenberg, and ArtoSalomaa, editors, Developments in LanguageTheory II.
World Scientific, Singapore,pages 102-111.Joshi, Aravind K. 1985.
How muchcontext-sensitivity is required to providestructural descriptions: Tree adjoininggrammars.
In David Dowty, LauriKarttunen, and Arnold Zwicky, editors,Natural Language Processing:Psycholinguistic, Computational, ndTheoretical Perspectives.
CambridgeUniversity Press, New York,pages 206--250.Joshi, Aravind K. 1987.
An introduction totree adjoining grammars.
In AlexisManaster Ramer, editor, Mathematics offLanguage.
John Benjamins, Amsterdam,pages 87-114.Joshi, Aravind K., Leon S. Levy, andM.
Takahashi.
1975.
Tree adjunctgrammars.
Journal of Computer and SystemsSciences, 10:136-163.Manaster Ramer, Alexis.
1993.
Capacity,complexity, construction.
Annals ofMathematics and Artificial Intelligence,8(1-2): Mathematics of language,pages 1-16.Manaster Ramer, Alexis.
1994.
Uses andmisuses of mathematics in linguistics.
XCongreso de Lenguajes Naturales yLenguajes Formales, Sevilla.Marcus, Solomon.
1967.
Algebraic Linguistics.Analytical Models.
Academic Press, NewYork.Marcus, Solomon.
1969.
Contextualgrammars.
Revue roumaine desmathdmatiques pures et appliqudes,14:1525-1534.Marcus, Solomon, editor.
1978.
La sdmiotiqueformelle du folklore.
Klincksieck,Paris/Publishing House of the RomanianAcademy, Bucharest.Marcus, Solomon.
1979.
Linguistics forprogramming languages.
Revue roumainede linguistique--Cahiers de linguistiquethdorique et appliqude, 16(1):29-38.Marcus, Solomon, editor.
1981-83.
ContextualAmbiguities inNatural and ArtificialLanguages.
Communication and CognitionMonographs, 2 volumes.
Ghent.Marcus, Solomon.
1997.
Contextualgrammars and natural anguages.
InGrzegorz Rozenberg and Arto Salomaa,editors, Handbook of Formal Languages,volume 2, pages 215-235.Marffn-Vide, Carlos.
1997.
Naturalcomputation for natural anguage.Fundamenta Informaticae, 31.2:117-124.Martfn-Vide, Carlos, Alexandru Mateescu,Joan Miquel-Verg~s, and Gheorghe P~un.1995.
Internal contextual grammars:Minimal, maximal and scattered use ofselectors.
In M. Koppel and EliyahuShamir, editors, Proceedings ofThe FourthBar-Ilan Symposium on Foundations ofArtificial Intelligence (BISFAI'95),pages 132-142.
Ramat Gan/Jerusalem.Also in M. Koppel and Eliyahu Shamir,editors, Proceedings ofthe fourth Bar-IlanSymposium on Foundations ofArt~cialIntelligence.
Focusing on Natural Languagesand Artificial Intelligence: Philosophical ndComputational Aspects.
AAAI Press, MenloPark, CA, 1997, pages 159-168.Marffn-Vide, Carlos and Gheorghe P~un.1998.
Structured contextual grammars.Grammars: To appear.Miquel-Verg4s, Joan.
1997.
Models AlgebraicsAnahlics del Llenguatge.
Un Exemple: lesGramdtiques Contextuals; FormalitzacidEstudi de les Seves Aplicacions.
Ph.D.dissertation, Rovira i Virgili University,Tarragona.Olivetti, Convegno.
1970.
Linguaggi nellaSocietd enella Tecnica.
Edizzioni diComunit$, Milano.Partee, Barbara H., Alice ter Meulen, andRobert E. Wall.
1990.
Mathematical Methodsin Linguistics.
Kluwer, Dordrecht.P~un, Gheorghe.
1976.
Languages associatedto a dramatic work.
Revue roumaine delinguistique: Cahiers de linguistique th~oriqueet appliqude, 13:605-611.P~un, Gheorghe.
1979.
A formal inguisticmodel of action systems.
Ars Semeiotica,2:33--47.P~un, Gheorghe.
1982.
Contextual Grammars.The Publishing House of the RomanianAcademy, Bucharest, in Romanian.P~un, Gheorghe.
1985.
On some openproblems about Marcus contextualgrammars.
International Journal of ComputerMathematics, 17:9-23.P~un, Gheorghe.
1994.
Marcus contextualgrammars.
After 25 years.
Bulletin of theEATCS, 52:263-273.P~un, Gheorghe.
1997.
Marcus ContextualGrammars.
Kluwer, Boston, Dordrecht.P~un, Gheorghe and Xuan My Nguyen.1980.
On the inner contextual grammars.Revue roumaine des mathdmatiques pures et273Computational Linguistics Volume 24, Number 2appliqu~es, 25:641-651.P~un, Gheorghe, Grzegorz Rozenberg, andArto Salomaa.
1994.
Marcus contextualgrammars: Modularity and leftmostderivation.
In Gheorghe P~un, editor,Mathematical Aspects of Natural and FormalLanguages.
World Scientific, Singapore,pages 375-392.Pullum, Geoffrey K. 1985.
On two recentattempts to show that English is not aCFL.
Computational Linguistics, 10:182-186.Pullum, Geoffrey K. 1986.
Footloose andcontext-free.
Natural Language andLinguistic Theory, 4(3):409-414.Pullum, Geoffrey K. 1987.
Nobody goesaround at LSA meetings offering odds.Natural Language and Linguistic Theory,5(2):303--309.Pullum, Geoffrey K. and Gerald Gazdar.1982.
Natural anguages and context-freelanguages.
Linguistics and Philosophy,4:471-504.Radzinski, Daniel.
1990.
Unboundedsyntactic opying in Mandarin Chinese.Linguistics and Philosophy, 13:113-127.Rosenkrantz, Daniel J.
1969.
Programmedgrammars and classes of formallanguages.
Journal of the ACM, 16:107-131.Rounds, William C., Alexis Manaster Ramer,and Joyce Friedman.
1987.
Finding naturallanguages a home in formal anguagetheory.
In Alexis Manaster Ramer, editor,Mathematics ofLanguage.
John Benjamins,Amsterdam, pages 375-392.Rozenberg, Grzegorz and Arto Salomaa.1980.
The Mathematical Theory of L Systems.Academic Press, New York.Rozenberg, Grzegorz and Arto Salomaa,editors.
1997.
Handbook of FormalLanguages, 3 volumes.
Springer, Berlin.Salomaa, Arto.
1973.
Formal Languages.Academic Press, New York.Savitch, Walter J.
1991.
Infinity is in the eyesof the beholder.
In C. Georgopoulos andR.
Ishihara, editors, InterdisciplinaryApproaches toLanguage: Essays in Honor ofS.-Y.
Kuroda.
Kluwer, Dordrecht,pages 487-500.Savitch, Walter J.
1993.
Why it might pay toassume that languages are infinite.
Annalsof Mathematics and ArtiJ~'cial Intelligence,8(1-2): Mathematics of language,pages 17-25.Savitch, Walter J., Emmon Bach, WilliamMarsh, and Gila Safran-Naveh, editors.1987.
The Formal Complexity of NaturalLanguage.
Kluwer, Dordrecht.Shieber, Stuart M. 1985.
Evidence againstthe context-freeness of natural anguage.Linguistics and Philosophy, 8:333-343.Sleator, Daniel and D. Temperley.
1991.Parsing English with a link grammar.Technical Report CMU-CS-91-196, Schoolof Computer Science, Carnegie MellonUniversity, Pittsburgh.274
