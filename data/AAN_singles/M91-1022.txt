GE:DESCRIPTION OF THE NLTOOLSET SYSTEM AS USED FO RMUC- 3George Krupka, Paul Jacobs, Lisa Rau, and Lucja Iwaiisk aArtificial Intelligence LaboratoryGE Research and Developmen tSchenectady, NY 12301 US Akrupka@crd .ge.comAbstrac tThe GE NLTooLSET is a set of text interpretation tools designed to be easily adapted to newdomains.
This report summarizes the system and its performance on the MUG-3 task.INTRODUCTIO NThe GE NLTooLsET aims at extracting and deriving useful information from text using a knowledge-based ,domain-independent core of text processing tools, and customizing the existing programs to each new task .The program achieves this transportability by using a core knowledge base and lexicon that adapts easil yto new applications, along with a flexible text processing strategy that is tolerant of gaps in the program' sknowledge base .The NLTooLSET's design provides each system component with access to a rich hand-coded knowledg ebase, but each component applies the knowledge selectively, avoiding the computation that a completeanalysis of each text would require .
The architecture of the system allows for levels of language analysis ,from rough skimming to in-depth conceptual interpretation .The NLTooLSET, in its first version, was behind GE 's participation in the MUCK-II conference .
SinceMUCK-II, the Toolset, now in Release 2 .1, has expanded to include a number of new capabilities, includinga text pre-processor for easier customization and better performance, broader lexical and syntactic coverage ,and a domain-independent module for applying word-sense preferences in text .
In addition to being teste din several new application areas, the Toolset has achieved about a 10 times speedup in words per minute sover MUCK-II, and can now partially interpret and tag word senses in arbitrary news stories, although it i svery difficult to evaluate this task-independent performance .
These basic enhancements preceded the otheradditions, including a discourse processing module ;, which were made for MUC-3 .The performance of the program on tasks such as MUCK-II and MUC-3 derives mainly from two designcharacteristics : central knowledge hierarchies and flexible control strategies .
A custom-built 10,000 word-rootlexicon and 1000-concept hierarchy provides a rich source of lexical information .
Entries are separated b ytheir senses, and contain special context clues to help in the sense-disambiguation process .
A morphologica lanalyzer contains semantics for about.
75 affixes, and can automatically derive the meanings of inflecte dentries not separately represented in the lexicon .
Domain-specific words and phrases are added to th elexicon by connecting them to higher-level concepts and categories present in the system 's core lexicon andconcept hierarchy.
Lexical analysis can also be restricted or biased according to the features of a domain .This is one aspect of the NLTooLSET that makes it highly portable from one domain to another .The language analysis strategy in the NLTooLSET uses fairly detailed, chart-style syntactic parsin gguided by conceptual expectations .
Domain-driven conceptual structures provide feedback in parsing, con-tribute to scoring alternative interpretations, help recovery from failed parses, and tie together informationacross sentence boundaries .
The interaction between linguistic and conceptual knowledge sources at the leve lof linguistic relations, called "relation-driven control" was a key system enhancement before MUC-3 .In addition to flexible control, the design of the NLTooLSET allows each knowledge source to influenc edifferent stages of processing .
For example, discourse processing starts before parsing, although many deci-sions about template merging and splitting are made after parsing .
This allows context to guide languag eanalysis, while language analysis still determines context .144The next section briefly describes the major portions of the NLTooLSET and its control flow ; the re-mainder of the paper will discuss the application of the Toolset to the MUC-3 task .SYSTEM OVERVIEWProcessing in the NLTooLSET divides roughly into three stages : (1) pre-processing, consisting mainly o fa pattern matcher and discourse processing module, (2) linguistic analysis, including parsing and semanticinterpretation, and (3) post-processing, or template filling .
Each stage of analysis applies a combination o flinguistic, conceptual, and domain knowledge, as shown in Figure 1 .Pre-processing Analysis Post-processingSyntax Tagging, bracketing Parsing ,attachment ScoringSemantics Collocations ,cluster analysisSense discrimination ,role mappingRoleextensionDomainTemplatesTemplate activation ,text segmentationAmbiguity pruning ,recoveryDefault filling ,merging, splitting, etc .Figure 1 : Stages of data extractionThe pre-processor uses lexico-semantic patterns to perform some initial segmentation of the text, iden-tifying phrases that are template activators, filtering out irrelevant text, combining and collapsing som elinguistic constructs, and marking portions of text that could describe discrete events .
This component isdescribed in [1] .
Linguistic analysis combines parsing and word sense-based semantic interpretation withdomain-driven conceptual processing .
The programs for linguistic analysis are largely those explained i n[2, 3]?the changes made for MUC-3 involved mainly some additional mechanisms for recovering from failedprocessing and heavy pruning of spurious parses .
Post-processing includes the final selection of template sand mapping semantic categories and roles onto those templates .
This component used the basic element sfrom MUCK-II, adding a number of specialized rules for handling guerrilla warfare, types, and refines th ediscourse structures to perform the template splitting and merging required for MUC-3 .The control flow of the system is primarily from linguistic analysis to conceptual interpretation to domai ninterpretation, but there is substantial feedback from conceptual and domain interpretation to linguistic anal-ysis .
The MUC-3 version of the Toolset includes our first implementation of a strategy called relation-drive ncontrol, which helps to mediate between the various knowledge sources involved in interpretation .
Basically,relation-driven control gives each linguistic relation in the text (such as subject-verb, verb-complement, o rverb-adjunct) a preference score based on its interpretation in context .
Because these relations can apply t oa great many different surface structures, relation-driven control provides a means of combining preference swithout the tremendous combinatorics of scoring many complete parses .
Effectively, relation-driven controlpermits a "beam" strategy for considering multiple interpretations without producing hundreds or thousandsof new paths through the linguistic chart .The knowledge base of the system, consisting of a feature and function (unification-style) gramma rwith associated linguistic relations, and the lexicon mentioned earlier, still proves transportable and largel ygeneric .
The core lexicon contains over 10,000 entries, of which 37 had to be restricted because of specialize dusage in the MUC-3 domain (such as device, which always means a bomb, and plant, which as a verb usuallymeans to place a bomb and as a noun usually means the target of an attack) .
The core grammar containsabout 170 rules, with 50 relations and 80 additional subcategories .
There were 23 MUC-specific additionsto this grammatical knowledge base, including 8 grammar rules, most of them dealing with unusual nou nphrases that describe organizations in the corpus .14 5The control, pre-processing, and transportable knowledge base were all extremely successful for MUG-3; remarkably, lexical and grammatical coverage, along with the associated problems in controlling searc hand selecting among interpretations, proved not to be the major stumbling blocks for our system?furthe rdistinguishing events and merging or splitting templates proved to be the major obstacle in obtaining abetter score .ANALYSIS OF TST1-009 9The common "walkthrough" example, TST1-0099, is a good example of many of the problems in analysi sand template filling, although it is somewhat unrepresentative of the difficulties in parsing because the ke ycontent is contained in fairly simple sentences.
We will explain briefly what our program did, then providedetails of the story-level and sentence-level interpretation with an analysis of the templates produced .Overview of Exampl eIn many ways, TST1-0099 is representative of how the Toolset performed on MUC-3 .
The program parsedmost of the key sentences, failed to parse some of the less relevant sentences, missed a key relationshi pbetween locations?thus failing to split a template into two separate events?and incorrectly included a nearlier bombing as part of a main event in the story .
One additional program fill was scored incorrect becaus ethe answer key had the wrong date .
The program thus derived 36 slots out of a possible 43, with 21 correct ,2 partial, 2 incorrect, and 11 spurious, for 51% recall, 61% precision, and 30% overgeneration .Detail of Message Ru nAs explained in the previous section, the Toolset uses pattern matching for pre-processing, followed b ydiscourse processing, parsing and semantic interpretation, and finally template-filling .Pre-processing uses pattern matching to manipulate the input text .
It recognizes relevant fillers, tags ,collapses constructs, and segments the text into fragments describing different events .
For example, th epattern matcher recognizes the phrase describing the bombing event in the first sentence of the text, collapsesthe conjunctive phrase the embassies of the PRC and the Soviet Union, and marks that as a complementize r(rather than a relative pronoun, pronoun, or determiner) .
In later sentences, it also marks locative phraseslike in the Lima residential district of San Isidro and located in Orrantia district .
The discourse processin gmodule does an initial text segmentation based on (1) definite and indefinite references like a car bomb andthe attack, (2) the relationship between events (e .g .
bombing and arson), and (3) cue phrases .
This identifiessix events :(1) the phrase bombed describes a bombing event ; the phrase the bombs marks its continuation ;(2) the phrase a car bomb exploded signifies a new bombing event ;(3) the temporal cue phrase meanwhile combined with the phrase two bombs in indefinite form signifie sanother bombing event ; the phrases the bombs and the attacks mark the continuation of this event ;(4) the phrase a Shining Path bombing indicates yet another bombing event ; the sentence gets delete dbecause the temporal information in the phrase some three years ago violates MUC-3 constraints ;(5) the cue phrase in another incident combined with the phrases killed and dynamite delineates anothe rbombing event ; the sentence gets deleted because the temporal information from the phrase three years ag oviolates MUC-3 constraints ;(6) the phrase burned indicates a new arson event .Linguistic analysis parses each sentence and produces (possibly alternative) semantic interpretations atthe sentence level .
These interpretations select word senses and roles, heavily favoring domain-specific senses .Post-processing maps the semantic interpretations onto templates, eliminating invalid fills (in this cas enone), combining certain multiple references (such as to the embassies), and adding certain information (lik eadding numbers and "guessing" terrorist groups as fillers when it fails to find evidence to the contrary) .Post-processing collapses three of the segments (events) produced during pre-processing into one template .146Sentence-level Interpretatio nThe following is the trace of the first two sentences of TST1-0099 : The "call" to Trumpet represents the en dof the first stage of semantic interpretation and the beginning of conceptual analysis, and the output tha tfollows this represents the mapping (or role extension) from semantic roles to templates .
; ; ; Trigger rules filtered 1 tokens (3 .457.
)1 *REDUCEI-CONTEXT* Bracketing :[BRACKET : [TEXT] ]POLICE HAVE REPORTED THAT TERRORISTS [DATE : TONIGHT ]BOMBED TH EEMBASSIES OF THE PRC AND THE SOVIET UNION .Creating temporal pp for "TONIGHT "1 *REDUCE2-CONTEXT* Bracketing :POLICE [AUX : HAVE REPORTED ][CMPLZR : THAT] TERRORISTS TONIGHT BOMBED [EMBASSY : THEEMBASSIES OF THE PRC AND THE SOVIET UNION] .1 *LIST-CONTEXT* Bracketing :POLICE HAVE REPORTED THAT TERRORISTS TONIGHT BOMBED [GLIST : THE EMBASSIES PRCAND EMBASSIES THE SOVIET UNION] .TOKEN DANGER : Creating list NP from "THE EMBASSIES PRC AND EMBASSIES THE SOVIET UNION "Calling Trumpet with Sexp :(VERB_REPORT 1(R-PATIENT(VERB_BOMB 1(R-PATIENT(COORDCONJ_AND 1(R-PART1 (NOUN_EMBASSY1 (R-PART (C-ENTITY) )(R-NATIONALITY (NATION-NAMECHINAI (R-NAME CHINA)) )(R-DEFINITE (DET_THE1))) )(R-PART1 (NOUN_EMBASSY1 (R-PART (C-ENTITY) )(R-NATIONALITY (NATION-NAMESOVIET-UNION1 (R-NAME SOVIET-UNION)))))) )(R-DATE(C-DATE-OF-OCCURRENCE (R-RELATIVE NO) (R-YEAR NIL) (R-DAY 1251) (R-MONTH OCT)) )(R-AGENT (C-VERB TERRORIZEI-ER) )(R-INSTRUMENT (C-BOMB))) )(R-COMMUNICATOR (NOUN_POLICE1)) )TRUMPET WARN : Breaking out core templates (C-BOMBING-TEMPLATE )Processing core template :(C-BOMBING-TEMPLATE (R-INSTRUMENT (C-BOMB) )(R-DATE (C-DATE-OF-OCCURRENCE (R-RELATIVE NO )(R-YEAR NIL )(R-DAY 1251 )(R-MONTH OCT)) )(R-TARGE T(COORDCONJ_AND 1(R-PART1(NOUN_EMBASSY1 (R-PART (C-ENTITY) )(R-NATIONALITY (NATION-NAME_CHINA1 (R-NAME CHINA)) )(R-DEFINITE (DET_THE1))) )(R-PART1(NOUN_EMBASSY1 (R-PART (C-ENTITY) )(R-NATIONALIT Y(NATION-NAME_SOVIET-UNION1 (R-NAME SOVIET-UNION)))))) )(R-PERPETRATOR (C-VERB TERRORIZE1-ER)))147Calling Trumpet with Sexp :(C-CAUSING (R-CAUSE (C-BOMB (R-DEFINITE (DET_THE1))) )(R-EFFEC T(COORDCONJ_BUT1 (R-PART1 (NOUN_DAMAGE1) )(R-PART2 (C-INJURY (R-POLARITY (DET_NO1)))))) )TRUMPET WARN : Breaking out core template s(C-INJURY-TEMPLATE C-DAMAGE-TEMPLATE )Assuming C-BOMBING-TEMPLATE and C-INJURY-TEMPLATE have same R-INSTRUMEN TAssuming C-BOMBING-TEMPLATE and C-DAMAGE-TEMPLATE have same R-INSTRUMEN TAssuming C-BOMBING-TEMPLATE and C-INJURY-TEMPLATE have same R-DAT EAssuming C-BOMBING-TEMPLATE and C-DAMAGE-TEMPLATE have same R-DAT EAssuming C-BOMBING-TEMPLATE and C-INJURY-TEMPLATE have same R-PERPETRATO RAssuming C-BOMBING-TEMPLATE and C-DAMAGE-TEMPLATE have same R-PERPETRATO RAssuming C-BOMBING-TEMPLATE and C-INJURY-TEMPLATE have same R-TARGE TAssuming C-BOMBING-TEMPLATE and C-DAMAGE-TEMPLATE have same R-TARGE TComparison of Program Answers with Answer KeyThe NLTooLsET results for TST1-0099 were the following templates :0 .
MESSAGE-IDTST1-MUC3-00991.
TEMPLATE-ID12.
INCIDT-DATE25 OCT 893.
INCIDT-TYPEBOMBING4.
CATEGORYTERRORIST ACT5.
INDIV-PERPS"TERRORISTS "6 .
ORG-PERPS"MAOIST \"SHINING PATH\" GROUP ""GUEVARIST \"TUPAC AMARU REVOLUTIONARY MOVEMENT\" "7.
PERP-CONFPOSSIBLE : "MAOIST \"SHINING PATH\" GROUP"POSSIBLE : "GUEVARIST \"TUPAC AMARU REVOLUTIONARY MOVEMENT\" "8.
PHYS-TGT-ID"VEHICLES ""EMBASSIES OF THE PRC ""EMBASSIES OF THE PRC AND THE SOVIET UNION "9.
PHYS-TGT-NUM 410 .
PH-TGT-TYPE TRANSPORT VEHICLE : "VEHICLES "DIPLOMAT OFFICE OR RESIDENCE : "EMBASSIES OF THE PRC "DIPLOMAT OFFICE OR RESIDENCE : "EMBASSIES OF THE PRC AND THE SOVIET UNION "11.
HUM-TGT-ID"SOVIET MARINES "12.
HUM-TGT-NUM 1513.
HM-TGT-TYPE ACTIVE MILITARY : "SOVIET MARINES "14.
TARGET-NATPEOPLES REP OF CHINA : "EMBASSIES OF THE PRC"USSR : "EMBASSIES OF THE PRC AND THE SOVIET UNION "USSR : "SOVIET MARINES "15.
INST-TYPE*16.
LOCATIONPERU : SAN ISIDRO (NEIGHBORHOOD) : ORRANTIA (DISTRICT )17.
PHYS-EFFECT SOME DAMAGE : "VEHICLES "SOME DAMAGE : "EMBASSIES OF THE PRC AND THE SOVIET UNION "SOME DAMAGE : "EMBASSIES OF THE PRC "18.
HUM-EFFECTNO INJURY : "SOVIET MARINES"148O .
MESSAGE-ID TST1-MUC3-009 91 .
TEMPLATE-ID 22 .
INCIDT-DATE 24 OCT 893 .
INCIDT-TYPE ARSON4 .
CATEGORY TERRORIST ACT5 .
INDIV-PERP S6.ORG-PERPS "SHINING PATH"7 .
PERP-CONF REPORTED AS FACT : "SHINING PATH "8 .
PHYS-TGT-ID "BUSES "9 .
PHYS-TGT-NUM 1 010 .
PH-TGT-TYPE TRANSPORT VEHICLE : "BUSES "11 .
HUM-TGT-I D12 .
HUM-TGT-NUM13 .
HM-TGT-TYP E14 .
TARGET-NAT15 .
INST-TYPE *16 .
LOCATION PERU : LIMA (CITY )17 .
PHYS-EFFECT SOME DAMAGE : "BUSES "18 .
HUM-EFFECTThe failure to recognize that San Isidro and Orrantia are distinct locations caused the program to combin ethe two bombings into one template (under the mistaken assumption that Orrantia is in San Isidro) .
We donot know now why the program did not fill in the City name "Lima", although this would not have affecte dthe score .
As a result of the location assumption, the Toolset got two extra fills for slots 8 and 10 in th efirst template (effectively by merging the templates), missed slot 9 entirely (because the number of target sis different), and got one extra fill in slot 14, in addition to partial credit for the location .The program correctly discarded the bombing of the bus in 1989 but failed to group the 15 wounde dSoviet marines correctly with that event (because of a simple bug which caused the deletion of the earlie revent before the wounding effect was processed), thus losing points also in Template 1, slots 11, 12, 13, 14 ,and 18 (even though slot 18 was correct except for the cross-reference) .The program got the correct date from tonight, October 25 .
The answer key has a range, October 24-25 .The second template produced by the Toolset was completely correct, but the score for this message is51% precision and 61% recall, mainly due to the combining of two possible templates into one .SUMMARY AND CONCLUSIO NMUC-3 is a very difficult task, involving a combination of language interpretation, conceptual and domainknowledge, along with many rules and strategies fo l. template filling .
The examples given here show not onl yhow our system performs this, but hopefully some of the limitations of the system and the penalties paid i nthe scoring for these mistakes .
While it is very difficult to attribute effects in the score to particular functionsof the programs, there is no question that the task adequately exercises most of the current features of oursystem.
It is equally clear that there is ample room for improvements from promising research areas, suchas implicit event reference, discourse processing and representation, and general reference, as well as fromtask-specific processing and more well-known problems such as general inference .References[1] Paul S .
Jacobs, George R. Krupka, and Lisa F .
Rau .
Lexico-semantic pattern matching as a companio nto parsing in text understanding .
In Fourth DARPA Speech and Natural Language Workshop, San Mateo ,CA, February 1991 .
Morgan-Kaufmann .
[2] Paul Jacobs and Lisa Rau .
SCISOR: Extracting information from on-line news .
Communications of th eAssociation for Computing Machinery, 33(11) :88-97, November 1990 .
[3] P. Jacobs .
TRUMP: A transportable language understanding program .
International Journal of Intelli-gent Systems, 6(4), 1991 .149
