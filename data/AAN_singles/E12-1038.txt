Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 377?386,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsInferring Selectional Preferences from Part-Of-Speech N-gramsHyeju Jang and Jack MostowProject LISTEN (www.cs.cmu.edu/~listen), School of Computer ScienceCarnegie Mellon UniversityPittsburgh, PA 15213, USAhyejuj@cs.cmu.edu, mostow@cs.cmu.eduAbstractWe present the PONG method to computeselectional preferences using part-of-speech(POS) N-grams.
From a corpus labeled withgrammatical dependencies, PONG learns thedistribution of word relations for each POSN-gram.
From the much larger but unlabeledGoogle N-grams corpus, PONG learns thedistribution of POS N-grams for a given pairof words.
We derive the probability that oneword has a given grammatical relation to theother.
PONG estimates this probability bycombining both distributions, whether or noteither word occurs in the labeled corpus.PONG achieves higher average precision on16 relations than a state-of-the-art baseline ina pseudo-disambiguation task, but lowercoverage and recall.1 IntroductionSelectional preferences specify plausible fillersfor the arguments of a predicate, e.g., celebrate.Can you celebrate a birthday?
Sure.
Can youcelebrate a pencil?
Arguably yes:  Today theAcme Pencil Factory celebrated its one-billionthpencil.
However, such a contrived example isunnatural because unlike birthday, pencil lacks astrong association with celebrate.
How can wecompute the degree to which birthday or pencilis a plausible and typical object of celebrate?Formally, we are interested in computing theprobability Pr(r | t, R), where (as Table 1specifies), t is a target word such as celebrate, ris a word possibly related to it, such as birthdayor pencil, and R is a possible relation betweenthem, whether a semantic role such as the agentof an action, or a grammatical dependency suchas the object of a verb.
We call t the ?target?because originally it referred to a vocabularyword targeted for instruction, and r its ?relative.
?Notation DescriptionR a relation between wordst a target wordr, r' possible relatives of tg a word N-gramgi and gj ith and jth words of gp the POS N-gram of gTable 1:  Notation used throughout this paperPrevious work on selectional preferences hasused them primarily for natural language analytictasks such as word sense disambiguation (Resnik,1997),  dependency parsing (Zhou et al 2011),and semantic role labeling (Gildea and Jurafsky,2002).
However, selectional preferences canalso apply to natural language generation taskssuch as sentence generation and questiongeneration.
For generation tasks, choosing theright word to express a specified argument of arelation requires knowing its connotations ?
thatis, its selectional preferences.
Therefore, it isuseful to know selectional preferences for manydifferent relations.
Such knowledge could havemany uses.
In education, they could help teachword connotations.
In machine learning theycould help computers learn languages.
Inmachine translation, they could help generatemore natural wording.This paper introduces a method named PONG(for Part-Of-Speech N-Grams) to computeselectional preferences for many differentrelations by combining part-of-speechinformation and Google N-grams.
PONGachieves higher precision on a pseudo-377disambiguation task than the best previous model(Erk et al 2010), but lower coverage.The paper is organized as follows.
Section 2describes the relations for which we computeselectional preferences.
Section 3 describesPONG.
Section 4 evaluates PONG.
Section 5relates PONG to prior work.
Section 6 concludes.2 Relations UsedSelectional preferences characterize constraintson the arguments of predicates.
Selectionalpreferences for semantic roles (such as agent andpatient) are generally more informative than forgrammatical dependencies (such as subject andobject).
For example, consider thesesemantically equivalent but grammaticallydistinct sentences:Pat opened the door.The door was opened by Pat.In both sentences the agent of opened, namelyPat, must be capable of opening something ?
aninformative constraint on Pat.
In contrast,knowing that the grammatical subject of openedis Pat in the first sentence and the door in thesecond sentence tells us only that they are nouns.Despite this limitation, selectional preferencesfor grammatical dependencies are still useful, fora number of reasons.
First, in practice theyapproximate semantic role labels.
For instance,typically the grammatical subject of opened is itsagent.
Second, grammatical dependencies can beextracted by parsers, which tend to be moreaccurate than current semantic role labelers.Third, the number of different grammaticaldependencies is large enough to capture diverserelations, but not so large as to have sparse datafor individual relations.
Thus in this paper, weuse grammatical dependencies as relations.A parse tree determines the basic grammaticaldependencies between the words in a sentence.For instance, in the parse of Pat opened the door,the verb opened has Pat as its subject and dooras its object, and door has the as its determiner.Besides these basic dependencies, we use twoadditional types of dependencies.Composing two basic dependencies yields acollapsed dependency (de Marneffe and Manning,2008).
For example, consider this sentence:The airplane flies in the sky.Here sky is the prepositional object of in, whichis the head of a prepositional phrase attached toflies.
Composing these two dependencies yieldsthe collapsed dependency prep_in between fliesand sky, which captures an important semanticrelation between these two content words:  sky isthe location where flies occurs.
Other functionwords yield different collapsed dependencies.For example, consider these two sentences:The airplane flies over the ocean.The airplane flies and lands.Collapsed dependencies for the first sentenceinclude prep_over between flies and ocean,which characterizes their relative verticalposition, and conj_and between flies and lands,which links two actions that an airplane canperform.
As these examples illustrate, collapsingdependencies involving prepositions andconjunctions can yield informative dependenciesbetween content words.Besides collapsed dependencies, PONG infersinverse dependencies.
Inverse selectionalpreferences are selectional preferences ofarguments for their predicates, such as apreference of a subject or object for its verb.They capture semantic regularities such as the setof verbs that an agent can perform, which tend tooutnumber the possible agents for a verb (Erk etal., 2010).3 MethodTo compute selectional preferences, PONGcombines information from a limited corpuslabeled with the grammatical dependenciesdescribed in Section 2, and a much largerunlabeled corpus.
The key idea is to abstractword sequences labeled with grammaticalrelations into POS N-grams, in order to learn amapping from POS N-grams to those relations.For instance, PONG abstracts the parsedsentence Pat opened the door as NN VB DT NN,with the first and last NN as the subject andobject of the VB.
To estimate the distribution ofPOS N-grams containing particular target andrelative words, PONG POS-tags Google N-grams (Franz and Brants, 2006).Section 3.1 derives PONG?s probabilisticmodel for combining information from labeledand unlabeled corpora.
Section 3.2 and Section3.3 describe how PONG estimates probabilitiesfrom each corpus.
Section 3.4 discusses asparseness problem revealed during probabilityestimation, and how we address it in PONG.3.1 Probabilistic modelWe quantify the selectional preference for arelative r to instantiate a relation R of a target t asthe probability Pr(r | t, R), estimated as follows.By the definition of conditional probability:378Pr( , , )Pr( | , ) Pr( , )r t Rr t R t RWe care only about the relative probability ofdifferent r for fixed t and R, so we rewrite it as:Pr( , , )r t RWe use the chain rule:Pr( | , ) Pr( | ) Pr( )R r t r t tand notice that t is held constant:Pr( | , ) Pr( | )R r t r tWe estimate the second factor as follows:Pr( , ) freq( , )Pr( | ) Pr( ) freq( )t r t rr t t tWe calculate the denominator freq(t) as thenumber of  N-grams in the Google N-gramcorpus that contain t, and the numerator freq(t, r)as the number of N-grams containing both t and r.To estimate the factor Pr(R | r, t) directly froma corpus of text labeled with grammaticalrelations, it would be trivial to count how often aword r bears relation R to target word t.However, the results would be limited to thewords in the corpus, and many relationfrequencies would be estimated sparsely ormissing altogether; t or r might not even occur.Instead, we abstract each word in the corpus asits part-of-speech (POS) label.
Thus we abstractThe big boy ate meat as DT JJ NN VB NN.
Wecall this sequence of POS tags a POS N-gram.We use POS N-grams to predict word relations.For instance, we predict that in any wordsequence with this POS N-gram, the JJ willmodify (amod) the first NN, and the second NNwill be the direct object (dobj) of the VB.This prediction is not 100% reliable.
Forexample, the initial 5-gram of The big boy atemeat pie has the same POS 5-gram as before.However, the dobj of its VB (ate) is not thesecond NN (meat), but the subsequent NN (pie).Thus POS N-grams predict word relations onlyin a probabilistic sense.To transform Pr(R | r, t) into a form we canestimate, we first apply the definition ofconditional probability:Pr( , , )Pr( | , ) Pr( , )R t rR t r t rTo estimate the numerator Pr(R, t, r), we firstmarginalize over the POS N-gram p:Pr( , , , )  Pr( , )pR t r pt rWe expand the numerator using the chain rule:Pr( | , , ) Pr( | , ) Pr( , )Pr( , )pR t r p p t r t rt rCancelling the common factor yields:Pr( | , , ) Pr( | , )pR p t r p t rWe approximate the first term Pr(R | p, t, r) asPr(R | p), based on the simplifying assumptionthat R is conditionally independent of t and r,given p.  In other words, we assume that given aPOS N-gram, the target and relative words t andr give no additional information about theprobability of a relation.
However, theirrespective positions i and j in the POS N-gram pmatter, so we condition the probability on them:Pr( | , , ) Pr( | , , )R p t r R p i jSumming over their possible positions, we getPr( | , )Pr( | , , ) Pr( | , )i jp i jR r tR p i j p t g r gAs Figure 1 shows, we estimate Pr(R | p, i, j) byabstracting the labeled corpus into POS N-grams.We estimate Pr(p | t = gi, r = gj) based on thefrequency of partially lexicalized POS N-gramslike DT JJ:red NN:hat VB NN among Google N-grams with t and r in the specified positions.Sections 3.2 and 3.3 describe how we estimatePr(R | p, i, j) and Pr(p | t = gi, r = gj), respectively.Note that PONG estimates relative rather thanabsolute probabilities.
Therefore it cannot (anddoes not) compare them against a fixed thresholdto make decisions about selectional preferences.3.2 Mapping POS N-grams to relationsTo estimate Pr(R | p, i, j), we use the PennTreebank Wall Street Journal (WSJ) corpus,which is labeled with grammatical relationsusing the Stanford dependency parser (Klein andManning, 2003).To estimate the probability Pr(R | p, i, j) of arelation R between a target at position i and arelative at position j in a POS N-gram p, wecompute what fraction of the word N-grams gwith POS N-gram p have relation R betweensome target t and relative r at positions i and j:Pr( | , , )freq( .
.POS( ) relation( , ) )freq( .
.POS( ) relation( , ))i ji jR p i jg s t g p g g Rg s t g p g g3.3 Estimating POS N-gram distributionsGiven a target and relative, we need to estimatetheir distribution of POS N-grams and positions.379Figure 1:  Overview of PONG.From the labeled corpus, PONG extracts abstract mappings from POS N-grams to relations.From the unlabeled corpus, PONG estimates POS N-gram probability given a target and relative.A labeled corpus is too sparse for this purpose,so we use the much larger unlabeled Google N-grams corpus (Franz and Brants, 2006).The probability that an N-gram with target t atposition i and relative r at position j will have thePOS N-gram p is:Pr( | , )freq( .
.POS( ) , , ))freq( .
.
)i ji ji jp t g r gg s t g p g t g rg s t g t g rTo compute this ratio, we first use a well-indexed table to efficiently retrieve all N-gramswith words t and r at the specified positions.
Wethen obtain their POS N-grams from the StanfordPOS tagger (Toutanova et al 2003), and counthow many of them have the POS N-gram p.3.4 Reducing POS N-gram sparsenessWe abstract word N-grams into POS N-grams toaddress the sparseness of the labeled corpus, buteven the POS N-grams can be sparse.
For n=5,the rarer ones occur too sparsely (if at all) in ourlabeled corpus to estimate their frequency.To address this issue, we use a coarser POStag set than the Penn Treebank POS tag set.
AsTable 2 shows, we merge tags for adjectives,nouns, adverbs, and verbs into four coarser tags.Coarse OriginalADJ JJ, JJR, JJSADVERB RB, RBR, RBSNOUN NN, NNS, NNP, NNPSVERB VB, VBD, VBG, VBN, VBP, VBZTable 2:  Coarser POS tag set used in PONGTo gauge the impact of the coarser POS tags,we calculated Pr(r | t, R) for 76 test instancesused in an earlier unpublished study by Liu Liu,a former Project LISTEN graduate student.
Eachinstance consists of two randomly chosen wordsin the WSJ corpus labeled with a grammaticalrelation.
Coarse POS tags increased coverage ofthis pilot set ?
that is, the fraction of instances forwhich PONG computes a probability ?
from 69%to 92%.Using the universal tag set (Petrov et al 2011)as an even coarser tag set is an interesting futuredirection, especially for other languages.
Itssmaller size (12 tags vs. our 23) should reducedata sparseness, but increase the risk of over-generalization.4 EvaluationTo evaluate PONG, we use a standard pseudo-disambiguation task, detailed in Section 4.1.Section 4.2 describes our test set.
Section 4.3lists the metrics we evaluate on this test set.Section 4.4 describes the baselines we comparePONG against on these metrics, and Section 4.5describes the relations we compare them on.Section 4.6 reports our results.
Section 4.7analyzes sources of error.4.1 Evaluation taskThe pseudo-disambiguation task (Gale et al1992; Schutze, 1992) is as follows:  given atarget word t, a relation R, a relative r, and arandom distracter r', prefer either r or r',whichever is likelier to have relation R to word t.This evaluation does not use a threshold:  justprefer whichever word is likelier according to themodel being evaluated.
If the model assigns onlyone of the words a probability, prefer it, based onthe assumption that the unknown probability ofthe other word is lower.
If the model assigns thesame probability to both words, or no probabilityto either word, do not prefer either word.3804.2 Test setAs a source of evaluation data, we used theBritish National Corpus (BNC).
As a commontest corpus for all the methods we evaluated, weselected one half of BNC by sorting filenamesalphabetically and using the odd-numbered files.We used the other half of BNC as a trainingcorpus for the baseline methods we comparedPONG to.A test set for the pseudo-disambiguation tasktask consists of tuples of the form (R, t, r, r').
Toconstruct a test set, we adapted the process usedby Rooth et al(1999) and Erk et al(2010).First, we chose 100 (R, t) pairs for eachrelation R at random from the test corpus.
Roothet al(1999) and Erk et al(2010) chose suchpairs from a training corpus to ensure that itcontained the target t.  In contrast, choosing pairsfrom an unseen test corpus includes target wordswhether or not they occur in the training corpus.To obtain a sample stratified by frequency,rather than skewed heavily toward high-frequency pairs, Erk et al(2010) drew (R, t)pairs from each of five frequency bands in theentire British National Corpus (BNC):  50-100occurrences; 101-200; 201-500; 500-1000; andmore than 1000.
However, we use only half ofBNC as our test corpus, so to obtain acomparable test set, we drew 20 (R, t) pairs fromeach of the corresponding frequency bands inthat half:  26-50 occurrences; 51-100; 101-250;251-500; and more than 500.For each chosen (R, t) pair, we drew a separate(R, t, r) triple from each of six frequency bands:1-25 occurrences; 26-50; 51-100; 101-250; 251-500; and more than 500.
We necessarily omittedfrequency bands that contained no such triples.We filtered out triples where r did not have themost frequent part of speech for the relation R.For example, this filter would exclude the triple(dobj, celebrate, the) because a direct object ismost frequently a noun, but the is a determiner.Then, like Erk et al(2010), we paired therelative r in each (R, t, r) triple with a distracter r'with the same (most frequent) part of speech asthe relative r, yielding the test tuple (R, t, r, r').Rooth et al(1999) restricted distractercandidates to words with between 30 and 3,000occurrences in BNC; accordingly, we chose onlydistracters with between 15 and 1,500occurrences in our test corpus.
We selected r'from these candidates randomly, with probabilityproportional to their frequency in the test corpus.Like Rooth et al(1999), we excluded asdistracters any actual relatives, i.e.
candidates r'where the test corpus contained the triple (R, t, r').Table 3 shows the resulting number of (R, t, r, r')test tuples for each relation.Relation R # tuples for R # tuples for RTadvmod 121 131amod 162 128conj_and 155 151dobj 145 167nn 173 158nsubj  97 124prep_of 144 153xcomp 139 140Table 3:  Test set size for each relation4.3 MetricsWe report four evaluation metrics:  precision,coverage, recall, and F-score.
Precision (called?accuracy?
in some papers on selectionalpreferences) is the percentage of all coveredtuples where the original relative r is preferred.Coverage is the percentage of tuples for whichthe model prefers r to r' or vice versa.
Recall isthe percentage of all tuples where the originalrelative is preferred, i.e., precision timescoverage.
F-score is the harmonic mean ofprecision and recall.4.4 BaselinesWe compare PONG to two baseline methods.EPP is a state-of-the-art model for which Erket al(2010) reported better performance thanboth Resnik?s (1996) WordNet model andRooth?s (1999) EM clustering model.
EPPcomputes selectional preferences usingdistributional similarity, based on the assumptionthat relatives are likely to appear in the samecontexts as relatives seen in the training corpus.EPP computes the similarity of a potentialrelative?s vector space representation to relativesin the training corpus.EPP has various options for its vector spacerepresentation, similarity measure, weightingscheme, generalization space, and whether to usePCA.
In re-implementing EPP, we chose theoptions that performed best according to Erk et al(2010), with one exception.
To save work, wechose not to use PCA, which Erk et al(2010)described as performing only slightly better inthe dependency-based space.381Relation Target Relative Descriptionadvmod verb adverb Adverbial modifieramod noun adjective Adjective modifierconj_and noun noun Conjunction with ?and?dobj verb noun Direct objectnn noun noun Noun compound modifiernsubj verb noun Nominal subjectprep_of noun noun Prepositional modifierxcomp verb verb Open clausal complementTable 4: Relations tested in the pseudo-disambiguation experiment.Relation names and descriptions are from de Marneffe and Manning (2008) except for prep_of.Target and relative POS are the most frequent POS pairs for the relations in our labeled WSJ corpus.RelationPrecision (%) Coverage (%) Recall (%) F-score (%)PONG EPP DEP PONG EPP DEP PONG EPP DEP PONG EPP DEPadvmod 78.7 - 98.6 72.1 - 69.2 56.7 - 68.3 65.9 - 80.7advmodT 89.0 71.0 97.4 69.5 100 59.5 61.8 71.0 58.0 73.0 71.0 72.7amod 78.8 - 99.0 90.1 - 61.1 71.0 - 60.5 74.7 - 75.1amodT 84.1 74.0 97.3 83.6 99.2 57.0 70.3 73.4 55.5 76.6 73.7 70.6conj_and 77.2 74.2 100 73.6 100 52.3 56.8 74.2 52.3 65.4 74.2 68.6conj_andT 80.5 70.2 97.3 74.8 100 49.7 60.3 70.2 48.3 68.9 70.2 64.6dobj 87.2 80.0 97.7 80.7 100 60.0 70.3 80.0 58.6 77.9 80.0 73.3dobjT 89.6 80.2 98.1 92.2 100 64.1 82.6 80.2 62.9 86.0 80.2 76.6nn 86.7 73.8 97.2 95.3 99.4 63.0 82.7 73.4 61.3 84.6 73.6 75.2nnT 83.8 79.7 99.0 93.7 100 60.8 78.5 79.7 60.1 81.0 79.7 74.8nsubj 76.1 77.3 100 69.1 100 42.3 52.6 77.3 42.3 62.2 77.3 59.4nsubjT 78.5 66.9 95.0 86.3 100 48.4 67.7 66.9 46.0 72.7 66.9 62.0prep_of 88.4 77.8 98.4 84.0 100 44.4 74.3 77.8 43.8 80.3 77.8 60.6prep_ofT 79.2 76.5 97.4 81.7 100 50.3 64.7 76.5 49.0 71.2 76.5 65.2xcomp 84.0 61.9 95.3 85.6 100 61.2 71.9 61.9 58.3 77.5 61.9 72.3xcompT 86.4 78.6 98.9 89.3 100 63.6 77.1 78.6 62.9 81.5 78.6 76.9average 83.0 74.4 97.9 82.6 99.9 56.7 68.7 74.4 55.5 75.0 74.4 70.5Table 5:  Coverage, Precision, Recall, and F-score for various relations; RT is the inverse of relation R.PONG uses POS N-grams, EPP uses distributional similarity, and DEP uses dependency parses.To score a potential relative r0, EPP uses thisformula:,, 0 0arg ( , ) ,( )( ) ( , )R tR tr Seen s R t R twt rSelpref r sim r rZHere sim(r0, r) is the nGCM similarity definedbelow between vector space representations of r0and a relative r seen in the training data:2121'( , ') exp( ( ) )'i iinb bnGCMinbia asim a aa awhere a aThe weight function wtr,t(a) is analogous toinverse document frequency in InformationRetrieval.DEP, our second baseline method, runs theStanford dependency parser to label the trainingcorpus with grammatical relations, and uses theirfrequencies to predict selectional preferences.To do the pseudo-disambiguation task, DEPcompares the frequencies of (R, t, r) and (R, t, r').4.5 Relations testedTo test PONG, EPP, and DEP, we chose themost frequent eight relations between contentwords in the WSJ corpus, which occur over10,000 times and are described in Table 4.
Wealso tested their inverse relations.
However, EPPdoes not compute selectional preferences foradjective and adverb as relatives.
For this reason,we did not test EPP on advmod and amodrelations with adverbs and adjectives as relatives.3824.6 Experimental resultsTable 5 displays results for all 16 relations.
Tocompute statistical significance conservatively incomparing methods, we used paired t-tests withN = 16 relations.PONG?s precision was significantly betterthan EPP (p<0.001) but worse than DEP(p<0.0001).
Still, PONG?s high precisionvalidates its underlying assumption that POS N-grams strongly predict grammaticaldependencies.On coverage and recall, EPP beat PONG,which beat DEP (p<0.0001).
PONG?s F-scorewas higher, but not significantly, than EPP?s(p>0.5) or DEP?s (p>0.02).4.7 Error analysisIn the pseudo-disambiguation task of choosingwhich of two words is related to a target, PONGmakes errors of coverage (preferring neitherword) and precision (preferring the wrong word).Coverage errors, which occurred 17.4% of thetime on average, arose only when PONG failedto estimate a probability for either word.
PONGfails to score a potential relative r of a target twith a specified relation R if the labeled corpushas no POS N-grams that (a) map to R, (b)contain the POS of t and r, and (c) match Googleword N-grams with t and r at those positions.Every relation has at least one POS N-gram thatmaps to it, so condition (a) never fails.
PONGuses the most frequent POS of t and r, and webelieve that condition (b) never fails.
However,condition (c) can and does fail when t and r donot co-occur in any Google N-grams, at least thatmatch a POS N-gram that can map to relation R.For example, oversee and diet do not co-occur inany Google N-grams, so PONG cannot score dietas a potential dobj of oversee.Precision errors, which occur 17% of the timeon average, arose when (a) PONG scored thedistracter but failed to score the true relative, or(b) scored them both but preferred the distracter.Case (a) accounted for 44.62% of the errors onthe covered test tuples.One likely cause of errors in case (b) is over-generalization when PONG abstracts a word N-gram labeled with a relation by mapping its POSN-gram to that relation.
In particular, the coarsePOS tag set may discard too much information.Another likely cause of errors is probabilitiesestimated poorly due to sparse data.
Theprobability of a relation for a POS N-gram rare inthe training corpus is likely to be inaccurate.
Sois the probability of a POS N-gram for rare co-occurrences of a target and relative in Googleword N-grams.
Using a smaller tag set mayreduce the sparse data problem but increase therisk of over-generalization.5 Relation to Prior WorkIn predicting selectional preferences, a keyissue is generalization.
Our DEP baseline simplycounts co-occurrences of target and relativewords in a corpus to predict selectionalpreferences, but only for words seen in thecorpus.
Prior work, summarized inTable 6, has therefore tried to infer the similarityof unseen relatives to seen relatives.
To illustrate,consider the problem of inducing that the directobjects of celebrate tend to be days or events.Resnik (1996) combined WordNet with alabeled corpus to model the probability thatrelatives of a predicate belong to a particularconceptual class.
This method could notice, forexample, that the direct objects of celebrate tendto belong to the conceptual class event.
Thus itcould prefer anniversary or occasion as theobject of celebrate even if unseen in its trainingcorpus.
However, this method depends stronglyon the WordNet taxonomy.Rather than use linguistic resources such asWordNet, Rooth et al(1999) and Wald et al(2008) induced semantically annotatedsubcategorization frames from unlabeled corpora.They modeled semantic classes as hiddenvariables, which they estimated using EM-basedclustering.
Ritter (2010) computed selectionalpreferences by using unsupervised topic modelssuch as LinkLDA, which infers semantic classesof words automatically instead of requiring a pre-defined set of classes as input.The contexts in which a linguistic unit occursprovide information about its meaning.
Erk(2007) and Erk et al(2010) modeled thecontexts of a word as the distribution of wordsthat co-occur with it.
They calculated thesemantic similarity of two words as the similarityof their context distributions according to variousmeasures.
Erk et al(2010) reported the state-of-the-art method we used as our EPP baseline.In contrast to prior work that explored varioussolutions to the generalization problem, we don?tso much solve this problem as circumvent it.Instead of generalizing from a training corpusdirectly to unseen words, PONG abstracts a wordN-gram to a POS N-gram and maps it to therelations that the word N-gram is labeled with.383Table 6:  Comparison with prior methods to compute selectional preferencesTo compute selectional preferences, whether thewords are in the training corpus or not, PONGapplies these abstract mappings to word N-gramsin the much larger Google N-grams corpus.Some prior work on selectional preferenceshas used POS N-grams and a large unlabeledcorpus.
The most closely related work we foundwas by Gormley et al(2011).
They usedpatterns in POS N-grams to generate test data fortheir selectional preferences model, but not toinfer preferences.
Zhou et al(2011) identifiedselectional preferences of one word for anotherReference Relation totargetLexicalresourcePrimary  corpus(labeled) &informationusedGeneralizationcorpus(unlabeled) &information usedMethodResnik,1996Verb-objectVerb-subjectAdjective-nounModifier-headHead-modifierSenses inWordNetnountaxonomyTarget, relative,and relation in aparsed, partiallysense-taggedcorpus (Browncorpus)none InformationtheoreticmodelRooth etal., 1999Verb-objectVerb-subjectnone Target, relative,and relation in aparsed corpus(parsed BNC)none EM-basedclusteringRitter,2010Verb-subjectVerb-objectSubject-verb-objectnone Subject-verb-object tuplesfrom 500 millionweb-pagesnone LDA modelErk, 2007 Predicate andSemantic rolesnone Target, relative,and relation in asemantic rolelabeled corpus(FrameNet)Words and theirrelations in aparsed corpus(BNC)Similaritymodel basedon word co-occurrenceErk et al2010SYN option:Verb-subjectVerb-object, andtheir inverserelationsSEM option:verb andsemantic rolesthat have nounsas their headwordin a primarycorpus, and theirinverse relationsnone Target, relative,and relation inSYN   option:  aparsed corpus(parsed BNC)SEM   option:  asemantic rolelabeled corpus(FrameNet)Two options:WORDSPACE:an unlabeledcorpus (BNC)DEPSPACE:Words and theirsubject and objectrelations in aparsed corpus(parsed BNC)Similaritymodel usingvector spacerepresentationof wordsZhou etal., 2011Any (relationsnot distinguished)none Counts of wordsin Web orGoogle N-gramnone PMI(PointwiseMutualInformation)This paper All grammaticaldependencies in aparsed corpus,and their inverserelationsnone POS N-gramdistribution forrelations inparsed WSJcorpusPOS N-gramdistribution fortarget and relativein Google N-gramCombine bothPOS N-gramdistributions384by using Pointwise Mutual Information (PMI)(Fano, 1961) to check whether they co-occurmore frequently in a large corpus than predictedby their unigram frequencies.
However, theirmethod did not distinguish among differentrelations.6 ConclusionThis paper describes, derives, and evaluatesPONG, a novel probabilistic model of selectionalpreferences.
PONG uses a labeled corpus to mapPOS N-grams to grammatical relations.
Itcombines this mapping with probabilitiesestimated from a much larger POS-tagged butunlabeled Google N-grams corpus.We tested PONG on the eight most commonrelations in the WSJ corpus, and their inverses ?more relations than evaluated in prior work.Compared to the state-of-the-art EPP baseline(Erk et al 2010), PONG averaged higherprecision but lower coverage and recall.Compared to the DEP baseline, PONG averagedlower precision but higher coverage and recall.All these differences were substantial (p < 0.001).Compared to both baselines, PONG?s average F-score was higher, though not significantly.Some directions for future work include:  First,improve PONG by incorporating models oflexical similarity explored in prior work.
Second,use the universal tag set to extend PONG to otherlanguages, or to perform better in English.
Third,in place of grammatical relations, use rich,diverse semantic roles, while avoiding sparsity.Finally, use selectional preferences to teach wordconnotations by using various relations togenerate example sentences or useful questions.AcknowledgmentsThe research reported here was supported by theInstitute of Education Sciences, U.S. Departmentof Education, through Grant R305A080157.
Theopinions expressed are those of the authors anddo not necessarily represent the views of theInstitute or the U.S. Department of Education.We thank the helpful reviewers and Katrin Erkfor her generous assistance.Referencesde Marneffe, M.-C. and Manning, C.D.
2008.Stanford Typed Dependencies Manual.http://nlp.stanford.edu/software/dependencies_manual.pdf, Stanford University, Stanford, CA.Erk, K. 2007.
A Simple, Similarity-Based Model forSelectional Preferences.
In Proceedings of the 45thAnnual Meeting of the Association ofComputational Linguistics, Prague, CzechRepublic, June, 2007, 216-223.Erk, K., Pad?, S. and Pad?, U.
2010.
A Flexible,Corpus-Driven Model of Regular and InverseSelectional Preferences.
Computational Linguistics36(4), 723-763.Fano, R. 1961.
Transmission  O F   Information:  AStatistical  Theory  of  Communications.
MITPress, Cambridge, MA.Franz, A. and Brants, T. 2006.
All Our N-Gram AreBelong to You.Gale, W.A., Church, K.W.
and Yarowsky, D. 1992.Work on Statistical Methods for Word SenseDisambiguation.
In Proceedings of the AAAI FallSymposium on Probabilistic Approaches to NaturalLanguage, Cambridge, MA, October 23?25, 1992,54-60.Gildea, D. and Jurafsky, D. 2002.
Automatic Labelingof Semantic Roles.
Computational Linguistics28(3), 245-288.Gormley, M.R., Dredze, M., Durme, B.V. and Eisner,J.
2011.
Shared Components Topic Models withApplication to Selectional Preference, NIPSWorkshop on Learning Semantics Sierra Nevada,Spain.im Walde, S.S., Hying, C., Scheible, C. and Schmid,H.
2008.
Combining Em Training and the MdlPrinciple for an Automatic Verb ClassificationIncorporating Selectional Preferences.
InProceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics,Columbus, OH,  2008, 496-504.Klein, D. and Manning, C.D.
2003.
AccurateUnlexicalized Parsing.
In Proceedings of the 41stAnnual Meeting of the Association forComputational Linguistics, Sapporo, Japan, July 7-12, 2003, E.W.
HINRICHS and D. ROTH, Eds.Petrov, S., Das, D. and McDonald, R.T. 2011.
AUniversal Part-of-Speech Tagset.
ArXiv1104.2086.Resnik, P. 1996.
Selectional Constraints: AnInformation-Theoretic Model and ItsComputational Realization.
Cognition 61, 127-159.Resnik, P. 1997.
Selectional Preference and SenseDisambiguation.
In ACL SIGLEX Workshop on385Tagging Text with Lexical Semantics: Why, What,and How, Washington, DC, April 4-5, 1997, 52-57.Ritter, A., Mausam and Etzioni, O.
2010.
A LatentDirichlet Allocation Method for SelectionalPreferences.
In Proceedings of the 48th AnnualMeeting of the Association for ComputationalLinguistics, Uppsala, Sweden,  2010, 424-434.Rooth, M., Riezler, S., Prescher, D., Carroll, G. andBeil, F. 1999.
Inducing a Semantically AnnotatedLexicon Via Em-Based Clustering.
In Proceedingsof the 37th Annual Meeting of the Association forComputational Linguistics on ComputationalLinguistics, College Park, MD,  1999, Associationfor Computational Linguistics, 104-111.Schutze, H. 1992.
Context Space.
In Proceedings ofthe AAAI Fall Symposium on IntelligentProbabilistic Approaches to Natural Language,Cambridge, MA,  1992, 113-120.Toutanova, K., Klein, D., Manning, C. and Singer, Y.2003.
Feature-Rich Part-of-Speech Tagging with aCyclic Dependency Network.
In Proceedings of theHuman Language Technology Conference andAnnual Meeting of the North American Chapter ofthe Association for Computational Linguistics(HLT-NAACL), Edmonton, Canada,  2003, 252?259.Zhou, G., Zhao, J., Liu, K. and Cai, L. 2011.Exploiting Web-Derived Selectional Preference toImprove Statistical Dependency Parsing.
InProceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics,Portland, OR,  2011, 1556?1565.386
