Statistical QA - Classifier vs. Re-ranker: What?s the difference?Deepak Ravichandran, Eduard Hovy, and Franz Josef OchInformation Sciences InstituteUniversity of Southern California4676 Admiralty WayMarina del Rey, CA 90292{ravichan,hovy,och}@isi.eduAbstractIn this paper, we show that we can ob-tain a good baseline performance forQuestion Answering (QA) by usingonly 4 simple features.
Using these fea-tures, we contrast two approaches usedfor a Maximum Entropy based QA sys-tem.
We view the QA problem as aclassification problem and as a re-ranking problem.
Our results indicatethat the QA system viewed as a re-ranker clearly outperforms the QA sys-tem used as a classifier.
Both systemsare trained using the same data.1 IntroductionOpen-Domain factoid Question Answering (QA)is defined as the task of answering fact-basedquestions phrased in Natural Language.
Exam-ples of some question and answers that fall inthe fact-based category are:1.
What is the capital of Japan?
- Tokyo2.
What is acetaminophen?
- Non-aspirin pain killer3.
Where is the Eiffel Tower?
- ParisThe architecture of most of QA systems consistsof two basic modules: the information retrieval(IR) module and the answer pinpointing module.These two modules are used in a typical pipelinearchitecture.For a given question, the IR module finds aset of relevant segments.
Each segment typicallyconsists of at most R sentences1 .
The answerpinpointing module processes each of these seg-ments and finds the appropriate answer phrase.1In our experiments we use R=1phrase.
Evaluation of a QA system is judged onthe basis on the final output answer and the cor-responding evidence provided by the segment.This paper focuses on the answer pinpointingmodule.
Typical QA systems perform re-rankingof candidate answers as an important step inpinpointing.
The goal is to rank the most likelyanswer first by using either symbolic or statisti-cal methods.
Some QA systems make use ofstatistical answer pinpointing (Xu et.
al, 2002;Ittycheriah, 2001; Ittycheriah and Salim, 2002)by treating it as a classification problem.
In thispaper, we cast the pinpointing problem in a sta-tistical framework and compare two approaches,classification and re-ranking.2 Statistical Answer Pinpointing2.1 Answer ModelingThe answer-pinpointing module gets as input aquestion q and a set of possible answer candi-dates }...{ 21 Aaaa .
It outputs one of the answer}...{ 21 Aaaaa ?
from the candidate answer set.We consider two ways of modeling this prob-lem.One approach is the traditional classificationview (Ittycheriah, 2001) where we present eachQuestion-Answer pair to the classifier whichclassifies it as either correct answer (true) or in-correct answer (false), based on some evidence(features).In this case, we model )},...{,|( 21 qaaaacP A .Here,false}{true,c = signifies the correctnessof the answer a  with respect to the question q .The probability )},...{,|( 21 qaaaacP A  for each QApair is modeled independently of other suchpairs.
Thus, for the same question, many QApairs are presented to the classifier as independ-ent events (histories).
If the training corpus con-tains Q questions with A answers for each ques-tion, the total number of events (histories) wouldbe equal to Q?A with two classes (futures) (cor-rect or incorrect answer) for each event.
Oncethe probabilities )},...{,|( 21 qaaaacP A  have beencomputed, the system has to return the best an-swer.
The following decision rule is used:)]},...{,|([maxarg 21 qaaaatruePa Aa=?Another way of viewing the problem is as are-ranking task.
This is possible because the QAtask requires the identification of only one cor-rect answer, instead of identifying all the correctanswer in the collection.
In this case, we model)},...{|( 21 qaaaaP A .
If the training corpus containsQ questions with A answers for each question,the total number of events (histories) would beequal to Q, with A classes (futures).
This viewrequires the following decision-rule to identifythe answer that seems most promising:)]},...{|([maxarg 21 qaaaaPa Aa=?In summary,Classifier Re-ranker#Events (Histo-ries)Q?A Q#Classes (Futures)per event2 Awhere,Q = total number of questions.A = total number of answer chunks consideredfor each question.2.2 Maximum Entropy formulationWe use Maximum Entropy to model the givenproblem both as a classifier and a re-ranker.
Wedefine M feature functions,Mmqaaaaf Am ,....,1),},...{,( 21 = , that may be usefulin characterizing the task.
Della Pietra et.
al(1995) contains good description of MaximumEntropy models.We model the classifier as follows:?
??===?121?,121,21)]},...{,(exp[)]},...{,(exp[)},...{,|(cMmAmcmMmAmcmAqaaaafqaaaafqaaaacP?
?where,},{;,....,1;,falsetruecMmcm ==?
are the modelparameters.The decision rule for choosing the best an-swer is:])},...{,([maxarg)]},...{,|([maxarg121,21?=?==MmAmtruemaAaqaaaafqaaaatruePa?The above decision rule requires comparison ofdifferent probabilities of theform )},...{,|( 21 qaaaatrueP A .
However, theseprobabilities are modeled as independent events(histories) in the classifier and hence the trainingcriterion does not make them directly compara-ble.For the re-ranker, we model the probabilityas:( )?
??===?12112121)]},...{,?(exp[)]},...{,(exp[},...{|aMmAmmMmAmmAqaaaafqaaaafqaaaaP?
?where,Mmm ,....,1; =?
are the model parameters.Note that for the classifier the model parametersare cm,?
, whereas for the re-ranker they are m?
.This is because for the classifier, each featurefunction has different weights associated witheach class (future).
Hence, the classifier hastwice the model parameters as compared to there-ranker.The decision rule for the re-ranker is given by:.?=?==MmAmmaAaqaaaafqaaaaPa12121)]},...{,([maxarg)]},...
{|([maxarg?The re-ranker makes the probabilities)},...{|( 21 qaaaaP A , considered for the decisionrule, directly comparable against each other, byincorporating them into the training criterionitself.
Table 1 summarizes the differences of thetwo models.2.3 Feature FunctionsUsing above formulation to model the probabil-ity distribution we need to come up with featuresfj.
We use only four basic feature functions forour system.1.
Frequency: It has been observed that thecorrect answer has a higher frequency(Magnini et al; 2002) in the collection ofanswer chunks (C).
Hence we count thenumber of time a potential answer occurs inthe IR output and use its logarithm as a fea-ture.
This is a positive continuous valuedfeature.2.
Expected Answer Class: Most of the currentQA systems employ some type of AnswerClass Identification module.
Thus questionslike ?When did Bill Clinton go to college?
?,would be identified as a question askingabout a time (or a time period), ?Where isthe sea of tranquility??
would be identifiedas a question asking for a location.
If the an-swer class matches the expected answerclass (derived from the question by the an-swer identification module) this feature fires(i.e., it has a value of 1).
Details of this mod-ule are explained in Hovy et al (2002).
Thisis a binary-valued feature.3.
Question Word Absent: Usually a correctanswer sentence contains a few of the ques-tion words.
This feature fires if the candidateanswer does not contain any of the questionwords.
This is also a binary valued feature.4.
Word Match: It is the sum of ITF2 values forthe words in the question that matches iden-tically with the words in the answer sen-tence.
This is a positive continuous valuedfeature.2.4 TrainingWe train our Maximum Entropy model usingGeneralized Iterative scaling (Darroch andRatcliff, 1972) approach by using YASMET3 .3 Evaluation MetricThe performance of the QA system is highlydependent on the performance of the two indi-vidual modules IR and answer-pinpointing.
Thesystem would have excellent performance ifboth have good accuracy.
Hence, we need agood evaluation metric to evaluate each of thesecomponents individually.
One standard metricfor IR is recall and precision.
We can modifythis metric for QA as follows:2ITF = Inverse Term Frequency.
We take a large inde-pendent corpus & estimate ITF(W) =1/(count(W)), whereW = Word.3YASMET.
(Yet Another Small Maximum EntropyToolkit) http://www-i6.informatik.rwth-aachen.de/Colleagues/och/software/YASMET.htmlClassifier Re-RankerModelingEqua-tion ?
??===?121?,121,21)]},...{,(exp[)]},...{,(exp[)},...{,|(cMmAmcmMmAmcmAqaaaafqaaaafqaaaacP???
??===?12112121)]},...{,?(exp[)]},...{,(exp[)},...{|(aMmAmmMmAmmAqaaaafqaaaafqaaaaP??Deci-sionRule])},...{,([maxarg)}},...{,|({maxarg121,21?=?==MmAtruemmaAaqaaaafqaaaatruePa?.?=?==MmAmmaAaqaaaafqaaaaPa12121)]},...{,([maxarg)]},...
{|([maxarg?Table 1 : Model comparison between a Classifier and Re-rankersegmentsanswer relevant  Total #returnedsegment  answer relevant  #Recall =returned segments Total #returned segmentsanswer relevant  #Precision =It is almost impossible to measure recall be-cause the IR collection is typically large and in-volves several hundreds of thousands ofdocuments.
Hence, we evaluate our IR by onlythe precision measure at top N segments.
Thismethod is actually a rather sloppy approximationto the original recall and precision measure.Questions with fewer correct answers in the col-lection would have a lower precision score ascompared to questions with many answers.Similarly, it is unclear how one would evaluateanswer questions with No Answer (NIL) in thecollection using this metric.
All these questionswould have zero precision from the IR collec-tion.The answer-pinpointing module is evaluatedby checking if the answer returned by the systemas the top ranked (#1) answer is correct/incorrectwith respect to the IR collection and the trueanswer.
Hence, if the IR system fails to returneven a single sentence that contains the correctanswer for the given question, we do not penal-ize the answer-pinpointing module.
It is againunclear how to evaluate questions with No an-swer (NIL).
(Here, for our experiments we at-tribute the error to the IR module.
)Finally, the combined system is evaluated byusing the standard technique, wherein the An-swer (ranked #1) returned by the system isjudged to be either correct or incorrect and thenthe average is taken.Question:1395 Who is Tom Cruise married to ?IR Output:1 Tom Cruise is married to actress Nicole Kidman and they have two adopted children .2 Tom Cruise is married to Nicole Kidman ...Output of Chunker: (The number to the left of each chunk records the IR sentence fromwhich that particular chunk came)1  Tom Cruise1  Tom1  Cruise1  is married1  married1  actress Nicole Kidman and they1  actress Nicole Kidman1  actress1  Nicole Kidman1  Nicole1  Kidman1  they1  two adopted children1  two1  adopted1  children2  Tom Cruise2  Tom2  Cruise2  is married2  married2  Nicole Kidman2  Nicole2  Kidman..Figure 1 : Candidate answer extraction for a question.4 Experiments4.1 FrameworkInformation RetrievalFor our experiments, we use the Web searchengine AltaVista.
For every question, we re-move stop-words and present all other questionwords as query to the Web search engine.
Thetop relevant documents are downloaded.
Weapply a sentence segmentor, and identify thosesentences that have high ITF overlapping wordswith the given question.
The sentences are thenre-ranked accordingly and only the top K sen-tences (segments) are presented as output of theIR system.Candidate Answer ExtractionFor a given question, the IR returns top Ksegments.
For our experiments a segment con-sists of one sentence.
We parse each of the sen-tences and obtain a set of chunks, where eachchunk is a node of the parse tree.
Each chunk isviewed as a potential answer.
For our experi-ments we restrict the number of potential an-swers to be at most 5000.
We illustrate thisprocess in Figure 1.Training/Test DataTable 2 : Training size and sources.Training +ValidationTestQuestion collec-tionTREC 9 +TREC 10TREC11Total questions 1192 500We use the TREC 9 and TREC 10 data setsfor training and the TREC 11 data set for testing.We initially apply the IR step as described aboveand obtain a set of at most 5000 answers.
Foreach such answer we use the pattern file sup-plied by NIST to tag answer chunks as eithercorrect (1) or incorrect (0).
This is a very noisyway of tagging data.
In some cases, even thoughthe answer chunk may be tagged as correct itmay not be supported by the accompanying sen-tence, while in other cases, a correct chunk maybe graded as incorrect, since the pattern file listdid not represent a exhaustive list of answers.We set aside 20% of the training data for valida-tion.4.2 Classifier vs. Re-RankerWe evaluate the performance of the QA systemviewed as a classifier (with a post-processingstep) and as a re-ranker.
In order to do a fairevaluation of the system we test the performanceof the QA system under varying conditions ofthe output of the IR system.
The results areshown in Table 3.The results should be read in the followingway: We use the same IR system.
However, dur-ing each run of the experiment we consider onlythe top K sentences returned by the IR systemK={1,10,50,100,150,200}.
The column ?
cor-rect?
represents the number of questions the en-tire QA (IR + re-ranker) system answeredcorrectly.
?
IR Loss?
represents the averagenumber of questions for which the IR failedcompletely (i.e., the IR did not return even a sin-gle sentence that contains the correct answer).The IR precision is the precision of the IR sys-tem for the number of sentences considered.
An-swer-pinpointing performance is based on themetric described above.
Finally, the overallscore is the score of the entire QA system.
(i.e.,precision at rank#1).The ?
Overall Precision" column indicatesthat the re-ranker clearly outperforms the classi-fier.
However, it is also very interesting to com-pare the performance of the re-ranker ?
OverallPrecision?
with the ?
Answer-Pinpointing preci-sion?
.
For example, in the last row, for the re-ranker the ?
Answer-Pinpointing Precision?
is0.5182 whereas the ?
Overall Precision?
is only0.34.
The difference is due to the performance ofthe poor performance of the IR system (?
IRLoss?
= 0.344).4.3 Oracle IR systemIn order to determine the performance of theanswer pinpointing module alone, we performthe so-called oracle IR experiment.
Here, wepresent to the answer pinpointing module onlythose sentences from IR that contain an answer4.The task of the answer pinpointing module is topick out of the correct answer from the givencollection.
We report results in Table 4.
In theseresults too the re-ranker has better performanceas compared to the classifier.
However, as wesee from the results, there is a lot of room forimprovement for the re-ranker system, even witha perfect IR system.5 DiscussionOur experiments clearly indicate that the QAsystem viewed as a re-ranker outperforms theQA system viewed as a classifier.
The differencestem from the following reasons:1.
The classification training criteria work on amore difficult objective function of trying tofind whether each candidate answer answersthe given question, as opposed to trying tofind the best answer for the given question.Hence, the same feature set that works forthe re-ranker need not work for the classi-fier.
The feature set used in this problem isnot good enough to help the classifier dis-tinguish between correct and incorrect an-4This was performed by extracting all the sentences thatwere judged to have the correct answer by human evalua-tors during the TREC 2002 evaluations.swers for the given question (even though itis good for the re-ranker to come up with thebest answer).2.
The comparison of probabilities across dif-ferent events (histories) for the classifier,during the decision rule process, is problem-atic.
This is because the probabilities, whichwe obtain after the classification approach,are only a poor estimate of the true probabil-ity.
The re-ranker, however, directly allowsthese probabilities to be comparable by in-corporating them into the model itself.3.
The QA system viewed as a classifier suf-fers from the problem of a highly unbal-anced data set.
We have less than 1%positive examples and more than 99% nega-tive examples (we had almost 4 milliontraining data events) in the problem.
Ittyche-riah (2001), and Ittycheriah and Roukos(2002), use a more controlled environmentfor training their system.
They have 23%positive examples and 77% negative exam-ples.
They prune out most of the incorrectanswer initially, using a pre-processing stepby using either a rule-based system (Ittyche-riah, 2001) or a statistical system (Ittyche-riah et al, 2002); and hence obtain a muchmore manageable distribution in the trainingphase of the Maximum Entropy model.Answer-PinpointingPrecision Number Correct Overall Precision IR Sen-tencesTotalques-tions IR Precision IR Loss Classifier Re-ranker Classifier Re-ranker Classifier Re-ranker1 500 0.266 0.742 0.0027 0.3565 29 46 0.058 0.09210 500 0.2018 0.48 0.0016 0.4269 7 111 0.014 0.22250 500 0.1155 0.386 0.0015 0.4885 6 150 0.012 0.3100 500 0.0878 0.362 0.0015 0.5015 5 160 0.01 0.32150 500 0.0763 0.35 0.0015 0.5138 5 167 0.01 0.334200 500 0.0703 0.344 0.0015 0.5182 3 170 0.01 0.34Table 3 : Results for Classifier and Re-ranker under varying conditions of IR.IR Sentences = Total IR sentences considered for every questionIR Precision = Precision @ (IR Sentences)IR Loss = (Number of Questions for which the IR did not produce a single answer)/(Total Questions)Overall Precision = (Number Correct)/(Total Questions)6 ConclusionThe re-ranker system is very robust in handlinglarge amounts of data and still produces reason-able results.
There is no need for a major pre-processing step (for eliminating undesirable in-correct answers from the training) or the post-processing step (for selecting the most promis-ing answer.
)We also consider it significant that a QA sys-tem with just 4 features (viz.
Frequency,Expected Answer Type, Question word absent,and ITF word match) is a good baseline systemand performs better than the median perform-ance of all the QA systems in the TREC 2002evaluations5.Ittycheriah (2001), and Ittycheriah and Rou-kos (2002) have shown good results by using arange of features for Maximum Entropy QA sys-tems.
Also, the results indicate that there isscope for research in IR for QA systems.
TheQA system has an upper ceiling on performancedue to the quality of the IR system.
The QAcommunity has yet to address these problems ina principled way, and the IR details of most ofthe system are hidden behind the complicatedsystem architecture.The re-ranking model basically changes theobjective function for training and the system isdirectly optimized on the evaluation functioncriteria (though still using Maximum Likelihoodtraining).
Also this approach seems to be veryrobust to noisy training data and is highly scal-able.Acknowledgements.This work was supported by the Advance Re-search and Development Activity (ARDA)?sAdvanced Question Answering for Intelligence(AQUAINT) Program under contract number5However, since the IR system used here was from theWeb, our results are not directly comparable with theTREC systems.MDA908-02-C-007.
The authors wish to ex-press particular gratitude to Dr. Abraham It-tycheriah, both for his supervision and educationof the first author during his summer visit toIBM TJ Watson Research Center in 2002 andfor his thoughtful comments on this paper,which was inspired by his work.ReferencesDarroch, J. N., and D. Ratcliff.
1972.
Generalizediterative scaling for log-linear models.
Annals ofMathematical Statistics, 43:1470?1480.Hermjakob, U.
1997.
Learning Parse and TranslationDecisions from Examples with Rich Context.Ph.D.
Dissertation, University of Texas at Austin,Austin, TX.Hovy, E.H., U. Hermjakob, D. Ravichandran.
2002.A Question/Answer Typology with Surface TextPatterns.
Proceedings of the DARPA Human Lan-guage Technology Conferenc,.
San Diego, CA,247?250.Ittycheriah, A.
2001.
Trainable Question AnsweringSystem.
Ph.D. Dissertation, Rutgers, The StateUniversity of New Jersey, New Brunswick, NJ.Ittycheriah., A., and S. Roukos.
2002.
IBM?S Ques-tion Answering System-TREC-11.
Proceedings ofTREC 2002, NIST, MD, 394?401.Magnini, B, M. Negri, R. Prevete, and H. Tanev.2002.
Is it the Right Answer?
Exploiting Web Re-dundancy for Answer Validation.
Proceedings ofthe 40th Meeting of the Association of Computa-tional Linguistics, Philadelphia, PA, 425?432.Della Pietra, S., V. Della Pietra, and J. Lafferty.1995.
Inducing Features of Random Fields, Tech-nical Report Department of Computer Science,Carnegie-Mellon University, CMU?CS-95?144.Xu, J., A. J. Licuanan, S. May, R. Miller, and R.Weischedel.
2002.
TREC2002QA at BBN: An-swer Selection and Confidence Estimation.
Pro-ceedings of TREC 2002.
NIST MD.
290?295Answer-PinpointingPrecision Total ques-tions IR precision Classifier Re-ranker429 1.0 0.156 0.578Table 4 : Performance with a perfect IR system
