Proceedings of the IJCNLP-08 Workshop on NER for South and South East Asian Languages, pages 89?96,Hyderabad, India, January 2008. c?2008 Asian Federation of Natural Language ProcessingNamed Entity Recognition for South Asian LanguagesAmit GoyalUniversity of Utah, School of ComputingSalt Lake City, Utahamitg@cs.utah.eduAbstractMuch work has already been done onbuilding named entity recognition systems.However most of this work has been con-centrated on English and other Europeanlanguages.
Hence, building a named entityrecognition (NER) system for South AsianLanguages (SAL) is still an open problembecause they exhibit characteristics differ-ent from English.
This paper builds anamed entity recognizer which also identi-fies nested name entities for the Hindi lan-guage using machine learning algorithm,trained on an annotated corpus.
However,the algorithm is designed in such a mannerthat it can easily be ported to other SouthAsian Languages provided the necessaryNLP tools like POS tagger and chunker areavailable for that language.
I compare re-sults of Hindi data with English data ofCONLL shared task of 2003.1 IntroductionIdentifying and classifying named-entities intoperson, location, organization or other names in atext is an important task for numerous applications.I focus here on building a named entity recognitionsystem that will automatically mark the boundariesand labels of the named entities (NEs) in runningtext.
The system also identifies nested named enti-ties which are a superset of the maximal entities.E.g.
?Lal Bahadur Shastri National Academy ofAdministration?
is an organization name and isreferred as maximal entity.
However it also con-tains ?Lal Bahadur Shastri?
as a person name pre-sent inside an organization name and which is re-ferred as a part of nested entity along with ?LalBahadur Shastri National Academy of Administra-tion?
as an organization name.To make the problem simpler, I split the prob-lem into three sub tasks.
The first (NER module) ofwhich identifies whether an entity is a NE or not;the second (NEC module) identifies the type oflabel associated with each entity; the third (NNEmodule) identifies the nested name entities (NNE).Labels considered for this task are: person, organi-zation and location names, measure, time, number,domain specific terms, abbreviation, title anddesignation.Conditional random fields (CRFs) (Lafferty etal.
2001) with a variety of novel and traditionalfeatures have been used as a classifier for abovethree modules.
CRFs are undirected graphicalmodels, a special case of which is linear chainswhich are well suited to sequence labeling tasks.They have shown to be useful in part of speechtagging (Lafferty et al 2001), shallow parsing (Shaand Pereira 2003), and named entity recognitionfor Hindi newswire data (Li and McCallum 2003).2 Related WorkNamed Entity Recognition (NER) has been con-sidered as subtask of Information Extraction.
Dif-ferent NER systems were evaluated as a part of theSixth Message Understanding Conference in 1995(MUC6).
The target language was English.
Palmerand Day (1997) have worked on Chinese, English,French, Japanese, Portuguese and Spanish andfound that the difficulty of the NER task was dif-ferent for the six languages but that a large part ofthe task could be performed with simple methods.89Cucerzan et al (1999) used both morphologicaland contextual clues for identifying named entitiesin English, Greek, Hindi, Rumanian and Turkish.With minimal supervision, they obtained overall Fmeasures between 40 and 70, depending on thelanguages used.
Collins (1999) showed that use ofunlabelled data for NER can reduce the require-ments for supervision to just 7 simple seed rules.The CoNLL shared task of 2002 and 2003 focusedon language independent NER and has performedevaluations on English, Spanish, Dutch and Ger-man and participating systems have performedwell.
Li and McCallum (2003) used CRFs and fea-ture induction (McCallum 2003) to get an F-scoreof 71.50 for Hindi language on test-set.
May et al(2003) used HMM to create NER for Hindi andCebuano.
Ekbal et al (2007) used lexical patternlearning from corpus data for NER for Bangla lan-guage.3 My ContributionsI focus here on building a NER system for theHindi language using conditional random fields(CRFs) using NLPAI Machine Learning Contest2007 data.
The system is built in such a mannerthat it could be easily ported to other languages.This method was evaluated on test set 1 and test set2 and attains a maximal F1 measure around 49.2and nested F1 measure around 50.1 for test-set 1;maximal F1 measure around 44.97 and nested F1measure 43.70 around  for test-set 2.
However thesystem achieves an F-measure of 58.85 on devel-opment set.
The great difference in the numberscould be due to some difference in test and devel-opment set.
I have also compared my results onHindi data with English data of CONLL sharedtask of 2003 by introducing interesting phenomenawhich are not present in English.
I perform ex-periments on English after removing capitalizationsince Hindi lacks such overt marking.
Also there isanother interesting phenomenon in Hindi or anyother SAL i.e.
a word can be a common noun aswell as a proper noun.
For example ?sambhabsinha?
is a name of a person but when I use ?samb-hab?
in a sentence ?yaha kaam mujse sambhabnahi?
It acts as a common noun meaning ?possible?in English.
Hindi is full of such cases making thetask more difficult.
Hence it becomes very difficultfor NER system to classify it as person or not.4 FeaturesThe success of any machine learning algorithmdepends on finding an appropriate combination offeatures.
This section outlines three types of fea-tures.4.1 Contextual features?
Word Window: A word window of size ncentered in position iw is the sequence ofwords in the sentence placed at   iw + jw po-sitions, with jw ?
[-n , +n].
For each word inthe window, word and it?s POS + its relativeposition jw forms a feature?
Chunk window: A chunk window of con-text size n centered in position ic is the se-quence of chunks in the sentence placed ic +jc positions, with jc ?
[-n , +n].
The tags (la-bels) of the chunks in the window + its rela-tive position jc form a feature.4.2 Statistical features?
Binary features: As name suggests thesefeatures have value 0 or 1.
These featuresare not mutually exclusive features that testwhether the following predicates hold in theword: all digits, 4 digit number, containshyphen, punctuation mark, acronym, alpha-numeric etc.
I also modeled whether a par-ticular word is a noun or not using the POSinformation.?
Trigger words: Using the annotated train-ing data I find all those words which have ahigh probability of being a number, meas-ure, abbreviation and time.
I model 4 binaryfeatures giving value 1 to high probablewords and 0 to the rest.
For example, highprobable words for number would be ?eka?,?xo?, ?wIna?, ?cAra?
etc.
(words here are inwx-notation) and will get a value as 1.4.3 Word Internal Feature?
Affixes: Some prefixes and suffixes aregood indicators for identifying certainclasses of entities.
Suffixes are typicallyeven more informative.
For example, suf-fixes like -bad , -pur, -pally are good indica-tors of a name of a location.90?
Words are also assigned a generalized?word class (WC)?
similar to Collins (2002),which replaces all letters with ?a?, digitswith ?0?, punctuation marks with ?p?, andother characters with ?-?.
There is a similar?brief class (BWC) (Settles 2004)?
whichcollapses consecutive characters into one.Thus the words ?D.D.T.?
and ?AB-1946?would both be given the featuresWC=apapap, BWC=apapap andWC=aap0000, BWC=ap0 respectively, inabove example hyphen forms the part ofpunctuation marks.
This feature has beenmodeled since this feature can be useful forboth unseen words as well as solving thedata sparsity problem.?
Stem of the Word was also obtained usinga morph analyzer.We have tried to use the different combination ofall these features for all three modules which I amgoing to discuss in the next section.
But beforeending there are few features which I haven?t usedand would like to use in future.
Bag of words i.e.form of the words in the window without consider-ing their position.
Gazetteer Features can also beuseful.
These features couldn?t be used due tocomputational reasons, lack of resources and time.5 Modules5.1 NER moduleThis module identifies whether an entity is a NE ornot.
I use well-known BIO model.
B denotes beginof an entity, I denotes inside an entity; O denotesoutside and is not part of any entity.
Here I haveonly one label i.e.
NE.
Hence it becomes a threeclass problem with B-NE, I-NE and O as outputlabels.
Here I am identifying NEs as it?s an easiertask as compare to classifying them among named-entity tag-set.
It is also done with a hope that thisinformation can be useful for NEC module.
Forexample in entity like ?Raja Ram Mohun Roy?tags would be ?Raja/B-NE Ram/I-NE Mohun/I-NERoy/I-NE.?
Similarly for ?Microsoft Corp.?
tagswould be ?Microsoft/B-NE Corp./I-NE.?
Wordslike ?tiger?, ?eat?, ?happy?
etc which are not NEsare tagged as O.5.2 NEC moduleHere I try to classify the NEs among variousclasses/labels like person (like Mahatma Gandhi),location(like Delhi) and organization(like Micro-soft Corp.) names, number (like one, two etc), time(like one day), measure (like 5 kg), domain spe-cific terms (Botany, zoology etc), title (Mr., TheSeven Year Itch), abbreviation (D.D.T.)
and desig-nation (Emperor).
Hence it becomes a 10 (la-bels/classes) * 2(B+I) = 20 + 1 (O which denotesremaining words) =21 class problem.
This moduleis independent from the previous module.
For ex-ample in entity like ?Raja Ram Mohun Roy?
tagswould be ?Raja/B-NEP Ram/I-NEP Mohun/I-NEPRoy/I-NEP.?
Similarly for ?Microsoft Corp.?
tagswould be ?Microsoft/B-NEO Corp./I-NEO.
?I could have tried labeling the identified named-entities from NER However; I found that this re-sults in a drop in accuracy.
Hence I use the outputof the NER module as one of the features for NEC.5.3 NNE moduleThe length of nested named entities is unboundedbut the majority contains at most 3 words.
There-fore, I try to train three classifiers to learn entitiesof length 1, 2 and 3 independently.
This allows usto learn nested entities since the bigger entities canhave different tags when compared to smaller enti-ties.
For example, Srinivas Bangalore will betagged as a name of a person by a classifier who istrained to classify NEs of length 2.
However, Srini-vas and Bangalore will be tagged as a name of aperson and location respectively by a classifierwhich is trained to classify entities of length 1.In this module also I use the same BIO modeland there will be 21 classes for each of the threeclassifiers.6 Experiments and DiscussionIn this section I describe the experiments I per-formed to evaluate presented algorithm with itsvariations.NLPAI 2007 NER contest Corpus, I was pro-vided annotated training and development datacomprising of 19825 and 4812 sentences respec-tively for Hindi.
The data is labeled with 10 labelsdescribed above in NEC module.
The average sen-tence length of the corpus is 24.5.
The first stepwas to enrich the data with POS, chunk informa-tion and root of the word using POS tagger, Chun-91ker (Avinesh et al 2007) and IIIT-Hyderabadmorph analyzer.
Hence porting this algorithm toany other SAL would require these tools for thatlanguage.In the training data, in about 50% sentences(i.e.10524 sentences) there was not even a singleNE.
Experimentally I found that the inclusion orexclusion of these sentences did not have a signifi-cant effect on system performance.
Hence I carriedall the remaining experiments with sentences con-taining NEs.
The reason for choosing it is it takesless time to train and more experiments could beperformed given the time constraints.Then I tried to find an appropriate set of featuresfor NER and NEC module.
For NNE I used thesame features as used in NEC module since I don?thave explicitly labeled data for nested entities.Tweaking and tuning of feature doesn?t affect theaccuracy significantly.For NER module, where I am trying to identifyname entities; context information seems to bemore informative than statistical features.
I use awindow of -1 to +1 for words, -2 to +2 POS andalso use features which are combinations of con-secutive POS tags and words.
For exampleRam/NNP eat/VB mangoes/NNS.
Combinationfeatures for word ?eat?
would be NNP/VB,VB/NNS, Ram/eat, eat/mangoes, NNP/VB/NNS,Ram/eat/mangoes.
The stem of the word and chunkinformation also doesn?t affect the accuracy.
Theprefixes and suffixes of length 3 and 4 are found toimprove the accuracy of the classifier.
For exampleHyderabad will have Hyd, Hyde, bad, abad as pre-fixes and suffixes of length 3 and 4 respectively.The word class (WC) and Brief word class (BWC)features are also very useful features for recogniz-ing named-entities.
I have achieved an F-measureof 64.28 by combination of all these features foridentifying name-entities on development set.
Ta-ble 1 shows the detailed results of named entityrecognition (NER) module.For NEC module, the contextual features as wellas statistical features are helpful in deciding towhich class a name-entity belongs.
I use word andPOS window of -1 to +1 as context.
No combina-tion features are being used as introduction of suchfeatures degrades the accuracy rather than improv-ing it.
However the statistical features are found tobe more useful in this case as compared to NER.Here also prefixes and suffixes of length 3 and 4are found to be useful.
BWC feature alone is suffi-Features  Precision  Recall  F-measureContextual 64.19 60.53 62.31Contextual+Word Internal64.84 63.73 64.28Table1: Detailed performance of NER module us-ing only contextual features and combining wordinternal features.Entity Precision Recall F-measureAbbreviation 43.21 36.46 39.55Designation 69.61 46.84 56.00Location 67.51 63.08 65.22Measure 73.98 72.84 73.41Number 70.41 87.74 78.13organization 49.71 39.73 44.16Person 61.18 47.37 53.40Title 31.82 14.00 19.44Terms 30.81 16.72 21.67Time 67.30 58.53 62.61Overall 62.60 55.52 58.85Table2: Detailed performance of the best featureset on development set for maximal/nested namedentities.-cient for classification, we don?t need to use WCfeature for improving the accuracy.
Chunk infor-mation and stem of the word doesn?t improve theaccuracy.I have modeled NER module so that the outputof that module can be used as feature for NEC.
Butusing it as a feature doesn?t improve the classifica-tion accuracy.
Also, I tried using the boundary in-formation from the NER module and combining itwith labels learned from NEC module.
It alsoseems to be a futile attempt.I have used unlabelled data i.e.
24630 sentencesprovided during the contest and used bootstrappingto make use of it.
I have doubled the data i.e.
50%manually annotated data and rest is system outputon unlabelled data i.e.
12323 sentences; we haveused only those sentences which contains at leastone NE.
With this data I almost get the same accu-racy as I got with only manually annotated data.Table 2 shows the detailed performance of the bestfeature set on development set for maximal/nestednamed entities using evaluation script of CONLLshared task of 2003.
I have used the evaluationscript of NLPAI contest to report results on Testset-1 and Test set-2 (which contains 1091 and 744sentences) for two systems in Table 3 and 4.
One92trained using only annotated data and the othertrained on annotated and bootstrapped data for thesame feature set which performed best on devel-opment set.
For test-set 2, system trained usingannotated and bootstrapped data performs betterthan the system trained using only annotated data.However, for test set1 both the systems performalmost same.
One of the reasons for less results ascompared to development set is I haven?t furtherclassified title tag into title object and title persontag and Test sets contain many such instances.I have trained a single classifier for all the enti-ties but we can use more classifiers and divide thetags in such a fashion that those which are closer toone another fall in one group.
For example we canclub number, time and measure in one group andcall them as number group since these are closer toeach other and train a classifier to automaticallyannotate these entities in running text.
Similarly,we can group person, number, and location andcall them as name group.
I have attempted a simi-lar experiment using the same features of NECmodule for number and name group but still thereis no improvement.For NNE module, I have used the same set offeatures which I have used in NEC module and Iam handling nested entities up to length of 3.
Sincethe development set is not enriched with nestedentities, it is difficult to optimize the features forthis module and the results would be same as NERmodule since nested entities are superset of maxi-mal entities.
For Test set-1 and Test set-2 Table 3and 4 are used to report results.For NEs like title there are fewer instances intraining data which is a reason for its low F-measure i.e.
19.44 on development set which iseven less than terms (i.e.
21.67) which are mostdifficult to learn.
Also here I have focused on alarge tag set but it would be interesting to concen-trate only on person, location and organizationnames, since most of the systems report accuracyfor these entities.
Hence I did some experimentswith Hindi data concentrating only on person, loca-tion and Organization but there is not so much in-crease in the performance.When I trained my system on English data(which I have made mono case) of Conll-2003shared task, with only contextual features, systemgets an overall F-measure of 84.09 on developmentset and 75.81 on test set which is far better thanHindi.
I have just used contextual features withEntity Test set1 Test set 2MaximalPrecision70.78 55.24MaximalRecall37.69 35.75MaximalF-Measure49.19 43.41NestedPrecision74.28 58.62NestedRecall37.73 33.07NestedF-Measure50.04 42.29Table3: System trained using only annotated dataEntity Test set1 Test set 2MaximalPrecision70.28 57.60MaximalRecall37.62 36.88MaximalF-Measure49.00 44.97NestedPrecision73.90 60.98NestedRecall37.93 34.05NestedF-Measure50.13 43.70Table 4: System trained using annotated and boot-strapped datawindow size of -1 to +1 for words, POS and chunkto achieve the results reported in Table 5 for testset.
The reason for using only contextual informa-tion is that these features give the maximum accu-racy and the rest of the features don?t increase theaccuracy by such a great amount.
Also the aimover here is to compare results with Hindi lan-guage and not to make the best NER system forEnglish language.Entity Precision Recall F-measurePerson 82.05 79.16 80.58Location 84.16 79.32 81.67Organization 70.76 67.01 68.83Misc.
73.71 61.11 66.82Overall 78.40 73.39 75.81Table 5: System trained on English mono case datausing contextual features93Also to include common noun phenomena in Eng-lish I have taken 10 random person names from thedata and replaced them with common nouns andthe results are really surprising.
By introducingthis, system achieves an F-measure of 84.32 ondevelopment set and 76.19 on test set which is bet-ter than the results on normal system.
The numberof tokens corresponding to these names in trainingdata is 500.
Table 6 contains the detailed results.Entity Precision Recall F-measurePerson 81.92 79.84 80.86Location 84.18 80.10 82.09Organization 71.98 67.13 69.47Misc.
73.04 60.97 66.46Overall 78.71 73.83 76.19Table 6: System trained on English mono case datawith common noun phenomena using contextualfeaturesThe results for English are far better than Hindilanguage.
The reason is English already has toolslike POS tagger and chunker which achieves an Fmeasure around 95 whereas for Hindi we onlyhave an F-measure of 85 for tagger and 80 forchunker.
This is the reason why the accuracy ofEnglish system didn?t fall when I removed capi-talization and introduced common noun phenom-ena since POS context and chunk context helps alot.
Since CONLL 2003 data is already POStagged and chunked, hence POS and chunks corre-spond to capitalized data.
To make it more even, Iran Stanford POS tagger (Toutanova et al 2003)on the same mono case CONLL 2003 data andthen train the model using only word and POS con-text.
The numbers drop on test set by more than15% as shown in Table 7.
For development set theoverall F-measure is around 74%.Entity Precision Recall F-measurePerson 66.97 53.93 59.75Location 68.57 56.54 61.98Organization 71.64 53.55 61.29Misc.
74.71 55.98 64.01Overall 69.69 54.84 61.38Table7: System trained on POS tagger ran onmono-case dataThese numbers are comparable to Hindi data.
Thereason is POS tagger performs badly after remov-ing capitalization.
Now the POS tagged data marksproper noun i.e.
NNP as common noun i.e.
NN orforeign word as FW.
The reason is it uses capitali-zation to mark NNP tag.
We still haven?t includedcommon noun phenomena.
So to do that, I take thecommon noun phenomenon English data and trainthe model using the same features as used above.Here also the system performs in the same way.There is just a decrease of 1% in F-measure of per-son class.
Table 8 contains the detailed results.
Theintroduction of common noun phenomena doesn?tseem to affect the performance too much.
The rea-son can be context helps in disambiguating be-tween the real ?cheese?
and the ?cheese?
which hasbeen made up by replacing it with ?John?.Entity Precision Recall F-measurePerson 65.48 53.37 58.81Location 68.23 56.18 61.62Organization 73.95 53.01 61.75Misc.
74.81 56.27 64.23Overall 69.74 54.45 61.16Table8: System trained on POS tagger ran onmono case data which contains common nounphenomenonAfter looking at these results, we can easily saythat if we can improve the performance of POStagger, we can do very well on the NER task.Without that it?s even difficult for English to givegood numbers.
It is correct that Hindi and SALdon?t have capitalization but we could make use ofmorphological features since most of SAL aremorphologically rich.
A hybrid approach involvingrules along with machine learning approach couldhelp us to improve POS tagger and NER systems.After seeing results on English we ask what arethe actual reasons for lower numbers on Hindidata?
Inconsistency of annotated data is one of thebig problems but it?s very difficult to create 100%correct manual data since we have chosen a finelygrained tagset.
Also the data used for Hindi is fromdifferent domains.
Hence due to which the lot ofterms doesn?t occur in corpus more than once.
Oneof the plausible reasons for bad results on test setfor Hindi compared to development set could be94difference in domain of test set.
Also due to lack ofresources like gazetteer for SAL the task becomesmore challenging to create everything fromscratch.
Also the accuracy of tagger, chunker andmorph analyzer are not as good as when we com-pare results with English.7 ConclusionIn conclusion, I have confirmed that use of ma-chine learning algorithm on annotated data forHindi language can be useful and the same algo-rithm can be useful for other languages.
I onlyneed to tune and tweak the features for a particularlanguage.
I have described some traditional andnovel features for Hindi language.
I have alsoshown that it?s better to directly classify name-entities into various labels or classes rather thanfirst recognizing them.
Also the attempt to makeuse of unlabelled data didn?t help much.Also I have showed that capitalization is one ofthe important clues for high performance of Eng-lish on various NLP applications.
But we couldalso recognize some other important clues in SALand can hope to do better than English withouthaving capitalization.Directions for future work include concentratingon a smaller tag set and trying to improve accuracyfor each of the label.
Since still we don?t haveenough labeled data for other SAL, it would beinteresting to try out some unsupervised or semi-supervised approaches.
Also I haven?t tried rulebased approach which could be very handy whencombined with some machine learning approach.Hence adopting a hybrid approach should help inimproving the accuracy of the system but still it?san open question.8 AcknowledgementsI would like to thank Prof. Rajeev Sangal, Dr. HalDaume III, Dr. Dipti Misra Sharma and AnilKumar Singh for their helpful comments and sug-gestions.ReferencesAndrew McCallum.
2003.
Efficiently Inducing Featuresof Conditional Random Fields.
In Proceedings of the19th Conference in UAI.Andrew McCallum and Wei Li.
2003.
Early results fornamed entity recognition with conditional randomfields, feature induction and web-enhanced lexicons.In Proceedings CoNLL 2003.Avinesh.PVS.
and Karthik G. 2007.
Part-Of-SpeechTagging and Chunking using Conditional RandomFields and Transformation Based Learning.
In Pro-ceedings of SPSAL2007Asif Ekbal and Sivaji Bandyopadhyay.
2007.
LexicalPattern Learning from Corpus Data for Named entityrecognition.
In Proceedings of ICON 2007.Burr Settles.
2004.
Biomedical Named Entity Recogni-tion Using Conditional Random Fields and Rich Fea-ture Sets.
In Proceedings of the International JointWorkshop on NLPBA.David D. Palmer and David S. Day.
1997.
A StatisticalProfile of the Named Entity Task.
In Proceedings ofFifth ACL Conference for ANLP.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 Shared Task: Language-IndependentNamed Entity Recognition.
In Proceedings of theCoNLL 2002.Erik F. Tjong Kim Sang and Fien De Meulder.
2003.Introduction to the CoNLL-2003 Shared Task: Lan-guage-Independent Named Entity Recognition.
InProceedings of the CoNLL 2003.Fei Sha and Fernando Pereira.
2003.
Shallow parsing-with conditional random fields.
In Proceedings of theHLT and NAACL 2003.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequencedata.
In Proceedings of ICML 2001.Kristina Toutanova and Christopher D. Manning.
2000.Enriching the Knowledge Sources Used in a Maxi-mum Entropy Part-of-Speech Tagger.
Proceedings ofthe Joint SIGDAT Conference on (EMNLP/VLC-2000), Hong Kong.Kristina Toutanova, Dan Klein, Christopher Manning,and Yoram Singer.
2003.
Feature-Rich Part-of-Speech Tagging with a Cyclic Dependency Network.In Proceedings of HLT-NAACL 2003.Michael Collins.
2002.
Ranking algorithms for named-entity extraction: Boosting and the voted perceptron.In Proceedings of ACL 2002.Michael Collins and Yoram Singer.
1999.
Unsupervisedmodels for named entity classification.
In Proceed-ings of the Joint SIGDAT Conference on EMNLPand Very Large Corpora.Silviu Cucerzan and David Yarowsky.
1999.
Languageindependent named entity recognition combining95morphological and contextual evidence.
In Proceed-ings of 1999 Joint SIGDAT Conference on EMNLPand VLC.Wei Li and Andrew McCallum.
2003.
Rapid Develop-ment of Hindi Named Entity Recognition Using Con-ditional Random Fields and Feature Induction.
InProceedings of ACM TALIP.96
