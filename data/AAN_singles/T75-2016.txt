THE BOUNDARIES OF LANGUAGE GENERATIONNeil M. GoldmanUSC - Information Sciences Inst ituteI.
INTRODUCTIONIn this paper I would like to addressseveral basical ly independent issuesrelating to the processes of naturallanguage generat ion (NLG) and research onmodel ing these processes.
In the subsect ion"Paradigms for Generation" I maintain that,viewed at a moderately  abstract level, thevast major i ty of current research in thisarea falls into a single model and focuseson the "tail end" of the language generat ionprocess.
The di f ference between indiv idualmodels seems to be based on di f fer ingassumptions or convict ions regarding thenature of "pre-generative" aspects oflanguage use.The subsect ion "Conceptual Generat ion"describes the part icular ized version of thisbasic model within which I work.
Theassumptions under ly ing this approach and theaspects of language generat ion which itattempts to account for are stressed.The discussion of "Generat ion andUnderstanding" addresses the question of whya heavy bias can be seen in the volume ofwork (at least in the fields ofcomputat ional  l inguist ics and Art i f ic ia lIntel l igence) on language understanding asopposed to language generation.
A relatedquest ion - whether the two parts of languageprocessing are suff ic ient ly dif ferent towarrant independent stature - is d iscussedbriefly.The conclus ion of the paper points upsome of the areas of inquiry which havescarcely been touched, yet which must bedeveloped before we can claim to have amodel of the overal l  process of languagegeneration.II.
PARADIGMS FOR GENERATIONA stra ight forward interpretat ion of theterm "natural  language generat ion" wouldal low that phrase to encompass all processeswhich contr ibute to the product ion of anatural  language expression, E, from somecontext, C. This leads to a "demonic"picture of generat ion as i l lustrated inFigure I.
Certain contexts produced bynon-generat ive processes in a modelcontaining a NLG component tr igger thatcomponent.
The language generator, inaddit ion to producing E, must alter thecontext suf f ic ient ly  to inhibit thereproduct ion of E ad infinitum.While this picture is suf f ic ient lygeneral to encompass virtual ly any proposedgenerat ive model, it is so non-commital  thatit does l itt le to expl icate NLG.
Thequest ion is merely resolved into twosubissues:(I) What const i tutes an NLG-act ivat ingcontext?74(2) What processes and knowledge areneeded to produce an appropr iate Ein such a context?Now in fact (I) has not been addressedas a serious problem in most work ongeneration.
The act ivat ing context hasalmost universal ly  been the existence ofsome " information to be communicated" in ad ist inguished cell in the context.
Anyprocess which "stores" into this cellimmediately awakens the generator whichproceeds to produce a natural  languageencoding of that information.
Contexta l terat ion by the generator consists s implyof erasing the special  cell.The paradigm which has evolved out ofthis decision is depicted in Figure 2.Models based on this paradigmdif ferent iated primari ly by:are(I) The representat ions used formessages to be encoded by thegenerator.
(2) The degree to which the generat ionbox interacts with the context(context-sensit iv i ty ofgeneration).The predominant formalisms forrepresenting messages are:(a (partial.)
specif icat ion ofsyntactic structure <I>(b semantic networks (consistingof case relations betweensemantic objects) <3,6>(c conceptual networks <2>The dividing line between semantic andconceptual networks is not clear-cut.
Theintended dist inct ion is that conceptualobjects and conceptual relat ions aredivorced from natural language, whereassemantic nets are constructed to representmeaning in terms of objects and relat ionsspeci f ical ly  designed for (some particular)natural language.Presumably one reason for separat ingi the select ion of a message from the task ofencoding that message into natural languagewas to free research on "generation" fromthe necessity of deal ing with context.
Butin recent years our generation models havel become more and more contextsensit ive - this is true at least of NLGmodels which treat message encoding as asubpart of some larger task.
Some of thesecontextual considerat ions appear to beindependent of any part icular targetlanguage - e.g., consultat ion of context indetermining which features of an objectshould be mentioned in its descr ipt ion<7> - while others depend on detai ledknowledge of the target language - e.g., thechoice of verbs and nouns to be used indescribing events <2>.
The increased use ofcontext is done to effect a more "natural"encoding of the message rather than simply a"legal" encoding.
In this respect there areimplicit in such NLG models certainassumptions about the use of context inlanguage understanding.
This matter will beelaborated somewhat later.The set of processes and knowledgeneeded to encode a message depends heavi lyon the message representat ion chosen.
Thisexistence of a formal grammar as therepository of syntactic knowledge about thetarget language has become standardpractice; transit ion network grammars arerepresentat ive of the current state of theart in this respect.
The progression fromsyntactic to semantic to conceptualrepresentat ions entails the use ofprogressively more knowledge about languageand communicat ion.
A semantic netrepresentat ion may need a theory of semanticcases and rules for mapping these intosurface cases  of the target language;' a!
75conceptual representat ion requires complexrules and extensive data to chooseappropriate words for the construct ion ofthe target language expression.III.
CONCEPTUAL GENERATIONMy own work in NLG falls within theparadigm described above under theassumption that the message to be expressedis avai lable only in a conceptualrepresentation.
This means that neither thewords of the target language (English) northe syntactic structures appropriate forencoding the message are init ial ly avai lableto the generator.
They must be deduced fromthe information content of the message.
(Actually one exception to this claim isclearly present in the model - an init ialpresumption is made that the message isencodeable  as a single English sentence.This is an unwarrented assumption whichhides a potent ia l ly  s ignif icant problem.
)Once actual words have been selected andorganized into an Engl ish-speci f ic  syntacticstructure, knowledge of Engl ish"l inearization" rules - e.g., thatadject ives precede the nouns theymodify - are used to produce a surfacestring.
This knowledge is contained in anAFSTN grammar and uti l ized by a methodintroduced by Simmons and Slocum <6>.By working from a conceptualrepresentation, a generator assumes theburden of account ing for two aspects oflanguage product ion general ly ignored inother models.
The first of these is wordselection, which is accounted for by apattern matching mechanism, namely decisiontrees (discr imination nets).
In order toaccount for the select ion of appropr iatewords, it is necessary to presume that thegenerator has extensive access to contextualinformation as well as access to inferent ialcapabi l i t ies and bel ief structures.
Thesecond aspect of generation which must beaddressed in the l inguistic encoding ofconceptual  graphs concerns the expression ofmeaning by structure in addit ion to i t sexpression by individual  words.
The caseframework of verbs is one source ofknowledge which deals with structuralencoding - e.g., in Engl ish the recipient ofan object can be encoded as a syntacticSUBJECT if the verb "receive" is used todescribe the transmission of that object.Other forms of structural  encoding are notdetermined by verb-related rules - e.g.,that the construct ion <container> OF<contents> can be used in Engl ish to expressthe re lat ionship between a container and theobject(s) it contains.The generat ion algor ithm demonstrates amixture of data-dr iven and goal -dr ivenbehavior.
In addit ion to the init ialgoal - "generate a SENTENCE expressing themeaning of the given graph" - choices madein the course of generat ion set upsub-goals - e.g., "express the RECIPIENT ofa t ransmiss ion and make it the SUBJECT ofthe structure being built."
The conceptualcontent of the message, however, drives theselect ion of a verb for the Engl ish sentenceand the construct ion of "optional"structural  segments.The choice of conceptual structures asa base for NLG was not made because of anypart icular designed (or accidental)suitabi l i ty of conceptual graphs for thispurpose.
Indeed it would be possible toalter the representat ions in ways whichwould s impl i fy the task of generation.
But,if a NLG model is to be uti l ized as a meansof t ransmitt ing information from a machineto a human, then the construct ion of thatinformation is a prerequis ite of itsencoding.
More importantly, for uses ofgeneration in " intel l igent" systems, theconstruct ion of the information is the mostt ime-consuming process.
For this reasonconceptual  structures are designed tofaci l i tate inference and memory integrat ioncapacity <5> - if necessary, at the expenseof ease of l inguistic analysis andgeneration.IV.
UNDERSTANDING AND GENERATIONFor several years there has been astrong emphasis on the problem ofunderstanding in computat ional  models, andrelat ively l ittle on problem of generation.In the proceedings of the past twoInternat ional  Joint Conferences onArt i f ic ia l  Intel l igence, for example, wefind eight papers deal ing with the analysisof natural  language, three descr ibing bothanalyt ic and generat ive portions of languageprocessing systems, and none devoted mainlyto NLG.
At least two reasons for this biasare discernable:(i) Resolut ion of ambiguity, longrecognized as one of the centralproblems of languageunderstanding, relies for itssolut ion on capabi l i t ies - l imitedinference, expectation, hypothesisgenerat ion and test ing - requiredby other "intel l igent" behavior.As long as language generat ion wasviewed as basical ly a matter ofcodi fy ing l inguistic knowledge, itappeared far less relevant to theAI community than did analysis.
(2) For those with a pragmatic bentthe lack of symmetry betweenrequirements of an analyzer andthose of a generator made researchon understanding of paramountimportance.
That is, for a givendomain of discourse, a machine canafford to express informationut i l iz ing a l imited vocabulary andwith l imited syntactic var ietywithout placing an unacceptableburden on a human conversant; toask a human to adhere toequivalent l imitat ions in his ownlanguage production could prohibitthe conduct of any interact ivedialogue (at least withoutextensive training).Furthermore, there exist a greatmany tasks which are current ly or76will soon be within the capacityof computers and which could beuseful ly extended by a naturallanguage "front end" - i.e., ananalyzer.
Corresponding needs fora natural language "back end" are Iharder to find, perhaps because we Iare so accustomed to using our mmachines in the computat ion ofnumerical  or tabular data, whichis seldom enhanced by expressionin natural language.Being of a pragmatic bent myself, atleast in spirit, I think the bias towardanalysis is justif ied.
But I expect that asthe boundaries of generat ion are pushed backand more work is done on the semanticaspects of generation, the view of _"analysis" and "generation" as disparate Iendeavors wil l  change considerably.
I see I far more Commonal i ty than disparity in thetwo enterprises.
Both require much the samecapacity for inference and deduction, albeitfor dif ferent purposes.
The knowledge of ?the syntactic structure of a language needed i to  understand that language is also neededto generate it, although the organizat ion ofthat knowledge may be different.
A similar icondit ion holds for knowledge of word I meanings and mappings from syntacticstructure to semantic or conceptualrelations.
Still, I do not bel ieve we areready for, or should even be str iving for, a I single representat ion and organizat ion ofthis knowledge which would permit its beingshared by both analyt ic and generat iveprocesses.
But a good deal of the fruits of iresearch can be shared.
IV.
NEW DIRECTIONS |It seems to me that there exist several Iproblem areas in the development of acomplete theory of language generat ion whichhave scarcely been touched.
Some of these Icould be, and possibly are being, prof i tably I addressed already; others seem to involveextremely long range goals.
Into the lattercategory I would put the issue of messageselect ion referred to earlier.
A theory I capable of account ing for message select ionin a general context would need a thoroughmot ivat ional  model, probably of both theinformat ion producing mechanism and the Ihuman information "consumer'.
Fortunately,  ~ Iadequate heur ist ics for message select ionare much easier to develop for task specif icdomains, so lack of a general theory is not I fl ikely to hinder either research on orappl icat ion of language generat iontechniques.However, a great deal can be done in Ithe short range on the use of context in I generation: (I) as it relates to thedeterminat ion of message encoding, and (2)in the modi f icat ion of context in ways whichaffect later analysis, generation, and I reasoning processes.Another frontier of research is in thecommunicat ive rather than l inguist ic aspects Iof NLG.
"Message selection" has been used I in this paper to refer to the choice of!information to be conveyed to a human.
Thenature of human communication is such thatit is generally necessary only to transmit asubpart of the totality of that message.Context and the understanding mechanisms ofthe information consumer are capable offilling in much vague or omittedinformation.
Winograd's heuristic fordescribing toy blocks addresses preciselythis issue - it amounts to an implicit modelprocedurally encoded in a generationprogram, of a process for finding theintended referent of an object description.While I would not push for incorporatingexplicit models of understanding in ourgeneration models, I believe much could begained by the addition of further implicitknowledge of this sort.BIBLIOGRAPHY<I> Friedman, J., A COMPUTER MODEL OFTRANSFORMATIONAL GRAMMAR, AmericanElsevier, New York, 1971<2> Goldman, N., "Sentence Paraphrasingfrom a Conceptual Base," CACM Vol.
18,No.
2, February 1975.<3> Klein, S., et.
al., "Automatic NovelWriting: A Status Report," Universityof Wisconsin TR 186, August 1973.<4> Riesbeck, C., "ComputationalUnderstanding of Natural Language UsingContext', AIM-238, Computer ScienceDepartment, Stanford University,Stanford, California.<5> Schank, R., et.
al., "Inference andParaphrase by Computer", Journal of theACM, July 1975.<6> Simmons, R., and Slocum, J.,"Generating English Discourse fromSemantic Networks", CACM, Vol.
15, No.10, October 1972.<7> Winograd, T., "Procedures as.
aRepresentation for Data in a ComputerProgram for Understanding NaturalLanguage", TR-84, M. I. T. ProjectMAC, February 1971.77input.
.
.
.
.
.
>st imul iIANA <--IL IY Is ---IIS < .
.
.
.. .
.
.
.
>I.
.
.
.
.
.
.
.
.
.
IC O N T E X T  I<GENERATI0NF igure  Iinput.
.
.
.
.
.
>s t imu l iIANA <--LYS - - -IS < .
.
.
.- - .
- ->I MESSAGEI SELECT IONIAl__I I, II l t----~JI .
.
.
.C O N T E X T- - - ->I e rase<GENERATI0NF igure  2Natura l. .
.
.
.
.
>LanguageNatura lLanguage" I1IIIIIIlIIIiI!III!
