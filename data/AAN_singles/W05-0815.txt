Proceedings of the ACL Workshop on Building and Using Parallel Texts, pages 95?98,Ann Arbor, June 2005. c?Association for Computational Linguistics, 2005Experiments Using MAR for Aligning Corpora?Juan Miguel VilarDepartamento de Lenguajes y Sistemas Informa?ticosUniversitat Jaume ICastello?n (Spain)jvilar@lsi.uji.esAbstractWe present some experiments conductedwithin the context of one of the sharedtasks of the ACL 2005 Workshop onBuilding and Using Parallel Texts.
Wehave employed a new model for findingthe alignments.
This new model takesa recursive approach in order to find thealignments.
As its computational costs arequite high, a method for splitting the train-ing sentences in smaller parts is used.1 IntroductionWe present the experiments we conducted within thecontext of the shared task of the track on buildingand using parallel texts for languages with scarceresources of the ACL 2005 Workshop on Build-ing and Using Parallel Texts.
The aim of the taskwas to align the words of sentence pairs in differ-ent language pairs.
We have participated using theRomanian-English corpora.We have used a new model, the MAR (from theSpanish initials of Recursive Alignment Model) thatallowed us to find structured alignments that werelater transformed in a more conventional format.The basic idea of the model is that the translation ofa sentence can be obtained in three steps: first, thesentence is divided in two parts; second, each partis translated separately using the same process; and?Work partially supported by Bancaixa through the project?Sistemas Inductivos, Estad?
?sticos y Estructurales, para la Tra-duccio?n Automa?tica (SIEsTA)?.third, the two translations are joined.
The high com-putational costs associated with the training of themodel made it necessary to split the training pairs insmaller parts using a simple heuristic.Initial work with this model can be seen in (Vi-lar Torres, 1998).
A detailed presentation can befound in (Vilar and Vidal, 2005).
This model sharessome similarities with the stochastic inversion trans-duction grammars (SITG) presented by Wu in (Wu,1997).
The main point in common is the num-ber of possible alignments between the two models.On the other hand, the parametrizations of SITGsand the MAR are completely different.
The gen-erative process of SITGs produces simultaneouslythe input and output sentences and the parametersof the model refer to the rules of the nontermi-nals.
This gives a clear symmetry to both inputand output sentences.
Our model clearly distin-guishes an input and output sentence and the pa-rameters are based on observable properties of thesentences (their lengths and the words composingthem).
Also, the idea of splitting the sentences un-til a simple structure is found in the Divisive Clus-tering presented in (Deng et al, 2004).
Again, themain difference is in the probabilistic modeling ofthe alignments.
In Divisive Clustering a uniform dis-tribution on the alignments is assumed while MARuses a explicit parametrization.The rest of the paper is structured as follows: thenext section gives an overview of the MAR, then weexplain the task and how the corpora were split, afterthat, how the alignments were obtained is explained,finally the results and conclusions are presented.952 The MARWe provide here a brief description of the model,a more detailed presentation can be found in (Vilarand Vidal, 2005).
The idea is that the translation ofa sentence x?
into a sentence y?
can be performed inthe following steps1:(a) If x?
is small enough, IBM?s model 1 (Brown etal., 1993) is employed for the translation.
(b) If not, a cut point is selected in x?
yielding twoparts that are independently translated applyingthe same procedure recursively.
(c) The two translations are concatenated either inthe same order that they were produced or sec-ond first.2.1 Model parametersApart from the parameters of model 1 (a stochas-tic dictionary and a discrete distribution of lenghts),each of the steps above defines a set of parameters.We will consider now each set in turn.Deciding the submodel The first decision iswhether to use IBM?s model 1 or to apply the MARrecursively.
This decision is taken on account of thelength of x?.
A table is used so that:Pr(IBM | x?)
?
MI(|x?|),Pr(MAR | x?)
?
MM (|x?|).Clearly, for every x?
we have that Pr(IBM | x?)
+Pr(MAR | x?)
= 1.Deciding the cut point It is assumed that theprobability of cutting the input sentence at a givenposition b is most influenced by the words around it:xb and xb+1.
We use a table B such that:Pr(b | x?)
?B(xb, xb+1)?|x?|?1i=1 B(xi, xi+1).That is, a weight is assigned to each pair of wordsand they are normalized in order to obtaing a properprobability distribution.1We use the following notational conventions.
A string orsequence of words is indicated by a bar like in x?, individualwords from the sequence carry a subindex and no bar like in xi,substrings are indicated with the first and last position like in x?ji .Finally, when the final position of the substring is also the lastof the string, a dot is used like in x?.iDeciding the concatenation direction The direc-tion of the concatenation is also decided as a func-tion of the two words adjacent to the cut point, thatis:Pr(D | b, x?)
?
DD(xb, xb+1),Pr(I | b, x?)
?
DI(xb, xb+1),where D stands for direct concatenation (i.e.the translation of x?b1 will precede the transla-tion of x?.b+1) and I stands for inverse.
Clearly,DD(xb, xb+1) + DI(xb, xb+1) = 1 for everypair (xb, xb+1).2.2 Final form of the modelWith these parameters, the final model is:pT (y?
| x?)
=MI(|x?|)pI(y?
| x?
)+MM (|x?|)|x?|?1?b=1B(xb, xb+1)?|x?|?1i=1 B(xi, xi+1)?
(DD(xb, xb+1)|y?|?1?c=1pT (y?c1 | x?b1)pT (y?.c+1 | x?.b+1)+DI(xb, xb+1)|y?|?1?c=1pT (y?.c+1 | x?b1)pT (y?c1 | x?.b+1))were pI represents the probability assigned bymodel 1 to a pair of sentences.2.3 Model trainingThe training of the model parameters is done max-imizing the likelihood of the training sample.
Foreach training pair (x?, y?)
and each parameter P rele-vant to it, the value ofC(P ) =PpT (y?
| x?)?
pT (y?
| x?)?
P(1)is computed.
This corresponds to the counts of Pin that pair.
As the model is polynomial on allits parameters except for the cuts (the B?s), Baum-Eagon?s inequality (Baum and Eagon, 1967) guar-antees that normalization of the counts increases thelikelihood of the sample.
For the cuts, Gopalakr-ishnan?s inequality (Gopalakrishnan et al, 1991) isused.96Table 1: Statistics of the training corpus.
Vocabularyrefers to the number of different words.Language Sentences Words VocabularyRomanian 48 481 976 429 48 503English 48 481 1 029 507 27 053The initial values for the dictionary are trainedusing model 1 training and then a series of itera-tions are made updating the values of every param-eter.
Some additional considerations are taken intoaccount for efficiency reasons, see (Vilar and Vidal,2005) for details.A potential problem here is the large number ofparameters associated with cuts and directions: twofor each possible pair of words.
But, as we are in-terested only in aligning the corpus, no provision ismade for the data sparseness problem.3 The taskThe aim of the task was to align a set of 200 transla-tion pairs between Romanian and English.
As train-ing material, the text of 1984, the Romanian Con-stitution and a collection of texts from the Web wereprovided.
Some details about this corpus can be seenin Table 1.4 Splitting the corpusTo reduce the high computational costs of training ofthe parameters of MAR, a heuristic was employed inorder to split long sentences into smaller parts witha length less than l words.Suppose we are to split sentences x?
and y?.
Webegin by aligning each word in y?
to a word in x?.Then, a score and a translation is assigned to eachsubstring x?ji with a length below l. The translation isproduced by looking for the substring of y?
which hasa length below l and which has the largest numberof words aligned to positions between i and j. Thepair so obtained is given a score equal to sum of: (a)the square of the length of x?ji ; (b) the square of thenumber of words in the output aligned to the input;and (c) minus ten times the sum of the square of thenumber of words aligned to a nonempty position outof x?ji and the number of words outside the segmentchosen that are aligned to x?ji .These scores are chosen with the aim of reduc-ing the number of segments and making them as?complete?
as possible, ie, the words they cover arealigned to as many words as possible.After the segments of x?
are so scored, the partitionof x?
that maximizes the sum of scores is computedby dynamic programming.The training material was split in parts up to tenwords in length.
For this, an alignment was obtainedby training an IBM model 4 using GIZA++ (Och andNey, 2003).
The test pairs were split in parts up totwenty words.
After the split, there were 141 945training pairs and 337 test pairs.
Information wasstored about the partition in order to be able to re-cover the correct alignments later.5 Aligning the corpusThe parameters of the MAR were trained as ex-plained above: first ten IBM model 1 iterations wereused for giving initial values to the dictionary proba-bilities and then ten more iterations for retraining thedictionary together with the rest of the parameters.The alignment of a sentence pair has the form of atree similar to those in Figure 1.
Each interior nodehas two children corresponding to the translation ofthe two parts in which the input sentence is divided.The leaves of the tree correspond to those segmentsthat were translated by model 1.As the reference alignments do not have this kindof structure it is necessary to ?flatten?
them.
Theprocedure we have employed is very simple: if weare in a leaf, every output word is aligned to everyinput word; if we are in an interior node, the ?flat?alignments for the children are built and then com-bined.
Note that the way leaves are labeled tends tofavor recall over precision.The flat alignment corresponding to the trees ofFigure 1 are:economia si finantele publiceeconomy and public financeandWinston se intoarse brusc .Winston turned round abruptly .97economia si finantele publiceeconomy and public financeeconomia sieconomy andfinantele publicepublic financeeconomiaeconomysiandfinantelefinancepublicepublicWinston se intoarse brusc .Winston turned round abruptly .Winston se intoarseWinston turned roundbrusc .abruptly .WinstonWinstonse intoarseturned roundbruscabruptly..Figure 1: Two trees representing the alignment of two pair of sentences.Precision Recall F-Measure AER0.5404 0.6465 0.5887 0.4113Table 2: Results for the task6 Results and discussionThe results for the alignment can be seen in Ta-ble 2.
As mentioned above, there is a certain prefer-ence for recall over precision.
For comparison, us-ing GIZA++ on the split corpus yields a precisionof 0.6834 and a recall of 0.5601 for a total AERof 0.3844.Note that although the definition of the task al-lowed to mark the alignment as either probable orsure, we marked all the alignments as sure, so pre-cision and recall measures are given only for surealignments.There are aspects that deserve further experimen-tation.
The first is the split of the original corpus.It would be important to evaluate its influence, andto try to find methods of using MAR without anysplit at all.
A second aspect of great importance isthe method used for ?flattening?.
The way leavesof the tree are treated probably could be improvedif the dictionary probabilities were somehow takeninto account.7 ConclusionsWe have presented the experiments done using anew translation model for finding word alignmentsin parallel corpora.
Also, a method for splitting theinput before training the models has been presented.ReferencesLeonard E. Baum and J.
A. Eagon.
1967.
An inequal-ity with applications to statistical estimation for prob-abilistic functions of Markov processes and to a modelfor ecology.
Bulletin of the American MathematicalSociety, 73:360?363.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Yonggang Deng, Shankar Kumar, and William Byrne.2004.
Bitext chunk alignment for statistical machinetranslation.
Research Note 50, CLSP Johns HopkinsUniversity, April.P.
S. Gopalakrishnan, Dimitri Kanevsky, Arthur Na?das,and David Nahamoo.
1991.
An inequality for ra-tional functions with applications to some statisticalproblems.
IEEE Transactions on Information Theory,37(1):107?113, January.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Juan Miguel Vilar and Enrique Vidal.
2005.
A recursivestatistical translation model.
In Workshop on Build-ing and Using Parallel Texts, Ann-Arbour (Michigan),June.Juan Miguel Vilar Torres.
1998.
Aprendizaje de Tra-ductores Subsecuenciales para su empleo en tareasde dominio restringido.
Ph.D. thesis, Departamentode Sistemas Informa?ticos y Computacio?n, UniversidadPolite?cnica de Valencia, Valencia (Spain).
(in Span-ish).Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3):377?403.98
