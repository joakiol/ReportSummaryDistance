Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 63?71,Avignon, France, April 23 - 24 2012. c?2012 Association for Computational LinguisticsRecovering dialect geography from an unaligned comparable corpusYves ScherrerLATLUniversite?
de Gene`veGeneva, Switzerlandyves.scherrer@unige.chAbstractThis paper proposes a simple metric of di-alect distance, based on the ratio betweenidentical word pairs and cognate word pairsoccurring in two texts.
Different variationsof this metric are tested on a corpus contain-ing comparable texts from different SwissGerman dialects and evaluated on the basisof spatial autocorrelation measures.
The vi-sualization of the results as cluster dendro-grams shows that closely related dialectsare reliably clustered together, while mul-tidimensional scaling produces graphs thatshow high agreement with the geographiclocalization of the original texts.1 IntroductionIn the last few decades, dialectometry hasemerged as a field of linguistics that investigatesthe application of statistical and mathematicalmethods in dialect research.
Also called quanti-tative dialectology, one of its purposes is to dis-cover the regional distribution of dialect similari-ties from aggregated data, such as those collectedin dialectological surveys.The work presented here aims to apply dialec-tometric analysis and visualization techniques toa different type of raw data.
We argue that classi-cal dialectological survey data are word-alignedby design, whereas our data set, a comparablemultidialectal corpus, has to be word-aligned byautomatic algorithms.We proceed in two steps.
First, we present acognate identification algorithm that allows us toextract cognate word pairs from the corpus.
Then,we measure how many of these cognate wordpairs are identical.
This ratio gives us a measureof dialectal distance between two texts that is thenshown to correlate well with geographic distance.The visualization of the resulting data allows us torecover certain characteristics of the Swiss Ger-man dialect landscape.The paper is structured as follows.
In Section 2,the multidialectal corpus is presented.
We thendiscuss how this corpus differs from classical di-alectological data, and how we can use techniquesfrom machine translation to extract the relevantdata (Section 3).
In Section 4, we define dialectdistance as a function of the number of cognateword pairs and identical word pairs.
Both types ofword pairs are in turn defined by different thresh-olds of normalized Levenshtein distance.
Sec-tion 5 deals with the evaluation and visualizationof the resulting data, the latter in terms of clus-tering and multi-dimensional scaling.
We discussthe results and conclude in Section 6.2 Data: the Archimob corpusThe Archimob corpus used in our experiments isa corpus of transcribed speech, containing textsfrom multiple Swiss German dialects.The Archimob project was started in 1998 asan oral history project with the aim of gatheringand archiving the people?s memory of the SecondWorld War period in Switzerland.1 555 survivingwitnesses were interviewed in all Swiss languageregions.
The interviews of the German-speakingwitnesses were conducted in their local dialect.With the goal of obtaining spontaneous di-alect data to complement ongoing work on di-alect syntax (Bucheli and Glaser, 2002; Friedli,2006; Steiner, 2006), researchers at the Univer-1Archimob stands for ?Archives de la mobilisation?
; seewww.archimob.ch.63BE1142: de vatter ?`sch lokomitiiffu?erer gs?`?` / de ?`sch dispensiert gs?`?` vom dienscht nattu?rlech / und/ zwo schwo?schtere / hani ghaa / wobii ei gsch / eini gschtoorben ?`sch u di ander ?`sch ?`schime autersheim / u soo bini ufgwachse ir lenggass / mit em / pruefsleer / mit wiiterbiudigna?chheer / ( ?
)Translation: the father has been a train driver / he has been dispensed from military service of course /and / two sisters / I have had / where one / one has died and the other is is in a home for theelderly / this is how I have grown up in the Lenggass / with a / apprenticeship / with furthereducation afterwards / ( ?
)ZH1270: min vatter isch / eh eeh / schlosser ha?t er gleert / und und isch aber da?n schoffo?o?r woordedur en verwante wo bim S. z zu?ri / gschafft ha?t und de` ha?t gsait / chum tue doch umsattlebim S. vediensch mee / und da?n ha?t de`e` schoffo?o?r gleert und das isch doozmaal ja na eeneren sa?ltene pruef gsii / da?n ha?t de` das gleert und ich bin schtolz gsii das min / vatter en / pruefghaa ha?t wo franzo?sischsch to?o?nt ha?t oder schoffo?o?r / ich han gfunde das seig en waansinigepruefTranslation: my father has / eh eeh / been a locksmith apprentice / and and has then become a driverthrough a relative who has worked at S. in Zurich and he said / come and switch jobs, at S.you earn more / and then he was a driver apprentice and this was rather a rare job at thattime / so he learned that and I was proud that my / father / had a job which sounded French,you know, chauffeur / I found that this was an extraordinary jobFigure 1: Excerpts of two informants?
turns in the Archimob corpus.
The excerpts contain identical cognate pairslike ?vatter, vatter?, and non-identical cognate pairs like ?
?`sch, isch?.sity of Zurich selected a subset of the Swiss Ger-man Archimob interviews and transcribed them.2The selection process ensured that only interviewsfrom non-mobile speakers (speakers that have notspent long periods of their life outside of their na-tive town) were retained, and that the most impor-tant dialect areas of German-speaking Switzer-land were represented.As a result, 16 interviews were selected fortranscription, amounting to 26 hours of speech.All texts were anonymized.
In order to ensureconsistency, all texts were transcribed by the sameperson.The interviews were transcribed using thespelling system of Dieth (1986).
This is an ortho-graphic transcription system which intends to beas phonetically transparent as possible, while re-maining readable for readers accustomed to Stan-dard German orthography (see Figure 1 for twoexamples).
For instance, the Dieth guidelines dis-tinguish ?` (IPA [I]) from i (IPA [i]), while Stan-dard German spelling only uses i.In our experiments, we discarded the inter-viewer?s questions and only used the witnesses?turns.
The whole corpus contains 183 000 words,with individual interviews ranging from 6 500 to16 700 words.
Excerpts of two interviews are2The corpus is not yet publicly available, awaiting thecompletion of further annotation layers.shown in Figure 1.
The place of residence of thewitness was given in the corpus metadata.It should be stressed that our data set is verysmall in comparison with other studies in the field:it contains 16 data points (texts) from 15 differentlocations.
Moreover, some dialect areas are notrepresented in the sample (e.g.
Graubu?nden in theSouth-East and Fribourg in the West).3 Therefore,the goal of the present study cannot be to inducea precise dialect landscape of German-speakingSwitzerland.
Rather, we aim to find out if geo-graphically close texts can be shown to be linguis-tically close, and if the most important dialectaldivisions of German-speaking Switzerland are re-flected in the classification of the texts.3 Corpora and word alignment3.1 Comparable corporaThe machine translation community generallydistinguishes between parallel and comparablecorpora (McEnery and Xiao, 2008).
A parallelcorpus consists of a source text and its transla-tions into other languages.
Hence, the differentlanguage versions share the same content and thesame order of paragraphs and sentences.
On theother hand, such corpora have been criticized forcontaining ?translationese?, i.e., wording which3For an overview of the geographic distribution of thetexts, see Figure 3.64is influenced by the grammatical and informa-tional structure of the source text and which is notnecessarily representative of the target language.In contrast, a comparable corpus is a collectionof original texts of different languages that sharesimilar form and content (typically, same genre,same domain and same time period).The Archimob corpus can be qualified as com-parable: all texts deal with the same subject andthe same time period (life in Switzerland at theoutbreak of the Second World War), and they arecollected in the same way, in the form of oral in-terviews guided by an interviewer.3.2 Word alignment in dialectologyDialectological analyses rely on word-aligneddata.
Traditionally, dialectological data are col-lected in surveys with the help of questionnaires.A typical question usually intends to elicit the lo-cal words or pronunciations of a given concept.The mere fact that two responses are linked to thesame question number of the questionnaire suf-fices to guarantee that they refer to the same con-cept.
This property leads us to consider dialecto-logical survey data as word-aligned by design.In contrast, the Archimob corpus is not aligned.Again, algorithms for aligning words in paralleland comparable corpora have been proposed inthe field of machine translation.
For large par-allel corpora, distributional alignment methodsbased solely on cooccurrence statistics are suffi-cient (Och and Ney, 2003; Koehn et al, 2007).For comparable corpora, the order and frequencyof occurrence of the words cannot be used asalignment cues.
Instead, the phonetic and ortho-graphic structures are used to match similar wordpairs (Simard et al, 1992; Koehn and Knight,2002; Kondrak and Sherif, 2006).
Obviously, thisapproach only works for cognate word pairs ?word pairs with a common etymology and simi-lar surface forms.
This task is known as cognateidentification.In the next section, we detail how cognate iden-tification is used to compute the distance betweendifferent dialect versions of a comparable corpus.4 Computing the linguistic similarity oftwo comparable textsThe hypothesis put forward in this paper is thatthe linguistic similarity of two comparable textscan be approximated by the degree of similarityof the cognate word pairs occurring in the texts.Computing the similarity of two texts amounts tothe following two tasks:1.
Given two texts, extract the set of word pairsthat are considered cognates.
This corre-sponds to the cognate identification task pre-sented above.2.
Given a set of cognate word pairs, determinethe proportion of word pairs that are consid-ered identical.The underlying intuition is that identically pro-nounced cognate words account for evidence thatthe two dialects are closely related, whereas dif-ferently pronounced cognate words are evidencethat the two dialects are distant.
Word pairs thatare not cognates are not relevant for our similaritymeasure.Let us illustrate the idea with an example:(1) es schto`o`t n?`d(2) wil si n?`d schtootIntuitively, two cognate word pairs can be foundin the texts (1) and (2): ?schto`o`t, schtoot?
and?n?`d, n?`d?.4 The words es, wil, si do not have cog-nate equivalents in the other text.
As a result, thetwo texts have a similarity of 12 , one of the twocognate pairs consisting of identical words.In the example above, we have assumed infor-mal meanings of cognate word pair and identicalword pair.
In the following sections, we definethese concepts more precisely.4.1 Identifying cognate word pairsMost recently proposed cognate identification al-gorithms are based on variants of Levenshtein dis-tance, or string edit distance (Levenshtein, 1966;Heeringa et al, 2006; Kondrak and Sherif, 2006).Levenshtein distance is defined as the smallestnumber of insertion, deletion and substitution op-erations required to transform one string into an-other.
(3)b i i s c h p i i ub i s c h p i l0 0 1 0 0 0 0 0 1 14Accented and unaccented characters are considered asdifferent.
See footnote 5.65Example (3) shows two words and the associatedoperation costs.
There are two deletion operationsand one substitution operation, hence Levenshteindistance between biischpiiu and bischpil is 3.5Among other proposals, Heeringa et al (2006)suggest normalizing Levenshtein distance by thelength of the alignment.
The underlying ideais that a Levenshtein distance of 2 for two longwords does not mean the same as a Levenshteindistance of 2 for two very short words.
In exam-ple (3), the length of the alignment is 10 (in thiscase, it is equal to the length of the longer word).Normalized Levenshtein distance is 310 = 0.3.A cognate identification algorithm based onnormalized Levenshtein distance requires athreshold such that only those word pairs whosedistance is below the threshold are consideredcognates.
In order to identify sensible values forthis threshold, we classified all word pairs of thecorpus according to their distance.
We evaluatednine thresholds between 0.05 and 0.4 to see if theyeffectively discriminate cognate pairs from non-cognate pairs.
The evaluation was done on thebasis of 100 randomly selected word pairs witha normalized Levenshtein distance lower or equalthan the respective threshold.In this evaluation, we distinguish between formcognates ?
words that represent the same inflectedforms of the same lemma ?, and lemma cognates?
words that represent different inflected forms ofthe same lemma.
Example (4) is a form cognatepair: it shows two dialectally different realiza-tions of the singular form of the Standard Germanlemma Gemeinde ?municipality?.
Example (5) isonly a lemma cognate pair: one of the word con-tains the plural ending -e, while the other word isa singular form.
(4) gmeind ?
gmaind(5) gmeind ?
gmaindeTable 1 shows the results of this evaluation.
Asthe distance threshold increases, the proportionof cognates drops while the proportion of non-cognates rises.
With thresholds higher than 0.25,the number of non-cognates surpasses the number5Note that we treat all characters in the same way: replac-ing o by k yields the same cost as replacing it by u or by o`.This simple approach may not be the optimal solution whendealing with similar dialects.
This issue will be addressed infuture work.of cognates.
We therefore expect the cognate de-tection algorithm to work best below this thresh-old.Let us conclude this section by some additionalremarks about the evaluation:?
The distinction between form cognates andlemma cognates cannot be easily opera-tionalized with an automatic approach.
Forinstance, the correspondance u ?
u?
may be aphonological one and distinguish two iden-tical forms of different dialects.
But it mayalso be a morphological correspondence thatdistinguishes singular from plural forms in-dependently of the dialect.
In the followingexperiments, we treat both types of cognatepairs in the same way.?
In practice, the reported figures are mea-sures of precision.
Recall may be estimatedby the number of cognates situated above agiven threshold.
While we have not eval-uated the entire distance interval, the givenfigures suggest that many true cognates areindeed found at high distance levels.
Thisissue may be addressed by improving thestring distance metric.?
Ambiguous words were not disambiguatedaccording to the syntactic context and the di-alect.
As a result, all identical word pairs(threshold 0.00) are considered form cog-nates, although some of them may be falsefriends.4.2 Identifying identical wordsIn common understanding, an identical word pairis a pair of words whose Levenshtein distance is0.
In some of the following experiments, we adoptthis assumption.However, we found it useful to relax this defi-nition in order to avoid minor inconsistencies inthe transcription and to neglect the smallest di-alect differences.
Therefore, we also carried outexperiments where identical word pairs were de-fined as having a normalized Levenshtein distanceof 0.10 or lower.4.3 ExperimentsRecall that we propose to measure the linguis-tic similarity of two texts by the ratio of iden-tical word pairs among the cognate word pairs.66Distance Word Form Lemma All Non- Non-threshold pairs cognates cognates cognates cognates words0.00 5230 100% 0% 100% 0% 0%0.05 5244 98% 0% 98% 0% 2%0.10 6611 94% 4% 98% 1% 1%0.15 10674 79% 16% 95% 4% 1%0.20 18582 55% 16% 71% 29% 0%0.25 27383 48% 13% 61% 38% 1%0.30 36002 40% 12% 52% 47% 1%0.35 49011 29% 10% 39% 61% 0%0.40 65955 20% 13% 33% 67% 0%Table 1: Manual evaluation of the cognate identification task.
Percentages are based on a random sample of 100word pairs with a normalized Levenshtein distance below or equal to the given threshold.
Form cognate andlemma cognate counts are summed up in the ?All cognates?
column.
The interviewees sometimes made falsestarts and stopped in the middle of the word; these incomplete words, together with obvious typing errors in thetranscription, are counted in the last column.Cognate pairs as well as identical word pairs arecharacterized by different thresholds of normal-ized Levenshtein distance.
We experiment withthresholds of 0.20, 0.25, 0.30, 0.35 and 0.40 forcognate word pairs, and with thresholds of 0 and0.10 for identical word pairs.4.4 Normalization by text lengthA major issue of using comparable corpora is thelarge variation in text length and vocabulary use.This has to be accounted for in our experiments.First, all counts refer to types of word pairs, nottokens.
We argue that the frequency of a word ina given text depends too much on the content ofthe text and is not truly representative of its di-alect.
Second, if few identical words are found,this does not necessarily mean that the two textsare dialectally distant, but may also be becauseone text is much shorter than the other.
Hence, theproportion of identical words is normalized by thenumber of cognate words contained in the shorterof the two texts.5 Evaluation and visualisationBy computing the linguistic distance for allpairs of texts in our corpus, we obtain a two-dimensional distance matrix.
Recent dialectomet-ric tradition provides several techniques to evalu-ate and visualize the data encoded in this matrix.First, one can measure how well the lin-guistic distances correlate with geographic dis-tances (Section 5.1).
Second, one can group thetexts into maximally homogeneous clusters (Sec-tion 5.2).
Third, one can plot the texts as datapoints on a two-dimensional graph and visuallycompare this graph with the geographical loca-tions of the texts (Section 5.3).5.1 Numerical measures of spatialautocorrelationA general postulate of spatial analysis is that ?onaverage, values at points close together in spaceare more likely to be similar than points furtherapart?
(Burrough and McDonnell, 1998, 100).This idea that the distance of attribute values cor-relates with their geographical distance is knownas spatial autocorrelation.
The same idea hasbeen coined the fundamental dialectological pos-tulate by Nerbonne and Kleiweg (2005, 10): ?Ge-ographically proximate varieties tend to be moresimilar than distant ones.
?Here, we use this postulate to evaluate the dif-ferent threshold combinations of our dialect sim-ilarity measure: the higher a threshold combi-nation correlates with geographic distance (i.e.,places of residence of the interviewees), the betterit is able to discriminate the dialects.
Here, the re-sults obtained with two correlation measures arereported.Local incoherence has been proposed by Ner-bonne and Kleiweg (2005).
The idea of this mea-sure is that the correlation between linguistic andgeographic distances is local and does not need tohold over larger geographical distances.
In prac-tice, for every data point, the 8 linguistically most67similar points6 are inspected according to theirlinguistic distance value.
Then, the geographicdistance of these pairs of points is measured andsummed up.
This means that high incoherencevalues represent poor measurements, while lowervalues stand for better results.The Mantel-Test (Sokal and Rohlf, 1995, 813-819) is a general statistical test which applies todata expressed as dissimilarities.
It is often usedin evolutionary biology and ecology, for example,to correlate genetic distances of animal popula-tions with the geographic distances of their range.The Mantel coefficient Z is computed by com-puting the Hadamard product of the two matri-ces.
The statistical significance of this coefficientis obtained by a randomization test.
A sample ofpermutations is created, whereby the elements ofone matrix are randomly rearranged.
The corre-lation level depends on the proportion of sampleswhose Z-value is higher than the Z-value of thereference matrix.
All experiments were carriedout with a sample size of 999 permutations, whichcorresponds to a simulated p-value of 0.001.Table 2 shows the results of both correlationmeasures for all experiments.
These results arein line with the manual evaluation of Table 1.
Atfirst, increasing the cognate pair threshold leadsto more data, and in consequence, to better re-sults.
Above 0.35 however, the added data is es-sentially noise (i.e., non-cognate pairs), and theresults drop again.According to local incoherence, the best thresh-old combination is ?0.10, 0.35?.
In terms of Man-tel test correlation, the ?0.10, 0.25?
threshold per-forms slightly better.
Adopting an identical pairthreshold of 0.00 results in slightly inferior corre-lations.5.2 ClusteringThe distance matrix can also be used as input to aclustering algorithm.
Clustering has become oneof the major data analysis techniques in dialec-tometry (Mucha and Haimerl, 2005), but has alsobeen used with plain text data in order to improveinformation retrieval (Yoo and Hu, 2006).Hierarchical clustering results in a dendrogramwhich represents the distances between every twodata points as a tree.
However, clustering is6The restriction to 8 points is the key of the local compo-nent of this measure.
The exact value of this parameter hasbeen determined empirically by the authors of the measure.Distance thresholds Local Mantel TestIdentical Cognate inc. r p0.00 0.20 0.59 0.56 0.0010.25 0.47 0.68 0.0010.30 0.49 0.66 0.0010.35 0.41 0.70 0.0010.40 0.46 0.65 0.0010.10 0.20 0.55 0.65 0.0010.25 0.41 0.73 0.0010.30 0.43 0.70 0.0010.35 0.37 0.72 0.0010.40 0.43 0.67 0.001Table 2: Correlation values for the different experi-ments.
The first and second columns define each ex-periment in terms of two Levenshtein distance thresh-olds.
For local incoherence, lower values are better.For the Mantel test figures, we report the correlationcoefficient r as well as the significance level p.known to be unreliable: small changes in the dis-tance matrix may result in completely differentdendrograms.
To counter this issue, noisy clus-tering has been proposed (Nerbonne et al, 2008):clustering is repeated 100 times, and at each run,random amounts of noise are added to the differ-ent cells of the distance matrix.
This gives anindication of the reliability of the resulting clus-ters.
Figure 2 shows a dendrogram obtained withnoisy clustering.
We used both group averageand weighted average clustering algorithms, anda noise level of 0.2.7 Figure 3 localizes the datapoints on a geographical map.
All clusters show areliability score of 92% or above.Clustering allows us to recover certain charac-teristics of the Swiss German dialect landscape.First, texts from the same canton (whose IDs con-tain the same two-letter abreviation) are groupedtogether with high reliability.
Second, the dendro-gram shows ?
albeit with lower reliability scores?
a three-fold East-West stratification with blueregions in the West (BE), green regions in Cen-tral Switzerland (AG, LU) and yellow areas in theEast (ZH, SZ, GL).
The border between Westernand Central dialects roughly corresponds to theso-called Bru?nig-Napf line.
The border betweenCentral and Eastern varieties is also confirmedby former dialectological research (Haas, 1982;Hotzenko?cherle, 1984).
Third, three dialects are7These are the default settings of the Gabmap program(Nerbonne et al, 2011).68AG1147LU1195 100AG1063100LU1261100BE1142BE1170 100BL10739292GL1048GL1207 100ZH1143ZH1270 100SZ120910092NW1007100BS1057SG1198100VS12121000.0 0.2 0.4 0.6Figure 2: Dendrogram obtained with a threshold setting of ?0.10, 0.35?.
The scale at the bottom of the graphicsrepresents the distance of the clusters, while the numbers on the vertical lines represent the reliability of theclusters (i.e.
in how many of the 100 runs a cluster has been found).BS1057BL1073BE1142BE1170VS1212SG1198GL1207GL1048SZ1209NW1007LU1195LU1261ZH1143ZH1270AG1147AG1063Figure 3: Geographic localization of the Archimobtexts, according to the place of residence of the in-terviewed persons.
The colors represent the linguisticdistance between texts; they correspond to the colorsused in the dendrogram of Figure 2.clearly considered as outliers: the Northwesterndialect of Basel (BS1057), the Northeastern di-alect of St. Gallen (SG1198), and most of all theSouthwestern Wallis dialect (VS1212).
Again,these observations are in line with common di-alectological knowledge.5.3 Multidimensional scalingThe Swiss German dialect landscape has beenknown to feature major East-West divisions (seeabove) as well as several levels of stratificationon the North-South axis.
Our hypothesis is thatthe linguistic distances represented in the distancematrix should be able to recover this mainly two-dimensional organization of Swiss German di-alects.
Since the distance matrix defines a multi-dimensional space in which all data points (texts)are placed, this space has to be reduced to two di-mensions.
For this purpose, we use multidimen-sional scaling.
If the linguistic distances are cor-rectly defined and the multidimensional scalingalgorithm truly extracts the two main dimensionsof variation, the resulting two-dimensional graphshould be comparable with a geographic map.Figure 4 shows the resulting graph for one ex-periment.
Figures 5 and 6 show the values of eachdata point in grey levels for the two first dimen-sions obtained by multi-dimensional scaling.One observes that the localization of datapoints in Figure 4 closely corresponds to theirgeographic location (as illustrated in Figure 3):the major North-South divisions as well as someEast-West divisions are clearly recovered.More surprisingly, the two main dimensions ofmultidimensional scaling correspond to diagonalsin geographic terms.
The first dimension (Fig-ure 5) allows to distinguish Northwestern fromSoutheastern variants, while the second dimen-sion (Figure 6) distinguishes Northeastern fromSouthwestern variants.
Instead of +-shaped di-alect divisions put forward by traditional dialec-tology, our approach rather finds X-shaped dialectdivisions.6 Discussion and future workWe have proposed a simple measure that approx-imates the linguistic distance between two textsaccording to the ratio of identical words amongthe cognate word pairs.
The definitions of iden-tical word pair and cognate word pair are op-erationalized with fixed thresholds of normalized69BS1057BL1073BE1142BE1170VS1212NW1007GL1207GL1048ZH1143SZ1209SG1198ZH1270LU1261LU1195AG1147AG1063Figure 4: Plot representing the first two dimensionsof multi-dimensional scaling applied to the experimentwith ?0.10, 0.35?
thresholds.Figure 5: Map representing the first dimension ofmulti-dimensional scaling (same experiment as Fig.
4).Figure 6: Map representing the second dimension ofmulti-dimensional scaling (same experiment as Fig.
4).Levenshtein distance.
The resulting distance ma-trix has been analyzed with correlation measures,and visualized with clustering and multidimen-sional scaling techniques.
The visualizations rep-resent the main characteristics of the Swiss Ger-man dialect landscape in a surprisingly faithfulway.The close relation obtained among texts fromthe same canton may suggest that the distancemeasure is biased towards proper nouns.
For ex-ample, two Zurich German texts are more likelyto use toponyms from the Zurich region thana Bernese German text.
If there are many ofthese (likely identically pronounced) toponyms,the similarity value will increase.
However, man-ual inspection of the relevant texts did not showsuch an effect.
Region-specific toponyms are rare.The results suggest that a more fine-grainedvariant of Levenshtein distance might be useful.In the following paragraphs, we present severalimprovements for future work.The results suggest that a more fine-grainedvariant of Levenshtein distance might improve theprecision and recall of the cognate detection al-gorithm.
Notably, it has been found that vowelschange more readily than consonant in closely re-lated language varieties.
In consequence, chang-ing one vowel by another should be penalized lessthan changing a vowel by a consonant (Mann andYarowsky, 2001).
The same holds for accentedvs.
non-accented characters.
Complex graphemesrepresenting a single phoneme appear rather fre-quently in the Dieth transcription system (e.g.
forlong vowels) and should also be treated sepa-rately.We should also mention that the proposedmethod likely faces a problem of scale.
Indeed,each word of each text has to be compared witheach word of each text.
This is only manageablewith a small corpus like ours.We conclude by pointing out a limitation of thisapproach: the automatic alignment process basedon the concept of cognate pairs obviously onlyworks for phonetically related word pairs.
Thiscontrasts with other dialectometric approachesbased on lexical differences, in whose data setsdifferent lemmas have been aligned.
Future workon the Archimob corpus shall add normalizationand lemmatization layers.
This information couldbe useful to improve word alignment beyond cog-nate pairs.70AcknowledgmentsThe author wishes to thank Prof. Elvira Glaser,Alexandra Bu?nzli, Anne Go?hring and AgnesKolmer (University of Zurich) for granting accessto the Archimob corpus and giving detailed infor-mation about its constitution.
Furthermore, theanonymous reviewers are thanked for most help-ful remarks.ReferencesClaudia Bucheli and Elvira Glaser.
2002.
The syn-tactic atlas of Swiss German dialects: empirical andmethodological problems.
In Sjef Barbiers, LeonieCornips, and Susanne van der Kleij, editors, Syn-tactic Microvariation, volume II.
Meertens InstituteElectronic Publications in Linguistics, Amsterdam.Peter A. Burrough and Rachael A. McDonnell.
1998.Principles of Geographical Information Systems.Oxford University Press, Oxford.Eugen Dieth.
1986.
Schwyzertu?tschi Diala?ktschrift.Sauerla?nder, Aarau, 2nd edition.Matthias Friedli.
2006.
Der Komparativan-schluss im Schweizerdeutschen ?
ein raumbilden-des Pha?nomen.
In Hubert Klausmann, editor,Raumstrukturen im Alemannischen, pages 103?108.Neugebauer, Graz/Feldkirch.Walter Haas, 1982.
Die deutschsprachige Schweiz,pages 71?160.
Benziger, Zu?rich.Wilbert Heeringa, Peter Kleiweg, Charlotte Gooskens,and John Nerbonne.
2006.
Evaluation of string dis-tance algorithms for dialectology.
In Proceedingsof the ACL 2006 Workshop on Linguistic Distances,pages 51?62, Sydney, Australia.Rudolf Hotzenko?cherle.
1984.
Die Sprachlandschaf-ten der deutschen Schweiz.
Sauerla?nder, Aarau.Philipp Koehn and Kevin Knight.
2002.
Learning atranslation lexicon from monolingual corpora.
InProceedings of the ACL 2002 Workshop on Unsu-pervised Lexical Acquisition (SIGLEX 2002), pages9?16, Philadelphia, PA.Philipp Koehn, Hieu Hoang, Alexandra Birch,Chris Callison-Burch, Marcello Federico, NicolaBertoldi, Brooke Cowan, Wade Shen, ChristineMoran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of the ACL 2007 demon-stration session, Prague, Czech Republic.Grzegorz Kondrak and Tarek Sherif.
2006.
Evaluationof several phonetic similarity algorithms on the taskof cognate identification.
In Proceedings of the ACL2006 Workshop on Linguistic Distances, pages 43?50, Sydney, Australia.Vladimir I. Levenshtein.
1966.
Binary codes capa-ble of correcting deletions, insertions, and reversals.Soviet Physics Doklady, 10(8):707?710.Gideon S. Mann and David Yarowsky.
2001.
Mul-tipath translation lexicon induction via bridge lan-guages.
In Proceedings of NAACL 2001, Pittsburgh,PA, USA.Tony McEnery and Richard Xiao.
2008.
Parallel andcomparable corpora: What is happening?
In Gu-nilla Anderman and Margaret Rogers, editors, In-corporating Corpora: The Linguist and the Trans-lator, chapter 2, pages 18?31.
Multilingual Matters,Clevedon.Hans-Joachim Mucha and Edgar Haimerl.
2005.
Au-tomatic validation of hierarchical cluster analysiswith application in dialectometry.
In C. Weihs andW.
Gaul, editors, Classification ?
the UbiquitousChallenge, pages 513?520.
Springer, Berlin.John Nerbonne and Peter Kleiweg.
2005.
Toward adialectological yardstick.
Journal of QuantitativeLinguistics, 5.John Nerbonne, Peter Kleiweg, Wilbert Heeringa, andFranz Manni.
2008.
Projecting dialect differ-ences to geography: Bootstrap clustering vs. noisyclustering.
In Christine Preisach, Lars Schmidt-Thieme, Hans Burkhardt, and Reinhold Decker, ed-itors, Data Analysis, Machine Learning, and Appli-cations.
Proceedings of the 31st Annual Meeting ofthe German Classification Society, pages 647?654.Springer, Berlin.John Nerbonne, Rinke Colen, Charlotte Gooskens, Pe-ter Kleiweg, and Therese Leinonen.
2011.
Gabmap?
a web application for dialectology.
Dialectologia,Special Issue, II:65?89.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Michel Simard, George F. Foster, and Pierre Isabelle.1992.
Using cognates to align sentences in bilin-gual corpora.
In Proceedings of the Fourth Inter-national Conference on Theoretical and Method-ological Issues in Machine Translation (TMI 1992),pages 67?81, Montre?al, Canada.Robert R. Sokal and F. James Rohlf.
1995.
Biometry:the principles and practice of statistics in biologicalresearch.
W.H.
Freeman, New York, 3rd edition.Janine Steiner.
2006.
Syntaktische Variation in derNominalphrase ?
ein Fall fu?r die Dialektgeographinoder den Soziolinguisten?
In Hubert Klausmann,editor, Raumstrukturen im Alemannischen, pages109?115.
Neugebauer, Graz/Feldkirch.Illhoi Yoo and Xiaohua Hu.
2006.
A comprehen-sive comparison study of document clustering for abiomedical digital library MEDLINE.
In Proceed-ings of the 6th ACM/IEEE-CS joint conference onDigital libraries, JCDL ?06, pages 220?229, ChapelHill, NC, USA.71
