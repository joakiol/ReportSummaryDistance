Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 459?464,Dublin, Ireland, August 23-24, 2014.Peking: Profiling Syntactic Tree Parsing Techniques for Semantic GraphParsingYantao Du, Fan Zhang, Weiwei Sun and Xiaojun WanInstitute of Computer Science and Technology, Peking UniversityThe MOE Key Laboratory of Computational Linguistics, Peking University{duyantao,ws,wanxiaojun}@pku.edu.cn, zhangf717@gmail.comAbstractUsing the SemEval-2014 Task 8 data, weprofile the syntactic tree parsing tech-niques for semantic graph parsing.
In par-ticular, we implement different transition-based and graph-based models, as well asa parser ensembler, and evaluate their ef-fectiveness for semantic dependency pars-ing.
Evaluation gauges how successfuldata-driven dependency graph parsing canbe by applying existing techniques.1 IntroductionBi-lexical dependency representation is quite pow-erful and popular to encode syntactic or semanticinformation, and parsing techniques under the de-pendency formalism have been well studied andadvanced in the last decade.
The major focus islimited to tree structures, which fortunately corre-spond to many computationally good properties.On the other hand, some leading linguistic theo-ries argue that more general graphs are needed toencode a wide variety of deep syntactic and se-mantic phenomena, e.g.
topicalization, relativeclauses, etc.
However, algorithms for statisticalgraph spanning have not been well explored be-fore, and therefore it is not very clear how gooddata-driven parsing techniques developed for treeparsing can be for graph generating.Following several well-established syntactictheories, SemEval-2014 task 8 (Oepen et al.,2014) proposes using graphs to represent seman-tics.
Considering that semantic dependency pars-ing is a quite new topic and there is little previ-ous work, we think it worth appropriately profil-ing successful tree parsing techniques for graphparsing.
To this end, we build a hybrid systemThis work is licenced under a Creative Commons Attribution4.0 International License.
Page numbers and proceedingsfooter are added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/that combines several important data-driven pars-ing techniques and evaluate their impact with thegiven data.
In particular, we implement differenttransition-based and graph-based models, as wellas a parser ensembler.Our experiments highlight the following facts:?
Graph-based models are more effective thantransition-based models.?
Parser ensemble is very useful to boost theparsing accuracy.2 ArchitectureWe explore two kinds of basic models: One istransition-based, and the other is tree approxima-tion.
Transition-based models are widely used fordependency tree parsing, and they can be adaptedto graph parsing (Sagae and Tsujii, 2008; Titovet al., 2009).
Here we implement 5 transition-based models for dependency graph parsing, eachof which is based on different transition system.The motivation of developing tree approxima-tion models is to apply existing graph-based treeparsers to generate graphs.
At the training time,we convert the dependency graphs from the train-ing data into dependency trees, and train second-order arc-factored models1.
At the test phase, weparse sentences using this tree parser, and convertthe output trees back into semantic graphs.
Wethink tree approximation can appropriately evalu-ate the possible effectiveness of graph-based mod-els for graph spanning.Finally, we integrate the outputs of differentmodels with a simple voter to boost the perfor-mance.
The motivation of using system combi-nation and the choice of voting is mainly due tothe experiments presented by (Surdeanu and Man-ning, 2010).
When we obtain all the outputs of1The mate parser (code.google.com/p/mate-tools/) is used.459these models, we combine them into a final result,which is better than any of them.
For combination,we explore various systems for this task, since em-pirically we know that variety leads to better per-formance.3 Transition-Based ModelsTransition-based models are usually used for de-pendency tree parsing.
For this task, we exploit itfor dependency graph parsing.A transition system S contains a set C of con-figurations and a set T of transitions.
A configu-ration c ?
C generally contains a stack ?
of nodes,a buffer ?
of nodes, and a set A of arcs.
The ele-ments in A is in the form (x, l, y), which denotesa arc from x to y labeled l. A transition t ?
T canbe applied to a configuration and turn it into a newone by adding new arcs or manipulating elementsof the stack or the buffer.
A statistical transition-based parser leverages a classifier to approximatean oracle that is able to generate target graphs bytransforming the initial configuration cs(x) into aterminal configuration ct?
Ct.An oracle of a given graph on sentence x is asequence of transitions which transform the initialconfiguration to the terminal configuration the arcset Actof which is the set of the arcs of the graph.3.1 Our Transition SystemsWe implemented 5 different transition systems forgraph parsing.
Here we describe two of themin detail, one is the Titov system proposed in(Titov et al., 2009), and the other is our Naivesystem.
The configurations of the two systemseach contain a stack ?, a buffer ?, and a set A ofarcs, denoted by ?
?, ?,A?.
The initial configura-tion of a sentence x = w1w2?
?
?wnis cs(x) =?
[0], [1, 2, ?
?
?
, n], {}?, and the terminal configu-ration set Ctis the set of all configurations withempty buffer.
These two transition systems areshown in 1.The transitions of the Titov system are:?
LEFT-ARCladds an arc from the front of thebuffer to the top of the stack, labeled l, intoA.?
RIGHT-ARCladds an arc from the top of thestack to the front of the buffer, labeled l, intoA.?
SHIFT removes the front of the buffer andpush it onto the stack;?
POP pops the top of the stack.?
SWAP swaps the top two elements of thestack.This system uses a transition SWAP to change thenode order in the stack, thus allowing some cross-ing arcs to be built.The transitions of the Naive system are similarto the Titov system?s, except that we can directlymanipulate all the nodes in the stack instead of justthe top two.
In this case, the transition SWAP is notneeded.The Titov system can cover a great proportion,though not all, of graphs in this task.
For morediscussion, see (Titov et al., 2009).
The Naivesystem, by comparison, covers all graphs.
Thatis to say, with this system, we can find an oraclefor any dependency graph on a sentence x. Othertransition systems we build are also designed fordependency graph parsing, and they can cover de-pendency graphs without self loop as well.3.2 Statistical DisambiguationFirst of all, we derive oracle transition sequencesfor every sentence, and train Passive-Aggressivemodels (Crammer et al., 2006) to predict next tran-sition given a configuration.
When it comes toparsing, we start with the initial configuration, pre-dicting next transition and updating the configura-tion with the transition iteratively.
And finally wewill get a terminal configuration, we then stop andoutput the arcs of the graph contained in the finalconfiguration.We extracted rich feature for we utilize a setof rich features for disambiguation, referencing toZhang and Nivre (2011).
We examine the severaltops of the stack and the one or more fronts of thebuffer, and combine the lemmas and POS tags ofthem in many ways as the features.
Additionally,we also derive features from partial parses such asheads and dependents of these nodes.3.3 Sentence ReversalReversing the order the words of a given sentenceis a simple way to yield heterogeneous parsingmodels, thus improving parsing accuracy of themodel ensemble (Sagae, 2007).
In our experi-ments, one transition system produces two mod-els, one trained on the normal corpus, and the otheron the corpus of reversed sentences.
Therefore wecan get 10 parse of a sentence based on 5 transitionsystems.460LEFT-ARCl(?|i, j|?,A)?
(?|i, j|?,A ?
{(j, l, i)})RIGHT-ARCl(?|i, j|?,A)?
(?|i, j|?,A ?
{(i, l, j)})SHIFT (?, j|?,A)?
(?|j, ?,A)POP (?|i, ?, A)?
(?, ?,A)SWAP (?|i|j, ?,A)?
(?|j|i, ?, A)Titov SystemLEFT-ARCkl(?|ik| .
.
.
|i2|i1, j|?,A)?
(?|ik| .
.
.
|i2|i1, j|?,A ?
{(j, l, ik)})RIGHT-ARCkl(?|ik| .
.
.
|i2|i1, j|?,A)?
(?|ik| .
.
.
|i2|i1, j|?,A ?
{(ik, l, j)})SHIFT (?, j|?,A)?
(?|j, ?,A)POPk(?|ik|ik?1| .
.
.
|i2|i1, ?, A)?
(?|ik?1| .
.
.
|i2|i1, ?, A)Naive SystemFigure 1: Two of our transition systems.4 Tree Approximation ModelsParsing based on graph spanning is quite challeng-ing since computational properties of the seman-tic graphs given by the shared task are less ex-plored and thus still unknown.
On the other hand,finding the best higher-order spanning for generalgraph is NP complete, and therefore it is not easy,if not impossible, to implement arc-factored mod-els with exact inference.
In our work, we use apractical idea to indirectly profile the graph-basedparsing techniques for dependency graph parsing.Inspired by the PCFG approximation idea (Fowlerand Penn, 2010; Zhang and Krieger, 2011) fordeep parsing, we study tree approximation ap-proaches for graph spanning.This tree approximation technique can be ap-plied to both transition-based and graph-basedparsers.
However, since transition systems thatcan directly handle build graphs have been devel-oped, we only use this technique to evaluate thepossible effectiveness of graph-based models forsemantic parsing.4.1 Graph-to-Tree TransformationIn particular, we develop different methods to con-vert a semantic graph into a tree, and use edgelabels to encode dependency relations as well asstructural information which helps to transform aconverted tree back to its original graph.
By thegraph-to-tree transformation, we can train a treeparser with a graph-annotated corpus, and utilizethe corresponding tree-to-graph transformation togenerate target graphs from the outputs of the treeparser.
Given that the tree-to-graph transformationis quite trivial, we only describe the graph-to-treetransformation approach.We use graph traversal algorithms to convert adirected graph to a directed tree.
The transforma-tion implies that we may lose, add or modify somedependency relations in order to make the graph atree.4.2 Auxiliary LabelsIn the transformed trees, we use auxiliary labels tocarry out information of the original graphs.
Toencode multiple edges to one, we keep the origi-nal label on the directed edge but may add otheredges?
information.
On the other hand, through-out most transformations, some edges must be re-versed to make a tree, so we need a symbol to in-dicate a edge on the tree is reversed during trans-formation.
The auxiliary labels are listed below:?
Label with following ?R: The symbol ?Rmeans this directed edge is reversed from theoriginal directed graph.?
Separator: Semicolon separates two encodedoriginal edges.?
[N ] followed by label: The symbol [N ] (Nis an integer) represents the head of the edge.The dependent is the current one, but the headis the dependent?s N -th ancestor where 1stancestor is its father and 2nd ancestor is itsfather?s father.See Figure 2 for example.4.3 Traversal StrategiesGiven directed graph (V,E), the task is to traverseall edges on the graph and decide how to changethe labels or not contain the edge on the output.We use 3 strategies for traversal.
Here we usex ?gy to denote the edge on graph, and x ?tythe edge on tree.461Mrs Ward was relievednoun ARG1 verb ARG1 verb ARG2adj ARG1rootMrs Ward was relievednoun ARG1?R verb ARG1 verb ARG2rootMrs Ward was relievednoun ARG1?R verb ARG2adj ARG1;[2]verb ARG1rootFigure 2: One dependency graph and two possibledependency trees after converting.Depth-first-search We try graph traversal bydepth-first-search starting from the root on the di-rected graph ignoring the direction of edges.
Dur-ing the traversal, we add edges to the directed treewith (perhaps new) labels.
We traverse the graphrecursively.
Suppose the depth-first-search is run-ning at the node x and the nodes set A which havebeen searched.
And suppose we find node y islinked to x on the graph (x ?gy or y ?gx).If y /?
A, we add the directed edge x ?ty to thetree immediately.
In the case of y ?gx, we add?R to the edge label.
If y ?
A, then y must be oneof the ancestors of x.
In this case, we add this in-formation to the label of the existing edge z ?tx.Since the distance between two nodes x and y issufficient to indicate the node y, we use the dis-tance to represent the head or dependent of thisdirected edge and add the label and the distance tothe label of z ?tx.
It is clear that the auxiliarylabel [N ] can be used for multiple edge encoding.Under this strategy, all edges can be encoded onthe tree.Breadth-first-search An alternative traversalstrategy is based on breadth-first-search startingfrom the root.
This search ignores the directionof edge too.
We regard the search tree as the de-pendency tree.
During the breadth-first-search, if(x, l, y) exists but node y has been searched, wejust ignore the edge.
Under this strategy, we maylose some edges.Iterative expanding This strategy is based ondepth-first-search but slightly different.
The strat-egy only searches through the forward edges onthe directed graph at first.
When there is no for-ward edge to expend, a traversed node linked tosome nodes that are not traversed must be the de-pendent of them.
Then we choose an edge and addit (reversed) to the tree and continue to expand thetree.
Also, we ignore the edges that does not sat-isfy the tree constraint.
We call this strategy iter-ative expanding.
When we need to expand outputtree, we need to design a strategy to decide whichedge to be add.
The measure to decide which nodeshould be expanded first is its possible location onthe tree and the number of nodes it can search dur-ing depth-first-search.
Intuitively, we want the re-versed edges to be as few as possible.
For thispurpose, this strategy is practical but not necessar-ily the best.
Like the Breadth-first-search strategy,this strategy may also cause edge loss.4.4 Forest-to-TreeAfter a primary searching process, if there is stilledge x ?gy that has not been searched yet, westart a new search procedure from x or y. Even-tually, we obtain a forest rather than a tree.
Tocombine disconnected trees in this forest to the fi-nal dependency tree, we use edges with label Noneto link them.
Let the node setW be the set of rootsof the trees in the forest, which are not connectedto original graph root.
The mission is to assign anode v /?
W for each w ?
W .
If we assign viforwi, we add the edge vi?
wilabeled by None tothe final dependency tree.
We try 3 strategies inthis step:?
For each w ?
W we look for the first nodev /?W on the left of w.?
For each w ?
W we look for the first nodev /?W on the right of w.?
By defining the distance between two nodesas how many words are there between the twowords, we can select the nearest node.
If thedistances of more than one node are equal,we choose v randomly.We also tried to link all of the nodes in W di-rectly to the root, but it does not work well.5 Model EnsembleWe have 19 heterogeneous basic models (10transition-based models, 9 tree approximationmodels), and use a simple voter to combine theiroutputs.462Algorithm DM PAS PCEDTDFS 0 0 0BFS 0.0117 0.0320 0.0328FEF 0.0127 0.0380 0.0328Table 1: Edge loss of transformation algorithms.For each pair of words of a sentence, we countthe number of the models that give positive pre-dictions.
If the number is greater than a threshold,we put this arc to the final graph, and label the arcwith the most common label of what the modelsgive.Furthermore, we find that the performance ofthe tree approximation models is better than thetransition based models, and therefore we takeweights of individual models too.
Instead of justcounting, we sum the weights of the models thatgive positive predictions.
The tree approximationmodels are assigned higher weights.6 ExperimentsThere are 3 subtasks in the task, namely DM, PAS,and PCEDT.
For subtask DM, we finally obtained19 models, just as stated in previous sections.For subtask PAS and PCEDT, only 17 models aretrained due to the tight schedule.The tree approximation algorithms may causesome edge loss, and the statistics are shown in Ta-ble 1.
We can see that DFS does not cause edgeloss, but edge losses of other two algorithm arenot negligible.
This may result in a lower recalland higher precision, but we can tune the final re-sults during model ensemble.
Edge loss in subtaskDM is less than those in subtask PAS and PCEDT.We present the performance of several repre-sentative models in Table 2.
We can see that thetree approximation models performs better thanthe transition-based models, which highlights theeffective of arc-factored models for semantic de-pendency parsing.
For model ensemble, besidesthe accuracy of each single model, it is also im-portant that the models to be ensembled are verydifferent.
As shown in Table 2, the evaluation be-tween some of our models indicates that our mod-els do vary a lot.Following the suggestion of the task organizers,we use section 20 of the train data as the devel-opment set.
With the help of development set,we tune the parameters of the models and ensem-Models DM PAS PCEDTTitov 0.8468 0.8754 0.6978Titovr0.8535 0.8928 0.7063Naive 0.8481 - -DFSn0.8692 0.9034 0.7370DFSl0.8692 0.9015 0.7246BFSn0.8686 0.8818 0.7247Titov vs. Titovr0.8607 0.8831 0.7613Titov vs.
Naive 0.9245 - -Titov vs. DFSn0.8590 0.8865 0.7650DFSnvs.
DFSl0.9273 0.9579 0.8688DFSnvs.
BFSn0.9226 0.9169 0.8367Table 2: Evaluation between some of our models.Labeled f-score on test set is shown.
Titovrstandsfor reversed Titov, DFSnfor DFS+nearest, DFSlfor DFS+left, and BFSnfor BFS+nearest.
The up-per part gives the performance, and the lower partgives the agreement between systems.Format LP LR LF LMDM 0.9027 0.8854 0.8940 0.2982PAS 0.9344 0.9069 0.9204 0.3872PCEDT 0.7875 0.7396 0.7628 0.1120Table 3: Final results of the ensembled model.bling.
We set the weight of each transition-basedmodel 1, and tree approximation model 2 in run1, 3 in run 2.
The threshold is set to a half of thetotal weight.
The final results given by the orga-nizers are shown in Table 3.
Compared to Table 2demonstrates the effectiveness of parser ensemble.7 ConclusionData-driven dependency parsing techniques havebeen greatly advanced during the parst decade.Two dominant approaches, i.e.
transition-basedand graph-based methods, have been well stud-ied.
In addition, parser ensemble has been shownvery effective to take advantages to combine thestrengthes of heterogeneous base parsers.
In thiswork, we propose different models to profile thethree techniques for semantic dependency pars-ing.
The experimental results suggest several di-rections for future study.AcknowledgementThe work was supported by NSFC (61300064,61170166 and 61331011) and National High-TechR&D Program (2012AA011101).463ReferencesKoby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
JOURNAL OF MA-CHINE LEARNING RESEARCH, 7:551?585.Timothy A. D. Fowler and Gerald Penn.
2010.
Ac-curate context-free parsing with combinatory cate-gorial grammar.
In Proceedings of the 48th AnnualMeeting of the Association for Computational Lin-guistics, Uppsala, Sweden, July.Stephan Oepen, Marco Kuhlmann, Yusuke Miyao,Daniel Zeman, Dan Flickinger, Jan Haji?c, AngelinaIvanova, and Yi Zhang.
2014.
SemEval 2014 Task8: Broad-coverage semantic dependency parsing.
InProceedings of the 8th International Workshop onSemantic Evaluation, Dublin, Ireland.Kenji Sagae and Jun?ichi Tsujii.
2008.
Shift-reducedependency DAG parsing.
In Proceedings of the22nd International Conference on ComputationalLinguistics (Coling 2008), pages 753?760, Manch-ester, UK, August.
Coling 2008 Organizing Com-mittee.Kenji Sagae.
2007.
Dependency parsing and domainadaptation with lr models and parser ensembles.
InIn Proceedings of the Eleventh Conference on Com-putational Natural Language Learning.Mihai Surdeanu and Christopher D. Manning.
2010.Ensemble models for dependency parsing: Cheapand good?
In Human Language Technologies:The 2010 Annual Conference of the North Ameri-can Chapter of the Association for ComputationalLinguistics, Los Angeles, California, June.Ivan Titov, James Henderson, Paola Merlo, andGabriele Musillo.
2009.
Online graph planarisa-tion for synchronous parsing of semantic and syn-tactic dependencies.
In Proceedings of the 21st in-ternational jont conference on Artifical intelligence,IJCAI?09, pages 1562?1567, San Francisco, CA,USA.
Morgan Kaufmann Publishers Inc.Yi Zhang and Hans-Ulrich Krieger.
2011.
Large-scalecorpus-driven PCFG approximation of an hpsg.
InProceedings of the 12th International Conference onParsing Technologies, Dublin, Ireland, October.Yue Zhang and Joakim Nivre.
2011.
Transition-baseddependency parsing with rich non-local features.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Portland, Oregon, USA, June.464
