Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 333?341,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPPhrase-Based Statistical Machine Translation as a Traveling SalesmanProblemMikhail Zaslavskiy?
Marc Dymetman Nicola CanceddaMines ParisTech, Institut Curie Xerox Research Centre Europe77305 Fontainebleau, France 38240 Meylan, Francemikhail.zaslavskiy@ensmp.fr {marc.dymetman,nicola.cancedda}@xrce.xerox.comAbstractAn efficient decoding algorithm is a cru-cial element of any statistical machinetranslation system.
Some researchers havenoted certain similarities between SMTdecoding and the famous Traveling Sales-man Problem; in particular (Knight, 1999)has shown that any TSP instance can bemapped to a sub-case of a word-basedSMT model, demonstrating NP-hardnessof the decoding task.
In this paper, we fo-cus on the reverse mapping, showing thatany phrase-based SMT decoding problemcan be directly reformulated as a TSP.
Thetransformation is very natural, deepens ourunderstanding of the decoding problem,and allows direct use of any of the pow-erful existing TSP solvers for SMT de-coding.
We test our approach on threedatasets, and compare a TSP-based de-coder to the popular beam-search algo-rithm.
In all cases, our method providescompetitive or better performance.1 IntroductionPhrase-based systems (Koehn et al, 2003) areprobably the most widespread class of StatisticalMachine Translation systems, and arguably one ofthe most successful.
They use aligned sequencesof words, called biphrases, as building blocks fortranslations, and score alternative candidate trans-lations for the same source sentence based on alog-linear model of the conditional probability oftarget sentences given the source sentence:p(T, a|S) = 1ZSexp?k?khk(S, a, T ) (1)where the hk are features, that is, functions of thesource string S, of the target string T , and of the?
This work was conducted during an internship atXRCE.alignment a, where the alignment is a representa-tion of the sequence of biphrases that where usedin order to build T from S; The ?k?s are weightsand ZS is a normalization factor that guaranteesthat p is a proper conditional probability distri-bution over the pairs (T,A).
Some features arelocal, i.e.
decompose over biphrases and can beprecomputed and stored in advance.
These typ-ically include forward and reverse phrase condi-tional probability features log p(t?|s?)
as well aslog p(s?|t?
), where s?
is the source side of thebiphrase and t?
the target side, and the so-called?phrase penalty?
and ?word penalty?
features,which count the number of phrases and words inthe alignment.
Other features are non-local, i.e.depend on the order in which biphrases appear inthe alignment.
Typical non-local features includeone or more n-gram language models as well asa distortion feature, measuring by how much theorder of biphrases in the candidate translation de-viates from their order in the source sentence.Given such a model, where the ?i?s have beentuned on a development set in order to minimizesome error rate (see e.g.
(Lopez, 2008)), togetherwith a library of biphrases extracted from somelarge training corpus, a decoder implements theactual search among alternative translations:(a?, T ?)
= arg max(a,T )P (T, a|S).
(2)The decoding problem (2) is a discrete optimiza-tion problem.
Usually, it is very hard to find theexact optimum and, therefore, an approximate so-lution is used.
Currently, most decoders are basedon some variant of a heuristic left-to-right search,that is, they attempt to build a candidate translation(a, T ) incrementally, from left to right, extendingthe current partial translation at each step with anew biphrase, and computing a score composed oftwo contributions: one for the known elements ofthe partial translation so far, and one a heuristic333estimate of the remaining cost for completing thetranslation.
The variant which is mostly used isa form of beam-search, where several partial can-didates are maintained in parallel, and candidatesfor which the current score is too low are prunedin favor of candidates that are more promising.We will see in the next section that some char-acteristics of beam-search make it a suboptimalchoice for phrase-based decoding, and we willpropose an alternative.
This alternative is based onthe observation that phrase-based decoding can bevery naturally cast as a Traveling Salesman Prob-lem (TSP), one of the best studied problems incombinatorial optimization.
We will show that thisformulation is not only a powerful conceptual de-vice for reasoning on decoding, but is also prac-tically convenient: in the same amount of time,off-the-shelf TSP solvers can find higher scoringsolutions than the state-of-the art beam-search de-coder implemented in Moses (Hoang and Koehn,2008).2 Related workBeam-search decodingIn beam-search decoding, candidate translationprefixes are iteratively extended with new phrases.In its most widespread variant, stack decoding,prefixes obtained by consuming the same numberof source words, no matter which, are grouped to-gether in the same stack1 and compete against oneanother.
Threshold and histogram pruning are ap-plied: the former consists in dropping all prefixeshaving a score lesser than the best score by morethan some fixed amount (a parameter of the algo-rithm), the latter consists in dropping all prefixesbelow a certain rank.While quite successful in practice, stack decod-ing presents some shortcomings.
A first one is thatprefixes obtained by translating different subsetsof source words compete against one another.
Inone early formulation of stack decoding for SMT(Germann et al, 2001), the authors indeed pro-posed to lazily create one stack for each subsetof source words, but acknowledged issues withthe potential combinatorial explosion in the num-ber of stacks.
This problem is reduced by the useof heuristics for estimating the cost of translatingthe remaining part of the source sentence.
How-1While commonly adopted in the speech and SMT com-munities, this is a bit of a misnomer, since the used data struc-tures are priority queues, not stacks.ever, this solution is only partially satisfactory.
Onthe one hand, heuristics should be computationallylight, much lighter than computing the actual bestscore itself, while, on the other hand, the heuris-tics should be tight, as otherwise pruning errorswill ensue.
There is no clear criterion to guidein this trade-off.
Even when good heuristics areavailable, the decoder will show a bias towardsputting at the beginning the translation of a certainportion of the source, either because this portionis less ambiguous (i.e.
its translation has largerconditional probability) or because the associatedheuristics is less tight, hence more optimistic.
Fi-nally, since the translation is built left-to-right thedecoder cannot optimize the search by taking ad-vantage of highly unambiguous and informativeportions that should be best translated far from thebeginning.
All these reasons motivate consideringalternative decoding strategies.Word-based SMT and the TSPAs already mentioned, the similarity betweenSMT decoding and TSP was recognized in(Knight, 1999), who focussed on showing thatany TSP can be reformulated as a sub-class of theSMT decoding problem, proving that SMT decod-ing is NP-hard.
Following this work, the exis-tence of many efficient TSP algorithms then in-spired certain adaptations of the underlying tech-niques to SMT decoding for word-based models.Thus, (Germann et al, 2001) adapt a TSP sub-tour elimination strategy to an IBM-4 model, us-ing generic Integer Programming techniques.
Thepaper comes close to a TSP formulation of de-coding with IBM-4 models, but does not pursuethis route to the end, stating that ?It is difficultto convert decoding into straight TSP, but a widerange of combinatorial optimization problems (in-cluding TSP) can be expressed in the more gen-eral framework of linear integer programming?.By employing generic IP techniques, it is how-ever impossible to rely on the variety of moreefficient both exact and approximate approacheswhich have been designed specifically for the TSP.In (Tillmann and Ney, 2003) and (Tillmann, 2006),the authors modify a certain Dynamic Program-ming technique used for TSP for use with an IBM-4 word-based model and a phrase-based model re-spectively.
However, to our knowledge, none ofthese works has proposed a direct reformulationof these SMT models as TSP instances.
We be-lieve we are the first to do so, working in our case334with the mainstream phrase-based SMT models,and therefore making it possible to directly applyexisting TSP solvers to SMT.3 The Traveling Salesman Problem andits variantsIn this paper the Traveling Salesman Problem ap-pears in four variants:STSP.
The most standard, and most studied,variant is the Symmetric TSP: we are given a non-directed graph G on N nodes, where the edgescarry real-valued costs.
The STSP problem con-sists in finding a tour of minimal total cost, wherea tour (also called Hamiltonian Circuit) is a ?cir-cular?
sequence of nodes visiting each node of thegraph exactly once;ATSP.
The Asymmetric TSP, or ATSP, is a vari-ant where the underlying graph G is directed andwhere, for i and j two nodes of the graph, theedges (i,j) and (j,i) may carry different costs.SGTSP.
The Symmetric Generalized TSP, orSGTSP: given a non-oriented graph G of |G|nodes with edges carrying real-valued costs, givena partition of these |G| nodes into m non-empty,disjoint, subsets (called clusters), find a circularsequence of m nodes of minimal total cost, whereeach cluster is visited exactly once.AGTSP.
The Asymmetric Generalized TSP, orAGTSP: similar to the SGTSP, but G is now a di-rected graph.The STSP is often simply denoted TSP in theliterature, and is known to be NP-hard (Applegateet al, 2007); however there has been enormousinterest in developing efficient solvers for it, bothexact and approximate.Most of existing algorithms are designed forSTSP, but ATSP, SGTSP and AGTSP may be re-duced to STSP, and therefore solved by STSP al-gorithms.3.1 Reductions AGTSP?ATSP?STSPThe transformation of the AGTSP into the ATSP,introduced by (Noon and Bean, 1993)), is illus-trated in Figure (1).
In this diagram, we assumethat Y1, .
.
.
, YK are the nodes of a given cluster,while X and Z are arbitrary nodes belonging toother clusters.
In the transformed graph, we in-troduce edges between the Yi?s in order to form acycle as shown in the figure, where each edge hasa large negative cost ?K.
We leave alone the in-coming edge to Yi from X , but the outgoing edgeFigure 1: AGTSP?ATSP.from Yi to X has its origin changed to Yi?1.
Afeasible tour in the original AGTSP problem pass-ing through X,Yi, Z will then be ?encoded?
as atour of the transformed graph that first traversesX , then traverses Yi, .
.
.
, YK , .
.
.
, Yi?1, then tra-verses Z (this encoding will have the same cost asthe original cost, minus (k ?
1)K).
Crucially, ifK is large enough, then the solver for the trans-formed ATSP graph will tend to traverse as manyK edges as possible, meaning that it will traverseexactly k ?
1 such edges in the cluster, that is, itwill produce an encoding of some feasible tour ofthe AGTSP problem.As for the transformation ATSP?STSP, severalvariants are described in the literature, e.g.
(Ap-plegate et al, 2007, p. 126); the one we use is from(Wikipedia, 2009) (not illustrated here for lack ofspace).3.2 TSP algorithmsTSP is one of the most studied problems in com-binatorial optimization, and even a brief review ofexisting approaches would take too much place.Interested readers may consult (Applegate et al,2007; Gutin, 2003) for good introductions.One of the best existing TSP solvers is imple-mented in the open source Concorde package (Ap-plegate et al, 2005).
Concorde includes the fastestexact algorithm and one of the most efficient im-plementations of the Lin-Kernighan (LK) heuris-tic for finding an approximate solution.
LK worksby generating an initial random feasible solutionfor the TSP problem, and then repeatedly identi-fying an ordered subset of k edges in the currenttour and an ordered subset of k edges not includedin the tour such that when they are swapped theobjective function is improved.
This is somewhat335reminiscent of the Greedy decoding of (Germannet al, 2001), but in LK several transformations canbe applied simultaneously, so that the risk of beingstuck in a local optimum is reduced (Applegate etal., 2007, chapter 15).As will be shown in the next section, phrase-based SMT decoding can be directly reformulatedas an AGTSP.
Here we use Concorde throughfirst transforming AGTSP into STSP, but it mightalso be interesting in the future to use algorithmsspecifically designed for AGTSP, which could im-prove efficiency further (see Conclusion).4 Phrase-based Decoding as TSPIn this section we reformulate the SMT decodingproblem as an AGTSP.
We will illustrate the ap-proach through a simple example: translating theFrench sentence ?cette traduction automatique estcurieuse?
into English.
We assume that the rele-vant biphrases for translating the sentence are asfollows:ID source targeth cette thist traduction translationht cette traduction this translationmt traduction automatique machine translationa automatique automaticm automatique machinei est iss curieuse strangec curieuse curiousUnder this model, we can produce, among others,the following translations:h ?
mt ?
i ?
s this machine translation is strangeh ?
c ?
t ?
i ?
a this curious translation is automaticht ?
s ?
i ?
a this translation strange is automaticwhere we have indicated on the left the ordered se-quence of biphrases that leads to each translation.We now formulate decoding as an AGTSP, inthe following way.
The graph nodes are all thepossible pairs (w, b), where w is a source word inthe source sentence s and b is a biphrase contain-ing this source word.
The graph clusters are thesubsets of the graph nodes that share a commonsource word w.The costs of a transition between nodes M andN of the graph are defined as follows:(a) If M is of the form (w, b) and N of the form(w?, b), in which b is a single biphrase, and w andw?
are consecutive words in b, then the transitioncost is 0: once we commit to using the first wordof b, there is no additional cost for traversing theother source words covered by b.
(b) If M = (w, b), where w is the rightmostsource word in the biphrase b, and N = (w?, b?
),where w?
6= w is the leftmost source word in b?,then the transition cost corresponds to the costof selecting b?
just after b; this will correspondto ?consuming?
the source side of b?
after havingconsumed the source side of b (whatever their rel-ative positions in the source sentence), and to pro-ducing the target side of b?
directly after the targetside of b; the transition cost is then the addition ofseveral contributions (weighted by their respective?
(not shown), as in equation 1):?
The cost associated with the features local tob in the biphrase library;?
The ?distortion?
cost of consuming thesource word w?
just after the source word w:|pos(w?)
?
pos(w) ?
1|, where pos(w) andpos(w?)
are the positions of w and w?
in thesource sentence.?
The language model cost of producing thetarget words of b?
right after the target wordsof b; with a bigram language model, this costcan be precomputed directly from b and b?.This restriction to bigram models will be re-moved in Section 4.1.
(c) In all other cases, the transition cost is infinite,or, in other words, there is no edge in the graphbetween M and N .A special cluster containing a single node (de-noted by $-$$ in the figures), and corresponding tospecial beginning-of-sentence symbols must alsobe included: the corresponding edges and weightscan be worked out easily.
Figures 2 and 3 givesome illustrations of what we have just described.4.1 From Bigram to N-gram LMSuccessful phrase-based systems typically employlanguage models of order higher than two.
How-ever, our models so far have the following impor-tant ?Markovian?
property: the cost of a path isadditive relative to the costs of transitions.
Forexample, in the example of Figure 3, the cost ofthis ?
machine translation ?
is ?
strange, can onlytake into account the conditional probability of theword strange relative to the word is, but not rela-tive to the words translation and is.
If we want toextend the power of the model to general n-gramlanguage models, and in particular to the 3-gram336Figure 2: Transition graph for the source sentencecette traduction automatique est curieuse.
Onlyedges entering or exiting the node traduction ?
mtare shown.
The only successor to [traduction ?mt] is [automatique ?
mt], and [cette ?
ht] is not apredecessor of [traduction ?
mt].Figure 3: A GTSP tours is illustrated, correspond-ing to the displayed output.case (on which we concentrate here, but the tech-niques can be easily extended to the general case),the following approach can be applied.Compiling Out for Trigram modelsThis approach consists in ?compiling out?
allbiphrases with a target side of only one word.We replace each biphrase b with single-word tar-get side by ?extended?
biphrases b1, .
.
.
, br, whichare ?concatenations?
of b and some other biphraseb?
in the library.2 To give an example, considerthat we: (1) remove from the biphrase library thebiphrase i, which has a single word target, and (2)add to the library the extended biphrases mti, ti,si, .
.
., that is, all the extended biphrases consist-ing of the concatenation of a biphrase in the librarywith i, then it is clear that these extended biphraseswill provide enough context to compute a trigramprobability for the target word produced immedi-ately next (in the examples, for the words strange,2In the figures, such ?concatenations?
are denoted by[b?
?
b] ; they are interpreted as encapsulations of first con-suming the source side of b?, whether or not this source sideprecedes the source side of b in the source sentence, produc-ing the target side of b?, consuming the source side of b, andproducing the target side of b immediately after that of b?.Figure 4: Compiling-out of biphrase i: (est,is).automatic and automatic respectively).
If we dothat exhaustively for all biphrases (relevant for thesource sentence at hand) that, like i, have a single-word target, we will obtain a representation thatallows a trigram language model to be computedat each point.The situation becomes clearer by looking at Fig-ure 4, where we have only eliminated the biphrasei, and only shown some of the extended biphrasesthat now encapsulate i, and where we show onevalid circuit.
Note that we are now able to as-sociate with the edge connecting the two nodes(est,mti) and (curieuse, s) a trigram cost becausemti provides a large enough target context.While this exhaustive ?compiling out?
methodworks in principle, it has a serious defect: if forthe sentence to be translated, there are m relevantbiphrases, among which k have single-word tar-gets, then we will create on the order of km ex-tended biphrases, which may represent a signif-icant overhead for the TSP solver, as soon as kis large relative to m, which is typically the case.The problem becomes even worse if we extend thecompiling-out method to n-gram language modelswith n > 3.
In the Future Work section below,we describe a powerful approach for circumvent-ing this problem, but with which we have not ex-perimented yet.5 Experiments5.1 Monolingual word re-orderingIn the first series of experiments we consider theartificial task of reconstructing the original wordorder of a given English sentence.
First, we ran-domly permute words in the sentence, and thenwe try to reconstruct the original order by max-337100 102 104?0.8?0.6?0.4?0.200.2Time (sec)DecoderscoreBEAM?SEARCHTSP100 102 104?0.4?0.3?0.2?0.100.1Time (sec)DecoderscoreBEAM?SEARCHTSP(a) (b) (c) (d)Figure 5: (a), (b): LM and BLEU scores as functions of time for a bigram LM; (c), (d): the same fora trigram LM.
The x axis corresponds to the cumulative time for processing the test set; for (a) and (c),the y axis corresponds to the mean difference (over all sentences) between the lm score of the outputand the lm score of the reference normalized by the sentence length N: (LM(ref)-LM(true))/N.
The solidline with star marks corresponds to using beam-search with different pruning thresholds, which result indifferent processing times and performances.
The cross corresponds to using the exact-TSP decoder (inthis case the time to the optimal solution is not under the user?s control).imizing the LM score over all possible permuta-tions.
The reconstruction procedure may be seenas a translation problem from ?Bad English?
to?Good English?.
Usually the LM score is usedas one component of a more complex decoderscore which also includes biphrase and distortionscores.
But in this particular ?translation task?from bad to good English, we consider that all?biphrases?
are of the form e ?
e, where e is anEnglish word, and we do not take into accountany distortion: we only consider the quality ofthe permutation as it is measured by the LM com-ponent.
Since for each ?source word?
e, there isexactly one possible ?biphrase?
e ?
e each clus-ter of the Generalized TSP representation of thedecoding problem contains exactly one node; inother terms, the Generalized TSP in this situationis simply a standard TSP.
Since the decoding phaseis then equivalent to a word reordering, the LMscore may be used to compare the performanceof different decoding algorithms.
Here, we com-pare three different algorithms: classical beam-search (Moses); a decoder based on an exact TSPsolver (Concorde); a decoder based on an approx-imate TSP solver (Lin-Kernighan as implementedin the Concorde solver) 3.
In the Beam-searchand the LK-based TSP solver we can control thetrade-off between approximation quality and run-ning time.
To measure re-ordering quality, we usetwo scores.
The first one is just the ?internal?
LMscore; since all three algorithms attempt to maxi-mize this score, a natural evaluation procedure isto plot its value versus the elapsed time.
The sec-3Both TSP decoders may be used with/or without a distor-tion limit; in our experiments we do not use this parameter.ond score is BLEU (Papineni et al, 2001), com-puted between the reconstructed and the originalsentences, which allows us to check how well thequality of reconstruction correlates with the inter-nal score.
The training dataset for learning the LMconsists of 50000 sentences from NewsCommen-tary corpus (Callison-Burch et al, 2008), the testdataset for word reordering consists of 170 sen-tences, the average length of test sentences is equalto 17 words.Bigram based reordering.
First we considera bigram Language Model and the algorithms tryto find the re-ordering that maximizes the LMscore.
The TSP solver used here is exact, that is,it actually finds the optimal tour.
Figures 5(a,b)present the performance of the TSP and Beam-search based methods.Trigram based reordering.
Then we considera trigram based Language Model and the algo-rithms again try to maximize the LM score.
Thetrigram model used is a variant of the exhaustivecompiling-out procedure described in Section 4.1.Again, we use an exact TSP solver.Looking at Figure 5a, we see a somewhat sur-prising fact: the cross and some star points havepositive y coordinates!
This means that, when us-ing a bigram language model, it is often possibleto reorder the words of a randomly permuted ref-erence sentence in such a way that the LM scoreof the reordered sentence is larger than the LM ofthe reference.
A second notable point is that theincrease in the LM-score of the beam-search withtime is steady but very slow, and never reaches thelevel of performance obtained with the exact-TSPprocedure, even when increasing the time by sev-338eral orders of magnitude.
Also to be noted is thatthe solution obtained by the exact-TSP is provablythe optimum, which is almost never the case ofthe beam-search procedure.
In Figure 5b, we re-port the BLEU score of the reordered sentencesin the test set relative to the original referencesentences.
Here we see that the exact-TSP out-puts are closer to the references in terms of BLEUthan the beam-search solutions.
Although the TSPoutput does not recover the reference sentences(it produces sentences with a slightly higher LMscore than the references), it does reconstruct thereferences better than the beam-search.
The ex-periments with trigram language models (Figures5(c,d)) show similar trends to those with bigrams.5.2 Translation experiments with a bigramlanguage modelIn this section we consider two real translationtasks, namely, translation from English to French,trained on Europarl (Koehn et al, 2003) and trans-lation from German to Spanish training on theNewsCommentary corpus.
For Europarl, the train-ing set includes 2.81 million sentences, and thetest set 500.
For NewsCommentary the trainingset is smaller: around 63k sentences, with a testset of 500 sentences.
Figure 6 presents Decoderand Bleu scores as functions of time for the twocorpuses.Since in the real translation task, the size of theTSP graph is much larger than in the artificial re-ordering task (in our experiments the median sizeof the TSP graph was around 400 nodes, some-times growing up to 2000 nodes), directly apply-ing the exact TSP solver would take too long; in-stead we use the approximate LK algorithm andcompare it to Beam-Search.
The efficiency of theLK algorithm can be significantly increased by us-ing a good initialization.
To compare the quality ofthe LK and Beam-Search methods we take a roughinitial solution produced by the Beam-Search al-gorithm using a small value for the stack size andthen use it as initial point, both for the LK algo-rithm and for further Beam-Search optimization(where as before we vary the Beam-Search thresh-olds in order to trade quality for time).In the case of the Europarl corpus, we observethat LK outperforms Beam-Search in terms of theDecoder score as well as in terms of the BLEUscore.
Note that the difference between the two al-gorithms increases steeply at the beginning, whichmeans that we can significantly increase the qual-ity of the Beam-Search solution by using the LKalgorithm at a very small price.
In addition, it isimportant to note that the BLEU scores obtained inthese experiments correspond to feature weights,in the log-linear model (1), that have been opti-mized for the Moses decoder, but not for the TSPdecoder: optimizing these parameters relatively tothe TSP decoder could improve its BLEU scoresstill further.On the News corpus, again, LK outperformsBeam-Search in terms of the Decoder score.
Thesituation with the BLEU score is more confuse.Both algorithms do not show any clear score im-provement with increasing running time whichsuggests that the decoder?s objective function isnot very well correlated with the BLEU score onthis corpus.6 Future WorkIn section 4.1, we described a general ?compilingout?
method for extending our TSP representationto handling trigram and N-gram language models,but we noted that the method may lead to combi-natorial explosion of the TSP graph.
While thisproblem was manageable for the artificial mono-lingual word re-ordering (which had only one pos-sible translation for each source word), it be-comes unwieldy for the real translation experi-ments, which is why in this paper we only consid-ered bigram LMs for these experiments.
However,we know how to handle this problem in principle,and we now describe a method that we plan to ex-periment with in the future.To avoid the large number of artificial biphrasesas in 4.1, we perform an adaptive selection.
Let ussuppose that (w, b) is a SMT decoding graph node,where b is a biphrase containing only one word onthe target side.
On the first step, when we evaluatethe traveling cost from (w, b) to (w?, b?
), we takethe language model component equal tominb??
6=b?,b?
log p(b?.v|b.e, b?
?.e),where b?.v represents the first word of the b?
tar-get side, b.e is the only word of the b targetside, and b?
?.e is the last word of the b??
tar-get size.
This procedure underestimates the totalcost of tour passing through biphrases that have asingle-word target.
Therefore if the optimal tourpasses only through biphrases with more than one339103 104 105?273?272.5?272?271.5?271Time (sec)DecoderscoreBEAM?SEARCHTSP (LK)103 104 1050.180.1850.19Time (sec)BLEUscoreBEAM?SEARCHTSP (LK)103 104?414?413.8?413.6?413.4?413.2?413Time (sec)DecoderscoreTSP (LK)BEAM?SEARCH103 1040.2420.2430.2440.245Time (sec)BLEUscoreTSP (LK)BEAM?SEARCH(a) (b) (c) (d)Figure 6: (a), (b): Europarl corpus, translation from English to French; (c),(d): NewsCommentary cor-pus, translation from German to Spanish.
Average value of the decoder and the BLEU scores (over 500test sentences) as a function of time.
The trade-off quality/time in the case of LK is controlled by thenumber of iterations, and each point corresponds to a particular number of iterations, in our experimentsLK was run with a number of iterations varying between 2k and 170k.
The same trade-off in the case ofBeam-Search is controlled by varying the beam thresholds.word on their target side, then we are sure thatthis tour is also optimal in terms of the tri-gramlanguage model.
Otherwise, if the optimal tourpasses through (w, b), where b is a biphrase hav-ing a single-word target, we add only the extendedbiphrases related to b as we described in section4.1, and then we recompute the optimal tour.
Iter-ating this procedure provably converges to an op-timal solution.This powerful method, which was proposed in(Kam and Kopec, 1996; Popat et al, 2001) in thecontext of a finite-state model (but not of TSP),can be easily extended to N-gram situations, andtypically converges in a small number of itera-tions.7 ConclusionThe main contribution of this paper has been topropose a transformation for an arbitrary phrase-based SMT decoding instance into a TSP instance.While certain similarities of SMT decoding andTSP were already pointed out in (Knight, 1999),where it was shown that any Traveling SalesmanProblem may be reformulated as an instance ofa (simplistic) SMT decoding task, and while cer-tain techniques used for TSP were then adapted toword-based SMT decoding (Germann et al, 2001;Tillmann and Ney, 2003; Tillmann, 2006), we arenot aware of any previous work that shows thatSMT decoding can be directly reformulated as aTSP.
Beside the general interest of this transfor-mation for understanding decoding, it also opensthe door to direct application of the variety of ex-isting TSP algorithms to SMT.
Our experimentson synthetic and real data show that fast TSP al-gorithms can handle selection and reordering inSMT comparably or better than the state-of-the-art beam-search strategy, converging on solutionswith higher objective function in a shorter time.The proposed method proceeds by first con-structing an AGTSP instance from the decodingproblem, and then converting this instance firstinto ATSP and finally into STSP.
At this point, adirect application of the well known STSP solverConcorde (with Lin-Kernighan heuristic) alreadygives good results.
We believe however that theremight exist even more efficient alternatives.
In-stead of converting the AGTSP instance into aSTSP instance, it might prove better to use di-rectly algorithms expressly designed for ATSPor AGTSP.
For instance, some of the algorithmstested in the context of the DIMACS implemen-tation challenge for ATSP (Johnson et al, 2002)might well prove superior.
There is also active re-search around AGTSP algorithms.
Recently neweffective methods based on a ?memetic?
strategy(Buriol et al, 2004; Gutin et al, 2008) have beenput forward.
These methods combined with ourproposed formulation provide ready-to-use SMTdecoders, which it will be interesting to compare.AcknowledgmentsThanks to Vassilina Nikoulina for her advice aboutrunning Moses on the test datasets.340ReferencesDavid L. Applegate, Robert E. Bixby, Vasek Chvatal,and William J. Cook.
2005.
Concordetsp solver.
http://www.tsp.gatech.edu/concorde.html.David L. Applegate, Robert E. Bixby, Vasek Chvatal,and William J. Cook.
2007.
The Traveling Sales-man Problem: A Computational Study (PrincetonSeries in Applied Mathematics).
Princeton Univer-sity Press, January.Luciana Buriol, Paulo M. Franc?a, and Pablo Moscato.2004.
A new memetic algorithm for the asymmetrictraveling salesman problem.
Journal of Heuristics,10(5):483?506.Chris Callison-Burch, Philipp Koehn, Christof Monz,Josh Schroeder, and Cameron Shaw Fordyce, edi-tors.
2008.
Proceedings of the Third Workshop onSMT.
ACL, Columbus, Ohio, June.Ulrich Germann, Michael Jahr, Kevin Knight, andDaniel Marcu.
2001.
Fast decoding and optimaldecoding for machine translation.
In In Proceedingsof ACL 39, pages 228?235.Gregory Gutin, Daniel Karapetyan, and Krasnogor Na-talio.
2008.
Memetic algorithm for the generalizedasymmetric traveling salesman problem.
In NICSO2007, pages 199?210.
Springer Berlin.G.
Gutin.
2003.
Travelling salesman and related prob-lems.
In Handbook of Graph Theory.Hieu Hoang and Philipp Koehn.
2008.
Design of theMoses decoder for statistical machine translation.
InACL 2008 Software workshop, pages 58?65, Colum-bus, Ohio, June.
ACL.D.S.
Johnson, G. Gutin, L.A. McGeoch, A. Yeo,W.
Zhang, and A. Zverovich.
2002.
Experimen-tal analysis of heuristics for the atsp.
In The Trav-elling Salesman Problem and Its Variations, pages445?487.Anthony C. Kam and Gary E. Kopec.
1996.
Documentimage decoding by heuristic search.
IEEE Transac-tions on Pattern Analysis and Machine Intelligence,18:945?950.Kevin Knight.
1999.
Decoding complexity in word-replacement translation models.
ComputationalLinguistics, 25:607?615.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InNAACL 2003, pages 48?54, Morristown, NJ, USA.Association for Computational Linguistics.Adam Lopez.
2008.
Statistical machine translation.ACM Comput.
Surv., 40(3):1?49.C.
Noon and J.C. Bean.
1993.
An efficient transforma-tion of the generalized traveling salesman problem.INFOR, pages 39?44.Kishore Papineni, Salim Roukos, Todd Ward, andWei J. Zhu.
2001.
BLEU: a Method for AutomaticEvaluation of Machine Translation.
IBM ResearchReport, RC22176.Kris Popat, Daniel H. Greene, Justin K. Romberg, andDan S. Bloomberg.
2001.
Adding linguistic con-straints to document image decoding: Comparingthe iterated complete path and stack algorithms.Christoph Tillmann and Hermann Ney.
2003.
Word re-ordering and a dynamic programming beam searchalgorithm for statistical machine translation.
Com-put.
Linguist., 29(1):97?133.Christoph Tillmann.
2006.
Efficient Dynamic Pro-gramming Search Algorithms For Phrase-BasedSMT.
In Workshop On Computationally Hard Prob-lems And Joint Inference In Speech And LanguageProcessing.Wikipedia.
2009.
Travelling Salesman Problem ?Wikipedia, The Free Encyclopedia.
[Online; ac-cessed 5-May-2009].341
