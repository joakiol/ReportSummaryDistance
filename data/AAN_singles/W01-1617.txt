Plug and Play Speech UnderstandingManny Rayner, Ian Lewin& Genevieve Gorrellnetdecisions LtdWellington House,East Road, Cambridge CB1 1BH, UKmanny.raynerjian.lewinjgenevieve.gorrell@netdecisions.comJohan BoyeTelia ResearchS-123 86 Farsta, Swedenjohan.boye@trab.seAbstractPlug and Play is an increasingly im-portant concept in system and networkarchitectures.
We introduce and de-scribe a spoken language dialogue sys-tem architecture which supports Plugand Playable networks of objects in itsdomain.
Each device in the network car-ries the linguistic and dialogue manage-ment information which is pertinent to itand uploads it dynamically to the rele-vant language processing components inthe spoken language interface.
We de-scribe the current state of our plug andplay demonstrator and discuss theoreti-cal issues that arise from our work.
Plugand Play forms a central topic for theDHomme project.1 IntroductionThe notion of Plug and Play nds its most natu-ral home in the world of networked home devices,where it oers at least the following two importantproperties the network of devices is dynamically recon-gurable as devices are brought online or dis-appear oine zero re-conguration by the user is requiredFrameworks for achieving Plug and Play gener-ally address this by including at least the following devices announce themselves on the networkwhen they are plugged into it (and also dis-cover the existence of others) devices describe their own capabilities, pro-vide a means for accessing them and canquery and access the capabilities of others devices should support, where possible, seam-less interaction with other devices.Plug and Play is, not surprisingly, viewed asa pre-requisite for the commercial success of net-worked devices in the home.
There are alreadyseveral promising candidate platforms for achiev-ing the necessary functionality, including Univer-sal Plug and Play (UPnP) (Microsoft, 2000) andJini (Oaks and Wong, 2000).
In this paper, weaddress the requirements on spoken dialogue in-terfaces that arise from a plug and play domain.We also present the current state of our Englishlanguage plug and play demonstrator for control-ling lamps, dimmers and sensors, previously de-scribed in (Rayner et al, 2001b).
(There is also aSwedish instantiation).First, however, we need briey to distinguishour notion from other notions of plug and playand recongurability.The notion of Plug and Play has been usedfor dialogue system toolkits in which the variousdierent language processing components them-selves (e.g.
recognition, parsing, generation anddialogue management) can be plugged in andout.
The most prominent instance of this isthe Darpa Communicator architecture (Goldschenand Loehr, 1999), which denes interoperabilitystandards for language processing components.The intention is simply that researchers and de-velopers can experiment with systems containingdierent instantiations of the language process-ing components.
The Communicator Architec-ture is not designed to address the special require-ments of a plug and play domain.
In fact, theCommunicator architecture does not support thedynamic re-conguration of language processingcomponents while the system is running.At a more general level, simple re-congurationof spoken language dialogue systems has of courselong been a goal of language engineering.
Butsuch re-conguration is nearly always viewed asthe problem of cross-domain or possibly cross-language porting, e.g.
(Glass, 1999).
Once onehas a cinema ticket booking service, for example,one may examine the eort required for book-ing train tickets, or for e-shopping in general oreven the \database access" scenario.
There arevarious toolkits, architectures and methodologiesfor rapidly and/or semi-expertly generating newinstances of dialogue systems, e.g.
by abstract-ing away from domain or application dependentfeatures of particular systems, e.g.
(Fraser andThornton, 1995; Kolzer, 1999), or `bottom-up' byaggregation of useful re-congurable components, e.g.
(Sutton et al 1998; Larsson and Traum,2000).
The automated within-domain recongu-ration required for a plug and play domain, hasnot, to our knowledge, been described previously.Pursuit of plug and play functionality (and itsrealization in strong and weak forms - discussed insection 3) forms a central theme of the DHommeproject.1In the rest of this paper, we begin by detail-ing our concrete Plug and Play scenario - devicecontrol in the home - with an example dialoguefrom our demonstrator and an outline of the maindialogue processing elements.
In section 3, we dis-tinguish strong and weak notions of Plug and Playand their applicability to spoken language inter-faces.
In section 4, we discuss the strong plugand play capability we have built into the recogni-tion, parsing and (context independent) semanticinterpretation system components of our demon-strator.
In section 5, we discuss some future workfor Plug and Play dialogue management.
Section6 contains our conclusions.2 A Plug and Play ScenarioIn this section we present example dialogues fromour current demonstrator and briey outline themain processing elements.
The domain is net-worked home devices, an area where plug and playis already in a reasonably advanced state of devel-opment and speech control looks highly attractive.2.1 Plug and Play ExamplesFigure 1 displays an example dialogue from ourcurrent demonstrator.In U1, the user asks for the TV to be switchedon.
The system reports (S1) that it simply doesnot understand the user.
(If it possessed \TV"in its recognition vocabulary and knew somethingabout the concept of TVs it could have reportedI don't know of any TV).
When a TV is pluggedinto the network (following U2), the system is ableto understand, and act on the user's repeated re-quest to switch on the TV.
The system reports on1This work is supported by EU 5th Frameworkproject IST-2000-26280 { see Acknowledgmentsits action (S3).
S4 illustrates another type of \er-ror" message.
When another TV is plugged intothe network, the system must now engage in dis-ambiguation behaviour whereas previously it hadno need to (S7).S10 illustrates that, in the absence of dimmablelights, \Dim" is not understood, and, possibly,not even recognized.
When a dimmable light isplugged in (or, at least, knowledge of dimmablelights is plugged in), then a more helpful errormessage can be given in S12.
Finally, when thegrammar is increased to cover new commands, thesystem may begin to make mistakes that it did notmake originally (S13).2.2 The current demonstratorOur demonstrator expects devices of three maintypes: switchable, dimmable and sensors.
Switch-able devices are binary state devices that can beset or queried.
Dimmable devices have a statevarying on a single scalar dimension which can beset, changed, or queried.
Sensors are similar butcan only be queried.Formally, these commands and queries are en-coded by a 4-ary slot-value structure and De-vice Grammars must generate semantic valuescontaining these slots.
(Not all slots are re-quired for every utterance, of course.)
Thefour slots are: op (lled by command or query);level; change and dir (on or o).
In or-der to identify devices, there are 4 other slots:device (light, tv .
.
.
), loc (bathroom, kitchen .
.
.
),device-spec (all, the .
.
. )
and pronoun (it, them.
.
.
).
For example, Is the light in the hall on?translates to h op=query dir=on device=lightdevice-spec=the loc=hall i. Dim everything byten percent translates to h op=command dir=odevice-spec=everything change=10 i. Switchthe hall and kitchen lights o translates to hop=command dir=o level=0 device=light hloc=kitchen loc=hall ii.Dialogue interpretation contains three stages.First, conjunctions in the input (which are treatedas just packed representations) are unpacked intoa set of (7-ary) slot-structures.
Secondly, a form ofellipsis resolution, \sticky defaults", takes place inwhich missing slots are lled in from their previ-ous values.
A fragmentary semantic value is sim-ply pasted over the corresponding parts of the lastone.
Thus And in the bathroom translates to hloc=bathroomi but the other required slots (e.g.device) are supplied from the previous represen-tation.
Finally reference resolution tries to deter-mine device identiers for pronouns and denitelydescribed devices.
Currently, devices are identi-ed only by their location and type so a simplematching procedure can be used.Following contextual interpretation, either aprogram (a command or sequence of such) or an`error' condition (e.g.
resolution failed to identifya device) will have been generated.
The systemmust then execute the program and/or generatefeedback.
Knowledge of how to execute these pro-grams, e.g.
that it is an `error' to try to switch ona light that is already on, and possible feedbackmessages are simply hardcoded into the DialogueManager.
There is a pre-dened set of feedbackmessage structures associated with each underly-ing action and their possible results.
Some exam-ple paraphrases of message structures are \The Xis now on", \The X is already on", \The X is nowat Y percent", \There is no X in the Y".3 Strong and Weak Plug and PlayIn its weakest form, Plug and Play refers only tothe ability to add a device to a network with-out manual conguration.
Knowledge distribu-tion is not included.
Standard Plug and Play forPC peripherals simply automates the matching upof physical devices with software device-specicdrivers in the PC.
Communication links betweenthem are established by reserving resources suchas shared memory and interrupt request numbers.The weak sense is very useful.
Users need notcongure their hardware via jumper switches orsoftware drivers by entering `magic' numbers inconguration les.In the strong sense, Plug and Play can referalso to modular, distributed knowledge.
Devicesnot only set up network communications but pub-lish information about themselves over it.
Otherdevices can obtain and use it.
In Jini, for exam-ple, a new printer device can register its printingservice (and java code for invoking methods on it)on the network.
Then, a word-processing applica-tion can nd it and congure itself to use it.
InUPnP, devices publish XML descriptions of theirinterfaces.The strong-weak contrast is not a sharp or bi-nary one.
The word-processor might know theindustry agreed printer interface and so displaya greyed-out print button if no printer is net-worked.
When a new type of printer is net-worked, it might supply additional print options(e.g.
\print colour") that the processor knowsnothing about.One desirable Plug and Play property in bothstrong and weak forms is commutativity, i.e.
thesystem understands the same commands in thesame way no matter which device is connectedrst.
It is less obvious whether disconnecting de-vice X should be the inverse operation of connect-ing device X.
This seems reasonable in a weak plugand play system, but in the strong case it wouldmean that the recognizer would cease to under-stand the word \TV" as soon as the TV were dis-connected.
This might be confusing for the user.The strong and weak senses of plug and playapply to spoken language dialogue interfaces.
Inthe weakest sense, the dialogue system might beentirely pre-congured to deal with all possibledevices and device-combinations.
The requiredknowledge is already present in the network.
Plugand Play then consists of identifying which partic-ular devices are currently networked and estab-lishing communication channels with them.
Inthe stronger sense, the components of the spokenlanguage dialogue interface acquire the knowledgepertinent to particular devices from those devices.So, as in example S1 above, the speech recognizermay not have the word \TV" in its vocabularyuntil a TV is plugged into the network.
The di-alogue manager may not be capable of uttering\That device is not dimmable" until a dimmabledevice is plugged into the network.
A stronglyPlug and Play system may therefore be distin-guishable from a weaker one by its behaviour inthe absence of certain device specic knowledge.If the relevant knowledge is present, one cannot becertain whether it was pre-congured or uploaded\on demand".Plug and Play also enforces a certain sort ofmodularity on the system.
Since devices must de-clare the information required to update the dia-logue components, a clear interface is provided forre-conguring the system for new types of deviceas well as a clearer picture of the internal structureof those dialogue components.
Indeed, it is reallyjust a design choice whether device knowledge is infact installed only when the device is plugged in.One may, for example, choose to optimize recog-nition performance on the set of devices actuallyinstalled by not loading information about otherdevices.
Alternatively, one might prefer to rec-ognize the names of devices not installed so thathelpful error messages can be delivered.Potentially, each component in a spoken lan-guage interface (recognizer, parser, interpreter, di-alogue manager etc.)
can be updated by informa-tion from a device in a Plug and Play domain.
Dif-ferent components might support dierent degreesof strength of the Plug and Play notion.
Further-more, dierent instantiations of these componentsmay require very dierent sorts of update.
Totake a very simple example, if recognition is car-ried out by a statistically trained language model,then updating this with information pertinent to aparticular device will evidently be a signicantlydierent task from updating a recognizer whichuses a grammar-based language model.Our current demonstrator program instantiatesa Plug and Play capability for recognition, parsingand context independent semantic analysis and isbuilt on top of the Nuance toolkit (Nuance Com-munications, 1999).
The next section discussesthe capability in detail.
Section 5 discusses andmakes some proposals for Plug and Play DialogueManagement.4 Distributed Grammar4.1 IntroductionIn this section, we will describe how we have ad-dressed the issues that arise when we attempt toapply the (strong) Plug and Play scenario to thetasks of speech recognition and language process-ing.
Each device will provide the knowledge thatthe speech interface needs in order to recognise thenew types of utterance relevant to the device inquestion, and convert these utterances into well-formed semantic representations.Let's start by considering what this means inpractice.
There are in fact a whole range of pos-sible scenarios to consider, depending on how thespeech understanding module is congured.
If themodule's construction is simple enough, there maybe no signicant problems involved in extendingit to oer Plug and Play functionality.
For ex-ample, the command vocabulary oered by thespeech interface may just consist of a list of xedphrases.
In this case, Plug and Play speech recog-nition becomes trivial: each device contributes thephrases it needs, after which they can be com-bined into a single grammar.
An approach ofthis kind fails however to scale up to an interfacewhich supports complex commands, in particularcommands which combine within the same utter-ance language referring to two or more dierentdevices.
For example, a command may addressseveral devices at once (\turn on the radio and theliving room light"); alternately, several commandsmay be combined into a single utterance (\switchon the cooker and switch o the microwave").
Ourexperience with practical spoken device interfacessuggests that examples like these are by no meansuncommon.Another architecture relatively easy to com-bine with Plug and Play is doing recognitionthrough a general large-vocabulary recogniser,and language-processing through device-specicphrase-spotting (Milward, 2000).
The recogniserstays the same irrespective of how many devicesare connected, so there are by denition no prob-lems at the level of speech recognition, and it is inprinciple possible to support complex commands.The main drawback, however, is that recognitionquality is markedly inferior compared to a systemin which recognition coverage is limited to the do-main dened by the current set of devices.Modern speech interfaces supporting complexcommands are typically specied using a rule-based grammar formalism dened by a platformlike Nuance (Nuance Communications, 1999) orSpeechWorks (Inc, 2001).
The type of grammarsupported is some subset of full CFG, extended toinclude semantic annotations.
Grammar rules de-ne the language model that constrains the recog-nition process, tuning it to the domain in order toachieve high performance.
(They also supply thesemantic rules that dene the output representa-tion; we will return to this point later).
If we wantto implement an ambitious Plug and Play speechrecognition module within this kind of framework,we have two top-level goals.
On the one hand, wewant to achieve high-quality speech recognition.At the same time, standard software engineeringconsiderations suggest that we want to minimizethe overlap between the rule-sets contributed byeach device: ideally, the device will only uploadthe specic lexical items relevant to it.It turns out that our software engineering ob-jectives conict to some extent with our initialgoal of achieving high-quality speech recognition.Consider a straightforward solution, in which thegrammatical information contributed by each de-vice consists purely of lexical entries, i.e.
entriesof the form<Nonterminal> --> <Terminal>In a CFG-based framework, this implies that wehave a central device-independent CFG grammar,which denes the other rules which link togetherthe nonterminals that appear on the left-hand-sides of the lexical rules.
The crucial question iswhat these lexical non-terminal symbols will be.Suppose, for concreteness, that we want our setof devices to include lights with dimmer switches,which will among other things accept commandslike \dim the light".
We might achieve this bymaking the device upload lexical rules of the roughformTRANSITIVE_VERB --> dimNOUN --> lightwhere the LHSs are conventional grammatical cat-egories.
(We will for the moment skip over thequestion of how to represent semantics).
The lex-ical rules might combine with general grammarrules of the formCOMMAND --> TRANSITIVE_VERB NPNP --> DET NOUNDET --> theThis kind of solution is easy to understand, but ex-perience shows that it leads to poor speech recog-nition.
The problem is that the language modelproduced by the grammar is underconstrained: itwill in particular allow any transitive verb to com-bine with any NP.
However, a verb like \dim" willonly combine with a restricted range of possibleNPs, and ideally we would like to capture thisfact.
What we really want to do is parameterisethe language model.
In the present case, we wantto parameterise the TRANSITIVE VERB \dim" withthe information that it only combines with objectNPs that can be used to refer to dimmable de-vices.
We will parameterise the NP and NOUNnon-terminals similarly.
The obvious way to dothis within the bounds of CFG is to specialise therules approximately as follows:COMMAND --> TRANS_DIM_VERB DIMMABLE_NPDIMMABLE_NP --> DET DIMMABLE_NOUNTRANS_DIM_VERB --> dimDIMMABLE_NOUN --> lightDET --> theUnfortunately, however, this defeats the originalobject of the exercise, since the \general" rulesnow make reference to the device-specic conceptof dimming.
What we want instead is a moregeneric treatment, like the following:COMMAND -->TRANSITIVE_VERB:[sem_obj_type=T]NP:[sem_type=T]NP:[sem_type=T] -->DET NOUN:[sem_type=T]DET --> theTRANSITIVE_VERB:[sem_obj_type=dimmable]--> dimNOUN:[sem_type=dimmable] --> lightThis kind of parameterisation of a CFG is notin any way new: it is simply unication gram-mar (Pullum and Gazdar, 1982; Gazdar et al,1985).
Thus our rst main idea is to raise thelevel of abstraction, formulating the device gram-mar at the level of unication grammars, andcompiling these down into the underlying CFGrepresentation.
There are now a number of sys-tems which can perform this type of compilation(Moore, 1998; Kiefer and Krieger, 2000); the ba-sic methods we use in our system are describedin detail elsewhere (Rayner et al, 2001a).
Here,we focus on the aspects that are required for \dis-tributed" unication grammars needed for Plugand Play.4.2 \Unication grammars meetobject-oriented programming".Our basic idea is to start with a general device-independent unication grammar, which imple-ments the core grammar rules.
In our prototype,there are 34 core rules.
Typical examples arethe NP conjunction and PP modications rules,schematicallyNP --> NP CONJ NPNP --> NP PPwhich are likely to occur in connection with anykind of device.
These rules are parameterised byvarious features.
For example, the set of featuresassociated with the NP category includes gram-matical number (singular or plural), WH (plus orminus) and sortal type (multiple options).Each individual type of device can extend thecore grammar in one of three possible ways:New lexical entries A device may add lexicalentries for device-specic words and phrases;e.g., a device will generally contribute at leastone noun used to refer to it.New grammar rules A device may add device-specic rules; e.g., a dimmer switch may in-clude rules for dimming and brightening, like\another X percent" or \a bit brighter".New feature values Least obviously, a devicemay extend the range of values that a gram-matical feature can take (see further below).For usual software engineering reasons, we nd itconvenient to divide the distributed grammar intomodules; the grammatical knowledge associatedwith a device may reside in more than one module.The grammar in our current demonstrator con-tains 21 modules, including the \core" grammardescribed above.
Each device typically requiresbetween two and ve modules.
For example, anon/o light switch loads three modules: the coregrammar, the general grammar for on/o switch-able devices, and the grammar specically foron/o switchable lights.
The core grammar, as al-ready explained, consists of linguistically orienteddevice-independent grammar rules.
The mod-ule for on/o switchable devices contains gram-mar rules specic to on/o switchable behaviour,which in general make use of the framework es-tablished by the general grammar.
For example,there are rules of the schematic formQUESTION -->isNP:[sem_type=device]ON_OFF_PHRASEPARTICLE_VERB:[particle_type=onoff]--> switchFinally, the module for on/o switchable lights isvery small, and just consists of a handful of lexi-cal entries for nouns like \light", dening these asnouns referring to on/o switchable devices.
Theway in which nouns of this kind can combine ishowever dened entirely by the on/o switchabledevice grammar and core grammar.The pattern here turns out to be the usual one:the grammar appropriate to a device is composedof a chain of modules, each one depending on theprevious link in the chain and in some way special-ising it.
Structurally, this is similar to the organ-isation of a piece of normal object-oriented soft-ware, and we have been interested to discover thatmany of the standard concepts of object-orientedprogramming carry over naturally to distributedunication grammars.
In the remainder of the sec-tion, we will expand on this analogy.If we think in terms of Java or a similar main-stream OO language, a major grammatical con-stituent like S, NP or PP has many of the prop-erties of an OO interface.
Grammar rules in onemodule can make reference to these constituents,letting rules in other modules implement theirdenition.
For example, the temperature sen-sor grammar module contains a small number ofhighly specialised rules, e.g.QUESTION -->what is the temperaturePP:[pp_type=location]QUESTION -->how many degrees is itPP:[pp_type=location]The point to note here is that the temperaturesensor grammar module does not dene the loca-tive PP construction; this is handled elsewhere,currently in the core grammar module.
The up-shot is that the temperature sensor module is ableto dene its constructions without worrying aboutthe exact nature of the locative PP construction.As a result, we were for instance able to upgradethe PP rules to include conjoined PPs (thus allow-ing e.g.
\what is the temperature in the kitchenand the living room") without in any way alter-ing the grammar rules in the temparature sensormodule22An ambitious treatment of conjunction might ar-guably also necessitate changes in the dialogue man-agement component specic to the temperature sen-sor device.
In the implemented system, conjunctionis uniformly treated as distributive, so \what is thetemperature in the kitchen and the living room" is au-In order for the scheme to work, the \interfaces"{ the major categories { naturally need to be well-dened.
In practice, this implies restrictions onthe way we handle three things: the set of syntac-tic features associated with a category, the rangeof possible values (the domain) associated witheach feature, and the semantics of the category.We consider each of these in turn.Most obviously, we need to standardise thefeature-set for the category.
At present, we de-ne most major categories in the core grammarmodule, to the extent of specifying there the fullrange of features associated with each category.It turns out, however, that it is sometimes desir-able not to x the domain of a feature in the coregrammar, but rather to allow this domain to beextended as new modules are added.
The issuesthat arise here are interesting, and we will discussthem in some detail.The problems occur primarily in connectionwith features mediating sortal constraints.
Aswe have already seen in examples above, mostconstituents will have at least one sortal fea-ture, encoding the sortal type of the constituent;there may also be further features encoding thesortal types of possible complements and ad-juncts.
For example, the V category has a fea-ture vtype encoding the sortal type of the V it-self, a feature obj sem np type encoding the sor-tal type of a possible direct object, and a featurevp modifiers type encoding the sortal type of apossible postverbal modier.Features like these pose two interrelated prob-lems.
First, the plug and play scenario impliesthat we cannot know ahead of time the whole do-main of a sortal feature.
It is always possible thatwe will connect a device whose associated gram-mar module requires denition of a new sortaltype, in order to enforce appropriate constraints inthe language model.
The second problem is thatit is still often necessary to dene grammar rulesreferring to sortal features before the domains ofthese features are known: in particular, the coremodule will contain many such rules.
Even beforeknowing the identity of any specic devices, gen-eral grammar rules may well want to distinguishbetween \device" NPs and \location" NPs.
Forexample, the general \where-question" rule hasthe formQUESTION --> where is NPHere, we prefer to constrain the NP so as to makeit refer only to devices, since the system currentlytomatically interpreted as equivalent to \what is thetemperature in the kitchen and what is the tempera-ture in the living room'.has no way to interpret a where question referringto a room, e.g.
\where is the bathroom".We have addressed these issues in a natural wayby adapting the OO-oriented idea of inheritance:specically, we dene a hierarchy of possible fea-ture values, allowing one feature value to inheritfrom another.
In the context of the \where isNP" rule above, we dene the rule in the coremodule; in this module, the sortal NP featuresem np type may only take the two values deviceand location, which we specify with the declara-tion3domain(sem_np_type, [location, device])This allows us to write the constrained \where is"rule asQUESTION -->where is NP:[sem_np_type=device]Suppose now that we add modules for both on/oswitchable and dimmable devices; we would liketo make these into distinct sortal types, calledswitchable device and dimmable device.
Wedo this by including the following declarations inthe \switchable" module:domain(sem_np_type,[location,device,switchable_device])specialises(switchable_device, device)and correspondingly in the \dimmable" module:domain(sem_np_type,[location,device,dimmable_device])specialises(dimmable_device, device)When all these declarations are combined atcompile-time, the eect is as follows.
The do-main of the sem np type feature is now theunion of the domains specied by each compo-nent, and is thus the set flocation, device,switchable device, dimmable deviceg.
Sinceswitchable device and dimmable device arethe precise values specialising device, the com-piler systematically replaces the original featurevalue device with the disjunctionswitchable_device \/ dimmable_deviceThus the \where is" rule now becomesQUESTION -->where isNP:[sem_np_type=switchable_device \/dimmable_device]3We have slightly simplied the form of the decla-ration for expository purposes.If new modules are added which further specialiseswitchable device, then the rule will again beadjusted by the compiler so as to include appropri-ate new elements in the disjunction.
The impor-tant point to notice here is that no change is madeto the original rule denition; in line with nor-mal OO thinking, the feature domain informationis distributed across several independent modules,and the changes occur invisibly at compile-time4.We have so far said nothing about how we dealwith semantics, and we conclude the section bysketching our treatment.
In fact, it is not clearto us that the demands of supporting Plug andPlay greatly aect semantics.
If they do, themost important practical consideration is proba-bly that plug and play becomes easier to realiseif the semantics are kept simple.
We have at anyrate adopted a minimal semantic representationscheme, and the lack of problems we have experi-enced with regard to semantics may partly be dueto this.The annotated CFG grammars produced by ourcompiler are in normal Nuance Grammar Speci-cation Language (GSL) notation, which includessemantics; unication grammar rules encode se-mantics using the distinguished feature sem, whichtranslates into the GSL return construction.
Sofor example the unication grammar rulesDEVICE_NOUN:[sem=light] --> lightDEVICE_NOUN:[sem=heater] --> heatertranslates into the GSL ruleDEVICE_NOUN[ light {return(light)}heater {return(heater)}]Unication grammar rules may contain variables,translating down into GSL variables; so for exam-ple,NP:[sem=[D, N]] -->DET:[sem=D]NOUN:[sem=N]translates into the GSL ruleNP (DET:d NOUN:n) {return(($d $n))}Our basic semantic representation is a form of fea-ture/value notation, extended to allow handling4Readers familiar with OO methodology maybe disturbed by the fact that the rule appearsto have been attached to the daughter nodes(switchable device dimmable device, etc), ratherthan to the mother device node.
We would argue thatthe rule is still conceptually attached to the devicenode, but that the necessity of eventually realising itin CFG form implies that it must be compiled in thisway, so that it can later be expanded into a separateCFG rule for each daughter.of conjunction.
We allow four types of semanticconstruction: Simple values, e.g.
light, heater.
Typicallyassociated with lexical entries. Feature/value pairs expressed in list no-tation, e.g.
[device, light], [location,kitchen].
These are associated with nouns,adjectives and similar constituents. Lists of feature/value pairs, e.g.
[[device,light], [location, kitchen]].
These areassociated with major constituents such asNP, PP, VP and S. Conjunctions of lists of feature/value pairs,e.g.
[and, [[device, light]], [[device,heater]]] These represent conjoined con-stituents, e.g.
conjoined NPs, PPs and Ss.This scheme makes it straightforward to write thesemantic parts of grammar rules.
Most often, therule just concatenates the semantic contributionsof its daughters: thus for example the semanticfeatures of the nominal PP rule are simplyNP:[sem=concat(Np, Pp)] -->NP:[sem=Np]PP:[sem=Pp]The semantic output of a conjunction rule is typ-ically the conjunction of its daughters excludingthe conjunction itself, e.g.NP:[sem=[and, Np1, Np2]] -->NP:[sem=Np1]andNP:[sem=Np2]5 Future Plug & Play workIn the future, we intend to move to a systemin which all dialogue components can be recon-gured by devices.
For example, in a completePlug and Play scenario, the possible device ac-tions themselves should be declared by devicesperhaps following UPNP standards in which de-vices publish all interface commands in the formactionname(arg1...argi) plus an internal statemodel of a simple vector of values.
In this sectionwe start with some very general observations onPlug and Play dialogue management and the roleof inference.
Then we outline a proposal for a rulebased formalism.At a very general level of course, indirectionsbetween executable actions and linguistic contentscan arise at several levels: the speech act level(\It's too warm in here"), the content level (\Howwarm is it?
"), as well through underdeterminationof contents either through pronominal or ellipti-cal constructions.
At the moment, our pronom-inal and elliptical resolution methods depend onvery simple `matching' algorithms.
In general, onemight at least want some sort of consistency checkbetween the linguistic properties expressed in anutterance and those of candidate objects referredto.
One might expect that inferential elements incontextual interpretation should be strongly Plugand Play - they will depend, for correctness ande?ciency, on tailoring to the current objects inthe domain.
The research project of uploadingrelevant axioms and meaning postulates from adevice to a general purpose inference engine thatcan be invoked in contextual resolution looks veryexciting.Evidently, higher pragmatic relations betweenwhat the user \strictly says" and possible deviceoperations are also very heavily inference based.At the moment, we simply encode any neces-sary inferences directly into the device grammarsand this su?ces to deal with certain simple be-haviours.
However, the requirement to encapsu-late all device behaviour in a Plug and Play man-ner imposes a signicant requirement.
For ex-ample, the most natural interaction with a ther-mometer is, for example, \How warm is it?"
or\What is the temperature?"
and not \Query thethermometer".
In our demonstrator, the (gram-mar derived) semantic values simply reect di-rectly the relevant device operations: h op=querydevice=thermometeri.
The strategy supports thesimple natural interactions it is designed to.
Iteven interacts tolerably well with our ellipsis andreference resolution methods.
\What is the tem-perature in the hall?
And in the living room?
"and \What is the temperature in the hall?
Whatis it in the living room?"
can both be correctlyinterpreted.
Other interactions are less natural.The default output when resolution cannot iden-tify a device N is \I don't know which N youmean".
However, asking for the temperature in aroom with several thermometers should probablynot result in \I don't know which temperature youmean".
It follows that prescribing all behaviour ina Plug and Play fashion is a signicant constraint.Indeed, a more general point can be made here.A problem has arisen because the inference fromservice required to service provider has become in-secure in the presence of other service providers.In the highly networked homes of the future, moresophisticated inference may be required just be-cause service level concepts will predominate overdevice level concepts.5.1 A Rule based formalismIn this section we assume that semanticvalues consist of 5-ary slot-structure withslots device class (dimmable light, TV .
.
.
),device attributes (kitchen, blue .
.
.
), pronoun(as before) and device-specifier (as before)and operation.
An operation is the actionthe user wants carried out, e.g.
switch on,set level(X), where X is a real number (fordimmable lights), set program(Y) etc.
(for Hi-Fis, TVs), and so on.
As in the grammar, deviceclasses are ordered in a hierarchy in a standardobject-oriented way.
Thus \dimmable light" is asubclass of \dimmable device" and inherits fromit.For strong plug and play, at least the follow-ing information must be loaded by a device intothe dialogue manager: the device interface (e.g.that the \switchable light" class has a switch onmethod; the feedback to the user; the update tothe system's device model generated by executingthe command.
Clearly, behaviour can also dependon the current state.
Reaction to \switch on thekitchen light" depends on whether the lamp is o,on, and whether there is a kitchen light.
We writea rule as command(A,B,C,D) where A is a com-mand, B is a class of devices for which A is appli-cable, C is a list of device attributes whose valuesmust be known in order to execute A, D is a list ofitems describing how the system should react onA.
Each item has the following components:precondition(X) | X is an executable proce-dure that tests the network state and returnstrue or false.
If true, the item can `re'.action(Y) | Y is the procedure to executefeedback(Z,R) | Z is the feedback for the userand can depend on R, the return value fromthe device operationupd(W,R) | W describes how the system'smodel of the network state should be up-dated.
Also W can depend on R.For example switching ona light might be encoded ascommand(switch; switchable light; [id = ID]; D)where D is a list of items in the above form ofwhich one describes behaviour when the light isalready o thus:[ precondition(light off(ID));action(switch on light(ID));feedback(light now on(ID); success);feedback(could not switch(ID); error);upd([dev update(ID; status= 1)]; success);upd([]; failure) ]light off and switch on light are proceduresprovided by the lamp.
The feedback to the userand the update rules depend on the result of theswitch on light procedure.6 ConclusionApplying the idea of plug and play to spoken di-alogue interfaces poses a number of interestingand important problems.
Since the linguistic anddialogue management information is distributedthroughout the network, a plug and play systemmust update its speech interface whenever a newdevice is connected.
In this paper, we have fo-cussed in particular on distributed grammars forplug and play speech recognition which we haveintegrated into our demonstrator system.
We havealso examined some issues and described a possi-ble approach to distributed dialogue managementwhich we plan to undertake in further work.AcknowledgmentsWe are very grateful to our partners in theDHomme project for discussion of the above ideas- especially on the importance and role of dier-ing strengths of Plug and Play.
The DHommeproject partners include netdecisions Ltd, SRI In-ternational, Telia Research AB, and the Universi-ties of Edinburgh, Gothenburg and Seville.ReferencesN.M.
Fraser and J.H.S.
Thornton.
1995.
Vocalist:A robust, portable spoken language dialoguesystem for telephone applications.
In Proc.
ofEurospeech '95, pages 1947{1950, Madrid.Gerald Gazdar, Ewan Klein, Georey Pullum,and Ivan Sag.
1985.
Generalized Phrase Struc-ture Grammar.
Harvard University Press, Cam-bridge, MA.J.R Glass.
1999.
Challenges for spoken dialoguesystems.
In Proc.
IEEE ASRU Workshop, Key-stone, CO.A.
Goldschen and D Loehr.
1999.
The role of thedarpa communicator architecture as a humancomputer interface for distributed simulations.In 1999 SISO Spring Simulation Interoperabil-ity Workshop, Orlando, Florida, March 1999.SpeechWorks Int Inc, 2001.
SpeechWorks.http://www.speechworks.com.
As at 31/01/01.B.
Kiefer and H. Krieger.
2000.
A context-freeapproximation of head-driven phrase structuregrammar.
In Proceedings of 6th Int.
Workshopon Parsing Technologies, pages 135{146.A.
Kolzer.
1999.
Universal dialogue specicationfor conversational systems.
In Proceedings ofIJCAI'99 Workshop on Knowledge & Reason-ing In Practical Dialogue Systems, Stockholm.S.
Larsson and D. Traum.
2000.
Information stateand dialogue management in the trindi dialoguemove engine toolkit.
Nat.Lang.
Engineering, 6.Microsoft, 2000.
Universal Plug and Play DeviceArchitecture.
http://www.upnp.org.
Version1.0, 8 June 2000.D.
Milward.
2000.
Distributing representation forrobust interpretation of dialogue utterances.
InProc.
of 38th ACL, Hong Kong, pages 133{141.R.
Moore.
1998.
Using natural language knowl-edge sources in speech recognition.
In Proceed-ings of the NATO Advanced Studies Institute.Nuance Communications, 1999.
Nuance SpeechRecognition System Developer's Manual version6.2.
1380 Willow Road, Menlo Park, CA 94025.S.
Oaks and H. Wong.
2000.
Jini in a Nutshell.O'Reilly.Georey K. Pullum and Gerald Gazdar.
1982.natural languages and context-free languages.Llinguistics and Philosophy, 4:471{504.M.
Rayner, J. Dowding, and B.A.
Hockey.
2001a.A baseline method for compiling typed unica-tion grammars into context free language mod-els.
Proc.
Eurospeech 2001, Aalborg.M.
Rayner, G. Gorrell, B.A.
Hockey, J. Dowding,and J. Boye.
2001b.
Do cfg based languagemodels need agreement constraints?
In Pro-ceedings of 2nd NAACL, Pittsburgh.S.
Sutton et al 1998.
Universal speech tools: Thecslu toolkit.
In Proc.
ICSLP-98, pages 3221{3224.Figure 1 - Example DialogueNetwork status: There is no TV.
There is a VCRin the TV room.U1 Switch on the televisionS1 Sorry, try againU2 Add new component (this must precededevice installation)Now, a TV is plugged into the living room.
Cur-rently we simulate this in a simple text interface,where devices and rooms are chosen from prede-ned lists.
The speech interface is then automati-cally recongured (this currently takes 29 seconds)U3 Switch on the televisionS3 The TV is now onU4 Switch on the televisionS4 The TV is already onU5a Add new componentNow, another TV is plugged into the kitchen.
Nospeech interface reconguration required this timeU5 Switch on the televisionS5 The living-room TV is already onU6 Switch on the VCRS6 The VCR is now onU7 Switch on the televisionS7 Sorry, I don't know which TV you meanU8 The television in the kitchenS8 The kitchen TV is now onU9a Add new componentNow, on-o lights are installed in the hall andbathroom.
Reconguration time is 27 secondsU9 Switch on the light in the hallS9 The hall light is now onU10 Dim the light to fty percentS10 Sorry, try again.U11a Add new componentNow, a dimmable light is installed in the kitchen.Reconguration time is 39 secondsU11 Dim the kitchen light to 50 percentS11 The light in the kitchen is now on at ftypercentU12 Dim the light in the hallS12 I'm sorry, the hall light can only be on oroU13 Turn on the light in the kitchen (misheardas \increase the light in the kitchen")S13 The kitchen light is now on at sixty per-cent
