Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 785?792Manchester, August 2008Toward a Psycholinguistically-Motivated Model of Language ProcessingWilliam SchulerComputer Science and EngineeringUniversity of Minnesotaschuler@cs.umn.eduSamir AbdelRahmanDepartment of Computer ScienceCairo Universitys.abdelrahman@fci-cu.edu.egTim MillerComputer Science and EngineeringUniversity of Minnesotatmill@cs.umn.eduLane SchwartzComputer Science and EngineeringUniversity of Minnesotalschwar@cs.umn.eduAbstractPsycholinguistic studies suggest a modelof human language processing that 1) per-forms incremental interpretation of spo-ken utterances or written text, 2) preservesambiguity by maintaining competing anal-yses in parallel, and 3) operates withina severely constrained short-term memorystore ?
possibly constrained to as fewas four distinct elements.
This paper de-scribes a relatively simple model of lan-guage as a factored statistical time-seriesprocess that meets all three of the abovedesiderata; and presents corpus evidencethat this model is sufficient to parse natu-rally occurring sentences using human-likebounds on memory.1 IntroductionPsycholinguistic studies suggest a model of humanlanguage processing with three important proper-ties.
First, eye-tracking studies (Tanenhaus et al,1995; Brown-Schmidt et al, 2002) suggest that hu-mans analyze sentences incrementally, assemblingand interpreting referential expressions even whilethey are still being pronounced.
Second, humansappear to maintain competing analyses in paral-lel, with eye gaze showing significant attention tocompetitors (referents of words with similar pre-fixes to the correct word), even relatively long af-ter the end of the word has been encountered, whenattention to other distractor referents has fallen off(Dahan and Gaskell, 2007).
Preserving ambigu-ity in a parallel, non-deterministic search like thismay account for human robustness to missing, un-known, mispronounced, or misspelled words.
Fi-nally, studies of short-term memory capacity sug-c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.gest human language processing operates within aseverely constrained short-term memory store ?possibly restricted to as few as four distinct ele-ments (Miller, 1956; Cowan, 2001).The first two observations may be taken toendorse existing probabilistic beam-search mod-els which maintain multiple competing analyses,pruned by contextual preferences and dead ends(e.g.
Roark, 2001).
But the last observation onmemory bounds imposes a restriction that untilnow has not been evaluated in a corpus study.
Cana simple, useful human-like processing model bedefined using these constraints?
This paper de-scribes a relatively simple model of language as afactored statistical time-series process that meetsall three of the above desiderata; and presentscorpus evidence that this model is sufficient toparse naturally occurring sentences using human-like bounds on memory.The remainder of this paper is organized as fol-lows: Section 2 describes some current approachesto incremental parsing; Section 3 describes a statis-tical framework for parsing using a bounded stackof explicit constituents; Section 4 describes an ex-periment to estimate the level of coverage of thePenn Treebank corpus that can be achieved withvarious stack memory limits, using a set of re-versible tree transforms, and gives accuracy resultsof a bounded-memory model trained on this cor-pus.2 BackgroundMuch work on cognitive modeling in psycholin-guistics is centered on modeling the concepts towhich utterances refer.
Coarsely, these conceptsmay correspond to activation patterns among neu-rons in specific regions of the brain.
In some the-ories, a short-term memory store of several unre-lated concepts may be retained by organizing theactivation of these concepts into compatible pat-terns, only a few of which can be reliably main-785tained (Smolensky and Legendre, 2006).
Activa-tion is then theorized to spread through and amongthese groups of concepts in proportion to somelearned probability that the concepts will be rel-evant (Anderson and Reder, 1999), with the mostactive concepts corresponding to the most likelylinguistic analyses.
Competition between rival ac-tivated groups of concepts (corresponding to in-complete linguistic analyses) has even been linkedto reading delays (Hale, 2003).This competition among mutually-exclusivevariously-activated short term memory stores ofconcepts, essentially a weighted disjunction overconjunctions of concepts, can be modeled in lan-guage understanding as simple Viterbi decoding ofa factored HMM-like time-series model (Schuleret al, in press).
In this model, concepts (corre-sponding to vectors of individuals in a first-orderworld model) are introduced and composed (viaset operations like intersection) in each hypothe-sized short-term memory store, using the elementsof the memory store as a stack.
These vectors ofindividuals can be considered a special case of vec-tors of concept elements proposed by Smolensky,with set intersection a special case of tensor prod-uct in the composition model.
Referents in thiskind of incremental model can be constrained by?
but still distinguished from ?
higher-level ref-erents while they are still being recognized.It is often assumed that this semantic con-cept composition proceeds isomorphically withthe composition of syntactic constituents (Frege,1892).
This parallel semantic and syntactic com-position is considered likely to be performed inshort-term memory because it has many of thecharacteristics of short-term memory processes,including nesting limits (Miller and Chomsky,1963) and susceptibility to degradation due to in-terruption.
Ericsson and Kintch (1995) propose atheory of long-term working memory that extendsshort-term memory, but only for inter-sententialreferences, which do seem to be retained acrossinterruptions in reading.
But while the relation-ship between competing probability distributionsin such a model and experimental reading timeshas been evaluated (e.g.
by Hale), the relationshipbetween the syntactic demands on a short-termmemory store and observations of human short-term memory limits is still largely untested.
Sev-eral models have been proposed to perform syntac-tic analysis using a bounded memory store.For example, Marcus (1980) proposed a deter-ministic parser with an explicit four-element work-ing memory store in order to model human parsinglimitations.
But this model only stores completeconstituents (whereas the model proposed in thispaper stores incompletely recognized constituents,in keeping with the Tanenhaus et al findings).
Asa result, the Marcus model relies on a suite of spe-cialized memory operations to compose completeconstituents out of complete constituents, whichare not independently cognitively motivated.Cascaded finite-state automata, as in FASTUS(Hobbs et al, 1996), also make use of a boundedstack, but stack levels in such systems are typicallydedicated to particular syntactic operations: e.g.a word group level, a phrasal level, and a clausallevel.
As a result, some amount of constituentstructure may overflow its dedicated level, and besacrificed (for example, prepositional phrase at-tachment may be left underspecified).Finite-state equivalent parsers (and thus,bounded-stack parsers) have asymptotically linearrun time.
Other parsers (Sagae and Lavie, 2005)have achieved linear runtime complexity withunbounded stacks in incremental parsing byusing a greedy strategy, pursuing locally mostprobable shift or reduce operations, conditionedon multiple surrounding words.
But without anexplicit bounded stack it is difficult to connectthese models to concepts in a psycholinguisticmodel.Abney and Johnson (1991) explore left-cornerparsing as a memory model, but again only interms of (complete) syntactic constituents.
Theapproach explored here is similar, but the trans-form is reversed to allow the recognizer to storerecognized structure rather than structures beingsought, and the transform is somewhat simpli-fied to allow more structure to be introduced intosyntactic constituents, primarily motivated by aneed to keep track of disconnected semantic con-cepts rather than syntactic categories.
Without thislink to disconnected semantic concepts, the syntaxmodel would be susceptible to criticism that theseparate memory levels could be simply chunkedtogether through repeated use (Miller, 1956).Roark?s (2001) top-down parser generates treesincrementally in a transformed representation re-lated to that used in this paper, but requires dis-tributions to be maintained over entire trees ratherthan stack configurations.
This increases the beam786width necessary to avoid parse failure.
Moreover,although the system is conducting a beam search,the objects in this beam are growing, so the recog-nition complexity is not linear, and the connectionto a bounded short-term memory store of uncon-nected concepts becomes somewhat complicated.The model described in this paper is arguablysimpler than many of the models described abovein that it has no constituent-specific mechanisms,yet it is able to recognize the rich syntactic struc-tures found in the Penn Treebank, and is stillcompatible with the psycholinguistic notion of abounded short-term memory store of conceptualreferents.3 Bounded-Memory Parsing with a TimeSeries ModelThis section describes a basic statistical framework?
a factored time-series model ?
for recogniz-ing hierarchic structures using a bounded store ofmemory elements, each with a finite number ofstates, at each time step.
Unlike simple FSA com-pilation, this model maintains an explicit represen-tation of active, incomplete phrase structure con-stituents on a bounded stack, so it can be readilyextended with additional variables that depend onsyntax (e.g.
to track hypothesized entities or rela-tions).
These incomplete constituents are relatedto ordinary phrase structure annotations through aseries of bidirectional tree transforms.
These trans-forms:1. binarize phrase structure trees into linguisti-cally motivated head-modifier branches (de-scribed in Section 3.1);2. transform right-branching sequences to left-branching sequences (described in Sec-tion 3.2); and3.
align transformed trees to an array of randomvariable values at each depth and time step ofa probabilistic time-series model (describedin Section 3.3).Following these transforms, a model can be trainedfrom example trees, then run as a parser on unseensentences.
The transforms can then be reversed toevaluate the output of the parser.
This representa-tion will ultimately be used to evaluate the cover-age of a bounded-memory model on a large corpusof tree-annotated sentences, and to evaluate the ac-curacy of a basic (unsmoothed, unlexicalized) im-plementation of this model in Section 4.It is important to note that these transformationsare not postulated to be part of the human recog-nition process.
In this model, sentences can berecognized and interpreted entirely in right-cornerform.
The transforms only serve to connect thisprocess to familiar representations of phrase struc-ture.3.1 Binary branching structureThis paper will attempt to draw conclusions aboutthe syntactic complexity of natural language, interms of stack memory requirements in incremen-tal (left-to-right) recognition.
These requirementswill be minimized by recognizing trees in a right-corner form, which accounts partially recognizedphrases and clauses as incomplete constituents,lacking one instance of another constituent yet tocome.In particular, this study will use the trees in thePenn Treebank Wall Street Journal (WSJ) corpus(Marcus et al, 1994) as a data set.
In order toobtain a linguistically plausible right-corner trans-form representation of incomplete constituents, thecorpus is subjected to another, pre-process trans-form to introduce binary-branching nonterminalprojections, and fold empty categories into non-terminal symbols in a manner similar to that pro-posed by Johnson (1998b) and Klein and Manning(2003).
This binarization is done in such a wayas to preserve linguistic intuitions of head projec-tion, so that the depth requirements of right-cornertransformed trees will be reasonable approxima-tions to the working memory requirements of a hu-man reader or listener.3.2 Right-Corner TransformPhrase structure trees are recognized in this frame-work in a right-corner form that can be mapped toand from ordinary phrase structure via reversibletransform rules, similar to those described byJohnson (1998a).
This transformed grammar con-strains memory usage in left-to-right traversal to abound consistent with the psycholinguistic resultsdescribed above.This right-corner transform is simply the left-right dual of a left-corner transform (Johnson,1998a).
It transforms all right branching sequencesin a phrase structure tree into left branching se-quences of symbols of the form A1/A2, denotingan incomplete instance of category A1lacking aninstance of category A2to the right.
These incom-plete constituent categories have the same form787a) binarized phrase structure tree:SNPNPJJstrongNNdemandPPINforNPNPposNNPNNPnewNNPNNPyorkNNPcityPOS?sNNSJJgeneralNNSNNobligationNNSbondsVPVBNVBNproppedPRTupNPDTtheNNJJmunicipalNNmarketb) result of right-corner transform:SS/NNS/NNS/NPS/VPNPNP/NNSNP/NNSNP/NNSNP/NPNP/PPNPNP/NNJJstrongNNdemandINforNPposNPpos/POSNNPNNP/NNPNNP/NNPNNPnewNNPyorkNNPcityPOS?sJJgeneralNNobligationNNSbondsVBNVBN/PRTVBNproppedPRTupDTtheJJmunicipalNNmarketFigure 1: Trees resulting from a) a binarization of a sample phrase structure tree for the sentence Strongdemand for New York City?s general obligations bonds propped up the municipal market, and b) a right-corner transform of this binarized tree.and much of the same meaning as non-constituentcategories in a Combinatorial Categorial Grammar(Steedman, 2000).Rewrite rules for the right-corner transform areshown below, first to flatten out right-branchingstructure:11The tree transforms presented in this paper will be de-fined in terms of destructive rewrite rules applied iterativelyto each constituent of a source tree, from leaves to root, andfrom left to right among siblings, to derive a target tree.
Theserewrites are ordered; when multiple rewrite rules apply to thesame constituent, the later rewrites are applied to the resultsof the earlier ones.
For example, the rewrite:A0.
.
.
A1?2?3.
.
.?A0.
.
.
?2?3.
.
.could be used to iteratively eliminate all binary-branchingnonterminal nodes in a tree, except the root.
In the notationused in this paper, Roman uppercase letters (Ai) are variablesmatching constituent labels, Roman lowercase letters (ai) arevariables matching terminal symbols, Greek lowercase lettersA1?1A2?2A3a3?A1A1/A2?1A2/A3?2A3a3A1?1A2A2/A3?2.
.
.?A1A1/A2?1A2/A3?2.
.
.then to replace it with left-branching structure:(?i) are variables matching entire subtree structure, Romanletters followed by colons, followed by Greek letters (Ai:?i)are variables matching the label and structure, respectively, ofthe same subtree, and ellipses (.
.
. )
are taken to match zeroor more subtree structures, preserving the order of ellipses incases where there are more than one (as in the rewrite shownabove).788A1A1/A2:?1A2/A3?2?3.
.
.?A1A1/A3A1/A2:?1?2?3.
.
.Here, the first two rewrite rules are applied iter-atively (bottom-up on the tree) to flatten all rightbranching structure, using incomplete constituentsto record the original nonterminal ordering.
Thethird rule is then applied to generate left-branchingstructure, preserving this ordering.
Note that thelast rewrite above leaves a unary branch at the left-most child of each flattened node.
This preservesthe nodes at which the original tree was not right-branching, so the original tree can be reconstructedwhen the right-corner transform concatenates mul-tiple right-branching sequences into a single left-branching sequence.An example of a right-corner transformed treeis shown in Figure 1(b).
An important property ofthis transform is that it is reversible.
Rewrite rulesfor reversing a right-corner transform are simplythe converse of those shown above.
The correct-ness of this can be demonstrated by dividing atree into maximal sequences of right branches (thatis, maximal sequences of adjacent right children).The first two ?flattening?
rewrites of the right-corner transform, applied to any such sequence,will replace the right-branching nonterminal nodeswith a flat sequence of nodes labeled with slashcategories, which preserves the order of the non-terminal category symbols in the original nodes.Reversing this rewrite will therefore generate theoriginal sequence of nonterminal nodes.
The finalrewrite similarly preserves the order of these non-terminal symbols while grouping them from theleft to the right, so reversing this rewrite will re-produce the original version of the flattened tree.3.3 Hierarchic Hidden Markov ModelsRight-corner transformed phrase structure treescan then be mapped to random variable positionsin a Hierarchic Hidden Markov Model (Murphyand Paskin, 2001), essentially a Hidden MarkovModel (HMM) factored into some fixed number ofstack levels at each time step.HMMs characterize speech or text as a sequenceof hidden states qt(in this case, stacked-up syn-tactic categories) and observed states ot(in thiscase, words) at corresponding time steps t. Amost likely sequence of hidden states q?1..Tcanthen be hypothesized given any sequence of ob-served states o1..T, using Bayes?
Law (Equation 2)and Markov independence assumptions (Equa-tion 3) to define a full P(q1..T| o1..T) probabil-ity as the product of a Transition Model (?A)prior probability P(q1..T)def=?tP?A(qt| qt-1) andan Observation Model (?B) likelihood probabilityP(o1..T| q1..T)def=?tP?B(ot| qt):q?1..T= argmaxq1..TP(q1..T| o1..T) (1)= argmaxq1..TP(q1..T)?P(o1..T| q1..T) (2)def= argmaxq1..TT?t=1P?A(qt| qt-1)?P?B(ot| qt) (3)Transition probabilities P?A(qt| qt-1) over com-plex hidden states qtcan be modeled using syn-chronized levels of stacked-up component HMMsin a Hierarchic Hidden Markov Model (HHMM)(Murphy and Paskin, 2001).
HHMM transitionprobabilities are calculated in two phases: a re-duce phase (resulting in an intermediate, marginal-ized state ft), in which component HMMs may ter-minate; and a shift phase (resulting in a modeledstate qt), in which unterminated HMMs transition,and terminated HMMs are re-initialized from theirparent HMMs.
Variables over intermediate ftandmodeled qtstates are factored into sequences ofdepth-specific variables ?
one for each of D levelsin the HMM hierarchy:ft= ?f1t.
.
.
fDt?
(4)qt= ?q1t.
.
.
qDt?
(5)Transition probabilities are then calculated as aproduct of transition probabilities at each level, us-ing level-specific reduce ?R and shift ?S models:P?A(qt|qt-1) =?ftP(ft|qt-1)?P(qt|ftqt-1) (6)def=?f1..DtD?d=1P?R(fdt|fd+1tqdt-1qd-1t-1)?P?S(qdt|fd+1tfdtqdt-1qd-1t) (7)with fD+1tand q0tdefined as constants.
In Viterbidecoding, the sums are replaced with argmax oper-ators.
This decoding process preserves ambiguityby maintaining competing analyses of the entirememory store.
A graphical representation of anHHMM with three levels is shown in Figure 3.Shift and reduce probabilities can then be de-fined in terms of finitely recursive Finite State Au-tomata (FSAs) with probability distributions over789d=1d=2d=3wordt=1 t=2 t=3 t=4 t=5 t=6 t=7 t=8 t=9 t=10 t=11 t=12 t=13 t=14 t=15strongdemandfornewyorkcity ?sgeneralobligationsbondsproppedup themunicipalmarket?
?
?
?
?
?
?
?
?
?
?
?
?
?
??
?
?
?NNP/NNPNNP/NNPNPpos/POS?
?
?
?VBN/PRT?
?
?
?NP/NNNP/PPNP/NPNP/NPNP/NPNP/NPNP/NNSNP/NNSNP/NNSS/VPS/VPS/NPS/NNS/NNFigure 2: Sample tree from Figure 1 mapped to qdtvariable positions of an HHMM at each stack depthd (vertical) and time step t (horizontal).
This tree uses only two levels of stack memory.
Values forfinal-state variables fdtare not shown.
Note that some nonterminal labels have been omitted; labels forthese nodes can be reconstructed from their children.transition, recursive expansion, and final-state sta-tus of states at each hierarchy level.
In simple HH-MMs, each intermediate variable is a boolean vari-able over final-state status fdt?
{0,1} and eachmodeled state variable is a syntactic, lexical, orphonetic state qdt.
The intermediate variable fdtistrue or false (equal to 1 or 0 respectively) accord-ing to ?F-Reduce if there is a transition at the levelimmediately below d, and false (equal to 0) withprobability 1 otherwise:2P?R(fdt| fd+1tqdt-1qd-1t-1)def={if fd+1t=0 : [fdt=0]if fd+1t=1 : P?F-Reduce(fdt| qdt-1, qd-1t-1)(8)where fD+1t= 1 and q0t= ROOT.Shift probabilities over the modeled variable qdtat each level are defined using level-specific tran-sition ?Q-Trans and expansion ?Q-Expand models:P?S(qdt| fd+1tfdtqdt-1qd-1t)def=??
?if fd+1t=0, fdt=0 : [qdt= qdt-1]if fd+1t=1, fdt=0 : P?Q-Trans(qdt| qdt-1qd-1t)if fd+1t=1, fdt=1 : P?Q-Expand(qdt| qd-1t)(9)where fD+1t= 1 and q0t= ROOT.
This modelis conditioned on final-state switching variables atand immediately below the current FSA level.
Ifthere is no final state immediately below the cur-rent level (the first case above), it deterministicallycopies the current FSA state forward to the nexttime step.
If there is a final state immediately be-low the current level (the second case above), it2Here [?]
is an indicator function: [?]
= 1 if ?
is true, 0otherwise.. .
.. .
.. .
.. .
.f3t?1f2t?1f1t?1q1t?1q2t?1q3t?1ot?1f3tf2tf1tq1tq2tq3totFigure 3: Graphical representation of a HierarchicHidden Markov Model.
Circles denote randomvariables, and edges denote conditional dependen-cies.
Shaded circles are observations.transitions the FSA state at the current level, ac-cording to the distribution ?Q-Trans.
And if the stateat the current level is final (the third case above),it re-initializes this state given the state at the levelabove, according to the distribution ?Q-Expand.
Theoverall effect is that higher-level FSAs are allowedto transition only when lower-level FSAs termi-nate.
An HHMM therefore behaves like a prob-abilistic implementation of a pushdown automaton(or shift?reduce parser) with a finite stack, wherethe maximum stack depth is equal to the numberof levels in the HHMM hierarchy.Figure 2 shows the transformed tree from Fig-ure 1 aligned to HHMM depth levels and timesteps.
Because it uses a bounded stack, recognitionin this model is asymptotically linear (Murphy andPaskin, 2001).This model recognizes right-corner transformedtrees constrained to a stack depth corresponding toobserved human short term memory limits.
This790HHMM depth limit sentences coverageno memory 127 0.32%1 memory element 3,496 8.78%2 memory elements 25,909 65.05%3 memory elements 38,902 97.67%4 memory elements 39,816 99.96%5 memory elements 39,832 100.00%TOTAL 39,832 100.00%Table 1: Percent coverage of right-corner trans-formed treebank sections 2?21 with punctuationomitted, using HHMMs with depth limits D fromzero to five.is an attractive model of human language process-ing because the incomplete syntactic constituentsit stores at each stack depth can be directly associ-ated with (incomplete) semantic referents, e.g.
byadding random variables over environment or dis-course referents at each depth and time step.
Ifthese referents are calculated incrementally, recog-nition decisions can be informed by the values ofthese variables in an interactive model of language,following Tanenhaus et al (1995).
The corpus re-sults described in the next section suggest that alarge majority of naturally occurring sentences canbe recognized using only three or four stack mem-ory elements via this transform.4 Empirical ResultsIn order to evaluate the coverage of this bounded-memory model, Sections 2?21 of the Penn Tree-bank WSJ corpus were transformed and mappedto HHMM variables as described in Section 3.3.
Inorder to counter possible undesirable effects of anarbitrary branching analysis of punctuation, punc-tuation was removed.
Coverage results on this cor-pus are shown in Table 1.Experiments training on transformed trees fromSections 2?21 of the WSJ Treebank, evaluatingreversed-transformed output sequences from Sec-tion 22 (development set) and Section 23 (test set),show an accuracy (F score) of 82.1% and 80.1%respectively.3 Although they are lower than thosefor state-of-the-art parsers, these results suggestthat the bounded-memory parser described here isdoing a reasonably good job of modeling syntac-tic dependencies, and therefore may have some3Using unsmoothed relative frequency estimates from thetraining set, a depth limit of D = 3, beam with of 2000, andno lexicalization.promise as a psycholinguistic model.Although recognition in this system is linear, itessentially works top-down, so it has larger run-time constants than a bottom-up CKY-style parser.The experimental system described above runs ata rate of about 1 sentence per second on a 64-bit 2.6GHz dual core desktop with a beam widthof 2000.
In comparison, the Klein and Manning(2003) CKY-style parser runs at about 5 sentencesper second on the same machine.
On sentenceslonger than 40 words, the HHMM and CKY-styleparsers are roughly equivalent, parsing at the rateof .21 sentences per second, versus .24 for theKlein and Manning CKY.But since it is linear, the HHMM parser can bedirectly integrated with end-of-sentence detection(e.g.
deciding whether ?.?
is a sentence delimiterbased on whether the words preceding it can bereduced as a sentence), or with n-gram languagemodels (if words are observations, this is simplyan autoregressive HMM topology).
The use ofan explicit constituent structure in a time seriesmodel also allows integration with models of dy-namic phenomena such as semantics and corefer-ence which may depend on constituency.
Finally,as a linear model, it can be directly applied tospeech recognition (essentially replacing the hid-den layer of a conventional word-based HMM lan-guage model).5 ConclusionThis paper has described a basic incremental pars-ing model that achieves worst-case linear timecomplexity by enforcing fixed limits on a stackof explicit (albeit incomplete) constituents.
Ini-tial results show a use of only three to four levelsof stack memory within this framework providesnearly complete coverage of the large Penn Tree-bank corpus.AcknowledgmentsThe authors would like to thank the anonymousreviewers for their input.
This research wassupported by National Science Foundation CA-REER/PECASE award 0447685.
The views ex-pressed are not necessarily endorsed by the spon-sors.791ReferencesAbney, Steven P. and Mark Johnson.
1991.
Memoryrequirements and local ambiguities of parsing strate-gies.
J. Psycholinguistic Research, 20(3):233?250.Anderson, J.R. and L.M.
Reder.
1999.
The fan effect:New results and new theories.
Journal of Experi-mental Psychology: General, 128(2):186?197.Brown-Schmidt, Sarah, Ellen Campana, andMichael K. Tanenhaus.
2002.
Reference res-olution in the wild: Online circumscription ofreferential domains in a natural interactive problem-solving task.
In Proceedings of the 24th AnnualMeeting of the Cognitive Science Society, pages148?153, Fairfax, VA, August.Cowan, Nelson.
2001.
The magical number 4 in short-term memory: A reconsideration of mental storagecapacity.
Behavioral and Brain Sciences, 24:87?185.Dahan, Delphine and M. Gareth Gaskell.
2007.
Thetemporal dynamics of ambiguity resolution: Evi-dence from spoken-word recognition.
Journal ofMemory and Language, 57(4):483?501.Ericsson, K. Anders and Walter Kintsch.
1995.Long-term working memory.
Psychological Review,102:211?245.Frege, Gottlob.
1892.
Uber sinn und bedeutung.Zeitschrift fur Philosophie und Philosophischekritik,100:25?50.Hale, John.
2003.
Grammar, Uncertainty and Sen-tence Processing.
Ph.D. thesis, Cognitive Science,The Johns Hopkins University.Hobbs, Jerry R., Douglas E. Appelt, John Bear,David Israel, Megumi Kameyama, Mark Stickel, andMabry Tyson.
1996.
Fastus: A cascaded finite-statetransducer for extracting information from natural-language text.
In Finite State Devices for NaturalLanguage Processing, pages 383?406.
MIT Press,Cambridge, MA.Johnson, Mark.
1998a.
Finite state approximation ofconstraint-based grammars using left-corner gram-mar transforms.
In Proceedings of COLING/ACL,pages 619?623.Johnson, Mark.
1998b.
PCFG models of linguistic treerepresentation.
Computational Linguistics, 24:613?632.Klein, Dan and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Compu-tational Linguistics, pages 423?430.Marcus, Mitchell P., Beatrice Santorini, and Mary AnnMarcinkiewicz.
1994.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19(2):313?330.Marcus, Mitch.
1980.
A theory of syntactic recognitionfor natural language.
MIT Press.Miller, George and Noam Chomsky.
1963.
Finitarymodels of language users.
In Luce, R., R. Bush, andE.
Galanter, editors, Handbook of Mathematical Psy-chology, volume 2, pages 419?491.
John Wiley.Miller, George A.
1956.
The magical number seven,plus or minus two: Some limits on our capacityfor processing information.
Psychological Review,63:81?97.Murphy, Kevin P. and Mark A. Paskin.
2001.
Lin-ear time inference in hierarchical HMMs.
In Proc.NIPS, pages 833?840.Roark, Brian.
2001.
Probabilistic top-down parsingand language modeling.
Computational Linguistics,27(2):249?276.Sagae, Kenji and Alon Lavie.
2005.
A classifier-basedparser with linear run-time complexity.
In Proceed-ings of the Ninth International Workshop on ParsingTechnologies (IWPT?05).Schuler, William, Stephen Wu, and Lane Schwartz.
inpress.
A framework for fast incremental interpre-tation during speech decoding.
Computational Lin-guistics.Smolensky, Paul and Ge?raldine Legendre.
2006.The Harmonic Mind: From Neural Computation toOptimality-Theoretic GrammarVolume I: CognitiveArchitecture.
MIT Press.Steedman, Mark.
2000.
The syntactic process.
MITPress/Bradford Books, Cambridge, MA.Tanenhaus, Michael K., Michael J. Spivey-Knowlton,Kathy M. Eberhard, and Julie E. Sedivy.
1995.
Inte-gration of visual and linguistic information in spokenlanguage comprehension.
Science, 268:1632?1634.792
