Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1434?1444, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsImproved Parsing and POS Tagging Using Inter-SentenceConsistency ConstraintsAlexander M. Rush1?
Roi Reichart1?
Michael Collins2 Amir Globerson31MIT CSAIL, Cambridge, MA, 02139, USA{srush|roiri}@csail.mit.edu2Department of Computer Science, Columbia University, New-York, NY 10027, USAmcollins@cs.columbia.edu3School of Computer Science and Engineering, The Hebrew University, Jerusalem, 91904, Israelgamir@cs.huji.ac.ilAbstractState-of-the-art statistical parsers and POStaggers perform very well when trained withlarge amounts of in-domain data.
When train-ing data is out-of-domain or limited, accuracydegrades.
In this paper, we aim to compen-sate for the lack of available training data byexploiting similarities between test set sen-tences.
We show how to augment sentence-level models for parsing and POS tagging withinter-sentence consistency constraints.
To dealwith the resulting global objective, we presentan efficient and exact dual decomposition de-coding algorithm.
In experiments, we addconsistency constraints to the MST parserand the Stanford part-of-speech tagger anddemonstrate significant error reduction in thedomain adaptation and the lightly supervisedsettings across five languages.1 IntroductionState-of-the-art statistical parsers and POS taggersperform very well when trained with large amountsof data from their test domain.
When training data isout-of-domain or limited, the performance of the re-sulting model often degrades.
In this paper, we aimto compensate for the lack of available training databy exploiting similarities between test set sentences.Most parsing and tagging models are defined at thesentence-level, which makes such inter-sentence in-formation sharing difficult.
We show how to aug-ment sentence-level models with inter-sentence con-straints to encourage consistent descisions in similar?
Both authors contributed equally to this work.contexts, and we give an efficient algorithm with for-mal guarantees for decoding such models.In POS tagging, most taggers perform very wellon word types that they have observed in trainingdata, but they perform poorly on unknown words.With a global objective, we can include constraintsthat encourage a consistent tag across all occur-rences of an unknown word type to improve accu-racy.
In dependency parsing, the parser can benefitfrom surface-level features of the sentence, but withsparse or out-of-domain training data these featuresare very noisy.
Using a global objective, we can addconstraints that encourage similar surface-level con-texts to exhibit similar syntactic behaviour.The first contribution of this work is the use ofMarkov random fields (MRFs) to model global con-straints between sentences in dependency parsingand POS tagging.
We represent each word as a node,the tagging or parse decision as its label, and addconstraints through edges.
MRFs allow us to includeglobal constraints tailored to these problems, and toreason about inference in the corresponding globalmodels.The second contribution is an efficient dual de-composition algorithm for decoding a global ob-jective with inter-sentence constraints.
These con-straints generally make direct inference challengingsince they tie together the entire test corpus.
To alle-viate this issue, our algorithm splits the global infer-ence problem into subproblems - decoding of indi-vidual sentences, and decoding of the global MRF.These subproblems can be solved efficiently throughknown methods.
We show empirically that by iter-atively solving these subproblems, we can find the1434exact solution to the global model.We experiment with domain adaptation andlightly supervised training.
We demonstrate thatglobal models with consistency constraints can im-prove upon sentence-level models for dependencyparsing and part-of-speech tagging.
For domainadaptation, we show an error reduction of up to 7.7%when adapting the second-order projective MSTparser (McDonald et al 2005) from newswire tothe QuestionBank domain.
For lightly supervisedlearning, we show an error reduction of up to 12.8%over the same parser for five languages and an errorreduction of up to 10.3% over the Stanford trigramtagger (Toutanova et al 2003) for English POS tag-ging.
The algorithm requires, on average, only 1.7times the costs of sentence-level inference and findsthe exact solution on the vast majority of sentences.2 Related WorkMethods that combine inter-sentence informationwith sentence-level algorithms have been applied toa number of NLP tasks.
The most similar models toour work are skip-chain CRFs (Sutton and Mccal-lum, 2004), relational markov networks (Taskar etal., 2002), and collective inference with symmetricclique potentials (Gupta et al 2010).
These mod-els use a linear-chain CRF or MRF objective mod-ified by potentials defined over pairs of nodes orclique templates.
The latter model makes use of La-grangian relaxation.
Skip-chain CRFs and collectiveinference have been applied to problems in IE, andRMNs to named entity recognition (NER) (Bunescuand Mooney, 2004).
Finkel et al(2005) also inte-grated non-local information into entity annotationalgorithms using Gibbs sampling.Our model can be applied to a variety of off-the-shelf structured prediction models.
In particular, wefocus on dependency parsing which is characterizedby a more complicated structure compared to the IEtasks addressed by previous work.Another line of work that integrates corpus-leveldeclarative information into sentence-level modelsincludes the posterior regularization (Ganchev et al2010; Gillenwater et al 2010), generalized expec-tation (Mann and McCallum, 2007; Mann and Mc-Callum, ), and Bayesian measurements (Liang et al2009) frameworks.
The power of these methods hasbeen demonstrated for a variety of NLP tasks, suchas unsupervised and semi-supervised POS taggingand parsing.
The constraints used by these worksdiffer from ours in that they encourage the posteriorlabel distribution to have desired properties such assparsity (e.g.
a given word can take a small numberof labels with a high probability).
In addition, thesemethods use global information during training asopposed to our approach which applies test-time in-ference global constraints.The application of dual decomposition for infer-ence in MRFs has been explored by Wainwright etal.
(2005), Komodakis et al(2007), and Globersonand Jaakkola (2007).
In NLP, Rush et al(2010)and Koo et al(2010) applied dual decomposition toenforce agreement between different sentence-levelalgorithms for parsing and POS tagging.
Work ondual decomposition for NLP is related to the workof Smith and Eisner (2008) who apply belief prop-agation to inference in dependency parsing, and toconstrained conditional models (CCM) (Roth andYih, 2005) that impose inference-time constraintsthrough an ILP formulation.Several works have addressed semi-supervisedlearning for structured prediction, suggesting objec-tives based on the max-margin principles (Altun andMcallester, 2005), manifold regularization (Belkinet al 2005), a structured version of co-training(Brefeld and Scheffer, 2006) and an entropy-basedregularizer for CRFs (Wang et al 2009).
The com-plete literature on domain adaptation is beyond thescope of this paper, but we refer the reader to Blitzerand Daume (2010) for a recent survey.Specifically for parsing and POS tagging, self-training (Reichart and Rappoport, 2007), co-training(Steedman et al 2003) and active learning (Hwa,2004) have been shown useful in the lightly su-pervised setup.
For parser adaptation, self-training(McClosky et al 2006; McClosky and Charniak,2008), using weakly annotated data from the tar-get domain (Lease and Charniak, 2005; Rimell andClark, 2008), ensemble learning (McClosky et al2010), hierarchical bayesian models (Finkel andManning, 2009) and co-training (Sagae and Tsujii,2007) achieve substantial performance gains.
For arecent survey see Plank (2011).
Constraints simi-lar to those we use for POS tagging were used bySubramanya et al(2010) for POS tagger adaptation.1435Their work, however, does not show how to decodea global, corpus-level, objective that enforces theseconstraints, which is a major contribution of this pa-per.Inter-sentence syntactic consistency has been ex-plored in the psycholinguistics and NLP literature.Phenomena such as parallelism and syntactic prim-ing ?
the tendency to repeat recently used syntacticstructures ?
have been demonstrated in human lan-guage corpora (e.g.
WSJ and Brown) (Dubey et al2009) and were shown useful in generative and dis-criminative parsers (e.g.
(Cheung and Penn, 2010)).We complement these works, which focus on con-sistency between consecutive sentences, and explorecorpus level consistency.3 Structured ModelsWe begin by introducing notation for sentence-level dependency parsing as a structured predictionproblem.
The goal of dependency parsing is tofind the best parse y for a tagged sentence x =(w1/t1, .
.
.
, wn/tn) with words w and POS tags t.Define the index set for dependency parsing asI(x) = {(m,h) : m ?
{1 .
.
.
n},h ?
{0 .
.
.
n},m 6= h}where h = 0 represents the root word.
A depen-dency parse is a vector y = {y(m,h) : (m,h) ?I(x)} where y(m,h) = 1 if m is a modifier of thehead word h. We define the set Y(x) ?
{0, 1}|I(x)|to be the set of all valid dependency parses for a sen-tence x.
In this work, we use projective dependencyparses, but the method also applies to the set of non-projective parse trees.Additionally, we have a scoring function f :Y(x)?
R. The optimal parse y?
for a sentence x isgiven by, y?
= argmaxy?Y(x) f(y).
This sentence-level decoding problem can often be solved effi-ciently.
For example in commonly used projec-tive dependency parsing models (McDonald et al2005), we can compute y?
efficiently using variantsof the Viterbi algorithm.For this work, we make the assumption that wehave an efficient algorithm to find the argmax off(y) +?
(m,h)?I(x)u(m,h)y(m,h) = f(y) + u ?
ywhere u is a vector in R|I(x)|.
In practice, u will bea vector of Lagrange multipliers associated with thedependencies of y in our dual decomposition algo-rithm given in Section 6.We can construct a very similar setting for POStagging where the goal is to find the best taggingy for a sentence x = (w1, .
.
.
, wn).
We skip theformal details here.We next introduce notation for Markov randomfields (MRFs) (Koller and Friedman, 2009).
AnMRF consists of an undirected graph G = (V,E),a set of possible labels for each node Li for i ?
{1, .
.
.
, |V |}, and a scoring function g. The indexset for MRFs isIMRF = {(i, l) : i ?
{1 .
.
.
|V |}, l ?
Li}?
{((i, j), li, lj) : (i, j) ?
E, li ?
Li, lj ?
Lj}A label assignment in the MRF is a binary vectorz with z(i, l) = 1 if the label l is selected at node iand z((i, j), li, lj) = 1 if the labels li, lj are selectedfor the nodes i, j.In applications such as parsing and POS tagging,some of the label assignments are not allowed.
Forexample, in dependency parsing the resulting struc-ture must be a tree.
Consequently, if every nodein the MRF corresponds to a word in a documentand its label corresponds to the index of its headword, the resulting dependency structure for eachsentence must be acyclic.
The set of all valid la-bel assignments (one label per node) is given byZ ?
{0, 1}|IMRF|.We score label assignments in the MRF with ascoring function g : Z ?
R. The best assignmentz?
in an MRF is given by, z?
= argmaxz?Z g(z).We focus on pairwise MRFs where this function g isa linear function of z whose parameters are denotedby ?g(z) = z ?
?
=?
(i,l)?IMRFz(i, l)?
(i, l) +?
((i,j),li,lj)?IMRFz((i, j), li, lj)?
((i, j), li, lj)As in parsing, we make the assumption that wehave an efficient algorithm to find the argmax ofg(z) +?
(i,l)?IMRF(x)u(i, l)z(i, l)1436He/PRP saw/VBD an/DT American/JJ man/NNThe/DT smart/JJ girls/NNS stood/VBD outside/RBDanny/DT walks/VBZ a/DT long/JJ distance/NNNNFigure 1: An example constraint from dependency pars-ing.
The black nodes are modifiers observed in the train-ing data.
Each gray node corresponds to a possible mod-ifier in the test corpus.
The constraint applies to all mod-ifiers in the context DT JJ.
The white node correspondsto the consensus POS tag of the head word of these mod-ifiers.4 A Parsing ExampleIn this section we give a detailed example of globalconstraints for dependency parsing.
The aim is toconstruct a global objective that encourages similarcontexts across the corpus to exhibit similar syntac-tic behaviour.
We implement this objective using anMRF with a node for each word in the test set.
Thelabel of each node is the index of the word it mod-ifies.
We add edges to this MRF to reward consis-tency among similar contexts.
Furthermore, we addnodes with a fixed label to incorporate contexts seenin the training data.Specifically, we say that the context of a word isits POS tag and the POS tags of some set of thewords around it.
We expand on this notion of con-text in Section 8; for simplicity we assume here thatthe context includes only the previous word?s POStag.
Our constraints are designed to bias words inthe same context to modify words with similar POStags.Figure 1 shows a global MRF over a small parsingexample with one training sentence and two test sen-tences.
The MRF contains a node associated witheach word instance, where the label of the node isthe index of the word it modifies.
In this corpus, thecontext DT JJ appears once in training and twice intesting.
We hope to choose head words with similarPOS tags for these two test contexts biased by theobserved training context.More concretely, for each context c ?
{1, .
.
.
, C}, we have a set Sc of associatedword indices (s,m) that appear in the context,where s is a sentence index and m is a positionin that sentence.
For instance, in our exampleS1 = {(1, 2), (2, 4)} consists of all positions inthe test set where we see JJ preceded by DT.Futhermore, we have a set Oc of indices (s,m,TR)of observed instances of the context in the trainingdata where TR denotes a training index.
In ourexample O1 = {(1, 4,TR)} consists of the onetraining instance.
We associate each word instancewith a single context c.We then define our MRF to include one consensusnode for each set Sc as well as a word node for eachinstance in the set Sc ?Oc.
Thus the set of variablescorresponds to V = {1, .
.
.
, C} ?
(?Cc=1 Sc ?
Oc).Additionally, we include an edge from each nodei ?
Sc?Oc to its consensus node c,E = {(i, c) : c ?
{1, .
.
.
, C}, i ?
Sc ?Oc}.
The word nodes from Schave the label set of possible head indices L(s,m) ={0, .
.
.
, ns} where ns is the length of the sentence s.The observed nodes from Oc have a singleton labelset L(s,m,TR) with the observed index.
The consen-sus nodes have the label set Lc = T ?
{NULL}where T is the set of POS tags and the NULL sym-bol represents the constraint being turned off.We can now define the scoring function g for thisMRF.
The scoring function aims to reward consis-tency among the head POS tag at each word and theconsensus node?
((i, c), li, lc) =???????
?1 if pos(li) = lc?2 if pos(li) is close to lc?3 lc = NULL0 otherwisewhere posmaps a word index to its POS tag.
The pa-rameters ?1 ?
?2 ?
?3 ?
0 determine the bonus foridentical POS tags, similar POS tags, and for turningoff the constraint .We construct a similar model for POS tagging.We choose sets Tc corresponding to the c?th un-known word type in the corpus.
The MRF graphis identical to the parsing case with Tc replacing Scand we no longer have Oc.
The label sets for theword nodes are now L(s,m) = T where the label is1437the POS tag chosen at that word, and the label set forthe consensus node is Lc = T ?
{NULL}.
We usethe same scoring function as in parsing to enforceconsistency between word nodes and the consensusnode.5 Global ObjectiveRecall the definition of sentence-level parsing,where the optimal parse y?
for a single sentencex under a scoring function f is given by: y?
=argmaxy?Y(x) f(y).
We apply this objective toa set of sentences, specified by the tuple X =(x1, ..., xr), and the product of possible parsesY(X) = Y(x1) ?
.
.
.
?
Y(xr).
The sentence-leveldecoding problem is to find the optimal dependencyparses Y ?
= (Y ?1 , ..., Y ?r ) ?
Y(X) under a globalobjectiveY ?
= argmaxY ?Y(X)F (Y ) = argmaxY ?Y(X)r?s=1f(Ys)where F : Y(X) ?
R is the global scoring func-tion.We now consider scoring functions where theglobal objective includes inter-sentence constraints.Objectives of this form will not factor directlyinto individual parsing problems; however, we canchoose to write them as the sum of two convenientterms: (1) A simple sum of sentence-level objec-tives; and (2) A global MRF that connects the localstructures.For convenience, we define the following indexset.J (X) = {(s,m, h) : s ?
{1, .
.
.
, r},(m,h) ?
I(xs)}This set enumerates all possible dependencies ateach sentence in the corpus.
We say the parses Ysare consistent with a label assignment z if for all(s,m, h) ?
J (X) we have that z((s,m), h) =Ys(m,h).
In other words, the labels in z match thehead words chosen in parse Ys.With this notation we can write the full global de-coding objective as(Y ?, z?)
= argmaxY ?Y(X), z?ZF (Y ) + g(z) (1)s.t.
?
(s,m, h) ?
J (X), z((s,m), h) = Ys(m,h)Set u(1)(s,m, h)?
0 for all (s,m, h) ?
J (X)for k = 1 to K doz(k) ?
argmaxz?Z(g(z) +?
(s,m,h)?J (X)u(k)(s,m, h)z((s,m), h))Y (k) ?
argmaxY ?Y(X)(F (Y ) ??
(s,m,h)?J (X)u(k)(s,m, h)Ys(m,h))if Y (k)s (m,h) = z(k)((s,m), h)for all (s,m, h) ?
J (X) thenreturn (Y (k), z(k))for all (s,m, h) ?
J (X),u(k+1)(s,m, h)?
u(k)(s,m, h) +?k(z(k)((s,m), h)?
Y (k)s (m,h))return (Y (K), z(K))Figure 2: The global decoding algorithm for dependencyparsing models.The solution to this objective maximizes the localmodels as well as the global MRF, while maintain-ing consistency among the models.
Specifically, theMRF we use in the experiments has a simple naiveBayes structure with the consensus node connectedto all relevant word nodes.The global objective for POS tagging has a similarform.
As before we add a node to the MRF for eachword in the corpus.
We use the POS tag set as ourlabels for each of these nodes.
The index set con-tains an element for each possible tag at each wordinstance in the corpus.6 A Global Decoding AlgorithmWe now consider the decoding question: how tofind the structure Y ?
that maximizes the global ob-jective.
We aim for an efficient solution that makesuse of the individual solvers at the sentence-level.For this work, we make the assumption that thegraph chosen for the MRF has small tree-width, e.g.our naive Bayes constraints, and can be solved effi-ciently using dynamic programming.Before we describe our dual decomposition al-gorithm, we consider the difficulty of solving theglobal objective directly.
We have an efficient dy-namic programming algorithm for solving depen-dency parsing at the sentence-level, and efficient al-gorithms for solving the MRF.
It follows that we1438could construct an intersected dynamic program-ming algorithm that maintains the product of statesover both models.
This algorithm is exact, but itis very inefficient.
Solving the intersected dynamicprogram requires decoding simultaneously over theentire corpus, with an additional multiplicative fac-tor for solving the MRF.
On top of this cost, we needto alter the internal structure of the sentence-levelmodels.In contrast, we can construct a dual decomposi-tion algorithm which is efficient, produces a certifi-cate when it finds an exact solution, and directlyuses the sentence-level parsing models.
Consideringagain the global objective of equation 1, we note thatthe difficulty in decoding this objective comes en-tirely from the constraints z((s,m), h) = Ys(m,h).If these were not there, the problem would factorinto two parts, an optimization of F over the testcorpus Y(X) and an optimization of g over possibleMRF assignments Z .
The first problem factors nat-urally into sentence-level parsing problems and thesecond can be solved efficiently given our assump-tions on the MRF topology G.Recent work has shown that a relaxation basedon dual decomposition often produces an exact so-lution for such problems (Koo et al 2010).
Toapply dual decomposition, we introduce Lagrangemultipliers u(s,m, h) for the agreement constraintsbetween the sentence-level models and the globalMRF.
The Lagrangian dual is the function L(u) =maxz g(z, u) + maxy F (y, u) whereg(z, u) = g(z) +?
(s,m,h)?J (X)u(s,m, h)z((s,m), h)),F (y, u) = F (Y ) ??
(s,m,h)?J (X)u(s,m, h)Ys(m,h)In order to find minu L(u), we use subgradient de-scent.
This requires computing g(z, u) and F (y, u)for fixed values of u, which by our assumptions fromSection 3 are efficient to calculate.The full algorithm is given in Figure 2.
We startwith the values of u initialized to 0.
At each itera-tion k, we find the best set of parses Y (k) over theentire corpus and the best MRF assignment z(k).
Wethen update the value of u based on the differencebetween Y (k) and z(k) and a rate parameter ?.
Onthe next iteration, we solve the same decoding prob-?
0.7 ?0.8 ?
0.9 1.0All Contexts 66.8 57.9 46.8 33.3Head in Context 76.0 67.9 57.2 42.3Table 1: Exploratory statistics for constraint selection.The table shows the percentage of context types for whichthe probability of the most frequent head tag is at least p.Head in Context refers to the subset of contexts where themost frequent head is within the context itself.
Numbersare based on Section 22 of the Wall Street Journal and aregiven for contexts that appear at least 10 times.lems modified by the new value of u.
If at any pointthe current solutions Y (k) and z(k) satisfy the con-sistency constraint, we return their current values.Otherwise, we stop at a max iteration K and returnthe values from the last iteration.We now give a theorem for the formal guaranteesof this algorithm.Theorem 1 If for some k ?
{1 .
.
.K} in the algo-rithm in Figure 2, Y (k)s (m,h) = z(k)(s,m, h) forall (s,m, h) ?
J , then (Y (k), z(k)) is a solution tothe maximization problem in equation 1.We omit the proof for brevity.
It is a slight variationof the proof given by Rush et al(2010).7 Consistency ConstraintsIn this section we describe the consistency con-straints used for the global models of parsing andtagging.Parsing Constraints.
Recall from Section 4 thatwe choose parsing constraints based on the wordcontext.
We encourage words in similar contexts tochoose head words with similar POS tags.We use a simple procedure to select which con-straints to add.
First define a context template tobe a set of offsets {r, .
.
.
, s} with r ?
0 ?
s thatspecify the neighboring words to include in a con-text.
In the example of Figure 1, the context tem-plate {?1, 0, 1, 2} applied to the word girls/NNSwould produce the context JJ NNS VBD RB.
Foreach word in the corpus, we consider all possibletemplates with s?
r < 4.
We use only contexts thatpredict the head POS of the context in the trainingdata with probability 1 and prefer long over shortcontexts.
Once we select the context of each word,we add a consensus node for each context type in1439the corpus.
We connect each word node to its corre-sponding consensus node.Local context does not fully determine the POStag of the head word, but for certain contexts it pro-vides a strong signal.
Table 1 shows context statis-tics for English.
For 46.8% of the contexts, the mostfrequent head tag is chosen ?
90% of the time.
Thepattern is even stronger for contexts where the mostfrequent head tag is within the context itself.
Inthis case, for 57.2% of the contexts the most fre-quent head tag is chosen ?
90% of the time.
Con-sequently, if more than one context can be selectedfor a word, we favor the contexts where the mostfrequent head POS is inside the context.POS Tagging Constraints.
For POS tagging, ourconstraints focus on words not observed in the train-ing data.
It is well-known that each word type ap-pears only with a small number of POS tags.
In Sec-tion 22 of the WSJ corpus, 96.35% of word typesappear with a single POS tag.In most test sets we are unlikely to see an un-known word more than once or twice.
To fix thissparsity issue, we import additional unannotatedsentences for each unknown word from the NewYork Times Section of the NANC corpus (Graff,1995).
These sentences give additional informationfor unknown word types.Additionally, we note that morphologically re-lated words often have similar POS tags.
We canexploit this relationship by connecting related wordtypes to the same consensus node.
We experimentedwith various morphological variants and found thatconnecting a word type with the type generated byappending the suffix ?s?
was most beneficial.
Foreach unknown word type, we also import sentencesfor its morphologically related words.8 Experiments and ResultsWe experiment in two common scenarios whereparsing performance is reduced from the fully su-pervised, in-domain case.
In domain adaptation, wetrain our model completely in one source domainand test it on a different target domain.
In lightly su-pervised training, we simulate the case where onlya limited amount of annotated data is available for alanguage.Base ST Model ERWSJ?
QTB 89.63 89.99 90.43 7.7QTB?WSJ 74.89 74.97 75.76 3.5Table 2: Dependency parsing UAS for domain adapta-tion.
WSJ is the Penn TreeBank.
QTB is the Question-Bank.
ER is error reduction.
Results are significant usingthe sign test with p ?
0.05.Data for Domain Adaptation We perform do-main adaptation experiments in English using theWSJ PennTreebank (Marcus et al 1993) and theQuestionBank (QTB) (Judge et al 2006).
In theWSJ ?
QTB scenario, we train on sections 2-21of the WSJ and test on the entire QTB (4000 ques-tions).
In the QTB?WSJ scenario, we train on theentire QTB and test on section 23 of the WSJ.Data for Lightly Supervised Training For allEnglish experiments, our data was taken from theWSJ PennTreebank: training sentences from Sec-tion 0, development sentences from Section 22, andtest sentences from Section 23.
For experimentsin Bulgarian, German, Japanese, and Spanish, weuse the CONLL-X data set (Buchholz and Marsi,2006) with training data taken from the official train-ing files.
We trained the sentence-level models with50-500 sentences.
To verify the robustness of ourresults, our test sets consist of the official test setsaugmented with additional sentences from the offi-cial training files such that each test file consists of25,000 words.
Our results on the official test sets arevery similar to the results we report and are omittedfor brevity.Parameters The model parameters, ?1, ?2, and ?3of the scoring function (Section 4) and ?
of theLagrange multipliers update rule (Section 6), weretuned on the English development data.
In our dualdecomposition inference algorithm, we use K =200 maximum iterations and tune the decay rate fol-lowing the protocol described by Koo et al(2010).Sentence-Level Models For dependency parsingwe utilize the second-order projective MST parser(McDonald et al 2005)1 with the gold-standardPOS tags of the corpus.
For POS tagging we usethe Stanford POS tagger (Toutanova et al 2003)2.1http://sourceforge.net/projects/mstparser/2http://nlp.stanford.edu/software/tagger.shtml144050 100 200 500Base ST Model (ER) Base ST Model (ER) Base ST Model (ER) Base ST Model (ER)Jap 79.10 80.19 81.78 (12.82) 81.53 81.59 83.09 (8.45) 84.84 85.05 85.50 (4.35) 87.14 87.24 87.44 (2.33)Eng 69.60 69.73 71.62 (6.64) 73.97 74.01 75.27 (4.99) 77.67 77.68 78.69 (4.57) 81.83 81.90 82.18 (1.93)Spa 71.67 71.72 73.19 (5.37) 74.53 74.63 75.41 (3.46) 77.11 77.09 77.44 (1.44) 79.97 79.88 80.04 (0.35)Bul 71.10 70.59 72.13 (3.56) 73.35 72.96 74.61 (4.73) 75.38 75.54 76.17 (3.21) 81.95 81.75 82.18 (1.27)Ger 68.21 68.28 68.83 (1.95) 72.19 72.29 72.76 (2.05) 74.34 74.45 74.95 (2.4) 77.20 77.09 77.51 (1.4)Table 3: Dependency parsing UAS by size of training set and language.
English data is from the WSJ.
Bulgarian,German, Japanese, and Spanish data is from the CONLL-X data sets.
Base is the second-order, projective dependencyparser of McDonald et al(2005).
ST is a self-training model based on Reichart and Rappoport (2007).
Model is thesame parser augmented with inter-sentence constraints.
ER is error reduction.
Using the sign test with p ?
0.05, all50, 100, and 200 results are significant, as are Eng and Ger 500.50 100 200 500Base Model (ER) Base Model (ER) Base Model (ER) Base Model (ER)Acc 79.67 81.77 (10.33) 85.42 86.37 (6.52) 88.63 89.37 (6.51) 91.59 91.98 (4.64)Unk 62.88 67.16 (11.53) 71.10 73.32 (7.68) 75.82 78.07 (9.31) 80.67 82.28 (8.33)Table 4: POS tagging accuracy.
Stanford POS tagger refers to the maximum entropy trigram tagger of Toutanova etal.
(2003).
Our inter-sentence POS tagger augments this baseline with global constraints.
ER is error reduction.
Allresults are significant using the sign test with p ?
0.05.Evaluation and Baselines To measure parsingperformance, we use unlabeled attachment score(UAS) given by the CONLL-X dependency parsingshared task evaluation script (Buchholz and Marsi,2006).
We compare the accuracy of dependencyparsing with global constraints to the sentence-leveldependency parser of McDonald et al(2005) and toa self-training baseline (Steedman et al 2003; Re-ichart and Rappoport, 2007).
The parsing baseline isequivalent to a single round of dual decomposition.For the self-training baseline, we parse the test cor-pus, append the labeled test sentences to the trainingcorpus, train a new parser, and then re-parse the testset.
We run this procedure for a single iteration.For POS tagging we measure token level POS ac-curacy for all the words in the corpus and also forunknown words (words not observed in the train-ing data).
We compare the accuracy of POS taggingwith global constraints to the accuracy of the Stan-ford POS tagger 3.Domain Adaptation Accuracy Results are pre-sented in Table 2.
The constrained model reducesthe error of the baseline on both cases.
Note thatwhen the base parser is trained on the WSJ corpus itsUAS performance on the QTB is 89.63%.
Yet, theconstrained model is still able to reduce the baselineerror by 7.7%.3We do not run self-training for POS tagging as it has beenshown unuseful for this application (Clark et al 2003).Lightly Supervised Accuracy The parsing resultsare given in Table 3.
Our model improves overthe baseline parser and self-training across all lan-guages and training set sizes.
The best results arefor Japanese and English with error reductions of2.33 ?
12.82% and 1.93 ?
6.64% respectively.
Theself-training baseline achieves small gains on somelanguages, but generally performs similarly to thestandard parser.The POS tagging results are given in Table 4.
Ourmodel improves over the baseline tagger for the en-tire training size range.
For 50 training sentenceswe reduce 10.33% of the overall error, and 11.53%of the error on unknown words.
Although the taggerperformance substantially improves when the train-ing set grows to 500 sentences, our model still pro-vides an overall error reduction of 4.64% and of8.33% for unknown words.9 DiscussionEfficiency Since dual decomposition often re-quires hundreds of iterations to converge, a naive im-plementation would be orders of magnitude slowerthan the underlying sentence-level model.
We usetwo techniques to speed-up the algorithm.First, we follow Koo et al(2010) and use lazydecoding as part of dual decomposition.
At each it-eration k, we cache the result of the MRF z(k) andset of parse tree Y (k).
In the next iteration, we only144150 100 150 200iteration0.00.20.40.60.81.01.21.4percentage ofparsing timeenglishgermanjapaneseFigure 3: Efficiency of dependency parsing decoding forthree languages.
The plot shows the speed of each iter-ation of the subgradient algorithm relative to a round ofunconstrained parsing.Most Effective ContextsWSJ?
QTB QTB?WSJWRB VBP VBD NN NN ,DT JJS NN IN IN PRP VBZVBP PRP VB JJ JJ NN ,DT NN NN VB IN JJ JJ NNRBS JJ NN IN NN POS NN NNTable 5: The five most effective constraint contexts fromthe domain adaptation experiments.
The bold POS tagindicates the modifier word of the context.Where/WRBVBNare/VBPdiamonds/NNSmined/VBN?How/WRBVBPdo/VBPyou/PRPmeasure/VBearthquakes/NNS?Why/WRBVBPdo/VBPpeople/NNSget/VBcalluses/NNS?VBPFigure 4: Subset of sentences with the context WRB VBPfrom WSJ?
QTB domain adaptation.
In the first round,the parser chooses VBN for the first sentence, which is in-consistent with similar contexts.
The constraints correctthis choice in later rounds.recompute the solution Y ?s for a sentence s if theweight u(s,m, h) for some m,h was updated.
Asimilar technique is applied to the MRF.Second, during the first iteration of the algorithmwe apply max-marginal based pruning using thethreshold defined by Weiss and Taskar (2010).
Thisproduces a pruned hypergraph for each sentence,which allows us to avoid recomputing parse featuresand to solve a simplified search problem.To measure efficiency, we compare the time spentin dual decomposition to the speed of unconstrainedinference.
Across experiments, the mean dual de-composition time is 1.71 times the cost of uncon-strained inference.
Figure 3 shows how this time isspent after the first iteration.
The early iterations arearound 1% of the total cost, and because of lazy de-coding this quickly drops to almost nothing.Exactness To measure exactness, we count thenumber of sentences for which we should removethe constraints in order for the model to reach con-vergence.
For dependency parsing, across languagesremoving constraints on 0.6% of sentences yieldsexact convergence.
Removing these constraints hasvery little effect on the final outcome of the model.For POS tagging, the algorithm finds an exact so-lution after removing constraints from 0.2% of thesentences.Constraint Analysis We can also look at the num-ber, size, and outcome of the constraints chosen inthe experiments.
In the lightly supervised experi-ments, the average number of constraints is 3298 for25000 tokens, where the median constraint connects19 different tokens.
Of these constraints around 70%are active (non-NULL).
The domain adaptation ex-periments have a similar number of constraints witharound 75% of constraints active.
In both experi-ments many of the constraints are found to be con-sistent after the first iteration, but as Figure 3 im-plies, other constraints take multiple iterations toconverge.Qualitative Analysis In order to understand whythese simple consistency constraints are effective,we take a qualitative look at the the domain adap-tation experiments on the QuestionBank.
Table 5ranks the five most effective contextual constraintsfrom both experiments.
For the WSJ?
QTB exper-iment, the most effective constraint relates the initalquestion word with an adjacent verb.
Figure 4 shows1442sentences where this constraint applies in the Ques-tionBank.
For the QTB?WSJ experiment, the ef-fective contexts are mostly long base noun phrases.These occur often in the WSJ but are rare in the sim-pler QuestionBank sentences.10 ConclusionIn this work we experiment with inter-sentenceconsistency constraints for dependency parsing andPOS tagging.
We have proposed a corpus-level ob-jective that augments sentence-level models withsuch constraints and described an exact and effi-cient dual decomposition algorithm for its decod-ing.
In future work, we intend to explore efficienttechniques for joint parameter learning for both theglobal MRF and the local models.Acknowledgments Columbia University gratefullyacknowledges the support of the Defense Advanced Re-search Projects Agency (DARPA) Machine Reading Pro-gram under Air Force Research Laboratory (AFRL)prime contract no.
FA8750-09-C-0181.
Any opinions,findings, and conclusions or recommendations expressedin this material are those of the author(s) and do notnecessarily reflect the view of DARPA, AFRL, or theUS government.
Alexander Rush was supported by aNational Science Foundation Graduate Research Fellow-ship.ReferencesY.
Altun and D. Mcallester.
2005.
Maximum marginsemi-supervised learning for structured variables.
InNIPS.M.
Belkin, P. Niyogi, and V. Sindhwani.
2005.
On man-ifold regularization.
In AISTATS.John Blitzer and Hal Daume.
2010.
Icml 2010 tutorialon domain adaptation.
In ICML.U.
Brefeld and T. Scheffer.
2006.
Semi-supervised learn-ing for structured output variables.
In ICML.S.
Buchholz and E. Marsi.
2006.
CoNLL-X shared taskon multilingual dependency parsing.
In CoNLL.R.C.
Bunescu and R.J. Mooney.
2004.
Collective infor-mation extraction with relational markov networks.
InACL.J.C.K Cheung and G. Penn.
2010.
Utilizing extra-sentential context for parsing.
In EMNLP.Stephen Clark, James Curran, and Miles Osborne.
2003.Bootstrapping pos taggers using unlabelled data.
InCoNLL.A.
Dubey, F. Keller, and P. Sturt.
2009.
A proba-bilistic corpus-based model of parallelism.
Cognition,109(2):193?210.Jenny Rose Finkel and Christopher Manning.
2009.
Hi-erarchical bayesian domain adaptation.
In NAACL.J.R.
Finkel, T. Grenager, and C. Manning.
2005.
In-corporating non-local information into information ex-traction systems by gibbs sampling.
In ACL.K.
Ganchev, J. Grac?a, J. Gillenwater, and B. Taskar.2010.
Posterior Regularization for Structured LatentVariable Models.
Journal of Machine Learning Re-search, 11:2001?2049.J.
Gillenwater, K. Ganchev, J. Grac?a, F. Pereira, andB.
Taskar.
2010.
Sparsity in dependency grammar in-duction.
In Proceedings of the ACL Conference ShortPapers.A.
Globerson and T. Jaakkola.
2007.
Fixing max-product: Convergent message passing algorithms formap lp-relaxations.
In NIPS.D.
Graff.
1995.
North american news text corpus.
Lin-guistic Data Consortium, LDC95T21.Rahul Gupta, Sunita Sarawagi, and Ajit A. Diwan.
2010.Collective inference for extraction mrfs coupled withsymmetric clique potentials.
JMLR.R.
Hwa.
2004.
Sample selection for statistical parsing.Computational Linguistics, 30(3):253?276.John Judge, Aoife Cahill, and Josef van Genabith.
2006.Questionbank: Creating a corpus of parse-annotatedquestions.
In ACL-COLING.D.
Koller and N. Friedman.
2009.
Probabilistic Graphi-cal Models: Principles and Techniques.
MIT Press.N.
Komodakis, N. Paragios, and G. Tziritas.
2007.MRF optimization via dual decomposition: Message-passing revisited.
In ICCV.T.
Koo, A.M.
Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In EMNLP.Matthew Lease and Eugene Charniak.
2005.
Parsingbiomedical literature.
In IJCNLP.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningfrom measurements in exponential families.
In ICML.G.S.
Mann and A. McCallum.
Generalized expectationcriteria for semi-supervised learning with weakly la-beled data.
Journal of Machine Learning Research,11:955?984.G.S.
Mann and A. McCallum.
2007.
Simple, robust,scalable semi-supervised learning via expectation reg-ularization.
In ICML.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of en-glish: The penn treebank.
Computational linguistics,19(2):313?330.1443David McClosky and Eugene Charniak.
2008.
Self-training for biomedical parsing.
In ACL, sort papers.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Reranking and self-training for parser adapta-tion.
In ACL.David McClosky, Eugene Charniak, and Mark Johnson.2010.
Automatic domain adapatation for parsing.
InNAACL.R.T.
McDonald, F. Pereira, K. Ribarov, and J. Hajic.2005.
Non-projective dependency parsing using span-ning tree algorithms.
In HLT/EMNLP.Barbara Plank.
2011.
Domain Adaptation for Parsing.Ph.d.
thesis, University of Groningen.R.
Reichart and A. Rappoport.
2007.
Self-trainingfor enhancement and domain adaptation of statisticalparsers trained on small datasets.
In ACL.Laura Rimell and Stephen Clark.
2008.
Adapting alexicalized-grammar parser to contrasting domains.
InEMNLP.D.
Roth and W. Yih.
2005.
Integer linear programminginference for conditional random fields.
In ICML.A.M.
Rush, D. Sontag, M. Collins, and T. Jaakkola.2010.
On dual decomposition and linear program-ming relaxations for natural language processing.
InEMNLP.Kenji Sagae and Junichi Tsujii.
2007.
Dependency pars-ing and domain adaptation with lr models and parserensembles.
In EMNLP-CoNLL.D.A.
Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In EMNLP.M.
Steedman, M. Osborne, A. Sarkar, S. Clark, R. Hwa,J.
Hockenmaier, P. Ruhlen, S. Baker, and J. Crim.2003.
Bootstrapping statistical parsers from smalldatasets.
In EACL.Amarnag Subramanya, Slav Petrov, and FernandoPereira.
2010.
Efficient graph-based semi-supervisedlearning of structured tagging models.
In EMNLP.C.
Sutton and A. Mccallum.
2004.
Collective segmen-tation and labeling of distant entities in informationextraction.
In In ICML Workshop on Statistical Re-lational Learning and Its Connections.B.
Taskar, P. Abbeel, and d. Koller.
2002.
Discriminativeprobabilistic models for relational data.
In UAI.K.
Toutanova, D. Klein, C.D.
Manning, and Y. Singer.2003.
Feature-rich part-of-speech tagging with acyclic dependency network.
In HLT-NAACL.M.
Wainwright, T. Jaakkola, and A. Willsky.
2005.
MAPestimation via agreement on trees: message-passingand linear programming.
In IEEE Transactions on In-formation Theory, volume 51, pages 3697?3717.Y.
Wang, G. Haffari, S. Wang, and G. Mori.
2009.A rate distortion approach for semi-supervised condi-tional random fields.
In NIPS.D.
Weiss and B. Taskar.
2010.
Structured prediction cas-cades.
In Proc.
of AISTATS, volume 1284.1444
