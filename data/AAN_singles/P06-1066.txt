Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 521?528,Sydney, July 2006. c?2006 Association for Computational LinguisticsMaximum Entropy Based Phrase ReorderingModel for Statistical Machine TranslationDeyi XiongInstitute of Computing TechnologyChinese Academy of SciencesBeijing, China, 100080Graduate School of Chinese Academy of Sciencesdyxiong@ict.ac.cnQun Liu and Shouxun LinInstitute of Computing TechnologyChinese Academy of SciencesBeijing, China, 100080{liuqun, sxlin}@ict.ac.cnAbstractWe propose a novel reordering model forphrase-based statistical machine transla-tion (SMT) that uses a maximum entropy(MaxEnt) model to predicate reorderingsof neighbor blocks (phrase pairs).
Themodel provides content-dependent, hier-archical phrasal reordering with general-ization based on features automaticallylearned from a real-world bitext.
Wepresent an algorithm to extract all reorder-ing events of neighbor blocks from bilin-gual data.
In our experiments on Chinese-to-English translation, this MaxEnt-basedreordering model obtains significant im-provements in BLEU score on the NISTMT-05 and IWSLT-04 tasks.1 IntroductionPhrase reordering is of great importance forphrase-based SMT systems and becoming an ac-tive area of research recently.
Compared withword-based SMT systems, phrase-based systemscan easily address reorderings of words withinphrases.
However, at the phrase level, reorderingis still a computationally expensive problem justlike reordering at the word level (Knight, 1999).Many systems use very simple models to re-order phrases 1.
One is distortion model (Ochand Ney, 2004; Koehn et al, 2003) which penal-izes translations according to their jump distanceinstead of their content.
For example, if N wordsare skipped, a penalty of N will be paid regard-less of which words are reordered.
This modeltakes the risk of penalizing long distance jumps1In this paper, we focus our discussions on phrases thatare not necessarily aligned to syntactic constituent boundary.which are common between two languages withvery different orders.
Another simple model is flatreordering model (Wu, 1996; Zens et al, 2004;Kumar et al, 2005) which is not content depen-dent either.
Flat model assigns constant probabili-ties for monotone order and non-monotone order.The two probabilities can be set to prefer mono-tone or non-monotone orientations depending onthe language pairs.In view of content-independency of the dis-tortion and flat reordering models, several re-searchers (Och et al, 2004; Tillmann, 2004; Ku-mar et al, 2005; Koehn et al, 2005) proposed amore powerful model called lexicalized reorder-ing model that is phrase dependent.
Lexicalizedreordering model learns local orientations (mono-tone or non-monotone) with probabilities for eachbilingual phrase from training data.
During de-coding, the model attempts to finding a Viterbi lo-cal orientation sequence.
Performance gains havebeen reported for systems with lexicalized reorder-ing model.
However, since reorderings are re-lated to concrete phrases, researchers have to de-sign their systems carefully in order not to causeother problems, e.g.
the data sparseness problem.Another smart reordering model was proposedby Chiang (2005).
In his approach, phrases are re-organized into hierarchical ones by reducing sub-phrases to variables.
This template-based schemenot only captures the reorderings of phrases, butalso integrates some phrasal generalizations intothe global model.In this paper, we propose a novel solution forphrasal reordering.
Here, under the ITG constraint(Wu, 1997; Zens et al, 2004), we need to con-sider just two kinds of reorderings, straight andinverted between two consecutive blocks.
There-fore reordering can be modelled as a problem of521classification with only two labels, straight andinverted.
In this paper, we build a maximum en-tropy based classification model as the reorderingmodel.
Different from lexicalized reordering, wedo not use the whole block as reordering evidence,but only features extracted from blocks.
This ismore flexible.
It makes our model reorder anyblocks, observed in training or not.
The wholemaximum entropy based reordering model is em-bedded inside a log-linear phrase-based model oftranslation.
Following the Bracketing Transduc-tion Grammar (BTG) (Wu, 1996), we built aCKY-style decoder for our system, which makesit possible to reorder phrases hierarchically.To create a maximum entropy based reorderingmodel, the first step is learning reordering exam-ples from training data, similar to the lexicalizedreordering model.
But in our way, any evidencesof reorderings will be extracted, not limited to re-orderings of bilingual phrases of length less than apredefined number of words.
Secondly, featureswill be extracted from reordering examples ac-cording to feature templates.
Finally, a maximumentropy classifier will be trained on the features.In this paper we describe our system and theMaxEnt-based reordering model with the associ-ated algorithm.
We also present experiments thatindicate that the MaxEnt-based reordering modelimproves translation significantly compared withother reordering approaches and a state-of-the-artdistortion-based system (Koehn, 2004).2 System Overview2.1 ModelUnder the BTG scheme, translation is morelike monolingual parsing through derivations.Throughout the translation procedure, three rulesare used to derive the translationA [ ]?
(A1, A2) (1)A ?
??
(A1, A2) (2)A ?
(x, y) (3)During decoding, the source sentence is seg-mented into a sequence of phrases as in a standardphrase-based model.
Then the lexical rule (3) 2 is2Currently, we restrict phrases x and y not to be null.Therefore neither deletion nor insertion is carried out duringdecoding.
However, these operations are to be considered inour future version of model.used to translate source phrase y into target phrasex and generate a block A.
Later, the straight rule(1) merges two consecutive blocks into a singlelarger block in the straight order; while the in-verted rule (2) merges them in the inverted order.These two merging rules will be used continuouslyuntil the whole source sentence is covered.
Whenthe translation is finished, a tree indicating the hi-erarchical segmentation of the source sentence isalso produced.In the following, we will define the model ina straight way, not in the dynamic programmingrecursion way used by (Wu, 1996; Zens et al,2004).
We focus on defining the probabilities ofdifferent rules by separating different features (in-cluding the language model) out from the ruleprobabilities and organizing them in a log-linearform.
This straight way makes it clear how rulesare used and what they depend on.For the two merging rules straight and inverted,applying them on two consecutive blocks A1 andA2 is assigned a probability Prm(A)Prm(A) = ???
?
4?LMpLM (A1,A2) (4)where the ?
is the reordering score of block A1and A2, ??
is its weight, and 4pLM (A1,A2) is theincrement of the language model score of the twoblocks according to their final order, ?LM is itsweight.For the lexical rule, applying it is assigned aprobability Prl(A)Prl(A) = p(x|y)?1 ?
p(y|x)?2 ?
plex(x|y)?3?plex(y|x)?4 ?
exp(1)?5 ?
exp(|x|)?6?p?LMLM (x) (5)where p(?)
are the phrase translation probabilitiesin both directions, plex(?)
are the lexical transla-tion probabilities in both directions, and exp(1)and exp(|x|) are the phrase penalty and wordpenalty, respectively.
These features are very com-mon in state-of-the-art systems (Koehn et al,2005; Chiang, 2005) and ?s are weights of fea-tures.For the reordering model ?, we define it on thetwo consecutive blocks A1 and A2 and their ordero ?
{straight, inverted}?
= f(o,A1, A2) (6)Under this framework, different reordering mod-els can be designed.
In fact, we defined four re-ordering models in our experiments.
The first one522is NONE, meaning no explicit reordering featuresat all.
We set ?
to 1 for all different pairs ofblocks and their orders.
So the phrasal reorder-ing is totally dependent on the language model.This model is obviously different from the mono-tone search, which does not use the inverted rule atall.
The second one is a distortion style reorderingmodel, which is formulated as?
={exp(0), o = straightexp(|A1|) + (|A2|), o = invertedwhere |Ai| denotes the number of words on thesource side of blocks.
When ??
< 0, this de-sign will penalize those non-monotone transla-tions.
The third one is a flat reordering model,which assigns probabilities for the straight and in-verted order.
It is formulated as?
={pm, o = straight1?
pm, o = invertedIn our experiments on Chinese-English tasks, theprobability for the straight order is set at pm =0.95.
This is because word order in Chinese andEnglish is usually similar.
The last one is the maxi-mum entropy based reordering model proposed byus, which will be described in the next section.We define a derivation D as a sequence of appli-cations of rules (1) ?
(3), and let c(D) and e(D)be the Chinese and English yields of D. The prob-ability of a derivation D isPr(D) =?iPr(i) (7)where Pr(i) is the probability of the ith applica-tion of rules.
Given an input sentence c, the finaltranslation e?
is derived from the best derivationD?D?
= argmaxc(D)=cPr(D)e?
= e(D?)
(8)2.2 DecoderWe developed a CKY style decoder that employs abeam search algorithm, similar to the one by Chi-ang (2005).
The decoder finds the best derivationthat generates the input sentence and its transla-tion.
From the best derivation, the best English e?is produced.Given a source sentence c, firstly we initiate thechart with phrases from phrase translation tableby applying the lexical rule.
Then for each cellthat spans from i to j on the source side, all pos-sible derivations spanning from i to j are gener-ated.
Our algorithm guarantees that any sub-cellswithin (i, j) have been expanded before cell (i, j)is expanded.
Therefore the way to generate deriva-tions in cell (i, j) is to merge derivations fromany two neighbor sub-cells.
This combination isdone by applying the straight and inverted rules.Each application of these two rules will generatea new derivation covering cell (i, j).
The score ofthe new generated derivation is derived from thescores of its two sub-derivations, reordering modelscore and the increment of the language modelscore according to the Equation (4).
When thewhole input sentence is covered, the decoding isover.Pruning of the search space is very important forthe decoder.
We use three pruning ways.
The firstone is recombination.
When two derivations inthe same cell have the same w leftmost/rightmostwords on the English yields, where w depends onthe order of the language model, they will be re-combined by discarding the derivation with lowerscore.
The second one is the threshold pruningwhich discards derivations that have a score worsethan ?
times the best score in the same cell.
Thelast one is the histogram pruning which only keepsthe top n best derivations for each cell.
In all ourexperiments, we set n = 40, ?
= 0.5 to get atradeoff between speed and performance in the de-velopment set.Another feature of our decoder is the k-best listgeneration.
The k-best list is very important forthe minimum error rate training (Och, 2003a)which is used for tuning the weights ?
for ourmodel.
We use a very lazy algorithm for the k-bestlist generation, which runs two phases similarly tothe one by Huang et al (2005).
In the first phase,the decoder runs as usual except that it keeps someinformation of weaker derivations which are to bediscarded during recombination.
This will gener-ate not only the first-best of final derivation butalso a shared forest.
In the second phase, thelazy algorithm runs recursively on the shared for-est.
It finds the second-best of the final deriva-tion, which makes its children to find their second-best, and children?s children?s second-best, untilthe leaf node?s second-best.
Then it finds the third-best, forth-best, and so on.
In all our experiments,we set k = 200.523The decoder is implemented in C++.
Using thepruning settings described above, without the k-best list generation, it takes about 6 seconds totranslate a sentence of average length 28.3 wordson a 2GHz Linux system with 4G RAM memory.3 Maximum Entropy Based ReorderingModelIn this section, we discuss how to create a max-imum entropy based reordering model.
As de-scribed above, we defined the reordering model ?on the three factors: order o, block A1 and blockA2.
The central problem is, given two neighborblocks A1 and A2, how to predicate their ordero ?
{straight, inverted}.
This is a typical prob-lem of two-class classification.
To be consistentwith the whole model, the conditional probabil-ity p(o|A1, A2) is calculated.
A simple way tocompute this probability is to take counts from thetraining data and then to use the maximum likeli-hood estimate (MLE)p(o|A1, A2) = Count(o,A1, A2)Count(A1, A2) (9)The similar way is used by lexicalized reorderingmodel.
However, in our model this way can?t workbecause blocks become larger and larger due to us-ing the merging rules, and finally unseen in thetraining data.
This means we can not use blocksas direct reordering evidences.A good way to this problem is to use features ofblocks as reordering evidences.
Good features cannot only capture reorderings, avoid sparseness, butalso integrate generalizations.
It is very straightto use maximum entropy model to integrate fea-tures to predicate reorderings of blocks.
Under theMaxEnt model, we have?
= p?
(o|A1, A2) = exp(?i ?ihi(o,A1, A2))?o exp(?i ?ihi(o,A1, A2))(10)where the functions hi ?
{0, 1} are model featuresand the ?i are weights of the model features whichcan be trained by different algorithms (Malouf,2002).3.1 Reordering Example ExtractionAlgorithmThe input for the algorithm is a bilingual corpuswith high-precision word alignments.
We obtainthe word alignments using the way of Koehn et al(2005).
After running GIZA++ (Och and Ney,targetsourceb1b2b3b4c1c2Figure 1: The bold dots are corners.
The ar-rows from the corners are their links.
Corner c1 isshared by block b1 and b2, which in turn are linkedby the STRAIGHT links, bottomleft and toprightof c1.
Similarly, block b3 and b4 are linked by theINVERTED links, topleft and bottomright of c2.2000) in both directions, we apply the ?grow-diag-final?
refinement rule on the intersectionalignments for each sentence pair.Before we introduce this algorithm, we intro-duce some formal definitions.
The first one isblock which is a pair of source and target contigu-ous sequences of wordsb = (si2i1 , tj2j1)b must be consistent with the word alignment M?
(i, j) ?
M, i1 ?
i ?
i2 ?
j1 ?
j ?
j2This definition is similar to that of bilingual phraseexcept that there is no length limitation over block.A reordering example is a triple of (o, b1, b2)where b1 and b2 are two neighbor blocks and ois the order between them.
We define each vertexof block as corner.
Each corner has four links infour directions: topright, topleft, bottomright, bot-tomleft, and each link links a set of blocks whichhave the corner as their vertex.
The topright andbottomleft link blocks with the straight order, sowe call them STRAIGHT links.
Similarly, we callthe topleft and bottomright INVERTED links sincethey link blocks with the inverted order.
For con-venience, we use b ??
L to denote that block bis linked by the link L. Note that the STRAIGHTlinks can not coexist with the INVERTED links.These definitions are illustrated in Figure 1.The reordering example extraction algorithm isshown in Figure 2.
The basic idea behind this al-gorithm is to register all neighbor blocks to theassociated links of corners which are shared bythem.
To do this, we keep an array to record link5241: Input: sentence pair (s, t) and their alignment M2: < := ?3: for each span (i1, i2) ?
s do4: find block b = (si2i1 , tj2j1) that is consistent with M5: Extend block b on the target boundary with one possi-ble non-aligned word to get blocks E(b)6: for each block b?
?
b?E(b) do7: Register b?
to the links of four corners of it8: end for9: end for10: for each corner C in the matrix M do11: if STRAIGHT links exist then12: < := <?
{(straight, b1, b2)},b1 ??
C.bottomleft, b2 ??
C.topright13: else if INVERTED links exist then14: < := <?
{(inverted, b1, b2)},b1 ??
C.topleft, b2 ??
C.bottomright15: end if16: end for17: Output: reordering examples <Figure 2: Reordering Example Extraction Algo-rithm.information of corners when extracting blocks.Line 4 and 5 are similar to the phrase extractionalgorithm by Och (2003b).
Different from Och,we just extend one word which is aligned to nullon the boundary of target side.
If we put somelength limitation over the extracted blocks and out-put them, we get bilingual phrases used in standardphrase-based SMT systems and also in our sys-tem.
Line 7 updates all links associated with thecurrent block.
You can attach the current blockto each of these links.
However this will increasereordering examples greatly, especially those withthe straight order.
In our Experiments, we just at-tach the smallest blocks to the STRAIGHT links,and the largest blocks to the INVERTED links.This will keep the number of reordering examplesacceptable but without performance degradation.Line 12 and 14 extract reordering examples.3.2 FeaturesWith the extracted reordering examples, we canobtain features for our MaxEnt-based reorderingmodel.
We design two kinds of features, lexi-cal features and collocation features.
For a blockb = (s, t), we use s1 to denote the first word of thesource s, t1 to denote the first word of the target t.Lexical features are defined on the single words1 or t1.
Collocation features are defined on thecombination s1 or t1 between two blocks b1 andb2.
Three kinds of combinations are used.
The firstone is source collocation, b1.s1&b2.s1.
The sec-ond is target collocation, b1.t1&b2.t1.
The last onehi(o, b1, b2) ={ 1, b1.t1 = E1, o = O0, otherwisehj(o, b1, b2) ={ 1, b1.t1 = E1, b2.t1 = E2, o = O0, otherwiseFigure 3: MaxEnt-based reordering feature tem-plates.
The first one is a lexical feature, and thesecond one is a target collocation feature, whereEi are English words, O ?
{straight, inverted}.is block collocation, b1.s1&b1.t1 and b2.s1&b2.t1.The templates for the lexical feature and the collo-cation feature are shown in Figure 3.Why do we use the first words as features?These words are nicely at the boundary of blocks.One of assumptions of phrase-based SMT is thatphrase cohere across two languages (Fox, 2002),which means phrases in one language tend to bemoved together during translation.
This indicatesthat boundary words of blocks may keep informa-tion for their movements/reorderings.
To test thishypothesis, we calculate the information gain ra-tio (IGR) for boundary words as well as the wholeblocks against the order on the reordering exam-ples extracted by the algorithm described above.The IGR is the measure used in the decision treelearning to select features (Quinlan, 1993).
Itrepresents how precisely the feature predicate theclass.
For feature f and class c, the IGR(f, c)IGR(f, c) = En(c)?
En(c|f)En(f) (11)where En(?)
is the entropy and En(?|?
)is the conditional entropy.
To our sur-prise, the IGR for the four boundary words(IGR(?b1.s1, b2.s1, b1.t1, b2.t1?, order) =0.2637) is very close to that for the two blockstogether (IGR(?b1, b2?, order) = 0.2655).Although our reordering examples do not coverall reordering events in the training data, thisresult shows that boundary words do providesome clues for predicating reorderings.4 ExperimentsWe carried out experiments to compare againstvarious reordering models and systems to demon-strate the competitiveness of MaxEnt-based re-ordering:1.
Monotone search: the inverted rule is notused.5252.
Reordering variants: the NONE, distortionand flat reordering models described in Sec-tion 2.1.3.
Pharaoh: A state-of-the-art distortion-baseddecoder (Koehn, 2004).4.1 CorpusOur experiments were made on two Chinese-to-English translation tasks: NIST MT-05 (news do-main) and IWSLT-04 (travel dialogue domain).NIST MT-05.
In this task, the bilingual train-ing data comes from the FBIS corpus with 7.06MChinese words and 9.15M English words.
The tri-gram language model training data consists of En-glish texts mostly derived from the English sideof the UN corpus (catalog number LDC2004E12),which totally contains 81M English words.
For theefficiency of minimum error rate training, we builtour development set using sentences of length atmost 50 characters from the NIST MT-02 evalua-tion test data.IWSLT-04.
For this task, our experiments werecarried out on the small data track.
Both thebilingual training data and the trigram languagemodel training data are restricted to the suppliedcorpus, which contains 20k sentences, 179k Chi-nese words and 157k English words.
We used theCSTAR 2003 test set consisting of 506 sentencepairs as development set.4.2 TrainingWe obtained high-precision word alignments us-ing the way described in Section 3.1.
Then weran our reordering example extraction algorithm tooutput blocks of length at most 7 words on the Chi-nese side together with their internal alignments.We also limited the length ratio between the targetand source language (max(|s|, |t|)/min(|s|, |t|))to 3.
After extracting phrases, we calculated thephrase translation probabilities and lexical transla-tion probabilities in both directions for each bilin-gual phrase.For the minimum-error-rate training, we re-implemented Venugopal?s trainer 3 (Venugopalet al, 2005) in C++.
For all experiments, we ranthis trainer with the decoder iteratively to tune theweights ?s to maximize the BLEU score on thedevelopment set.3See http://www.cs.cmu.edu/ ashishv/mer.html.
This is aMatlab implementation.PharaohWe shared the same phrase translation tablesbetween Pharaoh and our system since the twosystems use the same features of phrases.
In fact,we extracted more phrases than Pharaoh?s trainerwith its default settings.
And we also used our re-implemented trainer to tune lambdas of Pharaohto maximize its BLEU score.
During decoding,we pruned the phrase table with b = 100 (default20), pruned the chart with n = 100, ?
= 10?5(default setting), and limited distortions to 4(default 0).MaxEnt-based Reordering ModelWe firstly ran our reordering example extractionalgorithm on the bilingual training data withoutany length limitations to obtain reordering ex-amples and then extracted features from theseexamples.
In the task of NIST MT-05, weobtained about 2.7M reordering examples withthe straight order, and 367K with the invertedorder, from which 112K lexical features and1.7M collocation features after deleting thosewith one occurrence were extracted.
In the taskof IWSLT-04, we obtained 79.5k reorderingexamples with the straight order, 9.3k with theinverted order, from which 16.9K lexical featuresand 89.6K collocation features after deleting thosewith one occurrence were extracted.
Finally, weran the MaxEnt toolkit by Zhang 4 to tune thefeature weights.
We set iteration number to 100and Gaussian prior to 1 for avoiding overfitting.4.3 ResultsWe dropped unknown words (Koehn et al, 2005)of translations for both tasks before evaluatingtheir BLEU scores.
To be consistent with theofficial evaluation criterions of both tasks, case-sensitive BLEU-4 scores were computed For theNIST MT-05 task and case-insensitive BLEU-4scores were computed for the IWSLT-04 task 5.Experimental results on both tasks are shown inTable 1.
Italic numbers refer to results for whichthe difference to the best result (indicated in bold)is not statistically significant.
For all scores, wealso show the 95% confidence intervals computedusing Zhang?s significant tester (Zhang et al,2004) which was modified to conform to NIST?s4See http://homepages.inf.ed.ac.uk/s0450736/maxent toolkit.html.5Note that the evaluation criterion of IWSLT-04 is not to-tally matched since we didn?t remove punctuation marks.526definition of the BLEU brevity penalty.We observe that if phrasal reordering is totallydependent on the language model (NONE) weget the worst performance, even worse than themonotone search.
This indicates that our languagemodels were not strong to discriminate betweenstraight orders and inverted orders.
The flat anddistortion reordering models (Row 3 and 4) showsimilar performance with Pharaoh.
Although theyare not dependent on phrases, they really reorderphrases with penalties to wrong orders supportedby the language model and therefore outperformthe monotone search.
In row 6, only lexical fea-tures are used for the MaxEnt-based reorderingmodel; while row 7 uses lexical features and col-location features.
On both tasks, we observe thatvarious reordering approaches show similar andstable performance ranks in different domains andthe MaxEnt-based reordering models achieve thebest performance among them.
Using all featuresfor the MaxEnt model (lex + col) is marginallybetter than using only lex features (lex).4.4 Scaling to Large BitextsIn the experiments described above, collocationfeatures do not make great contributions to the per-formance improvement but make the total num-ber of features increase greatly.
This is a prob-lem for MaxEnt parameter estimation if it is scaledto large bitexts.
Therefore, for the integration ofMaxEnt-based phrase reordering model in the sys-tem trained on large bitexts, we remove colloca-tion features and only use lexical features fromthe last words of blocks (similar to those from thefirst words of blocks with similar performance).This time the bilingual training data contain 2.4Msentence pairs (68.1M Chinese words and 73.8MEnglish words) and two trigram language modelsare used.
One is trained on the English side ofthe bilingual training data.
The other is trained onthe Xinhua portion of the Gigaword corpus with181.1M words.
We also use some rules to trans-late numbers, time expressions and Chinese per-son names.
The new Bleu score on NIST MT-05is 0.291 which is very promising.5 Discussion and Future WorkIn this paper we presented a MaxEnt-based phrasereordering model for SMT.
We used lexical fea-tures and collocation features from boundarywords of blocks to predicate reorderings of neigh-Systems NIST MT-05 IWSLT-04monotone 20.1 ?
0.8 37.8 ?
3.2NONE 19.6 ?
0.8 36.3 ?
2.9Distortion 20.9 ?
0.8 38.8 ?
3.0Flat 20.5 ?
0.8 38.7 ?
2.8Pharaoh 20.8 ?
0.8 38.9 ?
3.3MaxEnt (lex) 22.0 ?
0.8 42.4 ?
3.3MaxEnt (lex + col) 22.2 ?
0.8 42.8 ?
3.3Table 1: BLEU-4 scores (%) with the 95% confi-dence intervals.
Italic numbers refer to results forwhich the difference to the best result (indicated inbold) is not statistically significant.bor blocks.
Experiments on standard Chinese-English translation tasks from two different do-mains showed that our method achieves a signif-icant improvement over the distortion/flat reorder-ing models.Traditional distortion/flat-based SMT transla-tion systems are good for learning phrase transla-tion pairs, but learn nothing for phrasal reorder-ings from real-world data.
This is our originalmotivation for designing a new reordering model,which can learn reorderings from training data justlike learning phrasal translations.
Lexicalized re-ordering model learns reorderings from trainingdata, but it binds reorderings to individual concretephrases, which restricts the model to reorderingsof phrases seen in training data.
On the contrary,the MaxEnt-based reordering model is not limitedby this constraint since it is based on features ofphrase, not phrase itself.
It can be easily general-ized to reorder unseen phrases provided that somefeatures are fired on these phrases.Another advantage of the MaxEnt-based re-ordering model is that it can take more fea-tures into reordering, even though they are non-independent.
Tillmann et.
al (2005) also use aMaxEnt model to integrate various features.
Thedifference is that they use the MaxEnt model topredict not only orders but also blocks.
To do that,it is necessary for the MaxEnt model to incorpo-rate real-valued features such as the block trans-lation probability and the language model proba-bility.
Due to the expensive computation, a localmodel is built.
However, our MaxEnt model is justa module of the whole log-linear model of transla-tion which uses its score as a real-valued feature.The modularity afforded by this design does notincur any computation problems, and make it eas-527ier to update one sub-model with other modulesunchanged.Beyond the MaxEnt-based reordering model,another feature deserving attention in our systemis the CKY style decoder which observes the ITG.This is different from the work of Zens et.
al.(2004).
In their approach, translation is generatedlinearly, word by word and phrase by phrase in atraditional way with respect to the incorporationof the language model.
It can be said that their de-coder did not violate the ITG constraints but notthat it observed the ITG.
The ITG not only de-creases reorderings greatly but also makes reorder-ing hierarchical.
Hierarchical reordering is moremeaningful for languages which are organized hi-erarchically.
From this point, our decoder is simi-lar to the work by Chiang (2005).The future work is to investigate other valuablefeatures, e.g.
binary features that explain blocksfrom the syntactical view.
We think that there isstill room for improvement if more contributingfeatures are used.AcknowledgementsThis work was supported in part by National HighTechnology Research and Development Programunder grant #2005AA114140 and National Nat-ural Science Foundation of China under grant#60573188.
Special thanks to Yajuan Lu?
fordiscussions of the manuscript of this paper andthree anonymous reviewers who provided valuablecomments.ReferencesAshish Venugopal, Stephan Vogel.
2005.
Considerations inMaximum Mutual Information and Minimum Classifica-tion Error training for Statistical Machine Translation.
Inthe Proceedings of EAMT-05, Budapest, Hungary May 30-31.Christoph Tillmann.
2004.
A block orientation model forstatistical machine translation.
In HLT-NAACL, Boston,MA, USA.Christoph Tillmann and Tong Zhang.
2005.
A LocalizedPrediction Model for statistical machine translation.
InProceedings of ACL 2005, pages 557?564.David Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings of ACL2005, pages 263?270.Dekai Wu.
1996.
A Polynomial-Time Algorithm for Statis-tical Machine Translation.
In Proceedings of ACL 1996.Dekai Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
Computa-tional Linguistics, 23:377?404.Franz Josef Och and Hermann Ney.
2000.
Improved statisti-cal alignment models.
In Proceedings of ACL 2000, pages440?447.Franz Josef Och.
2003a.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL 2003,pages 160?167.Franz Josef Och.
2003b.
Statistical Machine Translation:From Single-Word Models to Alignment Templates The-sis.Franz Josef Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.
Com-putational Linguistics, 30:417?449.Franz Josef Och, Ignacio Thayer, Daniel Marcu, KevinKnight, Dragos Stefan Munteanu, Quamrul Tipu, MichelGalley, and Mark Hopkins.
2004.
Arabic and Chinese MTat USC/ISI.
Presentation given at NIST Machine Transla-tion Evaluation Workshop.Heidi J.
Fox.
2002.
Phrasal cohesion and statistical machinetranslation.
In Proceedings of EMNLP 2002.J.
R. Quinlan.
1993.
C4.5: progarms for machine learning.Morgan Kaufmann Publishers.Kevin Knight.
1999.
Decoding complexity in wordreplace-ment translation models.
Computational Linguistics,Squibs & Discussion, 25(4).Liang Huang and David Chiang.
2005.
Better k-best parsing.In Proceedings of the Ninth International Workshop onParsing Technology, Vancouver, October, pages 53?64.Philipp Koehn, Franz Joseph Och, and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
In Proceedings ofHLT/NAACL.Philipp Koehn.
2004.
Pharaoh: a beam search decoder forphrase-based statistical machine translation models.
InProceedings of the Sixth Conference of the Association forMachine Translation in the Americas, pages 115?124.Philipp Koehn, Amittai Axelrod, Alexandra Birch Mayne,Chris Callison-Burch, Miles Osborne and David Talbot.2005.
Edinburgh System Description for the 2005 IWSLTSpeech Translation Evaluation.
In International Work-shop on Spoken Language Translation.R.
Zens, H. Ney, T. Watanabe, and E. Sumita.
2004.
Re-ordering Constraints for Phrase-Based Statistical MachineTranslation.
In Proceedings of CoLing 2004, Geneva,Switzerland, pp.
205-211.Robert Malouf.
2002.
A comparison of algorithms for maxi-mum entropy parameter estimation.
In Proceedings of theSixth Conference on Natural Language Learning (CoNLL-2002).Shankar Kumar and William Byrne.
2005.
Local phrasereordering models for statistical machine translation.
InProceedings of HLT-EMNLP.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.
Inter-preting BLEU/NIST scores: How much improvement dowe need to have a better system?
In Proceedings of LREC2004, pages 2051?
2054.528
