Proceedings of the 14th European Workshop on Natural Language Generation, pages 136?146,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsAbstractive Meeting Summarization with Entailment and FusionYashar Mehdad?Giuseppe Carenini?Frank W.
Tompa?
?Raymond T. NG?Department of Computer Science?University of British Columbia ?
?University of Waterloo{mehdad, carenini, rng}@cs.ubc.ca fwtompa@cs.uwaterloo.caAbstractWe propose a novel end-to-end frame-work for abstractive meeting summariza-tion.
We cluster sentences in the in-put into communities and build an entail-ment graph over the sentence communi-ties to identify and select the most relevantsentences.
We then aggregate those se-lected sentences by means of a word graphmodel.
We exploit a ranking strategy toselect the best path in the word graph asan abstract sentence.
Despite not relyingon the syntactic structure, our approachsignificantly outperforms previous modelsfor meeting summarization in terms of in-formativeness.
Moreover, the longer sen-tences generated by our method are com-petitive with shorter sentences generatedby the previous word graph model in termsof grammaticality.1 IntroductionThe huge amount of data generated every day inmeetings calls for developing automated methodsto efficiently process these data to meet users?needs.
Automatic summarization is a popular taskthat can help users to browse a large amount ofrecorder speech in text format.
This paper tacklesthe task of recorded meeting summarization, ad-dressing the key limitations of existing approachesby proposing the following contributions:1) Various approaches that were recently devel-oped for meeting summarization (such as (Gillicket al 2009; Garg et al 2009)) focus on extract-ing important sentences (or dialogue acts) fromspeech transcripts, either manual transcripts or au-tomatic speech recognition (ASR) output.
How-ever, it has been observed in the context of meet-ing summarization users generally prefer conciseabstracts over extracts, and abstracts lead to higherobjective task scores; likewise automatic abstrac-tive summaries are preferred in comparison withhuman extracts (Murray et al 2010).
Moreover,most of the abstractive summarization approachesfocus on one component of the system, such asgeneration (e.g., (Genest and Lapalme, 2010)) orcontent selection (e.g., (Murray et al 2012)), in-stead of developing the full framework for abstrac-tive summarization.
To address these limitations,as the main contribution of this paper, we pro-pose a full pipeline to generate an abstractive sum-mary for each meeting transcript.
Our system issimilar to that of Murray et al(2010) in termsof generating abstractive summaries for meetingtranscripts.
However, we take a lighter supervi-sion for the content selection phase and a differentapproach towards the language generation phase,which does not rely on the conventional NaturalLanguage Generation (NLG) architecture (Reiterand Dale, 2000).2) We propose a word graph based approachto aggregate and generate the abstractive sentencesummary.
Our work extends the word graphmethod proposed by Filippova (2010) with the fol-lowing novel contributions: i) We take advantageof lexical knowledge to merge similar nodes byfinding their relations in WordNet; ii) We gener-ate new sentences through generalization and ag-gregation of the original ones, which means thatour generated sentences are not necessarily com-posed of the original words; and iii) We adopt anew ranking strategy to select the best path in thegraph by taking the information content and thegrammaticality (i.e.
fluency) of the sentence intoconsideration.3) In order to generate an abstract summary fora meeting, we have to be able to capture the over-all content of the conversation.
This can be doneby identifying the essential content from the mostinformative sentences using the semantic relationsamong all sentences in a meeting transcript.
How-136ever, most current methods disregard such rela-tions in favor of statistical models of word distri-butions and frequencies.
Moreover, the data frommeeting transcripts often contains many highly re-dundant sentences.
As one of the key contribu-tions of this paper, we propose to build a multi-directional entailment graph over the sentences toidentify and select relevant information from themost informative sentences.4) The textual data from meeting conversa-tion transcripts are typically in a casual style anddo not exhibit a clear syntactic structure withproper grammar and spelling.
Therefore, abstrac-tive summarization approaches developed for for-mal texts, such as scientific or news articles, of-ten are not satisfactory when dealing with infor-mal texts.
Our proposed method for abstractivemeeting summarization requires minimal syntac-tic and structural information and is robust in deal-ing with text that suffers from transcription errors,ill-formed sentences and unknown lexical choicessuch as typically formed in meeting transcripts.We evaluate our system over meeting tran-scripts.
Our result compares favorably to the resultof previous extractive and abstractive approachesin terms of information content.
Moreover, weshow that our method can generate longer sen-tences with competitive grammaticality scores, incomparison with previous abstractive approaches.Furthermore, we evaluate the impact of each com-ponent of our system through an ablation test.As an additional result of our experiments, wealso show that using semantic relations (entail-ment graph) is important in efficiently performingthe final step of our summarization pipeline (i.e.,the sentence fusion).2 Abstractive SummarizationFrameworkSimilar to Murray et al(2010), our goal is togenerate a meeting summary, i.e.
a set of sen-tences, that could capture the semantics of themeeting.
While (Murray et al 2010) requiresextensive annotations to train several classifiersto detect important sentences, opinions and dia-log acts, we only use a subset of that annotation,which includes a human abstract and links fromeach sentence in the abstract to the source meet-ing sentences.
In addition, instead of generat-ing an abstractive sentence via the conventionalNLG pipeline (Reiter and Dale, 2000), we exploita word graph approach.LinkingDetectionEntailmentIdentifyCommunityDetection Entailment GraphWord GraphRankingSentence Fusion?
?
?- -1Figure 1: Meeting summarization framework.As shown in Figure 1, our framework consistsof three main components, which we describe inmore detail in the following sections.2.1 Community DetectionWhile some abstractive summary sentences arevery similar to original sentences from the meetingtranscript, others can be created by aggregatingand merging multiple sentences into an abstractsentence.
In order to generate such a sentence, weneed to identify which sentences from the originalmeeting transcript should be combined in gener-ated abstract sentences.
This task can be consid-ered as the first step of abstractive meeting sum-marization and is called ?abstractive communitydetection (ACD)?
(Murray et al 2012).
To per-form ACD, we follow the same method proposedby Murray et al(2012), in two steps:First, we classify sentence pairs according towhether or not they should be realized by a com-mon abstractive sentence.
For each pair, we ex-tract its structural and linguistic features, and wetrain a logistic regression classifier over all ourtraining data (described in Section 3.1) exploitingsuch features.
We run the trained classier over sen-tence pairs, predicting abstractive links betweensentences in the document.
The result can be rep-resented as an undirected graph where nodes arethe sentences, and edges represent whether twonodes are linked.Second, we have to identify which nodes (i.e.,sentences from the meeting transcript) can be clus-tered as a community to generate an abstract sen-tence.
For this purpose, we apply the CONGA al-gorithm (Gregory, 2007) for community detectionthat uses betweenness to detect communities in agraph.
The betweenness score for an edge is thenumber of shortest paths between pairs of nodesin the graph that run along that edge.If a sentence is not connected to at least oneother sentence in the first step, it?s assigned to itsown singleton community.137CDEFGA Bxx1Figure 2: Building an entailment graph over sentences.
Ar-rows and ?x?
represent the entailment direction and unknowncases respectively.2.2 Entailment GraphSentences in a community often include redundantinformation which are semantically equivalent butvary in lexical choices.
By identifying the seman-tic relations between the sentences in each com-munity, we can discover the information in onesentence that is semantically equivalent, novel, ormore/less informative with respect to the contentof the other sentences.Similar to earlier work (Lloret et al 2008;Mehdad et al 2010; Berant et al 2011; Adler etal., 2012; Mehdad et al 2013), we set this prob-lem as a variant of the Textual Entailment (TE)recognition task (Dagan and Glickman, 2004).
Webuild an entailment graph for each community ofsentences, where nodes are the linked sentencesand edges are the entailment relations betweennodes.
Given two sentences (s1 and s2), we aimat identifying the following cases:i) s1 and s2 express the same meaning (bidirec-tional entailment).
In such cases one of the sen-tences should be eliminated;ii) s1 is more informative than s2 (unidirectionalentailment).
In such cases, the entailing sentenceshould replace or complement the entailed one;iii) s1 contains facts that are not present in s2, andvice-versa (the ?unknown?
cases in TE parlance).In such cases, both sentences should remain.Figure 2 shows how entailment relations canhelp in selecting the sentences by removing the re-dundant and less informative ones.
As we show inthe figure, the sentence ?A?
entails ?E?, ?F?
and?G?, but not ?B?.
So we can keep ?A?
and ?B?and eliminate others.
For example, the sentence?we should discuss about the remote control andits color?
entails ?about the remote?, ?let?s talkabout the remote?
and ?um remote?s color?, butnot ?remote?s size is also important?.
So we cankeep ?we should discuss about the remote con-trol and its color?
and ?remote?s size is also im-portant?
and eliminate the others.
In this way,TE-based sentence identification can be designedto distinguish meaning-preserving variations fromtrue divergence, regardless of lexical choices andstructures.Similar to previous approaches in TE (e.g., (Be-rant et al 2011)), we use a supervised method.
Totrain and build the entailment graph, we performthree steps described in the following subsections.2.2.1 Training set collectionIn the last few years, TE corpora have been cre-ated and distributed in the framework of severalevaluation campaigns, including the RecognizingTextual Entailment (RTE) Challenge1 and Cross-lingual textual entailment for content synchroniza-tion2 (Negri et al 2012).
However, such datasetscannot directly support our application, since theRTE datasets are often composed of longer well-formed sentences and paragraphs (Bentivogli etal., 2009; Negri et al 2011).In order to collect a dataset that is more sim-ilar to the goal of our entailment framework, weselect a subset of the sixth and seventh RTE chal-lenge main task (i.e., RTE within a Corpus).
Ourdataset choice is based on the following reasons:i) the length of sentence pairs in RTE6 and RTE7is shorter than the others, and ii) RTE6 and RTE7main task datasets are originally created for sum-marization purpose, which is closer to our work.We sort the RTE6 and RTE7 dataset pairs basedon the sentence length and choose the first 2000samples with an equal number of positive and neg-ative examples.
The average length of words inour training data is 7 words.
There are certainlysome differences between our training set and oursentences from the meeting corpus.
However, thecollected training samples was the closest avail-able dataset to our needs.2.2.2 Feature representation and trainingWorking with meeting transcripts imposes someconstraints on feature selection.
Meeting tran-scripts are not often well-formed in terms of sen-1http://pascallin.ecs.soton.ac.uk/Challenges/RTE/2http://www.cs.york.ac.uk/semeval-2013/task8/138tence structure and contain errors.
This limitsour features to the lexical level.
Fortunately, lexi-cal models are less computationally expensive andeasier to implement and often deliver a strong per-formance for RTE (Sammons et al 2011).Our entailment decision criterion is based onsimilarity scores calculated with a sentence-to-sentence matching process.
Each example pair ofsentences (s1 and s2) is represented by a featurevector, where each feature is a specific similarityscore estimating whether s1 entails s2.We compute 18 similarity scores for each pairof sentences.
Before aggregating the similarityscores to form an entailment score, we normalizethe similarity scores by the length of s2 (in termsof lexical items), when checking the entailment di-rection from s1 to s2.
In this way, we can estimatethe portion of information/facts in s2 which is cov-ered by s1.The first five scores are computed based on theexact lexical overlap between the phrases: wordoverlap, edit distance, ngram-overlap, longestcommon subsequence and Lesk (Lesk, 1986).The other scores were computed using lexicalresources: WordNet (Fellbaum, 1998), VerbO-cean (Chklovski and Pantel, 2004), paraphrases(Denkowski and Lavie, 2010) and phrase match-ing (Mehdad et al 2011).
We used WordNetto compute the word similarity as the least com-mon subsumer between two words considering thesynonymy-antonymy, hypernymy-hyponymy, andmeronymy relations.
Then, we calculated the sen-tence similarity as the sum of the similarity scoresof the word pairs in Text and Hypothesis, nor-malized by the number of words in Hypothesis.We also use phrase matching features described in(Mehdad et al 2011) which consists of phrasalmatching at the level on ngrams (1 to 5 tokens).The rationale behind using different entailmentfeatures is that combining various scores will yielda better model (Berant et al 2011).To combine the entailment scores and optimizetheir relative weights, we train a Support VectorMachine binary classifier, SVMlight (Joachims,1999), over an equal number of positive and nega-tive examples.
This results in an entailment modelwith 95% accuracy over 2-fold and 5-fold cross-validation, which further proves the effectivenessof our feature set for this lexical entailment model.The reason that we gained a very high accuracyis because our selected sentences are a subsetof RTE6 and RTE7 with a shorter length (fewerwords) which makes the entailment recognitiontask much easier than recognizing entailment be-tween paragraphs or long sentences.2.2.3 Entailment graph edge labelingSince our training examples are labeled with bi-nary judgments, we are not able to train a three-way classifier.
Therefore, we set the edge label-ing problem as a two-way classification task thatcasts multidirectional entailment as a unidirec-tional problem, where each pair is analyzed check-ing for entailment in both directions (Mehdad etal., 2012).
In this condition, each original testexample is correctly classified if both pairs origi-nated from it are correctly judged (?YES-YES?
forbidirectional,?YES-NO?
and ?NO-YES?
for unidi-rectional entailment and ?NO-NO?
for unknowncases).
Two-way classification represents an intu-itive solution to capture multidimensional entail-ment relations.2.2.4 Identification and selectionBy assigning all entailment relations between theextracted sentence pairs, we identify relevant sen-tences to eliminate the redundant (in terms ofmeaning) and less informative ones.
In order toperform this task we follow a set of rules basedon the graph edge labels.
Note that since entail-ment is a transitive relation, our entailment graphis transitive i.e., if entail(s1,s2) and entail(s2,s3)then entail(s1,s3) (Berant et al 2011).Rule 1) Among the nodes that are connected withbidirectional entailment (semantically equivalentnodes) we keep only the one with more outgo-ing bidirectional and unidirectional entailment re-lations;Rule 2) If there is a chain of entailing nodes, wekeep the one that is the root of the chain and elim-inate others.2.3 Multi-sentence FusionSentence fusion is a well-known challenge forsummarization systems.
In this phase, our goalis to generate an understandable informative sen-tence that maximally captures the content of thesentences in a sentence community.There are several ways of generating an abstractsentence (e.g.
(Barzilay and McKeown, 2005; Liuand Liu, 2009; Ganesan et al 2010; Murray etal., 2010)); however, most of them rely heavilyon the syntactic structure.
We believe that such139weumshould/mustchoose/deter-mineThe remote control isbeca-usethecost/priceimportant/crucialuse of uhStart End1Figure 3: Word graph constructed from sentences (1-4) and possible fusion paths.
Double line nodes represent merged wordsin the graph.approaches are suboptimal, especially in dealingwith written conversational data (e.g., email) orthe data from speech transcripts, whether manualtranscription or automatic speech recognition out-put.
Instead, we apply an approach that does notrely on syntax, nor on a standard NLG architec-ture.
More specifically, we build a word graphfrom all the words of the sentences in a commu-nity and aggregate them in order to generate a newabstractive sentence.We perform the task of multi-sentence fusion intwo steps.
First, we construct a word graph oversentences in each community that were selectedfrom the entailment graph.
Second, we rank thevalid paths in the word graph and select the toppath as the abstract sentence summary.2.3.1 Constructing a Word GraphIn order to construct a word graph, our model ex-tends the word graph method proposed by Filip-pova (2010) with the following novel contribu-tions:1- The basic word graph method disregards se-mantic and lexical relations between the wordsin constructing the word graph, in favor of re-dundancy and word frequencies.
To move be-yond such limitation, we take advantage of lexi-cal knowledge to map the similar nodes by findingtheir relations in WordNet.
In this way, for exam-ple, two synonym words can be mapped into thesame node.2- Filippova?s approach is essentially extractivein nature, which means the generated sentence iscomposed by the same words from the originalsentences.
We move beyond this by generatingnew sentences through generalization and aggre-gation of the original ones.
This means that ourgenerated sentences are not necessarily composedof the original words.
In this way, we are one stepcloser to abstractive summarization.3- Our proposed method aggregates and gen-erates new readable sentences, regardless of theirlengths, that can semantically imply several orig-inal sentences, by taking the information contentand the readability (i.e.
fluency) of the sentenceinto consideration.Following Filippova?s method, given a set of re-lated sentences, we build a word graph by itera-tively adding sentences to it.
Figure 1 illustratesa small graph composed of 4 sentences, includingthe start and end nodes.1- we must determine the use of uh remote.2- The remote control is important because thecost.3- um we should choose the control.4- The remote price is crucial.As one of the main steps of word graph con-struction, we merge the words that have the samePOS tags under the following conditions:1) The words are identical (e.g.
?remote?
).2) The words are synonyms.
The replacementchoice is based on the word?s commonality, i.e.tfidf (e.g.
?important?
and ?crucial?
).3) The words form a hypernym/hyponym pair orshare a common hypernym.
Both words are re-placed by the hypernym (e.g.
?price?
and ?cost?
).4) The words are in an entailment relation.
Bothwords are replaced by the entailed one (e.g.
?pay?and ?buy?
).Note that, similar to Filippova?s approach,where merging is ambiguous we check the context(a word before and after in the sentence and theneighboring nodes in the graph) and select the can-didate that has larger overlap in the context, or theone with a greater frequency.
Similar to the origi-nal word graph model, we connect adjacent wordswith directed edges.
For the new nodes or uncon-nected nodes, we draw an edge with a weight of1.
In contrast, weights between already connectednodes are increased by 1.1402.3.2 Path Selection and RankingThe word graph, as described above, will generatemany sequences connecting start and end.
How-ever, it is likely that most of the paths are not read-able.
Since we are aiming at generating a goodabstractive sentence, some constraints need to beconsidered.A good abstractive sentence should cover mostof the concepts that exist in the original sentences.Moreover, it should be grammatically correct.In order to satisfy these constraints we adopt aranking strategy that combines the characteristicsof a good summary sentence.
To filter ungram-matical sentences, we prune the paths in which averb does not exist.
Our ranking formulation issummarized as below:Fluency: Our word graph process generatesmany possible paths as abstractive summaries.We need now to decide which of these pathsare more readable and fluent.
As in other areasof NLP (e.g.
machine translation and speechrecognition), the answer can be estimated by alanguage model.
We assign a probability Pr(P )to each path P based on a n-gram language model.Pr(P ) = mYi=1Pr(pi|pi 11 ) ?
mYi=1Pr(pi|pi 1i n+1)?mXi=1 logPr(pi|pi 1i n+1)Coverage: To identify the summary with the high-est coverage, we propose a second score that esti-mates the number of nouns that appear in the path.In order to reward the ranking score to cover moresalient nouns, we also consider the tfidf score ofnouns in the coverage formulation.C overage(P ) = Ppi2P tfidf (pi)Ppi2G tfidf (pi)where the pi are nouns and G is the graph.Edge weight: As a third score, we adopt the Filip-pova?s edge weighting formulation w(pi, pj) anddefine the edge weight of the path W (P ) as be-low: w(pi, pj) = freq(pi) + freq(pj)PP2Gpi,pj2Pdi?
(P, pi, pj) 1W (P ) = Pm 1i=1 w(pi, pi+1)m  1where the function diff(P, pi, pj) refers to thedistance between the offset positions pos(P, pi)of words pi and pj in path P and is defined as|pos(P, pj)   pos(P, pi)| and m is the number ofwords in path P .Ranking score: In order to generate a summarysentence that combines the scores above, weemploy a ranking model.
The purpose of sucha model is three-fold: i) to generate a morereadable and grammatical sentence; ii) to coverthe content of original sentences optimally; andiii) to favor strong connections between theconcepts.
Therefore, the final ranking score ofpath P is calculated over the normalized scores as:Score(P ) = Pr(P )?
Coverage(P )W (P )We select all the paths that contain at least oneverb and rerank them using our proposed rankingfunction to find the best path as the summary ofthe original sentences.3 Experiments and ResultsWe now describe a preliminary, formative evalua-tion of our framework, whose main goal is to as-sess strengths and weaknesses of the various com-ponents and generate ideas for further develop-ments.3.1 DatasetTo verify the effectiveness of our approach, we ex-periment with the AMI meeting corpus (Carlettaet al 2005) that consists of 140 multi-party meet-ings with a wide range of annotations, includingabstactive summaries for each meeting and linksfrom each sentence in the summary to the set ofsentences in the original transcripts that sentenceis summarizing.
We use this information as ourgold standard since it provides many examples inwhich a set of sentences in the meeting (a commu-nity) is linked to a human written sentence sum-marizing that community.141In our experiments, we use human authoredtranscripts.
Note that our approach is not specificto conversations, however it is designed to dealwith ill-formed sentences and structural errors.Moreover, the first two components of our sys-tem are supervised, while the word graph modelis completely unsupervised and domain indepen-dent.In order to train our logistic regression classifierfor the first phase of our pipeline, we randomlyselect a training set that consists of 98 meetings.Note that there are about one million sentence pairinstances in the training set since we consider ev-ery pairing of sentences within a meeting.
The restis selected as a test set for the evaluation phase.3.2 Experimental SettingsFor preprocessing our dataset we use OpenNLP3for tokenization and part-of-speech tagging.
Whenthe number of sentences in each community ismore than 10 (which happens in around 10% ofthe cases), the community is first clustered usingthe MajorClust (Stein and Niggemann, 1999) al-gorithm when sentences are represented as nor-malized tfidf vectors and the similarity betweenthe sentences is measured using cosine similarity.Then, each cluster is treated as a separate com-munity.
The clustering guarantees a higher over-lap between the sentences as well as a word graphwith fewer paths.
Next, we construct a word graphover each cluster and rank the valid paths.
Wechoose the first ranked path as the abstractive sum-mary of the cluster.
For our language model, weuse a tri-gram smoothed language model trainedusing the newswire text provided in the EnglishGigaword corpus (Graff and Cieri, ).3.3 Evaluation MetricsTo evaluate performance, we use the ROUGE-1and ROUGE-2 (unigram and bigram overlap) F1score, which correlate well with human rankingsof summary quality (Lin and Hovy, 2003).
Wealso ignore stopwords to reduce the impact of highoverlap when matching them.Furthermore, to evaluate the grammaticality ofour generated summaries in comparison with theoriginal word graph method, following commonpractice (Barzilay and McKeown, 2005), we ran-domly selected 10 meeting summaries (total 150sentences).
Then, we asked annotators to give one3http://opennlp.apache.org/Models ROUGE-1 ROUGE-2MMR-centroid 18 3MMR-cosine 21 -ILP 24 -TextRank 25.0 4.4ClusterRank 27.5 5.1Orig.
word graph 26.9 3.8Our model (-ent) 32.3 4.8Our model (GC) 32.1 4.0Our model (full) 28.7 4.2Table 1: Performance of different summarization algorithmson human transcripts for meeting conversations.
5of three possible ratings for each sentence in asummary based on grammaticality: perfect (2 pts),only one mistake (1 pt) and not acceptable (0 pts),ignoring the capitalization or punctuation.
Eachmeeting was rated by two annotators (ComputerScience graduate students).3.4 BaselinesWe compare our approach with various extrac-tive baselines: 1) MMR-centroid system (Car-bonell and Goldstein, 1998); 2) MMR-cosine sys-tem (Gillick et al 2009); 3) ILP-based system(Gillick et al 2009); 4) TextRank system (Mihal-cea and Tarau, 2004); and 5) ClusterRank system(Garg et al 2009) and with one abstractive base-line: 6) Original word graph model (Orig.
wordgraph) (Filippova, 2010).In order to measure the effectiveness of dif-ferent components, we also evaluated our sys-tem using human-annotated sentence communities(GC) in comparison with our community detectionmodel (full).
Moreover, we measure the perfor-mance of our system (GC) ablating the entailmentmodule (-ent).3.5 ResultsTable 1 shows the results for our proposed ap-proach in comparison with these strong baselinesfor meeting summarization.
The results show thatour model outperforms the baselines significantly4for ROUGE-1 over human transcripts for meet-ing conversations, which proves the effectivenessof our approach in dealing with summarization of5The MMR-cosine and ILP systems did not report theROUGE-2 score.4The statistical significance tests was calculated by ap-proximate randomization described in (Yeh, 2000).142Models Read.
R=2 R=1 R=0 Avg Len.Orig.
word graph 1.41 55% 32% 13% 8Our model 1.34 47% 39% 14% 14Table 2: Average rating and distribution over rating scores for abstractive word graph models.meeting conversations.
However, the ClusterRankand TextRank systems outperform our model forROUGE-2 score.
This can be due to word mergingand word replacement choices in the word graphconstruction (see Section 2.3.1), which sometimeschanges a word in a bigram and consequently de-creases the bigram overlap score.
A more detailedanalysis of this problem is left as future work.Note that there is a drop in ROUGE score whenwe use entailment in our system in comparisonwith ablating the entailment phase (-ent).
This ismainly due to the fact that the entailment phasefilters equivalent sentences.
This affects the re-sults negatively when such filtered sentences sharemany common words with our human-authoredabstracts.
We believe that this drop is partly as-sociated with our evaluation metric rather thanmeaning.
In other words, we expect no differencein performance when a human evaluation is ap-plied.
However, the entailment phase helps in im-proving the efficiency of our pipeline significantly.If each graph has e edges, n nodes, and p paths,then finding all the paths results in time complex-ity O((np + 1)(e + n)), using depth-first search.Decreasing the number of sentences will reducethe number of nodes and edges, which leads tothe smaller number of paths.
This is even moresignificant when there are many sentences in acommunity in comparison with the gold standard.Note that it?s impossible to finish the graph build-ing phase after 12 hours on a 2.3 GHz quad-coremachine without performing the entailment phase,when we use our community detection model.This would be especially problematic in a real-time setting.Comparing the gold standard sentence commu-nities (GC) and our fully automatic system, we cannotice that inaccuracies in the community detec-tion phase affects the overall performance.
How-ever, using our community detection model, westill outperform the previous models significantly.Table 2 shows grammaticality scores, distribu-tions over the three scores and average sentencelengths in tokens.
The results demonstrate that47% of the sentences generated by our method aregrammatically correct and 39% of the generatedsentences are almost correct.
In comparison withthe original word graph method, our model reportsslightly lower results for the grammaticality scoreand the percentage of correct sentences.
How-ever, considering the correlation between sentencelength and grammatical complexity, our model iscapable of generating longer sentences with moreinformation content (according to ROUGE) andcompetitive grammaticality scores.4 DiscussionAfter analyzing the results and through manualverification of some cases, we observe that our ap-proach produces some interestingly successful ex-amples.
Nevertheless, it appears that the perfor-mance is still far from satisfactory.
This leaves aninteresting challenge for the research communityto tackle.
We have identified five different sourcesof error:Type 1: Abstractive human-authored summaries:the nature of our method is based on extractingthe relevant sentences and generating an abstractsentence by aggregating such sentences.
Also dueto this, our generated abstracts are often infor-mal and closer to the transcripts?
style.
However,in many cases, the human-written summaries arecomposed by understanding the original sentencesand produce a formal style abstract sentence, of-ten using a different vocabulary and structure.
Forexample:Human-authored: The industrial designer and user in-terface designer presented the prototype they created,which was designed to look like a banana.System: Working on the principle of a fruit it?s basicallydesigned around a banana.Type 2: Evaluation method: The current evalu-ation methods fail to capture the meaning and re-lies only on matching the words at uni- or bigramlevel.
Therefore, we believe that a manual eval-uation can reveal more potential of our system ingenerating abstractive summaries that are closer tohuman-written summaries.143Human-authored: the project manager recapped the de-cisions made in the previous meeting.System: I told you guys about the three new require-ments ... so that was the last meeting.Type 3: Subjective abstractive summaries: of-ten it is not easy for humans to agree on one sum-mary for a meeting.
It is well known that inter-annotator agreement is quite low for the summa-rization task (Mani, 2001).
For example:Human-authored 1: They do tool training with a white-board and each person introduces themselves and drawstheir favorite animal on the board.Human-authored 2: The group introduced themselves toeach other and acquainted themselves with the meeting-room materials by drawing on the whiteboard.System: We are gonna know each other and then drawyour little animal.Type 4: Speaker information: since the nature ofour method is based on extracting the relevant sen-tences or speaker utterances, we do not take thespeaker information into consideration.
However,the human-written summaries for meetings takethe speaker into account.
We plan to extend ourframework to include this feature.
For example:Human-authored: The project manager opened themeeting and stated the agenda to the team members.System: I hope you?re ready for this functional designmeeting know at the end projects requirement.Type 5: Transcription errors: as mentioned be-fore, the meeting transcripts often contain struc-ture, grammar, vocabulary choice and dictation er-rors.
This always raises more challenges for algo-rithms dealing with such texts.
For example:Transcript: if it i if it isn?t more expensive for us to kmake because as far as I understand it.In light of this analysis, we conclude that amore comprehensive evaluation method (e.g., hu-man evaluation), including speaker information inthe pipeline and using text normalization tech-niques to reduce the effects of noisy transcripts canbetter reveal the potential of our system in dealingwith meeting summarization.5 Conclusion and Future WorkIn this paper, we study the problem of abstrac-tive meeting summarization, and propose a novelframework to generate summaries composed ofgrammatical sentences.
Within this framework,this paper makes three main contributions.
First,in contrast with most current methods based onfully extractive models, we propose to take advan-tage of a word graph model for sentence fusionto generate abstractive summary sentences.
Sec-ond, beyond most of the current approaches whichdisregard semantic information, we integrate se-mantics by means of building textual entailmentgraphs over sentence communities.
Third, ourframework uses minimal syntactic information incomparison with previous methods and does notrequire a domain specific, engineered conven-tional NLP component.We successfully applied our framework overa challenging meeting dataset, the AMI corpus.Some significant improvements over our dataset,in comparison with previous methods, demon-strates the potential of our approach in dealingwith meeting summarization.
Moreover, we provethat our model can generate longer sentences withonly a minimal loss in grammaticality.In light of the results of our preliminary forma-tive evaluation, future work will address the im-provement of the community detection and sen-tence fusion phases.
On the one hand, we plan toimprove our community detection graph by addingmore relevant features into our current supervisedmodel.
On the other hand, we plan to incorporatea better source of lexical knowledge in the wordgraph construction (e.g., YAGO or DBpedia).
Weare also interested in improving our ranking modelby assigning tuned weights to each component.
Inaddition, we are exploring the replacement of pro-nouns by their referents (e.g., replacing ?I?
by thename or role of the speaker) to improve both theentailment and word graph models.
Once we willhave explored all these improvements, we plan torun more comprehensive human evaluations.AcknowledgmentsWe would like to thank the anonymous review-ers for their valuable comments and suggestionsto improve the paper, our annotators for their valu-able work, and the NSERC Business IntelligenceNetwork for financial support.144ReferencesMeni Adler, Jonathan Berant, and Ido Dagan.
2012.Entailment-based text exploration with applicationto the health-care domain.
In Proceedings ofthe ACL 2012 System Demonstrations, ACL ?12,pages 79?84, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Regina Barzilay and Kathleen R. McKeown.
2005.Sentence Fusion for Multidocument News Sum-marization.
Comput.
Linguist., 31(3):297?328,September.Luisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
TheFifth PASCAL Recognizing Textual EntailmentChallenge.
In Proc Text Analysis Conference(TAC09.Jonathan Berant, Ido Dagan, and Jacob Goldberger.2011.
Global Learning of Typed Entailment Rules.In Proceedings of ACL, Portland, OR.Jaime Carbonell and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
In Proceed-ings of the 21st annual international ACM SIGIRconference on Research and development in infor-mation retrieval, SIGIR ?98, pages 335?336, NewYork, NY, USA.
ACM.Jean Carletta, Simone Ashby, Sebastien Bourban,Mike Flynn, Thomas Hain, Jaroslav Kadlec, VasilisKaraiskos, Wessel Kraaij, Melissa Kronenthal, Guil-laume Lathoud, Mike Lincoln, Agnes Lisowska, andMccowan Wilfried Post Dennis Reidsma.
2005.The AMI meeting corpus: A pre-announcement.
InProc.
MLMI, pages 28?39.Timothy Chklovski and Patrick Pantel.
2004.
VerbO-cean: Mining the Web for Fine-Grained SemanticVerb Relations.
In Dekang Lin and Dekai Wu, ed-itors, Proceedings of EMNLP 2004, pages 33?40,Barcelona, Spain, July.
Association for Computa-tional Linguistics.I.
Dagan and O. Glickman.
2004.
Probabilistic Tex-tual Entailment: Generic applied modeling of lan-guage variability.
In PASCAL Workshop on Learn-ing Methods for Text Understanding and Mining.Michael Denkowski and Alon Lavie.
2010.METEOR-NEXT and the METEOR paraphrase ta-bles: improved evaluation support for five targetlanguage.
In Proceedings of the Joint Fifth Work-shop on Statistical Machine Translation and Met-ricsMATR, WMT ?10, pages 339?342, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Christiane Fellbaum.
1998.
WordNet: An ElectronicLexical Database.
Bradford Books.Katja Filippova.
2010.
Multi-sentence compression:finding shortest paths in word graphs.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics, COLING ?10, pages 322?330, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Kavita Ganesan, ChengXiang Zhai, and Jiawei Han.2010.
Opinosis: a graph-based approach to abstrac-tive summarization of highly redundant opinions.
InProceedings of the 23rd International Conferenceon Computational Linguistics, COLING ?10, pages340?348, Stroudsburg, PA, USA.
Association forComputational Linguistics.Nikhil Garg, Benoit Favre, Korbinian Reidhammer,and Dilek Hakkani Tu?r.
2009.
ClusterRank: AGraph Based Method for Meeting Summarization.Idiap-RR Idiap-RR-09-2009, Idiap, P.O.
Box 592,CH-1920 Martigny, Switzerland, 6.Pierre-Etienne Genest and Guy Lapalme.
2010.
TextGeneration for Abstractive Summarization.
In Pro-ceedings of the Third Text Analysis Conference,Gaithersburg, Maryland, USA.
National Instituteof Standards and Technology, National Institute ofStandards and Technology.Dan Gillick, Korbinian Riedhammer, Benoit Favre, andDilek Hakkani-tr.
2009.
A global optimizationframework for meeting summarization.
In Proc.IEEE ICASSP, pages 4769?4772.David Graff and Christopher Cieri.
English GigawordCorpus?, year = 2003, institution = Linguistic DataConsortium, address = Philadelphia,.
Technical re-port.Steve Gregory.
2007.
An Algorithm to Find Over-lapping Community Structure in Networks.
InProceedings of the 11th European conference onPrinciples and Practice of Knowledge Discovery inDatabases, PKDD 2007, pages 91?102, Berlin, Hei-delberg.
Springer-Verlag.T.
Joachims.
1999.
Making large-Scale SVMLearningPractical.
LS8-Report 24, Universita?t Dortmund, LSVIII-Report.Michael Lesk.
1986.
Automatic sense disambigua-tion using machine readable dictionaries: how to tella pine cone from an ice cream cone.
In Proceed-ings of the 5th annual international conference onSystems documentation, SIGDOC ?86, pages 24?26,New York, NY, USA.
ACM.Chin-Yew Lin and Eduard Hovy.
2003.
Auto-matic evaluation of summaries using N-gram co-occurrence statistics.
In Proceedings of the 2003Conference of the North American Chapter of theAssociation for Computational Linguistics on Hu-man Language Technology - Volume 1, NAACL ?03,pages 71?78, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.145Fei Liu and Yang Liu.
2009.
From extractive to ab-stractive meeting summaries: can it be done by sen-tence compression?
In Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, ACLShort?09, pages 261?264, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Elena Lloret, O?scar Ferra?ndez, Rafael Mun?oz, andManuel Palomar.
2008.
A Text Summarization Ap-proach under the Influence of Textual Entailment.
InNLPCS, pages 22?31.I.
Mani.
2001.
Automatic summarization.
NaturalLanguage Processing, 3.
J. Benjamins PublishingCompany.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards cross-lingual textual entailment.In Human Language Technologies: The 2010 An-nual Conference of the North American Chapter ofthe Association for Computational Linguistics, HLT?10, pages 321?324, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.Yashar Mehdad, Matteo Negri, and Marcello Fed-erico.
2011.
Using bilingual parallel corpora forcross-lingual textual entailment.
In Proceedingsof the 49th Annual Meeting of the Association forComputational Linguistics: Human Language Tech-nologies - Volume 1, HLT ?11, pages 1336?1345,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Yashar Mehdad, Matteo Negri, and Marcello Federico.2012.
Detecting semantic equivalence and informa-tion disparity in cross-lingual documents.
In Pro-ceedings of the 50th Annual Meeting of the Associ-ation for Computational Linguistics: Short Papers- Volume 2, ACL ?12, pages 120?124, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Yashar Mehdad, Giuseppe Carenini, and RaymondNG T. 2013.
Towards Topic Labeling with PhraseEntailment and Aggregation.
In Proceedings ofNAACL 2013, pages 179?189, Atlanta, USA, June.Association for Computational Linguistics.R.
Mihalcea and P. Tarau.
2004.
TextRank: Bringingorder into texts.
In Proceedings of the 2004 Con-ference on Empirical Methods in Natural LanguageProcessing, July.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2010.
Generating and validating abstracts of meet-ing conversations: a user study.
In Proceedings ofthe 6th International Natural Language GenerationConference, INLG ?10, pages 105?113, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Gabriel Murray, Giuseppe Carenini, and Raymond Ng.2012.
Using the omega index for evaluating ab-stractive community detection.
In Proceedings ofWorkshop on Evaluation Metrics and System Com-parison for Automatic Summarization, pages 10?18,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Matteo Negri, Luisa Bentivogli, Yashar Mehdad,Danilo Giampiccolo, and Alessandro Marchetti.2011.
Divide and conquer: crowdsourcing the cre-ation of cross-lingual textual entailment corpora.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, EMNLP ?11,pages 670?679, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Matteo Negri, Alessandro Marchetti, Yashar Mehdad,Luisa Bentivogli, and Danilo Giampiccolo.
2012.Semeval-2012 task 8: cross-lingual textual entail-ment for content synchronization.
In Proceedings ofthe First Joint Conference on Lexical and Compu-tational Semantics - Volume 1: Proceedings of themain conference and the shared task, and Volume2: Proceedings of the Sixth International Workshopon Semantic Evaluation, SemEval ?12, pages 399?407, Stroudsburg, PA, USA.
Association for Com-putational Linguistics.Ehud Reiter and Robert Dale.
2000.
Building naturallanguage generation systems.
Cambridge Univer-sity Press, New York, NY, USA.Mark Sammons, V.G.Vinod Vydiswaran, and DanRoth.
2011.
Recognizing textual entailment.
InMultilingual Natural Language Applications: FromTheory to Practice.
Prentice Hall, Jun.Benno Stein and Oliver Niggemann.
1999.
On the Na-ture of Structure and Its Identification.
In Proceed-ings of the 25th International Workshop on Graph-Theoretic Concepts in Computer Science, WG ?99,pages 122?134, London, UK, UK.
Springer-Verlag.Alexander Yeh.
2000.
More accurate tests for the sta-tistical significance of result differences.
In Pro-ceedings of the 18th Conference on ComputationalLinguistics - Volume 2, COLING ?00, pages 947?953.
Association for Computational Linguistics.146
