PP-At tachment :  A Commit tee  Mach ine  ApproachMartha A. AlegreDepartament  de LSIUniversitat Polithcnica de Catalunyamalegre?ls i ,  upc.
esAgusti LloberasJosep M. SopenaLab.
de Neurocomputaci6Universitat de Barcelonapep~axon,  ps i .
ub.
esLab.
de NeurocomputacidUniversitat de Barcelonaagust  i?axon,  ps i .
ub.
esAbstractIn this paper we use various methods for multipleneural network combination in tasks of prepo-sitional phrase attachment.
Experiments withaggregation functions such as unweighted andweighted average, OWA operator, Choquet inte-gral and stacked generalization demonstrate hatcombining multiple networks improve the esti-mation of each individual neural network.
Usingthe Ratnaparkhi data set (the complete trainingset and the complete test set) we obtained an ac-curacy score of 86.08%.
In spite of the high costin computational time of neural net training, theresponse time in test mode is faster than othersmethods.1 IntroductionStructural ambiguity is one of the most seri-ous problems that Natural Language Processing(NLP) systems face.
This ambiguity takes placebecause the syntactic information alone does notsuffice to make an assignment decision.
Con-structions such as Prepositional Phrase (PP), co-ordination, or relative clauses are affected.
Anexhaustive study about the information eededto deal with this particular structural ambiguityhas not been carried out as of yet; nevertheless,in the current literature we can find several pro-posals.?
In certain cases, it seems that the informa-tion needed to solve the attachment comesfrom the general context.
(1.a) John has a telescope.
(1.b) He saw the girl with the telescope.In this particular case, a correct attachmentwould require a model representing the sit-uation in which the different entities are in-volved.
If this were true for all of the cases,determining PP assignment would requirehighly complex computation.?
In some other cases, the information de-termining the PP attachment seems tobe local.
Some works \[Woods et al 1972\],\[Boguraev, 1979\], \[Marcus et al 1993\] sug-gested several strategies that based their231decision-making on the relationships ex-isting between predicates and arguments-what \[Katz and Fodor, 1963\] called selec-tional restrictions.
Cases belonging to thisgroup seem to be easier to handle computa-tionally than the former ones.Regarding these different cases we can speakof two kinds of disambiguation mechanisms.
Onethat can be called a low level mechanism whichuses mainly information regarding selectional re-strictions between predicates and arguments.This mechanism uses a local context in orderto solve syntactic disambiguation: that which isconstituted by the predicate and its arguments.The second mechanism uses higher level infor-mation such as situation models.
If the lowlevel mechanism does not solve the ambiguity,the high level mechanism, which would be acti-vated later, should be able to do it.
There areempirical data that seem to support the fact thathuman beings use these two mechanisms both forword sense disambiguation and syntactic disam-biguation.
For a review see \[Sopena et al 1998\].1.1 Local d isambiguat ionThe low level disambiguation for the PP  is onetask that has been somewhat successfully treatedusing statistical methods.
Not all of the methodsuse the selectional restrictions mechanism sincethey don't make use of semantic lasses.
We willuse the term local disambiguation to encompassthe methods based on selectional restrictions aswell as those based on lexical association.1.2 Se lec t iona l  res t r i c t ions  and  PP -a t tachmentA system that correctly uses the information ofsemantic classes must first choose the correctsense of each word.
If a hierarchy is used, anadequate level of abstraction must be deter-mined.
In Figure 1 it is shown how the level ofabstraction can change depending on the verb.Considering the following examples :(3.a) Give access to documents(3.b) Give a present o the driverIn WordNet (WN) give has 27 senses, driver4 senses and present 3 senses.
With the co-occurrence of give, driver, present, the senses"give something to someone", "vehicle operator" and "gift" respectively are selected.
The othersenses not selected can be considered as noise.On the other hand, the adequate level of abstrac-tion of driver is PERSON.
The adequate level ofabstraction of present is OBJECT.
(2.a) To eat the strawberry with pleasureENTITYOBJECTSUBSTANCEFOODGREEN GOODSEDIBLE FRUITAdequate level ofabstractionof strawberry in (2.a)(2.b) To take a strawberry from the box.ENTITYOBJECTSUBSTANCE - - -FOODGREEN GOODSEDIBLE FRUITAdequate level ofabstractionof strawberry in (2.b)Figure 1.Most of the statistical methods that have used classesdo not carry out a prior disambiguation f the words\[Brill, Resnick 1994\], \[Ratnaparkhi et.
al 1994\] andothers, nor do they determine the adequate level ofabstraction.
Some that do make the determinationhave a poor level of efficiency.Table 1 shows the accuracy of the results reportedin previous work.
The worst results were obtainedwhen only classes were used.Stettina and Nagao  used the Ratnaparkhidata set but they eliminated 3,224 4-tuples(15~) from the training set containing contra-dicting examples.For reasons of complexity, the complete 4-tuple has not been considered simultaneously ex-cept in \[Ratnaparkhi et.
al 1994\].Classes of a given sense and classes of differ-ent senses of different words can have complexinteractions and the preceding methods  cannottake such interactions into account.Neural networks (NNs) are appropriates indealing with this complexity.
A very impor-232Author BestHindle and Rooth (1993) 80.0 %Resnik and Hearst (1993) 83.9 %WN Resnik and Hearst (1993) 75.0 %Ratnaparkhi et al (1994) 81.6 %Brill and Resnik (1994) 81.8Collins and Brooks (1995) 84.5Stettina and Nagao (1997) 88.0Sopena et al (1998) 86.2Li and Abe (1998) 82.4Table 1: Test and accuracy results%%%%%Classes Use ofRatnaparkhi setNo NoWN NoNoMIC YesWN NoNo YesWN YesWN NoWN Noreported in previous works.tant characteristic of NNs is their capacity todeal with multidimensional inputs.
They needmuch fewer parameters to achieve the same re-sult than traditional numerical methods.
Re-cently \[Barton, 1993\] has shown that feedfor-ward networks with one layer of sigmoidal non-linearities achieve an integrated squared errorof order O(?)
for input spaces of dimension d,where n is the number of units of the network.Traditional methods (series expansions) with nterms can only achieve an integrated squared er-0( (1~2d~ ror of order ~ j  j, for functions satisfyingthe same smoothness assumption.
NNs are sur-prisingly advantageous in high dimensional in-puts since the integrated squared error is inde-pendent of the input dimension.
They can com-pute very complex statistical functions, they aremodel free, and compared to the current meth-ods used by the statistical approach to NLP, NNsoffer the possibility of dealing with a more com-plex (non-linear and multivariant) approach.In the next section we describe a PP  attach-ment disambiguation system based on neuralnetworks that takes better advantage of the useof classes.2 A neural network approachto PP-at tachmentThe use of classes is fundamental when workingwith neural networks.
Using words alone with-out their classes in real texts, floods the memorycapacity of a neural network.
It is well knownthat the use of words creates huge probabilis-tic tables.
In Mdition, the use of classes suc-cessfully deals with problems of invariance re-lated to compositionality and binding that neu-ral networks have \[Sopena, 1996\].
PP  attach-ment can be considered as a classification prob-lem were 4-tuples are classified in two classes: asto whether it is attached to the noun or to theverb \[Sopena et al 1998\].These classes are represented in the outputunits.
When a local representation for classes isused (one class per unit) the output act iwtionof each unit can be interpreted as the Bayesianposterior probability that the pattern in the in-put belongs to the class represented by this unit.In our case we have two units: one represent-ing the class "attached to noun" and the otherthe class "attached to verb".
The activitation ofthese units will represent he respective proba-bility of attachment given the 4-tuple encoded inthe input.Given the set of words in the 4-tuple we haveto determine a way to represent senses and se-mantic class information.
Polysemy representsa problem when using word classes.
In order touse class information, two different proceduresare possible.
The first one consists in presentingall the classes of each sense of each word serially.The second one consists in the simultaneous pre-sentation of all the senses of all the words.
Inprevious works we have found that parallel pre-sentation improve results.The parallel procedure has the advantage ofdetecting in the network classes that are related233to others within the same slot or among differentslots.Presenting all of the classes simultaneously(including verb classes) allows us to detect com-plex interactions among them (either the classesof a particular sense or the classes of differentsenses of a particular word) that cannot be de-tected in most of the methods used so far.
Wehave been able to detect their existence in ourstudies on word sense disambiguation currentlybeing carrying out.
If we present simultaneouslyall the classes of all the senses of each word inthe 4-tuple we will have a very complex input.A system capable of dealing with such an inputwould be able to select classes (and consequentlysenses) which are compatible with other ones.Finally, and related to the above, most of thestatistical methods used in Natural LanguageProcessing are linear.
Multilayer feedforwardnetworks are non linear.
One of the objectives ofexperiments i  to see if introducing non-linearityimproves the results.2.1 Test and training dataWe used the same data set (the completetraining set and the complete test set) as\[Ratnaparkhi et.
al 1994\] for purposes of com-parison.
In this data set the 4-tuples of the testand training sets were extracted from Penn Tree-bank Wall Street Journal \[Marcus et al 1993\].The test data consisted of 3,097 4-tuples with20,801 4-tuples for the training data.The following process was run over both testand training data:All numbers were replaced by the string"whole_number"- All verbs and nouns were reduced to their mor-phological stem using WordNet.275 nouns which had not been found inWordNet\[Miller et.
a1.,1993\] were replaced bysynonyms using WordNet.- The remaining nouns and verbs which had notbeen found in WordNet were left uncoded.Proper nouns were replaced by WordNetclasses like "person", "business_organization',"social_group".- Prepositions found fewer than 20 times eitherwere not represented.2.2 CodificationThe input was divided into eight slots.
The firstfour slots represented 'verb', 'nl ' ,  'prep', and 'n2'respectively.
In slots 'n l '  and 'n2' each sense ofthe corresponding noun was encoded using allthe classes within the IS-A branch of the Word-Net hierarchy.
This was done from the corre-sponding hierarchy root node to its bottom-mostnode.
In the verb slot, the verb was encoded us-ing the IS-A-WAY-OF branches.
Each node inthe hierarchy received a local encoding.
Therewas a unit in the input for each node of theWordNet subset.
This unit was ON if it rep-resented a semantic class to which one of thesenses of the encoded word belonged.Using a local representation we needed a unitfor each class-synset.
The number class-synsetsin WordNet is too large for a neural network.
Inorder to reduce the number of input units we didnot use WordNet directly, but constructed a newhierarchy (a subset of WordNet) including onlythe classes that corresponded to the words thatbelonged to the training and test sets.A feedforward neural network can make gooduse of class information if there is a sufficientnumber of examples belonging to each class.
Forthat reason we also counted the number of timesthe different semantic classes appeared in thetraining and test sets.
The hierarchy was prunedtaking these statistics into account.
Given athreshold h, classes which appeared less than h%were not included.
In all the experiments of thispaper, we used tree cut thresholds of 2% .
Re-garding prepositions, only the 36 most fl'equentones were represented (those found more than20 times).
For those, a local encoding was used.The rest of the prepositions were left uncoded.The fifth slot represented the prepositions thatthe verb subcategorized.
By representing theprepositions, \[Sopena et al 1998\] had obtainedimproved results.
The reason for this improve-ment being that English verbs with semanticsimilarity may take on different prepositions (forexample, accuse with of and blame with for).Apart from semantic lasses, verbs can also be234classified on the basis of the kind of prepositionsthey make use of.The prepositions that the verbs subcatego-rize were initially extracted from COMLEX\[Wolff et al, 1995\].
Upon observation thatCOMMLEX does not consider all the subcatego-rized prepositions, we complemented COMLEXwith information extracted from training data.The prepositions of all the 4-tuples assigned tothe verb were considered.
The distinction be-tween PP  adjuncts and PP  close-related were notavailable in the Ratnaparkhi data set.
Therefore,we grouped the subcategorized prepositions bytheir verbs as well as those that govern PP ad-juncts.
Only the 36 most frequent prepositionswere represented.The sixth slot represented the prepositionsthat were governed by 'nl'.
Again, only the36 most frequent prepositions were represented.These prepositions were extracted from the 4-tuples of the training data whose attachmentswere to the noun.The next slot represented 15 units for the lex-icography verb files of WordNet.
WordNet has alarge number of verb root nodes, some of whichare not frequent.
Due to this fact, in some casesthe pruning that was carried out on the treemade root nodes disappear.
This lead to someof the verbs that belonged to this class not be-ing coded.
In order to avoid these cases, we usedthe names of the WordNet verb lexicographicalfiles to add a new top level in the WordNet verbclass hierarchy.
Finally, in the last slot there are2 units to indicate whether or not the N1 or N2respectively were proper nouns .Regarding the output, there were only twounits representing whether the PP  was attachedto the verb or to the noun.Feedforward networks with one hidden layerand full interconnectivity between layers wereused in all the experiments.
The networks weretrained with the backpropagation learning algo-rithm.
The activation function was the hyper-bolic tangent function.
The number of hiddenunits used was 0, 50 and 100.
For all simu-lations the momentum was 0, and the initialweight range 0.1.A validation set was constructed using 12,0294-tuples extracted from the Brown Corpus.In each run the networks were trained for 60epochs storing the epoch weights with the small-est error regarding the validation set, as well asthe weights of the 60th epoch (without the vali-dation set).3 ExperimentsTable 2 shows the results of 24 training simula-tions obtained in the test data using 0, 50 and100 hidden units respectively.
We show the bestresults by the networks acting individually.MethodsPerceptron50 Hidden100 HiddenBacked-OffBest Results83.08%85.18%85.37%84.50%60th epoch Seconds82.67 % 1684.21% 4484.50 % 78230Table 2: Results obtained with Backed-Off, 0,50 and 100 hidden units and time in seconds todisambiguate 3,097 4-tuples.In spite of the high cost in computationaltime of neural net training, the response time intest mode is up 3 times faster than Backed-Offmodel.
This is shown in Table 2 where the timetaken to disambiguate 3,097 4-tuples is given.In this problem we had a high level of noise:on one hand the inadequate senses of each wordin the 4-tuple.
Words in English have a highnumber of senses thus, in the input, the level ofnoise (inadequate sense) can reach 5 times thatof signal (correct sense).
In addition, the Rat-naparkhi data set contains many errors, some ofthem due to errors originating from the PennTreebank I.
This level of noise deteriorates thegeneralizing capacity of the neural network.There are many methods that permit a neuralnetwork to improve its capacity of generaliza-tion.
For reasons of complexity, the size of thenetwork that we are using places restrictions onthe selection of the method.
Of the methodsthat we are testing, committee machines allowus to improve results the most easily.2353.1 Exper iments with Committees ofnetworks:The performance of a committee machine\[Perrone, Cooper, 1993\], \[Perrone, 1994\] and\[Bishop C., 1995\] can outperform that of thebest single network used in isolation.As \[Kuncheva et al 1998\] points out, the pro-cess of combining multiple classifiers to achievehigher accuracy is given different names in theliterature apart from committee machines: com-bination, classifier fusion, mixture of experts,consensus aggregation, classifier ensembles, etc.We have applied the following algorithms: aver-age, weighted average, OWA operator, Choquetintegral and stacked generalization.3.1.1 Average  :Suppose we have a set of N trained networkmodels yi(x) where i = 1, ..., N.We can then write the mapping function ofeach network as the desired function t(x) plusan error function \[Bishop C., 1995\]:= t(x) + e (x)The average sum-of-squares error for modely~(x) can be written asEi = E\[(yi(x) - t(x)) 2\] = E\[e~\]The output of the committee is the averageof the outputs of the N networks that integratesthe committee, in the form1 NYCOM(X) = N yi(x).If we make the assumption that the errorsei(x) have zero mean and are uncorrelated, wehave1ECOM = ~EAvwhere ECOM is the average rror made by thecommittee and EAV is the average rror madeby the networks acting individually.In general, the errors ei(x) are highly cor-related but even then it is easy to show that\[Bishop C., 1995\]:ECOM ~_ EAVAs some members of the committee will invari-ably give better results than others, it is of inter-est to give more weight to some of the membersthan to others taking the form:Nya N(x) =i----1here wi is based on the error of the validationand learning set.3.1.2 The Ordered Weighted Averaging(OWA)  OperatorsIf w is a weighting vector of dimension n, thena mapping OWAw : R" ~ R is an OrderedWeighted Averaging (OWA) operator of dimen-sion n \[Yager, 1993\] :nOWA(y l , .
.
.
, yn )  =i----1where {a(1) , - .
- ,a (n)}  is a permutation of{1 , .
.
- ,n )  such that ya( i -  1) >_ ya(i) for alli=2 , .
.
.
,n .The OWA operator permits weighting the val-ues in relation to their ordering.Results are show in Tables 3, 4 and 5.3.1.3 Choquet  integral:The fuzzy integral introduced by\[Choquet G, 1954\] and the associated fuzzymeasures, provide a useful way for aggregationinformation.
A fuzzy measure u defined onthe measurable space (X,X) is a set functionu : X -+ \[0, 1\] satisfying the following axioms:(i)u(Q) = O, u(X) = 1 (boundary conditions)(ii)A C_ B --+ u(A) < u(B) (monotonicity)(X,X,u) is said to be a fuzzy measurable space.236If u is a fuzzy measure on X, then the Choquetintegral of a function f : X --+ R with respect ou is defined by:f fdu ~-~(f(y~(i)) - f(y~(i - 1)))u(A.s(i))i=1where f(y~(i)) indicates that the indices havebeen permuted so that0 < f(ys(1)) < ... < f(y~(n)) < 1,As(i) = ys(i),...,ys(n) and f(ys(O)) =0One characteristic property of Choquet inte-grals is monotonicity, i.e., increases of the inputlead to higher integral values.
Results are shownin Table 6 and Table 7.Nets6121824Average Weighted Average OWA84.92 % 85.34 % 85.18 %85.18 % 85.63 % 85.28 %85.21% 85.89 % 85.60 %85.31% 85.76 % 85.63 %Table 3: Results Committee machines.
50 hiddenlayersNets6 85.53 %12 85.34 %18 85.41%24 85.76 %Average Weighted Average OWA85.53 % 85.53 %85.53 % 85.76 %85.73 %85.92 %85.89 %86.02 %Table 4: Results Committee machines.
100 hiddenlayersNets6121824Average Weighted Average OWA84.98 % 85.11% 84.98 %84.85 % 85.15 % 85.02 %84.89 % 85.28 % 85.24 %85.24 % 85.41% 84.92 %Table 5: Results Committee machines in 60thepoch.
100 hidden layersNet 1 Net 2 Net 3 Choquet84.50 % 83.34 % 83.60 % 84.92 %84.50 % 84.24 % 84.18 % 85.02 %84.82 % 85.37 % 84.76 % 85.66 %84.57 % 84.57 % 85.24 % 84.79 %Table 6: Results Choquet integral.3.2 Stacked generalization:\[Wolpert, 1992\] provides one way of combiningtrained networks which partitions the data setin order to find an overall system which usuallyimproves generalization.
The idea is to train thelevel-0 networks first and then examine their be-havior when generalizing.
This provides a newtraining set which is used to train the level-1network.
The inputs consist of the outputs of allthe level-0 networks, and the target value is thecorresponding target value from the original fulldata set.
Our experiments using this method didnot give improved results (85.35%).Net 1 Net 2 Net 3 Net 4 Choquet84.24 % 84.05 % 84.50 % 83.66 % 85.28 %84.18 % 84.11% 84.50 % 84.24 % 85.60 %85.37 % 84.76 % 84.79 % 84.63 % 85.79 %84.79 % 84.76 % 85.37 % 84.82 % i 86.08 %Table 7: Results Choquet integral.4 Conc lus ionsNeural networks have been shown to be very suc-cessful in tasks such as pattern recognition orprediction in many different applications of busi-ness, biomedicine, engineering, astronomy, highenergy physics, etc.
Their results are similar andoften better than those of alternative models.The benefits of neural networks are well knownas was explained above.
Unfortunately neuralnetworks have not been very successful in the do-main of Natural Language Processing.
However,our system has obtained better results than anythat have been published to date using the com-plete Ratnaparkhi data set.
We also obtainedexcellent results in word sense disambiguation\[Moliner, 1998\].
Our success can be attributedto two things: on one hand, the use of seman-tic classes is fundamental to keep from floodingthe network's memory.
In other hand, the use ofcanonic thematic structures.
Finally, improve-ment on the generalization is an area in perma-nent development in the field of neural networks.We are developing new methods of generalization237which will allow us to improve our results evenmore.
Provisional results place us in the envi-ronment of 88% with Ratnaparkhi's data set.References\[Barron, 1993\] Universal Approximation Bounds forSuperposition of a Sigmoidal Function.
IEEETransactions on Information Theory, 39:930-945\[Bishop C., 1995\] Neural Networks for pattern recog-nition.\[Boguraev, 1979\] Automatic resolution of linguisticambiguities.
Ph.D. Computer Laboratory, Uni-versity of Cambridge.\[Brill, Resnick 1994\] A Rule-Based Approach toPrepositional Phrase Attachment Disambigua-tion.
In Proceedings of the Fifteenth Interna-tional Conferences on Computational Linguis-tics (COLING-94).\[Choquet G, 1954\] Theory of capacities.
Annales deL'Institut Fourier, 5, 1953-54, pp.
131-295\[Katz and Fodor, 1963\] .
The Structure of SemanticTheory.
Language, 39: 170-210.\[Kuncheva etal.
1998\] On Combining Multiple Clas-sifters by Fuzzy Templates, Proc.
NAFIPS'98.Pensacola.
Florida, pp.
193-197.\[Marcus et al 1993\] Building a Large AnnotatedCorpus of English: The Penn Treebank.
Com-putational Linguistics,19:313-330\[Miller et.
a1:,1993\] Introduction to WordNet: AnOnline Lexical Database.
Anonymous FTP, in-ternet:clarity.princeton.edu.\[Moliner, 1998\] Un enfoque neuronal de la desam-biguacion del sentido de las palabras (A NeuralNetwork approach to word sense disambigua-tion), Technical University of Catalonia.
LSIDept.
Ph.D. Thesis.
1998.\[Perrone, 1994\] General averaging results for con-vex optimization.
In M. C. mozer et al (Eds.
),Proceedings 1993 Connectionist Models SummerSchool, pp.
364-371.
Hillsdale, N J: LawrenceErlbaum.\[Perrone, Cooper, 1993\] When networks disagree:ensemble methods for hybrid neural networks.In R. J. Mammone (Ed.
), Artificial Neural Net-works for Speech and Vision,, pp.
126-142.
Lon-don: Chapman ~z Hall.\[Ratnaparkhi et.
al 1994\] A Maximum EntropyModel for Prepositional Phrase Attachment.
InProceedings of the ARPA Workshop on HumanLanguage Technology.\[Sopena, 1996\] Word sense disambiguation: a neu-ral network approach.
Technical report 3-96.Laboratori de Neurocomputacio.
University ofBarcelona.\[Sopena et al 1998\] A Connectionist Approach toPrepositional Phrase Attachment for RealWorld Texts.
In COLING-ACL'98.
pp.
1233-1237\[Wolff et al, 1995\] Comlex Word Classes.
C.S.Dept., New York U., Feb. prepared for theLinguistic Data Consortium, U. Pennsylvania.\[Wolpert, 1992\] Stacked generalization.
Neural Net-works 5 (2), pp.
241-259.\[Woods et al 1972\] The Lunar Sciences NaturalLanguage Information System: Final report Re-port 2378, Bolt, Beranek and Newman,Inc.,Cambridge, MA.\[Yager, 1993\] Families of OWA operators, Fuzzy Setsand Systems, 59 pp.
125-148.238
