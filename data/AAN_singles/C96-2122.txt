An Earley-type recognizer for dependency grammarVincenzo Lombardo and Leonardo LesmoDipartimento di lnformatica nd Centro di Scicnza CognitivaUniversith di Torinoc.so Svizzcra 185, 10149 Torino, Italye-mail: {vincenzo, lesmo}@di.unito.itAbstractThe paper is a first attempt to fill a gap in thedependency literature, by providing amathematical result on the complexity ofrecognition with a dependency grammar.
Thepaper describes an improved Earley-typerecognizer with a complexity O(IGl2n3).
Theimprovement is due to a precompilation f thedependency rules into parse tables, that determinethe conditions of applicability of two primaryactions, predict and scan, used in recognition.1 IntroductionDependency and constituency frameworks definedifferent syntactic structures.
Dependency grammarsdescribe the structure of a sentence in terms of binaryhead-modifier (also called dependency) relations onthe words of the sentence.
A dependency relation isan asymmetric relation between a word callexl head(governor, parent), and a word called modifier(dependent, daughter).
A word in the sentence canplay the role of the head in several dependencyrelations, i.e.
it can have several modifiers; but eachword can play the role of the modifier exactly once.One special word does not play the role of themodifier in any relation, and it is named the root.
Theset of the dependency relations that can be defined ona sentence form a tree, called the dependency tree(fig.
la).Although born in the same years, dependencysyntax (Tesniere 1959) and constituency, or phrasestructure, syntax (Chomsky 1956) (see fig.lb), havehad different impacts.
The mainstream of formalismsconsists ahnost exclusively of constituencyapproaches, but some of the original insights of thedependency tradition have found a role in theconstituency formalisms: in particular, the concept ofhead of a phrase and the use of grammatical relations.The identification of the head within a phrase hasbeen a major point of all the recent frameworks inlinguistics: the X-bar theory (Jackendoff 1977),defines phrases as projections of (pre)terminalsymbols, i.e.
word categories; in GPSG (Gazdar et al1985) and HPSG (Pollard, Sag 1987), each phrasestructure rule identifies a head and a relatedsubcategorization within its right-hand side; in HG(Pollard 1984) the head is involved in the so-calledhead-wrapping operations, which allow theformalism to go beyond the context-free power (Joshiet al 1991).Grmmnatical relations are the primitive entities ofrelational grammar (Perhnutter 1983) (classified as adependency-based theory in (Mercuk 1988)):cookedSUBJchef t i shT~r /  , ,the a(a)S/NP \  /V  kD N V NPI I " D N I Ithe dlef cooked a fishFigure 1.
A dependency tree (a) and a p.s.tree (b) for the sentence "The chef cooked afish".
The leftward or rightward orientationof the arrows in the dependency treerepresents the order constraints: themodifiers that precede the head stand on itsleft, the modifiers that follow the head standon its right.subject, object, xcomplement .... label the dependencyrelations when the head is a verb.
Grainmaticalrelations gained much popularity within theunification formalisms in early 1980%.
FUG (Kay1979) and LFG (Kaplan, Bresnan 1982) exhibitmechanisms for producing a relational (or functional)structure of the sentence, based on the merging offeature representations.All the recent constituency formal ismsacknowledge the importance of the lexicon, andreduce the amount of information brought by thephrasal categories.
The "lexicalization" of context-free grmnmars (Schabes, Waters 1993) points outmany similarities between the two paradigms(Rainbow, Joshi 1992).
Dependency syntax is anextremely lexicalized framework, because the phrasestructure component is totally absent.
Like the otherlexicalized frameworks, the dependency approachdoes not produce spurious grammars, and this facilityis of a practical interest, especially in writing realisticgrammars.
For instance, there are no heavilyambiguous, infinitely ambiguous or cyclicdependency grammars (such as S ~ SS; S ~ a; S --*~; see (Tomita 1985), pp.
72-73).723Dependency syntax is attractive because of theimmediate mapping of dependency structures on thepredicate-argmnents s ructure (accessible by thesemantic interpreter), and because of the treatment offree-word order constructs (Sgall et al 1986)(Mel'cuk 1988) (Hudson 1990).
A number of parsershave been developed for some dependencyframeworks (Fraser 1989) (Covington 1990) (Kwon,Yoon 1991) (Sleator, Temperley 1993) (Hahn et al1994) (Lai, Huang 1995): however, no result ofalgorithmic efficiency has been published as far aswe know.
The theoretical worst-case analysis ofO(n 3) descends from the (weak) equivalence betweenprojective dependency grammars (a restricted ofdependency grammars) and context-free grammars(Gaifman 1965), and not from an actual parsingalgorithm,This paper is a first attempt o fill a gap in theliterature between the linguistic merits of thedependency approach (widely debated) and themathematical properties of such formalisms (quitenegleted).
We describe an improved Earley-typerecognizer for a projective dependency formalism.
Asa starting point we have adopted a restricteddependency formalism with context-free power, that,for the sake of clearness, is described in the notationintroduced by Gaifman (1965).
The dependencygrammar is translated into a set of parse tables thatdetermine the conditions of applicability of theprimary parser operations.
Then the recognitionalgorithm consults the parse tables to build the sets ofitems as in Earley's algorithm for context-freegrammars.2 A dependency  fo rmal i smIn this section we introduce a dependency formalism.We express the dependency relations in terms of rulesthat are very similar to their constituency counterpart,i.e.
context-free grammars.
The formalism has beenadapted from (Gaiflnan 1965).
Less constraineddependency formalisms exist in the literature(Mel'cuk 1988) (Fraser, Hudson 1992), but nomathematical studies on their expressive power exist.A dependency grammar is a quintuple <S, C, W, L,T>, whereW is a finite set of symbols (vocabulary of words of anatural language),C is a set of syntactic ategories (preterminals, inconstituency terms),S is a non-empty set of root categories (C _ S),L is a set of category assignment rules of the form X:x, where XCC, x@W, andXYI Y2 ... Yi-1 Yi+l ... YmFigure 2 - A dependency rule: X is thegovernor, and Y1 ..... Ym are the dependentof X in the given order (X is in # position).T is a set of dependency rules of the form X(Y1 Y2... Yi-1 # Yi+l ... Ym), where XGC, Y1GC,.... Ym@C, and # is a special symbol that does notbelong to C. (see fig.
2).The modifier symbols Yj can take the form Yj*: asusual, this means that an indefinite number of Yj's(zero or more) may appear in an application of therule 1 .
In the sample grammar below, this extensionallows for several prepositional modifiers under asingle verbal or nominal head without introducingintermediate symbols; the predicate-argumentsstructure is immediately represented by a one-level(flat) dependency structure.Let x=al a2...ap ~W* be a sentence.
A dependencytree of x is a tree such that:1) the nodes are the symbols ai~W (l<i<p);2) a node ak,j has left daughters ak,1 ..... ak,j-1occurring in this order and right daughters ak,j+l,.... ak,q in this order if and only if there exist theroles Ak,l: ak,1 ..... Akj: akj ..... Ak,q: ak,q in L andthe rule Ak,j(Ak,1 ... Akj-I # Akj+l ... Ak,q) in T.We say that ak,1 ..... akj-1, ak,j+l ..... ak,q directlydepend on ak,j, or equivalently that ak,j directlygoverns ak, 1 ..... ak,j.1, akj+l ...... ak, q. akj and ak, h(h = 1 .
.
.
.
.
j - l ,  j+l .
.
.
.
.
q) are said to be in adependency relation, where ak,j is the head andak,h is the modifier, if there exists a sequence ofnodes ai, ai+l .
.
.
.
.
aj-l, aj such that ak directlydepends on ak-1 for each k such that i+l~k-~j, thenwe say that ai depends' on aj;3) it satisfies the condition ofprojectivity with respectto the order in x, that is, if ai depends directly on ajand ak intervenes between them (i<k<j or j<k<i),then either ak depends on a i or ak depends on aj(see fig.
3);4) the root is a unique symbol as such that As: as E Land As~S.The condition of projectivity limits the expressivepower of the formalism to be equivalent o thecontext-free power.
Intuitively, this principle statesajaiAiki' ak i| i| |i ii iajaiFigure 3.
The condition of projectivity.1 The use of the Kleene star is a notational change withrespect o Gaifman: however, it is not uncommon toallow the symbols on the right hand side of a rule to beregular expressions in order to augment the perspicuityof the syntactic representation, but not the expressivepower of the grammar (a similar extension appears in thecontext-free part of the LFG formalism (Kaplan, Bresnan1982)).724that a dependent is never separated from its governorby anything other than another dependent, ogetherwith its subtree, or by a dependent of its own.As an example, consider the grammarGI= <iV},iV,N, P, A, D},{I, saw, a, tall, old, man, in, the, park, with, telescope},{N: I, V: saw, D: a, A: tall, A: old, N: man, P: in, D: the,N: park, P: with, N: telescope}TI>,where T1 is the following set of dependency rules:t. V(N # P*); 2.
V(N # N P*);3.
N(A*#P*); 4.
N(DA* # P*);5.
P(# N); 6.
A(#); 7.
D(#).For instance, the two rules for the root categoryV(erb) specify that a verb (V) can dominate one ortwo nouns and some prepositions (*).3 Recognition with a dependencygrammar~\[he recognizer is an improved Earley-type algorithm,where the predictive component has been compiled ina set of parse tables.
We use two primary actions:predict, that corresponds to the top-down guessing ofa category, and scan, that corresponds to the scanningof the current input word.
In the subsection 3.1 wedescribe the data structures and the algorithms fortranslating the dependency rules into the parse tables:the dependency rules for a category are firsttranslated into a transition graph, and then thetransition graph is mapped onto a parse table.
In thesubsection 3.2 we present he Earley-type recognizer,that equals the most efficient recognizers for context-tree grmnmar.3,1 Transition graphs and parse tablesA transition graph is a pair (V, E), where V is a set ofvertices called states, and E is a set of directed edgeslabelled with a syntactic ategory or the symbol #.Given a grammar G=<S, C, W, L, T>, a state of thetransition graph for a category Cat ~ C is a'set ofdotted slxings of the Ibrm ".
13", where Cat(c~13) C T# * and et, \[~ E (C U { }) ; an edge is a triple <si, sj, Y>,where si, sj C V and Y G C U {#}.
A state thatcontains the dotted string "."
is called final; a finalstate signals that the recognit ion of one or moredependency rules has been completed.
The followingalgorithm constructs the transition graph for thecategory Cat:lSull.c.ti~ graph (('.at, G):initializations 0 := O;each rule in G of the iorm Cat(a) dos o := s 0 U {star ( .a )}V := is0};E := ~;expansionmllcattake a non-marked state s from V;mark s;each eatcgory Y G C U {#} doS' : :  ~ ;each dotted string r = .Y\[5 in s doi\[ Y is starreds' := s' U star(.Y\[~)e, l~s' :=s'  u {.F};endfor each dotted string;V := V U is'};E := E U {<s, s', Y>}each categoryuntil all states in V are marked;graph := <V,E>.star (dotted-string):set-of strings:= {dotted-string};rsW~a~ttake a non-marked dotted string ds from set-of-strings;mark ds;if ds has the form ".Y\[V' and Y is starred thenset-of-strings := set-of-strings U {".f~"}all dotted strings in set-of-strings are markedstar:= set-of-strings.The initial set of states consists of a single state so,that contains all the possible strings ".a", such thatCat(c0 is a dependency rule.
Each string is prefixedwith a dot.
The marked states are the states that wereexpanded in a previous step.
The expansion of a states takes into account each symbol Y that ilmnediatelyfol lows a dot (Y C C U {#}).
Y is a possiblecontinuation to a new state s', that contains the dottedstring ".\[3", where ".Y\[5" is a dotted string in s. s' isadded to the set of states, and a new edge from s to s'labelled with Y is added to the set of edges.
A dottedstring of the form .Y'13 is treated as a pair of dottedstrings {.Y'13, .\[3}, so to allow a number of iterations(one or more Y's follow) or no iteration (the firstsymbol in \[~ follows) in the next step.
The function"star" takes into account hese cases; the repeat loopaccounts for the case when the first symbol of 13 isstarred too.The transit ion graphs obtained for the f ivecategories of G1 are in fig.
4.
Conventionally, weindicate the non-final states as h and the final statesas Sk, where h and k are integers.The total number of states of  all the transitiongraphs for a grammar G is at most O(IGI), where IGIis the sum of the lengths of the dependency rules.
Thelength of a dependency rule Cat(c0 is the length of (~.Starting from the transition graph for a category Cat,we can build the parse table for Cat, i.e.
PTCa t.VI'Ca t is an array h x k, where h is the number ofstates of the transition graph and k is the nmnber ofsyntactic ategories in C. Each row is identified by apair <Cat, State>, where State is the label of a state ofthe corresponding transition graph; each column isassociated with a syntactic category.
In order toimprove the top-down algorithm we introduce theconcept of "first" of a category.
The first of thecategory Cat is the set of categories that appear asleftmost node of a subtree headed by Cat.
The first ofa category X is computed by a simple procedure thatwe omit here.
The function parse_table computes theparse tables of the various categories.
E(t-graphcat)returns the set of the edges of the graph t -graphca t.The contents of the entries in the parse tables are sets(poss ib ly  empty)  of  pred ic t  and scan.
Theinitialization step consists in setting all entries of thetable to the empty set.7250 1 $2 $3(a) category V P0 $1@' -<D(d) category A0 1 $2 0 $1(c) category P (e) category D"   cat<cat,state> '~<V,0><V,I><V,$2><V,$3><N,0>Fig.
4 - The transition graphs obtained for the grammar G 1.V N A p<scan,S2><predict(N),l>D<predict(N),l><predict(N),$3> <predict(P),$3> <predict(N),$3><predict(P),$3><scan,S2> <predict(D),l><N,I> <scan,S2><N,$2><P,O><predict(N),$2> <P,I><A,0><D,0><predict(N),l><predict(N),$3><predict(A),l><predict(A),l><predict(N),$2><scan,S1><predict(P),$2><scan, l><predict(N),$2><scan,S1 >Figure 5 - The parse tables for the grammar G 1although this does not happen for our simplegrammar G 1.3-2 A dependency recognizerThe dependency recognizer exhibits the same datastructures of Earley's recognizer (Earley 1970), butimproves the performance of that algorithm becauseof the precompilation of the predictive componentinto the parse tables.In order to recognize a sentence of n words, n+l setsSi of items are built.
An item is a quadruple<Category, State, Position, Depcat>where the first two elements (Category and State)correspond to a row of the parse table PTCategory,the third element (Position) gives the index i of theset Si where the recognition of a substructure began,and the fourth one (Depcat) is used to request hecompletion of a substructure headed by Depcat,parse-table (Cat, t-graphcat):initialize PTCat;for each edge <s, s', Y> in E(t-graphcat) doif YECthenfor each category Z ~ first(Y) 0saPTCat(<Cat, s>, Z) := PTCat(<Cat, s>, Z) UI~Cat(<Cat, ~ ,  Z) U{<predict(Y), s'>}elseif Y =#thenPTCat(<Cat, s>, Cat) := PTCat(<Cat, s>, Cat) U{<scan, s'>}endif:endforparse-table := PTCa t.The parse tables for the grammar G 1 are reported infig.
5.
The firsts are: first(V)=first(N)={N, A D};first(P)={P}; first(A)={A}; first(D)={D}.
Note thatan entry of a table can contain more than one action,726before continuing in the recognition of the largerslructure headed by Category (Depcat = "_" meansthat the item is not waiting for any completion).Sentence: w0 w 1 ... Wn.
1initializationeach root category V doINSERT <V, 0, 0, > into SO~a~dforbodyl~ i  fix/m0 ~ n doeach item P=<Cat, State, j, > in Si docompleter:if final(State) then.f.0Z each item <Cat', k, j', Cat> in S i d~INSERT <Cat', k, j', >into S i "predictor:if <predict(Cat'), State'>C l~l'Cat(<Cat, State> ?lnputcat)?
r - , ) , INSERT <Cat ,0, ~, > tntc Si.INSERT <Cat, State" j, Cat'>il\]to S imdifscaI l t I t ;F."if <scan, State'> C lrFCat(<Cat, State> ?Inputcat) th~INSERT <Cat, State', j _> into Si+le, nd~ each itemternfinationj~ <V,$k, O, > is in S n lh~ accept g\]~reject'File external loop of the algorithm cycles on the setsSi (0 < i < n); tile inner loop cycles on the items ofthe set Si of the form <Cat, State, j, > .
At each stepof the inner hoop, the action(s) given by the entry"<Cat, State> x lnputcat" in the parse table PTCa tis(are) executed (where lnputcat is one of thecategories of the current word).
Like in Earley'sparser there are three phases: completer, predictorand scanner.completer: When an item is in a final state (of theform $h), the algorithm looks for the items whichrepresent he beginning of  tile input portion justanalyzed: they are the l:i)ur-element i ems containedin the set referred by j.
These items are inserted intoSi after having set to "null" the fourth element (_).predictor: "<predict(Cat'), State'>" corresponds to aprediction of the category Cat' as a modifier of thecategory Cat and to the transition to State', in case asubstructure headed by Cat' is actually found.
This ismodeled by introducing two new items in the set:a) <Cat', 0, i, > ,  which represents the initial state ofthe transition graph of the category Cat' which willspan a portion of the input starting at i.
In F, arley'sterms, this item corresponds to all the dotted rulesof the form Cat'(.
cz).b) <Cat, State', j, Cat'>, which represents the arc ofthe transition graph of the category Cat, enteringthe state State' and labelled Cat'.
In Earley's terms,this item corresponds to a dotted rule of the formCat(~z .
Cat' l~).
The items including a non-nullDepcat are just passive receptors waiting to be re-activated later when (and ii) the recognition of thehypothesized substructure has successful lycompleted.scanner: "<scan, State'>" results in inserting a newitem <Cat, State', i, __> into the set Si+l.Let us trace the recognition of the sentence "I sawa tall old man in the park with a telescope".
The firstset SO (fig.
6) includes three items: the first one, <V,0, 0, > ,  is produced by the initialization; the nexttwo, <V, 1,0, N> arid <N, 0, 0, _> are produced bythe predictor (a N-headed subtree beginning inposition 0 must be recognized and, in case such arecognition occurs, the governing V can pass to state1).In S1 the first item <N, $2,0,  > is produced bytire scanner: it is the result of advancing on the inputstring according to the item <N, 0, 0, > in SO withan input noun "I" (the entry in the parse table PTN<N, 0> x N contains <scan,S2>).
The next item, <V,1,0,_> is produced by applying the completer to theitem in S 0 <V, 1,0, N>.$2 contains the item <V, $2,0, _>, obtained by thescanner, that advances on the verb "saw".
The otherfour items are the result of a double application of thepredictor, which, in a sense, builds a "chain" thatconsists of a noun governed by the root verb and of adeterminer governed by that noun; this is the onlyway, according to the grammar, to accomodate anincoming determiner when a verb is under analysis.The subsequent steps can easily be traced by thereader.
The input sentence is accepted because of theappearance in the last set of the item <V, $3,0,  > ,encoding that a structure headed by a verb (i.e.
a rootcategory), ending in a final state ($3), and coveringall the words from the beginning of the sentence hasbeen successfully recognized.The space complexity of the recognizer is O(IGIn2).
Each item is a quadruple <Cat, State, Position,Depcat>: Depcat is a constant of the grammar; thepairs of Cat and State are bounded by O(IGI);S O \[1\] <N, O, 2 _> <N, 1, 2, _> <V, $3, O, P> <N, 1, 7, ><V,O,O,_> <N, 1,2, D> <A,O, 4,_> <N,$2,2,P> Slola\] S12<V, 1, O, N> <D, O, 2, _> <N, 1, 2, A> S 9 \[with\] <P, 1, 9, _> <N, $2, 10, _><N, O, O, _> $7 ltlw\] <N, $2, 7, > <N, O, 10, > <P, $2, 9, >$3 \[tall\] S 5 #nan\] <P, 1, 6, _> <P, $2,6, > <P, $2,9, N> <N, $2,7, >S 1 \[saw\] <D,$1,2,_> <A,$1,4, > <N,O, 7, > <N,$2,2,_> <D,O, 10,_> <N,$2,2, ><N,$2,0, > <N, 1,2,_> <N, 1,2, > <P,$2,6, N> <V,$3,0,_> <N,I, IO, D> <V,$3,0,_><V, 1, O, > <A, O, 3, _> <D, O, 7, > <P, O, 9, _> <P, $2,6, _><N, 1, 2, A> $6 lin\] <N, 1, 7, D> <N, $2,7, P> S11 \[telescope\]$2 \[a\] <N, $2,2, > <N, $2,2, P> <D, $1,10, _><V, $2, O, _> S 4 \[old\] <V, $3, 2, _> $8 \[park\] <V, $3, O, P> <N, 1, 10, _><V, $3, O, N> <A, $1,3, _> <P, O, 6, > <D, $1,7, >Figure 6.
Sets of items generated in recognizing "I saw a tall old man in the park with a telescope".V27Position is bounded by O(n).
The number of suchquadruples in a set of items is bounded by O(IGI n)and there are n sets of items.The time complexity of the recognizer is O(IGI 2n3).
The phases canner and predictor execute atmost O(IGI) actions per item; the items are at mostO(IGI n 2) and the cost of these two phases for thewhole algorithm is O(IGl2n2).
The phase completerexecutes at most one action per pair of items.
Thevariables of such a pair of items are the two states(O(IGI2)), the two sets that contain them (O(n2)), andthe two positions (O(n2)).
But the pairs consideredare not all the possible pairs: one of the sets has theindex which is the same of one of the positions, andthe complexity of the completer is O(IGI 2 n3).
Thephase completer prevails on the other two phases andthe total complexity of the algorithm is O(IGI 2 n3).Even if the O-analysis is equivalent to Earley's, thephase of precompilation i to the parse tables allowsto save a lot of computation time needed by thepredictor.
All the possible predictions areprecomputed in the transition to a new state.
Asimilar device is presented in (Schabes 1990) forcontext-free grammars.4 ConclusionThe paper has described a recognition algorithm fordependency grammar.
The dependency formalism istranslated into parse tables, that determine theconditions of applicability of tile parser actions.
Therecognizer is an improved Earley-type algorithm,whose performances are comparable to the bestrecognizers for the context-free grammars, theformalism which is equivalent to the dependencyformalism described in this paper.
The algorithm hasbeen implemented in Common Lisp and runs underthe Unix operating system.
The next step in ourresearch will be to relax the condition of projectivityin order to improve the expressive power and to dealwith phenomena that go beyond the context-freepower.
These changes imply the restructuring ofsome parts of the recognizer, with a plausibleincrement of the complexity.ReferencesChomsky N., Three models for the description oflanguage, IRE Transactions on Information Theory,IT-2, 1956, 113-124.Covington M. A., Parsing DiscontinuousConstituents in Dependency Grammar,Computational Linguistics 16, 1990, 234-236.Covington M. A., An Empirically MotivatedReinterpretation f Dependency Grammar, Res.
Rep.AI-1994-01, Univ.
of Georgia (also on CompLingServer), 1994.Earley J., An Efficient Context-free ParsingAlgorithm.
Comm.
of the ACM 13,1970, 94-102.Fraser N.M., Parsing and Dependency Grammar,UCL Working Papers in Linguistics, 1989, 296-319.Fraser N.M., Hudson R. A., Inheritance in WordGrammar, Computational Linguistics 18, 1992, 133-158.Gaifman H., Dependency Systems and PhraseStructure Systems, Information and Control 8, 1965,304-337.Gazdar G., Klein E., Pullum G., Sag I.,Generalized Phrase Structure Grammars, BasilBlackwell, Oxford, 1985.Graham S. L., Harrison M. A., Ruzzo W. L., Animproved Context-Free Recognizer, ACM Trans.
onProgramming Languages and Systems 2, 1980, 415-462.Hahn U., Schacht S., Broker N., Concurrent,Object-Oriented Natural Language Parsing: TheParseTalk Model, CLIF Report 9/94, Albert-Ludwigs-Universitat, Freiburg, Germany.Hudson R., English Word Grammar, BasilBlackwell, Oxford, 1990.Jackendoff R., X-bar Syntax: A Study of PhraseStructure, MIT Press, 1977.Jacobs P.S., Rau L. F., Innovations in TextInterpretation, Artificial Intelligence Journal 63/1-2,1993, 143-191.Joshi A.K., Vijay-Shanker K., Weir D., TheConvergence of Mildly Context-sensitivegrammatical formalisms, in Sells P., Shieber S.,Wasow T.
(eds.
), Foundational Issues in NaturalLanguage Processing, MIT Press, 1991.Kaplan R., Bresnan J., Lexical-FunctionalGrammar: A Formal System for GrammaticalRepresentation, i  Bresnan J.
(ed.
), The mentalrepresentation fgrammatical relations, MIT Press,1982.Kay M., Functional Grammar, Proc.
5th Meetingof the Berkeley Linguistic Society, 1979, 142-158.Kwon H., Yoon A., Unification-BasedDependency Parsing of Governor-Final Languages,Proc.
IWPT 91, 1991, 182-192.Lai B.Y.T., Huang C., Dependency Grammar andthe Parsing of Chinese Sentences, UnpublishedManuscript on CompLing Server, 1995.Mel'cuk I., Dependency Syntax: Theory andPractice, SUNY Press, Albany, 1988.Perlmutter 1983, Studies in Relational Grammar 1,Univ.
of Chicago Press, Chicago, 1983.Pollard C.J., Generalized Phrase StructureGrammars, Head Grammars, and Natural Language,Ph.D.
Thesis, Stanford Univ., 1984.Pollard CJ., Sag I., An Information Based Syntaxand Semantics, vo1.1, Fundamentals, CSLI LectureNote 13.
CSLI, Stanford, 1987.Rambow O., Joshi A., A Formal Look atDependency Grammars and Phrase-StructureGrammars, with Special Consideration of Word-Order Phenomena, Proc.
of the Int.
Workshop on theMeaning-Text Theory, Darmstadt, 1992.Schabes Y., Polynomial Time and Space Shift-Reduce Parsing of Arbitrary Context-Free Grammars,Proc.
ACL 90, Pittsburgh (PA), 1990, 106-113.Schabes Y., Waters R. C., Lexicalized Context-Free Grammars, Proc.
ACL 93, 121-129.Sgall P., Haijcova E., Panevova J., The Meaning ofSentence in its Semantic and Pragmatic Aspects,D.Reidel Publ.
Co., Dordrecht, 1986.Sleator D. D., Temperley D., Parsing English witha Link Grammar, Proc.
oflWPT 93, 1993, .277-291.Tesniere L., Elements de Syntaxe Structurale,Kliensieck, Paris, 1959.Tomita M., Efficient Parsing for NaturalLanguage, Kluwer Acad.
Publ., 1985.728
