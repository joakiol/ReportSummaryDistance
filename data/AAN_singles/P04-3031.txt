NLTK: The Natural Language ToolkitSteven BirdDepartment of Computer Scienceand Software EngineeringUniversity of MelbourneVictoria 3010, Australiasb@csse.unimelb.edu.auEdward LoperDepartment of Computerand Information ScienceUniversity of PennsylvaniaPhiladelphia PA 19104-6389, USAedloper@gradient.cis.upenn.eduAbstractThe Natural Language Toolkit is a suite of program mod-ules, data sets, tutorials and exercises, covering symbolicand statistical natural language processing.
NLTK iswritten in Python and distributed under the GPL opensource license.
Over the past three years, NLTK hasbecome popular in teaching and research.
We describethe toolkit and report on its current state of development.1 IntroductionThe Natural Language Toolkit (NLTK) wasdeveloped in conjunction with a computationallinguistics course at the University of Pennsylvaniain 2001 (Loper and Bird, 2002).
It was designedwith three pedagogical applications in mind:assignments, demonstrations, and projects.Assignments.
NLTK supports assignments ofvarying difficulty and scope.
In the simplest assign-ments, students experiment with existing compo-nents to perform a wide variety of NLP tasks.
Asstudents become more familiar with the toolkit, theycan be asked to modify existing components, orto create complete systems out of existing compo-nents.Demonstrations.
NLTK?s interactive graphicaldemonstrations have proven to be very usefulfor students learning NLP concepts.
Thedemonstrations give a step-by-step executionof important algorithms, displaying the currentstate of key data structures.
A screenshot of thechart parsing demonstration is shown in Figure 1.Projects.
NLTK provides students with a flexibleframework for advanced projects.
Typical projectsmight involve implementing a new algorithm,developing a new component, or implementing anew task.We chose Python because it has a shallow learn-ing curve, its syntax and semantics are transparent,and it has good string-handling functionality.
Asan interpreted language, Python facilitates interac-tive exploration.
As an object-oriented language,Python permits data and methods to be encapsulatedand re-used easily.
Python comes with an extensivestandard library, including tools for graphical pro-gramming and numerical processing.
The recentlyadded generator syntax makes it easy to create inter-active implementations of algorithms (Loper, 2004;Rossum, 2003a; Rossum, 2003b).Figure 1: Interactive Chart Parsing Demonstration2 DesignNLTK is implemented as a large collection ofminimally interdependent modules, organizedinto a shallow hierarchy.
A set of core modulesdefines basic data types that are used throughout thetoolkit.
The remaining modules are task modules,each devoted to an individual natural languageprocessing task.
For example, the nltk.parsermodule encompasses to the task of parsing, orderiving the syntactic structure of a sentence;and the nltk.tokenizer module is devoted tothe task of tokenizing, or dividing a text into itsconstituent parts.2.1 Tokens and other core data typesTo maximize interoperability between modules, weuse a single class to encode information about nat-ural language texts ?
the Token class.
Each Tokeninstance represents a unit of text such as a word,sentence, or document, and is defined by a (partial)mapping from property names to values.
For exam-ple, the TEXT property is used to encode a token?stext content:1>>> from nltk.token import *>>> Token(TEXT="Hello World!
")<Hello World!>The TAG property is used to encode a token?s part-of-speech tag:>>> Token(TEXT="python", TAG="NN")<python/NN>The SUBTOKENS property is used to store a tok-enized text:>>> from nltk.tokenizer import *>>> tok = Token(TEXT="Hello World!
")>>> WhitespaceTokenizer().tokenize(tok)>>> print tok[?SUBTOKENS?
])[<Hello>, <World!>]In a similar fashion, other language processing taskssuch as word-sense disambiguation, chunking andparsing all add properties to the Token data struc-ture.In general, language processing tasks are formu-lated as annotations and transformations involvingTokens.
In particular, each processing task takesa token and extends it to include new information.These modifications are typically monotonic; newinformation is added but existing information is notdeleted or modified.
Thus, tokens serve as a black-board, where information about a piece of text iscollated.
This architecture contrasts with the moretypical pipeline architecture where each processingtask?s output discards its input information.
Wechose the blackboard approach over the pipelineapproach because it allows more flexibility whencombining tasks into a single system.In addition to the Token class and its derivatives,NLTK defines a variety of other data types.
Forinstance, the probability module defines classesfor probability distributions and statistical smooth-ing techniques; and the cfg module defines classesfor encoding context free grammars and probabilis-tic context free grammars.1Some code samples are specific to NLTK version 1.4.2.2 The corpus moduleMany language processing tasks must be developedand tested using annotated data sets or corpora.Several such corpora are distributed with NLTK,as listed in Table 1.
The corpus module definesclasses for reading and processing many of thesecorpora.
The following code fragment illustrateshow the Brown Corpus is accessed.>>> from nltk.corpus import brown>>> brown.groups()[?skill and hobbies?, ?popular lore?,?humor?, ?fiction: mystery?, ...]>>> brown.items(?humor?
)(?cr01?, ?cr02?, ?cr03?, ?cr04?, ?cr05?,?cr06?, ?cr07?, ?cr08?, ?cr09?
)>>> brown.tokenize(?cr01?
)<[<It/pps>, <was/bedz>, <among/in>,<these/dts>, <that/cs>, <Hinkle/np>,<identified/vbd>, <a/at>, ...]>A selection of 5% of the Penn Treebank corpus isincluded with NLTK, and it is accessed as follows:>>> from nltk.corpus import treebank>>> treebank.groups()(?raw?, ?tagged?, ?parsed?, ?merged?
)>>> treebank.items(?parsed?
)[?wsj_0001.prd?, ?wsj_0002.prd?, ...]>>> item = ?parsed/wsj_0001.prd?>>> sentences = treebank.tokenize(item)>>> for sent in sentences[?SUBTOKENS?]:...
print sent.pp() # pretty-print(S:(NP-SBJ:(NP: <Pierre> <Vinken>)(ADJP:(NP: <61> <years>)<old>)...2.3 Processing modulesEach language processing algorithm is implementedas a class.
For example, the ChartParser andRecursiveDescentParser classes each definea single algorithm for parsing a text.
We imple-ment language processing algorithms using classesinstead of functions for three reasons.
First, allalgorithm-specific options can be passed to the con-structor, allowing a consistent interface for applyingthe algorithms.
Second, a number of algorithmsneed to have their state initialized before they canbe used.
For example, the NthOrderTagger classCorpus Contents and Wordcount Example Application20 Newsgroups (selection) 3 newsgroups, 4000 posts, 780kw text classificationBrown Corpus 15 genres, 1.15Mw, tagged training & testing taggers, text classificationCoNLL 2000 Chunking Data 270kw, tagged and chunked training & testing chunk parsersProject Gutenberg (selection) 14 texts, 1.7Mw text classification, language modellingNIST 1999 IEER (selection) 63kw, named-entity markup training & testing named-entity recognizersLevin Verb Index 3k verbs with Levin classes parser developmentNames Corpus 8k male & female names text classificationPP Attachment Corpus 28k prepositional phrases, tagged parser developmentRoget?s Thesaurus 200kw, formatted text word-sense disambiguationSEMCOR 880kw, POS & sense tagged word-sense disambiguationSENSEVAL 2 Corpus 600kw, POS & sense tagged word-sense disambiguationStopwords Corpus 2,400 stopwords for 11 lgs text retrievalPenn Treebank (sample) 40kw, tagged & parsed parser developmentWordnet 1.7 180kw in a semantic network WSD, NL understandingWordlist Corpus 960kw and 20k affixes for 8 lgs spell checkingTable 1: Corpora and Corpus Samples Distributed with NLTKmust be initialized by training on a tagged corpusbefore it can be used.
Third, subclassing can be usedto create specialized versions of a given algorithm.Each processing module defines an interfacefor its task.
Interface classes are distinguished bynaming them with a trailing capital ?I,?
such asParserI.
Each interface defines a single actionmethod which performs the task defined by theinterface.
For example, the ParserI interfacedefines the parse method and the Tokenizerinterface defines the tokenize method.
Whenappropriate, an interface defines extended actionmethods, which provide variations on the basicaction method.
For example, the ParserI interfacedefines the parse n method which finds at most nparses for a given sentence; and the TokenizerIinterface defines the xtokenize method, whichoutputs an iterator over subtokens instead of a listof subtokens.NLTK includes the following modules:cfg, corpus, draw (cfg, chart, corpus,featurestruct, fsa, graph, plot, rdparser,srparser, tree), eval, featurestruct,parser (chart, chunk, probabilistic),probability, sense, set, stemmer (porter),tagger, test, token, tokenizer, tree, andutil.
Please see the online documentation fordetails.2.4 DocumentationThree different types of documentation are avail-able.
Tutorials explain how to use the toolkit, withdetailed worked examples.
The API documentationdescribes every module, interface, class, method,function, and variable in the toolkit.
Technicalreports explain and justify the toolkit?s design andimplementation.
All are available from http://nltk.sf.net/docs.html.3 Installing NLTKNLTK is available from nltk.sf.net, and ispackaged for easy installation under Unix, MacOS X and Windows.
The full distribution consistsof four packages: the Python source code (nltk);the corpora (nltk-data); the documentation(nltk-docs); and third-party contributions(nltk-contrib).
Before installing NLTK, it isnecessary to install Python version 2.3 or later,available from www.python.org.
Full installationinstructions and a quick start guide are availablefrom the NLTK homepage.As soon as NLTK is installed, users can run thedemonstrations.
On Windows, the demonstrationscan be run by double-clicking on their Pythonsource files.
Alternatively, from the Pythoninterpreter, this can be done as follows:>>> import nltk.draw.rdparser>>> nltk.draw.rdparser.demo()>>> nltk.draw.srparser.demo()>>> nltk.draw.chart.demo()4 Using and contributing to NLTKNLTK has been used at the University of Pennsylva-nia since 2001, and has subsequently been adoptedby several NLP courses at other universities, includ-ing those listed in Table 2.Third party contributions to NLTK include:Brill tagger (Chris Maloof), hidden Markov modeltagger (Trevor Cohn, Phil Blunsom), GPSG-stylefeature-based grammar and parser (Rob Speer, BobBerwick), finite-state morphological analyzer (Carlde Marcken, Beracah Yankama, Bob Berwick),decision list and decision tree classifiers (TrevorCohn), and Discourse Representation Theoryimplementation (Edward Ivanovic).NLTK is an open source project, and we wel-come any contributions.
There are several waysto contribute: users can report bugs, suggest fea-tures, or contribute patches on Sourceforge; userscan participate in discussions on the NLTK-Develmailing list2 or in the NLTK public forums; andusers can submit their own NLTK-based projectsfor inclusion in the nltk contrib directory.
Newcode modules that are relevant, substantial, orig-inal and well-documented will be considered forinclusion in NLTK proper.
All source code is dis-tributed under the GNU General Public License, andall documentation is distributed under a CreativeCommons non-commercial license.
Thus, poten-tial contributors can be confident that their workwill remain freely available to all.
Further infor-mation about contributing to NLTK is available athttp://nltk.sf.net/contrib.html.5 ConclusionNLTK is a broad-coverage natural language toolkitthat provides a simple, extensible, uniform frame-work for assignments, demonstrations and projects.It is thoroughly documented, easy to learn, and sim-ple to use.
NLTK is now widely used in researchand teaching.
Readers who would like to receiveoccasional announcements about NLTK are encour-aged to sign up for the low-volume, moderated mail-ing list NLTK-Announce.36 AcknowledgementsWe are indebted to our students and colleagues forfeedback on the toolkit, and to many contributorslisted on the NLTK website.2http://lists.sourceforge.net/lists/listinfo/nltk-devel3http://lists.sourceforge.net/lists/listinfo/nltk-announceGraz University of Technology, AustriaInformation Search and RetrievalMacquarie University, AustraliaIntelligent Text ProcessingMassachusetts Institute of Technology, USANatural Language ProcessingNational Autonomous University of Mexico, MexicoIntroduction to Natural Language Processingin PythonOhio State University, USAStatistical Natural Language ProcessingUniversity of Amsterdam, NetherlandsLanguage Processing and Information AccessUniversity of Colorado, USANatural Language ProcessingUniversity of Edinburgh, UKIntroduction to Computational LinguisticsUniversity of Magdeburg, GermanyNatural Language SystemsUniversity of Malta, MaltaNatural Language AlgorithmsUniversity of Melbourne, AustraliaHuman Language TechnologyUniversity of Pennsylvania, USAIntroduction to Computational LinguisticsUniversity of Pittsburgh, USAArtificial Intelligence Application DevelopmentSimon Fraser University, CanadaComputational LinguisticsTable 2: University Courses using NLTKReferencesEdward Loper and Steven Bird.
2002.
NLTK:The Natural Language Toolkit.
In Proceedingsof the ACL Workshop on Effective Tools andMethodologies for Teaching Natural LanguageProcessing and Computational Linguistics, pages62?69.
Somerset, NJ: Association for Computa-tional Linguistics.
http://arXiv.org/abs/cs/0205028.Edward Loper.
2004.
NLTK: Building a pedagogi-cal toolkit in Python.
In PyCon DC 2004.
PythonSoftware Foundation.
http://www.python.org/pycon/dc2004/papers/.Guido Van Rossum.
2003a.
An Introduction toPython.
Network Theory Ltd.Guido Van Rossum.
2003b.
The Python LanguageReference.
Network Theory Ltd.
