Discovering the Lexical Features of a LanguageEric Brill *Department of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104emaih brill@unagi.cis.upenn.edu1 In t roduct ionThis paper examines the possibility of automaticallydiscovering the lexieal features of a language.
Thereis strong evidence that the set of possible lexical fea-tures which can be used in a language is unbounded,and thus not innate.
Lakoff \[Lakoff 87\] describesa language in which the feature -I-woman-or-fire-or-dangerons-thing exists.
This feature is based uponancient folklore of the society in which it is used.
Ifthe set of possible lexieal features is indeed unbounded,then it cannot be part of the innate Universal Gram-mar and must be learned.
Even if the set is not un-bounded, the child is still left with the challenging taskof determining which features are used in her language.If a child does not know a priori what lexical fea-tures are used in her language, there are two sourcesfor acquiring this information: semantic and syntacticcues.
A learner using semantic cues could recognizethat words often refer to objects, actions, and proper-ties, and from this deduce the lexical features: noun,verb and adjective.
Pinker \[Pinker 89\] proposes thata combination of semantic ues and innate semanticprimitives could account for the acquisition of verb fea-tures.
He believes that the child can discover semanticproperties of a verb by noticing the types of actionstypically taking place when the verb is uttered.
Oncethese properties are known, says Pinker, they can beused to reliably predict the distributional behavior ofthe verb.
However, Gleitman \[Gleitman 90\] presentsevidence that semantic cues axe not sufficient for achild to acquire verb features and believes that theuse of this semantic information in conjunction withinformation about the subcategorization properties ofthe verb may be sufficient for learning verb features.This paper takes Gleitman's uggestion to the ex-treme, in hope of determining whether syntactic uesmay not just aid in feature discovery, but may be allthat is necessary.
We present evidence for the suffi-ciency of a strictly syntax-based model for discovering*The author would like to thank Mitch Marcus for valuablehelp.
This work was supported by AFOSR jointly under grantNo.
AFOSR-90-0066, and by ARO grant No.
DAAL 03-89-C0031 PRI.the lexical features of a language.
The work is basedupon the hypothesis that whenever two words are se-mantically dissimilar, this difference will manifest it-self in the syntax viaplaying out the notion51\]).
Most, if not all,For instance, there islexical distribution (in a sense,of distributional analysis \[Harrisfeatures have a semantic basis.a clear semantic difference be-tween most count and mass nouns.
But while meaningspecifies the core of a word class, it does not specifyprecisely what can and cannot be a member of a class.For instance, furniture is a mass noun in English, butis a count noun in French.
While the meaning of fur-niture cannot be sufficient for determining whether itis a count or mass noun, the distribution of the wordCall.Described below is a fully implemented programwhich takes a corpus of text as input and outputs afairly accurate word class list for the language in ques-tion.
Each word class corresponds to a lexical feature.The program runs in O(n 3) time and O(n 2) space,where n is the number of words in the lexicon.2 D iscover ing  Lex ica l  FeaturesThe program is based upon a Markov model.
AMarkov model is defined as:1.
A set of states2.
Initial state probabilities init(x)--3.
Transition probabilities trans(x,~)An important property of Markov models is that theyhave no memory other than that stored in the currentstate.
In other words, where X(t) is the value given bythe model at time t,P , (X( t )  = ~, I x ( t  - 1) = ~,_ ,  .
.
.
x (o )  = ~o) =Pr(X(t )  = ~tt \[ X ( t  -- 1) = at- l )In the model we use, there is a unique state for eachword in the lexicon.
We are not concerned with initialstate probabilities.
Transition probabilities representthe probability that word b will follow a and are esti-mated by examining a large corpus of text.
To estimatethe transition probability from state a to state b:3391.
Count the number of times b follows a in the corpus.2.
Divide this value by the number of times a occurs inthe corpus.Such a model is clearly insufficient for expressingthe grammar of a natural language.
However, thereis a great deal of information encoded in such a modelabout the distributional behavior of words with respectto a very local context, namely the context of imme-diately adjacent words.
For a particular word, thisinformation is captured in the set of transitions andtransition probabilities going into and out of the staterepresenting the word in the Markov model.Once the transition probabilities of the model havebeen estimated, it is possible to discover word classes.If  two states are sufficiently similar with respect o thetransitions into and out of them, then it is assumedthat the states are equivalent.
The set of all suffi-ciently similar states forms a word class.
By varyingthe level considered to be sufficiently similar, differentlevels of word classes can be discovered.
For instance,when only highly similar states are considered equiva-lent, one might expect animate nouns to form a class.When the similarity requirement is relaxed, this classmay expand into the class of all nouns.
Once wordclasses are found, lexical features can be extracted byassuming that there is a feature of the language whichaccounts for each word class.
Below is an example ac-tually generated by the program:With very strict state similarity requirements, HE andSHE form a class.
As the similarity requirement is re-laxed, the class grows to include I, forming the classof singular nominative pronouns.
Upon further relax-ation, THEY and WE form a class.
Next, (HE, SHE,I) and (THEY, WE) collapse into a single class, theclass of nominative pronouns.
YOU and IT collapseinto the class of pronouns which are both nominativeand accusative.
Note that next, YOU and IT mergewith the class of nominative pronouns.
This is becausethe program currently deals with bimodals by eventu-ally assigning them to the class whose characteristicsthey exhibit most strongly.
For another example ofthis, see HER below.3 Resu l ts  and Future Direc-t ionsThis algorithm was run on a Markov model trainedon the Brown Corpus, a corpus of approximately onemillion words \[Francis 82\].
The results, although pre-liminary, are very encouraging.
These are a few of theword classes found by the program:?
CAME WENT?
THEM ME HIM US?
HER HIS?
FOR ON BY IN WITH FROM AT?
THEIR MY OUR YOUR ITS?
ANY MANY EACH SOME?
MAY WILL COULD MIGHT WOULD CANSHOULD MUST?
FIRST LAST?
LITTLE MUCH?
MEN PEOPLE MANThis work is still in progress, and a number of dif-ferent directions are being pursued.
We are currentlyattempting to automatically acquire the suffixes of alanguage, and then trying to class words based uponhow they distribute with respect o suffixes.One problem with this work is that it is difficult tojudge results.
One can eye the results and see thatthe lexical features found seem to be correct, but howcan we judge that the features are indeed the correctones?
How can one set of hypothesized features mean-ingfully be compared to another set?
We are currentlyworking on an information-theoretic metric, similar tothat proposed by Jelinek \[Jelinek 90\] for scoring prob-abilistic context-free grammars, to score the quality ofhypothesized lexical feature sets.References\[Francis 82\] Francis, W. and H. Kucera.
(1982) Frequency Anal-ysis o.f English Usage: Le~c.icon and Grammar.Houghton Mifflin Co.\[G|eitman 90\] G|eitman, Lila.
(1990) "The Structural Sourcesof Verb Meanings."
Language Acquisition, Voltmae1, pp.
3-55.\[Harris 51\] Harris, Zeli 8.
(1951) Structural Lingulstics.
Chicago: Universityof Chicago Press.\[Jelinek 90\] Jellnek, F., J.D.
Lafferty & R.L.
Mercer.
(1990)"Basic Methods of Probahilistic Context FreeGrvannmrs."
I.B.M.
Technical Report, RC 16374.\[Lakoff87\] Lakoff, G. (1987) Women, Fire and DangerousThings: What Categories Reveal About the Mind.Chicago: University of Chicago Press.\[Pinker 89\] Pinker, S. Learnability and Cognition.
Cambridge:MIT Press.340
