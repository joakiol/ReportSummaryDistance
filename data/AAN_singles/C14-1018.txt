Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 172?182, Dublin, Ireland, August 23-29 2014.Building Large-Scale Twitter-Specific Sentiment Lexicon :A Representation Learning ApproachDuyu Tang\?, Furu Wei?, Bing Qin\?, Ming Zhou?, Ting Liu\\Research Center for Social Computing and Information Retrieval,Harbin Institute of Technology, China?Microsoft Research, Beijing, China{dytang, qinb, tliu}@ir.hit.edu.cn{fuwei, mingzhou}@microsoft.comAbstractIn this paper, we propose to build large-scale sentiment lexicon from Twitter with a representationlearning approach.
We cast sentiment lexicon learning as a phrase-level sentiment classificationtask.
The challenges are developing effective feature representation of phrases and obtainingtraining data with minor manual annotations for building the sentiment classifier.
Specifical-ly, we develop a dedicated neural architecture and integrate the sentiment information of tex-t (e.g.
sentences or tweets) into its hybrid loss function for learning sentiment-specific phraseembedding (SSPE).
The neural network is trained from massive tweets collected with positiveand negative emoticons, without any manual annotation.
Furthermore, we introduce the UrbanDictionary to expand a small number of sentiment seeds to obtain more training data for buildingthe phrase-level sentiment classifier.
We evaluate our sentiment lexicon (TS-Lex) by applyingit in a supervised learning framework for Twitter sentiment classification.
Experiment resultson the benchmark dataset of SemEval 2013 show that, TS-Lex yields better performance thanpreviously introduced sentiment lexicons.1 IntroductionA sentiment lexicon is a list of words and phrases, such as ?excellent?, ?awful?
and ?not bad?, eachof which is assigned with a positive or negative score reflecting its sentiment polarity and strength.Sentiment lexicon is crucial for sentiment analysis (or opining mining) as it provides rich sentiment in-formation and forms the foundation of many sentiment analysis systems (Pang and Lee, 2008; Liu, 2012;Feldman, 2013).
Existing sentiment lexicon learning algorithms mostly utilize propagation methods toestimate the sentiment score of each phrase.
These methods typically employ parsing results, syntac-tic contexts or linguistic information from thesaurus (e.g.
WordNet) to calculate the similarity betweenphrases.
For example, Baccianella et al.
(2010) use the glosses information from WordNet; Velikovich etal.
(2010) represent each phrase with its context words from the web documents; Qiu et al.
(2011) exploitthe dependency relations between sentiment words and aspect words.
However, parsing information andthe linguistic information from WordNet are not suitable for constructing large-scale sentiment lexiconfrom Twitter.
The reason lies in that WordNet cannot well cover the colloquial expressions in tweets, andit is hard to have reliable tweet parsers due to the informal language style.In this paper, we propose to build large-scale sentiment lexicon from Twitter with a representationlearning approach, as illustrated in Figure 1.
We cast sentiment lexicon learning as a phrase-level classi-fication task.
Our method contains two part: (1) a representation learning algorithm to effectively learnthe continuous representation of phrases, which are used as features for phrase-level sentiment classifica-tion, (2) a seed expansion algorithm that enlarge a small list of sentiment seeds to collect training data forbuilding the phrase-level classifier.
Specifically, we learn sentiment-specific phrase embedding (SSPE),which is a low-dimensional, dense and real-valued vector, by encoding the sentiment information and?This work was partly done when the first author was visiting Microsoft Research.
?Corresponding author.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/172SentimentClassifierSentimentLexiconPhrase EmbeddingNEG: goon looserSentiment SeedsTweets with EmoticonsSoooo nice~ :)It?s horrible :(SeedExpansionRepresentationLearningPOS: good :)NEG: poor :(NEU: when heTraining DataPOS: wanted faveNEU: again place[1.31,0.97] good:[0.99,1.17] coool:[-0.81,-0.7] bad:[-0.8,-0.72] mess:LearningAlgorithmFigure 1: The representation learning approach for building Twitter-specific sentiment lexicon.syntactic contexts into the continuous representation of phrases1.
As a result, the nearest neighbors in theembedding space of SSPE are favored to have similar semantic usage as well as the same sentiment po-larity.
To this end, we extend the existing phrase embedding learning algorithm (Mikolov et al., 2013b),and develop a dedicated neural architecture with hybrid loss function to incorporate the supervision fromsentiment polarity of text (e.g.
tweets).
We learn SSPE from tweets, leveraging massive tweets con-taining positive and negative emoticons as training set without any manual annotation.
To obtain moretraining data for building the phrase-level sentiment classifier, we exploit the similar words from UrbanDictionary2, which is a crowd-sourcing resource, to expand a small list of sentiment seeds.
Finally, weutilize the classifier to predict the sentiment score of each phrase in the vocabulary of SSPE, resulting inthe sentiment lexicon.We evaluate the effectiveness of our sentiment lexicon (TS-Lex) by applying it in a supervised learn-ing framework (Pang et al., 2002) for Twitter sentiment classification.
Experiment results on the bench-mark dataset of SemEval 2013 show that, TS-Lex yields better performance than previously introducedlexicons, including two large-scale Twitter-specific sentiment lexicons, and further improves the top-performed system in SemEval 2013 by feature combination.
The quality of SSPE is also evaluated byregarding SSPE as the feature for sentiment classification of the items in existing sentiment lexicons (Huand Liu, 2004; Wilson et al., 2005).
Experiment results show that SSPE outperforms existing embeddinglearning algorithms.
The main contributions of this work are as follows:?
To our best knowledge, this is the first work that leverages the continuous representation of phrasesfor building large-scale sentiment lexicon from Twitter;?
We propose a tailored neural architecture for learning the sentiment-specific phrase embedding frommassive tweets selected with positive and negative emoticons;?
We report the results that our lexicon outperforms existing sentiment lexicons by applying them ina supervised learning framework for Twitter sentiment classification.2 Related WorkIn this section, we give a brief review about building sentiment lexicon and learning continuous repre-sentation of words and phrases.2.1 Sentiment Lexicon LearningSentiment lexicon is a fundamental component for sentiment analysis, which can be built manually (Dasand Chen, 2007), through heuristics (Kim and Hovy, 2004) or using machine learning algorithms (Tur-ney, 2002; Li et al., 2012; Xu et al., 2013).
Existing studies typically employ machine learning methods1Word/unigram is also regarded as phrase in this paper.2http://www.urbandictionary.com/173and adopt the propagation method to build sentiment lexicon.
In the first step, a graph is built by re-garding each item (word or phrase) as a node and their similarity as the edge.
Then, graph propagationalgorithms, such as pagerank (Esuli and Sebastiani, 2007), label propagation (Rao and Ravichandran,2009) or random walk (Baccianella et al., 2010), are utilized to iteratively calculate the sentiment scoreof each item.
Under this direction, parsing results, syntactic contexts or linguistic clues in thesaurus aremostly explored to calculate the similarity between items.
Wiebe (2000) utilize the dependency triplesfrom an existing parser (Lin, 1994).
Qiu et al.
(2009; 2011) adopt dependency relations between senti-ment words and aspect words.
Esuli and Sebastiani (2005) exploit the glosses information from Wordnet.Hu and Liu (2004) use the synonym and antonym relations within linguistic resources.
Velikovich et al.
(2010) represent words and phrases with their syntactic contexts within a window size from the webdocuments.
Unlike the dominated propagation based methods, we explore the classification frameworkbased on representation learning for building large-scale sentiment lexicon from Twitter.To construct the Twitter-specific sentiment lexicon, Mohammad et al.
(2013) use pointwise mutualinformation (PMI) between each phrase and hashtag/emoticon seed words, such as #good, #bad, :) and:(.
Chen et al.
(2012) utilize the Urban Dictionary and extract the target-dependent sentiment expres-sions from Twitter.
Unlike Mohammad et al.
(2013) that only capture the relations between phrases andsentiment seeds, we exploit the semantic and sentimental connections between phrases through phraseembedding and propose a representation learning approach to build sentiment lexicon.2.2 Learning Continuous Representation of Word and PhraseContinuous representation of words and phrases are proven effective in many NLP tasks (Turian et al.,2010).
Embedding learning algorithms have been extensively studied in recent years (Bengio et al.,2013), and are dominated by the syntactic context based algorithms (Bengio et al., 2003; Collobert etal., 2011; Dahl et al., 2012; Huang et al., 2012; Mikolov et al., 2013a; Lebret et al., 2013; Sun et al.,2014).
To integrate the sentiment information of text into the word embedding, Maas et al.
(2011) extendthe probabilistic document model (Blei et al., 2003) and predict the sentiment of a sentence with theembedding of each word.
Labutov and Lipson (2013) learn task-specific embedding from an existingembedding and sentences with gold sentiment polarity.
Tang et al.
(2014) propose to learn sentiment-specific word embedding from tweets collected by emoticons for Twitter sentiment classification.
Unlikeprevious trails, we learn sentiment-specific phrase embedding with a tailored neural network.
UnlikeMikolov et al.
(2013b) that only use the syntactic contexts of phrases to learn phrase embedding, weintegrate the sentiment information of text into our method.
It is worth noting that we focus on learningthe continuous representation of words and phrases, which is orthogonal with Socher et al.
(2011; 2013)that learn the compositionality of sentences.3 MethodologyIn this section, we describe our method for building large-scale sentiment lexicon from Twitter within aclassification framework, as illustrated in Figure 1.
We leverage the continuous representation of phrasesas features, without parsers or hand-crafted rules, and automatically obtain the training data by seedexpansion from Urban Dictionary.
After the classifier is built, we employ it to predict the sentimentdistribution of each phrase in the embedding vocabulary, resulting in the sentiment lexicon.
To encodethe sentiment information into the continuous representation of phrases, we extend an existing phraseembedding learning algorithm (Mikolov et al., 2013b) and develop a tailored neural architecture to learnsentiment-specific phrase embedding (SSPE), as described in subsection 3.1.
To automatically obtainmore training data for building the phrase-level sentiment classifier, we use the similar words from UrbanDictionary to expand a small list of sentiment seeds, as described in subsection 3.2.3.1 Sentiment-Specific Phrase EmbeddingMikolov et al.
(2013b) introduce Skip-Gram to learn phrase embedding based on the context words ofphrases, as illustrated in Figure 2(a).Given a phrase wi, Skip-Gram maps it into its continuous representation ei.
Then, Skip-Gram utilizes174eiwi-2 wi-1 wi+1 wi+2wieiwi-2 wi-1 wi+1 wi+2wipolj(a) Skip-Gram (b) Our ModelsejsjFigure 2: The traditional Skip-Gram model and our neural architecture for learning sentiment-specificphrase embedding (SSPE).eito predict the context words of wi, namely wi?2, wi?1, wi+1, wi+2, et al.
Hierarchical softmax (Morinand Bengio, 2005) is leveraged to accelerate the training procedure because the vocabulary size of phrasetable is typically huge.
The objective of Skip-Gram is to maximize the average log probability:fsyntactic=1TT?i=1?
?c?j?c,j 6=0log p(wi+j|ei) (1)where T is the occurrence of each phrase in the corpus, c is the window size, eiis the embedding of thecurrent phrase wi, wi+jis the context words of wi, p(wi+j|ei) is calculated with hierarchical softmax.The basic softmax unit is calculated as softmaxi= exp(zi)/?kexp(zk).
We leave out the detailsof hierarchical softmax (Morin and Bengio, 2005; Mikolov et al., 2013b) due to the page limit.
It isworth noting that, Skip-Gram is capable to learn continuous representation of words and phrases withthe identical model (Mikolov et al., 2013b).To integrate sentiment information into the continuous representation of phrases, we develop a tailoredneural architecture to learn SSPE, as illustrated in Figure 2(b).
Given a triple ?wi, sj, polj?
as input,where wiis a phrase contained in the sentence sjwhose gold sentiment polarity is polj, our trainingobjective is to (1) utilize the embedding of wito predict its context words, and (2) use the sentencerepresentation sejto predict the gold sentiment polarity of sj, namely polj.
We simply average theembedding of phrases contained in a sentence as its continuous representation (Huang et al., 2012).
Theobjective of the sentiment part is to maximize the average of log sentiment probability:fsentiment=1SS?j=1log p(polj|sej) (2)where S is the occurrence of each sentence in the corpus,?kpoljk= 1.
For binary classificationbetween positive and negative, the distribution of [0,1] is for positive and [1,0] is for negative.
Our finaltraining objective is to maximize the linear combination of the syntactic and sentiment parts:f = ?
?
fsyntactic+ (1?
?)
?
fsentiment(3)where ?
weights the two parts.
Accordingly, the nearest neighbors in the embedding space of SSPE arefavored to have similar semantic usage as well as the same sentiment polarity.We train our neural model with stochastic gradient descent and use AdaGrad (Duchi et al., 2011) toupdate the parameters.
We empirically set embedding length as 50, window size as 3 and the learningrate of AdaGrad as 0.1.
Hyper-parameter ?
is tuned on the development set.
To obtain large-scaletraining corpus, we collect tweets from April, 2013 through TwitterAPI.
After filtering the tweets thatare too short (< 5 words) and removing @user and URLs, we collect 10M tweets (5M positive and 5Mnegative) with positive and negative emoticons3, which is are utilized as the training data to train ourneural model.
The vocabulary size is 750,000 after filtering the 1?4 grams through frequency.3We use the emoticons selected by Hu et al.
(2013), namely :) : ) :-) :D =) as positive and :( : ( :-( as negative ones.1753.2 Seed Expansion with Urban DictionaryUrban Dictionary is a web-based dictionary that contains more than seven million definitions until March,20134.
It was intended as a dictionary of slang, cultural words or phrases not typically found in standarddictionaries, but it is now used to define any word or phrase.
For each item in Urban Dictionary, there isa list of similar words contributed by volunteers.
For example, the similar words of ?cooool?
are ?cool?,?awesome?, ?coooool?, et al5and the similar words of ?not bad?
are ?good?, ?ok?
and ?cool?, et al6.These similar words are typically semantically close to and have the same sentiment polarity with thetarget word.
We conduct preliminary statistic on the items of Urban Dictionary from ?a?
to ?z?, andfind that there are total 799,430 items containing similar words and each of them has about 10.27 similarwords on average.We utilize Urban Dictionary to expand little sentiment seeds for collecting training data for buildingthe phrase-level sentiment classifier.
We manually label the top frequent 500 words from the vocabularyof SSPE as positive, negative or neutral.
After removing the ambiguous ones, we obtain 125 positive, 109negative and 140 neutral words, which are regarded as the sentiment seeds7.
Afterwards, we leveragethe similar words from Urban Dictionary to expand the sentiment seeds.
We first build a k-nearestneighbors (KNN) classifier by regarding the sentiment seeds as gold standard.
Then, we employ the KNNclassifier on the items of Urban Dictionary containing similar words, and predict a three-dimensionaldiscrete vector [knnpos, knnneg, knnneu] for each item, reflecting the hits numbers of sentiment seedswith different sentiment polarity in its similar words.
For example, the vector value of ?not bad?
is[10, 0, 0], which means that there are 10 positive seeds, 0 negative seeds and 0 neutral seeds occur inits similar words.
To ensure the quality of the expanded words, we set threshold for each category tocollect the items with high quality as expanded words.
Take the positive category as an example, wekeep an item as positive expanded word if it satisfies knnpos> knnneg+ thresholdposand knnpos>knnneu+ thresholdpossimultaneously.
We empirically set the thresholds of positive, negative andneutral as 6,3,2 respectively by balancing the size of expanded words in three categories.
After seedexpansion, we collect 1,512 positive, 1,345 negative and 962 neutral words, which are used as the trainingdata to build the phrase-level sentiment classifier.
We also tried the propagation methods to expand thesentiment seeds, namely iteratively added the similar words of sentiment seeds from Urban Dictionaryinto the expanded word collection.
However, the quantity of expanded words is less than the KNN-basedresults and the quality is relatively poor.After obtaining the training data and feature representation of phrases, we build the phrase-level clas-sifier with softmax, whose length is two for the positive vs negative case:y(w) = softmax(?
?
ei+ b) (4)where ?
and b are the parameters of classifier, eiis the embedding of the current phrase wi, y(w) is thepredicted sentiment distribution of item wi.
We employ the classifier to predict the sentiment distributionof each phrase in the vocabulary of SSPE, and save the phrases as well as their sentiment probability inthe positive (negative) lexicon if the positive (negative) probability is larger than 0.5.4 ExperimentIn this section, we conduct experiments to evaluate the effectiveness of our sentiment lexicon (TS-Lex)by applying it in the supervised learning framework for Twitter sentiment classification, as given insubsection 4.1.
We also directly evaluate the quality of SSPE as it forms the fundamental component forbuilding sentiment lexicon.
We use SSPE as the feature for sentiment classification of items in existingsentiment lexicons, as described in subsection 4.2.4http://en.wikipedia.org/wiki/Urban Dictionary5http://www.urbandictionary.com/define.php?term=cooool6http://www.urbandictionary.com/define.php?term=not+bad7We will publish the sentiment seeds later.1764.1 Twitter Sentiment ClassificationExperiment Setup and Dataset We conduct experiments on the benchmark Twitter sentiment classi-fication dataset (message-level) from SemEval 2013 (Nakov et al., 2013).
The training and developmentsets were completely released to task participants.
However, we were unable to download all the trainingand development sets because some tweets were deleted or not available due to modified authorizationstatus.
The statistic of the positive and negative tweets in our dataset are given in Table 1(b).
We trainpositive vs negative classifier with LibLinear (Fan et al., 2008) with default settings on the training set,tune parameters -c on the dev set and evaluate on the test set.
The evaluation metric is Macro-F1.
(a) Sentiment LexiconsLexicon Positive Negative TotalHL 2,006 4,780 6,786MPQA 2,301 4,150 6,451NRC-Emotion 2,231 3,324 5,555TS-Lex 178,781 168,845 347,626HashtagLex 216,791 153,869 370,660Sentiment140Lex 480,008 260,158 740,166(b) SemEval 2013 DatasetPositive Negative TotalTrain 2,642 994 3,636Dev 408 219 627Test 1,570 601 2,171Table 1: Statistic of sentiment lexicons and Twitter sentiment classification datasets.Results and Analysis We compare TS-Lex with HL8(Hu and Liu, 2004), MPQA9(Wilson et al.,2005), NRC-Emotion10(Mohammad and Turney, 2012), HashtagLex and Sentiment140Lex11(Moham-mad et al., 2013).
The statistics of TS-Lex and other sentiment lexicons are illustrated in Table 1(a).
HL,MPQA and NRC-Emotion are traditional sentiment lexicons with a relative small lexicon size.
Hashta-gLex and Sentiment140Lex are Twitter-specific sentiment lexicons.
We can find that, TS-Lex is largerthan the traditional sentiment lexicons.We evaluate the effectiveness of TS-Lex by applying it as the features for Twitter sentiment classifica-tion in the supervised learning framework (Pang et al., 2002).
We conduct experiments in two settings,namely only utilizing the lexicon features (Unique) and appending lexicon feature to existing featuresets (Appended).
In the first setting, we design the lexicon features as same as the top-performed Twit-ter sentiment classification system in SemEval201312(Mohammad et al., 2013).
For each sentimentpolarity (positive vs negative), the lexicon features are:?
total count of tokens in the tweet with score greater than 0;?
the sum of the scores for all tokens in the tweet;?
the maximal score;?
the non-zero score of the last token in the tweet;In the second experiment setting, we append the lexicon features to the existing basic feature.
We usethe feature sets of Mohammad et al.
(2013) excluding the lexicon feature as the basic feature, includingbag-of-words, pos-tagging, emoticons, hashtags, elongated words, etc.
Experiment results of the Uniquefeatures and Appended features from different sentiment lexicons on Twitter sentiment classification aregiven in Table 2(a).From Table 2(a), we can find that TS-Lex yields best performance in both Unique and Appendedfeature sets among all sentiment lexicons, including two large-scale Twitter-specific sentiment lexicons.The reason is that the classifier for building TS-Lex utilize (1) the well developed feature representationof phrases (SSPE), which captures the semantic and sentiment connections between phrases, and (2) theenlarged sentiment words through web intelligence as training data.
HashtagLex and Sentiment140Lex8http://www.cs.uic.edu/ liub/FBS/sentiment-analysis.html#lexicon9http://mpqa.cs.pitt.edu/lexicons/subj lexicon/10http://www.saifmohammad.com/WebPages/ResearchInterests.html11We utilize the unigram and bigram lexicons from HashtagLex and Sentiment140Lex.12http://www.saifmohammad.com/WebPages/Abstracts/NRC-SentimentAnalysis.htm177(a)Lexicon Unique AppendedHL 60.49 79.40MPQA 59.15 76.54NRC-Emotion 54.81 76.79HashtagLex 65.30 76.67Sentiment140Lex 72.51 80.68TS-Lex 78.07 82.36(b)Lexicon UniqueSeed 57.92Expand 60.69Lexicon(seed) 74.64TS-Lex 78.07Table 2: Macro-F1 on Twitter sentiment classification with different lexicon features.only utilize the relations between phrases and hashtag/emoticon seeds, yet do not well capture the con-nections between phrases.
In the Unique setting, the performances of the traditional lexicons (HL, MPQAand NRC-Emotion) are lower than large-scale Twitter-specific lexicons (HashtagLex, Sentiment140Lexand our lexicon).
The reason is that, tweets have the informal language style and contain slangs and di-verse multi-word phrases, which are not well covered by the traditional sentiment lexicons with a smallsize.
After incorporating the lexicon feature of TS-Lex into the top-performs system (Mohammad et al.,2013), we further improve the macro-F1 from 84.70% to 85.65%.Effect of Seed Expansion with Urban Dictionary To verify the effectiveness of seed expansionthrough Urban Dictionary, we conduct experiments by applying (1) sentiment seeds (Seed), (2) wordsafter expansion (Expand), (3) sentiment lexicon generated from the classifier only utilizing sentimentseeds as training data (Lexicon(seed)), (4) the final lexicon (TS-Lex) exploiting the expanded words astraining data to build sentiment classifier, to produce lexicon features, and only use them for Twittersentiment classification (Unique).
From Table 2(b), we find that the performance of sentiment seeds andexpanded words are relatively poor due to their low coverage.
Under this scenario, seed expansion yields2.77% improvement (from 57.92% to 60.69%) on macro-F1.
By utilizing the expanded words as trainingdata to build the phrase-level sentiment classifier, TS-Lex obtains 3.43% improvements on Twitter senti-ment classification (from 74.64% to 78.07%), which verifies the effectiveness of seed expansion throughUrban Dictionary.
In addition, we find that only using a small number of sentiment seeds as the trainingdata, we can obtain superior performance (74.64%) than all baseline lexicons.
This indicates that therepresentation learning approach effectively capture the semantic and sentimental connections betweenphrases through SSPE, and leverage them for building the sentiment lexicon.Effect of ?
in SSPE We tune the hyper-parameter ?
of SSPE on the development set of SemEval 2013,and study its influence on the performance of Twitter sentiment classification by applying the generatedlexicon as features.
We utilize the expanded words as training data to train softmax and only utilize thelexicon features (Unique) for Twitter sentiment classification.
Experiment results with different ?
areillustrated in Figure 3(a).From Figure 3(a), we can see that that SSPE performs better when ?
is in the range of [0.1, 0.3], whichis dominated by the sentiment information.
The model with ?
= 1 stands for Skip-Gram model.
Thesharp decline at ?
= 1 indicates the importance of sentiment information in learning sentiment-specificphrase embedding for building sentiment lexicon.Discussion In the experiment, we do not apply TS-Lex into the unsupervised learning framework forTwitter sentiment classification.
The reason is that the lexicon-based unsupervised method typicallyrequire the sentiment lexicon to have high precision, yet our task is to build large-scale lexicon (TS-Lex)with broad coverage.
We leave this as the future work, although we may set higher threshold (e.g.
largerthan 0.5) to increase the precision of TS-Lex and loose the recall.4.2 Evaluation of Different Representation Learning MethodsExperiment Setup and Dataset We conduct sentiment classification of items in two traditional senti-ment lexicons, HL (Hu and Liu, 2004) and MPQA (Wilson et al., 2005), to evaluate the effective of the1780 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 10.7350.740.7450.750.7550.760.7650.770.7750.780.785?Macro?F1TS?Lex(a) SSPE with different ?
on the development set for Twittersentiment classification.0.660.680.70.720.740.760.780.80.820.840.86Macro?F1C&W ReEmbed(C&W) W2V ReEmbed(W2V) MVSA SSPEMPQAHL(b) Sentiment classification of lexicons with different embed-ding learning algrithms.Figure 3: Experiment results with different settings.sentiment-specific phrase embedding (SSPE).
We train the positive vs negative classifier with LibLin-ear (Fan et al., 2008).
The evaluation metric is the macro-F1 of 5-fold cross validation.
The statistics ofHL and MPQA are listed in Table 1(a).Baseline Embedding Learning Algorithms We compare SSPE with the following embedding learn-ing algorithms:(1) C&W.
C&W is one of the most representative embedding learning algorithms (Collobert et al.,2011) for learning word embedding, which has been proven effective in many NLP tasks.
(2) W2V.
Mikolov et al.
(2013a) introduce Word2Vec for learning the continuous vectors for wordsand phrases.
We utilize Skip-Gram as it performs better than CBOW in the experiments.
(3) MVSA.
Maas et al.
(2011) learn word vectors for sentiment analysis with a probabilistic model ofdocuments utilizing the sentiment polarity of documents.
(4) ReEmbed.
Lebret et al.
(2013) learn task-specific embedding from existing embedding and task-specific corpus.
We utilize the training set of Twitter sentiment classification as the labeled corpus tore-embed words.
ReEmbed(C&W) and ReEmbed(W2V) stand for the use of different embedding resultsas the reference word embedding.The embedding results of the baseline algorithms and SSPE are trained with the same dataset andparameter sets.Results and Analysis Experiment results of the baseline embedding learning algorithms and SSPE aregiven in Figure 3(b).
We can see that SSPE yields best performance on both lexicons.
The reason is thatSSPE effectively encode the sentiment information of tweets as well as the syntactic contexts of phrasesfrom massive data into the continuous representation of phrases.
The performances of C&W and W2Vare relatively low because they only utilize the syntactic contexts of items, yet ignore the sentiment in-formation of text, which is crucial for sentiment analysis.
ReEmbed(C&W) and ReEmbed(W2V) achievebetter performance than C&W and W2V because the sentiment information of sentences are incorporatedinto the continuous representation of phrases.
There is a gap between ReEmbed and SSPE because SSPEleverages more sentiment supervision from massive tweets collected by positive and negative emoticons.5 ConclusionIn this paper, we propose building large-scale Twitter-specific sentiment lexicon with a representationlearning approach.
Our method contains two parts: (1) a representation learning algorithm to effectivelylearn the embedding of phrases, which are used as features for classification, (2) a seed expansion al-gorithm that enlarge a small list of sentiment seeds to obtain training data for building the phrase-levelsentiment classifier.
We introduce a tailored neural architecture and integrate the sentiment informationof tweets into its hybrid loss function for learning sentiment-specific phrase embedding (SSPE).
Welearn SSPE from the tweets collected by positive and negative emoticons, without any manual annota-179tion.
To collect more training data for building the phrase-level classifier, we utilize the similar wordsfrom Urban Dictionary to expand a small list of sentiment seeds.
The effectiveness of our sentimentlexicon (TS-Lex) has been verified through applied in the supervised learning framework for Twittersentiment classification.
Experiment results on the benchmark dataset of SemEval 2013 show that, TS-Lex outperforms previously introduced sentiment lexicons and further improves the top-perform systemin SemEval 2013 with feature combination.
In future work, we plan to apply TS-Lex into the unsuper-vised learning framework for Twitter sentiment classification.AcknowledgementsWe thank Nan Yang, Yajuan Duan and Yaming Sun for their great help.
This research was partly sup-ported by National Natural Science Foundation of China (No.61133012, No.61273321, No.61300113).ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebastiani.
2010.
Sentiwordnet 3.0: An enhanced lexical resourcefor sentiment analysis and opinion mining.
In LREC, volume 10, pages 2200?2204.Yoshua Bengio, R?ejean Ducharme, Pascal Vincent, and Christian Janvin.
2003.
A neural probabilistic languagemodel.
Journal of Machine Learning Research, 3:1137?1155.Yoshua Bengio, Aaron Courville, and Pascal Vincent.
2013.
Representation learning: A review and new perspec-tives.
IEEE Trans.
Pattern Analysis and Machine Intelligence.David M Blei, Andrew Y Ng, and Michael I Jordan.
2003.
Latent dirichlet allocation.
the Journal of machineLearning research, 3:993?1022.Lu Chen, Wenbo Wang, Meenakshi Nagarajan, Shaojun Wang, and Amit P Sheth.
2012.
Extracting diversesentiment expressions with target-dependent polarity from twitter.
In ICWSM.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
Journal of Machine Learning Research, 12:2493?2537.George E Dahl, Ryan P Adams, and Hugo Larochelle.
2012.
Training restricted boltzmann machines on wordobservations.
ICML.Sanjiv R Das and Mike Y Chen.
2007.
Yahoo!
for amazon: Sentiment extraction from small talk on the web.Management Science, 53(9):1375?1388.John Duchi, Elad Hazan, and Yoram Singer.
2011.
Adaptive subgradient methods for online learning and stochas-tic optimization.
The Journal of Machine Learning Research, pages 2121?2159.Andrea Esuli and Fabrizio Sebastiani.
2005.
Determining the semantic orientation of terms through gloss classifi-cation.
In Proceedings of the 14th ACM international conference on Information and knowledge management,pages 617?624.
ACM.Andrea Esuli and Fabrizio Sebastiani.
2007.
Pageranking wordnet synsets: An application to opinion mining.
InACL, volume 7, pages 442?431.Rong-En Fan, Kai-Wei Chang, Cho-Jui Hsieh, Xiang-Rui Wang, and Chih-Jen Lin.
2008.
Liblinear: A library forlarge linear classification.
The Journal of Machine Learning Research, 9:1871?1874.Ronen Feldman.
2013.
Techniques and applications for sentiment analysis.
Communications of the ACM,56(4):82?89.Ming Hu and Bing Liu.
2004.
Mining and summarizing customer reviews.
In Proceedings of the tenth ACMSIGKDD Conference on Knowledge Discovery and Data Mining, pages 168?177.Xia Hu, Jiliang Tang, Huiji Gao, and Huan Liu.
2013.
Unsupervised sentiment analysis with emotional signals.In Proceedings of the International World Wide Web Conference, pages 607?618.Eric H Huang, Richard Socher, Christopher D Manning, and Andrew Y Ng.
2012.
Improving word representationsvia global context and multiple word prototypes.
In ACL, pages 873?882.
ACL.180Soo-Min Kim and Eduard Hovy.
2004.
Determining the sentiment of opinions.
In Proceedings of the 20thinternational conference on Computational Linguistics, page 1367.
Association for Computational Linguistics.Igor Labutov and Hod Lipson.
2013.
Re-embedding words.
In Annual Meeting of the Association for Computa-tional Linguistics.R?emi Lebret, Jo?el Legrand, and Ronan Collobert.
2013.
Is deep learning really necessary for word embeddings?NIPS workshop.Fangtao Li, Sinno Jialin Pan, Ou Jin, Qiang Yang, and Xiaoyan Zhu.
2012.
Cross-domain co-extraction ofsentiment and topic lexicons.
In Proceedings of the 50th ACL, pages 410?419.
ACL, July.Dekang Lin.
1994.
Principar: an efficient, broad-coverage, principle-based parser.
In Proceedings of the 15thconference on COLING, pages 482?488.
Association for Computational Linguistics.Bing Liu.
2012.
Sentiment analysis and opinion mining.
Synthesis Lectures on Human Language Technologies,5(1):1?167.Andrew L Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Proceedings of the Annual Meeting of the Association forComputational Linguistics.Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013a.
Efficient estimation of word representationsin vector space.
ICLR.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean.
2013b.
Distributed representations ofwords and phrases and their compositionality.
The Conference on Neural Information Processing Systems.Saif M Mohammad and Peter D Turney.
2012.
Crowdsourcing a word?emotion association lexicon.
Computa-tional Intelligence.Saif M Mohammad, Svetlana Kiritchenko, and Xiaodan Zhu.
2013.
Nrc-canada: Building the state-of-the-art insentiment analysis of tweets.
Proceedings of the International Workshop on Semantic Evaluation.Frederic Morin and Yoshua Bengio.
2005.
Hierarchical probabilistic neural network language model.
In Proceed-ings of the international workshop on artificial intelligence and statistics, pages 246?252.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva, Veselin Stoyanov, Alan Ritter, and Theresa Wilson.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.
In Proceedings of the International Workshop on SemanticEvaluation, volume 13.Bo Pang and Lillian Lee.
2008.
Opinion mining and sentiment analysis.
Foundations and trends in informationretrieval, 2(1-2):1?135.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.
2002.
Thumbs up?
: sentiment classification using machinelearning techniques.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing,pages 79?86.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2009.
Expanding domain sentiment lexicon through doublepropagation.
In IJCAI, volume 9, pages 1199?1204.Guang Qiu, Bing Liu, Jiajun Bu, and Chun Chen.
2011.
Opinion word expansion and target extraction throughdouble propagation.
Computational linguistics, 37(1):9?27.Delip Rao and Deepak Ravichandran.
2009.
Semi-supervised polarity lexicon induction.
In Proceedings of the12th Conference of the European Chapter of the Association for Computational Linguistics, pages 675?682.Association for Computational Linguistics.Richard Socher, J. Pennington, E.H. Huang, A.Y.
Ng, and C.D.
Manning.
2011.
Semi-supervised recursiveautoencoders for predicting sentiment distributions.
In Conference on Empirical Methods in Natural LanguageProcessing, pages 151?161.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and ChristopherPotts.
2013.
Recursive deep models for semantic compositionality over a sentiment treebank.
In Conferenceon Empirical Methods in Natural Language Processing, pages 1631?1642.Yaming Sun, Lei Lin, Duyu Tang, Nan Yang, Zhenzhou Ji, and Xiaolong Wang.
2014.
Radical-enhanced chinesecharacter embedding.
arXiv preprint arXiv:1404.4714.181Duyu Tang, Furu Wei, Nan Yang, Ming Zhou, Ting Liu, and Bing Qin.
2014.
Learning sentiment-specific wordembedding for twitter sentiment classification.
In Procedding of the 52th Annual Meeting of Association forComputational Linguistics.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.
Word representations: a simple and general method forsemi-supervised learning.
Annual Meeting of the Association for Computational Linguistics.Peter D Turney.
2002.
Thumbs up or thumbs down?
: semantic orientation applied to unsupervised classification ofreviews.
In Proceedings of Annual Meeting of the Association for Computational Linguistics, pages 417?424.Leonid Velikovich, Sasha Blair-Goldensohn, Kerry Hannan, and Ryan McDonald.
2010.
The viability of web-derived polarity lexicons.
In Human Language Technologies: The 2010 Annual Conference of the North Amer-ican Chapter of the Association for Computational Linguistics, pages 777?785.
Association for ComputationalLinguistics.Janyce Wiebe.
2000.
Learning subjective adjectives from corpora.
In AAAI/IAAI, pages 735?740.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.
2005.
Recognizing contextual polarity in phrase-level senti-ment analysis.
In Proceedings of the Conference on Empirical Methods in Natural Language Processing, pages347?354.Liheng Xu, Kang Liu, Siwei Lai, Yubo Chen, and Jun Zhao.
2013.
Mining opinion words and opinion targets in atwo-stage framework.
In Proceedings of the 51st ACL, pages 1764?1773.
ACL.182
