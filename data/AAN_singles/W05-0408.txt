Proceedings of the ACL Workshop on Feature Engineering for Machine Learning in NLP, pages 57?64,Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsAutomatic identification of sentiment vocabulary: exploiting low associa-tion with known sentiment termsMichael Gamon Anthony AueNatural Language Processing Group Natural Language Processing GroupMicrosoft Research Microsoft Researchmgamon@microsoft.com anthaue@microsoft.comAbstractWe describe an extension to the techniquefor the automatic identification and label-ing of sentiment terms described in Tur-ney (2002) and Turney and Littman(2002).
Their basic assumption is thatsentiment terms of similar orientationtend to co-occur at the document level.We add a second assumption, namely thatsentiment terms of opposite orientationtend not to co-occur at the sentence level.This additional assumption allows us toidentify sentiment-bearing terms very re-liably.
We then use these newly identifiedterms in various scenarios for the senti-ment classification of sentences.
We showthat our approach outperforms Turney?soriginal approach.
Combining our ap-proach with a Naive Bayes bootstrappingmethod yields a further small improve-ment of classifier performance.
We finallycompare our results to precision and recallfigures that can be obtained on the samedata set with labeled data.1 IntroductionThe field of sentiment classification has receivedconsiderable attention from researchers in recentyears (Pang and Lee 2002, Pang et al 2004, Tur-ney 2002, Turney and Littman 2002, Wiebe et al2001, Bai et al 2004, Yu and Hatzivassiloglou2003 and many others).
The identification andclassification of sentiment constitutes a problemthat is orthogonal to the usual task of text classifi-cation.
Whereas in traditional text classification thefocus is on topic identification, in sentiment classi-fication the focus is on the assessment of thewriter?s sentiment toward the topic.Movie and product reviews have been the mainfocus of many of the recent studies in this area(Pang and Lee 2002, Pang et al 2004, Turney2002, Turney and Littman 2002).
Typically, thesereviews are classified at the document level, andthe class labels are ?positive?
and ?negative?.
Inthis work, in contrast, we narrow the scope of in-vestigation to the sentence level and expand the setof labels, making a threefold distinction between?positive?, ?neutral?, and ?negative?.
The narrow-ing of scope is motivated by the fact that for realis-tic text mining on customer feedback, thedocument level is too coarse, as described in Ga-mon et al (2005).
The expansion of the label set isalso motivated by real-world concerns; while it is agiven that review text expresses positive or nega-tive sentiment, in many cases it is necessary to alsoidentify the cases that don?t carry strong expres-sions of sentiment at all.Traditional approaches to text classification re-quire large amounts of labeled training data.
Ac-quisition of such data can be costly and time-consuming.
Due to the highly domain-specific na-ture of the sentiment classification task, movingfrom one domain to another typically requires theacquisition of a new set of training data.
For thisreason, unsupervised or very weakly supervisedmethods for sentiment classification are especially57desirable.1 Our focus, therefore, is on methods thatrequire very little data annotation.We describe a method to automatically identifythe sentiment vocabulary in a domain.
This methodrests on three special properties of the sentimentdomain:1. the presence of certain words can serve asa proxy for the class label2.
sentiment terms of similar orientation tendto co-occur3.
sentiment terms of opposite orientationtend to not co-occur at the sentence level.Turney (2002) and Turney and Littman (2002)exploit the first two generalizations for unsuper-vised sentiment classification of movie reviews.They use the two terms excellent and poor as seedterms to determine the semantic orientation ofother terms.
These seed terms can be viewed asproxies for the class labels ?positive?
and ?nega-tive?, allowing for the exploitation of otherwiseunlabeled data: Terms that tend to co-occur withexcellent in documents tend to be of positive orien-tation, and vice versa for poor.
Turney (2002)starts from a small (2 word) set of terms withknown orientation (excellent and poor).
Given a setof terms with unknown sentiment orientation, Tur-ney (2002) then uses the PMI-IR algorithm (Tur-ney 2001) to issue queries to the web anddetermine, for each of these terms, its pointwisemutual information (PMI) with the two seed wordsacross a large set of documents.
Term candidatesare constrained to be adjectives, which tend to bethe strongest bearers of sentiment.
The sentimentorientation (SO) of a term is then determined bythe difference between its association (PMI) withthe positive seed term excellent and its associationwith the negative seed term poor.
The resulting listof terms and associated sentiment orientations canthen be used to implement a classifier: semanticorientation of the terms in a document of unknownsentiment is added up, and if the overall score ispositive, the document is classified as being ofpositive sentiment, otherwise it is classified asnegative.Yu and Hatzivassiloglou (2003) extend this ap-proach by (1) applying it at the sentence level (in-stead of the document-level), (2) taking intoaccount non-adjectival parts-of-speech, and (3)1For domain-specificity of sentiment classification see Eng-str?m (2004) and Aue and Gamon (2005).using larger sets of seed words.
Their classificationgoal also differs from Turney?s: it is to distinguishopinion sentences from factual statements.Turney et al?s approach is based on the assump-tion that sentiment terms of similar orientation tendto co-occur in documents.
Our approach takes ad-vantage of a second assumption: At the sentencelevel, sentiment terms of opposite orientation tendnot to co-occur.
This is, of course, an assumptionthat will only hold in general, with exceptions.
Ba-sically, the assumption is that sentences of the fol-lowing form:I dislike X.I really like X.are more frequent than ?mixed sentiment?
sen-tences such asI dislike X but I really like Y.It has been our experience that this generaliza-tion does hold often enough to be useful.We propose to utilize this assumption to identifya set of sentiment terms in a domain.
We select theterms that have the lowest PMI scores on the sen-tence level with respect to a set of manually se-lected seed words.
If our assumption about lowassociation at the sentence level is correct, this setof low-scoring terms will be particularly rich insentiment terms.
We can then use this newly iden-tified set to:(1) use Turney?s method to find the orienta-tion for the terms and employ the termsand their scores in a classifier, and(2) use Turney?s method to find the orienta-tion for the terms and add the new termsas additional seed terms for a second it-erationAs opposed to Turney (2002), we do not use theweb as a resource to find associations, rather weapply the method directly to in-domain data.
Thishas the disadvantage of not being able to apply theclassification to any arbitrary domain.
It is worthnoting, however, that even in Turney (2002) thechoice of seed words is explicitly motivated bydomain properties of movie reviews.In the remainder of the paper we will describeresults from various experiments based on this as-sumption.
We also show how we can combine thismethod with a Naive Bayes bootstrapping ap-proach that takes further advantage of the unla-beled data (Nigam et al 2000).582 DataFor our experiments we used a set of car reviewsfrom the MSN Autos web site.
The data consist of406,818 customer car reviews written over a four-year period.
Aside from filtering out examples con-taining profanity, the data was not edited.
The re-views range in length from a single sentence (56%of all cases) to 50 sentences (a single review).
Lessthan 1% of reviews contain ten or more sentences.There are almost 900,000 sentences in total.
Whencustomers submitted reviews to the website, theywere asked for a recommendation on a scale of 1(negative) to 10 (positive).
The average score wasvery high, at 8.3, yielding a strong skew in favor ofpositive class labels.
We annotated a randomly-selected sample of 3,000 sentences for sentiment.Each sentence was viewed in isolation and classi-fied as positive, negative or neutral.
The neutralcategory was applied to sentences with no dis-cernible sentiment, as well as to sentences that ex-pressed both positive and negative sentiment.Three annotators had pair-wise agreement scores(Cohen?s Kappa score, Cohen 1960) of 70.10%,71.78% and 79.93%, suggesting that the task ofsentiment classification on the sentence level isfeasible but difficult even for people.
This set ofdata was split into a development test set of 400sentences and a blind test set of 2600 sentences.Sentences are represented as vectors of binaryunigram features.
The total number of observedunigram features is 72988.
In order to restrict thenumber of features to a manageable size, we disre-gard features that occur less than 10 times in thecorpus.
With this restriction we obtain a reducedfeature set of 13317 features.3 Experimental SetupOur experiments were performed as follows: Westarted with a small set of manually-selected andannotated seed terms.
We used 4 positive and 6negative seed terms.
We decided to use a few morenegative seed words because of the inherent posi-tive skew in the data that makes the identificationof negative sentences particularly hard.
The termswe used are:positive: negative:good  badexcellent lousylove  terriblehappy  hatesuckunreliableThere was no tuning of the set of initial seedterms; the 10 words were originally chosen intui-tively, as words that we observed frequently whenmanually inspecting the data.We then used these seed terms in two basicways: (1) We used them as seeds for a Turney-style determination of the semantic orientation ofwords in the corpus (semantic orientation, or SOmethod).
As mentioned above, this process isbased on the assumption that terms of similar ori-entation tend to co-occur.
(2) We used them tomine sentiment vocabulary from the unlabeled datausing the additional assumption that sentimentterms of opposite orientation tend not to co-occurat the sentence level (sentiment mining, or SMmethod).
This method yields a set of sentimentterms, but no orientation for that set of terms.
Wecontinue by using the SO method to find the se-mantic orientation for this set of sentiment terms,effectively using SM as a feature selection methodfor sentiment terminology.Pseudo-code for the SO and SM approaches isprovided in Figure 1 and Figure 2.
As a first stepfor both SO and SM methods (not shown in thepseudocode), PMI needs to be calculated for eachpair (f, s) of feature f and seed word s over the col-lection of feature vectors.Figure 1: SO method for determining semantic orienta-tion59Figure 2: SM method for mining sentiment termsIn the first scenario (using straightforward SO),features F range over all observed features in thedata (modulo the aforementioned count cutoff of10).
In the second scenario (SM + SO), features Frange over the n% of features with the lowest PMIscores with respect to any of the seed words thatwere identified using the sentiment mining tech-nique in Figure 2.The result of both SO and SM+SO is a list ofunigram features which have an associated seman-tic orientation score, indicating their sentiment ori-entation: the higher the score, the more ?positive?a term, and vice versa.This list of features and associated scores can beused to construct a simple classifier: for each sen-tence with unknown sentiment, we take the sum ofthe semantic orientation scores for all of the uni-grams in that sentence.
This overall score deter-mines the classification of the sentence as?positive?, ?neutral?
or ?negative?
as shown inFigure 3.Scoring and classifying sentence vectors:(1) assigning a sentence score:FOREACH feature f in sentence vector v:Score(v) = Score(v) + SO(f)(2) assigning a class label based on the sentence score:IF Score(v) > threshold1:Class(v) = ?positive?ELSE IF Score(v) < threshold1 AND Score(v) > threshold2:Class(v) = ?neutral?ELSEClass(v) = ?negative?Figure 3: Using SO scores for sentence scoring andclassificationThe two thresholds used in classification need tobe determined empirically by taking the distribu-tion of class values in the corpus into account.
Forour experiments we simply took the distribution ofclass labels in the 400 sentence development testset as an approximation of the overall class labeldistribution: we determined that distribution to be15.5% for negative sentences, 21.5% for neutralsentences, and 63.0% for positive sentences.Scores for all sentence vectors in the corpus arethen collected using the scoring part of the algo-rithm in Figure 3.
The scores are sorted and thethresholds are determined as the cutoffs for the top63% and bottom 15.5% of scores respectively.4 Results4.1.
Comparing SO and SM+SOIn our first set of experiments we manipulated thefollowing parameters:1. the choice of SO or SM+SO method2.
the choice of n when selecting the n% se-mantic terms with lowest PMI score in theSM methodThe tables below show the results of classifyingsentence vectors using the unigram features andassociated scores produced by SO and SO+SM.We used the 2,600-sentence manually-annotatedtest set described previously to establish thesenumbers.
Since the data exhibit a strong skew infavor of the positive class label, we measure per-formance not in terms of accuracy but in terms ofaverage precision and recall across the three classlabels, as suggested in (Manning and Sch?tze2002).Avg precision Avg recallSO  0.4481 0.4511Table 1: Using the SO approach.Table 1 shows results of using the SO methodon the data.
Table 2 presents the results of combin-ing the SM and SO methods for different values ofn.
The best results are shown in boldface.As a comparison between Table 1 and Table 2shows, the highest average precision and recallscores were obtained by combining the SM and SOmethods.
Using SM as a feature selection mecha-nism also reduces the number of features signifi-cantly.
While the SO method employed onsentence-level vectors uses 13,000 features, thebest-performing SM+SO combination uses only20% of this feature set, indicating that SM is in-deed effective in selecting the most important sen-timent-bearing terms.60We also determined that the positive impact ofSM is not just a matter of reducing the number offeatures.
If SO - without the SM feature selectionstep - is reduced to a comparable number of fea-tures by taking the top features according to abso-lute score, average precision is at 0.4445 andaverage recall at 0.4464.N=10 N=20 N=30 N=40 N=50AvgprecAvgrecAvgprecAvgrecAvgprecAvgrecAvgprecAvgrecAvgprecAvgrecSM+SOSO fromdocu-mentlevel0.4351 0.4377 0.4568 0.4605 0.4528 0.4557 0.4457 0.4478 0.4451 0.4475Table 2: combining SM and SO.Sentiment terms in top 100 SM terms Sentiment terms in top 100 SO termsexcellent, terrible, broke, junk, alright, bargain,grin, highest, exceptional, exceeded, horrible,loved, waste, ok, death, leaking, outstanding,cracked, rebate, warped, hooked, sorry, refuses,excellant, satisfying, died, biggest, competitive,delight, avoid, awful, garbage, loud, okay, com-petent, upscale, dated, mistake, sucks, superior,high, kill, neitherexcellent, happy, stylish, sporty, smooth, love,quiet, overall, pleased, plenty, dependable, solid,roomy, safe, good, easy, smaller, luxury, comfort-able, style, loaded, space, classy, handling, joy,small, comfort, size, perfect, performance, room,choice, recommended, package, compliments,awesome, unique, fun, holds, comfortably, ex-tremely, value, free, satisfied, little, recommend,limited, great, pleasureNon sentiment terms in top 100 SM terms Non sentiment terms in top 100 SO termsalternative, wont, below, surprisingly, main-tained, choosing, comparing, legal, vibration,seemed, claim, demands, assistance, knew, engi-neering, accelleration, ended, salesperson, per-formed, started, midsize, site, gonna, lets, plugs,industry, alternator, month, told, vette, 180,powertrain, write, mos, walk, causing, lift, es,segment, $250, 300m, wanna, february, mod,$50, nhtsa, suburbans, manufactured, tiburon,$10, f150, 5000, posted, tt, him, saw, jan,condition, very, handles, milage, definitely, defi-nately, far, drives, shape, color, price, provides,options, driving, rides, sports, heated, ride, sport,forward, expected, fairly, anyone, test, fits, stor-age, range, family, sedan, trunk, young, weve,black, college, suv, midsize, coupe, 30, shopping,kids, player, saturn, bose, truck, town, am, leather,stereo, car, husbandTable 3: the top 100 terms identified by SM and SOTable 3 shows the top 100 terms that were identi-fied by each SM and SO methods.
The terms arecategorized into sentiment-bearing and non-sentiment bearing terms by human judgment.
Thetwo sets seem to differ in both strength and orien-tation of the identified terms.
The SM-identifiedwords have a higher density of negative terms (22out of 43 versus 2 out of 49 for the SO-identifiedterms).
The SM-identified terms also express sen-timent more strongly, but this conclusion is moretentative since it may be a consequence of thehigher density of negative terms.4.2.
Multiple iterations: increasing thenumber of seed features by SM+SOIn a second set of experiments, we assessed thequestion of whether it is possible to use multipleiterations of the SM+SO method to gradually buildthe list of seed words.
We do this by adding the topn% of features selected by SM, along with theirorientation as determined by SO, to the initial setof seed words.
The procedure for this round of ex-periments is as follows:?
take the top n% of features identified bySM (we used n=1 for the reported re-61sults, since preliminary experimentswith other values for n did not improveresults)?
perform SO for these features to deter-mine their orientation?
take the top 15.5% negative and top63% positive (according to class labeldistribution in the development test set)of the features and add them as nega-tive/positive seed features respectivelyThis iteration increases the number of seed fea-tures from the original 10 manually-selected fea-tures to a total of 111 seed features.With this enhanced set of seed features we thenre-ran a subset of the experiments in Table 2.
Re-sults are shown in Table 4.
Increasing the numberof seed features through the SM feature selectionmethod increases precision and recall by severalpercentage points.
In particular, precision and re-call for negative sentences are boosted.AvgprecisionAvgrecallSM + SO, n=10,SO from document vectors 0.4826 0.48.76SM + SO, n=30,SO from document vectors 0.4957 0.4995SM + SO, n=50,SO from document vectors 0.4914 0.4952Table 4: Using 2 iterations to increase the seed featuresetWe also confirmed that these results are truly at-tributable to the use of the SM method for the firstiteration.
If we take an equivalent number of fea-tures with strongest semantic orientation accordingto the SO method and add them to the list of seedfeatures, our results degrade significantly (the re-sulting classifier performance is significantly dif-ferent at the 99.9% level as established by theMcNemar test).
This is further evidence that SM isindeed an effective method for selecting sentimentterms.4.3.
Using the SO classifier to bootstrap aNaive Bayes classifierIn a third set of experiments, we tried to improveon the results of the SO classifier by combining itwith the bootstrapping approach described in (Ni-gam et al 2000).
The basic idea here is to use theSO classifier to label a subset of the data DL.
Thislabeled subset of the data is then used to bootstrapa Naive Bayes (NB) classifier on the remainingunlabeled data DU using the Expectation Maximi-zation (EM) algorithm:(1) An initial naive Bayes classifier withparameters ?
is trained on the docu-ments in DL.
(2) This initial classifier is used to estimatea probability distribution over all classesfor each of the documents in DU.
(E-Step)(3) The labeled and unlabeled data are thenused to estimate parameters for a newclassifier.
(M-Step)Steps 2 and 3 are repeated until convergence isachieved when the difference in the joint probabil-ity of the data and the parameters falls below theconfigurable threshold ?
between iterations.
An-other free parameter, ?, can be used to control howmuch weight is given to the unlabeled data.For our experiments we used classifiers from thebest SM+SO combination (2 iterations at n=30)from Table 4 above to label 30% of the total data.Table 5 shows the average precision and recallnumbers for the converged NB classifier.2 In addi-tion to improving average precision and recall, theresulting classifier also has the advantage of pro-ducing class probabilities instead of simple scores.3AvgprecisionAvgrecallBootstrapped NBclassifier 0.5167 0.52Table 5: Results obtained by bootstrapping a NB classi-fier4.4.
Results from supervised learning:using small sets of labeled dataGiven infinite resources, we can always annotateenough data to train a classifier using a supervisedalgorithm that will outperform unsupervised orweakly-supervised methods.
Which approach totake depends entirely on how much time andmoney are available and on the accuracy require-ments for the task at hand.2In this experiment, ?
was set to 0.1 and ?
was set to 0.05.3We also experimented with labeling the whole data set with the best of our SOscore classifiers, and then training a linear Support Vector Machine classifier onthe data.
The results were considerably worse than any of the reported numbers,so they are not included in this paper.62To help situate the precision and recall numberspresented in the tables above, we trained SupportVector Machines (SVMs) using small amounts oflabeled data.
SVMs were trained with 500, 1000,2000, and 2500 labeled sentences.
Annotating2500 sentences represents approximately eight per-son-hours of work.
The results can be found in Ta-ble 5.
We were pleasantly surprised at how wellthe unsupervised classifiers described above per-form in comparison to state-of-the-art supervisedmethods (albeit trained on small amounts of data).Labeled ex-amplesAvg.
Preci-sionAvg.
Recall500 .4878 .49671000 .5161 .51052000 .5297 .52562500 .5017 .5083Table 6: Average precision and recall for SVMs forsmall numbers of labeled examples4.5.
Results on the movie domainWe also performed a small set of experiments onthe movie domain using Pang and Lee?s 2004 dataset.
This set consists of 2000 reviews, 1000 each ofvery positive and very negative reviews.
Since thisdata set is balanced and the task is only a two-wayclassification between positive and negative re-views, we only report accuracy numbers here.accuracy Training dataTurney(2002) 66% unsupervisedPang & Lee(2004) 87.15% supervisedAue & Ga-mon (2005) 91.4% supervisedSO 73.95% unsupervisedSM+SO toincrease seedwords, thenSO74.85% weakly super-visedTable 7: Classification accuracy on the movie reviewdomainTurney (2002) achieves 66% accuracy on themovie review domain using the PMI-IR algorithmto gather association scores from the web.
Pangand Lee (2004) report 87.15% accuracy using aunigram-based SVM classifier combined with sub-jectivity detection.
Aue and Gamon (2005) use asimple linear SVM classifier based on unigrams,combined with LLR-based feature reduction, toachieve 91.4% accuracy.
Using the Turney SOmethod on in-domain data instead of web dataachieves 73.95% accuracy (using the same twoseed words that Turney does).
Using one iterationof SM+SO to increase the number of seed words,followed by finding SO scores for all words withrespect to the enhanced seed word set, yields aslightly higher accuracy of 74.85%.
With addi-tional parameter tuning, this number can be pushedto 76.4%, at which point we achieve statistical sig-nificance at the 0.95 level according to the McNe-mar test, indicating that there is more room herefor improvement.
Any reduction of the number ofoverall features in this domain leads to decreasedaccuracy, contrary to what we observed in the carreview domain.
We attribute this observation to thesmaller data set.5 Discussion5.1 A note on statistical significanceWe used the McNemar test to assess whether twoclassifiers are performing significantly differently.This test establishes whether the accuracy of twoclassifiers differs significantly - it does not guaran-tee significance for precision and recall differ-ences.
For the latter, other tests have beenproposed (e.g.
Chinchor 1995), but time con-straints prohibited us from implementing any ofthose more computationally costly tests.For the results presented in the previous sectionsthe McNemar test established statistical signifi-cance at the 0.99 level over baseline (i.e.
the SOresults in Table 1) for the multiple iterations results(Table 4) and the bootstrapping approach (Table5), but not for the SM+SO approach (Table 2).5.2 Future workThis exploratory set of experiments indicates anumber of interesting directions for future work.
Ashortcoming of the present work is the manual tun-ing of cutoff parameters.
This problem could bealleviated in at least two possible ways:First, using a general combination of the rankingof terms according to SM and SO.
In other words,calculate the semantic weight of a term as a com-bination of SO and its rank in the SM scores.63Secondly, following a suggestion by an anony-mous reviewer, the Naive Bayes bootstrapping ap-proach could be used in a feedback loop to informthe SO score estimation in the absence of a manu-ally annotated parameter tuning set.5.3 SummaryOur results demonstrate that the SM method canserve as a valid tool to mine sentiment-rich vo-cabulary in a domain.
SM will yield a list of termsthat are likely to have a strong sentiment orienta-tion.
SO can then be used to find the polarity forthe selected features by association with the senti-ment terms of known polarity in the seed word list.Performing this process iteratively by first enhanc-ing the set of seed words through SM+SO yieldsthe best results.
While this approach does not com-pare to the results that can be achieved by super-vised learning with large amounts of labeled data,it does improve on results obtained by using SOalone.We believe that this result is relevant in two re-spects.
First, by improving average precision andrecall on the classification task, we move closer tothe goal of unsupervised sentiment classification.This is a very important goal in itself given theneed for ?out of the box?
sentiment techniques inbusiness intelligence and the notorious difficulty ofrapidly adapting to a new domain (Engstr?m 2004,Aue and Gamon 2005).
Second, the exploratoryresults reported here may indicate a general sourceof information for feature selection in natural lan-guage tasks: features that have a tendency to be incomplementary distribution (especially in smallerlinguistic units such as sentences) may often forma class that shares certain properties.
In otherwords, it is not only the strong association scoresthat should be exploited but also the particularlyweak (negative) associations.ReferencesAnthony Aue and Michael Gamon (2005): ?Customiz-ing Sentiment Classifiers to a New Domain: A CaseStudy.
Under review.Xue Bai, Rema Padman, and Edoardo Airoldi.
(2004).Sentiment Extraction from Unstructured Text UsingTabu Search-Enhanced Markov Blanket.
In: Proceed-ings of the International Workshop on Mining forand from the Semantic Web (MSW 2004), pp 24-35.Nancy A. Chinchor (1995): Statistical significance ofMUC-6 results.
Proceedings of the Sixth MessageUnderstanding Conference, pp.
39-44.J.
Cohen (1960): ?A coefficient of agreement for nomi-nal scales.?
In: Educational and Psychological meas-urements 20, pp.
37?46Charlotta Engstr?m.
2004.
Topic dependence in Senti-ment Classification.
MPhil thesis, University ofCambridge.Michael Gamon, Anthony Aue, Simon Corston-Oliver,and Eric Ringger.
(2005): ?Pulse: Mining CustomerOpinions from Free Text?.
Under review.Christopher D. Manning and Hinrich Sch?tze (2002):Foundations of Statistical Natural Language Process-ing.
MIT Press, Cambridge, London.Kamal Nigam, Andrew McCallum, Sebastian Thrun andTom Mitchell (2000): Text Classification from La-beled and Unlabeled Documents using EM.
In: Ma-chine Learning 39 (2/3), pp.
103-134.Bo Pang, Lillian Lee and Shivakumar Vaithyanathan(2002): ?Thumbs up?
Sentiment Classification usingMachine Learning Techniques?.
Proceedings ofEMNLP 2002, pp.
79-86.Bo Pang and Lillian Lee.
(2004).
A Sentimental Educa-tion: Sentiment Analysis Using Subjectivity Summa-rization Based on Minimum Cuts.
Proceedings ofACL 2004, pp.217-278.Peter D. Turney (2001): ?Mining the Web for Syno-nyms: PMI-IR versus LSA on TOEFL.?
In Proceed-ings of the Twelfth European Conference onMachine Learning, pp.
491-502.Peter D. Turney (2002): ?Thumbs up or thumbs down?Semantic orientation applied to unsupervised classi-fication of reviews?.
In: Proceedings of ACL 2002,pp.
417-424.Peter D. Turney and M. L. Littman (2002): ?Unsuper-vised Learning of Semantic Orientation from a Hun-dred-Billion-Word Corpus.?
Technical report ERC-1094 (NRC 44929), National Research Council ofCanada.Janyce Wiebe, Theresa Wilson and Matthew Bell(2001): ?Identifying Collocations for RecognizingOpinions?.
In: Proceedings of the ACL/EACL Work-shop on Collocation.Hong Yu and Vasileios Hatzivassiloglou (2003): ?To-wards Answering opinion Questions: SeparatingFacts from Opinions and Identifying the Polarity ofOpinion Sentences?.
In: Proceedings of EMNLP2003.64
