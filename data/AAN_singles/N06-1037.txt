Proceedings of the Human Language Technology Conference of the North American Chapter of the ACL, pages 288?295,New York, June 2006. c?2006 Association for Computational LinguisticsExploring Syntactic Features for Relation Extraction usinga Convolution Tree KernelMin ZHANG         Jie ZHANG        Jian SUInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613{mzhang, zhangjie, sujian}@i2r.a-star.edu.sgAbstractThis paper proposes to use a convolutionkernel over parse trees to model syntacticstructure information for relation extrac-tion.
Our study reveals that the syntacticstructure features embedded in a parsetree are very effective for relation extrac-tion and these features can be well cap-tured by the convolution tree kernel.Evaluation on the ACE 2003 corpusshows that the convolution kernel overparse trees can achieve comparable per-formance with the previous best-reportedfeature-based methods on the 24 ACE re-lation subtypes.
It also shows that ourmethod significantly outperforms the pre-vious two dependency tree kernels on the5 ACE relation major types.1 IntroductionRelation extraction is a subtask of information ex-traction that finds various predefined semantic re-lations, such as location, affiliation, rival, etc.,between pairs of entities in text.
For example, thesentence ?George Bush is the president of theUnited States.?
conveys the semantic relation?President?
between the entities ?George Bush?
(PER) and ?the United States?
(GPE: a Geo-PoliticalEntity --- an entity with land and a government (ACE, 2004)).Prior feature-based methods for this task(Kambhatla 2004; Zhou et al, 2005) employed alarge amount of diverse linguistic features, varyingfrom lexical knowledge, entity mention informa-tion to syntactic parse trees, dependency trees andsemantic features.
Since a parse tree contains richsyntactic structure information, in principle, thefeatures extracted from a parse tree should contrib-ute much more to performance improvement forrelation extraction.
However it is reported (Zhou etal., 2005; Kambhatla, 2004) that hierarchical struc-tured syntactic features contributes less to per-formance improvement.
This may be mainly due tothe fact that the syntactic structure information in aparse tree is hard to explicitly describe by a vectorof linear features.
As an alternative, kernel meth-ods (Collins and Duffy, 2001) provide an elegantsolution to implicitly explore tree structure featuresby directly computing the similarity between twotrees.
But to our surprise, the sole two-reporteddependency tree kernels for relation extraction onthe ACE corpus (Bunescu and Mooney, 2005; Cu-lotta and Sorensen, 2004) showed much lower per-formance than the feature-based methods.
Onemay ask: are the syntactic tree features very usefulfor relation extraction?
Can tree kernel methodseffectively capture the syntactic tree features andother various features that have been proven usefulin the feature-based methods?In this paper, we demonstrate the effectivenessof the syntactic tree features for relation extractionand study how to capture such features via a con-volution tree kernel.
We also study how to selectthe optimal feature space (e.g.
the set of sub-treesto represent relation instances) to optimize the sys-tem performance.
The experimental results showthat the convolution tree kernel plus entity featuresachieves slightly better performance than the pre-vious best-reported feature-based methods.
It alsoshows that our method significantly outperformsthe two dependency tree kernels (Bunescu andMooney, 2005; Culotta and Sorensen, 2004) on the5 ACE relation types.The rest of the paper is organized as follows.
InSection 2, we review the previous work.
Section 3discusses our tree kernel based learning algorithm.288Section 4 shows the experimental results and com-pares our work with the related work.
We concludeour work in Section 5.2 Related WorkThe task of relation extraction was introduced as apart of the Template Element task in MUC6 andformulated as the Template Relation task in MUC7(MUC, 1987-1998).Miller et al (2000) address the task of relationextraction from the statistical parsing viewpoint.They integrate various tasks such as POS tagging,NE tagging, template extraction and relation ex-traction into a generative model.
Their results es-sentially depend on the entire full parse tree.Kambhatla (2004) employs Maximum Entropymodels to combine diverse lexical, syntactic andsemantic features derived from the text for relationextraction.
Zhou et al (2005) explore various fea-tures in relation extraction using SVM.
They con-duct exhaustive experiments to investigate theincorporation and the individual contribution ofdiverse features.
They report that chunking infor-mation contributes to most of the performance im-provement from the syntactic aspect.The features used in Kambhatla (2004) andZhou et al (2005) have to be selected and carefullycalibrated manually.
Kambhatla (2004) use thepath of non-terminals connecting two mentions ina parse tree as the parse tree features.
Besides,Zhou et al (2005) introduce additional chunkingfeatures to enhance the parse tree features.
How-ever, the hierarchical structured information in theparse trees is not well preserved in their parse tree-related features.As an alternative to the feature-based methods,kernel methods (Haussler, 1999) have been pro-posed to implicitly explore features in a high di-mensional space by employing a kernel function tocalculate the similarity between two objects di-rectly.
In particular, the kernel methods could bevery effective at reducing the burden of featureengineering for structured objects in NLP research(Culotta and Sorensen, 2004).
This is because akernel can measure the similarity between two dis-crete structured objects directly using the originalrepresentation of the objects instead of explicitlyenumerating their features.Zelenko et al (2003) develop a tree kernel forrelation extraction.
Their tree kernel is recursivelydefined in a top-down manner, matching nodesfrom roots to leaf nodes.
For each pair of matchingnodes, a subsequence kernel on their child nodes isinvoked, which matches either contiguous orsparse subsequences of node.
Culotta and Sorensen(2004) generalize this kernel to estimate similaritybetween dependency trees.
One may note that theirtree kernel requires the matchable nodes must be atthe same depth counting from the root node.
Thisis a strong constraint on the matching of syntax soit is not surprising that the model has good preci-sion but very low recall on the ACE corpus (Zhaoand Grishman, 2005).
In addition, according to thetop-down node matching mechanism of the kernel,once a node is not matchable with any node in thesame layer in another tree, all the sub-trees belowthis node are discarded even if some of them arematchable to their counterparts in another tree.Bunescu and Mooney (2005) propose a shortestpath dependency kernel for relation extraction.They argue that the information to model a rela-tionship between entities is typically captured bythe shortest path between the two entities in thedependency graph.
Their kernel is very straight-forward.
It just sums up the number of commonword classes at each position in the two paths.
Wenotice that one issue of this kernel is that they limitthe two paths must have the same length, otherwisethe kernel similarity score is zero.
Therefore, al-though this kernel shows non-trivial performanceimprovement than that of Culotta and Sorensen(2004), the constraint makes the two dependencykernels share the similar behavior: good precisionbut much lower recall on the ACE corpus.Zhao and Grishman (2005) define a feature-based composite kernel to integrate diverse fea-tures.
Their kernel displays very good performanceon the 2004 version of ACE corpus.
Since this is afeature-based kernel, all the features used in thekernel have to be explicitly enumerated.
Similarwith the feature-based method, they also representthe tree feature as a link path between two entities.Therefore, we wonder whether their performanceimprovement is mainly due to the explicitly incor-poration of diverse linguistic features instead of thekernel method itself.The above discussion suggests that the syntacticfeatures in a parse tree may not be fully utilized inthe previous work, whether feature-based or ker-nel-based.
We believe that the syntactic tree fea-tures could play a more important role than that289reported in the previous work.
Since convolutionkernels aim to capture structural information interms of sub-structures, which providing a viablealternative to flat features, in this paper, we pro-pose to use a convolution tree kernel to exploresyntactic features for relation extraction.
To ourknowledge, convolution kernels have not been ex-plored for relation extraction1.3 Tree Kernels for Relation ExtractionIn this section, we discuss the convolution treekernel associated with different relation featurespaces.
In Subsection 3.1, we define seven differ-ent relation feature spaces over parse trees.
In Sub-section 3.2, we introduce a convolution tree kernelfor relation extraction.
Finally we compare ourmethod with the previous work in Subsection 3.3.3.1 Relation Feature SpacesIn order to study which relation feature spaces (i.e.,which portion of parse trees) are optimal for rela-tion extraction, we define seven different relationfeature spaces as follows (as shown in Figure 1):(1) Minimum Complete Tree (MCT):It is the complete sub-tree rooted by the node ofthe nearest common ancestor of the two entitiesunder consideration.
(2) Path-enclosed Tree (PT):It is the smallest common sub-tree including thetwo entities.
In other words, the sub-tree is en-closed by the shortest path linking the two entitiesin the parse tree (this path is also typically used asthe path tree features in the feature-based meth-ods).
(3) Chunking Tree (CT):It is the base phrase list extracted from the PT.We prune out all the internal structures of the PTand only keep the root node and the base phraselist for generating the chunking tree.1 Convolution kernels were proposed as a concept of kernelsfor a discrete structure by Haussler (1999) in machine learningstudy.
This framework defines a kernel between input objectsby applying convolution ?sub-kernels?
that are the kernels forthe decompositions (parts) of the objects.
Convolution kernelsare abstract concepts, and the instances of them are deter-mined by the definition of ?sub-kernels?.
The Tree Kernel(Collins and Duffy, 2001), String Subsequence Kernel (SSK)(Lodhi et al, 2002) and Graph Kernel (HDAG Kernel) (Su-zuki et al, 2003) are examples of convolution kernels in-stances in the NLP field.
(4) Context-Sensitive Path Tree (CPT):It is the PT extending with the 1st left sibling ofthe node of entity 1 and the 1st right sibling of thenode of entity 2.
If the sibling is unavailable, thenwe move to the parent of current node and repeatthe same process until the sibling is available orthe root is reached.
(5) Context-Sensitive Chunking Tree (CCT):It is the CT extending with the 1st left sibling ofthe node of entity 1 and the 1st right sibling of thenode of entity 2.
If the sibling is unavailable, thesame process as generating the CPT is applied.Then we do a further pruning process to guaranteethat the context structures of the CCT is still a listof base phrases.
(6) Flattened  PT (FPT):We define two criteria to flatten the PT in orderto generate the Flattened Parse tree: if the in andout arcs of a non-terminal node (except POS node)are both single, the node is to be removed; if anode has the same phrase type with its father node,the node is also to be removed.
(7) Flattened CPT (FCPT):We use the above two criteria to flatten the CPTtree to generate the Flattened CPT.Figure 1 in the next page illustrates the differentsub-tree structures for a relation instance in sen-tence ?Akyetsu testified he was powerless to stopthe merger of an estimated 2000 ethnic Tutsi's inthe district of Tawba.?.
The relation instance is anexample excerpted from the ACE corpus, where anACE-defined relation ?AT.LOCATED?
exists be-tween the entities ?Tutsi's?
(PER) and ?district?
(GPE).We use Charniak?s parser (Charniak, 2001) toparse the example sentence.
Due to space limita-tion, we do not show the whole parse tree of theentire sentence here.
Tree T1 in Figure 1 is theMCT of the relation instance example, where thesub-structure circled by a dashed line is the PT.For clarity, we re-draw the PT as in T2.
The onlydifference between the MCT and the PT lies inthat the MCT does not allow the partial productionrules.
For instance, the most-left two-layer sub-tree[NP [DT ?
E1-O-PER]] in T1 is broken apart inT2.
By comparing the performance of T1 and T2, wecan test whether the sub-structures with partialproduction rules as in T2 will decrease perform-ance.
T3 is the CT. By comparing the performanceof T2 and T3, we want to study whether the chunk-ing information or the parse tree is more effective290for relation extraction.
T4 is the CPT, where thetwo structures circled by dashed lines are the so-called context structures.
T5 is the CCT, where theadditional context structures are also circled bydashed lines.
We want to study if the limited con-text information in the CPT and the CCT can helpboost performance.
Moreover, we illustrate theother two flattened trees in T6 and T7.
The two cir-cled nodes in T2 are removed in the flattened trees.We want to study if the eliminated small structuresare noisy features for relation extraction.3.2 The Convolution Tree KernelGiven the relation instances defined in the previoussection, we use the same convolution tree kernel asthe parse tree kernel (Collins and Duffy, 2001) andthe semantic kernel (Moschitti, 2004).
Generally,we can represent a parse tree T by a vector of inte-ger counts of each sub-tree type (regardless of itsancestors):( )T?
= (# of sub-trees of type 1, ?, # of sub-trees of type i, ?, # of sub-trees of type n)This results in a very high dimensionality since thenumber of different sub-trees is exponential in itssize.
Thus it is computational infeasible to directlyuse the feature vector ( )T?
.
To solve the compu-T1): MCTT2): PTT3): CT T4):CPTT5):CCTT6):FPTT7):FCPTFigure 1.
Relation Feature Spaces of the Example Sentence ???
to stop the merger of an estimated2000 ethnic Tutsi's in the district of Tawba.
?, where the phrase type ?E1-O-PER?
denotesthat the current phrase is the 1st entity, its entity type is ?PERSON?
and its mention level is?NOMIAL?, and likewise for the other two phrase types ?E2-O-GPE?
and ?E-N-GPE?.291tational issue, we introduce the tree kernel functionwhich is able to calculate the dot product betweenthe above high dimensional vectors efficiently.
Thekernel function is defined as follows:1 1 2 21 2 1 2 1 21 2( , ) ( ), ( ) ( )[ ], ( )[ ]( ) ( )ii in N n N iK T T T T T i T iI n I n?
?
?
??
?=< >== ???
?
?where N1 and N2 are the sets of all nodes in trees T1and T2, respectively, and Ii(n) is the indicator func-tion that is 1 iff a sub-tree of type i occurs withroot at node n and zero otherwise.
Collins andDuffy (2002) show that 1 2( , )K T T  is an instance ofconvolution kernels over tree structures, and whichcan be computed in 1 2(| | | |)O N N?
by the follow-ing recursive definitions (Let 1 2( , )n n?
=1 2( ) ( )i ii I n I n??
):(1) if 1n  and 2n  do not have the same syntactic tagor their children are different then 1 2( , ) 0n n?
= ;(2) else if their children are leaves (POS tags), then1 2( , ) 1n n ??
= ?
;(3) else1( )1 2 1 21( , ) (1 ( ( , ), ( , )))nc njn n ch n j ch n j?=?
= +??
,where 1( )nc n is the number of the children of 1n ,( , )ch n j  is the jth child of node n  and?
( 0 1?< < ) is the decay factor in order to makethe kernel value less variable with respect to thetree sizes.3.3 Comparison with Previous WorkIt would be interesting to review the differencesbetween our method and the feature-based meth-ods.
The basic difference between them lies in therelation instance representation and the similaritycalculation mechanism.
A relation instance in ourmethod is represented as a parse tree while it isrepresented as a vector of features in the feature-based methods.
Our method estimates the similar-ity between two relation instances by only count-ing the number of sub-structures that are incommon while the feature methods calculate thedot-product between the feature vectors directly.The main difference between them is the differentfeature spaces.
By the kernel method, we implicitlyrepresent a parse tree by a vector of integer countsof each sub-structure type.
That is to say, we con-sider the entire sub-structure types and their occur-ring frequencies.
In this way, on the one hand, theparse tree-related features in the flat feature set2are embedded in the feature space of our method:?Base Phrase Chunking?
and ?Parse Tree?
fea-tures explicitly appear as substructures of a parsetree.
A few of entity-related features in the flat fea-ture set are also captured by our feature space: ?en-tity type?
and ?mention level?
explicitly appear asphrase types in a parse tree.
On the other hand, theother features in the flat feature set, such as ?wordfeatures?, ?bigram word features?, ?overlap?
and?dependency tree?
are not contained in our featurespace.
From the syntactic viewpoint, the tree repre-sentation in our feature space is more robust than?Parse Tree Path?
feature in the flat feature setsince the path feature is very sensitive to the smallchanges of parse trees (Moschitti, 2004) and it alsodoes not maintain the hierarchical information of aparse tree.
Due to the extensive exploration of syn-tactic features by kernel, our method is expected toshow better performance than the previous feature-based methods.It is also worth comparing our method with theprevious relation kernels.
Since our method onlycounts the occurrence of each sub-tree withoutconsidering its ancestors, our method is not limitedby the constraints in Culotta and Sorensen (2004)and that in Bunescu and Mooney (2005) as dis-cussed in Section 2.
Compared with Zhao andGrishman?s kernel, our method directly uses theoriginal representation of a parse tree while theyflatten a parse tree into a link and a path.
Given theabove improvements, our method is expected tooutperform the previous relation kernels.4 ExperimentsThe aim of our experiment is to verify the effec-tiveness of using richer syntactic structures and theconvolution tree kernel for relation extraction.4.1 Experimental SettingCorpus: we use the official ACE corpus for 2003evaluation from LDC as our test corpus.
The ACEcorpus is gathered from various newspaper, news-wire and broadcasts.
The same as previous work2 For the convenience of discussion, without losing generality,we call the features used in Zhou et al (2005) and Kambhatla(2004) flat feature set.292(Zhou et al, 2005), our experiments are carried outon explicit relations due to the poor inter-annotatoragreement in annotation of implicit relations andtheir limited numbers.
The training set consists of674 annotated text documents and 9683 relationinstances.
The test set consists of 97 documentsand 1386 relation instances.
The 2003 evaluationdefined 5 types of entities: Persons, Organizations,Locations, Facilities and GPE.
Each mention of anentity is associated with a mention type: propername, nominal or pronoun.
They further defined 5major relation types and 24 subtypes: AT (Base-In,Located?
), NEAR (Relative-Location), PART(Part-of, Subsidiary ?
), ROLE (Member, Owner?)
and SOCIAL (Associate, Parent?).
As previ-ous work, we explicitly model the argument orderof the two mentions involved.
We thus model rela-tion extraction as a multi-class classification prob-lem with 10 classes on the major types (2 for eachrelation major type and a ?NONE?
class for non-relation (except 1 symmetric type)) and 43 classeson the subtypes (2 for each relation subtype and a?NONE?
class for non-relation (except 6 symmet-ric subtypes)).
In this paper, we only measure theperformance of relation extraction models on?true?
mentions with ?true?
chaining of corefer-ence (i.e.
as annotated by LDC annotators).Classifier: we select SVM as the classifier used inthis paper since SVM can naturally work with ker-nel methods and it also represents the state-of-the-art machine learning algorithm.
We adopt the onevs.
others strategy and select the one with largestmargin as the final answer.
The training parametersare chosen using cross-validation (C=2.4 (SVM);?
=0.4(tree kernel)).
In our implementation, weuse the binary SVMLight developed by Joachims(1998) and Tree Kernel Toolkits developed byMoschitti (2004).Kernel Normalization: since the size of a parsetree is not constant, we normalize 1 2( , )K T T by divid-ing it by 1 1 2 2( , ) ( , )K T T K T T?
.Evaluation Method: we parse the sentence usingCharniak parser and iterate over all pair of men-tions occurring in the same sentence to generatepotential instances.
We find the negative samplesare 10 times more than the positive samples.
Thusdata imbalance and sparseness are potential prob-lems.
Recall (R), Precision (P) and F-measure (F)are adopted as the performance measure.4.2 Experimental ResultsIn order to study the impact of the sole syntacticstructure information embedded in parse trees onrelation extraction, we remove the entity informa-tion from parse trees by replacing the entity-relatedphrase type (?E1-O-PER?, etc., in Figure 1) with?NP?.
Then we carry out a couple of preliminaryexperiments on the test set using parse trees re-gardless of entity information.Feature Spaces P R FMinimum Complete Tree 77.45 38.39 51.34Path-enclosed Tree (PT) 72.77 53.80 61.87Chunking Tree (CT) 75.18 44.75 56.11Context-Sensitive PT(CPT) 77.87 42.80 55.23Context-Sensitive CT 78.33 40.84 53.69Flattened PT 76.86 45.69 57.31Flattened CPT 80.60 41.20 54.53Table 1.
Performance of seven relation featurespaces over the 5 ACE major types using parsetree information onlyTable 1 reports the performance of our definedseven relation feature spaces over the 5 ACE majortypes using parse tree information regardless ofany entity information.
This preliminary experi-ments show that:?
Overall the tree kernel over different relationfeature spaces is effective for relation extractionsince we use the parse tree information only.
Wewill report the detailed performance comparisonresults between our method and previous worklater in this section.?
Using the PTs achieves the best performance.This means the portion of a parse tree enclosedby the shortest path between entities can modelrelations better than other sub-trees.?
Using the MCTs get the worst performance.This is because the MCTs introduce too muchleft and right context information, which may benoisy features, as shown in Figure 1.
It suggeststhat only allowing complete (not partial) produc-tion rules in the MCTs does harm performance.?
The performance of using CTs drops by 5 in F-measure compared with that of using the PTs.This suggests that the middle and high-levelstructures beyond chunking is also very usefulfor relation extraction.293?
The context-sensitive trees show lower perform-ance than the corresponding original PTs andCTs.
In some cases (e.g.
in sentence ?the mergeof company A and company B?.
?, ?merge?
isthe context word), the context information ishelpful.
However the effective scope of contextis hard to determine.?
The two flattened trees perform worse than theoriginal trees, but better than the correspondingcontext-sensitive trees.
This suggests that theremoved structures by the flattened trees con-tribute non-trivial performance improvement.In the above experiments, the path-enclosed treedisplays the best performance among the sevenfeature spaces when using the parse tree structuralinformation only.
In the following incremental ex-periments, we incorporate more features into thepath-enclosed parse trees and it shows significantperformance improvement.Path-enclosed Tree (PT) P R FParse tree structure in-formation only72.77 53.80 61.87+Entity information  76.14 62.85 68.86+Semantic features 76.32 62.99 69.02Table 2.
Performance of Path-enclosed Treeswith different setups over the 5 ACE major typesTable 2 reports the performance over the 5 ACEmajor types using Path-enclosed trees enhancedwith more features in nodes.
The 1st row is thebaseline performance using structural informationonly.
We then integrate entity information, includ-ing Entity type and Mention level features, into thecorresponding nodes as shown in Figure 1.
The 2ndrow in Table 2 reports the performance of thissetup.
Besides the entity information, we furtherincorporate the semantic features used in Zhou etal.
(2005) into the corresponding leaf nodes.
The3rd row in Table 2 reports the performance of thissetup.
Please note that in the 2nd and 3rd setups, westill use the same tree kernel function with slightmodification on the rule (2) in calculating1 2( , )n n?
(see subsection 3.2) to make it considermore features associated with each individualnode: 1 2( , )  n n feature weight ??
= ?
.
From Table2, we can see that the basic feature of entity infor-mation is quite useful, which largely boosts per-formance by 7 in F-measure.
The finalperformance of our tree kernel method for relationextraction is 76.32/62.99/69.02 in preci-sion/recall/F-measure over the 5 ACE major types.Methods P R FOurs: convolution kernelover parse trees76.32(64.6)62.99(50.76)69.02(56.83)Kambhatla (2004):feature-based ME-(63.5)-(45.2)-(52.8)Zhou et al (2005):feature-based SVM77.2(63.1)60.7(49.5)68.0(55.5)Culotta and Sorensen(2004): dependency kernel67.1(-)35.0(-)45.8(-)Bunescu and Mooney(2005): shortest path de-pendency kernel65.5(-)43.8(-)52.5(-)Table 3.
Performance comparison, the numbers inparentheses report the performance over the 24ACE subtypes while the numbers outside paren-theses is for the 5 ACE major typesTable 3 compares the performance of differentmethods on the ACE corpus3.
It shows that ourmethod achieves the best-reported performance onboth the 24 ACE subtypes and the 5 ACE majortypes.
It also shows that our tree kernel methodsignificantly outperform the previous two depend-ency kernel algorithms by 16 in F-measure on the5 ACE relation types4.
This may be due to two rea-sons: one reason is that the dependency tree lacksthe hierarchical syntactic information, and anotherreason is due to the two constraints of the two de-pendency kernels as discussed in Section 2 andSubsection 3.3.
The performance improvement byour method suggests that the convolution tree ker-nel can explore the syntactic features (e.g.
parsetree structures and entity information) very effec-tively and the syntactic features are also particu-3 Zhao and Grishman (2005) also evaluated their algorithm onthe ACE corpus and got good performance.
But their experi-mental data is for 2004 evaluation, which defined 7 entitytypes with 44 entity subtypes, and 7 relation major types with27 subtypes, so we are not ready to compare with each other.4 Bunescu and Mooney (2005) used the ACE 2002 corpus,including 422 documents, which is known to have many in-consistencies than the 2003 version.
Culotta and Sorensen(2004) used an ACE corpus including about 800 documents,and they did not specify the corpus version.
Since the testingcorpora are in different sizes and versions, strictly speaking, itis not ready to compare these methods exactly and fairly.
ThusTable 3 is only for reference purpose.
We just hope that wecan get a few clues from this table.294larly effective for the task of relation extraction.
Inaddition, we observe from Table 1 that the featurespace selection (the effective portion of a parsetree) is also critical to relation extraction.Error Type # of error instanceFalse Negative 414False Positive 173Cross Type 97Table 4.
Error DistributionFinally, Table 4 reports the error distribution inthe case of the 3rd experiment in Table 2.
It showsthat 85.9% (587/684) of the errors result from rela-tion detection and only 14.1% (97/684) of the er-rors result from relation characterization.
This ismainly due to the imbalance of the posi-tive/negative instances and the sparseness of somerelation types on the ACE corpus.5 Conclusion and Future WorkIn this paper, we explore the syntactic features us-ing convolution tree kernels for relation extraction.We conclude that: 1) the relations between entitiescan be well represented by parse trees with care-fully calibrating effective portions of parse trees;2) the syntactic features embedded in a parse treeare particularly effective for relation extraction; 3)the convolution tree kernel can effectively capturethe syntactic features for relation extraction.The most immediate extension of our work is toimprove the accuracy of relation detection.
Wemay adopt a two-step method (Culotta and Soren-sen, 2004) to separately model the relation detec-tion and characterization issues.
We may integratemore features (such as head words or WordNetsemantics) into nodes of parse trees.
We can alsobenefit from the learning algorithm to study how tosolve the data imbalance and sparseness issuesfrom the learning algorithm viewpoint.
In the fu-ture, we would like to test our algorithm on theother version of the ACE corpus and to developfast algorithm (Vishwanathan and Smola, 2002) tospeed up the training and testing process of convo-lution kernels.Acknowledgements: We would like to thank Dr.Alessandro Moschitti for his great help in using hisTree Kernel Toolkits and fine-tuning the system.We also would like to thank the three anonymousreviewers for their invaluable suggestions.ReferencesACE.
2004.
The Automatic Content Extraction (ACE)Projects.
http://www.ldc.upenn.edu/Projects/ACE/Bunescu R. C. and Mooney R. J.
2005.
A Shortest PathDependency Kernel for Relation Extraction.EMNLP-2005Charniak E. 2001.
Immediate-head Parsing for Lan-guage Models.
ACL-2001Collins M. and Duffy N. 2001.
Convolution Kernels forNatural Language.
NIPS-2001Culotta A. and Sorensen J.
2004.
Dependency Tree Ker-nel for Relation Extraction.
ACL-2004Haussler D. 1999.
Convolution Kernels on DiscreteStructures.
Technical Report UCS-CRL-99-10, Uni-versity of California, Santa Cruz.Joachims T. 1998.
Text Categorization with SupportVecor Machine: learning with many relevant fea-tures.
ECML-1998Kambhatla Nanda.
2004.
Combining lexical, syntacticand semantic features with Maximum Entropy mod-els for extracting relations.
ACL-2004 (poster)Lodhi H., Saunders C., Shawe-Taylor J., Cristianini N.and Watkins C. 2002.
Text classification using stringkernel.
Journal of Machine Learning Research,2002(2):419-444Miller S., Fox H., Ramshaw L. and Weischedel R. 2000.A novel use of statistical parsing to extract informa-tion from text.
NAACL-2000Moschitti Alessandro.
2004.
A Study on ConvolutionKernels for Shallow Semantic Parsing.
ACL-2004MUC.
1987-1998.
The nist MUC website: http://www.itl.nist.gov/iaui/894.02/related_projects/muc/Suzuki J., Hirao T., Sasaki Y. and Maeda E. 2003.
Hi-erarchical Directed Acyclic Graph Kernel: Methodsfor Structured Natural Language Data.
ACL-2003Vishwanathan S.V.N.
and Smola A.J.
2002.
Fast ker-nels for String and Tree Matching.
NIPS-2002Zelenko D., Aone C. and Richardella A.
2003.
KernelMethods for Relation Extraction.
Journal of MachineLearning Research.
2003(2):1083-1106Zhao Shubin and Grishman Ralph.
2005.
ExtractingRelations with Integrated Information Using KernelMethods.
ACL-2005Zhou Guodong, Su Jian, Zhang Jie and Zhang Min.2005.
Exploring Various Knowledge in Relation Ex-traction.
ACL-2005295
