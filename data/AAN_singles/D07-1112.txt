Proceedings of the CoNLL Shared Task Session of EMNLP-CoNLL 2007, pp.
1051?1055,Prague, June 2007. c?2007 Association for Computational LinguisticsFrustratingly Hard Domain Adaptation for Dependency ParsingMark Dredze1 and John Blitzer1 and Partha Pratim Talukdar1 andKuzman Ganchev1 and Joa?o V. Grac?a2 and Fernando Pereira11CIS Dept., University of Pennsylvania, Philadelphia, PA 19104{mdredze|blitzer|partha|kuzman|pereira}@seas.upenn.edu2L2F ?
INESC-ID Lisboa/IST, Rua Alves Redol 9, 1000-029, Lisboa, Portugaljavg@l2f.inesc-id.ptAbstractWe describe some challenges of adaptationin the 2007 CoNLL Shared Task on DomainAdaptation.
Our error analysis for this tasksuggests that a primary source of error isdifferences in annotation guidelines betweentreebanks.
Our suspicions are supported bythe observation that no team was able to im-prove target domain performance substan-tially over a state of the art baseline.1 IntroductionDependency parsing, an important NLP task, can bedone with high levels of accuracy.
However, adapt-ing parsers to new domains without target domainlabeled training data remains an open problem.
Thispaper outlines our participation in the 2007 CoNLLShared Task on Domain Adaptation (Nivre et al,2007).
The goal was to adapt a parser trained ona single source domain to a new target domain us-ing only unlabeled data.
We were given around15K sentences of labeled text from the Wall StreetJournal (WSJ) (Marcus et al, 1993; Johansson andNugues, 2007) as well as 200K unlabeled sentences.The development data was 200 sentences of labeledbiomedical oncology text (BIO, the ONCO portionof the Penn Biomedical Treebank), as well as 200Kunlabeled sentences (Kulick et al, 2004).
The twotest domains were a collection of medline chem-istry abstracts (pchem, the CYP portion of the PennBiomedical Treebank) and the Child Language DataExchange System corpus (CHILDES) (MacWhin-ney, 2000; Brown, 1973).
We used the second or-der two stage parser and edge labeler of McDonaldet al (2006), which achieved top results in the 2006CoNLL-X shared task.
Preliminary experiments in-dicated that the edge labeler was fairly robust to do-main adaptation, lowering accuracy by 3% in the de-velopment domain as opposed to 2% in the source,so we focused on unlabeled dependency parsing.Our system did well, officially coming in 3rdplace out of 12 teams and within 1% of the top sys-tem (Table 1).
1 In unlabeled parsing, we scored1st and 2nd on CHILDES and pchem respectively.However, our results were obtained without adap-tation.
Given our position in the ranking, this sug-gests that no team was able to significantly improveperformance on either test domain beyond that of astate-of-the-art parser.After much effort in developing adaptation meth-ods, it is critical to understand the causes of thesenegative results.
In what follows, we provide an er-ror analysis that attributes domain loss for this taskto a difference in annotation guidelines between do-mains.
We then overview our attempts to improveadaptation.
While we were able to show limitedadaptation on reduced training data or with first-order features, no modifications improved parsingwith all the training data and second-order features.2 Parsing ChallengesWe begin with an error analysis for adaptation be-tween WSJ and BIO.
We divided the available WSJdata into a train and test set, trained a parser onthe train set and compared errors on the test setand BIO.
Accuracy dropped from 90% on WSJ to84% on BIO.
We then computed the fraction of er-rors involving each POS tag.
For the most common1While only 8 teams participated in the closed track with us,our score beat all of the teams in the open track.1051pchem l pchem ul childes ul bio ulOurs 80.22 83.38 61.37 83.93Best 81.06 83.42 61.37 -Mean 73.03 76.42 57.89 -Rank 3rd 2nd 1st -Table 1: Official labeled (l) and other unlabeled (ul)submitted results for the two test domains (pchemand childes) and development data accuracy (bio).The parser was trained on the provided WSJ data.POS types, the loss (difference in source and tar-get error) was: verbs (2%), conjunctions (5%), dig-its (23%), prepositions (4%), adjectives (3%), de-terminers (4%) and nouns (9%).
2 Two POS typesstand out: digits and nouns.
Digits are less than4% of the tokens in BIO.
Errors result from the BIOannotations for long sequences of digits which donot appear in WSJ.
Since these annotations are newwith respect to the WSJ guidelines, it is impossi-ble to parse these without injecting knowledge ofthe annotation guidelines.
3 Nouns are far morecommon, comprising 33% of BIO and 30% of WSJtokens, the most popular POS tag by far.
Addi-tionally, other POS types listed above (adjectives,prepositions, determiners, conjunctions) often attachto nouns.
To confirm that nouns were problem-atic, we modified a first-order parser (no second or-der features) by adding a feature indicating correctnoun-noun edges, forcing the parser to predict theseedges correctly.
Adaptation performance rose onBIO from 78% without the feature to 87% with thefeature.
This indicates that most of the loss comesfrom missing these edges.The primary problem for nouns is the differencebetween structures in each domain.
The annota-tion guidelines for the Penn Treebank flattened nounphrases to simplify annotation (Marcus et al, 1993),so there is no complex structure to NPs.
Ku?bler(2006) showed that it is difficult to compare thePenn Treebank to other treebanks with more com-plex noun structures, such as BIO.
Consider theWSJphrase ?the New York State Insurance Department?.The annotation indicates a flat structure, where ev-2We measured these drops on several other dependencyparsers and found similar results.3For example, the phrase ?
(R = 28% (10/26); K=10% (3/29);chi2 test: p=0.014).
?ery token is headed by ?Department?.
In contrast,a similar BIO phrase has a very different structure,pursuant to the BIO guidelines.
For ?the detoxi-cation enzyme glutathione transferase P1-1?, ?en-zyme?
is the head of the NP, ?P1-1?
is the head of?transferase?, and ?transferase?
is the head of ?glu-tathione?.
Since the guidelines differ, we observe nocorresponding structure in the WSJ.
It is telling thatthe parser labels this BIO example by attaching ev-ery token to the final proper noun ?P1-1?, exactly asthe WSJ guidelines indicate.
Unlabeled data cannotindicate that BIO uses a different standard.Another problem concerns appositives.
For ex-ample, the phrase ?Howard Mosher, president andchief executive officer,?
has ?Mosher?
as the headof ?Howard?
and of the appositive NP delimited bycommas.
While similar constructions occur in BIO,there are no commas to indicate this.
An example isthe above BIO NP, in which the phrase ?glutathionetransferase P1-1?
is an appositive indicating which?enzyme?
is meant.
However, since there are nocommas, the parser thinks ?P1-1?
is the head.
How-ever, there are not many right to left attaching nouns.In addition to a change in the annotation guide-lines for NPs, we observed an important differencein the distribution of POS tags.
NN tags were almosttwice as likely in the BIO domain (14% in WSJ and25% in BIO).
NNP tags, which are close to 10% ofthe tags in WSJ, are nonexistent in BIO (.24%).
Thecause for this is clear when the annotation guide-lines are considered.
The proper nouns in WSJ arenames of companies, people and places, while inBIO they are names of genes, proteins and chemi-cals.
However, for BIO these nouns are labeled NNinstead of NNP.
This decision effectively removesNNP from the BIO domain and renders all featuresthat depend on the NNP tag ineffective.
In our aboveBIO NP example, all nouns are labeled NN, whereasthe WSJ example contains NNP tags.
The largesttri-gram differences involve nouns, such as NN-NN-NN, NNP-NNP-NNP, NN-IN-NN, and IN-NN-NN.However, when we examine the coarse POS tags,which do not distinguish between nouns, these dif-ferences disappear.
This indicates that while theoverall distribution of POS tags is similar betweenthe domains, the fine grained tags differ.
These finegrained tags provide more information than coarsetags; experiments that removed fine grained tags1052hurt WSJ performance but did not affect BIO.Finally, we examined the effect of unknownwords.
Not surprisingly, the most significant dif-ferences in error rates concerned dependencies be-tween words of which one or both were unknownto the parser.
For two words that were seen in thetraining data loss was 4%, for a single unknownword loss was 15%, and 26% when both words wereunknown.
Both words were unknown only 5% ofthe time in BIO, while one of the words being un-known was more common, reflecting 27% of deci-sions.
Upon further investigation, the majority ofunknown words were nouns, which indicates thatunknown word errors were caused by the problemsdiscussed above.Recent theoretical work on domain adapta-tion (Ben-David et al, 2006) attributes adaptationloss to two sources: the difference in the distribu-tion between domains and the difference in label-ing functions.
Adaptation techniques focus on theformer since it is impossible to determine the lat-ter without knowledge of the labeling function.
Inparsing adaptation, the former corresponds to a dif-ference between the features seen in each domain,such as new words in the target domain.
The de-cision function corresponds to differences betweenannotation guidelines between two domains.
Our er-ror analysis suggests that the primary cause of lossfrom adaptation is from differences in the annotationguidelines themselves.
Therefore, significant im-provements cannot be made without specific knowl-edge of the target domain?s annotation standards.
Noamount of source training data can help if no rele-vant structure exists in the data.
Given the resultsfor the domain adaptation track, it appears no teamsuccessfully adapted a state-of-the-art parser.3 Adaptation ApproachesWe survey the main approaches we explored for thistask.
While some of these approaches provided amodest performance boost to a simple parser (lim-ited data and first-order features), no method addedany performance to our best parser (all data andsecond-order features).3.1 FeaturesA natural approach to improving parsing is to mod-ify the feature set, both by removing features lesslikely to transfer and by adding features that aremore likely to transfer.
We began with the first ap-proach and removed a large number of features thatwe believed transfered poorly, such as most featuresfor noun-noun edges.
We obtained a small improve-ment in BIO performance on limited data only.
Wethen added several different types of features, specif-ically designed to improve noun phrase construc-tions, such as features based on the lexical positionof nouns (common position in NPs), frequency ofoccurrence, and NP chunking information.
For ex-ample, trained on in-domain data, nouns that occurmore often tend to be heads.
However, none of thesefeatures transfered between domains.A final type of feature we added was based onthe behavior of nouns, adjectives and verbs in eachdomain.
We constructed a feature representationof words based on adjacent POS and words andclustered words using an algorithm similar to thatof Saul and Pereira (1997).
For example, our clus-tering algorithm grouped first names in one groupand measurements in another.
We then added thecluster membership as a lexical feature to the parser.None of the resulting features helped adaptation.3.2 DiversityTraining diversity may be an effective source foradaptation.
We began by adding information frommultiple different parsers, which has been shownto improve in-domain parsing.
We added featuresindicating when an edge was predicted by anotherparser and if an edge crossed a predicted edge, aswell as conjunctions with edge types.
This failedto improve BIO accuracy since these features wereless reliable at test time.
Next, we tried instancebagging (Breiman, 1996) to generate some diversityamong parsers.
We selected with replacement 2000training examples from the training data and trainedthree parsers.
Each parser then tagged the remain-ing 13K sentences, yielding 39K parsed sentences.We then shuffled these sentences and trained a finalparser.
This failed to improve performance, possiblybecause of conflicting annotations or because of lackof sufficient diversity.
To address conflicting annota-1053tions, we added slack variables to the MIRA learn-ing algorithm (Crammer et al, 2006) used to trainthe parsers, without success.
We measured diversityby comparing the parses of each model.
The dif-ference in annotation agreement between the threeinstance bagging parsers was about half the differ-ence between these parsers and the gold annotations.While we believe this is not enough diversity, it wasnot feasible to repeat our experiment with a largenumber of parsers.3.3 Target Focused LearningAnother approach to adaptation is to favor trainingexamples that are similar to the target.
We first mod-ified the weight given by the parser to each trainingsentence based on the similarity of the sentence totarget domain sentences.
This can be done by mod-ifying the loss to limit updates in cases where thesentence does not reflect the target domain.
We trieda number of criteria to weigh sentences without suc-cess, including sentence length and number of verbs.Next, we trained a discriminative model on the pro-vided unlabeled data to predict the domain of eachsentence based on POS n-grams in the sentence.Training sentences with a higher probability of be-ing in the target domain received higher weights,also without success.
Further experiments showedthat any decrease in training data hurt parser perfor-mance.
It would seem that the parser has no dif-ficulty learning important training sentences in thepresence of unimportant training examples.A related idea focused on words, weighing highlytokens that appeared frequently in the target domain.We scaled the loss associated with a token by a fac-tor proportional to its frequency in the target do-main.
We found certain scaling techniques obtainedtiny improvements on the target domain that, whilesignificant compared to competition results, are notstatistically significant.
We also attempted a sim-ilar approach on the feature level.
A very predic-tive source domain feature is not useful if it doesnot appear in the target domain.
However, limitingthe feature space to target domain features had noeffect.
Instead, we scaled each feature?s value by afactor proportional to its frequency in the target do-main and trained the parser on these scaled featurevalues.
We obtained small improvements on smallamounts of training data.4 Future DirectionsGiven our pessimistic analysis and the long list offailed methods, one may wonder if parser adapta-tion is possible at all.
We believe that it is.
First,there may be room for adaptation with our domainsif a common annotation scheme is used.
Second,we have stressed that typical adaptation, modifyinga model trained on the source domain, will fail butthere may be unsupervised parsing techniques thatimprove performance after adaptation, such as a rulebased NP parser for BIO based on knowledge of theannotations.
However, this approach is unsatisfyingas it does not allow general purpose adaptation.5 AcknowledgmentsWe thank Joel Wallenberg and Nikhil Dinesh fortheir informative and helpful linguistic expertise,Kevin Lerman for his edge labeler code, and KobyCrammer for helpful conversations.
Dredze is sup-ported by a NDSEG fellowship; Ganchev and Taluk-dar by NSF ITR EIA-0205448; and Blitzer byDARPA under Contract No.
NBCHD03001.
Anyopinions, findings, and conclusions or recommen-dations expressed in this material are those of theauthor(s) and do not necessarily reflect the views ofthe DARPA or the Department of Interior-NationalBusiness Center (DOI-NBC).ReferencesShai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2006.
Analysis of representations fordomain adaptation.
In NIPS.Leo Breiman.
1996.
Bagging predictors.
MachineLearning, 24(2):123?140.R.
Brown.
1973.
A First Language: The Early Stages.Harvard University Press.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
Journal of Machine LearningResearch, 7:551?585, Mar.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA).Sandra Ku?bler.
2006.
How do treebank annotationschemes influence parsing results?
or how not to com-pare apples and oranges.
In RANLP.1054S.
Kulick, A. Bies, M. Liberman, M. Mandel, R. Mc-Donald, M. Palmer, A. Schein, and L. Ungar.
2004.Integrated annotation for biomedical information ex-traction.
In Proc.
of the Human Language Technol-ogy Conference and the Annual Meeting of the NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT/NAACL).B.
MacWhinney.
2000.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum.M.
Marcus, B. Santorini, and M. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19(2):313?330.Ryan McDonald, Kevin Lerman, and Fernando Pereira.2006.
Multilingual dependency parsing with a two-stage discriminative parser.
In Conference on NaturalLanguage Learning (CoNLL).J.
Nivre, J.
Hall, S. Ku?bler, R. McDonald, J. Nils-son, S. Riedel, and D. Yuret.
2007.
The CoNLL2007 shared task on dependency parsing.
In Proc.of the CoNLL 2007 Shared Task.
Joint Conf.
on Em-pirical Methods in Natural Language Processing andComputational Natural Language Learning (EMNLP-CoNLL).Lawrence Saul and Fernando Pereira.
1997.
Aggre-gate and mixed-order markov models for statisticallanguage modeling.
In EMNLP.1055
