Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 14?19,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsLearning Multiview Embeddings of Twitter UsersAdrian Benton?adrian@cs.jhu.eduRaman Arora?arora@cs.jhu.edu?Human Language Technology Center of ExcellenceCenter for Language and Speech Processing, Johns Hopkins UniversityBaltimore, MD 21218 USA?Bloomberg LP, New York, NY 10022Mark Dredze?
?mdredze@cs.jhu.eduAbstractLow-dimensional vector representations arewidely used as stand-ins for the text of words,sentences, and entire documents.
These em-beddings are used to identify similar wordsor make predictions about documents.
In thiswork, we consider embeddings for social me-dia users and demonstrate that these can beused to identify users who behave similarly orto predict attributes of users.
In order to cap-ture information from all aspects of a user?sonline life, we take a multiview approach,applying a weighted variant of GeneralizedCanonical Correlation Analysis (GCCA) to acollection of over 100,000 Twitter users.
Wedemonstrate the utility of these multiview em-beddings on three downstream tasks: user en-gagement, friend selection, and demographicattribute prediction.1 IntroductionDense, low-dimensional vector representations (em-beddings) have a long history in NLP, and recent workon neural models have provided new and popular al-gorithms for training representations for word types(Mikolov et al, 2013; Faruqui and Dyer, 2014), sen-tences (Kiros et al, 2015), and entire documents (Leand Mikolov, 2014).
These embeddings often have niceproperties, such as capturing some aspects of syntax orsemantics and outperforming their sparse counterpartsat downstream tasks.While there are many approaches to generating em-beddings of text, it is not clear how to learn embeddingsfor social media users.
There are several different typesof data (views) we can use to build user representations:the text of messages they post, neighbors in their localnetwork, articles they link to, images they upload, etc.We propose unsupervised learning of representations ofusers with a variant of Generalized Canonical Correla-tion Analysis (GCCA) (Carroll, 1968; Van De Veldenand Bijmolt, 2006; Arora and Livescu, 2014; Rastogiet al, 2015), a multiview technique that learns a single,low-dimensional vector for each user best capturing in-formation from each of their views.
We believe thisis more appropriate for learning user embeddings thanconcatenating views into a single vector, since viewsmay correspond to different modalities (image vs. textdata) or have very different distributional properties.Treating all features as equal in this concatenated vec-tor is not appropriate.We offer two main contributions: (1) an applicationof GCCA to learning vector representations of socialmedia users that best accounts for all aspects of a user?sonline life, and (2) an evaluation of these vector repre-sentations for a set of Twitter users at three differenttasks: user engagement, friend, and demographic at-tribute prediction.2 Twitter User DataWe begin with a description of our dataset, whichis necessary for understanding the data available toour multiview model.
We uniformly sampled 200,000users from a stream of publicly available tweets fromthe 1% Twitter stream from April 2015.
To includetypical, English speaking users we removed users withverified accounts, more than 10,000 followers, or non-English profiles1.
For each user we collected their1,000 most recent tweets, and then filtered out non-English tweets.
Users without English tweets in Jan-uary or February 2015 were omitted, yielding a totalof 102,328 users.
Although limiting tweets to onlythese two months restricted the number of tweets wewere able to work with, it also ensured that our dataare drawn from a narrow time window, controlling fordifferences in user activity over time.
This allows usto learn distinctions between users, and not temporaldistinctions of content.
We will use this set of users tolearn representations for the remainder of this paper.Next, we expand the information available aboutthese users by collecting information about their so-cial networks.
Specifically, for each user mentionedin a tweet by one of the 102,328 users, we collect upto the 200 most recent English tweets for these usersfrom January and February 2015.
Similarly, we col-lected the 5,000 most recently added friends and fol-lowers of each of the 102,328 users.
We then sampled10 friends and 10 followers for each user and collected1Identified with langid (Lui and Baldwin, 2012).14up to the 200 most recent English tweets for these usersfrom January and February 2015.
Limits on the num-ber of users and tweets per user were imposed so thatwe could operate within Twitter?s API limits.
This datasupports several of our prediction tasks, as well as thefour sources for each user: their tweets, tweets of men-tioned users, friends and followers.3 User ViewsOur user dataset provides several sources of informa-tion on which we can build user views: text posted bythe user (ego) and people that are mentioned, friendedor followed by the user and their posted text.For each text source we can aggregate the manytweets into a single document, e.g.
all tweets writtenby accounts mentioned by a user.
We represent thisdocument as a bag-of-words (BOW) in a vector spacemodel with a vocabulary of the 20,000 most frequentword types after stopword removal.
We will considerboth count and TF-IDF weighted vectors.A common problem with these high dimensionalrepresentations is that they suffer from the curse ofdimensionality.
A natural solution is to apply a di-mensionality reduction technique to find a compactrepresentation that captures as much information aspossible from the original input.
Here, we considerprincipal components analysis (PCA), a ubiquitouslinear dimensionality reduction technique, as well asword2vec (Mikolov et al, 2013), a technique to learnnonlinear word representations.We consider the following views for each user.BOW: We take the bag-of-words (both count and TF-IDF weighted) representation of all tweets made byusers in that view (ego, mention, friend, or follower)following the above pre-processing.BOW-PCA: We run PCA and extract the top princi-pal components for each of the above views.
We alsoconsider all possible combinations of views obtainedby concatenating views before applying PCA, and con-catenating PCA-projected views.
By considering allpossible concatenation of views, we ensure that thismethod has access to the same information as multi-view methods.
Both the raw BOW and BOW-PCA rep-resentations have been explored in previous work fordemographic prediction (Volkova et al, 2014; Al Za-mal et al, 2012) and recommendation systems (Abel etal., 2011; Zangerle et al, 2013).Word2Vec: BOW-PCA is limited to linear representa-tions of BOW features.
Modern neural network basedapproaches to learning word embeddings, includingword2vec continuous bag of words and skipgram mod-els, can learn nonlinear representations that also cap-ture local context around each word (Mikolov et al,2013).
We represent each view as the simple averageof the word embeddings for all tokens within that view(e.g., all words written by the ego user).
Word em-beddings are learned on a sample of 87,755,398 tweetsand profiles uniformly sampled from the 1% Twitterstream in April 2015 along with all the tweets/profilescollected for our set of users ?
a total of over a billiontokens.
We use the word2vec tool, select either skip-gram or continuous bag-of-words embeddings on devdata for each prediction task, and train for 50 epochs.We use the default settings for all other parameters.NetSim: An alternative to text based representationsis to use the social network of users as a representation.We encode a user?s social network as a vector by treat-ing the users as a vocabulary, where users with simi-lar social networks have similar vector representations(NetSim).
An n-dimensional vector then encodes theuser?s social network as a bag-of-words over this uservocabulary.
In other words, a user is represented bya summation of the one-hot encodings of each neigh-boring user in their social network.
In this representa-tion, the number of friends two users have in commonis equal to the dot product between their social networkvectors.
We define the social network may be as one?sfollowers, friends, or the union of both.
The motiva-tion behind this representation is that users who havesimilar networks may behave in similar ways.
Suchnetwork features are commonly used to construct userrepresentations as well as to make user recommenda-tions (Lu et al, 2012; Kywe et al, 2012).NetSim-PCA: The PCA-projected representationsfor each NetSim vector.
This may be important forcomputing similarity, since users are now representedas dense vectors capturing linear correlations in thefriends/followers a user has.
NetSim-PCA is to NetSimas BOW-PCA is to BOW?
we apply PCA directly to theuser?s social network as opposed to the BOW represen-tations of users in that network.Each of these views can be treated independently asa user representation.
However, different downstreamtasks may favor different views.
For example, thefriend network is useful at recommending new friends,whereas the ego tweet view may be better at predict-ing what content a user will post in the future.
Pick-ing a single view may ignore valuable information asviews may contain complementary information, so us-ing multiple views improves on a single view.
One ap-proach is to concatenate multiple views together, butthis further increases the size of the user embeddings.In the next section, we propose an alternate approachfor learning a single embedding from multiple views.4 Learning Multiview User EmbeddingsWe use Generalized Canonical Correlation Analysis(GCCA) (Carroll, 1968) to learn a single embeddingfrom multiple views.
GCCA finds G,Uithat minimize:arg minG,Ui?i?G?XiUi?2Fs.t.
G?G = I (1)where Xi?
Rn?dicorresponds to the data matrix forthe ith view, Ui?
Rdi?kmaps from the latent spaceto observed view i, and G ?
Rn?kcontains all userrepresentations (Van De Velden and Bijmolt, 2006).15Since each view may be more or less helpful for adownstream task, we do not want to treat each viewequally in learning a single embedding.
Instead, weweigh each view differently in the objective:arg minG,Ui?iwi?G?XiUi?2Fs.t.
G?G = I, wi?
0 (2)where wiexplicitly expresses the importance of the ithview in determining the joint embedding.
The columnsof G are the eigenvectors of?iwiXi(X?iXi)?1X?i,and the solution for Ui= (X?iXi)?1X?iG.
In our ex-periments, we use the approach of Rastogi et al (2015)to learn G and Ui, since it is more memory-efficientthan decomposing the sum of projection matrices.GCCA embeddings were learned over combinationsof the views in ?3.
When available, we also considerGCCA-net, where in addition to the four text views, wealso include the follower and friend network views usedby NetSim-PCA.
For computational efficiency, each ofthese views was first reduced in dimensionality by pro-jecting its BOW TF-IDF-weighted representation to a1000-dimensional vector through PCA.2We add anidentity matrix scaled by a small amount of regulariza-tion, 10?8, to the per-view covariance matrices beforeinverting, for numerical stability, and use the formula-tion of GCCA reported in Van De Velden and Bijmolt(2006), which ignores rows with missing data (someusers had no data in the mention tweet view and someusers accounts were private).
We tune the weightingof each view i, wi?
{0.0, 0.25, 1.0}, discriminativelyfor each task, although the GCCA objective is unsuper-vised once the wiare fixed.We also consider a minor modification of GCCA,where G is scaled by the square-root of the singular val-ues of?iwiXiX?i, GCCA-sv.
This is inspired by pre-vious work showing that scaling each feature of multi-view embeddings by the singular values of the data ma-trix can improve performance at downstream tasks suchas image or caption retrieval (Mroueh et al, 2015).Note that if we only consider a single view, X1, withweight w1= 1, then the solution to GCCA-sv is iden-tical to the PCA solution for data matrix X1, withoutmean-centering.When we compare representations in the fol-lowing tasks, we sweep over embedding widthin {10, 20, 50, 100, 200, 300, 400, 500, 1000} for allmethods.
This applies to GCCA, BOW-PCA, NetSim-PCA, and Word2Vec.
We also consider concatena-tions of vectors for every possible subset of views:singletons, pairs, triples, and all views.
We tried ap-plying PCA directly to the concatenation of all 1000-dimensional BOW-PCA views, but this did not performcompetitively in our experiments.2We excluded count vectors from the GCCA experimentsfor computational efficiency since they performed similarlyto TF-IDF representations in initial experiments.5 Experimental SetupWe selected three user prediction tasks to demonstratethe effectiveness of the multi-view embeddings: userengagement prediction, friend recommendation anddemographic characteristics inference.
Our focus is toshow the performance of multiview embeddings com-pared to other representations, not on building the bestsystem for a given task.User Engagement Prediction The goal of user en-gagement prediction is to determine which topics a userwill likely tweet about, using hashtag as a proxy.
Thistask is similar to hashtag recommendation for a tweetbased on its contents (Kywe et al, 2012; She and Chen,2014; Zangerle et al, 2013).
Purohit et al (2011) pre-sented a supervised task to predict if a hashtag wouldappear in a tweet using features from the user?s net-work, previous tweets, and the tweet?s content.We selected the 400 most frequently used hashtagsin messages authored by our users and which first ap-peared in March 2015, randomly and evenly dividingthem into dev and test sets.
We held out the first 10users who tweeted each hashtag as exemplars of usersthat would use the hashtag in the future.
We ranked allother users by the cosine distance of their embeddingto the average embedding of these 10 users.
Since em-beddings are learned on data pre-March 2015, the hash-tags cannot impact the learned representations.
Perfor-mance is measured using precision and recall at k, aswell as mean reciprocal rank (MRR), where a user ismarked as correct if they used the hashtag.
Note thatthis task is different than that reported in Purohit et al(2011), since we are making recommendations at thelevel of users, not tweets.Friend Recommendation The goal of friend rec-ommendation/link prediction is to recommend/predictother accounts for a user to follow (Liben-Nowell andKleinberg, 2007).We selected the 500 most popular accounts ?
whichwe call celebrities ?
followed by our users, randomly,and evenly divided them into dev and test sets.
Werandomly select 10 users who follow each celebrityand rank all other users by cosine distance to the av-erage of these 10 representations.
The tweets of se-lected celebrities are removed during embedding train-ing so as not to influence the learned representations.We use the same evaluation as user engagement pre-diction, where a user is marked as correct if they followthe given celebrity.For both user engagement prediction and friend rec-ommendation we z-score normalize each feature, sub-tracting off the mean and scaling each feature indepen-dently to have unit variance, before computing cosinesimilarity.
We select the approach and whether to z-score normalize based on the development set perfor-mance.Demographic Characteristics Inference Our finaltask is to infer the demographic characteristics of a user(Al Zamal et al, 2012; Chen et al, 2015).16Model Dim P@1000 R@1000 MRRBOW 20000 0.009/0.005 0.241/0.157 0.006/0.006BOW-PCA 500 0.011/0.008 0.312/0.29 0.007/0.009NetSim NA 0.006/0.006 0.159/0.201 0.004/0.004NetSim-PCA 300 0.010/0.008 0.293/0.299 0.006/0.006Word2Vec 100 0.009/0.007 0.254/0.217 0.005/0.004GCCA 100 0.012/0.009 0.357/0.325 0.011/0.008GCCA-sv 500 0.012/0.010 0.359/0.334 0.010/0.011GCCA-net 200 0.013/0.009 0.360/0.346 0.011/0.011NetSize NA 0.001/0.001 0.012/0.012 0.000/0.000Random NA 0.000/0.000 0.002/0.008 0.000/0.000Table 1: Macro performance at user engagement predictionon dev/test.
Ranking of model performance was consistentacross metrics.
Precision is low since few users tweet a givenhashtag.
Values bolded by best test performance per metric.Baselines (bottom): NetSize: a ranking of users by the size oftheir local network; Random randomly ranks users.0 200 400 600 800 1000k0.000.020.040.060.080.100.120.140.16Micro Precision0 200 400 600 800 1000k0.000.010.020.030.040.050.060.07MacroPrecision0 200 400 600 800 1000k0.000.050.100.150.200.25Micro RecallBOWBOW-PCANetSimNetSim-PCAWord2VecGCCAGCCA-svGCCA-netNetSize0 200 400 600 800 1000k0.000.050.100.150.200.250.300.35MacroRecallFigure 1: The best-performing approaches on user engage-ment prediction as a function of k (number of recommenda-tions).
The ordering of methods is consistent across k.We use the dataset from Volkova et al (2014;Volkova (2015) which annotates 383 users for age(old/young), 383 for gender (male/female), and 396 po-litical affiliation (republican/democrat), with balancedclasses.
Predicting each characteristic is a binary su-pervised prediction task.
Each set is partitioned into 10folds, with two folds held out for test, and the othereight for tuning via cross-fold validation.
The pro-vided dataset contained tweets from each user, men-tioned users, friends and follower networks.
It did notcontain the actual social networks for these users, so wedid not evaluate NetSim, NetSim-PCA, or GCCA-net atthese prediction tasks.Each feature was z-score normalized before beingpassed to a linear-kernel SVM where we swept over10?4, .
.
.
, 104for the penalty on the error term, C.6 ResultsUser Engagement Prediction Table 1 shows resultsfor user engagement prediction and Figure 1 the preci-sion and recall curves as a function of number of rec-ommendations.
GCCA outperforms other methods forprecision and recall at 1000, and does close to the bestin terms of MRR.
Including network views (GCCA-E M Fr E+Fr E+M M+Fr E+M+FrE+M+Fr+FolViews0.000.050.100.150.200.250.300.35Macro R@1000GCCABOW-PCAFigure 2: Macro recall@1000 on user engagement predictionfor different combinations of text views.
Each bar shows thebest-performing model swept over dimensionality.
E: ego,M: mention, Fr: friend, Fol: followertweet views.Model Dim P@1000 R@1000 MRRBOW 20000 0.133/0.153 0.043/0.048 0.000/0.001BOW-PCA 20 0.311/0.314 0.101/0.102 0.001/0.001NetSim NA 0.406/0.420 0.131/0.132 0.002/0.002NetSim-PCA 500 0.445/0.439 0.149/0.147 0.002/0.002Word2Vec 200 0.260/0.249 0.084/0.080 0.001/0.001GCCA 50 0.269/0.276 0.089/0.091 0.001/0.001GCCA-sv 500 0.445/0.439 0.149/0.147 0.002/0.002GCCA-net 20 0.376/0.364 0.123/0.120 0.001/0.001NetSize NA 0.033/0.035 0.009/0.010 0.000/0.000Random NA 0.034/0.036 0.010/0.010 0.000/0.000Table 2: Macro performance for friend recommendation.Performance of NetSim-PCA and GCCA-sv are identicalsince the view weighting for GCCA-sv only selected solelythe friend view.
Thus, these methods learned identical userembeddings.Model age gender politicsBOW 0.771/0.740 0.723/0.662 0.934/0.975BOW-PCA 0.784/0.649 0.719/0.662 0.908/0.900BOW-PCA + BOW 0.767/0.688 0.660/0.714 0.937/0.9875GCCA 0.725/0.740 0.742/0.714 0.899/0.8125GCCA + BOW 0.764/0.727 0.657/0.701 0.940/0.9625GCCA-sv 0.709/0.636 0.699/0.714 0.871/0.850GCCA-sv + BOW 0.761/0.688 0.647/0.675 0.937/0.9625Word2Vec 0.790/0.753 0.777/0.766 0.927/0.938Table 3: Average CV/test accuracy for inferring demo-graphic characteristics.net and GCCA-sv) improves the performance further.The best performing GCCA setting placed weight 1on the ego tweet view, mention view, and friend view,while BOW-PCA concatenated these views, suggestingthat these were the three most important views but thatGCCA was able to learn a better representation.
Figure2 compares performance of different view subsets forGCCA and BOW-PCA, showing that GCCA uses infor-mation from multiple views more effectively for pre-dicting user engagement.Friend Recommendation Table 2 shows results forfriend prediction and Figure 3 similarly shows that per-formance differences between approaches are consis-tent across k (number of recommendations.)
Addingnetwork views to GCCA, GCCA-net, improves per-formance, although it cannot contend with NetSim or170 200 400 600 800 1000k0.00.10.20.30.40.50.60.70.8Micro Precision0 200 400 600 800 1000k0.00.10.20.30.40.50.60.70.8MacroPrecision0 200 400 600 800 1000k0.000.020.040.060.080.100.120.14Micro RecallBOWBOW-PCANetSimNetSim-PCAWord2VecGCCAGCCA-svGCCA-netNetSize0 200 400 600 800 1000k0.000.020.040.060.080.100.120.140.16MacroRecallFigure 3: Performance on friend recommendation varying k.NetSim-PCA, although GCCA-sv is able to meet theperformance of NetSim-PCA.
The best GCCA placednon-zero weight on the friend tweets view, and GCCA-net only places weight on the friend network view;the other views were not informative.
BOW-PCA andWord2Vec only used the friend tweet view.
This sug-gests that the friend view is the most important forthis task, and multiview techniques cannot exploit ad-ditional views to improve performance.
GCCA-sv per-forms identically to GCCA-net, since it only placedweight on the friend network view, learning identicalembeddings to GCCA-net.Demographic Characteristics Prediction Table 3shows the average cross-fold validation and test ac-curacy on the demographic prediction task.
GCCA +BOW and BOW-PCA + BOW are the concatenationof BOW features with GCCA and BOW-PCA, respec-tively.
The wide variation in performance is due tothe small size of the datasets, thus it?s hard to drawmany conclusions other than that GCCA seems to per-form well compared to other linear methods.
Word2Vecsurpasses other representations in two out of threedatasets.It is difficult to compare the performance of themethods we evaluate here to that reported in previouswork, (Al Zamal et al, 2012).
This is because they re-port cross-fold validation accuracy (not test), they con-sider a wider range of hand-engineered features, differ-ent subsets of networks, radial basis function kernelsfor SVM, and find that accuracy varies wildly acrossdifferent feature sets.
They report cross-fold validationaccuracy ranging from 0.619 to 0.805 for predictingage, 0.560 to 0.802 for gender, and 0.725 to 0.932 forpolitics.7 ConclusionWe have proposed several representations of Twitterusers, as well as a multiview approach that combinesthese views into a single embedding.
Our multiviewembeddings achieve promising results on three differ-ent prediction tasks, making use of both what a userwrites as well as the social network.
We found that eachtask relied on different information, which our methodsuccessfully combined into a single representation.We plan to consider other means for learning userrepresentations, including comparing nonlinear dimen-sionality reduction techniques such as kernel PCA(Sch?olkopf et al, 1997) and deep canonical correlationanalysis (Andrew et al, 2013; Wang et al, 2015).
Re-cent work on learning user representations with mul-titask deep learning techniques (Li et al, 2015), sug-gests that learning a nonlinear mapping from observedviews to the latent space can learn high quality userrepresentations.
One issue with GCCA is scalabil-ity: solving for G relies on an SVD of a large ma-trix that must be loaded into memory.
Online variantsof GCCA would allow this method to scale to largertraining sets and incrementally update representations.The PCA-reduced views for all 102,328 Twitter userscan be found here: http://www.dredze.com/datasets/multiview_embeddings/.AcknowledgementsThis research was supported in part by NSF BIGDATAgrant IIS-1546482 and a gift from Bloomberg LP.ReferencesFabian Abel, Qi Gao, Geert-Jan Houben, and Ke Tao.2011.
Analyzing user modeling on twitter for per-sonalized news recommendations.
In User Model-ing, Adaption and Personalization - 19th Interna-tional Conference, UMAP 2011, Girona, Spain, July11-15, 2011.
Proceedings, pages 1?12.Faiyaz Al Zamal, Wendy Liu, and Derek Ruths.
2012.Homophily and latent attribute inference: Inferringlatent attributes of twitter users from neighbors.
InInternation Conference on Weblogs and Social Me-dia (ICWSM).Galen Andrew, Raman Arora, Jeff Bilmes, and KarenLivescu.
2013.
Deep canonical correlation analy-sis.
In International Conference on Machine Learn-ing (ICML), pages 1247?1255.Raman Arora and Karen Livescu.
2014.
Multi-viewlearning with supervision for transformed bottleneckfeatures.
In Acoustics, Speech and Signal Process-ing (ICASSP), 2014 IEEE International Conferenceon, pages 2499?2503.
IEEE.J Douglas Carroll.
1968.
Generalization of canonicalcorrelation analysis to three or more sets of variables.In Convention of the American Psychological Asso-ciation, volume 3, pages 227?228.Xin Chen, Yu Wang, Eugene Agichtein, and FushengWang.
2015.
A comparative study of demographicattribute inference in twitter.
In Conference on We-blogs and Social Media (ICWSM).18Manaal Faruqui and Chris Dyer.
2014.
Improvingvector space word representations using multilin-gual correlation.
In Proceedings of the 14th Confer-ence of the European Chapter of the Association forComputational Linguistics, EACL 2014, April 26-30, 2014, Gothenburg, Sweden, pages 462?471.Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov,Richard S. Zemel, Antonio Torralba, Raquel Urta-sun, and Sanja Fidler.
2015.
Skip-thought vectors.CoRR, abs/1506.06726.Su Mon Kywe, Tuan-Anh Hoang, Ee-Peng Lim, andFeida Zhu.
2012.
On recommending hashtags intwitter networks.
In Social Informatics - 4th Interna-tional Conference, SocInfo 2012, Lausanne, Switzer-land, December 5-7, 2012.
Proceedings, pages 337?350.Quoc V Le and Tomas Mikolov.
2014.
Distributedrepresentations of sentences and documents.
In In-ternation Conference on Machine Learning (ICML).Jiwei Li, Alan Ritter, and Dan Jurafsky.
2015.Learning multi-faceted representations of individu-als from heterogeneous evidence using neural net-works.
arXiv preprint arXiv:1510.05198.David Liben-Nowell and Jon Kleinberg.
2007.
Thelink-prediction problem for social networks.
Journalof the American society for information science andtechnology, 58(7):1019?1031.Chunliang Lu, Wai Lam, and Yingxiao Zhang.
2012.Twitter user modeling and tweets recommendationbased on wikipedia concept graph.
In IJCAI Work-shop on Intelligent Techniques for Web Personaliza-tion and Recommender Systems.Marco Lui and Timothy Baldwin.
2012. langid.
py:An off-the-shelf language identification tool.
In As-sociation for Computational Linguistics (ACL): sys-tem demonstrations, pages 25?30.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in neural information processingsystems (NIPS), pages 3111?3119.Youssef Mroueh, Etienne Marcheret, and VaibhavaGoel.
2015.
Asymmetrically weighted cca and hier-archical kernel sentence embedding for multimodalretrieval.
arXiv preprint arXiv:1511.06267.Hemant Purohit, Yiye Ruan, Amruta Joshi, SrinivasanParthasarathy, and Amit Sheth.
2011.
Understand-ing user-community engagement by multi-facetedfeatures: A case study on twitter.
In WWW Work-shop on Social Media Engagement (SoME).Pushpendre Rastogi, Benjamin Van Durme, and RamanArora.
2015.
Multiview lsa: Representation learn-ing via generalized cca.
In North American Associ-ation for Computational Linguistics (NAACL).Bernhard Sch?olkopf, Alexander J. Smola, and Klaus-Robert M?uller.
1997.
Kernel principal componentanalysis.
In Artificial Neural Networks - ICANN ?97,7th International Conference, Lausanne, Switzer-land, October 8-10, 1997, Proceedings, pages 583?588.Jieying She and Lei Chen.
2014.
Tomoha: Topicmodel-based hashtag recommendation on twitter.In International conference on World wide web(WWW), pages 371?372.
International World WideWeb Conferences Steering Committee.Michel Van De Velden and Tammo HA Bijmolt.
2006.Generalized canonical correlation analysis of matri-ces with missing rows: a simulation study.
Psy-chometrika, 71(2):323?331.Svitlana Volkova, Glen Coppersmith, and BenjaminVan Durme.
2014.
Inferring user political pref-erences from streaming communications.
In Asso-ciation for Computational Linguistics (ACL), pages186?196.Svitlana Volkova.
2015.
Predicting Demographics andAffect in Social Networks.
Ph.D. thesis, Johns Hop-kins University, October.Weiran Wang, Raman Arora, Karen Livescu, and JeffBilmes.
2015.
On deep multi-view representationlearning.
In Proceedings of the 32nd InternationalConference on Machine Learning (ICML-15), pages1083?1092.Eva Zangerle, Wolfgang Gassler, and G?unther Specht.2013.
On the impact of text similarity functionson hashtag recommendations in microblogging en-vironments.
Social Network Analysis and Mining,3(4):889?898.19
