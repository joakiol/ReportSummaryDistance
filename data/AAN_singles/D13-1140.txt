Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387?1392,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsDecoding with Large-Scale Neural Language ModelsImproves TranslationAshish VaswaniUniversity of Southern CaliforniaDepartment of Computer Scienceavaswani@isi.eduYinggong ZhaoNanjing University, State Key Laboratoryfor Novel Software Technologyzhaoyg@nlp.nju.edu.cnVictoria Fossum and David ChiangUniversity of Southern CaliforniaInformation Sciences Institute{vfossum,chiang}@isi.eduAbstractWe explore the application of neural languagemodels to machine translation.
We develop anew model that combines the neural proba-bilistic language model of Bengio et al rec-tified linear units, and noise-contrastive esti-mation, and we incorporate it into a machinetranslation system both by reranking k-bestlists and by direct integration into the decoder.Our large-scale, large-vocabulary experimentsacross four language pairs show that our neu-ral language model improves translation qual-ity by up to 1.1 Bleu.1 IntroductionMachine translation (MT) systems rely upon lan-guage models (LMs) during decoding to ensure flu-ent output in the target language.
Typically, theseLMs are n-gram models over discrete representa-tions of words.
Such models are susceptible to datasparsity?that is, the probability of an n-gram ob-served only few times is difficult to estimate reli-ably, because these models do not use any informa-tion about similarities between words.To address this issue, Bengio et al(2003) pro-pose distributed word representations, in which eachword is represented as a real-valued vector in ahigh-dimensional feature space.
Bengio et al(2003)introduce a feed-forward neural probabilistic LM(NPLM) that operates over these distributed repre-sentations.
During training, the NPLM learns both adistributed representation for each word in the vo-cabulary and an n-gram probability distribution overwords in terms of these distributed representations.Although neural LMs have begun to rival or evensurpass traditional n-gram LMs (Mnih and Hin-ton, 2009; Mikolov et al 2011), they have not yetbeen widely adopted in large-vocabulary applica-tions such as MT, because standard maximum like-lihood estimation (MLE) requires repeated summa-tions over all words in the vocabulary.
A variety ofstrategies have been proposed to combat this issue,many of which require severe restrictions on the sizeof the network or the size of the data.In this work, we extend the NPLM of Bengio etal.
(2003) in two ways.
First, we use rectified lin-ear units (Nair and Hinton, 2010), whose activa-tions are cheaper to compute than sigmoid or tanhunits.
There is also evidence that deep neural net-works with rectified linear units can be trained suc-cessfully without pre-training (Zeiler et al 2013).Second, we train using noise-contrastive estimationor NCE (Gutmann and Hyva?rinen, 2010; Mnih andTeh, 2012), which does not require repeated summa-tions over the whole vocabulary.
This enables us toefficiently build NPLMs on a larger scale than wouldbe possible otherwise.We then apply this LM to MT in two ways.
First,we use it to rerank the k-best output of a hierarchi-cal phrase-based decoder (Chiang, 2007).
Second,we integrate it directly into the decoder, allowing theneural LM to more strongly influence the model.
Weachieve gains of up to 0.6 Bleu translating French,German, and Spanish to English, and up to 1.1 Bleuon Chinese-English translation.1387u1 u2inputwordsinputembeddingshiddenh1hiddenh2outputP(w | u)D?MC1 C2DFigure 1: Neural probabilistic language model (Bengio etal., 2003).2 Neural Language ModelsLet V be the vocabulary, and n be the order ofthe language model; let u range over contexts, i.e.,strings of length (n?1), and w range over words.
Forsimplicity, we assume that the training data is a sin-gle very long string, w1 ?
?
?wN , where wN is a specialstop symbol, </s>.
We write ui for wi?n+1 ?
?
?wi?1,where, for i ?
0, wi is a special start symbol, <s>.2.1 ModelWe use a feedforward neural network as shown inFigure 1, following Bengio et al(2003).
The inputto the network is a sequence of one-hot represen-tations of the words in context u, which we writeu j (1 ?
j ?
n ?
1).
The output is the probabilityP(w | u) for each word w, which the network com-putes as follows.The hidden layers consist of rec-tified linear units (Nair and Hinton,2010), which use the activation func-tion ?
(x) = max(0, x) (see graph atright).The output of the first hidden layer h1 ish1 = ?????????
?n?1?j=1C jDu j?????????
(1)where D is a matrix of input word embeddingswhich is shared across all positions, the C j are thecontext matrices for each word in u, and ?
is appliedelementwise.
The output of the second layer h2 ish2 = ?
(Mh1) ,where M is the matrix of connection weights be-tween h1 and h2.
Finally, the output layer is a soft-max layer,P(w | u) ?
exp(D?h2 + b)(2)where D?
is the output word embedding matrix and bis a vector of biases for every word in the vocabulary.2.2 TrainingThe typical way to train neural LMs is to maximizethe likelihood of the training data by gradient ascent.But the softmax layer requires, at each iteration, asummation over all the units in the output layer, thatis, all words in the whole vocabulary.
If the vocabu-lary is large, this can be prohibitively expensive.Noise-contrastive estimation or NCE (Gutmannand Hyva?rinen, 2010) is an alternative estimationprinciple that allows one to avoid these repeatedsummations.
It has been applied previously to log-bilinear LMs (Mnih and Teh, 2012), and we apply ithere to the NPLM described above.We can write the probability of a word w given acontext u under the NPLM asP(w | u) =1Z(u)p(w | u)p(w | u) = exp(D?h2 + b)Z(u) =?w?p(w?
| u) (3)where p(w | u) is the unnormalized output of the unitcorresponding to w, and Z(u) is the normalizationfactor.
Let ?
stand for the parameters of the model.One possibility would be to treat Z(u), instead ofbeing defined by (3), as an additional set of modelparameters which are learned along with ?.
But it iseasy to see that we can make the likelihood arbitrar-ily large by making the Z(u) arbitrarily small.In NCE, we create a noise distribution q(w).For each example uiwi, we add k noise samplesw?i1, .
.
.
, w?ik into the data, and extend the model toaccount for noise samples by introducing a random1388variable C which is 1 for training examples and 0 fornoise samples:P(C = 1,w | u) =11 + k?1Z(u)p(w | u)P(C = 0,w | u) =k1 + k?
q(w).We then train the model to classify examples astraining data or noise, that is, to maximize the con-ditional likelihood,L =N?i=1(log P(C = 1 | uiwi) +k?j=1log P(C = 0 | uiw?i j))with respect to both ?
and Z(u).We do this by stochastic gradient ascent.
The gra-dient with respect to ?
turns out to be?L?
?=N?i=1(P(C = 0 | uiwi)??
?log p(wi | ui) ?k?j=1P(C = 1 | uiw?i j)??
?log p(w?i j | ui))and similarly for the gradient with respect to Z(u).These can be computed by backpropagation.
Unlikebefore, the Z(u) will converge to a value that normal-izes the model, satisfying (3), and, under appropriateconditions, the parameters will converge to a valuethat maximizes the likelihood of the data.3 ImplementationBoth training and scoring of neural LMs are compu-tationally expensive at the scale needed for machinetranslation.
In this section, we describe some of thetechniques used to make them practical for transla-tion.3.1 TrainingDuring training, we compute gradients on an en-tire minibatch at a time, allowing the use of matrix-matrix multiplications instead of matrix-vector mul-tiplications (Bengio, 2012).
We represent the inputsas a sparse matrix, allowing the computation of theinput layer (1) to use sparse matrix-matrix multi-plications.
The output activations (2) are computedonly for the word types that occur as the positive ex-ample or one of the noise samples, yielding a sparsematrix of outputs.
Similarly, during backpropaga-tion, sparse matrix multiplications are used at boththe output and input layer.In most of these operations, the examples in aminibatch can be processed in parallel.
However, inthe sparse-dense products used when updating theparameters D and D?, we found it was best to di-vide the vocabulary into blocks (16 per thread) andto process the blocks in parallel.3.2 TranslationTo incorporate this neural LM into a MT system, wecan use the LM to rerank k-best lists, as has beendone in previous work.
But since the NPLM scoresn-grams, it can also be integrated into a phrase-basedor hierarchical phrase-based decoder just as a con-ventional n-gram model can, unlike a RNN.The most time-consuming step in computing n-gram probabilities is the computation of the nor-malization constants Z(u).
Following Mnih and Teh(2012), we set althe normalization constants to oneduring training, so that the model learns to produceapproximately normalized probabilities.
Then, whenapplying the LM, we can simply ignore normaliza-tion.
A similar strategy was taken by Niehues andWaibel (2012).
We find that a single n-gram lookuptakes about 40 ?s.The technique, described above, of grouping ex-amples into minibatches works for scoring of k-bestlists, but not while decoding.
But caching n-gramprobabilities helps to reduce the cost of the manylookups required during decoding.A final issue when decoding with a neural LMis that, in order to estimate future costs, we needto be able to estimate probabilities of n?-grams forn?
< n. In conventional LMs, this information isreadily available,1 but not in NPLMs.
Therefore, wedefined a special word <null> whose embedding isthe weighted average of the (input) embeddings ofall the other words in the vocabulary.
Then, to esti-mate the probability of an n?-gram u?w, we used theprobability of P(w | <null>n?n?u?
).1However, in Kneser-Ney smoothed LMs, this informationis also incorrect (Heafield et al 2012).1389setting dev 2004 2005 2006baseline 38.2 38.4 37.7 34.3reranking 38.5 38.6 37.8 34.7decoding 39.1 39.5 38.8 34.9Table 1: Results for Chinese-English experiments, with-out neural LM (baseline) and with neural LM for rerank-ing and integrated decoding.
Reranking with the neuralLM improves translation quality, while integrating it intothe decoder improves even more.4 ExperimentsWe ran experiments on four language pairs ?
Chi-nese to English and French, German, and Spanishto English ?
using a hierarchical phrase-based MTsystem (Chiang, 2007) and GIZA++ (Och and Ney,2003) for word alignments.For all experiments, we used four LMs.
The base-lines used conventional 5-gram LMs, estimated withmodified Kneser-Ney smoothing (Chen and Good-man, 1998) on the English side of the bitext and the329M-word Xinhua portion of English Gigaword(LDC2011T07).
Against these baselines, we testedsystems that included the two conventional LMs aswell as two 5-gram NPLMs trained on the samedatasets.
The Europarl bitext NPLMs had a vocab-ulary size of 50k, while the other NPLMs had a vo-cabulary size of 100k.
We used 150 dimensions forword embeddings, 750 units in hidden layer h1, and150 units in hidden layer h2.
We initialized the net-work parameters uniformly from (?0.01, 0.01) andthe output biases to ?
log |V |, and optimized them by10 epochs of stochastic gradient ascent, using mini-batches of size 1000 and a learning rate of 1.
Wedrew 100 noise samples per training example fromthe unigram distribution, using the alias method forefficiency (Kronmal and Peterson, 1979).We trained the discriminative models with MERT(Och, 2003) and the discriminative rerankers on1000-best lists with MERT.
Except where noted, weran MERT three times and report the average score.We evaluated using case-insensitive NIST Bleu.4.1 NIST Chinese-EnglishFor the Chinese-English task (Table 1), the trainingdata came from the NIST 2012 constrained track,excluding sentences longer than 60 words.
RulesFr-En De-En Es-Ensetting dev test dev test dev testbaseline 33.5 25.5 28.8 21.5 33.5 32.0reranking 33.9 26.0 29.1 21.5 34.1 32.2decoding 34.12 26.12 29.3 21.9 34.22 32.12Table 2: Results for Europarl MT experiments, withoutneural LM (baseline) and with neural LM for rerankingand integrated decoding.
The neural LM gives improve-ments across three different language pairs.
Superscript 2indicates a score averaged between two runs; all otherscores were averaged over three runs.without nonterminals were extracted from all train-ing data, while rules with nonterminals were ex-tracted from the FBIS corpus (LDC2003E14).
Weran MERT on the development data, which was theNIST 2003 test data, and tested on the NIST 2004?2006 test data.Reranking using the neural LM yielded improve-ments of 0.2?0.4 Bleu, while integrating the neuralLM yielded larger improvements, between 0.6 and1.1 Bleu.4.2 EuroparlFor French, German, and Spanish translation, weused a parallel text of about 50M words from Eu-roparl v7.
Rules without nonterminals were ex-tracted from all the data, while rules with nonter-minals were extracted from the first 200k words.
Weran MERT on the development data, which was theWMT 2005 test data, and tested on the WMT 2006news commentary test data (nc-test2006).The improvements, shown in Table 2, were moremodest than on Chinese-English.
Reranking withthe neural LM yielded improvements of up to 0.5Bleu, and integrating the neural LM into the decoderyielded improvements of up to 0.6 Bleu.
In onecase (Spanish-English), integrated decoding scoredhigher than reranking on the development data butlower on the test data ?
perhaps due to the differ-ence in domain between the two.
On the other tasks,integrated decoding outperformed reranking.4.3 Speed comparisonWe measured the speed of training a NPLM by NCE,compared with MLE as implemented by the CSLMtoolkit (Schwenk, 2013).
We used the first 200k139010 20 30 40 50 60 7001,0002,0003,0004,000Vocabulary size (?1000)Trainingtime(s)CSLMNCE k = 1000NCE k = 100NCE k = 10Figure 2: Noise contrastive estimation (NCE) is muchfaster, and much less dependent on vocabulary size, thanMLE as implemented by the CSLM toolkit (Schwenk,2013).lines (5.2M words) of the Xinhua portion of Giga-word and timed one epoch of training, for variousvalues of k and |V |, on a dual hex-core 2.67 GHzXeon X5650 machine.
For these experiments, weused minibatches of 128 examples.
The timings areplotted in Figure 2.
We see that NCE is considerablyfaster than MLE; moreover, as expected, the MLEtraining time is roughly linear in |V |, whereas theNCE training time is basically constant.5 Related WorkThe problem of training with large vocabularies inNPLMs has received much attention.
One strategyhas been to restructure the network to be more hi-erarchical (Morin and Bengio, 2005; Mnih and Hin-ton, 2009) or to group words into classes (Le et al2011).
Other strategies include restricting the vocab-ulary of the NPLM to a shortlist and reverting to atraditional n-gram LM for other words (Schwenk,2004), and limiting the number of training examplesusing resampling (Schwenk and Gauvain, 2005) orselecting a subset of the training data (Schwenk etal., 2012).
Our approach can be efficiently appliedto large-scale tasks without limiting either the modelor the data.NPLMs have previously been applied to MT, mostnotably feed-forward NPLMs (Schwenk, 2007;Schwenk, 2010) and RNN-LMs (Mikolov, 2012).However, their use in MT has largely been limitedto reranking k-best lists for MT tasks with restrictedvocabularies.
Niehues and Waibel (2012) integrate aRBM-based language model directly into a decoder,but they only train the RBM LM on a small amountof data.
To our knowledge, our approach is the firstto integrate a large-vocabulary NPLM directly into adecoder for a large-scale MT task.6 ConclusionWe introduced a new variant of NPLMs that com-bines the network architecture of Bengio et al(2003), rectified linear units (Nair and Hinton,2010), and noise-contrastive estimation (Gutmannand Hyva?rinen, 2010).
This model is dramaticallyfaster to train than previous neural LMs, and can betrained on a large corpus with a large vocabulary anddirectly integrated into the decoder of a MT system.Our experiments across four language pairs demon-strated improvements of up to 1.1 Bleu.
Code fortraining and using our NPLMs is available for down-load.2AcknowledgementsWe would like to thank the anonymous reviewers fortheir very helpful comments.
This research was sup-ported in part by DOI IBC grant D12AP00225.
Thiswork was done while the second author was visit-ing USC/ISI supported by China Scholarship Coun-cil.
He was also supported by the Research Fund forthe Doctoral Program of Higher Education of China(No.
20110091110003) and the National Fundamen-tal Research Program of China (2010CB327903).ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research.Yoshua Bengio.
2012.
Practical recommendations forgradient-based training of deep architectures.
CoRR,abs/1206.5533.Stanley F. Chen and Joshua Goodman.
1998.
An empir-ical study of smoothing techniques for language mod-2http://nlg.isi.edu/software/nplm1391eling.
Technical Report TR-10-98, Harvard UniversityCenter for Research in Computing Technology.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201?228.Michael Gutmann and Aapo Hyva?rinen.
2010.
Noise-contrastive estimation: A new estimation principle forunnormalized statistical models.
In Proceedings ofAISTATS.Kenneth Heafield, Philipp Koehn, and Alon Lavie.
2012.Language model rest costs and space-efficient storage.In Proceedings of EMNLP-CoNLL, pages 1169?1178.Richard Kronmal and Arthur Peterson.
1979.
On thealias method for generating random variables froma discrete distribution.
The American Statistician,33(4):214?218.Hai-Son Le, Ilya Oparin, Alexandre Allauzen, Jean-LucGauvain, and Franc?ois Yvon.
2011.
Structured outputlayer neural network language model.
In Proceedingsof ICASSP, pages 5524?5527.Toma?s?
Mikolov, Anoop Deoras, Stefan Kombrink, Luka?s?Burget, and Jan ?Honza?
C?ernocky?.
2011.
Em-pirical evaluation and combination of advanced lan-guage modeling techniques.
In Proceedings of IN-TERSPEECH, pages 605?608.Toma?s?
Mikolov.
2012.
Statistical Language ModelsBased on Neural Networks.
Ph.D. thesis, Brno Uni-versity of Technology.Andriy Mnih and Geoffrey Hinton.
2009.
A scalablehierarchical distributed language model.
In Advancesin Neural Information Processing Systems.Andriy Mnih and Yee Whye Teh.
2012.
A fast and sim-ple algorithm for training neural probabilistic languagemodels.
In Proceedings of ICML.Frederic Morin and Yoshua Bengio.
2005.
Hierarchicalprobabilistic neural network language model.
In Pro-ceedings of AISTATS, pages 246?252.Vinod Nair and Geoffrey E. Hinton.
2010.
Rectified lin-ear units improve restricted Boltzmann machines.
InProceedings of ICML, pages 807?814.Jan Niehues and Alex Waibel.
2012.
Continuousspace language models using Restricted BoltzmannMachines.
In Proceedings of IWSLT.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL,pages 160?167.Holger Schwenk and Jean-Luc Gauvain.
2005.
Trainingneural network language models on very large corpora.In Proceedings of EMNLP.Holger Schwenk, Anthony Rousseau, and MohammedAttik.
2012.
Large, pruned or continuous space lan-guage models on a GPU for statistical machine trans-lation.
In Proceedings of the NAACL-HLT 2012 Work-shop: Will We Ever Really Replace the N-gramModel?On the Future of Language Modeling for HLT, pages11?19.Holger Schwenk.
2004.
Efficient training of large neuralnetworks for language modeling.
In Proceedings ofIJCNN, pages 3059?3062.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21:492?518.Holger Schwenk.
2010.
Continuous-space languagemodels for statistical machine translation.
Prague Bul-letin of Mathematical Linguistics, 93:137?146.Holger Schwenk.
2013.
CSLM - a modular open-sourcecontinuous space language modeling toolkit.
In Pro-ceedings of Interspeech.M.D.
Zeiler, M. Ranzato, R. Monga, M. Mao, K. Yang,Q.V.
Le, P. Nguyen, A.
Senior, V. Vanhoucke, J. Dean,and G.E.
Hinton.
2013.
On rectified linear units forspeech processing.
In Proceedings of ICASSP.1392
