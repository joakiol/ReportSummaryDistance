Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, pages 112?119,Athens, Greece, 31 March 2009. c?2009 Association for Computational LinguisticsContext-theoretic Semantics for Natural Language: an OverviewDaoud ClarkeUniversity of SussexFalmer, Brighton, UKdaoud.clarke@gmail.comAbstractWe present the context-theoretic frame-work, which provides a set of rules for thenature of composition of meaning basedon the philosophy of meaning as context.Principally, in the framework the composi-tion of the meaning of words can be repre-sented as multiplication of their represen-tative vectors, where multiplication is dis-tributive with respect to the vector space.We discuss the applicability of the frame-work to a range of techniques in natu-ral language processing, including subse-quence matching, the lexical entailmentmodel of Dagan et al (2005), vector-basedrepresentations of taxonomies, statisticalparsing and the representation of uncer-tainty in logical semantics.1 IntroductionTechniques such as latent semantic analysis (Deer-wester et al, 1990) and its variants have beenvery successful in representing the meanings ofwords as vectors, yet there is currently no theoryof natural language semantics that explains howwe should compose these representations: whatshould the representation of a phrase be, given therepresentation of the words in the phrase?
In thispaper we present such a theory, which is basedon the philosophy of meaning as context, as epit-omised by the famous sayings of Wittgenstein(1953), ?Meaning just is use?
and Firth (1957),?You shall know a word by the company it keeps?.For the sake of brevity we shall present only asummary of our research, which is described infull in (Clarke, 2007), and we give a simplifiedversion of the framework, which nevertheless suf-fices for the examples which follow.We believe that the development of theories thatcan take vector representations of meaning beyondthe word level, to the phrasal and sentence lev-els and beyond are essential for vector based se-mantics to truly compete with logical semantics,both in their academic standing and in applicationto real problems in natural language processing.Moreover the time is ripe for such a theory: neverhas there been such an abundance of immediatelyavailable textual data (in the form of the world-wide web) or cheap computing power to enablevector-based representations of meaning to be ob-tained.
The need to organise and understand thenew abundance of data makes these techniques allthe more attractive since meanings are determinedautomatically and are thus more robust in compar-ison to hand-built representations of meaning.
Aguiding theory of vector based semantics wouldundoubtedly be invaluable in the application ofthese representations to problems in natural lan-guage processing.The context-theoretic framework does not pro-vide a formula for how to compose meaning;rather it provides mathematical guidelines for the-ories of meaning.
It describes the nature of thevector space in which meanings live, gives somerestrictions on how meanings compose, and pro-vides us with a measure of the degree of entail-ment between strings for any implementation ofthe framework.The remainder of the paper is structured as fol-lows: in Section 2 we present the framework; inSection 3 we present applications of the frame-work:?
We describe subsequence matching (Section3.1) and the lexical entailment model of (Da-gan et al, 2005) (Section 3.2), both of whichhave been applied to the task of recognisingtextual entailment.?
We show how a vector based representationof a taxonomy incorporating probabilistic in-formation about word meanings can be con-112d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6 d1 d2 d3 d4 d5 d6orange fruit orange ?
fruitFigure 1: Vector representations of two terms ina space L1(S) where S = {d1, d2, d3, d4, d5, d6}and their vector lattice meet (the darker shadedarea).structed in Section 3.3.?
We show how syntax can be representedwithin the framework in Section 3.4.?
We summarise our approach to representinguncertainty in logical semantics in Section3.5.2 Context-theoretic FrameworkThe context-theoretic framework is based on theidea that the vector representation of the meaningof a word is derived from the contexts in which itoccurs.
However it extends this idea to strings ofany length: we assume there is some set S con-taining all the possible contexts associated withany string.
A context theory is an implementa-tion of the context-theoretic framework; a key re-quirement for a context theory is a mapping fromstrings to vectors formed from the set of contexts.In vector based techniques, the set of contextsmay be the set of possible dependency relationsbetween words, or the set of documents in whichstrings may occur; in context-theoretic semanticshowever, the set of ?contexts?
can be any set.We continue to refer to it as a set of contextssince the intuition and philosophy which forms thebasis for the framework derives from this idea;in practice the set may even consist of logicalsentences describing the meanings of strings inmodel-theoretic terms.An important aspect of vector-based techniquesis measuring the frequency of occurrence ofstrings in each context.
We model this in a gen-eral way as follows: let A be a set consisting ofthe words of the language under consideration.The first requirement of a context theory is a map-ping x 7?
x?
from a string x ?
A?
to a vectorx?
?
L1(S)+, where L1(S) means the set of allfunctions from S to the real numbers R which arefinite under the L1 norm,?u?1 =?s?S|u(s)|and L1(S)+ restricts this to functions to the non-negative real numbers, R+; these functions arecalled the positive elements of the vector spaceL1(S).
The requirement that the L1 norm is finite,and that the map is only to positive elements re-flects the fact that the vectors are intended to repre-sent an estimate of relative frequency distributionsof the strings over the contexts, since a frequencydistribution will always satisfy these requirements.Note also that the l1 norm of the context vector ofa string is simply the sum of all its componentsand is thus proportional to its probability.The set of functions L1(S) is a vector space un-der the point-wise operations:(?u)(s) = ?u(s)(u+ v)(s) = u(s) + v(s)for u, v ?
L1(S) and ?
?
R, but it is also a latticeunder the operations(u ?
v)(s) = min(u(s), v(s))(u ?
v)(s) = max(u(s), v(s)).In fact it is a vector lattice or Riesz space (Alipran-tis and Burkinshaw, 1985) since it satisfies the fol-lowing relationshipsif u ?
v then ?u ?
?vif u ?
v then u+ w ?
v + w,where ?
?
R+ and ?
is the partial ordering asso-ciated with the lattice operations, defined by u ?
vif u ?
v = u.Together with the l1 norm, the vector latticedefines an Abstract Lebesgue space (Abramovichand Aliprantis, 2002) a vector space incorporatingall the properties of a measure space, and thus canalso be thought of as defining a probability space,where ?
and ?
correspond to the union and inter-section of events in the ?
algebra, and the normcorresponds to the (un-normalised) probability.2.1 Distributional GeneralityThe vector lattice nature of the space under consid-eration is important in the context-theoretic frame-work since it is used to define a degree of entail-ment between strings.
Our notion of entailment is113based on the concept of distributional generality(Weeds et al, 2004), a generalisation of the distri-butional hypothesis of Harris (1985), in which itis assumed that terms with a more general mean-ing will occur in a wider array of contexts, anidea later developed by Geffet and Dagan (2005).Weeds et al (2004) also found that frequencyplayed a large role in determining the directionof entailment, with the more general term oftenoccurring more frequently.
The partial orderingof the vector lattice encapsulates these propertiessince x?
?
y?
if and only if y occurs more frequentlyin all the contexts in which x occurs.This partial ordering is a strict relationship,however, that is unlikely to exist between any twogiven vectors.
Because of this, we define a degreeof entailmentEnt(u, v) = ?u ?
v?1?u?1.This value has the properties of a conditional prob-ability; in the case of u = x?
and v = y?
it is ameasure of the degree to which the contexts stringx occurs in are shared by the contexts string y oc-curs in.2.2 MultiplicationThe map from strings to vectors already tells us ev-erything we need to know about the compositionof words: given two words x and y, we have theirindividual context vectors x?
and y?, and the mean-ing of the string xy is represented by the vectorx?y.
The question we address is what relationshipshould be imposed between the representation ofthe meanings of individual words x?
and y?
and themeaning of their composition x?y.
As it stands, wehave little guidance on what maps from strings tocontext vectors are appropriate.The first restriction we propose is that vectorrepresentations of meanings should be compos-able in their own right, without consideration ofwhat words they originated from.
In fact we placea strong requirement on the nature of multiplica-tion on elements: we require that the multiplica-tion ?
on the vector space defines a lattice-orderedalgebra.
This means that multiplication is asso-ciative, distributive with respect to addition, andsatisfies u ?
v ?
0 if u ?
0 and v ?
0, i.e.
theproduct of positive elements is also positive.We argue that composition of context vectorsneeds to be compatible with concatenation ofwords, i.e.x?
?
y?
= x?y,i.e.
the map from strings to context vectors definesa semigroup homomorphism.
Then the require-ment that multiplication is associative can be seento be a natural one since the homomorphism en-forces this requirement for context vectors.
Sim-ilarly since all context vectors are positive theirproduct in the algebra must also be positive, thus itis natural to extend this to all elements of the alge-bra.
The requirement for distributivity is justifiedby our own model of meaning as context in textcorpora, described in full elsewhere.2.3 Context TheoryThe above requirements give us all we need to de-fine a context theory.Definition 1 (Context theory).
?A,S, ?, ?
?
definesa context theory if L1(S) is a lattice-ordered al-gebra under the multiplication defined by ?
and ?defines a semigroup homomorphism x 7?
x?
fromA?
to L1(S)+.3 Context Theories for NaturalLanguageIn this section we describe applications of thecontext-theoretic framework to applications incomputational linguistics and natural languageprocessing.
We shall commonly use a construc-tion in which there is a binary operation ?
on Sthat makes it a semigroup.
In this case L1(S) is alattice-ordered algebra with convolution as multi-plication:(u ?
v)(r) =?s?t=ru(s)v(t)for r, s, t ?
S and u, v ?
L1(S).
We denote theunit basis element associated with an element x ?S by ex, that is ex(y) = 1 if and only if y = x,otherwise ex(y) = 0.3.1 Subsequence MatchingA string x ?
A?
is called a ?subsequence?
ofy ?
A?
if each element of x occurs in y in thesame order, but with the possibility of other ele-ments occurring in between, so for example abbais a subsequence of acabcba in {a, b, c}?.
We de-note the set of subsequences of x (including theempty string) by Sub(x).
Subsequence match-ing compares the subsequences of two strings: the114more subsequences they have in common the moresimilar they are assumed to be.
This idea hasbeen used successfully in text classification (Lodhiet al, 2002) and recognising textual entailment(Clarke, 2006).We can describe such models using a contexttheory ?A,A?, ?, ?
?, where ?
is convolution inL1(A?)
andx?
= (1/2|x|)?y?Sub(x)ey,i.e.
the context vector of a string is a weighted sumof its subsequences.
Under this context theory x?
?y?, i.e.
x completely entails y if x is a subsequenceof y.Many variations on this context theory are pos-sible, for example using more complex mappingsto L1(A?).
The context theory can also be adaptedto incorporate a measure of lexical overlap be-tween strings, an approach that, although simple,performs comparably to more complex techniquesin tasks such as recognising textual entailment(Dagan et al, 2005)3.2 Lexical Entailment ModelGlickman and Dagan (2005) define their ownmodel of entailment and apply it to the task ofrecognising textual entailment.
They estimateentailment between words based on occurrencesin documents: they estimate a lexical entailmentprobability LEP(x, y) between two terms x and yto beLEP(x, y) ?
nx,ynywhere ny and nx,y denote the number of docu-ments that the word y occurs in and the words xand y both occur in respectively.We can describe this using a context theory?A,D, ?, ?
?, where D is the set of documents, andx?
(d) ={1 if x occurs in document d0 otherwise.
.In this case the estimate of LEP(x, y) coincideswith our own degree of entailment Ent(x, y).There are many ways in which the multiplica-tion ?
can be defined on L1(D).
The simplest onedefines ed ?
ef = ed if d = f and edef = 0 oth-erwise.
The effect of multiplication of the contextvectors of two strings is then set intersection:(x??y?
)(d) ={1 if x and y occur in document d0 otherwise.Model Accuracy CWSDirichlet (106) 0.584 0.630Dirichlet (107) 0.576 0.642Bayer (MITRE) 0.586 0.617Glickman (Bar Ilan) 0.586 0.572Jijkoun (Amsterdam) 0.552 0.559Newman (Dublin) 0.565 0.6Table 1: Results obtained with our Latent Dirichletprojection model on the data from the first Recog-nising Textual Entailment Challenge for two doc-ument lengths N = 106 and N = 107 using a cut-off for the degree of entailment of 0.5 at whichentailment was regarded as holding.
CWS is theconfidence weighted score ?
see (Dagan et al,2005) for the definition.Glickman and Dagan (2005) do not use thismeasure, possibly because the problem of datasparseness makes it useless for long strings.
How-ever the measure they use can be viewed as an ap-proximation to this context theory.We have also used this idea to determine en-tailment, using latent Dirichlet alocation to getaround the problem of data sparseness.
A modelwas built using a subset of around 380,000 docu-ments from the Gigaword corpus, and the modelwas evaluated on the dataset from the first Recog-nising Textual Entailment Challenge; the resultsare shown in Table 1.
In order to use the model, adocument length had to be chosen; it was foundthat very long documents yielded better perfor-mance at this task.3.3 Representing TaxonomiesIn this section we describe how the relationshipsdescribed by a taxonomy, the collection of is-a relationships described by ontologies such asWordNet (Fellbaum, 1989), can be embedded inthe vector lattice structure that is crucial to thecontext-theoretic framework.
This opens up theway to the possibility of new techniques thatcombine the vector-based representations of wordmeanings with the ontological ones, for example:?
Semantic smoothing could be applied tovector based representations of an ontology,for example using distributional similaritymeasures to move words that are distribution-ally similar closer to each other in the vectorspace.
This type of technique may allow the115benefits of vector based techniques and on-tologies to be combined.?
Automatic classification: representing thetaxonomy in a vector space may make iteasier to look for relationships between themeanings in the taxonomy and meanings de-rived from vector based techniques such aslatent semantic analysis, potentially aiding inclassifying word meanings in a taxonomy.?
The new vector representation could lead tonew measures of semantic distance, for ex-ample, the Lp norms can all be used tomeasure distance between the vector rep-resentations of meanings in a taxonomy.Moreover, the vector-based representation al-lows ambiguity to be represented by addingthe weighted representations of individualsenses.We assume that the is-a relation is a partial or-dering; this is true for many ontologies.
We wishto incorporate the partial ordering of the taxonomyinto the partial ordering of the vector lattice.
Wewill make use of the following result relating topartial orders:Definition 2 (Ideals).
A lower set in a partiallyordered set S is a set T such that for all x, y ?
S,if x ?
T and y ?
x then y ?
T .The principal ideal generated by an element x ina partially ordered set S is defined to be the lowerset?y(x) = {y ?
S : y ?
x}.Proposition 3 (Ideal Completion).
If S is a par-tially ordered set, then?y(?)
can be considered asa function from S to the powerset 2S .
Under thepartial ordering defined by set inclusion, the set oflower sets form a complete lattice, and ?y(?)
is acompletion of S, the ideal completion.We are also concerned with the probability ofconcepts.
This is an idea that has come aboutthrough the introduction of ?distance measures?on taxonomies (Resnik, 1995).
Since terms canbe ascribed probabilities based on their frequen-cies of occurrence in corpora, the concepts they re-fer to can similarly be assigned probabilities.
Theprobability of a concept is the probability of en-countering an instance of that concept in the cor-pus, that is, the probability that a term selectedat random from the corpus has a meaning that issubsumed by that particular concept.
This ensuresthat more general concepts are given higher proba-bilities, for example if there is a most general con-cept (a top-most node in the taxonomy, which maycorrespond for example to ?entity?)
its probabilitywill be one, since every term can be considered aninstance of that concept.We give a general definition based on this ideawhich does not require probabilities to be assignedbased on corpus counts:Definition 4 (Real Valued Taxonomy).
A real val-ued taxonomy is a finite set S of concepts with apartial ordering ?
and a positive real function pover S. The measure of a concept is then definedin terms of p asp?
(x) =?y??
(x)p(y).The taxonomy is called probabilistic if?x?S p(s) = 1.
In this case p?
refers to theprobability of a concept.Thus in a probabilistic taxonomy, the functionp corresponds to the probability that a term is ob-served whose meaning corresponds (in that con-text) to that concept.
The function p?
denotes theprobability that a term is observed whose meaningin that context is subsumed by the concept.Note that if S has a top element I then in theprobabilistic case, clearly p?
(I) = 1.
In studies ofdistance measures on ontologies, the concepts inS often correspond to senses of terms, in this casethe function p represents the (normalised) proba-bility that a given term will occur with the senseindicated by the concept.
The top-most conceptoften exists, and may be something with the mean-ing ?entity?
?intended to include the meaning ofall concepts below it.The most simple completion we consider is intothe vector lattice L1(S), with basis elements {ex :x ?
S}.Proposition 5 (Ideal Vector Completion).
Let Sbe a probabilistic taxonomy with probability dis-tribution function p that is non-zero everywhere onS.
The function ?
from S to L1(S) defined by?
(x) =?y??
(x)p(y)eyis a completion of the partial ordering of S un-der the vector lattice order of L1(S), satisfying??
(x)?1 = p?
(x).116entityorganismplantgrasscerealoat rice barleytreebeech chestnut oakFigure 2: A small example taxonomy extractedfrom WordNet (Fellbaum, 1989).Proof.
The function ?
is clearly order-preserving:if x ?
y in S then since?y(x) ?
?y(y) , neces-sarily ?
(x) ?
?(y).
Conversely, the only waythat ?
(x) ?
?
(y) can be true is if?y(x) ?
?y(y)since p is non-zero everywhere.
If this is the case,then x ?
y by the nature of the ideal completion.Thus ?
is an order-embedding, and since L1(S) isa complete lattice, it is also a completion.
Finally,note that ??
(x)?1 =?y??
(x) p(y) = p?
(x).This completion allows us to represent conceptsas elements within a vector lattice so that not onlythe partial ordering of the taxonomy is preserved,but the probability of concepts is also preserved asthe size of the vector under the L1 norm.3.4 Representing SyntaxIn this section we give a description link grammar(Sleator and Temperley, 1991) in terms of a con-text theory.
Link grammar is a lexicalised syntac-tic formalism which describes properties of wordsin terms of links formed between them, and whichis context-free in terms of its generative power; forthe sake of brevity we omit the details, although asample link grammar parse is show in Figure 3.Our formulation of link grammar as a contexttheory makes use of a construction called a freeinverse semigroup.
Informally, the free inversesemigroup on a set S is formed from elementsof S and their inverses, S?1 = {s?1 : s ?
S},satisfying no other condition than those of an in-verse semigroup.
Formally, the free inverse semi-group is defined in terms of a congruence rela-tion on (S?S?1)?
specifying the inverse propertyand commutativity of idempotents ?
see (Munn,they mashed their way through the thick mudadjdomsFigure 3: A link grammar parse.
Link types:s: subject, o: object, m: modifying phrases,a: adjective, j: preposition, d: determiner.1974) for details.
We denote the free inverse semi-group on S by FIS(S).Free inverse semigroups were shown by Munn(1974) to be equivalent to birooted word trees.
Abirooted word-tree on a set A is a directed acyclicgraph whose edges are labelled by elements of Awhich does not contain any subgraphs of the form?
a??
?
a??
?
or ?
a??
?
a??
?, together withtwo distinguished nodes, called the start node, 2and finish node, ?.An element in the free semigroup FIS(S) is de-noted as a sequence xd11 xd22 .
.
.
xdnn where xi ?
Sand di ?
{1,?1}.We construct the birooted word tree by startingwith a single node as the start node, and for each ifrom 1 to n:?
Determine if there is an edge labelled xi leav-ing the current node if di = 1, or arriving atthe current node if di = ?1.?
If so, follow this edge and make the resultingnode the current node.?
If not, create a new node and join it with anedge labelled xi in the appropriate direction,and make this node the current node.The finish node is the current node after the n iter-ations.The product of two elements x and y in the freeinverse semigroup can be computed by finding thebirooted word-tree of x and that of y, joining thegraphs by equating the start node of y with the fin-ish node of x (and making it a normal node), andmerging any other nodes and edges necessary toremove any subgraphs of the form ?
a??
?
a??
?or ?
a??
?
a??
?.
The inverse of an elementhas the same graph with start and finish nodes ex-changed.117We can represent parses of sentences in linkgrammar by translating words to syntactic cate-gories in the free inverse semigroup.
The parseshown earlier for ?they mashed their way throughthe thick mud?
can be represented in the inversesemigroup on S = {s,m, o, d, j, a} asss?1modd?1o?1m?1jdaa?1d?1j?1which has the following birooted word-tree (thewords which the links derive from are shown inbrackets):s(they,mashed)m(mashed, through)o(mashed,way)d(their,way)j(through,mud)d(the,mud)a(thick,mud)Let A be the set of words in the natural lan-guage under consideration, S be the set of linktypes.
Then we can form a context theory?A,FIS(S), ?, ?
?
where ?
is multiplication definedby convolution on FIS(S), and a word a ?
A ismapped to a probabilistic sum a?
of its link possiblegrammar representations (called disjuncts).
Thuswe have a context theory which maps a string xto elements of L1(FIS(S)); if there is a parse forthis string then there will be some component ofx?
which corresponds to an idempotent element ofFIS(S).
Moreover we can interpret the magnitudeof the component as the probability of that par-ticular parse, thus the context theory describes aprobabilistic variation of link grammar.3.5 Uncertainty in Logical SemanticsFor the sake of brevity, we summarise our ap-proach to representing uncertainty in logical se-mantics, which is described in full elsewhere.
Ouraim is to be able to reason with probabilistic infor-mation about uncertainty in logical semantics.
Forexample, in order to represent a natural languagesentence as a logical statement, it is necessaryto parse it, which may well be with a statisticalparser.
We may have hundreds of possible parsesand logical representations of a sentence, and as-sociated probabilities.
Alternatively, we may wishto describe our uncertainty about word-sense dis-ambiguation in the representation.
Incorporatingsuch probabilistic information into the representa-tion of meaning may lead to more robust systemswhich are able to cope when one component fails.The basic principle we propose is to first repre-sent unambiguous logical statements as a contexttheory.
Our uncertainty about the meaning of asentence can then be represented as a probabilitydistribution over logical statements, whether theuncertainty arises from parsing, word-sense dis-ambiguation or any other source.
Incorporatingthis information is then straightforward: the rep-resentation of the sentence is the weighted sumof the representation of each possible meaning,where the weights are given by the probability dis-tribution.Computing the degree of entailment using thisapproach is computationally challenging, howeverwe have shown that it is possible to estimate thedegree of entailment by computing a lower boundon this value by calculating pairwise degrees ofentailment for each possible logical statement.4 Related WorkMitchell and Lapata (2008) proposed a frameworkfor composing meaning that is extremely gen-eral in nature: there is no requirement for linear-ity in the composition function, although in prac-tice the authors do adopt this assumption.
Indeedtheir ?multiplicative models?
require compositionof two vectors to be a linear function of their ten-sor product; this is equivalent to our requirementof distributivity with respect to vector space addi-tion.Various ways of composing vector based repre-sentations of meaning were investigated by Wid-dows (2008), including the tensor product and di-rect sum.
Both of these are compatible with thecontext theoretic framework since they are dis-tributive with respect to the vector space addition.Clark et al (2008) proposed a method of com-posing meaning that generalises Montague seman-tics; further work is required to determine howtheir method of composition relates to the context-theoretic framework.Erk and Pado (2008) describe a method of com-position that allows the incorporation of selec-tional preferences; again further work is requiredto determine the relation between this work andthe context-theoretic framework.1185 ConclusionWe have given an introduction to the context-theoretic framework, which provides mathemat-ical guidelines on how vector-based representa-tions of meaning should be composed, how en-tailment should be determined between these rep-resentations, and how probabilistic informationshould be incorporated.We have shown how the framework can be ap-plied to a wide range of problems in computationallinguistics, including subsequence matching, vec-tor based representations of taxonomies and statis-tical parsing.
The ideas we have presented here areonly a fraction of those described in full in (Clarke,2007), and we believe that even that is only the tipof the iceberg with regards to what it is possible toachieve with the framework.AcknowledgmentsI am very grateful to my supervisor David Weirfor all his help in the development of these ideas,and to Rudi Lutz and the anonymous reviewers formany useful comments and suggestions.ReferencesY.
A. Abramovich and Charalambos D. Aliprantis.2002.
An Invitation to Operator Theory.
AmericanMathematical Society.Charalambos D. Aliprantis and Owen Burkinshaw.1985.
Positive Operators.
Academic Press.Stephen Clark, Bob Coecke, and MehrnooshSadrzadeh.
2008.
A compositional distribu-tional model of meaning.
In Proceedings ofthe Second Symposium on Quantum Interaction,Oxford, UK, pages 133?140.Daoud Clarke.
2006.
Meaning as context and subse-quence analysis for textual entailment.
In Proceed-ings of the Second PASCAL Recognising Textual En-tailment Challenge.Daoud Clarke.
2007.
Context-theoretic Semanticsfor Natural Language: an Algebraic Framework.Ph.D.
thesis, Department of Informatics, Universityof Sussex.Ido Dagan, Oren Glickman, and Bernardo Magnini.2005.
The pascal recognising textual entailmentchallenge.
In Proceedings of the PASCAL Chal-lenges Workshop on Recognising Textual Entail-ment.Scott Deerwester, Susan Dumais, George Furnas,Thomas Landauer, and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journalof the American Society for Information Science,41(6):391?407.Katrin Erk and Sebastian Pado.
2008.
A structuredvector space model for word meaning in context.
InProceedings of EMNLP.Christaine Fellbaum, editor.
1989.
WordNet: An Elec-tronic Lexical Database.
The MIT Press, Cam-bridge, Massachusetts.John R. Firth.
1957.
Modes of meaning.
In Papersin Linguistics 1934?1951.
Oxford University Press,London.Maayan Geffet and Ido Dagan.
2005.
The dis-tributional inclusion hypotheses and lexical entail-ment.
In Proceedings of the 43rd Annual Meet-ing of the Association for Computational Linguistics(ACL?05), University of Michigan.Oren Glickman and Ido Dagan.
2005.
A probabilis-tic setting and lexical cooccurrence model for tex-tual entailment.
In ACL-05 Workshop on EmpiricalModeling of Semantic Equivalence and Entailment.Zellig Harris.
1985.
Distributional structure.
In Jer-rold J. Katz, editor, The Philosophy of Linguistics,pages 26?47.
Oxford University Press.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedingsof ACL-08: HLT, pages 236?244, Columbus, Ohio,June.
Association for Computational Linguistics.W.
D. Munn.
1974.
Free inverse semigroup.
Proceed-ings of the London Mathematical Society, 29:385?404.Philip Resnik.
1995.
Using information content toevaluate semantic similarity in a taxonomy.
In IJ-CAI, pages 448?453.Daniel D. Sleator and Davy Temperley.
1991.
Pars-ing english with a link grammar.
Technical ReportCMU-CS-91-196, Department of Computer Sci-ence, Carnegie Mellon University.Julie Weeds, David Weir, and Diana McCarthy.
2004.Characterising measures of lexical distributionalsimilarity.
In Proceedings of the 20th InternationalConference of Computational Linguistics, COLING-2004, Geneva, Switzerland.Dominic Widdows.
2008.
Semantic vector products:Some initial investigations.
In Proceedings of theSecond Symposium on Quantum Interaction, Ox-ford, UK.Ludwig Wittgenstein.
1953.
Philosophical Investiga-tions.
Macmillan, New York.
G. Anscombe, trans-lator.119
