Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 301?308,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsOnline Learning Approaches in Computer Assisted TranslationPrashant Mathur?
?, Mauro Cettolo?, Marcello Federico??
University of Trento?
FBK - Fondazione Bruno KesslerTrento, Italy{prashant, cettolo, federico}@fbk.euAbstractWe present a novel online learning ap-proach for statistical machine translationtailored to the computer assisted transla-tion scenario.
With the introduction ofa simple online feature, we are able toadapt the translation model on the flyto the corrections made by the transla-tors.
Additionally, we do online adaptionof the feature weights with a large mar-gin algorithm.
Our results show that ouronline adaptation technique outperformsthe static phrase based statistical machinetranslation system by 6 BLEU points abso-lute, and a standard incremental adaptationapproach by 2 BLEU points absolute.1 IntroductionThe growing needs of the localization and trans-lation industry have recently boosted researcharound computer assisted translation (CAT) tech-nology.
The purpose of CAT is to increase the pro-ductivity of a human translator.
A CAT tool comesas a package of a Translation Memory (TM), built-in spell checkers, a dictionary, a terminology listetc.
which help the translator while translatinga sentence.
Recent research has led to the in-tegration of CAT tools with statistical machinetranslation (SMT) engines.
SMT makes use of alarge available parallel corpus to generate statisti-cal models for translation.
Due to their generaliza-tion capability, SMT systems are a good fit in thisscenario and a seamless integration of SMT en-gines in CAT have shown to increase translator?sproductivity (Federico et al 2012).Although automatic systems generate reliabletranslations they are not accurate enough to beused directly and need postedition by human trans-lators.
In state-of-the-art CAT tools, the SMT sys-tems are static in nature and so they cannot adaptto these corrections.
When a SMT system keepsrepeating the same error, productivity of transla-tors as well as their trust in SMT technology arenegatively affected.
As an example, technical doc-umentation typically contains a lot of repetitionsdue to the employed writing style and pervasiveuse of terminology.
Hence, in order to provideuseful hints, SMT systems are expected to behaveconsistently regarding the translation of domain-specific terms.
However, if the user edits the trans-lation of a technical term in the target text, mostcurrent SMT systems are incapable to learn fromthose corrections.Online learning is a machine learning taskwhere a predictor iteratively: (1) receives an inputand outputs a label, (2) receives the correct labelfrom a human and if the two labels do not match, itlearns from the mistake.
The task of learning fromuser corrections at the sentence level fits well theonline learning scenario, and its expected useful-ness is clearly related to the amount of repetitionsoccurring in the text.
The higher the number ofrepetititions in a document the more the SMT sys-tem has chances to translate consistently throughthe use of online learning.In this paper, we implemented two online learn-ing methods through which a phrase-based SMTsystem evolves over time, sentence after sentence,by taking advantage of the post-edition or transla-tion of the previous sentence by the user.1In the first approach, we focus on the translationmodel aspect of SMT which is represented by fiveconventional features, namely lexical and phrasetranslation probabilities in both directed and in-verted directions, plus a phrase penalty score.Translation, language and reordering models arecombined in a linear fashion to obtain a score for1Moses code is available in the github reposi-tory.
https://github.com/mtresearcher/mosesdecoder/tree/moses_onlinelearning301the translation hypothesis as shown in Equation 1.score(e?, f) = ?i?ihi(e?, f) (1)where hi(?)
are the feature functions representingthe models and ?i are the linear weights.
Thehighest scored translation is the best hypothesise?
output by the system.
We extend the transla-tion model with a new feature which provides ex-tra phrase-pair scores changing according to theuser feedback.
The scores of the new feature areadapted in a discriminative fashion, by reward-ing phrase-pairs observed in the search space andin the reference, and penalizing phrase-pairs ob-served in the search space but not in the reference.In the second approach, we also adapt the modelweights of the linear combination after each testsentence by using a margin infused relaxed algo-rithm (MIRA).For assessing the robustness of our methods, weperformed experiments on two datasets from dif-ferent domains and language pairs (?6).
More-over, our online learning approaches are comparedagainst a static baseline system and against the in-cremental adaptation approach proposed by Lev-enberg et.
al.
(2010) (?5).2 Related WorksSeveral online adaptation strategies have been pro-posed in the past, only a few deal with adaptationof post-edited/evaluation data while most worksare on adaptation over development data duringtuning of parameters (Och and Ney, 2003).2.1 Online Adaptation during TuningLiang et.
al.
(2006) improved SMT perfor-mance by online adaptation of scaling factors (?
in(1)) using averaged perceptron algorithm (Collins,2002).
They presented different strategies to up-date the SMT models towards reference or oracletranslation: (1) aggressively updating towards ref-erence, bold update; (2) update towards the ora-cle translation in N-Best list, local update; (3) ahybrid approach in which a bold update is per-formed when the reference is reachable, other-wise a local update is performed.
Liang and Klein(2009) compared two online EM algorithms, step-wise online EM (Sato and Ishii, 2000; Cappe?
andMoulines, 2007) and incremental EM (Neal andHinton, 1998) which they use to update the align-ment models (the generative component of SMT)on the fly.
However, stepwise EM is prone to fail-ure if mini-batch size and stepsize parameters arenot chosen correctly, while incremental EM re-quires substantial storage costs because it has tostore sufficient statistics for each sample.
Otherworks on online minimum error rate training inSMT (Och and Ney, 2003) that deserve mention-ing are (Hopkins and May, 2011; Hasler et al2011).2.2 Online Adaptation during DecodingCesa-Bianchi et.
al.
(2008) proposed an onlinelearning approach during decoding.
They con-struct a layer of online weights over the regu-lar feature weights and update these weights atsentence level using margin infused relaxed algo-rithm (Crammer and Singer, 2003); to our knowl-edge, this is the first work on online adaptationduring decoding.
Mart?
?nez-Go?mez et.
al.
(2011;2012) presented a comparison of online adapta-tion techniques in post editing scenario.
Theycompared different adaptation strategies on scal-ing factors and feature functions (respectively, ?and h(?)
in (1)).
However, they modified the fea-ture values during adaptation without any normal-ization, which disregards the initial assumption ofthe feature values being probabilities.In our approach, the value of the additional on-line feature can be modified during decoding with-out changing other feature values (probabilities)and thus preserving their probability distribution.3 Feature AdaptationIn the CAT scenario, the user receives a translationsuggestion for each source segment, post-edits itand finally approves it.
From the SMT point ofview, for each source segment the decoder ex-plores a search space of possible translations andfinally returns the best scoring one (bestHyp) tothe user.
The user possibly corrects this suggestionthus generating the final translation (postedit).Our online learning procedure is based on thefollowing idea.
For each N-best translation (candi-date) in the search space, we compute a similarityscore against the postedit using the sentence-levelBLEU metric (Lin and Och, 2004), a smoothedvariant of the popular BLEU metric (Papineniet al 2001).
We hence compare the similar-ity score of each candidate against the similar-ity score achieved by the bestHyp, that was alsocomputed against the postedit.
If the candidate302scores better than the bestHyp, then we promotethe building blocks, i.e.
phrase-pairs, of candi-date that were not used in bestHyp and demote thephrase-pairs used in bestHyp that were not usedfor candidate.
On the contrary, if the candidatescores worse than the bestHyp, we promote thebuilding blocks of bestHyp that are not in candi-date and demote those of candidate that are not inbestHyp.Our promotion/demotion mechanism could beimplemented by updating the features values ofthe phrase pairs used in the candidate and bestHyptranslations.
However, features in the translationmodels are conditional probabilities and perturb-ing a subset of them by also preserving their nor-malization constraints can be computationally ex-pensive.
Instead, we propose to introduce an addi-tional online feature which represents a goodnessscore of each phrase-pair in the test set.We call the set of phrase pairs used to generatea candidate as candidatePP and the set of phrasepairs used to generate the bestHyp as bestPP .
Theonline feature value of each phrase-pair is initial-ized to a constant and is updated according to theperceptron update (Rosenblatt, 1958) method.
Inparticular, the amount by which a current featurevalue is rewarded or penalized depends on a learn-ing rate ?
and on the difference between the modelscores (i.e.
h ?w) of candidate and bestHyp as cal-culated by the MT system.
A sketch of our onlinelearning procedure is shown in Algorithm 1.Algorithm 1: Online Learningforeach sourceSeg dobestHyp = Translate(sourceSeg);postedit = Human(bestHyp);for i = 1 ?
iterations doN-best=Nbest(source);foreach candidate ?
N-best dosign = sgn |sBLEU(candidate) -sBLEU(bestHyp)| ;foreach phrasePair ?
candidatePP doif phrasePair /?
bestPP thenf i = f i?1 + (?
?
(?h ?
w) ?sign);endendforeach phrasePair ?
bestPP doif phrasePair /?
candidatePP thenf i = f i?1 - (?
?
(?h ?
w) ?sign);endendendendendIn Algorithm 1, ?h ?
w is the above mentionedscore difference as computed by the decoder; mul-tiplied by ?, it is the margin, that is the value withwhich the online feature score (f ) of the phrasepair under processing is modified.
We can observethat the feature scores are unbounded and couldlead to instability of the algorithm; therefore, wenormalise the scores through the sigmoid function:f(x) = 21 + exp(x) ?
1 (2)4 Weight AdaptationIn addition to adapting the online feature values,we can also apply online adaptation on the fea-ture weights of the linear combination (eq.
1).
Inparticular, after translating each sentence we canadapt the parameters depending on how good thelast translation was.
A commonly used algorithmin this online paradigm for tuning of parameters isthe Margin Infused Relaxed Algorithm (MIRA).MIRA is an online large margin algorithm thatupdates the parameter w?
of a given model accord-ing to the loss that is occurred due to incorrectclassification.
In the case of SMT this margincan be coupled with the loss function, which inthis case is the complement of the sentence levelBLEU(sBLEU).
Thus, the loss function can beformulated as:l(y?)
= sBLEU(y?)?
sBLEU(y?)
(3)where y?
is the oracle (closest translation to thereference) and y?
is the candidate being processed.Ideally, this loss should correspond to the differ-ence between the model scores:?h ?
w?
= score(y?)?
score(y?)
(4)MIRA is an ultraconservative algorithm, meaningthat the update of the current weight vector is thesmallest possible value satisfying the constraintthat the variation incurred by the objective func-tion must not be larger than the variation incurredby the model (plus a non-negative slack variable?).
Formally, weight update at ith iteration is de-fined as:wi = argminw12?
||w ?
wi?1||2?
??
?conservative+ C???
?aggressive?j?jsubject tolj ?
?hj ?
w + ?j ?j ?
J ?
{1 .
.
.
N}(5)303where j ranges over all candidates in the N-best list, lj is the loss between oracle and thecandidate j, and ?hj ?
w is the correspondingdifference in the model scores.
C is an aggressiveparameter which controls the size of the update, ?is the learning rate of the algorithm and ?
is usu-ally a very small value (in our experiments we keptit as 0.0001).
After partial differentiation and lin-earizing the loss, equation 5 can be rewritten as:wi = wi?1 + ?
?
?j?j ?
?hjwhere?j = min{C, lj ?
?hj ?
w||?hj ||2}(6)We solve equation 5, by computing ?
withthe optimizer integrated in the Moses toolkit by(Hasler et al 2011).
Algorithm 2 gives anoverview of the online margin infused relaxed al-gorithm we implemented in Moses.Algorithm 2: Online Margin Infused Relaxedforeach sourceSeg dobestHyp = Translate(sourceSeg);postedit = Human(bestHyp);w0 = w;for i = 1 ?
iterations doN-best=Nbest(sourceSeg,wi?1);foreach candidatej ?
N-best doif ?hj ?
w + ?j ?
lj then?j = Optimize(lj , hj , w, C);wi = wi?1 + ?
?
?j ?j?hj ;endendendendIn the following section we overview a streambased adaptation method with which we exper-imentally compared our two online learning ap-proaches as it well fits the framework we are work-ing in.5 Stream based adaptationContinuously updating an SMT system to an in-coming stream of parallel data comes under streambased adaptation.
Levenberg et.
al.
(2010) pro-posed an incremental adaptation technique for thecore generative component of the SMT system,word alignments and language models (Leven-berg and Osborne, 2009).
To get the word align-ments on the new data they use a Stepwise onlineEM algorithm, where old counts (from previousalignment models) are interpolated with the newcounts.Since we work at the sentence level, on-the-fly computation of probabilities of translation andreordering models is expensive in terms of bothcomputational and memory requirements.
To savethese costs, we prefer using dynamic suffix ar-ray approach described in (Levenberg et al 2010;Callison-Burch et al 2005; Lopez, 2008).
Theyare used to efficiently store the source and the tar-get corpus and alignments in efficient data struc-ture, namely the suffix array.
When a phrasetranslation is asked by the decoder, the corpus issearched, the counts are collected and its probabil-ities are computed on the fly.
However, the currentimplementation in Moses of the stream based MTrelying on the suffix arrays is severely limited asit allows the computation of only three translationfeatures, namely the two direct translation proba-bilities and the phrase penalty.
This results in asignificant degradation of performance.6 Experiments6.1 DatasetsWe compared our online learning approaches(Sections 3 and 4) and the stream based adapta-tion method (Section 5) on two datasets from dif-ferent domains, namely Information Technology(IT) and TED talks, and two different languagepairs.
The IT domain dataset is proprietary, it in-volves the translation of technical documents fromEnglish to Italian and has been used in the fieldtest carried out under the MateCat project2.
Ex-periments are also conducted on English to FrenchTED talks dataset (Cettolo et al 2012) to assessthe robustness of the proposed approaches in a dif-ferent scenario and to provide results on a publiclyavailable dataset for the sake of reproducibility.The training, development (dev2010) and evalu-ation (tst20103) sets are the same as used in thelast IWSLT last evaluation campaigns.
In experi-ments on TED data, we considered the human ref-erence translations as post edits, even if they were2www.matecat.com3As the size of evaluation set in TED data is too large withrespect to the current implementation of our algorithms, weperformed evaluation on the first 200 sentences only.304actually generated from scratch.In our experiments, the extent of usefulness ofonline learning highly depends on the amount ofrepetition of text.
A reasonable way to measure thequantity of repetition in each document is throughthe repetition rate (Bertoldi et al 2013).
It com-putes the rate of non-singleton n-grams, n=1...4,averaging the values over sub-samples S of thou-sand words from the text, and then combining therate of each n-gram to a single score by using thegeometric mean.
Equation 7 shows the formulafor calculating the repetition rate of a document,where dict(n) represents the total number ofdifferent n-grams and nr is the number of differentn-grams occurring exactly r times:RR =( 4?n=1?S dict(n)?
n1?S dict(n))1/4(7)Statistics of the parallel sets and their repetitionrate on both sides are reported in Table 1.Domain Set #srcTok srcRR #tgtTok tgtRRITen?itTrain 57M na 60M naDev 3.3k 12.03 3.5k 11.87Test 3.3k 15.00 3.3k 14.57TEDen?frTrain 2.6M na 2.8M naDev 20k 3.43 20k 5.27Test 32k 4.08 34k 3.57Table 1: Statistics of the parallel data along withthe corresponding repetition rate (RR).It can be noted that the repetition rates of ITand TED sets are significantly different, partic-ularly high in IT documents, much lower in theTED talks.6.2 SystemsThe SMT systems were built using the Mosestoolkit (Koehn et al 2007).
Training data in eachdomain was used to create translation and lexicalreordering models.
We created a 5-gram LM forTED talks and a 6-gram LM for the IT domainusing IRSTLM (Federico et al 2008) with im-proved Kneser-Ney smoothing (Chen and Good-man, 1996) on the target side of the training paral-lel corpora.
The log linear weights for the baselinesystems are optimized using MERT (Och, 2003)provided in the Moses toolkit.
To counter the in-stability of MERT, we averaged the weights ofthree MERT runs in each case.
Performance ismeasured in terms of BLEU and TER (Snoveret al 2006) computed using the MultEval script(Clark et al 2011).
Since the implementations ofstandard Giza and of incremental Giza combinedwith dynamic suffix arrays are not comparable,we constructed two baselines, a standard phrasebased SMT system and an incremental Giza base-line (?5).
Details on experimental SMT systemswe built follow.Baseline This system was built on the paralleltraining data for each domain.
We run 5 iterationsof model 1, 5 of HMM (Vogel et al 1996), 3 ofmodel 3, 3 of model 4 (Brown et al 1993) us-ing MGiza (Gao and Vogel, 2008) toolkit to alignthe parallel corpus at word level.
Translation andreordering models were built using Moses, whilelog-linear weights were optimized with MERT onthe corresponding development sets.
The same ITbaseline system was used in the field test of Mate-Cat and the references in the IT data are actualpostedits of its translation.IncGiza Baseline We trained alignment modelswith incGiza++4 with 5 iterations of model 1 and10 iterations of the HMM model.
To build in-cremental Giza baselines, we used dynamic suf-fix arrays as implemented in Moses which allowthe addition of new parallel data during decod-ing.
In the incremental Giza baseline, once a sen-tence of the test set is translated, the sentence pair(source and target post-edit/reference) along withthe alignment provided by incGiza are added tothe models.Online learning systems We developed severalonline systems on top of the two aforementionedbaseline systems: (1) +O employ the additionalonline feature (Section 3) updated with Algorithm1; (2) +O+NS as (1) but with the online fea-ture normalized with the sigmoid function; (3)+W weights updated (Section 4) with Algorithm2; (4) +O+W combination of online feature andweight update; (5) +O+NS+W as system (4) withnormalized online feature score.In the online learning system we have three ad-ditional parameters: a weight for the online fea-ture, a learning rate for features (used in the per-ceptron update), and a learning rate for featureweights used by MIRA.
These additional param-eters were optimized by maximizing the BLEU4http://code.google.com/p/inc-giza-pp/305score on the devset and on top of already opti-mized feature weights.
For practical reasons, opti-mization of the parameters was run with the Sim-plex algorithm (Nelder and Mead, 1965).7 Results and DiscussionTables 2 and 3 collect results by the systems de-scribed in Section 6.2 on the IT and TED transla-tion tasks, respectively.In Table 2, the online system (1st block?+O+NS+W?
system with 10 iterations of onlinelearning) shows significant improvements, over 6BLEU points absolute above the baseline.
In thiscase the online feature can clearly take advantageof the high repetition rates observed in the IT devand test sets (Table 1).
Similarly, in the secondblock, the online system (2nd block ?+O+NS+W?with 10 iterations of online learning) outperformsIncGiza baseline, too.
It is interesting to note thatby continuously updating the baseline system af-ter each translation step, even the plain translationmodels are capable to learn from the correction inthe post-edited text.Figure 1 depicts learning curve of Baseline sys-tem, ?+O+NS?
(referred as +online feature) and?+O+NS+W?
(referred as +MIRA).
We plotted in-cremental BLEU scores after translation of eachsentence, thereby the last point on the plot showsthe corpus level BLEU on the whole test set.In Table 3, from the first block we can observethat online learning systems perform only slightlybetter than the baseline systems, the main reasonbeing the low repetition rate observed in the eval-uation set (as shown in Table 1).
The positive re-sults observed in the second block (?+O+W?
with10 iterations) are probably due to the larger roomfor improvement available for translation modelsimplemented with dynamic suffix arrays, as theyonly incorporate 3 features instead of 5.
Some-times, online learning systems show worse resultswith higher numbers of iterations, which seemsdue to overfitting.
It is also interesting to noticethat after optimization the weight value of the on-line feature was 0.509 for the IT task and 0.072 forthe TED talk task.
This confirms the different useand potential assigned to the online feature by theSMT systems in the two tasks.8 ConclusionWe have shown a new way to update the transla-tion model on the fly without changing the originalprobability distribution.
We empirically provedthat this method is robust and works for differ-ent domain datasets be it Information Technologyor TED talks.
In addition, if the repetition rate ishigh in the text, online learning works much bet-ter than if the rate is low.
We tested both with anunbounded and a bounded range on the online fea-ture and found out that bounded values producemore stable and consistent results.
From previousworks, it has been proven that MIRA works wellwith sparse features too, so, as for the future planwe would like to treat each phrase pair as a sparsefeature and tune the sparse weights using MIRA.From the results, it is evident that we have not usedany sort of stopping criterion for online learning; arandom of 1, 5 and 10 iterations were chosen in anaive way.
Our future plan will extend to workingon finding a stopping criterion for online learningprocess.AcknowledgementsThis work was supported by the MateCat project,which is funded by the EC under the 7th Frame-work Programme.ReferencesN.
Bertoldi, M. Cettolo, and M. Federico.
2013.Cache-based online adaptation for machine trans-lation enhanced computer assisted translation.
InProc.
of MT Summit, Nice, France.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The mathematics of statisticalmachine translation: Parameter estimation.
Compu-tational Linguistics, 19(2):263?312.C.
Callison-Burch, C. Bannard, and J. Schroeder.2005.
Scaling phrase-based statistical machinetranslation to larger corpora and longer phrases.
InProc.
of ACL, pages 255?262, Ann Arbor, US-MI.O.
Cappe?
and E. Moulines.
2009.
Online EM algo-rithm for latent data models.
Journal of the RoyalStatistical Society Series B (Statistical Methodol-ogy), 71(3):593?613.N.
Cesa-Bianchi, G. Reverberi, and S. Szedmak.
2008.Online learning algorithms for computer-assistedtranslation.
Technical report, SMART project(www.smart-project.eu).M.
Cettolo, C. Girardi, and M. Federico.
2012.
WIT3:web inventory of transcribed and translated talks.
InProc.
of EAMT, Trento, Italy.S.
F. Chen and J. Goodman.
1996.
An empirical studyof smoothing techniques for language modeling.
InProc.
of ACL, pages 310?318, Santa Cruz, US-CA.306System Bleu (?)
TER (?
)1 Iter 5 Iter 10 Iter 1 Iter 5 Iter 10 IterBaseline 38.46(1.79) - - 39.98(1.35) - -+O 39.88(1.77) 41.22(1.80) 41.16(1.74) 38.69(1.30) 37.78(1.32) 38.37(1.30)+O+NS 39.91(1.80) 40.54(1.79) 40.71(1.76) 38.67(1.31) 38.21(1.29) 38.17(1.31)+W 39.76(1.76) 38.16(1.77) 37.57(1.82) 38.58(1.27) 39.53(1.30) 39.93(1.30)+O+W 41.23(1.66) 40.29(1.54) 29.36(1.45) 37.53(1.26) 38.03(1.24) 49.08(1.25)+O+NS+W 41.19(1.86) 43.07(1.87) 45.13(1.74) 37.60(1.35) 36.43(1.43) 34.53(1.36)IncGiza Baseline 28.48(1.50) - - 49.23(1.43) - -+O 29.34(1.51) 27.80(1.49) 27.52(1.38) 47.86(1.41) 48.20(1.30) 51.01(1.53)+O+NS 28.69(1.53) 29.68(1.45) 29.36(1.49) 48.21(1.45) 47.51(1.45) 47.92(1.45)+W 28.25(1.56) 27.68(1.53) 27.57(1.50) 49.05(1.43) 48.74(1.36) 48.10(1.23)+O+W 29.36(1.61) 29.94(1.64) 25.95(1.25) 47.15(1.41) 46.56(1.31) 50.31(1.15)+O+NS+W 29.76(1.49) 30.28(1.54) 30.83(1.60) 46.62(1.39) 45.60(1.28) 46.54(1.31)Table 2: Result on the IT domain task (EN>IT).
Baseline is a standard phrase based SMT system, +Ohas the online feature, +NS adds normalization of online feature, +W has online weight adaptation.202530354045500  20  40  60  80  100  120  140  160  180BLEUScoreSentence Numberbaseline+online feature+MIRAFigure 1: Incremental BLEU vs. evaluation test size on the information-technology task.
Three systemsare tracked: Baseline, +online feature, +MIRASystem Bleu (?)
TER (?
)1 Iter 5 Iter 10 Iter 1 Iter 5 Iter 10 IterBaseline 22.18(1.23) - - 58.70(1.38) - -+O 22.17(1.19) 21.85(1.25) 21.51(1.23) 58.75(1.35) 59.22(1.36) 60.48(1.35)+O+NS 21.97(1.20) 22.37(1.20) 22.24(1.22) 58.86(1.37) 58.75(1.37) 59.09(1.40)+W 22.39(1.23) 21.44(1.20) 21.00(1.13) 58.96(1.40) 58.73(1.34) 58.71(1.28)+O+W 22.33(1.21) 22.11(1.22) 21.54(1.20) 58.63(1.37) 58.31(1.38) 58.70(1.36)+O+NS+W 22.34(1.23) 22.09(1.21) 21.62(1.18) 58.60(1.37) 58.48(1.36) 58.40(1.33)IncGiza Baseline 15.04(1.08) - - 72.64(1.34) - -+O 15.30(1.08) 15.47(1.10) 15.86(1.11) 72.33(1.35) 71.68(1.37) 71.09(1.36)+O+NS 15.21(1.09) 15.48(1.12) 15.48(1.11) 72.19(1.33) 72.06(1.36) 71.65(1.33)+W 14.81(1.08) 14.61(1.07) 14.73(1.08) 73.03(1.37) 74.69(1.48) 74.28(1.46)+O+W 15.08(1.08) 15.59(1.09) 16.42(1.11) 72.55(1.33) 70.98(1.32) 70.07(1.27)+O+NS+W 15.09(1.08) 15.64(1.08) 16.15(1.10) 72.57(1.34) 71.13(1.31) 70.61(1.33)Table 3: Result on the TED talk task (EN>FR).
Baseline is a standard phrase based SMT system, +Ohas the online feature, +NS adds normalization of online feature, +W includes online weight adaptation.307J.
Clark, C. Dyer, A. Lavie, and N. Smith.
2011.
Bet-ter hypothesis testing for statistical machine transla-tion: Controlling for optimizer instability.
In Proc.of ACL, Portland, US-OR.M.
Collins.
2002.
Discriminative training methodsfor hidden markov models: Theory and experimentswith perceptron algorithms.
In Proc.
of EMNLP,Philadelphia, US-PA.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.M.
Federico, N. Bertoldi, and M. Cettolo.
2008.IRSTLM: an open source toolkit for handling largescale language models.
In Proc.
of Interspeech,pages 1618?1621, Brisbane, Australia.M.
Federico, A. Cattelan, and M. Trombetti.
2012.Measuring user productivity in machine translationenhanced computer assisted translation.
In Proc.
ofAMTA, Bellevue, US-WA.Q.
Gao and S. Vogel.
2008.
Parallel implementationsof word alignment tool.
In Proc.
of SETQA-NLP,pages 49?57, Columbus, US-OH.E.
Hasler, B. Haddow, and P. Koehn.
2011.
Margininfused relaxed algorithm for Moses.
The PragueBulletin of Mathematical Linguistics, 96:69?78.M.
Hopkins and J.
May.
2011.
Tuning as ranking.
InProc.
of EMNLP, pages 1352?1362, Edinburgh, UK.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, C. Dyer, O. Bojar, A. Con-stantin, and E. Herbst.
2007.
Moses: open sourcetoolkit for statistical machine translation.
In Proc.of ACL Companion Volume of the Demo and PosterSessions, pages 177?180, Prague, Czech Republic.A.
Levenberg and M. Osborne.
2009.
Stream-basedrandomised language models for SMT.
In Proc.
ofEMNLP, pages 756?764, Singapore.A.
Levenberg, C. Callison-Burch, and M. Osborne.2010.
Stream-based translation models for statisti-cal machine translation.
In Proc.
of HLT-NAACL,Los Angeles, US-CA.P.
Liang and D. Klein.
2009.
Online EM for unsuper-vised models.
In Proc.
of NAACL, pages 611?619,Boulder, US-CO.P.
Liang, A.
Bouchard-Co?te?, D. Klein, and B. Taskar.2006.
An end-to-end discriminative approach to ma-chine translation.
In Proc.
of ACL, pages 761?768,Sydney, Australia.C.-Y.
Lin and F. J. Och.
2004.
Orange: a method forevaluating automatic evaluation metrics for machinetranslation.
In Proc.
of COLING, pages 501?507,Geneva, Switzerland.A.
Lopez.
2008.
Tera-scale translation models via pat-tern matching.
In Proc.
of COLING, pages 505?512,Manchester, UK.P.
Mart?
?nez-Go?mez, G. Sanchis-Trilles, and F. Casacu-berta.
2011.
Online learning via dynamic rerankingfor computer assisted translation.
In Proc.
of CI-CLing, pages 93?105, Tokyo, Japan.P.
Mart?
?nez-Go?mez, G. Sanchis-Trilles, and F. Casacu-berta.
2012.
Online adaptation strategies for statis-tical machine translation in post-editing scenarios.Pattern Recogn., 45(9):3193?3203.R.
Neal and G. E. Hinton.
1998.
A view of the EM al-gorithm that justifies incremental, sparse, and othervariants.
In Learning in Graphical Models, pages355?368.
Kluwer Academic Publishers.J.
A. Nelder and R. Mead.
1965.
A simplex methodfor function minimization.
The Computer Journal,7(4):308?313.F.J.
Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.
J. Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proc.
of ACL, pages160?167, Sapporo, Japan.K.
Papineni, S. Roukos, T. Ward, and W.-J.
Zhu.
2001.Bleu: a method for automatic evaluation of machinetranslation.
Research Report RC22176, IBM Re-search Division, Thomas J. Watson Research Center.F.
Rosenblatt.
1958.
The Perceptron: a probabilisticmodel for information storage and organization inthe brain.
Psychological Review, 65:386?408.M.-A.
Sato and S. Ishii.
2000.
On-line EM algorithmfor the normalized Gaussian network.
Neural Com-put., 12(2):407?432.M.
Snover, B. Dorr, R. Schwartz, L. Micciulla, and J.Makhoul.
2006.
A study of translation edit ratewith targeted human annotation.
In Proc.
of AMTA,Boston, US-MA.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-basedword alignment in statistical translation.
In Proc.
ofCOLING, pages 836?841, Copenhagen, Denmark.308
