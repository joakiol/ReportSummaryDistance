Semantic Role Labeling Via Generalized Inference Over ClassifiersVasin Punyakanok, Dan Roth, Wen-tau Yih, Dav Zimak Yuancheng TuDepartment of Computer Science Department of LinguisticsUniversity of Illinois at Urbana-Champaign{punyakan,danr,yih,davzimak,ytu}@uiuc.eduAbstractWe present a system submitted to the CoNLL-2004 shared task for semantic role labeling.The system is composed of a set of classifiersand an inference procedure used both to cleanthe classification results and to ensure struc-tural integrity of the final role labeling.
Lin-guistic information is used to generate featuresduring classification and constraints for the in-ference process.1 IntroductionSemantic role labeling is a complex task to discover pat-terns within sentences corresponding to semantic mean-ing.
We believe it is hopeless to expect high levels of per-formance from either purely manual classifiers or purelylearned classifiers.
Rather, supplemental linguistic infor-mation must be used to support and correct a learningsystem.
The system we present here is composed of twophases.First, a set of phrase candidates is produced using twolearned classifiers?one to discover beginning positionsand one to discover end positions for each argument type.Hopefully, this phase discovers a small superset of allphrases in the sentence (for each verb).In the second phase, the final prediction is made.
First,candidate phrases from the first phase are re-scored usinga classifier designed to determine argument type, givena candidate phrase.
Because phrases are considered as awhole, global properties of the candidates can be used todiscover how likely it is that a phrase is of a given ar-gument type.
However, the set of possible role-labelingsis restricted by structural and linguistic constraints.
Weencode these constraints using linear functions and useinteger programming to ensure the final prediction is con-sistent (see Section 4).2 SNoW Learning ArchitectureThe learning algorithm used is a variation of the Winnowupdate rule incorporated in SNoW (Roth, 1998; Roth andYih, 2002), a multi-class classifier that is specifically tai-lored for large scale learning tasks.
SNoW learns a sparsenetwork of linear functions, in which the targets (phraseborder predictions or argument type predictions, in thiscase) are represented as linear functions over a commonfeature space.
It incorporates several improvements overthe basic Winnow update rule.
In particular, a regular-ization term is added, which has the affect of trying toseparate the data with a think separator (Grove and Roth,2001; Hang et al, 2002).
In the work presented here weuse this regularization with a fixed parameter.Experimental evidence has shown that SNoW activa-tions are monotonic with the confidence in the predictionTherefore, it can provide a good source of probability es-timation.
We use softmax (Bishop, 1995) over the raw ac-tivation values as conditional probabilities.
Specifically,suppose the number of classes is n, and the raw activa-tion values of class i is acti.
The posterior estimation forclass i is derived by the following equation.score(i) = pi =eacti?1?j?n eactj3 First Phase: Find Argument CandidatesThe first phase is to predict the phrases of a given sen-tence that correspond to some argument (given the verb).Unfortunately, it turns out that it is difficult to predict theexact phrases accurately.
Therefore, the goal of the firstphase is to output a superset of the correct phrases by fil-tering out unlikely candidates.Specifically, we learn two classifiers, one to detectbeginning phrase locations and a second to detect endphrase locations.
Each multi-class classifier makes pre-dictions over forty-three classes ?
thirty-two argumenttypes, ten continuous argument types, one class to detectnot begging and one class to detect not end.
The follow-ing features are used:?
Word feature includes the current word, two wordsbefore and two words after.?
Part-of-speech tag (POS) feature includes the POStags of the current word, two words before and after.?
Chunk feature includes the BIO tags for chunks ofthe current word, two words before and after.?
Predicate lemma & POS tag show the lemma formand POS tag of the active predicate.?
Voice feature indicates the voice (active/passive) ofthe current predicate.
This is extracted with a simplerule: a verb is identified as passive if it follows a to-be verb in the same phrase chuck and its POS tagis VBN(past participle) or it immediately follows anoun phrase.?
Position feature describes if the current word is be-fore of after the predicate.?
Chunk pattern feature encodes the sequence ofchunks from the current words to the predicate.?
Clause tag indicates the boundary of clauses.?
Clause path feature is a path formed from a semi-parsed tree containing only clauses and chunks.Each clause is named with the chunk immediatelypreceding it.
The clause path is the path from predi-cate to target word in the semi-parsed tree.?
Clause position feature is the position of the tar-get word relative to the predicate in the semi-parsedtree containing only clauses.
Specifically, thereare four configurations?target word and predicateshare same parent, parent of target word is ancestorof predicate, parent of predicate is ancestor of targetword, or otherwise.Because each phrase consists of a single beginning anda single ending, these classifiers can be used to constructa set of potential phrases (by combining each predictedbegin with each predicted end after it of the same type).Although the outputs of this phase are potential ar-gument candidates, along with their types, the secondphase re-scores the arguments using all possible types.After eliminating the types from consideration, the firstphase achieves 98.96% and 88.65% recall (overall, with-out verb) on the training and the development set, respec-tively.
Because these are the only candidates that arepassed to the second phase, 88.65% is an upper boundof the recall for our overall system.4 Second Phase: Phrase ClassificationThe second phase of our system assigns the final argu-ment classes to (a subset) of the phrases supplied from thefirst phase.
This task is accomplished in two steps.
First,a multi-class classifier is used to supply confidence scorescorresponding to how likely individual phrases are tohave specific argument types.
Then we look for the mostlikely solution over the whole sentence, given the matrixof confidences and linguistic information that serves as aset of global constraints over the solution space.Again, the SNoW learning architecture is used to traina multi-class classifier to label each phrase to one ofthe argument types, plus a special class ?
no argument.Training examples are created from the phrase candidatessupplied from the first phase using the following features:?
Predicate lemma & POS tag, voice, position,clause Path, clause position, chunk pattern Samefeatures as the first phase.?
Word & POS tag from the phrase, including thefirst/last word and tag, and the head word1.?
Named entity feature tells if the target phrase is,embeds, overlaps, or is embedded in a named entity.?
Chunk features are the same as named entity (butwith chunks, e.g.
noun phrases).?
Length of the target phrase, in the numbers of wordsand chunks.?
Verb class feature is the class of the active predicatedescribed in the frame files.?
Phrase type uses simple heuristics to identify thetarget phrase like VP, PP, or NP.?
Sub-categorization describes the phrase structurearound the predicate.
We separate the clause wherethe predicate is in into three part ?
the predicatechunk, segments before and after the predicate.
Thesequence of the phrase types of these three segmentsis our feature.?
Baseline follows the rule of identifying AM-NEGand AM-MOD and uses them as features.?
Clause coverage describes how much of localclause (from the predicate) is covered by the targetphrase.?
Chunk pattern length feature counts the number ofpatterns in the phrase.?
Conjunctions join every pair of the above featuresas new features.?
Boundary words & POS tags include one or twowords/tags before and after the target phrase.1We use simple rules to first decide if a candidate phrasetype is VP, NP, or PP.
The headword of an NP phrase is theright-most noun.
Similarly, the left-most verb/proposition of aVP/PP phrase is extracted as the headword?
Bigrams are pairs of words/tags in the window fromtwo words before the target to the first word of thetarget, and also from the last word to two words afterthe phrase.?
Sparse colocation picks one word/tag from the twowords before the phrase, the first word/tag, the lastword/tag of the phrase, and one word/tag from thetwo words after the phrase to join as features.Alternately, we could have derived a scoring functionfrom the first phase confidences of the open and closedpredictors for each argument type.
This method hasproved useful in the literature for shallow parsing (Pun-yakanok and Roth, 2001).
However, it is hoped that ad-ditional global features of the phrase would be necessarydue to the variety and complexity of the argument types.See Table 1 for a comparison.Formally (but very briefly), the phrase classifier is at-tempting to assign labels to a set of phrases, S1:M , in-dexed from 1 to M .
Each phrase Si can take any labelfrom a set of phrase labels, P , and the indexed set ofphrases can take a set of labels, s1:M ?
PM .
If we as-sume that the classifier returns a score, score(Si = si),corresponding to the likelihood of seeing label si forphrase Si, then, given a sentence, the unaltered inferencetask that is solved by our system maximizes the score ofthe phrase, score(S1:M = s1:M ),s?1:M = argmaxs1:M?PMscore(S1:M = s1:M )= argmaxs1:M?PMM?i=1score(Si = si).
(1)The second step for phrase identification is eliminatinglabelings using global constraints derived from linguisticinformation and structural considerations.
Specifically,we limit the solution space through the used of a filterfunction, F , that eliminates many phrase labelings fromconsideration.
It is interesting to contrast this with previ-ous work that filters individual phrases (see (Carreras andMa`rquez, 2003)).
Here, we are concerned with globalconstraints as well as constraints on the phrases.
There-fore, the final labeling becomess?1:M = argmaxs1:M?F(PM)M?i=1score(Si = si) (2)The filter function used considers the following con-straints:1.
Arguments cannot cover the predicate except thosethat contain only the verb or the verb and the follow-ing word.2.
Arguments cannot overlap with the clauses (they canbe embedded in one another).3.
If a predicate is outside a clause, its arguments can-not be embedded in that clause.4.
No overlapping or embedding phrases.5.
No duplicate argument classes for A0-A5,V.6.
Exactly one V argument per sentence.7.
If there is C-V, then there has to be a V-A1-CV pat-tern.8.
If there is a R-XXX argument, then there has to be aXXX argument.9.
If there is a C-XXX argument, then there has to bea XXX argument; in addition, the C-XXX argumentmust occur after XXX.10.
Given the predicate, some argument classes are ille-gal (e.g.
predicate ?stalk?
can take only A0 or A1).Constraint 1 is valid because all the arguments of a pred-icate must lie outside the predicate.
The exception is forthe boundary of the predicate itself.
Constraint 1 throughconstraint 3 are actually constraints that can be evaluatedon a per-phrase basis and thus can be applied to the indi-vidual phrases at any time.
For efficiency sake, we elimi-nate these even before the second phase scoring is begun.Constraints 5, 8, and 9 are valid for only a subset of thearguments.These constraints are easy to transform into linear con-straints (for example, for each class c, constraint 5 be-comes?Mi=1[Si = c] ?
1) 2.
Then the optimum solutionof the cost function given in Equation 2 can be found byinteger linear programming3.
A similar method was usedfor entity/relation recognition (Roth and Yih, 2004).Almost all previous work on shallow parsing andphrase classification has used Constraint 4 to ensure thatthere are no overlapping phrases.
By considering addi-tional constraints, we show improved performance (seeTable 1).5 ResultsIn this section, we present results.
For the second phase,we evaluate the quality of the phrase predictor.
The re-sult first evaluates the phrase classifier, given the perfectphrase locations without using inference (i.e.
F(PM ) =PM ).
The second, adds inference to the phrase classifica-tion over the perfect classifiers (see Table 2).
We evaluatethe overall performance of our system (without assum-ing perfect phrases) by training and evaluating the phraseclassifier on the output from the first phase (see Table 3).Finally,since this is a tagging task, we compare thissystem with the basic tagger that we have, the CLCL2where [x] is 1 if x is true and 0 otherwise3(Xpress-MP, 2003) was used in all experiments to solve in-teger linear programming.Precision Recall F11st Phase, non-Overlap 70.54% 61.50% 65.711st Phase, All Const.
70.97% 60.74% 65.462nd Phase, non-Overlap 69.69% 64.75% 67.132nd Phase, All Const.
71.96% 64.93% 68.26Table 1: Summary of experiments on the development set.The phrase scoring is choosen from either the first phase or thesecond phase and each is evaluated by considering simply non-overlapping constraints or the full set of linguistic constraints.To make a fair comparison, parameters were set seperately tooptimize performance when using the first phase results.
Allresults are for overall performance.Precision Recall F1Without Inference 86.95% 87.24% 87.10With Inference 88.03% 88.23% 88.13Table 2: Results of second phase phrase prediction and in-ference assuming perfect boundary detection in the first phase.Inference improves performance by restricting label sequencesrather than restricting structural properties since the correctboundaries are given.
All results are for overall performanceon the development set.shallow parser from (Punyakanok and Roth, 2001), whichis equivalent to using the scoring function from the firstphase with only the non-overlapping constraints.
Table 1shows how how additional constraints over the standardnon-overlapping constraints improve performance on thedevelopment set4.6 ConclusionWe show that linguistic information is useful for semanticrole labeling used both to derive features and to derivehard constraints on the output.
We show that it is possibleto use integer linear programming to perform inferencethat incorporates a wide variety of hard constraints thatwould be difficult to incorporate using existing methods.In addition, we provide further evidence supporting theuse of scoring phrases over scoring phrase boundaries forcomplex tasks.Acknowledgments This research is supported byNSF grants ITR-IIS-0085836, ITR-IIS-0085980 and IIS-9984168, EIA-0224453 and an ONR MURI Award.
Wealso thank AMD for their equipment donation and DashOptimization for free academic use of their Xpress-MPsoftware.ReferencesC.
Bishop, 1995.
Neural Networks for Pattern Recognition,chapter 6.4: Modelling conditional distributions, page 215.Oxford University Press.4The test set was not publicly available to evaluate these re-sults.Precision Recall F?=1Overall 70.07% 63.07% 66.39A0 81.13% 77.70% 79.38A1 74.21% 63.02% 68.16A2 54.16% 41.04% 46.69A3 47.06% 26.67% 34.04A4 71.43% 60.00% 65.22A5 0.00% 0.00% 0.00AM-ADV 39.36% 36.16% 37.69AM-CAU 45.95% 34.69% 39.53AM-DIR 42.50% 34.00% 37.78AM-DIS 52.00% 67.14% 58.61AM-EXT 46.67% 50.00% 48.28AM-LOC 33.47% 34.65% 34.05AM-MNR 45.19% 36.86% 40.60AM-MOD 92.49% 94.96% 93.70AM-NEG 85.92% 96.06% 90.71AM-PNC 32.79% 23.53% 27.40AM-PRD 0.00% 0.00% 0.00AM-TMP 59.77% 56.89% 58.30R-A0 81.33% 76.73% 78.96R-A1 58.82% 57.14% 57.97R-A2 100.00% 22.22% 36.36R-A3 0.00% 0.00% 0.00R-AM-LOC 0.00% 0.00% 0.00R-AM-MNR 0.00% 0.00% 0.00R-AM-PNC 0.00% 0.00% 0.00R-AM-TMP 54.55% 42.86% 48.00V 98.37% 98.37% 98.37Table 3: Results on the test set.X.
Carreras and L. Ma`rquez.
2003.
Phrase recognition by filter-ing and ranking with perceptrons.
In Proceedings of RANLP-2003.A.
Grove and D. Roth.
2001.
Linear concepts and hidden vari-ables.
Machine Learning, 42(1/2):123?141.T.
Hang, F. Damerau, , and D. Johnson.
2002.
Text chunkingbased on a generalization of winnow.
Journal of MachineLearning Research, 2:615?637.V.
Punyakanok and D. Roth.
2001.
The use of classifiers insequential inference.
In NIPS-13; The 2000 Conference onAdvances in Neural Information Processing Systems, pages995?1001.
MIT Press.D.
Roth and W. Yih.
2002.
Probabilistic reasoning for entity& relation recognition.
In COLING 2002, The 19th Interna-tional Conference on Computational Linguistics, pages 835?841.D.
Roth and W. Yih.
2004.
A linear programming formulationfor global inference in natural language tasks.
In Proc.
ofCoNLL-2004.D.
Roth.
1998.
Learning to resolve natural language ambigui-ties: A unified approach.
In Proc.
of AAAI, pages 806?813.Xpress-MP.
2003.
Dash Optimization.
Xpress-MP.http://www.dashoptimization.com/products.html.
