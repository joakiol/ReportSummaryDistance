Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 2399?2409,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMultimodal Pivots for Image Caption TranslationJulian Hitschler and Shigehiko SchamoniComputational LinguisticsHeidelberg University69120 Heidelberg, Germany{hitschler,schamoni}@cl.uni-heidelberg.deStefan RiezlerComputational Linguistics & IWRHeidelberg University69120 Heidelberg, Germanyriezler@cl.uni-heidelberg.deAbstractWe present an approach to improve sta-tistical machine translation of image de-scriptions by multimodal pivots defined invisual space.
The key idea is to performimage retrieval over a database of imagesthat are captioned in the target language,and use the captions of the most similarimages for crosslingual reranking of trans-lation outputs.
Our approach does not de-pend on the availability of large amountsof in-domain parallel data, but only re-lies on available large datasets of monolin-gually captioned images, and on state-of-the-art convolutional neural networks tocompute image similarities.
Our experi-mental evaluation shows improvements of1 BLEU point over strong baselines.1 IntroductionMultimodal data consisting of images and naturallanguage descriptions (henceforth called captions)are an abundant source of information that has ledto a recent surge in research integrating languageand vision.
Recently, the aspect of multilingualityhas been added to multimodal language process-ing in a shared task at the WMT16 conference.1There is clearly also a practical demand for mul-tilingual image captions, e.g., automatic transla-tion of descriptions of art works would allow ac-cess to digitized art catalogues across languagebarriers and is thus of social and cultural interest;multilingual product descriptions are of high com-mercial interest since they would allow to widene-commerce transactions automatically to interna-tional markets.
However, while datasets of imagesand monolingual captions already include millions1http://www.statmt.org/wmt16/multimodal-task.htmlof tuples (Ferraro et al, 2015), the largest multi-lingual datasets of images and captions known tothe authors contain 20,000 (Grubinger et al, 2006)or 30,0002triples of images with German and En-glish descriptions.In this paper, we want to address the problemof multilingual captioning from the perspective ofstatistical machine translation (SMT).
In contrastto prior work on generating captions directly fromimages (Kulkarni et al (2011), Karpathy and Fei-Fei (2015), Vinyals et al (2015), inter alia), ourgoal is to integrate visual information into an SMTpipeline.
Visual context provides orthogonal in-formation that is free of the ambiguities of natu-ral language, therefore it serves to disambiguateand to guide the translation process by ground-ing the translation of a source caption in the ac-companying image.
Since datasets consisting ofsource language captions, images, and target lan-guage captions are not available in large quanti-ties, we would instead like to utilize large datasetsof images and target-side monolingual captions toimprove SMT models trained on modest amountsof parallel captions.Let the task of caption translation be defined asfollows: For production of a target caption eiofan image i, a system may use as input an imagecaption for image i in the source language fi, aswell as the image i itself.
The system may safelyassume that fiis relevant to i, i.e., the identifi-cation of relevant captions for i (Hodosh et al,2013) is not itself part of the task of caption trans-lation.
In contrast to the inference problem of find-ing e?
= argmaxep(e|f) in text-based SMT, mul-timodal caption translation allows to take into con-sideration i as well as fiin finding e?i:e?i= argmaxeip(ei|fi, i)2The dataset used at the WMT16 shared task is based ontranslations of Flickr30K captions (Rashtchian et al, 2010).2399In this paper, we approach caption translationby a general crosslingual reranking frameworkwhere for a given pair of source caption and im-age, monolingual captions in the target languageare used to rerank the output of the SMT sys-tem.
We present two approaches to retrieve tar-get language captions for reranking by pivotingon images that are similar to the input image.One approach calculates image similarity baseddeep convolutional neural network (CNN) repre-sentations.
Another approach calculates similar-ity in visual space by comparing manually anno-tated object categories.
We compare the multi-modal pivot approaches to reranking approachesthat are based on text only, and to standard SMTbaselines trained on parallel data.
Compared to astrong baseline trained on 29,000 parallel captiondata, we find improvements of over 1 BLEU pointfor reranking based on visual pivots.
Notably, ourreranking approach does not rely on large amountsof in-domain parallel data which are not availablein practical scenarios such as e-commerce local-ization.
However, in such scenarios, monolingualproduct descriptions are naturally given in largeamounts, thus our work is a promising pilot studytowards real-world caption translation.2 Related WorkCaption generation from images alone has only re-cently come into the scope of realistically solv-able problems in image processing (Kulkarni etal.
(2011), Karpathy and Fei-Fei (2015), Vinyalset al (2015), inter alia).
Recent approaches alsoemploy reranking of image captions by measuringsimilarity between image and text using deep rep-resentations (Fang et al, 2015).
The tool of choicein these works are neural networks whose deeprepresentations have greatly increased the qual-ity of feature representations of images, enablingrobust and semantically salient analysis of imagecontent.
We rely on the CNN framework (Socheret al, 2014; Simonyan and Zisserman, 2015) tosolve semantic classification and disambiguationtasks in NLP with the help of supervision sig-nals from visual feedback.
However, we considerimage captioning as a different task than captiontranslation since it is not given the information ofthe source language string.
Therefore we do notcompare our work to caption generation models.In the area of SMT, W?aschle and Riezler (2015)presented a framework for integrating a large, in-domain, target-side monolingual corpus into ma-chine translation by making use of techniquesfrom crosslingual information retrieval.
The in-tuition behind their approach is to generate one orseveral translation hypotheses using an SMT sys-tem, which act as queries to find matching, se-mantically similar sentences in the target side cor-pus.
These can in turn be used as templates forrefinement of the translation hypotheses, with theoverall effect of improving translation quality.
Ourwork can be seen as an extension of this method,with visual similarity feedback as additional con-straint on the crosslingual retrieval model.
Cal-ixto et al (2012) suggest using images as sup-plementary context information for statistical ma-chine translation.
They cite examples from thenews domain where visual context could poten-tially be helpful in the disambiguation aspect ofSMT and discuss possible features and distancemetrics for context images, but do not report ex-periments involving a full SMT pipeline using vi-sual context.
In parallel to our work, Elliott et al(2015) addressed the problem of caption transla-tion from the perspective of neural machine trans-lation.3Their approach uses a model which isconsiderably more involved than ours and reliesexclusively on the availability of parallel captionsas training data.
Both approaches crucially relyon neural networks, where they use a visuallyenriched neural encoder-decoder SMT approach,while we follow a retrieval paradigm for captiontranslation, using CNNs to compute similarity invisual space.Integration of multimodal information into NLPproblems has been another active area of re-cent research.
For example, Silberer and La-pata (2014) show that distributional word em-beddings grounded in visual representations out-perform competitive baselines on term similar-ity scoring and word categorization tasks.
Theorthogonality of visual feedback has previouslybeen exploited in a multilingual setting by Kiela etal.
(2015) (relying on previous work by Bergsmaand Van Durme (2011)), who induce a bilinguallexicon using term-specific multimodal represen-tations obtained by querying the Google image3We replicated the results of Elliott et al (2015) on theIAPR TC-12 data.
However, we decided to not include theirmodel as baseline in this paper since we found our hierarchi-cal phrase-based baselines to yield considerably better resultson IAPR TC-12 as well as on MS COCO.2400Image iTarget Hyp.
List NfiSource Caption fiRerankerF(r, Mfi)Source CaptionFinal Target Caption eiMultimodalPivot DocumentsTarget CaptionsMultimodalPivot DocumentsTarget CaptionsMultimodalPivot DocumentsMfi Target CaptionsMT DecoderTarget Hyp.
List Rfi^ Interpolated ModelMultimodal Retrieval Model  S(m, Nfi, i)Figure 1: Overview of model architecture.search engine.4Funaki and Nakayama (2015)use visual similarity for crosslingual documentretrieval in a multimodal and bilingual vectorspace obtained by generalized canonical correla-tion analysis, greatly reducing the need for paralleltraining data.
The common element is that CNN-based visual similarity information is used as a?hub?
(Funaki and Nakayama, 2015) or pivot con-necting corpora in two natural languages whichlack direct parallelism, a strategy which we applyto the problem of caption translation.3 Models3.1 OverviewFollowing the basic approach set out by W?aschleand Riezler (2015), we use a crosslingual retrievalmodel to find sentences in a target language doc-ument collection C, and use these to rerank targetlanguage translations e of a source caption f .The systems described in our work differ fromthat of W?aschle and Riezler (2015) in a numberof aspects.
Instead of a two-step architecture ofcoarse-grained and fine-grained retrieval, our sys-tem uses relevance scoring functions for retrievalof matches in the document collection C, and for4https://images.google.com/reranking of translation candidates that are basedon inverse document frequency of terms (Sp?arckJones, 1972) and represent variants of the popularTF-IDF relevance measure.A schematic overview of our approach is givenin Figure 1.
It consists of the following compo-nents:Input: Source caption fi, image i, target-side col-lection C of image-captions pairsTranslation: Generate unique list Nfiof kn-besttranslations, generate unique list Rfiof kr-best list of translations5using MT decoderMultimodal retrieval: For list of translationsNfi, find set Mfiof km-most relevant pairsof images and captions in a target-side col-lection C, using a heuristic relevance scoringfunction S(m,Nfi, i),m ?
CCrosslingual reranking: Use list Mfiof image-caption pairs to rerank list of translationsRfi, applying relevance scoring functionF (r,Mfi) to all r ?
RfiOutput: Determine best translation hypothesis e?iby interpolating decoder score drfor a hy-pothesis r ?
Rfiwith its relevance scoreF (r,Mfi) with weight ?
s.t.e?i= argmaxr?Rfidr+ ?
?
F (r,Mfi)The central concept is the scoring functionS(m,Nfi, i) which defines three variants oftarget-side retrieval (TSR), all of which make useof the procedure outlined above.
In the base-line text-based reranking model (TSR-TXT), weuse relevance scoring function STXT.
This func-tion is purely text-based and does not make useof multimodal context information (as such, itcomes closest to the models used for target-sideretrieval in W?aschle and Riezler (2015)).
In theretrieval model enhanced by visual informationfrom a deep convolutional neural network (TSR-CNN), the scoring function SCNNincorporates atextual relevance score with visual similarity in-formation extracted from the neural network.
Fi-nally, we evaluate these models against a rele-vance score based on human object-category an-notations (TSR-HCA), using the scoring function5In practice, the first hypothesis list may be reused.
Wedistinguish between the two hypothesis lists Nfiand Rfifornotational clarity since in general, the two hypothesis listsneed not be of equal length.2401SHCA.
This function makes use of the object an-notations available for the MS COCO corpus (Linet al, 2014) to give an indication of the effective-ness of our automatically extracted visual similar-ity metric.
The three models are discussed in detailbelow.3.2 Target Side Retrieval ModelsText-Based Target Side Retrieval.
In the TSR-TXT retrieval scenario, a match candidate m ?
Cis scored in the following way:STXT(m,Nfi) =Zm?n?Nfi?wn?tok(n)?wm?typ(m)?
(wm, wn)idf(wm),where ?
is the Kronecker ?-function, Nfiis the setof the kn-best translation hypotheses for a sourcecaption fiof image i by decoder score, typ(a)is a function yielding the set of types (unique to-kens) contained in a caption a,6tok(a) is a func-tion yielding the tokens of caption a, idf(w) isthe inverse document frequency (Sp?arck Jones,1972) of term w, and Zm=1|typ(m)|is a nor-malization term introduced in order to avoid bias-ing the system towards long match candidates con-taining many low-frequency terms.
Term frequen-cies were computed on monolingual data from Eu-roparl (Koehn, 2005) and the News Commentaryand News Discussions English datasets providedfor the WMT15 workshop.7Note that in thismodel, information from the image i is not used.Multimodal Target Side Retrieval using CNNs.In the TSR-CNN scenario, we supplement the tex-tual target-side TSR model with visual similar-ity information from a deep convolutional neu-ral network.
We formalize this by introduc-tion of the positive-semidefinite distance functionv(ix, iy) ?
[0,?)
for images ix, iy(smaller val-ues indicating more similar images).
The rele-vance scoring function SCNNused in this model6The choice for per-type scoring of reference captions wasprimarily driven by performance considerations.
Since cap-tions rarely contain repetitions of low-frequency terms, thishas very little effect in practice, other than to mitigate the in-fluence of stopwords.7http://www.statmt.org/wmt15/translation-task.htmltakes the following form:SCNN(m,Nfi, i)={STXT(m,Nfi)e?bv(im,i), v(im, i) < d0 otherwise,where imis the image to which the caption mrefers and d is a cutoff maximum distance, abovewhich match candidates are considered irrelevant,and b is a weight term which controls the impactof the visual distance score v(im, i) on the overallscore.8Our visual distance measure v was computedusing the VGG16 deep convolutional model of Si-monyan and Zisserman (2015), which was pre-trained on ImageNet (Russakovsky et al, 2014).We extracted feature values for all input and refer-ence images from the penultimate fully-connectedlayer (fc7) of the model and computed the Eu-clidean distance between feature vectors of im-ages.
If no neighboring images fell within dis-tance d, the text-based retrieval procedure STXTwas used as a fallback strategy, which occurred 47out of 500 times on our test data.Target Side Retrieval by Human Category An-notations.
For contrastive purposes, we evalu-ated a TSR-HCA retrieval model which makes useof the human object category annotations for MSCOCO.
Each image in the MS COCO corpus isannotated with object polygons classified into 91categories of common objects.
In this scenario, amatch candidatem is scored in the following way:SHCA(m,Nfi, i)= ?
(cat(im), cat(i))STXT(m,Nfi),where cat(i) returns the set of object categorieswith which image i is annotated.
The amountsto enforcing a strict match between the categoryannotations of i and the reference image im, thuspre-filtering the STXTscoring to captions for im-ages with strict category match.9In cases wherei was annotated with a unique set of object cate-gories and thus no match candidates with nonzeroscores were returned by SHCA, STXTwas used asa fallback strategy, which occurred 77 out of 500times on our test data.8The value of b = 0.01 was found on development dataand kept constant throughout the experiments.9Attempts to relax this strict matching criterion led tostrong performance degradation on the development test set.24023.3 Translation Candidate Re-scoringThe relevance score F (r,Mfi) used in the rerank-ing model was computed in the following way forall three models:F (r,Mfi) =ZMfi?m?Mfi?wm?typ(m)?wr?tok(r)?
(wm, wr)idf(wm)with normalization termZMfi= (?m?Mfi|tok(m)|)?1,where r is a translation candidate and Mfiis a listof km-top target side retrieval matches.
Becausethe model should return a score that is reflective ofthe relevance of r with respect toMfi, irrespectiveof the length of Mfi, normalization with respectto the token count of Mfiis necessary.
The termZMfiserves this purpose.4 Experiments4.1 Bilingual Image-Caption DataWe constructed a German-English parallel datasetbased on the MS COCO image corpus (Lin etal., 2014).
1,000 images were selected at randomfrom the 2014 training section10and, in a sec-ond step, one of their five English captions waschosen randomly.
This caption was then trans-lated into German by a native German speaker.Note that our experiments were performed withGerman as the source and English as the tar-get language, therefore, our reference data wasnot produced by a single speaker but reflects theheterogeneity of the MS COCO dataset at large.The data was split into a development set of 250captions, a development test set of 250 captionsfor testing work in progress, and a test set of500 captions.
For our retrieval experiments, weused only the images and captions that were notincluded in the development, development testor test data, a total of 81,822 images with 5English captions per image.
All data was to-kenized and converted to lower case using thecdec11utilities tokenized-anything.pland lowercase.pl.
For the German data, we10We constructed our parallel dataset using only the train-ing rather than the validation section of MS COCO so as tokeep the latter pristine for future work based on this research.11https://github.com/redpony/cdecSection Images Captions LanguagesDEV 250 250 DE-ENDEVTEST 250 250 DE-ENTEST 500 500 DE-ENRETRIEVAL (C) 81,822 409,110 ENTable 1: Number of images and sentences inMS COCO image and caption data used in exper-iments.performed compound-splitting using the methoddescribed by Dyer (2009), as implemented by thecdec utility compound-split.pl.
Table 1gives an overview of the dataset.
Our parallel de-velopment, development test and test data is pub-licly available.124.2 Translation BaselinesWe compare our approach to two baseline ma-chine translation systems, one trained on out-of-domain data exclusively and one domain-adaptedsystem.
Table 2 gives an overview of the trainingdata for the machine translation systems.Out-of-Domain Baseline.
Our baseline SMTframework is hierarchical phrase-based translationusing synchronous context free grammars (Chi-ang, 2007), as implemented by the cdec de-coder (Dyer et al, 2010).
Data from the Europarl(Koehn, 2005), News Commentary and CommonCrawl corpora (Smith et al, 2013) as provided forthe WMT15 workshop was used to train the trans-lation model, with German as source and Englishas target language.Like the retrieval dataset, training, developmentand test data was tokenized and converted to lowercase, using the same cdec tools.
Sentences withlengths over 80 words in either the source orthe target language were discarded before train-ing.
Source text compound splitting was per-formed using compound-split.pl.
Align-ments were extracted bidirectionally using thefast-align utility of cdec and symmetrizedwith the atools utility (also part of cdec) us-ing the grow-diag-final-and symmetriza-tion heuristic.
The alignments were then usedby the cdec grammar extractor to extract a syn-chronous context free grammar from the paralleldata.12www.cl.uni-heidelberg.de/decoco/2403Corpus Sentences Languages SystemEuroparl 1,920,209 DE-EN O/INews Commentary 216,190 DE-EN O/ICommon Crawl 2,399,123 DE-EN O/IFlickr30k WMT16 29,000 DE-EN IEuroparl 2,218,201 EN O/INews Crawl 28,127,448 EN O/INews Discussions 57,803,684 EN O/IFlickr30k WMT16 29,000 EN ITable 2: Parallel and monolingual data usedfor training machine translation systems.
Sen-tence counts are given for raw data without pre-processing.
O/I: both out-of-domain and in-domain system, I: in-domain system only.The target language model was trained onmonolingual data from Europarl, as well asthe News Crawl and News Discussions Englishdatasets provided for the WMT15 workshop (thesame data as was used for estimating term fre-quencies for the retrieval models) with the KenLMtoolkit (Heafield et al, 2013; Heafield, 2011).13We optimized the parameters of the translationsystem for translation quality as measured by IBMBLEU (Papineni et al, 2002) using the Margin In-fused Relaxed Algorithm (MIRA) (Crammer andSinger, 2003).
For tuning the translation modelsused for extraction of the hypothesis lists for finalevaluation, MIRA was run for 20 iterations on thedevelopment set, and the best run was chosen forfinal testing.In-Domain Baseline.
We also compared ourmodels to a domain-adapted machine translationsystem.
The domain-adapted system was iden-tical to the out-of-domain system, except that itwas supplied with additional parallel training datafrom the image caption domain.
For this purpose,we used 29,000 parallel German-English imagecaptions as provided for the WMT16 shared taskon multimodal machine translation.
The Englishcaptions in this dataset belong to the Flickr30kcorpus (Rashtchian et al, 2010) and are very sim-ilar to those of the MS COCO corpus.
The Ger-man captions are expert translations.
The Englishcaptions were also used as additional training datafor the target-side language model.
We generatedkn- and kr-best lists of translation candidates us-ing this in-domain baseline system.13https://kheafield.com/code/kenlm/Model knkmkr?TSR-TXT 300 500 5 5 ?
104TSR-CNN 300 300 5 70 ?
104TSR-HCA 300 500 5 10 ?
104Table 3: Optimized hyperparameter values usedin final evaluation.4.3 Optimization of TSR HyperparametersFor each of our retrieval models, we performed astep-wise exhaustive search of the hyperparame-ter space over the four system hyperparameters forIBM BLEU on the development set: The lengthof the kn-best list the entries of which are usedas queries for retrieval; the number of km-best-matching captions retrieved; the length of the fi-nal kr-best list used in reranking; the interpolationweight ?
of the relevance score F relative to thetranslation hypothesis log probability returned bythe decoder.
The parameter ranges to be exploredwere determined manually, by examining systemoutput for prototypical examples.
Table 3 givesan overview over the hyperparameter values ob-tained.For TSR-CNN, we initially set the cutoff dis-tance d to 90.0, after manually inspecting sets ofnearest neighbors returned for various maximumdistance values.
After optimization of retrieval pa-rameters, we performed an exhaustive search fromd = 80.0 to d = 100.0, with step size 1.0 on thedevelopment set, while keeping all other hyperpa-rameters fixed, which confirmed out initial choiceof d = 90.0 as the optimal value.Explored parameter spaces were identical for allmodels and each model was evaluated on the testset using its own optimal configuration of hyper-parameters.4.4 Significance TestingSignificance tests on the differences in transla-tion quality were performed using the approxi-mate randomization technique for measuring per-formance differences of machine translation sys-tems described in Riezler and Maxwell (2005) andimplemented by Clark et al (2011) as part of theMulteval toolkit.1414https://github.com/jhclark/multeval2404System BLEU ?
pcptpdpocdec out-dom.
25.5cdec in-dom.
29.6 0.00TSR-TXT 29.7 0.45 0.00TSR-CNN 30.6 0.04 0.02 0.00TSR-HCA 30.3 0.42 0.01 0.00 0.00System METEOR ?
pcptpdpocdec out-dom.
31.7cdec in-dom.
34.0 0.00TSR-TXT 34.1 0.41 0.00TSR-CNN 34.7 0.00 0.00 0.00TSR-HCA 34.4 0.09 0.00 0.00 0.00System TER ?
pcptpdpocdec out-dom.
49.3cdec in-dom.
46.1 0.00TSR-TXT 45.8 0.12 0.00TSR-CNN 45.1 0.03 0.00 0.00TSR-HCA 45.3 0.34 0.02 0.00 0.00Table 4: Metric scores for all systems and theirsignificance levels as reported by Multeval.
po-values are relative to the cdec out-of-domainbaseline, pd-values are relative to the cdec in-domain baseline, pt-values are relative to TSR-TXT and pc-values are relative to TSR-CNN.
Bestresults are reported in bold face.154.5 Experimental ResultsTable 4 summarizes the results for all modelson an unseen test set of 500 captions.
Domainadaptation led to a considerable improvement of+4.1 BLEU and large improvements in terms ofMETEOR and Translation Edit Rate (TER).
Wefound that the target-side retrieval model enhancedwith multimodal pivots from a deep convolutionalneural network, TSR-CNN and TSR-HCA, con-sistently outperformed both the domain-adaptedcdec baseline, as well as the text-based tar-get side retrieval model TSR-TXT.
These modelstherefore achieve a performance gain which goesbeyond the effect of generic domain-adaptation.The gain in performance for TSR-CNN and TSR-HCA was significant at p < 0.05 for BLEU, ME-TEOR, and TER.
For all evaluation metrics, thedifference between TSR-CNN and TSR-HCA wasnot significant, demonstrating that retrieval usingour CNN-derived distance metric could match re-trieval based the human object category annota-tions.15A baseline for which a random hypothesis was cho-sen from the top-5 candidates of the in-domain system liesbetween the other two baseline systems: 27.5 / 33.3 / 47.7(BLEU / METEOR / TER).a+,f+ a+,f?
a?,f+ a?,f?1027 1545Figure 2: Results of the human pairwise prefer-ence ranking experiment, given as the joint dis-tribution of both rankings: a+ denotes preferencefor TSR-CNN in terms of accuracy, f+ in terms offluency; a?
denotes preference for the in-domainbaseline in terms of accuracy, f?
in terms of flu-ency.The text-based retrieval baseline TSR-TXTnever significantly outperformed the in-domaincdec baseline, but there were slight nominal im-provements in terms of BLEU, METEOR andTER.
This finding is actually consistent withW?aschle and Riezler (2015) who report perfor-mance gains for text-based, target side retrievalmodels only on highly technical, narrow-domaincorpora and even report performance degradationon medium-diversity corpora such as Europarl.Our experiments show that it is the addition ofvisual similarity information by incorporation ofmultimodal pivots into the image-enhanced mod-els TSR-CNN and TSR-HCA which makes suchtechniques effective on MS COCO, thus uphold-ing our hypothesis that visual information can beexploited for improvement of caption translation.4.6 Human EvaluationThe in-domain baseline and TSR-CNN differed intheir output in 169 out of 500 cases on the testset.
These 169 cases were presented to a humanjudge alongside the German source captions in adouble-blinded pairwise preference ranking exper-iment.
The order of presentation was randomizedfor the two systems.
The judge was asked to rankfluency and accuracy of the translations indepen-dently.
The results are given in Figure 2.
Overall,there was a clear preference for the output of TSR-CNN.24054.7 ExamplesTable 5 shows example translations produced byboth cdec baselines, TSR-TXT, TSR-CNN, andTSR-HCA, together with source caption, image,and reference translation.
The visual informationinduced by target side captions of pivot images al-lows a disambiguation of translation alternativessuch as ?skirt?
versus ?rock (music)?
for the Ger-man ?Rock?, ?pole?
versus ?mast?
for the Ger-man ?Masten?, and is able to repair mistransla-tions such as ?foot?
instead of ?mouth?
for theGerman ?Maul?.5 Conclusions and Further WorkWe demonstrated that the incorporation of multi-modal pivots into a target-side retrieval model im-proved SMT performance compared to a strongin-domain baseline in terms of BLEU, METEORand TER on our parallel dataset derived from MSCOCO.
The gain in performance was comparablebetween a distance metric based on a deep convo-lutional network and one based on human objectcategory annotations, demonstrating the effective-ness of the CNN-derived distance measure.
Usingour approach, SMT can, in certain cases, profitfrom multimodal context information.
Crucially,this is possible without using large amounts of in-domain parallel text data, but instead using largeamounts of monolingual image captions that aremore readily available.Learning semantically informative distancemetrics using deep learning techniques is an areaunder active investigation (Wu et al, 2013; Wanget al, 2014; Wang et al, 2015).
Despite the factthat our simple distance metric performed com-parably to human object annotations, using suchhigh-level semantic distance metrics for captiontranslation by multimodal pivots is a promising av-enue for further research.The results were achieved on one language pair(German-English) and one corpus (MS COCO)only.
As with all retrieval-based methods, gener-alized statements about the relative performanceon corpora of various domains, sizes and qualitiesare difficult to substantiate.
This problem is aggra-vated in the multimodal case, since the relevanceof captions with respect to images varies greatlybetween different corpora (Hodosh et al, 2013).In future work, we plan to evaluate our approachin more naturalistic settings, such machine transla-tion for captions in online multimedia repositoriesImage:Source: Eine Person in einem Anzug undKrawatte und einem Rock.cdec out-dom: a person in a suit and tie and a rock .cdec in-dom: a person in a suit and tie and a rock .TSR-TXT: a person in a suit and tie and a rock .TSR-CNN: a person in a suit and tie and a skirt .TSR-HCA: a person in a suit and tie and a rock .Reference: a person wearing a suit and tie and askirtImage:Source: Ein Masten mit zwei Ampeln f?ur Aut-ofahrer.cdec out-dom: a mast with two lights for drivers .cdec in-dom: a mast with two lights for drivers .TSR-TXT: a mast with two lights for drivers .TSR-CNN: a pole with two lights for drivers .TSR-HCA: a pole with two lights for drivers .Reference: a pole has two street lights on it fordrivers .Image:Source: Ein Hund auf einer Wiese mit einemFrisbee im Maul.cdec out-dom: a dog on a lawn with a frisbee in thefoot .cdec in-dom: a dog with a frisbee in a grassy field .TSR-TXT: a dog with a frisbee in a grassy field .TSR-CNN: a dog in a grassy field with a frisbee inits mouth .TSR-HCA: a dog with a frisbee in a grassy field .Reference: a dog in a field with a frisbee in itsmouthTable 5: Examples for improved caption transla-tion by multimodal feedback.2406such as Wikimedia Commons16and digitized artcatalogues, as well as e-commerce localization.A further avenue of future research is improv-ing models such as that presented in Elliott etal.
(2015) by crucial components of neural MTsuch as ?attention mechanisms?.
For example,the attention mechanism of Bahdanau et al (2015)serves as a soft alignment that helps to guide thetranslation process by influencing the sequencein which source tokens are translated.
A similarmechanism is used in Xu et al (2015) to decidewhich part of the image should influence whichpart of the generated caption.
Combining thesetwo types of attention mechanisms in a neural cap-tion translation model is a natural next step in cap-tion translation.
While this is beyond the scope ofthis work, our models should provide an informa-tive baseline against which to evaluate such meth-ods.AcknowledgmentsThis research was supported in part by DFGgrant RI-2221/2-1 ?Grounding Statistical MachineTranslation in Perception and Action?, and byan Amazon Academic Research Award (AARA)?Multimodal Pivots for Low Resource MachineTranslation in E-Commerce Localization?.ReferencesDzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proceedings ofthe International Conference on Learning Represen-tations (ICLR), San Diego, California, USA.Shane Bergsma and Benjamin Van Durme.
2011.Learning bilingual lexicons using the visual similar-ity of labeled web images.
In Proceedings of theInternational Joint Conference on Artificial Intelli-gence (IJCAI), Barcelona, Spain.Iacer Calixto, Te?ofilo de Compos, and Lucia Spe-cia.
2012.
Images as context in statistical machinetranslation.
In Proceedings of the Workshop on Vi-sion and Language (VL), Sheffield, England, UnitedKingdom.David Chiang.
2007.
Hierarchical phrase-based trans-lation.
Computational Linguistics, 33(2):201?228.Jonathan Clark, Chris Dyer, Alon Lavie, and NoahSmith.
2011.
Better hypothesis testing for statistical16https://commons.wikimedia.org/wiki/Main_Pagemachine translation: Controlling for optimizer insta-bility.
In Proceedings of the Association for Compu-tational Lingustics (ACL), Portland, Oregon, USA.Koby Crammer and Yoram Singer.
2003.
Ultracon-servative online algorithms for multiclass problems.Journal of Machine Learning Research, 3:951?991.Chris Dyer, Adam Lopez, Juri Ganitkevitch, JohnathanWeese, Ferhan Ture, Phil Blunsom, Hendra Seti-awan, Vladimir Eidelman, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the Association for ComputationalLinguistics (ACL), Uppsala, Sweden.Chris Dyer.
2009.
Using a maximum entropy model tobuild segmentation lattices for mt.
In Proceedingsof Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies (NAACL HLT), Boul-der, Colorado, USA.Desmond Elliott, Stella Frank, and Eva Hasler.
2015.Multi-language image description with neural se-quence models.
CoRR, abs/1510.04709.Hao Fang, Li Deng, Margaret Mitchell, Saurabh Gupta,Piotr Dollar, John C. Platt, Forrest Iandola, JianfengGao, C. Lawrence Zitnick, Rupesh K. Srivastava,Xiaodeng He, and Geoffrey Zweit.
2015.
From cap-tions to visual concepts and back.
In In Proceedingsof the 28th IEEE Conference on Computer Visionand Pattern Recognition (CVPR), Boston, MA.Francis Ferraro, Nasrin Mostafazadeh, Ting-Hao (Ken-neth) Huang, Lucy Vanderwende, Jacob Devlin,Michel Galley, and Margaret Mitchell.
2015.
Asurvey of current datasets for vision and languageresearch.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing(EMNLP), Lisbon, Portugal.Ruka Funaki and Hideki Nakayama.
2015.
Image-mediated learning for zero-shot cross-lingual docu-ment retrieval.
In Proceedings of the Conference onEmpirical Methods in Natural Language Processing(EMNLP), Lisbon, Portugal.Michael Grubinger, Paul Clough, Henning M?uller, andThomas Deselaers.
2006.
The IAPR TC-12 bench-mark: A new evaluatioin resource for visual infor-mation systems.
In In Proceedings of LREC, Gen-ova, Italy.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modi-fied Kneser-Ney language model estimation.
In Pro-ceedings of the 51st Annual Meeting of the Asso-ciation for Computational Linguistics (ACL), Sofia,Bulgaria.Kenneth Heafield.
2011.
KenLM: faster and smallerlanguage model queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation(WMT), Edinburgh, Scotland, United Kingdom.2407Micah Hodosh, Peter Young, and Julia Hockenmaier.2013.
Framing image description as a ranking task:Data, models and evaluation metrics.
Journal of Ar-tificial Intelligence Research, 47:853?899.Andrey Karpathy and Li Fei-Fei.
2015.
Deep visual-semantic alignments for generating image descrip-tions.
In Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition (CVPR),Boston, Massachusetts, USA.Douwe Kiela, Ivan Vuli?c, and Stephen Clark.
2015.Visual bilingual lexicon induction with transferredconvnet features.
In Proceedings of the Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), Lisbon, Portugal.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In Proceedings of theMachine Translation Summit, Phuket, Thailand.Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Sim-ing Li, Yejin Choi, Alexander C Berg, and Tamara LBerg.
2011.
Baby talk: Understanding and generat-ing image descriptions.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition (CVPR), Colorado Springs, Colorado, USA.Tsung-Yi Lin, Michael Maire, Serge Belongie,Lubomir D. Bourdev, Ross B. Girshick, James Hays,Pietro Perona, Deva Ramanan, Piotr Doll?ar, andC.
Lawrence Zitnick.
2014.
Microsoft COCO: com-mon objects in context.
Computing Research Repos-itory, abs/1405.0312.Kishore Papineni, Salim Roukos, Todd Ard, and Wei-Jing Zhu.
2002.
Bleu: a method for automatic eval-uation of machine translation.
In Proceedings of the40th Annual Meeting of the Association for Compu-tational Linguistics (ACL), Philadelphia, Pennsylva-nia, USA.Cyrus Rashtchian, Peter Young, Micah Hodosh, andJulia Hockenmaier.
2010.
Collecting image annota-tions using amazon?s mechanical turk.
In Proceed-ings of the NAACL HLT 2010 Workshop on CreatingSpeech and Language Data with Amazon?s Mechan-ical Turk, Los Angeles, California, USA.Stefan Riezler and John Maxwell.
2005.
On some pit-falls in automatic evaluation and significance testingfor mt.
In Proceedings of the Workshop on Intrinsicand Extrinsic Evaluation Methods for MT and Sum-marization (MTSE) at the 43rd Annual Meeting ofthe Association for Computational Linguistics, AnnArbor, Michigan, USA.Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause,Sanjeev Satheesh, Sean Ma, Zhiheng Huang, An-drej Karpathy, Aditya Khosla, Michael S. Bernstein,Alexander C. Berg, and Fei-Fei Li.
2014.
Imagenetlarge scale visual recognition challenge.
ComputingResearch Repository, abs/1409.0575.Carina Silberer and Mirella Lapata.
2014.
Learn-ing grounded meaning representations with autoen-coders.
In Proceedings of the 52nd Annual Meet-ing of the Association for Computational Linguistics(ACL), Baltimore, Maryland, USA.Karen Simonyan and Andrew Zisserman.
2015.
Verydeep convolutional networks for large-scale imagerecognition.
In Proceedings of the InternationalConference on Learning Representations (ICLR),San Diego, CA.Jason Smith, Herve Saint-Amand, Magdalena Pla-mada, Philipp Koehn, Chris Callison-Burch, andAdam Lopez.
2013.
Dirt cheap web-scale paral-lel text from the Common Crawl.
In Proceedings ofthe Conference of the Association for ComputationalLinguistics (ACL), Sofia, Bulgaria.Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded compositional semantics for finding anddescribing images with sentences.
Transactionsof the Association for Computational Linguistics,2(1):207?218.Karen Sp?arck Jones.
1972.
A statistical interpretationof term specificity and its application in retrieval.Journal of Documentation, 28:11?21.Oriol Vinyals, Alexander Toshev, Samy Bengio, andDumitru Erhan.
2015.
Show and tell: A neural im-age caption generator.
In Proceedings of the IEEEConference on Computer Vision and Pattern Recog-nition (CVPR), Boston,Massachusetts, USA.Jiang Wang, Yang Song, Thomas Leung, Chuck Rosen-berg, Jingbin Wang, James Philbin, Bo Chen, andYing Wu.
2014.
Learning fine-grained image simi-larity with deep ranking.
In Proceedings of the Con-ference on Computer Vision and Pattern Recognition(CVPR), Columbus, Ohio, USA.Zhaowen Wang, Jianchao Yang, Zhe Lin, JonathanBrandt, Shiyu Chang, and Thomas Huang.
2015.Scalable similarity learning using large marginneighborhood embedding.
In Proceedings of theIEEE Winter Conference on Applications of Com-puter Vision, Washington, DC, USA.Katharina W?aschle and Stefan Riezler.
2015.
Integrat-ing a large, monolingual corpus as translation mem-ory into statistical machine translation.
In Proceed-ings of the 18th Annual Conference of the EuropeanAssociation for Machine Translation (EAMT), An-talya, Turkey.Pengcheng Wu, Steven C.H.
Hoi, Hao Xia, Peilin Zhao,Dayong Wang, and Chunyan Miao.
2013.
Onlinemultimodal deep similarity learning with applicationto image retrieval.
In Proceedings of the 21st ACMInternational Conference on Multimedia, Barcelona,Spain.2408Kelvin Xu, Jimmy Ba, Ryan Kiros, Kyunghyun Cho,Aaron Courville, Ruslan Salakhutdinov, RichardZemel, and Yoshua Bengio.
2015.
Show, attend andtell: Neural image caption generation with visual at-tention.
In Proceedings of the International Confer-ence on Machine Learning (ICML), Lille, France.2409
