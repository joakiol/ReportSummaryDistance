Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 91?96,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsMETA: A Unified Toolkit for Text Retrieval and AnalysisSean Massung, Chase Geigle and ChengXiang ZhaiComputer Science Department, College of EngineeringUniversity of Illinois at Urbana-Champaign{massung1,geigle1,czhai}@illinois.eduAbstractMETA is developed to unite machinelearning, information retrieval, and natu-ral language processing in one easy-to-usetoolkit.
Its focus on indexing allows it toperform well on large datasets, supportingonline classification and other out-of-corealgorithms.
META?s liberal open sourcelicense encourages contributions, and itsextensive online documentation, forum,and tutorials make this process straight-forward.
We run experiments and showMETA?s performance is competitive withor better than existing software.1 A Unified FrameworkAs NLP techniques become more and more ma-ture, we have great opportunities to use them todevelop and support many applications, such assearch engines, classifiers, and integrative applica-tions that involve multiple components.
It?s possi-ble to develop each application from scratch, butit?s much more efficient to have a general toolkitthat supports multiple application types.Existing tools tend to specialize on one partic-ular area, and as such there is a wide variety oftools one must sample when performing differentdata science tasks.
For text-mining tasks, this iseven more apparent; it is extremely difficult (if notimpossible) to find tools that support both tradi-tional information retrieval tasks (like tokeniza-tion, indexing, and search) alongside traditionalmachine learning tasks (like document classifica-tion, regression, and topic modeling).Table 1 compares META?s many featuresacross various dimensions.
Note that only METAsatisfies all the areas while other toolkits focus ona particular area.
In the case where the desiredfunctionality is scattered, data science students, re-searchers, and practitioners must find the appro-priate software packages for their needs and com-pile and configure each appropriate tool.
Then,there is the problem of data formatting?it is un-likely that the tools all have standardized upon asingle input format, so a certain amount of ?datamunging?
is required.
All of this detracts from theactual task at hand, which has a marked impact onproductivity.The goal of the META project is to addressthese issues.
In particular, we provide a uni-fying framework for existing machine learningand natural language processing algorithms, al-lowing researchers to quickly run controlled ex-periments.
We have modularized the feature gen-eration, instance representation, data storage for-mats, and algorithm implementations; this allowsusers to make seamless transitions along any ofthese dimensions with minimal effort.
Finally,META is dual-licensed under the University ofIllinois/NCSA Open Source Licence and the MITLicense to reach the broadest audience possible.Due to space constraints, in this paper, we onlydelve into META?s natural language processing(NLP), information retrieval (IR), and machinelearning (ML) components in section 3.
However,we briefly outline all of its components here:Feature generation.
META has a collection oftokenizers, filters, and analyzers that convert rawtext into a feature representation.
Basic featuresare n-gram words, but other analyzers make useof different parts of the toolkit, such as POS tag n-grams and parse tree features.
An arbitrary num-ber of feature representations may be combined;for example, a document could be represented asunigram words, bigram POS tags, and parse treerewrite rules.
Users can easily add their own fea-ture types as well, such as sentence length distri-bution in a document.Search.
The META search engine can store91Indri Lucene MALLET LIBLINEAR SVMMULTscikit CoreNLP METAIR IR ML/NLP ML ML ML/NLP ML/NLP allFeature generation X X X X X XSearch X X XClassification X X X X X XRegression X X X X X XPOS tagging X X XParsing X XTopic models X X Xn-gram LM XWord embeddings X X XGraph algorithms XMultithreading X X X X XTable 1: Toolkit feature comparison.
Citations for all toolkits may be found in their respective comparison sections.document feature vectors in an inverted index andscore them with respect to a query.
Rankersinclude vector space models such as OkapiBM25 (Robertson et al, 1994) and probabilisticmodels like Dirichlet prior smoothing (Zhai andLafferty, 2004).
A search demo is online1.Classification.
META includes a normalizedadaptive stochastic gradient descent (SGD) im-plementation (Ross et al, 2013) with pluggableloss functions, allowing creation of an SVM clas-sifier (among others).
Both `1(Tsuruoka et al,2009) and `2regularization are supported.
Ensem-ble methods for binary classifiers allow multiclassclassification.
Other classifiers like na?
?ve Bayesand k-nearest neighbors also exist.
A confusionmatrix class and significance testing framework al-low evaluation and comparison of different meth-ods and feature representations.Regression.
Regression via SGD predictsreal-valued responses from featurized documents.Evaluation metrics such as mean squared error andR2score allow model comparison.POS tagging.
META contains a linear-chainconditional random field for POS tagging andchunking applications, learned using `2regular-ized SGD (Sutton and McCallum, 2012).
It alsocontains an efficient greedy averaged perceptrontagger (Collins, 2002).Parsing.
A fast shift-reduce constituency parserusing generalized averaged perceptron (Zhu et al,2013) is META?s grammatical parser.
Parse treefeaturizers implement different types of structuraltree representations (Massung et al, 2013).
AnNLP demo online presents tokenization, POS-tagging, and parsing2.Topic models.
META can learn topic1https://meta-toolkit.org/search-demo.html2https://meta-toolkit.org/nlp-demo.htmlmodels over any feature representation usingcollapsed variational Bayes (Asuncion et al,2009), collapsed Gibbs sampling (Griffiths andSteyvers, 2004), stochastic collapsed variationalBayes (Foulds et al, 2013), or approximate dis-tributed LDA (Newman et al, 2009).n-gram language models (LMs).
META takesan ARPA-formatted input3and creates a languagemodel that can be queried for token sequenceprobabilities or used in downstream applicationslike SyntacticDiff (Massung and Zhai, 2015).Word embeddings.
The GloVe algorithm (Pen-nington et al, 2014) is implemented in a streamingframework and also features an interactive seman-tic relationship demo.
Word vectors can be usedin other applications as part of the META API.Graph algorithms.
Directed and undirectedgraph implementations exist and various algo-rithms such as betweenness centrality, PageRank,and myopic search are available.
Random graphgeneration models like Watts-Strogatz and prefer-ential attachment exist.
For these algorithms seeEasley and Kleinberg (2010).Multithreading.
When possible, META algo-rithms and applications are parallelized using C++threads to make full use of available resources.2 UsabilityConsistency across components is a key fea-ture that allows META to work well with largedatasets.
This is accomplished via a three-layerarchitecture.
On the first layer, we have tokeniz-ers, analyzers, and all the text processing that ac-companies them.
Once a document representa-tion is determined, this tool chain is run on a cor-pus.
The indexes are the second layer; they pro-3http://www.speech.sri.com/projects/srilm/manpages/ngram-format.5.html92vide an efficient format for storing processed data.The third layer?the application layer?interfacessolely with indexes.
This means that we may usethe same index for running an SVM as we do toevaluate a ranking function, without processingthe data again.Since all applications use these indexes, METAsupports out-of-core classification with some clas-sifiers.
We ran our large classification dataset thatdoesn?t fit in memory?Webspam (Webb et al,2006)?using the sgd classifier.
Where LIBLIN-EAR failed to run, META was able to finish theclassification in a few minutes.Besides using META?s rich built-in feature gen-eration, it is possible to directly use LIBSVM-formatted data.
This allows preprocessed datasetsto be run under META?s algorithms.
Additionally,META?s forward index (used for classifica-tion), is easily convertible to LIBSVM format.
Thereverse is also true: you may do feature genera-tion with META, and use it to generate input forany other program that supports LIBSVM format.META is hosted publicly on GitHub4, whichprovides the project with community involvementthrough its bug/issue tracker and fork/pull requestmodel.
Its API is heavily documented5, allowingthe creation of Web-based applications (listed insection 1).
The project website contains several tu-torials that cover the major aspects of the toolkit6to enable users to get started as fast as possiblewith little friction.
Additionally, a public forum7is accessible for all users to view and participate inuser support topics, community-written documen-tation, and developer discussions.A major design point in META is to allow formost of the functionality to be configured via aconfiguration file.
This enables minimal effort ex-ploratory data analysis without having to write (orrecompile) any code.
Designing the code in thisway also encourages the components of the systemto be pluggable: the entire indexing process, forexample, consists of several modular layers whichcan be controlled by the configuration file.An example snippet of a config file is givenbelow; this creates a bigram part-of-speech ana-lyzer.
Multiple [[analyzers]] sections maybe added, which META automatically combineswhile processing input.4https://github.com/meta-toolkit/meta/5https://meta-toolkit.org/doxygen/namespaces.html6https://meta-toolkit.org/7https://forum.meta-toolkit.org/[[analyzers]]method = "ngram-pos"ngram = 2filter = [{type = "icu-tokenizer"},{type = "ptb-normalizer"}]crf-prefix = "crf/model/folder"A simple class hierarchy allows users to add fil-ters, analyzers, ranking functions, and classifierswith full integration to the toolkit (e.g.
one mayspecify user-defined classes in the config file).
Theprocess for adding these is detailed in the METAonline tutorials.This low barrier of entry experiment setup easeled to META?s use in text mining and analysisMOOCs reaching over 40,000 students8,9.Multi-language support is hard to do correctly.Many toolkits sidestep this issue by only support-ing ASCII text or the OS language; META sup-ports multiple (non-romance) languages by de-fault, using the industry standard ICU library10.This allows META to tokenize arbitrarily-encodedtext in many languages.Unit tests ensure that contributors are confidentthat their modifications do not break the toolkit.Unit tests are automatically run after each commitand pull request, so developers immediately knowif there is an issue (of course, unit tests may be runmanually before committing).
The unit tests arerun in a continuous integration setup where METAis compiled and run on Linux, Mac OS X11, andWindows12under a variety of compilers and soft-ware development configurations.3 ExperimentsWe evaluate META?s performance in NLP, IR, andML tasks.
All experiments were performed on aworkstation with an Intel(R) Core(TM) i7-5820KCPU, 16 GB of RAM, and a 4 TB 5900 RPM disk.3.1 Natural Language ProcessingMETA?s part-of-speech taggers for English pro-vide quite reasonable performance.
It provides alinear-chain CRF tagger (CRF) as well as an av-eraged perceptron based greedy tagger (AP).
Wereport the token level accuracy on sections 22?24of the Penn Treebank, with a few prior model re-sults trained on sections 0?18 in Table 3.
?Hu-man annotators?
is an estimate based on a 3% er-ror rate reported in the Penn Treebank README8https://www.coursera.org/course/textretrieval9https://www.coursera.org/course/textanalytics10http://site.icu-project.org/11https://travis-ci.org/meta-toolkit/meta12https://ci.appveyor.com/project/skystrife/meta93CoreNLP METATraining Testing F1Training Testing F1Greedy7m 27s 18.6s86.717m 31s 12.9s86.98.85 GB 1.53 GB 0.79 GB 0.29 GBBeam (4)6h 10m 43s 46.8s89.92h 17m 25s 59.2s88.110.84 GB 3.83 GB 2.29 GB 0.94 GBTable 2: (NLP) Training/testing performance for the shift-reduce constituency parsers.
All models were trained for 40 iterationson the standard training split of the Penn Treebank.
Accuracy is reported as labeled F1from evalb on section 23.Extra Data AccuracyHuman annotators 97.0%CoreNLP X 97.3%LTag-Spinal 97.3%SCCN X 97.5%META (CRF) 97.0%META (AP) 96.9%Table 3: (NLP) Part-of-speech tagging token-level accura-cies.
?Extra data?
implies the use of large amounts of extraunlabeled data (e.g.
for distributional similarity features).Docs Size |D|avg|V |Blog06 3,215,171 26 GB 782.3 10,971,746Gov2 25,205,179 147 GB 515.5 21,203,125Table 4: (IR) The two TREC datasets used.
Uncleaned ver-sions of blog06 and gov2 were 89 GB and 426 GB respec-tively.and is likely overly optimistic (Manning, 2011).CoreNLP?s model is the result of Manning (2011),LTag-Spinal is from Shen et al (2007), and SCCNis from S?gaard (2011).
Both of META?s taggersare within 0.6% of the existing literature.META and CoreNLP both provide implementa-tions of shift-reduce constituency parsers, follow-ing the framework of Zhu et al (2013).
These canbe trained greedily or via beam search.
We com-pared the parser implementations in META andCoreNLP along two dimensions?speed, mea-sured in wall time, and memory consumption,measured as maximum resident set size?for bothtraining and testing a greedy and beam searchparser (with a beam size of 4).
Training was per-formed on the standard training split of sections 2?21 of the Penn Treebank, with section 22 used asa development set (only used by CoreNLP).
Sec-tion 23 was held out for evaluation.
The results aresummarized in Table 2.META consistently uses less RAM thanCoreNLP, both at training time and testing time.Its training time is slower than CoreNLP for thegreedy parser, but less than half of CoreNLP?straining time for the beam parser.
META?s beamparser has worse labeled F1score, likely the resultIndri Lucene MeTABlog06 55m 40s 20m 23s 11m 23sGov2 8h 13m 43s 1h 59m 42s 1h 12m 10sTable 5: (IR) Indexing speed.Indri Lucene MeTABlog06 31.02 GB 2.06 GB 2.84 GBGov2 170.50 GB 11.02 GB 10.24 GBTable 6: (IR) Index size.of its simpler model averaging strategy13.
Overall,however, META?s shift-reduce parser is competi-tive and particularly lightweight.3.2 Information RetrievalMETA?s IR performance is compared with twowell-known search engine toolkits: LUCENE?slatest version 5.5.014and INDRI?s version5.9 (Strohman et al, 2005)15.We use the TREC blog06 (Ounis et al,2006) permalink documents and TREC gov2 cor-pus (Clarke et al, 2004).
To ensure a more uni-form indexing environment, all HTML is cleanedbefore indexing.
In addition, each corpus is con-verted into a single file with one document per lineto reduce the effects of many file operations.During indexing, terms are lower-cased, stopwords are removed from a common list of 431 stopwords, Porter2 (META) or Porter (Indri, Lucene)stemming is performed, a maximum word lengthof 32 characters is set, original documents are notstored in the index, and term position informationis not stored16.We compare the following: indexing speed (Ta-ble 5), index size (Table 6), query speed (Table 7),and query accuracy (Table 8) with BM25 usingk1= 0.9 and b = 0.4.
We use the standardTREC queries associated with each dataset and13At training time, both CoreNLP and META perform model averaging, butMETA computes the average over all updates and CoreNLP performs cross-validation over a default of the best 8 models on the development set.14http://lucene.apache.org/15Indri 5.10 does not provide source code packages and thus could not beused.
It is also known as LEMUR.16For Indri, we are unable to disable positions information storage.94Indri Lucene MeTABlog06 55.0s 1.60s 3.67sGov2 24m 6.73s 57.53s 1m 3.98sTable 7: (IR) Query speed.Indri Lucene MeTAMAP P@10 MAP P@10 MAP P@10Blog06 29.13 63.20 29.10 63.60 32.34 64.70Gov2 25.96 53.69 30.23 59.26 29.97 57.43Table 8: (IR) Query performance via Mean Average Precisionand Precision at 10 documents.score each system?s search results with the usualtrec eval program17.META leads in indexing speed, though wenote that META?s default indexer is multithreadedand LUCENE does not provide a parallel one18.META creates the smallest index for gov2 whileLUCENE creates the smallest index for blog06;INDRI greatly lags behind both.
META followsLUCENE closely in retrieval speed, with INDRIagain lagging.
As expected, query performancebetween the three systems is relatively even, andwe attribute any small difference in MAP or preci-sion to idiosyncrasies during tokenization.3.3 Machine LearningMETA?s ML performance is compared with LI-BLINEAR (Fan et al, 2008), SCIKIT-LEARN (Pe-dregosa et al, 2011), and SVMMULTICLASS19.We focus on linear classification with SVM acrossthese tools (MALLET (McCallum, 2002) does notprovide an SVM, so it is excluded from the com-parisons).
Statistics for the four ML datasets canbe found in Table 9.The 20news dataset (Lang, 1995)20is split intoits standard 60% training and 40% testing sets bypost date.
The Blog dataset (Schler et al, 2006) issplit into 80% training and 20% testing randomly.Both of these two textual datasets were prepro-cessed using META using the same settings fromthe IR experiments.The rcv1 dataset (Lewis et al, 2004) was pro-cessed into a training and testing set using theprep rcv1 tool provided with Leon Bottou?sSGD tool21.
The resulting training set has 781,265documents and the testing set has 23,149.
The17http://trec.nist.gov/trec_eval/18Additionally, we did not feel that writing a correct and threadsafe indexeras a user is something to be reasonably expected.19http://www.cs.cornell.edu/people/tj/svm_light/svm_multiclass.html20http://qwone.com/?jason/20Newsgroups/21http://leon.bottou.org/projects/sgdDocs Size k Features20news 18,846 86 MB 20 112,377Blog 19,320 778 MB 3 548,812rcv1 804,414 1.1 GB 2 47,152Webspam 350,000 24 GB 2 16,609,143Table 9: (ML) Datasets used for k-class categorization.liblinear scikit SVMmultMETA20news79.4% 74.3% 67.1% 80.1%2.58s 0.326s 2.54s 0.648sBlog75.8% 76.2% 72.2% 72.2%61.3s 0.801s 17.5s 1.11srcv194.7% 94.0% 83.6% 94.8%17.6s 1.66s 2.01s 3.44sWebspam 797.4%799.4%11m 52s 1m 16sTable 10: (ML) Accuracy and speed classification results.Reported time is to both train and test the model.
For allexcept Webspam, this excludes IO.Webspam corpus (Webb et al, 2006) consistsof the subset of the Webb Spam Corpus usedin the Pascal Large Scale Learning Challenge22.The corpus was processed using the providedconvert.py into byte trigrams.
The first 80%of the resulting file is used for training and the last20% for testing.In Table 10, we can see that META performswell both in terms of speed and accuracy.
BothLIBLINEAR and SVMMULTICLASS were unableto produce models on the Webspam dataset due tomemory limitations and lack of a minibatch frame-work.
For SCIKIT-LEARN and META, we brokethe training data into 4 equal sized batches andran one iteration of SGD per batch.
The timingresult includes the time to load each chunk intomemory; for META this is from its forward-indexformat23and for SCIKIT-LEARN this is from LIB-SVM-formatted text files.4 ConclusionMETA is a valuable resource for text mining ap-plications; it is a viable and competitive alternativeto existing toolkits that unifies algorithms fromNLP, IR, and ML.
META is an extensible, con-sistent framework that enables quick developmentof complex application systems.AcknowledgementsThis material is based upon work supported by theNSF GRFP under Grant Number DGE-1144245.22ftp://largescale.ml.tu-berlin.de/largescale/23It took 12m 24s to generate the index.95ReferencesArthur Asuncion, Max Welling, Padhraic Smyth,and Yee Whye Teh.
2009.
On Smoothing andInference for Topic Models.
In UAI.Charles L. A. Clarke, Nick Craswell, and IanSoboroff.
2004.
Overview of the TREC 2004Terabyte Track.
In TREC.Michael Collins.
2002.
Discriminative TrainingMethods for Hidden Markov Models: Theoryand Experiments with Perceptron Algorithms.In EMNLP.David Easley and Jon Kleinberg.
2010.
Net-works, Crowds, and Markets: Reasoning Abouta Highly Connected World.
Cambridge Univer-sity Press, New York, NY, USA.R.
Fan, K. Chang, C. Hsieh, X. Wang, and C. Lin.2008.
LIBLINEAR: A Library for Large LinearClassification.
JMLR pages 1871?1874.J.
Foulds, L. Boyles, C. DuBois, P. Smyth, andM.
Welling.
2013.
Stochastic Collapsed Vari-ational Bayesian Inference for Latent DirichletAllocation.
In KDD.T.
L. Griffiths and M. Steyvers.
2004.
Finding Sci-entific Topics.
PNAS 101.Ken Lang.
1995.
Newsweeder: Learning to filternetnews.
In ICML.D.
D. Lewis, Y. Yang, T. G. Rose, and F. Li.
2004.RCV1: A New Benchmark Collection for TextCategorization Research.
JMLR 5.Christopher D. Manning.
2011.
Part-of-speechtagging from 97% to 100%: Is it time for somelinguistics?
In Proc.
CICLing.Sean Massung and ChengXiang Zhai.
2015.
Syn-tacticDiff: Operator-based Transformation forComparative Text Mining.
In IEEE Interna-tional Conference on Big Data.Sean Massung, ChengXiang Zhai, and Julia Hock-enmaier.
2013.
Structural Parse Tree Featuresfor Text Representation.
In Proc.
IEEE ICSC.Andrew Kachites McCallum.
2002.
MALLET:A Machine Learning for Language Toolkit.http://mallet.cs.umass.edu/.David Newman, Arthur Asuncion, PadhraicSmyth, and Max Welling.
2009.
Distributed Al-gorithms for Topic Models.
JMLR 10.I.
Ounis, C. Macdonald, M. de Rijke, G. Mishne,and I. Soboroff.
2006.
Overview of the TREC2006 Blog Track.
In TREC.F.
Pedregosa, G. Varoquaux, A. Gramfort,V.
Michel, B. Thirion, O. Grisel, M. Blondel,P.
Prettenhofer, R. Weiss, V. Dubourg, J. Van-derPlas, A. Passos, D. Cournapeau, M. Brucher,M.
Perrot, and E. Duchesnay.
2011.
Scikit-learn: Machine Learning in Python.
JMLR 12.Jeffrey Pennington, Richard Socher, and Christo-pher D. Manning.
2014.
GloVe: Global Vectorsfor Word Representation.
In EMNLP.Stephen E. Robertson, Steve Walker, Susan Jones,Micheline Hancock-Beaulieu, and Mike Gat-ford.
1994.
Okapi at TREC-3.
In TREC.St?ephane Ross, Paul Mineiro, and John Langford.2013.
Normalized online learning.
In UAI.Jonathan Schler, Moshe Koppel, Shlomo Arga-mon, and James W. Pennebaker.
2006.
Ef-fects of Age and Gender on Blogging.
In AAAISpring Symposium: Computational Approachesto Analyzing Weblogs.Libin Shen, Giorgio Satta, and Aravind Joshi.2007.
Guided learning for bidirectional se-quence classification.
In ACL.Anders S?gaard.
2011.
Semi-supervised con-densed nearest neighbor for part-of-speech tag-ging.
In ACL-HLT .Trevor Strohman, Donald Metzler, Howard Turtle,and W. Bruce Croft.
2005.
Indri: A language-model based search engine for complex queries(extended version).
IR 407, University of Mas-sachusetts.Charles Sutton and Andrew McCallum.
2012.
AnIntroduction to Conditional Random Fields.
InFoundations and Trends in Machine Learning.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and SophiaAnaniadou.
2009.
Stochastic gradient descenttraining for l1-regularized log-linear modelswith cumulative penalty.
In ACLIJCNLP.Steve Webb, James Caverlee, and Carlton Pu.2006.
Introducing the webb spam corpus: Us-ing email spam to identify web spam automati-cally.
In CEAS.ChengXiang Zhai and John Lafferty.
2004.
AStudy of Smoothing Methods for LanguageModels Applied to Information Retrieval.
ACMTrans.
Inf.
Syst.
22(2).Muhua Zhu, Yue Zhang, Wenliang Chen, MinZhang, and Jingbo Zhu.
2013.
Fast and Accu-rate Shift-Reduce Constituent Parsing.
In ACL.96
