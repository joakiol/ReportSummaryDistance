Proceedings of the 8th International Natural Language Generation Conference, pages 138?142,Philadelphia, Pennsylvania, 19-21 June 2014. c?2014 Association for Computational LinguisticsMulti-adaptive Natural Language Generation using Principal ComponentRegressionDimitra Gkatzia, Helen Hastie, and Oliver LemonSchool of Mathematical and Computer Sciences, Heriot-Watt University, Edinburgh{dg106, h.hastie, o.lemon}@hw.ac.ukAbstractWe present FeedbackGen, a system thatuses a multi-adaptive approach to Natu-ral Language Generation.
With the term?multi-adaptive?, we refer to a systemthat is able to adapt its content to dif-ferent user groups simultaneously, in ourcase adapting to both lecturers and stu-dents.
We present a novel approach tostudent feedback generation, which simul-taneously takes into account the prefer-ences of lecturers and students when de-termining the content to be conveyed ina feedback summary.
In this framework,we utilise knowledge derived from rat-ings on feedback summaries by extract-ing the most relevant features using Prin-cipal Component Regression (PCR) anal-ysis.
We then model a reward functionthat is used for training a ReinforcementLearning agent.
Our results with stu-dents suggest that, from the students?
per-spective, such an approach can generatemore preferable summaries than a purelylecturer-adapted approach.1 IntroductionSummarisation of time-series data refers to thetask of automatically generating reports from at-tributes whose values change over time.
Contentselection is the task of choosing what to say, i.e.what information is to be included in a report (Re-iter and Dale, 2000).
We consider the task of auto-matically generating feedback summaries for stu-dents describing their performance during the labof a computer science module over the semester.Various factors can influence students?
learn-ing such as difficulty of the material (Person etal., 1995), workload (Craig et al., 2004), atten-dance in lectures (Ames, 1992), etc.
These fac-tors change over time and can be interdependent.In addition, different stakeholders often have con-flicting goals, needs and preferences, for examplemanagers with employees, or doctors with patientsand relatives, or novice and expert users.
In ourdata, for instance, lecturers tend to comment onthe hours that the student studied, whereas the stu-dents disprefer this content.
In our previous work,we showed that lecturers and students have dif-ferent perceptions regarding what constitutes goodfeedback (Gkatzia et al., 2013).
Here, we present anovel approach to generation by adapting its con-tent to two user groups simultaneously.
Producingthe same summary for two groups is important asit allows for shared context and meaningful furtherdiscussion and reduces development time.2 Related WorkPrevious work on NLG systems that address morethan one user group employs different versions ofa system for each different user group (Gatt et al.,2009; Hunter et al., 2011; Mahamood and Reiter,2011), makes use of User Models (Janarthanamand Lemon, 2010; Thompson et al., 2004; Zuker-man and Litman, 2001) or personalises the outputto individual users using rules (Reiter et al., 1999).Our proposed system adapts the output to the pref-erences of more than one user type1, lecturers andstudents, but instead of developing many differentsystems or using User Models that describe differ-ent users, it attempts to model the middle groundbetween the preferences.In order to identify the users?
preferences, weapply Principal Components Regression (PCR(Jolliffe, 1982)) analysis to two datasets that con-tain lecturers?
and students?
ratings and identifythe most important variables from the principalcomponents, which are then included in a rewardfunction.
This hand-crafted reward function isused for training an RL agent for summarisation1Our approach is different to multi-objective optimisa-tion.138Raw Datafactors week 2 week 3 ... week 10marks 5 4 ... 5hours studied 1 2 ... 3... ... ... ... ...Trends from Datafactors trend(1) marks trend other(2) hours studied trend increasing(3) understandability trend decreasing(4) difficulty trend decreasing(5) deadlines trend increasing(6) health issues trend other(7) personal issues trend decreasing(8) lectures attended trend other(9) revision trend decreasingSummaryYour overall performance was excellentduring the semester.
Keep up the goodwork and maybe try some more challeng-ing exercises.
Your attendance was vary-ing over the semester.
Have a think abouthow to use time in lectures to improve yourunderstanding of the material.
You spent 2hours studying the lecture material onaverage.
You should dedicate more timeto study.
You seem to find the materialeasier to understand compared to thebeginning of the semester.
Keep up thegood work!
You revised part of the learn-ing material.
Have a think whether revis-ing has improved your performance.Table 1: The table on the top left shows an example of the time-series data.
The table on the bottom leftshows an example of described trends.
The box on the right presents a target summary.of time-series data.
Our previous work showedthat when comparing RL and supervised learningin the context of student feedback generation, stu-dents preferred the output generated by the RLsystem (Gkatzia et al., 2014a).
Therefore, here, weused RL rather than a supervised learning method.The work described here builds on work reportedin (Gkatzia et al., 2014b), which uses as a rewardfunction the average of the Lecturer-adapted andStudent-adapted reward functions.
However, thatmethod seems to cancel out the preferences of thetwo groups whereas PCR is able to identify rele-vant content for both groups.In the next section, we describe the data used,and the methodology for the multi-adaptive NLG,as well as two alternative systems.
In Section 4,we describe the comparison of these three systemsin a subjective evaluation and present the results inSection 5.
A discussion follows in Section 6 andfinally, future work is discussed in Section 7.3 MethodologyReinforcement Learning is a machine learningtechnique that defines how an agent learns to takeoptimal sequences of actions so as to maximize acumulative reward (Sutton and Barto, 1998).
Inour framework, the task of summarisation of time-series data is modelled as a Markov Decision Pro-cess, where the decisions on content selection cor-respond to a sequence of actions (see Section 3.2).Temporal Difference (TD) learning (Sutton andBarto, 1990) is used for training three agents ina simulated environment to learn to make optimalcontent selection decisions:1. by adapting to both groups simultaneously,2.
by adapting to lecturers,3.
by adapting to students.3.1 The DataFor this study, the dataset described in (Gkatzia etal., 2013) was used.
Table 1 presents an exam-ple of this dataset that describes a student?s learn-ing factors and an aligned feedback summary pro-vided by a lecturer.
The dataset is composed of37 similar instances.
Each instance consists oftime-series information about the student?s learn-ing routine and the selected templates that lec-turers used to provide feedback to this particu-lar student.
A template is a quadruple consist-ing of an id, a factor (bottom left of Ta-ble 1), a reference type (trend, week, aver-age, other) and surface text.
For instance,a template can be (1, marks, trend, ?Your markswere <trend>over the semester?).
The lexicalchoice for <trend>(i.e.
increasing or decreasing)depends on the values of time-series data.
Thereis a direct mapping between the values of factor139and reference type and the surface text.
The time-series factors are listed in Table 1.3.2 Actions and statesThe state consists of the time-series data and thenumber of factors which have so far been selectedto be talked about (the change of the value of thisvariable consequently introduces a state change).In order to explore the state space the agent se-lects a time-series factor (e.g.
marks, deadlinesetc.)
and then decides whether to talk about it ornot, until all factors have been considered.3.3 Reward functionThe reward function is the following cumulativemultivariate function:Reward = a+n?i=1bi ?
xi + c ?
lengthwhere X = {x1, x2, ..., xn} describes the cho-sen combinations of the factor trends observed inthe time-series data and a particular template (i.e.the way of mentioning a factor).
a, b and c are thecorrelation coefficients and length describes thenumber of factors selected to be conveyed in thefeedback summary.
The value of xi is given bythe function:xi =??
?1, the combination of a factor trendand a template type is included0, if not.The coefficients represent the level of preferencefor a factor to be selected and the way it is con-veyed in the summary.
In the training phase, theagent selects a factor and then decides whether totalk about it or not.
If the agent decides to referto a factor, the selection of the template is thenperformed in a deterministic way, i.e.
it selects thetemplate that results in higher reward.Each rated summary is transformed into a vec-tor of 91 binary features.
Each feature describesboth (1) the trend of a factor (e.g.
marks increas-ing, see also Table 1) and (2) the way that thisfactor could be conveyed in the summary (e.g.one possible way is referring to average, anotherpossible way is referring to increasing/decreasingtrend).
If both conditions are met, the value ofthe feature is 1, otherwise 0.
The 91 binary fea-tures describe all the different possible combina-tions.
For both the Lecturer-adapted and Student-adapted systems, the reward function is derivedfrom a linear regression analysis of the provideddataset, similarly to Walker et al.
(1997) andRieser et al.
(2010).3.3.1 Multi-adaptive Reward FunctionIn order to derive a reward function that finds abalance between the two above mentioned sys-tems, we use PCR to reduce the dimensionalityof the data and thus reduce the introduced noise.Through PCR we are able to reduce the numberof features and identify components of factors thatare deemed important to both parties to be used inthe reward function.PCR is a method that combines Principal Com-ponent Analysis (PCA) (Jolliffe, 1986) with lin-ear regression.
PCA is a technique for reducingthe dataset dimensionality while keeping as muchof the variance as possible.
In PCR, PCA is ini-tially performed to identify the principal compo-nents, in our case, the factors that contribute themost to the variance.
Then, regression is appliedto these principal components to obtain a vectorof estimated coefficients.
Finally, this vector istransformed back into the general linear regres-sion equation.
After performing this analysis onboth datasets (students and lecturers), we choosethe most important (i.e.
the ones that contributethe most to the variance) commoncomponents orfeatures resulting in 18 features which were usedin the reward function.
We then design a hand-crafted reward function taking into account thisPCR analysis.
The five most important featuresare shown in Table 2.factor trend way it is mentioned(1) marks stable average(2) hours studied decreasing trend(3) health issues decreasing weeks(4) lectures attended stable average(5) personal issues increasing trendTable 2: The top 5 features out of the 18 selectedthrough PCR analysis.4 EvaluationFeedbackGen is evaluated with real users againsttwo alternative systems: one that adapts to lectur-ers?
preferences and one that adapts to students?preferences.
The output of the three systems isranked by 30 computer science students from a va-riety of years of study.
Time-series data of threestudents are presented on graphs to each partici-pant, along with three feedback summaries (eachone generated by a different system), in randomorder, and they are asked to rank them in terms ofpreference.140Student-adapted {Ranking: 1st*} FeedbackGen {Ranking: 2nd*} Lecturer-adapted {Ranking: 3rd*}You did well at weeks 2, 3, 6, 8, 9 and 10,but not at weeks 4, 5 and 7.
Have a thinkabout how you were working well andtry to apply it to the other labs.
Your at-tendance was varying over the semester.Have a think about how to use time in lec-tures to improve your understanding ofthe material.
You found the lab exercisesnot very challenging.
You could try outsome more advanced material and exer-cises.
You dedicated more time study-ing the lecture material in the beginningof the semester compared to the end ofthe semester.
Have a think about whatis preventing you from studying.
Revis-ingmaterial during the semester will im-prove your performance in the lab.Your overall performance wasvery good during the semester.Keep up the good work and maybetry some more challenging exer-cises.
You found the lab exer-cises not very challenging.
Youcould try out some more advancedmaterial and exercises.
You dedi-cated more time studying the lec-ture material in the beginning ofthe semester compared to the endof the semester.
Have a think aboutwhat is preventing you from study-ing.
You have had other dead-lines during weeks 6 and 8.
Youmay want to plan your studying andwork ahead.Your overall performance was verygood during the semester.
Keep up thegood work and maybe try some morechallenging exercises.
You found thelab exercises not very challenging.
Youcould try out some more advanced mate-rial and exercises.
You dedicated moretime studying the lecture material in thebeginning of the semester compared tothe end of the semester.
Have a thinkabout what is preventing you from study-ing.
You have had other deadlines duringweeks 6 and 8.
You may want to planyour studying and work ahead.
You didnot face any health problems during thesemester.
You did not face any personalissues during the semester.Table 3: The table presents example outputs from the three different systems in order of highest ranked(bold signifies the chosen template content, * denotes significance with p <0.05 after comparing eachsystem with each other using Mann Whitney U test).5 ResultsTable 3 shows three summaries that have beengenerated by the different systems.
As we can seefrom Table 3, students significantly prefer the out-put of the system that is trained for their prefer-ences.
In contrast, students significantly dispre-fer the system that is trained for lecturers?
pref-erences.
Finally, they rank as second the systemthat captures the preferences of both lecturers andstudents, which shows that it might be feasible tofind middle ground between the preferences of twouser groups.
Significance testing is done usinga Mann Whitney U test (p <0.05), performing apair-wise comparison.6 DiscussionThe weights derived from the linear regressionanalysis vary from the Lecturer-adapted func-tion to the Student-adapted function.
For in-stance, the lecturers?
most preferred content ishours studied.
This, however, does not factorheavily into the student?s reward function, apartfrom the case where hours studied are de-creasing or remain stable (see also Table 2).Students like reading aboutpersonal issues when the number of issuesthey faced was increasing over the semester.
Onthe other hand, lecturers find it useful to giveadvice to all students who faced personal issuesduring the semester, hencepersonal issuesare included in the top 18 features (Table 2).Moreover, students seem to mostly prefer a feed-back summary that mentions the understandabilityof the material when it increases, which is positivefeedback.As reflected in Table 2, the analysis of PCRshowed that both groups found it useful to referto the average of marks when they remain stable.In addition, both groups found understandabilitywhen it increases useful, for a variety of reasons,for example lecturers might find it useful to en-courage students whereas students might prefer toreceive positive feedback.
Both groups also agreeon hours studied as described earlier.
On theother hand, both groups find mentioning the stu-dents?
difficulty when it decreases as positive.7 Future WorkIn the future, we plan to evaluate our methodol-ogy with lecturers and a larger sample of studentsacross different disciplines.
Moreover, we aim toport our methodology to a different domain, andtry to find the middle ground between the pref-erences of novices and expert users when sum-marising medical data while providing first aid.Finally, we want to compare the methodology pre-sented here to a multi-objective optimisation ap-proach (Fonseca and Flemming, 1993), where thepreferences of each user group will be modelled astwo different optimisation functions.AcknowledgementsThe research leading to this work has re-ceived funding from the EC?s FP7 programme:(FP7/2011-14) under grant agreement no.
248765(Help4Mood).141ReferencesCarole Ames.
1992.
Classrooms: Goals, structures,and student motivation.
Journal of Educational Psy-chology, 84(3):p261?71.Scotty D. Craig, Arthur C. Graesser, Jeremiah Sullins,and Barry Gholson.
2004.
Affect and learning: anexploratory look into the role of affect in learningwith autotutor.Carlos Fonseca and Peter Flemming.
1993.
Geneticalgorithms for multiobjective optimization: Formu-lation, discussion and generalization.
In 5th Inter-national Conference on Genetic Algorithms.Albert Gatt, Francois Portet, Ehud Reiter, JamesHunter, Saad Mahamood, Wendy Moncur, and So-mayajulu Sripada.
2009.
From data to text in theneonatal intensive care unit: Using NLG technologyfor decision support and information management.AI Communications, 22: 153-186.Dimitra Gkatzia, Helen Hastie, Srinivasan Ja-narthanam, and Oliver Lemon.
2013.
Generatingstudent feedback from time-series data using Rein-forcement Learning.
In 14th European Workshop inNatural Language Generation (ENLG).Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.2014a.
Comparing multi-label classification withreinforcement learning for summarisation of time-series data.
In 52nd Annual Meeting of the Associa-tion for Computational Linguistics (ACL).Dimitra Gkatzia, Helen Hastie, and Oliver Lemon.2014b.
Finding Middle Ground?
Multi-objectiveNatural Language Generation from Time-seriesdata.
In 14th Conference of the European Chap-ter of the Association for Computational Linguistics(EACL).Jim Hunter, Yvonne Freer, Albert Gatt, Yaji Sripada,Cindy Sykes, and D Westwater.
2011.
Bt-nurse:Computer generation of natural language shift sum-maries from complex heterogeneous medical data.American Medical Informatics Association, 18:621-624.Srinivasan Janarthanam and Oliver Lemon.
2010.Adaptive referring expression generation in spokendialogue systems: Evaluation with real users.
In11th Annual Meeting of the Special Interest Groupon Discourse and Dialogue (SIGDIAL).Ian T. Jolliffe.
1982.
A note on the use of principalcomponents in regression.
Journal of the Royal Sta-tistical Society, Series C: 31(3):300?303.Ian Jolliffe.
1986.
Principal Component Analysis.Springer-Verlag.Saad Mahamood and Ehud Reiter.
2011.
GeneratingAffective Natural Language for Parents of Neona-tal Infants.
In 13th European Workshop in NaturalLanguage Generation (ENLG).Natalie K. Person, Roger J. Kreuz, Rolf A. Zwaan, andArthur C. Graesser.
1995.
Pragmatics and peda-gogy: Conversational rules and politeness strategiesmay inhibit effective tutoring.
Journal of Cognitionand Instruction, 13(2):161-188.Ehud Reiter and Robert Dale.
2000.
Building natu-ral language generation systems.
Cambridge Uni-versity Press.Ehud Reiter, Roma Robertson, and Liesl Osman.
1999.Types of knowledge required to personalise smokingcessation letters.
Artificial Intelligence in Medicine:Proceedings of the Joint European Conference onArtificial Intelligence in Medicine and Medical De-cision Making.Verena Rieser, Oliver Lemon, and Xingkun Liu.
2010.Optimising information presentation for spoken dia-logue systems.
In 48th Annual Meeting of the Asso-ciation for Computational Linguistics (ACL).Richard Sutton and Andrew Barto.
1990.
Time deriva-tive models of pavlovian reinforcement.
Learningand Computational Neuroscience: Foundations ofAdaptive Networks, pages 497?537.Richart Sutton and Andrew Barto.
1998.
Reinforce-ment learning.
MIT Press.Cynthia A. Thompson, Mehmet H. Goker, and Pat Lan-gley.
2004.
A personalised system for conversa-tional recommendations.
Journal of Artificial Intel-ligence Research, 21(1).Marilyn Walker, Diane J Litman, Candace Kamm, andAlicia Abella.
1997.
PARADISE: A frameworkfor evaluating spoken dialogue agents.
In 8th con-ference on European chapter of the Association forComputational Linguistics (EACL).Ingrid Zukerman and Diane Litman.
2001.
Natu-ral language processing and user modeling: Syner-gies and limitations.
In User Modeling and User-Adapted Interaction, 11(1-2).142
