A MODEL OF PLAN INFERENCE THAT DISTINGUISHESBETWEEN THE BELIEFS OF ACTORS AND OBSERVERSMartha E. PollackArtificial Intell igence CenterandCenter for the Study of Language and InformationSRI International333 Ravenswood AvenueMenlo Park, CA 94025ABSTRACTExisting models of plan inference (PI) in conversation have as-sumed that the agent whose plan is being inferred (the actor)and the agent drawing the inference (the observer) have iden-tical beliefs about actions in the domain.
I argue that this as-sumption often results in failure of both the PI process and thecommunicative process that PI is meant to support.
In par-ticular, it precludes the principled generation of appropriateresponses to queries that arise from invalid plans.
I describea model of P1 that abandons this assumption.
It rests on ananalysis of plans as mental phenomena.
Judgements that aplan is invalid are associated with particular discrepancies be-tween the beliefs that the observer ascribes to the actor whenthe former believes that the latter has some plan, and the be-liefs that the observer herself holds.
I show that the contentof an appropriate response to a query is affected by the typesof any such discrepancies of belief judged to be present in theplan inferred to underlie that query.
The PI model describedhere has been implemented in SPIRIT, a small demonstrationsystem that answers questions about the domain of computermail.INTRODUCTIONThe importance of plan inference (PI) in models of conversa-tion has been widely noted in the computational-linguistics lit-erature.
Incorporating PI capabilities into systems that answerusers' questions has enabled such systems to handle indirectspeech acts \[13\], supply more information than is actually re-quested in a query \[2\], provide helpful information in responseto a yes/no query answered in the negative \[2\], disambiguaterequests \[17\], resolve certain forms of intersentential e lipsis\[6,11\], and handle such discourse phenomena s clarificationsubdialogues \[11\], and correction or "debugging ~ subdialoguesThe research reported in this paper has been made possible in part byan IBM Graduate Fellowship, in part by a gift from the Systems Develop-ment Foundation, and in part by support from the Defense Advanced Re-search Projects Agency under Contract N00039-84-K.0078 with the Spaceand Naval Warfare Command.
The views and conclusions contained inthis document are those of the author and should not be interpreted asrepresentative of the official policies, either expressed or implied, of theDefense Advanced Research Projects Agency or the United States Gov-ernment.!
am grateful to Barbara Grosz, James Allen, Phil Cohen, Amy Lansky,Candy Sidner and Bonnie Webber for their comments on an earlier draft.\[16,11\].The PI process in each of these systems, however, has as-sumed that the agent whose plan is being inferred (to whomI shall refer as the actor), and the agent drawing the infer-ence (to whom I shall refer as the observer), have identicalbeliefs about the actions in the domain.
Thus, Allen's model,which was one of the earliest accounts of PI in conversation 1and impired a great deal of the work done subsequently, in-cludes, as a typical PI rule, the following: "SBAW(P) ~iSBAW(ACT) if P is a precondition of ACT" \[2, page 120\].This rule can be glossed as "if the system (observer) believesthat an agent (actor) wants some proposition P to be true,then the system may draw the inference that the agent wantsto perform some action ACT of which P is a precondition.
"Note that it is left unstated precisely who it is--the observeror the actor---that believes that P is a precondition of ACT.If we take this to be a belief of the observer, it is not clearthat the latter will infer the actor's plan; on the other hand, ifwe consider it to he a belief of the actor, it is unclear how theobserver comes to have direct access to it.
In practice, thereis only a single set of operators relating preconditions and ac- /tion* in Allen's system; the belief in question is regarded asbeing both the actor's and the observer's.In many situations, an assumption that the re~v~nt beliefsof the actor are identical with those of the observer esultsin failure not only of the PI process, but also of:~he commu-nicative process that PI is meant to suppgrt.-In particular, itprecludes the principled generation of appropriate responsesto queries that arise from invalid plans.
In this paper, I reporton a model of Pl in conversation that distinguishes betweenthe beliefs of the actor and those of the observer.
The modelrests on an analysis of plans as mental phenomena: ~having aplan s is analyzed as having a particular configuration of k,c-lids and intentions.
Judgements that a plan is invalid areassociated with particular discrepancies between the beliefsthat the observer ascribes to the actor when the former be-lieves that the latter has some plan, and the beliefs observerherself holds.
I give an account of different ypes of plan in-validities, and show how this account provides an explanationfor certain regularities that are observable in cooperative re-sponses to questions.
The PI model described here has beenimplemented in SPIRIT, a small demonstration system thatanswers questions about the domain of computer mail.
More'Allen's article Izl summarizes his dissertation r .
.
.
.
.
ch Ill.207extensive discussion of both the PI model and SPIRIT can befound in my dissertation \[14\].PLANS AS MENTAL  PHENOMENAWe can distinguish between two views of plans.
As Bratman\[5, page 271\] has observed, there is an ambiguity in speakingof an agent's plan: "On the one hand, \[this\] could mean anappropriate abstract strncture--some sort of partial functionfrom circumstances to actions, perhaps.
On the other hand,\[it\] could mean an appropriate state of mind, one naturallydescribable in terms of such structures!
We might call theformer sense the data-structure view of plans, and the latterthe mental phenomenon view of plans.
Work in plan synthe-sis (e.g., Fikes and Nilsson \[8\], Sacerdoti \[15\], Wilkins \[18\],and Pednault \[12\]), has taken the data-structure view, con-sidering plans to be structures encoding aggregates of actionsthat, when performed in circumstances satisfying some speci-fied preconditions, achieve some specified results.
For the pur-poses of PI, however, it is much more useful to adopt a mentalphenomenon view and consider plans to be particular configu-rations of beliefs and intentions that some agent has.
After all,inferring another agent's plan means figuring out what actionshe "has in mind," and he may well be wrong about the effectsof those intended actions.Consider, for example, the plan I have to find out how Kathyis feeling.
Believing that Kathy is at the hospital, I plan to dothis by finding out the phone number of the hospital, callingthere, asking to be connected to Kathy's room, and finallysaying "How are you doing?"
If, unbeknownst to me, Kathyhas already been discharged, then executing my plan will notlead to my goal of finding out how she is feeling.
For me tohave a plan to do fl that consists of doing some collectionof actions I1, it is not necessary that the performance of IIactually lead to the performance of ft. What is necessary isthat I believe that its performance will do so.
This insight is atthe core of a view of plans as mental phenomena; in this viewa plan "exists"--i.e., gains its status as a plan--by virtue ofthe beliefs, as well as the intentions, of the person whose planit is.Further consideration of our common-sense conceptions ofwhat it means to have a plan leads to the following analysis\[14, Chap.
312:(PO) An agent G has a plan to do fl, that consists in doingsome set of acts II, provided that1.
G believes that he can execute ach act in I1.2.
G believes that executing the acts in I1 will entailthe performance of ft.3.
G believes that each act in I/plays a role in his plan.
(See discussion below.)4.
C intends to execute ach act in I1.5.
G intends to execute II as a way of doing B.2Although this definition ignores ome important issues of commitmentover time, as discussed by Bratman \[4\] and Cohen and Levesque \[71, it issufficient o support he PI process needed for many question-answeringsituations.
This is because, in such situations, unexpected changes inthe world that would force a reconsideration f the actor's intentions canusually be safely ignored.6.
G intends each act in II to play a role in his plan.The notion of an act playing a role in a plan is defined interms of two relationships over acts: generation, in the sensedefined by Goldman \[9\], and enablement.
Roughly, one actgenerates another if, by performing the first, the agent alsodoes the second; thus, saying to Kathy "How are you doing?
"may generate asking her how she is feeling.
Or, to take anexample from the computer-mail domain, typing DEL .
atthe prompt for a computer mail system may generate deletingthe current message, which may in turn generate cleaning outone's mail file.
In contrast, one act enables the generation of asecond by a third if the first brings about circumstances thatare necessary for the generation.
Thus, typing HEADER 15may enable the generation of deleting the fifteenth message bytyping DEL.
,  because it makes message 15 be the currentmessage, to which '. '
refers, s The difference between gener-ation and enablement consists largely in the fact that, whenan act a generates an act ~, the agent need only do a, andwill automatically be done also.
However, when a enables thegeneration of some "1 by fl, the agent needs to do somethingmore than just a to have done either fl or "t. In this paper,I consider only the inference of a restricted subset of plans,which I shall call simple plans.
An agent has a simple plan ifand only if he believes that all the acts in that plan play a rolein it by generating another act; i.e., if it includes no acts thathe believes are related to one another by enablement.It is important to distinguish between types of actions (act-types), such as typing DEL .
,  and actions themselves, uchas my typing DE/ .
.
right now.
Actions or acts--I will usethe two terms interchangeahly--can be thought of as triplesof act.type, agent, and time.
Generation is a relation overactions, not over act-types.
Not every case of an agent typingDEL ?
will result in the agent deleting the current message;for example, my typing it just now did not, because I was nottyping it to a computer mail system.
Similarly, executability--the relation expressed in Clause (1) of (P0) as "can execute"--applies to actions, and the objects of an agent's intentions are,in this model, also actions.Using the representation language specified in my thesis \[14\],which builds upon Allen's interval-based temporal logic \[3\], theconditions on G's having a simple plan to do fl can be encodedas follows:(P1) SIMPLE-PLAN(G ,a~,\[a~,..., a~-i  1,t2, tl )~(i) BEL(G,EXEC(ai,G,t2),tl), for i = 1 .
.
.
.
.
n A(ii) BEL(G,GEN(ai, cq+I,G,t2),tl), for i = 1 .... ,n-1 A(iii) INT(G,al, t2,tl), for i = 1 .
.
.
.
.
n A(iv) INT(G,by(ai, ai+l), t2,tl), for i = 1 .... ,n-1The left-hand side of (P1) denotes that the agent G has, attime tl, a simple plan to do an, consisting of doing the set ofacts {e l , .
.
.
,  an-l} at t2.
Note that all these are simultaneousacts; this is a consequence of the restriction to simple plans.The right-hand side of (P1) corresponds directly to (PO), ex-cept that, in keeping with the restriction to simple plans, spe-cific assertions about each act generating another eplace theSEnablement here thus differs from the usual binary relation in whichone action enables another.
Since this paper does not further considerplans with enabling actions, the advantages of the alternative definitionwill not be discussed.208more general statement regarding the fact that each act playsa role in the plan.
The relation BEL(G,P,t) should be takento mean that agent G believes proposition P throughout timeinterval t; INT(G,a, tz,tl) means that at time tl G intendsto do a at t2.
The relation EXEC(a,G,t) is true if and onlyif the act of G doing a at t is ezecutable, and the relationGEN(a,/ / ,G,t)  is true if and only if the act of G doing a att generates the act of G doing// at t. The function by mapstwo act-type terms into a third act-type term: if an agent Gintends to do by(a,//), then G intends to do the complex act//-by-a, i.e., he intends to do a in order to do// .
Further dis-cussion of these relations and functions can be found in Pollack\[14, Chap.
4\].Clause (i) of (P1) captures clause (1) of (P0).
4 Clause (iS) of(P1) captures both clauses (2) and (3) of (P0): when i takesthe value n-l, clause (iS) of (P1) captures the requirement,stated in clause (2) of (P01, that G believes his acts will entailhis goal; when i takes values between 1 and n-2, it capturesthe requirement of clause (3) of (P0), that G believes each ofhis acts plays a role in his plan.
Similarly, clause (iii) of (Pl)captures clause (4) of (P0), and clause (iv) of (P1) capturesclauses (5) and (6) of (PO).
(P1) can be used to state what it means for an actor to havean invalid simple plan: G has an invalid simple plan if andonly if he has the configuration of beliefs and intentions listedin (P1), where one or more of those beliefs is incorrect, and,consequently, one or more of the intentions i  unrealizable.
Thecorrectness of the actor's beliefs thus determines the validityof his plan: if all the beliefs that are part of his plan arecorrect, then all the intentions in it are realizable, and theplan is valid.
Validity in this absolute sense, however, is not ofprimary concern in modeling plan inference in conversation.What is important here is rather the observer's judgementof whether the actor's plan is valid.
It is to the analysis ofsuch invalidity judgements, and their effect on the question-answering process, that we now turn.PLAN INFERENCE INQUEST ION-ANSWERINGModels of the question-answering process often include a claimthat the respondent (R) must infer the plans of the questioner(Q).
So R is the observer, and Q the actor.
Building on theanalysis of plans as mental phenomena, we can say that, if Rbelieves that she has inferred Q's plan, there is some set of be-liefs and intentions atisfying (P1) that R believes Q has (or isat least likely to have).
Then there are particular discrepanciesthat may arise between the beliefs that R ascribes to Q whenshe believes he has some plan, and the beliefs that R herselfholds.
Specifically, R may not herself believe one or more ofthe beliefs, corresponding to Clauses (i) and (iS) of (P1), thatshe ascribes to Q.
We can associate such discrepancies with41n fact, it captures more: to encode Clause (i) of (P0), the pacameter1 in Clause (i) of (PI) need only vary between I and n-l.
However, giventhe relationship between EXEC and GEN specified in Pollack \[t4\], namelyEX EC(a, G, t) A GEN (a, ~, G, t) ~ EXEC(~,  G, t)the instance of Clause (i) of (P1) with i=n is a consequence of the instanceof Clause (i) with i=n-1 and the instance of Clause (iS) with i=n-l .
Asimilar argument can be made about Clause (iii).R's judgement that the plan she has inferred is invalid, s Thetype of any invalidities, defined in terms of the clauses of (PI)that contain the discrepant beliefs, can be shown to influencethe content of a cooperative response.
However, they do notfully determine it: the plan inferred to underlie a query, alongwith any invalidities it is judged to have, are but two factorsaffecting the response-generation process, the most significantothers being factors of relevance and salience.I will illustrate the effect of invalidity judgements on re-sponse content with a query of the form "I want to performan act of ~, so I need to find out how to perform an act of a,"in which the goal is explicit, as in example (1) below?
:(I) "I want to prevent Tom from reading my mail file.
Howcan I set the permissions on it to faculty-read only?
~In questions in which no goal is mentioned explicitly, analysisdepends upon inferring a plan leading to a goal that is rea-sonable in the domain situation.
Let us assume that, givenquery (1), R has inferred that Q has the simple plan that con-sists only in setting the permissions to faculty-read only, andthereby directly preventing Tom from reading the file, i.e.
:(2)BEL(R,SIMPLE-PLAN(Q, prevent (mmfile,read,tom),\[set-permissions(mmfile,read,faeult y)\],t2, t l ) ,tz)Later in this paper, I will describe the process by which R cancome to have this belief.
Bear in mind that, by (P1), (2) canbe expanded into a set of beliefs that R has about Q's beliefsand intentions.The first potential discrepancy is that R may believe to befalse some belief, corresponding to Clause (i) of (PI), that,by virtue of (2), she ascribes to Q.
In such a case, I will saythat she believes that some action in the inferred plan is un-e=~utable.
Examples of responses in which R conveys thisinformation are (3) (in which R believes that at least one in-tended act is unexecutable) and (4) (in which R believes thatat least two intended acts are unexeeutable):(3) "There ia no way for you to set the permissions on a tile tofaculty-read only.
What you can do is move it into a password-protected subdirectory; that will prevent Tom from readingit.
"(4) "There is no way far you to set the permissions on a fileto faculty.read only, nor is there any way for you to preventTom from reading it.
"SThle auumee that R always believes that her own beliefs are completeand correct.
Such an usumption is not an unreasonable one for question-answering systems to make.
More general conversational systems mustabandon this usumption,  sometimes updating their own beliefs upon de-tecting a discrepancy.eThe analysis below is related to that provided by 2oshi, Webber, andWeischedel \[10}.
There are significant differences in my approach, how-ever, which involve (i) a different structural analysis, which applies ane=-scala6111lll to agtions rather than plans and introduces incoherence (thislatter notion I dellne in the next section); (ii) a claim that the types ofinvtlldlties (e.g., formedness, executability of the queried action, and ex-ecutsbility of a goal action) are independent of one another; and (iii) aclaim that recognition of any invalidities, while necessary for determiningwhat information to include in an appropriate response, is not in itselfsufficient for this purpose.
Also, Joshi et el.
do not consider the questionof how invalid plans can be inferred.209The discrepancy resulting in (3) is represented in (5); the dis-crepancy in (4) is represented in (5) plus (6):(5) BEL(R,B EL(Q,EXEC(set-permissions(mmfile,read,facult y),Q,tz),tl),t~)ABEL(R,-,EXEC(set-permissions(mmfile,read,facult y),Q,t2),t~)(6) BEL(R,BEL(Q,EXEC(prevent(mmfile,read,tom),Q,t2),tl),ti)ABEL(R,--EXgC(prevent (ram file,read,tom),Q,t2),h)The second potential discrepancy is that R may believe falsesome belief corresponding to Clause (ii) of (P1) that, by virtueof (2), she ascribes to Q. I will then say that she believes theplan to be ill-formed.
In this ease, her response may con~'eythat the intended acts in the plan will not fit together as ex-pected, as in (7), which might be uttered if R believes it to bemutually believed by R and Q that Tom is the system man-ager:(7) "Well, the command is SET PROTECT ION ---- (Fac-u l ty:Read),  but that won't keep Tom out: file permissionsdon't apply to the system manager.
"The discrepancy resulting in (7) is (8):(8)BEL(R,BEL(Q,GEN(set-permissions(mmfile,read,facult y),prevent (ram file,read,tom),Q,t2),tl),h)ABEL(R,-~G EN (set-permissions(mmfile,read,facult y),prevent (mmfile,read,tom),Q,t2),h)Alternatively, there may be some combination of these dis-crepancies between R's own beliefs and those that R attributesto Q, as reflected in a response such as (9):(9) "There is no way for you to set the permissions to faculty-read only; and even if you could, it wouldn't keep Tom out:tile permissions don't apply to the system manager.
"The discrepancies ncoded in (5) and (8) together might resultin (9).Of course, it is also possible that no discrepancy exists atall, in which ease I will say that R believes that Q's plan isvalid.
A response such as (10) can be modeled as arising froman inferred plan that R believes valid:(10) "Type SET PROTECT ION = (Facul ty :Read) .
"Of the eight possible combinations of formedness, exe-curability of the queried act and executability of the goal act,seven are possible: the only logically incompatible combina-tion is a well-formed plan with an executable queried act, butunexecutable goal act.
This range of invalidities accounts for agreat deal of the information conveyed in naturally occurringdialogues.
But there is an important regularity that the PImodel does not yet explain.A PROBLEM FOR PLANINFERENCEIn all of the preceding cases, R has intuitively "made sense" ofQ's query, by determining some underlying plan whose com-ponents he understands, though she may also believe that theplan is flawed.
For instance in (7), R has determined that Qmay mistakenly believe that, when one sets the permissions ona file to allow a particular access to a particular group, no onewho is not a member of that group can gain access to the file.This (incorrect) belief explains why Q believes that setting thepermissions will prevent Tom from reading the file.There are also cases in which R may not even be able to"make sense" of Q's query.
As a somewhat whimsical example,imagine Q saying:(11) ~I want to talk to Kathy, so I need to Fred out how tos tand  on my head.
~In many contexts, aperfectly reasonable response to this queryis ~Huh?
~.
Q's query is incoherent: R cannot understand whyQ believes that finding out how to stand on his head (or stand-ing on his head) will lead to talking with Kathy.
One can, ofcourse, construct scenarios in which Q's query makes perfectsense: Kathy might, for example, be currently hanging by herfeet in gravity boots.
The point here is not to imagine suchcircumstances in which Q's query would be coherent, but in-stead to realize that there are many circumstances in which itwould not.The judgement that a query is incoherent is not the same asa judgement that the plan inferred to underlie it is ill-formed.To see this, contrast example (11) with the following:(12) al want to talk to Kathy.
Do you know the phone numberat the hospital?
"Here, if R believes that Kathy has already been dischargedfrom the hospital, she may judge the plan she infers to underlieQ's query to be ill-formed, and may inform him that callingthe hospital will not lead to talking to Kathy.
She can eveninform him why the plan is ill-formed, namely, because Kathyis no longer at the hospital.
This differs from (11), in which Rcannot inform Q of the reason his plan is invalid, because shecannot, on an intuitive level, even determine what his plan is.Unfortunately, the model as developed so far does not dis-tinguish between incoherence and ill-formedness.
The reasonis that, given a reasonable account of semantic interpretation,it is transparent from the query in (11) that Q intends totalk to Kathy, intends to find out how to stand on his head,and intends his doing the latter to play a role in his plan todo the former and that he also believes that he can talk toKathy, believes that he can find out how to stand on his head,and believes that his doing the latter will play a role in his210plan to do the former.
~ But these beliefs and intentions areprecisely what are required to have a plan according to (P0).Consequently, after hearing (11), R can, in fact, infer a planunderlying Q's query, namely the obvious one: to find out howto stand on his head (or to stand on his head) in order to talkto Kathy.
Then, since R does not herself believe that the for-mer act will lead to the latter, on the analysis so far given, wewould regard R as judging Q's plan to be ill-formed.
But thisis not the desired analysis: the model should instead capturethe fact that R cannot make sense of Q's query here--that itis incoherent.Let us return to the set of examples about setting the per-missions on a file, discussed in the previous ection.
In her se-mantic interpretation of the query in (1), R may come to havea number of beliefs about Q's beliefs and intentions.
Specifi-cally, all of the following may be tr~e:(13) BEL(R,BgL(Q,gXEC(set-permissions(mmfile,read,facult y),q,tz),tl),t~)(14) BEL(R,BEL(Q,gXEC(prevent(mmfile,read,tom),Q,t2),t l ) ,t~)(15) BEL(R,BEL(Q,G EN(set-permissions(mmfile,read,facult y),prevent (mmfile,read,tom),Q,tz),tl),t~)(16) BEL( R,I NT(Q,set-permissions(mm file,read,facult y),t2,~l),tt)(17) BEL(R,INT(Q,prevent (mmfile,read,tom),t2,t l ) ,t~)(18) BEL(R,I iT(Q,by(set-permissions(mmfile,read,facult y),prevent (mmfile,read,tom)),t2 , t l ) ,tl)Together, (13)-(18) are sumcient for R's believing that Q hasthe simple plan as expressed in (2).
This much is not surpris-ing.
In effect Q has stated in his query what his plan is--toprevent Tom from reading the file by setting the permission onit to faculty-read only--so, of course, R should be able to inferjust that.
And if R further believes that the system managercan override file permissions and that Tom is the system man-ager, but also that Q does not know the former fact, R willjudge that Q's plan is ill-formed, and may provide a responsesuch as that in (7).
There is a discrepancy here between thebelief R ascribes to Q in satisfaction of Clause (ii) of (P l ) - -namely, that expressed in (15)--and R's own beliefs about thedomain.But what if R, instead of believing that it is mutually be-lieved by Q and R that Tom is the system manager, believesthat they mutually believe that he is a faculty member?
Inthis case, (13)-(18) may still be true.
However we do not wantto say that this case is indistinguishable from the previous one.7Actually, the requirement that Q have these beliefs may be slightlytoo strong; see Pollack \[14, Chap.
3\] for discussion.In the previous case, R understood the source of Q's erroneousbelief: she realized that Q did not know that the system man-ager could override file protections, and therefore thought hat,by setting permissions to restrict access to a group that Tom isnot a member of, he could prevent Tom from reading the file.In contrast, in the current ease, R cannot really understandQ's plan: she cannot determine why Q believes that he willprevent Tom from reading the file by setting the permissionson it to faculty-read only, given that Q believes that Tom is afaculty member.
This current case is like the case in (11): Q'squery is incoherent to R.To capture the difference between iil-formedness and inco-herence, I will claim that, when an agent R is asked a questionby an actor Q, R needs to attempt o ascribe to Q more thanjust a set of beliefs and intentions atisfying (Pl).
Specifi-cally, for each belief satisfying Clause (ii) of (Pl), R must alsoascribe to Q another belief that explains the former in a cer-tain specifiable way.
The beliefs that satisfy Clause (ii) arebeliefs about the relation between two particular actions: forinstance, the plan underlying query (12) includes Q's beliefthat his action of calling the hospital at tz will generate hisaction of establishing a communication channel to Kathy att2.
This belief can be explained by a belief Q has about therelation between the act-types ~calling a location" and ~estab-lishing a communication channel to an agent."
Q may believethat sets of the former type generate acts of the latter typeprovided that the agent to whom the communication channelis to be established is at the location to be called.
Such a beliefcan be encoded using the predicate CGEN, which can be read"conditionally generates," as follows:(19)BEL(Q, CGEN(call(X),establish-channel(Y),at(X,Y)), tlThe relation CGEN(a, B, C) is true if and only if acts of type aperformed when condition C holds will generate acts of type #.Thus, the sentence CGEN(a, B, C) can be seen as one possibleinterpretation of a hieran=hical p anning operator with headerB, preconditions C, and body a.
Conditional generation is arelation between two act-types and a set of conditions; gener-ation, which is a relation between two actions, can be definedin terms of conditional generation.In reasoning about (12), R can attribute to Q the belief ex-pressed in (19), combined with a belief that Kathy will be atthe hospital at time t2.
Together, these beliefs explain Q's be-lief that, by calling the hospital at t2, he will establish a com-mtmieation channel to Kathy.
Similarly, in reasoning aboutquery (1) in the case in which R does not believe that Q knowsthat Tom is a faculty member, R can ascribe to Q the beliefsthat, by setting the permissions on a file to restrict access to apartieulac group, one denies access to everyone who is neithers member of that group nor the system manager, as expressedin (20):(20)BEL(R,BEL(Q,CGEN(set-permissions(X,P,Y),prevent(X,P,Z),-,member(g,Y)),h),tt)She can also ascribe to Q the belief that Tom is not a mem-ber of the faculty, (or more precisely, that Tom will not be amember of the faculty at the intended performance time tz),i.e.,211?
I(21)BEL(R,BEL(Q, HOLDS(-~member(tom,facuity),t2),tl),tl}The conjunction of these two beliefs explains Q's further belief,expressed in (15), that, by setting the permissions to faculty-read only at t2, he can prevent Tom from reading the file.In contrast, in example (11), R has no basis for ascribing toQ beliefs that will explain why he thinks that standing on hishead will lead to talking with Kathy.
And, in the version ofexample (1) in which R believes that Q believes that Tom is afaculty member, R has no basis for ascribing to Q a belief thatexplains Q's belief that setting the permissions to faculty-readonly will prevent Tom from reading the file.Explanatory beliefs are incorporated in the PI model by theintroduction of ezplanatory plans, or eplans.
Saying that anagent R believes that another agent Q has some eplan is short-hand for describing a set of beliefs possessed by R, specifically:(P2) (R,EPLAN(Q,~n,\[al .
.
.
.
.
an-l\],\[pl .
.
.
.
.
Pn-l\],t2, t l ) , t l  )(i) BEL(R,BEL(Q,EXEC(cq,Q,t2),tl),tl),for i = 1,...,n A(ii) BEL(R,BEL(Q,G EN(~, ai+t,Q,t2),tt),tl ),for i = 1,...,n-I A(iii) BEL(R,INT(Q,~I, tz, tl),tl),for i = 1,... ,  n A(iv) BEL(R,INT(Q,by~al, ai+l), t2, tl),tl),for i = 1,... ,n-1 A(v) BEL(R,BEL(Q,pi, tl),tl),where each Pi isCGEN(ai, cq+l, Ci) A HOLDS(Ci, t2)I claim that the PI process underlying cooperative question-answering can be modeled as an attempt o infer an eplan,i.e., to form a set of beliefs about the questioner's beliefs andintentions that satisfies (P2).
Thus the next question to askis: how can R come to have such a set of beliefs?THE INFERENCE PROCESSIn the complete PI model, the inference of an eplan is a two-stage process.
First, R infers beliefs and intentions that Qplausibly has.
Then when she has found some set of themethat is large enough to account for Q's query, their epistemiestatus can be upgraded, from beliefs and intentions that R be-lieves Q plausibly has, to beliefs and intentions that R will, forthe purposes of forming her response, consider Q actually tohave.
Within this paper, however, I will blur the distinctionbetween attitudes that R believes Q plausibly has and atti-tudes that R believes Q indeed has; in consequence I will alsoomit discussion of the second stage of the PI process.A set of plan inference rules encodes the principles by whichan inferring agent R can reason from some set of beliefs andintentions--call this the antecedent eplan--that she thinks Qhas, to some further set of beliefs and intentions--call this theconsequent eplan--that she also thinks he has.
The beliefs andintentions that the antecedent eplan comprises are a propersubset of those that the consequent eplan comprises.
To reasonfrom antecedent eplan to consequent eplan, R must attributesome explanatory belief to Q on the basis of something otherthan just Q's query.
In more detail, if part of R's belief thatQ has the antecedent eplan is a belief that Q intends to dosome act a, and R has reason to believe that Q believes thatact-type a conditionally generates act-type 3' under conditionC, then R can infer that Q intends to do a in order to do %believing as well that C will hold at performance time.
R canalso reason in the other direction: if part of her belief that Qhas some plausible eplan is a belief that Q intends to do someact a and R has reason to believe that Q believes that act-typeconditionally generates act-type a under condition C, thenR can infer that Q intends to do "~ in order to do a, believingthat C will hold st performance time.The plan inference rules encode the pattern of reasoning ex-pressed in the last two sentences.
Different plan inference rulesencode the different bases upon which R may decide that Qmay believe that a conditional generation relation holds be-tween some a, an act of which is intended as part of the an-tecedent eplan, and some % This ascription of beliefs, as wellas the ascription of intentions, is a nonmonotonic process.
Forarbitrary proposition P, R will only decide that Q may believethat P if R has no reason to believe Q believes that -~P.In the most straightforward case, R will ascribe to Q a be-lief about s conditional generation relation that she herselfbelieves true.
This reasoning can be encoded in the represen-tation language in rule (PI1):(P I I )  BEL(R,EPLAN(Q,an,\[al .
.
.
.
.
an-a\],\[pl .
.
.
.
.
On-t\],t2,h),h)ABEL(R,CGEN(an, % C),q)BEL( R,EPLAN(Q,%\[al .
.
.
.
.
a,\],\[pl .
.
.
.
.
p, \],t2, tl ),tl )where p, ~.
CGEN(ar,,"I, C) ^  HOLDS(C, t2)This rule says that, if R's belief that Q has some eplan includesa belief that Q intends to do an act an, and R also believes thatact-type a~ conditionally generates some "~ under condition C,then R can (nonmonotonically) infer that Q has the additionalintention of doing a ,  in order to do ~--i.e., that he intends todo by(an, "~).
Q's having this intention depends upon his alsohaving the supporting belief that a n conditionally generates ~'under some condition C, and the further belief that this C willhold at performance time.
A rule symmetric to (PI1) is alsoneeded since R can not only reason about what acts might begenerated by an act that she already believes Q intends, butalso about what acts might generate such an act.Consider R's use of (PI1) in attempting to infer the planunderlying query (1)) R herself has a particular belief aboutthe relation between the act-types "setting the permissions on?
file" and "preventing someone access to the file," a belief wecan encode as follows:(22) BEL{ R,CG EN (met-permissions(X,P,Y),prevent(X,P,Z),-~member(Z,Y) A--system-mgr(Z)),q)From query (1}, R can directly attribute to Q two trivialeplans:sI have simplified somewhat in the following account for presentationalpurposes.
A step-by-step account of this inference process is given inPoll~ck \[14, Chap.
6\].212( 23 ) B E L( R, E P b A N ( Q,set-p ermissions( mmfile,read,facult y ),\[ \],t2, t,),tl)(24)BEL(R,EPLAN(Q,prevent(mmfile,read,tom),\[ \],t2  tl ),tl)The belief in (23) is justified by the fact that (13) satisfiesClause (i) of (P2), (16) satisfies Clause (iv) of (P2), andClauses (ii), (iii), and (v) are vacuously satisfied.
An anal-ogous argument applies to (24).Now, if R applies (PII), she will attribute to Q exactly thesame belief as she herself has, as expressed in (22), along witha belief that the condition C specified there will hold at t2.That is, as part of her belief that a particular eplan underlies(1), R will have the following belief:(25) BEL(R,BEL(Q,CG EN(set-permissions(X,P,Y),prevent(X,P,Z),-,member(Z,Y) A -~system-mrg(Z))AHOLDS(-,member(tom,faeulty)A --system-mgr(tom), tz),tl),q)The belief that R attributes to Q, as expressed in (25), isan explanatory belief supporting (15).
Note that it is not thesame explanatory belief that was expressed in (20) and (21).
In(25), the discrepancy between R's beliefs and R's beliefs aboutQ's beliefs is about whether Tom is the system manager.
Thisdiscrepancy may result in a response like (26), which conveysdifferent information than does (7} about the source of thejudged ill-formedness.
(26) "Well, the command is SET PROTECT ION = (Fac-ulty:Read),  but that won't keep Tom out: he's the systemmanager.
"(PI1) (and its symmetric partner) are not sufficient o modelthe inference of the eplan that results in (7).
This is because, inusing (PI1), R is restricted to ascribing to Q the same beliefsabout the relation between domain act-types as she herselfhas.
~ The eplan that results in (7) includes a belief that Rattributes to Q involving a relation between act-types that Rbelieves false, specifically, the CGEN relation in (20).
Whatis needed to derive this is a rule such as (PI2):(PI2) BEL(R,EPLAN(Q,on,\[al .
.
.
.
.
an-l\],\[pl .
.
.
.
.
Pn-l\],t2, t,),q )ABEL(R,CGEN(an, 7, C~ A. .
.
A Cm),tl)--4BEL(R,EPLAN(Q,7,\[al, .
.
.
,  a \],\[pl .
.
.
.
.
p,\],tz, q ),q )where p, = CGEN(an, % CIA.. .ACi- IACi+IA.. .ACm)AHOLDS(C,  A .
.
.
A Ci-1 A Ci+l h .
.
.A  Cm,t2)~Hence, existing PI systems that equate R's and Q's beliefs aboutactions could, in principle, have handled examples uch as (26) whichr,: ~:Sre only the use of (PI1), although they have not done so.
Further,whi\]~ they could have handled the particular type of invalidity that can beinferred using (PII), without an analysis of the general problem of invalidplans and their effects on cooperative responses, these systems would needto treat this as a special case in which a variant response is required.What (PI2) expresses i that R may ascribe to Q a belief abouta relation between act-types that is a slight variation of oneshe herself has.
What (PI2) asserts is that, if there is someCGEN relation that R believes true, she may attribute to Qa belief in a similar CGEN relation that is stronger, in that itis missing one of the required conditions.
If R uses (PI2) inattempting to infer the plan that underlies query (1), she maydecide that Q's belief about the conditions under which settingthe permissions on a file prevents omeone from accessing thefile do not include the person's not being the system manager.This can result in R attributing to Q the explanatory belief in(20) and (21), which, in turn, may result in a response such asthat in (7).Of course, both the kind of discrepancy that may be in-troduced by (PI1) and the kind that is always introduced by(PI2) may be present simultaneously, resulting in a responselike (27):(27) "Well, the command is SET PROTECT ION = (Fac-ulty:Read),  but that won't keep Tom out: he's the systemmanager, and file permissions don't apply to the system man-ager.
"(PI2) represents just one kind of variation of her own beliefsthat R may consider attributing to Q.
Additional PI rulesencode other variations and can also be used to encode anytypical misconceptions that R may attribute to Q.IMPLEMENTATIONThe inference process described in this paper has been imple-mented in SPIRIT, a System for Plan Inference that Reasonsabout Invalidities Too.
SPIRIT infers and evaluates the plansunderlying questions asked by users about the domain of com-puter mail.
It also uses the result of its inference and eval-uation to generate simulated cooperative responses.
SPIRITis implemented in C-Prolog, and has run on several differ-ent machines, ineludinga Sun Workstation, a Vax 11-750,and a DEC-20.
SPIRIT is a demonstration system, imple-mented to demonstrate the PI model developed in this work;consequently only a few key examples, which are sufficient odemonstrate SPIRIT's capabilities, have been implemented.Of course, SPIRIT's knowledge base could be expanded in astraightforward manner.
SPIRIT has no mechanisms for com-puting relevance or salience and, consequently, always pro-duces as complete an answer as possible.CONCLUSIONIn this paper I demonstrated that modeling cooperative con-versation, in particular cooperative question-answcring, re-quires a model of plan inference that distinguishes betweenthe beliefs of actors and those of observers.
I reported on sucha model, which rests on an analysis of plans as mental phenom-ena.
Under this analysis there can be discrepancies between anagent's own beliefs and the beliefs that she ascribes to an actorwhen she thinks he has some plan.
Such discrepancies were as-sociated with the observer's judgement that the actor's plan isinvalid.
Then the types of any invalidities judged to be presentin a plan inferred to underlie a query were shown to affect thecontent of a cooperative response.
1 further suggested that, to213guarantee a cooperative r sponse, the observer must attemptto ascribe to the questioner more than just a set of beliefs andintentions sufficient to believe that he has some plan: she mustalso attempt to ascribe to him beliefs that explain those beliefsand intentions.
The eplan construct was introduced to capturethis requirement.
Finally, I described the process of inferringeplans--that is, of ascribing to another agent beliefs and in-tentions that explain his query and can influence a responseto it.REFERENCES\[1\] James F. Allen.
A Plan Based Approach to Speech ActRecognition.
Technical Report TR 121/79, University ofToronto, 1979.\[2\] James F. Allen.
Recognizing intentions from naturallanguage utterances.
In Michael Brady and Robert C.Berwlck, editors, Computational Models of Discourse,pages 107-166, MIT Press, Cambridge, Mass., 1983.\[3\] James F. Allen.
Towards a general theory of action andtime.
Artificial Intelligence, 23(2):123-154, 1984.\[4\] Michael Bratman.
Intention, Plans and Practical Reason.Harvard University Press, Cambridge, Ma., forthcoming.\[5\] Michael Bratman.
Taking plans seriously.
Social Theoryand Practice, 9:271-287, 1983.\[6\] M. Sandra Carberry.
Pragmatic Modeling in InformationSystem Interfaces.
PhD thesis, University of Delaware,1985.\[7\] Philip R. Cohen and Hector J. Levesque.
Speech acts andrationality.
In Proceedings of the e3rd Conference of theAssociation for Computational Linguistics, pages 49-59,Stanford, Ca., 1985.\[8\] R. E. Fikes and Nils J. Nilsson.
Strips: a new approachto the application of theorem proving to problem solving.Artificial Intelligence, 2:189-208, 1971.\[9\] Alvin I. Goldman.
A Theory of Human Action.
Prentice-Hail, Englewood Cliffs, N.J., 1970.\[10\] Aravind K. Joshi, Bonnie Webber, and Ralph Weischedel.Living up to expectations: computing expert responses.In Proceedings of the Fourth National Conference on Ar-tificial Intelligence, pages 169-175, Austin, Tx., 1984.\[11\] Diane Litman.
Plan Recognition and Discourse Analy-sis: An Integrated Approach for Understanding Dialogues.PhD thesis, University of Rochester, 1985.\[12\] Edwin P.D.
Pednault.
Preliminary Report on a Theory ofPlan Synthesis.
Technical Report 358, SRI International,1985.\[13\] C. Raymond Perranlt and James F. Allen.
A plan-basedanalysis of indirect speech acts.
American Journal ofComputational Linguistics, 6:167-182, 1980.\[14\] Martha E. Pollack.
Inferring Domain Plans in @~estion-Answering.
PhD thesis, University of Pennsylvania, 1986.\[15\] Earl D. Sacerdoti.
A Structure for Plans and Behavior.American Elsevier, New York, 1977.\[16\] Candaee L. Sidner.
Plan parsing for intended responserecognition in discourse.
Computational Intelligence,I(I), 1985.\[17\] Candace L. Sidner.
What the speaker means: the recogni-tion of speakers' plans in discourse.
International Journalof Computers and Mathematics, 9:71-82, 1983.\[18\] David E. Wilkins.
Domain-independent planning: rep-resentation and plan generation.
Artificial Intelligence,22:269--301, 984.214
