Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 1048?1056,Suntec, Singapore, 2-7 August 2009. c?2009 ACL and AFNLPWord or Phrase?Learning Which Unit to Stress for Information Retrieval?Young-In Song?
and Jung-Tae Lee?
and Hae-Chang Rim?
?Microsoft Research Asia, Beijing, China?Dept.
of Computer & Radio Communications Engineering, Korea University, Seoul, Koreayosong@microsoft.com?, {jtlee,rim}@nlp.korea.ac.kr?AbstractThe use of phrases in retrieval models hasbeen proven to be helpful in the literature,but no particular research addresses theproblem of discriminating phrases that arelikely to degrade the retrieval performancefrom the ones that do not.
In this paper, wepresent a retrieval framework that utilizesboth words and phrases flexibly, followedby a general learning-to-rank method forlearning the potential contribution of aphrase in retrieval.
We also present use-ful features that reflect the compositional-ity and discriminative power of a phraseand its constituent words for optimizingthe weights of phrase use in phrase-basedretrieval models.
Experimental results onthe TREC collections show that our pro-posed method is effective.1 IntroductionVarious researches have improved the qualityof information retrieval by relaxing the tradi-tional ?bag-of-words?
assumption with the use ofphrases.
(Miller et al, 1999; Song and Croft,1999) explore the use n-grams in retrieval mod-els.
(Fagan, 1987; Gao et al, 2004; Met-zler and Croft, 2005; Tao and Zhai, 2007) usestatistically-captured term dependencies within aquery.
(Strzalkowski et al, 1994; Kraaij andPohlmann, 1998; Arampatzis et al, 2000) studythe utility of various kinds of syntactic phrases.Although use of phrases clearly helps, there stillexists a fundamental but unsolved question: Do allphrases contribute an equal amount of increase inthe performance of information retrieval models?Let us consider a search query ?World Bank Crit-icism?, which has the following phrases: ?world?This work was done while Young-In Song was with theDept.
of Computer & Radio Communications Engineering,Korea University.bank?
and ?bank criticism?.
Intuitively, the for-mer should be given more importance than its con-stituents ?world?
and ?bank?, since the meaningof the original phrase cannot be predicted fromthe meaning of either constituent.
In contrast, arelatively less attention could be paid to the lat-ter ?bank criticism?, because there may be alter-nate expressions, of which the meaning is still pre-served, that could possibly occur in relevant docu-ments.
However, virtually all the researches ig-nore the relation between a phrase and its con-stituent words when combining both words andphrases in a retrieval model.Our approach to phrase-based retrieval is moti-vated from the following linguistic intuitions: a)phrases have relatively different degrees of signif-icance, and b) the influence of a phrase should bedifferentiated based on the phrase?s constituents inretrieval models.
In this paper, we start out bypresenting a simple language modeling-based re-trieval model that utilizes both words and phrasesin ranking with use of parameters that differenti-ate the relative contributions of phrases and words.Moreover, we propose a general learning-to-rankbased framework to optimize the parameters ofphrases against their constituent words for re-trieval models that utilize both words and phrases.In order to estimate such parameters, we adapt theuse of a cost function together with a gradient de-scent method that has been proven to be effectivefor optimizing information retrieval models withmultiple parameters (Taylor et al, 2006; Metzler,2007).
We also propose a number of potentiallyuseful features that reflect not only the characteris-tics of a phrase but also the information of its con-stituent words for minimizing the cost function.Our experimental results demonstrate that 1) dif-ferentiating the weights of each phrase over wordsyields statistically significant improvement in re-trieval performance, 2) the gradient descent-basedparameter optimization is reasonably appropriate1048to our task, and 3) the proposed features can dis-tinguish good phrases that make contributions tothe retrieval performance.The rest of this paper is organized as follows.The next section discusses previous work.
Section3 presents our learning-based retrieval frameworkand features.
Section 4 reports the evaluations ofour techniques.
Section 5 finally concludes the pa-per and discusses future work.2 Previous WorkTo date, there have been numerous researches toutilize phrases in retrieval models.
One of themost earliest work on phrase-based retrieval wasdone by (Fagan, 1987).
In (Fagan, 1987), the ef-fectiveness of proximity-based phrases (i.e.
wordsoccurring within a certain distance) in retrievalwas investigated with varying criteria to extractphrases from text.
Subsequently, various typesof phrases, such as sequential n-grams (Mitra etal., 1997), head-modifier pairs extracted from syn-tactic structures (Lewis and Croft, 1990; Zhai,1997; Dillon and Gray, 1983; Strzalkowski et al,1994), proximity-based phrases (Turpin and Mof-fat, 1999), were examined with conventional re-trieval models (e.g.
vector space model).
The ben-efit of using phrases for improving the retrievalperformance over simple ?bag-of-words?
modelswas far less than expected; the overall perfor-mance improvement was only marginal and some-times even inconsistent, specifically when a rea-sonably good weighting scheme was used (Mitraet al, 1997).
Many researchers argued that thiswas due to the use of improper retrieval modelsin the experiments.
In many cases, the early re-searches on phrase-based retrieval have only fo-cused on extracting phrases, not concerning abouthow to devise a retrieval model that effectivelyconsiders both words and phrases in ranking.
Forexample, the direct use of traditional vector spacemodel combining a phrase weight and a wordweight virtually yields the result assuming inde-pendence between a phrase and its constituentwords (Srikanth and Srihari, 2003).In order to complement the weakness, a numberof research efforts were devoted to the modelingof dependencies between words directly within re-trieval models instead of using phrases over theyears (van Rijsbergen, 1977; Wong et al, 1985;Croft et al, 1991; Losee, 1994).
Most stud-ies were conducted on the probabilistic retrievalframework, such as the BIM model, and aimed onproducing a better retrieval model by relaxing theword independence assumption based on the co-occurrence information of words in text.
Althoughthose approaches theoretically explain the relationbetween words and phrases in the retrieval con-text, they also showed little or no improvementsin retrieval effectiveness, mainly because of theirstatistical nature.
While a phrase-based approachselectively incorporated potentially-useful relationbetween words, the probabilistic approaches forceto estimate parameters for all possible combina-tions of words in text.
This not only bringsparameter estimation problems but causes a re-trieval system to fail by considering semantically-meaningless dependency of words in matching.Recently, a number of retrieval approaches havebeen attempted to utilize a phrase in retrieval mod-els.
These approaches have focused to model sta-tistical or syntactic phrasal relations under the lan-guage modeling method for information retrieval.
(Srikanth and Srihari, 2003; Maisonnasse et al,2005) examined the effectiveness of syntactic re-lations in a query by using language modelingframework.
(Song and Croft, 1999; Miller et al,1999; Gao et al, 2004; Metzler and Croft, 2005)investigated the effectiveness of language model-ing approach in modeling statistical phrases suchas n-grams or proximity-based phrases.
Some ofthem showed promising results in their experi-ments by taking advantages of phrases soundly ina retrieval model.Although such approaches have made clear dis-tinctions by integrating phrases and their con-stituents effectively in retrieval models, they didnot concern the different contributions of phrasesover their constituents in retrieval performances.Usually a phrase score (or probability) is simplycombined with scores of its constituent words byusing a uniform interpolation parameter, whichimplies that a uniform contribution of phrasesover constituent words is assumed.
Our study isclearly distinguished from previous phrase-basedapproaches; we differentiate the influence of eachphrase according to its constituent words, insteadof allowing equal influence for all phrases.3 Proposed MethodIn this section, we present a phrase-based retrievalframework that utilizes both words and phrases ef-fectively in ranking.10493.1 Basic Phrase-based Retrieval ModelWe start out by presenting a simple phrase-basedlanguage modeling retrieval model that assumesuniform contribution of words and phrases.
For-mally, the model ranks a document D according tothe probability of D generating phrases in a givenquery Q, assuming that the phrases occur indepen-dently:s(Q;D) = P (Q|D) ?|Q|?i=1P (qi|qhi , D) (1)where qi is the ith query word, qhi is the head wordof qi, and |Q| is the query size.
To simplify themathematical derivations, we modify Eq.
1 usinglogarithm as follows:s(Q;D) ?|Q|?i=1log[P (qi|qhi , D)] (2)In practice, the phrase probability is mixed withthe word probability (i.e.
deleted interpolation) as:P (qi|qhi ,D)?
?P (qi|qhi ,D)+(1??
)P (qi|D) (3)where ?
is a parameter that controls the impact ofthe phrase probability against the word probabilityin the retrieval model.3.2 Adding Multiple ParametersGiven a phrase-based retrieval model that uti-lizes both words and phrases, one would definitelyraise a fundamental question on how much weightshould be given to the phrase information com-pared to the word information.
In this paper, wepropose to differentiate the value of ?
in Eq.
3according to the importance of each phrase byadding multiple free parameters to the retrievalmodel.
Specifically, we replace ?
with well-known logistic function, which allows both nu-merical and categorical variables as input, whereasthe output is bounded to values between 0 and 1.Formally, the input of a logistic function is aset of evidences (i.e.
feature vector) X generatedfrom a given phrase and its constituents, whereasthe output is the probability predicted by fitting Xto a logistic curve.
Therefore, ?
is replaced as fol-lows:?
(X) = 11 + e?f(X) ?
?
(4)where ?
is a scaling factor to confine the output tovalues between 0 and ?.f(X) = ?0 +|X|?i=1?ixi (5)where xi is the ith feature, ?i is the coefficient pa-rameter of xi, and ?0 is the ?intercept?, which isthe value of f(X) when all feature values are zero.3.3 RankNet-based Parameter OptimizationThe ?
parameters in Eq.
5 are the ones we wishto learn for resulting retrieval performance via pa-rameter optimization methods.
In many cases, pa-rameters in a retrieval model are empirically de-termined through a series of experiments or auto-matically tuned via machine learning to maximizea retrieval metric of choice (e.g.
mean averageprecision).
The most simple but guaranteed waywould be to directly perform brute force searchfor the global optimum over the entire parame-ter space.
However, not only the computationalcost of this so-called direct search would becomeundoubtfully expensive as the number of parame-ters increase, but most retrieval metrics are non-smooth with respect to model parameters (Met-zler, 2007).
For these reasons, we propose to adapta learning-to-rank framework that optimizes mul-tiple parameters of phrase-based retrieval modelseffectively with less computation cost and withoutany specific retrieval metric.Specifically, we use a gradient descent methodwith the RankNet cost function (Burges et al,2005) to perform effective parameter optimiza-tions, as in (Taylor et al, 2006; Metzler, 2007).The basic idea is to find a local minimum of a costfunction defined over pairwise document prefer-ence.
Assume that, given a query Q, there isa set of document pairs RQ based on relevancejudgements, such that (D1, D2) ?
RQ impliesdocument D1 should be ranked higher than D2.Given a defined set of pairwise preferences R, theRankNet cost function is computed as:C(Q,R) =??Q?Q??
(D1,D2)?RQlog(1 + eY ) (6)whereQ is the set of queries, and Y = s(Q;D2)?s(Q;D1) using the current parameter setting.In order to minimize the cost function, we com-pute gradients of Eq.
6 with respect to each pa-rameter ?i by applying the chain rule:?C?
?i =??Q?Q??(D1,D2)?RQ?C?Y?Y?
?i (7)where ?C?Y and ?Y?
?i are computed as:?C?Y =exp[s(Q;D2)?
s(Q;D1)]1 + exp[s(Q;D2)?
s(Q;D1)] (8)1050?Y?
?i =?s(Q;D2)?
?i ??s(Q;D1)?
?i (9)With the retrieval model in Eq.
2 and ?
(X),f(X) in Eq.
4 and 5, the partial derivate ofs(Q;D) with respect to ?i is computed as follows:?s(Q;D)??i=|Q|?i=1xi?(X)(1?
?(X)?
)?
(P (qi|qhi,D)?P (qi|D))?
(X)P (qi|qhi , D) + (1?
?
(X))P (qi|D)(10)3.4 FeaturesWe experimented with various features that arepotentially useful for not only discriminating aphrase itself but characterizing its constituents.
Inthis section, we report only the ones that havemade positive contributions to the overall retrievalperformance.
The two main criteria consideredin the selection of the features are the followings:compositionality and discriminative power.Compositionality FeaturesFeatures on phrase compositionality are designedto measure how likely a phrase can be representedas its constituent words without forming a phrase;if a phrase in a query has very high composition-ality, there is a high probability that its relevantdocuments do not contain the phrase.
In this case,emphasizing the phrase unit could be very risky inretrieval.
In the opposite case that a phrase is un-compositional, it is obvious that occurrence of aphrase in a document can be a stronger evidenceof relevance than its constituent words.Compositionality of a phrase can be roughlymeasured by using corpus statistics or its linguis-tic characteristics; we have observed that, in manytimes, an extremely-uncompositional phrase ap-pears as a noun phrase, and the distance betweenits constituent words is generally fixed within ashort distance.
In addition, it has a tendency to beused repeatedly in a document because its seman-tics cannot be represented with individual con-stituent words.
Based on these intuitions, we de-vise the following features:Ratio of multiple occurrences (RMO): This is areal-valued feature that measures the ratio of thephrase repeatedly used in a document.
The valueof this feature is calculated as follows:x =?
?D;count(wi?whi ,D)>1count(wi?whi , D)count(wi ?
whi , C) + ?
(11)where wi ?
whi is a phrase in a given query,count(x, y) is the count of x in y, and ?
is a small-valued constant to prevent unreliable estimationby very rarely-occurred phrases.Ratio of single-occurrences (RSO): This is a bi-nary feature that indicates whether or not a phraseoccurs once in most documents containing it.
Thiscan be regarded as a supplementary feature ofRMO.Preferred phrasal type (PPT): This feature indi-cates the phrasal type that the phrase prefers in acollection.
We consider only two cases (whetherthe phrase prefers verb phrase or adjective-nounphrase types) as features in the experiments1.Preferred distance (PD): This is a binary featureindicating whether or not the phrase prefers longdistance (> 1) between constituents in the docu-ment collection.Uncertainty of preferred distance (UPD): We alsouse the entropy (H) of the modification distance(d) of the given phrase in the collection to measurethe compositionality; if the distance is not fixedand is highly uncertain, the phrase may be verycompositional.
The entropy is computed as:x = H(p(d = x|wi ?
whi)) (12)where d ?
1, 2, 3, long and all probabilities areestimated with discount smoothing.
We simplyuse two binary features regarding the uncertaintyof distance; one indicates whether the uncertaintyof a phrase is very high (> 0.85), and the otherindicates whether the uncertainty is very low (<0.05)2.Uncertainty of preferred phrasal type (UPPT): Assimilar to the uncertainty of preferred distance, theuncertainty of the preferred phrasal type of thephrase can be also used as a feature.
We considerthis factor as a form of a binary feature indicatingwhether the uncertainty is very high or not.Discriminative Power FeaturesIn some cases, the occurrence of a phrase can be avaluable evidence even if the phrase is very likelyto be compositional.
For example, it is well knownthat the use of a phrase can be effective in retrievalwhen its constituent words appear very frequentlyin the collection, because each word would have avery low discriminative power for relevance.
Onthe contrary, if a constituent word occurs very1For other phrasal types, significant differences were notobserved in the experiments.2Although it may be more natural to use a real-valued fea-ture, we use these binary features because of the two practicalreasons; firstly, it could be very difficult to find an adequatetransformation function with real values, and secondly, thetwo intervals at tails were observed to be more important thanthe rest.1051rarely in the collection, it could not be effectiveto use the phrase even if the phrase is highly un-compositional.
Similarly, if the probability that aphrase occurs in a document where its constituentwords co-occur is very high, we might not need toplace more emphasis on the phrase than on words,because co-occurrence information naturally in-corporated in retrieval models may have enoughpower to distinguish relevant documents.
Basedon these intuitions, we define the following fea-tures:Document frequency of constituents (DF): Weuse the document frequency of a constituent astwo binary features: one indicating whether theword has very high document frequency (>10%of documents in a collection) and the other oneindicating whether it has very low document fre-quency (<0.2% of documents, which is approxi-mately 1,000 in our experiments).Probability of constituents as phrase (CPP): Thisfeature is computed as a relative frequency of doc-uments containing a phrase over documents wheretwo constituent words appear together.One interesting fact that we observe is that doc-ument frequency of the modifier is generally astronger evidence on the utility of a phrase in re-trieval than of the headword.
In the case of theheadword, we could not find an evidence that ithas to be considered in phrase weighting.
It seemsto be a natural conclusion, because the importanceof the modifier word in retrieval is subordinate tothe relation to its headword, but the headword isnot in many phrases.
For example, in the case ofthe query ?tropical storms?, retrieving a documentonly containing tropical can be meaningless, but adocument about storm can be meaningful.
Basedon this observation, we only incorporate documentfrequency features of syntactic modifiers in the ex-periments.4 ExperimentsIn this section, we report the retrieval perfor-mances of the proposed method with appropriatebaselines over a range of training sets.4.1 Experimental SetupRetrieval models: We have set two retrieval mod-els, namely the word model and the (phrase-based)one-parameter model, as baselines.
The rankingfunction of the word model is equivalent to Eq.
2,with ?
in Eq.
3 being set to zero (i.e.
the phraseprobability makes no effect on the ranking).
Theranking function of the one-parameter model isalso equivalent to Eq.
2, with ?
in Eq.
3 used ?asis?
(i.e.
as a constant parameter value optimizedusing gradient descent method, without being re-placed to a logistic function).
Both baseline mod-els cannot differentiate the importance of phrasesin a query.
To make a distinction from the base-line models, we will name our proposed methodas a multi-parameter model.In our experiments, all the probabilities in allretrieval models are smoothed with the collectionstatistics by using dirichlet priors (Zhai and Laf-ferty, 2001).Corpus (Training/Test): We have conductedlarge-scale experiments on three sets of TREC?sAd Hoc Test Collections, namely TREC-6, TREC-7, and TREC-8.
Three query sets, TREC-6 top-ics 301-350, TREC-7 topics 351-400, and TREC-8 topics 401-450, along with their relevance judg-ments have been used.
We only used the title fieldas query.When performing experiments on each queryset with the one-parameter and the multi-parameter models, the other two query sets havebeen used for learning the optimal parameters.
Foreach query in the training set, we have generateddocument pairs for training by the following strat-egy: first, we have gathered top m ranked doc-uments from retrieval results by using the wordmodel and the one-parameter model (by manuallysetting ?
in Eq.
3 to the fixed constants, 0 and 0.1respectively).
Then, we have sampled at most rrelevant documents and n non-relevant documentsfrom each one and generated document pairs fromthem.
In our experiments, m, r, and n is set to100, 10, and 40, respectively.Phrase extraction and indexing: We evaluateour proposed method on two different types ofphrases: syntactic head-modifier pairs (syntac-tic phrases) and simple bigram phrases (statisti-cal phrases).
To index the syntactic phrases, weuse the method proposed in (Strzalkowski et al,1994) with Connexor FDG parser3, the syntacticparser based on the functional dependency gram-mar (Tapanainen and Jarvinen, 1997).
All neces-sary information for feature values were indexedtogether for both syntactic and statistical phrases.To maintain indexes in a manageable size, phrases3Connexor FDG parser is a commercial parser; the demois available at: http://www.connexor.com/demo1052Test set ?
Training set6 ?
7+8 7 ?
6+8 8 ?
6+7Model Metric \ Query all partial all partial all partialWord MAP 0.2135 0.1433 0.1883 0.1876 0.2380 0.2576(Baseline 1) R-Prec 0.2575 0.1894 0.2351 0.2319 0.2828 0.2990P@10 0.3660 0.3333 0.4100 0.4324 0.4520 0.4517One-parameter MAP 0.2254 0.1633?
0.1988 0.2031 0.2352 0.2528(Baseline 2) R-Prec 0.2738 0.2165 0.2503 0.2543 0.2833 0.2998P@10 0.3820 0.3600 0.4540 0.4971 0.4580 0.4621Multi-parameter MAP 0.2293?
0.1697?
0.2038?
0.2105?
0.2452 0.2701(Proposed) R-Prec 0.2773 0.2225 0.2534 0.2589 0.2891 0.3099P@10 0.4020 0.3933 0.4540 0.4971 0.4700 0.4828Table 1: Retrieval performance of different models on syntactic phrases.
Italicized MAP values withsymbols ?
and ?
indicate statistically significant improvements over the word model according to Stu-dent?s t-test at p < 0.05 level and p < 0.01 level, respectively.
Bold figures indicate the best performedcase for each metric.that occurred less than 10 times in the documentcollections were not indexed.4.2 Experimental ResultsTable 1 shows the experimental results of the threeretrieval models on the syntactic phrase (head-modifier pair).
In the table, partial denotes theperformance evaluated on queries containing morethan one phrase that appeared in the document col-lection4; this shows the actual performance differ-ence between models.
Note that the ranking re-sults of all retrieval models would be the same asthe result of the word model if a query does notcontain any phrases in the document collection,because P (qi|qhi , D) would be calculated as zeroeventually.
As evaluation measures, we used themean average precision (MAP), R-precision (R-Prec), and precisions at top 10 ranks (P@10).As shown in Table 1, when a syntactic phrase isused for retrieval, one-parameter model trained bygradient-descent method generally performs bet-ter than the word model, but the benefits are in-consistent; it achieves approximately 15% and 8%improvements on the partial query set of TREC-6 and 7 over the word model, but it fails to showany improvement on TREC-8 queries.
This maybe a natural result since the one-parameter modelis very sensitive to the averaged contribution ofphrases used for training.
Compared to the queriesin TREC-6 and 7, the TREC-8 queries containmore phrases that are not effective for retrieval4The number of queries containing a phrase in TREC-6,7, and 8 query set is 31, 34, and 29, respectively.(i.e.
ones that hurt the retrieval performance whenused).
This indicates that without distinguishingeffective phrases from ineffective phrases for re-trieval, the model trained from one training set forphrase would not work consistently on other un-seen query sets.Note that the proposed model outperforms allthe baselines over all query sets; this shows thatdifferentiating relative contributions of phrasescan improve the retrieval performance of the one-parameter model considerably and consistently.As shown in the table, the multi-parameter modelimproves by approximately 18% and 12% on theTREC-6 and 7 partial query sets, and it alsosignificantly outperforms both the word modeland the one-parameter model on the TREC-8query set.
Specifically, the improvement on theTREC-8 query set shows one advantage of usingour proposed method; by separating potentially-ineffective phrases and effective phrases based onthe features, it not only improves the retrievalperformance for each query but makes parameterlearning less sensitive to the training set.Figure 1 shows some examples demonstratingthe different behaviors of the one-parameter modeland the multi-parameters model.
On the figure, theun-dotted lines indicate the variation of averageprecision scores when ?
value in Eq.
3 is manu-ally set.
As ?
gets closer to 0, the ranking formulabecomes equivalent to the word model.As shown in the figure, the optimal point of ?
isquiet different from query to query.
For example,in cases of the query ?ferry sinking?
and industrial10530.350.40.450.50.550.60.650.70 0.1 0.2 0.3 0.4 0.5AvgPrlambdaPerformance variation for the query ?ferry sinking?varing lambdaone-parametermultiple-parameter0.30.350.40.450.50.550.60.650 0.1 0.2 0.3 0.4 0.5AvgPrlambdaPerformance variation for the query ?industrial espionage?varing lambdaone-parametermultiple-parameter0.320.330.340.350.360.370.380.390 0.05 0.1 0.15 0.2 0.25 0.3 0.35 0.4 0.45 0.5AvgPrlambdaPerformance variation for the query ?
declining birth rates?varing lambdaone-parametermultiple-parameter 0.20.250.30.350.40.450 0.1 0.2 0.3 0.4 0.5AvgPrlambdaPerformance variation for the query ?amazon rain forest?varing lambdaone-parametermultiple-parameterFigure 1: Performance variations for the queries ?ferry sinking?, ?industrial espionage?, ?declining birthrate?
and ?Amazon rain forest?
according to ?
in Eq.
3.espionage?
on the upper side, the optimal point isthe value close to 0 and 1 respectively.
This meansthat the occurrences of the phrase ?ferry sinking?in a document is better to be less-weighted inretrieval while ?industrial espionage?
should betreated as a much more important evidence than itsconstituent words.
Obviously, such differences arenot good for one-parameter model assuming rela-tive contributions of phrases uniformly.
For bothopposite cases, the multi-parameter model signifi-cantly outperforms one-parameter model.The two examples at the bottom of Figure 1show the difficulty of optimizing phrase-based re-trieval using one uniform parameter.
For example,the query ?declining birth rate?
contains two dif-ferent phrases, ?declining rate?
and ?birth rate?,which have potentially-different effectiveness inretrieval; the phrase ?declining rate?
would notbe helpful for retrieval because it is highly com-positional, but the phrase ?birth rate?
could be avery strong evidence for relevance since it is con-ventionally used as a phrase.
In this case, wecan get only small benefit from the one-parametermodel even if we find optimal ?
from gradientdescent, because it will be just a compromisedvalue between two different, optimized ?s.
Forsuch query, the multi-parameter model could bemore effective than the one-parameter model byenabling to set different ?s on phrases accord-ing to their predicted contributions.
Note that themulti-parameter model significantly outperformsthe one-parameter model and all manually-set ?sfor the queries ?declining birth rate?
and ?Amazonrain forest?, which also has one effective phrase,?rain forest?, and one non-effective phrase, ?Ama-zon forest?.Since our method is not limited to a particulartype of phrases, we have also conducted experi-ments on statistical phrases (bigrams) with a re-duced set of features directed applicable; RMO,RSO, PD5, DF, and CPP; the features requiringlinguistic preprocessing (e.g.
PPT) are not used,because it is unrealistic to use them under bigram-based retrieval setting.
Moreover, the feature UPDis not used in the experiments because the uncer-5In most cases, the distance between words in a bigramis 1, but sometimes, it could be more than 1 because of theeffect of stopword removal.1054Test ?
TrainingModel Metric 6 ?
7+8 7 ?
6+8 8 ?
6+7Word MAP 0.2135 0.1883 0.2380(Baseline 1) R-Prec 0.2575 0.2351 0.2828P@10 0.3660 0.4100 0.4520One-parameter MAP 0.2229 0.1979 0.2492?
(Baseline 2) R-Prec 0.2716 0.2456 0.2959P@10 0.3720 0.4500 0.4620Multi-parameter MAP 0.2224 0.2025?
0.2499?
(Proposed) R-Prec 0.2707 0.2457 0.2952P@10 0.3780 0.4520 0.4600Table 2: Retrieval performance of different models, using statistical phrases.tainty of preferred distance does not vary much forbigram phrases.
The results are shown in Table 2.The results of experiments using statisticalphrases show that multi-parameter model yieldsadditional performance improvement againstbaselines in many cases, but the benefit is in-significant and inconsistent.
As shown in Table 2,according to the MAP score, the multi-parametermodel outperforms the one-parameter model onthe TREC-7 and 8 query sets, but it performsslightly worse on the TREC-6 query set.We suspect that this is because of the lackof features to distinguish an effective statisticalphrases from ineffective statistical phrase.
In ourobservation, the bigram phrases also show a verysimilar behavior in retrieval; some of them arevery effective while others can deteriorate the per-formance of retrieval models.
However, in caseof using statistical phrases, the ?
computed by ourmulti-parameter model would be often similar tothe one computed by the one-parameter model,when there is no sufficient evidence to differen-tiate a phrase.
Moreover, the insufficient amountof features may have caused the multi-parametermodel to overfit to the training set easily.The small size of training corpus could be an an-other reason.
The number of queries we used fortraining is less than 80 when removing a query notcontaining a phrase, which is definitely not a suf-ficient amount to learn optimal parameters.
How-ever, if we recall that the multi-parameter modelworked reasonably in the experiments using syn-tactic phrases with the same training sets, the lackof features would be a more important reason.Although we have not mainly focused on fea-tures in this paper, it would be strongly necessaryto find other useful features, not only for statisticalphrases, but also for syntactic phrases.
For exam-ple, statistics from query logs and the probabilityof snippet containing a same phrase in a query isclicked by user could be considered as useful fea-tures.
Also, the size of the training data (queries)and the document collection may not be sufficientenough to conclude the effectiveness of our pro-posed method; our method should be examined ina larger collection with more queries.
Those willbe one of our future works.5 ConclusionIn this paper, we present a novel method to differ-entiate impacts of phrases in retrieval accordingto their relative contribution over the constituentwords.
The contributions of this paper can be sum-marized in three-fold: a) we proposed a generalframework to learn the potential contribution ofphrases in retrieval by ?parameterizing?
the fac-tor interpolating the phrase weight and the wordweight on features and optimizing the parametersusing RankNet-based gradient descent algorithm,b) we devised a set of potentially useful featuresto distinguish effective and non-effective phrases,and c) we showed that the proposed method can beeffective in terms of retrieval by conducting a se-ries of experiments on the TREC test collections.As mentioned earlier, the finding of additionalfeatures, specifically for statistical phrases, wouldbe necessary.
Moreover, for a thorough analysison the effect of our framework, additional experi-ments on larger and more realistic collections (e.g.the Web environment) would be required.
Thesewill be our future work.1055ReferencesAvi Arampatzis, Theo P. van der Weide, Cornelis H. A.Koster, and P. van Bommel.
2000.
Linguistically-motivated information retrieval.
In Encyclopedia ofLibrary and Information Science.Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier,Matt Deeds, Nicole Hamilton, and Greg Hullender.2005.
Learning to rank using gradient descent.
InProceedings of ICML ?05, pages 89?96.W.
Bruce Croft, Howard R. Turtle, and David D. Lewis.1991.
The use of phrases and structured queries ininformation retrieval.
In Proceedings of SIGIR ?91,pages 32?45.Martin Dillon and Ann S. Gray.
1983.
Fasit: Afully automatic syntactically based indexing system.Journal of the American Society for Information Sci-ence, 34(2):99?108.Joel L. Fagan.
1987.
Automatic phrase indexing fordocument retrieval.
In Proceedings of SIGIR ?87,pages 91?101.Jianfeng Gao, Jian-Yun Nie, Guangyuan Wu, and Gui-hong Cao.
2004.
Dependence language model forinformation retrieval.
In Proceedings of SIGIR ?04,pages 170?177.Wessel Kraaij and Rene?e Pohlmann.
1998.
Comparingthe effect of syntactic vs. statistical phrase indexingstrategies for dutch.
In Proceedings of ECDL ?98,pages 605?617.David D. Lewis and W. Bruce Croft.
1990.
Term clus-tering of syntactic phrases.
In Proceedings of SIGIR?90, pages 385?404.Robert M. Losee, Jr. 1994.
Term dependence: truncat-ing the bahadur lazarsfeld expansion.
InformationProcessing and Management, 30(2):293?303.Loic Maisonnasse, Gilles Serasset, and Jean-PierreChevallet.
2005.
Using syntactic dependency andlanguage model x-iota ir system for clips mono andbilingual experiments in clef 2005.
In WorkingNotes for the CLEF 2005 Workshop.Donald Metzler and W. Bruce Croft.
2005.
A markovrandom field model for term dependencies.
In Pro-ceedings of SIGIR ?05, pages 472?479.Donald Metzler.
2007.
Using gradient descent to opti-mize language modeling smoothing parameters.
InProceedings of SIGIR ?07, pages 687?688.David R. H. Miller, Tim Leek, and Richard M.Schwartz.
1999.
A hidden markov model informa-tion retrieval system.
In Proceedings of SIGIR ?99,pages 214?221.Mandar Mitra, Chris Buckley, Amit Singhal, and ClaireCardie.
1997.
An analysis of statistical and syn-tactic phrases.
In Proceedings of RIAO ?97, pages200?214.Fei Song and W. Bruce Croft.
1999.
A general lan-guage model for information retrieval.
In Proceed-ings of CIKM ?99, pages 316?321.Munirathnam Srikanth and Rohini Srihari.
2003.
Ex-ploiting syntactic structure of queries in a languagemodeling approach to ir.
In Proceedings of CIKM?03, pages 476?483.Tomek Strzalkowski, Jose Perez-Carballo, and MihneaMarinescu.
1994.
Natural language information re-trieval: Trec-3 report.
In Proceedings of TREC-3,pages 39?54.Tao Tao and ChengXiang Zhai.
2007.
An explorationof proximity measures in information retrieval.
InProceedings of SIGIR ?07, pages 295?302.Pasi Tapanainen and Timo Jarvinen.
1997.
A non-projective dependency parser.
In Proceedings ofANLP ?97, pages 64?71.Michael Taylor, Hugo Zaragoza, Nick Craswell,Stephen Robertson, and Chris Burges.
2006.
Opti-misation methods for ranking functions with multi-ple parameters.
In Proceedings of CIKM ?06, pages585?593.Andrew Turpin and Alistair Moffat.
1999.
Statisti-cal phrases for vector-space information retrieval.
InProceedings of SIGIR ?99, pages 309?310.C.
J. van Rijsbergen.
1977.
A theoretical basis for theuse of co-occurrence data in information retrieval.Journal of Documentation, 33(2):106?119.S.
K. M. Wong, Wojciech Ziarko, and Patrick C. N.Wong.
1985.
Generalized vector spaces model ininformation retrieval.
In Proceedings of SIGIR ?85,pages 18?25.Chengxiang Zhai and John Lafferty.
2001.
A studyof smoothing methods for language models appliedto ad hoc information retrieval.
In Proceedings ofSIGIR ?01, pages 334?342.Chengxiang Zhai.
1997.
Fast statistical parsing ofnoun phrases for document indexing.
In Proceed-ings of ANLP ?97, pages 312?319.1056
