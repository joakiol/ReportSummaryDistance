Proceedings of the Second Workshop on Statistical Machine Translation, pages 212?215,Prague, June 2007. c?2007 Association for Computational LinguisticsUCB System Description for the WMT 2007 Shared TaskPreslav NakovEECS, CS divisionUniversity of California at BerkeleyBerkeley, CA 94720nakov@cs.berkeley.eduMarti HearstSchool of InformationUniversity of California at BerkeleyBerkeley, CA 94720hearst@ischool.berkeley.eduAbstractFor the WMT 2007 shared task, the UCBerkeley team employed three techniques ofinterest.
First, we used monolingual syntac-tic paraphrases to provide syntactic varietyto the source training set sentences.
Sec-ond, we trained two language models: asmall in-domain model and a large out-of-domain model.
Finally, we made use of re-sults from prior research that shows that cog-nate pairs can improve word alignments.
Wecontributed runs translating English to Span-ish, French, and German using various com-binations of these techniques.1 IntroductionModern Statistical Machine Translation (SMT) sys-tems are trained on aligned sentences of bilingualcorpora, typically from one domain.
When tested ontext from that same domain, such systems demon-strate state-of-the art performance; however, onout-of-domain text the results can get significantlyworse.
For example, on the WMT 2006 SharedTask evaluation, the French to English translationBLEU scores dropped from about 30 to about 20 fornearly all systems, when tested on News Commen-tary rather than Europarl (Koehn and Monz, 2006).Therefore, this year the shared task organizershave provided 1M words of bilingual News Com-mentary training data in addition to the Europarldata (about 30M words), thus challenging the par-ticipants to experiment with domain adaptation.Below we describe our domain adaptation exper-iments, trying to achieve better results on the NewsCommentary data.
In addition to training on bothdata sets, we make use of monolingual syntacticparaphrases of the English side of the data.2 Monolingual Syntactic ParaphrasingIn many cases, the testing text contains ?phrases?that are equivalent, but syntactically different fromthe phrases learned on training, and the potential fora high-quality translation is missed.
We address thisproblem by using nearly equivalent syntactic para-phrases of the original sentences.
Each paraphrasedsentence is paired with the foreign translation that isassociated with the original sentence in the trainingdata.
This augmented training corpus can then beused to train an SMT system.
Alternatively, we canparaphrase the test sentences making them closer tothe target language syntax.Given an English sentence, we parse it with theStanford parser (Klein and Manning, 2003) and thengenerate paraphrases using the following syntactictransformations:1.
[NP NP1 P NP2]?
[NP NP2 NP1].inequality in income?
income inequality.2.
[NP NP1 of NP2]?
[NP NP2 poss NP1].inequality of income?
income?s inequality.3.
NPposs ?
NP.income?s inequality?
income inequality.4.
NPposs ?
NPPPof .income?s inequality?
inequality of income.5.
NPNC ?
NPposs.income inequality?
income?s inequality.6.
NPNC ?
NPPP .income inequality?
inequality in incomes.212Sharply rising income inequality has raised the stakes of the economic game .Sharply rising income inequality has raised the economic game ?s stakes .Sharply rising income inequality has raised the economic game stakes .Sharply rising inequality of income has raised the stakes of the economic game .Sharply rising inequality of income has raised the economic game ?s stakes .Sharply rising inequality of income has raised the economic game stakes .Sharply rising inequality of incomes has raised the stakes of the economic game .Sharply rising inequality of incomes has raised the economic game ?s stakes .Sharply rising inequality of incomes has raised the economic game stakes .Sharply rising inequality in income has raised the stakes of the economic game .Sharply rising inequality in income has raised the economic game ?s stakes .Sharply rising inequality in income has raised the economic game stakes .Sharply rising inequality in incomes has raised the stakes of the economic game .Sharply rising inequality in incomes has raised the economic game ?s stakes .Sharply rising inequality in incomes has raised the economic game stakes .Table 1: Sample sentence and automatically generated paraphrases.
Paraphrased NCs are in italics.7.
remove that where optionalI think that he is right?
I think he is right.8.
add that where optionalI think he is right?
I think that he is right.where:poss possessive marker: ?
or ?s;P preposition;NPPP NP with internal PP-attachment;NPPPof NP with internal PP headed by of;NPposs NP with internal possessive marker;NPNC NP that is a Noun Compound.While the first four and the last two transfor-mations are purely syntactic, (5) and (6) are not.The algorithm must determine whether a possessivemarker is feasible for (5) and must choose the cor-rect preposition for (6).
In either case, for noun com-pounds (NCs) of length 3 or more, it also needs tochoose the position to modify, e.g., inquiry?s com-mittee chairman vs. inquiry committee?s chairman.In order to ensure accuracy of the paraphrases,we use statistics gathered from the Web, using avariation of the approaches presented in Lapata andKeller (2004) and Nakov and Hearst (2005).
We usepatterns to generate possible prepositional or copulaparaphrases in the context of the preceding and thefollowing word in the sentence, First we split theNC into two parts N1 and N2 in all possible ways,e.g., beef import ban lifting would be split as: (a)N1=?beef?, N2=?import ban lifting?, (b) N1=?beefimport?, N2=?ban lifting?, and (c) N1=?beef importban?, N2=?lifting?.
For every split, we issue exactphrase queries to the Google search engine usingthe following patterns:"lt N1 poss N2 rt""lt N2 prep det N ?1 rt""lt N2 that be det N ?1 rt""lt N2 that be prep det N ?1 rt"where: lt is the word preceding N1 in the originalsentence or empty if none, rt is the word followingN2 in the original sentence or empty if none, possis a possessive marker (?s or ?
), that is that, whichor who, be is is or are, det is a determiner (the, a,an, or none), prep is one of the 8 prepositions usedby Lauer (1995) for semantic interpretation of NCs:about, at, for, from, in, of, on, and with, and N ?1 canbe either N1, or N1 with the number of its last wordchanged from singular/plural to plural/singular.For all splits, we collect the number of page hitsfor each instantiation of each pattern, filtering outthe paraphrases whose page hit count is less than 10.We then calculate the total number of page hitsH forall paraphrases (for all splits and all patterns), andretain those ones whose page hits count is at least10% of H .
Note that this allows for multiple para-phrases of an NC.
If no paraphrases are retained, we213repeat the above procedure with lt set to the emptystring.
If there are still no good paraphrases, we setthe rt to the empty string.
If this does not help ei-ther, we make a final attempt, by setting both lt andrt to the empty string.Table 1 shows the paraphrases for a sample sen-tence.
We can see that income inequality is para-phrased as inequality of income, inequality of in-comes, inequality in income and inequality in in-comes; also economic game?s stakes becomes eco-nomic game stakes and stakes of the economic game.3 ExperimentsTable 2 shows a summary of our submissions: theofficial runs are marked with a ?.
For our experi-ments, we used the baseline system, provided by theorganizers, which we modified in different ways, asdescribed below.3.1 Domain AdaptationAll our systems were trained on both corpora.?
Language models.
We used two languagemodels (LM) ?
a small in-domain one (trainedonNews Commentary) and a big out-of-domainone (trained on Europarl).
For example, for EN?
ES (from English to Spanish), on the low-ercased tuning data set, using in-domain LMonly achieved a BLEU of 0.332910, while us-ing both LMs yielded 0.354927, a significanteffect.?
Cognates.
Previous research has found thatusing cognates can help get better word align-ments (and ultimately better MT results), espe-cially in case of a small training set.
We usedthe method described in (Kondrak et al, 2003)in order to extract cognates from the two datasets.
We then added them as sentence pairs tothe News Commentary corpus before trainingthe word alignment models1 for ucb3, ucb4 anducb5.1Following (Kondrak et al, 2003), we considered words oflength 4 or more, we required the length ratio to be between710 and107 , and we accepted as potential cognates all pairs forwhich the longest common subsequence ratio (LCSR) was 0.58or more.
We repeated 3 times the cognate pairs extracted fromthe Europarl, and 4 times the ones from News Commentary.?
Phrases.
The ucb5 system uses the Europarldata in order to learn an additional phrase ta-ble and an additional lexicalized re-orderingmodel.3.2 Paraphrasing the Training SetIn two of our experiments (ucb3, ucb4 and ucb5),we used a paraphrased version of the training NewsCommentary data, using all rules (1)-(8).
We trainedtwo separate MT systems: one on the original cor-pus, and another one on the paraphrased version.We then used both resulting lexicalized re-orderingmodels and a merged phrase table with extra para-meters: if a phrase appeared in both phrase tables,it now had 9 instead of 5 parameters (4 from eachtable, plus a phrase penalty), and if it was in oneof the phrase tables only, the 4 missing parameterswere filled with 1e-40.The ucb5 system is also trained on Europarl,yielding a third lexicalized re-ordering model andadding 4 new parameters to the phrase table entries.Unfortunately, longer sentences (up to 100 to-kens, rather than 40), longer phrases (up to 10 to-kens, rather than 7), two LMs (rather than justone), higher-order LMs (order 7, rather than 3),multiple higher-order lexicalized re-ordering mod-els (up to 3), etc.
all contributed to increased sys-tem?s complexity, and, as a result, time limitationsprevented us from performing minimum-error-ratetraining (MERT) (Och, 2003) for ucb3, ucb4 anducb5.
Therefore, we used the MERT parameter val-ues from ucb1 instead, e.g.
the first 4 phrase weightsof ucb1 were divided by two, copied twice and usedin ucb3 as the first 8 phrase-table parameters.
Theextra 4 parameters of ucb5 came from training a sep-arate MT system on the Europarl data (scaled ac-cordingly).3.3 Paraphrasing the Test SetIn some of our experiments (ucb2 and ucb4), givena test sentence, we generated the single most-likelyparaphrase, which makes it syntactically closer toSpanish and French.
Unlike English, which makesextensive use of noun compounds, these languagesstrongly prefer connecting the nouns with a preposi-tion (and less often turning a noun into an adjective).Therefore, we paraphrased all NCs using preposi-tions, by applying rules (4) and (6).
In addition, we214Languages System LM size Paraphrasing Cognates?
Extra phrases MERTNews Europarl train?
test?
Europarl finished?EN?
ES ucb1?
3 5 +ucb2 3 5 + +ucb3 5 7 + +ucb4 5 7 + + +ucb5 5 7 + + +EN?
FR ucb3 5 7 + +ucb4?
5 7 + + +EN?
DE ucb1?
5 7 + +ucb2 5 7 + + +Table 2: Summary of our submissions.
All runs are for the News Commentary test data.
The officialsubmissions are marked with a star.applied rule (8), since its Spanish/French equivalentque (as well as the German da?)
is always obliga-tory.
These transformations affected 927 out of the2007 test sentences.
We also used this transformeddata set when translating to German (however, Ger-man uses NCs as much as English does).3.4 Other Non-standard SettingsBelow we discuss some non-standard settings thatdiffer from the ones suggested by the organizers intheir baseline system.
First, following Birch et al(2006), who found that higher-order LMs give bet-ter results2, we used a 5-gram LM for News Com-mentary, and 7-gram LM for Europarl (as opposedto 3-gram, as done normally).
Second, for all runswe trained our systems on all sentences of length upto 100 (rather than 40, as suggested in the baselinesystem).
Third, we used a maximum phrase lengthlimit of 10 (rather than 7, as typically done).
Fourth,we used both a lexicalized and distance-based re-ordering models (as opposed to lexicalized only, asin the baseline system).
Finally, while we did notuse any resources other than the ones provided bythe shared task organizers, we made use of Web fre-quencies when paraphrasing the training corpus, asexplained above.4 Conclusions and Future WorkWe have presented various approaches to domainadaptation and their combinations.
Unfortunately,2They used a 5-gram LM trained on Europarl, but wepushed the idea further, using a 7-gram LM with a Kneser-Neysmoothing.computational complexity and time limitations pre-vented us from doing proper MERT for the interest-ing more complex systems.
We plan to do a properMERT training and to study the impact of the indi-vidual components in isolation.Acknowledgements: This work supported in partby NSF DBI-0317510.ReferencesAlexandra Birch, Chris Callison-Burch, Miles Osborne, andPhilipp Koehn.
2006.
Constraining the phrase-based, jointprobability statistical translation model.
In Proc.
of Work-shop on Statistical Machine Translation, pages 154?157.Dan Klein and Christopher Manning.
2003.
Accurate unlexi-calized parsing.
In Proceedings of ACL ?03.Philipp Koehn and Christof Monz.
2006.
Manual and auto-matic evaluation of machine translation between europeanlanguages.
In Proceedings on the Workshop on StatisticalMachine Translation, pages 102?121.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.
2003.Cognates can improve statistical translation models.
In Pro-ceedings of NAACL, pages 46?48.Mirella Lapata and Frank Keller.
2004.
The web as a base-line: Evaluating the performance of unsupervised web-basedmodels for a range of nlp tasks.
In Proceedings of HLT-NAACL ?04.Mark Lauer.
1995.
Corpus statistics meet the noun compound:some empirical results.
In Proceedings of ACL ?95.Preslav Nakov and Marti Hearst.
2005.
Search engine statisticsbeyond the n-gram: Application to noun compound bracket-ing.
In Proceedings of CoNLL ?05.Franz Josef Och.
2003.
Minimum error rate training in sta-tistical machine translation.
In Proceedings of ACL, pages160?167.215
