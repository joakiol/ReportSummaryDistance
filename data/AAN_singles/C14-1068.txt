Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 721?730, Dublin, Ireland, August 23-29 2014.Towards Syntax-aware Compositional Distributional Semantic ModelsLorenzo FerroneDepartment of Enterprise EngineeringUniversity of Rome ?Tor Vergata?Via del Politecnico, 1 00173 Romalorenzo.ferrone@gmail.comFabio Massimo ZanzottoDepartment of Enterprise EngineeringUniversity of Rome ?Tor Vergata?Via del Politecnico, 1 00173 Romafabio.massimo.zanzotto@uniroma2.itAbstractCompositional Distributional Semantics Models (CDSMs) are traditionally seen as an entire dif-ferent world with respect to Tree Kernels (TKs).
In this paper, we show that under a suitableregime these two approaches can be regarded as the same and, thus, structural information anddistributional semantics can successfully cooperate in CSDMs for NLP tasks.
Leveraging ondistributed trees, we present a novel class of CDSMs that encode both structure and distribu-tional meaning: the distributed smoothed trees (DSTs).
By using DSTs to compute the similarityamong sentences, we implicitly define the distributed smoothed tree kernels (DSTKs).
Exper-iment with our DSTs show that DSTKs approximate the corresponding smoothed tree kernels(STKs).
Thus, DSTs encode both structural and distributional semantics of text fragments asSTKs do.
Experiments on RTE and STS show that distributional semantics encoded in DSTKsincrease performance over structure-only kernels.1 IntroductionCompositional distributional semantics is a flourishing research area that leverages distributional seman-tics (see Turney and Pantel (2010), Baroni and Lenci (2010)) to produce meaning of simple phrasesand full sentences (hereafter called text fragments).
The aim is to scale up the success of word-levelrelatedness detection to longer fragments of text.
Determining similarity or relatedness among sentencesis useful for many applications, such as multi-document summarization, recognizing textual entailment(Dagan et al., 2013), and semantic textual similarity detection (Agirre et al., 2013).Compositional distributional semantics models (CDSMs) are functions mapping text fragments tovectors (or higher-order tensors).
Functions for simple phrases directly map distributional vectors ofwords to distributional vectors for the phrases (Mitchell and Lapata, 2008; Baroni and Zamparelli, 2010;Clark et al., 2008; Grefenstette and Sadrzadeh, 2011; Zanzotto et al., 2010).
Functions for full sentencesare generally defined as recursive functions over the ones for phrases (Socher et al., 2011; Socher et al.,2012; Kalchbrenner and Blunsom, 2013).
Distributional vectors for text fragments are then used as innerlayers in neural networks, or to compute similarity among text fragments via dot product.CDSMs generally exploit structured representations txof text fragments x to derive their meaningf(tx), but the structural information, although extremely important, is obfuscated in the final vectors.Structure and meaning can interact in unexpected ways when computing cosine similarity (or dot prod-uct) between vectors of two text fragments, as shown for full additive models in (Ferrone and Zanzotto,2013).
Smoothed tree kernels (STK) (Mehdad et al., 2010; Croce et al., 2011) instead realize a clearerinteraction between structural information and distributional meaning.
STKs are specific realizations ofconvolution kernels (Haussler, 1999) where the similarity function is recursively (and, thus, composition-ally) computed.
Distributional vectors are used to represent word meaning in computing the similarityamong nodes.
STKs, however, are not considered part of the CDSMs family.
As usual in kernel machinesThis work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedingsfooter are added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/721(Cristianini and Shawe-Taylor, 2000), STKs directly compute the similarity between two text fragmentsx and y over their tree representations txand ty, that is, STK(tx, ty).
The function f that maps treesinto vectors is only implicitly used, and, thus, STK(tx, ty) is not explicitly expressed as the dot productor the cosine between f(tx) and f(ty).
Such a function f , which is the underlying reproducing functionof the kernel (Aronszajn, 1950), is a CDSM since it maps trees to vectors by using distributional mean-ing.
However, the huge dimensionality of Rn(since it has to represent the set of all possible subtrees)prevents to actually compute the function f(t), which thus can only remain implicit.Distributed tree kernels (DTK) (Zanzotto and Dell?Arciprete, 2012) partially solve the last problem.DTKs approximate standard tree kernels (such as (Collins and Duffy, 2002)) by defining an explicitfunction DT that maps trees to vectors in Rmwhere m  n and Rnis the explicit space for treekernels.
DTKs approximate standard tree kernels (TK), that is, ?DT (tx), DT (ty)?
?
TK(tx, ty), byapproximating the corresponding reproducing function (Aronszajn, 1950).
Thus, these distributed treesare small vectors that encode structural information.
In DTKs tree nodes u and v (and then also words)are represented by nearly orthonormal vectors, that is, vectors?u and?v such that ?
?u,?v ?
?
?
(?u,?v )where ?
is the Kroneker?s delta.
This is in contrast with distributional semantics vectors where ?
?u,?v ?is allowed to be any value in [0, 1] according to the similarity between the words v and u.
Thus, earlyattempts to include distributional vectors in the DTs failed (Zanzotto and Dell?Arciprete, 2011).In this paper, leveraging on distributed trees, we present a novel class of CDSMs that encode bothstructure and distributional meaning: the distributed smoothed trees (DST).
DSTs carry structure and dis-tributional meaning on a 2-dimensional tensor (a matrix): one dimension encodes the structure and onedimension encodes the meaning.
By using DSTs to compute the similarity among sentences with a gen-eralized dot product (or cosine), we implicitly define the distributed smoothed tree kernels (DSTK) whichapproximate the corresponding STKs.
We present two DSTs along with the two smoothed tree kernels(STKs) that they approximate.
We experiment with our DSTs to show that their generalized dot productsapproximate STKs by directly comparing the produced similarities and by comparing their performanceson two tasks: recognizing textual entailment (RTE) and semantic similarity detection (STS).
Both exper-iments show that the dot product on DSTs approximates STKs and, thus, DSTs encode both structuraland distributional semantics of text fragments in tractable 2-dimensional tensors.
Experiments on STSand RTE show that distributional semantics encoded in DSTs increases performance over structure-onlykernels.
DSTs are the first positive way of taking into account both structure and distributional meaningin CDSMs.The rest of the paper is organized as follows.
Section 2 introduces the basic notation used in the paper.Section 3 describe our distributed smoothed trees as compositional distributional semantic models thatcan represent both structural and semantic information.
Section 4 reports on the experiments.
Finally,Section 5 draws some conclusions.2 NotationBefore describing the distributed smoothed trees (DST) we introduce a formal way to denoteconstituency-based lexicalized parse trees, as DSTs exploit this kind of data structures.Lexicalized trees are denoted with the letter t and N(t) denotes the set of non terminal nodes of treet.
Each non-terminal node n ?
N(t) has a label lncomposed of two parts ln= (sn, wn): snis thesyntactic label, while wnis the semantic headword of the tree headed by n, along with its part-of-speechtag.
For example, the root node of the tree in Fig.1 has the label S:booked::v where S is the syntacticinformation and booked::v is the semantic head of the whole tree.
Terminal nodes of trees are treateddifferently, these nodes represent only wordswnwithout any additional information, and their labels thusonly consist of the word itself (see Fig.
1).
The structure of a tree is represented as follows: Given a treet, h(t) is its root node and s(t) is the tree formed from t but considering only the syntactic structure (thatis, only the snpart of the labels), ci(n) denotes i-th child of a node n. As usual for constituency-basedparse trees, pre-terminal nodes are nodes that have a single terminal node as child.
Finally,?sn?
Rmand?wn?
Rkrepresent respectively distributed vectors for node labels snand distributional vectors forwords wn, whereas T represents the matrix of a tree t encoding structure and distributional meaning.The difference between distributed and distributional vectors is described in the next section.722S:booked::vXXXXXNP:we::pPRP:we::pWeVP:booked::vPPPPV:booked::vbookedNP:flight::nHHHDT:the::dtheNN:flight::nflightFigure 1: A lexicalized treesS(t) = {S:booked::vll,,NP VP,VP:booked::v@ V NP,NP:we::pPRP,S:booked::v@ NPPRPVP,S:booked::vQQNP VP@ V NP, .
.
.
,VP:booked::vbb""VbookedNP@ DT NN, .
.
.
}Figure 2: Subtrees of the tree t in Figure 1 (a non-exhaustive list)3 Distributed Smoothed Trees as Compositional Distributional Semantic ModelsWe define Distributed Smoothed Trees as recursive functions DST mapping lexicalized trees t to Rm?kwhere matrices T = DST (t) encode both syntactic structures and distributional vectors.
DSTs arethus compositional distributional models, as they map lexicalized trees to matrices, and they are definedrecursively on distributed vectors for syntactic node labels and distributional vectors for words.
In thefollowing we introduce DSTs: Section 3.1 gives a rough idea of the method, Section 3.2 describes howto recursively encode structures in vectors by means of distributed trees (Zanzotto and Dell?Arciprete,2012), and finally Section 3.3 merges distributed trees and distributional semantic vectors in matrices.3.1 The method in a glanceWe describe here the approach in a few sentences.
In line with tree kernels over structures (Collins andDuffy, 2002), we introduce the set S(t) of the subtrees tiof a given lexicalized tree t. A subtree tiis inthe set S(t) if s(ti) is a subtree of s(t) and, if n is a node in ti, all the siblings of n in t are in ti.
For eachnode of tiwe only consider its syntactic label sn, except for the head h(ti) for which we also considerits semantic component wn.
Figure 2 reports a sample for the subtrees of the tree in Fig.
1 The recursivefunctions DSTs we define compute the following:T =?ti?S(t)Tiwhere Tiis the matrix associated to each subtree ti.
The similarity between two text fragments a and brepresented as lexicalized trees taand tbcan be computed using the Frobenius product between the twomatrices Taand Tb, that is:?Ta,Tb?F=?tai?S(ta)tbj?S(tb)?Tai,Tbj?F(1)We want to obtain that the product ?Tai,Tbj?Fapproximates the dot product between the distributionalvectors of the head words (?Tai,Tbj?F?
??h(tai),?h(tbj)?)
whenever the syntactic structure of the subtreesis the same (that is s(tai) = s(tbj)), and ?Tai,Tbj?F?
0 otherwise.
This property is expressed as:?Tai,Tbj?F?
?
(s(tai), s(tbj)) ?
??h(tai),?h(tbj)?
(2)7233.2 Representing Syntactic Structures with Distributed TreesDistributed trees (Zanzotto and Dell?Arciprete, 2012) recursively encode syntactic trees t in small vectorsby means of a recursive function DT .
These DTs preserve structural information as the dot productbetween the DTs of two trees approximates the classical tree kernels TK as defined by Collins andDuffy (2002), that is, TK(ta, tb) ?
?DT (ta), DT (tb)?.
To obtain this result, distributed trees DT (t) aredefined as follows:DT (t) =?ti?S(t)?
?|N(ti)|?s(ti) (3)where S(t) is again the set of the subtrees of t,?s(ti) are vectors in Rmcorresponding to tree fragment tiand?
?|N(ti)|is the weight of subtree tiin the final feature space, with ?
being the traditional parameterused to penalize large subtrees and |N(ti)| being the number of nodes in ti.
The approximation of treekernels is then given by the fact that ??s(ti),?s(tj)?
?
?
(s(ti), s(tj)).
Vectors with this property are calleddistributed vectors.
A key feature of the distributed vectors of subtrees?s(ti) is that these vectors are builtcompositionally from a setN of nearly orthonormal random vectors?sn, that are associated to each nodelabel.
Given a subtree s(ti), the related vector is obtained as:?s(ti) =?sn1?sn2.
.
.?snk=?
(sn,wn)?N(ti)?snwhere node vectors?sniare ordered according to a depth-first visit of subtree tiand is a vector composi-tion operation, specifically the shuffled circular convolution1.
This function guarantees that two differentsubtrees have nearly orthonormal vectors (see (Zanzotto and Dell?Arciprete, 2012) for more details).
Forexample, the fifth tree t5of set S(t) in Figure 2 is?s(t5) =?S  (?NP  (?V P  (?V?NP ))).
Thus, DTsin Equation 3 can be recursively defined as:DT (t) =?n?N(t)?
(n) (4)where ?
(n) is recursively defined as follows:?
(n) ={??
(?sn?w) if n is a pre-terminal node??
?sn(?i(?sci(n)+ ?
(ci(n)))) if n is an internal node(5)The vector ?
(n) encodes all the subtrees that have root in n along with their penalizing weight?
?|N(ti)|,that is:?
(n) =?ti?S(t)?h(ti)=n?
?|N(ti)|?s(ti)This is what we need in order to define our distributed smoothed trees.3.3 Representing distributional meaning and distributed structure with matricesWe now move from distributed trees (encoded as small vectors) to distributed smoothed trees (DST)represented as matrices.
DST is a function that maps trees t to matrices T. In analogy with Equation 4,DST is defined as:DST (t) =?n?N(t)S(n)where S(n) is now defined as:S(n) = ?
(n)?wn>1The shuffled circular convolution  is defined as?a?b = s1(?a ) ?
s2(?b ) where ?
is the circular convolution and s1ands2are two different (but fixed) random permutations of vector elements.724where ?
(n) is the one defined in Equation 5 and (?
)>is vector transposition.
By combining the twoequations, DST (t) is the sum of the matrices described in Equation 1:DST (t) =?n?N(t)?ti?S(t)?h(ti)=n?
?|N(ti)|?s(ti)?wn>=?ti?S(t)?s(ti)?wn>where n is h(ti) and Ti=?s(ti)?wh(ti)>is the outer product between the distributed vector?s(ti) andthe distributional vector?wh(ti).
There is an important property of the outer product that applies to theFrobenius product: ?
?a?w>,?b?v>?F= ?
?a ,?b ?
?
?
?w,?v ?.
Using this property, we have that Equation 2 issatisfied as:?Ti,Tj?F= ??s(ti),?s(tj)?
?
??wh(ti),?wh(tj)?
?
?
(s(ti), s(tj)) ?
?
?wh(ti),?wh(tj)?We refer to the Frobenius product of two distributed smoothed trees as distributed smoothed tree kernel(DSTK).
These DSTKs are approximating the smoothed tree kernels described in the next section.
Wepropose two versions of our DSTKs according to how we produce distributional vectors for words.
Wehave a plain version DSTK0when we use distributional vectors?wnas they are, and a slightly modifiedversion DSTK+1when we use as distributional vectors?wn?=(1?wn).3.4 The Approximated Smoothed Tree KernelsThe two CDSMs we proposed, that is, the two distributed smoothed tree kernelsDSTK0andDSTK+1,are approximating two specific tree kernels belonging to the smoothed tree kernels class (e.g., (Mehdadet al., 2010; Croce et al., 2011)).
These two specific smoothed tree kernels recursively compute (but, therecursive formulation is not given here) the following general equation:STK(ta, tb) =?ti?S(ta)tj?S(tb)?
(ti, tj)where ?
(ti, tj) is the similarity weight between two subtrees tiand tj.
DTSK0and DSTK+1approx-imate respectively STK0and STK+1where the weights are defined as follows:?0(ti, tj) = ??wh(ti),?wh(tj)?
?
?
(s(ti), s(tj)) ??
?|N(ti)|+|N(tj)|?+1(ti, tj) = (?
?wh(ti),?wh(tj)?+ 1) ?
?
(s(ti), s(tj)) ??
?|N(ti)|+|N(tj)|STK+1is actually computing a sum between STK0and the tree kernel (Collins and Duffy, 2002).4 Experimental investigation4.1 Experimental set-upGeneric settings We experimented with two datasets: the Recognizing Textual Entailment datasets(RTE) (Dagan et al., 2006) and the the Semantic Textual Similarity 2013 datasets (STS) (Agirre et al.,2013).
The STS task consists of determining the degree of similarity (ranging from 0 to 5) betweentwo sentences.
We used the data for core task of the 2013 challenge data.
The STS datasets contains5 datasets: headlines, OnWN, FNWN, SMT and MSRpar, which contains respectively 750, 561, 189,750 and 1500 pairs.
The first four datasets were used for testing, while all the training has been doneon the fifth.
RTE is instead the task of deciding whether a long text T entails a shorter text, typicallya single sentence, called hypothesis H .
It has been often seen as a classification task (see (Dagan etal., 2013)).
We used four datasets: RTE1, RTE2, RTE3, and RTE5, with the standard split betweentraining and testing.
The dev/test distribution for RTE1-3, and RTE5 is respectively 567/800, 800/800,800/800, and 600/600 T-H pairs.
Distributional vectors are derived with DISSECT (Dinu et al., 2013)from a corpus obtained by the concatenation of ukWaC (wacky.sslmit.unibo.it), a mid-2009 dump of725RTE1 RTE2 RTE3 RTE5 headl FNWN OnWN SMTSTK0vs DSTK01024 0.86 0.84 0.90 0.84 0.87 0.65 0.95 0.772048 0.87 0.84 0.91 0.84 0.90 0.65 0.96 0.77STK+1vs DSTK+11024 0.81 0.77 0.83 0.72 0.88 0.53 0.93 0.662048 0.82 0.78 0.84 0.74 0.91 0.56 0.94 0.67Table 1: Spearman?s correlation between Distributed Smoothed Tree Kernels and Smoothed Tree Kernelsthe English Wikipedia (en.wikipedia.org) and the British National Corpus (www.natcorp.ox.ac.uk), fora total of about 2.8 billion words.
We collected a 35K-by-35K matrix by counting co-occurrence of the30K most frequent content lemmas in the corpus (nouns, adjectives and verbs) and all the content lemmasoccurring in the datasets within a 3 word window.
The raw count vectors were transformed into positivePointwise Mutual Information scores and reduced to 300 dimensions by Singular Value Decomposition.This setup was picked without tuning, as we found it effective in previous, unrelated experiments.
Tobuild our DTSKs and for the two baseline kernels TK and DTK, we used the implementation of thedistributed tree kernels2.
We used: 1024 and 2048 as the dimension of the distributed vectors, the weight?
is set to 0.4 as it is a value generally considered optimal for many applications (see also (Zanzotto andDell?Arciprete, 2012)).
The statistical significance, where reported, is computed according to the signtest.Direct correlation settings For the direct correlation experiments, we used the RTE data sets and thetesting sets of the STS dataset (that is, headlines, OnWN, FNWN, SMT).
We computed the Spearman?scorrelation between values produced by our DSTK0and DSTK+1and produced by the standard ver-sions of the smoothed tree kernel, that is, respectively, STK0and STK+1.
We obtained text fragmentpairs by randomly sampling two text fragments in the selected set.
For each set, we produced exactly thenumber of examples in the set, e.g., we produced 567 pairs for RTE1 dev, etc..Task-based settings For the task-based experiments, we compared systems using the standard evalua-tion measure and the standard split in the respective challenges.
As usual in RTE challenges the measureused is the accuracy, as testing sets have the same number of entailment and non-entailment pairs.
ForSTS, we used MSRpar as training, and we used the 4 test sets as testing.
We compared systems usingthe Pearson?s correlation as the standard evaluation measure for the challenge3.
Thus, results can becompared with the results of the challenge.As classifier and regression learner, we used the java version of LIBSVM (Chang and Lin, 2011).
Inthe two tasks we used in a different way our DSTs (and the related STKs) within the learners.
In thefollowing, we refer to instances in RTE or STS as pairs p = (ta, tb) where taand tbare the two parsetrees for the two sentences a and b for STS and for the text a and the hypothesis b in RTE.We will indicate with K(p1, p2) the final kernel used in the learning algorithm, which takes as in-put two training instances, while we will use ?
to denote either any of our DSTK (that is, ?
(x, y) =?DST (x), DST (y)?)
or any of the standard smoothed tree kernels (that is, ?
(x, y) = STK(x, y)).In STS, we encoded only similarity feature between the two sentences.
Thus, we used two classes ofkernels: (1) the syntactic/semantic class (SS) with the final kernel defined as K(p1, p2) = (?
(ta1, tb1) ??
(ta2, tb2) + 1)2; and, (2) the SS class along with token-based similarity (SSTS) where the final kernel isK(p1, p2) = (?
(ta1, tb1) ?
?
(ta2, tb2) + TS(a1, b1) ?
TS(a2, b2) + 1)2where TS(a, b) counts the percent ofthe common content tokens in a and b.In RTE, we followed standard approaches (Dagan et al., 2013; Zanzotto et al., 2009), that is, weexploited two models: a model with only a rewrite rule feature space (RR) and a model with the previousspace along with a token-level similarity feature (RRTWS).
The two models use our DSTs and thestandard STKs in the following way as kernel functions: (1) RR(p1, p2) = ?
(ta1, ta2) + ?
(tb1, tb2); (2)RRTS(p1, p2) = ?
(ta1, ta2) + ?
(tb1, tb2) + (TWS(a1, b1) ?
TS(a2, b2) + 1)2where TWS is a weightedtoken similarity as in Corley and Mihalcea (2005).2http://code.google.com/p/distributed-tree-kernels/3Correlations are obtained with the organizers?
script726SS SSTSheadl FNWN OnWN SMT Average headl FNWN OnWN SMT AverageTS ?
?
?
?
?
0.701 0.311 0.515 0.323 0.462Add ?
?
?
?
?
0.691 0.268 0.511 0.317 0.446Mult ?
?
?
?
?
0.291 ?0.03 0.228 0.291 0.201DTK 0.448 0.118 0.162 0.301 0.257 0.698 0.311 0.510 0.329 0.462TK 0.456 0.145 0.158 0.303 0.265?0.699 0.316 0.511 0.329 0.463?DSTK00.491 0.155 0.358 0.305 0.327?0.700 0.314 0.519 0.327 0.465STK00.490 0.159 0.349 0.305 0.325?0.700 0.314 0.519 0.327 0.465?DSTK+10.475 0.138 0.266 0.304 0.295 0.700 0.314 0.519 0.327 0.465STK+10.478 0.156 0.259 0.305 0.299?0.700 0.314 0.519 0.327 0.465?Table 2: Task-based analysis: Correlation on Semantic Textual Similarity ( ?
is different from DTK, TK,DSTK+1, and STK+1with a stat.sig.
of p > 0.1; ?
the difference between the kernel and its distributedversion is not stat.sig.
)We also used two standard and simple CDSMs to compare with: the Additive model (Add) and theMultiplicative model (Mult) as firstly discussed in Mitchell and Lapata (2008).
The Additive Modelperforms a sum of all the distributional vectors of the content words in the text fragment and the Multi-plicative model performs an element-wise product among all the content vectors.
These are used in theabove models as ?
(a, b).Finally, to investigate whether our DSTKs behave better than purely structural models, we experi-mented with the classical tree kernel (TK) (Collins and Duffy, 2002) and the distributed tree kernel (DTK)(Zanzotto and Dell?Arciprete, 2012).
Again, these kernels are used in the above models as ?
(ta, tb).4.2 ResultsTable 1 reports the results for the correlation experiments.
We report the Spearman?s correlations overthe different sets (and different dimensions of distributed vectors) between our DSTK0and the STK0(first two rows) and between our DSTK+1and the corresponding STK+1(second two rows) .
Thecorrelation is above 0.80 in average for both RTE and STS datasets in the case of DSTK0and theSTK0.
The correlation between DSTK+1and the corresponding STK+1is instead a little bit lower.This depends on the fact that DSTK+1is approximating the sum of two kernels the TK and the STK0(as STK+1is the sum of the two kernels).
Then, the underlying feature space is bigger with respect to theone of STK0and, thus, approximating it is more difficult.
The approximation also depends on the size ofthe distributed vectors.
Higher dimensions yield to better approximation: if we increase the distributedvectors dimension from 1024 to 2048 the correlation between DSTK+1and STK+1increases up to0.80 on RTE and up to 0.77 on STS.
This direct analysis of the correlation shows that our CDSM areapproximating the corresponding kernel function and there is room of improvement by increasing the sizeof distributed vectors.
Task-based experiments confirm the above trend.
Table 2 and Table 3, respectively,report the correlation of different systems on STS and the accuracies of the different systems on RTE.Our CDSMs are compared against baseline systems (Add,Mult, TK, andDTK) in order to understandwhether in the specific tasks our more complex model is interesting, and against, again, the systems withthe corresponding smoothed tree kernels in order to explore whether our DSTKs approximate systemsbased on STKs.
For all this set of experiment we fixed the dimension of the distributed vectors to1024.
Table 2 is organized as follows: columns 2-6 report the correlation of the STS systems basedon syntactic/semantic similarity (SS) and columns 7-11 report the accuracies of SS systems along withtoken-based similarity (SSTS).
The first observation for this task is that baseline systems based only onthe token similarity (first row) behave extremely well.
These results are above many models presentedin the 2013 Shared Task (see (Agirre et al., 2013)).
This can be disappointing as we cannot appreciatedifferences among methods in the columns SSTS.
But, focusing on the results without this importanttoken-based similarity, we can better understand if our model is capturing both structural and semanticinformation, that is, if DSTKs behave similarly to STKs.
It is also useless to compare results of DSTKsand STKs to the Add baseline model as Add is basically doing a weighted count of the common words727RR RRTWSRTE1 RTE2 RTE3 RTE5 Average RTE1 RTE2 RTE3 RTE5 AverageAdd 0.541 0.496 0.507 0.520 0.516 0.560 0.538 0.643 0.578 0.579Mult 0.495 0.481 0.497 0.528 0.500 0.533 0.563 0.642 0.586 0.581DTK 0.533 0.515 0.516 0.530 0.523 0.583 0.601 0.643 0.621 0.612TK 0.561 0.552 0.531 0.54 0.546 0.608 0.627 0.648 0.630 0.628DSTK00.571 0.551 0.547 0.531 0.550?0.628 0.616 0.650 0.625 0.629?STK00.586 0.563 0.538 0.545 0.558?0.638 0.618 0.648 0.636 0.635?DSTK+10.588 0.562 0.555 0.541 0.561?0.638 0.621 0.646 0.652 0.639?STK+10.586 0.562 0.542 0.546 0.559?0.638 0.618 0.650 0.636 0.635?Table 3: Task-based analysis: Accuracy on Recognizing Textual Entailment ( ?
is different from DTKand TK wiht a stat.sig.
of p > 0.1; ?
the difference between the kernel and its distributed counterpart isnot statistically significant.
)that is exactly what the token-based similarity is doing.
Add slightly decreases the performance ofthe token-based similarity.
The Mult model instead behaves very poorly.
Comparing rows in the SScolumns, we can discover that DSTK0and DSTK+1behave significantly better than DTK and thatDSTK0behave better than the standard TK.
Thus, our DSTKs are positively exploitng distributionalsemantic information along with structural information.
Moreover, both DSTK0and DSTK+1behavesimilarly to the corresponding models with standard kernels STKs.
Results in this task confirm thatstructural and semantic information are both captured by CDSMs based on DSTs.Table 3 is organized as follows: columns 2-6 report the accuracy of the RTE systems based on rewriterules (RR) and columns 7-11 report the accuracies of RR systems along with token similarity (RRTS).Results on RTE are extremely promising as all the models including structural information and distribu-tional semantics have better results than the two baseline models with a statistical significance of 93.7%.For RR models DTSK0, STK0, DSTK+1, and STK+1have an average accuracy 7.9% higher thanAdd and 11.4% higher than Mult model.
For RRTS, the same happens with an average accuracy 9.58%higher than Add and 9.2% higher than the Mult.
This task is more sensible to syntactic informationthan STS.
As expected (Mehdad et al., 2010), STKs behave also better than tree kernels exploiting onlysyntactic information.
But, more importantly, our CDSMs based on the DSTs are behaving similarlyto these smoothed tree kernels, in contrast to what reported in (Zanzotto and Dell?Arciprete, 2011).
In(Polajnar et al., 2013), it appears that results of the Zanzotto and Dell?Arciprete (2011)?s method arecomparable to the results of STKs for STS, but this is mainly due to the flattening of the performancegiven by the lexical token similarity feature which is extremely relevant in STS.
Even if distributed treekernels do not approximate well tree kernels with distributed vectors dimension of 1024, our smoothedversions of the distributed tree kernels approximate correctly the corresponding smoothed tree kernels.Their small difference is not statistically significant (less than 70%).
The fact that our DSTKs behavesignificantly better than baseline models in RTE and they approximate the corresponding STKs showsthat it is possible to positively exploit structural information in CDSMs.5 Conclusions and Future WorkDistributed Smoothed Trees (DST) are a novel class of Compositional Distributional Semantics Mod-els (CDSM) that effectively encode structural information and distributional semantics in tractable 2-dimensional tensors, as experiments show.
The paper shows that DSTs contribute to close the gap be-tween two apparently different approaches: CDSMs and convolution kernels (Haussler, 1999).
Thiscontribute to start a discussion on a deeper understanding of the representation power of structural infor-mation of existing CDSMs.References[Agirre et al.2013] Eneko Agirre, Daniel Cer, Mona Diab, Aitor Gonzalez-Agirre, and Weiwei Guo.
2013.
*sem2013 shared task: Semantic textual similarity.
In Second Joint Conference on Lexical and Computational728Semantics (*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic TextualSimilarity, pages 32?43, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
[Aronszajn1950] N. Aronszajn.
1950.
Theory of reproducing kernels.
Transactions of the American MathematicalSociety, 68(3):337?404.
[Baroni and Lenci2010] Marco Baroni and Alessandro Lenci.
2010.
Distributional memory: A general frameworkfor corpus-based semantics.
Comput.
Linguist., 36(4):673?721, December.
[Baroni and Zamparelli2010] Marco Baroni and Roberto Zamparelli.
2010.
Nouns are vectors, adjectives are ma-trices: Representing adjective-noun constructions in semantic space.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing, pages 1183?1193, Cambridge, MA, October.
Associationfor Computational Linguistics.
[Chang and Lin2011] Chih-Chung Chang and Chih-Jen Lin.
2011.
LIBSVM: A library for support vector ma-chines.
ACM Transactions on Intelligent Systems and Technology, 2:27:1?27:27.
Software available athttp://www.csie.ntu.edu.tw/?cjlin/libsvm.
[Clark et al.2008] Stephen Clark, Bob Coecke, and Mehrnoosh Sadrzadeh.
2008.
A compositional distributionalmodel of meaning.
Proceedings of the Second Symposium on Quantum Interaction (QI-2008), pages 133?140.
[Collins and Duffy2002] Michael Collins and Nigel Duffy.
2002.
New ranking algorithms for parsing and tagging:Kernels over discrete structures, and the voted perceptron.
In Proceedings of ACL02.
[Corley and Mihalcea2005] Courtney Corley and Rada Mihalcea.
2005.
Measuring the semantic similarity of texts.In Proc.
of the ACL Workshop on Empirical Modeling of Semantic Equivalence and Entailment, pages 13?18.Association for Computational Linguistics, Ann Arbor, Michigan, June.
[Cristianini and Shawe-Taylor2000] Nello Cristianini and John Shawe-Taylor.
2000.
An Introduction to SupportVector Machines and Other Kernel-based Learning Methods.
Cambridge University Press, March.
[Croce et al.2011] Danilo Croce, Alessandro Moschitti, and Roberto Basili.
2011.
Structured lexical similarity viaconvolution kernels on dependency trees.
In Proceedings of the Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?11, pages 1034?1046, Stroudsburg, PA, USA.
Association for ComputationalLinguistics.
[Dagan et al.2006] Ido Dagan, Oren Glickman, and Bernardo Magnini.
2006.
The pascal recognising textualentailment challenge.
In Quionero-Candela et al., editor, LNAI 3944: MLCW 2005, pages 177?190.
Springer-Verlag, Milan, Italy.
[Dagan et al.2013] Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto.
2013.
RecognizingTextual Entailment: Models and Applications.
Synthesis Lectures on Human Language Technologies.
Morgan& Claypool Publishers.
[Dinu et al.2013] Georgiana Dinu, Nghia The Pham, and Marco Baroni.
2013.
DISSECT: DIStributional SEman-tics Composition Toolkit.
In Proceedings of ACL (System Demonstrations), pages 31?36, Sofia, Bulgaria.
[Ferrone and Zanzotto2013] Lorenzo Ferrone and Fabio Massimo Zanzotto.
2013.
Linear compositional distribu-tional semantics and structural kernels.
In Proceedings of the Joint Symposium of Semantic Processing (JSSP),pages ?.
[Grefenstette and Sadrzadeh2011] Edward Grefenstette and Mehrnoosh Sadrzadeh.
2011.
Experimental supportfor a categorical compositional distributional model of meaning.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, EMNLP ?11, pages 1394?1404, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.
[Haussler1999] David Haussler.
1999.
Convolution kernels on discrete structures.
Technical report, University ofCalifornia at Santa Cruz.
[Kalchbrenner and Blunsom2013] Nal Kalchbrenner and Phil Blunsom.
2013.
Recurrent convolutional neural net-works for discourse compositionality.
Proceedings of the 2013 Workshop on Continuous Vector Space Modelsand their Compositionality.
[Mehdad et al.2010] Yashar Mehdad, Alessandro Moschitti, and Fabio Massimo Zanzotto.
2010.
Syntac-tic/semantic structures for textual entailment recognition.
In Human Language Technologies: The 2010 AnnualConference of the North American Chapter of the Association for Computational Linguistics, HLT ?10, pages1020?1028, Stroudsburg, PA, USA.
Association for Computational Linguistics.729[Mitchell and Lapata2008] Jeff Mitchell and Mirella Lapata.
2008.
Vector-based models of semantic composi-tion.
In Proceedings of ACL-08: HLT, pages 236?244, Columbus, Ohio, June.
Association for ComputationalLinguistics.
[Polajnar et al.2013] Tamara Polajnar, Laura Rimell, and Douwe Kiela.
2013.
Ucam-core: Incorporating struc-tured distributional similarity into sts.
In Second Joint Conference on Lexical and Computational Semantics(*SEM), Volume 1: Proceedings of the Main Conference and the Shared Task: Semantic Textual Similarity,pages 85?89, Atlanta, Georgia, USA, June.
Association for Computational Linguistics.
[Socher et al.2011] Richard Socher, Eric H. Huang, Jeffrey Pennington, Andrew Y. Ng, and Christopher D. Man-ning.
2011.
Dynamic pooling and unfolding recursive autoencoders for paraphrase detection.
In Advances inNeural Information Processing Systems 24.
[Socher et al.2012] Richard Socher, Brody Huval, Christopher D. Manning, and Andrew Y. Ng.
2012.
SemanticCompositionality Through Recursive Matrix-Vector Spaces.
In Proceedings of the 2012 Conference on Empir-ical Methods in Natural Language Processing (EMNLP).
[Turney and Pantel2010] Peter D. Turney and Patrick Pantel.
2010.
From frequency to meaning: Vector spacemodels of semantics.
J. Artif.
Intell.
Res.
(JAIR), 37:141?188.
[Zanzotto and Dell?Arciprete2011] Fabio Massimo Zanzotto and Lorenzo Dell?Arciprete.
2011.
Distributed struc-tures and distributional meaning.
In Proceedings of the Workshop on Distributional Semantics and Composi-tionality, pages 10?15, Portland, Oregon, USA, June.
Association for Computational Linguistics.
[Zanzotto and Dell?Arciprete2012] F.M.
Zanzotto and L. Dell?Arciprete.
2012.
Distributed tree kernels.
In Pro-ceedings of International Conference on Machine Learning, pages 193?200.
[Zanzotto et al.2009] Fabio Massimo Zanzotto, Marco Pennacchiotti, and Alessandro Moschitti.
2009.
A machinelearning approach to textual entailment recognition.
NATURAL LANGUAGE ENGINEERING, 15-04:551?582.
[Zanzotto et al.2010] Fabio Massimo Zanzotto, Ioannis Korkontzelos, Francesca Fallucchi, and Suresh Manand-har.
2010.
Estimating linear models for compositional distributional semantics.
In Proceedings of the 23rdInternational Conference on Computational Linguistics (COLING), August,.730
