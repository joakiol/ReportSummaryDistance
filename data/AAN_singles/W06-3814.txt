Workshop on TextGraphs, at HLT-NAACL 2006, pages 89?96,New York City, June 2006. c?2006 Association for Computational LinguisticsEvaluating and optimizing the parametersof an unsupervised graph-based WSD algorithmEneko Agirre, David Mart?
?nez, Oier Lo?pez de Lacalle and Aitor SoroaIXA NLP GroupUniversity of Basque CountryDonostia, Basque Contrya.soroa@ehu.esAbstractVe?ronis (2004) has recently proposedan innovative unsupervised algorithm forword sense disambiguation based onsmall-world graphs called HyperLex.
Thispaper explores two sides of the algorithm.First, we extend Ve?ronis?
work by opti-mizing the free parameters (on a set ofwords which is different to the target set).Second, given that the empirical compar-ison among unsupervised systems (andwith respect to supervised systems) is sel-dom made, we used hand-tagged corporato map the induced senses to a standardlexicon (WordNet) and a publicly avail-able gold standard (Senseval 3 EnglishLexical Sample).
Our results for nounsshow that thanks to the optimization ofparameters and the mapping method, Hy-perLex obtains results close to supervisedsystems using the same kind of bag-of-words features.
Given the informationloss inherent in any mapping step and thefact that the parameters were tuned for an-other set of words, these are very interest-ing results.1 IntroductionWord sense disambiguation (WSD) is a key en-abling technology.
Supervised WSD techniques arethe best performing in public evaluations, but needlarge amounts of hand-tagging data.
Existing hand-annotated corpora like SemCor (Miller et al, 1993),which is annotated with WordNet senses (Fellbaum,1998) allow for a small improvement over the simplemost frequent sense heuristic, as attested in the all-words track of the last Senseval competition (Sny-der and Palmer, 2004).
In theory, larger amountsof training data (SemCor has approx.
500M words)would improve the performance of supervised WSD,but no current project exists to provide such an ex-pensive resource.Supervised WSD is based on the ?fixed-list ofsenses?
paradigm, where the senses for a target wordare a closed list coming from a dictionary or lex-icon.
Lexicographers and semanticists have longwarned about the problems of such an approach,where senses are listed separately as discrete enti-ties, and have argued in favor of more complex rep-resentations, where, for instance, senses are denseregions in a continuum (Cruse, 2000).Unsupervised WSD has followed this line ofthinking, and tries to induce word senses directlyfrom the corpus.
Typical unsupervised WSD sys-tems involve clustering techniques, which group to-gether similar examples.
Given a set of inducedclusters (which represent word uses or senses1),each new occurrence of the target word will be com-pared to the clusters and the most similar cluster willbe selected as its sense.Most of the unsupervised WSD work has beenbased on the vector space model (Schu?tze, 1998;Pantel and Lin, 2002; Purandare and Pedersen,2004), where each example is represented by a vec-tor of features (e.g.
the words occurring in thecontext).
Recently, Ve?ronis (Ve?ronis, 2004) has1Unsupervised WSD approaches prefer the term ?word uses?to ?word senses?.
In this paper we use them interchangeably torefer to both the induced clusters, and to the word senses fromsome reference lexicon.89proposed HyperLex, an application of graph mod-els to WSD based on the small-world propertiesof cooccurrence graphs.
Hand inspection of theclusters (called hubs in this setting) by the authorwas very positive, with hubs capturing the mainsenses of the words.
Besides, hand inspection of thedisambiguated occurrences yielded precisions over95% (compared to a most frequent baseline of 73%)which is an outstanding figure for WSD systems.We noticed that HyperLex had some free param-eters and had not been evaluated against a publicgold standard.
Besides, we were struck by the fewworks where supervised and unsupervised systemswere evaluated on the same test data.
In this pa-per we use an automatic method to map the inducedsenses to WordNet using hand-tagged corpora, en-abling the automatic evaluation against availablegold standards (Senseval 3 English Lexical Sam-ple S3LS (Mihalcea et al, 2004)) and the automaticoptimization of the free parameters of the method.The use of hand-tagged corpora for tagging makesthis algorithm a mixture of unsupervised and super-vised: the method to induce senses in completelyunsupervised, but the mapping is supervised (albeitvery straightforward).This paper is structured as follows.
We firstpresent the graph-based algorithm as proposed byVe?ronis, reviewing briefly the features of small-world graphs.
Section 3 presents our framework formapping and evaluating the induced hubs.
Section 4introduces parameter optimization.
Section 5 showsthe experiment setting and results.
Section 6 ana-lyzes the results and presents related work.
Finally,we draw the conclusions and advance future work.2 HyperLexBefore presenting the HyperLex algorithm itself, webriefly introduce small-world graphs.2.1 Small world graphsThe small-world nature of a graph can be explainedin terms of its clustering coefficient and characteris-tic path length.
The clustering coefficient of a graphshows the extent to which nodes tend to form con-nected groups that have many edges connecting eachother in the group, and few edges leading out ofthe group.
On the other side, the characteristic pathlength represents ?closeness?
in a graph.
See (Wattsand Strogatz, 1998) for further details on these char-acteristics.Randomly built graphs exhibit low clustering co-efficients and are believed to represent somethingvery close to the minimal possible average pathlength, at least in expectation.
Perfectly orderedgraphs, on the other side, show high clustering coef-ficients but also high average path length.
Accordingto Watts and Strogatz (1998), small-world graphs liebetween these two extremes: they exhibit high clus-tering coefficients, but short average path lengths.Barabasi and Albert (1999) use the term ?scale-free?
to graphs whose degree probability follow apower-law2.
Specifically, scale free graphs followthe property that the probability P (k) that a vertexin the graph interacts with k other vertices decays asa power-law, following P (k) ?
k??.
It turns outthat in this kind of graphs there exist nodes centrallylocated and highly connected, called hubs.2.2 The HyperLex algorithm for WSDThe HyperLex algorithm builds a cooccurrencegraph for all pairs of words cooccurring in the con-text of the target word.
Ve?ronis shows that this kindof graph fulfills the properties of small world graphs,and thus possess highly connected components inthe graph.
The centers or prototypes of these com-ponents, called hubs, eventually identify the mainword uses (senses) of the target word.We will briefly introduce the algorithm here,check (Ve?ronis, 2004) for further details.
For eachword to be disambiguated, a text corpus is collected,consisting of the paragraphs where the word occurs.From this corpus, a cooccurrence graph for the tar-get word is built.
Nodes in the graph correspond tothe words3 in the text (except the target word itself).Two words appearing in the same paragraph are saidto cooccur, and are connected with edges.
Each edgeis assigned with a weight which measures the rela-tive frequency of the two words cooccurring.
Specif-ically, let wij be the weight of the edge4 connecting2Although scale-free graphs are not necessarily smallworlds, a lot of real world networks are both scale-free andsmall worlds.3Following Ve?ronis, we only work on nouns for the timebeing.4Note that the cooccurrence graph is undirected, i.e.
wij =wji90nodes i and j, thenwij = 1?
max[P (i | j), P (j | i)]P (i | j) =freqijfreqjand P (j | i) =freqijfreqiThe weight of an edge measures how tightly con-nected the two words are.
Words which always oc-cur together receive a weight of 0.
Words rarelycooccurring receive weights close to 1.Once the cooccurrence graph is built, a simple it-erative algorithm is executed to obtain its hubs.
Ateach step, the algorithm finds the vertex with high-est relative frequency5 in the graph, and, if it meetssome criteria, it is selected as a hub.
These criteriaare determined by a set of heuristic parameters, thatwill be explained later in Section 4.
After a vertex isselected to be a hub, its neighbors are no longer eli-gible as hub candidates.
At any time, if the next ver-tex candidate has a relative frequency below a cer-tain threshold, the algorithm stops.Once the hubs are selected, each of them is linkedto the target word with edges weighting 0, and theMinimum Spanning Tree (MST) of the whole graphis calculated and stored.The MST is then used to perform word sense dis-ambiguation, in the following way.
For every in-stance of the target word, the words surrounding itare examined and confronted with the MST.
By con-struction of the MST, words in it are placed underexactly one hub.
Each word in the context receivesa set of scores s, with one score per hub, where allscores are 0 except the one corresponding to the hubwhere it is placed.
If the scores are organized in ascore vector, all values are 0, except, say, the i-thcomponent, which receives a score d(hi, v), whichis the distance between the hub hi and the node rep-resenting the word v. Thus, d(hi, v) assigns a scoreof 1 to hubs and the score decreases as the nodesmove away from the hub in the tree.For a given occurrence of the target word, thescore vectors of all the words in the context areadded, and the hub that receives the maximum scoreis chosen.5In cooccurrence graphs, the relative frequency of a vertexand its degree are linearly related, and it is therefore possible toavoid the costly computation of the degree.BasecorpushyperLex_wsd hyperLex_wsdhyperLexEvaluatorTaggedcorpusTestcorpusMappingcorpusMSTmatrixMappingFigure 1: Design for the automatic mapping and evaluationof HyperLex algorithm against a gold standard (test corpora).3 Evaluating unsupervised WSD systemsAll unsupervised WSD algorithms need some addi-tion in order to be evaluated.
One alternative, as in(Ve?ronis, 2004), is to manually decide the correct-ness of the hubs assigned to each occurrence of thewords.
This approach has two main disadvantages.First, it is expensive to manually verify each occur-rence of the word, and different runs of the algo-rithm need to be evaluated in turn.
Second, it is notan easy task to manually decide if an occurrence ofa word effectively corresponds with the use of theword the assigned hub refers to, especially consid-ering that the person is given a short list of wordslinked to the hub.
We also think that instead of judg-ing whether the hub returned by the algorithm is cor-rect, the person should have independently taggedthe occurrence with hubs, which should have beenthen compared to the hub returned by the system.A second alternative is to evaluate the system ac-cording to some performance in an application, e.g.information retrieval (Schu?tze, 1998).
This is a veryattractive idea, but requires expensive system devel-opment and it is sometimes difficult to separate thereasons for the good (or bad) performance.A third alternative would be to devise a methodto map the hubs (clusters) returned by the systemto the senses in a lexicon.
Pantel and Lin (2002)automatically map the senses to WordNet, and thenmeasure the quality of the mapping.
More recently,the mapping has been used to test the system onpublicly available benchmarks (Purandare and Ped-91Default p180 p1800 p6700value Range Best Range Best Range Bestp1 5 2-3 2 1-3 2 1-3 1p2 10 3-4 3 2-4 3 2-4 3p3 0.9 0.7-0.9 0.7 0.5-0.7 0.5 0.3-0.7 0.4p4 4 4 4 4 4 4 4p5 6 6-7 6 3-7 3 1-7 1p6 0.8 0.5-0.8 0.6 0.4-0.8 0.7 0.6-0.95 0.95p7 0.001 0.0005-0.001 0.0009 0.0005-0.001 0.0009 0.0009-0.003 0.001Table 1: Parameters of the HyperLex algorithmersen, 2004; Niu et al, 2005).
See Section 6 formore details on these systems.Yet another possibility is to evaluate the inducedsenses against a gold standard as a clustering task.Induced senses are clusters, gold standard senses areclasses, and measures from the clustering literaturelike entropy or purity can be used.
As we wanted tofocus on the comparison against a standard data-set,we decided to leave aside this otherwise interestingoption.In this section we present a framework for au-tomatically evaluating unsupervised WSD systemsagainst publicly available hand-tagged corpora.
Theframework uses three data sets, called Base corpus,Mapping corpus and Test corpus:?
The Base Corpus: a collection of examples ofthe target word.
The corpus is not annotated.?
The Mapping Corpus: a collection of examplesof the target word, where each corpus has beenmanually annotated with its sense.?
The Test Corpus: a separate collection, also an-notated with senses.The evaluation framework is depicted in Figure 1.The first step is to execute the HyperLex algorithmover the Base corpus in order to obtain the hubs ofa target word, and the generated MST is stored.
Asstated before, the Base Corpus is not tagged, so thebuilding of the MST is completely unsupervised.In a second step (left part in Figure 1), we assign ahub score vector to each of the occurrences of targetword in the Mapping corpus, using the MST calcu-lated in the previous step (following the WSD al-gorithm in Section 2.2).
Using the hand-annotatedsense information, we can compute a mapping ma-trix M that relates hubs and senses in the followingway.
Suppose there are m hubs and n senses for thetarget word.
Then, M = {mij} 1 ?
i ?
m, 1 ?j ?
n, and each mij = P (sj |hi), that is, mij is theprobability of a word having sense j given that it hasbeen assigned hub i.
This probability can be com-puted counting the times an occurrence with sensesj has been assigned hub hi.This mapping matrix will be used to transformany hub score vector h?
= (h1, .
.
.
, hm) returnedby the WSD algorithm into a sense score vectors?
= (s1, .
.
.
, sn).
It suffices to multiply the scorevector by M , i.e., s?
= h?M .In the last step (right part in Figure 1), we applythe WSD algorithm over the Test corpus, using againthe MST generated in the first step, and returning ahub score vector for each occurrence of the targetword in the test corpus.
We then run the Evaluator,which uses the M mapping matrix in order to con-vert the hub score vector into a sense score vector.The Evaluator then compares the sense with high-est weight in the sense score vector to the sense thatwas manually assigned, and outputs the precisionfigures.Preliminary experiments showed that, similar toother unsupervised systems, HyperLex performsbetter if it sees the test examples when building thegraph.
We therefore decided to include a copy of thetraining and test corpora in the base corpus (discard-ing all hand-tagged sense information, of course).Given the high efficiency of the algorithm this posesno practical problem (see efficiency figures in Sec-tion 6).4 Tuning the parametersAs stated before, the behavior of the HyperLex algo-rithm is influenced by a set of heuristic parameters,that affect the way the cooccurrence graph is built,the number of induced hubs, and the way they areextracted from the graph.
There are 7 parameters intotal:p1 Minimum frequency of edges (occurrences)p2 Minimum frequency of vertices (words)p3 Edges with weights above this value are removedp4 Context containing fewer words are not processed92word train test MFS default p180 p1800 p6700argument 221 111 51.4 51.4 51.4 51.4 51.4arm 266 133 82.0 82.0 80.5 82.0 82.7atmosphere 161 81 66.7 67.9 70.4 70.4 67.9audience 200 100 67.0 69.0 71.0 74.0 77.0bank 262 132 67.4 69.7 75.0 76.5 75.0degree 256 128 60.9 60.9 60.9 62.5 63.3difference 226 114 40.4 40.4 41.2 46.5 49.1difficulty 46 23 17.4 30.4 30.4 39.1 26.1disc 200 100 38.0 66.0 75.0 70.0 76.0image 146 74 36.5 63.5 62.2 67.6 64.9interest 185 93 41.9 49.5 41.9 47.3 51.6judgment 62 32 28.1 28.1 28.1 53.1 50.0organization 112 56 73.2 73.2 73.2 71.4 73.2paper 232 117 25.6 42.7 39.3 47.9 53.8party 230 116 62.1 67.2 64.7 65.5 67.2performance 172 87 32.2 44.8 46.0 54.0 59.8plan 166 84 82.1 81.0 79.8 81.0 83.3shelter 196 98 44.9 45.9 49.0 48.0 54.1sort 190 96 65.6 64.6 64.6 65.6 64.6source 64 32 65.6 59.4 56.2 62.5 62.5Average: 54.5 59.9 60.3 63.0 64.6(Over S2LS) 51.9 56.2 57.5 58.7 60.0Table 2: Precision figures for nouns over the test corpus (S3LS).
The second and third columns show the number of occurrencesin the train and test splits.
The MFS column corresponds to the most frequent sense.
The rest of columns correspond to differentparameter settings: default for the default setting, p180 for the best combination over 180, etc..
The last rows show the micro-average over the S3LS run, and we also add the results on the S2LS dataset (different sets of nouns) to confirm that the same trendshold in both datasets.p5 Minimum number of adjacent vertices a hub must havep6 Max.
mean weight of the adjacent vertices of a hubp7 Minimum frequency of hubsTable 1 lists the parameters of the HyperLex al-gorithm, and the default values proposed for them inthe original work (second column).Given that we have devised a method to efficientlyevaluate the performance of HyperLex, we are ableto tune the parameters against the gold standard.
Wefirst set a range for each of the parameters, and eval-uated the algorithm for each combination of the pa-rameters on a collection of examples of differentwords (Senseval 2 English lexical-sample, S2LS).This ensures that the chosen parameter set is validfor any noun, and is not overfitted to a small set ofnouns.6 The set of parameters that obtained the bestresults in the S2LS run is then selected to be runagainst the S3LS dataset.We first devised ranges for parameters amountingto 180 possible combinations (p180 column in Ta-ble 2), and then extended the ranges to amount to1800 and 6700 combinations (columns p1800 andp6700).6In fact, previous experiments showed that optimizing theparameters for each word did not yield better results.5 Experiment setting and resultsTo evaluate the HyperLex algorithm in a standardbenchmark, we applied it to the 20 nouns in S3LS.We use the standard training-test split.
Followingthe design in Section 3, we used both the trainingand test sets as the Base Corpus (ignoring the sensetags, of course).
The Mapping Corpus comprisedthe training split only, and the Test corpus the testsplit only.
The parameter tuning was done in a simi-lar fashion, but on the S2LS dataset.In Table 2 we can see the number of examplesof each word in the different corpus and the resultsof the algorithm.
We indicate only precision, as thecoverage is 100% in all cases.
The left column,named MFS, shows the precision when always as-signing the most frequent sense (relative to the trainsplit).
This is the baseline of our algorithm as ouralgorithm does see the tags in the mapping step (seeSection 6 for further comments on this issue).The default column shows the results for the Hy-perLex algorithm with the default parameters as setby Ve?ronis, except for the minimum frequency ofthe vertices (p2 in Table 1), which according to somepreliminary experiments we set to 3.
As we can see,the algorithm with the default settings outperforms930.550.560.570.580.590.60.610.620.5  0.55  0.6  0.65  0.7  0.75  0.8  0.85  0.9  0.95  1PrecisionSimilarityParameter spaceBest fitting lineFigure 2: Dispersion plot of the parameter space for 6700combinations.
The horizontal axis shows the similarity of a pa-rameter set w.r.t.
the best parameter set using the cosine.
Thevertical axis shows the precision in S2LS.
The best fitting lineis also depicted.the MFS baseline by 5.4 points average, and in al-most all words (except plan, sort and source).The results for the best of 180 combinations of theparameters improve the default setting (0.4 overall),Extending the parameter space to 1800 and 6700 im-proves the precision up to 63.0 and 64.6, 10.1 overthe MFS (MFS only outperforms HyperLex in thebest setting for two words).
The same trend can beseen on the S2LS dataset, where the gain was moremodest (note that the parameters were optimized forS2LS).6 Discussion and related workWe first comment the results, doing some analysis,and then compare our results to those of Ve?ronis.
Fi-nally we overview some relevant work and reviewthe results of unsupervised systems on the S3LSbenchmark.6.1 Comments on the resultsThe results show clearly that our exploration of theparameter space was successful, with the widest pa-rameter space showing the best results.In order to analyze whether the search in the pa-rameter space was making any sense, we drew a dis-persion plot (see Figure 2).
In the top right-hand cor-ner we have the point corresponding to the best per-forming parameter set.
If the parameters were notconditioning the good results, then we would haveexpected a random cloud of points.
On the contrary,we can see that there is a clear tendency for thosedefault p180 p1800 p6700hubs defined 9.2 ?3.8 15.3 ?5.7 38.6 ?11.8 77.7?18.7used 8.4 ?3.5 14.4 ?5.3 30.4 ?9.3 45.2?13.3senses defined 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5 5.4 ?1.5used 2.6 ?1.2 2.5 ?1 3.1 ?1.1 3.2?1.2senses in test 5.1 ?1.3 - - -Table 3: Average number of hubs and senses (along with thestandard deviation) for three parameter settings.
Defined meansthe number of hubs induced, and used means the ones actuallyreturned by HyperLex when disambiguating the test set.
Thesame applies for senses, that is, defined means total number ofsenses (equal for all columns), and used means the senses thatwere actually used by HyperLex in the test set.
The last rowshows the actual number of senses used by the hand-annotatorsin the test set.parameter sets most similar to the best one to obtainbetter results, and in fact the best fitting line shows aclearly ascending slope.Regarding efficiency, our implementation of Hy-perLex is extremely fast.
Doing the 1800 combina-tions takes 2 hours in a 2 AMD Opteron processorsat 2GHz and 3Gb RAM.
A single run (building theMST, mapping and tagging the test sentences) takesonly 16 sec.
For this reason, even if an on-line ver-sion would be in principle desirable, we think thatthis batch version is readily usable.6.2 Comparison to (Ve?ronis, 2004)Compared to Ve?ronis we are inducing larger num-bers of hubs (with different parameters), using lessexamples to build the graphs and obtaining moremodest results (far from the 90?s).
Regarding the lat-ter, our results are in the range of other S3LS WSDsystems (see below), and the discrepancy can be ex-plained by the way Ve?ronis performed his evaluation(see Section 3).Table 3 shows the average number of hubs forthe four parameter settings.
The average numberof hubs for the default setting is larger than that ofVe?ronis (which ranges between 4 and 9 per word),but quite close to the average number of senses.
Theexploration of the parameter space prefers parame-ter settings with even larger number of hubs, and thefigures shows that most of them are actually usedfor disambiguation.
The table also shows that, afterthe mapping, less than half of the senses are actu-ally used, which seems to indicate that the mappingtends to favor the most frequent senses.Regarding the actual values of the parametersused (c.f.
Table 1), we had to reduce the value94of some parameters (e.g.
the minimum frequencyof vertices) due to the smaller number of of exam-ples (Ve?ronis used from 1900 to 8700 examples perword).
In theory, we could explore larger parame-ter spaces, but Table 1 shoes that the best setting forthe 6700 combinations has no parameter in a rangeboundary (except p5, which cannot be further re-duced).All in all, the best results are attained with smallerand more numerous hubs, a kind of micro-senses.A possible explanation for this discrepancy withVe?ronis could be that he was inspecting by handthe hubs that he got, and perhaps was biased by thefact that he wanted the hubs to look more like stan-dard senses.
At first we were uncomfortable withthis behavior, so we checked whether HyperLex wasdegenerating into a trivial solution.
We simulateda clustering algorithm returning one hub per exam-ple, and its precision was 40.1, well below the MFSbaseline.
We also realized that our results are inaccordance with some theories of word meaning,e.g.
the ?indefinitely large set of prototypes-within-prototypes?
envisioned in (Cruse, 2000).
We nowthink that the idea of having many micro-senses isvery attractive for further exploration, especially ifwe are able to organize them into coarser hubs.6.3 Comparison to related workTable 4 shows the performance of different systemson the nouns of the S3LS benchmark.
When not re-ported separately, we obtained the results for nounsrunning the official scorer program on the filteredresults, as available in the S3LS web page.
The sec-ond column shows the type of system (supervised,unsupervised).We include three supervised systems, the winnerof S3LS (Mihalcea et al, 2004), an in-house system(kNN-all, CITATION OMITTED) which uses opti-mized kNN, and the same in-house system restrictedto bag-of-words features only (kNN-bow), i.e.
dis-carding other local features like bigrams or trigrams(which is what most unsupervised systems do).
Thetable shows that we are one point from the bag-of-words classifier kNN-bow, which is an impressiveresult if we take into account the information loss ofthe mapping step and that we tuned our parameterson a different set of words.
The full kNN system isstate-of-the-art, only 4 points below the S3LS win-System Type Prec.
Cov.S3LS-best Sup.
74.9 0.99kNN-all Sup.
70.3 1.0kNN-bow Sup.
65.7 1.0HyperLex Unsup(S3LS) 64.6 1.0Cymfony Unsup(10%-S3LS) 57.9 1.0Prob0 Unsup.
(MFS-S3) 55.0 0.98MFS - 51.5 1.0Ciaosenso Unsup (MFS-Sc) 53.95 0.90clr04 Unsup (MFS-Sc) 48.86 1.0duluth-senserelate Unsup 47.48 1.0(Purandare andPedersen, 2004)Unsup (S2LS) - -Table 4: Comparison of HyperLex and MFS baseline to S3LSsystems for nouns.
The last system was evaluated on S2LS.ner.Table 4 also shows several unsupervised systems,all of which except Cymfony and (Purandare andPedersen, 2004) participated in S3LS (check (Mi-halcea et al, 2004) for further details on the sys-tems).
We classify them according to the amount of?supervision?
they have: some have have access tomost-frequent information (MFS-S3 if counted overS3LS, MFS-Sc if counted over SemCor), some use10% of the S3LS training part for mapping (10%-S3LS), and some use the full amount of S3LS train-ing for mapping (S3LS).
Only one system (Duluth)did not use in any way hand-tagged corpora.Given the different typology of unsupervised sys-tems, it?s unfair to draw definitive conclusions froma raw comparison of results.
The system comingcloser to ours is that described in (Niu et al, 2005).They use hand tagged corpora which does not needto include the target word to tune the parameters ofa rather complex clustering method which does uselocal information (an exception to the rule of unsu-pervised systems).
They do use the S3LS trainingcorpus for mapping.
For every sense the target word,three of its contexts in the train corpus are gathered(around 10% of the training data) and tagged.
Eachcluster is then related with its most frequent sense.Only one cluster may be related to a specific sense,so if two or more clusters map to the same sense,only the largest of them is retained.
The mappingmethod is similar to ours, but we use all the avail-able training data and allow for different hubs to beassigned to the same sense.Another system similar to ours is (Purandare andPedersen, 2004), which unfortunately was evaluatedon Senseval 2 data.
The authors use first and second95order bag-of-word context features to represent eachinstance of the corpus.
They apply several clusteringalgorithms based on the vector space model, limitingthe number of clusters to 7.
They also use all avail-able training data for mapping, but given their smallnumber of clusters they opt for a one-to-one map-ping which maximizes the assignment and discardsthe less frequent clusters.
They also discard somedifficult cases, like senses and words with low fre-quencies (10% of total occurrences and 90, respec-tively).
The different test set and mapping systemmake the comparison difficult, but the fact that thebest of their combinations beats MFS by 1 point onaverage (47.6% vs. 46.4%) for the selected nounsand senses make us think that our results are morerobust (nearly 10% over MFS).7 Conclusions and further workThis paper has explored two sides of HyperLex: theoptimization of the free parameters, and the empir-ical comparison on a standard benchmark againstother WSD systems.
We use hand-tagged corporato map the induced senses to WordNet senses.Regarding the optimization of parameters, weused a another testbed (S2LS) comprising differentwords to select the best parameter.
We consistentlyimprove the results of the parameters by Ve?ronis,which is not perhaps so surprising, but the methodallows to fine-tune the parameters automatically to agiven corpus given a small test set.Comparing unsupervised systems against super-vised systems is seldom done.
Our results indicatethat HyperLex with the supervised mapping is onpar with a state-of-the-art system which uses bag-of-words features only.
Given the information lossinherent to any mapping, this is an impressive re-sult.
The comparison to other unsupervised systemsis difficult, as each one uses a different mappingstrategy and a different amount of supervision.For the future, we would like to look more closelythe micro-senses induced by HyperLex, and see ifwe can group them into coarser clusters.
We alsoplan to apply the parameters to the Senseval 3 all-words task, which seems well fit for HyperLex: thebest supervised system only outperforms MFS bya few points in this setting, and the training cor-pora used (Semcor) is not related to the test corpora(mainly Wall Street Journal texts).Graph models have been very successful in somesettings (e.g.
the PageRank algorithm of Google),and have been rediscovered recently for natural lan-guage tasks like knowledge-based WSD, textual en-tailment, summarization and dependency parsing.We would like to test other such algorithms in thesame conditions, and explore their potential to inte-grate different kinds of information, especially thelocal or syntactic features so successfully used bysupervised systems, but also more heterogeneous in-formation from knowledge bases.ReferencesA.
L. Barabasi and R. Albert.
1999.
Emergence of scal-ing in random networks.
Science, 286(5439):509?512,October.D.
A. Cruse, 2000.
Polysemy: Theoretical and Com-putational Approaches, chapter Aspects of the Micro-structure of Word Meanings.
OUP.C.
Fellbaum.
1998.
WordNet: An Electronic LexicalDatabase.
MIT Press.R.
Mihalcea, T. Chklovski, and A. Kilgarriff.
2004.
Thesenseval-3 english lexical sample task.
In R. Mihal-cea and P. Edmonds, editors, Senseval-3 proceedings,pages 25?28.
ACL, July.G.A.
Miller, C. Leacock, R. Tengi, and R.Bunker.
1993.A semantic concordance.
In Proc.
of the ARPA HLTworkshop.C.
Niu, W. Li, R. K. Srihari, and H. Li.
2005.
Wordindependent context pair classification model for wordsense disambiguation.
In Proc.
of CoNLL-2005.P.
Pantel and D. Lin.
2002.
Discovering word sensesfrom text.
In Proc.
of KDD02.A.
Purandare and T. Pedersen.
2004.
Word sense dis-crimination by clustering contexts in vector and simi-larity spaces.
In Proc.
of CoNLL-2004.H.
Schu?tze.
1998.
Automatic word sense discrimination.Computational Linguistics, 24(1):97?123.B.
Snyder and M. Palmer.
2004.
The english all-wordstask.
In Proc.
of SENSEVAL.J.
Ve?ronis.
2004.
HyperLex: lexical cartography for in-formation retrieval.
Computer Speech & Language,18(3):223?252.D.
J. Watts and S. H. Strogatz.
1998.
Collec-tive dynamics of ?small-world?
networks.
Nature,393(6684):440?442, June.96
