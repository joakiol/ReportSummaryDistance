The Measure  o f  a Mode l  *Rebecca  Brucet ,  danyce  Wiebe~:,  Ted  PedersenttDepar tment  of Computer  Science and EngineeringSouthern Methodist  UniversityDallas, TX 75275-0112and:~Department of Computer  ScienceNew Mexico State UniversityLos Cruces, NM 88003rbruce~seas.smu.edu,  wiebe@cs.nmsu.edu, pedersen@seas.smu.eduAbstractThis paper describes measures for evaluating thethree determinants of how well a probabilistic elas-sifter performs on a given test set.
These determi-nants are the appropriateness, for the test set, ofthe results of (1)feature selection, (2) formulationof the parametric form of the model, and (3) pa-rameter estimation.
These are part of any modelformulation procedure, even if not broken out asseparate steps, so the tradeoffs explored in thispaper are relevant o a wide variety of methods.The measures are demonstrated in a large experi-ment, in which they are used to analyze the resultsof roughly 300 classifiers that perform word-sensedisambiguation.IntroductionThis paper presents techniques that can be usedto analyze the formulation of a probabilistic clas-sifter.
As part of this presentation, we applythese techniques to the results of a large num-ber of classifiers, developed using the method-ology presented in (2), (3), (4), (5), (12) and(16), which tag words according to their meanings(i.e., that perform word-sense disambiguation).Other NLP tasks that have been performed usingprobabilistic classifiers include part-of-speech tag-ging (11), assignment of semantic lasses (8), cuephrase identification (9), prepositional phrase at-tachment (15), other grammatical disambiguationtasks (6), anaphora resolution (7) and even trans-lation equivalence (1).
In fact, it could be arguedthat any problem with a known set of possible so-lutions can be cast as a classification problem.A probabilistic lassifier assigns, out of a setof possible classes, the one that is most probableaccording to a probabilistic model.
The model ex-presses the relationships among the classificationvariable (the variable representing the classifica-tion tag) and var\]ables that correspond to prop-*This research Was supported by the Office of NavalResearch under grant number N00014-95-1-0776.erties of the ambiguous object and the context inwhich it occurs (the non-classification variables).Each model uniquely defines a classifier.The basic premise of a probabilistic approachto classification is that the process of assigning ob-ject classes is non-deterministic, i.e., there is no in-fallible indicator of the correct classification.
Thepurpose of a probabilistic model is to characterizethe uncertainty in the classification process.
Theprobabilistie model defines, for each class and eachambiguous object, the probability that the objectbelongs to that class, given the values of the non-classification variables.The main steps in developing a probabilisticclassifier and performing classification on the basisof a probability model are the following.
11.
Feature Selection: selecting informativecontextual features.
These are the properties ofthe ambiguous object and the context in whichit occurs that are indicative of its classification.Typically, each feature is represented asa randomvariable (a non-classification variable) in the prob-abilistic model.
Here we will use Fi to designate arandom variable that corresponds to the ith con-textual feature, and fl to designate the value ofFi.
The contextual features play a very importantrole in the performance ofa model.
They are therepresentation f context in the model, and it is onthe basis of them that we must distinguish amongthe classes of objects.2.
Selection of the parametr ic  form ofthe model.
The form of the model expressesthe joint distribution of all variables as a func-tion of the values of a set of unknown parameters.Therefore, the parametric form of a model spec-ifies a family of distributions.
Each member ofthat family corresponds to a different set of valuesfor the unknown parameters.
The form of a model1Although these are always involved in developingprobabilistic lassifiers, they may not be broken outinto three separate steps in a particular method; anexample is decision tree induction (14).101specifies the stochastic relationships, the interde-pendencies, that exist among the variables.
Theparameters define the distributions of the sets ofinterdependent variables, i.e., the probabilities ofthe various combinations of the values of the inter-dependent variables.
As an illustration, considerthe following three parametric forms, each spec-ifying different sets of interdependencies amongvariables in describing the joint distribution ofa classification variable, Tag, and a set of non-classification variables, F1 through Fn.
In theequations below, tag represents the value of theclassification variable and the fi's denote the val-ues of the non-classification variables.The model for interdependence among all vari-ables:V tag, f l ,  f2 , .
.
.
,  fn P(tag, f l ,  f2 .
.
.
.
, fn) =P(tag, f l , f2 , .
.
.
, fn )  (1)The model for conditional independenceamong all non-classification variables given thevalue of the classification variable:V tag, f l , f2 , .
.
.
, fn  P(tag, f l , f2  .
.
.
.
, fn)  =P(f l l tag) x .. .
x P(fnltag) x P(tag) (2)The model for independence among all vari-ables:V tag, f l , f~ , .
.
.
, fn  P ( tag , f l , f2 , .
.
.
, fn )  =P(tag) x P( f l )  x P(f2) x .
.
.
x P( fn)  (3)The objective in defining the parametric form ofa model is to describe the relationships among allvariables in terms of only the most important in-terdependencies.
While it is always true that allvariables can be treated as interdependent (equa-tion 1), if there are several features, such a modelcould have too many parameters to estimate inpractice.
The greater the number of interdepen-deneies expressed in a model the more complex themodel is said to be.3.
Est imat ion of  the model  parametersf rom the training data.
While the form of amodel identifies the relationships among the vari-ables, the parameters express the uncertainty in-herent in those relationships.
Recall that the pa-rameters of a model describe the distributions ofthe sets of interdependent variables by defining thelikelihood of seeing each combination of the valuesof those variables.
For example, the parameters ofthe model for independence are the following:V tag, f l , f~ , .
.
.
, f r ,  :P(tag), P( f l ) ,  P(f2) .
.
.
.
, P(f,~)There are no interdependencies n the model forindependence, so the parameters describe the dis-tributions of the individual variables.102In the model for conditional independencestated in equation 2, the parameters are as fol-lows:V tag, f l , f2 , .
.
.
, fn  :P(f l l tag) .
.
.
.
, P(fnltag), P(tag)Each parameter in this model describes the dis-tribution of the tag in combination with a singlecontextual feature.The parameters of any model are eslima~edif their values are based on functions of a datasample (i.e., statistics) as opposed to properties ofthe population.4.
Assessment  of  the  l ike l ihood of  eachtag: use of the completed model to compute theprobability of assigning each tag to the ambiguousobject, given the values of the non-classificationvariables.
This probability function is the follow-ing conditional or context-specific distribution oftags, where the f i 's now denote the values assumedby the non-classification variables in the specificcontext being considered.V ~ag P(taglf l  , f2, f3 , .
.
.
,  fn) (4)5.
Ambigu i ty  reso lut ion :  assignment, tothe ambiguous object, of the tag with the high-est probability of having occurred in combinationwith the known values of the non-classificationvariables.
This assignment is based on the fol-lowing function (where t~g is the value assigned):ar  grnaz:tag = tag P ( tag l f l , f2 , f3 , .
.
.
, fn )  (5)In most cases, 2 the process of applying a prob-abilistic model to classification (i.e., steps (4) and(5) above) is straightforward.
The focus of thiswork is on formulating a probabilistic model (steps(1)-(3)); these steps are crucial to the success ofany probabilistic lassifier.
We describe measuresthat can be used to evaluate the effect of each ofthese three steps on classifier performance.
Usingthese measures, we demonstrate hat it is possibleto analyze the contribution of each step as wellas the interdependencies that exist between thesesteps.The remainder of this paper is organized asfollows.
The first section is a description of theexperimental setup used for the investigations per-formed in this paper.
Next, the evaluation mea-sures that we propose are presented, followed by adiscussion of the results and finally a presentationof our conclusions.2When the values of all non-classification variablesare known and there are no interdependent ambigui-ties among the classes.IThe Exper imenta l  SetupIn this paper, we analyze the performance of clas-sifters developed for the disambiguation of twelvedifferent words.
For each of these words, we de-velop a range of classifiers based on models of vary-ing complexity.
Our purpose is to study the con-tribution that each of feature selection, selectionof the form of a model, and parameter estima-tion makes to overall model performance.
In thissection, we describe the basic experimental setupused in these evaluations, in particular, the pro-tocol used in the disambiguation experiments andthe procedure used to formulate ach model.P ro toco l  fo r  the  D isambiguat ionExper imentsThere are three parameters that define a word-sense disambiguation experiment: (1 ) the  choiceof words and word meanings (their number andtype), (2) the method used to identify the "cor-rect" word meaning, and (3) the choice of textfrom which the data is taken.
In these experi-ments, the complete set of non-idiomatic sensesdefined in the Longman's Dictionary of Contem-porary English (LDOCE) (13) is used as the tagset for each word to be disambiguated.
For eachuse of a targeted word, the best tag, from amongthe set of LDOCE sense tags, is determined by ahuman judge.
The tag assigned by the classifier isaccepted as correct only when it is identical to thetag pre-selected by the human judge.All data used in these experiments are takenfrom the Penn Treebank Wall Street Journal cor-pus (10).
This corpus was selected because of itsavailability and size.
Further, the POS categoriesassigned in the Penn 'IYeebank corpus are used toresolve syntactic ambiguity so that word-meaningdisambiguation ccurs only after the syntactic at-egory of a word has been identified.The following words were selected for disam-biguation based on their relatively high frequencyof occurrence and the appropriateness of theirsense distinctions for the textual domain.
* Nouns: interest, bill, concern, and drug.?
Verbs: close, help, agree, and include.?
Adjectives: chief, public, last, and common.Because word senses from a particular dictionaryare used, the degree of ambiguity for each word isfixed, and the overall evel of ambiguity addressedby the experiment is determined by this selectionof words.
For each of these words, the sense tagsand their distributions in the data are presentedin Tables 1 through 3.103Noun senses of interest: (total count: 2368)1 "readiness to give attention": 15%2 "quality of causing attention tobe given" : <1~3 "activity, subject, etc., which onegives time and attention to": 3~4 "advantage, advancement, or favor": 8~5 "a share in a company, business, etc.
": 21~6 "money paid for the use of money" : 53?~Noun senses of concern: (total count: 1488)1 "a matter that is of interest orimportance": 3~2 "serious care or interest": 2~3 "worry; anxiety": 32~4 "a business; firm" : 64~Noun senses of bill: (total count: 1335)1 "a plan for a law, written down forthe government to consider": 69~2 "a list of things bought and theirprice": 10g4 "a piece of paper money" (extendedto include treasury bills): 21~Noun senses of drug: (total count: 1217)1 "a medicine or material used formaking medicines": 58~2 "a habit-forming substance": 42~Table 1: Data summary-  Nouns.Feature SelectionFor simplicity, the contextual features used in allmodels were selected per the following schema.
Allmodels developed for each of the 12 words incorpo-rate the following types of contextual features: onemorphological feature, three collocation-specificfeatures, and four class-based features, with POScategories serving as the word classes.
All modelsdeveloped for the same word (which are models ofvarying complexity) contain the same features.The morphological feature describes only thesuffix of the base lexeme of the word to be dis-ambiguated: the presence or absence of the plu-ral form, in the case of the nouns, and the suffixindicating tense, in the case of the verbs; the ad-jectives have no morphological feature under thisdefinition.The values of the class-based variables are aset of 25 POS tags derived from the first letter ofthe tags used in the Penn Treebank corpus.
Eachmodel contains four variables representing class-based contextual features: the POS tags of the twowords immediately preceding and the two wordsimmediately succeeding the ambiguous word.
Allvariables are confined to sentence boundaries; ex-tension beyond the sentence boundary is indicatedby a null POS tag (e.g., when the ambiguous wordVerb senses of close: (total count: 1534)1 "to (cause to) shut": 2%2 "to (cause to) be not open tothe public": 2~3 "to (cause to) stop operation": 20~4 "to (cause to) end": 68~6 "to (cause to) come together bymaking less space between": 2~7 "to close a deal" (extended froman idiomatic usage): 6~Verb senses of agree: (total count: 1356)1 "to accept an idea, opinion, etc., esp.after unwillingness or argument": 78~2 "to have or share the same opinion,feeling, or purpose": 22?~3 "to be happy together;get on well together": <1~Verb senses of include: (total count: 1558)1 "to have as a part; contain inaddition to other parts": 91~2 "to put in with something else -human subject": 9%Verb senses of help: (total count: 1398)1 "to do part of the work for -human object": 20%2 "to encourage, improve, or producefavorable conditions for -inanimate object": 75~3 "to make better human object":4 "to avoid; prevent; change -inanimate object": 1Table 2: Data summary - Verbs.Adjective senses of common:(total count:l 111) |1 "belonging to or shared equally /" 7~1by2ormore  :2 "found or happening often andin many places; usual": 83 "widely known; general; ordinary": 3%4 "of no special quality; ordinary": 1%6 "technical, having the samerelationship to 2 or morequantities" : < 1%7 "as in the phrase 'common stock' "(not in LDOCE): 80%Adjective senses of last: (total count: 3180)1 "after all others": 6%2 "on the occasion nearest in the past": 93%3 "least desirable (not in LDOCE)": <1%Adjective senses of chief.
(total count: 1036) I86% 1 "highest in rank":2 "most important; main": 14~Adjective senses of public: (total count: 867)1 "of, to, by, for, or concerningpeople in general": 56~2 "for the use of everyone; not private": 8~3 "in the sight or hearing of manypeople; not secret or private 11~4 "known to all or to many": 3~5 "connected or concerned with theaffairs of the people,esp.
with government": 1656 "(of a company) to become apublic company" (extendedfrom an idiomatic usage): 6~7 "as in the phrase 'public TV'or 'public radio"' (not in LDOCE): 1~Table 3: Data summary - Adjectives.appears at the start of the sentence, the POS tagsto the left have the value null).Three collocation-specific variables are in-cluded in each model, where the term collocationis used loosely to refer to a specific spelling formoccurring in the same sentence as the ambigu-ous word.
While collocation-specific variables are,by definition, specific to the word being disam-biguated, the procedure used to select hem is gen-eral.
The search for collocation-specific variablesis limited to the 400 most frequent spelling formsin a data sample composed of sentences containingthe ambiguous word.
Out of those 400, the threespelling forms whose presence was found to be themost dependent on the value of the classificationvariable, using the test for independence d scribedin (12), were selected as the collocational variablesfor that word.104Formulat ion  o f  a Range o f  Parametr i cFormsTo support these experiments, for each word, arange of models of varying complexity were for-mulated, with each model defining a new classifier.To distinguish among these models, we introducea measure of model complexity: the total numberof pairwise interdependencies that are specified inthe model.
For each word, the model of maximalcomplexity is the model in which all variables areconsidered to be interdependent (equation 1).
Themodel of minimal complexity formulated for eachword is the model in which all non-classificationvariables are considered to be conditionally inde-pendent given the value of the classification vari-able (equation 2); this is the simplest model thatstill uses each non-classification variable in pre-dicting the value of the classification variable.The formulation of these models is conductedas a series of stepwise refinements, tarting withthe model of maximal complexity.
At each step, anew model is formulated from the current model asfollows (initially the current model is the startingmodel).
Each of the pairwise interdependencies nthe current model is evaluated, using a goodness-of-fit test.
The test used is an exact test (12) forevaluating the interdependency between two vari-ables, where two variables are interdependent ifthey are not conditionally (or fully) independent.The test determines the degree to which that in-terdependency is manifested in the training data.The new (less complex) model formulated is thecurrent model with the interdependency that isleast apparent in the training data removed.
Thenew model is used to classify the test data andthen serves as the current model in the next sim-plification step.
A more complete description ofthis procedure can be found in (2).Parameter  Es t imat ionIn these experiments, we use maximum-likelihoodestimates (M.?.
estimates) of the model parame-ters.
The theoretical motivation behind this ap-proach is intuitively appealing: the model param-eters are represented by the numerical values thatmaximize the probability of generating the train-ing data from a model of the specified form.
Theimplementation is straightforward.
For each set ofinterdependent variables in the model, the associ-ated parameters are the probabilities of the combi-nations of the values of those variables.
The esti-mates of those parameters are equal to the relativefrequencies with which those combinations occurin the training data.
The drawback is that the esti-mates of parameters corresponding to events thatoccur infrequently in the training data are not re-liable; for example, if an event is not observed inthe training data, then the estimated probabilityof that event is zero.Descr ip t ion  o f  Eva luat ion  MeasuresThis paper describes measures that can be usedto examine the appropriateness, for the test set,of the features used in a model, the parametricform of the model, and the parameter estimates.Figures 1-12 plot model complexity against a num-ber of model performance measures.
The gaps be-tween the overall classification performance of amodel (indicated as "Overall Model" in the fig-ures) and the other measures is variously due toerror introduced by the three factors under study.We first define all of the performance measuresshown in the figures, and then discuss what canbe concluded from the relationships among mea-sures.Below, a completed model is a model in whichthe features have been specified; the parametricform has been specified; and the parameters havebeen estimated.1.
Overa l l  Mode l  Per fo rmance .
Givena completed model in which the parameters havebeen estimated from the training data:the overall model performance is the percent-age of the test set tagged correctly by a classifierusing that model to tag the test set.Comments :  Other widely-used loss func-tions are entropy, cross-entropy, and squared er-ror.2.
Lower  Bound.
Let FT be the mostfrequently-occurring (correct) tag for a word inthe test set.
The lower bound for that word isthe percentage of the test set assigned tag FT.Comments :  The classification performanceof a probabilistie model should not be worse thanthat of the simplest model, the model for indepen-dence:V tag, f l , f2, .
.
.
, fn P(tag, f l , f2 , .
.
.
, fn )=P(tag) x P(ffl) x P(f2) ?
.
.
.x  P(fn) (6)Because the probability of seeing each value of theclassification variable (i.e., each tag) is indepen-dent of the context, this model assigns every ob-ject the most frequently occurring tag:argflrla~tag = taa e(taal/1, Y2, Y3, ..., Y,) =ar f fma~tag P(tag) (T)Therefore, the proportion of the test set belongingto the most frequently occurring tag establishesthe lower bound on model performance.
For ex-ample, if60% of the instances of the target word inthe test set have the same sense, say sense 1, thenthe lower bound for model performance is 60%.3.
Recal l .
Given a completed model inwhich the parameters have been estimated fromthe training data:Recall is the percentage of the test set that isassigned some tag (correct or not) by a classifierusing that model to tag the test set.Comments :  An ambiguous word in the testset is not assigned a tag when the parameter esti-mates characterizing its context are zero.
BecauseM.L.
parameter estimates are used, all combina-tions of variable values that are not observed inthe training data are not expected to occur (havezero probability).The percentage of the test set that is assigneda tag corresponds to the percentage of the combi-nations of variable values observed in the test setthat were also observed in the training data.4.
P rec is ion .
Given a completed model in105which the parameters have been estimated fromthe training data:Of the portion of the test set that is assignedsome tag by a classifier using that model to tag thetest set, precision is the percentage that is taggedcorrectly.Comments :  Equivalently, this measure is:1 - (recall - overal lModelPerformance) (8)We will use the term misclassification errorfor 1 -prec is ion ,  which is the gap between recalland overall model performance.5.
Appropr ia teness  o f  the  Parametr leForm for  the  Test  Set (or, the Measure  ofForm).
This measure is computed to be identi-cal to the overall model performance, except hatthe parameters are estimated from the test  data,rather than the training data.
That is, given acompleted model in which the parameters havebeen estimated from the test  data:The appropriateness of the parametric formfor the test set is the percentage of the test settagged correctly by a classifier using that modelto tag the test set.Comments :  Because the model is trainedand tested on the same data, the parameter es-timates are optimal for that data.
Thus, variationof this performance measure is due only to differ-ences in the parametric form of the model.6.
Appropr ia teness  o f  the  Feature  Setfor  the  Test  Set (or, the Measure  of  Feature -Set).
This is equal to the measure of form of themaximally-complex model (i.e., the model that in-dudes all possible interdependencies).Comments :  Recall that the measure of forminvolves a model that is both trained and testedon the test set.
When the model is maximallycomplex and the parameters are estimated fromthe same data that is being tagged, the modeldescribes the exact joint distribution apparent inthat data.
Suppose that, for each combination ofthe values of the non-classification variables thatoccurs in the test set, the tag is the same for alloccurrences (and is the correct one).
Then, thefeatures are perfect for the test set: each combi-nation of non-classification variables that occurs inthe test data uniquely determines the correct ag.In this case, the performance of the full model isnecessarily 100%.If the performance is not 100%, since themodel describes the exact joint distribution, thedegraded performance can only be due to the lackof complete discriminatory power of the features--i.e., there are combinations of feature values withwhich more than one tag occurs.
The incorrect an-swers are the less frequent ags in contexts where106there are multiple tags (see equation 4).Consider the gap between recall and overallmodel performance, i.e., misclassification error.This gap is the percentage of the objects taggedthat were tagged incorrectly.
The incorrectness idue to some combination of (1) the features beingimperfect, (2) the form being inadequate, and (3)the parameter estimates being inappropriate.
Inthe remainder of this paper, we will analyze thecontribution of each of these three factors, usingthe performance measures defined above.ResultsIn Figures 1 through 12 we use the measures de-scribed above to analyze the performance of a se-ries of models for each of the 12 words listed inthe section on experimental setup.
For each word,we formulate a range of models of varying com-plexity.
The model of maximal complexity is themodel in which all variables are considered to beinterdependent (equation 1).
The model of min-imal complexity that is formulated is the modelin which all non-classification variables are con-sidered to be conditionally independent given thevalue of the classification variable (equation 2).For each word to be disambiguated there isa figure depicting the various measures of modelperformance as a function of model complexity,where model complexity ranges from the maximalto the minimal model.Our purpose is to study the effect that each ofthe three facets of model formulation has on modelperformance.
By evaluating each facet indepen-dently we can gauge the impact that each has onthe overall performance of a classifier.
This is im-portant for many reasons, but here our primaryconcern is understanding the limitations of modelperformance.Using the measures described previously, weare able to demonstrate four main points regard-ing model formulation.
Note that all measuresused in establishing these claims are applied withrespect o some specific test set and therefore theresults are dependent on the characteristics of theparticular test set being used.The  feature  set  f ixes  the  upper  boundo f  mode l  per fo rmance .As discussed in item 6, if the feature set is idealfor the test set, then each context will uniquelycorrespond to a single tag.
In other words, the fea-ture set is an infallible indicator of the correct ag.When this is not the case (i.e., there are contextsin which two or more tags occur), then all but themost frequently occurring tag (for that context)will be misclassified, and there is nothing that canbe changed with regards to the parametric formor the parameter estimates to remedy this situa-tion.
Therefore the feature set establishes the up-per bound of model performance.
This is demon-strated in Figures 1 through 12.
It is interestingto note that for four of the words ("bill", "chief","include", and "concern") the feature set was op-timal for the test set (i.e., the measure of feature-set was 100%).
Even in the worst case, the errorintroduced by the lack of discriminatory power ofthe feature set did not exceed 8%.
Note that whenthe feature set is not optimal, the resulting erroraffects the precision of the model.
This can be ob-served by comparing the gap between recall andoverall model performance (the misclassificationerror, equivalent to 1 -prec is ion)  for models withrelatively large feature-related rror (such as themodels for "public") to that of models in which thefeatures are optimal, such as those for "bill" and"include".
When the feature set is optimal, it con-tributes nothing to misclassification error.
Whenthis is the case, misclassification error is strictly afunction of the appropriateness of the parametricform and the parameter estimates.
We considermeasures of these contributions next.As  the  complex i ty  o f  the  mode l  isreduced,  impor tant  in fo rmat ion  is lostf rom the  parametr i c  fo rm.The measure of the appropriateness of the para-metric form (the measure of form) is included inthe performance measures plotted in Figures 1through 12.
When the model is maximally com-plex, this measure indicates the quality of the fea-ture set, as discussed above.
As soon as the com-plexity of the model is reduced, the model formis no longer an exact expression of the distribu-tion apparent in the test set; assumptions of con-ditional independence have been introduced intothe model.
The process used to reduce modelcomplexity assures that each time an assumptionof conditional independence is made (i.e., an in-terdependency between two variables is removed),it is, at least in a local sense, the most appro-priate one to have selected based on an analysisof the training data.
In Figures 1 through 12 wesee that, up to a point, judicious selection of theconditional independence assumptions allows us toreduce model complexity without impacting ourability to characterize the distribution of tags inthe test set (i.e., starting from the right, the para-metric form remains flat for some time as com-plexity is decreased).
But, in all cases, as the pro-cess of reducing model complexity continues, themodel oses its ability to properly characterize thisdistribution.
This failure to properly characterizethe test set occurs when the interdependencies r -moved from the model are important in describingthe conditional distribution of the tags given thevalues of the non-classification variables (equation4).
The exact point at which this occurs variesin Figures 1 through 12, but the fact that it doesoccur is apparent in the drop-off of the measure ofform as well as in the increase in misclassificationerror that accompanies that drop.
In all figures, asthe measure of form drops, the gap between recalland overall model performance increases, indicat-ing the contribution that the inappropriateness ofmodel form makes to misclassification error.As the complexi ty  of  the model  isreduced,  the  qua l i ty  o f  the  parameterest imates improves .The final factor contributing to misclassificationerror is the quality of the parameter estimates.The gap between the measure of form and theoverall model performance is the error that re-sults from using parameter estimates made fromthe training data as opposed to using parametersthat exactly describe the characteristics of the testset (recall that the only difference between thesemeasures i whether the parameters are estimatedfrom the test set or from the training data).
Inall figures, this gap shrinks dramatically as thecomplexity of the model is reduced.
The decreasein this gap indicates that the quality of the pa-rameter estimates made from the training dataimproves as model complexity is reduced.
Simi-larly, this improvement is reflected in recall, whichalso increases as the complexity of the model is re-duced.The  qua l i ty  o f  the  non-zero  parameteres t imates  can  be  i so la ted .In the previous ubsection, we considered the qual-ity of the parameter estimates by considering theoverall model performance.
The negative ffect ofthe parameter estimates on this measure includesboth losses due to lack of recall and losses dueto incorrect tagging.
We can isolate the lossesdue to incorrect agging in certain cases, namelywhen the measure of form is 100%.
When themeasure of form is 100%, there is no error dueto the parametric form or to the feature set (seethe discussion of the measure of feature-set above).Thus, the lack of precision (i.e., the misclassifica-tion error) is due only to the inappropriatenessof the parameter estimates for the test set.
Forfour of the words--"bill", "chief", "concern", and"include"--the measure of form for the most com-plex models is 100%.
For these models, the pre-cision is very good, ranging from roughly 95% for"bill" to 100% for "include."
What lack of pre-cision there is (for models with measure of formof 100%) is due to inappropriateness of non-zero107parameter stimates.D iscuss ionBefore concluding, it is important to discuss theinterdependencies that exist among the three de-terminants of model performance.
The idealmodel is, of course, one in which all three are op-timal.
But is it possible to design a model that isoptimal in all three using a fixed amount of train-ing data?
Not surprisingly, the answer for mostinteresting problems is no.
An optimal set of fea-tures is one that serves to fully distinguish amongthe tags being assigned.
An optimal set (if oneexists) or even a reasonably good set is likely tobe large for any interesting problem.
Defining agood model of the joint distribution of a large setof variables using a fixed amount of training datais a process of finding the level of model complex-ity that provides the right balance between qualityof form and quality of parameter stimates (whereonly the most important interdependencies arein-cluded at each complexity level).The need for this balance is demonstrated inFigures 1 through 12 and can be explained as fol-lows.
Reducing the complexity of a model entailsreducing the number of interdependencies speci-fied in the form of that model and this, in turn,results in a reduction in the number of model pa-rameters.
While reducing the number of modelparameters increases the quality of the parameterestimates, reducing the number of interdependen-cies specified in the model results in a loss of infor-mation.
This loss negatively affects the character-ization of the joint distribution by the parametricform.
Thus, the best overall model performance isobtained when the appropriate balance is reached.Conc lus ionsThis paper described measures for evaluating thethree determinants of how well a probabilistic clas-sifter performs on a given test set.
These determi-nants are the appropriateness, for the test set, ofthe results of (1) features selection, (2) formulationof the parametric form of the model, and (3) pa-rameter estimation.
These are part of any modelformulation procedure, even if not broken out asseparate steps, so the tradeoffs explored in thispaper are relevant o a wide variety of methods.The measures were demonstrated in a large exper-iment, in which they were used to analyze the re-suits of roughly 300 classifiers that perform word-sense disambiguation.
These evaluations suggestthat the three determinants of model performanceare not independent and that the best overallmodel performance is obtained when they are ap-propriately balanced.108References\[1\] P. Brown, 3.
Cocke, S. Della Pietra,V.
Della Pietra, F. 3elinek, 3.
Lafferty,R.
Mercer, and P. Roossin.
A statisticalapproach to machine translation.
Computa-tional Linguistics, 16(2):79-85, 1990.\[2\] R. Bruce.
A Statistical Method for Word.Sense Disambiguation.
PhD thesis, Dept.
ofComputer Science, New Mexico State Univer-sity, 1995.\[3\] R. Bruce and J. Wiebe.
A new approach toword sense disambiguation.
In Proceedingsof the ARPA Workshop on Human LanguageTechnology, pages 236-241, Plainsboro, N3,March 1994.\[4\] R. Bruce and J. Wiebe.
Word-sense dis-ambiguation using decomposable models.
InProceedings of the 3~nd Annual Meeting ofthe Association for Computational Linguis-tics (ACL-94), pages 139-146, Las Cruces,NM, June 1994.\[5\] R. Bruce and 3.
Wiebe.
Towards the acquisi-tion and representation f a broad-coveragelexicon.
In Working Notes of the AAA1Spring Symposium on Representation andAcquisition of Lexical Knowledge, pages 15-20, Palo Alto, CA, March 1995.\[6\] E. Charniak and G. Carroll.
Context-sensitive statistics for improved grammaticallanguage models.
In Proceedings of the 12thNational Conference on Artificial Intelligence(AAAI-gg), pages 728-733, Seattle, WA, Au-gust 1994.\[7\] I. Dagan and A. Itai.
Automatic acquisitionof constraints for the resolution of anaphoraand syntactic ambiguities.
In Proceedingsof the International Conference on Computa-tional Linguistics, volume 3, pages 330-332,1990.\[8\] J. Grishman and J.
Sterling.
Acquisition ofselectional patterns.
In Proceedings of the14th International Conference on Computa-tional Linguistics (COLING-9g), pages 658-664, Nantes, France, August 1992.\[9\] 3.
Hirschberg and D. Litman.
Empirical stud-ies on the disambiguation of cue phrases.Computational Linguistics, 19(3):501-530,1993.\[10\] M. Marcus,B.
Santorini, and M. Marcinkiewicz.
Build-ing a large annotated corpus of English: ThePenn Treebank.
Computational Linguistics,19(2):313-330, 1993.\[11\] M. Meteer, R. Schwartz, and R. Weischedel.POST: Using probabilities in language pro-ieessing.
In Proceedings of the 12th Interna-tional Joint Conference on Artificial Intelli-gence (IJCAI-91), volume 2, pages 960-965,Sydney, Australia, 1991.\[12\] T. Pedersen, M. Kayaalp, and R. Bruce.
Sig-nificant lexical relationships.
In Proceedingsoff the 13th National Conference on ArtificialIntelligence (AAAI-96), Portland, OR, Au-gust 1996.\[13\] P. Proctor (Editor in Chief).
Longman Dic-tionary of Contempora~ English.
LongmanGroup Ltd., Essex, UK, 1978.\[14\] J. Quinlan.
Induction of decision trees.
Ma-chine Learning, 1:81-106, 1986.\[15\] A. Ratnaparkhi and S. Roukos.
A maximumentropy model for prepositional phrase at-tachment.
In Proceedings of the ARPA Work-shop on Human Language Technology, pages250-255, Plainsboro, N J, March 1994.\[16\] J. Wiebe and R. Bruce.
Probabilistic las-sifters for tracking point of view.
In Work-ing Notes of the AAAI  Spring Symposium onEmpirical Methods in Discourse Interpreta-tion and Generation, Palo Alto, CA, March1995.1091009080Performanc%nMeasure --v(percentage) 60504O30r I i i i i i, oo .o ,o ,  ooo  o ~Overall Model ?Lower Bound mForm ?
*""Recall xFeature SetI I I I I10 15 20 25 30 35Number of Interdependencies100908OPerformance?0Measure(percentage)60504030_ " .
- ~ , ~ = - ~ ~I I I I10 15 20 25Number of InterdependeuciesFigure 1: "agree" Figure 3: "chief"1009080PerformanceT0Measure(percentage)00504O30'.
oo:..oo:oooAoooo:I I I I I I10 15 20 25 30 35Number of InterdependenciesI I I I I I100 ~,... ,o oe**oooo?-~-~oooo~?
@?
~D.O,O, Cr 0PerformanceT0Measure(percentage)60504030 I I I I l t10 15 20 25 30 35Number of InterdependenciesFigure 2: "bill" Figure 4: "close"1101009080PerformancMeasure e70(percentage)60504030I I I Ip, iOverall Model ?Lower BoundForm "*""RecallFeature Set -0 -I I I I10 15 20 25Number of Interdependencies1009080Pefformanc%nMeasure - -v(percentage) 605O403OI I I I I IpO O~> O O O O O O O O ' I~ 'O 'OOO O ~>2MMMMM;I I I i I I1O 15 20 25 30 35Number of InterdependenciesFigure 5: "common" Figure 7: "drug"1009080Performance70Measure(percentage)605O4030I I I I I IO00000?O00OO 'O 'O 'OOoOOo000 0"I I I I I I10 15 20 25 30 35Number of Interdependencies1009080Performance70Measure(percentage)60504030I I I !
I I?0o  o o a, 01D.~0 O o ?
00  o OO,O '???
0 ~?o .-II f I I l 1710 15 20 25 30 35Number of InterdependenciesFigure 6: "concern" Figure 8: "help"1111009080Performanc%nMeasure - - "(percentage)605O4030I I I I I I~O O OO,O,O,o, O O O O O O O OOO'O'O'O'~O O  (~ILower BoundForm "* ?
"Recall ?Feature SetI I l I I I10 15 20 25 30 35Number of Interdependencies1009080PerformanceTnMeasure "(percentage)605O4O3OI I I 1 ' "  |I I I I10 15 20 25Number of InterdependenciesFigure 9: "include" Figure 11: "last"100908OPerformanceT0Measure(percentage)60504030L I  " I I I ....
I IO O O O O O O O'O'O'O~O'O' Q , ~ O O  ' ' 'O'O  ~I I I I I Ii0 15 20 25 30 35Number of Interdependencies1009080PerformanceT0Measure(percentage)60504030I I I I.
~ O .
,O.O'O.OO*O~OOO'~-O~'O'_I I I I10 15 20 25Number of InterdependenciesFigure 10: "interest" Figure 12: "public"112
