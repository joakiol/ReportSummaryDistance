Proceedings of the Second Workshop on Statistical Machine Translation, pages 167?170,Prague, June 2007. c?2007 Association for Computational LinguisticsNgram-based statistical machine translation enhanced with multipleweighted reordering hypothesesMarta R. Costa-jussa`, Josep M. Crego, Patrik Lambert, Maxim KhalilovJose?
A. R. Fonollosa, Jose?
B. Marin?o and Rafael E. BanchsDepartment of Signal Theory and CommunicationsTALP Research Center (UPC)Barcelona 08034, Spain(mruiz,jmcrego,lambert,khalilov,adrian,canton,rbanchs)@gps.tsc.upc.eduAbstractThis paper describes the 2007 Ngram-based sta-tistical machine translation system developed atthe TALP Research Center of the UPC (Uni-versitat Polite`cnica de Catalunya) in Barcelona.Emphasis is put on improvements and extensionsof the previous years system, being highlightedand empirically compared.
Mainly, these includea novel word ordering strategy based on: (1) sta-tistically monotonizing the training source cor-pus and (2) a novel reordering approach basedon weighted reordering graphs.
In addition, thissystem introduces a target language model basedon statistical classes, a feature for out-of-domainunits and an improved optimization procedure.The paper provides details of this system par-ticipation in the ACL 2007 SECOND WORK-SHOP ON STATISTICAL MACHINE TRANSLA-TION.
Results on three pairs of languages arereported, namely from Spanish, French and Ger-man into English (and the other way round) forboth the in-domain and out-of-domain tasks.1 IntroductionBased on estimating a joint-probability model betweenthe source and the target languages, Ngram-based SMThas proved to be a very competitive alternatively tophrase-based and other state-of-the-art systems in previ-ous evaluation campaigns, as shown in (Koehn and Monz,2005; Koehn and Monz, 2006).Given the challenge of domain adaptation, efforts havebeen focused on improving strategies for Ngram-basedSMT which could generalize better.
Specifically, a novelreordering strategy is explored.
It is based on extendingthe search by using precomputed statistical information.Results are promising while keeping computational ex-penses at a similar level as monotonic search.
Addition-ally, a bonus for tuples from the out-of-domain corpus isintroduced, as well as a target language model based onstatistical classes.
One of the advantages of working withstatistical classes is that they can easily be used for anypair of languages.This paper is organized as follows.
Section 2 brieflyreviews last year?s system, including tuple definition andextraction, translation model and feature functions, de-coding tool and optimization criterion.
Section 3 delvesinto the word ordering problem, by contrasting last yearstrategy with the novel weighted reordering input graph.Section 4 focuses on new features: both tuple-domainbonus and target language model based on classes.
Lateron, Section 5 reports on all experiments carried out forWMT 2007.
Finally, Section 6 sums up the main conclu-sions from the paper and discusses future research lines.2 Baseline N-gram-based SMT SystemThe translation model is based on bilingual n-grams.
Itactually constitutes a language model of bilingual units,referred to as tuples, which approximates the joint proba-bility between source and target languages by using bilin-gual n-grams.Tuples are extracted from a word-to-word aligned cor-pus according to the following two constraints: first, tu-ple extraction should produce a monotonic segmentationof bilingual sentence pairs; and second, no smaller tuplescan be extracted without violating the previous constraint.For all experiments presented here, the translationmodel consisted of a 4-gram language model of tuples.In addition to this bilingual n-gram translation model, thebaseline system implements a log linear combination offour feature functions.
These four additional models are:a target language model (a 5-gram model of words);a word bonus; a source-to-target lexicon model and atarget-to-source lexicon model, both features provide acomplementary probability for each tuple in the transla-tion table.The decoder (called MARIE) for this translation sys-167tem is based on a beam search 1.This baseline system is actually the same system usedfor the first shared task ?Exploiting Parallel Texts for Sta-tistical Machine Translation?
of the ACL 2005 Work-shop on Building and Using Parallel Texts: Data-DrivenMachine Translation and Beyond.
A more detailed de-scription of the system can be found in (Marin?o et al,2006).3 Baseline System Enhanced with aWeighted Reordering Input GraphThis section briefly describes the statistical machine re-ordering (SMR) technique.
Further details on the archi-tecture of SMR system can be found on (Costa-jussa` andFonollosa, 2006).3.1 ConceptThe SMR system can be seen as a SMT system whichtranslates from an original source language (S) to a re-ordered source language (S?
), given a target language(T).
The SMR technique works with statistical wordclasses (Och, 1999) instead of words themselves (partic-ularly, we have used 200 classes in all experiments).Figure 1: SMR approach in the (A) training step (B) inthe test step (the weight of each arch is in brackets).3.2 Using SMR technique to improve SMT trainingThe original source corpus S is translated into the re-ordered source corpus S?
with the SMR system.
Fig-ure 1 (A) shows the corresponding block diagram.
Thereordered training source corpus and the original trainingtarget corpus are used to build the SMT system.The main difference here is that the training is com-puted with the S?2T task instead of the S2T original task.Figure 2 (A) shows an example of the alignment com-puted on the original training corpus.
Figure 2 (B) showsthe same links but with the source training corpus in adifferent order (this training corpus comes from the SMRoutput).
Although, the quality in alignment is the same,the tuples that can be extracted change (notice that thetuple extraction is monotonic).
We are able to extract1http://gps-tsc.upc.es/veu/soft/soft/marie/smaller tuples which reduces the translation vocabularysparseness.
These new tuples are used to build the SMTsystem.Figure 2: Alignment and tuple extraction (A) originaltraining source corpus (B) reordered training source cor-pus.3.3 Using SMR technique to generate multipleweighted reordering hypothesesThe SMR system, having its own search, can generate ei-ther an output 1-best or an output graph.
In decoding, theSMR technique generates an output graph which is usedas an input graph by the SMT system.
Figure 1 (B) showsthe corresponding block diagram in decoding: the SMRoutput graph is given as an input graph to the SMT sys-tem.
Hereinafter, this either SMR output graph or SMTinput graph will be referred to as (weighted) reorderinggraph.
The monotonic search in the SMT system is ex-tended with reorderings following this reordering graph.This reordering graph has multiple paths and each pathhas its own weight.
This weight is added as a featurefunction in the log-linear framework.
Figure 3 shows theweighted reordering graph.The main difference with the reordering technique forWMT06 (Crego et al, 2006) lies in (1) the tuples are ex-tracted from the word alignment between the reorderedsource training corpus and the given target training cor-pus and (2) the graph structure: the SMR graph providesweights for each reordering path.4 Other features and functionalitiesIn addition to the novel reordering strategy, we considertwo new features functions.4.1 Target Language Model based on StatisticalClassesThis feature implements a 5-gram language model of tar-get statistical classes (Och, 1999).
This model is trainedby considering statistical classes, instead of words, for168Figure 3: Weighted reordering input graph for SMT sys-tem.the target side of the training corpus.
Accordingly, the tu-ple translation unit is redefined in terms of a triplet whichincludes: a source string containing the source side ofthe tuple, a target string containing the target side of thetuple, and a class string containing the statistical classescorresponding to the words in the target strings.4.2 Bonus for out-of-domain tuplesThis feature adds a bonus to those tuples which comesfrom the training of the out-of-domain task.
This featureis added when optimizing with the development of theout-of-domain task.4.3 OptimizationFinally, a n-best re-ranking strategy is implementedwhich is used for optimization purposes just as pro-posed in http://www.statmt.org/jhuws/.
This procedureallows for a faster and more efficient adjustment of modelweights by means of a double-loop optimization, whichprovides significant reduction of the number of transla-tions that should be carried out.
The current optimizationprocedure uses the Simplex algorithm.5 Shared Task Framework5.1 DataThe data provided for this shared task corresponds to asubset of the official transcriptions of the European Par-liament Plenary Sessions 2.
Additionally, there was avail-able a smaller corpus called News-Commentary.
For alltasks and domains, our training corpus was the catenationof both.2http://www.statmt.org/wmt07/shared-task/5.2 Processing detailsWord Alignment.
The word alignment is automati-cally computed by using GIZA++ 3 in both directions,which are symmetrized by using the union operation.
In-stead of aligning words themselves, stems are used foraligning.
Afterwards case sensitive words are recovered.Spanish Morphology Reduction.
We implemented amorphology reduction of the Spanish language as a pre-processing step.
As a consequence, training data sparse-ness due to Spanish morphology was reduced improvingthe performance of the overall translation system.
In par-ticular, the pronouns attached to the verb were separatedand contractions as del or al are splited into de el or ael.
As a post-processing, in the En2Es direction we useda POS target language model as a feature (instead of thetarget language model based on classes) that allowed torecover the segmentations (de Gispert, 2006).Language Model Interpolation.
In other to betteradapt the system to the out-of-domain condition, thetarget language model feature was built by combiningtwo 5-gram target language models (using SRILM 4).One was trained from the EuroParl training data set, andthe other from the available, but much smaller, news-commentary data set.
The combination weights for theEuroParl and news-commentary language models wereempirically adjusted by following a minimum perplexitycriterion.
A relative perplexity reduction around 10-15%respect to original EuroParl language model was achievedin all the tasks.5.3 Experiments and ResultsThe main difference between this year?s and last year?ssystems are: the amount of data provided; the word align-ment; the Spanish morphology reduction; the reorderingtechnique; the extra target language model based on sta-tistical classes (except for the En2Es); and the bonus forthe out-of-domain task (only for the En2Es task).Among them, the most important is the reorderingtechnique.
That is why we provide a fair comparison be-tween the reordering patterns (Crego and Marin?o, 2006)technique and the SMR reordering technique.
Table 1shows the system described above using either reorder-ing patterns or the SMR technique.
The BLEU calcula-tion was case insensitive and sensitive to tokenization.Table 2 presents the BLEU score obtained for the 2006test data set comparing last year?s and this year?s systems.The computed BLEU scores are case insensitive, sensi-tive to tokenization and uses one translation reference.The improvement in BLEU results shown from UPC-jm3http://www.fjoch.com/GIZA++.html4http://www.speech.sri.com/projects/srilm/169Task Reordering patterns SMR techniquees2en 31.21 33.34en2es 31.67 32.33Table 1: BLEU comparison: reordering patterns vs. SMRtechnique.Task UPC-jm 2006 UPC 2007in-d out-d in-d out-des2en 31.01 27.92 33.34 32.85en2es 30.44 25.59 32.33 33.07fr2en 30.42 21.79 32.44 26.93en2fr 31.75 23.30 32.30 27.03de2en 24.43 17.57 26.54 21.63en2de 17.73 10.96 19.74 15.06Table 2: BLEU scores for each of the six translation di-rections considered (computed over 2006 test set) com-paring last year?s and this year?s system results (in-domain and out-domain).2006 Table 2 and reordering patterns Table 1 in the En-glish/Spanish in-domain task comes from the combina-tion of: the additional corpora, the word alignment, theSpanish morphology reduction and the extra target lan-guage model based on classes (only in the Es2En direc-tion).6 Conclusions and Further WorkThis paper describes the UPC system for the WMT07Evaluation.
In the framework of Ngram-based system, anovel reordering strategy which can be used for any pairof languages has been presented and it has been showedto significantly improve translation performance.
Ad-ditionally two features has been added to the log-linealscheme: the target language model based on classes andthe bonus for out-of-domain translation units.7 AcknowledgmentsThis work has been funded by the European Union un-der the TC-STAR project (IST-2002-FP6-506738) andthe Spanish Government under grant TEC2006-13964-C03 (AVIVAVOZ project).ReferencesM.R.
Costa-jussa` and J.A.R.
Fonollosa.
2006.
Statisticalmachine reordering.
In EMNLP, pages 71?77, Sydney,July.
ACL.J.M.
Crego and J.B. Marin?o.
2006.
Reordering experi-ments for n-gram-based smt.
In SLT, pages 242?245,Aruba.Josep M. Crego, Adria` de Gispert, Patrik Lambert,Marta R. Costa-jussa`, Maxim Khalilov, Rafael Banchs,Jose?
B. Marin?o, and Jose?
A. R. Fonollosa.
2006.
N-gram-based smt system enhanced with reordering pat-terns.
In WMT, pages 162?165, New York City, June.ACL.Adria` de Gispert.
2006.
Introducing Linguistic Knowl-edge in Statistical Machine Translation.
Ph.D. thesis,Universitat Polite`cnica de Catalunya, December.Philipp Koehn and Christof Monz.
2005.
Shared task:Statistical machine translation between european lan-guages.
In WMT, pages 119?124, Michigan, June.ACL.Philipp Koehn and Christof Monz.
2006.
Manual andautomatic evaluation of machine translation betweeneuropean languages.
In WMT, pages 102?121, NewYork City, June.
ACL.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-jussa`.2006.
N-gram based machine translation.
Computa-tional Linguistics, 32(4):527?549, December.F.J.
Och.
1999.
An efficient method for determin-ing bilingual word classes.
In EACL, pages 71?76,Bergen, Norway, June.170
