A Semantic Kernel for Predicate Argument ClassificationAlessandro Moschitti and Cosmin Adrian BejanUniversity of Texas at DallasHuman Language Technology Research InstituteRichardson, TX 75083-0688, USAalessandro.moschitti@utdallas.eduady@hlt.utdallas.eduAbstractAutomatically deriving semantic structuresfrom text is a challenging task for machinelearning.
The flat feature representations, usu-ally used in learning models, can only partiallydescribe structured data.
This makes difficultthe processing of the semantic information thatis embedded into parse-trees.In this paper a new kernel for automatic clas-sification of predicate arguments has been de-signed and experimented.
It is based on sub-parse-trees annotated with predicate argumentinformation from PropBank corpus.
This ker-nel, exploiting the convolution properties ofthe parse-tree kernel, enables us to learn whichsyntactic structures can be associated with thearguments defined in PropBank.
Support Vec-tor Machines (SVMs) using such a kernel clas-sify arguments with a better accuracy thanSVMs based on linear kernel.1 IntroductionSeveral linguistic theories, e.g.
(Jackendoff, 1990), claimthat semantic information in natural language texts isconnected to syntactic structures.
Hence, to deal with nat-ural language semantics, the learning algorithm should beable to represent and process structured data.
The classi-cal solution adopted for such tasks is to convert syntaxstructures in a flat feature representation, which is suit-able for a given learning model.
The main drawback isstructures may not be properly represented by flat fea-tures as: (1) these latter may not be able to capture therequired properties or (2) the feature designer may notknow what structure properties enable the processing ofsemantic information.In particular, these problems arise for semantic infor-mation represented via predicate argument structures de-fined on syntactic parse trees.
For example, Figure 1shows the parse tree of the sentence: "Paul givesa lecture in Rome" along with the annotation ofpredicate arguments.A predicate may be a verb or a noun or an adjectivewhereas generally Arg 0 stands for agent, Arg 1 for di-rect object or theme or patient and ArgM may indicatelocations, as in our example.
A standard for predicate ar-gument annotation is provided in the PropBank project(Kingsbury and Palmer, 2002).
It has produced onemillion word corpus annotated with predicate-argumentstructures on top of the Penn Treebank 2 Wall Street Jour-nal texts.
In this way, for a large number of the PennTreeBank parse-trees, there are available predicate anno-tations in a style similar to that shown in Figure 1.PredicateArg.
0Arg.
MSNNPD NVPV Paulingivesa lecturePPIN NRomeArg.
1Figure 1: Predicate arguments in a parse-tree representation.In PropBank only verbs are considered to be predicateswhereas arguments are labeled sequentially from Arg 0to Arg 91.
In addition to these core arguments, adjunctivearguments are marked up.
They include functional tags,e.g.
ArgM-DIR indicates a directional, ArgM-LOC in-dicates a locative and ArgM-TMP stands for a temporal.An example of PropBank markup is:1Other arguments are: Arg 2 for indirect object or benefac-tive or instrument or attribute or end state, Arg 3 for start pointor benefactive or attribute, Arg4 for end point and so on.
[Arg10 Analysts ] have been [predicate1 expecting ] [Arg11a GM-Jaguar pact ] that would [predicate2 give ] [Arg22 theU.S.
car maker ] [Arg21 an eventual 30% state in the BritishCompany ].Automatically recognizing the boundaries and classi-fying the type of arguments allows Natural LanguageProcessing systems (e.g.
Information Extraction, Ques-tion Answering or Summarization) to answer questionssuch as ?Who?, ?When?, ?What?, ?Where?, ?Why?, andso on.Given the importance of this task for Natural Lan-guage Processing applications, several machine learningapproaches for argument identification and classificationhave been developed (Gildea and Jurasky, 2002; Sur-deanu et al, 2003; Hacioglu et al, 2003; Chen and Ram-bow, 2003; Gildea and Hockenmaier, 2003).
Their com-mon characteristic is the adoption of feature spaces thatmodel predicate-argument structures in a flat representa-tion.
The major problem of this choice is that there isno linguistic theory that supports the selection of syntac-tic features to recognize semantic structures.
As a con-sequence, researchers are still trying to extend the basicfeatures with other ones, e.g.
(Surdeanu et al, 2003), toimprove the flat feature space.Convolution kernels are a viable alternative to flat fea-ture representation that aims to capture the structural in-formation in term of sub-structures.
The kernel functionscan be used to measure similarities between two objectswithout explicitly evaluating the object features.
Thatis, we do not need to understand which syntactic featuremay be suited for representing semantic data.
We needonly to define the similarity function between two seman-tic structures.
An example of convolution kernel on theparse-tree space is given in (Collins and Duffy, 2002).The aim was to design a novel syntactic parser by look-ing at the similarity between the testing parse-trees andthe correct parse-trees available for training.In this paper, we define a kernel in a semantic struc-ture space to learn the classification function of predicatearguments.
The main idea is to select portions of syn-tactic/semantic trees that include the target <predicate,argument> pair and to define a kernel function betweenthese objects.
If our similarity function is well defined thelearning model will converge and provide an effective ar-gument classification.Experiments on PropBank data show not only thatSupport Vector Machines (SVMs) trained with the pro-posed semantic kernel converge but also that they have ahigher accuracy than SVMs trained with a linear kernelon the standard features proposed in (Gildea and Jurasky,2002).
This provides a piece of evidence that convolutionkernel can be used to learn semantic linguistic structures.Moreover, interesting research lines on the use of ker-nel for NLP are enabled, e.g.
question classification inQuestion/Answering or automatic template designing inInformation Extraction.The remaining of this paper is organized as follows:Section 2 defines the Predicate Argument Extractionproblem and the standard solution to solve it.
In Section3 we present our approach based on the parse-tree kernelwhereas in Section 4 we show our comparative resultsbetween SVMs using standard features and the proposedkernel.
Finally, Section 5 summarizes the conclusions.2 Automatic Predicate-ArgumentextractionGiven a sentence in natural language, all the predicatesassociated with its verbs have to be identified along withtheir arguments.
This problem can be divided in two sub-tasks: (a) detection of the target argument boundaries,i.e.
all its compounding words, and (b) classification ofthe argument type, e.g.
Arg0 or ArgM.A direct approach to learn both detection and classifi-cation of predicate arguments is summarized by the fol-lowing steps:1.
Given a sentence from the training-set, generate afull syntactic parse-tree;2. let P and A be the set of predicates and the set ofparse-tree nodes (i.e.
the potential arguments), re-spectively;3. for each pair <p, a> ?
P ?A:?
extract the feature representation set, Fp,a;?
if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in T+(positive examples), otherwise put it in T?
(negative examples).For example, in Figure 1, for each combination of thepredicate give with the nodes N, S, VP, V, NP, PP, D orIN the instances F?give?,a are generated.
In case the nodea exactly covers Paul, a lecture or in Rome, it will be apositive instance otherwise it will be a negative one, e.g.F?give?,?IN?.The above T+ and T?
sets can be re-organized as posi-tive T+argi and negative T?argi examples for each argumenti.
In this way, an individual ONE-vs-ALL classifier foreach argument i can be trained.
We adopted this solutionas it is simple and effective (Pradhan et al, 2003).
In theclassification phase, given a sentence of the test-set, allits Fp,a are generated and classified by each individualclassifier.
As a final decision, we select the argument as-sociated with the maximum value among the scores pro-vided by the SVMs2, i.e.
argmaxi?S Ci, where S isthe target set of arguments.2This is a basic method to pass from binary categorization2.1 Standard feature spaceThe discovering of relevant features is, as usual, a com-plex task, nevertheless there is a common consensus onthe basic features that should be adopted.
These stan-dard features, firstly proposed in (Gildea and Jurasky,2002), refer to a flat information derived from parse trees,i.e.
Phrase Type, Predicate Word, Head Word, GoverningCategory, Position and Voice.
Table 1 presents the stan-dard features and exemplifies how they are extracted froma given parse tree.- Phrase Type: This feature indicates the syntactic typeof the phrase labeled as a predicate argument, e.g.
NPfor Arg1 in Figure 1.- Parse Tree Path: This feature contains the path inthe parse tree between the predicate and the argumentphrase, expressed as a sequence of nonterminal labelslinked by direction (up or down) symbols, e.g.
V ?
VP?
NP for Arg1 in Figure 1.- Position: Indicates if the constituent, i.e.
the potentialargument, appears before or after the predicate in thesentence, e.g.
after for Arg1 and before for Arg0 (seeFigure 1).- Voice: This feature distinguishes between active orpassive voice for the predicate phrase, e.g.
active forevery argument (see Figure 1).- Head Word: This feature contains the head word of theevaluated phrase.
Case and morphological informationare preserved, e.g.
lecture for Arg1 (see Figure 1).- Governing Category: This feature applies to nounphrases only, and it indicates if the NP is dominated bya sentence phrase (typical for subject arguments withactive voice predicates), or by a verb phrase (typical forobject arguments), e.g.
the NP associated with Arg1 isdominated by a verbal phrase VP (see Figure 1).- Predicate Word: In our implementation this featureconsists of two components: (1) the word itself withthe case and morphological information preserved, e.g.gives for all arguments; and (2) the lemma which rep-resents the verb normalized to lower case and infinitiveform, e.g.
give for all arguments (see Figure 1).Table 1: Standard features extracted from parse-trees.For example, the Parse Tree Path feature represents thepath in the parse-tree between a predicate node and one ofits argument nodes.
It is expressed as a sequence of non-terminal labels linked by direction symbols (up or down),e.g.
in Figure 1, V?VP?NP is the path between the pred-icate to give and the argument 1, a lecture.
If two pairs<p1, a1> and <p2, a2> have a Path that differs even forone character (e.g.
a node in the parse-tree) the matchwill not be carried out, preventing the learning algorithmto generalize well on unseen data.
In order to address alsointo a multi-class categorization problem; several optimizationhave been proposed, e.g.
(Goh et al, 2001).this problem, next section describes a novel kernel spacefor predicate argument classification.3 A semantic kernel for argumentclassificationWe consider the predicate argument structures annotatedin PropBank as our semantic space.
Many semantic struc-tures may constitute the objects of our space.
Some possi-bilities are: (a) the selection of the whole sentence parse-tree, in which the target predicate is contained or (b) theselection of the sub-tree that encloses the whole predi-cate annotation (i.e.
all its arguments).
However, bothchoices would cause an exponential explosion on the po-tential sub-parse-trees that have to be classified duringthe testing phase.
In fact, during this phase we do notknow which are the arguments associated with a predi-cate.
Thus, we need to build all the possible structures,which contain groups of potential arguments for the tar-get predicate.
More in detail, assuming that S is the set ofPropBank argument types, and m is the maximum num-ber of entries that the target predicate can have, we haveto evaluate(|S|m)argument combinations for each targetpredicate.In order to define an efficient semantic space we se-lect as objects only the minimal sub-structures that in-clude one predicate with only one of its arguments.
Forexample, Figure 2 illustrates the parse-tree of the sen-tence "Paul delivers a lecture in formalstyle".
The circled substructures in (a), (b) and (c) areour semantic objects associated with the three argumentsof the verb to deliver, i.e.
<deliver, Arg0>, <deliver,Arg1> and <deliver, ArgM>.
In this formulation, onlyone of the above structures is associated with each pred-icate/argument pair, i.e.
Fp,a contain only one of the cir-cled sub-trees.We note that our approach has the following properties:?
The overall semantic feature space F contain sub-structures composed of syntactic information em-bodied by parse-tree dependencies and semantic in-formation under the form of predicate/argument an-notation.?
This solution is efficient as we have to classify atmaximum |A| nodes for each predicate, i.e.
the setof the parse-tree nodes of a testing sentence.?
A constituent cannot be part of two different argu-ments of the target predicate, i.e.
there is no over-lapping between the words of two arguments.
Thus,two semantic structures Fp1,a1 and Fp2,a23, asso-3Fp,a was defined as the set of features of our objects<p, a>.
Since in our kernel we have only one element in Fp,awith an abuse of notation we use it to indicate the objects them-selves.SNNPD NVPV PaulindeliversatalkPPINNPjjFdeliver, Arg0formalNstyleArg.
0a) SNNPD NVPV PaulindeliversatalkPPINNPjjformalNstyleFdeliver, Arg1b) SNNPD NVPV PaulindeliversatalkPPINNPjjformalNstyleArg.
1Fdeliver, ArgMc)Arg.
MFigure 2: Semantic feature space for predicate argument classification.ciated with two different arguments, cannot be in-cluded one in the other.
This property is importantbecause, a convolution kernel would not be effectiveto distinguish between an object and its sub-parts.Once defined our semantic space we need to design akernel function to measure a similarity between two ob-jects.
These latter may still be seen as described by com-plex features but such a similarity is carried out avoidingthe explicit feature computation.
For this purpose we de-fine a mapping ?
: F ?
F ?
such as:~x = (x1, ..., x|F |) ?
?
(~x) = (?1(~x), .., ?|F ?|(~x)),where F ?
allows us to design an efficient semantic kernelK(~x, ~z) =<?
(~x) ?
?
(~z)>.3.1 The Semantic Kernel (SK)Given the semantic objects defined in the previous sec-tion, we design a convolution kernel in a way similarto the parse-tree kernel proposed in (Collins and Duffy,2002).
Our feature set F ?
is the set of all possible sub-structures (enumerated from 1 to |F ?|) of the semanticobjects extracted from PropBank.
For example, Figure3 illustrates all valid fragments of the semantic structureFdeliver,Arg1 (see also Figure 2).
It is worth noting thatthe allowed sub-trees contain the entire (not partial) pro-duction rules.
For instance, the sub-tree [NP [D a]] isexcluded from the set of the Figure 3 since only a partof the production NP ?
D N is used in its generation.However, this constraint does not apply to the productionVP?
V NP PP along with the fragment [VP [V NP]] asthe subtree [VP [PP [...]]] is not considered part of thesemantic structure.Even if the cardinality of F ?
will be very large the eval-uation of the kernel function is polynomial in the numberof parse-tree nodes.More precisely, a semantic structure ~x is mapped in?
(~x) = (h1(~x), h2(~x), ...), where the feature functionhi(~x) simply counts the number of times that the i-th sub-structure of the training data appears in ~x.
LetNPD NatalkNPD NNPD NaD Na   talkNPD N NPD NVPVdeliversatalkVdeliversNPD NVPVatalkNPD NVPVNPD NVPVaNPDVPVtalkNaNPD NVPVdeliverstalkNPD NVPVdelivers NP D NVPVdeliversNPVPV NPVPVdeliverstalkFigure 3: All 17 valid fragments of the semantic structure as-sociated with Arg 1 (see Figure 2).Ii(n) be the indicator function: 1 if the sub-structurei is rooted at node n and 0 otherwise.
It follows thath(~x) = ?n?N Ii(n), where N is the set of the ~x?s nodes.Therefore, the kernel4 function is:K(~x, ~z) = ~h(~x) ?
~h(~z) ==?i( ?nx?NxIi(nx))( ?nz?NzIi(nz)) ==?nx?Nx?nz?Nz?iIi(nx)Ii(nz) (1)where Nx and Nz are the nodes in x and z, respec-tively.
In (Collins and Duffy, 2002), it has been shownthat Eq.
1 can be computed in O(|Nx| ?
|Nz|) by eval-uating ?
(nx, nz) =?i Ii(nx)Ii(nz) with the followingrecursive equations:?
if the production at nx and nz are different then?
(nx, nz) = 0;4Additionally, we carried out the normalization in the kernelspace, thus the final kernel is K?
(~x, ~z) = K(~x,~z)?K(~x,~x)?K(~z,~z).?
if the production at nx and nz are the same, and nxand nz are pre-terminals then?
(nx, nz) = 1; (2)?
if the production at nx and nz are the same, and nxand nz are not pre-terminals then?
(nx, nz) =nc(nx)?j=1(1 + ?
(ch(nx, j), ch(nz, j))),(3)where nc(nx) is the number of children of nx andch(n, i) is the i-th child of the node n. Note that asthe productions are the same ch(nx, i) = ch(nz, i).This kind of kernel has the drawback of assigning moreweight to larger structures while the argument type doesnot depend at all on the size of its structure.
In fact twosentences such as:(1) [Arg0 Paul ][predicate delivers ] [Arg1 a lecture] and(2) [Arg0 Paul ][predicate delivers ][Arg1 a plan on the de-tection of theorist groups active in the North Iraq]have the same argument type with a very different size.To overcome this problem we can scale the relative im-portance of the tree fragments with their size.
For thispurpose a parameter ?
is introduced in equations 2 and 3obtaining:?
(nx, nz) = ?
(4)?
(nx, nz) = ?nc(nx)?j=1(1+?
(ch(nx, j), ch(nz, j))) (5)It is worth noting that even if the above equationsdefine a kernel function similar to the one proposed in(Collins and Duffy, 2002), the substructures on which SKoperates are different from the parse-tree kernel.
For ex-ample, Figure 3 shows that structures such as [VP [V][NP]], [VP [V delivers ] [NP]] and [VP [V] [NP [DTN]]] are valid features, but these fragments (and manyothers) are not generated by a complete production, i.e.VP?
V NP PP.
As a consequence they are not includedin the parse-tree kernel representation of the sentence.3.2 Comparison with Standard FeaturesWe have synthesized the comparison between stan-dard features and the SK representation in the follow-ing points.
First, SK estimates a similarity betweentwo semantic structures by counting the number ofsub-structures that are in common.
As an example,the similarity between the two structures in Figure 2,F?delivers?,Arg0 and F?delivers?,Arg1, is equal to 1 sincethey have in common only the [V delivers] substruc-ture.
Such low value depends on the fact that differentargument types tend to appear in different structures.On the contrary, if two structures differ only for a fewnodes (especially terminal or near terminal nodes) thesimilarity remains quite high.
For example, if we changethe tense of the verb to deliver (Figure 2) in delivered,the [VP [V delivers] NP] subtree will be transformedin [VP [VBD delivered] NP], where the NP is un-changed.
Thus, the similarity with the previous structurewill be quite high as: (1) the NP with all sub-parts willbe matched and (2) the small difference will not highlyaffect the kernel norm and consequently the final score.This conservative property does not apply to the ParseTree Path feature which is very sensible to small changesin the tree-structure, e.g.
two predicates, expressed in dif-ferent tenses, generate two different Path features.Second, some information contained in the standardfeatures is embedded in SK: Phrase Type, Predicate Wordand Head Word explicitly appear as structure fragments.For example, in Figure 3 are shown fragments like [NP[DT] [N]] or [NP [DT a] [N talk]] which explicitly en-code the Phrase Type feature NP for Arg 1 in Figure 2.b.The Predicate Word is represented by the fragment [Vdelivers] and the Head Word is present as [N talk].Finally, Governing Category, Position and Voice can-not be expressed by SK.
This suggests that a combinationof the flat features (especially the named entity class (Sur-deanu et al, 2003)) with SK could furthermore improvethe predicate argument representation.4 The ExperimentsFor the experiments, we used PropBank(www.cis.upenn.edu/?ace) along with Penn-TreeBank5 2 (www.cis.upenn.edu/?treebank)(Marcus et al, 1993).
This corpus contains about 53,700sentences and a fixed split between training and testingwhich has been used in other researches (Gildea andJurasky, 2002; Surdeanu et al, 2003; Hacioglu et al,2003; Chen and Rambow, 2003; Gildea and Hocken-maier, 2003; Gildea and Palmer, 2002; Pradhan et al,2003).
In this split, Sections from 02 to 21 are used fortraining, section 23 for testing and sections 1 and 22 asdeveloping set.
We considered all PropBank argumentsfrom Arg0 to Arg9, ArgA and ArgM even if only Arg0from Arg4 and ArgM contain enough training/testingdata to affect the global performance.
In Table 2 somecharacteristics of the corpus used in our experiments arereported.The classifier evaluations were carried out usingthe SVM-light software (Joachims, 1999) available athttp://svmlight.joachims.org/ with the de-fault linear kernel for the standard feature evaluations.5We point out that we removed from the Penn TreeBank thespecial tags of noun phrases like Subj and TMP as parsers usu-ally are not able to provide this information.Table 2: Characteristics of the corpus used in the experiments.Number of Args Number of uniquetrain.
test-set Std.
featuresArg0 34,955 2,030 12,520Arg1 44,369 2,714 14,442Arg2 10,491 579 6,422Arg3 2,028 106 1,591Arg4 1,611 67 918ArgM 30,464 1,930 7,647Total 123,918 7,426 21,710For processing our semantic structures, we implementedour own kernel and we used it inside SVM-light.The classification performances were evaluated usingthe f1 measure for single arguments as each of them hasa different Precision and Recall and by using the accu-racy for the final multi-class classifier as the global Pre-cision = Recall = accuracy.
The latter measure allowsus to compare the results with previous literature works,e.g.
(Gildea and Palmer, 2002; Surdeanu et al, 2003; Ha-cioglu et al, 2003; Chen and Rambow, 2003; Gildea andHockenmaier, 2003).To evaluate the effectiveness of our new kernel we di-vided the experiments in 3 steps:?
The evaluation of SVMs trained with standard fea-tures in a linear kernel, for comparison purposes.?
The estimation of the ?
parameter (equations 4 and5) for SK from the validation-set .?
The performance measurement of SVMs, using SKalong with ?
computed in the previous step.Additionally, both Linear and SK kernels were evalu-ated using different percentages of training data to com-pare the gradients of their learning curves.4.1 SVM performance on Linear and SemanticKernelThe evaluation of SVMs using a linear kernel on the stan-dard features did not raise particular problems.
We usedthe default regularization parameter (i.e., C = 1 for nor-malized kernels) and we tried a few cost-factor values(i.e., j ?
{0.1, 1, 2, 3, 4, 5}) to adjust the rate betweenprecision and recall.
Given the huge amount of trainingdata, we used only 30% of training-set in these valida-tion experiments.
Once the parameters were derived, welearned 6 different classifiers (one for each role) and mea-sured their performances on the test-set.For SVM, using the Semantic Kernel, we derived thata good ?
parameter for the validation-set is 0.4.
In Figure4 we report the curves, f1 function of ?, for the 3 largest(in term of training examples) arguments on the test-set.0.820.850.880.910.940 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9?f1Arg0Arg1ArgMFigure 4: SVM f1 for Arg0, Arg1 and ArgM with respect todifferent ?
values.We note that the maximal value from the validation-set isalso the maximal value from the test-set for every argu-ment.
This suggests that: (a) it is easy to detect an optimalparameter and (b) there is a common (to all arguments) ?-value which defines how much the size of two structuresimpacts on their similarity.
Moreover, some experimentsusing ?
greater than 1 have shown a remarkable decreasein performance, i.e.
a correct ?
seems to be essential toobtain a good generalization6 of the training-set.Table 3: f1 of SVMs using linear and semantic kernel com-pared to literature models for argument classification.Args SVM SVM Prob.
C4.5STD SK STD STD EXTArg0 87.79 88.35 - - -Arg1 82.43 86.25 - - -Arg2 54.10 68.52 - - -Arg3 31.65 49.46 - - -Arg4 62.81 66.66 - - -ArgM 91.97 94.07 - - -multi-classaccuracy 84.07 86.78 82.8 78.76 83.74Table 3 reports the performances of SVM trained withthe standard features (STD column) and with the Seman-tic Kernel (SK column).
In columns Prob.
and C4.5 arereported the results for argument classification achievedin (Gildea and Palmer, 2002) and (Surdeanu et al, 2003).This latter used C4.5 model on standard feature set (STDsub-column) and on an extended feature set (EXT sub-column).
We note that: (a) SVM performs better thanthe probabilistic approach and C4.5 learning model inde-pendently of the adopted features and (b) the SemanticKernel considerably improves the standard feature set.In order to investigate if SK generalizes better than the6For example, ?
= 1 would generate low kernel values be-tween small and large structures.
This is in contrast with theobservation in Section 3.1, i.e.
argument type is independent ofits constituent size.linear kernel, we measured the performances by select-ing different percentages of training data.
Figure 5 showsthe curves for the three roles Arg0, Arg1 and ArgM, re-spectively for linear and semantic kernel whereas Figure6 shows the multi-class classifier f1 plots.0.70.730.760.790.820.850.880.910.940 15 30 45 60 75 90% Training Dataf1Arg0-SK Arg1-SK ArgM-SKArg0-STD Arg1-STD ArgM-STDFigure 5: Arg0, Arg1 and ArgM evaluations over SK and thelinear kernel of standard features with respect to different per-centages of training data.0.70.730.760.790.820.850.880 10 20 30 40 50 60 70 80 90 100% Trai ing DataAccuracySKSTDFigure 6: Accuracy of the multi-class classifier using standardfeatures and SK with respect to different percentages of trainingdata.We note that not only SK produces higher accuracybut also the gradient of the learning curves is higher: forexample, Figure 6 shows that with only 20% of trainingdata, SVM using SK approaches the accuracy of SVMtrained with all data on standard features.Additionally, we carried out some preliminary exper-iments for argument identification (boundary detection),but the learning algorithm was not able to converge.
Infact, for this task the non-inclusion property (discussedin Section 3) does not hold.
A constituent ai, which hasincorrect boundaries, can include or be included in thecorrect argument ac.
Thus, the similarity K(ai, ac) be-tween ai and ac is quite high preventing the algorithm tolearn the structures of correct arguments.4.2 Discussion and Related WorkThe material of the previous sections requires a discus-sion of the following points: firstly, in Section 3.2 wehave noted that some standard features are explicitlycoded in SK but Governing Category, Position and Voicefeatures are not expressible as a single fragment of a se-mantic structure.
For example, to derive the Position ofan argument relatively to the target predicate is required avisit of the tree.
No parse-tree information, i.e.
node tagsor edges, explicitly indicates this feature.
A similar ratio-nale applies to Governing Category and Voice, even if forthe latter some tree fragments may code the to be feature.Since these three features have been proved important forrole classification we argue that either (a) SK implicitlyproduces this kind of information or (b) SK is able to pro-vide a different but equally effective information whichallows it to perform better than the standard features.
Inthis latter case, it would be interesting to study whichfeatures can be backported from SK to the linear kernelto obtain a fast and improved system (Cumby and Roth,2003).
As an example, the fragment [VP [V NP]] definesa sort of sub-categorization frame that may be used tocluster together syntactically similar verbs.Secondly, it is worth noting that we compared SKagainst a linear kernel of standard features.
A recentstudy, (Pradhan et al, 2003), has suggested that a poly-nomial kernel with degree = 2 performs better than thelinear one.
Using such a kernel, the authors obtained88% in classification but we should take into accountthat they also used a larger set of flat features, i.e.
sub-categorization information (e.g.
VP?
V NP PP for thetree in Figure 1), Named Entity Classes and a Partial Pathfeature.Thirdly, this is one of the first massive use of convo-lution kernels for Natural Language Processing tasks, wetrained SK and tested it on 123,918 and 7,426 arguments,respectively.
For training each large argument (in termof instances) were required more than 1.5 billion of ker-nel iterations.
This was a little time consuming (abouta couple of days for each argument on a Intel Pentium4, 1,70 GHz, 512 Mbytes Ram) as the SK computationcomplexity is quadratic in the number of semantic struc-ture nodes7.
This prevented us to carry out cross/fold val-idation.
An important aspect is that a recent paper (Vish-wanathan and Smola, 2002) assesses that the tree-kernelcomplexity can be reduced to linear one; this would makeour approach largely applicable.Finally, there is a considerable work in Natural Lan-guage Processing oriented kernel (Collins and Duffy,2002; Lodhi et al, 2000; Ga?rtner, 2003; Cumby andRoth, 2003; Zelenko et al, 2003) about string, parse-7More precisely, it is O(|Fp,a|2) where Fp,a is the largestsemantic structure of the training data.tree, graph, and relational kernels but, to our knowledge,none of them was used to derive semantic informationon the form of predicate argument structures.
In particu-lar, (Cristianini et al, 2001; Kandola et al, 2003) addressthe problem of semantic similarity between two terms byusing, respectively, document sets as term context andthe latent semantic indexing.
Both techniques attemptto cluster together terms that express the same meaning.This is quite different in means and purpose of our ap-proach that derives more specific semantic informationexpressed as argument/predicate relations.5 ConclusionsIn this paper, we have experimented an original kernelbased on semantic structures from PropBank corpus.
Theresults have shown that:?
the Semantic Kernel (SK) can be adopted to classifypredicate arguments defined in PropBank;?
SVMs using SK performs better than SVMs trainedwith the linear kernel of standard features; and?
the higher gradient in the accuracy/training percent-age plots shows that SK generalizes better than thelinear kernel.Finally, SK suggests that some features, contained inthe fragments of semantic structures, should be back-ported in a flat feature space.
Conversely, the good per-formance of the linear kernel suggests that standard fea-tures, e.g.
Head Word, Predicate Word should be empha-sized in the definition of a convolution kernel for argu-ment classification.
Moreover, other selections of predi-cate/argument substructures (able to capture different lin-guistic relations) as well as kernel combinations (e.g.
flatfeatures with SK) could furthermore improve semanticshallow parsing.6 AcknowledgementsThis research has been sponsored by the ARDAAQUAINT program.
In addition, we would like to thankprof.
Sanda Harabagiu to support us with interesting ad-vices.
Many thanks to the anonymous reviewers for theirprofessional and committed suggestions.ReferencesJohn Chen and Owen Rambow.
2003.
Use of deep linguisticfeatures for the recognition and labeling of semantic argu-ments.
In Proceedings EMNLP03.Michael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In Proceedings of ACL02.Nello Cristianini, John Shawe-Taylor, and Huma Lodhi.
2001.Latent semantic kernels.
In Proceedings of ICML01, pages66?73, Williams College, US.
Morgan Kaufmann Publish-ers, San Francisco, US.Chad Cumby and Dan Roth.
2003.
Kernel methods for rela-tional learning.
In Proceedings of ICML03.Thomas Ga?rtner.
2003.
A survey of kernels for structured data.SIGKDD Explor.
Newsl., 5(1):49?58.Daniel Gildea and Julia Hockenmaier.
2003.
Identifying se-mantic roles using combinatory categorial grammar.
In Pro-ceedings of EMNLP03.Daniel Gildea and Daniel Jurasky.
2002.
Automatic labeling ofsemantic roles.
Computational Linguistic, 28(3):496?530.Daniel Gildea and Martha Palmer.
2002.
The necessity of pars-ing for predicate argument recognition.
In Proceedings ofACL02, Philadelphia, PA.King-Shy Goh, Edward Chang, and Kwang-Ting Cheng.
2001.SVM binary classifier ensembles for image classification.Proceedings of CIKM01, pages 395?402.Kadri Hacioglu, Sameer Pradhan, Wayne Ward, Jim Martin, andDan Jurafsky.
2003.
Shallow semantic parsing using Sup-port Vector Machines.
Technical report.R.
Jackendoff.
1990.
Semantic Structures, Current Studiesin Linguistics series.
Cambridge, Massachusetts: The MITPress.T.
Joachims.
1999.
Making large-scale SVM learningpractical.
In B. Schlkopf, C. Burges, and MIT-Press.A.
Smola (ed.
), editors, Advances in Kernel Methods - Sup-port Vector Learning.J.
Kandola, N. Cristianini, and J. Shawe-Taylor.
2003.
Learn-ing semantic similarity.
In Advances in Neural InformationProcessing Systems, volume 15.Paul Kingsbury and Martha Palmer.
2002.
From TreeBank toPropBank.
In Proceedings of LREC02, Las Palmas, Spain.Huma Lodhi, Craig Saunders, John Shawe-Taylor, Nello Cris-tianini, and Christopher Watkins.
2000.
Text classificationusing string kernels.
In NIPS, pages 563?569.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of english: The Penn Tree-Bank.
Computational Linguistics, 19:313?330.Sameer Pradhan, Kadri Hacioglu, Wayne Ward, James H. Mar-tin, and Daniel Jurafsky.
2003.
Semantic role parsing:Adding semantic structure to unstructured text.
In Proceed-ings of ICDM03.Mihai Surdeanu, Sanda M. Harabagiu, John Williams, and JohnAarseth.
2003.
Using predicate-argument structures for in-formation extraction.
In Proceedings of ACL03, Sapporo,Japan.S.V.N.
Vishwanathan and A.J.
Smola.
2002.
Fast kernels onstrings and trees.
In Proceedings of Neural Information Pro-cessing Systems.D.
Zelenko, C. Aone, and A. Richardella.
2003.
Kernel meth-ods for relation extraction.
Journal of Machine Learning Re-search.
