Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 30?41,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsExploring the utility of joint morphological and syntactic learningfrom child-directed speechStella Franksfrank@inf.ed.ac.ukFrank Kellerkeller@inf.ed.ac.ukILCC, School of InformaticsUniversity of EdinburghEdinburgh, EH8 9AB, UKSharon Goldwatersgwater@inf.ed.ac.ukAbstractChildren learn various levels of linguisticstructure concurrently, yet most existing mod-els of language acquisition deal with onlya single level of structure, implicitly assum-ing a sequential learning process.
Developingmodels that learn multiple levels simultane-ously can provide important insights into howthese levels might interact synergistically dur-ing learning.
Here, we present a model thatjointly induces syntactic categories and mor-phological segmentations by combining twowell-known models for the individual tasks.We test on child-directed utterances in Englishand Spanish and compare to single-task base-lines.
In the morphologically poorer language(English), the model improves morphologicalsegmentation, while in the morphologicallyricher language (Spanish), it leads to bettersyntactic categorization.
These results providefurther evidence that joint learning is useful,but also suggest that the benefits may be dif-ferent for typologically different languages.1 IntroductionModels of language acquisition seek to infer lin-guistic structure from data with minimal amounts ofprior knowledge, in order to discover which char-acteristics of the input data are useful for learn-ing, and thus potentially utilised by human learners.Most previous work has focused on learning individ-ual aspects of linguistic structure.
However, childrenclearly learn multiple aspects in parallel, rather thansequentially, implying that models of language ac-quisition should also incorporate joint learning.
Jointmodels investigate the interaction between differentlevels of linguistic structure during learning.
Theseinteractions are often (but not necessarily) synergis-tic, enabling better, more robust, learning by makinguse of cues from multiple sources.
Recent modelsusing joint learning to model language acquisitionhave spanned various domains including phonology,word segmentation, syntax and semantics (Feldmanet al 2009; Elsner et al 2012; Doyle and Levy,2013; Johnson, 2008; Kwiatkowski et al 2012).In this paper we examine the joint learning ofsyntactic categories and morphology, which are ac-quired by children at roughly the same age (Clark,2003b), implying possible interactions in the learn-ing process.
Both morphology and word order de-pend on categorising words based on their morpho-syntactic function.
However, previous models ofsyntactic category learning have relied principallyon surrounding context, i.e., word order constraints,whereas models of morphology use word-internalcues.
Our joint model integrates both sources ofinformation, allowing the model to flexibly weighthem according to their utility.Languages differ in the richness of their mor-phology and strictness of word order.
These char-acteristics appear to be (anti)correlated, with richmorphology co-occurring with free word order andvice versa (Blake, 2001; McFadden, 2003).
Thetimecourse of acquisition is also influenced by lan-guage typology: learners of morphologically richlanguages become productive in morphology ear-lier (Xanthos et al 2011), suggesting that richermorphology may be more salient for learners thanimpoverished morphology.
Sentence comprehensionin children also shows cross-linguistic differencesin the cues used to make sense of non-canonicalsentence structure: learners of a morphologicallyrich language (Turkish) disregard word order in30favour of morphology, whereas learners of En-glish favour word order (Slobin, 1982; MacWhin-ney et al 1984).
These interactions between mor-phology and word order suggest that a joint modelwill be better able to support the differences in cuestrength (rich morphology versus strict word order),and thus be more language-general, than single-taskmodels.Both syntactic category and morphology induc-tion have been the focus of much recent work.
(SeeHammarstro?m and Borin (2011) for an overviewof unsupervised morphology learning, likewiseChristodoulopoulos et al(2010) for a comparisonof part of speech/syntactic category induction sys-tems.)
However, given the tightly coupled nature ofthese two tasks, there has been surprisingly littlework in joint learning of morphology and syntac-tic categories.
Systems for inducing syntactic cat-egories often make use of morpheme-like features,such as word-final characters (Smith and Eisner,2005; Haghighi and Klein, 2006; Berg-Kirkpatricket al 2010; Lee et al 2010), or model wordsat the character-level (Clark, 2003a; Blunsom andCohn, 2011), but do not include morphemes ex-plicitly.
Other systems (Dasgupta and Ng, 2007;Christodoulopoulos et al 2011) use morphologi-cal segmentations learned by a separate morphologymodel as features in a pipeline approach.Models of morphology induction generally oper-ate over a lexicon, i.e.
a list of word types, ratherthan token corpora (Goldsmith, 2006; Creutz andLagus, 2007; Kurimo et al 2010).
These modelsfind morphological categories on the basis of word-internal features, without taking syntactic contextinto account (which is of course not available in alexicon).Lee et al(2011) and Sirts and Aluma?e (2012)present models that infer morphological segmenta-tions and syntactic categories jointly, although Leeet al(2011) do not evaluate the inferred syntacticcategories.
Both make use of a word-type constraintwhich limits each word form to a single analysis(i.e., all instances of ducks are assigned to a singlecategory and will have the same morpheme analy-sis, ignoring the gold standard distinction between aplural noun and third person singular verb).
This canmake inference more tractable, and often increasesperformance, but does not respect the ambiguity in-herent in natural language, both over syntactic cat-egories and morphological analyses.
The degree ofambiguity is language dependent, so that even if atype-constraint is perhaps relatively unproblematicin English, it will pose problems in morphologicallyricher languages.
Furthermore, these two modelsmake use of an array of heuristics that may not allowthem to be easily generalisable across languages anddatasets (e.g., likelihood scaling (Sirts and Aluma?e,2012), sequential suffix matching (Lee et al 2011)).In this paper, we present a joint model composedof two well-known individual models.
This allowsus to cleanly investigate the effects of joint learningand its potential benefits over the single task models.The simplicity of our models also allows us to avoidmodelling and inference heuristics.Previous models have used adult-directed writtentexts, which differs significantly from the type oflanguage available to child learners.
We test our jointmodel on child-directed utterances in English (amorphologically poor language) and Spanish (withricher morphology)1.
Our results indicate that ourjoint model is able to flexibly accommodate lan-guages with differing levels of morphological rich-ness.
The joint model matches the performance ofsingle task models on both tasks, demonstrating thatthe additional complexity is not a problem (i.e., itdoes not add noise).
Moreover, the joint model im-proves performance significantly on the task corre-sponding to the language?s weaker cue, indicating atransfer of information from the stronger cue.
Thefact that the nature of this improvement varies bylanguage provides evidence that joint learning caneffectively accommodate typological diversity.2 ModelThe task is to assign word tokens to part of speechcategories and simultaneously segment the tokensinto morphemes.
We assume a relatively simple yetcommonly used concatenative morphology whichmodels a word as a stem plus (possibly null) suffix2.1There are languages with much richer morphology thanSpanish, but none with a child-directed corpus suitably anno-tated for evaluation.2Fullwood and O?Donnell (2013) recently presented amodel of non-concatenative morphology that could be inte-grated into this model; however, it does not perform well on En-glish (and presumably other mostly concatenative languages).31Since this is an unsupervised model, the inferred cat-egories and morphemes lack meaningful labels, butideally will correspond to gold standard categoriesand morphemes.2.1 Word OrderWe model a sequence of words as a Hidden MarkovModel (HMM) with a non-parametric emission dis-tribution.
As usual, the latent states of the HMM rep-resent syntactic categories.
The tag sequence is gen-erated by a trigram Dirichlet-multinomial distribu-tion, where transition parameters ?
are drawn froma symmetric Dirichlet distribution with the hyperpa-rameter ?t .
Each tag ti in the sequence is then drawnfrom the transition distribution conditioned on theprevious two tags:?
(t,t ?)
?
Dir(?t)ti = t|ti?1 = t?, ti?2 = t??,??
Mult(?
(t ?,t ??
))This model is token-based, permitting differenttokens of the same word type to have differentsyntactic categories.
Most recent models have in-cluded a constraint forcing all tokens of a giventype into the same category, which improves per-formance but often complicates inference.
TheBayesian HMM?s performance is therefore not state-of-the-art, but is comparable to other token-basedmodels (Christodoulopoulos et al 2010) and themodel is easy to extend within the Bayesian frame-work, allowing us to compare multiple versions.This part of the model is parametric, operat-ing over a fixed number of tags T , and is iden-tical to the formulation of tag transitions in theBayesian HMM (Goldwater and Griffiths, 2007).However, we replace the BHMM?s emission dis-tribution with the morphologically-informed distri-butions described below.
As in the BHMM, theemission distributions are conditioned on the tag,i.e., each tag has its own morphology.2.2 MorphologyThe morphology model introduced by Goldwateret al(2006) generates morphological analyses for aset of tokens.
These analyses consist of a tag plus astem and suffix pair, which are concatenated to formthe observed words.
Both stem s and suffix f aregenerated from Dirichlet-multinomials conditionedon the tag t:??
Dir(??)t|??
Mult(?)??
Dir(?s)s|t,??
Mult(?t)??
Dir(?
f )f |t,??
Mult(?t)The ?s are hyperparameters governing the Dirich-let distributions from which the multinomials ?,?,?are drawn.
In turn, t,s, and f are drawn from thesemultinomials.The probability of a word under this model is thesum of the probabilities of all possible analyses l =(t,s, f ):P0(w) =?lP0(l) = ?t,s, f s.t.s?
f=wP(s|t)P( f |t)P(t) (1)where s?
f = w denotes that the concatenation ofstem and suffix results in the word w.On its own, this distribution over morphologi-cal analyses makes independence assumptions thatare too strong: most word tokens of a word typehave the same analysis, but P0 will re-generatethat analysis for every token.
To resolve this prob-lem, a Pitman-Yor process (PYP) is placed over thegenerating distribution above.
The Pitman-Yor pro-cess has been found to be useful for representingthe power-law distributions common in natural lan-guage (Teh, 2006; Goldwater and Griffiths, 2007;Blunsom and Cohn, 2011).The distribution of draws from a Pitman-Yor pro-cess (which, in our case, determines the distribu-tion of word tokens with each morphological anal-ysis) is commonly described using the metaphor ofa Chinese restaurant.
A series of customers (tokensz = z1 .
.
.zN) enter a restaurant with an infinite num-ber of initially empty tables.
Upon entering, eachcustomer is seated at a table k with probabilityp(zi = k|z1 .
.
.zi?1,a,b) = (2){nk?ai?1+b if 1?
k ?
KKa+bi?1+b if k = K +132tk skfk lkKziwiNFigure 1: Plate diagram depicting the morphology model(adapted from Goldwater et al(2006)).
Hyperparametershave been omitted for clarity.
The left-hand plate depictsthe base distribution P0; note that the morphological anal-yses lk are generated deterministically as (tk,sk, fk).
Theobserved words wi are also deterministic given zi = k andlk, since wi = sk?
fk.where nk is the number of customers already sittingat table k, K is the total number of tables occupied bythe i?1 previous customers, and 0?
a < 1 and b?
0are hyperparameters of the process.
The probabilityof being seated at a table increases with the numberof customers already seated at that table, creating a?rich-get-richer?
power-law distribution of tokens totables; a and b control the amount of reuse of exist-ing tables, with smaller values leading to more reuse.Crucially, each table serves a dish generated bythe base distribution P0?i.e., the dish is a morpho-logical analysis lk = (t,s, f )?and all the customersseated at the same table share the same dish, whichis generated only once (at the point when that tableis first occupied).
The model can thus reuse the anal-ysis for a particular word and avoid regenerating thesame analysis multiple times.
Note that multiple ta-bles may have identical analyses, lk = lk?
.
Figure 1illustrates how the full PYP morphology model gen-erates the observed sequence of word tokens.2.3 Combined ModelThe full model (Figure 2) combines the latent tag se-quence with the morphology model.
Tag tokens aregenerated conditioned on local context, not the basedistribution, as in the morphology model.
Instead ofa single PYP generating morphological analyses forall tokens, as in the Goldwater et al(2006) model,we have a separate PYP for each tag type, i.e., eachtag has its own restaurant with its own customers(the tokens labeled with that tag) and its own mor-phological analyses.
The distribution of customersti?2 ti?1 ti ziwisk fklkNKtTFigure 2: Plate diagram depicting the joint model.
Hyper-parameters have been omitted for clarity.
The L-shapedplate contains the tokens, while the square plates containthe morphological analyses.
The t are latent tags, zi is anassignment to a morphological analysis lk = (sk, fk), andwi is the observed word.
T is the number of distinct tags,and Kt the number of tables used by tag type t.in each of the tag-specific restaurants is still deter-mined by Equation 2, except that all of the countsand indices are with respect to only the tokens andtables assigned to that tag.Each tag-specific PYP (restaurant) also has a sep-arate base distribution, P(t)0 , resulting in distinct dis-tributions over stems and suffixes for each tag.
Theanalyses generated by the base distributions consistof (stem, suffix) pairs; the tag is given by the identityof the generating PYP.P(t)0 (w) =?lP(t)0 (l = (s, f )) = ?s, f s.t.s?
f=wP(s|t)P( f |t)(3)The full joint posterior distribution of a sequenceof words, tags, and morpheme analyses is shown inFigure 3.
Note that all tag-specific morphology mod-els share the same Pitman-Yor parameters a and b.3 InferenceWe use Gibbs sampling for inference over the threesets of discrete variables: tags t, their assignments tomorphological analyses (tables) z, and the analysesthemselves l.Each iteration of the sampler has two stages: Firstthe morphological analyses l are sampled, and theneach token samples a new tag and a new assignmentto an analysis/table.
Because the table assignments33P(t, l,z|?t ,a,b,?s,?
f ) =P(t|?t)P(l|t,?s,?
f )P(z|a,b) (4)P(t|?t) =N?i=2P(ti|ti?1, ti?2,t1...i?1,?t) =T?t,t ?=1?(T?t)?
(ntt ?
+T?t)T?t ??=1?
(ntt ?t ??
+?t)?(?t)(5)P(l|t,?s,?
f ) =T?t=1Kt?k=1Pt(lk = (s, f )|l1...k?1,?s,?
f ) (6)=T?t=1?(S?s)?
(mt +S?s)?(F?
f )?
(mt +F?
f )S?s=1?
(mts +?s)?(?s)F?f=1?
(mt f +?
f )?(?
f )(7)P(z|a,b) =T?t=1Nt?i=1P(zi|t,z1...i?1,a,b) (8)=T?t=1?(1+b)?
(nt +b)Kt?k=1(ka+b)?(nk?a)?
(1?a)(9)Figure 3: The posterior distribution of our joint model.
Because the sequence of words w is deterministic givenanalyses l and assignments to analyses (tables) z, the joint posterior over all variables P(w,t, l,z|?t ,a,b,?s,?
f ) isequal to P(t, l,z|?t ,a,b,?s,?
f ) when lzi = wi for all i, and 0 otherwise.
We give equations for the non-zero case.
nsrefer to token counts, ms to table counts.
We add two dummy tokens at the start, end, and between sentences to padthe context history.are conditioned on tags (i.e., a token must be as-signed to a table in the correct PYP restaurant) re-sampling the tag requires immediate resampling ofthe table assignment as well.3.1 InitializationThe tags are initialized uniformly at random.
Foreach token, a segmentation point is chosen uni-formly at random (we disallow segmentations witha null stem).
If this segmentation is new within thePYP associated with that token?s tag, a new table iscreated for the token in that PYP.
If it matches an ex-isting analysis, zi is sampled from the existing tablesk plus a possible new table k?.3.2 Morphological AnalysesEach lk represents the morphological analysis for theset of tokens assigned to table k. Resampling thesegmentation point (stem and suffix identity) of theanalysis changes the segmentation of all of the wordtokens assigned to that analysis.
Note that the tag isnot included in lk in the combined model, becausethe tag identity is dependent on the local contexts ofall the tokens seated at the table.Analyses are sampled from a product of Dirichlet-multinomial posteriors as follows:p(lk = (s, f )|t, l\k) =m\ks +?sm\k +S?sm\kf +?
fm\k +F?
f(10)where ms and m f are the number of analyses forthis tag that share a stem or suffix with lk, and mis the total number of analyses for this tag.
S andF are the total number of stems and suffixes in themodel.
l\k indicates that the current analysis lk hasbeen removed from the distribution and the appro-priate counts, to create the correct conditioning dis-tribution for the Gibbs sampler.3.3 TagsTags are sampled from the product of posteri-ors of the transition and emission distributions.The transition distribution is a standard Dirichlet-multinomial posterior.
Calculating the emission dis-tribution probability, i.e.
the marginal probability ofthe word given the tag, involves summing over theprobability of all the existing tables in the given PYPthat emit the correct word, plus the probability ofa new table being created, which also includes theprobability of a new analysis from P(t)0 .34More precisely, tags are sampled from the follow-ing distribution:p(ti = t|wi = w,t\i,z\i, l,?t ,a,b) (11)?
p(ti = t|ti?1, ti?2,t\i,?t)?
p(w|t,z\i, l)= p(ti = t|ti?1, ti?2,t\i,?t)?
( ?k s.t.
lk=wp(zi = k|t,w,z\i)+ p(zi = knew|t,w,z\i))=nti?2ti?1t +?tnti?2ti?1 +T?t?
( ?k s.t.
lk=wnk?ant +b+Kta+bnt +bP(t)0 (w))where lk = w matches tables compatible with w,i.e., the concatenation of stem and suffix form theword, slk ?
flk = w. nk is the number of words as-signed to the table k and Kt is the total number oftables in the PYP for tag t. Note that all counts areobtained after the removal of the current ti and zi,i.e., from t\i and z\i.3.4 Table AssignmentsOnce a new tag has been sampled for a token, the ta-ble assignment must be resampled conditioned onthe new tag.
The assignment zi is drawn over allcompatible tables in the tag?s PYP (that is, wherelk = w), plus a possible new table:p(zi = k|ti = t,w,z\i,a,b) ?
(12){nk?ant+bif 1?
k ?
KtKt a+bnt+bP(t)0 (w) if k = Kt +1P(t)0 is calculated by summing over the probabilityof all possible segmentations for a new analysis forword wi, using Equation 3.
If a new table is drawn(k > Kt) then we also sample a new analysis for thattable from P(t)0 .4 Preliminary ExperimentsAn important argument for joint learning is that itaffords increased flexibility and robustness across awider range of input data.
A model that relies onword order cannot learn syntactic categories from amorphologically complex language with free wordorder; likewise a model attempting to categorisewords using morphology alone will fail on a lan-guage without morphology.
An effective joint modelLanguage Aabdc fefh pomo rtut usstcdcc bcba gghh npop npoocdca aaaa fefh hfeg pnonLanguage Bnoom.no usrs.st bbdb.ac cbab.cc cdaa.ccrttt.uu cbab.aa mnom.oo ccda.bc onmm.omrruu.ts npop.mm gehg.fh trrt.uu tssu.uuTable 1: Example sentences in the synthetic languages.Words in Category 1 are made of characters a-d, Cate-gory 2 e-h, Category 3 m-p, Category 4 r-u.
Suffixes inLanguage B are separated with periods (.)
for illustrativepurposes only.will be able to make use of the different cues in bothlanguage types in a flexible way.In order to test the proposed model, we run twoexperiments on synthetic languages, which simulatelanguages in which either word order or morphologyis the sole cue.
Most natural languages fall betweenthese extremes, but these experiments show that ourmodel can capture the full spectrum.Language A is a strict word order language lack-ing morphology.
It has a vocabulary of 200 wordtypes, split into four different categories.
The 50word types in each category are created by com-bining four letters, with replacement, into four-letterwords, with a different set of letters used in each cat-egory3.
Words within a category may thus share be-ginning or ending characters, which could be positedas stems or suffixes by the model, but since only50 of 256 possible strings are used, there will beno strong evidence for consistent stem and suffixes(i.e.
stems appearing with multiple suffixes and viceversa).
Each sentence in Language A consists of fivewords in one of twenty possible category sequences.In these sequences, each category is either followedby itself or the next category (i.e.
[2,2,2,3,4] is validbut [2,4,3,1,4] is not).
Word order is thus stronglyconstrained by category membership.Language B has free word order, with categorymembership signalled by suffixes.
Words are cre-3We achieved the same results with a language using thesame four characters in all categories, but using different char-acters makes the categories human-readable.
The model doesnot have a orthographic/phonological component and so willnot recognise the within-category similarity, other than possi-bly positing spurious stems or suffixes.35-70000-60000-50000-40000-30000-20000-1000000  200  400  600  800  1000LogProbabilityIterationLangA TotalLangA TransitionsLangA MorphologyLangB TotalLangB TransitionsLangB MorphologyFigure 4: Log probability of the sampler state over 1000iterations on Languages A and B.ated by the concatenation of a stem and a suffix,where the stems are the same as the words in lan-guage A (50 stems in each of four categories).
Oneof six category-specific suffixes is appended to eachstem, resulting in 300 word types per category.
Eachsuffix is two letters long, created by combining threepossible letters (the same letters used to create thestems), thus making mis-segmentation possible (forinstance, up to three of the suffixes could have thesame final letter).
Sentences are again five wordslong, but the sequence of categories is drawn at ran-dom, resulting in uniformly random word order.
SeeTable 1 for example sentences in both languages.We create a 5000 word corpus for each language,and run our model on these corpora.
Hyperparame-ters are set to the same values in both languages4.We run the sampler on each dataset for 1000 it-erations with simulated annealing.
In both cases,the correct solution is found by iteration 500.
Fig-ure 4 shows that the morphology component con-tinues to increase the log probability by increasingthe number of tokens seated at a table.
Note thatthe correct solution in Language A involves learn-ing a very peaked transition distribution as well as aneven more extreme distribution over suffixes (whereonly the null suffix has high probability), whereasthe same distributions in Language B are much flat-ter.
The fact that the same hyperparameter setting is4The PYP parameters are set to a = 0.1,b = 1.0 and theHMM transition parameter ?t = 1.0; the parameters in the basedistribution are ?s,?
f = 0.001,?k = 0.5.able to correctly identify the two language extremesindicates that the model is robust to hyperparametervalues.These experiments demonstrate that our jointmodel is able to learn correctly even when only ei-ther morphology or word order is informative in alanguage.
We now turn to acquisition data from nat-ural languages in which both morphology and wordorder are useful cues but to varying degrees.5 CDS Experiments5.1 DataWe use two corpora, Eve (Brown, 1973) and Or-nat (Ornat, 1994), from the CHILDES database(MacWhinney, 2000).
These corpora consist of thechild-directed utterances heard by two children,the former learning English and the latter Spanish.These have been annotated for part of speech cate-gories and morphemes.The CHILDES corpora are tagged with a very richset of part of speech tags (74 tags), which we col-lapse to a smaller set of tags5.
The Eve corpus has61224 tokens and is thus larger than the Spanish cor-pus, which has 40497 tokens.
However, the Englishcorpus has only 17 gold suffix types, while Spanishhas 83.
The increased richness of Spanish morphol-ogy also has an effect on the number of word types inthe corpus: the Spanish dataset has 3046 word types,whereas the larger English dataset has only 1957.Morphology is annotated using a stem-affix en-coding which does not directly correspond to oursegmentation-based model.
The word running is an-notated as run-ING, jumping as jump-ING; the anno-tation is thus agnostic about ortho-morphemic seg-mentation (i.e., whether to segment as run.ning orrunn.ing), whereas the model is forced to choosea segmentation point.
Syncretic suffixes (sharingan identical surface form) are disambiguated: singsis annotated as sing-3S, plums as plum-PL.
Con-versely, the annotation scheme merges allomorphsinto a single suffix: infinitive verbs in Spanish,for instance, are encoded as ending with -INF,corresponding to -ar, -er, and -ir surface forms.5These are 13 for English (ADJ, ADV, AUX, CONJ, DET,INF, NOUN, NEG, OTH, PART, PREP, PRO, VERB) and 10for Spanish, since the gold standard does not distinguish AUX,PART or INF.36We ignore irregular/non-affixing forms annotatedwith & (e.g.
was, annotated as be&PAST) anduse only hyphen-separated suffixes to evaluate.Where multiple suffixes are concatenated together(e.g., dog-DIM-PL) we treat this as a single suffix(-DIM-PL) for evaluation purposes.In Spanish, many words are annotated as havinga suffix of effectively zero length, e.g.
the imper-ative gusta is annotated as gusta-2S&IMP.
We re-place these suffixes (where the stem is equal to theword) with a null suffix, excluding them from eval-uation, as they are impossible for a segmentation-based model to find.5.2 EvaluationTags are evaluated using VM (Rosenberg andHirschberg, 2007), as has become standard for thistask (Christodoulopoulos et al 2010).
VM is a mea-sure of the normalised cross-entropy between goldand proposed clusters; it ranges between 0 and 100,with higher scores being better.We also use VM to evaluate the morphologicalsegmentation: all tokens with a common suffix areclustered together, and these clusters are comparedagainst the gold suffix clusters6.
Using a clusteringmetric avoids the need to evaluate against a gold seg-mentation point (which the annotation lacks).
Tagmembership is added to the non-null model suffixes,so that a final -s suffix found in tag 2 is distinguishedfrom the same suffix found in tag 8 (creating suffixes-s-T8 and -s-T2), analogous to the gold annotationdistinction between syncretic morphemes -PL and-3S.Note that ceiling performance of our model onSuffix VM will be below 100, since our model can-not cluster allomorphs, which are represented by asingle abstract morpheme in the gold standard.5.3 BaselinesWe test the full model, MORTAG, against a numberof variations to investigate the advantages of jointlymodelling the two tasks.Two variants remove the transition distributions,and thus local syntactic context, from the model.6We also evaluated stem morpheme clusters and found near-ceiling performance due to the high number of null-suffix wordsin both corpora.MORTAGNOTRANS is the full model without tran-sitions between tag tokens; morphology PYP drawsremain conditioned on token tags.
We add a Dirich-let prior over tags (?t = 0.1) to encourage tag spar-sity (analogous to the transition distribution in thefull model).
MORCLUSTERS is the original modelof Goldwater et al(2006), in which tags (calledclusters in the original) are drawn by P0.MORTAGNOSEG is a variant in which the onlyavailable suffix is the null suffix; thus segmentationsare trivial and only tags are inferred.
This modelis approximately equivalent to a simple BayesianHMM but with the addition of PYPs within theemission distribution.
We also evaluate against tagsfound by the BHMM, with a Dirichlet-multinomialemission distribution and no morphology.MORTAGTRUETAGS is the full model but with alltags fixed to their gold values.
This model gives usoracle-type results for morphology.
(Due to the an-notation scheme used in CHILDES, oracle morpho-logical segmentations are unavailable, so we wereunable to test a model with gold morphology and in-ferred tags.
)5.4 Experimental ProcedureHyperparameter values for the Pitman-Yor processwere found using grid search on a development set(Section 10 of Eve and Section 8 of Ornat; these sec-tions are removed from the dataset we report resultson).
We use the values which give the best SuffixVM performance on the development data; howeverwe stress that the development results did not varygreatly over a wide range of hyperparameter values,and only deteriorated significantly at extreme valuesof a.There are a number of other hyperparameters inthe model which we set to fixed values.
The transi-tion hyperparameter ?t is set to 0.1 in all models.We set the hyperparameters for the stem and suf-fix distributions in the morphology base distributionP0 to 0.001 for both ?s and ?
f ; ?k over tags in theMORCLUSTERS model is set to 0.5.
The number ofpossible stems and suffixes is given by the dataset: inthe Eve dataset there are 5339 candidate stems and6617 candidate suffixes; in the Ornat dataset thesenumbers are 8649 and 6598, respectively.
The num-ber of tags available to the model is set to the numberof gold tags in the data.37Tag VM Suffix VMMORTAG 59.1(1.9) 41.9(10.0)MORCLUSTERS 22.4(1.0)?
28.0(11.9)?MORTAGNOTRANS 19.3(1.2)?
24.4(5.2)?MORTAGNOSEG 59.4(1.7) ?BHMM 56.2(2.3)?
?MORTAGTRUETAGS ?
42.5(5.2)Table 2: English Eve corpus results.
Standard deviationsare in parentheses; ?
denotes a significant difference fromthe MORTAG model.Sampling is run for 5000 iterations with anneal-ing.
Inspection of the posterior log-likelihood indi-cates that the models converge after about 1000 it-erations.
We run inference over all models ten timesand report the average performance.
Significance isreported using the non-parametric Wilcoxon rank-sum test with a significance level of ?< 0.05.5.5 Results: EnglishResults on the English Eve corpus are shown in Ta-ble 2.
We use PYP parameters a = 0.3 and b = 10,though we found similar performance over a widerange of values of a and b.
Our results show a clearimprovement in the morphological segmentationsfound by the joint model and stable tagging perfor-mance across all models with context information.The syntactic clusters found by models usingonly morphological patterns, MORTAGNOTRANSand MORCLUSTERS, are clearly inferior and lead tolow Tag VM results.
The models with local syntac-tic context all perform approximately equally wellin terms of finding tags.
We find no improvement ontagging performance in English when adding mor-phology, compared to the MORTAGNOSEG base-line in which words are not segmented.
However, wedo see a small but significant improvement over theBHMM for both of these models, due to the replace-ment of the multinomial emission distribution in theBHMM with the PYP.Morphological segmentations, as measured bySuffix VM, clearly improve with the addition of lo-cal contexts (and the ensuing better tags): the fullmodel outperforms the baselines without syntacticcontexts.
On this dataset, the joint MORTAG modeleven matches the performance of the model us-Tag VM Suffix VMMORTAG 43.4(2.6) 41.4(2.5)MORCLUSTERS 20.3(2.5)?
46.5(3.2)MORTAGNOTRANS 14.4(1.7)?
36.4(2.0)?MORTAGNOSEG 39.6(3.7)?
?BHMM 36.4(0.7)?
?MORTAGTRUETAGS ?
59.8(0.4)?Table 3: Spanish Ornat corpus results.
Standard devia-tions are in parentheses; ?
denotes a significant differencefrom the MORTAG model.ing oracle tags.
The standard deviation over Suf-fix VM scores is quite large for MORTAG andMORCLUSTERS; this is due to frequent words hav-ing two high probability segmentations (most no-tably is, which in some runs was segmented as i.s).5.6 Results: SpanishFor the Spanish Ornat corpus, we found slightly dif-ferent optimal PYP hyperparameters and set a = 0.1and b = 0.1.
Results are shown in Table 3.The Spanish results pattern in the opposite wayas English.
Here we see a statistically significantimprovement in tagging performance of the fulljoint model over both models without morphology(MORTAGNOSEG and BHMM).
Models withoutcontext information again find much worse tags,mainly because (as in English) function words arenot identifiable by suffixes.However, the full model does not find better mor-phological segmentations than the MORCLUSTERSmodel, despite better tags (the two models?
SuffixVM scores are not statistically significantly differ-ent).
We also see that the difference between the seg-mentations found by the model using gold tags andestimated tags is quite large.
This is due to the ora-cle model finding the rarer suffixes which were notdistinguished by the models with noisier tags.
Thisdemonstrates the importance of syntactic categorisa-tion for the morpheme induction task, and suggeststhat a more sophisticated tagging model (with betterperformance) may yet improve morpheme segmen-tation performance in Spanish.386 ConclusionWe have presented a model of joint syntactic cate-gory and morphology induction.
Operating within agenerative Bayesian framework means that combin-ing single-task components is straightforward andwell-founded.
Our model is token-based, allowingfor syntactic and morphemic ambiguity.To our knowledge, this is the first joint model tobe tested on child-directed speech data, which is lesscomplex than the newswire corpora used by previ-ous joint models.
Child-directed speech may be sim-ple enough for joint learning not to be necessary: ourresults indicate the contrary, namely that joint learn-ing is indeed helpful when learning from realisticacquisition data.We tested this model on two languages with dif-ferent morphological characteristics.
On English, alanguage with relatively little morphology, espe-cially in child directed speech, we found that bet-ter categorisation of words yielded much better mor-phology in terms of suffixes learned.
Conversely, inSpanish we saw less difference on the morphologytask between models with categories inferred solelyfrom morphemic patterns and models that also usedlocal syntactic context for categorisation.
However,in Spanish we saw an improvement in the taggingtask when morphology information was included.This suggests that English and Spanish make dif-ferent word-order and morphology trade-offs.
In En-glish, local context provides at least as much in-formation as morphology in terms of determiningthe correct syntactic category, but knowing a goodestimate of the correct syntactic category is use-ful for determining a word?s morphology.
In Span-ish, a word?s morphology can more easily be deter-mined simply by looking at frequent suffixes withina purely morphological system.
On the other hand,word order is freer, making local syntactic contextunreliable, so taking morphological information intoaccount can improve tagging.
These differences be-tween languages demonstrate the benefits of jointlearning, which enables the learner to more flexiblyutilise the information available in the input data.ReferencesTaylor Berg-Kirkpatrick, Alexandre Bouchard-Cote,John DeNero, and Dan Klein.
Painless unsuper-vised learning with features.
In Proceedings of theNorth American Association for ComputationalLinguistics (NAACL), 2010.Barry J. Blake.
Case.
Cambridge University Press,2001.Phil Blunsom and Trevor Cohn.
A hierarchicalPitman-Yor process HMM for unsupervised partof speech induction.
In Proceedings of the 49thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), 2011.Roger Brown.
A first language: The early stages.Harvard University Press, Cambridge, MA, 1973.Christos Christodoulopoulos, Sharon Goldwater,and Mark Steedman.
Two decades of unsuper-vised POS induction: How far have we come?
InProceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics (ACL),2010.Christos Christodoulopoulos, Sharon Goldwater,and Mark Steedman.
A Bayesian mixture modelfor part-of-speech induction using multiple fea-tures.
In Proceedings of the 16th Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), 2011.Alexander Clark.
Combining distributional andmorphological information for part of speech in-duction.
In Proceedings of the 10th annual Meet-ing of the European Association for Computa-tional Linguistics (EACL), 2003a.Eve V. Clark.
First Language Acquisition.
Cam-bridge University Press, 2003b.Mathias Creutz and Krista Lagus.
Unsupervisedmodels for morpheme segmentation and morphol-ogy learning.
ACM Transactions on Speech andLanguage Processing, 4(1):1?34, 2007.Sajib Dasgupta and Vincent Ng.
Unsupervised part-of-speech acquisition for resource-scarce lan-guages.
In Proceedings of the 12th Conferenceon Empirical Methods in Natural Language Pro-cessing (EMNLP), 2007.Gabriel Doyle and Roger Levy.
Combining multi-ple information types in Bayesian word segmenta-tion.
In Proceedings of NAACL-HLT 2013, pages117?126, 2013.39Micha Elsner, Sharon Goldwater, and Jacob Eisen-stein.
Bootstrapping a unified model of lexicaland phonetic acquisition.
In Proceedings of the50th Annual Meeting of the Association for Com-putational Linguistics (ACL), 2012.Naomi Feldman, Thomas Griffiths, and James Mor-gan.
Learning phonetic categories by learning alexicon.
In Proceedings of the 31st Annual Con-ference of the Cognitive Science Society (CogSci),2009.Michelle A. Fullwood and Timothy J. O?Donnell.Learning non-concatenative morphology.
In Pro-ceedings of the Workshop on Cognitive Modelingand Computational Linguistics, 2013.John Goldsmith.
An algorithm for the unsupervisedlearning of morphology.
Natural Language Engi-neering, 12(4):353?371, December 2006.Sharon Goldwater and Thomas L. Griffiths.
Afully Bayesian approach to unsupervised part-of-speech tagging.
In Proceedings of the 45th An-nual Meeting of the Association for Computa-tional Linguistics (ACL), 2007.Sharon Goldwater, Thomas L. Griffiths, and MarkJohnson.
Interpolating between types and to-kens by estimating power-law generators.
In Ad-vances in Neural Information Processing Systems18, 2006.Aria Haghighi and Dan Klein.
Prototype-drivengrammar induction.
In Proceedings of the 44thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), 2006.Harald Hammarstro?m and Lars Borin.
Unsupervisedlearning of morphology.
Computational Linguis-tics, 37(2):309?350, 2011.Mark Johnson.
Using Adaptor Grammars to iden-tify synergies in the unsupervised acquisition oflinguistic structure.
In Proceedings of the 46thAnnual Meeting of the Association for Computa-tional Linguistics (ACL), 2008.Mikko Kurimo, Sami Virpioja, and Ville T. Turunen.Proceedings of the MorphoChallenge 2010 work-shop.
Technical Report TKK-ICS-R37, AaltoUniversity School of Science and Technology, Es-poo, Finland, 2010.Tom Kwiatkowski, Sharon Goldwater, Luke Zettel-moyer, and Mark Steedman.
A probabilisticmodel of syntactic and semantic acquisition fromchild-directed utterances and their meanings.
InProceedings of the 13th Conference of the Eu-ropean Chapter of the Association for Computa-tional Linguistics (EACL), 2012.Yoong Keok Lee, Aria Haghighi, and Regina Barzi-lay.
Simple type-level unsupervised POS tagging.In Proceedings of the 48th Annual Meeting of theAssociation for Computational Linguistics (ACL),2010.Yoong Keok Lee, Aria Haghighi, and Regina Barzi-lay.
Modeling syntactic context improves mor-phological segmentation.
In Proceedings ofFifteenth Conference on Computational NaturalLanguage Learning, 2011.Brian MacWhinney.
The CHILDES Project: Toolsfor Analyzing Talk.
Lawrence Erlbaum Asso-ciates, Mahwah, NJ, 2000.Brian MacWhinney, Elizabeth Bates, and ReinholdKliegl.
Cue validity and sentence interpretationin English, German, and Italian.
Journal of Ver-bal Learning and Verbal Behavior, 23:127?150,1984.Thomas McFadden.
On morphological case andword-order freedom.
In Proceedings of the An-nual Meeting of the Berkeley Linguistics Society,volume 29, pages 295?306, 2003.S.
Lopez Ornat.
La adquisicion de la lengua espag-nola.
Siglo XXI, Madrid, 1994.Andrew Rosenberg and Julia Hirschberg.
V-measure: A conditional entropy-based externalcluster evaluation measure.
In Proceedings of the12th Conference on Empirical Methods in Natu-ral Language Processing (EMNLP), 2007.Kairit Sirts and Tanel Aluma?e.
A hierarchicalDirichlet process model for joint part-of-speechand morphology induction.
In Proceedings ofthe Conference of the North American Chapterof the Association for Computational Linguistics(NAACL), 2012.Dan Slobin.
Universal and particular in the acqui-sition of language.
In Eric Wanner and Lila R.Gleitman, editors, Language acquisition: the state40of the art, pages 128?170.
Cambridge UniversityPress, 1982.Noah A. Smith and Jason Eisner.
Contrastive esti-mation: Training log-linear models on unlabeleddata.
In Proceedings of the 43rd Annual Meetingof the Association for Computational Linguistics(ACL), 2005.Yee Whye Teh.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Pro-ceedings of the 44th Annual Meeting of the As-sociation for Computational Linguistics (ACL),2006.Aris Xanthos, Sabine Laaha, Steven Gillis, UrsulaStephany, Ayhan Aksu-Koc?, Anastasia Christofi-dou, Natalia Gagarina, Gordana Hrzica, F. Ni-han Ketrez, Marianne Kilani-Schoch, KatharinaKorecky-Kro?ll, Melita Kovac?evic?, Klaus Laalo,Marijan Palmovic?, Barbara Pfeiler, Maria D.Voeikova, and Wolfgang U. Dressler.
On the roleof morphological richness in the early develop-ment of noun and verb inflection.
First Language,31(4):461?479, 2011.41
