Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 183?188,October 25-29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsLearning Hierarchical Translation SpansJingyi Zhang1,2, Masao Utiyama3, Eiichro Sumita3, Hai Zhao1,21Center for Brain-Like Computing and Machine Intelligence, Department of ComputerScience and Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China2Key Laboratory of Shanghai Education Commission for Intelligent Interactionand Cognitive Engineering, Shanghai Jiao Tong Unviersity, Shanghai, 200240, China3National Institute of Information and Communications Technology3-5Hikaridai, Keihanna Science City, Kyoto, 619-0289, Japanzhangjingyizz@gmail.com, mutiyama/eiichiro.sumita@nict.go.jp,zhaohai@cs.sjtu.edu.cnAbstractWe propose a simple and effective ap-proach to learn translation spans forthe hierarchical phrase-based translationmodel.
Our model evaluates if a sourcespan should be covered by translationrules during decoding, which is integratedinto the translation system as soft con-straints.
Compared to syntactic con-straints, our model is directly acquiredfrom an aligned parallel corpus and doesnot require parsers.
Rich source sidecontextual features and advanced machinelearning methods were utilized for thislearning task.
The proposed approach wasevaluated on NTCIR-9 Chinese-Englishand Japanese-English translation tasks andshowed significant improvement over thebaseline system.1 IntroductionThe hierarchical phrase-based (HPB) translationmodel (Chiang, 2005) has been widely adopted instatistical machine translation (SMT) tasks.
TheHPB translation rules based on the synchronouscontext free grammar (SCFG) are simple and pow-erful.One drawback of the HPB model is the appli-cations of translation rules to the input sentenceare highly ambiguous.
For example, a rule whoseEnglish side is ?X1 by X2?
can be applied to anyword sequence that has ?by?
in them.
In Figure 1,this rule can be applied to the whole sentence aswell as to ?experiment by tomorrow?.In order to tackle rule application ambiguities,a few previous works used syntax trees.
Chi-ang (2005) utilized a syntactic feature in the HPBI  will  nish  this  experiment  by  tomorrow?
?
?
??
??
??
??
?
?Figure 1: A translation example.model, which represents if the source span cov-ered by a translation rule is a syntactic constituent.However, the experimental results showed thisfeature gave no significant improvement.
Insteadof using the undifferentiated constituency feature,(Marton and Resnik, 2008) defined different softsyntactic features for different constituent typesand obtained substantial performance improve-ment.
Later, (Mylonakis and Sima?an, 2011) in-troduced joint probability synchronous grammarsto integrate flexible linguistic information.
(Liuet al., 2011) proposed the soft syntactic constraintmodel based on discriminative classifiers for eachconstituent type and integrated all of them into thetranslation model.
(Cui et al., 2010) focused onhierarchical rule selection using many features in-cluding syntax constituents.These works have demonstrated the benefits ofusing syntactic features in the HPB model.
How-ever, high quality syntax parsers are not alwayseasily obtained for many languages.
Without thisproblem, word alignment constraints can also beused to guide the application of the rules.Suppose that we want to translate the Englishsentence into the Chinese sentence in Figure 1, atranslation rule can be applied to the source span?finish this experiment by tomorrow?.
Nonethe-less, if a rule is applied to ?experiment by?, thenthe Chinese translation can not be correctly ob-tained, because the target span projected from ?ex-183periment by?
contains words projected from thesource words outside ?experiment by?.In general, a translation rule projects one con-tinuous source word sequence (source span) intoone continuous target word sequence.
Meanwhile,the word alignment links between the source andtarget sentence define the source spans wheretranslation rules are applicable.
In this paper, wecall a source span that can be covered by a trans-lation rule without violating word alignment linksa translation span.Translation spans that have been correctly iden-tified can guide translation rules to function prop-erly, thus (Xiong et al., 2010) attempted to useextra machine learning approaches to determineboundaries of translation spans.
They used twoseparate classifiers to learn the beginning and end-ing boundaries of translation spans, respectively.A source word is marked as beginning (ending)boundary if it is the first (last) word of a translationspan.
However, a source span whose first and lastwords are both boundaries is not always a transla-tion span.
In Figure 1, ?I?
is a beginning boundarysince it is the first word of translation span ?I will?and ?experiment?
is an ending boundary since it isthe last word of translation span ?finish this exper-iment?
, but ?I will finish this experiment?
is not atranslation span.
This happens because the trans-lation spans are nested or hierarchical.
Note that(He et al., 2010) also learned phrase boundaries toconstrain decoding, but their approach identifiedboundaries only for monotone translation.In this paper, taking fully into account thattranslation spans being nested, we propose anapproach to learn hierarchical translation spansdirectly from an aligned parallel corpus thatmakes more accurate identification over transla-tion spans.The rest of the paper is structured as follows:In Section 2, we briefly review the HPB transla-tion model.
Section 3 describes our approach.
Wedescribe experiments in Section 4 and conclude inSection 5.2 Hierarchical Phrase-based TranslationChiang?s HPB model is based on a weightedSCFG.
A translation rule is like: X ?
?
?, ?,?
?,where X is a nonterminal, ?
and ?
are source andtarget strings of terminals and nonterminals, and?is a one-to-one correspondence between nontermi-nals in ?
and ?.
The weight of each rule is:w (X ?
?
?, ?,??)
=?tht(X ?
?
?, ?,??
)?t(1)where htare the features defined on the rules.Rewriting begins with a pair of linked start sym-bols and ends when there is no nonterminal left.Let D be a derivation of the grammar, f (D) ande (D) be the source and target strings generatedby D. D consists of a set of triples ?r, i, j?, eachof which stands for applying a rule r on a spanf (D)ji.
The weight of D is calculated as:w (D) =??r,i,j?
?Dw (r)?
Plm(e)?lm?
exp (?
?wp|e|)(2)where w (r) is the weight of rule r, the last twoterms represent the language model and wordpenalty, respectively.3 Learning Translation SpansWe will describe how to learn translation spans inthis section.3.1 Our ModelWe make a series of binary classifiers{C1, C2, C3, ...} to learn if a source spanf (D)jishould be covered by translation rules dur-ing translation.
Ckis trained and tested on sourcespans whose lengths are k, i.e., k = j ?
i+ 1.1Cklearns the probabilityPk(v|f (D) , i, j) (3)where v ?
{0, 1}, v = 1 represents a rule is ap-plied on f (D)ji, otherwise v = 0.Training instances for these classifiers are ex-tracted from an aligned parallel corpus accordingto Algorithm 1.
For example, ?I will?
and ?willfinish?
are respectively extracted as positive andnegative instances in Figure 1.Note that our model in Equation 3 only usesthe source sentence f (D) in the condition.
Thismeans that the probabilities can be calculated be-fore translation.
Therefore, the predicted prob-abilities can be integrated into the decoder con-veniently as soft constraints and no extra time isadded during decoding.
This enables us to userich source contextual features and various ma-chine learning methods for this learning task.1We indeed can utilize just one classifier for all sourcespans.
However, it will be difficult to design features for sucha classifier unless only boundary word features are adopted.On the contrary, we can fully take advantage of rich informa-tion about inside words as we turn to the fixed span lengthapproach.1843.2 Integration into the decoderIt is straightforward to integrate our model intoEquation 2.
It is extended asw (D) =??r,i,j?
?Dw (r)?
Plm(e)?lm?
exp (??wp|e|)?
Pk(v = 1|f (D) , i, j)?k(4)where ?kis the weight for Ck.During decoding, the decoder looks up theprobabilities Pkcalculated and stored before de-coding.Algorithm 1 Extract training instances.Input: A pair of parallel sentence fn1and em1withword alignments A.Output: Training examples for {C1, C2, C3, ...}.1: for i = 1 to n do2: for j = i to n do3: if ?eqp, 1 ?
p ?
q ?
m& ?
(k, t) ?
A, i ?
k ?
j, p ?
t ?
q& ?
(k, t) ?
A, i ?
k ?
j ?
p ?
t ?
qthen4: fjiis a positive instance for Cj?i+15: else6: fjiis a negative instance for Cj?i+17: end if8: end for9: end for3.3 ClassifiersWe compare two machine learning methods forlearning a series of binary classifiers.For the first method, each Ckis individuallylearned using the maximum entropy (ME) ap-proach (Berger et al., 1996):Pk(v|f (D) , i, j) =exp(?t?tht(v, f (D) , i, j))?v?exp(?t?tht(v?, f (D) , i, j))(5)where htis a feature function and ?tis weightof ht.
We use rich source contextual fea-tures: unigram, bigram and trigram of the phrase[fi?3, ..., fj+3].As the second method, these classification tasksare learned in the continuous space using feed-forward neural networks (NNs).
Each Ckhasthe similar structure with the NN language model(Vaswani et al., 2013).
The inputs to the NN areindices of the words: [fi?3, ..., fj+3].
Each sourceword is projected into an N dimensional vector.The output layer has two output neurons, whosevalues correspond to Pk(v = 0|f (D) , i, j) andPk(v = 1|f (D) , i, j).For both ME and NN approaches, words thatoccur only once or never occur in the trainingcorpus are treated as a special word ?UNK?
(un-known) during classifier training and predicting,which can reduce training time and make the clas-sifier training more smooth.4 ExperimentWe evaluated the effectiveness of the proposed ap-proach for Chinese-to-English (CE) and Japanese-to-English (JE) translation tasks.
The datasets of-ficially provided for the patent machine translationtask at NTCIR-9 (Goto et al., 2011) were used inour experiments.
The detailed training set statis-tics are given in Table 1.
The development and testSOURCE TARGETCE#Sents 954k#Words 37.2M 40.4M#Vocab 288k 504kJE#Sents 3.14M#Words 118M 104M#Vocab 150k 273kTable 1: Data sets.sets were both provided for CE task while only thetest set was provided for JE task.
Therefore, weused the sentences from the NTCIR-8 JE test setas the development set.
Word segmentation wasdone by BaseSeg (Zhao et al., 2006; Zhao and Kit,2008; Zhao et al., 2010; Zhao and Kit, 2011; Zhaoet al., 2013) for Chinese and Mecab2for Japanese.To learn the classifiers for each translation task,the training set and development set were put to-gether to obtain symmetric word alignment us-ing GIZA++ (Och and Ney, 2003) and the grow-diag-final-and heuristic (Koehn et al., 2003).
Thesource span instances extracted from the alignedtraining and development sets were used as thetraining and validation data for the classifiers.The toolkit Wapiti (Lavergne et al., 2010) wasadopted to train ME classifiers using the classi-cal quasi-newton optimization algorithm with lim-ited memory.
The NNs are trained by the toolkitNPLM (Vaswani et al., 2013).
We chose ?recti-fier?
as the activation function and the logarithmicloss function for NNs.
The number of epochs wasset to 20.
Other parameters were set to default2http://sourceforge.net/projects/mecab/files/185SpanlengthCE JERateME NNRateME NNP N P N P N P N1 2.67 0.93 0.63 0.93 0.64 1.08 0.85 0.79 0.86 0.802 1.37 0.83 0.70 0.82 0.75 0.73 0.69 0.84 0.71 0.873 0.86 0.70 0.80 0.73 0.83 0.52 0.56 0.89 0.63 0.904 0.62 0.57 0.81 0.67 0.88 0.36 0.48 0.93 0.54 0.935 0.48 0.52 0.90 0.61 0.91 0.26 0.30 0.96 0.47 0.956 0.40 0.47 0.91 0.58 0.92 0.20 0.25 0.97 0.41 0.967 0.34 0.40 0.93 0.53 0.93 0.16 0.14 0.98 0.33 0.978 0.28 0.35 0.94 0.46 0.94 0.13 0 1 0.32 0.979 0.22 0.28 0.96 0.37 0.96 0.10 0 1 0.25 0.9810 0.15 0.21 0.97 0.28 0.97 0.08 0 1 0.23 0.99Table 2: Classification accuracies.
The Rate column represents ratio of positive instances to negativeinstances; the P and N columns give classification accuracies for positive and negative instances.values.
The training time of one classifier on a12-core 3.47GHz Xeon X5690 machine was 0.5h(2.5h) using ME (NN) approach for CE task; 1h(4h) using ME (NN) approach for JE task .The classification results are shown in Table 2.Instead of the undifferentiated classification accu-racy, we present separate classification accuraciesfor positive and negative instances.
The big differ-ence between classification accuracies for positiveand negative instances was caused by the unbal-anced rate of positive and negative instances in thetraining corpus.
For example, if there are morepositive training instances, then the classifier willtend to classify new instances as positive and theclassification accuracy for positive instances willbe higher.
In our classification tasks, there are lesspositive instances for longer span lengths.Since the word order difference of JE task ismuch more significant than that of CE task, thereare more negative Japanese translation span in-stances than Chinese.
In JE tasks, the ME classi-fiers C8, C9and C10predicted all new instances tobe negative due to the heavily unbalanced instancedistribution.As shown in Table 2, NN outperformed ME ap-proach for our classification tasks.
As the spanlength growing, the advantage of NN becamemore significant.
Since the classification accura-cies deceased to be quite low for source spans withmore than 10 words, only {C1, ..., C10} were inte-grated into the HPB translation system.For each translation task, the recent versionof Moses HPB decoder (Koehn et al., 2007)with the training scripts was used as the base-line (Base).
We used the default parameters forMoses, and a 5-gram language model was trainedon the target side of the training corpus by IRSTLM Toolkit3with improved Kneser-Ney smooth-ing.
{C1, ..., C10} were integrated into the base-line with different weights, which were tuned byMERT (Och, 2003) together with other featureweights (language model, word penalty,...) underthe log-linear framework (Och and Ney, 2002).BLEU-n n-gram precisionsMethod TER 4 1 2 3 4CEBase 49.39- - 33.07- - 69.9/40.7/25.8/16.9BLM 48.60 33.93 70.0/41.4/26.6/17.6ME 49.02- 33.63- 70.0/41.2/26.3/17.4NN 48.09++ 34.35++ 70.1/41.9/27.0/18.0JEBase 57.39- - 30.13- - 67.1/38.3/23.0/14.0BLM 56.79 30.81 67.7/38.9/23.6/14.5ME 56.48 31.01 67.6/39.0/23.8/14.7NN 55.96++ 31.77++ 67.8/39.7/24.6/15.4Table 3: Translation results.
The symbol ++ (- -)represents a significant difference at the p < 0.01level and - represents a significant difference at thep < 0.05 level against the BLM.We compare our method with the baseline andthe boundary learning method (BLM) (Xiong etal., 2010) based on Maximum Entropy MarkovModels with Markov order 2.
Table 3 reportsBLEU (Papineni et al., 2002) and TER (Snoveret al., 2006) scores.
Significance tests are con-ducted using bootstrap sampling (Koehn, 2004).Our ME classifiers achieve comparable translationimprovement with the BLM and NN classifiers en-hance translation system significantly compared toothers.
Table 3 also shows that the relative gainwas higher for higher n-grams, which is reason-able since the higher n-grams have higher ambi-guities in the translation rule application.It is true that because of multiple parallel sen-tences, a source span can be applied with transla-3http://hlt.fbk.eu/en/irstlm186tion rules in one sentence pair but not in anothersentence pair.
So we used the probability scoreas a feature in the decoding.
That is, we did notuse classification results directly but use the prob-ability score for softly constraining the decodingprocess.5 ConclusionWe have proposed a simple and effective transla-tion span learning model for HPB translation.
Ourmodel is learned from aligned parallel corpora andpredicts translation spans for source sentence be-fore translating, which is integrated into the trans-lation system conveniently as soft constraints.
Wecompared ME and NN approaches for this learn-ing task.
The results showed that NN classifiers onthe continuous space model achieved both higherclassification accuracies and better translation per-formance with acceptable training times.AcknowledgmentsHai Zhao were partially supported by CSC fund(201304490199), the National Natural ScienceFoundation of China (Grant No.60903119, GrantNo.61170114, and Grant No.61272248), the Na-tional Basic Research Program of China (GrantNo.2013CB329401), the Science and Technol-ogy Commission of Shanghai Municipality (GrantNo.13511500200), the European Union SeventhFramework Program (Grant No.247619), and theart and science interdiscipline funds of Shang-hai Jiao Tong University, a study on mobilizationmechanism and alerting threshold setting for on-line community, and media image and psychologyevaluation: a computational intelligence approach.ReferencesAdam Berger, Vincent Della Pietra, and Stephen DellaPietra.
1996.
A maximum entropy approach to nat-ural language processing.
Computational linguis-tics, 22(1):39?71.David Chiang.
2005.
A hierarchical phrase-basedmodel for statistical machine translation.
In Pro-ceedings of the 43rd Annual Meeting on Associationfor Computational Linguistics, pages 263?270.
As-sociation for Computational Linguistics.Lei Cui, Dongdong Zhang, Mu Li, Ming Zhou, andTiejun Zhao.
2010.
A joint rule selection modelfor hierarchical phrase-based translation.
In Pro-ceedings of the ACL 2010 Conference Short Papers,pages 6?11.
Association for Computational Linguis-tics.Isao Goto, Bin Lu, Ka Po Chow, Eiichiro Sumita, andBenjamin K Tsou.
2011.
Overview of the patentmachine translation task at the ntcir-9 workshop.
InProceedings of NTCIR, volume 9, pages 559?578.Zhongjun He, Yao Meng, and Hao Yu.
2010.
Learn-ing phrase boundaries for hierarchical phrase-basedtranslation.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 383?390.
Association for Computational Lin-guistics.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics on Human Language Technology-Volume 1, pages 48?54.
Association for Computa-tional Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, et al.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,pages 177?180.
Association for Computational Lin-guistics.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In EMNLP, pages388?395.Thomas Lavergne, Olivier Capp?e, and Franc?ois Yvon.2010.
Practical very large scale crfs.
In Proceed-ings of the 48th Annual Meeting of the Associationfor Computational Linguistics, pages 504?513, Up-psala, Sweden, July.
Association for ComputationalLinguistics.Lemao Liu, Tiejun Zhao, Chao Wang, and HailongCao.
2011.
A unified and discriminative soft syn-tactic constraint model for hierarchical phrase-basedtranslation.
In the Thirteenth Machine TranslationSummit, pages 253?260.
Asia-Pacific Associationfor Machine Translation.Yuval Marton and Philip Resnik.
2008.
Soft syntac-tic constraints for hierarchical phrased-based trans-lation.
In ACL, pages 1003?1011.Markos Mylonakis and Khalil Sima?an.
2011.
Learn-ing hierarchical translation structure with linguis-tic annotations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Lin-guistics: Human Language Technologies-Volume 1,pages 642?652.
Association for Computational Lin-guistics.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for sta-tistical machine translation.
In Proceedings of the40th Annual Meeting on Association for Computa-tional Linguistics, pages 295?302.
Association forComputational Linguistics.187Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate trainingin statistical machine translation.
In Proceedings ofthe 41st Annual Meeting on Association for Compu-tational Linguistics-Volume 1, pages 160?167.
As-sociation for Computational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th annual meeting on association for compu-tational linguistics, pages 311?318.
Association forComputational Linguistics.Matthew Snover, Bonnie Dorr, Richard Schwartz, Lin-nea Micciulla, and John Makhoul.
2006.
A study oftranslation edit rate with targeted human annotation.In Proceedings of association for machine transla-tion in the Americas, pages 223?231.Ashish Vaswani, Yinggong Zhao, Victoria Fossum,and David Chiang.
2013.
Decoding with large-scale neural language models improves translation.In Proceedings of the 2013 Conference on Em-pirical Methods in Natural Language Processing,pages 1387?1392, Seattle, Washington, USA, Oc-tober.
Association for Computational Linguistics.Deyi Xiong, Min Zhang, and Haizhou Li.
2010.Learning translation boundaries for phrase-baseddecoding.
In Human Language Technologies: The2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 136?144.
Association for Computa-tional Linguistics.Hai Zhao and Chunyu Kit.
2008.
Exploiting unla-beled text with different unsupervised segmentationcriteria for chinese word segmentation.
Research inComputing Science, 33:93?104.Hai Zhao and Chunyu Kit.
2011.
Integrating unsu-pervised and supervised word segmentation: Therole of goodness measures.
Information Sciences,181(1):163?183.Hai Zhao, Chang-Ning Huang, and Mu Li.
2006.
Animproved chinese word segmentation system withconditional random field.
In Proceedings of the FifthSIGHAN Workshop on Chinese Language Process-ing, pages 162?165.
Sydney: July.Hai Zhao, Chang-Ning Huang, Mu Li, and Bao-LiangLu.
2010.
A unified character-based tagging frame-work for chinese word segmentation.
ACM Trans-actions on Asian Language Information Processing(TALIP), 9(2):5.Hai Zhao, Masao Utiyama, Eiichiro Sumita, and Bao-Liang Lu.
2013.
An empirical study on word seg-mentation for chinese machine translation.
In Com-putational Linguistics and Intelligent Text Process-ing, pages 248?263.
Springer.188
