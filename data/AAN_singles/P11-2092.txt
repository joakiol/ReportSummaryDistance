Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 524?528,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsImproved Modeling of Out-Of-Vocabulary Words Using MorphologicalClassesThomas Mu?ller and Hinrich Schu?tzeInstitute for Natural Language ProcessingUniversity of Stuttgart, Germanymuellets@ims.uni-stuttgart.deAbstractWe present a class-based language model thatclusters rare words of similar morphologytogether.
The model improves the predic-tion of words after histories containing out-of-vocabulary words.
The morphological fea-tures used are obtained without the use of la-beled data.
The perplexity improvement com-pared to a state of the art Kneser-Ney model is4% overall and 81% on unknown histories.1 IntroductionOne of the challenges in statistical language mod-eling are words that appear in the recognition taskat hand, but not in the training set, so called out-of-vocabulary (OOV) words.
Especially for produc-tive language it is often necessary to at least reducethe number of OOVs.
We present a novel approachbased on morphological classes to handling OOVwords in language modeling for English.
Previouswork on morphological classes in English has notbeen able to show noticeable improvements in per-plexity.
In this article class-based language mod-els as proposed by Brown et al (1992) are used totackle the problem.
Our model improves perplex-ity of a Kneser-Ney (KN) model for English by 4%,the largest improvement of a state-of-the-art modelfor English due to morphological modeling that weare aware of.
A class-based language model groupswords into classes and replaces the word transitionprobability by a class transition probability and aword emission probability:P (w3|w1w2) = P (c3|c1c2) ?
P (w3|c3).
(1)Brown et al and many other authors primarily usecontext information for clustering.
Niesler et al(1998) showed that context clustering works betterthan clusters based on part-of-speech tags.
How-ever, since the context of an OOV word is unknownand it therefore cannot be assigned to a cluster, OOVwords are as much a problem to a context-basedclass model as to a word model.
That is why weuse non-distributional features ?
features like mor-phological suffixes that only depend on the shape ofthe word itself ?
to design a new class-based modelthat can naturally integrate unknown words.In related work, factored language models(Bilmes and Kirchhoff, 2003) were proposed tomake use of morphological information in highlyinflecting languages such as Finnish (Creutz et al,2007), Turkish (Creutz et al, 2007; Yuret and Bic?ici,2009) and Arabic (Creutz et al, 2007; Vergyri etal., 2004) or compounding languages like German(Berton et al, 1996).
The main idea is to replacewords by sequences of factors or features and toapply statistical language modeling to the resultingfactor sequences.
If, for example, words were seg-mented into morphemes, an unknown word wouldbe split into an unseen sequence, which could be rec-ognized using discounting techniques.
However, ifone morpheme, e.g.
the stem, is unknown to the sys-tem, the fundamental problem remains unsolved.Our class-based model uses a number of featuresthat have not been used in factored models (e.g.,shape and length features) and achieves ?
in con-trast to factored models ?
good perplexity gains forEnglish.524is capital(w) first character of w is an uppercase letteris all capital(w) ?
c ?
w : c is an uppercase lettercapital character(w) ?
c ?
w : c is an uppercase letterappears in lowercase(w) ?capital character(w) ?
w?
?
?Tspecial character(w) ?
c ?
w : c is not a letter or digitdigit(w) ?
c ?
w : c is a digitis number(w) w ?
L([+ ?
?
][0 ?
9] (([., ][0 ?
9])|[0 ?
9]) ?
)not special(w) ?
(special character(w) ?
digit(w) ?
is number(w))Table 1: Predicates of the capitalization and special character groups.
?T is the vocabulary of the training corpus T ,w?
is obtained from w by changing all uppercase letters to lowercase and L(expr) is the language generated by theregular expression expr.2 Morphological FeaturesThe feature vector of a word consists of four partsthat represent information about suffixes, capitaliza-tion, special characters and word length.
For thesuffix group, we define a binary feature for eachof the 100 most frequent suffixes learned on thetraining corpus by the Reports algorithm (Keshava,2006), a general purpose unsupervised morphologylearning algorithm.
One additional binary feature isused for all other suffixes learned by Reports, in-cluding the empty suffix.The feature groups capitalization and specialcharacters are motivated by the analysis shown inTable 2.
Our goal is to improve OOV modeling.The table shows that most OOV words (f = 0) arenumbers (CD), names (NP), and nouns and adjec-tives (NN, NNS, JJ).
This distribution is similar tohapax legomena (f = 1), but different from the POSdistribution of all tokens.
Capitalization and specialcharacter features are of obvious utility in identify-ing the POS classes NP and CD since names in En-glish are usually capitalized and numbers are writ-ten with digits and special characters such as commaand period.
To capture these ?shape?
properties of aword, we define the features listed in Table 1.The fourth feature group is length.
Short wordsoften have unusual distributional properties.
Exam-ples are abbreviations and bond credit ratings likeAaa.
To represent this information in the lengthpart of the vector, we define four binary features forlengths 1, 2, 3 and greater than 3.
The four partsof the vector (suffixes, capitalization, special char-acters, length) are weighted equally by normalizingthe subvector of each subgroup to unit length.We designed the four feature groups to groupword types to either resemble POS classes or to in-duce an even finer sub-partitioning.
UnsupervisedPOS clustering is a hard task in English and it is vir-tually impossible if a word?s context (which is notavailable for OOV items) is not taken into account.For example, there is no way we can learn that ?the?and ?a?
are similar or that ?child?
has the same re-lationship to ?children?
as ?kid?
does to ?kids?.
Butas our analysis in Table 2 shows, part of the benefitof morphological analysis for OOVs comes from anappropriate treatment of names and numbers.
Thesuffix feature group is useful for categorizing OOVnouns and adjectives because there are very few ir-regular morphemes like ?ren?
in children in Englishand OOV words are likely to be regular words.So even though morphological learning based onthe limited information we use is not possible in gen-eral, it can be partially solved for the special case ofOOV words.
Our experimental results in Section 5confirm that this is the case.
We also testes prefixesand features based on word stems.
However, theyproduced inferior clustering solutions.3 The Language ModelAs mentioned before in the literature, e.g.
by Mal-tese and Mancini (1992), class-based models onlyoutperform word models in cases of insufficientdata.
That is why we use a frequency-based ap-proach and only include words below a certain to-ken frequency threshold ?
in the clustering process.A second motivation is that the contexts of low fre-quency words are more similar to the expected con-texts of OOV words.Given a training corpus, all words with a fre-525tag types tokensf = 1 f = 0 (OOV)CD 0.39 0.38 0.05NP 0.35 0.35 0.14NN 0.10 0.10 0.17NNS 0.05 0.06 0.07JJ 0.05 0.06 0.07V* 0.04 0.05 0.15?
0.98 0.99 0.66Table 2: Proportion of dominant POS for types with train-ing set frequencies f ?
{0, 1} and for tokens.
V* consistsof all verb POS tags.quency below the threshold ?
are partitioned intok clusters using the bisecting k-means algorithm(Steinbach et al, 2000).
The cluster of an OOVword w can be defined as the cluster whose centroidis closest to the feature vector of w. The formerlyremoved high-frequency words are added as single-ton clusters to produce a complete clustering.
How-ever, OOV words can only be assigned to the orig-inal k-means clusters.
Over this clustering a class-based trigram model can be defined, as introducedby Brown et al (1992).
The word transition proba-bility of such a model is given by equation 1, whereci denotes the cluster of the word wi.
The classtransition probability P (c3|c1c2) is estimated usingthe unsmoothed maximum likelihood estimate.
Theemission probability is defined as follows:P (w3|c3) =????
?1 if c(w3) > ?
(1 ?
?)
c(w3)Pw?c3c(w) if ??c(w3)>0?
if c(w3) = 0where c(w) is the frequency of w in the training set.?
is estimated on held-out data.
The morphologi-cal language model is then interpolated with a modi-fied Kneser-Ney trigram model.
In this interpolationthe parameters ?
depend on the cluster c2 of the his-tory word w2, i.e.
:P (w3|w1w2) = ?
(c2) ?
PM (w3|w1w2)+ (1 ?
?
(c2)) ?
PKN (w3|w1w2).This setup may cause overfitting as every high fre-quent word w2 corresponds to a singleton class.
Agrouping of several words into equivalence classescould therefore further improve the model; this,however, is beyond the scope of this article.
We es-timate optimal parameters ?
(c2) using the algorithmdescribed by Bahl et al (1991).4 Experimental SetupWe compare the performance of the described modelwith a Kneser-Ney model and an interpolated modelbased on part-of-speech (POS) tags.
The relation be-tween words and POS tags is many-to-many, but wetransform it to a many-to-one relation by labelingevery word ?
independent of its context ?
with itsmost frequent tag.
OOV words are treated equallyeven though their POS classes would not be knownin a real application.
Treetagger (Schmid, 1994) wasused to tag the entire corpus.The experiments are carried out on a Wall StreetJournal (WSJ) corpus of 50 million words that issplit into training set (80%), valdev (5%), valtst(5%), and test set (10%).
The number of distinct fea-ture vectors in training set, valdev and validation set(valdev+valtst) are 632, 466, and 512, respectively.As mentioned above, the training set is used to learnsuffixes and the maximum likelihood n-gram esti-mates.
The unknown word rate of the validation setis ?
?
0.028.We use two setups to evaluate our methods.
Thefirst uses valdev for parameter estimation and valtstfor testing and the second the entire validation set forparameter estimation and the test set for testing.
Allmodels with a threshold greater or equal to the fre-quency of the most frequent word type are identical.We use ?
as the threshold to refer to these models.In a similar manner, the cluster count ?
denotes aclustering where two words are in the same clusterif and only if their features are identical.
This is thefinest possible clustering of the feature vectors.5 ResultsTable 3 shows the results of our experiments.
TheKN model yields a perplexity of 88.06 on valtst (toprow).
For small frequency thresholds overfitting ef-fects cause that the interpolated models are worsethan the KN model.
We can see that a clusteringof the feature vectors is not necessary as the differ-ences between all cluster models are small and c?is the overall best model.
Surprisingly, morphologi-cal clustering and POS classes are close even though526?
cPOS c1 c50 c100 c?0 88.06 88.06 88.06 88.06 88.061 89.74 89.84 89.73 89.74 89.745 89.07 89.36 89.07 89.06 89.0710 88.59 89.01 88.58 88.57 88.5850 86.72 87.58 86.69 86.68 86.68102 85.92 87.06 85.92 85.91 85.89103 84.43 86.88 84.83 84.77 84.56104 85.22 87.59 85.89 85.73 85.26105 86.82 87.99 87.44 87.32 86.79?
87.31 88.06 87.96 87.92 87.62?
cPOS c1 c50 c100 c?0 813.50 813.50 813.50 813.50 813.501 181.25 206.17 182.78 183.62 184.435 152.51 185.54 154.52 152.98 153.8310 147.48 186.12 149.34 147.98 147.4850 146.21 203.10 142.21 140.67 140.46102 149.06 215.54 143.95 142.48 141.67103 173.91 279.02 164.22 159.04 150.13104 239.72 349.54 221.42 208.85 180.57105 317.13 373.98 318.04 297.18 236.90?
348.76 378.38 366.92 357.80 292.34Table 3: Perplexities for different frequency thresholds ?
and cluster models.
In the left table, perplexity is calculatedover all events P (w3|w1w2) of the valtst set.
On the right side, the subset of events where w1 or w2 are unknown istaken into account.
The overall best results for class models and POS models are highlighted in bold.the POS class model uses oracle information to as-sign the right POS to an unknown word.
The optimalthreshold is ?
= 103 ?
the bolded perplexity values84.43 and 84.56; that means that only 1.35% of theword types were excluded from the morphologicalclustering (86% of the tokens).
The improvementover the KN model is 4%.In a second evaluation we reduce the perplexitycalculations to predictions of the form P (w3|w1w2)where w1 or w2 are OOV words.
On such an eventthe KN model has to back off to a bigram or evenunigram estimate, which results in inferior predic-tions and higher perplexity.
The perplexity for theKN model is 813.50 (top row).
A first observationis that the perplexity of model c1 starts at a goodvalue, but worsens with rising values for ?
?
10.The reason is the dominance of proper nouns andcardinal numbers at a frequency threshold of one andin the distribution of OOV words (cf.
Table 2).
Thec1 model with ?
= 1 is specialized for predictingwords after unknown nouns and cardinal numbersand two thirds of the unknown words are of exactlythat type.
However, with rising ?, other word classesget a higher influence and different probability dis-tributions are superimposed.
The best morphologi-cal model c?
reduces the KN perplexity of 813.50to 140.46 (bolded), an improvement of 83%.As a final experiment, we evaluated our methodon the test set.
In this case, we used the entirevalidation set for parameter tuning (i.e., valdev andvaltst).
The overall perplexity of the KN model is88.28, the perplexities for the best POS and c?
clus-ter model for ?
= 1000 are 84.59 and 84.71 respec-tively, which corresponds again to an improvementof 4%.
For unknown histories the KN model per-plexity is 767.25 and the POS and c?
cluster modelperplexities at ?
= 50 are 150.90 and 144.77.
Thus,the morphological model reduces perplexity by 81%compared to the KN model.6 ConclusionWe have presented a new class-based morphologicallanguage model.
In an experiment the model outper-formed a modified Kneser-Ney model, especially inthe prediction of the continuations of histories con-taining OOV words.
The model is entirely unsuper-vised, but works as well as a model using part-of-speech information.Future Work.
We plan to use our model for do-main adaptation in applications like machine trans-lation.
We then want to extend our model to otherlanguages, which could be more challenging, as cer-tain languages have a more complex morphologythan English, but also worthwhile, if the unknownword rate is higher.
Preliminary experiments onGerman and Finnish show promising results.
Themodel could be further improved by using contex-tual information for the word clustering and traininga classifier based on morphological features to as-sign OOV words to these clusters.Acknowledgments.
This research was funded byDFG (grant SFB 732).
We would like to thank Hel-mut Schmid and the anonymous reviewers for theirvaluable comments.527ReferencesLalit R. Bahl, Peter F. Brown, Peter V. de Souza,Robert L. Mercer, and David Nahamoo.
1991.
A fastalgorithm for deleted interpolation.
In Speech Com-munication and Technology, pages 1209?1212.Andre Berton, Pablo Fetter, and Peter Regel-Brietzmann.1996.
Compound words in large-vocabulary Germanspeech recognition systems.
In Spoken Language, vol-ume 2, pages 1165 ?1168 vol.2, October.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InHuman Language Technology, NAACL ?03, pages 4?6.
Association for Computational Linguistics.Peter F. Brown, Peter V. de Souza, Robert L. Mercer, Vin-cent J. Della Pietra, and Jenifer C. Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional Linguistics, 18:467?479, December.Mathias Creutz, Teemu Hirsima?ki, Mikko Kurimo, AnttiPuurula, Janne Pylkko?nen, Vesa Siivola, Matti Var-jokallio, Ebru Arisoy, Murat Sarac?lar, and AndreasStolcke.
2007.
Morph-based speech recognitionand modeling of out-of-vocabulary words across lan-guages.
ACM Transactions on Speech and LanguageProcessing, 5:3:1?3:29, December.Samarth Keshava.
2006.
A simpler, intuitive approachto morpheme induction.
In PASCAL Challenge Work-shop on Unsupervised Segmentation of Words intoMorphemes, pages 31?35.Giulio Maltese and Federico Mancini.
1992.
An auto-matic technique to include grammatical and morpho-logical information in a trigram-based statistical lan-guage model.
In Acoustics, Speech, and Signal Pro-cessing, volume 1, pages 157 ?160 vol.1, March.Thomas R. Niesler, Edward W.D.
Whittaker, andPhilip C. Woodland.
1998.
Comparison of part-of-speech and automatically derived category-based lan-guage models for speech recognition.
In Acoustics,Speech and Signal Processing, volume 1, pages 177?180 vol.1, May.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In New Methods in Lan-guage Processing, pages 44?49.Michael Steinbach, George Karypis, and Vipin Kumar.2000.
A comparison of document clustering tech-niques.
In KDD Workshop on Text Mining.Dimitra Vergyri, Katrin Kirchhoff, Kevin Duh, and An-dreas Stolcke.
2004.
Morphology-based languagemodeling for Arabic speech recognition.
In SpokenLanguage Processing, pages 2245?2248.Deniz Yuret and Ergun Bic?ici.
2009.
Modeling morpho-logically rich languages using split words and unstruc-tured dependencies.
In International Joint Conferenceon Natural Language Processing, pages 345?348.
As-sociation for Computational Linguistics.528
