Proceedings of the Fourth Workshop on Statistical Machine Translation , pages 42?46,Athens, Greece, 30 March ?
31 March 2009. c?2009 Association for Computational LinguisticsCombining Multi-Engine Translations with MosesYu Chen1, Michael Jellinghaus1, Andreas Eisele1,2,Yi Zhang1,2,Sabine Hunsicker1, Silke Theison1, Christian Federmann2, Hans Uszkoreit1,21: Universita?t des Saarlandes, Saarbru?cken, Germany2: Deutsches Forschungszentrum fu?r Ku?nstliche Intelligenz GmbH, Saarbru?cken, Germany{yuchen,micha,yzhang,sabineh,sith}@coli.uni-saarland.de{eisele,cfedermann,uszkoreit}@dfki.deAbstractWe present a simple method for generatingtranslations with the Moses toolkit (Koehnet al, 2007) from existing hypotheses pro-duced by other translation engines.
Asthe structures underlying these translationengines are not known, an evaluation-based strategy is applied to select sys-tems for combination.
The experimentsshow promising improvements in terms ofBLEU.1 IntroductionWith the wealth of machine translation systemsavailable nowadays (many of them online andfor free), it makes increasing sense to investigateclever ways of combining them.
Obviously, themain objective lies in finding out how to integratethe respective advantages of different approaches:Statistical machine translation (SMT) and rule-based machine translation (RBMT) systems of-ten have complementary characteristics.
Previouswork on building hybrid systems includes, amongothers, approaches using reranking, regenerationwith an SMT decoder (Eisele et al, 2008; Chenet al, 2007), and confusion networks (Matusov etal., 2006; Rosti et al, 2007; He et al, 2008).The approach by (Eisele et al, 2008) aimedspecifically at filling lexical gaps in an SMT sys-tem with information from a number of RBMTsystems.
The output of the RBMT engines wasword-aligned with the input, yielding a total ofseven phrase tables which where simply concate-nated to expand the phrase table constructed fromthe training corpus.
This approach differs from theconfusion network approaches mainly in that thefinal hypotheses do not necessarily follow any ofthe input translations as the skeleton.
On the otherhand, it emphasizes that the additional translationsshould be produced by RBMT systems with lexi-cons that cannot be learned from the data.The present work continues on the same trackas the paper mentioned above but implements anumber of important changes, most prominentlya relaxation of the restrictions on the number andtype of input systems.
These differences are de-scribed in more detail in Section 2.
Section 3 ex-plains the implementation of our system and Sec-tion 4 its application in a number of experiments.Finally, Section 5 concludes this paper with a sum-mary and some thoughts on future work.2 Integrating Multiple Systems ofUnknown Type and QualityWhen comparing (Eisele et al, 2008) to thepresent work, our proposal is more general in away that the requirement for knowledge about thesystems is minimum.
The types and the identitiesof the participated systems are assumed unknown.Accordingly, we are not able to restrict ourselvesto a certain class of systems as (Eisele et al, 2008)did.
We rely on a standard phrase-based SMTframework to extract the valuable pieces from thesystem outputs.
These extracted segments are alsoused to improve an existing SMT system that wehave access to.While (Eisele et al, 2008) included translationsfrom all of a fixed number of RBMT systemsand added one feature to the translation model foreach system, integrating all given system outputsin this way in our case could expand the searchspace tremendously.
Meanwhile, we cannot relyon the assumption that all candidate systems ac-tually have the potential to improve our baseline.This implies the need for a first step of system se-lection where the best candidate systems are iden-tified and a limited number of them is chosen to beincluded in the combination.
Our approach wouldnot work without a small set of tuning data beingavailable so that we can evaluate the systems forlater selection and adjust the weights of our sys-tems.
Such tuning data is included in this year?s42task.In this paper, we use the Moses decoder to con-struct translations from the given system outputs.We mainly propose two slightly different ways:One is to construct translation models solely fromthe given translations and the other is to extendan existing translation model with these additionaltranslations.3 ImplementationDespite the fact that the output of current MT sys-tems is usually not comparable in quality to hu-man translations, the machine-generated transla-tions are nevertheless ?parallel?
to the input sothat it is straightforward to construct a translationmodel from data of this kind.
This is the spiritbehind our method for combining multiple trans-lations.3.1 Direct combinationClearly, for the same source sentence, we expectto have different translations from different trans-lation systems, just like we would expect from hu-man translators.
Also, every system may have itsown advantages.
We break these translations intosmaller units and hope to be able to select the bestones and form them into a better translation.One single translation of a few thousand sen-tences is normally inadequate for building a re-liable general-purpose SMT system (data sparse-ness problem).
However, in the system combina-tion task, this is no longer an issue as the systemonly needs to translate sentences within the dataset.When more translation engines are available,the size of this set becomes larger.
Hence,we collect translations from all available systemsand pair them with the corresponding input text,thus forming a medium-sized ?hypothesis?
cor-pus.
Our system starts processing this corpuswith a standard phrase-based SMT setup, using theMoses toolkit (Koehn et al, 2007).The hypothesis corpus is first tokenized andlowercased.
Then, we run GIZA++ (Och andNey, 2003) on the corpus to obtain word align-ments in both directions.
The phrases are extractedfrom the intersection of the alignments with the?grow?
heuristics.
In addition, we also generatea reordering model with the default configurationas included in the Moses toolkit.
This ?hypothe-sis?
translation model can already be used by theMoses decoder together with a language model toperform translations over the corresponding sen-tence set.3.2 Integration into existing SMT systemSometimes, the goal of system combination is notonly to produce a translation but also to improveone of the systems.
In this paper, we aim at incor-porating the additional system outputs to improvean out-of-domain SMT system trained on the Eu-roparl corpus (Koehn, 2005).
Our hope is that theadditional translation hypotheses could bring innew phrases or, more generally, new informationthat was not contained in the Europarl model.
Inorder to facilitate comparisons, we use in-domainLMs for all setups.We investigate two alternative ways of integrat-ing the additional phrases into the existing SMTsystem: One is to take the hypothesis translationmodel described in Section 3.1, the other is toconstruct system-specific models constructed withonly translations from one system at a time.Although the Moses decoder is able to workwith two phrase tables at once (Koehn andSchroeder, 2007), it is difficult to use this methodwhen there is more than one additional model.The method requires tuning on at least six morefeatures, which expands the search space for thetranslation task unnecessarily.
We instead inte-grate the translation models from multiple sourcesby extending the phrase table.
In contrast to theprior approach presented in (Chen et al, 2007) and(Eisele et al, 2008) which concatenates the phrasetables and adds new features as system markers,our extension method avoids duplicate entries inthe final combined table.Given a set of hypothesis translation models(derived from an arbitrary number of system out-puts) and an original large translation model to beimproved, we first sort the models by quality (seeSection 3.3), always assigning the highest priorityto the original model.
The additional phrase tablesare appended to the large model in sorted ordersuch that only phrase pairs that were never seenbefore are included.
Lastly, we add new features(in the form of additional columns in the phrase ta-ble) to the translation model to indicate each pair?sorigin.3.3 System evaluationSince both the system translations and the ref-erence translations are available for the tuning43set, we first compare each output to the referencetranslation using BLEU (Papineni et al, 2001)and METEOR (Banerjee and Lavie, 2005) and acombined scoring scheme provided by the ULCtoolkit (Gimenez and Marquez, 2008).
In our ex-periments, we selected a subset of 5 systems forthe combination, in most cases, based on BLEU.On the other hand, some systems may be de-signed in a way that they deliver interesting uniquetranslation segments.
Therefore, we also measurethe similarity among system outputs as shown inTable 2 in a given collection by calculating aver-age similarity scores across every pair of outputs.de-en fr-en es-en en-de en-fr en-esNum.
20 23 28 15 16 9Median 19.87 26.55 22.50 13.78 24.76 23.70Range 16.37 17.06 9.74 4.75 11.05 13.94Top 5 de-en fr-en es-en en-de en-fr en-esMedian 22.26 27.93 26.43 15.21 26.62 26.61Range 4.31 4.76 5.71 1.71 0.68 5.56Table 1: Statistics of system outputs?
BLEU scoresThe range of BLEU scores cannot indicate thesimilarity of the systems.
The direction with themost systems submitted is Spanish-English buttheir respective performances are very close toeach other.
As for the selected subset, the English-French systems have the most similar performancein terms of BLEU scores.
The French-Englishtranslations have the largest range in BLEU but thesimilarity in this group is not the lowest.de-en fr-en es-en en-de en-fr en-esAll 34.09 46.48 61.83 31.74 44.95 38.11Selected 36.65 56.16 56.06 33.92 52.78 57.25Table 2: Similarity of the system outputsIdeally, we should select systems with highestquality scores and lowest similarity scores.
ForGerman-English, we selected the three with thehighest METEOR scores and another two withhigh METEOR scores but low similarity scores tothe first three.
For the other language directions,we chose five systems from different institutionswith the highest scores.3.4 Language modelsWe use a standard n-gram language model foreach target language using the monolingual train-ing data provided in the translation task.
TheseLMs are thus specific to the same domain as theinput texts.
Moreover, we also generate ?hypoth-esis?
LMs solely based on the given system out-puts, that is, LMs that model how the candidatesystems convey information in the target language.These LMs do not require any additional trainingdata.
Therefore, we do not require any trainingdata other than the given system outputs by usingthe ?hypothesis?
language model and the ?hypoth-esis?
translation model.3.5 TuningAfter building the models, it is essential to tunethe SMT system to optimize the feature weights.We use Minimal Error Rate Training (Och, 2003)to maximize BLEU on the complete developmentdata.
Unlike the standard tuning procedure, we donot tune the final system directly.
Instead, we ob-tain the weights using models built from the tuningportion of the system outputs.For each combination variant, we first trainmodels on the provided outputs corresponding tothe tuning set.
This system, called the tuning sys-tem, is also tuned on the tuning set.
The initialweights of any additional features not included inthe standard setting are set to 0.
We then adapt theweights to the system built with translations cor-responding to the test set.
The procedure and thesettings for building this system must be identicalto that of the tuning system.4 ExperimentsThe purpose of this exercise is to understand thenature of the system combination task in prac-tice.
Therefore, we restrict ourselves to the train-ing data and system translations provided by theshared task.
The types of the systems that pro-duced the translations are assumed to be unknown.We report results for six translation directions be-tween four languages.4.1 Data and baselineWe build an SMT system from release v4 of theEuroparl corpus (Koehn, 2005), following a stan-dard routine using the Moses toolkit.
The sys-tem also includes 5-gram language models trainedon in-domain corpora of the respective target lan-guages using SRILM (Stolcke, 2002).The systems in this paper, including the base-line, are all tuned on the same 501-sentence tuningset.
Note also that the provided n-best outputs areexcluded in our experiments.444.2 ResultsThe experiments include three different setups fordirect system combination, involving only hypoth-esis translation models.
System S0, the baselinefor this group, uses a hypothesis translation modelbuilt with all available system translations and ahypothesis LM (also from the machine-generatedoutputs).
S1 differs from S0 in that the LM in S1 isgenerated from a large news corpus.
S2 consists oftranslation models built with only the five selectedsystems.
The BLEU scores of these systems areshown in Table 3.de-en fr-en es-en en-de en-fr en-esTop 1 21.16 30.91 28.54 14.96 26.55 27.84Mean 17.29 23.78 21.39 12.76 22.96 21.43S0 20.46 27.50 23.35 13.95 27.29 25.59S1 21.76 28.05 25.49 15.16 27.70 26.09S2 21.71 24.98 27.26 15.62 24.28 25.22Table 3: BLEU scores of direct system combina-tionWhen all outputs are included, the combinedsystem can always produce translations better thanmost of the systems.
When only a hypothesis LMis used, the BLEU scores are always higher thanthe average BLEU scores of the outputs.
It evenoutperforms the top system for English-French.This simple setup (S0) is certainly a feasible so-lution when no additional data is available and nosystem evaluation is possible.
This approach ap-pears to be more effective on typically difficultlanguage pairs that involve German.As for the systems with normal language mod-els, neither of the systems ensure better transla-tions.
The translation quality is not completelydetermined by the number of included translationsand their quality.
On the other hand, the outputset with higher diversity (Table 2) usually leadsto better combination results.
This observation isconsistent with the results from the system inte-gration experiments shown in Table 4.de-en fr-en es-en en-de en-fr en-esBas 19.13 25.07 24.55 13.59 23.67 23.67Med 17.99 24.56 20.70 13.19 24.19 22.12All 21.40 28.00 27.75 15.21 27.20 26.41Top5 21.70 26.01 28.53 15.52 27.87 27.92Table 4: BLEU scores of integrated SMT systems(Bas: Baseline, Med: Median)There are two variants in our experiments onsystem integration.
All in Table 4 represents thesystem that integrates the complete hypothesistranslation model with the Europarl model, whileTop 5 refers to the system that incorporates the fivesystem-specific models separately.
Both setups re-sult in an improvement over the baseline Europarl-based SMT system.
BLEU scores increase by upto 4.25 points.
The integrated SMT system some-times produces translations better than the bestsystem (7 out of 12 cases).5 ConclusionThis work uses the Moses toolkit to combinetranslations from multiple engines in a simple way.The experiments on six translation directions showinteresting results: The final translations are al-ways better than the majority of the given systems,while the combination performs better than thebest system in half the cases.
A similar approachwas applied to improve an existing SMT systemwhich was built in a domain different from the testtask.
We achieved improvements in all cases.There are many possible future directions tocontinue this work.
As we have shown, the qual-ity of the combined system is more related to thediversity of the involved systems than to the num-ber of the systems or their quality.
Hand-pickedsystems lead to better combinations than those se-lected by BLEU scores.
It would be interestingto develop a more comprehensive system selectionstrategy.AcknowledgmentsThis work was supported by the EuroMatrixproject (IST-034291) which is funded by theEuropean Community under the Sixth Frame-work Programme for Research and TechnologicalDevelopment.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An automatic metric for MT evaluation with im-proved correlation with human judgments.
In Pro-ceedings of the ACL Workshop on Intrinsic and Ex-trinsic Evaluation Measures for Machine Transla-tion and/or Summarization, pages 65?72, Ann Ar-bor, Michigan, June.
Association for ComputationalLinguistics.Yu Chen, Andreas Eisele, Christian Federmann, EvaHasler, Michael Jellinghaus, and Silke Theison.2007.
Multi-engine machine translation with anopen-source SMT decoder.
In Proceedings of45WMT07, pages 193?196, Prague, Czech Republic,June.
Association for Computational Linguistics.Andreas Eisele, Christian Federmann, Herve?
Saint-Amand, Michael Jellinghaus, Teresa Herrmann, andYu Chen.
2008.
Using Moses to integrate mul-tiple rule-based machine translation engines into ahybrid system.
In Proceedings of the Third Work-shop on Statistical Machine Translation, pages 179?182, Columbus, Ohio, June.
Association for Compu-tational Linguistics.Jesus Gimenez and Lluis Marquez.
2008.
A smor-gasbord of features for automatic MT evaluation.In Proceedings of the Third Workshop on Statisti-cal Machine Translation, pages 195?198, Colum-bus, Ohio, June.
Association for Computational Lin-guistics.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-HMM-based hypothesis alignment for combining outputsfrom machine translation systems.
In Proceedingsof the 2008 Conference on Empirical Methods inNatural Language Processing, pages 98?107, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Philipp Koehn and Josh Schroeder.
2007.
Experi-ments in domain adaptation for statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 224?227,Prague, Czech Republic, June.
Association for Com-putational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbs.
2007.Moses: Open source toolkit for statistical ma-chine translation.
In Proceedings of Annual meet-ing of the Association for Computation Linguis-tics (acl), demonstration session, pages 177?180,Prague, Czech, June.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofMT Summit 2005.Evgeny Matusov, Nicola Ueffing, and Hermann Ney.2006.
Computing consensus translation from multi-ple machine translation systems using enhanced hy-potheses alignment.
In Conference of the EuropeanChapter of the Association for Computational Lin-guistics, pages 33?40, Trento, Italy, April.Franz Josef Och and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL ?03:Proceedings of the 41st Annual Meeting on Asso-ciation for Computational Linguistics, pages 160?167, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
BLEU: a method for automaticevaluation of machine translation.
In Proceedingsof the 40th Annual Meeting on Association for Com-putational Linguistics, pages 311?318, Morristown,NJ, USA.
Association for Computational Linguis-tics.Antti-Veikko I. Rosti, Spyridon Matsoukas, andRichard M. Schwartz.
2007.
Improved word-levelsystem combination for machine translation.
InACL.Andreas Stolcke.
2002.
SRILM - an extensible lan-guage modeling toolkit.
In the 7th InternationalConference on Spoken Language Processing (IC-SLP) 2002, Denver, Colorado.46
