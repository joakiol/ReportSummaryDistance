Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics, pages 28?36,Sofia, Bulgaria, August 8, 2013. c?2013 Association for Computational LinguisticsStatistical Representation of Grammaticality Judgements: the Limits ofN-Gram ModelsAlexander Clark, Gianluca Giorgolo, and Shalom LappinDepartment of Philosophy, King?s College Londonfirstname.lastname@kcl.ac.ukAbstractWe use a set of enriched n-gram models to trackgrammaticality judgements for different sorts ofpassive sentences in English.
We construct thesemodels by specifying scoring functions to map thelog probabilities (logprobs) of an n-gram model fora test set of sentences onto scores which dependon properties of the string related to the parame-ters of the model.
We test our models on classifica-tion tasks for different kinds of passive sentences.Our experiments indicate that our n-gram modelsachieve high accuracy in identifying ill-formed pas-sives in which ill-formedness depends on local rela-tions within the n-gram frame, but they are far lesssuccessful in detecting non-local relations that pro-duce unacceptability in other types of passive con-struction.
We take these results to indicate some ofthe strengths and the limitations of word and lexicalclass n-gram models as candidate representations ofspeakers?
grammatical knowledge.1 IntroductionMost advocates (Pereira, 2000; Bod et al 2003)and critics (Chomsky, 1957; Fong et al 2013) of aprobabilistic view of grammatical knowledge haveassumed that this view identifies the grammaticalstatus of a sentence directly with the probability ofits occurrence.
By contrast, we seek to character-ize grammatical knowledge statistically, but with-out reducing grammaticality directly to probabil-ity.
Instead we specify a set of scoring proceduresfor mapping the logprob value of a sentence intoa relative grammaticality score, on the basis of theproperties of the sentence and of the logprobs thatan n-gram word model generates for the corpuscontaining the sentence.
A scoring procedure inthis set generates scores in terms of which we con-struct a grammaticality classifier, using a param-eterized standard deviation from the mean value.The classifier provides a procedure for testing theaccuracy of different scoring criteria in separat-ing grammatical from ungrammatical passive sen-tences.We evaluate this approach by applying it tothe task of distinguishing well and ill-formed sen-tences with passive constructions headed by fourdifferent sorts of verbs: intransitives (appear,last), pseudo-transitives, which take a restrictedset of notional objects (laugh a hearty laugh,weigh 10 kg), ambiguous transitives, which allowboth agentive and thematic subjects (the jeans /the tailor fitted John), and robust transitives thatpassivize freely (write, move).
Intransitives andpseudo-transitives generally yield ill-formed pas-sives.
Passives formed from ambiguous transitivestend to be well-formed only on the agentive read-ing.
Robust transitives, for the most part, yieldacceptable passives, even if they are semantically(or pragmatically) odd.Experimenting with several scoring proceduresand alternative values for our standard deviationparameter, we found that our classifier can distin-guish pairwise between elements of the first twoclasses of passives and those of the latter two witha high degree of accuracy.
However, its perfor-mance is far less reliable in identifying the differ-ence between ambiguous and robust transitive pas-sives.
The first classification task relies on locallexical patterns that can be picked up by n-grammodels, while the second requires identification ofanomalous relations between passivized verbs andby-phrases, which are not generally accessible tomeasurement within the range of an n-gram.We also observed that as we increased the sizeof the training corpus, the performance of our en-riched models on the classification task also in-creased.
This result suggests that better n-gramlanguage models are more sensitive to the sorts ofpatterns that our scoring procedures rely on to gen-erate accurate grammaticality classifications.We note the important difference between28grammaticality and acceptability.
Following stan-dard assumptions, we take grammaticality to bea theoretical notion, and acceptability to be anempirically testable property.
Acceptability is, inpart, determined by grammaticality, but also byfactors such as sentence length, processing limi-tations, semantic acceptability and many other el-ements.
Teasing apart these two concepts, and ex-plicating their precise relationship raises a host ofsubtle methodological issues that we will not ad-dress here.
Oversimplifying somewhat, we are try-ing to reconstruct a gradient notion of grammati-cality which is derived from probabilistic models,that can serve as a core component of a full modelof acceptability.We distinguish our task from the standard taskof error detection in NLP (e.g.
Post (2011)),that can be used in various language processingsystems, such as machine translation (Pauls andKlein, 2012), language modeling and so on.
Inerror detection, the problem is a supervised learn-ing task.
Given a corpus of examples labeled asgrammatical or ungrammatical, the problem is tolearn a classifier to distinguish them.
We use su-pervised learning as well, but only to measure theupper bound of an unsupervised learning method.We assume that native speakers do not, in general,have access to systematic sets of ungrammaticalsentences that they can use to calibrate their judge-ment of acceptability.
Rather ungrammatical sen-tences are unusual or unlikely.
However, we usesome ungrammatical sentences to set an optimalthreshold for our scoring procedures.2 Enriched N-Gram Language ModelsWe assume that we have some high quality lan-guage model which defines a probability distri-bution over whole sentences.
As has often beennoted, it is not possible to reduce grammatical-ity directly to a probability of this type, for sev-eral reasons.
First, if one merely specifies a fixedprobability value as a threshold for grammatical-ity, where strings are deemed to be grammaticalif and only if their probability is higher than thethreshold, then one is committed to the existenceof only a finite number of grammatical sentences.The probabilities of the possible strings of wordsin a language sum to 1, and so at most 1/ sen-tences can have a probability of at least .
Second,probability can be affected by factors that do notinfluence grammaticality.
For example, the word?yak?
is rarer (and therefore less probable) than theword ?horse?, but this does not affect the relativegrammaticality of ?I saw a horse?
versus ?I saw ayak?.
Third, a short ungrammatical sentence mayhave a higher probability than a long grammaticalsentence with many rare words.In spite of these arguments against a naive re-duction of grammaticality, probabilistic inferencedoes play a role in linguistic judgements, as in-dicated by the fact that they are often gradient.Probabilistic inference is pervasive throughout alldomains of cognition (Chater et al 2006), andtherefore it is plausible to assume that knowledgeof language is also probabilistic in nature.
More-over language models do seem to play a crucialrole in speech recognition and sentence process-ing.
Without them we would not be able to under-stand speech in a noisy environment.We propose to accommodate these differentconsiderations by using a scoring function to mapprobabilities to grammaticality rankings.
Thisfunction does not apply directly to probabilities,but rather to the parameters of the language model.The probability of a particular sentence with re-spect to a log-linear language model will be theproduct of certain parameters: in log space, thesum.
We define scores that operate on this collec-tion of parameters.2.1 ScoresWe have experimented with scores of two differ-ent types that correlate with the grammaticalityof a sentence.
Those of the first type are dif-ferent implementations of the idea of normaliz-ing the logprob assigned by an n-gram model toa string by eliminating the significance of factorsthat do not influence the grammatical status of asentence, such as sentence length and word fre-quency.
Scores of the second type are based on theintuition that the (un)grammaticality of a sentenceis largely determined by its problematic compo-nents.
These scores are functions of the lowestscoring n-grams in the sentence.Mean logprob (ML) This score is the logprobof the entire sentence divided by the length of thesentence, or equivalently the mean of the logprobsfor the single trigrams:ML = 1n logPTRIGRAM(?w1, .
.
.
, wn?
)By normalizing the logprob for the entire sentenceby its length we eliminate the effect of sentencelength on the acceptability score.29Weighted mean logprob (WML) This score iscalculated by dividing the logprob of the entiresentence by the sum of the unigram probabilitiesof the lexical items that compose the sentence:WML = logPTRIGRAM(?w1,...,wn?)logPUNIGRAM(?w1,...,wn?
)This score eliminates at the same time the effect ofthe length of the sentence and the lower probabil-ity assigned to sentences with rare lexical items.Synctactic log odds ratio (SLOR) This scorewas first used by Pauls and Klein (2012) andperforms a normalization very similar to WML(we will see below that in fact the two scores arebasically equivalent):SLOR =logPTRIGRAM(?w1,...,wn?)?logPUNIGRAM(?w1,...,wn?
)nMinimum (Min) This score is equal to the low-est logprob assigned by the model to the n-gramsof the sentence divided by the unigram logprob ofthe lexical item heading the n-gram:Min = mini[logP (wi|wi?2wi?1)logP (wi)]In this way, if a single n-gram is assigned a lowprobability (normalized for the frequency of itshead lexical item), then this low score is in somesense propagated to the whole sentence.Mean of the first quartile (MFQ) This scoreis a generalization of the Min score.
We orderthe single n-gram logprobs from the lowest to thehighest, and we consider the first (lowest) quar-tile.
We then normalize the logprobs for these n-grams by the unigram probability of the head lex-ical item, and we take the mean of these scores.In this way we obtain a score that is more robustthan the simple Min, as, in general, a grammaticalanomaly influences the logprob of more than onen-gram.2.2 N-Gram ModelsWe are using n-gram models on the understand-ing that they are fundamentally inadequate for de-scribing natural languages in their full syntacticcomplexity.
In spite of their limitations, they are agood starting point, as they perform well as lan-guage models across a wide range of languagemodeling tasks.
They are easy to train, as theydo not require annotated training data.We do not expect that our n-gram based gram-maticality scores will be able to idenitfy all of thecases of ungrammaticality that we encounter.
Ourworking hyposthesis is that they can capture casesof ill-formedness that depend on local factors, thatcan be identified within n-gram frames, as op-posed to those which involve non-local relations.If these models can detect local grammaticality vi-olations, then we will have a basis for thinkingthat richer, more structured language models canrecognize non-local as well as local sources of un-grammaticality.3 Experiments with PassivesRather than trying to test the performance of thesemodels over all types of ungrammaticality, welimit ourselves to a case study of the passive.
Bytightly controlling the verb types and grammat-ical construction to which we apply our modelswe are better able to study the power and the lim-its of these models as candidate representations ofgrammatical knowledge.3.1 Types of PassivesOur controlled experiments on passives are, inpart, inspired by speakers?
judgments discussed inAmbridge et al(2008).
Their experimental workmeasures the acceptability of various passive sen-tences.The active-passive alternation in English is ex-emplified by the pair of sentences?
John broke the window.?
The window was broken by John.The acceptability of the passive sentence de-pends largely on lexical properties of the verb.Some verbs do not allow the formation of the pas-sive, as in the case of pure intransitive verbs likeappear, discussed below, which permit neither theactive transitive, nor the passive.We conducted some prelimiary experiments,not reported here, on modelling the data on pas-sives from recent work in progress that Ben Am-bridge and his colleagues are doing, and whichhe was kind enough to make available to us.
Weobserved that the scores we obtained for our lan-guage models did not fully track these judgements,but we did notice that we obtained much bettercorrelation at the low end of the judgment distri-bution.
In Ambridge?s current data this judgementrange corresponds to passives constructed with in-transitive verbs.The Ambridge data indicates that the capacityof verbs to yield well-formed passive verb phrases30forms a continuum.
Studying the judgement pat-terns in this data we identified four reasonablysalient points along this hierarchial continuum.First, at the low end, we have intransitiveslike appear: (*John appeared the book.
*Thebook was appeared).
Next we have what may bedescribed as pseudo-transitives verbs like laugh,which permit only notional NP objects and do noteasily passivize (Mary laughed a hearty laugh/*ajoke.
?A hearty laugh/*A joke was laughed byMary) above them.
These are followed by casesof ambiguous transitives like fit, which, in activeform, carry two distinct readings that correspondto an agentive and a thematic subject, respectively.?
The tailor fitted John for a new suit.?
The jeans fitted JohnOnly the agentive reading can be passivized.?
John was fitted by the tailor.?
*John was fitted by the jeans.Finally, the most easily passivized verbs are ro-bust transitives, which take the widest selection ofNP subjects in passive form (John wrote the book.The book was written by John).This continuum causes well-formedness in pas-sivization to be a gradient property, as the Am-bridge data illustrates.
Passives tend to be moreor less acceptable along this spectrum.
The gradi-ence of acceptability for passives implies the par-tial overlap of the score distributions for the differ-ent types of passives that our experiments show.The experiments were designed to test our hy-pothesis that n-gram based language models arecapable of detecting ungrammatical patterns onlyin cases where they do not depend on relationsbetween words that cross the n-word boundaryapplied in training.
Therefore we expect such amodel to be capable of detecting the ungrammati-cality of a sentence like A horrible death was diedby John, because the trigrams death was died, wasdied by and died by John are unlikely to appearin any corpus of English.
On the other hand, wedo not expect a trigram model to store the infor-mation necessary to identify the relative anomalyof a sentence like Two hundred people were heldby the theater, because all the trigrams (as well asthe bigrams and the unigrams) that constitute thesentence are likely to appear with reasonable fre-quency in a large corpus of English.The experiments generalize this observationand test the performance of n-gram models on awider range of verb types.
To quantify the per-formance of the different models we derive simpleclassifiers using the scores we have defined andtesting them in a binary classification task.
Thistask measures the ability of the classifier to dis-tinguish between grammatical sentences, and sen-tences containing different types of grammaticalerrors.The models are trained in an unsupervised man-ner using only corpus data, which we assume to beuniformly grammatical.
In order to evaluate thescoring methods, we use some supervised data toset the optimal value of a simple threshold.
This isnot however a supervised classification task: wewant to see how well the scores could be usedto separate grammatical and ungrammatical data,and though unorthodox, this seems a more directway of measuring this conditional property thanstipulating some fixed threshold.3.2 Training dataWe used the British National Corpus (BNC) (BNCConsortium, 2007) to obtain our training data.
Wetrained six different language models, using sixdifferent subcorpora of the BNC.
The first modelused the entire collection of written texts anno-tated in the BNC, for a total of approximately 100million words.
The other models were trained onincreasingly smaller portions of the written textscollection: 40 million words, 30 million words, 15million words, 7.6 million words, and 3.8 millionwords.
We constructed these corpora by randomlysampling an appropriate number of complete sen-tences.All models were trained on word sequences.For smoothing the n-gram probability distribu-tions we used Kneser-Ney interpolation, as de-scribed in Goodman (2001).3.3 Test dataWe constructed the test data for our hypothesis ina controlled fashion.
We first compiled a list ofverbs for each of the four verb types that we con-sider (intransitives, pseudo-transitives, ambiguoustransitives, and robust transitives).
We selectedverbs from the BNC that appeared at least 100times in their past participle form in the entire cor-pus in order to ensure a sufficient number of pas-31sive uses in the training data.1 We selected 40 in-transitive verbs, 13 pseudo transitives, 23 ambigu-ous transitives and 40 transitive verbs.
To clas-sify the verbs we relied on our intuitions as nativespeakers of English.Using these lists we automatically generatedfour corpora by selecting an agent and a patientfrom a predefined pool of NPs, randomly select-ing a determiner (if necessary) and a number (ifthe NP allows plurals).
The resulting corpora areof the following sizes:?
intransitive verbs ?
24480 words, 3240 sen-tences,?
pseudo transitive verbs ?
7956 words, 1053sentences,?
ambiguous transitive verbs ?
14076 words,1863 sentences,?
robust transitive verbs ?
24480 words, 3240sentences.Each corpus was evaluated by the six models.We computed our derived scores for each sentenceon the basis of the logprobs that the language mod-els assigns.3.4 Binary classifiersFor each model and for each score we constructeda set of simple binary classifiers on the basis ofthe results obtained for the transitive verb corpus.We took the mean of each score assigned by themodel to the transitive sentences, and we set dif-ferent thresholds by subtracting from this valuea number of standard deviations ranging from 0to 2.75.
The rationale behind these classifiers isthat, assuming the passives of the robust transi-tives to be grammatical, the scores for the othercases should be comparatively lower.
Thereforeby setting a threshold ?to the left?
of the mean weshould be able to distinguish between grammati-cal sentences, whose score is to the right of thethreshold, and ungrammatical ones, expected to ahave a score lower than the threshold.
Formallythe classifier is defined as follows:cs(w) ={+ if s(w) ?
m?
S ?
??
otherwise(1)1Notice that in most cases the past participle form is thesame as the simple past form, and for this reason we set thethreshold to such a high value.where s is one of our scores, w is the sentence tobe classified, s(w) represents the value assignedby the score to sentence w, m is the mean forthe score in the transitive condition, ?
is the stan-dard deviation for the score again in the transitivecondition, and S is a factor by which we movethe threshold away from the mean.
The classi-fier assigns the grammatical (+) tag only to thosesentences that are assigned values higher than thethreshold m?
S ?
?.Alternatively in terms of the widely used z-score, defined as zs(w) = (s(w) ?m)/?
we cansay that w is classified as grammatical iff zs(w) ?
?S.4 ResultsFor reasons of space we will limit the presenta-tion of our detailed results to the 100 million wordmodel, as it offers the sharpest effects.
We will,however, also report comparisons on the most im-portant metrics for the complete set of models.In Figure 1 we show the distribution of the fivescores for the four different corpora (transitive,ambiguous, pseudo, and intransitive) obtained us-ing the 100 million word model.
In all cases weobserve the same general pattern: the sentences inthe corpus generated with robust transitives are as-signed comparatively high scores, and these grad-ually decrease when we consider the ambiguous,the pseudo and the intransitive conditions.
Inter-estingly, this order reflects the degree of ?transi-tivity?
that these verb types exhibit.
Notice, how-ever, that the four conditions seem to group intotwo different macro-distributions.
On the rightwe have the transitive-ambiguous sentences andon the left the pseudo-intransitive cases.
This par-tially confirms our hypothesis that n-gram mod-els have problems recognizing lexical dependen-cies that determine the felicitousness of passivesconstructed using ambiguous transitive verbs, asthese are, for the most part, non-local.
Neverthe-less, it is important to note that the overlap of thedistributions for these two cases is also due to thefact that many cases in the ambiguous transitivecorpus are indeed grammatical.Figure 2 summarizes the (balanced) accuraciesobtained by our classifiers for each comparison,by each model.
These results confirm our hy-pothesis that the classifiers tend to perform betterwhen distinguishing passive sentences constructedwith a robust transitive verbs from those headed by32Logprob ML WMLSLOR Min MFQ0.000.050.100.150.00.51.01.52.002460.00.51.01.52.0024601234?30 ?25 ?20 ?15 ?2.5 ?2.0 ?1.5 ?1.0 ?0.9 ?0.8 ?0.7 ?0.6 ?0.50.0 0.5 1.0 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75 ?1.75 ?1.50 ?1.25 ?1.00 ?0.75densityconditiontransitiveambiguouspseudointransitiveFigure 1: Distributions of the six scores Logprob, ML, WML, SLOR, Min and MFQ for the four differ-ent conditions (robust transitive passives, ambiguous transitive passives, pseudo transitive passives andintransitive passives) for the 100 million words language model.pseudo-transitives and intransitives.In the comparison between transitive and am-biguous transitive sentences, the classifiers are?stuck?
at around 60% accuracy.
Using largertraining corpora produces only a marginal im-provement.
This contrasts with what we observefor the transitive/pseudo and transitive/intransitiveclassification tasks.
In the transitive/pseudo task,we already obtain reasonable accuracy with themodel trained with the smallest BNC subset.Oddly, the overall best result is achieved with 30million words, although the result obtained withthe model trained on the full BNC corpus is notmuch lower.
For the transitive/intransitive classifi-cation task we observe a much steadier and largergrowth in accuracy, reaching the overall best resultof 85.1%.
Table 1 reports the best results for eachcomparison by each language model.
For eachcondition we report the best accuracy obtained, thecorresponding F1 score, the score that achieves thebest result, and the best accuracy obtained by justusing the logprobs.
These results are obtained us-ing different values for the S parameter.
However,in general the best results are obtained when the Sparameter is set to a value in the interval [0.5, 1.5].In comparing the performance of the individ-ual scores, we first notice that, while for the tran-sitive/ambiguous comparison all scores performpretty much at the same level, there is a clear hier-archy between scores for the other comparisons.We observe that the baseline raw logprob as-signed by the n-grams models performs muchworse than the scores, resulting in roughly 10%less accuracy than the best performing score in ev-ery condition.
ML performs slightly better, obtain-ing around 5% greater accuracy than logprob as apredictor.
This shows that even though the lengthof the sentences in our test data is relatively con-stant (between 9 and 11 words), there is still animprovement if we take this structural factor intoaccount.
The two scores WML and SLOR displaythe same pattern, showing that they are effectivelyequivalent.
This is not surprising given that theyare designed to modify the raw logprob by tak-33transitiveambiguous transitivepseudo transitiveintransitivel l l l l l l l l l l ll l l l l ll l l l l l l l ll l l ll l ll l ll ll l ll l ll ll l l l l l l l l l l ll l l l l l l l ll l l l l l ll l lll l l l l l l llll l ll l l l llll l l l l l l l l l l ll l l l l l l ll l l l l l ll ll ll l l l l ll l l ll l ll l ll l l l l l l l l l l ll l l l l l l l ll ll l l l l l l l ll ll ll l l ll l l ll l ll l l l lll l ll l l l l l l l l l l ll l l l l l l l l ll l ll l ll l l l l ll l ll l ll l l l ll l l llll l ll llll l l l l l l l l l l ll l l l l l l l l ll ll l ll l l l l l ll l l l l l l l l lll ll l ll l l l ll l ll l l ll l l l lllll lll l l l lllll lllll l ll l l l ll l l lllllllll l ll ll l l l ll llll l ll ll l lllllll l l l llllllll ll l l l l ll lllllll llllll ll l l l l ll l ll l ll l l l l llllll l l l llllllllll l l l l ll l llll ll l llllll ll ll l ll lll ll lll l l l llllll llll l l lllllllllllllll ll ll lllllllll l l llll l l l l lll ll l ll ll l l llllllll l lllllllllllllll l l ll l lllllll lllll ll l lllll ll lll ll l llllllll l l llllllll ll ll l lllll lll l lllll ll l l lllll l l l l ll ll l l ll l l l lllll l lll l l llllll llllll ll l l l llllllll l l llll l ll l l l l l ll l l l ll ll l llllllll l l l lllll lll ll l l l l ll lllllll l llll l ll l l l l lllll l ll ll l lllllllll ll l llllllll lllll l l l lllllllll ll lllll ll ll l l l lllll lll l l l llllllllll l l lllllllllllllll l lll llllllll l l lllll ll ll l l lllll l lll ll l llllllllll l l llllllllll lllll l l llllllllll lllllll ll l lllllll llll l l lllllllll ll llllllllll l l l llllllllllll l llllll0.50.60.70.80.50.60.70.80.50.60.70.80.50.60.70.80.50.60.70.80.50.60.70.83.8M7.6M15M30M40M100M0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75 0.00 0.25 0.50 0.75 1.00 1.25 1.50 1.75 2.00 2.25 2.50 2.75SBalancedaccuracy ScorellllllLogprobMLWMLSLORMinMFQFigure 2: Accuracies for the classifiers for each model.
S represents the number of standard deviations?to the left?
of the mean of the transitive condition score, used to set the threshold.ing into account exactly the same factors (lengthof the sentence and frequency of the unigrams thatcompose the sentence).
These two scores performgenerally better in the transitive/ambiguous com-parison, and they achieve good performance whenthe size of the training model is small.
However,for the most part, the two scores derived from thelogprobs of the least probable n-grams in the sen-tence, Min and MFQ, get the best results.
Minexhibits erratic behavior (mainly due to its non-normal distribution for each condition, as shownin figure 1), and it seems to be more stable onlyin the presence of a large training set.
MFQ hasa much more robust contour, as it is significantlyless dependent on the choice of S.5 Conclusions and Future WorkIn Clark and Lappin (2011) we propose a modelof negative evidence that uses probability of oc-currence in primary linguistic data as the basis forestimating non-grammaticality through relatively34Model Comparison Best accuracy F1 Best performing score Logprob accuracytransitive/ambiguous 60.9% 0.7 SLOR 57.3%3.8M transitive/pseudo 77% 0.81 MFQ 67.6%transitive/intransitive 73.8% 0.72 SLOR 65.6%transitive/ambiguous 62.9% 0.68 MFQ 57.8%7.6M transitive/pseudo 78.5% 0.76 MFQ 69.1%transitive/intransitive 75.8% 0.72 MFQ 67.3%transitive/ambiguous 62.3% 0.66 WML 57.8%15M transitive/pseudo 72.6% 0.78 SLOR 66.5%transitive/intransitive 79.5% 78.3 MFQ 69.5%transitive/ambiguous 63.3% 0.75 WML 58.9%30M transitive/pseudo 83.1% 0.88 Min 71.2%transitive/intransitive 81.8% 0.82 MFQ 72.2%transitive/ambiguous 63.8% 0.75 SLOR 59.5%40M transitive/pseudo 80.1% 0.86 Min 69.7%transitive/intransitive 83.5% 0.83 SLOR 72.6%transitive/ambiguous 63.3% 0.75 SLOR 58.4%100M transitive/pseudo 80.3% 0.9 MFQ 71.3%transitive/intransitive 85.1% 0.85 SLOR 73.8%Table 1: Best accuracieslow frequency in a sample of this data.
Here wefollow Clark et al(2013) in effectively invertingthis strategy.We identify a set of scoring functions based onparameters of probabilistic models that we use todefine a grammaticality threshold, which we useto classify strings as grammatical or ill-formed.This model offers a stochastic characterisation ofgrammaticality without reducing grammaticalityto probability.We expect enriched lexical n-gram models ofthe kind that we use here to be capable of rec-ognizing the distinction between grammatical andungrammatical sentences when it depends on localfactors within the frame of the n-grams on whichthey are trained.
We further expect them not to beable to identify this distinction when it depends onnon-local relations that fall outside of the n-gramframe.It might be thought that this hypothesis con-cerning the capacities and limitations of n-grammodels is too obvious to require experimental sup-port.
In fact, this is not the case.
Reali and Chris-tiansen (2005) show that n-gram models can beused to distinguish grammatical from ungrammat-ical auxiliary fronted polar questions with a highdegree of success.
More recently Frank et al(2012) argue for the view that a purely sequen-tial, non-hierarchical view of linguistic structure isadequate to account for most aspects of linguisticknowledge and processing.We have constructed an experiment with differ-ent (pre-identified) passive structures that providessignificant support for our hypothesis that lexicaln-gram models are very good at capturing localsyntactic relations, but cannot handle more distantdependencies.In future work we will be experimenting withmore expressive language models that can repre-sent non-local syntactic relations.
We will pro-ceed conservatively by first extending our enrichedlexical n-gram models to chunking models, andthen to dependency grammar models, using onlyas much syntactic structure as is required to iden-tify the judgement patterns that we are studying.To the extent that this research is successful itwill provide motivation for the view that syntacticknowledge is inherently probabilistic in nature.AcknowledgmentsThe research described in this paper was done in theframework of the Statistical Models of Grammaticality(SMOG) project at King?s College London, funded by grantES/J022969/1 from the Economic and Social Research Coun-cil of the UK.
We are grateful to Ben Ambridge for providingus with the data from his experiments and for helpful dis-cussion of the issues that we address in this paper.
We alsothank the three anonymous CMCL 2013 reviewers for usefulcomments and suggestions, that we have taken account of inpreparing the final version of the paper.35ReferencesBen Ambridge, Julian M Pine, Caroline F Rowland, andChris R Young.
2008.
The effect of verb semanticclass and verb frequency (entrenchment) on childrens andadults graded judgements of argument-structure overgen-eralization errors.
Cognition, 106(1):87?129.BNC Consortium.
2007.
The British National Corpus, ver-sion 3 (BNC XML Edition).
Distributed by Oxford Uni-versity Computing Services on behalf of the BNC Consor-tium.R.
Bod, J. Hay, and S. Jannedy.
2003.
Probabilistic linguis-tics.
MIT Press.N.
Chater, J.B. Tenenbaum, and A. Yuille.
2006.
Probabilis-tic models of cognition: Conceptual foundations.
Trendsin Cognitive Sciences, 10(7):287?291.N.
Chomsky.
1957.
Syntactic Structures.
Mouton, TheHague.A.
Clark and S. Lappin.
2011.
Linguistic Nativism and thePoverty of the Stimulus.
Wiley-Blackwell, Malden, MA.A.
Clark, G. Giorgolo, and S. Lappin.
2013.
Towards a sta-tistical model of grammaticality.
In Proceedings of the35th Annual Conference of the Cognitive Science Society.Sandiway Fong, Igor Malioutov, Beracah Yankama, andRobert C. Berwick.
2013.
Treebank parsing andknowledge of language.
In Aline Villavicencio, ThierryPoibeau, Anna Korhonen, and Afra Alishahi, editors, Cog-nitive Aspects of Computational Language Acquisition,Theory and Applications of Natural Language Processing,pages 133?172.
Springer Berlin Heidelberg.Stefan Frank, Rens Bod, and Morten Christiansen.
2012.How hierarchical is language use?
In Proceedings of theRoyal Society B, number doi: 10.1098/rspb.2012.1741.J.T.
Goodman.
2001.
A bit of progress in language model-ing.
Computer Speech & Language, 15(4):403?434.A.
Pauls and D. Klein.
2012.
Large-scale syntactic languagemodeling with treelets.
In Proceedings of the 50th AnnualMeeting of the Association for Computational Linguistics,pages 959?968.
Jeju, Korea.F.
Pereira.
2000.
Formal grammar and information theory:together again?
Philosophical Transactions of the RoyalSociety of London.
Series A: Mathematical, Physical andEngineering Sciences, 358(1769):1239?1253.M.
Post.
2011.
Judging grammaticality with tree substitutiongrammar derivations.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguistics:Human Language Technologies, pages 217?222.F.
Reali and M.H.
Christiansen.
2005.
Uncovering the rich-ness of the stimulus: Structure dependence and indirectstatistical evidence.
Cognitive Science, 29(6):1007?1028.36
