Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for HLT, pages 44?52,Dublin, Ireland, August 23rd 2014.Significance of Bridging Real-world Documents and NLP TechnologiesTadayoshi Hara Goran Topic?
Yusuke Miyao Akiko AizawaNational Institute of Informatics2-1-2 Hitotsubashi, Chiyoda-ku, Tokyo 101-8430, Japan{harasan, goran topic, yusuke, aizawa}@nii.ac.jpAbstractMost conventional natural language processing (NLP) tools assume plain text as their input,whereas real-world documents display text more expressively, using a variety of layouts, sentencestructures, and inline objects, among others.
When NLP tools are applied to such text, usersmust first convert the text into the input/output formats of the tools.
Moreover, this awkwardlyobtained input typically does not allow the expected maximum performance of the NLP tools tobe achieved.
This work attempts to raise awareness of this issue using XML documents, wheretextual composition beyond plain text is given by tags.
We propose a general framework fordata conversion between XML-tagged text and plain text used as input/output for NLP tools andshow that text sequences obtained by our framework can be much more thoroughly and efficientlyprocessed by parsers than naively tag-removed text.
These results highlight the significance ofbridging real-world documents and NLP technologies.1 IntroductionRecent advances in natural language processing (NLP) technologies have allowed us to dream aboutapplying these technologies to large-scale text, and then extracting a wealth of information from the textor enriching the text itself with various additional information.
When actually considering the realizationof this dream, however, we are faced with an inevitable problem.
Conventional NLP tools usually assumean ideal situation where each input text consists of a plain word sequence, whereas real-world documentsdisplay text more expressively using a variety of layouts, sentence structures, and inline objects, amongothers.
This means that obtaining valid input for a target NLP tool is left completely to the users, whohave to program pre- and postprocessors for each application to convert their target text into the requiredformat and integrate the output results into their original text.
This additional effort reduces the viabilityof technologies, while the awkwardly obtained input does not allow the expected maximum benefit ofthe NLP technologies to be realized.In this research, we raise awareness of this issue by developing a framework that simplifies this con-version and integration process.
We assume that any textual composition beyond plain text is captured bytags in XML documents, and focus on the data conversion between XML-tagged text and the input/outputformats of NLP tools.
According to our observations, the data conversion process is determined by thetextual functions of the XML-tags utilized in the target text, of which there seem to be only four types.We therefore devise a conversion strategy for each of the four types.
After all tags in the XML tagset ofthe target text have been classified by the user into the four types, data conversion and integration can beexecuted automatically using our strategies, regardless of the size of the text (see Figure 1).In the experiments, we apply our framework to several types of XML documents, and the results showthat our framework can extract plain text sequences from the target XML documents by classifying only20% or fewer of the total number of tag types.
Furthermore, with the obtained sequences, two typicalparsers succeed in processing entire documents with a much greater coverage rate and using much lessparsing time than with naively tag-removed text.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/44Formatted Input(e.g.
plain text)Output analysis(e.g.
parse trees)<p> A user task is a scenario of use of the UI, ?
the UMLnotation.
In our case, we use the CTT (Concur Task Tree)<cite>[ <bibref bibrefs="paterno-ctte-2001,paterno-ctt-2001" separator="," show="Number" yyseparator=","/>]</cite></p>A user task is a scenario of use of the UI, ?
the UML notation.In our case, we use the CTT (Concur Task Tree) [1]<sentence id=?s0?><cons>?</cons></sentence><sentence id=?s1?>?<cons><tok>[</tok></cons><cons><cons><tok>1</tok></cons> ?<cons><tok>]</tok></cons>?</sentence>IndependentDecorationObjectMeta-infoTag1Tag2?
(Classifyinto 4 types)TextstructuredbyXMLTextstructuredbyXMLTextstructuredby XMLTagsetDataconversion NLP tools(Parser)Figure 1: Proposed data conversion framework for applying NLP tools to text structured as XMLTag type Criteria for classification Strategies for data conversionIndependent To represent a region syntactically Remove the tag and tagged regionindependent from the surrounding text ?
(apply tools to the tagged region independently)?
recover the (analyzed) tagged region after applying the toolsDecoration To set the display style of Remove only the tagthe tagged region at the same level as ?
recover the tag after applying the toolsthe surrounding textObject To represent the minimal object Replace the tag (and the tagged region) with a plain wordunit that should be handled in the ?
(do not process the tagged region further)the same level as the surrounding text ?
recover the tag (and region) after applying the toolsMeta-info To describe the display style Remove the tag and tagged regionsetting or additional information ?
(do not process the tagged region further)?
recover the tag and region after applying the toolsTable 1: Four types of tags and the data conversion strategy for each typeThe contribution of this work is to demonstrate the significance of bridging real-world documents andNLP technologies in practice.
We show that, if supported by a proper framework, conventional NLP toolsalready have the ability to process real-world text without significant loss of performance.
We expect thedemonstration to promote further discussion on real-world document processing.In Section 2, some related research attempts are introduced.
In Section 3, the four types of textualfunctions for XML tags and our data conversion strategies for each of these are described and imple-mented.
In Section 4, the efficiency of our framework and the adequacy of the obtained text sequencesfor use in NLP tools are examined using several types of documents.2 Related WorkTo the best of our knowledge, no significant work on a unified methodology for data conversion betweentarget text and the input/output formats of NLP tools has been published.
Some NLP tools providescripts for extracting valid input text for the tools from real-world documents; however, even these scriptsassume specific formats for the documents.
For example, deep syntactic parsers such as the C&C Parser(Clark and Curran, 2007) and Enju (Ninomiya et al., 2007) assume POS-tagged sentences as input, andtherefore the distributed packages for the parsers1 contain POS-taggers together with the parsers.
ThePOS-taggers assume plain text sentences as their input.As the work most relevant to our study, UIMA (Ferruci et al., 2006) deals with various annotationsin an integrated framework.
However, in this work, the authors merely proposed the framework and did1[C&C Parser]: http://svn.ask.it.usyd.edu.au/trac/candc/wiki / [Enju]: http://kmcs.nii.ac.jp/enju/45New UI is shown.
The UI is more useful  than XYZ          , and ... .text indexmarkCite1Notice that ?
.
notecite<text>New UI</text> is shown.
The UI is more useful than XYZ <indexmark>?
</indexmark> in <cite>[ ?
]</cite><note>Notice that ?
.</note>, and ?
.sentence sentenceNew UI is shown.
The UI is more useful  than XYZ          , and ... .Cite1 sentence<sentence><text>New UI</text> is shown.</sentence> <sentence>The UI is more useful  than XYZ<indexmark>?
</indexmark> in <cite>[?
]</cite><note><sentence>Notice that ?
.
</sentence></note>, and ?
.</sentence>(a)(c)(d)(b)text indexmark notecite Notice that ?
.Meta-infoDecorationIndependentObjectFigure 2: Example of executing our strategynot explain how the given text can be used in a target annotation process such as parsing.
Some projectsbased on the UIMA framework, such as RASP4UIMA (Andersen et al., 2008), U-compare (Kano etal., 2011), and Kachako (Kano, 2012)2, have developed systems where the connections between variousdocuments and various tools are already established.
Users, however, can utilize only the text and toolpairs that have already been integrated into the systems.
GATE (Cunningham et al., 2013) is based ona similar concept to UIMA; it supports XML documents as its input, while the framework also requiresintegration of tools into the systems.In our framework, although availability of XML documents is assumed, a user can apply NLP tools tothe documents without modifying the tools; instead, this is achieved by merely classifying the XML-tagsin the documents into a small number of functional types.3 Data Conversion FrameworkWe designed a framework for data conversion between tagged text and the input/output formats of NLPtools based on the four types of textual functions of tags.
First, we introduce the four tag types andthe data conversion strategy for each.
Then, we introduce the procedure for managing the entire dataconversion process using the strategies.3.1 Strategies for the Four Tag TypesThe functions of the tags are classified into only four types, namely, Independent, Decoration, Object,and Meta-info, and for each of these types, a strategy for data conversion is described, as given in Table1.
This section explains the types and their strategies using a simple example where we attempt to applya sentence splitter to the text given in Figure 2(a).
The target text has four tags, ?<note>?, ?<text>?,?<cite>?, and ?<indexmark>?, denoting, respectively, Independent, Decoration, Object, and Meta-info tags.
We now describe each of the types.Regions enclosed by Independent tags contain syntactically independent text, such as titles, sections,and so on.
In some cases, a region of this type is inserted into the middle of another sentence, likethe ?<note>?
tags in the example, which represent footnote text.
The data conversion strategy for textcontaining these tags is to split the enclosed region into multiple subregions and apply the NLP toolsseparately to each subregion.2[U-compare]: http://u-compare.org/ / [Kachako]: http://kachako.org/kano/46<?xml ?><document ?><title>Formal approaches ?
</title><creator> ?
</creator><abstract>This research ?
</abstract><section><title>Introduction</title><para><p><text>New UI</text> is shown.
The UI is more useful than XYZ<indexmark> ?
</indexmark> in <cite>[ ?
]</cite><note>Notice that ?
</note> and ?
.</p></para></section><bibliography> ?
</bibliography></document>(a) XML document?xml documentcreator section bibliographytitle abstractThis research ?
Formal ?paratitleIntroductionptextNew UI indexmark?
cite[?]
noteNotice ?
.
and ?
.inis ?
XYZ(b) Structure of XML documentFigure 3: Example XML documentDecoration tags, on the other hand, do not necessarily guarantee the independence of the enclosedtext regions, and are utilized mainly for embossing the regions visually, such as changing the font orcolor of the text (?<text>?
in the example), paragraphing sections3, and so on.
The data conversionstrategy for text containing these tags is to remove the tags before inputting the text into the NLP tools,and then to recover the tags afterwards4.Regions enclosed by Object tags contain special descriptions for representing objects treated as singlesyntactic components in the surrounding text.
The regions do not consist of natural language text, andtherefore cannot be analyzed by NLP tools5.
The data conversion strategy for text containing this tag isto replace the enclosed region with some proper character sequence before inputting the text into NLPtools, and then to recover the replaced region afterwards.Regions enclosed by Meta-info tags are not targets of NLP tools, mainly because the regions are notdisplayed, but utilized for other purposes, such as creating index pages (like this ?<indexmark>?)6.
Thedata conversion strategy for text containing these tags is to delete the tagged region before inputting thetext into NLP tools, and then to recover the region afterwards.According to the above strategies, which are summarized in Table 1, conversion of the example textin Figure 2(a) is carried out as follows.
In the first step, tags are removed from the text, whilst retainingtheir offsets in the resulting tag-less sequence shown in (b).
?Cite1?
in the sequence is a plain wordutilized to replace the ?<cite>?
tag region.
For the ?<note>?
tag, we recursively apply our strategiesto its inner region, with two plain text regions ?New UI ...?
and ?Notice that ...?
consequently input intothe sentence splitter.
Thereafter, sentence boundary information is returned as shown in (c), and finally,using the retained offset information of the tags, the obtained analysis and original tag information areintegrated to produce the XML-tagged sequence shown in (d).3.2 Procedure for Efficient Tag Classification and Data ConversionIn actual XML documents as shown in Figure 3(a), a number of tags are introduced and tagged regionsare multi-layered as illustrated in Figure 3(b) (where black and white boxes represent, respectively, XMLtags and plain text regions, and regions enclosed by tags are placed below the tags in the order theyappear.).
We implemented a complete data conversion procedure for efficiently classifying tags in textdocuments into the four types and simultaneously obtaining plain text sequences from such documents,3In some types of scientific articles, one sentence can be split into two paragraph regions.
It depends on the target textwhether a paragraph tag is classified as Independent or Decoration.4The tags may imply that the enclosed regions constitute chunks of text, which may be suitable for use in NLP tools.5In some cases, natural language text is used for parts of the descriptions, for example, itemization or tables in scientificarticles.
How the inner textual parts are generally associated with the surrounding text would be discussed in our future work.For the treatment of list structures, we can learn more from A?
?t-Mokhtar et al.
(2003).6If the tagged region contains analyzable text, it depends on the user policy whether NLP tools should be applied to theregion, that is, whether to classify the tag as Independent.47@plain_text_sequences = ();   # plain text sequences input to NLP tools@recovery_info = ();                 # information for recovering original document after applying NLP tools@unknown = ();                        # unknown tagsfunction data_convert ($target_sequence, $seq_ID) {if ($target_sequence contains any tags) {   # process one instance of tag usage in a target sequence$usage = (pick one instance of top-level tag usage in $target_sequence);$tag = (name of the top-level tag in $usage);@attributes = (attributes and their values for the top-level tag in $usage);$region = (region in $target_sequence enclosed by tag $tag in $usage); $tag_and_region =  (region in $target_sequence consisting of $region & tag $tag enclosing it);if ($tag ?
@independent)             { remove $tag_and_region from $target_sequence;add [?independent?, $tag, @attributes,  $seq_ID, $seq_ID + 1,(offset in $target_sequence where $tag_and_region should be inserted) ] to @recovery_info;data_convert($region,  $seq_ID + 1);  } # process the tagged region separatelyelse if ($tag ?
@decoration)         { remove only tag $tag enclosing $region from $target_sequence; add [?decoration?, $tag, @attributes,(offsets in $target_sequence where $region begans and ends)] to @recovery_info; }else if ($tag ?
@object)                 { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?object?, $uniq, $tag_and_region] to @recovery_info; }else if ($tag ?
@meta_info)          { remove $tag_and_region from $target_sequence;add [?meta_info?, $tag_and_region,(offset in $target_sequence where $tag_and_region should be inserted)] to @recovery_info; }else                                                     { replace $tag_and_region in $target_sequence with a unique plain word $uniq;add [?unknown?, $uniq, $tag_and_region] to @recovery_info;if ($tag ?
@unknown) { add $tag to @unknown; }  }data_convert($target_sequence, $seq_ID);  # process the remaining tags}else {  # a plain text sequence is obtainedadd [$seq_id,  $target_sequence] to @plain_text_sequences;} }function main ($XML_document) {data_convert ($XML_document, 0);return @plain_text_sequences, @recovery_info, @unknown;}Figure 4: Pseudo-code algorithm for data conversion from XML text to plain text for use in NLP toolsas given by the pseudo-code algorithm in Figure 4.
In the remainder of this section, we explain how thealgorithm works.Our data conversion procedure applies the strategies for the four types of tags recursively from thetop-level tags to the lower tags.
The @independent, @decoration, @object and @meta info listscontain, respectively, Independent, Decoration, Object, and Meta-info tags, which have already beenclassified by the user.
When applied to a target document, the algorithm uses the four lists and strategiesgiven in the previous section in its first attempt at converting the document into plain text sequences,storing unknown (and therefore unprocessed) tags, if any, in @unknown.
After the entire document hasbeen processed for the first time, the user classifies any reported unknown tags.
This process is repeateduntil no further unknown tags are encountered.In the first iteration of processing the document in Figure 3(a) the algorithm is applied to the targetdocument with the four tag lists empty.
In the function ?data convert?, top-level tags in the document,?<?xml>?
and ?<document>?, are detected as yet-to-be classified tags and added to @unknown.The tags and their enclosed regions in the target document are replaced with unique plain text such as?UN1?
and ?UN2?, and the input text thus becomes a sequence consisting of only plain words like ?UN1UN2?.
The algorithm then adds the sequence to @plain text sequences and terminates.
The user thenclassifies the reported yet-to-be classified tags in @unknown into the four tag lists, and the algorithm48Article # ar- # total tags # classified tags (# types) #obtain-type ticles (# types) I D O M Total ed seq.PMC 1,000 1,357,229( 421) 32,109(12) 62,414( 8) 48205( 9) 33,953(56) 176,681( 85) 25,679ArX.
300 1,969,359(210?)
5,888(15) 46,962(12) 60,194( 8) 7,960(17) 121,004( 52) 4,167ACL 67 130,861( 66?)
3,240(24) 14,064(29) 4,589(15) 2,304(19) 24,197( 87) 2,293Wiki.
300 223,514( 60?)
3,530(12) 11,197( 8) 1,470(28) 11,360(67) 27,557(115) 2,286(ArX.
: arXiv.org, Wiki.
: Wikipedia, I: Independent, D: Decoration, O: Object, M: Meta-info)Table 2: Classified tags and obtained sequences for each type of articleTreat- Parsing with Enju parser Parsing with Stanford parserArticle ed tag * # sen- ** Time Avg.
# failures * # sen- ** Time Avg.
# failurestype classes tences (s) (**/*) (rate) tences (s) (**/*) (rate)None 159,327 209,783 1.32 4,721 ( 2.96%) 170,999 58,865 0.39 18,621 (10.89%)PMC O/M 112,285 135,752 1.21 810 ( 0.72%) 126,176 50,741 0.44 11,881 ( 9.42%)All 126,215 132,250 1.05 699 ( 0.55%) 139,805 63,295 0.49 11,338 ( 8.11%)None 74,762 108,831 1.46 2,047 ( 2.74%) 75,672 27,970 0.43 10,590 (13.99%)ArX.
O/M 41,265 89,200 2.16 411 ( 1.00%) 48,666 24,630 0.57 5,457 (11.21%)All 43,208 87,952 2.04 348 ( 0.81%) 50,504 26,360 0.58 5,345 (10.58%)None 19,571 15,142 0.77 115 ( 0.59%) 17,166 5,047 0.29 1,095 ( 6.38%)ACL O/M 9,819 9,481 0.97 63 ( 0.64%) 11,182 4,157 0.37 616 ( 5.51%)All 11,136 8,482 0.76 39 ( 0.35%) 12,402 4,871 0.39 587 ( 4.73%)None 10,561 14,704 1.39 1,161 (10.99%) 14,883 3,114 0.24 1,651 (11.09%)Wiki.
O/M 5,026 6,743 1.34 67 ( 1.33%) 6,173 2,248 0.38 282 ( 4.57%)All 6,893 6,058 0.88 61 ( 0.88%) 8,049 2,451 0.31 258 ( 3.21%)(ArX.
: arXiv, Wiki.
: Wikipedia, O/M: Object and Meta-info)Table 3: Impact on parsing performance of plain text sequences extracted using classified tagsstarts its second iteration7.In the case of Independent/Decoration tags, the algorithm splits the regions enclosed by thetags/removes only the tags from the target text, and recursively processes the obtained text sequence(s)according to our strategies.
In the splitting/removal operation, the algorithm stores in @recovery info,the locations (offsets) in the obtained text where the tags should be inserted in order to recover the tagsand textual structures after applying the NLP tools.
In the case of Object/Meta-info tags, regions en-closed by these tags are replaced with unique plain text/omitted from the target text, which means that theinner regions are not unpacked and processed (with relevant information about the replacement/omittingprocess also stored in @recovery info).
This avoids unnecessary classification tasks for tags that areutilized only in the regions, and therefore minimizes user effort.When no further unknown tags are reported, sufficient tag classification has been done to obtain plaintext sequences for input into NLP tools, with the sequences already stored in @plain text sequences.After applying NLP tools to the obtained sequences, @recovery info is used to integrate the anno-tated output from the tools into the original XML document by merging the offset information8, andconsequently to recover the structure of the original document.4 ExperimentsWe investigated whether the algorithm introduced in Section 3.2 is robustly applicable to different typesof XML documents and whether the obtained text sequences are adequate for input into NLP tools.
Theresults of this investigation highlight the significance of bridging real-world text and NLP technologies.4.1 Target DocumentsOur algorithm was applied to four types of XML documents: three types of scientific ar-ticles, examples of which were, respectively, downloaded from PubMed Central (PMC)(http://www.ncbi.nlm.nih.gov/pmc/tools/ftp/), arXiv.org (http://arxiv.org/) and ACL Anthology7The user can delay the classification for some tags to later iterations.8When crossover of tag regions occur, the region in the annotated output is divided into subregions at the crossover point.49(http://anthology.aclweb.org/)9, and a web page type, examples of which were downloaded fromWikipedia (http://www.wikipedia.org/).
The articles obtained from PMC were originally given in anXML format, while those from arXiv.org and ACL Anthology were given in XHTML (based on XML),and those from Wikipedia were given in HTML, with the HTML articles generated via intermediateXML files.
These four types of articles were therefore more or less based on valid XML (or XML-like)formats.
For our experiments, we randomly selected 1,000 PMC articles, randomly selected 300arXiv.org articles, collected 67 (31 long and 36 short) ACL 2014 conference papers without anyconversion errors (see Footnote 9), and randomly downloaded 300 Wikipedia articles.Each of the documents contained a variety of textual parts; we decided to apply the NLP tools to thetitles of the articles and sections, abstracts, and body text of the main sections in the scientific articles,and to the titles of the articles, body text headings, and the actual body text of the Wikipedia articles.According to these policies, we classified the tags appearing in all articles of each type.4.2 Efficiency of Tag ClassificationTable 2 summarizes the classified tags and obtained sequences for each type of document.
The secondto ninth columns give the numbers of utilized articles, tags (in tokens and types) in the documents, eachtype of tag actually classified and processed, and obtained text sequences, respectively10.
Using simpleregular-expression matching, we found no remaining tagged regions in the obtained sequences.
Fromthis we concluded that our framework at least succeeded in converting XML-tagged text into plain text.For the PMC articles, we obtained plain text sequences by classifying only a fifth or less of the totalnumber of tag types, that is, focusing on less than 15% of the total tag occurrences in the documents(comparing the third and eighth columns).
This is because the tags within the regions enclosed byObject and Meta-info tags were not considered by our procedure.
For each of the arXiv.org, ACL andWikipedia articles, a similar effect was implied by the fact that the number of classified tags was lessthan 20% of the total occurrences of all tags.4.3 Adequacy of Obtained Sequences for Use in NLP ToolsWe randomly selected several articles from each article type, and confirmed that the obtained text se-quences consisted of valid sentences, which could be directly input into NLP tools and which thoroughlycovered the content of the original articles.
Then, to evaluate the impact of this adequacy in a more prac-tical situation, we input the obtained sequences (listed in Table 2) into two typical parsers, namely, theEnju parser for deep syntactic/semantic analysis, and the Stanford parser (de Marneffe et al., 2006)11 forphrase structure and dependency analysis12.
Table 3 compares the parsing performance on three typesof plain text sequences obtained by different strategies: simply removing all tags, processing Objectand Meta-info tags using our framework and removing the remaining tags, and processing all the tagsusing our framework.
For each combination of parser and article type, we give the number of detectedsentences13, the total parsing time, the average parsing time per sentence14, and the number/ratio ofsentences that could not be parsed15.For all article types, the parsers, especially the Enju parser, succeeded in processing the entire articlewith much higher coverage (see the fourth column for each parser) and in much less time (see the third9The XHTML version of 178 ACL 2014 conference papers were available at ACL Anthology.
Each of the XHTML fileswas generated by automatic conversion of the original article using LaTeXML (http://dlmf.nist.gov/LaTeXML/).10For Wikipedia, arXiv.org and ACL articles, since HTML/XHTML tag names represent more abstract textual functions,the number of different tag types was much smaller than for PMC articles (see ?
in the table).
To better capture the textualfunctions of the tagged regions, we used the combination of the tag name and its selected attributes as a single tag.
The numberof classified tags for Wikipedia, arXiv.org and ACL given in the table reflects this decision.11http://nlp.stanford.edu/software/lex-parser.shtml12The annotated output from the parsers was integrated into the original XML documents by merging the offset information,and the structures of the original documents were consequently recovered.
The recovered structures were input to xmllint, aUNIX tool for parsing XML documents, and the tool succeeded in parsing the structures without detecting any error.13For the Enju parser, we split each text sequence into sentences using GeniaSS [http://www.nactem.ac.uk/y-matsu/geniass/].14For the Enju parser, the time spent parsing failed sentences was also considered.15For the Stanford parser, the maximum sentence length was limited to 50 words using the option settings because severalsentences caused parsing failures, even after increasing the memory size from 150 MB to 2 GB, which terminated the wholeprocess.
50column for each parser) using the text sequences obtained by treating some (Object and Meta-info)or all tags with our framework than with those sequences obtained by merely removing the tags.
Thisis mainly because the text sequences obtained by merely removing the tags contained some embeddedinserted sentences (specified by Independent tags), bare expressions consisting of non natural language(non-NL) principles (specified by Object tags), and sequences not directly related to the displayed text(specified by Meta-info tags), which confused the parsers.
In particular, treating Object and Meta-infotags drastically improved parsing performance, since non-NL tokens were excluded from the analysis.Compared with treating Object/Meta-info tags, treating all tags, that is, additionally treating Inde-pendent tags and removing the remaining tags as Decoration tags, increased the number of detectedsentences.
This is because Independent tags provide solid information for separating text sequencesinto shorter sequences and thus prompting the splitting of sequences into shorter sentences, which de-creased parsing failure by preventing a lack of search space for the Enju parser and by increasing target(?
50 word) sentences for the Stanford parser.
Treating all tags increased the total time for the Stanfordparser since a decrease in failed (> 50 word) sentences directly implied an increase in processing cost,whereas, for the Enju parser, the total time decreased since the shortened sentences drastically narrowedthe required search space.4.4 Significance of Bridging Real-world Documents and NLP TechnologiesAs demonstrated above, the parsers succeeded in processing the entire article with much higher coverageand in much less time with the text sequences obtained by our framework than with those sequencesobtained by merely removing the tags.
Then, what does such thorough and efficient processing bringabout?
If our target is shallow analysis of documents which can be achieved by simple approaches suchas counting words, removing tags will suffice; embedded sentences do not affect word count, and non-NLsequences can be canceled by a large amount of valid sequences in the target documents.Such shallow approaches, however, cannot satisfy the demands on more detailed or precise analysisof documents: discourse analysis, translation, grammar extraction, and so on.
In order to be sensitive tosubtle signs from the documents, information uttered even in small parts of text cannot be overlooked,under the condition that sequences other than body text are excluded.This process of plain text extraction is a well-established procedure in NLP research; in order toconcentrate on precise analysis of natural language phenomena, datasets have been arranged in the formatof plain text sequences, and, using those datasets, plenty of remarkable achievements have been reportedin various NLP tasks while brand-new tasks have been found and tackled.But what is the ultimate goal of these challenges?
Is it to just parse carefully arranged datasets?
We allknow this to be just a stepping stone to the real goal: to parse real-world, richly-formatted documents.
Aswe demonstrated, if supported by a proper framework, conventional NLP tools already have the abilityto process real-world text without significant loss of performance.
Adequately bridging target real-worlddocuments and NLP technologies is thus a crucial task for taking advantage of full benefit brought byNLP technologies in ubiquitous application of NLP.5 ConclusionWe proposed a framework for data conversion between XML-tagged text and input/output formats ofNLP tools.
In our framework, once each tag utilized in the XML-tagged text has been classified as oneof the four types of textual functions, the conversion is automatically done according to the classification.In the experiments, we applied our framework to several types of documents, and succeeded in obtainingplain text sequences from these documents by classifying only a fifth of the total number of tag typesin the documents.
We also observed that with the obtained sequences, the target documents were muchmore thoroughly and efficiently processed by parsers than with naively tag-removed text.
These resultsemphasize the significance of bridging real-world documents and NLP technologies.We are now ready for public release of a tool for conversion of XML documents into plain text se-quences utilizing our framework.
We would like to share further discussion on applying NLP tools tovarious real-world documents for increased benefits from NLP.51AcknowledgementsThis research was partially supported by ?Data Centric Science: Research Commons?
at the ResearchOrganization of Information and Systems (ROIS), Japan.ReferencesSalah A?
?t-Mokhtar, Veronika Lux, and ?Eva Ba?nik.
2003.
Linguistic parsing of lists in structured documents.
InProceedings of Language Technology and the Semantic Web: 3rd Workshop on NLP and XML (NLPXML-2003),Budapest, Hungary, April.?.
Andersen, J. Nioche, E.J.
Briscoe, and J. Carroll.
2008.
The BNC parsed with RASP4UIMA.
In Proceedingsof the 6th Language Resources and Evaluation Conference (LREC 2008), pages 865?869, Marrakech, Morocco,May.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCG and log-linearmodels.
Computational Linguistics, 33(4):493?552.H.
Cunningham, V. Tablan, A. Roberts, and K. Bontcheva.
2013.
Getting more out of biomedical documents withGATE?s full lifecycle open source text analytics.
PLoS Comput Biol, 9(2).Marie-Catherine de Marneffe, Bill MacCartney, and Christopher D. Manning.
2006.
Generating typed dependencyparses from phrase structure parses.
In Proceedings of the 5th Language Resources and Evaluation Conference(LREC 2006), pages 449?454, Genoa, Italy, May.David Ferruci, Adam Lally, Daniel Gruhl, Edward Epstein, Marshall Schor, J. William Murdock, Andy Frenkiel,Eric W. Brown, Thomas Hampp, Yurdaer Doganata, Christopher Welty, Lisa Amini, Galina Kofman, Lev Koza-kov, and Yosi Mass.
2006.
Towards an interoperability standard for text and multi-modal analytics.
TechnicalReport RC24122, IBM Research Report.Yoshinobu Kano, Makoto Miwa, Kevin Cohen, Larry Hunter, Sophia Ananiadou, and Jun?ichi Tsujii.
2011.U-Compare: a modular NLP workflow construction and evaluation system.
IBM Journal of Research andDevelopment, 55(3):11:1?11:10.Yoshinobu Kano.
2012.
Kachako: a hybrid-cloud unstructured information platform for full automation of servicecomposition, scalable deployment and evaluation.
In Proceedings in the 1st International Workshop on Analyt-ics Services on the Cloud (ASC), the 10th International Conference on Services Oriented Computing (ICSOC2012), Shanghai, China, November.Takashi Ninomiya, Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.
2007.
A log-linear model with ann-gram reference distribution for accurate HPSG parsing.
In Proceedings of the 10th International Conferenceon Parsing Technologies (IWPT?07), Prague, Czech Republic, June.52
