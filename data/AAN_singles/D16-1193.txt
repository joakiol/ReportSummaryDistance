Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1882?1891,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Neural Approach to Automated Essay ScoringKaveh Taghipour and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore13 Computing DriveSingapore 117417{kaveh, nght}@comp.nus.edu.sgAbstractTraditional automated essay scoring systemsrely on carefully designed features to evaluateand score essays.
The performance of suchsystems is tightly bound to the quality of theunderlying features.
However, it is laboriousto manually design the most informative fea-tures for such a system.
In this paper, we de-velop an approach based on recurrent neuralnetworks to learn the relation between an es-say and its assigned score, without any fea-ture engineering.
We explore several neuralnetwork models for the task of automated es-say scoring and perform some analysis to getsome insights of the models.
The results showthat our best system, which is based on longshort-term memory networks, outperforms astrong baseline by 5.6% in terms of quadraticweighted Kappa, without requiring any fea-ture engineering.1 IntroductionThere is a recent surge of interest in neural networks,which are based on continuous-space representationof the input and non-linear functions.
Hence, neuralnetworks are capable of modeling complex patternsin data.
Moreover, since these methods do not de-pend on manual engineering of features, they can beapplied to solve problems in an end-to-end fashion.SENNA (Collobert et al, 2011) and neural machinetranslation (Bahdanau et al, 2015) are two notableexamples in natural language processing that oper-ate without any external task-specific knowledge.
Inthis paper, we report a system based on neural net-works to take advantage of their modeling capacityand generalization power for the automated essayscoring (AES) task.Essay writing is usually a part of the student as-sessment process.
Several organizations, such asEducational Testing Service (ETS)1, evaluate thewriting skills of students in their examinations.
Be-cause of the large number of students participat-ing in these exams, grading all essays is very time-consuming.
Thus, some organizations have been us-ing AES systems to reduce the time and cost of scor-ing essays.Automated essay scoring refers to the process ofgrading student essays without human interference.An AES system takes as input an essay written fora given prompt, and then assigns a numeric score tothe essay reflecting its quality, based on its content,grammar, and organization.
Such AES systems areusually based on regression methods applied to a setof carefully designed features.
The process of fea-ture engineering is the most difficult part of buildingAES systems.
Moreover, it is challenging for hu-mans to consider all the factors that are involved inassigning a score to an essay.Our AES system, on the other hand, learns thefeatures and relation between an essay and its scoreautomatically.
Since the system is based on recur-rent neural networks, it can effectively encode theinformation required for essay evaluation and learnthe complex patterns in the data through non-linearneural layers.
Our system is among the first AESsystems based on neural networks designed with-out any hand-crafted features.
Our results showthat our system outperforms a strong baseline and1https://www.ets.org1882achieves state-of-the-art performance in automatedessay scoring.
In order to make it easier for other re-searchers to replicate our results, we have made thesource code of our system publicly available2.The rest of this paper is organized as follows.
Sec-tion 2 gives an overview of related work in the liter-ature.
Section 3 describes the automated essay scor-ing task and the evaluation metric used in this paper.We provide the details of our approach in Section4, and present and discuss the results of our experi-mental evaluation in Section 5.
Finally, we concludethe paper in Section 6.2 Related WorkThere exist many automated essay scoring systems(Shermis and Burstein, 2013) and some of them arebeing used in high-stakes assessments.
e-rater (At-tali and Burstein, 2004) and Intelligent Essay As-sessor (Foltz et al, 1999) are two notable examplesof AES systems.
In 2012, a competition on auto-mated essay scoring called ?Automated Student As-sessment Prize?
(ASAP)3 was organized by Kaggleand sponsored by the Hewlett Foundation.
A com-prehensive comparison of AES systems was madein the ASAP competition.
Although many AES sys-tems have been developed to date, they have beenbuilt with hand-crafted features and supervised ma-chine learning algorithms.Researchers have devoted a substantial amount ofeffort to design effective features for automated es-say scoring.
These features can be as simple as es-say length (Chen and He, 2013) or more compli-cated such as lexical complexity, grammaticality of atext (Attali and Burstein, 2004), or syntactic features(Chen and He, 2013).
Readability features (Zeschet al, 2015) have also been proposed in the liter-ature as another source of information.
Moreover,text coherence has also been exploited to assess theflow of information and argumentation of an essay(Chen and He, 2013).
A detailed overview of thefeatures used in AES systems can be found in (Zeschet al, 2015).
Moreover, some attempts have beenmade to address different aspects of essay writingindependently.
For example, argument strength andorganization of essays have been tackled by some2https://github.com/nusnlp/nea3https://www.kaggle.com/c/asap-aesresearchers through designing task-specific featuresfor each aspect (Persing et al, 2010; Persing and Ng,2015).Our system, however, accepts an essay text asinput directly and learns the features automaticallyfrom the data.
To do so, we have developed amethod based on recurrent neural networks to scorethe essays in an end-to-end manner.
We have ex-plored a variety of neural network models in this pa-per to identify the most suitable model.
Our bestmodel is a long short-term memory neural network(Hochreiter and Schmidhuber, 1997) and is trainedas a regression method.
Similar recurrent neural net-work approaches have recently been used success-fully in a number of other NLP tasks.
For exam-ple, Bahdanau et al (2015) have proposed an atten-tive neural approach to machine translation based ongated recurrent units (Cho et al, 2014).
Neural ap-proaches have also been used for syntactic parsing.In (Vinyals et al, 2015), long short-term memorynetworks have been used to obtain parse trees byusing a sequence-to-sequence model and formulat-ing the parsing task as a sequence generation prob-lem.
Apart from these examples, recurrent neuralnetworks have also been used for opinion mining (Ir-soy and Cardie, 2014), sequence labeling (Ma andHovy, 2016), language modeling (Kim et al, 2016;Sundermeyer et al, 2015), etc.3 Automated Essay ScoringIn this section, we define the automated essay scor-ing task and the evaluation metric used for assessingthe quality of AES systems.3.1 Task DescriptionAutomated essay scoring systems are used in evalu-ating and scoring student essays written based on agiven prompt.
The performance of these systems isassessed by comparing their scores assigned to a setof essays to human-assigned gold-standard scores.Since the output of AES systems is usually a real-valued number, the task is often addressed as a su-pervised machine learning task (mostly by regres-sion or preference ranking).
Machine learning algo-rithms are used to learn the relationship between theessays and reference scores.18833.2 Evaluation MetricThe output of an AES system can be comparedto the ratings assigned by human annotators usingvarious measures of correlation or agreement (Yan-nakoudakis and Cummins, 2015).
These measuresinclude Pearson?s correlation, Spearman?s correla-tion, Kendall?s Tau, and quadratic weighted Kappa(QWK).
The ASAP competition adopted QWK asthe official evaluation metric.
Since we use theASAP data set for evaluation in this paper, we alsouse QWK as the evaluation metric in our experi-ments.Quadratic weighted Kappa is calculated as fol-lows.
First, a weight matrix W is constructed ac-cording to Equation 1:Wi,j =(i?
j)2(N ?
1)2 (1)where i and j are the reference rating (assigned bya human annotator) and the hypothesis rating (as-signed by an AES system), respectively, and N isthe number of possible ratings.
A matrix O is cal-culated such that Oi,j denotes the number of essaysthat receive a rating i by the human annotator anda rating j by the AES system.
An expected countmatrix E is calculated as the outer product of his-togram vectors of the two (reference and hypothe-sis) ratings.
The matrix E is then normalized suchthat the sum of elements in E and the sum of ele-ments in O are the same.
Finally, given the matricesO and E, the QWK score is calculated according toEquation 2:?
= 1?
?i,j Wi,jOi,j?i,j Wi,jEi,j(2)In our experiments, we compare the QWK scoreof our system to well-established baselines.
Wealso perform a one-tailed paired t-test to determinewhether the obtained improvement is statisticallysignificant.4 A Recurrent Neural Network ApproachRecurrent neural networks are one of the most suc-cessful machine learning models and have attractedthe attention of researchers from various fields.Compared to feed-forward neural networks, recur-rent neural networks are theoretically more powerfuland are capable of learning more complex patternsfrom data.
Therefore, we have mainly focused onrecurrent networks in this paper.
This section givesa description of the recurrent neural network archi-tecture that we have used for the essay scoring taskand the training process.4.1 Model ArchitectureThe neural network architecture that we have used inthis paper is illustrated in Figure 1.
We now describeeach layer in our neural network in detail.Lookup Table Layer: The first layer of ourneural network projects each word into a dLT di-mensional space.
Given a sequence of wordsW represented by their one-hot representations(w1,w2, ?
?
?
,wM ), the output of the lookup tablelayer is calculated by Equation 3:LT (W) = (E.w1,E.w2, ?
?
?
,E.wM ) (3)where E is the word embeddings matrix and will belearnt during training.Convolution Layer: Once the dense represen-tation of the input sequence W is calculated, it isfed into the recurrent layer of the network.
How-ever, it might be beneficial for the network to ex-tract local features from the sequence before apply-ing the recurrent operation.
This optional charac-teristic can be achieved by applying a convolutionlayer on the output of the lookup table layer.
Inorder to extract local features from the sequence,the convolution layer applies a linear transformationto all M windows in the given sequence of vec-tors4.
Given a window of dense word representa-tions x1,x2, ?
?
?
,xl, the convolution layer first con-catenates these vectors to form a vector x?
of lengthl.dLT and then uses Equation 4 to calculate the out-put vector of length dc:Conv(x?)
= W.x?
+ b (4)In Equation 4, W and b are the parameters of thenetwork and are shared across all windows in thesequence.4The number of input vectors and the number of output vec-tors of the convolution layer are the same because we pad thesequence to avoid losing border windows.1884w1w2wM-1wM......w3......Lookup table layerConvolution layerRecurrent layerMean over timeScoreLinear layer with Sigmoid activationFigure 1: The convolutional recurrent neural network architecture.The convolution layer can be seen as a functionthat extracts feature vectors from n-grams.
Sincethis layer provides n-gram level information to thesubsequent layers of the neural network, it can po-tentially capture local contextual dependencies inthe essay and consequently improve the perfor-mance of the system.Recurrent Layer: After generating embeddings(whether from the convolution layer or directly fromthe lookup table layer), the recurrent layer starts pro-cessing the input to generate a representation for thegiven essay.
This representation should ideally en-code all the information required for grading the es-say.
However, since the essays are usually long,consisting of hundreds of words, the learnt vectorrepresentation might not be sufficient for accuratescoring.
For this reason, we preserve all the inter-mediate states of the recurrent layer to keep trackof the important bits of information from process-ing the essay.
We experimented with basic recur-rent units (RNN) (Elman, 1990), gated recurrentunits (GRU) (Cho et al, 2014), and long short-termmemory units (LSTM) (Hochreiter and Schmidhu-ber, 1997) to identify the best choice for our task.Since LSTM outperforms the other two units, weonly describe LSTM in this section.Long short-term memory units are modified re-current units that can cope with the problem of van-ishing gradients more effectively (Pascanu et al,2013).
LSTMs can learn to preserve or forget theinformation required for the final representation.
Inorder to control the flow of information during pro-cessing of the input sequence, LSTM units make useof three gates to discard (forget) or pass the informa-tion through time.
The following equations formallydescribe the LSTM function:it = ?
(Wi.xt + Ui.ht?1 + bi)ft = ?
(Wf .xt + Uf .ht?1 + bf )c?t = tanh(Wc.xt + Uc.ht?1 + bc)ct = it ?
c?t + ft ?
ct?1ot = ?
(Wo.xt + Uo.ht?1 + bo)ht = ot ?
tanh(ct)(5)xt and ht are the input and output vectors at time t,respectively.
Wi, Wf , Wc, Wo, Ui, Uf , Uc, andUo are weight matrices and bi, bf , bc, and bo arebias vectors.
The symbol ?
denotes element-wisemultiplication and ?
represents the sigmoid func-tion.Mean over Time: The outputs of the recurrentlayer, H = (h1,h2, ?
?
?
,hM ), are fed into themean-over-time layer.
This layer receives M vec-tors of length dr as input and calculates an averagevector of the same length.
This layer?s function isdefined in Equation 6:MoT (H) = 1MM?t=1ht (6)1885The mean-over-time layer is responsible for aggre-gating the variable number of inputs into a fixedlength vector.
Once this vector is calculated, it isfed into the linear layer to be mapped into a score.Instead of taking the mean of the intermediate re-current layer states ht, we could use the last statevector hM to compute the score and remove themean-over-time layer.
However, as we will showin Section 5.2, it is much more effective to use themean-over-time layer and take all recurrent statesinto account.Linear Layer with Sigmoid Activation: Thelinear layer maps its input vector generated by themean-over-time layer to a scalar value.
This map-ping is simply a linear transformation of the in-put vector and therefore, the computed value is notbounded.
Since we need a bounded value in therange of valid scores for each prompt, we apply asigmoid function to limit the possible scores to therange of (0, 1).
The mapping of the linear layer afterapplying the sigmoid activation function is given byEquation 7:s(x) = sigmoid(w.x + b) (7)where x is the input vector (MoT (H)), w is theweight vector, and b is the bias value.We normalize all gold-standard scores to [0, 1]and use them to train the network.
However, dur-ing testing, we rescale the output of the network tothe original score range and use the rescaled scoresto evaluate the system.4.2 TrainingWe use the RMSProp optimization algorithm(Dauphin et al, 2015) to minimize the mean squarederror (MSE) loss function over the training data.Given N training essays and their correspondingnormalized gold-standard scores s?i , the model com-putes the predicted scores si for all training essaysand then updates the network parameters such thatthe mean squared error is minimized.
The loss func-tion is shown in Equation 8:MSE(s, s?)
= 1NN?i=1(si ?
s?i )2 (8)Additionally, we make use of dropout regularizationto avoid overfitting.
We also clip the gradient if thenorm of the gradient is larger than a threshold.We do not use any early stopping methods.
In-stead, we train the neural network model for a fixednumber of epochs and monitor the performance ofthe model on the development set after each epoch.Once training is finished, we select the model withthe best QWK score on the development set.5 ExperimentsIn this section, we describe our experimental setupand present the results.
Moreover, an analysis of theresults and some discussion are provided in this sec-tion.5.1 SetupThe dataset that we have used in our experiments isthe same dataset used in the ASAP competition runby Kaggle (see Table 1 for some statistics).
We usequadratic weighted Kappa as the evaluation metric,following the ASAP competition.
Since the test setused in the competition is not publicly available, weuse 5-fold cross validation to evaluate our systems.In each fold, 60% of the data is used as our train-ing set, 20% as the development set, and 20% as thetest set.
We train the model for a fixed number ofepochs and then choose the best model based on thedevelopment set.
We tokenize the essays using theNLTK5 tokenizer, lowercase the text, and normalizethe gold-standard scores to the range of [0, 1].
Dur-ing testing, we rescale the system-generated normal-ized scores to the original range of scores and mea-sure the performance.Prompt #Essays Avg length Scores1 1,783 350 2?122 1,800 350 1?63 1,726 150 0?34 1,772 150 0?35 1,805 150 0?46 1,800 150 0?47 1,569 250 0?308 723 650 0?60Table 1: Statistics of the ASAP dataset.In order to evaluate the performance of our sys-tem, we compare it to a publicly available open-source6 AES system called ?Enhanced AI Scor-5http://www.nltk.org6https://github.com/edx/ease1886ing Engine?
(EASE).
This system is the best open-source system that participated in the ASAP com-petition, and was ranked third among all 154 par-ticipating teams.
EASE is based on hand-craftedfeatures and regression methods.
The features thatare extracted by EASE can be categorized into fourclasses:?
Length-based features?
Parts-of-Speech (POS)?
Word overlap with the prompt?
Bag of n-gramsAfter extracting the features, a regression algorithmis used to build a model based on the training data.The details of the features and the results of usingsupport vector regression (SVR) and Bayesian linearridge regression (BLRR) are reported in (Phandi etal., 2015).
We use these two regression methods asour baseline systems.Our system has several hyper-parameters thatneed to be set.
We use the RMSProp optimizer withdecay rate (?)
set to 0.9 to train the network and weset the base learning rate to 0.001.
The mini-batchsize is 32 in our experiments7 and we train the net-work for 50 epochs.
The vocabulary is the 4,000most frequent words in the training data and all otherwords are mapped to a special token that representsunknown words.
We regularize the network by us-ing dropout (Srivastava et al, 2014) and we set thedropout probability to 0.5.
During training, the normof the gradient is clipped to a maximum value of10.
We set the word embedding dimension (dLT ) to50 and the output dimension of the recurrent layer(dr) to 300.
If a convolution layer is used, the win-dow size (l) is set to 3 and the output dimension ofthis layer (dc) is set to 50.
Finally, we initialize thelookup table layer using pre-trained word embed-dings8 released by Zou et al (2013).
Moreover, thebias value of the linear layer is initialized such thatthe network?s output before training is almost equalto the average score in the training data.7To create mini-batches for training, we pad all essays in amini-batch using a dummy token to make them have the samelength.
To eliminate the effect of padding tokens during train-ing, we mask them to prevent the network from miscalculatingthe gradients.8http://ai.stanford.edu/?wzou/mtWe have performed several experiments to iden-tify the best model architecture for our task.
Thesearchitectural choices are summarized below:?
Convolutional vs. recurrent neural network?
RNN unit type (basic RNN, GRU, or LSTM)?
Using mean-over-time over all recurrent statesvs.
using only the last recurrent state?
Using mean-over-time vs. an attention mecha-nism?
Using a recurrent layer vs. a convolutional re-current layer?
Unidirectional vs. bidirectional LSTMWe have used 8 Tesla K80 GPUs to perform our ex-periments in parallel.5.2 Results and DiscussionIn this section, we present the results of our eval-uation by comparing our system to the above-mentioned baselines (SVR and BLRR).
Table 2(rows 1 to 4) shows the QWK scores of our sys-tems on the eight prompts from the ASAP dataset9.This table also contains the results of our statisticalsignificance tests.
The baseline score that we haveused for hypothesis testing is underlined and the sta-tistically significant improvements (p < 0.05) overthe baseline are marked with ?*?.
It should be notedthat all neural network models in Table 2 are unidi-rectional and include the mean-over-time layer.
Ex-cept for the CNN model, convolution layer is notincluded in the networks.According to Table 2, all model variations are ableto learn the task properly and perform competitivelycompared to the baselines.
However, LSTM per-forms significantly better than all other systems andoutperforms the baseline by a large margin (4.1%).However, basic RNN falls behind other models anddoes not perform as accurately as GRU or LSTM.9To aggregate the QWK scores of all prompts, Fisher trans-formation was used in the ASAP competition before averagingQWK scores.
However, we found that applying Fisher trans-formation only slightly changes the scores.
(If we apply thismethod to aggregate QWK scores, our best ensemble system(row 7, Table 2) would obtain a QWK score of 0.768.)
There-fore we simply take the average of QWK scores across prompts.1887ID Systems Prompts1 2 3 4 5 6 7 8 Avg QWK1 CNN 0.797 0.634 0.646 0.767 0.746 0.757 0.746 0.687 0.7222 RNN 0.687 0.633 0.552 0.744 0.732 0.757 0.743 0.553 0.6753 GRU 0.616 0.591 0.668 0.787 0.795 0.800 0.752 0.573 0.6984 LSTM 0.775 0.687 0.683 0.795 0.818 0.813 0.805 0.594 0.746*5 CNN (10 runs) 0.804 0.656 0.637 0.762 0.752 0.765 0.750 0.680 0.726*6 LSTM (10 runs) 0.808 0.697 0.689 0.805 0.818 0.827 0.811 0.598 0.756*7 (5) + (6) 0.821 0.688 0.694 0.805 0.807 0.819 0.808 0.644 0.761*8 EASE (SVR) 0.781 0.621 0.630 0.749 0.782 0.771 0.727 0.534 0.6999 EASE (BLRR) 0.761 0.606 0.621 0.742 0.784 0.775 0.730 0.617 0.705Table 2: The QWK scores of the various neural network models and the baselines.
The baseline for the statistical significance testsis underlined and statistically significant improvements (p < 0.05) are marked with ?
*?.This behaviour is probably because of the relativelylong sequences of words in essays.
GRU and LSTMhave been shown to ?remember?
sequences and long-term dependencies much more effectively and there-fore, we believe this is the reason behind RNN?s rel-atively poor performance.Additionally, we perform some experiments toevaluate ensembles of our systems.
We create vari-ants of our network by training with different ran-dom initializations of the parameters.
To combinethese models, we simply take the average of thescores predicted by these networks.
This approachis shown to improve performance by reducing thevariance of the model and therefore make the predic-tions more accurate.
Table 2 (rows 5 and 6) showsthe results of CNN and LSTM ensembles over 10runs.
Moreover, we combine CNN ensembles andLSTM ensembles together to make the predictions(row 7).As shown in Table 2, ensembles of models alwayslead to improvements.
We obtain 0.4% and 1.0%improvement from CNN and LSTM ensembles, re-spectively.
However, our best model (row 7 in Table2) is the ensemble of 10 instances of CNN modelsand 10 instances of LSTM models and outperformsthe baseline BLRR system by 5.6%.It is possible to use the last state of the recurrentlayer to predict the score instead of taking the meanover all intermediate states.
In order to observe theeffects of this architectural choice, we test the net-work with and without the mean-over-time layer.The results of this experiment are presented in Ta-ble 3, clearly showing that the neural network failsto learn the task properly in the absence of the mean-over-time layer.
When the mean-over-time layer isnot used in the model, the network needs to effi-ciently encode the whole essay into a single statevector and then use it to predict the score.
How-ever, when the mean-over-time layer is included, themodel has direct access to all intermediate statesand can recall the required intermediate informationmuch more effectively and therefore is able to pre-dict the score more accurately.Systems Avg QWKLSTM 0.746*LSTM w/o MoT 0.540LSTM+attention 0.731*CNN+LSTM 0.708BLSTM 0.699EASE (SVR) 0.699EASE (BLRR) 0.705Table 3: The QWK scores of LSTM neural network vari-ants.
The baseline for the statistical significance tests is un-derlined and statistically significant improvements (p < 0.05)are marked with ?
*?.Additionally, we experiment with three other neu-ral network architectures.
Instead of using mean-over-time to average intermediate states, we usean attention mechanism (Bahdanau et al, 2015) tocompute a weighted sum of the states.
In this case,we calculate the dot product of the intermediatestates and a vector trained by the neural network,and then apply a softmax operation to obtain thenormalized weights.
Another alternative is to add aconvolution layer before feeding the embeddings tothe recurrent LSTM layer (CNN+LSTM) and eval-uate the model.
We also use a bidirectional LSTMmodel (BLSTM), in which the sequence of words1888is processed in both directions and the intermediatestates generated by both LSTM layers are mergedand then fed into the mean-over-time layer.
The re-sults of testing these architectures are summarizedin Table 3.The attention mechanism significantly improvesthe results compared to LSTM without mean-over-time, but it does not perform as well as LSTM withmean-over-time.
The other two architectural choicesdo not lead to further improvements over the LSTMneural network.
This observation is in line with thefindings of some other researchers (Kadlec et al,2015) and is probably because of the relatively smallnumber of training examples compared to the capac-ity of the models.We have also compared the accuracy of our bestsystem (shown as ?AES?)
with human performance,presented in Table 4.
To do so, we calculate theagreement (QWK scores) between our system andeach of the two human annotators separately (?AES- H1?
and ?AES - H2?
), as well as the agreement be-tween the two human annotators (?H1 - H2?).
Ac-cording to Table 4, the performance of our system onaverage is very close to human annotators.
In fact,for some of the prompts, the agreement between oursystem and the human annotators is even higher thanthe agreement between human annotators.
In gen-eral, we can conclude that our method is just belowthe upper limit and approaching human-level perfor-mance.We also compare our system to a recently pub-lished automated essay scoring method based onneural networks (Alikaniotis et al, 2016).
Instead ofperforming cross validation, Alikaniotis et al (2016)partition the ASAP dataset into two parts by using80% of the data for training and the remaining 20%for testing.
For comparison, we also carry out anexperiment on the same training and test data usedin (Alikaniotis et al, 2016).
Following how QWKscores are computed in Alikaniotis et al (2016), in-stead of calculating QWK for each prompt sepa-rately and averaging them, we calculate the QWKscore for the whole test set, by setting the minimumscore to 0 and the maximum score to 60.
Usingthis evaluation setup, our LSTM system achieves aQWK score of 0.987, higher than the QWK scoreof 0.96 of the best system in (Alikaniotis et al,2016).
In this way of calculating QWK scores, sinceFigure 2: Score variations per timestamp.
All scores are nor-malized to the range of [0, 1].the majority of the test essays have a much smallerscore range (see Table 1) compared to [0, 60], thedifferences between the system-predicted scores andthe gold-standard scores will be small most of thetime.
For example, more than 55% of the essaysin the test set have a score range of [0, 3] or [0, 4]and therefore, for these prompts, the differences be-tween human-assigned gold-standard scores and thescores predicted by an AES system will be small inthe range of [0, 60].
For this reason, in contrast toprompt-specific QWK calculation, the QWK scoresare much higher in this evaluation setting and farexceed the QWK score for human agreement whencomputed in a prompt-specific way (see Table 4).Interpreting neural network models and the inter-actions between nodes is not an easy task.
However,it is possible to gain an insight of a network by an-alyzing the behavior of particular nodes.
In orderto understand how our neural network assigns thescores, we monitor the score variations while test-ing the model.
Figure 2 displays the score variationsfor three essays after processing each word (at eachtimestamp) by the neural network.
We have selecteda poorly written essay, a well written essay, and anaverage essay with normalized gold-standard scoresof 0.2, 0.8, and 0.6, respectively.According to Figure 2, the network learns to takeessay length into account and assigns a very lowscore to all short essays with fewer than 50 words,regardless of the content.
This pattern recurs forall essays and is not specific to the three selectedessays in Figure 2.
However, if an essay is longenough, the content becomes more important andthe AES system starts discriminating well written1889Description Prompts1 2 3 4 5 6 7 8 Avg QWKAES - H1 0.750 0.684 0.662 0.759 0.751 0.791 0.731 0.607 0.717AES - H2 0.767 0.690 0.632 0.762 0.769 0.775 0.752 0.530 0.710H1 - H2 0.721 0.812 0.769 0.851 0.753 0.776 0.720 0.627 0.754Table 4: Comparison with human performance.
H1 and H2 denote human rater 1 and human rater 2, respectively, and AES refersto our best system (ensemble of CNN and LSTM models).essays from poorly written ones.
As shown in Fig-ure 2, the model properly assigns a higher score tothe well written essay 2, while giving lower scoresto the other essays.
This observation confirms thatthe model successfully learns the required featuresfor automated essay scoring.
While it is difficult toassociate different parts of the neural network modelwith specific features, it is clear that appropriate in-dicators of essay quality are being learnt, includingessay length and essay content.6 ConclusionIn this paper, we have proposed an approach basedon recurrent neural networks to tackle the task of au-tomated essay scoring.
Our method does not rely onany feature engineering and automatically learns therepresentations required for the task.
We have ex-plored a variety of neural network model architec-tures for automated essay scoring and have achievedsignificant improvements over a strong open-sourcebaseline.
Our best system outperforms the baselineby 5.6% in terms of quadratic weighted Kappa.
Fur-thermore, an analysis of the network has been per-formed to get an insight of the recurrent neural net-work model and we show that the method effectivelyutilizes essay content to extract the required infor-mation for scoring essays.AcknowledgmentsThis research is supported by Singapore Ministryof Education Academic Research Fund Tier 2 grantMOE2013-T2-1-150.
We are also grateful to theanonymous reviewers for their helpful comments.ReferencesDimitrios Alikaniotis, Helen Yannakoudakis, and MarekRei.
2016.
Automatic text scoring using neural net-works.
In Proceedings of the 54th Annual Meeting ofthe Association for Computational Linguistics.Yigal Attali and Jill Burstein.
2004.
Automated essayscoring with e-rater R?
v. 2.0.
Technical report, Educa-tional Testing Service.Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-gio.
2015.
Neural machine translation by jointlylearning to align and translate.
In Proceedings of the3rd International Conference on Learning Represen-tations.Hongbo Chen and Ben He.
2013.
Automated essayscoring by maximizing human-machine agreement.In Proceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing.Kyunghyun Cho, Bart van Merrienboer, Caglar Gulcehre,Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk,and Yoshua Bengio.
2014.
Learning phrase represen-tations using RNN encoder?decoder for statistical ma-chine translation.
In Proceedings of the 2014 Confer-ence on Empirical Methods in Natural Language Pro-cessing.Ronan Collobert, Jason Weston, Le?on Bottou, MichaelKarlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.Journal of Machine Learning Research, 12:2493?2537.Yann N. Dauphin, Harm de Vries, and Yoshua Bengio.2015.
Equilibrated adaptive learning rates for non-convex optimization.
In Advances in Neural Informa-tion Processing Systems 28.Jeffrey L. Elman.
1990.
Finding structure in time.
Cog-nitive Science, 14(2):179?211.Peter W Foltz, Darrell Laham, and Thomas K Landauer.1999.
The Intelligent Essay Assessor: Applications toeducational technology.
Interactive Multimedia Elec-tronic Journal of Computer-Enhanced Learning.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural Computation, 9(8):1735?1780.Ozan Irsoy and Claire Cardie.
2014.
Opinion miningwith deep recurrent neural networks.
In Proceedingsof the 2014 Conference on Empirical Methods in Nat-ural Language Processing.Rudolf Kadlec, Martin Schmid, and Jan Kleindienst.2015.
Improved deep learning baselines for Ubuntucorpus dialogs.
In Proceesings of the NIPS 20151890Workshop on Machine Learning for Spoken LanguageUnderstanding and Interaction.Yoon Kim, Yacine Jernite, David Sontag, and Alexan-der M Rush.
2016.
Character-aware neural languagemodels.
In Proceedings of the Thirtieth AAAI Confer-ence on Artificial Intelligence.Xuezhe Ma and Eduard Hovy.
2016.
End-to-end se-quence labeling via bi-directional LSTM-CNNs-CRF.In Proceedings of the 54th Annual Meeting of the As-sociation for Computational Linguistics.Razvan Pascanu, Tomas Mikolov, and Yoshua Bengio.2013.
On the difficulty of training recurrent neural net-works.
In Proceedings of the 30th International Con-ference on Machine Learning.Isaac Persing and Vincent Ng.
2015.
Modeling argumentstrength in student essays.
In Proceedings of the 53rdAnnual Meeting of the Association for ComputationalLinguistics and the 7th International Joint Conferenceon Natural Language Processing.Isaac Persing, Alan Davis, and Vincent Ng.
2010.
Mod-eling organization in student essays.
In Proceedings ofthe 2010 Conference on Empirical Methods in NaturalLanguage Processing.Peter Phandi, Kian Ming A. Chai, and Hwee Tou Ng.2015.
Flexible domain adaptation for automated essayscoring using correlated linear regression.
In Proceed-ings of the 2015 Conference on Empirical Methods inNatural Language Processing.Mark D. Shermis and Jill Burstein, editors.
2013.
Hand-book of Automated Essay Evaluation: Current Appli-cations and New Directions.
Routledge.Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky,Ilya Sutskever, and Ruslan Salakhutdinov.
2014.Dropout: A simple way to prevent neural networksfrom overfitting.
Journal of Machine Learning Re-search, 15(1):1929?1958.Martin Sundermeyer, Hermann Ney, and Ralf Schlu?ter.2015.
From feedforward to recurrent LSTM neuralnetworks for language modeling.
IEEE/ACM Trans-actions on Audio, Speech, and Language Processing,23(3):517?529.Oriol Vinyals, Lukasz Kaiser, Terry Koo, Slav Petrov,Ilya Sutskever, and Geoffrey Hinton.
2015.
Grammaras a foreign language.
In Advances in Neural Informa-tion Processing Systems 28.Helen Yannakoudakis and Ronan Cummins.
2015.
Eval-uating the performance of automated text scoring sys-tems.
In Proceedings of the Tenth Workshop on Inno-vative Use of NLP for Building Educational Applica-tions.Torsten Zesch, Michael Wojatzki, and Dirk Scholten-Akoun.
2015.
Task-independent features for auto-mated essay grading.
In Proceedings of the TenthWorkshop on Innovative Use of NLP for Building Ed-ucational Applications.Will Y. Zou, Richard Socher, Daniel Cer, and Christo-pher D. Manning.
2013.
Bilingual word embeddingsfor phrase-based machine translation.
In Proceedingsof the 2013 Conference on Empirical Methods in Nat-ural Language Processing.1891
