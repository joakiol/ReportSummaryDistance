Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 229?237,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsLanguage Identification: The Long and the Short of the MatterTimothy Baldwin and Marco LuiDept of Computer Science and Software EngineeringUniversity of Melbourne, VIC 3010 Australiatb@ldwin.net, saffsd@gmail.comAbstractLanguage identification is the task of identify-ing the language a given document is writtenin.
This paper describes a detailed examina-tion of what models perform best under dif-ferent conditions, based on experiments acrossthree separate datasets and a range of tokeni-sation strategies.
We demonstrate that the taskbecomes increasingly difficult as we increasethe number of languages, reduce the amountof training data and reduce the length of docu-ments.
We also show that it is possible to per-form language identification without having toperform explicit character encoding detection.1 IntroductionWith the growth of the worldwide web, ever-increasing numbers of documents have becomeavailable, in more and more languages.
This growthhas been a double-edged sword, however, in thatcontent in a given language has become more preva-lent but increasingly hard to find, due to the web?ssheer size and diversity of content.
While the ma-jority of (X)HTML documents declare their charac-ter encoding, only a tiny minority specify what lan-guage they are written in, despite support for lan-guage declaration existing in the various (X)HTMLstandards.1 Additionally, a single encoding can gen-erally be used to render a large number of languagessuch that the document encoding at best filters outa subset of languages which are incompatible withthe given encoding, rather than disambiguates thesource language.
Given this, the need for automaticmeans to determine the source language of web doc-1http://dev.opera.com/articles/view/mama-head-structure/uments is crucial for web aggregators of varioustypes.There is widespread misconception of languageidentification being a ?solved task?, generally asa result of isolated experiments over homogeneousdatasets with small numbers of languages (Hugheset al, 2006; Xia et al, 2009).
Part of the motivationfor this paper is to draw attention to the fact that, asa field, we are still a long way off perfect languageidentification of web documents, as evaluated underrealistic conditions.In this paper we describe experiments on lan-guage identification of web documents, focusing onthe broad question of what combination of tokenisa-tion strategy and classification model achieves thebest overall performance.
We additionally evalu-ate the impact of the volume of training data andthe test document length on the accuracy of lan-guage identification, and investigate the interactionbetween character encoding detection and languageidentification.One assumption we make in this research, follow-ing standard assumptions made in the field, is that alldocuments are monolingual.
This is clearly an un-realistic assumption when dealing with general webdocuments (Hughes et al, 2006), and we plan to re-turn to investigate language identification over mul-tilingual documents in future work.Our contributions in this paper are: the demon-stration that language identification is: (a) trivialover datasets with smaller numbers of languagesand approximately even amounts of training data perlanguage, but (b) considerably harder over datasetswith larger numbers of languages with more skewin the amount of training data per language; byte-based tokenisation without character encoding de-tection is superior to codepoint-based tokenisation229with character encoding detection; and simple co-sine similarity-based nearest neighbour classifica-tion is equal to or better than models including sup-port vector machines and naive Bayes over the lan-guage identification task.
We also develop datasetsto facilitate standardised evaluation of languageidentification.2 Background ResearchLanguage identification was arguably established asa task by Gold (1967), who construed it as a closedclass problem: given data in each of a predefined setof possible languages, human subjects were askedto classify the language of a given test document.
Itwasn?t until the 1990s, however, that the task waspopularised as a text categorisation task.The text categorisation approach to languageidentification applies a standard supervised classi-fication framework to the task.
Perhaps the best-known such model is that of Cavnar and Tren-kle (1994), as popularised in the textcat tool.2The method uses a per-language character frequencymodel, and classifies documents via their relative?out of place?
distance from each language (seeSection 5.1).
Variants on this basic method in-clude Bayesian models for character sequence pre-diction (Dunning, 1994), dot products of word fre-quency vectors (Darnashek, 1995) and information-theoretic measures of document similarity (Aslamand Frost, 2003; Martins and Silva, 2005).
Morerecently, support vector machines (SVMs) and ker-nel methods have been applied to the task of lan-guage identification task with success (Teytaud andJalam, 2001; Lodhi et al, 2002; Kruengkrai et al,2005), and Markov logic has been used for joint in-ferencing in contexts where there are multiple evi-dence sources (Xia et al, 2009).Language identification has also been carried outvia linguistically motivated models.
Johnson (1993)used a list of stop words from different languages toidentify the language of a given document, choos-ing the language with the highest stop word over-lap with the document.
Grefenstette (1995) usedword and part of speech (POS) correlation to de-termine if two text samples were from the sameor different languages.
Giguet (1995) developed a2http://www.let.rug.nl/vannoord/TextCat/cross-language tokenisation model and used it toidentify the language of a given document basedon its tokenisation similarity with training data.Dueire Lins and Gonc?alves (2004) considered theuse of syntactically-derived closed grammatical-class models, matching syntactic structure ratherthan words or character sequences.The observant reader will have noticed that someof the above approaches make use of notions suchas ?word?, typically based on the naive assumptionthat the language uses white space to delimit words.These approaches are appropriate in contexts wherethere is a guarantee of a document being in one ofa select set of languages where words are space-delimited, or where manual segmentation has beenperformed (e.g.
interlinear glossed text).
However,we are interested in language identification of webdocuments, which can be in any language, includ-ing languages that do not overtly mark word bound-aries, such as Japanese, Chinese and Thai; whilerelatively few languages fall into this categories,they are among the most populous web languagesand therefore an important consideration.
There-fore, approaches that assume a language is space-delimited are clearly not suitable for our purposes.Equally, approaches which make assumptions aboutthe availability of particular resources for each lan-guage to be identified (e.g.
POS taggers, or the ex-istence of precompiled stop word lists) cannot beused.Language identification has been applied in anumber of contexts, the most immediate applica-tion being in multilingual text retrieval, where re-trieval results are generally superior if the languageof the query is known, and the search is restrictedto only those documents predicted to be in that lan-guage (McNamee and Mayfield, 2004).
It can alsobe used to ?word spot?
foreign language terms inmultilingual documents, e.g.
to improve parsing per-formance (Alex et al, 2007), or for linguistic corpuscreation purposes (Baldwin et al, 2006; Xia et al,2009; Xia and Lewis, 2009).3 DatasetsIn the experiments reported in this paper, we em-ploy three novel datasets, with differing propertiesrelevant to language identification research:230Corpus Documents Languages Encodings Document Length (bytes)EUROGOV 1500 10 1 17460.5?39353.4TCL 3174 60 12 2623.2?3751.9WIKIPEDIA 4963 67 1 1480.8?4063.9Table 1: Summary of the three language identification datasetsFigure 1: Distribution of languages in the three datasets(vector of languages vs. the proportion of documents inthat language)EUROGOV: longer documents, all in a single en-coding, spread evenly across a relatively small num-ber (10) of Western European languages; this datasetis comparable to the datasets conventionally used inlanguage identification research.
As the name wouldsuggest, the documents were sourced from the Euro-GOV document collection, as used in the 2005 Web-CLEF task.TCL: a larger number of languages (60) across awider range of language families, with shorter docu-ments and a range of character encodings (12).
Thecollection was manually sourced by the Thai Com-putational Linguistics Laboratory (TCL) in 2005from online news sources.WIKIPEDIA: a slightly larger number of lan-guages again (67), a single encoding, and shorterdocuments; the distribution of languages is intendedto approximate that of the actual web.
This col-lection was automatically constructed by taking thedumps of all versions of Wikipedia with 1000 ormore documents in non-constructed languages, andrandomly selecting documents from them in a bias-preserving manner (i.e.
preserving the documentdistribution in the full collection); this is intended torepresent the document language bias observed onthe web.
All three corpora are available on request.We outline the characteristics of the three datasetsin Table 1.
We further detail the language distri-bution in Figure 1, using a constant vector of lan-guages for all three datasets, based on the order oflanguages in the WIKIPEDIA dataset (in descendingorder of documents per language).
Of note are thecontrasting language distributions between the threedatasets, in terms of both the languages representedand the relative skew of documents per language.
Inthe following sections, we provide details of the cor-pus compilation and document sampling method foreach dataset.4 Document RepresentationAs we are interested in performing language iden-tification over arbitrary web documents, we re-quire a language-neutral document representationwhich does not make artificial assumptions about thesource language of the document.
Separately, thereis the question of whether it is necessary to deter-mine the character encoding of the document in or-der to extract out character sequences, or whetherthe raw byte stream is sufficient.
To explore thisquestion, we experiment with two document repre-sentations: (1) byte n-grams, and (2) codepoint n-grams.
In both cases, a document is represented as afeature vector of token counts.Byte n-grams can be extracted directly withoutexplicit encoding detection.
Codepoint n-grams, onthe other hand, require that we know the characterencoding of the document in order to perform to-kenisation.
Additionally, they should be based on acommon encoding to prevent: (a) over-fragmentingthe feature space (e.g.
ending up with discrete fea-ture spaces for euc-jp, s-jis and utf-8 inthe case of Japanese); and (b) spurious matches be-tween encodings (e.g.
Japanese hiragana and Ko-rean hangul mapping onto the same codepoint ineuc-jp and euc-kr, respectively).
We use uni-231code as the common encoding for all documents.In practice, character encoding detection is an is-sue only for TCL, as the other two datasets are ina single encoding.
Where a character encoding wasprovided for a document in TCL and it was possi-ble to transcode the document to unicode based onthat encoding, we used the encoding information.
Incases where a unique encoding was not provided,we used an encoding detection library based on theMozilla browser.3 Having disambiguated the encod-ing for each document, we transcoded it into uni-code.5 ModelsIn our experiments we use a number of differentlanguage identification models, as outlined below.We first describe the nearest-neighbour and nearest-prototype models, and a selection of distance andsimilarity metrics combined with each.
We thenpresent three standalone text categorisation models.5.1 Nearest-Neighbour and Nearest-PrototypeModelsThe 1-nearest-neighbour (1NN) model is a commonclassification technique, whereby a test documentD is classified based on the language of the clos-est training document Di (with language l(Di)), asdetermined by a given distance or similarity metric.In nearest-neighbour models, each training doc-ument is represented as a single instance, mean-ing that the computational cost of classifying a testdocument is proportional to the number of trainingdocuments.
A related model which aims to reducethis cost is nearest-prototype (AM), where each lan-guage is represented as a single instance, by mergingall of the training instances for that language into asingle centroid via the arithmetic mean.For both nearest-neighbour and nearest-prototypemethods, we experimented with three similarity anddistance measures in this research:Cosine similarity (COS): the cosine of the anglebetween two feature vectors, as measured by the dotproduct of the two vectors, normalised to unit length.Skew divergence (SKEW): a variant of Kullback-Leibler divergence, whereby the second distribution3http://chardet.feedparser.org/(y) is smoothed by linear interpolation with the first(x) using a smoothing factor ?
(Lee, 2001):s?
(x, y) = D(x || ?y + (1?
?
)x)where:D(x || y) =?ixi(log2 xi ?
log2 yi)In all our experiments, we set ?
to 0.99.Out-of-place (OOP): a ranklist-based distancemetric, where the distance between two documentsis calculated as (Cavnar and Trenkle, 1994):oop(Dx, Dy) =?t?Dx?Dyabs(RDx(t)?RDy(t))RD(t) is the rank of term t in document D, basedon the descending order of frequency in documentD; terms not occurring in document D are conven-tionally given the rank 1 + maxi RD(ti).5.2 Naive Bayes (NB)Naive Bayes is a popular text classification model,due to it being lightweight, robust and easy to up-date.
The language of test document D is predictedby:l?
(D) = arg maxli?LP (li)|V |?j=1P (tj |li)ND,tjND,tj !where L is the set of languages in the training data,ND,tj is the frequency of the jth term in D, V is theset of all terms, and:P (t|li) =1 +?|D |k=1 Nk,tP (li|Dk)|V |+ ?|V |j=1?|D |k=1 Nk,tjP (li|Dk)In this research, we use the rainbow imple-mentation of multinominal naive Bayes (McCallum,1996).5.3 Support Vector Machines (SVM)Support vector machines (SVMs) are one of themost popular methods for text classification, largelybecause they can automatically weight large num-bers of features, capturing feature interactions in theprocess (Joachims, 1998; Manning et al, 2008).
Thebasic principle underlying SVMs is to maximize the232margin between training instances and the calculateddecision boundary based on structural risk minimi-sation (Vapnik, 1995).In this work, we have made use of bsvm,4 animplementation of SVMs with multiclass classifica-tion support (Hsu et al, 2008).
We only report re-sults for multi-class bound-constrained support vec-tor machines with linear kernels, as they were foundto perform best over our data.6 Experimental MethodologyWe carry out experiments over the cross-product ofthe following options, as described above:model (?7): nearest-neighbour (COS1NN,SKEW1NN, OOP1NN), nearest-prototype(COSAM, SKEWAM),5 NB, SVMtokenisation (?2): byte, codepointn-gram (?3): 1-gram, 2-gram, 3-gramfor a total of 42 distinct classifiers.
Each classi-fier is run across the 3 datasets (EUROGOV, TCLand WIKIPEDIA) based on 10-fold stratified cross-validation.We evaluate the models using micro-averagedprecision (P?
), recall (R?)
and F-score (F?
), as wellas macro-averaged precision (PM ), recall (RM ) andF-score (FM ).
The micro-averaged scores indicatethe average performance per document; as we al-ways make a unique prediction per document, themicro-averaged precision, recall and F-score are al-ways identical (as is the classification accuracy).The macro-averaged scores, on the other hand, indi-cate the average performance per language.
In eachcase, we average the precision, recall and F-scoreacross the 10 folds of cross validation.6As a baseline, we use a majority class, or ZeroR,classifier (ZEROR), which assigns the language withhighest prior in the training data to each of the testdocuments.4http://www.csie.ntu.edu.tw/?cjlin/bsvm/5We do not include the results for nearest-prototype classi-fiers with the OOP distance metric as the results were consid-erably lower than the other methods.6Note that this means that the averaged FM is not necessar-ily the harmonic mean of the averaged PM andRM .Model Token PM RM FM P?/R?/F?ZEROR ?
.020 .084 .032 .100COS1NN byte .975 .978 .976 .975COS1NN codepoint .968 .973 .970 .971COSAM byte .922 .938 .926 .937COSAM codepoint .908 .930 .913 .931SKEW1NN byte .979 .979 .979 .977SKEW1NN codepoint .978 .978 .978 .976SKEWAM byte .974 .972 .972 .969SKEWAM codepoint .974 .972 .973 .970OOP1NN byte .953 .952 .953 .949OOP1NN codepoint .961 .960 .960 .957NB byte .975 .973 .974 .971NB codepoint .975 .973 .974 .971SVM byte .989 .985 .987 .987SVM codepoint .988 .985 .986 .987Table 2: Results for byte vs. codepoint (bigram) tokeni-sation over EUROGOV7 ResultsIn our experiments, we first compare the differentmodels for fixed n-gram order, then come back tovary the n-gram order.
Subsequently, we examinethe relative performance of the different models ontest documents of differing lengths, and finally lookinto the impact of the amount of training data fora given language on the performance for that lan-guage.7.1 Results for the Different Models andTokenisation StrategiesFirst, we present the results for each of the classifiersin Tables 2?4, based on byte or codepoint tokenisa-tion and bigrams.
In each case, we present the bestresult in each column in bold.The relative performance over EUROGOV andTCL is roughly comparable for all methods barringSKEW1NN, with near-perfect scores over all 6 eval-uation metrics.
SKEW1NN is near-perfect over EU-ROGOV and TCL, but drops to baseline levels overWIKIPEDIA; we return to discuss this effect in Sec-tion 7.2.In the case of EUROGOV, the near-perfect re-sults are in line with our expectations for the dataset,based on its characteristics and results reported forcomparable datasets.
The results for WIKIPEDIA,however, fall off considerably, with the best modelachieving an FM of .671 and F?
of .869, due to233Model Token PM RM FM P?/R?/F?ZEROR ?
.003 .017 .005 .173COS1NN byte .981 .975 .975 .982COS1NN codepoint .931 .930 .925 .961COSAM byte .967 .975 .965 .965COSAM codepoint .979 .977 .974 .964SKEW1NN byte .984 .974 .976 .987SKEW1NN codepoint .910 .210 .320 .337SKEWAM byte .962 .959 .950 .972SKEWAM codepoint .968 .961 .957 .967OOP1NN byte .964 .945 .951 .974OOP1NN codepoint .901 .892 .893 .933NB byte .905 .905 .896 .969NB codepoint .722 .711 .696 .845SVM byte .981 .973 .977 .984SVM codepoint .979 .970 .974 .980Table 3: Results for byte vs. codepoint (bigram) tokeni-sation over TCLthe larger number of languages, smaller documents,and skew in the amounts of training data per lan-guage.
All models are roughly balanced in the rel-ative scores they attain for PM , RM and FM (i.e.there are no models that have notably higherPM rel-ative to RM , for example).The nearest-neighbour models outperform thecorresponding nearest-prototype models to varyingdegrees, with the one exception of SKEW1NN overWIKIPEDIA.
The nearest-prototype classifiers werecertainly faster than the nearest-neighbour classi-fiers, by roughly an order of 10, but this is morethan outweighed by the drop in classification per-formance.
With the exception of SKEW1NN overWIKIPEDIA, all methods were well above the base-lines for all three datasets.The two methods which perform consistently wellat this point are COS1NN and SVM, with COS1NNholding up particularly well under micro-averagedF-score while NB drops away over WIKIPEDIA, themost skewed dataset; this is due to the biasing effectof the prior in NB.Looking to the impact of byte- vs. codepoint-tokenisation on classifier performance over the threedatasets, we find that overall, bytes outperformcodepoints.
This is most notable for TCL andWIKIPEDIA, and the SKEW1NN and NB models.Given this result, we present only results for byte-based tokenisation in the remainder of this paper.Model Token PM RM FM P?/R?/F?ZEROR ?
.004 .013 .007 .328COS1NN byte .740 .646 .671 .869COS1NN codepoint .685 .604 .625 .835COSAM byte .587 .634 .573 .776COSAM codepoint .486 .556 .483 .725SKEW1NN byte .005 .013 .008 .304SKEW1NN codepoint .006 .013 .007 .241SKEWAM byte .605 .617 .588 .844SKEWAM codepoint .552 .575 .532 .807OOP1NN byte .619 .518 .548 .831OOP1NN codepoint .598 .486 .520 .807NB byte .496 .454 .442 .851NB codepoint .426 .349 .360 .798SVM byte .667 .545 .577 .845SVM codepoint .634 .494 .536 .818Table 4: Results for byte vs. codepoint (bigram) tokeni-sation over WIKIPEDIAThe results for byte tokenisation of TCL are par-ticularly noteworthy.
The transcoding into unicodeand use of codepoints, if anything, hurts perfor-mance, suggesting that implicit character encodingdetection based on byte tokenisation is the best ap-proach: it is both more accurate and simplifies thesystem, in removing the need to perform encodingdetection prior to language identification.7.2 Results for Differing n-gram SizesWe present results with byte unigrams, bigrams andtrigrams in Table 5 for WIKIPEDIA.7 We omit re-sults for the other two datasets, as the overall trend isthe same as for WIKIPEDIA, with lessened relativedifferences between n-gram orders due to the rela-tive simplicity of the respective classification tasks.SKEW1NN is markedly different to the other meth-ods in achieving the best performance with uni-grams, moving from the worst-performing methodby far to one of the best-performing methods.
Thisis the result of the interaction between data sparse-ness and heavy-handed smoothing with the ?
con-stant.
Rather than using a constant ?
value for alln-gram orders, it may be better to parameterise itusing an exponential scale such as ?
= 1?
?n (with7The results for OOP1NN over byte trigrams are missingdue to the computational cost associated with the method, andour experiment hence not having run to completion at the timeof writing.
Extrapolating from the results for the other twodatasets, we predict similar results to bigrams.234Model n-gram PM RM FM P?/R?/F?ZEROR ?
.004 .013 .007 .328COS1NN 1 .644 .579 .599 .816COS1NN 2 .740 .646 .671 .869COS1NN 3 .744 .656 .680 .862COSAM 1 .526 .543 .487 .654COSAM 2 .587 .634 .573 .776COSAM 3 .553 .632 .545 .761SKEW1NN 1 .691 .598 .625 .848SKEW1NN 2 .005 .013 .008 .304SKEW1NN 3 .005 .013 .004 .100SKEWAM 1 .552 .569 .532 .740SKEWAM 2 .605 .617 .588 .844SKEWAM 3 .551 .631 .554 .825OOP1NN 1 .519 .446 .468 .747OOP1NN 2 .619 .518 .548 .831NB 1 .576 .578 .555 .778NB 2 .496 .454 .442 .851NB 3 .493 .435 .432 .863SVM 1 .585 .505 .523 .812SVM 2 .667 .545 .577 .845SVM 3 .717 .547 .594 .840Table 5: Results for different n-gram orders overWIKIPEDIA?
= 0.01, e.g.
), based on the n-gram order.
Weleave this for future research.For most methods, bigrams and trigrams are bet-ter than unigrams, with the one notable exceptionof SKEW1NN.
In general, there is little separatingbigrams and trigrams, although the best result for isachieved slightly more often for bigrams than for tri-grams.For direct comparability with Cavnar and Tren-kle (1994), we additionally carried out a preliminaryexperiment with hybrid byte n-grams (all of 1- to 5-grams), combined with simple frequency-based fea-ture selection of the top-1000 features for each n-gram order.
The significance of this setting is that itis the strategy adopted by textcat, based on theoriginal paper of Cavnar and Trenkle (1994) (withthe one exception that we use 1000 features ratherthan 300, as all methods other than OOP1NN bene-fitted from more features).
The results are shown inTable 6.Compared to the results in Table 5, SKEW1NN andSKEWAM both increase markedly to achieve the bestoverall results.
OOP1NN, on the other hand, risesslightly, while the remaining three methods actuallyModel PM RM FM P?/R?/F?ZEROR .004 .013 .007 .328COS1NN .735 .664 .682 .865COSAM .592 .626 .580 .766SKEW1NN .789 .708 .729 .902SKEWAM .681 .718 .680 .870OOP1NN .697 .595 .626 .864SVM .669 .500 .544 .832Table 6: Results for mixed n-grams (1?5) and feature se-lection over WIKIPEDIA (a la?
Cavnar and Trenkle (1994))drop back slightly.
Clearly, there is considerablymore experimentation to be done here with mixedn-gram models and different feature selection meth-ods, but the results indicate that some methods cer-tainly benefit from n-gram hybridisation and featureselection, and also that we have been able to sur-pass the results of Cavnar and Trenkle (1994) withSKEW1NN in an otherwise identical framework.7.3 Breakdown Across Test Document LengthTo better understand the impact of test documentsize on classification accuracy, we divided the testdocuments into 5 equal-size bins according to theirlength, measured by the number of tokens.
We thencomputed F?
individually for each bin across the 10folds of cross validation.
We present the breakdownof results for WIKIPEDIA in Figure 2.WIKIPEDIA shows a pseudo-logarithmic growthin F?
(= P?
= R?)
as the test document size in-creases.
This fits with our intuition, as the modelhas progressively more evidence to base the classi-fication on.
It also suggests that performance overshorter documents appears to be the dominating fac-tor in the overall ranking of the different methods.In particular, COS1NN and SVM appear to be able toclassify shorter documents most reliably, leading tothe overall result of them being the best-performingmethods.While we do not show the graph for reasons ofspace, the equivalent graph for EUROGOV displaysa curious effect: F?
drops off as the test documentsget longer.
Error analysis of the data indicates thatthis is due to longer documents being more likelyto be ?contaminated?
with either data from a sec-ond language or extra-linguistic data, such as largetables of numbers or chemical names.
This sug-gests that all the models are brittle when the assump-235Figure 2: Breakdown of F?
over WIKIPEDIA for testdocuments of increasing lengthFigure 3: Per-language FM for COS1NN, relative to thetraining data size (in MB) for that languagetion of strict monolingualism is broken, or whenthe document is dominated by extra-linguistic data.Clearly, this underlines our assumption of monolin-gual documents, and suggests multilingual languageidentification is a fertile research area even in termsof optimising performance over our ?monolingual?datasets.7.4 Performance Relative to Training Data SizeAs a final data point in our analysis, we calculatedthe FM for each language relative to the amount oftraining data available for that language, and presentthe results in the form of a combined scatter plot forthe three datasets in Figure 3.
The differing distri-butions of the three datasets are self-evident, withmost languages in EUROGOV (the squares) bothhaving reasonably large amounts of training data andachieving high FM values, but the majority of lan-guages in WIKIPEDIA (the crosses) having very lit-tle data (including a number of languages with notraining data, as there is a singleton document in thatlanguage in the dataset).
As an overall trend, we canobserve that the greater the volume of training data,the higher the FM across all three datasets, but thereis considerable variation between the languages interms of their FM for a given training data size (thecolumn of crosses for WIKIPEDIA to the left of thegraph is particularly striking).8 ConclusionsWe have carried out a thorough (re)examination ofthe task of language identification, that is predict-ing the language that a given document is writtenin, focusing on monolingual documents at present.We experimented with a total of 7 models, andtested each over two tokenisation strategies (bigramsvs.
codepoints) and three token n-gram orders (un-igrams, bigrams and trigrams).
At the same timeas reproducing results from earlier research on howeasy the task can be over small numbers of lan-guages with longer documents, we demonstratedthat the task becomes much harder for larger num-bers of languages, shorter documents and greaterclass skew.
We also found that explicit characterencoding detection is not necessary in language de-tection, and that the most consistent model overallis either a simple 1-NN model with cosine similar-ity, or an SVM with a linear kernel, using a bytebigram or trigram document representation.
We alsoconfirmed that longer documents tend to be easier toclassify, but also that multilingual documents causeproblems for the standard model of language identi-fication.AcknowledgementsThis research was supported by a Google ResearchAward.ReferencesBeatrice Alex, Amit Dubey, and Frank Keller.
2007.Using foreign inclusion detection to improve parsingperformance.
In Proceedings of the Joint Conference236on Empirical Methods in Natural Language Process-ing and Computational Natural Language Learning2007 (EMNLP-CoNLL 2007), pages 151?160, Prague,Czech Republic.Javed A. Aslam and Meredith Frost.
2003.
Aninformation-theoretic measure for document similar-ity.
In Proceedings of 26th International ACM-SIGIRConference on Research and Development in Informa-tion Retrieval (SIGIR 2003), pages 449?450, Toronto,Canada.Timothy Baldwin, Steven Bird, and Baden Hughes.2006.
Collecting low-density language materials onthe web.
In Proceedings of the 12th Australasian WebConference (AusWeb06).
http://www.ausweb.scu.edu.au/ausweb06/edited/hughes/.William B. Cavnar and John M. Trenkle.
1994.
N-gram-based text categorization.
In Proceedings of theThird Symposium on Document Analysis and Informa-tion Retrieval, Las Vegas, USA.Marc Darnashek.
1995.
Gauging similarity with n-grams: Language-independent categorization of text.Science, 267:843?848.Rafael Dueire Lins and Paulo Gonc?alves.
2004.
Au-tomatic language identification of written texts.
InProceedings of the 2004 ACM Symposium on AppliedComputing (SAC 2004), pages 1128?1133, Nicosia,Cyprus.Ted Dunning.
1994.
Statistical identification of lan-guage.
Technical Report MCCS 940-273, ComputingResearch Laboratory, New Mexico State University.Emmanuel Giguet.
1995.
Categorization according tolanguage: A step toward combining linguistic knowl-edge and statistic learning.
In Proceedings of the4th International Workshop on Parsing Technologies(IWPT-1995), Prague, Czech Republic.E.
Mark Gold.
1967.
Language identification in thelimit.
Information and Control, 5:447?474.Gregory Grefenstette.
1995.
Comparing two languageidentification schemes.
In Proceedings of Analisi Sta-tistica dei Dati Testuali (JADT), pages 263?268.Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.2008.
A practical guide to support vector classifica-tion.
Technical report, Department of Computer Sci-ence National Taiwan University.Baden Hughes, Timothy Baldwin, Steven Bird, JeremyNicholson, and Andrew MacKinlay.
2006.
Recon-sidering language identification for written languageresources.
In Proceedings of the 5th InternationalConference on Language Resources and Evaluation(LREC 2006), pages 485?488, Genoa, Italy.Thorsten Joachims.
1998.
Text categorization with sup-port vector machines: learning with many relevant fea-tures.
In Proceedings of the 10th European Confer-ence on Machine Learning, pages 137?142, Chemnitz,Germany.Stephen Johnson.
1993.
Solving the problem of lan-guage recognition.
Technical report, School of Com-puter Studies, University of Leeds.Canasai Kruengkrai, Prapass Srichaivattana, VirachSornlertlamvanich, and Hitoshi Isahara.
2005.
Lan-guage identification based on string kernels.
In Pro-ceedings of the 5th International Symposium on Com-munications and Information Technologies (ISCIT-2005), pages 896?899, Beijing, China.Lillian Lee.
2001.
On the effectiveness of the skew diver-gence for statistical language analysis.
In Proceedingsof Artificial Intelligence and Statistics 2001 (AISTATS2001), pages 65?72, Key West, USA.Huma Lodhi, Craig Saunders, John Shawe-Taylor, NelloCristianini, and Chris Watkins.
2002.
Text classifica-tion using string kernels.
Journal of Machine LearningResearch, 2:419?444.Christopher D. Manning, Prabhakar Raghavan, and Hin-rich Schu?tze.
2008.
Introduction to Information Re-trieval.
Cambridge University Press, Cambridge, UK.Bruno Martins and Ma?rio J. Silva.
2005.
Language iden-tification in web pages.
In Proceedings of the 2005ACM symposium on Applied computing, pages 764?768, Santa Fe, USA.Andrew Kachites McCallum.
1996.
Bow: A toolkit forstatistical language modeling, text retrieval, classifica-tion and clustering.
http://www.cs.cmu.edu/?mccallum/bow.Paul McNamee and JamesMayfield.
2004.
CharacterN -gram Tokenization for European Language Text Re-trieval.
Information Retrieval, 7(1?2):73?97.Olivier Teytaud and Radwan Jalam.
2001.
Kernel-based text categorization.
In Proceedings of theInternational Joint Conference on Neural Networks(IJCNN?2001), Washington DC, USA.Vladimir N. Vapnik.
1995.
The Nature of StatisticalLearning Theory.
Springer-Verlag, Berlin, Germany.Fei Xia and William Lewis.
2009.
Applying NLP tech-nologies to the collection and enrichment of languagedata on the web to aid linguistic research.
In Pro-ceedings of the EACL 2009 Workshop on LanguageTechnology and Resources for Cultural Heritage, So-cial Sciences, Humanities, and Education (LaTeCH ?SHELT&R 2009), pages 51?59, Athens, Greece.Fei Xia, William Lewis, and Hoifung Poon.
2009.
Lan-guage ID in the context of harvesting language data offthe web.
In Proceedings of the 12th Conference of theEACL (EACL 2009), pages 870?878, Athens, Greece.237
