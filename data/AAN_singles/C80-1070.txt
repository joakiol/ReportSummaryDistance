A TR IAL  OF JAPANESE TEXT INPUT SYSTEMUS ING SPEECH RECOGNIT IONK.Shirai Y.Fukazawa T.Matzui II.MatzuuraDepartment of Electrical EngineeringWaseda University3-4-10kubo Shinjuku-kuTokyo, 160 JapanSummarySince written Japanese texts are expressed bymany kinds of characters, input technique ismost difficult when Japanese information is proc-essed by computers.
Therefore a task-independentJapanese text input system which has a speechanalyzer as a main input device and a keyboardas an auxiliary device was designed and it hasbeen implemented.
The outline and experience ofthis system is described in this paper.The system consists of the phoneme discrimina-tion part and the word discrimination part.Acoustic analysis and phonemic discriminationare effectively performed using an estimationmethod of the articulatory motion from speechwaves in the phoneme discrimination part.
And itsoutputs are lattices of Japanese pseudo-phonemescorresponding to speech inputs.In the word discrimination part, phonemic stringsare corrected using various kinds of a prioriknowledge anu transformed into suitable kinds ofcharacters.
On behalf of it, this part mainlyconsists of the refrieval of the word dictionaryindexed by vowels, and the similarity calculationusing dynamic programming.i.
IntroductionSpeech recognition systems can be classifiedaccording to whether they recognize isolated orconnected words.
But in either case, word recog-nition has been thought a basic problem.
In thespeech understanding system, a method of utiliz-ing various kinds of a priori knowledge, whichis obtained from a phonemic, a syntactic anda semantic level on clearly defined tasks, hasbeen developed.
And it has been clarified howinformation at each level contributes to recog-nition.
I-~While many understanding systems set the goalat the understanding of contents of speech, ournain object is the task-independent Japanesetexts input system.
Because a system which ut i -lizes properties of a task too much is not sopractical and the phonemic discrimination mustbe the core of the speech recognition system.This system consists of the phoneme discrimina~tion part and the word discrimination part.In the phoneme discrimination part, connectedspoken words is segmented into discrete phonemicstrings.
The main difficulties at this step arethe large variation of each utterance and thecoarticulation, and the difference betweentalkers.
So it is important to select acousticparameters which aren't relatively affected bythese variations.This system employs the feature extractonmethod based on the speech production model.
Thearticulatory motion for the utterance is estimat-ed from speech waves, and the phonemic discrimi-nation is performed using the trace of them.
Thefollowings are the merits to describe the acous-tic feature at an articulatory level:(i) Processing of coarticulation may be consid-ered most clearly in relation to the physio-logical factor of speech production.
(2) Adaptation to a speaker is easier than othermethods.Nevertheless it is almost impossible to const-ruct perfect phonemic strings corresponding tospoken words only by the phonemic discrimination.So in the word discrinimation part, the phonemicstrings are corrected using various kinds ofa priori information.
At the same time, it isneccessary to transform the phonemic strings intothe suitable kinds of characters in order to printthe texts.
Because written Japanese texts areexpressed by many kinds of characters, which areKanji (ideo-graphic character), Hira-gana (cursiveform of Kana, Japanese syllabary), Kata-kana(square form of Kana) and special symbols.The confusion matrix between phonemes, thephonemic connective rules, the word dictionaryand so on have been already used as a prioriinformation in word discrimination systems,and their validity has been also ascertainedl '~?6Therefore, theword discrimination part mainlyconsists of the retrieval of the word dictionarywhich is indexed by vowels, and the similaritycalculation using dynamic programming and thephonemic confusion matrix.
Because the phonemediscrimination part can recognize vowels morecorrectly than consonants and the phonemicconfusion matrix reflects the characteristics ofthe phoneme discrimination part exceedingly.In order to translate from Kana to Kanji, some ,~systems with the word dictionary for Kana-Kanjitranslation and with syntax and semanticsanalyzer, are suggested.
But the task domainshould be restricted so as to get the high trans-lation rate.Main difficulties of Kana-kanji translationare as follows:(i) Segmentation from input character s'tringsto words.
(2) Identification of synonymsIn this system, identifiers inserted from a sim-ple keyboard serve as spaces between words.
Andthe difficulty of the latter is solved by redun-dant inputs since speech input is much easierthan other input techniques.m--464--micropho~ne phoneme recognitionkeyboardsyntact i c  ~_approach l l \]~wo~a ~efe~en~e~___~matching I KANJI printerIFig 1-1 Block diagraB of the systemThe paper is structured as follows.
The nextsection gives a feature extraction method usingthe articulatory model.
And then the segmentationof the continuous speech, and the discriminationtechnique of vowels and consonants are described.Section 3 gives an outline of the word discrimi-nation method which uses the word dictionary?
constructed systematically and employs somepriori knowledge.
Section 4 concludes with abrief description of the results obtained withthe system thus far..2.
Phoneme discrimination.
?2.1 SegmentationIn the recognition of continuous speech, it iseffective to discriminate voiced, unvoiced andsilence.
Next four parameters are used forvoiced-unvoiced-silence decision.
141) Power of s igna l .
E:lO.log\[1 N1s2(n)\]Nn ~- (2-1)where N is the number of samples in one frame.0 , SO 100 250h i"""""i"'h"d""""h"'h"*h"'h"d""h"'h"ih"'h"'hmh"''d'"'hmh"i""h"d""""i"h"'hmh"'"'...... 1' ' \[frame\]a w a"*'?"
f- -q Is i lenceE,Fig.
2-1 Example of analysis /mikawasima/2) The ratio of the power in low frequency andthe total power of the signal.Power in low frequency = N Er= Total Power n~ (n) /n-~ 2(n)3) Zero-crossing count in one frame.
(2-2)4) Normalized autocorrelation coefficient atunit delay, Pl asN / / '  N N-1 i2-3)P l=n\ [ lS (n )s (n -1 ) / / (n~lS2(n) ) (  Z s2(n))n=0These parameters  are computed for  each frameof samples and the example of  ana lys i s  shown inF ig .2-1 .
The dec is ion is  performed using thenext quadrat i c  d i sc r iminat ion  funct ion .d i(x)=(x-~i)tc i-l(x-~i )+Inl cil (2-4)After this decision is made for each frame,smoothing algoritl~ is used .
By this smoothingalgorithm, noises and disturbances in the VUS-decision can be modified.
The result of thisdecision is shown in Fig.2-1.2.2 Articulatory modelSeveral authors have studied the estimation ofthe vocal tract shape from the speech wave.
Ifthe artieulatory motion is successfully estimat-ed, the result may be a good feature of thespeech and e f fec t ive  for  the recogn i t ion .
7-9 Inth i s  paper ,  a new technique is  used to es t imatethe vocal  t rac t  shape, which depends on theprec ise  ana lys i s  of the rea l  data of  the ar t i cu -latory mechanism.
10-13In this section, the articulatory model whichconstitutes the base of this method is intro-duced.
This model relies on the statisticalanalysis of the real data and makes it possibleto represent the position and the shape of eacharticulatory organ very precisely using thesmall number of parameters.
And further, thephysiological and phonologicalconstraints areincluded automatically in the model.The total configuration of the model is shownin Fig.2-2.
The jaw is assumed to rotate withthe fixed radius FJ about the fixed point F, andthe location J is given by the jaw angle Xj.
Thelip shape is specified by the lip opening heightL h, the width L w and the protruction Lp relatingto the jaw position on the midsagittal plane.The tongue contour is described in terms of asemi-polar coordinate system fixed to the lowerVelum p A L A T E_5 ter .
.F(O,O) p J T ?ngu'0oso.iption of orticu ato.yG --G'(ag,bg)Glottis--465--jaw and represented as a 13 dimensional vectorr=(rl ..... rla).
The variability of the usualarticulatory process, there may be some limita-tions on account of physiological and phonologi-cal constraints, which can be expressed by thestrong correlation of the position of eachsegment along the tongue contour.
Therefore, itbecomes possible to represent the tongue shapeeffectively using only a few parameters.
Theseparameters can be extracted from the statisticalanalysis of X-ray data.A principal component analysis is applied andthe tongue contour vector for vowels r v may beexpressed in a linear form as,r v = li=l XTiVi + rL (2-5)where vi's (i=l,2 ..... p) are eigenvectors, andr v is a mean vector for vowels which corre-sponds roughly to the neutral tongue contour.The eigenvectors are calculated from the nextequation C v. = ~.v.
(2-6)V 1 1 1where C v is the covariance matrix which is de-fined by NI _ - _ ?
)T}(2-7)C v = ~ ~=l{(rvk rv) (rvl~ ,,and Xi i s  the cor respond ing  e igenva lue  to sa t -i s fy  the character i s t i c  equat ionIcv - ~I I  = o ~2-~The same s tas t i ca l  techn ique as descr ibedabove may be used fo r  the express ion  o f  l ipshape.
( k --L h 1Js + L bI'w = \[ k2'Jl~3+ ~';Lp \]~where Os s ign i f ies  the d is tanc ,19between the up-per and the lower incisors.
The active movementof the lips is reflected in the second term, andX L is the lip parameter.The total model with the nasal cavity is shownin Fig.2-3 and the characteristics of the artic-ulatory parameters are summarized in Table 2-1.Details about the model are found in the refer-ences.2.3 Estimation of articulatory motionIn this section, the estimation of 'the articu-F. \]Let an m-dimensional vector y be the acousticfeature to represent the vocal tract transferfunction of the model.
In this study the ceps-trum coefficients are adopted as the acousticparameters.
The acoustic parameters are express-ed asa nonlinear function h(x) of the articula-tory parameters which means the transformationfrom x to y through the speech production model.On the other hand, let sY be acoustic parametersmeasured from the speech wave after glottal andradiation characteristics are removed.
Then, theestimate ~ of the articulatory parameters is ob-tained as to minimize the next cost function.2 (2-10) J(=k 3=11 sYk -b(Xk)lf~+ II ?k I1~* II % -~k-l l lrwhere R,Q and F are the matr i ces  of  we ight ,  k i sframe number and ~k-1 i s  the est imate at theprev ious  frame.
In the cost funct ion ,  the 1stterm i s  the weighted square er rors  between theacoust i c  parameters of  the model and the measur-ed va lues .
The 2nd and 3rd terms are used to re -strict the deviation from the neutral condition(x=0) and from the estimate of the previousframe, respectively.
These terms are also effi ~cient to reduce the compensation of the articu-latory parameters.rilslatory parameters from the speech wave is con:sidered in the framework of the speech produc- Glottistion model.
This problem becomes nonlinear opti- XGmization of parameters under a certain criterion, -(---9+and it must be solved by an iterative procedure.Therefore, the uniquness of the solution and thestability of the convergence are significantproblems.
In this method, these problems aresolved by introducing constraints to confine thearticulatory parameters within a restricted re-gion and by the selection of the appropriate in- +itial estimate using the continuity property.And also the model adjustment to the speakerbrings good effects for the estimation.andlatoryTable 2-1 Articulatery parametersArticulatoryparametersTongue Tonguc J:tw IApXTI ?T2 Xj X LGlottis VehlmX G X Nback high open round open openfront low clese spread close ?lose- -466-If the method is applied to a speaker, firstof all, the mean length of the vocal tractshould be adjusted for the speaker.
The lengthfactor SFK which gives a uniform change in thevocal tract length, and the articulatory param-eters can be estimated simultaneously, Once themean vocal tract length is fixed, only the ar-ticulatory parameters are estimated.
In Fig.
2-4the estimated result for the utterance /aiueo/is shown.
15-172.4 Normalization of articulatory parametersThe size and shape of each articulatory organare different between talkers, and further themanner of articulation varies each other.
Thesedifferences make difficult the talker independentspeech recognition problem.
Therefore, a methodto be able to adapt for the speaker differencesis inevitable for speech recognition.In the estimated artieulatory parameters inthe preceding section, the average length of thevocal tract is taken into consideration and au-tomatically normalized.
While the vocal tractlength is the most significant in this problem,other factors still remain.voca l  t rac t  rea l  es t imatedshape spectrum spectrumglottisThe differences in Japanese 5 vowels betweenadult females are shown in Fig.2-6.
In order tocancel these differences, a linear transforma-tion which changes the distribution of femalearticulatory parameters into that of male onesi:s introduced.Number of  ma le  apoiker l l  - I0XT2lulI I I I I  I I lle lI l li ; /o/X~Fix.2-$ DLstr lbut ion for  male speakers (A group)in  XTI-X~2 spacilx~ ,.~.r of fe,.l.
peak.,,.
- lO lul ~ ,  ~A, , ,  - .
,~ ' ') /'/"l, ~ l al ",1 I0-- " ' " , /  '~I~',~,: ' \ I  , -- I I I I III i~ ~  I tl I i  II I I I Ii I I I I II~L - - -L~ I I I , I Iu _ I l l| |- PLB.2-6 Dist r ibut?on for  female speakers (b group)in XTI -X~2 space0-9_o3OkHz ~ frame\]-Fig.
2-4 Estimated resultfor the utterance /aiueo/lipsNw.ber of  female speakers - 10XT2,%/ i /Ioi /u/  ,X~I|PiJ,2~7 D is t r ibut ion  fo r  female speakers (b group)in XTI-X~2 space a f te r  l inear  t ransformat ion- -467-X=AX+B (2 - i i )wherex=(XT1XT2,Xj,XL) ~ :modif ied vectors  coming' near  the male d i s t r ibut ion .X=(XTI,XT2,Xj,XL) T :es t imat ion  va lue forthe femaleThe matr i ces  A and B are ca icu la ted  by theleas t  squares method.
In Fig.
2-5 ,2-6 and 2-7distribution for 10 males and females are shownrespectively.
The distribution of Fig.2-6 istransformed by the above linear transformationinto Fig.2-7.
It should be noticed that the ma-trices A and B were determined by data of i0 fe-male who are not contained in the group shown inFig.2-6.
The results for the vowel recognitionexperiment are shown in Table 2-2.In this experiment, the number of subjects is50, that is 30 males and 20 females.
Each sub-ject uttered every vowel five times.
They aregrouped every ten subjects, then there are threemale groups A, B, C and two female groups a, b.Experiment I ; Recognition of male voices afterlearning by male voicesExperiment II ; Recognition of female voicesafter learning by female voicesExperiment HI; Recognition of female voicesafter leaning by male voicesExperiment ~;  Recognition of female voicesusing normalization of lineartransformation, where the refer- "ence is made b Z male voicesThese results show the effectiveness of thearticulatory parameters for the talker independ-ent speech recognition and the normalizationprocedure is useful for the male-female trans-formation.2.5 Discrimination of continuous vowels by usin~articulatory parametersThe locus of estimated articulatory parametersabout /aiueo/ is shown in Fig.
2-8.Initially, continuous vowels are discriminatedframe by frame using a quadratic discriminationfunction of articulatory parameters.
As this re-Table 2-2 Confusion matrix for the result of vowel recognitionExperiment I 98.8% Experiment ~I 94.9%/ i //u//e //o//a /  / i /  /u /  /e /  /o //a ,  298 - 2296 4 - /i/- 300 - - /u /- 2 - 298 /e /9 - 1 - 290 /o/Experiment 1I 98.0%I~  /a /  li/ /u/ /el /o//a // i //u//el/el99 - 196 499 1i002 2 - 96lal I l l  lul ~el Ioi295 - 1 4- 286 - 14 -1 298 1 -2 1 9 288 -37 6 257Exper iment  ~ 97.3%lal l i l  lul ~el Iol/a/ 289 - Iili/ - 298 - 2/u/ - 299 1/el  - 15 - 285/o/ I ii 288sults involve transient parts, it is necessaryto estimate them.
Then S(k) is computed as theparameter of discriminating them.S(k)= I~Tl(k+l) -XTl(k 1 1+ \[XTo(k+ 1)-XTo+\ ]X j (k+I ) -X j (k ) J+JXL~+I) -XL(k~t  k) l (2-12)I t  can be regarded the va l ley  of S(k) ~s thestable part and the high part of S(k) than thethreshold t as transient part.
But, it is diffi-cult to distiguish between stable part and tran-sient part enough.
So stable part is decidedwith changing threshold t.The speakers in this experiment are 2 male a-dults.
Each speaker spoke 23 kinds of continuous5 vowels 2 times.
This continuous vowels involveall 60 kinds of connected vowel VIV2Va(VI,V2,V3:a,i,u,e,o) such as aie, oei.Table 2-3 shows the errors occurred by coartic-ulation.f rontFig.
2-8The locus of articulatoryparametersTable 2-\]low3 Vowel discriminationXT2h igh/a iueo /.
.
.
.
.
.
.
.
.
ierrorsspeaker S.M.NO.I No.
2 No.
3!
/aiueol2 /aeiou/3 /aeuol/4 / iaoiu/ 5o iu~ouu \ [6ao~o5 / iuoea/ 5uo--u6 l ieaou/7 /uaieo/ 178 /uaoel/ le i~i i  7a?~?
8 uaO~uOO~ei--ii \[ e i~ i9 /ueaio/ \ [9eai~eeiI0 /uoaeo/II /eiuae/212 /euioa/ e ~ i13 /eoaui/14 /eouei/15 /oiaue/16 /oieua/17 /ouiae/18 /auoua/19 /uieia/20  /eauau/21 /eoioe/ ; ioe~iue22 /oaiai/  ~la i~ie i23 /oeueu/)a~o\[Oei~ ii20 io~ue O2~i~ iL~ e~i i  22i ae~ieeL2 2~OU~OO auo~ao24\[3 u ie~ueL~au~ "Ou_~L5ueu~ulu25ia i~  ieiT.S.No.426ao~a28uao~uou29eai~eel3Oeau~eeuJoe ~ lue31--468-2.6 Consonant recognition using articulatoryparametersBy the data of the gliding section betweenconsonant and vowel, consonants are discriminat-ed.
In this discrimination experiment, a methodof DPmatching applied for the loci of the artic-ulatory parameters which are estimated for thegliding section.
Simple acoustic parameters areused together.Speech data in this experiment is uttered by2 male adults, each speaker spoke continuousVzCV2.
(Vz is fixed at /a/, V2 is vowel /a/,/i/,/u/,/e/,/o/, C is consonant /g/,/z/,/d/,/b/,/h/,/s/,/p/,/t/,/k/ where /adi/,/adu/ were omitted.
)One data is for reference pattern, another isfor the test.Since the articulatory organs show relativelyspeedy motion in the interval from the consonantto the vowel, the frame length is selected as 12ESTi~TION OF ~' liATQIING FINkLPAR/~TI!I~Fig.2-8 .Block diagra m of the recogn i t ion  experimentTable 2-4 Results of the recognition experiment~ m / ~  /~ /~/ga/ 20 o o o2 /za/ 2 z6 o/cYa/ o o 2o o./hV o o o 2,)n'/',ca/ /aa/ / ta /  /pa,' /ha/20 0 0 0 0/aa/ 1 17 2 o o/tus/ 6 0 13 1 o/pa/ \] o 1 ~.
1/ha/ 0 1 0 0 19/~/  / z i /  /h i /Iq i l  18 1 1, /~ /  0 2O 0/bL/ 0 1 19/',ct/ /,,hi/ /c:~./ /p i /  ,44j/k:L/ 13 0 2 0 5/~ht /  0 20 0 0 0/d~L/ 0 0 20 0 0/~/  o : e 20 o,q~/ 5 0 0 0 15/cS./ /=u/ /bu/loW' 19 1 o/~v' 2 18 o./Ix*/ 0 0 2O/~/  /de/ /,,,,//ge/ 20 0 0/~/  0 18 2/de/ o 1 19Foe/ o o l/ .
.
/itsu/i pu /,,~/ ~o ,4~/o ~1elo I~I19 /pc//he/~'~ 19ol 17,o/ I~ol /bol/go/ 15 0 3 2/zo/ 0 17 3 0/do/ 0 1 16 3~o/  0 I 4 15vo iced  : 91 .4  %/k~V /~V /b~u/ /pu/ /m/19 0 0 1 00 16 1 1 01 9 10 0 01 0 0 18 10 0 o 1 19/~1 leml ~eel /be/ ~,,el19 0 0 0 10 20 0 0 00 2 14 4 ' O0 0 1 16 30 0 0 2 18/ko/ 1"/ 0 3 o o/~:~/ o 2o 0 o o/ to /  0 1 17 2 0lpo/ 1 o 4 14 II /h  o /  0 0 0 4 16unvo iced  : 85 .4  %ms with Hamming Window and the pitch syncronousmethod is used for the analysis.
For the estima-tion initial value is obtained by the piecewiselinear estimation method at stable vowel section.And articulatory parameters are estimated back-ward to consonant section, using the parametersat the previous frame as the initial value.
Theprocess of the discrimination experiment isroughly shown in Fig.2-9.Initially, we estimate the articulatory param-eters of every data by the method mentioned a-bove and perform the end-free DP matching forthe reference pattern of every kind of data foreach speaker.
The total power and the power inhigh frequency are also used for the discrimi-nation.
For the recognition of the voiced con-sonants,the results of the DP matching of artic-ulatory parameters are reliable.
For that ofthe unvoiced consonants,the acoustic parametersare mainly used.
These results are shown inTable 2-4.The attempt to extract the feature of conso-nants from the gliding part of the articulatorymotion in the following vowel was successful tosome extent.
But there are some problems in theaccuracy in the estimation of the articulatoryparameters.
And the individual difference hasinfluence on the parameters.3.
Word discrimination3.i How to input Japanese textsIdeally speaking, texts which are arbitrarilypronounced should be recognized perfectly andsuitable Kana-Kanji translation should be per-formed.
But the phonemic discrimination, theprocessing of synonyms and so on are actuallyvery difficult.
So the system uses a simple key-board as an auxiliary input device, it has only 7keys and it is enough manageable by one hand.In case of input, first of all, a key indicat-ing the kind of the character must be pushed.
Forexample, 'H' indicates Hira-gana.
For Kanji, thereading of the compound word, On-yomi (the phone-tical reading of Kanji) and Kun-yomi (the Japa-nese rendering of Kanji) constructing the wordare pronounced.
Since the acoustic input is mucheasier than the other input techniques, someredundant data can be inputted with less burden.In the current version, a key indicating thekind of the reading pronounced is pushed for thesake of the easy processing.For Hira-gana, a speaker may pronounce a wordwhich contains its reading as the ending.
Other-wise a user just pronounces Japanese texts as away of speaking with the control informationfrom the keyboard.control part t main control character\[sub control characterinput data <phonemic partl ph?nemic data~phonemic probabilityFig 3-i the component of input datato the word discrimination part--469--3.2 Input dataThe component of input data to the word dis-crimination part is indicated in Fig.3-1.The phonemic part is an acoustic data whichis recognized in the phoneme discrimination part.If it cannot decide only one phonemic candidate,the phoneme and its probability for each candi-date are passed as a lattice.
That is to say,input data is a lattice of Japanese pseudo-phonemes corresponding to the acoustic input.Fig.3-2 is an example of input data.The control part is inputted from the keyboardin order to correct the phonemic data and performeffective Kana-Kanji translation.l e t te rreadingmeaningtensa i  wa kyuju-kyu paasento no ase dearuGenius i s  n inety -n ine  per  cent persp i ra t ion .data C J ( tensa i )  T TENSAIY (ten) T TEN , (area) T A~Y (sai)  T SAI , ( tosh i )  T TOSHIHH Y (wa) T WAS Y (ku) T KUY (ku) T KUK Y (paasento) T PAASENTOH Y (no) T NOfi Y (ase) T ASE , (Ran) T KANH Y (dearu) T DEARU* (ten) - - -  t8p6k3 e7al n7m7Fig 342 Example of  an input  dataparameters as fol lows are gotS ; sequence of vowelsI N ; number of  vowelsLR ; re l iab le  phonemesget the res t r i c ted  d ic t ionary(gL)ith N ~ r ie t  BL with SJF.
:ou.
r" ' ca lcu la te  the s imi la r i ty  for the BL items Isat i s fy ing  the R res t r i c t ion  \[\[Ithen,some of the most s imi la r  items,CA are l \ ]\ [se lected as cand idates  ___~lL ; boundary of s imi la r i ty(given)Fig 3~3 Algorithm to decide some candidates3.3 Word discrimination algorithmFor Kanji, redundant data, which consists ofa reading of a compound word, On-yomi, and Kun-yomi, are given.
Some candidates of Kanji corre-sponding to each reading are sought using thealgorithm of Fig.3-3.
This algorithm depends onthe fact that vowels can be discriminated moreprecisely than consonants, and the tendency whichone phoneme is apt to be misdiscriminated fromothers are statistically known previously.Kanji dictionary used contains about i000readings of Kanji and corresponding characters.This dictionary is divided by the number ofvowels and further indexed by the sequence ofthem.
A part of it is shown in Fig.3-4._reading of  KAN3I \] d i c t ionarynt~nber o f  vowels.
.vowo,??
?
?
.items.
.
.
.
? '
'Fig 3-4 Example of  a d ic t ionarySimilarity between a discriminated phonemicstring and a word contained in Kanji dictionaryis computed by dynamic programming using theconfusion matrix.
This matrix is made using theresults of the recognition experiments and con-tains the misdiscrimination probability to otherphonemes, the probability of omission and thatof addition of other phonemes.
Among these, theprobability of addition depends upon the tendencythat the addition is influenced by the followingphoneme as tile nature of the phoneme discrimi-nation part.In dynamic programming, it is assumed that thenumber of the continuous omission or addition ofphonemes is less than two.
The similarity S(D,W)between the phonemic string D whose length is I,ans W whose length is J is calculated as follows:s (D ,w)  = g(1 ,1 ) / ig( i , j )  = max{ L l ( i , j ) , La ( i , j ) , Lo ( i , j )}Ll(i,j) = sim(i,j)+g(i+l,j+l)La(i,j ) = sim(i,j+l)+g(i+l,j+2)+adp(i,j)Lo(i,j) = sim(i+l, j) +g (i+2, j +I) +omp (j)g(l,j) = sup (J-j)g(i,J) = sup (I-i)where sim(i,j) is the similarity between phonemei and j, and given from the confusion matrix,adp(i,j) is the probability indicating that pho-neme i is added before phoneme j, omp(i) isthe probability omitting i, and sup(i) is thepenalty value of unmatched length.This algorithm selects some candidates of Kanjiwhich have a high matching score.
Since someredundant data is given for each kanji, one mostsuitable character is selected and printed.- -470For Hira-gana, the system doesn't have redun-dant readings originally in contrast with KanjiAs it is especially difficult to discriminatetoo short phonemic strings, phonemic correctionsare neccessary.
So the system utilizes the natureof Japanese that almost all short Hira-ganastrings are found as the inflectional part orJoshi (auxiliary word in Japanese).
For theformer, a user may pronounce a word which con-tains the inflection and the system uses them asa redundant data using the inflection dictionary.For the latter, the dictionary containing Joshiand its connection information is used as a prioriinformation.In case of Kata-kana, as the usage is restrictedto the words of foreign origin, the phonemiccorrection is performed using the loanworddictionary.
At this time, the algorithm ofFig.3-2 is available.Besides, Japanese text input system mustprocess special characters, for example numbers,alphabets, punctuation marks and so on.
But theyare a few, so the above algorithm can processedthem using special character table.4.
ConclusionAt present, the system uses YHP-21MX and thespecial purpose hardware for the phonemic dataextraction, and the phoneme and the word discri-mination part are built on HITAC 8800/8700 at theComputing Centre of the University of Tokyo.Though the performance, the facility and thebottle-neck as the total system has not beenclarified as yet, partially some results andproblems have been found.The phoneme discrimination part aims at recog-nizing connected speech uttered by an unspecifiedspeaker.
On behalf of it, the feature extractionmethod at the articulatory level is tried.
Asa result, stable articulatory motion is estimatedwith the satisfactory precision for vowels, andthe validity of it is confirmed.
And the adapta-tion for speakers is effectively performed.
Forconsonants, the feature extraction method usingthe transient part of the articulatory motionis tried, but it has a little drawpoint at theprecision and the stability of articulatoryparameters.
It is also influenced by a speaker.A combination to the other sound parameters willperhaps make the system improve.For the word discrimination, the syntax hasnot been used except for Hira-gana at thisversion.
Hereafter it is planned to incorporatethe syntactic information as much as possiblefor the improvement of the performance.
But thetrade-off between the facility and the processingspeed is an important problem.AcknowledgmentThe authors wish to thank J.Kubota, T. Kobayashiand M.Ohashi for their contributions to designingand developing this system.References(i) Lea,W.A.
Ed.
'Trends in Speech Recognition'Prentice-Hall (1980)(2) Ready,D.R.
Ed.
'Speech Recognition' AcademicPress (1975)(3) Newell,A.
et al'Speech UnderstandingSystem: Final Report of a Study Group'North-Holland (1973)(4) Denes,P.
'The Design and Operation of theMechanical Speech Recognition at UniversityCollege London' Jour.
Brit.
I.R.E.
Vol.19No.4 pp.219-234 (1959)(5) Woods,W.A.
'Motivation and Overview ofSPEECHLIS: An experimental Prototype forSpeech Understanding Research' IEEE Trans.A.S.S.P.
VoI.ASSP-23 pp.2 (1975)(6) Sakai,T.,Nakagawa,S.
'A Classification Methodof Spoken Words in Continuous Speech for ManySpeakers' Jour.
Inf.
Proc.
Soci.
of JapanVol.17 No.7 pp.650-658 (1976)(7)Shirai,K.
'Feature extraction and sentencerecognition algorithm in speech input system'4th Int.
J. Conf.
on Artificial Intelligence,506-513, 1975.(8)Wakita,H.
'Direct estimation of the vocal tractshape by inverse filtering of acoustic speechwaveform, IEEE Trans., ASSP-23, 574-580, 1975.(9)Nakajima,T.,Omura,T.,Tanaka,H.,Ishizaki,S.
'Estimation of vocal tract area functions byadaptive inverse filtering methods' Bull.Electrotech.
Lab., 37, 467-481, 1973.
(10)Stevens,K.N., House,A.S., 'Development of aquantitative description of vowel articula-tion',JASA, 27, 484-493, 1955.
(ll)Coker,C.H.,Fujimura,O.,'Model for specifica-tion of the vocal-tract area function',JASA,40, 1271, 1966.(12)Lindblom,B.,Sunberg,J.
'Acoustical conse-quences of lip, tongue, jaw and larynx move-ment' JASA, 50, 1166-1179,1971.
(13)Shirai,K.,H~nda,M.,,An articulatory modeland the estimation of articulatory parame-ters by nonlinear regression method' Trans.IECE, Vol.
J59-A, No.8, 668-674, 1976.
(14)Atal,B.S.,Rabiner,L.R,,'A pattern recogni-tion approach to voiced-unvoiced-silenceclassification with applications to speechrecognition' IE~E Transo, ASSP-24, 201-212,1976.
(15)Shirai,K.,Honda,M.,'Estimation of articula-tory parameters from speech waves' Trans.IECE, Vol.61-A, No.5, 409-416,1978.
(16)Shirai,K.,Honda,M.,'Feature extraction forspeech recognition based on articulatorymodel' Proc.
of 4th Int.
J. Conf.
on PatternRecognition 1978.
(17)Shirai,K.,Matzui,T.,'Estimation of articula-tory states from nasal sounds' Trans.
IECE,Vol.
J63-A, No.2, 1980.--471--
