Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 1029?1037,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsAutomatic Metaphor Interpretation as a Paraphrasing TaskEkaterina ShutovaComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKEkaterina.Shutova@cl.cam.ac.ukAbstractWe present a novel approach to metaphor in-terpretation and a system that produces lit-eral paraphrases for metaphorical expressions.Such a representation is directly transferableto other applications that can benefit from ametaphor processing component.
Our methodis distinguished from the previous work in thatit does not rely on any hand-crafted knowl-edge about metaphor, but in contrast employsautomatically induced selectional preferences.Being the first of its kind, our system is capa-ble of paraphrasing metaphorical expressionswith a high accuracy (0.81).1 IntroductionMetaphors arise when one concept is viewed interms of the properties of the other.
In other wordsit is based on similarity between the concepts.
Sim-ilarity is a kind of association implying the presenceof characteristics in common.
Here are some exam-ples of metaphor.
(1) News travels fast.
(Lakoff and Johnson, 1980)(2) How can I kill a process?
(Martin, 1988)(3) And then my heart with pleasure fills,And dances with the daffodils.1In metaphorical expressions seemingly unrelatedfeatures of one concept are associated with another1taken from the verse ?I wandered lonely as a cloud?
writtenby William Wordsworth in 1804.concept.
In the example (2) the computational pro-cess is viewed as something alive and, therefore,its forced termination is associated with the act ofkilling.Metaphorical expressions represent a great vari-ety, ranging from conventional metaphors, which wereproduce and comprehend every day, e.g.
those in(1) and (2), to poetic and largely novel ones, suchas (3).
The use of metaphor is ubiquitous in naturallanguage text and it is a serious bottleneck in auto-matic text understanding.
In order to estimate thefrequency of the phenomenon, we conducted a cor-pus study on a subset of the British National Corpus(BNC) (Burnard, 2007) representing various genres.We manually annotated metaphorical expressions inthis data and found that 241 out of 761 sentencescontained a metaphor or (rarely) an idiom.
Due tosuch a high frequency of their use, a system capa-ble of interpreting metaphorical expressions in unre-stricted text would become an invaluable componentof any semantics-oriented NLP application.Automatic processing of metaphor can be clearlydivided into two subtasks: metaphor recognition(distinguishing between literal and metaphoricallanguage in text) and metaphor interpretation (iden-tifying the intended literal meaning of a metaphori-cal expression).
Both of them have been repeatedlyaddressed in NLP.To date the most influential account of metaphorrecognition has been that of Wilks (1978).
Accord-ing to Wilks, metaphors represent a violation of se-lectional restrictions in a given context.
Consider thefollowing example.
(4) My car drinks gasoline.
(Wilks, 1978)1029The verb drink normally takes an animate subjectand a liquid object.
Therefore, drink taking a car asa subject is an anomaly, which may as well indicatemetaphorical use of drink.Most approaches to metaphor interpretation relyon task-specific hand-coded knowledge (Fass, 1991;Martin, 1990; Narayanan, 1997; Narayanan, 1999;Feldman and Narayanan, 2004; Barnden and Lee,2002; Agerri et al, 2007) and produce interpreta-tions in a non-textual format.
However, the ultimateobjective of automatic metaphor processing is a typeof interpretation that can be directly embedded intoother systems to enhance their performance.
Thus,we define metaphor interpretation as a paraphrasingtask and build a system that automatically derivesliteral paraphrases for metaphorical expressions inunrestricted text.In summary, our system (1) produces a list ofall possible paraphrases for a metaphorical expres-sion (induced automatically from a large corpus);(2) ranks the paraphrases according to their likeli-hood derived from the corpus; (3) discriminates be-tween literal and figurative paraphrases by detect-ing selectional preference violation and outputs theliteral ones; and (4) disambiguates the sense of theparaphrases using WordNet (Fellbaum, 1998) inven-tory of senses.We tested our system on a collection of metaphor-ical expressions representing verb-subject and verb-object constructions, where the verb is usedmetaphorically.
To compile this dataset we manuallyannotated such phrases in a subset of the BNC usingthe metaphor identification procedure (MIP) (Prag-glejaz Group, 2007).
We then evaluated the qualityof paraphrasing with the help of human annotatorsand created a gold standard for this task.2 Experimental DataSince we focus on single-word metaphors expressedby a verb, our annotation task can be viewed asverb classification according to whether the verbsare used metaphorically or literally.
However, someverbs have weak or no potential of being a metaphorand, thus, our study is not concerned with them.
Weexcluded the following verb classes: (1) auxiliaryverbs; (2) modal verbs; (3) aspectual verbs (e.g.
be-gin, start, finish); (4) light verbs (e.g.
take, give, put,get, make).2.1 The CorpusOur corpus is a subset of the BNC.
We sampledtexts representing various genres: literature, news-paper/journal articles, essays on politics, interna-tional relations and history, radio broadcast (tran-scribed speech).
The corpus contains 761 sentencesand 13642 words.2.2 Annotation SchemeThe annotation scheme we use is based on theprinciples of the metaphor identification procedure(MIP) developed by Pragglejaz Group (2007).
Weadopt their definition of basic sense of a word andtheir approach to distinguishing basic senses fromthe metaphorical ones.
MIP involves metaphor an-notation at the word level as opposed to identifyingmetaphorical relations (between words) or source?target domain mappings (between concepts or do-mains).
Such annotation can be viewed as a formof word sense disambiguation with an emphasis onmetaphoricity.In order to discriminate between the verbs usedmetaphorically and literally we use the followingprocedure as part of our guidelines:1.
For each verb establish its meaning in contextand try to imagine a more basic meaning of thisverb on other contexts.
As defined in the frame-work of MIP (Pragglejaz Group, 2007) basicmeanings normally are: (1) more concrete; (2)related to bodily action; (3) more precise (asopposed to vague); (4) historically older.2.
If you can establish the basic meaning that isdistinct from the meaning of the verb in thiscontext, the verb is likely to be used metaphor-ically.Consider the following example sentence:(5) If he asked her to post a letter or buy some razorblades from the chemist, she was transportedwith pleasure.In this sentence one needs to annotate four verbs thatare underlined.
The first 3 verbs are used in their ba-sic sense, i.e.
literally (ask in the context of ?a per-son asking another person a question or a favour?
;1030post in the context of ?a person posting/sending aletter?
; buy in the sense of ?making a purchase?
).Thus, they are tagged as literal.
The verb trans-port, however, in its basic sense is used in the con-text of ?goods being transported/carried by a vehi-cle?.
The context in this sentence involves ?a per-son being transported by a feeling?, which contraststhe basic sense in that the agent of transporting isan EMOTION as opposed to a VEHICLE.
Thus, wecan infer that the use of transport in this sentence ismetaphorical.2.3 Annotation ReliabilityWe tested reliability of this annotation scheme usingmultiple annotators on a subset of the corpus.
Therest of the annotation was done by a single annota-tor.Annotators We had three independent volunteer an-notators, who were all native speakers of Englishand had some linguistics background.Material and Task All of them received the sametext taken from the BNC containing 142 verbs toannotate.
They were asked to classify verbs asmetaphorical or literal.Guidelines and Training The annotators receivedwritten guidelines (2 pages) and were asked to do asmall annotation exercise (2 sentences containing 8verbs in total).
The goal of the exercise was to en-sure they were at ease with the annotation format.Interannotator Agreement We evaluate reliabilityof our annotation scheme by assessing interannota-tor agreement in terms of ?
(Siegel and Castellan,1988).
The classification was performed with theagreement of 0.64 (?
), which is considered reliable.The main source of disagreement was the high con-ventionality of some expressions, i.e.
cases wherethe metaphorical etymology could be clearly traced,but the senses are highly lexicalized.2.4 Phrase SelectionOnly the phrases that were tagged as metaphoricalby all of the annotators were included in the test set.Here are some examples of such phrases: memo-ries were slipping away; hold the truth back; stirredan unfathomable excitement; factors shape results;mending their marriage; brushed aside the accusa-tions etc.
In order to avoid extra noise we placedsome additional criteria to select the test phrases:(1) exclude phrases where subject or object referentis unknown, e.g.
containing pronouns such as in inwhich they [changes] operated; (2) exclude phraseswhose metaphorical meaning is realised solely inpassive constructions (e.g.
sociologists have beeninclined to [..]); (3) exclude phrases where the sub-ject or object of interest are represented by a namedentity (e.g.
Then Hillary leapt into the conversa-tion); (4) exclude multiword metaphors (e.g.
go onpilgrimage with Raleigh or put out to sea with Ten-nyson).
The resulting test set contains 62 metaphor-ical expressions.3 The MethodThe system takes phrases containing annotatedsingle-word metaphors (where a verb is usedmetaphorically, its context is used literally) as in-put.
It generates a list of possible paraphrases thatcan occur in the same context and ranks them ac-cording to their likelihood derived from the cor-pus.
Subsequently it identifies shared features of theparaphrases and the metaphorical verb using Word-Net hierarchy of concepts and removes the unrelatedconcepts.
Among the related paraphrases it thenidentifies the literal ones given the context relying onthe automatically induced selectional preferences.3.1 The Model for Paraphrase RankingWe model the likelihood of a particular paraphraseas a joint probability of the following events: theinterpretation (another term to replace the one usedmetaphorically) i co-occurring with the other lexi-cal items from its context w1, ..., wN in the relationsr1, ..., rN respectively.Li = P (i, (w1, r1), (w2, r2), ..., (wN , rN )), (1)where w1, ..., wN and r1, ..., rN represent the fixedcontext of the term used metaphorically in the sen-tence.
This context will be kept as part of the para-phrase, and the term used metaphorically will be re-placed.We take each relation of the term in a phrase to beindependent from the other relations of this term inthis phrase.
E.g.
for a verb in the presence of boththe subject and the object the Verb-Subject andVerb-Object relations would be considered tobe independent events within the model.
This yields1031the following approximation:P (i, (w1, r1), (w2, r2), ..., (wN , rN )) =P (i) ?
P ((w1, r1)|i) ?
... ?
P ((wN , rN )|i).
(2)We can calculate the probabilities using maximumlikelihood estimationP (i) =f(i)?k f(ik), (3)P (wn, rn|i) =f(wn, rn, i)f(i), (4)where f(i) is the frequency of the interpretation onits own,?k f(ik) is the number of times this partof speech is attested in the corpus and f(wn, rn, i)- the frequency of the co-occurrence of the interpre-tation with the context word wn in the relation rn.By performing appropriate substitutions into (2) weobtainP (i, (w1, r1), (w2, r2), ..., (wN , rN )) =f(i)?k f(ik)?f(w1, r1, i)f(i)?
... ?f(wN , rN , i)f(i)=?Nn=1 f(wn, rn, i)(f(i))N?1 ?
?k f(ik)(5)This model is then used to rank the possible re-placements of the term used metaphorically in thefixed context according to the data.3.2 Parameter EstimationThe parameters of the model were estimated fromthe British National Corpus that was parsed usingthe RASP parser of Briscoe et al (2006).
We usedthe grammatical relations (GRs) output of RASPfor BNC created by Andersen et al (2008).
Thesame output of RASP was used to identify the GRsin the metaphorical expressions themselves, as themetaphor corpus from which they were extractedis a subset of the BNC.
To obtain the counts forf(wn, rn, i) we extracted all the terms appearing inthe corpus in the relation rn with wn for each lexicalitem - relation pair.
The initial list of replacementsfor the metaphorical term was constructed by takingan overlap of the lists of terms for each lexical item- relation pair.3.3 Identifying Shared Meanings in WordNetIt should be noted that the context-based modeldescribed in 3.1 overgenerates and hence there isa need to further narrow the search space.
Itis acknowledged in the linguistics community thatmetaphor is to a great extent based on similarity be-tween the concepts involved.
We exploit this fact torefine paraphrasing.
After obtaining the initial listof possible substitutes for the metaphorical term, wefilter out the terms whose meaning does not shareany common features with that of the metaphoricalterm.
Consider a Computer Science metaphor kill aprocess, which stands for terminate a process.
Thebasic sense of kill implies an end or termination oflife.
Thus, termination is the shared element of themetaphorical verb and its literal interpretation.Such overlap of features can be identified usingthe hyponymy relations in the WordNet taxonomy.Within the initial list of paraphrases we select theterms that are a hypernym of the metaphorical termor share a common hypernym with it2.
To maxi-mize the accuracy we restrict the hypernym searchto three level distance in the taxomomy.
The filteredlists of metaphorical verb replacements for some ofthe phrases from our dataset together with their log-likelihood are demonstrated in Table 1.
Selectingthe highest ranked paraphrase from this list as a lit-eral interpretation will serve as a baseline.3.4 Filtering Based on Selectional PreferencesThe obtained lists contain some irrelevant para-phrases (e.g.
contain the truth for hold back thetruth) and some paraphrases where the substitute isused metaphorically again (e.g.
suppress the truth).However, the task is to identify the literal interpreta-tion, therefore, these need to be removed.One way of dealing with both problems at onceis to take into account selectional preferences of theverbs in our list.
The verbs used metaphorically arelikely to demonstrate strong semantic preference forthe source domain, e.g.
suppress would select formovements (political) rather than ideas, or truth, (thetarget domain), whereas the ones used literally (e.g.,2We excluded the expressions containing a term whosemetaphorical sense is included in WordNet from the test set,to ensure that the system does not rely on this extra hand-codedknowledge about metaphor.1032Log-likelihood ReplacementVerb-DirectObjecthold back truth:-13.09 contain-14.15 conceal-14.62 suppress-15.13 hold-16.23 keep-16.24 defendstir excitement:-14.28 create-14.84 provoke-15.53 make-15.53 elicit-15.53 arouse-16.23 stimulate-16.23 raise-16.23 excite-16.23 conjureSubject-Verbreport leak:-11.78 reveal-12.59 issue-13.18 disclose-13.28 emerge-14.84 expose-16.23 discoverTable 1: The list of paraphrases with the initial rankingconceal) would select for truth.
This would poten-tially allow us to filter out non-literalness, as well asunrelated verbs, by selecting the verbs that the nounin the metaphorical expression matches best.We automatically acquired selectional preferencedistributions of the verbs in the paraphrase lists(for Verb-Subject and Verb-Object rela-tions) from the BNC parsed by RASP.
We first clus-tered 2000 most frequent nouns in the BNC into 200clusters using the algorithm of Sun and Korhonen(2009).
The obtained clusters formed our selectionalpreference classes.
We adopted the association mea-sure proposed by Resnik (1993) and successfully ap-plied to a number of tasks in NLP including wordsense disambiguation (Resnik, 1997).
Resnik mod-els selectional preference of a verb in probabilisticterms as the difference between the posterior distri-bution of noun classes in a particular relation withthe verb and their prior distribution in that syntac-tic position regardless of the identity of the predi-cate.
He quantifies this difference using the relativeentropy (or Kullback-Leibler distance), defining theAssociation ReplacementVerb-DirectObjecthold back truth:0.1161 conceal0.0214 keep0.0070 suppress0.0022 contain0.0018 defend0.0006 holdstir excitement:0.0696 provoke0.0245 elicit0.0194 arouse0.0061 conjure0.0028 create0.0001 stimulate?
0 raise?
0 make?
0 exciteSubject-Verbreport leak:0.1492 disclose0.1463 discover0.0674 reveal0.0597 issue?
0 emerge?
0 exposeTable 2: The list of paraphrases reranked using selec-tional preferencesselectional preference strength as follows.SR(v) = D(P (c|v)||P (c)) =?cP (c|v) logP (c|v)P (c),(6)where P (c) is the prior probability of the noun class,P (c|v) is the posterior probability of the noun classgiven the verb and R is the grammatical relation inquestion.
Selectional preference strength measureshow strongly the predicate constrains its arguments.In order to quantify how well a particular argumentclass fits the verb, Resnik defines another measurecalled selectional association:AR(v, c) =1SR(v)P (c|v) logP (c|v)P (c).
(7)We use this measure to rerank the paraphrases andfilter out those not well suited or used metaphor-ically.
The new ranking is demonstrated in Table2.
The expectation is that the paraphrase in the firstrank (i.e.
the verb with which the noun in question1033has the highest association) represents the literal in-terpretation.3.5 Sense DisambiguationAnother feature of our system is that having identi-fied literal interpretations, it is capable to performtheir word sense disambiguation (WSD).
Disam-biguated metaphorical interpretations are potentiallya useful source of information for NLP applicationsdealing with word senses.We adopt WordNet representation of a sense.Disambiguation is performed by selecting WordNetnodes containing those verbs that share a commonhypernym with the metaphorical verb.
The list ofdisambiguated interpretations for a random selectionof phrases from our dataset is demonstrated in Table3.
However, we did not evaluate the WSD of theparaphrases at this stage.4 Evaluation and DiscussionWe evaluated the paraphrases with the help of hu-man annotators in two different experimental set-tings.Setting 1: the annotators were presented with a setof sentences containing metaphorical expressionsand their rank 1 paraphrases produced by the systemand by the baseline.
They were asked to mark theones that have the same meaning as the term usedmetaphorically and are used literally in the contextof the paraphrase expression as correct.We had 7 volunteer annotators who were all na-tive speakers of English (one bilingual) and had noor sparse linguistic expertise.
Their agreement onthe task was 0.62 (?
), whereby the main sourceof disagreement was the presence of highly lexi-calised metaphorical paraphrases.
We then evalu-ated the system performance against their judgmentsin terms of accuracy.
Accuracy measures the pro-portion of correct literal interpretations among theparaphrases in rank 1.
The results are demonstratedin Table 4, the final systems identifies literal para-phrases with the accuracy of 0.81.Setting 2: the annotators were presented with a setof sentences containing metaphorical expressionsand asked to write down all suitable literal para-phrases for the highlighted metaphorical verbs.
Wehad 5 volunteer subjects for this experiment (notethat these were people not employed in the previ-ous setting); they were all native speakers of En-glish and had some linguistics background.
We thencompiled a gold standard by incorporating all of theannotations.
E.g.
the gold standard for the phrasebrushed aside the accusations contains the verbs re-jected, ignored, disregarded, dismissed, overlooked,discarded.We compared the system output against the goldstandard using mean reciprocal rank (MRR) as ameasure.
MRR is traditionally used to evaluate theperformance of Question-Answering systems.
Weadapted this measure in order to be able to assessranking quality beyond rank 1 and the recall of oursystem.
An individual metaphorical expression re-ceives a score equal to the reciprocal of the rank atwhich the first correct literal interpretation (accord-ing to the human gold standard) is found among thetop five paraphrases, or 0 if none of the five para-phrases contains a correct interpretation.
Once theindividual reciprocal ranks of metaphorical expres-sions are estimated their mean is computed acrossthe dataset.
The MRR of our system equals 0.63and that of the baseline is 0.55.
However, it shouldbe noted that given that our task is open-ended, itis hard to construct a comprehensive gold standard.For example, for the phrase stir excitement most an-notators suggested only one paraphrase create ex-citement, which is found in rank 3.
However, the topranks of the system output are occupied by provokeand stimulate, which are more precise paraphrases,although they have not occurred to the annotators.Such examples result in the system?s MRR beingsignificantly lower than its accuracy at rank 1.The obtained results are promising, the selec-tional preference-based reranking yields a consider-able improvement in accuracy (26%) over the base-line.
However, for one of the phrases in the dataset,mend marriage, the new ranking overruns the cor-rect top suggestion of the baseline, improve mar-riage, and outputs repair marriage as the most likelyliteral interpretation.
This is due to both the conven-tionality of some metaphorical senses (in this caserepair) and to the fact that some verbs, e.g.
improve,expose a moderate selectional preference strength,i.e.
they are equally associated with a large numberof classes.
This demonstrates potential drawbacks ofthe selectional preference-based solutions.
Another1034Met.
Expression Top Int.
Its WordNet SenseVerb-DirectObjectstir excitement provoke (arouse-1 elicit-1 enkindle-2 kindle-3 evoke-1 fire-7 raise-10 provoke-1) - call forth(emotions, feelings, and responses): ?arouse pity?
; ?raise a smile?
; ?evoke sympathy?inherit state acquire (get-1 acquire-1) - come into the possession of something concrete or abstract: ?She gota lot of paintings from her uncle?
; ?They acquired a new pet?reflect concern manifest (attest-1 certify-1 manifest-1 demonstrate-3 evidence-1) - provide evidence for; standas proof of; show by one?s behavior, attitude, or external attributes: ?The buildings inRome manifest a high level of architectural sophistication?
; ?This decision demonstrateshis sense of fairness?brush aside accusation reject (reject-1) - refuse to accept or acknowledge: ?we reject the idea of starting a war?
; ?Thejournal rejected the student?s paper?Verb-Subjectcampaign surged improve (better-3 improve-2 ameliorate-2 meliorate-2) - to make better: ?The editor improvedthe manuscript with his changes?report leaked disclose (unwrap-2 disclose-1 let on-1 bring out-9 reveal-2 discover-6 expose-2 divulge-1break-15 give away-2 let out-2) - make known to the public information that was pre-viously known only to a few people or that was meant to be kept a secret: ?The auctionhouse would not disclose the price at which the van Gogh had sold?
; ?The actress won?treveal how old she is?tension mounted lift (rise-1 lift-4 arise-5 move up-2 go up-1 come up-6 uprise-6) - move upward: ?The foglifted?
; ?The smoke arose from the forest fire?
; ?The mist uprose from the meadows?Table 3: Disambiguated paraphrases produced by the systemRelation Baseline SystemVerb-DirectObject 0.52 0.79Verb-Subject 0.57 0.83Average 0.55 0.81Table 4: Accuracy with the evaluation setting 1controvertial example was the metaphorical expres-sion tension mounted, for which the system pro-duced a paraphrase tension lifted with the oppositemeaning.
This error is likely to have been triggeredby the feature similarity component, whereby one ofthe senses of lift would stem from the same node inWordNet as the metaphorical sense of mount.5 Related WorkAccording to Conceptual Metaphor Theory (Lakoffand Johnson, 1980) metaphor can be viewed as ananalogy between two distinct domains - the targetand the source.
Consider the following example:(6) He shot down all of my arguments.
(Lakoff andJohnson, 1980)A mapping of a concept of argument (target) tothat of war (source) is employed here.
The idea ofsuch interconceptual mappings has been exploited insome NLP systems.One of the first attempts to identify and inter-pret metaphorical expressions in text automaticallyis the approach of Fass (1991).
It originates inthe work of Wilks (1978) and utilizes hand-codedknowledge.
Fass (1991) developed a system calledmet*, capable of discriminating between literal-ness, metonymy, metaphor and anomaly.
It doesthis in three stages.
First, literalness is distin-guished from non-literalness using selectional pref-erence violation as an indicator.
In the case that non-literalness is detected, the respective phrase is testedfor being a metonymic relation using hand-codedpatterns (such as CONTAINER-for-CONTENT).
Ifthe system fails to recognize metonymy, it pro-ceeds to search the knowledge base for a relevantanalogy in order to discriminate metaphorical re-lations from anomalous ones.
E.g., the sentencein (4) would be represented in this framework as(car,drink,gasoline), which does not satisfy the pref-erence (animal,drink,liquid), as car is not a hy-ponym of animal.
met* then searches its knowl-edge base for a triple containing a hypernym ofboth the actual argument and the desired argumentand finds (thing,use,energy source), which repre-sents the metaphorical interpretation.Almost simultaneously with the work of Fass(1991), Martin (1990) presents a Metaphor Inter-1035pretation, Denotation and Acquisition System (MI-DAS).
The idea behind this work is that the morespecific conventional metaphors descend from thegeneral ones.
Given an example of a metaphor-ical expression, MIDAS searches its database fora corresponding metaphor that would explain theanomaly.
If it does not find any, it abstracts fromthe example to more general concepts and repeatsthe search.
If it finds a suitable general metaphor, itcreates a mapping for its descendant, a more specificmetaphor, based on this example.
This is also hownovel metaphors are acquired.
MIDAS has been in-tegrated with the Unix Consultant (UC), the systemthat answers users questions about Unix.Another cohort of approaches relies on perform-ing inferences about entities and events in the sourceand target domains for metaphor interpretation.These include the KARMA system (Narayanan,1997; Narayanan, 1999; Feldman and Narayanan,2004) and the ATT-Meta project (Barnden and Lee,2002; Agerri et al, 2007).
Within both systemsthe authors developed a metaphor-based reasoningframework in accordance with the theory of concep-tual metaphor.
The reasoning process relies on man-ually coded knowledge about the world and operatesmainly in the source domain.
The results are thenprojected onto the target domain using the concep-tual mapping representation.
The ATT-Meta projectconcerns metaphorical and metonymic descriptionof mental states and reasoning about mental statesusing first order logic.
Their system, however, doesnot take natural language sentences as input, butlogical expressions that are representations of smalldiscourse fragments.
KARMA in turn deals with abroad range of abstract actions and events and takesparsed text as input.Veale and Hao (2008) derive a ?fluid knowledgerepresentation for metaphor interpretation and gen-eration?, called Talking Points.
Talking Points are aset of characteristics of concepts belonging to sourceand target domains and related facts about the worldwhich the authors acquire automatically from Word-Net and from the web.
Talking Points are then orga-nized in Slipnet, a framework that allows for a num-ber of insertions, deletions and substitutions in def-initions of such characteristics in order to establisha connection between the target and the source con-cepts.
This work builds on the idea of slippage inknowledge representation for understanding analo-gies in abstract domains (Hofstadter and Mitchell,1994; Hofstadter, 1995).
Consider the metaphorMake-up is a Western burqa:Make-up =>?
typically worn by women?
expected to be worn by women?
must be worn by women?
must be worn by Muslim womenBurqa <=By doing insertions and substitutions the system ar-rives from the definition typically worn by women tothat of must be worn by Muslim women, and thus es-tablish a link between the concepts of make-up andburqa.
Veale and Hao (2008), however, did not eval-uate to which extent their method is useful to inter-pret metaphorical expressions occurring in text.6 ConclusionsWe presented a novel approach to metaphor interpre-tation and a system that produces literal paraphrasesfor metaphorical expressions.
Such a representationis directly transferable to other applications that canbenefit from a metaphor processing component.
Ourmethod is distinguished from the previous work inthat it does not rely on any hand-crafted knowledge,other than WordNet, but in contrast employs auto-matically induced selectional preferences.Our system is the first of its kind and it is capa-ble of paraphrasing metaphorical expressions with ahigh accuracy (0.81).
Although we reported resultson a test set consisting of verb-subject and verb-object metaphors only, we are convinced that thedescribed interpretation techniques can be similarlyapplied to other parts of speech and a wider rangeof syntactic constructions.
Extending the system todeal with more types of phrases is part of our futurework.AcknowledgmentsI am very grateful to Anna Korhonen, SimoneTeufel, Ann Copestake and the reviewers for theirhelpful feedback on this work, Lin Sun for sharinghis noun clustering data and the volunteer annota-tors.
My studies and, thus, this research are fundedby generosity of Cambridge Overseas Trust.1036ReferencesR.
Agerri, J.A.
Barnden, M.G.
Lee, and A.M.Wallington.2007.
Metaphor, inference and domain-independentmappings.
In Proceedings of International Confer-ence on Recent Advances in Natural Language Pro-cessing (RANLP-2007), pages 17?23, Borovets, Bul-garia.O.
E. Andersen, J. Nioche, E. Briscoe, and J. Carroll.2008.
The BNC parsed with RASP4UIMA.
InProceedings of the Sixth International Language Re-sources and Evaluation Conference (LREC?08), Mar-rakech, Morocco.J.A.
Barnden and M.G.
Lee.
2002.
An artificial intelli-gence approach to metaphor understanding.
Theoriaet Historia Scientiarum, 6(1):399?412.E.
Briscoe, J. Carroll, and R. Watson.
2006.
The secondrelease of the rasp system.
In Proceedings of the COL-ING/ACL on Interactive presentation sessions, pages77?80.L.
Burnard.
2007.
Reference Guide for theBritish National Corpus (XML Edition).URL=http://www.natcorp.ox.ac.uk/XMLedition/URG/.D.
Fass.
1991. met*: A method for discriminatingmetonymy and metaphor by computer.
ComputationalLinguistics, 17(1):49?90.J.
Feldman and S. Narayanan.
2004.
Embodied meaningin a neural theory of language.
Brain and Language,89(2):385?392.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lexi-cal Database (ISBN: 0-262-06197-X).
MIT Press, firstedition.D.
Hofstadter and M. Mitchell.
1994.
The Copy-cat Project: A model of mental fluidity and analogy-making.
In K.J.
Holyoak and J.
A. Barnden, editors,Advances in Connectionist and Neural ComputationTheory, Ablex, New Jersey.D.
Hofstadter.
1995.
Fluid Concepts and CreativeAnalogies: Computer Models of the FundamentalMechanisms of Thought.
HarperCollins Publishers.G.
Lakoff and M. Johnson.
1980.
Metaphors We Live By.University of Chicago Press, Chicago.J.
H. Martin.
1988.
Representing regularities in themetaphoric lexicon.
In Proceedings of the 12th con-ference on Computational linguistics, pages 396?401.J.
H. Martin.
1990.
A Computational Model of MetaphorInterpretation.
Academic Press Professional, Inc., SanDiego, CA, USA.S.
Narayanan.
1997.
Knowledge-based action represen-tations for metaphor and aspect (KARMA).
Technicalreport, PhD thesis, University of California at Berke-ley.S.
Narayanan.
1999.
Moving right along: A computa-tional model of metaphoric reasoning about events.
InIn Proceedings of the National Conference on Artifi-cial Intelligence (AAAI 99), pages 121?128, Orlando,Florida.Pragglejaz Group (P. Crisp, R. Gibbs, A. Cienki, G.Low, G. Steen, L. Cameron, E. Semino, J. Grady, A.Deignan and Z. Kovecses).
2007.
MIP: A method foridentifying metaphorically used words in discourse.Metaphor and Symbol, 22:1?39.P.
Resnik.
1993.
Selection and Information: A Class-based Approach to Lexical Relationships.
Ph.D. the-sis, Philadelphia, PA, USA.P.
Resnik.
1997.
Selectional preference and sense disam-biguation.
In ACL SIGLEX Workshop on Tagging Textwith Lexical Semantics, Washington, D.C.S.
Siegel and N. J. Castellan.
1988.
Nonparametricstatistics for the behavioral sciences.
McGraw-HillBook Company, New York, USA.L.
Sun and A. Korhonen.
2009.
Improving verb clus-tering with automatically acquired selectional prefer-ences.
In Proceedings of the 2009 Conference onEmpirical Methods in Natural Language Processing,pages 638?647, Singapore, August.T.
Veale and Y. Hao.
2008.
A fluid knowledge rep-resentation for understanding and generating creativemetaphors.
In Proceedings of the 22nd Interna-tional Conference on Computational Linguistics (Col-ing 2008), pages 945?952, Manchester, UK.Y.
Wilks.
1978.
Making preferences more active.
Artifi-cial Intelligence, 11(3):197?223.1037
