Coling 2010: Poster Volume, pages 656?664,Beijing, August 2010Unsupervised Discriminative Language Model Trainingfor Machine Translation using Simulated Confusion SetsZhifei Li and Ziyuan Wang and Sanjeev Khudanpur and Jason EisnerCenter for Language and Speech ProcessingJohns Hopkins Universityzhifei.work@gmail.com,{zwang40, khudanpur, eisner}@jhu.eduAbstractAn unsupervised discriminative trainingprocedure is proposed for estimating alanguage model (LM) for machine trans-lation (MT).
An English-to-English syn-chronous context-free grammar is derivedfrom a baseline MT system to capturetranslation alternatives: pairs of words,phrases or other sentence fragments thatpotentially compete to be the translationof the same source-language fragment.Using this grammar, a set of impostorsentences is then created for each En-glish sentence to simulate confusions thatwould arise if the system were to processan (unavailable) input whose correct En-glish translation is that sentence.
An LMis then trained to discriminate betweenthe original sentences and the impostors.The procedure is applied to the IWSLTChinese-to-English translation task, andpromising improvements on a state-of-the-art MT system are demonstrated.1 Discriminative Language ModelingA language model (LM) constitutes a crucial com-ponent in many tasks such as machine translation(MT), speech recognition, information retrieval,handwriting recognition, etc.
It assigns a pri-ori probabilities to word sequences.
In general,we expect a low probability for an ungrammat-ical or implausible word sequence.
The domi-nant LM used in such systems is the so-calledn-gram model, which is typically derived from alarge corpus of target language text via maximumlikelihood estimation, mitigated by some smooth-ing or regularization.
Due to the Markovian as-sumptions implicit in n-gram models, however,richer linguistic and semantic dependencies arenot well captured.
Rosenfeld (1996) and Khu-danpur and Wu (2000) address such shortcom-ing by using maximum entropy models with long-span features, while still working with a locallynormalized left-to-right LM.
The whole-sentencemaximum entropy LM of Rosenfeld et al (2001)proposes a globally normalized log-linear LM in-corporating several sentence-wide features.The n-gram as well as the whole-sentencemodel are generative or descriptive models oftext.
However, in a task like Chinese-to-EnglishMT, the de facto role of the LM is to discriminateamong the alternative English translations beingcontemplated by the MT system for a particularChinese input sentence.
We call the set of suchalternative translations a confusion set.
Since aconfusion set is typically a minuscule subset ofthe set of all possible word sequences, it is ar-guably better to train the LM parameters so as tomake the best candidate in the confusion set morelikely than its competitors, as done by Roark et al(2004) for speech recognition and by Li and Khu-danpur (2008) for MT.
Note that identifying thebest candidate requires supervised training data?bilingual text in case of MT?which is expensivein many domains (e.g.
weblog or newsgroup) andfor most language pairs (e.g.
Urdu-English).We propose a novel discriminative LM in thispaper: a globally normalized log-linear LM thatcan be trained in an efficient and unsupervisedmanner, using only monolingual (English) text.The main idea is to exploit (translation) un-certainties inherent in an MT system to de-rive an English-to-English confusion grammar(CG), illustrated in this paper for a Hiero sys-tem (Chiang, 2007).
From the bilingual syn-chronous context-free grammar (SCFG) used inHiero, we extract a monolingual SCFG, with rulesof the kind, X ?
?strong tea, powerful tea?
or656X ?
?in X1, in the X1?.
Thus our CG is also anSCFG that generates pairs of English sentencesthat differ from each other in ways that alterna-tive English hypothesis considered during transla-tion would differ from each other.
This CG is thenused to ?translate?
each sentence in the LM train-ing corpus into what we call its confusion set ?
aset of other ?sentences?
with which that sentencewould likely be confused by the MT system, wereit to be the target translation of a source-languagesentence.
Sentences in the training corpus, eachpaired with its confusion set, are then used to traina discriminative LM to prefer the training sen-tences over the alternatives in their confusion sets.Since the monolingual CG and the bilingualHiero grammar are both SCFGs, the confusionsets are isomorphic with translation hypergraphsthat are used by supervised discriminative train-ing.
The confusion sets thus simulate the super-vised case, with a key exception: lack of any(Chinese) source-language information.
There-fore, only target-side ?language model?
probabil-ities may be estimated from confusion sets.We carry out this discriminative training proce-dure, and empirically demonstrate promising im-provements in translation quality.2 Discriminative LM Training2.1 Whole-sentence Maximum Entropy LMWe aim to train a globally normalized log-linearlanguage model p?
(y) of the formp?
(y) = Z?1 ef(y)??
(1)where y is an English sentence, f(y) is a vectorof arbitrary features of y, ?
is the (weight) vec-tor of model parameters, and Z def= ?y?
ef(y?)??
isa normalization constant.
Given a set of Englishtraining sentences {yi}, the parameters ?
may bechosen to maximize likelihood, as??
= argmax??ip?(yi).
(2)This is the so called whole-sentence maximumentropy (WSME) language model1 proposed by1Note the contrast with the maximum entropy n-gramLM (Rosenfeld, 1996; Khudanpur and Wu, 2000), where thenormalization is performed for each n-gram history.Rosenfeld et al (2001).
Training the model of(2) requires computing Z, a sum over all possibleword sequences y?
with any length, which is com-putationally intractable.
Rosenfeld et al (2001)approximate Z by random sampling.2.2 Supervised Discriminative LM TrainingIn addition to the computational disadvantage, (2)also has a modeling limitation.
In particular, ina task like MT, the primary role of the LM is todiscriminate among alternative translations of agiven source-language sentence.
This set of alter-natives is typically a minuscule subset of all pos-sible target-language word sequences.
Therefore,a better way to train the global log-linear LM,given bilingual text {(xi, yi)}, is to generate thereal confusion set N (xi) for each input sentencexi using a specific MT system, and to adjust ?
todiscriminate between the reference translation yiand y?
?
N (xi) (Roark et al, 2004; Li and Khu-danpur, 2008).For example, one may maximize the condi-tional likelihood of the bilingual training data as??
= argmax??ip?
(yi |xi) (3)= argmax??ief(xi,yi)???y?
?N (xi) ef(xi,y?)??
,which entails summing over only the candidatetranslations y?
of the given input xi.
Furthermore,if the features f(xi, y) are depend on only the out-put y, i.e.
on the English-side features of the bilin-gual text, the resulting discriminative model maybe interpreted as a language model.Finally, in a Hiero style MT system, if f(xi, y)depends on the target-side(s) of the bilingual rulesused to construct y from xi, we essentially have asyntactic LM.2.3 Unsupervised Discriminative Trainingusing Simulated Confusion SetsWhile the supervised discriminative LM traininghas both computational and modeling advantagesover the WSME LM, it relies on bilingual data,which is expensive to obtain for several domainsand language pairs.
For such cases, we proposea novel discriminative language model, which is657still a global log-linear LM with the modeling ad-vantage and computational efficiency of (3) but re-quires only monolingual text {yi} for training ?.Specifically, we propose to modify (3) as??
= argmax??ip?
(yi | N (yi)) (4)= argmax??ief(yi)???y?
?N (yi) ef(y?)??
,where N (yi) is a simulated confusion set for yiobtained by applying a confusion grammar to yi,as detailed in Section 3.
Our hope is that N (yi)resembles the actual confusion set N (xi) that anMT system would generate if it were given the in-put sentence xi.Like (3), the maximum likelihood training of(4) does not entail the expensive computation of aglobal normalization constant Z, and is thereforevery efficient.
Unlike (3) however, where the inputxi for each output yi is needed to create N (xi),the model of (4) can be trained in an unsupervisedmanner with only {yi}.3 Unsupervised Discriminative Trainingof the Language Model for MTThe following is thus the proposed procedure forunsupervised discriminative training of the LM.1.
Extract a confusion grammar (CG) from thebaseline MT system.2.
?Translate?
each English sentence in the LMtraining corpus, using the CG as an English-to-English translation model, to generate asimulated confusion set.3.
Train a discriminative language model on thesimulated confusion sets, using the corre-sponding original English sentences as thetraining references.The trained model may then be used for actual MTdecoding.
We next describe each step in detail.3.1 Extracting a Confusion GrammarWe assume a synchronous context free grammar(SCFG) formalism for the confusion grammar(CG).
While the SCFG used by the MT systemis bilingual, the CG we extract will be monolin-gual, with both the source and target sides beingEnglish.
Some example CG rules are:X ?
?
strong tea , powerful tea ?
,X ?
?X0 at beijing , beijing ?s X0 ?
,X ?
?X0 of X1 , X0 of the X1 ?
,X ?
?X0 ?s X1 , X1 of X0 ?
.Like a regular SCFG, a CG contains rules withdifferent ?arities?
and reordering of the nontermi-nals (as shown in the last example) capturing theconfusions that the MT system encounters whenchoosing word senses, reordering patterns, etc.3.1.1 Extracting a Confusion Grammar fromthe Bilingual GrammarThe confusion grammar is derived from the MTsystem?s bilingual grammar.
In Hiero, the bilin-gual rules are of the form X ?
?c, e?, whereboth c and e may contain (a matched number of)nonterminal symbols.
For every c which appearson the source-side of two different Hiero rulesX ?
?c, e1?
and X ?
?c, e2?, we extract two CGrules, X ?
?e1, e2?
and X ?
?e2, e1?, to capturethe confusion the MT system would face were itto encounter c in its input.
For each Hiero ruleX ?
?c, e?, we also extract X ?
?e, e?, the iden-tity rule.
Therefore, if a pattern c appears with |E|different translation options, we extract |E|2 dif-ferent CG rules from c. In our current work, therules of the CG are unweighted.3.1.2 Test-set Specific Confusion GrammarsIf the bilingual grammar contains all the rulesthat are extractable from the bilingual training cor-pus, the resulting confusion grammar is likely tobe huge.
As a way of reducing computation, thebilingual grammar can be restricted to a specifictest set, and only rules used by the MT system fortranslating the test set used for extracting the CG.2To economize further, one may extract a CGfrom the translation hypergraphs that are gener-ated for the test-set.
Recall that a node in a hy-pergraph corresponds to a specific source (Chi-nese) span, and the node has many incident hy-peredges, each associated with a different bilin-2Test-set specific CGs are of course only practical for off-line applications.658gual rule.
Therefore, all the bilingual rules asso-ciated with the incoming hyperedges of a givennode translate the same Chinese string.
At eachhypergraph node, we extract CG rules to representthe competing English sides as described above.Note that even though different rules associatedwith a node may have different ?arity,?
we extractCG rules only from pairs of bilingual rules thathave the same arity.A CG extracted from only the bilingual rulepairs incident on the same node in the test hy-pergraphs is, of course, much smaller than a CGextracted from the entire bilingual grammar.
Itis also more suitable for our task, since the testhypergraphs have already benefited from a base-line n-gram LM and pruning, removing all confu-sions that are easily resolved (rightly or wrongly)by other system components.3.2 Generating Simulated Confusion SetsFor each English sentence y in the training cor-pus, we use the extracted CG to produce a simu-lated confusion set N (y).
This is done like a reg-ular MT decoding pass, because we can treat theCG as a Hiero style ?translation?
grammar3 for anEnglish-to-English translation system.Since the CG is an SCFG, the confusion setN (y) generated for a sentence y is a hypergraph,encoding not only the alternative sentences y?
butalso the hierarchical derivation tree for each y?from y (e.g., which phrase in y has been re-placed with what in y?).
As usual, many differ-ent derivation trees d may correspond to the samestring/sentence y?
due to spurious ambiguity.
Weuse D(y) to denote the set of derivations d, whichis a hypergraph representation of N (y).Figure 1 presents an example confusion hy-pergraph for the English sentence y =?a cat onthe mat,?
containing four alternative hypotheses:3To make sure that we produce at least one derivation treefor each y, we need to add to the CG the following two gluerules, as done in Hiero (Chiang, 2007).S ?
?X0 , X0 ?
,S ?
?S0 X1 , S0 X1 ?
.We also add an out of vocabulary rule X ?
?word, oov?
foreach word in y and set the cost of this rule to a high value sothat the OOV rule will get used only when the CG does notknow how to ?translate?
the word.X ?
?
a cat , the cat ?X ?
?
the mat , the mat ?X ?
?X0 on X1 , X0 X1 ?X ?
?X0 on X1 , X0 ?s X1 ?X ?
?X0 on X1 , X1 on X0 ?X ?
?X0 on X1 , X1 of X0 ?S ?
?X0 , X0 ?
(a) An example confusion grammar.a0cat1on2the3mat4S?
?X0,X0?X0,5X0,2X3,5X ?
?
a cat , the cat ?
X ?
?
the mat , the mat ?X ?
?X0 on X1 , X0 X1 ?X ?
?X0 on X1 , X0 ?s X1 ?
X ?
?X0 on X1 , X1 of X0 ?X ?
?X0 on X1 , X1 on X0 ?S0,5(b) An example hypergraph generated by the confusiongrammar of (a) for the input sentence ?a cat on the mat.
?Figure 1: Example confusion grammar and simulatedconfusion hypergraph.
Given an input sentence y = ?a caton the mat,?
the confusion grammar of (a) generates a hyper-graph D(y) shown in (b), which represents the confusion setN (y) containing four alternative sentences y?.N (y) = { ?the cat the mat,?
?the cat ?s the mat,?
?the mat of the cat,?
?the mat on the cat?
}.Notice that each competitor y?
?
N (y) can beregarded as the result of a ?round-trip?
translationy ?
x ?
y?, in which we reconstruct a possibleChinese source sentence x that our Hiero bilin-gual grammar could translate into both y and y?.4We will train our LM to prefer y, which was ac-tually observed.
Our CG-based round-trip forcesx?
y?
to use the same hierarchical segmentationof x as y ?
x did.
This constraint leads to effi-cient training but artificially reduces the diversity4This is because of the way we construct our CG from theHiero grammar.
However, the identity and glue rules in ourCG allow almost any portion of y to be preserved untrans-lated through the entire y ?
x ?
y?
process.
Much of ywill necessarily be preserved in the situation where the CG isextracted from a small test set and hence has few non-identityrules.
See (Li, 2010) for further discussion.659ofN (y).
In other recent work (Li et al, 2010), wehave taken the round-trip view more seriously, byimputing likely source sentences x and translatingthem back to separate, weighted confusion forestsN (y), without any same-segmentation constraint.3.3 Confusion-based Discriminative TrainingWith the training sentences yi and their simulatedconfusion sets N (yi) ?
represented as hyper-graphs D(yi)) ?
we can perform the discrimi-native training using any of a number of proce-dures such as MERT (Och, 2003) or MIRA asused by Chiang et al (2009).
In our paper, weuse hypergraph-based minimum risk (Li and Eis-ner, 2009),??
= argmin??iRisk?
(yi) (5)= argmin?
?i?d?D(yi)L(Y(d), yi)p?
(d |D(yi)),where L(y?, yi) is the loss (e.g negated BLEU) in-curred by producing y?
when the true answer is yi,Y(d) is the English yield of a derivation d, andp?
(d |D(yi)) is defined as,p?
(d |D(yi)) = ef(d)??
?d?D(yi) ef(d)?
?, (6)where f(d) is a feature vector over d. We willspecify the features in Section 5, but in generalthey should be defined such that the training willbe efficient and the actual MT decoding can usethem conveniently.The objective of (5) is differentiable and thuswe can optimize ?
by a gradient-based method.The risk and its gradient on a hypergraph canbe computed by using a second-order expectationsemiring (Li and Eisner, 2009).3.3.1 Iterative TrainingIn practice, the full confusion set N (y) definedby a confusion grammar may be too large and wehave to perform pruning when training our model.But the pruning itself may depend on the modelthat we aim to train.
How do we solve this circu-lar dependency problem?
We adopt the followingprocedure.
Given an initial model ?, we generate ahypergraph (with pruning) for each y, and train anoptimal ??
of (5) on these hypergraphs.
Then, weuse the optimal ??
to regenerate a hypergraph foreach y, and do the training again.
This iterates un-til convergence.
This procedure is quite similar tothe k-best MERT (Och, 2003) where the traininginvolves a few iterations, and each iteration uses anew k-best list generated using the latest model.3.4 Applying the Discriminative LMFirst, we measure the goodness of our languagemodel in a simulated task.
We generate simulatedconfusion sets N (y) for some held out Englishsentences y, and test how well p?
(d |D(y)) canrecover y from N (y).
This is merely a proof ofconcept, and may be useful in deciding which fea-tures f(d) to employ for discriminative training.The intended use of our model is, of course, foractual MT decoding (e.g., translating Chinese toEnglish).
Specifically, we can add the discrimina-tive model into an MT pipeline as a feature, andtune its weight relative to other models in the MTsystem, including the baseline n-gram LM.4 Related and Similar WorkThe detailed relation between the proposed pro-cedure and other language modeling techniqueshas been discussed in Sections 1 and 2.
Here, wereview two other methods that are related to ourmethod in a broader context.4.1 Unsupervised Training of GlobalLog-linear ModelsOur method is similar to the contrastive estimation(CE) of Smith and Eisner (2005) and its succes-sors (Poon et al, 2009).
In particular, our confu-sion grammar is like a neighborhood function inCE.
Also, our goal is to improve both efficiencyand accuracy, just as CE does.
However, thereare two important differences.
First, the neigh-borhood function in CE is manually created basedon human insights about the particular task, whileour neighborhood function, generated by the CG,is automatically learnt (e.g., from the bilingualgrammar) and specific to the MT system beingused.
Therefore, our neighborhood function ismore likely to be informative and adaptive to thetask.
Secondly, when tuning ?, CE uses the maxi-mum likelihood training, but we use the minimum660risk training of (5).
Since our training uses a task-specific loss function, it is likely to perform betterthan maximum likelihood training.4.2 Paraphrasing ModelsOur method is also related to methods for train-ing paraphrasing models (Quirk et al, 2004; Ban-nard and Callison-Burch, 2005; Callison-Burch etal., 2006; Madnani et al, 2007).
Specifically, theform of our confusion grammar is similar to thatof the paraphrase model they use, and the waysof extracting the grammar/model are also similaras both employ a second language (e.g., Chinesein our case) as a pivot.
However, while a ?trans-lation?
rule in a paraphrase model is expected tocontain a pair of phrases that are good alterna-tives for each other, a confusion rule in our CGis based on an MT system processing unseen testdata and contains pairs of phrases that are typi-cally bad (and only rarely good) alternatives foreach other.The motivation and goal are also different.
Forexample, the goal of Bannard and Callison-Burch(2005) is to extract paraphrases with the help ofparallel corpora.
Callison-Burch et al (2006) aimto improve MT quality by adding paraphrases inthe translation table, while Madnani et al (2007)aim to improve the minimum error rate training byadding the automatically generated paraphrasesinto the English reference sets.
In contrast, ourmotivation is to train a discriminative languagemodel to improve MT (by using the confusiongrammar to decide what alternatives the modelshould learn to discriminate).5 Experimental ResultsWe have applied the confusion-based discrimina-tive language model (CDLM) to the IWSLT 2005Chinese-to-English text translation task5 (Eck andHori, 2005).
We see promising improvementsover an n-gram LM for a solid Joshua-basedbaseline system (Li et al, 2009).5.1 Data Partitions for Training & TestingFour kinds of data are used for CDLM training:5This is a relatively small task compared to, say, the NISTMT tasks.
We worked on it for a proof-of-concept.
Havingbeen successful, we are now investigating larger MT tasks.# sentencesData Usage ZH ENSet1 TM & LM training 40k 40kSet2 Min-risk training 1006 1006?16Set3 CDLM training ?
1006?16Set4 Test 506 506?16Table 1: Data sets used.
Set1 contains translation-equivalentChinese-English sentence pairs, while for each Chinese sen-tence in Set2 and Set4, there are 16 English translations.
Set3happens to be the English side of Set2 due to lack of ad-ditional in-domain English text, but this is not noteworthy;Set3 could be any in-domain target-language text corpus.Set1 a bilingual training set on which 10 individ-ual MT system components are trained,Set2 a small bilingual, in-domain set for tuningrelative weights of the system components,Set3 an in-domain monolingual target-languagecorpus for CDLM training, andSet4 a test set on which improvements in MT per-formance is measured.We partition the IWSLT data into four such sub-sets as listed in Table 1.5.2 Baseline MT SystemOur baseline translation model components areestimated from 40k pairs of utterances from thetravel domain, called Set1 in Table 1.
We use a 5-gram language model with modified Kneser-Neysmoothing (Chen and Goodman, 1998), trained onthe English side of Set1, as our baseline LM.The baseline MT system comprises 10 com-ponent models (or ?features?)
that are standardin Hiero (Chiang, 2007), namely the baselinelanguage model (BLM) feature, three baselinetranslation model features, one word-insertionpenalty (WP) feature, and five arity features ?three to count how many rules with an arity ofzero/one/two are used in a derivation, and twoto count how many times the unary and binaryglue rules are used in a derivation.
The rela-tive weights of these 10 features are tuned viahypergraph-based minimum risk training (Li andEisner, 2009) on the bilingual data Set2.The resulting MT system gives a BLEU score of48.5% on Set4, which is arguably a solid baseline.6615.3 Unsupervised Training of the CDLMWe extract a test-set specific CG from the hyper-graphs obtained by decoding Set2 and Set4, as de-scribed in Section 3.1.2.
The number of rules inthe bilingual grammar and the CG are about 167kand 1583k respectively.
The CG is used as the?translation?
model to generate confusion hyper-graphs for sentences in Set3.Two CDLMs, corresponding to different fea-ture sets f(d) in equation (6), were trained.Only n-gram LM Features: We consider aCDLM with only two features f(d): a base-line LM feature (BLM) that equals the 5-gram probability of Y(d) and a word penaltyfeature (WP) equal to the length of Y(d).Target-side Rule Bigram Features6: For eachCG rule used in d, we extract counts of bi-grams that appear on the target-side of theCG rule.
For example, if the confusion ruleX ?
?X0 of X1 , X0 of the X1 ?
is used ind, the bigram features in f(d) whose countsare incremented are: ?X of,?
?of the?
and?the X .
?7 Note that the indices on the non-terminals in the rule have been removed.
Toavoid very rare features, we only considerthe 250 most freqent terminal symbol (En-glish words) in the English of Set1 and mapall other terminal symbols into a single class.Finally, we replace the identities of wordswith their dominant POS tags.
These restric-tions result in 525 target-side rule bigram(TsRB) features f(d) in the model of (6).For each choice of the feature vector f(d), be it2- or 527-dimensional, we use the training proce-dure of Section 3.3.1 to iteratively minimize theobjective of (5) and get the CDLM parameter ?
?.Note that each English sentence in Set3 has 15other paraphrases.
We generate a separate confu-sion hypergraph D(y) for each English sentencey, but for each such hypergraph we use both yand its 15 paraphrases as ?reference translations?when computing the risk L(Y(d), {y}) in (5).86Note that these features are novel in MT.7With these target-side rule-based features, our LM is es-sentially a syntactic LM, not just an LM on English strings.8We take unfair advantage of this unusual dataset to com-5.4 Results on Monolingual SimulationWe first probe how our novel CDLM performs asa language model itself.
One usually uses the per-plexity of the LM on some unseen text to measureits goodness.
But since we did not optimize theCDLM for likelihood, we instead examine howit performs in discriminating between a good En-glish sentence and sentences with which the MTsystem may confuse that sentence.
The test is per-formed as follows.
For each test English sentencey of Set4, the confusion grammar defines a fullconfusion set N (y) via a hypergraph D(y).
Weuse a LM to pick the most likely y?
from N (y),and then compute its BLEU score by using y andits 15 paraphrase sentences as references.
Thehigher the BLEU, the better is the LM in pickingout a good translation from N (y).Table 2 shows the results9 under a regular n-gram LM and the two CDLMs described in Sec-tion 5.3.The baseline LM (BLM) entails no weight op-timization a la (5) on Set3.
The CDLM with theBLM and word pentaly (WP) features improvesover the baseline LM.
Compared to either of them,the CDLM with the target-side rule bigram fea-tures (TsRB) performs dramatically better.5.5 Results on MT Test DataWe now examine how our CDLM performs duringactual MT decoding.
To incorporate the CDLMinto MT decoding, we add the log-probability (6)of a derivation d under the CDLM as an additionalbat an unrelated complication?a seemingly problematic in-stability in the minimum risk training procedure.As an illustration of this problem, we note that in super-vised tuning of the baseline MT system (|f(d)|=10) with500 sentences from Set2, the BLEU score on Set4 varies from38.6% to 44.2% to 47.8% if we use 1, 4 and 16 referencetranslations during the supervised training respectively.
Wechoose a system tuned on 16 references on Set2 as our base-line.
In order not to let the unsupervised CDLM trainingsuffer from this unrelated limitation of the tuning procedure,we give it too the benefit of being able to compute risk onSet3 using y plus its 15 paraphrases.We wish to emphasize that this trait of Set3 having 15paraphrases for each sentence is otherwise unnecessary, anddoes not detract much from the main claim of this paper.9Note that the scores in Table 2 are very low compared toscores for actual translation from Chinese shown in Table 3.This is mainly because in this monolingual simulation, theLM is the only model used to rank the y?
?
N (y).
Said dif-ferently, y?
is being chosen in Table 2 entirely for its fluencywith no consideration whatsoever for its adequacy.662LM used for Features used BLEUrescoring BLM WP TsRB on Set4Baseline LM X 12.8CDLM X X 14.2CDLM X X X 25.3Table 2: BLEU scores in monolingual simulations.
Rescor-ing the confusion sets of English sentences created using theCG shows that the CDLM with TsRB features recovers hy-potheses much closer to the sentence that generated the con-fusion set than does the baseline n-gram LM.Model used Features used BLEUfor rescoring 10 models TsRB on Set4Joshua X 48.5+ CDLM X X 49.5Table 3: BLEU scores on the test set.
The baseline MT sys-tem has ten models/features, and the proposed system hasone additional model, the CDLM.
Note that for the CDLM,only the TsRB features are used during MT decoding.feature, on top of the 10 features already presentin baseline MT system (see Section 5.2).
We then(re)tune relative weights for these 11 features onthe bilingual data Set2 of Table 1.Note that the MT system also uses the BLM andWP features whose weights are now retuned onSet2.
Therefore, when integrating a CDLM intoMT decoding, it is mathematically equivalent touse only the TsRB features of the CDLM, withthe corresponding weights as estimated alongsideits ?own?
BLM and WP features during unsuper-vised discriminative training on Set3.Table 3 reports the results.
A BLEU score im-provement of 1% is seen, reinforcing the claimthat the unsupervised CDLM helps select bettertranslations from among the system?s alternatives.5.6 Goodness of Simulated Confusion SetsThe confusion set N (y) generated by applyingthe CG to an English sentence y aims to simulatethe real confusion set that would be generated bythe MT system if the system?s input was the Chi-nese sentence whose English translation is y. Weinvestigate, in closing, how much the simulatedconfusion set resembles to the real one.
Sincewe know the actual input-output pairs (xi, yi) forSet4, we generate two confusion sets: the simu-lated set N (yi) and the real one N (xi).One way to measure the goodness of N (yi) asa proxy for N (xi), is to extract the n-gram typesn-gram Precision Recallunigram 36.5% 48.2%bigram 10.1% 12.8%trigram 3.7% 4.6%4-gram 2.0% 2.4%Table 4: n-gram precision and recall of simulated con-fusion sets relative to the true confusions when translatingChinese sentences.
The n-grams are collected from k-beststrings in both cases, with k = 100.
The precision and recallchange little when varying k.witnessed in the two sets, and compute the ratio ofthe number of n-grams in the intersection to thenumber in their union.
Another is to measure theprecision and recall of N (yi) relative to N (xi).Table 4 presents such precision and recall fig-ures.
For convenience, the n-grams are collectedfrom the 100-best strings, instead of the hyper-graph D(yi) and D(xi).
Observe that the sim-ulated confusion set does a reasonably good jobon the real unigram confusions but the simulationneeds improving for higher order n-grams.6 ConclusionsWe proposed a novel procedure to discrimina-tively train a globally normalized log-linear lan-guage model for MT, in an efficient and unsu-pervised manner.
Our method relies on the con-struction of a confusion grammar, an English-to-English SCFG that captures translation alterna-tives that an MT system may face when choosinga translation for a given input.
For each Englishtraining sentence, we use this confusion gram-mar to generate a simulated confusion set, fromwhich we train a discriminative language modelthat will prefer the original English sentence oversentences in the confusion set.
Our experimentsshow that the novel CDLM picks better alterna-tives than a regular n-gram LM from simulatedconfusion sets, and improves performance in areal Chinese-to-English translation task.7 AcknowledgementsThis work was partially supported by the NationalScience Foundation via grants No?
SGER-0840112and RI-0963898, and by the DARPA GALE pro-gram.
The authors thank Brian Roark and Dami-anos Karakos for insightful discussions.663ReferencesBannard, Colin and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In ACL?05: Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, pages597?604, Morristown, NJ, USA.
Association forComputational Linguistics.Callison-Burch, Chris, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proceedings of the mainconference on Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 17?24, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Chen, Stanley F. and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical report.Chiang, David, Kevin Knight, and Wei Wang.
2009.11,001 new features for statistical machine transla-tion.
In NAACL, pages 218?226.Chiang, David.
2007.
Hierarchical phrase-basedtranslation.
Computational Linguistics, 33(2):201?228.Eck, Matthias and Chiori Hori.
2005.
Overview of theiwslt 2005 evaluation campaign.
In In Proc.
of theInternational Workshop on Spoken Language Trans-lation.Khudanpur, Sanjeev and Jun Wu.
2000.
Maximum en-tropy techniques for exploiting syntactic, semanticand collocational dependencies in language model-ing.
In Computer Speech and Language, number 4,pages 355?372.Li, Zhifei and Jason Eisner.
2009.
First- and second-order expectation semirings with applications tominimum-risk training on translation forests.
InProceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing, pages40?51, Singapore, August.
Association for Compu-tational Linguistics.Li, Zhifei and Sanjeev Khudanpur.
2008.
Large-scalediscriminative n-gram language models for statisti-cal machine translation.
In AMTA, pages 133?142.Li, Zhifei, Chris Callison-Burch, Chris Dyer, JuriGanitkevitch, Sanjeev Khudanpur, Lane Schwartz,Wren Thornton, Jonathan Weese, and Omar.Zaidan.
2009.
Joshua: An open source toolkitfor parsing-based machine translation.
In WMT09,pages 26?30.Li, Zhifei, Ziyuan Wang, Jason Eisner, and SanjeevKhudanpur.
2010.
Minimum imputed risk trainingfor machine translation.
In review.Li, Zhifei.
2010.
Discriminative training and varia-tional decoding in machine translation via novel al-gorithms for weighted hypergraphs.
PHD Disserta-tion, Johns Hopkins University.Madnani, Nitin, Necip Fazil Ayan, Philip Resnik, andBonnie J. Dorr.
2007.
Using paraphrases for pa-rameter tuning in statistical machine translation.
InProceedings of the Workshop on Statistical MachineTranslation, Prague, Czech Republic, June.
Associ-ation for Computational Linguistics.Och, Franz Josef.
2003.
Minimum error rate train-ing in statistical machine translation.
In ACL, pages160?167.Poon, Hoifung, Colin Cherry, and Kristina Toutanova.2009.
Unsupervised morphological segmentationwith log-linear models.
In NAACL ?09: Proceed-ings of Human Language Technologies: The 2009Annual Conference of the North American Chap-ter of the Association for Computational Linguis-tics, pages 209?217, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Quirk, Chris, Chris Brockett, and William Dolan.2004.
Monolingual machine translation for para-phrase generation.
In In Proceedings of the 2004Conference on Empirical Methods in Natural Lan-guage Processing, pages 142?149.Roark, Brian, Murat Saraclar, Michael Collins, andMark Johnson.
2004.
Discriminative languagemodeling with conditional random fields and theperceptron algorithm.
In Proceedings of the 42ndMeeting of the Association for Computational Lin-guistics (ACL?04), Main Volume, pages 47?54,Barcelona, Spain, July.Rosenfeld, Roni, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language mod-els: a vehicle for linguistic-statistical integration.Computers Speech and Language, 15(1).Rosenfeld, Roni.
1996.
A maximum entropy approachto adaptive statistical language modeling.
In Com-puter Speech and Language, number 3, pages 187?228.Smith, Noah A. and Jason Eisner.
2005.
Contrastiveestimation: Training log-linear models on unlabeleddata.
In Proceedings of the Association for Compu-tational Linguistics (ACL 2005), Ann Arbor, Michi-gan.664
