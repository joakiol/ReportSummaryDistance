Japanese Dialogue Corpus of Multi-Level AnnotationThe Japanese  D iscourse  Research  In i t ia t ivehttp://www.slp.cs.rit sumei.ac.jp/dt ag/Abst ractThis paper describes a Japanesedialogue corpus annotated withmulti-level information built by theJapanese Discourse Research Initia-tive, Japanese Society for ArtificialIntelligence.
The annotation in-formation consists of speech, tran-scription delimited by slash units,prosodic, part of speech, dialogueacts and dialogue segmentation.
Inthe project, we used the corpus forobtaining new findings by examiningthe relationship between linguisticinformation and dialogue acts, thatbetween prosodic information anddialogue segment, and the charac-teristics of agreement/disagreementexpressions and non-sentence le-ments.1 In t roduct ionThis paper describes a Japanese dialogue cor-pus annotated with multi-level informationsuch as speech, linguistic and discourse infor-mation built by the Japanese Discourse Re-search Initiative, supported by Japanese So-ciety for Artificial Intelligence.Dialogue corpora are now indispensable tospeech and language research communities.
?The corpora have been used not only for ex-amining the relationship between speech andlinguistic phenomena, but also for building ?speech and language understanding systems.Sharing corpora among researchers is mostdesirable since creating the corpora needsconsiderable cost like writing and revising an-notation manuals, annotating the data, andchecking the consistency and reliability of theannotated ata.
Discourse Research Initia-tive was set up in March of 1996 by US, Eu-ropean, and Japanese researchers to developstandardized iscourse annotation schemes(Carletta et al, 1997; Core et al, 1998).The efforts of the initiative have been called'standardization', but this naming is mislead-ing at least.
In typical standardizing ef-forts, as done in audio-visual and telecom-munication technologies, commercial compa-nies try to expand the market for their prod-ucts or interfaces by the standard.
The ob-jective of standardizing efforts in discourse isto promote interactions among discourse re-searchers and thereby provide a solid founda-tion for corpus-based discourse research, dis-pensing with duplicating resource making ef-forts and increasing sharable resources.In cooperation with this initiative,Japanese Discourse Research Initiative hasstarted in Japan in May 1996, supported byJapanese Society for Artificial Intelligence(JDRI, 1996; Ichikawa et al, 1999).
Theactivities of the initiative involve:creating and revising annotation schemesbased on the survey of the existingschemes and annotation experiments,annotating corpora based on the pro-posed annotation schemes, anddoing research using the corpora not onlyfor examining the utility of the schemesand corpora but also for obtaining newfindings.ESPSP~.
~ , ChaS~Prosodic \] \ I Part-of-speech Slash unitWord alignment \] I Dialogue acts IFigure 1: The relations among the annotationinformationIn the following, a Japanese dialoguecorpus of multi-level annotation is demon-strated.
The annotation schemes deal withthe information for speech, transcription seg-mented by utterance units, called 'slashunits,' prosody, part of speech, dialogue actsand dialogue segment.
Figure 1 shows the re-lations among the annotation information.2 Speech  Sound and Transcr ip t ionThe corpus consists of a collection of 14 task-oriented ialogues, each performed by two na-tive speakers of Japanese.
The total time ofthe 14 dialogues is 53 minutes.
The tasks in-clude scheduling, route guidance, telephoneshopping, and so on.
We set the roles of thetwo speakers and the goal of the task but nopre-defined scenarios.
For example, in thescheduling task, the speakers were given theroles of a private secretary and a client, andasked to arrange a meting appointment.The speech sound of the two speakers partici-pating in a dialogue was recorded on separatechannels, which enables us to perform accu-rate acoustic/prosodic analysis even for over-lapped talks.
The transcription contains or-thographic representations in Kanji and thestarting and ending time of each utterance,where an utterance is defined as a continuousspeech region delimited by pauses of 400 msecor longer.3 P rosod ic  In fo rmat ion  andPar t -o f - speechThe prosodic information and the part-of-speech tags were assigned (semi-)automatically using the speech soundand the transcription.3.1 Prosod ic  informat ionProsody has been widely recognized as oneof the important factors which relate to dis-course structures, dialogue acts, informa-tion status, and so on.
Informative corporashould, in the first place, contain some formof prosodic information.At this stage, our corpus merely includes,as prosodic information, raw values of fun-damental frequency, voicing probability, andrms energy, which were obtained from thespeech sound using speech analysis softwareESPS/waves-b (Entropic, 1996) and simplepost-processing for smoothing.
The futurecorpus will contain more abstract descriptionsof prosodic events uch as accents and bound-ary tones.3.2 Par t -o f -speechThe part-of-speech is another basic in-formation for speech recognition, syntac-tic/semantic parsing, and dialogue processingas well as linguistic and psycholinguistic anal-ysis of spoken discourse.Part-of-speech tags were, first, obtained au-tomatically from the transcription using themorphological analysis system ChaSen (Mat-sumoto et al, 1999), and, then, correctedmanually.
The tag set was extended to coverfilled pauses and contracted forms peculiar tospontaneous speech, and some dialects.
Thetagged corpus will be used as a part of thetraining data for the statistical learning mod-ule of ChaSen to improve its performance forspontaneous speech, which can be used for fu-ture applications.3.3 Word  a l ignmentIn some applications uch as co-reference r s-olution utilizing prosodic correlates of given-new status of words, it is useful to know theprosodic information of particular words orpl~rases.
In order to obtain such informa-tion, the correspondence b tween the word se-quence and the speech sound must be given.Our corpus contains the information for thestarting and the ending time of every word.The time-stamp of each word in an ut-terance was obtained automatically from thespeech sound and the part-of-speech using theforced alignment function of speech recogni-tion software HTK (Odell et al, 1997) withthe tri-phone model for Japanese speech de-veloped by the IPA dictation software project(Shikano et al, 1998).
Apparent errors werecorrected manually with reference to soundwaveforms and spectrograms obtained anddisplayed on a screen by ESPS/waves+ (En-tropic, 1996).4 Ut terance  Un i ts4.1 S lash unitsIn the transcription, an utterance is definedas a continuous peech region delimited bypauses of 400 msec or longer.
However, thisdefinition of the utterances does not corre-spond to the units for discourse annotation.For example, the utterances are sometimesinterrupted by the partner.
For reliable dis-course annotation, analysis units must be con-structed from the utterances defined above.Following Meteer and Taylor (1995), we callsuch a unit 'slash unit.
'4.2 Cr i ter ia  for determin ing slashuni tsThe criteria for determining slash units inJapanese were defined with reference to thosefor English (Meteer and Taylor, 1995).
Theslash units were annotated mantually with ref-erence to the speech sound and transcriptionof dialogues.Single utterances as slash unit Singleutterances which can be thought o representsentences conceptually are qualified as a slashunit.
Figure 2 shows examples of slash unitsby single utterances ( lash units are delimitedby the symbol '/').In the cases where the word order is in-verted, the utterances are regarded as a slashfA: hai/ ;{response}(yes.
)A: kochira chi~ annais~utemudesu /;{a s ingle sentence}(This is the sightseeing guidesystem.
)A: ryoukin niha fukumarete orimasen gabetto 1200 en de goyoui saseteitadakimasu /;{a complex sentence}(This is not included in the charge.We offer the service forthe separate charge of 1200 yen.
)\ jFigure 2: Examples of single utterances asslash unitI shuppatsu chiten kara --(From the s tar t ing  point)--  nishi gawa ni --(to the west)--  sukoshi dake ikimasu /(move a little)Figure 3: An example of multiple utterancesas slash unitunit only if the utterances with normalizedword order are qualified as a slash unit.A sequence of one speaker's speech that ter-minates with a hesitation, an interruption anda slip of the tongue, but does not continue inthe speaker's next utterance is also qualifiedas a slash unit.Mult ip le ut terances  as s lash unit Whencollection of multiple utterances form a sen-tence, as in Figure 3, they are qualified as oneslash unit.
In slash units spanning multipleutterances, the symbol ' - - '  is marked both atthe end of the first utterance and at the startof the last utterance.4.3 Non sentence  e lementsNon sentence elements consist of 'aiduti', con-junction markers, discourse markers, fillers3fA: sukoshi dake itte /(move a little)B: ~2 /(ok)A: {D de} hidari naname shitani({D "then} I;o your lefv.
and down)Figure 4: An example of a slash unit definedby discourse markersand non speech elements, which are enclosedby {S ...}, {C ..
.
}, {D ...}, {F ..
.
}, and{N ... }, respectively.
These elements can beused to define a slash unit.
For example, when'aiduti' is expressed by the words such as "hai(yes, yeah, right)", "un (yes, yeah, right)" and"ee (mmm, yeah)" or by word repetition, it isregarded as an utterance.
Otherwise, 'aiduti'is not qualified as an independent slash unit.The main function of discourse markers isto show the relations between utterances, likestarting a new topic, changing topics, andrestarting an interrupted conversation.
Thewords such as "mazu (first, firstly)", "dewa(then, ok)", "tsumari (I mean, that meansthat)" and "sorede (and so)" may become dis-course markers when they appear at the headof the utterances.
An utterance just beforethe one with discourse markers is qualified asa slash unit (Figure 4).In the Switchboard project(Meteer andTaylor, 1995), our {S .
.
.  }
(aiduti) category isnot regarded as a separate category.
Howeverin Japanese dialogue, signals that indicate aheater's attention to speaker's utterances, areexpressed frequently.
For this reason, we cre-ated 'aiduti' as a separate category.
Other-wise {A .
.
.
}(aside), {E. .
.
}(Expl ict  editingterm), the restart and the repair are not an-notated in our scheme at the present stage.5 D ia logue  ActsIdentifying dialogue act of the slash unit isdifficult task because the mapping betweensurface form and dialogue act is not obvious.In addition, some slash units have more thanone function, e.g.
answering question withstating additional information.
Consideringabove problems, DAMSL architecture codesvarious functions at one utterance, such asforward looking function, backward lookingfunction, etc.However, it is difficult to determine thefunction of the isolated utterance.
We hadshown that assumptions ofdialogue structureand exchange structure improved agreementscore among coders (Ichikawa et al, 1999).Therefore, we define our dialogue act taggingscheme as hierarchical refinement from the ex-change structure.The annotation scheme for dialogue actsincludes a set of rules to identify the func-tion of each slash unit based on the theory ofspeech act (Searle, 1969) and discourse anal-ysis (Coulthhard, 1992; Stenstr6m, 1994).This scheme provides a basis for examiningthe local structure of dialogues./ \?
Task-oriented dialogue(Opening)Problem solving(Closing)?
Problem solvingExchange +?
ExchangeInitiation(Response)/Initiation*(Response)*(Follow-up)(Follow-up)Figure 5: Model for task-oriented dialoguesIn general, a dialogue 1 is modeled withproblem solving subdialogues, ometimes pre-ceded by opening subdialogue (e.g., greeting)and followed by closing subdialogue (e.g., ex-pressing ratitude).
A problem solving sub-dialogue consists of initiating and respondingl In this paper, we limit our attention to task-oriented ialogues, which are the main target of thestudy in computational linguistics and spoken dia-logue research.4f( In i t ia t ion)41 A: chikatetsu no ekimei ha?
(What's the name of the subwaystation?
)(Response)42 B: chikatetsu no teramachi eki ninarimasu(The name of the subway station isTeramachi.
)(Follow-up)43 A: hai(Ok.)JFigure 6: An example problem solving subdi-alogue with the exchange structureutterances, sometimes followed by followingup utterances (Figure 5).Figure 6 shows an example problem solvingsubdialogue with the exchange structure.In this scheme, dialogue acts, the elementsof the exchange structure, are classified intothe tags shown in Figure 7.6 D ia logue  St ructure  andConst ra in ts  on Mu l t ip leExchanges6.1 Dialogue SegmentIn the previous discourse model(Grosz andSidner, 1986), a discourse segment has a be-ginning and an ending utterances and mayhave smaller discourse segments in it.
It is notan easy task to identify such segments withthe nesting structure for spoken dialogues,because the structure of a dialogue is oftenvery complicated ue to the interaction of twospeakers.
In a preliminary experiment of cod-ing segments in spoken dialogues, there were alot of disagreements on the granularity or therelation of the segments and on identifyingending utterances of the segment.
An alterna-tive scheme of coding the dialogue structure(DS) is necessary to build dialogue corporaannotated with the discourse level structure.Our scheme annotates spoken dialogues/Dialogue managementOpen, CloseInitiationRequest, Suggest, Persuade, Propose,Confirm, Yes-no question, Wh-question,Promise, Demand, Inform, Other assert,Other initiate.?
ResponsePositive, Negative, Answer, Other re-sponse.?
Follow-upUnderstand?
Response with InitiationThe element of this category is repre-sented as Response type / Initiation type.JFigure 7: The classification of dialogue actswith boundary marking of the DS, instead ofidentifying a beginning and an ending utter-ance of each DS.
A building block of dialoguesegments i identified based on the exchangesexplained in Section 5.
A dialogue segment(DS) tag is inserted before initiating utter-ances because the initiating utterances can bethought of as a start of new discourse seg-ments.The DS tag consists of a topic break index(TBI), a topic name and a segment relation.TBI signifies the degree of topic dissimilaritybetween the DSs.
TBI takes the value of 1 or2: the boundary with TBI 2 is less continuousthan the one with TBI 1 with regard to thetopic.
The topic name is labeled by coders'subjective judgment.
The segment relationindicates the one between the preceding andthe following segments, which is classified intothe following categories.clarificationsuspends the exchange and makes a clar-ification in order to obtain informationnecessary to answer the partner's utter-ance;5: room for  a lec ture :  \]38 A: {F e} heya wa dou simashou ka?
(How about meeting room'?
)\[I: small-sized meeting room: clarification\]39 B: heya wa shou-kaigishitsu wa aite masu ka?
(Can I use the small-sized meeting room?
)40 h: {F to} kayoubi no {F e} 14 ji han kara wa {F e) shou-kao~itsu wa aite imasen(The small meeting room is not available from 14:30 on Tuesday.
)\ [1:the la rge-s i zed  meeting room: \]41 A: dai-kaigishitsu ga tukae masu(You can use the large meeting room.
)\[i: room for a lecture: return\]42 B: {D soreja) dai-ka~ishitsu de onegaishimasu(Ok.
Please book the large meeting room.
)Figure 8: An example dialogue with the dialogue segment tags?
interruptionstarts a different opic from the previousone during or after the partner's explana-tory utterances; and?
returngoes back to the previous topic after theclarification or the interruption.Figure 8 shows an example dialogue anno-tated with the DS tags.6.2 Constra ints  on mult ip leexchangesAnnotation of dialogue segments mostly de-pends on the coders' intuitive judgment ontopic dissimilarity between the segments.
Inorder to lighten the burden of the coders'judgment, the structural constraints on multi-ple exchanges are experimentally introduced.The constraints can be classified into twotypes: one concerns embedding exchanges(relevance type 1) and the other is neighbor-ing exchanges (relevance type 2).In relevance type 1, the relation of an initi-ating utterance and its responding utteranceis shown by attaching the id number of the ini-tiating utterance to the responding utterance.This id number can indicates non-adjacentinitiation-response pairs including embeddedexchanges inside.In relevance type 2, the structures of neigh-boring exchanges such as chaining, coupling,elliptical coupling (StenstrSm, 1994) are in-troduced.
Chaining takes the pattern of \[A:IB:R\] \[A:I B:R\] (in both exchanges, peakerA initiates an utterance and speaker B re-sponds to A).
Coupling is the pattern of \[A:IB:R\] \[B:I A:R\].
(speaker A initiates, speakerB both responds and initiates and speakerA responds to B).
Elliptical coupling is thepattern of \[A:I\] \[B:I A:R\], equivalent to theone in which B's second response is omittedin coupling.
Relevance type 2 shows whetherthe above structures of neighboring exchangescan be observed or not.
Figure 9 shows an ex-ample of annotation of relevance types 1 and2.7 Corpus  Bui ld ing  ToolsIn the experiments, various tools for tran-scription and annotation were used.
For tran-scription, the automatics segmentizer (TIME)and the online transcriber (PV) were used(Horiuchi et al, 1999).
The former lists up/<Yes-no question> <relevance no>\]27 A: hatsukano jyuuji kara ha aite irun de syou ka?
( Is the room available from lOam on the 20th?
)\[<Yes-no question> <relevance yes>\]28 B: kousyuu shitsu desu ka?
(Are you mentioning the seminar room?
)\[<Positive> <0028>\]29 A: hai(Yes.
)\[<Negative> <0027>\]30 B: hatsuka ha aite oNmasen( I t  i s  not ava i lab le  on the 20th.
)\[<Understand>\]31 A: soudesu ka(Ok.)JFigure 9: An example dialogue with relevance types 1 and 2candidates for unit utterances according tothe parameter for the length of silences.
Thelatter displays energy measurement of eachspeaker's utterance on the two windows usinga speech data file.
Users can see any part ofa dialogue using the scroll bar, and can hearspeech for both speakers or each speaker byselecting any region of the windows using amouse.For prosodic and part of speech annotation,the speech analysis software ESPS/waves+(Entropic, 1996), speech recognition softwareHTK (Odell et al, 1997) with the tri-phonemodel for Japanese speech developed by theIPA dictation software project (Shikano et al,1998) and the morphological analysis ystemChaSe, (Matsumoto et al, 1999) were used.For discourse annotation, Dialogue Anno-tation Tool (DAT) had been used in the previ-ous experiments (Core and Allen, 1997).
Al-though DAT had a consistency check betweensome functions in one sentence, we need morewide-ranging consistency check because ourscheme has assumptions of dialogue structureand exchange structure.
Therefore it is dis-satisfying but the modification of the tool toour need is not easy.
Thus, for the moment,we decided to use just a simple transcriptionviewer and sound player (TV) (Horiuchi etal., 1999), which enables us to hear the soundof utterances on the transcription.Our project does not have any intention tocreate new tools.
Rather we do want to useany existing tools if they suit or can be eas-ily modified to satisfy our needs.
The toolsof MATE project(Carletta and Isard, 1999),which also directs multi-level annotation, canbe a good candidate for our project.
In thenear future, we will examine if we can effec-tively use their tools in the project.8 Conc lus ionThis paper described a Japanese dialogue cor-pus annotated with multi-level informationbuilt by the Japanese Discourse Research Ini-tiative supported by Japanese Society for Ar-tificial Intelligence.
The annotation informa-tion includes speech, transcription delimitedby slash units, prosodic, part of speech, dia-logue acts and dialogue segmentation.
In theproject (JSAI, 2000), we used the corpus forobtaining new findings by examining:?
the relationship between linguistic infor-mation and dialogue acts?
the relationship between \]prosodic infor-mation and dialogue segment, and?
the characteristics of agree-ment/disagreement expressions andnon-sentence elements.This year we plan to quadruple the size of thecorpus and make it publicly available as soonas we finish the annotation and its verifica-tion.Re ferencesJ.
Carletta nd A. Isard.
1999.
The MATE Anno-tation Workbench: User Requirements.
In TheProceedings of the A CL'99 Workshop on To-wards Standards and Tools for Discourse Tag-ging, pages 11-17.J.
Carletta, N. Dahlback, N. Reithinger,and M. A. Walker.
1997.
Standardsfor Dialogue Coding in Natural LanguageProcessing.
f tp  ://f~p.
cs.
uni- sb.
de/pub/dagstzthl/report e/97/9706, ps.
gz.M.
Core and J. Allen.
1997.
Coding Dialogueswith the DAMSL Annotation Scheme.
In TheProceedings of AAAI Fall Symposium on Com- "Cmunicative Action in Humans and Machines,pages 28-35.M.
Core, M. Ishizaki, J. Moore, C. Nakatani,N.
Reithinger, D. Traum, and S. Tutiya.
1998.The Report of the Third Workshop of theDiscourse Research Initiative, Chiba CorpusProject.
Technical Report 3, Chiba University.M.
Coulthhard, editor.
1992.
Advances in SpokenDiscourse Analysis.
Routledge.Entropic Research Laboratory, Inc. 1996.ESPS/waves+ 5.1.1 Reference Guide.Grosz, B. J. and Sidner, C.L.
1986.
Attention, In-tentions, and the Structure of Discourse, Com-putational Linguistics, 12(3), pages 175-204.Y.
Horiuchi, Y. Nakano, H. Koiso, M. Ishizaki,H.
Suzuki, M. Okada, M. Makiko, S. Tutiya,and A. Ichikawa.
1999.
The Design and Sta-tistical Characterization f the Japanese MapTask Dialogue Corpus.
Japanese Society of Ar-tificial Intelligence, 14(2).A.
Ichikawa, M. Araki, Y.
Horiuchi., M. Ishizaki,S.
Itabashi, T. Itoh, H. Kashioka, K. Kato,H.
Kikuchi, H. Koiso, T. Kumagai, A. Kure-matsu, K. Maekawa, S. Nakazato, M. Tamoto,S.
Tutiya, Y. Yamashita, and T. Yoshimura.1999.
Evaluation of Annotation Schemes forJapanese Discourse.
In Proceedings of ACL'99Workshop on Towards Standards and Tools forDiscourse Tagging, pages 26-34.Japanese Discourse Research Initiative.
http://www.
slp.
cs.
rit sumei, ac.
jp/dt ag/.Y.
Matsumoto, A. Kitauchi, T. Yamashita,Y.
Hirano, H. Matsuda, and M. Asa-hara.
Japanese morphological nalysis sys-tem ChaSen version 2.0 manual (2nd edi-tion).
1999.
Technical Report NAIST-IS-TR99012, Graduate School of Information Sci-ence, Nara Institute of Science and Technol-ogy.
http://el, aist-nara, ac.
jp/lab/nlt/chas en/manual2/manual, pdf.M.
Meteer and A. Taylor.
1995.
Dysflu-ency Annotation Stylebook for the Switch-board Corpus.
ftp://ftp, cis.
upenn, edu/pub/treebank/swbd/do c/DFL-book, ps.
gz.Japanese Society for Artificial Intelligence.
2000.Technical Report of SIG on Spoken LanguageUnderstanding and Dialogue Processing.
SIG-SLUD-9903.J.
Odell, D. Ollason, V. Valtchev, and P. Wood-land.
1997.
The HTK Book (for HTK Ver-sion 2.1).
Cambridge UniversityJ.
R. Searle.
1969.
Speech Acts: An Essay in thePhilosopy of Language.
Cambridge UniversityPress.K.
Shikano, T. Kawahara, K. Ito, K. Takeda,A.
Yamada, T. Utsuro, T. Kobayashi, N. Mine-matsu, and M. Yamamoto.
1998.
The Devel-opment of Basic Softwares for the Dictation ofJapanese Speech: Research Report 1998.A.
B. StenstrSm.
1994.
An Introduction to SpokenInteraction.
Longman.
