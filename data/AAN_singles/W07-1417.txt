Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 101?106,Prague, June 2007. c?2007 Association for Computational LinguisticsThe Role of Sentence Structure in Recognizing Textual EntailmentCatherine BlakeSchool of Information and Library ScienceUniversity of North Carolina at Chapel HillChapel Hill, NC 27599-3360cablake@email.unc.eduAbstractRecent research suggests that sentencestructure can improve the accuracy ofrecognizing textual entailments andparaphrasing.
Although backgroundknowledge such as gazetteers, WordNetand custom built knowledge bases arealso likely to improve performance, ourgoal in this paper is to characterize thesyntactic features alone that aid inaccurate entailment prediction.
Wedescribe candidate features, the role ofmachine learning, and two final decisionrules.
These rules resulted in an accuracyof 60.50 and 65.87% and averageprecision of 58.97 and 60.96% inRTE3Test and suggest that sentencestructure alone can improve entailmentaccuracy by 9.25 to 14.62% over thebaseline majority class.1 IntroductionUnderstanding written language is a non-trivialtask.
It takes years for children to read, andambiguities of written communication remain longafter we learn the basics.
Despite these apparentcomplexities, the bag-of-words (BOW) approach,which ignores structure both within a sentence andwithin a document, continues to dominateinformation retrieval, and to some extent documentsummarization and paraphrasing and entailmentsystems.The rational behind the BOW approach is in partsimplicity (it is much easier and lesscomputationally expensive to compare terms inone sentence with terms in another, than togenerate the sentence structure); and in partaccuracy, the BOW approach continues to achievesimilar if not improved performance thaninformation retrieval systems employing deeplanguage or logical based representations.
Thisperformance is surprising when you consider that aBOW approach could not distinguish between thevery different meaning conveyed by: (1)Slowdown so that you don?t hit theriders on the road and (2)Don?tslow down so you hit the riders onthe road.
A system that employed a syntacticrepresentation of these sentences however, coulddetect that the don?t modifier applies to hit infirst sentence and to slow second.In contrast to information retrieval, researchersin paraphrase and entailment detection haveincreased their use of sentence structure.
Fewerthan half of the submissions in the firstRecognizing Textual Entailment challenge (RTE1)employed syntax (13/28, 46%) (Dagan, Glickman,& Magnini, 2005), but more than two-thirds (28/41, 68%) of the second RTE challenge (RTE2)submissions employed syntax (Bar-Haim et al,2006).
Furthermore, for the first time, the RTE2results showed that systems employing deeplanguage features, such as syntactic or logicalrepresentations of text, could outperform thepurely semantic overlap approach typified byBOW.
Earlier findings such as (Vanderwende,Coughlin, & Dolan, 2005) also suggest thatsentence structure plays an important role inrecognizing textual entailment and paraphrasingaccurately.Our goal in this paper is to explore the degree towhich sentence structure alone influences theaccuracy of entailment and paraphrase detection.101Other than a lexicon (which is used to identify thebase form of a term), our approach uses nobackground knowledge, such as WordNet (Miller,1995), extensive dictionaries (Litkowski, 2006) orcustom-built knowledge-bases (Hickl et al, 2006)that have been successfully employed by othersystems.
While such semantic knowledge shouldimprove entailment performance, we deliberatelyavoid these sources to isolate the impact ofsentence structure alone.2 System Architecture2.1 Lexical ProcessingOur approach requires an explicit representation ofstructure in both the hypothesis (HSent) and test(TSent) sentence(s).
Systems in RTE challengesemploy a variety of parsers.
In RTE2 the mostpopular sentence structure was generated byMinipar (Lin, 1998), perhaps because it is also oneof the fastest parsers.
Our system uses the typeddependency tree generated by the Stanford Parser(Klein & Manning, 2002).
A complete set of parsertags and the method used to map from aconstituent to a typed dependency grammar can befound in (de Marneffe et al, 2006).
Figure 1 showsan example typed dependency grammar for pair id355 in the RTE3Test set.2.2 LexiconOur proposed approach requires the base form ofeach term.
We considered two lexicons for thispurpose: WordNet (Miller, 1995) and theSPECIALIST lexicon (National Library ofMedicine, 2000).
The latter is part of the NationalLibrary of Medicine?s (NLM) Unified MedicalLanguage System (UMLS) and comprises termsdrawn from medical abstracts, and dictionaries,both medical and contemporary.With 412,149 entries, the SPECIALIST lexicon(version 2006AA) is substantially larger than the5,947 entries in WordNet (Version 3.0).
Tounderstand the level of overlap between thelexicons we loaded both into an oracle database.Our subsequent analysis revealed that of theWordNet entries, 5008 (84.1%) had amorphological base form in the SPECIALISTlexicon.
Of the 548 distinct entries that differedbetween the two lexicons, 389 differed becauseeither the UMLS (214 terms) or WordNet (11terms) did not have a base form.
These resultssuggest that although the NLM did not developtheir lexicon for news articles, the entries in theSPECIALIST lexicon subsumes most terms foundin the more frequently used WordNet lexicon.Thus, our system uses the base form of terms fromthe SPECIALIST lexicon.2.3 Collapsing Preposition PathsPrevious work (Lin & Pantel, 2001) suggests theutility of collapsing paths through prepositions.The type dependency does have a preposition tag,prep, however, we found that the parser typicallyassigns a more general tag, such as dep (see thedep tag in Figure 1 between wrapped and by).Instead of using the prep tag, the system collapsespaths that contain a preposition from theSPECIALIST lexicon.
For example, the systemFigure 1.
Dependency grammar tree for pair identifier 355 in the RTE3Test102collapses four paths in TSentEG millions ofvisitors, wrapped in 1995, wrapped by Christo,and wrapped began before.2.4 Base Level Sentence FeaturesThe typed dependency grammar, such as thatshown in Figure 1, can produce many differentfeatures that may indicate entailment.
Our currentimplementation uses the following four base levelfeatures.
(1) Subject: The system identifies the subject(s)of a sentence using heuristics and the parsersubject tags nsubjpass and nsubj.
(2) Object: The system uses the parser tag dobj toidentify the object(s) in each sentence.
(3) Verb: The system tags all terms linked witheither the subject or the object as a verb.
Forexample, wrapped is tagged as the verb wrapfrom the link wrapped nsubjpassReichstag shown in Figure 1.
(4) Preposition: As described in section 2.3 thesystem collapses paths that include apreposition.The subject feature had the most coverage of thebase level features and the system identified atleast one subject for 789 of the 800 hypothesessentences in RTE3Devmt.
We wrote heuristics thatuse the parser tags to identify the subject of theremaining 11 sentences.
The system found subjectsfor seven of those eight remaining hypothesissentences (3 were duplicate sentences).
In contrast,the object feature had the least coverage, with thesystem identifying objects for only 480 of the 800hypotheses in the RTE3 revised development set(RTE3Devmt).In addition to the head noun of a subject,modifying nouns can also be important torecognize entailment.
Consider the underlinedsection of TSentEG: which was later bought bythe Russian state-owned oil companyRosneft.
This sentence would lend support tohypotheses sentences that start with TheBaikalfinasgroup was bought by ?
and endwith any of the following phrases an oilcompany, a company, Rosneft, the RosneftCompany, the Rosneft oil company, a Russiancompany, a Russian Oil company, a state-owned company etc.
Our system ensures thedetection of these valid entailments by addingnoun compounds and all modifiers associated withthe subject and object term.2.5 Derived Sentence FeaturesWe reviewed previous RTE challenges and asubset of RTE3Devmt sentences before arriving atthe following derived features that build on thebase level features described in 2.4.
The featuresthat use ?opposite?
approximate the differencebetween passive and active tense.
For eachhypothesis sentence, the system records both thenumber of matches (#match), and the percentage ofmatches (%match) that are supported by the testsentence(s).
(1) Triple: The system compares the subject-verb-objects in HSent with the corresponding triplein TSent.
(2) Triple Opposite: The system matches theverbs in both HSent and TSent, but matchesthe subject in HSent with the object in TSent.
(3) Triple Subject Object: This featureapproximates the triple in (1) by comparingonly the subject and the object in HSent withTSent, but ignoring the verb.
(4) Triple Subject Object Opposite: The systemcompares the objects in HSent with thesubjects in TSent.
(5) Subject Subject: In addition to the triples usedin the derived features 1-4, the system storessubject-verb and object-verb pairs.
Thisfeature compares the distinct number ofsubjects in HSent with those in TSent.
(6) Verb Verb: The system compares only theverb in the subject-verb, object-verb tuples inHSent with those in TSent.
(7) Subject Verb: The system compares thedistinct subjects in HSent with the distinctverbs in TSent.
(8) Verb Subject: The system compares the verbin HSent with the subject in TSent.
(9) Verb Preposition: The system compares boththe preposition and verb in HSent with thosein TSent.
(10) Subject Preposition: The system comparesboth the subject and preposition in HSent withthose in TSent.
(11) Subject Word: The system compares thedistinct subjects in HSent with the distinctwords in TSent.
This is the most general of all11 derived features used in the current system1032.6 Combining FeaturesA final decision rule requires a combination of thederived features in section 2.5.
We used bothprevious RTE challenges and machine learningover the derived features to inform the finaldecision rules.
For the latter, we chose a decisiontree classifier because in addition to classificationaccuracy, we are also interested in gaining insightinto the underlying syntactic features that producethe highest predictive accuracy.The decision trees shown in Figure 2 weregenerated using the Oracle Data Miner 10.2.0.2.Tree (A) suggests that if there is less than a63.33% similarity between the number of subjectsin the hypothesis sentence and the words in any ofthe test sentences (feature 11), that the hypothesissentence is not entailed by the test sentence(s).
TheNO prediction from this rule would be correct in71% cases, and assigning NO would apply to 42%of sentences in the development set.
A YESprediction would be correct in 69% of sentences,and a YES prediction would take place in 57% ofsentences in the development set.
Tree (B) alsosuggests that an increase in the number of matchesbetween the subject in the hypothesis sentence andthe words used in the test sentence(s) is indicativeof an entailment.Decision Tree (A)If Subject-Word match <= 63.3%NO YESAccuracy=71% Accuracy=69%Coverage=43% Coverage= 57%Decision Tree (B)If Subject-Word match <= 1.5NO  YESAccuracy=58% Accuracy=64%Coverage=52%  Coverage=48%Figure 2.
Decision trees generated for the revisedRTE3Devmt set during decision rule development.Although tempting to implement the decisiontree with the highest accuracy, we should firstconsider the greedy search employed by thisalgorithm.
At each level of recursion, a decisiontree algorithm selects the single feature that bestimproves performance (in this case, the purity ofthe resulting leaves, i.e.
so that sentences in eachleaf have all YES or all NO responses).Now consider feature 1, where the subject, verband object triple in the hypothesis sentencematches the corresponding triple in a test sentence.Even though the predictive accuracy of this featureis high (74.36%), it is unlikely that this feature willprovide the best purity because only a smallnumber of sentences (39 in RTE3Devmt) match.Similarly, a subject-object match has the highestpredictive accuracy of any feature in RTE3Devmt(78.79%), but again few sentences (66 inRTE3Devm) match.2.7 Final Decision RulesWe submitted two different decision rules to RTE3based on thresholds set to optimize performance inRTE3Devmt set.
The thresholds do not consider thesource of a sentence, i.e.
from informationextraction, summarization, information retrieval orquestion answering activities.The first decision rule adds the proportion ofmatches for each of the derived features describedin section 2.5 and assigns YES when the totalproportion is greater than or equal to a threshold2.4.
Thus, the first decision rule overly favorssentences where the subject, verb and object matchboth HSent and TSent because if a sentence pairmatches on feature 1, then the system also counts amatch for features 3, 4, 5, and 8.
This lack offeature independence is intentional, and consistentwith our intuition that feature 1 is a good indicatorof entailment.To arrive at the second decision rule, weconsidered the features proposed by decision treeswith a non-greedy search strategy that favors highquality features even when only a small percentageof sentences match.
The second rule predicts YESunder the following conditions: when the subject,verb, and object of HSent match those in anyTSent (feature 1), in either order (feature 2) orwhen the subject and object from the HSent triplematch any TSent (feature 3), or when the TSentsubject matches >= 80% of the HSent subjectterms (feature 5) or when the TSent subject andpreposition matches >=70% of those in HSent(feature 10) or when TSent word matches >= 70%of the subject terms in the HSent  sentence (feature11).104RTE3Devmt RTE3Test RTE2AllFeatureTotal Pos %Accy Total Pos %Accy Total Pos %Accy1 Triple 35 26 74.29 37 24 64.86 47 35 74.472 Triple Opposite 4 3 75.00 9 4 44.44 2 1 50.003 Triple Subj Obj 66 52 78.79 76 47 61.84 102 69 67.654 Triple Subj Obj Opp.
9 4 44.44 16 7 43.75 10 5 50.005 Subject-Subject 750 397 52.93 760 404 53.16 777 391 50.326 Verb-Verb 330 196 59.39 345 181 52.46 395 208 52.667 Subject-Verb 297 178 59.93 291 168 57.73 292 154 52.748 Verb-Subject 348 196 56.32 369 207 56.10 398 212 53.279 Verb-Preposition 303 178 58.75 312 167 53.53 355 190 53.5210 Subject-Preposition 522 306 58.62 540 310 57.41 585 303 51.7911 Subject-Word 771 406 52.66 769 407 52.93 790 395 50.00Table 1.
Coverage and accuracy of each derived feature for RTE3 revised development collection(RTE3Devmt), the RTE3 Test collection (RTE3Test ) and the entire RTE2 collection (RTE2All).3 ResultsThe experiments were completed using the revisedRTE3 development set (RTE3Devmt) before theRTE3Test results were released.
The remainingRTE2 and RTE3Test analyses were then conducted.3.1 Accuracy of Derived FeaturesTable 1 shows the accuracy of any match betweenthe derived features described in section 2.5.Complete matching triples (feature 1), andmatching subjects and objects in the triple (feature2) provide the highest individual accuracy.010203040506070809010010 20 30 40 50 60 70 80 90 100Match (%)Accuracy(%)Figure 3.
Correlation between accuracy and thepercentage subjects in HSent that have acorresponding subject in TSent (feature 5).The results in Table 1 do not consider the degreeof feature match.
For example, only one of thewords from TSent in sentence 525?s (RTE3Devmt)matched the eight subject terms in correspondingHSent.
If the derived features outlined in section2.7 did capture the underlying structure of anentailment, you would expect an increased matchwould correlate with increased accuracy.
Weexplored the correlations for each of the derivedfeatures.
Figure 3 suggests entailment accuracyincreases with an increase in the percentage ofTSent subject terms that match HSent terms.
(feature 5) and demonstrates why we set the 80%threshold for feature 5 in the second decision rule.3.2 Accuracy of Decision RulesOf the 800 sentences in RTE3Devmt, the annotatorslabeled 412 as an entailment.
Thus, without anyinformation about HSent or TSent, the systemwould assign YES (the majority class) to eachsentence, which would result in 51.50% accuracy.The first decision rule considers the totalpercentage match of all features defined in section2.5.
We arrived at a threshold of 2.4 by ranking thedevelopment set in decreasing order the totalpercentage match and identifying where thethreshold would lead to an accuracy of around65%.
Many sentences had a threshold of around2.4, and the overall accuracy of the first decisionon the RTE3Devmt set was 62.38%, compared to60.50% in RTE3Test.
We consider the first decisionrule a baseline and the second rule is our realsubmission.The second rule uses only a sub-set of thederived features (1, 2, 3, 5, 10, and 11) andincludes thresholds for features 5, 10 and 11.
Theaccuracy of the second decision rule on RTE3Devmt105set was 71.50%, compared with an accuracy of65.87 % on RTE3Test.Our results are consistent with previous RTE2findings (Bar-Haim et al, 2006) where taskperformance varies with respect to the sentencesource.
Both rules had similar (poor) performancefor information extraction (50.00 vs. 50.50%).Both rules had moderate performance forsummarization (56.50 vs. 60.50%) and goodperformance for information retrieval (70.00 vs.75.50%).
The second decision rule constantly out-performed the first, with the largest increase of11.5% in the question answering activity (65.50 vs.77.00%).Both decision rules lend themselves well toranking sentences in decreasing order from themost to the least certain entailment.
Averageprecision is calculated using that ranking andproduces a perfect score when all sentence pairsthat are entailments (+ve) are listed before all thesentence pairs that are not (-ve) (Voorhees &Harman., 1999).
The average precision of the firstand second decision rules was 58.97% and 60.96%respectively.
The variation in precision also variedwith respect to the sentence source (IE, IR, QA andSUM) of 48.52, 65.93, 72.38, and 56.04% for thefirst decision rule and 48.32, 72.71, 78.75 and56.69% for the second decision rule.4 ConclusionsAlthough most systems include both syntax andsemantics to detect entailment and paraphrasing,our goal in this paper was to measure the impact ofsentence structure alone.
We developed twodecision rules that each use features from a typeddependency grammar representation the hypothesisand test sentences.
The first decision rule considersall features and the second considers only a sub-setof features, and adds thresholds to ensure that thesystem does not consider dubious matches.Thresholds for both rules were established usingsentences in RTE3Devmt only.
The second rule out-performed the first on RTE3Test, both with respectto accuracy (60.50% vs. 65.87%) and averageprecision (58.97% vs. 60.96%).These results are particularly encouraging giventhat our approach requires no backgroundknowledge (other than the lexicon) and that thiswas the first time we participated in RTE.
Theresults suggest that sentence structure alone canimprove entailment prediction by between 9.25-14.62% alone, over the majority class baseline(51.52% in RTE3Test) and they provided additionalsupport to the growing body of evidence thatsentence structure will continue to play a role inthe accurate detection of textual entailments andparaphrasing.ReferencesBar-Haim, R., Dagan, I., Dolan, B., Ferro, L.,Giampiccolo, D., Magnini, B., et al (2006).
TheSecond PASCAL Recognising Textual EntailmentChallenge.
Venice, Italy.Dagan, I., Glickman, O., & Magnini, B.
(2005).
ThePASCAL Recognising Textual Entailment Challenge.Southampton, U.K.de Marneffe, M.-C., MacCartney, B., Grenager, T., Cer,D., Rafferty, A., & Manning, C. D. (2006).
Learningto distinguish valid textual entailments, In TheSecond PASCAL Challenges Workshop onRecognising Textual Entailment, Venice, Italy.Hickl, A., Williams, J., Bensley, J., Roberts, K., Rink,B., & Shi, Y.
(2006).
Recognizing TextualEntailment with LCC?s GROUNDHOG System InThe Second PASCAL Recognising TextualEntailment Challenge, Venice, Italy.Klein, D., & Manning, C. D. (2002).
Fast ExactInference with a Factored Model for NaturalLanguage Parsing.
Paper presented at the Advancesin Neural Information Processing Systems.Lin, D. (1998).
Dependency-based Evaluation ofMINIPAR.
In Workshop on the Evaluation ofParsing Systems, First International Conference onLanguage Resources and Evaluation, Granada,Spain.Lin, D., & Pantel, P. (2001).
Induction of semanticclasses from natural language text.
In The 7thInternational conference on Knowledge discoveryand Data Mining, San Francisco, CA.Litkowski, K. (2006).
Componential Analysis forRecognizing Textual Entailment.
In The SecondPASCAL Challenges Workshop on RecognisingTextual Entailment, Venice, Italy.Miller, G. (1995).
WordNet: A Lexical Database forEnglish.
Communications of the ACM, 38(11), 39-41.National Library of Medicine.
(2000).
The SPECIALISTLexicon, fromwww.nlm.nih.gov/pubs/factsheets/umlslex.htmlVanderwende, L., Coughlin, D., & Dolan, B.
(2005).What Syntax can Contribute in Entailment Task.
ThePASCAL Challenges Workshop on RecognisingTextual Entailment.
Southhampton, UK.Voorhees, E. M., & Harman., D. (1999).
Overview ofthe seventh text retrieval conference.
In The SeventhText REtrieval Conference (TREC-7).106
