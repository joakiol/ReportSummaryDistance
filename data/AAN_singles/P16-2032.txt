Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 195?200,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsIs This Post Persuasive?Ranking Argumentative Comments in the Online ForumZhongyu Wei12, Yang Liu2and Yi Li21School of Data Science, Fudan University, Shanghai, P.R.China2Computer Science Department, The University of Texas at DallasRichardson, Texas 75080, USA{zywei,yangl,yili}@hlt.utdallas.eduAbstractIn this paper we study how to identify per-suasive posts in the online forum discus-sions, using data from Change My Viewsub-Reddit.
Our analysis confirms thatthe users?
voting score for a comment ishighly correlated with its metadata infor-mation such as published time and authorreputation.
In this work, we propose andevaluate other features to rank commentsfor their persuasive scores, including tex-tual information in the comments and so-cial interaction related features.
Our ex-periments show that the surface textualfeatures do not perform well compared tothe argumentation based features, and thesocial interaction based features are effec-tive especially when more users partici-pate in the discussion.1 IntroductionWith the popularity of online forums such as ide-bate1and convinceme2, researchers have beenpaying increasing attentions to analyzing per-suasive content, including identification of argu-ing expressions in online debates (Trabelsi andZa?ane, 2014), recognition of stance in ideolog-ical online debates (Somasundaran and Wiebe,2010; Hasan and Ng, 2014; Ranade et al, 2013b),and debate summarization (Ranade et al, 2013a).However, how to automatically determine if a textis persuasive is still an unsolved problem.Text quality and popularity evaluation has beenstudied in different domains in the past fewyears (Louis and Nenkova, 2013; Tan et al, 2014;Park et al, 2016; Guerini et al, 2015).
However,1http://idebate.org/2http://convinceme.netquality evaluation of argumentative text in the on-line forum has some unique characterisitcs.
First,persuasive text contains argument that is not com-mon in other genres.
Second, beside the text it-self, the interplay between a comment and what itresponds to is crucial.
Third, the community re-action to the comment also needs to be taken intoconsideration.In this paper, we propose several sets of featuresto capture the above mentioned characteristics forpersuasive comment identification in the online fo-rum.
We constructed a dataset from a sub-forumof Reddit3, namely change my view (CMV)4.
Wefirst analyze the corpus and show the correlationbetween the human voting score for an argumen-tative comment and its entry order and author rep-utation.
Then for the comment ranking task, wepropose three sets of features including surfacetext features, social interaction based features andargumentation based features.
Our experimentalresults show that the argumentation based featureswork the best in the early stage of the discussionand the effectiveness of social interaction featuresincreases when the number of comments in thediscussion grows.2 Dataset and Task2.1 DataOn CMV, people initiate a discussion thread witha post expressing their thoughts toward a specifictopic and other users reply with arguments fromthe opposite side in order to change the initiator?smind.
The writing quality on CVM is quite goodsince the discussions are monitored by modera-tors.
Besides commenting, users can vote on dif-ferent replies to indicate which one is more per-suasive than others.
The total amount of upvotes3https://www.reddit.com4https://www.reddit.com/r/changemyview195Thread # 1,785Comment # 374,472Comment # / Thread # 209.79Author # 32,639Unique Author # / Thread # 70.67Delta Awarded Thread # 886 (49.6%)Delta Awarded Comment # 2,056 (0.5%)Table 1: Statistics of the CMV dataset.minus the down votes is called karma, indicatingthe persuasiveness of the reply.
Users can alsogive delta to a comment if it changes their orig-inal mind about the topic.
The comment is thennamed delta awarded comment (DAC), and thethread containing a DAC is noted as delta awardedthread.We use a corpus collected from CMV.5Theoriginal corpus contains all the threads publishedbetween Jan. 2014 and Jan. 2015.
We kept thethreads with more than 100 comments to form ourexperimental dataset6.
The basic statistics of thedataset can be seen in Table 1.Figure 1a shows the distribution of the karmascores in the dataset.
We can see that the karmascore is highly skewed, similar to what is reportedin (Jaech et al, 2015).
42% of comments obtaina karma score of exactly one (i.e., no votes be-yond the author), and around 15% of commentshave a score less than one.
Figure 1b and 1c showthe correlation of the karma score with two meta-data features, author reputation7and entry order,respectively.
We can see the karma score of a com-ment is highly related to its entry order.
In gen-eral, the earlier a comment is posted, the higherkarma score it obtains.
The average score is lessthan one when it is posted after 30 comments.
Fig-ure 1c shows that authors of comments with higherkarma scores tend to have higher reputation on av-erage.2.2 TaskTan et al (2016) explored the task of mind changeby focusing on delta awarded comments usingtheir CMV data.
However, the percentage of deltaawarded comments is quite low, as shown in Ta-ble 1 (the percentage of comments obtained deltais as low as 0.5%).
In addition, a persuasive com-ment is not necessarily delta awarded.
It can be5The data was shared with us by researchers at the Uni-versity of Washington.6Please contact authors about sharing the data set.7This is the number of deltas the author has received.of high quality but does not change other people?smind.
Our research thus uses the karma scoreof a comment, instead of delta, as the referenceto represent the persuasiveness of the comment.Our analysis also shows that delta awarded com-ments generally have high karma scores (78.7% ofDACs obtain a higher karma score than the medianvalue in each delta awarded thread), indicating thekarma score is correlated with the delta value.Using karma scores as ground truth, Jaech etal.
(2015) proposed a comment ranking task onseveral sub-forums of Reddit.
In order to reducethe impact of timing, they rank each set of 10connective comments.
However, their setting isnot suitable for our task.
First, at the later stageof the discussion, comments posted connectivelyin terms of time can belong to different sub-treesof the discussion, and thus can be viewed or re-acted with great difference.
Second, as shown inFigure 1b, comments entered in later stage obtainlittle attention from audience.
This makes theirkarma scores less reliable as the ground-truth ofpersuasiveness.To further control the factor of timing, we definethe task as ranking the first-N comments in eachthread.
The final karma scores of these N com-ments are used to determine their reference rankfor evaluation.
We study two setups for this rank-ing task.
First we use information until the timepoint when the thread contains only these N com-ments.
Second we allow the system to access morecomments than N .
Our goal is to investigate if wecan predict whether a comment is persuasive andhow the community reacts to a comment in the fu-ture.3 Methods3.1 Ranking ModelA pair-wise learning-to-rank model (RankingSVM (Joachims, 2002)) is used in our task.
Wefirst construct the training set including pairs ofcomments.
In each pair, the first comment is morepersuasive than the second one.
Considering thattwo samples with similar karma scores might notbe significantly different in terms of their persua-siveness, we propose to use a modified score toform training pairs in order to improve the learn-ing efficacy.
We group comments into 7 bucketsbased on their karma scores, [-?, 0], (0, 1], (1, 5],(5, 10], (10, 20], (20, 50] and (50, +?].
We thenuse the bucket number (0 - 6) of each comment1960 10 20 30 40 50karma scorepercentage0%10%20%30%40%50%(a) Karma score distribution?40?200204060entry orderkarmascore1 5 9 13 17 21 25 29(b) Karma score vs entry order01020304050rank of karma scoreauthorreputation1 5 9 13 17 21 25 29(c) Karma score vs author reputationFigure 1: Karma value distributions in the CMV dataset.Feature Category Feature Name Feature DescriptionSurface Text Featureslength # of the words, sentences and paragraphs in c.url # of urls contained in c.unique # of words # of unique words in c.punctuation # of punctuation marks in c.unique # of POS # of unique POS tags in c.Social Interaction Featurestree size The tree size generated by c and rc.reply num The number of replies obtained by c and rc.tree height The height of the tree generated by by c and rc.Is root reply Is c a root reply of the post?Is leaf Is c a leaf of the tree generated by rc?location The position of c in the tree generated by rc.Argumentation Related Featuresconnective words Number of connective words in c.modal verbs Number of modal verbs included in c.argumentative sentence Number and percentage of argumentative sentences.argument relevance Similarity with the original post and parent comment.argument originality Maximum similarity with comments published earlier.Table 2: Feature list (c: the comment; rc: the root comment of c.)as its modified score.
We use all the formed pairsto train our ranker.
In order to be consistent, weuse the first-N comments in the training threads toconstruct the training samples to predict the rankfor the first-N comments in a test thread.3.2 FeaturesWe propose several key features that we hypoth-esize are predictive of persuasive comments.
Thefull feature list is given in Table 2.?
Surface Text Features8: In order to capture thebasic textual information, we use the commentlength and content diversity represented as thenumber of words, POS tags, URLs, and punctu-ation marks.
We also explored unigram featuresand named entity based features, but they didnot improve system performance and are thusnot included.?
Social Interaction Features: We hypothesizethat if a comment attracts more social attention8Stanford CoreNLP (Manning et al, 2014) was used topreprocess the text (i.e., comment splitting, sentence tok-enization, POS tagging and NER recognition.
).from the community, it is more likely to be per-suasive, therefore we propose several social in-teraction features to capture the community re-action to a comment.
Besides the reply tree gen-erated by the comment, we also consider the re-ply tree generated by the root comment9for fea-ture computing.
The tree size is the number ofcomments in the reply tree.
The position of c isits level in the reply tree (the level of root nodeis zero).?
Argumentation Related Features: We believea comment?s argumentation quality is a good in-dicator of its persuasiveness.
In order to cap-ture the argumentation related information, wepropose two sub-groups of features based onthe comment itself and the interplay betweenthe comment and other comments in the discus-sion.
a) Local features: we trained a binaryclassifier to classify sentences as argumentativeand non-argumentative using features proposedin (Stab and Gurevych, 2014).
We then use thenumber and percentage of argumentative sen-9It is a comment that replies to the original post directly.197Approach NDCG@1 NDCG@5 NDCG@10random 0.258 0.440 0.564author 0.382 0.567 0.664entry-order 0.460 0.600 0.689LTRtext0.372 0.558 0.658LTRsocial0.475?0.650?0.718?LTRarg0.475?0.652?0.725?LTRtext+social0.494?0.666?0.733?LTRtext+arg0.485?0.654?0.729?LTRsocial+arg0.502??0.674??0.740?LTRT+S+A0.508??0.676??0.743??LTRall0.521??0.685??0.752?
?Table 3: Performance of first-10 comments rank-ing (T+S+A: the combination of the three sets offeatures we proposed; all: the combination of twometa-data features and our features; bold: the bestperformance in each column; ?
: the approach issignificantly better than both metadata baselines(p <0.01); ?
: the approach is significantly betterthan LTR approaches using a single category offeatures (p <0.01).
).tences predicted by the classifier as features.Besides, we include some features used in theclassifier directly (i.e.
number of connectivewords10and modal verbs).
b) Interactive fea-tures: for these features, we consider the simi-larity of a comment and its parent comment, theoriginal post, and all the previously publishedcomments.
We use cosine similarity computedbased on the term frequency vector representa-tion.
Intuitively a comment needs to be relevantto the discussed topic and possibly have someoriginal convincing opinions or arguments to re-ceive a high karma score.4 Experimental ResultsWe use 5-fold cross-validation in our experiments.Normalized discounted cumulative gain (NDCG)score (Ja?rvelin and Keka?la?inen, 2000) is used asthe evaluation metric for our First-N commentsranking task.
In this study, N is10.4.1 Experiment I: Using N Comments forRankingTable 3 shows the results for first-10 commentsranking using information from only these 10comments.
As shown in Figure 1, metadata fea-tures, entry order and author?s reputation are cor-related with the karma score of a comment.
We10We constructed a list of connective words including 55entries (e.g., because, therefore etc.
).thus use these two values as baselines.
We alsoinclude the performance of the random baselinefor comparison11.
For our ranking based models(LTR?
), we compare using the three sets of fea-tures described in Section.
3.2 (noted as text, so-cial and arg respectively), individually or in com-bination.
We report NDCG scores for position 1,5 and 10 respectively.
The followings are somefindings.?
Both metadata based baselines generate signif-icantly12better results compared to the randombaseline.
Baseline entry-order performs muchbetter than author, suggesting that the entry or-der is more indicative for the karma score of acomment.?
The surface text features are least effectiveamong the three sets of features, and the per-formance using them is even worse than thetwo metadata baselines.
This might be becausethe general writing quality of the comments inCMV is high because of the policy of the forum.Therefore, the surface text features we used arenot very discriminative for comment ranking.A further analysis of features in this categoryshows that length is the most effective feature.?
Argumentation based features have the best per-formance among the three categories.
Its per-formance is significantly better than surface textfeatures, consistent with our expectation that ar-gumentation related features are useful for per-suasiveness evaluation.
Our additional experi-ments show that interactive features are moreeffective than local features.
This might be be-cause the argumentation features and modelswe use are not perfect.
Future research is stillneeded to better represent argumentation infor-mation in the text.?
When combining two categories of features,the performance of the ranker increases con-sistently.
The performance can be further im-proved by combining all the three categories offeatures we proposed (the improvement com-pared to using a single feature category is signif-icant).
The best results are achieved by LTRall,i.e., combining two metadata features and fea-tures we proposed.11The performance of random baseline is high because ofthe tie of reference karma scores.12Significance is computed by two tailed t-test.19810 20 30 40 500.660.680.700.720.740.760.780.80# of commentsNDCG@10LTRtext LTRsocial LTRarg LTRT+S+AFigure 2: Results using various number of com-ments in the thread for ranking.4.2 Experiment II: Using Varying Numbersof Comments for RankingWith the evolving discussion, there will be morecomments joining the thread providing more in-formation for social interaction based features.
Inorder to show the impact of different features atdifferent discussion stage, we conduct another ex-periment by ranking first-10 comments with vary-ing numbers of comments in the test thread for fea-ture computing.
The result of the experiment isshown in Figure 2.
The performance of LTRtextand LTRargremain the same since their featurevalues are not affected by the new coming com-ments.
The performance of LTRsocialincreasesconsistently when the number of comments grows,and it outperforms LTRargwhen the number ofcomments is more than 20.
LTRT+S+Ahas alwaysthe best performance, benefiting from the combi-nation of different types of features.5 Related WorkOur work is most related to two lines of work,including text quality evaluation and research onReddit.com.Text quality: Text quality and popularity eval-uation has been studied in different domains in thepast few years.
Louis and Nenkova (2013) imple-mented features to capture aspects of great writingin science journalism domain.
Tan et al (2014)looked into the effect of wording while predict-ing the popularity of social media content.
Park etal.
(2016) developed an interactive system to as-sist human moderators to select high quality news.Guerini et al (2015) modeled a notion of euphonyand explored the impact of sounds on differentforms of persuasiveness.
Their research focusedon the phonetic aspect instead of language usage.Reddit based research: Reddit has been usedrecently for research on social news analysisand recommendation (e.g., (Buntain and Golbeck,2014)).
Researchers also analyzed the languageuse on Reddit.
Jaech et al (2015) studied howlanguage use affects community reaction to com-ments in Reddit.
Tan et al (2016) analyzed theinteraction dynamics and persuasion strategies inCMV.6 ConclusionIn this paper, we studied the impact of differentsets of features on the identification of persuasivecomments in the online forum.
Our experiment re-sults show that argumentation based features workthe best in the early stage of the discussion, whilethe effectiveness of social interaction based fea-tures increases when the number of comments inthe thread grows.There are three major future directions for thisresearch.
First, the approach for argument mod-eling in this paper is lexical based, which limitsthe effectiveness of argumentation related featuresfor our task.
It is thus crucial to study more ef-fective ways for argument modeling.
Second, wewill explore persuasion behavior of the argumen-tative comments and study the correlation betweenthe strength of the argument and different persua-sion behaviors.
Third, we plan to automaticallyconstruct an argumentation corpus including pairsof arguments from two opposite sides of the topicfrom CMV, and use this for automatic disputingargument generation.AcknowledgmentsWe thank the anonymous reviewers for their de-tailed and insightful comments on this paper.
Thework is partially supported by DARPA ContractNo.
FA8750-13-2-0041 and AFOSR award No.FA9550-15-1-0346.
Any opinions, findings, andconclusions or recommendations expressed arethose of the authors and do not necessarily reflectthe views of the funding agencies.
We thank TrangTran, Hao Fang and Mari Ostendorf at Universityof Washington for sharing the Reddit data theycollected.199ReferencesCody Buntain and Jennifer Golbeck.
2014.
Identify-ing social roles in reddit using network structure.
InProceedings of the Companion Publication of the23rd International Conference on World Wide WebCompanion, pages 615?620.Marco Guerini, Go?zde?Ozbal, and Carlo Strapparava.2015.
Echoes of persuasion: The effect of eu-phony in persuasive communication.
arXiv preprintarXiv:1508.05817.Kazi Saidul Hasan and Vincent Ng.
2014.
Why areyou taking this stance?
identifying and classifyingreasons in ideological debates.
In Proceedings of theConference on Empirical Methods in Natural Lan-guage Processing, pages 751?762.Aaron Jaech, Vicky Zayats, Hao Fang, Mari Osten-dorf, and Hannaneh Hajishirzi.
2015.
Talking tothe crowd: What do people react to in online discus-sions?
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages2026?2031.Kalervo Ja?rvelin and Jaana Keka?la?inen.
2000.
IR eval-uation methods for retrieving highly relevant docu-ments.
In Proceedings of the 23rd annual interna-tional ACM SIGIR conference on Research and de-velopment in information retrieval, pages 41?48.Thorsten Joachims.
2002.
Optimizing search en-gines using clickthrough data.
In Proceedings of theEighth ACM SIGKDD International Conference onKnowledge Discovery and Data Mining, pages 133?142.Annie Louis and Ani Nenkova.
2013.
What makeswriting great?
first experiments on article qualityprediction in the science journalism domain.
Trans-actions of the Association for Computational Lin-guistics, 1:341?352.Christopher D. Manning, Mihai Surdeanu, John Bauer,Jenny Finkel, Steven J. Bethard, and David Mc-Closky.
2014.
The Stanford CoreNLP natural lan-guage processing toolkit.
In Proceedings of theAssociation for Computational Linguistics SystemDemonstrations, pages 55?60.Deokgun Park, Simranjit Sachar, Nicholas Diakopou-los, and Niklas Elmqvist.
2016.
Supporting com-ment moderators in identifying high quality onlinenews comments.
In Proceedings of the 2016 CHIConference on Human Factors in Computing Sys-tems.Sarvesh Ranade, Jayant Gupta, Vasudeva Varma, andRadhika Mamidi.
2013a.
Online debate summa-rization using topic directed sentiment analysis.
InProceedings of the Second International Workshopon Issues of Sentiment Discovery and Opinion Min-ing, page 7.Sarvesh Ranade, Rajeev Sangal, and Radhika Mamidi.2013b.
Stance classification in online debates byrecognizing users?
intentions.
In Proceedings ofSpecial Interest Group on Discourse and Dialogue,pages 61?69.Swapna Somasundaran and Janyce Wiebe.
2010.
Rec-ognizing stances in ideological on-line debates.
InProceedings of the NAACL HLT 2010 Workshop onComputational Approaches to Analysis and Genera-tion of Emotion in Text, pages 116?124.Christian Stab and Iryna Gurevych.
2014.
Identifyingargumentative discourse structures in persuasive es-says.
In Proceedings of the Conference on Empiri-cal Methods in Natural Language Processing, pages46?56.Chenhao Tan, Lillian Lee, and Bo Pang.
2014.
Theeffect of wording on message propagation: Topic-and author-controlled natural experiments on twitter.arXiv preprint arXiv:1405.1438.Chenhao Tan, Vlad Niculae, Cristian Danescu-Niculescu-Mizil, and Lillian Lee.
2016.
Winningarguments: Interaction dynamics and persuasionstrategies in good-faith online discussions.
arXivpreprint arXiv:1602.01103.Amine Trabelsi and Osmar R Za?ane.
2014.
Findingarguing expressions of divergent viewpoints in on-line debates.
In Proceedings of the 5th Workshopon Language Analysis for Social Media (LASM)@EACL, pages 35?43.200
