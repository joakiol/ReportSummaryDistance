Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 161?167,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsGeneralized Character-Level Spelling Error CorrectionNoura Farra, Nadi Tomeh?, Alla Rozovskaya, Nizar HabashCenter for Computational Learning Systems, Columbia University{noura,alla,habash}@ccls.columbia.edu?LIPN, Universit?
Paris 13, Sorbonne Paris Cit?nadi.tomeh@lipn.univ-paris13.frAbstractWe present a generalized discrimina-tive model for spelling error correctionwhich targets character-level transforma-tions.
While operating at the charac-ter level, the model makes use of word-level and contextual information.
In con-trast to previous work, the proposed ap-proach learns to correct a variety of er-ror types without guidance of manually-selected constraints or language-specificfeatures.
We apply the model to cor-rect errors in Egyptian Arabic dialect text,achieving 65% reduction in word errorrate over the input baseline, and improv-ing over the earlier state-of-the-art system.1 IntroductionSpelling error correction is a longstanding NaturalLanguage Processing (NLP) problem, and it hasrecently become especially relevant because of themany potential applications to the large amountof informal and unedited text generated online,including web forums, tweets, blogs, and email.Misspellings in such text can lead to increasedsparsity and errors, posing a challenge for manyNLP applications such as text summarization, sen-timent analysis and machine translation.In this work, we present GSEC, a Generalizedcharacter-level Spelling Error Correction model,which uses supervised learning to map input char-acters into output characters in context.
The ap-proach has the following characteristics:Character-level Corrections are learned at thecharacter-level1using a supervised sequence la-beling approach.Generalized The input space consists of allcharacters, and a single classifier is used to learn1We use the term ?character?
strictly in the alphabeticsense, not the logographic sense (as in the Chinese script).common error patterns over all the training data,without guidance of specific rules.Context-sensitive The model looks beyond thecontext of the current word, when making a deci-sion at the character-level.Discriminative The model provides the free-dom of adding a number of different features,which may or may not be language-specific.Language-Independent In this work, we in-tegrate only language-independent features, andtherefore do not consider morphological or lin-guistic features.
However, we apply the modelto correct errors in Egyptian Arabic dialect text,following a conventional orthography standard,CODA (Habash et al, 2012).Using the described approach, we demonstratea word-error-rate (WER) reduction of 65% over ado-nothing input baseline, and we improve overa state-of-the-art system (Eskander et al, 2013)which relies heavily on language-specific andmanually-selected constraints.
We present a de-tailed analysis of mistakes and demonstrate thatthe proposed model indeed learns to correct awider variety of errors.2 Related WorkMost earlier work on automatic error correctionaddressed spelling errors in English and built mod-els of correct usage on native English data (Ku-kich, 1992; Golding and Roth, 1999; Carlsonand Fette, 2007; Banko and Brill, 2001).
Ara-bic spelling correction has also received consider-able interest (Ben Othmane Zribi and Ben Ahmed,2003; Haddad and Yaseen, 2007; Hassan et al,2008; Shaalan et al, 2010; Alkanhal et al, 2012;Eskander et al, 2013; Zaghouani et al, 2014).Supervised spelling correction approachestrained on paired examples of errors and their cor-rections have recently been applied for non-nativeEnglish correction (van Delden et al, 2004; Li etal., 2012; Gamon, 2010; Dahlmeier and Ng, 2012;161Rozovskaya and Roth, 2011).
Discriminativemodels have been proposed at the word-level forerror correction (Duan et al, 2012) and for errordetection (Habash and Roth, 2011).In addition, there has been growing work on lex-ical normalization of social media data, a some-what related problem to that considered in this pa-per (Han and Baldwin, 2011; Han et al, 2013;Subramaniam et al, 2009; Ling et al, 2013).The work of Eskander et al (2013) is themost relevant to the present study: it presentsa character-edit classification model (CEC) usingthe same dataset we use in this paper.2Eskan-der et al (2013) analyzed the data to identify theseven most common types of errors.
They devel-oped seven classifiers and applied them to the datain succession.
This makes the approach tailored tothe specific data set in use and limited to a specificset of errors.
In this work, a single model is con-sidered for all types of errors.
The model consid-ers every character in the input text for a possiblespelling error, as opposed to looking only at cer-tain input characters and contexts in which theyappear.
Moreover, in contrast to Eskander et al(2013), it looks beyond the boundary of the cur-rent word.3 The GSEC Approach3.1 Modeling Spelling Correction at theCharacter LevelWe recast the problem of spelling correction intoa sequence labeling problem, where for each inputcharacter, we predict an action label describinghow to transform it to obtain the correct charac-ter.
The proposed model therefore transforms agiven input sentence e = e1, .
.
.
, enof n char-acters that possibly include errors, to a correctedsentence c of m characters, where corrected char-acters are produced by one of the following fouractions applied to each input character ei:?
ok: eiis passed without transformation.?
substitute ?
with(c): eiis substituted witha character c where c could be any characterencountered in the training data.?
delete: eiis deleted.?
insert(c): A character c is inserted beforeei.
To address errors occurring at the end2Eskander et al (2013) also considered a slower, moreexpensive, and more language-specific method using a mor-phological tagger (Habash et al, 2013) that outperformed theCEC model; however, we do not compare to it in this paper.Input Action Labelk substitute-with(c)o okr insert(r)e okc okt okd deleteTable 1: Character-level spelling error correction processon the input word korectd, with the reference word correctTrain Dev TestSentences 10.3K 1.67K 1.73KCharacters 675K 106K 103KWords 134K 21.1K 20.6KTable 2: ARZ Egyptian dialect corpus statisticsof the sentence, we assume the presence of adummy sentence-final stop character.We use a multi-class SVM classifier to predict theaction labels for each input character ei?
e. Adecoding process is then applied to transform theinput characters accordingly to produce the cor-rected sentence.
Note that we consider the spacecharacter as a character like any other, which givesus the ability to correct word merge errors withspace character insertion actions and word split er-rors with space character deletion actions.
Table 1shows an example of the spelling correction pro-cess.In this paper, we only model single-edit actionsand ignore cases where a character requires mul-tiple edits (henceforth, complex actions), such asmultiple insertions or a combination of insertionsand substitutions.
This choice was motivated bythe need to reduce the number of output labels, asmany infrequent labels are generated by complexactions.
An error analysis of the training data, de-scribed in detail in section 3.2, showed that com-plex errors are relatively infrequent (4% of data).We plan to address these errors in future work.Finally, in order to generate the training datain the described form, we require a parallel cor-pus of erroneous and corrected reference text (de-scribed below), which we align at the characterlevel.
We use the alignment tool Sclite (Fiscus,1998), which is part of the SCTK Toolkit.3.2 Description of DataWe apply our model to correcting Egyptian Ara-bic dialect text.
Since there is no standard dialectorthography adopted by native speakers of Ara-bic dialects, it is common to encounter multiple162Action % Errors Example Error?
ReferenceSubstitute 80.9EAlif A @ forms ( @/@/ @/@A?/?A/?A) 33.3 AHdhm?
?Hdhm ??Yg@?
?
?Yg@EYa ?/?
forms ( y/?)
26.7 ?ly?
?l???????
?Eh/~ ?/?
, h/w ?/?
forms 14.9 kfrh?
kfr~ ?Q????Q?
?Eh/H ?/h forms 2.2 ht?mlhA?
Ht?mlhA A????J??
A???
?JkOther substitutions 3.8 AltAny~?
Al?Any~?JK AJ?
@??JK AJ?
@ ; dA?
dh @X?
?XInsert 10.5EPInsert {A} 3.0 ktbw?
ktbwA ?J.J??
@?J.J?EPInsert {space} 2.9 mAtz?l??
mA tz?l????QKA????
?QK A?Other insertion actions 4.4 Aly?
Ally ??@????
@Delete 4.7EDel{A} 2.4 whmA?
whm A????
??
?Other deletion actions 2.3 wfyh?
wfy ?J???
??
?Complex 4.0 mykwn??
mA ykwn????J????
?KA?Table 3: Character-level distribution of correction labels.
We model all types of transformations except complex actions, andrare Insert labels with counts below a tuned threshold.
The Delete label is a single label that comprises all deletion actions.Labels modeled by Eskander et al (2013) are marked withE, andEPfor cases modeled partially, for example, the Insert{A}would only be applied at certain positions such as the end of the word.spellings of the same word.
The CODA orthogra-phy was proposed by Habash et al (2012) in anattempt to standardize dialectal writing, and weuse it as a reference of correct text for spellingcorrection following the previous work by Eskan-der et al (2013).
We use the same corpus (la-beled "ARZ") and experimental setup splits usedby them.
The ARZ corpus was developed bythe Linguistic Data Consortium (Maamouri et al,2012a-e).
See Table 2 for corpus statistics.Error Distribution Table 3 presents the distri-bution of correction action labels that correspondto spelling errors in the training data together withexamples of these errors.3We group the ac-tions into: Substitute, Insert, Delete, and Complex,and also list common transformations within eachgroup.
We further distinguish between the phe-nomena modeled by our system and by Eskanderet al (2013).
At least 10% of all generated actionlabels are not handled by Eskander et al (2013).3.3 FeaturesEach input character is represented by a featurevector.
We include a set of basic features inspiredby Eskander et al (2013) in their CEC system andadditional features for further improvement.Basic features We use a set of nine basic fea-tures: the given character, the preceding and fol-lowing two characters, and the first two and last3Arabic transliteration is presented in the Habash-Soudi-Buckwalter scheme (Habash et al, 2007).
For more informa-tion on Arabic orthography in NLP, see (Habash, 2010).two characters in the word.
These are the samefeatures used by CEC, except that CEC doesnot include characters beyond the word boundary,while we consider space characters as well as char-acters from the previous and next words.Ngram features We extract sequences of char-acters corresponding to the current character andthe following and previous two, three, or fourcharacters.
We refer to these sequences as bi-grams, trigrams, or 4-grams, respectively.
Theseare an extension of the basic features and allowthe model to look beyond the context of the cur-rent word.3.4 Maximum Likelihood Estimate (MLE)We implemented another approach for error cor-rection based on a word-level maximum likeli-hood model.
The MLE method uses a unigrammodel which replaces each input word with itsmost likely correct word based on counts from thetraining data.
The intuition behind MLE is that itcan easily correct frequent errors; however, it isquite dependent on the training data.4 Experiments4.1 Model EvaluationSetup The training data was extracted to gener-ate the form described in Section 3.1, using theSclite tool (Fiscus, 1998) to align the input andreference sentences.
A speech effect handling stepwas applied as a preprocessing step to all models.163This step removes redundant repetitions of charac-ters in sequence, e.g.,QJJJ?
ktyyyyyr ?veeeeery?.The same speech effect handling was applied byEskander et al (2013).For classification, we used the SVM implemen-tation in YamCha (Kudo and Matsumoto, 2001),and trained with different variations of the fea-tures described above.
Default parameters wereselected for training (c=1, quadratic kernel, andcontext window of +/- 2).In all results listed below, the baseline corre-sponds to the do-nothing baseline of the input text.Metrics Three evaluation metrics are used.
Theword-error-rate WER metric is computed by sum-ming the total number of word-level substitutionerrors, insertion errors, and deletion errors in theoutput, and dividing by the number of words in thereference.
The correct-rate Corr metric is com-puted by dividing the number of correct outputwords by the total number of words in the refer-ence.
These two metrics are produced by Sclite(Fiscus, 1998), using automatic alignment.
Fi-nally, the accuracy Acc metric, used by Eskanderet al (2013), is a simple string matching metricwhich enforces a word alignment that pairs wordsin the reference to those of the output.
It is cal-culated by dividing the number of correct outputwords by the number of words in the input.
Thismetric assumes no split errors in the data (a wordincorrectly split into two words), which is the casein the data we are working with.Character-level Model Evaluation The per-formance of the generalized spelling correctionmodel (GSEC) on the dev data is presented in thefirst half of Table 4.
The results of the Eskan-der et al (2013) CEC system are also presentedfor the purpose of comparison.
We can see thatusing a single classifier, the generalized model isable to outperform CEC, which relies on a cascadeof classifiers (p = 0.03 for the basic model andp < 0.0001 for the best model, GSEC+4grams).4Model Combination Evaluation Here wepresent results on combining GSEC with theMLE component (GSEC+MLE).
We combine thetwo models in cascade: the MLE component isapplied to the output of GSEC.
To train the MLEmodel, we use the word pairs obtained from theoriginal training data, rather than from the outputof GSEC.
We found that this configuration allows4Significance results are obtained using McNemar?s test.Approach Corr%/WER Acc%Baseline 75.9/24.2 76.8CEC 88.7/11.4 90.0GSEC 89.7/10.4* 90.3*GSEC+2grams 90.6/9.5* 91.2*GSEC+4grams 91.0/9.2* 91.6*MLE 89.7/10.4 90.5CEC + MLE 90.8/9.4 91.5GSEC+MLE 91.0/9.2 91.3GSEC+4grams+ MLE 91.7/8.3* 92.2*Table 4: Model Evaluation.
GSEC represents the gener-alized character-level model.
CEC represents the character-level-edit classification model of Eskander et al (2013).Rows marked with an asterisk (*) are statistically signifi-cant compared to CEC (for the first half of the table) orCEC+MLE (for the second half of the table), with p < 0.05.us to include a larger sample of word pair errorsfor learning, because our model corrects manyerrors, leaving fewer example pairs to train anMLE post-processor.
The results are shown in thesecond half of Table 4.We first observe that MLE improves the per-formance of both CEC and GSEC.
In fact,CEC+MLE and GSEC+MLE perform similarly(p = 0.36, not statistically significant).
Whenadding features that go beyond the word bound-ary, we achieve an improvement over MLE,GSEC+MLE, and CEC+MLE, all of which aremostly restricted within the boundary of the word.The best GSEC model outperforms CEC+MLE(p < 0.0001), achieving a WER of 8.3%, corre-sponding to 65% reduction compared to the base-line.
It is worth noting that adding the MLE com-ponent allows Eskander?s CEC to recover varioustypes of errors that were not modeled previously.However, the contribution of MLE is limited towords that are in the training data.
On the otherhand, because GSEC is trained on character trans-formations, it is likely to generalize better to wordsunseen in the training data.Results on Test Data Table 5 presents the re-sults of our best model (GSEC+4grams), and bestmodel+MLE.
The latter achieves a 92.1% Accscore.
The Acc score reported by Eskander et al(2013) for CEC+MLE is 91.3% .
The two resultsare statistically significant (p < 0.0001) with re-spect to CEC and CEC+MLE respectively.Approach Corr%/WER Acc%Baseline 74.5/25.5 75.5GSEC+4grams 90.9/9.1 91.5GSEC+4grams+ MLE 91.8/8.3 92.1Table 5: Evaluation on test data.1644.2 Error AnalysisTo gain a better understanding of the performanceof the models on different types of errors and theirinteraction with the MLE component, we separatethe words in the dev data into: (1) words seen inthe training data, or in-vocabulary words (IV), and(2) out-of-vocabulary (OOV) words not seen inthe training data.
Because the MLE model mapsevery input word to its most likely gold word seenin the training data, we expect the MLE compo-nent to recover a large portion of errors in the IVcategory (but not all, since an input word can havemultiple correct readings depending on the con-text).
On the other hand, the recovery of errors inOOV words indicates how well the character-levelmodel is doing independently of the MLE compo-nent.
Table 6 presents the performance, using theAcc metric, on each of these types of words.
Hereour best model (GSEC+4grams) is considered.#Inp Words Baseline CEC+MLE GSEC+MLEOOV 3,289 (17.2%) 70.7 76.5 80.5IV 15,832 (82.8%) 78.6 94.6 94.6Total 19,121 (100%) 77.2 91.5 92.2Table 6: Accuracy of character-level models shown sepa-rately on out-of-vocabulary and in-vocabulary words.When considering words seen in the trainingdata, CEC and GSEC have the same performance.However, when considering OOV words, GSECperforms significantly better (p < 0.0001), veri-fying our hypothesis that a generalized model re-duces dependency on training data.
The data isheavily skewed towards IV words (83%), whichexplains the generally high performance of MLE.We performed a manual error analysis on a sam-ple of 50 word errors from the IV set and foundthat all of the errors came from gold annotation er-rors and inconsistencies, either in the dev or train.We then divided the character transformations inthe OOV words into four groups: (1) charactersthat were unchanged by the gold (X-X transforma-tions), (2) character transformations modeled byCEC (X-Y CEC), (3) character transformations notmodeled by CEC, and which include all phenom-ena that were only partially modeled by CEC (X-Ynot CEC), and (4) complex errors.
The character-level accuracy on each of these groups is shown inTable 7.Both CEC and GSEC do much better on thesecond group of character transformations (thatis, X-Y CEC) than on the third group (X-Y notCEC).
This is not surprising because the formerType #Chars Example CEC GSECX-X 16502 m-m, space-space 99.25 99.33X-Y 609 ~-h, h-~,?A-A 80.62 83.09(CEC) A-?A, y-?X-Y 161 t-?
, del{w} 31.68 43.48(not CEC) n-ins{space}Complex 32 n-ins{A}{m} 37.5 15.63Table 7: Character-level accuracy on different transforma-tion types for out-of-vocabulary words.
For complex trans-formations, the accuracy represents the complex categoryrecognition rate, and not the actual correction accuracy.transformations correspond to phenomena that aremost common in the training data.
For GSEC,they are learned automatically, while for CEC theyare selected and modeled explicitly.
Despite thisfact, GSEC generalizes better to OOV words.
Asfor the third group, both CEC and GSEC per-form more poorly, but GSEC corrects more errors(43.48% vs. 31.68% accuracy).
Finally, CEC isbetter at recognizing complex errors, which, al-though are not modeled explicitly by CEC, cansometimes be corrected as a result of applyingmultiple classifiers in cascade.
Dealing with com-plex errors, though there are few of them in thisdataset, is an important direction for future work,and for generalizing to other datasets, e.g., (Za-ghouani et al, 2014).5 ConclusionsWe showed that a generalized character-levelspelling error correction model can improvespelling error correction on Egyptian Arabic data.This model learns common spelling error patternsautomatically, without guidance of manually se-lected or language-specific constraints.
We alsodemonstrate that the model outperforms existingmethods, especially on out-of-vocabulary words.In the future, we plan to extend the model to useword-level language models to select between topcharacter predictions in the output.
We also planto apply the model to different datasets and differ-ent languages.
Finally, we plan to experiment withmore features that can also be tailored to specificlanguages by using morphological and linguisticinformation, which was not explored in this paper.AcknowledgmentsThis publication was made possible by grantNPRP-4-1058-1-168 from the Qatar National Re-search Fund (a member of the Qatar Foundation).The statements made herein are solely the respon-sibility of the authors.165ReferencesMohamed I. Alkanhal, Mohammed A. Al-Badrashiny,Mansour M. Alghamdi, and Abdulaziz O. Al-Qabbany.
2012.
Automatic Stochastic ArabicSpelling Correction With Emphasis on Space Inser-tions and Deletions.
IEEE Transactions on Audio,Speech & Language Processing, 20:2111?2122.Michele Banko and Eric Brill.
2001.
Scaling to veryvery large corpora for natural language disambigua-tion.
In Proceedings of 39th Annual Meeting of theAssociation for Computational Linguistics, pages26?33, Toulouse, France, July.Chiraz Ben Othmane Zribi and Mohammed BenAhmed.
2003.
Efficient Automatic Correctionof Misspelled Arabic Words Based on ContextualInformation.
In Proceedings of the Knowledge-Based Intelligent Information and Engineering Sys-tems Conference, Oxford, UK.Andrew Carlson and Ian Fette.
2007.
Memory-basedcontext-sensitive spelling correction at web scale.
InProceedings of the IEEE International Conferenceon Machine Learning and Applications (ICMLA).Daniel Dahlmeier and Hwee Tou Ng.
2012.
A beam-search decoder for grammatical error correction.
InProceedings of the 2012 Joint Conference on Empir-ical Methods in Natural Language Processing andComputational Natural Language Learning, pages568?578.Huizhong Duan, Yanen Li, ChengXiang Zhai, andDan Roth.
2012.
A discriminative model forquery spelling correction with latent structural svm.In Proceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning,EMNLP-CoNLL ?12, pages 1511?1521, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Ramy Eskander, Nizar Habash, Owen Rambow, andNadi Tomeh.
2013.
Processing spontaneous orthog-raphy.
In The Conference of the North AmericanChapter of the Association for Computational Lin-guistics: Human Language Technologies, NAACLHLT ?13.Jon Fiscus.
1998.
Sclite scoring package ver-sion 1.5.
US National Institute of StandardTechnology (NIST), URL http://www.
itl.
nist.gov/iaui/894.01/tools.Michael Gamon.
2010.
Using mostly native data tocorrect errors in learners?
writing.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 163?171, LosAngeles, California, June.Andrew R. Golding and Dan Roth.
1999.
A Winnowbased approach to context-sensitive spelling correc-tion.
Machine Learning, 34(1-3):107?130.Nizar Habash and Ryan M. Roth.
2011.
Using deepmorphology to improve automatic error detection inarabic handwriting recognition.
In Proceedings ofthe 49th Annual Meeting of the Association for Com-putational Linguistics: Human Language Technolo-gies - Volume 1, HLT ?11, pages 875?884, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Nizar Habash, Abdelhadi Soudi, and Tim Buckwalter.2007.
On Arabic Transliteration.
In A. van denBosch and A. Soudi, editors, Arabic Computa-tional Morphology: Knowledge-based and Empiri-cal Methods.
Springer.Nizar Habash, Mona Diab, and Owen Rambow.2012.
Conventional orthography for dialectal Ara-bic.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Thierry Declerck, Mehmet U?gurDo?gan, Bente Maegaard, Joseph Mariani, JanOdijk, and Stelios Piperidis, editors, Proceedingsof the Eight International Conference on LanguageResources and Evaluation (LREC?12), Istanbul,Turkey, may.
European Language Resources Asso-ciation (ELRA).Nizar Habash, Ryan Roth, Owen Rambow, Ramy Es-kander, and Nadi Tomeh.
2013.
MorphologicalAnalysis and Disambiguation for Dialectal Arabic.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies(NAACL-HLT), Atlanta, GA.Nizar Habash.
2010.
Introduction to Arabic NaturalLanguage Processing.
Morgan & Claypool Publish-ers.Bassam Haddad and Mustafa Yaseen.
2007.
Detectionand Correction of Non-Words in Arabic: A HybridApproach.
International Journal of Computer Pro-cessing Of Languages (IJCPOL).Bo Han and Timothy Baldwin.
2011.
Lexical normali-sation of short text messages: Makn sens a# twitter.In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies-Volume 1, pages 368?378.Association for Computational Linguistics.Bo Han, Paul Cook, and Timothy Baldwin.
2013.Lexical normalization for social media text.
ACMTransactions on Intelligent Systems and Technology(TIST), 4(1):5.Ahmed Hassan, Sara Noeman, and Hany Hassan.2008.
Language Independent Text Correction us-ing Finite State Automata.
In Proceedings of the In-ternational Joint Conference on Natural LanguageProcessing (IJCNLP 2008).Taku Kudo and Yuji Matsumoto.
2001.
Chunkingwith support vector machines.
In Proceedings ofthe second meeting of the North American Chap-ter of the Association for Computational Linguisticson Language technologies, NAACL ?01, pages 1?8, Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.Karen Kukich.
1992.
Techniques for AutomaticallyCorrecting Words in Text.
ACM Computing Sur-veys, 24(4).166Yanen Li, Huizhong Duan, and ChengXiang Zhai.2012.
A generalized hidden markov model with dis-criminative training for query spelling correction.
InProceedings of the 35th international ACM SIGIRconference on Research and development in infor-mation retrieval, SIGIR ?12, pages 611?620, NewYork, NY, USA.
ACM.Wang Ling, Chris Dyer, Alan W Black, and IsabelTrancoso.
2013.
Paraphrasing 4 microblog normal-ization.
In Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Process-ing, pages 73?84, Seattle, Washington, USA, Octo-ber.
Association for Computational Linguistics.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012a.Egyptian Arabic Treebank DF Part 1 V2.0.
LDCcatalog number LDC2012E93.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012b.Egyptian Arabic Treebank DF Part 2 V2.0.
LDCcatalog number LDC2012E98.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012c.Egyptian Arabic Treebank DF Part 3 V2.0.
LDCcatalog number LDC2012E89.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012d.Egyptian Arabic Treebank DF Part 4 V2.0.
LDCcatalog number LDC2012E99.Mohamed Maamouri, Ann Bies, Seth Kulick, SondosKrouna, Dalila Tabassi, and Michael Ciul.
2012e.Egyptian Arabic Treebank DF Part 5 V2.0.
LDCcatalog number LDC2012E107.Alla Rozovskaya and Dan Roth.
2011.
Algorithm se-lection and model adaptation for esl correction tasks.In Proc.
of the Annual Meeting of the Association ofComputational Linguistics (ACL), Portland, Oregon,6.
Association for Computational Linguistics.Khaled Shaalan, Rana Aref, and Aly Fahmy.
2010.
Anapproach for analyzing and correcting spelling er-rors for non-native Arabic learners.
Proceedings ofInformatics and Systems (INFOS).L Venkata Subramaniam, Shourya Roy, Tanveer AFaruquie, and Sumit Negi.
2009.
A survey of typesof text noise and techniques to handle noisy text.In Proceedings of The Third Workshop on Analyticsfor Noisy Unstructured Text Data, pages 115?122.ACM.Sebastian van Delden, David B. Bracewell, and Fer-nando Gomez.
2004.
Supervised and unsupervisedautomatic spelling correction algorithms.
In Infor-mation Reuse and Integration, 2004.
Proceedings ofthe 2004 IEEE International Conference on, pages530?535.Wajdi Zaghouani, Behrang Mohit, Nizar Habash, Os-sama Obeid, Nadi Tomeh, Alla Rozovskaya, NouraFarra, Sarah Alkuhlani, and Kemal Oflazer.
2014.Large scale Arabic error annotation: Guidelines andframework.
In Proceedings of the 9th edition of theLanguage Resources and Evaluation Conference.167
