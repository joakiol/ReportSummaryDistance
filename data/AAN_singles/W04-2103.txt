Linguistic Preprocessing for Distributional Classification of WordsViktor PEKARCLG, University of WolverhamptonMB 114, Stafford RoadWolverhampton, UK, WV1 1SBv.pekar@wlv.ac.ukAbstractThe paper is concerned with automaticclassification of new lexical items intosynonymic sets on the basis of their co-occurrence data obtained from a corpus.
Ourgoal is to examine the impact that differenttypes of linguistic preprocessing of the co-occurrence material have on the classificationaccuracy.
The paper comparatively studiesseveral preprocessing techniques frequentlyused for this and similar tasks and makesconclusions about their relative merits.
Wefind that a carefully chosen preprocessingprocedure achieves a relative effectivenessimprovement of up to 88% depending on theclassification method in comparison to thewindow-based context delineation, along withusing much smaller feature space.1 IntroductionWith the fast development of text miningtechnologies, automated management of lexicalresources is presently an important research issue.A particular text mining task often requires alexical database (e.g., a thesaurus, dictionary, or aterminology) with a specific size, topic coverage,and granularity of encoded meaning.
That is why alot of recent NLP and AI research has beenfocusing on finding ways to speedily build orextend a lexical resource ad hoc for an application.One attractive idea to address this problem is toelicit the meanings of new words automaticallyfrom a corpus relevant to the application domain.To do this, many approaches to lexical acquisitionemploy the distributional model of word meaninginduced from the distribution of the word acrossvarious lexical contexts of its occurrence found inthe corpus.
The approach is now being activelyexplored for a wide range of semantics-relatedtasks including automatic construction of thesauri(Lin, 1998; Caraballo, 1999), their enrichment(Alfonseca and Manandhar, 2002; Pekar and Staab,2002), acquisition of bilingual lexica from non-aligned (Kay and R?scheisen, 1993) and non-parallel corpora (Fung and Yee, 1998), learning ofinformation extraction patterns from un-annotatedtext (Riloff and Schmelzenbach, 1998).However, because of irregularities in corpusdata, corpus statistics cannot guarantee optimalperformance, notably for rare lexical items.
Inorder to improve robustness, recent research hasattempted a variety of ways to incorporate externalknowledge into the distributional model.
In thispaper we investigate the impact produced by theintroduction of different types of linguisticknowledge into the model.Linguistic knowledge, i.e., the knowledge aboutlinguistically relevant units of text and relationsholding between them, is a particularly convenientway to enhance the distributional model.
On theone hand, although describing the ?surface?properties of the language, linguistic notionscontain conceptual information about the units oftext they describe.
It is therefore reasonable toexpect that the linguistic analysis of the context ofa word yields additional evidence about itsmeaning.
On the other hand, linguistic knowledgeis relatively easy to obtain: linguistic analyzers(lemmatizers, PoS-taggers, parsers, etc) do notrequire expensive hand-encoded resources, theirapplication is not restricted to particular domains,and their performance is not dependent on theamount of the textual data.
All these characteristicsfit very well with the strengths of the distributionalapproach: while enhancing it with externalknowledge, linguistic analyzers do not limit itscoverage and portability.This or that kind of linguistic preprocessing iscarried out in many previous applications of theapproach.
However, these studies seldom motivatethe choice of a particular preprocessing procedure,concentrating rather on optimization of otherparameters of the methodology.
Very few studiesexist that analyze and compare different techniquesfor linguistically motivated extraction ofdistributional data.
The goal of this paper is toexploire in detail a range of variables in themorphological and syntactic processing of thecontext information and reveal the merits anddrawbacks of their particular settings.The outline of the paper is as follows.
Section 2describes the preprocessing methods under study.Section 3 describes the settings for their empiricalevaluation.
Section 4 details the experimentalresults.
Section 5 discusses related work.
Section 6summarizes the results and presents theconclusions from the study.2 Types of Linguistic PreprocessingIn order to prepare a machine-processablerepresentation of a word from particular instancesof its occurrence, one needs to decide on, firstly,what is to be understood by the context of a word?suse, and, secondly, which elements of that contextwill constitute distributional features.
Astraightforward decision is to take a certain numberof words or characters around the target word to beits occurrence context, and all uninterrupted lettersequences within this delineation to be its features.However, one may ask the question if elements ofthe text most indicative of the target word?smeaning can be better identified by looking at thelinguistic analysis of the text.In this paper we empirically study the followingtypes of linguistic preprocessing.1.
The use of original word forms vs. their stemsvs.
their lemmas as distributional features.
It is notevident what kind of morphological preprocessingof context words should be performed, if at all.Stemming of context words can be expected tohelp better abstract from their particularoccurrences and to emphasize their invariablemeaning.
It also relaxes the stochastic dependencebetween features and reduces the dimensionality ofthe representations.
In addition to theseadvantages, lemmatization also avoids confusingwords with similar stems (e.g., car vs. care, ski vs.sky, aide vs. aid).
On the other hand,morphological preprocessing cannot be error-freeand it may seem safer to simply use the originalword forms and preserve their intended meaning asmuch as possible.
In text categorization, stemminghas not been conclusively shown to improveeffectiveness in comparison to using original wordforms, but it is usually adopted for the sake ofshrinking the dimensionality of the feature space(Sebastiani, 2002).
Here we will examine both theeffectiveness and the dimensionality reduction thatstemming and lemmatization of context wordsbring about.2.
Morphological decomposition of contextwords.
A morpheme is the smallest meaningfulunit of the language.
Therefore decomposingcontext words into morphemes and using them asfeatures may eventually provide more fine-grainedevidence about the target word.
Particularly, wehypothesize that using roots of context wordsrather than their stems or lemmas will highlightlexical similarities between context wordsbelonging to different parts of speech (e.g.,different, difference, differentiate) or differing onlyin affixes (e.g., build  and rebuild ).3.
Different syntactically motivated methods ofdelimiting the context of the word?s use.
Thelexical context permitting occurrence of the targetword consists of words and phrases whosemeanings have something to do with the meaningof the target word.
Therefore, given that syntacticdependencies between words presuppose certainsemantic relations between them, one can expectsyntactic parsing to point to most useful contextwords.
The questions we seek answers to are: Aresyntactically related words indeed more revealingabout the meaning of the target word than spatiallyadjacent ones?
Which types of syntacticdependencies should be preferred for delimitingthe context of a target word?s occurrence?4.
Filtering out rare context words.
The typicalpractice of preprocessing distributional data is toremove rare word co-occurrences, thus aiming toreduce noise from idiosyncratic word uses andlinguistic processing errors and at the same timeform more compact word representations (e.g.,Grefenstette, 1993; Ciaramita, 2002).
On the otherhand, even single occurrence word pairs make up avery large portion of the data and many of them areclearly meaningful.
We compare the quality of thedistributional representations with and withoutcontext words that occurred only once with thetarget word.3 Evaluation3.1 Experimental TaskThe preprocessing techniques were evaluated onthe task of automatic classification of nouns intosemantic classes.
The evaluation of eachpreprocessing method consisted in the following.A set of nouns N each belonging to one semanticclass c?C was randomly split into ten equal parts.Co-occurrence data on the nouns was collected andpreprocessed using a particular method underanalysis.
Then each noun n?N was represented asa vector of distributional features: nr = (vn,1, vn,2, ?vn,i), where the values of the features are thefrequencies of n occurring in the lexical contextcorresponding to v. At each experimental run, oneof the ten subsets of the nouns was used as the testdata and the remaining ones as the train data.
Thereported effectiveness measures are microaveragedprecision scores averaged over the ten runs.
Thestatistical significance of differences betweenperformance of particular preprocessing methodsreported below was estimated by means of the one-tailed paired t-test.3.2 DataThe set of nouns each provided with a classlabel to be used in the experiments was obtained asfollows.
We first extracted verb-noundependencies from the British National Corpus,where nouns are either direct or prepositionalobjects to verbs.
Each noun that occurred withmore than 20 different verbs was placed into asemantic class corresponding to the WordNetsynset of its most frequent sense.
The resultingclasses with less than 2 nouns were discarded.Thus we were left with 101 classes, eachcontaining 2 or 3 nouns.3.3 Classification MethodsTwo classification algorithms were used in thestudy: Na?ve Bayes and Rocchio, which werepreviously shown to be quite robust on highlydimensional representations on tasks includingword classification (e.g., Tokunaga et al, 1997,Ciaramita, 2002).The Na?ve Bayes algorithm classifies a testinstance n by finding a class c that maximizesp(c|nr ).
Assuming independence between features,the goal of the algorithm can be stated as:)|()(maxarg)|(maxarg inviiii cvpcpncp ??
?where p(ci) and p(v|ci) are estimated during thetraining process from the corpus data.The Na?ve Bayes classifier was the binaryindependence model, which estimates p(v|ci)assuming the binomial distribution of featuresacross classes.
In order to introduce theinformation inherent in the frequencies of featuresinto the model all input probabilities werecalculated from the real values of features, assuggested in (Lewis, 1998).The Rocchio classifier builds a vector for eachclass c?C from the vectors of training instances.The value of jth feature in this vector is computed as:||||,,, cvcvv ci jici jijc??
??
?-?= gbwhere the first part of the equation is the averagevalue of the feature in the positive trainingexamples of the class, and the second part is itsaverage value in the negative examples.
Theparameters b and g control the influence of thepositive and negative examples on the computedvalue, usually set to 16 and 4, correspondingly.Once vectors for all classes are built, a test instanceis classified by measuring the similarity betweenits vector and the vector of each class andassigning it to the class with the greatest similarity.In this study, all features of the nouns weremodified by the TFIDF weight before the training.4 Results4.1 Syntactic ContextsThe context of the target word?s occurrence canbe delimited syntactically.
In this view, eachcontext word is a word that enters in a syntacticdependency relation with the target word, beingeither the head or the modifier in the dependency.For example, in the sentence She bought a nice hatcontext words for hat are bought (the head of thepredicate-object relation) and nice (the attributivemodifier).We group typical syntactic relations of a nountogether based on general semantic relations theyindicate.
We define five semantic types ofdistributional features of nouns that can beextracted by looking at the dependencies theyparticipate in.A.
verbs in the active form, to which the targetnouns are subjects (e.g., the committeediscussed (the issue), the passengers  got on (abus), etc);B. active verbs, to which the target nouns aredirect or prepositional objects (e.g., hold  ameeting; depend on a friend); passive verbs towhich the nouns are subjects (e.g., the meetingis held);C. adjectives and nouns used as attributes orpredicatives to the target nouns (e.g., a tallbuilding, the building is tall; amateur actor,the actor is an amateur);D. prepositional phrases, where the target nounsare heads (e.g., the box in the room); weconsider three possibilities to constructdistributional features from such adependency: with the preposition (in_room,D1), without it (room, D2), and creating toseparate features for the preposition and thenoun (in and room, D3).E.
prepositional phrases, where the target nounsare modifiers (the ball in the box); as with typeD, three subtypes are identified: E1 (ball_in ),E2 (ball), and E3 (ball and in);We compare these feature types to each otherand to features extracted by means of the window-based context delineation.
The latter were collectedby going over occurrences of each noun with awindow of three words around it.
This particularsize of the context window was chosen followingfindings of a number of studies indicating thatsmall context windows, i.e.
2-3 words, best capturethe semantic similarity between words (e.g., Levyet al, 1998; Ciaramita, 2002).
Thereby, a commonstoplist was used to remove too general contextwords.
All the context words experimented with atthis stage were lemmatized; those, which co-occurred with the target noun only once, wereremoved.We first present the results of evaluation ofdifferent types of features formed fromprepositional phrases involving target nouns (seeTable 1).Na?ve Bayes Rocchio #dimD1  23.405  16.574 11271D2  18.571  13.879 5876D3  19.095  13.879 5911E1  28.166  17.619 7642E2  25.31  13.067 3433E3  26.714  13.067 3469Table 1.
Different kinds of features derived fromprepositional phrases involving target nouns.On both classifiers and for both types D and E,the performance is noticeably higher when thecollocation of the noun with the preposition is usedas one single feature (D1 and E1).
Using only thenouns as separate features decreases classificationaccuracy.
Adding the prepositions to them asindividual features improves the performance veryslightly on Na?ve Bayes, but has no influence onthe performance of Rocchio.
Comparing types D1and E1, we see that D1 is clearly more effective,particularly on Na?ve Bayes, and uses around 30%less features than E1.NB Rocchio #dimA 21.052 15.075 1533B 34.88 29.889 4039C 36.357 28.242 4607D1 23.405 16.574 11271E1 28.166 17.619 7642Window 38.261 18.767 35902Table 2.
Syntactically-defined types of features.Table 2 describes the results of the evaluation ofall the five feature types described above.
OnNa?ve Bayes, each of the syntactically-definedtypes yields performance inferior to that of thewindow-based features.
On Rocchio, window-based is much worse than B and C, but iscomparable to A, D1 and E1.
Looking at thedimensionality of the feature space each methodproduces, we see that the window-based featuresare much more numerous than any of thesyntactically-defined ones, although collected fromthe same corpus.
The much larger feature howeverspace does not yield a proportional increase inclassification accuracy.
For example, there arearound seven times less type C features thanwindow-based ones, but they are only 1.9% lesseffective on Na?ve Bayes and significantly moreeffective on Rocchio.Among the syntactically-defined features,types B and C perform equally well, no statisticalsignificance between their performances was foundon either NB or Rocchio.
In fact, the ranking of thefeature types wrt their performance is the same forboth classifiers: types B and C trail E1 by a largemargin, which is followed by D1, type A being theworst performer.
The results so far suggest thatadjectives and verbs near which target nouns areused as objects provide the best evidence about thetarget nouns?
meaning.We further tried collapsing different types offeatures together.
In doing so, we appended a tag toeach feature describing its type so as to avoidconfusing context words linked by differentsyntactic relations to the target noun (see Table 3).The best result was achieved by combining all thefive syntactic feature types, clearly outperformingthe window-based context delineation on bothNa?ve Bayes (26% improvement, p<0.05) andRocchio (88% improvement, p<0.001) and stillusing 20% smaller feature space.
The combinationof B and C produced only slightly worse results (thedifferences not significant for either classifiers), butusing over 3 times smaller feature space.NB Rocchio #dimB+C 43.071 35.426 8646B+C+D1+E1 47.357 36.469 27559A+B+C+D1+E1 48.309 36.829 29092D1+E1 30.095 22.26 18913Window 38.261 18.767 35902Table 3.
Combinations of syntactically-definedfeature types.4.2 Original word forms vs. stems vs. lemmasWe next looked at the performance resultingfrom stemming and lemmatization of contextwords.
Since morphological preprocessing is likelyto differently affect nouns, verbs, and adjectives,we study them on data of types B (verbs), C(adjectives), and the combination of D1 and E1(nouns) from the previous experiment.
Stemmingwas carried out using the Porter stemmer.Lemmatization was performed using a pattern-matching algorithm which operates on PoS-taggedtext and consults the WordNet database forexceptions.
As before, context words that occurredonly once with a target noun were discarded.
Table4 describes the results of these experiments.NB Rocchio #dimVerbsOriginal 35.333 31.648 9906Stem 35.357 27.665 7506Lemma 34.88 29.889 4039AdjectivesOriginal 37.309 28.911 4765Stem 36.833 29.168 4390Lemma 36.357 28.242 4607NounsOriginal 28.69 23.076 19628Stem 29.19 22.176 19141Lemma 20.976 22.26 15642Table 4.
Morphological preprocessing of verbs,adjectives, and nouns.There is very little difference in effectivenessbetween these three methods (except forlemmatized nouns on NB).
As a rule, thedifference between them is never greater than 1%.In terms of the size of feature space, lemmatizationis most advisable for verbs (32% reduction offeature space compared with the original verbforms), which is not surprising since the verb is themost infected part of speech in English.
Thefeature space reduction for nouns was around 25%.Least reduction of feature space occurs whenapplying lemmatization to adjectives, which inflectonly for degrees of comparison.4.3 Morphological decompositionWe further tried constructing features for a targetnoun on the basis of morphological analysis ofwords occurring in its context.
As in theexperiments with stemming and lemmatization, inorder to take into account morphologicaldifferences between parts of speech, the effects ofmorphological decomposition of context wordswas studied on the distributional data of types B(verbs), C (adjectives), and D1+E1 (nouns).The decomposition of words into morphemeswas carried out as follows.
From ?Merriam-Webster's Dictionary of Prefixes, Suffixes, andCombining Forms?1 we extracted a list of 12verbal, 59 adjectival and 138 nounal suffixes, aswell as 80 prefixes, ignoring affixes consisting ofonly one character.
All suffixes for a particularpart-of-speech and all prefixes were sortedaccording to their character length.
First, allcontext words were lemmatized.
Then, examiningthe part-of-speech of the context word, presence ofeach affix with it was checked by simple stringmatching, starting from the top of the corres-ponding array of affixes.
For each word, only oneprefix and only one suffix was matched.
In thisway, every word was broken down into maximumthree morphemes: the root, a prefix and a suffix.Two kinds of features were experimented with:one where features corresponded to the roots of thecontext words and one where all morphemes of thecontext word (i.e., the root, prefix and suffix)formed separate features.
When combining featurescreated from context words belonging to differentparts-of-speech, no tags were used in order to maproots of cognate words to the same feature.
Theresults of these experiments are shown in Table 5.roots roots+affixeslemmasNa?ve bayesB 37.261 35.833 34.88C 38.738 39.214 36.357D1+E1 29.119 25.785 30.095B+C 43.976 42.071 43.547B+C+D1+E1 46.88 45.452 48.309RocchioB 24.241 24.061 29.889C 27.803 27.901 28.242D1+E1 13.267 12.87 22.26B+C 28.747 28.019 35.426B+C+D1+E1 28.863 30.752 36.469Table 5.
Distributional features derived from themorphological analysis of context words.On Na?ve Bayes, using only roots increases theclassification accuracy for B, C, and B+Ccompared to the use of lemmas.
The improvement,however, is not significant.
Inclusion of affixesdoes not produce any perceptible effect on theperformance.
In all other cases and when theRocchio classifier is used, decomposition of wordsinto morphemes consistently decreasesperformance compared to the use of their lemmas.These results seem to suggest that the union ofthe root with the affixes constitutes the most1 Available at www.spellingbee.com/pre_suf_comb.pdfoptimal ?container?
for distributional information.Decomposition of words into morphemes oftencauses loss of a part of this information.
It seemsthere are few affixes with the meaning so abstractthat they can be safely discarded.4.4 Filtering out rare context wordsTo study the effect of removing singletoncontext words, we compared the quality ofclassifications with and without them.
The resultsare shown in Table 6.NB Rocchio #dimWithout singletonsB 34.88 29.889 4039C 36.357 28.242 4607B+C 43.547 35.426 8646A+B+C+D1+E1 48.309 36.829 29092Window 38.261 18.767 35902With singletonsB 38.361 25.164 14024C 39.261 28.387 9898B+C 45.
29.535 23922A+B+C+D1+E1 44.
25.31 98703Window 41.142 19.037 94606Table 4: The effect of removing rare context words.The results do not permit making anyconclusions as to the enhanced effectivenessresulting from discarding rare co-occurrences.Discarding singletons, however, does considerablyreduce the feature space.
The dimensionalityreduction is especially large for the datasetsinvolving types B, D1 and E1, where each feature isa free collocation of a noun or a verb with apreposition, whose multiple occurrences are muchless likely than multiple occurrences of anindividual context word.5 Related workA number of previous studies compared differentkinds of morphological and syntacticpreprocessing performed before inducing a co-occurrence model of word meaning.Grefenstette (1993) studied two contextdelineation methods of English nouns: thewindow-based and the syntactic, whereby all thedifferent types of syntactic dependencies of thenouns were used in the same feature space.
Hefound that the syntactic technique produced betterresults for frequent nouns, while less frequentnouns were more effectively modeled by thewindowing technique.
He explained these resultsby the fact that the syntactic technique extractsmuch fewer albeit more useful features and thesmall number of features extracted for rare nounsis not sufficient for representing their distributionalbehavior.Alfonseca and Manandhar (2002) compareddifferent types of syntactic dependencies of a nounas well as its ?topic signature?, i.e.
the featurescollected by taking the entire sentence as thecontext of its occurrence, in terms of theirusefulness for the construction of its distributionalrepresentation.
They found that the besteffectiveness is achieved when using acombination of the topic signature with the ?objectsignature?
(a list of verbs and prepositions towhich the target noun is used as an argument) andthe ?subject signature?
(a list of verbs to which thenoun is used as a subject).
The ?modifiersignature?
containing co-occurring adjectives anddeterminers produced the worst results.Pado and Lapata (2003) investigated differentpossibilities to delimit the context of a target wordby considering the syntactic parse of the sentence.They examined the informativeness of featuresarising from using the window-based contextdelineation, considering the sum of dependenciesthe target word is involved in, and considering theentire argument structure of a verb as the contextof the target word, so that, e.g.
an object can be afeature for a subject of that verb.
Their studydiscovered that indirect syntactic relations withinan argument structure of a verb generally yieldbetter results than using only direct syntacticdependencies or the windowing technique.Ciaramita (2002) looked at how the performanceof automatic classifiers on the word classificationtask is affected by the decomposition of targetwords into morphologically relevant features.
Hefound that the use of suffixes and prefixes of targetnouns is indeed more advantageous, but this wastrue only when classifying words into large wordclasses.
These classes are formed on the basis ofquite general semantic distinctions, which are oftenreflected in the meanings of their affixes.
In additionto that, the classification method used involvedfeature selection, which ensured that uselessfeatures resulting from semantically empty affixesand errors of the morphological decomposition didnot harm the classification accuracy.6 ConclusionIn this study we examined the impact whichlinguistic preprocessing of distributional dataproduce on the effectiveness and efficiency ofsemantic classification of nouns.Our study extends previous work along thefollowing lines.
First, we have compared differenttypes of syntactic dependencies of the target nounin terms of the informativeness of the distributionalfeatures constructed from them.
We find that themost useful dependencies are the adjectives andnouns used as attributes to the target nouns and theverbs near which the target nouns are used asdirect or prepositional objects.
The most effectiverepresentation overall is obtained when using allthe syntactic dependencies of the noun.
We findthat it is clearly more advantageous than thewindowing technique both in terms ofeffectiveness and efficiency.
The combination ofthe attribute and object dependencies also producesvery good classification accuracy, which is onlyinsignificantly worse than that of the combinationof all the dependency types, while using severaltimes more compact feature space.We further looked at the influence of stemmingand lemmatization of context words on theperformance.
The study did not reveal anyconsiderable differences in effectiveness obtainedby stemming or lemmatization of context wordsversus the use of their original forms.
Lemma-tization, however, allows to achieve the greatestreduction of the feature space.
Similarly, theremoval of rare word co-occurrences from thetraining data could not be shown to consistentlyimprove effectiveness, but was very beneficial interms of dimensionality reduction, notably forfeatures corresponding to word collocations.Finally, we examined whether morphologicaldecomposition of context words helps to obtainmore informative features, but found thatindiscriminative decomposition of all contextwords into morphemes and using them as separatefeatures actually more often decreases performancerather than increases it.
These results seem toindicate that morphological analysis of contextwords should be accompanied by some featureselection procedure, which would identify thoseaffixes which are too general and can be safelystripped off and those which are sufficientlyspecific and whose unity with the root bestcaptures relevant context information.7 AcknowledgementsThe research was supported by the RussianFoundation Basic Research grant #03-06-80008.We thank our colleauges Steffen Staab andAndreas Hotho for fruitful discussions during thework on this paper.ReferencesE.
Alfonseca and S. Manandhar.
2002.
Extending alexical ontology by a combination ofdistributional semantics signatures.
InProceedings of 13th International Conference onKnowledge Engineering and KnowledgeManagement, pages 1-7.S.
Caraballo.
1999.
Automatic construction of ahypernym-labeled noun hierarchy from text.
InProceedings of ACL?99, pages 120-126.M.
Ciaramita.
2003.
Boosting automatic  lexicalacquisition with morphological information.
InProceedings of the ACL?02 Workshop onUnsupervised Lexical Acquisition, pages 17-25.P.
Fung and L.Y.
Yee.
An IR approach fortranslating new words from nonparallel,comparable texts In Proceedings of COLING-ACL?98, pages 414-420.G.
Grefenstette.
1993.
Evaluation techniques forautomatic semantic extraction: comparingsyntactic and window based approaches.
InProceedings of the SIGLEX Workshop onAcquisition of Lexical Knowledge from Text,Columbus, Ohio.M.
Kay and M. R?scheisen.
1993.
Text-translationalignment.
Computational Linguistics.19(1):121-142.J.
Levy, J. Bullinaria, and M. Patel.
1998.Explorations in the derivation of word co-occurrence statistics.
South Pacific Journal ofPsychology, 10(1), 99-111.D.
Lewis.
1998.
Naive (Bayes) at forty: Theindependence assumption in information re-trieval.
In Proceedings of ECML?98, pages 4-15.D.
Lin.
1998.
Automatic retrieval and clustering ofsimilar words.
In Proceedings of the COLING-ACL?98, pages 768-773.S.
Pado and M. Lapata.
2003.
Constructingsemantic space models from parsed corpora.
InProceedings of ACL?03, pages 128-135.V.
Pekar and S. Staab.
2002.
Factoring thestructure of a taxonomy into a semanticclassification decision.
In: Proceedings ofCOLING?02, pages 786-792.E.
Riloff and M. Schmelzenbach.
1998.
Anempirical approach to conceptual case frameacquisition.
In: Proceedings of the 6th Workshopon Very Large Corpora.F.
Sebastiani.
2002.
Machine learning inautomated text categorization.
ACM ComputingSurveys, 34(1): 1-47.T.
Tokunaga, A. Fujii, M. Iwayama, N. Sakurai,and H. Tanaka.
1997.
Extending a thesaurus byclassifying words.
In Proceedings of the ACL-EACL Workshop on Automatic InformationExtraction and Building of Lexical SemanticResources, pages 16-21.
