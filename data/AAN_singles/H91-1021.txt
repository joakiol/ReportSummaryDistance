Augmented Role Filling Capabilities forSemantic Interpretation of Spoken LanguageLewis Norton, Marcia Linebarger, Deborah Dahl, and Nghi NguyenUnisys Center for Advanced Information TechnologyPaoli, Pennsylvania 19301ABSTRACTThis paper describes recent work on the Unisys ATIS Spo-ken Language System, and reports benchmark results on nat-ural language, spoken language, and speech recognition.
Wedescribe enhancements to the system's emantic processingfor handling non.transparent argument structure and enhance-ments to the system's pragmatic processing of material in art.swers displayed to the user.
We found that the system's scoreon the natural anguage benchmark test decreased from ~8~o to36~ without hese enhancements.
We also report results forthree spoken language systems, Unisys natural language cou-pled with MIT-Summit speech recognition, Unisys natural an-guage coupled wish MIT-Lincoln Labs speech recognition andUnisys natural anguage coupled with BBN speech recognition.Speech recognition results are reported on the results of theUnisys natural anguage selecting a candidate from the MIT-Summit N-best (N=16).INTRODUCTIONImproving the performance of spoken language systems re-quires addressing issues along several fxonts, including ba-sic improvements in natural language processing and speechrecognition as well as issues of integration of these compo-nents in spoken language systems.
In this paper we report theresults of our recent work in each of these areas.
*One major area of work has been in the the semantic andpragmatic omponents of the Unisys natural  language process-ins system.
The work in semantics enhances the robustnessof semantic processing by allowing parses which do not di-rectly express the argument structure expected by semanticsto nevertheless be processed in a rule-governed way.
In thearea of pragmatics we have extended our techniques for bring-ing material displayed to the user into the dialog context tohandle several additional classes of references to material inthe display.?
This work was supported by DARPA contract N000014-89-C0171, administered bythe Office of Naval Research.
We are grate-fuI to Victor Zue of MIT, Doug Paul of MIT Lincoln Laboratoriesand John Mak.houl of BBN for making output from their speechrecognition systems available to us.
We also wish to thank TimFinln, Rich Fritzson, Don McKay, Jim Meldlnger, and Jan Pastorof Unisys and Lynette Hirschnmn of MIT for their contributions tothis work.In the area of integration of speech and natural anguage, wereport on an experiment with three spoken language systems,coupling the same Unisys natural  language system to threedifferent speech recognisers as shown in Figure 1.~" :.,~:.~:~:~:~,-:~,.
: l~.'-'.
;,-',.~i.~~.,~,.,.,.,~,~,.i-.,.-,~:~il:- :+: P t iq0tY  x-:  .."Figure 1: Un lsys  natura l  language + mul t ip lespeech  recogn lsersWe believe this is a very promising technique for evaluat-ing the components of spoken language systems.
Using thistechnique we can make a very straightforward compe~ison ofthe performance of the recognizers in a spoken language con-text.
Furthermore, this technique also allows us to make afine-gralned comparision of the interaction between speech andnatural anguage in the three systems by looking at such ques-tions as the relative proportion of speech recognizer outputsthat fail to parse, fall to receive a semantic analysis and so on.Finally, we report on speech recognition results obtained byfiltering the N-best (N=16) from MIT-Summlt through theUnisys natural language system.
We note that there was ahigher error rate for context-dependent speech as compared tocontext-independent speech (54.6% compared to 45.8~) andsuggest wo hypotheses which may account for this difference.SEMANTICSWhen evaluating our system after the Hidden Valley work-shop, we observed two phenomena bout PUNDIT (the Unlsysnatttral anguage understanding system \[3\]) that warrantedimprovement.
The first was that PUNDIT's semantic inter-preter was sometimes fA;llng to correctly recognize predicate125argument relationships for syntactic constituents that werenot immediately associated with their intended head.
The sec-ond was that PUNDIT was producing different representationsfor queries with different syntactic/lexical content but identi-cal (or nearly identical) semantic ontent.
We see both of theseshortcomings as due to what we will term "non-transparentargument structure": syntactic representations in which syn-tactic constituents are not associated with their intended head,or semantic representations in which predicate-argument re-lationships are underspecified.
Our approach to dealing withthese shortcomings has been to maintain a rule-governed ap-proach to role-filling despite non-transparent syntactic andsemantic structures.
We believe that the extensions we areabout to describe are especially relevant o Spoken LanguageUnderstanding, because non-transparent argument structureappears to be particularly characteristic of spontaneous spo-ken utterances, for reasons we will sketch below.The  semant ic  in terpreter  and  non- t ransparent  parsesSemantic interpretation i PUNDIT is the process of instan-t iating the arguments of case frame structures called decom-positions, which are associated with verbs and selected nounsand adjectives (\[7\]).
The arguments of decompositions areassigned thematic role labels such as agent, patient, source,and so forth.
Semantic interpretation is a search for suit-able grammatical role/thematic role correspondences, usingsyntax-semantics mapping rules, which specify what syntac-tic constituents may fill a particular ole; and semantic lassconstraints, which specify the semantic properties required ofpotential fillers.
The syntactic constraints on potential rolefillers are of two types: CATEGORIAL constraints, which re-quire that the potential filler be of a certain grammatical typesuch as subject, object, or prepositional phrase; and ATTACH-MENT constraints, which require that the potential filler occurwithin the phrase headed by the predicate of which it is to bean argument.
The categorial constraints are stated explicitlyin the syntax-semantics mapping rules; the latter are implicitin the functioning of the semantic interpreter.
For example,the source role of flight_C, the domain model predicate as-sociated with the noun "flight", can, in accordance with thesyntax-semantics mapping rules, be filled by the entity associ-ated with the object of a " f rom'-pp occurring within the samenoun phrase as "flight" (The flight from Boston takes threehours).
Unfortunately, the parse does not always express theargument structure of the sentence as transparently as it doesin this example; constituents that should provide role fillersfor a predicate are not always syntactically associated withthe predicate.
There are several causes for such a mismatchbetween the parse and the intended interpretation.
They in-clude ( l )  a variety of syntactic deformations which we willrefer to as extraposition ( What flights do you have to Boston,where the " to ' -pp  belongs in the subject np; I need ticket in-formation from Boston to Dallas, where the pp's modify theprenominal noun "ticket", not the head noun "information";or I toant a cheaper flight than Delta 66, where the " than ' -ppmodifies "cheaper", not "flight"), (2) metonymy ( I  toant the$50.00 flight, where the speaker means that s/he wants theflight whose FARE is $50.00), and (3) suboptimal parses (e.g.,parses with incorrect pp-attachment).Our changes to the semantic interpreter allow it to fill rolescorrectly in cases such as the above, util ising its existingknowledge of syntax-semantics correspondences, but relaxingcertain expectations about the syntactic attachment of role-filling constituents.
Thus the CATEGORIAL constraints remainin force, but the ATTACHMENT constraints have been loosenedsomewhat.
The system now identifies prepositional phrasesand adverbs which have not frilled a role in the predicate withwhich they are syntactically associated, and offers them asrole fillers to fillers of this predicate.
This strategy applies re-cursively to fillers of fillers of roles; for example, in What typesof ground transportation services are available from the air-port in Atlanta to downtown A tlanta f, the two final pp's ulti-mately fill roles in the decomposition associated with "groundtransportation" since neither "types" nor "services" has map-ping rules to consume them.
The same mechanism already inplace for role-filling is employed in these cases, the only differ-ence being that unused syntactic onstituents are propagateddownward.
Note that we continue to take syntax into account;we do not wish to ignore the syntax of leftover constituentsand fill roles indiscriminately on the basis of semantic proper-ties alone.We conducted an experiment to assess the effects of thesechanges upon the system's performance, using a set of 138queries (both class A and non-class A) on which the systemwas previously trained.
The measure of performance used wasthe standard ATIS metric of the number of correct answersminus the number incorrect.
Disabling the semantic hangesdescribed above lowered the system's core from 82 to 63, adecrease of 23~.The  app l i ca t ion  modu le  and  non- t ransparent  semant icrepresentat ionsOur second improvement was directed at cases where PUN-DIT's semantic interpreter may have correctly represented themeaning of a sentence but in an irregular way.
For exam-pie, the instantiated decomposition produced for "flights fromBoston" is:f i ight_?
(~ i ight l ,  source (boston) .
.
.
.
)while "flights leaving Boston" resulted in:~ l ight  C ( f l ight  1, source (_ ) ,  .
.
.
)loavoP ( l save l ,f l i ght  ( f l ight  1) ,source (boston), .
.
.
)Clearly it would be preferable for the flight_C decompositionto be the same in both cases, but in the second case the sourcerole of the decomposition associated with flightl was unfilled,although it could be inferred from the leaveP decompositionthat the flight's source was Boston.
In other words, PUNDIThad not captured a basic synonymy relation between thesenp's.Our response to this was to augment the semantic inter-preter with a routine which can perform inferences involvingmore than one decomposition.
The actual inferences are ex-pressed in the form of rules which are domain-dependent; theinference-performlng mechanism is domain-independent.
Forthe above example, we have written a rule which, paraphrased126in English, says that if a verb is one of a class of motion verbsused to express flying (e.g., "leave"), and if the source role ofthis verb is filled, propagate that filler to the source role ofthe flight involved.
Thus the flight_C decomposition becomesthe same for both inputs.
Thirty-four such rules have beenwritten for the ATIS domain, and we estimate that they areapplicable to 10% to 15% of the training data.The payoff rom this extension comes in the use of PUNDIT'soutput by application modules.
For the ATIS domain, the ap-plication module is the program that takes PUNDIT's outputand uses it to formulate a query to the ATIS DB.
It is obviouslyadvantageous for the creation and maintenance of an applica-tion module that its input be regularized to the greatest extentpossible, thus making such a module simpler, and avoiding du-plication of code to compensate for non-regularized input indifferent application modules.When we ran the same set of 138 queries used in the exper-iment described in the previous ubsection without the rulesjust discussed (but with the semantics improvements of theprevious ubsection), the system's core dropped from 82 to62, or 24%.
There appears to be little interaction between thesemantics improvements and the rules of this subsection-theyapply to different phenomena in input data.PRAGMATICSIn our June 1990 workshop aper (\[6\]), we described a fea-ture of our system which we included to handle correctly aparticular kind of discourse phenomenon.
In particular, in theATIS domain there are frequent references to flights by flightnnmher (e.g., "Delta flight 123") which the user means to beunambiguous, but which in general have to be disambiguatedin context.
The reason is that the user learned about "Delta123" from some previous answer, where it was returned as oneof the flights between two cities City1 and City2.
The problemis that "Delta 123" may have additional legs; for instance itmay go on from City2 to City3.
The user, when asking forthe fares for "Delta 123", is presumably interested only in theCity1 to City2 fare, not the City2 to City3 one and not theCity1 to City3 one.
So our system looked back at previous an-swers to find a mention of "Delta 123", thereby determiningthe flight leg of interest.This kind of disamhiguation can take other forms, and wehave added some of them to our system since June.
One ofthese capabilities i illustrated by the two queries What  doesLH meanf  and What  does EQP meanf  Without context,the first of these cannot be correctly answered, because "LH"is a code for both an airline and a fare class.
The secondof these queries would yield a table with two rows, one rowfor each table for which "EQP" is one of the table's columnheadings.
In both of these queries, however, the user is askingfor clarification of something which has been presented as partof a previous answer display.
So what our system needs todo, and does do, is refer back to previous answers much inthe spirit of the "Delta 123" example above.
For the firstquery, we will find the most recent answer which has "LH" asa column entry in some row: for the second we will find themost recent answer which has "EQP" as a column heading.Our system can then make the proper disambiguation andpresent he user with an appropriate cooperative response tothe follow-up query.
There were only a handful of follow-upqueries of this form in the training data, hut the extension tohandle them was easy to add given the code in place to handlethe "Delta 123" example.Similarly, the training data contained numerous instancesof queries such as What  are the c laues .
~ In the absence ofcontext, the best answer to this seems to be a llst of the morethan 100 different fare classes.
However, queries uch as theseinvariably follow the display of some fare classes in either flighttables or fare tables.
The cooperative r sponse, then, is to dis-play a table of fare classes whose rows have been limited tothose classes previously mentioned in the most recent flightor fare table.
Our system also uses a generalization of thisalgorithm to filter requests for other kinds of codes, such asrestrictions, ground transportation codes, aircraft codes, andmeal codes.
In all, from the TI  training data (\[2\]) we havenoticed 19 follow-up queries (out of 912) which now get thecorrect answer in context because of this extension to our sys-tem; there may be more queries which requite this extensionthat we have not yet processed correctly for other reasons.We make it possible to refer to previous answer tables inour system by means of the following mechanism.
Wheneveran answer table is returned, a discourse ntity representing itis added to the discourse context, and a semantics for this en-tity is provided.
Roughly speaking, if the query leading to theanswer table is a request for X, the semantics can be thoughtof as being "the table of X" (\[6\]).
For example, if the querywas a request for flights from City1 to City2, the semantics as-signed to the discourse ntity representing the answer is "thetable of flights from City1 to City2".
Note that we do NOTcreate discourse ntities for each row (particular flights fromCity1 to City2 in the example) or for each column entry in arow (e.g., the departure time of a particular flight from Citylto City2).
Doing so would make the discourse context munan-ageably large.
But the table (complete with column headings)is available and accessible to our system, and can be searchedfor particular values when it is desirable to do so, as in thecapabilities being described in this section.The techniques just described epend on the availabilityof previous ANSWERS.
Some of the follow-up queries whichthey enable to be answered correctly could perhaps be han-dled by reference to previous QUERIES only, particularly inthe special case where there is known to be only one previousquery.
We believe that our techniques are superior for at leasttwo reasons.
First, in the presence of more than one previousquery, the answers to those previous queries are for our systema more compact and modular epresentation f the content ofthose queries than the discourse ntities created while analys-ing the queries themselves; in short, it is simply easier to findwhat we want in the answers rather than in our representa-tions of the queries.
Second, there are follow-up queries whichcannot he answered unless reference is made to previous an-swers, so such techniques are necessary in a complete system.Therefore, why not use them whenever they can be used, evenwhen alternative techniques might be available?The February 1991 D1 pairs test, which limited context de-pendency to dependency which could be resolved by examina-tion of a single previous query (and not its answer), provides127additional data on the applicability of these methods.
In par-ticular, 27 of the 38 pairs involved the disambiguation f aflight number to the flight leg of interest.
It appears that fouradditional queries can be successfully answered by the tech-nique we discussed above for handling the query What arethe classes?
The remaining 7 queries appear to be such thatreference to previous answers is not helpful.SPOKEN LANGUAGE SYSTEMSWe describe here the five spoken language tests in whichwe participated.
Our methodology in these tests has been tocouple the speech recognition output from different recogniz-ers to the same natural anguage processing system.
Becausethe natural anguage component and the application moduleare held constant in these systems, this methodology providesus with a means of comparing the performance of speech rec-ognlzers in a spoken language context.Class .A: UnSays PUNDIT sys tem coup led  wi th  MITSummitThe spoken language system used in this test consists of theUnisys PUNDIT natural language system coupled via an N-bestinterface to the MIT SUMMIT speech recognition system.
Wewill refer to this system as Unisys-MIT.
These results were runwith N=16, except for 4 utterances which could not be runat N=16 because of insufficient memory in the speech recog-nition system.
N=I  was used for these utterances.
SUMMITproduced the N-best and PUNDIT selected from the N-best themost acoustically probable utterance which also passed PUN-DIT's syntactic, semantic, and pragmatic onstraints.
PUNDITthen processed the selected candidate to produce the spokenlanguage system output.
The value of N of 16 was selected onthe basis of experiments reported in \[1\], which demonstratedthat using larger N's than 10-15 leads to a situation where thechance of getting an F begins to outweigh the possible benefitof additional T's.The SUMMIT system is a speaker-independent continuousspeech recognition system developed at the MIT Laboratoryof Computer Science.
It is described in \[10\].Un lsys  PUNDIT coup led  w i th  L inco ln  Labs Speech Rec-ognizerThe spoken language system used in this test consists of theUnisys PUNDIT natural language system loosely coupled to theMIT Lincoln Labs speech recognition system.
The LincolnLabs system selected the top-1 output, which PUNDIT thenprocessed to produce the spoken language system output.The LincoLn Labs system is a speaker independent continu-ous speech recognition system which was trained on a corpusof 5020 training sentences from 37 speakers.
It used a bigrambackoff language model of perplexity 17.8.
The system is de-scribed in more detail in \[8\].Class A: Un lsys  PUNDIT sys tem coup led  wi th  BBN BY-BLOSIn this test N-best output from the BBN BYBLOS system asdescribed in \[4\] was input to PUNDIT.
As in the system whichused the MIT N-best, we used an N of 16.
The N-best fromClass Number T F ScoreClass A 145 queries 84 14 48.3~Class D1 38 pairs 24 0 63.2~Class AO 11 queries 1 0 9.1%Class D10 2 pairs 0 1 -50%Table 1: Un isys  Sys tem ScoresBBN was the output from BYBLOS rescored using cross-wordmodels and a 4-gram model and then reordered before inputto the natural language system.Opt iona l  Class A TestsWe also report on spoken language results on the optionalclass A test, using both the Unlsys-MIT system and theUnisys-BBN systems described above.SPEECH RECOGNIT ION TESTSThe speecli recognition tests were done using the naturallanguage constraints provided by the Unisys PUNDIT natu-ral language system to select one candidate from the N-bestoutput of the MIT Laboratory of Computer Science SUMMITspeech recognition system.
Using an N of 16, PUNDIT selectedthe first candidate of the N-best which passed its natural lan-guage constraints based on syntactic, semantic and pragmaticknowledge.
If all candidates were rejected by the natural lan-guage system, the first candidate in the N-best was consideredto be the recognized string.BENCHMARK RESULTSNatura l  Language Common Task Eva luat lonUnisys attempted all four of the nature,\] language tests;both the required and the optional class A and class D1 tests.Our scores as released by NIST are as shown in table 1.
Theoverall evel of success is unimpressive.
For the class A test,which corresponds most closely to the test last June, our per-formance is not much better, in spite of eight more months ofwork on our system.
(If the scoring algorithm in effect nowhad been in effect in June, our score then would have been42.2Ye) As this paper is being written, we have not had thetime to examine our performance on a sentence by sentencebasis.
It appears likely, however, that the amount of train-ing data has not yet adequately covered the full range of thevarious ways that people can formulate queries to the ATISdatabase.We are fairly pleased that our "false alarm" rate has notgone up since June.
It was 11% then; if we take the 196 sen-tences involved in the latest 4 tests as a single group, we findour rate of F's to be less than 8%.
When we discuss our spo-ken language results in a subsequent section, we will see thatalthough the rate of correct answers drops noticeably when aspeech recognizer is added to the system, the rate of incorrectanswers does not  appear to increase.
The importance of a low128"false alarm" rate is well appreciated by spoken language un-derstanding researchers; from a user's point of view nothingcould be worse than an answer which is wrong although theuser may have no way of telling it is wrong.
It will be im-portant o lower the rate of such errors to a level well belowS%.Our best performance came on the D1 pairs test.
One wouldhave expected a lower score on any test that requires two con-secutive sentences to be understood than on a test of self-contained sentences.
While we wish we could claim that ourwork discussed in the earlier section on pragmatics was instru-mental in achieving our score, it appears that much of whatwe added to our system did not come into play in this test.A more likely explanation of the unexpectedly high score isthat when a user queries the system in a mode which utilizesfollow-up queries, he or she tends to use simpler individualqueries.
Perhaps a user who does not use follow-up queries istrying to put more into each individual query.
Some evidencefor this is that our score for just the 20 distinct class A an-tecedent sentences for the D1 pairs test was 75%, well aboveour 48.3% score for all the class A sentences.
Even more strik-ing is the fact that of the 9 speakers represented in this roundof tests, only two contributed more than 3 pairs to the classD1 test-speakers CK and CO contributed 13 pairs each.
Ourscores restricted to just those two speakers were 93~ for theclass A test and 65% for the class D1 test (100% for speakerCO in the class D1 test!
).The optional tests clearly were too small to have much sig-nificance.
It is not surprising that our system proved to beincapable at this point of dealing with extraneous words in theinput queries, for we have made no efforts as yet to compen-sate for such inputs.
These tests will be useful as a benchxnarkfor comparison after we have addressed such issues.Semant ics  Extens ions  and  the Common Task TestsIn the section on semantics we reported the results of twoexperiments hat we ran to assess the effects of extensions toour system.
We performed the same tests using the data ofthe latest class A test of 145 queries.
When the extensionsto our semantic interpreter were removed, our performancedropped to 72 T, 19 F, or 36.6%, a decrease of 24% from ourscore of 48.3%.
This reinforces our belief that these extensionsare very important and useful.
When we ran the test withoutthe rules relating multiple decompositions, our performancewas 83 T, 14 F, or 47.6%, a decrease of less than 2%.
Thislatter finding was most surprising-basically it implies that inthe 1991 test data there were virtually no constructions of thekind which those rules enable us to process, because the ab-sence of the rules relating the decompositions correspondingto those constructions resulted in almost no reduction in ourscore.
In particular, there must have been no nouns modi-fied by relative clauses ("flights that arrive before noon") orparticipial modifiers ("flights serving dinner").
This has someimplication regarding the distribution of various forms of syn-tactic expression across speakers, for phenomena which weredearly significant in our training data apparently were absentfrom 9 speakers' worth of test data.The above experiments imply that our system as of lastJune would have gotten a score of less than 35% on the cur-rent class A test, for the extensions discussed in the sectionon semantics were not the only improvements we have madeto our system.
This is another indication of variability amongspeakers; for our system the 5 speakers of last June's testwere easier to process.
It appears to us that larger test setsare necessary to make a broad evaluation of natural languageunderstanding capabilities.
(We do not extend this suggestionto tests involving speech input because of the level of effortthat would consume.)
We have already noted the absence ofrelative clauses and participial modifiers in the recent classA test.
We also noticed that 23 of 145, or 16%, of the sen-tences used the word "available", usually in constructions like"what X is available", while this word only appeared in 4% ofthe pilot training data.
In the class D1 test, there were fewdiscourse phenomena represented, and we noted in an ear-lier section that over 70% (27 of 38) of the D1 pairs involvedjust the phenomenon of flight leg disambiguation.
Tests ofsuch size, then, are not broadly representative of the range ofquery formulations in the ATIS domain.Related to the last point is the suspicion that the few thou-sand sentences of training data are themselves too few to rep-resent the range of user queries for this domain.
We havenoticed that fewer new words are appearing in the more re-cent sets of training data, so vocabulary closure is probablyoccurring.
Even so, in the 145 class A queries of the recenttest, our system found 12 with unknown words, or 8% of thequeries.
This was actually higher than the 5.5% we experi-enced with the test last June, but that is more a comment onthe variability due to small test size.
It is an open questionwhether more and more training data is the answer to makingour systems more complete, however.
After all, larger volumesof data are both expensive to collect and expensive to trainfrom.
The lack of closure for the syntactic and semantic vari-ation in user queries presents a challenge for further researchin spoken language understanding.
It may we\]\] be that we willhave to begin studying reasonable ways in which the variationin the range of user expression can be limited, without undulycontrainlng the user in the natural performance of the task.Spoken Language Eva luat ionsUnisys-MIT The spoken language results for this systemwere 29 T, 15 F, and 101 NA, for a weighted score of 9.7%.
Thesystem examined an average of 6.5 candidates in the N-bestbefore finding an acceptable one.
Of all candidates consideredby the system, we found that 85~ were rejected by the syntaxcomponent and 3% by the semantics/praKmatics component,and 11% were accepted by both components.
It should bepointed out that the syntax component uses a form of com-piled semantics constraints during its search for parses (\[5\]),thus the resnlts for purely syntactic rejection are not as high asappears in this comparison, because some semantic onstraintsare applied during parsing.
After a candidate is accepted byboth syntax and semantics, the search in the N-best is termi-nated.
However, the application component, which containsa great deal of information about domain-specific pragmatics,can also reject syntactically and semantically acceptable in-puts for which it cannot construct a sensible database query.In fact, a syntactically and semanticul\]y acceptable candidatewas found in 75% of the N-best candidate lists, but a call was129100'!
?~ 70??
:Z  15LE - - Iz 10 - J2 3 4 5 6 7 8 9 10 11 12 13 14 15 16= -~Location of Selected candidate I (all rejected) i1~ MiT N-beSt ~ BBN N-best \]Figure 2: Compar i son  o f  Locat ion  o f  AcceptedCand idate  in  N-best  (N=16)  fo r  Un lsys -MIT  andUn isys -BBN Systemsmade for only 30% of inputs.
The application component wasnot able to rnal~e a sensible call for the remaining inputs.The false alarm (or F) rate we observed in this test wasaround 10~, which is consistent with our previous poken lan-guage results (\[1\]) and with our natural  language results, asdiscussed above.Unisys-BBN This system received a score of 77 T, 20 Fand 48 NA for a weighted score of 39.3%.
In this system74% of all inputs were rejected by syntax, 11% of inputs wereaccepted by syntax but rejected by semantics and 15~ wereaccepted by both syntax and semantics.
The false alarm rateis 13.8~, which is slightly higher but in the same range asprevious false alarm rates.As can be seen in Figure 2, in general the system found anacceptable candidate arlier in the N-best with the BBN N-best than with the MIT N-best.
The average location of theselected candidate in the N-best with the BBN data was 3.8compared to 6.5 with the MIT N-best.Unisys-LL Using the top-one candidate from the LincolnLabs speech recognizer the spoken language results for thissystem were 32 T, 5 F and 108 NA for a weighted score of18.6~.
The false alarm rate for this system was only 3.4~,which is lower than that for the other spoken language andnatural  language systems on which we report in this paper.There is no obvious explanation for this.
The simple hypoth-esis of better speech recognition in the Unisys-LL system willnot suffice, because the BBN system has better speech recog-nition but the false alarm rate is higher than the Unisys-LLrate.
In addition, the Unisys system's performance on the NLtest tells us how the system would do given perfect speechrecognition, and the false alarm rate there is around 8~.
Onepossible hypothesis is that the bigram language model usedin the Lincoln Labs system is in some sense more conserva-tive than the language models used in the BBN and MITsystems and consequently prevents ome of the inputs whichmight have led to an F in the natural  anguage system frombeing recognized well enough for the natural  anguage systemto generate an F.In this system, based on one input per utterance, we foundthat 59% of the inputs failed to receive a syntactic analysis(including compiled semantics, as discussed above) and 2%failed to receive a semantic analysis.
No database call couldbe generated for 13% of the inputs and a call was made forthe remaining 25~ of the inputs.Eva luat ion of  the Natura l  Language System In \[1\] wereported on a technique for evaluation of the natural anguagecomponent of our spoken language system, based on the ques-tion of how often did the natural anguage system do the rightthing.
If the reference answer for an utterance is found in theN-best, the right thing for the natural  anguage system is tofind the reference answer (or a semantic equivalent) in the N-best and give the right answer.
The operational definition ofdoing the right thing, then, is for the system to receive a "T"on such inputs.
On the other hand if the reference answer isnot in the N-best the right thing for the system to do is toeither find a semantic equivalent to the reference answer or toreject all inputs.
Thus, doing the right thing in the case of noreference answer can be operationally defined as "T" + "NA' .Reference Referencein N-best not in N-bestPundit right 54% 90%Pundit wrong 45% 10%Overa l l84%16%Table 2: PUNDIT's per fo rmance  on  C lass  A (145quer ies ) ,  depend ing  on  whether  o r  not  re ferencequery  occur red  in  N-best  (N=16)  f rom MIT -LCSSPREC.Reference Referencein N-best not in N-best OverallPundit right 69~ 81~ 73~Pundit wrong 31% \] 19% 27~Table 3: PUNDIT's per fo rmance  on  C lass  A (145quer ies ) ,  depend ing  on  whether  o r  not  re fer -ence  query  occur red  in  N-best  (N=16)  f rom BBNSPREC.Several interesting comparisons can be made based on ta-bles 2 and 3.
To begin with, it seems clear that the BBNN-best is better than the MIT N-best based on three quitedistinct measures - first of all the speech recognition score isbetter (16.1% word error rate for BBN vs. 43.6~ word er-ror rate for MIT), secondly, the spoken language score (withthe natural language system held constant) for Unisys-BBNis better than Unlsys-MIT (39.3% for Unisys-BBN vs. 9.7%for Unlsys-MIT) and thirdly, the reference answer occurredin MIT's  top 16 candidates only 15~ of the time vs. 65% ofthe time for the BBN N-best.
Thus this experiment allows us130to ask the question of what effect does better speech recogni-tion have on the interaction between speech recognition andnatural language?In the case where the reference answer is in the N-best,PUNDIT does much better with the BBN N-best.
Since lesssearch in the N-best is required with BBN data the referenceanswer or equivalent is likely to be found sooner, and conse-quently there will be fewer chances for PUNDIT to find a syn-tactically and semantically acceptable sentence in the N-bestwhich differs crucially from what was uttered.
On the otherhand, PUNDIT actually does better with the poorer speech rec-ogniser output from MIT when the reference answer is not inthe N-best.
We suspect hat the poorer speech recognizer out-put is in some sense easier to reject; that is, it is more likelyto seriously violate the syntactic and semantic onstraints ofEnglish.
If this is so then it is possible that a relatively ac-cepting natural anguage system might work wen with worsespeech recognition outputs (because ven a relatively accept-ing natural language system can reject very had inputs), butwith better speech recognizer output one might get good per-formance with a stricter natural language system.
We plan totest this hypothesis in future research.It is natural to ask why we should care about what to dowith poorer speech recognizer output; one would tlllnlc thatwe should use the best recognizer output possible.
The answeris that many potential applications have requirements such aslarge vocabulary size which are somewhat at odds with highaccuracy, consequently the best recognizer output availablemay nevertheless berelatively inaccurate.
Thus it is importantto have speech/natu.~al language integration strategies whichallow us to fine tune the interaction to compensate for lessaccurate speech recognition.Opt ional  Class A We used both the Unisys-MIT systemand the UUIsys-BBN system for this test.
For both speechrecognizers in this test of eleven utterances with verbal dele-tions we received two T's and sero F's for a weighted score of18.2%.
There is too little test data in this condition to drawreliable conclusions from the results.Compar i son  of  Spoken Language SystemsWe believe coupling of a single natural language system withmultiple speech recognition systems has the potential for beinga very useful technique for comparing speech recognizers in aspoken language context.
Of course speech recognizers can hecompared on the basis of word and sentence accuracy, but wedo not know how direct the mapping is between these mea-sures of performance and spoken language performance.
Themost direct comparision for spoken language valuation, then,is to define an experimental condition in which the systems tobe compared iffer only in the speech recognition component.Not only is this strategy useful for comparing system levelmeasurements of performance of speech recognizers, hut it isalso useful for more fine grained analyses of the interactionbetween the speech recognition component and the naturallanguage system.Figure 3 shows the distribution of T's, F's and NA's forspecific queries across the three systems.Note that for 52 queries, or 36~ of the total, the systemsreceived the same score, although in no case did all three sys-tents receive an "F ' .
The largest difference among the threesystems was in the number of cases where Unisys-BBN re-ceived a "T" but the other two systems received an "NA'.This occurred for 31 queries.Another interesting comparison is to look at the cases whereUnisys-MIT and Unisys-BBN issued a call based on the ffi~stcandidate in the N-best, since this corresponds to the one-bestinterface used in Unisys-LL.
In Unisys-MIT twenty-seven callswere issued based on the first candidate, out of a total of 45cans.
Of the calls issued on the first candidate, 15 received ascore of T and 12 received a score of F, for a weighted scoreof 2%.
In Unisys-BBN the first candidate was selected fromthe N-best 70% of the time.
26 of these candidates resultedin scores of "F" and 42 resulted in a "T" for a weighted scoreof 11%.Overall, the number of calls made was quite similar for theUnisys-LL and Unisys-MIT systems (25% of utterances forUnisys-LL and 30% for Unisys-MIT), but it was much higherfor Unisys-BBN (67%).
In all three systems most of the in-puts were rejected by the syntax component (59% of all inputsfor Unisys-LL, 74% of all inputs for Unisys-BBN and 85% ofall inputs for Unisys-MIT).
We can compare this to a base-line syntactic falluxe of 14% of inputs on the Unisys naturallanguage test.
(Note that since multiple inputs per utteranceare possible with the N-best systems, the N-best vs. one-bestsystems are not strictly comparable.
)Speech Recogn i t ion  Eva luat lonsUsing speech recognition data from MIT, we submitted re-suits for the Class A, Class D1, Class AO and Class D10speech recognition tests, shown in tables 4, 5, 6, and 7.As expected, we observed a higher error rate for the op-tional tests, which contained verbal deletions, and we alsoobserved a wide range of performance across speakers.
Thecomparison ofD1 pales and Class A speech recognition showedpoorer word recognition in the D1 pairs than in the Class Atest.
An average 45.8% word error rate was observed for theClass A utterances compared to a 54.6% error rate for the D1utterances.
As tables 4 and 6 show, this was fairly consis-tent across speakers, except for speaker CJ.
There are at leasttwo hypotheses which may explain this higher error in contextdependent spontaneous tterances.
One hypothesis uggeststhat the higher error rate may be due in part to the presenceof prosodic phenomena common in dialog such as destress-ing of "old" information.
Because the specific dialog contextaffects the pronunciation of words corresponding to old andnew information, the training data used so far may not pro-vide a complete sample of how words are pronounced in a widerange of dialog contexts, consequently leading to poorer wordrecognition.
Another hypothesis i  based on the fact that thecontext-dependent sentences contain many references to flightnumbers.
Flight numbers may be difficult to recognize he-cause there is very little opportunity for syntactic or semanticinformation to constrain which number was uttered.CONCLUSIONSIn this paper we presented benchmark test results on nat-ural language understanding, spoken language understanding131Speaker \ [Co, ,  Sub \[ Del I Ins I E~ S. Err ICE 56.0 33.3 10.6 \ ]  5.1 49.1 95.0CH 47.4 44.7 7.9 23 .7  76.3 100.0CI 45.6 46,8 7,6 24,0 78.4 100.0CJ 75.8 18.3 5.9 3.1 27.3 84.6CK 56 .9  29.4 13.7 2.0 45.1 91.7CL 75.0 22.5 2.5 9,7 34.7 84.6CM 31.1 62.1 6.8 15.9 84.8 100.0CO 74.1 19.1 6.8 8.6 34.6 100.0CP !
71.7 26.5 1.8 7.5 35.8 88 .9I Average I 63.5 30.3 I 6.2 I 9.3 \] 45.8 91.2 ITable 4: Sys tem Summary  Percentages  by  Speakerfo r  C lass  A\ ]Speaker I Corr I Sub \ ]De l  I Ins \] Err \ ]S .
ErrCE 59.5 40.5 0.0 16.2 56.8 100.0c I  21.9 71.9 6.2 21.9 lOO.O lOO.OCJ 72.6 17.7 9.7 1.6 29.0 100.0CM 42.7 !
50.7 6.7 21.3 78.7 100.0\ [Average I 51.5 \[42:2 6.3 I 14.6 \[ 63.1 I 100.0Table 5: Sys tem Summary  Percentages  by  Speakerfo r  C lass  .4,0Speaker \] Corr Sub I De\] I Ins Err S. Err \[CH 40.0 53.3 6.7 13.3 73.3 100.0CI 24.2 46.3 29.5 15.8 91.6 100.0CJ 83.9 10.7 5.4 3.6 19.6 50.0CK 48.0 39.9 12.2 2.7 54.7 100.0CL 66.7 31.7 1.7 8.3 41.7 83.3CO 54.7 27.3 18.0 3.1 48.4 94.4CP 80.0 20.0 0.0 8.0 28.0 100.0Average \[ 52.0 34.1 I 13.9 \[ 6.6 54.6 91.4Table 6: Sys tem Summary  Percentages  by  Speakerfo r  C lass  D1Speaker \[ Cot, I Sub \] Del \[ Ins Err S. ErrCM { { \[ \[ 34.8 65.2 0.0 26.1 91.3 100.0Average \[ 59.6 \[ 40.4 \[ 0.0 \[ 15.8 56.1 100.0Table 7: Sys tem Summary  Percentages  by  Speakerfo r  C lass  D10and speech recognition.
Our weighted score for the Class Anatural language test was 48.3%, for the D1 pairs, 63.2%,for the Class AO test, 9.1% and for the DO test, -50%.We presented five benchmark tests of spoken language sys-tems, Unisys-MIT on Class A, which received a weightedscore of 9.7%, Unisys-MIT on Class AO, which received aweighted score of 18.2~, Unlsys-LL on Class A, which receiveda weighted score of 18.6%, Unisys-BBN on Class A, which re-ceived a weighted score of 39.6%, and Unlsys-BBN on ClassAO, which received a weighted score of 18.2%.
Fined\]y, wepresented speech recognition results using the Unisys naturallanguage system as a filter on the N-best output of the MITSUMMIT system.The semantics enhancements o the natural language sys-tem are motivating us to revisit the tightly integrated archl-tecture of semantics/pragmatics processing in our system, be-cause with these enhancements, semantic information regard-ing a discourse entity can become available to the processingat a much later point than previously.
Thus, pragmatic pro-cessing must be invoked at a later point to ensure that ed\]relevant semantic information has been exploited.The spoken language results are especially interesting, be-cause we are now beginning to be able to look at the inter-actions of the natural language system with different speechrecognisers,, and see how to tune the natural language systemto make the best use of the information available from thevarious speech recognlsers.
We believe that it is importantto make these kinds of comparisons and we are planning towork with at least one other speech recognition system usingthe N-best interface.
We also plan to begin exploring moretightly coupled systems using the stack decoder architecture(\[9\]).REFERENCES\[1\] Deborah A. Dald, Lynette Hirschman, Lewis M. Norton,Marcia C. Linebarger, David Magerman, and Cather-ine N. Ball.
Training and evaluation of spoken languageunderstanding system.
In Peoceedings of the DARPASpeech and Language Workshop, Hidden Valley, PA, June1990.\[2\] Charles T. Hemphill, John J. Godfrey, and George R.Doddington.
The ATIS spoken language systems pilot cor-pus.
In Proceedings of the DARPA Speech and LanguageWorkshop, Hidden Valley, PA, June 1990.\[3\] L. Hirschman, M. PaLmer, J. Dowding, D. Dahl,M.
Linebarger, R. Passonneau, F.-M. Lang, C. Ball, andC.
Weir.
The PUNDIT natural-language processing sys-tem.
In AI Systems in Government Conf.
Computer So-ciety of the IEEE, March 1989.\[4\] F. Kubala, S. Austin, C. Barry, J. Makhoul, P. Placeway,and R. Schwaxts.
BYBLOS speech recognition benclmmrkresults.
In Proceedings of the Darpa Speech and NaturalLanguage Workshop, Asilomar, CA, February 1991.\[5\] F.-M. Lang and L. Hirschman.
Improved portability andparsing through interactive acquisition of semantic infor-mation.
In Proc.
of the Second Conf.
on Applied NaturalLanguage Processing, Austin, TX, February 1988.132\[6\] Lewis M. Norton, Deborah A. Dahl, Donald P. McKay,Lynette Hirsclunan, Marcla C. Linebarger, David MaKer-man, and.
Catherine N. Ball.
Management and evaluationof interactive dialog in the air travel domain.
In Pro-ceedings of the DARPA Speech and Language Workshop,Hidden Valley, PA, 3une 1990.\[7\] Ma~tha Palmer.
Semantic Processing for" Finite Domains.Cambridge University Press, Cambridge, England, 1990.\[8\] D. B. Paul.
New results with the Lincoln tied-mixtureHMM CSI~ system.
In Proceedings ofthe DARPA Speechand Natural Language WorI~shop, February 1991.\[9\] Douglas B. Paul.
A CS11-NL interface specification.
InProceedinga of the DARPA Speech and Natural LanguageWort~shop, 1989.\[10\] V. Zue, 3.
Glass, D. Goddeau, Dave Goodine, LynetteHixsclunan, H. Leung, M. Phillips, 3.
Polifxonl, andS.
Seneff.
Development and pre\]hninv~y evaluation of theMIT ATIS system.
In Proceedings of the DARPA Speechand Natural Language Workshop, February 3991.BBN-TMITBBN-F1Mrr  INA Lincoln LabsT F NA .. ~.... ..F 2 1 4NA 15 1 31BBN-NAMf f  FNALincoln LabsT F NA1 0 i 0o ~ i i  si1 2 10Uncoln LabsT F NA0 0 41 0 2?
-.- -.
:-x~o 1 ~iQuery by query ?omparlton of results for three speech reoognlzers with Unlsys NLShaded ce~ represent agreement arnoag all three systemsFigure 3: Compar i son  o f  resu l t s  f rom three  spokenlanguage sys tems133
