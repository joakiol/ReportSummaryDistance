Proceedings of the 25th International Conference on Computational Linguistics, pages 124?125,Dublin, Ireland, August 23-29 2014.Towards automatic annotation of communicative gesturingKristiina JokinenUniversity of TartuEstoniakristiina.jokinen@ut.eeGraham WilcockUniversity of HelsinkiFinlandgraham.wilcock@helsinki.fiAbstractWe report on-going work on automatic annotation of head and hand gestures in videos of conversational inter-action.
The Anvil annotation tool was extended by two plugins for automatic face and hand tracking.
The resultsof automatic annotation are compared with the human annotations on the same data.1 IntroductionHand and head movements are important in human communication as they not only accompanyspeech to emphasize the message, but also coordinate and control the interaction.
However, videoanalysis of human behaviour is a slow and resource-consuming procedure even by trained annotatorsusing tools such as Anvil (Kipp 2001).
There is an urgent need for more advanced tools to speed upthe process by performing higher-level annotation functions automatically.We use two Anvil plugins, a face tracker (Jongejan 2012) and a hand tracker (Saatmann 2014), thatautomatically create annotations for head and hand movements.
Objects are recognized based on visu-al  features  such  as  colour  and  texture,  and  Haar-liked  digital  image  features,  using  OpenCV frame-work.
Motion trajectories are estimated by calculating the mean velocity and acceleration during thetime span of a set of frames (we experimented with 7 frames as more than 10 makes the algorithm in-sensitive for quick, short movements).
Movement annotations with respect to velocity and accelerationare marked on the appropriate Anvil track, to indicate the movement and its start and stop.
The inter-face has controls for minimum saturation threshold and for how many frames to skip (Figure 1).Figure 1 Anvil interface of the new hand tracker plugin.2 Comparison of human and automatic annotationsCompared with human annotation the trackers are good at detecting some movements but prone tomis-detecting other movements.
Problems occurred e.g.
when the hue of the hands was similar to thebackground colour, or if the direction of the movement is reversed quickly, so that the time span is notlong enough to detect velocity up to the thresholds (short head movements).
Acceleration annotationdid not recognize movements if they start and stop slowly.
Changing the detection threshold can im-prove results, but is a trade-off as it prevents small movements being detected.
However, the pluginswill be of great help in multimodal analysis.
Using the plugins reduces the time spent on annotatingthese movements, which in turn results annotations in increased productivity.This work is licensed under a Creative Commons Attribution 4.0 International Licence.
Page numbers and proceedings footerare added by the organisers.
Licence details: http://creativecommons.org/licenses/by/4.0/124Here we present a more detailed analysis of the human and automatic annotations with reference toface tracking.
The annotated hand and head movements are listed in Table 1.
From the collected datawe used four sample videos, each about six minutes long, altogether 45 303 frames.
Table 2 shows thenumber of elements automatically recognized using velocity and acceleration, with precision scores,i.e.
manually annotated gestures correctly recognized by the automatic annotation.Head movements Hand movementsNod down Backward Waggle BothNod up Forward Shake SingleTurn sideways Tilt Other ComplexOtherTable 1.
Annotation features for head and hand movements.Gesture Manual annotation Velocity  AccelerationNodDown 149 110  (74%) 108  (72%)NodUp 42 15   (36%) 27   (64%)TurnSide 40 29   (73%) 27   (68%)HeadBackward 27 18   (67%) 14   (52%)HeadForward 21 17   (81%) 18   (86%)Tilt 57 35   (61%) 29   (51%)Waggle 12 11   (92%) 8   (67%)HeadOther 3 2   (67%) 1   (33%)Total 351 237  (73%) 232  (66%)Table 2.
Manual and automatic head movement annotations for 4 videos.Precision: Velocity 73%, Acceleration 66%Figure 2 shows two examples of the annotation results on the Anvil annotation board, one where theface tracker recognized head movements appropriately, and one where the face tracker ?invented?movements which the human annotator does not recognize as communicative gestures.Figure 2.
Face tracker detecting manual annotation categories (left) and inventing face movements (right).3 Future workFollowing the work outlined in Jokinen and Scherer (2012), we will compare the top-down linguistic-pragmatic analysis of movements with the bottom-up signal-level observations.
We will also use amachine-learning approach to analyse if there are any systematics with the problematic cases.
We mayalso explore if a recognized movement can be automatically interpreted with respect to communicativeintentions.
In human-robot interaction, the automatic gesture recognition model can be used to studythe robot?s understanding of the situation and of human control gestures, cf.
Han et al.
(2012).ReferencesHan, J., Campbell, N., Jokinen, K. and Wilcock, G. (2012).
Investigating the use of non-verbal cues in human-robot interaction with a Nao robot, in Proceedings of 3rd IEEE International Conference on Cognitive Info-communications (CogInfoCom 2012), Kosice, 679-683.Jokinen, K. and Scherer S. (2012).
Embodied Communicative Activity in Cooperative Conversational Interac-tions - studies in Visual Interaction Management.
Acta Polytechnica Hungarica.
9(1), pp.
19-40.Jongejan, B.
(2012) Automatic annotation of face velocity and acceleration in Anvil.
Proceedings of the Lan-guage Resources and Evaluation Conference (LREC-2012).
Istanbul, Turkey.Kipp, M. (2001).
Anvil ?
A generic annotation tool for multimodal dialogue.
Proceedings of the Seventh Euro-pean Conference on Speech Communication and Technology, pp.
1367-1370.Saatmann, P. (2014).
Experiments With Hand-tracking Algorithm in Video Conversations.
Proceedings of the5th Nordic Symposium on Multimodal Communication.125
