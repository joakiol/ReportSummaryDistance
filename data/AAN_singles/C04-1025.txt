A grammar formalism and parser for linearization-based HPSGMichael W. Daniels and W. Detmar MeurersDepartment of LinguisticsThe Ohio State University222 Oxley Hall1712 Neil AvenueColumbus, OH 43210daniels|dm@ling.osu.eduAbstractLinearization-based HPSG theories are widelyused for analyzing languages with relativelyfree constituent order.
This paper introducesthe Generalized ID/LP (GIDLP) grammar for-mat, which supports a direct encoding of suchtheories, and discusses key aspects of a parserthat makes use of the dominance, precedence,and linearization domain information explicitlyencoded in this grammar format.
We showthat GIDLP grammars avoid the explosion inthe number of rules required under a traditionalphrase structure analysis of free constituent or-der.
As a result, GIDLP grammars support moremodular and compact grammar encodings andrequire fewer edges in parsing.1 IntroductionWithin the framework of Head-Driven Phrase Struc-ture Grammar (HPSG), the so-called linearization-based approaches have argued that constraints onword order are best captured within domains thatextend beyond the local tree.
A range of analysesfor languages with relatively free constituent orderhave been developed on this basis (see, for example,Reape, 1993; Kathol, 1995; Mu?ller, 1999; Donohueand Sag, 1999; Bonami et al, 1999) so that it is at-tractive to exploit these approaches for processinglanguages with relatively free constituent order.This paper introduces a grammar format that sup-ports a direct encoding of linearization-based HPSGtheories.
The Generalized ID/LP (GIDLP) formatexplicitly encodes the dominance, precedence, andlinearization domain information and thereby sup-ports the development of efficient parsing algorithmmaking use of this information.
We make thisconcrete by discussing key aspects of a parser forGIDLP grammars that integrates the word order do-mains and constraints into the parsing process.2 Linearization-based HPSGThe idea of discontinuous constituency was first in-troduced into HPSG in a series of papers by MikeReape (see Reape, 1993 and references therein).1The core idea is that word order is determined notat the level of the local tree, but at the newly intro-duced level of an order domain, which can includeelements from several local trees.
We interpret thisin the following way: Each terminal has a cor-responding order domain, and just as constituentscombine to form larger constituents, so do their or-der domains combine to form larger order domains.Following Reape, a daughter?s order domainenters its mother?s order domain in one of twoways.
The first possibility, domain union, formsthe mother?s order domain by shuffling together itsdaughters?
domains.
The second option, domaincompaction, inserts a daughter?s order domain intoits mother?s.
Compaction has two effects:Contiguity: The terminal yield of a compactedcategory contains all and only the terminal yield ofthe nodes it dominates; there are no holes or addi-tional strings.LP Locality: Precedence statements only con-strain the order among elements within the samecompacted domain.
In other words, precedenceconstraints cannot look into a compacted domain.Note that these are two distinct functions of do-main compaction: defining a domain as covering acontiguous stretch of terminals is in principle in-dependent of defining a domain of elements forLP constraints to apply to.
In linearization-basedHPSG, domain compaction encodes both aspects.Later work (Kathol and Pollard, 1995; Kathol,1995; Yatabe, 1996) introduced the notion of partialcompaction, in which only a portion of the daugh-ter?s order domain is compacted; the remaining ele-ments are domain unioned.1Apart from Reape?s approach, there have been proposalsfor a more complete separation of word order and syntacticstructure in HPSG (see, for example, Richter and Sailer, 2001and Penn, 1999).
In this paper, we focus on the majority oflinearization-based HPSG approaches, which follow Reape.3 Processing linearization-based HPSGFormally, a theory in the HPSG architecture con-sists of a set of constraints on the data structuresintroduced in the signature; thus, word order do-mains and the constraints thereon can be straight-forwardly expressed.
On the computational side,however, most systems employ parsers to efficientlyprocess HPSG-based grammars organized around aphrase structure backbone.
Phrase structure rulesencode immediate dominance (ID) and linear prece-dence (LP) information in local trees, so they cannotdirectly encode linearization-based HPSG, whichposits word order domains that can extend the lo-cal trees.The ID/LP grammar format (Gazdar et al, 1985)was introduced to separate immediate dominancefrom linear precedence, and several proposals havebeen made for direct parsing of ID/LP grammars(see, for example, Shieber, 1994).
However, thedomain in which word order is determined still isthe local tree licensed by an ID rule, which is insuf-ficient for a direct encoding of linearization-basedHPSG.The LSL grammar format as defined by Suhre(1999) (based on Go?tz and Penn, 1997) allows el-ements to be ordered in domains that are largerthan a local tree; as a result, categories are not re-quired to cover contiguous strings.
Linear prece-dence constraints, however, remain restricted to lo-cal trees: elements that are linearized in a wordorder domain larger than their local tree cannotbe constrained.
The approach thus provides valu-able worst-case complexity results, but it is inade-quate for encoding linearization-based HPSG theo-ries, which crucially rely on the possibility to ex-press linear precedence constraints on the elementswithin a word order domain.In sum, no grammar format is currently availablethat adequately supports the encoding of a process-ing backbone for linearization-based HPSG gram-mars.
As a result, implementations of linearization-based HPSG grammars have taken one of two op-tions.
Some simply do not use a parser, such as thework based on ConTroll (Go?tz and Meurers, 1997);as a consequence, the efficiency and terminationproperties of parsers cannot be taken for granted insuch approaches.The other approaches use a minimal parser thatcan only take advantage of a small subset of the req-uisite constraints.
Such parsers are typically limitedto the general concept of resource sensitivity ?
ev-ery element in the input needs to be found exactlyonce ?
and the ability to require certain categoriesto dominate a contiguous segment of the input.Some of these approaches (Johnson, 1985; Reape,1991) lack word order constraints altogether.
Others(van Noord, 1991; Ramsay, 1999) have the gram-mar writer provide a combinatory predicate (suchas concatenate, shuffle, or head-wrap) for each rulespecifying how the string coverage of the mother isdetermined from the string coverages of the daugh-ter.
In either case, the task of constructing a wordorder domain and enforcing word order constraintsin that domain is left out of the parsing algorithm;as a result, constraints on word order domains eithercannot be stated or are tested in a separate clean-upphase.4 Defining GIDLP Grammars2To develop a grammar format for linearization-based HPSG, we take the syntax of ID/LP rulesand augment it with a means for specifying whichdaughters form compacted domains.
A GeneralizedID/LP (GIDLP) grammar consists of four parts: aroot declaration, a set of lexical entries, a set ofgrammar rules, and a set of global order constraints.We begin by describing the first three parts, whichare reminiscent of context-free grammars (CFGs),and then address order constraints in section 4.1.The root declaration has the form root(S , L) andstates the start symbol S of the grammar and anylinear precedence constraints L constraining the rootdomain.Lexical entries have the form A ?
t and link thepre-terminal A to the terminal t, just as in CFGs.Grammar rules have the form A ?
?;C.
Theyspecify that a non-terminal A immediately domi-nates a list of non-terminals ?
in a domain wherea set of order constraints C holds.Note that in contrast to CFG rules, the order ofthe elements in ?
does not encode immediate prece-dence or otherwise contribute to the denotationalmeaning of the rule.
Instead, the order can be usedto generalize the head marking used in grammarsfor head-driven parsing (Kay, 1990; van Noord,1991) by additionally ordering the non-head daugh-ters.32Due to space limitations, we focus here on introducing thesyntax of the grammar formalism and giving an example.
Wewill also base the discussion on simple term categories; noth-ing hinges on this, and when using the formalism to encodelinearization-based HPSG grammars, one will naturally use thefeature descriptions known from HPSG as categories.3By ordering the right-hand side of a rule so that those cate-gories come first that most restrict the search space, it becomespossible to define a parsing algorithm that makes use of thisinformation.
For an example of a construction where order-ing the non-head daughters is useful, consider sentences withAcI verbs like I see him laugh.
Under the typical HPSG analy-If the set of order constraints is empty, we obtainthe simplest type of rule, exemplified in (1).
(1) S?
NP, VPThis rule says that an S may immediately dominatean NP and a VP, with no constraints on the relativeordering of NP and VP.
One may precede the other,the strings they cover may be interleaved, and mate-rial dominated by a node dominating S can equallybe interleaved.4.1 Order ConstraintsGIDLP grammars include two types of order con-straints: linear precedence constraints and com-paction statements.4.1.1 Linear Precedence ConstraintsLinear precedence constraints can be expressed intwo contexts: on individual rules (as rule-level con-straints) and in compaction statements (as domain-level constraints).
Domain-level constraints canalso be specified as global order constraints, whichhas the effect that they are specified for each singledomain.All precedence constraints enforce the followingproperty: given any appropriate pair of elements inthe same domain, one must completely precede theother for the resulting parse to be valid.
Precedenceconstraints may optionally require that there be nointervening material between the two elements: thisis referred to as immediate precedence.
Precedenceconstraints are notated as follows:?
Weak precedence: A<B.?
Immediate precedence: AB.A pair of elements is considered appropriatewhen one element in a domain matches the symbolA, another matches B, and neither element domi-nates the other (it would otherwise be impossible toexpress an order constraint on a recursive rule).The symbols A and B may be descriptions or to-kens.
A category in a domain matches a descrip-tion if it is subsumed by it; a token refers to a spe-cific category in a rule, as discussed below.
A con-straint involving descriptions applies to any pair ofelements in any domain in which the described cat-egories occur; it thus can also apply more than oncewithin a given rule or domain.
Tokens, on the otherhand, can only occur in rule-level constraints andsis (Pollard and Sag, 1994), see combines in a ternary structurewith him and laugh.
Note that the constituent that is appropriatein the place occupied by him here can only be determined onceone has looked at the other complement, laugh, from which itis raised.refer to particular RHS members of a rule.
In thispaper, tokens are represented by numbers referringto the subscripted indices on the RHS categories.In (2) we see an example of a rule-level linearprecedence constraint.
(2) A?
NP1, V2, NP3; 3<VThis constraint specifies that the token 3 in the rule?sRHS (the second NP) must precede any constituentsdescribed as V occurring in the same domain (thisincludes, but is not limited to, the V introduced bythe rule).4.1.2 Compaction StatementsAs with LP constraints, compaction statements ex-ist as rule-level and as global order constraints; theycannot, however, occur within other compactionstatements.
A rule-level compaction statement hasthe form ?
?, A, L?, where ?
is a list of tokens,A is the category representing the compacted do-main, and L is a list of domain-level precedenceconstraints.
Such a statement specifies that the con-stituents referenced in ?
form a compacted domainwith category A, inside of which the order con-straints in L hold.
As specified in section 2, a com-pacted domain must be contiguous (contain all andonly the terminal yield of the elements in that do-main), and it constitutes a local domain for LP state-ments.It is because of partial compaction that the secondcomponent A in a compaction statement is needed.If only one constituent is compacted, the resultingdomain will be of the same category; but when mul-tiple categories are fused in partial compaction, thecategory of the resulting domain needs to be deter-mined so that LP constraints can refer to it.The rule in (3) illustrates compaction: each of theS categories forms its own domain.
In (4) partialcompaction is illustrated: the V and the first NPform a domain named VP to the exclusion of thesecond NP.
(3) S?
S1, Conj2, S3;12, 23, ?
[1], S, ?[]?
?, ?
[3], S, ?[]??
(4) VP?
V1, NP2, NP3; ?
[1, 2], VP, ?[]?
?One will often compact only a single categorywithout adding domain-specific LP constraints, sowe introduce the abbreviatory notation of writingsuch a compacted category in square brackets.
Inthis way (3) can be written as (5).
(5) S?
[S1], Conj2, [S3]; 12, 23A final abbreviatory device is useful when the en-tire RHS of a rule forms a single domain, whichSuhre (1999) refers to as ?left isolation?.
This is de-noted by using the token 0 in the compaction state-ment if linear precedence constraints are attached,or by enclosing the LHS category in square brack-ets, otherwise.
(See rules (13d) and (13j) in sec-tion 6 for an example of this notation.
)The formalism also supports global compactionstatements.
A global compaction statement has theform ?A, L?, where A is a description specifying acategory that always forms a compacted domain,and L is a list of domain-level precedence con-straints applying to the compacted domain.4.2 ExamplesWe start with an example illustrating how a CFGrule is encoded in GIDLP format.
A CFG rule en-codes the fact that each element of the RHS imme-diately precedes the next, and that the mother cat-egory dominates a contiguous string.
The context-free rule in (6) is therefore equivalent to the GIDLPrule shown in (7).
(6) S?
Nom V Acc(7) [S]?
V1, Nom2, Acc3; 21, 13In (8) we see a more interesting example of aGIDLP grammar.
(8) a) root(A, [])b) A?
B1, C2, [D3]; 2<3c) B?
F1, G2, E3d) C?
E1, D2, I3; ?
[1,2], H, ?[]?
?e) D?
J1, K2f) Lexical entries: E?
e, .
.
.g) E<F(8a) is the root declaration, stating that an inputstring must parse as an A; the empty list shows thatno LP constraints are specifically declared for thisdomain.
(8b) is a grammar rule stating that an Amay immediately dominate a B, a C, and a D; it fur-ther states that the second constituent must precedethe third and that the third is a compacted domain.
(8c) gives a rule for B: it dominates an F, a G, and anE, in no particular order.
(8d) is the rule for C, illus-trating partial compaction: its first two constituentsjointly form a compacted domain, which is giventhe name H. (8e) gives the rule for D and (8f) spec-ifies the lexical entries (here, the preterminals justrewrite to the respective lowercase terminal).
Fi-nally, (8g) introduces a global LP constraint requir-ing an E to precede an F whenever both elementsoccur in the same domain.Now consider licensing the string efjekgikj withthe above grammar.
The parse tree, recording whichrules are applied, is shown in (9).
Given that thedomains in which word order is determined can belarger than the local trees, we see crossing brancheswhere discontinuous constituents are licensed.
(9) AB C [D]E F [D E]H G I K JJ Ke f j e k g i k jTo obtain a representation in which the order do-mains are represented as local trees again, we candraw a tree with the compacted domains formingthe nodes, as shown in (10).
(10) AH De f j e k g i k jThere are three non-lexical compacted domainsin the tree in (9): the root A, the compacted D, andthe partial compaction of D and E forming the do-main H within C. In each domain, the global LPconstraint E < F must be obeyed.
Note that thestring is licensed by this grammar even though thesecond occurrence of E does not precede the F. ThisE is inside a compacted domain and therefore is notin the same domain as the F, so that the LP con-straint does not apply to those two elements.
Thisillustrates the property of LP locality: domain com-paction acts as a ?barrier?
to LP application.The second aspect of domain compaction, con-tiguity, is also illustrated by the example, in con-nection with the difference between total and partialcompaction.
The compaction of D specified in (8b)requires that the material it dominates be a contigu-ous segment of the input.
In contrast, the partialcompaction of the first two RHS categories in rule(8d) requires that the material dominated by D andE, taken together, be a continuous segment.
Thisallows the second e to occur between the two cate-gories dominated by D.Finally, the two tree representations above illus-trate the separation of the combinatorial potentialof rules (9) from the flatter word order domains(10) that the GIDLP format achieves.
It would, ofcourse, be possible to write phrase structure rulesthat license the word order domain tree in (10) di-rectly, but this would amount to replacing a set ofgeneral rules with a much greater number of flatterrules corresponding to the set of all possible waysin which the original rules could be combined with-out introducing domain compaction.
Mu?ller (2004)discusses the combinatorial explosion of rules thatresults for an analysis of German if one wants toflatten the trees in this way.
If recursive rules such asadjunction are included ?
which is necessary sinceadjuncts and complements can be freely intermixedin the German Mittelfeld ?
such flattening will noteven lead to a finite number of rules.
We will returnto this issue in section 6.5 A Parsing Algorithm for GIDLPWe have developed a GIDLP parser based on Ear-ley?s algorithm for context-free parsing (Earley,1970).
In Earley?s original algorithm, each edgeencodes the interval of the input string it covers.With discontinuous constituents, however, that is nolonger an option.
In the spirit of Johnson (1985)and Reape (1991), and following Ramsay (1999),we represent edge coverage with bitvectors, storedas integers.
For instance, 00101 represents an edgecovering words one and three of a five-word sen-tence.4Our parsing algorithm begins by seeding the chartwith passive edges corresponding to each word inthe input and then predicting a compacted instanceof the start symbol covering the entire input; eachfinal completion of this edge will correspond to asuccessful parse.As with Earley?s algorithm, the bulk of the workperformed by the algorithm is borne by two steps,prediction and completion.
Unlike the context-freecase, however, it is not possible to anchor thesesteps to string positions, proceeding from left toright.
The strategy for prediction used by Suhre(1999) for his LSL parser is to predict every ruleat every position.
While this strategy ensures thatno possibility is overlooked, it fails to integrate anduse the information provided by the word order con-straints attached to the rules ?
in other words, theparser receives no top-down guidance.
Some of theedges generated by prediction therefore fall prey tothe word order constraints later, in a generate-and-test fashion.
This need not be the case.
Once onedaughter of an active edge has been found, the otherdaughters should only be predicted to occur in stringpositions that are compatible with the word orderconstraints of the active edge.
For example, con-sider the edge in (11).
(11) A?
B1 ?
C2 ; 1<24Note that the first word is the rightmost bit.This notation represents the point in the parse dur-ing which the application of this rule has been pre-dicted, and a B has already been located.
Assumingthat B has been found to cover the third position of afive-word string, two facts are known.
From the LPconstraint, C cannot precede B, and from the gen-eral principle that the RHS of a rule forms a parti-tion of its LHS, C cannot overlap B.
Thus C cannotcover positions one, two, or three.5.1 Compiling LP Constraints into BitmasksWe can now discuss the integration of GIDLP wordorder constraints into the parsing process.
A centralinsight of our algorithm is that the same data struc-ture used to describe the coverage of an edge canalso encode restrictions on the parser?s search space.This is done by adding two bitvectors to each edge,in addition to the coverage vector: a negative mask(n-mask) and a positive mask (p-mask).
Efficientbitvector operations can then be used to compute,manipulate, and test the encoded constraints.Negative Masks The n-mask constrains the set ofpossible coverage vectors that could complete theedge.
The 1-positions in a masking vector representthe positions that are masked out: the positions thatcannot be filled when completing this edge.
The 0-positions in the negative mask represent positionsthat may potentially be part of the edge?s cover-age.
For the example above, the coverage vector forthe edge is 00100 since only the third word B hasbeen found so far.
Assuming no restrictions from ahigher rule in the same domain, the n-mask for C is00111, encoding the fact that the final coverage vec-tor of the edge for A must be either 01000, 10000,or 11000 (that is, C must occupy position four, po-sition five, or both of these positions).
The negativemask in essence encodes information on where theactive category cannot be found.Positive Masks The p-mask encodes informationabout the positions the active category must occupy.This knowledge arises from immediate precedenceconstraints.
For example, consider the edge in (12).
(12) D?
E1 ?
F2 ; 12If E occupies position one, then F must at least oc-cupy position two; the second position in the posi-tive mask would therefore be occupied.Thus in the prediction step, the parser considerseach rule in the grammar that provides the symbolbeing predicted, and for each rule, it generates bit-masks for the new edge, taking both rule-level anddomain-level order constraints into account.
Theresulting masks are checked to ensure that there isenough space in the resulting mask for the minimumnumber of categories required by the rule.5Then, as part of each completion step, the parsermust update the LP constraints of the active edgewith the new information provided by the passiveedge.
As edges are initially constructed from gram-mar rules, all order constraints are initially ex-pressed in terms of either descriptions or tokens.
Asthe parse proceeds, these constraints are updated interms of the actual locations where matching con-stituents have been found.
For example, a constraintlike 1 < 2 (where 1 and 2 are tokens) can be up-dated with the information that the constituent cor-responding to token 1 has been found as the firstword, i.e.
as position 00001.In summary, compiling LP constraints into bit-masks in this way allows the LP constraints to beintegrated directly into the parser at a fundamentallevel.
Instead of weeding out inappropriate parsesin a cleanup phase, LP constraints in this parser canimmediately block an edge from being added to thechart.6 EvaluationAs discussed at the end of section 4.2, it is possibleto take a GIDLP grammar and write out the discon-tinuity.
All non-domain introducing rules must befolded into the domain-introducing rules, and theneach permitted permutation of a RHS must becomea context-free rule on its own ?
generally, at the costof a factorial increase in the number of rules.This construction indicates the basis for a prelim-inary assessment of the GIDLP formalism and itsparser.
The grammar in (13) recognizes a very smallfragment of German, focusing on the free word or-der of arguments and adjuncts in the so-called Mit-telfeld that occurs to the right of either the finite verbin yes-no questions or the complementizer in com-plementized sentences.6(13) a) root(s, [])b) s?
s(cmp)1c) s?
s(que)1d) s(cmp)?
cmp1, clause2;?
[0], s(cmp), ?cmp< , <v( )?
?e) s(que)?
clause1; ?
[0], s(que), ?v( )< ?
?f) clause?
np(n)1, vp25This optimization only applies to epsilon-free grammars.Further work in this regard can involve determining the minu-mum and maximum yields of each category; some opti-mizations involving this information can be found in (Haji-Abdolhosseini and Penn, 2003).6The symbol is used to denote the set of all categories.g) vp?
v(ditr)1, np(a)2, np(d)3h) vp?
adv1, vp2i) vp?
v(cmp)1, s(cmp)2j) [np(Case)]?
det(Case)1, n(Case)2;12k) v(ditr)?
gab q) v(cmp)?
denktl) comp?
dass r) det(nom)?
derm) det(dat)?
der s) det(acc)?
dasn) n(nom)?
Mann t) n(dat)?
Frauo) n(acc)?
Buch u) adv?
gesternp) adv?
dortThe basic idea of this grammar is that domain com-paction only occurs at the top of the head path, af-ter all complements and adjuncts have been found.When the grammar is converted into a CFG, theeffect of the larger domain can only be mimickedby eliminating the clause and vp constituents alto-gether.As a result, while this GIDLP grammar has 10syntactic rules, the corresponding flattened CFG (al-lowing for a maximum of two adverbs) has 201rules.
In an experiment, the four sample sentencesin (14)7 were parsed with both our prototype GIDLPparser (using the GIDLP grammar) as well as avanilla Earley CFG parser (using the CFG); the re-sults are shown in (15).
(14) a) Gab der Mann der Frau das Buch?b) dass das Buch der Mann der Frau gab.c) dass das Buch gestern der Mann dort derFrau gab.d) Denkt der Mann dass das Buch gesternder Mann dort der Frau gab?
(15)Active Edges Passive EdgesSentence GIDLP CFG GIDLP CFGa) 18 327 16 15b) 27 338 18 16c) 46 345 27 27d) 75 456 36 24Averaging over the four sentences, the GIDLPgrammar requires 89% fewer active edges.
It alsogenerates additional passive edges corresponding tothe extra non-terminals vp and clause.
It is impor-tant to keep in mind that the GIDLP grammar ismore general than the CFG: in order to obtain a fi-nite number of CFG rules, we had to limit the num-ber of adverbs.
When using a grammar capable of7The grammar and example sentences are intended as a for-mal illustration, not a linguistic theory; because of this andspace limitations, we have not provided glosses.handling longer sentences with more adverbs, thenumber of CFG rules (and active edges, as a conse-quence) increases factorially.Timings have not been included in (15); it is gen-erally the case that the GIDLP parser/grammar com-bination was slower than the CFG/Earley parser.This is an artifact of the use of atomic categories,however.
For the large feature structures used ascategories in HPSG, we expect the larger numbersof edges encountered while parsing with the CFG tohave a greater impact on parsing time, to the pointwhere the GIDLP grammar/parser is faster.7 SummaryIn this abstract, we have introduced a grammar for-mat that can be used as a processing backbone forlinearization-based HPSG grammars that supportsthe specification of discontinuous constituents andword order constraints on domains that extend be-yond the local tree.
We have presented a prototypeparser for this format illustrating the use of orderconstraint compilation techniques to improve effi-ciency.
Future work will concentrate on additionaltechniques for optimized parsing as well as the ap-plication of the parser to feature-based grammars.We hope that the GIDLP grammar format will en-courage research on such optimizations in general,in support of efficient processing of relatively freeconstituent order phenomena using linearization-based HPSG.ReferencesOlivier Bonami, Danie`le Godard, and Jean-MarieMarandin.
1999.
Constituency and word orderin French subject inversion.
In Gosse Bouma etal., editor, Constraints and Resources in NaturalLanguage Syntax and Semantics.
CSLI.Cathryn Donohue and Ivan A.
Sag.
1999.
Domainsin Warlpiri.
In Abstracts of the Sixth Int.
Confer-ence on HPSG, pages 101?106, Edinburgh.Jay Earley.
1970.
An efficient context-free parsingalgorithm.
Communications of the ACM, 13(2).Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum,and Ivan A.
Sag.
1985.
Generalized PhraseStructure Grammar.
Harvard University Press.Thilo Go?tz and W. Detmar Meurers.
1997.
TheConTroll system as large grammar developmentplatform.
In Proceedings of the EACL Workshop?Computational Environments for Grammar De-velopment and Linguistic Engineering?, Madrid.Thilo Go?tz and Gerald Penn.
1997.
A proposedlinear specification language.
Volume 134 in Ar-beitspapiere des SFB 340, Tu?bingen.Mohammad Haji-Abdolhosseini and Gerald Penn.2003.
ALE reference manual.
Univ.
Toronto.Mark Johnson.
1985.
Parsing with discontinuousconstituents.
In Proceedings of ACL, Chicago.Andreas Kathol and Carl Pollard.
1995.
Extraposi-tion via complex domain formation.
In Proceed-ings of ACL, pages 174?180, Boston.Andreas Kathol.
1995.
Linearization-Based Ger-man Syntax.
Ph.D. thesis, Ohio State University.Martin Kay.
1990.
Head-driven parsing.
In MasaruTomita, editor, Current Issues in Parsing Tech-nology.
Kluwer, Dordrecht.Stefan Mu?ller.
1999.
Deutsche Syntax deklarativ.Niemeyer, Tu?bingen.Stefan Mu?ller.
2004.
Continuous or discontinuousconstituents?
A comparison between syntacticanalyses for constituent order and their process-ing systems.
Research on Language and Compu-tation, 2(2):209?257.Gerald Penn.
1999.
Linearization and WH-extraction in HPSG: Evidence from Serbo-Croatian.
In Robert D. Borsley and AdamPrzepio?rkowski, editors, Slavic in HPSG.
CSLI.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven Phrase Structure Grammar.
University ofChicago Press, Chicago.Allan M. Ramsay.
1999.
Direct parsing with dis-continuous phrases.
Natural Language Engi-neering, 5(3):271?300.Mike Reape.
1991.
Parsing bounded discontinuousconstituents: Generalisations of some commonalgorithms.
In Mike Reape, editor, Word Orderin Germanic and Parsing.
DYANA R1.1.C.Mike Reape.
1993.
A Formal Theory of Word Or-der: A Case Study in West Germanic.
Ph.D. the-sis, University of Edinburgh.Frank Richter and Manfred Sailer.
2001.
On the leftperiphery of German finite sentences.
In W. Det-mar Meurers and Tibor Kiss, editors, Constraint-Based Approaches to Germanic Syntax.
CSLI.Stuart M. Shieber.
1984.
Direct parsing of ID/LPgrammars.
Linguistics & Philosophy, 7:135?154.Oliver Suhre.
1999.
Computational aspects ofa grammar formalism for languages with freerword order.
Diplomarbeit.
(= Volume 154 in Ar-beitspapiere des SFB 340, 2000).Gertjan van Noord.
1991.
Head corner parsing fordiscontinuous constituency.
In ACL Proceedings.Shuichi Yatabe.
1996.
Long-distance scramblingvia partial compaction.
In Masatoshi Koizumi,Masayuki Oishi, and Uli Sauerland, editors,Formal Approaches to Japanese Linguistics 2.MITWPL.
