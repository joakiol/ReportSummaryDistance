Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 364?372,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Skip-Chain Conditional Random Field forRanking Meeting Utterances by Importance?Michel GalleyColumbia UniversityDepartment of Computer ScienceNew York, NY 10027, USAgalley@cs.columbia.eduAbstractWe describe a probabilistic approach to content se-lection for meeting summarization.
We use skip-chain Conditional Random Fields (CRF) to modelnon-local pragmatic dependencies between pairedutterances such as QUESTION-ANSWER that typi-cally appear together in summaries, and show thatthese models outperform linear-chain CRFs andBayesian models in the task.
We also discuss dif-ferent approaches for ranking all utterances in a se-quence using CRFs.
Our best performing systemachieves 91.3% of human performance when evalu-ated with the Pyramid evaluation metric, which rep-resents a 3.9% absolute increase compared to ourmost competitive non-sequential classifier.1 IntroductionSummarization of meetings faces many challengesnot found in texts, i.e., high word error rates, ab-sence of punctuation, and sometimes lack of gram-maticality and coherent ordering.
On the otherhand, meetings present a rich source of structuraland pragmatic information that makes summariza-tion of multi-party speech quite unique.
In par-ticular, our analyses of patterns in the verbal ex-change between participants found that adjacencypairs (AP), a concept drawn from the conver-sational analysis literature (Schegloff and Sacks,1973), have particular relevance to summarization.APs are pairs of utterances such as QUESTION-ANSWER or OFFER-ACCEPT, in which the secondutterance is said to be conditionally relevant on thefirst.
We show that there is a strong correlation be-tween the two elements of an AP in summariza-tion, and that one is unlikely to be included if theother element is not present in the summary.Most current statistical sequence models in nat-ural language processing (NLP), such as hidden?This material is based on research supported in part bythe U.S. National Science Foundation (NSF) under GrantsNo.
IIS-0121396 and IIS-05-34871, and the Defense Ad-vanced Research Projects Agency (DARPA) under ContractNo.
HR0011-06-C-0023.
Any opinions, findings and con-clusions or recommendations expressed in this material arethose of the author and do not necessarily reflect the views ofthe NSF or DARPA.Markov models (HMMs) (Rabiner, 1989), are lin-ear chains that only encode local dependenciesbetween utterances to be labeled.
In multi-partyspeech, the two elements of an AP are gener-ally arbitrarily distant, and such models can onlypoorly account for dependencies underlying APsin summarization.
We use instead skip-chain se-quence models (Sutton and McCallum, 2004),which allow us to explicitly model dependenciesbetween distant utterances, and turn out to be par-ticularly effective in the summarization task.In this paper, we compare two types of networkstructures?linear-chain and skip-chain?and twotypes of network semantics?Bayesian Networks(BNs) and Conditional Random Fields (CRFs).We discuss the problem of estimating the classposterior probability of each utterance in a se-quence in order to extract the N most proba-ble ones, and show that the cost assigned by aCRF to each utterance needs to be locally nor-malized in order to outperform BNs.
After ana-lyzing the predictive power of a large set of dura-tional, acoustical, lexical, structural, and informa-tion retrieval features, we perform feature selec-tion to have a competitive set of predictors to testthe different models.
Empirical evaluations usingtwo standard summarization metrics?the Pyra-mid method (Nenkova and Passonneau, 2004b)and ROUGE (Lin, 2004)?show that the bestperforming system is a CRF incorporating bothorder-2 Markov dependencies and skip-chain de-pendencies, which achieves 91.3% of human per-formance in Pyramid score, and outperforms ourbest-performing non-sequential model by 3.9%.2 CorpusThe work presented here was applied to the ICSIMeeting Corpus (Janin et al, 2003), a corpusof ?naturally-occurring?
meetings, i.e.
meetingsthat would have taken place anyway.
Their styleis quite informal, and topics are primarily con-cerned with speech, natural language, artificial364intelligence, and networking research.
The cor-pus contains 75 meetings, which are 60 minuteslong on average, and involve a number of partic-ipants ranging from 3 to 10 (6 on average).
Thetotal number of unique speakers is 60, includ-ing 26 non-native English speakers.
Experimentsin this paper are based either on human ortho-graphic transcriptions or automatic speech recog-nition output, which were available for all meet-ings.
For automatic recognition, we used the ICSI-SRI-UW speech recognition system (Mirghaforiet al, 2004), a state-of-the-art conversational tele-phone speech (CTS) recognizer whose languageand acoustic models were adapted to the meetingdomain.
It achieves 34.8% WER on the ICSI cor-pus, which is indicative of the difficulty involvedin processing meetings automatically.We also used additional annotation that hasbeen developed to support higher-level analyses ofmeeting structure, in particular the ICSI MeetingRecorder Dialog act (MRDA) corpus (Shriberg etal., 2004).
Dialog act (DA) labels describe thepragmatic function of utterances, e.g.
a STATE-MENT or a BACKCHANNEL.
This auxiliary cor-pus consists of over 180,000 human-annotateddialog act labels (?
= .8), for which so-calledadjacency pair (AP) relations (e.g., APOLOGY-DOWNPLAY) were also labeled.
This latter anno-tation was used to train an AP classifier that is in-strumental in automatically determining the struc-ture of our sequence models.
Note that, in the caseof three or more speakers, adjacency pair is ad-mittedly an unfortunate term, since labeled APsare generally not adjacent (e.g., see Table 1), butwe will nevertheless use the same terminology toenforce consistency with previous work.To train and evaluate our summarizer, we useda corpus of extractive summaries produced at theUniversity of Edinburgh (Murray et al, 2005).
Foreach of the 75 meetings, human judges were askedto select transcription utterances segmented by DAto include in summaries, resulting in an averagecompression ratio of 6.26% (though no strict limitwas imposed).
Inter-labeler agreement was mea-sured using six meetings that were summarized bymultiple coders (average ?
= .323).
While thislevel of agreement is quite low, this situation isnot uncommon to summarization, since there maybe many good summaries for a given document;a main challenge lies in using evaluation schemesthat properly accounts for this diversity.3 Content selectionState sequence Markov models such as hiddenMarkov models (Rabiner, 1989) have been highlysuccessful in many speech and natural languageprocessing applications, including summarization.Following an intuition that the probability of agiven sentence may be locally conditioned on theprevious one, Conroy (2004) built a HMM-basedsummarizer that consistently ranked among thetop systems in recent Document UnderstandingConference (DUC) evaluations.Inter-sentential influences become more com-plex in the case of dialogues or correspondences,especially when they involve multiple parties.In the case of summarization of conversationalspeech, Zechner (2002) found, for instance, thata simple technique consisting of linking togetherquestions and answers in summaries?and thuspreventing the selection of orphan questions oranswers?significantly improved their readabilityaccording to various human summary evaluations.In email summarization (Rambow et al, 2004),Shrestha and McKeown (2004) obtained good per-formance in automatic detection of questions andanswers, which can help produce summaries thathighlight or focus on the question and answer ex-change.
In a combined chat and email summariza-tion task, a technique (Zhou and Hovy, 2005) con-sisting of identifying APs and appending any rele-vant responses to topic initiating messages was in-strumental in outperforming two competitive sum-marization baselines.The need to model pragmatic influences, suchas between a question and an answer, is also preva-lent in meeting summarization.
In fact, question-answer pairs are not the only discourse relationsthat we need to preserve in order to create co-herent summaries, and, as we will see, most in-stances of APs would need to be preserved to-gether, either inside or outside the summary.
Ta-ble 1 displays an AP construction with one state-ment (A part) and three respondents (B parts).This example illustrates that the number of turnsbetween constituents of APs is variable and thusdifficult to model with standard sequence models.This example also illustrates some of the predic-tors investigated in this paper.
First, many speak-ers respond to A?s utterance, which is generally astrong indicator that the A utterance should be in-cluded.
Secondly, while APs are generally char-acterized in terms of pre-defined dialog acts, such365Time Speaker AP Transcript1480.85-1493.91 1 A are - are those d- delays adjustable?
see a lot of people who actually build stuffwith human computer interfaces understand that delay, and - and so when you -by the time you click it it?ll be right on because it?ll go back in time to put the -1489.71-1489.94 2 yeah.1493.95-1495.41 3 B yeah, uh, not in this case.1494.31-1495.83 2 B it could do that, couldn?t it.1495.1-1497.07 4 B we could program that pretty easily , couldn?t we?Table 1: Snippet of a meeting displaying an AP construction, where a question (A) initiates three responses (B).
Sentences initalic are not present in the reference summary.as OFFER-ACCEPT, we found that the type of di-alog act has much less importance than the ex-istence of the AP connection itself (APs in thedata represent a great variety of DA pairs, includ-ing many that are not characterized as APs in thelitterature?e.g., STATEMENT-STATEMENT in thetable).
Since DAs seem to matter less than adja-cency pairs, the aim will be to build techniques toautomatically identify such relations and exploitthem in utterance selection.In the current work, we use skip-chain sequencemodels (Sutton and McCallum, 2004) to repre-sent dependencies between both contiguous ut-terances and paired utterances appearing in thesame AP constructions.
The graphical represen-tations of skip-chain models, such as the CRF rep-resented in Figure 1, are composed of two types ofedges: linear-chain and skip-chain edges.
The lat-ter edges model AP links, which we represent asa set of (s, d) index pairs (note that no more thanone AP may share the same second element d).The intuition that the summarization labels (?1or 1) are highly correlated with APs is confirmedin Table 2.
While contiguous labels yt?1 and ytseem to seldom influence each other, the correla-tion between AP elements ys and yd is particularlystrong, and they have a tendency to be either bothincluded or both excluded.
Note that the secondtable is not symmetric, because the data allows anA part to be linked to multiple B parts, but notvice-versa.
While counts in Table 2 reflect hu-man labels, we only use automatically predicted(s, d) pairs in the experiments of the remainingpart of this paper.
To find these pairs automati-cally, we trained a non-sequential log-linear modelthat achieves a .902 accuracy (Galley et al, 2004).4 Skip-Chain Sequence ModelsIn this paper, we investigate conditional modelsfor paired sequences of observations and labels.
Inthe case of utterance selection, the observation se-quence x = x1:T = (x1, .
.
.
, xT ) represents localStatementx 1x 2x 3x 4x 5BackChannelStatementStatementStatementy 1y 2y 3y 4y 5Figure 1: A skip-chain CRF with pragmatic-level links.Linear-chain edges yt = 1 yt = ?1yt?1 = 1 529 7742yt?1 = ?1 7742 116040Skip-chain edges yd = 1 yd = ?1ys = 1 6792 2191ys = ?1 1479 121591Table 2: Contingency tables: while the correlation betweenadjacent labels yt?1 and yt is not significant (?2 = 2.3,p > .05), empirical evidence clearly shows that ys and ydinfluence each other (?2 = 78948, p < .001).summarization predictors (see Section 6), and thebinary sequence y = y1:T = (y1, .
.
.
, yT ) (whereyt ?
{?1, 1}) determines which utterances mustbe included in the summary.
In a discriminativeframework, we concentrate our modeling effort onestimating p(y|x) from data, and do not explicitlymodel the prior probability p(x), since x is fixedduring testing anyway.Many probabilistic approaches to modeling se-quences have relied on directed graphical mod-els, also known as Bayesian networks (BN),1 inparticular hidden Markov models (Rabiner, 1989)and conditional Markov models (McCallum et al,2000).
However, prominent recent approacheshave focused on undirected graphical models, inparticular conditional random fields (CRF) (Laf-ferty et al, 2001), and provided state-of-the-artperformance in many NLP tasks.
In our work, wewill provide empirical results for state sequencemodels of both semantics, and we will now de-1In the existing literature, sequence models that satisfy theMarkovian condition?i.e., the state of the system at time tdepend only on its immediate past t?
k:t?
1 (typically justt?
1)?are generally termed dynamic Bayesian networks(DBN).
Since the particular models under investigation, i.e.skip-chain models, do not have this property, we will simplyrefer to them as Bayesian networks.366scribe skip-chain models for both BNs and CRFs.In a BN, the probability of the sequence y fac-torizes as a product of probabilities of local predic-tions yt conditioned on their parents pi(yt) (Equa-tion 1).
In a CRF, the probability of the sequence yfactorizes according to a set of clique potentials{?c}c?C , where C is represents the cliques of theunderlying graphical model (Equation 2).pBN(y|x) =T?i=1pBN(yt|x, pi(yt)) (1)pCRF(y|x) ?
?c?C?c(xc,yc) (2)We parameterize these BNs and CRFs as log-linear models, and factorize both BN?s local pre-diction probabilities and CRF?s clique potentialsusing two types of feature functions.
Linear-chainfeature functions fj(yt?k:t,x, t) represent localdependencies that are consistent with an order-kMarkov assumption.
For instance, one such func-tion could be a predicate that is true if and only ifyt?1 = 1, yt = ?1, and (xt?1, xt) indicates thatboth utterances are produced by the same speaker.Given a set of skip edges S = {(st, t)} specifyingsource and destination indices, skip-chain featurefunctions gj(yst , yt,x, st, t) exploit dependenciesbetween variables that are arbitrarily distant inthe chain.
For instance, the finding that OFFER-REJECT pairs are often linked in summaries mightbe encoded as a skip-chain feature predicate thatis true if and only if yst = 1, yt = 1, and the firstword of the t-th utterance is ?no?.Log-linear models for skip-chain sequencemodels are defined in terms of weights {?k} and{?k}, one for each feature function.
In the case ofBNs, we write:log pBN(yt|x, pi(yt)) ?J?j=1?jfj(x,yt?k:t, t) +J ?
?j=1?jgj(x, yst , yt, st, t)We can reduce a particular skip-chain CRF to rep-resent only the set of cliques along (yt?1, yt) adja-cency edges and (yst , yt) skip edges, resulting inonly two potential functions:log ?LIN(x,yt?k:t, t) =J?j=1?jfj(x,yt?k:t, t)log ?SKIP(x, yst , yt, t) =J ?
?j=1?jgj(x, yst , yt, st, t)4.1 Inference and Parameter EstimationOur CRF and BN models were designed us-ing MALLET (McCallum, 2002), which providestools for training log-linear models with L-BFGSoptimization techniques and maximize the log-likelihood of our training dataD = (x(i),y(i))Ni=1,and provides probabilistic inference algorithms forlinear-chain BNs and CRFs.Most previous work with CRFs containing non-local dependencies used approximate probabilis-tic inference techniques, including TRP (Suttonand McCallum, 2004) and Gibbs sampling (Finkelet al, 2005).
Approximation is needed whenthe junction tree of a graphical model is associ-ated with prohibitively large cliques.
For exam-ple, the worse case reported in (Sutton and Mc-Callum, 2004) is a clique of 61 nodes.
In thecase of skip-chain models representing APs, theinference problem is somewhat simpler: loops inthe graph are relatively short, 98% of AP edgesspan no more than 5 time slices, and the maximumclique size in the entire data is 5.
While exact in-ference might be possible in our case, we used thesimpler approach of adapting standard inferencealgorithms for linear-chain models.Specifically, to account for skip-edges, we useda technique inspired by (Sha and Pereira, 2003),in which multiple state dependencies, such as anorder-2 Markov model, are encoded using auxil-iary tags.
For instance, an order-2 Markov modelis parameterized using state triples yt?2:t, and eachpossible triple is converted to a label zt = yt?2:t.Using these auxiliary labels only, we can thenuse the standard forward-backward algorithm forcomputing marginal distributions in linear-chainCRFs, and Viterbi decoding in linear-chain CRFsand BNs.
The only requirement is to ensure thata transition between zt and zt+1 is forbidden ifthe sub-states yt?1:t common to both states differ,i.e., is assigned an infinite cost.
This approach canbe extended to the case of skip-chain transitions.For instance, an order-1 Markov model with skip-edges can be constructed using zt = (yst , yt?1, yt)triples, where the first element yst represents thelabel at the source of the skip-edge.
Similarly tothe case of order-2 Markov models, we need toensure that only valid sequences of labels are con-sidered, which is trivial to enforce if we assumethat no skip edge ranges more than a predefinedthreshold of k time slices.While this approach is not exact, it still provides367competitive performance as we will see in Sec-tion 8.
In future work, we plan to explore moreaccurate probabilistic inference techniques.5 Ranking Utterances by ImportanceAs we will see in Section 8, using the actual{?1, 1} label predictions of our BNs and CRFsleads to significantly sub-optimal results, whichmight be explained by the following reasons.
First,our models are optimized to maximize the condi-tional log-likelihood of the training data, a mea-sure that does not correlate well with utility mea-sures generally used in retrieval oriented taskssuch as summarization, especially when facedwith a significant class imbalance (only 6.26%of reference instances are positive).
Second, theMAP decision rule doesn?t give us the freedom toselect an arbitrary number of sentences in orderto satisfy any constraint on length.
Instead of us-ing actual predictions, it seems more reasonableto compute the posterior probability of each lo-cal prediction yt, and extract the N most probablesummary sentences (yr1 , .
.
.
, yrk), where N maydepend on a length expressed in number of words,as it is the case in our evaluation in Section 7.BNs assign probability distributions over entiresequences by estimating the probability of each in-dividual instance yt in the sequence (Equation 1),and seem thus particularly suited for ranking utter-ances.
A first approach is then to rank utterancesaccording to the cost of predicting yt = 1 at eachtime step on the Viterbi path.
While these costsare well-formed (negative log) probabilities in thecase of BNs, they cannot be interpreted as such inthe case of CRFs, and turn out to produce poor re-sults with CRFs.
Indeed, the set of CRF potentialsassociated with each time step have no immedi-ate probabilistic interpretation, and cannot be useddirectly to rank sentences.
Since BNs and CRFsare here parameterized as log-linear models andrely on the same set of feature functions, a secondapproach is to use CRF-trained model parametersto build a BN classifier that assigns a probabilityto each yt.
Specifically, the CRF model is firstused to generate label predicitons y?, from whichthe locally-normalized model estimates the costof predicting y?t = 1 given a label history y?1:t?1.This ensures that we have a well-formed probabil-ity distribution at each time slice, while capitaliz-ing on the good performance of CRF models.Lexical features:?
n-grams (n ?
3)?
number of words?
number of digits?
number of consecutive repeatsInformation retrieval features:?
max/sum/mean frequency of all terms in ut?
max/sum/mean idf score?
max/sum/mean tf ?idf score?
cosine similarity between word vector of ut with cen-troid of of the meeting?
scores of LSA with 5, 10, 50, 100, 200, 300 conceptsAcoustic features:?
seconds of silence before/during/after the turn?
speech rate?
min/max/mean/median/stddev/onset/outset f0 of utter-ance t, and of first and last word?
min/max/mean/stddev energy?
.05, .25, .5, .75, .95 quantiles of f0 and energy?
pitch range?
f0 mean absolute slopeDurational and structural features:?
duration of the previous/current/next utterance?
relative position within meeting (i.e., index t)?
relative position within speaker turn?
large number of structural predicates, i.e.
?is the previ-ous utterance of the same speaker???
number of APs initiated in ytDiscourse features:?
lexical cohesion score (for topic shifts) (Hearst, 1994)?
first and second word of utterance, if in cue word list?
number of pronouns?
number of fillers and fluency devices (e.g., ?uh?, ?um?)?
number of backchannel and acknowledgment tokens(e.g., ?uh-huh?, ?ok?, ?right?
)Table 3: Features for extractive summarization.
Unless oth-erwise mentioned, we refer to features of utterance t whoselabel yt we are trying to predict.6 Features for extractive summarizationWe started our analyses with a large collectionof features found to be good predictors in ei-ther speech (Inoue et al, 2004; Maskey andHirschberg, 2005; Murray et al, 2005) or textsummarization (Mani and Maybury, 1999).
Ourgoal is to build a very competitive feature set thatcapitalizes on recent advances in summarization ofboth genres.
Table 3 lists some important features.There is strong evidence that lexical cues suchas ?significant?
and ?great?
are strong predictorsin many summarization tasks (Edmundson, 1968).Such cues are admittedly quite genre specific,so we did not want to commit ourselves to anyspecific list, which may not carry over well toour specific speech domain, and we automaticallyselected a list of n-grams (n ?
3) using cross-validation on the training data.
More specifically,we computed the mutual information of each n-368Transcript:I think-one thingthat makes adifferenceis thisDCoffsetcompensation.
1-13Didyou havea lookatmeeting digitsif theyhavea them?
14-26I didn't.
No.
27-29Hmm.
30No.The DC componentis negligible.Allmikeshave DC removal.
31-41Yeah.
42Because there's asample andhold intheA-to-D. 43-51AndI also,um, did some experiments about normalizing the phase.
52-62Andcameupwith aweb pagepeople cantake alook at.
63-75Model1 (len=20):31-4143-51Model2 (len=22):31-4152-62Model3 (len=24):52-6263-75Peer (len=22):1-1343-51Optimal (len=22):31-4152-621 1 2 3 4 3 3 2 2Speaker:Figure 2: Model, peer, and ?optimal?
summaries are all extracts taken from the same transcription.gram with the class variable, and selected for eachn the 200 best scoring n-grams.
Other lexical fea-tures include: the number of digits, which is help-ful for identifying sections of the meetings whereparticipants collect data by recording digits; thenumber of repeats, which may indicate the kind ofhesitations and disfluencies that negatively corre-lates with what is included in the summary.The information retrieval feature set containsmany features that are generally found helpful insummarization, in particular tf ?idf and scores de-rived from centroid methods.
In particular, weused the latent semantic analysis (LSA) featurediscussed in (Murray et al, 2005), which attemptsto determine sentence importance through singu-lar value decomposition, and whose resulting sin-gular values and singular vectors can be exploitedto associate each utterance a degree of relevance toone of the top-n concepts of the meetings (where nrepresents the number of dimensions in the LSA).We used the same scoring mechanism as (Mur-ray et al, 2005), though we extracted features formany different n values.Acoustic features extracted with Praat(Boersma and Weenink, 2006) were normal-ized by channel and speaker, including manyraw features such as f0 and energy.
Structuralfeatures listed in the table are those computedfrom the sequence model before decoding, e.g.,the duration that separates the two elementsof an AP.
Finally, discourse features representpredictors that may substitute to DA labels.
WhileDA tagging is not directly our concern, it ispresumably helpful to capitalize on discoursecharacteristics of utterances involved in adjacencypairs, since different types of dialog acts may beunequally likely to appear in a summary.7 EvaluationEvaluating summarization is a difficult problemand there is no broad consensus on how to bestperform this task.
Two metrics have becomequite popular in multi-document summarization,namely the Pyramid method (Nenkova and Pas-sonneau, 2004b) and ROUGE (Lin, 2004).
Pyra-mid and ROUGE are techniques looking for con-tent units repeated in different model summaries,i.e., summary content units (SCUs) such as clausesand noun phrases for the Pyramid method, and n-grams for ROUGE.
The underlying hypothesis isthat different model sentences, clauses, or phrasesmay convey the same meaning, which is a reason-able assumption when dealing with reference sum-maries produced by different authors, since it isquite unlikely that any two abstractors would usethe exact same words to convey the same idea.Our situation is however quite different, sinceall model summaries of a given document are ut-terance extracts of that same document, as this canbeen seen in the excerpt of Figure 2.
In our ownannotation of three meetings with SCUs definedas in (Nenkova and Passonneau, 2004a), we foundthat repetitions and reformulation of the same in-formation are particularly infrequent, and that tex-tual units that express the same content amongmodel summaries are generally originating fromthe same document sentence (e.g., in the figure,the first sentence in model 1 and 2 emanate fromthe same document sentence).
Very short SCUs(e.g., base noun phrases) sometimes appeared indifferent locations of a meeting, but we think it isproblematic to assume that connections betweensuch short units are indicative of any similarityof sentential meaning: the contexts are different,and words may be uttered by different speakers,which may lead to unrelated or conflicting prag-matic forces.
For instance, an SCU realized as?DC offset?
and ?DC component?
appears in twodifferent sentences in the figure, i.e.
those iden-tified as 1-13 and 31-41.
However, the two sen-tences have contradictory meanings, and it wouldbe unfortunate to increase the score of a peer sum-mary containing the former sentence because the369latter is included in some model summaries.For all these reasons, we believe that sum-marization evaluation in our case should rely onthe following restrictive matching: two summaryunits should be considered equivalent if and onlyif they are extracted from the same location inthe original document (e.g., the ?DC?
appearingin models 1 and 2 is not the same as the ?DC?
inthe peer summary, since they are extracted fromdifferent sentences).
This constraint on the match-ing is reflected in our Pyramid evaluation, and wedefine an SCU as a word and its document po-sition, which lets us distinguish (?DC?,11) from(?DC?,33).
While this restriction on SCUs forcesus to disregard scarcely occurring paraphrases andrepetitions of the same information, it provides thebenefit of automated evaluation.Once all SCUs have been identified, the Pyra-mid method is applied as in (Nenkova and Passon-neau, 2004b): we compute a scoreD by adding foreach SCU present in the summary a score equalto the number of model summaries in which thatSCU appears.
The Pyramid score P is computedby dividing D by the maximum D?
value that isobtainable given the constraint on length.
For in-stance, the peer summary in the figure gets a scoreD = 9 (since the 9 SCUs in range 43-51 occur inone model), and the maximum obtainable score isD?
= 44 (all SCUs of the optimal summary ap-pear in exactly two model summaries), hence thepeer summary?s score is P = .204.While our evaluation scheme is similar to com-paring the binary predictions of model and peersummaries?each prediction determining whethera given transcription word is included or not?and averaging precision scores over all peer-modelpairs, the Pyramid evaluation differs on an im-portant point, which makes us prefer the Pyramidevaluation method: the maximum possible Pyra-mid score is always guaranteed to be 1, but av-erage precision scores can become arbitrarily lowas the consensus between summary annotators de-creases.
For instance, the average precision scoreof the optimal summary in the figure is PR = 23 .22Precision scores of the optimal summary comparedagainst the the three model summaries are .5, 1, and .5, re-spectively, and hence average 23 .
We can show that P =PR/PR?, where PR?
is the average precision of the op-timal summary.
Lack of space prevent us from providing aproof, so we will just show that the equality holds in our ex-ample: since the peer summary?s precision scores against thethree model summaries are respectively 922 , 0, and 0, we havePR/PR?
= ( 966 )/(23 ) =944 = P .FEATURE F?=11 utterance duration .2462 100-dimension LSA .2683 duration of utterance t?
1 .2754 time between utterances s and d = t .2815 IDF mean .2846 meeting position .2867 number of APs initiated in t .2888 duration of utterance t + 1 .2889 number of fillers .28910 .25-quantile of energy .29011 number of lexical repeats .29212 lexical cohesion score .29413 f0 mean of last word of utterance t .29414 LSA 50 dimensions .29515 utterances (t,t + 1) by same speaker .29816 speech rate .30217 ?is that?
.30318 ?for the?
.30319 (ut?1,ut) by same speaker .30520 ?to try?
.30521 ?meetings?
.30522 utterance starts with ?and?
.30623 ?we have?
.30624 ?new?
.30725 utterance starts with ?what?
.307Table 4: Forward feature selection.In the case of the six test meetings, which all haveeither 3 or 4 model summaries, the maximum pos-sible average precision is .6405.8 ExperimentsWe follow (Murray et al, 2005) in using the samesix meetings as test data, since each of these meet-ings has multiple reference summaries.
The re-maining 69 meetings were used for training, whichrepresent in total more than 103,000 training in-stances (or DA units), of which 6,464 are posi-tives (6.24%).
The multi-reference test set con-tains more than 28,000 instances.The goal of a preliminary experiment was to de-vise a set of useful predictors from a full set of1171.
We performed feature selection by incre-mentally growing a log-linear model with order-0 features f(x, yt) using a forward feature selec-tion procedure similar to (Berger et al, 1996).Probably due to the imbalance between positiveand negative samples, we found it more effectiveto rank candidate features by gains in F -measure(through 5-fold cross validation on the entire train-ing set).
The increase inF1 by adding new featuresto the model is displayed in Table 4; this greedysearch resulted in a set S of 217 features.We now analyze the performance of differentsequence models on our test set.
The target lengthof each summary was set to 12.7% of the numberof words of the full document, which is the aver-370age on the entire training data (the average on thetest data is 12.9%).
In Table 5, we use an order-0CRF to compare S against all features and variouscategorical groupings.
Overall, we notice lexicalpredictors and statistics derived from them (e.g.LSA features) represent the most helpful featuregroup (.497), though all other features combinedachieve a competitive performance (.476).Table 6 displays performance for sequencemodels incorporating linear-chain features of in-creasing order k. Its second column indicateswhat criterion was used to rank utterances.
In thecase of ?pred?, we used actual model {?1, 1} pre-dictions, which in all cases generated summariesmuch shorted than the allowable length, and pro-duced poor performance.
?Costs?
and ?norm-CRF?refer to the two ranking criteria presented in Sec-tion 5, and it is clear that the performance of CRFsdegrades with increasing orders without local nor-malization.
While the contingency counts in Ta-ble 2 only hinted a limited benefit of linear-chainfeatures, empirical results show the contrary?especially for order k = 2.
However, the furtherincrease of k causes overfitting, and skip-chainfeatures seem a better way to capture non-localdependencies while keeping the number of modelparameters relatively small.
Overall, the additionof skip-chain edges to linear-chain models providenoticeable improvement in Pyramid scores.
Oursystem that performed best on cross-validationdata is an order-2 CRF with skip-chain transitions,which achieves a Pyramid score of P = .554.We now assess the significance of our resultsby comparing our best system against: (1) a leadsummarizer that always selects the first N utter-ances to match the predefined length; (2) humanperformance, which is obtained by leave-one-outcomparisons among references (Table 7); (3) ?op-timal?
summaries generated using the procedureexplained in (Nenkova and Passonneau, 2004b)by ranking document utterances by the number ofmodel summaries in which they appear.
It ap-pears that our system is considerably better thanthe baseline, and achieves 91.3% of human per-formance in terms of Pyramid scores, and 83% ifusing ASR transcription.
This last result is partic-ularly positive if we consider our strong relianceon lexical features.For completeness, we also included standardROUGE (1, 2, and L) scores in Table 7, whichwere obtained using parameters defined for theFEATURE SET Plexical .471IR .415lexical + IR .497acoustic .407structural/durational .478acoustic + structural/durational .476all features .507selected features (S) .515Table 5: Pyramid score for each feature set.MODEL RANKING k = 1 2 3linear-chain BN pred .241 .267 .269linear-chain BN costs .512 .519 .525skip-chain BN costs .543 .549 .542linear-chain CRF pred .326 .36 .348linear-chain CRF costs .508 .475 .447linear-chain CRF norm-CRF .53 .548 .54skip-chain CRF norm-CRF .541 .554 .559Table 6: Pyramid scores for different sequence models, wherek stands for the order of linear-chain features.
The value inbold is the performance of the model that was selected aftera 5-fold cross validation on the training data, which obtainedthe highest F1 score.SUMMARIZER P R-1 R-2 R-Lbaseline .188 .501 .210 .495skip-chain CRF (transcript) .554 .715 .442 .709skip-chain CRF (ASR) .504 .714 .42 .706human .607 .720 .477 .715optimal 1 .791 .648 .788Table 7: Pyramid, and average ROUGE scores for summariesproduces by a baseline (lead summarizer), our best system,humans, and the optimal summarizer.DUC-05 evaluation.
Since system summarieshave on average approximately the same lengthas references, we only report recall measures ofROUGE (precision and F averages are within ?.002).3 It may come as a surprise that our best sys-tem (both with ASR and true words) performs al-most as well as humans; it seems more reasonableto conclude that, in our case, ROUGE has troublediscriminating between systems with moderatelyclose performance.
This seems to confirm our im-pression that content evaluation in our task shouldbe based on exact matches.We performed a last experiment to compare ourbest system against Murray et al (2005), who usedthe same test data, but constrained summary sizesin terms of number of DA units instead of words.In their experiments, 10% of DAs had to be se-lected.
Our system achieves .91 recall, .5 preci-sion, and .64 F1 with the same length constraint.3Human performance with ROUGE was assessed bycross-validating reference summaries of each meeting (i.e.,n references for a given meeting resulted in n evaluationsagainst the other references).
We used the same leave-one-out procedure with other summarizers, in order to get resultscomparable to humans.371The discrepancy between recall and precision islargely due to the fact that generated summariesare on average much longer than model summaries(10% vs. 6.26% of DAs), which explains why ourprecision is relatively low in this last evaluation.The best ROUGE-1 measure reported in (Murrayet al, 2005) is .69 recall, which is significantlylower than ours according to confidence intervals.9 ConclusionAn order-2 CRF with skip-chain dependencies de-rived from the automatic analysis of participantinteraction was shown to outperform linear-chainBNs and CRFs, despite the incorporation in allcases of the same competitive set of predictorsresulting from cross-validated feature selection.Compared to an order-0 CRF model, the absoluteincrease in performance is 3.9% (7.5% relative in-crease), which indicates that it is helpful to useskip-chain sequence models in the summarizationtask.
Our best performing system reaches 91.3%of human performance, and scales relatively wellon automatic speech recognition output.AcknowledgmentsThis work has benefited greatly from suggestionsand advice from Kathleen McKeown.
I also wouldlike to thank Jean Carletta, Steve Renals andGabriel Murray for giving me access to their sum-marization corpus, Ani Nenkova for helpful dis-cussions about summarization evaluation, MichaelCollins, Daniel Ellis, Julia Hirschberg, and OwenRambow for useful preliminary discussions, andthree anonymous reviewers for their insightfulcomments on an earlier version of this paper.ReferencesA.
Berger, S. Della Pietra, and V. Della Pietra.
1996.
A max-imum entropy approach to natural language processing.Computational Linguistics, 22(1):39?72.P.
Boersma and D. Weenink.
2006.
Praat: doing phoneticsby computer.
http://www.praat.org/.J.
Conroy, J. Schlesinger, J. Goldstein, and D. O?Leary.
2004.Left-brain/right-brain multi-document summarization.
InDUC 04 Conference Proceedings.H.P.
Edmundson.
1968.
New methods in automatic extract-ing.
Journal of the ACM, 16(2):264?285.J.
Finkel, T. Grenager, and C. Manning.
2005.
Incorporatingnon-local information into information extraction systemsby gibbs sampling.
In Proc.
of ACL, pages 363?370.M.
Galley, K. McKeown, J. Hirschberg, and E. Shriberg.2004.
Identifying agreement and disagreement in conver-sational speech: Use of bayesian networks to model prag-matic dependencies.
In Proc.
of ACL, pages 669?676.M.
Hearst.
1994.
Multi-paragraph segmentation of exposi-tory text.
In Proc.
of ACL, pages 9?16.A.
Inoue, T. Mikami, and Y. Yamashita.
2004.
Improvementof speech summarization using prosodic information.
InProc.
of Speech Prosody.A.
Janin, D. Baron, J. Edwards, D. Ellis, D. Gelbart, N. Mor-gan, B. Peskin, T. Pfau, E. Shriberg, A. Stolcke, andC.
Wooters.
2003.
The ICSI meeting corpus.
In Proc.of ICASSP.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Conditionalrandom fields: Probabilistic models for segmenting andlabeling sequence data.
In Proc.
of ICML, pages 282?289.C.-Y.
Lin.
2004.
ROUGE: a package for automatic evalua-tion of summaries.
In Proc.
of workshop on text summa-rization, ACL-04.I.
Mani and M. Maybury.
1999.
Advances in Automatic TextSummarization.
MIT Press.S.
Maskey and J. Hirschberg.
2005.
Comparing lexial,acoustic/prosodic, discourse and structural features forspeech summarization.
In Proc.
of Eurospeech.A.
McCallum, D. Freitag, and F. Pereira.
2000.
Maxi-mum entropy markov models for information extractionand segmentation.
In Proc.
of ICML.A.
McCallum.
2002.
MALLET: A machine learning forlanguage toolkit.
http://mallet.cs.umass.edu.N.
Mirghafori, A. Stolcke, C. Wooters, T. Pirinen, I. Bulyko,D.
Gelbart, M. Graciarena, S. Otterson, B. Peskin, andM.
Ostendorf.
2004.
From switchboard to meetings: De-velopment of the 2004 ICSI-SRI-UW meeting recognitionsystem.
In Proc.
of ICSLP.G.
Murray, S. Renals, J. Carletta, and J. Moore.
2005.
Eval-uating automatic summaries of meeting recordings.
InProc.
of the ACL Workshop on Intrinsic and ExtrinsicEvaluation Measures for MT and/or Summarization.A.
Nenkova and R. Passonneau.
2004a.
Evaluating con-tent selection in human- or machine-generated summaries:The pyramid scoring method.
Technical Report CUCS-025-03, Columbia University, CS Department.A.
Nenkova and R. Passonneau.
2004b.
Evaluating con-tent selection in summarization: The pyramid method.
InProc.
of HLT/NAACL, pages 145?152.L.
Rabiner.
1989.
A tutorial on hidden markov models andselected applications in speech recogntion.
Proc.
of theIEEE, 77(2):257?286.O.
Rambow, L. Shrestha, J. Chen, and C. Lauridsen.
2004.Summarizing email threads.
In Proc.
of HLT-NAACL.E.
Schegloff and H. Sacks.
1973.
Opening up closings.Semiotica, 7-4:289?327.F.
Sha and F. Pereira.
2003.
Shallow parsing with conditionalrandom fields.
In Proc.
of NAACL, pages 134?141.L.
Shrestha and K. McKeown.
2004.
Detection of question-answer pairs in email conversations.
In Proc.
of COLING,pages 889?895.E.
Shriberg, R. Dhillon, S. Bhagat, J. Ang, and H. Carvey.2004.
The ICSI meeting recorder dialog act (MRDA) cor-pus.
In SIGdial Workshop on Discourse and Dialogue,pages 97?100.C.
Sutton and A. McCallum.
2004.
Collective segmenta-tion and labeling of distant entities in information extrac-tion.
Technical Report TR # 04-49, University of Mas-sachusetts.K.
Zechner.
2002.
Automatic summarization of open domainmulti-party dialogues in diverse genres.
ComputationalLiguistics, 28(4):447?485.L.
Zhou and E. Hovy.
2005.
Digesting virtual ?geek?
culture:The summarization of technical internet relay chats.
InProc.
of ACL, pages 298?305.372
