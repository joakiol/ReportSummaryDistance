Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 23?32,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsA Neural Network for Coordination Boundary PredictionJessica FiclerComputer Science DepartmentBar-Ilan UniversityIsraeljessica.ficler@gmail.comYoav GoldbergComputer Science DepartmentBar-Ilan UniversityIsraelyoav.goldberg@gmail.comAbstractWe propose a neural-network based model forcoordination boundary prediction.
The net-work is designed to incorporate two signals:the similarity between conjuncts and the ob-servation that replacing the whole coordina-tion phrase with a conjunct tends to producea coherent sentences.
The modeling makesuse of several LSTM networks.
The modelis trained solely on conjunction annotations ina Treebank, without using external resources.We show improvements on predicting coor-dination boundaries on the PTB compared totwo state-of-the-art parsers; as well as im-provement over previous coordination bound-ary prediction systems on the Genia corpus.1 IntroductionCoordination is a common syntactic phenomena, ap-pearing in 38.8% of the sentences in the Penn Tree-bank (PTB) (Marcus et al, 1993), and in 60.71%of the sentences in the Genia Treebank (Ohta et al,2002).
However, predicting the correct conjunctsspan remain one of the biggest challenges for state-of-the-art syntactic parsers.
Both the Berkeley andZpar phrase-structure parsers (Petrov et al, 2006;Zhang and Clark, 2011) achieve F1 scores of around69% when evaluated on their ability to recover coor-dination boundaries on the PTB test set.
For exam-ple, in:?He has the government?s blessing to [build churches]and [spread Unificationism] in that country.
?the conjuncts are incorrectly predicted by bothparsers:Berkeley: ?He has the government?s blessing to [buildchurches] and [spread Unificationism in that country].
?Zpar: ?He [has the government?s blessing to buildchurches] and [spread Unificationism in that country].
?In this work we focus on coordination boundaryprediction, and suggest a specialized model for thistask.
We treat it as a ranking task, and learn a scor-ing function over conjuncts candidates such that thecorrect candidate pair is scored above all other can-didates.
The scoring model is a neural network withtwo LSTM-based components, each modeling a dif-ferent linguistic principle: (1) conjuncts tend to besimilar (?symmetry?
); and (2) replacing the coor-dination phrase with each of the conjuncts usuallyresult in a coherent sentence (?replacement?).
Thesymmetry component takes into account the con-juncts?
syntactic structures, allowing to capture sim-ilarities that occur in different levels of the syntac-tic structure.
The replacement component considersthe coherence of the sequence that is produced whenconnecting the participant parts.
Both of these sig-nals are syntactic in nature, and are learned solelybased on information in the Penn Treebank.
Ourmodel substantially outperforms both the Berkeleyand Zpar parsers on the coordination prediction task,while using the exact same training corpus.
Seman-tic signals (which are likely to be based on resourcesexternal to the treebank) are also relevant for coor-dination disambiguation (Kawahara and Kurohashi,2008; Hogan, 2007) and provide complementary in-formation.
We plan to incorporate such signals infuture work.232 BackgroundCoordination is a very common syntactic construc-tion in which several sentential elements (called con-juncts) are linked.
For example, in:?The Jon Bon Jovi Soul Foundation [was founded in2006] and1 [exists to combat issues that force (fam-ilies) and2 (individuals) into economic despair].
?The coordinator and1 links the conjuncts surroundedwith square brackets and the coordinator and2 linksthe conjuncts surrounded with round brackets.Coordination between NPs and between VPs arethe most common, but other grammatical functionscan also be coordinated: ?
[relatively active]ADJPbut [unfocused]ADJP?
; ?
[in]IN and [out]IN themarket?.
While coordination mostly occurs be-tween elements with the same syntactic category,cross-category conjunctions are also possible: (?Al-ice will visit Earth [tomorrow]NP or [in the nextdecade]PP?).
Less common coordinations involvenon-constituent elements ?
[equal to] or [higherthan]?, argument clusters (?Alice visited [4 plan-ets] [in 2014] and [3 more] [since then]?
), and gap-ping (?
[Bob lives on Earth] and [Alice on Saturn]?
)(Dowty, 1988).2.1 Symmetry between conjunctsCoordinated conjuncts tend to be semantically re-lated and have a similar syntactic structure.
For ex-ample, in (a) and (b) the conjuncts include similarwords (China/Asia, marks/yen) and have identicalsyntactic structures.PPPPINforNPNNPChinaCCandPPINforNPNNPAsia(a)NPNPCD1.8690NNSmarksCCandNPCD139.75NNSyen(b)Symmetry holds also in larger conjuncts, such as in:(c)NPNPNNincome PPINof NPNPQP429.9 billion NNSrublesPRN(US$ 693.4)CCand NPVBZexpenditures PPINof NPNPQP489.9 billion NNSrublesPRN(US$ 790.2)Similarity between conjuncts was used as a guidingprinciple in previous work on coordination disam-biguation (Hogan, 2007; Shimbo and Hara, 2007;Hara et al, 2009).2.2 ReplaceabilityReplacing a conjunct with the whole coordinationphrase usually produce a coherent sentence (Hud-dleston et al, 2002).
For example, in ?Ethan hasdeveloped [new products] and [a new strategy]?,replacement results in: ?Ethan has developed newproducts?
; and ?Ethan has developed a new strat-egy?, both valid sentences.
Conjuncts replace-ment holds also for conjuncts of different syntac-tic types, e.g.
: ?inactivation of tumor-suppressorgenes, [alone] or [in combination], appears crucialto the development of such scourges as cancer.
?.While both symmetry and replacebility are strongcharacteristics of coordination, neither principleholds universally.
Coordination between syntacti-cally dissimilar conjuncts is possible (?tomorrowand for the entirety of the next decade?
), and thereplacement principle fails in cases of ellipsis, gap-ping and others (?The bank employs [8,000 peoplein Spain] and [2,000 abroad]?
).2.3 Coordination in the PTBCoordination annotation in the Penn Treebank (Mar-cus et al, 1993) is inconsistent (Hogan, 2007) andlacks internal structure for NPs with nominal mod-ifiers (Bies et al, 1995).
In addition, conjuncts inthe PTB are not explicitly marked.
These deficien-cies led previous works on coordination disambigua-tion (Shimbo and Hara, 2007; Hara et al, 2009;Hanamoto et al, 2012) to use the Genia treebankof biomedical text (Ohta et al, 2002) which explic-itly marks coordination phrases.
However, using theGenia corpus is not ideal since it is in a specialized24domain and much smaller than the PTB.
In this workwe rely on a version of the PTB released by Ficlerand Goldberg (2016) in which the above deficienciesare manually resolved.
In particular, coordinatingelements, coordination phrases and conjunct bound-aries are explicitly marked with specialized functionlabels.2.4 Neural Networks and NotationWe use w1:n to indicate a list of vectorsw1, w2, .
.
.
wn and wn:1 to indicate the reversed list.We use ?
for vector concatenation.
When a discretesymbol w is used as a neural network?s input, thecorresponding embedding vector is assumed.A multi-layer perceptron (MLP) is a non linearclassifier.
In this work we take MLP to mean aclassifier with a single hidden layer: MLP (x) =V ?
g(Wx + b) where x is the network?s input, gis an activation function such as ReLU or Sigmoid,and W , V and b are trainable parameters.
RecurrentNeural Networks (RNNs) (Elman, 1990) allow therepresentation of arbitrary sized sequences.
In thiswork we use LSTMs (Hochreiter and Schmidhuber,1997), a variant of RNN that was proven effective inmany NLP tasks.
LSTM(w1:n) is the outcome vec-tor resulting from feeding the sequence w1:n into theLSTM in order.
A bi-directional LSTM (biLSTM)takes into account both the past w1:i and the futurewi:n when representing the element in position i:biLSTM(w1:n, i) = LSTMF (w1:i) ?
LSTMB(wn:i)where LSTMF reads the sequence in its regular or-der and LSTMB reads it in reverse.3 Task Definition and ArchitectureGiven a coordination word in a sentence, the coor-dination prediction task aims to returns the two con-juncts that are connected by it, or NONE if the worddoes not function as a coordinating conjunction of arelevant type.1 Figure 1 provides an example.Our system works in three phases: first, we deter-mine if the coordinating word is indeed part of a con-junction of a desired type.
We then extract a rankedlist of candidate conjuncts, where a candidate is a1We consider and, or, but, nor as coordination words.
Incase of more than two coordinated elements (conjuncts), we fo-cus on the two conjuncts which are closest to the coordinator.Sentence:And1 the city decided to treat its guests morelike royalty or2 rock stars than factory owners.Expected output:and1: NONEor2: (11-11) royalty ; (12-13) rock starsSentence:The president is expected to visit Minnesota, NewYork and1 North Dakota by the end of the year.Expected output:and1: (9-10) New York ; (12-13) North DakotaFigure 1: The coordination prediction task.pair of spans of the form ((i, j), (l,m)).
The can-didates are then scored and the highest scoring pairis returned.
Section 4 describes the scoring model,which is the main contribution of this work.
Thecoordination classification and candidate extractioncomponents are described in Section 5.4 Candidate Conjunctions ScoringOur scoring model takes into account two signals,symmetry between conjuncts and the possibility ofreplacing the whole coordination phrase with its par-ticipating conjuncts.4.1 The Symmetry ComponentAs noted in Section 2.1, many conjuncts spans havesimilar syntactic structure.
However, while the sim-ilarity is clear to human readers, it is often not easyto formally define, such as in:?about/IN half/NN its/PRP$ revenue/NNand/CCmore/JJR than/IN half/NN its/PRP$ profit/NN?If we could score the amount of similarity be-tween two spans, we could use that to identify cor-rect coordination structures.
However, we do notknow the similarity function.
We approach this bytraining the similarity function in a data-dependentmanner.
Specifically, we train an encoder that en-codes spans into vectors such that vectors of similarspans will have a small Euclidean distance betweenthem.
This architecture is similar to Siamese Net-works, which are used for learning similarity func-tions in vision tasks (Chopra et al, 2005).25VPVBcut NPPRP$their NNSrisksVPVBtake NPNNSprofitsRVPVBcut cut VB VP R LVPRNPPRP$their their PRP$ NP R VP L VPLNPNNSrisks risks NNS NP L VP RVPVBtake take VB VP R LVPNPNNSprofits profits NNS NP VP LEuclidean DistanceFigure 2: Illustration of the symmetry scoring component that takes into account the conjuncts syntactic structures.
Each conjuncttree is decomposed into paths that are fed into the path-LSTMs (squares).
The resulting vectors are fed into the symmetry LSTMfunction (circles).
The outcome vectors (blue circles) are then fed into the euclidean distance function.Given two spans of lengths k and m with cor-responding vector sequences u1:k and v1:m we en-code each sequences using an LSTM, and take theeuclidean distance between the resulting representa-tions:Sym(u1:k, v1:m) = ||LSTM(u1:k)?
LSTM(v1:m)||The network is trained such that the distance is min-imized for compatible spans and large for incompat-ible ones in order to learn that vectors that representcorrect conjuncts are closer than vectors that do notrepresent conjuncts.What are the elements in the sequences to be com-pared?
One choice is to take the vectors ui to cor-respond to embeddings of the ith POS in the span.This approach works reasonably well, but does notconsider the conjuncts?
syntactic structure, whichmay be useful as symmetry often occurs on a higherlevel than POS tags.
For example, in:NPNPNNtomorrowPPINatCD16:00CCorNPNPNPthe dayPPafter tomorrowPPINatCD12:00the similarity is more substantial in the third level ofthe tree than in the POS level.A way to allow the model access to higher levelsof syntactic symmetry is to represent each word asthe projection of the grammatical functions from theword to the root.2 For example, the projections forthe first conjunct in Figure 2 are:2Similar in spirit to the spines used in Carreras et al (2008)and Shen et al (2003).VPVBcutVPNPPRP$theirVPNPNNSrisksThis decomposition captures the syntactic contextof each word, but does not uniquely determine thestructure of the tree.
To remedy this, we add tothe paths special symbols, R and L, which marksthe lowest common ancestors with the right and leftwords respectively.
These are added to the pathabove the corresponding nodes.
For example con-sider the following paths which corresponds to theabove syntactic structure:RVPVBcutLVPRNPPRP$theirVPLNPNNSrisksThe lowest common ancestor of ?their?
and ?risks?is NP.
Thus, R is added after NP in the path of?their?
and L is added after NP in the path of?risks?.
Similarly, L and R are added after the VPin the ?their?
and ?cut?
paths.The path for each word is encoded using anLSTM receiving vector embeddings of the elementsin the path from the word to the root.
We then use theresulting encodings instead of the POS-tag embed-dings as input to the LSTMs in the similarity func-tion.
Figure 2 depicts the complete process for thespans ?cut their risks?
and ?take profits?.Using syntactic projections requires the syntacticstructures of the conjuncts.
This is obtained by run-ning the Berkeley parser over the sentence and tak-ing the subtree with the highest probability from the26Sentence:Rudolph Agnew, [55 years old] and [former chairman of CGF PLC] ,was named a nonexecutive director.wi?1 wi wj wk wl wm wm+1?
??
?
?
??
?
?
??
?
?
??
?Pre Conj1 Conj2 PostExpansions:Rudolph Agnew, 55 years old ,was named a nonexecutive director.Rudolph Agnew, former chairman of CGF PLC ,was named a nonexecutive director.Figure 3: The correct conjuncts spans of the coordinator and in the sentence and the outcome expansions.corresponding cell in the CKY chart.3 In both ap-proaches, the POS embeddings are initialized withvectors that are pre-trained by running word2vec(Mikolov et al, 2013) on the POS sequences in PTBtraining set.4.2 The Replacement ComponentThe replacement component is based on the obser-vation that, in many cases, the coordination phrasecan be replaced with either one of its conjuncts whilestill preserving a grammatical and semantically co-herent sentence (Section 2.2)When attempting such a replacement on incorrectconjuncts, the resulting sentence is likely to be eithersyntactically or semantically incorrect.
For exam-ple, in the following erroneous analysis: ?RudolphAgnew, [55 years old] and [former chairman] ofConsolidated Gold Fields PLC?
replacing the con-junction with the first conjunct results in the se-mantically incoherent sequence ?Rudolph Agnew,55 years old of Consolidated Golden Fields, PLC?.4Our goal is to distinguish replacements resultingfrom correct conjuncts from those resulting from er-roneous ones.
To this end, we focus on the connec-tion points.
A connection point in a resulting sen-tence is the point where the sentence splits into twosequences that were not connected in the originalsentence.
For example, consider the sentence in Fig-ure 3.
It has four parts, marked as Pre, Conj1, Conj2and Post.
Replacing the coordination phrase Conj1and Conj2 with Conj2 results in a connection point3The parser?s CKY chart did not include a tree for 10% ofthe candidate spans, which have inside probability 0 and outsideprobability > 0.
For those, we obtained the syntactic structureby running the parser on the span words only.4While uncommon, incorrect conjuncts may also result invalid sentences, e.g.
?He paid $ 7 for cold [drinks] and [pizza]that just came out of the oven.
?between Pre and Conj2.
Likewise, replacing the co-ordination phrase with Conj1 results in connectionpoint between Conj1 and Post.In order to model the validity of the connectionpoints, we represent each connection point as theconcatenation of a forward and reverse LSTMs cen-tered around that point.
Specifically, for the spans inFigure 3 the two connection points are representedas:LSTMF (Rudolph,...,old)?LSTMB(director,...,was,,)andLSTMF (Rudolph,Agnew,,)?LSTMB(director,...,former)Formally, assuming wordsw1:n in a sentence withcoordination at position k and conjuncts wi:j andwl:m,5 the connection points are between w1:j andwm+1:n; and between w1:i?1 and wl:n. The twoconnection points representations are then concate-nated, resulting in a replacement vector:REPL(w1:n, i, j, l,m) =CONPOINT(w1:n, i?
1, l) ?
CONPOINT(w1:n, j,m+ 1)where:CONPOINT(w1:n, i, j) =LSTMF (w1:i) ?
LSTMB(wn:j)We use two variants of the replacement vectors,corresponding to two levels of representation.
Thefirst variant is based on the sentence?s words, whilethe second is based on its POS-tags.4.3 Parser based FeaturesIn addition to the symmetry and replacement sig-nals, we also incorporate some scores that are de-rived from the Berkeley parser.
As detailed in Sec-tion 5, a list of conjuncts candidates are extracted5Usually j = k ?
1 and l = k + 1, but in some casespunctuation symbols may interfere.27from the CKY chart of the parser.
The candidatesare then sorted in descending order according to themultiplication of inside and outside scores of thecandidate?s spans:6 I(i,j) ?O(i,j) ?
I(l,m) ?O(l,m).Each candidate {(i, j), (l,m)} is assigned two nu-merical features based on this ranking: its position inthe ranking, and the ratio between its score and thescore of the adjacent higher-ranked candidate.
Weadd an additional binary feature indicating whetherthe candidate spans are in the 1-best tree predictedby the parser.
These three features are denoted asFeats(i, j, l,m).4.4 Final Scoring and TrainingFinally, the score of a candidate {(i, j), (l,m)} in asentence with words w1:n and POS tags p1:n is com-puted as:SCORE(w1:n, p1:n, {(i, j), (l,m)}) =MLP (Sym(vPathi:j , vPathl:m )?Repl(w1:n, i, j, l,m)?Repl(p1:n, i, j, l,m)?
Feats(i, j, l,m) )where vPathi:j and vPathl:m are the vectors resulting fromthe path LSTMs, and Sym, Repl and Feats are thenetworks defined in Sections 4.1 ?
4.3 above.
Thenetwork is trained jointly, attempting to minimize apairwise ranking loss function, where the loss foreach training case is given by:loss = max(0, 1?
(y?
?
yg))where y?
is the highest scoring candidate and yg isthe correct candidate.
The model is trained on allthe coordination cases in Section 2?21 in the PTB.5 Candidates Extraction and SupportingClassifiersCandidates Extraction We extract candidatespans based on the inside-outside probabilities as-signed by the Berkeley parser.
Specifically, to obtain6Inside-Outside probabilities (Goodman, 1998) representthe probability of a span with a given non-terminal symbol.The inside probability I(N,i,j) is the probability of generatingwords wi, wi+1, ..., wj given that the root is the non-terminalN .
The outside probability O(N,i,j) is the probability of gen-erating words w1, w2, ..., wi?1, the non-terminal N and thewords wj+1, wj+2, ..., wn with the root S.candidates for conjunct span we collect spans thatare marked with COORD, are adjacent to the coor-dinating word, and have non-zero inside or outsideprobabilities.
We then take as candidates all possi-ble pairs of collected spans.
On the PTB dev set,this method produces 6.25 candidates for each co-ordinating word on average and includes the correctcandidates for 94% of the coordinations.Coordination Classification We decide whether acoordination word wk in a sentence w1:n functionsas a coordinator by feeding the biLSTM vector cen-tered around wk into a logistic classifier:?
(v ?
biLSTM(w1:n, k) + b).The training examples are all the coordination words(marked with CC) in the PTB training set.
Themodel achieves 99.46 F1 on development set and99.19 F1 on test set.NP coordinations amount to about half of thecoordination cases in the PTB, and previous workis often evaluated specifically on NP coordination.When evaluating on NP coordination, we departfrom the unrealistic scenario used in most previouswork where the type of coordination is assumed tobe known a-priori, and train a specialized model forpredicting the coordination type.
For a coordinationcandidate {(i, j), (l,m)} with a coordinator wk, wepredict if it is NP coordination or not by feedinga logistic classifier with a biLSTM vector centeredaround the coordinator and constrained to the candi-date spans:?
(v ?
biLSTM(wi:m, k) + b).The training examples are coordinations in the PTBtraining set, where where a coordinator is consid-ered of type NP if its head is labeled with NP orNX.
Evaluating on gold coordinations results in F1scores of 95.06 (dev) and 93.89 (test).6 ExperimentsWe evaluate our models on their ability to identifyconjunction boundaries in the extended Penn Tree-bank (Ficler and Goldberg, 2016) and Genia Tree-bank (Ohta et al, 2002)7.When evaluating on the PTB, we compare to theconjunction boundary predictions of the generative7http://www-tsujii.is.s.u-tokyo.ac.jp/GENIA28Dev TestP R F P R FBerkeley 70.14 70.72 70.42 68.52 69.33 68.92Zpar 72.21 72.72 72.46 68.24 69.42 68.82Ours 72.34 72.25 72.29 72.81 72.61 72.7Table 1: Coordination prediction on PTB (All coordinations).Dev TestP R F P R FBerkeley 67.53 70.93 69.18 69.51 72.61 71.02Zpar 69.14 72.31 70.68 69.81 72.92 71.33Ours 75.17 74.82 74.99 76.91 75.31 76.1Table 2: Coordination prediction on PTB (NP coordinations).Berkeley parser (Petrov et al, 2006) and the discrim-inative Zpar parser (Zhang and Clark, 2011).
Whenevaluating on the Genia treebank, we compare to theresults of the discriminative coordination-predictionmodel of Hara et al (2009).86.1 Evaluation on PTBBaseline Our baseline is the performance of theBerkeley and Zpar parsers on the task presented inSection 3, namely: for a given coordinating word,determine the two spans that are being conjoinedby it, and return NONE if the coordinator is notconjoining spans or conjoins spans that are not ofthe expected type.
We convert predicted trees toconjunction predictions by taking the two phrasesthat are immediately adjacent to the coordinatoron both sides (ignoring phrases that contain solelypunctuation).
For example, in the following Zpar-predicted parse tree the conjunct prediction is (?Feb.8, 1990?,?May 10, 1990?).NPNPFeb.
8, 1990CCandNPMay 10, 1990,,ADJPrespectivelyCases in which the coordination word is the left-most or right-most non-punctuation element in itsphrase (e.g.
(PRN (P -)(CC and)(S it?sbeen painful)(P -))) are considered as no-coordination (?None?
).8Another relevant model in the literature is (Hanamoto et al,2012), however the results are not directly comparable as theyuse a slightly different definition of conjuncts, and evaluate ona subset of the Genia treebank, containing only trees that wereproperly converted to an HPSG formalism.We consider two setups.
In the first we are inter-ested in all occurrences of coordination, and in thesecond we focus on NP coordination.
The secondscenario requires typed coordinations.
We take thetype of a parser-predicted coordination to be the typeof the phrase immediately dominating the coordina-tion word.Evaluation Metrics We measure precision and re-call compared to the gold-annotated coordinationspans in the extended PTB, where an exampleis considered correct if both conjunct boundariesmatch exactly.
When focusing on NPs coordina-tions, the type of the phrase above the CC level mustmatch as well, and phrases of type NP/NX are con-sidered as NP coordination.Results Tables (1) and (2) summarize the results.The Berkeley and Zpar parsers perform similarlyon the coordination prediction task.
Our proposedmodel outperforms both parsers, with a test-set F1score of 72.7 (3.78 F1 points gain over the betterparser) when considering all coordinations, and test-set F1 score of 76.1 (4.77 F1 points gain) when con-sidering NP coordination.6.2 Evaluation on GeniaTo compare our model to previous work, we evalu-ate also on the Genia treebank (Beta), a collectionof constituency trees for 4529 sentences from Med-line abstracts.
The Genia treebank coordination an-notation explicitly marks coordination phrases witha special function label (COOD), making the cor-pus an appealing resource for previous work on co-ordination boundary prediction (Shimbo and Hara,2007; Hara et al, 2009; Hanamoto et al, 2012).Following Hara et al (2009), we evaluate the mod-els?
ability to predict the span of the entire coordi-nation phrase, disregarding the individual conjuncts.For example, in ?My plan is to visit Seychelles, koSamui and Sardinia by the end of the year?
the goalis to recover ?Seychelles, ko Samui and Sardinia?.This is a recall measure.
We follow the exact proto-col of Hara et al (2009) and train and evaluate themodel on 3598 coordination phrases in Genia Tree-bank Beta and report the micro-averaged results ofa five-fold cross validation run.9 As shown by Hara9We thank Kazuo Hara for providing us with the exact de-tails of their splits.29Sym Correct: Retail sales volume was [down 0.5% from the previous three months] and [up 1.2% from a year earlier].Incorrect: Everyone was concerned about the [general narrowness of the rally] and [failure of the OTC market] to get into plus territory.Repw Correct: The newsletter said [she is 44 years old] and [she studied at the University of Puerto Rico School of Medicine].Incorrect: But Robert Showalter said no special [bulletins] or [emergency meetings of the investors?
clubs] are planned .Repp Correct: [On the Big Board floor] and [on trading desks], traders yelped their approval.Incorrect: It suddenly burst upward 7.5 as Goldman, Sachs & Co. [stepped in] and [bought almost] every share offer, traders said.Figure 4: Correct in incorrect predictions by the individual components.COOD # Our Model Hara et alOverall 3598 64.14 61.5NP 2317 65.08 64.2VP 465 71.82 54.2ADJP 321 74.76 80.4S 188 17.02 22.9PP 167 56.28 59.9UCP 60 51.66 36.7SBAR 56 91.07 51.8ADVP 21 80.95 85.7Others 3 33.33 66.7Table 3: Recall on the Beta version of Genia corpus.
Numbersfor Hara et al are taken from their paper.et al (2009), syntactic parsers do not perform wellon the Genia treebank.
Thus, in our symmetry com-ponent we opted to not rely on predicted tree struc-tures, and instead use the simpler option of repre-senting each conjunct by its sequence of POS tags.To handle coordination phrases with more than twoconjuncts, we extract candidates which includes upto 7 spans and integrate the first and the last spanin the model features.
Like Hara et al, we use goldPOS.Results Table 3 summarizes the results.
Our pro-posed model achieves Recall score of 64.14 (2.64Recall points gain over Hara et al) and significantlyimproves the score of several coordination types.6.3 Technical DetailsThe neural networks (candidate scoring model andsupporting classifiers) are implemented using thepyCNN package.10.In the supporting models we use words embed-ding of size 50 and the Sigmoid activation function.The LSTMs have a dimension of 50 as well.
Themodels are trained using SGD for 10 iterations overthe train-set, where samples are randomly shuffledbefore each iteration.
We choose the model with thehighest F1 score on the development set.All the LSTMs in the candidate scoring modelhave a dimension of 50.
The input vectors for the10https://github.com/clab/cnn/tree/master/pycnnAll types NPsP R F P R FSym 67.13 67.06 67.09 69.69 72.08 70.86Repp 69.26 69.18 69.21 69.73 71.16 70.43Repw 56.97 56.9 56.93 59.78 64.3 61.95Feats 70.92 70.83 70.87 72.23 73.22 72.72Joint 72.34 72.25 72.29 75.17 74.82 74.99Table 4: Performance of the individual components on PTBsection 22 (dev).
Sym: Symmetry.
Repp: POS replace-ment.
Repw: Word replacement.
Feats: features extracted fromBerkeley parser.
Joint: the complete model.symmetry LSTM is of size 50 as well.
The MLPin the candidate scoring model uses the Relu acti-vation function, and the model is trained using theAdam optimizer.
The words and POS embeddingsare shared between the symmetry and replacmentcomponents.
The syntactic label embeddings are forthe path-encoding LSTM, We perform grid searchwith 5 different seeds and the following: [1] MLPhidden layer size (100, 200, 400); [2] input embed-dings size for words, POS and syntactic labels (100,300).
We train for 20 iterations over the train set,randomly shuffling the examples before each itera-tion.
We choose the model that achieves the highestF1 score on the dev set.7 AnalysisOur model combines four signals: symmetry, word-level replacement, POS-level replacement and fea-tures from Berkeley parser.
Table 4 shows the PTBdev-set performance of each sub-model in isolation.On their own, each of the components?
signals isrelatively weak, seldom outperforming the parsers.However, they provide complementary information,as evident by the strong performance of the jointmodel.
Figure 4 lists correct and incorrect predic-tions by each of the components, indicating that theindividual models are indeed capturing the patternsthey were designed to capture ?
though these pat-terns do not always lead to correct predictions.308 Related WorkThe similarity property between conjuncts was ex-plored in several previous works on coordinationdisambiguation.
Hogan (2007) incorporated thisprinciple in a generative parsing model by changingthe generative process of coordinated NPs to condi-tion on properties of the first conjunct when gener-ating the second one.
Shimbo and Hara (2007) pro-posed a discriminative sequence alignment model todetect similar conjuncts.
They focused on disam-biguation of non-nested coordination based on thelearned edit distance between two conjuncts.
Theirwork was extended by Hara et al (2009) to han-dle nested coordinations as well.
The discrimina-tive edit distance model in these works is similar inspirit to our symmetry component, but is restrictedto sequences of POS-tags, and makes use of a se-quence alignment algorithm.
We compare our re-sults to Hara et al?s in Section 6.2.
Hanamoto et al(2012) extended the previous method with dual de-composition and HPSG parsing.
In contrast to thesesymmetry-directed efforts, Kawahara et al (2008)focuses on the dependency relations that surroundthe conjuncts.
This kind of semantic informationprovides an additional signal which is complemen-tary to the syntactic signals explored in our work.Our neural-network based model easily supports in-corporation of additional signals, and we plan to ex-plore such semantic signals in future work.9 ConclusionsWe presented an neural-network based model for re-solving conjuncts boundaries.
Our model is basedon the observation that (a) conjuncts tend to be sim-ilar and (b) that replacing the coordination phrasewith a conjunct results in a coherent sentence.
Ourmodels rely on syntactic information and do notincorporate resources external to the training tree-banks, yet improve over state-of-the-art parsers onthe coordination boundary prediction task.AcknowledgmentsThis work was supported by The Israeli ScienceFoundation (grant number 1555/15) as well asthe German Research Foundation via the German-Israeli Project Cooperation (DIP, grant DA 1600/1-1).ReferencesAnn Bies, Mark Ferguson, Karen Katz, Robert Mac-Intyre, Victoria Tredinnick, Grace Kim, Mary AnnMarcinkiewicz, and Britta Schasberger.
1995.
Brack-eting guidelines for treebank ii style penn treebankproject.
University of Pennsylvania, 97:100.Xavier Carreras, Michael Collins, and Terry Koo.
2008.Tag, dynamic programming, and the perceptron forefficient, feature-rich parsing.
In Proceedings of theTwelfth Conference on Computational Natural Lan-guage Learning, pages 9?16.
Association for Compu-tational Linguistics.Sumit Chopra, Raia Hadsell, and Yann LeCun.
2005.Learning a similarity metric discriminatively, with ap-plication to face verification.
In Computer Vision andPattern Recognition, 2005.
CVPR 2005.
IEEE Com-puter Society Conference on, volume 1, pages 539?546.
IEEE.David Dowty.
1988.
Type raising, functional compo-sition, and non-constituent conjunction.
In Catego-rial grammars and natural language structures, pages153?197.
Springer.Jeffrey L Elman.
1990.
Finding structure in time.
Cog-nitive science, 14(2):179?211.Jessica Ficler and Yoav Goldberg.
2016.
Coordinationannotation extension in the penn tree bank.
Associa-tion for Computational Linguistics.Joshua Goodman.
1998.
Parsing inside-out.
arXivpreprint cmp-lg/9805007.Atsushi Hanamoto, Takuya Matsuzaki, and Jun?ichi Tsu-jii.
2012.
Coordination structure analysis using dualdecomposition.
In Proceedings of the 13th Conferenceof the European Chapter of the Association for Com-putational Linguistics, pages 430?438.
Association forComputational Linguistics.Kazuo Hara, Masashi Shimbo, Hideharu Okuma, andYuji Matsumoto.
2009.
Coordinate structure analysiswith global structural constraints and alignment-basedlocal features.
In Proceedings of the Joint Conferenceof the 47th Annual Meeting of the ACL and the 4thInternational Joint Conference on Natural LanguageProcessing of the AFNLP: Volume 2-Volume 2, pages967?975.
Association for Computational Linguistics.Sepp Hochreiter and Ju?rgen Schmidhuber.
1997.
Longshort-term memory.
Neural computation, 9(8):1735?1780.Deirdre Hogan.
2007.
Coordinate noun phrase disam-biguation in a generative parsing model.
Associationfor Computational Linguistics.Rodney Huddleston, Geoffrey K Pullum, et al 2002.The cambridge grammar of english.
Language.
Cam-bridge: Cambridge University Press, page 1275.31Daisuke Kawahara and Sadao Kurohashi.
2008.
Coor-dination disambiguation without any similarities.
InProceedings of the 22nd International Conference onComputational Linguistics-Volume 1, pages 425?432.Association for Computational Linguistics.Mitchell P Marcus, Mary Ann Marcinkiewicz, and Beat-rice Santorini.
1993.
Building a large annotated cor-pus of english: The penn treebank.
Computational lin-guistics, 19(2):313?330.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word representa-tions in vector space.
arXiv preprint arXiv:1301.3781.Tomoko Ohta, Yuka Tateisi, and Jin-Dong Kim.
2002.The genia corpus: An annotated research abstract cor-pus in molecular biology domain.
In Proceedings ofthe second international conference on Human Lan-guage Technology Research, pages 82?86.
MorganKaufmann Publishers Inc.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of the 21stInternational Conference on Computational Linguis-tics and the 44th annual meeting of the Association forComputational Linguistics, pages 433?440.
Associa-tion for Computational Linguistics.Libin Shen, Anoop Sarkar, and Aravind K Joshi.
2003.Using ltag based features in parse reranking.
In Pro-ceedings of the 2003 conference on Empirical methodsin natural language processing, pages 89?96.
Associ-ation for Computational Linguistics.Masashi Shimbo and Kazuo Hara.
2007.
A discrimi-native learning model for coordinate conjunctions.
InEMNLP-CoNLL, pages 610?619.Yue Zhang and Stephen Clark.
2011.
Syntactic process-ing using the generalized perceptron and beam search.Computational linguistics, 37(1):105?151.32
