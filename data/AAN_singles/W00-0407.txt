Evaluation of Phrase-Representation Summarizationbased on Information Retrieval TaskMamiko OKA Yoshihiro UEDAIndustry Solutions Company,Fuji Xerox Co., Ltd.430 Sakai, Nakai-machi, Ashigarakami-gun, Kanagawa, Japan, 259-0157oka.mamiko@fujixerox.co.jp Ueda.Yoshihiro@fujixerox.co.jpAbstractWe have developed an improved task-basedevaluation method of summarization, theaccuracy of which is increased by specifyingthe details of the task including backgroundstories, and by assigning ten subjects persummary sample.
The method also servesprecision/recall pairs for a variety of situa-tions by introducing multiple levels ofrelevance assessment.
The method is appliedto prove phrase-represented summary ismost effective to select relevant documentsfrom information retrieval results.IntroductionSummaries are often used to select relevantdocuments from information retrieval results.The goal of summarization for such "indicative"use is to serve fast and accurate judgement.
Wehave developed the concept of the "at-a-glance"summary, and its realization in the Japaneselanguage - "phrase-representation summariza-tiola" - to achieve this goal (Ueda, et al 2000).We have conducted an evaluation experiment toverify the effectiveness of this summarizationmethod.There are two strategies for evaluatingsummarization systems: intrinsic and extrinsic(Jing, et al 1998).
Intrinsic methods measure asystem's quality mainly by comparing thesystem's output with an "ideal" summary.Extrinsic methods measure a system's perfor-mance in a particular task.
The aim of thephrase-representation summarization method isfast and accurate judgement in selectingdocuments in information retrieval.
Thus, weadopted a task-based method to evaluatewhether the goal was achieved.
Task-basedevaluation has recently drawn the attention inthq summarization field, because the assumptionthat there is only one "ideal" summary isconsidered to be incorrect, and some experi-ments on information retrieval were reported(Jing, et al 1998) (Mani, et al 1998) (Mochizu-ki and Okunura 1999).
However, there is nostandard evaluation method, and we considerthat there are some shortcomings in the existingmethods.
Thus, we have developed an improvedevaluation method and carried out a relativelylarge experiment.In this paper, we first give an overview of thephrase-representation summarization method.We then consider the evaluation method andshow the result of an experiment based on theimproved method to demonstrate he effective-ness of phrase-representation summarization.1 Phrase-Representation SummarizationMost automatic summarization systems adoptthe "sentence xtraction" method, which gives ascore to every sentence based on such charac-teristics as the frequency of a word or theposition where it appears, and selects entenceswith high scores.
In such a way, long andcomplex sentences tend to be extracted.However, a long and complex sentence isdifficult to read and understand, and therefore itis not a suitable unit to compose a summary foruse in selecting documents.To avoid the burden of reading such long andcomplex sentences, we have developed thephrase-representation summarization method,which represents he outline of a document by aseries of short and simple expressions("phrases") that contain key concepts.
We usethe word "phrase" to representthe simplicity59characteristic I in a word.The phrase-represented summary has thefollowing characteristics.
(1) At-a-glance comprehensionBecause ach unit is short and simple, the useris able to grasp the meaning at a glance.
(2) Adequate informativenessUnlike extracted sentences, phrases created bythis method are not accompanied by informa-tion unnecessary for relevance judgement.
(3) Wide coverage of topicsUnits composing a summary are relatively .short, and point various positions of theoriginal text.
Therefore, even a genericsummary includes various topics written in aJdocument.,~ phrase-represented summary is generatedas follows.1.
Syntactic analysis to extract he relation-ships between words2.
Selection of an important relation (twoword sequences connected by an arc) as a"core"3.
Addition of relations necessary for theunity of the phrase's meaning (e.g., essen-tial cases)4.
Generation of the surface phrase from theselected relationsAn important relation is selected byconsidering both the importance of a word andthat of  a relation between words.
For example,predicate-argument relations are consideredimportant and noun-modifier relations are givenlow importance scores.
Steps \[2\] to \[4\] arerepeated until specified amount of phrases areobtained.
Before selecting a new "core," thestores for the already selected words aredecreased to suppress overuse of the samewords.Fig.
1 shows a sample summary created froma news article 2 put on WWW.
The underlinedwords constitute the core relation of each phrase.The word "phrase" as used here is not used in thelinguistic sense, but an expression for "short" and"simple."
In Japanese, there is no rigid linguisticdistinction between a "phrase" mad a "clause.
"2 The original text in Japanese and its outline inEnglish can be seen in the following URL.http://www, fuiixerox.eo.ip/release/2000/0224..purcha.se.html (in Japanese)... acauire chemical toner business 3Fuji Xerox ... acouires chemical tonerbusiness of Nippon Carbide Industries Co.,Inc .
.
.
.... new chemical toner that contributes toreduce cost in  laser nrinters and to lowerenergy consumption ...... strengthen...supplies bu iness ...manufacturing facilities of Hayatsuki Plant, ...... uniform...each particle ...Fig.
1" A sample summary2 Evaluation Method2.1 Summar izat ion  Methods  to beComparedIn this experiment, we compare the effectivenessof phrase-represented summaries to summariescreated by other commonly used summarizationmethods.
From the viewpoint of  the phrase-represented summary, we focus the comparisonof the units that constitute summaries.
The unitsto be compared with phrases are sentences(created by the sentence xtraction method) andwords (by the keyword enumeration method).We also compare "leading fixed-lengthcharacters," which are often used as substitutesfor summaries by WWW search engines.
Thegeneration method for each summary isdescribed as follows.
(A) Leading fixed-length characters: extractthe first 80 characters of  the documentbody.
(B) Sentence xtraction summarization: selectimportant sentences from a document.The importance score of each sentence iscalculated from the simple sum of the im-portance scores of the words in a sentence(Zechner 1996).
(C) Phrase-representation summarization:described in Chapter 1.
(D) Keyword enumeration summarization: listup important words or compound nouns.http://www, fujixerox.co.jp/headlineJ2000/0308__ntone,r_biz,hlml (in English)s This phrase lacks the subject because the originalsentence lacks it.
Cases are usually omitted inJapanese if they can be easily inferred.60IIIII In (B), (C), and (D), the same method ofcalcuiating the importance scores of words is 18ilm~'-,~t~tmm\]m ~~~.~.
.~used in common, and lengths of summaries are Ill..I kept o be 60 to80 characters.
~ ~  .
, , ,~~e~As you can see each summary is generic, i.e.
.
/  not created for any specific queries.
Because the ~ .
~I phrase-representation summarization method isapplied to Japanese, we examine the effective- Relevanti )ness of these four methods in Japanese.
=__.- .1Relevant i~  2I 2.2 Previous WorkThe best-known example of task-based Irrelevant--- .... 3 -~  !
I ~I evaluation on information retrieval is the ad hoc ~ ~ ~  I task in the TIPSTER Text Summarization Evaluation Conference (SUMMAC) (Mani, et al ' ') Accuracy 1998).
Hand (1997) details the proposed task-based evaluation under TIPSTER.
Jing, et al(1998) describe how various parameters affectthe evaluation result through a relatively largetask-based experiment.
Evaluation conferenceslike SUMMAC are not yet held for Japanesesummarization systems 4.
Mochizuki andOkumura (1999) applied the SUMMACmethodology to Japanese summarizationmethods for the first time.
Most previousexperiments are concerned with SUMMAC,accordingly the methods resemble each other.2.3 Framework of Evaluation~Fhe framework of task-based evaluation oninformation retrieval is shown in Fig.
2.Task-based evaluation in general consists ofthe following three steps:(l) Data preparation: Assume an informationneed, create a query for the informationneed, and prepare simulated search resultswith different types of summaries.
(2) Relevance assessment: Using the summa-des, human subjects assess the relevanceof the search results to the assumed in-formation eeds.
(3) Measuring performance: Measure theaccuracy of the subjects' assessment bycomparing the subjects' judgement withthe correct relevance.
The assessmentprocess is also timed.4 It is planning to be held in 2000.
Furtherinformation is in the following URL.http://www.rd.nacsis.acAp/-ntcadm/workshop/ann2p-en.htmlFig.2: Framework of Task-Based EvaluationWe designed our evaluation method throughdetailed examination of previous work.
Theconsideration points are compared to theSUMMAC ad hoc task (Table l).
A sectionnumber will be found in the "*" column if wemade an improvement.
Details will be discussedin the section indicated by the number in thenext chapter.3 Improvements3.1 Description of QuestionsTo assess the relevance accurately, the situationof information retrieval should be realisticenough for the subjects to feel as if they reallywant to know about a given question.
Theprevious experiments gave only a shortdescription of a topic.
We consider it is notsufficiently specific and the interpretation of aquestion must varied with the subjects.We selected two topics ("moon cake" and"journey in Malay.
Peninsula") and assumedthree questions.
To indicate to the subjects, weset detailed situation including the motivation toknow about that or the use of the informationobtained for each question.
This method satisfiesthe restriction "to limit the variation inassessment between readers" in the MLUCEProtocol (Minel, et ai.
1997).61For each topic, ten documents are selectedfrom search results by major WWW searchengines, so that more than five relevantdocuments are included for each question.
Thetopics, the outline of the questions, the queriesfor WWW search, and the number of relevantdocuments are shown in Table 2.
The descrip-tion of Question-a2 that was given to thesubjects is shown in Fig.
3.One day just after the mid-autumn festival, mycolleague Mr. A brought some moon cakes tothe office.
He said that one of his Chinesefriends had given them to him.
They rooked sonew to us that we shared and ate them at acoffee break.
Chinese eat moon cakes at themid-autumn festival while Japanese havedumplings then.
Someone asked a questionwhy Chinese ate moon cakes, to-which nobodygave the answer.
Some cakes tasted sweet aswe  expected; some were stuffed with saltyfillings like roasted pork.
Ms. B said that therewere over fifty kinds of filling.
Her story mademe think of a question:What kinds of filling are there for mooncakes sold at the mid-autumn festival inChinese society?Fig.
3: An example of question (Question-a2)3.2 Number  of Subjects per  SummarySampleIn the previous experiments, one to threesubjects were assigned to each summary sample.Because the judgement must vary with thesubjects even if a detailed situation is given, weassigned ten subjects per summary sample toreduce the influence of each person's assessment.The only requirement for subjects is that theyshould be familiar with WWW search process.3.3 Relevance LevelsIn the previous experiments, a subject reads asummary and judges whether it is relevant orirrelevant.
However, a summary sometimes doesnot give enough information for relevancejudgement.
In actual information retrievalsituations, selecting criteria vary depending onthe question, the motivation, and othercircumstances.
We will not examine dubiousdocuments if sufficient information is obtainedor we do not have sufficient ime, and we willexamine dubious documents when an exhaustivesurvey is required.
Thus, here we introduce fourrelevance levels L0 to L3 to simulate variouscases in the experiment.
L3, L2, and L1 areconsidered relevant, the confidence becomeslower in order.
To reduce the variance ofinterpretation by subjects, we define each levelas follows.L3: The answer to the given question is found.in a summary.L2: A clue to the answer is found in a sum-mary.L l :Apparent clues are not found, but it isprobable that the answer is contained in thewhole document.L0: A summary is not relevant o the questionat all.If  these are applied to the case of the fare ofthe Malay Railway, the criteria will beinterpreted as follows.L3:An expression like "the berth charge ofthe second class is about RMI5"  is in asummary.L2: An expression like "I looked into the fareof the train" is in a summary.L I :A  summary describes about a trip by theMalay Railway, but the fare is not referred init.3.4 Measures of AccuracyIn the previous experiments, precision and recal lare used to measure accuracy.
There are twodrawbacks to these measurements: (1) thevariance of the subjects' assessment makes themeasure inaccurate, and (2) performance of eachsummary sample is not measured.Precision and recall are widely used tomeasure information retrieval performance.
Inthe evaluation of summarization, they arecalculated as follows.62IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIDocuments hat are actuallyrelevant in SPrecision = Documents hat are assessedrelevant by a subject (S)Documents hat are assessedrelevant by a subjectReca l l  = Relevant documentsIn the previous experiments, the assessmentstandard was not fixed, and some subjectstended to make the relevant set broader andothers narrower.
The variance reduces thesignificance of the average precision and recallvalue.
Because we introduced four relevancelevels and showed the assessment criteria to thesubjects, we can assume three kinds of relevancedocument sets: L3 only, L3 + L2, and L3 + L2 +L1.
The set composed only of the documentswith L3 assessment should have a high precisionscore.
This case represents a user wants to knowonly high-probability information, for example,the user is hurried, or just one answer issufficient.
The set including L1 documentsshould get a high recall score.
This caserepresents a user wants to know any informationconcerned with a specific question.Precision and recall represent he perfor-mance of a summarization method for certainquestion, however they do not indicate thereason why the method presents higher or lowerperformance.
To find the reasons and improve asummarization method based on them, it isuseful to analyze quality and performanceconnected together for each summary sample.Measuring each summary's performance isnecessary for such analysis.
Therefore, weintroduce the relevance score, which representsthe correspondence between the  subjectjudgement and the correct document relevance.The score of each pair of subject judgement anddocument relevance is shown in Table 3.By averaging scores of all subjects for everysample, summary's performances are compared.By averaging scores of all summary samples forevery summarization method, method'sperformances are compared.Table 1 : Experimental MethodDocument source NewspaperQuestionNumber of questionsNumber of documentsquestionSummary typeSummarizationmethodspersystems or(TREC collection)Selected from TREC topics2050User-focused summary11 systemsWWWNewly created, including 3.1the detailed situation310Generic summary4 methods that utilizedifferent unitsqm n ~&b~n~m~ ~ ~ ~ ~Subject 21 information analysts 40 persons who usuallyuse WWW searchNumber of subjects assigned to 1 or 2 10 3.2each summary sampleRelevance l vels 2 levels 4 levels 3.3(Relevant or irrelevant) (L0, LI, L2, L3)~:0)~:.
~ff6rmance.measurmgph.ase.
: . '
: ,  :...- .
?
:~".
.
i .,.
... : .
......
:.:.'.~:.
'.
::2:7::~,::,~ '~ :?~.~:~:~Measure  of accuracy Precision and recall Precision and recall \] 3.4Relevance score I63Q-a 1 MooncakeTable 2: Topics and Questions5Q-a2Q-b Journey inMalayPeninsulaWhat is the origin of the Chinese custom to have mooncakes inthe mid-autumn?moon cake&mid-autumn What kinds of fillings are there in moon cakes?About the train between Singapore and Bankok: SingaporeHow much does it cost?
&How long does it take?
BankokWhat is the difference in the equipment by the class?
&(A document containing one of these information is railwayregarded as relevant.
)6zTable 3: Relevance Score: ?
.
~ .
~ , ~ % _ , ~ % ~  .
.~~'~ ~ u ~  Relevant Relevant Relevant~ $ ~ ~  L3 L2 LI~ 10 8 5Relevant IrrelevantL0 L0-2 2IrrelevantL1-5IrrelevantL2-8IrrelevantL3-104 Exper iment Results4.1 Accuracy4.1.1 Precision and RecallThe precision and recall are shown in Fig.
4, andthe F-measure is shown in Fig.
5.
The F-measureis the balanced score of precision and recall,calculated as follows:2 * precision * recall"- F-measure = precision + recallFigures 4 and 5 show that the phrase-represented summary (C) presents the highestperformance.
It satisfies both the high precisionand the high recall requirements.
Because thereare various situations in WWW searches,phrase-representation sumtnarization isconsidered suitable in any cases.4.1.2 Relevance ScoreThe relevance score for each question is shownin Fig.
6.
The phrase-represented summary (C)gets the highest score on average, and the best inQuestion-a2 and Question-b.
For Question-al,though all summaries get poor scores, thesentence xtraction summary (B) is the bestamong them.4.2 TimeThe time required to assess relevance is shownin Fig.
7.
The time for Question-a isa sum of thetimes for Questions al and a2.
In the Question-acase, phrase-represented summary (C) requiresthe shortest time.
For Question-b, leading fixed-length characters (A) requires the shortest time,and this result is different from the intuition.This requires further examination.64IIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIIc -o .ut~ "5t~Only L30.90.80.70.60.50.4L3 + L2--O.-- ............... - ] "  .........,0  ~ ~- .
- ,- "+"* - - : ' i  .
.
.
.
.
.
.
.
.
.
.
.?
1 ,3 .~+_L I .
_ J0 0.5 1RecallFig.
4: Precision.& Recall- .
-  A I?
--A- B~ C--o-- D'E,0.810.77 -" .
.
.
.
.O.6.-.
: ~ -- A0.5~-0.4" --5 ?
B0.3 ?
C0.11~ .
ND...1 ,...1 +,-" _.1O + ,aHigh presicion High recallFig.
5: F-measuret,.,OOOOt -5~i:43210A B C DFig.
6: Relevance Score-_3 Q-a 1:[] Q-a2iUQ-b ii?
Aver lage :"C.m.350~3025,20,15,10vA B CFig.
7: Time?
Q-aiQ-bi AverageDTable 4: Summaries Containing CluesQ-alA[B IC ID5A[  B~'ac  [ D6Q-bA t B I c I  D7o l  3 151  6- 17.73 I 8 I 6.435 DiscussionHere we analyze the experiment result frommultiple viewpoint,s: the constituent unit ofsummaries and .the characteristics of questionsand documents in Section 5.1 and 5.2.
We thendiscuss advantages of our experimental methodin Section 5.3, and language dependency of theexperiment result in Section 5.4.655.1 Comparison of Constituent UnitThe units that constitute a summary may affectthe judging process; if the unit is long, thenumber of units appeared in a summary maydecrease and the summary contains fewer keyconcepts in the original document.
We countedthe number of the summaries that contain theclues to the given questions (see Table 4).
Theaverage numbers are 0.3, 2.0, 4.3 and 4.7 for (A)fixed-length characters, (B) sentences, (C)phrases and (D) words, respectively.
The phrase-represented summary (C) and the keywordenumeration summary (D) widely cover thetopics, and they are about twice as wide as thesentence xtraction summary (B).
The leadingfixed-length characters (A) contain very fewclues and this fact supports that this summarypresents the worst performance (see Section4.1).In order to compare a summary's perfor-mance with a summary's quality, we calculatethe average relevance score of summaries thatcontain clues.
These scores are also shown inTable 4.
The average score represents theinformativeness of each summary.
Table 4shows that the sentence xtraction summary (B)and the phrase-represented summary (C) get, relatively high scores, but vary with the question.Th is  is because sentences and phrases aresufficiently informative in most cases, butsentences tend to contain unnecessary informa-tion, and phrases tend to lack necessaryinformation.
The keyword enumerationsummary (D) gets a relatively low score.
This isbecause a word is not sufficiently informative toenable judgement of whether it is clue to theanswer, and relations among words are lacked.These analyses upport he two characteris-tics of the phrase-represented summariesdescribed in Chapter 1, that is, adequateinformativeness and wide coverage of topics.5.2 Influence of Question and DocumentThe most suitable summarization method maydepend on the type of question and/or document.In the experiment results (see Section 4.1.2), thesentence extraction summary (B) and the phrase-represented summary (C) get the highestrelevance score.
Therefore, here we focus onthose two summarization methods and considerthe influence of questions and documents.In selecting questions, we consider twofactors may affect performance.
One is whichunit an answer is expressed in.
Another iswhether clue words easily come to mind.If an answer is expressed as a relation of apredicate to its arguments, the phrase-representation summarization may be suitable.Question-a2 and Question-b are of this case.
Ifan answer is expressed as compound relations,e.g., reason-consequence relations or cause-result relations, the sentence extractionsummarization may be required.
And, if ananswer is expressed in complex relations ofsentences, any summarization method of thefour is not suitable.
Questions that sk historicalbackground or complicated procedures areexamples of this kind, e.g., Question-al.As for another factor, if clue words easilycome to mind, the phrase-represented summaryis suitable for any unit in which an answer isexpressed.
This is because the clues are foundmore easily in short phrases than in longsentences.In selecting documents, whether a question isrelevant to the main topic of a document affectsthe performance, because we use genericsummaries.
By sentence xtraction summariza-tion, the answer is extracted as a summary onlywhen the question is relevant o the main topic.Phrase-represented summary is able to covertopics more widely, for example, one of themain topics or detailed escription of each topic(see Section 5.1).
Because the characteristic ofthe document is independent of the question,which summaries cannot be predicted, and thusthe phrase-represented summary will give betterresults.Through these discussions, we conclude thatthe phrase-representation summarization issuitable for various cases, while the sentenceextraction summarization is for only somerestricted cases.
Though the samples ofquestions and documents are relatively few inour experiment, it is sufficient to show theeffectiveness of the phrase-representationsummarization.665.3 Advantages of our ExperimentalMethodOur experimental method has the followingadvantages.
(1) More exact assessment(2)Serves precision/recall pairs for a variety ofsituations(3)Helps further analysis of problems of asummarization method5.3.1 More Exact AssessmentOur experimental method provides more exactrelevance assessment in the following ways.
(a) More detailed escription of a questionWe asked the subjects to assess the relevance offull documents to each question after theexperiment.
Result shows that 93% of thesubject judgements match the assumed relevance,while only 69% match in the same kind ofassessment in SUMMAC.
The percentage thatall judgements per document agreed theassumed relevance is 33%, while only 17% inSUMMAC.
This is because the subjectscomprehended the questions correctly by givendetailed information about he situation.
(b) More subjects assigned per summary sampleWe assigned ten subjects to each summarysample, while only one or two subjects wereused in SUMMAC.
We examined the differenceof judgement between the average of tensubjects and the first subject of the ten.
Result"shows that 47% of the first subject's judgementdiffer more than one level from the average.This proves that the assessment varies from onesubject o another, even if a detailed situation isgiven.
(c) Finer levels of relevanceWe introduced four levels of relevance, bywhich ambiguity of relevance can be expressedbetter.5.3.2 Serves precision~recall pairs for a varie(Fof situationsAccording to the four levels of relevance, weassume three kinds of relevance document sets.This enables to plot the PR curve.In evaluation conferences like SUMMAC,various summarization methods that aredeveloped for different purposes must becompared.
Using such a PR curve, each methodcan be compared in a criterion that matches itspurpose.5.3.3 Helps further analysis of problems of asummarization methodWe have introduced the relevance score, whichallows each summary to be evaluated.
Using thisscore, we can analyze the extrinsic evaluationresult and the intrinsic evaluation resultconnected together, for example, an evaluationresult based on information retrieval task andthat based on Q & A task using the samequestions.
Through such analyses, the textquality of summaries or the adequate informa-tiveness can be xamined.
We ourselves got a lotof benefit from the analysis to find problems andimprove the quality of the summary.5.4 Language dependencyThough experiment method may be applied toany other languages, we must consider thepossibility that our result depends on thelanguage characteristics.
Japanese text is writtenby mixing several kinds of characters; Kanacharacters (Hiragana and Katakana) and Kanji(Chinese) characters, and alphabetic haractersare also used.
Kanji characters are mainly usedto represent concept words and Hiraganacharacters are used for function words.
The factthat they play the different roles makes it easy tofind the full words.
Also Kanji is a kind ofideogram and each character has its ownmeaning.
Thus, most words can be expressed by1 to 3 Kanji characters tomake short phrases (15- 20 characters) ufficiently informative.Though the basic algorithm to create phrase-represented summary itself can be applied toother languages by replacing its analysiscomponent and generation component, similarexperiment in that language is required to provethe effectiveness of the phrase-representedsummary.ConclusionWe proposed an improved method of task-basedevaluation on information retrieval.
This methodcan be used to evaluate the performance ofsummarization methods more accurately than ispossible by the methods used in previous work.We carried out a relatively large experimentusing this method, the results of which show that67phrase-representation summarization is effectiveto select relevant documents from informationretrieval results.AcknowledgementsWe would like to thank our company memberswho gave valuable suggestions and participatedin the experiment.ReferencesHand, T. F. (1997).
"A Proposal for Task-basedEvaluation of Text Summarization Systems."
InProceedings of the ACL/EACL'97 Workshop onIntelligent Scalable Text Summarization, pp31-38.Jing, H., Barzilay, R., McKeown, K. and Elhadad, M.(1998).
"Summarization Evaluation Methods:Experiments and Analysis."
In Intelligent TextSummarization.
pp51-59.
AAAI Press.Mani, I., House, D., Klein,G., Hirschman, L.
Obrst,L., Firmin, T., Chizanowski, M., and Sundheim, B.(1998).
"'The TIPSTER SUMMAC Text Summari-zation Evaluation.'"
Technical Report MTR98W0000138, MITRE Technical Report.Minel, J.-L., Nugier, S. and Piat, G. (1997).
"'How toAppreciate the Quality of Automatic Text Summa-rization?
Examples of FAN and MLUCE Protocolsand their Results on SERAPHIN."
In Proc.
of theACL/EACL'97 Workshop on Intelligent ScalableText Summarization, pp.25-30.Mochizuki, H and Okumura, M. (1999).
"Evaluationof Summarization Methods based on InformationRetrieval Task."
In Notes of SIGNL of theInformation Processing Society of Japan, 99-NL-132, pp41-48.
(In Japanese)Salton, G. (1989).
Automatic Text Processing: TheTransformation, Analysis, and Retrieval ot""- Information by Computer.
Addison-WesleyPublishing Company, Inc.Ueda, Y., Oka, M., Koyama, T. and Miyauchi, T.(2000).
"Toward the "At-a-glance" Summary:Phrase-representation Summarization Method.
"submitted to COLING2000.Zechner, K. (1996).
"Fast Generation of Abstractsfrom General Domain Text Corpora by ExtractingRelevant Sentences."
In Proc.
of COLING-96, pp.986-989.68
