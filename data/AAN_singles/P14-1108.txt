Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 1145?1154,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsA Generalized Language Model as the Combination of Skipped n-gramsand Modified Kneser-Ney SmoothingRene Pickhardt, Thomas Gottron, Martin K?orner, Steffen StaabInstitute for Web Science and Technologies,University of Koblenz-Landau, Germany{rpickhardt,gottron,mkoerner,staab}@uni-koblenz.dePaul Georg Wagner and Till SpeicherTypology GbRmail@typology.deAbstractWe introduce a novel approach for build-ing language models based on a system-atic, recursive exploration of skip n-grammodels which are interpolated using modi-fied Kneser-Ney smoothing.
Our approachgeneralizes language models as it containsthe classical interpolation with lower or-der models as a special case.
In this pa-per we motivate, formalize and presentour approach.
In an extensive empiricalexperiment over English text corpora wedemonstrate that our generalized languagemodels lead to a substantial reduction ofperplexity between 3.1% and 12.7% incomparison to traditional language mod-els using modified Kneser-Ney smoothing.Furthermore, we investigate the behaviourover three other languages and a domainspecific corpus where we observed consis-tent improvements.
Finally, we also showthat the strength of our approach lies inits ability to cope in particular with sparsetraining data.
Using a very small train-ing data set of only 736 KB text we yieldimprovements of even 25.7% reduction ofperplexity.1 Introduction motivationLanguage Models are a probabilistic approach forpredicting the occurrence of a sequence of words.They are used in many applications, e.g.
wordprediction (Bickel et al, 2005), speech recogni-tion (Rabiner and Juang, 1993), machine trans-lation (Brown et al, 1990), or spelling correc-tion (Mays et al, 1991).
The task language modelsattempt to solve is the estimation of a probabilityof a given sequence of words wl1= w1, .
.
.
, wl.The probability P (wl1) of this sequence can bebroken down into a product of conditional prob-abilities:P (wl1) =P (w1) ?
P (w2|w1) ?
.
.
.
?
P (wl|w1?
?
?wl?1)=l?i=1P (wi|w1?
?
?wi?1) (1)Because of combinatorial explosion and datasparsity, it is very difficult to reliably estimate theprobabilities that are conditioned on a longer sub-sequence.
Therefore, by making a Markov as-sumption the true probability of a word sequenceis only approximated by restricting conditionalprobabilities to depend only on a local contextwi?1i?n+1of n ?
1 preceding words rather than thefull sequencewi?11.
The challenge in the construc-tion of language models is to provide reliable esti-mators for the conditional probabilities.
While theestimators can be learnt?using, e.g., a maximumlikelihood estimator over n-grams obtained fromtraining data?the obtained values are not very re-liable for events which may have been observedonly a few times or not at all in the training data.Smoothing is a standard technique to over-come this data sparsity problem.
Various smooth-ing approaches have been developed and ap-plied in the context of language models.
Chenand Goodman (Chen and Goodman, 1999) in-troduced modified Kneser-Ney Smoothing, whichup to now has been considered the state-of-the-art method for language modelling over the last15 years.
Modified Kneser-Ney Smoothing isan interpolating method which combines the es-timated conditional probabilities P (wi|wi?1i?n+1)recursively with lower order models involving ashorter local contextwi?1i?n+2and their estimate forP (wi|wi?1i?n+2).
The motivation for using lowerorder models is that shorter contexts may be ob-served more often and, thus, suffer less from datasparsity.
However, a single rare word towards theend of the local context will always cause the con-text to be observed rarely in the training data andhence will lead to an unreliable estimation.1145Because of Zipfian word distributions, mostwords occur very rarely and hence their true prob-ability of occurrence may be estimated only verypoorly.
One word that appears at the end of a localcontext wi?1i?n+1and for which only a poor approx-imation exists may adversely affect the conditionalprobabilities in language models of all lengths ?leading to severe errors even for smoothed lan-guage models.
Thus, the idea motivating our ap-proach is to involve several lower order modelswhich systematically leave out one position in thecontext (one may think of replacing the affectedword in the context with a wildcard) instead ofshortening the sequence only by one word at thebeginning.This concept of introducing gaps in n-gramsis referred to as skip n-grams (Ney et al, 1994;Huang et al, 1993).
Among other techniques, skipn-grams have also been considered as an approachto overcome problems of data sparsity (Goodman,2001).
However, to best of our knowledge, lan-guage models making use of skip n-grams mod-els have never been investigated to their full ex-tent and over different levels of lower order mod-els.
Our approach differs as we consider all pos-sible combinations of gaps in a local context andinterpolate the higher order model with all possi-ble lower order models derived from adding gapsin all different ways.In this paper we make the following contribu-tions:1.
We provide a framework for using modifiedKneser-Ney smoothing in combination with asystematic exploration of lower order modelsbased on skip n-grams.2.
We show how our novel approach can indeedeasily be interpreted as a generalized versionof the current state-of-the-art language mod-els.3.
We present a large scale empirical analysisof our generalized language models on eightdata sets spanning four different languages,namely, a wikipedia-based text corpus andthe JRC-Acquis corpus of legislative texts.4.
We empirically observe that introducing skipn-gram models may reduce perplexity by12.7% compared to the current state-of-the-art using modified Kneser-Ney models onlarge data sets.
Using small training data setswe observe even higher reductions of per-plexity of up to 25.6%.The rest of the paper is organized as follows.We start with reviewing related work in Section 2.We will then introduce our generalized languagemodels in Section 3.
After explaining the evalua-tion methodology and introducing the data sets inSection 4 we will present the results of our evalu-ation in Section 5.
In Section 6 we discuss why ageneralized language model performs better thana standard language model.
Finally, in Section 7we summarize our findings and conclude with anoverview of further interesting research challengesin the field of generalized language models.2 Related WorkWork related to our generalized language modelapproach can be divided in two categories: var-ious smoothing techniques for language modelsand approaches making use of skip n-grams.Smoothing techniques for language modelshave a long history.
Their aim is to overcome datasparsity and provide more reliable estimators?inparticular for rare events.
The Good Turing es-timator (Good, 1953), deleted interpolation (Je-linek and Mercer, 1980), Katz backoff (Katz,1987) and Kneser-Ney smoothing (Kneser andNey, 1995) are just some of the approaches tobe mentioned.
Common strategies of these ap-proaches are to either backoff to lower order mod-els when a higher order model lacks sufficienttraining data for good estimation, to interpolatebetween higher and lower order models or to inter-polate with a prior distribution.
Furthermore, theestimation of the amount of unseen events fromrare events aims to find the right weights for in-terpolation as well as for discounting probabilitymass from unreliable estimators and to retain it forunseen events.The state of the art is a modified version ofKneser-Ney smoothing introduced in (Chen andGoodman, 1999).
The modified version imple-ments a recursive interpolation with lower ordermodels, making use of different discount valuesfor more or less frequently observed events.
Thisvariation has been compared to other smooth-ing techniques on various corpora and has shownto outperform competing approaches.
We willreview modified Kneser-Ney smoothing in Sec-tion 2.1 in more detail as we reuse some ideas todefine our generalized language model.1146Smoothing techniques which do not rely on us-ing lower order models involve clustering (Brownet al, 1992; Ney et al, 1994), i.e.
grouping to-gether similar words to form classes of words, aswell as skip n-grams (Ney et al, 1994; Huang etal., 1993).
Yet other approaches make use of per-mutations of the word order in n-grams (Schukat-Talamazzini et al, 1995; Goodman, 2001).Skip n-grams are typically used to incorporatelong distance relations between words.
Introduc-ing the possibility of gaps between the words inan n-gram allows for capturing word relations be-yond the level of n consecutive words without anexponential increase in the parameter space.
How-ever, with their restriction on a subsequence ofwords, skip n-grams are also used as a techniqueto overcome data sparsity (Goodman, 2001).
In re-lated work different terminology and different def-initions have been used to describe skip n-grams.Variations modify the number of words which canbe skipped between elements in an n-gram as wellas the manner in which the skipped words are de-termined (e.g.
fixed patterns (Goodman, 2001) orfunctional words (Gao and Suzuki, 2005)).The impact of various extensions and smooth-ing techniques for language models is investigatedin (Goodman, 2001; Goodman, 2000).
In partic-ular, the authors compared Kneser-Ney smooth-ing, Katz backoff smoothing, caching, clustering,inclusion of higher order n-grams, sentence mix-ture and skip n-grams.
They also evaluated com-binations of techniques, for instance, using skipn-gram models in combination with Kneser-Neysmoothing.
The experiments in this case followedtwo paths: (1) interpolating a 5-gram model withlower order distribution introducing a single gapand (2) interpolating higher order models withskip n-grams which retained only combinations oftwo words.
Goodman reported on small data setsand in the best case a moderate improvement ofcross entropy in the range of 0.02 to 0.04.In (Guthrie et al, 2006), the authors investi-gated the increase of observed word combinationswhen including skips in n-grams.
The conclusionwas that using skip n-grams is often more effectivefor increasing the number of observations than in-creasing the corpus size.
This observation alignswell with our experiments.2.1 Review of Modified Kneser-NeySmoothingWe briefly recall modified Kneser-Ney Smoothingas presented in (Chen and Goodman, 1999).
Mod-ified Kneser-Ney implements smoothing by inter-polating between higher and lower order n-gramlanguage models.
The highest order distributionis interpolated with lower order distribution as fol-lows:PMKN(wi|wi?1i?n+1) =max{c(wii?n+1) ?
D(c(wii?n+1)), 0}c(wi?1i?n+1)+ ?high(wi?1i?n+1)?PMKN(wi|wi?1i?n+2) (2)where c(wii?n+1) provides the frequency countthat sequence wii?n+1occurs in training data, D isa discount value (which depends on the frequencyof the sequence) and ?highdepends onD and is theinterpolation factor to mix in the lower order dis-tribution1.
Essentially, interpolation with a lowerorder model corresponds to leaving out the firstword in the considered sequence.
The lower ordermodels are computed differently using the notionof continuation counts rather than absolute counts:?PMKN(wi|(wi?1i?n+1)) =max{N1+(?wii?n+1) ?
D(c(wii?n+1)), 0}N1+(?wi?1i?n+1?
)+ ?mid(wi?1i?n+1)?PMKN(wi|wi?1i?n+2)) (3)where the continuation counts are defined asN1+(?wii?n+1) = |{wi?n: c(wii?n) > 0}|, i.e.the number of different words which precede thesequencewii?n+1.
The term ?midis again an inter-polation factor which depends on the discountedprobability mass D in the first term of the for-mula.3 Generalized Language Models3.1 Notation for Skip n-gram with k SkipsWe express skip n-grams using an operator no-tation.
The operator ?iapplied to an n-gramremoves the word at the i-th position.
For in-stance: ?3w1w2w3w4= w1w2w4, where isused as wildcard placeholder to indicate a re-moved word.
The wildcard operator allows for1The factors ?
and D are quite technical and lengthy.
Asthey do not play a significant role for understanding our novelapproach we refer to Appendix A for details.1147larger number of matches.
For instance, whenc(w1w2w3aw4) = x and c(w1w2w3bw4) = y thenc(w1w2w4) ?
x + y since at least the two se-quences w1w2w3aw4and w1w2w3bw4match thesequence w1w2w4.
In order to align with stan-dard language models the skip operator applied tothe first word of a sequence will remove the wordinstead of introducing a wildcard.
In particular theequation ?1wii?n+1= wii?n+2holds where theright hand side is the subsequence ofwii?n+1omit-ting the first word.
We can thus formulate the in-terpolation step of modified Kneser-Ney smooth-ing using our notation as?PMKN(wi|wi?1i?n+2) =?PMKN(wi|?1wi?1i?n+1).Thus, our skip n-grams correspond to n-gramsof which we only use k words, after having appliedthe skip operators ?i1.
.
.
?in?k3.2 Generalized Language ModelInterpolation with lower order models is motivatedby the problem of data sparsity in higher ordermodels.
However, lower order models omit onlythe first word in the local context, which might notnecessarily be the cause for the overall n-gram tobe rare.
This is the motivation for our general-ized language models to not only interpolate withone lower order model, where the first word in asequence is omitted, but also with all other skip n-gram models, where one word is left out.
Combin-ing this idea with modified Kneser-Ney smoothingleads to a formula similar to (2).PGLM(wi|wi?1i?n+1) =max{c(wii?n+1) ?
D(c(wii?n+1)), 0}c(wi?1i?n+1)+ ?high(wi?1i?n+1)n?1?j=11n?1?PGLM(wi|?jwi?1i?n+1)(4)The difference between formula (2) and formula(4) is the way in which lower order models areinterpolated.Note, the sum over all possible positions inthe context wi?1i?n+1for which we can skip aword and the according lower order modelsPGLM(wi|?j(wi?1i?n+1)).
We give all lower ordermodels the same weight1n?1.The same principle is recursively applied in thelower order models in which some words of thefull n-gram are already skipped.
As in modi-fied Kneser-Ney smoothing we use continuationcounts for the lower order models, incorporatingthe skip operator also for these counts.
Incor-porating this directly into modified Kneser-Neysmoothing leads in the second highest model to:?PGLM(wi|?j(wi?1i?n+1)) = (5)max{N1+(?j(wii?n)) ?
D(c(?j(wii?n+1))), 0}N1+(?j(wi?1i?n+1)?
)+?mid(?j(wi?1i?n+1))n?1?k=1k 6=j1n?2?PGLM(wi|?j?k(wi?1i?n+1))Given that we skip words at different positions,we have to extend the notion of the count functionand the continuation counts.
The count functionapplied to a skip n-gram is given by c(?j(wii?n))=?wjc(wii?n), i.e.
we aggregate the count informa-tion over all words which fill the gap in the n-gram.
Regarding the continuation counts we de-fine:N1+(?j(wii?n)) = |{wi?n+j?1:c(wii?n)>0}| (6)N1+(?j(wi?1i?n)?)
= |{(wi?n+j?1, wi) :c(wii?n)>0}| (7)As lowest order model we use?just as done fortraditional modified Kneser-Ney (Chen and Good-man, 1999)?a unigram model interpolated with auniform distribution for unseen words.The overall process is depicted in Figure 1, il-lustrating how the higher level models are recur-sively smoothed with several lower order ones.4 Experimental Setup and Data SetsTo evaluate the quality of our generalized lan-guage models we empirically compare their abil-ity to explain sequences of words.
To this end weuse text corpora, split them into test and trainingdata, build language models as well as generalizedlanguage models over the training data and applythem on the test data.
We employ established met-rics, such as cross entropy and perplexity.
In thefollowing we explain the details of our experimen-tal setup.4.1 Data SetsFor evaluation purposes we employed eight differ-ent data sets.
The data sets cover different domainsand languages.
As languages we considered En-glish (en), German (de), French (fr), and Italian(it).
As general domain data set we used the fullcollection of articles from Wikipedia (wiki) in thecorresponding languages.
The download dates ofthe dumps are displayed in Table 1.1148Figure 1: Interpolation of models of different or-der and using skip patterns.
The value of n in-dicates the length of the raw n-grams necessaryfor computing the model, the value of k indicatesthe number of words actually used in the model.The wild card symbol marks skipped words inan n-gram.
The arrows indicate how a higher or-der model is interpolated with lower order mod-els which skips one word.
The bold arrows cor-respond to interpolation of models in traditionalmodified Kneser-Ney smoothing.
The lighter ar-rows illustrate the additional interpolations intro-duced by our generalized language models.de en fr itNov 22ndNov 04thNov 20thNov 25thTable 1: Download dates of Wikipedia snapshotsin November 2013.Special purpose domain data are provided bythe multi-lingual JRC-Acquis corpus of legislativetexts (JRC) (Steinberger et al, 2006).
Table 2gives an overview of the data sets and providessome simple statistics of the covered languagesand the size of the collections.StatisticsCorpus total words unique wordsin Mio.
in Mio.wiki-de 579 9.82JRC-de 30.9 0.66wiki-en 1689 11.7JRC-en 39.2 0.46wiki-fr 339 4.06JRC-fr 35.8 0.46wiki-it 193 3.09JRC-it 34.4 0.47Table 2: Word statistics and size of of evaluationcorporaThe data sets come in the form of structured textcorpora which we cleaned from markup and tok-enized to generate word sequences.
We filtered theword tokens by removing all character sequenceswhich did not contain any letter, digit or commonpunctuation marks.
Eventually, the word token se-quences were split into word sequences of lengthn which provided the basis for the training andtest sets for all algorithms.
Note that we did notperform case-folding nor did we apply stemmingalgorithms to normalize the word forms.
Also,we did our evaluation using case sensitive trainingand test data.
Additionally, we kept all tokens fornamed entities such as names of persons or places.4.2 Evaluation MethodologyAll data sets have been randomly split into a train-ing and a test set on a sentence level.
The train-ing sets consist of 80% of the sentences, whichhave been used to derive n-grams, skip n-gramsand corresponding continuation counts for valuesof n between 1 and 5.
Note that we have traineda prediction model for each data set individually.From the remaining 20% of the sequences we haverandomly sampled a separate set of 100, 000 se-quences of 5 words each.
These test sequenceshave also been shortened to sequences of length 3,and 4 and provide a basis to conduct our final ex-periments to evaluate the performance of the dif-ferent algorithms.We learnt the generalized language models onthe same split of the training corpus as the stan-dard language model using modified Kneser-Neysmoothing and we also used the same set of test se-quences for a direct comparison.
To ensure rigourand openness of research the data set for trainingas well as the test sequences and the entire sourcecode is open source.2 3 4We compared theprobabilities of our language model implementa-tion (which is a subset of the generalized languagemodel) using KN as well as MKN smoothing withthe Kyoto Language Model Toolkit5.
Since wegot the same results for small n and small data setswe believe that our implementation is correct.In a second experiment we have investigatedthe impact of the size of the training data set.The wikipedia corpus consists of 1.7 bn.
words.2http://west.uni-koblenz.de/Research3https://github.com/renepickhardt/generalized-language-modeling-toolkit4http://glm.rene-pickhardt.de5http://www.phontron.com/kylm/1149Thus, the 80% split for training consists of 1.3 bn.words.
We have iteratively created smaller train-ing sets by decreasing the split factor by an orderof magnitude.
So we created 8% / 92% and 0.8%/ 99.2% split, and so on.
We have stopped at the0.008%/99.992% split as the training data set inthis case consisted of less words than our 100ktest sequences which we still randomly sampledfrom the test data of each split.
Then we traineda generalized language model as well as a stan-dard language model with modified Kneser-Neysmoothing on each of these samples of the train-ing data.
Again we have evaluated these languagemodels on the same random sample of 100, 000sequences as mentioned above.4.3 Evaluation MetricsAs evaluation metric we use perplexity: a standardmeasure in the field of language models (Manningand Sch?utze, 1999).
First we calculate the crossentropy of a trained language model given a testset usingH(Palg) = ?
?s?TPMLE(s) ?
log2Palg(s) (8)Where Palgwill be replaced by the probabilityestimates provided by our generalized languagemodels and the estimates of a language model us-ing modified Kneser-Ney smoothing.
PMLE, in-stead, is a maximum likelihood estimator of thetest sequence to occur in the test corpus.
Finally,T is the set of test sequences.
The perplexity isdefined as:Perplexity(Palg) = 2H(Palg)(9)Lower perplexity values indicate better results.5 Results5.1 BaselineAs a baseline for our generalized language model(GLM) we have trained standard language modelsusing modified Kneser-Ney Smoothing (MKN).These models have been trained for model lengths3 to 5.
For unigram and bigram models MKN andGLM are identical.5.2 Evaluation ExperimentsThe perplexity values for all data sets and variousmodel orders can be seen in Table 3.
In this tablewe also present the relative reduction of perplexityin comparison to the baseline.model lengthExperiments n = 3 n = 4 n = 5wiki-de MKN 1074.1 778.5 597.1wiki-de GLM 1031.1 709.4 521.5rel.
change 4.0% 8.9% 12.7%JRC-de MKN 235.4 138.4 94.7JRC-de GLM 229.4 131.8 86.0rel.
change 2.5% 4.8% 9.2%wiki-en MKN 586.9 404 307.3wiki-en GLM 571.6 378.1 275rel.
change 2.6% 6.1% 10.5%JRC-en MKN 147.2 82.9 54.6JRC-en GLM 145.3 80.6 52.5rel.
change 1.3% 2.8% 3.9%wiki-fr MKN 538.6 385.9 298.9wiki-fr GLM 526.7 363.8 272.9rel.
change 2.2% 5.7% 8.7%JRC-fr MKN 155.2 92.5 63.9JRC-fr GLM 153.5 90.1 61.7rel.
change 1.1% 2.5% 3.5%wiki-it MKN 738.4 532.9 416.7wiki-it GLM 718.2 500.7 382.2rel.
change 2.7% 6.0% 8.3%JRC-it MKN 177.5 104.4 71.8JRC-it GLM 175.1 101.8 69.6rel.
change 1.3% 2.6% 3.1%Table 3: Absolute perplexity values and relativereduction of perplexity from MKN to GLM on alldata sets for models of order 3 to 5As we can see, the GLM clearly outperformsthe baseline for all model lengths and data sets.In general we see a larger improvement in perfor-mance for models of higher orders (n = 5).
Thegain for 3-gram models, instead, is negligible.
ForGerman texts the increase in performance is thehighest (12.7%) for a model of order 5.
We alsonote that GLMs seem to work better on broad do-main text rather than special purpose text as thereduction on the wiki corpora is constantly higherthan the reduction of perplexity on the JRC cor-pora.We made consistent observations in our secondexperiment where we iteratively shrank the sizeof the training data set.
We calculated the rela-tive reduction in perplexity from MKN to GLM1150for various model lengths and the different sizesof the training data.
The results for the EnglishWikipedia data set are illustrated in Figure 2.We see that the GLM performs particularly wellon small training data.
As the size of the trainingdata set becomes smaller (even smaller than theevaluation data), the GLM achieves a reduction ofperplexity of up to 25.7% compared to languagemodels with modified Kneser-Ney smoothing onthe same data set.
The absolute perplexity valuesfor this experiment are presented in Table 4.model lengthExperiments n = 3 n = 4 n = 580% MKN 586.9 404 307.380% GLM 571.6 378.1 275rel.
change 2.6% 6.5% 10.5%8% MKN 712.6 539.8 436.58% GLM 683.7 492.8 382.5rel.
change 4.1% 8.7% 12.4%0.8% MKN 894.0 730.0 614.10.8% GLM 838.7 650.1 528.7rel.
change 6.2% 10.9% 13.9%0.08% MKN 1099.5 963.8 845.20.08% GLM 996.6 820.7 693.4rel.
change 9.4% 14.9% 18.0%0.008% MKN 1212.1 1120.5 1009.60.008% GLM 1025.6 875.5 750.3rel.
change 15.4% 21.9% 25.7%Table 4: Absolute perplexity values and relativereduction of perplexity from MKN to GLM onshrunk training data sets for the EnglishWikipediafor models of order 3 to 5Our theory as well as the results so far suggestthat the GLM performs particularly well on sparsetraining data.
This conjecture has been investi-gated in a last experiment.
For each model lengthwe have split the test data of the largest EnglishWikipedia corpus into two disjoint evaluation datasets.
The data set unseen consists of all test se-quences which have never been observed in thetraining data.
The set observed consists only oftest sequences which have been observed at leastonce in the training data.
Again we have calcu-lated the perplexity of each set.
For reference, alsothe values of the complete test data set are shownin Table 5.model lengthExperiments n = 3 n = 4 n = 5MKNcomplete586.9 404 307.3GLMcomplete571.6 378.1 275rel.
change 2.6% 6.5% 10.5%MKNunseen14696.8 2199.8 846.1GLMunseen13058.7 1902.4 714.4rel.
change 11.2% 13.5% 15.6%MKNobserved220.2 88.0 43.4GLMobserved220.6 88.3 43.5rel.
change ?0.16% ?0.28% ?0.15%Table 5: Absolute perplexity values and relativereduction of perplexity from MKN to GLM for thecomplete and split test file into observed and un-seen sequences for models of order 3 to 5.
Thedata set is the largest English Wikipedia corpus.As expected we see the overall perplexity valuesrise for the unseen test case and decline for the ob-served test case.
More interestingly we see that therelative reduction of perplexity of the GLM overMKN increases from 10.5% to 15.6% on the un-seen test case.
This indicates that the superior per-formance of the GLM on small training corporaand for higher order models indeed comes from itsgood performance properties with regard to sparsetraining data.
It also confirms that our motivationto produce lower order n-grams by omitting notonly the first word of the local context but system-atically all words has been fruitful.
However, wealso see that for the observed sequences the GLMperforms slightly worse than MKN.
For the ob-served cases we find the relative change to be neg-ligible.6 DiscussionIn our experiments we have observed an im-provement of our generalized language modelsover classical language models using Kneser-Neysmoothing.
The improvements have been ob-served for different languages, different domainsas well as different sizes of the training data.
Inthe experiments we have also seen that the GLMperforms well in particular for small training datasets and sparse data, encouraging our initial mo-tivation.
This feature of the GLM is of partic-ular value, as data sparsity becomes a more andmore immanent problem for higher values of n.This known fact is underlined also by the statis-11510%5%10%15%20%25%30%0.1 1 10 100 1000relativechangeinperplexitydata set size [mio words]Relative change of perplexity for GLM over MKNMKN (baseline) for n=3,4, and 5n=5n=4n=3Figure 2: Variation of the size of the training data on 100k test sequences on the English Wikipedia dataset with different model lengths for GLM.tics shown in Table 6.
The fraction of total n-grams which appear only once in our Wikipediacorpus increases for higher values of n. However,for the same value of n the skip n-grams are lessrare.
Our generalized language models leveragethis additional information to obtain more reliableestimates for the probability of word sequences.wn1total uniquew10.5% 64.0%w1w25.1% 68.2%w1w38.0% 79.9%w1w49.6% 72.1%w1w510.1% 72.7%w1w2w321.1% 77.5%w1w3w428.2% 80.4%w1w2w428.2% 80.7%w1w4w531.7% 81.9%w1w3w535.3% 83.0%w1w2w531.5% 82.2%w1w2w3w444.7% 85.4%w1w3w4w552.7% 87.6%w1w2w4w552.6% 88.0%w1w2w3w552.3% 87.7%w1w2w3w4w564.4% 90.7%Table 6: Percentage of generalized n-grams whichoccur only once in the English Wikipedia cor-pus.
Total means a percentage relative to the totalamount of sequences.
Unique means a percentagerelative to the amount of unique sequences of thispattern in the data set.Beyond the general improvements there is anadditional path for benefitting from generalizedlanguage models.
As it is possible to better lever-age the information in smaller and sparse data sets,we can build smaller models of competitive per-formance.
For instance, when looking at Table 4we observe the 3-gram MKN approach on the fulltraining data set to achieve a perplexity of 586.9.This model has been trained on 7 GB of text andthe resulting model has a size of 15 GB and 742Mio.
entries for the count and continuation countvalues.
Looking for a GLM with comparable butbetter performance we see that the 5-gram modeltrained on 1% of the training data has a perplexityof 528.7.
This GLM model has a size of 9.5 GBand contains only 427 Mio.
entries.
So, using a farsmaller set of training data we can build a smallermodel which still demonstrates a competitive per-formance.7 Conclusion and Future Work7.1 ConclusionWe have introduced a novel generalized languagemodel as the systematic combination of skip n-grams and modified Kneser-Ney smoothing.
Themain strength of our approach is the combinationof a simple and elegant idea with an an empiri-cally convincing result.
Mathematically one cansee that the GLM includes the standard languagemodel with modified Kneser-Ney smoothing as asub model and is consequently a real generaliza-tion.In an empirical evaluation, we have demon-strated that for higher orders the GLM outper-forms MKN for all test cases.
The relative im-provement in perplexity is up to 12.7% for largedata sets.
GLMs also performs particularly wellon small and sparse sets of training data.
On a very1152small training data set we observed a reduction ofperplexity by 25.7%.
Our experiments underlinethat the generalized language models overcome inparticular the weaknesses of modified Kneser-Neysmoothing on sparse training data.7.2 Future workA desirable extension of our current definition ofGLMs will be the combination of different lowerlower order models in our generalized languagemodel using different weights for each model.Such weights can be used to model the statisticalreliability of the different lower order models.
Thevalue of the weights would have to be chosen ac-cording to the probability or counts of the respec-tive skip n-grams.Another important step that has not been con-sidered yet is compressing and indexing of gen-eralized language models to improve the perfor-mance of the computation and be able to storethem in main memory.
Regarding the scalabilityof the approach to very large data sets we intend toapply the Map Reduce techniques from (Heafieldet al, 2013) to our generalized language models inorder to have a more scalable calculation.This will open the path also to another interest-ing experiment.
Goodman (Goodman, 2001) ob-served that increasing the length of n-grams incombination with modified Kneser-Ney smooth-ing did not lead to improvements for values ofn beyond 7.
We believe that our generalizedlanguage models could still benefit from such anincrease.
They suffer less from the sparsity oflong n-grams and can overcome this sparsity wheninterpolating with the lower order skip n-gramswhile benefiting from the larger context.Finally, it would be interesting to see how ap-plications of language models?like next wordprediction, machine translation, speech recogni-tion, text classification, spelling correction, e.g.
?benefit from the better performance of generalizedlanguage models.AcknowledgementsWe would like to thank Heinrich Hartmann fora fruitful discussion regarding notation of theskip operator for n-grams.
The research lead-ing to these results has received funding from theEuropean Community?s Seventh Framework Pro-gramme (FP7/2007-2013), REVEAL (Grant agreenumber 610928).ReferencesSteffen Bickel, Peter Haider, and Tobias Scheffer.2005.
Predicting sentences using n-gram languagemodels.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methodsin Natural Language Processing, HLT ?05, pages193?200, Stroudsburg, PA, USA.
Association forComputational Linguistics.Peter F Brown, John Cocke, Stephen A Della Pietra,Vincent J Della Pietra, Fredrick Jelinek, John D Laf-ferty, Robert L Mercer, and Paul S Roossin.
1990.A statistical approach to machine translation.
Com-putational linguistics, 16(2):79?85.Peter F. Brown, Peter V. deSouza, Robert L. Mer-cer, Vincent J. Della Pietra, and Jenifer C. Lai.1992.
Class-based n-gram models of natural lan-guage.
Comput.
Linguist., 18(4):467?479, Decem-ber.Stanley Chen and Joshua Goodman.
1998.
An em-pirical study of smoothing techniques for languagemodeling.
Technical report, TR-10-98, HarvardUniversity, August.Stanley Chen and Joshua Goodman.
1999.
Anempirical study of smoothing techniques for lan-guage modeling.
Computer Speech & Language,13(4):359?393.Jianfeng Gao and Hisami Suzuki.
2005.
Long dis-tance dependency in language modeling: An em-pirical study.
In Keh-Yih Su, Junichi Tsujii, Jong-Hyeok Lee, and OiYee Kwong, editors, NaturalLanguage Processing IJCNLP 2004, volume 3248of Lecture Notes in Computer Science, pages 396?405.
Springer Berlin Heidelberg.Irwin J.
Good.
1953.
The population frequencies ofspecies and the estimation of population parameters.Biometrika, 40(3-4):237?264.Joshua T. Goodman.
2000.
Putting it all together:language model combination.
In Acoustics, Speech,and Signal Processing, 2000.
ICASSP ?00.
Proceed-ings.
2000 IEEE International Conference on, vol-ume 3, pages 1647?1650 vol.3.Joshua T. Goodman.
2001.
A bit of progress in lan-guage modeling ?
extended version.
Technical Re-port MSR-TR-2001-72, Microsoft Research.David Guthrie, Ben Allison, Wei Liu, Louise Guthrie,and York Wilks.
2006.
A closer look at skip-gram modelling.
In Proceedings LREC?2006, pages1222?1225.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modifiedkneser-ney language model estimation.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics.1153Xuedong Huang, Fileno Alleva, Hsiao-Wuen Hon,Mei-Yuh Hwang, Kai-Fu Lee, and Ronald Rosen-feld.
1993.
The sphinx-ii speech recognition sys-tem: an overview.
Computer Speech & Language,7(2):137 ?
148.F.
Jelinek and R.L.
Mercer.
1980.
Interpolated estima-tion of markov source parameters from sparse data.In Proceedings of the Workshop on Pattern Recogni-tion in Practice, pages 381?397.S.
Katz.
1987.
Estimation of probabilities from sparsedata for the language model component of a speechrecognizer.
Acoustics, Speech and Signal Process-ing, IEEE Transactions on, 35(3):400?401.Reinhard Kneser and Hermann Ney.
1995.
Im-proved backing-off for m-gram language modeling.In Acoustics, Speech, and Signal Processing, 1995.ICASSP-95., 1995 International Conference on, vol-ume 1, pages 181?184.
IEEE.Christopher D. Manning and Hinrich Sch?utze.
1999.Foundations of statistical natural language process-ing.
MIT Press, Cambridge, MA, USA.Eric Mays, Fred J Damerau, and Robert L Mercer.1991.
Context based spelling correction.
Informa-tion Processing & Management, 27(5):517?522.Hermann Ney, Ute Essen, and Reinhard Kneser.
1994.On structuring probabilistic dependences in stochas-tic language modelling.
Computer Speech & Lan-guage, 8(1):1 ?
38.Lawrence Rabiner and Biing-Hwang Juang.
1993.Fundamentals of Speech Recognition.
Prentice Hall.Ernst-G?unter Schukat-Talamazzini, R Hendrych, RalfKompe, and Heinrich Niemann.
1995.
Permugramlanguage models.
In Fourth European Conferenceon Speech Communication and Technology.Ralf Steinberger, Bruno Pouliquen, Anna Widiger,Camelia Ignat, Tomaz Erjavec, Dan Tufis, andDaniel Varga.
2006.
The jrc-acquis: A multi-lingual aligned parallel corpus with 20+ languages.In LREC?06: Proceedings of the 5th InternationalConference on Language Resources and Evaluation.A Discount Values and Weights inModified Kneser NeyThe discount valueD(c) used in formula (2) is de-fined as (Chen and Goodman, 1999):D(c) =??????
?0 if c = 0D1if c = 1D2if c = 2D3+if c > 2(10)The discounting values D1, D2, and D3+are de-fined as (Chen and Goodman, 1998)D1= 1 ?
2Yn2n1(11a)D2= 2 ?
3Yn3n2(11b)D3+= 3 ?
4Yn4n3(11c)with Y =n1n1+n2and niis the total number of n-grams which appear exactly i times in the trainingdata.
The weight ?high(wi?1i?n+1) is defined as:?high(wi?1i?n+1) = (12)D1N1(wi?1i?n+1?)+D2N2(wi?1i?n+1?)+D3+N3+(wi?1i?n+1?
)c(wi?1i?n+1)And the weight ?mid(wi?1i?n+1) is defined as:?mid(wi?1i?n+1) = (13)D1N1(wi?1i?n+1?)+D2N2(wi?1i?n+1?)+D3+N3+(wi?1i?n+1?)N1+(?wi?1i?n+1?
)where N1(wi?1i?n+1?
), N2(wi?1i?n+1?
), andN3+(wi?1i?n+1?)
are analogously defined toN1+(wi?1i?n+1?
).1154
