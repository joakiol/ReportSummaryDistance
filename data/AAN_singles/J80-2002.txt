A Parsing Algorithm That Extends PhrasesDanie l  ChesterDepar tment  of  Computer  Sc iencesThe  Un ivers i ty  of Texas  at  Aust inAust in ,  Texas  78712It is desirable for a parser to be able to extend a phrase even after it has beencombined into a larger syntactic unit.
This paper presents an algorithm that does thisin two ways, one dealing with "right extension" and the other with "left recursion".A brief comparison with other parsing algorithms shows it to be related to the left-corner parsing algorithm, but it is more flexible in the order that it permits phrases tobe combined.
It has many of the properties of the sentence analyzers of Marcus andRiesbeck, but is independent of the language theories on which those programs arebased.1.
IntroductionTo analyze a sentence of a natural language, acomputer program recognizes the phrases within thesentence, builds data structures, such as conceptualrepresentations, for each of them and combinesthose structures into one that corresponds to theentire sentence.
The algorithm which recognizes thephrases and invokes the structure-building proce-dures is the parsing algorithm implemented by theprogram.
The parsing algorithm is combined with aset of procedures for deciding between alternativeactions and for building the datastructures.
Since itis organized around phrases, it is primarily con-cerned with syntax, while the procedures it callsdeal with non-syntactic parts of the analysis.
Whenthe program is run, there may be a complex inter-play between the code segments that handle syntaxand those that handle semantics and pragmatics, butthe program organization can still be abstracted intoa (syntactic) parsing algorithm and a set of proce-dures that are called to augment hat algorithm.By taking the view that the parsing algorithmrecognizes the phrases in a sentence, that is, thecomponents of its surface structure and how theycan be decomposed, it suffices to specify the syntaxof a natural language, at least approximately, with acontext-free phrase structure grammar, the rules ofwhich serve as phrase decomposit ion rules.
Al-though linguists have developed more elaborategrammars for this purpose, most computer programsfor sentence analysis, e.g., Heidorn (1972), Wino-grad (1972) and Woods (1970), specify the syntaxwith such a grammar, or something equivalent, andthen augment that grammar with procedures anddata structures to handle the non-context-free com-ponents of the language.
The notion of parsingalgorithm is therefore restricted in this paper to analgorithm that recognizes phrases in accordance witha context-free phrase structure grammar.Since the parsing algorithm of a sentence analysisprogram determines when data structures get com-bined, it seems reasonable to expect that the actionsof the parser should reflect the actions on the datastructures.
In particular, the combination of phrasesinto larger phrases can be expected to coincide withthe combination of corresponding data structuresinto larger data structures.
This happens naturallywhen the computer program is such that it calls theprocedures for combining data structures at thesame time the parsing algorithm indicates that thecorresponding phrases should be combined.1.1 Other Parsing AlgorithmsAccording to one classification of parsing algor-ithms (Aho and Ullman 1972), most analysis pro-grams are based on algorithms that are either "top-down",  "bot tom-up"  or " left -corner" ,  though ac-cording to a recent study by Grishman (1975), thetop-down and bottom-up approaches are dominant.The principle of top-down parsing is that the rules ofthe controlling grammar are used to generate a sen-tence that matches the one being analyzed.
A seri-Copyright 1980 by the Association for Computational Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial dvantage and the Journal reference and this copyright notice are includedon the first page.
To copy otherwise, or to republish, requires a fee and/or specific permission.0362-613X/80/020087-10501.00American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980 87Daniel Chester A Parsing Algorithm That Extends Phrasesous problem with this approach, if computed phrasesare supposed to correspond to natural phrases in asentence, is that the parser cannot handle left-branching phrases.
But such phrases occur in Eng-lish, Japanese, Turkish and other natural anguages(Chomsky 1965, Kimball 1973, Lyons 1970).The principle of bottom-up arsing is that a se-quence of phrases whose types match the right-handside of a grammar ule is reduced to a phrase of thetype on the left-hand side of the rule.
None of thematching is done until all the phrases are present;this can be ensured by matching the phrase types inthe right-to-left order shown in the grammar ule.The difficulty with this approach is that the analysisof the first part of a sentence has no influence onthe analysis of latter parts until the results of theanalyses are finally combined.
Efforts to overcomethis difficulty lead naturally to the third approach,left-corner parsing.Left-corner parsing, like bottom-up arsing, re-duces phrases whose types match the right-hand sideof a grammar ule to a phrase of the type on the-~ left-hand side of the rule; the difference is that the'< ~,types listed in the right-hand side of the rule are/j0"x~matched from left to right for left-corner parsing~.
~ instead of from right to left.
This technique gets its~ name from the fact that the first phrase found cor-~ responds to the left-most symbol of the right-handb'~side of the grammar ule, and this symbol has beencalled the left corner of the rule.
(When a grammarrule is drawn graphically with its left-hand side asthe parent node and the symbols of the right-handside as the daughters, forming a triangle, the left-most symbol is the left corner of the triangle.)
Oncethe left-corner phrase has been found, the grammarrule can be used to predict what kind of phrase willcome next.
This is how the analysis of the first partof a sentence can influence the analysis of laterparts.Most programs based on augmented transitionnetworks employ a top-down parser to which regis-ters and structure building routines have been add-ed, e.g., Kaplan (1972) and Wanner and Maratsos(1978).
It is important o note, however, that theconcept of augmented transition networks is a par-ticular way to represent linguistic knowledge; it doesnot require that the program using the networksoperate in top-down fashion.
In an early paper byWoods (1970), alternative algorithms that can beused with augmented transition networks are dis-cussed, including the bottom-up and Earley algor-ithms.The procedure-based programs of Winograd(1972) and Novak (1976) are basically top-downparsers, too.
The NLP program of Heidorn (1972)employs an augmented phrase structure grammar tocombine phrases in a basically bottom-up fashion.Likewise, PARRY, the program written by Colby(Parkison, Colby and Faught 1977), uses a kind ofbottom-up method to drive a computer model of aparanoid.1.2 A New Parsing AlgorithmThis paper presents a parsing algorithm that al-lows data structures to be combined as soon as pos-sible.
The algorithm permits a structure A to be ~.~combined with a structure B to form a structure C,and then to enlarge B to form a new structure B,  ~_ .~This new structure is to be formed in such a wayA~L~A~ ~that C is now composed of A and B' instead of A l~XO~yand B.
The algorithm permits these actions on datastructures because it permits similar actions onphrases, namely, phrases are combined with otherphrases and afterward are extended to encompassmore words in the sentence being analyzed.
Thisbehavior of combining phrases before all of theircomponents have been found is called closure byKimball (1973).
It is desirable because it permitsthe corresponding data structures to be combinedand to influence the construction of other datastructures sooner than they otherwise could.In the next section of this paper the desired be-havior for combining phrases is discussed in moredetail to show the two kinds of actions that are re-quired.
Then the algorithm itself is explained andits operation illustrated by examples, with somedetails of an experimental implementation beinggiven, also.
Finally, this algorithm is compared tothose used in the sentence analysis programs ofMarcus and Riesbeck, and some concluding remarksare made.2.
Phrase ExtensionA parser that extends phrases combines phrasesbefore it has found all of their components.
Whenparsing the sentence(1) This is the cat that caught he ratthat stole the cheese.it combines the first"this is the cat", inphrase, then extendsfour words into the phrasewhich "the cat" is a nounthat noun phrase to "the catthat caught the rat", in which "the rat" is a nounphrase, and finally extends that noun phrase to "therat that stole the cheese."
This is apparently howpeople parse that sentence, because, as Chomsky(1965) noted, it is natural to break up the sentenceinto the fragmentsthis is the catthat caught he ratthat stole the cheese88 American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980Daniel Chester A Parsing Algorithm That Extends Phrases(by adding intonation, pauses or commas) ratherthan at the boundaries of the major noun phrases:this isthe cat that caughtthe rat that stolethe cheeseLikewise, when the parser parses the sentence(2) The rat stole the cheese in the pantryby the bread.it forms the phrase "the rat stole the cheese", thenextends the noun phrase "the cheese" to "thecheese in the pantry"  and extends that phrase to"the cheese in the pantry by the bread".These two examples show the two kinds of ac-tions needed by a parser in order to exhibit the be-havior called closure.
Each of these actions willnow be described in more detail, using the followinggrammar, G 1:S -> NP VPNP -> ProNP -> NPINP -> NPI Re lP ro  VPVP -> V NPPro -> thisNPI -> Det NNPI -> NPI PPRe lP ro  -> thatV -> isV -> caughtV -> sto leDet -> theN -> catN -> ratN -> cheeseN -> pant ryN -> breadPP -> Prep  NPPrep  -> inP rep  -> by2.1 R ight  Extens ionThe first kind of action needed by the parser isto add more components to a phrase according to arule whose right-hand side extends the right-handside of another rule.
This is illustrated by sentence1.
With grammar G1, the phrase "this is the cat"has the phrase structuresNP VPi t \Pro V NPl I Ith is  is NPIt \Det Nl Ithe catIn order to extend the NP phrase "the cat",  thesubstructureNPINPII \Det NI Ithe catmust be changed to the substructureNP'NP VPi \ i ,Det N that  VI I Ithe cat caughtNPINPIi \Det Nl Ithe ratThe parser, in effect, must extend the NP phraseaccording to the rule NP- -> NP1 RelPro VP,whose right-hand side extends the right-hand side ofthe rule NP - -> NP1.There is a simple way to indicate in a grammarwhen a phrase can be extended in this way: whentwo rules have the same left-hand side, and theright-hand side of the first rule is an initial segmentof the right-hand side of the second rule, a specialmark can be placed after that initial segment in thesecond rule and then the first rule can be discarded.The special mark can be interpreted as saying thatthe rest of the right-hand side of the rule is option-al.
Using * as the special mark, the ruleNP - ->  NP1 * RelPro VP can replace the rulesNP - ->  NP1 and NP - ->  NP1 RelPro VP in thegrammar G1.American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980 89Daniel Chester A Parsing Algorithm That Extends PhrasesOur algorithm therefore requires a modifiedgrammar.
For the general case, a grammar is modi-fied by execution of the following step:1) For each rule of the form X - -> Y1 ... Yk,whenever there is another rule of the formX - -> Y1 ... Yk Yk+l  ... Yn, replace thisother rule by X - -> Y1 ... Yk * Yk+l  ...Yn, provided Yk+ 1 is not *When no more rules can be produced by the abovestep, the following step is executed:2) Delete each rule of the form X - -> Y1 ...Yk for which there is another rule of theform X - -> Y1 ... Yk Yk+l  ... Yn.The mark * tells the parser to combine the phrasesthat correspond to the symbols preceding * as if therule ended there.
After doing that, the parser triesto extend the combination by looking for phrases tocorrespond to the symbols following *2.2 Left RecursionThe second kind of action needed by the parseris to extend a phrase according to a left-recursiverule.
A left-recursive rule is a rule of the formX - ->  X Y1 ... Yn.
The action based on a left-recursive rule is i l lustrated by sentence 2 above.The phrase "the rat stole the cheese" has the phrasestructuresNP VPNPI V NPf \ i iDet N sto le  NPII I i \the rat  Det  NI Ithe cheeseIn order to extend the phrase "the cheese", the sub-structureNPIi \Det NI Ithe cheesemust be changed to the substructureNPI,NPI PPl \ i \Det N PrepI I Ithe cheese  inNPiNPII \DetItheNipant ryThe parser, in effect, must extend the NP1 phraseaccording to the left-recursive rule NP1 - ->  NP1PP.In the general case, after finding a phrase andcombining it with others, the parser must look formore phrases to combine with it according to someleft-recursive rule in the grammar.
Left-recursiverules, therefore, play a special role in this algorithm.It might be thought that left-recursive rules arethe only mechanism needed to extend phrases.
Thisis not true because phrases extended in this wayhave a different property from those extended byright extension.
In particular, left-recursive rulescan be applied several times to extend the samephrase, which is not always the desired behavior fora given language phenomenon.
For instance, anynumber  of PP phrases can follow an NP1 phrasewithout connecting words or punctuation because ofthe left-recursive rule NP1 - -> NP1 PP.
If, on theother hand, NP - ->  NP RelPro VP were used in-stead of NP - -> NP1 RelPro VP, any number ofrelative clauses (RelPro and VP phrase pairs) couldfollow an NP phrase, which is generally ungrammat-ical in English unless the clauses are separated bycommas or connective words like "and" and "or".3.
Detai ls of the Parsing Algor i thmThe parsing algorithm that provides for bothright extension and left recursion will now be de-scribed.
It works with structures called phrase sta-tus elements to keep track of the status of phrasesduring the sentence analysis.
In this section, afterthese elements are defined, the algorithm is present-ed as a set of operations to be performed on a listof phrase status elements.
Then the algorithm isapplied to sentences 1 and 2 to show how it works,and some refinements that were added in an experi-mental implementation are discussed.90 American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980Daniel Chester A Parsing Algorithm That Extends Phrases3.1 Phrase Status E lementsIn order to combine and extend phrases properly,a parser  must keep track of the status of eachphrase; in particular, it must note what kind ofphrase will be formed, what component phrases arestill needed to complete the phrase, and whether thephrase is a new one or an extension.
This informa-tion can be represented by a phrase status element.A phrase status element is a triple of the form(Yk ... Yn,X,F) where, for some symbols Y1, ...,Yk-1, there is a grammar ule of the form X - -> Y1... Yk-1 Yk ... Yn, and F is a flag that has one ofthe values n, e, or p, which stand for "new","extendible" and "progressing", respectively.
Intui-tively, this triple means that the phrase in questionis an X phrase and that it will be completed whenphrases of types Yk through Yn are found.
If F =n, the phrase will be a new phrase.
If F -- e, thephrase is ready for extension into a longer X phrase,but none of the extending phrase components havebeen found yet.
If F = p, however, some of theextending phrase components have been found al-ready and the rest must be found to complete thephrase.
The status of being ready for extension hasto be distinguished from the status of having theextension in progress, because it is during the readystatus that the decision whether to try to extend thephrase is made.3.2 The AlgorithmThe parsing algorithm for extending phrases isembodied in a set of operations for manipulating alist of phrase status elements, called the element list,according to a given modified grammar and a givensentence to be analyzed.
Beginning with an emptyelement list, the procedure is applied by performingthe operations repeatedly until the list contains onlythe element (,S,n), where S is the grammar's tartsymbol, and all of the words in the given sentencehave been processed.The following are the six operations which areapplied to the element list:1.
Replace an element of the form (* Y1 ...Yn,X,F) with the pair (Y1 ... Yn,X,e) (,X,F).2.
Replace an element of the form (,X,p) withthe element (Y1 ... Yn,X,e) if there is a left-recursive rule of the form X - -> X Y1 ... Ynin the grammar; if there is no such rule, de-lete the element.3.
Replace adjacent phrase status elements of theform (,X,n) (X Y1 ... Yn,Z,F) with the pair(,X,p) (Y1 ... Yn,Z,F').
If the flag F = e,then F'  = p; otherwise, F'  -- F.4.
Replace an element of the form (,X,n) withthe pair (,X,p) (Y1 ... Yn,Z,n) if there is agrammar ule of the form Z - -> X Y1 ... Ynin the grammar, provided the rule is not left-recursive.5.
Get the next word W from the sentence andadd the element (,W,n) to the front (left) endof the element list.6.
Delete an element of the form (Y1 ... Yn,X,e).These operations are applied one at a time, in arbi-trary order.
If more than one operation can cause achange in the element list at any given time, thenone of them is selected for actual application.
Themanner of selection is not specified here becausethat is a function of the data structures and proce-dures that would have to be added to incorporatethe algorithm into a complete sentence analysis pro-gram.These operations have a fairly simple purpose:the major goal is to find new phrases, which arerepresented by phrase status elements of the form(,X,n).
Initially, by application of operation 5, aword of the sentence to be analyzed is made into anew phrase.
When a new phrase is found, whetherby operation 5 or some other operation, there aretwo ways that can be used to find a larger phrase.One way is to attempt to find a new phrase thatbegins with the just-found phrase; this is the pur-pose of operation 4.
Once an X phrase is found,the element (,X,n) is replaced by (,X,p) (Y1 ...Yn,Z,n) for some grammar rule of the formZ - -> X Y1 ... Yn and some Z different from X.The element (Y1 ... Yn,Z,n) represents an attemptto find a Z phrase, of which the first component,the X phrase, has been found.The second way that can be used to make a larg-er phrase is to make the X phrase a component of aphrase that has already been started.
In operation3, the element (,X,n) represents a new X phrasethat can be used in this way.
An immediately pre-ceding phrase has been made part of some unfin-ished Z phrase, represented by the element (X Y1 ...Yn,Z,F).
Since the symbol X is the first symbol inthe first part of this element, the Z phrase can con-tain an X phrase at this point in the sentence, so theX phrase is put in the Z phrase, and the result ofthis action is indicated by the new elements (,X,p)(Y1 ... Yn,Z,F') .In both operations 3 and 4, the element (,X,n)itself is replaced by (,X,p).
The flag value p indi-cates, in this case, that the X phrase has alreadybeen added to some larger phrase.
Operation 2 triesto extend the X phrase by creating the element(Y1 ... Yn,X,e) for some left-recursive grammarrule X - -> X Y1 ... Yn, indicating that the XAmerican Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980 91Daniel Chester A Parsing Algorithm That Extends Phrasesphrase can be extended if it is followed by phrasesof types Y1 .
.
.
.
.
Yn, in that order.
If a Y1 phrasecomes next in the sentence, the extension begins byan application of operation 5, which changes theflag from e to p to indicate that the extension is inprogress.
If there is no Y1 phrase, however, the Xphrase cannot be extended, so the element is deleted.by operation 6, allowing the parser to go on to thenext phrase.In the event a phrase status element has the form(* Y1 ... Yn,X,F), the X phrase can be consideredcompleted.
It can also be extended, however, if it isfollowed by phrases of types Y1 through Yn.
Oper-ation 1 creates the new element (,X,F) to indicatethe completion of the X phrase, and the new ele-ment (Y1 ... Yn,X,e) to indicate its possible exten-sion.
Again, if extension turns out not to be possi-ble, the element can be deleted by operation 6 sothat parsing can continue.3.3 ExamplesAs examples of how the algorithm works, consid-er how it applies to sentences 1 and 2.
GrammarG1 must be modified first, but this consists only ofsubstituting the rule NP --> NP1 * RelPro VP forthe two NP rules in the original grammar, as de-scribed earlier.
Starting with an empty element list,the sentence "this is the cat that caught he rat thatstole the cheese" is processed as shown in Table 1.By operation 5, the word "this" is removed from thesentence and the element (,this,n) is added to thelist.
By operation 4, this element is replaced by(,this,p) (,Pro,n), which is shortened to (,Pro,n) byoperation 2.
By two applications of operations 4and 2, the element list becomes (,NP,n), then(VP,S,n).
The element (,is,n) is now added to thefront of the list.
This element is changed, by opera-tions 4 and 2 again, to (,V,n) and then to(NP,VP,n).
The words "the" and "cat" are proc-essed similarly to produce the element list (,N,n)(N,NPI,n) (NP,VP,n) (VP,S,n) at step 20.At this point, operation 3 is applied to combine(,N,n) with (N,NPI,n), yielding (,N,p) (,NPI,n) intheir place.
By operation 2, element (,N,p) is re-moved.
The first element of the list is now(,NPI,n), which by operation 4 is changed to(,NPI,p) (* RelPro VP,NP,n).
When operation 2 isapplied this time, because there is a left-recursivegrammar ule for NP1 phrases, element (,NPI,p) isreplaced by (PP,NPI,e).
Operation 1 is applied tothe next element o eliminate the special mark * thatappears in it, changing the element list to(PP,NPI,e) (RelPro VP,NP,e) (,NP,n) (NP,VP,n)(VP,S,n) at step 25.At this point, the element (,NP,n) represents theNP phrase "the cat".
Operation 3 is applied to itand the next element, and then operation 2 to theresult, reducing those two elements to (,VP,n).
Byoperations 3 and 2 again, this element is combinedwith (VP,S,n) to produce (,S,n), which at this pointrepresents the phrase "this is the cat."
If thisphrase were the whole sentence, operation 6 couldbe applied, reducing the element list to (,S,n) andthe sentence would be successfully parsed.
Thereare more words in the sentence, however, so otheroperations are tried.The next word, "that", is processed by opera-tions 5, 4 and 2 to add (,RelPro,n) to the front ofthe list.
Since the grammar does not allow a PPphrase to begin with the word "that", operation 6 isapplied to eliminate the element (PP,NPI,e), whichrepresents the possibility of an extension of an NP1phrase by a PP phrase.
The next element, (RelProVP,NP,e), represents the possibility of an extensionof an NP phrase when it is followed by a RelProphrase, however, so operations 3 and 2 are applied,changing the element list to (VP,NP,p) (,S,n).
Notethat the flag value e has changed to p; this meansthat a VP phrase must  be found now to completethe NP phrase extension or this sequence of opera-tions will fail.By.
continuing in this fashion, the sentence isparsed.
Since no new details of how the algorithmworks would be illustrated by continuing the narra-tion, the continuation is omitted.The sentence "the rat stole the cheese in thepantry by the bread" is parsed in a similar fashion.The only detail that is significantly different fromthe previous sentence is that after the element(RelPro VP,NP,e) is deleted by operation 6, insteadof (PP,NPI,e), a new situation occurs, in which aphrase can attach to one of several phrases waitingto be extended.
The situation occurs after the sen-tence corresponding to the phrase "the rat stole thecheese" is represented by the element (,S,n) when itfirst appears on the element list.
When the PPphrase "in the pantry" is found, the element(PP,NPI,e) changes to (,NPI,p), indicating that theNP1 phrase "the cheese" has been extended to "thecheese in the pantry".
By operation 2, the element(,NPI,p) is changed to (PP,NPI,e) so that the NP1phrase can be extended again.
But "the pantry" isan NP1 phrase also, which means that an element ofthe form (PP,NPI,e) has been created to extend itas well.
Thus, when the next PP phrase, "by thebread" is found, it can attach to either of the earlierNP1 phrases.
The parser does not decide whichattachment to make, as that depends on non-syntaxrelated properties of the data structures that wouldbe associated with the phrases in a complete sen-tence analyzer.
In this example the PP phrase canbe attached to the NP1 phrase "the cheese", which92 American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980Daniel Chester A Parsing Algorithm That Extends PhrasesSTEPI23456789I01112131415161718192O212223242526272829303132333435ELEMENT LIST OPERATION(,the,n)(,the,p) (,Det,n)(,Det,n)(,Det,p) (N,NPI,n)(N,NPI,n)(,cat,n) (N,NPI.,n)(,cat,p) (,N,n) (N,NPI,n)(,N,n) (N,NPI,n)(,N,p) (,NPl,n)(,NPl,n)(,NPl,p) (* RelPro VP,NP,n)(PP,NPI,e) (* RelPro VP,NP,n)(PP,NPI,e) (RelPro VP,NP,e) (,NP,n) (NP,VP,n)(PP,NPl,e) (RelPro VP,NP,e) (,NP,p) (,VP,n)(,this,n) 5(,this,p) (,Pro,n) 4(,Pro,n) 2(,Pro,p) (,NP,n) 4(,NP,n) 2(,NP,p) (VP, S,n) 4(VP,S,n) 2(,is,n) (VP,S,n) 5(,is,p) (,V,n) (VP,S,n) 4(,V,n) (VP, S,n) 2(,V,p) (NP,VP,n) (VP,S,n) 4(NP,VP, n) (VP, S,n) 2(NP,VP,n) (VP,S,n) 4(NP,VP,n) (VP,S,n) 2(NP,VP,n) (VP,S,n) 4(NP,VP,n) (VP,S,n) 4(NP,VP,n) (VP,S,n) 2(NP,VP,n) (VP,S,n) 5(NP,VP,n) (VP,S,n) 4(NP,VP,n) (VP,S,n) 2(NP,VP,n) (VP,S,n) 3(NP,VP,n) (VP,S,n) 2(NP,VP,n) (VP,S,n) 4(NP,VP,n) (VP,S,n) 2(VP,S,n) I(VP,S,n) 3(PP,NPI,e) (RelPro VP,NP,e) (,VP,n) (VP,S,n)(PP,NPI,e) (RelPro VP,NP,e) (,VP,p) (,S,n)(PP,NPl,e) (RelPro VP,NP,e) (,S,n)(,that,n) (PP,NPI,e) (RelPro VP,NP,e) (,S,n)(,that,p) (,RelPro,n) (PP,NPI,.e) (RelPro VP,NP,e) (,S,n)(,RelPro,n) (PP,NPI,e) (RelPro VP,NP,e) (,S,n)(,RelPro,n) (RelPro VP,NP,e) (,S,n)(,RelPro,p) (VP,NP,p) (,S,n)(VP, NP,p)(,S,n)etc.232542632Table 1.
Trace of parsing algorithm on sentence 1.means that the intervening element (PP,NPI ,e)  at-tempting to expand the NP1 phrase "the pantry"has to be deleted.3.4 Const ra ints  on the Operat ionsThe algorithms for top-down, bottom-up andleft-corner parsing are usually presented so that alloperations are performed on the top of a stack thatcorresponds to our element list.
We have not con-strained our algorithm in this way because to do sowould prevent the desired closure behavior.
In par-ticular, in the sentence "this is the cat that caughtthe rat that stole the cheese," the NP phrase "thecat" would not combine into the phrase "this is thecat" until the rest of the sentence was analyzed ifsuch a constraint were enforced.
This is becauseoperation 1 would create an element for extendingthe NP phrase that would have to be disposed offirst before the element (,NP,n), created also bythat operation, could combine with anything else.Thus, constraining the operations to apply only tothe front end of the element list would nullify thealgorithm's purpose.American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980 93 "Daniel Chester A Parsing Algorithm That Extends PhrasesOur algorithm can be viewed as a modification ofthe left-corner parser.
In fact, if a grammar is notmodified before use with our algorithm, and if oper-ation 4 is not restricted to non-left-recursive rules,and if operation 2 is modified to delete all elementsof the form (,X,p), then our algorithm would actual-ly be a left-corner parser.3.5 Experimental ProgramThe algorithm has been presented here in a sim-ple form to make its exposition easier and its oper-ating principles clearer.
When it was tried out in anexperimental program, several techniques were usedto make it work more efficiently.
For example,operations 1 and 2 were combined with the otheroperations so that they were, in effect, applied assoon as possible.
Operation 3 was also applied assoon as possible.
The other operations cannot begiven a definite order for their application; thatmust be determined by the non-syntactic proceduresthat are added to the parser.Another technique increased efficiency by apply-ing operation 4 only when the result is consistentwith the grammar.
Suppose grammar G1 containedthe rule Det - -> that as well as the rule RelPro -->that.
When the word "that" in the sentence "this isthe cat that caught the rat that stole the cheese" isprocessed, the element list contains the triple(,that,n) (PP,NPI ,e)  (RelPro VP,NP,e) at onepoint.
The grammar permits operation 4 to be ap-plied to (,that,n) to produce either (,Det,n) or(,RelPro,n), but only the latter can lead to a suc-cessful parse because the grammar does not allowk0~\ ]or  either a PP phrase or a RelPro phrase to beginV~,~'x  with a Det phrase.
The technique for guiding oper-.x' f f~\ation 4 so that it produces only useful elements con-~<~\s i s ts  of computing beforehand all the phrase types\<h~ ~that can be-gin each phrase.
Then-the- fo l lowing"~,,k v "operation is used ~n p~ace ~ operation 4:4'a.
If an element of the form (,X,n) is at the(right) end of the list, replace it with thepair (,X,p) (Y1 ... Yn,Z,n) when there is agrammar ule of the form Z --> X Y1 ... Ynin the grammar, provided the rule is notleft-recursive.4'b.
If an element of the form (,X,n) is followedby an element of the form (U1 ... Um,V,F),replace (,X,n) with the pair (,X,p) (Y1 ...Yn,Z,n) when there is a grammar ule of theform Z - -> X Y1 ... Yn in the grammar,provided the rule is not left-recursive, and aZ phrase can begin a U1 phrase or Z = U1.It is sufficient to consider only the first elementafter an element of the form (,X,n) because if oper-ation 4'b cannot be applied, either that first elementcan be deleted by operation 6 or the parse is goingto fail anyway.
Thus, in our example above, opera-tion 6 can be used to delete the element (PP,NPI ,e)so that operation 4'b can be applied to (,that,n)(RelPro VP,NP,e).
This technique is essentially thesame as the selective filter techniqueo described byGriffiths and Petrick (1965) for left-corner parsing~ g o r i t h m ,  in their terminology).Another technique increased eff iciency furtherby postponing the decision about which of severalgrammar ules to apply via operations 3 or 4' for aslong as possible.
The grammar ules were stored inLisp list structures so that rules having the sameleft-hand side and a common initial segment in theirright-hand side shared a common list structure, forexample, if the grammar consists of the rulesX->YZUX->YZVW->YZUthese rules are stored as the list structure( (x (z (u)(v ) )  )(w (z (u ) ) )  )which is stored on the property list for Y.
The com-mon initial segment shared by the first two rules isrepresented by the same path to the atom Z in thelist structure.
The component (X (Z (U) (V))) inthis list structure means that a Y phrase can be fol-lowed by a Z phrase and then either a U phrase or aV phrase to make an X phrase.
When a Y phrase isfound, and it is decided to try to find an X phrase,this component makes it possible to look for a Zphrase, but it postpones the decision as to whetherthe Z phrase should be followed by a U phrase or aV phrase until after the Z phrase has been found.The left-hand sides (X and W) of the rules arelisted first to facilitate operation 4'b.
This techni-que is similar to a technique used by Irons (1961)and described by Griffiths and Petrick (1965).4.
Related WorkThere are two sentence analysis programs withparsing algorithms that resemble ours in many ways,though theirs have been described in terms that areintimately tied to the particular semantic and syn-tactic representations u ed by those programs.
Theprograms are PARSIFAL,  by Marcus (1976,1978a,b) and the analyser of Riesbeck (1975a,b).94 American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980Daniel Chester A Parsing Algorithm That Extends Phrases4.1 PARSIFAL (Marcus)The basic structural unit of PARS IFAL  is thenode, which corresponds approximately to ourphrase status element.
Nodes are kept in two datastructures, a pushdown stack called the active nodestack, and a three-place constituent buffer.
(Thebuffer actually has five places in it, but the proce-dures that use it work with only three places at atime.)
The grammar rules are encoded into rulepackets.
Since the organization of these packets hasto do with the efficient selection of appropriategrammar ules and the invocation of procedures foradding structural details to nodes, the procedures wewant to ignore while looking at the parsing algor-ithm of PARSIFAL,  we will ignore the rule packetsin this comparison.
The essential fact about rulepackets is that they examine only the top node ofthe stack, the S or NP node nearest the top of thestack, and up to three nodes in the buffer.The basic operations that can be performed onthese structures include attaching the first node inthe buffer to the top node of the stack, which corre-sponds to operation 3 in our algorithm, creating anew node that holds one or more of the nodes in thebuffer, which corresponds to operation 4, and reac-tivating a node (pushing it onto the stack) that hasalready been attached to another node so that morenodes can be attached to it, which corresponds tothe phrase extending operations 1,2, and 6.
PARSI-FAL  has one operation that is not similar to theoperations of our algorithm, which is that it cancreate nodes for dummy NP phrases called traces.These nodes are intended to provide a way to ac-count for phenomena that would otherwise requiretransformational  grammar rules to explain them.Our algorithm does not allow such an operation; ifsuch an operation should prove to be necessary,however, it would not be hard to add, or its effectcould be produced by the procedures called.One of the benefits of having a buffer in PARSI-FAL is that the buffer allows for a kind of look-ahead based on phrases instead of just words.
Thusthe decision about what grammar ule to apply tothe first node in the buffer can be based on thephrases that follow a certain point in the sentenceunder analysis instead of just the words.
The systemcan look fu r ther  ahead this way and still keep astrict limit on the amount of look-ahead available.We can get a similar effect with our algorithm if werestrict the application of its operations to the firstfour or five phrase status elements in the elementlist.
In a sense, the first five elements of the listcorrespond to the buffer in PARSIFAL and the restof the list corresponds to the stack.
In fact, in arecent modification of PARS IFAL  (Shipman andMarcus 1979) the buffer and stack were combinedinto a single data structure closely resembling ourelement list.4.2 Riesbeck's AnalyzerThe basic structural unit of Riesbeck's analyzeris the Conceptual Dependency structure as developedby Schank (1973,1975).
A Conceptual Dependencystructure is intended to represent he meaning of aword, phrase or sentence.
The details of what aConceptual Dependency structure is will not be dis-cussed here.The monitor in Riesbeck's analyzer has no list orstack on which operations are performed; instead, ithas some global variables that serve the same pur-pose.
Only a few of these variables concern ushere.
The variable WORD holds the current wordbeing looked at and can be thought of as the frontelement of our element list.
The variable SENSEholds the sense of WORD or of the noun phrase ofwhich WORD is the head.
It is like the second ele-ment in our list.
The equivalent o the third ele-ment in our list is the variable REQUESTS,  whichholds a list of pattern-action rules.
There are someother variables (such as ART- INT)  that on occasionserve as the fourth element of the list.Unlike the controllers in many other analysisprograms, Riesbeck's monitor is not driven explicitlyby a grammar.
Instead, the syntactic information ituses is buried in the pattern-action rules attached toeach word and word sense within his program's lexi-con.
Take, for example, the common sense of theverb "give": one pattern-action rule says that if theverb is followed by a noun phrase denoting a per-son, the sense of that phrase is put in the recipientcase slot of the verb.
Another pattern-action rulesays that if a following noun phrase denotes an ob-ject, the sense of the phrase is put in the object caseslot of the verb.
These pattern-action rules corre-spond to having grammar ules of the form VP - ->give, and VP - -> VP NP, where the pattern-actionrules describe two different ways that a VP phraseand an NP phrase can combine into a VP phrase.There is a third pattern-action rule that changes thecurrent sense of the word "to" in case it is encoun-tered later in the sentence, but that is one of theactions that occurs below the syntactic level.Noun phrases are treated by Riesbeck's monitorin a special way.
Unmodif ied nouns are consideredto be noun phrases directly, but phrases beginningwith an article or adjective are handled by a specialsubroutine that collects the following adjectives andnouns before building the corresponding ConceptualDependency structure.
Once the whole noun phraseis found, the monitor examines the REQUESTS listto see if there are any pattern-action rules that canuse the noun phrase.
If so, the associated action istaken and the rule is marked so that it will not beused twice.
The monitor is started with a pattern-action rule on the REQUESTS list that puts a be-ginning noun phrase in the subject case slot ofwhatever verb that follows.
(There are provisionsAmerican Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980 95Daniel Chester A Parsing Algorithm That Extends Phrasesto reassign structures to different slots if later wordsof the sentence require it.
)It can be seen that Riesbeck's analysis programworks essentially by putting noun phrases in thecase slots of verbs as the phrases are encountered inthe sentence under analysis.
In a syntactic sense, itbuilds a phrase of type sentence (first noun phraseplus verb) and then extends that phrase as far aspossible, much as our algorithm does using left-recursive grammar ules.
Prepositions and connec-tives between simple sentences complicate the proc-ess somewhat, but the process is still similar to oursat the highest level of program control.5.
Concluding RemarksSince our parsing algorithm deals only with syn-tax, it is not complete in itself, but can be combinedwith a variety of conceptual representations to makea sentence analyzer.
Whenever an operation thatcombines phrases is performed, procedures to com-bine data structures can be called as well.
Whenthere is a choice of operations to be performed, pro-cedures to make the choice by examining the datastructures involved can be called, too.
Because ouralgorithm combines phrases sooner than others,there is greater opportunity for the data structuresto influence the progress of the parsing process.This makes the resulting sentence analyzer behavenot only in a more human-l ike way (the closureproperty),  but also in a more efficient way becauseit is less likely to have to back up.Although the programs of Marcus and Riesbeckshare many of these same properties, the syntacticprocessing aspects of those programs are not clearlyseparated from the particular conceptual representa-tions on which they are based.
We believe that theparsing algorithm presented here captures many ofthe important properties of those programs so thatthey may be applied to conceptual representationsbased on other theories of natural language.ReferencesAho, A. and UUman J., 1972.
The Theory of Parsing, Translationand Compiling, Vol.
1, Prentice-Hall Inc., New Jersey.Chomsky, N. 1965.
Aspects of the Theory of Syntax, MIT Press,Cambridge, Mass.Griffiths, T. and Petrick S., 1965.
On the relative efficiencies ofcontext-free grammar ecognizers.
CACM 8, May, 289-300.Grishman, R., 1975.
A Survey of Syntactic Analysis Proceduresfor Natural Language.
Courant Computer Science Report#8, Courant Institute of Mathematical Sciences, New YorkUniversity, August.
Also appears on AJCL Microfiche 47,1976.Heidorn, G., 1972.
Natural language inputs to a simulationprogramming system.
Report No.
NPS-55HD72101A, NavalPostgraduate School, Monterey, Calif., October.Irons, E., 1961.
A syntax directed compiler for ALGOL 60.CACM 4, January, 51-55.Kaplan, R., 1972.
Augmented transition etworks as psycholog-ical models of sentence comprehension.
Artificial Intelligence3, 77-100.Kimball, J., 1973.
Seven principles of surface structure parsingin natural anguage.
Cognition 2, 15-47.Lyons, J., 1970.
Chomsky.
Fontana/Collins, London.Marcus, M., 1976.
A design for a parser for English.
ACM '76Proceedings, Houston, Texas, Oct 20-22, 62-68.Marcus, M., 1978a.
Capturing linguistic generalizations in aparser for English.
Proceedings of the Second National Con-ference of the Canadian Society for Computational Studies ofIntelligence, University of Toronto, Toronto, Ontario, July19-21, 64-73.Marcus, M., 1978b.
A computational ccount of some const-raints on language.
Theoretical Issues in Natural LanguageProcessing-2, D. WaitS, general chairman, University of Illi-nois at Urbana-Champaign, July 25-27,236-246.Novak, G., 1976.
Computer understanding of physics problemsstated in natural anguage.
AJCL microfiche 53.Parkison, R., Colby, K. and Faught, W., 1977.
Conversationallanguage comprehension using integrated pattern-matchingand parsing.
Artificial Intelligence 9, October, 111-134.Riesbeck, C., 1975a.
Computational understanding.
TheoreticalIssues in Natural Language Processing, R. Schank and B.Nash-Webber, eds., Cambridge, Mass., June 10-13, 11-16.Riesbeck, C., 1975b.
Conceptual analysis.
In Schank (1975),83-156.Sehank, R., 1973.
Identification of conceptualizations underly-ing natural language.
In Schank R. and Colby, K., eds.,Computer Models of Thought and Language, W. H. Freemanand Company, San Francisco.Schank, R., 1975.
Conceptual Information Processing.
AmericanElsevier, New York.Shipman, D., and Marcus, M., 1979.
Towards minimal datastructures for deterministic parsing.
IJCAI-79, Tokyo, Aug20-23, 815-817.Wanner, IE., and Maratsos, M., 1978.
Linguistic Theory andPsychological Reality, M. Halle, J. Bresnan, G. Miller, eds.,MIT Press, Cambridge, Mass., 119-161.Winograd, T., 1972.
Understanding Natural Language.
AcademicPress, New York.Woods, W., 1970.
Transition network grammars for naturallanguage analysis.
CACM 13, October, 591-606.Daniel Chester is an assistant professor in the De-partment o f  Computer Sciences of the University ofTexas at Austin.
He received his Ph.D. in mathemat-ics from the University of California at Berkeley in1973.96 American Journal of Computational Linguistics, Volume 6, Number 2, April-June 1980
