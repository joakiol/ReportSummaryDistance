SESS ION 12: INFORMATION RETRIEVALDonna llarmanNational Institute of Standards and TechnologyBuilding 225/A216Gaithersburg, MD 20899Research in information retrieval is enjoying renewed in-terest by many different communities.
Commercial retrievalsystems, which in the past have concentrated on Booleanpattern matching methodologies, are beginning to look intomore sophisticated search methods, including complex sta-tistical and/or  natural anguage processing systems.
Thishas spurred new interest in research in information re-trieval in this community and also in the academic om-munities.
Technology is being additionally pushed by thecurrent emphasis on electronic communication, includingdigital libraries and the interlinking of oflices to allow of-rice automation.
The availability of electronic text recordsin huge (and exponentially-growing) quantities, and therapidly-expanding Internet access by potential users, is athird factor in promoting research.ARPA has contributed to this increased interest by spon-soring a new test collection for information retrieval.
Thewidespread availability of the T IPSTER collection has al-lowed research on large-scale, real-world retrieval problems.This has not only opened up new areas of research thatwere not discovered using the smMler test collections, buthas provided proof that the more complex retrieval systemsdo indeed scale up to handle realistic text collections.The first paper in this session, "Overview of the Sec-ond Text Retrieval Conference (TREC-2) ' ,  by Donna Har-man, il lustrates the use of this collection in a massive cross-system evaluation.
The TREC-2 conference, held in Augustof 1993, compared results from 31 different retrieval systemsworking with the T IPSTER collection.
These systems usedmany different approaches to retrieval, including manuallyconstructed patterns, automatically constructed statisticalqueries that were input to statistical retrieval systems, andnatural anguage approaches to information retrieval.
Thepaper discusses the T IPSTER test collection, the evaluationmethods used in TREC, and the results from the conference.The next two papers in the session represent systems thatappeared in TREG-2.
The first of these papers discusses amostly statistical system and the second of these papersdiscusses a system using natural anguage processing tech-niques.The paper "Learning from Relevant Documents in LargeScale Routing Retrieval", by K.L.
Kwok and L. Grunfeld,discusses experiments performed using a routing or filter-ing paradigm.
This type of information retrieval assumesthat users have a standing request for information, suchas in an electronic dissemination service or an intelligenceoperation.
There exists training information in the formof previously-seen documents considered relevant, and thistraining information is used to produce better queries.
Thispaper discusses in detail the problems of learning from full-text relevant documents, which range in length from a shortparagraph to many hundreds of pages.
This problem iscompounded by the availability of large numbers of suchrelevant documents.
Many experiments were performed todiscover the optimal method of selecting which (and whatparts) of documents to use for training, and the results aregiven in the paper.The next paper, "Document Representation i  NaturalLanguage Text Retrieval", by Tomek Strzalkowski, dis-cusses experiments performed using mostly the adhoc re-trieval paradigm.
In this case the documents are known inadvance, but the information is requested on an "adhoc"basis.
There is no training data, and systems are often re-quired to deal with short user requests that might not mapwell onto the terminology used in the documents.
One wayaround this problem is to automatically transform the userquery into a linguistic structure that is expanded to bettermap into the document collection.
This paper presents aseries of experiments in automatically locating useful lin-gulstic fragments of documents to match against such amodified user query.
One of the main issues dealt with hereis the correct term weighting for these fragments.Information retrieval is not limited to the matching oftextual material; two of the papers in the session deal withspeech retrieval systems.
The first of these papers describesa modification of traditional information retrieval methodsto handle speech, whereas the second paper uses traditionalspeech recognition technology with information retrieval asthe application.The paper, "Assessing the Retrieval Effectiveness of aSpeech Retrieval System by Simulating Recognition Er-rors", by Peter Schauble and Ulrike Glavltsch, deals withretrieval of speech (speech "documents").
Their retrievalsystem uses phonetically motivated subword units as op-posed to complete words for indexing of speech.
The useof subwords as index terms means that the system can beused against either speech or text, and that techniques tra-ditionally used in text retrieval can be modified for use withspeech.
The production of these subwords is dependent oncurrent speech recognition technology, which is known tobe error-prone.
This paper presents ome experiments us-ing simulated speech recognition errors against well-knowninformation retrieval test collections (textual) to see whateffects these errors have on retrieval performance.The second of these papers, "Speech-Based Retrieval us-ing Semantic Co-Occurrence Filtering", by Julian Kupiec,Don Kimber, and Vijay Balasubramanian, uses a standardhidden Markov model as input to a text retrieval system.The issue in this paper is how to deal with the very large(generally unrestricted) vocabulary size that is normal formost text retrieval applications.
Speech input using largevocabularies (and possibly many different speakers) is likely349to produce many inaccurate words so that a direct pho-netic dictionary lookup would not be reasonable.
The paperpresents a method using an n-best word selection to locatethe user's query words, and then uses co-occurrence ofthesen-best query word lists to locate relevant documents.The final paper in this session, = '(Almost) '  AutomaticSemantic Feature Extraction from Technical Text ~, by Earjeev Agarwal, does not deal directly with information re-trieval, but with the production of data that would be use-ful in an information retrieval system.
Some of the newerretrieval systems use knowledge bases to supplement (or re-place) the document indices.
This paper deals with naturallanguage processing methods that allow faster acquisition ofsuch iinformation.
The methods discussed also allow fasterporting of all types of natural anguage systems into new do-mmns by providing machine-~d to the building of semanticknow]ledge.350
