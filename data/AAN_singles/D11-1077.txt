Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 836?845,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsIdentification of Multi-word Expressions byCombining Multiple Linguistic Information SourcesYulia TsvetkovLanguage Technologies InstituteCarnegie Mellon Universityyulia.tsvetkov@gmail.comShuly WintnerDepartment of Computer ScienceUniversity of Haifashuly@cs.haifa.ac.ilAbstractWe propose an architecture for expressingvarious linguistically-motivated features thathelp identify multi-word expressions in nat-ural language texts.
The architecture com-bines various linguistically-motivated clas-sification features in a Bayesian Network.We introduce novel ways for computingmany of these features, and manually de-fine linguistically-motivated interrelationshipsamong them, which the Bayesian networkmodels.
Our methodology is almost en-tirely unsupervised and completely language-independent; it relies on few language re-sources and is thus suitable for a large num-ber of languages.
Furthermore, unlike muchrecent work, our approach can identify ex-pressions of various types and syntactic con-structions.
We demonstrate a significant im-provement in identification accuracy, com-pared with less sophisticated baselines.1 IntroductionMulti-word Expressions (MWEs) are lexical itemsthat consist of multiple orthographic words (e.g.,ad hoc, by and large, New York, kick the bucket).MWEs are numerous and constitute a significantportion of the lexicon of any natural language (Jack-endoff, 1997; Erman and Warren, 2000; Sag etal., 2002).
They are a heterogeneous class of con-structions with diverse sets of characteristics, dis-tinguished by their idiosyncratic behavior.
Mor-phologically, some MWEs allow some of their con-stituents to freely inflect while restricting (or pre-venting) the inflection of other constituents.
Insome cases MWEs may allow constituents to un-dergo non-standard morphological inflections thatthey would not undergo in isolation.
Syntactically,some MWEs behave like words while other arephrases; some occur in one rigid pattern (and a fixedorder), while others permit various syntactic trans-formations.
Semantically, the compositionality ofMWEs is gradual, ranging from fully compositionalto idiomatic (Bannard et al, 2003).Because of their prevalence and irregularity,MWEs must be stored in lexicons of natural lan-guage processing applications.
Correct handling ofMWEs has been proven beneficial for various ap-plications, including information retrieval, buildingontologies, text alignment, and machine translation.We propose a novel architecture for identifyingMWEs of various types and syntactic categories inmonolingual corpora.
Unlike much existing work,which focuses on a particular syntactic construction,our approach addresses MWEs of all types by focus-ing on the general idiosyncratic properties of MWEsrather than on specific properties of each sub-classthereof.
While we only evaluate our methodol-ogy on bi-grams, it can in principle be extendedto longer MWEs.
The architecture uses BayesianNetworks (BN) to express multiple interdependentlinguistically-motivated features.First, we automatically generate a small (training)set of MWE and non-MWE bi-grams (positive andnegative instances, respectively).
We then define aset of linguistically-motivated features that embodyobserved characteristics of MWEs.
We augmentthese by features that reflect collocation measures.Finally, we define dependencies among these fea-tures, expressed in the structure of a Bayesian Net-work model, which we then use for classification.This is a directed graph, whose nodes express thefeatures used for classification, and whose edges de-836fine causal relationships among these features.
Inthis architecture, learning does not result in a blackbox, expressed solely as feature weights.
Rather, thestructure of the BN allows us to learn the impact ofdifferent MWE features on the classification.
Theresult is a new unsupervised method for identifyingMWEs of various types in text corpora.
It com-bines statistics with a large array of linguistically-motivated features, organized in an architecture thatreflects interdependencies among the features.The contribution of this work is manifold.
First,we show how to generate training material (al-most) automatically, so the method is almost com-pletely unsupervised.
The methodology we advo-cate is thus language-independent, requiring rela-tively few language resources, and is therefore op-timal for medium-density languages (Varga et al,2005).
Second, we propose several linguistically-motivated features that can be computed from dataand that are demonstrably productive for improv-ing the accuracy of MWE identification.
These fea-ture focus on the expression of linguistic idiosyn-crasies of various types, a phenomenon typical ofMWEs.
We propose novel computational model-ing of many of these features; in particular, we ac-count for the morphological idiosyncrasy of MWEsusing a histogram of the number of inflected forms,in a technique that draws from image processing.Third, we advocate the use of Bayesian Networksas a mechanism for expressing manually-crafted de-pendencies among features; the use of BN signifi-cantly improves the classification accuracy.
Finally,we demonstrate the utility of our methodology byapplying it to Hebrew.1 Our evaluation shows thatthe use of linguistically-motivated features results inreduction of 23% of the errors compared with a col-location baseline; organizing the knowledge in a BNreduces the error rate by additional 8.7%.After discussing related work in the next section,we describe in Section 3 the methodology we pro-pose, including a detailed discussion of the featuresand their implementation.
Section 4 provides a thor-ough evaluation of the results.
We conclude withsuggestions for future research.1To facilitate readability we use a transliteration of Hebrewusing Roman characters; the letters used, in Hebrew lexico-graphic order, are abgdhwzxTiklmns?pcqrs?t.2 Related WorkEarly approaches to MWEs identification concen-trated on their collocational behavior (Church andHanks, 1990).
Pecina (2008) compares 55 differ-ent association measures in ranking German Adj-N and PP-Verb collocation candidates.
He showsthat combining different collocation measures usingstandard statistical classification methods improvesover using a single collocation measure.
Other re-sults (Chang et al, 2002; Villavicencio et al, 2007)suggest that some collocation measures (especiallyPMI and Log-likelihood) are superior to others foridentifying MWEs.Soon, however, it became clear that mere co-occurrence measurements are not enough to identifyMWEs, and their linguistic properties should be ex-ploited as well (Piao et al, 2005).
Hybrid methodsthat combine word statistics with linguistic informa-tion exploit morphological, syntactic and semanticidiosyncrasies to extract idiomatic MWEs.Ramisch et al (2008) evaluate a number of asso-ciation measures on the task of identifying EnglishVerb-Particle Constructions and German Adjective-Noun pairs.
They show that adding linguistic infor-mation (mostly POS and POS-sequence patterns) tothe association measure yields a significant improve-ment in performance over using pure frequency.Several works address the lexical fixedness or syn-tactic fixedness of (certain types of) MWEs in orderto extract them from texts.
An expression is con-sidered lexically fixed if replacing any of its con-stituents by a semantically (and syntactically) sim-ilar word generally results in an invalid or literalexpression.
Syntactically fixed expressions prohibit(or restrict) syntactic variation.
For example, Van deCruys and Villada Moiro?n (2007) use lexical fixed-ness to extract Dutch Verb-Noun idiomatic com-binations (VNICs).
Bannard (2007) uses syntac-tic fixedness to identify English VNICs.
Anotherwork uses both the syntactic and the lexical fixed-ness of VNICs in order to distinguish them fromnon-idiomatic ones, and eventually to extract themfrom corpora (Fazly and Stevenson, 2006).While these approaches are in line with ours, theyrequire lexical semantic resources (e.g., a databasethat determines semantic similarity among words)and syntactic resources (parsers) that are unavail-837able for Hebrew (and many other languages).
Ourapproach only requires morphological processingand a bilingual dictionary, which are more readily-available for several languages.
Note also thatthese approaches target a specific syntactic construc-tion, whereas ours is adequate for various types ofMWEs.Several properties of Hebrew MWEs are de-scribed by Al-Haj (2010); Al-Haj and Wintner(2010) use them in order to construct an SVM-basedclassifier that can distinguish between MWE andnon-MWE noun-noun constructions in Hebrew.
Thefeatures of the SVM reflect several morphologicaland morpho-syntactic properties of such construc-tions.
The resulting classifier performs much bet-ter than a na?
?ve baseline, reducing over one third ofthe errors.
We rely on some of these insights, aswe implement more of the linguistic properties ofMWEs.
Again, our methodology is not limited to aparticular construction: indeed, we demonstrate thatour general methodology, trained on automatically-generated, general training data, performs almost aswell as the noun-noun-specific approach of Al-Hajand Wintner (2010) on the very same dataset.Recently, Tsvetkov and Wintner (2010b) intro-duced a general methodology for extracting MWEsfrom bilingual corpora, and applied it to Hebrew.The results were a highly accurate set of HebrewMWEs, of various types, along with their Englishtranslations.
A major limitation of this work is thatit can only be used to identify MWEs in the bilingualcorpus, and is thus limited in its scope.
We use thismethodology to extract both positive and negativeinstances for our training set in the current work; butwe extrapolate the results much further by extend-ing the method to monolingual corpora, which aretypically much larger than bilingual ones.Bayesian Networks have only scarcely been usedfor classification in natural language applications.For example, BN were used for POS tagging of un-known words (Peshkin et al, 2003); dependencyparsing (Savova and Peshkin, 2005); and docu-ment classification (Lam et al, 1997; Calado et al,2003; Denoyer and Gallinari, 2004).
Very recently,Ramisch et al (2010) have used BN for PortugueseMWE identification.
The features used for classi-fication were of two kinds: (1) various collocationmeasures; (2) bi-grams aligned together by an auto-matic word aligner applied to a parallel (Portuguese-English) corpus.
A BN was used to combine the pre-dictions of the various features on the test set, butthe structure of the network is not described.
Thecombined classifier resulted in a much higher accu-racy than any of the two methods alone.
However,the BN does not play any special role in this work,and its structure does not reflect any insights or intu-itions on the structure of the problem domain or oninterdependencies among features.We, too, acknowledge the importance of combin-ing different types of knowledge in the hard task ofMWE identification.
In particular, we also believethat collocation measures are highly important forthis task, but cannot completely solve the problem:linguistically-motivated features are mandatory inorder to improve the accuracy of the classifier.
Inthis work we focus on various properties of differenttypes of MWEs, and define general features that mayaccurately apply to some, but not necessarily all ofthem.
An architecture of Bayesian Networks is op-timal for this task: it enables us to define weighteddependencies among features, such that certain fea-tures are more significant for identifying some classof MWEs, whereas others are more prominent inidentifying other classes.
As we show below, this ar-chitecture results in significant improvements over amore na?
?ve combination of features.3 Methodology3.1 MotivationThe task we address is identification of MWEs, ofvarious types and syntactic constructions, in mono-lingual corpora.2 Several properties of MWEs makethis task challenging: MWEs exhibit idiosyncrasieson a variety of levels, orthographic, morphological,syntactic and of course semantic (Al-Haj, 2010).They are also extremely diverse: for example, onthe semantic dimension alone, MWEs cover an en-tire spectrum, ranging from frozen, fixed idioms tofree combinations of words (Bannard et al, 2003).Such a complex task calls for a combination ofmultiple approaches, and much research indeed sug-gests ?hybrid?
approaches to MWE identification2For simplicity, we focus on bi-grams of tokens (MWEs oflength 2) in this work; the methodology, however, is easily ex-tensible to longer n-grams.838(Duan et al, 2009; Weller and Fritzinger, 2010;Ramisch et al, 2010; Hazelbeck and Saito, 2010).We believe that Bayesian Networks provide an op-timal architecture for expressing various pieces ofknowledge aimed at MWE identification, for the fol-lowing reasons (Heckerman, 1995):?
In contrast to many other classification meth-ods, BN can learn (and express) causal relation-ships between features.
This facilitates betterunderstanding of the problem domain.?
BN can encode not only statistical data, but alsoprior domain knowledge and human intuitions,in the form of interdependencies among fea-tures.
We do indeed use this possibility here.3.2 Linguistically-motivated FeaturesBased on the observations of Al-Haj (2010), wedefine several linguistically-motivated features thatare aimed at capturing some of the unique proper-ties of MWEs.
While many idiosyncratic propertiesof MWEs have been previously studied, we intro-duce novel ways to express those properties as com-putable features informing a classifier.
Note thatmany of the features we describe below are com-pletely language-independent; others are applicableto a wide range of languages, while few are specificto morphologically-rich languages, and can be ex-hibited in different ways in different languages.
Themethodology we advocate, however, is completelyuniversal.A common theme for all these features is idiosyn-cracy: they are all aimed at locating some linguis-tic property on which MWEs may differ from non-MWEs.
Below we detail these properties, alongwith the features that we define to reflect them.
Inall cases, the feature is applied to a candidate MWE,defined here as a bi-gram of tokens (all possible bi-grams are potential candidates).
To compute the fea-tures, we use a 46M-token monolingual Hebrew cor-pus (Itai and Wintner, 2008), which we pre-processas in Tsvetkov and Wintner (2010b).
All statisticsare computed from this large corpus.
Likewise, wecompute these features on a small training corpus,which we generate automatically (see Section 3.4).Orthographic variation Sometimes, MWEs arewritten with dashes instead of inter-token spaces.We define a binary feature, DASH, whose value is 1iff the dash character appears in some surface formof the candidate MWE.
For example, xd-cddi (onesided ) ?unilateral?.Hapax legomena MWEs sometimes include con-stituents that have no usage outside the particularexpression, and are hence not included in lexicons.We define a feature, HAPAX, whose value is a binaryvector with 1 in the i-th place iff the i-th word of thecandidate is not in the lexicon, and does not occurin other bi-grams at the same location.
For exam-ple, hwqws pwqws ?hocus-pocus?.
In order to filterout potential errors, candidates must occur at least 5times in the corpus in order for this feature to fire.Frozen form MWE constituents sometimes occurin one fixed, frozen form.
We define a feature,FROZEN, whose value is a binary vector with 1 in thei-th place iff the i-th word of the candidate never in-flects in the context of this expression.
Example: bitxwlim (house-of sick-people) ?hospital?
; the nounxwlim must be in the plural in this MWE.Partial morphological inflection In some cases,MWE constituents undergo a (strict but non-empty)subset of the full inflections that they would undergoin isolation.
We capture this property with a tech-nique that has been proven useful in the area of im-age processing (Jain, 1989, Section 7.3).
We com-pute a histogram of the distribution in the corpus ofall the possible surface forms of each constituent ofan MWE candidate.
Such histograms can compactlyrepresent distributional information on morphologi-cal behavior, in the same way that histograms of thedistribution of gray levels in a picture are used torepresent the picture itself.Our assumption is that the inflection histogramsof non-MWEs are more uniform than the histogramsof MWEs, in which some inflections may be morefrequent and others may be altogether missing.
Ofcourse, restrictions on the histogram may stem fromthe part of speech of the expression; such constraintsare captured by dependencies in the BN structure.Since each MWE is idiosyncratic in its ownway, we do not expect the histograms of MWEs tohave some specific pattern, except non-uniformity.We therefore sort the columns of each histogram,thereby losing information pertaining to the specific839inflections, and retaining only information about theidiosyncrasy of the histogram.
Offline, we computethe average histogram for positive and negative ex-amples: The average histogram of MWEs is shorterand less uniform than the average histogram of non-MWEs.
We define as feature, HIST, the L1 (Manhat-tan) distance between the histogram of the candidateand the closest average histogram.For example, the MWE bit mepv (house-of law)?court?
occurs in the following inflected forms:bit hmepv ?the court?
(75%); bit mepv ?a court?
(15%); bti hmepv ?the courts?
(8%); and bti mepv?courts?
(2%).
The histogram for this candidateis thus (75, 15, 8, 2).
In contrast, the non-MWEtxwm mepv (domain-of law) ?domain of the law?,which is syntactically identical, occurs in nine dif-ferent inflected forms, and its sorted histogram is(59, 14, 7, 7, 5, 2, 2, 2, 2).Context We hypothesize that MWEs tend to con-strain their (semantic) context more strongly thannon-MWEs.
We expect words that occur imme-diately after MWEs to vary less freely than wordsthat immediately follow other expressions.
One mo-tivation for this hypothesis is the observation thatMWEs tend to be less polysemous than free com-binations of words, thereby limiting the possible se-mantic context in which they can occur.We define a feature, CONTEXT, as follows.
Wefirst compute a histogram of the frequencies ofwords following each candidate MWE.
We trim thetail of the histogram by removing words whose fre-quency is lower than 0.1% (the expectation is thatnon-MWEs would have a much longer tail).
Off-line, we compute the same histograms for positiveand negative examples and average them as above.The value of CONTEXT is 1 iff the histogram of thecandidate is closer (in terms of L1 distance) to thepositive average.For example, the histogram of bit mepv ?court?includes 15 values, dominated by bit mepv yliwn?supreme court?
(20%) and bit mepv mxwzi ?dis-trict court?
(13%), followed by contexts whose fre-quency ranges between 5% and 0.6%.
In con-trast, the non-MWE txwm mepv ?domain-of law?has a much shorter histogram, namely (12, 11, 6):over 70% of the words following this expression oc-cur less than 0.1% and are hence in the trimmed tail.Syntactic diversity MWEs can belong to variouspart of speech categories.
We define as feature, POS,the category of the candidate, with values obtainedby selecting frequent tuples of POS tags.
For exam-ple, Noun-Noun, PropN-PropN, Noun-Adj, etc.Translational equivalents Since MWEs are of-ten idiomatic, they tend to be translated in a non-literal way, sometimes to a single word.
We usea dictionary to generate word-by-word translationsof candidate MWEs to English, and check the num-ber of occurrences of the English literal translationin a large English corpus.3 Due to differences inword order between the two languages, we createtwo variants for each translation, corresponding toboth possible orders.
We expect non-MWEs to havesome literal translational equivalent (possibly withfrequency that correlates with their frequency in He-brew), whereas for MWEs we expect no (or few) lit-eral translations.
We define a binary feature, TRANS,whose value is 1 iff some literal translation of thecandidate occurs more than 5 times in the corpus.For example, the MWE htxtn ym (marry with )?marry?
is literally translated as with marry, marrywith, together marry and marry together, none ofwhich occurs in the corpus.Collocation As a baseline, statistical associationmeasure, we use a heuristic variant of pointwise mu-tual information (PMI), promoting also collocationswhose constituents are frequent (Tsvetkov and Wint-ner, 2010b).
We define a binary feature, PMI, withvalues (low and high) reflecting the threshold thatmaximizes the accuracy of MWE classification inTsvetkov and Wintner (2010b).3.3 Feature Interdependencies Expressed as aBayesian NetworkA Bayesian Network (Jensen and Nielsen, 2007) isorganized as a graph whose nodes are random vari-ables and whose edges represent interdependenciesamong those variables.
We use a particular typeof BN, known as causal networks, in which di-rected edges lead to a variable from each of its directcauses.
This facilitates the expression of domainknowledge (and intuitions, beliefs, etc.)
as struc-tural properties of the network.
We use the BN as3We use a 120M-token newspaper corpus.840a classification device: training amounts to comput-ing the joint probability distribution of the trainingset, whereas classification maximizes the posteriorprobability of the particular node (variable) beingqueried.For MWE identification we define a BN whosenodes correspond to the features described in Sec-tion 3.2.
In addition, we define a node MWE forthe complete classification task.
Over these nodeswe impose the structure depicted graphically in Fig-ure 1.
This structure, which we motivate below, ismanually defined: it reflects our understanding ofthe problem domain and is a result of thorough ex-perimentations.
That said, it can of course be mod-ified in various ways, and in particular, new nodescan be easily added to reflect additional features.MWEHAPAXDASH CNTXTPOSHISTPMITRANSFRZNFigure 1: Bayesian Network for MWE identificationAll nodes depend on MWE, as all are affectedby whether or not the candidate is a MWE.
ThePOS of an expression influences its morphologicalinflection, hence the edges from POS to HIST andto FROZEN.
For example, Hebrew noun-noun con-structions allow their constituents to undergo the fullinflectional paradigm, but when such a constructionis a MWE, inflection is severely constrained (Al-Hajand Wintner, 2010); similarly, when one of the con-stituents of a MWE is a conjunction, the entire ex-pression is very likely to be frozen.Hapaxes clearly affect all statistical metrics,hence the edge from HAPAX to PMI, and also theexistence of literal translation, since if a word is notin the lexicon, it does not have a translation, hencethe edge from HAPAX to TRANS.
Also, we assumethat there is a correlation between the frequency (andPMI) of a candidate and whether or not a literaltranslation of the expression exists, hence the edgefrom PMI to TRANS.
The edges from PMI and HISTto CONTEXT are justified by the correlation betweenthe frequency and variability of an expression andthe variability of the context in which it occurs.Once the structure of the network is established,the conditional probabilities of each dependencyhave to be determined.
We compute the conditionalprobability tables from our training data (see below)using Weka (Hall et al, 2009), and obtain valuesfor P (X | X1, .
.
.
, Xk) for each variable X and allvariables Xi, 1 ?
i ?
k, such that the graph in-cludes an edge from Xi to X (parents of X).
Wethen perform inference on the network in order tocompute P (Xmwe | X1, .
.
.
, Xk), where Xmwecorresponds to the node MWE, and X1, .
.
.
, Xk arethe variables corresponding to all other nodes in thenetwork.
Using Bayes Rule,P (Xmwe | X1, .
.
.
, Xk) ?P (X1, .
.
.
, Xk | Xmwe)?
P (Xmwe)We define the prior, P (Xmwe), to be 0.41:this is the percentage of MWEs in WordNet 1.7(Fellbaum, 1998).
The conditional probabilitiesP (X1, .
.
.
, Xk | Xmwe) are determined by Wekafrom the conditional probability tables:P (X1, .
.
.
, Xk | Xmwe) = ?ki=1P (Xi | pai)where k is the number of nodes in the BN (other thanXmwe) and pai is the set of parents of Xi.3.4 Automatic Generation of Training DataFor training we need samples of positive and nega-tive instances of MWEs, each associated with a vec-tor of the values of all features discussed in Sec-tion 3.2.
We generate this training material auto-matically.
We use a small Hebrew-English bilin-gual corpus (Tsvetkov and Wintner, 2010a).
Weword-align the corpus with Giza++ (Och and Ney,2003), and then apply the (completely unsupervised)841algorithm of Tsvetkov and Wintner (2010b), whichextracts MWE candidates from the aligned corpusand re-ranks them using statistics computed from alarge monolingual corpus.
The core idea behind thismethod is that MWEs tend to be translated in non-literal ways; in a parallel corpus, words that are 1:1aligned typically indicate literal translations and arehence unlikely constituents of MWEs.The result is a set of 134,001 Hebrew bi-gramtypes (from the bilingual corpus), classified as either1:1 aligned (implying they are likely not MWEs)or unaligned (in which case they may or may notbe MWEs).
In addition, for each bi-gram wehave a PMI score; naturally, higher PMI scoresare indicative of MWEs.
We thus divide the setinto four classes: aligned bi-grams with high PMIscore, aligned bi-grams with low PMI score, mis-aligned with high PMI and misaligned with lowPMI.
Aligned bi-grams, independently of their PMIscore, are more likely non-MWEs; high-PMI mis-aligned bi-grams are very likely MWEs; and the sta-tus of low-PMI misaligned bi-grams is unclear, andmust be further investigated.
This is summarized inTable 1.Misaligned AlignedHigh PMI MWE non-MWELow PMI unclear non-MWETable 1: Classification of bi-gramsWe set the threshold that separates low PMI fromhigh PMI as in Tsvetkov and Wintner (2010b).
Theresults of this classification is depicted in Table 2.Misaligned Aligned TotalHigh PMI 2,203 493 2,696Low PMI 61,314 69,991 131,305Total 63,517 70,484 134,001Table 2: Statistics of the sample space from which thetraining set is generatedWe assume that all bi-grams in the ?Aligned?
col-umn are non-MWEs.
Additionally, we assume thatthe 2,203 misaligned bi-grams with high PMI scoresare likely MWEs.
As for the set of over 61,000 mis-aligned low-PMI bi-grams, certainly many of themare non-MWEs, but some may be MWEs, and weare interested in including them as positive examplesof MWEs with low PMI scores.
We therefore manu-ally annotate a sample of 50 MWEs from this partic-ular set (we had to manually go over a few thousandsof bi-grams to select this sample).
This is the onlysupervision provided in this work.The remaining question is how to determine thesizes of samples from each of the other three classes.We use two guidelines: first, we would like the ra-tio of MWEs to non-MWEs in the training set to be41 : 59, reflecting the ratio in WordNet (the priorMWE probability).
Second, we would like classifi-cation by PMI score only to yield a reasonable base-line; the baseline is defined as the ratio of the sum ofhigh-PMI MWEs plus low-PMI non-MWEs to thesize of the training set.
We choose 67%, the PMIbaseline reported by Al-Haj and Wintner (2010).
Asa result of these two considerations, we end up withtraining sets whose sizes are depicted in Table 3.
Werandomly select from the sample space this many in-stances for each class.
Since much of the procedureof preparing training data is automatic, the resultsmay be somewhat noisy.
As Bayesian Network areknown to be robust to noisy data, we expect the BNto compensate for this problem.MWE non-MWE TotalHigh PMI 300 232 532Low PMI 50 272 322Total 350 504 854Table 3: Sizes of each training set4 Results and EvaluationWe use the training set described above for train-ing and evaluation: we perform 10-fold cross vali-dation experiments, reporting Precision, Recall, Ac-curacy and F-measure in three setups: one (SVM)in which we train an SVM classifier4 with thefeatures described in Section 3.2; one (BN-auto)in which we train a BN but let Weka determineits structure (using the K2 algorithm); and one(BN) in which we train a Bayesian Network whosestructure reflects manually-crafted linguistically-motivated knowledge, as depicted in Figure 1.
The4We use Weka SMO with the PolyKernel setup; experimen-tation with several other kernels yielded worse results.842results, along with the PMI baseline figures, arelisted in Table 4.Accuracy Prec.
Recall F-scorePMI 66.98% 0.73 0.67 0.67BN-auto 71.19% 0.71 0.71 0.71SVM 74.59% 0.75 0.75 0.75BN 76.82% 0.77 0.77 0.77Table 4: 10-fold cross validation evaluation resultsThe linguistically-motivated features defined inSection 3.2 are clearly helpful in the classificationtask: the accuracy of the SVM, informed by thesefeatures, is close to 75%, reducing the error rateof the PMI baseline by 23%.
The contributionof the Bayesian Network is also highly significant,reducing almost 7% more errors (8.7% of the er-rors made by the SVM classifier), or a total of al-most 30% error-rate reduction with respect to thebaseline.
Interestingly, a BN whose structure doesnot reflect prior knowledge, but is rather learned au-tomatically, performs poorly.
It is the combinationof linguistically-motivated features with feature in-terdependencies reflecting domain knowledge thatcontribute to the best performance.As a further demonstration of the utility of ourapproach, we evaluate the algorithm on an addi-tional test set that was used for evaluation in the past(Tsvetkov and Wintner, 2010b; Al-Haj and Wintner,2010).
This is a small annotated corpus, NN, of He-brew noun-noun constructions.
The corpus consistsof 413 high-frequency bi-grams of the same syntac-tic construction; of those, 178 are tagged as MWEs(in this case, noun compounds) and 235 as non-MWEs.
This corpus consolidates the annotation ofthree annotators: only instances on which all threeagreed were included.
Since it includes both posi-tive and negative instances, this corpus facilitates arobust evaluation of precision and recall.We train a Bayesian Network on the training setdescribed in Section 3.4 and use it to classify the setNN.
We compare the results of this classifier with aPMI baseline (using the same threshold as above),and also with the classification results reported byAl-Haj and Wintner (2010) (AW); the latter reflects10-fold cross-validation evaluation using the entireset, so it should be considered an upper bound forany classifier that uses a general training corpus.The results are depicted in Table 5.
They clearlydemonstrate that the linguistically-motivated fea-tures we define provide a significant improvement inclassification accuracy over the baseline PMI mea-sure.
Note that our F-score, 0.77, is very close tothe best result of 0.79 obtained by Al-Haj and Wint-ner (2010) as the average of 10-fold cross valida-tion runs, using only high-frequency noun-noun con-structions for training.
We interpret this result as afurther proof of the robustness of our architecture.Accuracy Precision Recall F-scorePMI 71.43% 0.71 0.71 0.71BN 77.00% 0.77 0.77 0.77AW 80.77% 0.77 0.81 0.79Table 5: Evaluation results: noun-noun constructionsFinally, we have used the trained BN to classifythe entire set of bi-grams present in the (Hebrewside of the) parallel corpus described in Tsvetkovand Wintner (2010a).
Of the 134,000 candidates,only 4,000 are classified as MWEs.
We sort thislist of potential MWEs by the probability assignedby the BN to the positive value of the variableXmwe.
The resulting sorted list is dominated byhigh-PMI bi-grams, especially proper names, all ofwhich are indeed MWEs.
The first non-MWE (falsepositive) occurs in the 50th place on the list; it iscrpt niqwla ?France Nicolas?, which is obviously asub-sequence of the larger MWE, neia crpt niqwlasrqwzi ?French president Nicolas Sarkozy?.
Simi-lar sub-sequences are also present, but only five arein the top-100.
Such false positives can be reducedwhen longer MWEs are extracted, as it can be as-sumed that a sub-sequence of a longer MWE doesnot have to be identified.
Other false positives in thetop-100 include some highly frequent expressions,but over 85 of the top-100 are clearly MWEs.While more careful evaluation is required in orderto estimate the rate of true positives in this list, wetrust that the vast majority of the positive results areindeed MWEs.5 Conclusions and future workWe presented a novel architecture for identifyingMWEs in text corpora.
The main insights we em-843phasize are sophisticated computational encoding oflinguistic knowledge that focuses on the idiosyn-cratic behavior of such expressions.
This is reflectedin two ways in our work: by defining computablefeatures that reflect different facets of irregulari-ties; and by framing the features as part of a largerBayesian Network that accounts for interdependen-cies among them.
We also introduce a method forautomatically generating a training set for this task,which renders the classification almost entirely un-supervised.
The result is a nearly-unsupervised,language-independent classification method that canidentify MWEs of various lengths, types and con-structions.
Evaluation on Hebrew shows significantimprovement in the accuracy of the classifier com-pared with the state of the art.The modular architecture of BN facilitates easyexploration with more features.
We are currently in-vestigating the contribution of various other sourcesof information to the classification task.
For exam-ple, Hebrew lacks large-scale lexical semantic re-sources.
However, it is possible to literally trans-late a MWE candidate to English and rely on theEnglish WordNet for generating synonyms of the lit-eral translation.
Such ?literal synonyms?
can then beback-translated to Hebrew.
The assumption is thatif a back-translated expression has a high PMI, theoriginal candidate is very likely not a MWE.
Whilesuch a feature may contribute little on its own, in-corporating it in a well-structured BN may improveperformance.While our methodology is applicable to MWEsof any length, we have so far only evaluated it on bi-grams.
In the future, we intend to extend the evalu-ation to longer n-grams.
We also plan to apply themethodology to languages other than Hebrew.AcknowledgmentsThis research was supported by THE ISRAELSCIENCE FOUNDATION (grants No.
137/06,1269/07).
We are grateful to Gennadi Lemberskyfor his continuous help.ReferencesHassan Al-Haj and Shuly Wintner.
2010.
Identifyingmulti-word expressions by leveraging morphologicaland syntactic idiosyncrasy.
In Proceedings of the 23rdInternational Conference on Computational Linguis-tics (COLING 2010), pages 10?18, Beijing, China,August.
Coling 2010 Organizing Committee.Hassan Al-Haj.
2010.
Hebrew multiword expressions:Linguistic properties, lexical representation, morpho-logical processing, and automatic acquisition.
Mas-ter?s thesis, University of Haifa, February.Colin Bannard, Timothy Baldwin, and Alex Lascarides.2003.
A statistical approach to the semantics of verb-particles.
In Diana McCarthy Francis Bond, Anna Ko-rhonen and Aline Villavicencio, editors, Proceedingsof the ACL 2003 Workshop on Multiword Expressions:Analysis, Acquisition and Treatment, pages 65?72.Colin Bannard.
2007.
A measure of syntactic flexibilityfor automatically identifying multiword expressions incorpora.
In Proceedings of the Workshop on A BroaderPerspective on Multiword Expressions, pages 1?8.
As-sociation for Computational Linguistics.Pa?vel Calado, Marco Cristo, Edleno Silva De Moura,Nivio Ziviani, Berthier A. Ribeiro-Neto, and Mar-cos Andre?
Gonc?alves.
2003.
Combining link-basedand content-based methods for web document classifi-cation.
In Proceedings of CIKM-03, 12th ACM Inter-national Conference on Information and KnowledgeManagement, pages 394?401, New Orleans, US.
ACMPress, New York, US.Baobao Chang, Pernilla Danielsson, and Wolfgang Teu-bert.
2002.
Extraction of translation unit fromChinese-English parallel corpora.
In Proceedings ofthe first SIGHAN workshop on Chinese language pro-cessing, pages 1?5, Morristown, NJ, USA.
Associa-tion for Computational Linguistics.Kenneth Ward Church and Patrick Hanks.
1990.
Wordassociation norms, mutual information, and lexicogra-phy.
Computational Linguistics, 16(1):22?29.Ludovic Denoyer and Patrick Gallinari.
2004.
Bayesiannetwork model for semi-structured document classi-fication.
Information Processing and Management,40(5):807?827.Jianyong Duan, Mei Zhang, Lijing Tong, and Feng Guo.2009.
A hybrid approach to improve bilingual mul-tiword expression extraction.
In Thanaruk Theera-munkong, Boonserm Kijsirikul, Nick Cercone, andTu-Bao Ho, editors, Advances in Knowledge Discov-ery and Data Mining, volume 5476 of Lecture Notesin Computer Science, pages 541?547.
Springer, Berlinand Heidelberg.Britt Erman and Beatrice Warren.
2000.
The idiom prin-ciple and the open choice principle.
Text, 20(1):29?62.Afsaneh Fazly and Suzanne Stevenson.
2006.
Automat-ically constructing a lexicon of verb phrase idiomaticcombinations.
In Proceedings of the 11th Conferenceof the European Chapter of the Association for Com-putational Linguistics (EACL), pages 337?344.844Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Language, Speech and Com-munication.
MIT Press.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: an update.SIGKDD Explorations, 11(1):10?18.Gregory Hazelbeck and Hiroaki Saito.
2010.
A hybridapproach for functional expression identification in ajapanese reading assistant.
In Proceedings of the 2010Workshop on Multiword Expressions: from Theory toApplications, pages 81?84, Beijing, China, August.Coling 2010 Organizing Committee.David Heckerman.
1995.
A tutorial on learning withBayesian networks.
Technical Report MSR-TR-95-06, Microsoft Research, March.Alon Itai and Shuly Wintner.
2008.
Language resourcesfor Hebrew.
Language Resources and Evaluation,42(1):75?98, March.Ray Jackendoff.
1997.
The Architecture of the LanguageFaculty.
MIT Press, Cambridge, USA.Anil K. Jain.
1989.
Fundamentals of digital image pro-cessing.
Prentice-Hall, Inc., NJ, USA.Finn V. Jensen and Thomas D. Nielsen.
2007.
BayesianNetworks and Decision Graphs.
Springer, 2nd edition.Wai Lam, Kon F. Low, and Chao Y. Ho.
1997.
Using abayesian network induction approach for text catego-rization.
In Martha E. Pollack, editor, Proceedings ofIJCAI-97, 15th International Joint Conference on Ar-tificial Intelligence, pages 745?750, Nagoya, JP.
Mor-gan Kaufmann Publishers, San Francisco, US.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Pavel Pecina.
2008.
A machine learning approach tomultiword expression extraction.
In Proceedings ofthe LREC Workshop Towards a Shared Task for Multi-word Expressions.Leonid Peshkin, Avi Pfeffer, and Virginia Savova.
2003.Bayesian nets in syntactic categorization of novelwords.
In Proceedings of the 2003 Conference of theNorth American Chapter of the Association for Com-putational Linguistics on Human Language Technol-ogy: companion volume of the Proceedings of HLT-NAACL 2003?short papers - Volume 2, NAACL ?03,pages 79?81, Morristown, NJ, USA.
Association forComputational Linguistics.Scott Songlin Piao, Paul Rayson, Dawn Archer, and TonyMcEnery.
2005.
Comparing and combining a se-mantic tagger and a statistical tool for mwe extraction.Computer Speech and Language, 19(4):378?397.Carlos Ramisch, Paulo Schreiner, Marco Idiart, andAlline Villavicencio.
2008.
An evaluation of meth-ods for the extraction of multiword expressions.
InProceedings of the LREC Workshop Towards a SharedTask for Multiword Expressions.Carlos Ramisch, Helena de Medeiros Caseli, AlineVillavicencio, Andre?
Machado, and Maria Finatto.2010.
A hybrid approach for multiword expressionidentification.
In Thiago Pardo, Anto?nio Branco,Aldebaro Klautau, Renata Vieira, and Vera de Lima,editors, Computational Processing of the PortugueseLanguage, volume 6001 of Lecture Notes in ComputerScience, pages 65?74.
Springer.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword expres-sions: A pain in the neck for NLP.
In Proceedings ofthe Third International Conference on Intelligent TextProcessing and Computational Linguistics (CICLING2002), pages 1?15, Mexico City, Mexico.Virginia Savova and Leonid Peshkin.
2005.
Dependencyparsing with dynamic bayesian network.
In Proceed-ings of the 20th national conference on Artificial intel-ligence - Volume 3, pages 1112?1117.
AAAI Press.Yulia Tsvetkov and Shuly Wintner.
2010a.
Automaticacquisition of parallel corpora from websites with dy-namic content.
In Proceedings of the Seventh confer-ence on International Language Resources and Eval-uation (LREC?10), pages 3389?3392.
European Lan-guage Resources Association (ELRA), May.Yulia Tsvetkov and Shuly Wintner.
2010b.
Extractionof multi-word expressions from small parallel corpora.In Proceedings of the 23rd International Conferenceon Computational Linguistics (COLING 2010), Au-gust.Tim Van de Cruys and Begon?a Villada Moiro?n.
2007.Semantics-based multiword expression extraction.
InProceedings of the Workshop on A Broader Perspec-tive on Multiword Expressions, pages 25?32, Prague,Czech Republic, June.
Association for ComputationalLinguistics.Da?niel Varga, Pe?ter Hala?csy, Andra?s Kornai, ViktorNagy, La?szlo?
Ne?meth, and Viktor Tro?n.
2005.
Par-allel corpora for medium density languages.
In Pro-ceedings of RANLP?2005, pages 590?596.Aline Villavicencio, Valia Kordoni, Yi Zhang, MarcoIdiart, and Carlos Ramisch.
2007.
Validation andevaluation of automatically acquired multiword ex-pressions for grammar engineering.
In Proceedingsof the 2007 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning (EMNLP-CoNLL), pages1034?1043.Marion Weller and Fabienne Fritzinger.
2010.
A hy-brid approach for the identification of multiword ex-pressions.
In Proceedings of the SLTC 2010 Workshopon Compounds and Multiword Expressions, October.845
