Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1702?1712,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsEvaluating Text Segmentation using Boundary Edit DistanceChris FournierUniversity of OttawaOttawa, ON, Canadacfour037@eecs.uottawa.caAbstractThis work proposes a new segmentationevaluation metric, named boundary simi-larity (B), an inter-coder agreement coef-ficient adaptation, and a confusion-matrixfor segmentation that are all based upon anadaptation of the boundary edit distance inFournier and Inkpen (2012).
Existing seg-mentation metrics such as Pk, WindowD-iff, and Segmentation Similarity (S) areall able to award partial credit for nearmisses between boundaries, but are biasedtowards segmentations containing few ortightly clustered boundaries.
Despite S?simprovements, its normalization also pro-duces cosmetically high values that over-estimate agreement & performance, lead-ing this work to propose a solution.1 IntroductionText segmentation is the task of splitting text intosegments by placing boundaries within it.
Seg-mentation is performed for a variety of purposesand is often a pre-processing step in a larger task.E.g., text can be topically segmented to aid videoand audio retrieval (Franz et al, 2007), questionanswering (Oh et al, 2007), subjectivity analysis(Stoyanov and Cardie, 2008), and even summa-rization (Haghighi and Vanderwende, 2009).A variety of segmentation granularities, oratomic units, exist, including segmentations at themorpheme (e.g., Sirts and Aluma?e 2012), word(e.g., Chang et al 2008), sentence (e.g., Rey-nar and Ratnaparkhi 1997), and paragraph (e.g.,Hearst 1997) levels.
Between each atomic unit liesthe potential to place a boundary.
Segmentationscan also represent the structure of text as beingorganized linearly (e.g., Hearst 1997), hierarchi-cally (e.g., Eisenstein 2009), etc.
Theoretically,segmentations could also contain varying bound-ary types, e.g., two boundary types could differen-tiate between act and scene breaks in a play.Because of its value to natural language pro-cessing, various text segmentation tasks havebeen automated such as topical segmentation?for which a variety of automatic segmenters exist(e.g., Hearst 1997, Malioutov and Barzilay 2006,Eisenstein and Barzilay 2008, and Kazantseva andSzpakowicz 2011).
This work addresses how tobest select an automatic segmenter and which seg-mentation metrics are most appropriate to do so.To select an automatic segmenter for a particu-lar task, a variety of segmentation evaluation met-rics have been proposed, including Pk (Beefer-man and Berger, 1999, pp.
198?200), WindowDiff(WD; Pevzner and Hearst 2002, p. 10), and mostrecently Segmentation Similarity (S; Fournier andInkpen 2012, p. 154?156).
Each of these met-rics have a variety of flaws: Pk and WindowD-iff both under-penalize errors at the beginning ofsegmentations (Lamprier et al, 2007) and have abias towards favouring segmentations with few ortightly-clustered boundaries (Niekrasz and Moore,2010), while S produces overly optimistic valuesdue to its normalization (shown later).To overcome the flaws of existing text segmen-tation metrics, this work proposes a new series ofmetrics derived from an adaptation of boundaryedit distance (Fournier and Inkpen, 2012, p. 154?156).
This new metric is named boundary similar-ity (B).
A confusion matrix to interpret segmenta-tion as a classification problem is also proposed,allowing for the computation of information re-trieval (IR) metrics such as precision and recall.1In this work: ?2 reviews existing segmentationmetrics; ?3 proposes an adaptation of boundaryedit distance, a new normalization of it, a newconfusion matrix for segmentation, and an inter-1An implementation of boundary edit distance, bound-ary similarity, B-precision, and B-recall, etc.
is provided athttp://nlp.chrisfournier.ca/1702coder agreement coefficient adaptation; ?4 com-pares existing segmentation metrics to those pro-posed herein; ?5 evaluates S and B based inter-coder agreement; and ?6 compares B, S, and WDwhile evaluating automatic segmenters.2 Related Work2.1 Segmentation EvaluationMany early studies evaluated automatic seg-menters using information retrieval (IR) metricssuch as precision, recall, etc.
These metrics lookedat segmentation as a binary classification prob-lem and were very harsh in their comparisons?nocredit was awarded for nearly missing a boundary.Near misses occur frequently in segmentation?although manual coders often agree upon the bulkof where segment lie, they frequently disagreeupon the exact position of boundaries (Artsteinand Poesio, 2008, p. 40).
To attempt to overcomethis issue, both Passonneau and Litman (1993) andHearst (1993) conflated multiple manual segmen-tations into one that contained only those bound-aries which the majority of coders agreed upon.
IRmetrics were then used to compare automatic seg-menters to this majority solution.
Such a major-ity solution is unsuitable, however, because it doesnot contain actual subtopic breaks, but instead theconflation of a collection of potentially disagree-ing solutions.
Additionally, the definition of whatconstitutes a majority is subjective (e.g., Passon-neau and Litman (1993, p. 150), Litman and Pas-sonneau (1995), Hearst (1993, p. 6) each used 4/7,3/7, and > 50%, respectively).To address the issue of awarding partialcredit for an automatic segmenter nearly missinga boundary?without conflating segmentations,Beeferman and Berger (1999, pp.
198?200) pro-posed a new metric named Pk.
Pevzner and Hearst(2002, pp.
3?4) explain Pk well: a window of sizek?where k is half of the mean manual segmen-tation length?is slid across both automatic andmanual segmentations.
A penalty is awarded ifthe window?s edges are found to be in differingor the same segments within the manual segmen-tation and the automatic segmentation disagrees.Pk is the sum of these penalties over all windows.Measuring the proportion of windows in error al-lows Pk to penalize a fully missed boundary byk windows, whereas a nearly missed boundary ispenalized by the distance that it is offset.Pk was not without issue, however.
Pevznerand Hearst (2002, pp.
5?10) identified that Pk:i) penalizes false negatives (FNs)2 more than falsepositives (FPs); ii) does not penalize full misseswithin k units of a reference boundary; iii) penal-ize near misses too harshly in some situations; andiv) is sensitive to internal segment size variance.To solve Pk?s issues, Pevzner and Hearst (2002,pp.
10) proposed a modification referred to asWindowDiff (WD).
Its major difference is in howit decides to penalized windows: within a window,if the number of boundaries in the manual segmen-tation (Mij) differs from the number of bound-aries in the automatic segmentation (Aij), then apenalty is given.
The ratio of penalties over win-dows then represents the degree of error betweenthe segmentations, as in Equation 1.
This changebetter allowed WD to: i) penalize FPs and FNsmore equally;3 ii) Not skip full misses; iii) Lessharshly penalize near misses; and iv) Reduce itssensitivity to internal segment size variance.WD(M,A) = 1N ?
kN?k?i=1,j=i+k(|Mij ?Aij | > 0) (1)WD did not, however, solve all of the issuesrelated to window-based segmentation compari-son.
WD, and inherently Pk: i) Penalize er-rors less at the beginning and end of segmenta-tions (Lamprier et al, 2007); ii) Are biased to-wards favouring automatic segmentations with ei-ther few or tightly-clustered boundaries (Niekraszand Moore, 2010); iii) Calculate window size kinconsistently;4 iv) Are not symmetric5 (meaningthat they cannot be used to produce a pairwisemean of multiple manual segmentations6).Segmentation Similarity (S; Fournier andInkpen 2012, pp.
154?156) took a different ap-proach to comparing segmentations.
Instead of us-ing windows, the work proposes a new restrictededit distance called boundary edit distance whichdifferentiates between full and near misses.
S then2I.e., a boundary present in the manual but not the auto-matic segmentation, and the reverse for a false positive.3Georgescul et al (2006, p. 48) noted that WD interpretsa near miss as a FP probabilistically more than as a FN.4k must be an integer, but half of a mean may be a frac-tion, thus rounding must be used, but no rounding methodis specified.
It is also not specified whether k should be setonce during a study or recalculated for each comparison?this work assumes the latter.5Window size is calculated only upon the manual segmen-tation, meaning that one must be a manual and other an auto-matic segmentation.6This also means that WD and Pk cannot be adapted tocompute inter-coder agreement coefficients.1703normalizes the counts of full and near misses iden-tified by boundary edit distance, as shown in Equa-tion 2, where sa and sb are the segmentations, ntis the maximum distance that boundaries may spanto be considered a near miss, edits(sa, sb, nt) is theedit distance, and pb(D) is the number of potentialboundaries in a document D (pb(D) = |D| ?
1).S(sa, sb, nt) = 1?
|edits(sa, sb, nt)|pb(D) (2)Boundary edit distance models full misses asthe addition/deletion of a boundary, and nearmisses as n-wise transpositions.
An n-wise trans-position is the act of swapping the position ofa boundary with an empty position such that itmatches a boundary in the segmentation comparedagainst (up to a spanning distance of nt).
S alsoscales the severity of a near miss by the distanceover which it is transposed, allowing it to scalethe penalty of a near misses much like WD.
S isalso symmetric, allowing it to be used in pairwisemeans and inter-coder agreement coefficients.The usage of an edit distance that supportedtranspositions to compare segmentations was anadvancement over window-based methods, butboundary edit distance and its normalization S arenot without problems, specifically: i) This edit dis-tance uses string reversals (ABCD =?
DCBA)to perform transpositions, making it cumbersometo analyse individual pairs of boundaries betweensegmentations; ii) S is sensitive to variations in thetotal size of a segmentation, leading it to favourvery sparse segmentations with few boundaries;iii) S produces cosmetically high values, makingit difficult to interpret and causing over-estimationof inter-coder agreement.
In this work, these defi-ciencies are demonstrated and a new set of metricsare proposed as replacements.2.2 Inter-Coder AgreementInter-coder agreement coefficients are used tomeasure whether a group of human judges (i.e.coders) agree with each other greater than chance.Such coefficients are used to determine the relia-bility and replicability of the coding scheme andinstructions used to collect manual codings (Car-letta, 1996).
Although direct interpretation of suchcoefficients is difficult, they are an invaluable toolwhen comparing segmentation data that has beencollected with differing labels and when estimat-ing the replicability of a study.
A variety of inter-coder agreement coefficients exist, but this workfocuses upon a selection of those discussed by Art-stein and Poesio (2008), specifically: Scott?s pi(Scott, 1955) Fleiss?
multi-pi (pi?, Fleiss 1971)7,Cohen?s ?
(Cohen, 1960), and multi-?
(?
?, Daviesand Fleiss 1982).
Their general forms are shown inEquation 3, where Aa represents actual agreement,and Ae expected (i.e., chance) agreement betweencoders.
?, pi, ?
?, and pi?
= Aa ?
Ae1?
Ae (3)When calculating agreement between manualsegmenters, boundaries are considered labels andtheir positions the decisions.
Unfortunately, be-cause of the frequency of near misses that oc-cur in segmentation, using such labels and de-cisions causes inter-coder agreement coefficientsto drastically underestimate actual agreement?much like how automatic segmenter performanceis underestimated when segmentation is treatedas a binary classification problem.
Hearst (1997,pp.
53?54) attempted to adapt pi?
to award par-tial credit for near misses by using the percentageagreement metric of Gale et al (1992, p. 254) tocompute actual agreement?which conflates mul-tiple manual segmentations together according towhether a majority of coders agree upon a bound-ary or not.
Unfortunately, such a method of com-puting agreement grossly inflates results, and ?thestatistic itself guarantees at least 50% agreementby only pairing off coders against the majorityopinion?
(Isard and Carletta, 1995, p. 63).Fournier and Inkpen (2012, pp.
154?156) pro-posed using pairwise mean S for actual agree-ment to allow inter-coder agreement coefficientsto award partial credit for near misses.
Unfor-tunately, because S produces cosmetically highvalues, it also causes inter-coder agreement coef-ficients to drastically overestimates actual agree-ment.
This work demonstrates this deficiency andproposes and evaluates a solution.3 A New Proposal for Edit-Based TextSegmentation EvaluationIn this section, a new boundary edit distance basedsegmentation metric and confusion matrix is pro-posed to solve the deficiencies of S for both seg-mentation comparison and inter-coder agreement.7Sometimes referred to as K (Siegel and Castellan, 1988).17043.1 Boundary Edit DistanceIn this section, Boundary Edit Distance (BED; asproposed in Fournier and Inkpen 2012, pp.
154?156) is introduced in more detail, and a few termi-nological and conceptual changes are made.Boundary Edit Distance uses three main edit op-erations to model segmentation differences:?
Additions/deletions (AD; referred to origi-nally as substitutions) for full misses;?
Substitutions (S; not shown for brevity) forconfusing one boundary type with another;?
n-wise transpositions (T) for near misses.These edit operations are symmetric and oper-ate upon the set of boundaries that occur at eachpotential boundary position in a pair of segmenta-tions.
An example of how these edit operations areapplied8 is shown in Figure 1, where a near miss(T), a matching pair of boundaries (M), and twofull misses (ADs) are shown with the maximumdistance that a transposition can span (nt) set to 2potential boundaries (i.e., only adjacent positionscan be transposed).s1 2 4 4 4s2 3 3 6 2TMADADFigure 1: Boundary edit operationsIn Figure 1, the location of the errors is clearlyshown.
Importantly, however, pairs of boundariesbetween the segmentations can be seen that rep-resent the decisions made, and the correctness ofthese decisions.
Imagine that s1 is a manual seg-mentation, and s2 is an automatic segmenter?s hy-pothesis.
The transposition is a partially correctdecision, or boundary pair.
The match is a correctboundary pair.
The additions/deletions, however,could be one of two erroneous decisions: to notplace an expected boundary (FN), or to place a su-perfluous boundary (FP).9This work proposes assigning a correctnessscore for each boundary pair/decision (shown inTable 1) and then using the mean of this score asa normalization of boundary edit distance.
Thisinterpretation intuitively relates boundary edit dis-tance to coder judgements, making it ideal for8A complete explanation of Boundary Edit Distance is de-tailed in Fournier (2013, Section 4.1.2).9Also note that the ADs are close together, and if nt > 2,then they would be considered a T, and not two ADs?this isone way to award partial credit for near misses.calculating actual agreement in inter-coder agree-ment coefficients and comparing segmentations.Pair CorrectnessMatch 1Addition/deletion 0Transposition 1?
wt span(Te, nt)Substitution 1?
ws ord(Se,Tb)Table 1: Correctness of boundary pair3.2 Boundary SimilarityThe new boundary edit distance normalizationproposed herein is referred to as boundary similar-ity (B).
Assuming that boundary edit distance pro-duces sets of edit operations where Ae is the set ofadditions/deletions, Te the set of n-wise transpo-sitions, Se the set of substitutions, and BM the setof matching boundary pairs, boundary similaritysimilarity can be defined as shown in Equation 4?one minus the incorrectness of each boundary pairover the total number of boundary pairs.B(s1, s2, nt) = 1?|Ae|+ wt span(Te, nt) + ws ord(Se,Tb)|Ae|+ |Te|+ |Se|+ |BM |(4)This form, one minus a penalty function, waschosen so that it was easier to compare againstother penalty functions considered (not shownhere for brevity).
This normalization was also cho-sen because it is equivalent to mean boundary paircorrectness and so that it ranges in value from 0 to1.
In the worst case, a segmentation comparisonwill result in no matches, no near misses, no sub-stitutions, andX full misses, i.e., |Ae| = X and allother terms in Equation 4 are zero, meaning that:B = 1?
X + 0 + 0X + 0 + 0 + 0= 1?
X/X = 1?
1 = 0In the best case, a segmentation comparison willresult in X matches, no near misses, no substitu-tions, and no full misses, i.e., |BM | = X and allother terms in Equation 4 are zero, meaning that:B = 1?
0 + 0 + 00 + 0 + 0 +X= 1?
0/X = 1?
0 = 1For all other scenarios, varying numbers ofmatches, near misses, substitutions and full misseswill result in values of B between 0 and 1.Equation 4 takes two segmentations (in any or-der), and the maximum transposition spanningdistance (nt).
This distance represents the great-est offset between boundary positions that couldbe considered a near miss and can be used to scale1705the severity of a near miss.
A variety of scalingfunctions could be used, and this work arbitrarilychooses a simple fraction to represent each trans-position?s severity in terms of its distance from itspaired boundary over nt plus a constant wt (0 bydefault), as shown in Equation 5.wt span(Te, nt) =|Te|?j=1(wt +abs(Te[j][1]?
Te[j][2])nt ?
1)(5)If multiple boundary types are used, then sub-stitution edit operations would occur when oneboundary type was confused with another.
As-signing each boundary type tb ?
Tb a number onan ordinal scale, substitutions can be weighted bytheir distance on this scale over the maximum dis-tance plus a constant ws (0 by default), as shownin Equation 6.ws ord(Se,Tb) =|Se|?j=1(ws +abs(Se[j][1]?
Se[j][2])max(Tb)?min(Tb))(6)These scaling functions allow for edit penaltiesto range from 0 to ws/t plus some linear distance.3.3 A Confusion Matrix for SegmentationThe mean correctness of each pair (i.e., B) gives anindication of just how similar one segmentation isto another, but what if one wants to identify somespecific attributes of the performance of an auto-matic segmenter?
Is the segmenter confusing oneboundary type with another, or is it very precisebut has poor recall?
The answers to these ques-tions can be obtained by looking at text segmenta-tion as a multi-class classification problem.This work proposes using a task?s set of bound-ary types (Tb) and the lack of a boundary (?
)to represent the set of segmentation classes ina boundary classification problem.
Using theseclasses, a confusion matrix (defined in Equation 7)can be created which sums boundary pair correct-ness so that information-retrieval metrics can becalculated that award partial credit to near missesby scaling edits operations.CM(a, p) =????????????
?|BM,a| + ws ord(Sa,pe , Tb)+wt span(Ta,pe , nt) if a = pws ord(Sa,pe , Tb)+wt span(Ta,pe , nt) if a 6= p|Ae,a| if p = ?|Ae,p| if a = ?
(7)An example confusion matrix is shown in Fig-ure 2 from which IR metrics such as precision, re-call, and F?-measure can be computed (referred toas B-precision, B-recall, etc.
).ActualPredicted B Non-BB CM(1, 1) CM(?, 1)Non-B CM(1,?)
CM(?,?
)Figure 2: Example confusion matrix (Tb = {1})3.4 B-Based Inter-coder AgreementFournier and Inkpen (2012, p. 156?157) adaptedfour inter-coder agreement formulations providedby Artstein and Poesio (2008) to use S to awardpartial credit for near misses, but because S pro-duces cosmetically high agreement values theygrossly overestimate agreement.
To solve thisissue, this work instead proposes using micro-average B (i.e., mean boundary pair correctnessover all documents and codings compared) tosolve this issue (demonstrated in ?5) because itdoes not over-estimate actual agreement (demon-strated in ?4 and 5).4 Discussion of Segmentation MetricsBefore analysing how each metric compares toeach other upon a large data set, it would be usefulto investigate how they act on a smaller scale.
Tothat end, this section discusses how each metric in-terprets a set of hypothetical segmentations of anexcerpt of a poem by Coleridge (1816, pp.
55?58)titled Kubla Khan (shown in Figure 3)?chosen ar-bitrarily for its brevity (and beauty).
These seg-mentations are topical and at the line-level.1.
In Xanadu did Kubla Khan2.
A stately pleasure-dome decree:3.
Where Alph, the sacred river, ran4.
Through caverns measureless to man5.
Down to a sunless sea.6.
So twice five miles of fertile ground7.
With walls and towers were girdled round:8.
And here were gardens bright with sinuous rills,9.
Where blossomed many an incense-bearing tree;10.
And here were forests ancient as the hills,11.
Enfolding sunny spots of greenery.Figure 3: Excerpt from the poem Kubla Khan (Co-leridge, 1816, pp.
55?58) with line numbersTopical segmentations of this poem are difficultto produce because there is still some structuralform (i.e., punctuation) which may dictate wherea boundary lies, but the imagery, places, times, andsubjects of the poem appear to twist and wind likea vision in a dream.
Thus, placing a topical bound-ary in this text is a highly subjective task.
Onehypothetical topical segmentation of the excerpt isshown in Figure 4.
In this section, a variety of1706contrived automatic segmentations are comparedto this manual segmentation to illustrate how eachmetric reacts to different mistakes.Lines Description1?2 Kubla Khan and his decree3?5 Waterways6?11 Fertile ground and greeneryFigure 4: A hypothetical manual segmentationAssuming that Figure 4 represents an accept-able manual segmentation (m), how would eachmetric react to an automatic segmentation (a) thatcombines the segments 1?2 and 3?5 together?This would represent a full miss, or a false neg-ative, as shown in Figure 5.
S interprets these seg-mentations as being quite similar, yet, the auto-matic segmentation is missing a boundary.
B and1?WD,10 in this case, better reflect this error.maS B 1?WD0.9 0.5 0.777?k = 2Figure 5: False negativeHow would each metric react to an automaticsegmentation that is very close to placing theboundaries correctly, but makes the slight mis-take of thinking that the segment on waterways(3?5) ends a bit too early?
This would repre-sent a near miss, as shown in Figure 6.
S and1?WD incorrectly interpret this error as beingequivalent to the previous false negative?a trou-bling result.
Segmentation comparison metricsshould be able to discern between the full and anear miss shown in these two figures, and an au-tomatic segmenter that nearly misses a boundaryshould be awarded a better score than one whichfully misses a boundary?B recognizes this andawards the near miss a higher score.maS B 1?WD0.9 0.75 0.777?k = 2Figure 6: Near missHow would each metric react to an automaticsegmentation that adds an additional boundary be-tween line 8 and 9?
This would not be idealbecause such a boundary falls in the middle ofa cohesive description of a garden, representing10WD is reported as 1?WD because WD is normally apenalty metric where a value of 0 is ideal, unlike S and B. Ad-ditionally, k = 2 for all examples in this section because WDcomputes k from the manual segmentationm, which does notchange in these examples.a full miss, or false positive, as in Figure 7.
Sand 1?WD incorrectly interpret this error as be-ing equivalent to the previous two errors?an evenmore troubling result.
In this case, there are twomatching boundaries and a pair that do not match,which is arguably preferable to the full miss andone match in Figure 5, but not to the match andnear miss in Figure 6.
B recognizes this, andawards a higher score to this automatic segmenterthan that in Figure 5, but below Figure 6.maS B 1?WD0.9 0.666?
0.777?k = 2Figure 7: False positiveHow would each metric react to an automaticsegmentation that compensates for its lack of pre-cision by spuriously adding boundaries in clustersaround where it thinks that segments should beginor end?
This is shown in Figure 8.
This kind ofbehaviour is finally penalized differently by S and1?WD (unlike the other errors shown in this sec-tion), but it only barely results in a dip in their val-ues.
B also penalizes this behaviour, but does somuch more harshly?in B?s interpretation, this isas egregious as committing a false negative (e.g.,Figure 5)?an arguably correct interpretation, ifthe evaluation desires to maximize similarity witha manual segmentation.maS B 1?WD0.8 0.5 0.666?k = 2Figure 8: Cluster of false positivesThese short demonstrations of how S, B, and1?WD interpret error should lead one to con-clude that: i) WD can penalize near misses tothe same degree as full misses?overly harshly;ii) Both S and WD are not very discriminatingwhen small segments are analysed; and iii) B isthe only one of the three metrics that is able tooften discriminate between these situations.
B, ifused to rank these automatic segmenters, wouldrank them from best to worst performing as: thenear miss, false positive, and then a tie betweenthe false negative and cluster of false positives?areasonable ranking in the context of an evaluationseeking similarity with a manual segmentation.5 Segmentation AgreementHaving a bit more confidence in B compared to Sand WD on a small scale from the previous sec-17070.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8P(miss) while P(near) = 0.09210.750.800.850.900.951.00pi?valueusingS2345678910(a) S-based pi?
showing increasing fullmisses with constant near misses0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8P(miss) while P(near) = 0.09210.20.40.60.81.0pi?valueusingB b2345678910(b) B-based pi?
showing increasing fullmisses with constant near misses2 3 4 5 6 7 8 9 10Coders (quantity)0.00.20.40.60.81.0pi?valueusingB bS B(c) S and B based pi?
with fully randomsegmentationsFigure 9: Artificial data sets illustrating how pi adapted to use either S or B reacts to increasing fullmisses and random segmentations and varying numbers of coderstion, it makes sense to analyse some larger datasets.
Two such data sets are The Stargazer dataset collected by Hearst (1997) and The Moonstonedata set collected by Kazantseva and Szpakowicz(2012).
Both are linear topical segmentations atthe paragraph level with only one boundary type,but that is where their similarities end.The Stargazer text is a science magazine articletitled ?Stargazers look for life?
(Baker, 1990) seg-mented by 7 coders and was one of twelve articleschosen for its length (between 1,800 and 2,500words) and for having little structural demarca-tion.
?The Moonstone?
is a 19th century romancenovel by Collins (1868) segmented by 4?6 codersper chapter; of its 23 chapters, 2 were coded in apilot study and another 20 were coded individuallyby 27 undergraduate English students in 5 groups.For the Stargazer data set, using S-based pi?,an inter-coder agreement coefficient of 0.7562 isobtained?a reasonable level by content analysisstandards.
Unfortunately, this value is highly in-flated, and B-based pi?
gives a much more conser-vative coefficient at 0.4405.
For the Moonstonedata set, the agreement coefficients for each groupof 4?6 coders using S-based pi?
is again over-inflated at 0.91, 0.92, 0.90, 0.94, 0.83.
B-basedpi?
instead reports that the coefficients should be0.20, 0.18, 0.40, 0.38, 0.23.Which of these coefficients should be trusted?Is agreement in these data sets high or low?
Tohelp answer that, this work looks at how the codersin the data sets behaved.
If the segmenters in theMoonstone data set truly agreed with each other,then they should have all behaved similarly.
Onemeasure of coder behaviour is the frequency thatthey placed boundaries (normalized by their op-portunity to place boundaries, i.e.
the sum of thepotential boundaries in the chapters that each seg-mented).
This normalized frequency is shown per1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27Coder0.000.050.100.150.200.25BoundariesperpotentialboundaryFigure 11: Normalized boundaries placed by eachcoder in the Moonstone data set (with mean?SD)coder in Figure 11 for The Moonstone data set,along with bars indicating the mean and one stan-dard deviation above and below.
As can be seen,the coders fluctuated wildly in the frequency withwhich they placed boundaries?some (e.g., coder7) to degrees exceeding 2 standard deviations.
TheMoonstone data set as a whole does not exhibitcoders who behaved similarly, supporting the as-sertion by B-based pi?
that these coders do notagree well (though pockets of agreement exist).How can it be demonstrated that S-basedagreement over-estimates agreement, and B-basedagreement does not?
One way to demonstrate thisis through simulation.
By estimating parametersfrom the large Moonstone data set such as the dis-tribution of internal segment sizes produced by allcoders, a random segmentation of the novel withsimilar characteristics can be created.
From thissingle random segmentation, other segmentationcan be created with a probability of either placingan offset boundary (i.e., a near miss) or placingan extra/omitting a boundary (i.e., a full miss)?a pseudo-coding.
Manipulating these probabilitiesand keeping the probability of a near miss at a con-stant natural level should produce a slowly declin-1708Random Human BayesSeg APS MinCutAutomatic segmenter0.800.820.840.860.880.900.920.94S?valuen = 90 n = 90 n = 90 n = 90 n = 90(a) SRandom Human BayesSeg APS MinCutArtificial Segmenter0.20.30.40.50.6B bmeanand95%CIsn = 1057 n = 841 n = 964 n = 738 n = 871(b) BRandom Human BayesSeg APS MinCutAutomatic segmenter0.350.400.450.500.550.600.650.700.751?WD?valuen = 90 n = 90 n = 90 n = 90 n = 90(c) 1?WDFigure 10: Mean performance of 5 segmenters using varying metrics with 95% confidence intervalsing amount of agreement which is unaffected bythe number of pseudo-coders.
This is not appar-ent, however, for S-based pi?
in Figure 9a; as theprobability of a full miss increases, agreement ap-pears to rise and varies depending upon the num-ber of pseudo-coders.
B-based pi?
however showsdeclining agreement and little to no variation de-pending upon the number of pseudo-coders, asshown in Figure 9b.If instead of creating pseudo-coders from a ran-dom segmentation a series of random segmenta-tions with the same parameters were generated, aproperly functioning inter-coder agreement coef-ficient should report some agreement (due to thesimilar parameters used to create the segmenta-tions) but it should be quite low.
Figure 9c showsthis, and that S-based pi?
drastically over-estimateswhat should be very low agreement whereas B-based pi?
properly reports low agreement.From these demonstrations, it is evident thatS-based inter-coder agreement coefficients dras-tically over-estimate agreement, as does S itselfin pairwise mean form.
B-based coefficients,however, properly discriminate between levels ofagreement regardless of the number of coders anddo not over-estimate.6 Evaluation of Automatic SegmentersHaving looked at how S, WD, and B perform ata small scale in ?4 and on larger data set in ?5,this section demonstrates the use of these met-rics to evaluate some automatic segmenters.
Threeautomatic segmenters were trained?or had theirparameters estimated upon?The Moonstone dataset, including MinCut; (Malioutov and Barzilay,2006), BayesSeg; (Eisenstein and Barzilay, 2008),and APS (Kazantseva and Szpakowicz, 2011).To put this evaluation into context, an upper andlower bound were also created comprised of a ran-dom coder from the manual data (Human) and arandom segmenter (Random), respectively.
Theseautomatic segmenters, and the upper and lowerbounds, were created, trained, and run by anotherresearcher (Anna Kazantseva) with their labels re-moved during the development of the metrics de-tailed herein (to improve the impartiality of theseanalyses).
An ideal segmentation evaluation met-ric should, in theory, place the three automatic seg-menters between the upper and lower bounds interms of performance if the metrics, and the seg-menters, function properly.The mean performance of the upper and lowerbounds upon the test set of the Moonstone dataset using S, B, and WD are shown in Figure 10a?10c along with 95% confidence intervals.
Despitethe difference in the scale of their values, both Sand WD performed almost identically, placing thethree automatic segmenters between the upper andlower bounds as expected.
For S, statistically sig-nificant differences11 (?
= 0.05) were found be-tween all segmenters except between APS?humanand MinCut?BayesSeg, and WD could only findsignificant differences between the automatic seg-menters and the upper and lower bounds.
B, how-ever, shows a marked deviation, and places Min-Cut and APS statistically significantly below therandom baseline with only BayesSeg between theupper and lower bounds?to a significant degree.Why would pairwise mean B act in such anunexpected manner?
The answer lies in a fur-ther analysis using the confusion matrix proposedearlier to calculate B-precision and B-recall (asshown in Table 2).
From the values in Table 2,all three automatic segmenters appear to have B-precision above the baseline and below the upperbound, but the B-recall of both APS and MinCutis below that of the random baseline (illustrated11Using Kruskal-Wallis rank sum multiple comparisontests (Siegel and Castellan, 1988, pp.
213-214) for S andWD and the Wilcoxon-Nemenyi-McDonald-Thompson test(Hollander and Wolfe, 1999, p. 295) for B.1709B n B-P B-R B-F1 TP FP FN TNRandom 0.2640?
0.0129 1057 0.3991 0.4673 0.4306 279.0 420 318 4236.0Human 0.5285?
0.0164 841 0.6854 0.7439 0.7135 444.5 204 153 4451.5BayesSeg 0.3745?
0.0146 964 0.5247 0.6224 0.5694 361.0 327 219 4346.0APS 0.2873?
0.0163 738 0.6773 0.3403 0.4530 212.0 101 411 4529.0MinCut 0.2468?
0.0141 871 0.4788 0.3496 0.4041 215.0 234 400 4404.0Table 2: Mean performance of 5 segmenters using micro-average B, B-precision (B-P), B-recall (B-R),and B-F?-measure (B-F1) along with the associated confusion matrix values for 5 segmenters0.0 0.2 0.4 0.6 0.8 1.0B?
recall0.00.20.40.60.81.0B?precisionRandomHumanBayesSegAPSMinCutFigure 12: Mean B-precision versus B-recall of 5automatic segmentersin Figure 12).
These automatic segmenters weredeveloped and performance tuned using WD, thusit would be expected that they would perform asthey did according to WD, but the evaluation usingB highlights WD?s bias towards sparse segmenta-tions (i.e., those with low B-recall)?a failing thatS also appears to share.
Mean B shows an un-biased ranking of these automatic segmenters interms of the upper and lower bounds.
B, then,should be preferred over S and WD for an un-biased segmentation evaluation that assumes thatsimilarity to a human solution is the best measureof performance for a task.7 ConclusionsIn this work, a new segmentation evaluation met-ric, referred to as boundary similarity (B) isproposed as an unbiased metric, along with aboundary-edit-distance-based (BED-based) con-fusion matrix to compute predictably biased IRmetrics such as precision and recall.
Additionally,a method of adapting inter-coder agreement coef-ficients to award partial credit for near misses isproposed that uses B as opposed to S for actualagreement so as to not over-estimate agreement.B overcomes the cosmetically high values of Sand, the bias towards segmentations with few ortightly-clustered boundaries of WD?manifestingin this work as a bias towards precision over recallfor both WD and S. When such precision is desir-able, however, B-precision can be computed froma BED-based confusion matrix, along with otherIR metrics.
WD and Pk should not be preferredbecause their biases do not occur consistently inall scenarios, whereas BED-based IR metrics offerexpected biases built upon a consistent, edit-based,interpretation of segmentation error.B also allows for an intuitive comparison ofboundary pairs between segmentations, as op-posed to the window counts of WD or the sim-plistic edit count normalization of S. When an un-biased segmentation evaluation metric is desired,this work recommends the usage of B and the useof an upper and lower bound to provide context.Otherwise, if the evaluation of a segmentation taskrequires some biased measure, the predictable biasof IR metrics computed from a BED-based con-fusion matrix is recommended.
For all evalua-tions, however, a justification for the biased/un-biased metrics used should be given, and morethan one metric should be reported so as to allowa reader to ascertain for themselves whether a par-ticular automatic segmenter?s bias in some manneris cause for concern or not.8 Future WorkFuture work includes adapting this work to anal-yse hierarchical segmentations and using it to at-tempt to explain the low inter-coder agreement co-efficients reported in topical segmentation tasks.AcknowledgementsI would like to thank Anna Kazantseva for her in-valuable feedback and data.
Additionally, I wouldlike to thank my thesis committee members?StanSzpakowicz, James Green, and Xiaodan Zhu?fortheir feedback along with my supervisor DianaInkpen and colleague Martin Scaiano.1710ReferencesArtstein, Ron and Massimo Poesio.
2008.
Inter-coder agreement for computational linguistics.Computational Linguistics 34(4):555?596.Baker, David.
1990.
Stargazers look for life.
SouthMagazine 117:76?77.Beeferman, Doug and Adam Berger.
1999.
Sta-tistical models for text segmentation.
MachineLearning 34:177?210.Carletta, Jean.
1996.
Assessing Agreement onClassification Tasks: The Kappa Statistic.
Com-putational Linguistics 22(2):249?254.Chang, Pi-Chuan, Michel Galley, and Christo-pher D. Manning.
2008.
Optimizing Chineseword segmentation for machine translation per-formance.
In Proceedings of the Third Work-shop on Statistical Machine Translation.
Asso-ciation for Computational Linguistics, Strouds-burg, PA, USA, pages 224?232.Cohen, Jacob.
1960.
A Coefficient of Agreementfor Nominal Scales.
Educational and Psycho-logical Measurement 20:37?46.Coleridge, Samuel Taylor.
1816.
Christabel,Kubla Khan, and the Pains of Sleep.
John Mur-ray.Collins, Wilkie.
1868.
The Moonstone.
TinsleyBrothers.Davies, Mark and Joseph L. Fleiss.
1982.
Measur-ing agreement for multinomial data.
Biometrics38:1047?1051.Eisenstein, Jacob.
2009.
Hierarchical text seg-mentation from multi-scale lexical cohesion.In Proceedings of Human Language Technolo-gies: The 2009 Annual Conference of the NorthAmerican Chapter of the Association for Com-putational Linguistics.
Association for Com-putational Linguistics, Stroudsburg, PA, USA,pages 353?361.Eisenstein, Jacob and Regina Barzilay.
2008.Bayesian unsupervised topic segmentation.
InProceedings of the 2008 Conference on Em-pirical Methods in Natural Language Process-ing.
Association for Computational Linguistics,Morristown, NJ, USA, pages 334?343.Fleiss, Joseph L. 1971.
Measuring nominal scaleagreement among many raters.
PsychologicalBulletin 76:378?382.Fournier, Chris and Diana Inkpen.
2012.
Segmen-tation Similarity and Agreement.
In Proceed-ings of Human Language Technologies: The2012 Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics.
Association for Computational Lin-guistics, Stroudsburg, PA, USA, pages 152?161.Fournier, Christopher.
2013.
Evaluating Text Seg-mentation.
Master?s thesis, University of Ot-tawa.Franz, Martin, J. Scott McCarley, and Jian-MingXu.
2007.
User-oriented text segmentation eval-uation measure.
In Proceedings of the 30th An-nual International ACM SIGIR Conference onResearch and Development in Information Re-trieval.
Association for Computing Machinery,Stroudsburg, PA, USA, pages 701?702.Gale, William, Kenneth Ward Church, and DavidYarowsky.
1992.
Estimating upper and lowerbounds on the performance of word-sense dis-ambiguation programs.
In Proceedings ofthe 30th Annual Meeting of the Associationfor Computational Linguistics.
Association forComputational Linguistics, Stroudsburg, PA,USA, pages 249?256.Georgescul, Maria, Alexander Clark, and SusanArmstrong.
2006.
An analysis of quantita-tive aspects in the evaluation of thematic seg-mentation algorithms.
In Proceedings of the7th SIGdial Workshop on Discourse and Dia-logue.
Association for Computational Linguis-tics, Stroudsburg, PA, USA, pages 144?151.Haghighi, Aria and Lucy Vanderwende.
2009.Exploring content models for multi-documentsummarization.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Confer-ence of the North American Chapter of the As-sociation for Computational Linguistics.
Asso-ciation for Computational Linguistics, Strouds-burg, PA, USA, NAACL ?09, pages 362?370.Hearst, Marti A.
1993.
TextTiling: A QuantitativeApproach to Discourse.
Technical report, Uni-versity of California at Berkeley, Berkeley, CA,USA.Hearst, Marti A.
1997.
TextTiling: SegmentingText into Multi-paragraph Subtopic Passages.Computational Linguistics 23:33?64.Hollander, Myles and Douglas A. Wolfe.
1999.1711Nonparametric Statistical Methods.
John Wi-ley & Sons, 2nd edition.Isard, Amy and Jean Carletta.
1995.
Replicabilityof transaction and action coding in the map taskcorpus.
In AAAI Spring Symposium: EmpiricalMethods in Discourse Interpretation and Gen-eration.
pages 60?66.Kazantseva, Anna and Stan Szpakowicz.
2011.Linear Text Segmentation Using Affinity Prop-agation.
In Proceedings of the 2011 Conferenceon Empirical Methods in Natural LanguageProcessing.
Association for Computational Lin-guistics, Edinburgh, Scotland, UK., pages 284?293.Kazantseva, Anna and Stan Szpakowicz.
2012.Topical Segmentation: a Study of Human Per-formance.
In Proceedings of Human LanguageTechnologies: The 2012 Annual Conferenceof the North American Chapter of the Asso-ciation for Computational Linguistics.
Associ-ation for Computational Linguistics, Strouds-burg, PA, USA, pages 211?220.Lamprier, Sylvain, Tassadit Amghar, BernardLevrat, and Frederic Saubion.
2007.
On eval-uation methodologies for text segmentation al-gorithms.
In Proceedings of the 19th IEEE In-ternational Conference on Tools with ArtificialIntelligence.
IEEE Computer Society, Washing-ton, DC, USA, volume 2, pages 19?26.Litman, Diane J. and Rebecca J. Passonneau.1995.
Combining multiple knowledge sourcesfor discourse segmentation.
In Proceedingsof the 33rd Annual Meeting of the Associationfor Computational Linguistics.
Association forComputational Linguistics, Stroudsburg, PA,USA, pages 108?115.Malioutov, Igor and Regina Barzilay.
2006.
Min-imum cut model for spoken lecture segmen-tation.
In Proceedings of the 21st Interna-tional Conference on Computational Linguis-tics and the 44th annual meeting of the Asso-ciation for Computational Linguistics.
Associ-ation for Computational Linguistics, Strouds-burg, PA, USA, pages 25?32.Niekrasz, John and Johanna D. Moore.
2010.
Un-biased discourse segmentation evaluation.
InProceedings of the IEEE Spoken LanguageTechnology Workshop, SLT 2010.
IEEE 2010,pages 43?48.Oh, Hyo-Jung, Sung Hyon Myaeng, and Myung-Gil Jang.
2007.
Semantic passage segmentationbased on sentence topics for question answer-ing.
Information Sciences 177(18):3696?3717.Passonneau, Rebecca J. and Diane J. Litman.1993.
Intention-based segmentation: humanreliability and correlation with linguistic cues.In Proceedings of the 31st Annual Meetingof the Association for Computational Linguis-tics.
Association for Computational Linguistics,Stroudsburg, PA, USA, pages 148?155.Pevzner, Lev and Marti A. Hearst.
2002.
A cri-tique and improvement of an evaluation metricfor text segmentation.
Computational Linguis-tics 28:19?36.Reynar, Jeffrey C. and Adwait Ratnaparkhi.
1997.A maximum entropy approach to identifyingsentence boundaries.
In Proceedings of the5th Conference on Applied Natural LanguageProcessing.
Association for Computational Lin-guistics, Stroudsburg, PA, USA, pages 16?19.Scott, William A.
1955.
Reliability of contentanalysis: The case of nominal scale coding.Public Opinion Quarterly 19:321?325.Siegel, Sidney and N. J. Castellan.
1988.
Non-parametric Statistics for the Behavioral Sci-ences, McGraw-Hill, New York, USA, chapter9.8.
2nd edition.Sirts, Kairit and Tanel Aluma?e.
2012.
A Hierar-chical Dirichlet Process Model for Joint Part-of-Speech and Morphology Induction.
In Proceed-ings of Human Language Technologies: The2012 Annual Conference of the North AmericanChapter of the Association for ComputationalLinguistics.
Association for Computational Lin-guistics, Stroudsburg, PA, USA, pages 407?416.Stoyanov, Veselin and Claire Cardie.
2008.
Topicidentification for fine-grained opinion analysis.In Proceedings of the 22nd International Con-ference on Computational Linguistics.
Associ-ation for Computational Linguistics, Strouds-burg, PA, USA, pages 817?824.1712
