Automatic Acquisition of Hierarchical Transduction Modelsfor Machine TranslationHiyan  A lshawi  Sr in ivas  Bangalore Shona DouglasAT&T Labs Research180 Park Avenue, P.O.
Box 971F lorham Park, NJ 07932 USAAbstractWe describe a method for the fully automaticlearning of hierarchical finite state translationmodels.
The input to the method is transcribedspeech utterances and their corresponding hu-man translations, and the output is a set ofhead transducers, i.e.
statistical exical head-outward transducers.
A word-alignment func-tion and a head-ranking function are first ob-tained, and then counts are generated for hy-pothesized state transitions of head transduc-ers whose lexical translations and word orderchanges are consistent with the alignment.
Themethod has been applied to create an English-Spanish translation model for a speech trans-lation application, with word accuracy of over75% as measured by a string-distance ompari-son to three reference translations.1 In t roduct ionThe fully automatic onstruction of translationlnodels offers benefits in terms of developmenteffort and potentially in robustness over meth-ods requiring hand-coding of linguistic informa-tion.
However, there are disadvantages to theautomatic approaches proposed so far.
The var-ious methods described by Brown et.
al (1990;1993) do not take into account he natural struc-turing of strings into phrases.
Example-basedtranslation, exemplified by the work of Sumitaand Iida (1995), requires very large amountsof training material.
The number of statesin a simple finite state model such as thoseused by Vilar et al (1996) becomes extremelylarge when faced with languages with large wordorder differences.
The work reported in Wu(1997), which uses an inside-outside type oftraining algorithm to learn statistical context-free transduction, has a similar motivation tothe current work, but the models we describehere, being fully lexical, are more suitable fordirect statistical modelling.In this paper, we show that both the net-work topology and parameters of a head trans-ducer translation model (Alshawi, 1996b) canbe learned fully automatically from a bilingualcorpus.
It has already been shown (Alshawi etal., 1997) that a head transducer model withhand-coded structure can be trained to give bet-ter accuracy than a comparable transfer-basedsystem, with smaller model size, computationalrequirements, and development effort.We have applied the learning method to cre-ate an English-Spanish translation model for alimited domain, with word accuracy of over 75%measured by a string distance comparison (asused in speech recognition) to three referencetranslations.
The resulting translation modelhas been used as a component of an English-Spanish speech translation system.We first present the steps of the transduc-tion training method in Section 2.
In Section 3we describe how we obtain an alignment func-tion from source word subsequences to targetword subsequences for each transcribed utter-ance and its translation.
The construction ofstates and transitions is specified in Section 4;the method for selecting phrase head words isdescribed in Section 5.
The string comparisonevaluation metric we use is described in Sec-tion 6, and the results of testing the method ina limited domain of English-Spanish translationare reported in Section 7.2 Overv iew2.1 Lexical  head t ransducersIn our training method, we follow the simplelexical head transduction model described byAlshawi (1996b) which can be regarded as atype of statistical dependency grammar trans-41duction.
This type of transduction model con-sists of a collection of head transducers; the pur-pose of a particular transducer is to translatea specific source word w into a target word v,and further to translate the pair of sequences ofdependent words to the left and right of w tosequences of dependents to the left and right ofc.
When applied recursively, a set of such trans-ducerb effects a hierarchical transduction of thesource string into the target string.A distinguishing property of head transduc-ers, as compared to 'standard' finite state trans-ducers is that they perform a transduction out-wards from a 'head' word in the input stringrather than by traversing the input string fromleft to right.
A head transducer for translatingsource word tt, to target word v consists of a setof states qo(w : v),ql(w : v),q~(w : v ) , .
.
,  andtransitions of the form:(qi(w : v), qj(w : v), Wd, Vd, a', ~)where the transition is from s tate  qi(w : v) tostate qj(w : v), reading the next source depen-dent Wd at position c~ relative to w and writinga target dependent vd at position fi relative tov.
Positions left of a head (in the source or tar-get) are indicated with negative integers, whilethose right of the head are indicated with posi-tive integers.The head transducers we use also include thefollowing probability parameters for start, tran-sition, and stop events:P(start ,  q(w : v)\]w)P(qj (w: v), Wd, Vd, a', fllqi(w : v))P(stoplq(w : v))In the present work, when a model is ap-plied to translate a source sentence, the cho-sen derivation of the target string is the deriva-tion that maximizes the product of the abovetransducer event probabilities.
The transduc-tion search algorithm we use to apply the trans-lation model is a bottom-up dynamic program-ruing algorithm similar to the analysis algorithmfor relational head acceptors described by A1-shawl (1996a).2.2 Tra in ing methodThe training method is organized into two mainstages, an alignment stage followed by a trans-ducer construction stage as shown in Figure 1.I ... wt .
.
.
w" w .
.
.w~.
.
.I f (w)  ... f(w ) ...
I ... f(w ) ...Figure 2: Partitioning the source and targetaround a head w with respect o fThe single input to the training process is abitext corpus, constructed by taking each ut-terance in a corpus of transcribed speech andhaving it manually translated.
We use the termbitext in what follows to refer to a pair consist-ing of the transcription of a single utterance andits translation.The steps in the training procedure are as fol-lows:1.
For each bitext, compute an alignment func-tion f from source words to target words, usingthe method described in Section 3.2.
Partition the source into a head word w andsubstrings to the left and right of w (as shownin Figure 2).
The extents of the partitions pro-jected onto the target by f must not overlap.Any selection of the head satisfying this con-straint is valid but the selection method usedinfluences accuracy (Section 5).3.
Continue partitioning the left and right sub-strings recursively around sub-heads wl and wr.4.
Trace hypothesized head-transducer transi-tions that would output the translations of theleft and right dependents of w (i.e.
wl and wr)at the appropriate positions in the target string,indicated by f .
This step is described in moredetail below in Section 4.5.
Apply step 4 recursively to partitions headedby wl and wr, and then their dependents, untilall left and right partitions have at most oneword.6.
Aggregate hypothesized transitions to formthe counts of a maximum likelihood head trans-duction model.The recursive partioning of the source and tar-get strings gives the hierarchical decompositionfor head transduction.
In step 2, the constraint42bitexts bitexts bitexts source textPairingExtractioneventtraceModel BuilderalignmentmodelAlignmentSearchalignments - -HeadSelectionrankedheadsvTransducer IConstructioneventt raceModel BuildertranslationmodelI TransductionV l Searchtranslated textFigure 1: Head transducer training methodon target partitions ensures that the transduc-tion hypothesized in training does not containcrossing dependency structures in the target.3 A l ignmentThe first stage in the training process is ob-taining, for each bitext, an a l ignment  funct ionf : W ~ V mapping word subsequences W inthe source to word subsequences V in the tar-get.
In this process an alignment model is con-structed which specifies a cost for each pairing(W, V) of source and target subsequences, andall alignment search is carried out to minimizethe sum of the costs of a set of pairings whichcompletely maps the bitext source to its target.3.1 A l ignment  mode lThe cost of a pairing is composed of a weightedcombination of cost functions.
We currently usetwo.The first cost function is the ?
correlationmeasure (cf the use of ?2 in Gale and Church(1991)) computed as follows:=(bc -  ad)x/(a + b)(c + d)(a + c)(b + d)wherea = nv  -- n~,i~vb = nw, yc = N - nv  - nw + nw,  vd = nw - nw,  vN is the total number of bitexts, nv  the numberof bitexts in which V appears in the target, nwthe number of bitexts in which W appears inthe source, and nw,  y the number of bitexts inwhich W appears in the source and V appearsin the target.We tried using the log probabilities of tar-get subsequences given source subsequences (cfBrown et al (1990)) as a cost function insteadof ?
but ?
resulted in better performance of ourtranslation models.The second cost function used is a distancemeasure which penalizes pairings in which thesource subsequence and target subsequence arein very different positions in their respectivesentences.
Different weightings of distance tocorrelation costs can be used to bias the modeltowards more or less parallel alignments for dif-ferent language pairs.433.2 A l ignment  searchThe agenda-based alignment search makes useof dynamic programming to record the best costseen for all partial alignments covering the samesource and target subsequence; partial align-ments coming off the agenda that have a highercost for the same coverage are discarded andtake 11o further part in the search.
An effortlimit on the number of agenda items processed isused to ensure reasonable speed in the search re-gardless of sentence length.
An iterative broad-ening strategy is used, so that at breadth i onlythe i lowest cost pairings for each source subse-quence are allowed in the search, with the resultthat most optimal alignments are found well be-fore the effort limit is reached.In the experiment reported in Section 7,source and target subsequences of lengths 0, 1and 2 were allowed in pairings.4 T ransducer  const ruct ionBuilding a head transducer involves creating ap-propriate head transducer states and tracing hy-pothesized head-transducer transitions betweenthem that are consistent with the occurrenceof the pairings (W, f (W))  in each aligned bi-text.
When a source sequence W in an align-ment pairing consists of more than one word,the least frequent of these words in the train-ing corpus is taken to be the primary word ofthe subsequence.
It is convenient to extend thedomain of an alignment function f to includeprimary words w by setting f(w) = f(W).The main transitions that are traced in ourconstruction are those that map heads, wl andwr, of the the right and left dependent phrasesof w (see Figure 2) to their translations as indi-cated in the alignment.
The positions of thesedependents in the target string are computedby comparing the positions of f(wt) and f(wr)to the position of l: = f(w).
The actual statesand transitions in the construction are specifiedbelow.Additional transitions are included for casesof compounding, i.e.
those for which the sourcesubsequence in an alignment function pairingconsists of more than one word.
Specifically,the source subsequence W may be a compoundconsisting of a primary word w together witha secondary word w'.
There are no additionaltransitions for cases in which the target subse-quence V = f(w) of an alignment function pair-ing has more than one word.
For the purposes ofthe head-transduction model constructed, suchcompound target subsequences are effectivelytreated as single words (containing space char-acters).
That is, we are constructing a tran-ducer for (w : V).We use the notation Q(w : V) for states ofthe constructed head transducer.
Here Q is anadditional symbol e.g.
"initial" for identifyinga specific state of this transducer.
A state suchas initial(w : V) mentioned in the constructionis first looked up in a table of states createdso far in the training procedure; and created ifnecessary.
A bar above a substring denotes thenumber of words preceding the substring in thesource or target string.We give the construction for the case illus-trated in Figure 2, i.e.
one left dependent wt,one right dependent wr, and a single secondaryword w' to the left of w. Figure 3 shows theresult as part of a finite state transition dia-gram.
The other transition arrows shown in thediagram will arise from other bitext alignmentscontaining (w : V) pairings.
Other cases cov-ered by our algorithm (e.g.
a single left depen-dent but no right dependent) are simple vari-ants.Wt :(.w, f(wt)v)-1 :0~) left~,(w : V)-1:31) raid~ I(w V)+t:32)) :i.az(w : v)Figure 3: States and transitions constructed forthe partition shown in Figure 21.
Mark initial(w : V) as an initial state forthe transducer.2.
Include a transition consuming the secondary44word w t without any target output:(initial(w: V), leftw,(W : V), w', e, -1,  0),where e is the empty string.3.
Include a transition for mapping the sourcedependent wl to the target dependent f(wt):(le ftw,(w : V), midw~(w : V), wt, f(wl), -1,/31)where 13l = f(wt) - V.4.
Include a transition for mapping the sourcedependent wr to the target dependent f(w~):(midw,(w : V), f inal (w : V), w~, f(wr), +l,/3r)where/3,.
= f(w,.)
- Y..5.
Mark f inal (w : 1 I) as a final state for thetransducer.The inclusion of transitions, and the markingof states as initial or final, are treated as eventobservation counts for a statistical head trans-duction model.
More specifically, they are usedas counts for maximum likelihood estimation ofthe transducer start, transition, and stop prob-abilities specified in Section 2.5 Head se lec t ionWe have been using the following monolingualmetrics which can be applied to either thesource or target language to predict the likeli-hood of a word being the head word of a string.Distance: The distance between a dependentand its head.
In general, the likelihood of ahead-dependent relation decreases as distanceincreases (Collins, 1996).Word frequency: The frequency of occurrenceof a word in the training corpus.IVord 'complezity': For languages with pho-netic orthography such as English, 'complexity'of a word can be measured in terms of numberof characters in that word.Optionality: This metric is intended to iden-tify optional modifiers which are less likely tobe heads.
For each word we find trigrams withthe word of interest as the middle word andcompare the distribution of these trigrams withthe distribution of the bigrams formed from theouter pairs of words.
If these two distributionsare strongly correlated then the word is highlyoptional.Each of the above metrics provides a score forthe likelihood of a word being a head word.
Aweighted sum of these scores is used to producea ranked list of head words given a string for usein step 2 of the training algorithm in Section 2.If the metrics are applied to the target languageinstead of the source, the ranking of a sourceword is taken from the ranking of the targetword it is aligned with.In Section 7, we show the effectiveness of ap-propriate head selection in terms of the trans-lation performance and size of the head trans-ducer model in the context of an English-Spanish translation system.6 Eva luat ion  methodThere is no agreed-upon measure of machinetranslation quality.
For our current purposeswe require a measure that is objective, reliable,and that can be calculated automatically.We use here the word accuracy measure ofthe string distance between a reference stringand a result string, a measure standardly usedin the automatic speech recognition (ASR) com-munity.
While for ASR the reference is a humantranscription of the original speech and the re-sult the output of the speech recognition processrun on the original speech, we use the measureto compare two different ranslations of a givensource, typically a human translation and a ma-chine translation.The string distance metric is computed byfirst finding a transformation of one string intoanother that minimizes the total weight of sub-stitutions, insertions and deletions.
(We usethe same weights for these operations as in theNIST ASR evaluation software (NIS, 1997).)
Ifwe write S for the resulting number of substi-tions, I for insertions, D for deletions, and Rfor number of words in the reference translationstring, we can express the metric as follows:word accuracy = (1 D+S+I )_RThis measure has the merit of being com-pletely automatic and non-subjective.
How-ever, taking any single translation as referenceis unrealistically unfavourable, since there is arange of acceptable translations.
To increasethe reliability of the measure, therefore, we giveeach system translation the best score it receivesagainst any of a number of independent humantranslations of the same source.45wfwsysmax source  length5 10 15 20 >2045.8 46.5 45.2 44.5 44.079.4 78.3 77.3 75.2 74.1Table 1: Word accuracy (percent) against thesingle held-out human translation7 Eng l i sh -Span ish  exper imentThe training and test data for the experimentsreported here were taken from a set of tran-scribed utterances from the air travel infor-mation system (ATIS) corpus together with atranslation of each utterance to Spanish.
Anutterance is typically a single sentence but issometimes more than one sentence spoken in se-quence.
There were 14418 training utterances,a total of 140788 source words, corresponding to167865 target words.
This training set was usedas input to alignment model construction; align-ment search was carried out only on sentencesup to length 15, a total of 11542 bitexts.
Trans-duction training (including head ranking) wascarried out on the 11327 alignments obtained.Tlle test set used in the evaluations reportedhere consisted of 336 held-out English sentences.We obtained three separate human translationsof this test set:t r l  was translated by the same translationbureau as the training data;tr2 was translated by a different ranslationbureau;c r l  was a correction of the output of thetrained system by a professional translator.The models evaluated aresys:  the automatically trained head trans-duction model;wfw: a baseline word-for-word model inwhich each English word is translated by theSpanish word most highly correlated with it inthe corpus.Table 1 shows the word accuracy percent-ages (see Section 6) for the trained system sysand the word-for-word baseline wfw against r l(the original held-out translations) at varioussource sentence lengths.
The trained systemhas word accuracy of 74.1% on sentences of alllengths; on sentences up to length 15 (the lengthon which the transduction model was trained)the score was 77.3%.max source  length5 10 15 20 >20wfw 46.2 47.5 46.6 45.8 45.3sys  80.1 81.6 81.0 79.3 78.5Table 2: Word accuracy (percent) against theclosest of three human translationsHead selectorBaseline(Random Heads)In SourceIn Target (sys)Wordaccuracy64.7%71.4%74.1%Number ofparameters108K67K66KTable 3: Translation performance with differenthead selection methodsTable 2 shows the word accuracy percentagesfor the trained system sys and the word-for-word baseline wfw against any of the three ref-erence translations t r l ,  c r l ,  and tr2.
That is,for each output string the human translationclosest to it is taken as the reference transla-tion.
With this more accurate measure, the sys-tem's word accuracy is 78.5% on sentences of alllengths.Table 3 compares the performance of thetranslation system when head words are se-lected (a) at random (baseline), (b) with headselection in the source language, and (c) withhead selection in the target language, i.e., select-ing source heads that are aligned with the high-est ranking target head words.
The reference forword accuracy here is the single reference trans-lation t r l .
Note that the 'In Target' head selec-tion method is the one used in training trans-lation model sys.
The use of head selectionmetrics improves on random head selection interms of translation accuracy and number of pa-rameters.
An interesting twist, however, is thatapplying the metrics to target strings performsbetter than applying the metrics to the sourcewords directly.8 Conc lud ing  remarksWe have described a method for learning a headtransduction model automatically from trans-lation examples.
Despite the simplicity of thecurrent version of this method, the experiment46we reported in this paper demonstrates thatthe method leads to reasonable performancefor English-Spanish translation in a limited do-main.
We plan to increase the accuracy of themodel using the kind of statistical modelingtechniques that have contributed to improve-ments in automatic learning of speech recogni-tion models in recent years.
We have startedto experiment with learning models for morechallenging language pairs such as English toJapanese that exhibit more variation in wordorder and complex lexical transformations.Re ferencesH.
Alshawi, A.L.
Buchbaum, and F. Xia.1997.
A Comparison of Head Trandsucersand Transfer for a Limited Domain Transla-tion Application.
In 35 th Annual Meeting ofthe Association for Computational Linguis-tics.
Madrid, Spain, August.H.
Alshawi.
1996a.
Head automata nd bilin-gual tiling: Translation with minimal repre-sentations.
In 34th Annual Meeting of theAssociation for Computational Linguistics,pages 167-176, Santa Cruz, California.H.
Alshawi.
1996b.
Head automata for speechtranslation.
In International Conference on.Spoken Language Processing, Philadelphia,Pennsylvania.P.J.
Brown, J. Cocke, S. Della Pietra, V. DellaPietra, J. Lafferty, R. Mercer, and P. Rossin.1990.
A Statistical Approach to Ma-chine Translation.
Computational Linguis-tics, 16(2):79-85.P.J.
Brown, S.A. Della Pietra, V.J.
Della Pietra,and R.L.
Mercer.
1993.
The mathematics ofmachine translation: Parameter estimation.Computational Linguistics, 16(2):263-312.Michael John Collins.
1996.
A new statisticalparser based on bigram lexical dependencies.In 34th Meeting of the Association for Com-putational Linguistics, pages 184-191, SantaCruz.W.A.
Gale and K.W.
Church.
1991.
Identify-iug word correspondences in parallel texts.In Proceedings of the Fourth DARPA Speechand Natural Language Processing Workshop,pages 152-157, Pacific Grove, California.National Institute of Standards and Technology,http://www.itl.nist.gov/div894, 1997.
Spo-47ken Natural Language Processing Group Webpage.Eiichiro Sumita and Hitoshi Iida.
1995.
Hetero-geneous computing for example-based trans-lation of spoken language.
In 6 th Interna-tional Conference on Theoretical nd Method-ological Issues in Machine Translation, pages273-286, Leuven, Belgium.J.M.
Vilar, V. M. Jim~nez, J.C. Amengual,A.
Castellanos, D. Llorens, and E. Vidal.1996.
Text and speech translation by meansof subsequential transducers.
Natural Lan-guage Engineering, 2(4) :351-354.Dekai Wu.
1997.
Stochastic inversion trans-duction grammars and bilingual parsing ofparallel corpora.
Computational Linguistics,23(3):377-404.
