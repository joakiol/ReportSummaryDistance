An Information-Theoretic Empirical Analysis of Dependency-BasedFeature Types for Word Prediction ModelsDekai WU *, ZHAO Jun*, SUI Zhifang **?
1" Human Language Technology CenterDepartment ofComputer ScienceUniversity of Science & Technology, HKUST, Clear Water Bay, Hong KongComputational Linguistics InstituteDepartment ofComputer Science & TechnologyPeking University, Beijing, 100871, P.R.China{dekai, zhaojun,AbstractOver the years, many proposals have beenmade to incorporate assorted types of feature inlanguage models.
However, discrepanciesbetween training sets, evaluation criteria,algorithms, and hardware nvironments make itdifficult to compare the models objectively.
Inthis paper, we take an information theoreticapproach to select feature types in a systematicmanner.
We describe a quantitative analysis ofthe information gain and the informationredundancy for various combinations of featuretypes inspired by both dependency structure andbigram structure, using a Chinese treebank andtaking word prediction as the object.
Theexperiments yield several conclusions on thepredictive value of several feature types andfeature types combinations for word prediction,which are expected to provide guidelines forfeature type selection in language modeling.1 IntroductionThere are many types of features that alanguage model can use to predict a word in asentence.
Standard n-gram models use theimmediately preceding words.
Other fixedphysical distance feature types may inspect wordclasses or parts of speech.
Grammatically-basedfeature types may also be used, such as thesuizf}@cs.ust.hkincident syntactic and semantic relations or theother words involved in those relations.
Ourultimate aim is to determine which combinationof feature types is optimal for languagemodeling.
Unfortunately, the state of knowledgein this regard is very limited.
Many languagemodels have been published inspired by one ormore of these feature types I11121131141151, butdiscrepancies between training sets, evaluationcriteria, algorithms, and hardware nvironmentsmake it difficult, if not impossible, to comparethe models objectively.
The paper uses aninformation theoretic approach to select featuretypes for language modeling in a systematicmanner.
We are concerned with quantitativeanalysis of the information quantity, informationgain and the information redundancy for variousfeature type combinations in both dependencygrammar structure and adjacent bigram structure.The experiments yield a number of conclusionson the predictive value of various feature typesand the combinations thereof, which can provideuseful information on what level of performancegain can be expected in principle from a bigrammodel augmented with long distancedependency features.
The results are expected toprovide a reliable reference for feature typeselection in language modeling.We have used Chinese data for theexperiments in this paper.
Strictly speaking, our138Iconclusions apply only to Chinese.
However, weactually expect very similar results on English,and all our preliminary experiments on Englishdata do bear this out I61.
We believe the generalmethodology as well as many of the specificconclusions apply tO a wide range of languages.We will begin by introducing an informationtheoretic framework for feature type selectionand analysis.
We then describe the experimentalsetup.
Finally, we discuss a number of claimsderiving from the eXperimental evidence.2 Framework2.1 Features for Language ModelsA language model predicts a given wordbased on its history.
By the laws of conditionalprobabilities, a language model can berepresented in left-to-right fashion asP(S) = P(wo )P(w I \[ hl ).
.
.
P(w i I hi)...P(wn \[ h,)where S denotes a sequence of words w0, w~ .. .
.
.w,, and ha denotes the history of w~ (0 < i _< n).In order to construct a language model, theindividual probabilities p(wilhi) should beestimated from the training set.
Since there aretoo many possible histories but not enoughevidence in the training set, several feature typesmust be used to divide the space of possiblehistories into equivalence classes via themap~ :h i fl,fx,'",fK )\[hi \] to make the modelfeasible in the implementation.
In speechrecognition, these feature types are most oftenfixed physical position based features, as in N-gram models.
The feature types can be the wordsbefore the predicted word or the part-of-speechof the words before the predicted word.
In orderto remedy the linguistic implausibility andinefficient usage of the training set of N-grammodels, we would like to incorporategrammatically-based feature types into thelanguage model, which could incorporate thepredictive power of words that lie outside of N-gram range tvHSl.
However, we would like to do sowithout sacrificing the known performanceadvantages of N-gram models t91.
We follow thegeneral approach of the aforementioned authorsin taking dependency grammar as a framework,since it extends N-gram models more naturallythan stochastic context-free grammars.The feature types studied in this paper arecombinations of the fixed physical distancefeatures and grammatically based features listedin Table 1 and graphically depicted in Figure 1.To understand the feature types, consider thetask of predicting "{~\[1~ (zuo4 ye4, assignment)"in the example sentence shown in Figure 2.
Wedenote this word by O, which stands for"observed".
The word bigram feature B is thenearest preceding word of O, in this case "~3~(yingl wen2, English)".
The nearest wordmodifying O is denoted by M, and is also "~:3~5(yingl wen2, English)" in this case.
Conversely,the nearest preceding word modified by O isdenoted by R, "4~ (zuo4, do)" here.
BP is thepart of speech of ":9~:3~5 (yingl wen2, English)",in this case "n(noun)".
Similarly, MP is the POSof "~3~:(yingl wen2, English)", and RP is thePOS "v(verb)" for "~i~ (zuo4, do)".
Themodifying type or dependency relation between"~:  (yingl wen2, English)" and "{~IJL (zuo4ye4, assignment)" is denoted by MT, in this case"np(noun phrase)".
RT is the modifying typebetween "~(zuo4, do)" and "{~ ~(zuo4 ye4,assignment)", here "vp(verb phrase)".Faced with so many feature types, one of thedilemmas for language modeling is whichfeature types, or feature type combinations,should be used.
The experience has shown thatthe feature types should not be selected byintuition.139BMRTable 1: The feature types used in the training setNearest preceding word BP POS of BNearest preceding word MP POS of M MT Modifying type between M andmodifying O ONearest preceding word RP POS of R RT Modifying type between R andmodified by O Ofeature type : Rfeature type : Mfeature type : R+Bfeature type : RT+BFigure-l:feature type : RTfeature type : MTfeature type:M+Bfeature type:MT+Bfeature type: R+RTfeature type:M+MTfeature type: R+Mfeature type:RT+MT/ RT~Some models using the combination of the bigram featuresand dependency-grrammar-based featureszuo2 tianl xia4 wu3 di4 di4 zaii4yesterday afternoon brother inFigure 2: An example sentence toIn order to obtain a more reliable reference toguide the addition of structural features to astochastic language model, our objective is toestablish in principle the amount of informationavailable from various long distance dependencyfeatures and feature combinations.
This can beregarded as an upper bound on the improvementthat could be obtained by augmenting a languagejiao4 shi4 zuo4 yingl wen2 zuo4 ye4classroom do English assignmentR M,B 0describe ach feature type listed in Table 2model with the corresponding features.
Weevaluate the informativeness of several featuretypes in bigram and dependency grammaticalstructure from the viewpoint of informationtheory.
The experiments draw some conclusionson which feature types should be selected orshould not be selected given specific baselineassumptions, and provide a ranking of the140feature types according to their importance fromthis viewpoint.2.2 Information-based Model for FeatureType AnalysisWe now introduce some relevant conceptsrfrom information theory that we adopt as afoundation for analyzing feature types.Information quantity (IQ).
The informationquantity of a feature type F to the predicted wordO is defined using the standard definition of?
\ [10 \ ]  average mutual ififormatmn ; we define IQ asthe average mutual information between F and O.IQ(F;O) = Ep(F?
)\[ l?g p(F~p(o)Jp(FO) 1Information gain (IG).
The information gain ofadding F 2 on top Of a baseline model that alreadyemploys Fi for predicting word O is defined asthe average mutual information between thepredicted word O and feature type F 2, given thatfeature type F~ is known.I p(FzOI E1 ) 1IG(F2;O I El) = Ee(FiF20) log p(F2 i F1)P(Oi F~)JInformation redundancy (IR).
The above twodefinitions lead naturally to a complementaryconcept of information redundancy.
IR(F1,Fz;O)denotes the redundant information between Fiand F2 in predicting O, which is defined as thedifference between IQ(F2;O) and IG(Fz;OIF1), orthe difference between IQ(F~ ;O) and IG(F~ ;OIF2).IR( F l , Fz;O)= IQ(Fz;O)-IG(F2;OI F1)= IQ(Fi;O)-IG(F\];OI F2)We shall use IG to select the feature typeseries, and use IR to analyze the overlappedidegree between the variant and the baseline.3 The Corpus Used in the ExperimentsThe training corpus used in our experiments isa treebank consisting of Chinese primary schooltexts mjt12\].
The basic statistics characterizing thetraining set are summarized in Table 2.Table 2: Statistics of the training corpusCorpus Size (words)Total Sentences(sentences)Average Sentence Length (words)Vocabulary Size(words)POS TagsPhrase Types52,6094,13912.7115,3192614In the experiments, we use 80% of the abovecorpus as a training set for estimating thevarious co-occurrence probabilities, while 10%of the corpus is used as a testing set to computethe information gain, information quantity, andinformation redundancy.
The feature types weused in the experiments are those shown inTable 1.4 Experimental Results and AnalysisOur experiments aim to quantitativelyestablish the amount of information intrinsicallypresent in each feature type, and the informationgain of each feature type on the top of variousbaselines.
We were led to a number ofconclusions on the predictive power of variousfeature types and feature types combinations,some in support of traditional linguistic intuitionand some more surprising.
These observationsprovide guidelines for language modeling.Below, we warm up with a well-knownobservation, and then move on to more focussedanalysis.4.1 Grammatically motivated featuretypes do not easily yield as muchpredictive information as simple bigrams.From a traditional linguistics viewpoint, R (thenearest preceding word modified by the141\[qiang2 sheuTg4 l gua4 zheO di4 tu2wall on hang aspectual marker mapFigure 3: The dependency grammatical structure of Chinese sentence "JC~/qiang2/wall _lJshang4/on ~-~/gua4/hang~/zheO/(aspectual m rker) ~d~\[\]/di4 tu2/map."
(There is a map hanging on the wall.
)vptal cong2 gongl yuan2 zou3he from garden walk~J ~dao4 hai3 bianlto seasideFigure 4: The dependency grammatical structure of Chinese sentence '"ft~/ta 1/he ,,~hJcong2/from ~ \[\]/gonglyuan2/garden ~.P_/zou3/walk ~ J/dao4/to ~J~_/hai3 bianl/seaside.
(He walks from the garden to the seaside.
)predicted word O) should be more significant forword prediction than the bigram predictor B (thenearest preceding word of the predicted word O).Consider the sentence showed in Figure 3,where O is "~ \[\]/di4tu2/map", B is theaspectual marker "7/zheO",  and R is "/gua41hang".
It seems somehow obvious that R("~/gua4/hang") should be more predictive for0 ("~ \[\] /di4tu2/map ") than B (the aspectualmarker "~/zheO").
However, as is well knownin speech recognition and statistical NLPresearch, the opposite turns out to be true.
Thisis corroborated by the empirical informationquantities hown in Table 3, which shows that Bhas the largest information quantity in all of thefeature types.
That bigram features outperformthe grammatically-based f atures is commonlyattributed to the predictive power of lexicalassociation.Table 3: Evidence for 4.1 (See text)IQ(B;O)=3.826 IQ(MT;O)=0.971IQ(M;O)=2.237 IQ(RT;O)=0.954IQ(R;O)= 1 .581  IQ(MP;O)=0.818IQ(BP;O)= 1 .493  IQ(RP;O)=0.663Similarly, M (the nearest preceding wordmodifying the predicted word O) should be moresignificant for word prediction than B (thenearest preceding word of the predicted word O).For example, consider the sentence showed inFigure 4, where O is "~/zou3/walk", then B is "\[\] /gong lyuan2/garden" and M is "/cong2/from".
Again, it seems that M (",hJk/cong2/from") ought to be more predictive to O(" j~ /zou3/walk") than B (" ~_~ \[\]/gonglyuan2/garden"), butfrom Table 3 we seethat the opposite is true.From a linguistic viewpoint, the explanationfor the fact that R (IQ(R;O)=l.581) is lesspredictive than B (IQ(B;O)=3.826) may be asfollows.
Within a sentence, every word has142Iexactly one B and one R feature.
But on onehand, the B feature always lies to the left of Osince it is by definition the preceding word,while on the other hand, R generally lies to theright of O in Chinese sentences (with a fewnotable except!ons such as prepositionalphrases).
When R is not in the history precedingO, it cannot be used to predict O.Similarly, a possible factor in the fact that M(IQ(M;O)=2.237) is less predictive than B is thatM sometimes lies to the right of O. Anotherfactor in the cas'e of M is that none of the leafnodes in a dependency tree have an M.4.2 A l though R (the word modif ied by thepredicted word) is less effective than M(the word modify ing the predicted word)when they are used individually for wordpredict ion, R is more effective than M ifthey are used On top of a standard b igrammodel  (the feature B).Consider the' following measurements fromour experiments': IQ(R;O)=l.581 bits which isless than IQ(M;O)=2.237 bits, whereasIG(R;OIB)=0.683 bits which is greater thanIG(M;OIB)=0.541 bits.
That is, given a baselinebigram model employing only B features,augmenting thei model with R features bringsmore information than augmenting it with Mfeatures.
Therefore, in principle, the languagemodel which incorporates bigram and featuretype R can achieve higher performance than themodel which incorporates bigram and M.We believe: this because there is moreinformation redundancy between M and B thanbetween R and'B.
From the above data, we seethat there exists large information redundancyboth between B and R (IR(B,R;O)=0.898) andbetween B and M (IR(B,M;O)=l.696).
Oneexplanation is that often B and M are in fact thesame word, where the nearest preceding wordmodifies the predicted word.
For example,consider the sentence in Figure 5, where "{~lk/zuo4ye4/assignment" is he predicted O, and Band M are the same word " ~ 3~2/ying4wen2/English ".It is also possible that B and R are the sameword, where the nearest preceding word ismodified by the predicted word.
For example,the dependency grammatical structure of thephrase "~/zai4/in ~/jiao4shi4/classroom" isshowed in Figure 6.
Here, " ~l~/jiao4shi4/classroom" is the predicted O, and Band R are the same word ":i~E/zai4/in".In Chinese (as well as in English), the headword typically lies at the end of the phrase.
Thismakes B more likely to be M than R, so theinformation redundancy between B and M islarger than that between B and R.4.3 If M (the nearest preceding wordmodifying the predicted word O) is one ofthe feature types of the baseline, MT (themodifying type between M and O) willbring less information gain for wordprediction.We are interested in knowing how much non-redundant information is present in MT if M isincluded in the baseline.
To assess this, weconducted the following experiment, whichfocuses directly on the relationship between MTand the two words involved.143zuo2 tianl xia4 wu3 di4 di4 zai4 jiao4 shi4 zuo4yesterday afternoon brother in classroom do/f?Pyingl wen2E n g l is h __.as.signment...._. I_.
____....___ IM ,B  0Figure 5: The dependency grammatical structure of "~SC/yinglwen2/English ~'~31klzuo4ye4/assignment "I~, T~- .~.~zuo2 tianl xia4 wu3 di4 di4yesterday afternoon brotherzai4 jiao4 shi4in classroomR,B 0zuo4 yingl wen2 zuo4 ye4do English assignmentFigure 6: The dependency structure of "~/zai4/in ~Jjiao4shi4/classroom"MTzuo2 tianl xia4 wu3yesterday afternoonM,B  0di4 di4 zai4 jiao4 shi4 zuo4 yingl wen2 zuo4 ye4brother in classroom do English assignmentFigure 7: The dependency structure of the phrase "~7~,./zuo2tian l/yesterday -ff ~/xia4wu3/aftemoon"xue2 sheng3 men2 zheng4 zaii4 jiao4 shi4student (plural marker) (-ing) in classroomRT ...j-vp--...~xie3 lun4 wen2write paperR,B 0f e vt ?
Figure 8: The dependency structureo th phrase ~/xle3/write -~3~/lun4wen2/paper"We measured the information gain of MTover M to be only IG(MT;OIM)=0.110 bits,while the information redundancy of MT and Mis a much larger IR(MT, M;O)=0.861 bits.
Thismeans that the prediction information for O in M(which at IQ(M;O)=2.237 bits is much larger,incidentally, than that in MT at IQ(MT;O)=0.971bits) contains almost all the predictioninformation for O in MT.
The corresponding144Table4: Information gain measurements in agreedy searchBaselinenullBB,RB,R,MB,R,M,RTB,R,M,RT, MTB R,M,RT, MT, BPB,R,M,RT, MT, BP, RPInformation Gain of the VariantsB3.826 1.5810.683M2.2370.5410.388RT0.9540.5850.0930.084MT BP0.971 1.4930.533 0.1080.381 0.0890.061 0.0630.059 0.052- -  0 .046RP0.6630.4990.0330.0310.0090.0080.007MP0.8180.4730.3310.0140.0110.0070.0030.002linguistic explanation may be as follows.
Thelexical identities of the predicted word O and itsmodifying word M involved in a dependencyrelation determine to a large extent the type ofmodification relation MT that holds between Oand its modifying Word M.Consider the sentence in Figure 7.
In thephrase " @ :N. /zuo2tianl/yesterday -V/xia4wu3/afternooW', just knowing the identityof the two words "@~/zuo2tianl/yesterday"and " -It ~/xia4v~u3/afternoon" is enough topredict with near certainty that the relationbetween them is time phrase (tp), thus giving thefollowing dependency structure as Figure 7.4.4 If R (the nearest preceding wordmodified by the predicted word O) is oneof the feature types of the baseline, RT(the modifying type between R and O)will bring less information gain for wordprediction.This simply mirrors the immediatelypreceding point, except that R is the modifiedword (parent) instead of the modifying word(child).
In this case, we measured theinformation gain iof RT over R to be onlyIG(RT;OIR)=0.271 bits, while the informationredundancy of R T and R is a much largerIR(RT, R;O)=0.683: bits.
This means that theinformation in R (IQ(R;O)=I.581 bits) containsalmost all the information in MT(IQ(RT;O)=0.954 bits).
The correspondinglinguistic explanation is as follows.
The lexicalidentities of the words (R, O) involved in adependency relation determine to a large extentthe type of modification relation RT that holdsbetween O and the word it modifies, R.Consider the sentence in Figure 8, theidentity of the words "~/xie3/write" and "J,~3~:/lun4wen2/paper" determine with near certaintythat their relationship is verb phrase (vp):4.5 Among the feature types in {B, BP, M,MP, MT, R, RP, RT}, the preference orderfor selecting feature types is B, R, M, RT,MT, BP, RP, MP.We used the metric IG to obtain a ranking forfeature types according to their predictiveness.This ranking only considers information gain; itignores complexity (for a practical application,we would also consider the complexity of themodel at the same time.).
To obtain this order,we performed a greedy search where at eachstep we selected the next most informativefeature type (i.e., the feature type that has thelargest information gain).
The empiricalinformation gain measurements in each searchstep is shown in Table 4, where the featurewhich has the boldface IG in each column is thefeature type selected in that step, andIG(F;O\[NulI)=IQ(F;O).This preference ordering can serve as aguideline for selecting feature type combinationsin a language model.
That is to say, given the145L~= ~ ~ - -~-  ?
_g41 ~ - -il 2 - -  ~ ~.~ ~ - -...........B R M RT MT BP RP MPfeature typeFigure 9: Cumulative information quantity of selected feature type combinations with 1-8 feature typesfeature type set {B, BP, M, MP, MT, R, RP, RT},if a language model uses only one feature type,feature type B should be used; if a languagemodel uses two feature types, the feature typecombination {B, R} should be used, and so on.However, we can see from Figure 9 that theadditional information gain falls off rapidlywhen more than three feature types are selected.5.
Conc lus ionWe have described a series of corpus-basedanalyses that take a Chinese treebank andquantify the information gain and theinformation redundancy for various feature typescombinations involving both dependency andbigram feature types.
The analysis yields severalinteresting conclusions that explain linguisticobservations from an information theoretic pointof view, and in addition will find practical use inthe design of language models.
Althoughperhaps we have been aware of some of theobservations to varying extents, here weintroduce a methodology that uses concreteevidence drawn from real contexts in order togive more reliable and objective results.We have already begun conducting similarexperiments on an English training corpus \[61,which so far yield the same types of behaviordescribed in this paper.
We aim to discoverwhich, if any, claims about the informationpresent in dependency based features arepeculiar to Chinese language, which are peculiarto English, and which are common acrossmultiple languages.Based on the analysis, we will design,construct, and incrementally refine newlanguage models for written and spoken Englishand Chinese that incorporate varying levels oflinguistic structure.
These models will aim tocapture regularities that arise from long-distancedependencies, which n-gram models cannotrepresent.
At the same time, we will retain asmany of the n-gram parameters as needed tocapture important lexical dependencies.References\[1\] A. Stolcke, C. Chelba, D. Engle, V. Jimenez, L.Mangu, H. Printz, E. Ristad, R. Rosenfeld, D. Wu, EJelinek and S. Khudanpur, "Dependency languagemodeling", 1996 Large Vocabulary ContinuousSpeech Recognition Summer Research WorkshopTechnical Report.
Research Note 24, Center forLanguage and Speech Processing, Johns HopkinsUniversity, Baltimore, MD, April 1997.146\[2\] Della Pietra, S. and V. Della Pietra, "Inducingfeatures of random fields", IEEE Transactions onPattern Analysis and Machine Intelligence, 19(4),April 1997, pp.380-;393.\[3\] Ciprian Chelba', David Engle, Frederick Jelinek,Victor Jimenex, Sanjeev Khudanpur, Lidia Mangu,Harry Printz, Eric Ristad, Ronald Rosenfeld, AndreasStolcke, Dekai Wu i "Structure and performance of adependency langu~age model", Proceedings ofiEurospeech'97, 1997.\[4\] Ney, Hermann., "On structuring probabilisticdependency in stochastic language modeling",Computer Speech & Language 8: 1-38, 1994.\[5\] John Lafferty, Daniel Sleator, Davy Temperley,"Grammatical trigrams: A probabilistic model of linkgrammar", Proceedings of the 1992 AAAI FallSymposium on Probabilistic Approaches to NaturalLanguage, Cambridge, MA, 1992.\[6\] Dekai WU, SUI Zhifang, ZHAO Jun, "Aninformation-based method for selecting feature typesfor word prediction", to appear in Proceeding ofEurospeech'99, 1999.\[7\] Michael John Collins, "A new statistical parserbased on bigram lexical dependencies", in:Proceedings of the 34rd Annual Meeting of theAssociation for Computational Linguistics, 1996.\[8\] Michael Collins, "Three generative, lexicalisedmodels for statistical parsing", in: Proceedings of the35rd Annual Meeting of the Association forComputational Linguistics, 1997.\[9\] S. Della Pietra, V. Della Pietra, J. Gillett, J.Lafferty, H. Printz, L. Ures, "Inference and estimationof a long-range trigram model", 1994.\[10\] Cover T. M., Thomas J.
A., Elements ofInformation Theory, Wiley., New York, 1991.\[11\] ZHOU Qiang, Phrase Bracketing andAnnotating on Chinese Language Corpus,Dissertation for Doctor Degree \[Peking University\],Beijing, China, 1996.\[12\] YU Shiwen, ZHOU Qiang, ZHANG Wei,ZHANG Yunyun, ZHAN Weidong, CHANG Baobao,SUI Zhifang, "Tagged Singapore Chinese primaryschool text", Communications of COLIPS 5, 1995.147
