Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 402?412,Baltimore, Maryland, USA, June 23-25 2014.c?2014 Association for Computational LinguisticsIncremental Joint Extraction of Entity Mentions and RelationsQi Li Heng JiComputer Science DepartmentRensselaer Polytechnic InstituteTroy, NY 12180, USA{liq7,jih}@rpi.eduAbstractWe present an incremental joint frame-work to simultaneously extract entity men-tions and relations using structured per-ceptron with efficient beam-search.
Asegment-based decoder based on the ideaof semi-Markov chain is adopted to thenew framework as opposed to traditionaltoken-based tagging.
In addition, by virtueof the inexact search, we developed a num-ber of new and effective global featuresas soft constraints to capture the inter-dependency among entity mentions andrelations.
Experiments on Automatic Con-tent Extraction (ACE)1corpora demon-strate that our joint model significantlyoutperforms a strong pipelined baseline,which attains better performance than thebest-reported end-to-end system.1 IntroductionThe goal of end-to-end entity mention and re-lation extraction is to discover relational struc-tures of entity mentions from unstructured texts.This problem has been artificially broken downinto several components such as entity mentionboundary identification, entity type classificationand relation extraction.
Although adopting sucha pipelined approach would make a system com-paratively easy to assemble, it has some limita-tions: First, it prohibits the interactions betweencomponents.
Errors in the upstream componentsare propagated to the downstream componentswithout any feedback.
Second, it over-simplifiesthe problem as multiple local classification stepswithout modeling long-distance and cross-task de-pendencies.
By contrast, we re-formulate thistask as a structured prediction problem to revealthe linguistic and logical properties of the hidden1http://www.itl.nist.gov/iad/mig//tests/acestructures.
For example, in Figure 1, the outputstructure of each sentence can be interpreted as agraph in which entity mentions are nodes and re-lations are directed arcs with relation types.
Byjointly predicting the structures, we aim to addressthe aforementioned limitations by capturing: (i)The interactions between two tasks.
For exam-ple, in Figure 1a, although it may be difficult fora mention extractor to predict ?1,400?
as a Per-son (PER) mention, the context word ?employs?between ?tire maker?
and ?1,400?
strongly in-dicates an Employment-Organization (EMP-ORG)relation which must involve a PER mention.
(ii)The global features of the hidden structure.
Var-ious entity mentions and relations share linguis-tic and logical constraints.
For example, wecan use the triangle feature in Figure 1b to en-sure that the relations between ?forces?, and eachof the entity mentions ?Somalia/GPE?, ?Haiti/GPE?and ?Kosovo/GPE?, are of the same type (Physical(PHYS), in this case).Following the above intuitions, we introducea joint framework based on structured percep-tron (Collins, 2002; Collins and Roark, 2004) withbeam-search to extract entity mentions and rela-tions simultaneously.
With the benefit of inexactsearch, we are also able to use arbitrary globalfeatures with low cost.
The underlying learningalgorithm has been successfully applied to someother Natural Language Processing (NLP) tasks.Our task differs from dependency parsing (such as(Huang and Sagae, 2010)) in that relation struc-tures are more flexible, where each node can havearbitrary relation arcs.
Our previous work (Li etal., 2013) used perceptron model with token-basedtagging to jointly extract event triggers and argu-ments.
By contrast, we aim to address a more chal-lenging task: identifying mention boundaries andtypes together with relations, which raises the is-sue that assignments for the same sentence withdifferent mention boundaries are difficult to syn-402The tire maker| {z }ORGstill employs 1,400| {z }PER.EMP-ORG(a) Interactions between Two Tasks... US|{z}GPEforces| {z }PERin Somalia| {z }GPE, Haiti|{z}GPEand Kosovo| {z }GPE.EMP-ORGPHYSconj andGPEPERGPEPHYSPHYSconj and(b) Example of Global FeatureFigure 1: End-to-End Entity Mention and Relation Extraction.chronize during search.
To tackle this problem,we adopt a segment-based decoding algorithm de-rived from (Sarawagi and Cohen, 2004; Zhang andClark, 2008) based on the idea of semi-Markovchain (a.k.a, multiple-beam search algorithm).Most previous attempts on joint inference of en-tity mentions and relations (such as (Roth and Yih,2004; Roth and Yih, 2007)) assumed that entitymention boundaries were given, and the classifiersof mentions and relations are separately learned.As a key difference, we incrementally extract en-tity mentions together with relations using a singlemodel.
The main contributions of this paper are asfollows:1.
This is the first work to incrementally predictentity mentions and relations using a singlejoint model (Section 3).2.
Predicting mention boundaries in the jointframework raises the challenge of synchroniz-ing different assignments in the same beam.
Wesolve this problem by detecting entity mentionson segment-level instead of traditional token-based approaches (Section 3.1.1).3.
We design a set of novel global features basedon soft constraints over the entire output graphstructure with low cost (Section 4).Experimental results show that the proposedframework achieves better performance thanpipelined approaches, and global features providefurther significant gains.2 Background2.1 Task DefinitionThe entity mention extraction and relationextraction tasks we are addressing are thoseof the Automatic Content Extraction (ACE)program2.
ACE defined 7 main entity typesincluding Person (PER), Organization (ORG),Geographical Entities (GPE), Location (LOC),2http://www.nist.gov/speech/tests/aceFacility (FAC), Weapon (WEA) and Vehicle(VEH).
The goal of relation extraction3is toextract semantic relations of the targeted typesbetween a pair of entity mentions which ap-pear in the same sentence.
ACE?04 defined 7main relation types: Physical (PHYS), Person-Social (PER-SOC), Employment-Organization(EMP-ORG), Agent-Artifact (ART), PER/ORGAffiliation (Other-AFF), GPE-Affiliation(GPE-AFF) and Discourse (DISC).
ACE?05 keptPER-SOC, ART and GPE-AFF, split PHYS intoPHYS and a new relation type Part-Whole,removed DISC, and merged EMP-ORG andOther-AFF into EMP-ORG.Throughout this paper, we use?
to denote non-entity or non-relation classes.
We consider rela-tion asymmetric.
The same relation type with op-posite directions is considered to be two classes,which we refer to as directed relation types.Most previous research on relation extractionassumed that entity mentions were given In thiswork we aim to address the problem of end-to-endentity mention and relation extraction from rawtexts.2.2 Baseline SystemIn order to develop a baseline system repre-senting state-of-the-art pipelined approaches, wetrained a linear-chain Conditional Random Fieldsmodel (Lafferty et al, 2001) for entity mention ex-traction and a Maximum Entropy model for rela-tion extraction.Entity Mention Extraction Model We re-castthe problem of entity mention extraction as a se-quential token tagging task as in the state-of-the-art system (Florian et al, 2006).
We applied theBILOU scheme, where each tag means a token isthe Beginning, Inside, Last, Outside, and Unit ofan entity mention, respectively.
Most of our fea-tures are similar to the work of (Florian et al,3Throughout this paper we refer to relation mention as re-lation since we do not consider relation mention coreference.4032004; Florian et al, 2006) except that we do nothave their gazetteers and outputs from other men-tion detection systems as features.
Our additionalfeatures are as follows:?
Governor word of the current token based on de-pendency parsing (Marneffe et al, 2006).?
Prefix of each word in Brown clusters learnedfrom TDT5 corpus (Sun et al, 2011).Relation Extraction Model Given a sentencewith entity mention annotations, the goal of base-line relation extraction is to classify each mentionpair into one of the pre-defined relation types withdirection or ?
(non-relation).
Most of our relationextraction features are based on the previous workof (Zhou et al, 2005) and (Kambhatla, 2004).
Wedesigned the following additional features:?
The label sequence of phrases covering the twomentions.
For example, for the sentence in Fig-ure 1a, the sequence is ?NP VP NP?.
We alsoaugment it by head words of each phrase.?
Four syntactico - semantic patterns described in(Chan and Roth, 2010).?
We replicated each lexical feature by replacingeach word with its Brown cluster.3 Algorithm3.1 The ModelOur goal is to predict the hidden structure ofeach sentence based on arbitrary features and con-straints.
Let x ?
X be an input sentence, y??
Ybe a candidate structure, and f(x, y?)
be the fea-ture vector that characterizes the entire structure.We use the following linear model to predict themost probable structure y?
for x:y?
= argmaxy?
?Y(x)f(x, y?)
?w (1)where the score of each candidate assignment isdefined as the inner product of the feature vectorf(x, y?)
and feature weights w.Since the structures contain both entity men-tions relations, and we also aim to exploit globalfeatures.
There does not exist a polynomial-timealgorithm to find the best structure.
In practicewe apply beam-search to expand partial configu-rations for the input sentence incrementally to findthe structure with the highest score.3.1.1 Joint Decoding AlgorithmOne main challenge to search for entity mentionsand relations incrementally is the alignment of dif-ferent assignments.
Assignments for the same sen-tence can have different numbers of entity men-tions and relation arcs.
The entity mention ex-traction task is often re-cast as a token-level se-quential labeling problem with BIO or BILOUscheme (Ratinov and Roth, 2009; Florian et al,2006).
A naive solution to our task is to adopt thisstrategy by treating each token as a state.
How-ever, different assignments for the same sentencecan have various mention boundaries.
It is un-fair to compare the model scores of a partial men-tion and a complete mention.
It is also difficult tosynchronize the search process of relations.
Forexample, consider the two hypotheses ending at?York?
for the same sentence:AllanU-PER from?
NewB-ORG YorkI-ORG Stock ExchangeAllanU-PER from?
NewB-GPE YorkL-GPE Stock ExchangePHYSPHYSThe model would bias towards the incorrect as-signment ?New/B-GPEYork/L-GPE?
since it canhave more informative features as a completemention (e.g., a binary feature indicating if theentire mention appears in a GPE gazetter).
Fur-thermore, the predictions of the two PHYS rela-tions cannot be synchronized since ?New/B-FACYork/I-FAC?
is not yet a complete mention.To tackle these problems, we employ the idea ofsemi-Markov chain (Sarawagi and Cohen, 2004),in which each state corresponds to a segmentof the input sequence.
They presented a vari-ant of Viterbi algorithm for exact inference insemi-Markov chain.
We relax the max operationby beam-search, resulting in a segment-based de-coder similar to the multiple-beam algorithm in(Zhang and Clark, 2008).
Let?d be the upper boundof entity mention length.
The k-best partial assign-ments ending at the i-th token can be calculated as:B[i] = k-BESTy??
{y[1..i]|y[1:i?d]?B[i?d], d=1...?d}f(x, y?)
?wwhere y[1:i?d]stands for a partial configurationending at the (i-d)-th token, and y[i?d+1,i]corre-sponds to the structure of a new segment (i.e., sub-sequence of x) x[i?d+1,i].
Our joint decoding algo-rithm is shown in Figure 2.
For each token indexi, it maintains a beam for the partial assignmentswhose last segments end at the i-th token.
Thereare two types of actions during the search:404Input: input sentence x = (x1, x2, ..., xm).k: beam size.T ?
{?
}: entity mention type alphabet.R?
{?
}: directed relation type alphabet.4dt: max length of type-t segment, t ?
T ?
{?
}.Output: best configuration y?
for x1 initialize m empty beams B[1..m]2 for i?
1...m do3 for t ?
T ?
{?}
do4 for d?
1...dt, y??
B[i?
d] do5 k ?
i?
d+ 16 B[i]?
B[i] ?
APPEND(y?, t, k, i)7 B[i]?
k-BEST(B[i])8 for j ?
(i?
1)...1 do9 buf?
?10 for y??
B[i] do11 if HASPAIR(y?, i, j) then12 for r ?
R ?
{?}
do13 buf?
buf ?
LINK(y?, r, i, j)14 else15 buf?
buf ?
{y?
}16 B[i]?
k-BEST(buf)17 return B[m][0]Figure 2: Joint Decoding for Entity Men-tions and Relations.
HASPAIR(y?, i, j) checksif there are two entity mentions in y?thatend at token xiand token xj, respectively.APPEND(y?, t, k, i) appends y?with a type-tsegment spanning from xkto xi.
SimilarlyLINK(y?, r, i, j) augments y?by assigning a di-rected relation r to the pair of entity mentionsending at xiand xjrespectively.1.
APPEND (Lines 3-7).
First, the algorithmenumerates all possible segments (i.e., subse-quences) of x ending at the current token withvarious entity types.
A special type of seg-ment is a single token with non-entity label (?
).Each segment is then appended to existing par-tial assignments in one of the previous beams toform new assignments.
Finally the top k resultsare recorded in the current beam.2.
LINK (Lines 8-16).
After each step of APPEND,the algorithm looks backward to link the newlyidentified entity mentions and previous ones (ifany) with relation arcs.
At the j-th sub-step,it only considers the previous mention endingat the j-th previous token.
Therefore different4The same relation type with opposite directions is con-sidered to be two classes in R.configurations are guaranteed to have the samenumber of sub-steps.
Finally, all assignmentsare re-ranked with new relation information.There are m APPEND actions, each is followed byat most (i?1) LINK actions (line 8).
Therefore theworst-case time complexity is O(?d ?k ?m2), where?d is the upper bound of segment length.3.1.2 Example Demonstrationthetiremaker stillemploys1,400.
?PERORG...xy EMP-ORGFigure 3: Example of decoding steps.
x-axisand y-axis represent the input sentence and en-tity types, respectively.
The rectangles denote seg-ments with entity types, among which the shadedones are three competing hypotheses ending at?1,400?.
The solid lines and arrows indicate cor-rect APPEND and LINK actions respectively, whilethe dashed indicate incorrect actions.Here we demonstrate a simple but concrete ex-ample by considering again the sentence describedin Figure 1a.
Suppose we are at the token ?1,400?.At this point we can propose multiple entity men-tions with various lengths.
Assuming ?1,400/PER?,?1,400/??
and ?
(employs 1,400)/PER?
are possi-ble assignments, the algorithm appends these newsegments to the partial assignments in the beamsof the tokens ?employs?
and ?still?, respectively.Figure 3 illustrates this process.
For simplicity,only a small part of the search space is presented.The algorithm then links the newly identified men-tions to the previous ones in the same configu-ration.
In this example, the only previous men-tion is ?
(tire maker)/ORG?.
Finally, ?1,400/PER?
willbe preferred by the model since there are moreindicative context features for EMP-ORG relationbetween ?
(tire maker)/PER?
and ?1,400/PER?.4053.2 Structured-Perceptron LearningTo estimate the feature weights, we use struc-tured perceptron (Collins, 2002), an extensionof the standard perceptron for structured pre-diction, as the learning framework.
Huang etal.
(2012) proved the convergency of structuredperceptron when inexact search is applied withviolation-fixing update methods such as early-update (Collins and Roark, 2004).
Since we usebeam-search in this work, we apply early-update.In addition, we use averaged parameters to reduceoverfitting as in (Collins, 2002).Figure 4 shows the pseudocode for struc-tured perceptron training with early-update.
HereBEAMSEARCH is identical to the decoding algo-rithm described in Figure 2 except that if y?, theprefix of the gold standard y, falls out of the beamafter each execution of the k-BEST function (line 7and 16), then the top assignment z and y?are re-turned for parameter update.
It is worth noting thatthis can only happen if the gold-standard has a seg-ment ending at the current token.
For instance, inthe example of Figure 1a, B[2] cannot trigger anyearly-update since the gold standard does not con-tain any segment ending at the second token.Input: training set D = {(x(j), y(j))}Ni=1,maximum iteration number TOutput: model parameters w1 initialize w?
02 for t?
1...T do3 foreach (x, y) ?
D do4 (x, y?, z)?
BEAMSEARCH (x, y,w)5 if z 6= y then6 w?
w + f(x, y?)?
f(x, z)7 return wFigure 4: Perceptron algorithm with beam-search and early-update.
y?is the prefix of thegold-standard and z is the top assignment.3.3 Entity Type ConstraintsEntity type constraints have been shown effectivein predicting relations (Roth and Yih, 2007; Chanand Roth, 2010).
We automatically collect a map-ping table of permissible entity types for each rela-tion type from our training data.
Instead of apply-ing the constraints in post-processing inference,we prune the branches that violate the type con-straints during search.
This type of pruning canreduce search space as well as make the input forparameter update less noisy.
In our experiments,only 7 relation mentions (0.5%) in the dev set and5 relation mentions (0.3%) in the test set violatethe constraints collected from the training data.4 FeaturesAn advantage of our framework is that we caneasily exploit arbitrary features across the twotasks.
This section describes the local features(Section 4.1) and global features (Section 4.2) wedeveloped in this work.4.1 Local FeaturesWe design segment-based features to directly eval-uate the properties of an entity mention instead ofthe individual tokens it contains.
Let y?
be a pre-dicted structure of a sentence x.
The entity seg-ments of y?
can be expressed as a list of triples(e1, ..., em), where each segment ei= ?ui, vi, ti?is a triple of start index ui, end index vi, and entitytype ti.
The following is an example of segment-based feature:f001(x, y?, i) =????
?1 if x[y?.ui,y?.vi]= tire makery?.t(i?1), y?.ti= ?,ORG0 otherwiseThis feature is triggered if the labels of the (i?1)-th and the i-th segments are ?
?,ORG?, and the textof the i-th segment is ?tire maker?.
Our segment-based features are described as follows:Gazetteer features Entity type of each segmentbased on matching a number of gazetteers includ-ing persons, countries, cities and organizations.Case features Whether a segment?s words areinitial-capitalized, all lower cased, or mixture.Contextual features Unigrams and bigrams ofthe text and part-of-speech tags in a segment?scontextual window of size 2.Parsing-based features Features derived fromconstituent parsing trees, including (a) the phrasetype of the lowest common ancestor of the tokenscontained in the segment, (b) the depth of the low-est common ancestor, (c) a binary feature indicat-ing if the segment is a base phrase or a suffix of abase phrase, and (d) the head words of the segmentand its neighbor phrases.In addition, we convert each triple ?ui, vi, ti?
toBILOU tags for the tokens it contains to imple-ment token-based features.
The token-based men-406tion features and local relation features are identi-cal to those of our pipelined system (Section 2.2).4.2 Global Entity Mention FeaturesBy virtue of the efficient inexact search, we areable to use arbitrary features from the entirestructure of y?
to capture long-distance dependen-cies.
The following features between related entitymentions are extracted once a new segment is ap-pended during decoding.Coreference consistency Coreferential entitymentions should be assigned the same entity type.We determine high-recall coreference links be-tween two segments in the same sentence usingsome simple heuristic rules:?
Two segments exactly or partially string match.?
A pronoun (e.g., ?their?,?it?)
refers to previousentity mentions.
For example, in ?they haveno insurance on their cars?, ?they?
and ?their?should have the same entity type.?
A relative pronoun (e.g., ?which?,?that?, and?who?)
refers to the noun phrase it modifies inthe parsing tree.
For example, in ?the startingkicker is nikita kargalskiy, who may be 5,000miles from his hometown?, ?nikita kargalskiy?and ?who?
should both be labeled as persons.Then we encode a global feature to checkwhether two coreferential segments share the sameentity type.
This feature is particularly effectivefor pronouns because their contexts alone are of-ten not informative.Neighbor coherence Neighboring entity men-tions tend to have coherent entity types.
For ex-ample, in ?Barbara Starr was reporting from thePentagon?, ?Barbara Starr?
and ?Pentagon?
areconnected by a dependency link prep from andthus they are unlikely to be a pair of PER men-tions.
Two types of neighbor are considered: (i)the first entity mention before the current segment,and (ii) the segment which is connected by a sin-gle word or a dependency link with the currentsegment.
We take the entity types of the two seg-ments and the linkage together as a global feature.For instance, ?PER prep from PER?
is a featurefor the above example when ?Barbara Starr?
and?Pentagon?
are both labeled as PER mentions.Part-of-whole consistency If an entity men-tion is semantically part of another mention (con-nected by a prep of dependency link), they shouldbe assigned the same entity type.
For example,in ?some of Iraq?s exiles?, ?some?
and ?exiles?are both PER mentions; in ?one of the town?s twomeat-packing plants?, ?one?
and ?plants?
are bothFAC mentions; in ?the rest ofAmerica?, ?rest?
and?America?
are both GPE mentions.4.3 Global Relation FeaturesRelation arcs can also share inter-dependencies orobey soft constraints.
We extract the followingrelation-centric global features when a new rela-tion hypothesis is made during decoding.Role coherence If an entity mention is involvedin multiple relations with the same type, then itsroles should be coherent.
For example, a PERmention is unlikely to have more than one em-ployer.
However, a GPE mention can be a physicallocation for multiple entity mentions.
We combinethe relation type and the entity mention?s argumentroles as a global feature, as shown in Figure 5a.Triangle constraint Multiple entity mentionsare unlikely to be fully connected with the samerelation type.
We use a negative feature to penalizeany configuration that contains this type of struc-ture.
An example is shown in Figure 5b.Inter-dependent compatibility If two entitymentions are connected by a dependency link, theytend to have compatible relations with other enti-ties.
For example, in Figure 5c, the conj and de-pendency link between ?Somalia?
and ?Kosovo?indicates they may share the same relation typewith the third entity mention ?forces?.Neighbor coherence Similar to the entity men-tion neighbor coherence feature, we also combinethe types of two neighbor relations in the samesentence as a bigram feature.5 Experiments5.1 Data and Scoring MetricMost previous work on ACE relation extractionhas reported results on ACE?04 data set.
Aswe will show later in our experiments, ACE?05made significant improvement on both relationtype definition and annotation quality.
Thereforewe present the overall performance on ACE?05data.
We removed two small subsets in informalgenres - cts and un, and then randomly split the re-maining 511 documents into 3 parts: 351 for train-ing, 80 for development, and the rest 80 for blindtest.
In order to compare with state-of-the-art wealso performed the same 5-fold cross-validation onbnews and nwire subsets of ACE?04 corpus as inprevious work.
The statistics of these data sets407(GPE Somalia)(PER forces)(GPE US)EMP-ORGEMP-ORG?
(a)(GPE Somalia)(PER forces)(GPE Haiti)PHYSPHYSPHYS?
(b)(GPE Somalia)(PER forces)(GPE Kosovo)PHYSPHYSconj and(c)Figure 5: Examples of Global Relation Features.0 5 10 15 20 25# of training iterations0.700.720.740.760.780.80F_1 scoremention local+globalmention local(a) Entity Mention Performance0 5 10 15 20 25# of training iterations0.300.350.400.450.500.55F_1 scorerelation local+globalrelation local(b) Relation PerformanceFigure 6: Learning Curves on Development Set.are summarized in Table 1.
We ran the StanfordCoreNLP toolkit5to automatically recover the truecases for lowercased documents.Data Set # sentences # mentions # relationsACE?05Train 7,273 26,470 4,779Dev 1,765 6,421 1,179Test 1,535 5,476 1,147ACE?04 6,789 22,740 4,368Table 1: Data Sets.We use the standard F1measure to evaluate theperformance of entity mention extraction and re-lation extraction.
An entity mention is consideredcorrect if its entity type is correct and the offsetsof its mention head are correct.
A relation men-tion is considered correct if its relation type iscorrect, and the head offsets of two entity men-tion arguments are both correct.
As in Chan and5http://nlp.stanford.edu/software/corenlp.shtmlRoth (2011), we excluded the DISC relation type,and removed relations in the system output whichare implicitly correct via coreference links for faircomparison.
Furthermore, we combine these twocriteria to evaluate the performance of end-to-endentity mention and relation extraction.5.2 Development ResultsIn general a larger beam size can yield better per-formance but increase training and decoding time.As a tradeoff, we set the beam size as 8 through-out the experiments.
Figure 6 shows the learn-ing curves on the development set, and comparesthe performance with and without global features.From these figures we can clearly see that globalfeatures consistently improve the extraction per-formance of both tasks.
We set the number oftraining iterations as 22 based on these curves.5.3 Overall PerformanceTable 2 shows the overall performance of variousmethods on the ACE?05 test data.
We compareour proposed method (Joint w/ Global) with thepipelined system (Pipeline), the joint model withonly local features (Joint w/ Local), and two hu-man annotators who annotated 73 documents inACE?05 corpus.We can see that our approach significantly out-performs the pipelined approach for both tasks.
Asa real example, for the partial sentence ?a marcherfrom Florida?
from the test data, the pipelined ap-proach failed to identify ?marcher?
as a PER men-tion, and thus missed the GEN-AFF relation be-tween ?marcher?
and ?Florida?.
Our joint modelcorrectly identified the entity mentions and theirrelation.
Figure 7 shows the details when thejoint model is applied to this sentence.
At thetoken ?marcher?, the top hypothesis in the beamis ???,??
?, while the correct one is ranked sec-ond best.
After the decoder processes the token?Florida?, the correct hypothesis is promoted tothe top in the beam by the Neighbor Coherencefeatures for PER-GPE pair.
Furthermore, after408ModelEntity Mention (%)Relation (%) Entity Mention + Relation (%)Score P R F1P R F1P R F1Pipeline 83.2 73.6 78.1 67.5 39.4 49.8 65.1 38.1 48.0Joint w/ Local 84.5 76.0 80.0 68.4 40.1 50.6 65.3 38.3 48.3Joint w/ Global 85.2 76.9 80.8 68.9 41.9 52.1 65.4 39.8 49.5Annotator 1 91.8 89.9 90.9 71.9 69.0 70.4 69.5 66.7 68.1Annotator 2 88.7 88.3 88.5 65.2 63.6 64.4 61.8 60.2 61.0Inter-Agreement 85.8 87.3 86.5 55.4 54.7 55.0 52.3 51.6 51.9Table 2: Overall performance on ACE?05 corpus.steps hypotheses rank(a)ha?
marcher?i1ha?
marcherPERi2(b)ha?
marcher?
from?i1ha?
marcherPER from?i4(c)ha?
marcherPER from?
FloridaGPEi1ha?
marcher?
from?
FloridaGPEi2(d)ha?
marcherPER from?
FloridaGPEiGEN-AFF1ha?
marcher?
from?
FloridaGPEi4Figure 7: Two competing hypotheses for ?amarcher from Florida?
during decoding.linking the two mentions by GEN-AFF relation,the ranking of the incorrect hypothesis ???,??
?is dropped to the 4-th place in the beam, resultingin a large margin from the correct hypothesis.The human F1score on end-to-end relation ex-traction is only about 70%, which indicates it is avery challenging task.
Furthermore, the F1scoreof the inter-annotator agreement is 51.9%, whichis only 2.4% above that of our proposed method.Compared to human annotators, the bottleneckof automatic approaches is the low recall of rela-tion extraction.
Among the 631 remaining miss-ing relations, 318 (50.3%) of them were causedby missing entity mention arguments.
A lot ofnominal mention heads rarely appear in the train-ing data, such as persons (?supremo?, ?shep-herd?, ?oligarchs?, ?rich?
), geo-political entitymentions (?stateside?
), facilities (?roadblocks?,?cells?
), weapons (?sim lant?, ?nukes?)
and ve-hicles (?prams?).
In addition, relations are oftenimplicitly expressed in a variety of forms.
Someexamples are as follows:?
?Rice has been chosen by President Bush tobecome the new Secretary of State?
indicates?Rice?
has a PER-SOC relation with ?Bush?.?
?U.S.
troops are now knocking on the door ofBaghdad?
indicates ?troops?
has a PHYS rela-tion with ?Baghdad?.?
?Russia and France sent planes to Baghdad?
in-dicates ?Russia?
and ?France?
are involved inan ART relation with ?planes?
as owners.In addition to contextual features, deeper se-mantic knowledge is required to capture such im-plicit semantic relations.5.4 Comparison with State-of-the-artTable 3 compares the performance on ACE?04corpus.
For entity mention extraction, our jointmodel achieved 79.7% on 5-fold cross-validation,which is comparable with the best F1score 79.2%reported by (Florian et al, 2006) on single-fold.
However, Florian et al (2006) used somegazetteers and the output of other Information Ex-traction (IE) models as additional features, whichprovided significant gains ((Florian et al, 2004)).Since these gazetteers, additional data sets and ex-ternal IE models are all not publicly available, it isnot fair to directly compare our joint model withtheir results.For end-to-end entity mention and relation ex-traction, both the joint approach and the pipelinedbaseline outperform the best results reportedby (Chan and Roth, 2011) under the same setting.6 Related WorkEntity mention extraction (e.g., (Florian et al,2004; Florian et al, 2006; Florian et al, 2010; Zi-touni and Florian, 2008; Ohta et al, 2012)) andrelation extraction (e.g., (Reichartz et al, 2009;Sun et al, 2011; Jiang and Zhai, 2007; Bunescuand Mooney, 2005; Zhao and Grishman, 2005;Culotta and Sorensen, 2004; Zhou et al, 2007;Qian and Zhou, 2010; Qian et al, 2008; Chanand Roth, 2011; Plank and Moschitti, 2013)) havedrawn much attention in recent years but were409ModelEntity Mention (%)Relation (%) Entity Mention + Relation (%)Score P R F1P R F1P R F1Chan and Roth (2011) - 42.9 38.9 40.8 -Pipeline 81.5 74.1 77.6 62.5 36.4 46.0 58.4 33.9 42.9Joint w/ Local 82.7 75.2 78.8 64.2 37.0 46.9 60.3 34.8 44.1Joint w/ Global 83.5 76.2 79.7 64.7 38.5 48.3 60.8 36.1 45.3Table 3: 5-fold cross-validation on ACE?04 corpus.
Bolded scores indicate highly statistical significantimprovement as measured by paired t-test (p < 0.01)usually studied separately.
Most relation extrac-tion work assumed that entity mention boundariesand/or types were given.
Chan and Roth (2011) re-ported the best results using predicted entity men-tions.Some previous work used relations and en-tity mentions to enhance each other in jointinference frameworks, including re-ranking (Jiand Grishman, 2005), Integer Linear Program-ming (ILP) (Roth and Yih, 2004; Roth and Yih,2007; Yang and Cardie, 2013), and Card-pyramidParsing (Kate and Mooney, 2010).
All thesework noted the advantage of exploiting cross-component interactions and richer knowledge.However, they relied on models separately learnedfor each subtask.
As a key difference, our ap-proach jointly extracts entity mentions and rela-tions using a single model, in which arbitrary softconstraints can be easily incorporated.
Some otherwork applied probabilistic graphical models forjoint extraction (e.g., (Singh et al, 2013; Yu andLam, 2010)).
By contrast, our work employs anefficient joint search algorithm without modelingjoint distribution over numerous variables, there-fore it is more flexible and computationally sim-pler.
In addition, (Singh et al, 2013) used gold-standard mention boundaries.Our previous work (Li et al, 2013) used struc-tured perceptron with token-based decoder tojointly predict event triggers and arguments basedon the assumption that entity mentions and otherargument candidates are given as part of the in-put.
In this paper, we solve a more challeng-ing problem: take raw texts as input and identifythe boundaries, types of entity mentions and rela-tions all together in a single model.
Sarawagi andCohen (2004) proposed a segment-based CRFsmodel for name tagging.
Zhang and Clark (2008)used a segment-based decoder for word segmenta-tion and pos tagging.
We extended the similar ideato our end-to-end task by incrementally predictingrelations along with entity mention segments.7 Conclusions and Future WorkIn this paper we introduced a new architecturefor more powerful end-to-end entity mention andrelation extraction.
For the first time, we ad-dressed this challenging task by an incrementalbeam-search algorithm in conjunction with struc-tured perceptron.
While detecting mention bound-aries jointly with other components raises the chal-lenge of synchronizing multiple assignments inthe same beam, a simple yet effective segment-based decoder is adopted to solve this problem.More importantly, we exploited a set of global fea-tures based on linguistic and logical properties ofthe two tasks to predict more coherent structures.Experiments demonstrated our approach signifi-cantly outperformed pipelined approaches for bothtasks and dramatically advanced state-of-the-art.In future work, we plan to explore more soft andhard constraints to reduce search space as well asimprove accuracy.
In addition, we aim to incorpo-rate other IE components such as event extractioninto the joint model.AcknowledgmentsWe thank the three anonymous reviewers for theirinsightful comments.
This work was supported bythe U.S. Army Research Laboratory under Coop-erative Agreement No.
W911NF-09-2-0053 (NS-CTA), U.S. NSF CAREER Award under GrantIIS-0953149, U.S. DARPA Award No.
FA8750-13-2-0041 in the Deep Exploration and Filteringof Text (DEFT) Program, IBM Faculty Award,Google Research Award and RPI faculty start-upgrant.
The views and conclusions contained inthis document are those of the authors and shouldnot be interpreted as representing the official poli-cies, either expressed or implied, of the U.S. Gov-ernment.
The U.S. Government is authorized toreproduce and distribute reprints for Governmentpurposes notwithstanding any copyright notationhere on.410ReferencesRazvan C. Bunescu and Raymond J. Mooney.
2005.
Ashortest path dependency kernel for relation extrac-tion.
In Proc.
HLT/EMNLP, pages 724?731.Yee Seng Chan and Dan Roth.
2010.
Exploiting back-ground knowledge for relation extraction.
In Proc.COLING, pages 152?160.Yee Seng Chan and Dan Roth.
2011.
Exploitingsyntactico-semantic structures for relation extrac-tion.
In Proc.
ACL, pages 551?560.Michael Collins and Brian Roark.
2004.
Incremen-tal parsing with the perceptron algorithm.
In Proc.ACL, pages 111?118.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proc.
EMNLP,pages 1?8.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proc.
ACL,pages 423?429.Radu Florian, Hany Hassan, Abraham Ittycheriah,Hongyan Jing, Nanda Kambhatla, Xiaoqiang Luo,Nicolas Nicolov, and Salim Roukos.
2004.
A sta-tistical model for multilingual entity detection andtracking.
In Proc.
HLT-NAACL, pages 1?8.Radu Florian, Hongyan Jing, Nanda Kambhatla, andImed Zitouni.
2006.
Factorizing complex models:A case study in mention detection.
In Proc.
ACL.Radu Florian, John F. Pitrelli, Salim Roukos, and ImedZitouni.
2010.
Improving mention detection robust-ness to noisy input.
In Proc.
EMNLP, pages 335?345.Liang Huang and Kenji Sagae.
2010.
Dynamic pro-gramming for linear-time incremental parsing.
InACL, pages 1077?1086.Liang Huang, Suphan Fayong, and Yang Guo.
2012.Structured perceptron with inexact search.
In Proc.HLT-NAACL, pages 142?151.Heng Ji and Ralph Grishman.
2005.
Improving nametagging by reference resolution and relation detec-tion.
In Proc.
ACL, pages 411?418.Jing Jiang and ChengXiang Zhai.
2007.
A systematicexploration of the feature space for relation extrac-tion.
In Proc.
HLT-NAACL.Nanda Kambhatla.
2004.
Combining lexical, syntac-tic, and semantic features with maximum entropymodels for information extraction.
In Proc.
ACL,pages 178?181.Rohit J. Kate and Raymond Mooney.
2010.
Joint en-tity and relation extraction using card-pyramid pars-ing.
In Proc.
ACL, pages 203?212.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In Proc.
ICML, pages 282?289.Qi Li, Heng Ji, and Liang Huang.
2013.
Joint eventextraction via structured prediction with global fea-tures.
In Proc.
ACL, pages 73?82.Marie-Catherine De Marneffe, Bill Maccartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InProc.
LREC, pages 449,454.Tomoko Ohta, Sampo Pyysalo, Jun?ichi Tsujii, andSophia Ananiadou.
2012.
Open-domain anatomi-cal entity mention detection.
In Proc.
ACL Work-shop on Detecting Structure in Scholarly Discourse,pages 27?36.Barbara Plank and Alessandro Moschitti.
2013.
Em-bedding semantic similarity in tree kernels for do-main adaptation of relation extraction.
In Proc.ACL, pages 1498?1507.Longhua Qian and Guodong Zhou.
2010.
Clustering-based stratified seed sampling for semi-supervisedrelation classification.
In Proc.
EMNLP, pages 346?355.Longhua Qian, Guodong Zhou, Fang Kong, QiaomingZhu, and Peide Qian.
2008.
Exploiting constituentdependencies for tree kernel-based semantic relationextraction.
In Proc.
COLING, pages 697?704.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProc.
CONLL, pages 147?155.Frank Reichartz, Hannes Korte, and Gerhard Paass.2009.
Composite kernels for relation extraction.
InProc.
ACL-IJCNLP (Short Papers), pages 365?368.Dan Roth and Wen-tau Yih.
2004.
A linear program-ming formulation for global inference in natural lan-guage tasks.
In Proc.
CoNLL.Dan Roth and Wen-tau Yih.
2007.
Global inferencefor entity and relation identification via a lin- earprogramming formulation.
In Introduction to Sta-tistical Relational Learning.
MIT.Sunita Sarawagi and William W. Cohen.
2004.
Semi-markov conditional random fields for informationextraction.
In Proc.
NIPS.Sameer Singh, Sebastian Riedel, Brian Martin, Jiap-ing Zheng, and Andrew McCallum.
2013.
Jointinference of entities, relations, and coreference.
InProc.
CIKM Workshop on Automated KnowledgeBase Construction.Ang Sun, Ralph Grishman, and Satoshi Sekine.
2011.Semi-supervised relation extraction with large-scaleword clustering.
In Proc.
ACL, pages 521?529.411Bishan Yang and Claire Cardie.
2013.
Joint inferencefor fine-grained opinion extraction.
In Proc.
ACL,pages 1640?1649.Xiaofeng Yu and Wai Lam.
2010.
Jointly identifyingentities and extracting relations in encyclopedia textvia a graphical model approach.
In Proc.
COLING(Posters), pages 1399?1407.Yue Zhang and Stephen Clark.
2008.
Joint word seg-mentation and pos tagging using a single perceptron.In Proc.
ACL, pages 1147?1157.Shubin Zhao and Ralph Grishman.
2005.
Extractingrelations with integrated information using kernelmethods.
In Proc.
ACL, pages 419?426.Guodong Zhou, Jian Su, Jie Zhang, and Min Zhang.2005.
Exploring various knowledge in relation ex-traction.
In Proc.
ACL, pages 427?434.Guodong Zhou, Min Zhang, Dong-Hong Ji, andQiaoming Zhu.
2007.
Tree kernel-based relationextraction with context-sensitive structured parsetree information.
In Proc.
EMNLP-CoNLL, pages728?736.Imed Zitouni and Radu Florian.
2008.
Mention detec-tion crossing the language barrier.
In Proc.
EMNLP,pages 600?609.412
