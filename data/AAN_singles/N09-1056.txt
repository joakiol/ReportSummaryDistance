Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 494?502,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsFor a few dollars less: Identifying review pages sans human labelsLuciano BarbosaDept.
of Computer ScienceUniversity of UtahSalt Lake City, UT 84112, USA.lbarbosa@cs.utah.eduRavi Kumar Bo Pang Andrew TomkinsYahoo!
Research701 First AveSunnyvale, CA 94089, USA.
{ravikumar,bopang,atomkins}@yahoo-inc.comAbstractWe address the problem of large-scale auto-matic detection of online reviews without us-ing any human labels.
We propose an efficientmethod that combines two basic ideas: Build-ing a classifier from a large number of noisyexamples and using the structure of the web-site to enhance the performance of this classi-fier.
Experiments suggest that our method iscompetitive against supervised learning meth-ods that mandate expensive human effort.1 IntroductionShoppers are migrating to the web and online re-views are playing a critical role in affecting theirshopping decisions, online and offline.
Accordingto two surveys published by comScore (2007) andHorrigan (2008), 81% of web users have done on-line research on a product at least once.
Amongreaders of online reviews, more than 70% reportedthat the reviews had a significant influence on theirpurchases.
Realizing this economic potential, searchengines have been scrambling to cater to such userneeds in innovative ways.
For example, in responseto a product-related query, a search engine mightwant to surface only review pages, perhaps via a ?fil-ter by?
option, to the user.
More ambitiously, theymight want to dissect the reviews, segregate theminto novice and expert judgments, distill sentiments,and present an aggregated ?wisdom of the crowds?opinion to the user.
Identifying review pages is theindispensable enabler to fulfill any such ambition;nonetheless, this problem does not seem to havebeen addressed at web scale before.Detecting review webpages in a few, review-onlywebsites is an easy, manually-doable task.
A largefraction of the interesting review content, however,is present on pages outside such websites.
This iswhere the task becomes challenging.
Review pagesmight constitute a minority and can be buried ina multitude of ways among non-review pages ?for instance, the movie review pages in nytimes.com, which are scattered among all news articles, orthe product review pages in amazon.com, whichare accessible from the product description page.
Anautomatic and scalable method to identify reviewsis thus a practical necessity for the next-generationsearch engines.
The problem is actually more gen-eral than detecting reviews: it applies to detectingany ?horizontal?
category such as buying guides, fo-rums, discussion boards, FAQs, etc.Given the nature of these problems, it is tempt-ing to use supervised classification.
A formidablebarrier is the labeling task itself since human la-bels need time and money.
On the other hand, itis easier to generate an enormous number of low-quality labeled examples through purely automaticmethods.
This prompts the question: Can we do re-view detection by focusing just on the textual con-tent of a large number of automatically obtained butlow-quality labeled examples, perhaps also utilizingthe site structure specific to each website?
And howwill it compare to the best supervised classificationmethod?
We address these questions in this paper.Main contributions.
We propose the first end-to-end method that can operate at web scale to effi-ciently detect review pages.
Our method is basedon using simple URL-based clues to automatically494partition a large collection of webpages into twonoisy classes: One that consists mostly of reviewwebpages and another that consists of a mixtureof some review but predominantly non-review web-pages (more details in Section 4.2).We analyze the use of a naive Bayes classifier inthis noisy setting and present a simple algorithm forreview page classification.
We further enhance theperformance of this classifier by incorporating infor-mation about the structure of the website that is man-ifested through the URLs of the webpages.
We dothis by partitioning the website into clusters of web-pages, where the clustering delicately balances theinformation in the site-unaware labels provided bythe classifier in the previous step and the site struc-ture encoded in the URL tokens; a decision tree isused to accomplish this.
Our classification methodfor noisily-labeled examples and the use of site-specific cues to improve upon a site-independentclassifier are general techniques that may be appli-cable in other large-scale web analyses.Experiments on 2000 hand-labeled webpagesfrom 40 websites of varying sizes show that besidesbeing computationally efficient, our human-label-free method not only outperforms those based onoff-the-shelf subjectivity detection but also remainscompetitive against the state-of-the-art supervisedtext classification that relies on editorial labels.2 Related workThe related work falls into roughly four categories:Document- and sentence-level subjectivity detec-tion, sentiment analysis in the context of reviews,learning from noisy labeled examples, and exploit-ing site structure for classification.Given the subjective nature of reviews, document-level subjectivity classification is closely related toour work.
There have been a number of approachesproposed to address document-level subjectivity innews articles, weblogs, etc.
(Yu and Hatzivas-siloglou, 2003; Wiebe et al, 2004; Finn and Kush-merick, 2006; Ni et al, 2007; Stepinski and Mit-tal, 2007).
Ng et al (2006) experiment with reviewidentification for known domains using datasets withclean labels (e.g., movie reviews vs. movie-relatednon-reviews), a setting different from that of ours.Pang and Lee (2008b) present a method on re-ranking documents that are web search results for aspecific query (containing the word review) basedon the subjective/objective distinction.
Given the na-ture of the query, they implicitly detect reviews fromunknown sources.
But their re-ranking algorithmonly applies to webpages known to be (roughly) re-lated to the same narrow subject.
Since the web-pages in our datasets cover not only a diverse rangeof websites but also a diverse range of topics, theirapproach does not apply.
To the best of our knowl-edge, there has been no work on identifying reviewpages at the scale and diversity we consider.Subjectivity classification of within-documentitems, such as terms, has been an active line of re-search (Wiebe et al (2004) present a survey).
Iden-tifying subjective sentences in a document via off-the-shelf packages is an alternative way of detect-ing reviews without (additional) human annotations.In particular, the OpinionFinder system (Riloff andWiebe, 2003; Wiebe and Riloff, 2005) is a state-of-the-art knowledge-rich sentiment-analysis system.We will use it as one of our baselines and compareits performance with our methods.There has been a great deal of previous work insentiment analysis that worked with reviews, butthey were typically restricted to using reviews ex-tracted from one or two well-known sources, by-passing automatic review detection.
Examples ofsuch early work include (Turney, 2002; Pang et al,2002; Dave et al, 2003; Hu and Liu, 2004; Popescuand Etzioni, 2005).
See Pang and Lee (2008a) fora more comprehensive survey.
Building a collectionof diverse review webpages, not limited to one ortwo hosts, can better facilitate such research.Learning from noisy examples has been studiedfor a long time in the learning theory community(Angluin and Laird, 1988).
Learning naive Bayesclassifiers from noisy data (either features or labelsor both) was studied by Yang et al (2003).
Theirfocus, however, is to reconstruct the underlying con-ditional probability distributions from the observednoisy dataset.
We, on the other hand, rely on the vol-ume of labels to drown the noise.
Along this spirit,Snow et al (2008) show that obtaining multiple low-quality labels (through Mechanical Turk) can ap-proach high-quality editorial labels.
Unlike in theirsetting, we do not have multiple low-quality labelsfor the same URL.
The extensive body of work in495semi-supervised learning or learning from one classis also somewhat relevant to our work.
A major dif-ference is that they tend to work with small amountof clean, labeled data.
In addition, many semi-supervised/transductive learning algorithms are notefficient for web-scale data.Using site structure for web analysis tasks hasbeen addressed in a variety of contexts.
For ex-ample, Kening et al (2005) exploit the structureof a website to improve classification.
On a re-lated note, co-training has also been used to utilizeinter-page link information in addition to intra-pagetextual content: Blum and Mitchell (1998) use an-chor texts pointing to a webpage as the alternative?view?
of the page in the context of webpage clas-sification.
Their algorithm is largely site-unawarein that it does not explicitly exploit site structures.Utilizing site structures also has remote connectionsto wrapper induction, and there is extensive litera-ture on this topic.
Unfortunately, the methods in allof these work require human labeling, which is pre-cisely what our work is trying to circumvent.3 MethodologyIn this section we describe our basic methodologyfor identifying review pages.
Our method consistsof two main steps.
The first is to use a large amountof noisy training examples to learn a basic classifierfor review webpages; we adapt a simple naive Bayesclassifier for this purpose.
The second is to improvethe performance of this basic classifier by exploitingthe website structure; we use a decision tree for this.Let P be the set of all webpages.
Let C+ denotethe positive class, i.e., the set of all review pages andlet C?
denote the negative class, i.e., the set of allnon-review pages.
Each webpage p is exactly in oneof C+ or C?, and is labeled +1 or ?1 respectively.3.1 Learning from large amounts of noisy dataPrevious work using supervised or semi-supervisedlearning approaches for sentiment analysis assumesrelatively high-quality labels that are produced ei-ther via human annotation or automatically gener-ated through highly accurate rules (e.g., assigningpositive or negative label to a review according toautomatically extracted star ratings).We examine a different scenario where we can au-tomatically generate large amount of relatively low-quality labels.
Section 4.2 describes the processin more detail, but briefly, in a collection of pagescrawled from sites that are very likely to host re-views, those with the word review in their URLsare very likely to contain reviews (the noisy posi-tive set C?+) and the rest of the pages on those sitesare less likely to contain reviews (the more noisynegative set C??).
More formally, for a webpagep, suppose Pr[p ?
C+ | p ?
C?+] = ?
andPr[p ?
C+ | p ?
C??]
= ?, where 1 > ??
> 0.Can we still learn something useful from C?+ and C?
?despite the labels being highly noisy?The following analysis is based on a naive Bayesclassifier.
We chose naive Bayes classifier since thelearning phase can easily be parallelized.Given a webpage (or a document) p representedas a bag of features {fi}, we wish to assign a classargmaxc?{C+,C?}
Pr[c | p] to this webpage.
NaiveBayes classifiers assume fi?s to be conditionally in-dependent and we have Pr[p | c] = ?Pr[fi | c].Let ri = Pr[fi | C+]/Pr[fi | C?]
denote the con-tribution of each feature towards classification, andrc = Pr[C+]/Pr[C?]
denote the ratio of class pri-ors.
First note thatlog Pr[C+|p]Pr[C?|p] = log(Pr[C+]Pr[C?]
?Pr[p|C+]Pr[p|C?
])= log(Pr[C+]Pr[C?]
?
?ri)= log rc +?
log ri.A webpage p receives label +1 iff Pr[C+ | p] >Pr[C?
| p], and by above, if and only if ?
log ri >?
log rc.When we do not have a reasonable estimate ofPr[C+] and Pr[C?
], as in our setting, the best wecan do is to assume rc = 1.
In this case, p receiveslabel +1 if and only if?
log ri > 0.
Thus, a featurefi with log ri > 0 has a positive contribution to-wards p being labeled +1; call fi to be a ?positive?feature.
Typically we use relative-frequency estima-tion of Pr[c] and Pr[fi | c] for c ?
{C+, C?}.
Now,how does the estimation from a dataset with noisylabels compare with the estimation from a datasetwith clean labels?To examine this, we calculate the following:Pr[fi | C?+] = ?Pr[fi | C+] + (1?
?)
Pr[fi | C?
],Pr[fi | C??]
= ?
Pr[fi | C+] + (1?
?)
Pr[fi | C?
].Let r?i = Pr[fi| eC+]Pr[fi| eC?]
=?ri+(1??)?ri+(1??)
.
Clearly r?i is mono-tonic but not linear in ri.
Furthermore, it is bounded:496(1?
?)/(1?
?)
?
r?i ?
?/?.
However,r?i > 1 ??
?ri + (1?
?)
> ?ri + (1?
?)??
(???
)ri > (???)
??
ri > 1,wherethe last step used ?
> ?.
Thus, the sign of log r?i isthe same as that of log ri, i.e., a feature contribut-ing positively to?
log ri will continue to contributepositively to?
log r?i (although its magnitude is dis-torted) and vice versa.The above analysis motivates an alternative modelto naive Bayes.
Instead of each feature fi placinga weighted vote log r?i in the final decision, we trustonly the sign of log r?i, and let each feature fi place avote for the class C+ (respectively, C?)
if log r?i > 0(respectively, log r?i < 0).
Intuitively, this modeljust compares the number of ?positive?
features andthe number of ?negative?
features, ignoring the mag-nitude (since it is distorted anyway).
This is pre-cisely our algorithm: For a given threshold ?, thefinal label nbu?
(p) of a webpage p is given bynbu?
(p) = sgn (?
sgn(log r?i)?
?)
,where sgn is the sign function.
For comparisonpurposes, we also indicate the ?weighted?
version:nbw?
(p) = sgn (?
log r?i ?
?)
.If ?
= 0, we omit ?
and use nb to denote a genericlabel assigned by any of the above algorithms.Note that even though our discussions were fortwo-class and in particular, review classification,they are equally applicable to a wide range of clas-sification tasks in large-scale web-content analysis.Our analysis of learning from automatically gener-ated noisy examples is thus of independent interest.3.2 Utilizing site structureCan the structure of a website be exploited to im-prove the classification of webpages given by nb(?
)?While not all websites are well-organized, quite anumber of them exhibit certain structure that makesit possible to identify large subsites that contain onlyreview pages.
Typically but not always this structureis manifested through the tokens in the URL corre-sponding to the webpage.
For instance, the patternhttp://www.zagat.com/verticals/PropertyDetails.aspx?VID=a&R=b,where a,b are numbers, is indicative of allwebpages in zagat.com that are reviews ofrestaurants.
In fact, we can think of this as ageneralization of having the keyword review inthe URL.
Now, suppose we have an initial labelingnb(p) ?
{?1} for each webpage p produced by aclassifier (as in the previous section, or one that istrained on a small set of human annotated pages),can we further improve the labeling using thepattern in the URL structure?It is not immediate how to best use the URLstructure to identify the review subsites.
First,URLs contain irrelevant information (e.g., the to-ken verticals in the above example), thus clus-tering by simple cosine similarity may not dis-cover the review subsites.
Second, the subsitemay not correspond to a subtree in the URL hi-erarchy, i.e., it is not reasonable to expect allthe review URLs to share a common prefix.Third, the URLs contain a mixture of path com-ponents (e.g., www.zagat.com/verticals/PropertyDetails.aspx) and key-value pairs(e.g., VID=a and R=b) and hence each token (re-gardless of its position) in the URL could play arole in determining the review subsite.
Furthermore,conjunction of presence/absence of certain tokens inthe URL may best correspond to subsite member-ship.
In light of these, we represent each URL (andhence the corresponding webpage) by a bag {gi} oftokens obtained from the URL.
We perform a crudeform of feature selection by dropping tokens thatare either ubiquitous (occurring in more than 99%of URLs) or infrequent (occurring in fewer than 1%of URLs) in a website; neither yields useful infor-mation.Our overall approach will be to use gi?s to par-tition P into clusters {Ci} of webpages such thateach cluster Ci is predominantly labeled as eitherreview or non-review by nb(?).
This automati-cally yields a new label cls(p) for each page p,which is the majority label of the cluster of p:cls(p) = sgn(?q?C(p) nb(q)),where C(p) is the cluster of p. To this end, we usea decision tree classifier to build the clusters.
Thisclassifier will use the features {gi} and the target la-bels nb(?).
The classifier is trained on all the web-pages in the website and in the obtained decisiontree, each leaf, consisting of pages with the sameset of feature values leading down the path, corre-sponds to a cluster of webpages.
Note that the clus-ters delicately balance the information in the site-unaware labels nb(?)
and the site structure encoded497in the URLs (given by gi?s).
Thus the label cls(p)can be thought of as a smoothed version of nb(p).Even though we can expect most clusters to be ho-mogeneous (i.e., pure reviews or non-reviews), theabove method can produce clusters that are inher-ently heterogeneous.
This can happen if the web-site URLs are organized such that many subsitescontain both review and non-review webpages.
Totake this into account, we propose the followinghybrid approach that interpolates between the un-smoothed labels given by nb(?)
and the smoothedlabels given by cls(?).
For a cluster Ci, the dis-crepancy disc(Ci) = ?p?Ci [cls(p) 6= nb(p)]; thisquantity measures the number of disagreements be-tween the majority label cls(p) and the original labelnb(p) for each page p in the cluster.
The decisiontree guarantees disc(Ci) ?
|Ci|/2.
We call a clusterCi to be ?-homogeneous if disc(Ci) ?
?|Ci|, where?
?
[0, 1/2].
For a fixed ?, the hybrid label of a web-page p is given byhyb?
(p) ={ cls(p) if C(p) is ?-homogeneous,nb(p) otherwise.Note that hyb1/2(p) = cls(p) and hyb0(p) = nb(p).Note that in the above discussions, any clusteringmethod that can incorporate the site-unaware labelsnb(?)
and the site-specific tokens in gi?s could havebeen used; off-the-shelf decision tree was merely aspecific way to realize this.4 DataIt is crucial for this study to create a dataset thatis representative of a diverse range of websites thathost reviews over different topics in different styles.We are not aware of any extensive index of onlinereview websites and we do not want to restrict ourstudy to a few well-known review aggregation web-sites (such as yelp.com or zagat.com) sincethis will not represent the less popular and more spe-cialized ones.
Instead, we utilized user-generatedtags for webpages, available on social bookmarkingwebsites such as del.icio.us.We obtained (a sample of) a snapshot of URL?tagpairs from del.icio.us.
We took the top onethousand sites with review* tags; these websiteshopefully represent a broad coverage.
We were ableto crawl over nine hundred of these sites and the re-sulting collection of webpages served as the basisof the experiments in this paper.
We refer to thesewebsites (or the webpages from these sites, when itis clear from the context) as Sall.4.1 Gold-standard test setWhen the websites are as diverse as represented inSall, there is no perfect automatic way to generatethe ground truth labels.
Thus we sampled a numberof pages for human labeling as follows.First, we set aside 40 sites as the test sites (S40).In order to represent different types of websites (tothe best we can), we sampled the 40 sites so that S40covers different size ranges, since large-scale web-sites and small-scale websites are often quite dif-ferent in style, topic, and content.
We uniformlysampled 10 sites from each of the four size cate-gories (roughly, sites with 100?5K, 5K?25K, 25K?100K, and 100K+ webpages)1.
Indeed, S40 (as didSall) covered a wide range of topics (e.g., games,books, restaurants, movies, music, and electronics)and styles (e.g., dedicated review sites, product sitesthat include user reviews, newspapers with movie re-view sections, religious sites hosting book reviews,and non-English review sites).We then sampled 50 pages to be labeled from eachsite in S40.
Since there are some fairly large sitesthat have only a small number of review pages, auniform sampling may yield no review webpagesfrom those sites.
To reflect the natural distribu-tion on a website and to represent pages from bothclasses, the webpages were sampled in the follow-ing way.
For each website in S40, 25 pages wereuniformly sampled (representing the natural distri-bution) and 25 pages were sampled from among?equivalence classes?
based on URLs so that pagesfrom each major URL pattern were represented.Here, each webpage in the site is represented by aURL signature containing the most frequent tokensthat occur in the URLs in that site and all pages withthe same signature form an equivalence class.For our purposes, a webpage is considered a re-view if it contains significant amount of textual in-formation expressing subjective opinions on or per-sonal experiences with a given product / service.When in doubt, the guiding principle is whether1As we do not want to waste human annotation on sites withno reviews at all, a quick pre-screening process eliminated can-didate sites that did not seem to host any reviews.498a page can be a satisfactory result page for userssearching for reviews.
More specifically, the humanannotation labeled each webpage, after thoroughlyexamining the content, with one of the followingseven intuitive labels: ?single?
(contains exactly onereview), ?multiple?
(concatenation of more than onereview), ?no?
(clearly not a review page), ?empty?
(looks like a page that could contain reviews but hadnone), ?login?
(a valid user login needed to look atthe content), ?hub?
(a pointer to one or more reviewpages), and ?ambiguous?
(border-line case, e.g., awebpage with a one line review).
The first two labelswere treated as +1 (i.e., reviews) and the last five la-bels were treated as ?1 (i.e., non-reviews).
Out ofthe 2000 pages, we obtained 578 pages labeled +1and the 1422 pages labeled ?1.
On a pilot study us-ing two human judges, we obtained 78% inter-judgeagreement for the seven labels and 92% inter-judgeagreement if we collapse the labels to ?1.
Percent-ages of reviews in our samples from different sitesrange from 14.6% to 93.9%.Preprocessing for text-based analysis.
We pro-cessed the crawled webpages using lynx to ex-tract the text content.
To discard templated content,which is an annoying issue in large-scale web pro-cessing, and HTML artifacts, we used the followingpreprocessing.
First, the HTML tags <p>, <br>,</tr>, and </td> were interpreted as paragraphbreaks, the ?.?
inside a paragraph was interpreted asa sentence break, and whitespace was used to tok-enize words in a sentence.
A sentence is considered?good?
if it has at least seven alphabetic words anda paragraph is considered ?good?
if it has at leasttwo good sentences.
After extracting the text us-ing lynx, only the good paragraphs were retained.This effectively removes most of the templated con-tent (e.g., navigational phrases) and retains most ofthe ?natural language?
texts.
Because of this pre-processing, 485 pages out of 2000 turned out to beempty and these were discarded (human labels on97% of these empty pages were ?1).4.2 Dataset with noisy labelsAs discussed in Section 3.1, our goal is to obtain alarge noisy set of positive and negative labeled ex-amples.
We obtained these labels for the webpagesin the training sites, Srest, which is essentially Sall \S40.
First, the URLs in Srest were tokenized using aunigram model based on an English dictionary; thisis so that strings such as reviewoftheday areproperly interpreted.C?+: To be labeled +1, the path-component ofthe URL of the webpage has to contain the tokenreview.
Our assumption is that such pages arehighly likely to be review pages.
On a uniform sam-ple of 100 such pages in Sall, 90% were found to begenuine reviews.
Thus, we obtained a collection ofwebpages with slightly noisy positive labels.C??
: The rest of the pages in Srest were labeled?1.
Clearly this is a noisy negative set since not allpages containing reviews have review as part oftheir URLs (recall the example from zagat.com);thus many pages in C??
can still be reviews.While the negative labels in Srest are more noisythan the positive labels, we believe most of the non-review pages are in C?
?, and as most websites con-tain a significant number of non-review pages, thepercentage of reviews in C??
is smaller than that inC?+ (the assumption ??
in Section 3.1).We collected all the paragraphs (as defined ear-lier) from both C?+ and C??
separately.
We elim-inated duplicate paragraphs (this further mitigatesthe templates issue, especially for sites generatedby content-management software), and trained a un-igram language model as in Section 3.1.5 EvaluationsThe evaluations were conducted on the 1515 labeled(non-empty) pages in S40 described in Section 4.1.We report the accuracy (acc.)
as well as precision(prec.
), recall (rec.
), and f-measure (fmeas.)
for C+.Trivial baselines.
Out of the 1515 labeled pages,565 were labeled +1 and 950 were labeled ?1.
Ta-ble 1 summarizes the performance of baselines thatalways predict one of the classes and a baseline thatrandomly select a class according to the class dis-tribution S40.
As we can see, the best accuracyis .63, the best f-measure is .54, and they cannotbe achieved by the same baseline.
Before present-acc.
prec.
rec.
fmeas.always C?
.63 - 0 -always C+ .37 .37 1 .54random .53 .37 .37 .37Table 1: Trivial baseline performances.499ing the main results of our methods, we introducea much stronger baseline that utilizes a knowledge-rich subjectivity detection package.5.1 Using subjectivity detectorsThis baseline is motivated by the fact that reviewsoften contain extensive subjective content.
There aremany existing techniques that detect subjectivity intext.
OpinionFinder (http://www.cs.pitt.edu/mpqa/opinionfinderrelease/) is awell-known system that processes documents andautomatically identifies subjective sentences inthem.
OpinionFinder uses two subjective sentenceclassifiers (Riloff and Wiebe, 2003; Wiebe andRiloff, 2005).
The first (denoted opfA) focuses onyielding the highest accuracy; the second (denotedopfB) optimizes precision at the expense of recall.The methods underlying OpinionFinder incorporateextensive tools from linguistics (including, speechactivity verbs, psychological verbs, FrameNet verbsand adjectives with frame ?experiencer?, among oth-ers) and machine learning.
In terms of performance,previous work has shown that OpinionFinder is achallenging system to improve upon for review re-trieval (Pang and Lee, 2008b).
Computationally,OpinionFinder is very expensive and hence unattrac-tive for large-scale webpage analysis (running Opin-ionFinder on 1515 pages took about five hours).Therefore, we also propose a light-weight subjectiv-ity detection mechanism called lwd, which countsthe number of opinion words in each sentence in thetext.
The opinion words (5403 of them) were ob-tained from an existing subjectivity lexicon (http://www.cs.pitt.edu/mpqa).We ran both opfA and opfB on the tokenized text(running them on raw HTML produced worse re-sults).
Each sentence in the text was labeled subjec-tive or objective.
We experimented with two waysto label a document using sentence-level subjectiv-ity labels.
We labeled a document +1 if it containedat least k subjective sentences (denoted as opf?
(k),where k > 0 is the absolute threshold), or at leastf fraction of its sentences were labeled subjective(denoted as opf?
(f), where f ?
(0, 1] is the rela-tive threshold).
We conducted exhaustive parametersearch with both opfA and opfB.
For instance, theperformances of opfA as a function of the thresh-olds, both absolute and relative, is shown in Fig-ure 1.
Table 2 summarizes the best performancesof opf?
(k) (first two rows) and opf?
(f) (next tworows), in terms of accuracy and f-measure (bold-faced).
Similarly, for lwd, we labeled a document+1 if at least k sentences have at least ` opin-ion words (denoted lwd(k, `).)
Table 2 once againshows the best performing parameters for both accu-racy and f-measure for lwd.
Our results indicate thata simple method such as lwd can come very close toa sophisticated system such as opf?.acc.
prec.
rec.
fmeas.opfA(2) .704 .597 .634 .615opfB(2) .659 .526 .857 .652opfA(.17) .652 .529 .614 .568opfB(.36) .636 .523 .797 .632lwd(1, 4) .716 .631 .572 .600lwd(1, 1) .666 .538 .740 .623Table 2: Best performances of opf?
and lwd methods.Figure 1: Performance of opfA as a function of thresh-olds: Absolute and relative.5.2 Main resultsAs stated earlier, we do not have any prior knowl-edge about the value of ?
and hence have to workwith ?
= 0.
To investigate the implications ofthis assumption, we study the performance of nbu?and nbw?
as a function of ?.
The accuracy and f-measures are plotted in Figure 2.
There are three500acc.
prec.
rec.
fmeas.nbu .753 .652 .726 .687cls .756 .696 .616 .654hyb1/3 .777 .712 .674 .693Table 3: Performance of our methods.conclusions that can be drawn from this study: (i)The peak values of accuracy and f-measure are com-parable for both nbu?
and nbw?
, (ii) at ?
= 0, nbu ismuch better than nbw, in terms of both accuracy andf-measure, and (iii) the best performance of nbu?
oc-curs at ?
?
0.
Given the difficulty of obtaining ?
ifone were to use nbw?
, the above conclusions vali-date our intuition and the algorithm in Section 3.1.Figure 2: Performance as threshold changes: Comparingnbu?
(marked as (u)) with nbw?
(marked as (w)).Table 3 shows the performance of the site-specificmethod outlined in Section 3.2.
The clusterswere generated using the unpruned J48 decisiontree in Weka (www.cs.waikato.ac.nz/ml/weka).
In our experiments, we set ?
= 1/3 as anatural choice for the hybrid method.
As we seethe performance of nbu is about 7% better than thebest performance using a subjectivity-based method(in terms of accuracy).
The performance of thesmoothed labels (decision tree-based clustering) iscomparable to that of nbu.
However, the hybridmethod hyb1/3 yields an additional 3% relative im-provement over nbu.
Paired t-test over the accura-cies for these 40 sites shows both hyb1/3 and nbuto be statistically significantly better than the opf?with best accuracy (with p < 0.05, p < 0.005,respectively), and hyb1/3 to be statistically signifi-cantly better than nbu (with p < 0.05).5.3 Cross-validation on S40While the main focus of our paper is to studyhow to detect reviews without human labels, wepresent cross validation results on S40 as a compar-ison point.
The goal of this experiment is to get asense of the best possible accuracy and f-measurenumbers using labeled data and the state-of-the-art method for text classification, namely, SVMs.In other words, the performance numbers obtainedthrough SVMs and cross-validation can be thoughtof as realistic ?upper bounds?
on the performance ofcontent-based review detection.
We used SVMlight(svmlight.joachims.org) for this purpose.The cross-validation experiment was conductedas follows.
We split the data by site to simulate themore realistic setting where pages in the test set donot necessarily come from a known site.
Each foldconsisted of one site from each size category; thus,36 of the 40 sites in S40 were used for training andthe remainder for testing.
Over ten folds, the aver-age performance was: accuracy .795, precision .759,recall .658, and f-measure .705.Thus our methods in Section 3 come reason-ably close to the ?upper bound?
given by SVMsand human-labeled data.
In fact, while the su-pervised SVMs statistically significantly outperformnbu, they are statistically indistinguishable fromhyb1/3 via paired t-test over site-level accuracies.6 ConclusionsIn this paper we proposed an automatic method toperform efficient and large-scale detection of re-views.
Our method is based on two principles:Building a classifier from a large number of noisylabeled examples and using the site structure to im-prove the performance of this classifier.
Extensiveexperiments suggest that our method is competitiveagainst supervised learning methods that depend onexpensive human labels.
There are several interest-ing avenues for future research, including improv-ing the current method for exploiting the site struc-ture.
On a separate note, previous research has ex-plicitly studied sentiment analysis as an applicationof transfer learning (Blitzer et al, 2007).
Given thediverse range of topics present in our dataset, ad-dressing topic-dependency is also an interesting fu-ture research direction.501ReferencesDana Angluin and Philip D. Laird.
1988.
Learning fromnoisy examples.
Machine Learning, 2(4):343?370.John Blitzer, Mark Dredze, and Fernando Pereira.
2007.Biographies, Bollywood, boom-boxes and blenders:Domain adaptation for sentiment classification.
InProceedings of 45th ACL.Avrim Blum and Tom Mitchell.
1998.
Combining la-beled and unlabeled data with co-training.
In Proceed-ings of 11th COLT, pages 92?100.comScore.
2007.
Online consumer-generated re-views have significant impact on offline pur-chase behavior.
Press Release, November.http://www.comscore.com/press/release.asp?press=1928.Kushal Dave, Steve Lawrence, and David M. Pennock.2003.
Mining the peanut gallery: Opinion extractionand semantic classification of product reviews.
In Pro-ceedings of 12th WWW, pages 519?528.Aidan Finn and Nicholas Kushmerick.
2006.
Learn-ing to classify documents according to genre.
JASIST,7(5):1506?1518.John A. Horrigan.
2008.
Online shopping.
Pew Internet& American Life Project Report.Minqing Hu and Bing Liu.
2004.
Mining opinion fea-tures in customer reviews.
In Proceedings of 19thAAAI, pages 755?760.Gao Kening, Yang Leiming, Zhang Bin, Chai Qiaozi, andMa Anxiang.
2005.
Automatic classification of webinformation based on site structure.
In Cyberworlds,pages 552?558.Vincent Ng, Sajib Dasgupta, and S. M. Niaz Arifin.
2006.Examining the role of linguistic knowledge sources inthe automatic identification and classification of re-views.
In Proceedings of 21st COLING/44th ACLPoster, pages 611?618.Xiaochuan Ni, Gui-Rong Xue, Xiao Ling, Yong Yu, andQiang Yang.
2007.
Exploring in the weblog spaceby detecting informative and affective articles.
In Pro-ceedings of 16th WWW, pages 281?290.Bo Pang and Lillian Lee.
2008a.
Opinion mining andsentiment analysis.
Foundations and Trends in Infor-mation Retrieval, 2(1-2):1?135.Bo Pang and Lillian Lee.
2008b.
Using very simplestatistics for review search: An exploration.
In Pro-ceedings of 22nd COLING.
Poster.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classification using ma-chine learning techniques.
In Proceedings of EMNLP,pages 79?86.Ana-Maria Popescu and Oren Etzioni.
2005.
Extractingproduct features and opinions from reviews.
In Pro-ceedings of HLT/EMNLP, pages 339?346.Ellen Riloff and Janyce Wiebe.
2003.
Learning extrac-tion patterns for subjective expressions.
In Proceed-ings of EMNLP, pages 105?112.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast - but is itgood?
Evaluating non-expert annotations for naturallanguage tasks.
In Proceedings of EMNLP.Adam Stepinski and Vibhu Mittal.
2007.
A fact/opinionclassifier for news articles.
In Proceedings of 30th SI-GIR, pages 807?808.Peter Turney.
2002.
Thumbs up or thumbs down?
Se-mantic orientation applied to unsupervised classifica-tion of reviews.
In Proceedings of 40th ACL, pages417?424.Janyce M. Wiebe and Ellen Riloff.
2005.
Creating sub-jective and objective sentence classifiers from unanno-tated texts.
In Proceedings of CICLing, pages 486?497.Janyce M. Wiebe, Theresa Wilson, Rebecca Bruce,Matthew Bell, and Melanie Martin.
2004.
Learn-ing subjective language.
Computational Linguistics,30(3):277?308.Yirong Yang, Yi Xia, Yun Chi, and Richard R. Muntz.2003.
Learning naive Bayes classifier from noisy data.Technical Report 56, UCLA.Hong Yu and Vasileios Hatzivassiloglou.
2003.
Towardsanswering opinion questions: Separating facts fromopinions and identifying the polarity of opinion sen-tences.
In Proceedings of EMNLP, pages 129?136.502
