Squibs and DiscussionsEstimation of Probabilistic Context-FreeGrammars IZhiyi Chi*Brown UniversityStuart Geman*Brown UniversityThe assignment of probabilities to the productions of a context-free grammar may generate animproper distribution: the probability of all finite parse trees is less than one.
The conditionfor proper assignment is rather subtle.
Production probabilities can be estimated from parsedor unparsed sentences, and the question arises as to whether or not an estimated system isautomatically proper.
We show here that estimated production probabilities always yield properdistributions.1.
IntroductionContext-free grammars (CFG's) are useful because of their relatively broad coverageand because of the availability of efficient parsing algorithms.
Furthermore, CFG's arereadily fit with a probability distribution (to make probabilistic CFG's--or PCFG's),rendering them suitable for ambiguous languages through the maximum a posteriorirule of choosing the most probable parse.For each nonterminal symbol, a (normalized) probability is placed on the set of allproductions from that symbol.
Unfortunately, this simple procedure runs into an un-expected complication: the language generated by the grammar may have probabilityless than one.
The reason is that the derivation tree may have probability greater thanzero of never terminating--some ass can be lost to infinity.
This phenomenon is wellknown and well understood, and there are tests for "tightness" (by which we meantotal probability mass equal to one) involving a matrix derived from the expectedgrowth in numbers of symbols generated by the probabilistic rules (see for exampleBooth and Thompson \[1973\], Grenander \[1976\], and Harris \[1963\]).What if the production probabilities are estimated from data?
Suppose, for ex-ample, that we have a parsed corpus that we treat as a collection of (independent)samples from a grammar.
It is reasonable to hope that if the trees in the sample are fi-nite, then an estimate of production probabilities based upon the sample will producea system that assigns probability zero to the set of infinite trees.
For example, there isa simple maximum-likelihood prescription for estimating the production probabilitiesfrom a corpus of trees (see Section 2), resulting in a PCFG.
Is it tight?
If the corpus isunparsed then there is an iterative approach to maximum-likelihood estimation (theEM or Baum-Welsh algorithm--again, see Section 2) and the same question arises: dowe get actual probabilities or do the estimated PCFG's assign some mass to infinitetrees?
We will show that in both cases the estimated probability is tight.
2* Division of Applied Mathematics, Brown University, Providence, RI 02912 USA1 Note added in proof: An alternative proof of one of our main results (see Corollary, Section 3) recentlyappeared in the IEEE Transactions on Pattern Analysis and Machine Intelligence (S,~nchez and Bened(\[1997\]).2 When estimating from an unparsed corpus, we shall assume a model without null or unit productions;see Section 2.Computational Linguistics Volume 24, Number 2Wetherell (1980) has asked a similar question: a scheme (different from maximumlikelihood) is introduced for estimating production probabilities from an unparsedcorpus, and it is conjectured that the resulting system is tight.
(Wetherell and othersuse the designation "consistent" instead of "tight," but in statistics, consistency refersto the asymptotic correctness of an estimator.
)A trivial example is the CFG with one nonterminal nd one terminal symbol, inChomsky normal form:A ~ AAa ~ awhere a is the only terminal symbol.
Assign probability p to the first production(A ~ AA) and q = 1 -p  to the second (A ~ a).
Let Sh be the total probability ofall trees with depth less than or equal to h. For example, $2 = q corresponding toA ~ a, and $3 = q + pq2 corresponding to {A ~ a} tO {A ~ AA, A --~ a,A --~ a}.
Ingeneral, Sh+l = q + pSi.
(Condition on the first production: with probability q the treeterminates and with probability p it produces two nonterminal symbols, each of whichmust now terminate with depth less than or equal to h.) It is not hard to show thatSh is nondecreasing and converges to min(1, I), meaning that a proper probability isa obtained if and only if p < ~.What if p is estimated from data?
Given a set of finite parse trees wl, w2 .
.
.
.
.
w,, themaximum-likelihood estimator for p (see Section 2) is, sensibly enough, the "relativefrequency" estimatory'~nlf(A ~ AA; wi)~i=1 \[f(A ~ AA; wi) + f (A  ~ a; wi)\]where f(.
;w) is the number of occurrences of the production "."
in the tree w. Thesentence a m, although ambiguous (there are multiple parses when m > 2), alwaysinvolves m - 1 of the A ~ AA productions and m of the A ~ a productions.
Hencef (A  ~ AA; Odi) < f (A  ~ a; odi) for each wi.
Consequently:f (A  ---+ AA;wi) < l \ [ f (A ~ AA;~i) + f (A  ~ a;wi)\]for each wi, and ~ < ?.
The maximum-likelihood probability is tight.If only the yields (left-to-right sequence of terminals) Y(o;1), Y(w2) .
.
.
.
.
Y(wn) areavailable, the EM algorithm can be used to iteratively "climb" the likelihood surface(see Section 2).
In the simple example here, the estimator converges in one step andis the same ~ as if we had observed the entire parse tree for each wi.
Thus, ~ is againless than ?
and the distribution is again tight.2.
Max imum-L ike l ihood  Es t imat ionMore generally, let G -- (V, T, R, S) denote a context-free grammar with finite variableset V, start symbol S E V, finite terminal set T, and finite production (or rule) set R.(We use R in place of the more typical P to avoid confusion with probabilities.)
Eachproduction in R has the form A ~ oL, where A E V and o~ E (VUT)*.
In the usual way,probabilities are introduced through the productions: P : R --~ \[0,1\] such that VA E V:p(A -~ c~) = 1.
(1)orE(rUT)*s .
t .
(A~c~)ER300Chi and Geman Probabilistic Context-Free GrammarsGiven a set of finite parse trees cab ca2,..., can, drawn independently according tothe distribution imposed by p, we wish to estimate p.In terms of the frequency function f, introduced in Section 1, the likelihood of thedata isL = L(p;cal,ca2 .
.
.
.
.
con)n= II II p(A-  Yi=1 (A~)ERRecall the derivation of the maximum-likelihood estimator of p: The log of the likeli-hood is:n~ ~f (A  --+ a;cai)logp(A ~ a).
(2)AEV a s.t.
i=1The function p : R ~ \[0,1\] subject o (1) that maximizes (2) satisfies:6AEV ?
?
(A~o~)ERAAp(A ~ a) + f (A  ~ a;cai)logp(A ~ a) = 0i=1V(B ~/3) E R where {AA }AEV are Lagrange multipliers.
Denote the maximum-likelihoodestimator by fi:n BAB q- ~ i= l f (  --+ /3 ;ca ; )  = 0 V(S ~ /3) E Rf,(B +/3)Since ~ f i (B+/3)=l )f l  sA .
(8~f l )ea~(B --~/3) = ~=l f (B  --~/3; cai)c~ s .
t .
H <B-~)e~ i=l f (B  ---+ o4cai)(3)The maximum-likelihood estimator is the natural, "relative frequency," estimator.Suppose B E V is unobserved among the parse trees cabc02,-..,can.
Then we canassign fi(B --+ fl) arbitrarily, requiring only that (1) be respected.
Evidently the likeli-hood is unaffected by the particular assignment of fi(B --~ fl).
Furthermore, it is nothard to see that any such B has probability zero of arising in any derivation that isbased upon the maximum-likelihood probabilitiesg--~ence th issue of tightness isindependent of this assignment.We will show that if f~ is the set of all (finite) parse trees generated by G, and if f~(ca)is the probability of ca ff fl under the maximum-likelihood production probabilities,then fi(f~) = 1.3 Consider any sequence of productions that leads from S to B.
If the parent (antecedent) of B arose inthe sample, then the last production has ~ probability zero and hence the sequence has probabilityzero.
Otherwise, move "up" through the ancestors of B until finding the first variable in the S- to -Bsequence represented in the sample (certainly S is represented).
Apply the same reasoning to theproduction from that variable, and conclude that the given sequence has/3 probability zero.301Computational Linguistics Volume 24, Number 22.1 The  EM A lgor i thmUsually the derivation trees are unobserved--the sample, or corpus, contains onlythe yields Y(wl), Y(w2) .
.
.
.
.
Y(wn) (Y(wi) E T* for each 1 < i < n).
The likelihood issubstantially more complex, since p(Y(w)) is now a marginal probability; we need tosum over the set of w E f~ that yield Y(w):p(Y(w)) = E p(Y(w')).wlEU~Y(w?
)=Y(oa)In the case where only yields are observed, the treatment is complicated consider-ably by the possibility of null productions (A --, 0) and unit productions (A ~ B E V).If, however, the language of the grammar does not include the null string, then there isan equivalent grammar (one with the same language) that has no null productions andno unit productions (cf.
Hopcroft & Ullman \[1979\], Theorem 4.4).
It is, then, perhapsbest to simplify the treatment by assuming that there are no null or unit productions.Therefore, when the corpus consists of yields only, we shall assume a priori a modelfree of null and unit productions, and study tightness for probabilities estimated undersuch a model.
Based upon the results of Stolcke \[1995\] it is likely that this restrictioncan be relaxed, but we have not pursued this.Letting ~y denote {w Efk Y(w) = Y}, the likelihood of the corpus becomesnH E H P(A--'~oL)f(A~;~)"i=1 ~OE~y(~i) (A---~o~)ERAnd the maximum-likelihood equation becomes+ p(B fl) Ei=l EwEfly(wi, I-I(A--.
)cR p(A -~ a)f(A-~";~) = 0fT(B ~ /3) = ~iL1 Ep~f(B ~ fl;w)lw E ~y(~,)\] (4),~ s,,, " B E(B_~,E ~ Ei=IEpV( ~ a;w)lw E ~Y(o~,)\]where E~ is expectation under fi and where "\]w E~-~y(wi)" means  "conditioned on0.2 E ~-~Y(wi)'"There is no hope for a closed form solution, but (4) does suggest an iterationscheme, which, as it turns out, "climbs" the likelihood surface (though there are noguarantees about approaching a global maximum): Let P0 be an arbitrary assignmentrespecting (1).
Define a sequence of probabilities, ~n, by the iteration~n+i(B ~ fl) = ~i'=1E~,\[f(B ~ fl;w)iw E fly(w,)\] (5)~ ~" ~7=, E~,\[f(B ~ a;wliw E aY(o,D\] (8~a)ERThe right-hand side is manageable, as long as we can manageably compute all possibleparses of a sentence (yield) Y(w).
(More efficient approaches exist; see Baker \[1979\].
)This iteration procedure is an instance of the EM Algorithm.
Baum \[1972\] first intro-duced it for hidden Markov models (regular grammars) and Baker \[1979\] extendedit to the problem addressed here (estimation for context-free grammars).
Dempster,Laird, and Rubin \[1977\] put the idea into a much more general setting and coined the302Chi and Geman Probabilistic Context-Free Grammarsterm EM for Expectation-Maximization.
The right-hand side of (5) is computed us-ing the expected frequencies under j~,; pn+l is then the maximum-likelihood estimator,treating the expected frequencies as though they were observed frequencies.The issue of tightness comes up again.
We will show that pn(f~) = 1 for each n > 0.3.
Tightness of the Maximum-Likelihood EstimatorGiven a context-free grammar G = (V, T, R, S), let f2 be the set of finite parse trees, letp : R ~ \[0,1\] be a system of production probabilities satisfying (1), and let wl, w2,.
?., w,be a set (sample) of finite parse trees 0;k EfL  For now, null and unit productions arepermitted.
Finally, let ~ be the maximum-likelihood estimator of p, as defined by (3).
(See also the remarks following \[3\] concerning variables unobserved in wl, 0;2 .
.
.
.
, w,.
)More generally, ~ will refer to the probability distribution on (possibly infinite) parsetrees induced by the maximum-likelihood estimator.Theorem~b(~) = 1ProofLet qA = p (derivation tree rooted with A fails to terminate).
We will show that qs = 0(i.e., derivation trees rooted with S always terminate).For each A E V, let F(A; w) be the number of instances of A in w and let F(A; w)be the number of nonroot instances of A in w. Given oz E (V U T)*, let nA(cZ) be thenumber of instances of A in the string o~, and, finally, let ai be the ith component ofthe string o~.
For any A E V:qAytqA Z F(A; wi)i=1= p(UBEv U .
.
.
.
.
t~,~ U i s,t.
oti=B{Oli fails to terminate})-~ Z p (U  a s.t.
B~c, \[-Ji s.t.
o~i=B {Oli fails to terminate}) (A~c,)ERBEV= Z Z fi(A ~ c~)~(Ui s.t.
ozi=B{Oli fails to terminate}lA ~ ol)BEV a s.t.
B~c, (A~c~)ER< ~ ~ ~(A~cz)nB(eOqBBEV a s.t.
aEc~{ Y\]~, .~, nB(a) n ol;wi) } .... Y'~i__,f(A ---*s, " a B*V E(A~,e  a ~ i= l f (  ----r OGWi){ ~in=l E ,~,~ nB(ol)f(A --'~ Ol;Wi) } .....qB nBEV E i=I  Z ,Z~'i~af(A ~ ?Gwi){ ~in=l ~ ~.'.
"E~ nn(ol)f(A "---~ Ol;Wi) } Z (A~cQER = qB nBEV Zi=I F(A; ?0i)Z qB ~ Z nB(OOf(m ~ OGCdi)BEV i=1 a s.t.
Be~ (A~o,)ER303Computational Linguistics Volume 24, Number 2Sum over A E V:E qA E F(A;wi) _< E qB ~ E E nB(ol)f(A ~ c~;wi)AEV i~-1 BEV i=1 AEV c~ s.t.
B~a(A~c~)ERn= ~ qB ~l:(B;wi)BEV i=1i .
e .
/HqA E(Ie(A;wi) - F(A;wi)) ~ 0AEV i=1Clearly, for every i = 1 ,2 , .
.
.
,n  F(A;wi) = F(A;wi) whenever A # S and F(S;wi) <F(S; wi).
Hence qs = 0, completing the proof of the theorem.
\[\]Now let ~, be the system of probabilities produced by the nth iteration of the EMAlgorithm (5):CorollaryIf R contains no null productions and no unit productions, then ~,(f~) = 1 Vn > 1.ProofAlmost identical, except hat we use (5) in place of (3) and end up with:nE qA EEG_1\[F(A;wi) - F(A;wi)lw C fly(w,)\] ~ 0.AEV i=1(6)In the absence of unit productions and null productions, F(A;w) < 21w \[ (twice thelength of the string w).
Hence the expectations in (6) are finite.
Furthermore, F(A; w)and F(A; ~) satisfy the same conditions as before: I:(A; w) = F(A; w) except when A = S,in which case F(A; w) < F(A; w).
Again, we conclude that qs = O.
\[\]AcknowledgmentsWe are indebted to Mark Johnson forencouraging us to look at this problem inthe first place, and for much good advicealong the way.
This work was supported bythe Army Research Office(DAAL03-92-G-0115), the National ScienceFoundation (DMS-9217655), and the Officeof Naval Research (N00014-96-1-0647).ReferencesBaker, J. K. 1979.
Trainable grammars forspeech recognition.
In SpeechCommunications Papers of the 97th Meeting ofthe Acoustical Society of America,pages 547-550, Cambridge, MA.Baum, L. E. 1972.
An inequality andassociated maximization techniques instatistical estimation of probabilisticfunctions of Markov processes.Inequalities, 3:1-8.Booth, T. L. and R. A. Thompson.
1973.Applying probability measures to abstractlanguages.
IEEE Trans.
on Computers,C-22:442-450.Dempster, A., N. Laird, and D. Rubin.
1977.Maximum likelihood from incompletedata via the EM algorithm.
Journal of theRoyal Statistical Society, Series B, 39:1-38.Grenander, U.
1976.
Lectures in Pattern TheoryVolume 1, Pattern Synthesis.Springer-Verlag, New York.Harris, T. E. 1963.
The Theory of BranchingProcesses.
Springer-Verlag, Berlin.Hopcroft, J. E. and J. D. Ullman.
1979.Introduction to Automata Theory, Languages,304Chi and Geman Probabilistic Context-Free Grammarsand Computation.
Addison Wesley.S~nchez, J.
A. and J. M. Benedf.
1997.Consistency of stochastic ontext-freegrammars from probabilistic estimationbased on growth transformations.
IEEETransactions on Pattern Analysis and MachineIntelligence, 19:1052-1055.Stolcke, A.
1995.
An efficient probabilisticcontext-free parsing algorithm thatcomputes prefix probabilities.Computational Linguistics, 21:165-201.Wetherell, C. S. 1980.
Probabilisticlanguages: A review and some openquestions.
Computing Surveys, 12:361-379.305
