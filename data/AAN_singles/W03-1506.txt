Multi-Language Named-Entity Recognition System based on HMMKuniko SAITO and Masaaki NAGATANTT Cyber Space Laboratories, NTT Corporation1-1 Hikari-no-oka Yokosuka-Shi Kanagawa, 239-0847 Japan{saito.kuniko,nagata.masaaki}@lab.ntt.co.jpAbstractWe introduce a multi-languagenamed-entity recognition system based onHMM.
Japanese, Chinese, Korean andEnglish versions have already beenimplemented.
In principle, it can analyzeany other language if we have trainingdata of the target language.
This systemhas a common analytical engine and it canhandle any language simply by changingthe lexical analysis rules and statisticallanguage model.
In this paper, wedescribe the architecture and accuracy ofthe named-entity system, and reportpreliminary experiments on automaticbilingual named-entity dictionaryconstruction using the Japanese andEnglish named-entity recognizer.1.
IntroductionThere is increasing demand for cross-languageinformation retrieval.
Due to the development of theWorld Wide Web, we can access informationwritten in not only our mother language but alsoforeign languages.
One report has English as thedominant language of web pages (76.6 %), followedby Japanese (2.77 %), German (2.28 %), Chinese(1.69 %), French (1.09 %), Spanish (0.81 %), andKorean (0.65 %) [1].
Internet users who are notfluent in English finds this situation far fromsatisfactory; the many useful information sources inEnglish are not open to them.To implement a multi-language informationretrieval system, it is indispensable to developmulti-language text analysis techniques such asmorphological analysis and named-entityrecognition.
They are needed in many naturallanguage processing applications such as machinetranslation, information retrieval, and informationextraction.We developed a multi-language named-entityrecognition system based on HMM.
This system ismainly for Japanese, Chinese, Korean and English,but it can handle any other language if we havetraining data of the target language.
This system hasa common analytical engine and only the lexicalanalysis rules and statistical language model needbe changed to handle any other language.
Previousworks on multi-language named-entity recognitionare mainly for European languages [2].
Our systemis the first one that can handle Asian languages, asfar as we know.In the following sections, we first describe thesystem architecture and language model of ournamed-entity recognition system.
We then describethe evaluation results of our system.
Finally, wereport preliminary experiments on the automaticconstruction of a bilingual named-entity dictionary.2.
System ArchitectureOur goal is to build a practical multi-languagenamed-entity recognition system for multi-languageinformation retrieval.
To accomplish our aim, thereare several conditions that should be fulfilled.
Firstis to solve the differences between the features oflanguages.
Second is to have a good adaptability toa variety of genres because there are an endlessvariety of texts on the WWW.
Third is to combinehigh accuracy and processing speed because theusers of information retrieval are sensitive toprocessing speed.
To fulfill the first condition, wedivided our system architecture into languagedependent parts and language independent parts.For the second and third conditions, we used acombination of statistical language model andoptimal word sequence search.
Details of thelanguage model and word sequence search arediscussed in more depth later; we start with anexplanation of the system's architecture.Figure 1 overviews the multi-languagenamed-entity recognition system.
We haveimplemented Japanese (JP), Chinese (CN), Korean(KR) and English (EN) versions, but it can, inprinciple, treat any other language.There are two language dependent aspects.
Oneinvolves the character encoding system, and theother involves the language features themselvessuch as orthography, the kinds of character types,and word segmentation.
We adopted a charactercode converter for the former and a lexical analyzerfor the latter.In order to handle language independent aspects,we adopted N-best word sequence search and astatistical language model in the analytical engine.The following sections describe the charactercode converter, lexical analyzer, and analyticalengine.2.1.
Character Code ConversionIf computers are to handle multilingual text,it is essential to decide the character set andits encoding.
The character set is a collectionof characters and encoding is a mappingbetween numbers and characters.
Onecharacter set could have several encodingschemes.
Hundreds of character sets andattendant encoding schemes are used on aregional basis.
Most of them are standardsfrom the countries where the language isspoken, and differ from country to country.Examples include JIS from Japan, GB fromChina and KSC from Korea; EUC-JP,EUC-CN and EUC-KR are the correspondingencoding schemes [3].
We call these encodingschemes ?local codes?
in this paper.
It isimpossible for local code to handle twodifferent character sets at the same time, soUnicode was invented to bring together allthe languages of the world [4].
In Unicode,character type is defined as Unicode propertythrough the assignment of a range of codepoints such as alphanumerics, symbols, kanji(Chinese character), hiragana (Japanesesyllabary character), hangul (Koreancharacter) and so on.
The proposed lexicalanalyzer allows us to define arbitraryproperties other than those defined by theUnicode standard.The character code converter changes theinput text encoding from local code toUnicode and the output from Unicode to localcode.
That is, the internal code of our systemis Unicode (UCS-4).
Our system can acceptEUC-JP, EUC-CN, EUC-KR and UTF-8 asinput-output encoding schemes.
In principle,we can use any encoding scheme if theencoding has round-trip conversion mappingbetween Unicode.
We assume that the inputencoding is either specified by the user, orautomatically detected by using conventionaltechniques such as [5].2.2.
Lexical AnalyzerThe lexical analyzer recognizes words in theinput sentence.
It also plays an importantrole in solving the language differences, thatis, it generates adequate word candidates forevery language.The lexical analyzer uses regularexpressions and is controlled by lexicalanalysis rules that reflect the differences inlanguage features.
We assume the followingthree language features;1. character type and word length2.
orthography and spacing3.
word candidate generationThe features can be set as parameters in thelexical analyzer.
We explain these threefeatures in the following sections.Figure 1.
System OverviewNE recognized text (local code)Language X Plain text (local code)Lexical Analysis RuleCharacter Code Converter (local code to Unicode)Character Code Converter (Unicode to local code)Lexical AnalyzerWord CandidatesJP    CNStatisticalLanguage Model(Dictionaries)KR   ENNERecognizerMorphAnalyzerN-bestWordSequenceSearchAnalytical Engine2.2.1 Character Type and Word LengthTable 1 shows the varieties of charactertypes in each language.
Character typesinfluence the average word length.
Forexample, in Japanese, kanji (Chinesecharacter) words have about 2 charactersand katakana (phonetic character usedprimarily to represent loanwords) words areabout 5 characters long such as ??????(password)?.
In Chinese, most kanji wordshave 2 characters but proper nouns fornative Chinese are usually 3 characters, andthose representing loanwords are about 4characters long such as ?????
(Beckham)?.In Korean, one hangul corresponds to onekanji and one hangul consists of oneconsonant - one vowel - one consonant, soloanwords written in hangul are about 3characters long such as ????
(internet)?.Character type and word length are relatedto word candidate generation in section 2.2.3.Table 1.
Character Types2.2.2 Orthography and SpacingThere is an obvious difference inorthography between each language, that is,European languages put a space betweenwords while Japanese and Chinese do not.
InKorean, spaces are used to delimit phrases(called as eojeol in Korean) not words, andspace usage depends greatly on theindividual.Therefore, another important role of thelexical analyzer is to handle spaces.
InJapanese and Chinese, spaces should usuallybe recognized as tokens, but in English andKorean, spaces must be ignored because itindicates words or phrases.
For example, thefollowing analysis results are preferred;I have a pen ??I/pronoun?
?have/verb?
?a/article?
?pen/noun?and never must be analyzed as follows;?I/pronoun?
?
/space?
?have/verb?
?
/space??a/article?
?
/space?
?pen/noun?There are, however, many compound nounsthat include spaces such as ?New York?,?United States?
and so on.
In this case, spacesmust be recognized as a character in acompound word.
In Korean, it is necessarynot only to segment one phrase separated bya space like Japanese, but also to recognizecompound words including spaces likeEnglish.These differences in handling spaces arerelated to the problem of whether spacesmust be included in the statistical languagemodel or not.
In Japanese and Chinese, it israre for spaces to appear in a sentence, sothe appearance of a space is an importantclue in improving analysis accuracy.
InEnglish and Korean, however, they are usedso often that they don?t have any importantmeaning in the contextual sense.The lexical analyzer can treat spacesappropriately.
The rules for Japanese andChinese, always recognize a space as a token,while for those for English and Koreanconsider spaces only a part of compoundwords such as ?New York?.2.2.3 Word Candidate GenerationIn our system, the analytical engine can listall dictionary word candidates from the inputstring by dictionary lookup.
However, it isalso necessary to generate word candidatesfor other than dictionary words, i.e.
unknownwords candidates.
We use the lexicalanalyzer to generate word candidates thatare not in the dictionary.It is more difficult to generate wordcandidates for Asian languages than forEuropean languages, because Asianlanguages don?t put a space between wordsas mentioned above.The first step in word candidate generationis to make word candidates from the inputstring.
The simplest way is to list allsubstrings as word candidates at every pointin the sentence.
This technique can be usedfor any language but its disadvantage is thatthere are so many linguistically meaninglesscandidates that it takes too long to calculatethe probabilities of all combinations of theENJPCNKRalphabet symbol numberalphabet symbol number kanji hiraganakatakanaalphabet symbol number kanjialphabet symbol number kanji hangulcandidates in the following analytical process.A much more effective approach is to limitword candidates to only those substringsthat are likely to be words.The character types are often helpful inword candidate generation.
For example, across-linguistic characteristic is thatnumbers and symbols are often used forserial numbers, phone numbers, blocknumbers, and so on, and some distinctivecharacter strings of alphabets and symbolssuch as ?http://www??
and?name@abc.mail.address?
are URLs,Email-addresses and so on.
This is notfoolproof since the writing styles often differfrom language to language.
Furthermore, itis better to generate such kinds of wordcandidates based on the longest matchmethod because substrings of thesecandidates do not usually constitute a word.In Japanese, a change between charactertypes often indicates a word boundary.
Forexample, katakana words are loanwords andso must be generated based on the longestmatch method.
In Chinese and Korean,sentences mainly consist of one charactertype, such as kanji or hangul, so thecharacter types are not as effective for wordrecognition as they are in Japanese.
However,changes from kanji or hangul toalphanumerics and symbols often indicateword changes.And word length is also useful to put alimit on the length of word candidates.
It is awaste of time to make long kanji words(length is 5 or more characters) in Japaneseunless the substring matched with thedictionary, because its average length isabout 2 characters.
In Korean, althoughhanguls (syllabaries) are converted into asequence of hangul Jamo (consonant orvowel) internally in order to facilitate themorphological analysis, the length of hangulwords are defined in hangul syllabaries.We designed the lexical analyzer so that itcan correctly treat spaces and wordcandidate generation depending on thecharacter types for each language.
Table 2shows sample lexical analysis rules forJapanese (JP) and English (EN).
Forexample, in Japanese, if character type iskanji or hiragana, the lexical analyzerattempts to output word candidates withlengths of 1 to 3.
If character type iskatakana, alphabet, or number, it generatesone candidate based on the longest matchmethod until character type changes.
If theinput is ?1500km?, word candidates are ?1500?and ?km?.
Subset character strings such as ?1?,?15?, ?500?, ?k?
and ?m?
are never output ascandidates.
It is possible for a candidate toconsist of several character types.
Japanesehas many words that consist of kanji andhiragana such as ????
(away from)?.
In anylanguage there are many words that consistof numbers and alphabetic characters suchas ?2nd?, or alphabetic characters andsymbols such as ?U.N.?.
Furthermore, if wewant to treat positional notation and decimalnumbers, we may need to change theUnicode properties, that is, we add ?.?
and ?,?to number-property.
The character type?compound?
in English rule indicatescompound words.
The lexical analyzergenerates a compound word (up to 2 wordslong) with recognition of the space betweenthem.
In Japanese, a space is alwaysrecognized as one word, a symbol.Table 3 shows the word candidates outputby the lexical analyzer following the rules ofTable 2.
The Japanese and English inputsare parallel sentences.
It is apparent that theefficiency of word candidate generationimproves dramatically compared to the caseof generating all character strings asCharacter Typekanjihiraganakatakanaalphabetnumbersymbolkanji ?
hiraganaalphabetnumbersymbolcompoundWord Length1-31-3until type changesuntil type changesuntil type changes11-3until type changesuntil type changes1up to 2 wordsJPENTable 2.
Lexical Analysis Rulecandidates at every point in a sentence.
InJapanese, kanji and hiragana strings becomeseveral candidates with lengths of 1 to 3, andalphabet and katakana strings become onecandidate based on the longest matchmethod until character type changes.
InEnglish, single words and compound wordsare recognized as candidates.
Only thecandidates that are not in the dictionarybecome unknown word candidates in theanalytical engine.2.3.
Analytical engineThe analytical engine consists of N-bestword sequence search and a statisticallanguage model.
Our system uses a wordbigram model for morphological analysis anda hidden Markov model for named-entityrecognition.
These models are trained fromtagged corpora that have been manuallyword segmented, part-of-speech tagged, andnamed-entity recognized respectively.
SinceN-best word sequence search and statisticallanguage model don?t depend on language,we can apply this analytical engine to alllanguages.
This makes it possible to treatany language if a corpus is available fortraining the language model.
The nextsection explains the hidden Markov modelused for named-entity recognition.3.
Named-entity Recognition ModelThe named-entity task is to recognizeentities such as organizations, personalnames, and locations.
Several papers havetackled named-entity recognition throughthe use of Markov model (HMM) [6],maximum entropy method (ME) [7, 8], andsupport vector machine (SVM) [9].
It isgenerally said that HMM is inferior to MEand SVM in terms of accuracy, but issuperior with regard to training andprocessing speed.
That is, HMM is suitablefor applications that require realtimeresponse or have to process large amounts oftext such as information retrieval.
Weextended the original HMM reported by BBN.BBN?s named-entity system is for Englishand offers high accuracy.The HMM used in BBN's system isdescribed as follows.
Let the morphemesequence be nwwW L1=  and Name Class(NC) sequence be nNCNCNC L1= .
Here, NCrepresents the type of named entity such asorganization, personal name, or location.
Thejoint probability of word sequence and NCsequence ),(),( ii NCwPNCWP ?= arecalculated as follows;(1) if 1??
ii NCNC),|(),|(),( 111 ???
?= iiiiiiii NCNCwPwNCNCPNCwP(2) if 1?= ii NCNC  and 1+= ii NCNC),|(),( 1 iiiii NCwwPNCwP ?=(3) if 1?= ii NCNC  and 1+?
ii NCNC),|(),|(),( 1 iiiiiii NCwendPNCwwPNCwP ><?= ?Here, the special symbol >< end  indicatesthe end of an NC sequence.In this model, morphological analysis andnamed-entity recognition can be performedat the same time.
This is preferable for Asianlanguages because they have some ambiguityabout word segmentation.
To adapt BBN?sHMM for Asian languages, we extended theoriginal HMM as follows.
Due to theTokyo Disneylandis 10 km away fromthe Tokyo station.?????????????????10km???????????????????
??
????
??
????
??
????
???10km?
??
????
??
????
??
????
????Tokyo?
?Tokyo Disneyland??Disneyland?
?Disneyland is??is??10??km?
?km away??away?
?away from??from?
?from the??the?
?the Tokyo??Tokyo?
?Tokyo station??station??.
?Input sentenceWord CandidatesTable 3.
Outputs of Lexical Analyzerambiguity of word segmentation,morphological analysis is performed prior toapplying the HMM; the analysis uses a wordbigram model and N-best candidates (ofmorpheme sequence) are output as a wordgraph structure.
Named-entity recognition isthen performed over this word graph usingthe HMM.
We use a forward-DPbackward-A* N-best search algorithm to getN-best morpheme sequence candidates [10].In this way, multiple morpheme candidatesare considered in named-entity recognitionand the problem of word segmentationambiguity is mitigated.BBN's original HMM used a back-off modeland smoothing to avoid the sparse dataproblem.
We changed this smoothing tolinear interpolation to improve the accuracy,and in addition, we used not only themorpheme frequency of terms but also partof speech frequency.
Table 4 shows the linearinterpolation scheme used here.
Underlineditems are added in our model.
The weight foreach probability was decided fromexperiments.4.
ExperimentsTo evaluate our system, we preparedoriginal corpora for Japanese, Chinese,Korean and English.
The material wasmainly taken from newspapers and Webtexts.
We used the morpheme analysisdefinition of Pen Tree Bank for English [11],Jtag for Japanese [12], Beijing Univ.
forChinese [13] and MATEC99 for Korean [14].The named-entity tag definitions were basedon MUC [15] for English and IREX [16] forJapanese.
We defined Chinese and Koreannamed-entity tags following the JapaneseIREX specifications.
Table 5 showsdictionary and corpus size.
Dictionary wordsmeans the size of the dictionary formorphological analysis.
Total words andsentences represent the size of the corpus fornamed-entity recognition.Named-entity accuracy is expressed interms of recall and precision.
We also use theF-measure to indicate the overallperformance.
It is calculated as follows;(4)       precisionrecallprecisionrecallF+?
?=2Table 6 shows the F-measure for alllanguages.
Since we used our originalcorpora in this evaluation, we cannotcompare our results to those of previousworks.
Accordingly, we also evaluated SVMusing our original corpora (see Table 6) [17].The accuracy of HMM and SVM wereapproximately equivalent.
But the analysisspeed of HMM was ten times faster thanthat of SVM [9].
This means that our systemis very fast and has state-of-the-art accuracyin four languages.We noted that the accuracy of SVM isunusually lower than that of HMM forJapanese.
We have not yet confirmed thecause of this, but a plausible argument is asfollows.
First, the word segmentationambiguity has a worse affect on accuracythan expected.
Since current SVMimplementations can not handle N-bestmorpheme candidates and lower-ordercandidates are not considered innamed-entity recognition.
Second, SVM maynot suit the analysis of irregular,ill-structured, and informal sentences suchas Web texts.
Our original corpus data wasdictionary words17,546436,157147,585182,523total words144,708143,408410,1881,456,130sentences5,9214,79312,82439,943ENJPCNKRTable 5.
Dictionary and Corpus SizeTable 6.
Named Entity Accuracy (F-measure(%))HMM88.281.084.579.9SVM84.757.389.582.1ENJPCNKR),|( 11 ??
iii wNCNCP  ),|( 11 ?ii NCNCwP  ),|( 11 NCwwP ii ?
),|( 11 ??
iii posNCNCP   ),|( 11 ?ii NCNCposP  ),|( 11 NCposposP ii ?
)|( 1?ii NCNCP      )|( 1NCwP i       )|( 1NCwP i)(NCP            )|( 1NCposP i     )|( 1NCposP iNCofnumber/1Table 4.
Linear Interpolation Schemetaken from newspapers and Web texts, theformer contains complete and grammaticalsentences unlike the latter.
It is often saidthat HMM is robust enough to analyze thesedirty sentences.
It is, anyhow, our next stepto analyze the results of named-entityrecognition in more detail.5.
Application to Bilingual LexiconExtraction from Parallel TextIn order to illustrate the benefit of ourmulti-language named-entity recognitionsystem, we conducted a simple experimenton extracting bilingual named-entity lexiconsfrom parallel texts.
It is very difficult togather bilingual lexicons of named entitiesbecause there are an enormous number ofnew named entities.
Establishing a bilingualnamed-entity dictionary automatically wouldbe extremely useful.There are 3 steps in extracting a bilinguallexicon as follows;1. recognize named entity from parallel text2.
extract bilingual lexicon candidates3.
winnow the candidates to yield areasonable lexicon listThe multi-language named-entityrecognition system is used in the first step.In this step, the parallel texts are analyzedmorphologically and named entities arerecognized.In the second step, bilingual lexiconcandidates are listed automatically underthe following conditions;?word sequence up to 5 words?include one or more named entities?does not include function wordsThe cooccurrence frequency of candidates iscalculated at the same time.In the third step, reasonable lexicons arecreated from the candidates.
To judge thesuitability of the candidates to be enteredinto a bilingual lexicon, we use thetranslation model called the IBM model [18].Let a word sequence in language X  belxxX L1=  and let the corresponding wordsequence in language Y  be myyY L1= .
Here,)1( lixi ??
and )1( mjy j ??
represent oneword.
In IBM model 1, the conditionalprobability )|( XYP  is calculated as follows;(5)       ??
?===+li ijmjlxytXYP m 11)1()|()|( ?where ?
is constant.
)|( ij xyt  is translationprobability and is estimated by applying theEM algorithm to a large number of paralleltexts.Since the longer word sequences X and Yare, the smaller )|( XYP  becomes, the valueof )|( XYP  cannot be compared when a wordsequence length changes.
Therefore, weimproved equation (5) to take into accountthe difference in a word sequence length andcooccurrence frequency as follows;(6)  )|()|()()()|( XYEXYPYmatchXmatchfreqXYS ??
?=freq     : cooccurrence frequency ofX and Y in parallel text)(Xmatch : ratio of 0)|( ?ij xyt  in X)(Ymatch : ratio of 0)|( ?ij xyt  in Y??
?===+li ijmjlxytXYE m 11)1()|()|( ?
)|( ij xyt is the average of )|( ij xyt .
)|( XYS  isused as a measure of candidate suitability.We used Japanese-English news articlealignment data as parallel texts that isreleased by CRL [19, 20].
In this data,articles and sentences are alignedautomatically.
We separated the parallel textinto a small set (about 1000 sentences) and aNorth KoreaUnited StatesInternational Monetary FundSoviet UnionMiddle EastNorth AtlanticTreaty OrganizationU.S.
President Bill ClintonNorth AmericanFree Trade AgreementEuropean CommunityTaiwan StraitClinton administrationU.N.
General AssemblyTokyo Stock Exchange??????????????????????????????????????????????????????????????????
?Table 7.
List of Bilingual Lexiconslarge set (about 150 thousand sentences).
Weextracted bilingual lexicons from a small setand )|( ij xyt  was estimated from a large set.Table 7 shows bilingual lexicons thatachieved very high scores.
It can be said thatthey are adequate as bilingual lexicons.Though a more detailed evaluation is afuture task, the accuracy is about 86 % forthe top 50 candidates.
This suggests that theproposed system can be applied to bilinguallexicon extraction for automatically creatingbilingual dictionaries of named entities.ConclusionWe developed a multi-language named-entityrecognition system based on HMM.
We haveimplemented Japanese, Chinese, Korean andEnglish versions, but in principle it can handle anylanguage if we have training data for the targetlanguage.
Our system is very fast and hasstate-of-the-art accuracy.References[1] Google: 1.6 Billion Served.
Wired, December2000, pp.118-119 (2000).
[2] Cucerzan, S. and Yarowsky, D.: LanguageIndependent Named Entity RecognitionCombining Morphological and ContextualEvidence, Proceedings of the 1999 JointSIGDAT Conference on Empirical Method inNatural Language Processing and Very LargeCorpora (EMNLP/VLC-99), College Park, pp.90-99 (1999)[3] Lunde, K.: CJKV Information Processing,O?REILY, (1999).
[4] The Unicode Consortium.
: The Unicode Standard,version 3.0, Addison-Wesley Longman, (2000).
[5] Kikui, G.: Identifying the Coding System andLanguage of On-line Documents on the Internet,Proceedings of the 16th InternationalConference on Computational Linguistics(COLING-96), pp.
652?657 (1996)[6] Bikel, D. M., Schwartz, R. and Weischedel, R. M.:An Algorithm that Learns What?s in a Name,Machine Learning, Vol.
34, No.
1-3, pp.
211-231(1999)[7] Borthwick, A., Sterling, J., Agichtein, E. andGrishman, R.: Exploiting Diverse KnowledgeSources via Maximum Entropy, Proceedings ofthe 6th Workshop on Very Large Corpora(VLC-98), pp.
152-160 (1998).
[8] Uchimoto, K., Murada, M., Ma, Q., Ozaku, H. andIsahara, H.: Named Entity Extraction Based onA Maximum Entropy Model and TransformationRules, Proceedings of the 38th Annual Meetingof the Association for Computational Linguistics(ACL-00), pp.
326-335 (2000).
[9] Isozaki, H. and Kazawa, H.: Efficient SupportVector Classifiers for Named Entity Recognition,Proceedings of the 19th InternationalConference on Computational Linguistics(COLING-02), pp.
390-396 (2002).
[10] Nagata, M.: A Stochastic JapaneseMorphological Analyzer Using a Forward-DPBackward-A* N-Best Search Algorithm,Proceedings of the 15th InternationalConference on Computational Linguistics(COLING-94), pp.
201-207 (1994).
[11] Marcus, M. P., Santorini, B. and Marcinkiewicz,M.
A.: Building a large annotated corpus ofEnglish: The Penn Treebank, ComputationalLinguistics, Vol.
19, No.2, pp.
313-330 (1993).
[12] Fuchi, T. and Takagi, S.: JapaneseMorphological Analyzer using WordCo-occurrence -JTAG-, Proceedings of 36thAnnual Meeting of the Association forComputational Linguistics and 17th InternationalConference on Computational Linguistics(ACL-COLING-98),  pp.
409-413 (1998).
[13] Yu, Shiwen.
et.
al.
: The GrammaticalKnowledge-base of Contemporary Chinese --- AComplete Specification (????????????
), Tsinghua University Press, (1992).
[14] ETRI.
: Part-of-Speech Tagset Guidebook ????
??
???
), Unpublished Manual, (1999)[15] DARPA: Proceedings of the 7th MessageUnderstanding Conference (MUC-7) (1998).
[16] IREX Committee (ed.
), 1999.
Proceedings ofthe IREX workshop.
http://nlp.cs.nyu.edu/irex/[17] Kudo, T and Matsumoto, Y.: Chunking withSupport Vector Machines, Proceedings of theSecond Meeting of the North American Chapterof the Association for Computational Linguistics(NAACL-01), pp.
192-199 (2001).
[18] Brown, P.F., Pietra, S. A. D., Pietra, V. J. D. andMercer, R. L.: The Mathematics of StatisticalMachine Translation: Parameter Estimation,Computational Linguistics, Vol.
19, No.
2, pp.263-311 (1993)[19] Utiyama, M. and Isahara, H.: Reliable Measuresfor Aligning Japanese-English News Article andSentences, Proceedings of the 41st AnnualMeeting of the Association for ComputationalLinguistics (ACL-03) (2003).
[20] Japanese-English News Article Alignment Data,http://www2.crl.go.jp/jt/a132/members/mutiyama/jea/index.html (2003)
