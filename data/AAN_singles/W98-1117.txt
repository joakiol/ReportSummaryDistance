A Maximum-Entropy Partial Parser for Unrestricted TextWojc iech  Skut  and Thors ten  BrantsUniversit~t des SaarlandesComputational  LinguisticsD-66041 Saarbrficken, Germany{ skut, brant s} @coli.
uni-sb, deAbst ractThis paper describes a partial parser that as-signs syntactic structures to sequences of part-of-speech tags.
The program uses the maximumentropy parameter stimation method, which M-lows a flexible combination of different know-ledge sources: the hierarchical structure, partsof speech and phrasal categories.
In effect, theparser goes beyond simple bracketing and recog-nises even fairly complex structures.
We giveaccuracy figures for different applications of theparser.1 In t roduct ionThe maximum entropy framework has provedto be a powerful modelling tool in many ar-eas of natural language processing.
Its ap-plications range from sentence boundary dis-ambiguation (Reynar and Ratnaparkhi, 1997)to part-of-speech tagging (Ratnaparkhi, 1996),parsing (Ratnaparkhi, 1997) and machine trans-lation (Berger et al, 1996).In the present paper, we describe a partialparser based on the maximum entropy mod-elling method.
After a synopsis of the maximumentropy framework in section 2, we present hemotivation for our approach and the techniquesit exploits (sections 3 and 4).
Applications andresults are the subject of the sections 5 and 6.2 Max imum Ent ropy  Mode l l ingThe expressiveness and modelling power of themaximum entropy approach arise from its abil-ity to combine information coming from differ-ent knowledge sources.
Given a set X of possi-ble histories and a set Y of futures, ~e can char-acterise vents from the joint event space X, Yby defining a number of features, i.e., equiva-lence relations over X x Y.
By defining thesefeatures, we express our insights about informa-tion relevant o modelling.In such a formalisation, the maximum en-tropy technique consists in finding a model that(a) fits the empirical expectations of the pre-defined features, and (b) does not assume any-thing specific about events that are not sub-ject to constraints imposed by the features.
Inother words, we search for the maximum en-tropy probability distribution p*:p" = argmax g (p)pEPwhere P = {p:p meets the empirical feature ex-pectations} and H(p) denotes the entropy of p.For parameter estimation, we can use the Im-proved Iterative Scaling (IIS) algorithm (Bergeret al., 1996), which assumes p to have the form:p(x, y) =where fi : X x Y ~ {0, 1} is the indicator func-tion of the i-th feature, Ai the weight assignedto this feature, and Z a normalisation constant.IIS iteratively adjusts the weights (Ai) of thefeatures; the model converges to the maximumentropy distribution.One of the most attractive properties of themaximum entropy approach is its ability to copewith feature decomposition and overlapping fea-tures.
In the following sections, we will showhow these advantages can be exploited for par-tial parsing, i.e., the recognition of syntacticstructures of limited depth.3 Context  In fo rmat ion  for Pars ingAn interesting feature of many partial parsersis that they recognise phrase boundaries mainlyon the basis of cues provided by strictly local143contexts.
Regardless of whether or not abstrac-tions such as phrases occur in the model, mostof the relevant information is contained irectlyin the sequence of words and part-of-speech tagsto be processed.An archetypal representative of this approachis the method described by Church (1988), whoused corpus frequencies to determine the bound-aries of simple non, recursive NPs.
For each pairof part-of-speech tags ti, tj, the probability of anNP boundary ('\[' or '\]') occurring between ti andtj is computed.
On the basis of these contextprobabilities, the program inserts the symbols'\[' and '\]' into sequences of part-of-speech tags.Information about lexical contexts also sig-nificantly improves the performance of deepparsers.
For instance, Joshi and Srinivas(1994) encode partial structures in the Tree Ad-joining Grammar framework and use taggingtechniques to restrict a potentially very largeamount of alternative structures.
Here, the con-text incorporates information about both theterminal yield and the syntactic structure builtso far.Local configurations of words and p~irts ofspeech are a particularly important knowledgesource for lexicalised grammars.
In the LinkGrammar framework (Lagerty et al, 1992;Della Pietra et al, 1994), strictly local contextsare naturally combined with long-distance in-formation coming from long-range trigrams.Since modelling syntactic context is a veryknowledge-intensive problem, the maximum en-tropy framework seems to be a particularly ap-propriate approach.
Ratnaparkhi (1997) intro-duces several conteztual predicates which pro-vide rich information about the syntactic con-text of nodes in a tree (basically, the structureand category of nodes dominated by or dom-inating the current phrase).
These predicatesare used to guide the actions of a parser.The use of a rich set of contextual features isalso the basic idea of the approach taken by Her-mjakob and Mooney (1997), who employ predi-cates capturing syntactic and semantic ontextin their parsing and machine translation system.4 A Par t ia l  Parser  for GermanThe basic idea underlying our appr*oach to par-tial parsing can be characterised as follows:?
An appropriate ncoding format makes itpossible to express all relevant lexical, cat-egorial and structural information in a fi-nite alphabet of structural tags assigned towords (section 4.1).?
Given a sequence of words tagged withpart-of-speech labels, a Markov model isused to determine the most probable se-quence of structural tags (section 4.2).?
Parameter estimation is based on the max-imum entropy technique, which takes fulladvantage of the multi-dimensional charac-ter of the structural tags (section 4.3).The details of the method employed are ex-plained in the remainder of this section.4.1  Re levant  Contextua l  In fo rmat ionThree pieces of information associated with aword wi are considered relevant o the parser:?
the part-of-speech tag ti assigned to wi?
the structural relation ri between wi andits predecessor wi-1?
the syntactic ategory ca of parent(wi)On the basis of these three dimensions, struc-tural tags are defined as triples of the formSi = (ti,ri,ca).
For better readability, we willsometimes use attribute-value matrices to de-note such tags.=Since we consider structures of limited depth,only seven values of the REL attribute are dis-tinguished.r /=0 if parent(wi) = parent(wi_l)+ if parent(wi) = parent2(wi_l)++ if parent(wi) = parent3(wi_l)- if parent2(wi) = parent(wi_l)- -  if parent3(wi) = parent(wi_l)= if parent2(wi) = parent2(wi_t)1 elseIf more than one of the conditions above aremet, the first of the corresponding tags in the144list is assigned.
Figure 1 exemplifies the encod-ing format.r2=0 7 .2=+ r2=++7"2 = - --_7" 2 = - - - -?
- ~ ?
"v~mr2 - -Figure 1: Tags r 2 assigned to word w2These seven values of the ri attribute aremostly sufficient o represent the structure ofeven fairly complex NPs, PPs and APs, involv-ing PP and genitive NP attachment as well ascomplex prenominal modifiers.
The only NPcomponents that are not treated here are rela-tive clauses and infinitival complements.
A Ger-man prepositional phrase and its encoding areshown in figure 2.etwa 50 000ADV CARD CARD-- ".OatR."
".O=tSgapprox.
50 000mit DMAPPR NN- -  " .D l J t  R1 ++with DMpro JahrAPPR NN- -  " .O .
t~- 0per year4.2 A Markov ian ParserThe task of the parser is to determine the bestsequence of triples (ti, ri, Ci ) for a given sequenceof part-of-speech tags (to, tl,...tn).
Since theattributes TAG, REL and CAT can take onlya finite number of values, the number of suchtriples will also be finite, and they can be usedto construct a 2-nd order Markov model.
Thetriples Si = (ti,ri,ci) are states of the model,which emits POS tags (tj) as signals.In this respect, our approach does not muchdiffer from standard part-of-speech taggingtechniques.
We simply assign the most probablesequence of structural tags S = (So, &,.
.
.
,  &)to a sequence of part-of-speech tags T =(to, tt,...,tn).
Assuming the Markov property,we obtain:argmax P( SIT) (1)S= argmax P(S) .
P(TIS)Rk= argmax r I  P(SilSi-2, S i - t )P(t i lS i )R i=1The part-of-speech tags are encoded in thestructural tag (the ti dimension), so S uniquelydetermines T. Therefore, we have P(ti\[Si) = 1if Si = (ti, ri, ci) and 0 otherwise, which simpli-?
ties calculations.4.3 Parameter  Es t imat ionThe more interesting aspect of our parser is theestimation of contextual probabilities, i.e., cal-culating the probability of a structural tag Si(the "future") conditional on its immediate pre-decessors Si- 1 and Si-2 (the "history").history futureCAT: c/-2\]ti-2J"CAT: ~_ I\]REL:  r i -1  /TAG: t i - l J"CAT: ci\]REL: riTAG: tiFigure 2: A sample structure.explained in Appendix B.The labels are In the following two subsections, we contrastthe traditional HMM estimation method andthe maximum entropy approach.1454.3.1 L inear  In terpo la t ionOne possible way of parameter estimation is touse standard HMM techniques while treatingthe triples Si = (ti, ci,ri} as atoms.
Trigramprobabilities are estimated from an annotatedcorpus by using relative frequencies r:=/(&-2,&-l,&)/ (S i -2,  Si-1)A standard method of handling sparse data is touse a linear combination of unigrams, bigrams,and trigrams/5:/5(&l&-2,&-,) = Air(&)+X2r(S/l&-1)+Aar(&l&-2,  &- l )The Ai denote weights for different context sizesand sum up to 1.
They are commonly estimatedby deleted interpolation (Brown et hi., 1992).4.3.2 FeaturesA disadvantage of the traditional method is thatit considers only full n-grams Si-n+l,  ..., Si andignores a lot of contextual information, suchas regular behaviour of the single attributesTAG, REL and CAT.
The maximum entropyapproach offers an attractive alternative in thisrespect since we are now free to define fea-tures accessing different constellations of the at-tributes.
For instance, we can abstract over oneor more dimensions, like in the context descrip-tion in figure 1.history future\[REL: 0 REL:!TAG: NNJTable 1: A partial trigram featureSuch "partial n-grams" permit a better ex-ploitation of information coming from con-texts observed in the training data.
Wesay that a feature fk defined by the triple(Mi-2, Mi-1, Mi) of attribute-value matrices isactive on a trigram context (S~_2, S~_i, S~) (i.e.,fk(S~_ 2, S~_1, S~) = 1) iff Mj unifies with theattribute-value matrix /t'I~ encoding the infor-mation contained in S~ for j = i - 2, i - 1, i. Anovel context wou ld  on average activate morefeatures than in the s tandard HMM approach,wh ich  treats the (ti, r i ,  c~> triples as atoms.The  actual features are extracted f rom thetraining corpus in the following way:  we  first de-fine a number of feature patterns that say whichattributes of a trigram context are relevant.
Allfeature pattern instantiations that occur in thetraining corpus are stored; this procedure yieldsseveral thousands of features for each pattern.After computing the weights Ai of the fea-tures occurring in the training sample, we cancalculate the contextual probability of a multi-dimensional structural tag Si following the twotags Si-2 and Si-l:1 .
e~,'~i'Ii(Si-2'&-"sd p(&l&-2, &-,) = EWe achieved the best results with 22 empir-ically determined feature patterns comprisingfull and partial n-grams, n _< 3.
These patternsare listed in Appendix A.5 App l i ca t ionsBelow, we discuss two applications of our max-imum entropy parser: treebank annotation andchunk parsing of unrestricted text.
For preciseresults, see section 6.5.1 T reebank  Annotat ionThe partial parser described here is used for cor-pus  annotation in a treebank project, cf.
(Skutet hi., 1997).
The annotation process is more in-teractive than in the Penn Treebank approach(Marcus et hi., 1994), where a sentence is firstpreprocessed bya partial parser and then editedby a human annotator.
In our method, man-ual and automatic annotation steps are closelyinterleaved.
Figure 3 exemplifies the human-computer interaction during annotation.The annotations encode four kinds of linguis-tic information: 1) parts of speech and inflec-tion, 2) structure, 3) phrasal categories (nodelabels), 4) grammatical functions (edge labels).Part-of-speech tags are assigned in a prepro-cessing step.
The automatic instantiation of la-bels is integrated into the assignment of struc-tures.
The annotator marks the words andphrases to be grouped into a new substructure,and the node and edge labels are inserted by theprogram, cf.
(Brants et al, 1997).146Das Volumen lag in besseren Zeiten bei etwaART NN VVFIN APPR ADJA NN APPR ADVOef.Ne~.Norn.Sg Neut.Nom.Sg."
3.Sg.Past.lnd Oat Comp.
'.DaLP1.SI Fem.Dat.Pt" Dat - -acht Millionen TonnenCARD NN NNFern.DatPl."
Fem.Nom.P1.
"$.NFigure 3: A chunked sentence (in better times, the volume was around eight million tons).
Gram-matical function labels: NK nominal kernel component, AC adposition, NMC number component,MO modifier.Initially, such annotation increments werejust local trees of depth one.
In this mode, theannotation of the PP bei etwa acht MillionenTonnen (\[at\] around eight million tons) involvesthree annotation steps (first the number phraseacht Millionen, then the AP, and the PP).
Eachtime, the annotator highlights the immediateconstituents of the phrase being constructed.The use of the partial parser described in thispaper makes it possible to construct he wholePP in only one step: The annotator marks thewords dominated by the PP node, and the inter-nal structure of the new phrase is assigned auto-matically.
This significantly reduces the amountof manual annotation work.
The method yieldsreliable results in the case of phrases that ex-hibit a fairly rigid internal structure.
More than88% of all NPs, PPs and APs are assigned thecorrect structure, including PP attachment andcomplex prenominal modifiers.Further examples of structures recognised bythe parser are shown in figure 4.
A more de-tailed description of the annotation mode canbe found in (Brants and Skut, 1998).5.2 NP  ChunkerApart from treebank annotation, our partialparser can be used to chunk part-of-speechtagged text into major phrases.
Unlike in theprevious application, the tool now has to deter-mine not only the internal structure, but alsothe external boundaries of phrases.
This makesthe task more difficult; especially for determin-ing FP attachment.However, if we restrict the coverage of theparser to the prenominal part of the NP/PP ,  itperforms quite well, correctly assigning almost95% of all structural tags, which corresponds toa bracketing precision of ca.
87%.6 Resu l tsIn this section, we report the results of a cross-validation of the parser carried out on the Ne-Gra Treebank (Skut et al, 1997).
The corpuswas converted into structural tags and parti-tioned into a training and a testing part (90%and 10%, respectively).
We repeated this proce-dure ten times with different partit ionings;theresults of these test runs were averaged.The weights of the features used by themaximum entropy parser were determined withthe help of the Maximum Entropy ModellingToolkit, cf.
(Ristad, 1996).
The number of fea-tures reached 120,000 for the full training cor-pus (12,000 sentences).
Interestingly, taggingaccuracy decreased after after 4-5 iterations ofImproved Iterative Scaling, so only 3 iterationswere carried out in each of the test runs.The accuracy measures employed are ex-plained as follows.tags: the percentage of structural tags with thecorrect value r~ of the REL attribute,bracket ing:  the percentage of correctly recog-nised nodes,label led bracket ing:  like bracketing, but in-cluding the syntactic ategory of the nodes,s t ruc tura l  match :  the percentage ofcorrectlyrecognised tree structures (top-level chunksonly, labelling is ignored).147D'3(?
I ?Ein geradezu pathetischer Aufruf zumART ADV ADJA NN APPRARTAn almost pathetic call for agemeinsamen KampfADJA NNjoint fight+for einen gerechten FriedenAPPR ART ADJA NNfor a just peace?t0r Oklob4r geplsnlenAPF'f~ NN AJDJAfor Oclob,~.
ptan:,',edEine Kostprobe lu l l  emem ProgrsmmART NN APtPR ART NNA samc~e ol ?
~ogt=mT~Jer ell>l~',memellenA~ ART M3JAVedo,~'~,duf~l zw,lchen Rock- ~ Chormu~kNN AP'PR TRUNC KON NNbetweert rock and ?ho~ mut?E'3?irn Nachtragshaushalt vorgeseheneAPPRART NN AOJAin the additional budget plannedOber die StellenverteilungAPPR ART NNabout the allocation of jobsin der VerwaltungAPPR ART NNin the administrationFigure 4: Examples of complex NPs and PPs correctly recognised by the parser.
In the treebankapplication, such phrases are part of larger structures.
The external boundaries (the first andthe last word of the examples) are highlighted by an annotator, the parser recognises the internalboundaries and assigns labels.6.1 T reebank  App l icat ionIn the treebank application, information aboutthe external boundaries of a phrase is suppliedby an annotator.
To imitate this situation, weextracted from the NeGra corpus all sequencesof part-of-speech tags spanned by NPs PPs,APs and complex adverbials.
Other tags wereleft out since they do not appear in chunksrecognised by the parser.
Thus, the sentenceshown in figure 3 contributed three substringsto the chunk corpus: ART NN, APPR ADJA NNand APPR ADV CARD NN NN, which would alsobe typical annotator input.
A designated sepa-rator character was used to mark chunk bound-aries.Table 2 shows the performance of the parseron the chunk corpus.148Table 2: Recall and precision results for the in-teractive annotation mode.measuretagsbracketinglab.
brack.struct, matchtotal correct129822 12343556715 4971556715 4741537942 33450recall I prec.95.1%87.7% 89.1%83.6% 84.8%88.2% 88.0%6.2 Chunk ing  Appl icat ionTable 3 shows precision and recall for the chunk-ing application, i.e., the recognition of kernelNPs and PPs in part-of-speech tagged text.Post-nominal PP attachment is ignored.
Un-like in the treebank application, there is no pre-editing by a human expert.
The absolute num-bers differ from those in table 2 because cer-tain structures are ignored.
The total numberof structural tags is higher since we now parsewhole sentences rather then separate chunks.In addition to the four accuracy measuresdefined above, we also give the percentageof chunks with correctly recognised externalboundaries (irrespective ofwhether or not thereare errors concerning their internal structure).Table 3: Recall and precision for the chunk-ing application.
The parser recognises only theprenominal part of the NP/PP  (without focusadverbs uch as also, only, etc.).measuretagsbracketinglab.
brack.struct, matchext.
boundstotal16699551912519124659946599correct15854145241438134142243833recall I prec.94.9%87.2% 86.9%84.4% 84.2%88.9% 87.6%94.1% 93.4%6.3 Compar ison  to a S tandard  TaggerIn the following, we compare the performance ofthe maximum-entropy arser with the precisionof a standard HMM-based approach trained onthe same data, but using only the frequenciesof complete trigrams, bigrams and unigrams,whose probabilities are smoothed by linear in-terpolation, as described in section 4.3.1.Figure 5 shows the percentage ofcorrectly as-signed values ri of the R.EL attribute dependingon the size of the training corpus.
Generally, themaximum entropy approach outperforms thelinear extrapolation technique by about 0.5% -1.5%, which corresponds to a 1% - 3% differencein structural match.
The difference decreases asthe size of the training sample grows.
For thefull corpus consisting of 12,000 sentences, thelinear interpolation tagger is still inferior to themaximum entropy one, but the difference in pre-cision becomes insignificant (0.2%).
Thus, themaximum entropy technique seems to particu-larly advantageous in the case of sparse data.7 ConclusionWe have demonstrated a partial parser capa-ble of recognising simple and complex NPs,PPs and APs in unrestricted German text.The maximum entropy parameter estimationmethod allows us to optimally use the con-text information contained in the training sam-ple.
On the other hand, the parser can still beviewed as a Markov model, which guaranteeshigh efficiency (processing in linear time).
Theprogram can be trained even with a relativelysmall amount of treebank data; then it can be Jused for parsing unrestricted pre-tagged text.As far as coverage is concerned, our parsercan handle recursive structures, which is an ad-vantage compared to simpler techniques uchas that described by Church (1988).
On theother hand, the Markov assumption underlyingour approach means that only strictly local de-pendencies are recognised.
For full parsing, onewould probably need non-local contextual infor-mation, such as the long-range trigrams in LinkGrammar (Della Pietra et al, 1994).Our future research will focus on exploitingmorphological nd lexical knowledge for partialparsing.
Lexical context is particularly relevantfor the recognition of genitive NP and PP at-tachment, as well as complex proper names.
Wehope that our approach will benefit from re-lated work on this subject, cf.
(Ratnaparkhiet al, 1994).
Further precision gain can alsobe achieved by enriching the structural context,e.g.
with information about the category of thegrandparent ode.8 AcknowledgementsThis work is part of the DFG Collaborative Re-search Programme 378 Resource-Adaptive.
Cog-1499~ correct959095.1%94.9%I i I I I0 2000 4000 6000 8000 10000 12000# training sentencesFigure 5: Tagging precision achieved by the maximum entropy parser (-t-) and a tagger usinglinear interpolation (--e--).
Precision is shown for different numbers of training sentences.rive Processes, Project C3 Concurrent Gram-mar Processing.Many thanks go to Eric S. Ristad.
We usedhis freely available Maximum Entropy.
Mod-elling Toolkit to estimate context probabilities.ReferencesAdam L. Berger, Stephen A. Della Pietra, andVincent J. Della Pietra.
1996.
A maxim-mentropy approach to natural language pro-cessing.
Computational Linguistics Vol.
22No.
1, 22(1):39-71.Thorsten Brants and Wojciech Skut.
1998.
Au-tomation of treebank annotation.
In Proceed-ings of NeMLaP-3, Sydney, Australia.Thorsten Brants, Wojciech Skut, and BrigitteKrenn.
1997.
Tagging grammatical func-tions.
In Proceedings of EMNLP-97, Provi-dence, RI, USA.P.
F. Brown, V. 3.
Della Pietra, Peter V:de Souza, Jenifer C. Lai, and Robert L. Mer-cer.
1992.
Class-based n-gram models ofnatural language.
Computational Linguistics,18(4):467-479.Kenneth Ward Church.
1988.
A stochasticparts program and noun phrase parser forunrestricted text.
In Proc.
Second Confer-ence on Applied Natural Language Process-ing, pages 136-143, Austin, Texas, USA.S.
Della Pietra, V. Della Pietra, J. Gillett,J.
Lafferty, H. Printz, and L. Ures.
1994.Inference and estimation of a long-range tri-gram model.
In Proceedings of the Second In-ternational Colloquium on Grammatical In-ference and Applications, Lecture Notes inArtificial Intelligence.
Springer Verlag.Ulf Hermjakob and Raymond J. Mooney.
1997.Learning parse and translation decisions fromexamples with rich contexts.
In Proceedingsof A CL-97, pages 482 - 489, Madrid, Spain.Aravind K. Joshi and B. Srinivas.
1994.
Dis-ambiguation of super parts of speech (or su-pertags).
In Proceedings COLING 9~, Kyoto,Japan.John Lafferty, Daniel Sleator, and Davy Tem-perley.
1992.
Grammatical trigrams: A prob-abilistic model of link grammar.
In Proceed-ings of the AAAI  Conference on ProbabilisticApproaches to Natural Language.Mitchell Marcus, Beatrice Santorini, andMary Ann Marcinkiewicz.
1994.
Building alarge annotated corpus of English: the PennTreebank.
In Susan Armstrong, editor, UsingLarge Corpora.
MIT Press.Adwait Ratnaparkhi, Jeff Reynar, and SalimRoukos.
1994.
A maximum entropy modelfor prepositional phrase attachment.
In Pro-ceedings of the ARPA Human Language Tech-nology Workshop, pages 250--255.Adwait Ratnaparkhi.
1996.
A maximum en-tropy model for part-of-speech tagging.
InProceedings of EMNLP- g6, Philadelphia, Pa.,150USA.Adwait Ratnaparkhi.
1997.
A linear observedtime statistical parser based on maximum en-tropy models.
In Proceedings of EMNLP-97,Providence, RI, USA.Jeffrey Reynar and Adwait Ratnaparkhi.
1997.A maximum entropy approach to identify-ing sentence boundaries.
In Proceedings ofANLP-97, Washington, DC, USA.Eric Sven Ristad, 1996.
Maximum EntropyModelling Toolkit, User's Manual.
Prince-ton University, Princeton.
Available atcmp-lg/9612005.Wojciech Skut, Brigitte Krenn, ThorstenBrants, and Hans Uszkoreit.
1997.
An anno-tation scheme for free word order languages.In Proceedings ofANLP-97, Washington, DC.Christine Thielen and Anne Schiller.
1995.Ein kleines und erweitertes Tagset fiirsDeutsche.
In Tagungsberichte d s Arbeitstr-effens Lexikon + Text 17./18.
Februar 199~,Schlofl Hohentiibingen.
Lexicographica SeriesMawr, Tiibingen.
Niemeyer.Append ix  A: Feature  Pat ternsBelow, we give the 22 n-gram feature patternsused in our experiments.tad?2ht~.
Mhistoryr, t, c r, t, cr sibl , t, c r, t, cr, C r, Ct r sibl , Cr sibl , t r, tr , t  r t ,cr ,c r t ,cr r t ,cr , t , c  r t,Cr , t , c  r t,Ct r t ,Cr t , cr t , cr t ,Cr sibl, tr, tctrfuturer ,  t ,  Cr, t, Cr, t, Cr, t, Cr, tr, tr, crr, tr~ cr, cr, t ,  Cr,  tr, cr, tr ,  tr~ cr,  t .rr ,  t ,  Cr ,  tr sibl , tThe symbols r (REL), t (TAG), and c (CAT)indicate which attributes are taken into accountwhen generating a feature according to a partic-ular pattern, r sibl is a binary-valued attributesaying whether the word under considerationand its immediate predecessor are siblings (i.e.,whether or not r = 0).Append ix  B:  TagsetsThis section contains descriptions of tags usedin this paper.
These are not complete lists.B.1 Par t -o f -Speech  TagsWe use the Stuttgart-T/.ibingen-Tagset.
Thecomplete set is described in (Thielen andSchiller, 1995).ADJA  attributive adjectiveADV adverbAPPR prepositionAPPRART preposition with determinerART articleCARD cardinal numberKON ConjunctionNE proper nounNN common ounPROAV pronominal dverbTRUNC first part of truncated nounVAFIN finite auxiliaryVAINF infinite auxiliaryVMFIN finite modal verbWFIN finite verbWPP past participle of main verbB.2 Phrasa l  Categor iesAP adjective phraseMPN multi-word proper nounN M multi token numeralNP noun phrasePP prepositional phraseS sentenceVP verb phraseB.3 Grammat ica l  Funct ionsAC adpositional case markerHD headMO modifierMNR post-nominal modifierNG negationNK noun kernelNMC numerical componentOA accusative objectOC clausal objectPNC proper noun component$8 subject151
