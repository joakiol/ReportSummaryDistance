The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 326?336,Montre?al, Canada, June 3-8, 2012. c?2012 Association for Computational LinguisticsEvaluating the Meaning of Answers to Reading Comprehension QuestionsA Semantics-Based ApproachMichael Hahn Detmar MeurersSFB 833 / Seminar fu?r SprachwissenschaftUniversita?t Tu?bingen{mhahn,dm}@sfs.uni-tuebingen.deAbstractThere is a rise in interest in the evaluation ofmeaning in real-life applications, e.g., for as-sessing the content of short answers.
The ap-proaches typically use a combination of shal-low and deep representations, but little use ismade of the semantic formalisms created bytheoretical linguists to represent meaning.In this paper, we explore the use of the un-derspecified semantic formalism LRS, whichcombines the capability of precisely repre-senting semantic distinctions with the ro-bustness and modularity needed to representmeaning in real-life applications.We show that a content-assessment approachbuilt on LRS outperforms a previous approachon the CREG data set, a freely available cor-pus of answers to reading comprehension ex-ercises by learners of German.
The use of sucha formalism also readily supports the integra-tion of notions building on semantic distinc-tions, such as the information structuring indiscourse, which we show to be useful for con-tent assessment.1 IntroductionThere is range of systems for the evaluation of shortanswers.
While the task is essentially about eval-uating sentences based on their meaning, the ap-proaches typically use a combination of shallow anddeep representations, but little use is made of the se-mantic formalisms created by theoretical linguists torepresent meaning.
One of the reasons for this is thatsemantic structures are difficult to derive because ofthe complex compositionality of natural language.Another difficulty is that form errors in the input cre-ate problems for deep processing, which is requiredfor extracting semantic representations.On the other hand, semantic representations havethe significant advantage that they on the one handabstract away from variation in the syntactic real-ization of the same meaning and on the other handclearly expose those distinctions which do make adifference in meaning.
For example, the differencebetween dog bites man and man bites dog is stillpresent in deeper syntactic or semantic representa-tions, while semantic representations abstract wayfrom meaning-preserving form variation, such as theactive-passive alternation (dog bites man ?
man wasbitten by dog).
This suggests that sufficiently robustapproaches using appropriate semantic formalismscan be useful for the evaluation of short answers.In this paper, we explore the use of Lexical Re-source Semantics (Richter and Sailer, 2003), oneof the underspecified semantic formalisms combin-ing the capability of precisely representing seman-tic distinctions with the robustness and modularityneeded to represent meaning in real-life applica-tions.
Specifically, we address the task of evaluatingthe meaning of answers to reading comprehensionexercises.We will base our experiments on the freely avail-able data set used for the evaluation of the CoMiC-DE system (Meurers et al, 2011), which does notuse semantic representations.
The data consists ofanswers to reading comprehension exercise writtenby learners of German together with questions andcorresponding target answers.3262 Related WorkThere are several systems which assess the contentof short answers.
Mitchell et al (2002) use hand-crafted patterns which indicate correct answers to aquestion.
Similarly, Nielsen et al (2009) use manu-ally annotated word-word relations or ?facets?.
Pul-man and Sukkarieh (2005) use machine learningto automatically find such patterns.
Other systemsevaluate the correctness of answers by comparingthem to one or more manually annotated target an-swers.
C-Rater (Leacock and Chodorow, 2003) andthe system of Mohler et al (2011) compare the syn-tactic parse to the parse of target answers.
A com-parison of a range of content assessment approachescan be found in Ziai et al (2012).The work in this paper is most similar to a lineof work started by Bailey and Meurers (2008), whopresent a system for automatically assessing an-swers to reading comprehension questions writtenby learners of English.
The basic idea is to alignthe student answers to a target answer using a par-allel approach with several levels on which wordsor chunks can be matched to each other.
Classifica-tion is done by a machine learning component.
TheCoMiC-DE system for German is also based on thisapproach (Meurers et al, 2011).In terms of broader context, the task is relatedto the research on Recognizing Textual Entailment(RTE) (Dagan et al, 2006).
In particular, align-ment (e.g., MacCartney et al, 2008, Sammons et al,2009) and graph matching approaches (Haghighi etal., 2005, Rus et al, 2007) are broadly similar to ourapproach.3 General Setup3.1 Empirical challenge: CREGOur experiments are based on the freely availableCorpus of Reading comprehension Exercises in Ger-man (CREG, Ott et al, 2012) .
It consists of texts,questions, target answers, and corresponding studentanswers written by learners of German.
For eachstudent answer, two independent annotators evalu-ated whether it correctly answers the question.
An-swers were only assessed with respect to meaning;the assessment is in principle intended to be inde-pendent of grammaticality and orthography.
Thetask of our system is to decide which answers cor-rectly answer the given question and which do not.3.2 Formal basis: Lexical Resource SemanticsLexical Resource Semantics (LRS) (Richter andSailer, 2003) is an underspecified semantic formal-ism which embeds model-theoretic semantic lan-guages like IL or Ty2 into constraint-based typedfeature structure formalisms as used in HPSG.
Itis formalized in the Relational Speciate ReentrancyLanguage (RSRL) (Richter, 2000).While classical formal semantics uses fully ex-plicit logical formulae, the idea of underspecifiedformalisms such as LRS is to derive semantic rep-resentations which are not completely specified andsubsume a set of possible resolved expressions, thusabstracting away from ambiguities, in particular, butnot exclusively, scope ambiguities.As an example for the representations, considerthe ambiguous example (1) from the CREG corpus.
(1) AlleallZimmerroomshabenhavenichtnoteineaDusche.shower?Not every room has a shower.?
?No room has a shower.
?The LRS representation of (1) is shown in Figure1, where INCONT (INTERNAL CONTENT) encodesthe core semantic contribution of the head, EXCONT(EXTERNAL CONTENT) the semantic representationof the sentence, and PARTS is a list containing thesubterms of the representation.??????????
?INCONT haben(e)EXCONT APARTS?A, haben(e), ?x1(B?
C),zimmer(x1), ?x2 (D ?
E), ?
F,dusche(x2), subj(e,x1), obj(e,x2)?e(haben(e) ?
subj(e,x1) ?
obj(e,x2)???????????
?Ex2(D & E)(haben(e) & subj(e,x1) & obj(e,x2))FAx1(B    C)zimmer(x1) dusche(x2) EeAFigure 1: LRS and dominance graph for (1)The representation also includes a set of subtermconstraints, visualized as a dominance graph at the327bottom of the figure.
The example (1) has severalreadings, which is reflected in the fact that the rel-ative scope of the two quantifiers and the negationis not specified.
The different readings of the sen-tence can be obtained by identifying each of themeta-variables A, .
.
.
, F with one of the subformu-las.
Meta-variables are labels that indicate where aformula can be plugged in; they are only part of theunderspecified representation and do not occur in theresolved representation.This illustrates the main strengths of an under-specified semantic formalism such as LRS for prac-tical applications.
All elements of the semantic rep-resentation are explicitly available on the PARTS list,with dominance constraints and variable bindingsproviding separate control over the structure of therepresentation.
The underspecified nature of LRSalso supports partial analyses for severely ill-formedinput or fragments, which is problematic for clas-sical approaches to semantic compositionality suchas Montague semantics (Montague, 1973).
Anotheradvantage of LRS as an underspecified formalismis that it abstracts away from the computationallycostly combinatorial explosion of possible readingsof ambiguous sentences, yet it also is able to rep-resent fine-grained semantic distinctions which aredifficult for shallow semantic methods to capture.3.3 Our general approachIn a first step, LRS representations for the studentanswer, the target answer, and the question are auto-matically derived on the basis of the part-of-speechtags assigned by TreeTagger (Schmid, 1994) and thedependency parses by MaltParser (Nivre and Hall,2005) in the way discussed in Hahn and Meurers(2011).
In this approach, LRS structures are de-rived in two steps.
First, surface representationsare mapped to syntax-semantics-interface represen-tations, which abstract away from some form vari-ation at the surface.
In the second step, rules mapthese interface representations to LRS representa-tions.
The approach is robust in that it always resultsin an LRS structure, even for ill-formed sentences.Our system then aligns the LRS representationsof the target answer and the student answer to eachother and also to the representation of the ques-tion.
Alignment takes into account both local crite-ria, in particular semantic similarity, and global cri-teria, which measure the extent to which the align-ment preserves structure on the level of variables anddominance constraints.The alignments between answers and the questionare used to determine which elements of the seman-tic representations are focused in the sense of In-formation Structure (von Heusinger, 1999; Kruijff-Korbayova?
and Steedman, 2003; Krifka, 2008), anactive field of research in linguistics addressing thequestion how the information in sentences is pack-aged and integrated into discourse.Overall meaning comparison in our approach isthen done based on a set of numerical scores com-puted from potential alignments and their quality.Given its LRS basis, we will call the system CoSeC-DE (Comparing Semantics in Context).4 Aligning Meaning RepresentationsThe alignment is done on the level of the PARTS lists,on which all elements of the semantic representationare available:Definition 1.
An alignment a between two LRSrepresentations S and T with PARTS lists pn1 andqm1 is an injective partial function from {1,...,n} to{1,...,m}.Requiring a to be injective ensures that every ele-ment of one representation can be aligned to at mostone element of the other representation.
Note thatthis definition is symmetrical in the sense that thedirection can be inverted simply by inverting the in-jective alignment function.To automatically derive alignments, we define amaximization criterion which combines three fac-tors measuring different aspects of alignment qual-ity.
In addition to i) the similarity of the align-ment links, the quality Q of the alignment a takesinto account the structural correspondence betweenaligned elements by evaluating the consistency ofalignments ii) with respect to the induced variablebindings ?
and, and iii) with respect to dominanceconstraints:Q(a, ?|S, T ) = linksScore(a|S, T )?
variableScore(?)?
dominanceScore(a|S, T )(1)The approach thus uses a deep representation ab-stracting away from the surface, but the meaning328comparison approach on this deep level is flat, yetat the same time is able to take into account struc-tural criteria.
In consequence, the approach is mod-ular because it uses the minimal building blocks ofsemantic representations, but is able to make use ofthe full expressive power of the semantic formalism.4.1 Evaluating the Quality of Alignment LinksThe quality of an alignment link between two ex-pressions is evaluated by recursively evaluating thesimilarity of their components.
In the base case,variables can be matched with any variable of thesame semantic type:sim(x?
, y? )
= 1Meta-variables can be matched with any meta-variable of the same semantic type:sim(A?
,B? )
= 1For predicates with arguments, both the predicatename and the arguments are compared:sim(P1(ak1), P2(bk1)) =sim(P1, P2) ?k?i=1sim(ai, bi)(2)If the predicates have different numbers of argu-ments, similarity is zero.
Linguistically well-knownphenomena where the number of arguments of se-mantically similar predicates differ do not cause aproblem for this definition, because semantic rolesare linked to the verbal predicate via grammaticalfunction terms such as subj and obj predicating overa Davidsonian event variable, as in Figure 1.1For formulas with generalized quantifiers, thequantifiers, the variables, the scopes and the restric-tors are compared:sim(Q1x1(?
?
?
), Q2x2(?
?
?))
=sim(Q1, Q2) ?
sim(x1, x2)?sim(?, ?)
?
sim(?, ?
)(3)Lambda abstraction is dealt with analogously.The similarity sim(P1, P2) of names of predicatesand generalized quantifiers takes into account sev-eral sources of evidence and is estimated as the max-imum of the following quantities:1In this paper, we simply use grammatical function namesin place of semantic role labels in the formulas.
A more sophis-ticated, real mapping from syntactic functions to semantic rolescould usefully be incorporated.As a basic similarity, the Levenshtein distancenormalized to the interval [0,1] (with 1 denotingidentity and 0 total dissimilarity) is used.
This ac-counts for the high frequency of spelling errors inlearner language.Synonyms in GermaNet (Hamp and Feldweg,1997) receive the score 1.For numbers, the (normalized) difference|n1?n2|max(n1,n2)is used.For certain pairs of dissimilar elements which be-long to the same category, constant costs are de-fined.
This encourages the system to align these el-ements, unless the structural factors, i.e., the qualityof the unifier and the consistency with dominanceconstraints, discourage this.
Such constants are de-fined for pairs of grammatical function terms.
Otherconstants are defined for pairs of numerical termsand for pairs of terms encoding affirmative and neg-ative natural language expressions and logical nega-tion.Having defined how to compute the quality forsingle alignment links, we still need to define how tocompute the combined score of the alignment links,which we define to be the sum of the qualities of thelinks:linksScore(a|pn1 , qm1 ) =n?k=1{sim(pk, qa(k)) if a(k) is defined,?NULL else.
(4)The quality of a given overall alignment thus isdetermined by the quality of the alignment links ofthe PARTS elements which are aligned.
For thosePARTS elements not aligned, a constant cost ?NULLmust be paid, which, however, may be smaller thana costly alignment link in another overall alignment.4.2 Evaluating UnifiersAlignments between structurally corresponding se-mantic elements should be preferred.
For situationsin which they structurally do not correspond, thismay have the effect of dispreferring the pairing ofelements which in terms of the words on the surfaceare identical or very similar.
Consider the sentencepair in (2), where Frau in (2a) syntactically corre-sponds to Mann in (2b).329(2) a. EineaFrauwomansiehtseeseinenaMannman?A woman sees a man.?b.
EinaMannmansiehtseeseineaFrauwoman?A man sees a woman.
?On the level of the semantic representation, thisis reflected in the correspondence between the vari-ables x1 and y1, both of which occur as argumentsof subj, as shown in Figure 2.Ex2x(Dx &&&&&&)&habensujD&&& be,1oDxF&&& beEx2x(DxAu((D&&& bB&&&&&&)&habB,1oDxF&&& bB&&&&&&C&zaiBnsujD&&& iBEj1oDxF&&& iB&&&&&&C&zaieAu((D&&& ieEj1oDxF&&& ieFigure 2: An excerpt of an alignment between the PARTSlists of (2a) on the left and (2b) on the right.
Dotted align-ment links are the ones only plausible on the surface.Our solution to capture this distinction is to usethe concept of a unifier, well-known from logic pro-gramming.
A unifier for terms ?, ?
is a substitu-tion ?
such that ??
= ??.
Every alignment in-duces a unifier, which unifies all variables which arematched by the alignment.The alignment in Figure 2 (without the dottedlinks) induces the unifier?1 = [(x1, y1) 7?
z1; (x2, y2) 7?
z2].If links between the matching predicates mann andfrau, respectively, are added, one also has to unify x1with y2 and x2 with y1 and thus obtains the unifier?2 = [(x1, x2, y1, y2) 7?
z].Intuitively, a good unifier unifies only variableswhich correspond to the same places in the seman-tic structures to be aligned.
In the case of Figure 2,choosing an alignment including the dotted links re-sults in the unifier ?2 which unifies x1 and x2 ?
yetthey are structurally different, with one belonging tothe subject and the other one to the object.In general, it can be expected that an alignmentwhich preserves the structure will not unify two dis-tinct variables from the same LRS representation,since they are known to be structurally distinct.
Sowe want to capture the information loss resultingfrom unification.
This intuition is captured by (5),which answers the following question: Given somevariable z in a unified expression, how many addi-tional bits do we need on average2 to encode theoriginal pair of variables x, y in the PARTS lists pand q, respectively?H(?)
=1Zp,q?z?Ran(?)W?
(z) log(W?
(z)) (5)where W?
(z) = |{x ?
V ar(p)|x?
= z}|?
|{y ?
V ar(q)|y?
= z}|(6)Zp,q = |V ar(p)| ?
|V ar(q)| (7)The value of a unifier ?
is then defined as follows:variableScore(?)
=(1?H(?)H?
)k(8)where k is a numerical parameter with 0 ?
k ?
1and H?
is a (tight) upper bound on H(?)
obtainedby evaluating the worst unifier, i.e., the unifier thatunifies all variables H?
= log(Zp,q).4.3 Evaluating consistency with dominanceconstraintsWhile evaluating unifiers ensures that alignmentspreserve the structure on the level of variables, it isalso important to evaluate their consistency with thedominance structure of the underspecified semanticrepresentations, such as the one we saw in Figure 1.Consider the following pair:(3) a. PeterPeterkommtcomesundandHansHanskommtcomesnicht.not?Peter comes and Hans does not come.?b.
PeterPeterkommtcomesnichtnotundandHansHanskommt.comes?Peter does not come and Hans comes.
?While the words and also the PARTS lists of thesentences are identical, they clearly differ in mean-ing.
Figure 3 on the next page shows the LRS domi-nance graphs for the two sentences together with an2For simplicity, it is assumed that every combination inV ar(p)?
V ar(q) occurs the same number of times.330alignment between them.
The semantic differencebetween the two sentences is reflected in the posi-tion of the negation in the dominance graph: whileit dominates kommen(e2) ?
subj(e2,hans) in (3a), itdominates kommen(f1) ?
subj(f1,peter) in (3b).To account for this issue, we evaluate the consis-tency of the alignment with respect to dominanceconstraints.
An alignment a is optimally consistentwith respect to dominance structure if it defines anisomorphism between its range and its domain withrespect to the relation / ?is dominated by?.Figure 3 shows an alignment which aligns allmatching elements in (3b) and (3a).
The link be-tween the negations violates the isomorphism re-quirement: the negation dominates kommen(e2) ?subj(e2,hans) in (3a), while it does not dominate thecorresponding elements in (3b).
An optimally con-sistent alignment will thus leave the negations un-aligned.
Unaligned negations can later be used inthe overall meaning comparison as strong evidencethat the sentences do not mean the same.dominanceScore measures how ?close?
a is todefining an isomorphism.
We use the following sim-ple score, which is equal to 1 if and only if a definesan isomorphism:dominanceScore(a|S, T ) =11 +?i,j?Dom(a) ????
?pi / pj ,pi .
pj ,qa(i) / qa(j),qa(i) .
qa(j)????
(9)where ?
is a function taking four truth values as itsarguments.
It measures the extent to which the iso-morphism requirement is violated by an alignment.?
(t1, t2, t1, t2) is defined as 0 because there is noviolation if the dominance relation between pi andpj is equal to that between the elements they arealigned with, qa(i) and qa(j).
For other combinationsof truth values, ?
should be set to values greater thanzero, empirically determined on a development set.4.4 Finding the best alignmentBecause of the use of non-local criteria in the max-imization criterion Q(a, ?|S, T ) defined in equation(1), an efficient method is needed to find the align-ment maximizing the criterion.
We exploit the struc-ture inherent in the set of possible alignments to ap-ply the A* algorithm (Russel and Norvig, 2010).
Wefirst generalize the notion of an alignment.Definition 2.
A partial alignment of order i is anindex i together with an alignment which does nothave alignment links for any pj with j > i.A partial alignment can be interpreted as a classof alignments which agree on the first i elements.Definition 3.
The refinements ?
(a) of the partialalignment a (of order i) are the partial alignments bsuch that (1) b is of order i+1, and (2) a and b agreeon {1, ..., i}.Intuitively, refinements of an alignment of order iare obtained by deciding how to align element i+1.?
induces a tree over the set of partial alignments,whose leaves are exactly the complete alignments.A simple optimistic estimate for the value of allcomplete descendants of an alignment a of order i isgiven by the following expression:optimistic(a, ?|S, T ) = variableScore(?
)?dominanceScore(a, S, T )?
(linksScorei(a, ?|p, q)+n?k=i+1heuristic(k, a, pn1 , qm1 ))(10)where linksScorei is the sum in (4) restrictedto 1 ?
k ?
i, and heuristic(k, a, pn1 , qm1 ) is0 if pk is aligned and a simple, optimistic esti-mate for the quality of the best possible align-ment link containing pk if pk is unaligned.
Itis estimated as the maximum of ?NULL andmax{sim(pk, qj) | qj unaligned}.The estimate in (10) is optimistic in the sensethat it provides an upper bound on the values of allcomplete alignments below a.
It defines a mono-tone heuristic and thus allows complete and optimalsearch using the A* algorithm.
To obtain an efficientimplementation, additional issues such as the orderof elements in the PARTS lists were taken care of.
Asthey do not play a role for the conceptualization ofour approach, they are not discussed here.The crucial part at this point of the discussionis that the A* search can determine the best align-ment between two PARTS lists.
As mentioned inthe overview in section 3.3, we compute three such331Ex2(2D &)hEa2(2b &)ensuu)jE)e ,1oFE)eAB)C)znsuu)jE)h ,1oFE)hAi mj,r2(2d&Ec2(2 &hE 2(2!
&ensuu)jEe ,1oFEeAB)C)znsuu)jEh ,1oFEhAimj,"2(2#$Figure 3: Alignment between the dominance graphs of (3a) and (3b).
The red dotted link violates isomorphism.alignments: between the student and the target an-swer, between the question and the student answer,and between the question and the target answer.5 From Alignment to Meaning ComparisonBased on the three alignments computed using thejust discussed algorithm, we now explore differentoptions for computing whether the student answeris correct or not.
We discuss several alternatives,all involving the computation of a numerical scorebased on the alignments.
For each of these scores, athreshold is empirically determined, over which thestudent answer is considered to be correct.Basic Scores The simplest score, ALIGN, is com-puted by dividing the alignment quality Q betweenthe student answer and the target answer as definedin equation (1) by the number of elements in thesmaller PARTS list.
Two other scores are computedbased on the number of alignment links betweenstudent and target answer, which for the EQUAL-Student score is divided by the number of elementsof the PARTS list of the student answer, and for theEQUAL-Target score by those of the target answer.For dealing with functional elements, i.e., predi-cates like subj, obj, quantifiers and the lambda op-erator, we tried out three options.
The straight caseis the one mentioned above, treating all elements onthe PARTS list equally (EQUAL).
As a second op-tion, to see how important the semantic relations be-tween words are, and how much is just the effect ofthe elements themselves, we defined a score whichignores functional elements (IGNORE).
A third pos-sibility is to weight elements so that functional andnon-functional ones differ in impact (WEIGHTED).Each of the three scores (EQUAL, IGNORE,WEIGHTED) is either divided by the number of el-ements of the PARTS list of the student answer orthe target, resulting in six scores.
In addition, threemore scores result from computing the average ofthe student and target answer scores.Information Structure Scores Basing meaningcomparison on actual semantic representation alsoallows us to directly take into account InformationStructure as a structuring of the meaning of a sen-tence in relation to the discourse.
Bailey and Meur-ers (2008), Meurers et al (2011), and Mohler et al(2011) showed that excluding those parts of the an-swer which are mentioned (given) in the questiongreatly improves classification accuracy.
Meurerset al (2011) argue that the relevant linguistic as-pect is not whether the material was mentioned inthe question, but the distinction between focus andbackground in Information Structure (Krifka, 2008).The focus essentially is the information in the an-swer which selects between the set of alternativesthat the question raises.This issue becomes relevant, e.g., in the case of?or?
questions, where the focused information de-termining whether the answer is correct is explicitlygiven in the question.
This is illustrated by the ques-tion in (4) with target answer (5a) and student an-swer (5b), from the CREG corpus.
While all wordsin the answers are mentioned in the question, thepart of the answers which actually answer the ques-tion are the focused elements shown in boldface.
(4) IstisdietheWohnungflatinineinemaAltbauold buildingoderorNeubau?new building(5) a. DietheWohnungflatistisinineinemaAltbau.old.buildingb.
DietheWohnungflatistisinineinemaNeubau.new.building332To realize a focus-based approach, one naturallyneeds a component which automatically identifiesthe focus of an answer in a question-answer pair.
Asa first approximation, this currently is implementedby a module which marks the elements of the PARTSlists of the answers for information structure.
El-ements which are not aligned to the question aremarked as focused.
Furthermore, in answers to ?or?questions, it marks as focused all elements whichare aligned to the semantic contribution of a wordbelonging to one of the alternatives.
?Or?
questionsare recognized by the presence of oder (?or?)
and theabsence of a wh-word.While previous systems simply ignored all wordsgiven in the question during classification, our sys-tem aligns all elements and recognizes givennessbased on the alignments.
Therefore, givenness isstill recognized if the surface realization is differ-ent.
Furthermore, material which incidentally is alsofound in the question, but which is structurally dif-ferent, is not assumed to be given.Scores using information structure were obtainedin the way of the BASIC scores but counting onlythose elements which are recognized as focused(FOCUS).
For comparison, we also used the samescores with givenness detection instead of focus de-tection, i.e., in these scores, all elements aligned tothe question were excluded (GIVEN).Annotating semantic rather than surface represen-tations for information structure has the advantagethat the approach can be extended to cover focus-ing of relations in addition to focusing of entities.The general comparison approach also is compat-ible with more sophisticated focus detection tech-niques capable of integrating a range of cues, in-cluding syntactic cues and specialized constructionssuch as clefts, or prosodic information for spokenlanguage answers ?
an avenue we intend to pursuein future research.Dissimilar score We also explored one special-ized score paying particular attention to dissimi-lar aligned elements, as mentioned in section 4.1.Where a focused number is aligned to a differentnumber, or a focused polarity expression is alignedto the opposite polarity, or a logical negation is notaligned, then 0 is returned as score, i.e., the studentanswer is false.
In all other cases, the DISSIMILARscore is identical to the WEIGHTED-Average FOCUSscore, i.e., the score based on the average of the stu-dent and target scores with weighting and focus de-tection.6 Experiments6.1 CorpusWe base the experiments on the 1032 answers fromthe CREG corpus which are used in the evaluationof the CoMiC-DE system reported by Meurers et al(2011).
The corpus is balanced, i.e., the numbers ofcorrect and of incorrect answers are the same.
It con-tains only answers where the two human annotatorsagreed on the binary label.6.2 SetupThe alignment algorithm contains a set of numeri-cal parameters which need to be determined empir-ically, such as ?NULL and the function ?.
In a firststep, we optimized these parameters and the weightsused in the WEIGHTED scores using grid search ona development set of 379 answers.
These answersare from CREG, but do not belong to the 1032 an-swers used for testing.
We used the accuracy of theDISSIMILAR score as performance metric.In our experiment, we explored each score sep-arately to predict which answers are correct andwhich not.
For each score, classification is basedon a threshold which is estimated as the arithmeticmean of the average score of correct and the averagescore of incorrect answers.
Training and testing wasperformed using the leave-one-out scheme (Weissand Kulikowski, 1991).
When testing on a particularanswer, student answers answering the same ques-tion were excluded from training.6.3 ResultsFigure 4 shows the accuracy results obtained in ourexperiments together with the result of CoMiC-DEon the same dataset.
With an accuracy of up to86.3%, the WEIGHTED-Average FOCUS score out-perform the 84.6% reported for CoMiC-DE (Meur-ers et al, 2011) on the same dataset.
This is remark-able given that CoMiC-DE uses several (but com-parably shallow) levels of linguistic abstraction forfinding alignment links, whereas our approach is ex-clusively based on the semantic representations.333Score BASIC GIVEN FOCUSALIGN 77.1EQUALStudent 69.8 75.3 75.2Target 70.0 75.5 75.2Average 76.6 80.8 80.7IGNOREStudent 75.8 80.1 80.3Target 77.2 82.2 82.3Average 79.8 84.7 84.9WEIGHTEDStudent 75.0 80.6 80.7Target 76.1 83.3 83.3Average 80.9 86.1 86.3DISSIMILAR 85.9CoMiC-DE 84.6Figure 4: Classification accuracy of CoSeC-DEThe fact that WEIGHTED-Average outperformsthe IGNORE-Average scores shows that the inclu-sion of functional element (i.e., predicates like subj,obj), which are not available to approaches basedon aligning surface strings, improves the accuracy.3On the other hand, the lower performance of EQUALshows that functional elements should be treated dif-ferently from content-bearing elements.Of the 13.7% answers misclassified byWEIGHTED-Average FOCUS, 53.5% are falsenegatives and 46.5% are false positives.We also investigated the impact of grammaticalityon the result by manually annotating a sample of 220student answers for grammatical well-formedness,66% of which were ungrammatical.
On this sam-ple, grammatical and ungrammatical student an-swers were evaluated with essentially the same ac-curacy (83% for ungrammatical answers, 81% forgrammatical answers).The decrease in accuracy of the COMBINED scoreover the best score can be traced to some yes-no-questions which have an unaligned negation but arecorrect.
On the other hand, testing only on answerswith focused numbers results in an accuracy of 97%.The performance of GIVEN and FOCUS scores3We also evaluated IGNORE scores using parameter valuesoptimized for these scores, but their performance was still be-low those of the corresponding WEIGHTED-Average scores.compared to BASIC confirms that information struc-turing helps in targeting the relevant parts of the an-swers.
Since CoMiC-DE also demotes given mate-rial, the better GIVEN results of our approach mustresult from other aspects than the information struc-ture awareness.
Unlike previous approaches, the FO-CUS scores support reference to the material focusedin the answers.
However, since currently the FOCUSscores only differs from the GIVEN scores for alter-native questions, and the test corpus only containsseven answers to such ?or?
questions, we see no se-rious quantitative difference in accuracy between theFOCUS and GIVENNESS results.While the somewhat lower accuracy of the scoreALIGN shows that the alignment scores are not suf-ficient for classification, the best-performing scoresdo not require much additional computation and donot need any information that is not in the align-ments or the automatic focus annotation.7 Future WorkThe alert reader will have noticed that our ap-proach currently does not support many-to-manyalignments.
As is known, e.g., from phrase-basedmachine translation, this is an interesting avenue fordealing with non-compositional expressions, whichwe intend to explore in future work.
The align-ment approach can be adapted to such alignmentsby adding a factor measuring the quality of many-to-many links to linkScore (4) and optimistic (10).8 ConclusionWe presented the CoSeC-DE system for evaluatingthe content of answers to reading comprehensionquestions.
Unlike previous content assessment sys-tems, it is based on formal semantics, using a novelapproach for aligning underspecified semantic rep-resentations.
The approach readily supports the in-tegration of important information structural differ-ences in a way that is closely related to the informa-tion structure research in formal semantics and prag-matics.
Our experiments showed the system to out-perform our shallower multi-level system CoMiC-DE on the same CREG-1032 data set, suggestingthat formal semantic representations can indeed beuseful for content assessment in real-world contexts.334AcknowledgementsWe are grateful to the three anonymous BEA re-viewers for their very encouraging and helpful com-ments.ReferencesStacey Bailey and Detmar Meurers.
2008.
Diagnos-ing meaning errors in short answers to reading com-prehension questions.
In Joel Tetreault, Jill Burstein,and Rachele De Felice, editors, Proceedings of the 3rdWorkshop on Innovative Use of NLP for Building Edu-cational Applications (BEA-3) at ACL?08, pages 107?115, Columbus, Ohio.Ido Dagan, Oren Glickman, and Bernardo Magnini.2006.
The PASCAL Recognising Textual Entail-ment Challenge.
In J. Quionero-Candela, I. Dagan,B.
Magnini, and F. d?Alch Buc, editors, MachineLearning Challenges, volume 3944 of Lecture Notesin Computer Science, pages 177?190.
Springer.Aria D. Haghighi, Andrew Y. Ng, and Christopher D.Manning.
2005.
Robust textual inference via graphmatching.
In Proceedings of the conference on Hu-man Language Technology and Empirical Methods inNatural Language Processing, pages 387?394.
Asso-ciation for Computational Linguistics.Michael Hahn and Detmar Meurers.
2011.
On deriv-ing semantic representations from dependencies: Apractical approach for evaluating meaning in learnercorpora.
In Kim Gerdes, Eva Hajicov, and Leo Wan-ner, editors, Depling 2011 Proceedings, pages 94?103,Barcelona.Birgit Hamp and Helmut Feldweg.
1997.
GermaNet - aLexical-Semantic Net for German.
In Proceedings ofACL workshop Automatic Information Extraction andBuilding of Lexical Semantic Resources for NLP Ap-plications, pages 9?15.Manfred Krifka.
2008.
Basic notions of informationstructure.
Acta Linguistica Hungarica, 55(3):243?276.Ivana Kruijff-Korbayova?
and Mark Steedman.
2003.Discourse and information structure.
Journal of Logic,Language and Information (Introduction to the SpecialIssue), pages 249?259.Claudia Leacock and Martin Chodorow.
2003.
C-rater:Automated scoring of short-answer questions.
Com-puters and the Humanities, 37(4):389?405.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment model fornatural language inference.
In Proceedings of the Con-ference on Empirical Methods in Natural LanguageProcessing, pages 802?811.
Association for Compu-tational Linguistics.Detmar Meurers, Ramon Ziai, Niels Ott, and JaninaKopp.
2011.
Evaluating answers to reading compre-hension questions in context: Results for German andthe role of information structure.
In Proceedings of theTextInfer 2011 Workshop on Textual Entailment, pages1?9, Edinburgh, Scotland, UK, July.
Association forComputational Linguistics.Tom Mitchell, Terry Russell, Peter Broomhead, andNicola Aldridge.
2002.
Towards robust computerisedmarking of free-text responses.
In Proceedings ofthe 6th International Computer Assisted Assessment(CAA) Conference.Michael Mohler, Razvan Bunescu, and Rada Mihalcea.2011.
Learning to grade short answer questions usingsemantic similarity measures and dependency graphalignments.
In Proceedings of the 49th Annual Meet-ing of the Association for Comnputational Linguistics,pages 752?762.Richard Montague.
1973.
The Proper Treatment of Qun-tification in Ordinary English.
In Jaakko Hintikka,Julius Moravcsik, and Patrick Suppes, editors, Ap-proaches to Natural Language, pages 221?242.
Rei-del, Dordrecht.Rodney D. Nielsen, Wayne Ward, and James H. Martin.2009.
Recognizing entailment in intelligent tutoringsystems.
Natural Language Engineering, 15(4):479?501.Joakim Nivre and Johan Hall.
2005.
Maltparser: Alanguage-independent system for data-driven depen-dency parsing.
In Proceedings of the Fourth Workshopon Treebanks and Linguistic Theories, pages 13?95.Niels Ott, Ramon Ziai, and Detmar Meurers.
2012.
Cre-ation and analysis of a reading comprehension exercisecorpus: Towards evaluating meaning in context.
InThomas Schmidt and Kai Wrner, editors, MultilingualCorpora and Multilingual Corpus Analysis, HamburgStudies in Multilingualism (HSM).
Benjamins, Ams-terdam.Stephen G. Pulman and Jana Z. Sukkarieh.
2005.
Au-tomatic short answer marking.
In Proceedings of the2nd Workshop on Building Educational ApplicationsUsing NLP, pages 9?16.Frank Richter and Manfred Sailer.
2003.
Basic Conceptsof Lexical Resource Semantics.
In Arnold Beckmannand Norbert Preining, editors, ESSLLI 2003 ?
CourseMaterial I, volume 5 of Collegium Logicum, pages 87?143, Wien.
Kurt Go?del Society.Frank Richter.
2000.
A Mathematical Formalismfor Linguistic Theories with an Application in Head-Driven Phrase Structure Grammar.
Phil.
dissertation,Eberhard-Karls-Universita?t Tu?ingen.Vasile Rus, Arthur Graesser, and Kirtan Desai.
2007.Lexico-syntactic subsumption for textual entailment.335Recent Advances in Natural Language Processing IV:Selected Papers frp, RANLP 2005, pages 187?196.Stuart Russel and Peter Norvig.
2010.
Artificial Intelli-gence.
A Modern Approach.
Pearson, 2nd edition.Mark Sammons, V.G.Vinod Vydiswaran, Tim Vieira,Nikhil Johri, Ming-Wei Chang, Dan Goldwasser,Vivek Srikumar, Gourab Kundu, Yuancheng Tu, KevinSmall, Joshua Rule, Quang Do, and Dan Roth.
2009.Relation Alignment for Textual Entailment Recogni-tion.
In Text Analysis Conference (TAC).Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In Proceedings of the In-ternational Conference on New Methods in LanguageProcessing, pages 44?49.Klaus von Heusinger.
1999.
Intonation and InformationStructure.
The Representation of Focus in Phonologyand Semantics.
Habilitationssschrift, Universita?t Kon-stanz, Konstanz, Germany.Sholom M. Weiss and Casimir A. Kulikowski.
1991.Computer systems that learn.
Morgan Kaufmann, SanMateo, CA.Ramon Ziai, Niels Ott, and Detmar Meurers.
2012.Short answer assessment: Establishing links betweenresearch strands.
In Proceedings of the 7th Workshopon Innovative Use of NLP for Building EducationalApplications (BEA-7) at NAACL-HLT 2012, Montreal.336
