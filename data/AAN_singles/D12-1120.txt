Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1313?1323, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsBiased Representation Learning for Domain AdaptationFei Huang , Alexander YatesTemple UniversityComputer and Information Sciences324 Wachman HallPhiladelphia, PA 19122{fhuang,yates}@temple.eduAbstractRepresentation learning is a promising tech-nique for discovering features that allow su-pervised classifiers to generalize from a sourcedomain dataset to arbitrary new domains.
Wepresent a novel, formal statement of the rep-resentation learning task.
We argue that be-cause the task is computationally intractablein general, it is important for a representa-tion learner to be able to incorporate expertknowledge during its search for helpful fea-tures.
Leveraging the Posterior Regularizationframework, we develop an architecture for in-corporating biases into representation learn-ing.
We investigate three types of biases, andexperiments on two domain adaptation tasksshow that our biased learners identify signif-icantly better sets of features than unbiasedlearners, resulting in a relative reduction in er-ror of more than 16% for both tasks, with re-spect to existing state-of-the-art representationlearning techniques.1 IntroductionSupervised natural language processing (NLP) sys-tems have been widely used and have achieved im-pressive performance on many NLP tasks.
Howev-er, they exhibit a significant drop-off in performancewhen tested on domains that differ from their train-ing domains.
(Gildea, 2001; Sekine, 1997; Pradhanet al 2007) One major cause for poor performanceon out of-domain texts is the traditional representa-tion used by supervised NLP systems (Ben-David etal., 2007).
Most systems depend on lexical features,which can differ greatly between domains, so thatimportant words in the test data may never be seenin the training data.
The connection between word-s and labels may also change across domains.
Forinstance, ?signaling?
appears only as a present par-ticiple (VBG) in WSJ text (as in, ?signaling that...?
),but predominantly as a noun (as in ?signaling path-way?)
in biomedical text.Recently, several authors have found that learningnew features based on distributional similarity cansignificantly improve domain adaptation (Blitzer etal., 2006; Huang and Yates, 2009; Turian et al2010; Dhillon et al 2011).
This framework is at-tractive for several reasons: experimentally, learnedfeatures can yield significant improvements over s-tandard supervised models on out-of-domain test-s.
Moreover, since the representation-learning tech-niques are unsupervised, they can easily be appliedto arbitrary new domains.
There is no need to supplyadditional labeled examples for each new domain.Traditional representations still hold one signif-icant advantage over representation-learning, how-ever: because features are hand-crafted, these rep-resentations can readily incorporate the linguisticor domain expert knowledge that leads to state-of-the-art in-domain performance.
In contrast, the on-ly guide for existing representation-learning tech-niques is a corpus of unlabeled text.To address this shortcoming, we introducerepresentation-learning techniques that incorporatea domain expert?s preferences over the learned fea-tures.
For example, out of the set of all possi-ble distributional-similarity features, we might pre-fer those that help predict the labels in a labeledtraining data set.
To capture this preference, wemight bias a representation-learning algorithm to-wards features with low joint entropy with the labelsin the training data.
This particular biased form of1313representation learning is a type of semi-supervisedlearning that allows our system to learn task-specificrepresentations from a source domain?s training da-ta, rather than the single representation for all tasksproduced by current, unsupervised representation-learning techniques.We present a novel formal statement of represen-tation learning, and demonstrate that it is computa-tionally intractable in general.
It is therefore criti-cal for representation learning to be flexible enoughto incorporate the intuitions and knowledge of hu-man experts, to guide the search for representationsefficiently and effectively.
Leveraging the Posteri-or Regularization framework (Ganchev et al 2010),we present an architecture for learning representa-tions for sequence-labeling tasks that allows for bi-ases.
In addition to a bias towards task-specific rep-resentations, we investigate a bias towards repre-sentations that have similar features across domain-s, to improve domain-independence; and a bias to-wards multi-dimensional representations, where d-ifferent dimensions are independent of one another.In this paper, we focus on incorporating the bias-es with HMM-type representations (Hidden MarkovModel).
However, this technique can also be ap-plied to other graphical model-based representationswith little modification.
Our experiments show thaton two different domain-adaptation tasks, our biasedrepresentations improve significantly over unbiasedones.
In a part-of-speech tagging experiment, ourbest model provides a 25% relative reduction in er-ror over a state-of-the-art Chinese POS tagger, anda 19% relative reduction in error over an unbiasedrepresentation from previous work.The next section describes background and previ-ous work.
Section 3 introduces our framework forlearning biased representations.
Section 4 describeshow we estimate parameters for the biased objectivefunctions efficiently.
Section 5 details our experi-ments and results, and section 6 concludes and out-lines directions for future work.2 Background and Previous Work2.1 Terminology and NotationA representation is a set of features that describe da-ta points.
Formally, given an instance set X , it is afunctionR : X ?
Y for some suitable space Y (of-ten Rd), which is then used as the input space for aclassifier.
For instance, a traditional representationfor POS tagging over vocabulary V would include(in part) |V | dimensions, and would map a word to abinary vector with a 1 in only one of the dimensions.By a structured representation, we mean a functionR that incorporates some form of joint inference.
Inthis paper, we use Viterbi decoding of variants ofHidden Markov Models (HMMs) for our structuredrepresentations, although our techniques are appli-cable to arbitrary (Dynamic) Bayes Nets.
A domainis a probability distribution D over the instance setX ; R(D) denotes the induced distribution over Y .In domain adaptation tasks, a learner is given sam-ples from a source domain DS , and is evaluated onsamples from a target domain DT .2.2 Theoretical BackgroundBen-David et al(2010) give a theoretical analysisof domain adaptation which shows that the choiceof representation is crucial.
A good choice is onethat minimizes error on the training data, but equallyimportant is that the representation must make datafrom the two domains look similar.
Ben-David et alshow that for every hypothesis h, we can provablybound the error of h on the target domain by its erroron the source domain plus a measure of the distancebetween DS and DT :Ex?DTL(x,R, f, h) ?
Ex?DSL(x,R, f, h)+ d1(R(DS), R(DT ))where L is a loss function, f is the target function,and the variation divergence d1 is given byd1(D,D?)
= 2 supB?B|PrD[B]?
PrD?
[B]| (1)where B is the set of measurable sets under D,D?.2.3 Problem FormulationBen-David et als theory provides learning bound-s for domain adaptation under a fixed R. We nowreformulate this theory to define the task of repre-sentation learning for domain adaptation as the fol-lowing optimization problem: given a set of unla-beled instances US drawn from the source domainand unlabeled instances UT from the target domain,as well as a set of labeled instances LS drawn from1314the source domain, identify a function R?
from thespace of possible representationsR:R?
= argminR?R{minh?H(Ex?DSL(x,R, f, h))+ d1(R(DS), R(DT ))}(2)Unlike most learning problems, where the repre-sentation R is fixed, this problem formulation in-volves a search over the space of representation-s and hypotheses.
The equation also highlights animportant underlying tension: the best representa-tion for the source domain would naturally includedomain-specific features, and allow a hypothesis tolearn domain-specific patterns.
We are aiming, how-ever, for the best general classifier, that happens tobe trained on training data from one or a few do-mains.
Domain-specific features would contributeto distance between domains, and to classifier errorson data taken from unseen domains.
By optimizingfor this combined objective function, we allow theoptimization method to trade off between featuresthat are best for classifying source-domain data andfeatures that allow generalization to new domains.Naturally, the objective function in Equation 2 iscompletely intractable.
Just finding the optimal hy-pothesis for a fixed representation of the training da-ta is intractable for many hypothesis classes.
Andthe d1 metric is intractable to compute from samplesof a distribution, although Ben-David et alproposesome tractable bounds (2007; 2010).
We view Equa-tion 2 as a high-level goal rather than a computableobjective.
We leverage prior knowledge to bias therepresentation learner towards attractive regions ofthe representations space R, and we develop effi-cient, greedy optimization techniques for learningeffective representations.2.4 Previous WorkThere is a long tradition of research on representa-tions for NLP, mostly falling into one of three cat-egories: 1) vector space models and dimensionalityreduction techniques (Salton and McGill, 1983; Tur-ney and Pantel, 2010; Sahlgren, 2005; Deerwester etal., 1990; Honkela, 1997) 2) using structured repre-sentations to identify clusters based on distributionalsimilarity, and using those clusters as features (Linand Wu, 2009; Candito and Crabbe?, 2009; Huangand Yates, 2009; Ahuja and Downey, 2010; Turi-an et al 2010; Huang et al 2011); 3) and struc-tured representations that induce multi-dimensionalreal-valued features (Dhillon et al 2011; Emami etal., 2003; Morin and Bengio, 2005).
Our work fall-s into the second category, but builds on the pre-vious work by demonstrating how to improve thedistributional-similarity clusters with prior knowl-edge.
To our knowledge, we are the first to applysemi-supervised representation learning techniquesfor structured NLP tasks.Most previous work on domain adaptation has fo-cused on the case where some labeled data is avail-able in both the source and target domains (Daume?III, 2007; Jiang and Zhai, 2007; Daume?
III et al2010).
Learning bounds are known (Blitzer et al2007; Mansour et al 2009).
A few authors haveconsidered domain adaptation with no labeled datafrom the target domain (Blitzer et al 2006; Huanget al 2011) by using features based on distributionalsimilarity.
We demonstrate empirically that incorpo-rating biases into this type of representation-learningprocess can significantly improve results.3 Biased Representation LearningAs before, let US and UT be unlabeled data, and LSbe labeled data from the source domain only.
Pre-vious work on representation learning with HiddenMarkov Models (HMMs) (Huang and Yates, 2009)has estimated parameters ?
for the HMM from un-labeled data alone, and then determined the Viterbi-optimal latent states for training and test data to pro-duce new features for a supervised classifier.
Theobjective function for HMM learning in this case ismarginal log-likelihood, optimized using the Baum-Welch algorithm:L(?)
=?x?US?UTlog?yp(x,Y = y|?)
(3)where x is a sentence, Y is the sequence of latentrandom variables for the sentence, and y is an in-stance of the latent sequence.
The joint distributionin an HMM factors into observation and transitiondistributions, typically mixtures of multinomials:p(x,y|?)
= P (y1)P (x1|y1)?i?2P (yi|yi?1)P (xi|yi)1315Innocentbystanders are often theJJ NNS RB VBP DTy1y2 y3 y4 y5...victimsy6NNSInnocentbystanders are often the victims...E?entropy(Y,z)P(Y)p1p2p3pmpnKL(pm|| pn)Monday, March 26, 12Figure 1: Illustration of how the entropy bias is incor-porated into HMM learning.
The dotted oval shows thespace of desired distributions in the hidden space, whichhave small or zero entropy with the real labels.
The learn-ing algorithm aims to maximize the log-likelihood of theunlabeled data, and to minimize the KL divergence be-tween the real distribution, pm, and the closest desireddistribution, pn.Intuitively, this form of representation learning i-dentifies clusters of distributionally-similar words:those words with the same Viterbi-optimal latent s-tate.
The Viterbi-optimal latent states are then usedas features for the supervised classifier.
Our previ-ous work (2009) has shown that the features fromthe learned HMM significantly improve the accura-cy of POS taggers and chunkers on benchmark do-main adaptation datasets.We use the HMM model from our previous work(2009) as our baseline.
Our techniques follow thesame general setup, as it provides an efficient andempirically-proven starting point for exploring (onepart of) the space of possible representations.
Note,however, that the HMM on its own does not provideeven an approximate solution to the objective func-tion in our problem formulation (Eqn.
2), since itmakes no attempt to find the representation that min-imizes loss on labeled data.
To address this and otherconcerns, we modify the objective function for HM-M training.
Specifically, we encode biases for rep-resentation learning by defining a set of properties ?that we believe a good representation function wouldminimize.
One possible bias is that the HMM statesshould be predictive of the labels in labeled trainingdata.
We can encode this as a property that computesthe entropy between the HMM states and the label-s. For example, in Figure 1, we want to learn thebest HMM distribution for the sentence ?Innocen-t bystanders are often the victims?
for POS taggingtask.
The hidden sequence y1, y2, y3, y4, y5, y6 canhave any distribution p1, p2, p3, ..., pm, ..., pn fromthe latent space Y .
Since we are doing POS tagging,we want the distribution to learn the information en-coded in the original POS labels ?JJ NNS RB VBPDT NNS?.
Therefore, by calculating the entropy be-tween the hidden sequence and real labels, we canidentify a subset of desired distributions that havelow entropy, shown in the dotted oval.
By minimiz-ing the KL divergence between the learned distribu-tion and the set of desired distributions, we can findthe best distribution which is the closest to our de-sire.The following subsections describe the specificproperties we investigate; here we show how to in-corporate them into the objective function.
Let zbe the sequence of labels in LS , and let ?
(x,y, z)be a property of the completed data that we wishthe learned representation to minimize, based on ourprior beliefs.
Let Q be the subspace of the possibledistributions over Y that have a small expected val-ue for ?
: Q = {q(Y)|EY?q[?
(x,Y, z)] ?
?
}, forsome constant ?.
We then add penalty terms to theobjective function (3) for the divergence between theHMM distribution p and the ?good?
distributions q,as well as for ?:L(?)?minq,?
[KL(q(Y)||p(Y|x, ?))
+ ?|?|] (4)s.t.
EY?q[?
(x,Y, z)] ?
?
(5)where KL is the Kullback-Leibler divergence, and?
is a free parameter indicating how important thebias is compared with the marginal log likelihood.To incorporate multiple biases, we define a vec-tor of properties ?, and we constrain each property?i ?
?i.
Everything else remains the same, exceptthat in the penalty term ?|?|, the absolute value isreplaced with a suitable norm: ?
???.
To allow our-selves to place weights on the relative importanceof the different biases, we use a norm of the form?x?A =?
(xtAx), where A is a diagonal matrixwhose diagonal entries Aii are free parameters thatprovide weights on the different properties.
For our1316experiments, we set the free parameters ?
and Aiiusing a grid search over development data, as de-scribed in Section 5.13.1 A Bias for Task-specific RepresentationsCurrent representation learning techniques are unsu-pervised, so they will generate the exact same repre-sentation for different tasks.
Yet it is exceedinglyrare that two state-of-the-art NLP systems for differ-ent tasks share the same feature set, even if they dotend to share some core set of lexical features.Traditional non-learned (i.e., manually-engineered) representations essentially alwaysinclude task-specific features.
In response, wepropose to bias our representation learning suchthat the learned representations are optimized for aspecific task.
In particular, we propose a propertythat measures how difficult it is to predict the labelsin training data, given the learned latent states.Our entropy property uses conditional entropy ofthe labels given the latent state as the measure ofunpredictability:?entropy(y, z) = ??iP?
(yi, zi) log P?
(zi|yi) (6)where P?
is the empirical probability and i indicatesthe ith position in the data.
We can plug this featureinto Equation 5 to obtain a new version of Equation4 as an objective function for task-specific represen-tations.
We refer to this model as HMM+E.
Un-like previous formulations for supervised and semi-supervised dimensionality reduction (Zhang et al2007; Yang et al 2006), our framework works effi-ciently for structured representations.3.2 A Bias for Domain-Independent FeaturesFollowing the theory in Section 2.2, we devise a bi-ased objective to provide an explicit mechanism forminimizing the distance between the source and tar-get domain.
As before, we construct a property ofthe completed data:?distance(y) = d1(P?S , P?T )where P?S(Y ) is the empirical distribution over la-tent state values estimated from source-domain la-tent states, and similarly for P?T (Y ).
Essentially,1Note that ?, unlike A and ?, is not a free parameter.
It isexplicitly minimized in the modified objective function.minimizing this property will bias the the represen-tation towards features that appear approximately asoften in the source domain as the target domain.
Werefer to the model trained with a bias of minimiz-ing ?distance as HMM+D, and the model with both?distance and ?entropy biases as HMM+D+E.3.3 A Bias for Multi-DimensionalRepresentationsWords are multidimensional objects.
In English,words can be nouns or verbs, singular or plural,count or mass, just to name a few dimensions alongwhich they may vary.
Factorial HMMs (FHMM-s) (Ghahramani and Jordan, 1997) can learn multi-dimensional models, but inference and learning arecomplex and computationally expensive even in su-pervised settings.
Our previous work (2010) creat-ed a multi-dimensional representation called an ?I-HMM?
by training several HMM layers indepen-dently; we showed that by finding several latent cat-egories for each word, this representation can pro-vide useful and domain-independent features for su-pervised learners.
In this work, we also learn a sim-ilar multi-dimensional model (I-HMM+D+E), butwithin each layer we add in the two biases describedabove.
While more efficient than FHMMs, the draw-back of these I-HMM-based models is that thereis no mechanism to encourage the different HMMmodels to learn different things.
As a result, the lay-ers may produce similar or equivalent features de-scribing the dominant aspect of distributional sim-ilarity in the data, but miss features that are lessstrong, but still important, in the data.To encourage learning a truly multi-dimensionalrepresentation, we add a bias towards I-HMM mod-els in which each layer is different from all previ-ous layers.
We define an entropy-based predictabili-ty property that measures how predictable each pre-vious layer is, given the current one.
Formally, letyli denote the hidden state at the ith position in lay-er l of the model.
For a given layer l, this proper-ty measures the conditional entropy of ym given yl,summed over layers m < l, and subtracts this fromthe maximum possible entropy:?predictl (y) = MAX+?i;m<lP?
(yli, ymi ) log P?
(ymi |yli)The entropy between layer l and the previous layer-1317s m measures how unpredictable the previous lay-ers are, given layer l. By biasing the model suchthat MAX minus the entropy approaches zero, weencourage layer l towards completely different fea-tures from previous layers.
We call the model withthis bias P-HMM+D+E.4 Efficient Parameter EstimationSeveral machine learning paradigms have been de-veloped recently for incorporating biases and con-straints into parameter estimation (Liang et al2009; Chang et al 2007; Mann and McCallum,2007).
We leverage the Posterior Regularization(PR) framework for our problem because of its flex-ibility in handling different kinds of biases; we pro-vide a brief overview of the technique here, but see(Ganchev et al 2010) for full details.4.1 Overview of PRPR introduces a modified EM algorithm to handleconstrained objectives, like Equation 4.
The modi-fied E-step estimates a distribution q(Y) that is closeto the current estimate of p(Y|x, ?
), but also closeto the ideal set of distributions that (in expectation)have ?
= 0 for each property ?.
The M step re-mains the same, except that it re-estimates parame-ters with respect to expected latent states computedwith q rather than p.E step:qt+1 = argminqmin?KL(q(Y)||p(Y|x, ?t)) + ?
???s.t.
Eq[?
(x,Y, z)] ?
?M step:?t+1 = argmax?Eqt+1 [log p(x,Y|?t))]To make the optimization task in the E-step moretractable, PR transforms it to a dual problem:max??0,???????
log?Yp(Y|x, ?)
exp{????
(x,Y, z)}where ????
is the dual norm of ???.
The gradient ofthis dual objective is ?Eq[?
(x,Y, z)].
A projectedsubgradient descent algorithm is used to perform theoptimization.4.2 Modifying ?
for TractabilityIn unstructured settings, this optimization problemis relatively straightforward.
However, for struc-tured representations, we need to ensure that thedynamic programming algorithms needed for infer-ence remain tractable for the biased objectives.
Forefficient PR over structured models, the properties ?need to be decomposed as a sum over the cliques inthe structured model.
Unfortunately, the propertieswe mention above do not decompose so nicely, sowe must resort to approximations.In order to efficiently compute the expected val-ue of the entropy property with respect to Y ?
q,we need to be able to compute each componen-t EYi?q[?entropy(Yi, zi)] separately.
Yet P?
dependson the setting of other latent states Yj in the corpus.To avoid this problem, we pre-compute the expectedempirical distributions over the completed data.
Foreach specific value y and z:P?q(y, z) =1|LS |?x|x|?i=11[zi = z]q(Yi = y)P?q(y) =1|LS |?x|x|?i=1q(Yi = y)These expected empirical distributions P?q can becomputed efficiently using standard inference algo-rithms, such as the forward algorithm for HMMs.Note that P?q depends on q, but unlike the originalP?
from Equation 6, they do not depend on the datacompletions y.
Thus we can compute P?q once foreach qt, and then substitute it for P?
for all valuesof Y in the computation of EY?q?entropy(Y, z),making this computation tractable.
For the entropy-based predictability properties, the calculation issimilar, but instead of using the label z, we use thedecoded states yli from previous layers.For the distance property, Ben-David et als anal-ysis depends on a particular notion of distance (E-qn.
1) that is computationally intractable.
They alsopropose more tractable lower bounds, but these areagain incompatible with the PR framework.
Sinceno computationally feasible exact algorithm existsfor this distance feature, we resort to a crude but ef-ficient approximation of this measure: for each pos-1318sible value y of the latent states, we define:?disty (y) =?i|xi?US1[yi = y]q(Yi = y)|US |?
?i|xi?UT1[yi = y]q(Yi = y)|UT |Each of these individual properties is tractable forstructured models.
Combining these properties us-ing the ??
?A norm results in a Euclidean distance(weighted byA) between the frequencies of featuresin each domain, rather than d1 distance.5 ExperimentsWe tested the structured representations with biaseson two NLP tasks: Chinese POS tagging and En-glish NER.
In both cases, we use a domain adapta-tion setting where no labeled data is available for thetarget domain ?
a particularly difficult setting, butone that provides a strong test for an NLP system?sability to generalize .
In our work (Huang and Yates,2009), we used a plain HMM for domain adaptationtasks in which there is labeled source data and un-labeled source and target data, but no labeled targetdata for training.
Therefore, here, we use the HMMtechnique as a baseline, and build on it by includingbiases.5.1 Chinese POS taggingWe use the UCLA Corpus of Written Chinese,which is a part of The Lancaster Corpus of Man-darin Chinese (LCMC).
The UCLA Corpus consistsof 11,192 sentences of word-segmented and POS-tagged text in 13 genres.
We use gold-standardword segmentation labels during training and test-ing.
The LCMC tagset consists of 50 Chinese POStags.
Each genre averages 5284 word tokens, for atotal of 68,695 tokens among all genres.
We use the?news?
genre as our source domain and randomly se-lect 20% of every other genre as labeled test data.
Totrain our representation models, we use the ?news?text, plus the remaining 80% of the texts from theother genres.
We use 90% of the labeled news textfor training, and 10% for development.
We replacehapax legomena in the unlabeled data with the spe-cial symbol *UNKNOWN*, and also do the samefor word types in the labeled test sets that never ap-pear in our unlabeled training texts.0.8880.8930.8980.9030.9080.9130.9180.9230.9280.1 1 10 100 1000Accuracy?
(log scale)News Domain (development data)alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100Figure 2: Grid search for parameters on news textFollowing our previous HMM setup in (Huangand Yates, 2009) for consistency, we use an HMMwith 80 latent states.
For our multi-layer models,we use 7 layers of HMMs.
We tuned the free pa-rameters ?
and A on development data.
We varied?
from 0.1 to 1000.
To tune A, we start by settingthe diagonal entry for ?entropy to 1, without loss ofgenerality.
We then tie all the entries in A for ?distyto a single parameter ?, and tie all of the entries for?predicty to a parameter ?.
We vary ?
and ?
over theset {0.01,0.1,1,10,100}.
Figure 2 shows our resultsfor ?
and ?
on news development data.
A settingof ?
= 0.01 and ?
= 100 performs best, with all?
= 100 doing reasonably well.
Results for eachof these models on the general fiction test text con-firm the general trends seen on development data ?a comforting sign, since this indicates we can opti-mize the free parameters on in-domain developmentdata, rather than requiring labeled data from the tar-get domain.
Our models tended to perform betterwith increasing ?
on development data, though withdiminishing returns.
We pick the largest setting test-ed, ?
= 100, for our final models.We use a linear-chain Conditional Random Field(CRF) for our supervised classifier.
To incorporatethe learned representations, we use the Viterbi Algo-rithm to find the optimal latent state sequence fromeach HMM-based model and then use the optimalstates as features in the CRF.
Table 1 presents thefull list of features in the CRF.
To handle Chinese,we add in two features introduced in previous work(Wang et al 2009): radical features and repeatedcharacters.
A radical is a portion of a Chinese char-acter that consists of a small number of pen or brushstrokes in a regular pattern.13190.820.830.840.850.860.870.880.890.90.910.920.1 1 10 100 1000Accuracy?ORJVFDOHGeneral Fiction Domain (test data)alpha=0.01 alpha=0.1 alpha=1 alpha=10 alpha=100Figure 3: Validating parameter settings on fiction textCRF Feature SetTransition?z1[zj = z]?z,z?1[zj = z and zj?1 = z?
]Word?w,z1[xj = w and zj = z]Radical?z,r1[?c?xjradical(c) = r and zj = z]Repeated Words?A,B,z1[xj = AABB and zj = z]?A,z1[(xj = AAor xj = AA/) and zj = z]?A,B,z1[xj = ABAB and zj = z]Features from Representation Learning?y,l,z1[ylj = y and zj = z]Table 1: Features used in our Chinese POS taggingCRF systems.
c represents a character within a word.Table 2 shows our results.
We compare againstthe Baseline CRF without any additional representa-tions and the unbiased HMM, a state-of-the-art do-main adaptation technique from previous work, overall 13 domains (source and target).
We also com-pare against a state-of-the-art Chinese POS taggerfor in-domain text, the CRF-based Stanford tagger(Tseng et al 2005), retrained for this corpus.
H-MM+D+E outperforms the Stanford tagger on 10out of 12 target domains and the unbiased HMM onall domains, while the P-HMM+D+E outperform-s the Stanford tagger (2.6% average improvement)and HMM (1.7%) on all 12 target domains.
The I-HMM+D+E is slightly better than the HMM+D+E(.3%), but incorporating the multi-dimensional bias(P-HMM+D+E) adds an additional 0.6% improve-ment.Our interpretation for the success of I-HMM+D+E and P-HMM+D+E is that the increasein the state space of the models yields improvedperformance.
Because P-HMM+D+E biases againstredundant states found in I-HMM+D+E, it effective-ly increases the state space beyond I-HMM+D+E.Ahuja and Downey (2010) and our own work withHMMs as representations (2010) have previouslyshown that increasing the state space of the HMMcan significantly improve the representation, butmemory constraints eventually prevent furtherprogress this way.
The I-HMM+D+E and P-HMM+D+E models can provide similar benefits,but because they split parameters across multipleHMMs, they can accommodate much greater statespaces in the same amount of memory.We also tested the entropy and distance biasesseparately.
Figure 4 shows the result of the distance-biased HMM+D on the general-fiction test text, aswe vary ?
over the set {0.1,1,10,100,1000} (we ob-served similar results for other domains).
For all val-ues of ?, the biased representation outperforms theunbiased HMM.
There is also a strong negative cor-relation between the expected value of ?
?distance?and the resulting accuracy, as expected from Ben-David et als theoretical analysis.
The HMM+Emodel outperforms the HMM on the (source) newsdomain by 0.3%, but actually performs worse formost target domains.
We suspect that the entropyfeature, which is learned only from labeled source-domain data, makes the representation biased to-wards features that are important in the source do-main only.
However, after we add in the distancebias and a parameter to balance the weights fromboth biases, the representation is able to capture thelabel information as well as the target domain fea-tures.
Thus, the representation won?t solely dependon source data.
HMM+D+E, which combines bothbiases, outperforms HMM+D, suggesting that task-specific features for domain adaptation can be help-ful, but only if there is some control for the domain-independence of the features.5.2 English Named Entity RecognitionTo evaluate on a second task, we turn to Named En-tity Recognition.
We use the training data from the1320news (source) lore reli humour gen-fic essay mystery romance sci-fi skill science adv-fic report avgwords 9774 5428 3248 3326 4913 5214 5774 5489 3070 5464 5262 5071 6662 5284CRF w/o HMM 93.8 85.0 80.0 85.4 85.0 83.8 84.7 86.0 82.8 78.2 82.2 77.1 85.3 84.5HMM+E 97.1 88.2 83.1 87.5 87.4 89.2 89.5 87.1 86.7 82.1 87.2 79.4 91.7 88.3Stanford 98.8 88.4 83.5 89.0 87.5 88.4 87.4 87.5 88.6 82.7 86.0 82.1 91.7 88.7HMM 96.9 89.7 85.2 89.6 89.4 89.0 90.1 89.0 87.0 84.9 87.8 80.0 91.4 89.2HMM+D 97.4 89.9 85.4 89.4 89.6 89.9 90.1 88.6 87.9 85.3 87.9 80.0 92.0 89.5HMM+D+E 97.7 90.1 86.1 89.8 90.9 89.7 90.3 89.8 88.4 85.6 87.9 81.2 92.0 89.9I-HMM+D+E 97.8 90.5 87.0 89.1 91.1 90.2 90.0 90.5 89.8 86.0 87.1 82.2 92.1 90.2P-HMM+D+E 98.2 91.5 87.7 89.0 91.8 91.0 89.9 91.4 90.4 87.0 87.7 83.4 92.4 90.8Table 2: POS tagging accuracy: The P-HMM+D+E tagger outperforms the unbiased HMM tagger and theStanford tagger on all target domains.
The ?avg?
column includes source-domain development data results.
Differ-ences between the P-HMM+D+E and the Stanford tagger are statistically significant at p < 0.01 on average and on 11out of 12 target domain.
We used the two-tailed Chi-square test with Yates?
correction.0.8940.8950.8960.8970.8980.8990.90.9010.9020.9030.9044.55E -05 4.60E-05 4.65E -05 4.70E-05 4.75E -05 4.80E-05Accuracy?E q (?distance)?HMM+D on General Fiction Test?=1000?=100?=1  ?=0.1?=10Unconstrained HMMFigure 4: Greater distance between domains correlateswith worse target-domain tagging accuracy.CoNLL 2003 shared task for our labeled training set,consisting of 204k tokens from the newswire do-main.
We tested the system on the MUC7 formalrun test data, consisting of 59k tokens of stories onthe telecommunications and aerospace industries.To train our representations, we use the CoNL-L training data and the MUC7 training data withoutlabels.
We again use a CRF, with features introducedby Zhang and Johnson (2003) for our baseline.
Weuse the same setting of free parameters from ourPOS tagging experiments.Results are shown in Table 3.
Our best biasedrepresentation P-HMM+D+E outperformed the un-biased HMM representation by 3.6%, and beats theI-HMM+D+E by 1.6%.
The domain-distance andmulti-dimensional biases help most, while the task-specific bias helps somewhat, but only when thedomain-distance bias is included.
The best sys-System F1CRF without HMM 66.15HMM+E 74.25HMM 75.06HMM+D 75.75HMM+D+E 76.03I-HMM+D+E 77.04P-HMM+D+E 78.62Table 3: English Named Entity recognition resultstem tested on this dataset achieved a slightly bet-ter F1 score (78.84) (Turian et al 2010), but useda much larger training corpus (they use RCV1 cor-pus which contains approximately 63 million token-s).
Other studies (Turian et al 2010; Huang etal., 2011) have performed a detailed comparison be-tween these types of systems, so we concentrate oncomparisons between biased and unbiased represen-tations here.5.3 Does the task-specific bias actually help?In this section, we test whether the task-specificbias (entropy bias) actually learns something task-specific.
We learn the entropy-biased representa-tions for two tasks on the same set of sentences,labeled differently for the two tasks: English POStagging and Named Entity Recognition.
Then weswitch the representations to see whether they willhelp or hurt the performance on the other task.
Werandomly picked 500 sentences from WSJ section1321Representation/Task POS Accuracy NER F1HMM 88.5 66.3HMM+E(POS labels) 89.7 64.5HMM+E(NER labels) 86.5 68.0Table 4: Results of POS tagging and Named Entityrecognition tasks with different representations.
With theentropy-biased representation, the system has better per-formance on the task which the bias is trained for, butworse performance on the other task.0-18 as our labeled training data and 500 sentencesfrom WSJ section 20-23 as testing data.
BecauseWSJ data does not have gold standard NER tags,we manually labeled these sentences with NER tags.For simplicity, we only use three types of NER tags:person, organization and location.
The result isshown in Table 4.
When the entropy bias uses la-bels from the same task as the classifier, the perfor-mance is improved: about 1.2% in accuracy on POStagging and 1.7% in F1 score on NER.
Switchingthe representations for the tasks actually hurts theperformance compared with the unbiased represen-tation.
The results suggest that the entropy bias doesindeed yield a task-specific representation.6 Conclusion and Future WorkWe introduce three types of biases into represen-tation learning for sequence labeling using the PRframework.
Our experiments on POS tagging andNER indicate domain-independent biases and multi-dimensional biases significantly improve the repre-sentations, while the task-specific bias improves per-formance on out-of-domain data if it is combinedwith the domain-independent bias.
Our results indi-cate the power of representation learning in buildingdomain-agnostic classifiers, but also the complexi-ty of the task and the limitations of current tech-niques, as even the best models still fall significantlyshort of in-domain performance.
Important consid-erations for future work include identifying furthereffective and tractable biases, and extending beyondsequence-labeling to other types of NLP tasks.AcknowledgmentsThis research was supported in part by NSF grantIIS-1065397.ReferencesArun Ahuja and Doug Downey.
2010.
Improved extrac-tion assessment through better language models.
InProceedings of the Annual Meeting of the North Amer-ican Chapter of the Association of Computational Lin-guistics (NAACL-HLT).Shai Ben-David, John Blitzer, Koby Crammer, and Fer-nando Pereira.
2007.
Analysis of representationsfor domain adaptation.
In Advances in Neural Infor-mation Processing Systems 20, Cambridge, MA.
MITPress.Shai Ben-David, John Blitzer, Koby Crammer, AlexKulesza, Fernando Pereira, and Jennifer WortmanVaughan.
2010.
A theory of learning from differentdomains.
Machine Learning, 79:151?175.John Blitzer, Ryan McDonald, and Fernando Pereira.2006.
Domain adaptation with structural correspon-dence learning.
In EMNLP.John Blitzer, Koby Crammer, Alex Kulesza, FernandoPereira, and Jenn Wortman.
2007.
Learning boundsfor domain adaptation.
In Advances in Neural Infor-mation Processing Systems.M.
Candito and B. Crabbe?.
2009.
Improving generativestatistical parsing with semi-supervised word cluster-ing.
In IWPT, pages 138?141.M.
Chang, L. Ratinov, and D. Roth.
2007.
Guiding semi-supervision with constraint-driven learning.
In Pro-ceedings of the ACL.Hal Daume?
III, Abhishek Kumar, and Avishek Saha.2010.
Frustratingly easy semi-supervised domainadaptation.
In Proceedings of the ACL Workshop onDomain Adaptation (DANLP).Hal Daume?
III.
2007.
Frustratingly easy domain adapta-tion.
In ACL.S.
C. Deerwester, S. T. Dumais, T. K. Landauer, G. W.Furnas, and R. A. Harshman.
1990.
Indexing by latentsemantic analysis.
Journal of the American Society ofInformation Science, 41(6):391?407.Paramveer S. Dhillon, Dean Foster, and Lyle Ungar.2011.
Multi-view learning of word embeddings viacca.
In Neural Information Processing Systems (NIP-S).A.
Emami, P. Xu, and F. Jelinek.
2003.
Using a con-nectionist model in a syntactical based language mod-el.
In Proceedings of the International Conference onSpoken Language Processing, pages 372?375.Kuzman Ganchev, Joa?o Grac?a, Jennifer Gillenwater, andBen Taskar.
2010.
Posterior regularization for struc-tured latent variable models.
Journal of MachineLearning Research, 11:10?49.Zoubin Ghahramani and Michael I. Jordan.
1997.
Facto-rial hidden markov models.
Machine Learning, 29(2-3):245?273.1322Daniel Gildea.
2001.
Corpus Variation and Parser Per-formance.
In Conference on Empirical Methods inNatural Language Processing.T.
Honkela.
1997.
Self-organizing maps of words fornatural language processing applications.
In In Pro-ceedings of the International ICSC Symposium on SoftComputing.Fei Huang and Alexander Yates.
2009.
Distributionalrepresentations for handling sparsity in supervised se-quence labeling.
In Proceedings of the Annual Meet-ing of the Association for Computational Linguistics(ACL).Fei Huang and Alexander Yates.
2010.
Exploringrepresentation-learning approaches to domain adapta-tion.
In Proceedings of the ACL 2010 Workshop onDomain Adaptation for Natural Language Processing(DANLP).Fei Huang, Alexander Yates, Arun Ahuja, and DougDowney.
2011.
Language models as representation-s for weakly supervised nlp tasks.
In Conference onNatural Language Learning (CoNLL).Jing Jiang and ChengXiang Zhai.
2007.
Instance weight-ing for domain adaptation in NLP.
In ACL.P.
Liang, M. I. Jordan, and D. Klein.
2009.
Learningfrom measurements in exponential families.
In Inter-national Conference on Machine Learning (ICML).D.
Lin and X Wu.
2009.
Phrase clustering for discrimi-native learning.
In ACL-IJCNLP, pages 1030?1038.G.
S. Mann and A. McCallum.
2007.
Simple, robust,scalable semi-supervised learning via expectation reg-ularization.
In In Proc.
ICML.Y.
Mansour, M. Mohri, and A. Rostamizadeh.
2009.
Do-main adaptation with multiple sources.
In Advances inNeural Information Processing Systems.F.
Morin and Y. Bengio.
2005.
Hierarchical probabilisticneural network language model.
In Proceedings of theInternational Workshop on Artificial Intelligence andStatistics, pages 246?252.Sameer Pradhan, Wayne Ward, and James H. Martin.2007.
Towards robust semantic role labeling.
In Pro-ceedings of NAACL-HLT, pages 556?563.M.
Sahlgren.
2005.
An introduction to random indexing.In In Methods and Applications of Semantic IndexingWorkshop at the 7th International Conference on Ter-minology and Knowledge Engineering (TKE).G.
Salton and M.J. McGill.
1983.
Introduction to Mod-ern Information Retrieval.
McGraw-Hill.Satoshi Sekine.
1997.
The domain dependence of pars-ing.
In Proc.
Applied Natural Language Processing(ANLP), pages 96?102.Huihsin Tseng, Daniel Jurafsky, and Christopher Man-ning.
2005.
Morphological features help pos taggingof unknown words across language varieties.
In Pro-ceedings of the Fourth SIGHAN Workshop on ChineseLanguage Processing.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of theAnnual Meeting of the Association for ComputationalLinguistics (ACL), pages 384?394.P.
D. Turney and P. Pantel.
2010.
From frequency tomeaning: Vector space models of semantics.
Journalof Artificial Intelligence Research, 37:141?188.Lijie Wang, Wanxiang Che, and Ting Liu.
2009.
Ansvmtool-based chinese pos tagger.
In Journal of Chi-nese Information Processing.X.
Yang, H. Fu, H. Zha, and J. Barlow.
2006.
Semi-supervised nonlinear dimensionality reduction.
InProceedings of the 23rd International Conference onMachine Learning.T.
Zhang and D. Johnson.
2003.
A robust risk mini-mization based named entity recognition system.
InCoNLL.D.
Zhang, Z.H.
Zhou, and S. Chen.
2007.
Semi-supervised dimensionality reduction.
In Proceedingsof the 7th SIAM International Conference on DataMining.1323
