Using Coreference Chains for Text Summarizat ionSa l iha  Azzam and Kev in  Humphreys  and Rober t  Ga izauskas{s. azzam, k. humphreys, r. gaizauskas }@dcs.
shef.
ac.
ukDepar tment  of Computer  Science, University of SheffieldRegent Court,  Portobel lo RoadSheffield S1 4DP UKAbst ractWe describe the use of coreference chains for theproduction of text summaries, using a variety ofcriteria to select a 'best' chain to represent themain topic of a text.
The approach as been im-plemented within an existing MUC coreferencesystem, which constructs a full discourse modelof texts, including information about changesof focus, which can be used in the  selectionof chains.
Some preliminary experiments onthe automatic evaluation of summaries are alsodescribed, using existing tools to attempt toreplicate some of the recent SUMMAC manualevaluations.1 In t roduct ionIn this paper we report preliminary work whichexplores the use of coreference chains to con-struct text summaries.
Sparck Jones (1993) hasdescribed summarization as a two stage processof (1) building a representation of the sourcetext and (2) generating a summary represent-ation from the source representation and pro-ducing an output text from this summary rep-resentation.
Our source representation is a setof coreference chains - specifically those chainsof referring expressions produced by an inform-ation extraction system designed to participatein the MUC-7 coreference task (DARPA, 1998).Our summary representation is a 'best chain',selected from the set of coreference chains bythe application of one or more heuristics.
Theoutput summary is simply the concatenation of(some subset of) sentences from the source textwhich contain one or more expressions occurringin the selected coreference chain.The intuition underlying this approach is thattexts are in large measure 'about' some centralentity, which is effectively the topic, or focusof the discourse.
This intuition may be false -there may be more than one entity of centralconcern, or events or relations between entitiesmay be the principal topic of the text.
However,it is at the very least an interesting experimentto see to what extent a principal coreferencechain can be used to generate a summary.
Fur-ther, this approach, which we have implementedand preliminarily evaluated, could easily be ex-tended to allow summaries to be generated from(parts of) the best n coreference chains, or fromevent, as well as object , coreference chains.The use of document extracts formed fromcoreference chains is not novel.
Bagga andBaldwin (1998) describe a technique for cross-document coreference which involves extractingthe set of all sentences containing expressionsin a coreference chain for a specific entity (e.g.John Smith) from each of several documents.They then employ a thresholded vector spacesimilarity measure between these document ex-tracts to decide whether the documents are dis-cussing the same entity (i.e.
the same JohnSmith).
Baldwin and Morton (1998) describea query-sensitive (i.e.
user-focused) summariz-ation technique that involves extracting sen-tences from a document which contain phrasesthat corefer with expressions in the query.
Theresulting extract is used to support relevancyjudgments with respect o the query.The use of chains of related expressions indocuments to select sentences for inclusion in ageneric (i.e.
non-user-focused) summary is alsonot novel.
Barzilay and Elhadad (1997) de-scribe a technique for text summarization basedon lexical chains.
Their technique, which buildson work of Morris and Hirst (1994), and ulti-mately Halliday and Hasan (1976) who stressedthe role of lexical cohesion in text coherence,is to form chains of lexical items across a textbased on the items' semantic relatedness as in-77dicated by a thesaurus (WordNet in their case).These lexical chains serve as their source rep-resentation, from which a summary represent-ation is produced using heuristics for choosingthe 'best' lexical chains.
From these the sum-mary is produced by employing a further heur-istic to select the 'best' sentences from each ofthe selected lexical chains.The novelty in our work is to combine theidea of a document extract based on coreferencechains with the idea of chains of related expres-sions serving to indicate sentences for inclusionin a generic summary (though we explore theuse of coreference between query and text as atechnique for generating user-focused summar-ies as well).Returning to Halliday and Hasan, one cansee how this idea has merit within their frame-work.
They identify four principal mechanismsby which text coherence is achieved - reference,substitution and ellipsis, conjunction and lexicalcohesion.
If lexical cohesion is a useful relationto explore for getting at the 'aboutness' of atext, and hence for generating summaries, thenso too may reference (separately, or in conjunc-tion with lexical cohesion).
Indeed, identifyingchains of coreferential expressions in text hascertain strengths over identifying chains of ex-pressions related merely on lexical semanticalgrounds.
For, there is no doubt that commonreference, correctly identified, directly ties dif-ferent parts of a text together - they are liter-ally 'about'  the same thing; lexical semantic re-latedness, as indicated by an external resource,can never conclusively establish this degree ofrelatedness, nor indeed can the resource guar-antee that semantic relatedness will be foundwhen it exists.
Further, lexical cohesion tech-niques ignore pronomial anaphora, and hencetheir frequency counts of key terms, used bothfor identifying best chains and best sentenceswithin best chains, may often be inaccurate, asfocal referents will often be pronominalised.Of course there are drawbacks to acoreference-based approach.
Lexical cohesionrelations are relatively easy to compute and donot rely on full text processing - this makessummarisation techniques based on them rapidand robust.
Coreference relations tend to re-quire more complex techniques to compute.Our view, however, is that summarisation re-search is still in early stages and that we needto explore many techniques to understand theirstrengths and weaknesses in terms of the typeand quality of the summaries they produce.If coreference-based techniques can yield goodsummaries, this will provide impetus to makecoreference technologies better and faster.The basic coreference chain technique we de-scribe in this paper yields generic summar-ies as opposed to user-focused summaries, asthese terms have been used in relation to theT IPSTER SUMMAC text summarization eval-uation exercise (Mani et al, 1998).
That is, thesummaries aim to satisfy a wide readership bysupplying information about the 'most import-ant' entity in the text.
But of course this tech-nique could also be used to generate summar-ies tailored to a user(group) through use with apreprocessor that analyzed a user-supplied topicdescription and selected one or more entitiesfrom the topic description to use in filteringcoreference chains found in the full source doc-ument.The rest of this paper is organised as fol-lows.
In Section 2 we briefly describe the sys-tem we use for computing coreference relations.Section 3 describes various heuristics we haveimplemented for extracting a 'best' coreferencechain from the set of coreference chains com-puted for a text; and, it discusses how we se-lect 'best' sentences to include in the summaryfrom those source text sentences containing re-ferring expressions in the 'best' chain.
Section4 presents a simple example and shows the dif-ferent summaries that different heuristics pro-duce.
Section 5 describes the limited evaluationwe have been able to carry out to date, but moreimportantly introduces what we believe to be anovel and interesting way of reusing some of theMUC materials for assessing summaries.2 Core ference  in  the  LaS IE  sys temThe LaSIE system (Gaizauskas et al, 1995) hasbeen designed as a general purpose IE systemwhich can conform to the MUC task specific-ations for named entity identification, corefer-ence resolution, IE template element and re-lation identification, and the construction ofscenario-specific IE templates.
The system hasa pipeline architecture which processes a textone sentence at a time and consists of three prin-78cipal processing stages: lexical preprocessing,parsing plus semantic interpretation, and dis-course interpretation.
The overall contributionsof these stages may be briefly described as fol-lows (see (Gaizauskas et al, 1995) for furtherdetails):lexical preprocess ing reads and tokenisesthe raw input text, performs phrasalmatching against lists of proper names,identifies sentence boundaries, tags thetokens with parts-of-speech, performs mor-phological analysis;pars ing and semant ic  in terpretat ionbuilds lexical and phrasal chart edges ina feature-based formalism then does twopass chart parsing, pass one with a specialnamed entity grammar, pass two with ageneral grammar, and, after selecting a'best parse', which may have only partialcoverage, constructs a predicate-argumentrepresentation f each sentence;discourse in terpretat ion  adds the informa-tion from the predicate-argument repres-entation to a hierarchically structured se-mantic net which encodes the system'sworld and domain model, adds additionalinformation presupposed by the input, per-forms coreference r solution between ewand existing instances in the world model,and adds any information consequent uponthe new input.The domain model is encoded as a hierarchyof domain-relevant concept nodes, each with anassociated attribute-value structure describingproperties of the concept.
As a text is pro-cessed, instances of concepts mentioned in atext are added to the domain model, populat-ing it to become a text-, or discourse-, specificmodel.Coreference resolution is carried out by at-tempting to merge each newly added instancewith instances already present in the discoursemodel.
The basic mechanism, detailed in Ga-izauskas and Humphreys (1997), is to examine,for each pair of newly added and existing in-stances: semantic type consistency/similarityin the concept hierarchy; attribute value con-sistency/similarity, and a set of heuristic rules,some specific to particular types of anaphorasuch as pronouns, which can act to rule out aproposed merge.
These rules can refer to vari-ous lexical, syntactic, semantic, and positionalinformation about instances, and have mainlybeen developed through the analysis of train-ing data.
A recent addition, however, has beenthe integration of a more theoretically motiv-.ated focus-based algorithm for the resolutionof pronominal anaphora (Azzam et al, 1998).This includes the maintenance of a set of fo-cus registers within the discourse interpreter,to model changes of focus through a text andprovide additional information for the selectionof antecedents.The discourse interpreter maintains an expli-cit representation f coreference chains createdas a result of instances being merged in the dis-course model.
Each instance has an associatedattribute recording its position in the originaltext in terms of character positions.
When in-stances are merged, the result is a single in-stance with multiple positions which, taken to-gether, represent a coreference hain.3 Core ference  Cha in  Se lec t ionThe summarisation mechanism is implementedas an additional module in the LaSIE system.It processes all the coreference chains built bythe discourse interpreter and applies various cri-teria, described below, to select a 'best' chain.The set of sentences in which entries in the bestchain occur are then identified, using their char-acter positions, and concatenated together toact as a summary.3.1 Selection Cr i ter iaThe summarisation module implements severalselection criteria which can be applied either in-dependently or in combination, as specified inparameters of the module.
The current set ofcriteria is not fixed, however, and is simply in-tended to represent common intuitive heuristicsas a starting point for further experimentation.Length of Chain This criteria simply prefersthe chain containing the most entries,which represents he most frequently men-tioned instance in a text, and so possiblythe most important instance.
In the caseof several chains of equal maximum length,the spread is used in addition to select asingle chain.79Spread of  Chain This criteria involves thecalculation of the distance, in byte off-sets, between the earliest and latest entryin each chain.
The chain which spansthe greatest portion of the original textis preferred, corresponding to the intuitionthat an instance mentioned throughout atext, rather than just being frequently men-tioned in one specific section, may be themost important instance.
If this criteriafails to select a single chain, the length cri-teria will be used in addition.Star t  of Cha in  A further intuition is that in-stances mentioned at the start of a text, orin a title if present, may be more import-ant than those only introduced part waythrough.
This criteria acts to require thatthe earliest entry in a chain is within eitherthe title or the first paragraph of a text.Again, other criteria will be needed to se-lect a single chain if several start in the firstparagraph.
This criteria may be more ap-propriate for particular text genres, such asnewswires, than the previous two.3.2 Focus ChainsAn additional selection mechanism, which maybe combined with the above criteria in severalways, is the 'reduction' of each coreference hainto what may be called a "focus chain".
Thismakes use of the focus registers built withinthe discourse interpreter to track changes in themain focus of each clause in the text.
A focuschain is the subset of a coreference hain whichcontains only those mentions of an instance thatoccur as the focus of a clause.
For example,an indefinite noun phrase occurring as a logicalsubject may be recorded in the focus registers,and so be retained in a focus chain, whereas asubsequent definite noun phrase embedded inan optional prepositional phrase will not be infocus and will be removed.The criteria listed above can all be applied tofocus chains in exactly the same way as corefer-ence chains.
However, the point at which thecoreference hains are reduced is significant: se-lecting the longest coreference chain and sub-sequently removing all non-focus entries beforeoutput may give different results than an initialselection of the longest focus chain.
The formersequence of operations acts to filter the entrieswithin the 'best' coreference chain, possibly toreduce the final length of a summary, whereasthe latter selects on the basis of the frequency(in combination with the length criteria) or theposition (with spread) of focus itself.
Only thelatter sequence is considered inany detail below,although the implementation does allow experi-mentation with the use of focus chains at severalalternative stages.4 Example  OutputThis section illustrates the kind of summariesproduced by the different heuristics, using anexample from the MUC-6 evaluation corpus ofnewswire articles (much reduced for inclusionhere by omitting the eight final paragraphs):<DOC><HL> Economy: Washington, an Exchange Ally,Seems To Be Strong Candidate to Head SEC</HL>< TXT>Consuela Washington, a longtime House stafferand an expert in securities laws, is a leading candid-ate to be chairwoman of the Securities and ExchangeCommission in the Clinton administration.Ms.
Washington, ~ years old, would be the firstwoman and the first black to head the five-membercommission that oversees the securities markets.Ms.
Washington's candidacy is being championedby several powerful awmakers including her boss,Chairman John Dingell (D., Mich.) of the HouseEnergy and Commerce Committee.
She currently isa counsel to the committee.
Ms. Washington andMr.
Dingell have been considered allies of the secur-ities exchanges, while banks and futures exchangeshave often fought with them.A graduate of Harvard Law School, Ms. Wash-ington worked as a lawyer for the corporate financedivision of the SEC in the late 1970s.
She has beena congressional staffer since 1979.Separately, Clinton transition o~cials saidthat Frank Newman, 50, vice chairman and chieffinancial o~eer of BankAmerica Corp., is expectedto be nominated as assistant Treasury secretaryfor domestic finance.
Mr. Newman, who would begiving up a job that pays $1 million a year, wouldoversee the Treasury's auctions of governmentsecurities as well as banking issues.
He would reportdirectly to Treasury Secretary-designate LloydBentsen.(...
)Using the 'length of chain' criteria selectsthe coreference chain for the person Consuela80Washington, and the following summary isproduced:<HL> Economy: Washington, an Exchange Ally,Seems To Be Strong Candidate to Head SEC</HL>Consuela Washington, a longtime House stafferand an expert in securities laws, is a leading candid-ate to be chairwoman of the Securities and ExchangeCommission in the Clinton administration.Ms.
Washington, 44 years old, would be the firstwoman and the first black to head the five-membercommission that oversees the securities markets.Ms.
Washington's candidacy is being championedby several powerful awmakers including her boss,Chairman John Dingell (D., Mich.) of the HouseEnergy and Commerce Committee.
She currently isa counsel to the committee.
Ms. Washington andMr.
Dingell have been considered allies of the secur-ities exchanges, while banks and futures exchangeshave often fought with them,A graduate of Harvard Law School, Ms. Wash-ington worked as a lawyer for the corporate financedivision o/the SEC in the late 1970s.
She has beena congressional staffer since 1979.Using the 'spread of chain' selects the core-ference chain for the person Clinton, used hereas a noun modifier:Consuela Washington, a longtime House stafferand an expert in securities laws, is a leading candid-ate to be chairwoman of the Securities and ExchangeCommission in the Clinton administration.Separately, Clinton transition officials saidthat Frank Newman, 50, vice chairman and chieffinancial officer of BankAmeriea Corp., is expectedto be nominated as assistant Treasury secretary fordomestic finance.Restricting the selection to focus chains only,the 'length of chain' criteria again choosesthe Ms. Washington chain, but this does notinclude the elements occurring in phrases whereMs.
Washington is not in focus.
The summaryis therefore reduced by omitting the non-focusmentions:<HL> Economy: Washington, an Exchange Ally,Seems To Be Strong Candidate to Head SEC</HL>Consuela Washington, a longtime House stafferand an expert in securities laws, is a leading candid-ate to be chairwoman of the Securities and ExchangeCommission in the Clinton administration.Ms.
Washington, 44 years old, would be the firstwoman and the first black to head the five-membercommission that oversees the securities markets.She currently is a counsel to the committee.A graduate o/Harvard Law School, Ms. Wash-ington worked as a lawyer for the corporate financedivision o/ the SEC in the late 1970s.
She has beena congressional staffer since 1979.This same summary is also obtained when the'spread of chain' criteria is applied to the focuschains.
The Clinton chain selected above doesnot include any focus elements, and the Con-suela Washington focus chain is the most spreadas well as the longest.5 Eva luat ing  Summar iesEvaluating the merit of a summary is a diffi-cult task.
The most extensive effort to dateto develop a framework for assessing summar-ies has been the T IPSTER SUMMAC evalu-ation exercise (Mani et al, 1998).
In this sec-tion we first review the SUMMAC evaluationmeasures and propose how we might 'simulate'these without expensive human judges; then, wedescribe how we have carried out one of thesesimulated evaluations to evaluate the summar-ization technique described in the previous sec-tions.5.1 The  SUMMAC Eva luat ionThis exercise involved a number of differenttasks and a number of different evaluation meas-ures.
The measures divided into extrinsic meas-ures - those that ignore the content of the sum-mary and assess it solely according to how usefulit is in enabling an agent to perform some meas-urable task - and intrinsic measures - thosethat examine the content of the summary andattempt to pass some judgment on it directly.In brief, the four SUMMAC tasks were:1.
Ad Hoc Task The summariser is givena topic description and a set of docu-ments and produces user-focused summar-ies based on the topic description.
Thesummaries and the topic description (alongwith some source documents for control)are passed to a judge who reads the sum-maries passes relevance judgments on themwith respect o the topic.
These judgmentsare scored against 'true' relevance judg-ments previously established for the fulldocuments with respect o the topic.
This81is an extrinsic evaluation that measures theutility of a summarization system at gen-erating user-focused summaries capable ofsupporting a relevance judgment.2.
Categorization Task The summariser isgiven a set of documents only and generatesa summary of each.
The summaries (againalong with some source documents for con-trol) along with five topic descriptions axegiven to a judge who reads the summar-ies and categorizes them with respect othe five topics, or "none of the above", ifnone is deemed appropriate.
These cat-egorization judgments are scored against'true' categorization judgments previouslyestablished for the full documents with re-spect to the topics.
This is an extrinsicevaluation that measures the utility of asummarization system at generating en-eric summaries capable of supporting a cat-egorization judgment.3.
Question-Answering Task The summariseris given a set of documents and a smallset of topic descriptions (three in the ac-tual evaluation) and produces 'informat-ive', user-focused summaries.
A judge isfirst given the topic descriptions and foreach produces a set of topic questions,each capturing an 'obligatory' aspect of thetopic - something that has to be presentin a document to make it relevant o thetopic.
The judge then reads the set of rel-evant, full documents obe used in the eval-uation and for each produces an answer keythat contains answers to each of the topicquestions.
Finally the judge reads the sum-maries and scores each against he answerkey for the document, deciding for eachtopic question whether the summary sup-plies a correct, partially correct, or miss-ing answer.
This is an intrinsic evaluationthat measures the utility of a summar-ization system at generating 'informative'user-focused summaries capable of actingas surrogates for the original document foran agent charged with answering the ques-tions implicit in a topic description.4.
Acceptability Task The summariser is givena set of documents only and generates asummary of each.
The summaries andthe full documents from which they weregenerated axe given to a judge who readsthe summaries and documents and passesa binary 'goodness' judgment on the sum-maries, with no prior guidelines pecifyingwhat constitutes a good summary.
Thisis an intrinsic evaluation that was carriedout merely to see what sort~ of correlationmight emerge between the extrinsic evalu-ations and naive intrinsic evaluation.This is a very _valuable framework, but forthose without access to the resources to instan-tiate the framework, particularly the judgingfunctions, it is hard to use it to assess asummar-ization system.
Thus, we have considered waysto automate the judging function and have iden-tified several, which while by no means perfect,may at least provide some useful informationabout the value of the summaries produced.For the ad hoc task, we propose replacing thejudge with an IR system that given the topic de-scription assesses both the summaries and fulldocuments for relevance.
These judgments maythen be compared against he 'true' relevancejudgments.
The IR system's ability to judgefull documents may be taken as a baseline meas-ure, and the (predicted) loss in accuracy of thatsystem relative to the baseline when assessingthe summaries should provide a measure of howgood the summaries are.
This will not be anabsolute measure, of course, but one which mayserve to compare different summarization sys-tems, or different parameter settings of the samesystem.Similarly, for the categorization task, a doc-ument categorization system could be bench-marked against he full documents, and then,given this baseline, run against he output of thesummarisation system.
Again, an assessment ofsummaries relative to the baseline would be ob-tained which could provide significant insights,without he cost of a human judge.Finally, for the question answering task, wepropose that some of the earlier MUC resourcescould be reused.
First, we assume that a MUCtemplate defines the obligatory aspects of sometopic description, and synthesize a narrativetopic description from the template (a sort of'novelisation' of the template).
As in the pre-ceding case an IE system could be benchmarkedagainst a set of texts for which filled answer keys82exist by scoring its output answer keys using theMUC scoring software.
It could then be runover the summaries produced by a summariserwhich has been given the full texts and the syn-thesized topic description.
As with the previ-ous cases, the difference between the benchmarkcomputed on the full texts, and scores producedby automatically filling templates from the sum-maries should provide a measure of the informa-tion loss in the summary, and hence an indirectmeasure of its value.5.2 An  Initial Evaluat ionWe have not to date been able to carry out theproposed evaluations in full on our summariza-tion system.
In particular, since the goal of thebasic summarization model we have implemen-ted is to produce generic summaries, only theautomated categorization evaluation is reallyappropriate.However, since the MUC materials and scor-ing software were available to us, we were eagerto attempt some form of an automated ques-tion answering evaluation, as described in thelast section, using the existing LaSIE system.We therefore adopted a crude technique to sim-ulate topic processing by the generic summar-ization system: the topic description (a TREC-style narrative) from the MUC-6 managementsuccession IE task definition, was prepended,as the first paragraph, to each text.
We thenran our summarisation system using the 'startof chain' criteria to select only those coreferencechains which included a link between the topicdescription and the text.
This scenario is sim-ilar to Baldwin and Morton (1998) who also usecoreference between query, or topic description,and text to generate a user-focused, 'indicative'summary.
Our approach, however, differs fromtheirs in that no special purpose mechanism isused to relate a query to a text, and also thedetail of selecting sentences for inclusion in thesummary is much simpler.
Further, our evalu-ation is completely different in that they judgethe summaries in terms of their capacity to sup-port a relevancy judgment (i.e.
the SUMMACad hoc task) whereas we evaluate the summariesin terms of their capacity tosupport  a questionanswering-type task (MUC template filling).Our technique was tried for 30 of the MUC-6evaluation texts, and summaries produced usingthe length and spread criteria, with and withoutthe initial selection of focus chains.
The fullLaSIE MUC-6 IE system was then run with thesummaries as input, and the resulting extractedtemplates cored.
As a baseline, performanceof the LaSIE system with the full texts as inputwas 45% recall, 64% precision, with 13 of 16 rel-evant texts correctly identified, by the produc-tion of a filled template, and 1 text proposedspuriously.
Performance using the summariesfrom each set of criteria was:Length of core ference  chain:6Z reca l l  81Z prec is ion4 texts  re levant17.7~ of original text word countSpread of coreference chain:2~ recall 89~ precisionI text relevant15.6~ of original text  word countLength of focus chain:I~ recall 80~ precisioni text relevant6.1~ of original text word countSpread of focus chain:i~ recall 80~ precisioni text relevant6.1~ of original text word countThe relevance figures alone give some meas-ure of the information loss between the full textsand the summaries, but the very low recall inall cases suggests that the summaries are reallynot suited to this task.
One significant reasonfor this is that the MUC-6 topic description re-quires the identification of an event involving3 instance types - an organisation, a manage-ment post and a person - while the productionof the summaries i based on the selection of asingle chain representing a single instance.
Theoccurrence of all 3 required instances within thesentences of a single chain can therefore be ex-pected to be rare.
A more fruitful use of thetopic description would be to produce a sum-mary from all chains linking the text to thetopic, rather than a single 'best' chain.
Altern-atively, for event-based topics, chains of eventcoreference r lations could be used.Out of interest, the evaluation was also runon summaries produced without the criteria re-quiring a link between the topic and the text,i.e.
the generic summaries aiming to capture83the single main topic of each text.
The sum-maries produced are considerably longer thanwith the topic description, but this gives con-siderably higher recall for the question answer-ing task.
However, given that the summariserhere is attempting to capture the main topic ofeach text, independently of the question topic,these results are really a measure of the extentto which the text topics and question topic co-incide in the corpus, rather than the suitabilityof the generic summaries for the question an-swering task.Length of core ference  chain:33~ reca l l  67~ prec is ion10 texts  re levant  2 spur ious87.0~ of o r ig ina l  text  word countSpread of  core ference  chain:30~ reca l l  65~ prec is ion10 texts  re levant  2 spur ious41.5~ of  o r ig ina l  text  word countLength of focus  chain:197, reca l l  73Y, p rec i s ion8 texts relevant19.6~.
of original text word countSpread of focus chain:19~, recall 73Y.
precision8 texts relevant18.2~.
of original text word countA much more detailed analysis of these initialresults is required before firm conclusions canbe drawn, but the methodology does representa useful step towards an automated evaluationprocedure.6 Conc lus ionThe use of coreference chains for the productionof text summaries appears to be a promisingtechnique.
Further investigation into the mostappropriate selection criteria for different pur-poses is required, and the current implement-ation is useful for such experimentation.
Todetermine progress, however, automatic eval-uation of summaries will be extremely valu-able, and we have described initial experimentstowards this goal using existing tools and re-sources.
The current results demonstrate theremay need to be subtle interactions between theselection of coreference chains and given topicdescriptions, such as the use of event chains forcertain tasks, but the simple selection of a singlechain may be still be useful as the basis for gen-eric summaries.Re ferencesS.
Azzam, K. Humphreys, and R. Gaizauskas.
1998.Evaluating a focus-based approach to anaphoraresolution.
In Proceedings of COLING-ACL'98,pages 74-78.A.
Bagga and B. Baldwin.
1998.
Entity-based cross-document coreferencing using the vector spacemodel.
In Proceedings of the COLING-ACL'98Joint Conference (the 17th International Confer-ence on Computational Linguistics and 36th An-nual Meeting of the Association for Computa-tional Linguisities, Montreal, August.B.
Baldwin and T.S.
Morton.
1998.
Dynamiccoreference-based ummarization.
In Proceedingsof the Conference on Empirical Methods in Nat-ural Language Processing (EMNLP'98).R.
Barzilay and M. Elhadad.
1997.
Using lexicalchains for text summarization.
In Proceedings ofthe ACL Workshop on Intelligent Scalable TextSummarization, pages 10-17, Madrid, July.DARPA: Defense Advanced Research ProjectsAgency.
1998.
Proceedings of the Seventh Mes-sage Understanding Conference (MUC-7).
Avail-able at http;//www .muc.
saic.
com.R.
Gaizanskas and K. Humphreys.
1997.
Quantit-ative Evaluation of Coreference Algorithms in anInformation Extraction System.
Technical reportCS - 97 - 19, Department of Computer Science,University of Sheffield.R.
Gaizauskas, T. Wakao, K Humphreys, H. Cun-ningham, and Y. Wilks.
1995.
Description of theLaSIE system as used for MUC-6.
In Proceedingsof the Sixth Message Understanding Conference(MUC-6), pages 207-220.
Morgan Kaufmann.M.A.K.
Halliday and R. Hasan.
1976.
Cohesion inEnglish.
Longman, London.I.
Mani, D. House, G. Klein, L. Hirschman, L. Obrst,T.
Firmin, M. Chrzanowski, and B. Sundheim.1998.
The TIPSTER SUMMAC text summariza-tion evaluation: Final report.
MITRE TechnicalReport MTR 98W0000138, MITRE.J.
Morris and G. Hirst.
1994.
Lexical cohesion com-puted by thesanral relations as an indicator ofthe structure of text.
Computational Linguistics,17(1):21-45.K.
Sparck Jones.
1993.
What might be in sum-mary?
In Knorz, Krause, and Womser-Hacker,editors, Information Retrieval 93: Von der Mod-ellie~ung zur Anwendung, pages 9-26.
Universit-atsverlag Konstanz.84
