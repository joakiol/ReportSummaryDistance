Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 1179?1188,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsThe Influence of Discourse on SyntaxA Psycholinguistic Model of Sentence ProcessingAmit DubeyAbstractProbabilistic models of sentence com-prehension are increasingly relevant toquestions concerning human languageprocessing.
However, such models are of-ten limited to syntactic factors.
This paperintroduces a novel sentence processingmodel that consists of a parser augmentedwith a probabilistic logic-based modelof coreference resolution, which allowsus to simulate how context interacts withsyntax in a reading task.
Our simulationsshow that a Weakly Interactive cognitivearchitecture can explain data which hadbeen provided as evidence for the StronglyInteractive hypothesis.1 IntroductionProbabilistic grammars have been found to beuseful for investigating the architecture of thehuman sentence processing mechanism (Jurafsky,1996; Crocker and Brants, 2000; Hale, 2003;Boston et al, 2008; Levy, 2008; Demberg andKeller, 2009).
For example, probabilistic modelsshed light on so-called locality effects: contrastthe non-probabilistic hypothesis that dependantswhich are far away from their head always causeprocessing difficulty for readers due to the costof storing the intervening material in memory(Gibson, 1998), compared to the probabilisticprediction that there are cases when farawaydependants facilitate processing, because readershave more time to predict the head (Levy, 2008).Using a computational model to address funda-mental questions about sentence comprehensionmotivates the work in this paper.So far, probabilistic models of sentence pro-cessing have been largely limited to syntacticfactors.
This is unfortunate because many out-standing questions in psycholinguistics concerninteractions between different levels of process-ing.
This paper addresses this gap by buildinga computational model which simulates theinfluence of discourse on syntax.Going beyond the confines of syntax alone is asufficiently important problem that it has attractedattention from other authors.
In the literature onprobabilistic modeling, though, the bulk of thiswork is focused on lexical semantics (e.g.
Pado?et al, 2006; Narayanan and Jurafsky, 1998) oronly considers syntactic decisions in the preceed-ing text (e.g.
Dubey et al, 2009; Levy and Jaeger,2007).
This is the first model we know of whichintroduces a broad-coverage sentence processingmodel which takes the effect of coreference anddiscourse into account.A major question concerning discourse-syntaxinteractions involves the strength of communica-tion between discourse and syntactic information.The Weakly Interactive (Altmann and Steed-man, 1988) hypothesis states that a discoursecontext can reactively prune syntactic choicesthat have been proposed by the parser, whereasthe Strongly Interactive hypothesis posits thatcontext can proactively suggest choices to thesyntactic processor.Support for Weak Interaction comes fromexperiments in which there are temporary ambi-guities, or garden paths, which cause processingdifficulty.
The general finding is that supportivecontexts can reduce the effect of the garden path.However, Grodner et al (2005) found that sup-portive contexts even facilitate the processing ofunambiguous sentences.
As there are no incorrectanalyses to prune in unambiguous structures, theauthors claimed their results were not consistentwith the Weakly Interactive hypothesis, andsuggested that their results were best explained bya Strongly Interactive processor.The model we present here implements theWeakly Interactive hypothesis, but we will showthat it can nonetheless successfully simulate theresults of Grodner et al (2005).
There are threemain parts of the model: a syntactic processor,a coreference resolution system, and a simplepragmatics processor which computes certainlimited forms of discourse coherence.
FollowingHale (2001) and Levy (2008), among others, thesyntactic processor uses an incremental proba-bilistic Earley parser to compute a metric whichcorrelates with increased reading difficulty.
Thecoreference resolution system is implemented1179in a probabilistic logic known as Markov Logic(Richardson and Domingos, 2006).
Finally, thepragmatics processing system contains a small setof probabilistic constraints which convey someintuitive facts about discourse processing.
Thethree components form a pipeline, where each partis probabilistically dependent on the previous one.This allows us to combine all three into a singleprobability for each reading of an input sentence.The rest of the paper is structured as follows.
InSection 2, we discuss the details two experimentsshowing support of the Weakly and Strongly In-teractive hypotheses: we discuss Grodner et al?sresult on unambiguous syntactic structures and wepresent a new experiment on involving a gardenpath which was designed to be similar to the Grod-ner et al experiment.
Section 3 introduces techni-cal details of model, and Section 4 shows the pre-dictions of the model on the experiments discussedin Section 2.
Finally, we discuss the theoreticalconsequences of these predictions in Section 5.2 Cognitive Experiments2.1 Discourse and Ambiguity ResolutionThere is a fairly large literature on garden pathexperiments involving context (Crain and Steed-man, 1985; Mitchell et al, 1992, ibid).
Theexperiments by Altmann and Steedman (1988)involved PP attachment ambiguity.
Other authors(e.g.
Spivey and Tanenhaus, 1998) have usedreduced relative clause attachment ambiguity.
Inorder to be more consistent with the design of theexperiment in Section 2.2, however, we performedour own reading-time experiment which partiallyreplicated previous results.1The experimental items all had a target sen-tence containing a relative clause, and one of twopossible context sentences, one of which supportsthe relative clause reading and the other whichdoes not.The context sentence was one of:(1) a.
There were two postmen, one ofwhom was injured and carried byparamedics, and another who wasunhurt.b.
Although there was a medical emer-gency at the post office earlier today,regular mail delivery was unaffected.1This experiment was previously reported by Dubey et al(2010).The target sentences, which were drawn from theexperiment of McRae et al (1998), were eitherthe reduced or unreduced sentences similar to:(2) The postman who was carried by theparamedics was having trouble breathing.The reduced version of the sentence is producedby removing the words who was.
We measuredreading times in the underlined region, which isthe first point at which there is evidence for therelative clause interpretation.
The key evidence isgiven by the word ?by?, but the previous word is in-cluded as readers often do not fixate on short func-tion words, but rather process them while overtlyfixating on the previous word (Rayner, 1998).The relative clauses in the target sentence act asrestrictive relative clauses, selecting one referentfrom a larger set.
The target sentences are there-fore more coherent in a context where a restrictedset and a contrast set are easily available, than onein which these sets are absent.
This makes thecontext in Example (1-a) supportive of a reducedrelative reading, and the context in Example (1-b)unsupportive of a reduced relative clause.
Otherexperiments, for instance Spivey and Tanenhaus(1998), used an unsupportive context where onlyone postman was mentioned.
Our experimentsused a neutral context, where no postmen arementioned, to be more similar to the Grodner etal.
experiment, as described below.Overall, there were 28 items, and 28 partici-pants read these sentences using an EyeLink IIeyetracker.
Each participant read items one at atime, with fillers between subsequent items so asto obfuscate the nature of the experiment.Results An ANOVA revealed that all conditionswith a supportive context were read faster than onewith a neutral context (i.e.
a main effect of con-text), and all conditions with unambiguous syntaxwere read faster than those with a garden path(i.e.
a main effect of ambiguity).
Finally, therewas a statisically significant interaction betweensyntax and discourse whereby context decreasesreading times much more when a garden path ispresent compared to an unambiguous structure.
Inother words, a supportive context helped reducethe effect of a garden path.
This is the predictionmade by both the Weakly Interactive and StronglyInteractive hypothesis.
The pattern of results areshown in Figure 2a in Section 4, where they aredirectly compared to the model results.11802.2 Discourse and Unambiguous SyntaxAs mentioned in the Introduction, Grodner et al(2005) proposed an experiment with a supportiveor unsupportive discourse followed by an unam-biguous target sentence.
In their experiment, thetarget sentence was one of the following:(3) a.
The director that the critics praisedat a banquet announced that he wasretiring to make room for youngtalent in the industry.b.
The director, who the critics praisedat a banquet, announced that he wasretiring to make room for youngtalent in the industry.They also manipulated the context, which waseither supportive of the target, or a null context.The two supportive contexts are:(4) a.
A group of film critics praised adirector at a banquet and anotherdirector at a film premiere.b.
A group of film critics praised adirector and a producer for lifetimeachievement.The target sentence in (3-a) is a restrictiverelative clause, as in the garden path exper-iments.
However, the sentence in (3-b) is anon-restrictive relative clause, which does notassume the presence of a constrast set.
Therefore,the context (4-a) is only used with the restrictiverelative clause, and the context (4-b), where onlyone director is mentioned, is used as the contextfor the non-restrictive relative clause.
In theconditions with a null context, the target sentencewas not preceded by any contextual sentence.Results Grodner et al measured residualreading times, i.e.
reading times compared toa baseline in the embedded subject NP (?thecritics?).
They found that the supportive contextsdecreased reading time, and that this effect wasstronger for restrictive relatives compared to non-restricted relatives.
As there was no garden path,and hence no incorrect structure for the discourseprocessor to prune, the authors conclude that thismust be evidence for the Strongly Interactivehypothesis.
Unlike the garden path experimentabove, these results do not appear to be consistentwith a Weakly Interactive model.
We plot theirresults in Figure 3a in Section 4, where they areSNPNPThe postmanVPVBDcarriedPPINbyNP-LGSThe paramedicsVP.
.
.
(a) Standard WSJ TreeSNPNPbaseThe postmanVP-LGSVBD1carriedPP:byIN:bybyNPbase-LGSThe paramedicsVP.
.
.
(b) Minimally Modified TreeFigure 1: A schematic representation of the smallest set ofgrammar transformations which we found were required toaccurately parse the experimental items.directly compared to the model results.
Becausethese results are computed as regressions against abaseline, a reading time of 0ms indicates averagedifficulty, with negative numbers showing somefacilitation has occured, and positive numberindicating reading difficulty.3 ModelThe model comprises three parts: a parser, acoreference resolution system, and a pragmaticssubsystem.
Let us look at each individually.3.1 ParserThe parser is an incremental unlexicalized proba-bilistic Earley parser, which is capable of comput-ing prefix probabilities.
A PCFG parser outputsthe generative probability Pparser(w, t), where w isthe text and t is a parse tree.
A probabilistic Earleyparser can retrieve all possible derivations at wordi (Stolcke, 1995), allowing us to compute the prob-ability P (wi .
.
.
w0) =?t Pparser(wi .
.
.
w0, t).Using the prefix probability, we can computethe word-by-word Surprisal (Hale, 2001), bytaking the log ratio of the previous word?s prefixprobability against this word?s prefix probability:log(P (wi?1 .
.
.
w0)P (wi .
.
.
w0))(1)Higher Surprisal scores are interpreted as1181being correlated with more reading difficulty, andlikewise lower scores with greater reading ease.For most of the remainder of the paper we willsimply refer to the prefix probability at word i asP (w).
While the prefix probability as presentedhere is suitable for syntax-based computations, amain technical contribution of our model, detailedin Sections 3.2 and 3.3 below, is that we includenon-syntactic probabilities in the computation ofSurprisal.As per Hale?s original suggestion, our parsercan compute Surprisal using an exhaustive search,which entails summing over each licensed deriva-tion.
This can be done efficiently using the packedrepresentation of an Earley chart.
However, asthe coreference processor takes trees as input, wemust therefore unpack parses before resolvingreferential ambiguity.
Given the ambiguity of ourgrammar, this is not tractable.
Therefore, we onlyconsider an n-best list when computing Surprisal.As other authors have found that a relatively smallset of analyses can give meaningful predictions(Brants and Crocker, 2000; Boston et al, 2008),we set n = 10.The parser is trained on the Wall Street Journal(WSJ) section of the Penn treebank.
Unfortu-nately, the standard WSJ grammar is not able togive correct incremental parses to our experimen-tal items.
We found we could resolve this problemby using four simple transformations, which areshown in Figure 1: (i) adding valency informationto verb POS tags (e.g.
VBD1 represents a tran-sitive verb); (ii) we lexicalize ?by?
prepositions;(iii) VPs containing a logical subject (i.e.
theagent), get the -LGS label; (iv) non-recursiveNPs are renamed NPbase (the coreference systemtreats each NPbase as a markable).3.2 Discourse ProcessorThe primary function of the discourse processingmodule is to perform coreference resolution foreach mention in an incrementally processed text.Because each mention in a coreference chains istransitive, we cannot use a simple classifier, asthey cannot enforce global transitivity constraints.Therefore, this system is implemented in MarkovLogic (Richardson and Domingos, 2006), aprobabilistic logic, which does allow us to includesuch constraints.Markov Logic attempts to combine logicwith probabilities by using a Markov randomfield where logical formulas are features.
TheExpression MeaningCoref (x , y) x is coreferent with y.First(x ) x is a first mention.Order(x , y) x occurs before y.SameHead(x , y)Do x and y share thesame syntactic head?ExactMatch(x , y) x and y are same string.SameNumber(x , y) x and y match in number.SameGender(x , y) x and y match in gender.SamePerson(x , y) x and y match in person.Distance(x , y , d)The distance betweenx and y, in sentences.Pronoun(x ) x is a pronoun.EntityType(x , e)x has entity type e(person, organization, etc.
)Table 1: Predicates used in the Markov Logic NetworkMarkov Logic Network (MLN) we used for oursystem uses similar predicates as the MLN-basedcorference resolution system of Huang et al(2009).2 Our MLN uses the predicates listedin Table 1.
Two of these predicates, Coref andFirst , are the output of the MLN ?
they providea labelling of coreference mentions into entityclasses.
Note that, unlike Huang et al, we assumean ordering on x and y if Coref (x , y) is true: ymust occur earlier in the document than x. Theremaining predicates in Table 1 are a subset offeatures used by other coreference resolutionsystems (cf.
Soon et al, 2001).
The predicateswe use involve matching strings (checking if twomentions share a head word or if they are exactlythe same string), matching argreement features (ifthe gender, number or person of pairs of NPs arethe same; especially important for pronouns), thedistance between mentions, and if mentions havethe same entity type (i.e.
do they refer to a person,organization, etc.)
As our main focus is not toproduce a state-of-the-art coreference system, wedo not include predicates which are irrevelant forour simulations even if they have been shown to beeffective for coreference resolution.
For example,we do not have predicates if two mentions are inan apposition relationship, or if two mentions aresynonyms for each other.Table 2 lists the actual logical formulae whichare used as features in the MLN.
It should be2As we are not interested in unsupervised inference, thesystem of Poon and Domingos (2008) was unsuitable for ourneeds.1182Description RuleTransitivityCoref (x , z ) ?
Coref (y , z ) ?Order(x , y)?
Coref (x , y)Coref (x , y) ?
Coref (y , z )?
Coref (x , z )Coref (x , y) ?
Coref (x , z ) ?Order(y , z )?
Coref (y , z )First MentionsCoref (x , y)?
?First(x )First(x )?
?Coref (x , y)String MatchExactMatch(x , y)?
Coref (x , y)SameHead(x , y)?
Coref (x , y)PronounPronoun(x ) ?
Pronoun(y) ?
SameGender(x , y)?
Coref (x , y)Pronoun(x ) ?
Pronoun(y) ?
SameNumber(x , y)?
Coref (x , y)Pronoun(x ) ?
Pronoun(y) ?
SamePerson(x , y)?
Coref (x , y)OtherEntityType(x , e) ?
EntityType(y , e)?
Coref (x , y)Distance(x , y ,+d)?
Coref (x , y)Table 2: Rules used in the Markov Logic Networknoted that, because we are assuming an orderon the arguments of Coref (x , y), we need threeformulae to capture transivity relationships.
Totest that the coreference resolution system wasproducing meaningful results, we evaluated oursystem on the test section of the ACE-2 dataset.Using b3 scoring (Bagga and Baldwin, 1998),which computes the overlap of a proposed set withthe gold set, the system achieves an F -score of65.4%.
While our results are not state-of-the-art,they are reasonable considering the brevity of ourfeature list.The discourse model is run iteratively at eachword.
This allows us to find a globally bestassignment at each word, which can be reanalyzedat a later point in time.
It assumes there is amention for each base NP outputted by the parser,and for all ordered pairs of mentions x, y, itoutputs all the ?observed?
predicates (i.e.
ev-erything but First and Coref ), and feeds themto the Markov Logic system.
At each step, wecompute both the maximum a posteriori (MAP)assignment of coreference relationships as wellas the probability that each individual coreferenceassignment is true.
Taken together, they allowus to calculate, for a coreference assignment c,Pcoref(c|w, t) where w is the text input (of theentire document until this point), and t is the parseof each tree in the document up to and includingthe current incremental parse.
As we have previ-ously calculated Pparser(w, t), it is then possibleto compute the joint probability P (c, w, t) at eachword, and therefore the prefix probability P (w)due to syntax and coreference.
Overall, we have:P (w) =?c?tP (c, w, t)=?c?tPcoref(c|w, t)Pparser(w, t)Note that we only consider one possible as-signment of NPs to coreference entities per parse,as we only retrieve the probabilities of the MAPsolution.3.3 Pragmatics ProcessorThe effect of context in the experiments describedin Section 2 cannot be fully explained using acoreference resolution system alone.
In the caseof restrictive relative clauses, the referential ?mis-match?
in the unsupported conditions is causedby an expectation elicited by a restrictive relativeclause which is inconsistent with the previousdiscourse when there is no salient restricted subsetof a larger set.
When the larger set is not foundin the discourse, the relative clause becomesincoherent given the context, causing readingdifficulty.
Modeling this coherence constraint isessentially a pragmatics problem, and is under thepurview of the pragmatics processor in our sys-tem.
The pragmatics processor is quite specialisedand, although the information it encapsulates isquite intuitive, it nonetheless relies on hand-codedexpert knowledge.The pragmatics processor takes as input anincremental pragmatics configuration p andcomputes the probability Pprag(p|w, t, c).
Thepragmatics configuration we consider is quitesimple.
It is a 3-tuple where one element is trueif the current noun phrase being processed is adiscourse new definite noun phrase, the second1183element is true if the current NP is a discoursenew indefinite noun phrase, and the final elementis true if we encounter an unsupported restrictiverelative clause.
We simply conjecture that thereis little processing cost (and hence a high proba-bility) if the entire vector is false; there is a smallprocessing cost for discourse new indefinites,a slightly larger processing cost for discoursenew definites and a large processing cost for anincoherent reduced relative clause.The first two elements of the 3-tuple dependon the identity of the determiner as recovered bythe parser, and on whether the coreference systemadduces the predicate First for the current NP.As the coreference system wasn?t designed tofind anaphoric contrast sets, these sets were foundusing a simple post-processing check.
This post-processing approach worked well for our experi-mental items, but finding such sets is, in general,quite a difficult problem (Modjeska et al, 2003).The distribution Pprag(p|w, t, c) applies aprocessing penalty for an unsupported restrictiverelative clause whenever a restrictive relativeclause is in the n best list.
Because Surprisalcomputes a ratio of probabilities, this in ef-fect means we only pay this penality whenan unsupported restrictive relative clause firstappears in the n best list (otherwise the effect iscancelled out).
The penalty for discourse newentities is applied on the first word (ignoringpunctuation) following the end of the NP.
Thisspillover processing effect is simply a matter ofmodeling convenience: without it, we would haveto compute Surprisal probabilities over regionsrather than individual words.
Thus, the overallprefix probability can be computed as: P (w) =?p,c,t Pprag(p|w, t, c)Pcoref(c|w, t)Pparser(w, t),which is then substituted in Equation (1) to get aSurprisal prediction for the current word.4 Evaluation4.1 MethodWhen modeling the garden path experiment wepresented in Section 2.1, we compute Surprisalvalues on the word ?by?, which is the earliest pointat which there is evidence for a relative clauseinterpretation.
For the Grodner et al experiment,we compute Surprisal values on the relativiser?who?
or ?that?.
Again, this is the earliest point atwhich there is evidence for a relative clause, anddepending upon the presence or absence of a pre-ceding comma, it will be known to be restrictiveor nonrestrictive clause.
In addition to the overallSurprisal values, we also compute syntacticSurprisal scores, to test if there is any benefit fromthe discourse and pragmatics subsystems.
As weare outputting n best lists for each parse, it isalso straightforward to compute other measureswhich predict reading difficulty, including pruning(Jurafsky, 1996), whereby processing difficultyis predicted when a parse is removed from the nbest list, and attention shift (Crocker and Brants,2000), which predicts parsing difficulty at wordswhere the most highly ranked parse flips from oneinterpretation to another.For the garden path experiment, the simulationwas run on each of the 28 experimental items ineach of the 4 conditions, resulting in a total of 112runs.
For the Grodner et al experiment, the sim-ulation was run on each of the 20 items in eachof the 4 conditions, resulting in a total of 80 runs.For each run, the model was reset, purging all dis-course information gained while reading earlieritems.
As the system is not stochastic, two runs us-ing the exact same items in the same condition willproduce the same result.
Therefore, we made noattempt to model by-subject variability, but we didperform by-item ANOVAs on the system output.4.2 ResultsGarden Path Experiment The simulatedresults of our experiment are shown in Figure 2.Comparing the full simulated results in Figure 2bto the experimental results in Figure 2a, we findthat the simulation, like the actual experiment,finds both main effects and an interaction: thereis a main effect of context whereby a supportivecontext facilitates reading, a main effect of syntaxwhereby the garden path slows down reading,and an interaction in that the effect of context isstrongest in the garden path condition.
All theseeffects were highly significant at p < 0.01.
Thepattern of results between the full simulation andthe experiment differed in two ways.
First, thesimulated results suggested a much larger readingdifficulty due to ambiguity than the experimentalresults.
Also, in the unambiguous case, the modelpredicted a null cost of an unsupportive context onthe word ?by?, because the model bears the costof an unsupportive context earlier in the sentence,and assumes no spillover to the word ?by?.
Finally,we note that the syntax-only simulation, shown inFigure 2c, only produced a main effect of ambigu-1184(a) Results from our garden pathexperiment(b) Simulation of our garden pathexperiment(c) Syntax-only simulationFigure 2: The simulated results predict the same interaction as the garden path experiment, but show a stronger main effect ofambiguity, and no influence of discourse in the unambiguous condition on the word ?by?.
(a) Results from the Grodner et alexperiment(b) Simulation of the Grodner et alexperiment(c) Syntax-only simulationFigure 3: The simulated results predict the outcome of the Grodner et al experiment.ity, and was not able to model the effect of context.Grodner et al Experiment The simulatedresults of the Grodner et al experiment are shownin Figure 3.
In this experiment, the pattern ofsimulated results in Figure 3b showed a muchcloser resemblance to the experimental results inFigure 3a than the garden path experiment.
Thereis a main effect of context, which is much strongerin the restrictive relative case compared to non-restrictive relatives.
As with the garden pathexperiment, the ANOVA reported that all effectswere significant at the p < 0.01 level.
Again, aswe can see from Figure 3c, there was no effectof context in the syntax-only simulation.
The nu-merical trend did show a slight facilitation in theunrestricted supported condition, with a Surprisalof 4.39 compared to 4.41 in the supported case,but this difference was not significant.4.3 DiscussionWe have shown that our incremental sentence pro-cessor augmented with discourse processing cansuccessfully simulate syntax-discourse interactioneffects which have been shown in the literature.The difference between a Weakly Interactiveand Strongly Interactive model can be thought ofcomputationally in terms of a pipeline architectureversus joint inference.
In a weaker sense, evena pipeline architecture where the discourse caninfluence syntactic probabilities could be claimedto be a Strongly Interactive model.
However,as our model uses a pipeline where syntacticprobabilities are independent of the discourse,we claim that our model is Weakly Interactive.Unlike Altmann and Steedman, who posited thatthe discourse processor actually removes parsinghypotheses, we were able to simulate this pruningbehaviour by simply re-weighting parses in ourcoreference and pragmatics modules.The fact that a Weakly Interactive system cansimulate the result of an experiment proposed insupport of the Strongly Interactive hypothesis isinitially counter-intuitive.
However, this naturallyfalls out from our decision to use a probabilistic1185SNPNPbaseThe postmanVP-LGSVBD1carriedPP:byIN:byby.
.
.. .
.
(a) Best parse: p = 9.99 ?
10?10 mainclause, expecting more dependentsSNPNPbaseThe postmanVP-LGSVBD1carriedPP:byIN:byby.
.
.
(b) 2nd parse: p = 9.93 ?
10?10main clause, no more dependentsSNPNPbaseThe postmanVP-LGSVBD1carriedPP:byIN:byby.
.
.. .
.
(c) 3rd parse: p = 7.69?
?10 relativeclauseFigure 4: The top three parses on the word ?by?
in the ourfirst experimental item.model: a lower probability, even in an unambigu-ous structure, is associated with increased readingdifficulty.
As an aside, we note that when usingrealistic computational grammars, even the struc-tures used in the Grodner et al experiment arenot unambiguous.
In the restrictive relative clausecondition, even though there was not any compe-tition between a relative and main clause reading,our n best list was at all times filled with analyses.For example, on the word ?who?
in the restrictedrelative clause condition, the parser is alreadypredicting both the subject-relative (?the postmanwho was bit by the dog?)
and object-relative (?thepostman who the dog bit?)
readings.Overall, these results are supportive of thegrowing importance of probabilistic reasoning asa model of human cognitive behaviour.
Therefore,especially with respect to sentence processing,it is necessary to have a proper understandingof how probabilities are linked to real-worldbehaviours.
We note that Surprisal does indeedshow processing difficulty on the word ?by?
inthe garden path experiment.
However, Figure 4(which shows the top three parses on the word?by?)
indicates that not only are there still mainclause interpretations present, but in fact, thetop two parses are main clause interpretations.This is also true if we limit ourselves to syntacticprobabilities (which are the probabilities listedin Figure 4).
This suggests that neither Jurafsky(1996)?s notion of pruning as processing difficultynor Crocker and Brants (2000) notion of attentionshifts would correctly predict higher reading timeson a region containing the word ?by?.
In fact, themain clause interpretation remains the highest-ranked interpretation until it is finally pruned at anauxiliary of the main verb of the sentence (?Thepostman carried by the paramedics was having?
).This result is curious as our experimental itemsclosely match some of those simulated by Crockerand Brants (2000).
We conjecture that the differ-ence between our attention shift prediction andtheirs is due to differences in the grammar.
It ispossible that using a more highly tuned grammarwould result in attention shift making the correctprediction, but this possibly shows one benefit ofusing Surprisal as a linking hypothesis.
BecauseSurprisal sums over several derivations, it is notas reliant upon the grammar as the attention shiftor pruning linking hypotheses.5 ConclusionsThe main result of this paper is that it is possibleto produce a Surprisal-based sentence process-ing model which can simulate the influence ofdiscourse on syntax in both garden path andunambiguous sentences.
Computationally, theinclusion of Markov Logic allowed the discoursemodule to compute well-formed coreferencechains, and opens two avenues of future re-search.
First, it ought to be possible to make theprobabilistic logic more naturally incremental,rather than re-running from scratch at each word.Second, we would like to make greater use of thelogical elements by applying it to problems whereinference is necessary, such as resolving bridginganaphora (Haviland and Clark, 1974).Our primary cognitive finding that our model,which assumes the Weakly Interactive hypothesis(whereby discourse is influenced by syntax in areactive manner), is nonetheless able to simulatethe experimental results of Grodner et al (2005),which were claimed by the authors to be in1186support of the Strongly Interactive hypothesis.This suggests that the evidence is in favour of theStrongly Interactive hypothesis may be weakerthan thought.Finally, we found that the attention shift(Crocker and Brants, 2000) and pruning (Jurafsky,1996) linking theories are unable to correctlysimulate the results of the garden path experiment.Although our main results above underscore theusefulness of probabilistic modeling, this obser-vation emphasizes the importance of finding atenable link between probabilities and behaviours.AcknowledgementsWe would like to thank Frank Keller, PatrickSturt, Alex Lascarides, Mark Steedman, MirellaLapata and the anonymous reviewers for theirinsightful comments.
We would also like to thankESRC for their financial supporting on grantRES-062-23-1450.ReferencesGerry Altmann and Mark Steedman.
Inter-action with context during human sentenceprocessing.
Cognition, 30:191?238, 1988.Amit Bagga and Breck Baldwin.
Algorithms forscoring coreference chains.
In The First Inter-national Conference on Language Resourcesand Evaluation Workshop on LinguisticsCoreference (LREC 98), 1998.Marisa Ferrara Boston, John T. Hale, ReinholdKliegl, and Shravan Vasisht.
Surprising parseractions and reading difficulty.
In Proceedingsof ACL-08:HLT, Short Papers, pages 5?8, 2008.Thorsten Brants and Matthew Crocker.
Probabilis-tic parsing and psychological plausibility.
InProceedings of 18th International Conferenceon Computational Linguistics (COLING-2000),pages 111?117, 2000.Stephen Crain and Mark Steedman.
On not beingled down the garden path: the use of contextby the psychological syntax processor.
InD.
Dowty, L. Karttunen, and A. Zwicky, edi-tors, Natural language parsing: Psychological,computational, and theoretical perspectives.Cambridge University Press, 1985.Matthew Crocker and Thorsten Brants.
Widecoverage probabilistic sentence processing.Journal of Psycholinguistic Research, 29(6):647?669, 2000.Vera Demberg and Frank Keller.
A computationalmodel of prediction in human parsing: Unifyinglocality and surprisal effects.
In Proceedingsof the 29th meeting of the Cognitive ScienceSociety (CogSci-09), 2009.Amit Dubey, Frank Keller, and Patrick Sturt.A probabilistic corpus-based model of paral-lelism.
Cognition, 109(2):193?210, 2009.Amit Dubey, Patrick Sturt, and Frank Keller.
Theeffect of discourse inferences on syntactic am-biguity resolution.
In Proceedings of the 23rdAnnual CUNY Conference on Human SentenceProcessing (CUNY 2010), page 151, 2010.Ted Gibson.
Linguistic complexity: Locality ofsyntactic dependencies.
Cognition, 68:1?76,1998.Daniel J. Grodner, Edward A. F. Gibson, andDuane Watson.
The influence of contextualconstrast on syntactic processing: Evidence forstrong-interaction in sentence comprehension.Cognition, 95(3):275?296, 2005.John T. Hale.
A probabilistic earley parser asa psycholinguistic model.
In In Proceedingsof the Second Meeting of the North AmericanChapter of the Asssociation for ComputationalLinguistics, 2001.John T. Hale.
The information conveyed bywords in sentences.
Journal of PsycholinguisticResearch, 32(2):101?123, 2003.Susan E. Haviland and Herbert H. Clark.
What?snew?
acquiring new information as a processin comprehension.
Journal of Verbal Learningand Verbal Behavior, 13:512?521, 1974.Shujian Huang, Yabing Zhang, Junsheng Zhou,and Jiajun Chen.
Coreference resolution usingmarkov logic.
In Proceedings of the 2009Conference on Intelligent Text Processing andComputational Linguistics (CICLing 09), 2009.D.
Jurafsky.
A probabilistic model of lexical andsyntactic access and disambiguation.
CognitiveScience, 20:137?194, 1996.Roger Levy.
Expectation-based syntactic compre-hension.
Cognition, 106(3):1126?1177, March2008.Roger Levy and T. Florian Jaeger.
Speakersoptimize information density through syntacticreduction.
In Proceedings of the TwentiethAnnual Conference on Neural InformationProcessing Systems, 2007.1187Ken McRae, Michael J. Spivey-Knowlton, andMichael K. Tanenhaus.
Modeling the influenceof thematic fit (and other constraints) in on-linesentence comprehension.
Journal of Memoryand Language, 38:283?312, 1998.Don C. Mitchell, Martin M. B. Corley, and AlanGarnham.
Effects of context in human sentenceparsing: Evidence against a discourse-baedproposal mechanism.
Journal of ExperimentalPsychology: Learning, Memory and Cognition,18(1):69?88, 1992.Natalia N. Modjeska, Katja Markert, and MalvinaNissim.
Using the web in machine learning forother-anaphora resolution.
In Proceedings ofthe 2003 Conference on Empirical Methods inNatural Language Processing (EMNLP-2003),pages 176?183, Sapporo, Japan, 2003.Shrini Narayanan and Daniel Jurafsky.
Bayesianmodels of human sentence processing.
In Pro-ceedings of the 20th Annual Conference of theCognitive Science Society (CogSci 98), 1998.Ulrike Pado?, Matthew Crocker, and Frank Keller.Modelling semantic role plausability in humansentence processing.
In Proceedings of the 28thAnnual Conference of the Cognitive ScienceSociety (CogSci 2006), pages 657?662, 2006.Hoifung Poon and Pedro Domingos.
Joint unsu-pervised coreference resolution with markovlogic.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural LanguageProcessing (EMNLP-08), 2008.Keith Rayner.
Eye movements in reading andinformation processing: 20 years of research.Psychological Bulletin, 124(3):372?422, 1998.Matthew Richardson and Pedro Domingos.Markov logic networks.
Machine Learning, 62(1-2):107?136, 2006.W.
M. Soon, H. T. Ng, and D. C. Y. Lim.
Amachine learning approach to coreferenceresolution of noun phrases.
ComputationalLinguistics, 27(4):521?544, 2001.M.
J. Spivey and M. K. Tanenhaus.
Syntacticambiguity resolution in discourse: Modelingthe effects of referential context and lexicalfrequency.
Journal of Experimental Psychol-ogy: Learning, Memory and Cognition, 24(6):1521?1543, 1998.Andreas Stolcke.
An efficient probabilisticcontext-free parsing algorithm that computesprefix probabilities.
Computational Linguistics,21(2):165?201, 1995.1188
