Extracting Parallel Phrases from Comparable DataSanjika Hewavitharana and Stephan VogelLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{sanjika,vogel+}@cs.cmu.eduAbstractMining parallel data from comparable corporais a promising approach for overcoming thedata sparseness in statistical machine trans-lation and other NLP applications.
Even iftwo comparable documents have few or noparallel sentence pairs, there is still poten-tial for parallelism in the sub-sentential level.The ability to detect these phrases creates avaluable resource, especially for low-resourcelanguages.
In this paper we explore threephrase alignment approaches to detect paral-lel phrase pairs embedded in comparable sen-tences: the standard phrase extraction algo-rithm, which relies on the Viterbi path; aphrase extraction approach that does not relyon the Viterbi path, but uses only lexical fea-tures; and a binary classifier that detects par-allel phrase pairs when presented with a largecollection of phrase pair candidates.
We eval-uate the effectiveness of these approaches indetecting alignments for phrase pairs that havea known alignment in comparable sentencepairs.
The results show that the Non-Viterbialignment approach outperforms the other twoapproaches on F1 measure.1 IntroductionStatistical Machine Translation (SMT), like manynatural language processing tasks, relies primarilyon parallel corpora.
The translation performance ofSMT systems directly depends on the quantity andthe quality of the available parallel data.
However,such corpora are only available in large quantitiesfor a handful of languages, including English, Ara-bic, Chinese and some European languages.
Muchof this data is derived from parliamentary proceed-ings, though a limited amount of newswire text isalso available.
For most other languages, especiallyfor less commonly used languages, parallel data isvirtually non-existent.Comparable corpora provide a possible solutionto this data sparseness problem.
Comparable doc-uments are not strictly parallel, but contain roughtranslations of each other, with overlapping infor-mation.
A good example for comparable documentsis the newswire text produced by multilingual newsorganizations such as AFP or Reuters.
The de-gree of parallelism can vary greatly, ranging fromnoisy parallel documents that contain many paral-lel sentences, to quasi parallel documents that maycover different topics (Fung and Cheung, 2004).The Web is by far the largest source of compara-ble data.
Resnik and Smith (2003) exploit the sim-ilarities in URL structure, document structure andother clues for mining the Web for parallel docu-ments.
Wikipedia has become an attractive source ofcomparable documents in more recent work (Smithet al, 2010).Comparable corpora may contain parallel data indifferent levels of granularity.
This includes: par-allel documents, parallel sentence pairs, or parallelsub-sentential fragments.
To simplify the processand reduce the computational overhead, the paral-lel sentence extraction is typically divided into twotasks.
First, a document level alignment is iden-tified between comparable documents, and second,the parallel sentences are detected within the iden-tified document pairs.
Cross-lingual information re-trieval methods (Munteanu and Marcu, 2005) and61Proceedings of the 4th Workshop on Building and Using Comparable Corpora, pages 61?68,49th Annual Meeting of the Association for Computational Linguistics,Portland, Oregon, 24 June 2011. c?2011 Association for Computational LinguisticsFigure 1: Sample comparable sentences that contain parallel phrasesother similarity measures (Fung and Cheung, 2004)have been used for the document alignment task.Zhao and Vogel (2002) have extended parallel sen-tence alignment algorithms to identify parallel sen-tence pairs within comparable news corpora.
Till-mann and Xu (2009) introduced a system that per-forms both tasks in a single run without any doc-ument level pre-filtering.
Such a system is usefulwhen document level boundaries are not available inthe comparable corpus.Even if two comparable documents have few orno parallel sentence pairs, there could still be paral-lel sub-sentential fragments, including word transla-tion pairs, named entities, and long phrase pairs.
Theability to identify these pairs would create a valu-able resource for SMT, especially for low-resourcelanguages.
The first attempt to detect sub-sententialfragments from comparable sentences is (Munteanuand Marcu, 2006).
Quirk et al (2007) later ex-tended this work by proposing two generative mod-els for comparable sentences and showed improve-ments when applied to cross-domain test data.
Inboth these approaches the extracted fragment datawas used as additional training data to train align-ment models.
Kumano et al (2007) have proposed aphrasal alignment approach for comparable corporausing the joint probability SMT model.
While thisapproach is appealing for low-resource scenarios asit does not require any seed parallel corpus, the highcomputational cost is a deterrent in its applicabilityto large corpora.In this paper we explore several phrase alignmentapproaches to detect parallel phrase pairs embeddedin comparable sentence pairs.
We assume that com-parable sentence pairs have already been detected.Our intention is to use the extracted phrases directlyin the translation process, along with other phrasepairs extracted from parallel corpora.
In particular,we study three alignment approaches:?
the standard phrase extraction algorithm, whichrelies on the Viterbi path of the word alignment;?
a phrase extraction approach that does not relyon the Viterbi path, but only uses lexical fea-tures;?
and a binary classifier to detect parallel phrasepairs when presented with a large collection ofphrase pair candidates.We evaluate the effectiveness of these approachesin detecting alignments for phrase pairs that have aknown translation a comparable sentence pair.
Sec-tion 2 introduces the phrase alignment problem incomparable sentences and discusses some of thechallenges involved.
It also explains the differ-ent alignment approaches we explore.
Section 3presents the experimental setup and the results ofthe evaluation.
We conclude, in section 4, with ananalysis of the results and some directions for futurework.62Figure 2: Word-to-word alignment pattern for (a) a parallel sentence pair (b) a non-parallel sentence pair2 Parallel Phrase ExtractionFigure 1 shows three sample sentences that were ex-tracted from Gigaword Arabic and Gigaword En-glish collections.
For each comparable sentencepair, the Arabic sentence is shown first, followed byits literal English translation (in Italics).
The Englishsentence is shown next.
The parallel sections in eachsentence are marked in boldface.
In the first two sen-tences pairs, the English sentence contains the fulltranslation of the Arabic sentence, but there are addi-tional phrases on the English side that are not presenton the Arabic sentence.
These phrases appear at thebeginning of sentence 1 and at the end of sentence2.
In sentence 3, there are parallel phrases as wellas phrases that appear only on one side.
The phrase?to Iraq?
appears only on the Arabic sentence whilethe phrase ?the former Egyptian foreign minister?appears only on the English side.Standard word alignment and phrase alignmentalgorithms are formulated to work on parallel sen-tence pairs.
Therefore, these standard algorithms arenot well suited to operate on partially parallel sen-tence pairs.
Presence of non-parallel phrases mayresult in undesirable alignments.Figure 2 illustrates this phenomenon.
It comparesa typical word alignment pattern in a parallel sen-tence pair (a) to one in a non-parallel sentence pair(b).
The darkness of a square indicates the strengthof the word alignment probability between the corre-sponding word pair.
In 2(a), we observe high proba-bility word-to-word alignments (dark squares) overthe entire length of the sentences.
In 2(b), we seeone dark area above ?weapons of mass destruction?,corresponding to the parallel phrase pair, and somescattered dark spots, where high frequency Englishwords pair with high frequency Arabic words.
Thisspurious alignments pose problems to the phrasealignment, and indicate that word alignment prob-abilities alone might not be sufficient.Our aim is to identify such parallel phrase pairsfrom comparable sentence pairs.
In the followingsubsections we briefly explain the different phrasealignment approaches we use.2.1 Viterbi AlignmentHere we use the typical phrase extraction approachused by Statistical Machine Translation systems:obtain word alignment models for both directions(source to target and target to source), combine theViterbi paths using one of many heuristics, and ex-tract phrase pairs from the combined alignment.
Weused Moses toolkit (Koehn et al, 2007) for this task.To obtain the word alignments for comparable sen-tence pairs, we performed a forced alignment usingthe trained models.2.2 Binary ClassifierWe used a Maximum Entropy classifier as oursecond approach to extract parallel phrase pairsfrom comparable sentences.
Such classifiers havebeen used in the past to detect parallel sentencepairs in large collections of comparable documents(Munteanu and Marcu, 2005).
Our classifier is sim-ilar, but we apply it at phrase level rather than atsentence level.
The classifier probability is defined63as:p(c|S, T ) = exp (?ni=1 ?ifi(c, S, T ))Z(S, T ) , (1)where S = sL1 is a source phrase of length L andT = tK1 is a target phrase of length K. c ?
{0, 1}is a binary variable representing the two classes ofphrases: parallel and not parallel.
p(c|S, T ) ?
[0, 1]is the probability where a value p(c = 1|S, T ) closeto 1.0 indicates that S and T are translations of eachother.
fi(c, S, T ) are feature functions that are co-indexed with respect to the class variable c. The pa-rameters ?i are the weights for the feature functionsobtained during training.
Z(S, T ) is the normal-ization factor.
In the feature vector for phrase pair(S, T ), each feature appears twice, once for eachclass c ?
{0, 1}.The feature set we use is inspired by Munteanuand Marcu (2005) who define the features basedon IBM Model-1 (Brown et al, 1993) alignmentsfor source and target pairs.
However, in our ex-periments, the features are computed primarily onIBM Model-1 probabilities (i.e.
lexicon).
We donot explicitly compute IBM Model-1 alignments.
Tocompute coverage features, we identify alignmentpoints for which IBM Model-1 probability is abovea threshold.
We produce two sets of features basedon IBM Model-1 probabilities obtained by trainingin both directions.
All the features have been nor-malized with respect to the source phrase length Lor the target phrase length K. We use the following11 features:1.
Lexical probability (2): IBM Model-1 logprobabilities p(S|T ) and p(T |S)2.
Phrase length ratio (2): source length ratioK/L and target length ratio L/K3.
Phrase length difference (1): source length mi-nus target length, L?K4.
Number of words covered (2): A source words is said to be covered if there is a target wordt ?
T such that p(s|t) > , where  = 0.5.Target word coverage is defined accordingly.5.
Number of words not covered (2): This is com-puted similarly to 4. above, but this time count-ing the number of positions that are not cov-ered.6.
Length of the longest covered sequence ofwords (2)To train the classifier, we used parallel phrasespairs extracted from a manually word-aligned cor-pus.
In selecting negative examples, we followed thesame approach as in (Munteanu and Marcu, 2005):pairing all source phrases with all target phrases, butfilter out the parallel pairs and those that have highlength difference or a low lexical overlap, and thenrandomly select a subset of phrase pairs as the neg-ative training set.
The model parameters are esti-mated using the GIS algorithm.2.3 Non-Viterbi (PESA) AlignmentA phrase alignment algorithm called ?PESA?
thatdoes not rely on the Viterbi path is described in (Vo-gel, 2005).
PESA identifies the boundaries of thetarget phrase by aligning words inside the sourcephrase with words inside the target phrase, and sim-ilarly for the words outside the boundaries of thephrase pair.
It does not attempt to generate phrasealignments for the full sentence.
Rather, it identifiesthe best target phrase that matches a given sourcephrase.
PESA requires a statistical word-to-wordlexicon.
A seed parallel corpus is required to au-tomatically build this lexicon.This algorithm seems particularly well suited inextracting phrase pairs from comparable sentencepairs, as it is designed to not generate a completeword alignment for the entire sentences, but to findonly the target side for a phrase embedded in thesentence.
We briefly explain the PESA alignmentapproach below.Instead of searching for all possible phrase align-ments in a parallel sentence pair, this approachfinds the alignment for a single source phrase S =s1 .
.
.
sl.
Assume that we have a parallel sentencepair (sJ1 , tI1) which contains the source phrase S inthe source sentence sJ1 .
Now we want to find thetarget phrase T = t1 .
.
.
tk in the target sentence tI1which is the translation of the source phrase.A constrained IBM Model-1 alignment is now ap-plied as follows:?
Source words inside phrase boundary arealigned only with the target words inside thephrase boundary.
Source words outside the64phrase boundary are only aligned with targetwords outside the phrase boundary.?
Position alignment probability for the sentence,which is 1/I in IBM Model-1, is modified to be1/k inside the source phrase and to 1/(I ?
k)outside the phrase.Figure 3 shows the different regions.
Given thesource sentence and the source phrase from positionj1 to j2, we want to find the boundaries of the targetphrase, i1 and i2.
The dark area in the middle is thephrase we want to align.
The size of the blobs ineach box indicates the lexical strength of the wordpair.Figure 3: PESA Phrase alignmentThe constrained alignment probability is calculatedas follows:p(s|t) =??j1?1?j=1?i/?
(i1...i2)1I ?
kp(sj |ti)????
?j2?j=j1i2?i=i11kp(sj |ti)??
(2)???J?j=j2+1?i/?
(i1...i2)1I ?
kp(sj |ti)?
?p(t|s) is similarly calculated by switching sourceand target sides in equation 2:p(t|s) =??i1?1?i=1?i/?
(j1...j2)1J ?
l p(ti|sj)????
?i2?i=i1j2?j=j11l p(ti|sj)??
(3)??
?I?i=i2+1?j /?
(j1...j2)1J ?
l p(ti|sj)?
?To find the optimal target phrase boundaries, we in-terpolate the two probabilities in equations 2 and 3and select the boundary (i1, i2) that gives the highestprobability.
(i1, i2) = argmaxi1,i2{(1?
?)
log(p(s|t))+ ?
log(p(t|s))} (4)The value of ?
is estimated using held-out data.PESA can be used to identify all possible phrasepairs in a given parallel sentence pair by iteratingover every source phrase.
An important difference isthat each phrase is found independently of any otherphrase pair, whereas in the standard phrase extrac-tion they are tied through the word alignment of thesentence pair.There are several ways we can adapt the non-Viterbiphrase extraction to comparable sentence.?
Apply the same approach assuming the sen-tence pair as parallel.
The inside of the sourcephrase is aligned to the inside of the targetphrase, and the outside, which can be non-parallel, is aligned the same way.?
Disregard the words that are outside the phrasewe are interested in.
Find the best target phraseby aligning only the inside of the phrase.
Thiswill considerably speed-up the alignment pro-cess.3 Experimental Results3.1 Evaluation SetupWe want to compare the performance of the differ-ent phrase alignment methods in identifying paral-lel phrases embedded in comparable sentence pairs.651 2 3 4 5 6 7 8 9 10 Alltest set 2,826 3,665 3,447 3,048 2,718 2,414 2,076 1,759 1,527 1,378 24,858test set (found) 2,746 2,655 1,168 373 87 29 7 2 1 0 7,068Table 1: N-gram type distribution of manually aligned phrases setUsing a manually aligned parallel corpus, and twomonolingual corpora, we obtained a test corpus asfollows: From the manually aligned corpus, we ob-tain parallel phrase pairs (S, T ).
Given a source lan-guage corpus S and a target language corpus T , foreach parallel phrase pair (S, T ) we select a sentences from S which contains S and a target sentencet from T which contains T .
These sentence pairsare then non-parallel, but contain parallel phrases,and for each sentence pair the correct phrase pairis known.
This makes it easy to evaluate differentphrase alignment algorithms.Ideally, we would like to see the correct targetphrase T extracted for a source phrase S. How-ever, even if the boundaries of the target phrase donot match exactly, and only a partially correct trans-lation is generated, this could still be useful to im-prove translation quality.
We therefore will evaluatethe phrase pair extraction from non-parallel sentencepairs also in terms of partial matches.To give credit to partial matches, we define pre-cision and recall as follows: Let W and G denotethe extracted target phrase and the correct referencephrase, respectively.
Let M denote the tokens in Wthat are also found in the reference G. ThenPrecision = |M ||W | ?
100 (5)Recall = |M ||G| ?
100 (6)These scores are computed for each extracted phrasepair, and are averaged to produce precision and re-call for the complete test set.
Finally, precision andrecall are combined to generated the F-1 score in thestandard way:F1 = 2 ?
Precision ?RecallPrecision+Recall (7)3.2 EvaluationWe conducted our experiments on Arabic-Englishlanguage pair.
We obtained manual alignments for663 Arabic-English sentence pairs.
From this, weselected 300 sentences, and extracted phrase pairsup to 10 words long that are consistent with the un-derlying word alignment.
From the resulting list ofphrase pairs, we removed the 50 most frequentlyoccurring pairs as well as those only consisting ofpunctuations.
Almost all high frequency phrases arefunction words, which are typically covered by thetranslation lexicon.
Line 1 in Table 1 gives the n-gram type distribution for the source phrases.Using the phrase pairs extracted from the manu-ally aligned sentences, we constructed a comparablecorpus as follows:1.
For each Arabic phrase, we search the ArabicGigaword1 corpus for sentences that containthe phrase and select up to 5 sentences.
Sim-ilarly, for each corresponding English phrasewe select up to 5 sentences from English Gi-gaword2.2.
For each phrase pair, we generate the Cartesianproduct of the sentences and produce a sen-tence pair collection.
I.e.
up to 25 comparablesentence pairs were constructed for each phrasepair.3.
We only select sentences up to 100 words long,resulting in a final comparable corpus consist-ing of 170K sentence pairs.Line 2 in Table 1 gives the n-gram type distribu-tion for the phrase pairs for which we found both asource sentence and a target sentence in the mono-lingual corpora.
As expected, the longer the phrases,the less likely it is to find them in even larger cor-pora.We consider the resulting set as our comparablecorpus which we will use to evaluate all alignmentapproaches.
In most sentence pairs, except for thephrase pair that we are interested in, the rest of thesentence does not typically match the other side.1Arabic Gigaword Fourth Edition (LDC2009T30)2English Gigaword Fourth Edition (LDC2009T13)66Lexicon Viterbi Classifier PESAExact P R F1 Exact P R F1 Exact P R F1Lex-Full 43.56 65.71 57.99 61.61 54.46 81.79 85.29 85.29 67.94 93.34 86.80 90.22Lex-1/3 42.95 65.68 56.69 60.85 53.57 81.32 88.34 84.69 67.28 93.23 86.17 89.56Lex-1/9 41.10 63.60 51.15 56.70 52.38 80.30 86.64 83.35 65.81 91.95 84.73 88.19Lex-1/27 41.02 62.10 49.38 55.01 52.51 80.51 83.84 82.14 63.23 89.41 82.06 85.57Lex-BTEC 19.10 26.94 23.63 25.18 18.76 45.90 36.17 40.46 17.45 46.70 36.28 40.83Table 2: Results for Alignment Evaluation of test phrasesWe obtained the Viterbi alignment using stan-dard word alignment techniques: IBM4 word align-ment for both directions, Viterbi path combinationusing heuristics (?grow-diag-final?)
and phrase ex-traction from two-sided training, as implemented inthe Moses package (Koehn et al, 2007).
Becausethe non-parallel segments will lead the word align-ment astray, this may have a negative effect on thealignment in the parallel sections.
Alignment mod-els trained on parallel data are used to generate theViterbi alignment for the comparable sentences.
Wethen extract the target phrases that are aligned tothe embedded source phrases.
A phrase pair is ex-tracted only when the alignment does not conflictwith other word alignments in the sentence pair.
Thealignments are not constrained to produce contigu-ous phrases.
We allow unaligned words to be presentin the phrase pair.
For each source phrase we se-lected the target phrase that has the least number ofunaligned words.The classifier is applied at the phrase level.
Wegenerate the phrase pair candidates as follows: Fora given target sentence we generate all n-grams upto length 10.
We pair each n-gram with the sourcephrase embedded in the corresponding source sen-tence to generate a phrase pair.
From the 170 thou-sand sentence pairs, we obtained 15.6 million phrasepair candidates.
The maximum entropy classifier isthen applied to the phrase pairs.
For each sourcephrase, we pick the target candidate for which p(c =1, S, T ) has the highest value.For the PESA alignment we used both inside andoutside alignments, using only lexical probabilities.For each source phrase pair, we select the best scor-ing target phrase.As our goal is to use these methods to extractparallel data for low resource situations, we testedeach method with several lexica, trained on differ-ent amounts of initial parallel data.
Starting from thefull corpus with 127 million English tokens, we gen-erated three additional parallel corpora with 1/3, 1/9and 1/27 of the original size.
The 1/9 and 1/27 cor-pora (with 13 million and 4 million English words)can be considered medium and small sized corpora,respectively.
These two corpora are a better matchto the resource levels for many languages.
We alsoused data from the BTEC (Kikui et al, 2003) cor-pus.
This corpus contains conversational data fromthe travel domain, which is from a different genrethan the document collections.
Compared to othercorpora, it is much smaller (about 190 thousand En-glish tokens).Table 2 gives the results for all three alignment ap-proaches.
Results are presented as percentages of:exact matches found (Exact), precision (P), recall(R) and F1.
The Viterbi alignment gives the lowestperformance.
This shows that the standard phraseextraction procedure, which works well for parallelsentence, is ill-suited for partially parallel sentences.Despite the fact that the classifier incorporates sev-eral features including the lexical features, the per-formance of the PESA alignment, which uses onlythe lexical features, has consistently higher precisionand recall than the classifier.
This demonstrates thatcomputing both inside and outside probabilities forthe sentence pair helps the phrase extraction.
Theclassifier lacks this ability because the phrase pairis evaluated in isolation, without the context of thesentence.Except for the BTEC corpus, the performancedegradation is minimal as the lexicon size is re-duced.
This shows that the approaches are robustfor smaller parallel amounts of parallel data.Instead of using token precision, an alternative67method of evaluating partial matches, is to givecredit based on the length of the overlap betweenthe extracted phrase and the reference.
Precision andrecall can then be defined based on the longest com-mon contiguous subsequence, similar to (Bourdail-let et al, 2010).
Results obtained using this methodswere similar to the results in Table 2.4 Conclusion and Future WorkIn this paper we explored several phrase alignmentapproaches for extracting phrase pairs that are em-bedded inside comparable sentence pairs.
We usedthe standard Viterbi phrase alignment, a maximumentropy classifier that works on phrase pairs, and anon-Viterbi PESA alignment in the evaluation pro-cess.
The results show that PESA outperforms boththe Viterbi approach and the classifier, in both preci-sion and recall.We plan to extend the PESA framework to usenot only lexical features, but other features similarto the ones used in the classifier.
We believe thiswill further improve the alignment accuracy.While this paper focuses on comparisons of dif-ferent phrase alignment approaches in a realistic, yetcontrolled manner by selecting appropriate compa-rable sentence pairs for given phrase pairs, futureexperiments will focus on finding new phrase pairsfrom comparable corpora and evaluating the poten-tial utility of the extracted data in the context of anend-to-end machine translation system.ReferencesJulien Bourdaillet, Ste?phane Huet, Philippe Langlais, andGuy Lapalme.
2010.
TransSearch: from a bilingualconcordancer to a translation finder.
Machine Trans-lation, 24(3-4):241?271, dec.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19(2):263?311.Pascale Fung and Percy Cheung.
2004.
Mining verynon-parallel corpora: Parallel sentence and lexicon ex-traction via bootstrapping and EM.
In In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing, pages 57?63, Barcelona, Spain.Genichiro Kikui, Eiichiro Sumita, Toshiyuki Takezawa,and Seiichi Yamamoto.
2003.
Creating corporafor speech-to-speech translation.
In In Proc.
of EU-ROSPEECH 2003, pages 381?384, Geneva.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-stantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the Association forComputational Linguistics, Prague, Czech Republic,June.Tadashi Kumano, Hideki Tanaka, and Takenobu Toku-naga.
2007.
Extracting phrasal alignments from com-parable corpora by using joint probability smt model.In In Proceedings of the International Conference onTheoretical and Methodological Issues in MachineTranslation, Skvde, Sweden, September.Dragos Stefan Munteanu and Daniel Marcu.
2005.
Im-proving machine translation performance by exploit-ing non-parallel corpora.
Computational Linguistics,31(4):477?504.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting parallel sub-sentential fragments from non-parallel corpora.
In Proceedings of the 21st Interna-tional Conference on Computational Linguistics andthe 44th Annual Meeting of the Association for Com-putational Linguistics, pages 81?88, Sydney, Aus-tralia.Chris Quirk, Raghavendra U. Udupa, and Arul Menezes.2007.
Generative models of noisy translations withapplications to parallel fragment extraction.
In Pro-ceedings of the Machine Translation Summit XI, pages377?384, Copenhagen, Denmark.Philip Resnik and Noah Smith.
2003.
The web as a par-allel corpus.
Computational Linguistics, 29(3):349?380.Jason R. Smith, Chris Quirk, and Kristina Toutanova.2010.
Extracting parallel sentences from compara-ble corpora using document level alignment.
In Pro-ceedings of the Human Language Technologies/NorthAmerican Association for Computational Linguistics,pages 403?411.Christoph Tillmann and Jian-Ming Xu.
2009.
A sim-ple sentence-level extraction algorithm for comparabledata.
In Companion Vol.
of NAACL HLT 09, Boulder,CA, June.Stephan Vogel.
2005.
PESA: Phrase pair extractionas sentence splitting.
In Proceedings of the MachineTranslation Summit X, Phuket, Thailand, September.Bing Zhao and Stephan Vogel.
2002.
Full-text storyalignment models for chinese-english bilingual newscorpora.
In Proceedings of the ICSLP ?02, September.68
