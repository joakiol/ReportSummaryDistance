Web Corpus Mining by instance of WikipediaRu?diger Gleim, Alexander Mehler & Matthias DehmerBielefeld University, D-33615 Bielefeld, GermanyRuediger.Gleim@uni-bielefeld.deAlexander.Mehler@uni-bielefeld.deTechnische Universita?t Darmstadt, Fachbereich Informatikdehmer@tk.informatik.tu-darmstadt.deAbstractIn this paper we present an approach tostructure learning in the area of web doc-uments.
This is done in order to approachthe goal of webgenre tagging in the area ofweb corpus linguistics.
A central outcomeof the paper is that purely structure ori-ented approaches to web document classi-fication provide an information gain whichmay be utilized in combined approaches ofweb content and structure analysis.1 IntroductionIn order to reliably judge the collocative affinity oflinguistic items, it has to be considered that judge-ments of this kind depend on the scope of certaingenres or registers.
According to Stubbs (2001),words may have different collocates in differenttext types or genres and therefore may signal oneof those genres when being observed.
Conse-quently, corpus analysis requires, amongst oth-ers, a comparison of occurrences in a given textwith typical occurrences in other texts of the samegenre (Stubbs, 2001, p. 120).This raises the question how to judge the mem-bership of texts, in which occurrences of linguisticitems are observed, to the genres involved.
Evi-dently, because of the size of the corpora involved,this question is only adequately answered by ref-erence to the area of automatic classification.
Thisholds all the more for web corpus linguistics (Kil-garriff and Grefenstette, 2003; Baroni and Bernar-dini, 2006) where large corpora of web pages,whose membership in webgenres is presently un-known, have to be analyzed.
Consequently, webcorpus linguistics faces two related task:1.
Exploration: The task of initially exploringwhich webgenres actually exist.2.
Categorization: The task of categorizing hy-pertextual units according to their member-ship in the genres being explored in the latterstep.In summary, web corpus linguistics is in needof webgenre-sensitive corpora, that is, of corporain which for the textual units being incorporatedthe membership to webgenres is annotated.
Thisin turn presupposes that these webgenres are firstof all explored.Currently to major classes of approaches canbe distinguished: On the one hand, we find ap-proaches to the categorization of macro structures(Amitay et al, 2003) such as web hierarchies, di-rectories and corporate sites.
On the other hand,this concerns the categorization of micro struc-tures as, for example, single web pages (Klein-berg, 1999) or even page segments (Mizuuchi andTajima, 1999).
The basic idea of all these ap-proaches is to perform categorization as a kind offunction learning for mapping web units above, onor below the level of single pages onto at mostone predefined category (e.g.
genre label) per unit(Chakrabarti et al, 1998).
Thus, these approachesfocus on the categorization task while disregardingthe exploration task.
More specifically, the ma-jority of these approaches utilizes text categoriza-tion methods in conjunction with HTML markup,metatags and link structure beyond bag-of-wordrepresentations of the pages?
wording as input offeature selection (Yang et al, 2002) ?
in somecases also of linked pages (Fu?rnkranz, 1999).What these approaches are missing is a moregeneral account of web document structure as asource of genre-oriented categorization.
That is,67they solely map web units onto feature vectors bydisregarding their structure.
This includes linkagebeyond pairwise linking as well as document inter-nal structures according to the Document ObjectModel (DOM).
A central pitfall of this approach isthat it disregards the impact of genre membershipto document structure and, thus, the signalling ofthe former by the latter (Ventola, 1987).
Thereforea structure-sensitive approach is needed in the areaof corpus linguistics which allows for automaticwebgenre tagging.
That is, an approach whichtakes both levels of structuring of web documentsinto account: On the level of their hyperlink-basedlinkage and on the level of their internal structure.In this paper we present an algorithm as apreliminary step for tackling the exploration andcategorization task together.
More specifically,we present an approach to unsupervised structurelearning which uses tree alignment algorithms assimilarity kernels and cluster analysis for class de-tection.
The paper includes a comparative study ofseveral approaches to tree alignment as a source ofsimilarity measuring of web documents.
Its cen-tral topics are:?
To what extent is it possible to predict themembership of a web document in a certaingenre (or register) solely on grounds of itsstructure when its lexical content and othercontent bearing units are completely deleted?In other words, we ask to what extent struc-ture signals membership in genre.?
A more methodical question regards thechoice of appropriate measures of structuralsimilarity to be included into structure learn-ing.
In this context, we comparatively studyseveral variants of measuring similarities oftrees, that is, tree edit distance as well as aclass of algorithms which are based on treelinearizations as input to sequence alignment.Our overall findings hint at two critical points:First, there is a significant contribution ofstructure-oriented methods to webgenre catego-rization which is unexplored in predominant ap-proaches.
Second, and most surprisingly, allmethods analyzed toughly compete with a methodbased on random linearization of input documents.Why is this research important for web corpuslinguistics?
An answer to this question can be out-lined as follows:?
We explore a further resource of reliably tag-ging web genres and registers, respectively,in the form of document structure.?
We further develop the notion of webgenreand thus help to make document structure ac-cessible to collocation and other corpus lin-guistic analyses.In order to support this argumentation, we firstpresent a structure insensitive approach to web cat-egorization in section (2).
It shows that this in-sensitivity systematically leads to multiple cate-gorizations which cannot be traced back to ambi-guity of category assignment.
In order to solvethis problem, an alternative approach to structurelearning is presented in sections (3.1), (3.2) and(3.3).
This approach is evaluated in section (3.4)on grounds of a corpus of Wikipedia articles.
Thereason for utilizing this test corpus is that thecontent-based categories which the explored webdocuments belong to are known so that we can ap-ply the classical apparatus of evaluation of webmining.
The final section concludes and prospectsfuture work.2 Hypertext CategorizationThe basic assumption behind present day ap-proaches to hypertext categorization is as follows:Web units of similar function/content tend to havesimilar structures.
The central problem is thatthese structures are not directly accessible by seg-menting and categorizing single web pages.
Thisis due to polymorphism and its reversal relationof discontinuous manifestation: Generally speak-ing, polymorphism occurs if the same (hyper-)textual unit manifests several categories.
Thisone-to-many relation of expression and contentunits is accompanied by a reversal relation accord-ing to which the same content or function unitis distributed over several expression units.
Thiscombines to a many-to-many relation betweenexplicit, manifesting web structure and implicit,manifested functional or content-based structure.Our hypothesis is that if polymorphism is aprevalent characteristic of web units, web pagescannot serve as input of categorization since poly-morphic pages simultaneously instantiate severalcategories.
Moreover, these multiple categoriza-tions are not simply resolved by segmenting thefocal pages, since they possibly manifest cate-gories only discontinuously so that their features68do not provide a sufficient discriminatory power.In other words: We expect polymorphism and dis-continuous manifestation to be accompanied bymany multiple categorizations without being re-ducible to the problem of disambiguating categoryassignments.
In order to show this, we performa categorization experiment according to the clas-sical setting of function learning, using a corpusof the genre of conference websites.
Since thesewebsites serve recurrent functions (e.g.
paper sub-mission, registration etc.)
they are expected to bestructured homogeneously on the basis of stable,recurrent patterns.
Thus, they can be seen as goodcandidates of categorization.The experiment is performed as follows: Weapply support vector machine (SVM) classifica-tion which proves to be successful in case ofsparse, high dimensional and noisy feature vec-tors (Joachims, 2002).
SVM classification is per-formed with the help of the LibSVM (Hsu et al,2003).
We use a corpus of 1,078 English con-ference websites and 28,801 web pages.
Hyper-text representation is done by means of a bag-of-features approach using about 85,000 lexicaland 200 HTML features.
This representation wasdone with the help of the HyGraph system whichexplores websites and maps them onto hypertextgraphs (Mehler and Gleim, 2005).
Following (Hsuet al, 2003), we use a Radial Basis Function ker-nel and make optimal parameter selection basedon a minimization of a 5-fold cross validation er-ror.
Further, we perform a binary categorizationfor each of the 16 categories based on 16 trainingsets of pos./neg.
examples (see table 1).
The sizeof the training set is 1,858 pages (284 sites); thesize of the test set is 200 (82 sites).
We perform 3experiments:1.
Experiment A ?
one against all: First we ap-ply a one against all strategy, that is, we useX \ Yi as the set of negative examples forlearning category Ci where X is the set of alltraining examples and Yi is the set of positiveexamples of Ci.
The results are listed in table(1).
It shows the expected low level of effec-tivity: recall and precession perform very lowon average.
In three cases the classifiers failcompletely.
This result is confirmed whenlooking at column A of table (2): It showsthe number of pages with up to 7 categoryassignments.
In the majority of cases no cat-egory could be applied at all ?
only one-thirdCategory rec.
prec.Abstract(s) 0.2 1.0Accepted Papers 0.3 1.0Call for Papers 0.1 1.0Committees 0.5 0.8Contact Information 0 0Exhibition 0.4 1.0Important Dates 0.8 1.0Invited Talks 0 0Menu 0.7 0.7Photo Gallery 0 0Program, Schedule 0.8 1.0Registration 0.9 1.0Sections, Sessions, Plenary etc.
0.1 0.3Sponsors and Partners 0 0Submission Guidelines etc.
0.5 0.8Venue, Travel, Accommodation 0.9 1.0Table 1: The categories of the conference websitegenre applied in the experiment.of the pages was categorized.2.
Experiment B ?
lowering the discriminatorypower: In order to augment the number ofcategorizations, we lowered the categories?selectivity by restricting the number of neg-ative examples per category to the numberof the corresponding positive examples bysampling the negative examples according tothe sizes of the training sets of the remain-ing categories.
The results are shown in ta-ble (2): The number of zero categorizationsis dramatically reduced, but at the same timethe number of pages mapped onto more thanone category increases dramatically.
Thereare even more than 1,000 pages which aremapped onto more than 5 categories.3.
Experiment C ?
segment level categorization:Thirdly, we apply the classifiers trained onthe monomorphic training pages on segmentsderived as follows: Pages are segmented intospans of at least 30 tokens reflecting segmentborders according to the third level of thepages?
document object model trees.
Col-umn C of table (2) shows that this scenariodoes not solve the problem of multiple cate-gorizations since it falls back to the problemof zero categorizations.
Thus, polymorphismis not resolved by simply segmenting pages,as other segmentations along the same line ofconstraints confirmed.There are competing interpretations of these re-sults: The category set may be judged to be wrong.But it reflects the most differentiated set appliedso far in this area.
Next, the representation model69number of ca- A B Ctegorizations page level page level segment level0 12,403 346 27,1481 6,368 2387 9,3542 160 5076 1373 6 5258 14 0 3417 05 0 923 06 0 1346 07 0 184 0Table 2: The number of pages mapped onto0, 1, ..., 7 categories in experiment A, B and C.may be judged to be wrong, but actually it is usu-ally applied in text categorization.
Third, the cate-gorization method may be seen to be ineffective,but SVMs are known to be one of the most ef-fective methods in this area.
Further, the clas-sifiers may be judged to be wrong ?
of coursethe training set could be enlarged, but already in-cludes about 2,000 manually selected monomor-phic training units.
Finally, the focal units (i.e.web pages) may be judged to be unsystematicallypolymorph in the sense of manifesting several log-ical units.
It is this interpretation which we believeto be supported by the experiment.If this interpretation is true, the structure ofweb documents comes into focus.
This raisesthe question, what can be gained at all when ex-ploring the visible structuring of documents asfound on the web.
That is, what is the infor-mation gain when categorizing documents solelybased on their structures.
In order to approach thisquestion we perform an experiment in structure-oriented classification in the next section.
Aswe need to control the negative impact of poly-morphism, we concentrate on monomorphic pageswhich uniquely belong to single categories.
Thiscan be guaranteed with the help of Wikipedia arti-cles which, with the exception of special disam-biguation pages, only address one topic respec-tively.3 Structure-Based Categorization3.1 MotivationIn this section we investigate how far a corpus ofdocuments can be categorized by solely consid-ering the explicit document structure without anytextual content.
It is obvious that we cannot ex-pect the results to reach the performance of con-tent based approaches.
But if this approach allowsto significantly distinguish between categories incontrast to a reference random decider we can con-clude that the involvement of structure informa-tion may positively affect categorization perfor-mance.
A positive evaluation can be seen to mo-tivate an implementation of the Logical DocumentStructure (LDS) algorithm proposed by Mehler etal.
(2005) who include graph similarity measuringas its kernel.
We expect the same experiment toperform significantly better on the LDS instead ofthe explicit structures.
However this experimentcan only be seen as a first attempt.
Further studieswith larger corpora are required.3.2 Experiment setupIn our experiment, we chose a corpus of articlesfrom the German Wikipedia addressing the fol-lowing categories:?
American Presidents (41 pages)?
European Countries (50 pages)?
German Cities (78 pages)?
German Universities (93 pages)With the exception of the first category most ar-ticles, being represented as a HTML web page,share a typical, though not deterministic visiblestructure.
For example a Wikipedia article about acity contains an info box to the upper right whichlists some general information like district, pop-ulation and geographic location.
Furthermore anarticle about a city contains three or more sectionswhich address the history, politics, economicsand possibly famous buildings or persons.
Like-wise there exist certain design guidelines by theWikipedia project to write articles about countriesand universities.
However these guidelines are notalways followed or they are adapted from one caseto another.
Therefore, a categorization cannot relyon definite markers in the content.
Nevertheless,the expectation is that a human reader, once he hasseen a few samples of each category, can with highprobability guess the category of an article by sim-ple looking at the layout or visible structure and ig-noring the written content.
Since the layout (esp.the structure) of a web page is encoded in HTMLwe consider the structure of their DOM1-trees forour categorization experiment.
If two articles ofthe same category share a common visible struc-ture, this should lead to a significant similarity of1Document Object Model.70the DOM-trees.
The articles of category ?Ameri-can Presidents?
form an exception to this principleup to now because they do not have a typical struc-ture.
The articles about the first presidents are rel-atively short whereas the articles about the recentpresidents are much more structured and complex.We include this category to test how well a struc-ture based categorizer performs on such diversestructurations.
We examine two corpus variants:I.
All HTML-Tags of a DOM-tree are used forsimilarity measurement.II.
Only those HTML-tags of a DOM-tree areused which have an impact on the visiblestructure (i.e.
inline tags like font or i are ig-nored).Both cases, I and II, do not include any textnodes.
That is, all lexical content is ignored.
Bydistinguishing these two variants we can examinewhat impact these different degrees of expressive-ness have on the categorization performance.3.3 Distance measurement and clusteringThe next step of the experiment is marked by apairwise similarity measurement of the wikipediaarticles which are represented by their DOM-treesaccording to the two variants described in section3.2.
This allows to create a distance matrix whichrepresents the (symmetric) distances of a given ar-ticle to any other.
In a subsequent and final stepthe distance matrix will be clustered and the re-sults analyzed.How to measure the similarity of two DOM-trees?
This raises the question what exactly thesubject of the measurement is and how it can beadequately modeled.
Since the DOM is a tree andthe order of the HTML-tags matters, we chooseordered trees.
Furthermore we want to representwhat tag a node represents.
This leads to orderedlabeled trees for representation.
Since trees area common structure in various areas such as im-age analysis, compiler optimization and bio infor-matics (i.e.
RNA analysis) there is a high inter-est in methods to measure the similarity betweentrees (Tai, 1979; Zhang and Shasha, 1989; Klein,1998; Chen, 2001; Ho?chsmann et al, 2003).
Oneof the first approaches with a reasonable compu-tational complexity was introduced by Tai (1979)who extended the problem of sequence edit dis-tance to trees.T2T1Post-order linearization and alignment:T2ST1SHTMLHEADTITLE H1 PHTMLHEADTITLE H1TITLE HEAD H1 P HTMLTITLE HEAD H1 HTML<gap>BODYBODYBODY BODYFigure 1: An example for Post-order linearizationand sequence alignment.The following description of tree edit distancesis due to Bille (2003): The principle to computethe edit distance between two trees T1, T2is tosuccessively perform elementary edit operationson the former tree to turn it into the formation ofthe latter.
The edit operations on a given tree Tare as follows: Relabel changes the label of a nodev ?
T .
Delete deletes a non-root node v ?
T witha parent node w ?
T .
Since v is being deleted,its child nodes (if any) are inserted as children ofnode w. Finally the Insert operation marks thecomplement of delete.
Next, an edit script S isa list of consecutive edit operations which turn T1into T2.
Given a cost function for each edit opera-tion the cost of S is the sum of its elementary op-eration costs.
The optimal edit script (there is pos-sibly more than one) between T1and T2is givenby the edit script of minimum cost which equalsthe tree edit distance.There are various algorithms known to com-pute the edit distance (Tai, 1979; Zhang andShasha, 1989; Klein, 1998; Chen, 2001).
Theyvary in computational complexity and whetherthey can be used for general purpose or un-der special restrictions only (which allows forbetter optimization).
In this experiment weuse the general-purpose algorithm of Zhangand Shasha (1989) which shows a complexityof O(|T1||T2|min(L1,D1)min(L2,D2)) where|Ti|, Li, Di denote the number of nodes, the num-ber of leafs and the depth of the trees respectively.The approach of tree edit distance forms a goodbalance between accurate distance measurementof trees and computational complexity.
However,especially for large corpora it might be usefulto examine how well other (i.e.
faster) methods71perform.
We therefore consider another class ofalgorithms for distance measurement which arebased on sequence alignments via dynamic pro-gramming.
Since this approach is restricted to thecomparison of sequences, a suitable linearizationof the DOM trees has to be found.
For this task weuse several strategies of tree node traversal: Pre-Order, Post-Order and Breath-First-Search (BFS)traversal.
Figure (1) shows a linearization of twosample trees using Post-Order and how the result-ing sequences STi may have been aligned for thebest alignment distance.
We have enhanced thelabels of the linearized nodes by adding the in-and out degrees corresponding to the former po-sition of the nodes in the tree.
This informationcan be used during the computation of the align-ment cost: An example of this is that the align-ment of two nodes with identical HTML-tags butdifferent in/out degrees will result in a higher costthan in cases where these degrees match.
Follow-ing this strategy, at least part of the structure in-formation is preserved.
This approach is followedby Dehmer (2005) who develops a special form oftree linearization which is based on tree levels.Obviously, a linearization poses a loss of struc-ture information which has impact on the resultsof distance measurement.
But the computationalcomplexity of sequence alignments (O(n2)) is sig-nificantly better than of tree edit distances.
Thisleads to a trade-off between the expressiveness ofthe DOM-Tree representation (in our case tree vs.linearization to a sequence) and the complexity ofthe algorithms to compute the distance thereon.In order to have a baseline for tree linearizationtechniques we have also tested random lineariza-tions.
According to this method, trees are trans-formed into sequences of nodes in random order.For our experiment we have generated 16 randomlinearizations and computed the median of theircategorization performances.Next, we perform pairwise distance measure-ments of the DOM-trees using the set of algo-rithms described above.
We then apply two clus-tering methods on the resulting distance matrices:hierarchical agglomerative and k-means cluster-ing.
Hierarchical agglomerative clustering doesnot need any information on the expected numberof clusters so we examine all possible clusteringsand chose the one maximizing the F -measure.However we also examine how well hierarchicalclustering performs if the number of partitions isrestricted to the number of categories.
In contrastto the previous approach, k-means needs to be in-formed about the number of clusters in advance,which in the present experiment equals the num-ber of categories, which in our case is four.
Be-cause we know the category of each article we canperform an exhaustive parameter study to maxi-mize the well known efficiency measures purity,inverse purity and the combined F -measure.3.4 Results and discussionThe tables (3) and (5) show the results for cor-pus variant I (using all HTML-tags) and variantII (using structure relevant HTML-tags only) (seesection 3.2).
The general picture is that hierarchi-cal clustering performs significantly better than k-means.
However this is only the case for an un-restricted number of clusters.
If we restrict thenumber of clusters for hierarchical clustering tothe number of categories, the differences becomemuch less apparent (see tables 4 and 6).
The onlyexception to this is marked by the tree edit dis-tance: The best F -measure of 0.863 is achievedby using 58 clusters.
If we restrict the number ofclusters to 4, tree edit still reaches an F -measureof 0.710 which is significantly higher than the bestk-means result of 0.599.As one would intuitively expect the resultsachieved by the tree edit distance are much bet-ter than the variants of tree linearization.
The editdistance operates on trees whereas the other algo-rithms are bound to less informative sequences.Interestingly, the differences become much lessapparent for the corpus variant II which consistsof the simplified DOM-trees (see section 3.2).
Wecan assume that the advantage of the tree edit dis-tance over the linearization-based approaches di-minishes, the smaller the trees to be compared are.The performance of the different variants of treelinearization vary only significantly in the case ofunrestricted hierarchical clustering (see tables 3and 5).
In the case of k-means as well as in thecase of restricting hierarchical clustering to ex-actly 4 clusters, the performances are about equal.In order to provide a baseline for better ratingthe cluster results, we perform random clustering.This leads to an F -measure of 0.311 (averagedover 1,000 runs).
Content-based categorizationexperiments using the bag of words model havereported F -measures of about 0.86 (Yang, 1999).The baseline for the different variants of lin-72Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specificaltree edit distance hierarchical 58 0.863 0.996 0.786 none weighted linkagepost-order linearization hierarchical 13 0.775 0.809 0.775 spearman single linkagepre-order linearization hierarchical 19 0.741 0.817 0.706 spearman single linkagetree level linearization hierarchical 36 0.702 0.882 0.603 spearman single linkagebfs linearization hierarchical 13 0.696 0.698 0.786 spearman single linkagetree edit distance k-means 4 0.599 0.618 0.641 - cosine distancepre-order linearization k-means 4 0.595 0.615 0.649 - cosine distancepost-order linearization k-means 4 0.593 0.615 0.656 - cosine distancetree level linearization k-means 4 0.593 0.603 0.649 - cosine distancerandom lin.
(medians only) - - 0.591 0.563 0.795 - -bfs linearization k-means 4 0.580 0.595 0.656 - cosine distance- random 4 0.311 0.362 0.312 - -Table 3: Evaluation results using all tags.Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specificaltree edit distance hierarchical 4 0.710 0.698 0.851 spearman single linkagebfs linearization hierarchical 4 0.599 0.565 0.851 none weighted linkagetree level linearization hierarchical 4 0.597 0.615 0.676 spearman complete linkagepost-order linearization hierarchical 4 0.595 0.615 0.683 spearman average linkagepre-order linearization hierarchical 4 0.578 0.599 0.660 cosine average linkageTable 4: Evaluation results using all tags and hierarchical clustering with a fixed number of clusters.earization is given by random linearizations: Weperform 16 random linearizations, run the differ-ent variants of distance measurement as well asclustering and compute the median of the best F -measure values achieved.
These are 0.591 for cor-pus variant I and 0.581 for the simplified vari-ant II.
These results are in fact surprising becausethey are only little worse than the other lineariza-tion techniques.
This result may indicate that ?in the present scenario ?
the linearization basedapproaches to tree distance measurement are notsuitable because of the loss of structure informa-tion.
More specifically, this raises the followingantithesis: Either, the sequence-oriented modelsof measuring structural similarities taken into ac-count are insensitive to the structuring of web doc-uments.
Or: this structuring only counts whatregards the degrees of nodes and their labels ir-respective of their order.
As tree-oriented meth-ods perform better, we view this to be an argu-ment against linearization oriented methods, atleast what regards the present evaluation scenarioto which only DOM trees are input but not moregeneral graph structures.The experiment has shown that analyzing thedocument structure provides a remarkable amountof information to categorization.
It also shows thatthe sensitivity of the approaches used in differentcontexts needs to be further explored which wewill address in our future research.4 ConclusionWe presented a cluster-based approach to struc-ture learning in the area of web documents.
Thiswas done in order to approach the goal of a com-bined algorithm of webgenre exploration and cat-egorization.
As argued in section (1), such an al-gorithm is needed in web corpus linguistics forwebgenre tagging as a prerequisite of measuringgenre-sensitive collocations.
In order to evaluatethe present approach, we utilized a corpus of wiki-based articles.
The evaluation showed that there isan information gain when measuring the similar-ities of web documents irrespective of their lexi-cal content.
This is in the line of the genre modelof systemic functional linguistics (Ventola, 1987)which prospects an impact of genre membershipon text structure.
As the corpus used for evalua-tion is limited to tree-like structures, this approachis in need for further development.
Future workwill address this task.
This regards especially theclassification of graph-like representations of webdocuments which take their link structure into ac-count.ReferencesEinat Amitay, David Carmel, Adam Darlow, RonnyLempel, and Aya Soffer.
2003.
The connectivitysonar: detecting site functionality by structural pat-terns.
In Proc.
of the 14th ACM conference on Hy-pertext and Hypermedia, pages 38?47.Marco Baroni and Silvia Bernardini, editors.
2006.WaCky!
Working papers on the Web as corpus.Gedit, Bologna, Italy.Philip Bille.
2003.
Tree edit distance, alignment dis-tance and inclusion.
Technical report TR-2003-23.Soumen Chakrabarti, Byron Dom, and Piotr Indyk.73Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specificaltree edit distance hierarchical 51 0.756 0.905 0.691 none weighted linkagepre-order linearization hierarchical 20 0.742 0.809 0.771 spearman single linkagepost-order linearization hierarchical 23 0.732 0.813 0.756 spearman single linkagetree level linearization hierarchical 2 0.607 0.553 0.878 spearman weighted linkagebfs linearization hierarchical 4 0.589 0.603 0.641 cosine weighted linkagetree edit distance k-means 4 0.713 0.718 0.718 - cosine distancepre-order linearization k-means 4 0.587 0.603 0.634 - cosine distancetree level linearization k-means 4 0.584 0.603 0.641 - cosine distancebfs linearization k-means 4 0.583 0.599 0.637 - cosine distancepost-order linearization k-means 4 0.582 0.592 0.630 - cosine distancerandom lin.
(medians only) - - 0.581 0.584 0.674 - -- random 4 0.311 0.362 0.312 - -Table 5: Evaluation results using structure relevant tags only.Similarity Algorithm Clustering Algorithm # Clusters F-Measure Purity Inverse Purity PW Distance Method-Specificaltree edit distance hierarchical 4 0.643 0.645 0.793 spearman average linkagepost-order linearization hierarchical 4 0.629 0.634 0.664 spearman average linkagetree level linearization hierarchical 4 0.607 0.595 0.679 spearman weighted linkagebfs linearization hierarchical 4 0.589 0.603 0.641 cosine weighted linkagepre-order linearization hierarchical 4 0.586 0.603 0.660 cosine complete linkageTable 6: Evaluation results using all tags and hierarchical clustering with a fixed number of clusters.1998.
Enhanced hypertext categorization using hy-perlinks.
In Proceedings of ACM SIGMOD Inter-national Conference on Management of Data, pages307?318.
ACM.Weimin Chen.
2001.
New algorithm for ordered tree-to-tree correction problem.
Journal of Algorithms,40(2):135?158.Matthias Dehmer.
2005.
Strukturelle Analyse Web-basierter Dokumente.
Ph.D. thesis, Technische Uni-versita?t Darmstadt, Fachbereich Informatik.Johannes Fu?rnkranz.
1999.
Exploiting structural infor-mation for text classification on the WWW.
In Pro-ceedings of the Third International Symposium onAdvances in Intelligent Data Analysis, pages 487?498, Berlin/New York.
Springer.M.
Ho?chsmann, T. To?ller, R. Giegerich, and S. Kurtz.2003.
Local similarity in rna secondary struc-tures.
In Proc.
Computational Systems Bioinformat-ics, pages 159?168.Chih-Wei Hsu, Chih-Chung Chang, and Chih-Jen Lin.2003.
A practical guide to SVM classification.Technical report, Department of Computer Scienceand Information Technology, National Taiwan Uni-versity.Thorsten Joachims.
2002.
Learning to classify textusing support vector machines.
Kluwer, Boston.Adam Kilgarriff and Gregory Grefenstette.
2003.
In-troduction to the special issue on the web as corpus.Computational Linguistics, 29(3):333?347.P.
Klein.
1998.
Computing the edit-distance betweenunrooted ordered trees.
In G. Bilardi, G. F. Italiano,A.
Pietracaprina, and G. Pucci, editors, Proceedingsof the 6th Annual European Symposium, pages 91?102, Berlin.
Springer.Jon M. Kleinberg.
1999.
Authoritative sources ina hyperlinked environment.
Journal of the ACM,46(5):604?632.Alexander Mehler and Ru?diger Gleim.
2005.
The netfor the graphs ?
towards webgenre representationfor corpus linguistic studies.
In Marco Baroni andSilvia Bernardini, editors, WaCky!
Working paperson the Web as corpus.
Gedit, Bologna, Italy.Alexander Mehler, Ru?diger Gleim, and MatthiasDehmer.
2005.
Towards structure-sensitive hyper-text categorization.
In Proceedings of the 29th An-nual Conference of the German Classification Soci-ety, Berlin.
Springer.Yoshiaki Mizuuchi and Keishi Tajima.
1999.
Findingcontext paths for web pages.
In Proceedings of the10th ACM Conference on Hypertext and Hyperme-dia, pages 13?22.Michael Stubbs.
2001.
On inference theories and codetheories: Corpus evidence for semantic schemas.Text, 21(3):437?465.K.
C. Tai.
1979.
The tree-to-tree correction problem.Journal of the ACM, 26(3):422?433.Eija Ventola.
1987.
The Structure of Social Interac-tion: a Systemic Approach to the Semiotics of Ser-vice Encounters.
Pinter, London.Yiming Yang, Sean Slattery, and Rayid Ghani.
2002.A study of approaches to hypertext categorization.Journal of Intelligent Information Systems, 18(2-3):219?241.Yiming Yang.
1999.
An evaluation of statistical ap-proaches to text categorization.
Journal of Informa-tion Retrieval, 1(1/2):67?88.K.
Zhang and D. Shasha.
1989.
Simple fast algorithmsfor the editing distance between trees and relatedproblems.
SIAM Journal of Computing, 18:1245?1262.74
