Proceedings of NAACL-HLT 2013, pages 1206?1215,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsKnowledge-Rich Morphological Priors for Bayesian Language ModelsVictor Chahuneau Noah A. Smith Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA 15213, USA{vchahune,nasmith,cdyer}@cs.cmu.eduAbstractWe present a morphology-aware nonparamet-ric Bayesian model of language whose priordistribution uses manually constructed finite-state transducers to capture the word forma-tion processes of particular languages.
Thisrelaxes the word independence assumptionand enables sharing of statistical strengthacross, for example, stems or inflectionalparadigms in different contexts.
Our modelcan be used in virtually any scenario wheremultinomial distributions over words wouldbe used.
We obtain state-of-the-art results inlanguage modeling, word alignment, and un-supervised morphological disambiguation fora variety of morphologically rich languages.1 IntroductionDespite morphological phenomena?s salience inmost human languages, many NLP systems treatfully inflected forms as the atomic units of language.By assuming independence of lexical stems?
vari-ous surface forms, this avoidance approach exacer-bates the problem of data sparseness.
If it is em-ployed at all, morphological analysis of text tendsto be treated as a preprocessing step to other NLPmodules.
While this latter disambiguation approachhelps address data sparsity concerns, it has substan-tial drawbacks: it requires supervised learning fromexpert-annotated corpora, and determining the op-timal morphological granularity is labor-intensive(Habash and Sadat, 2006).Neither approach fully exploits the finite-statetransducer (FST) technology that has been so suc-cessful for modeling the mapping between surfaceforms and their morphological analyses (Karttunenand Beesley, 2005), and the mature collections ofhigh quality transducers that already exist for manylanguages (e.g., Turkish, Russian, Arabic).
Muchlinguistic knowledge is encoded in such FSTs.In this paper, we develop morphology-aware non-parametric Bayesian language models that bring to-gether hand-written FSTs with statistical modelingand require no token-level annotation.
The sparsityissue discussed above is addressed by hierarchicalpriors that share statistical strength across differentinflections of the same stem by backing off to wordformation models that piece together morphemes us-ing FSTs.
Furthermore, because of the nonparamet-ric formulation of our models, the regular morpho-logical patterns found in the long tail of word typeswill rely more heavily on deeper analysis, while fre-quent and idiosyncratically behaved forms are mod-eled opaquely.Our prior can be used in virtually any generativemodel of language as a replacement for multino-mial distributions over words, bringing morphologi-cal awareness to numerous applications.
For variousmorphologically rich languages, we show that:?
our model can provide rudimentary unsuper-vised disambiguation for a highly ambiguousanalyzer;?
integrating morphology into n-gram languagemodels allows better generalization to unseenwords and can improve the performance of ap-plications that are truly open vocabulary; and?
bilingual word alignment models also bene-fit greatly from sharing translation information1206across stems.We are particularly interested in low-resource sce-narios, where one has to make the most of thesmall quantity of available data, and overcomingdata sparseness is crucial.
If analyzers exist in suchsettings, they tend to be highly ambiguous, and an-notated data for learning to disambiguate are alsolikely to be scarce or non-existent.
Therefore, in ourexperiments with Russian, we compare two analyz-ers: a rapidly-developed guesser, which models reg-ular inflectional paradigms but contains no lexiconor irregular forms, and a high-quality analyzer.2 Word Models with MorphologyIn this section, we describe a generative model ofword formation based on Pitman-Yor processes thatgenerates word types using a finite-state morpho-logical generator.
At a high level, the process firstproduces lexicons of stems and inflectional patterns;then it generates a lexicon of inflected forms us-ing the finite-state generator.
Finally, the inflectedforms are used to generate observed data.
Differentindependence assumptions can be made at each ofthese levels to encode beliefs about where stems, in-flections, and surface forms should share statisticalstrength.2.1 Pitman-Yor ProcessesOur work relies extensively on Pitman-Yor pro-cesses, which provide a flexible framework for ex-pressing backoff and interpolation relationships andextending standard models with richer word distri-butions (Pitman and Yor, 1997).
They have beenshown to match the performance of state-of-the-artlanguage models and to give estimates that followappropriate power laws (Teh, 2006).A draw from a Pitman-Yor process (PYP), de-noted G ?
PY(d, ?,G0), is a discrete distributionover a (possibly infinite) set of events, which we de-note abstractly E .
The process is parameterized by adiscount parameter 0 ?
d < 1, a strength parameter?
> ?d, and a base distribution G0 over the eventspace E .In this work, our focus is on the base distributionG0.
We place vague priors on the hyperparametersd ?
U([0, 1]) and (?
+ d) ?
Gamma(1, 1).
Infer-ence in PYPs is discussed below.2.2 Unigram Morphology ModelThe most basic expression of our model is a uni-gram model of text.
So far, we only assume thateach word can be analyzed into a stem and a se-quence of morphemes forming an inflection pattern.LetGs be a distribution over stems,Gp be a distribu-tion over inflectional patterns, and let GENERATE bea deterministic mapping from ?stem, pattern?
pairsto inflected word forms.1 An inflected word type isgenerated with the following process, which we des-ignate MP(Gs, Gd,GENERATE):stem ?
Gspattern ?
Gpword = GENERATE(stem, pattern)For example, in Russian, we might sample stem= ?????
?,2 pattern = STEM+Adj+Pl+Dat, andobtain word = ?????
?.This model could be used directly to generate ob-served tokens.
However, we have said nothing aboutGs and Gp, and the assumption that stems and pat-terns are independent is clearly unsatisfying.
Wetherefore assume that both the stem and the patterndistributions are generated from PY processes, andthat MP(Gs, Gp,GENERATE) is itself the base dis-tribution of a PYP.Gs ?
PY(ds, ?s, G0s)Gp ?
PY(dp, ?p, G0p)Gw ?
PY(d, ?,MP(Gs, Gp,GENERATE))A draw Gw from this PYP is a unigram distribu-tion over tokens.2.3 Base Stem Model G0sIn general there are an unbounded number of stemspossible in any language, so we set G0s to be charac-ter trigram model, which we statically estimate, withKneser-Ney smoothing, from a large corpus of wordtypes in the language being modeled.
While usingfixed parameters estimated to maximize likelihood is1The assumption of determinism is only inappropriate incases of inflectional spelling variants (e.g., modeled vs. mod-elled) or pronunciation variants (e.g., reduced forms in certainenvironments).2??????
(pronounced [pr5tCij]) = other1207questionable from the perspective of Bayesian learn-ing, it is tremendously beneficial for computationalreasons.
For some applications (e.g., word align-ment), the set of possible stems for a corpus S can beprecomputed, so we will also experiment with usinga uniform stem distribution based on this set.2.4 Base Pattern Model G0pSeveral choices are possible for the base pattern dis-tribution:MP0 We can assume a uniformG0p when the num-ber of patterns is small.MP1 To be able to generalize to new patterns, wecan draw the length of the pattern from a Poissondistribution and generate morphemes one by onefrom a uniform distribution.MP2 A more informative prior is a Markov chainof morphemes, where each morpheme is generatedconditional on the preceding morpheme.The choice of the base pattern distribution coulddepend on the complexity of the inflectional patternsproduced by the morphological analyzer, reflectingthe type of morphological phenomena present in agiven language.
For example, the number of possi-ble patterns can practically be considered finite inRussian, but this assumption is not valid for lan-guages with more extensive derivational morphol-ogy like Turkish.2.5 Posterior InferenceFor most applications, rather than directly gener-ating from a model using the processes outlinedabove, we seek to infer posterior distributions overlatent parameters and structures, given a sample ofdata.Although there is no known analytic form ofthe PYP density, it is possible to marginalize thedraws from it and to work directly with observa-tions.
This marginalization produces the classi-cal Chinese restaurant process representation (Teh,2006).
When working with the morphology mod-els we are proposing, we also need to marginalizethe different latent forms (stems s and patterns p)that may have given rise to a given word w. Thus,we require that the inverse relation of GENERATE isavailable to compute the marginal base word distri-bution:p(w | G0w) =?GENERATE(s,p)=wp(s | Gs) p(p | Gp)Since our approach encodes morphology usingFSTs, which are invertible, this poses no problem.To illustrate, consider the Russian word ?????
?,which may be analyzed in several ways:??????
+Adj +Sg +Neut +Instr??????
+Adj +Sg +Masc +Instr??????
+Adj +Pl +Dat???????
+Verb +Pl +1P??????
+Pro +Sg +InsBecause the set of possible analyses is in generalsmall, marginalization is fast and complex blockedsampling is not necessary.Finally, to infer hyperparameter values (d, ?, .
.
.
),a Metropolis-Hastings update is interleaved withGibbs sampling steps for the rest of the hidden vari-ables.3Having described a model for generating words,we now show its usage in several contexts.3 Unsupervised MorphologicalDisambiguationGiven a rule-based morphological analyzer encodedas an unweighted FST and a corpus on which theanalyzer has been run ?
possibly generating multi-ple analyses for each token ?
we can use our un-igram model to learn a probabilistic model of dis-ambiguation in an unsupervised setting (i.e., with-out annotated examples).
The corpus is assumed tobe generated from the unigram distribution Gw, andthe base stem model is set to a fixed character tri-gram model.4 After learning the parameters of themodel, we can find for each word in the vocabularyits most likely analysis and use this as a crude dis-ambiguation step.3The proposal distribution for Metropolis-Hastings is a Betadistribution (d) or a Gamma distribution (?+d) centered on theprevious parameter values.4Experiments suggest that this is important to constrain themodel to realistic stems.12083.1 Morphological GuessersFinite-state morphological analyzers are usuallyspecified in three parts: a stem lexicon, which de-fines the words in the language and classifies theminto several categories according to their grammat-ical function and their morphological properties; aset of prefixes and suffixes that can be applied toeach category to form surface words; and possiblyalternation rules that can encode exceptions andspelling variations.
The combination of these partsprovides a powerful framework for defining a gener-ative model of words.
Such models can be reversedto obtain an analyzer.
However, while the two latterparts can be relatively easy to specify, enumeratinga comprehensive stem lexicon is a time consumingand necessarily incomplete process, as some cate-gories are truly open-class.To allow unknown words to be analyzed, onecan use a guesser that attempts to analyze wordsmissing in the lexicon.
Can we eliminate the stemlexicon completely and use only the guesser?
Thisis what we try to do by designing a lexicon-freeanalyzer for Russian.
A guesser was developedin three hours; it is prone to over-generation andproduces ambiguous analyses for most wordsbut covers a large number of morphological phe-nomena (gender, case, tense, etc.).
For example,the word ?????
?5 can be correctly analyzed as????
?+Noun+Masc+Prep+Sg but also as the in-correct forms: ??????+Verb+Pres+2P+Pl,?????
?+Noun+Fem+Dat+Sg, ????-?
?+Noun+Fem+Prep+Sg, and more.3.2 Disambiguation ExperimentsWe train the unigram model on a 1.7M-word cor-pus of TED talks transcriptions translated into Rus-sian (Cettolo et al 2012) and evaluate our ana-lyzer against a test set consisting of 1,500 gold-standard analyses obtained from the morphologydisambiguation task of the DIALOG 2010 confer-ence (Lya?evskaya et al 2010).6Each analysis is composed of a lemma (?????
),a part of speech (Noun), and a sequence of ad-ditional functional morphemes (Masc,Prep,Sg).We consider only open-class categories: nouns, ad-5??????
= Hebrew (masculine noun, prepositional case)6http://ru-eval.rujectives, adverbs and verbs, and evaluate the outputof our model with three metrics: the lemma accu-racy, the part-of-speech accuracy, and the morphol-ogy F -measure.7As a baseline, we consider picking a random anal-ysis from output of the analyzer or choosing themost frequent lemma and the most frequent morpho-logical pattern.8 Then, we use our model with eachof the three versions of the pattern model describedin ?2.2.
Finally, as an upper bound, we use the goldstandard to select one of the analyses produced bythe guesser.Since our evaluation is not directly comparableto the standard for this task, we use for referencea high-quality analyzer from Xerox9 disambiguatedwith the MP0 model (all of the models have veryclose accuracy in this case).Model Lemma POS Morph.Random 29.8 70.9 50.2Frequency 31.1 74.4 48.8Guesser MP0 60.0 82.2 66.3Guesser MP1 58.9 82.5 69.5Guesser MP2 59.9 82.4 65.5Guesser oracle 68.4 84.9 83.0Xerox MP0 83.6 96.4 78.1Table 1: Russian morphological disambiguation.Considering the amount of effort put in develop-ing the guesser, the baseline POS tagging accuracyis relatively good.
However, the disambiguation islargely improved by using our unigram model withrespect to all the evaluation categories.
We are stillfar from the performance of a high-quality analyzerbut, in absence of such a resource, our techniquemight be a sensible option.
We also note that there isno clear winner in terms of pattern model, and con-clude that this choice is task-specific.7F -measure computed for the set of additional morphemesand averaged over the words in the corpus.8We estimate these frequencies by assuming each analysis ofeach token is uniformly likely, then summing fractional counts.9http://open.xerox.com/Services/fst-nlp-tools/Pages/morphology12094 Open Vocabulary Language ModelsWe now integrate our unigram model in a hierar-chical Pitman-Yor n-gram language model (Fig.
1).The training corpus words are assumed to begenerated from a distribution Gnw drawn fromPY(dn, ?n, Gn?1w ), where Gn?1w is defined recur-sively down to the base model G0w.
Previous workTeh (2006) simply used G0w = U(V ) where V isthe word vocabulary, but in our case G0w is the MPdefined in ?2.2.G2wG3w G1wd3, ?3 d2, ?2 d1, ?1Gsds, ?sGp G0pdp, ?pG0swFigure 1: The trigram version of our language model rep-resented as a graphical model.
G1w is the unigram modelof ?2.2.We are interested in evaluating our model in anopen vocabulary scenario where the ability to ex-plain new unseen words matters.
We expect ourmodel to be able to generalize better thanks to thecombination of a morphological analyzer and a stemdistribution which is less sparse than the word dis-tribution (for example, for the 1.6M word Turkishcorpus, |V | ?
3.5|S| ?
140k).To integrate out-of-vocabulary words in our eval-uation, we use infinite base distributions: G0w (in thebaseline model) or G0s (in the MP) are character tri-gram models.
We define perplexity of a held-out testcorpus in the standard way:ppl = exp(?1NN?i=1log p (wi | wi?n+1 ?
?
?wi?1))but compared to the common practice, we do notneed to discount OOVs from this sum since themodel vocabulary is infinite.
Note that we alsomarginalize by summing over all the possible analy-ses for a given word when computing its base prob-ability according to the MP.4.1 Language Modeling ExperimentsWe train several trigram models on the Russian TEDtalks corpus used in the previous section.
Our base-line is a hierarchical PY trigram model with a tri-gram character model as the base word distribution.We compare it with our model using the same char-acter model for the base stem distribution.
Both ofthe morphological analyzers described in the previ-ous section help obtaining perplexity reductions (Ta-ble 2).
We ran a similar experiment on the Turkishversion of this corpus (1.6M words) with a high-quality analyzer (Oflazer, 1994) and obtain evenlarger gains (Table 3).Model pplPY-character LM 563Guesser MP2 530Xerox MP2 522Table 2: Evaluation of the Russian n-gram model.Model pplPY-character LM 1,640Oflazer MP2 1,292Table 3: Evaluation of the Turkish n-gram model.These results can partly be attributed to the highOOV rate in these conditions: 4% for the Russiancorpus and 6% for the Turkish corpus.4.2 Predictive Text InputIt is difficult to know whether a decrease in perplex-ity, as measured in the previous section, will result ina performance improvement in downstream applica-tions.
As a confirmation that correctly modeling newwords matters, we consider a predictive task with atruly open vocabulary and that requires only a lan-guage model: predictive text input.Given some text, we encode it using a lossy de-terministic character mapping, and try to recover theoriginal content by computing the most likely wordsequence.
This task is inspired by predictive textinput systems available on cellphones with a 9-keykeypad.
For example, the string gave me a cupis encoded as 4283 63 2 287, which could alsobe decoded as: hate of a bus.1210Silfverberg et al(2012) describe a system de-signed for this task in Finnish, which is composedof a weighted finite-state morphological analyzertrained on IRC logs.
However, their system is re-stricted to words that are encoded in the analyzer?slexicon and does not use context for disambiguation.In our experiments, we use the same Turkish TEDtalks corpus as the previous section.
As a baseline,we use a trigram character language model.
We pro-duce a character lattice which encodes all the pos-sible interpretations for a word and compose it witha finite-state representation of the character LM us-ing OpenFST (Allauzen et al 2007).
Alternatively,we can use a unigram word model to decode this lat-tice, backing off to the character language model ifno solution is found.
Finally, to be able to make useof word context, we can extract the k most likelypaths according to the character LM and produce aword lattice, which is in turn decoded with a lan-guage model defined over the extracted vocabulary.Model WER CERCharacter LM 48.37 16.721-gram + character LM 8.50 3.281-gram MP2 6.46 2.373-gram + character LM 7.86 3.073-gram MP2 5.73 2.15Table 4: Evaluation of Turkish predictive text input.We measure word and character error rate (WER,CER) on the predicted word sequence and observelarge improvements in both of these metrics by mod-eling morphology, both at the unigram level andwhen context is used (Table 4).Preliminary experiments with a corpus of 1.6MTurkish tweets, an arguably more appropriate do-main this task, show smaller but consistent improv-ing: the trigram word error rate is reduced from 26%to 24% when our model is used.4.3 LimitationsWhile our model is an important step forward inpractical modeling of OOVs using morphologicalprocesses, we have made the linguistically naive as-sumption that morphology applies inside the lan-guage?s lexicon but has no effect on the process thatput inflected lexemes together into sentences.
In thisregard, our model is a minor variant on traditional n-gram models that work with ?opaque?
word forms.How to best relax this assumption in a computation-ally tractable way is an important open question leftfor future work.5 Word Alignment ModelMonolingual models of language are not the onlymodels that can benefit from taking into accountmorphology.
In fact, alignment models are a goodcandidate for using richer word distributions: theyassume a target word distribution conditioned oneach source word.
When the target language is mor-phologically rich, classic independence assumptionsproduce very weak models unless some kind of pre-processing is applied to one side of the corpus.
Analternative is to use our unigram model as a wordtranslation distribution for each source word in thecorpus.Our alignment model is based on a simple variantof IBM Model 2 where the alignment distribution isonly controlled by two parameters, ?
and p0 (Dyer etal., 2013).
p0 is the probability of the null alignment.For a source sentence f of length n, a target sentencee of lengthm and a latent alignment a, we define thefollowing alignment link probabilities (j 6= 0):p(ai = j | n,m) ?
(1?
p0) exp(?????
?im ?jn????)?
controls the flatness of this distribution: larger val-ues make the probabilities more peaked around thediagonal of the alignment matrix.Each target word is then generated given a sourceword and a latent alignment link from the wordtranslation distribution p(ei | fai , Gw).
Note thatthis is effectively a unigram distribution over tar-get words, albeit conditioned on the source wordfj .
Here is where our model differs from classicalignment models: the unigram distribution Gw isassumed be generated from a PY process.
There aretwo choices for the base word distribution:?
As a baseline, we use a uniform base distribu-tion over the target vocabulary: G0w = U(V ).?
We define a stem distribution Gs[f ] for eachsource word f , a shared pattern distributionGp,and set G0w[f ] = MP(Gs[f ], Gp).
In this case,1211we obtain the model depicted in Fig.
2.
Thestem and the pattern models are also given PYpriors with uniform base distribution (G0s =U(S)).Finally, we put uninformative priors on the align-ment distribution parameters: p0 ?
Beta(?, ?)
iscollapsed and ?
?
Gamma(k, ?)
is inferred usingMetropolis-Hastings.f eap0 Gwdw, ?wGpG0sGsG0pdp, ?p?, ds, ?sFigure 2: Our alignment model, represented as a graphi-cal model.ExperimentsWe evaluate the alignment error rate of our modelsfor two language pairs with rich morphology on thetarget side.
We compare to alignments inferred us-ing IBM Model 4 trained with EM (Brown et al1993),10 a version of our baseline model (describedabove) without PY priors (learned using EM), andthe PY-based baseline.
We consider two languagepairs.English-Turkish We use a 2.8M word cleanedversion of the South-East European Times corpus(Tyers and Alperen, 2010) and gold-standard align-ments from ?akmak et al(2012).
Our morphologi-cal analyzer is identical to the one used in the previ-ous sections.English-Czech We use the 1.3M word NewsCommentary corpus and gold-standard alignments10We use the default GIZA++ stage training scheme:Model 1 + HMM + Model 3 + Model 4.from Bojar and Prokopov?
(2006).
The morpholog-ical analyzer is provided by Xerox.Results Results are shown in Table 5.
Our lightlyparameterized model performs much better thanIBM Model 4 in these small-data conditions.
Withan identical model, we find PY priors outperformtraditional multinomial distributions.
Adding mor-phology further reduced the alignment error rate, forboth languages.AERModel en-tr en-csModel 4 52.1 34.5EM 43.8 28.9PY-U(V ) 39.2 25.7PY-U(S) 33.8 24.8Table 5: Word alignment experiments on English-Turkish(en-tr) and English-Czech (en-cs) data.As an example of how our model generalizes bet-ter, consider the sentence pair in Fig.
3, taken fromthe evaluation data.
The two words composing theTurkish sentence are not found elsewhere in the cor-pus, but several related inflections occur.11 It istherefore trivial for the stem-base model to find thecorrect alignment (marked in black), while all theother models have no evidence for it and choose anarbitrary alignment (gray points).I wasnotabletofinishmyhomework?devimibitiremedimFigure 3: A complex Turkish-English word alignment(alignment points in gray: EM/PY-U(V ); black: PY-U(S)).6 Related WorkComputational morphology has received consider-able attention in NLP since the early work on two-level morphology (Koskenniemi, 1984; Kaplan and11?devinin, ?devini, ?devleri; bitmez, bitirileceg?inden,bitmesiyle, ...1212Kay, 1994).
It is now widely accepted that finite-state transducers have sufficient power to expressnearly all morphological phenomena, and the XFSTtoolkit (Beesley and Karttunen, 2003) has con-tributed to the practical adoption of this modelingapproach.
Recently, open-source tools have been re-leased: in this paper, we used Foma (Hulden, 2009)to develop the Russian guesser.Since some inflected forms have several possibleanalyses, there has been a great deal of work on se-lecting the intended one in context (Hakkani-T?r etal., 2000; Hajic?
et al 2001; Habash and Rambow,2005; Smith et al 2005; Habash et al 2009).
Ourdisambiguation model is closely related to genera-tive models used for this purpose (Hakkani-T?r etal., 2000).Rule-based analysis is not the only approachto modeling morphology, and many unsupervisedmodels have been proposed.12 Heuristic segmenta-tion approaches based on the minimum descriptionlength principle (Goldsmith, 2001; Creutz and La-gus, 2007; de Marcken, 1996; Brent et al 1995)have been shown to be effective, and Bayesianmodel-based versions have been proposed as well(Goldwater et al 2011; Snyder and Barzilay, 2008;Snover and Brent, 2001).
In ?3, we suggested a thirdway between rule-based approaches and fully un-supervised learning that combines the best of bothworlds.Morphological analysis or segmentation is crucialto the performance of several applications: machinetranslation (Goldwater and McClosky, 2005; Al-Haj and Lavie, 2010; Oflazer and El-Kahlout, 2007;Minkov et al 2007; Habash and Sadat, 2006, in-ter alia), automatic speech recognition (Creutz et al2007), and syntactic parsing (Tsarfaty et al 2010).Several methods have been proposed to integratemorphology into n-gram language models, includ-ing factored language models (Bilmes and Kirch-hoff, 2003), discriminative language modeling (Ar?-soy et al 2008), and more heuristic approaches(Monz, 2011).Despite the fundamentally open nature of the lex-icon (Heaps, 1978), there has been distressingly lit-12Developing a high-coverage analyzer can be a time-consuming process even with the simplicity of modern toolkits,and unsupervised morphology learning is an attractive problemfor computational cognitive science.tle attention to the general problem of open vocabu-lary language modeling problem (most applicationsmake a closed-vocabulary assumption).
The classicexploration of open vocabulary language modelingis Brown et al(1992), which proposed the strategyof interpolating between word- and character-basedmodels.
Character-based language models are re-viewed by Carpenter (2005).
So-called hybrid mod-els that model both words and sublexical units havebecome popular in speech recognition (Shaik et al2012; Parada et al 2011; Bazzi, 2002).
Open-vocabulary language language modeling has also re-cently been explored in the context of assistive tech-nologies (Roark, 2009).Finally, Pitman-Yor processes (PYPs) have be-come widespread in natural language processingsince they are natural power-law generators.
It hasbeen shown that the widely used modified Kneser-Ney estimator (Chen and Goodman, 1998) for n-gram language models is an approximation of theposterior predictive distribution of a language modelwith hierarchical PYP priors (Goldwater et al 2011;Teh, 2006).7 ConclusionWe described a generative model which makes useof morphological analyzers to produce richer worddistributions through sharing of statistical strengthbetween stems.
We have shown how it can be in-tegrated into several models central to NLP appli-cations and have empirically validated the effective-ness of these changes.
Although this paper mostlyfocused on languages that are well studied and forwhich high-quality analyzers are available, our mod-els are especially relevant in low-resource scenariosbecause they do not require disambiguated analyses.In future work, we plan to apply these techniques tolanguages such as Kinyarwanda, a resource-poor butmorphologically rich language spoken in Rwanda.It is our belief that knowledge-rich models can helpbridge the gap between low- and high-resource lan-guages.AcknowledgmentsWe thank Kemal Oflazer for making his Turkish lan-guage morphological analyzer available to us and Bren-dan O?Connor for gathering the Turkish tweets used in1213the predictive text experiments.
This work was spon-sored by the U. S. Army Research Laboratory and theU.
S. Army Research Office under contract/grant numberW911NF-10-1-0533.ReferencesH.
Al-Haj and A. Lavie.
2010.
The impactof Arabic morphological segmentation on broad-coverage English-to-Arabic statistical machine trans-lation.
Proc.
of AMTA.Cyril Allauzen, Michael Riley, Johan Schalkwyk, Woj-ciech Skut, and Mehryar Mohri.
2007.
OpenFst: Ageneral and efficient weighted finite-state transducerlibrary.
In Implementation and Application of Au-tomata, pages 11?23.Ebru Ar?soy, Brian Roark, Izhak Shafran, and MuratSara?lar.
2008.
Discriminative n-gram language mod-eling for Turkish.
In Proc.
of Interspeech.Issam Bazzi.
2002.
Modelling out-of-vocabulary wordsfor robust speech recognition.
Ph.D. thesis, MIT.K.R.
Beesley and L. Karttunen.
2003.
Finite-state mor-phology: Xerox tools and techniques.
CSLI, Stanford.Jeff A. Bilmes and Katrin Kirchhoff.
2003.
Factoredlanguage models and generalized parallel backoff.
InProc.
of NAACL.Ondr?ej Bojar and Magdalena Prokopov?.
2006.
Czech-English word alignment.
In Proc.
of LREC.Michael R. Brent, Sreerama K. Murthy, and AndrewLundberg.
1995.
Discovering morphemic suffixes: Acase study in MDL induction.
In Proceedings of theFifth International Workshop on Artificial Intelligenceand Statistics.Peter F. Brown, Vincent J. Della Pietra, Stephen A.Della Pietra, Robert L. Mercer, and Jennifer C. Lai.1992.
An estimate of an upper bound for the entropyof English.
Computational Linguistics, 18(1):31?40.P.
F. Brown, V. J. D. Pietra, S. A. D. Pietra, and R. L.Mercer.
1993.
The mathematics of statistical machinetranslation: Parameter estimation.
Computational Lin-guistics, 19(2):263?311.Bob Carpenter.
2005.
Scaling high-order character lan-guage models to gigabytes.
In Proceedings of the ACLWorkshop on Software.Mauro Cettolo, Christian Girardi, and Marcello Federico.2012.
WIT3: Web inventory of transcribed and trans-lated talks.
In Proc.
of EAMT.Stanley F. Chen and Joshua Goodman.
1998.
An empiri-cal study of smoothing techniques for language model-ing.
Technical Report TR-10-98, Harvard University.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing, 4(1).M.
Creutz, T. Hirsim?ki, M. Kurimo, A. Puurula,J.
Pylkk?nen, V. Siivola, M. Varjokallio, E. Arisoy,M.
Sara?lar, and A. Stolcke.
2007.
Morph-basedspeech recognition and modeling of out-of-vocabularywords across languages.
ACM Transactions on Speechand Language Processing, 5(1):3.Carl G. de Marcken.
1996.
Unsupervised Language Ac-quisition.
Ph.D. thesis, MIT.Chris Dyer, Victor Chahuneau, and Noah A. Smith.2013.
A simple, fast, and effective reparameterizationof IBM Model 2.
In Proc.
of NAACL.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics, 27(2):153?198.S.
Goldwater and D. McClosky.
2005.
Improving statis-tical MT through morphological analysis.
In Proc.
ofEMNLP.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2011.
Producing power-law distributions anddamping word frequencies with two-stage languagemodels.
Journal of Machine Learning Research,12:2335?2382.Nizar Habash and Owen Rambow.
2005.
Arabic tok-enization, part-of-speech tagging, and morphologicaldisambiguation in one fell swoop.
In Proc.
of ACL.Nizar Habash and Fatiha Sadat.
2006.
Arabic prepro-cessing schemes for statistical machine translation.
InProc.
of NAACL.Nizar Habash, Owen Rambow, and Ryan Roth.
2009.MADA+TOKAN: A toolkit for Arabic tokenization,diacritization, morphological disambiguation, POStagging, stemming and lemmatization.
In Proceedingsof the Second International Conference on Arabic Lan-guage Resources and Tools.Jan Hajic?, P. Krbec, P.
Kve?ton?, K. Oliva, and V. Petrovic?.2001.
Serial combination of rules and statistics.
InProc.
of ACL.D.
Z. Hakkani-T?r, Kemal Oflazer, and G. T?r.
2000.Statistical morphological disambiguation for aggluti-native languages.
In Proc.
of COLING.Harold Stanley Heaps.
1978.
Information Retrieval:Computational and Theoretical Aspects.
AcademicPress.M.
Hulden.
2009.
Foma: a finite-state compiler and li-brary.
In Proc.
of EACL.Ronald M. Kaplan and Martin Kay.
1994.
Regular mod-els of phonological rule systems.
Computational Lin-guistics, 20(3):331?378.Lauri Karttunen and Kenneth R. Beesley.
2005.
Twenty-five years of finite-state morphology.
In Inquiries intoWords, Constraints and Contexts, pages 71?83.
CSLI.Kimmo Koskenniemi.
1984.
A general computationalmodel for word-form recognition and production.
InProc.
of ACL-COLING.1214O.
Lya?evskaya, I. Astaf?yeva, A. Bonch-Osmolovskaya,A.
Garej?ina, Y. Gri?ina, V. D?yac?kov, M. Ionov,A.
Koroleva, M. Kudrinskij, A. Lityagina, Y. Luc?ina,Y.
Sidorova, S. Toldova, S. Savc?uk, and S. Ko-val?.
2010.
Ocenka metodov avtomatic?eskogoanaliza teksta: morfologic?eskie parseri russkogoyazyka.
Komp?juternaya lingvistika i intellektual?nyetexnologii (Computational linguistics and intellectualtechnologies).Einat Minkov, Kristina Toutanova, and Hisami Suzuki.2007.
Generating complex morphology for machinetranslation.
In Proc.
of ACL.Christof Monz.
2011.
Statistical machine translationwith local language models.
In Proc.
of EMNLP.Kemal Oflazer and I?lknur Durgar El-Kahlout.
2007.
Ex-ploring different representational units in English-to-Turkish statistical machine translation.
In Proc.
ofStatMT.K.
Oflazer.
1994.
Two-level description of Turk-ish morphology.
Literary and Linguistic Computing,9(2):137?148.Carolina Parada, Mark Dredze, Abhinav Sethy, and AriyaRastrow.
2011.
Learning sub-word units for open vo-cabulary speech recognition.
In Proc.
of ACL.Jim Pitman and Marc Yor.
1997.
The two-parameterPoisson-Dirichlet distribution derived from a stablesubordinator.
Annals of Probability, 25(2):855?90.Brian Roark.
2009.
Open vocabulary language model-ing for binary response typing interfaces.
TechnicalReport CSLU-09-001, Oregon Health & Science Uni-versity.M.
Ali Basha Shaik, David Rybach, Stefan Hahn, RalfSchlu?ter, and Hermann Ney.
2012.
Hierarchical hy-brid language models for open vocabulary continuousspeech recognition using wfst.
In Proc.
of SAPA.M.
Silfverberg, K. Lind?n, and M. Hyv?rinen.
2012.Predictive text entry for agglutinative languages usingunsupervised morphological segmentation.
In Proc.of Computational Linguistics and Intelligent Text Pro-cessing.Noah A. Smith, David A. Smith, and Roy W. Tromble.2005.
Context-based morphological disambiguationwith random fields.
In Proc.
of EMNLP.Matt G. Snover and Michael R. Brent.
2001.
A Bayesianmodel for morpheme and paradigm identification.
InProc.
of ACL.Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
In Proc.
of ACL.Yee Whye Teh.
2006.
A hierarchical Bayesian languagemodel based on Pitman-Yor processes.
In Proc.
ofACL.Reut Tsarfaty, Djam?
Seddah, Yoav Goldberg, SandraK?bler, Marie Candito, Jennifer Foster, Yannick Vers-ley, Ines Rehbein, and Lamia Tounsi.
2010.
Statisticalparsing of morphologically rich languages: What, howand whither.
In Proc.
of Workshop on Statistical Pars-ing of Morphologically Rich Languages.F.
Tyers and M.S.
Alperen.
2010.
South-east europeantimes: A parallel corpus of Balkan languages.
InProceedings of the LREC workshop on Exploitationof multilingual resources and tools for Central and(South) Eastern European Languages.M.
Talha ?akmak, S?leyman Acar, and G?ls?en Eryig?it.2012.
Word alignment for English-Turkish languagepair.
In Proc.
of LREC.1215
