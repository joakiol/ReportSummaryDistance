Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 335?344,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsAccurate Context-Free Parsing with Combinatory Categorial GrammarTimothy A. D. Fowler and Gerald PennDepartment of Computer Science, University of TorontoToronto, ON, M5S 3G4, Canada{tfowler, gpenn}@cs.toronto.eduAbstractThe definition of combinatory categorialgrammar (CCG) in the literature variesquite a bit from author to author.
How-ever, the differences between the defini-tions are important in terms of the lan-guage classes of each CCG.
We provethat a wide range of CCGs are stronglycontext-free, including the CCG of CCG-bank and of the parser of Clark and Cur-ran (2007).
In light of these new results,we train the PCFG parser of Petrov andKlein (2007) on CCGbank and achievestate of the art results in supertagging ac-curacy, PARSEVAL measures and depen-dency accuracy.1 IntroductionCombinatory categorial grammar (CCG) is a vari-ant of categorial grammar which has attracted in-terest for both theoretical and practical reasons.On the theoretical side, we know that it is mildlycontext-sensitive (Vijay-Shanker and Weir, 1994)and that it can elegantly analyze a wide range oflinguistic phenomena (Steedman, 2000).
On thepractical side, we have corpora with CCG deriva-tions for each sentence (Hockenmaier and Steed-man, 2007), a wide-coverage parser trained on thatcorpus (Clark and Curran, 2007) and a system forconverting CCG derivations into semantic repre-sentations (Bos et al, 2004).However, despite being treated as a single uni-fied grammar formalism, each of these authors usevariations of CCG which differ primarily on whichcombinators are included in the grammar and therestrictions that are put on them.
These differencesare important because they affect whether themild context-sensitivity proof of Vijay-Shankerand Weir (1994) applies.
We will provide a gen-eralized framework for CCG within which the fullvariation of CCG seen in the literature can be de-fined.
Then, we prove that for a wide range ofCCGs there is a context-free grammar (CFG) thathas exactly the same derivations.
Included in thisclass of strongly context-free CCGs are a grammarincluding all the derivations in CCGbank and thegrammar used in the Clark and Curran parser.Due to this insight, we investigate the potentialof using tools from the probabilistic CFG com-munity to improve CCG parsing results.
ThePetrov parser (Petrov and Klein, 2007) uses la-tent variables to refine the grammar extracted froma corpus to improve accuracy, originally usedto improve parsing results on the Penn treebank(PTB).
We train the Petrov parser on CCGbankand achieve the best results to date on sentencesfrom section 23 in terms of supertagging accuracy,PARSEVAL measures and dependency accuracy.These results should not be interpreted as proofthat grammars extracted from the Penn treebankand from CCGbank are equivalent.
Bos?s systemfor building semantic representations from CCGderivations is only possible due to the categorialnature of CCG.
Furthermore, the long distance de-pendencies involved in extraction and coordina-tion phenomena have a more natural representa-tion in CCG.2 The Language Classes of CombinatoryCategorial GrammarsA categorial grammar is a grammatical systemconsisting of a finite set of words, a set of cate-gories, a finite set of sentential categories, a finitelexicon mapping words to categories and a rulesystem dictating how the categories can be com-bined.
The set of categories are constructed from afinite set of atoms A (e.g.
A = {S,NP,N,PP})and a finite set of binary connectives B (e.g.B = {/, \}) to build an infinite set of categoriesC(A,B) (e.g.
C(A,B) = {S, S\NP, (S\NP )/NP, .
.
.}).
For a category C , its size |C| is the335number of atom occurrences it contains.
When notspecified, connectives are left associative.According to the literature, combinatory cate-gorial grammar has been defined to have a vari-ety of rule systems.
These rule systems vary froma small rule set, motivated theoretically (Vijay-Shanker and Weir, 1994), to a larger rule set,motivated linguistically, (Steedman, 2000) to avery large rule set, motivated by practical cover-age (Hockenmaier and Steedman, 2007; Clark andCurran, 2007).
We provide a definition generalenough to incorporate these four main variants ofCCG, as well as others.A combinatory categorial grammar (CCG) is acategorial grammar whose rule system consists ofrule schemata where the left side is a sequence ofcategories and the right side is a single categorywhere the categories may include variables overboth categories and connectives.
In addition, ruleschemata may specify a sequence of categoriesand connectives using the .
.
.
convention1 .
When.
.
.
appears in a rule, it matches any sequence ofcategories and connectives according to the con-nectives adjacent to the .
.
.. For example, the ruleschema for forward composition is:X/Y, Y/Z ?
X/Zand the rule schema for generalized forwardcrossed composition is:X/Y, Y |1Z1|2 .
.
.
|nZn ?
X|1Z1|2 .
.
.
|nZnwhere X, Y and Zi for 1 ?
i ?
n are variablesover categories and |i for 1 ?
i ?
n are variablesover connectives.
Figure 1 shows a CCG deriva-tion from CCGbank.A well-known categorial grammar which is nota CCG is Lambek categorial grammar (Lambek,1958) whose introduction rules cannot be charac-terized as combinatory rules (Zielonka, 1981).2.1 Classes for defining CCGWe define a number of schema classes generalenough that the important variants of CCG can bedefined by selecting some subset of the classes.
Inaddition to the schema classes, we also define tworestriction classes which define ways in which therule schemata from the schema classes can be re-stricted.
We define the following schema classes:1The .
.
.
convention (Vijay-Shanker and Weir, 1994) isessentially identical to the $ convention of Steedman (2000).
(1) Application?
X/Y, Y ?
X?
Y,X\Y ?
X(2) Composition?
X/Y, Y/Z ?
X/Z?
Y \Z,X\Y ?
X\Z(3) Crossed Composition?
X/Y, Y \Z ?
X\Z?
Y/Z,X\Y ?
X/Z(4) Generalized Composition?
X/Y, Y/Z1/ .
.
.
/Zn ?
X/Z1/ .
.
.
/Zn?
Y \Z1\ .
.
.
\Zn,X\Y ?
X\Z1\ .
.
.
\Zn(5) Generalized Crossed Composition?
X/Y, Y |1Z1|2 .
.
.
|nZn?
X|1Z1|2 .
.
.
|nZn?
Y |1Z1|2 .
.
.
|nZn,X\Y?
X|1Z1|2 .
.
.
|nZn(6) Reducing Generalized Crossed CompositionGeneralized Composition or GeneralizedCrossed Composition where |X| ?
|Y |.
(7) Substitution?
(X/Y )|1Z, Y |1Z ?
X|1Z?
Y |1Z, (X\Y )|1Z ?
X|1Z(8) D Combinator2?
X/(Y |1Z), Y |2W ?
X|2(W |1Z)?
Y |2W,X\(Y |1Z) ?
X|2(W |1Z)(9) Type-Raising?
X ?
T/(T\X)?
X ?
T\(T/X)(10) Finitely Restricted Type-Raising?
X ?
T/(T\X) where ?X,T ?
?
S for fi-nite S?
X ?
T\(T/X) where ?X,T ?
?
S for fi-nite S(11) Finite Unrestricted Variable-Free Rules?
~X ?
Y where ?
~X, Y ?
?
S for finite S2Hoyt and Baldridge (2008) argue for the inclusion of theD Combinator in CCG.336Mr.
Vinken is chairman of Elsevier N.V. , the Dutch publishing group .N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .NNNPNP [conj]NNPNPNP\NPNPNPS[dcl]\NPNNPS[dcl]S[dcl]Figure 1: A CCG derivation from section 00 of CCGbank.We define the following restriction classes:(A) Rule Restriction to a Finite SetThe rule schemata in the schema classes of aCCG are limited to a finite number of instan-tiations.
(B) Rule Restrictions to Certain Categories 3The rule schemata in the schema classes of aCCG are limited to a finite number of instan-tiations although variables are allowed in theinstantiations.Vijay-Shanker and Weir (1994) define CCG tobe schema class (4) with restriction class (B).Steedman (2000) defines CCG to be schemaclasses (1-5), (6), (10) with restriction class (B).2.2 Strongly Context-Free CCGsProposition 1.
The set of atoms in any derivationof any CCG consisting of a subset of the schemaclasses (1-8) and (10-11) is finite.Proof.
A finite lexicon can introduce only a finitenumber of atoms in lexical categories.Any rule corresponding to a schema in theschema classes (1-8) has only those atoms on theright that occur somewhere on the left.
Rules inclasses (10-11) can each introduce a finite numberof atoms, but there can be only a finite number of3Baldridge (2002) introduced a variant of CCG wheremodalities are added to the connectives / and \ along withvariants of the combinatory rules based on these modalities.Our proofs about restriction class (B) are essentially identicalto proofs regarding the multi-modal variant.such rules, limiting the new atoms to a finite num-ber.Definition 1.
The subcategories for a category care c1 and c2 if c = c1 ?
c2 for ?
?
B and c if c isatomic.
Its second subcategories are the subcate-gories of its subcategories.Proposition 2.
Any CCG consisting of a subsetof the rule schemata (1-3), (6-8) and (10-11) hasderivations consisting of only a finite number ofcategories.Proof.
We first prove the proposition excludingschema class (8).
We will use structural inductionon the derivations to prove that there is a bound onthe size of the subcategories of any category in thederivation.
The base case is the assignment of alexical category to a word and the inductive step isthe use of a rule from schema classes (1-4), (6-7)and (10-11).Given that the lexicon is finite, there is a boundk on the size of the subcategories of lexical cate-gories.
Furthermore, there is a bound l on the sizeof the subcategories of categories on the right sideof any rule in (10) and (11).
Let m = max(k, l).For rules from schema class (1), the categoryon the right is a subcategory of the first categoryon the left, so the subcategories on the right arebound by m. For rules from schema classes (2-3),the category on the right has subcategories X andZ each of which is bound in size by m since theyoccur as subcategories of categories on the left.For rules from schema class (6), since reduc-ing generalized composition is a special case of re-337ducing generalized crossing composition, we needonly consider the latter.
The category on the righthas subcategories X|1Z1|2 .
.
.
|n?1|Zn?1 and Zn.Zn is bound in size by m because it occurs asa subcategory of the second category on the left.Then, the size of Y |1Z1|2 .
.
.
|n?1|Zn?1 must bebound by m and since |X| ?
|Y |, the size ofX|1Z1|2 .
.
.
|n?1|Zn?1 must also be bound by m.For rules from schema class (7), the category onthe right has subcategories X and Z .
The size ofZ is bound by m because it is a subcategory of acategory on the left.
The size of X is bound bym because it is a second subcategory of a categoryon the left.Finally, the use of rules in schema classes (10-11) have categories on the right that are boundedby l, which is, in turn, bounded by m. Then, byproposition 1, there must only be a finite numberof categories in any derivation in a CCG consistingof a subset of rule schemata (1-3), (6-7) and (10-11).The proof including schema class (8) is essen-tially identical except that k must be defined interms of the size of the second subcategories.Definition 2.
A grammar is strongly context-freeif there exists a CFG such that the derivations ofthe two grammars are identical.Proposition 3.
Any CCG consisting of a subsetof the schema classes (1-3), (6-8) and (10-11) isstrongly context-free.Proof.
Since the CCG generates derivationswhose categories are finite in number let C be thatset of categories.
Let S(C,X) be the subset of Cmatching category X (which may have variables).Then, for each rule schema C1, C2 ?
C3 in (1-3)and (6-8), we construct a context-free rule C ?3 ?C ?1, C ?2 for each C ?i in S(C,Ci) for 1 ?
i ?
3.Similarly, for each rule schema C1 ?
C2 in (10),we construct a context-free rule C ?2 ?
C ?1 whichresults in a finite number of such rules.
Finally, foreach rule schema ~X ?
Z in (11) we construct acontext-free rule Z ?
~X.
Then, for each entry inthe lexicon w ?
C , we construct a context-freerule C ?
w.The constructed CFG has precisely the samerules as the CCG restricted to the categories in Cexcept that the left and right sides have been re-versed.
Thus, by proposition 2, the CFG has ex-actly the same derivations as the CCG.Proposition 4.
Any CCG consisting of a subset ofthe schema classes (1-3), (6-8) and (10-11) alongwith restriction class (B) is strongly context-free.Proof.
If a CCG is allowed to restrict the use ofits rules to certain categories as in schema class(B), then when we construct the context-free rulesby enumerating only those categories in the set Callowed by the restriction.Proposition 5.
Any CCG that includes restrictionclass (A) is strongly context-free.Proof.
We construct a context-free grammar withexactly those rules in the finite set of instantiationsof the CCG rule schemata along with context-free rules corresponding to the lexicon.
ThisCFG generates exactly the same derivations as theCCG.We have thus proved that of a wide range of therule schemata used to define CCGs are context-free.2.3 Combinatory Categorial Grammars inPracticeCCGbank (Hockenmaier and Steedman, 2007)is a corpus of CCG derivations that was semi-automatically converted from the Wall Street Jour-nal section of the Penn treebank.
Figure 2 showsa categorization of the rules used in CCGbank ac-cording to the schema classes defined in the pre-ceding section where a rule is placed into the leastgeneral class to which it belongs.
In addition tohaving no generalized composition other than thereducing variant, it should also be noted that in allgeneralized composition rules, X = Y implyingthat the reducing class of generalized compositionis a very natural schema class for CCGbank.If we assume that type-raising is restricted tothose instances occurring in CCGbank4, then aCCG consisting of schema classes (1-3), (6-7) and(10-11) can generate all the derivations in CCG-bank.
By proposition 3, such a CCG is stronglycontext-free.
One could also observe that sinceCCGbank is finite, its grammar is not only acontext-free grammar but can produce only a finitenumber of derivations.
However, our statement ismuch stronger because this CCG can generate allof the derivations in CCGbank given only the lex-icon, the finite set of unrestricted rules and the fi-nite number of type-raising rules.4Without such an assumption, parsing is intractable.338Schema Class Rules InstancesApplication 519 902176Composition 102 7189Crossed Composition 64 14114Reducing Generalized 50 612Crossed CompositionGeneralized Composition 0 0Generalized Crossed 0 0CompositionSubstitution 3 4Type-Raising 27 3996Unrestricted Rules 642 335011Total 1407 1263102Figure 2: The rules of CCGbank by schema class.The Clark and Curran CCG Parser (Clark andCurran, 2007) is a CCG parser which uses CCG-bank as a training corpus.
Despite the fact thatthere is a strongly context-free CCG which gener-ates all of the derivations in CCGbank, it is stillpossible that the grammar learned by the Clarkand Curran parser is not a context-free grammar.However, in addition to rule schemata (1-6) and(10-11) they also include restriction class (A) byrestricting rules to only those found in the train-ing data5.
Thus, by proposition 5, the Clark andCurran parser is a context-free parser.3 A Latent Variable CCG ParserThe context-freeness of a number of CCGs shouldnot be considered evidence that there is no ad-vantage to CCG as a grammar formalism.
Unlikethe context-free grammars extracted from the Penntreebank, these allow for the categorial semanticsthat accompanies any categorial parse and for amore elegant analysis of linguistic structures suchas extraction and coordination.
However, becausewe now know that the CCG defined by CCGbankis strongly context-free, we can use tools from theCFG parsing community to improve CCG parsing.To illustrate this point, we train the Petrovparser (Petrov and Klein, 2007) on CCGbank.The Petrov parser uses latent variables to refinea coarse-grained grammar extracted from a train-ing corpus to a grammar which makes much morefine-grained syntactic distinctions.
For example,5The Clark and Curran parser has an option, which is dis-abled by default, for not restricting the rules to those that ap-pear in the training data.
However, they find that this restric-tion is ?detrimental to neither parser accuracy or coverage?
(Clark and Curran, 2007).in Petrov?s experiments on the Penn treebank, thesyntactic category NP was refined to the morefine-grained NP 1 and NP 2 roughly correspond-ing to NP s in subject and object positions.
Ratherthan requiring such distinctions to be made in thecorpus, the Petrov parser hypothesizes these splitsautomatically.The Petrov parser operates by performing afixed number of iterations of splitting, mergingand smoothing.
The splitting process is doneby performing Expectation-Maximization to de-termine a likely potential split for each syntacticcategory.
Then, during the merging process someof the splits are undone to reduce grammar sizeand avoid overfitting according to the likelihoodof the split against the training data.The Petrov parser was chosen for our experi-ments because it refines the grammar in a mathe-matically principled way without altering the na-ture of the derivations that are output.
This isimportant because the input to the semantic back-end and the system that converts CCG derivationsto dependencies requires CCG derivations as theyappear in CCGbank.3.1 ExperimentsOur experiments use CCGbank as the corpus andwe use sections 02-21 for training (39603 sen-tences), 00 for development (1913 sentences) and23 for testing (2407 sentences).CCGbank, in addition to the basic atoms S, N ,NP and PP , also differentiates both the S andNP atoms with features allowing more subtle dis-tinctions.
For example, declarative sentences areS[dcl], wh-questions are S[wq] and sentence frag-ments are S[frg] (Hockenmaier and Steedman,2007).
These features allow finer control of the useof combinatory rules in the resulting grammars.However, this fine-grained control is exactly whatthe Petrov parser does automatically.
Therefore,we trained the Petrov parser twice, once on theoriginal version of CCGbank (denoted ?Petrov?
)and once on a version of CCGbank without thesefeatures (denoted ?Petrov no feats?).
Furthermore,we will evaluate the parsers obtained after 0, 4, 5and 6 training iterations (denoted I-0, I-4, I-5 andI-6).
When we evaluate on sets of sentences forwhich not all parsers return an analysis, we reportthe coverage (denoted ?Cover?
).We use the evalb package for PARSEVALevaluation and a modified version of Clark and339Parser Accuracy % No feats %C&C Normal Form 92.92 93.38C&C Hybrid 93.06 93.52Petrov I-5 93.18 93.73Petrov no feats I-6 - 93.74Figure 3: Supertagging accuracy on the sentencesin section 00 that receive derivations from the fourparsers shown.Parser Accuracy % No feats %C&C Hybrid 92.98 93.43Petrov I-5 93.10 93.59Petrov no feats I-6 - 93.62Figure 4: Supertagging accuracy on the sentencesin section 23 that receive derivations from thethree parsers shown.Curran?s evaluate script for dependency eval-uation.
To determine statistical significance, weobtain p-values from Bikel?s randomized parsingevaluation comparator6, modified for use with tag-ging accuracy, F-score and dependency accuracy.3.2 Supertag EvaluationBefore evaluating the parse trees as a whole, weevaluate the categories assigned to words.
In thesupertagging literature, POS tagging and supertag-ging are distinguished ?
POS tags are the tradi-tional Penn treebank tags (e.g.
NN, VBZ and DT)and supertags are CCG categories.
However, be-cause the Petrov parser trained on CCGbank hasno notion of Penn treebank POS tags, we can onlyevaluate the accuracy of the supertags.The results are shown in figures 3 and 4 wherethe ?Accuracy?
column shows accuracy of the su-pertags against the CCGbank categories and the?No feats?
column shows accuracy when featuresare ignored.
Despite the lack of POS tags in thePetrov parser, we can see that it performs slightlybetter than the Clark and Curran parser.
The dif-ference in accuracy is only statistically significantbetween Clark and Curran?s Normal Form modelignoring features and the Petrov parser trained onCCGbank without features (p-value = 0.013).3.3 Constituent EvaluationIn this section we evaluate the parsers using thetraditional PARSEVAL measures which measurerecall, precision and F-score on constituents in6http://www.cis.upenn.edu/ dbikel/software.htmlboth labeled and unlabeled versions.
In addition,we report a variant of the labeled PARSEVALmeasures where we ignore the features on the cat-egories.
For reasons of brevity, we report the PAR-SEVAL measures for all sentences in sections 00and 23, rather than for sentences of length is lessthan 40 or less than 100.
The results are essentiallyidentical for those two sets of sentences.Figure 5 gives the PARSEVAL measures on sec-tion 00 for Clark and Curran?s two best modelsand the Petrov parser trained on the original CCG-bank and the version without features after variousnumbers of training iterations.
Figure 7 gives theaccuracies on section 23.In the case of Clark and Curran?s hybrid model,the poor accuracy relative to the Petrov parsers canbe attributed to the fact that this model choosesderivations based on the associated dependenciesat the expense of constituent accuracy (see section3.4).
In the case of Clark and Curran?s normalform model, the large difference between labeledand unlabeled accuracy is primarily due to the mis-labeling of a small number of features (specifi-cally, NP[nb] and NP[num]).
The labeled accu-racies without features gives the results when fea-tures are disregarded.Due to the similarity of the accuracies and thedifference in the coverage between I-5 of thePetrov parser on CCGbank and I-6 of the Petrovparser on CCGbank without features, we reevalu-ate their results on only those sentences for whichthey both return derivations in figures 6 and 8.These results show that the features in CCGbankactually inhibit accuracy (to a statistically signifi-cant degree in the case of unlabeled accuracy onsection 00) when used as training data for thePetrov parser.Figure 9 gives a comparison between the Petrovparser trained on the Penn treebank and on CCG-bank.
These numbers should not be directly com-pared, but the similarity of the unlabeled measuresindicates that the difference between the structureof the Penn treebank and CCGbank is not large.73.4 Dependency EvaluationThe constituent-based PARSEVAL measures aresimple to calculate from the output of the Petrovparser but the relationship of the PARSEVAL7Because punctuation in CCG can have grammaticalfunction, we include it in our accuracy calculations result-ing in lower scores for the Petrov parser trained on the Penntreebank than those reported in Petrov and Klein (2007).340Labeled % Labeled no feats % Unlabeled %Parser R P F R P F R P F CoverC&C Normal Form 71.14 70.76 70.95 80.66 80.24 80.45 86.16 85.71 85.94 98.95C&C Hybrid 50.08 49.47 49.77 58.13 57.43 57.78 61.27 60.53 60.90 98.95Petrov I-0 74.19 74.27 74.23 74.66 74.74 74.70 78.65 78.73 78.69 99.95Petrov I-4 85.86 85.78 85.82 86.36 86.29 86.32 89.96 89.88 89.92 99.90Petrov I-5 86.30 86.16 86.23 86.84 86.70 86.77 90.28 90.13 90.21 99.90Petrov I-6 85.95 85.68 85.81 86.51 86.23 86.37 90.22 89.93 90.08 99.22Petrov no feats I-0 - - - 72.16 72.59 72.37 76.52 76.97 76.74 99.95Petrov no feats I-5 - - - 86.67 86.57 86.62 90.30 90.20 90.25 99.90Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95 99.84Figure 5: Constituent accuracy on all sentences from section 00.Labeled % Labeled no feats % Unlabeled %Parser R P F R P F R P FPetrov I-5 86.56 86.46 86.51 87.10 87.01 87.05 90.43 90.33 90.38Petrov no feats I-6 - - - 87.45 87.37 87.41 90.99 90.91 90.95p-value - - - 0.089 0.090 0.088 0.006 0.008 0.007Figure 6: Constituent accuracy on the sentences in section 00 that receive a derivation from both parsers.Labeled % Labeled no feats % Unlabeled %Parser R P F R P F R P F CoverC&C Normal Form 71.15 70.79 70.97 80.73 80.32 80.53 86.31 85.88 86.10 99.58Petrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67 99.83Petrov no feats I-6 - - - 87.49 87.49 87.49 90.81 90.82 90.81 99.96Figure 7: Constituent accuracy on all sentences from section 23.Labeled % Labeled no feats % Unlabeled %Parser R P F R P F R P FPetrov I-5 86.94 86.80 86.87 87.47 87.32 87.39 90.75 90.59 90.67Petrov no feats I-6 - - - 87.48 87.49 87.49 90.81 90.82 90.81p-value - - - 0.463 0.215 0.327 0.364 0.122 0.222Figure 8: Constituent accuracy on the sentences in section 23 that receive a derivation from both parsers.Labeled % Unlabeled %Parser R P F R P F CoverPetrov on PTB I-6 89.65 89.97 89.81 90.80 91.13 90.96 100.00Petrov on CCGbank I-5 86.94 86.80 86.87 90.75 90.59 90.67 99.83Petrov on CCGbank no feats I-6 87.49 87.49 87.49 90.81 90.82 90.81 99.96Figure 9: Constituent accuracy for the Petrov parser on the corpora on all sentences from Section 23.Mr.
Vinken is chairman of Elsevier N.V. , the Dutch publishing group .N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .Figure 10: The argument-functor relations for the CCG derivation in figure 1.341Mr.
Vinken is chairman of Elsevier N.V. , the Dutch publishing group .N/N N S[dcl]\NP/NP N NP\NP/NP N/N N , NP [nb]/N N/N N/N N .Figure 11: The set of dependencies obtained by reorienting the argument-functor edges in figure 10.Labeled % Unlabeled %Parser R P F R P F CoverC&C Normal Form 84.39 85.28 84.83 90.93 91.89 91.41 98.95C&C Hybrid 84.53 86.20 85.36 90.84 92.63 91.73 98.95Petrov I-0 79.87 78.81 79.34 87.68 86.53 87.10 96.45Petrov I-4 84.76 85.27 85.02 91.69 92.25 91.97 96.81Petrov I-5 85.30 85.87 85.58 92.00 92.61 92.31 96.65Petrov I-6 84.86 85.46 85.16 91.79 92.44 92.11 96.65Figure 12: Dependency accuracy on CCGbank dependencies on all sentences from section 00.Labeled % Unlabeled %Parser R P F R P FC&C Hybrid 84.71 86.35 85.52 90.96 92.72 91.83Petrov I-5 85.50 86.08 85.79 92.12 92.75 92.44p-value 0.005 0.189 0.187 < 0.001 0.437 0.001Figure 13: Dependency accuracy on the section 00 sentences that receive an analysis from both parsers.Labeled % Unlabeled %Parser R P F R P FC&C Hybrid 85.11 86.46 85.78 91.15 92.60 91.87Petrov I-5 85.73 86.29 86.01 92.04 92.64 92.34p-value 0.013 0.278 0.197 < 0.001 0.404 0.005Figure 14: Dependency accuracy on the section 23 sentences that receive an analysis from both parsers.Training Time Parsing Time Training RAMParser in CPU minutes in CPU minutes in gigabytesClark and Curran Normal Form Model 1152 2 28Clark and Curran Hybrid Model 2672 4 37Petrov on PTB I-0 1 5 2Petrov on PTB I-5 180 20 8Petrov on PTB I-6 660 21 16Petrov on CCGbank I-0 1 5 2Petrov on CCGbank I-4 103 70 8Petrov on CCGbank I-5 410 600 14Petrov on CCGbank I-6 2760 2880 24Petrov on CCGbank no feats I-0 1 5 2Petrov on CCGbank no feats I-5 360 240 7Petrov on CCGbank no feats I-6 1980 390 13Figure 15: Time and space usage when training on sections 02-21 and parsing on section 00.342scores to the quality of a parse is not entirely clear.For this reason, the word to word dependenciesof categorial grammar parsers are often evaluated.This evaluation is aided by the fact that in additionto the CCG derivation for each sentence, CCG-bank also includes a set of dependencies.
Fur-thermore, extracting dependencies from a CCGderivation is well-established (Clark et al, 2002).A CCG derivation can be converted into de-pendencies by, first, determining which argumentsgo with which functors as specified by the CCGderivation.
This can be represented as in figure10.
Although this is not difficult, some care mustbe taken with respect to punctuation and the con-junction rules.
Next, we reorient some of theedges according to information in the lexical cat-egories.
A language for specifying these instruc-tions using variables and indices is given in Clarket al (2002).
This process is shown in figures 1,10 and 11 with the directions of the dependenciesreversed from Clark et al (2002).We used the CCG derivation to dependencyconverter generate included in the C&C toolspackage to convert the output of the Petrov parserto dependencies.
Other than a CCG derivation,their system requires only the lexicon of edge re-orientation instructions and methods for convert-ing the unrestricted rules of CCGbank into theargument-functor relations.
Important for the pur-pose of comparison, this system does not dependon their parser.An unlabeled dependency is correct if the or-dered pair of words is correct.
A labeled depen-dency is correct if the ordered pair of words is cor-rect, the head word has the correct category andthe position of the category that is the source ofthat edge is correct.
Figure 12 shows accuraciesfrom the Petrov parser trained on CCGbank alongwith accuracies for the Clark and Curran parser.We only show accuracies for the Petrov parsertrained on the original version of CCGbank be-cause the dependency converter cannot currentlygenerate dependencies for featureless derivations.The relatively poor coverage of the Petrovparser is due to the failure of the dependency con-verter to output dependencies from valid CCGderivations.
However, the coverage of the depen-dency converter is actually lower when run on thegold standard derivations indicating that this cov-erage problem is not indicative of inaccuracies inthe Petrov parser.
Due to the difference in cover-age, we again evaluate the top two parsers on onlythose sentences that they both generate dependen-cies for and report those results in figures 13 and14.
The Petrov parser has better results by a sta-tistically significant margin for both labeled andunlabeled recall and unlabeled F-score.3.5 Time and Space EvaluationAs a final evaluation, we compare the resourcesthat are required to both train and parse with thePetrov parser on the Penn Treebank, the Petrovparser on the original version of CCGbank, thePetrov parser on CCGbank without features andthe Clark and Curran parser using the two mod-els.
All training and parsing was done on a 64-bitmachine with 8 dual core 2.8 Ghz Opteron 8220CPUs and 64GB of RAM.
Our training times aremuch larger than those reported in Clark and Cur-ran (2007) because we report the cumulative timespent on all CPUs rather than the maximum timespent on a CPU.
Figure 15 shows the results.As can be seen, the Clark and Curran parserhas similar training times, although signifi-cantly greater RAM requirements than the Petrovparsers.
In contrast, the Clark and Curran parser issignificantly faster than the Petrov parsers, whichwe hypothesize to be attributed to the degreeto which Clark and Curran have optimized theircode, their use of C++ as opposed to Java andtheir use of a supertagger to prune the lexicon.4 ConclusionWe have provided a number of theoretical resultsproving that CCGbank contains no non-context-free structure and that the Clark and Curran parseris actually a context-free parser.
Based on theseresults, we trained the Petrov parser on CCGbankand achieved state of the art results in terms ofsupertagging accuracy, PARSEVAL measures anddependency accuracy.This demonstrates the following.
First, the abil-ity to extract semantic representations from CCGderivations is not dependent on the language classof a CCG.
Second, using a dedicated supertagger,as opposed to simply using a general purpose tag-ger, is not necessary to accurately parse with CCG.AcknowledgmentsWe would like to thank Stephen Clark, James Cur-ran, Jackie C. K. Cheung and our three anonymousreviewers for their insightful comments.343ReferencesJ.
Baldridge.
2002.
Lexically Specified Deriva-tional Control in Combinatory Categorial Gram-mar.
Ph.D. thesis, University of Edinburgh.J.
Bos, S. Clark, M. Steedman, J. R Curran, andJ.
Hockenmaier.
2004.
Wide-coverage semanticrepresentations from a CCG parser.
In Proceedingsof COLING, volume 4, page 1240?1246.S.
Clark and J. R. Curran.
2007.
Wide-Coverage ef-ficient statistical parsing with CCG and Log-Linearmodels.
Computational Linguistics, 33(4):493?552.S.
Clark, J. Hockenmaier, and M. Steedman.
2002.Building deep dependency structures with a wide-coverage CCG parser.
In Proceedings of the 40thMeeting of the ACL, page 327?334.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:a corpus of CCG derivations and dependency struc-tures extracted from the penn treebank.
Computa-tional Linguistics, 33(3):355?396.F.
Hoyt and J. Baldridge.
2008.
A logical basis forthe d combinator and normal form in CCG.
In Pro-ceedings of ACL-08: HLT, page 326?334, Colum-bus, Ohio.
Association for Computational Linguis-tics.J.
Lambek.
1958.
The mathematics of sen-tence structure.
American Mathematical Monthly,65(3):154?170.S.
Petrov and D. Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACLHLT 2007, page 404?411.M.
Steedman.
2000.
The syntactic process.
MITPress.K.
Vijay-Shanker and D. Weir.
1994.
The equivalenceof four extensions of context-free grammars.
Math-ematical Systems Theory, 27(6):511?546.W.
Zielonka.
1981.
Axiomatizability of Ajdukiewicz-Lambek calculus by means of cancellation schemes.Zeitschrift fur Mathematische Logik und Grundla-gen der Mathematik, 27:215?224.344
