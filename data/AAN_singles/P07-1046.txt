Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 360?367,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsThe Utility of a Graphical Representation of Discourse Structure inSpoken Dialogue SystemsMihai RotaruUniversity of PittsburghPittsburgh, USAmrotaru@cs.pitt.eduDiane J. LitmanUniversity of PittsburghPittsburgh, USAlitman@cs.pitt.eduAbstractIn this paper we explore the utility of theNavigation Map (NM), a graphical repre-sentation of the discourse structure.
We runa user study to investigate if users perceivethe NM as helpful in a tutoring spoken dia-logue system.
From the users?
perspective,our results show that the NM presence al-lows them to better identify and follow thetutoring plan and to better integrate the in-struction.
It was also easier for users toconcentrate and to learn from the system ifthe NM was present.
Our preliminaryanalysis on objective metrics furtherstrengthens these findings.1 IntroductionWith recent advances in spoken dialogue systemtechnologies, researchers have turned their atten-tion to more complex domains (e.g.
tutoring(Litman and Silliman, 2004; Pon-Barry et al,2006), technical support (Acomb et al, 2007),medication assistance (Allen et al, 2006)).
Thesedomains bring forward new challenges and issuesthat can affect the usability of such systems: in-creased task complexity, user?s lack of or limitedtask knowledge, and longer system turns.In typical information access dialogue systems,the task is relatively simple: get the informationfrom the user and return the query results withminimal complexity added by confirmation dia-logues.
Moreover, in most cases, users haveknowledge about the task.
However, in complexdomains things are different.
Take for exampletutoring.
A tutoring dialogue system has to discussconcepts, laws and relationships and to engage incomplex subdialogues to correct user misconcep-tions.
In addition, it is very likely that users of suchsystems are not familiar or are only partially famil-iar with the tutoring topic.
The length of systemturns can also be affected as these systems need tomake explicit the connections between parts of theunderlying task.Thus, interacting with such systems can be char-acterized by an increased user cognitive load asso-ciated with listening to often lengthy system turnsand the need to integrate the current information tothe discussion overall (Oviatt et al, 2004).We hypothesize that one way to reduce theuser?s cognitive load is to make explicit two piecesof information: the purpose of the current systemturn, and how the system turn relates to the overalldiscussion.
This information is implicitly encodedin the intentional structure of a discourse as pro-posed in the Grosz & Sidner theory of discourse(Grosz and Sidner, 1986).Consequently, in this paper we propose using agraphical representation of the discourse structureas a way of improving the performance of com-plex-domain dialogue systems (note that graphicaloutput is required).
We call it the Navigation Map(NM).
The NM is a dynamic representation of thediscourse segment hierarchy and the discourse seg-ment purpose information enriched with severalfeatures (Section 3).
To make a parallel with geog-raphy, as the system ?navigates?
with the userthrough the domain, the NM offers a cartographicview of the discussion.
While a somewhat similargraphical representation of the discourse structurehas been explored in one previous study (Rich andSidner, 1998), to our knowledge we are the first totest its benefits (see Section 6).360As a first step towards understanding the NM ef-fects, here we focus on investigating whether usersprefer a system with the NM over a system withoutthe NM and, if yes, what are the NM usage pat-terns.
We test this in a speech based computer tutor(Section 2).
We run a within-subjects user study inwhich users interacted with the system both withand without the NM (Section 4).Our analysis of the users?
subjective evaluationof the system indicates that users prefer the versionof the system with the NM over the version with-out the NM on several dimensions.
The NM pres-ence allows the users to better identify and followthe tutoring plan and to better integrate the instruc-tion.
It was also easier for users to concentrate andto learn from the system if the NM was present.Our preliminary analysis on objective metrics fur-ther strengthens these findings.2 ITSPOKEITSPOKE (Litman and Silliman, 2004) is a state-of-the-art tutoring spoken dialogue system for con-ceptual physics.
When interacting with ITSPOKE,users first type an essay answering a qualitativephysics problem using a graphical user interface.ITSPOKE then engages the user in spoken dialogue(using head-mounted microphone input and speechoutput) to correct misconceptions and elicit morecomplete explanations, after which the user revisesthe essay, thereby ending the tutoring or causinganother round of tutoring/essay revision.All dialogues with ITSPOKE follow a question-answer format (i.e.
system initiative): ITSPOKEasks a question, users answer and then the processis repeated.
Deciding what question to ask, in whatorder and when to stop is hand-authored before-hand in a hierarchical structure.
Internally, systemquestions are grouped in question segments.In Figure 1, we show the transcript of a sampleinteraction with ITSPOKE.
The system is discussingthe problem listed in the upper right corner of thefigure and it is currently asking the question Tu-tor5.
The left side of the figure shows the interac-tion transcript (not available to the user at run-time).
The right side of the figure shows the NMwhich will be discussed in the next section.Our system behaves as follows.
First, based onthe analysis of the user essay, it selects a questionsegment to correct misconceptions or to elicit morecomplete explanations.
Next the system asks everyquestion from this question segment.
If the useranswer is correct, the system simply moves on tothe next question (e.g.
Tutor2?Tutor3).
For incor-rect answers there are two alternatives.
For simplequestions, the system will give out the correct an-swer accompanied by a short explanation andmove on to the next question (e.g.
Tutor1?Tutor2).For complex questions (e.g.
applying physicslaws), ITSPOKE will engage into a remediationsubdialogue that attempts to remediate user?s lackof knowledge or skills (e.g.
Tutor4?Tutor5).
Theremediation subdialogue for each complex ques-tion is specified in another question segment.Our system exhibits some of the issues welinked in Section 1 with complex-domain systems.Dialogues with our system can be long and com-plex (e.g.
the question segment hierarchical struc-ture can reach level 6) and sometimes the system?sturn can be quite long (e.g.
Tutor2).
User?s reducedknowledge of the task is also inherent in tutoring.3 The Navigation Map (NM)We use the Grosz & Sidner theory of discourse(Grosz and Sidner, 1986) to inform our NM de-sign.
According to this theory, each discourse has adiscourse purpose/intention.
Satisfying the maindiscourse purpose is achieved by satisfying severalsmaller purposes/intentions organized in a hierar-chical structure.
As a result, the discourse is seg-mented into discourse segments each with an asso-ciated discourse segment purpose/intention.
Thistheory has inspired several generic dialogue man-agers for spoken dialogue systems (e.g.
(Rich andSidner, 1998)).The NM requires that we have the discoursestructure information at runtime.
To do that, wemanually annotate the system?s internal representa-tion of the tutoring task with discourse segmentpurpose and hierarchy information.
Based on thisannotation, we can easily construct the discoursestructure at runtime.
In this section we describe ourannotation and the NM design choices we made.Figure 1 shows the state of the NM after turn Tu-tor5 as the user sees it on the interface (NM linenumbering is for exposition only).
Note that Figure1 is not a screenshot of the actual system interface.The NM is the only part from the actual systeminterface.
Figure 2 shows the NM after turn Tutor1.We manually annotated each system ques-tion/explanation for its intention(s)/purpose(s).Note that some system turns have multiple inten-361tions/purposes thus multiple discourse segmentswere created for them.
For example, in Tutor1 thesystem first identifies the time frames on which theanalysis will be performed (Figure 1&2, NM2).Next, the system indicates that it will discuss aboutthe first time frame (Figure 1&2, NM3) and then itasks the actual question (Figure 2, NM4).Thus, in addition to our manual annotation ofthe discourse segment purpose information, wemanually organized all discourse segments from aquestion segment in a hierarchical structure thatreflects the discourse structure.At runtime, while discussing a question seg-ment, the system has only to follow the annotatedhierarchy, displaying and highlighting the dis-course segment purposes associated with the ut-tered content.
For example, while uttering Tutor1,the NM will synchronously highlight NM2, NM3and NM4.
Remediation question segments (e.g.NM12) or explanations (e.g.
NM5) activated by in-correct answers are attached to the structure underthe corresponding discourse segment.3.1 NM Design ChoicesIn our graphical representation of the discoursestructure, we used a left to right indented layout.
Inaddition, we made several design choices to enrichthe NM information content and usability.Figure 2.
NM state after turn Tutor1Correct answers.
In Figure 2 we show the stateof the NM after uttering Tutor1.
The current dis-course segment purpose (NM4) indicates that thesystem is asking about the relationship between thetwo velocities.
While we could have kept the sameinformation after the system was done with thisdiscourse segment, we thought that users willbenefit from having the correct answer on thescreen (recall NM4 in Figure 1).
Thus, the NM wasenhanced to display the correct answer after thesystem is done with each question.
We extractedthe correct answer from the system specificationsfor each question and manually created a new ver-sion of the discourse segment purpose that includesthis information.123456TUTOR1: To analyze this problem we will first de-scribe the motion of the person and his keys while heis holding them.
Then we will look at the motion ofthe person and his keys after he lets go of them.Let's begin by looking at the motion of the man andhis keys while he is holding them.How does his velocity compare to that of his keys?User answer1: man?s velocity is bigger [incorrect]TUTOR2: Well?
The problem statement says that theman is holding his keys motionless in front of hisface.
That means they are both falling together andare not moving with respect to each other.
Hence theirvelocities are the same.Now let's see what happens when he releases them.To do so we will first find the individual forcesexerted on the man and his keys, then both net forces,both accelerations, and finally we will describe andcompare the velocities.So what are the forces exerted on the man after hereleases his keys?
Please, specify their directions.User answer2: gravity vertically down [correct]TUTOR3: Good.
Because there is just one force, grav-ity's, it is trivial to determine the NET force.
So whatis the direction of the NET force on the person?User answer3: down [correct]TUTOR4: Ok.
Considering the net force, does the per-son have an acceleration?
If yes, specify its direction.User answer4: the man has no acceleration [incorrect]TUTOR5: Well?
To analyze the man's accelera-tion we will use Newton's second law.
What isthe definition of this law?Problem: Suppose a man is in a free-falling elevator and isholding his keys motionless right in front of his face.
He thenlets go.
What will be the position of the keys relative to theman's face as time passes?
Explain.12345678910111213141516171819Figure 1.
Transcript of a sample ITSPOKE speech interaction (left).
The NM as the user sees it after turn Tutor5362Limited horizon.
Since in our case the systemdrives the conversation (i.e.
system initiative), wealways know what questions would be discussednext.
We hypothesized that by having access tothis information, users will have a better idea ofwhere instruction is heading, thus facilitating theirunderstanding of the relevance of the current topicto the overall discussion.
To prevent informationoverload, we only display the next discourse seg-ment purpose at each level in the hierarchy (seeFigure 1, NM14, NM16, NM17 and NM19; Figure 2,NM5); additional discourse segments at the samelevel are signaled through a dotted line.
To avoidhelping the students answer the current question incases when the next discourse segment hints/de-scribes the answer, each discourse segment has anadditional purpose annotation that is displayedwhen the segment is part of the visible horizon.Auto-collapse.
To reduce the amount of infor-mation on the screen, discourse segments dis-cussed in the past are automatically collapsed bythe system.
For example, in Figure 1, NM Line 3 iscollapsed in the actual system and Lines 4 and 5are hidden (shown in Figure1 to illustrate our dis-course structure annotation.).
The user can expandnodes as desired using the mouse.Information highlight.
Bold and italics fontwere used to highlight important information (whatand when to highlight was manually annotated).For example, in Figure 1, NM2 highlights the twotime frames as they are key steps in approachingthis problem.
Correct answers are also highlighted.We would like to reiterate that the goal of thisstudy is to investigate if making certain types ofdiscourse information explicitly available to theuser provides any benefits.
Thus, whether we havemade the optimal design choices is of secondaryimportance.
While, we believe that our annotationis relatively robust as the system questions follow acarefully designed tutoring plan, in the future wewould like to investigate these issues.4 User StudyWe designed a user study focused primarily onuser?s perception of the NM presence/absence.
Weused a within-subject design where each user re-ceived instruction both with and without the NM.Each user went through the same experimentalprocedure: 1) read a short document of backgroundmaterial, 2) took a pretest to measure initial phys-ics knowledge, 3) worked through 2 problems withITSPOKE 4) took a posttest similar to the pretest, 5)took a NM survey, and 6) went through a briefopen-question interview with the experimenter.In the 3rd step, the NM was enabled in only oneproblem.
Note that in both problems, users did nothave access to the system turn transcript.
Aftereach problem users filled in a system question-naire in which they rated the system on variousdimensions; these ratings were designed to coverdimensions the NM might affect (see Section 5.1).While the system questionnaire implicitly probedthe NM utility, the NM survey from the 5th stepexplicitly asked the users whether the NM was use-ful and on what dimensions (see Section 5.1)To account for the effect of the tutored problemon the user?s questionnaire ratings, users were ran-domly assigned to one of two conditions.
The usersin the first condition (F) had the NM enabled in thefirst problem and disabled in the second problem,while users in the second condition (S) had the op-posite.
Thus, if the NM has any effect on the user?sperception of the system, we should see a decreasein the questionnaire ratings from problem 1 toproblem 2 for F users and an increase for S users.Other factors can also influence our measure-ments.
To reduce the effect of the text-to-speechcomponent, we used a version of the system withhuman prerecorded prompts.
We also had to ac-count for the amount of instruction as in our sys-tem the top level question segment is tailored towhat users write in the essay.
Thus the essayanalysis component was disabled; for all users, thesystem started with the same top level questionsegment which assumed no information in the es-say.
Note that the actual dialogue depends on thecorrectness of the user answers.
After the dialogue,users were asked to revise their essay and then thesystem moved on to the next problem.The collected corpus comes from 28 users (13 inF and 15 in S).
The conditions were balanced forgender (F: 6 male, 7 female; S: 8 male, 7 female).There was no significant differences between thetwo conditions in terms of pretest (p<0.63); in bothconditions users learned (significant differencebetween pretest and posttest, p<0.01).5 Results5.1 Subjective metricsOur main resource for investigating the effect ofthe NM was the system questionnaires given after363each problem.
These questionnaires are identicaland include 16 questions that probed user?s percep-tion of ITSPOKE on various dimensions.
Userswere asked to answer the questions on a scale from1-5 (1 ?
Strongly Disagree, 2 ?
Disagree, 3 ?Somewhat Agree, 4 ?
Agree, 5 ?
Strongly Agree).If indeed the NM has any effect we should observedifferences between the ratings of the NM problemand the noNM problem (i.e.
the NM is disabled).Table 1 lists the 16 questions in the question-naire order.
The table shows for every question theaverage rating for all condition-problem combina-tions (e.g.
column 5: condition F problem 1 withthe NM enabled).
For all questions except Q7 andQ11 a higher rating is better.
For Q7 and Q11(italicized in Table 1) a lower rating is better asthey gauge negative factors (high level of concen-tration and task disorientation).
They also served asa deterrent for negligence while rating.To test if the NM presence has a significant ef-fect, a repeated-measure ANOVA with between-subjects factors was applied.
The within-subjectsfactor was the NM presence (NMPres) and thebetween-subjects factor was the condition (Cond)1.The significance of the effect of each factor andtheir combination (NMPres*Cond) is listed in thetable with significant and trend effects highlightedin bold (see columns 2-4).
Post-hoc t-tests betweenthe NM and noNM ratings were run for each con-dition (?s?/?t?marks significant/trend differences).Results for Q1-6Questions Q1-6 were inspired by previous workon spoken dialogue system evaluation (e.g.
(Walker et al, 2000)) and measure user?s overallperception of the system.
We find that the NMpresence significantly improves user?s perceptionof the system in terms of their ability to concen-trate on the instruction (Q3), in terms of their incli-nation to reuse the system (Q6) and in terms of thesystem?s matching of their expectations (Q4).There is a trend that it was easier for them to learnfrom the NM enabled version of the system (Q2).Results for Q7-13Q7-13 relate directly to our hypothesis that users1 Since in this version of ANOVA the NM/noNM rat-ings come from two different problems based on thecondition, we also run an ANOVA in which the within-subjects factor was the problem (Prob).
In this case, theNM effect corresponds to an effect from Prob*Condwhich is identical in significance with that of NMPres.benefit from access to the discourse structure in-formation.
These questions probe the user?s per-ception of ITSPOKE during the dialogue.
We findthat for 6 out 7 questions the NM presence has asignificant/trend effect (Table 1, column 2).Structure.
Users perceive the system as havinga structured tutoring plan significantly2 more in theNM problems (Q8).
Moreover, it is significantlyeasier for them to follow this tutoring plan if theNM is present (Q11).
These effects are very clearfor F users where their ratings differ significantlybetween the first (NM) and the second problem(noNM).
A difference in ratings is present for Susers but it is not significant.
As with most of the Susers?
ratings, we believe that the NM presentationorder is responsible for the mostly non-significantdifferences.
More specifically, assuming that theNM has a positive effect, the S users are asked torate first the poorer version of the system (noNM)and then the better version (NM).
In contrast, Fusers?
task is easier as they already have a highreference point (NM) and it is easier for them tocriticize the second problem (noNM).
Other factorsthat can blur the effect of the NM are domainlearning and user?s adaptation to the system.Integration.
Q9 and Q10 look at how well usersthink they integrate the system questions in both aforward-looking fashion (Q9) and a backwardlooking fashion (Q10).
Users think that it is sig-nificantly easier for them to integrate the currentsystem question to what will be discussed in thefuture if the NM is present (Q9).
Also, if the NM ispresent, it is easier for users to integrate the currentquestion to the discussion so far (Q10, trend).
ForQ10, there is no difference for F users but a sig-nificant one for S users.
We hypothesize that do-main learning is involved here: F users learn betterfrom the first problem (NM) and thus have lessissues solving the second problem (noNM).
In con-trast, S users have more difficulties in the firstproblem (noNM), but the presence of the NMeases their task in the second problem.Correctness.
The correct answer NM feature isuseful for users too.
There is a trend that it is easierfor users to know the correct answer if the NM ispresent (Q13).
We hypothesize that speech recog-nition and language understanding errors are re-2 We refer to the significance of the NMPres factor (Ta-ble 1, column 2).
When discussing individual experi-mental conditions, we refer to the post-hoc t-tests.364sponsible for the non-significant NM effect on thedimension captured by Q12.Concentration.
Users also think that the NMenabled version of the system requires less effort interms of concentration (Q7).
We believe that hav-ing the discourse segment purpose as visual inputallows the users to concentrate more easily on whatthe system is uttering.
In many of the open ques-tion interviews users stated that it was easier forthem to listen to the system when they had the dis-course segment purpose displayed on the screen.Results for Q14-16Questions Q14-16 were included to probe user?spost tutoring perceptions.
We find a trend that inthe NM problems it was easier for users to under-stand the system?s main point (Q14).
However, interms of identifying (Q15) and correcting (Q16)problems in their essay the results are inconclusive.We believe that this is due to the fact that the essayinterpretation component was disabled in this ex-periment.
As a result, the instruction did not matchthe initial essay quality.
Nonetheless, in the open-question interviews, many users indicated usingthe NM as a reference while updating their essay.In addition to the 16 questions, in the systemquestionnaire after the second problem users wereasked to choose which version of the system theypreferred the most (i.e.
the first or the second prob-lem version).
24 out 28 users (86%) preferred theNM enabled version.
In the open-question inter-view, the 4 users that preferred the noNM version(2 in each condition) indicated that it was harderfor them to concurrently concentrate on the audioand the visual input (divided attention problem)and/or that the NM was changing too fast.To further strengthen our conclusions from thesystem questionnaire analysis, we would like tonote that users were not asked to directly comparethe two versions but they were asked to individu-ally rate two versions which is a noisier process(e.g.
users need to recall their previous ratings).The NM surveyWhile the system questionnaires probed users?NM usage indirectly, in the second to last step inthe experiments, users had to fill a NM surveyTable 1.
System questionnaire resultsQuestionOverall NMPres CondNMPres*Cond1.
The tutor increased my understanding of the subject 0.518 0.898 0.862 4.0 > 3.9 4.0 > 3.92.
It was easy to learn from the tutor 0.100 0.813 0.947 3.9 > 3.6 3.9 > 3.53.
The tutor helped me to concentrate 0.016 0.156 0.854 3.5 > 3.0 3.9 >t 3.44.
The tutor worked the way I expected it to 0.034 0.886 0.157 3.5 > 3.4 3.9 >s 3.15.
I enjoyed working with the tutor 0.154 0.513 0.917 3.5 > 3.2 3.7 > 3.46.
Based on my experience using the tutor to learn physics, Iwould like to use such a tutor regularly0.004 0.693 0.988 3.7 >s 3.2 3.5 >s 3.0During the conversation with the tutor:7.
... a high level of concentration is required to follow the tutor 0.004 0.534 0.545 3.5 <s 4.2 3.9 <t 4.38.
... the tutor had a clear and structured agenda behind itsexplanations0.008 0.340 0.104 4.4 >s 3.6 4.3 > 4.19.
... it was easy to figure out where the tutor's instruction wasleading me0.017 0.472 0.593 4.0 >s 3.4 4.1 > 3.710.
... when the tutor asked me a question I knew why it wasasking me that question0.054 0.191 0.054 3.5 ~ 3.5 4.3 >s 3.511.
... it was easy to loose track of where I was in the interactionwith the tutor0.012 0.766 0.048 2.5 <s 3.5 2.9 < 3.012.
...
I knew whether my answer to the tutor's question wascorrect or incorrect0.358 0.635 0.804 3.5 > 3.3 3.7 > 3.413.
... whenever I answered incorrectly, it was easy to know thecorrect answer after the tutor corrected me0.085 0.044 0.817 3.8 > 3.5 4.3 > 3.9At the end of the conversation with the tutor:14.
... it was easy to understand the tutor's main point 0.071 0.056 0.894 4.0 > 3.6 4.4 > 4.115.
...
I knew what was wrong or missing from my essay 0.340 0.965 0.340 3.9 ~ 3.9 3.7 < 4.016.
...
I knew how to modify my essay 0.791 0.478 0.327 4.1 > 3.9 3.7 < 3.8P1       P2NM     noNMP2       P1NM     noNMAverage ratingANOVA F condition S condition365which explicitly asked how the NM helped them, ifat all.
The answers were on the same 1 to 5 scale.We find that the majority of users (75%-86%)agreed or strongly agreed that the NM helped themfollow the dialogue, learn more easily, concentrateand update the essay.
These findings are on parwith those from the system questionnaire analysis.5.2 Objective metricsOur analysis of the subjective user evaluationsshows that users think that the NM is helpful.
Wewould like to see if this perceived usefulness isreflected in any objective metrics of performance.Due to how our experiment was designed, the ef-fect of the NM can be reliably measured only inthe first problem as in the second problem the NMis toggled3; for the same reason, we can not use thepretest/posttest information.Our preliminary investigation 4  found severaldimensions on which the two conditions differed inthe first problem (F users had NM, S users didnot).
We find that if the NM was present the inter-action was shorter on average and users gave morecorrect answers; however these differences are notstatistically significant (Table 2).
In terms ofspeech recognition performance, we looked at twometrics: AsrMis and SemMis (ASR/Semantic Mis-recognition).
A user turn is labeled as AsrMis if theoutput of the speech recognition is different fromthe human transcript (i.e.
a binary version of WordError Rate).
SemMis are AsrMis that change thecorrectness interpretation.
We find that if the NMwas present users had fewer AsrMis and fewerSemMis (trend for SemMis, p<0.09).In addition, a ?2 dependency analysis showedthat the NM presence interacts significantly withboth AsrMis (p<0.02) and SemMis (p<0.001), withfewer than expected AsrMis and SemMis in the3 Due to random assignment to conditions, before thefirst problem the F and S populations are similar (e.g.
nodifference in pretest); thus any differences in metricscan be attributed to the NM presence/absence.
However,in the second problem, the two populations are not simi-lar anymore as they have received different forms ofinstruction; thus any difference has to be attributed tothe NM presence/absence in this problem as well as tothe NM absence/presence in the previous problem.4 Due to logging issues, 2 S users are excluded from thisanalysis (13 F and 13 S users remaining).
We run thesubjective metric analysis from Section 5.1 on this sub-set and the results are similar.NM condition.
The fact that in the second problemthe differences are much smaller (e.g.
2% forAsrMis) and that the NM-AsrMis and NM-SemMis interactions are not significant anymore,suggests that our observations can not be attributedto a difference in population with respect to sys-tem?s ability to recognize their speech.
We hy-pothesize that these differences are due to the NMtext influencing users?
lexical choice.Metric F (NM) S (noNM) p# user turns 21.8 (5.3) 22.8 (6.5) 0.65% correct turns 72% (18%) 67% (22%) 0.59AsrMis 37% (27%) 46% (28%) 0.46SemMis 5% (6%) 12% (14%) 0.09Table 2.
Average (standard deviation) forobjective metrics in the first problem6 Related workDiscourse structure has been successfully used innon-interactive settings (e.g.
understanding spe-cific lexical and prosodic phenomena (Hirschbergand Nakatani, 1996) , natural language generation(Hovy, 1993), essay scoring (Higgins et al, 2004)as well as in interactive settings (e.g.
predic-tive/generative models of postural shifts (Cassell etal., 2001), generation/interpretation of anaphoricexpressions (Allen et al, 2001), performance mod-eling (Rotaru and Litman, 2006)).In this paper, we study the utility of the dis-course structure on the user side of a dialogue sys-tem.
One related study is that of (Rich and Sidner,1998).
Similar to the NM, they use the discoursestructure information to display a segmented inter-action history (SIH): an indented view of the inter-action augmented with purpose information.
Thispaper extends over their work in several areas.
Themost salient difference is that here we investigatethe benefits of displaying the discourse structureinformation for the users.
In contrast, (Rich andSidner, 1998) never test the utility of the SIH.Their system uses a GUI-based interaction (nospeech/text input, no speech output) while we lookat a speech-based system.
Also, their underlyingtask (air travel domain) is much simpler than ourtutoring task.
In addition, the SIH is not alwaysavailable and users have to activate it manually.Other visual improvements for dialogue-basedcomputer tutors have been explored in the past(e.g.
talking heads (Graesser et al, 2003)).
How-ever, implementing the NM in a new domain re-quires little expertise as previous work has shown366that na?ve users can reliably annotate the informa-tion needed for the NM (Passonneau and Litman,1993).
Our NM design choices should also have anequivalent in a new domain (e.g.
displaying therecognized user answer can be the equivalent ofthe correct answers).
Other NM usages can also beimagined: e.g.
reducing the length of the systemturns by removing text information that is implic-itly represented in the NM.7 Conclusions & Future workIn this paper we explore the utility of the Naviga-tion Map, a graphical representation of the dis-course structure.
As our first step towards under-standing the benefits of the NM, we ran a userstudy to investigate if users perceive the NM asuseful.
From the users?
perspective, the NM pres-ence allows them to better identify and follow thetutoring plan and to better integrate the instruction.It was also easier for users to concentrate and tolearn from the system if the NM was present.
Ourpreliminary analysis on objective metrics showsthat users?
preference for the NM version is re-flected in more correct user answers and lessspeech recognition problems in the NM version.These findings motivate future work in under-standing the effects of the NM.
We would like tocontinue our objective metrics analysis (e.g.
see ifusers are better in the NM condition at updatingtheir essay and at answering questions that requirecombining facts previously discussed).
We alsoplan to run an additional user study with a be-tween-subjects experimental design geared towardsobjective metrics.
The experiment will have twoconditions: NM present/absent for all problems.The conditions will then be compared in terms ofvarious objective metrics.
We would also like toknow which information sources represented in theNM (e.g.
discourse segment purpose, limited hori-zon, correct answers) has the biggest impact.AcknowledgementsThis work is supported by NSF Grants 0328431and 0428472.
We would like to thank Shimei Pan,Pamela Jordan and the ITSPOKE group.ReferencesK.
Acomb, J. Bloom, K. Dayanidhi, P. Hunter, P.Krogh, E. Levin and R. Pieraccini.
2007.
TechnicalSupport Dialog Systems: Issues, Problems, and Solu-tions.
In Proc.
of Workshop on Bridging the Gap:Academic and Industrial Research in Dialog Technologies.J.
Allen, G. Ferguson, B. N., D. Byron, N. Chambers,M.
Dzikovska, L. Galescu and M. Swift.
2006.
Ches-ter: Towards a Personal Medication Advisor.
Journalof Biomedical Informatics, 39(5).J.
Allen, G. Ferguson and A. Stent.
2001.
An architec-ture for more realistic conversational systems.
InProc.
of Intelligent User Interfaces.J.
Cassell, Y. I. Nakano, T. W. Bickmore, C. L. Sidnerand C. Rich.
2001.
Non-Verbal Cues for DiscourseStructure.
In Proc.
of ACL.A.
Graesser, K. Moreno, J. Marineau, A. Adcock, A.Olney and N. Person.
2003.
AutoTutor improves deeplearning of computer literacy: Is it the dialog or thetalking head?
In Proc.
of Artificial Intelligence inEducation (AIED).B.
Grosz and C. L. Sidner.
1986.
Attentions, intentionsand the structure of discourse.
Computational Lin-guistics, 12(3).D.
Higgins, J. Burstein, D. Marcu and C. Gentile.
2004.Evaluating Multiple Aspects of Coherence in StudentEssays.
In Proc.
of HLT-NAACL.J.
Hirschberg and C. Nakatani.
1996.
A prosodic analy-sis of discourse segments in direction-giving mono-logues.
In Proc.
of ACL.E.
Hovy.
1993.
Automated discourse generation usingdiscourse structure relations.
Articial Intelligence,63(Special Issue on NLP).D.
Litman and S. Silliman.
2004.
ITSPOKE: An intelli-gent tutoring spoken dialogue system.
In Proc.
ofHLT/NAACL.S.
Oviatt, R. Coulston and R. Lunsford.
2004.
When DoWe Interact Multimodally?
Cognitive Load and Mul-timodal Communication Patterns.
In Proc.
of Interna-tional Conference on Multimodal Interfaces.R.
Passonneau and D. Litman.
1993.
Intention-basedsegmentation: Human reliability and correlation withlinguistic cues.
In Proc.
of ACL.H.
Pon-Barry, K. Schultz, E. O. Bratt, B. Clark and S.Peters.
2006.
Responding to Student Uncertainty inSpoken Tutorial Dialogue Systems.
InternationalJournal of Artificial Intelligence in Education, 16.C.
Rich and C. L. Sidner.
1998.
COLLAGEN: A Col-laboration Manager for Software Interface Agents.User Modeling and User-Adapted Interaction, 8(3-4).M.
Rotaru and D. Litman.
2006.
Exploiting DiscourseStructure for Spoken Dialogue Performance Analy-sis.
In Proc.
of EMNLP.M.
Walker, D. Litman, C. Kamm and A. Abella.
2000.Towards Developing General Models of Usabilitywith PARADISE.
Natural Language Engineering.367
