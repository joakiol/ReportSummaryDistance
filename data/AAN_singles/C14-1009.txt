Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 79?88, Dublin, Ireland, August 23-29 2014.Joint Inference and Disambiguation of Implicit Sentiments viaImplicature ConstraintsLingjia Deng1, Janyce Wiebe1,2, Yoonjung Choi21Intelligent Systems Program, University of Pittsburgh2Department of Computer Science, University of Pittsburghlid29@pitt.edu, wiebe@cs.pitt.edu, yjchoi@cs.pitt.eduAbstractThis paper addresses implicit opinions expressed via inference over explicit sentiments andevents that positively/negatively affect entities (goodFor/badFor, gfbf events).
We incorporatethe inferences developed by implicature rules into an optimization framework, to jointly improvesentiment detection toward entities and disambiguate components of gfbf events.
The frameworksimultaneously beats the baselines by more than 10 points in F-measure on sentiment detectionand more than 7 points in accuracy on gfbf polarity disambiguation.1 IntroductionPrevious work in NLP on sentiment analysis has mainly focused on explicit sentiments.
However, asnoted in (Deng and Wiebe, 2014), many opinions are expressed implicitly, as shown by this example:Ex(1) The reform would lower health care costs, which would be a tremendous positive change across the entirehealth-care system.There is an explicit positive sentiment toward the event of ?reform lower costs?.
However, in expressingthis sentiment, the writer also implies he is negative toward the ?costs?, since he?s happy to see the costsbeing decreased.
Moreover, the writer may be positive toward ?reform?
since it contributes to the ?lower?event.
Such inferences may be seen as opinion-oriented implicatures (i.e., defeasible inferences)1.We develop a set of rules for inferring and detecting implicit sentiments from explicit sentiments andevents such as ?lower?
(Wiebe and Deng, 2014).
In (Deng et al., 2013), we investigate such events,defining a badFor (bf) event to be an event that negatively affects the theme and a goodFor (gf) event tobe an event that positively affects the theme of the event.2Here, ?lower?
is a bf event.
According to theirannotation scheme, goodFor/badFor (gfbf) events have NP agents and themes (though the agent may beimplicit), and the polarity of a gf event may be changed to bf by a reverser (and vice versa).The ultimate goal of this work is to utilize gfbf information to improve detection of the writer?s senti-ments toward entities mentioned in the text.
However, this requires resolving several ambiguities: (Q1)Given a document, which spans are gfbf events?
(Q2) Given a gfbf text span, what is its polarity, gfor bf?
(Q3) Is the polarity of a gfbf event being reversed?
(Q4) Which NP in the sentence is the agentand which is the theme?
(Q5) What are the writer?s sentiments toward the agent and theme, positiveor negative?
Fortunately, the implicature rules in (Deng and Wiebe, 2014) define dependencies amongthese ambiguities.
As in Ex(1), the sentiments toward the agent and theme, the sentiment toward the gfbfevent (positive or negative), and the polarity of the gfbf event (gf or bf) are all interdependent.
Thus,rather than having to take a pipeline approach, we are able to develop an optimization framework whichexploits these interdependencies to jointly resolve the ambiguities.Specifically, we develop local detectors to analyze the four individual components of gfbf events,(Q2)-(Q5) above.
Then, we propose an Integer Linear Programming (ILP) framework to conduct globalThis work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/1Specifically, we focus on generalized conversational implicature (Grice, 1967; Grice, 1989).2Compared to (Deng et al., 2013), we change the term ?object?
to ?theme?
as the later is more appropriate for this task.79inference, where the gfbf events and their components are variables and the interdependencies defined bythe implicature rules are encoded as constraints over relevant variables in the framework.
The reason wedo not address (Q1) is that the gold standard we use for evaluation contains sentiment annotations onlytoward the agents and themes of gfbf events.
We are only able to evaluate true hits of gfbf events.
Thus,the input to the system is the set of the text spans marked as gfbf events in the corpus.
The results showthat, compared to the local detectors, the ILP framework improves sentiment detection by more than 10points in F-measure and disambiguating gfbf polarity by more than 7 points in the accuracy, without anyloss in accuracy for other two components.2 Related WorkMost work in sentiment analysis focuses on classifying explicit sentiments and extracting explicit opinionexpressions, holders and targets (Wiebe et al., 2005; Johansson and Moschitti, 2013; Yang and Cardie,2013).
There is some work investigating features that directly indicate implicit sentiments (Zhang andLiu, 2011; Feng et al., 2013).
In contrast, we focus on how we can bridge between explicit and implicitsentiments via inference.
To infer the implicit sentiments related to gfbf events, some work mines varioussyntactic patterns (Choi and Cardie, 2008), proposes linguistic templates (Zhang and Liu, 2011; Anandand Reschke, 2010; Reschke and Anand, 2011), or generates a lexicon of patient polarity verbs (Goyalet al., 2013).
Different from their work, which do not cover all cases relevant to gfbf events, (Deng andWiebe, 2014) defines a generalized set of implicature rules and proposes a graph-based model to achievesentiment propagation between the agents and themes of gfbf events.
However, that system requiresall of the gfbf information (Q1)-(Q4) to be input from the manual annotations; the only ambiguity itresolves is sentiments toward entities.
In contrast, the method in this paper tackles four ambiguitiessimultaneously.
Further, as we will see below in Section 6, the improvement over the local detectors bythe current method is greater than that by the previous method, even though it operates over the noisyoutput of local components automatically.Different from pipeline architectures, where each step is computed independently, joint inference hasoften achieved better results.
Roth and Yih (2004) formulate the task of information extraction usingInteger Linear Programming (ILP).
Since then, ILP has been widely used in various tasks in NLP, in-cluding semantic role labeling (Punyakanok et al., 2004; Punyakanok et al., 2008; Das et al., 2012),joint extraction of opinion entities and relations (Choi et al., 2006; Yang and Cardie, 2013), co-referenceresolution (Denis and Baldridge, 2007), and summarization (Martins and Smith, 2009).
The most similarILP model to ours is (Somasundaran and Wiebe, 2009), which improves opinion polarity classificationusing discourse constraints in an ILP model.
However, their work addresses discourse relations amongexplicit opinions in different sentences.3 GoodFor/BadFor Event and ImplicatureThis work addresses sentiments toward, in general, states and events which positively or negativelyaffect entities.
Deng et al.
(2013) (hereafter DCW) identify a clear case that occurs frequently in opinionsentences, namely the gfbf events mentioned above.
As defined in DCW, a gf event is an event thatpositively affects the theme of the event and a bf event is an event that negatively affects the theme.According to the annotation schema, gfbf events have NP agents and themes (though the agent may beimplicit).
In the sentence ?President Obama passed the bill?, the agent of the gf ?passed?
is ?PresidentObama?
and the theme is ?the bill?.
In the sentence ?The bill was denied?, the agent of the bf ?wasdenied?
is implicit.
The polarity of a gf event may be changed to bf by a reverser (and vice versa).
Forexample, in ?The reform will not worsen the economy,?
?not?
is a reverser and it reverses the polarityfrom bf to gf.3The constraints we encode in the ILP framework described below are based on implicature rules in(Deng and Wiebe, 2014).
Table 1 gives two rule schemas, each of which defines four specific rules.
In3DCW also introduce retainers.
We don?t analyze retainers in this work since they do not affect the polarity of gfbfs, andonly 2.5% of gfbfs have retainers in the corpus.80s(gfbf) gfbf ?
s(agent) s(theme) s(gfbf) gfbf ?
s(agent) s(theme)1 positive gf ?
positive positive 3 positive bf ?
positive negative2 negative gf ?
negative negative 4 negative bf ?
negative positiveTable 1: Rule Schema 1 & Rule Schema 3 (Deng and Wiebe, 2014)the table, s(?)
= ?
means that the writer?s sentiment toward ?
is ?, where ?
is a gfbf event, or the agentor theme of a gfbf event, and ?
is either positive or negative.
P?
Q means to infer Q from P.Applying the rules to Ex(1): the writer expresses a positive sentiment (?positive?)
toward a bf event(?lower?
), thus matching Case 3 in Table 1.
We infer that the writer is positive toward the agent (?re-form?)
and negative toward the theme (?costs?).
Two other rule schemas (not shown) make the sameinferences as Rule Schemas 1 and 3 but in the opposite direction.
As we can see, if two entities partic-ipate in a gf event, the writer has the same sentiment toward the agent and theme, while if two entitiesparticipate in a bf event, the writer has opposite sentiments toward them.
Later we use this observationin our experiments.4 Global Optimization FrameworkOptimization is performed over two sets of variables.
The first set is GFBF, containing a variable foreach gfbf event in the document.
The other set is Entity, containing a variable for each agent or themecandidate.
Each variable k in GFBF has its corresponding agent and theme variables, i and j, in Entity.The three form a triple unit, ?i, k, j?.
The set Triple consists of each ?i, k, j?, recording the correspon-dence between variables in GFBF and Entity.
The goal of the framework is to assign optimal labels tovariables in Entity and GFBF.
We first introduce how we recognize candidates for agents and themes,then introduce the optimization framework, and then define local scores that are input to the framework.4.1 Local Agents and Theme Candidates DetectorWe extract two agent candidates and two theme candidates for each gfbf event (one each will ultimatelybe chosen by the ILP model).4We use syntax, and the output of the SENNA (Collobert et al., 2011)semantic role labeling tool.
SENNA labels the A0 (subject), A1 (object), and A2 (indirect object) spansfor each predicate, if possible.
To extract the semantic agent candidate: If SENNA labels a span as A0of the gfbf event, we consider it as the semantic agent; if there is no A0 but A1 is labeled, we considerA1; if there is no A0 or A1 but A2 is labeled, we consider A2.
To extract the syntactic agent candidate,we find the nearest noun in front of the gfbf span, and then extract any other word that depends on thenoun according to the dependency parse.
Similarly, to extract the semantic theme candidate, we considerA1, A2, A0 in order.
To extract the syntactic theme candidate, the same procedure is conducted as forthe syntactic agent, but the nearest noun should be after the gfbf.
If there is no A0, A1 or A2, then thereis only one agent candidate, implicit and only one theme candidate, null.
We treat a null theme as anincorrect span in the later evaluations.
If the two agent (theme) candidate spans are the same, there isonly one candidate.4.2 Integer Linear Programming FrameworkWe use Integer Linear Programming (ILP) to assign labels to variables.
Variables in Entity will beassigned positive or negative, representing the writer?s sentiments toward them.
We may have two candi-date agents for a gfbf and that we will choose between them.
Thus, only one agent is assigned a positiveor negative label; the other is considered to be an incorrect agent of the gfbf (similarly for the theme can-didates).
Each variable in GFBF will be assigned the label gf or bf.
Optionally, it may also be assignedthe label reversed.
Label gf or bf is the polarity of the gfbf event; reversed is assigned if the polarity isreversed (e.g., for ?not harmed?, the labels are bf and reversed).The objective function of the ILP is:4This framework is able to handle any number of candidates.
The methods we tried using more candidates did not performas well - the gain in recall was offset by larger losses in precision.81minu1gf,u1bf...(?
1 ??i?GFBF?Entity?c?Lipicuic)+??i,k,j??Triple?ikj+??i,k,j?
?Triple?ikj(1)subject touic?
{0, 1}, ?i, c ?ikj, ?ikj?
{0, 1},?
?i, k, j?
?
Triple (2)where Liis the set of labels given to ?i ?
GFBF ?
Entity.
If i ?
GFBF, Liis {gf, bf, reversed} ({gf,bf, r}, for short).
If i ?
Entity, Liis {positive, negative} ({pos, neg}, for short).
uicis a binary in-dicator representing whether the label c is assigned to the variable i.
When an indicator variable is 1,the corresponding label is selected.
picis the score given by local detectors, introduced in the followingsections.
Variables ?ikjand ?ikjare binary slack variables that correspond to the gfbf implicature con-straints of ?i, k, j?.
When a given slack variable is 1, the corresponding triple violates the implicatureconstraints.
Minimizing the objective function could achieve two goals at the same time.
The first part(?1 ?
?i?cpicuic) tries to select a set of labels that maximize the scores given by the local detectors.The second part (?ikj?ikj+?ikj?ikj) aims at minimizing the cases where gfbf implicature constraintsare violated.
Here we do not force each triple to obey the implicature constraints, but to minimize theviolating cases.
For each variable, we have defined constraints:?c?LGFBF?ukc= 1, ?k ?
GFBF (3)?i?Entity?i,k,j?
?Triple?c?LEntityuic= 1,?k ?
GFBF (4)?j?Entity,?i,k,j?
?Triple?c?LEntityujc= 1,?k ?
GFBF (5)where LGFBF?in Equation (3) is a subset of LGFBF, consisting of {gf, bf}.
Equation (3) means agfbf must be either gf or bf.
But it is free to choose whether it is being reversed.
Recall that we have twoagent candidates (a1,a2) for a gfbf.
Thus we have four agent indicators in Equation (4): ua1,pos, ua1,neg,ua2,posand ua2,neg.
Equation (4) ensures that three of them are 0 and one of them is 1.
For instance,ua1,posassigned 1 means that candidate a1 is selected to be the agent span and pos is selected to be itspolarity.
In this way, the framework disambiguates the agent span and sentiment polarity simultaneously.
(Similar comments apply for the theme candidates in Equation (5).
)According to the implicature rules in Table 1 in Section 3, the writer has the same sentiment towardentities in a gf relation.
Thus, for each triple unit ?i, k, j?, the gf constraints are applied via the following:|?i,?i,k,j?ui,pos?
?j,?i,k,j?uj,pos|+ |uk,gf?
uk,r| <= 1 + ?ikj, ?k ?
GFBF (6)|?i,?i,k,j?ui,neg?
?j,?i,k,j?uj,neg|+ |uk,gf?
uk,r| <= 1 + ?ikj, ?k ?
GFBF (7)We use |uk,gf?
uk,r| to represent whether this triple is gf.
In Equation (6), if this value is 1, then thetriple should follow the gf constraints.
In that case, ?ikj= 0 means that the triple doesn?t violate thegf constraints, and |?iui,pos?
?juj,pos| must be 0.
Further, in this case,?iui,posand?juj,posareconstrained to be of the same value (both 1 or 0) ?
that is, entities i and j must be both positive or bothnot positive.
However, if ?ikj= 1, Equation (6) does not constrain the values of the variables at all.
If|uk,gf?
uk,r| is 0, representing that the triple is not gf, then Equation (6) does not constrain the valuesof the variables.
Similar comments apply to Equation (7).In contrast, the writer has opposite sentiments toward entities in a bf relation.|?i,?i,k,j?ui,pos+?j,?i,k,j?uj,pos?
1|+ |uk,bf?
uk,r| <= 1 + ?ikj, ?k ?
GFBF (8)|?i,?i,k,j?ui,neg+?j,?i,k,j?uj,neg?
1|+ |uk,bf?
uk,r| <= 1 + ?ikj,?k ?
GFBF (9)We use |uk,bf?
uk,r| to represent whether this triple is bf.
In Equation (8), if a triple is bf and theconstraints are not violated, then |?iui,pos+?juj,pos?
1| must be 0.
Further, in this case,?iui,pos82ugfubfur|ugf?
ur| |ubf?
ur| ugfubfur|ugf?
ur| |ubf?
ur|A 1 0 0 1 0 C 0 1 0 0 1B 0 1 1 1 0 D 1 0 1 0 1Table 2: Truth table of being reversed or not (k is omitted)and?juj,posare constrained to be of the opposite value ?
that is, if entity i is positive then entity j mustnot be positive.
Similar comments apply to Equation (9).Note that above we use |uk,gf?uk,r| and |uk,bf?uk,r| to represent whether a triple is gf or bf.
In Table2, we show that they always take opposite values and that they are consistent with the actual polarities.In Table 2, Case A means the triple is gf and Case B means the triple is bf but it is reversed.
In bothcases, |ugf?
ur| = 1, indicating that the triple should follow the gf constraints.
Similarly for Case Cand Case D to follow the bf constraints.4.3 Local GoodFor/BadFor Score: pk,gf, pk,bfWe utilize a sense-level gfbf lexicon by (Choi et al., 2014).
In total there are 6,622 gf senses and 3,290bf senses.
The gf lexicon covers 64% of the gf words in the corpus and the bf lexicon covers 42% of thebf words.
We then look up the gfbf span k in the gfbf lexicon.
If k only appears in the gf lexicon, thenpk,gf= 1 ?
 and pk,bf= .
Here  = 0.0001, to prevent there being any 0 scores in our computation.If k only appears in the bf lexicon, then pk,bf= 1 ?
 and pi,gf= .
If k appears in both the gf and bflexicon, and there are a senses in the gf lexicon and b senses in the bf lexicon, then pk,gf= a/(a + b)and pk,bf= b/(a + b).
If k is not in either lexicon, then pk,gf= pk,bf= .
If there is more than oneword in the gfbf span, we take the maximum score.4.4 Local Reversed Score: pk,rAs introduced in Section 3, a reverser changes the polarity of a gfbf.
First, we build reverser lexiconsfrom Wilson?s shifter lexicon (2008), namely the entries labeled as genshifter, negation, and shiftneg.We create two lexicons: one with the verbs and the other with the non-verb entries, excluding nouns,adjectives, and adverbs, since most non-verb reversers are prepositions or subordinating conjunctions.There are 219 reversers in the entire corpus; 134 (61.19%) are instances of words in one of the twolexicons.
Based on the lexicon, we categorize reversers into three classes.
Examples are shown below.Ex(2) They will not be able to water down your coverage.Ex(3) ... how a massive new bureaucracy will cut costs without hurting the old and the helpless.Ex(4) The new law includes new rules to prevent insurance companies from overcharging patients.Negation: An instance in this category is ?not?
in Ex(2).
If any word in the gfbf span has a negdependency relation according to the Stanford dependency parser, then we consider the gfbf to be negated(i.e., reversed).
In this case the path between the negator and the gfbf is labeled neg and the length of thepath is one.Other Non-Verb: This category consists of words such as ?without?
in Ex(3) (others are ?never?
and?few?, etc).
These words lower the extent of the gfbf event.
We look in the sentence for instances ofwords in the non-verb reverser lexicon, which are not tagged as noun, verb, adj, or adv.
For any found,we examine the path in the dependency parse between the potential reverser and the gfbf span.
If thepath has at least one of advmod, pcomp, cc, xcomp, nsubj, neg and the length of the path is less than four(learnt from development set), the event is considered to be reversed.Verb: In Ex(4), the verb ?prevent?
stops the gfbf event ?overcharging?
from happening.
We call suchwords Verb reverser (others are ?prohibit?
and ?ban?, etc).
We look in the sentence for instances of wordsin the verb reverser lexicon.
For any that appear before the gfbf span in the sentence, if the path has atleast one of xcomp, pcomp, obj and the length of the path is less than four, then the event is reversed.For the triple ?companies, overcharging, patients?
in Ex(4), though it is reversed by ?prevent?, the agentof the reverser, which is ?law?, is different from the agent of the gfbf, which is ?companies?, so the bf83within the ?overcharging?
event is not reversed.5Though we extract the Verb reversers to evaluate theperformance of recognizing a reverser, in the optimization framework, gfbf events with Verb reversersare not considered to be reversed, since almost all Verb reversers introduce new agents.Different from other scores, pk,rcould be negative.
According to the heuristics above, the probabilityof a gfbf event being reversed decreases as the length of the path increases.
We define pk,rso it isinversely proportional to the length of the path.
Further, to make sense of a gfbf triple ?agent, gfbf,theme?, where, e.g., the local detectors label it ?pos, bf, pos?, the framework is choosing the smallerone from (a) ?1 ?
pk,r?
uk,r(it has a reverser) versus (b) 1 ?
?ikj(it is an exception to the rules).
Theframework assigns uk,r= 0 and ?ikj= 1 if ?1 ?
pk,r> 1.
It assigns uk,r= 1 and ?ikj= 0 if?1 ?
pk,r<= 1.
For gfbf events which have Negation or Other Non-verb reversers, since we use thelength four as a threshold in the heuristics above, we define pk,r=1d?54, so that ?1 ?
pk,r=54?1d> 1if d > 4.
For gfbf events for which no reverser word appears in the sentence, or those which only haveVerb reversers, pk,r= ?1 ?54(so ?1 ?
pk,r> 1), so that the framework chooses case (b) (choosing thegfbf event to be not reversed).4.5 Local Sentiment Score: pi,pos, pi,negIn the corpus of DCW, only the writer?s sentiments toward the agents and the themes of gfbf events areannotated.
Thus, since there are many false negatives of sentiments toward entities, the corpus doesnot support training a classifier.
Therefore, we adopt the same local sentiment detector from (Dengand Wiebe, 2014), using available resources to detect writer?s sentiments toward all agent and themecandidates.6The sentiment scores range from 0.5 to 1.5 Co-reference In the FrameworkSo far the constraints in the framework are within a gfbf triple.
Consider the following example:Ex(5) The reform will decrease the healthcare costs and improve the medical qualify as expected.The two gfbfs, ?decrease?
and ?improve?
have the same agent, ?reform?.
Thus, if there is more thanone gfbf in a sentence, and the path between the two gfbfs in dependency parse contains only conj orxcomp, and there is no other noun between the latter gfbf and the conjunction, we assume the two agentsare the same and the sentiments toward them should be the same.
Thus, for any i, j ?
Entity, if i, jco-refer7, or they are the same agent as described above, Coref(i, j) = 1 (otherwise 0).
We add twomore constraints, similar to the gf constraints in Equations (6) and (7), as shown in Equation (10) and(11).
where ?ijis a slack variable, e(i) is the set of agent/theme candidates linked to the same gfbf as iis.
If Coref(i, j) = 0, Equations (10) and (11) do not constrain the variables.
The objective function inEquation (12) is updated to incorporate these new constraints.|?e(i)ui,pos?
?e(j)uj,pos|+ Coref(i, j) <= 1 + ?ij,?i, j ?
Entity (10)|?e(i)ui,neg?
?e(j)uj,neg|+ Coref(i, j) <= 1 + ?ij, ?i, j ?
Entity (11)minu1gf,u1bf...(?
1 ??i?GFBF?Entity?c?Lipicuic)+??i,k,j??Triple?ikj+??i,k,j?
?Triple?ikj+?i,j?Entity?ij(12)6 Experiment and PerformanceIn this section we introduce the data we use, the baseline methods, the evaluations and the results.
Inaddition, we give examples illustrating how opinion inference may improve performances.5DCW defines here is a triple chain: ?law, prevent ?companies, overcharging, patients??.
The reverser is changing thepolarity between ?law?
and ?patients?, but it does not change the polarity between ?companies?
and ?patients?.6We use Opinion Extractor (Johansson and Moschitti, 2013) , opinionFinder (Wilson et al., 2005), MPQA subjectivitylexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966) and a connotation lexicon (Feng et al., 2013), to detectwriter?s sentiments toward all agent and theme candidates, and all gfbf events.
We adopt Rule 1 and Rule 3 to infer from thesentiment toward event to the sentiment toward theme.
Then we conduct a majority voting based on the results.7We use the co-reference resolution system from (Stoyanov et al., 2010).846.1 Experiment DataWe use the ?Affordable Care Act?
corpus of DCW, consisting of 134 online editorials and blogs.
In total,there are 1,762 annotated triples, out of which 692 are gf or retainers and 1,070 are bf or reversers.
Fromthe writer?s perspective, 1,495 noun phrases are annotated positive, 1,114 noun phrases are negativeand the remaining 8 are neutral.
This indicates that there are many opinions in the corpus.
Out of 134documents in the corpus, 3 do not have any annotation.
6 are used as a development set to develop theheuristics in Sections 4 and 5.
We use the remaining 125 for the experiments.6.2 Baseline Methods and Evaluation MetricsWe compare the output of the global optimization framework with the outputs of baseline systems builtfrom the local detectors in Section 4.
For the gfbf polarity and reverser ambiguities, the local detectorsdirectly provide a disambiguation result.
For the agent/theme span and sentiment ambiguities, the localsentiment detector assigns positive and negative scores to each candidate.
The framework chooses amongthe combined options.
Thus, for comparison, we build a baseline system that combines the outputs ofthe local agent/theme candidate detector and the local sentiment detector.Recall from Section 4, a variable k ?
GFBF has two agent candidates, a1 and a2 ?
Entity.
Togetherthere are four binary indicator variables: ua1,pos, ua1,neg, ua2,posand ua2,neg.
Among these indicatorvariables whose corresponding local scores (e.g., pa1,posis the score of ua1,pos) are larger than 0.5,the baseline system (denoted Local) chooses the one with the largest local sentiment score.
If there isa tie, it prefers the variable representing the semantic candidate.
If there is still a tie, it chooses thevariable representing the majority polarity (positive).
If all the local scores of the four variables are0.5 (neutral), Local fails to recognize any sentiment for that entity, so it assigns 0 to all the indicatorvariables.
Local+coref takes the maximum local score of the entities if they co-ref, and assigns eachentity the maximum score before disambiguation.Another baseline, Majority, always chooses the semantic candidate and the majority polarity.To evaluate the performance in detecting sentiment, we use precision, recall, and F-measure.
We donot take into account any agent or theme manually annotated as neutral (there are only 8).P =#(auto=gold & gold!=neutral)#auto!=neutralAccuracy = R =#(auto=gold & gold!=neutral)#gold!=neutralF =2*P*RP+R(13)In the equations, auto is the system?s output and gold is the gold-standard label from annotations.
Sincewe don?t take into account any neutral agent or theme, #gold!=neutral equals to all nodes in the exper-iment set.
Thus accuracy is equal to recall.
We only report recall here.
Here we have two definitionsof auto=gold: (1) Strict evaluation means that, by saying auto=gold, the agent/theme must have thesame polarity and must be the same NP as the gold standard, and (2) Relaxed evaluation means theagent/theme has the same polarity as the gold standard, regardless whether the span is correct or not.Note that according to DCW, an implicit agent isn?t annotated with any sentiment.
Thus, for animplicit agent in gold, if auto outputs the span ?implicit?, we treat it as a correct span with correctpolarity, regardless what sentiment auto gives to it.
If auto outputs any span other than ?implicit?, wetreat it as a wrong span with wrong polarity, regardless of its sentiment as well.
For the theme span, ifauto outputs a ?null?
theme candidate, we treat it as a wrong span but we evaluate its sentiment accordingto gold.To evaluate extracting candidate span, we use accuracy.
The baseline for this task always chooses thesemantic candidate.
To evaluate gfbf polarity and reverser, we also use accuracy.Note that although we evaluate the performance in different tasks separately, the framework resolvesall the ambiguities at the same time.6.3 ResultsWe report the performance results for (A) sentiment detection in Table 3, on two sets.
One is the subsetcontaining the agents and themes where auto has the correct spans with gold.
The other is the set ofall agents and themes.
As shown in Table 3, ILP significantly improves performance, approximately10-20 points on F-measure over different baselines.
Though Local has a competitive precision with85correct span subset whole set, strict eval whole set, relaxed evalP R F P R F P R F1 ILP 0.6421 0.6421 0.6421 0.4401 0.4401 0.4401 0.5939 0.5939 0.59392 Local 0.6409 0.3332 0.4384 0.4956 0.2891 0.3652 0.5983 0.3490 0.44083 ILP+coref 0.6945 0.6945 0.6945 0.4660 0.4660 0.4660 0.6471 0.6471 0.64714 Local+coref 0.6575 0.3631 0.4678 0.5025 0.3103 0.3836 0.6210 0.3834 0.47415 Majority 0.5792 0.5792 0.5792 0.3862 0.3862 0.3862 0.5462 0.5462 0.5462Table 3: Performances of sentiment detectionILP, it has a much lower recall.
That means the local sentiment detector cannot recognize implicitsentiments toward most entities.
But ILP is able to recognize more entities correctly.
By adding coref,performance improves for both ILP and Local.
In comparison to (Deng and Wiebe, 2014), our currentmethod improves more in F-measure (2.43 points more) over local sentiment detector than the earlierwork, even though the earlier work takes the manual annotations of all the gfbf information as input.In terms of the other tasks: For (B) agent/theme span, the baseline achieves 66.67% in accuracy, com-pared to 68.54% and 67.10% for ILP and ILP+coref, respectively.
For (C) gfbf polarity, the baselinehas an accuracy of 70.68%, whereas ILP achieves 77.25% and ILP+coref achieves 77.47%, respectively,both 7 points higher.
This improvement is interesting because it represents cases in which the optimiza-tion framework is able to infer the correct polarity even though the gfbf span is not recognized by thelocal detector (i.e., the span isn?t in the gfbf lexicon).
For (D) reverser, the baseline is 88.07% in accu-racy.
ILP and ILP+coref are competitive with the baseline: 89% and 88.07% respectively.
Note that bothour local detector and ILP surpass the majority class (not reversed) which has an accuracy of 86.60%.Following (Akkaya et al., 2009), since ILP is unsupervised without multiple runs, we adopt McNe-mar?s test to measure statistical significance of our improvements (Dietterich, 1998).
In Table 3, theimprovements in recalls of Line 1 over 2, Line 3 over 4, and Lines 1&3 over 5 are statistically significantat the p < .001 level.
The improvements of Line 3 over 1 are statistically significant at the p < .005level.
For accuracy of gfbf polarity, the improvement is significant at the p < .001 level.6.4 ExamplesThis sections gives simplified examples to illustrate how the framework can improve over the localdetectors.
The explicit sentiment clues referred to in this section are from MPQA lexicon.Ex(6) The reform would curb skyrocketing costs in the long run.The local sentiment detector assigns ?costs?
negative due to the single sentiment clue, ?skyrocketing?.Since the agent and theme are in a bf triple, and the writer is negative toward that theme, we can inferthe writer is positive toward the agent.
This illustrates how we improve recall on sentiments.Ex(7) The supposedly costly reform will curb skyrocketing costs in the long run.In Ex(7), agent ?reform?
is labeled negative because ?costly?
is a negative clue in the lexicon.
(?sup-posedly?
is not in it.)
However, in Ex(7), it is actually positive.
The agent?s negative score is 0.6, andits positive score is 0.5 due to the absence of a positive clue.
Since the theme is negative too, by the bfconstraints, we expect to see a positive agent.
If we were to assign negative to the agent, the objectivefunction would have -0.6 subjectivity score and +1 in violation penalty, together giving +0.4.
If we as-sign positive, the subjectivity score is -0.5, and there is no violation, resulting in a total score of -0.5.Thus, the framework correctly chooses the positive label.
This shows how we can improve precision onsentiments.Ex(8) The great reform will curb skyrocketing costs in the long run.In this case, the agent is positive and the theme is negative.
If the gfbf word ?curb?
is not in the lexicon,we could still infer its polarity.
Given that the entities in the triple have different sentiments, to not violate86the implicature rules, the framework will assign it bf, or assign it gf along with reversed.
However, thereis no reverser word in the sentence, so the reversed score pr= ?54.
The framework will assign thereverser indicator ur= 0, in order to avoid a gain in the objective function by ?1 ?
pr?
ur.
Thusthe framework assigns the label bf to ?curb?.
This is how the framework can improve the accuracy ofrecognizing gfbf polarity.7 ConclusionThe ultimate goal of this work is to utilize gfbf information to improve detection of the writer?ssentiments toward entities mentioned in the text.
Using an unsupervised optimization framework thatincorporates gfbf implicature rules as constraints, our method improves over local sentiment recognitionby almost 20 points in F-measure and over all sentiment baselines by over 10 points in F-measure.
Theglobal optimization framework jointly infers the polarity of gfbf events, whether or not they are reversed,which candidate NPs are the agent and theme, and the writer?s sentiments toward them.
In additionto beating the baselines for sentiment detection, the framework significantly improves the accuracy ofgfbf polarity disambiguation.
This work not only automatically utilizes gfbf information to improvesentiment detection, it also proposes a framework for jointly solving various ambiguities related to gfbfevents.Acknowledgement This work was supported in part by DARPA-BAA-12-47 DEFT grant.
We wouldlike to thank the anonymous reviewers for their helpful feedback.ReferencesCem Akkaya, Janyce Wiebe, and Rada Mihalcea.
2009.
Subjectivity word sense disambiguation.
In Proceedingsof the 2009 Conference on Empirical Methods in Natural Language Processing: Volume 1 - Volume 1, EMNLP?09, pages 190?199, Stroudsburg, PA, USA.
Association for Computational Linguistics.Pranav Anand and Kevin Reschke.
2010.
Verb classes as evaluativity functor classes.
In Interdisciplinary Work-shop on Verbs.
The Identification and Representation of Verb Features.Yejin Choi and Claire Cardie.
2008.
Learning with compositional semantics as structural inference for subsen-tential sentiment analysis.
In Proceedings of the 2008 Conference on Empirical Methods in Natural LanguageProcessing, pages 793?801, Honolulu, Hawaii, October.
Association for Computational Linguistics.Yejin Choi, Eric Breck, and Claire Cardie.
2006.
Joint extraction of entities and relations for opinion recognition.In Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing, EMNLP ?06,pages 431?439, Stroudsburg, PA, USA.
Association for Computational Linguistics.Yoonjung Choi, Janyce Wiebe, and Lingjia Deng.
2014.
Lexical acquisition for opinion inference: A sense-levellexicon of benefactive and malefactive events.
In 5th Workshop on Computational Approaches to Subjectivity,Sentiment & Social Media Analysis.Ronan Collobert, Jason Weston, L?eon Bottou, Michael Karlen, Koray Kavukcuoglu, and Pavel Kuksa.
2011.Natural language processing (almost) from scratch.
J. Mach.
Learn.
Res., 12:2493?2537, November.Dipanjan Das, Andr?e FT Martins, and Noah A Smith.
2012.
An exact dual decomposition algorithm for shallowsemantic parsing with constraints.
In Proceedings of the First Joint Conference on Lexical and ComputationalSemantics-Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedingsof the Sixth International Workshop on Semantic Evaluation, pages 209?217.
Association for ComputationalLinguistics.Lingjia Deng and Janyce Wiebe.
2014.
Sentiment propagation via implicature constraints.
In Meeting of theEuropean Chapter of the Association for Computational Linguistics (EACL-2014).Lingjia Deng, Yoonjung Choi, and Janyce Wiebe.
2013.
Benefactive/malefactive event and writer attitude anno-tation.
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2:Short Papers), pages 120?125, Sofia, Bulgaria, August.
Association for Computational Linguistics.87Pascal Denis and Jason Baldridge.
2007.
Joint determination of anaphoricity and coreference resolution usinginteger programming.
In Human Language Technologies 2007: The Conference of the North American Chap-ter of the Association for Computational Linguistics; Proceedings of the Main Conference, pages 236?243,Rochester, New York, April.
Association for Computational Linguistics.Thomas G. Dietterich.
1998.
Approximate statistical tests for comparing supervised classification learning algo-rithms.
Neural Computation, 10:1895?1923.Song Feng, Jun Sak Kang, Polina Kuznetsova, and Yejin Choi.
2013.
Connotation lexicon: A dash of sentimentbeneath the surface meaning.
In Proceedings of the 51th Annual Meeting of the Association for ComputationalLinguistics (Volume 2: Short Papers), Sofia, Bulgaria, Angust.
Association for Computational Linguistics.Amit Goyal, Ellen Riloff, and Hal Daum III.
2013.
A computational model for plot units.
ComputationalIntelligence, 29(3):466?488.Herbert Paul Grice.
1967.
Logic and conversation.
The William James lectures.Herbert Paul Grice.
1989.
Studies in the Way of Words.
Harvard University Press.Richard Johansson and Alessandro Moschitti.
2013.
Relational features in fine-grained opinion analysis.
Compu-tational Linguistics, 39(3).Andr?e F. T. Martins and Noah a. Smith.
2009.
Summarization with a joint model for sentence extraction andcompression.
In Proceedings of the Workshop on Integer Linear Programming for Natural Langauge Processing- ILP ?09, pages 1?9, Morristown, NJ, USA.
Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and Dav Zimak.
2004.
Semantic role labeling via integer linearprogramming inference.
In Proceedings of the 20th international conference on Computational Linguistics,page 1346.
Association for Computational Linguistics.Vasin Punyakanok, Dan Roth, and Wen-tau Yih.
2008.
The importance of syntactic parsing and inference insemantic role labeling.
Computational Linguistics, 34(2):257?287.Kevin Reschke and Pranav Anand.
2011.
Extracting contextual evaluativity.
In Proceedings of the Ninth Interna-tional Conference on Computational Semantics, IWCS ?11, pages 370?374, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Dan Roth and Wen-tau Yih.
2004.
A linear programming formulation for global inference in natural languagetasks.
In CONLL.Swapna Somasundaran and Janyce Wiebe.
2009.
Recognizing stances in online debates.
In Proceedings of theJoint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on NaturalLanguage Processing of the AFNLP, pages 226?234, Suntec, Singapore, August.
Association for ComputationalLinguistics.P.J.
Stone, D.C. Dunphy, M.S.
Smith, and D.M.
Ogilvie.
1966.
The General Inquirer: A Computer Approach toContent Analysis.
MIT Press, Cambridge.Veselin Stoyanov, Claire Cardie, Nathan Gilbert, Ellen Riloff, David Buttler, and David Hysom.
2010.
Corefer-ence resolution with reconcile.
In Proceedings of the ACL 2010 Conference Short Papers, ACLShort ?10, pages156?161, Stroudsburg, PA, USA.
Association for Computational Linguistics.Janyce Wiebe and Lingjia Deng.
2014.
An account of opinion implicatures.
arXiv:1404.6491v1 [cs.CL].Janyce Wiebe, Theresa Wilson, and Claire Cardie.
2005.
Annotating expressions of opinions and emotions inlanguage ann.
Language Resources and Evaluation, 39(2/3):164?210.Theresa Wilson, Janyce Wiebe, , and Paul Hoffmann.
2005.
Recognizing contextual polarity in phrase-levelsentiment analysis.
In HLP/EMNLP, pages 347?354.Theresa Wilson.
2008.
Fine-grained subjectivity analysis.
Ph.D. thesis, Doctoral Dissertation, University ofPittsburgh.Bishan Yang and Claire Cardie.
2013.
Joint Inference for Fine-grained Opinion Extraction.
In Proceedings ofACL, pages 1640?1649.Lei Zhang and Bing Liu.
2011.
Identifying noun product features that imply opinions.
In Proceedings of the49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pages575?580, Portland, Oregon, USA, June.
Association for Computational Linguistics.88
