Proceedings of the Ninth Workshop on Innovative Use of NLP for Building Educational Applications , pages 54?60,Baltimore, Maryland USA, June 26, 2014.c?2014 Association for Computational LinguisticsSurprisal as a Predictor of Essay QualityGaurav KharkwalDepartment of PsychologyCenter for Cognitive ScienceRutgers University, New Brunswickgaurav.kharkwal@gmail.comSmaranda MuresanDepartment of Computer ScienceCenter for Computational Learning SystemsColumbia Universitysmara@ccls.columbia.eduAbstractModern automated essay scoring systemsrely on identifying linguistically-relevantfeatures to estimate essay quality.
Thispaper attempts to bridge work in psy-cholinguistics and natural language pro-cessing by proposing sentence process-ing complexity as a feature for automatedessay scoring, in the context of Englishas a Foreign Language (EFL).
To quan-tify processing complexity we used a psy-cholinguistic model called surprisal the-ory.
First, we investigated whether es-says?
average surprisal values decreasewith EFL training.
Preliminary resultsseem to support this idea.
Second, we in-vestigated whether surprisal can be effec-tive as a predictor of essay quality.
Theresults indicate an inverse correlation be-tween surprisal and essay scores.
Overall,the results are promising and warrant fur-ther investigation on the usability of sur-prisal for essay scoring.1 IntroductionStandardized testing continues to be an integralpart of modern-day education, and an importantarea of research in educational technologies is thedevelopment of tools and methodologies to facil-itate automated evaluation of standardized tests.Unlike multiple-choice questions, automated eval-uation of essays presents a particular challenge.The specific issue is the identification of a suitableevaluation rubric that can encompass the broadrange of responses that may be received.Unsurprisingly then, much emphasis has beenplaced on the development of Automated EssayScoring (henceforth, AES) systems.
Notable AESsystems include Project Essay Grade (Page, 1966;Ajay et al., 1973), ETS?s e-raterR?
(Burstein et al.,1998; Attali and Burstein, 2006), Intelligent Es-say AssessorTM(Landauer et al., 2003), BETSY(Rudner and Liang, 2002), and Vantage Learn-ing?s IntelliMetricTM(Elliot, 2003).
The commonthread in most modern AES systems is the iden-tification of various observable linguistic features,and the development of computational models thatcombine those features for essay evaluation.One aspect of an essay?s quality that almost allAES systems do not yet fully capture is sentenceprocessing complexity.
The ability to clearly andconcisely convey information without requiringundue effort on the part of the reader is one hall-mark of good writing.
Decades of behavioral re-search on language comprehension has suggestedthat some sentence structures are harder to com-prehend than others.
For example, passive sen-tences, such as the girl was pushed by the boy,are known to be harder to process than semanti-cally equivalent active sentences, such as the boypushed the girl (Slobin, 1966; Forster and Ol-brei, 1972; Davison and Lutz, 1985; Kharkwal andStromswold, 2013).
Thus, it is likely that the over-all processing complexity of the sentence struc-tures used in an essay could influence its perceivedquality.One reason why sentence processing complex-ity has not yet been fully utilized is the lackof a suitable way of quantifying it.
This paperproposes the use of a psycholinguistic model ofsentence comprehension called surprisal theory(Hale, 2001; Levy, 2008) to quantify sentence pro-cessing complexity.
The rest of the paper is orga-nized as follows.
Section 2 describes the surprisaltheory, and discusses its applicability in modelingsentence processing complexity.
Section 3 detailsour investigation on whether essays?
average sur-prisal values decrease following English as a For-eign Language training.
Section 4 presents a studywhere we investigated whether surprisal can be ef-fective as a predictor of essay quality.
Lastly, Sec-54The judge who angered the criminal slammed the gavel Mean5.64 6.94 6.93 11.60 2.32 9.19 16.92 1.94 4.68 7.35The judge who the criminal angered slammed the gavel Mean5.64 6.94 6.93 4.20 9.21 13.73 16.65 2.21 4.69 7.80Table 1: Surprisal values of two example relative-clause sentences.
The values were computed using atop-down parser by Roark et al.
(2009) trained on the Wall Street Journal corpus.tion 5 concludes the paper.2 Surprisal TheoryThe surprisal theory (Hale, 2001; Levy, 2008)estimates the word-level processing complexityas the negative log-probability of a word giventhe preceding context (usually, preceding syntac-tic context).
That is:Complexity(wi) ?
?
logP (wi|w1...i?1, CONTEXT)Essentially, the surprisal model measures pro-cessing complexity at a word as a function ofhow unexpected the word is in its context.
Sur-prisal is minimized (i.e.
approaches zero) when aword must appear in a given context (i.e., whenP (wi|w1...i?1, CONTEXT) = 1), and approachesinfinity as a word becomes less and less likely.Crucially, the surprisal theory differs from n-grambased approaches by using an underlying languagemodel which includes a lexicon and a syntacticgrammar (the language model is usually a Prob-abilistic Context-Free Grammar, but not restrictedto it).To better understand surprisal, consider the fol-lowing two example sentences:(1) The judge who angered the criminal slammedthe gavel.
(2) The judge who the criminal angered slammedthe gavel.Both sentences are center-embedded relativeclause sentences that differ in whether the subjector the object is extracted from the relative clause.Critically, they both share the same words differ-ing only in their relative order.
Behavioral stud-ies have found that object-extracted relative clausesentences (2) are harder to process than subject-extracted relative clause sentences (1) (King andJust, 1991; Gordon et al., 2001; Grodner andGibson, 2005; Staub, 2010; Traxler et al., 2002;Stromswold et al., 1996).
The surprisal values ateach word position of the two example sentencesare shown in Table 1.As we can see from Table 1, the mean surprisalvalue is greater for the object-extracted relativeclause sentence.
Hence, the surprisal theory cor-rectly predicts greater processing cost for that sen-tence.
Furthermore, it allows for a finer-grainedanalysis of where the processing cost might occur,specifically at the onset of the relative clause (the)and the end (angered).
Other differences, such asgreatest difficulty at the main verb are shared withthe subject-extracted relative clause, and are plau-sible because both sentences are center-embedded.These predictions are consistent with patterns ob-served in behavioral studies (Staub, 2010).In addition to relative clauses, the surprisal the-ory has been used to model various other behav-ioral findings (Levy, 2008; Levy and Keller, 2012).Moreover, corpora analyses examining surprisal?seffectiveness revealed a high correlation betweenword-level suprisal values and the correspondingreading times, which act as a proxy for processingdifficulties (Demberg and Keller, 2008; Boston etal., 2008; Frank, 2009; Roark et al., 2009).Thus, the surprisal theory presents itself as aneffective means of quantifying processing com-plexity of sentences, and words within them.
Next,we discuss a series of evaluations that we per-formed to determine whether surprisal values re-flect quality of written essays.3 Experiment 1In the first experiment, we investigate whether anessay?s mean surprisal value decreases after suit-able English as a Foreign Language (EFL) educa-tional training.
Here, we make the assumption thatEFL training improves a person?s overall writingquality, and that surprisal value acts as a proxy forwriting quality.55Total Syntactic LexicalTopic Term Mean SD Mean SD Mean SDAnalysisTerm 1 6.34 3.32 2.37 1.86 3.97 3.24Term 2 6.28 3.30 2.34 1.85 3.94 3.23Arg.Term 1 6.24 3.29 2.34 1.85 3.90 3.23Term 2 6.15 3.36 2.28 1.85 3.87 3.24Table 2: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal forAnalysis and Argumentation essays3.1 CorpusWe used the Uppsala Student English corpus pro-vided by the Department of English at UppsalaUniversity (Axelsson, 2000).
The corpus con-tained 1,489 essays written by 440 Swedish uni-versity students of English at three different lev-els.
The total number of words was 1,221,265,and the average length of an essay was 820 words.The essays were written on a broad range of top-ics, and their lengths were limited to be between700-800 words.
The topics were divided based onstudent education level, with 5 essay topics writtenby first-term students, 8 by second-term students,and 1 by third-term students.To facilitate comparison, we chose similar top-ics from the first and second-term sets.
We thushad two sets of essays.
The first set consistedof Analysis essays which are written as a causalanalysis of some topic, such as ?television and itsimpact on people.?
The second set consisted ofArgumentation essays where students argue for oragainst a topic or viewpoint.
We further imposedthe restriction that only essays written by the samestudent in both terms were selected.
That is, if astudent wrote an essay on a chosen topic in the firstterm, but not the second, or vice-versa, their essaywas not considered.
This selection resulted in 38pairs of Analysis essays and 20 pairs of Argumen-tation essays across the two terms, for a total of116 essays.3.2 Computing SurprisalWe computed the surprisal value of each wordin an essay by using a broad-coverage top-downparser developed by Roark et al.
(2009).
Theparser was trained on sections 02-24 of the WallStreet Journal corpus of the Penn Treebank (Mar-cus et al., 1993).
Essentially, the parser com-putes a word?s surprisal value as the negative log-probability of the word given the preceding wordsusing prefix probabilities.
Thus, the surprisalvalue of the ithword is calculated as:SURPRISAL(wi) = ?
logPrefixProb(w1...i)PrefixProb(w1...i?1)Moreover, it decomposes each word?s surprisalvalue into two components: syntactic surprisaland lexical surprisal.
Syntactic surprisal measuresthe degree of unexpectedness of the part-of-speechcategory of a word given the word?s sentential con-text.
On the other hand, lexical surprisal measuresthe degree of unexpectedness of the word itselfgiven its sentential context and a part-of-speechcategory.For every essay, we measured the syntactic, lex-ical, and total (i.e., summed) surprisal values foreach word.
Subsequently, the averages of the threesurprisal values were computed for every essay,and those means were used for further analyses.Henceforth, surprisal values for an essay refers totheir mean surprisal values.3.3 Results and DiscussionTable 2 reports the means and standard deviationsof the three surprisal measures of the essays.1Ascan be seen, there seems to be a reduction in allthree surprisal values across terms, and secondterm essays tend to have a lower mean surprisalthan first term essays.
To analyze these differ-ences, we computed linear mixed-effect regressionmodels (Baayen, 2008; Baayen et al., 2008) for thetwo essay categories.
Each model included Termas a fixed factor and Student as a random intercept.While our analysis shows that essays in the sec-ond term have an overall mean surprisal valuesless than than essays in the first term, these differ-ences were not statistically significant.
There are anumber of factors that could have influenced theseresults.
We made an assumption that only a singleterm of EFL training could significantly improve1It is important to note here that these means and standarddeviations are computed on mean surprisal values per essaysand not surprisal values at individual words.56Total Syntactic LexicalScore Mean SD Mean SD Mean SDLow 6.22 0.39 2.46 0.22 3.76 0.29Medium 6.10 0.34 2.35 0.17 3.75 0.26High 6.09 0.28 2.27 0.14 3.82 0.24Table 3: Means and standard deviations of total surprisal, syntactic surprisal, and lexical surprisal for thethree different essay score levelsessay quality, and hence decrease overall surprisalvalues of essays.
However, it is likely that a sin-gle term of training is insufficient, and perhaps thelack of a significant difference between surprisalvalues reflects no improvement in essay qualityacross the two terms.
Unfortunately, these essayswere not previously scored, and thus we were un-able to assess whether essay quality improved overterms.4 Experiment 2In the second experiment, we directly examinedwhether surprisal values are related to essay qual-ity by using a dataset of pre-scored essays.4.1 CorpusFor this experiment, we used a corpus of essayswritten by non-native English speakers.
These es-says are a part of the Educational Testing Service?scorpus which was used in the first shared task inNative Language Identification (Blanchard et al.,2013)2.The corpus consisted of 12,100 essays, with atotal number of 4,142,162 words, and the averagelength of an essay was 342 words.
The essayswere on 8 separate topics, which broadly askedstudents to argue for or against a topic or a view-point.
Each essay was labeled with an English lan-guage proficiency level (High, Medium, or Low)based on the judgments of human assessment spe-cialists.
The distribution of the essays per score-category was: Low = 1,325; Medium = 6,533; andHigh = 4,172.
In order to ensure an equitable com-parison, and to balance each group, we decided tochoose 1,325 essays per score-category, for a totalof 3,975 essays.4.2 Computing SurprisalAs in Experiment 1, for every essay we measuredthe syntactic, lexical, and total surprisal values foreach word.
We computed the averages of the three2Copyrightc?
2014 ETS.
www.ets.orgsurprisal values, and used those means for furtheranalysis.4.3 Results and DiscussionTable 3 reports the means and standard deviationsof the three surprisal values for every essay perscore-category.
We analyzed the differences be-tween the means using linear mixed-effects regres-sion models (Baayen, 2008; Baayen et al., 2008).Essay Score was treated as a fixed effect and Es-say Topic was included as a random intercept.
Theresults indicate that Low-scoring essays had a sig-nificantly greater mean total surprisal value thanMedium or High-scoring essays.
However, the dif-ference in mean total surprisal values for Mediumand High-scoring essays was not significant.
Onthe other hand, for syntactic and lexical surprisal,the means for all three essay score levels were sig-nificantly different from one another.We further evaluated the three surprisal valuesby performing a correlation test between them andthe essay scores.
Table 4 reports the output of thecorrelation tests.
All three surprisal values werefound to be significantly inversely correlated withessay scores.
However, only syntactic surprisalobtained a correlation coefficient of a sufficientlylarge magnitude of 0.39.A similar evaluation was performed by Attaliand Burstein (2006) in their evaluation of thefeatures used in ETS?s e-rater system.
Interest-ingly, the magnitude of the correlation coefficientfor syntactic surprisal reported here is within therange of coefficients corresponding to e-rater?sfeatures when they were correlated with TOEFLessay scores (see Attali and Burstein, 2006, Table2).
Granted, a direct comparison between coef-ficients is not recommended as the datasets usedwere different, such a finding is still promising.Overall, the results shed a positive light on the useof surprisal, specifically syntactic surprisal, as afeature for automated essay scoring.Despite the promising pattern of our results,57Dep Var ?
t-value p-valueTotal -.15 -9.87 < .001Syntactic -.39 -26.53 < .001Lexical .08 5.35 < .001Table 4: Pearson?s R coefficients between the three surprisal values and the essay scoresthey must be taken with a grain of salt.
The datasetthat we used did not contain the actual scores ofthe essays, and we had to work with broad classi-fications of essay scores into Low, Medium, andHigh score levels.
A possible avenue of futurework is to test whether these results hold when us-ing finer-grain essays scores.5 Conclusions and Future WorkWe proposed the use of the surprisal theory toquantify sentence processing complexity for useas a feature in essay scoring.
The results are en-couraging, and warrant further evaluation of sur-prisal?s effectiveness in determining essay quality.One point of concern is that the relationshipbetween mean surprisal values and essay scoresis likely to vary depending on the general qual-ity of the essays.
Here, we used a corpus of es-says written by non-native English speakers, andas such, these essays are bound to be of a loweroverall quality than essays written by native En-glish speakers.
For example, consider the fol-lowing, somewhat questionable, sentences chosenfrom the subset of essays having a High score:(3) Some people might think that traveling in agroup led by a tour guide is a good way.
(4) This is possible only if person understandsideas and concept.
(5) It is an important decision, how to plan yoursyllabus.These examples suggest that even high-scoringessays written by non-native English speakers maynot necessarily be flawless, and as such, gram-matical acceptability may play a crucial role indetermining their overall quality.
Therefore, itis possible that for lower-quality essays, highsurprisal values reflect the presence of gram-matical errors.
On the other hand, for better-written essays, moderate-to-high surprisal valuesmay reflect structural variability, which arguablyis preferable to monotonous essays with simplersentence structures.
Thus, it is likely that the re-lation between surprisal values and essay scoresdepends on the overall quality of the essays ingeneral.
For an equitable evaluation, further testswill need to determine surprisal?s efficacy over abroader range of essays.Another critical point is the choice of corpusused to compute surprisal.
Whatever choice ismade essentially dictates and constrains the gram-mar of the language under consideration.
Here, weused the WSJ corpus and, thus, implicitly made anassumption about the underlying language model.Therefore, in our case, a good essay, i.e.
one witha lower surprisal score, would be one which isstylistically closer to the WSJ corpus.
Future workwill need to investigate the role played by the un-derlying language model, with special emphasison evaluating language models that are specific tothe task at hand.
In other words, it would be in-teresting to compare a surprisal model that is builtusing a collection of previous essays with a sur-prisal model that uses a broader language model.Lastly, our evaluations were aimed at determin-ing whether surprisal can be an effective predictorof essay quality.
Further tests will need to evaluatehow well the measure contributes to essay scorepredictions when compared to related approachesthat rely on non-syntactic language models, suchas n-grams.
Moreover, future work will need todetermine whether adding mean surprisal values toan AES system results in a performance improve-ment.AcknowledgmentsWe are indebted to ETS for sharing their data withus, and supporting us through this project.
Thiswork would not be possible without their help.
Weare also thankful to the reviewers for their help-ful and encouraging comments.
The opinions setforth in this publication are those of the author(s)and not ETS.ReferencesHelen B. Ajay, P. I. Tillett, and Ellis B.
Page.
1973.Analysis of essays by computer (AEC-II).
Final58Report to the National Center for Educational Re-search and Development for Project, (8-0101).Yigal Attali and Jill Burstein.
2006.
Automated essayscoring with e-raterR?
v. 2.
The Journal of Technol-ogy, Learning and Assessment, 4(3).Margareta W. Axelsson.
2000.
USE ?
the Uppsala Stu-dent English corpus: An instrument for needs anal-ysis.
ICAME Journal, 24:155?157.Harald R. Baayen, Douglas J. Davidson, and Dou-glas M. Bates.
2008.
Mixed-effects modeling withcrossed random effects for subjects and items.
Jour-nal of memory and language, 59(4):390?412.Harald R. Baayen.
2008.
Analyzing linguistic data:A practical introduction to statistics using R. Cam-bridge University Press.Daniel Blanchard, Joel Tetreault, Derrick Higgins,Aoife Cahill, and Martin Chodorow.
2013.TOEFL11: A corpus of non-native english.
Edu-cational Testing Service.Marisa Boston, John Hale, Reinhold Kliegl, UmeshPatil, and Shravan Vasishth.
2008.
Parsing costs aspredictors of reading difficulty: An evaluation usingthe potsdam sentence corpus.
Journal of Eye Move-ment Research, 2(1):1?12.Jill Burstein, Karen Kukich, Susanne Wolff, Chi Lu,and Martin Chodorow.
1998.
Enriching automatedessay scoring using discourse marking.
In Proceed-ings of the Workshop on Discourse Relations andDiscourse Marking, pages 206?210.Alice Davison and Richard Lutz.
1985.
Measuringsyntactic complexity relative to discourse context.In David R. Dowty, Lauri Karttunen, and Arnold M.Zwicky, editors, Natural language parsing: Psy-chological, computational, and theoretical perspec-tives, pages 26?66.
Cambridge: Cambridge Univer-sity Press.Vera Demberg and Frank Keller.
2008.
Data from eye-tracking corpora as evidence for theories of syntacticprocessing complexity.
Cognition, 109:193?210.Scott Elliot, 2003.
Automated essay scoring: a crossdisciplinary approach, chapter IntelliMetric: Fromhere to validity, pages 71?86.
Lawrence ErlbaumAssociates, Mahwah, NJ.Kenneth Forster and Ilmar Olbrei.
1972.
Seman-tic heuristics and syntactic analysis.
Cognition,2(3):319?347.Stefan L Frank.
2009.
Surprisal-based comparison be-tween a symbolic and a connectionist model of sen-tence processing.
In Proceedings of the 31st annualconference of the cognitive science society, pages1139?1144.
Cognitive Science Society Austin, TX.Peter C. Gordon, Randall Hendrick, and Marcus John-son.
2001.
Memory interference during languageprocessing.
Journal of Experimental Psychology:Learning, Memory and Cognition, 27:1411?1423.Daniel Grodner and Edward Gibson.
2005.
Conse-quences of the serial nature of linguistic input forsentenial complexity.
Cognitive Science, 29(2):261?290.John Hale.
2001.
A probabilistic Earley parser asa psycholinguistic model.
In Proceedings of the2nd Conference of the North American Chapter ofthe Association for Computational Linguistics, vol-ume 2, pages 159?166, Pittsburgh, PA.Gaurav Kharkwal and Karin Stromswold.
2013.Good-enough language processing: Evidence fromsentence-video matching.
Journal of psycholinguis-tic research, 43(1):1?17.Jonathan King and Marcel A.
Just.
1991.
Individ-ual differences in sentence processing: The role ofworking memory.
Journal of Memory and Lan-guage, 30:580?602.Thomas K. Landauer, Darrell Laham, and Peter W.Foltz, 2003.
Automated essay scoring: a cross dis-ciplinary approach, chapter Automated scoring andannotation of essays with the Iintelligent Essay As-sessor, pages 87?112.
Lawrence Erlbaum Asso-ciates, Mahwah, NJ.Roger Levy and Frank Keller.
2012.
Expectationand locality effects in german verb-final structures.Journal of Memory and Language.Roger Levy.
2008.
Expectation-based syntactic com-prehension.
Cognition, 106(3):1126?1177.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of English: The Penn Treebank.
Com-putational linguistics, 19(2):313?330.Ellis B.
Page.
1966.
The imminence of grading essaysby computer.
Phi Delta Kappan, 47(5):238?243.Brian Roark, Asaf Bachrach, Carlos Cardenas, andChristophe Pallier.
2009.
Deriving lexical andsyntactic expectation-based measures for psycholin-guistic modeling via incremental top-down parsing.In Proceedings of the 2009 Conference on EmpiricalMethods in Natural Language Processing: Volume1-Volume 1, pages 324?333.
Association for Com-putational Linguistics.Lawrence M. Rudner and Tahung Liang.
2002.
Au-tomated essay scoring using Bayes?
theorem.
TheJournal of Technology, Learning and Assessment,1(2).Dan Slobin.
1966.
Grammatical transformationsand sentence comprehension in childhood and adult-hood.
Journal of Verbal Learning and Verbal Be-havior, 5(3):219?227.59Adrian Staub.
2010.
Eye movements and process-ing difficulty in object relative clauses.
Cognition,116(1):71?86.Karin Stromswold, David Caplan, Nathaniel Alpert,and Scott Rauch.
1996.
Localization of syntac-tic comprehension by position emission tomogra-phy.
Brain and Language, 52:452?473.Matthew J. Traxler, Robin K. Morris, and Rachel E.Seely.
2002.
Processing subject and object relativeclauses: Evidence from eye movements.
Journal ofMemory and Language, 47:69?90.60
