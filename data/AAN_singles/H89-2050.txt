Contextually-Based Data-Derived PronunciationNetworks for Automatic Speech Recognition*Francine R. ChenXEROX PALO ALTO I:~ESEARCH CENTER3333 Coyote Hill RoadPalo Alto, CA 94304AbstractThe context in which a phoneme occurs leads to consistent differences in how it is pronounced.
Phonologistsemploy a variety of contextual descriptors, based on factors such as stress and syllable boundaries, to explainphonological variation.
However, in developing pronunciation networks for speech recognition systems, littleexplicit use is made of context other than the use of whole word models and use of triphone models.This paper describes the creation of pronunciation etworks using a wide variety of contextual factorswhich allow better prediction of pronunciation variation.
We use a phoneme level representation whichpermits easy addition of new words to the vocabulary, with a flexible context representation which allowsmodeling of long-range ffects, extending over syllables and across word-boundaries.
In order to incorporatea wide variety of factors in the creation of pronunciation networks, we used data-derived context rees, whichpossess properties useful for pronunciation network creation.IntroductionThe context in which a phoneme occurs leads to consistent differences in how it is pronounced.
Pho-nologists employ a variety of contextual descriptors to explain phonological variation.
These descriptorsare theoretically motivated by studies of different languages and are comprised of many factors, such asstress and syllable part.
However, in current speech recognition systems, only a few contextual descriptorsare employed when developing pronunciation etworks.
In these systems, generally the effects of only thepreceding and following sounds, as in triphone models, or implicit within-word contextual effects, as in wholeword models, are modeled.Context-dependent triphone models have been found to be better models than context-free phone models,or monophones, and are used now by many recognition systems (e.g.
Chow, 1986).
More recently, Lee (1988,1989) introduced the idea of clustered triphones, in which triphones exhibiting similar coarticulatory behaviorare grouped together.
Clustered triphones require less training data because there are fewer models, resultingin better training of the models which have been defined.Paul (1988) has conducted studies comparing the recognition rates of whole word models and triphonemodels.
In his studies, he found that whole word models provided somewhat better recognition rates overtriphone models, and that triphone models have much better recognition rates than monophone models.However, in these studies, context across word boundaries was not modeled.
In a subsequent study, Paul(1989) found that use of triphone models which modeled context across word boundaries provided signifi-cantly better recognition rates over word-internal triphone models.
Extrapolating, one would expect evenbetter performance with whole word models which also model contextual effects across word boundaries.However, the amount of data necessary to train such a model for a moderate size vocabulary could be pro-hibitive.
Furthermore, addition of a new word to the vocabulary would require many new tokens of the wordbecause at least one token of the word in each context in which it could appear would be required.Instead of words, the use of a smaller representational unit, such as phones, with an enriched set ofcontextual descriptors can provide models which capture many of the contextual effects that whole word*This work was sponsored in part by the Defense Advanced Research Projects Agency (DOD), under the Information Scienceand Technology Office, contract #N00140-86-C-8996.374contextual factor valuespreceding phonemefollowing phonemepreceding phonefollowing phonesyllable partstresssyllable boundary typefoot boundary typeword boundary typecluster typeopen syllable?true vowel?function word?
(all phonemes) + sentence boundary(all phonemes) + sentence boundary(all phones) + deletion + sentence boundary(all phones) + deletion + sentence boundaryonset, nucleus, codaprimary, secondary, unstressedinitial, final, internal, initial-and-finalinitial, final, internal, initial-and-finalinitial, final, internal, initial-and-finalonset, coda, niltrue, falsetrue, falsetrue, falseTable 1: Contexts used in pronunciation experimentsmodels capture.
In addition, phone models have a smaller training data requirement and provide a moregeneral framework for adding new words to the vocabulary.Because more contextual effects can be captured by using a wide variety of factors, use of these factorswhen creating pronunciation networks from dictionary baseforms allows better predictions of pronunciationvariants.
Better modeling of observed phonological variation can result in better performance of speechrecognition systems.
A case in point is the work by Weintraub et al (1989) who found that phonologicalmodeling improved their recognition results.
In their work, the phonological rules were derived by hand.This paper describes the creation of pronunciation networks using a wide variety of contextual factors.
Wepropose a systematic methodology for creating pronunciation networks which can capture the predominantcontextual effects using a representation at the phoneme level.
With the use of a phoneme representation,new words can be added without the need for additional training data, as would be necessary in whole wordmodels.
Our representation also allows us to capture long-range ffects.
In this study, some factors extendover syllables and across word boundaries.Contextua l  Factors  Represented  in  Context  T reesLinguists describe the context of a phoneme using many types of theoretically motivated factors, suchas stress and word boundary.
Each contextual factor describing the context of a phoneme has a value.
Forexample, the factor stress may take on any one value of primary, secondary, or unstressed.In this work, we describe the context of a phoneme using a set of linguistically motivated contextualfactors.
These factors and their corresponding values are listed in Table 1.
Some of the factors are structuresnormally associated with several phonemes.
For example, the factor syllable part may take on the valueonset, which may be composed of up to three phonemes, as in the sequence/str/ .
In such cases, we assignthe factor value to each phoneme within the structure.
In our example, the exemplars/s / ,  t / ,  and / r / inan /s t r / sequence  would each be assigned a syllable part value of onset.
This representation allows modelingof long-range contextual effects simultaneously with a local phoneme representation, which simplifies theaddition of new words to a recognition vocabulary.If the context of a phoneme is described by simultaneously using all the contextual factors listed inTable 1, a prohibitive amount of data would be required to form an adequate description of each phonemein each context.
Each contextual factor represents a separate dimension, and with such a large number ofdimensions, the distribution of phonemes in a context will be sparse.
One way to handle this difficulty isto build a "context ree," in which a subset of contextual factors is selected using a greedy algorithm which375word-boundary - typei n i t ia~,i.ntte,r'na ~word-boundary - typef~na~118treSs ' I",,,,~ s t re  ss \] ~m.g'trease,~Figure 1: An illustrative context tree for some phoneme Xminimizes the loss of information at each selection.
A combination of tree induction for selecting factors andclustering for grouping factor values is used to create a context tree.
The number of leaves in the tree andbranching of the tree is determined by the data.
In the next paragraph we give a brief description of contexttrees.
A more detailed description of context trees and how they are created is given in Chen and Shrager(1989).
An alternate method for grouping contextual factors, based on binary splitting of subspaces until apreset number of clusters is formed, is given by Sagayama (1989).A context ree is an n-ary decision tree which models the relationship between contextual factors and theallophones which occur in different contexts.
We create context rees from a set of training exemplars usingdata derived from the hand-transcribed "sx" sentences of the T IM IT  database (Lamel, et al, 1986; Fisheret al, 1987).
An illustrative tree describing the distribution of allophones in context for some phoneme X isshown in Figure 1.
The nodes of a context ree represent the values of a particular contextual factor.
In theexample, node 1 corresponds to the contextual factor word-boundary-type with the value initial or internal.Each leaf of a context ree encodes the distribution of allophones in each context.
In general, more than oneallophone occurs in a context because phoneme realizations are not deterministic.
For example, leaf 2 of theexample corresponds to the realizations of X, which is realized as the allophone y 70% and as z 30% of thetime when in a word-final and either primary or secondary stress context.The representation used in context rees permits flexible modeling of contextual effects.
Since the contexttrees are derived from data in which phonemes are described by a set of contextual factors, long-rangecontextual effects are modeled.
Also, since each factor represents a separate dimension, a factor ignoresstructures which are irrelevant o it.
Thus, a contextual factor such as preceding phoneme xtends acrossword boundaries.P ronunc ia t ion  Network  Creat ionThis section describes a systematic method for creating pronunciation etworks in which a wide varietyof contextual factors are used.
In addition to using more contextual factors, our method of network creationhas a number of inherent advantages, uch as the ability to estimate allophone distributions from partialcontextual descriptions.
This method is data-intensive, using the data to determine possible pronunciationsas well as to estimate the probabilities associated with each pronunciation.Mapp ing  D ic t ionary  P ronunc ia t ionsDictionary pronunciations of words are relatively coarse-grained in that they do not indicate allophonicvariation.
Phone-based speech recognition systems generally represent words using allophones rather than376dictionary baseforms because the allophones of a dictionary "phoneme" may be very different, as measuredby the acoustic similarity metrics commonly employed in recognition systems.
The allophonically-basedpronunciations are usually represented compactly in a pronunciation etwork.
In this section, we describethe creation of pronunciation etworks from dictionary baseforms.
The networks are produced by mappingthe dictionary pronunciation of a word into an allophonic representation specified by the context rees.
Thedictionary that we use was developed at Xerox PARC and is called the X-Dictionary t.The mapping from a dictionary baseform to a set of possible pronunciations i characterized by thesubstitution, deletion, and insertion of sounds.
Each dictionary sound may be realized as an allophone orit may be deleted.
Therefore, substitution and deletion of a dictionary phoneme may be treated identicallyin mapping a dictionary baseform into a network of allophones.
A context tree, which we call a "phoneme"tree, is used to describe the observed substitutions and deletions of a dictionary phoneme in transcriptionsof speech.
One phoneme tree is created for each of the 45 dictionary phonemes and the data in each treedefines the set of allophones observed in each context for a dictionary phoneme.In addition to modeling substitutions and deletions, as the phoneme trees do, pronunciation etworkcreation also requires modeling of insertions.
Insertions do not fit the substitution/deletion model in whicha "phoneme" is realized as an allophone.
Instead, insertions may occur between any pair of "phonemes."
Inaddition, one must also model when insertions do not occur so that the probability of an insertion in anycontext can be predicted.
These requirements are met by representing all insertions and non-insertions inone tree in which the contextual factors are redefined to be a set applicable to insertions.
The contextualfactors describing insertions do not describe the transformation of an underlying phoneme.
Instead, thefactors describe the context of the phonemes adjacent o where an insertion can occur.
Contextual factorsdescribing insertions are derived from factors describing the context of a phoneme by replacing each factorwith ones describing the context of adjacent phonemes.
For example, the factor stress is replaced withstress-of-preceding-phoneme and stress-of-following-phoneme.
Since the new factors describe the context ofadjacent phonemes, the value sentence-boundary is added to the allowable values of each factor to indicatethe beginning or end of a sentence.
In organizing the data to build an "insertion" tree, all pairs of phonemesin the training data are checked for whether or not an insertion occurred between them.
If so, the context andinsertion is noted; if not, the context and the fact that no insertion occurred is also noted.
The "insertion"tree predicts when insertions can occur as well as what type of insertion can occur in a particular context.Network Creat ionNetworks are created word by word and can be joined to produce a pronunciation etwork for a recognitionsystem.
Networks created using our method explicitly model cross-word-boundary contextual effects.
Ifthe context at the word boundaries is unknown, the possible allophones and corresponding probabilities areenumerated for each context value.
Alternatively, if the context of the word is specified, only the allophonesfor the word boundary context are used.
Since the context of the word is known, word-boundary phonemescan be treated the same as word-internal phonemes.To create a word network, a two-pass process is used.
First, each dictionary "phoneme" in a word ismapped to the allophone distribution represented by the leaf in a phoneme tree corresponding to the contextin which the phoneme occurs, producing a sequence of allophones representing the sequence of phonemes (seeFigure 2a).
The context of a leaf in a phoneme tree is described by the contextual factor values encounteredin traversing a phoneme tree from the root node to the leaf.
Contextual constraints associated with theallophones from a leaf are matched to contextual constraints of adjacent allophones.
If the phoneme is wordinitial or word final and the context at the word boundaries is not specified, then the allophones for eachcontext must be incorporated into the network.
Insertions are then added between the leaf values whenthe context for an insertion is compatible.
Insertions are added after substitutions and deletions because1"The X-Dictionary has been checked for consistency and has been augmented from entries in standard ictionaries to includefoot boundary indicators.377a)n .
94o------~-~-q-o~;si'?
;n .
94c) Af  1 .0n .94  tc l  .25_eh  1.0 ~en .06~v i  .0~ s 1.0 _Figure 2: Pronunciation etwork for "fence": a) initial arcs b) arcs connected c) insertions added.the context in which an insertion occurs is dependent upon adjacent phones, which is determined by thephoneme realizations.Using our method based on context rees, the pronunciation etwork produced for the word "fence" isshown in Figure 2.
In creating this network, we made the simplifying assumption that we would not usethe contextual factors describing adjacent phones for modeling substitutions and deletions.
This producesthe simple network in Figure 2b in which each set of arcs from a leaf node are connected at the beginningand end.
Addition of insertions, in which we do include the contextual factors describing adjacent phones,produces the network shown in Figure 2c.In creating this network, we also assumed that the word was spoken in isolation and therefore precededand followed by silence.
Had we not specified a context, the boundaries of the word would be much morebushy with additional arcs representing the different possible allophones and probabilities in various contexts.For example, an /s /  in word-final position is more likely to be palatalized and pronounced as a \[~\] whenfollowed by a /y /o r  a /~/ ,  as in "fence your" or "fence should," than when followed by a vowel, as in "fenceany."
When the context of a word is not specified, possible palatalization is modeled through the additionof two arcs which require that the following phoneme can cause palatalization, such as /y /o r /g / .
One arcrepresents the \[s\] allophone and the other arc represents the \[g\] allophone; the probability of the two arcssum to 1.0.
The original Is\] arc remains untouched with a probability of 1.0 and now has a constraint onits following context prohibiting any following phoneme which can cause palatalization o f / s / .
When allword boundary contexts are listed, unobserved cross-word-boundary contexts may be handled by includinga "default" in which context is ignored.
That is, the "default" context is composed of all observed allophonesacross all contexts.
A more detailed estimate, based on a partial context specification, requires actual ookupin a context ree.Because of limited training data, some of the words to be represented may contain a context value whichhas not been observed in the training data.
However, each node of the tree contains the distribution ofallophones for the partial context represented by the node.
Thus, the allophones for unobserved contextscan be estimated from a partial context specification by tracing down the tree as far as consistent with theobserved contextual factor values describing a phonemic baseform.P roper t ies  o f  Context  T reesThe context rees possess properties which are useful for producing pronunciation networks.
As noted inthe previous ection, allophone distributions in unobserved contexts may be easily estimated from a partial378context specification by tracing down the tree until no context matches.
Other properties of the contexttrees permit ease of context combination, specification of natural groups for tying (Jelinek and Mercer,1980) in hidden Markov models (HMM), representation f allophone probabilities, and systematic reductionof network size.In the tree-induction method, the leaves of the tree, by definition, represent mutually exclusive contexts.This simplifies the comparison and combination of contexts during network creation.
For example, findinga set of arcs with a compatible context is simplified if one can assume that once a matching constraint isfound, one need not look any further.In HMMs, tying is used to constrain one probability in a network to be equal to another, the underlyingidea being that equivalent items should be assigned the same probability.
Each leaf of a phoneme treerepresents all the allophones of that phoneme which have been observed in a particular context.
Thuseach leaf is a natural group for tying.
In a network representation i  which the labels are on the arcs, theprobability assigned to an arc should be tied to all other arcs with the same label from the same leaf.Many rule sets have been developed to describe phonological variation.
However, by using a data-intensiveapproach, allophone probabilities in a context may be directly estimated.
Furthermore, counts of allophonesin context can be used to reduce the size of pronunciation etworks by removing unlikely allophones.The probabilities in a context tree can be further refined if the network is used in an HMM which istrained.
The probabilities provide a good initial estimate, and more importantly, the absence of unlikelyallophones in the network allow more robust training to be performed.In the creation of pronunciation etworks, it is hard to define an "optimum" number of pronunciationsto represent.
With only a few pronunciations, recognition performance may not be optimal because themodeling of pronunciation variation in words is left to the front-end.
With many pronunciations, recognizerperformance may be poor because the amount of training data is sparse and unlikely pronunciations mayconfuse the recognizer.
This is the problem described by SRI as the problem of "many additional parametersto be estimated with the same amount of training data" (Weintraub et al, 1989).
SRI uses measures ofcoverage and overcoverage and accordingly modifies by hand the phonological rules they use.With context trees, this problem can be handled at a phoneme level.
Given a large data set, contexttrees tend to overgenerate pronunciations because ach new allophonic realization of a phoneme in a contexttranslates into another possible arc in a network.
But because context trees contain count information onallophones in context, pruning can be used in a systematic way to remove the more unlikely pronunciations,thus reducing the number of pronunciations in a network.
To remove unlikely allophones in a given context,pruning is performed on the allophone distributions within a leaf.
Pruning can be based upon counts ofan allophone in a leaf or upon the percentage of exemplars of an allophone in a leaf.
In the first case,allophones are removed if they are based on only a few exemplars.
In the second case, unlikely allophonesare removed.
In each case, the arcs representing the removed allophones are not created.
In addition toreducing the number of pronunciations, pruning may also result jn more robust predictions.
For example,in a given context, one may observe just a couple exemplars of an allophone in several hundred realizations.These allophones may be due to transcription error, and so it is judicious to remove them.SummaryThis paper describes a systematic, data-intensive method for creating pronunciation networks.
A phonemerepresentation with an enriched set of contextual descriptors i advocated for providing a general frameworkin which new words may be easily added to the vocabulary.
A wide variety of factors is used to model con-textual effects, including long-range and cross word boundary phenomena.
The large number of dimensionsentailed by a greater number of contextual descriptors is handled through the use of context trees for pre-dicting allophonic variation.
Context trees were shown to possess attributes, such as the ability to estimatedistributions from partial contexts and the capacity to systematically reduce the size of a network based onthe tree data, that make the trees a good representation from which to create pronunciation etworks.379ReferencesF.
Chen and \].
Shrager, "Automatic discovery of contextual rules describing phonological variation," Proc.DARPA Speech and Natural Language Workshop, pp.
284-289, Feb. 1989.Y.
Chow, R. Schwartz, S. Roucos, O. Kimball, P. Price, R. Kubala, M. Dunham, M. Krasner, and J.Makhoul, "The role of word-dependent coarticulatory effects in a phoneme-based speech recognitionsystem," Proc.
ICASSP, pp.
1593-1596, 1986.W.
Fisher, V. Zue, J. Bernstein, D. Pallett, "An acoustic-phonetic data base," J. Acoust.
Soc.
Am., Suppl.1, vol.
81, 1987.F.
\]elinek and R. Mercer, "Interpolated estimation of markov source parameters from sparse data," Proc.Pattern Recognition i  Practice Workshop, E. Gelsema nd L. Kanal, eds., North-Holland, 1980.L.
Lamel, R. Kassel, S. Seneff, "Speech database development: design and analysis of the acoustic-phoneticcorpus," Proc.
DARPA Speech Recognition Workshop, L. Baumann, ed., pp.
100-109, 1986.K.-F. Lee, Large-Vocabulary Speaker-Independent Conlinuous Speech Recognition: The SPHINX System,Doctoral Dissertation, Carnegie Mellon University, Pittsburgh, PA, April 1988.K-F. Lee, H-W. Hon, M-Y.
Hwang, S. Mahajan, R. Reddy, "The SPHINX speech recognition system,"Proc.
ICASSP, pp.
445-448, 1989.D.
Paul and E. Martin, "Speaker stress-resistant continuous speech recognizer," Proc.
ICASSP, pp.
283-286, 1988.D.
Paul, "The Lincoln robust continuous speech recognizer," Proc.
ICASSP, pp.
449-452, 1989.S.
Sagayama, "Phoneme nvironment clustering for speech recognition," Proc.
ICASSP, pp.397-400, 1989.M.
Weintraub, H. Murveit, M. Cohen, P. Price, J. Bernstein, G. Baldwin, and D. Bell, "Linguistic con-straints in hidden markov model based speech recognition," Proc.
ICASSP, pp.
699-702, 1989.380
