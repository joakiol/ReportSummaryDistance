Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 984?992,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsDetecting Emails Containing Requests for ActionAndrew Lampert ??
?CSIRO ICT CentrePO Box 76Epping 1710Australiaandrew.lampert@csiro.auRobert Dale?Centre for Language TechnologyMacquarie University 2109Australiardale@science.mq.edu.auCecile ParisCSIRO ICT CentrePO Box 76Epping 1710Australiacecile.paris@csiro.auAbstractAutomatically finding email messages thatcontain requests for action can provide valu-able assistance to users who otherwise strug-gle to give appropriate attention to the ac-tionable tasks in their inbox.
As a speechact classification task, however, automaticallyrecognising requests in free text is particularlychallenging.
The problem is compounded bythe fact that typical emails contain extrane-ous material that makes it difficult to isolatethe content that is directed to the recipient ofthe email message.
In this paper, we reporton an email classification system which iden-tifies messages containing requests; we thenshow how, by segmenting the content of emailmessages into different functional zones andthen considering only content in a small num-ber of message zones when detecting requests,we can improve the accuracy of message-levelautomated request classification to 83.76%, arelative increase of 15.9%.
This representsan error reduction of 41% compared with thesame request classifier deployed without emailzoning.1 IntroductionThe variety of linguistic forms that can be usedto express requests, and in particular the frequencywith which indirect speech acts are used in email, isa major source of difficulty in determining whetheran email message contains one or more requests.Another significant problem arises from the fact thatwhether or not a request is directed at the recipient ofthe email message depends on where in the messagethe request is found.
Most obviously, if the request ispart of a replied-to message that is contained withinthe current message, then it is perhaps more likelythat this request was directed at the sender of thecurrent message.
However, separating out contentintended for the recipient from other extraneous con-tent is not as simple as it might appear.
Segmentingemail messages into their different functional partsis hampered by the lack of standard syntax used bydifferent email clients to indicate different messageparts, and by the ad hoc ways in which people varythe structure and layout of messages.In this paper, we present our results in classifyingmessages according to whether or not they containrequests, and then show how a separate classifierthat aims to determine the nature of the zones thatmake up an email message can improve upon theseresults.
Section 2 contains some context and moti-vation for this work before we briefly review rele-vant related work in Section 3.
Then, in Section 4,we describe a first experiment in request classifica-tion using data gathered from a manual annotationexperiment.
In analysing the errors made by thisclassifier, we found that a significant number of er-rors seemed to arise from the inclusion of content inparts of a message (e.g., quoted reply content) thatwere not authored by the current sender, and thuswere not relevant other than as context for interpret-ing the current message content.
Based on this anal-ysis, we hypothesised that segmenting messages intotheir different functional parts, which we call emailzones, and then using this information to consideronly content from certain parts of a message for re-quest classification, would improve request classifi-984cation performance.To test this hypothesis, we developed an SVM-based automated email zone classifier configuredwith graphic, orthographic and lexical features; thisis described in more detail in (Lampert et al, 2009).Section 5 describes how we improve request classi-fication performance using this email zone classifier.Section 6 summarises the performance of our re-quest classifiers, with and without automated emailzoning, along with an analysis of the contribution oflexical features to request classification, discussionof request classification learning curves, and a de-tailed error analysis that explores the sources of re-quest classification errors.
Finally, in Section 7, weoffer pointers to future work and some concludingremarks.2 Background and MotivationPrevious research has established that users rou-tinely use email for managing requests in the work-place ?
e.g., (Mackay, 1988; Ducheneaut and Bel-lotti, 2001).
Such studies have highlighted howmanaging multiple ongoing tasks through emailleads to information overload (Whittaker and Sid-ner, 1996; Bellotti et al, 2003), especially in theface of an ever-increasing volume of email.
Theresult is that many users have difficulty giving ap-propriate attention to requests hidden in their emailwhich require action or response.
A particularly lu-cid summary of the requirements placed on emailusers comes from work by Murray (1991), whoseethnographic research into the use of electronic mes-saging at IBM highlighted that:[Managers] would like to be able to trackoutstanding promises they have made,promises made to them, requests they?vemade that have not been met and requestsmade of them that they have not fulfilled.This electronic exchange of requests and commit-ments has previously been identified as a fundamen-tal basis of the way work is delegated and com-pleted within organisations.
Winograd and Floreswere among the first to recognise and attempt toexploit this with their Coordinator system (Wino-grad and Flores, 1986).
Their research into organ-isational communication concluded that ?Organisa-tions exist as networks of directives and commis-sives?.
It is on this basis that our research exploresthe use of requests (directive speech acts) and com-mitments (commissive speech acts) in email.
In thispaper, we focus on requests; feedback from usersof the request and commitment classifier plug-in forMicrosoft Outlook that we have under developmentsuggests that, at least within the business context ofour current users, requests are the more important ofthe two phenomena.Our aim is to create tools that assist email usersto identify and manage requests contained in incom-ing and outgoing email.
We define a request as anutterance that places an obligation on an email re-cipient to schedule an action; perform (or not per-form) an action; or to respond with some speechact.
A simple example might be Please call whenyou have a chance.
A more complicated request isDavid will send you the latest version if there havebeen any updates.
If David (perhaps cc?ed) is a re-cipient of an email containing this second utterance,the utterance functions as a (conditional) request forhim, even though it is addressed as a commitment toa third-party.
In real-world email, requests are fre-quently expressed in such subtle ways, as we discussin Section 4.A distinction can be drawn between message-level identification?i.e., the task of determiningwhether an email message contains a request ?and utterance-level identification?i.e., determin-ing precisely where and how the request is ex-pressed.
In this paper, we focus on the task ofmessage-level identification, since utterance-levelidentification is a significantly more problematictask: it is often the case that, while we might agreethat a message contains a request or commitment,it is much harder to determine the precise extent ofthe text that conveys this request (see (Lampert etal., 2008b) for a detailed discussion of some of theissues here).3 Related WorkOur request classification work builds on influentialideas proposed by Winograd and Flores (1986) intaking a language/action perspective and identifyingspeech acts in email.
While this differs from the ap-proach of most currently-used email systems, which985routinely treat the content of email messages as ho-mogeneous bags-of-words, there is a growing bodyof research applying ideas from Speech Act Theory(Austin, 1962; Searle, 1969) to analyse and enhanceemail communication.Khosravi and Wilks (1999) were among the firstto automate message-level request classification inemail.
They used cue-phrase based rules to clas-sify three classes of requests: Request-Action,Request-Information and Request-Permission.
Un-fortunately, their approach was quite brittle, with therules being very specific to the computer support do-main from which their email data was drawn.Cohen, Carvalho and Mitchell (2004) developedmachine learning-based classifiers for a number ofemail speech acts.
They performed manual emailzoning, but didn?t explore the contribution this madeto the performance of their various speech act clas-sifiers.
For requests, they report peak F-measure of0.69 against a majority class baseline accuracy ofapproximately 66%.
Cohen, Carvalho and Mitchellfound that unweighted bigrams were particularlyuseful features in their experiments, out-performingother features applied.
They later applied a series oftext normalisations and n-gram feature selection al-gorithms to improve performance (Carvalho and Co-hen, 2006).
We apply similar normalisations in ourwork.
While difficult to compare due to the use of adifferent email corpus that may or may not excludeannotation disagreements, our request classifier per-formance exceeds that of the enhanced classifier re-ported in (Carvalho and Cohen, 2006).Goldstein and Sabin (2006) have also worked onrelated email classification tasks.
They use verbclasses, along with a series of hand-crafted form-and phrase-based features, for classifying what theyterm email genre, a task which overlaps signifi-cantly with email speech act classification.
Theirresults are difficult to compare since they include amix of form-based classifications like response withmore intent-based classes such as request.
For re-quests, the results are rather poor, with precision ofonly 0.43 on a small set of personal mail.The SmartMail system (Corston-Oliver et al,2004) is probably the most mature previous workon utterance-level request classification.
SmartMailattempted to automatically extract and reformulateaction items from email messages for the purpose ofadding them to a user?s to-do list.
The system em-ployed a series of deep linguistic features, includingphrase structure and semantic features, along withword and part-of-speech n-gram features.
The au-thors found that word n-grams were highly predic-tive for their classification task, and that there waslittle difference in performance when the more ex-pensive deep linguistic features were added.
Basedon this insight, our own system does not employdeeper linguistic features.
Unfortunately, the re-sults reported reveal only the aggregate performanceacross all classes, which involves a mix of bothform-based classes (such as signature content ad-dress lines and URL lines), and intent-based classes(such as requests and promises).
It is thus very dif-ficult to directly compare the results with our sys-tem.
Additionally, the experiments were performedover a large corpus of messages that are not avail-able for use by other researchers.
In contrast, weuse messages from the widely-available Enron emailcorpus (Klimt and Yang, 2004) for our own experi-ments.While several of the above systems involve man-ual processes for removing particular parts of mes-sage bodies, none employ a comprehensive, auto-mated approach to email zoning.We focus on the combination of email zoningand request classification tasks and provide detailsof how email zoning improves request classification?
a task not previously explored.
To do so, we re-quire an automated email zone classifier.
We exper-imented with using the Jangada system (Carvalhoand Cohen, 2004), but found similar shortcomingsto those noted by Estival et al (2007).
In particular,Jangada did not accurately identify forwarded or re-ply content in email messages from the email Enroncorpus that we use.
We achieved much better perfor-mance with our own Zebra zone classifier (Lampertet al, 2009); it is this system that we use for emailzoning throughout this paper.4 Email Request ClassificationIdentifying requests requires interpretation of the in-tent that lies behind the language used.
Given this, itis natural to approach the problem as one of speechact identification.
In Speech Act Theory, speechacts are categories like assertion and request that986capture the intentions underlying surface utterances,providing abstractions across the wide variety of dif-ferent ways in which instances of those categoriesmight be realised in linguistic form.
In this paperwe focus on the speech acts that represent requests,where people are placing obligations upon others viaactionable content within email messages.The task of building automated classifiers is dif-ficult since the function of conveying a request doesnot neatly map to a particular set of language forms;requests often involve what are referred to as indi-rect speech acts.
While investigating particular sur-face forms of language is relatively unproblematic,it is widely recognised that ?investigating a collec-tion of forms that represent, for example, a partic-ular speech act leads to the problem of establish-ing which forms constitute that collection?
(Archeret al, 2008).
Email offers particular challenges asit has been shown to exhibit a higher frequency ofindirect speech acts than other media (Hassell andChristensen, 1996).
We approach the problem bygathering judgments from human annotators and us-ing this data to train supervised machine learning al-gorithms.Our request classifier works at the message-level,marking emails as requests if they contain one ormore request utterances.
As noted earlier, we definea request as an utterance from the email sender thatplaces an obligation on a recipient to schedule anaction (e.g., add to a calendar or task list), performan action, or respond.
Requests may be conditionalor unconditional in terms of the obligation they im-pose on the recipient.
Conditional requests requireaction only if a stated condition is satisfied.
Previousannotation experiments have shown that conditionalrequests are an important phenomena and occur fre-quently in email (Scerri et al, 2008; Lampert et al,2008a).
Requests may also be phrased as either adirect or indirect speech act.Although some linguists distinguish betweenspeech acts that require a physical response andthose that require a verbal or information response,e.g., (Sinclair and Coulthard, 1975), we followSearle?s approach and make no such distinction.
Wethus consider questions requiring an informationalresponse to be requests, since they place an obliga-tion on the recipient to answer.1Additionally, there are some classes of requestwhich have been the source of systematic humandisagreement in our previous annotation experi-ments.
One such class consists of requests forinaction.
Requests for inaction, sometimes calledprohibitives (Sadock and Zwicky, 1985), prohibitaction or request negated action.
An example is:Please don?t let anyone else use the computer in theoffice.
As they impose an obligation on the sender,we consider requests for inaction to be requests.Similarly, we consider that meeting announcements(e.g., Today?s Prebid Meeting will take place inEB32c2 at 3pm) and requests to read, open or oth-erwise act on documents attached to email messages(e.g., See attached) are also requests.Several complex classes of requests are particu-larly sensitive to the context for their interpretation.Reported requests are one such class.
Some reportedrequests, such as Paul asked if you could put to-gether a summary of your accomplishments in anemail, clearly function as a request.
Others do notimpose an obligation on the recipient, e.g., Sorry forthe delay; Paul requested your prize to be sent outlate December.
The surrounding context must beused to determine the intent of utterances like re-ported requests.
Such distinctions are often difficultto automate.Other complex requests include instructions.Sometimes instructions are of the kind that onemight ?file for later use?.
These tend to not bemarked as requests.
Other instructions, such as Youruser id and password have been set up.
Please fol-low the steps below to access the new environment,are intended to be executed more promptly.
Tem-poral distance between receipt of the instruction andexpected action is an important factor to distinguishbetween requests and non-requests.
Another influ-encing property is the likelihood of the trigger eventthat would lead to execution of the described ac-tion.
While the example instructions above are likelyto be executed, instructions for how to handle sus-pected anthrax-infected mail are (for most people)unlikely to be actioned.Further detail and discussion of these and other1Note, however, that not all questions are requests.
Rhetori-cal questions are perhaps the most obvious class of non-requestquestions.987challenges in defining and interpreting requests inemail can be found in (Lampert et al, 2008b).
Inparticular, that paper includes analysis of a series ofcomplex edge cases that make even human agree-ment in identifying requests difficult to achieve.4.1 An Email Request ClassifierOur request classifier is based around an SVM clas-sifier, implemented using Weka (Witten and Frank,2005).
Given an email message as input, completewith header information, our binary request classi-fier predicts the presence or absence of request ut-terances within the message.For training our request classifier, we use emailfrom the database dump of the Enron email corpusreleased by Andrew Fiore and Jeff Heer.2 This ver-sion of the corpus has been processed to remove du-plicate messages and to normalise sender and recipi-ent names, resulting in just over 250,000 email mes-sages.
No attachments are included.Our request classifier training data is drawn froma collection of 664 messages that were selected atrandom from the Enron corpus.
Each message wasannotated by three annotators, with overall kappaagreement of 0.681.
From the full dataset of 664messages, we remove all messages where annota-tors disagreed for training and evaluating our requestclassifier, in order to mitigate the effects of annota-tion noise, as discussed in (Beigman and Klebanov,2009).
The unanimously agreed data set used fortraining consists of 505 email messages.4.2 Request Classification FeaturesThe features we use in our request classifier are:?
message length in characters and words;?
number and percentage of capitalised words;?
number of non alpha-numeric characters;?
whether the subject line contains markers ofemail replies or forwards (e.g.
Re:, Fw:);?
the presence of sender or recipient names;?
the presence of sentences that begin with amodal verb (e.g., might, may, should, would);?
the presence of sentences that begin with aquestion word (e.g, who, what, where, when,why, which, how);2http://bailando.sims.berkeley.edu/enron/enron.sql.gz?
whether the message contains any sentencesthat end with a question mark; and?
binary word unigram and word bigram fea-tures for n-grams that occur at least three timesacross the training set.Before generating n-gram features, we normalisethe message text as shown in Table 1, in a mannersimilar to Carvalho and Cohen (2006).
We also addtokens marking the start and end of sentences, de-tected using a modified version of Scott Piao?s sen-tence splitter (Piao et al, 2002), and tokens markingthe start and end of the message.Symbol Used Patternnumbers Any sequence of digitsday Day names or abbreviationspronoun-object Objective pronouns: me, her, him, us, thempronoun-subject Subjective pronouns: I, we, you, he, she, theyfiletype .doc, .pdf, .ppt, .txt, .xls, .rtfmulti-dash 3 or more sequential ?-?
charactersmulti-underscore 3 or more sequential ?
?
charactersTable 1: Normalisation applied to n-gram featuresOur initial request classifier achieves an accuracy of72.28%.
Table 2 shows accuracy, precision, recalland F-measure results, calculated using stratified 10-fold cross-validation, compared against a majorityclass baseline.
Given the well-balanced nature ofour training data (52.08% of messages contain a re-quest), this is a reasonable basis for comparison.Majority Baseline No Zoning ClassifierRequest Non-Request Request Non-RequestAccuracy 52.08% 72.28%Precision 0.521 0.000 0.729 0.716Recall 1.000 0.000 0.745 0.698F-Measure 0.685 0.000 0.737 0.707Table 2: Request classifier results without email zoningAn error analysis of the predictions from our initialrequest classifier uncovered a series of classificationerrors that appeared to be due to request-like sig-nals being picked up from parts of messages such asemail signatures and quoted reply content.
It seemedlikely that our request classifier would benefit froman email zone classifier that could identify and ig-nore such message parts.9885 Improving Request Classification withEmail ZoningRequests in email do not occur uniformly across thezones that make up the email message.
There arespecific zones of a message in which requests arelikely to occur.Unfortunately, accurate classification of emailzones is difficult, hampered by the lack of standardsyntax used by different email clients to indicate dif-ferent message parts, and by the ad hoc ways inwhich people vary the structure and layout of theirmessages.
For example, different email clients indi-cate quoted material in a variety of ways.
Some pre-fix every line of the quoted message with a charactersuch as ?>?
or ?|?, while others indent the quotedcontent or insert the quoted message unmodified,prefixed by a message header.
Sometimes the newcontent is above the quoted content (a style knownas top-posting); in other cases, the new content mayappear after the quoted content (bottom-posting) orinterleaved with the quoted content (inline reply-ing).
Confounding the issue further is that users areable to configure their email client to suit their in-dividual tastes, and can change both the syntax ofquoting and their quoting style (top, bottom or in-line replying) on a per message basis.Despite the likelihood of some noise being in-troduced through mis-classification of email zones,our hypothesis was that even imperfect informationabout the functional parts of a message should im-prove the performance of our request classifier.Based on this hypothesis, we integrated Zebra(Lampert et al, 2009), our SVM-based email zoneclassifier, to identify the different functional parts ofemail messages.
Using features that capture graphic,orthographic and lexical information, Zebra classi-fies and segments the body text into nine differentemail zones: author content (written by the cur-rent sender), greetings, signoffs, quoted reply con-tent, forwarded content, email signatures, advertis-ing, disclaimers, and automated attachment refer-ences.
Zebra has two modes of operation, classi-fying either message fragments ?
whitespace sepa-rated sets of contiguous lines ?
or individual lines.We configure Zebra for line-based zone classifica-tion, and use it to extract only lines classified as au-thor, greeting and signoff text.
We remove the con-tent of all other zones before we evaluate featuresfor request classification.6 Results and DiscussionClassifying the zones in email messages and ap-plying our request classifier to only relevant mes-sage parts significantly increases the performanceof the request classifier.
As noted above, withoutzoning, our request classifier achieves accuracy of72.28% and a weighted F-measure (weighted be-tween the F-measure for requests and non-requestsbased on the relative frequency of each class) of0.723.
Adding the zone classifier, we increase theaccuracy to 83.76% and the weighted F-measure to0.838.
This corresponds to a relative increase inboth accuracy and weighted F-measure of 15.9%,which in turn corresponds to an error reduction ofmore than 41%.
Table 3 shows a comparison ofthe results of the non-zoning and zoning requestclassifiers, generated using stratified 10-fold cross-validation.
In a two-tailed paired t-test, run over teniterations of stratified 10-fold cross-validation, theincrease in accuracy, precision, recall and f-measurewere all significant at p=0.01.No Zoning With ZoningRequest Non-Request Request Non-RequestAccuracy 72.28% 83.76%*Precision 0.729 0.716 0.849* 0.825*Recall 0.745 0.698 0.837* 0.839*F-Measure 0.737 0.707 0.843* 0.832*Table 3: Request classifier results with and without emailzoning (* indicates a statistically significant difference atp=0.01)6.1 Lexical Feature ContributionAs expected, lexical information is crucial to re-quest classification.
When we experimented with re-moving all lexical (n-gram) features, the non-zoningrequest classifier accuracy dropped to 57.62% andthe zoning request classifier accuracy dropped to61.78%.
In contrast, when we apply only n-gramfeatures, we achieve accuracy of 71.49% for thenon-zoning classifier and 83.36% for the zoningclassifier.
Clearly, lexical information is critical foraccurate request classification, regardless of whetheremail messages are zoned.989Using Information Gain, we ranked the n-gramfeatures in terms of their usefulness.
Table 4 showsthe top-10 unigrams and bigrams for our non-zoningrequest classifier.
Using these top-10 n-grams (plusour non-n-gram features), we achieve only 66.34%accuracy.
These top-10 n-grams do not seem toalign well with linguistic intuitions, illustrating howthe noise from irrelevant message parts hampers per-formance.
In particular, there were several similar,apparently automated messages that were annotated(as non-requests) which appear to be the source ofseveral of the top-10 n-grams.
This strongly sug-gests that without zoning, the classifier is not learn-ing features from the training set at a useful level ofgenerality.Word Unigrams Word BigramsWord 1 Word 2pronoun-object let pronoun-objectplease pronoun-object knowiso start-sentence nopronoun-subject start datehourahead hour :attached ; houraheadlet hourahead hourwestdesk start-sentence startparsing westdesk /if iso finalTable 4: Top 10 useful n-grams for our request classifierwithout zoning, ranked by Information GainIn contrast, once we add the zoning classifier, thetop-10 unigrams and bigrams appear to correspondmuch better with linguistic intuitions about the lan-guage of requests.
These are shown in Table 5.
Us-ing these top-10 n-grams (plus our non-n-gram fea-tures), we achieve 80% accuracy.
This suggests that,even with our relatively small amount of trainingdata, the zone classifier helps the request classifierto extract fairly general n-gram features.Interestingly, although lexical features are veryimportant, the top three features ranked by Informa-tion Gain are non-lexical: message length in words,the number of non-alpha-numeric characters in themessage and the number of capitalised words in themessage.Word Unigrams Word BigramsWord 1 Word 2please ?
end-sentence?
pronoun-object knowpronoun-object let pronoun-objectif start-sentence pleasepronoun-subject if pronoun-subjectlet start-sentence thanksto please letknow pronoun-subject havethanks thanks commado start dateTable 5: Top 10 useful n-grams for our request classifierwith zoning, ranked by Information Gain6.2 Learning CurvesFigure 1 shows a plot of accuracy, precision andrecall versus the number of training instancesused to build the request classifier.
These re-sults are calculated over zoned email bodies, us-ing the average across ten iterations of stratified10-fold cross-validation for each different sizedset of training instances, implemented via theFilteredClassifier with the Resample fil-ter in Weka.
Given our pool of 505 agreed mes-sage annotations, we plot the recall and precision fortraining instance sets of size 50 to 505 messages.There is a clear trend of increasing performanceas the training set size grows.
It seems reasonable toassume that more data should continue to facilitatebetter request classifier performance.
To this end,we are annotating more data as part of our currentand future work.6.3 Error AnalysisTo explore the errors made by our request classifier,we examined the output of our zoning request clas-sifier using our full feature set, including all wordn-grams.Approximately 20% of errors relate to requeststhat are implicit, and thus more difficult to detectfrom surface features.
Another 10% of errors aredue to attempts to classify requests in inappropri-ate genres of email messages.
In particular, bothmarketing messages and spam frequently includerequest-like, directive utterances which our annota-tors all agreed would not be useful to mark as re-990Figure 1: Learning curve showing recall, accuracy andprecision versus the number of training instancesquests for an email user.
Not unreasonably, our clas-sifier is sometimes confused by the content of thesemessages, mistakenly marking requests where ourannotators did not.
We intend to resolve these classi-fication errors by filtering out such messages beforewe apply the request classifier.Another 5% of errors are due to request contentoccurring in zones that we ignore.
The most com-mon case is content in a forwarded zone.
Sometimesemail senders forward a message as a form of taskdelegation; because we ignore forwarded content,our request classifier misses such requests.
We didexperiment with including content from forwardedzones (in addition to the author, greeting and sig-noff zones), but found that this reduced the perfor-mance of our request classifier, presumably due tothe additional noise from irrelevant content in otherforwarded material.
Forwarded messages are thussomewhat difficult to deal with.
One possible ap-proach would be to build sender-specific profiles thatmight allow us to deal with forwarded content (andpotentially content from other zones) differently fordifferent users, essentially learning to adapt to thedifferent styles of different email users.A further 5% of errors involve errors in the zoneclassifier, which leads to incorrect zone labels be-ing applied to zone content that we would wish toinclude for our request classifier.
Examples includeauthor content being mistakenly identified as signa-ture content.
In such cases, we incorrectly removerelevant content from the body text that is passedto our request classifier.
Improvements to the zoneclassifier would resolve these issues.As part of our annotation task, we also askedcoders to mark the presence of pleasantries.
Wedefine a pleasantry as an utterance that could be arequest in some other context, but that does not func-tion as a request in the context of use under consid-eration.
Pleasantries are frequently formulaic, anddo not place any significant obligation on the recip-ient to act or respond.
Variations on the phrase Letme know if you have any questions are particularlycommon in email messages.
The context of the en-tire email message needs to be considered to distin-guish between when such an utterance functions asa request and when it should be marked as a pleas-antry.
Of the errors made by our request classifier,approximately 5% involve marking messages con-taining only pleasantries as containing a request.The remaining errors are somewhat diverse.Close to 5% involve errors interpreting requests as-sociated with attached files.
The balance of almost50% of errors involve a wide range of issues, frommisspellings of key words such as please to a lackof punctuation cues such as question marks.7 ConclusionRequest classification, like any form of automatedspeech act recognition, is a difficult task.
Despitethis inherent difficulty, the automatic request clas-sifier we describe in this paper correctly labels re-quests at the message level in 83.76% of email mes-sages from our annotated dataset.
Unlike previouswork that has attempted to automate the classifi-cation of requests in email, we zone the messageswithout manual intervention.
This improves accu-racy by 15.9% relative to the performance of thesame request classifier without the assistance of anemail zone classifier to focus on relevant messageparts.
Although some zone classification errors aremade, error analysis reveals that only 5% of errorsare due to zone misclassification of message parts.This suggests that, although zone classifier perfor-mance could be further improved, it is likely thatfocusing on improving the request classifier usingthe existing zone classifier performance will lead togreater performance gains.991ReferencesDawn Archer, Jonathan Culpeper, and Matthew Davies,2008.
Corpus Linguistics: An International Hand-book, chapter Pragmatic Annotation, pages 613?642.Mouton de Gruyter.John L Austin.
1962.
How to do things with words.
Har-vard University Press.Eyal Beigman and Beata Beigman Klebanov.
2009.Learning with annotation noise.
In Proceedings of theJoint Conference of the 47th Annual Meeting of theACL and the 4th IJCNLP, pages 280?287, Singapore.Victoria Bellotti, Nicolas Ducheneaut, Mark Howard,and Ian Smith.
2003.
Taking email to task: Thedesign and evaluation of a task management centredemail tool.
In Computer Human Interaction Confer-ence, CHI, pages 345?352, Ft Lauderdale, Florida.Vitor R Carvalho and William W Cohen.
2004.
Learningto extract signature reply lines from email.
In Pro-ceedings of First Conference on Email and Anti-Spam(CEAS), Mountain View, CA, July 30-31.Vitor R. Carvalho and William W. Cohen.
2006.
Improv-ing email speech act analysis via n-gram selection.
InProceedings of HLT/NAACL 2006 - Workshop on Ana-lyzing Conversations in Text and Speech, pages 35?41,New York.William W. Cohen, Vitor R. Carvalho, and Tom M.Mitchell.
2004.
Learning to classify email into?speech acts?.
In Conference on Empirical Meth-ods in Natural Language Processing, pages 309?316,Barcelona, Spain.Simon H. Corston-Oliver, Eric Ringger, Michael Gamon,and Richard Campbell.
2004.
Task-focused summa-rization of email.
In ACL-04 Workshop: Text Summa-rization Branches Out, pages 43?50.Nicolas Ducheneaut and Victoria Bellotti.
2001.
E-mailas habitat: an exploration of embedded personal infor-mation management.
Interactions, 8(5):30?38.Dominique Estival, Tanja Gaustad, Son Bao Pham, WillRadford, and Ben Hutchinson.
2007.
Author profilingfor English emails.
In Proceedings of the 10th Con-ference of the Pacific Association for ComputationalLinguistics, pages 263?272, Melbourne, Australia.Jade Goldstein and Roberta Evans Sabin.
2006.
Usingspeech acts to categorize email and identify email gen-res.
In Proceedings of the 39th Hawaii InternationalConference on System Sciences, page 50b.Lewis Hassell and Margaret Christensen.
1996.
Indi-rect speech acts and their use in three channels of com-munication.
In Proceedings of the First InternationalWorkshop on Communication Modeling - The Lan-guage/Action Perspective, Tilburg, The Netherlands.Hamid Khosravi and Yorick Wilks.
1999.
Routing emailautomatically by purpose not topic.
Journal of NaturalLanguage Engineering, 5:237?250.Bryan Klimt and Yiming Yang.
2004.
Introducing theEnron corpus.
In Proceedings of the Conference onEmail and Anti-Spam (CEAS).Andrew Lampert, Robert Dale, and Ce?cile Paris.
2008a.The nature of requests and commitments in email mes-sages.
In Proceedings of EMAIL-08: the AAAI Work-shop on Enhanced Messaging, pages 42?47, Chicago.Andrew Lampert, Robert Dal e, and Ce?cile Paris.
2008b.Requests and commitments in email are more complexthan you think: Eight reasons to be cautious.
In Pro-ceedings of Australasian Language Technology Work-shop (ALTA2008), pages 55?63, Hobart, Australia.Andrew Lampert, Robert Dale, and Ce?cile Paris.
2009.Segmenting email message text into zones.
In Pro-ceedings of Empirical Methods in Natural LanguageProcessing, pages 919?928, Singapore.Wendy E. Mackay.
1988.
More than just a communica-tion system: Diversity in the use of electronic mail.
InACM conference on Computer-supported cooperativework, pages 344?353, Portland, Oregon, USA.Denise E. Murray.
1991.
Conversation for Action: TheComputer Terminal As Medium of Communication.John Benjamins Publishing Co.Scott S L Piao, Andrew Wilson, and Tony McEnery.2002.
A multilingual corpus toolkit.
In Proceedingsof 4th North American Symposium on Corpus Linguis-tics, Indianapolis.Jerry M. Sadock and Arnold Zwicky, 1985.
LanguageTypology and Syntactic Description.
Vol.I ClauseStructure, chapter Speech act distinctions in syntax,pages 155?96.
Cambridge University Press.Simon Scerri, Myriam Mencke, Brian David, andSiegfried Handschuh.
2008.
Evaluating the ontologypowering smail ?
a conceptual framework for seman-tic email.
In Proceedings of the 6th LREC Conference,Marrakech, Morocco.John R. Searle.
1969.
Speech Acts: An Essay in thePhilosophy of Language.
Cambridge University Press.John Sinclair and Richard Malcolm Coulthard.
1975.
To-wards and Analysis of Discourse - The English used byTeachers and Pupils.
Oxford University Press.Steve Whittaker and Candace Sidner.
1996.
Email over-load: exploring personal information management ofemail.
In ACM Computer Human Interaction confer-ence, pages 276?283.
ACM Press.Terry Winograd and Fernando Flores.
1986.
Under-standing Computers and Cognition.
Ablex PublishingCorporation, Norwood, New Jersey, USA, 1st edition.ISBN: 0-89391-050-3.Ian Witten and Eiba Frank.
2005.
Data Mining: Prac-tical machine learning tools and techniques.
MorganKaufmann, San Francisco, 2nd edition.992
