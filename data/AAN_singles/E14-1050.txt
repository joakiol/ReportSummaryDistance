Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 472?481,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsUsing Distributional Similarity of Multi-way Translations to PredictMultiword Expression CompositionalityBahar Salehi,?
?Paul Cook?and Timothy Baldwin???
NICTA Victoria Research Laboratory?
Department of Computing and Information SystemsThe University of MelbourneVictoria 3010, Australiabsalehi@student.unimelb.edu.au, paulcook@unimelb.edu.au, tb@ldwin.netAbstractWe predict the compositionality of multi-word expressions using distributional sim-ilarity between each component word andthe overall expression, based on transla-tions into multiple languages.
We evaluatethe method over English noun compounds,English verb particle constructions andGerman noun compounds.
We show thatthe estimation of compositionality is im-proved when using translations into multi-ple languages, as compared to simply us-ing distributional similarity in the sourcelanguage.
We further find that string sim-ilarity complements distributional similar-ity.1 Compositionality of MWEsMultiword expressions (hereafter MWEs) arecombinations of words which are lexically, syntac-tically, semantically or statistically idiosyncratic(Sag et al., 2002; Baldwin and Kim, 2009).
Muchresearch has been carried out on the extraction andidentification of MWEs1in English (Schone andJurafsky, 2001; Pecina, 2008; Fazly et al., 2009)and other languages (Dias, 2003; Evert and Krenn,2005; Salehi et al., 2012).
However, considerablyless work has addressed the task of predicting themeaning of MWEs, especially in non-English lan-guages.
As a step in this direction, the focus ofthis study is on predicting the compositionality ofMWEs.An MWE is fully compositional if its meaningis predictable from its component words, and it isnon-compositional (or idiomatic) if not.
For ex-ample, stand up ?rise to one?s feet?
is composi-1In this paper, we follow Baldwin and Kim (2009) inconsidering MWE ?identification?
to be a token-level disam-biguation task, and MWE ?extraction?
to be a type-level lex-icon induction task.tional, because its meaning is clear from the mean-ing of the components stand and up.
However, themeaning of strike up ?to start playing?
is largelyunpredictable from the component words strikeand up.In this study, following McCarthy et al.
(2003)and Reddy et al.
(2011), we consider composition-ality to be graded, and aim to predict the degreeof compositionality.
For example, in the datasetof Reddy et al.
(2011), climate change is judgedto be 99% compositional, while silver screen is48% compositional and ivory tower is 9% com-positional.
Formally, we model compositionalityprediction as a regression task.An explicit handling of MWEs has been shownto be useful in NLP applications (Ramisch, 2012).As an example, Carpuat and Diab (2010) proposedtwo strategies for integrating MWEs into statisti-cal machine translation.
They show that even alarge scale bilingual corpus cannot capture all thenecessary information to translate MWEs, and thatin adding the facility to model the compositional-ity of MWEs into their system, they could improvetranslation quality.
Acosta et al.
(2011) showedthat treating non-compositional MWEs as a sin-gle unit in information retrieval improves retrievaleffectiveness.
For example, while searching fordocuments related to ivory tower, we are almostcertainly not interested in documents relating toelephant tusks.Our approach is to use a large-scale multi-waytranslation lexicon to source translations of MWEsand their component words, and then model therelative similarity between each of the componentwords and the MWE, using distributional similar-ity based on monolingual corpora for the sourcelanguage and each of the target languages.
Ourhypothesis is that using distributional similarityin more than one language will improve the pre-diction of compositionality.
Importantly, in orderto make the method as language-independent and472broadly-applicable as possible, we make no use ofcorpus preprocessing such as lemmatisation, andrely only on the availability of a translation dictio-nary and monolingual corpora.Our results confirm our hypothesis that distri-butional similarity over the source language in ad-dition to multiple target languages improves thequality of compositionality prediction.
We alsoshow that our method can be complemented withstring similarity (Salehi and Cook, 2013) to furtherimprove compositionality prediction.
We achievestate-of-the-art results over two datasets.2 Related WorkMost recent work on predicting the composi-tionality of MWEs can be divided into twocategories: language/construction-specific andgeneral-purpose.
This can be at either the token-level (over token occurrences of an MWE in a cor-pus) or type-level (over the MWE string, indepen-dent of usage).
The bulk of work on composition-ality has been language/construction-specific andoperated at the token-level, using dedicated meth-ods to identify instances of a given MWE, andspecific properties of the MWE in that languageto predict compositionality (Lin, 1999; Kim andBaldwin, 2007; Fazly et al., 2009).General-purpose token-level approaches suchas distributional similarity have been commonlyapplied to infer the semantics of a word/MWE(Schone and Jurafsky, 2001; Baldwin et al., 2003;Reddy et al., 2011).
These techniques are basedon the assumption that the meaning of a word ispredictable from its context of use, via the neigh-bouring words of token-level occurrences of theMWE.
In order to predict the compositionality ofa given MWE using distributional similarity, thedifferent contexts of the MWE are compared withthe contexts of its components, and the MWE isconsidered to be compositional if the MWE andcomponent words occur in similar contexts.Identifying token instances of MWEs is not al-ways easy, especially when the component wordsdo not occur sequentially.
For example considerput on in put your jacket on, and put your jacketon the chair.
In the first example put on is anMWE while in the second example, put on is asimple verb with prepositional phrase and not aninstance of an MWE.
Moreover, if we adopt a con-servative identification method, the number of to-ken occurrences will be limited and the distribu-tional scores may not be reliable.
Additionally,for morphologically-rich languages, it can be dif-ficult to predict the different word forms a givenMWE type will occur across, posing a challengefor our requirement of no language-specific pre-processing.Pichotta and DeNero (2013) proposed a token-based method for identifying English phrasalverbs based on parallel corpora for 50 languages.They show that they can identify phrasal verbs bet-ter when they combine information from multiplelanguages, in addition to the information they getfrom a monolingual corpus.
This finding lendsweight to our hypothesis that using translation dataand distributional similarity from each of a rangeof target languages, can improve compositionalityprediction.
Having said that, the general applica-bility of the method is questionable ?
there aremany parallel corpora involving English, but forother languages, this tends not to be the case.Salehi and Cook (2013) proposed a general-purpose type-based approach using translationdata from multiple languages, and string similar-ity between the MWE and each of the compo-nent words.
They use training data to identify thebest-10 languages for a given family of MWEs, onwhich to base the string similarity, and once againfind that translation data improves their resultssubstantially.
Among the four string similaritymeasures they experimented with, longest com-mon substring was found to perform best.
Theirproposed method is general and applicable to dif-ferent families of MWEs in different languages.
Inthis paper, we reimplement the method of Salehiand Cook (2013) using longest common substring(LCS), and both benchmark against this methodand combine it with our distributional similarity-based method.3 Our ApproachTo predict the compositionality of a given MWE,we first measure the semantic similarity betweenthe MWE and each of its component words2usingdistributional similarity based on a monolingualcorpus in the source language.
We then repeat theprocess for translations of the MWE and its com-ponent words into each of a range of target lan-guages, calculating distributional similarity using2Note that we will always assume that there are twocomponent words, but the method is easily generalisable toMWEs with more than two components.473MWE component1 component2score1 score2TranslationsTranslate(using Panlex)DS(using Wikiepdia)Translate(using Panlex)Translate(using Panlex)DS(using Wikiepdia)Figure 1: Outline of our approach to computingthe distributional similarity (DS) of translationsof an MWE with each of its component words,for a given target language.
score1and score2are the similarity for the first and second compo-nents, respectively.
We obtain translations fromPanlex, and use Wikipedia as our corpus for eachlanguage.a monolingual corpus in the target language (Fig-ure 1).
We additionally use supervised learning toidentify which target languages (or what weightsfor each language) optimise the prediction of com-positionality (Figure 2).
We hypothesise that byusing multiple translations ?
rather than only in-formation from the source language ?
we will beable to better predict compositionality.We optionally combine our proposed approachwith string similarity, calculated based on themethod of Salehi and Cook (2013), using LCS.Below, we detail our method for calculating dis-tributional similarity in a given language, the dif-ferent methods for combining distributional simi-larity scores into a single estimate of composition-ality, and finally the method for selecting the targetlanguages to use in calculating compositionality.3.1 Calculating Distributional SimilarityIn order to be consistent across all languages andbe as language-independent as possible, we calcu-CSmethod CSmethodScore1 for each language Score2 for each language21 )1( ss ??
?
?Compositionality  scores1 s2Figure 2: Outline of the method for combin-ing distributional similarity scores from multiplelanguages, across the components of the MWE.CSmethodrefers to one of the methods describedin Section 3.2 for calculating compositionality.late distributional similarity in the following man-ner for a given language.Tokenisation is based on whitespace delimitersand punctuation; no lemmatisation or case-foldingis carried out.
Token instances of a given MWEor component word are identified by full-token n-gram matching over the token stream.
We assumethat all full stops and equivalent characters forother orthographies are sentence boundaries, andchunk the corpora into (pseudo-)sentences on thebasis of them.
For each language, we identify the51st?1050th most frequent words, and considerthem to be content-bearing words, in the mannerof Sch?utze (1997).
This is based on the assump-tion that the top-50 most frequent words are stopwords, and not a good choice of word for calculat-ing distributional similarity over.
That is not to saythat we can?t calculate the distributional similarityfor stop words, however (as we will for the verbparticle construction dataset ?
see Section 4.3.2)they are simply not used as the dimensions in ourcalculation of distributional similarity.We form a vector of content-bearing wordsacross all token occurrences of the target word,474on the basis of these content-bearing words.
Dis-tributional similarity is calculated over these con-text vectors using cosine similarity.
Accord-ing to Weeds (2003), using dependency rela-tions with the neighbouring words of the targetword can better predict the meaning of the targetword.
However, in line with our assumption of nolanguage-specific preprocessing, we just use wordco-occurrence.3.2 Calculating CompositionalityFirst, we need to calculate a combined composi-tionality score from the individual distributionalsimilarities between each component word and theMWE.
Following Reddy et al.
(2011), we combinethe component scores using the weighted mean (asshown in Figure 2):comp = ?s1+ (1?
?
)s2(1)where s1and s2are the scores for the first andthe second component, respectively.
We use dif-ferent ?
settings for each dataset, as detailed inSection 4.3.We experiment with a range of methods for cal-culating compositionality, as follows:CSL1: calculate distributional similarity usingonly distributional similarity in the sourcelanguage corpus (This is the approach usedby Reddy et al.
(2011), as discussed in Sec-tion 2).CSL2N: exclude the source language, and com-pute the mean of the distributional similarityscores for the best-N target languages.
Thevalue of N is selected according to trainingdata, as detailed in Section 3.3.CSL1+L2N: calculate distributional similarityover both the source language (CSL1) andthe mean of the best-N languages (CSL2N),and combine via the arithmetic mean.3Thisis to examine the hypothesis that usingmultiple target languages is better than justusing the source language.CSSVR(L1+L2 ): train a support vector regressor(SVR: Smola and Sch?olkopf (2004)) over thedistributional similarities for all 52 languages(source and target languages).3We also experimented with taking the mean over all thelanguages ?
target and source ?
but found it best to com-bine the scores for the target languages first, to give moreweight to the source language.CSstring: calculate string similarity using theLCS-based method of Salehi and Cook(2013).4CSstring+L1: calculate the mean of the stringsimilarity (CSstring) and distributional sim-ilarity in the source language (Salehi andCook, 2013).CSall: calculate the mean of the string similarity(CSstring) and distributional similarity scores(CSL1and CSL2N).3.3 Selecting Target LanguagesWe experiment with two approaches for combin-ing the compositionality scores from multiple tar-get languages.First, inCSL2N(andCSL1+L2NandCSallthatbuild off it), we use training data to rank the targetlanguages according to Pearson?s correlation be-tween the predicted compositionality scores andthe gold-standard compositionality judgements.Based on this ranking, we take the best-N lan-guages, and combine the individual composition-ality scores by taking the arithmetic mean.
We se-lect N by determining the value that optimises thecorrelation over the training data.
In other words,the selection ofN and accordingly the best-N lan-guages are based on nested cross-validation overtraining data, independently of the test data for thatiteration of cross-validation.Second in CSSVR(L1+L2 ), we combine thecompositionality scores from the source and all 51target languages into a feature vector, and train anSVR over the data using LIBSVM.54 ResourcesIn this section, we describe the resources requiredby our method, and also the datasets used to eval-uate our method.4.1 Monolingual Corpora for DifferentLanguagesWe collected monolingual corpora for each of 52languages (51 target languages + 1 source lan-guage) from XML dumps of Wikipedia.
Theselanguages are based on the 54 target languages4Due to differences in our random partitioning, our re-ported results over the two English datasets differ slightlyover the results of Salehi and Cook (2013) using the samemethod.5http://www.csie.ntu.edu.tw/?cjlin/libsvm475used by Salehi and Cook (2013), excluding Span-ish because we happened not to have a dump ofSpanish Wikipedia, and also Chinese and Japanesebecause of the need for a language-specific wordtokeniser.
The raw corpora were preprocessed us-ing the WP2TXT toolbox6to eliminate XML tags,HTML tags and hyperlinks, and then tokenisa-tion based on whitespace and punctuation was per-formed.
The corpora vary in size from roughly750M tokens for English, to roughly 640K tokensfor Marathi.4.2 Multilingual DictionaryTo translate the MWEs and their components,we follow Salehi and Cook (2013) in using Pan-lex (Baldwin et al., 2010).
This online dictio-nary is massively multilingual, covering more than1353 languages.
For each MWE dataset (see Sec-tion 4.3), we translate the MWE and componentwords from the source language into each of the51 languages.In instances where there is no direct translationin a given language for a term, we use a pivot lan-guage to find translation(s) in the target language.For example, the English noun compound silverscreen has direct translations in only 13 languagesin Panlex, including Vietnamese (ma`n bac) butnot French.
There is, however, a translation ofma`n bac into French (cine?ma), allowing us toinfer an indirect translation between silver screenand cine?ma.
In this way, if there are no directtranslations into a particular target language, wesearch for a single-pivot translation via each of ourother target languages, and combine them all to-gether as our set of translations for the target lan-guage of interest.In the case that no translation (direct or indirect)can be found for a given source language term intoa particular target language, the compositionalityscore for that target language is set to the averageacross all target languages for which scores can becalculated for the given term.
If no translations areavailable for any target language (e.g.
the term isnot in Panlex) the compositionality score for eachtarget language is set to the average score for thattarget language across all other source languageterms.6http://wp2txt.rubyforge.org/4.3 DatasetsWe evaluate our proposed method over threedatasets (two English, one German), as describedbelow.4.3.1 English Noun Compounds (ENC)Our first dataset is made up of 90 binary Englishnoun compounds, from the work of Reddy et al.(2011).
Each noun compound was annotated bymultiple annotators using the integer scale 0 (fullynon-compositional) to 5 (fully compositional).
Afinal compositionality score was then calculatedas the mean of the scores from the annotators.If we simplistically consider 2.5 as the thresholdfor compositionality, the dataset is relatively wellbalanced, containing 48% compositional and 52%non-compositional noun compounds.
FollowingReddy et al.
(2011), in combining the component-wise distributional similarities for this dataset, weweight the first component in Equation 1 higherthan the second (?
= 0.7).4.3.2 English Verb Particle Constructions(EVPC)The second dataset contains 160 English verb par-ticle constructions (VPCs), from the work of Ban-nard (2006).
In this dataset, a verb particle con-struction consists of a verb (the head) and a prepo-sitional particle (e.g.
hand in, look up or battle on).For each component word (the verb and parti-cle, respectively), multiple annotators were askedwhether the VPC entails the component word.
Inorder to translate the dataset into a regression task,we calculate the overall compositionality as thenumber of annotations of entailment for the verb,divided by the total number of verb annotations forthat VPC.
That is, following Bannard et al.
(2003),we only consider the compositionality of the verbcomponent in our experiments (and as such ?
= 1in Equation 1).One area of particular interest with this datasetwill be the robustness of the method to functionwords (the particles), both under translation andin terms of calculating distributional similarity, al-though the findings of Baldwin (2006) for Englishprepositions are at least encouraging in this re-spect.
Additionally, English VPCs can occur in?split?
form (e.g.
put your jacket on, from ourearlier example), which will complicate identifi-cation, and the verb component will often be in-flected and thus not match under our identificationstrategy (for both VPCs and the component verbs).476Dataset Language Frequency FamilyENCItalian 100 RomanceFrench 99 RomanceGerman 86 GermanicVietnamese 83 Viet-MuongPortuguese 62 RomanceEVPCBulgarian 100 SlavicBreton 100 CelticOccitan 100 RomanceIndonesian 100 IndonesianSlovenian 100 SlavicGNCPolish 100 SlavicLithuanian 99 BalticFinnish 74 UralicBulgarian 72 SlavicCzech 40 SlavicTable 1: The 5 best languages for the ENC, EVPCand GNC datasets.
The language family is basedon Voegelin and Voegelin (1977).4.3.3 German Noun Compounds (GNC)Our final dataset is made up of 246 German nouncompounds (von der Heide and Borgwaldt, 2009;Schulte im Walde et al., 2013).
Multiple anno-tators were asked to rate the compositionality ofeach German noun compound on an integer scaleof 1 (non-compositional) to 7 (compositional).The overall compositionality score is then calcu-lated as the mean across the annotators.
Note thatthe component words are provided as part of thedataset, and that there is no need to perform de-compounding.
Following Schulte im Walde et al.
(2013), we weight the first component higher inEquation 1 (?
= 0.8) when calculating the overallcompositionality score.This dataset is significant in being non-English,and also in that German has relatively rich mor-phology, which we expect to impact on the iden-tification of both the MWE and the componentwords.5 ResultsAll experiments are carried out using 10 iterationsof 10-fold cross validation, randomly partitioningthe data independently on each of the 10 iterations,and averaging across all 100 test partitions in ourpresented results.
In the case of CSL2Nand othermethods that make use of it (i.e.
CSL1+L2NandCSall), the languages selected for a given trainingfold are then used to compute the compositionalityscores for the instances in the test set.
Figures 3a,3b and 3c are histograms of the number of timeseach N is selected over 100 folds on ENC, EVPCand GNC datasets, respectively.
From the his-tograms, N = 6, N = 15 and N = 2 are the mostcommonly selected settings for ENC, EVPC andGNC, respectively.
That is, multiple languages aregenerally used, but more languages are used forEnglish VPCs than either of the compound noundatasets.
The 5 most-selected languages for ENC,EVPC and GNC are shown in Table 1.
As wecan see, there are some languages which are al-ways selected for a given dataset, but equally thecommonly-selected languages vary considerablybetween datasets.Further analysis reveals that 32 (63%) targetlanguages for ENC, 25 (49%) target languagesfor EVPC, and only 5 (10%) target languages forGNC have a correlation of r ?
0.1 with gold-standard compositionality judgements.
On theother hand, 8 (16%) target languages for ENC, 2(4%) target languages for EVPC, and no target lan-guages for GNC have a correlation of r ?
?0.1.5.1 ENC ResultsEnglish noun compounds are relatively easy toidentify in a corpus,7because the components oc-cur sequentially, and the only morphological vari-ation is in noun number (singular vs. plural).
Inother words, the precision for our token match-ing method is very high, and the recall is alsoacceptably high.
Partly as a result of the easeof identification, we get a high correlation ofr = 0.700 for CSL1(using only source languagedata).
Using only target languages (CSL2N), theresults drop to r = 0.434, but when we combinethe two (CSL1+L2N), the correlation is higherthan using only source or target language data, atr = 0.725.
When we combine all languages us-ing SVR, the results rise slightly higher again tor = 0.744, which is slightly above the correla-tion of the state-of-the-art method of Salehi andCook (2013), which combines their method withthe method of Reddy et al.
(2011) (CSstring+L1).These last two results support our hypothesis thatusing translation data can improve the predictionof compositionality.
The results for string similar-ity on its own (CSstring, r = 0.644) are slightlylower than those using only source language dis-tributional similarity, but when combined with7Although see Lapata and Lascarides (2003) for discus-sion of the difficulty of reliably identifying low-frequencyEnglish noun compounds.4770 5 10 15 20 250510152025bestNFrequency(a) ENC0 5 10 15 20 2502468101214161820best NFrequency(b) EVPC0 5 10 15 20 2502468101214161820best NFrequency(c) GNCFigure 3: Histograms displaying how many times a given N is selected as the best number of languagesover each dataset.
For example, according to the GNC chart, there is a peak for N = 2, which showsthat over 100 folds, the best-2 languages achieved the highest correlation on 18 folds.Method Summary of the Method ENC EVPC GNCCSL1Source language 0.700 0.177 0.141CSL2NBest-N target languages 0.434 0.398 0.113CSL1+L2NSource + best-N target languages 0.725 0.312 0.178CSSVR(L1+L2 )SVR (Source + all 51 target languages) 0.744 0.389 0.085CSstringString Similarity (Salehi and Cook, 2013) 0.644 0.385 0.372CSstring+L1CSstring+CSL1(Salehi and Cook, 2013) 0.739 0.360 0.353CSallCSL1+ CSL2N+ CSstring0.732 0.417 0.364Table 2: Pearson?s correlation on the ENC, EVPC and GNC datasetsCSL1+L2N(i.e.
CSall) there is a slight rise in cor-relation (from r = 0.725 to r = 0.732).5.2 EVPC ResultsEnglish VPCs are hard to identify.
As discussedin Section 2, VPC components may not occur se-quentially, and even when they do occur sequen-tially, they may not be a VPC.
As such, our sim-plistic identification method has low precision andrecall (hand analysis of 927 identified VPC in-stances would suggest a precision of around 74%).There is no question that this is a contributor tothe low correlation for the source language method(CSL1; r = 0.177).
When we use target lan-guages instead of the source language (CSL2N),the correlation jumps substantially to r = 0.398.When we combine English and the target lan-guages (CSL1+L2N), the results are actually lowerthan just using the target languages, because ofthe high weight on the target language, which isnot desirable for VPCs, based on the source lan-guage results.
Even for CSSVR(L1+L2 ), the re-sults (r = 0.389) are slightly below the targetlanguage-only results.
This suggests that whenpredicting the compositionality of MWEs whichare hard to identify in the source language, it mayactually be better to use target languages only.
Theresults for string similarity (CSstring: r = 0.385)are similar to those for CSL2N.
However, as withthe ENC dataset, when we combine string simi-larity and distributional similarity (CSall), the re-sults improve, and we achieve the state-of-the-artfor the dataset.In Table 3, we present classification-based eval-478Method Precision Recall F-score (?
= 1) AccuracyBannard et al.
(2003) 60.8 66.6 63.6 60.0Salehi and Cook (2013) 86.2 71.8 77.4 69.3CSall79.5 89.3 82.0 74.5Table 3: Results (%) for the binary compositionality prediction task on the EVPC datasetuation over a subset of EVPC, binarising the com-positionality judgements in the manner of Bannardet al.
(2003).
Our method achieves state-of-the-artresults in terms of overall F-score and accuracy.5.3 GNC ResultsGerman is a morphologically-rich language, withmarking of number and case on nouns.
Giventhat we do not perform any lemmatization or otherlanguage-specific preprocessing, we inevitablyachieve low recall for the identification of nouncompound tokens, although the precision shouldbe nearly 100%.
Partly because of the resultantsparseness in the distributional similarity method,the results for CSL1are low (r = 0.141), al-though they are lower again when using target lan-guages (r = 0.113).
However, when we combinethe source and target languages (CSL1+L2N) theresults improve to r = 0.178.
The results forCSSVR(L1+L2 ), on the other hand, are very low(r = 0.085).
Ultimately, simple string similar-ity achieves the best results for the dataset (r =0.372), and this result actually drops slightly whencombined with the distributional similarities.To better understand the reason for the lacklus-tre results using SVR, we carried out error analysisand found that, unlike the other two datasets, abouthalf of the target languages return scores whichcorrelate negatively with the human judgements.When we filter these languages from the data, thescore for SVR improves appreciably.
For example,over the best-3 languages overall, we get a corre-lation score of r = 0.179, which is slightly higherthan CSL1+L2N.We further investigated the reason for gettingvery low and sometimes negative correlations withmany of our target languages.
We noted thatabout 24% of the German noun compounds inthe dataset do not have entries in Panlex.
Thiscontrasts with ENC where only one instance doesnot have an entry in Panlex, and EVPC where allVPCs have translations in at least one language inPanlex.
We experimented with using string sim-ilarity scores in the case of such missing transla-tions, as opposed to the strategy described in Sec-tion 4.2.
The results for CSSVR(L1+L2 )rose tor = 0.269, although this is still below the correla-tion for just using string similarity.Our results on the GNC dataset using stringsimilarity are competitive with the state-of-the-artresults (r = 0.45) using a window-based distribu-tional similarity approach over monolingual Ger-man data (Schulte im Walde et al., 2013).
Note,however, that their method used part-of-speech in-formation and lemmatisation, where ours does not,in keeping with the language-independent philos-ophy of this research.6 Conclusion and Future WorkIn this study, we proposed a method to predict thecompositionality of MWEs based on monolingualdistributional similarity between the MWE andeach of its component words, under translationinto multiple target languages.
We showed thatusing translation and multiple target languages en-hances compositionality modelling, and also thatthere is strong complementarity between our ap-proach and an approach based on string similarity.In future work, we hope to address the ques-tion of translation sparseness, as observed for theGNC dataset.
We also plan to experiment with un-supervised morphological analysis methods to im-prove identification recall, and explore the impactof tokenization.
Furthermore, we would like to in-vestigate the optimal number of stop words andcontent-bearing words for each language, and tolook into the development of general unsupervisedmethods for compositionality prediction.AcknowledgementsWe thank the anonymous reviewers for theirinsightful comments and valuable suggestions.NICTA is funded by the Australian government asrepresented by Department of Broadband, Com-munication and Digital Economy, and the Aus-tralian Research Council through the ICT Centreof Excellence programme.479ReferencesOtavio Acosta, Aline Villavicencio, and Viviane Mor-eira.
2011.
Identification and treatment of multi-word expressions applied to information retrieval.In Proceedings of the Workshop on Multiword Ex-pressions: from Parsing and Generation to the RealWorld, pages 101?109, Portland, USA.Timothy Baldwin and Su Nam Kim.
2009.
Multiwordexpressions.
In Nitin Indurkhya and Fred J. Dam-erau, editors, Handbook of Natural Language Pro-cessing.
CRC Press, Boca Raton, USA, 2nd edition.Timothy Baldwin, Colin Bannard, Takaaki Tanaka, andDominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL-2003 Workshop on MultiwordExpressions: Analysis, Acquisition and Treatment,pages 89?96, Sapporo, Japan.Timothy Baldwin, Jonathan Pool, and Susan M Colow-ick.
2010.
Panlex and lextract: Translating allwords of all languages of the world.
In Proceed-ings of the 23rd International Conference on Com-putational Linguistics: Demonstrations, pages 37?40, Beijing, China.Timothy Baldwin.
2006.
Distributional similarity andpreposition semantics.
In Patrick Saint-Dizier, ed-itor, Computational Linguistics Dimensions of Syn-tax and Semantics of Prepositions, pages 197?210.Springer, Dordrecht, Netherlands.Colin Bannard, Timothy Baldwin, and Alex Las-carides.
2003.
A statistical approach to the seman-tics of verb-particles.
In Proceedings of the ACL2003 workshop on Multiword expressions: analysis,acquisition and treatment-Volume 18, pages 65?72,Sapporo, Japan.Colin James Bannard.
2006.
Acquiring Phrasal Lexi-cons from Corpora.
Ph.D. thesis, University of Ed-inburgh.Marine Carpuat and Mona Diab.
2010.
Task-basedevaluation of multiword expressions: a pilot studyin statistical machine translation.
In Human Lan-guage Technologies: The 2010 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, pages 242?245, LosAngeles, USA.Ga?el Dias.
2003.
Multiword unit hybrid extraction.
InProceedings of the ACL 2003 Workshop on Multi-word Expressions: Analysis, Acquisition and Treat-ment, pages 41?48, Sapporo, Japan.Stefan Evert and Brigitte Krenn.
2005.
Using smallrandom samples for the manual evaluation of statis-tical association measures.
Computer Speech andLanguage, Special Issue on Multiword Expressions,19(4):450?466.Afsaneh Fazly, Paul Cook, and Suzanne Stevenson.2009.
Unsupervised type and token identification ofidiomatic expressions.
Computational Linguistics,35(1):61?103.Su Nam Kim and Timothy Baldwin.
2007.
Detectingcompositionality of English verb-particle construc-tions using semantic similarity.
In Proceedings ofthe 7th Meeting of the Pacific Association for Com-putational Linguistics (PACLING 2007), pages 40?48, Melbourne, Australia.Mirella Lapata and Alex Lascarides.
2003.
Detect-ing novel compounds: The role of distributional ev-idence.
In Proceedings of the 11th Conference ofthe European Chapter for the Association of Compu-tational Linguistics (EACL-2003), pages 235?242,Budapest, Hungary.Dekang Lin.
1999.
Automatic identification ofnon-compositional phrases.
In Proceedings of the37th annual meeting of the Association for Compu-tational Linguistics on Computational Linguistics,pages 317?324, College Park, USA.Diana McCarthy, Bill Keller, and John Carroll.2003.
Detecting a continuum of compositionalityin phrasal verbs.
In Proceedings of the ACL 2003workshop on Multiword expressions: analysis, ac-quisition and treatment-Volume 18, pages 73?80,Sapporo, Japan.Pavel Pecina.
2008.
Lexical Association Measures:Collocation Extraction.
Ph.D. thesis, Faculty ofMathematics and Physics, Charles University inPrague, Prague, Czech Republic.Karl Pichotta and John DeNero.
2013.
Identify-ing phrasal verbs using many bilingual corpora.
InProceedings of the 2013 Conference on EmpiricalMethods in Natural Language Processing (EMNLP2013), Seattle, USA.Carlos Ramisch.
2012.
A generic framework for mul-tiword expressions treatment: from acquisition toapplications.
In Proceedings of ACL 2012 StudentResearch Workshop, pages 61?66, Jeju Island, Ko-rea.Siva Reddy, Diana McCarthy, and Suresh Manandhar.2011.
An empirical study on compositionality incompound nouns.
In Proceedings of IJCNLP, pages210?218, Chiang Mai, Thailand.Ivan Sag, Timothy Baldwin, Francis Bond, Ann Copes-take, and Dan Flickinger.
2002.
Multiword ex-pressions: A pain in the neck for NLP.
In Pro-ceedings of the 3rd International Conference onIntelligent Text Processing Computational Linguis-tics (CICLing-2002), pages 189?206, Mexico City,Mexico.Bahar Salehi and Paul Cook.
2013.
Predictingthe compositionality of multiword expressions usingtranslations in multiple languages.
In Proceedingsof the Second Joint Conference on Lexical and Com-putational Semantics, volume 1, pages 266?275, At-lanta, USA.480Bahar Salehi, Narjes Askarian, and Afsaneh Fazly.2012.
Automatic identification of Persian light verbconstructions.
In Proceedings of the 13th Inter-national Conference on Intelligent Text ProcessingComputational Linguistics (CICLing-2012), pages201?210, New Delhi, India.Patrick Schone and Dan Jurafsky.
2001.
Is knowledge-free induction of multiword unit dictionary head-words a solved problem.
In Proceedings of the 6thConference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2001), pages 100?108,Hong Kong, China.Sabine Schulte im Walde, Stefan M?uller, and StephenRoller.
2013.
Exploring vector space models topredict the compositionality of German noun-nouncompounds.
In Proceedings of the Second JointConference on Lexical and Computational Seman-tics, Atlanta, USA.Hinrich Sch?utze.
1997.
Ambiguity Resolution in Lan-guage Learning.
CSLI Publications, Stanford, USA.Alex J Smola and Bernhard Sch?olkopf.
2004.
A tu-torial on support vector regression.
Statistics andComputing, 14(3):199?222.Charles Frederick Voegelin and Florence MarieVoegelin.
1977.
Classification and index of theworld?s languages, volume 4.
New York: Elsevier.Claudia von der Heide and Susanne Borgwaldt.
2009.Assoziationen zu Unter, Basis und Oberbegriffen.Eine explorative Studie.
In Proceedings of the 9thNorddeutsches Linguistisches Kolloquium, pages51?74.Julie Elizabeth Weeds.
2003.
Measures and applica-tions of lexical distributional similarity.
Ph.D. the-sis, University of Sussex.481
