Coling 2010: Poster Volume, pages 692?700,Beijing, August 2010Contextual Recommendation based on Text MiningYize Li, Jiazhong Nie, Yi ZhangSchool of EngineeringUniversity of California Santa Cruz{yize,niejiazhong,yiz}@soe.ucsc.eduBingqing WangSchool of Computer Science TechnologyFudan Universitywbq@fudan.edu.cnBaoshi Yan, Fuliang WengResearch and Technology CenterRobert Bosch LLCBaoshi.Yan@us.bosch.comFuliang.Weng@us.bosch.comAbstractThe potential benefit of integrating con-textual information for recommendationhas received much research attention re-cently, especially with the ever-increasinginterest in mobile-based recommendationservices.
However, context based recom-mendation research is limited due to thelack of standard evaluation data with con-textual information and reliable technol-ogy for extracting such information.
Asa result, there are no widely accepted con-clusions on how, when and whether con-text helps.
Additionally, a system of-ten suffers from the so called cold startproblem due to the lack of data for train-ing the initial context based recommenda-tion model.
This paper proposes a novelsolution to address these problems withautomated information extraction tech-niques.
We also compare several ap-proaches for utilizing context based ona new data set collected using the pro-posed solution.
The experimental resultsdemonstrate that 1) IE-based techniquescan help create a large scale context datawith decent quality from online reviews,at least for restaurant recommendations;2) context helps recommender systemsrank items, however, does not help pre-dict user ratings; 3) simply using contextto filter items hurts recommendation per-formance, while a new probabilistic latentrelational model we proposed helps.1 IntroductionIn the information retrieval community, one ma-jor research focus is developing proactive re-trieval agent that acts in anticipation of informa-tion needs of a user and recommends informationto the user without requiring him/her to issue anexplicit query.
The most popular examples of suchkind of proactive retrieval agent are recommendersystems.
Over the last several years, researchin standard recommender systems has been im-proved significantly, largely due to the availabilityof large scale evaluation data sets such as Netflix.The current research focus goes beyond the stan-dard user-item rating matrix.
As researchers startto realize that the quality of recommendations de-pends on time, place and a range of other rele-vant users?
context, how to integrate contextualinformation for recommendation is becoming anever increasingly important topic in the researchagenda (Adomavicius and Ricci, 2009).One major challenge in context-aware recom-mendation research is the lack of large scale an-notated data set.
Ideally, a good research dataset should contain contextual information besidesusers?
explicit ratings on items.
However, suchkinds of data sets are not readily available forresearchers.
Previous research work in contextbased recommendation usually experiments on asmall data set collected through user studies.
Al-though undoubtedly useful, this approach is lim-ited because 1) user studies are usually very ex-pensive and their scales are small; 2) it is very hardfor the research community to repeat such study;and 3) a personalized contextual system may not6921 I was very excited to try this place and my wife took me here on my birthday.
.
.We ordered a side of the brussell sprouts and they were the highlight of the night.2 A friend of mine suggested we meet up here for a night of drinks.
.
.
This actuallya restaurant with a bar in it, but when we went it was 10pm and .
.
.Table 1: Examples of the restaurant reviewssucceed until a user has interacted with it for along period of time to enable context based rec-ommendation models well trained.On the other hand, a large amount of re-view documents from web sites such as tri-padvisor.com, yelp.com, cnet.com, amazon.com,are available with certain contextual information,such as time and companion, implicitly in the re-views (see Table 1 for examples).
This offers us anopportunity to apply information extraction tech-niques for obtaining contextual information fromthe review texts.
Together with users?
explicit rat-ings on items, this might lead to a large researchdata set for context based recommendation andconsequently address the cold start issue in therecommender systems.
This paper describes themethods that extract the contextual informationfrom online reviews and their impact on the rec-ommendation quality at different accuracy levelsof the extraction methods.Another challenge is how to integrate contex-tual information into existing recommendation al-gorithms.
Existing approaches can be classifiedinto three major categories: pre-filtering, post-filtering and the modeling based approaches (Okuet al, 2007; Adomavicius and Tuzhilin, 2008).Pre-filtering approaches utilize contextual infor-mation to select data for that context, and then pre-dict ratings using a traditional recommendationmethod on the selected data (Adomavicius et al,2005).
Post-filtering approaches first predict rat-ings on the whole data using traditional methods,then use the contextual information to adjust re-sults.
Both methods separate contextual informa-tion from the rating estimation process and leadsto unsatisfying findings.
For example, Adomavi-cious et al (2005) found neither standard col-laborative filtering nor contextual reduction-basedmethods dominate each other in all the cases.
Inthe modeling based approaches, contextual infor-mation is used directly in the rating predictionprocess.
For example, Oku et al (2007) proposea context-aware SVM-based predictive model toclassify restaurants into ?positive?
and ?negative?classes, and contextual information is included asadditional input features for the SVM classifier.However, treating recommendation as classifica-tion is not a common approach, and does not takeadvantage of the state of art collaborative filteringtechniques.
In this paper, we propose a new prob-abilistic model to integrate contextual informationinto the state of art factorization based collabora-tive filtering approach, and compare it with sev-eral baselines.2 Mining Contextual Information fromTextual OpinionsThe context includes any information that can beused to characterize the situation of entities.
Ex-amples of context are: location, identity and stateof people, companions, time, activities of the cur-rent user, the devices being used etc.
(Lee etal., 2005).
Without loss of generality, we lookedinto widely available restaurant review data.
Morespecifically, we investigated four types of contex-tual information for a dining event, as they mightaffect users?
dining decisions, and they have notbeen studied carefully before.
The four types ofcontextual information are: Companion (whethera dining event involves multiple people), Occa-sion (for what occasions the event is), Time (whattime during the day) and Location (in which citythe event happens).2.1 Text Mining ApproachesWe developed a set of algorithms along with exist-ing NLP tools (GATE (Cunningham et al, 2002)etc.)
for this task.
More detailed description ofthese algorithms is given below.Time: we classified the meal time into thefollowing types: ?breakfast?, ?lunch?, ?dinner?,?brunch?, ?morning tea?, ?afternoon tea?.
We693compiled a list of lexicons for these different typesof meal times, and used a string matching methodto find the explicit meal times from reviews.
Here,the meal time with an expression, such as ?6pm?,was extracted using ANNIE?s time named entityrecognition module from the GATE toolkit.
Forexample, if a user says, ?When we went there, itwas 10pm?, we infer that it was for dinner.Occasion: The ANNIE?s time named en-tity recognition module recognizes certain specialdays from text.
We augmented ANNIE?s lookupfunction with a list of holidays in the United Statesfrom Wikipedia1 as well as some other occasions,such as birthdays and anniversaries.Location: Ideally, a location context would bea user?s departure location to the selected restau-rant.
However, such information rarely exists inthe review texts.
Therefore, we used the locationinformation from a user?s profile to approximate.Companion: Extracting a companion?s infor-mation accurately from review data is more diffi-cult.
We utilized two methods to address the chal-lenge:Companion-Baseline: This is a string match-ing based approach.
First, we automatically gen-erated a lexicon of different kinds of compan-ion words/phrases by using prepositional patterns,such as ?with my (our) NN (NNS)?.
We extractedthe noun or noun phrases from the prepositionalphrases as the companion terms, which were thensorted by frequency of occurrence and manuallyverified.
This led to a lexicon of 167 entries.Next, we grouped these entries into 6 main cate-gories of companions: ?family?, ?friend?, ?cou-ple?, ?colleague?, ?food-buddy?
and ?pet?.
Fi-nally, the review is tagged as one or more of thecompanion categories if it contains a correspond-ing word/phrase in that lexicon.Companion-Classifier: In order to achieve bet-ter precision, we sampled and annotated 1000sentences with companion terms from the corpusand built three classifiers: 1) a MaxEnt classi-fier with bag-of-words features, 2) a rule-basedclassifier, 3) a hybrid classifier.
For the rule-based classifier, we looked into the structural as-pects of the window where companion terms oc-1http://en.wikipedia.org/wiki/List of holidays bycountry#United States of Americacurred, specifically, the adjacent verbs and prepo-sitions associated with those terms.
We collectedhigh frequency structures including verbs, verb-proposition combinations, and verb-genitive com-binations from the whole corpus, and then con-structed a list of rules to decide whether a compan-ion context exists based on these structures.
Forthe hybrid classifier, we used the patterns identi-fied by the rule-based classifier as features for theMaxEnt model (Ratnaparkhi, 1998).
To train theclassifier, we also included features such as POStags of the verb and of the candidate companionterm, the occurrence of a meal term (e.g.
?lunch?,?dinner?
), the occurrence of pronouns (e.g.
?we?or ?us?)
and the genitive of the companion term.Based on the evaluation results (using 5-fold crossvalidation) shown in Table 2, the hybrid classifieris the best performing classifier and it is used forthe subsequent experiments in the paper.Words Rule HybridPrecision 0.7181 0.7238 0.7379Recall 0.8962 0.8947 0.9143F-Score 0.7973 0.8003 0.8167Table 2: Evaluation results for the bag-of-words-based classifier (Words), the rule-based classifier(Rule) and the hybrid classifier (Hybrid)3 Recommendation based on ContextualInformationNext we consider how to integrate various con-textual information into recommender systems.Assume there are N items and M users.
Eachuser reviews a set of items in the system.
Thedata set can be represented as a set of quadrupletD = (y, i, j, c), where i is the index of user, j isthe index of item, c is a vector describing the con-text of this rating data, and y is the rating value.Let c = (c1, ..., ck), where each component ckrepresents a type of context, such as ?dinner time?or ?location=San Jose?.
The observed features(meta data) of user i and item j are representedas vectors fi and fj respectively, where each com-ponent in the vector represents a type of feature,such as ?gender of the user?
or ?price range ofthe restaurant?.
In the rest of this paper, we in-694tegrate context c into the user?s observed featuresfi.
This makes fi a dynamic feature vector, whichwill change with different context.
The goal isto predict ratings for candidate items given user iand context c, and recommend the top items.
Wepresent two recommendation models for integrat-ing contextual information in this section.3.1 Boolean ModelThe Boolean Model filters out items that do notmatch the context.
The Boolean model itself re-turns an item set instead of a ranked list.
We fur-ther rank the items by predicted rating values.
Wescore items by the Boolean model as follows:s(j) ={sm(j) if item j matches the context??
otherwise(1)where sm(j) is the predicted rating computed us-ing a rating prediction method m, such as a Col-laborative Filtering model without using context.3.2 Probabilistic Latent Relational ModelWe propose a novel Probabilistic Latent Rela-tional Model (PLRM) for integrating contextualinformation.
In a context-aware recommendersystem, a user?s interest for item is influenced bytwo factors: (1) the user?s long-term preference,which can be learned from users?
rating history;(2) the current context (how the item matches thecurrent context).
To capture the two factors si-multaneously, we introduce a new probabilisticmodel by assuming the rating value yi,j,c followsa Gaussian distribution with mean ui,j,c and vari-ance 1/?
(y):yi,j,c ?
N (ui,j,c, 1/?
(y)) (2)ui,j,c = uTi Avj + (Wufi)T (Wvfj) (3)where ui and vj are the hidden representations ofuser i and item j to be learned from rating data,and Wu and Wv are feature transformation matri-ces for users and items respectively.
In Equation(3), the first term uTi Avj is the estimation basedon user?
long term preferences, where A = {a} isa matrix modeling the interaction between ui andvj .2 The second term (Wufi)T (Wvfj) is the esti-2We introduce A matrix so that the model can alsobe used to model multiple different types of relation-mation based on current context and the observedfeatures of users and items, since the context c isintegrated into user?s observed features fi.
{U, V,A,W} are the parameters of the modelto be estimated from the training data set D,where W = {Wu,Wv} = {w} , U ={u1,u2, ...uN} and V = {v1,v2, ...vM}.
We as-sume the prior distribution of the parameters fol-low the Gaussian distributions centered on 0.
Weuse 1/?(u),1/?
(v), 1/?
(w) and 1/?
(a) to representthe variance of the corresponding Gaussian distri-butions.
The effect of the prior distribution is sim-ilar to the ridge regression (norm-2 regularizer)commonly used in machine learning algorithms tocontrol model complexity and avoid overfitting.The proposed model is motivated by well per-forming recommendation models in the literature.It generalizes several existing models.
If we set Ato the identity matrix and Wu,Wv to zero matri-ces, the model presented in Equation (3) is equiv-alent to the well known norm-2 regularized singu-lar value decomposition, which performs well onthe Netflix competition(Salakhutdinov and Mnih,2007).
If we set A to zero matrix and Wu to iden-tity matrix, the Model (3) becomes the bilinearmodel that works well on Yahoo news recommen-dation task (Chu and Park, 2009).Based on the above model assumption, the jointlikelihood of all random variables (U , V , A, Wand D) in the system is:P (U, V,A,W,D) =?
(i,j,c,y)?DP (yi,j,c|ui,vj , fi, fj , A,Wu,Wv)?iP (ui)?jP (vj)P (A)P (Wu)P (Wv)(4)3.3 Parameter EstimationWe use a modified EM algorithm for parame-ter estimation to find the posterior distribution of(U, V ) and max a posterior (MAP) of (A,W ).The estimation can be used to make the final pre-ships/interactions jointly, where each type of relationshipcorresponds to a different A matrix.
For the task in this pa-per, A is not required and can be set to the identity matrixfor simplicity.
However, we leave A as parameters to be es-timated in the rest of this paper for generality.695dictions as follows:y?i,j,c =?ui,vjP (ui)P (vj)(uTi Avj+(Wufi)TWvfj)duidvjE Step: the Variational Bayesian approach is usedto estimate the posterior distributions of U and V .Assuming (A,W ) are known, based on Equation4, we haveP (U, V |A,W,D) ??
(y,i,j,c)?DN (uTi Avj + (Wufi)TWvfj , 1/?
(y))?M?i=1N (ui|0, 1/?
(u)I)N?j=1N (vj |0, 1/?
(v)I)Deriving the exact distribution and use it to predicty will result in intractable integrals.
Thus we ap-proximate the posterior with a variational distribu-tion Q(U, V ) = ?Mi=1Q(ui)?Nj=1Q(vj).
Q(ui)and Q(vj) are restricted to Gaussian distributionsso that predicting y using Bayesian inference withQ(U, V ) will be straightforward.
Q(U, V ) can beestimated by minimizing the KL-divergence be-tween it and P (U, V |A,W,D).
Since Q(U, V ) isfactorized into individual Q(ui) and Q(vj), wecan first focus on one Q(ui) (or Q(vj)) at a timeby fixing/ignoring other factors.
For space consid-erations, we omit the derivation in this paper.
Theoptimal Q(ui) is N (u?i,?i), where u?i = ?idi,?
?1i =?(y,i,j,c)?D?
(y)A(v?jv?Tj + ?j)AT+ ?
(u)Idi =?(y,i,j,c)?D?
(y)y?Av?jSimilarly, the optimal Q(vj) isN (v?j,?j), wherev?j = ?jej ,?
?1j =?(y,i,j,c)?D?
(y)AT (u?iu?Ti + ?i)A+ ?
(v)Iej =?(y,i,j,c)?D?
(y)y?AT v?jM Step: Based on the approximate pos-terior estimation Q(U, V ) derived in the Estep, the maximum a posteriori estimationof {A,W} can be found by maximizingthe expected posterior likelihood {A?, W?}
=argmaxA,W EQ(U,V )(logP (A,W,U, V |D)).This can be done using the conjugate gradientdescent method, and the gradient of A,Wu,Wvcan be calculated as follows:??
?A =?(y,i,j,c)?D?(y)((y?
?
y)u?iv?Tj+ u?iu?Ti A?j + ?iAv?jv?Tj + ?iA?j)+ ?(a)A???Wu=?(y,i,j,c)?D?(y)(y?
?
y)WvfjfTi+ ?(w)Wu???Wv=?(y,i,j,c)?D?(y)(y?
?
y)WufifTj+ ?
(w)Wvwhere ?
= EQ(U,V )(logP (A,W,U, V |D)) andy?
= u?Ti Av?j + (Wufi)TWvfj .4 Experimental Methodology4.1 Data CollectionWe collected an evaluation data set from a pop-ular review web site where users review ser-vices/products and provide integer ratings from 1to 5.
The user profile and the description of items,such as user gender and the category of restau-rants are also collected.
The data set used in thispaper includes the restaurants in Silicon Valley(Bay area) and the users who ever reviewed theserestaurants.
We extract context from the reviewtexts.
The four kinds of context considered in ourpaper are described in Section 2.1.
For each typeof context, we create a subset, in which all reviewscontain the corresponding contextual information.Finally we construct four sub data sets and eachdata set is described by the corresponding con-text type: Time, Location, Occasion and Compan-ion.
We use ?All?
to represent the whole data set.Statistics about each data set are described in Ta-ble 3.696(a) Time (b) Location (c) Occasion(d) Companion (e) AllFigure 1: Performance on the top-K recommendation task.
The plots focus on the top 20% rankingregion.Dataset #Ratings #Users #ItemsAll 756,031 82,892 12,533Location 583,051 56,026 12,155Time 229,321 49,748 10,561Occasion 22,732 12,689 4,135Companion 196,000 47,545 10,246Table 3: Statistics of data4.2 Experimental SetupWe design the experiments to answer the follow-ing questions: 1) Does including contextual in-formation improve the recommendation perfor-mance?
2) How does the probabilistic latent re-lational modeling approach compare with pre-filtering or post-filtering approaches?
3) Howdoes the extraction quality of the contextual infor-mation affect the recommendation performance?To answer the first question, we compare theperformance of the Probabilistic Latent RelationalModel on a standard collaborative filtering settingwhere only rating information is considered, in-dicated by Nocontext.
We also evaluate the per-formance of the Probabilistic Latent RelationalModel when integrating contextual information,indicated by Context-X, where X represents thetype of contextual information considered.
Toanswer the second question, we compare theperformance of Context-X with the pre-filteringBoolean Model, which first uses the context to se-lect items and then ranks them using scores com-puted by Nocontext.
To answer the third question,we compare the recommendation performance fordifferent extraction precision.
The performanceon the following two recommendation tasks arereported in this paper:Top-K Recommendation: We rank the itemsby the predicted rating values and retrieve the topK items.
This task simulates the scenario wherea real recommender system usually suggests a listof ranked K items to a user.
To simulate the sce-nario that we only want to recommend the 5-staritems to users, we treat 5-star rating data in testingdata as relevant.
Ideally, classic IR measures suchas Precision and Recall are used to evaluate therecommendation algorithms.
However, withoutcomplete relevance judgements, standard IR eval-uation is almost infeasible.
Thus we use a varia-tion of the evaluation method proposed by Koren(Koren, 2008).Rating Prediction: Given an active user i and atarget item j, the system predicts the rating of user697Training on Sub Data set Training on the Whole Data setTesting Data ItemAvg Nocontext Context ItemAvg Nocontext ContextTime 1.1517 1.0067 1.0067 1.1052 0.9829 0.9822Companion 1.2657 1.0891 1.0888 1.2012 1.0693 1.0695Occasion 1.2803 1.1381 1.1355 1.2121 1.0586 1.0583Location 1.1597 1.0209 1.0206 1.1597 1.0183 1.0183All context - - - 1.1640 1.0222 1.0219Table 4: RMSE on the rating prediction taskTime CompanionBaseline CompanionClassifier Occasion#Reviews 300 300 300 200#Contexts 115 148 114 207Precision 84.4% 62.2% 77.1% -Recall 80.2% 95.8% 91.7% -F-Score 82.2% 75.4% 83.8% Accuracy 78.3%Table 5: Performance of the context extraction modulei on item j.
The prediction accuracy is measuredby Root Mean Square Error (RMSE), which iscommonly used in collaborative filtering research.This task simulates the scenario that we need toguess a user?s rating about an item, given that theuser has already purchased/selected the item.For each data set (Time, Companion, Location,Occasion and All), we randomly sample 10% fortesting, 80% for training and 10% for validation.5 Experimental Results5.1 Performance on Top-K RecommendationFigure 1(a)-(e) shows the ranking performance oneach data set.
The x-axis is the rank and the y-axisis the portion of relevant products covered by thislevel of rank.
The results across all data sets areconsistent.
With contextual information, PLRMContext-X outperforms Nocontext, whereas usingcontext to pre-filter items (Boolean) does not help.It means that contextual information can help ifused appropriately, however improperly utilizingcontext, such as simply using it as a boolean filter,may hurt the recommendation performance.
Ourproposed PLRM is an effective way to integratecontextual information.5.2 Performance on Rating Prediction TaskTable 4 summaries the RMSE results of differ-ent approaches on the rating prediction task.
TheRMSE of simply using item?s average rating valueas the prediction is also reported as a referencesince it is a commonly used approach by non per-sonalized recommender systems.
For each con-text, we can either train the model only on the sub-set that consists of rating data with related context,or train on a bigger data set by adding the ratingdata without related context.
The results on bothsettings are reported here.
Table 4 shows that uti-lizing context does not affect the prediction accu-racy.
We may wonder why the effects of addingcontext is so different on the rating task comparedwith the ranking task.
One possible explanationis that the selection process of a user is influencedby context, while how the user rates an item afterselecting it is less relevant to context.
For exam-ple, when a user wants to have a breakfast, he mayprefer a cafeteria rather than a formal restaurant.However, how the user rates this cafeteria is morebased on user?s experiences in the cafeteria, suchas quality of services, food, price, environment,etc.5.3 How does Text Mining Accuracy AffectRecommendationTo evaluate the extraction performance on ?Com-panion?, ?Time?
and ?Occasion?, we randomlysample some reviews and evaluate the perfor-698mance on the samples3.
The results are shown inTable 5.
Compared with other contexts, the ex-traction of companion context is more challengingand the string matching baseline algorithm pro-duces significantly inferior results.
However, byusing a MaxEnt classifier with features selection,we can boost the precision of the companion con-text extraction to a level comparable to other con-texts.To further investigate the relationship betweenthe quality of the extracted context and the perfor-mance of the recommender system, we comparethe recommendation performance of Companion-Baseline and Companion-Classifier in Figure1(d).
It shows that improving the quality of theextraction task leads to a significant improvementon the recommender systems?
top-K ranking task.6 ConclusionsReviews widely available online contain a largeamount of contextual information.
This paperproposes to leverage information extraction tech-niques to help recommender systems to trainbetter context-aware recommendation models bymining reviews.
We also introduce a probabilis-tic latent relation model for integrating the cur-rent context and the user?s long term preferences.This model takes the advantages of traditional col-laborative filtering approaches (CF).
It also cap-tures the interaction between contextual informa-tion and item characteristics.
The experimentalresults demonstrate that context is an importantfactor that affects user choices.
If properly used,contextual information helps ranking based rec-ommendation systems, probably because contextinfluences users?
purchasing decisions.
Besides,more accurate contextual information leads to bet-ter recommendation models.
However, contextualinformation does not help the user rating predic-tion task significantly, probably because contextdoesn?t matter much given the user has alreadychosen a restaurant.As the first step towards using the information3We sample 300 reviews for ?Time?
and ?Companion?evaluation.
Due to the extremely low probability of occur-rence of Occasion context, we futher sample 200 reviewscontaining Occasion-related expressions and only evaluateextraction accuracy on these samplesextraction techniques to help contextual recom-mendation, the techniques used in this paper arefar from optimal.
In the future, we will researchmore effective text mining techniques for contex-tual extraction(Mazur and Dale, 2008; McCallumet al, 2000; Lafferty et al, 2001) at the same timeincreasing the amount of annotated review datafor better classifier performance through activelylearning (Laws and Schu?tze, 2008).
We also planto work towards a better understanding of con-textual information in recommender systems, andexplore other types of contextual information indifferent types of recommendation tasks besidesrestaurant recommendations.7 AcknowledgementsPart of this research is funded by National Sci-ence Foundation IIS-0713111 and the Institute ofEducation Science.
Any opinions, findings, con-clusions or recommendations expressed in this pa-per are the authors?, and do not necessarily reflectthose of the sponsors.
Bingqing Wang?s work isdone during his stay in the Research and Technol-ogy Center, Robert Bosch LLC.ReferencesAdomavicius, Gediminas and Francesco Ricci.
2009.Recsys?09 workshop 3: workshop on context-awarerecommender systems, cars-2009.
In Proceedingsof the 3rd ACM Conference on Recommender Sys-tems, RecSys 2009, pages 423?424.Adomavicius, Gediminas and Alexander Tuzhilin.2008.
Context-aware recommender systems.
InProceedings of the 2nd ACM Conference on Rec-ommender Systems, RecSys 2008, pages 335?336.Adomavicius, Gediminas, Ramesh Sankaranarayanan,Shahana Sen, and Alexander Tuzhilin.
2005.Incorporating contextual information in recom-mender systems using a multidimensional approach.ACM Transactions on Information Systems (TOIS),23(1):103?145.Chu, Wei and Seung-Taek Park.
2009.
Personalizedrecommendation on dynamic content using predic-tive bilinear models.
In Proceedings of the 18th In-ternational Conference on World Wide Web, WWW2009, pages 691?700.Cunningham, Hamish, Diana Maynard, KalinaBontcheva, and Valentin Tablan.
2002.
A frame-work and graphical development environment for699robust nlp tools and applications.
In Proceedings ofthe 40th Anniversary Meeting of the Association forComputational Linguistics, ACL 2002, pages 168?175.Koren, Yehuda.
2008.
Factorization meets theneighborhood: a multifaceted collaborative filteringmodel.
In Proceedings of the 14th ACM SIGKDDInternational Conference on Knowledge Discoveryand Data Mining, SIGKDD 2008, pages 426?434.Lafferty, John D., Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the 18th Inter-national Conference on Machine Learning, ICML2001, pages 282?289.Laws, Florian and Hinrich Schu?tze.
2008.
Stoppingcriteria for active learning of named entity recogni-tion.
In Proceedings of the 22nd International Con-ference on Computational Linguistics, Coling 2008,pages 465?472, August.Lee, Hong Joo, Joon Yeon Choi, and Sung Joo Park.2005.
Context-aware recommendations on the mo-bile web.
In On the Move to Meaningful InternetSystems 2005: OTM 2005 Workshops, pages 142?151.Mazur, Pawel and Robert Dale.
2008.
What?s thedate?
high accuracy interpretation of weekdaynames.
In Proceedings of the 22nd InternationalConference on Computational Linguistics, Coling2008, pages 553?560.McCallum, Andrew, Dayne Freitag, and FernandoC.
N. Pereira.
2000.
Maximum entropy markovmodels for information extraction and segmenta-tion.
In Proceedings of the 17th International Con-ference on Machine Learning, ICML 2000, pages591?598.Oku, Kenta, Shinsuke Nakajima, Jun Miyazaki, andShunsuke Uemura.
2007.
Investigation for design-ing of context-aware recommendation system usingsvm.
In Proceedings of the International MultiCon-ference of Engineers and Computer Scientists 2007,IMECS 2007, pages 970?975.Ratnaparkhi, A.
1998.
MAXIMUM ENTROPY MOD-ELS FOR NATURAL LANGUAGE AMBIGUITYRESOLUTION.
Ph.D. thesis, University of Penn-sylvania.Salakhutdinov, Ruslan and Andriy Mnih.
2007.
Prob-abilistic matrix factorization.
In Advances in NeuralInformation Processing Systems 20, Proceedings ofthe 21st Annual Conference on Neural InformationProcessing Systems, NIPS 2007, pages 1257?1264.700
