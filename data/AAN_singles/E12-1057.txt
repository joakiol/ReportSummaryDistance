Proceedings of the 13th Conference of the European Chapter of the Association for Computational Linguistics, pages 561?569,Avignon, France, April 23 - 27 2012. c?2012 Association for Computational LinguisticsThe effect of domain and text type on text prediction qualitySuzan Verberne, Antal van den Bosch, Helmer Strik, Lou BovesCentre for Language StudiesRadboud University Nijmegens.verberne@let.ru.nlAbstractText prediction is the task of suggestingtext while the user is typing.
Its main aimis to reduce the number of keystrokes thatare needed to type a text.
In this paper, weaddress the influence of text type and do-main differences on text prediction quality.By training and testing our text predic-tion algorithm on four different text types(Wikipedia, Twitter, transcriptions of con-versational speech and FAQ) with equalcorpus sizes, we found that there is a cleareffect of text type on text prediction qual-ity: training and testing on the same texttype gave percentages of saved keystrokesbetween 27 and 34%; training on a differ-ent text type caused the scores to drop topercentages between 16 and 28%.In our case study, we compared a num-ber of training corpora for a specific dataset for which training data is sparse: ques-tions about neurological issues.
We foundthat both text type and topic domain playa role in text prediction quality.
Thebest performing training corpus was a setof medical pages from Wikipedia.
Thesecond-best result was obtained by leave-one-out experiments on the test questions,even though this training corpus was muchsmaller (2,672 words) than the other cor-pora (1.5 Million words).1 IntroductionText prediction is the task of suggesting text whilethe user is typing.
Its main aim is to reduce thenumber of keystrokes that are needed to type atext, thereby saving time.
Text prediction algo-rithms have been implemented for mobile devices,office software (Open Office Writer), search en-gines (Google query completion), and in special-needs software for writers who have difficultiestyping (Garay-Vitoria and Abascal, 2006).
In mostapplications, the scope of the prediction is thecompletion of the current word; hence the often-used term ?word completion?.The most basic method for word completion ischecking after each typed character whether theprefix typed since the last whitespace is uniqueaccording to a lexicon.
If it is, the algorithm sug-gests to complete the prefix with the lexicon en-try.
The algorithm may also suggest to complete aprefix even before the word?s uniqueness point isreached, using statistical information on the pre-vious context.
Moreover, it has been shown thatsignificantly better prediction results can be ob-tained if not only the prefix of the current wordis included as previous context, but also previ-ous words (Fazly and Hirst, 2003) or characters(Van den Bosch and Bogers, 2008).In the current paper, we follow up on this workby addressing the influence of text type and do-main differences on text prediction quality.
Briefmessages on mobile devices (such as text mes-sages, Twitter and Facebook updates) are of a dif-ferent style and lexicon than documents typed inoffice software (Westman and Freund, 2010).
Inaddition, the topic domain of the text also influ-ences its content.
These differences may cause analgorithm trained on one text type or domain toperform poorly on another.The questions that we aim to answer in this pa-per are (1) ?What is the effect of text type dif-ferences on the quality of a text prediction algo-rithm??
and (2) ?What is the best choice of train-ing data if domain- and text type-specific data issparse??.
To answer these questions, we performthree experiments:1.
A series of within-text type experiments onfour different types of Dutch text: Wikipediaarticles, Twitter data, transcriptions of con-561versational speech and web pages of Fre-quently Asked Questions (FAQ).2.
A series of across-text type experiments inwhich we train and test on different texttypes;3.
A case study using texts from a specific do-main and text type: questions about neuro-logical issues.
Training data for this combi-nation of language (Dutch), text type (FAQ)and domain (medical/neurological) is sparse.Therefore, we search for the type of trainingdata that gives the best prediction results forthis corpus.
We compare the following train-ing corpora:?
The corpora that we compared in thetext type experiments: Wikipedia, Twit-ter, Speech and FAQ, 1.5 Million wordsper corpus.?
A 1.5 Million words training corpus thatis of the same domain as the target data:medical pages from Wikipedia;?
The 359 questions from the neuro-QAdata themselves, evaluated in a leave-one-out setting (359 times training on358 questions and evaluating on the re-maining questions).The prospective application of the third seriesof experiments is the development of a text predic-tion algorithm in an online care platform: an on-line community for patients seeking informationabout their illness.
In this specific case the targetgroup is patients with language disabilities due toneurological disorders.The remainder of this paper is organized as fol-lows: In Section 2 we give a brief overview of textprediction methods discussed in the literature.
InSection 3 we present our approach to text predic-tion.
Sections 4 and 5 describe the experimentsthat we carried out and the results we obtained.We phrase our conclusions in Section 6.2 Text prediction methodsText prediction methods have been developed forseveral different purposes.
The older algorithmswere built as communicative devices for peoplewith disabilities, such as motor and speech impair-ments.
More recently, text prediction is developedfor writing with reduced keyboards, specificallyfor writing (composing messages) on mobile de-vices (Garay-Vitoria and Abascal, 2006).All modern methods share the general idea thatprevious context (which we will call the ?buffer?
)can be used to predict the next block of charac-ters (the ?predictive unit?).
If the user gets correctsuggestions for continuation of the text then thenumber of keystrokes needed to type the text isreduced.
The unit to be predicted by a text pre-diction algorithm can be anything ranging from asingle character (which actually does not save anykeystrokes) to multiple words.
Single words arethe most widely used as prediction units becausethey are recognizable at a low cognitive load forthe user, and word prediction gives good resultsin terms of keystroke savings (Garay-Vitoria andAbascal, 2006).There is some variation among methods in thesize and type of buffer used.
Most methods usecharacter n-grams as buffer, because they are pow-erful and can be implemented independently of thetarget language (Carlberger, 1997).
In many al-gorithms the buffer is cleared at the start of eachnew word (making the buffer never larger thanthe length of the current word).
In the paperby (Van den Bosch and Bogers, 2008), two ex-tensions to the basic prefix-model are compared.They found that an algorithm that uses the previ-ous n characters as buffer, crossing word borderswithout clearing the buffer, performs better thanboth a prefix character model and an algorithmthat includes the full previous word as feature.
Inaddition to using the previously typed charactersand/or words in the buffer, word characteristicssuch as frequency and recency could also be takeninto account (Garay-Vitoria and Abascal, 2006).Possible evaluation measures for text predic-tion are the proportion of words that are correctlypredicted, the percentage of keystrokes that couldmaximally be saved (if the user would alwaysmake the correct decision), and the time saved bythe use of the algorithm (Garay-Vitoria and Abas-cal, 2006).
The performance that can be obtainedby text prediction algorithms depends on the lan-guage they are evaluated on.
Lower results are ob-tained for higher-inflected languages such as Ger-man than for low-inflected languages such as En-glish (Matiasek et al 2002).
In their overview oftext prediction systems, (Garay-Vitoria and Abas-cal, 2006) report performance scores ranging from29% to 56% of keystrokes saved.An important factor that is known to influencethe quality of text prediction systems, is training562set size (Lesher et al 1999; Van den Bosch,2011).
The paper by (Van den Bosch, 2011) showslog-linear learning curves for word prediction (aconstant improvement each time the training cor-pus size is doubled), when the training set size isincreased incrementally from 102 to 3?107 words.3 Our approach to text predictionWe implement a text prediction algorithm forDutch, which is a productive compounding lan-guage like German, but has a somewhat simplerinflectional system.
We do not focus on the effectof training set size, but on the effect of text typeand topic domain differences.Our approach to text prediction is largely in-spired by (Van den Bosch and Bogers, 2008).
Weexperiment with two different buffer types that arebased on character n-grams:?
?Prefix of current word?
contains all char-acters of only the word currently keyed in,where the buffer shifts by one character posi-tion with every new character.?
?Buffer15?
buffer also includes any othercharacters keyed in belonging to previouslykeyed-in words.Modeling character history beyond the currentword can naturally be done with a buffer model inwhich the buffer shifts by one position per charac-ter, while a typical left-aligned prefix model (thatnever shifts and fixes letters to their positional fea-ture) would not be able to do this.In the buffer, all characters from the text arekept, including whitespace and punctuation.
Thepredictive unit is one token (word or punctuationsymbol).
In both the buffer and the prediction la-bel, any capitalization is kept.
At each point in thetyping process, our algorithm gives one sugges-tion: the word that is the most likely continuationof the current buffer.We save the training data as a classification dataset: each character in the buffer fills a feature slotand the word that is to be predicted is the classi-fication label.
Figures 1 and 2 give examples ofeach of the buffer types Prefix and Buffer15 thatwe created for the text fragment ?tot een niveau?in the context ?stelselmatig bij elke verkiezing toteen niveau van?
?
(structurally with each electionto a level of ).
We use the implementation of theIGTree decision tree algorithm in TiMBL (Daele-mans et al 1997) to train our models.3.1 EvaluationWe evaluate our algorithms on corpus data.
Thismeans that we have to make assumptions aboutuser behaviour.
We assume that the user confirmsa suggested word as soon as it is suggested cor-rectly, not typing any additional characters beforeconfirming.
We evaluate our text prediction al-gorithms in terms of the percentage of keystrokessaved K:K =?ni=0(Fi)??ni=0(Wi)?ni=0(Fi)?
100 (1)in which n is the number of words in the testset, Wi is the number of keystrokes that have beentyped before the word i is correctly suggestedand Fi is the number of keystrokes that would beneeded to type the complete word i.
For example,our algorithm correctly predicts the word niveauafter the context i n g t o t e e n n iv in the test set.
Assuming that the user confirmsthe word niveau at this point, three keystrokeswere needed for the prefix niv.
So, Wi = 3 andFi = 6.
The number of keystrokes needed forwhitespace and punctuation are unchanged: thesehave to be typed anyway, independently of thesupport by a text prediction algorithm.4 Text type experimentsIn this section, we describe the first and second se-ries of experiments.
The case study on questionsfrom the neurological domain is described in Sec-tion 5.4.1 DataIn the text type experiments, we evaluate our textprediction algorithm on four different types ofDutch text: Wikipedia, Twitter data, transcriptionsof conversational speech, and web pages of Fre-quently Asked Questions (FAQ).
The Wikipediacorpus that we use is part of the Lassy cor-pus (Van Noord, 2009); we obtained a versionfrom the summer of 2010.1 The Twitter dataare collected continuously and automatically fil-tered for language by Erik Tjong Kim Sang (TjongKim Sang, 2011).
We used the tweets from allusers that posted at least 19 tweets (excludingretweets) during one day in June 2011.
This isa set of 1 Million Twitter messages from 30,0001http://www.let.rug.nl/vannoord/trees/Treebank/Machine/NLWIKI20100826/COMPACT/563t tott o tott o t tote eene e eene e n eenn niveaun i niveaun i v niveaun i v e niveaun i v e a niveaun i v e a u niveauFigure 1: Example of buffer type ?Prefix?
for the text fragment ?
(elke verkiezing) tot een niveau?.
Un-derscores represent whitespaces.l k e v e r k i e z i n g totk e v e r k i e z i n g t tote v e r k i e z i n g t o totv e r k i e z i n g t o t totv e r k i e z i n g t o t eene r k i e z i n g t o t e eenr k i e z i n g t o t e e eenk i e z i n g t o t e e n eeni e z i n g t o t e e n niveaue z i n g t o t e e n n niveauz i n g t o t e e n n i niveaui n g t o t e e n n i v niveaun g t o t e e n n i v e niveaug t o t e e n n i v e a niveaut o t e e n n i v e a u niveauFigure 2: Example of buffer type ?Buffer15?
for the text fragment ?
(elke verkiezing) tot een niveau?.Underscores represent whitespaces.different users.
The transcriptions of conversa-tional speech are from the Spoken Dutch Corpus(CGN) (Oostdijk, 2000); for our experiments, weonly use the category ?spontaneous speech?.
Weobtained the FAQ data by downloading the first1,000 pages that Google returns for the query ?faq?with the language restriction Dutch.
After clean-ing the pages from HTML and other coding, theresulting corpus contained approximately 1.7 Mil-lion words of questions and answers.4.2 Within-text type experimentsFor each of the four text types, we compare thebuffer types ?Prefix?
and ?Buffer15?.
In each ex-periment, we use 1.5 Million words from the cor-pus to train the algorithm and 100,000 words totest it.
The results are in Table 1.4.3 Across-text type experimentsWe investigate the importance of text type differ-ences for text prediction with a series of experi-ments in which we train and test our algorithm ontexts of different text types.
We keep the size ofthe train and test sets the same: 1.5 Million wordsand 100,000 words respectively.
The results are inTable 2.4.4 Discussion of the resultsTable 1 shows that for all text types, the bufferof 15 characters that crosses word borders givesbetter results than the prefix of the current wordonly.
We get a relative improvement of 35% (forFAQ) to 62% (for Speech) of Buffer15 comparedto Prefix-only.Table 2 shows that text type differences havean influence on text prediction quality: all across-text type experiments lead to lower results thanthe within-text type experiments.
From the re-sults in Table 2, we can deduce that of the fourtext types, speech and Twitter language resem-ble each other more than they resemble the othertwo, and Wikipedia and FAQ resemble each othermore.
Twitter and Wikipedia data are the leastsimilar: training on Wikipedia data makes the textprediction score for Twitter data drop from 29.2 to16.5%.22Note that the results are not symmetric.
For example,564Table 1: Results from the within-text type experiments in terms of percentages of saved keystrokes.Prefix means: ?use the previous characters of the current word as features?.
Buffer 15 means ?use a bufferof the previous 15 characters as features?.Prefix Buffer15Wikipedia 22.2% 30.5%Twitter 21.3% 29.2%Speech 20.7% 33.4%FAQ 20.2% 27.2%Table 2: Results from the across-text type experiments in terms of percentages of saved keystrokes, usingthe best-scoring configuration from the within-text type experiments: a buffer of 15 charactersTrained on Tested on Wikipedia Tested on Twitter Tested on Speech Tested on FAQWikipedia 30.5% 16.5% 22.3% 24.9%Twitter 17.9% 29.2% 27.9% 20.7%Speech 19.7% 22.5% 33.4% 21.0%FAQ 22.6% 18.2% 22.9% 27.2%5 Case study: questions aboutneurological issuesOnline care platforms aim to bring together pa-tients and experts.
Through this medium, patientscan find information about their illness, and get incontact with fellow-sufferers.
Patients who sufferfrom neurological damage may have communica-tive disabilities because their speaking and writ-ing skills are impaired.
For these patients, existingonline care platforms are often not easily accessi-ble.
Aphasia, for example, hampers the exchangeof information because the patient has problemswith word finding.In the project ?Communicatie en revalidatieDigiPoli?
(ComPoli), language and speech tech-nologies are implemented in the infrastructure ofan existing online care platform in order to fa-cilitate communication for patients suffering fromneurological damage.
Part of the online care plat-form is a list of frequently asked questions aboutneurological diseases with answers.
A user canbrowse through the questions using a chat-by-clickinterface (Geuze et al 2008).
Besides reading thelisted questions and answers, the user has the op-tion to submit a question that is not yet included intraining on Wikipedia, testing on Twitter gives a different re-sult from training on Twitter, testing on Wikipedia.
This isdue to the size and domain of the vocabularies in both datasets and the richness of the contexts (in order for the algo-rithm to predict a word, it has to have seen it in the train set).If the test set has a larger vocabulary than the train set, a lowerproportion of words can be predicted than when it is the otherway around.the list.
The newly submitted questions are sent toan expert who answers them and adds both ques-tion and answer to the chat-by-click database.
Intyping the question to be submitted, the user willbe supported by a text prediction application.The aim of this section is to find the best train-ing corpus for newly formulated questions in theneurological domain.
We realize that questionsformulated by users of a web interface are dif-ferent from questions formulated by experts forthe purpose of a FAQ-list.
Therefore, we plan togather real user data once we have a first versionof the user interface running online.
For develop-ing the text prediction algorithm that is behind theinitial version of the application, we aim to findthe best training corpus using the questions fromthe chat-by-click data as training set.5.1 DataThe chat-by-click data set on neurological issuesconsists of 639 questions with corresponding an-swers.
A small sample of the data (translated toEnglish) is shown in Table 3.
In order to create thetest data for our experiments, we removed dupli-cate questions from the chat-by-click data, leavinga set of 359 questions.3In the previous sections, we used corpora of100,000 words as test collections and we calcu-lated the percentage of saved keystrokes over the3Some questions and answers are repeated several timesin the chat-by-click data because they are located at differentplaces in the chat-by-click hierarchy.565Table 3: A sample of the neuro-QA data, translated to English.question 0 505 Can (P)LS be cured?answer 0 505 Unfortunately, a real cure is not possible.
However, things can be done to combat the effects of thediseases, mainly relieving symptoms such as stiffness and spasticity.
The phisical therapist and reha-bilitation specialist can play a major role in symptom relief.
Moreover, there are medications that canreduce spasticity.question 0 508 How is (P)LS diagnosed?answer 0 508 The diagnosis PLS is difficult to establish, especially because the symptoms strongly resemble HSPsymptoms (Strumpell?s disease).
Apart from blood and muscle research, several neurological examina-tions will be carried out.Table 4: Results for the neuro-QA questions only in terms of percentages of saved keystrokes, usingdifferent training sets.
The text prediction configuration used in all settings is Buffer15.
The test samplesare 359 questions with an average length of 7.5 words.
The percentages of saved keystrokes are meansover the 359 questions.Training corpus # words Mean % of saved keystrokes inneuro-QA questions (stdev)OOV-rateTwitter 1.5 Million 13.3% (12.5) 28.5%Speech 1.5 Million 14.1% (13.2) 26.6%Wikipedia 1.5 Million 16.1% (13.1) 19.4%FAQ 1.5 Million 19.4% (15.6) 20.0%Medical Wikipedia 1.5 Million 28.1% (16.5) 7.0%Neuro-QA questions (leave-one-out) 2,672 26.5% (19.9) 17.8%complete test corpus.
In the reality of our casestudy however, users will type only brief frag-ments of text: the length of the question they wantto submit.
This means that there is potentially alarge deviation in the effectiveness of the text pre-diction algorithm per user, depending on the con-tent of the small text they are typing.
Therefore,we decided to evaluate our training corpora sepa-rately on each of the 359 unique questions, so thatwe can report both mean and standard deviationof the text prediction scores on small (realisticallysized) samples.
The average number of words perquestion is 7.5; the total size of the neuro-QA cor-pus is 2,672 words.5.2 ExperimentsWe aim to find the training set that gives the besttext prediction result for the neuro-QA questions.We compare the following training corpora:?
The corpora that we compared in the text typeexperiments: Wikipedia, Twitter, Speech andFAQ, 1.5 Million words per corpus.?
A 1.5 Million words training corpus that isof the same topic domain as the target data:Wikipedia articles from the medical domain;?
The 359 questions from the neuro-QA datathemselves, evaluated in a leave-one-out set-ting (359 times training on 358 questions andevaluating on the remaining questions).In order to create the ?medical Wikipedia?
cor-pus, we consulted the category structure of theWikipedia corpus.
The Wikipedia category ?Ge-neeskunde?
(Medicine) contains 69,898 pages andin the deeper nodes of the hierarchy we see manynon-medical pages, such as trappist beers (or-dered under beer, booze, alcohol, Psychoactivedrug, drug, and then medicine).
If we remove allpages that are more than five levels under the ?Ge-neeskunde?
category root, 21,071 pages are left,which contain fairly over the 1.5 Million wordsthat we need.
We used the first 1.5 Million wordsof the corpus in our experiments.The text prediction results for the different cor-pora are in Table 4.
For each corpus, the out-of-vocabulary rate is given: the percentage of wordsin the Neuro-QA questions that do not occur in thecorpus.45.3 Discussion of the resultsWe measured the statistical significance of themean differences between all text predictionscores using a Wilcoxon Signed Rank test onpaired results for the 359 questions.
We found that4The OOV-rate for the Neuro-QA corpus itself is the av-erage of the OOV-rate of each leave-one-out experiment: theproportion of words that only occur in one question.5660 10 20 30 40 50 600.00.20.40.60.81.0ECDFs for text prediction scores on Neuro?QA questionsusing six different training corporaText prediction scoresCumulativePercentoftest corpusTwitterSpeechWikipediaFAQNeuro?QA (leave?one?out)Medical WikipediaFigure 3: Empirical CDFs for text prediction scores on Neuro-QA data.
Note that the curves that are atthe bottom-right side represent the better-performing settings.the difference between the Twitter and Speech cor-pora on the task is not significant (P = 0.18).The difference between Neuro-QA and MedicalWikipedia is significant with P = 0.02; all otherdifferences are significant with P < 0.01.The Medical Wikipedia corpus and the leave-one-out experiments on the Neuro-QA data givebetter text prediction scores than the other corpora.The Medical Wikipedia even scores slightly betterthan the Neuro-QA data itself.
Twitter and Speechare the least-suited training corpora for the Neuro-QA questions, and FAQ data gives a bit better re-sults than a general Wikipedia corpus.These results suggest that both text type andtopic domain play a role in text prediction qual-ity, but the high scores for the Medical Wikipediacorpus shows that topic domain is even more im-portant than text type.5 The column ?OOV-rate?shows that this is probably due to the high cover-age of terms in the Neuro-QA data by the Medical5We should note here that we did not control for domaindifferences between the four different text types.
They areintended to be ?general domain?
but Wikipedia articles willnaturally be of different topics than conversational speech.Wikipedia corpus.Table 4 also shows that the standard devia-tion among the 359 samples is relatively large.For some questions, we 0% of the keystrokes aresaved, while for other, scores of over 80% are ob-tained (by the Neuro-QA and Medical Wikipediatraining corpora).
We further analyzed the differ-ences between the training sets by plotting the Em-pirical Cumulative Distribution Function (ECDF)for each experiment.
An ECDF shows the devel-opment of text prediction scores (shown on the X-axis) by walking through the test set in 359 steps(shown on the Y-axis).The ECDFs for our training corpora are in Fig-ure 3.
Note that the curves that are at the bottom-right side represent the better-performing settings(they get to a higher maximum after having seena smaller portion of the samples).
From Figure 3,it is again clear that the Neuro-QA and MedicalWikipedia corpora outperform the other trainingcorpora, and that of the other four, FAQ is the best-performing corpus.
Figure 3 also shows a largedifference in the sizes of the starting percentiles:The proportion of samples with a text prediction567Histogram of text prediction scores for the Neuro?QAquestions trained on Medical Wikipediapercentage of keystrokes savedFrequency0 20 40 60 80020406080Figure 4: Histogram of text prediction scoresfor the Neuro-QA questions trained on MedicalWikipedia.
Each bin represents 36 questions.score of 0% is less than 10% for the MedicalWikipedia up to more than 30% for Speech.We inspected the questions that get a text pre-diction score of 0%.
We see many medical termsin these questions, and many of the utterances arenot even questions, but multi-word terms repre-senting topical headers in the chat-by-click data.Seven samples get a zero-score in the output of allsix training corpora, e.g.:?
glycogenose III.?
potassium-aggrevated myotonias.26 samples get a zero-score in the output of alltraining corpora except for Medical Wikipedia andNeuro-QA itself.
These are mainly short headingswith domain-specific terms such as:?
idiopatische neuralgische amyotrofie.?
Markesbery-Griggs distale myopathie.?
oculopharyngeale spierdystrofie.Interestingly, the ECDFs show that the Med-ical Wikipedia and Neuro-QA corpora cross ataround percentile 70 (around the point of 40%saved keystrokes).
This indicates that although themeans of the two result samples are close to eachother, the distribution the scores for the individ-ual questions is different.
The histograms of bothdistributions (Figures 4 and 5) confirm this: thealgorithm trained on the Medical Wikipedia cor-pus leads a larger number of samples with scoresHistogram of text prediction scores for leave?one?outexperiments on Neuro?QA questionspercentage of keystrokes savedFrequency0 20 40 60 80020406080Figure 5: Histogram of text prediction scoresfor leave-one-out experiments on Neuro-QA ques-tions.
Each bin represents 36 questions.around the mean, while the leave-one-out exper-iments lead to a larger number of samples withlow prediction scores and a larger number of sam-ples with high prediction scores.
This is also re-flected by the higher standard deviation for Neuro-QA than for Medical Wikipedia.Since both the leave-one-out training on theNeuro-QA questions and the Medical Wikipedialed to good results but behave differently for dif-ferent portions of the test data, we also evaluated acombination of both corpora on our test set: Wecreated training corpora consisting of the Medi-cal Wikipedia corpus, complemented by 90% ofthe Neuro-QA questions, testing on the remaining10% of the Neuro-QA questions.
This led to meanpercentage of saved keystrokes of 28.6%, not sig-nificantly higher than just the Medical Wikipediacorpus.6 ConclusionsIn Section 1, we asked two questions: (1) ?Whatis the effect of text type differences on the qualityof a text prediction algorithm??
and (2) ?What isthe best choice of training data if domain- and texttype-specific data is sparse?
?By training and testing our text prediction al-gorithm on four different text types (Wikipedia,Twitter, transcriptions of conversational speechand FAQ) with equal corpus sizes, we found thatthere is a clear effect of text type on text predictionquality: training and testing on the same text type568gave percentages of saved keystrokes between 27and 34%; training on a different text type causedthe scores to drop to percentages between 16 and28%.In our case study, we compared a number oftraining corpora for a specific data set for whichtraining data is sparse: questions about neuro-logical issues.
We found significant differencesbetween the text prediction scores obtained withthe six training corpora: the Twitter and Speechcorpora were the least suited, followed by theWikipedia and FAQ corpus.
The highest scoreswere obtained by training the algorithm on themedical pages from Wikipedia, immediately fol-lowed by leave-one-out experiments on the 359neurological questions.
The large differences be-tween the lexical coverage of the medical domainplayed a central role in the scores for the differenttraining corpora.Because we obtained good results by boththe Medical Wikipedia corpus and the neuro-QAquestions themselves, we opted for a combinationof both data types as training corpus in the initialversion of the online text prediction application.Currently, a demonstration version of the appli-cation is running for ComPoli-users.
We hope tocollect questions from these users to re-train ouralgorithm with more representative examples.AcknowledgmentsThis work is part of the research programme?Communicatie en revalidatie digiPoli?
(Com-Poli6), which is funded by ZonMW, the Nether-lands organisation for health research and devel-opment.ReferencesJ.
Carlberger.
1997.
Design and Implementation of aProbabilistic Word Prediciton Program.
Master the-sis, Royal Institute of Technology (KTH), Sweden.W.
Daelemans, A.
Van Den Bosch, and T. Weijters.1997.
IGTree: Using trees for compression and clas-sification in lazy learning algorithms.
Artificial In-telligence Review, 11(1):407?423.A.
Fazly and G. Hirst.
2003.
Testing the efficacy ofpart-of-speech information in word completion.
InProceedings of the 2003 EACL Workshop on Lan-guage Modeling for Text Entry Methods, pages 9?16.6http://lands.let.ru.nl/?strik/research/ComPoli/N.
Garay-Vitoria and J. Abascal.
2006.
Text predictionsystems: a survey.
Universal Access in the Informa-tion Society, 4(3):188?203.J.
Geuze, P. Desain, and J. Ringelberg.
2008.
Re-phrase: chat-by-click: a fundamental new mode ofhuman communication over the internet.
In CHI?08extended abstracts on Human factors in computingsystems, pages 3345?3350.
ACM.G.W.
Lesher, B.J.
Moulton, D.J.
Higginbotham, et al1999.
Effects of ngram order and training text sizeon word prediction.
In Proceedings of the RESNA?99 Annual Conference, pages 52?54.Johannes Matiasek, Marco Baroni, and Harald Trost.2002.
FASTY - A Multi-lingual Approach to TextPrediction.
In Klaus Miesenberger, Joachim Klaus,and Wolfgang Zagler, editors, Computers HelpingPeople with Special Needs, volume 2398 of Lec-ture Notes in Computer Science, pages 165?176.Springer Berlin / Heidelberg.N.
Oostdijk.
2000.
The spoken Dutch corpus:overview and first evaluation.
In Proceedings ofLREC-2000, Athens, volume 2, pages 887?894.Erik Tjong Kim Sang.
2011.
Het gebruik van Twit-ter voor Taalkundig Onderzoek.
In TABU: Bulletinvoor Taalwetenschap, volume 39, pages 62?72.
InDutch.A.
Van den Bosch and T. Bogers.
2008.
Efficientcontext-sensitive word completion for mobile de-vices.
In Proceedings of the 10th international con-ference on Human computer interaction with mobiledevices and services, pages 465?470.
ACM.A.
Van den Bosch.
2011.
Effects of context and re-cency in scaled word completion.
ComputationalLinguistics in the Netherlands Journal, 1:79?94,12/2011.G.
Van Noord.
2009.
Huge parsed corpora in LASSY.In Proceedings of The 7th International Workshopon Treebanks and Linguistic Theories (TLT7).S.
Westman and L. Freund.
2010.
Information Interac-tion in 140 Characters or Less: Genres on Twitter.
InProceedings of the third symposium on InformationInteraction in Context (IIiX), pages 323?328.
ACM.569
