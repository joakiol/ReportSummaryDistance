Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 268?277,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsLearning to Win by Reading Manuals in a Monte-Carlo FrameworkS.R.K.
Branavan David Silver * Regina BarzilayComputer Science and Artificial Intelligence LaboratoryMassachusetts Institute of Technology{branavan, regina}@csail.mit.edu* Department of Computer ScienceUniversity College Londond.silver@cs.ucl.ac.ukAbstractThis paper presents a novel approach for lever-aging automatically extracted textual knowl-edge to improve the performance of controlapplications such as games.
Our ultimate goalis to enrich a stochastic player with high-level guidance expressed in text.
Our modeljointly learns to identify text that is relevantto a given game state in addition to learn-ing game strategies guided by the selectedtext.
Our method operates in the Monte-Carlosearch framework, and learns both text anal-ysis and game strategies based only on envi-ronment feedback.
We apply our approach tothe complex strategy game Civilization II us-ing the official game manual as the text guide.Our results show that a linguistically-informedgame-playing agent significantly outperformsits language-unaware counterpart, yielding a27% absolute improvement and winning over78% of games when playing against the built-in AI of Civilization II.
11 IntroductionIn this paper, we study the task of grounding lin-guistic analysis in control applications such as com-puter games.
In these applications, an agent attemptsto optimize a utility function (e.g., game score) bylearning to select situation-appropriate actions.
Incomplex domains, finding a winning strategy is chal-lenging even for humans.
Therefore, human playerstypically rely on manuals and guides that describepromising tactics and provide general advice aboutthe underlying task.
Surprisingly, such textual infor-mation has never been utilized in control algorithmsdespite its potential to greatly improve performance.1The code, data and complete experimental setup for thiswork are available at http://groups.csail.mit.edu/rbg/code/civ.The natural resources available where a populationsettles affects its ability to produce food and goods.Build your city on a plains or grassland square witha river running through it if possible.Figure 1: An excerpt from the user manual of the gameCivilization II.Consider for instance the text shown in Figure 1.This is an excerpt from the user manual of the gameCivilization II.2 This text describes game locationswhere the action ?build-city?
can be effectively ap-plied.
A stochastic player that does not have accessto this text would have to gain this knowledge thehard way: it would repeatedly attempt this action ina myriad of states, thereby learning the characteri-zation of promising state-action pairs based on theobserved game outcomes.
In games with large statespaces, long planning horizons, and high-branchingfactors, this approach can be prohibitively slow andineffective.
An algorithm with access to the text,however, could learn correlations between words inthe text and game attributes ?
e.g., the word ?river?and places with rivers in the game ?
thus leveragingstrategies described in text to better select actions.The key technical challenge in leveraging textualknowledge is to automatically extract relevant infor-mation from text and incorporate it effectively into acontrol algorithm.
Approaching this task in a super-vised framework, as is common in traditional infor-mation extraction, is inherently difficult.
Since thegame?s state space is extremely large, and the statesthat will be encountered during game play cannot beknown a priori, it is impractical to manually anno-tate the information that would be relevant to thosestates.
Instead, we propose to learn text analysisbased on a feedback signal inherent to the controlapplication, such as game score.2http://en.wikipedia.org/wiki/Civilization II268Our general setup consists of a game in a stochas-tic environment, where the goal of the player is tomaximize a given utility function R(s) at state s.We follow a common formulation that has been thebasis of several successful applications of machinelearning to games.
The player?s behavior is deter-mined by an action-value function Q(s, a) that as-sesses the goodness of an action a in a given states based on the features of s and a.
This function islearned based solely on the utilityR(s) collected viasimulated game-play in a Monte-Carlo framework.An obvious way to enrich the model with textualinformation is to augment the action-value functionwith word features in addition to state and actionfeatures.
However, adding all the words in the docu-ment is unlikely to help since only a small fraction ofthe text is relevant for a given state.
Moreover, evenwhen the relevant sentence is known, the mappingbetween raw text and the action-state representationmay not be apparent.
This representation gap canbe bridged by inducing a predicate structure on thesentence?e.g., by identifying words that describeactions, and those that describe state attributes.In this paper, we propose a method for learning anaction-value function augmented with linguistic fea-tures, while simultaneously modeling sentence rele-vance and predicate structure.
We employ a multi-layer neural network where the hidden layers rep-resent sentence relevance and predicate parsing de-cisions.
Despite the added complexity, all the pa-rameters of this non-linear model can be effectivelylearned via Monte-Carlo simulations.We test our method on the strategy game Civiliza-tion II, a notoriously challenging game with an im-mense action space.3 As a source of knowledge forguiding our model, we use the official game man-ual.
As a baseline, we employ a similar Monte-Carlo search based player which does not have ac-cess to textual information.
We demonstrate that thelinguistically-informed player significantly outper-forms the baseline in terms of number of games won.Moreover, we show that modeling the deeper lin-guistic structure of sentences further improves per-formance.
In full-length games, our algorithm yieldsa 27% improvement over a language unaware base-3Civilization II was #3 in IGN?s 2007 list of top video gamesof all time (http://top100.ign.com/2007/ign top game 3.html)line, and wins over 78% of games against the built-in, hand-crafted AI of Civilization II.42 Related WorkOur work fits into the broad area of grounded lan-guage acquisition where the goal is to learn linguis-tic analysis from a situated context (Oates, 2001;Siskind, 2001; Yu and Ballard, 2004; Fleischmanand Roy, 2005; Mooney, 2008a; Mooney, 2008b;Branavan et al, 2009; Vogel and Jurafsky, 2010).Within this line of work, we are most closely relatedto reinforcement learning approaches that learn lan-guage by proactively interacting with an external en-vironment (Branavan et al, 2009; Branavan et al,2010; Vogel and Jurafsky, 2010).
Like the abovemodels, we use environment feedback (in the formof a utility function) as the main source of supervi-sion.
The key difference, however, is in the languageinterpretation task itself.
Previous work has focusedon the interpretation of instruction text where inputdocuments specify a set of actions to be executed inthe environment.
In contrast, game manuals providehigh-level advice but do not directly describe thecorrect actions for every potential game state.
More-over, these documents are long, and use rich vocabu-laries with complex grammatical constructions.
Wedo not aim to perform a comprehensive interpreta-tion of such documents.
Rather, our focus is on lan-guage analysis that is sufficiently detailed to help theunderlying control task.The area of language analysis situated in a gamedomain has been studied in the past (Eisenstein etal., 2009).
Their method, however, is different bothin terms of the target interpretation task, and the su-pervision signal it learns from.
They aim to learnthe rules of a given game, such as which moves arevalid, given documents describing the rules.
Ourgoal is more open ended, in that we aim to learnwinning game strategies.
Furthermore, Eisenstein etal.
(2009) rely on a different source of supervision ?game traces collected a priori.
For complex games,like the one considered in this paper, collecting suchgame traces is prohibitively expensive.
Thereforeour approach learns by actively playing the game.4In this paper, we focus primarily on the linguistic aspectsof our task and algorithm.
For a discussion and evaluation ofthe non-linguistic aspects please see Branavan et al (2011).2693 Monte-Carlo Framework for ComputerGamesOur method operates within the Monte-Carlo searchframework (Tesauro and Galperin, 1996), whichhas been successfully applied to complex computergames such as Go, Poker, Scrabble, multi-playercard games, and real-time strategy games, amongothers (Gelly et al, 2006; Tesauro and Galperin,1996; Billings et al, 1999; Sheppard, 2002; Scha?fer,2008; Sturtevant, 2008; Balla and Fern, 2009).Since Monte-Carlo search forms the foundation ofour approach, we briefly describe it in this section.Game Representation The game is defined by alarge Markov Decision Process ?S,A, T,R?.
HereS is the set of possible states, A is the space of legalactions, and T (s?|s, a) is a stochastic state transitionfunction where s, s?
?
S and a ?
A.
Specifically, astate encodes attributes of the game world, such asavailable resources and city locations.
At each stepof the game, a player executes an action a whichcauses the current state s to change to a new states?
according to the transition function T (s?|s, a).While this function is not known a priori, the pro-gram encoding the game can be viewed as a blackbox from which transitions can be sampled.
Finally,a given utility function R(s) ?
R captures the like-lihood of winning the game from state s (e.g., anintermediate game score).Monte-Carlo Search Algorithm The goal of theMonte-Carlo search algorithm is to dynamically se-lect the best action for the current state st.
This se-lection is based on the results of multiple roll-outswhich measure the outcome of a sequence of ac-tions in a simulated game ?
e.g., simulations playedagainst the game?s built-in AI.
Specifically, startingat state st, the algorithm repeatedly selects and exe-cutes actions, sampling state transitions from T .
Ongame completion at time ?
, we measure the finalutility R(s?
).5 The actual game action is then se-lected as the one corresponding to the roll-out withthe best final utility.
See Algorithm 1 for details.The success of Monte-Carlo search is based onits ability to make a fast, local estimate of the ac-5In general, roll-outs are run till game completion.
However,if simulations are expensive as is the case in our domain, roll-outs can be truncated after a fixed number of steps.procedure PlayGame ()Initialize game state to fixed starting states1 ?
s0for t = 1 .
.
.
T doRun N simulated gamesfor i = 1 .
.
.
N do(ai, ri)?
SimulateGame(s)endCompute average observed utility for each actionat ?
arg maxa1Na?i:ai=ariExecute selected action in gamest+1 ?
T (s?|st, at)endprocedure SimulateGame (st)for u = t .
.
.
?
doCompute Q function approximationQ(s, a) = ~w ?
~f(s, a)Sample action from action-value function in-greedy fashion:au ?
{uniform(a ?
A) with probability arg maxaQ(s, a) otherwiseExecute selected action in game:su+1 ?
T (s?|su, au)if game is won or lost breakendUpdate parameters ~w of Q(s, a)Return action and observed utility:return at, R(s?
)Algorithm 1: The general Monte-Carlo algorithm.tion quality at each step of the roll-outs.
Statesand actions are evaluated by an action-value func-tion Q(s, a), which is an estimate of the expectedoutcome of action a in state s. This action-valuefunction is used to guide action selection during theroll-outs.
While actions are usually selected to max-imize the action-value function, sometimes other ac-tions are also randomly explored in case they aremore valuable than predicted by the current estimateof Q(s, a).
As the accuracy of Q(s, a) improves,the quality of action selection improves and vice270versa, in a cycle of continual improvement (Suttonand Barto, 1998).In many games, it is sufficient to maintain a dis-tinct action-value for each unique state and actionin a large search tree.
However, when the branch-ing factor is large it is usually beneficial to approx-imate the action-value function, so that the valueof many related states and actions can be learnedfrom a reasonably small number of simulations (Sil-ver, 2009).
One successful approach is to modelthe action-value function as a linear combination ofstate and action attributes (Silver et al, 2008):Q(s, a) = ~w ?
~f(s, a).Here ~f(s, a) ?
Rn is a real-valued feature function,and ~w is a weight vector.
We take a similar approachhere, except that our feature function includes latentstructure which models language.The parameters ~w of Q(s, a) are learned based onfeedback from the roll-out simulations.
Specifically,the parameters are updated by stochastic gradientdescent by comparing the current predicted Q(s, a)against the observed utility at the end of each roll-out.
We provide details on parameter estimation inthe context of our model in Section 4.2.The roll-outs themselves are fully guided by theaction-value function.
At every step of the simula-tion, actions are selected by an -greedy strategy:with probability  an action is selected uniformlyat random; otherwise the action is selected greed-ily to maximize the current action-value function,arg maxaQ(s, a).4 Adding Linguistic Knowledge to theMonte-Carlo FrameworkIn this section we describe how we inform thesimulation-based player with information automat-ically extracted from text ?
in terms of both modelstructure and parameter estimation.4.1 Model StructureTo inform action selection with the advice providedin game manuals, we modify the action-value func-tion Q(s, a) to take into account words of the doc-ument in addition to state and action information.Conditioning Q(s, a) on all the words in the docu-ment is unlikely to be effective since only a smallHidden layer encodingsentence relevanceOutput layerInput layer: Deterministic featurelayer:Hidden layer encodingpredicate labelingFigure 2: The structure of our model.
Each rectan-gle represents a collection of units in a layer, and theshaded trapezoids show the connections between layers.A fixed, real-valued feature function ~x(s, a, d) transformsthe game state s, action a, and strategy document d intothe input vector ~x.
The first hidden layer contains twodisjoint sets of units ~y and ~z corresponding to linguis-tic analyzes of the strategy document.
These are softmaxlayers, where only one unit is active at any time.
Theunits of the second hidden layer ~f(s, a, d, yi, zi) are a setof fixed real valued feature functions on s, a, d and theactive units yi and zi of ~y and ~z respectively.fraction of the document provides guidance relevantto the current state, while the remainder of the textis likely to be irrelevant.
Since this information isnot known a priori, we model the decision about asentence?s relevance to the current state as a hid-den variable.
Moreover, to fully utilize the infor-mation presented in a sentence, the model identifiesthe words that describe actions and those that de-scribe state attributes, discriminating them from therest of the sentence.
As with the relevance decision,we model this labeling using hidden variables.As shown in Figure 2, our model is a four layerneural network.
The input layer ~x represents thecurrent state s, candidate action a, and documentd.
The second layer consists of two disjoint sets ofunits ~y and ~z which encode the sentence-relevanceand predicate-labeling decisions respectively.
Eachof these sets of units operates as a stochastic 1-of-nsoftmax selection layer (Bridle, 1990) where only asingle unit is activated.
The activation function forunits in this layer is the standard softmax function:p(yi = 1|~x) = e~ui?~x/ ?ke~uk?~x,where yi is the ith hidden unit of ~y, and ~ui is theweight vector corresponding to yi.
Given this acti-271vation function, the second layer effectively modelssentence relevance and predicate labeling decisionsvia log-linear distributions, the details of which aredescribed below.The third feature layer ~f of the neural network isdeterministically computed given the active units yiand zj of the softmax layers, and the values of theinput layer.
Each unit in this layer corresponds toa fixed feature function fk(st, at, d, yi, zj) ?
R. Fi-nally the output layer encodes the action-value func-tion Q(s, a, d), which now also depends on the doc-ument d, as a weighted linear combination of theunits of the feature layer:Q(st, at, d) = ~w ?
~f,where ~w is the weight vector.Modeling Sentence Relevance Given a strategydocument d, we wish to identify a sentence yi thatis most relevant to the current game state st and ac-tion at.
This relevance decision is modeled as a log-linear distribution over sentences as follows:p(yi|st, at, d) ?
e~u??
(yi,st,at,d).Here ?
(yi, st, at, d) ?
Rn is a feature function, and~u are the parameters we need to estimate.Modeling Predicate Structure Our goal here isto label the words of a sentence as either action-description, state-description or background.
Sincethese word label assignments are likely to be mu-tually dependent, we model predicate labeling as asequence prediction task.
These dependencies donot necessarily follow the order of words in a sen-tence, and are best expressed in terms of a syn-tactic tree.
For example, words corresponding tostate-description tend to be descendants of action-description words.
Therefore, we label words in de-pendency order ?
i.e., starting at the root of a givendependency tree, and proceeding to the leaves.
Thisallows a word?s label decision to condition on thelabel of the corresponding dependency tree parent.Given sentence yi and its dependency parse qi, wemodel the distribution over predicate labels ~ei as:p(~ei |yi, qi) =?jp(ej |j, ~e1:j?1, yi, qi),p(ej |j, ~e1:j?1, yi, qi) ?
e~v??
(ej ,j,~e1:j?1,yi,qi).Here ej is the predicate label of the jth word beinglabeled, and ~e1:j?1 is the partial predicate labelingconstructed so far for sentence yi.In the second layer of the neural network, theunits ~z represent a predicate labeling ~ei of every sen-tence yi ?
d. However, our intention is to incorpo-rate, into action-value function Q, information fromonly the most relevant sentence.
Thus, in practice,we only perform a predicate labeling of the sentenceselected by the relevance component of the model.Given the sentence selected as relevant and itspredicate labeling, the output layer of the networkcan now explicitly learn the correlations betweentextual information, and game states and actions ?for example, between the word ?grassland?
in Fig-ure 1, and the action of building a city.
This allowsour method to leverage the automatically extractedtextual information to improve game play.4.2 Parameter EstimationLearning in our method is performed in an onlinefashion: at each game state st, the algorithm per-forms a simulated game roll-out, observes the out-come of the game, and updates the parameters ~u,~v and ~w of the action-value function Q(st, at, d).These three steps are repeated a fixed number oftimes at each actual game state.
The informationfrom these roll-outs is used to select the actual gameaction.
The algorithm re-learns Q(st, at, d) for ev-ery new game state st.
This specializes the action-value function to the subgame starting from st.Since our model is a non-linear approximation ofthe underlying action-value function of the game,we learn model parameters by applying non-linearregression to the observed final utilities from thesimulated roll-outs.
Specifically, we adjust the pa-rameters by stochastic gradient descent, to mini-mize the mean-squared error between the action-value Q(s, a) and the final utility R(s? )
for eachobserved game state s and action a.
The resultingupdate to model parameters ?
is of the form:??
= ??2??
[R(s?
)?Q(s, a)]2= ?
[R(s?
)?Q(s, a)]?
?Q(s, a; ?
),where ?
is a learning rate parameter.This minimization is performed via standard errorbackpropagation (Bryson and Ho, 1969; Rumelhart272et al, 1986), which results in the following onlineupdates for the output layer parameters ~w:~w ?
~w + ?w [Q?R(s? )]
~f(s, a, d, yi, zj),where ?w is the learning rate, and Q = Q(s, a, d).The corresponding updates for the sentence rele-vance and predicate labeling parameters ~u and ~v are:~ui ?
~ui + ?u [Q?R(s? )]
Q ~x [1?
p(yi|?
)],~vi ?
~vi + ?v [Q?R(s? )]
Q ~x [1?
p(zi|?
)].5 Applying the ModelWe apply our model to playing the turn-based strat-egy game, Civilization II.
We use the official man-ual 6 of the game as the source of textual strategyadvice for the language aware algorithms.Civilization II is a multi-player game set on a grid-based map of the world.
Each grid location repre-sents a tile of either land or sea, and has variousresources and terrain attributes.
For example, landtiles can have hills with rivers running through them.In addition to multiple cities, each player controlsvarious units ?
e.g., settlers and explorers.
Gamesare won by gaining control of the entire world map.In our experiments, we consider a two-player gameof Civilization II on a grid of 1000 squares, wherewe play against the built-in AI player.Game States and Actions We define the game stateof Civilization II to be the map of the world, the at-tributes of each map tile, and the attributes of eachplayer?s cities and units.
Some examples of the at-tributes of states and actions are shown in Figure 3.The space of possible actions for a given city or unitis known given the current game state.
The actionsof a player?s cities and units combine to form the ac-tion space of that player.
In our experiments, on av-erage a player controls approximately 18 units, andeach unit can take one of 15 actions.
This results ina very large action space for the game ?
i.e., 1021.To effectively deal with this large action space, weassume that given the state, the actions of a singleunit are independent of the actions of all other unitsof the same player.Utility Function The Monte-Carlo algorithm usesthe utility function to evaluate the outcomes of6www.civfanatics.com/content/civ2/reference/Civ2manual.zipMap tile attributes:City attributes:Unit attributes:- Terrain type (e.g.
grassland, mountain, etc)- Tile resources (e.g.
wheat, coal, wildlife, etc)- City population- Amount of food produced- Unit type (e.g., worker, explorer, archer, etc)- Is unit in a city ?1 if action=build-city& tile-has-river=true& action-words={build,city}& state-words={river,hill}0 otherwise1 if action=build-city& tile-has-river=true& words={build,city,river}0 otherwise1 if label=action& word-type='build'& parent-label=action0 otherwiseFigure 3: Example attributes of the game (box above),and features computed using the game manual and theseattributes (box below).simulated game roll-outs.
In the typical applicationof the algorithm, the final game outcome is used asthe utility function (Tesauro and Galperin, 1996).Given the complexity of Civilization II, running sim-ulation roll-outs until game completion is impracti-cal.
The game, however, provides each player with agame score, which is a noisy indication of how wellthey are currently playing.
Since we are playing atwo-player game, we use the ratio of the game scoreof the two players as our utility function.Features The sentence relevance features ~?
and theaction-value function features ~f consider the at-tributes of the game state and action, and the wordsof the sentence.
Some of these features compute textoverlap between the words of the sentence, and textlabels present in the game.
The feature function ~?used for predicate labeling on the other hand oper-ates only on a given sentence and its dependencyparse.
It computes features which are the Carte-sian product of the candidate predicate label withword attributes such as type, part-of-speech tag, anddependency parse information.
Overall, ~f , ~?
and~?
compute approximately 306,800, 158,500, and7,900 features respectively.
Figure 3 shows someexamples of these features.2736 Experimental SetupDatasets We use the official game manual for Civi-lization II as our strategy guide.
This manual uses alarge vocabulary of 3638 words, and is composed of2083 sentences, each on average 16.9 words long.Experimental Framework To apply our method tothe Civilization II game, we use the game?s opensource implementation Freeciv.7 We instrument thegame to allow our method to programmatically mea-sure the current state of the game and to executegame actions.
The Stanford parser (de Marneffe etal., 2006) was used to generate the dependency parseinformation for sentences in the game manual.Across all experiments, we start the game at thesame initial state and run it for 100 steps.
At eachstep, we perform 500 Monte-Carlo roll-outs.
Eachroll-out is run for 20 simulated game steps beforehalting the simulation and evaluating the outcome.For our method, and for each of the baselines, werun 200 independent games in the above manner,with evaluations averaged across the 200 runs.
Weuse the same experimental settings across all meth-ods, and all model parameters are initialized to zero.The test environment consisted of typical PCswith single Intel Core i7 CPUs (4 hyper-threadedcores each), with the algorithms executing 8 simula-tion roll-outs in parallel.
In this setup, a single gameof 100 steps runs in approximately 1.5 hours.Evaluation Metrics We wish to evaluate two as-pects of our method: how well it leverages tex-tual information to improve game play, and the ac-curacy of the linguistic analysis it produces.
Weevaluate the first aspect by comparing our methodagainst various baselines in terms of the percent-age of games won against the built-in AI of Freeciv.This AI is a fixed algorithm designed using exten-sive knowledge of the game, with the intention ofchallenging human players.
As such, it provides agood open-reference baseline.
Since full games canlast for multiple days, we compute the percentage ofgames won within the first 100 game steps as our pri-mary evaluation.
To confirm that performance underthis evaluation is meaningful, we also compute thepercentage of full games won over 50 independentruns, where each game is run to completion.7http://freeciv.wikia.com.
Game version 2.2Method % Win % Loss Std.
Err.Random 0 100 ?Built-in AI 0 0 ?Game only 17.3 5.3 ?
2.7Sentence relevance 46.7 2.8 ?
3.5Full model 53.7 5.9 ?
3.5Random text 40.3 4.3 ?
3.4Latent variable 26.1 3.7 ?
3.1Table 1: Win rate of our method and several baselineswithin the first 100 game steps, while playing against thebuilt-in game AI.
Games that are neither won nor lost arestill ongoing.
Our model?s win rate is statistically signif-icant against all baselines except sentence relevance.
Allresults are averaged across 200 independent game runs.The standard errors shown are for percentage wins.Method % Wins Standard ErrorGame only 45.7 ?
7.0Latent variable 62.2 ?
6.9Full model 78.8 ?
5.8Table 2: Win rate of our method and two baselines on 50full length games played against the built-in AI.7 ResultsGame performance As shown in Table 1, our lan-guage aware Monte-Carlo algorithm substantiallyoutperforms several baselines ?
on average winning53.7% of all games within the first 100 steps.
Thedismal performance, on the other hand, of both therandom baseline and the game?s own built-in AI(playing against itself) is an indicator of the diffi-culty of the task.
This evaluation is an underesti-mate since it assumes that any game not won withinthe first 100 steps is a loss.
As shown in Table 2, ourmethod wins over 78% of full length games.To characterize the contribution of the languagecomponents to our model?s performance, we com-pare our method against two ablative baselines.
Thefirst of these, game-only, does not take advantageof any textual information.
It attempts to model theaction value function Q(s, a) only in terms of theattributes of the game state and action.
The per-formance of this baseline ?
a win rate of 17.3% ?effectively confirms the benefit of automatically ex-tracted textual information in the context of our task.The second ablative baseline, sentence-relevance, is274After the road is built, use the settlers to start improving the terrain.S S AA AA ASWhen the settlers becomes active, chose build road.A AS SS AUse settlers or engineers to improve a terrain square within the city radiusA A A SA SSSSS ?
?Phalanxes are twice as effective at defending cities as warriors.You can rename the city if you like, but we'll refer to it as washington.Build the city on plains or grassland with a river running through it.There are many different strategies dictating the order in which advances are researchedFigure 4: Examples of our method?s sentence relevanceand predicate labeling decisions.
The box above showstwo sentences (identified by check marks) which werepredicted as relevant, and two which were not.
The boxbelow shows the predicted predicate structure of threesentences, with ?S?
indicating state description,?A?
ac-tion description and background words unmarked.
Mis-takes are identified with crosses.identical to our model, but lacks the predicate label-ing component.
This method wins 46.7% of games,showing that while identifying the text relevant tothe current game state is essential, a deeper struc-tural analysis of the extracted text provides substan-tial benefits.One possible explanation for the improved perfor-mance of our method is that the non-linear approx-imation simply models game characteristics better,rather than modeling textual information.
We di-rectly test this possibility with two additional base-lines.
The first, random-text, is identical to our fullmodel, but is given a document containing randomtext.
We generate this text by randomly permut-ing the word locations of the actual game manual,thereby maintaining the document?s overall statisti-cal properties.
The second baseline, latent variable,extends the linear action-value function Q(s, a) ofthe game only baseline with a set of latent variables?
i.e., it is a four layer neural network, where the sec-ond layer?s units are activated only based on gameinformation.
As shown in Table 1 both of these base-lines significantly underperform with respect to ourmodel, confirming the benefit of automatically ex-tracted textual information in the context of this task.Sentence Relevance Figure 4 shows examples ofthe sentence relevance decisions produced by ourmethod.
To evaluate the accuracy of these decisions,we ideally require a ground-truth relevance annota-tion of the game?s user manual.
This however, is20 40 60 80 100Game step00.20.40.60.81SentencerelevanceaccuracySentence relevanceMoving averageFigure 5: Accuracy of our method?s sentence relevancepredictions, averaged over 100 independent runs.impractical since the relevance decision is depen-dent on the game context, and is hence specific toeach time step of each game instance.
Therefore, forthe purposes of this evaluation, we modify the gamemanual by adding to it sentences randomly selectedfrom the Wall Street Journal corpus (Marcus et al,1993) ?
sentences that are highly unlikely to be rel-evant to game play.
We then evaluate the accuracywith which sentences from the original manual arepicked as relevant.In this evaluation, our method achieves an averageaccuracy of 71.8%.
Given that our model only has todifferentiate between the game manual text and theWall Street Journal, this number may seem disap-pointing.
Furthermore, as can be seen from Figure 5,the sentence relevance accuracy varies widely as thegame progresses, with a high average of 94.2% dur-ing the initial 25 game steps.In reality, this pattern of high initial accuracy fol-lowed by a lower average is not entirely surprising:the official game manual for Civilization II is writ-ten for first time players.
As such, it focuses on theinitial portion of the game, providing little strategyadvice relevant to subsequence game play.8 If this isthe reason for the observed sentence relevance trend,we would also expect the final layer of the neuralnetwork to emphasize game features over text fea-tures after the first 25 steps of the game.
This isindeed the case, as can be seen from Figure 6.To further test this hypothesis, we perform an ex-periment where the first 50 steps of the game areplayed using our full model, and the subsequent 50steps are played without using any textual informa-8This is reminiscent of opening books for games like Chessor Go, which aim to guide the player to a playable middle game.27520 40 60 80Game step00.51TextfeatureimportanceTextfeaturesdominateGamefeaturesdominate1.5Figure 6: Difference between the norms of the text fea-tures and game features of the output layer of the neuralnetwork.
Beyond the initial 25 steps of the game, ourmethod relies increasingly on game features.tion.
This hybrid method performs as well as ourfull model, achieving a 53.3% win rate, confirm-ing that textual information is most useful duringthe initial phase of the game.
This shows that ourmethod is able to accurately identify relevant sen-tences when the information they contain is mostpertinent to game play.Predicate Labeling Figure 4 shows examples of thepredicate structure output of our model.
We eval-uate the accuracy of this labeling by comparing itagainst a gold-standard annotation of the game man-ual.
Table 3 shows the performance of our methodin terms of how accurately it labels words as state,action or background, and also how accurately it dif-ferentiates between state and action words.
In ad-dition to showing a performance improvement overthe random baseline, these results display two cleartrends: first, under both evaluations, labeling accu-racy is higher during the initial stages of the game.This is to be expected since the model relies heav-ily on textual features only during the beginning ofthe game (see Figure 6).
Second, the model clearlyperforms better in differentiating between state andaction words, rather than in the three-way labeling.To verify the usefulness of our method?s predi-cate labeling, we perform a final set of experimentswhere predicate labels are selected uniformly at ran-dom within our full model.
This random labelingresults in a win rate of 44% ?
a performance similarto the sentence relevance model which uses no pred-icate information.
This confirms that our methodis able identify a predicate structure which, whilenoisy, provides information relevant to game play.Method S/A/B S/ARandom labeling 33.3% 50.0%Model, first 100 steps 45.1% 78.9%Model, first 25 steps 48.0% 92.7%Table 3: Predicate labeling accuracy of our method and arandom baseline.
Column ?S/A/B?
shows performanceon the three-way labeling of words as state, action orbackground, while column ?S/A?
shows accuracy on thetask of differentiating between state and action words.state: grassland "city"state: grassland "build"action: settlers_build_city "city"action: set_research "discovery"game attribute wordFigure 7: Examples of word to game attribute associa-tions that are learned via the feature weights of our model.Figure 7 shows examples of how this textual infor-mation is grounded in the game, by way of the asso-ciations learned between words and game attributesin the final layer of the full model.8 ConclusionsIn this paper we presented a novel approach forimproving the performance of control applicationsby automatically leveraging high-level guidance ex-pressed in text documents.
Our model, which op-erates in the Monte-Carlo framework, jointly learnsto identify text relevant to a given game state in ad-dition to learning game strategies guided by the se-lected text.
We show that this approach substantiallyoutperforms language-unaware alternatives whilelearning only from environment feedback.AcknowledgmentsThe authors acknowledge the support of the NSF(CAREER grant IIS-0448168, grant IIS-0835652),DARPA Machine Reading Program (FA8750-09-C-0172) and the Microsoft Research New FacultyFellowship.
Thanks to Michael Collins, TommiJaakkola, Leslie Kaelbling, Nate Kushman, SashaRush, Luke Zettlemoyer, the MIT NLP group, andthe ACL reviewers for their suggestions and com-ments.
Any opinions, findings, conclusions, or rec-ommendations expressed in this paper are those ofthe authors, and do not necessarily reflect the viewsof the funding organizations.276ReferencesR.
Balla and A. Fern.
2009.
UCT for tactical assaultplanning in real-time strategy games.
In 21st Interna-tional Joint Conference on Artificial Intelligence.Darse Billings, Lourdes Pen?a Castillo, Jonathan Scha-effer, and Duane Szafron.
1999.
Using probabilis-tic knowledge and simulation to play poker.
In 16thNational Conference on Artificial Intelligence, pages697?703.S.R.K Branavan, Harr Chen, Luke Zettlemoyer, andRegina Barzilay.
2009.
Reinforcement learning formapping instructions to actions.
In Proceedings ofACL, pages 82?90.S.R.K Branavan, Luke Zettlemoyer, and Regina Barzilay.2010.
Reading between the lines: Learning to maphigh-level instructions to commands.
In Proceedingsof ACL, pages 1268?1277.S.R.K.
Branavan, David Silver, and Regina Barzilay.2011.
Non-linear monte-carlo search in civilization ii.In Proceedings of IJCAI.John S. Bridle.
1990.
Training stochastic model recog-nition algorithms as networks can lead to maximummutual information estimation of parameters.
In Ad-vances in NIPS, pages 211?217.Arthur E. Bryson and Yu-Chi Ho.
1969.
Applied optimalcontrol: optimization, estimation, and control.
Blais-dell Publishing Company.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typeddependency parses from phrase structure parses.
InLREC 2006.Jacob Eisenstein, James Clarke, Dan Goldwasser, andDan Roth.
2009.
Reading to learn: Constructingfeatures from semantic abstracts.
In Proceedings ofEMNLP, pages 958?967.Michael Fleischman and Deb Roy.
2005.
Intentionalcontext in situated natural language learning.
In Pro-ceedings of CoNLL, pages 104?111.S.
Gelly, Y. Wang, R. Munos, and O. Teytaud.
2006.Modification of UCT with patterns in Monte-CarloGo.
Technical Report 6062, INRIA.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated cor-pus of english: The penn treebank.
ComputationalLinguistics, 19(2):313?330.Raymond J. Mooney.
2008a.
Learning language from itsperceptual context.
In Proceedings of ECML/PKDD.Raymond J. Mooney.
2008b.
Learning to connect lan-guage and perception.
In Proceedings of AAAI, pages1598?1601.James Timothy Oates.
2001.
Grounding knowledgein sensors: Unsupervised learning for language andplanning.
Ph.D. thesis, University of MassachusettsAmherst.David E. Rumelhart, Geoffrey E. Hinton, and Ronald J.Williams.
1986.
Learning representations by back-propagating errors.
Nature, 323:533?536.J.
Scha?fer.
2008.
The UCT algorithm applied to gameswith imperfect information.
Diploma Thesis.
Otto-von-Guericke-Universita?t Magdeburg.B.
Sheppard.
2002.
World-championship-caliber Scrab-ble.
Artificial Intelligence, 134(1-2):241?275.D.
Silver, R. Sutton, and M. Mu?ller.
2008.
Sample-based learning and search with permanent and tran-sient memories.
In 25th International Conference onMachine Learning, pages 968?975.D.
Silver.
2009.
Reinforcement Learning andSimulation-Based Search in the Game of Go.
Ph.D.thesis, University of Alberta.Jeffrey Mark Siskind.
2001.
Grounding the lexical se-mantics of verbs in visual perception using force dy-namics and event logic.
Journal of Artificial Intelli-gence Research, 15:31?90.N.
Sturtevant.
2008.
An analysis of UCT in multi-playergames.
In 6th International Conference on Computersand Games, pages 37?49.Richard S. Sutton and Andrew G. Barto.
1998.
Rein-forcement Learning: An Introduction.
The MIT Press.G.
Tesauro and G. Galperin.
1996.
On-line policy im-provement using Monte-Carlo search.
In Advances inNeural Information Processing 9, pages 1068?1074.Adam Vogel and Daniel Jurafsky.
2010.
Learning tofollow navigational directions.
In Proceedings of theACL, pages 806?814.Chen Yu and Dana H. Ballard.
2004.
On the integrationof grounding language and learning objects.
In Pro-ceedings of AAAI, pages 488?493.277
