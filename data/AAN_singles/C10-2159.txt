Coling 2010: Poster Volume, pages 1391?1398,Beijing, August 2010Unsupervised Part of Speech Tagging Using Unambiguous Substitutesfrom a Statistical Language ModelMehmet Ali YatbazDept.
of Computer EngineeringKoc?
University?myatbaz@ku.edu.tr dyuret@ku.edu.trDeniz YuretDept.
of Computer EngineeringKoc?
University?AbstractWe show that unsupervised part of speechtagging performance can be significantlyimproved using likely substitutes for tar-get words given by a statistical languagemodel.
We choose unambiguous substi-tutes for each occurrence of an ambiguoustarget word based on its context.
The partof speech tags for the unambiguous sub-stitutes are then used to filter the entry forthe target word in the word?tag dictionary.A standard HMM model trained using thefiltered dictionary achieves 92.25% accu-racy on a standard 24,000 word corpus.1 IntroductionWe define the unsupervised part-of-speech (POS)tagging problem as predicting the correct part-of-speech tag of a word in a given context usingan unlabeled corpus and a dictionary with possi-ble word?tag pairs0 The performance of an un-supervised POS tagging system depends highlyon the quality of the word?tag dictionary (Bankoand Moore, 2004).
We propose a dictionary fil-tering procedure based on likely substitutes sug-gested by a statistical language model.
The pro-cedure reduces the word?tag dictionary size andleads to significant improvement in the accuracyof the POS models.Probabilistic models such as the hidden Markovmodel (HMM) trained by expectation maximiza-tion (EM), maximum a posteriori (MAP) esti-mation, and Bayesian methods have been used0In the POS literature the term ?unsupervised?
is typi-cally used to describe systems that do not directly use thetagged data.
However, many of the unsupervised systems,including ours, uses the tag?word dictionary.to solve the unsupervised POS tagging problem(Merialdo, 1994; Goldwater and Griffiths, 2007).All of these approaches first learn the parametersconnecting the hidden structure to the observedsequence of variables and then identify the mostprobable values of the hidden structure for a givenobserved sequence.
They differ in the way theyestimate the model parameters.
HMM-EM esti-mates model parameters by using the maximumlikelihood estimation (MLE), MAP defines a priordistribution over parameters and finds the param-eter values that maximize the posterior distribu-tion given data, and Bayesian methods integrateover the posterior of the parameters to incorporateall possible parameter settings into the estimationprocess.
Some baseline results and performancereports from the literature are presented in Table 1.
(Johnson, 2007) criticizes the standard EMbased HMM approaches because of their poor per-formance on the unsupervised POS tagging andtheir tendency to assign equal number of wordsto each hidden state.
(Mitzenmacher, 2004) fur-ther claims that words have skewed POS tag dis-tributions, and a Bayesian method with sparse pri-ors over the POS tags may perform better thanHMM estimated with EM.
(Goldwater and Grif-fiths, 2007) uses a fully Bayesian HMM modelthat averages over all possible parameter values.Their model achieves 86.8% tagging accuracywith sparse POS priors and outperforms 74.50%accuracy of the standard second order HMM-EM(3-gram tag model) on a 24K word subset ofthe Penn Treebank corpus.
(Smith and Eisner,2005) take a different approach and use the con-ditional random fields estimated using contrastiveestimation which outperforms the HMM-EM and1391Accuracy System64.2 Random baseline74.4 Second order HMM82.0 First order HMM86.8 Fully Bayesian approach with sparse priors (Goldwater and Griffiths, 2007)88.6 CRF/CE (Smith and Eisner, 2005)91.4 EM-HMM with language specific information, good initialization and manual adjustments to standarddictionary (Goldberg et al, 2008)91.8 Minimized models for EM-HMM with 100 random restarts (Ravi and Knight, 2009).94.0 Most frequent tag baselineTable 1: Tagging accuracy on a 24K-word corpus.
All the systems ?
except (Goldwater and Griffiths,2007) ?
use the same 45 tag dictionary that is constructed from the Penn Treebank.Bayesian methods by achieving 88.6% accuracyon the same 24K corpus.Despite the fact that HMM-EM has a poor repu-tation in POS literature (Goldberg et al, 2008) hasshown that with good initialization together withsome language specific features and language de-pendent constraints HMM-EM achieves 91.4%accuracy.
Aside from the language specific infor-mation and the good initialization, they also man-ually reduce the noise in the word?tag dictionary.
(Ravi and Knight, 2009) focus on the POS tagcollection to find the smallest POS model that ex-plain the data.
They apply integer programmingto construct a minimal bi-gram POS tag set anduse this set to constrain the training phase of theEM algorithm.
The model trained by EM is usedto reduce the dictionary and these steps are iter-atively repeated until no further improvement isobserved.
Their model achieves 91.6% accuracyon the 24K word corpus (with 100 random startsthis goes up to 91.8%).
The main advantage ofthis model is the restriction of the tag set so thatrare POS tags or the noise in the corpus do not getincorporated into the estimation process.Language models for disambiguation: Recentwork has shown that statistical language modelstrained on large amounts of unlabeled text canbe used to improve the performance on variousdisambiguation problems.
The language modelis used to generate likely substitutes for the tar-get word in the given context and these benefitthe disambiguation process to the extent that thelikely substitutes are unambiguous or have dif-ferent ambiguities compared to the target word.Using statistical language models based on largecorpora for unsupervised word sense disambigua-tion and lexical substitution has been explored in(Yuret, 2007; Hawker, 2007; Yuret and Yatbaz,2010).
Unsupervised morphological disambigua-tion in agglutinative languages using likely sub-stitutes has been shown to improve on standardmethods in (Yatbaz and Yuret, 2009).In this paper we use the statistical languagemodel to reduce the possible number of tags perword to help the disambiguation process.
Specif-ically we assume that the same hidden tag se-quence that has generated a particular test sen-tence can also generate artificial sentences whereone of the words has been replaced with a likelysubstitute.
POS tags of the likely substitutes canthen be used to reduce the tag set of the targetword.
Thus, the substitutes are implicitly incorpo-rated into the disambiguation process for reducingthe noise and the rare tags in the dictionary.Currency gyrations can whipsaw(VB/NN) the funds .Currency gyrations can withdraw(VB) the funds .Currency gyrations can restore(VB) the funds .Currency gyrations can modify(VB) the funds .Currency gyrations can justify(VB) the funds .Currency gyrations can regulate(VB) the funds .Table 2: Sample artificial sentences generated fora test sentence from the Penn Treebank.Table 2 presents an example where the likelyunambiguous replacements of the target word?whipsaw?
for a given sentence taken from thePenn Treebank (Marcus et al, 1994) are listed.
Inthis example each substitute is an unambiguousverb (VB), confirming our assumption that eachartificial sentence comes from the same hidden se-quence.
For all occurrences of the word ?whip-saw?, our reduction algorithm will count the POStags of the likely substitutes and remove the tags1392that have not been observed from the dictionary.Assuming that the first sentence in Table 2 is theonly sentence in which we observe ?whipsaw?,the ?NN?
tag of ?whipsaw?
will be removed.The next section describes the details of ourdictionary reduction method.
Section 3 explainsthe details of statistical language model.
We ex-perimentally demonstrate that the word?tag dic-tionary reduced by the substitutes improve theperformance by constraining the unsupervisedmodel in Section 4.
Finally, Section 5 commentson the results and discusses the possible exten-sions of our method.2 Dictionary ReductionOur main assumption is that likely replacementsof a target word should have the same POS tagas the target word in a given context.
Motivatedby this idea we propose a new procedure that au-tomatically reduces the dictionary size by usingthe unambiguous replacements of the target word.For all occurrences of the target word the pro-cedure counts the POS tags of the replacementwords and removes the unobserved POS tags ofthe target word from the dictionary.Our approach is based on the idea that similarwords in a given context should have the same tagsequence.
To reduce the dictionary with the helpof the replacement words similar to a target wordw, we follow three rules:1.
Choose the replacement word from unam-biguous substitutes that are likely to appearin the target word context.2.
Substitutes must be observed in the trainingcorpus.3.
Count the tags of the replacement for all oc-currences of the target word.4.
Remove the tags that are not observed as thetag of replacements in any occurrences of thetarget word.The first rule is used to increase the likelihoodof getting a replacement word with the same POStag.
The second rule makes sure that the size ofthe vocabulary does not change.
The third ruledetermines the unused POS tags in all occurrencesof w and finally, last rule removes the unobservedtags of w from the dictionary.We use the standard first order HMM to test theperformance of our method.
In a standard nth or-der HMM each hidden state is conditioned by itsn preceding hidden states and each observation isconditioned by its corresponding hidden state.
InPOS tagging, the observed variable sequence isa sentence s and the hidden variables ti are thePOS tags of the words wi in s. The HMM pa-rameters ?
can be estimated by using Baum-WelchEM algorithm on an unlabeled training corpus D(Baum, 1972).
The tag sequence that maximizesPr(t|s, ??)
can be identified by the Viterbi searchalgorithm.3 Statistical Language ModelingIn order to estimate highly probable replacementwords for a given wordw in the context cw, we usean n-gram language model.
The context is definedas the 2n?1 word windoww?n+1 .
.
.
w0 .
.
.
wn?1and it is centered at the target word position.
Theprobability of a word in a given context can beestimated as:P (w0 = w|cw) ?
P (w?n+1 .
.
.
w0 .
.
.
wn?1) (1)= P (w?n+1)P (w?n+2|w?n+1).
.
.
P (wn?1|wn?2?n+1) (2)?
P (w0|w?1?n+1)P (w1|w0?n+2).
.
.
P (wn?1|wn?20 ) (3)where wji represents the sequence of wordswiwi+1 .
.
.
wj .
In Equation 1, Pr(w|cw) is pro-portional to Pr(w?n+1 .
.
.
w0 .
.
.
wn+1) since thecontext of the target word replacements is fixed.Terms without w0 are common for every replace-ment in Equation 2 therefore they have beendropped in Equation 3.
Finally, because of theMarkov property of n-gram language model, onlyn?
1 words are used as a conditional context.The probabilities in Equation 3 are estimatedusing a 4 gram language model for all the wordsin the vocabulary of D that are unambiguous andhave a common tag with the target word w. Thewords with the highest Pr(r|cw) where r ?
D areselected as the replacement words of w in cw.1393To get accurate domain independent proba-bility estimates we used the Web 1T data-set(Brants and Franz, 2006), which contains thecounts of word sequences up to length five in a1012 word corpus derived from publicly accessi-ble Web pages.
The SRILM toolkit is used to train5-gram language model (Stolcke, 2002).
The lan-guage model parameters are optimized by using arandomly selected 24K words corpus from PennTreebank.
In order to efficiently apply the lan-guage model to a given test corpus, the vocabularysize is limited to the words seen in the test corpus.4 ExperimentsIn this section we present a number of experi-ments measuring the performance of several vari-ants of our algorithm.
The models in this sec-tion are trained1 and tested on the same unlabeleddata therefore there aren?t any out-of-vocabularywords.
The experiments in this section focus on:(1) the analysis of the dictionary reduction (2) thenumber of the substitutes used for each ambigu-ous word and (3) the size of the word?tag dictio-nary.4.1 DatasetWe trained HMM-EM models on a corpus thatconsists of the first 24K words of the Penn Tree-bank corpus.
To be consistent with the POS tag-ging literature, the tag dictionary is constructed bylisting all observed tags for each word in the entirePenn Treebank.
Nearly 55% of the words in PennTreebank corpus are ambiguous and the averagenumber of tags is 2.3.Groups Member POS tags Count %Noun NN/NNP/NNS/NNPS 7511 31.30Verb VBD/VB/VBZ/VBN/VBG/VBP 3285 13.69Adj JJ/JJR/JJS 1718 7.16Adv RB/RBR 742 3.09Pronoun CD/PRP/PRP$ 1397 5.82Content Noun/Verb/Adj/Adv/Pronoun 14653 61.05Function Other 9347 38.95Total All 45 POS tags 24K 100.00Table 3: Group names, members, number and per-centage of the words according to their gold POStags.1The GMTK tool is used to train HMM-EM model on anunlabeled corpus (Bilmes and Zweig, 2002).Table 3 shows the POS speech groups and theirdistributions in the 24K word corpus.
We reportthe model accuracy on several POS groups.
Ourmotivation is to determine HMM-EM model ac-curacies on the subgroups before and after imple-menting the dictionary reduction procedure.4.2 BaselineTable 4 presents some standard baselines for com-parison.
We define a random and a most frequenttag (MFT) baseline on the 24K corpus.
The ran-dom baseline is calculated by randomly pickingone of the tags of each word and it also representsthe amount of ambiguity in the corpus.
The MFTbaseline simply selects the most frequent POS tagof each word from the 1M word Penn Treebankcorpus (counts of the first 24K words is not in-cluded in the 1M word corpus).
If the target worddoes not exist in the training set, the MFT base-line randomly picks one of the possible tags of themissing word.The first and second order HMMs can betreated as the unsupervised baselines.
These unsu-pervised baselines are calculated by training uni-formly initialized first and second order HMMs onthe target corpus without any smoothing.
All theinitial parameters of HMM-EM are uniformly ini-tialized to observe only the effects of the artificialsentences on the performance of HMM-EM.The success of the MFT baseline on the Noun,Adj, Pronoun and function word groups showsthat tag distributions of the words in these groupsare more skewed towards to one of the availabletags.
The MFT baseline performs poorly, com-pared to the above groups, on V erb, and Advwhich is due to the less skewed POS tag behav-ior of these tags.The POS tagging literature widely uses the sec-ond order HMM as the baseline model; how-ever, the performance of this model can be out-performed by an unsupervised first order HMMmodel or a simple MFT baseline as presented inTable 4.
A point worth noting is that although thefirst order HMM and the MFT baseline have sim-ilar content word accuracies, the MFT baseline issignificantly better on the function words.
Thisis expected since EM tends to assign words uni-formly to the available POS tags.
Thus EM can1394Noun Verb Adj Adv Pronoun Content Function Total(%)Random Baseline 76.98 53.87 68.46 72.98 87.64 71.59 52.64 64.213-gram HMM 77.43 68.16 78.06 73.32 94.85 76.88 70.45 74.382-gram HMM 92.22 83.84 85.22 83.96 95.56 89.42 70.49 82.05MFT Baseline 96.11 80.30 88.56 83.15 98.75 91.28 98.25 93.99Table 4: Percentages of words tagged correctly by different models using standard dictionary.not capture the highly skewed behavior of func-tion words.
Moreover the amount of skewness af-fects the accuracy of the EM such that the perfor-mance gain of the MFT baseline over the first or-der HMM on function words is around 28%-30%while the performance gain on Noun, Adj andPronoun is around 3%-4%.4.3 Reduced DictionaryEM can not capture the sparse structure of theword distributions therefore it tends to assignequal number of words to each POS tag.
Togetherwith the noisy word?tag dictionary great portionof the function words are tagged with very rarePOS tags.
The abuse of the rare tags is presentedin Table 5 in a similar fashion with (Ravi andKnight, 2009).
The count of replacement wordPOS tags and the removed rare POS tags of 2 er-roneous function words are also shown in Table 5.Word Tag Gold EM Replacementdictionary tagging tagging POS countsof {RB,RP,IN} IN(632) IN(0) IN(2377)RP(0) RP(632) RP(0)RB(0) RB(0) RB(850)a {LS,SYM,NNP, DT(458) DT(0) DT(513)FW,JJ,IN,DT} IN(1) IN(0) IN(317)JJ(2) JJ(0) JJ(1329)SYM(1) SYM(258) SYM(0)LS(0) LS(230) LS(0)Table 5: Removed POS tags of the given wordsare shown in bold.The results obtained with the dictionary that isreduced by using 5 replacements are presentedin Table 6.
Note that with reduced dictionarythe uniformly initialized first order HMM-EMachieves 91.85% accuracy.
Dictionary reductionalso removes some of the useful tags thereforethe upper?bound (oracle score) of the 24K datasetbecomes 98.15% after the dictionary reduction.We execute 100 random restarts of the EM algo-rithm and select the model with the highest corpuslikelihood, our model achieves 92.25% accuracywhich is the highest accuracy reported for the 24Kcorpus so far.As Table 6 shows, the effect of the dictionaryreduction on the function words is higher thanthe effect on the content words.
The main reasonfor this situation is, function words are frequentlytagged with one of its tags which is also the reasonfor the high accuracy of the majority voting basedbaseline on the function words.The reduced dictionary (RD) removes the rareproblematic POS tags of the words as a result theaccuracy on the content and function words showsa drastic improvement compared to HMM modelstrained with the original dictionary.Pos 2-gram HMM 2-gram HMM RDgroups accuracy(%) accuracy(%)Noun 92.22 94.01Verb 83.84 84.90Adj 85.22 89.52Adv 83.96 85.18Pronoun 95.56 95.92Content 89.42 91.18Function 70.49 92.92All 82.05 91.85Table 6: Percentages of the correctly taggedwords by different models with modified dictio-nary.
The dictionary size is reduced by using thetop 5 replacements of each target word.4.4 More DataIn this set of experiments we doubled the size ofthe data and trained HMM-EM models on a cor-pus that consists of the first 48K words of the PennTreebank corpus.
Our aim is to observe the effectof more data on our dictionary reduction proce-1395dure.
Using the 5 replacements of each ambigu-ous word we reduce the dictionary and train a newHMM-EM model using this dictionary.
The ad-ditional data together with 100 random starts in-creases the model accuracy to 92.47% on the 48Kcorpus.Pos 3-gram HMM RD 2-gram HMM RDgroups accuracy(%) accuracy(%)Noun 89.45 93.47Verb 85.56 88.99Adj 86.02 87.53Adv 94.44 95.92Pronoun 94.08 94.04Content 88.91 91.97Function 92.44 92.26All 90.31 92.09Table 7: Percentages of the correctly taggedwords by the first and second order HMM-EMmodel trained on the 48K corpus with reduceddictionary.
The dictionary size is reduced by usingthe top 5 replacements of each target word.As we mentioned before, when the model istrained using the original dictionary, the perfor-mance gap between the first order HMM the sec-ond order HMM is around 8% as presented in Ta-ble 4.
On the other hand, when we use the re-duced dictionary together with more data the ac-curacy gap between the second order and the firstorder HMM-EM becomes less than 2% as shownin Table 7.
This confirms the hypothesis that thelow performance of the second order HMM is dueto data sparsity in the 24K-word dataset, and bet-ter results may be achieved with the second orderHMM in larger datasets.4.5 Number of ReplacementsIn this set of experiments we vary the number ofartificial replacement words per each ambiguousword in s. We run our method on the 24K corpuswith 1, 5, 10, 25 and 50 replacement words perambiguous word and we present the results in Ta-ble 8.
The performance of our method affected bythe the number of replacements and highest scoreis achieved when 5 replacements are used.
Incor-porating the probability of the substitutes into themodel rather than using a hard cutoff may be abetter solution.Number of 2-gram HMM RDreplacements accuracy(%)none 82.051 89.655 91.8510 90.0925 89.9750 89.83Table 8: Percentages of the correctly taggedwords by the models trained on the 24K corpuswith different reduced dictionaries.
The dictio-nary size is reduced by using different number re-placements.4.6 17-TagsetTo observe the effect our method on a model withcoarse grained dictionary, we collapsed the 45?tagset treebank dictionary to a 17?tagset coarsedictionary (Smith and Eisner, 2005).
The POSliterature after the work of Smith and Eisner fol-lows this tradition and also tests the models on this17?tagset.
Table 9 summarizes the previously re-ported results on coarse grained POS tagging.
Oursystem achieves 92.9% accuracy where the ora-cle accuracy of 24K dataset with the reduced 17?tagset dictionary is 98.3% and the state-of-the-artsystem IP+EM scores 96.8%.Model Accuracy Data SizeBHMM 87.3 24KCE+spl 88.7 24KRD 92.9 24KLDA+AC 93.4 1MInitEM-HMM 93.8 1MIP+EM 96.8 24KTable 9: Performance of different systems usingthe coarse grained dictionary.The IP+EM system constructs a model that de-scribes the data by using minimum number of bi-gram POS tags then uses this model to reduce thedictionary size (Ravi and Knight, 2009).
InitEM-HMM uses the language specific information to-gether with good initialization and it achieves93.8% accuracy on the 1M word treebank corpus.LDA+AC semi-supervised Bayesian model withstrong ambiguity class component given the mor-phological features of words and scores 93.4% onthe 1M word treebank corpus.
(Toutanova andJohnson, 2007).
CE+spl is HMM model estimated1396by contrastive estimation method and achieves88.7% accuracy (Smith and Eisner, 2005).
Fi-nally, BHMM is a fully Bayesian approach thatuses sparse POS priors and scores 87.3% (Gold-water and Griffiths, 2007).5 ContributionsIn this paper we proposed a dictionary reductionmethod that can be applied to unsupervised tag-ging problems.
With the help of a statistical lan-guage model, our system creates artificial replace-ments that are assumed to have the same POS tagas the target word and use them to reduce the sizeof the word?tag dictionary.
To test our methodwe used HMM-EM as the unsupervised model.Our method significantly improves the predictionaccuracy of the unsupervised first order HMM-EM system in all of the POS groups and achieves92.25% and 92.47% word tagging accuracy on the24K and 48K word corpora respectively.
We alsotested our model on a coarse grained dictionarywith 17 tags and achieved an accuracy of 92.8%.In this work, we show that unambiguous re-placements of an ambiguous word can reduce theamount of the ambiguity thus replacement wordsmight also be incorporated into the other unsuper-vised disambiguation problems.AcknowledgmentsThis work was supported in part by the Scien-tific and Technical Research Council of Turkey(TU?BI?TAK Project 108E228).ReferencesBanko, Michele and Robert C. Moore.
2004.
Part ofspeech tagging in context.
In COLING ?04: Pro-ceedings of the 20th international conference onComputational Linguistics, page 556, Morristown,NJ, USA.
Association for Computational Linguis-tics.Baum, L.E.
1972.
An inequality and associated maxi-mization technique in statistical estimation for prob-abilistic functions of Markov processes.
Inequali-ties, 3(1):1?8.Bilmes, J. and G. Zweig.
2002.
The Graphical ModelsToolkit: An open source software system for speechand time-series processing.
In IEEE InternationalConference On Acoustics Speech and Signal Pro-cessing, volume 4, pages 3916?3919.Brants, T. and A. Franz.
2006.
Web 1T 5-gram Ver-sion 1.
Linguistic Data Consortium, Philadelphia.Goldberg, Y., M. Adler, and M. Elhadad.
2008.
Emcan find pretty good hmm pos-taggers (when givena good start).
Proceedings of ACL-08.
Columbus,OH, pages 746?754.Goldwater, S. and T. Griffiths.
2007.
A fully Bayesianapproach to unsupervised part-of-speech tagging.In Annual Meeting-Assosiation for ComputationalLinguistics, volume 45, page 744.Hawker, Tobias.
2007.
Usyd: Wsd and lexical substi-tution using the web1t corpus.
In Proceedings of theFourth International Workshop on Semantic Eval-uations (SemEval-2007), pages 446?453, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Johnson, M. 2007.
Why doesnt EM find good HMMPOS-taggers.
In Proceedings of the 2007 Joint Con-ference on Empirical Methods in Natural LanguageProcessing and Computational Natural LanguageLearning (EMNLP-CoNLL), pages 296?305.Marcus, M.P., B. Santorini, and M.A.
Marcinkiewicz.1994.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational linguis-tics, 19(2):313?330.Merialdo, B.
1994.
Tagging english text witha probabilistic model.
Computational linguistics,20(2):155?171.Mitzenmacher, M. 2004.
A brief history of generativemodels for power law and lognormal distributions.Internet mathematics, 1(2):226?251.Ravi, Sujith and Kevin Knight.
2009.
Minimizedmodels for unsupervised part-of-speech tagging.
InACL-IJCNLP ?09: Proceedings of the Joint Confer-ence of the 47th Annual Meeting of the ACL and the4th International Joint Conference on Natural Lan-guage Processing of the AFNLP: Volume 1, pages504?512, Morristown, NJ, USA.
Association forComputational Linguistics.Smith, Noah A. and Jason Eisner.
2005.
Contrastiveestimation: training log-linear models on unlabeleddata.
In ACL ?05: Proceedings of the 43rd AnnualMeeting on Association for Computational Linguis-tics, pages 354?362, Morristown, NJ, USA.
Associ-ation for Computational Linguistics.Stolcke, A.
2002.
SRILM-an extensible languagemodeling toolkit.
In Seventh International Confer-ence on Spoken Language Processing, volume 3.1397Toutanova, K. and M. Johnson.
2007.
A BayesianLDA-based model for semi-supervised part-of-speech tagging.
In Proceedings of NIPS, volume 20.Yatbaz, Mehmet Ali and Deniz Yuret.
2009.
Unsuper-vised morphological disambiguation using statisti-cal language models.
In NIPS 2009 Workshop onGrammar Induction, Representation of Languageand Language Learning.Yuret, Deniz and Mehmet Ali Yatbaz.
2010.
Thenoisy channel model for unsupervised word sensedisambiguation.
Computational Linguistics, 36(1),March.Yuret, Deniz.
2007.
KU: Word sense disambigua-tion by substitution.
In Proceedings of the FourthInternational Workshop on Semantic Evaluations(SemEval-2007), pages 207?214, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.1398
