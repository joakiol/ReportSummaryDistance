From Grammar to Lexicon: UnsupervisedLearning of Lexical SyntaxMichae l  R. Brent*Johns Hopkins UniversityImagine a language that is completely unfamiliar; the only means of studying it are an ordinarygrammar book and a very large corpus of text.
No dictionary is available.
How can easily recog-nized, surface grammatical f cts be used to extract from a corpus as much syntactic informationas possible about individual words?
This paper describes an approach based on two principles.First, rely on local morpho-syntactic cues to structure rather than trying to parse entire sentences.Second, treat these cues as probabilistic rather than absolute indicators of syntactic structure.Apply inferential statistics to the data collected using the cues, rather than drawing a categoricalconclusion from a single occurrence of a cue.
The effectiveness ofthis approach for inferring thesyntactic frames of verbs is supported by experiments onan English corpus using a program calledLerner.
Lerner starts out with no knowledge of content words--it bootstraps from determiners,auxiliaries, modals, prepositions, pronouns, complementizers, coordinating conjunctions, andpunctuation.1.
IntroductionThis paper presents a study in the automatic acquisition of lexical syntax from natu-rally occurring English text.
It focuses on discovering the kinds of syntactic phrasesthat can be used to represent the semantic arguments of particular verbs.
For example,want can take an infinitive argument and hope a tensed clause argument, but not viceversa:(1) a.b.c.d.John wants Mary to be happy.John hopes that Mary is happy.
*John wants that Mary is happy.
*John hopes Mary to be happy.This study focuses on the ability of verbs to take arguments represented by infinitives,tensed clauses, and noun phrases erving as both direct and indirect objects.
Theselexical properties are similar to those that Chomsky (1965) termed subcategorizationframes, but to avoid confusion the properties under study here will be referred to assyntactic frames or simply frames.The general framework for the problems addressed in this paper can be thought ofas follows.
Imagine a language that is completely unfamiliar; the only means of study-ing it are an ordinary grammar book and a very large corpus of text (or transcribedspeech).
No dictionary is available.
How can easily recognized, surface grammatical* Department of Cognitive Science, Johns Hopkins University, Baltimore MD 21218;michael@mail.cog.jhu.edu(~) 1993 Association for Computational LinguisticsComputational Linguistics Volume 19, Number 2facts be used to extract from a corpus as much syntactic information as possible aboutindividual words?The scenario outlined above is adopted in this paper as a framework for basicresearch in computational language acquisition.
However, it is also an abstraction ofthe situation faced by engineers building natural anguage processing (NLP) systemsfor more familiar languages.
The lexicon is a central component of NLP systems andit is widely agreed that current lexical resources are inadequate.
Language ngineershave access to some but not all of the grammar, and some but not all of the lexicon.
Themost easily formalized and most reliable grammatical facts tend to be those involvingauxiliaries, rnodals, and determiners, the agreement and case properties of pronouns,and so on.
These vary little from speaker to speaker, topic to topic, register to register.Unfortunately, this information is not sufficient o parse sentences completely, a factthat is underscored by the current state of the parsing art.
If sentences cannot be parsedcompletely and reliably then the syntactic frames used in them cannot be determinedreliably.
How, then, can reliable, easily formalized grammatical information be usedto extract syntactic facts about words from a corpus?This paper suggests the following approach:Do not try to parse sentences completely.
Instead, rely on localmorpho-syntactic cues such as the following facts about English: (1) Theword following a determiner is unlikely to be functioning as a verb;(2) The sequence that the typically indicates the beginning of a clause.Do not try to draw categorical conclusions about a word on the basis ofone or a fixed number of examples.
Instead, attempt o determine thedistribution of exceptions to the expected correspondence b tween cuesand syntactic frames.
Use a statistical model to determine whether thecooccurrence of a verb with cues for a frame is too regular to beexplained by randomly distributed exceptions.The effectiveness of this approach for inferring the syntactic frames of verbs is sup-ported by experiments using an implementation called Lerner.
In the spirit of the prob-lem stated above, Lerner starts out with no knowledge of content words--it bootstrapsfrom determiners, auxiliaries, modals, prepositions, pronouns, complementizers, co-ordinating conjunctions, and punctuation.
Lerner has two independent componentscorresponding to the two strategies listed above.
The first component identities en-tences where a particular verb is likely to be exhibiting a particular syntactic frame.It does this using local cues, such as the that the cue.
This component keeps track ofthe number of times each verb appears with cues for each syntactic frame as well asthe total number of times each verb occurs.
This process can be described as collectingobservations and its output as an observations table.
A segment of an actual observa-tions table is shown in Table 4.
The observations table serves as input to the statisticalmodeler, which ultimately decides whether the accumulated evidence that a particularverb manifests a particular syntactic frame in the input is reliable enough to warranta conclusion.To the best of my knowledge, this is the first attempt o design a system thatautonomously earns syntactic frames from naturally occurring text.
The goal of learn-ing syntactic frames and the learning framework described above lead to three majordifferences between the approach reported here and most recent work in learninggrammar from text.
First, this approach leverages a little a priori grammatical knowl-edge using statistical inference.
Most work on corpora of naturally occurring language244Michael R. Brent From Grammar to Lexiconeither uses no a priori grammatical knowledge (Brill and Marcus 1992; Ellison 1991;Finch and Chater 1992; Pereira and Schabes 1992), or else it relies on a large and com-plex grammar (Hindle 1990, 1991).
One exception is Magerman and Marcus (1991),in which a small grammar is used to aid learning.
1 A second difference is that thework reported here uses inferential rather than descriptive statistics.
In other words,it uses statistical methods to infer facts about the language as it exists in the mindsof those who produced the corpus.
Many other projects have used statistics in a waythat summarizes facts about he text but does not draw any explicit conclusions fromthem (Finch and Chater 1992; Hindle 1990).
On the other hand, Hindle (1991) doesuse inferential statistics, and Brill (1992) recognizes the value of inference, althoughhe does not use inferential statistics per se.
Finally, many other projects in machinelearning of natural anguage use input that is annotated in some way, either withpart-of-speech tags (Brill 1992; Brill and Marcus 1992; Magerman and Marcus 1990) orwith syntactic brackets (Pereira and Schabes 1992).The remainder of the paper is organized as follows.
Section 2describes the morpho-syntactic cues Lerner uses to collect observations.
Section 3 presents the main contribu-tion of this paper--the statistical model and experiments supporting its effectiveness.Finally, Section 4 draws conclusions and lays out a research program in machine learn-ing of natural language.2.
Collecting ObservationsThis section describes the local morpho-syntactic cues that Lerner uses to identifylikely examples of particular syntactic frames.
These cues must address two problems:finding verbs in the input and identifying phrases that represent arguments to theverb.
The next two subsections present cues for these tasks.
The cues presented hereare not intended to be the last word on local cues to structure in English; they aremerely intended to illustrate the feasibility of such cues and demonstrate how thestatistical model accommodates their probabilistic correspondence to the true syntacticstructure of sentences.
Variants of these cues are presented in Brent (1991a, 1991b).
Thefinal subsection summarizes the procedure for collecting observations and discusses asample of the observations table collected from the Brown corpus.2.1 Finding VerbsLerner identifies verbs in two stages, each carried out on a separate pass through thecorpus.
First, strings that sometimes occur as verbs are identified.
Second, occurrencesof those strings in context are judged as likely or unlikely to be verbal occurrences.The second stage is necessary because of lexical ambiguity.The first stage uses the fact that all English verbs can occur both with and withoutthe suffix -ing.
Words are taken as potential verbs if and only if they display thisalternation i  the corpus.
2There are a few words that meet this criterion but do notoccur as verbs, including income~incoming (,incame/incomed), ear~earring, her~herring,and middle~middling.
However, the second stage of verb detection, combined with thestatistical criteria, prevent hese pairs from introducing errors.1 Brill and Marcus (1992) use a single grammatical rule in the test phase to supplement the rules theirsystem learns, but no grammatical knowledge is used in the learning phase.2 Morphological analyzers typically use a root lexicon to resolve the ambiguities in morphologicaladjustment rules (Karttunen 1983).
The system described here uses rules similar to those of Karttunenand Wittenburg (1983), but it resolves the ambiguities using only the contents of the corpus.
Thistechnique will be described in a subsequent paper.245Computational Linguistics Volume 19, Number 2Lerner assumes that a potential verb is functioning as a verb unless the contextsuggests otherwise.
In particular, an occurrence of a potential verb is taken as a non-verbal occurrence only if it follows a determiner or a preposition other than to.
Forexample, was talking would be taken as a verb, but a talk would not.
This precautionreduces the likelihood that a singular count noun will be mistaken for a verb, sincesingular count nouns are frequently preceded by a determiner.Finally, the only morphological forms that are used for learning syntactic framesare the stem form and the -ing form.
There are several reasons for this.
First, formsending in -s are potentially ambiguous between third person singular present verbsand plural nouns.
Since plural nouns are not necessarily preceded by determiners(I like to take walks), they could pose a significant ambiguity problem.
Second, pastparticiples do not generally take direct objects: knows me and knew me are OK, butnot ?
is known me.
Further, the past tense and past participle forms of some verbs areidentical, while those of others are distinct.
As a result, using the -ed forms wouldhave complicated the statistical model substantially.
Since the availability of raw textis not generally a limiting factor, it makes sense to wait for the simpler cases.2.2 Identifying Argument PhrasesWhen a putative occurrence of a verb is found, the next step is to identify the syntactictypes of nearby phrases and determine whether or not they are likely to be argumentsof the verb.First, assume that a phrase P and a verb V have been identified in some sentence.Lerner's trategy for determining whether P is an argument to V has two components:.2.If P is a noun phrase (NP), take it as an argument only if there isevidence that it is not the subject of another clause.Regardless of P's category, take it as an argument only if it occurs to theright of V and there are no potential attachment points for P between Vand P.For example, suppose that the sequence that the were identified as the left boundary ofa clause in the sentence I want to tell him that the idea won'tfly.
Because pronouns likehim almost never take relative clauses, and because pronouns are known at the outset,Lerner concludes that the clause beginning with that the is probably an argument ofthe verb tell.
3 It is always possible that it could be an argument of the previous verbwant, but Lerner treats that as unlikely.
On the other hand, if the sentence were I wantto tell the boss that the idea won'tfly, then Lerner cannot determine whether the clausebeginning with that the is an argument to tell or is instead related to boss, as in I wantto fire the boss that the workers don't trust.Now consider specific cues for identifying argument phrases.
The phrase typesfor which data are reported here are noun phrases, infinitive verb phrases (VPs), andtensed clauses.
These phrase types yield three syntactic frames with a single argumentand three with two arguments, as shown in Table 1.
The cues used for identifyingthese frames are shown in Tables 2 and 3.
Table 2 defines lexical categories that arereferred to in Table 3.
The category V in Table 3 starts out empty and is filled as verbsare detected on the first pass.
"cap" stands for any capitalized word and "cap+" forany sequence of capitalized words.
These cues are applied by matching them againstthe string of words immediately to the right of each verb.
For example, a verb V is3 Thanks to Don Hindle for this observation (personal communication).246Michael R. Brent From Grammar to LexiconTable 1The six syntactic frames studied in this paper.SF Description Good Example Bad ExampleNP only greet them *arrive themtensed clause hope he'll attend *want he'll attendinfinitive hope to attend *greet to attendNP & clause tell him he's a fool *yell him he's a foolNP & infinitive want him to attend *hope him to attendNP & NP tell him the story *shout him the storyTable 2Lexical categories used in the definitions of the cues.SUBJ: I I he I she I we \[ theyOBJ: me \[ him \[ us \[ themSUBJ OBJ: you \[ it \[ yours \[ hers \[ ours \[ theirsDET: a \[ an \[ the \[ her \[ hiS \[ its \[ my\[ our \[ their \[ your \[ this \[ that \[ whose+TNS: has \[ have\[ had \[ am \[ is\[ are \[ was \[ were \[ do \[does \[ did \[ can \[ could \[ may \[ might \[ must \[wouldCC: when \[ before \[ after \[ as \[ while \[ ifPUNC: \[ ?
\[ !
\[ , \[ ; \[ :will \[Table 3Cues for syntactic frames.
The category V is initially empty and is filledout during the first pass.
"cap" stands for any capitalized word and "cap+"stands for any sequence of capitalized words.Frame Symbol CuesNP only NP (0BJ \[ SUBJ_0BJ I cap) (PUNC i CC)Tensed Clause c l  ( that  (DET \] SUBJ \] SUBJ_0BJ \[ cap+))SUBJ I(SUBJ_0BJ +TNS)Infinitive VP inf  to VNP & clause NPcl (0BJ i SUBJ_0BJ \] cap+) c lNP & infinitive NPinf (0BJ \[ SUBJ_0BJ \[ cap+) infNP & NP (dat.)
NPNP (0nJ I SUBJ_OBJ I cap+) NP247Computational Linguistics Volume 19, Number 2Table 4A sample of the data collected from the untagged Brown Corpus using the cues of Table 3.V NP NPNP NPcl NPinf cl inf V NP NPNP NPcl NPinf cl infrecall 42 3 4 recur 5recede 5 redeem 3receive 106 4 redirect 2reckon 10 rediscover 2recognize 71 6 6 reduce 85 2recommend 32 2 1 reek 2reconcile 5 reel 2record 97 2 2 refer 43 1recount 5 refine 4recover 14 4 1 reflect 41 1recreate 2 refresh 4recruit 11 refuel 3refuse 22 1recorded as having occurred with a direct object and no other argument phrase if Vis followed by a pronoun of ambiguous case and then a coordinating conjunction, asin I'll see you when you return from Mexico.
The coordinating conjunction makes itunlikely that the pronoun is the subject of another clause, as in I see you like champagne.It also makes it unlikely that the verb has an additional NP argument, as in I'II tell youmy secret recipe.2.3 Summary and Sample DataTo summarize, the procedure for collecting observations from a corpus is as follows:1.
Go through the corpus once finding pairs of words such that one is theresult of adding the suffix -ing to the other, applying appropriatemorphological djustment rules.
List members of such pairs as verbs.2.
Go through the corpus again.
At each word w that is on the list of verbs,(a) If w is not preceded by a preposition or a determiner, incrementthe number of times that w appears as a verb.
(b) If any of the cues listed in Table 3 match the words immediatelyfollowing w, increment the number of times that w appears tooccur in the corresponding frame.3.
Combine the data for the stem form and the -ing form.Table 4 shows an alphabetically contiguous portion of the observations table thatresults from applying this procedure to the Brown Corpus (untagged).
Each row rep-resents data collected from one pair of words, including both the -ing form and thestem form.
The first column, titled V, represents he total number of times the wordoccurs in positions where it could be functioning as a verb.
Each subsequent columnrepresents a single frame.
The number appearing in each row and column representsthe number of times that the row's verb cooccurred with cues for the column's frame.Zeros are omitted.
Thus recall and recalling occurred a combined total of 42 times,excluding those occurrences that followed determiners or prepositions.
Three of thoseoccurrences were followed by a cue for a single NP argument and four were followedby cues for a tensed clause argument.248Michael R. Brent From Grammar to LexiconTable 5Judgments based on the observations in Table 4, made bythe method of Section 3.recall NP, clrecognize NP, clrecover NPrefuse infThe cues are fairly rare, so verbs in Table 4 that occur fewer than 15 times tendnot to occur with these cues at all.
Further, these cues occur fairly often in structuresother than those they are designed to detect.
For example, record, recover, and refer alloccurred with cues for an infinitive, although none of them in fact takes an infinitiveargument.
The sentences responsible for these erroneous observations are:(2) (a)(b)(c)(d)But I shall campaign on the Meyner record to meet the needs ofthe years ahead.Sposato needed a front, some labor stiff with a clean record toact as business agent of the Redhook local.Then last season the Birds tumbled as low as 11-18 on May 19before recovering to make a race of it and total 86 victories.But I suspect hat the old Roman was referr ing to change madeunder military occupat ion--the sort of change which Tacituswas talking about when .
.
.
.In (2a,b) record occurs as a noun.
In (2c) recover is a verb but the infinitive VP, to make arace of it .
.
.
.
does not appear to be an argument.
In any case, it does not bear the samerelation to the verb as the infinitive arguments of verbs like try, want, hope, ask, andrefuse.
In (2d) refer is a verb but to change is a PP rather than an infinitive.The remainder of this paper  describes and evaluates a method for making judg-ments about the ability of verbs to appear in particular syntactic frames on the basisof noisy data like that of Table 4.
Given the data in Table 4, that method yields thejudgments in Table 5.3.
Statistical ModelingAs noted above, the correspondence between syntactic structure and the cues thatLerner uses is not perfect.
Mismatches between cue and structure are problematicbecause naturally occurring language provides no negative evidence.
If a V verb isfol lowed by a cue for some syntactic frame S, that provides evidence that V does occurin frame S, but there is no analogous source of evidence that V does not occur inframe S.The occurrence of mismatches between cue and structure can be thought of asa random process where each occurrence of a verb V has some non-zero probabil ityof being followed by a cue for a frame S, even if V cannot in fact occur in S. If thismodel is accurate, the more times V occurs, the more likely it is to occur at least oncewith a cue for S. The intransitive verb arrive, for example, will eventually occur witha cue for an NP argument, if enough text is considered.
A learner that considers asingle occurrence of verb followed by a cue to be conclusive vidence will eventuallycome to the false conclusion that arrive is transitive.
In other words, the information249Computational Linguistics Volume 19, Number 2provided by the cues will eventually be washed out by the noise.
This problem isinherent in learning from naturally occurring language, since infallible parsing is notpossible.
The only way to prevent it is to consider the frequency with which eachverb occurs with cues for each frame.
In other words, to consider each occurrence ofV without a cue for S as a small bit of evidence against V being able to occur in frameS.
This section describes a statistical technique for weighing such evidence.Given a syntactic frame S, the statistical model treats each verb V as analogousto a biased coin and each occurrence of V as analogous to a flip of that coin.
Anoccurrence that is followed by a cue for S corresponds to one outcome of the coin flip,say heads; an occurrence without a cue for S corresponds to tails.
4 If the cues wereperfect predictors of syntactic structure then a verb V that does not in fact occur inframe S would never appear with cues for S--the coin would never come up heads.Since the cues are not perfect, such verbs do occur with cues for S. The problem is todetermine when a verb occurs with cues for S often enough that all those occurrencesare unlikely to be errors.In the following discussion, a verb that in fact occurs in frame S in the input isdescribed as a +S verb; one that does not is described as a -S  verb.
The statisticalmodel is based on the following approximation: for fixed S, all -S  verbs have equalprobability of being followed by a cue for S. Let ~r-s stand for that probability.
~r-smay vary from frame to frame, but not from verb to verb.
Thus, errors might bemore common for tensed clauses than for NPs, but the working hypothesis is thatall intransitives, uch as saunter and arrive, are about equally likely to be followed bya cue for an NP argument.
If the error probability ~r-s were known, then we coulduse the standard hypothesis testing method for binomial frequency data.
For example,suppose 7r-s = .05--on average, one in twenty occurrences of a -S  verb is followed bya cue for S. If some verb V occurs 200 times in the corpus, and 20 of those occurrencesare followed by cues for S, that ought to suggest hat V is unlikely to have probability.05 of being followed by a cue for S, and hence V is unlikely to be -S.  Specifically,the chance of flipping 20 or more heads out of 200 tosses of a coin with a five percentchance of coming up heads each time is less than three in 1000.
On the other hand, it isnot all that unusual to flip 2 or more heads out of 20 on such a coin-- it  happens aboutone time in four.
If a verb occurs 20 times in the corpus and 2 of those occurrencesare followed by cues for S, it is quite possible that V is -S  and that the 2 occurrenceswith cues for S are explained by the five percent error rate on -S  verbs.The next section reviews the hypothesis-testing method and gives the formulas forcomputing the probabilities of various outcomes of coin tosses, given the coin's bias.It also provides empirical evidence that, for some values of 7r_s, hypothesis-testingdoes a good job of distinguishing +S verbs from -S  verbs that occur with cues forS because of mismatches between cue and structure.
The following section proposesa method for estimating ~r-s and provides empirical evidence that its estimates arenearly optimal.3.1 Hypothesis TestingThe statistical component of Lerner is designed to prevent he information providedby the cues from being washed out by the noise.
The basic approach is hypothesistesting on binomial frequency data (Kalbfleisch 1985).
Specifically, a verb V is shown to4 Given a verb V, the outcomes of the coins for different S's are treated as approximately independent,even though they cannot be perfectly independent.
Their dependence ould be modeled using amultinomial rather than a binomial model, but the experimental data suggest that this is unnecessary.250Michael R. Brent From Grammar to Lexiconbe +S by assuming that it is -S  and then showing that if this were true, the observedpattern of cooccurrence of V with cues for S would be extremely unlikely.3.1.1 Binomial Frequency Data.
In order to use the hypothesis testing method weneed to estimate the probability ~r-s that an occurrence of a verb V will be followedby a cue for S if V is -S. In this section it is assumed that 7r_s is known.
The nextsection suggests a means of estimating Tr_s.
In both sections it is also assumed that foreach +S verb, V, the probability that V will be followed by a cue for S is greater than7r_s.
Other than that, no assumptions are made about the probability that a +S verbwill be followed by a cue for S. For example, two verbs with transitive senses, such ascut and walk, may have quite different frequencies of cooccurrence with cues for NP.It does not matter what these frequencies are as long as they are greater than lr_Np.If a coin has probability p of flipping heads, and if it is flipped n times, the prob-ability of its coming up heads exactly m times is given by the binomial distribution:n~P(m,n,p) - m!
(n - m)!
pm(1 - p)n-m (1)The probability of coming up heads m or more times is given by the obvious sum:nP(m+, n, p) = ~ P(i, n, p) (2)i=mAnalogously, P(m+, n, ~r-s) gives the probability that m or more occurrences of a -Sverb V will be followed by a cue for S out of n occurrences total.If m out of n occurrences of V are followed by cues for S, and if P(m+, n, ~r-s)is quite small, then it is unlikely that V is -S .
Traditionally, a threshold less than orequal to .05 is set such that a hypothesis is rejected if, assuming the hypothesis weretrue, the probability of outcomes as extreme as the observed outcome would be belowthe threshold.
The confidence attached to this conclusion increases as the thresholddecreases.3.1.2 Experiment.
The experiment presented in this section is aimed at determininghow well the method presented above can distinguish +S verbs from -S  verbs.
Letp-s be an estimate of 7r_s.
It is conceivable that P(m+,n,p-s) might not be a goodpredictor of whether or not a verb is +S, regardless of the estimate p-s. For example,if the correspondence b tween the cues and the structures they are designed to detectwere quite weak, then many -S  verbs might have lower P(m+,n,p-s) than many+S verbs.
This experiment measures the accuracy of binomial hypothesis testing onthe data collected by Lerner's cues as a function of p-s.
In addition to showing thatP(m+, n, P-s) is good for distinguishing +S and -S  verbs, these data provide a baselineagainst which to compare methods for estimating the error rate 7r_s.Method The cues described in Section 2 were applied to the Brown Corpus (un-tagged version).
Equation 2 was applied to the resulting data with a cutoff ofP(m+,n,p_s) < .02 and p-s varying between 2 -5 (1 error in every 32 occurrences)and 2 -13 (1 error in every 8192 occurrences).
The resulting judgments were comparedto the blind judgments of a single judge.
One hundred ninety-three distinct verbswere chosen at random from the tagged version of the Brown Corpus for comparison.Common verbs are more likely to be included in the test sample than rare verbs, but251Computational Linguistics Volume 19, Number 2Table 6Comparison of automatic lassification to hand judgments for tensed-clausecomplement as a function of estimated error rate p (Brown Corpus).
PRE =(TP / TP + FP); REC = (TP / TP + FN).- log2 p-ct p-c1 TP FP TN FN MC %MC PRE REC5 .0312 13 0 30 20 20 32 1.00 .396 .0156 19 0 30 14 14 22 1.00 .587 .0078 22 1 29 11 12 19 .96 .678 .0039 25 1 29 8 9 14 .96 .769 .0020 27 3 27 6 9 14 .90 .8210 .0010 29 5 25 4 9 14 .85 .8811 .0005 31 8 22 2 10 16 .79 .9412 .0002 31 13 17 2 15 24 .70 .9413 .0001 33 19 11 0 19 30 .63 1.00no verb is included more than once.
Each verb was scored for a given frame onlyif it cooccurs with a cue for that frame at least once.
Thus, although 193 verbs wererandomly selected from the corpus for scoring, only the 63 that cooccur with a cue fortensed clause at least once were scored for the tensed-clause frame.
This proceduremakes it possible to evaluate the hypothesis-testing method on data collected by thecues, rather than evaluating the cues per se.
It also makes the judgment ask mucheasier--it is not necessary to determine whether a verb can appear in a frame in prin-ciple, only whether it does so in particular sentences.
There were, however, five caseswhere the judgments were unclear.
These five were not scored.
See Appendix C fordetails.Results The results of these comparisons are summarized in Table 6 (tensed clause)and Table 7 (infinitive).
Each row shows the performance of the hypothesis-testingprocedure for a different estimate P-s of the error-rate 7r_s.
The first column showsthe negative logarithm of P-s, which is varied from 5 (1 error in 32 occurrences) to13 (1 error in 8192 occurrences).
The second column shows P-s in decimal notation.The next four columns show the number of true positives (TP)--verbs judged +S bothby machine and by hand; false positives (FP)--verbs judged +S by machine, -S  byhand; true negatives (TN)--verbs judged -S  both by machine and by hand; and falsenegatives (FN)--verbs judged -S  by machine, +S by hand.
The numbers representdistinct verbs, not occurrences.
The seventh column shows the number of verbs thatwere misclassified (MC)--the sum of false positives and false negatives.
The eighthcolumn shows the percentage of verbs that were misclassified (%MC).
The next-to-lastcolumn shows the precision (PRE)--the true positives divided by all verbs that Lernerjudged to be +S.
The final column shows the recall (REC)--the true positives dividedby all verbs that were judged +S by hand.Discussion For verbs taking just a tensed clause argument, Table 6 shows that,given the right estimate P-s of lr_s, it is possible to classify these 63 verbs with only 1false positive and 8 false negatives.
If the error rate were ignored or approximated aszero then the false positives would go up to 19.
On the other hand, if the error ratewere taken to be as high as 1 in 25 then the false negatives would go up to 20.
In thiscase, the sum of both error types is minimized with 2 -8 < P-c1 _< 2 -1?.
Table 7 showssimilar results for verbs taking just an infinitive argument, where misclassificationsare minimized with p-inf = 2-7 .252Michael R. Brent From Grammar to LexiconTable 7Comparison of automatic lassification to hand judgments for infinitivecomplement, as a function of estimated error rate p (Brown Corpus).- log 2 p-i,/ p-i,f TP FP TN FN MC %MC PRE REC5 .0312 14 0 33 13 13 22 1.00 .526 .0156 16 0 33 11 11 18 1.00 .597 .0078 19 1 32 8 9 15 .95 .708 .0039 22 6 27 5 11 18 .79 .819 .0020 22 8 25 5 13 22 .73 .8110 .0010 24 12 21 3 15 25 .67 .8911 .0005 24 14 19 3 17 28 .63 .8912 .0002 26 19 14 1 20 33 .58 .9613 .0001 27 26 7 0 26 43 .51 1.003.2 Estimating the Error RateAs before, assume that an occurrence of a -S  verb is followed by a cue for S withprobabil ity 7r_s.
Also as before, assume that for each +S verb V, the probabil ity thatan occurrence of V is followed by a cue for S is greater than 7r_s.It is useful to think of the verbs in the corpus as analogous to a large bag ofcoins with various biases, or probabilities of coming up heads.
The only assumptionabout the distribution of biases is that there is some definite but unknown min imumbias 7r_s.
5 Determining whether or not a verb appears in frame S is analogous todetermining, for some randomly selected coin, whether its bias is greater than ~r-s.The only available evidence comes from selecting a number  of coins at random andfl ipping them.
The previous section showed how this can be done given an estimateof ~r-s.Suppose a series of coins is drawn at random from the bag.
Each coin is fl ipped Ntimes.
It is then assigned to a histogram bin representing the number  of times it cameup heads.
At the end of this sampling procedure bin i contains the number  of coinsthat came up heads exactly i times out of N. Such a histogram is shown in Figure 1,where N = 40.
If N is large enough and enough coins are fl ipped N times, one wouldexpect the following:1.
The coins whose probabil ity of turning up heads is ~r-s (the minimum)should cluster at the low-heads end of the histogram.
That is, thereshould be some 0 __G j0 _< N such that most of the coins that turn up j0heads or fewer have probabil ity 7r_s, and, conversely, most coins withprobabil ity ~r-s turn up j0 heads or fewer.2.
Suppose j0 were known.
Then the portion of the histogram below j0should have a roughly binomial shape.
In Figure 1, for example, the firsteight bins have roughly the shape one would expect if j0 were 8.
Incontrast, the first 16 bins do not have the shape one would expect if j05 If the number of coins is taken to be infinite, then the biases must be not only greater than ~r-s butbounded above ~r-s.253Computational Linguistics Volume 19, Number 2NumberofCoins00 8 16 24 32 40Number of Heads HippedFigure 1A histogram illustrating a binomially shaped istribution i  the first eight bins..were 16---their height drops to zero for two stretches before risingsignificantly above zero again.
Specifically, the height of the i th histogrambin should be roughly proportional to P(i, N, P-s), with N the fixedsample size and P-s an estimate of 7r_s.Suppose again that j0 were known.
Then the average rate at which thecoins in bins j0 or lower flip heads is a good estimate of ~r-s.The estimation procedure tries out each bin as a possible estimate of j0.
Each estimateof j0 leads to an estimate of ~r-s and hence to an expected shape for the first j0 histogrambins.
Each estimate j of j0 is evaluated by comparing the predicted istribution in thefirst j bins to the observed istribution--the better the fit, the better the estimate.Moving from coins to verbs, the procedure works as follows.
For some fixed N,consider the first N occurrences of each verb that occurs at least N times in the input.
(A uniform sample size N is needed only for estimating 7r-s.
Given an estimate of 7r-s,verbs with any number of occurrences can be classified.)
Let S be some syntactic frameand let H\[i\] be the number of distinct verbs that were followed by cues for S exactlyi times out of N--i.e., the height of the ith histogram bin.
Assume that there is some1 ~ j0 _< N such that most -S  verbs are followed by cues for S j0 times or fewer, andconversely most verbs that are followed by cues for S j0 times or fewer are -S  verbs.For each possible estimate j of j0 there is a corresponding estimate of 7r_s; namely, theaverage rate at which verbs in the first j bins are followed by cues for S. Choosingthe most plausible estimate of 7r_s thus comes down to choosing the most plausibleestimate of j0, the boundary between the -S  verbs and the rest of the histogram.
Toevaluate the plausibility of each possible estimate j of j0, measure the fit between thepredicted istribution of -S  verbs, assuming j is the boundary of the -S  cluster, andthe observed istribution of the -S  verbs, also assuming j is the boundary of the -Scluster.
Given j, let p-s stand for the average rate at which verbs in bins j or lowerare followed by cues for S. The predicted istribution for -S  verbs is proportional toP(i,N,p-s) for 0 < i < N. The observed distribution of -S  verbs, assuming j is theboundary of the -S  cluster, is H\[i\] for 0 < i < j and 0 for j < i < N. Measure the fitbetween the predicted and observed istributions by normalizing both to have unitarea and taking the sum over 0 < i < N of the squares of the differences between thetwo distributions at each bin i.254Michael R. Brent From Grammar to LexiconTable 8Comparison of automatic lassification using the Brown Corpus to handjudgments.
The estimate p-s is made with N = 100.
The probabilitythreshold is .02.S j p-s TP FP TN FN MC %MC PRE RECcl 2 0.0037 25 1 28 8 9 15 .96 .76inf 2 0.0048 22 1 32 5 6 10 .96 .81NPcl 1 0.0002 3 2 2 0 2 29 .60 1.00NPinf 1 0.0005 5 0 3 2 2 20 1.00 .71NPNP 3 0.0004 3 0 3 3 3 33 1.00 .50NP 4 0.0132 52 1 5 59 60 51 .98 .47total 110 5 73 74 79 30 .96 .60In pseudo-code ,  the procedure  is as fol lows:ESTIMATE-P(H \[\], N)area := H\[0\], rain-sum-of-squares := oo, best-estimate := I;Try each value of j from 1 to N as an estimate of jofor j from I tO Np-s :=0area  := area  + H~\ ]fo r  i f rom 0 to  jNormalize the -S  bins to area 1.H\[i\] H'\[i\] := areaEstimate ~r-s by the average cooccurrence rate forthe first j bins--those presumed to hold - S verbsP-s := p-s + (-~ , H'\[i\])Check the fit, assuming j is the :kS boundarysum-of-squares := 0for i from 0 to NCompute the predicted istribution/or bin iN!
~i /1 P := ~H-s  -P -s )  N-iVerbs in the first bins j and below are presumed - Sif i<_jthen  normal i zed-observed  := H'\[i\]e lse  normal i zed-observed  := 0sum-of -squares  := sum-of -squares  + (normal i zed-observed  -- p)2Choose the p-s yielding the best~itif sum-of-squares < min-sum-of-squaresthen rain-sum-of-squares := sum-of-squaresbest-estimate := P-sreturn best-estimate3.2.1 Exper iment .
This sect ion eva luates  the proposed  est imat ion technique empi r i -cal ly in terms of the errors it y ie lds  when the cues of Section 2 are app l ied  to theBrown Corpus.
The sample  select ion and  scor ing procedures  are the same as in theprev ious  section.
When ~r-s is es t imated  us ing  sample  size N -- 100, Table 8 shows255Computational Linguistics Volume 19, Number 2the results for each of the six frames.
Varying N between 50 and 150 results in nosignificant change in the estimated error rates.One way to judge the value of the estimation and hypothesis-testing methods i  toexamine the false positives.
Three of the five false positives result from errors in verbdetection that are not distributed uniformly across verbs.
In particular, shock, board, andnear are used more often as nonverbs than as verbs.
This creates many opportunitiesfor nonverbal occurrences of these words to be mistaken for verbal occurrences.
Otherverbs, like know, are unambiguous and thus are not subject o this type of error.
As aresult, these rrors violate the model's assumption that errors are distributed uniformlyacross verbs and highlight he limitations of the model.
The remaining false positiveswere touch and belong, both mistaken as taking an NP followed by a tensed clause.The touch error was caused by the capitalization of the first word of a line of poetry:I knew not what did to a friend belongTill I stood up, true friend, by thy true side;Till was mistaken for a proper name.
The belong error was caused by mistaking amatrix clause for an argument in:With the blue flesh of night touching him he stood under a gentle hillcaressing the flageolet with his lips, making it whisper.It seems likely that such input would be much rarer in more mundane sources of text,such as newspapers of record, than in the diverse Brown Corpus.The results for infinitives and clauses can also be judged by comparison to theoptimal classifications rates from Tables 6 and 7.
In both cases the classification appearsto be right in the optimal range.
In fact, the estimated error rate for infinitives producesa better classification than any of those shown in Table 7.
(It falls at a value betweenthose shown.)
The classification ofclauses and infinitives remains in the optimal rangewhen the probability threshold is varied from .01 to .05.Overall the tradeoff between improved precision and reduced recall seems quitegood, as compared to doing no noise reduction (P-s = 0).
The only possible excep-tion is the NP frame, where noise reduction causes 59 false negatives in exchangefor preventing only 5 false positives.
This is partly explained by the different priorprobabilities of the different frames.
Most verbs can take a direct object argument,whereas most verbs cannot ake a direct object argument followed by a tensed clauseargument.
There is no way to know this in advance.
There may be other factors aswell.
If the error rate for the NP cues is substantially ower than 1 out of 100, then itcannot be estimated accurately with sample size N = 100.
On the other hand, if thesample size N is increased substantially there may not be enough verbs that occur Ntimes or more in the corpus.
So a larger corpus might improve the recall rate for NP.4.
General DiscussionThis paper explores the possibility of using simple grammatical regularities to learnlexical syntax.
The data presented in Tables 6, 7, and 8 provide evidence that it ispossible to learn significant aspects of English lexical syntax in this way.
Specifically,these data suggest that neither a large parser nor a large lexicon is needed to recoverenough syntactic structure for learning lexical syntax.
Rather, it seems that significant256Michael R. Brent From Grammar to Lexiconlexical syntactic information can be recovered using a few approximate cues alongwith statistical inference based on a simple model of the cues' error distributions.4.1 Other Syntactic FramesThe lexical entry of a verb can specify other syntactic frames in addition to the sixstudied here.
In particular, many verbs take prepositional phrases (PPs) headed bya particular preposition or class of prepositions.
For example, put requires a locationas a second argument, and locations are often represented by PPs headed by locativeprepositions.Extending Lerner to detect PPs is trivial.
Since the set of prepositions in the lan-guage is essentially fixed, all prepositions can be included in the initial lexicon.
De-tecting a PP requires nothing more than detecting a preposition.
6 The statistical modelcan, of course, be applied without modification.The problem, however, is determining which PPs are arguments and which areadjuncts.
There are clearly cases where a prepositional phrase can occur in a clausenot by virtue of the lexical entry of the verb but rather by virtue of nonlexical facts ofEnglish syntax.
For instance, almost any verb can occur with a temporal PP headedby on, as in John arrived on Monday.
Such PPs are called adjuncts.
On the other hand,the sense of on in John sprayed water on the ceiling is quite different.
This sense, it can beargued, is available only because the lexical entry of spray specifies a location argumentthat can be realized as a PP.
If anything significant is to be learned about individualwords, the nonspecific ooccurrences of verbs with PPs (adjuncts) must be separatedfrom the specific ones (arguments).
It is not clear how a machine learning systemcould do this, although frequency might provide some clue.
Worse, however, thereare many cases in which even trained linguists lack clear intuitions.
Despite a numberof attempts to formulate necessary and sufficient conditions for the argument/adjunctdistinction (e.g., Jackendoff 1977), there are many cases for which the various criteriado not agree or the judgments are unclear (Adams and Macfarland 1991).
Thus, thePenn Treebank does not make the argument/adjunct distinction because their judgesdo not agree often enough.
Until a useful definition that trained humans can agree onis developed, it would seem fruitless to attempt machine learning experiments in thisdomain.4.2 Limitations of the Statistical ModelAlthough the results of this study are generally encouraging, they also point to somelimitations of the statistical model presented here.
First, it does not take into accountvariation in the percentage of verbs that can appear in each frame.
For example, mostverbs can take an NP argument, while very few can take an NP followed by a tensedclause.
This results in too few verbs being classified as +NP and too many being clas-sified as +NPcl, as shown in Table 8.
Second, it does not take into account he factthat for some words with verbal senses most of their occurrences are verbal, whereasfor others most of their occurrences are nonverbal.
For example, operate occurs ex-clusively as a verb while board occurs much more often as a noun than as a verb.Since the cues are based on the assumption that the word in question is a verb, boardpresents many more opportunities for error than operate.
This violates the assump-tion that the probability of error for a given frame is approximately uniform acrossverbs.6 The preposition/particle distinction isset aside here in order to focus on the more problematicargument/adjunct distinction.257Computational Linguistics Volume 19, Number 2Table 9Distribution of occurrences among morphological forms in the BrownCorpus.
The ambiguous words board and project show a pattern ofdistribution distinct from that of the unambiguous verbs operate andfollow.project 52 board 111 operate 48 follow 76projects 54 boards 31 operates 15 follows 72projected 10 boarded 3 operated 26 followed 150projecting 5 boarding 1 operating 55 following 97These limitations do not constitute a major impediment to applications of the cur-rent results.
For example, an applied system can be provided with the rough estimatesthat 80-95 percent of verbs take a direct object, while 1-2 percent ake a direct objectfollowed by a tensed clause.
Such estimates can be expected to reduce misclassifica-tion significantly.
Further, an existing dictionary could be used to "train" a statisticalmodel on familiar verbs.
A trained system would probably be more accurate in clas-sifying new verbs.
Finally, the lexical ambiguity problem could probably be reducedsubstantially in the applied context by using a statistical tagging program (Brill 1992;Church 1988).For addressing basic questions in machine learning of natural anguage the solu-tions outlined above are not attractive.
All of those solutions provide the learner withadditional specific knowledge of English, whereas the goal for the machine learningeffort should be to replace specific knowledge with general knowledge about he typesof regularities to be found in natural anguage.There is one approach to the lexical ambiguity problem that does not requiregiving the learner additional specific knowledge.
The problem is as follows: wordsthat occur frequently as, say, nouns are likely to have a different error rate fromunambiguous verbs.
If it were known which words occur primarily as verbs andwhich occur primarily as nouns then separate rror rate estimates could be madefor each.
This would reduce the rate of false positive errors even without any furtherinformation about which particular occurrences are nominal and which are verbal.
Oneway to distinguish primarily nominal words from primarily verbal words is by therelative frequencies of their various inflected forms.
For example, Table 9 shows thecontrast in the distribution of inflected forms between project and board on the one handand operate and follow on the other.
Project and board are two words whose frequentoccurrence as nouns has caused Lerner to make false positive errors.
In both cases,the stem and -s forms are much more common than the -ed and -ing forms.
Comparethis to the distribution for the unambiguous verbs operate and follow.
In these cases thediversity of frequencies i  much lower and does not display the characteristic patternof a word that occurs primarily as a noun--  -ing and -ed forms that are much rarerthan the -s and stem forms.
Similar characteristic patterns exist for words that occurprimarily as adjectives.
Recognizing such ambiguity patterns automatically wouldallow a separate rror rate to be estimated for the highly ambiguous words.4.3 Future WorkFrom the perspective of computational language acquisition, a natural direction inwhich to extend this work is to develop algorithms for learning some of the specificknowledge that was programmed into the system described above.
Consider the mor-258Michael R. Brent From Grammar to Lexiconphological adjustment rules according to which, for example, the final "e" of bite isdeleted when the suffix -ing is added, yielding biting rather than ,"biteing."
Lernerneeds to know such rules in order to determine whether or not a given word occursboth with and without he suffix -ing.
Experiments are under way on an unsupervisedprocedure that learns such rules from English text, given only the list of English verbalsuffixes.
This work is being extended further in the direction of discovering the mor-phemic suffixes themselves and discovering the ways in which these suffixes alternatein paradigms.
The short-term goal is to develop algorithms that can learn the rules ofinflection in English starting from only a corpus and a general notion of the nature ofmorphological regularities.Ultimately, this line of inquiry may lead to algorithms that can learn much ofthe grammar of a language starting with only a corpus and a general theory of thekinds of formal regularities to be found in natural languages.
Some elements of syntaxmay not be learnable in this way (Lightfoot 1991), but the lexicon, morphology, andphonology together make up a substantial portion of the grammar of a language.
Ifit does not prove possible to learn these aspects of grammar starting from a generalontology of linguistic regularities and using distributional nalysis then that, too, isan interesting result.
It would suggest that the task requires a more substantive initialtheory of possible grammars, or some semantic information about input sentences, orboth.
In any case this line of inquiry promises to shed light on the nature of language,learning, and language learning.ReferencesAdams, L., and Macfarland, T.
(1991).
"Testing for adjuncts."
In Proceedings,Second Annual Meeting of the FormalLinguistics Society of Midamerica.
FormalLinguistics Society of Midamerica.Brent, M. R. (1991a).
Automatic acquisition ofsubcategorization frames from unrestrictedEnglish.
Doctoral dissertation,Massachusetts In titute of Technology,Cambridge MA.Brent, M. R. (1991b).
"Automatic acquisitionof subcategorization frames fromuntagged text."
In Proceedings, 29th AnnualMeeting of the ACL.
209-214.Brill, E. (1992).
"A simple rule-based part ofspeech tagger."
In Darpa Workshop onSpeech and Natural Language.
Harriman.Brill, E., and Marcus, M.
(1992).
"Automatically acquiring phrase structureusing distributional nalysis."
In DarpaWorkshop on Speech and Natural Language.Harriman.Chomsky, N. (1965).
Aspects of the Theory ofSyntax.
MIT Press.Church, K. (1988).
"A stochastic partsprogram and noun phrase parser forunrestricted text."
In Proceedings, SecondACL Conference on Applied NLP.Ellison, T. M. (1991).
"Discovering planarsegregations."
In AAAI Spring Symposiumon Machine Learning of Natural Language andOntology.Finch, S., and Chater, N.
(1992).
"Bootstrapping syntactic ategories usingstatistical methods."
In Background andExperiments inMachine Learning of NaturalLanguage: Proceedings ofthe 1st SHOEWorkshop.
Katholieke Universiteit,Brabant, Holland.Hindle, D. (1990).
"Noun classification frompredicate argument s ructures."
InProceedings, 28th Annual Meeting of the ACL.268-275.Hindle, D. (1991).
"Structural ambiguity andlexical relations."
In Proceedings, 29thAnnual Meeting of the ACL.
229-236.Jackendoff, R. (1977).
X-bar Syntax: A Studyof Phrase Structure.
Volume 2 of LinguisticInquiry Monograph.
MIT Press.Kalbfleisch, J. G. (1985).
Probability andStatistical Inference, Volume 2, SecondEdition.
Springer-Verlag.Karttunen, L. (1983).
"KIMMO: A generalmorphological processor."
TexasLinguistics Forum, 22(22), 165-186.Karttunen, L., and Wittenburg, K. (1983).
"Atwo-level morphological nalysis ofEnglish."
Texas Linguistics Forum, 22(22),217-223.Lightfoot, D. W. (1991).
How to SetParameters.
MIT Press.Magerman, D., and Marcus, M.
(1990).
"Parsing a natural language using mutualinformation statistics."
In Proceedings,Eighth National Conference on Arti~cialIntelligence.259Computational Linguistics Volume 19, Number 2Magerman, D., and Marcus, M.
(1991).
"Distituent parsing and grammarinduction."
In AAAI Spring Symposium onMachine Learning of Natural Language andOntology.Pereira, F., and Schabes, Y.
(1992).
"Inside-outside reestimation from partiallybracketed corpora."
In Proceedings, 30thAnnual Meeting of the Association forComputational Linguistics.Appendix A: Test WordsThe experiments described above used the following 193 verbs, selected at randomfrom the tagged version of the Brown Corpus.
Forms of be and have were excluded,as were modal verbs such as must and should.abandon account acquire act add announce anticipate appear arch ask attemptattend attest avoid bear believe belong bend board boil bring bristle brush buildbuzz call cap cast choose choreograph close come concern conclude considercontain convert culminate cut deal decrease defend elegate deliver denouncedeny depend esign determine develop die dine discourage dispatch disunitedrink duplicate liminate merge nd enter equate rect execute xist expectextend face fail fall feed feel fight figure find fly follow get give glow guidehear help hijack hire hope impart impede improve include increase indicateinform instruct inure issue keep learn let live look make mean measure meetmine miss mount mourn near offer open oppose organize own pardon pickleplan play plead prefer prepare present prevent progress project provide questionquote range reappear receive recommend remember remind repeat report requestresign retire return save say season seat see seem serve set settle shift ship shocksign sing speak spend spice sponsor stand start stay study succeed suffer suggestsupport surprise swept ake talk tell term terminate think touch treat trembletrust try turn understand unite unload use visit weep wheel wipe wish wonderwork writeAppendix B: Complete OutputOf the 193 verbs listed above, Lerner detects 174 in the untagged version of the BrownCorpus.
Of these 174, there are 87 for which Lerner does not find sufficient evidenceto prove that they have any of the six syntactic frames in question.
Some of thesegenuinely do not appear in the corpus with cues for any of the six, while others doappear with cues, but not often enough to provide reliable evidence.
Given more text,sufficient evidence might eventually accumulate for many of these verbs.The 87 that were detected but not assigned any frames are as follows:account act anticipate arch attend bear bend boil bristle brush buzz cast closecontain convert culminate deal decrease delegate deliver depend esign de-termine develop dine discourage dispatch drink emerge nd equate rect existextend fall figure fly glow hire increase instruct issue live look measure minemiss mount mourn open oppose organize own present prevent progress projectquestion quote range reappear receive recommend repeat report retire returnseason seat settle ship sign sing speak spend sponsor stand stay succeed suffertalk term terminate tremble turn weep wheelThe 87 verbs for which Lerner does find sufficient evidence to assign one or moreframes are shown in Table 10.
Reading across each row, a verb is assigned those frames260Michael R. Brent From Grammar  to LexiconTable 10The lexicon that Lerner produces when restricted to the 193 test verbs.NP NPNP NPcl NPinf cl infabandon NP makeacquire NP meanadd cl meet NPannounce cl near NPappear cl inf offer NPask NP NPinf planattempt inf play NPattest cl plead NPavoid NP preferbelieve NP cl prepare NPbelong NPcl provideboard inf rememberbuild NP remindcall NP NPNP requestchoose inf save NPcome inf sayconcern NP see NPconclude cl seemconsider NP cl serve NPcut NP setdefend NP shift NPdenounce NP shockdeny NP cl start NPeliminate NP study NPenter NP suggestexecute NP support NPexpect NP NPinf cl inf surpriseface NP take NPfail inf tell NPfeel cl thinkfight NP inf touch NPfind NP cl treat NPfollow NP trustget NP NPinf inf trygive NP NPNP understand NPguide NP unload NPhear NP use NPhelp NP NPinf inf visit NPimprove NP wipe NPinclude NP wishinform NPcl wonderkeep NP worklearn cl inf write NPlet NPNP NPNP NPcl NPinf cl infNPcl infNPclNPNP NPcl NPinfNPclinfclinfclcl infcl infclclcl infinfinfclinfclclinfcl infclinfwhose symbols appear in its row.
For easy reference by frame, all the symbols for agiven frame are aligned in one column.Appendix C: Difficult JudgmentsThe results provided in Tables 6, 7, and 8 are based on hand judgments of the examplesfound by the cues.
In most cases these judgments were clear, but there were fivedifficult judgments.
These five, which were not scored, are discussed below.
In allcases except he last Lerner did not find sufficient evidence to warrant a conclusion.261Computational Linguistics Volume 19, Number 2Provide with a tensed clauseProvided and providing, when they occur without auxiliaries, clearly take a tensedclause, as in (3a).. a.b.Squat-style lifters and leg-split lifters would both benefitenormously by practicing those variations providing that theyremember to make alternate sets with the left and right leg to thefront.There must be a restriction in the deed to provide that the customermay not be charged more than the current market price for the oil,Provided and providing do not appear to be functioning as verbs when they take a tensedclause.
Tensed forms of provide do not take a tensed clause, as in ,"She {provided,provides} that he have enough to live on."
However, the infinitive form of provide in(3b) is clearly functioning as a verb.
The best generalization of these observations iunclear.Act with an infinitive(4) "E.
B."
compared John Brown to Moses in that they were both acting todeliver millions from oppression.In (4), it is not clear whether the infinitive is an argument, as it would be in "theywere both trying to deliver .
.
.
,"  or a purpose adjunct, as in "they were both breakingthe law to deliver .
.
.
.
"Live with an infinitiveLive occurred twice in the expression live to see and once in the expression live to hear.If this expression were completely fixed, then its properties hould not be consideredproperties of the verb live.
However, the infinitive can be any perception verb andpossibly a few others.
The expression is limited, but not frozen, so the appropriateconclusion is unclear.Work with a single NPWork takes a direct object in the sense meaning form or mold.
The cues do not detectthis sense of work in the corpus, but they do find it followed by adverbial NPs suchas "every day" and in the expression work their will.Give with a single NPLerner judged give to take a single NP based on seven examples of which six weremistaken.
The seventh was the sentence "They continued to give an arm-elevation.
"This is ungrammatical in my dialect, but it is clearly an example of give with a singleNP complement.262
