A Generative Constituent-Context Model for Improved Grammar InductionDan Klein and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305-9040{klein, manning}@cs.stanford.eduAbstractWe present a generative distributional model for theunsupervised induction of natural language syntaxwhich explicitly models constituent yields and con-texts.
Parameter search with EM produces higherquality analyses than previously exhibited by un-supervised systems, giving the best published un-supervised parsing results on the ATIS corpus.
Ex-periments on Penn treebank sentences of compara-ble length show an even higher F1 of 71% on non-trivial brackets.
We compare distributionally in-duced and actual part-of-speech tags as input data,and examine extensions to the basic model.
We dis-cuss errors made by the system, compare the sys-tem to previous models, and discuss upper bounds,lower bounds, and stability for this task.1 IntroductionThe task of inducing hierarchical syntactic structurefrom observed yields alone has received a great dealof attention (Carroll and Charniak, 1992; Pereira andSchabes, 1992; Brill, 1993; Stolcke and Omohun-dro, 1994).
Researchers have explored this problemfor a variety of reasons: to argue empirically againstthe poverty of the stimulus (Clark, 2001), to use in-duction systems as a first stage in constructing largetreebanks (van Zaanen, 2000), or to build better lan-guage models (Baker, 1979; Chen, 1995).In previous work, we presented a conditionalmodel over trees which gave the best published re-sults for unsupervised parsing of the ATIS corpus(Klein and Manning, 2001b).
However, it sufferedfrom several drawbacks, primarily stemming fromthe conditional model used for induction.
Here, weimprove on that model in several ways.
First, weconstruct a generative model which utilizes the samefeatures.
Then, we extend the model to allow mul-tiple constituent types and multiple prior distribu-tions over trees.
The new model gives a 13% reduc-tion in parsing error on WSJ sentence experiments,including a positive qualitative shift in error types.Additionally, it produces much more stable results,does not require heavy smoothing, and exhibits a re-liable correspondence between the maximized ob-jective and parsing accuracy.
It is also much faster,not requiring a fitting phase for each iteration.Klein and Manning (2001b) and Clark (2001) taketreebank part-of-speech sequences as input.
We fol-lowed this for most experiments, but in section 4.3,we use distributionally induced tags as input.
Perfor-mance with induced tags is somewhat reduced, butstill gives better performance than previous models.2 Previous WorkEarly work on grammar induction emphasized heu-ristic structure search, where the primary inductionis done by incrementally adding new productions toan initially empty grammar (Olivier, 1968; Wolff,1988).
In the early 1990s, attempts were made to dogrammar induction by parameter search, where thebroad structure of the grammar is fixed in advanceand only parameters are induced (Lari and Young,1990; Carroll and Charniak, 1992).1 However, thisappeared unpromising and most recent work has re-turned to using structure search.
Note that both ap-proaches are local.
Structure search requires waysof deciding locally which merges will produce a co-herent, globally good grammar.
To the extent thatsuch approaches work, they work because good lo-cal heuristics have been engineered (Klein and Man-ning, 2001a; Clark, 2001).1On this approach, the question of which rules are includedor excluded becomes the question of which parameters are zero.Computational Linguistics (ACL), Philadelphia, July 2002, pp.
128-135.Proceedings of the 40th Annual Meeting of the Association forSNPNN0 FactoryNNS1 payrollsVPVBD2 fellPPIN3 inNN4 September 5543210543210StartEnd543210543210StartEnd543210543210StartEndSpan Label Constituent Context?0,5?
S NN NNS VBD IN NN  ?
?0,2?
NP NN NNS  ?
VBD?2,5?
VP VBD IN NN NNS ?
?3,5?
PP IN NN VBD ?
?0,1?
NN NN  ?
NNS?1,2?
NNS NNS NN ?
VBD?2,3?
VBD VBD NNS ?
IN?3,4?
IN IN VBD ?
NN?4,5?
NN NNS IN ?
(a) (b) (c)Figure 1: (a) Example parse tree with (b) its associated bracketing and (c) the yields and contexts for each constituent span in thatbracketing.
Distituent yields and contexts are not shown, but are modeled.Parameter search is also local; parameters whichare locally optimal may be globally poor.
A con-crete example is the experiments from (Carroll andCharniak, 1992).
They restricted the space of gram-mars to those isomorphic to a dependency grammarover the POS symbols in the Penn treebank, andthen searched for parameters with the inside-outsidealgorithm (Baker, 1979) starting with 300 randomproduction weight vectors.
Each seed converged toa different locally optimal grammar, none of themnearly as good as the treebank grammar, measuredeither by parsing performance or data-likelihood.However, parameter search methods have a poten-tial advantage.
By aggregating over only valid, com-plete parses of each sentence, they naturally incor-porate the constraint that constituents cannot cross?
the bracketing decisions made by the grammarmust be coherent.
The Carroll and Charniak exper-iments had two primary causes for failure.
First,random initialization is not always good, or neces-sary.
The parameter space is riddled with local like-lihood maxima, and starting with a very specific, butrandom, grammar should not be expected to workwell.
We duplicated their experiments, but used auniform parameter initialization where all produc-tions were equally likely.
This allowed the interac-tion between the grammar and data to break the ini-tial symmetry, and resulted in an induced grammarof higher quality than Carroll and Charniak reported.This grammar, which we refer to as DEP-PCFG willbe evaluated in more detail in section 4.
The sec-ond way in which their experiment was guaranteedto be somewhat unencouraging is that a delexical-ized dependency grammar is a very poor model oflanguage, even in a supervised setting.
By the F1measure used in the experiments in section 4, an in-duced dependency PCFG scores 48.2, compared toa score of 82.1 for a supervised PCFG read fromlocal trees of the treebank.
However, a superviseddependency PCFG scores only 53.5, not much bet-ter than the unsupervised version, and worse than aright-branching baseline (of 60.0).
As an example ofthe inherent shortcomings of the dependency gram-mar, it is structurally unable to distinguish whetherthe subject or object should be attached to the verbfirst.
Since both parses involve the same set of pro-ductions, both will have equal likelihood.3 A Generative Constituent-Context ModelTo exploit the benefits of parameter search, we useda novel model which is designed specifically to en-able a more felicitous search space.
The funda-mental assumption is a much weakened version ofclassic linguistic constituency tests (Radford, 1988):constituents appear in constituent contexts.
A par-ticular linguistic phenomenon that the system ex-ploits is that long constituents often have short, com-mon equivalents, or proforms, which appear in sim-ilar contexts and whose constituency is easily dis-covered (or guaranteed).
Our model is designedto transfer the constituency of a sequence directlyto its containing context, which is intended to thenpressure new sequences that occur in that contextinto being parsed as constituents in the next round.The model is also designed to exploit the successesof distributional clustering, and can equally well beviewed as doing distributional clustering in the pres-ence of no-overlap constraints.3.1 Constituents and ContextsUnlike a PCFG, our model describes all contigu-ous subsequences of a sentence (spans), includingempty spans, whether they are constituents or non-constituents (distituents).
A span encloses a se-quence of terminals, or yield, ?, such as DT JJ NN.A span occurs in a context x , such as ?VBZ, wherex is the ordered pair of preceding and following ter-minals ( denotes a sentence boundary).
A bracket-ing of a sentence is a boolean matrix B, which in-dicates which spans are constituents and which arenot.
Figure 1 shows a parse of a short sentence, thebracketing corresponding to that parse, and the la-bels, yields, and contexts of its constituent spans.Figure 2 shows several bracketings of the sen-tence in figure 1.
A bracketing B of a sentence isnon-crossing if, whenever two spans cross, at mostone is a constituent in B.
A non-crossing bracket-ing is tree-equivalent if the size-one terminal spansand the full-sentence span are constituents, and allsize-zero spans are distituents.
Figure 2(a) and (b)are tree-equivalent.
Tree-equivalent bracketings Bcorrespond to (unlabeled) trees in the obvious way.A bracketing is binary if it corresponds to a binarytree.
Figure 2(b) is binary.
We will induce trees byinducing tree-equivalent bracketings.Our generative model over sentences S has twophases.
First, we choose a bracketing B accordingto some distribution P(B) and then generate the sen-tence given that bracketing:P(S, B) = P(B)P(S|B)Given B, we fill in each span independently.
Thecontext and yield of each span are independent ofeach other, and generated conditionally on the con-stituency Bi j of that span.P(S|B) =?
?i, j ?
?spans(S) P(?i j , xi j |Bi j )=?
?i, j ?
P(?i j |Bi j )P(xi j |Bi j)The distribution P(?i j |Bi j) is a pair of multinomialdistributions over the set of all possible yields: onefor constituents (Bi j = c) and one for distituents(Bi j = d).
Similarly for P(xi j |Bi j ) and contexts.The marginal probability assigned to the sentence Sis given by summing over all possible bracketings ofS: P(S) =?B P(B)P(S|B).2To induce structure, we run EM over this model,treating the sentences S as observed and the brack-etings B as unobserved.
The parameters 2 of2Viewed as a model generating sentences, this model is defi-cient, placing mass on yield and context choices which will nottile into a valid sentence, either because specifications for posi-tions conflict or because yields of incorrect lengths are chosen.However, we can renormalize by dividing by the mass placed onproper sentences and zeroing the probability of improper brack-etings.
The rest of the paper, and results, would be unchangedexcept for notation to track the renormalization constant.543210543210StartEnd543210543210StartEnd543210543210StartEnd(a) Tree-equivalent (b) Binary (c) CrossingFigure 2: Three bracketings of the sentence in figure 1: con-stituent spans in black.
(b) corresponds to the binary parse infigure 1; (a) does not contain the ?2,5?
VP bracket, while (c)contains a ?0,3?
bracket crossing that VP bracket.the model are the constituency-conditional yieldand context distributions P(?|b) and P(x |b).
IfP(B) is uniform over all (possibly crossing) brack-etings, then this procedure will be equivalent to soft-clustering with two equal-prior classes.There is reason to believe that such soft cluster-ings alone will not produce valuable distinctions,even with a significantly larger number of classes.The distituents must necessarily outnumber the con-stituents, and so such distributional clustering willresult in mostly distituent classes.
Clark (2001) findsexactly this effect, and must resort to a filtering heu-ristic to separate constituent and distituent clusters.To underscore the difference between the bracketingand labeling tasks, consider figure 3.
In both plots,each point is a frequent tag sequence, assigned tothe (normalized) vector of its context frequencies.Each plot has been projected onto the first two prin-cipal components of its respective data set.
The leftplot shows the most frequent sequences of three con-stituent types.
Even in just two dimensions, the clus-ters seem coherent, and it is easy to believe thatthey would be found by a clustering algorithm inthe full space.
On the right, sequences have beenlabeled according to whether their occurrences areconstituents more or less of the time than a cutoff(of 0.2).
The distinction between constituent anddistituent seems much less easily discernible.We can turn what at first seems to be distributionalclustering into tree induction by confining P(B) toput mass only on tree-equivalent bracketings.
In par-ticular, consider Pbin(B) which is uniform over bi-nary bracketings and zero elsewhere.
If we take thisbracketing distribution, then when we sum over datacompletions, we will only involve bracketings whichcorrespond to valid binary trees.
This restriction isthe basis for our algorithm.NPVPPPUsually a ConstituentRarely a Constituent(a) Constituent Types (b) Constituents vs. DistituentsFigure 3: The most frequent yields of (a) three constituent types and (b) constituents and distituents, as context vectors, projectedonto their first two principal components.
Clustering is effective at labeling, but not detecting constituents.3.2 The Induction AlgorithmWe now essentially have our induction algorithm.We take P(B) to be Pbin(B), so that all binary treesare equally likely.
We then apply the EM algorithm:E-Step: Find the conditional completion likeli-hoods P(B|S,2) according to the current 2.M-Step: Fix P(B|S,2) and find the 2?
which max-imizes?B P(B|S,2) log P(S, B|2?
).The completions (bracketings) cannot be efficientlyenumerated, and so a cubic dynamic program simi-lar to the inside-outside algorithm is used to calcu-late the expected counts of each yield and context,both as constituents and distituents.
Relative fre-quency estimates (which are the ML estimates forthis model) are used to set 2?.To begin the process, we did not begin at the E-step with an initial guess at 2.
Rather, we began atthe M-step, using an initial distribution over com-pletions.
The initial distribution was not the uniformdistribution over binary trees Pbin(B).
That was un-desirable as an initial point because, combinatorily,almost all trees are relatively balanced.
On the otherhand, in language, we want to allow unbalancedstructures to have a reasonable chance to be discov-ered.
Therefore, consider the following uniform-splitting process of generating binary trees over kterminals: choose a split point at random, then recur-sively build trees by this process on each side of thesplit.
This process gives a distribution Psplit whichputs relatively more weight on unbalanced trees, butonly in a very general, non language-specific way.This distribution was not used in the model itself,however.
It seemed to bias too strongly against bal-anced structures, and led to entirely linear-branchingstructures.The smoothing used was straightforward.
Foreach yield ?
or context x , we added 10 counts of thatitem as a constituent and 50 as a distituent.
This re-flected the relative skew of random spans being morelikely to be distituents.
This contrasts with our previ-ous work, which was sensitive to smoothing method,and required a massive amount of it.4 ExperimentsWe performed most experiments on the 7422 sen-tences in the Penn treebank Wall Street Journal sec-tion which contained no more than 10 words af-ter the removal of punctuation and null elements(WSJ-10).
Evaluation was done by measuring un-labeled precision, recall, and their harmonic meanF1 against the treebank parses.
Constituents whichcould not be gotten wrong (single words and en-tire sentences) were discarded.3 The basic experi-ments, as described above, do not label constituents.An advantage to having only a single constituentclass is that it encourages constituents of one type tobe found even when they occur in a context whichcanonically holds another type.
For example, NPsand PPs both occur between a verb and the end ofthe sentence, and they can transfer constituency toeach other through that context.Figure 4 shows the F1 score for various meth-ods of parsing.
RANDOM chooses a tree uniformly3Since reproducible evaluation is important, a few morenotes: this is different from the original (unlabeled) bracket-ing measures proposed in the PARSEVAL standard, which didnot count single words as constituents, but did give points forputting a bracket over the entire sentence.
Secondly, bracket la-bels and multiplicity are just ignored.
Below, we also presentresults using the EVALB program for comparability, but we notethat while one can get results from it that ignore bracket labels,it never ignores bracket multiplicity.
Both these alternativesseem less satisfactory to us as measures for evaluating unsu-pervised constituency decisions.133048607182 87020406080100LBRANCHRANDOMDEP-PCFGRBRANCH CCMSUP-PCFGUBOUND  Figure 4: F1 for various models on WSJ-10.01020304050607080901002 3 4 5 6 7 8 9Percentfiffffifl "!# # fiffffi$%fiffffi&'  "( )"( *+ ,-.&0/1&02$134$%Figure 5: Accuracy scores for CCM-induced structures by spansize.
The drop in precision for span length 2 is largely dueto analysis inside NPs which is omitted by the treebank.
Alsoshown is F1 for the induced PCFG.
The PCFG shows higheraccuracy on small spans, while the CCM is more even.at random from the set of binary trees.4 This isthe unsupervised baseline.
DEP-PCFG is the re-sult of duplicating the experiments of Carroll andCharniak (1992), using EM to train a dependency-structured PCFG.
LBRANCH and RBRANCH choosethe left- and right-branching structures, respectively.RBRANCH is a frequently used baseline for super-vised parsing, but it should be stressed that it en-codes a significant fact about English structure, andan induction system need not beat it to claim adegree of success.
CCM is our system, as de-scribed above.
SUP-PCFG is a supervised PCFGparser trained on a 90-10 split of this data, usingthe treebank grammar, with the Viterbi parse right-binarized.5 UBOUND is the upper bound of how wella binary system can do against the treebank sen-tences, which are generally flatter than binary, limit-ing the maximum precision.CCM is doing quite well at 71.1%, substantiallybetter than right-branching structure.
One commonissue with grammar induction systems is a tendencyto chunk in a bottom-up fashion.
Especially since4This is different from making random parsing decisions,which gave a higher score of 35%.5Without post-binarization, the F1 score was 88.9.System UP UR F1 CBEMILE 51.6 16.8 25.4 0.84ABL 43.6 35.6 39.2 2.12CDC-40 53.4 34.6 42.0 1.46RBRANCH 39.9 46.4 42.9 2.18COND-CCM 54.4 46.8 50.3 1.61CCM 55.4 47.6 51.2 1.45Figure 6: Comparative ATIS parsing results.the CCM does not model recursive structure explic-itly, one might be concerned that the high overallaccuracy is due to a high accuracy on short-spanconstituents.
Figure 5 shows that this is not true.Recall drops slightly for mid-size constituents, butlonger constituents are as reliably proposed as shortones.
Another effect illustrated in this graph is that,for span 2, constituents have low precision for theirrecall.
This contrast is primarily due to the singlelargest difference between the system?s inducedstructures and those in the treebank: the treebankdoes not parse into NPs such as DT JJ NN, whileour system does, and generally does so correctly,identifying N units like JJ NN.
This overproposaldrops span-2 precision.
In contrast, figure 5 alsoshows the F1 for DEP-PCFG, which does exhibit adrop in F1 over larger spans.The top row of figure 8 shows the recall of non-trivial brackets, split according the brackets?
labelsin the treebank.
Unsurprisingly, NP recall is high-est, but other categories are also high.
Becausewe ignore trivial constituents, the comparatively lowS represents only embedded sentences, which aresomewhat harder even for supervised systems.To facilitate comparison to other recent work, fig-ure 6 shows the accuracy of our system when trainedon the same WSJ data, but tested on the ATIS cor-pus, and evaluated according to the EVALB pro-gram.6 The F1 numbers are lower for this corpusand evaluation method.7 Still, CCM beats not onlyRBRANCH (by 8.3%), but also the previous condi-tional COND-CCM and the next closest unsupervisedsystem (which does not beat RBRANCH in F1).6EMILE and ABL are lexical systems described in (van Za-anen, 2000; Adriaans and Haas, 1999).
CDC-40, from (Clark,2001), reflects training on much more data (12M words).7The primary cause of the lower F1 is that the ATIS corpusis replete with span-one NPs; adding an extra bracket aroundall single words raises our EVALB recall to 71.9; removing allunaries from the ATIS gold standard gives an F1 of 63.3%.Rank Overproposed Underproposed1 JJ NN NNP POS2 MD VB TO CD CD3 DT NN NN NNS4 NNP NNP NN NN5 RB VB TO VB6 JJ NNS IN CD7 NNP NN NNP NNP POS8 RB VBN DT NN POS9 IN NN RB CD10 POS NN IN DTFigure 7: Constituents most frequently over- and under-proposed by our system.4.1 Error AnalysisParsing figures can only be a component of evaluat-ing an unsupervised induction system.
Low scoresmay indicate systematic alternate analyses ratherthan true confusion, and the Penn treebank is asometimes arbitrary or even inconsistent gold stan-dard.
To give a better sense of the kinds of errors thesystem is or is not making, we can look at which se-quences are most often over-proposed, or most oftenunder-proposed, compared to the treebank parses.Figure 7 shows the 10 most frequently over- andunder-proposed sequences.
The system?s main errortrends can be seen directly from these two lists.
Itforms MD VB verb groups systematically, and it at-taches the possessive particle to the right, like a de-terminer, rather than to the left.8 It provides binary-branching analyses within NPs, normally resultingin correct extra N constituents, like JJ NN, whichare not bracketed in the treebank.
More seriously,it tends to attach post-verbal prepositions to the verband gets confused by long sequences of nouns.
Asignificant improvement over earlier systems is theabsence of subject-verb groups, which disappearedwhen we switched to Psplit(B) for initial comple-tions; the more balanced subject-verb analysis hada substantial combinatorial advantage with Pbin(B).4.2 Multiple Constituent ClassesWe also ran the system with multiple constituentclasses, using a slightly more complex generativemodel in which the bracketing generates a labelingwhich then generates the constituents and contexts.The set of labels for constituent spans and distituentspans are forced to be disjoint.Intuitively, it seems that more classes should help,8Linguists have at times argued for both analyses: Halliday(1994) and Abney (1987), respectively.by allowing the system to distinguish different typesof constituents and constituent contexts.
However,it seemed to slightly hurt parsing accuracy overall.Figure 8 compares the performance for 2 versus 12classes; in both cases, only one of the classes wasallocated for distituents.
Overall F1 dropped veryslightly with 12 classes, but the category recall num-bers indicate that the errors shifted around substan-tially.
PP accuracy is lower, which is not surprisingconsidering that PPs tend to appear rather option-ally and in contexts in which other, easier categoriesalso frequently appear.
On the other hand, embed-ded sentence recall is substantially higher, possiblybecause of more effective use of the top-level sen-tences which occur in the signature context ?.The classes found, as might be expected, rangefrom clearly identifiable to nonsense.
Note that sim-ply directly clustering all sequences into 12 cate-gories produced almost entirely the latter, with clus-ters representing various distituent types.
Figure 9shows several of the 12 classes.
Class 0 is themodel?s distituent class.
Its most frequent mem-bers are a mix of obvious distituents (IN DT, DT JJ,IN DT, NN VBZ) and seemingly good sequences likeNNP NNP.
However, there are many sequences of3 or more NNP tags in a row, and not all adjacentpairs can possibly be constituents at the same time.Class 1 is mainly common NP sequences, class 2 isproper NPs, class 3 is NPs which involve numbers,and class 6 is N sequences, which tend to be lin-guistically right but unmarked in the treebank.
Class4 is a mix of seemingly good NPs, often from posi-tions like VBZ?NN where they were not constituents,and other sequences that share such contexts withotherwise good NP sequences.
This is a danger ofnot jointly modeling yield and context, and of notmodeling any kind of recursive structure.
Class 5 ismainly composed of verb phrases and verb groups.No class corresponded neatly to PPs: perhaps be-cause they have no signature contexts.
The 2-classmodel is effective at identifying them only becausethey share contexts with a range of other constituenttypes (such as NPs and VPs).4.3 Induced Parts-of-SpeechA reasonable criticism of the experiments presentedso far, and some other earlier work, is that we as-sume treebank part-of-speech tags as input.
ThisClasses Tags Precision Recall F1 NP Recall PP Recall VP Recall S Recall2 Treebank 63.8 80.2 71.1 83.4 78.5 78.6 40.712 Treebank 63.6 80.0 70.9 82.2 59.1 82.8 57.02 Induced 56.8 71.1 63.2 52.8 56.2 90.0 60.5Figure 8: Scores for the 2- and 12-class model with Treebank tags, and the 2-class model with induced tags.Class 0 Class 1 Class 2 Class 3 Class 4 Class 5 Class 6NNP NNP NN VBD DT NN NNP NNP CD CD VBN IN MD VB JJ NNNN IN NN NN JJ NNS NNP NNP NNP CD NN JJ IN MD RB VB JJ NNSIN DT NNS VBP DT NNS CC NNP IN CD CD DT NN VBN IN JJ JJ NNDT JJ NNS VBD DT JJ NN POS NN CD NNS JJ CC WDT VBZ CD NNSNN VBZ TO VB NN NNS NNP NNP NNP NNP CD CD IN CD CD DT JJ NN JJ IN NNP NNFigure 9: Most frequent members of several classes found.criticism could be two-fold.
First, state-of-the-artsupervised PCFGs do not perform nearly so wellwith their input delexicalized.
We may be reduc-ing data sparsity and making it easier to see a broadpicture of the grammar, but we are also limiting howwell we can possibly do.
It is certainly worth explor-ing methods which supplement or replace tagged in-put with lexical input.
However, we address herethe more serious criticism: that our results stemfrom clues latent in the treebank tagging informa-tion which are conceptually posterior to knowledgeof structure.
For instance, some treebank tag dis-tinctions, such as particle (RP) vs. preposition (IN)or predeterminer (PDT) vs. determiner (DT) or ad-jective (JJ), could be said to import into the tagsetdistinctions that can only be made syntactically.To show results from a complete grammar induc-tion system, we also did experiments starting witha clustering of the words in the treebank.
We usedbasically the baseline method of word type cluster-ing in (Schu?tze, 1995) (which is close to the meth-ods of (Finch, 1993)).
For (all-lowercased) wordtypes in the Penn treebank, a 1000 element vectorwas made by counting how often each co-occurredwith each of the 500 most common words imme-diately to the left or right in Treebank text and ad-ditional 1994?96 WSJ newswire.
These vectorswere length-normalized, and then rank-reduced byan SVD, keeping the 50 largest singular vectors.The resulting vectors were clustered into 200 wordclasses by a weighted k-means algorithm, and thengrammar induction operated over these classes.
Wedo not believe that the quality of our tags matchesthat of the better methods of Schu?tze (1995), muchless the recent results of Clark (2000).
Nevertheless,using these tags as input still gave induced structuresubstantially above right-branching.
Figure 8 shows010203040506070800 4 8 12 16 20 24 28 32 36 40Iterations  0.00M0.05M0.10M0.15M0.20M0.25M0.30M0.35M     F1log-likelihoodFigure 10: F1 is non-decreasing until convergence.the performance with induced tags compared to cor-rect tags.
Overall F1 has dropped, but, interestingly,VP and S recall are higher.
This seems to be due to amarked difference between the induced tags and thetreebank tags: nouns are scattered among a dispro-portionally large number of induced tags, increasingthe number of common NP sequences, but decreas-ing the frequency of each.4.4 Convergence and StabilityAnother issue with previous systems is their sensi-tivity to initial choices.
The conditional model ofKlein and Manning (2001b) had the drawback thatthe variance of final F1, and qualitative grammarsfound, was fairly high, depending on small differ-ences in first-round random parses.
The model pre-sented here does not suffer from this: while it isclearly sensitive to the quality of the input tagging, itis robust with respect to smoothing parameters anddata splits.
Varying the smoothing counts a factorof ten in either direction did not change the overallF1 by more than 1%.
Training on random subsetsof the training data brought lower performance, butconstantly lower over equal-size splits.
Moreover,there are no first-round random decisions to be sen-sitive to; the soft EM procedure is deterministic.0204060801000 10 20 30 40Iterations   NPPPVPSFigure 11: Recall by category during convergence.Figure 10 shows the overall F1 score and the datalikelihood according to our model during conver-gence.9 Surprisingly, both are non-decreasing as thesystem iterates, indicating that data likelihood in thismodel corresponds well with parse accuracy.10 Fig-ure 11 shows recall for various categories by itera-tion.
NP recall exhibits the more typical pattern ofa sharp rise followed by a slow fall, but the othercategories, after some initial drops, all increase untilconvergence.
These graphs stop at 40 iterations.
Thesystem actually converged in both likelihood and F1by iteration 38, to within a tolerance of 10?10.
Thetime to convergence varied according to smooth-ing amount, number of classes, and tags used, butthe system almost always converged within 80 iter-ations, usually within 40.5 ConclusionsWe have presented a simple generative model forthe unsupervised distributional induction of hierar-chical linguistic structure.
The system achieves thebest published unsupervised parsing scores on theWSJ-10 and ATIS data sets.
The induction algo-rithm combines the benefits of EM-based parame-ter search and distributional clustering methods.
Wehave shown that this method acquires a substan-tial amount of correct structure, to the point thatthe most frequent discrepancies between the inducedtrees and the treebank gold standard are systematicalternate analyses, many of which are linguisticallyplausible.
We have shown that the system is not re-liant on supervised POS tag input, and demonstratedincreased accuracy, speed, simplicity, and stabilitycompared to previous systems.9The data likelihood is not shown exactly, but rather weshow the linear transformation of it calculated by the system.10Pereira and Schabes (1992) find otherwise for PCFGs.ReferencesStephen P. Abney.
1987.
The English Noun Phrase in its Sen-tential Aspect.
Ph.D. thesis, MIT.Pieter Adriaans and Erik Haas.
1999.
Grammar inductionas substructural inductive logic programming.
In JamesCussens, editor, Proceedings of the 1st Workshop on Learn-ing Language in Logic, pages 117?127, Bled, Slovenia.James K. Baker.
1979.
Trainable grammars for speech recogni-tion.
In D. H. Klatt and J. J. Wolf, editors, Speech Communi-cation Papers for the 97th Meeting of the Acoustical Societyof America, pages 547?550.Eric Brill.
1993.
Automatic grammar induction and parsing freetext: A transformation-based approach.
In ACL 31, pages259?265.Glenn Carroll and Eugene Charniak.
1992.
Two experiments onlearning probabilistic dependency grammars from corpora.In C. Weir, S. Abney, R. Grishman, and R. Weischedel, edi-tors, Working Notes of the Workshop Statistically-Based NLPTechniques, pages 1?13.
AAAI Press.Stanley F. Chen.
1995.
Bayesian grammar induction for lan-guage modeling.
In ACL 33, pages 228?235.Alexander Clark.
2000.
Inducing syntactic categories by con-text distribution clustering.
In The Fourth Conference onNatural Language Learning.Alexander Clark.
2001.
Unsupervised induction of stochasticcontext-free grammars using distributional clustering.
In TheFifth Conference on Natural Language Learning.Steven Paul Finch.
1993.
Finding Structure in Language.
Ph.D.thesis, University of Edinburgh.M.
A. K. Halliday.
1994.
An introduction to functional gram-mar.
Edward Arnold, London, 2nd edition.Dan Klein and Christopher D. Manning.
2001a.
Distribu-tional phrase structure induction.
In Proceedings of the FifthConference on Natural Language Learning (CoNLL 2001),pages 113?120.Dan Klein and Christopher D. Manning.
2001b.
Natural lan-guage grammar induction using a constituent-context model.In Advances in Neural Information Processing Systems, vol-ume 14.
MIT Press.K.
Lari and S. J.
Young.
1990.
The estimation of stochasticcontext-free grammars using the inside-outside algorithm.Computer Speech and Language, 4:35?56.Donald Cort Olivier.
1968.
Stochastic Grammars and LanguageAcquisition Mechanisms.
Ph.D. thesis, Harvard University.Fernando Pereira and Yves Schabes.
1992.
Inside-outside rees-timation from partially bracketed corpora.
In ACL 30, pages128?135.Andrew Radford.
1988.
Transformational Grammar.
Cam-bridge University Press, Cambridge.Hinrich Schu?tze.
1995.
Distributional part-of-speech tagging.In EACL 7, pages 141?148.Andreas Stolcke and Stephen M. Omohundro.
1994.
Induc-ing probabilistic grammars by Bayesian model merging.
InGrammatical Inference and Applications: Proceedings ofthe Second International Colloquium on Grammatical Infer-ence.
Springer Verlag.M.
van Zaanen.
2000.
ABL: Alignment-based learning.
InCOLING 18, pages 961?967.J.
G. Wolff.
1988.
Learning syntax and meanings throughoptimization and distributional analysis.
In Y.
Levy, I. M.Schlesinger, and M. D. S. Braine, editors, Categoriesand processes in language acquisition, pages 179?215.Lawrence Erlbaum, Hillsdale, NJ.
