Proceedings of the Eighth Meeting of the ACL Special Interest Group on Computational Phonology at HLT-NAACL 2006, pages 69?78,New York City, USA, June 2006. c?2006 Association for Computational LinguisticsLearning Probabilistic Paradigms for Morphologyin a Latent Class ModelErwin ChanDept.
of Computer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA  19104echan3@seas.upenn.eduAbstractThis paper introduces the probabilisticparadigm, a probabilistic, declarativemodel of morphological structure.
We de-scribe an algorithm that recursively ap-plies Latent Dirichlet Allocation with anorthogonality constraint to discover mor-phological paradigms as the latent classeswithin a suffix-stem matrix.
We apply thealgorithm to data preprocessed in severaldifferent ways, and show that when suf-fixes are distinguished for part of speechand allomorphs  or gender/conjugationalvariants are merged, the model is able tocorrectly learn morphological paradigmsfor English and Spanish.
We compare oursystem with Linguistica (Goldsmith2001), and discuss the advantages of theprobabilistic paradigm over Linguistica?ssignature representation.1 IntroductionIn recent years researchers have addressed the taskof unsupervised learning of declarative representa-tions of morphological structure.
These modelsinclude the signature of (Goldsmith 2001), the con-flation set of (Schone and Jurafsky 2001), theparadigm of (Brent et.
al.
2002), and the inflec-tional class of (Monson 2004).
While these repre-sentations group morphologically related words insystematic ways, they are rather different from theparadigm, the representation of morphology in tra-ditional grammars.
A paradigm lists the prototypi-cal morphological properties of lexemes belongingto a particular part of speech (POS) category; forexample, a paradigm for regular English verbswould include the suffixes {$,ed$,ing$,s$}1.Hand-built computational implementations ofparadigms as inheritance hierarchies includeDATR (Evans and Gazdar 1996) and FunctionalMorphology (Forsberg and Ranta 2004).
The twoprincipal ways in which learned models have dif-fered from paradigms are that: 1) they do not havePOS types, and 2) they are not abstractions thatgeneralize beyond the words of the input corpus.There are important reasons for learning aPOS-associated, paradigmatic representation ofmorphology.
Currently, the dominant technologyfor morphological analysis involves mapping be-tween inflected and base of forms of words withfinite-state transducers (FSTs), a procedural modelof morphological relations.
Rewrite rules are hand-crafted and compiled into FSTs, and it would bebeneficial if these rules could be learned automati-cally.
One line of research in computational mor-phology has been directed towards learning finite-state mapping rules from some sort of paradig-matic structure, where all morphological forms andPOS types are presumed known for a set of lex-emes (Clark 2001, Kazakov and Manandhar 2001,Oflazer et.
al.
2001, Zajac 2001, Albright 2002).This can be accomplished by first deciding on abase form, then learning rules to convert otherforms of the paradigm into this base form.
If onecould develop an unsupervised algorithm for learn-ing paradigms, it could serve as the input to rule-learning procedures, effectively leading to an en-tirely unsupervised system for learning FSTs fromraw data.
This is our long-term goal.1$ is the null suffix.69An alternative approach is to skip the paradigmformulation step and construct a procedural modeldirectly from raw data.
(Yarowsky and Wicen-towski 2000) bootstrap inflected and base formsdirectly from raw data and learn mappings betweenthem.
Their results are quite successful, but themorphological information they learn is not struc-tured as clearly as a paradigmatic model.
(Freitag2005) constructs a morphological automaton,where nodes are clustered word types and arcs aresuffixation rules.This paper addresses the problem of finding anorganization of stems and suffixes as probabilisticparadigms (section 2), a model of morphologycloser to linguistic notion of paradigm than previ-ously proposed models.
We encode the morpho-logical structure of a language in a matrixcontaining frequencies of words, and formulate theproblem of learning paradigms as one of findinglatent classes within the matrix.
We present a re-cursive LDA, a learning algorithm based on LatentDirichlet Allocation (section 3), and show that un-der certain conditions (section 5), it can correctlylearn morphological paradigms for English andSpanish.
In section 6, we compare the probabilisticparadigm to the signature model of (Goldsmith2001).
In section 7, we sketch some ideas for howto make our system more unsupervised and morelinguistically adequate.We assume a model of morphology whereeach word is the concatenation of a stem and a sin-gle suffix representing all of the word's morpho-logical and POS properties.
Although this is a verysimplistic view of morphology, there are manyhitherto unresolved computational issues for learn-ing even this basic model, and we consider it nec-essary to address these issues before developingmore sophisticated models.
For a stem/suffix rep-resentation, the task of learning a paradigm fromraw data involves proposing suffixes and stems,proposing segmentations, and systematically orga-nizing stems and suffixes into classes.
One diffi-culty is suffix allomorphy: a suffix has multipleforms depending on its phonological environment(e.g.
s$/es$).
Another problem is suffix cate-gorial ambiguity (s$ is ambiguous for noun andverb uses).
Finally, lexemes appear in only a subsetof their potential forms, due to sparse data.
An un-supervised learner needs to be able to handle all ofthese difficulties in order to discover abstract para-digmatic classes.In this paper, we are primarily interested inhow the co-occurrence of stems and suffixes in acorpus leads them to be organized into paradigms.We use data preprocessed with correct segmenta-tions of words into stems and suffixes, in order tofocus on the issue of determining what additionalknowledge is needed.
We demonstrate that para-digms for English and Spanish can be successfullylearned when tokens have been assigned POS tagsand allomorphs or gender/conjugational variantsare given a common representation.
Our learningalgorithm is not supervised since the target conceptof gold standard "input" POS category of stems isnot known, but rather it is an unsupervised algo-rithm that relies on preprocessed data for optimalperformance.2 The Probabilistic ParadigmWe introduce the probabilistic paradigm, a prob-abilistic, declarative model of regular morphology.The probabilistic paradigm model consists of threematrices: the data matrix D, the morphologicalprobabilities matrix M, and the lexical probabilitiesmatrix L. Let m be the number of stems, n thenumber of stems, and p the number of paradigms.The D matrix encodes the joint distribution of lexi-cal and morphological information in a corpus.
It isof size m x n, and each cell contains the fre-quency of the word formed by concatenating theappropriate stem and suffix.
The M matrix is ofsize m x p, and each column contains the condi-tional probabilities of each suffix given a para-digm.
The L matrix is of size p x n, and containsthe conditional probabilities of each paradigmgiven a stem.
Each suffix should belong to exactlyone paradigm, and the suffixes of a particularparadigm should be conditionally independent.Each column of the M matrix defines a canonicalparadigm, a set of suffixes that attach to stems as-sociated with that paradigm.
A lexical paradigm isthe full set of word forms for a particular stem, andis an instantiation of the canonical paradigm for aparticular stem.The probabilistic paradigm is not well-developed as the usual notion of "paradigm" inlinguistics.
First, the system employs no labelssuch as "noun", "plural", "past", etc.
Second, prob-abilistic paradigms have only a top-level categori-zation; induced ?verb?
paradigms, for example, arenot substructured into different tenses or conjuga-70tions.
Third, we do not distinguish between inflec-tional and derivational morphology; traditionalgrammars place derived forms in separate lexicalparadigms.
Fourth, we do not handle syncretism,where one suffix belongs in multiple slots of theparadigm.
Fifth, we do not yet not handle irregularand sub-regular forms.
Despite these drawbacks,our paradigms have an important advantage overtraditional paradigms, in being probabilistic andtherefore able to model language usage.3 Learning the probabilistic paradigm in alatent class modelWe learn the parameters of the probabilistic para-digm model by applying a dimensionality reduc-tion algorithm to the D matrix, in order to producethe M and L matrices.
This reduces the size of therepresentation from m*n to m*p + p*n. The mainidea is to discover the latent classes (paradigms)which represent the underlying structure of the in-put matrix.
This handles two important problems:1) that words occur in a subset of their possiblemorphological forms in a corpus, and 2) that thewords formed from a particular stem can belong tomultiple POS categories.
The second problem canbe quantified as follows: in our English data,14.3% of types occur with multiple open-class basePOS categories, accounting for 56.5% of tokens;for Spanish, 13.7% of types, 37.8% of tokens.3.1 LDA model for morphologyThe dimensionality reduction algorithm that weemploy is Latent Dirichlet Allocation (LDA) (Bleiet.
al.
2003).
LDA is a generative probabilisticmodel for discrete data.
For the application of topicdiscovery within a corpus of documents, a docu-ment consists of a mixture of underlying topics,and each topic consists of a probability distributionover the vocabulary.
The topic proportions aredrawn from a Dirichlet distribution, and the wordsare drawn from a multinomial over the topic.
Prob-ability distributions of documents and words areconditionally independent of topics.
LDA producestwo non-negative parameter matrices, Gamma andBeta: Gamma is the matrix of Dirichlet posteriors,encoding the distribution of documents and topics;Beta encodes the distribution of words and topics.The mapping of the data structures of LDA tothe probabilistic paradigm is as follows.
Thedocument-word matrix is analogous to the suffix-stem D matrix.
For morphology, a "document" is amultiset of tokens in a corpus, such that each ofthose tokens decomposes into a stem and a speci-fied suffix.
Different underlying canonical para-digms ("topics") can be associated with suffixes,and each canonical paradigm allows a set of stems("words").
For a suffix-stem ("document-word")matrix of size m x n and k latent classes, theGamma matrix is of size m x k, and the Beta ma-trix is of size k x n. The Gamma matrix, normal-ized by column, is the M matrix, and the Betamatrix, normalized by row, is the L matrix.3.2 Recursive LDAOne standard issue in using these types of algo-rithms is selecting the number of classes.
To dealwith this, we have formulated a recursive wrapperalgorithm for LDA that accomplishes a divisiveclustering of suffixes.
LDA is run at each stage tofind the local Gamma and Beta matrices.
To splitthe suffixes into two classes, we assign each suffixto the class for which its probability is greater, byexamining the Gamma matrix.
The input matrix isthen divided into two smaller matrices based onthis split, and the algorithm continues with eachsubmatrix.
The result is a binary tree describing thesuffix splits at each node.To construct a classification of suffixes intoparadigms, it is necessary to make a cut in the tree.Assuming that suffix splits are optimal, we start atthe root of the tree and go down until reaching anode where there is sufficient uncertainty aboutwhich class a suffix should belong to.
A good splitof suffixes is one where the vectors of probabilitiesof suffixes given a class are orthogonal; we canfind such a split by minimizing the cosine of thetwo columns of the node's Gamma matrix (we callthis the "Gamma cosine").
Thus, a node at whichsuffixes should not be split has a high Gamma co-sine, and when encountering such a node, a cutshould be made.
The suffixes below this node aregrouped together as a paradigm; tree structure be-low the cut node is ignored.
In our experiments wehave selected thresholds for the Gamma cosine, butwe do not know if there is a single value thatwould be successful cross-linguistically.
After thetree has been cut, the Gamma and Beta matricesfor ancestor nodes are normalized and combined toform the M and L matrices for the language.71Another issue is dealing with suboptimal solu-tions.
Random initializations of parameters leadthe EM training procedure to local maxima in thesolution space, and as a result LDA produces dif-fering suffix splits across different runs.
To getaround this, we simply run LDA multiple times (25in our experiments) and choose the solution thatminimizes the Gamma cosine.We also experimented with minimizing theBeta cosine.
The Beta matrix represents stem am-biguity with respect to a suffix split.
Since thereare inherently ambiguous stems, one should notexpect the Beta cosine value to be extremely low.Minimizing the Beta cosine sometimes made theBeta matrix "too disambiguated" and forced therepresentation of ambiguity into Gamma matrix,thereby inflating the Gamma cosine and causingincorrect classifications of suffixes.4 DataWe conducted experiments on English andSpanish.
For English, we chose the Penn Treebank(Marcus et.
al.
1993), which is already POS-tagged; for Spanish, we chose an equivalent-sizedportion of newswire (Graff and Galegos 1999),POS-tagged by the FreeLing morphological ana-lyzer (Carreras et.
al.
2004).
We restricted our datato nouns, verbs, adjectives, and adverbs.
Wordsthat did not follow canonical suffixation patternsfor their POS category (irregulars, foreign words,incorrectly tagged words, etc.)
were excluded.
Wesegmented each word into stem and suffix for aspecified set of suffixes.
Rare suffixes were ex-cluded, such as many English adjective-formingsuffixes and Spanish 2nd person plural forms.Stems were not lemmatized, with the result thatthere can be multiple stem variants of a particularlemma, as with the words stemm.ing$ andstem.s$.
Tokens were not disambiguated forword sense.
Stems that occurred with only one suf-fix were excluded.We use several different representations of suf-fixes in constructing the data matrices: 1) merged,labeled suffixes; 2) merged, unlabeled suffixes; 3)unmerged, unlabeled suffixes.
For unmerged suf-fixes, allomorphs2 are represented in their originalspelling.
A merged suffix is a common representa-2We abuse the standard usage of the term "allomorph"to include gender and conjugational variants.tion for the multiple surface manifestations of anunderlyingly identical suffix.
Suffixes also can beunlabeled, or labeled with base POS tags.
For anexample, a verb created would be segmented ascreate.d$ with an unmerged, labeled suffix, orcreate.d/ed$V with a merged, labeled suffix.Labels disambiguate otherwise categorically am-biguous suffixes.The gold standard for each language lists thesuffixes that belong to a paradigm for stems of aparticular POS category.
We call this the "input"POS category, which is not indicated in annota-tions and is the concept to be predicted.
Thisshould be differentiated from the "output" POSlabels on the suffixes: for example, ly$R attachesto stems of the input category ?adjective?.
Eachsuffix is an atomic entity, so the system actuallyhas no concept of output POS categories.
All thatwe require is that distinct suffixes are given dis-tinct symbols.In the English gold standard (Table 1), eachslashed pair of suffixes denotes one merged form;the unmerged forms are the individual suffixes.ally$R is the suffix ly$R preceded by an epen-thetic vowel, as in the word basically.
In theSpanish gold standard (Table 2), each slashedgroup of suffixes corresponds to one merged form.For adjectives and nouns, a$ and o$ are feminineand masculine singular forms, and as$ and os$are the corresponding plurals.
$ and s$ do nothave gender; es$ is a plural allomorph.mente/amente$R is a derivational suffix.
Thefirst two groups of verbal suffixes are past partici-ples, agreeing in number and gender.
For the otherverb forms, when three are listed they correspondto forms for the 1st, 2nd, and 3rd conjugations.When there are two, the first is for the 1st conjuga-tion, and the other is identical for the 2nd and 3rd.o$V has the same form across all three conjuga-tions.Adjectives: $A, d/ed$A,r/er$A, ally/ly$RNouns: $N, 's$N, es/s$NVerbs: $V, d/ed$V, es/s$V,ing$V, ing$A, ing$N, r/er$NTable 1.
Gold standard for English72Adjectives: a/o/$A, as/os/es/s$A,mente/amente$RNouns:      a/o/$N, as/os/es/s$NVerbs:      ada/ida/ado/ido$V,adas/idas/ados/idos$V, ando/iendo$V,ar/er/ir$V, o$V, as/es$V, a/e$V,amos/emos/imos$V, an/en$V, aba/?a$V,?bamos/?amos$V, aban/?an$V,ar?/er?/ir?$V, ar?/er?/ir?$V,aremos/eremos/iremos$V, ar?n/er?n/ir?n$V,?/?$V, ?/i?$V, aron/ieron$V,ar?a/er?a/ir?a$V, ar?an/er?an/ir?an$VTable 2.
Gold standard for Spanish5 Experiments5.1 Merged, labeled suffixesFigure 1 shows the recursion tree for English datapreprocessed with merged, labeled suffixes.
Toproduce a classification of suffixes into paradigms,we start at the root and go down until reachingnodes with a Gamma cosine greater than or equalto the threshold.
The cut for a threshold of .0009produces three paradigms exactly matching thegold standard for verbs, adjectives, and nouns, re-spectively.
Table 3 shows the complete M matrix,which contains suffix probabilities for each para-digm.
Table 4 shows a portion of the L matrix,which contains the probabilities of stems belongingto paradigms.
We list the stems that are most am-biguous with respect to paradigm membership(note that this table does not specify the words thatbelong to each category, only their stems).
"Verb" "Adj" "Noun"$A 0.000 0.829 0.000d/ed$A 0.020 0.000 0.000r/er$A 0.000 0.033 0.000ing$A 0.008 0.000 0.000$N 0.000 0.000 0.706's$N 0.000 0.000 0.036r/er$N 0.037 0.000 0.000ing$N 0.065 0.000 0.000es/s$N 0.000 0.000 0.257ally/ly$R 0.000 0.138 0.000$V 0.342 0.000 0.000d/ed$V 0.284 0.000 0.000ing$V 0.133 0.000 0.000es/s$V 0.110 0.000 0.000Table 3.
M matrix for English merged, labeledsuffixes.
Columns: p(suff|paradigm).1: .0004$A d/ed$A r/er$A ing$A $N 's$N r/er$N ing$Nes/s$N ally/ly$R $V d/ed$V ing$V es/s$V2: .0000$A d/ed$A r/er$A ing$Ar/er$N ing$N ally/ly$R$V d/ed$V ing$V es/s$V9: .1413$N 's$N es/s$N3: .0009d/ed$A ing$A r/er$N ing$N$V d/ed$V ing$V es/s$V10: .9271$N 's$N11: .0000es/s$N6: .1604$A r/er$Aally/ly$R4: .0061d/ed$A $Vd/ed$V ed/s$V5: .0000ing$A r/er$Ning$N ing$V7: .0000$A8: .0000r/er$Aally/ly$R?nouns??verbs?
?adjectives?Figure 1.
Recursion tree for English merged,labeled suffixes.
Each node shows its currentsuffix set, and the Gamma cosine value for thesplit.
Dotted lines indicate paradigms for aGamma cosine threshold of .0009.
"Verb" "Adj"  "Noun"reset  0.333  0.292  0.375blunt  0.445  0.278  0.277calm  0.417  0.375  0.209total  0.312  0.462  0.226clean  0.478  0.319  0.203parallel  0.222  0.278  0.500alert  0.500  0.222  0.277sound  0.483  0.184  0.333compound  0.372  0.171  0.457pale  0.417  0.417  0.166fine  0.254  0.230  0.516premier  0.235  0.235  0.529brief  0.175  0.524  0.301polish  0.250  0.556  0.194ski  0.378  0.108  0.513fake  0.200  0.600  0.200light  0.092  0.427  0.481foster  0.226  0.161  0.613bottom  0.107  0.304  0.589repurchase  0.333  0.095  0.571Table 4.
Portion of L matrix for English merged,labeled suffixes, sorted by lowest entropy.Columns: p(paradigm|stem).73Next, we examine the morphological and lexicalconditional probabilities in the M and L matrices.It is possible that even though the correctclassification of suffixes into paradigms waslearned, the probabilities may be off.
Table 5shows, however, that the M and L matrices are anextremely accurate approximation of the truemorphological and lexical probabilities.
We haveincluded statistics for the corresponding Spanishexperiment; the paradigms that were discovered forSpanish also match the gold standard.English Spanish# suffixes 14 26# stems 7315 5115CRE M  .0002 bits  .0003 bitsCRE L  .0006 bits  .0020 bitsTable 5.
Comparison of M and L matrices withtrue morphological and lexical probabilities, byconditional relative entropy (CRE).5.2 Unmerged, labeled suffixesThe next experiments tested the effect of allomor-phy on paradigm discovery, using data where suf-fixes are labeled but not merged.
There arecompeting pressures at work in determining howallomorphs are assigned to paradigms: on the onehand, the disjointedness of stem sets for allo-morphs would tend to place them in separate para-digms; on the other hand, if those stem sets haveother suffixes in common that belong to the sameparadigm, the allomorphs might likewise be placedin that paradigm.
In our experiments, we found thatthere was much more variability across runs thanin the merged suffix cases.
In English, for exam-ple, the suffix es$N was sometimes placed in the"verb" paradigm, although the maximally orthogo-nal solution placed it in the ?noun?
paradigm.Figure 2 shows the recursion tree and para-digms for Spanish.
Gold standard noun and adjec-tive categories are fragmented into multipleparadigms in the tree.
Although nouns have acommon parent node (2), the nouns of the differentgenders are placed in separate paradigms -- this isbecause a noun can have only one gender.
Theverbs are all in a single paradigm (node 10).
Node11 contains all the first-conjugation verbs, andnode 12 contains all the second/third-conjugationverbs.
The reason that they are not in separateparadigms is that a$V is shared by stems of allthree conjugations, which leads to a split that is notquite orthogonal.The case of adjectives is the most interesting.Gendered and non-gendered adjective stems aredisjoint, so adjectives appear in two separate sub-trees (nodes 4, 13).
In node 4, the gender-ambiguous plural es$A is in conflict with the plu-ral s$A, but it would conflict with two pluralsas$A and os$A if it were placed in node 13.amente$R appears together in node 14 because itshares stems with the feminine adjectives.amente$R also shares stems with verbs, as it isalso the derivational suffix which attaches to ver-bal past participles in the feminine "a" form.
Thisis probably why the group of adjectives at node 13is a sister to the verb nodes.
The allomorphmente$R attaches to non-gendered adjectives, andis thus in the first adjective group.1: .00002: .0000 9: .00093: .00004: .0008$A es$A s$Amente$R5: .0264o$N os$N7: .0000$N es$Ns$N10: .00988: .0101a$N as$N6: .000011: .0107a$V aba$V aban$V ada$Vadas$V ado$V ados$Vamos$V an$V ando$V ar$Varemos$V aron$V ar?$Var?n$V ar?$V ar?a$Var?an$V as$V o$V?bamos$V ?$V12: .0056e$V emos$V en$V er$V eremos$Ver?$V er?n$V er?$V er?a$Ver?an$V es$V ida$V idas$V ido$Vidos$V iendo$V ieron$V imos$Vir$V iremos$V ir?$V ir?n$Vir?$V ir?a$V ir?an$V i?$V ?$V?$V ?a$V ?amos$V ?an$V13: .002114: .0000a$A as$Aamente$R15: .0133o$A os$A?verbs??adjectives??adjectives?
?nouns?
?nouns?
?nouns?Figure 2.
Recursion tree for Spanish, unmerged,labeled suffixes, with Gamma cosine values.
Dot-ted lines indicate paradigms for a Gamma cosinethreshold of .0021.5.3 Unmerged, unlabeled suffixesThe case of unmerged, unlabeled suffixes is not assuccessful.
In the Gamma matrix for the root node(Table 6), there is no orthogonal division of thesuffixes, as indicated by the high Gamma cosinevalue of .1705.
Despite this, the algorithm has dis-covered useful information.
There is a subpara-74digm of unambiguous suffixes {'s$,ally$}, andanother of {d$,ed$,ing$,r$}.
The other suf-fixes ($,er$,es$,ly$,s$) are ambiguous.
Theambiguity of ly$ seems to be a secondary effect:since adjectives with the null suffix $ are found tobe ambiguous, ly$ is likewise ambiguous.$   [0.9055]   0.0703's$   [0.0351]   0.0000ally$   [0.0007]   0.0000d$    0.0000   [0.1139]ed$    0.0000   [0.1332]er$   [0.0087]   0.0084es$   [0.0089]   0.0001ing$    0.0000   [0.1176]ly$    0.0033   [0.0603]r$    0.0000   [0.0198]s$    0.0378   [0.4764]Table 6.
Gamma matrix for root node, English,unmerged, unlabeled suffixes; the categorizationis shown with brackets.
Columns indicatep(suffix|class).6 Comparison with LinguisticaIn this section, we compare our system withLinguistica3 (Goldsmith 2001), a freely availableprogram for unsupervised discovery of morpho-logical structure.
We focus our attention on Lin-guistica's representation of morphology, ratherthan the algorithm used to learn it.
Linguisticatakes a list of word types, proposes segmentationsof words into stems and suffixes, and organizesthem into signatures.
A signature is a non-probabilistic data structure that groups together allstems that share a common set of suffixes.
Eachstem belongs to exactly one signature, and the setof suffixes for each signature is unique.
For exam-ple, running Linguistica on our raw English text,there is a signature {$,ful$,s$} for the stems{resource, truth, youth}, indicating themorphology of the words {resource$,truth$, youth$, resourceful$, truth-ful$, youthful$, resources$, truths$,youths$}.
There are no POS types in the system.Thus, even for a prototypically "noun" signaturesuch as {$,'s$}, it is quite possible that not all ofthe words that the signature represents are actuallynouns.
For example, the word structure$ is in3http://linguistica.uchigago.eduthis signature, but occurs both as a noun (59 times)and a verb (2 times) in our corpus.The signature model can be derived from thesuffix-stem data matrix, by first converting allpositive counts to 1, and then placing in separategroups all the stems that have the same 0/1 columnpattern.
Another way to view the signature is as aspecial case of the probabilistic paradigm where allprobabilities are restricted to being 0 or 1, for ifthis were so, the only way to fit the data would beto let there be a canonical paradigm for every dif-ferent subset of suffixes that some stem appearswith.
In theory, it is possible for the number of sig-natures to be exponential in the number of suffixes;in practice, Linguistica finds hundreds of signa-tures for English and Spanish.
Although there hasbeen work on reducing the number of signatures(Goldwater and Johnson 2004; Hu et.
al.
2005,who report a reduction of up to 30%), the numberof remaining signatures is still two orders of mag-nitude greater than the number of canonical para-digms we find.
The simplest explanation for this isthat a suffix can be listed many times in the differ-ent signatures, but only has one entry in  the Mmatrix of the probabilistic paradigm.It is important for a natural language system tohandle out-of-vocabulary words.
A signature doesnot predict the forms of potential but unseen formsof stems.
To some extent Linguistica could ac-commodate this, as it identifies when one signa-ture's suffixes are a proper subset of another's, butit does not handle cases where suffixes are partiallyoverlapping.
One principal advantage of the prob-abilistic paradigm is that the canonical paradigmallows the instantiation of a lexical paradigm con-taining a complete set of predicted word forms fora stem.Since Linguistica is a system that starts fromraw text, it may seem that it cannot be directlycompared to our work, which assumes that seg-mentations and suffixes are already known.
How-ever, it is possible to run Linguistica on our data bydoing further preprocessing.
We rewrite the corpusin such a way that Linguistica can detect correctmorphological and POS information for each to-ken.
Each token is replaced by the concatenation ofits stem, the dummy string 12345, and a single-character encoding of its merged suffix.
For exam-ple, the token accelerate.d/ed$V is mapped toaccelerate12345D, where D represents d/ed$V.The omnipresence of the dummy string enables75Linguistica to discover all the desired stems andsuffixes, but no more.
By mapping the input corpusin this way, we can examine the type of grammarthat Linguistica would find if it knew the informa-tion that we have assumed in the previous experi-ments.
Linguistica found 565 signatures from the"cooked" English data (Figure 3).
50% of wordtypes are represented by the first 13 signatures.1.
{ $N, es/s$N } 1540abortion absence accent acceptanceaccident accolade accommodation2.
{ $N, 's$N } 1168aba abbie abc academy achenbaum acluadams addington addison adobe3.
{ $N, 's$N, es/s$N } 224accountant acquisition actoradministration airline airport alliance5.
{ $A, ally/ly$R } 319abrupt absolute abundant accurateactual additional adequate adroit6.
{ $A, $N, es/s$N } 173abrasive acid activist adhesive adultafghan african afrikaner aggregate7.
{ $V, d/ed$V, es/s$V } 135abate achieve administer afflictaggravate alienate amass apologize9.
{ $V, d/ed$V, ing$V, es/s$V } 73abound absorb adopt applaud assertassist attend attract avert avoid13.
{ $N, $V, d/ed$V, es/s$N, es/s$V } 44advocate amount attribute battlebounce cause compromise declineFigure 3.
Selected top signatures for merged, labeledsuffix English data.
Each signature shows the suffix set,number of stems, and several example stems.
Ranking isby log(num stems)* log(num suffixes).We have formulated two metrics to evaluate thequality of a collection of signatures or paradigms.Ideally, all suffixes of a particular signature wouldbe of the same category, and all the words of a par-ticular category would be contained within onesignature.
POS fragmentation measures to whatextent the words of an input POS category are scat-tered across different signatures.
It is the averagenumber of bits required to encode the probabilitydistribution of some category?s words over signa-tures.
Signature impurity measures the extent towhich the suffixes of a signature are of mixed in-put POS types.
It is the expected value of the num-ber of bits required to encode the probabilitydistribution of some signature?s suffixes over inputPOS categories.
Table 7 shows that, according tothese metrics, the signature does not organize mor-phological information as efficiently as probabilis-tic paradigms4.
Linguistica?s impurity scores arereasonably low because many of the signatureswith the most stems are categorically homogene-ous.
Fragmentation scores show that the placementof the majority of words within top signatures off-sets the scattering of a POS category?s suffixesover many signatures.
(1) POS fragmentation  =P   )) P of words|S(( ?????????
?ph(2) Signature impurity  =??
????????
?SSS.numstems)) S|P ((S.numstems phh: entropyP: input POS categoriesS: signatures / paradigmsLinguistica Recursive LDAEnglish fragmentation 5.422 bits  0 bitsEnglish impurity .404 bits  0 bitsSpanish fragmentation 6.084 bits  0 bitsSpanish impurity .332 bits   0 bitsTable 7.
Comparison of Linguistica and recursive LDAon merged, labeled suffix data.
The maximum possibleimpurity for 3 POS categories is log2(3) = 1.585 bits.Finally, a morphological grammar should reflectthe general, abstract morphological structure of thelanguage from which a corpus was sampled.
Totest for consistency of morphological grammarsacross corpora, we split our cooked English datainto two equal parts.
Linguistica found 449 totalsignatures for the first half and 462 for the second.296 signatures were common to both (in terms ofthe suffixes contained by the signatures).
Of the3506 stems shared by both data sets, 1831 (52.2%)occurred in the same signature.
Of the top 50 sig-natures for each half-corpus, 45 were in common,and 1651 of 2403 shared stems (68.7%) occurredin the same signature.
Recursive LDA found the4Our scores would not be so good if we had chosen apoor Gamma cosine threshold value for classification.However, Linguistica?s scores cannot be decreased, asthere is only one signature model for a fixed set ofstems and suffixes.76same canonical paradigms for both data sets(which matched the gold standard).
Differences inword counts between the corpus halves alteredstem inventories and lexical probabilities, but notthe structure of the canonical paradigms.
Our sys-tem thus displays a robustness to corpus choicethat does not hold for Linguistica.7 Future WorkThis section sketches some ideas for future work toincrease the linguistic adequacy of the system, andto make it more unsupervised.1.
Bootstrapping: for fully unsupervised learning,we need to hypothesize stems and suffixes.
Theoutput of recursive LDA indicates which suffixesmay be ambiguous.
To bootstrap a disambiguatorfor the different categorial uses of these suffixes,one could use various types of distributional in-formation, as well as knowledge of partial para-digmatic structure for non-ambiguous suffixes.2.
Automated detection of cut nodes: currently thesystem requires that the user select a Gamma co-sine threshold for extracting paradigms from therecursion tree.
We would like to automate thisprocess, perhaps with different heuristics.3.
Suffix merging and formulation of generationrules: when we decide that two suffixes should bemerged (based on some measures of distributionalsimilarity and word-internal context), we also needto formulate phonological (i.e., spelling) rules todetermine which surface form to use when instan-tiating a form from the canonical paradigm.4.
Non-regular forms: we can take advantage ofempty cells in the data matrix in order to identifynon-regularities such as suppletives, stem variants,semi-regular subclasses, and suffix allomorphs.
Ifthe expected frequency of a word form (as derivedfrom the M matrix and frequency of a stem) is rela-tively high but the value in the D matrix is zero,this is evidence that a non-regular form may oc-cupy this cell.
Locating irregular words could usemethods similar to those of (Yarowsky and Wicen-towski 2000), who pair irregular inflections andtheir roots from raw text.
Stem variants and allo-morphic suffixes could be detected in a similarmanner, by finding sets of stems/suffixes with mu-tually exclusive matrix entries.5.
Multiple morphological properties per word: wecurrently represent all morphological and POS in-formation with a single suffix.
The learning algo-rithm and representation could perhaps bemodified to allow for multiple morphologicalproperties.
One could perform recursive LDA on aparticular morphological property, then take eachof the learned paradigms and perform recursiveLDA again, but for a different morphologicalproperty.
This method might discover Spanish con-jugational classes as subclasses within ?verbs?.8 DiscussionThis paper has introduced the probabilistic para-digm model of morphology.
It has some importantbenefits: it is an abstract, compact representation ofa language's morphology, it accommodates lexicalambiguity, and it predicts forms of words not seenin the input data.We have formulated the problem of learningprobabilistic paradigms as one of discovering la-tent classes within a suffix-stem count matrix,through the recursive application of LDA with anorthogonality constraint.
Under optimal data condi-tions, it can learn the correct paradigms, and alsomodels morphological and lexical probabilitiesextremely accurately.
It is robust to corpus choice,so we can say that it learns a morphological gram-mar for the language.
This is a new application ofmatrix factorization algorithms, and an usual one:whereas in document topic modeling, one tries tofind that a document consists of multiple topics,we want to find orthogonal decompositions whereeach suffix (document) belongs to only one inputPOS category (topic).We have demonstrated that the algorithm cansuccessfully learn morphological paradigms forEnglish and Spanish under the conditions thatsegmentations are known, categorically ambiguoussuffixes have been distinguished, and allomorphshave been merged.
When suffixes have not beenmerged, there is a tendency to place allomorphicvariants in different paradigms.
The algorithm isthe least successful in the unmerged, unlabeledcase, as ambiguous suffixes do not allow for aclear split of suffixes into paradigms.
However, theprogram output indicates which suffixes are poten-tially ambiguous or unambiguous, and this infor-mation could be used by bootstrapping proceduresfor suffix disambiguation.Some of the behavior of the learning algorithmcan be explained in terms of several constraints.First, LDA assumes conditional independence of77documents (suffixes) given topics (paradigms).
Astem should be able to occur with each suffix of acanonical paradigm.
But if a stem occurs with oneallomorphic variant of a suffix, we know that itnecessarily cannot occur with the other.
Therefore,allomorphy violates conditional independence ofsuffixes given a paradigm, and we cope with thisby merging allomorphs.
Second, LDA also as-sumes conditional independence of words (stems)given topics (paradigm).
As our data contains stemvariants, this assumption does not hold either, butit is a less serious violation due to the large numberof total stems.
Third, we have imposed the con-straint of orthogonality of suffixes and paradigms,which is not required by LDA (and actually unde-sired in document topic modeling, since documentscan contain multiple topics).
Orthogonal suffixsplits are possible when categorically ambiguoussuffixes have been disambiguated.In conclusion, we view morphology learningas a process of manipulating the representation ofdata to fit a learnable computational model.
Thealternative would be to complicate the model andlearning algorithm to accommodate raw data andall its concurrent ambiguities and dependencies.We hypothesize that successful, fully unsupervisedlearning of linguistically adequate representationsof morphology will be more easily accomplishedby first bootstrapping the sorts of information thatwe have assumed, or, in other words, fitting thedata to the model.AcknowledgementsThis work was supported by the National ScienceFoundation under grant NSF IIS-0415138.
Theauthor thanks Mitch Marcus and anonymous re-viewers for their helpful comments.ReferencesA.
Albright.
2002.
The identification of bases in mor-phological paradigms.
Ph.D. thesis, UCLA.D.
Blei, A. Ng, and M. Jordan.
2003.
Latent dirichletallocation.
Journal of Machine Learning Research 3,993-1022.X.
Carreras, I. Chao, L.
Padr?, and M. Padr?.
2004.FreeLing: an open-source suite of language analyz-ers.
Proceedings of LREC.
Lisbon, Portugal.A.
Clark.
2001.
Learning morphology with pair hiddenmarkov models.
Proceedings of the Student Work-shop at ACL.R.
Evans and G. Gazdar.
1996.
DATR: A language forlexical knowledge representation.
ComputationalLinguistics 22(2), 167-216.M.
Forsberg and A. Ranta.
2004.
Functional morphol-ogy.
Proceedings of the ICFP, 213-223.
ACM Press.D.
Freitag.
2005.
Morphology induction from term clus-ters.
Proceedings of CoNLL.J.
Goldsmith.
2001.
Unsupervised learning of the mor-phology of a natural language.
Computational Lin-guistics 27(2), 153-198.S.
Goldwater and M. Johnson.
2004.
Priors in bayesianlearning of phonological rules.
Proceedings ofSIGPHON.D.
Graff and G. Gallegos.
1999.
Spanish newswire text,volume 2.
Linguistic Data Consortium, Philadelphia, PA.Y.
Hu, I. Matveeva, J. Goldsmith, and C. Sprague.2005.
Using morphology and syntax together in un-supervised learning.
Workshop on Psychocomputa-tional Models of Human Language Acquisition.D.
Kazakov and S. Manandhar.
2001.
Unsupervisedlearning of word segmentation rules with genetic al-gorithms and inductive logic programming.
MachineLearning 43, 121-162.M.
Marcus, B. Santorini and M.A.
Marcinkiewicz.1993.
Building a large annotated corpus of English:The Penn Treebank.
Computational Linguistics19(2), 313-330.C.
Monson, A. Lavie, J. Carbonell, and L. Levin.
2004.Unsupervised induction of natural language mor-phology inflection classes.
Proc.
of SIGPHON.K.
Oflazer, S. Nirenburg, and M. McShane.
2001.
Boot-strapping morphological analyzers by combininghuman elicitation and machine learning.
Computa-tional Linguistics 27(1), 59-85.P.
Schone and D. Jurafsky.
2001.
Knowledge-free in-duction of inflectional morphologies.
Proc.
NAACL.M.
Snover, G. Jarosz, and M. Brent.
2002.
Unsuper-vised learning of morphology using a novel directedsearch algorithm: taking the first step.
Proceedings ofSIGPHON.D.
Yarowsky and R. Wicentowski.
2000.
Minimallysupervised morphological analysis by multimodalalignment.
Proceedings of ACL.R.
Zajac.
2001.
Morpholog: constrained and supervisedlearning of morphology.
Proceedings of CoNLL.78
