Coling 2010: Poster Volume, pages 144?152,Beijing, August 2010The True Score of Statistical Paraphrase GenerationJonathan Chevelu1,2 Ghislain Putois2 Yves Lepage3(1) GREYC, universite?
de Caen Basse-Normandie(2) Orange Labs(3) Waseda University{jonathan.chevelu,ghislain.putois}@orange-ftgroup.com,yves.lepage@aoni.waseda.jpAbstractThis article delves into the scoring func-tion of the statistical paraphrase genera-tion model.
It presents an algorithm forexact computation and two applicative ex-periments.
The first experiment analysesthe behaviour of a statistical paraphrasegeneration decoder, and raises some is-sues with the ordering of n-best outputs.The second experiment shows that a majorboost of performance can be obtained byembedding a true score computation in-side a Monte-Carlo sampling based para-phrase generator.1 IntroductionA paraphrase generator is a program which, givena source sentence, produces a new sentence withalmost the same meaning.
The modification placeis not imposed but the paraphrase has to differfrom the original sentence.Paraphrase generation is useful in applicationswhere it is needed to choose between differentforms to keep the most fit.
For instance, automaticsummary can be seen as a particular paraphras-ing task (Barzilay and Lee, 2003) by selecting theshortest paraphrase.
They can help human writersby proposing alternatives and having them choosethe most appropriate (Max and Zock, 2008).Paraphrases can also be used to improve nat-ural language processing (NLP) systems.
Inthis direction, (Callison-Burch et al, 2006) triedto improve machine translations by enlargingthe coverage of patterns that can be translated.In the same way, most NLP systems like in-formation retrieval (Sekine, 2005) or question-answering (Duclaye et al, 2003), based on pat-tern recognition, can be improved by a paraphrasegenerator.Most of these applications need a n-best set ofsolutions in order to rerank them according to atask-specific criterion.In order to produce the paraphrases, a promis-ing approach is to see the paraphrase genera-tion problem as a statistical translation problem.In that approach, the target language becomesthe same as the source language (Quirk et al,2004; Bannard and Callison-Burch, 2005; Maxand Zock, 2008).The first difficulty of this approach is the needof a paraphrase table.
A paraphrase table is amonolingual version of a translation table in thestatistical machine translation (SMT) field.
In thisfield, the difficulty is basically overcome by us-ing huge aligned bilingual corpora like the Eu-roparl (Koehn, 2005) corpus.
In the paraphrasegeneration field, one needs a huge aligned mono-lingual corpus to build a paraphrase table.The low availability of such monolingual cor-pora nurtures researches in order to find heuris-tics to produce them (Barzilay and Lee, 2003;Quirk et al, 2004).
On the other hand, an interest-ing method proposed by (Bannard and Callison-Burch, 2005) tries to make a paraphrase table us-ing a translation table learned on bilingual cor-pora.
The method uses a well-known heuris-tic (Lepage and Denoual, 2005) which says thatif two sentences have the same translation, thenthey should be paraphrases of each others.Another aspect, less studied, is the generationprocess of paraphrases, i.e.
the decoding processin SMT.
This process is subject to combinatorial144explosions.
Heuristics are then frequently used todrive the exploration process in the a priori in-tractable high dimensional spaces.
On the onehand, these heuristics are used to build a para-phrase step by step according to the paraphrasetable.
On the other hand, they try to evaluate therelevance of a step according to the global para-phrase generation model.
The SMT model scoreis related to the path followed to generate a para-phrase.
Because of the step-by-step computation,different ways can produce the same paraphrase,but with different scores.
Amongst these scores,the best one is the true score of a paraphrase ac-cording to the SMT model.Most paraphrase generators use some standardSMT decoding algorithms (Quirk et al, 2004) orsome off-the-shelf decoding tools like MOSES.The goal of these decoders is to find the best pathin the lattice produced by the paraphrase table.This is basically achieved by using dynamic pro-gramming ?
especially the Viterbi algorithm ?
andbeam searching (Koehn et al, 2007).
The bestparaphrase proposed by these programs is knownnot to be the optimal paraphrase.
One can evenquestion if the score returned is the true score.We first show in Section 2 that in the particulardomain of statistical paraphrase generation, onecan compute true a posteriori scores of generatedparaphrases.
We then explore some applicationsof the true score algorithm in the paraphrase gen-eration field.
In Section 3, we show that scores re-turned by SMT decoders are not always true scoresand they plague the ranking of output n-best solu-tions.
In Section 4, we show that the true score cangive a major boost for holistic paraphrases gener-ators which do not rely on decoding approaches.2 True Score Computing2.1 ContextThe phrase based SMT model (Koehn et al, 2003)can be transposed to paraphrase generation as fol-lows:t?
= arg maxtP (t)?
P (s|t, B)where s is the source sentence, t the target sen-tence i.e.
the paraphrase, t?
the best paraphraseand B a model of the noisy channel between thesource and target languages i.e.
the paraphrase ta-ble.
This can be decomposed into:t?
?
arg maxt,IP (t)?i?IP (sIi |tIi , B)where I is a partition of the source sentence andxIi the ith segment in the sentence x.
For a givencouple of s, t sentences, it exists several segmen-tations I with different probabilities.This is illustrated in Example 1.
Depending onthe quality of the paraphrase table, one can find upto thousands of paraphrase segments for a sourcesentence.
Note that the generated paraphrases arenot always semantically or even syntactically cor-rect, as in P2.
P3 illustrates the score evaluationproblem: it can be generated by applying to thesource sentence the sequences of transformations{T1, T2} , {T1, T4, T5} or even {T5, T1, T4}.
.
.Example 1 DecodingSource sentence:The dog runs after the young cat.Paraphrase table excerpt:T1: P(the beast | the dog) = 0.8T2: P(the kitten | the young cat) = 0.7T3: P(after it | after the) = 0.4T4: P(the | the young) = 0.05T5: P(cat | kitten) = 0.1Some possible generated paraphrases:P1: the beast runs after the young cat.P2: *the dog runs after it young cat.P3: the beast runs after the kitten.We define the score of a potential paraphrase tfollowing a segmentation I as:ZIt = P (t)?i?IP (sIi |tIi , B)The true score of a potential paraphrase t is de-fined as:Z?t = maxI ZIt145Because of high-dimension problems, decodersapply sub-optimal algorithms to search for t?.They produce estimated solutions over all possibleparaphrases t and over all possible segmentationsI .
Actually, for a given paraphrase t, they con-sider only some ZIt where they should estimateZ?I .
SMT decoders are overlooking the partition-ing step in their computations.There is no reason for the decoder solution toreach the true score.
Troubles arise when oneneeds the scores of generated paraphrases, for in-stance when the system must produce an orderedn-best solution.
What is the relevance of the es-timated scores ?
and orders ?
with respect to thetrue scores ?
and orders ?
of the model?
Is the truescore able to help the generation process?2.2 AlgorithmLet us first adopt the point of view proposedin (Chevelu et al, 2009).
The paraphrase gener-ation problem can be seen as an exploration prob-lem.
We seek the best paraphrase according to ascoring function in a space to search by applyingsuccessive transformations.
This space is com-posed of states connected by actions.
An actionis a transformation rule with a place where it ap-plies in the sentence.
States are a sentence witha set of possible actions.
Applying an action ina given state consists in transforming the sentenceof the state and removing all rules that are no moreapplicable.
In this framework, each state, exceptthe root, can be a final state.The SMT approach fits within this point of view.However, generation and evaluation need not to becoupled any longer.
Computing the true score ofa generated paraphrase is in reality a task com-putationally easier than generating the best para-phrases.
Once the target result is fixed, the num-ber of sequences transforming the source sentenceinto the target paraphrase becomes computation-ally tractable under a reasonable set of assump-tions:A1: the transformation rules have disjoint sup-ports (meaning that no rule in the sequenceshould transform a segment of the sentencealready transformed by one of of the previ-ous applied rules) ;A2: no reordering model is applied during theparaphrasing transformation.Under this set of assumptions, the sequence (or-dered) of transformation rules becomes a set (un-ordered) of transformation rules.
One can there-fore easily determine all the sets of transforma-tion rules from the source sentence to the tar-get paraphrase: they are a subset of the cross-product set of every transformation rule with asource included in the source sentence and witha result included in the target paraphrase.
Andthis cross-product set remains computationallytractable.
Note that to guarantee a solution, thecorpus of all rules should be augmented with anidentity rule for each word of the source sentence(with an associated probability of applicability setto 1) missing in the paraphrase table.The algorithm for computing ex post the truescore is given on algorithm 1.Algorithm 1 Algorithm for true scoreLet S be the source sentence.Let T be the target sentence.Let R : sR ?
tR be a transformation ruleLet map : (S, T )?
C be a functionLet C = {?
}?shead|S = shead.stail,?R ?
{?|sR = shead, T = tR.ttail}C = C ?
({R}?map(Stail, Ttail))return CLet score be the scoring function for a transfor-mation rule settruescoreS,?
(T ) = arg maxc?map(S,T )(score(c))For our toy example, we would get the stepsshown in Example 2.3 True Score of SMT DecodersWe have shown that it is possible to computethe true score according to the paraphrase model.We now evaluate scores from a state-of-the-art146Example 2 True Score ComputationGenerated sets:{R1}, {R1, R3}, {R1, R2},{R1, R4}, {R1, R4, R5},{R3},{R2},{R4}, {R4, R5},{R5}For a better readability, all identity rules are omitted.The true scores are computed as in the following examples:score( ?the dog runs after the small cat.?
?
?the beast runs after it small cat?
)= score({R1})score( ?the dog runs after the small cat.?
?
?the beast runs after the kitten?
)= max(score({R1, R2}), score({R1, R4, R5}))decoder against this baseline.
In particular, weare interested in the order of n-best outputs.
Weuse the MOSES decoder (Koehn et al, 2007) as arepresentative SMT decoder inside the system de-scribed below.3.1 System descriptionParaphrase generation tools based on SMT meth-ods need a language model and a paraphrase table.Both are computed on a training corpus.The language models we use are n-gram lan-guage models with back-off.
We use SRILM (Stol-cke, 2002) with its default parameters for this pur-pose.
The length of the n-grams is five.To build a paraphrase table, we use a variantof the construction method via a pivot languageproposed in (Bannard and Callison-Burch, 2005).The first step consists in building a bilingual trans-lation table from the aligned corpus.
Given asource phrase si and another phrase ti in a differ-ent language, a bilingual translation table providesthe two probabilities p(si|ti) and p(ti|si).
We useGIZA++ (Och and Ney, 2003) with its default pa-rameters to produce phrase alignments.
The para-phrase table is then built from the phrase transla-tion table.
The probability for a phrase si to beparaphrased by a phrase s?i in the same languageis estimated by the sum of each round-trip from sito s?i through any phrase ti of a pivot language.The construction of this table is very simple.Given a bilingual translation table sorted by pivotphrases, the algorithm retrieves all the phraseslinked with the same pivot (named a pivot clus-ter).
For each ordered pair of phrases, the programassigns a probability that is the product of thereprobabilities.
This process realizes a self-join ofthe bilingual translation table.
It produces a para-phrase table composed of tokens, instead of items.The program just needs to sum up all probabilitiesfor all entries with identical paraphrase tokens toproduce the final paraphrase table.Three heuristics are used to prune the para-phrase table.
The first heuristic prunes any entryin the paraphrase table composed of tokens with aprobability lower than a threshold .
The second,called pruning pivot heuristic, consists in deletingall pivot clusters larger than a threshold ?
.
Thelast heuristic keeps only the ?most probable para-phrases for each source phrase in the final para-phrase table.
For this study, we empirically fix = 10?5, ?
= 200 and ?
= 20.The MOSES scoring function is set by fourweighting factors ?
?, ?LM , ?D, ?W .
Conven-tionally, these four weights are adjusted during atuning step on a training corpus.
The tuning step isinappropriate for paraphrasing because there is nosuch tuning corpus available.
We empirically set??
= 1, ?LM = 1, ?D = 10 and ?W = 0.
Thismeans that the paraphrase table and the languagemodel are given the same weight, no reordering isallowed and no specific sentence length is favored.3.2 Experimental ProtocolFor experiments reported in this paper, we useone of the largest, multi-lingual, freely availablealigned corpus, Europarl (Koehn, 2005).
It con-sists of European parliament debates.
We chooseFrench as the language for paraphrases and En-glish as the pivot language.
For this pair oflanguages, the corpus consists of 1,723,705 sen-tences.
Note that the sentences in this corpusare long, with an average length of 30 words perFrench sentence and 27.8 for English.
We ran-domly extract 100 French sentences as a test cor-147pus.For each source sentence from the test corpus,the SMT decoder tries to produce a 100-best dis-tinct paraphrase sequence.
Using the algorithm 1,we compute the true score of each paraphrase andrerank them.
We then compare orders output bythe decoder with the true score order by using theKendall rank correlation coefficient (?A) (Kendall,1938).
In this context, the Kendall rank corre-lation coefficient considers each couple of para-phrases and checks if their relative order is pre-served by the reranking.
The ?A formula is:?A =np ?
ni12n(n?
1)where np the number of preserved orders, nd thenumber of inverted orders and n the number of el-ements in the sequence.
The coefficient provides ascore ?
between -1 and 1 ?
that can be interpretedas a correlation coefficient between the two or-ders.
In order to compare same length sequences,we filter out source sentences when MOSES cannot produce enough distinct paraphrases.
The testcorpus is therefore reduced to 94 sentences.3.3 ResultsThe evolution of ?A means relative to the lengthof the n-best sequence is given Figure 1.
The ?Ameans drops to 0.73 with a standard deviation of0.41 for a 5-best sequence which means that theorders are clearly different but not decorrelated.A finer study of the results reveals that amongstthe generated paraphrases, 32% have seen theirscore modified.
18% of the MOSES 1-best para-phrases were not optimal anymore after the truescore reranking.
After reranking, the old top bestsolutions have dropped to a mean rank of 2.0 ?17.7 (40th rank at worse).
When consideringonly the paraphrases no longer optimal, they havedropped to a mean rank of 6.8?
12.9.From the opposite point of view, new top para-phrases after reranking have come from a meanrank of 4.4 ?
12.1.
When considering only theparaphrases that were not optimal, they have comefrom a mean rank of 21.2?23.5.
Some have comefrom the 67th rank.
Even an a posteriori rerank-ing would not have retrieved this top solution ifthe size of MOSES n-best list were too short.
Thisn-best paraphrase sequence sizemeansKendallrankcorrelationcoefficient10 20 30 40 50 60 70 80 90 1000.70.750.80.850.90.95Figure 1: Evolution of ?A means relative to thelength of the n-best sequenceadvocates for a direct embedding of the true scorefunction inside the generation process.In this section we have shown that MOSESscores are not consistent with the true score asexpected from the paraphrase model.
In partic-ular, the n-best paraphrase sequence computed byMOSES is not trustworthy while it is an input forthe task system.4 True Score to boost Monte-Carlobased Paraphrase GenerationThere exist other less common approaches morelenient than the Viterbi algorithm, which are holis-tic, i.e.
they work on the whole sentence ratherthan step-by-step.
The Monte-Carlo based Para-phrase Generation algorithm (MCPG) proposedin (Chevelu et al, 2009) turns out to be an inter-esting algorithm for the study of paraphrase gen-eration.
It does not constraint the scoring functionto be incremental.
In this section, we embed thenon incremental true score function in MCPG todrive the generation step and produce n-best or-ders compliant with the paraphrase model, andshow that the true score function can be used toprovide a major boost to the performance of such148an algorithm.4.1 DescriptionThe MCPG algorithm is a derivative of the Up-per Confidence bound applied to Tree algorithm(UCT).
UCT (Kocsis and Szepesva?ri, 2006), aMonte-Carlo planning algorithm, has recently be-come popular in two-player game problems.UCT has some interesting properties:?
it expands the search tree non-uniformly andfavours the most promising sequences, with-out pruning branch;?
it can deal with high branching factors;?
it is an any-time algorithm and returns bestsolutions found so far when interrupted;?
it does not require expert domain knowledgeto evaluate states.These properties make it ideally suited for prob-lems with high branching factors and for whichthere is no strong evaluation function.For the same reasons, this algorithm is inter-esting for paraphrase generation.
In particular, itdoes not put constraint on the scoring function.
Adiagram of the MCPG algorithm is presented Fig-ure 2.The main part of the algorithm is the samplingstep.
An episode of this step is a sequence ofstates and actions, s1, a1, s2, a2, .
.
.
, sT , from theroot state to a final state.
Basically, a state is apartially generated paraphrase associated with aset of available actions.
A final state is a poten-tial paraphrase.
An action is a transformation rulefrom the paraphrase table.
During an episode con-struction, there are two ways to select the action aito perform from a state si.If the current state was already explored in aprevious episode, the action is selected accord-ing to a compromise between exploration and ex-ploitation.
This compromise is computed usingthe UCB-Tunned formula (Auer et al, 2001) as-sociated with the RAVE heuristic (Gelly and Sil-ver, 2007).
If the current state is explored forthe first time, its score is estimated using Monte-Carlo sampling.
In other words, to complete theSource sentenceExploration/exploitationcompromiseStatealreadyexplored?Monte-Carlo samplingEnoughiterations?New root selection stepFinalstate?Output paraphraseSampling stepYesNoYesNoNoYesFigure 2: The MCPG algorithm.149episode, the actions ai, ai+1, .
.
.
, aT?1, aT are se-lected randomly until reaching a final state.At the end of each episode, a reward is com-puted for the final state sT using a scoring func-tion, and the value of each (state, action) pair ofthe episode is updated.
Then, the algorithm com-putes another episode with the new values.Periodically, the sampling step is stopped andthe best action at the root state is selected.
Thisaction is then definitively applied and a samplingis restarted from the new root state.
The actionsequence is incrementally built and selected afterbeing sufficiently sampled.
For our experiment,we have chosen to stop sampling regularly after afixed amount ?
of episodes.The adaptation of the original algorithm takesplace in the (state, action) value updating proce-dure.
Since the goal of the algorithm is to max-imise a scoring function, it uses the maximumreachable score from a state as value instead ofthe score expectation.
This algorithm suits theparadigm recalled in Section 2 for paraphrase gen-eration.To provide scores comparable with the para-phrase model scores, the standard version ofMCPG has to apply rules until the whole sourcesentence is covered.
With this behaviour, MCPGacts in a monolingual ?translator?
mode.The embedding of the true score algorithm inMCPG has given meaningful scores to all states.The algorithm needs not to ?translate?
the wholesentence to get a potential paraphrase and itsscore.
This MCPG algorithm in ?true-score?
modecan choose to stop its processing with segmentsstill unchanged, which solves, amongst others,out-of-vocabulary questions found in decoder-based approaches.4.2 Experimental ProtocolFor this experiment, we reuse the paraphrase ta-ble and the corpora generated for the experimentpresented in Section 3.2;We compare the 1-best outputs from MOSESreranked by the true score function and fromMCPG in both ?translator?
and ?true-score?modes.
For MCPG systems, we set the followingparameters: ?
= 100,000 iterations.1-best paraphrase index(ordered by MOSES reranked scores)Paraphrasescore(inlog)20 40 60 80 100-500-400-300-200-1000Figure 3: Comparison of paraphrase generators.Top: the MOSES baseline; middle and bold: the?true-score?
MCPG; down: the ?translator?
MCPG.The use of ?true-score?
improves the MCPG per-formances.
MCPG reaches MOSES performancelevel.4.3 ResultsFigure 3 presents a comparison between thescores from each systems, ordered by MOSESreranked scores.The boost of performance gained by using truescores inside the MCPG algorithm reaches a meansof 28.79 with a standard deviation of 34.19.
Themean difference between ?true-score?
MCPG andMOSES is ?14.13 (standard deviation 19.99).
Al-though the performance remains inferior to theMOSES true score baseline, it still leads to animprovement over the ?translator?
MCPG system.The later system has a mean difference of perfor-mance with MOSES of?42.92 (standard deviationof 40.14).The true score reduces the number of transfor-mations needed to generate a paraphrase, whichsimplifies the exploration task.
Moreover, it re-duces the number of states in the explorationspace: two sets of transformations producing thesame paraphrase now leads to the same state.These points explain why MCPG has become moreefficient.Although MCPG is improved by embedding the150true score algorithm, there is still room for im-provement.
In its current version, MCPG does notadapt the number of exploration episodes to theinput sentence.5 Conclusion and perspectivesIn this paper, we have developed a true scoring al-gorithm adapted to the statistical paraphrase gen-eration model.
We have studied its impacts on acommon SMT decoder and a Monte-Carlo sam-pling based paraphrase generator.
It has revealedthat the n-best outputs by SMT decoders were notviable.
It has also proved useful in simplifying theexploration task and in improving holistic para-phrase generators.Thanks to the boost introduced by the true scorealgorithm in holistic paraphrase generators, theirperformances are now on a par with scores pro-duced by statistical translation decoders.
More-over, they produce guaranteed ordering, and en-able the integration of a global task scoring func-tion, which seems still out of reach for decoder-based systems.A more general problem remains open: whatdo the scores and the orders output by the modelmean when compared to a human subjective eval-uation?In preliminary results on our test corpus, lessthan 37% of the MOSES generated paraphrases canbe considered both syntactically correct and se-mantically a paraphrase of their original sentence.One could study the relations between scores fromthe model and subjective evaluations to create pre-dictive regression models.
The true score algo-rithm can autonomously score existing paraphrasecorpora which could be used to adapt the SMT tun-ing step for paraphrase generation.We note that the hundredth best paraphrasesfrom MOSES have a score close to the best para-phrase: the mean difference is 5.9 (standard de-viation 4.5) on our test corpus.
This is smallerthan the mean difference score between MOSESand MCPG.
In (Chevelu et al, 2009), both systemswere rated similar by a subjective evaluation.
Onecould question the relevance of small score differ-ences and why the best paraphrase should be se-lected instead of the hundred next ones.
Given thecurrent state of the art, the next step to improveparaphrase generation does not lie in score opti-misation but in refining the model and its com-ponents: the language model and the paraphrasetable.Human based evaluations reveal that the currentmost important issue of paraphrase generation liesin the syntax (Chevelu et al, 2009).
It seems dif-ficult to assess the syntax of a potential paraphrasewhile not considering it as a whole, which is im-possible with a local scoring function inherent tothe SMT decoding paradigm.
Holistic paraphrasegenerators have now reached a level of perfor-mance comparable to SMT decoders, without suf-fering from their limitations.
They are paving theway for experiments with more complex semanticand linguistic models to improve paraphrase gen-eration.151ReferencesAuer, P., N. Cesa-Bianchi, and C. Gentile.
2001.Adaptive and self-confident on-line learning algo-rithms.
Machine Learning.Bannard, Colin and Chris Callison-Burch.
2005.
Para-phrasing with bilingual parallel corpora.
In ACL?05: Proceedings of the 43rd Annual Meeting onAssociation for Computational Linguistics, pages597?604, Morristown, NJ, USA.
Association forComputational Linguistics.Barzilay, Regina and Lillian Lee.
2003.
Learn-ing to paraphrase: An unsupervised approach us-ing multiple-sequence alignment.
In HLT-NAACL2003: Main Proceedings, pages 16?23.Callison-Burch, Chris, Philipp Koehn, and Miles Os-borne.
2006.
Improved statistical machine transla-tion using paraphrases.
In Proceedings of the mainconference on Human Language Technology Con-ference of the North American Chapter of the As-sociation of Computational Linguistics, pages 17?24, Morristown, NJ, USA.
Association for Compu-tational Linguistics.Chevelu, Jonathan, Thomas Lavergne, Yves Lepage,and Thierry Moudenc.
2009.
Introduction of a newparaphrase generation tool based on Monte-Carlosampling.
In Su, Keh-Yih, Jian Su, Janyce Wiebe,and Haizhou Li, editors, Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 249?252, Singapoure, August.
Association for Computa-tional Linguistics.Duclaye, Florence, Franc?ois Yvon, and Olivier Collin.2003.
Learning paraphrases to improve a question-answering system.
In In Proceedings of the 10thConference of EACL Workshop Natural LanguageProcessing for Question-Answering, page 3541.Gelly, Sylvain and David Silver.
2007.
Combining on-line and offline knowledge in UCT.
In 24th Interna-tional Conference on Machine Learning (ICML?07),pages 273?280, June.Kendall, Maurice G. 1938.
A New Measure of RankCorrelation.
Biometrika, 1?2(30):81?89, June.Kocsis, Levente and Csaba Szepesva?ri.
2006.
Ban-dit based monte-carlo planning.
In 17th Euro-pean Conference on Machine Learning, (ECML?06),pages 282?293, September.Koehn, Philipp, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
InProceedings of the Human Language TechnologyConference of the North American Chapter of theAssociation for Computational Linguistics (HLT-NAACL), pages 48?54, Edmonton, May.
Associa-tion for Computational Linguistics.Koehn, Philipp, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bo-jar, Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Annual Meeting of the Associationfor Computation Linguistics (ACL), DemonstrationSession, pages 177?180, June.Koehn, Philipp.
2005.
Europarl: A parallel corpusfor statistical machine translation.
In Proceedingsof MT Summit.Lepage, Yves and Etienne Denoual.
2005.
Automaticgeneration of paraphrases to be used as translationreferences in objective evaluation measures of ma-chine translation.
In IWP2005.Max, Aure?lien and Michael Zock.
2008.
Lookingup phrase rephrasings via a pivot language.
InProceedings of the 22nd International Conferenceon Computational Linguistics (Coling 2008), pages97?104, Manchester, UK, August.
Coling 2008 Or-ganizing Committee.Och, Franz Josef and Hermann Ney.
2003.
A sys-tematic comparison of various statistical alignmentmodels.
Computational Linguistics, 29(1):19?51.Quirk, Chris, Chris Brockett, and Bill Dolan.
2004.Monolingual machine translation for paraphrasegeneration.
In Lin, Dekang and Dekai Wu, edi-tors, the 2004 Conference on Empirical Methodsin Natural Language Processing, pages 142?149.,Barcelona, Spain, 25-26 July.
Association for Com-putational Linguistics.Sekine, Satoshi.
2005.
Automatic paraphrase dis-covery based on context and keywords between nepairs.
In Proceedings of International Workshop onParaphrase (IWP2005).Stolcke, Andreas.
2002.
Srilm ?
an extensible lan-guage modeling toolkit.
In Proceedings of Interna-tional Conference on Spoken Language Processing.152
