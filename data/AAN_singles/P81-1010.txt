What Makes Evaluation Hard?1.0 THE GOAL OF EVALUATIONIdeally, an evaluation technique shoulddescribe an algorithm that an evaluator coulduse that would result in a score or a vectorof scores that depict the level ofperformance of the natural language systemunder test.
The scores should mirror thesubjective evaluation of the system that aqual i f ied judge would make.
The evaluationtechnique should yield consistent scores formult iple tests of one system, and the scoresfor several systems should serve as a meansfor comparison among systems.
Unfortunately,there is no such evaluation technique fornatural language understanding systems.
Inthe following sections, I wi l l  attempt tohighl ight some of the dif f icult ies2.0 PERSPECTIVE OF THE EVALUATIONThe first problem is to determine whothe "qualif ied judge" is whose judgements areto be modeled by the evaluation.
One view isthat he be an expert in languageunderstanding.
As such, his primary interestwould be in the l inguistic and conceptualcoverage of the system.
He may attach thegreatest weight to the coverage ofconstruct ions and concepts which he knows tobe dif f icult  to include in a computerprogram.Another view of the judge is that he isa user of the system.
His primary interestis in whether the system can understand himwell enough to satisfy his needs.
This Judgewil l  put greatest weight on the system'sabil ity to handle his most crit icall inguistic and conceptual requirements:those used most frequently and those whichoccur  infrequently but must be satisfied.This judge will also want to compare thenatural language system to othertechnologies.
Furthermore, he may attachstrong weight to systems which can be learnedquickly, or whose use may be easilyremembered, or which takes time to learn butprovides the user with considerable poweronce it is learned.The characterist ics of the judge are notan impediment to evaluation, but if thecharacterist ics are not clearly understood,the meaning of the results wil l  be confused.3.0 TESTING WXTH USERS3.1 Who Are The Users?It is surprising to think that naturallanguage research has existed as long as ithas and that the statement of the goals isstill as vague as it is.
In particular,l ittle commitment is made on what kind ofuser a natural language understanding systemis intended to serve.
In particular, l ittleis specified about what the users know aboutthe domain and the language understandingsystem.
The taxonomy below is presented asHarry TennantPO Box 225621, M/S 371Texas Instruments, Inc.Dallas, Texas 75265an example of user character ist ics based onwhat the user knows about the domain and thesystem.Classes of Users of database query systemsV Familiar with the database and itssoftwareIV Familiar with the database and theinteraction languageIll Familiar with the contents of databaseII Familiar with the domain of appl icationI Passing knowledge of the domain ofapplicationOf course, as users gain experience witha system, they will continual ly attempt toadapt to its quirks.
If the purpose of theevaluation is to demonstrate that the naturallanguage understanding system is merelyuseable, adaptation resents no problem.However, if natural language is being used toallow the user to express himself in hisaccustomed manner, adaptation does becomeimportant.
Again, the goals of naturallanguage systems have been left vague.
Arenatural language systems to be i) immediatelyuseful, 2) easi ly learned 3) highlyexpressive or 4) readily remembered throughperiods of disuse?
The evaluation shouldattempt to test for these goals specifically,and must control for factors such asadaptation.What a user knows (either throughinstruction or experience) about the domain,the database and the interaction languagehave a signif icant effect on how he wil lexpress himself.
Database query systemsusually expect a certain level of use ofdomain or database specific jargon, andfamil iarity with construct ions that arecharacterist ic of the domain.
A system mayperform well for class IV users with querieslike,i) What are the NORMU for AAFs in 71 bymonth?However, it may fare poorly for class I userswith queries like,2) I need to find the length of time thatthe attack planes could not be flown in1971 because they were undergoingmaintenance.
Exclude all preventativemaintenance, and give me totals for eachplane for each month.3.2 What Does Success Rate Mean?A common method for generating dataagainst which to test a system is to haveusers use it, then calculate how successfulthe system was at satisfying user needs.
Ifthe evaluation attempts to calculate thefraction of questions that the systemunderstood, it is important to characterizehow diff icult  the queries were to understand.For example, twelve queries of the form,373) How many hours of down time did plane 3have in January, 19714) How many hours of down time did plane 3have in February, 1971will h~ip the success rate more than onequery like,5) How many hours of down time did plane 3have in each month of 1971,However, ~ne query like 5 returns as muchinformation as the other twelve.
In testingPLANES (Tennant, 1981\], the users whosequestions were understood with the highestrates of success actually had less success atsolving the problems they were trying tosolve.
They spent much of their time askingmany easy, repetitive questions and so didnot have time to attempt some of theproblems.
Other users who asked more compactquestions had plenty of time to hammer awayat the queries that the system had thegreatest difficulty understanding.Another difficulty with success ratemeasurement is the characteristics of theproblems given to users compared to the kindof problems anticipate~ by the system.
Ionce asked a set of users to write someproblems for other users to attempt to solveusing PLANES.
The problem authors werefamiliar with the general domain of discourseof pLANES, but did not have any experienceusing it.
The problems they devised were~easonable given the domain, but were largelybeyond the scope of PLANES ~ conceptualcoverage.
Users had very low success rateswhen attempting to solve these problems.
Incontrast, problems that I had devised, fullyaware of pLANES ~ areas of most completeCoverage (and devised to be easy for PLANES},yielded much higher success rates.
Smallwonder.
The point is that unless the matchbetween the problems and a system'sconceptual coverage can be characterlsed,success ~ates mean little.4?0 TAXONOMY OF CAPABILITIESTesting a natural language system forits performance with with users is anengineering approach.
Another approach is tocompare the elements that are known to beinvolved in understanding language againstthe capabilities of the system.
This hasbeen called "sharpshooting" by some of theimplementers of natural language systems.
Anevaluator probes the system under test tofind conditions under which it fails.
Tomake this an organized approach, theevaluator should base his probes on ataxonomy of phenomena that are relevant tolanguage understanding.
A standard taxonomycould be developed for doing evaluations.Our knowledge of language is incompleteat best.
Any taxonomy is bound to generatedisagreement.
However, it seems that most ofthe disagreements describing language are notover what the phenomena of language are, butover how we might best understand and modelthose phenomena.
The taxonomy will becomequite large, but this is only representativeof the fact that understanding language is avery complex process.
The taxonomy approachfaces the problem of complexity directly.The taxonomy approach to evaluationforces examination of the broad range ofissues of natural language processing.
Itprovides a relatively objective means forassessing the full range of capabilities of anatural language understanding system.
Italso avoids the problems listed aboveinherent in evaluation through user testing.It does, however, have some unpleasantattributes.
First, it does not provide aneasy basis for comparison of systems.Ideally an evaluation would produce a metricto allow one to say "system A is better thansystem B".
Appealing as it is, naturallanguage understanding is probably toocomplex for a simple metric to be meaningful.Second, the taxonomy approach does notprovide a means for compar ison  of naturallanguage understanding to other technologies.That comparison can be done rather well withuser testing, however.Third, the taxonomy approach ignores therelative importance of phenomena and theinteraction between phenomena and domains ofdiscourse.
In response to this difficulty,an evaluation should include the analysis ofa simulated natural language system.
Thesimulated system would consist of a htnnanInterprete~ who acts as an intermediarybetween users and the programs or data theyare trying to use.
Dialogs are recorded,then those dialogs are analyzed in light ofthe taxonomies of features.
In this way, thecapabilities of the system can be compared tothe needs of the users.
The relativeimportance of phenomena can be determinedthis way.
Furthermore, users" language canbe studied without them adapting to thesystem's limitations.The ~axonomy of phenomena mentionedabove is intended to Include both lingulstlcphenomena and concepts.
The linguisticphenomena relate to how ideas may beunderstood.
There is an extensive literatureon this.
The concepts are the ideas whichmust be understood.
This is much moreextensive, and much more domain specific.Work in knowledge representation is partiallyfocused on learning what concepts need to berepresented, then attempting to representthem.
Consequently, ther~ is a taxonomy ofconcepts implicit in the knowledgerepresentation literature.ReferenceTennant, Harry.
Evaluation of NaturalLanguage processors.
Ph.D. Thesis,University of Illinois, Urbana, Illiniois,1981.38
