Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 148?153,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsIntegrating an Unsupervised Transliteration Model intoStatistical Machine TranslationNadir DurraniUniversity of Edinburghdnadir@inf.ed.ac.ukHieu Hoang Philipp KoehnUniversity of Edinburghhieu.hoang,pkoehn@inf.ed.ac.ukHassan SajjadQatar Computing Research Institutehsajjad@@qf.org.qaAbstractWe investigate three methods for integrat-ing an unsupervised transliteration modelinto an end-to-end SMT system.
We in-duce a transliteration model from paralleldata and use it to translate OOV words.Our approach is fully unsupervised andlanguage independent.
In the methodsto integrate transliterations, we observedimprovements from 0.23-0.75 (?
0.41)BLEU points across 7 language pairs.
Wealso show that our mined transliterationcorpora provide better rule coverage andtranslation quality compared to the goldstandard transliteration corpora.1 IntroductionAll machine translation (MT) systems suffer fromthe existence of out-of-vocabulary (OOV) words,irrespective of the amount of data available fortraining.
OOV words are mostly named entities,technical terms or foreign words that can be trans-lated to the target language using transliteration.Much work (Al-Onaizan and Knight, 2002;Zhao et al., 2007; Kashani et al., 2007; Habash,2009) has been done on transliterating named enti-ties and OOVs, and transliteration has been shownto improve MT quality.
Transliteration has alsoshown to be useful for translating closely relatedlanguage pairs (Durrani et al., 2010; Nakov andTiedemann, 2012), and for disambiguation (Her-mjakob et al., 2008; Azab et al., 2013).
How-ever, despite its utility, a transliteration moduledoes not exist in the commonly used MT toolk-its, such as Moses (Koehn et al., 2007).
One of themain reasons is that the training data, a corpus oftransliteration pairs, required to build a translitera-tion system, is not readily available for many lan-guage pairs.
Even if such a training data is avail-able, mechanisms to integrate transliterated wordsinto MT pipelines are unavailable in these toolkits.Generally, a supervised transliteration system istrained separately outside of an MT pipeline, anda na?
?ve approach, to replace OOV words with their1-best transliterations in the post/pre-processingstep of decoding is commonly used.In this work i) we use an unsupervised modelbased on Expectation Maximization (EM) to in-duce transliteration corpus from word aligned par-allel data, which is then used to train a translitera-tion model, ii) we investigate three different meth-ods for integrating transliteration during decoding,that we implemented within the Moses toolkit.
Tothe best of our knowledge, our work is the fore-most attempt to integrate unsupervised translitera-tion model into SMT.This paper is organized as follows.
Section 2describes the unsupervised transliteration miningsystem, which automatically mines transliterationpairs from the same word-aligned parallel corpusas used for training the MT system.
Section 3 de-scribes the transliteration model that is trained us-ing the automatically extracted pairs.
Section 4presents three methods for incorporating translit-eration into the MT pipeline, namely: i) replac-ing OOVs with the 1-best transliteration in a post-decoding step, ii) selecting the best translitera-tion from the list of n-best transliterations usingtransliteration and language model features in apost-decoding step, iii) providing a transliterationphrase-table to the decoder on the fly where itcan consider all features to select the best translit-eration of OOV words.
Section 5 presents re-sults.
Our integrations achieved an average im-provement of 0.41 BLEU points over a competi-tive baseline across 7 language pairs (Arabic, Ben-gali, Farsi, Hindi, Russian, Telugu and Urdu-into-English).
An additional experiment showed thatour system provides better rule coverage as op-posed to another built from gold standard translit-eration corpus and produces better translations.1482 Transliteration MiningThe main bottleneck in building a transliterationsystem is the lack of availability of translitera-tion training pairs.
It is, however, fair to assumethat any parallel data would contain a reasonablenumber of transliterated word pairs.
Transliter-ation mining can be used to extract such wordpairs from the parallel corpus.
Most previoustechniques on transliteration mining generally usesupervised and semi-supervised methods (Sherifand Kondrak, 2007; Jiampojamarn et al., 2010;Darwish, 2010; Kahki et al., 2012).
This con-strains the mining solution to language pairs forwhich training data (seed data) is available.
A fewresearchers proposed unsupervised approaches tomine transliterations (Lee and Choi, 1998; Sajjadet al., 2011; Lin et al., 2011).
We adapted the workof Sajjad et al.
(2012) as summarized below.Model: The transliteration mining model is amixture of two sub-models, namely: a translit-eration and a non-transliteration sub-model.
Theidea is that the transliteration model would as-sign higher probabilities to transliteration pairscompared to the probabilities assigned by a non-transliteration model to the same pairs.
Consider aword pair (e, f), the transliteration model prob-ability for the word pair is defined as follows:ptr(e, f) =?a?Align(e,f)|a|?j=1p(qj)where Align(e, f) is the set of all possible se-quences of character alignments, a is one align-ment sequence and qjis a character alignment.The non-transliteration model deals with theword pairs that have no character relationship be-tween them.
It is modeled by multiplying sourceand target character unigram models:pntr(e, f) =|e|?i=1pE(ei)|f |?i=1pF(fi)The transliteration mining model is definedas an interpolation of the transliteration sub-modeland the non-transliteration sub-model:p(e, f) = (1?
?
)ptr(e, f) + ?pntr(e, f)?
is the prior probability of non-transliteration.The non-transliteration model does not changeduring training.
We compute it in a pre-processingstep.
The transliteration model learns characteralignment using expectation maximization (EM).See Sajjad et al.
(2012) for more details.3 Transliteration ModelNow that we have transliteration word pairs, wecan learn a transliteration model.
We segment thetraining corpus into characters and learn a phrase-based system over character pairs.
The translitera-tion model assumes that source and target charac-ters are generated monotonically.1Therefore wedo not use any reordering models.
We use 4 basicphrase-translation features (direct, inverse phrase-translation, and lexical weighting features), lan-guage model feature (built from the target-side ofmined transliteration corpus), and word and phrasepenalties.
The feature weights are tuned2on a dev-set of 1000 transliteration pairs.4 Integration to Machine TranslationWe experimented with three methods for integrat-ing transliterations, described below:Method 1: involves replacing OOVs in the out-put with the 1-best transliteration.
The success ofMethod 1 is solely contingent on the accuracy ofthe transliteration model.
Also, it ignores con-text which may lead to incorrect transliteration.For example, the Arabic word transliteratesto ?Bill?
when followed by ?Clinton?
and ?Bell?if preceded by ?Alexander Graham?.Method 2: provides n-best transliterations toa monotonic decoder that uses a monolinguallanguage model and a transliteration phrase-translation table to rescore transliterations.
Wecarry forward the 4 translation model features usedin the transliteration system to build a transliter-ation phrase-table.
We additionally use an LM-OOV feature which counts the number of wordsin a hypothesis that are unknown to the lan-guage model.
Smoothing methods such as Kneser-Ney assign significant probability mass to unseenevents, which may cause the decoder to make in-correct transliteration selection.
The LM-OOVfeature acts as a prior to penalize such hypotheses.Method 3: Method 2 can not benefit from all in-decoding features and phenomenon like reorder-ing.
It transliterates Urdu compound(Arabian Sea) to ?Sea Arabian?, if is an un-known word.
In method 3, we feed the translitera-tion phrase-table directly into the first-pass decod-ing which allows reordering of UNK words.
We1Mining algorithm also makes this assumption.2Tuning data is subtracted from the training corpus whiletuning to avoid over-fitting.
After the weights are tuned, weadd it back, retrain GIZA, and estimate new models.149use the decoding-graph-backoff option in Moses,that allows multiple translation phrase tables andback-off models.
As in method 2, we also use theLM-OOV feature in method 3.35 EvaluationData: We experimented with 7 language pairs,namely: Arabic, Bengali, Farsi, Hindi, Russian,Telugu and Urdu-into-English.
For Arabic4andFarsi, we used the TED talks data (Cettolo et al.,2012) made available for IWSLT-13, and we usedthe dev2010 set for tuning and the test2011 andtest2012 sets for evaluation.
For Indian languageswe used the Indic multi-parallel corpus (Post etal., 2012), and we used the dev and test sets pro-vided with the parallel corpus.
For Russian, weused WMT-13 data (Bojar et al., 2013), and weused half of the news-test2012 for tuning and otherhalf for testing.
We also evaluated on the news-test2013 set.
For all, we trained the languagemodel using the monolingual WMT-13 data.
SeeTable 1 for data statistics.Lang TraintmTraintrDev Test1Test2AR 152K 6795 887 1434 1704BN 24K 1916 775 1000FA 79K 4039 852 1185 1116HI 39K 4719 1000 1000RU 2M 302K 1501 1502 3000TE 45K 4924 1000 1000UR 87K 9131 980 883Table 1: No.
of sentences in Training Data andMined Transliteration Corpus (Types) (Traintr)Baseline Settings: We trained a Moses systemreplicating the settings used in competition-gradesystems (Durrani et al., 2013b; Birch et al., 2013):a maximum sentence length of 80, GDFA sym-metrization of GIZA++ alignments (Och and Ney,2003), an interpolated Kneser-Ney smoothed 5-gram language model with KenLM (Heafield,2011) used at runtime, a 5-gram OSM (Dur-rani et al., 2013a), msd-bidirectional-fe lexical-3Method 3 is desirable in cases where the decoder cantranslate or transliterate a word.
For example Hindi wordcan be translated to ?Border?
and also transliteratedto name ?Seema?.
Identifying such candidates that can betranslated or transliterated is a challenge.
Machine learningtechniques (Goldwasser and Roth, 2008; Kirschenbaum andWintner, 2009) and named entity recognizers (Klementievand Roth, 2006; Hermjakob et al., 2008) have been used forthis purpose.
Though, we only focus on OOV words, method3 can be used if such a classifier/NE tagger is available.4Arabic and Urdu are segmented using MADA (Habashand Sadat, 2006) and UWS (Durrani and Hussain, 2010).ized reordering, sparse lexical and domain fea-tures (Hasler et al., 2012), a distortion limit of6, 100-best translation options, MBR decoding(Kumar and Byrne, 2004), Cube Pruning (Huangand Chiang, 2007), and the no-reordering-over-punctuation heuristic.
We tuned with the k-bestbatch MIRA (Cherry and Foster, 2012).5Transliteration Miner: The miner extractstransliterations from a word-aligned parallel cor-pus.
We only used word pairs with 1-to-1 align-ments.6Before feeding the list into the miner, wecleaned it by removing digits, symbols, word pairswhere source or target is composed from less than3 characters, and words containing foreign char-acters that do not belong to this scripts.
We ranthe miner with 10 iterations of EM.
The numberof transliteration pairs (types) extracted for eachlanguage pair is shown in Table 1 (Traintr).Transliteration System: Before evaluating ourintegrations into the SMT system, we performedan intrinsic evaluation of the transliteration systemthat we built from the mined pairs.
We formedtest data for Arabic?English (1799 pairs), Hindi?English (2394 pairs) and Russian?English (1859pairs) by concatenating the seed data and goldstandard transliteration pairs both provided for theShared Task on Transliteration mining (Kumaranet al., 2010).
Table 2 shows precision and recall ofthe mined transliteration system (MTS).AR HI RUPrecision (1-best Accuracy) 20.0% 25.3% 46.1%Recall (100-best Accuracy) 80.2% 79.3% 87.5%Table 2: Precision and Recall of MTSThe precision (1-best accuracy) of the translit-eration model is quite low.
This is because thetransliteration corpus is noisy and contains imper-fect transliteration pairs.
For example, the minerextracted the pair ( , Australasia), whilethe correct transliteration is ?Australia?.
We canimprove the precision by tightening the miningthreshold probability.
However, our end goal is toimprove end-to-end MT and not the transliterationsystem.
We observed that recall is more importantthan precision for overall MT quality.
We providean empirical justification for this when discussingthe final experiments.5Retuning the transliteration features was not helpful, de-fault weights are used.6M-N/1-N alignments are less likely to be transliterations.150MT Experiments: Table 3 gives a comprehen-sive evaluation of the three methods of integra-tion discussed in Section 4 along with the num-ber7of OOV words (types) in different tests.
Wereport BLEU gains (Papineni et al., 2002) obtainedby each method.
Method 1 (M1), that replacesOOV words with 1-best transliteration gave an av-erage improvement of +0.13.
This result can be at-tributed to the low precision of the transliterationsystem (Table 2).
Method 2 (M2), that translit-erates OOVs in second pass monotonic decoding,gave an average improvement of +0.39.
Slightlyhigher gains were obtained using Method 3 (M3),that integrates transliteration phrase-table insidedecoder on the fly.
However, the efficacy of M3incomparison to M2is not as apparent, as M2pro-duced better results than M3in half of the cases.Lang Test B0M1M2M3OOVAR iwslt1126.75 +0.12 +0.36 +0.25 587iwslt1229.03 +0.10 +0.30 +0.27 682BN jhu1216.29 +0.12 +0.42 +0.46 1239FA iwslt1120.85 +0.10 +0.40 +0.31 559iwslt1216.26 +0.04 +0.20 +0.26 400HI jhu1215.64 +0.21 +0.35 +0.47 1629RU wmt1233.95 +0.24 +0.55 +0.49 434wmt1325.98 +0.25 +0.40 +0.23 799TE jhu1211.04 -0.09 +0.40 +0.75 2343UR jhu1223.25 +0.24 +0.54 +0.60 827Avg 21.9 +0.13 +0.39 +0.41 950Table 3: End-to-End MT Evaluation ?
B0=Baseline, M1= Method1, M2= Method2, M3=Method3, BLEU gains shown for each methodIn an effort to test whether improving translit-eration precision would improve end-to-end SMTresults, we carried out another experiment.
Insteadof building a transliteration system from minedcorpus, we built it using the gold standard corpus(for Arabic, Hindi and Russian), that we also usedpreviously to do an intrinsic evaluation.
We thenreplaced our mined transliteration systems withthe gold standard transliteration systems, in thebest performing SMT systems for these languages.Table 4 shows a comparison of performances.
Al-though the differences are small, systems usingmined transliteration system (MTS) outperformedits counterpart that uses gold standard translitera-tion system (GTS), except in Hindi?English where7Note that not all OOVs can be transliterated.
This num-ber is therefore an upper bound what can be transliterated.both systems were equal.AR HI RUiwslt11iwslt12jhu12wmt12iwslt13MTS 27.11 29.33 16.11 34.50 26.38GST 26.99 29.20 16.11 34.33 26.22Table 4: Comparing Gold Standard Transliteration(GST) and Mined Transliteration SystemsIn the error analysis we found that the GSTsystem suffered from sparsity and did not pro-vide enough coverage of rules to produce righttransliterations.
For example, Arabic drops thedeterminer (al), but such additions were notobserved in gold transliteration pairs.
Arabicword (Gigapixel) is therefore translit-erated to ?algegabksl?.
Similarly the GST systemlearned no transliteration pairs to account for therule ?b ?
p?
and therefore erroneously translit-erated (Spurlock) to ?Sbrlok?.
Similarobservations were true for the case of Russian?English.
The rules ?a?
u?
and ?y?
?
were notobserved in the gold set, and hence(hurricane) was transliterated to ?herricane?
and(Talbot) to ?Talboty?.
This shows thatbetter recall obtained from the mined pairs led tooverall improvement.6 ConclusionWe incorporated unsupervised transliteration min-ing model into standard MT pipeline to automati-cally transliterate OOV words without needing ad-ditional resources.
We evaluated three methodsfor integrating transliterations on 7 language pairsand showed improvements ranging from 0.23-0.75(?
0.41) BLEU points.
We also showed that ourmined transliteration corpus provide better recalland overall translation quality compared to thegold standard transliteration corpus.
The unsu-pervised transliteration miner and its integrationto SMT has been made available to the researchcommunity via the Moses toolkit.AcknowledgmentsWe wish to thank the anonymous reviewers andKareem Darwish for their valuable feedback onan earlier draft of this paper.
The research lead-ing to these results has received funding fromthe European Union Seventh Framework Pro-gramme (FP7/2007-2013) under grant agreementn?287658.
This publication only reflects the au-thors?
views.151ReferencesYaser Al-Onaizan and Kevin Knight.
2002.
Translat-ing Named Entities Using Monolingual and Bilin-gual Resources.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics.Mahmoud Azab, Houda Bouamor, Behrang Mohit, andKemal Oflazer.
2013.
Dudley North visits NorthLondon: Learning When to Transliterate to Arabic.In Proceedings of the 2013 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 439?444, Atlanta, Georgia, June.
Associationfor Computational Linguistics.Alexandra Birch, Nadir Durrani, and Philipp Koehn.2013.
Edinburgh SLT and MT System Descriptionfor the IWSLT 2013 Evaluation.
In Proceedingsof the 10th International Workshop on Spoken Lan-guage Translation, pages 40?48, Heidelberg, Ger-many, December.Ondrej Bojar, Christian Buck, Chris Callison-Burch,Christian Federmann, Barry Haddow, PhilippKoehn, Christof Monz, Matt Post, Radu Soricut,and Lucia Specia.
2013.
Findings of the 2013Workshop on Statistical Machine Translation.
InEighth Workshop on Statistical Machine Transla-tion, WMT-2013, pages 1?44, Sofia, Bulgaria.Mauro Cettolo, Christian Girardi, and Marcello Fed-erico.
2012.
WIT3: Web Inventory of Transcribedand Translated Talks.
In Proceedings of the 16thConference of the European Association for Ma-chine Translation (EAMT), pages 261?268, Trento,Italy, May.Colin Cherry and George Foster.
2012.
Batch Tun-ing Strategies for Statistical Machine Translation.
InProceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,pages 427?436, Montr?eal, Canada, June.
Associa-tion for Computational Linguistics.Kareem Darwish.
2010.
Transliteration Mining withPhonetic Conflation and Iterative Training.
In Pro-ceedings of the 2010 Named Entities Workshop, Up-psala, Sweden.Nadir Durrani and Sarmad Hussain.
2010.
Urdu WordSegmentation.
In Human Language Technologies:The 2010 Annual Conference of the North AmericanChapter of the Association for Computational Lin-guistics, pages 528?536, Los Angeles, California,June.
Association for Computational Linguistics.Nadir Durrani, Hassan Sajjad, Alexander Fraser, andHelmut Schmid.
2010.
Hindi-to-Urdu MachineTranslation through Transliteration.
In Proceedingsof the 48th Annual Conference of the Association forComputational Linguistics, Uppsala, Sweden.Nadir Durrani, Alexander Fraser, Helmut Schmid,Hieu Hoang, and Philipp Koehn.
2013a.
CanMarkov Models Over Minimal Translation UnitsHelp Phrase-Based SMT?
In Proceedings of the51st Annual Meeting of the Association for Compu-tational Linguistics, Sofia, Bulgaria, August.
Asso-ciation for Computational Linguistics.Nadir Durrani, Barry Haddow, Kenneth Heafield, andPhilipp Koehn.
2013b.
Edinburgh?s Machine Trans-lation Systems for European Language Pairs.
InProceedings of the Eighth Workshop on StatisticalMachine Translation, Sofia, Bulgaria, August.
As-sociation for Computational Linguistics.Dan Goldwasser and Dan Roth.
2008.
Active Sam-ple Selection for Named Entity Transliteration.
InProceedings of ACL-08: HLT, Short Papers, pages53?56, Columbus, Ohio, June.
Association for Com-putational Linguistics.Nizar Habash and Fatiha Sadat.
2006.
Arabic Pre-processing Schemes for Statistical Machine Transla-tion.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Companion Vol-ume: Short Papers, pages 49?52, New York City,USA, June.
Association for Computational Linguis-tics.Nizar Habash.
2009.
REMOOV: A Tool for OnlineHandling of Out-of-Vocabulary Words in MachineTranslation.
In Proceedings of the Second Interna-tional Conference on Arabic Language Resourcesand Tools, Cairo, Egypt, April.
The MEDAR Con-sortium.Eva Hasler, Barry Haddow, and Philipp Koehn.
2012.Sparse Lexicalised Features and Topic Adaptationfor SMT.
In Proceedings of the seventh Interna-tional Workshop on Spoken Language Translation(IWSLT), pages 268?275.Kenneth Heafield.
2011.
KenLM: Faster and SmallerLanguage Model Queries.
In Proceedings of theSixth Workshop on Statistical Machine Translation,pages 187?197, Edinburgh, Scotland, United King-dom, 7.Ulf Hermjakob, Kevin Knight, and Hal Daum?e III.2008.
Name Translation in Statistical MachineTranslation - Learning When to Transliterate.
InProceedings of the 46th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies, Columbus, Ohio.Liang Huang and David Chiang.
2007.
Forest Rescor-ing: Faster Decoding with Integrated LanguageModels.
In Proceedings of the 45th Annual Meet-ing of the Association of Computational Linguistics,pages 144?151, Prague, Czech Republic, June.
As-sociation for Computational Linguistics.Sittichai Jiampojamarn, Kenneth Dwyer, ShaneBergsma, Aditya Bhargava, Qing Dou, Mi-YoungKim, and Grzegorz Kondrak.
2010.
Transliteration152Generation and Mining with Limited Training Re-sources.
In Proceedings of the 2010 Named EntitiesWorkshop, Uppsala, Sweden.Ali El Kahki, Kareem Darwish, Ahmed Saad El Din,and Mohamed Abd El-Wahab.
2012.
Transliter-ation Mining Using Large Training and Test Sets.In Proceedings of the 2012 Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics: Human Language Technologies,NAACL HLT ?12.Mehdi M. Kashani, Eric Joanis, Roland Kuhn, GeorgeFoster, and Fred Popowich.
2007.
Integration ofan Arabic Transliteration Module into a StatisticalMachine Translation System.
In Proceedings of theSecond Workshop on Statistical Machine Transla-tion, Prague, Czech Republic.Amit Kirschenbaum and Shuly Wintner.
2009.
LightlySupervised Transliteration for Machine Translation.In Proceedings of the 12th Conference of the Euro-pean Chapter of the ACL (EACL 2009), pages 433?441, Athens, Greece, March.
Association for Com-putational Linguistics.Alexandre Klementiev and Dan Roth.
2006.
Namedentity transliteration and discovery from multilin-gual comparable corpora.
In Proceedings of theHuman Language Technology Conference of theNAACL, Main Conference, pages 82?88, New YorkCity, USA, June.
Association for ComputationalLinguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondrej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: OpenSource Toolkit for Statistical Machine Translation.In Proceedings of the 45th Annual Meeting of theAssociation for Computational Linguistics, Demon-stration Program, Prague, Czech Republic.Shankar Kumar and William J. Byrne.
2004.
Mini-mum Bayes-Risk Decoding for Statistical MachineTranslation.
In HLT-NAACL, pages 169?176.A Kumaran, Mitesh M. Khapra, and Haizhou Li.
2010.Whitepaper of news 2010 shared task on transliter-ation mining.
In Proceedings of the 2010 NamedEntities Workshop, pages 29?38, Uppsala, Sweden,July.
Association for Computational Linguistics.Jae-Sung Lee and Key-Sun Choi.
1998.
Englishto Korean Statistical Transliteration for InformationRetrieval.
Computer Processing of Oriental Lan-guages, 12(1):17?37.Wen-Pin Lin, Matthew Snover, and Heng Ji.
2011.Unsupervised Language-Independent Name Trans-lation Mining from Wikipedia Infoboxes.
In Pro-ceedings of the First workshop on UnsupervisedLearning in NLP, pages 43?52, Edinburgh, Scot-land, July.
Association for Computational Linguis-tics.Preslav Nakov and J?org Tiedemann.
2012.
Com-bining Word-Level and Character-Level Models forMachine Translation Between Closely-Related Lan-guages.
In Proceedings of the 50th Annual Meet-ing of the Association for Computational Linguis-tics (Volume 2: Short Papers), pages 301?305, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Franz J. Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Mod-els.
Computational Linguistics, 29(1).Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofthe 40th Annual Meeting on Association for Compu-tational Linguistics, ACL ?02, pages 311?318, Mor-ristown, NJ, USA.Matt Post, Chris Callison-Burch, and Miles Osborne.2012.
Constructing Parallel Corpora for Six IndianLanguages via Crowdsourcing.
In Proceedings ofthe Seventh Workshop on Statistical Machine Trans-lation, pages 401?409, Montr?eal, Canada, June.
As-sociation for Computational Linguistics.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2011.
An Algorithm for Unsupervised Translitera-tion Mining with an Application to Word Alignment.In Proceedings of the 49th Annual Conference ofthe Association for Computational Linguistics, Port-land, USA.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.2012.
A Statistical Model for Unsupervised andSemi-supervised Transliteration Mining.
In Pro-ceedings of the 50th Annual Conference of the Asso-ciation for Computational Linguistics, Jeju, Korea.Tarek Sherif and Grzegorz Kondrak.
2007.
Bootstrap-ping a Stochastic Transducer for Arabic-EnglishTransliteration Extraction.
In Proceedings of the45th Annual Meeting of the Association for Compu-tational Linguistics, Prague, Czech Republic.Bing Zhao, Nguyen Bach, Ian Lane, and Stephan Vo-gel.
2007.
A Log-Linear Block TransliterationModel based on Bi-Stream HMMs.
In HumanLanguage Technologies 2007: The Conference ofthe North American Chapter of the Association forComputational Linguistics, Rochester, New York.153
