Automatic Detection and Correction of Repairs inHuman-Computer Dialog*Elizabeth Shriberg~ John Bear, John DowelingSRI In ternat iona lMenlo Park ,  Cal i fornia 94025ABSTRACTWe have analyzed 607 sentences of spontaneous human-computer speech data containing repairs (drawn from a cor-pus of 10,718).
We present here criteria and techniques forautomatical ly detecting the presence of a repair, its loca-tion, and making the appropriate correction.
The criteria in-volve integrat ion of knowledge from several sources: pat ternmatching, syntactic and semantic analysis, and acoustics.1.
INTRODUCTIONSpontaneous spoken language often includes peech thatis not intended by the speaker to be part of the con-tent of the utterance.
This speech must be detectedand deleted in order to correctly identify the intendedmeaning.
This broad class of disfluencies encompassesa number of phenomena, including word fragments, in-terjections, filled pauses, restarts, and repairs.
We areanalyzing the repairs in a large subset (over ten thou-sand sentences) of spontaneous speech data collected forthe DARPA spoken language program.
We have cate-gorized these disfluencies as to type and frequency, andare investigating methods for their automatic detectionand correction.
Here we report promising results on de-tection and correction of repairs by combining patternmatching, syntactic and semantic analysis, and acous-tics.The problem of disfluent speech for language under-standing systems has been noted but has received limitedattention, ttindle \[5\] attempts to delimit and correctrepairs in spontaneous human-human dialog, based ontranscripts containing an "edit signal," or external andreliable marker at the "expunction point," or point of in-terruption.
Carbonell and Hayes \[4\] briefly describe re-*This research was supported by the Defense Advanced Re-search Projects Agency under Contract ONR N00014-90-C-0085with the Office of Naval Research.
It was also supported by aGrant, NSF IRI-8905249, from the NationM Science Foundation.The views and conclusions contained in this document are those ofthe authors and should not be interpreted as necessarily represent-ing the official policies, either expressed or implied, of the DefenseAdvanced Research Projects Agency of the U.S. Government, orof the National Science Foundation.lEl izabeth Shriberg is also affiliated with the Department ofPsychology at the University of California t Berkeley.covery strategies for broken-off and restarted utterancesin textual input.
Ward \[13\] addresses repairs in sponta-neous speech, but does not attempt o identify or correctthem.
Our approach is most similar to that of Hindle.
Itdiffers, however, in that we make no assumption aboutthe existence of an explicit edit signal.
As a reliable editsignal has yet to be found, we take it as our problem tofind the site of the repair automatically.It is the case, however, that cues to repair exist overa range of syllables.
Research in speech productionhas shown that repairs tend to be marked prosodically\[8\] and there is perceptual evidence from work usinglowpass-filtered speech that human listeners can detectthe occurrence of a repair in the absence of segmentalinformation \[9\].In the sections that follow, we describe in detail our cor-pus of spontaneous speech data and present an analysisof the repair phenomena observed.
In addition, we de-scribe ways in which pattern matching, syntactic andsemantic anMysis, and acoustic analysis can be helpfulin detecting and correcting these repairs.
We use patternmatching to determine an initial set of possible repairs;we then apply information from syntactic, semantic, andacoustic analyses to distinguish actual repairs from falsepositives.2.
THE CORPUSThe data we are analyzing were collected at six sites 1as part of DARPA's Spoken Language Systems project.The corpus contains digitized waveforms and transcrip-tions of a large number of sessions in which subjectsmade air travel plans using a computer.
In the majorityof sessions, data were collected in a Wizard of Oz setting,in which subjects were led to believe they were talking toa computer, but in which a human actually interpretedand responded to queries.
In a small portion of the ses-sions, data were collected using SRI's Spoken LanguageSystem \[12\]), in which no human intervention was in-1The sites were: AT&T, Bolt Beranek and Newman, CarnegieMellon University, Massachusetts Institute of Technology, SRI In-ternational, and Texas Instruments, Inc.419volved.
Relevant o the current paper is the fact thatalthough the speech was spontaneous, it was somewhatplanned (subjects pressed a button to begin speaking tothe system) and the transcribers who produced lexicaltranscriptions of the sessions were instructed to markwords they inferred were verbally deleted by the speakerwith special symbols.
For further description of the cor-pus, see MADCOW \[10\].3.
CHARACTERIST ICS  ANDDISTRIBUT ION OF  REPAIRSOf the ten thousand sentences in our corpus, 607 con-tained repairs.
We found that of sentences longer thannine words, 10% contained repairs.
While this is lowerthan rates reported elsewhere for human-human dialog(Levelt \[7\] reports a rate of 34%), it is still large enoughto be significant.
And, as system developers move to-ward more closely modeling human-human i teraction,the percentage is likely to rise.3 .1  Notat ionIn order to classify these repairs, and to facilitate com-munication among the authors, it was necessary for us todevelop a notational system that would: (1) be relativelysimple, (2) capture sufficient detail, and (3) describe theI want ti-M1-whatM1show me flightsM1I want a flightM1I want to leaveR1what areM~ M2... fly to bostonR1 M,... fly from bostonM~ R1what areX Xflights to boston.M1what are the faresM1dMly flightsX M1one way flightZ X M1depart before ...R1what are the faresM1 M2from bostonR~ M~from denverM~ R~are there any flightsvast majority of repairs observed.
The notation is de-scribed fully in \[2\].The basic aspects of the notation include marking theinterruption point, its extent, and relevant correspon-dences between words in the region.
To mark the site ofa repair, corresponding to Hindle's "edit signal" \[5\], weuse a vertical bar (I).
To express the notion that wordson one side of the repair correspond to words on theother, we use a combination of a letter plus a numericalindex.
The letter M indicates that two words match ex-actly.
R indicates that the second of the two words wasintended by the speaker to replace the first.
The twowords must be similar, either of the same lexical cate-gory, or morphological variants of the same base form(including contraction pairs like I/ I 'd).
Any other wordwithi, a repair is notated with X.
A hyphen affixed toa symbol indicates a word fragment.
In addition, cer-tain cue words, such as "sorry" or "oops" (marked withCR) as well as filled pauses (CF) are also labeled if theyoccur immediately before the site of a repair.3 .2  D is t r ibut ionWhile only 607 sentences contained eletions, some sen-tences contained more than one, for a total of 646 dele-tions.
Table 2 gives the breakdown of deletions bylength, where length is defined as the number of con-secutive deleted words or word fragments.
Most of thedeletions were fairly short.
One or two word deletions ac-counted for 82% of the data.
We categorized the length1 and length 2 repairs according to their transcriptions.The results are summarized in Table 3.
For the pur-pose of simplicity, we have in this table combined casesinvolving fragments (which always occurred as the sec-ond word) with their associated full-word patterns.
Theoverall rate of fragments for the length 2 repairs was34%.4.
S IMPLE  PATTERN MATCHINGWe analyzed a subset of 607 sentences containing repairsand concluded that certain simple pattern-matchingtechniques could successfully detect a number of them.Deletion Length Occurrences Percentage1 376 59%2 154 24%3 52 8%4 25 4%5 23 4%6+ 16 3%Table 1: Examples of Notation Table 2: Distribution of Repairs by Length420Type Pattern FrequencyLength 1 RepairsFragments M1- ,  R i - ,  X -  61%Repeats M1 {M1 16%Insertions M1 \[ X1 ... XiM1 7%Replacement R1 \[R1 9%Other X\[X 5%Length 2 RepairsRepeats M1 M2 {M1 M2 28%Replace 2nd M1 R1 I M1 R1 27%Insertions M1M2IM1X1 .
.
.XiM2 19%Replace 1st R1 Mi JR1 M1 10%Other ...\[... 17%Table 3: Distribution of Repairs by TypeThe pattern matching component reported on here looksfor the following kinds of subsequences:stems largely from the overlap of related patterns.
Manysentences contain a subsequence of words that matchnot one but several patterns.
For example the phrase"FLIGHT <word> FLIGHT" matches three differentpatterns:show the FLIGHT earliest FLIGHTM1 { X M1show the FLIGHT time FLIGHT dateM1 Ri I M1 R1show the delta FLIGHT united FLIGHTR1 M1 I R1 M1Each of these sentences i a false positive for the othertwo patterns.
Despite these problems of overlap, patternmatching is useful in reducing the set of candidate sen-tences to be processed for repairs.
Instead of applyingdetailed and possibly time-intensive analysis techniquesto 10,000 sentences, we can increase fficiency by limit-ing ourselves to the 500 sentences selected by the patternmatcher, which has (at least on one measure) a 75% re-call rate.
The repair sites hypothesized by the patternmatcher constitute useful input for further processingbased on other sources of information.?
Simple syntactic anomalies, such as "a the" or "tofrom".?
Sequences of identical words such as "<I> <would><like> <a> <book> I would like a flight ..."?
Matching single words surrounding a cue wordlike "sorry," for example "from" in this case: "Iwould like to see the flights <from> <philadelphia><i 'm> <sorry> from denver to philadelphia.
"Of the 406 sentences with nontrivial repairs in our data(more editing necessary than deleting fragments andfilled pauses), the program successfully corrected 177.It found 132 additional sentences with repairs but madethe wrong correction.
There were 97 sentences that con-tained repairs which it did not find.
In addition, out ofthe 10,517 sentence corpus (10 ,718-  201 trivial), it in-correctly hypothesized that an additional 191 containedrepairs.
Thus of 10,517 sentences of varying lengths, itpulled out 500 as possibly containing a repair and missed97 sentences actually containing a repair.
Of the 500 thatit proposed as containing a repair, 62% actually did and38% did not.
Of the 62% that had repairs, it made theappropriate correction for 57%.These numbers how that although pattern matching isuseful in identifying possible repairs, it is less success-ful at making appropriate corrections.
This problem5.
NATURAL LANGUAGECONSTRAINTSHere we describe xperiments conducted to measure theeffectiveness of a natural anguage processing system indistinguishing repairs from false positives.
A false pos-itive is a repair pattern that incorrectly matches a sen-tence or part of a sentence.
We conducted the experi-ments using the syntactic and semantic omponents ofthe Gemini natural anguage processing system.
Gem-ini is an extensive reimplementation f the Core Lan-guage Engine \[1\].
It includes modular syntactic and se-mantic components, integrated into an efficient all-pathsbottom-up arser \[11\]).
Gemini was trained on a 2,200sentence subset of the full 10,718-sentence corpus (onlythose annotated as class A or D).
Since this subset ex-cluded the unanswerable (class X) sentences, Gemini'scoverage on the full corpus is only an estimated 70% forsyntax, and 50% for semantics.
2 Nonetheless, the re-sults reported here are promising, and should improveas syntactic and semantic overage increase.We tested Gemini on a subset of the data that the pat-2Gemini's yntactic overage of the 2,200 sentence dataset iwas trained on (the set of annotated and answerable MADCOWqueries) is approximately 91%, while its semantic overage is ap-proximately 77%.
On a fair test of the February 1992 test set,Gemini's yntactic overage was 87% and semantic overage was71%.421Syntax OnlyMarked Markedas  asRepair False PositiveRepairs 68 (96%) 56 (30%)False Positives 3 (4%) 131 (70%)Syntax and SemanticsMarked Markedas  asRepair False PositiveRepairs 64 (85%) 23 (20%)False Positives 11 (15%) 90 (80%)Table 4: Syntax and Semantics Resultstern matcher eturned as likely to contain a repair.
Weexcluded all sentences that contained fragments, result-ing in a dataset of 335 sentences, of which 179 containedrepairs and 176 contained false positives.
The approachwas as follows: for each sentence, parsing was attempted.If parsing succeeded, the sentence was marked as a falsepositive.
If parsing did not succeed, then pattern match-ing was used to detect possible repairs, and the edits as-sociated with the repairs were made.
Parsing was thenreattempted.
If parsing succeeded at this point, the sen-tence was marked as a repair.
Otherwise, it was markedas  NO OP IN ION.Since multiple repairs and false positives can occur inthe same sentence, the pattern matching process is con-strained to prefer fewer repairs to more repairs, andshorter epairs to longer repairs.
This is done to favor ananalysis that deletes the fewest words from a sentence.It is often the case that more drastic repairs would resultin a syntactically and semantically well-formed sentence,but not the sentence that the speaker intended.
For in-stance, the sentence "show me <flights> daily flights toboston" could be repaired by deleting the words "flightsdaily", and would then yield a grammatical sentence, butin this case the speaker intended to delete only "flights.
"Table 4 shows the results of these experiments.
We ranthem two ways: once using syntactic constraints aloneand again using both syntactic and semantic onstraints.As can be seen, Gemini is quite accurate at detectinga repair, although somewhat less accurate at detectinga false positive.
Furthermore, in cases where Geminidetected a repair, it produced the intended correctionin 62 out of 68 cases for syntax alone, and in 60 out of64 cases using combined syntax and semantics.
In bothcases, a large number of sentences (29% for syntax, 50%for semantics) received a NO OPINION evaluation.
TheNO OPINION cases were evenly split between repairs andfalse positives in both tests.The main points to be noted from Table 4 are that withsyntax alone, the system is quite accurate in detectingrepairs, and with syntax and semantics working together,it is accurate at detecting false positives.
However, sincethe coverage of syntax and semantics will always be lowerthan the coverage of syntax alone, we cannot comparethese rates directly.6.
ACOUSTICSA third source of information that can be helpful in de-tecting repairs is acoustics.
While acoustics alone cannottackle th e problem of locating repairs, since any prosodicpatterns found in repairs will be found in fluent speech,acoustic information can be quite effective when com-bined with other sources of information, particularly,pattern matching.Our approach in studying the ways in which acousticsmight be helpful was to begin by looking at two pat-terns conducive to acoustic measurement and compar-ison.
First, we focused on patterns in which there isonly one matched word, and in which the two occur-rences of that word are either adjacent or separated byonly one word.
Matched words allow for comparisonsof word duration; proximity helps avoid variability dueto global intonation contours not associated with thepatterns themselves.
We present here analyses for theMi\[M1 ("flights for <one> one person") and MI\[XM1("<flight> earliest flight") repairs, and their associatedfalse positives ("u s air five one one," "a flight on flightnumber five one one," respectively).Second, we have done a preliminary analysis of repairsin which a word such as "no" or "well" was presentas an editing expression \[6\] at the point of interrup-tion ("...flights <between> <boston> <and> <dallas><no> between oakland and boston").
False positives forthese cases are instances in which the cue word functionsin its usual lexical sense ("I want to leave boston no laterthan one p m.").
Hirshberg and Litman \[3\] have shownthat cue words that function differently can be distin-guished perceptually by listeners on the basis of prosody.Thus, we sought to determine whether acoustic analysiscould help in deciding, when such words were present,whether or not they marked the interruption point of arepair.In both analyses, a number of features were measuredto allow for comparisons between the words of interest.422False Positives(N--24)Repairs(N=12)Pauses before/after Xbefore afterX X(only) (only).08 .58.83 .00F0of Xgreater thanF0 of1st ~I1.08.92less thanF0 of1st M1.42.08Table 5: Acoustic Characteristics of M1 IXM1 RepairsPauses afterX (only)andFO of X less thanFO of 1st M1Pauses beforeX (only)andF0 of X greater thanF0 of 1st M1False Positives .58 .00Repairs .00 .92Table 6: Combining Acoustic Characteristics ofM11XM1 RepairsWord onsets and offsets were labeled by inspection ofwaveforms and parameter files (pitch tracks and spec-trograms) obtained using the Entropic Waves softwarepackage.
Files with questionable pitch tracks were ex-cluded from the analysis.
An average F0 value for wordsof interest was determined by simply averaging, within alabeled word, all 10-ms frame values having a probabilityof voicing above 0.20.In examining the MilM1 repair pattern, we found thatthe strongest distinguishing cue between the repairs(g  = 20) and the false positives (g  = 20) was the in-terval between the offset of the first word and the onsetof the second.
False positives had a mean gap of 42ms (s.d.
= 55.8) as opposed to 380 ms (s.d.
= 200.4)for repairs.
A second difference found between the twogroups was that, in the case of repairs, there was a sta-tistically reliable reduction in duration for the secondoccurrence of M1, with a mean difference of 53.4 ms.However because false positives howed no reliable dif-ference for word duration, this was a much less usefulpredictor than gap duration.
F0 of the matched wordswas not helpful in separating repairs from false positives;both groups showed a highly significant correlation for,and no significant difference between, the mean F0 of thematched words.and rarely before the X in the false positives.
Note thatvalues do not add up to 100% because cases of no pauses,or pauses on both sides are not included in the table.
Asecond distinguishing characteristic was the F0 value ofX.
For repairs, the inserted word was nearly alwayshigher in F0 than the preceding M1; for false positives,this increase in F0 was rarely observed.
Table 6 showsthe results of combining the acoustic constraints in Ta-ble 5.
As can be seen, although acoustic features maybe helpful individually, certain combinations of featureswiden the gap between observed rates of repairs and falsepositives possessing the relevant set of features.Finally, in a preliminary study of the cue words "no"and "well," we compared 9 examples of these words atthe site of a repair to 15 examples of the same wordsoccurring in fluent speech.
We found that these groupswere quite distinguishable on the basis of simple prosodicfeatures.
Table 7 shows the percentage of repairs versusfalse positives characterized by a clear rise or fall in F0,lexical stress, and continuity of the speech immediatelypreceding and following the editing expression ("contin-uous" means there is no silent pause on either side ofthe cue word).
As 'can be seen, at least for this limiteddata set, cue words marking repairs were quite distin-guishable from those same words found in fluent stringson the basis of simple prosodic features.A different set of features was found to be useful in dis-tinguishing repairs from false positives for the M11XM1pattern.
These features are shown in Table 5.
Cell val-ues are percentages of repairs or false positives that pos-sessed the characteristics indicated in the columns.
De-spite the small data set, some suggestive trends emerge.For example, for cases in which there was a pause (de-fined for purposes of this analysis as a silence of greaterthan 200 ms) on only one side of the inserted word, thepause was never after the insertion (X) for the repairsF0 F0 Lexical Continuousrise fall stress speechRepairs .00 1.00 .00 .00False positives .87 .00 .87 .73Table 7: Acoustic Characteristics of Cue WordsAlthough one cannot draw conclusions from such limited423data sets, such results are nevertheless interesting.
Theyillustrate that acoustics can indeed play a role in dis-tinguishing repairs from false positives, but only if eachpattern is examined individually, to determine which fea-tures to use, and how to combine them.
Analysis of addi-tional patterns and access to a larger database of repairswill help us better determine the ways in which acousticscan play a role in detection of repairs.7.
CONCLUSIONIn summary, disfluencies occur at high enough rates inhuman-computer dialog to merit consideration.
In con-trast to earlier approaches, we have made it our goal todetect and correct repairs automatically, without assum-ing an explicit edit signal.
Without such an edit signal,however, repairs are easily confused both with false pos-itives and with other repairs.
Preliminary results showthat pattern matching is effective at detecting repairswithout excessive overgeneration.
Our syntax-only ap-proach is quite accurate at detecting repairs and correct-ing them.
Acoustics is a third source of information thatcan be tapped to provide corroborating evidence abouta hypothesis, given the output of a pattern matcher.While none of these knowledge sources by itself is suffi-cient, we propose that by combining them, and possiblyothers, we can greatly enhance our ability to detect andcorrect repairs.
As a next step, we intend to explore ad-ditional aspects of the syntax and semantics of repairs,analyze further acoustic patterns, and examine corporawith higher rates of disfluencies.ACKNOWLEDGMENTSWe would like to thank Patti  Price for her helpful com-ments on earlier drafts, as well as for her participationin the development of the notational system used.
Wewould also like to thank Robin Lickley for his helpfulfeedback on the acoustics ection.REFERENCES1.
Alshawi, H, Carter, D., van Eijck, J., Moore, R. C.,Moran, D. B., Pereira, F., Pulman, S., and A. Smith(1988) Research Programme In Natural Language Pro-cessing: July 1988 Annual Report, SRI InternationalTech Note, Cambridge, England.2.
Bear, J., Dowding, J., Price, P., and E. E. Shriberg(1992) "Labeling Conventions for Notating Grammat-ical Repairs in Speech," unpublished manuscript, o ap-pear as an SRI Tech Note.3.
Hirschberg, J. and D. Litman (1987) "Now Let's TalkAbout Now: Identifying Cue Phrases Intonationally,"Proceedings of the ACL, pp.
163-171.4.
Carbonell, J. and P. Hayes, P., (1983) "Recovery Strate-gies for Parsing Extragrammatical Language," Ameri-can Journal of Computational Linguistics, Vol.
9, Num-bers 3-4, pp.
123-146.5.
Hindle, D. (1983) "Deterministic Parsing of SyntacticNon-fluencies," Proceedings of the A CL, pp.
123-128.6.
Hockett, C. (1967) "Where the Tongue Slips, There SlipI," in To Honor Roman Jakobson: Vol.
2, The Hague:Mouton.7.
Levelt, W. (1983) "Monitoring and self-repair inspeech," Cognition, Vol.
14, pp.
41-104.8.
Levelt, W., and A. Cutler (1983) "Prosodic Marking inSpeech Repair," Journal of Semantics, Vol.
2, pp.
205-217.9.
Lickley, R., R. Shillcock, and E. Bard (1991) "Process-ing Disfluent Speech: How and when are disfluenciesfound?"
Proceedings of the Second European Confer-ence on Speech Communication and Technology, Vol.
3,pp.
1499-1502.10.
MADCOW (1992) "Multi-site Data Collection for aSpoken Language Corpus," Proceedings of the DARPASpeech and Natural Language Workshop, February 23-26, 1992.11.
Moore, R. and J. Dowding (1991) "Efficient Bottom-upParsing," Proceedings of the DARPA Speech and NaturalLanguage Workshop, February 19-22, 1991, pp.
200-203.12.
Shriberg, E., Wade, E., and P. Price (1992) "Human-Machine Problem Solving Using Spoken Language Sys-tems (SLS): Factors Affecting Performance and UserSatisfaction," Proceedings of the DARPA Speech andNatural Language Workshop, February 23-26, 1992.13.
Ward, W. (1991) "Evaluation of the CMU ATIS Sys-tem," Proceedings of the DARPA Speech and NaturalLanguage Workshop, February 19-22, 1991, pp.
101-105.424
