Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 165?174,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsMachine Translation without Words through Substring AlignmentGraham Neubig1,2, Taro Watanabe2, Shinsuke Mori1, Tatsuya Kawahara11Graduate School of Informatics, Kyoto UniversityYoshida Honmachi, Sakyo-ku, Kyoto, Japan2National Institute of Information and Communication Technology3-5 Hikari-dai, Seika-cho, Soraku-gun, Kyoto, JapanAbstractIn this paper, we demonstrate that accu-rate machine translation is possible withoutthe concept of ?words,?
treating MT as aproblem of transformation between characterstrings.
We achieve this result by applyingphrasal inversion transduction grammar align-ment techniques to character strings to traina character-based translation model, and us-ing this in the phrase-based MT framework.We also propose a look-ahead parsing algo-rithm and substring-informed prior probabil-ities to achieve more effective and efficientalignment.
In an evaluation, we demonstratethat character-based translation can achieveresults that compare to word-based systemswhile effectively translating unknown and un-common words over several language pairs.1 IntroductionTraditionally, the task of statistical machine trans-lation (SMT) is defined as translating a source sen-tence fJ1 = {f1, .
.
.
, fJ} to a target sentence eI1 ={e1, .
.
., eI}, where each element of fJ1 and eI1 isassumed to be a word in the source and target lan-guages.
However, the definition of a ?word?
is of-ten problematic.
The most obvious example of thislies in languages that do not separate words withwhite space such as Chinese, Japanese, or Thai, inwhich the choice of a segmentation standard hasa large effect on translation accuracy (Chang etal., 2008).
Even for languages with explicit wordThe first author is now affiliated with the Nara Institute of Sci-ence and Technology.boundaries, all machine translation systems performat least some precursory form of tokenization, split-ting punctuation and words to prevent the sparsitythat would occur if punctuated and non-punctuatedwords were treated as different entities.
Sparsityalso manifests itself in other forms, including thelarge vocabularies produced by morphological pro-ductivity, word compounding, numbers, and propernames.
A myriad of methods have been proposedto handle each of these phenomena individually,including morphological analysis, stemming, com-pound breaking, number regularization, optimizingword segmentation, and transliteration, which weoutline in more detail in Section 2.These difficulties occur because we are translat-ing sequences of words as our basic unit.
On theother hand, Vilar et al (2007) examine the possibil-ity of instead treating each sentence as sequences ofcharacters to be translated.
This method is attrac-tive, as it is theoretically able to handle all sparsityphenomena in a single unified framework, but hasonly been shown feasible between similar languagepairs such as Spanish-Catalan (Vilar et al, 2007),Swedish-Norwegian (Tiedemann, 2009), and Thai-Lao (Sornlertlamvanich et al, 2008), which havea strong co-occurrence between single characters.As Vilar et al (2007) state and we confirm, accu-rate translations cannot be achieved when applyingtraditional translation techniques to character-basedtranslation for less similar language pairs.In this paper, we propose improvements to thealignment process tailored to character-based ma-chine translation, and demonstrate that it is, in fact,possible to achieve translation accuracies that ap-165proach those of traditional word-based systems us-ing only character strings.
We draw upon recentadvances in many-to-many alignment, which allowsfor the automatic choice of the length of units tobe aligned.
As these units may be at the charac-ter, subword, word, or multi-word phrase level, weconjecture that this will allow for better characteralignments than one-to-many alignment techniques,and will allow for better translation of uncommonwords than traditional word-based models by break-ing down words into their component parts.We also propose two improvements to the many-to-many alignment method of Neubig et al (2011).One barrier to applying many-to-many alignmentmodels to character strings is training cost.
In theinversion transduction grammar (ITG) framework(Wu, 1997), which is widely used in many-to-manyalignment, search is cumbersome for longer sen-tences, a problem that is further exacerbated whenusing characters instead of words as the basic unit.As a step towards overcoming this difficulty, we in-crease the efficiency of the beam-search technique ofSaers et al (2009) by augmenting it with look-aheadprobabilities in the spirit of A* search.
Secondly,we describe a method to seed the search process us-ing counts of all substring pairs in the corpus to biasthe phrase alignment model.
We do this by definingprior probabilities based on these substring countswithin the Bayesian phrasal ITG framework.An evaluation on four language pairs with differ-ing morphological properties shows that for distantlanguage pairs, character-based SMT can achievetranslation accuracy comparable to word-based sys-tems.
In addition, we perform ablation studies,showing that these results were not possible with-out the proposed enhancements to the model.
Fi-nally, we perform a qualitative analysis, which findsthat character-based translation can handle unseg-mented text, conjugation, and proper names in a uni-fied framework with no additional processing.2 Related Work on Data Sparsity in SMTAs traditional SMT systems treat all words as singletokens without considering their internal structure,major problems of data sparsity occur for less fre-quent tokens.
In fact, it has been shown that thereis a direct negative correlation between vocabularysize (and thus sparsity) of a language and transla-tion accuracy (Koehn, 2005).
Sparsity causes trou-ble for alignment models, both in the form of incor-rectly aligned uncommon words, and in the form ofgarbage collection, where uncommon words in onelanguage are incorrectly aligned to large segmentsof the sentence in the other language (Och and Ney,2003).
Unknown words are also a problem duringthe translation process, and the default approach isto map them as-is into the target sentence.This is a major problem in agglutinative lan-guages such as Finnish or compounding languagessuch as German.
Previous works have attempted tohandle morphology, decompounding and regulariza-tion through lemmatization, morphological analysis,or unsupervised techniques (Nie?en and Ney, 2000;Brown, 2002; Lee, 2004; Goldwater and McClosky,2005; Talbot and Osborne, 2006; Mermer and Ak?n,2010; Macherey et al, 2011).
It has also been notedthat it is more difficult to translate into morpho-logically rich languages, and methods for modelingtarget-side morphology have attracted interest in re-cent years (Bojar, 2007; Subotin, 2011).Another source of data sparsity that occurs in alllanguages is proper names, which have been handledby using cognates or transliteration to improve trans-lation (Knight and Graehl, 1998; Kondrak et al,2003; Finch and Sumita, 2007), and more sophisti-cated methods for named entity translation that com-bine translation and transliteration have also beenproposed (Al-Onaizan and Knight, 2002).Choosing word units is also essential for creat-ing good translation results for languages that donot explicitly mark word boundaries, such as Chi-nese, Japanese, and Thai.
A number of works havedealt with this word segmentation problem in trans-lation, mainly focusing on Chinese-to-English trans-lation (Bai et al, 2008; Chang et al, 2008; Zhang etal., 2008b; Chung and Gildea, 2009; Nguyen et al,2010), although these works generally assume that aword segmentation exists in one language (English)and attempt to optimize the word segmentation inthe other language (Chinese).We have enumerated these related works todemonstrate the myriad of data sparsity problemsand proposed solutions.
Character-based transla-tion has the potential to handle all of the phenom-ena in the previously mentioned research in a single166unified framework, requiring no language specifictools such as morphological analyzers or word seg-menters.
However, while the approach is attractiveconceptually, previous research has only been showneffective for closely related language pairs (Vilar etal., 2007; Tiedemann, 2009; Sornlertlamvanich etal., 2008).
In this work, we propose effective align-ment techniques that allow character-based transla-tion to achieve accurate translation results for bothclose and distant language pairs.3 Alignment MethodsSMT systems are generally constructed from a par-allel corpus consisting of target language sentencesE and source language sentences F .
The first stepof training is to find alignments A for the words ineach sentence pair.We represent our target and source sentences aseI1 and fJ1 .
ei and fj represent single elements ofthe target and source sentences respectively.
Thesemay be words in word-based alignment models orsingle characters in character-based alignment mod-els.1 We define our alignment as aK1 , where eachelement is a span ak = ?s, t, u, v?
indicating that thetarget string es, .
.
.
, et and source string fu, .
.
.
, fvare aligned to each-other.3.1 One-to-Many AlignmentThe most well-known and widely-used models forbitext alignment are for one-to-many alignment, in-cluding the IBM models (Brown et al, 1993) andHMM alignment model (Vogel et al, 1996).
Thesemodels are by nature directional, attempting to findthe alignments that maximize the conditional prob-ability of the target sentence P (eI1|fJ1 ,aK1 ).
Forcomputational reasons, the IBM models are re-stricted to aligning each word on the target side toa single word on the source side.
In the formal-ism presented above, this means that each ei mustbe included in at most one span, and for each spanu = v. Traditionally, these models are run in bothdirections and combined using heuristics to createmany-to-many alignments (Koehn et al, 2003).However, in order for one-to-many alignmentmethods to be effective, each fj must contain1Some previous work has also performed alignment usingmorphological analyzers to normalize or split the sentence intomorpheme streams (Corston-Oliver and Gamon, 2004).enough information to allow for effective alignmentwith its corresponding elements in eI1.
While this isoften the case in word-based models, for character-based models this assumption breaks down, as thereis often no clear correspondence between characters.3.2 Many-to-Many AlignmentOn the other hand, in recent years, there have beenadvances in many-to-many alignment techniquesthat are able to align multi-element chunks on bothsides of the translation (Marcu and Wong, 2002;DeNero et al, 2008; Blunsom et al, 2009; Neu-big et al, 2011).
Many-to-many methods can be ex-pected to achieve superior results on character-basedalignment, as the aligner can use information aboutsubstrings, which may correspond to letters, mor-phemes, words, or short phrases.Here, we focus on the model presented by Neu-big et al (2011), which uses Bayesian inference inthe phrasal inversion transduction grammar (ITG,Wu (1997)) framework.
ITGs are a variety of syn-chronous context free grammar (SCFG) that allowsfor many-to-many alignment to be achieved in poly-nomial time through the process of biparsing, whichwe explain more in the following section.
PhrasalITGs are ITGs that allow for non-terminals that canemit phrase pairs with multiple elements on boththe source and target sides.
It should be notedthat there are other many-to-many alignment meth-ods that have been used for simultaneously discov-ering morphological boundaries over multiple lan-guages (Snyder and Barzilay, 2008; Naradowskyand Toutanova, 2011), but these have generally beenapplied to single words or short phrases, and it is notimmediately clear that they will scale to aligning fullsentences.4 Look-Ahead BiparsingIn this work, we experiment with the alignmentmethod of Neubig et al (2011), which can achievecompetitive accuracy with a much smaller phrase ta-ble than traditional methods.
This is important inthe character-based translation context, as we wouldlike to use phrases that contain large numbers ofcharacters without creating a phrase table so largethat it cannot be used in actual decoding.
In thisframework, training is performed using sentence-167Figure 1: (a) A chart with inside probabilities in boxesand forward/backward probabilities marking the sur-rounding arrows.
(b) Spans with corresponding look-aheads added, and the minimum probability underlined.Lightly and darkly shaded spans will be trimmed whenthe beam is log(P ) ?
?3 and log(P ) ?
?6 respectively.wise block sampling, acquiring a sample for eachsentence by first performing bottom-up biparsing tocreate a chart of probabilities, then performing top-down sampling of a new tree based on the probabil-ities in this chart.An example of a chart used in this parsing canbe found in Figure 1 (a).
Within each cell of thechart spanning ets and fvu is an ?inside?
probabil-ity I(as,t,u,v).
This probability is the combinationof the generative probability of each phrase pairPt(ets,fvu) as well as the sum the probabilities overall shorter spans in straight and inverted order2I(as,t,u,v) = Pt(ets, fvu)+?s?S?t?u?U?vPx(str)I(as,S,u,U )I(aS,t,U,v)+?s?S?t?u?U?vPx(inv)I(as,S,U,v)I(aS,t,u,U )where Px(str) and Px(inv) are the probability ofstraight and inverted ITG productions.While the exact calculation of these probabilitiescan be performed in O(n6) time, where n is the2Pt can be specified according to Bayesian statistics as de-scribed by Neubig et al (2011).length of the sentence, this is impractical for all butthe shortest sentences.
Thus it is necessary to usemethods to reduce the search space such as beam-search based chart parsing (Saers et al, 2009) orslice sampling (Blunsom and Cohn, 2010).3In this section we propose the use of a look-aheadprobability to increase the efficiency of this chartparsing.
Taking the example of Saers et al (2009),spans are pushed onto a different queue based ontheir size, and queues are processed in ascending or-der of size.
Agendas can further be trimmed basedon a histogram beam (Saers et al, 2009) or probabil-ity beam (Neubig et al, 2011) compared to the besthypothesis a?.
In other words, we have a queue dis-cipline based on the inside probability, and all spansak where I(ak) < cI(a?)
are pruned.
c is a constantdescribing the width of the beam, and a smaller con-stant probability will indicate a wider beam.This method is insensitive to the existence ofcompeting hypotheses when performing pruning.Figure 1 (a) provides an example of why it is unwiseto ignore competing hypotheses during beam prun-ing.
Particularly, the alignment ?les/1960s?
com-petes with the high-probability alignment ?les/the,?so intuitively should be a good candidate for prun-ing.
However its probability is only slightly higherthan ?anne?es/1960s,?
which has no competing hy-potheses and thus should not be trimmed.In order to take into account competing hypothe-ses, we can use for our queue discipline not only theinside probability I(ak), but also the outside proba-bility O(ak), the probability of generating all spansother than ak, as in A* search for CFGs (Klein andManning, 2003), and tic-tac-toe pruning for word-based ITGs (Zhang and Gildea, 2005).
As the cal-culation of the actual outside probability O(ak) isjust as expensive as parsing itself, it is necessary toapproximate this with heuristic function O?
that canbe calculated efficiently.Here we propose a heuristic function that is de-signed specifically for phrasal ITGs and is com-putable with worst-case complexity of n2, comparedwith the n3 amortized time of the tic-tac-toe pruning3Applying beam-search before sampling will sample froman improper distribution, although Metropolis-in-Gibbs sam-pling (Johnson et al, 2007) can be used to compensate.
How-ever, we found that this had no significant effect on results, sowe omit the Metropolis-in-Gibbs step for experiments.168algorithm described by (Zhang et al, 2008a).
Dur-ing the calculation of the phrase generation proba-bilities Pt, we save the best inside probability I?
foreach monolingual span.I?e (s, t) = max{a?=?s?,t?,u?,v??;s?=s,t?=t}Pt(a?
)I?f (u, v) = max{a?=?s?,t?,u?,v??;u?=u,v?=v}Pt(a?
)For each language independently, we calculate for-ward probabilities ?
and backward probabilities ?.For example, ?e(s) is the maximum probability ofthe span (0, s) of e that can be created by concate-nating together consecutive values of I?e :?e(s) = max{S1,...,Sx}I?e (0, S1)I?e (S1, S2) .
.
.
I?e (Sx, s).Backwards probabilities and probabilities over f canbe defined similarly.
These probabilities are calcu-lated for e and f independently, and can be calcu-lated in n2 time by processing each ?
in ascendingorder, and each ?
in descending order in a fashionsimilar to that of the forward-backward algorithm.Finally, for any span, we define the outside heuristicas the minimum of the two independent look-aheadprobabilities over each languageO?
(as,t,u,v) = min(?e(s) ?
?e(t), ?f (u) ?
?f (v)).Looking again at Figure 1 (b), it can be seenthat the relative probability difference between thehighest probability span ?les/the?
and the spans?anne?es/1960s?
and ?60/1960s?
decreases, allowingfor tighter beam pruning without losing these goodhypotheses.
In contrast, the relative probability of?les/1960s?
remains low as it is in conflict with ahigh-probability alignment, allowing it to be dis-carded.5 Substring Prior ProbabilitiesWhile the Bayesian phrasal ITG framework usesthe previously mentioned phrase distribution Pt dur-ing search, it also allows for definition of a phrasepair prior probability Pprior(ets,fvu), which can ef-ficiently seed the search process with a bias towardsphrase pairs that satisfy certain properties.
In thissection, we overview an existing method used to cal-culate these prior probabilities, and also propose anew way to calculate priors based on substring co-occurrence statistics.5.1 Word-based PriorsPrevious research on many-to-many translation hasused IBM model 1 probabilities to bias phrasalalignments so that phrases whose member words aregood translations are also aligned.
As a representa-tive of this existing method, we adopt a base mea-sure similar to that used by DeNero et al (2008):Pm1(e,f) =M0(e,f)Ppois(|e|;?
)Ppois(|f |;?
)M0(e,f) =(Pm1(f |e)Puni(e)Pm1(e|f)Puni(f))12 .Ppois is the Poisson distribution with the averagelength parameter ?, which we set to 0.01.
Pm1 is theword-based (or character-based) Model 1 probabil-ity, which can be efficiently calculated using the dy-namic programming algorithm described by Brownet al (1993).
However, for reasons previously statedin Section 3, these methods are less satisfactorywhen performing character-based alignment, as theamount of information contained in a character doesnot allow for proper alignment.5.2 Substring Co-occurrence PriorsInstead, we propose a method for using raw sub-string co-occurrence statistics to bias alignments to-wards substrings that often co-occur in the entiretraining corpus.
This is similar to the method ofCromieres (2006), but instead of using these co-occurrence statistics as a heuristic alignment crite-rion, we incorporate them as a prior probability ina statistical model that can take into account mutualexclusivity of overlapping substrings in a sentence.We define this prior probability using three countsover substrings c(e), c(f), and c(e,f).
c(e) andc(f) count the total number of sentences in whichthe substrings e and f occur respectively.
c(e,f) isa count of the total number of sentences in which thesubstring e occurs on the target side, and f occurson the source side.
We perform the calculation ofthese statistics using enhanced suffix arrays, a datastructure that can efficiently calculate all substringsin a corpus (Abouelhoda et al, 2004).4While suffix arrays allow for efficient calculationof these statistics, storing all co-occurrence countsc(e,f) is an unrealistic memory burden for larger4Using the open-source implementation esaxx http://code.google.com/p/esaxx/169corpora.
In order to reduce the amount of mem-ory used, we discount every count by a constant d,which we set to 5.
This has a dual effect of reducingthe amount of memory needed to hold co-occurrencecounts by removing values for which c(e,f) < d, aswell as preventing over-fitting of the training data.
Inaddition, we heuristically prune values for which theconditional probabilities P (e|f) or P (f |e) are lessthan some fixed value, which we set to 0.1 for thereported experiments.To determine how to combine c(e), c(f), andc(e,f) into prior probabilities, we performed pre-liminary experiments testing methods proposed byprevious research including plain co-occurrencecounts, the Dice coefficient, and ?-squared statistics(Cromieres, 2006), as well as a newmethod of defin-ing substring pair probabilities to be proportional tobidirectional conditional probabilitiesPcooc(e,f) = Pcooc(e|f)Pcooc(f |e)/Z=(c(e,f) ?
dc(f) ?
d)(c(e,f) ?
dc(e) ?
d)/Zfor all substring pairs where c(e,f) > d and whereZ is a normalization term equal toZ =?
{e,f ;c(e,f)>d}Pcooc(e|f)Pcooc(f |e).The experiments showed that the bidirectional con-ditional probability method gave significantly betterresults than all other methods, so we adopt this forthe remainder of our experiments.It should be noted that as we are using discount-ing, many substring pairs will be given zero proba-bility according to Pcooc.
As the prior is only sup-posed to bias the model towards good solutions andnot explicitly rule out any possibilities, we linearlyinterpolate the co-occurrence probability with theone-to-many Model 1 probability, which will giveat least some probability mass to all substring pairsPprior(e,f) = ?Pcooc(e,f) + (1 ?
?
)Pm1(e,f).We put a Dirichlet prior (?
= 1) on the interpolationcoefficient ?
and learn it during training.6 ExperimentsIn order to test the effectiveness of character-basedtranslation, we performed experiments over a varietyof language pairs and experimental settings.de-en fi-en fr-en ja-enTM (en) 2.80M 3.10M 2.77M 2.13MTM (other) 2.56M 2.23M 3.05M 2.34MLM (en) 16.0M 15.5M 13.8M 11.5MLM (other) 15.3M 11.3M 15.6M 11.9MTune (en) 58.7k 58.7k 58.7k 30.8kTune (other) 55.1k 42.0k 67.3k 34.4kTest (en) 58.0k 58.0k 58.0k 26.6kTest (other) 54.3k 41.4k 66.2k 28.5kTable 1: The number of words in each corpus for TM andLM training, tuning, and testing.6.1 Experimental SetupWe use a combination of four languages with En-glish, using freely available data.
We selectedFrench-English, German-English, Finnish-Englishdata from EuroParl (Koehn, 2005), with develop-ment and test sets designated for the 2005 ACLshared task on machine translation.5 We also didexperiments with Japanese-English Wikipedia arti-cles from the Kyoto Free Translation Task (Neu-big, 2011) using the designated training and tuningsets, and reporting results on the test set.
These lan-guages were chosen as they have a variety of inter-esting characteristics.
French has some inflection,but among the test languages has the strongest one-to-one correspondence with English, and is gener-ally considered easy to translate.
German has manycompound words, which must be broken apart totranslate properly into English.
Finnish is an ag-glutinative language with extremely rich morphol-ogy, resulting in long words and the largest vocab-ulary of the languages in EuroParl.
Japanese doesnot have any clear word boundaries, and uses logo-graphic characters, which contain more informationthan phonetic characters.With regards to data preparation, the EuroParldata was pre-tokenized, so we simply used the to-kenized data as-is for the training and evaluation ofall models.
For word-based translation in the Kyototask, training was performed using the provided tok-enization scripts.
For character-based translation, notokenization was performed, using the original textfor both training and decoding.
For both tasks, weselected as training data all sentences for which both5http://statmt.org/wpt05/mt-shared-task170de-en fi-en fr-en ja-enGIZA-word 24.58 / 64.28 / 30.43 20.41 / 60.01 / 27.89 30.23 / 68.79 / 34.20 17.95 / 56.47 / 24.70ITG-word 23.87 / 64.89 / 30.71 20.83 / 61.04 / 28.46 29.92 / 68.64 / 34.29 17.14 / 56.60 / 24.89GIZA-char 08.05 / 45.01 / 15.35 06.91 / 41.62 / 14.39 11.05 / 48.23 / 17.80 09.46 / 49.02 / 18.34ITG-char 21.79 / 64.47 / 30.12 18.38 / 62.44 / 28.94 26.70 / 66.76 / 32.47 15.84 / 58.41 / 24.58en-de en-fi en-fr en-jaGIZA-word 17.94 / 62.71 / 37.88 13.22 / 58.50 / 27.03 32.19 / 69.20 / 52.39 20.79 / 27.01 / 38.41ITG-word 17.47 / 63.18 / 37.79 13.12 / 59.27 / 27.09 31.66 / 69.61 / 51.98 20.26 / 28.34 / 38.34GIZA-char 06.17 / 41.04 / 19.90 04.58 / 35.09 / 11.76 10.31 / 42.84 / 25.06 01.48 / 00.72 / 06.67ITG-char 15.35 / 61.95 / 35.45 12.14 / 59.02 / 25.31 27.74 / 67.44 / 48.56 17.90 / 28.46 / 35.71Table 2: Translation results in word-based BLEU, character-based BLEU, and METEOR for the GIZA++ and phrasalITG models for word and character-based translation, with bold numbers indicating a statistically insignificant differ-ence from the best system according to the bootstrap resampling method at p = 0.05 (Koehn, 2004).source and target were 100 characters or less,6 thetotal size of which is shown in Table 1.
In character-based translation, white spaces between words weretreated as any other character and not given any spe-cial treatment.
Evaluation was performed on tok-enized and lower-cased data.For alignment, we use the GIZA++ implementa-tion of one-to-many alignment7 and the pialign im-plementation of the phrasal ITG models8 modifiedwith the proposed improvements.
For GIZA++, weused the default settings for word-based alignment,but used the HMM model for character-based align-ment to allow for alignment of longer sentences.For pialign, default settings were used except forcharacter-based ITG alignment, which used a prob-ability beam of 10?4 instead 10?10.9 For decoding,we use the Moses decoder,10 using the default set-tings except for the stack size, which we set to 1000instead of 200.
Minimum error rate training was per-formed to maximize word-based BLEU score for allsystems.11 For language models, word-based trans-lation uses a word 5-gram model, and character-based translation uses a character 12-gram model,both smoothed using interpolated Kneser-Ney.6100 characters is an average of 18.8 English words7http://code.google.com/p/giza-pp/8http://phontron.com/pialign/9Improvement by using a beam larger than 10?4 wasmarginal, especially with co-occurrence prior probabilities.10http://statmt.org/moses/11We chose this set-up to minimize the effect of tuning crite-rion on our experiments, although it does indicate that we musthave access to tokenized data for the development set.6.2 Quantitative EvaluationTable 2 presents a quantitative analysis of the trans-lation results for each of the proposed methods.
Asprevious research has shown that it is more diffi-cult to translate into morphologically rich languagesthan into English (Koehn, 2005), we perform exper-iments translating in both directions for all languagepairs.
We evaluate translation quality using BLEUscore (Papineni et al, 2002), both on the word andcharacter level (with n = 4), as well as METEOR(Denkowski and Lavie, 2011) on the word level.It can be seen that character-based translationwith all of the proposed alignment improvementsgreatly exceeds character-based translation usingone-to-many alignment, confirming that substring-based information is necessary for accurate align-ments.
When compared with word-based trans-lation, character-based translation achieves better,comparable, or inferior results on character-basedBLEU, comparable or inferior results on METEOR,and inferior results on word-based BLEU.
The dif-ferences between the evaluation metrics are due tothe fact that character-based translation often getswords mostly correct other than one or two letters.These are given partial credit by character-basedBLEU (and to a lesser extent METEOR), but markedentirely wrong by word-based BLEU.Interestingly, for translation into English,character-based translation achieves higher ac-curacy compared to word-based translation onJapanese and Finnish input, followed by German,171fi-en ja-enITG-word 2.851 2.085ITG-char 2.826 2.154Table 3: Human evaluation scores (0-5 scale).Ref: directive on equalitySource Unk.
Word: tasa-arvodirektiivi(13/26) Char: equality directiveRef: yoshiwara-juku stationTarget Unk.
Word: yoshiwara no eki(5/26) Char: yoshiwara-juku stationRef: world health organisationUncommon Word: world health(5/26) Char: world health organisationTable 4: The major gains of character-based translation,unknown, hyphenated, and uncommon words.and finally French.
This confirms that character-based translation is performing well on languagesthat have long words or ambiguous boundaries, andless well on language pairs with relatively strongone-to-one correspondence between words.6.3 Qualitative EvaluationIn addition, we performed a subjective evaluation ofJapanese-English and Finnish-English translations.Two raters evaluated 100 sentences each, assigninga score of 0-5 based on how well the translation con-veys the information contained in the reference.
Wefocus on shorter sentences of 8-16 English words toease rating and interpretation.
Table 3 shows thatthe results are comparable, with no significant dif-ference in average scores for either language pair.Table 4 shows a breakdown of the sentences forwhich character-based translation received a scoreof at 2+ points more than word-based.
It can be seenthat character-based translation is properly handlingsparsity phenomena.
On the other hand, word-basedtranslation was generally stronger with reorderingand lexical choice of more common words.6.4 Effect of Alignment MethodIn this section, we compare the translation accura-cies for character-based translation using the phrasalITG model with and without the proposed improve-ments of substring co-occurrence priors and look-ahead parsing as described in Sections 4 and 5.2.fi-en en-fi ja-en en-jaITG +cooc +look 28.94 25.31 24.58 35.71ITG +cooc -look 28.51 24.24 24.32 35.74ITG -cooc +look 28.65 24.49 24.36 35.05ITG -cooc -look 27.45 23.30 23.57 34.50Table 5: METEOR scores for alignment with and withoutlook-ahead and co-occurrence priors.Figure 5 shows METEOR scores12 for experi-ments translating Japanese and Finnish.
It can beseen that the co-occurrence prior gives gains in allcases, indicating that substring statistics are effec-tively seeding the ITG aligner.
The introduced look-ahead probabilities improve accuracy significantlywhen substring co-occurrence counts are not used,and slightly when co-occurrence counts are used.More importantly, they allow for more aggressivebeam pruning, increasing sampling speed from 1.3sent/s to 2.5 sent/s for Finnish, and 6.8 sent/s to 11.6sent/s for Japanese.7 Conclusion and Future DirectionsThis paper demonstrated that character-based trans-lation can act as a unified framework for handlingdifficult problems in translation: morphology, com-pound words, transliteration, and segmentation.One future challenge includes scaling training upto longer sentences, which can likely be achievedthrough methods such as the heuristic span prun-ing of Haghighi et al (2009) or sentence splittingof Vilar et al (2007).
Monolingual data could alsobe used to improve estimates of our substring-basedprior.
In addition, error analysis showed that word-based translation performed better than character-based translation on reordering and lexical choice,indicating that improved decoding (or pre-ordering)and language modeling tailored to character-basedtranslation will likely greatly improve accuracy.
Fi-nally, we plan to explore the middle ground betweenword-based and character based translation, allow-ing for the flexibility of character-based translation,while using word boundary information to increaseefficiency and accuracy.12Similar results were found for character and word-basedBLEU, but are omitted for lack of space.172ReferencesMohamed I. Abouelhoda, Stefan Kurtz, and Enno Ohle-busch.
2004.
Replacing suffix trees with enhancedsuffix arrays.
Journal of Discrete Algorithms, 2(1).Yaser Al-Onaizan and Kevin Knight.
2002.
Translat-ing named entities using monolingual and bilingual re-sources.
In Proc.
ACL.Ming-Hong Bai, Keh-Jiann Chen, and Jason S. Chang.2008.
Improving word alignment by adjusting Chi-nese word segmentation.
In Proc.
IJCNLP.Phil Blunsom and Trevor Cohn.
2010.
Inducing syn-chronous grammars with slice sampling.
In Proc.HLT-NAACL, pages 238?241.Phil Blunsom, Trevor Cohn, Chris Dyer, and Miles Os-borne.
2009.
A Gibbs sampler for phrasal syn-chronous grammar induction.
In Proc.
ACL.Ondr?ej Bojar.
2007.
English-to-Czech factored machinetranslation.
In Proc.
WMT.Peter F. Brown, Vincent J.Della Pietra, Stephen A. DellaPietra, and Robert L. Mercer.
1993.
The mathematicsof statistical machine translation: Parameter estima-tion.
Computational Linguistics, 19.Ralf D. Brown.
2002.
Corpus-driven splitting of com-pound words.
In Proc.
TMI.Pi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Proc.WMT.Tagyoung Chung and Daniel Gildea.
2009.
Unsuper-vised tokenization for machine translation.
In Proc.EMNLP.Simon Corston-Oliver and Michael Gamon.
2004.
Nor-malizing German and English inflectional morphologyto improve statistical word alignment.
Machine Trans-lation: From Real Users to Research.Fabien Cromieres.
2006.
Sub-sentential alignment us-ing substring co-occurrence counts.
In Proc.
COL-ING/ACL 2006 Student Research Workshop.John DeNero, Alex Bouchard-Co?te?, and Dan Klein.2008.
Sampling alignment structure under a Bayesiantranslation model.
In Proc.
EMNLP.Michael Denkowski and Alon Lavie.
2011.
Meteor1.3: Automatic Metric for Reliable Optimization andEvaluation of Machine Translation Systems.
In Proc.WMT.Andrew Finch and Eiichiro Sumita.
2007.
Phrase-basedmachine transliteration.
In Proc.
TCAST.Sharon Goldwater and David McClosky.
2005.
Improv-ing statistical MT through morphological analysis.
InProc.
EMNLP.Aria Haghighi, John Blitzer, John DeNero, and DanKlein.
2009.
Better word alignments with supervisedITG models.
In Proc.
ACL.Mark Johnson, Thomas Griffiths, and Sharon Goldwa-ter.
2007.
Bayesian inference for PCFGs via Markovchain Monte Carlo.
In Proc.
NAACL.Dan Klein and Christopher D. Manning.
2003.
A* pars-ing: fast exact Viterbi parse selection.
In Proc.
HLT.Kevin Knight and Jonathan Graehl.
1998.
Machinetransliteration.
Computational Linguistics, 24(4).Phillip Koehn, Franz Josef Och, and Daniel Marcu.
2003.Statistical phrase-based translation.
In Proc.
HLT,pages 48?54.Philipp Koehn.
2004.
Statistical significance tests formachine translation evaluation.
In Proc.
EMNLP.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
In MT Summit.Grzegorz Kondrak, Daniel Marcu, and Kevin Knight.2003.
Cognates can improve statistical translationmodels.
In Proc.
HLT.Young-Suk Lee.
2004.
Morphological analysis for sta-tistical machine translation.
In Proc.
HLT.Klaus Macherey, Andrew Dai, David Talbot, AshokPopat, and Franz Och.
2011.
Language-independentcompound splitting with morphological operations.
InProc.
ACL.Daniel Marcu and William Wong.
2002.
A phrase-based,joint probability model for statistical machine transla-tion.
In Proc.
EMNLP.Cos?kun Mermer and Ahmet Afs?
?n Ak?n.
2010.
Unsu-pervised search for the optimal segmentation for sta-tistical machine translation.
In Proc.
ACL Student Re-search Workshop.Jason Naradowsky and Kristina Toutanova.
2011.
Unsu-pervised bilingual morpheme segmentation and align-ment with context-rich hidden semi-Markov models.In Proc.
ACL.Graham Neubig, Taro Watanabe, Eiichiro Sumita, Shin-suke Mori, and Tatsuya Kawahara.
2011.
An unsuper-vised model for joint phrase alignment and extraction.In Proc.
ACL, pages 632?641, Portland, USA, June.Graham Neubig.
2011.
The Kyoto free translation task.http://www.phontron.com/kftt.ThuyLinh Nguyen, Stephan Vogel, and Noah A. Smith.2010.
Nonparametric word segmentation for machinetranslation.
In Proc.
COLING.Sonja Nie?en and Hermann Ney.
2000.
Improving SMTquality with morpho-syntactic analysis.
In Proc.
COL-ING.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proc.
COLING.173Markus Saers, Joakim Nivre, and Dekai Wu.
2009.Learning stochastic bracketing inversion transductiongrammars with a cubic time biparsing algorithm.
InProc.
IWPT, pages 29?32.Benjamin Snyder and Regina Barzilay.
2008.
Unsuper-vised multilingual learning for morphological segmen-tation.
Proc.
ACL.Virach Sornlertlamvanich, Chumpol Mokarat, and Hi-toshi Isahara.
2008.
Thai-lao machine translationbased on phoneme transfer.
In Proc.
14th AnnualMeeting of the Association for Natural Language Pro-cessing.Michael Subotin.
2011.
An exponential translationmodel for target language morphology.
In Proc.
ACL.David Talbot and Miles Osborne.
2006.
Modelling lexi-cal redundancy for machine translation.
In Proc.
ACL.Jo?rg Tiedemann.
2009.
Character-based PSMT forclosely related languages.
In Proc.
13th AnnualConference of the European Association for MachineTranslation.David Vilar, Jan-T. Peter, and Hermann Ney.
2007.
Canwe translate letters.
In Proc.
WMT.Stephan Vogel, Hermann Ney, and Christoph Tillmann.1996.
HMM-based word alignment in statistical trans-lation.
In Proc.
COLING.Dekai Wu.
1997.
Stochastic inversion transductiongrammars and bilingual parsing of parallel corpora.Computational Linguistics, 23(3).Hao Zhang and Daniel Gildea.
2005.
Stochastic lexical-ized inversion transduction grammar for alignment.
InProc.
ACL.Hao Zhang, Chris Quirk, Robert C. Moore, andDaniel Gildea.
2008a.
Bayesian learning ofnon-compositional phrases with synchronous parsing.Proc.
ACL.Ruiqiang Zhang, Keiji Yasuda, and Eiichiro Sumita.2008b.
Improved statistical machine translation bymultiple Chinese word segmentation.
In Proc.
WMT.174
