Modeling Local Context for Pitch Accent PredictionShimei PanDepartment of Computer ScienceColumbia UniversityNew York, NY, 10027, USApan@cs.columbia.eduJulia HirschbergAT&T Labs-ResearchFlorham Park, NJ, 07932-0971, USAjulia@research.att.comAbstractPitch accent placement is a majortopic in intonational phonology re-search and its application to speechsynthesis.
What factors inuencewhether or not a word is madeintonationally prominent or not isan open question.
In this paper,we investigate how one aspect of aword's local context | its colloca-tion with neighboring words | inu-ences whether it is accented or not.Results of experiments on two tran-scribed speech corpora in a medicaldomain show that such collocationinformation is a useful predictor ofpitch accent placement.1 IntroductionIn English, speakers make some words moreintonationally prominent than others.
Thesewords are said to be accented or to bearpitch accents.
Accented words are typicallylouder and longer than their unaccented coun-terparts, and their stressable syllable is usu-ally aligned with an excursion in the funda-mental frequency.
This excursion will dierin shape according to the type of pitch ac-cent.
Pitch accent type, in turn, inuenceslisteners' interpretation of the accented wordor its larger syntactic constituent.
Previousresearch has associated pitch accent with vari-ation in various types of information status,including the given/new distinction, focus, andcontrastiveness, inter alia.
Assigning pitch ac-cent in speech generation systems which em-ploy speech synthesizers for output is thus crit-ical to system performance: not only must oneconvey meaning naturally, as humans would,but one must avoid conveying mis-informationwhich reliance on the synthesizers' defaultsmay result in.The speech generation work discussed hereis part of a larger eort in developing an intel-ligent multimedia presentation generation sys-tem called MAGIC (Medical Abstract Gen-eration for Intensive Care) (Dalal et al,1996).
In MAGIC, given a patient's medicalrecord stored at Columbia Presbyterian Medi-cal Center (CPMC)'s on-line database system,the system automatically generates a post-operative status report for a patient who hasjust undergone bypass surgery.
There are twomedia-specic generators in MAGIC: a graph-ics generator which automatically producesgraphical presentations from database entities,and a spoken language generator which auto-matically produces coherent spoken languagepresentations from these entities.
The graph-ical and the speech generators communicatewith each other on they to ensure that thenal multimedia output is synchronized.In order to produce natural and coherentspeech output, MAGIC's spoken language gen-erator models a collection of speech features,such as accenting and intonational phrasing,which are critical to the naturalness and intel-ligibility of output speech.
In order to assignthese features accurately, the system needs toidentify useful correlates of accent and phraseboundary location to use as predictors.
Thiswork represents part of our eorts in identi-fying useful predictors for pitch accent place-ment.Pitch accent placement has long been a re-search focus for scientists working on phonol-ogy, speech analysis and synthesis (Bolinger,1989; Ladd, 1996).
In general, syntactic fea-tures are the most widely used features inpitch accent predication.
For example, part-of-speech is traditionally the most useful sin-gle pitch accent predictor (Hirschberg, 1993).Function words, such as prepositions and ar-ticles, are less likely to be accented, whilecontent words, such as nouns and adjectives,are more likely to be accented.
Other lin-guistic features, such as inferred given/newstatus (Hirschberg, 1993; Brown, 1983), con-trastiveness (Bolinger, 1961), and discoursestructure (Nakatani, 1998), have also been ex-amined to explain accent assignment in largespeech corpora.
In a previous study (Pan andMcKeown, 1998; Pan andMcKeown, 1999), weinvestigated how features such as deep syntac-tic/semantic structure and word informative-ness correlate with accent placement.
In thispaper, we focus on how local context inuencesaccent patterns.
More specically, we investi-gate how word collocation inuences whethernouns are accented or not.Determining which nouns are accented andwhich are not is challenging, since part-of-speech information cannot help here.
So, otheraccent predictors must be found.
There aresome advantages in looking only at one wordclass.
We eliminate the interaction betweenpart-of-speech and collocation, so that the in-uence of collocation is easier to identify.
Italso seems likely that collocation may have agreater impact on content words, like nouns,than on function words, like prepositions.Previous researchers have speculated thatword collocation aects stress assignment ofnoun phrases in English.
For example, JamesMarchand (1993) notes how familiar colloca-tions change their stress, witness the Americanpronunciation of `Little House' [in the televi-sion series Little House on the Prairie], wherestress used to be on HOUSE, but now, since theseries is so familiar, is placed on the LITTLE.That is, for collocated words, stress shifts tothe left element of the compound.
However,there are numerous counter-examples: con-sider apple PIE, which retains a right stresspattern, despite the collocation.
So, the ex-tent to which collocational status aects ac-cent patterns is still unclear.Despite some preliminary investigation(Liberman and Sproat, 1992), word colloca-tion information has not, to our knowledge,been successfully used to model pitch accentassignment; nor has it been incorporated intoany existing speech synthesis systems.
In thispaper, we empirically verify the usefulness ofword collocation for accent prediction.
In Sec-tion 2, we describe our annotated speech cor-pora.
In Section 3, we present a description ofthe collocation measures we investigated.
Sec-tion 4 to 7 describe our analyses and machinelearning experiments in which we attempt topredict accent location.
In Section 8 we sumup our results and discuss plans for further re-search.2 Speech CorporaFrom the medical domain described in Section1, we collected two speech corpora and one textcorpus for pitch accent modeling.
The speechcorpora consist of one multi-speaker sponta-neous corpus, containing twenty segments andtotaling fty minutes, and one read corpus ofve segments, read by a single speaker and to-taling eleven minutes of speech.
The text cor-pus consists of 3.5 million words from 7375 dis-charge summaries of patients who had under-gone surgery.
The speech corpora only covercardiac patients, while the text corpus coversa larger group of patients and the majority ofthem have also undergone cardiac surgery.The speech corpora were rst transcribed or-thographically and then intonationally, usingthe ToBI convention for prosodic labeling ofstandard American English (Silverman et al,1992).
For this study, we used only binary ac-cented/deaccented decisions derived from theToBI tonal tier, in which location and typeof pitch accent is marked.
After ToBI label-ing, each word in the corpora was tagged withpart-of-speech, from a nine-element set: noun,verb, adjective, adverb, article, conjunction,pronoun, cardinal, and preposition.
The spon-taneous corpus was tagged by hand and theread tagged automatically.
As noted above,we focus here on predicting whether nouns areaccented or not.3 Collocation MeasuresWe used three measures of word collocation toexamine the relationship between collocationand accent placement: word bigram pre-dictability, mutual information, and theDice coefficient.
While word predictabil-ity is not typically used to measure collocation,there is some correlation between word collo-cation and predictability.
For example, if twowords are collocated, then it will be easy topredict the second word from the rst.
Sim-ilarly, if one word is highly predictable givenanother word, then there is a higher possibilitythat these two words are collocated.
Mutualinformation (Fano, 1961) and the Dice coe?-cient (Dice, 1945) are two standard measuresof collocation.
In general, mutual informationmeasures uncertainty reduction or departurefrom independence.
The Dice coe?cient is acollocation measure widely used in informationretrieval.
In the following, we will give a moredetailed denitions of each.Statistically, bigram word predictability isdened as the log conditional probability ofword wi, given the previous word wi 1:Pred(wi) = log(Prob(wijwi 1))Bigram predictability directly measures thelikelihood of seeing one word, given theoccurrence of the previous word.
Bi-gram predictability has two forms: abso-lute and relative.
Absolute predictability isthe value directly computed from the for-mula.
For example, given four adjacentwords wi 1; wi; wi+1and wi+2, if we assumeProb(wijwi 1) = 0:0001, Prob(wi+1jwi) =0:001, and Prob(wi+2jwi+1) = 0:01, the abso-lute bigram predictability will be -4, -3 and-2 for wi; wi+1and wi+2.
The relative pre-dictability is dened as the rank of absolutepredictability among words in a constituent.In the same example, the relative predictabil-ity will be 1, 2 and 3 for wi; wi+1and wi+2,where 1 is associated with the word with thelowest absolute predictability.
In general, thehigher the rank, the higher the absolute pre-dictability.
Except in Section 7, all the pre-dictability measures mentioned in this paperuse the absolute form.We used our text corpus to compute bigramword predictability for our domain.
When cal-culating the word bigram predictability, werst ltered uncommon words (words occur-ring 5 times or fewer in the corpus) then usedthe Good-Turing discount strategy to smooththe bigram.
Finally we calculated the log con-ditional probability of each word as the mea-sure of its bigram predictability.Two measures of mutual information wereused for word collocation: pointwise mu-tual information, which is dened as :I1(wi 1;wi) = logPr(wi 1; wi)Pr(wi 1)Pr(wi)and average mutual information, whichis dened as:I2(wi 1;wi) =Pr(wi 1; wi) logPr(wi 1; wi)Pr(wi 1)Pr(wi)+Pr(wi 1; wi) logPr(wi 1; wi)Pr(wi 1)Pr(wi)+Pr(wi 1; wi) logPr(wi 1; wi)Pr(wi 1)Pr(wi)+Pr(wi 1; wi) logPr(wi 1; wi)Pr(wi 1)Pr(wi)The same text corpus was used to computeboth mutual information measures.
Only wordpairs with bigram frequency greater than vewere retained.The Dice coe?cient is dened as:Dice(wi 1; wi) =2 Pr(wi 1; wi)Pr(wi 1) + Pr(wi)Here, we also use a cut o threshold of ve tolter uncommon bigrams.Although all these measures are correlated,one measure can score word pairs quite dier-ently from another.
Table 1 shows the top tencollocations for each metric.In the predictability top ten list, we havepairs like scarlet fever where fever is very pre-dictable from scarlet (in our corpus, scarlet isalways followed by fever), thus, it ranks high-est in the predictability list.
Since scarlet canbe di?cult to predict from fever, these typesof pairs will not receive a very high score us-ing mutual information (in the top 5% in I1sorted list and in the top 20% in I2list) andDice coe?cient (top 22%).
From this table, itis also quite clear that I1tends to rank un-common words high.
All the words in the topten I1list have a frequency less than or equalPred I1I2Dicechief complaint polymyalgia rheumatica The patient greeneld ltercerebrospinaluid hemiside stepper present illness Guillain Barrefolic acid Pepto Bismol hospital course Viet Namperiprocedural complications Glen Cove p o Neo Synephrinenormoactive bowel hydrogen peroxide physical exam polymyalgia rheumaticauric acid Viet Nam i d hemiside stepperpostpericardiotomy syndrome Neo Synephrine coronary artery Pepto BismolStaten Island otitis media postoperative day Glen Covescarlet fever Lo Gerfo saphenous vein present illnesspericardiotomy syndrome Chlor Trimeton medical history chief complaintTable 1: Top Ten Most Collocated Words for Each Measureto seven (we lter all the pairs occurring fewerthan six times).Of the dierent metrics, only bigram pre-dictability is a unidirectional measure.
It cap-tures how the appearance of one word aectsthe appearance of the following word.
In con-trast, the other measures are all bidirectionalmeasures, making no distinction between therelative position of elements of a pair of col-located items.
Among the bidirectional mea-sures, point-wise mutual information is sensi-tive to marginal probabilities Pr(wordi 1) andPr(wordi).
It tends to give higher values asthese probabilities decrease, independently ofthe distribution of their co-occurrence.
TheDice coe?cient, however, is not sensitive tomarginal probability.
It computes conditionalprobabilities which are equally weighted inboth directions.Average mutual information measures thereduction in the uncertainty, of one word,given another, and is totally symmetric.
SinceI2(wordi 1; wordi)=I2(wordi;wordi 1), theuncertainty reduction of the rst word, giventhe second word, is equal to the uncer-tainty reduction of the second word, given therst word.
Further more, because I2(wordi;wordi 1) = I2(wordi;wordi 1), the uncer-tainty reduction of one word, given another,is also equal to the uncertainty reduction offailing to see one word, having failed to seethe other.Since there is considerable evidence thatprior discourse context, such as previous men-tion of a word, aects pitch accent decisions,it is possible that symmetric measures, suchas mutual information and the Dice coe?-cient, may not model accent placement aswell as asymmetric measures, such as bigrampredictability.
Also, the bias of point-wisemutual information toward uncommon wordscan aect its ability to model accent assign-ment, since, in general, uncommon words aremore likely to be accented (Pan and McKe-own, 1999).
Since this metric disproportion-ately raises the mutual information for un-common words, making them more predictablethan their appearance in the corpus warrants,it may predict that uncommon words are morelikely to be deaccented than they really are.4 Statistical AnalysesIn order to determine whether word collo-cation is useful for pitch accent prediction,we rst employed Spearman's rank correlationtest (Conover, 1980).In this experiment, we employed a unigrampredictability-based baseline model.
The un-igram predictability of a word is dened asthe log probability of a word in the text cor-pus.
The maximum likelihood estimation ofthis measure is:logFreq(wi)PiFreq(wi)The reason for choosing this as the baselinemodel is not only because it is context inde-pendent, but also because it is eective.
Ina previous study (Pan and McKeown, 1999),we showed that when this feature is used, itis as powerful a predictor as part-of-speech.When jointly used with part-of-speech infor-mation, the combined model can perform sig-nicantly better than each individual model.When tested on a similar medical corpus, thiscombined model also outperforms a compre-hensive pitch accent model employed by theBell Labs' TTS system (Sproat et al, 1992;Hirschberg, 1993; Sproat, 1998), where dis-course information, such as given/new, syntac-tic information, such as POS, and surface in-formation, such as word distance, are incorpo-rated.
Since unigram predictability is contextindependent.
By comparing other predictorsto this baseline model, we can demonstrate theimpact of context, measured by word colloca-tion, on pitch accent assignment.Table 2 shows that for our read speechcorpus, unigram predictability, bigram pre-dictability and mutual information are all sig-nicantly correlated (p < 0:001) with pitch ac-cent decision.1However, the Dice coe?cientshows only a trend toward correlation (p <0:07).
In addition, both bigram predictabil-ity and (pointwise) mutual information show aslightly stronger correlation with pitch accentthan the baseline.
When we conducted a sim-ilar test on the spontaneous corpus, we foundthat all but the baseline model are signicantlycorrelated with pitch accent placement.
Sinceall three models incorporate a context wordwhile the baseline model does not, these re-sults suggest the usefulness of context in ac-cent prediction.
Overall, for all the dierentmeasures of collocation, bigram predictabilityexplains the largest amount of variation in ac-cent status for both corpora.
We conducted asimilar test using trigram predictability, wheretwo context words, instead of one, were usedto predict the current word.
The results areslightly worse than bigram predictability (forthe read corpus r =  0:167, p < 0:0001; forthe spontaneous r =  0:355, p < 0:0001).The failure of the trigram model to improveover the bigram model may be due to sparsedata.
Thus, in the following analysis, we focuson bigram predictability.
In order to furtherverify the eectiveness of word predictabilityin accent prediction, we will show some exam-ples in our speech corpora rst.
Then we willdescribe how machine learning helps to derivepitch accent prediction models using this fea-ture.
Finally, we show that both absolute pre-dictability and relative predictability are use-ful for pitch accent prediction.1Since pointwise mutual information performed con-sistently better than average mutual information in ourexperiment, we present results only for the former.5 Word Predictability and AccentIn general, nouns, especially head nouns, arevery likely to be accented.
However, cer-tain nouns consistently do not get accented.For example, Table 3 shows some collocationscontaining the word cell in our speech cor-pus.
For each context, we list the collocatedpair, its most frequent accent pattern in ourcorpus (upper case indicates that the wordwas accented and lower case indicates thatit was deaccented), its bigram predictability(the larger the number is, the more predictablethe word is), and the frequency of this ac-cent pattern, as well as the total occurrenceof the bigram in the corpus.
In the rst ex-Word Pair Pred(cell) Freq[of] CELL -3.11 7/7[RED] CELL -1.119 2/2[PACKED] cell -0.5759 4/6[BLOOD] cell -0.067 2/2Table 3: cell Collocationsample, cell in [of ] CELL is very unpredictablefrom the occurrence of of and always receives apitch accent.
In [RED] CELL, [PACKED] cell,and [BLOOD] cell, cell has the same semanticmeaning, but dierent accent patterns: cell in[PACKED] cell and [BLOOD] cell is more pre-dictable and deaccented, while in [RED] CELLit is less predictable and is accented.
Theseexamples show the inuence of context andits usefulness for bigram predictability.
Otherpredictable nouns, such as saver in CELLsaver usually are not accented even when theyfunction as head nouns.
Saver is deaccented inten of the eleven instances in our speech cor-pus.
Its bigram score is -1.5517, which is muchhigher than that of CELL (-4.6394{3.1083 de-pending upon context).
Without collocationinformation, a typical accent prediction sys-tem is likely to accent saver, which would beinappropriate in this domain.6 Accent Prediction ModelsBoth the correlation test results and direct ob-servations provide some evidence on the useful-ness of word predictability.
But we still need todemonstrate that we can successfully use thisfeature in automatic accent prediction.
In or-der to achieve this, we used machine learningCorpus Read Spontaneousr p-value r p-valueBaseline (Unigram) r =  0:166 p = 0:0002 r =  0:02 p = 0:39Bigram Predictability r =  0:236 p < 0:0001 r =  0:36 p < 0:0001Pointwise Mutual Information r =  0:185 p < 0:0001 r =  0:177 p < 0:0001Dice Coe?cient r =  0:079 p = 0:066 r =  0:094 p < 0:0001Table 2: Correlation of Dierent Collocation Measures with Accent Decisiontechniques to automatically build accent pre-diction models using bigram word predictabil-ity scores.We used RIPPER (Cohen, 1995b) to ex-plore the relations between predictability andaccent placement.
RIPPER is a classication-based rule induction system.
From annotatedexamples, it derives a set of ordered if-thenrules, describing how input features can beused to predict an output feature.
In orderto avoid overtting, we use 5-fold cross valida-tion.
The training data include all the nouns inthe speech corpora.
The independent variablesused to predict accent status are the unigramand bigram predictability measures, and thedependent variable is pitch accent status.
Weused a majority-based predictability model asour baseline (i.e.
predict accented).In the combined model, both unigram andbigram predictability are used together for ac-cent prediction.
From the results in Table 4,we see that the bigram model consistently out-performs the unigram model, and the com-bined model achieves the best performance.To evaluate the signicance of the improve-ments achieved by incorporating a contextword, we use the standard error produced byRIPPER.
Two results are statistically signif-icant when the results plus or minus twicethe standard error do not overlap (Cohen,1995a).
As shown in Table 4, for the readcorpus, except for the unigram model, all themodels with bigram predictability performedsignicantly better than the baseline model.However, the bigram model and the combinedmodel failed to improve signicantly over theunigram model.
This may result from toosmall a corpus.
For the spontaneous corpus,the unigram, bigram and the combined modelall achieved signicant improvement over thebaseline.
The bigram also performed signi-cantly better than the unigram model.
Thecombined model had the best performance.
Italso achieved signicant improvement over theunigram model.The improvement of the combined modelover both unigram and bigram models maybe due to the fact that some accent patternsthat are not captured by one are indeed cap-tured by the other.
For example, accent pat-terns for street names have been extensivelydiscussed in the literature (Ladd, 1996).
Forexample, street in phrases like (e.g.
FIFTHstreet) is typically deaccented while avenue(e.g.
Fifth AVENUE) is accented.
While itseems likely that the conditional probabilityof Pr(StreetjFifth) is no higher than that ofPr(AvenuejFifth), the unigram probability ofPr(street) is probably higher than that of av-enue Pr(avenue).2.
So, incorporating bothpredictability measures may tease apart theseand similar cases.7 Relative PredictabilityIn the our previous analysis, we showed the ef-fectiveness of absolute word predictability.
Wenow consider whether relative predictability iscorrelated with a larger constituent's accentpattern.
The following analysis focuses on ac-cent patterns of non-trivial base NPs.3Forthis study we labeled base NPs by hand forthe corpora described in Section 2.
For eachbase NP, we calculate which word is the mostpredictable and which is the least.
We wantto see, when comparing with its neighboring2For example, in a 7.5M word general news corpus(from CNN and Reuters), street occurs 2115 times andavenue just 194.
Therefore, the unigram predictabil-ity of street is higher than that of avenue.
The mostcommon bigram with street is Wall Street which occurs116 times and the most common bigram with avenue isPennsylvania Avenue which occurs 97.
In this domain,the bigram predictability for street in Fifth Street is ex-tremely low because this combination never occurred,while that for avenue in Fifth Avenue is -3.0995 whichis the third most predictable bigrams with avenue asthe second word.3Non-recursive noun phrases containing at least twoelements.Corpus Predictability Model Performance Standard Errorbaseline model 81.98%unigram model 82.86%  0.93Read bigram predictability model 84.41%  1.10unigram+bigram model 85.03%  1.04baseline model 70.03%unigram model 72.22%  0.62Spontaneous bigram model 74.46%  0.30unigram+bigram model 77.43%  0.51Table 4: Ripper Results for Accent Status PredictionModel Predictability Total Accented Word Not Accented Accentabilityunigram Least Predictable 1206 877 329 72.72%Most Predictable 1198 485 713 40.48%bigram Least Predictable 1205 965 240 80.08%Most Predictable 1194 488 706 40.87%Table 5: Relative Predictability and Accent Statuswords, whether the most predictable word ismore likely to be deaccented.
As shown in Ta-ble 5, the \total" column represents the totalnumber of most (or least) predictable wordsin all baseNPs4.
The next two columns indi-cate how many of them are accented and deac-cented.
The last column is the percentage ofwords that are accented.
Table 5 shows thatthe probability of accenting a most predictableword is between 40:48% and 45:96% and thatof a least predictable word is between 72:72%and 80:08%.
This result indicates that rela-tive predictability is also a useful predictor fora word's accentability.8 DiscussionIt is di?cult to directly compare our resultswith previous accent prediction studies, todetermine the general utility of bigram pre-dictability in accent assignment, due to dif-ferences in domain and the scope of our task.For example, Hirschberg (1993) built a com-prehensive accent prediction model using ma-chine learning techniques for predicting ac-cent status for all word classes for a text-to-speech system, employing part-of-speech, var-ious types of information status inferred fromthe text, and a number of distance metrics,as well as a complex nominal predictor devel-oped by Sproat (1992).
An algorithm makinguse of these features achieved 76.5%-80% ac-cent prediction accuracy for a broadcast news4The total number of most predictable words is notequal to that of least predictable words due to ties.corpus, 85% for sentences from the ATIS cor-pus of spontaneous elicited speech, and 98.3%success on a corpus of laboratory read sen-tences.
Liberman and Sproat's (1992) successin predicting accent patterns for complex nom-inals alone, using rules combining a numberof features, achieved considerably higher suc-cess rates (91% correct, 5.4% acceptable, 3.6%unacceptable when rated by human subjects)for 500 complex nominals of 2 or more ele-ments chosen from the AP Newswire.
Our re-sults, using bigram predictability alone, 77%for the spontaneous corpus and 85% for theread corpus, and using a dierent success es-timate, while not as impressive as (Libermanand Sproat, 1992)'s, nonetheless demonstratethe utility of a relatively untested feature forthis task.In this paper, we have investigated severalcollocation-based measures for pitch accentprediction.
Our initial hypothesis was thatword collocation aects pitch accent place-ment, and that the more predictable a wordis in terms of its local lexical context, themore likely it is to be deaccented.
In orderto verify this claim, we estimated three col-location measures: word predictability, mu-tual information and the Dice coe?cient.
Wethen used statistical techniques to analyze thecorrelation between our dierent word collo-cation metrics and pitch accent assignmentfor nouns.
Our results show that, of all thecollocation measures we investigated, bigramword predictability has the strongest correla-tion with pitch accent assignment.
Based onthis nding, we built several pitch accent mod-els, assessing the usefulness of unigram andbigram word predictability {as well as a com-bined model{ in accent predication.
Our re-sults show that the bigram model performsconsistently better than the unigram model,which does not incorporate local context in-formation.
However, our combined model per-forms best of all, suggesting that both con-textual and non-contextual features of a wordare important in determining whether or notit should be accented.These results are particularly important forthe development of future accent assignmentalgorithms for text-to-speech.
For our contin-uing research, we will focus on two directions.The rst is to combine our word predictabilityfeature with other pitch accent predictors thathave been previously used for automatic accentprediction.
Features such as information sta-tus, grammatical function, and part-of-speech,have also been shown to be important deter-minants of accent assignment.
So, our nalpitch accent model should include many otherfeatures.
Second, we hope to test whether theutility of bigram predictability can be gener-alized across dierent domains.
For this pur-pose, we have collected an annotated AP newsspeech corpus and an AP news text corpus,and we will carry out a similar experiment inthis domain.9 AcknowledgmentsThanks for C. Jin, K. McKeown, R. Barzi-lay, J. Shaw, N. Elhadad, M. Kan, D. Jor-dan, and anonymous reviewers for the help ondata preparation and useful comments.
Thisresearch is supported in part by the NSF GrantIRI 9528998, the NLM Grant R01 LM06593-01and the Columbia University Center for Ad-vanced Technology in High Performance Com-puting and Communications in Healthcare.ReferencesD.
Bolinger.
1961.
Contrastive accent and con-trastive stress.
language, 37:83{96.D.
Bolinger.
1989.
Intonation and Its Uses.
Stan-ford University Press.G.
Brown.
1983.
Prosodic structure and thegiven/new distinction.
In A. Cutler and D.R.Ladd, ed., Prosody: Models and Measurements,pages 67{78.
Springer-Verlag, Berlin.P.
Cohen.
1995a.
Empirical methods for articialintelligence.
MIT press, Cambridge, MA.W.
Cohen.
1995b.
Fast eective rule induction.In Proc.
of the 12th International Conference onMachine Learning.W.
J. Conover.
1980.
Practical NonparametricStatistics.
Wiley, New York, 2nd edition.M.
Dalal, S. Feiner, K. McKeown, S. Pan, M. Zhou,T.
Hoellerer, J. Shaw, Y. Feng, and J. Fromer.1996.
Negotiation for automated generation oftemporal multimedia presentations.
In Proc.
ofACM Multimedia 96, pages 55{64.Lee R. Dice.
1945.
Measures of the amount ofecologic association between species.
Journal ofEcology, 26:297{302.Robert M. Fano.
1961.
Transmission of Informa-tion: A Statistical Theory of Communications.MIT Press, Cambridge, MA.J.
Hirschberg.
1993.
Pitch accent in context: pre-dicting intonational prominence from text.
Ar-ticial Intelligence, 63:305{340.D.
Robert Ladd.
1996.
Intonational Phonology.Cambridge University Press, Cambridge.M.
Liberman and R. Sproat.
1992.
The stress andstructure of modied noun phrases in English.In I.
Sag, ed., Lexical Matters, pages 131{182.University of Chicago Press.J.
Marchand.
1993.
Message posted on HUMAN-IST mailing list, April.C.
Nakatani.
1998.
Constituent-based accent pre-diction.
In Proc.
of COLING/ACL'98, pages939{945, Montreal, Canada.S.
Pan and K. McKeown.
1998.
Learning intona-tion rules for concept to speech generation.
InProc.
of COLING/ACL'98, Montreal, Canada.S.
Pan and K. McKeown.
1999.
Word informa-tiveness and automatic pitch accent modeling.In Proc.
of the Joint SIGDAT Conference onEMNLP and VLC, pages 148{157.K.
Silverman, M. Beckman, J. Pitrelli, M. Osten-dorf, C. Wightman, P. Price, J. Pierrehumbert,and J. Hirschberg.
1992.
ToBI: a standard forlabeling English prosody.
In Proc.
of ICSLP92.R.
Sproat, J. Hirschberg, and D. Yarowsky.
1992.A corpus-based synthesizer.
In Proc.
of IC-SLP92, pages 563{566, Ban.R.
Sproat, ed.
1998.
Multilingual Text-to-SpeechSynthesis: The Bell Labs Approach.
Kluwer.
