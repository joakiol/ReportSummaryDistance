Lexical ambiguity and Information Retrieval revisitedJ u l io  Gonza lo  Anse lmo Pe f ias  Fe l i sa  Verde joUNEDCiudad Universitaria, s.n.28040 Madrid - Spain{j u l io ,  anse lmo,  f e l i sa}@ieec ,  uned.
esAbst rac tA number of previous experiments on the role oflexical ambiguity, in Information Retrieval are re-produced on the'IR-Semcor test collection (derivedfrom Semcor), where both queries and documentsare hand-tagged ;with phrases, Part-Of-Speech andWordNet 1.5 senses.Our results indicate that a) Word Sense Disambigua-tion can be more beneficial to Information Retrievalthan the experiments ofSanderson (1994) with arti-ficially ambiguous pseudo-words suggested, b) Part-Of-Speech tagging does not seem to help Improvingretrieval, even if it is manually annotated, c) Usingphrases as indexing terms is not a good strategy ifno partial credit is given to the phrase components.1 In t roduct ionA major difficulty to experiment with lexical am-biguity issues in Information Retrieval is always todifferentiate he effects of the indexing and retrievalstrategy being tested from the effects of tagging er-rors.
Some examples are:1.
In (RichardSon and Smeaton, 1995), a sophisti-cated retrieval system based on conceptual sim-ilarity resultled in a decrease of IR performance.It was not possible, however, to distinguish theeffects of the strategy and the effects of auto-matic Wordl Sense Disambiguation (WSD) er-rors.
In (Smeaton and Quigley, 1996), a simi-lar strategy and a combination of manual dis-ambiguation and very short documents -imagecaptions- pioduced, however, an improvementof IR perforinance.2.
In (Krovetz, 1997), discriminating word senseswith differefit Part-Of-Speech (as annotated bythe Church :POS tagger) also harmed retrievalefficiency.
Krovetz noted than more than halfof the words in a dictionary that differ in POSare related i n meaning, but he could not decidewhether the decrease of performance was dueto the loss of such semantic relatedness or toautomatic POS tagging errors.3.
In (Sanderson, 1994), the problem of dis-cerning the effects of differentiating wordsenses from the effects of inaccurate dis-ambiguation was overcome using artificiallycreated pseudo-words (substituting, for in-stance, all occurrences of banana or kalashnikovfor banana/kalashnikov) that could be disam-biguated with 100% accuracy (substituting ba-nana/kalashnikov back to the original term ineach occurrence, ither banana or kalashnikov).He found that IR processes were quite resistantto increasing degrees of lexical ambiguity, andthat disambiguation harmed IR efficiency if per-formed with less that 90% accuracy.
The ques-tion is whether real ambiguous words would be-have as pseudo-words.4.
In (Schiitze and Pedersen, 1995) it was shownthat sense discriminations extracted from thetest collections may enhance text retrieval.However, the static sense inventories in dictio-naries or thesauri -such as WordNet- have notbeen used satisfactorily in IR.
For instance, in(Voorhees, 1994), manual expansion of TRECqueries with semantically related words fromWordNet only produced slight improvementswith the shortest queries.In order to deal with these problems, we designedan IR test collection which is hand annotated withPart-Of-Speech and semantic tags from WordNet1.5.
This collection was first introduced in (Gonzaloet al, 1998) and it is described in Section 2.
Thiscollection is quite small for current IR standards (itis only slightly bigger than the TIME collection),but offers a unique chance to analyze the behaviorof semantic approaches to IR before scaling them upto TREC-size collections (where manual tagging isunfeasible).In (Gonzalo et al, 1998), we used the manualannotations in the IR-Semcor collection to showthat indexing with WordNet synsets can give sig-nificant improvements o Text Retrieval, even forlarge queries.
Such strategy works better than thesynonymy expansion in (Voorhees, 1994), probablybecause it identifies ynonym terms but, at the same195time, it differentiates word  senses.In this paper we use a variant of the IR -Semcorcollection to revise the results of the experiments bySanderson (Sanderson, 1994) and Krovetz (Krovetz,1997) cited above.
The  first one is reproduced usingboth ambiguous  pseudo-words and real ambiguouswords, and the qualitative results compared.
Thispermits us to know if our results are compatible withSanderson experiments or not.
The  effect of lexicalambiguity on IR  processes is discussed in Section 3,and the sensitivity of recall/precision to Word  SenseDisambiguat ion errors in Section 4.
Then,  the exper-iment by Krovetz is reproduced with automatic andmanual ly  produced POS annotations in Section 5, inorder to discern the effect of annotating POS f romthe effect of erroneous annotations.
Finally, the rich-ness of mult iwords in WordNet  1.5 and of phrase an-notations in the IR -Semcor  collection are exploitedin Section 6 to test whether  phrases are good index-ing terms or not.2 The  IR -SEMCOR tes t  co l lec t ionThe  best -known publicly available corpus hand-tagged with WordNet  senses is SEMCOR (Miller etal., 1993), a subset of the Brown Corpus  of about100 documents  that occupies about 2.4 Mb.
of text(22Mb.
including annotations).
The  collection israther heterogeneous, covering politics, sports, mu-sic, cinema, philosophy, excerpts f rom fiction novels,scientific texts...We adapted SEMCOR in order to build a test col-lection -that we call IR-SEMCOR- in four manualsteps:?
We have split the documents in Semcor 1.5 toget coherent chunks of text for retrieval.
Wehave obtained 171 fragments with an averagelength of 1331 words per fragment.
The newdocuments in Semcor 1.6 have been added with-out modification (apart from mapping Wordnet1.6 to WordNet 1.5 senses), up to a total of 254documents.?
We have extended the original TOPIC tags ofthe Brown Corpus with a hierarchy of sub-tags,assigning a set of tags to each text in our col-lection.
This is not used in the experimentsreported here.?
We have written a summary for each of the first171 fragments, with lengths varying between 4and 50 words and an average of 22 words persummary.
Each summary is a human expla-nation of the text contents, not a mere bag ofrelated keywords.?
Finally, we have hand-tagged each of the sum-maries with WordNet 1.5 senses.
When a wordor term was not present in the database, it wasleft unchanged.
In general, such terms corre-spond to proper nouns; in particular, groups(vg.
Fulton_County_Grand_Jury), persons (Cer-vantes) or locations (Fulton).We also generated a list of "stop-senses" and a listof "stop-synsets", automatically translating a stan-dard list of stop words for English.In our first experiments (Gonzalo et al, 1998;Gonzalo et al, 1999), the summaries were used asqueries, and every query was expected to retrieveexactly one document (the one summarized by thequery).
In order to have a more standard set ofrelevance judgments, we have used the following as-sumption here: if an original Semcor document wassplit into n chunks in our test collection, the sum-mary of each of the chunks should retrieve all thechunks of the original document.
This gave us 82queries with an average of 6.8 relevant documentsper query.
In order to test the plausibility of this ar-tificial set of relevance judgments, we produced analternative set of random relevance judgments.
Thisis used as a baseline and included for comparison inall the results presented in this paper.The retrieval engine used in the experiments re-ported here is the INQUERY system (Callan et al,1992).3 Lexical Ambiguity and IRSanderson used a technique previously introducedin (Yarowski, 1993) to evaluate Word Sense Disam-biguators.
Given a text collection, a (size 2) pseudo-word collection is obtained by substituting all oc-currences of two randomly chosen words (say, bankand spring) by a new ambiguous word (bank/spring).Disambiguating each occurrence of this pseudo-wordconsists on finding whether the original term was ei-ther bank or spring.
Note that we are not strictlydiscriminating senses, but also conflating synonymsenses of different words.
We previously showed(Gonzalo et al, 1998) that WordNet synsets seembetter indexing terms than senses.Sanderson used an adapted version of the Reuterstext categorization collection for his experiment, andproduced versions with pseudo-words of size 2 to 10words per pseudo-word.
Then he evaluated the de-crease of IR performance as the ambiguity of theindexing terms is increased.
He found that the re-sults were quite insensitive to ambiguity, except forthe shortest queries.We have reproduce Sanderson's experiment forpseudo-words ranging from size 1 (unmodified) tosize 5.
But when the pseudo-word bank/spring is dis-ambiguated as spring, this term remains ambiguous:it can be used as springtime, or hook, or to jump, etc.We have, therefore, produced another collection of"ambiguity 0", substituting each word by its Word-Net 1.5 semantic tag.
For instance, spring could be196P~E?5403530:2520'151000Figure 1 : Effects of ambiguityi i !
iall (82) queries -e--*:'-.. 24 shortest quedes -+---...~.-..~--.
22 longest quedes -o--~ : .
Random baseline .......... ""-':.-.........
bas~!
).ne ..................................................................................................................................................................................................................synsets wolrds Size 2 ps~udowords Sizle 3 Sizle 4 Size1 2 3 4substituted for n07062238, which is a unique identi-fier for the synset {spring, springtime: the season o/growth}.The results of the experiment can be seen in Fig-ure 1.
We provide 10-point average precision mea-sures 1 for ambiguity 0 (synsets), 1 (words), and 2to 5 (pseudo-words of size 2,3,4,5).
Three curvesare plotted: all queries, shortest queries, and longerqueries.
It can be: seen that:?
The decrease of IR performance from synset in-dexing to word indexing (the slope of the left-most part of: the figure) is more accused thanthe effects of adding pseudoword ambiguity (therest of the figure).
Thus, reducing real ambi-guity seems more useful than reducing pseudo-word ambiguity.?
The curve for shorter queries have a higherslope, confirming that resolving ambiguity ismore benefitial when the relative contributionof each query term is higher.
This is true bothfor real ambiguity and pseudo-word ambiguity.Note, however , that the role of real ambiguity ismore important for longer queries than pseudo-word ambiguity: the curve for longer querieshas a high slope from synsets to words, but it isvery smooth from size 1 to size 5 pseudo-words.?
In our experiments, horter queries behave bet-ter than longer queries for synset indexing (theleftmost points of the curves).
This unexpected1The 10-point average precision is a standard IR measureobtained by averaging precision at recall points 10, 20,... 1O0.behavior is idiosyncratic of the collection: ourdocuments are fragments from original Semcortexts, and we hypothesize that fragments of onetext are relevant o each other.
The shortersummaries are correlated with text chunks thathave more cohesion (for instance, a Semcortext is split into several IRSemcor documentsthat comment on different baseball matches).Longer summaries behave the other way round:IRSemcor documents correspond to less cohe-sive text chunks.
As introducing ambiguity ismore harming for shorter queries, this effect isquickly shadowed by the effects of ambiguity.4 WSD and IRThe second experiment carried out by Sandersonwas to disambiguate the size 5 collection introduc-ing fixed error rates (thus, the original pseudo-wordcollection would correspond to 100% correct disam-biguation).
In his collection, disambiguating below90% accuracy produced worse results than not dis-ambiguating at all.
He concluded that WSD needsto be extremely accurate to improve retrieval resultsrather than decreasing them.We have reproduce his experiment with our size5 pseudo-words collection, ranging from 0% to 50%error rates (100% to 50% accuracy).
In this case,we have done a parallel experiment performing realWord Sense Disambiguation on the original text col-lection, introducing the fixed error rates with respectto the manual semantic tags.
The error rate is un-derstood as the percentage of polysemous words in-197"3"B~2C050454035,3025201510500Figure 2: Effects of WSD errors.
Real words versus pseudo wordsi I i i I t i lSynset indexing with WSD errors -e---Text (no disambiguation thresold for real words) .....Size 5 pseudowords with WSD errors -+--Size 5 pseudowords (no desambiguation thresold for size 5 pseudowords) ...........Random retdeval (baseline) ...... t~xL-  ....... .
g:.... .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
__._____~_ .
.. .
.
.
.
.
+ .
.
.
.
.
.
.
.
.
.
-4-  .
.
.
.
.
.
.
.
.
.
.
- I~ .
.size 5 pseudowords '-,+.. .+..." ' .
.po?
- "_as_eJ!o~ ..............................................................................................................I I I I I I I I I5 10 15 20 25 30 35 40 45 50Percentage of WSD errorscorrectly disambiguated.The results of both experiments can be seen inFigure 2.
We have plotted 10-point average preci-sion in the Y-axis against increasing percentage oferrors in the X-axis.
The curve representing realWSD has as a threshold the 10-pt average precisionfor plain text, and the curve representing pseudo-disambiguation on the size-5 pseudo-word collectionhas as threshold the results for the size-5 collectionwithout disambiguation.
From the figure it can beseen that:?
In the experiment with size 5 pseudo-word is-ambiguation, our collections eems to be moreresistant to WSD errors than the Reuters collec-tion.
The 90% accuracy threshold is now 75%.?
The experiment with real disambiguation ismore tolerant o WSD errors.
Above 60% accu-racy (40% error rate) it is possible to improvethe results of retrieval with plain text.The discrepancy between the behavior of pseudo-words and real ambiguous terms may reside in thenature of real polysemy:?
Unlike the components of a pseudo-word, thedifferent meanings of a real, polysemous wordare often related.
In (Buitelaar, 1998) it isestimated that only 5% of the word stems inWordNet can be viewed as true homonyms (un-related senses), while the remaining 95% poly-semy can be seen as predictable xtensions ofa core sense (regular polysemy).
Therefore, adisambiguation error might be less harmful if astrongly related term is chosen.
This fact Msosuggests that Information Retrieval does notnecessarily demand full disambiguation.
Ratherthan picking one sense and discarding the rest,WSD in IR should probably weight senses ac-cording to their plausibility, discarding only theless likely ones.
This is used in (Schiitze andPedersen, 1995) to get a 14% improvement ofthe retrieval performance disambiguating witha co-occurrence-based in uced thesaurus.
Thisis an issue that arises naturally when translat-ing queries for Cross-Language Text Retrieval,in contrast to Machine Translation.
A Ma-chine Translation system has to choose one sin-gle translation for every term in the sourcedocument.
However, a translation of a queryin Cross-Language r trieval has to pick up alllikely translations for each word in the query.In (Gonzalo et al, 1999) we argue that mappinga word into word senses (or WordNet synsets)is strongly related to that problem.Although the average polysemy of the termsin the Semcor collection is around five (as inSanderson's experiment), the average polysemyof WordNet 1.5 terms is between 2 and 3.
Thereason is that polysemy is correlated with fre-quency of usage.
That means that the best dis-criminators for aquery  will be (in general) theless polysemous terms.
The more polysemousterms are more frequent and thus worse dis-criminators, and disambiguation errors are not198100908070605040302010010Figure 3: Effects of manual and automatic POS taggingi t ~ i i J i iNo tagsBrill POS tagging -4--.Manual POS tags -D--' Random baseline ..x .....I I I I I I I I20 30 40 50 60 70 80 90 100recallas harmful as for the pseudo-words experiment.5 POS tagg ing  and IRAmong many other issues, Krovetz tested to whatextent Part-Of-Speech information was a goodsource of evidence for sense discrimination.
Heannotated words in the T IME collection with theChurch Part-Of-Speech tagger, and found that per-formance decreased.
Krovetz was unable to deter-mine whether the results were due to the taggingstrategy or to the errors made by the tagger.
Heobserved that, in many cases, words were related inmeaning despite a difference in Part-Of-Speech (forinstance, in "summer shoes design" versus "they de-sign sandals").
But he also found that not all errorsmade by the tagger cause a decrease in retrieval per-formance.We have reproduced the experiment by Krovetzin our test collection, using the Brill POS tagger,on one hand, and the manual POS annotations, onthe other.
The precision/recall curves are plotted inFigure 3 against plain text retrieval.
That curvesdoes not show any significant difference between thethree approaches.
A more detailed examination ofsome representative queries is more informative:5.1 Manua l  POS tagg ing  vs. p la in  textAnnotating Part-Of-Speech misses relevant informa-tion for some queries.
For instance, a query con-taining "talented baseball playe~' can be matchedagainst a relevant document containing "is one ofthe top talents of the time", because stemming con-flates talented and talent.
However, POS tagginggives ADg/talent versus N/talent, which do notmatch.
Another example is "skilled diplomat of anAsian Countrff' versus "diplomatic policy", whereN/diplomat and ADJ/diplomat are not matched.However, the documents where the matchingterms agree in category are ranked much higher withPOS tagging, because there are less competing doc-uments.
The two effects seem to compensate, pro-ducing a similar recall/precision curve on overall.Therefore, annotating Part-Of-Speech does notseem worthy as a standalone indexing strategy, evenif tagging is performed manually.
Perhaps givingpartial credit to word occurrences with differentPOS would be an interesting alternative.Annotating POS, however, can be a useful inter-mediate task for IR.
It is, for instance, a first steptowards semantic annotation, which produced muchbetter results in our experiments.5.2 Brill vs. manual taggingAlthough the Brill tagger makes more mistakes thanthe manual annotations (which are not error freeanyway), the mistakes are not completely corre-lated to retrieval decrease.
For instance, a queryabout "summer shoe design" is manually annotatedas "summer/N shoe/N design/N", while the Brilltagger produces "summer/N shoe/N design/if'.
Butan appropriate document contains "Italian designedsandals", which is manually annotated as "Ital-ian/ADJ designed/ADg sandals/N" (no match), butas "Italian/ADJ designed/V sandals/IV" by the Brilltagger (matches design and designed after stem-ming).199In general, comparing with no tagging, the au-tomatic and the manual tagging behave in a verysimilar way.6 Phrase  index ingWordNet is rich in multiword entries (more than55000 variants in WordNet 1.5).
Therefore, suchcollocations are annotated as single entries in theSemcor and IR-Semcor collections.
The manual an-notation also includes name expressions for persons,groups, locations, institutions, etc., such as DrewCentennial Church or Mayor-nominate Ivan AllenYr..
In (Krovetz, 1997), it is shown that the detec-tion of phrases can be useful for retrieval, althoughit is crucial to assign partial credit also to the com-ponents of the collocation.We have performed an experiment o comparethree different indexing strategies:1.
Use plain text both for documents and queries,without using phrase information.2.
Use manually annotated phrases as single in-dexing units in documents and queries.
Thismeans that New_York is a term unrelated tonew or York (which seems clearly beneficialboth for weighting and retrieval), but also thatDrew_Centennial_Church would be a single in-dexing term unrelated to church, which can leadto precise matchings, but also to lose correctquery/document correlations.3.
Use plain text for documents, but exploitthe INQUERY #phrase query operator forthe collocations in the query.
For instance,meeting of the National_Football_League is ex-pressed as #sum(meeting #phrase(Nat iona lFootba l l  League)) in the query language.The #phrase operator assigns credit to the par-tial components of the phrase, while priming itsco-occurrence.The results of the experiments can be seen in Fig-ure 4.
Overall, indexing with multiwords behavesslightly worse than standard word indexing.
Usingthe INQUERY #phrase operator behaves imilarlyto word indexing.A closer look at some case studies, however, givesmore information:?
In some cases, simply indexing with phrases isobviously the wrong choice.
For instance, aquery containing "candidate in governor's_race"does not match "opened his race for governor'.This supports the idea that it is crucial to assigncredit to the partial components of a phrase,and also.
that it may be useful to look for co-occurrence beyond one word windows.?
Phrase indexing works much better when thequery is longer and there are relevant termsapart from one or more multiwords.
In suchcases, a relevant document containing just onequery term is ranked much higher with phraseindexing, because false partial matches witha phrase are not considered.
Just using the#phrase operator behaves mostly like no phraseindexing for these queries, because this filteringis not achieved.Phrase indexing seems more adequate when thequery is intended to be precise, which is not thecase of our collection (we assume that the sum-mary of a fragment has all the fragments in theoriginal text as relevant documents).
For in-stance, "story of a famous strip cartoonist" isnot related -with phrase indexing- to a docu-ment containing "detective_story".
This is cor-rect if the query is intended to be strict, al-though in our collection these are fragments ofthe same text and thus we are assuming theyare related.
The same happens with the query"The board_of_regents of Paris_Junior_Collegehas named the school's new president", whichis not related to "Junior or Senior High SchoolTeaching Certificate".
This could be the rightdecision in a different relevance judgment setup,but it is wrong for our test collection.7 Conclus ionsWe have revised a number of previous experimentsregarding lexical ambiguity and Information Re-trieval, taking advantage of the manual annotationsin our IR-Semcor collection.
Within the limitationsof our collection (mainly its reduced size), we canextract some conclusions:?
Sense ambiguity could be more relevant o In-formation Retrieval than suggested by Sander-son's experiments with pseudo-words.
In par-ticular, his estimation that 90% accuracy isneeded to benefit from Word Sense Disambigua-tion techniques does not hold for real ambiguouswords in our collection.?
Part-Of-Speech information, even if manuallyannotated, seems too discriminatory for Infor-mation Retrieval purposes.
This clarifies theresults obtained by Krovetz with an automaticPOS tagger.?
Taking phrases as indexing terms may decreaseretrieval efficiency.
Phrase indexing could bemore useful, anyway, when the queries demandsa very precise kind of documents, and when thenumber of available documents is high.In our opinion, lexical ambiguity will become acentral topic for Information Retrieval as the impor-tance of Cross-Language Retrieval grows (something200"5100I9O8O70605040302010010Figure 4: Effects of phrase indexingI , i I I I i ,No phrase indexing --e--k Phrase indexing -~-' ,~ #phrase operator in queries -G--' , ~  Random baseline ..x .....?,l I I I I I I I20 30 40 50 60 70 80 90 1 O0recallthat the increasing multilinguality of Internet is al-ready producing).
Although the problem of WordSense Disambigu~ation s still far from being solved,we believe that specific disambiguation for (Cross-Language) Information Retrieval could achieve goodresults by weight!ng candidate senses without a spe-cial commitment to Part-Of-Speech differentiation.An interesting point is that the WordNet struc-ture is not well suited for IR in this respect, as itkeeps noun, verb and adjective synsets completelyunrelated.
The EuroWordNet multilingual database(Vossen, 1998), on the other hand, features cross-part-of-speech semantic relations that could be use-ful in an IR setting.AcknowledgmentsThanks to Douglas Oard for the suggestion that orig-inated this work.ReferencesP.
Buitelaar.
1998.
CoreLex: systematic poly-semy and underspeci\]ication.
Ph.D. thesis, De-partment of Computer Science, Brandeis Univer-sity, Boston.J.
Callan, B. Croft, and S. Harding.
1992.
The IN-QUERY retrieval system.
In Proceedings of the3rd Int.
Conference on Database and Expert Sys-tems applications.J.
Gonzalo, M. F. Verdejo, I. Chugur, andJ.
Cigarrgm.
1.998.
Indexing with Wordnetsynsets can improve Text Retrieval.
In Proceed-ings of the COLING/ACL Workshop on Usageof WordNet in Natural Language Processing Sys-tems.J.
Gonzalo, F. Verdejo, and I. Chugur.
1999.
Us-ing EuroWordNet in a concept-based approach toCross-Language T xt Retrieval.
Applied ArtificialIntelligence, Special Issue on Multilinguality in theSoftware Industry: the AI contribution.R.
Krovetz.
1997.
Homonymy and polysemyin Information Retrieval.
In Proceedings ofACL/EACL '97.G.
A. Miller, C. Leacock, R. Tengi, and R. T.Bunker.
1993.
A semantic oncordance.
In Pro-ceedings of the ARPA Workshop on Human Lan-guage Technology.
Morgan Kauffman.R.
Richardson and A.F.
Smeaton.
1995.
UsingWordnet in a knowledge-based approach to In-formation Retrieval.
In Proceedings of the BCS-IRSG Colloquium, Crewe.M.
Sanderson.
1994.
Word Sense Disambiguationand Information Retrieval.
In Proceedings of 17thInternational Conference on Research and Devel-opment in Information Retrieval.H.
Schiitze and J. Pedersen.
1995.
Information Re-trieval based on word senses.
In Fourth AnnualSymposium on Document Analysis and Informa-tion Retrieval.A.F.
Smeaton and A. Quigley.
1996.
Experimentson using semantic distances between words in im-age caption retrieval.
In Proceedings of the 19 thInternational Conference on Research and Devel-opment in Information Retrieval.Ellen M. Voorhees.
1994.
Query expansion using201lexical-semantic relations.
In Proceedings of the17th International Conference on Research andDevelopment in Information Retrieval.Vossen, P. (ed).
1998.
Euro WordNet: a multilingualdatabase with lexical semantic networks.
KluwerAcademic Publishers.D.
Yarowski.
1993.
One sense per collocation.
InProceedings of ARPA Human Language Technol-ogy Workshop.202
