Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing, pages 20?30,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsEncoding Source Language with Convolutional Neural Network forMachine TranslationFandong Meng1Zhengdong Lu2Mingxuan Wang1Hang Li2Wenbin Jiang1Qun Liu3,11Key Laboratory of Intelligent Information Processing,Institute of Computing Technology, Chinese Academy of Sciences{mengfandong,wangmingxuan,jiangwenbin,liuqun}@ict.ac.cn2Noah?s Ark Lab, Huawei Technologies{Lu.Zhengdong,HangLi.HL}@huawei.com3ADAPT Centre, School of Computing, Dublin City UniversityAbstractThe recently proposed neural networkjoint model (NNJM) (Devlin et al,2014) augments the n-gram target lan-guage model with a heuristically cho-sen source context window, achievingstate-of-the-art performance in SMT.In this paper, we give a more sys-tematic treatment by summarizing therelevant source information through aconvolutional architecture guided bythe target information.
With dif-ferent guiding signals during decod-ing, our specifically designed convolu-tion+gating architectures can pinpointthe parts of a source sentence that arerelevant to predicting a target word,and fuse them with the context of en-tire source sentence to form a unifiedrepresentation.
This representation, to-gether with target language words, arefed to a deep neural network (DNN)to form a stronger NNJM.
Experimentson two NIST Chinese-English trans-lation tasks show that the proposedmodel can achieve significant improve-ments over the previous NNJM by upto +1.08 BLEU points on average.1 IntroductionLearning of continuous space representationfor source language has attracted much at-tention in both traditional statistical machinetranslation (SMT) and neural machine trans-lation (NMT).
Various models, mostly neuralnetwork-based, have been proposed for repre-senting the source sentence, mainly as the en-coder part in an encoder-decoder framework(Bengio et al, 2003; Auli et al, 2013; Kalch-brenner and Blunsom, 2013; Cho et al, 2014;Sutskever et al, 2014).
There has been somequite recent work on encoding only ?relevant?part of source sentence during the decodingprocess, most notably neural network jointmodel (NNJM) in (Devlin et al, 2014), whichextends the n-grams target language model byadditionally taking a fixed-length window ofsource sentence, achieving state-of-the-art per-formance in statistical machine translation.In this paper, we propose novel convolu-tional architectures to dynamically encode therelevant information in the source language.Our model covers the entire source sentence,but can effectively find and properly summa-rize the relevant parts, guided by the informa-tion from the target language.
With the guidingsignals during decoding, our specifically de-signed convolution architectures can pinpointthe parts of a source sentence that are relevantto predicting a target word, and fuse them withthe context of entire source sentence to form aunified representation.
This representation, to-gether with target words, are fed to a deep neu-ral network (DNN) to form a stronger NNJM.Since our proposed joint model is purely lexi-calized, it can be integrated into any SMT de-coder as a feature.Two variants of the joint model are alsoproposed, with coined name tagCNN andinCNN, with different guiding signals usedfrom the decoding process.
We integrate theproposed joint models into a state-of-the-artdependency-to-string translation system (Xieet al, 2011) to evaluate their effectiveness.Experiments on NIST Chinese-English trans-lation tasks show that our model is ableto achieve significant improvements of +2.0BLEU points on average over the baseline.
Ourmodel also outperforms Devlin et al (2014)?sNNJM by up to +1.08 BLEU points.20(a) tagCNN (b) inCNNFigure 1: Illustration for joint LM based on CNN encoder.RoadMap: In the remainder of this paper,we start with a brief overview of joint languagemodel in Section 2, while the convolutional en-coders, as the key component of which, will bedescribed in detail in Section 3.
Then in Sec-tion 4 we discuss the decoding algorithm withthe proposed models.
The experiment resultsare reported in Section 5, followed by Section 6and 7 for related work and conclusion.2 Joint Language ModelOur joint model with CNN encoders can be il-lustrated in Figure 1 (a) & (b), which consists1) a CNN encoder, namely tagCNN or inCNN,to represent the information in the source sen-tences, and 2) an NN-based model for predict-ing the next words, with representations fromCNN encoders and the history words in targetsentence as inputs.In the joint language model, the probabil-ity of the target word en, given previous ktarget words {en?k, ?
?
?, en?1} and the repre-sentations from CNN-encoders for source sen-tence S aretagCNN: p(en|?1(S, {a(en)}), {e}n?1n?k)inCNN: p(en|?2(S, h({e}n?1n?k)), {e}n?1n?k),where ?1(S, {a(en)}) stands for the represen-tation given by tagCNN with the set of indexes{a(en)} of source words aligned to the targetword en, and ?2(S, h({e}n?1n?k)) stands for therepresentation from inCNN with the attentionsignal h({e}n?1n?k).Let us use the example in Figure 1, wherethe task is to translate the Chinese sentenceinto English.
In evaluating a target lan-guage sequence ?holds parliamentand presidential?, with ?holdsparliament and?
as the proceedingwords (assume 4-gram LM), and the affiliatedsource word1of ?presidential?
being?Z?ongt?ong?
(determined by word align-ment), tagCNN generates ?1(S, {4}) (the in-dex of ?Z?ongt?ong?
is 4), and inCNN gener-ates ?2(S, h(holds parliament and)).The DNN component then takes"holds parliament and" and(?1or ?2) as input to give the con-ditional probability for next word, e.g.,p("presidential"|?1|2, {holds,parliament, and}).3 Convolutional ModelsWe start with the generic architecture forconvolutional encoder, and then proceed totagCNN and inCNN as two extensions.1For an aligned target word, we take its aligned sourcewords as its affiliated source words.
And for an unalignedword, we inherit its affiliation from the closest alignedword, with preference given to the right (Devlin et al,2014).
Since the word alignment is of many-to-many,one target word may has multi affiliated source words.21Figure 2: Illustration for the CNN encoders.3.1 Generic CNN EncoderThe basic architecture is of a generic CNN en-coder is illustrated in Figure 2 (a), which has afixed architecture consisting of six layers:Layer-0: the input layer, which takes wordsin the form of embedding vectors.
In ourwork, we set the maximum length of sen-tences to 40 words.
For sentences shorterthan that, we put zero padding at the be-ginning of sentences.Layer-1: a convolution layer after Layer-0,with window size = 3.
As will be dis-cussed in Section 3.2 and 3.3, the guid-ing signal are injected into this layer for?guided version?.Layer-2: a local gating layer after Layer-1, which simply takes a weighted sumover feature-maps in non-adjacent win-dow with size = 2.Layer-3: a convolution layer after Layer-2, weperform another convolution with windowsize = 3.Layer-4: we perform a global gating overfeature-maps on Layer-3.Layer-5: fully connected weights that mapsthe output of Layer-4 to this layer as thefinal representation.3.1.1 ConvolutionAs shown in Figure 2 (a), the convolution inLayer-1 operates on sliding windows of words(width k1), and the similar definition of win-dows carries over to higher layers.
Formally,for source sentence input x = {x1, ?
?
?
,xN},the convolution unit for feature map of type-f(among F`of them) on Layer-` isz(`,f)i(x) = ?
(w(`,f)?z(`?1)i+ b(`,f)),` = 1, 3, f = 1, 2, ?
?
?
, F`(1)where?
z(`,f)i(x) gives the output of feature mapof type-f for location i in Layer-`;?
w(`,f)is the parameters for f on Layer-`;?
?(?)
is the Sigmoid activation function;?
?z(`?1)idenotes the segment of Layer-`?1for the convolution at location i , while?z(0)idef= [x>i, x>i+1, x>i+2]>concatenates the vectors for 3 words fromsentence input x.3.1.2 GatingPrevious CNNs, including those for NLPtasks (Hu et al, 2014; Kalchbrenner et al,2014), take a straightforward convolution-pooling strategy, in which the ?fusion?
deci-sions (e.g., selecting the largest one in max-pooling) are based on the values of feature-maps.
This is essentially a soft template match-ing, which works for tasks like classification,but harmful for keeping the composition func-tionality of convolution, which is critical formodeling sentences.
In this paper, we proposeto use separate gating unit to release the scorefunction duty from the convolution, and let itfocus on composition.22We take two types of gating: 1) for Layer-2, we take a local gating with non-overlappingwindows (size = 2) on the feature-maps of con-volutional Layer-1 for representation of seg-ments, and 2) for Layer-4, we take a globalgating to fuse all the segments for a global rep-resentation.
We found that this gating strategycan considerably improve the performance ofboth tagCNN and inCNN over pooling.?
Local Gating: On Layer-1, for every gat-ing window, we first find its original in-put (before convolution) on Layer-0, andmerge them for the input of the gating net-work.
For example, for the two windows:word (3,4,5) and word (4,5,6) on Layer-0,we use concatenated vector consisting ofembedding for word (3,4,5,6) as the inputof the local gating network (a logistic re-gression model) to determine the weightfor the convolution result of the two win-dows (on Layer-1), and the weighted sumare the output of Layer-2.?
Global Gating: On Layer-3, for feature-maps at each location i, denoted z(3)i, theglobal gating network (essentially soft-max, parameterized wg), assigns a nor-malized weight?
(z(3)i) = ew>gz(3)i/?jew>gz(3)j,and the gated representation on Layer-4 is given by the weighted sum?i?
(z(3)i)z(3)i.3.1.3 Training of CNN encodersThe CNN encoders, including tagCNN andinCNN that will be discussed right below, aretrained in a joint language model described inSection 2, along with the following parameters?
the embedding of the words on source andthe proceeding words on target;?
the parameters for the DNN of joint lan-guage model, include the parameters ofsoft-max for word probability.The training procedure is identical to that ofneural network language model, except that theparallel corpus is used instead of a monolin-gual corpus.
We seek to maximize the log-likelihood of training samples, with one sam-ple for every target word in the parallel corpus.Optimization is performed with the conven-tional back-propagation, implemented as sto-chastic gradient descent (LeCun et al, 1998)with mini-batches.3.2 tagCNNtagCNN inherits the convolution and gatingfrom generic CNN (as described in Section3.1), with the only modification in the inputlayer.
As shown in Figure 2 (b), in tagCNN,we append an extra tagging bit (0 or 1) to theembedding of words in the input layer to indi-cate whether it is one of affiliated wordsx(AFF)i= [x>i1]>, x(NON-AFF)j= [x>j0]>.Those extended word embedding will then betreated as regular word-embedding in the con-volutional neural network.
This particular en-coding strategy can be extended to embed morecomplicated dependency relation in source lan-guage, as will be described in Section 5.4.This particular ?tag?
will be activated in aparameterized way during the training for pre-dicting the target words.
In other words, thesupervised signal from the words to predictwill find, through layers of back-propagation,the importance of the tag bit in the ?affiliatedwords?
in the source language, and learn to putproper weight on it to make tagged words standout and adjust other parameters in tagCNNaccordingly for the optimal predictive perfor-mance.
In doing so, the joint model can pin-point the parts of a source sentence that are rel-evant to predicting a target word through thealready learned word alignment.3.3 inCNNUnlike tagCNN, which directly tells the loca-tion of affiliated words to the CNN encoder,inCNN sends the information about the pro-ceeding words in target side to the convolu-tional encoder to help retrieve the informationrelevant for predicting the next word.
This isessentially a particular case of attention model,analogous to the automatic alignment mecha-nism in (Bahdanau et al, 2014), where the at-23??/VV?
?/NN ??/NN??/NN?/CC?
?/NNChinese:  ??
??
??
?
??
?
?English:  Chile   holds  parliament  and  presidential   elections????
X1:NN(a)(b)Chile    holds    X1??
(c)holdsFigure 3: Illustration for a dependency tree (a) with three head-dependents relations in shadow,an example of head-dependents relation rule (b) for the top level of (a), and an example of headrule (c).
?X1:NN?
indicates a substitution site that can be replaced by a subtree whose root haspart-of-speech ?NN?.
The underline denotes a leaf node.tention signal is from the state of a generativerecurrent neural network (RNN) as decoder.Basically, the information from proceedingwords, denoted as h({e}n?1n?k), is injected intoevery convolution window in the source lan-guage sentence, as illustrated in Figure 2 (c).More specifically, for the window indexed byt, the input to convolution is given by the con-catenated vector?zt= [h({e}n?1n?k), x>t, x>t+1, x>t+2]>.In this work, we use a DNN to transformthe vector concatenated from word-embeddingfor words {en?k?
?
?
, en?k} into h({e}n?1n?k),with sigmoid activation function.
Through lay-ers of convolution and gating, inCNN can 1)retrieve the relevant segments of source sen-tences, and 2) compose and transform theretrieved segments into representation recog-nizable by the DNN in predicting the wordsin target language.
Different from that oftagCNN, inCNN uses information from pro-ceeding words, hence provides complementaryinformation in the augmented joint languagemodel of tagCNN.
This has been empiricallyverified when using feature based on tagCNNand that based on inCNN in decoding withgreater improvement.4 Decoding with the Joint ModelOur joint model is purely lexicalized, andtherefore can be integrated into any SMT de-coders as a feature.
For a hierarchical SMTdecoder, we adopt the integrating method pro-posed by Devlin et al (2014).
As inheritedfrom the n-gram language model for perform-ing hierarchical decoding, the leftmost andrightmost n?
1 words from each constituentshould be stored in the state space.
We ex-tend the state space to also include the in-dexes of the affiliated source words for eachof these edge words.
For an aligned targetword, we take its aligned source words as itsaffiliated source words.
And for an unalignedword, we use the affiliation heuristic adoptedby Devlin et al (2014).
In this paper, we in-tegrate the joint model into the state-of-the-artdependency-to-string machine translation de-coder as a case study to test the efficacy of ourproposed approaches.
We will briefly describethe dependency-to-string translation model andthen the description of MT system.4.1 Dependency-to-String TranslationIn this paper, we use a state-of-the-artdependency-to-string (Xie et al, 2011) decoder(Dep2Str), which is also a hierarchical de-coder.
This dependency-to-string model em-ploys rules that represent the source side ashead-dependents relations and the target sideas strings.
A head-dependents relation (HDR)is composed of a head and all its dependentsin dependency trees.
Figure 3 shows a depen-dency tree (a) with three HDRs (in shadow),24an example of HDR rule (b) for the top levelof (a), and an example of head rule (c).
HDRrules are constructed from head-dependents re-lations.
HDR rules can act as both translationrules and reordering rules.
And head rules areused for translating source words.We adopt the decoder proposed by Menget al (2013) as a variant of Dep2Str trans-lation that is easier to implement with com-parable performance.
Basically they extractthe HDR rules with GHKM (Galley et al,2004) algorithm.
For the decoding procedure,given a source dependency tree T , the de-coder transverses T in post-order.
The bottom-up chart-based decoding algorithm with cubepruning (Chiang, 2007; Huang and Chiang,2007) is used to find the k-best items for eachnode.4.2 MT DecoderFollowing Och and Ney (2002), we use a gen-eral loglinear framework.
Let d be a derivationthat convert a source dependency tree into a tar-get string e. The probability of d is defined as:P (d) ?
?i?i(d)?i(2)where ?iare features defined on derivationsand ?iare the corresponding weights.
Our de-coder contains the following features:Baseline Features:?
translation probabilities P (t|s) andP (s|t) of HDR rules;?
lexical translation probabilities PLEX(t|s)and PLEX(s|t) of HDR rules;?
rule penalty exp(?1);?
pseudo translation rule penalty exp(?1);?
target word penalty exp(|e|);?
n-gram language model PLM(e);Proposed Features:?
n-gram tagCNN joint language modelPTLM(e);?
n-gram inCNN joint language modelPILM(e).Our baseline decoder contains the first eightfeatures.
The pseudo translation rule (con-structed according to the word order of a HDR)is to ensure the complete translation when nomatched rules is found during decoding.
Theweights of all these features are tuned viaminimum error rate training (MERT) (Och,2003).
For the dependency-to-string decoder,we set rule-threshold and stack-threshold to10?3, rule-limit to 100, stack-limit to 200.5 ExperimentsThe experiments in this Section are designed toanswer the following questions:1.
Are our tagCNN and inCNN joint lan-guage models able to improve translationquality, and are they complementary toeach other?2.
Do inCNN and tagCNN benefit fromtheir guiding signal, compared to ageneric CNN?3.
For tagCNN, is it helpful to embed moredependency structure, e.g., dependencyhead of each affiliated word, as additionalinformation?4.
Can our gating strategy improve the per-formance over max-pooling?5.1 SetupData: Our training data are extracted fromLDC data2.
We only keep the sentence pairsthat the length of source part no longer than40 words, which covers over 90% of the sen-tence.
The bilingual training data consist of221K sentence pairs, containing 5.0 millionChinese words and 6.8 million English words.The development set is NIST MT03 (795 sen-tences) and test sets are MT04 (1499 sen-tences) and MT05 (917 sentences) after filter-ing with length limit.Preprocessing: The word alignments are ob-tained with GIZA++ (Och and Ney, 2003) onthe corpora in both directions, using the ?grow-diag-final-and?
balance strategy (Koehn et al,2003).
We adopt SRI Language Modeling2The corpora include LDC2002E18, LDC2003E07,LDC2003E14, LDC2004T07, LDC2005T06.25Systems MT04 MT05 AverageMoses 34.33 31.75 33.04Dep2Str 34.89 32.24 33.57+ BBN-JM (Devlin et al, 2014) 36.11 32.86 34.49+ CNN (generic) 36.12* 33.07* 34.60+ tagCNN 36.33* 33.37* 34.85+ inCNN 36.92* 33.72* 35.32+ tagCNN + inCNN 36.94* 34.20* 35.57Table 1: BLEU-4 scores (%) on NIST MT04-test and MT05-test, of Moses (default settings),dependency-to-string baseline system (Dep2Str), and different features on top of Dep2Str: neuralnetwork joint model (BBN-JM), generic CNN, tagCNN, inCNN and the combination of tagCNNand inCNN.
The boldface numbers and superscript?indicate that the results are significantlybetter (p<0.01) than those of the BBN-JM and the Dep2Str baseline respectively.
?+?
stands foradding the corresponding feature to Dep2Str.Toolkit (Stolcke and others, 2002) to train a4-gram language model with modified Kneser-Ney smoothing on the Xinhua portion of theEnglish Gigaword corpus (306 million words).We parse the Chinese sentences with StanfordParser into projective dependency trees.Optimization of NN: In training the neuralnetwork, we limit the source and target vocab-ulary to the most frequent 20K words for bothChinese and English, covering approximately97% and 99% of two corpus respectively.
Allthe out-of-vocabulary words are mapped to aspecial token UNK.
We used stochastic gradientdescent to train the joint model, setting the sizeof minibatch to 500.
All joint models used a 3-word target history (i.e., 4-gram LM).
The di-mension of word embedding and the attentionsignal h({e}n?1n?k) for inCNN are 100.
For theconvolution layers (Layer 1 and Layer 3), weapply 100 filters.
And the final representationof CNN encoders is a vector with dimension100.
The final DNN layer of our joint model isthe standard multi-layer perceptron with soft-max at the top layer.Metric: We use the case-insensitive 4-gram NIST BLEU3as our evaluation met-ric, with statistical significance test with sign-test (Collins et al, 2005) between the proposedmodels and two baselines.3ftp://jaguar.ncsl.nist.gov/mt/resources/mteval-v11b.pl5.2 Setting for Model ComparisonsWe use the tagCNN and inCNN joint lan-guage models as additional decoding fea-tures to a dependency-to-string baseline sys-tem (Dep2Str), and compare them to the neu-ral network joint model with 11 source con-text words (Devlin et al, 2014).
We usethe implementation of an open source toolkit4with default configuration except the globalsettings described in Section 5.1.
Since ourtagCNN and inCNN models are source-to-target and left-to-right (on target side), we onlytake the source-to-target and left-to-right typeNNJM in (Devlin et al, 2014) in compari-son.
We call this type NNJM as BBN-JM here-after.
Although the BBN-JM in (Devlin et al,2014) is originally tested in the hierarchicalphrase-based (Chiang, 2007) SMT and string-to-dependency (Shen et al, 2008) SMT, it isfairly versatile and can be readily integratedinto Dep2Str.5.3 The Main ResultsThe main results of different models are givenin Table 1.
Before proceeding to more detailedcomparison, we first observe that?
the baseline Dep2Str system gives BLEU0.5+ higher than the open-source phrase-based system Moses (Koehn et al, 2007);?
BBN-JM can give about +0.92 BLEUscore over Dep2Str, a result similar as re-ported in (Devlin et al, 2014).4http://nlg.isi.edu/software/nplm/26Systems MT04 MT05 AverageDep2str 34.89 32.24 33.57+tagCNN 36.33 33.37 34.85+tagCNN dep 36.54 33.61 35.08Table 2: BLEU-4 scores (%) of tagCNNmodel with dependency head words as addi-tional tags (tagCNN dep).Clearly from Table 1, tagCNN and inCNNimprove upon the Dep2Str baseline by +1.28and +1.75 BLEU, outperforming BBN-JM inthe same setting by respectively +0.36 and+0.83 BLEU, averaged on NIST MT04 andMT05.
These indicate that tagCNN andinCNN can individually provide discrimina-tive information in decoding.
It is worth not-ing that inCNN appears to be more informativethan the affiliated words suggested by the wordalignment (GIZA++).
We conjecture that thisis due to the following two facts?
inCNN avoids the propagation of mis-takes and artifacts in the already learnedword alignment;?
the guiding signal in inCNN providescomplementary information to evaluatethe translation.Moreover, when tagCNN and inCNN are bothused in decoding, it can further increase itswinning margin over BBN-JM to +1.08 BLEUpoints (in the last row of Table 1), indicatingthat the two models with different guiding sig-nals are complementary to each other.The Role of Guiding Signal It is slight sur-prising that the generic CNN can also achievethe gain on BLEU similar to that of BBN-JM, since intuitively generic CNN encodes theentire sentence and the representations shouldin general far from optimal representation forjoint language model.
The reason, as we con-jecture, is CNN yields fairly informative sum-marization of the sentence (thanks to its so-phisticated convolution and gating architec-ture), which makes up some of its loss onresolution and relevant parts of the sourcesenescence.
That said, the guiding signal inboth tagCNN and inCNN are crucial to theSystems MT04 MT05 AverageDep2Str 34.89 32.24 33.57+inCNN 36.92 33.72 35.32+inCNN-2-pooling 36.33 32.88 34.61+inCNN-4-pooling 36.46 33.01 34.74+inCNN-8-pooling 36.57 33.39 34.98Table 3: BLEU-4 scores (%) of inCNN mod-els implemented with gating strategy and kmax-pooling, where k is of {2, 4, 8}.power of CNN-based encoder, as can be eas-ily seen from the difference between the BLEUscores achieved by generic CNN, tagCNN, andinCNN.
Indeed, with the signal from the al-ready learned word alignment, tagCNN cangain +0.25 BLEU over its generic counterpart,while for inCNN with the guiding signal fromthe proceeding words in target, the gain is moresaliently +0.72 BLEU.5.4 Dependency Head in tagCNNIn this section, we study whether tagCNN canfurther benefit from encoding richer depen-dency structure in source language in the input.More specifically, the dependency head wordscan be used to further improve tagCNN model.As described in Section 3.2, in tagCNN, weappend a tagging bit (0 or 1) to the embeddingof words in the input layer as tags on whetherthey are affiliated source words.
To incorpo-rate dependency head information, we extendthe tagging rule in Section 3.2 to add anothertagging bit (0 or 1) to the word-embedding fororiginal tagCNN to indicate whether it is partof dependency heads of the affiliated words.For example, if xiis the embedding of an af-filiated source word and xjthe dependencyhead of word xi, the extended input of tagCNNwould containx(AFF, NON-HEAD)i= [x>i1 0]>x(NON-AFF, HEAD)j= [x>j0 1]>If the affiliated source word is the root of asentence, we only append 0 as the second tag-ging bit since the root has no dependency head.From Table 2, with the help of dependencyhead information, we can improve tagCNN by+0.23 BLEU points averagely on two test sets.275.5 Gating Vs. Max-poolingIn this section, we investigate to what extentthat our gating strategy can improve the trans-lation performance over max pooling, with thecomparisons on inCNN model as a case study.For implementation of inCNN with max-pooling, we replace the local-gating (Layer-2)with max-pooling with size 2 (2-pooling forshort), and global gating (Layer-4) with k max-pooling (?k-pooling?
), where k is of {2, 4, 8}.Then, we use the mean of the outputs of k-pooling as the final input of Layer-5.
In do-ing so, we can guarantee the input dimensionof Layer-5 is the same as the architecture withgating.
From Table 3, we can clearly seethat our gating strategy can improve translationperformance over max-pooling by 0.34?0.71BLEU points.
Moreover, we find 8-poolingyields performance better than 2-pooling.
Weconjecture that this is because the useful rel-evant parts for translation are mainly concen-trated on a few words of the source sentence,which can be better extracted with a larger poolsize.6 Related WorkThe seminal work of neural network languagemodel (NNLM) can be traced to Bengio et al(2003) on monolingual text.
It is recently ex-tended by Devlin et al (2014) to include ad-ditional source context (11 source words) inmodeling the target sentence, which is clearlymost related to our work, with however two im-portant differences: 1) instead of the ad hocway of selecting a context window in (Devlinet al, 2014), our model covers the entire sourcesentence and automatically distill the contextrelevant for target modeling; 2) our convo-lutional architecture can effectively leverageguiding signals of vastly different forms andnature from the target.Prior to our model there is also work onrepresenting source sentences with neural net-works, including RNN (Cho et al, 2014;Sutskever et al, 2014) and CNN (Kalchbren-ner and Blunsom, 2013).
These work typi-cally aim to map the entire sentence to a vec-tor, which will be used later by RNN/LSTM-based decoder to generate the target sentence.As demonstrated in Section 5, the representa-tion learnt this way cannot pinpoint the rele-vant parts of the source sentences (e.g., wordsor phrases level) and therefore is inferior tobe directly integrated into traditional SMT de-coders.Our model, especially inCNN, is inspiredby is the automatic alignment model proposedin (Bahdanau et al, 2014).
As the first effortto apply attention model to machine transla-tion, it sends the state of a decoding RNN asattentional signal to the source end to obtain aweighted sum of embedding of source wordsas the summary of relevant context.
In con-trast, inCNN uses 1) a different attention sig-nal extracted from proceeding words in partialtranslations, and 2) more importantly, a con-volutional architecture and therefore a highlynonlinear way to retrieve and summarize therelevant information in source.7 Conclusion and Future WorkWe proposed convolutional architectures forobtaining a guided representation of the entiresource sentence, which can be used to augmentthe n-gram target language model.
With differ-ent guiding signals from target side, we devisetagCNN and inCNN, both of which are testedin enhancing a dependency-to-string SMT with+2.0 BLEU points over baseline and +1.08BLEU points over the state-of-the-art in (De-vlin et al, 2014).
For future work, we will con-sider encoding more complex linguistic struc-tures to further enhance the joint model.AcknowledgmentsMeng, Wang, Jiang and Liu are supportedby National Natural Science Foundation ofChina (Contract 61202216).
Liu is partiallysupported by the Science Foundation Ireland(Grant 12/CE/I2267 and 13/RC/2106) as partof the ADAPT Centre at Dublin City Univer-sity.
We sincerely thank the anonymous re-viewers for their thorough reviewing and valu-able suggestions.References[Auli et al2013] Michael Auli, Michel Galley,Chris Quirk, and Geoffrey Zweig.
2013.
Jointlanguage and translation modeling with recur-rent neural networks.
In Proceedings of the282013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1044?1054,Seattle, Washington, USA, October.
[Bahdanau et al2014] Dzmitry Bahdanau,Kyunghyun Cho, and Yoshua Bengio.
2014.Neural machine translation by jointly learn-ing to align and translate.
arXiv preprintarXiv:1409.0473.
[Bengio et al2003] Yoshua Bengio, RjeanDucharme, Pascal Vincent, and ChristianJauvin.
2003.
A neural probabilistic lan-guage model.
Journal OF Machine LearningResearch, 3:1137?1155.
[Chiang2007] David Chiang.
2007.
Hierarchicalphrase-based translation.
Computational Lin-guistics, 33(2):201?228.
[Cho et al2014] Kyunghyun Cho, Bart van Mer-rienboer, Caglar Gulcehre, Dzmitry Bahdanau,Fethi Bougares, Holger Schwenk, and YoshuaBengio.
2014.
Learning phrase representa-tions using rnn encoder?decoder for statisticalmachine translation.
In Proceedings of the 2014Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1724?1734, Doha, Qatar, October.
[Collins et al2005] Michael Collins, PhilippKoehn, and Ivona Ku?cerov?a.
2005.
Clauserestructuring for statistical machine translation.In Proceedings of the 43rd Annual Meetingon Association for Computational Linguistics,pages 531?540.
[Devlin et al2014] Jacob Devlin, Rabih Zbib,Zhongqiang Huang, Thomas Lamar, RichardSchwartz, and John Makhoul.
2014.
Fast androbust neural network joint models for statisticalmachine translation.
In Proceedings of the 52ndAnnual Meeting of the Association for Compu-tational Linguistics (Volume 1: Long Papers),pages 1370?1380, Baltimore, Maryland, June.
[Galley et al2004] Michel Galley, Mark Hopkins,Kevin Knight, and Daniel Marcu.
2004.What?s in a translation rule.
In Proceedings ofHLT/NAACL, volume 4, pages 273?280.
Boston.
[Hu et al2014] Baotian Hu, Zhengdong Lu, HangLi, and Qingcai Chen.
2014.
Convolutionalneural network architectures for matching natu-ral language sentences.
In NIPS.
[Huang and Chiang2007] Liang Huang and DavidChiang.
2007.
Forest rescoring: Faster de-coding with integrated language models.
InAnnual Meeting-Association For ComputationalLinguistics, volume 45, pages 144?151.
[Kalchbrenner and Blunsom2013] Nal Kalchbren-ner and Phil Blunsom.
2013.
Recurrent contin-uous translation models.
In Proceedings of the2013 Conference on Empirical Methods in Nat-ural Language Processing, pages 1700?1709,Seattle, Washington, USA, October.
[Kalchbrenner et al2014] Nal Kalchbrenner, Ed-ward Grefenstette, and Phil Blunsom.
2014.
Aconvolutional neural network for modelling sen-tences.
ACL.
[Klein and Manning2002] Dan Klein and Christo-pher D Manning.
2002.
Fast exact inferencewith a factored model for natural language pars-ing.
In Advances in neural information process-ing systems, volume 15, pages 3?10.
[Koehn et al2003] Philipp Koehn, Franz Josef Och,and Daniel Marcu.
2003.
Statistical phrase-based translation.
In Proceedings of the 2003Conference of the North American Chapterof the Association for Computational Linguis-tics on Human Language Technology-Volume 1,pages 48?54.
[Koehn et al2007] Philipp Koehn, Hieu Hoang,Alexandra Birch, Chris Callison-Burch, Mar-cello Federico, Nicola Bertoldi, Brooke Cowan,Wade Shen, Christine Moran, Richard Zens,Chris Dyer, Ondrej Bojar, Alexandra Constantin,and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Pro-ceedings of the 45th Annual Meeting of the Asso-ciation for Computational Linguistics Compan-ion Volume Proceedings of the Demo and PosterSessions, pages 177?180, Prague, Czech Repub-lic, June.
[LeCun et al1998] Y. LeCun, L. Bottou, G. Orr, andK.
Muller.
1998.
Efficient backprop.
In NeuralNetworks: Tricks of the trade.
Springer.
[Meng et al2013] Fandong Meng, Jun Xie, LinfengSong, Yajuan L?u, and Qun Liu.
2013.
Trans-lation with source constituency and dependencytrees.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 1066?1076, Seattle, Washington,USA, October.
[Och and Ney2002] Franz Josef Och and HermannNey.
2002.
Discriminative training and max-imum entropy models for statistical machinetranslation.
In Proceedings of the 40th AnnualMeeting on Association for Computational Lin-guistics, pages 295?302.
[Och and Ney2003] Franz Josef Och and HermannNey.
2003.
A systematic comparison of vari-ous statistical alignment models.
Computationallinguistics, 29(1):19?51.
[Och2003] Franz Josef Och.
2003.
Minimum errorrate training in statistical machine translation.
InProceedings of the 41st Annual Meeting on As-sociation for Computational Linguistics-Volume1, pages 160?167.29[Shen et al2008] Libin Shen, Jinxi Xu, and RalphWeischedel.
2008.
A new string-to-dependencymachine translation algorithm with a target de-pendency language model.
In Proceedings ofACL-08: HLT, pages 577?585.
[Stolcke and others2002] Andreas Stolcke et al2002.
Srilm-an extensible language modelingtoolkit.
In Proceedings of the internationalconference on spoken language processing, vol-ume 2, pages 901?904.
[Sutskever et al2014] Ilya Sutskever, Oriol Vinyals,and Quoc V. Le.
2014.
Sequence to se-quence learning with neural networks.
CoRR,abs/1409.3215.
[Xie et al2011] Jun Xie, Haitao Mi, and Qun Liu.2011.
A novel dependency-to-string model forstatistical machine translation.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, pages 216?226.30
