A New Probabilistic Model for Title GenerationRong JinLanguage Technology InstituteCarnegie Mellon University5000 Forbes Ave.Pittsburgh, PA15213, U. S. A.rong+@cs.cmu.eduAlexander G. HauptmannDepartment of Computer ScienceCarnegie Mellon University5000 Forbes Ave.Pittsburgh, PA15213, U. S. A.alex+@cs.cmu.eduAbstractTitle generation is a complex task involvingboth natural language understanding andnatural language synthesis.
In this paper, wepropose a new probabilistic model for titlegeneration.
Different from the previousstatistical models for title generation, whichtreat title generation as a generation processthat converts the ?document representation?of information directly into a ?titlerepresentation?
of the same information, thismodel introduces a hidden state called?information source?
and divides titlegeneration into two steps, namely the step ofdistilling the ?information source?
from theobservation of a document and the step ofgenerating a title from the estimated?information source?.
In our experiment, thenew probabilistic model outperforms theprevious model for title generation in termsof both automatic evaluations and humanjudgments.IntroductionCompared with a document, a title provides acompact representation of the information andtherefore helps people quickly capture the mainidea of a document without spending time on thedetails.
Automatic title generation is a complextask, which not only requires finding the titlewords that reflects the document content but alsodemands ordering the selected title words intohuman readable sequence.
Therefore, it involvesin both nature language understanding andnature language synthesis, which distinguishestitle generation from other seemingly similartasks such as key phrase extraction or automatictext summarization where the main concern oftasks is identify important information unitsfrom documents (Mani & Maybury., 1999).The statistical approach toward title generationhas been proposed and studied in the recentpublications (Witbrock & Mittal, 1999; Kennedy& Hauptmann, 2000; Jin & Hauptmann, 2001).The basic idea is to first learn the correlationbetween the words in titles (title words) and thewords in the corresponding documents(document words) from a given training corpusconsisting of document-title pairs, and thenapply the learned title-word-document-wordcorrelations to generate titles for unseendocuments.Witbrock and Mittal (1999) proposed astatistical framework for title generation wherethe task of title generation is decomposed intotwo phases, namely the title word selectionphase and the title word ordering phase.
In thephase of title word selection, each title word isscored based on its indication of the documentcontent.
During the title word ordering phase,the ?appropriateness?
of the word order in a titleis scored using ngram statistical language model.The sequence of title words with highest score inboth title word selection phase and title wordordering phase is chosen as the title for thedocument.
The follow-ups within thisframework mainly focus on applying differentapproaches to the title word selection phase (Jin& Hauptmann, 2001; Kennedy & Hauptmann,2000).However, there are two problems with thisframework for title generation.
They are:?
A problem with the title word orderingphase.
The goal of title word selection phase isto find the appropriate title words for documentand the goal of title word ordering phase is tofind the appropriate word order for the selectedtitle words.
In the framework proposed byWitbrock and Mittal (1999), the title wordordering phase is accomplished by using ngramlanguage model (Clarkson & Rosenfeld, 1997)to predict the probability P(T), i.e.
howfrequently the word sequence T is used as a titlefor a document.
Of course, the probability forthe word sequence T to be used as a title for anydocument is definitely influenced by thecorrectness of the word order in T. However, thefactor whether the words in the sequence T arecommon words or not will also have greatinfluence on the chance of seeing the sequence Tas a title.
Word sequence T with many rarewords, even with a perfect word order, will bedifficult to match with the content of mostdocuments and has small chance to be used as atitle.
As the result, using probability P(T) for thepurpose of ordering title words can cause thegenerated titles to include unrelated commontitle words.
The obvious solution to this problemis to somehow eliminate the bias of favouringcommon title words from probability P(T) andleave it only with the task of the word ordering.?
A problem with the title word selectionphase.
The title word selection phase isresponsible for coming up with a set of titlewords that reflect the meaning of the document.In the framework proposed by Witbrock andMittal (1999), every document word has anequal vote for title words.
However, title onlyneeds to reflect the main content of a documentnot every single detail of that document.Therefore, letting all the words in the documentparticipate equally in the selection of title wordscan cause a large variance in choosing titlewords.
For example, common words usuallyhave little to do with the content of documents.Therefore, allowing common words of adocument equally compete with the contentwords in the same document in choosing titlewords can seriously degrade the quality ofgenerated titles.The solution we proposed to this problem is tointroduce a hidden state called ?informationsource?.
This ?information source?
will samplethe important content word out of a documentand a title will be computed based on thesampled ?information source?
instead of theoriginal document.
By striping off the commonwords through the ?information source?
state, weare able to reduce the noise introduced bycommon words to the documents in selectingtitle words.
The schematic diagram for the ideais shown in Figure 1, together with theschematic diagram for the framework byWitbrock and Mittal.
As indicated by Figure 1,the old framework for title generation has only asingle ?channel?
connecting the document wordsto the title words while the new model containstwo ?channels?
with one connecting thedocument words to the ?information source?state and the other connecting the ?informationsource?
state to the title words.T itle  W o rd s{ T W }D o c u m e n t W o rd s{ D W }P (T W |D W )O ld  M o d e lT itle  W ord s{ T W }D o c u m e n t W o rd s{ D W }In fo rm a tio n  S o u rc e{ D W ?
: c o n te n t w o rd }N e w  M o d e lP (D W ?|D W )P (T W |D W )Fig.
1: Graphic representation for previous title generationmodel and new model for title generation.1 Probabilistic Title Generation ModelIn the language of probabilistic theory, the goalof creating a title T for a document D can beformalized as the search of the word sequence Tthat can be best generated by the document D, or)|?
(maxarg?DTPTT=  (1)Therefore, the key of a probabilistic model fortitle generation is how to estimate the probabilityP(T|D).
i.e.
the probability of having a wordsequence T as the title for the document D.In this section, we will first describe the oldframework using probability theory andassociate the two problems of the old frameworkwith the flaw in estimation of the probabilityP(T|D).
Then a solution to each of the twoproblems will be presented and the new modelbased on the old framework for title generationwith the adaptation of the solutions will bedescribed at the end of this section.1.1 Formal Description of OldFramework for Title GenerationIn terms of probability theory, the oldframework can be interpreted as approximatingthe probability P(T|D) as a product of two termswith term P({tw?T}|D) responsible for the titleword selection and term P(T) responsible for thetitle word ordering and the probability P(T|D)can be written as:)()|}({)|( TPDTtwPDTP ??
(2)where {tw?T} stands for the set of words in thetitle T. Since P({tw?T}|D) stands for theprobability of using the set of  words tw in wordsequence T as title words given the observationof the document D, it corresponds to the titleword selection phase.
P(T) stands for theprobability of using word sequence T as a titlefor any document.
Since word sequences withwrong word orders are rarely seen as titles forany document, the word order in word sequenceT is an important factor in determining thefrequency of seeing word sequence T as a titlefor documents and therefore it can be associatedwith the title word ordering phase.1.2 Problem with the title word orderingphaseIn the old framework for title generation, termP(T) is used for ordering title words into acorrect sequence.
However, term P(T) is notonly influenced by the word order in T, but alsowhether words in T are common words.
A wordsequence T with a set of rare words will havesmall chance to be used as a title for anydocument even if the word order in T isperfectly correct.
On the other side, a title T witha set of common words can have a good chanceto be a title for some documents even its wordorder is problematic.
Therefore, the probabilityfor a word sequence T to be used as a title, i.e.P(T), is determined by both the?appropriateness?
of the word order of T and the?rareness?
of the words in T and doesn?tappropriately represent the process of title wordordering whose only goal is to identify a correctword order with the given words.In terms of formal analysis, the problem with thetitle word selection phase can be attributed to theoversimplified approximation for probabilityP(T|D).
According to the chain rule inprobability theory, the approximation for P(T|D)in Equation (2) is quite problematic and a morereasonable expansion for probability P(T|D)should be following:})({/)()|}({})?{|()|}({)|(TtwPTPDTtwPTtwTPDTtwPDTP??=???
(3)where P({tw?T}) stands for the probability ofusing the set of word {tw?T} in titles withoutconsidering the word order.
The differencebetween Equations (3) and (2) is that, Equation(2) uses term P(T) directly for title wordordering phase while Equation (3) divides termP(T) by term P({tw?T}) and uses the result ofdivision for title word ordering process.
Becauseterm P({tw?T}) concerns only with thepopularity of the words tw in sequence T,dividing P(T) by P({tw?T}) has the effect ofremoving the bias of favouring popular titlewords from term P(T).
Therefore, termP(T)/P({tw?T}) is determined mainly by theword order in T and not influenced by thepopularity of title words in T.1.3 Problem with title word selectionphaseAs already discussed in the introduction section,the old framework for title generation allows allthe words in the document equally participate inselecting title words and therefore, the finalchoice of title words may be influencedsignificantly by the common words in thedocument which have nothing to do with thecontent of the document.
Thus, we suggest asolution to this problem by introducing a hiddenstate called ?information source?
which is able tosampled the important content words from theoriginal document.
To find an optimal title for adocument, we will create the title from the?distilled information source?
instead of theoriginal document.To allow titles being generated from the?distilled information source?
instead of theoriginal document, we can expand theprobability P(T|D) as the sum of theprobabilities P(T| ?information source?
S) overall the possible ?information sources?
S, whereprobability P(T|S) stands for the probability ofusing the word sequence T as the title for the?information source?
S. Formally, this idea canbe expressed as:?=SDSPSTPDTP )|()|()|(  (4)where symbol S stands for a possible?information source?
S for the document D. InEquation (4), term P(T|S)P(S|D) represents theidea of two noisy channels, with term P(S|D)corresponding to the first channel that samples?information source?
S out of the originaldocument D and term P(T|S) corresponding tothe second noisy channel that creates title Tfrom the ?distilled information source?
S. Sincethe first noisy channel, i.e.
P(S|D), is new to theold framework for title generation, we will focuson the discussion of the noisy channel P(S|D).Since the motivation of introducing the hiddenstate ?information source?
S is to strip off thecommon words and have important contentwords kept, we want the noisy channel P(S|D) tobe a sampling process where important contentwords have higher chances to be selected thancommon words.
Let function g(dw,D) stands forthe importance of the word dw related to thedocument D. Then, the word samplingdistribution should be proportional to the wordimportance function g(dw,D).
Therefore, we canwrite the probability P(S|D)??
?SdwDdwgDSP ),()|(  (5)As indicated by Equation (5), the probability for?information source?
S to represent the contentof the document D, i.e.
P(S|D), is proportional tothe product of the importance function values forall the words selected by ?information source?
S.1.4 A New Model for Title GenerationThe new model is based on the old frameworkwith the proposed solutions to the problems ofthe old framework.
As the summary ofdiscussions in the previous two subsections, theessential idea of this new model is in twoaspects:?
Creating titles from the distilled?information source?.
To prevent thecommon words in the document from votingfor title words, in the new model, titles willbe created from the estimated ?informationsource?
which has common document wordsstripped off.?
Subtract the influence of the?commonness?
of title words from P(T).
Inthe old framework for title generation, termP(T) is associated with the title wordordering phase.
Since both the word orderand the word ?commonness?
can influencethe occurring probability of the sequence T,i.e.
P(T), we need to subtract the factor ofword ?commonness?
from term P(T), whichresults in term P(T)/P({tw?T}) for the titleword ordering phase.T itle  W ords{T W 1, T W 2, ?
, T W m }D ocum ent W ords{D W 1, D W 2, ?
, D W n}Info rm ation So urce{D W ?1 , D W ?2 , ?
, D W ?m  }Sam ple content w ords D W ?
o ut o fall the w ords D W  using g(D W ,D )C rea te  title  w ord T W  from  D W ?susing P (T W |D W ?
)W ord  Sequence TO rder selected  title  w ord  in  a  sequenceusing P (T )/P ({tw ?
T })Fig.
2: Representation of the title generation scheme usedby the new model.
n is the number of words in thedocument and m is the number of words in the title.Therefore, by putting Equations (2), (4) and (5)together, our new model for title generation canbe expressed as?
???
?S dwDdwgSTtwPTtwPTPDTP ),()|}({})({)()|?
(  (6)By further assuming that the number of words inany ?information source?
S is equal to thenumber of words in the title T and, words in titleT are created from the ?information source?
S byfirst aligning every title word with a differentword in the ?information source?
S and thengenerating every title word tw from its aligneddocument word dw according to the probabilitydistribution P(tw|dw), Equation (5) can besimplified as?
??
??
?Ttw DdwDdwgdwtwPTtwPTPDTP ),()|(})({)()|(  (7)Equation (7) is the center of the newprobabilistic model for title generation.
Thereare three components in Equation (7).
They areword importance function g(dw,D), title-word-document-word translation probability P(tw|dw)and the word ordering component P(T)/P({tw?T}).
A schematic diagram in Figure 2shows how a title is generated from a documentin the new model through the three components.As shown in Figure 2, a sampling process basedon the word importance function g(dw,D) willbe applied to the original document to generatethe ?information source?
set containing mostcontent words.
Then, a set of title words will bescored according to probability P(tw|dw?)
basedon the words dw?
selected by the ?informationsource?.
Finally, the word ordering process isapplied to the chosen title words tw usingP(T)/P({tw?T}).1.5 Estimation of ComponentsTo implement the new model for titlegeneration, we need to know how to estimateeach of the three components.?
The word importance function g(dw,D).
Ininformation retrieval, normalized tf.idf value hasbeen used as the measurement of the importanceof a term to a document (Salton & Buckley,1988).
Therefore, we can adapt normalized tf.idfvalue as the word importance function g(dw,D).Therefore, function g(dw,D) can be written as?= dw dwidfDdwtfdwidfDdwtfDdwg )(),(/)(),(),(  (8)?
The title-word-document-word ?translation?probability P(tw|dw).
The title-word-document-word ?translation?
probability can be estimatedusing statistical translation model.
Similar to thework of Kennedy and Hauptmann (2000), wecan treat a document and its title as a?translation?
pair with the document as in?verbose?
language and the title as in ?concise?language.
Therefore, title-word-document-word?translation?
probability P(tw|dw) can be learnedfrom the training corpus using statisticaltranslation model (Brown et al, 1990).?
Word ordering component P(T)/P({tw?T}).There are two terms in this component, namelyP(T) and P({tw?T}).
As already used by the oldframework for title generation, P(T) can beestimated using a ngram statistical languagemodel (Clarkson & Rosenfeld, 1997).
The termP({tw?T}), by assuming the independencebetween words tw, can be written as the productof the occurring probability of each tw in T, i.e.?
???
Ttw twPTtwP )(})({ .With the expressions for g(dw,D) andP({tw?T}) substituted into Equation (6), wehave the final expression for our model, i.e???????????????????????????????????
????
??
?Ttw DdwTtwTDdwdwidfDdwtfdwtwPtwPdwidfDdwtfTPDTP)(),()|()()(),()()|( ||(9)2 EvaluationIn this experiment, we introduce twodifferent of evaluations, i.e.
a F1 metric forautomatic evaluation and human judgmentsto evaluate the quality of machine-generatedtitles.F1 metric is a common evaluation metricthat has been widely used in informationretrieval and automatic text summarization.Witbrock and Mittal (1999) used the F1measurement (Rjiesbergen, 1979) as theirperformance metric.
For an automaticallygenerated title Tauto, F1 is measured againstthe correspondent human assigned titleThuman as follows:recallprecisionrecallprecision2F1+?
?=  (10)Here, precision and recall is measured as thenumber of identical words shared by titleTauto and Thuman over the number of words intitle Tauto and the number of words in titleThuman respectively.Unfortunately, this metric ignores syntax andhuman readability.
In this paper, we also askedpeople to judge the quality of machine-generatedtitles.
There are five different quality categories,namely ?very good?, ?good?, ?ok?, ?bad?,?extremely bad?.
A simple score scheme isdeveloped with score 5 for the category ?verygood?, score 4 for ?good?, score 3 for ?ok?, score2 for ?bad?
and score 1 for ?extremely bad?.
Theaverage score of human judgment is used asanother evaluation metric.3 Experiment3.1 Experiment DesignThe experimental dataset comes from a CD of1997 broadcast news transcriptions published byPrimary Source Media [PrimarySourceMedia,1997].
There were a total of 50,000 documentsand corresponding titles in the dataset.
Thetraining dataset was formed by randomlypicking four documents-title pairs from everyfive pairs in the original dataset.
Thus, the sizeof training corpus was 40,000 documents withcorresponding titles.
Only 1000 documentsrandomly selected from the remaining 10,000documents are used as test collection because ofcomputation expensiveness of applyinglanguage model to sequentialize the title words.To see the effectiveness of our new model fortitle generation, we implemented the frameworkproposed by Witbrock and Mittal (1999) andconducted a contrastive experiment.
The lengthof generated titles was fixed to be 6 for bothmethods and all the stop words in the title areremoved.3.2 Examples of Machine-GeneratedTitlesTable 1 and 2 give 5 examples of the titlesgenerated by the old framework and the newprobabilistic model, respectively.
The true titlesare also listed in Table 1 and 2 for the purpose ofcomparison.As shown in Table 1, one common problem withthis set of machine-generated titles is thatcommon title words are highly favoured.
Forexample, the phrase ?president clinton?
is acommon title phrase and appears in 3 out of 5titles and frequently is not necessary.
As alreadydiscussed in previous sections, the problem ofover-favouring common title words in the oldframework can be attributed to the use of termP(T) for the title word ordering phase.
The otherproblem with the set of generated titles in Table1 is that, sometimes machine-generated titlescontain words that have nothing to do with thecontent of the document.
For example, the thirdmachine-generated title in Table 1 is ?presidentclinton budget tax tobacco settlement?
while theoriginal corresponding title is ?senate funds fightagainst underage smoking?.
By the comparisonof the two titles, we can see that the word?budget?
has little to do with the content of thestory and shouldn?t be selected as title words.We think this problem is due to the fact that inthe old framework for title generation, all thewords in the document have an equal chance tovote for their favourite title words and the votesof common words in the document can causeunrelated title words to be selected.Table 1: Examples of titles generated by the oldframework.
Stopwords are removedOriginal Titles Machine-generated Titlesbill lann lee president clinton affirmative actionsupreme courtresearchers say stress cancause heart diseasestress heart disease medical newsdaysenate funds fight againstunderage smokingpresident clinton budget taxtobacco settlementreaction to john f.kennedy jr. speaking outabout his familyjoe kennedy family reactionentertainment newsclinton?s fast track questand other storiesvice president clinton gorecampaign fundraisingAs shown in Table 2, the titles generated by thenew model appear to be more relevant to thecontent of the document by comparison to theoriginal titles.
Furthermore, the titles in Table 2appear to ?smoother?
than the titles listed inTable 1 and don?t have unnecessary commonwords in titles.
We believe it is due to the effectsof both modified process for the title wordordering and dual noisy channel model.
Byreplacing term P(T)/P({tw?T}) with term P(T),we make the title word selection phaseconcentrate on finding the correct word orderand therefore avoid the problem of overlyfavouring common title words.
With theintroduction of the hidden state ?informationsource?, the title words will be selected based onthe sampled important content words andtherefore the noise introduced by commonwords in the document is reduced dramatically.Table 2: Examples of titles generated by newprobabilistic model.
Stopwords are removedOriginal Titles Machine-generated Titlesbill lann lee civil rights nominee bill lann leeresearchers say stress cancause heart diseasestudy links everyday stress heartdiseasesenate funds fight againstunderage smokingcompanies settlement tobaccodeal tax lawsreaction to john f. kennedyjr.
speaking out about hisfamilygeorge magazine discusses joekennedy familyclinton?s fast track questand other storiessenate vote fast track tradeauthority3.3 Results and DiscussionsThe F1 score of each method is computed basedon the comparison of the 1000 generated titles totheir original titles using Equation (10).
Tocollect human judgments for machine-generatedtitles, we randomly chose 100 documents out ofthe 1000 test documents and sent the machine-generated titles by both methods to the assessorfor the quality judgment.
The F1 scores and theaverage scores of human judgments for the oldframework and the new probabilistic model arelisted in Table 3.Table 3: Evaluation results of the old frameworkand the new probabilistic modelF1 Human Judg.Old model 0.21 2.09New model 0.26 3.07As seen from Table 1, the F1 score for the newprobabilistic model is better than the score forthe old model with 0.26 for the new model and0.21 for the old model.
Since the F1 metricbasically measures the word overlappingbetween machine-generated titles and theoriginal titles, the fact that the new model isbetter than the old model in terms of F1 metricindicates that the new model does a better jobthan the old model in terms of finding titlewords appropriate for documents.
Moreimportant, in terms of human judgments, thenew model also outperforms the old modelsignificantly, which implies that titles generatedby the new model is more readable than the titlesgenerated by the old model.
Based on these twoobservations, we can conclude that the newprobabilistic model for title generation iseffective in generating human readable titles.ConclusionIn this paper, we propose a new probabilisticmodel for title generation.
The advantages of thenew model over the old framework are on themodification of the title word ordering phase andthe introduction of the hidden state ?informationsource?.
In the contrastive experiment, the newmodel outperforms the old model significantlyin terms of both the automatic evaluation metricand the human judgments of the qualities of thegenerated titles.
Therefore, we conclude that ournew probabilistic model is effective in creatinghuman readable titles.AcknowledgementsThe authors are grateful to the anonymousreviewers for their comments, which havehelped improve the quality of the paper.
Thismaterial is based in part on work supported byNational Science Foundation under CooperativeAgreement No.
IRI-9817496.
Partial support forthis work was provided by the National ScienceFoundation's National Science, Mathematics,Engineering, and Technology Education DigitalLibrary Program under grant DUE-0085834.This work was also supported in part by theAdvanced Research and Development Activity(ARDA) under contract number MDA908-00-C-0037.
Any opinions, findings, and conclusionsor recommendations expressed in this materialare those of the authors and do not necessarilyreflect the views of the National ScienceFoundation or ARDA.ReferencesI.
Mani and M. T. Maybury (1999) Advances inAutomatic Text.
MIT press, pp 51?53.M.
Witbrock and V. Mittal (1999) Ultra-Summarization: A Statistical Approach toGenerating Highly Condensed Non-ExtractiveSummaries, Proceedings of SIGIR 99, Berkeley,CAR.
Jin and A. G. Hauptmann (2001) Learn to SelectGood Title Word: A New Approach based onReverse Information Retrieval, ICML 2001.P.
Kennedy and A. G. Hauptmann (2000) AutomaticTitle Generation for the Informedia MultimediaDigital Library, ACM Digital Libraries, DL-2000,San Antonio TexasP.
R. Clarkson and R. Rosenfeld (1997) StatisticalLanguage Modeling Using the CMU-CambridgeToolkit.
Proceedings ESCA Eurospeech.G.
Salton and C. Buckeley (1988) Term-weightingapproaches in automatic text retrieval.
InformationProcessing and Management, 24, 513?523.P.
Brown, S. Cocke, S. Della Pietra, Della Pietra, F.Jelinek, J. Lafferty, R. Mercer, and Roossin (1990)A Statistical Approach to Machine Translation.Computational Linguistics V. 16, No.
2.V.
Rjiesbergen (1979) Information Retrieval.
Chapter7.
Butterworths, London.
