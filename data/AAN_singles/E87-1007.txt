HOW TO DETECT GRAMMATICAL ERRORS IN A TEXT WITHOUT PARSING ITEric Steven AtwellArtificial Intelligence GroupDepartment of Computer StudiesLeeds University, Leeds LS2 9JT, U.K.(EARN/BITNET: eric%leeds.ai@ac.uk)ABSTRACTThe Constituent Likelihood Automatic Word-taggingSystem (CLAWS) was originally designed for the low-levelgrammatical analysis of the million-word LOB Corpus ofEnglish text samples.
CLAWS does not attempt a full parse,but uses a firat-order Markov model of language to assignword-class labels to words.
CLAWS can be modified todetect grammatical errors, essentially by flagging unlikelyword-class transitions in the input text.
This may seem to bean intuitively implausible and theoretically inadequate modelof natural language syntax, but nevertheless it cansuccessfully pinpoint most grammatical errors in a text.Several modifications to CLAWS have been explored.
Theresulting system cannot detect all errors in typed documents;but then neither do far more complex systems, which attempta full parse, requiting much greater computation.Checking Grammar in TextsA number of ~rcbers  have experimented with ways tocope with grammatically ill-formed English input (forexample, \[Carboneil and Hayes 83\], \[Charniak 83\], \[Granger83\], \[Hayes and Mouradian 81\], \[Heidorn et al82\], \[Jensenet al83\], \[Kwasny and Sondheimer 81\], \[Weischedel andBlack 80\], \[Weischedel and Sondheimer 83\]).
However, themajority of these systems are designed for Natural Languageinterfaces to software systems, and so can assume arestricted vocabulary and syntax; for example, the systemdiscussed by \[Fass 83\] had a vocabulary of less than 50words.
This may be justifiable for a NL front-end to acomputer system such as a Database Query system, sinceeven an artificial subset of English may be more acceptableto users than a formal command or query language.However, for automated text-checking in Word Processing,we cannot reasonably ask the WP user to restrict theirEnglish text in this way.
This means that WP text-checkingsystems must be extremely robust, capable of analysing avery wide range of lexical and syntactic constructs.Otherwise, the grammar checker is liable to flag manyconstructs which are in fact acceptable to humans, buthappen not to be included in the system's limited grammar.A system which not only performs yntactic analysis of text,but also pinpoints grammatical errors, must be assessedalong two orthogonal scales rather than a single 'accuracy'measure:RECALL -"number of words/constructs correctly flagged as errors""total number of 'true' errors that should be flagged"PRECISION ="number of words/constructs correctly flagged as errors""total number of wordslconstructs flagged by the system"It is easy to optimise one of these performance measuresat the expense of the other, flagging (nearly) ALL words in atext will guarantee optimal recall (i.e.
(nearly) all actualerrors will be flagged) but at a low precision; andconversely, reducing the number of words flagged to nearlyzero should raise the precision but lower the recall.
Theproblem is to balance this trade-off to arrive at recall ANDprecision levels acceptable to WP users.
A system whichcan accept a limited subset of English (and reject (or flag aserroneous) anything else) may have a reasonable recall rate;that is, most of the 'true' errors will probably be included inthe rejected text.
However, the precision rate is liable to beunacceptable to the WP user:, large amounts of the input textwill effectively be marked as potentially erroneous, with noindication of where' within this text the actual errors lie.
Oneway to deal with this problem is to increase the size andpower of the parser and underlying rammar to deal withsomething nearer the whole gamut of English syntax; this isthe approach taken by IBM's EPISTLE project (see \[Heidornet al82\], \[Jensen et al83\]).
Unfortunately, this can lead to avery large and computationally expensive system: \[Heidornet al82\] reported that the EPISTLE system required a 4Mbvirtual machine (although a more efficient implementationunder development should require less memory).The UNIX Writer's Workbench collection of programs(see \[Cherry and Macdonald 83\], \[Cherry et ai 83\]) isprobably the most widely-used system for WP text-checking(and also one of the most widely-used NLP systems overall -see \[AtweU 86\], \[Hubert 85\]).
This system includes anumber of separate programs to check for different types offaults, including misspellings, cliches, and cee, ain stylisticinfelicities such as overly long (or short) sentences.However, it lacks a general-purpose grammar checker, thenearest program is a tool to filter out doubled words (as in "Isigned the the contract").
Although there is a programPARTS which assigns a part of speech tag to each word inthe text (as a precursor to the stylistic analysis programs),this program uses a set of localized heuristic rules todisambiguate words according to context; and these roles arebased on the underlying assumption that the input sentencesare grammatically well-formed.
So, there is no clear way tomodify PARTS to flag grammatical errors, unless weintroduce a radically different mechanism for disambiguatingword*tags according to contexu38LOB and CLAWSOne such alternative word-tag disambiguation mechanismwas developed for the analysis of the Lancaster-Oslo/Bergen(LOB) Corpus.
The LOB Corpus is a million-wordcollection of English text samples, used for experimentationand inspiration in computational linguistics and relatedstudies (see for example \[Leech et al83a\], \[Atwellforthcoming b\]).
CLAWS, the Constituent-LikelihoodAutomatic Word-tagging System (\[Leech et al83b\], \[Atwellet al84\]), was developed to annotate the raw text with basicgranmlatical information, to make it more useful forlinguistic research; CLAWS did not attempt a full parse ofeach sentence, but simply marked each word with agrammatical code from a set of 133 WORDTAGS.
Theword-tagged LOB Corpus is now available to otherresearchers ( ee \[Johansson etai 86\]).CLAWS was originally implemented in Pascal, but it iscurrently being recoded in C and in POPLOG Prolog.CLAWS can deal with Unrestricted English text inputincluding "noisy" or ill-formed sentences, because it is basedon Constituent Likelihood Grammar, a novel probabilisticapproach to grammatical description and analysis describedin \[Atwell 83\].
A Constituent Likelihood Grammar is usedto calculate likelihoods for competing putative analysis; notonly does this tell us which is the 'best' analysis, but it alsoshows how 'good' this analysis is.
For assigning word-tagsto words, a simple Markovian model can be used instead ofa probabilistic rewrite-role system (such as a prohabilisticcontext-free grammar); this greatly simplifies processing.CLAWS first uses a dictionary, sufflxlist and other defaultroutines to assign a set of putative tags to each word; then,for each sequence of ambiguously-tagged words, thelikelihood of every possible combination or 'chain' of tags isevaluated, and the best chain is chosen.
The likelihood ofeach chain of tags is evaluated as a product of all the 'links'(tag-pair-likelihoods) in the sequence; tag-pair likelihood is afunction of the frequency of that sequence of two tags in asample of tagged text, compared to the frequency of each ofthe two tags individually.An important advantage of this simple Markovian modelis that word-tagging is done without parsing: there is noneed to work out higher-level constituent-structure tr esbefore assigning unambiguous word-tags to words.
Despiteits simplicity, this technique is surprisingly robust andsuccessful: CLAWS has been used to analyse a wide varietyof Unrestricted English, including extracts form newspapers,novels, diaries, learned journals, E.E.C.
regulations, etc., witha consistent accuracy of c96%.
Although the system did nothave parse trees available in deciding word-classes, onlycA% of words in the LOB Corpus had to have their assignedwordtag corrected by manual editing (see \[Atwell 81, 82\]).Another important advantage of the simple Markovianmodel is that it is relatively straightforward to transfer themodel from English to other Natural Languages.
The basicstatistical model remains, only the dictionary and Markoviantag-pair frequency table need to be replaced.
We areexperimenting with the possibility of (partially) automatingeven this process - see \[Atweli 86a, 86b, forthcoming c\],\[Atwell and Drakos 87\].The general Constituent Likelihood approach togrammatical nalysis, and CLAWS in particular, can be usedto analyse text including ill-formed syntax.
Moreimportantly, it can also be adapted to flag syntactic errors intexts; unlike other techniques for error-detection, thesemodifications of CLAWS lead to only limited increases inprocessing requirements.
In fact, various different ypes ofmodification are possible, yielding varying degrees ofsuccess in error-detection.
Several different echniques havebeen explored.Error LikelihoodsA very simple adaptation of CLAWS (simple in theory atleast) is to augment the tag*pair frequency table with a tag-pair error  l ikel ihood table.
As in the original system,CLAWS uses the tag-pair frequency table and theConstituent Likelihood formulae to find the best word-tag foreach word.
Having found the best tag for each word, everycooccurring pair of tags in the analysis is re-assessed: theERRO~_-LIKELIHOOD of each tag-pair is checked.
Error-likelihood is a measure of how frequently a given tag-pairoccurs in an error as compared to how frequently it occurs invalid text.
For example, if the user types... my fa r ther  was  ...CLAWS will yield the word-tag analysis... PP$  RBR BEDZ ...which means <possessive personal pronoun>,<comparative adverb>, <past singular BE>.
This analysis isthen passed to the checking module, which uses tag-pairfrequency statistics extracted from copious samples of error-full texts.
These should show that tag-pairs <PP$ RBR> and<RBR BEDZ> often occur where there is a typing error, andrarely occur in grammatically correct constructs; o an errorcan be flagged at the corresponding point in the text.Although the adjustment to the model is theoreticallysimple, the tag-pair error likelihood frequency figuresrequired could only be gleaned by human analysis of hugeamounts of error-full text.
Our initial efforts to collect anEr ror  Corpus  convinced us that this approach wasimpractical because of the time and effort required to collectthe necessary data.
In any case, an alternative techniquewhich managed without a separate table of tag-pair errorlikelihoods turns out to be quite successful.Low Absolute LikelihoodsThis alternative technique involved using CLAWSunmodified to choose the best tag for each word, as before,and then measuring ABSOLUTE LIKELIHOODS of tag-pairs.
Instead of a separate tag-pair error likelihood table toassess the grammaticality, the same tag-pair frequency tableis used for tag-assignment and error-detection.
The tag-pairfrequency table gives frequencies for grammatically well-formed text, so the second module simply assumes that if alow-likelihood tag pair occurs in the input text, it indicates agrammatical error.
In the example above, tag-pairs <PP$RBR> and <RBR BEDZ> have low likelihoods (as theyoccur only rarely in grammatically well-formed text), so anerror can be diagnosed.Figure 1 is a fuller example of this approach to errordiagnosis.
This shows the analysis of a short text; pleasenote that the text was constructed for illustration purposes39only, and the characters mentioned bear no resemblance toreal living people!
The text contains many mis-typed words,but these mistakes would not be detected by a conventionalspelling-checker, since the error-forms happen to coincidewith other legal English words; the only way that theseerrors can be detected is by noticing that the resultantphrases and clauses are ungrammatical.
The granunar-checking program first divides the input text into words.Note that this is not entirely trivial: for example, encliticssuch as I'll, won't are split into two words I ?
'II, will+ n't.
The left-hand column in Figure I shows the sequenceof words in the sample text, one word per line.
The secondcolumn shows the grammatical tag chosen using theConstituent Likelihood model as best in the given context.The third column shows the absolute likelihood of thechosen grammatical tag; this likelihood is normalised relativeto a threshold, so that values greater than one constitute"acceptable" grammatical nalyses, whereas values less thanone am indicative of unacceptably improbable grammar.Whenever the absolute likelihood value falls below thisacceptability threshold, the flag ERROR?
is output in thefourth column, to draw visual attention to the putative rror.Thus, for example, the first word in the text, my, is taggedPP$ (possessive personal pronoun), and this tag has anormalisad absolute likelihood of over 15, which isacceptable; the second word, farther, is tagged RBR(comparative adverb), but this time the absolute likelihood isbelow one (0.264271), so the word is flagged as a putativeERROR?This technique is extremely primitive, yet appears towork fairly well.
There is no longer any need to gathererror-likelihoods from an Error Corpus.
However, thedefinition of what constitutes a "low" likelihood is notstraightforward.
On the whole, there is a reasonably clearcorrelation between words marked ERROR?
and actualmistakes, so clearly low values can be taken as diagnostic oferrors, once the question of what constitutes "lowness" hasbeen defined rigorously.
In the example, the acceptabilitylevel is defined in terms of a simple threshold: likelihoodsare normalised so values below 1.000000 are deemed toolow to be acceptable.
The appropriate normalisetion scalingfactor was found empirically.
Unfortunately, a threshold atthis level would mean some minor troughs would not beflagged, e.g.
clever in I stole a meat clever .... (which wastagged JJ (adjective) but should have been the noun cleaver )has a normalised likelihood of 4.516465; tame in thegruesome tame ofEroc Attwell... (which was also tagged JJ(adjective) but should have been the noun tale ) also has anormalised likelihood of 4.516465; and the phrase won day(which should have been one day ) involves a normalisedlikelihood of 4.060886 (although this is, strictly speaking,associated with day rather than won, an error flag would besufficiently close to the actual error to draw the user'sattention to it).
However, if we raised the threshold (oralternatively changed the normalisation function so that thesenormalised likelihoods are below 1.000000), then morewords would be flagged, lowering the precision of errordiagnosis.
In some cases, error diagnosis would be"blurred", since sometime-'~ words immediately before and/orafter the error also have low likelihoods; for example, was inmy farther vms very crawl.., has a likelihood of 1.216545.Worse, some error flags would appear in completelyinappropriate places, with no true errors in the immediatecontext; for example, the exclamation mark at the end of hewon't get away with this!
has a likelihood of 4.185351 andso would probably be flagged as an error if the thresholdwere raised.Mother way to define a trough would be as a localminimum, that is, a point on where points immediatelybefore and after have higher likelihood values, even a troughwith a quite high value is flagged this way so long assurrounding points are even higher.
This would catch clever,tame and won day mentioned above.
However, strictlyspeaking several other words not currently flagged in Figure1 are also local minima, for example my in perhaps myfriends would ... and ~ in he ba/d at me /f \[ ...
So, thisdefinition is liable to cause a greater number of 'red herring'valid words to be erroneously flagged as putative mistakes,again leading to a worse precision.Once an optimal threshold or other computationaldefinition of low likelihood has been chosen, it is a simplematter to amend the output routine to produce output in asimplified format acceptable to Word Processor users,without grammatical tags or likelihood ratings but withputative errors flagged.
However, even with an optimalmeasure of lowness, the success rate is unlikely to beperfect.
The model deliberately incorporates onlyrudimentary knowledge about English: a lexicon of wordsand their wordtags, and a tag-pair frequency matrixembodying knowledge of tag cooccurrence likelihoods.Certain types of error are unlikely to be detected withoutsome further knowledge.
One limited augmentation to thissimple model involves the addition of error tags to theanalysis procedure.Error-TagsA rather more sophisticated technique for taking syntacticcontext into account involves adding ERROR-TAGS tolexical entries.
These are the tags of any similar words(where these are different from the word's own tags).
In theanalysis phase, the system must then choose the best tag(from error-teg(s) and 'own' tag(s)) according to syntacticcontext, still using the unmodified CLAWS Constituent-Likelihood model.
For example, in the sentence l am veryhit.
an error can be diagnosed if the system works out thatthe tags of input word hit ( NN, VB, VBD, and VBN -<singular cormnon noun>, <verb infinitive>, <verb pasttense>, <verb past participle>) are all much less likely in thegiven context han J3 (<adjective>), known to be the tag of asimilar word ( hot ).
So, a rather more soph/sticated rror-detection system includes knowledge not just about tags ofwords, but also about what alternative word-classes would beplausible if the input was an error.
This information consistsin an additional field in lexicon entries: each dictionary entrymust hold (i) the word itself, (ii) the word's own tags, and(iii) the error-tags associated with the word.
For example:WORD TAG(S) ERROR-TAG(S).
.
?form NN IN# RI#.
?
?hit NN VB VBD VBN JJ#o.
?prophecy NN VB#.
.o40Note that error-tags are marked with # to distinguishthem from own tags.
CLAWS then chooses the best tag foreach word as usual.
However, in the final output, instead ofeach word being marked with the chosen word-tag, wordsassociated with an ERROR TAG are flagged as potentialerrors.To illustrate why error-tags might help in error diagnosis,notice that dense in I maid several dense in h/s ... does nothave a below-threshold absolute likelihood, and so is notflagged as a putative rror.
An error-tag based system couldcalculate that the best sequence of tags (allowing error-tags)for the word sequence several dense in his ... is \[AP NNS~IN PP$\] (<post-determiner>, <plural common noun>,<preposition>, <possessive personal pronoun>).
Since NNSis an error-tag, an error is flagged.
However, the simplerabsolute likelihood based model does not allow for theoption of choosing NNS as the tag for dense, and is forcedto choose the best of the 'own' tags; this in turn causes amistagging of /n as NNU (<abbreviated unit ofmeasurement>, since \[JJ NNU\] (<adjective> <abbreviatedunit of measurement>) is likelier than \[JJ IN\] (<adjective><preposition>).
Furthermore, \[JJ NNU\] turns out not to bean exceptionally unusual tag cooccurrence.
The point of allthis is that, without error-tags, the the system may mistagwords immediately before or after error-words, and thismistagging may well distort he absolute likelihoods used forerror diagnosis.This error-tag-based technique was originally proposedand illustrated in \[Atwell 83\].
The method has been testedwith a small test lexicon, but we have yet to build acomplete dictionary with error-tags for all words.
Addingerror tags to a large lexicon is a non-trivial research task;and adding error-tags to the analysis stage increasescomputation, since there are more tags to choose between foreach word.
So far, we have not found conclusive videncethat the success rate is increased significantly; this requiresfurther investigation.
Also to be more fully investigated ishow to take account of other relevant factors in errordiagnosis, in addition to error-tags.Full CohortsIn theory at least, the Constituent-Likelihood methodcould be generslised to take account of all relevantcontextual factors, not just syntactic bonding.
This could bedone by generating COHORTS for each input word, andthen choosing the cohort-member word which fits the contextbest.
For example, if the sentence you were very hit wereinput, the following cohorts would be generated:you yew ewewere where wearvery vary veeryhit hot hut hat(the term "cohort" is adapted from \[Marslen-Wilson 85\]with a slight modification of meaning).
Cohorts of similarwords can be discovered from the spelling-check dictionaryusing the same algorithm employed to suggest correctionsfor misspellings in current systems; these techniques arefairly well-understood (see, for example, \[Yannakoudekisand Fawthrop\], \[Veronis 87\], \[Borland 85\]).
Next, eachmember of a cohort is assigned a relative likelihood rating,taking into account relevant factors including:i) the degree of similarity to the word actually typed (thismeasure would be available anyway, as it has to becalculated uring cohort generation; the actual word typedgets a similarity factor of 1, and other members of the cohortget appropriate lower weights)ii) the 'degree of fit' in the given syntactic context(measured as the syntactic constituent likelihood bondbetween the tag(s) of each cohort member and the tag(s) ofthe words before and after, using the CLAWS constituentlikelihood formulae);iii) the frequency of usage in general English (commonwords like "you" and "very" get a high weighting factor, rarewords like "ewe", "yew", and "veery" get a much lowerweighting; word relative frequency figures can be gleanedfrom statistical studies of large Corpora, such as \[Hoflandand Johansson 82\], \[Francis and Kucera 82\], \[Carroll et al71\]);iv) if a cohort member occurs in a grammatical idiom orpreferred collocation with surrounding words, then itsrelative weighting is increased (e.g.
in the context "fish and...", ch/ps gets a higher collocation weighting than chops );collocation preferences can also be elicited from studies oflarge corpora using techniques such as those of \[Sinclair etal 70\];v) domain-dependent lexical preferences should ideally betaken into account, for example in an electronics manualcurrent should get a higher domain weighting than currant.All these factors are multiplied (using appropriateweightings) to yield a relative likelihood rating for eachmember of the cohort.
The cohort-member with the highestrating is (probably) the intended word; if the word actuallytylied is different, an error can be diagnosed, andfurthermore a correction can be offered to the user.Unfortunately, although this approach may seem sensiblein theory, in practice it would require a huge R&D effort togather the statistical information needed to drive such asystem, and the resulting model would be computationallycomplex and expensive.
It would be more sensible to try toincorporate only those features which contribute significantlyto increased error-detection, and ignore all other factors.This means we must test the existing error-detection systemextensively, and analyse the failures to try to discover whatadditional knowledge would be useful to the system.Error CorpusThe error-likelihoud and full-cohort techniques wouldappear to give the best error-detection rates, but require vastcomputations to build a general-purpose system from scratch.The error-tag technique also requires a substantial researcheffort to build a large general-purpose lexicon.
A version ofthe Constituent Likelihood Automatic Word-tagging Systemmodified to use the ABSOLUTE LIKELIHOOD method oferror-detection has been more extensively tested; this systemcannot detect all grammatical errors, but appears to be quitesuccessful with certain classes of errors.
To test alternativeprototypes, we are building up an ERROR CORPUS of textscontaining errors.
The LOB Corpus includes many errors41which appeared in the original published texts; these aremarked SIC in the text, and noted in the Manual whichcomes with the Corpus files, \[Johansson et al78\].
Theinitial Error Coqms consisted in these errors, and it is beingadded to from other sources (see Acknowledgements below).The errors in the Error Corpus can be (manually) classifiedaccording to the kind of processing required for detection(the examples below starts with a LOB line referencenumber):A: non-word error-forms, where the error can be foundby simple dictionary-lookup; for example,A21 115 As the news pours in from around the world,beleagared (SIC) Berlin this weekend is a city on a razor'sedge.B: error-forms involving valid English words in aninvalid grammatical context, the kind of en, or the CLAWS-based approach could be expected to dete~ (these may hedue to spelling or typing or grammatical mistakes by thetypist, but this is irrelevant here: the classification isaccording to the type of processing required by the detectionprogram); for exampleE18 121 Unlike an oil refinery one cannot grumble muchabout the fumes, smell and industrial dirt, generally, for littlecomes out of the chimney except possibly invisible gasses.
(SIC)C: error-forms which are valid English words, but in anabnormal grammatical/semantic context, which a CLAWS-type system would not detect, but which could conceivablyhe caught by a very sophisticated parser, for example,breaking 'long-distance' number agreement roles as in.415 170 It is, however, reported that the tariff on textile.
?and cars imported from the Common Market are (SIC) to bereduced by 10 per cent.D: lexicaily and syntactically valid error-forms whichwould require "intelligenf' semantic analysis for detection;for example,P17 189 She did not imagine that he would pay her a visitexcept in Frank's interest, and when she hurried into theroom where her mother was trying in vain to learn thereason of his visit, her first words were of her fiancee.
(SIC)or\[(29 35 He had then sown (SIC) her up with a needle, and,after a time she had come hack to him cured and able tobear more children.Collection and detailed analysis of texts for this ErrorCorpus is still in progress at the time of writing; but oneimportant early impression is that different sources showwidely different distributions of error-classes.
For example,a sample of 150 errors from three different sources showsthe following distribution:i) Published (and hence manually proofread) text:A:52% B:28% C:8% D:12%ii) essays by 11- and 12-year-old children:A:36% B:38% C: 16% D: 10%iii) non-native English speakers:A:4% B:48% C:12% D:36%Because of this great variation, precision and recall ratesare also liable to vary greatly according to text source.
In aproduction version of the system, the 'unusualness' threshold(or other measure) used to decide when to flag putativeerrors will be chosen by the user, so that users can optimiseprecision or recall.
It is not clear how this kind of user-customisation could be built into other WP text-checkingsystems; but it is an obvious side-benefit of a ConstituentLikelihood based system.ConduslousThe figures above indicate that a CLAWS-basedgrammar-checker would be paff.iculady useful to non-nativeEnglish speakers; but even for this class of users, precisionand recall are imperfecL The CLAWS-based system isinadequate on its own, but should properly be used as onetool amongst many; for example as an augmentation to theWriter's Workbench collection of text-critiquing andproofreading programs, or in conjunction with other EnglishLanguage Teaching tools such as a computerised ELTdictionary (such as those discussed by \[Akkerman et al85\]or \[Atwell forthcoming a\].
Other systems for dealing withsyntactically ill-formed English attempt a full grammaticalparse of each input sentence, and in addition require error-recovery routines of varying degrees of sophistication.
Thisinvolves much more processing than the CLAWS-basedsystem; and yet even these systems fall to diagnose all errorsin a text.
Cleady, the Constituent-Likelihood en~r-detectiontechnique is ideally suited to applications where fastprocessing and relatively small computing requirements areof paramount impoff,ance, nd for users who find imperfecterror-detection better than none at all.
I freely admit that thesystem has not yet been comprehensively tested on a widevariety of WP users; as with all AI research systems, a lot afwork still has to be done to engineer a generally-acceptablecommercial product.
We are cun-ently looking for sponsorsand collaborators for this research: anyone interested indeveloping the prototype into a robust system (for example,to be integrated into a WP system) is invited to contact heauthor!ACKNOWLEDGEMENTSThis paper was originally produced in 1986 asDepartment of Computer Studies Research Report no.212,Leeds University.
I gratefully acknowledge the help ofsupervisors, colleagues and friends at the Universities ofLancaster and .Leeds.
The original CLAWS system wasdeveloped by Ian Marshall, Roger Garside, Geoffrey Leechand myself at Lancaster University, for a project funded bythe Social Science Research Council.
Stephen Elliott spent alot of time building up the Error Corpus and testing variantsof the error-detection system, funded by an ICL ResearchAssoeiateship.
Pauline McCrorie and Matthias Wongworked on the POPLOG prolog and C versions of CLAWS.Various other colleagues have also offered advice andencouragement, pa~cularly Geoffrey Sampson, StuartRoberts, Chris Paice, Lita Taylor, Andrew Beale, Susan42Blackwell, and Barbara Booth.REFERENCESAkkerman, Erik, Pieter Masereeuw, and Willem Meijs 1985Designing a computerized lexicon for linguistic purposesRodopi, AmsterdamAtwell, Eric Steven 1981 LOB Corpus Tagging Project:Manual Pre-edit Handbook.
Departments of ComputerStudies and Linguistics, University of LancasterAtwell, Eric Steven 1982 LOB Corpus Tagging Project:Manual Postedit Handbook (A mini-grammar of LOBCorpus English, examining the types of error commonlymade during automatic (computational) analysis of ordinarywritten EnglishJ Departments of Computer Studies andLinguistics, University of LancasterAtweil, Eric Steven 1983 "Constituent-Likelihood Grarnmar"in Newsletter of the International Computer Archive ofModern English (ICAME NEWS) 7: 34-67, Not~vegianComputing Centre for the Humanities, Bergen UniversityAtwell, Eric Steven 1986a Extracting a Natural Languagegrammar from raw text Department of Computer StudiesResearch Report no.208, University of LeedsAtwell, Eric Steven 1986b, "A parsing expert system whichlearns from corpus analysis" in Willem Meijs (ed) CorpusLinguistics and Beyond: Proceedings of the SeventhInternational Conference on English Language Research onCompuleriaed Corpora, Amsterdam, Netherlands Rodopi,AmsterdamAtwell, Eric Steven 1986c "Beyond the micro: advancedsoftware for research and teaching from computer scienceand artificial intelligence" in Leech, Geoffrey and Candlin,Christopher (eds.)
Computers in English language teachingand research: selected papers from the British CouncilSymposium on computers in English language ducation andresearch, Lancaster, England 167-183, LongumnAtwell, Eric Steven (forthcoming a) "A lexical database forEnglish learners and users: the Oxford Advanced Learner'sDictionary" to appear in Proceedings of ICDBHSS87, the1987 International Conference on DataBases in theHumanities and Social Sciences, Montgomery, Alabama,USAAtwell, Eric Steven (forthcoming b) "Transforming a ParsedCorpus into a Corpus Parser", to appear in Proceedings ofthe 1987 ICAME 8th International Conference on EnglishLanguage Research on Computerised Corpora, Helsinki,FinlandAtweli, Eric Steven (forthcoming c) "An Expert System forthe Automatic Discovery of Particles" to appear inProceedings of the 1987 International Conference on theStudy of Particles, Berlin, East GermanyAtwell, Eric Steven, Geoffrey Leech and Roger Garside1984, "Analysis of the LOB Corpus: progress andprospects", in Jan Aarts and Willem Meijs (ed), CorpusLinguistics; Proceedings of the \[CAME Conference on theuse of computer corpora in English Language Research,N~imegen, Netherlands Rodopi.E Atwell and N Drakes, "Pattern Recognition Applied to theAcquisition of a Grammatical Classification System fromUnrestricted English Text" to appear in the Proceedings ofthe Association for Computational Linguistics ThirdEuropean Chapter Conference, 1987 (forthcoming).Bofland International Inc. 1985 Turbo Lightning: Owner'sHandbook Bodand International, Scotts Valley, CaliforniaUSACarbonell, Jaime and Philip Hayes 1983 "Recovery strategiesfor parsing extragrammatical language" in American Journalof Computational Linguistics 9(3-4): 123-146Carroll, John, Peter Davies, and Barry Richman 1971 TheAmerican Heritage word frequency book Houghton Mifflin /American HeritageCharniak.
Eugene 1983 "A parser with something foreveryone" in Margaret King (ed) Parsing Natural LanguageAcademic Press, LondonCherry, L, Fox, M, Frase, L, Gingrich, P, Keenan, S, andMacdonald, N 1983 "Computer aids for text analysis" in BellLaboratories Records, May/June: 10-16Cherry, Lorinda and Macdonald, Nina 1983 "The Writer'sWorkBench software" in BYTE October: 241-248Fass, Dan, and Yorick Wilks 1983 "Preference semantics,ili-formedness, and metaphor" in American Journal ofComputational Linguistics 9(3-4): 178-187Francis, W Nelson, and Henry Kucera 1982 Frequencyanalysis of English usage: lexicon and grammar HoughtonMifflinGranger, Richard 1983 "The NOMAD system: expectation-based detection and correction of en~rs during understandingof syntactically and semanticany ill-formed text" in AmericanJournal of Computational Linguistics 9(3-4): 188-196Hayes, Philip J, and G V Mouradian 1981 "Flexible Parsing"in American Journal of Computational Linguistics 7(4):232-242Heidom, G E, Jensen, K, Miller, L A, Byrd, R J, andChodorow, M S, 1982 "The EPISTLE text-critiquingsystem" in IBM Systems Journal 21(3): 305-326Hofland, Knut and Stig Johansson 1982 Word frequencies inBritish and American English LongmanHubert, Henry 1985 Computers and Composition: anannotated bibliography, English Education 534 ResourceReport, University of British ColumbiaJensen, K, Heidom, G E, Miller, L A, and Ravin, Y 1983"Parse fitting and prose fixing: getting a hold on ill-formedness" in American Journal of ComputationalLinguistics 9(3-4): 147-16043Johansson, Stig, Geoffrey Leech and Helen Geodluck 1978Manual of information to accompany the l.ancaster-OslolBergen Corpus of British English, for use with digitalcomputers Department of English, Oslo UniversityJohansson, Stig, Eric Atwell, Roger Garside, and GeoffreyLeech 1986 The Tagsed LOB Corpus Norwegian ComputingCentre for the Humanities, University of Bergen, Norway.Kwasny, S and Nonman Sondheimer 1981 "Relaxationtechniques for parsing grammatically ill-formed input innatural language understanding systems" in AmericanJournal of Computational Linguistics 7(2): 99-108Leech, Geoffrey, Roger Garside, and Eric Steven Atwell1983a, "Recent developments in the use of computer corporain English language research" in Transactions of thePhilological Society 1983: 23-40.Leech, Geoffrey, Roger Garside, and Eric Steven AtweU1983b, "The Automatic Grammatical Tagging of the LOBCorpus" in Newsletter ofthe International Computer Archiveof Modern English (ICAME NEWS) 7: 13-33, NorwegianComputing Centre for the Humanities, Bergen UniversityMarslen-Wilson, W D 1985 "Aspects of human speechunderstanding" in Fallside, Frank and Woods, William (eds.
)Computer speech processing, Prentice-HallSinclair, John, Jones, S, and Daley, R 1970 English lexicalstudies, Report to OSTI on project C/LP/08; Dept of English,Birmingham UniversityVeronis, Jean 1987 "Correction of phonographic errors innatural language processing" in Oakman, Robert andPantonial, Barbara (eds.)
ICCH87: Proceedings ofthe EighthInternational Conference on Computers and the Humanities.Department of Computer Science, University of SouthCarolinaWeischedel, Ralph, and John Black 1980 "Respondingintelligently to unparsable inputs" in American Journal ofComputational Linguistics 6(2) 97-109Weischedel, Ralph, and Norman Sonc~eimer 1983 "Meta-roles as a basis for processing ill-formed input" in AmericanJournal of Computational Linguistics 9(3-4):161-177Yannakoudakis, E J, and Fawthrop, D 1983 "The rules ofspelling errors" in Information processing and management19(2): 87-9944Figure l. Sample output with low Likelihoods flagged.m7 PP$ 15 .2 f7~farther RBR 0.264271 ERROR?wss BEDZ 1.216545very QL 22.13'7197crawl NN 0.289613 ERROR?103.174992he " PP3A 90897396bsld J J  0..271961 ERROR?at IN 17.2o'7397me PPIO 29.279452Jr CS IL4009051 PPIA 71.313009dud JJ 0.271961 ERROR?arvlthlng PN 0.088.53,5 ERROR?wrong J J  1.682160, , 24.477376and CC 82.096966sometimes RB 29.179920he PP3A 9,.921162would MD 64.525545hot J J  O220232 ERROR?and CC 24.663~0bit NN 20.028340me PP10 0.062'710 ERROR?, , 18.500350until CS 29.8731331 PP1A 71.313009wss BEDZ 95.448591so QL 22.137197week NN 0.289613 ERROR?and CC 42.917870miserable NN 20.028340that CS 18.4392111 PPIA 71_313009wanted VBD 13S~15263to TO 20.4,t526~due J J  0.216826 ERROR?21.911547?redly RB 36?64715, , 48.44~0013won VBD 2&4~13eday NN 4.0EQ686, , 84.114626I PPIA 36.536284decided VBD 135.815263to TO 2&44S266got VBD 0.102690 ERROR?my PP$ 30.396041won VBD 0.099010 ERROR?back RP 2L849187on IN 10.259310him PP30 29.2794.52: : 3.2,42075I PPIA 4.764065' l l  MD 64.525545mike NN 0.123308 ERROR?him PP30 0.062710 ERROR?pay VB 10.708764;; ; 1.396258he PP3A 4.7640~5will ~ 64.525545n't  XNOT 95.159151get VB 0.14.q38 ERROR?away RB 29.196041with IN 38.186770this DT 21.792427!
!
4.1853.51I PPIA 90.897396stole VBD 135.815263?
AT 39.564677meat NN 191.684559clever J J  4.S16465, , 24.477376?
rid CC 82.096986i PPIA 2,5.834909mid  NN 0.0S9657 ERROR?seversl AP 2.085110dense J J  8.725460in NNU 33.948608his PP$ 0.306138 ERROR?hid VBD 0.099010 ERROR?with IN 34.451138it PP3 9.309486I I 11.826017it PP3 62.337141must MD 4&8?S000have HV 43.~3082hurt VB  0.52728'7 ERROR?a AT  45.661755lit VBD 0.037789 ERROR?!
I 22.778418son NN 9.189478the AT1 4.149936gruesome NN 160.254821tame J J  4 ?164~5of  IN 17.237397Erue NN 54.835271Attweli NN 26.2543~appeared VBN 8-8E7370in NNU 4.870130all ARN 0.265393 ERROR?the ATI 3.499841papers NNS 40.46"749070.S42872per hap?
RB 3(,,.56d3 L5my PP$ $.4TM~$friends NNS 44.477694would MD 15.005662learnt VBN 0.237220 ERROR?to TO 34.470793spell NN 0.061Z50 ERROR?my PP$ 0.545207 ERROR?name NN 51.946085correctly J J  4..516465at IN 17.237397last AP 10.850327!
!
3.437432Figure 1.
Sample output with low Likelihoods flagged..45
