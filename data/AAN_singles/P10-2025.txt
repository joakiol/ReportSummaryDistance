Proceedings of the ACL 2010 Conference Short Papers, pages 137?141,Uppsala, Sweden, 11-16 July 2010. c?2010 Association for Computational LinguisticsWord Alignment with Synonym RegularizationHiroyuki Shindo, Akinori Fujino, and Masaaki NagataNTT Communication Science Laboratories, NTT Corp.2-4 Hikaridai Seika-cho Soraku-gun Kyoto 619-0237 Japan{shindo,a.fujino}@cslab.kecl.ntt.co.jpnagata.masaaki@lab.ntt.co.jpAbstractWe present a novel framework for wordalignment that incorporates synonymknowledge collected from monolinguallinguistic resources in a bilingual proba-bilistic model.
Synonym information ishelpful for word alignment because wecan expect a synonym to correspond tothe same word in a different language.We design a generative model for wordalignment that uses synonym informationas a regularization term.
The experimentalresults show that our proposed methodsignificantly improves word alignmentquality.1 IntroductionWord alignment is an essential step in most phraseand syntax based statistical machine translation(SMT).
It is an inference problem of word cor-respondences between different languages givenparallel sentence pairs.
Accurate word alignmentcan induce high quality phrase detection and trans-lation probability, which leads to a significant im-provement in SMT performance.
Many wordalignment approaches based on generative mod-els have been proposed and they learn from bilin-gual sentences in an unsupervised manner (Vo-gel et al, 1996; Och and Ney, 2003; Fraser andMarcu, 2007).One way to improve word alignment qualityis to add linguistic knowledge derived from amonolingual corpus.
This monolingual knowl-edge makes it easier to determine correspondingwords correctly.
For instance, functional wordsin one language tend to correspond to functionalwords in another language (Deng and Gao, 2007),and the syntactic dependency of words in each lan-guage can help the alignment process (Ma et al,2008).
It has been shown that such grammaticalinformation works as a constraint in word align-ment models and improves word alignment qual-ity.A large number of monolingual lexical seman-tic resources such as WordNet (Miller, 1995) havebeen constructed in more than fifty languages(Sagot and Fiser, 2008).
They include word-level relations such as synonyms, hypernyms andhyponyms.
Synonym information is particularlyhelpful for word alignment because we can ex-pect a synonym to correspond to the same wordin a different language.
In this paper, we explore amethod for using synonym information effectivelyto improve word alignment quality.In general, synonym relations are defined interms of word sense, not in terms of word form.
Inother words, synonym relations are usually con-text or domain dependent.
For instance, ?head?and ?chief?
are synonyms in contexts referring toworking environment, while ?head?
and ?forefront?are synonyms in contexts referring to physical po-sitions.
It is difficult, however, to imagine a con-text where ?chief?
and ?forefront?
are synonyms.Therefore, it is easy to imagine that simply replac-ing all occurrences of ?chief?
and ?forefront?
with?head?
do sometimes harm with word alignmentaccuracy, and we have to model either the contextor senses of words.We propose a novel method that incorporatessynonyms from monolingual resources in a bilin-gual word alignment model.
We formulate a syn-onym pair generative model with a topic variableand use this model as a regularization term with abilingual word alignment model.
The topic vari-able in our synonym model is helpful for disam-biguating the meanings of synonyms.
We extendHM-BiTAM, which is a HMM-based word align-ment model with a latent topic, with a novel syn-onym pair generative model.
We applied the pro-posed method to an English-French word align-ment task and successfully improved the word137Figure 1: Graphical model of HM-BiTAMalignment quality.2 Bilingual Word Alignment ModelIn this section, we review a conventional gener-ative word alignment model, HM-BiTAM (Zhaoand Xing, 2008).HM-BiTAM is a bilingual generative modelwith topic z, alignment a and topic weight vec-tor ?
as latent variables.
Topic variables suchas ?science?
or ?economy?
assigned to individualsentences help to disambiguate the meanings ofwords.
HM-BiTAM assumes that the nth bilin-gual sentence pair, (En, Fn), is generated under agiven latent topic zn ?
{1, .
.
.
, k, .
.
.
,K}, whereK is the number of latent topics.
Let N be thenumber of sentence pairs, and In and Jn be thelengths of En and Fn, respectively.
In this frame-work, all of the bilingual sentence pairs {E,F} ={(En, Fn)}Nn=1 are generated as follows.1.
?
?
Dirichlet (?
): sample topic-weight vector2.
For each sentence pair (En, Fn)(a) zn ?
Multinomial (?
): sample the topic(b) en,i:In |zn ?
p (En |zn;?
): sample Englishwords from a monolingual unigram model giventopic zn(c) For each position jn = 1, .
.
.
, Jni.
ajn ?
p (ajn |ajn?1;T ): sample an align-ment link ajn from a first order Markov pro-cessii.
fjn ?
p (fjn |En, ajn , zn;B ): sample atarget word fjn given an aligned sourceword and topicwhere alignment ajn = i denotes source word eiand target word fjn are aligned.
?
is a parame-ter over the topic weight vector ?, ?
= {?k,e} isthe source word probability given the kth topic:p (e |z = k ).
B = {Bf,e,k} represents the wordtranslation probability from e to f under the kthtopic: p (f |e, z = k ).
T ={Ti,i?
}is a state tran-sition probability of a first order Markov process.Fig.
1 shows a graphical model of HM-BiTAM.The total likelihood of bilingual sentence pairs{E,F} can be obtained by marginalizing out la-tent variables z, a and ?,p (F,E; ?)
=?z?ap (F,E, z, a, ?
; ?)
d?, (1)where ?
= {?, ?, T,B} is a parameter set.
Inthis model, we can infer word alignment a by max-imizing the likelihood above.3 Proposed Method3.1 Synonym Pair Generative ModelWe design a generative model for synonym pairs{f, f ?}
in language F , which assumes that thesynonyms are collected from monolingual linguis-tic resources.
We assume that each synonym pair(f, f ?)
is generated independently given the same?sense?
s. Under this assumption, the probabilityof synonym pair (f, f ?)
can be formulated as,p(f, f ?)?
?sp (f |s ) p(f ?
|s)p (s) .
(2)We define a pair (e, k) as a representation ofthe sense s, where e and k are a word in a dif-ferent language E and a latent topic, respectively.It has been shown that a word e in a differentlanguage is an appropriate representation of s insynonym modeling (Bannard and Callison-Burch,2005).
We assume that adding a latent topic k forthe sense is very useful for disambiguating wordmeaning, and thus that (e, k) gives us a good ap-proximation of s. Under this assumption, the syn-onym pair generative model can be defined as fol-lows.p({f, f ?
}; ??)??
(f,f ?
)?e,kp(f |e, k; ??
)p(f ?|e, k; ??
)p(e, k; ??
),(3)where ??
is the parameter set of our model.3.2 Word Alignment with SynonymRegularizationIn this section, we extend the bilingual genera-tive model (HM-BiTAM) with our synonym pairmodel.
Our expectation is that synonym pairs138Figure 2: Graphical model of synonym pair gen-erative processcorrespond to the same word in a different lan-guage, thus they make it easy to infer accurateword alignment.
HM-BiTAM and the synonymmodel share parameters in order to incorporatemonolingual synonym information into the bilin-gual word alignment model.
This can be achievedvia reparameterizing ??
in eq.
3 as,p(f??
?e, k; ??)?
p (f |e, k;B ) , (4)p(e, k; ??)?
p (e |k;? )
p (k;?)
.
(5)Overall, we re-define the synonym pair modelwith the HM-BiTAM parameter set ?,p({f, f ?
}; ?)?
1?k?
?k??
(f,f ?
)?k,e?k?k,eBf,e,kBf ?,e,k.
(6)Fig.
2 shows a graphical model of the synonympair generative process.
We estimate the param-eter values to maximize the likelihood of HM-BiTAM with respect to bilingual sentences andthat of the synonym model with respect to syn-onym pairs collected from monolingual resources.Namely, the parameter estimate, ?
?, is computedas??
= argmax?
{log p(F,E; ?)
+ ?
log p({f, f ?
}; ?
)},(7)where ?
is a regularization weight that shouldbe set for training.
We can expect that the secondterm of eq.
7 to constrain parameter set ?
andavoid overfitting for the bilingual word alignmentmodel.
We resort to the variational EM approach(Bernardo et al, 2003) to infer ??
following HM-BiTAM.
We omit the parameter update equationdue to lack of space.4 Experiments4.1 Experimental SettingFor an empirical evaluation of the proposedmethod, we used a bilingual parallel corpus ofEnglish-French Hansards (Mihalcea and Pedersen,2003).
The corpus consists of over 1 million sen-tence pairs, which include 447 manually word-aligned sentences.
We selected 100 sentence pairsrandomly from the manually word-aligned sen-tences as development data for tuning the regu-larization weight ?, and used the 347 remainingsentence pairs as evaluation data.
We also ran-domly selected 10k, 50k, and 100k sized sentencepairs from the corpus as additional training data.We ran the unsupervised training of our proposedword alignment model on the additional trainingdata and the 347 sentence pairs of the evaluationdata.
Note that manual word alignment of the347 sentence pairs was not used for the unsuper-vised training.
After the unsupervised training, weevaluated the word alignment performance of ourproposed method by comparing the manual wordalignment of the 347 sentence pairs with the pre-diction provided by the trained model.We collected English and French synonym pairsfrom WordNet 2.1 (Miller, 1995) and WOLF 0.1.4(Sagot and Fiser, 2008), respectively.
WOLF is asemantic resource constructed from the PrincetonWordNet and various multilingual resources.
Weselected synonym pairs where both words were in-cluded in the bilingual training set.We compared the word alignment performanceof our model with that of GIZA++ 1.03 1 (Vo-gel et al, 1996; Och and Ney, 2003), and HM-BiTAM (Zhao and Xing, 2008) implemented byus.
GIZA++ is an implementation of IBM-model4 and HMM, and HM-BiTAM corresponds to ?
=0 in eq.
7.
We adopted K = 3 topics, followingthe setting in (Zhao and Xing, 2006).We trained the word alignment in two direc-tions: English to French, and French to English.The alignment results for both directions were re-fined with ?GROW?
heuristics to yield high preci-sion and high recall in accordance with previouswork (Och and Ney, 2003; Zhao and Xing, 2006).We evaluated these results for precision, recall, F-measure and alignment error rate (AER), whichare standard metrics for word alignment accuracy(Och and Ney, 2000).1http://fjoch.com/GIZA++.html13910k Precision Recall F-measure AERGIZA++ standard 0.856 0.718 0.781 0.207with SRH 0.874 0.720 0.789 0.198HM-BiTAM standard 0.869 0.788 0.826 0.169with SRH 0.884 0.790 0.834 0.160Proposed 0.941 0.808 0.870 0.123(a)50k Precision Recall F-measure AERGIZA++ standard 0.905 0.770 0.832 0.156with SRH 0.903 0.759 0.825 0.164HM-BiTAM standard 0.901 0.814 0.855 0.140with SRH 0.899 0.808 0.853 0.145Proposed 0.947 0.824 0.881 0.112(b)100k Precision Recall F-measure AERGIZA++ standard 0.925 0.791 0.853 0.136with SRH 0.934 0.803 0.864 0.126HM-BiTAM standard 0.898 0.851 0.874 0.124with SRH 0.909 0.860 0.879 0.114Proposed 0.927 0.862 0.893 0.103(c)Table 1: Comparison of word alignment accuracy.The best results are indicated in bold type.
Theadditional data set sizes are (a) 10k, (b) 50k, (c)100k.4.2 Results and DiscussionTable 1 shows the word alignment accuracy of thethree methods trained with 10k, 50k, and 100k ad-ditional sentence pairs.
For all settings, our pro-posed method outperformed other conventionalmethods.
This result shows that synonym infor-mation is effective for improving word alignmentquality as we expected.As mentioned in Sections 1 and 3.1, the mainidea of our proposed method is to introduce la-tent topics for modeling synonym pairs, and thento utilize the synonym pair model for the regu-larization of word alignment models.
We expectthe latent topics to be useful for modeling poly-semous words included in synonym pairs and toenable us to incorporate synonym information ef-fectively into word alignment models.
To con-firm the effect of the synonym pair model withlatent topics, we also tested GIZA++ and HM-BiTAM with what we call Synonym ReplacementHeuristics (SRH), where all of the synonym pairsin the bilingual training sentences were simply re-placed with a representative word.
For instance,the words ?sick?
and ?ill?
in the bilingual sentences# vocabularies 10k 50k 100kEnglish standard 8578 16924 22817with SRH 5435 7235 13978French standard 10791 21872 30294with SRH 9737 20077 27970Table 2: The number of vocabularies in the 10k,50k and 100k data sets.were replaced with the word ?sick?.
As shown inTable 2, the number of vocabularies in the Englishand French data sets decreased as a result of em-ploying the SRH.We show the performance of GIZA++ and HM-BiTAM with the SRH in the lines entitled ?withSRH?
in Table 1.
The GIZA++ and HM-BiTAMwith the SRH slightly outperformed the standardGIZA++ and HM-BiTAM for the 10k and 100kdata sets, but underperformed with the 50k dataset.
We assume that the SRH mitigated the over-fitting of these models into low-frequency wordpairs in bilingual sentences, and then improved theword alignment performance.
The SRH regardsall of the different words coupled with the sameword in the synonym pairs as synonyms.
For in-stance, the words ?head?, ?chief?
and ?forefront?
inthe bilingual sentences are replaced with ?chief?,since (?head?, ?chief?)
and (?head?, ?forefront?)
aresynonyms.
Obviously, (?chief?, ?forefront?)
arenot synonyms, which is detrimented to word align-ment.The proposed method consistently outper-formed GIZA++ and HM-BiTAM with the SRHin 10k, 50k and 100k data sets in F-measure.The synonym pair model in our proposed methodcan automatically learn that (?head?, ?chief?)
and(?head?, ?forefront?)
are individual synonyms withdifferent meanings by assigning these pairs to dif-ferent topics.
By sharing latent topics betweenthe synonym pair model and the word alignmentmodel, the synonym information incorporated inthe synonym pair model is used directly for train-ing word alignment model.
The experimental re-sults show that our proposed method was effec-tive in improving the performance of the wordalignment model by using synonym pairs includ-ing such ambiguous synonym words.Finally, we discuss the data set size used for un-supervised training.
As shown in Table 1, usinga large number of additional sentence pairs im-proved the performance of all the models.
In allour experimental settings, all the additional sen-140tence pairs and the evaluation data were selectedfrom the Hansards data set.
These experimentalresults show that a larger number of sentence pairswas more effective in improving word alignmentperformance when the sentence pairs were col-lected from a homogeneous data source.
However,in practice, it might be difficult to collect a largenumber of such homogeneous sentence pairs fora specific target domain and language pair.
Onedirection for future work is to confirm the effectof the proposed method when training the wordalignment model by using a large number of sen-tence pairs collected from various data sources in-cluding many topics for a specific language pair.5 Conclusions and Future WorkWe proposed a novel framework that incorpo-rates synonyms from monolingual linguistic re-sources in a word alignment generative model.This approach utilizes both bilingual and mono-lingual synonym resources effectively for wordalignment.
Our proposed method uses a latenttopic for bilingual sentences and monolingual syn-onym pairs, which is helpful in terms of wordsense disambiguation.
Our proposed method im-proved word alignment quality with both smalland large data sets.
Future work will involve ex-amining the proposed method for different lan-guage pairs such as English-Chinese and English-Japanese and evaluating the impact of our pro-posed method on SMT performance.
We will alsoapply our proposed method to a larger data setsof multiple domains since we can expect a fur-ther improvement in word alignment accuracy ifwe use more bilingual sentences and more mono-lingual knowledge.ReferencesC.
Bannard and C. Callison-Burch.
2005.
Paraphras-ing with bilingual parallel corpora.
In Proceed-ings of the 43rd Annual Meeting on Association forComputational Linguistics, pages 597?604.
Asso-ciation for Computational Linguistics Morristown,NJ, USA.J.
M. Bernardo, M. J. Bayarri, J. O. Berger, A. P.Dawid, D. Heckerman, A. F. M. Smith, and M. West.2003.
The variational bayesian EM algorithm for in-complete data: with application to scoring graphicalmodel structures.
In Bayesian Statistics 7: Proceed-ings of the 7th Valencia International Meeting, June2-6, 2002, page 453.
Oxford University Press, USA.Y.
Deng and Y. Gao.
2007.
Guiding statistical wordalignment models with prior knowledge.
In Pro-ceedings of the 45th Annual Meeting of the As-sociation of Computational Linguistics, pages 1?8,Prague, Czech Republic, June.
Association for Com-putational Linguistics.A.
Fraser and D. Marcu.
2007.
Getting the struc-ture right for word alignment: LEAF.
In Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 51?60, Prague, Czech Republic,June.
Association for Computational Linguistics.Y.
Ma, S. Ozdowska, Y.
Sun, and A.
Way.
2008.Improving word alignment using syntactic depen-dencies.
In Proceedings of the ACL-08: HLT Sec-ond Workshop on Syntax and Structure in Statisti-cal Translation (SSST-2), pages 69?77, Columbus,Ohio, June.
Association for Computational Linguis-tics.R.
Mihalcea and T. Pedersen.
2003.
An evaluationexercise for word alignment.
In Proceedings of theHLT-NAACL 2003 Workshop on building and usingparallel texts: data driven machine translation andbeyond-Volume 3, page 10.
Association for Compu-tational Linguistics.G.
A. Miller.
1995.
WordNet: a lexical database forEnglish.
Communications of the ACM, 38(11):41.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of the 38th AnnualMeeting on Association for Computational Linguis-tics, pages 440?447.
Association for ComputationalLinguistics Morristown, NJ, USA.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.B.
Sagot and D. Fiser.
2008.
Building a free Frenchwordnet from multilingual resources.
In Proceed-ings of Ontolex.S.
Vogel, H. Ney, and C. Tillmann.
1996.
HMM-based word alignment in statistical translation.
InProceedings of the 16th Conference on Computa-tional Linguistics-Volume 2, pages 836?841.
Asso-ciation for Computational Linguistics Morristown,NJ, USA.B.
Zhao and E. P. Xing.
2006.
BiTAM: Bilingualtopic admixture models for word alignment.
In Pro-ceedings of the COLING/ACL on Main ConferencePoster Sessions, page 976.
Association for Compu-tational Linguistics.B.
Zhao and E. P. Xing.
2008.
HM-BiTAM: Bilingualtopic exploration, word alignment, and translation.In Advances in Neural Information Processing Sys-tems 20, pages 1689?1696, Cambridge, MA.
MITPress.141
