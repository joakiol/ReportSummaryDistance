Introduction to the Special Issue onNatural Language GenerationRobert Dale*Macquarie UniversityDonia ScowUniversity of BrightonBarbara Di Eugenio tUniversity of Pittsburgh1.
IntroductionThere are two sides to natural anguage processing.
On the one hand, work in naturallanguage understanding is concerned with the mapping from some surface represen-tation of linguistic material expressed as speech or text--to an underlying repre-sentation of the meaning carried by that surface representation.
But there is also thequestion of how one maps from some underlying representation f meaning into textor speech: this is the domain of natural anguage generation.Whether our end-goal is the construction of artifacts that use natural languagesintelligently, the formal characterization of phenomena in human languages, or thecomputational modeling of the human language processing mechanism, we cannotignore the fact that language is both spoken (or written) and heard (or read).
Both areequally large and important problems, but the literature contains much less work onnatural anguage generation (NLG) than it does on natural anguage understanding(NLU).
There are many reasons why this might be so, although clearly an importantone is that researchers in natural anguage understanding in some sense start out witha more well-defined task: the input is known, and there is a lot of it around.
This is notthe case in natural anguage generation: there, it is the desired output that is known,but the input is an unknown; and while the world is awash with text waiting to beprocessed, there are fewer instances of what we might consider appropriate inputs forthe process of natural anguage generation.
For researchers in the field, this highlightsthe fundamental question that always has to be asked: What do we generate from?Despite this problem, the natural language generation community is a thrivingone, with a research base that has been developing steadily--although perhaps at aslower pace because of the smaller size of the community--for just as long as workin natural anguage understanding.
It should not be forgotten that much of NLP hasits origins in the early work on machine translation in the 1950s; and that to carry outmachine translation, one has to not only analyze existing texts but also to generatenew ones.
The early machine translation experiments, however, did not recognize theproblems that give modern work in NLG its particular character.
The first significantpieces of work in the field appeared uring the 1970s; in particular, Goldman's workon the problem of lexicalizing underlying conceptual material (Goldman 1974) and* School of Mathematics, Physics, Computing and Electronics, Sydney NSW 2109, Australiat Learning Research and Development Center, 3939 O'Hara Street, Pittsburgh, PA 15260, U.S.A.:~ Information Technology Research Institute, Lewes Road, Brighton BN2 4GJ, UK(~) 1998 Association for Computational LinguisticsComputational Linguistics Volume 24, Number 3Davey's work on the generation of paragraph-long descriptions of tic-tac-toe games(Davey 1979) were among the first to focus on issues unique to NLG.
The field reallytook off, however, in the 1980s; for those working in NLG, the decade began with abang, and the Ph.D. theses of McDonald (1980), Appelt (1981), and McKeown (1982)have had a lasting impact on the shape of the field)But what has happened in the last fifteen years since those major pieces of workfirst appeared?
Although one does find articles on NLG in the pages of ComputationalLinguistics and other journals in the field, and papers on generation do appear at themajor NLP conferences, the quantity and range of work being carried out in NLGtends to be underrepresented in these forums.
Instead, the community has tendedto present its results at the two biennial series of workshops--one European and oneinternational--that ve sprung up in the last ten years.
Many of these workshops haveled to books: see Kempen (1987); McDonald and Bolc (1988); Zock and Sabah (1988);Dale, Mellish, and Zock (1990); Paris, Swartout, and Mann (1991); Dale et al (1992);Horacek and Zock (1993); and Adorni and Zock (1996).
This special issue of Computa-tional Linguistics was inspired by discussions at the International Workshop on NaturalLanguage Generation held in Herstmonceux in 1996; the aim of the volume you arereading is to show the wider computational linguistics community something of therange of activities in NLG.2.
Some Perspectives on Natural Language GenerationWhat is natural language generation about?
A definition offered by McDonald (1987,983) over ten years ago has stood the test of time:Natural language generation is the process of deliberately constructing a naturallanguage text in order to meet specified communicative goals.A more recent definition with a slightly different emphasis can be found in Reiter andDale (1997, 57):Natural language generation is the subfield of artificial intelligence andcomputational linguistics that is concerned with the construction of computersystems that can produce understandable texts in.
.
.
human languages fromsome underlying non-linguistic representation f information.Both definitions pick out some of the foci of interest that give work in NLG its distinc-tive flavor.
From the first we note the emphasis on deliberate choice as the fundamentaloperation that underlies much work in the area, and on the generation of texts asopposed to single sentences; from the second, we note the emphasis on underlyingrepresentations of information that may be nonlinguistic in nature.
Each of these pointsbears some elaboration:In work in NLG, a major concern is that of choosing between differentways of doing things, as the same content can often be expressed inmany different ways.
Although some of these choices may indeed bearbitrary, there is a view in NLG that a great many are not, and thechoices between different ways to say things---different ways to structurea text, different ways to refer to objects, the use of different syntactic1 The latter two works are more widely available inrevised form as Appelt (1985) and McKeown (1985).346Dale, Di Eugenio, and Scott Introductionconstructions, and of different words to realize underlyingconcepts--need to be motivated in some way.
Much research in NLG isoriented towards uncovering those motivations.There is a sense in which work in NLU tends to start with the sentenceas the principal focus of inquiry.
However, for much work in NLG, theprimary focus is the text or discourse: although there are many importantissues involved in the generation of sentential forms, those working inNLG research ave long accepted that discourse-level issues are just asimportant, and probably more so.
This relates to the previous point: it isoften only by considering the context within which a sentence is beinggenerated that the appropriate choice of surface form can be made.The input representation provided to an NLG system may be symbolic(for example, an expert system knowledge base) or numeric (forexample, a database containing stock market prices) but it is generallynonlinguistic n nature.
Early work in the field relied on the use ofhand-crafted knowledge sources, which sometimes meant hat therepresentations used embodied unspoken assumptions.
More recentwork has been able to take advantage of representations created for otherpurposes; using these as the input to the generation process reinforcesthe realization that the elements of the underlying representation maynot correspond in a straightforward way to words and sentences.Much work in NLG thus concerns itself with pragmatics and discourse-level consider-ations.
Interestingly, these too have been somewhat underrepresented in the standardcomputational linguistics forums, where the bulk of the work carried out is often inthe area of well-specified and rigorous formal treatments of sentential phenomena.There is an important point here that bears emphasizing: natural language gener-ation is not the inverse of the process of parsing.
2 Those working in NLG generallybreak down the process of generating a text into a number of stages, and it is onlythe last of these--generally referred to as surface realization--that corresponds toany-thing like the inverse of parsing.
If we want to seek the mirror-image ofwork in NLGwithin research in natural anguage understanding, we have to consider the entireanalysis process, all the way through to plan recognition i  multisentential discoursesor dialogues.This is perhaps an appropriate place to review what the task of NLG is nowcommonly seen to involve:First, there is the question of content determination: deciding what tosay.
This impacts at both macro and micro levels.
At the macro levelresearchers in NLG are concerned with how the content of amultisentential text, or of a turn in a dialogue, can be determined.
At amicro level, researchers are concerned with how the content ofappropriate referring expressions can be worked out.
In each case theproblem is how to select he right information from that which isavailable; it is rarely appropriate to say everything we could say.2 It should be noted, however, that here is a body of work that looks at the use of bidirectionalgrammars, where acommon declarative representation of grarmnatical knowledge is used both forparsing and for realization; see, for example, Shieber et al (1990).347Computational Linguistics Volume 24, Number 3There is also the question of text structure: texts are not just randomcollections of sentences; they exhibit a structure that plays a key role inconveying their meaning.
Researchers in NLG are concerned withelucidating mechanisms for determining the most appropriate structuresto use in particular circumstances, and with working out how theinformation to be conveyed can best be packaged into paragraph- andsentence-sized chunks.Closer to the kinds of issues that concern those working in parsing, thereare the problems of surface realization and lexicalization: once thecontent of individual sentences has been determined, this still has to bemapped into morphologically and grammatically well-formed words andsentences.
Where the underlying representation expresses informationalelements at a granularity that does not map easily into words, decisionsabout how to lexicalize the conceptual material have to be taken.These are the kinds of issues that have driven much research in NLG over thelast 15 years.
Our understanding of the issues has come a long way in that time.
Thisissue of Computational Linguistics contains what is no more than a snapshot of work inthe field at the current ime; it should be read against he background of the broaderpicture we have attempted to sketch here, albeit briefly.
In the next section, we provideshort summaries of the papers collected together in this special issue.3.
An Overview of the IssueFrom the 25 papers originally submitted to the special issue, our reviewers helpedus eventually select five.
There were many more papers that, given space, we wouldhave included; we hope that some of these will appear in subsequent regular issuesof Computational Linguistics.3.1 Chu-Carroll and CarberryAs we mentioned earlier, "deciding what to say" is a key issue in NLG.
Chu-Carrolland Carberry's paper focuses on strategies for selecting the content of responses incollaborative planning dialogues.
The authors concentrate on situations in which thesystem and the user have different beliefs that they attempt to reconcile, namely: caseswhen the system needs to gather further information in order to decide whether toaccept a proposal from the user, and cases when the system must negotiate with theuser to resolve a detected conflict in beliefs.
In both cases, the implemented algorithmsidentify the subset of beliefs that the system believes will most effectively help solveeither its uncertainty or the conflict in beliefs; further, the system chooses an appro-priate strategy and produces a response that initiates a subdialogue addressing the:impasse in conversation.Two other points deserve special note.
First, the computational model is based ona small but convincing corpus study.
Second, the authors conducted a formal, evenif limited, evaluation of their prototype implementation.
The evaluation consists ofhuman raters grading the system's actual response and some distractors, obtainedby selectively altering the system's response generation strategies.
The evaluation issuggestive that the proposed strategies and their implementation are effective.348Dale, Di Eugenio, and Scott Introduction3.2 StedeAddressing the issue of "deciding how to say it," Stede focuses on the role of thelexicon and of lexical choice within an NLG system.
More specifically, Stede describeshow his approach can generate verbal alternations that change the aspectual categoryof the verb.
One such alternation is the causative, as in The mechanic drained the oil fromthe engine, the causative form of The oil drained from the engine.
Stede takes the lexicon tobe the central device for mapping between domain representations and intermediatesemantic representations.
Alternations are generated by applying one or more rulesin a predetermined order to a basic lexical form; the choice of a specific alternation isdetermined by parameters such as salience.
The intermediate semantic representationof a sentence is the input for the surface generator--in this case, Penman (Penmangroup 1989).Interesting aspects of Stede's approach are the capacity of his system to generatefine-grained istinctions of meaning, and the attention paid to both linguistic con-straints and computational concerns.
Although only English is discussed in this paper,Stede's ystem uses the same mechanism to generate alternations in German as well.3.3 Mittal, Moore, Carenini, and RothGeneration technology is now increasingly finding a place in applied systems.
Onesuch application is described by Mittal, Moore, Carenini, and Roth, who have devel-oped a system to generate captions to accompany the graphical presentations producedby SAGE (Roth et al 1994).
It is well known that the interpretation of even simple, con-ventional graphics can be difficult without accompanying textual pointers (e.g., keys,labels of axes, and the like).
SAtE is innovative in its ability to produce novel graphicsfor highly abstract and complex data.
The comprehension of these presentations ioften heavily reliant on captions: extended textual descriptions of the relation of thepresentation to the data it depicts.Mittal et al show how a SAGE graphic, together with information about the per-ceptual complexity of its elements and the structure of its underlying data, can be usedto generate an effective multisentential caption.
This is demonstrated through exam-ples in the domain of housing sales; however, with the exception of the lexicon, thecaption generator is fully domain independent.
Although the system has not yet beenformally evaluated, we are told that users of SAGE report that the generated captionscontribute positively to their understanding of complex graphical presentations.3.4 Radev and McKeownAutomated text summarization is a practical problem of increasing interest, especiallywith the ever-widening dissemination of the World Wide Web; this is an obvious areawhere NLG can contribute.
Radev and McKeown describe an application of NLGtechniques towards the end of producing summaries of a kind that are, as the au-thors argue, beyond the scope of current statistical summarizers.
There are two mainelements to their approach.
The first is the use of "surrunarization operators" that com-pare data structures containing information derived from different sources and thusallow the system to produce summaries of several input messages; the second is theuse of a technique for identifying proper names and related descriptions from on-linetext so that these can be used to extend the descriptions provided in summaries.The data structures used as input to generate the summary texts are filled MUC-style templates; the task of identifying key information in source texts and extractingit has already been carried out.
The paper thus provides an excellent application ofestablished technologies, with new mechanisms being developed to complete the pic-349Computational Linguistics Volume 24, Number 3ture; the work shows well how NLG techniques can make a real difference to theimportant ask area of summarization.3.50berlanderTexts are generated to be read, and while generators can provide a range of texts for agiven context, the question of what expression is most appropriate remains nontrivial.Typically nowadays, the designers of NLG systems tune their generators to produceexpressions compatible with those found in a corpus of "good" exemplars from thedomain in question (see, e.g., Scott and Power \[1994\] and Paris et al \[1995\]).
But thisapproach is not always possible (e.g., for novel domains), and even in cases wherethere is an available corpus, judgments of quality can often only be made by appeal-ing to convention.
Psycholinguistic studies suggest hemselves as a useful source ofguidance but this too can be problematic: what speakers or writers typically produceoften conflicts with their preferences as perceivers, as shown, for example, with re-gard to referring expressions by Stevenson and her colleagues (Stevenson, Crawley,and Kleinman 1994; Stevenson and Urbanowicz 1995).Oberlander discusses this apparent paradox with reference to the generation ofreferring expressions--in particular, the suggestion by Dale and Reiter (1995) thatgeneration algorithms for definite noun phrases hould be based on observations abouthuman language production rather than on a strict observation of the Gricean maxims(Grice 1975).
Oberlander calls this the Spike Lee maxim: Do the right thing--where"right" is that which is human and simple.
He shows that, when generating referringexpressions, we can't always tell whether the right thing is to mimic the preferencesof language producers or language perceivers, since these preferences often conflict.He argues that until we develop a more sophisticated view of the expectations ofspeakers and hearers, developers of NLG systems hould probably stick to the SpikeLee maxim: even with its known limitations it produces more natural results than areachieved by following a strict interpretation of the Gricean maxims--and we wouldall agree that even that is better than the Cole Porter maxim.
34.
Future Directions for Research in Natural Language GenerationWe said earlier that NLG research as come a long way since its beginnings, but thereis still a long way to go.
What does the future hold?
Crystal-ball gazing is always arisky business, but on the basis of our experience and some of the issues that ariseboth in the work presented here and in other submissions to the special issue, wewould suggest he following aspects of NLG will be seen as important areas in thenext five years.Microplanning.
Ever since Thompson (1977), there has been a tendency to see NLGas involving two problems, which Thompson characterized as being concerned withdecisions of strategy and tactics: in short, questions about what to say and questionsabout how to say it.
In the field, this translated into work in the two areas of textplanning and linguistic realization, with researchers often declaring themselves asworking on one or the other.
In more recent years, there has been the realization thatsomething is required in the middle; this was most notably expressed in Meteer's workon what she called "the generation gap" (Meteer 1990).
This has given rise to a bodyof work that explores questions of what is often referred to as microplanning: once a3 This being: Anything oes.350Dale, Di Eugenio, and Scott Introductiontext planning process has worked out the overall structure of a text and the contentto be conveyed, how is this information packaged into sentences?
Serious work herehas only just begun: there are a great many unresolved issues, and in many cases thequestions themselves are unclear.Multimodal generation.
Real text is not disembodied.
It always appears in context, andin particular within some medium--for example, on a page, on a screen, or in aspeech stream.
As soon as we begin to consider the generation of text in context,we immediately have to countenance issues of typography and orthography (for thewritten form) and prosody (for the spoken form).
These questions can rarely be dealtwith as afterthoughts.
This is perhaps most obvious in the case of systems that generateboth text and graphics and attempt to combine these in sensible ways.
We predict hatthe World Wide Web will be a major factor in forcing some of the issues here: if systemsare to automatically generate the text on Web pages (see, for example, Milosavljevicand Dale \[1996\]), then they also need to consider other elements of that container.Reusable resources.
It may be an indication of a maturing of some subareas of NLGresearch that we are now in a position where there are reusable components for par-ticular tasks.
Specifically, three linguistic realization packages, FUF/SURGE (Elhadad1993a, 1993b; Elhadad and Robin 1996), PENMAN/NIGEL (Penman group 1989), andits descendant KPML/NIGEL (Bateman 1997), are widely used in the field.
For any-thing other than simple applications, it is now questionable whether it makes sense tobuild a linguistic realization component from scratch.
We may expect other kinds ofreusable components o be developed within the research community within the next5-10 years; it is developments of this kind that signal significant progress, since beingable to reuse the work of others obviously has the potential to increase research pro-ductivity.
In related developments, there is a growing interest within the communityin defining a reference architecture for NLG; if successful, this is likely to stimulatefurther esearch and development in NLG through the provision of a modular baselinefor development, comparison, and evaluation.Evaluation.
Although there have been attempts at the evaluation of NLG techniquesand systems in the past, formal evaluation has only recently come to the fore.
Forexample, systems have been evaluated by using human judges to assess the qualityof the texts produced (Lester and Porter 1997; Chu-Carroll and Carberry, this issue);by comparing the system's performance to that of humans (Yeh and Mellish 1997);by corpus-based evaluation (Robin and McKeown 1996); and indirectly through "taskefficacy" measures (Young 1997).
The major stumbling block for progress is determin-ing what metrics and methods hould be used: for example, how can the quality ofan output text be measured?
Because of the different nature of the task, it is unlikelythat methods that have been used in NLU, such as the evaluation process adoptedin the Message Understanding Conferences, can be carried over to generation.
Daleand Mellish (1998) suggest hat the NLG community could make progress by devis-ing specific evaluation methods for NLG subtasks uch as content determination, textstructuring, and realization; this "glass box" approach is likely to result in a clearerunderstanding of how to evaluate NLG systems as a whole.The particular foci we have just outlined are specific to work in NLG.
However,just as corpus-based methods have become very important in NLU research, we mayexpect his to happen increasingly in work on NLG too.
Raw or coded text has beenused by researchers to investigate strategies in a number of different areas of NLG, asdemonstrated in the papers by Radev and McKeown and by Chu-Carroll and Carberry351Computational Linguistics Volume 24, Number 3in this issue.
Given the emphasis within NLG research on text-level issues, a majorbottleneck for work here is the encoding of corpora with semantic and discoursestructural features; see Di Eugenio, Moore, and Paolucci (1997).
These are needed touncover plausible text-structuring and microplanning strategies, but annotating cor-pora for such features will remain a laborious manual task at least for the foreseeablefuture.
This effort may be alleviated if sharable corpora become available throughthe Discourse Resource Initiative (http://www.georgetown.edu/luperfoy/Discourse-Treebank / dri-home.html).With so many rich seams to mine, natural anguage generation has a promisingfuture.
We mentioned at the outset hat researchers in NLG face the unique problem ofdeciding what to generate from: Yorick Wilks is credited with pointing out that, whilethe problem of natural anguage understanding is somewhat like counting from oneto infinity, researchers in natural anguage generation face the problem of countingfrom infinity to one.
In order to make progress, researchers in NLG pick a reasonablyhigh number and get to work; as researchers in NLU climb the numerical ladder fromthe other end, we can expect hat some of the big numbers discovered in NLG willprove to be of use in NLU too.AcknowledgmentsWe offer our grateful thanks to the body ofreviewers who did so much work inhelping us put this issue together.
We alsoacknowledge the many fruitful discussionswe have had with our colleagues, includingespecially Giuseppe Carenini for sharing hisnotes on evaluation i NLG and EhudReiter for his observations on the state ofthe field.ReferencesAdorni, G. and M. Zock, editors.
1996.Trends in Natural Language Generation.Lecture Notes in Artificial Intelligence.Springer-Verlag, Berlin.Appelt, Douglas E. 1981.
Planning NaturalLanguage Utterances toSatisfy MultipleGoals.
Ph.D. thesis, Stanford University,Stanford, CA.
Available as SRI TechnicalNote 259.Appelt, Douglas E. 1985.
Planning EnglishSentences.
Cambridge University Press,Cambridge.Bateman, John A.
1997.
Enabling technologyfor multilingual natural languagegeneration: The KPML developmentenvironment.
Natural LanguageEngineering, 3:15-55.Dale, Robert, Eduard H. Hov3~ DietmarR6sner, and Oliviero Stock, editors.
1992.Aspects of Automated Natural LanguageGeneration.
Lecture Notes in ArtificialIntelligence.
Springer-Verlag, Berlin.Dale, Robert and Christopher S. Mellish.1998.
Towards evaluation in naturallanguage generation.
In Proceedings oftheFirst International Conference on LanguageResources and Evaluation, Granada, Spain,May 28-30.Dale, Robert, Chris Mellish, and MichaelZock, editors.
1990.
Current Research inNatural Language Generation.
AcademicPress, New York.Dale, Robert and Ehud Reiter.
1995.Computational interpretations of theGricean maxims in the generation ofreferring expressions.
Cognitive Science,18:233-263.Dave~ Anthony C. 1979.
DiscourseProduction.
Edinburgh University Press,Edinburgh.Di Eugenio, Barbara, Johanna D. Moore,and Massimo Paolucci.
1997.
Learningfeatures that predict cue usage.
InProceedings ofthe 35th Annual Meeting,pages 80-87, Madrid, Spain.
Associationfor Computational Linguistics.Elhadad, Michael.
1993a.
FUF: Theuniversal unifier--user manual version5.2.
Technical Report CUCS-038-91,Columbia University.Elhadad, Michael.
1993b.
UsingArgumentation toControl Lexical Choice: AUnification-based Implementation.
Ph.D.thesis, Computer Science Department,Columbia University.Elhadad, Michael and Jacques Robin.
1996.An overview of SURGE: A reusablecomprehensive syntactic realisationcomponent.
In Proceedings ofthe 8thInternational Workshop on Natural LanguageGeneration (Demos and Posters),Herstmonceux, Sussex, UK, June.Goldman, Neil M. 1974.
Computer Generationof Natural Language from a Deep ConceptualBase.
Ph.D. thesis, Stanford University.Dale, Di Eugenio, and Scott IntroductionAvailable as Stanford AI LaboratoryMemo AIM-247 or CS Technical ReportCS-74-461.Grice, H. P. 1975.
Logic and Conversation.In P. Cole and J. L. Morgan, editors,Syntax and Semantics 3:Speech Acts.Academic Press.Horacek, H. and M. Zock, editors.
1993.New Concepts in Natural LanguageGeneration.
Pinter, London.Kempen, Gerard, editor.
1987.
NaturalLanguage Generation: New Results inArtificial Intelligence, Psychology andLinguistics.
NATO ASI Series No.
135.Martinus Nijhoff Publishers, Boston,Dordrecht.Lester, James C. and Bruce W. Porter.
1997.Developing and empirically evaluatingrobust explanation generators: TheKNIGHT experiments.
ComputationalLinguistics, 23(1):65-102.McDonald, David D. 1980.
Natural LanguageProduction as a Process of Decision Makingunder Constraint.
Ph.D. thesis, MIT,Cambridge, MA.McDonald, David D. 1987.
Naturallanguage generation.
In Stuart C. Shapiro,editor, Encyclopedia ofArtificial Intelligence.John Wiley and Sons, pages 642-655.McDonald, David D. and Leonard Bolc.1988.
Natural Language Generation Systems.Springer-Verlag, New York NY.McKeown, Kathleen R. 1982.
GeneratingNatural Language Text in Response toQuestions About Database Structure.
Ph.D.thesis, University of Pennsylvania,Philadelphia, PA, May.
Available asTechnical Report MS-CIS-82-05.McKeown, Kathleen R. 1985.
Text Generation:Using Discourse Strategies and FocusConstraints to Generate Natural LanguageText.
Cambridge University Press,Cambridge.Meteer, Marie.
1990.
The Generation Gap: TheProblem of Expressibility in Text Planning.Ph.D.
thesis, University of Massachusetts.Milosavljevic, Maria and Robert Dale.
1996.Strategies for comparison i  encyclopediadescriptions.
In Proceedings ofthe EighthInternational Natural Language GenerationWorkshop, pages 161-170, Herstmonceux,Sussex, UK, June.Paris, C4cile, Keith Vander Linden, MarkusFischer, Anthony Hartley, Lyn Pemberton,Richard Power, and Donia Scott.
1995.
Asupport ool for writing multilingualinstructions.
In Proceedings ofthe FourteenthInternational Joint Conference on ArtificialIntelligence, pages 1398-1404.Paris, C6cile L., William R. Swartout, andWilliam C. Mann, editors.
1991.
NaturalLanguage Generation i  Artificial Intelligenceand Computational Linguistics.
KluwerAcademic Publishers, Boston.Penman group.
1989.
Documentation f thePenman Sentence Generation System.USC Information Sciences Institute,Marina del Rey, CA.Reiter, Ehud and Robert Dale.
1997.Building applied natural languagegeneration systems.
Natural LanguageEngineering, 3:57-87.Robin, Jacques and Kathleen McKeown.1996.
Empirically designing andevaluating a new revision-based modelfor summary generation.
ArtificialIntelligence, 85:135--179.Roth, Steven F., John Kolojejchick, JoeMattis, and Jade Goldstein.
1994.Interactive graphic design usingautomatic presentation k owledge.
InProceedings ofCHI'94: Human Factors inComputing Systems, pages 193-200.Scott, Donia and Richard Power, editors.1994.
Characteristics of administrativeforms in English, German and Italian.Deliverable EV-1, GIST project LRE062-09.
http: / / ecate.itc.it:1024 / projects /gist / gist-bibliography.html.Shieber, Stuart, Gertjan van Noord,Fernando Pereira, and Robert Moore.1990.
Semantic head-driven generation.Computational Linguistics, 16(1):30-42.Stevenson, Rosemary Rosalind Crawley,and David Kleinman.
1994.
Thematicroles, focus and the representation fevents.
Language and Cognitive Processes,9:519-548.Stevenson, Rosemary and AgnieszkaUrbanowicz.
1995.
Structural focusing,thematic role focusing and thecomprehension f pronouns.
InProceedings ofthe Seventeenth AnnualConference ofthe Cognive Science Society,pages 328-332.Thompson, Henry S. 1977.
Strategy andtactics: A model for language production.In W. A.
Beach, S. E. Fox, andS.
Philosoph, editors, Papers from the 13thRegional Meeting of the Chicago LinguisticsSociety, pages 651-668, Chicago, IL.Yeh, Ching-Long and Chris Mellish.
1997.An empirical study of the generation ofanaphora in Chinese.
ComputationalLinguistics, 23(1):169-190.Young, R. Michael.
1997.
GeneratingDescriptions of Complex Activities.
Ph.D.thesis, Intelligent Systems Program,University of Pittsburgh.Zock, Michael and G6rard Sabah, editors.1988.
Advances in Natural LanguageGeneration: An Interdisciplinary Perspective.Ablex Publishing Corp., Norwood, NJ.
