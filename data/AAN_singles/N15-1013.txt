Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 123?132,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsExtractive Summarisation Based on Keyword Profile and Language ModelHan Xu Eric Martin Ashesh MahidadiaSchool of Computer Science and EngineeringUNSW, Sydney, NSW, Australia, 2052hanx,emartin,ashesh@cse.unsw.edu.auAbstractWe present a statistical framework to extractinformation-rich citation sentences that sum-marise the main contributions of a scientificpaper.
In a first stage, we automatically dis-cover salient keywords from a paper?s citationsummary, keywords that characterise its maincontributions.
In a second stage, exploiting theresults of the first stage, we identify citationsentences that best capture the paper?s maincontributions.
Experimental results show thatour approach using methods rooted in quan-titative statistics and information theory out-performs the current state-of-the-art systemsin scientific paper summarisation.1 Introduction and MotivationScience is not an isolated endeavour, but benefitsfrom and expands on the work of others, with moreor less cross fertilisation between disciplines.
Theinterdependent nature of research has naturally re-sulted in a network of scientific areas with dense in-terconnections between related fields.
Though re-search is a highly specialised activity, researchersfind themselves constantly in need to explore thenetwork further from the core of their research.Tools that can facilitate understanding the key con-tributions of papers in those parts of the network be-ing explored can only prove highly valuable.As an example of such tools, we focus on anapplication that automatically extracts information-rich sentences describing the main contributions ofa given paper.
From which corpus the extractioncould take place?
A natural answer is the abstract ofthe paper.
However, the contributions as perceivedby the authors can significantly deviate from thosejudged extrospectively by the community over time(Mei and Zhai, 2008).
Instead, we take as corpusthe set of citing sentences to the paper (from otherpapers).
Indeed, those sentences can arguably bedeemed as a form of crowd-sourced review of thepaper?s main contributions.
The set of citing sen-tences is referred to as the citation summary of thetarget paper.
Elkiss et al (2008) carried out a large-scale study and confirmed that citation summariescontain extra information that does not appear in pa-per abstracts.
In addition, they found that the ?self-cohesion?, measured as the average cosine similar-ity between sentences, is consistently higher in a pa-per?s citation summary than in its abstract: the for-mer is more focused than the latter in describing pa-pers?
main contributions.
This work presents our ef-forts in advancing research along this direction.Section 2 formally defines the problem we aimto solve: summarise scientific papers using themost informative and diversified part of their cita-tion summaries.
It surveys several prominent relatedstudies, and introduces the data used in our experi-ments and evaluations.
In Section 3, we present ourstatistical framework built upon quantitative statis-tics and information theory.
In Section 4, we eval-uate and compare the performance of our methodwith state-of-the-art systems.
We conclude andpoint to future directions in Section 5.2 Problem StatementThe problem we tackle in this paper is to generatean extractive summary (usually, we will simply saysummary) from its citation summary.
More specifi-cally, we opt for a two stage approach.
In the firststage, we automatically discover salient keywordsfrom a paper?s citation summary, keywords that areessential in characterising the paper?s main contribu-tions.
The second stage, exploiting the results of thefirst stage, identifies citation sentences (to the paper)that best capture the paper?s main contributions.A word of caution: by utilising only citation sum-maries, one should not expect to obtain well formu-lated, readily consumable summaries of papers.
In-deed, a citation sentence may be not all about thecited paper, but also talk about the citing paper andother co-cited papers, which disqualify citation sum-123maries as a premium source of sentences for build-ing highly readable summaries (Siddharthan andTeufel, 2007).
Moreover, a summary built from cit-ing sentences that come for a pool of multiple cit-ing papers is bound to lack coherence.
Therefore,it is more appropriate to consider that the output ofsuch a system is to extrinsically gauge a system?s ef-fectiveness in indexing information-rich citing sen-tences containing keywords that facilitate rapidlygrasping a paper?s important contributions, ratherthan be treated as a polished, readable summary forhuman consumption (Qazvinian et al, 2013).2.1 Related WorkQazvinian and Radev (2008) first experimentedwith citation summary based paper summarisations.They proposed a graph-based method, C-LexRank,that first generates a citation summary network fora paper by mapping citing sentences to vertices andcreating edges from their lexical similarities.
Clus-ters of sentences capturing the same contributionof the paper are then identified through link-basedcommunity detection.
Finally, the most central sen-tence of each cluster is found using a weightedrandom walk and selected to form a paper sum-mary meant to comprehensively cover the paper?smain contributions.
Mohammad et al (2009) furtheradapted the C-LexRank to multi-document sum-marisation in an attempt to generate surveys for sci-entific paradigms.In a later paper, Qazvinian et al (2010) proposeda more computationally efficient summariser thatdoes not require clustering citing sentences.
As afirst step, key phrases are automatically identifiedas significant n-grams with positive point-wise di-vergence (Tomokiyo, 2003) from a foreground lan-guage model estimated using the citation summaryof a paper w.r.t.
a background language model builtfrom a large set of paper abstracts.
A greedy algo-rithm is subsequently applied to select citing sen-tences and form a summary that maximises keyphrase coverage.Mei and Zhai (2008) presented a sophisticatedgenerative approach that frames summarisation un-der an Information Retrieval (IR) context.
Specifi-cally, an impact language model for a paper is firstbuilt as a mixture of a language model estimatedfrom the paper?s own text, and a weighted citationlanguage model based on its collective citation con-texts, using a compound coefficient reflecting botha sentence?s proximity to the citation label (anchor)in the citing paper and the citing paper?s authoritycalculated from the citation network using PageR-ank (Brin and Page, 1998).
Finally, documents (sen-tences in the target paper) that are closest to thequery (the impact language model of the target pa-per) are extracted to form a summary using ad-hocdocument retrieval.
Note that Mei and Zhai (2008)utilised extra information (i.e., paper full texts andcitation networks) to produce summaries that con-sist of sentences from papers?
own texts rather thantheir citation summaries, making their task relatedto but different to ours.2.2 DataThe experiments and evaluations presented herehave been based on Qazvinian?s single paper sum-marisation corpus1.
The dataset consists of 25highly cited papers in the ACL Anthology Network(AAN) (Radev et al, 2009) from 5 different do-mains: Dependency Parsing (DP), Phrase BasedMachine Translation (PBMT), Text Summarisation(SUM), Question Answering (QA) and Textual En-tailment (TE).
There are two files provided for eachpaper: a citation summary file containing all citingsentences to it, and a manually constructed key factfile containing its main contributions hand pickedby human annotators after reading the citation sum-mary.
The manual annotation has been performedindependently by annotators, and a phrase needed tobe marked by at least 2 annotators to be qualified ascapturing a paper?s key fact (Qazvinian and Radev,2008).
This corpus represents a gold standard in re-search paper summarisation and it has been widelyused in system evaluations (Qazvinian and Radev,2008; Qazvinian et al, 2010).3 Our ApproachIn this section, we first introduce our quantitativestatistical method to automatically construct a key-word profile of a paper and statistically capture apaper?s main contributions in terms of words fromits citation summary.
We then discuss how we con-struct a keyword profile language model.
Finally, weelaborate on how we cast the task of sentence selec-tion from the citation summary as language modeldivergence based IR in a probabilistic framework.3.1 Paper Keyword ProfileAs indicated in Section 1, the citation summary of apaper can be deemed a collective review of its con-tributions.
Therefore, the main contributions of a1http://www-personal.umich.edu/?vahed/data.html124paper are salient keywords, those keywords whichare commonly used by its citers to refer to it andare statistically over-represented in the paper?s cita-tion summary w.r.t.
the overall distribution of suchwords across other papers?
citation summaries.
Putanother way, the salience of a word in characteris-ing a paper?s main contributions is qualified alongover-representedness and exclusiveness dimensions.Clearly, a proper statistical model of words distribu-tion is required in order to measure words?
saliencein a paper?s citation summary.
Consider five papers,D1, .
.
.
, D5with citation summaries CS1, .
.
.
, CS5.We aim at identifying salient keywords from D1?scitation summary CS1, that map to D1?s main con-tributions.
To decide whether a word W is a charac-terising keyword of D1, we first collect all n citingsentences containing W from CS1, .
.
.
, CS5; sup-pose there are n = 20 of them.
Then for each citingsentence S amongst those 20, we perform the binarytest: success iff S belongs toCS1.
Suppose that thereare k = 18 successes and 2 failures.
This representsa surprising observation: one would expect a wordof no characterising power to appear in roughly thesame number of sentences in CS1, .
.
.
, CS5, assum-ing all citation summaries have the same number ofsentences2.
So one would heuristically conclude thatW is a good candidate keyword for D1, a keywordthat is likely to represent a main contribution.The previous process can be abstracted as sam-pling without replacement from a finite set whoseelements can be classified into mutually exclusivebinary categories, which itself follows a Hypergeo-metric distribution.
Let N be the total number ofciting sentences in citation summaries for papers be-longing to collection C, K be the number of sen-tences in paper D?s citation summary, n be the to-tal number of citing sentences containing a certainword W , and X be the number of citing sentencescontaining W in D?s citation summary.
The proba-bility of observing exactly k citing sentences in D?scitation summary containing W is:H(X=k|N,K,n)=(Kk)(N Kn k)(Nn)(1)We can then calculate a p-value to the observednumber of x citing sentences in D?s citation sum-mary that contain word W using the Hypergeomet-ric test, which in turn is used to measure word W ?ssalience in characterising D?s main contributions:S(W )def= P (X x)=1 Px 1i=0H(X=i|N,K,n)) (2)2This assumption is only made to simplify the discussion.The smaller the value of S(W ), the more salient Wis. Also, words not appearing in D?s citation sum-mary have a maximum p-value of 1.0, and commonwords appearing in many papers?
citation summariesare expected to have larger p-values than words thatare more exclusively used when citing paper D.It is worth pointing out that the above formulationcan be equivalently expressed as applying the one-tailed Fisher?s exact test to measure strengths of sta-tistical associations between words and paper?s ci-tation summaries at the sentence level.
Our choiceof this statistical procedure has been informed by(Moore, 2004).
Prior to this work, Dunning (1993)was pointing out that some commonly used meth-ods such as the Pearson?s  2test are inappropriatefor measuring textual associations due to the factthat the underlying normality assumption is usu-ally violated in textual data.
He was subsequentlyintroducing the log-likelihood ratio test (LLR) andshowing that it can yield more reliable results.
TheLLR was then and has since been widely adoptedin statistical NLP as a measure of strength of as-sociation (Moore, 2004).
For instance, Lin andHovy (2000) successfully applied LLR in mining?topic signatures?
of pre-classified document col-lections.
But to further verify LLR?s validity ap-plied to rare events, Moore (2004) performed an em-pirical study comparing results obtained using LLRand Fisher?s exact test on bilingual word associationand found that albeit being a good approximation toFisher?s exact test, LLR can still introduce a sub-stantial amount of error and the author went on toadvocate the use of Fisher?s exact test where com-putationally feasible.
Recall that we measured as-sociational strengths at the sentence level.
This re-sulted in marginal frequencies in the order of onlyhundreds for Qazvinian?s small corpus.
We there-fore followed this empirical advice and used the one-tailed Fisher?s exact test (i.e., Hypergeometric test)as our measure of textual association to perform key-word profiling of a scientific paper.To obtain a set of keywords likely to map to apaper?s main contributions, one can simply sort allwords according to their statistical significance andpick the top few (e.g., 10 words with the smallest p-values).
A more statistically tenable scheme wouldbe to identify the keywords of a paper as all wordsappearing in its citation summary with p-values be-low some significance level.
A technicality here isthat in the identification of keywords, multiple Hy-pergeometric tests have been performed.
For exam-ple, all unique words that appeared in the collection125of citation summaries have been individually testedfor their salience in a target paper?s citation sum-mary in succession.
The significance level used toqualify a word as a keyword thus requires correctionfor multiple tests to reduce type I errors.
However,we shall show that the rigid statistical significance isnot crucial in our subsequent building of a keywordlanguage model for a paper, and so we did not per-form multiple tests corrections, but simply used theraw p-values in subsequent analysis.Another technicality is special handling of cita-tion anchors.
Cited authors?
names, almost system-atically appearing in citing sentences, are bound tobe identified as salient keywords.
We thus substi-tuted all citation anchors appearing in a paper?s cita-tion summary with the pseudo token ?targetanchor?if they refer to the target paper, and ?otheranchor?
ifthey refer to other co-cited papers.Furthermore, our keyword profiling approach al-lows for a flexible control of the level of selective-ness in its statistical procedure through the choiceof the benchmarking collection C. For example,we can choose to use a heterogeneous collection ofpapers covering multiple domains.
Words that aresalient in characterising a domain may then evalu-ate to a high salience for a paper in C on that do-main (e.g., word ?parsing?
for domain DependencyParsing (DP)).
We can also choose C to be a homo-geneous collection of papers from the same domain.Only words that are salient in characterising a sin-gle paper will then be evaluated to a high saliencefor that paper (e.g., if C is on DP, ?parsing?
willnot show up as a salient word for any paper in C).Recall from Section 2.2 that we use as data papersfrom five domains.
We exploited the homogeneityof this data and performed keyword profiling intra-domain.
This effectively made the keyword profil-ing all the more selective that the keywords identi-fied for a paper only characterise its unique contribu-tions w.r.t.
its domain, using five highly cited papers.We shall show in the next section that it is this highselectiveness in keyword profiling that bestows ourapproach its high discriminative power.For paper P05-1013, Table 2 lists the top 10 key-words identified from its citation summary usingour method, while Table 1 lists the humanly se-lected gold standard key facts (Qazvinian and Radev,2008).
It can be seen that our method is highly ef-fective in identifying the paper?s main contributionswhich closely mirror those picked by human experts.We term our word list ranked by p-values the key-word profile of the paper; it statistically and objec-tively captures words?
salience (measured along thedimensions of over-representedness and exclusive-ness) in characterising the paper?s main contribu-tions using the statistical surprise given by Hyper-geometric tests.
While only unigram keywords wereconsidered here, our method can be easily extendedto cope with higher order n-gram ?key phrases?.This is left for future work.Fact id Fact OccurencePyramidtier1non-projective 1519pseudo-projective 6projectivizing 1projective graphs 1projectivizationtransformation14czech 68swedish 52data-driven 46training data 25 maltparser 4 43nonterminal categoriesin constituency1 1Table 1: Gold standard key facts of P05-1013 (Qazvinianand Radev, 2008) ordered by importance.
The pyramidtier might not be the sum of the occurrences of facts, asmultiple facts can appear in the same sentence.Salience rank Word P-value1 non-projective 1.54e-082 pseudo-projective 5.61e-063 transformation 4.47e-054 transformations 1.26e-045 maltparser 3.48e-046 swedish 7.53e-047 danish 1.56e-038 following 2.64e-039 arcs 2.64e-0310 dependencies 4.43e-03Table 2: Extracted keywords for P05-1013, ranked by de-creasing Hypergeometric test significance.3.2 Keyword Profile Language ModelEach sentence in a paper?s citation summary coverskeywords (possibly none) that map to the paper?smain contributions.
Intuitively, a good summarisa-tion should be short, and consist of citing sentencesthat maximise keywords coverage w.r.t.
an arbitrar-ily imposed summary length limit (Qazvinian andRadev, 2008).
A good summariser should thus pickciting sentences that contain as many non-redundantkeywords as possible.
We have shown in the last sec-126tion that not all keywords are of equal importance,so a good summariser should favour sentences cov-ering the most important ones.
Intuitively, the key-word profile of a paper containing valuable informa-tion on words?
salience in characterising the paper?smain contributions should be utilised to drive such adiscriminative sentence selection process.Based on the previous considerations, we use apaper?s keyword profile to build a discriminative un-igram language model that directly encodes words?salience as pseudo generative probabilities to facili-tate the seamless incorporation of such informationinto a generic probabilistic framework.
More specif-ically, we directly translate words?
salience (in theform of p-values) into a discriminative unigram lan-guage model of a paper that assigns high probabili-ties to its characterising keywords.
The pseudo gen-erative probability of word W according to a paperD?s keyword profile language model Mkpis:P (W |Mkp)= 1Zlog(S(W )) (3)where S(W ) denotes the salience of word W incharacterising paper D calculated using (2), and Zis a normalisation factor.
An intuitive interpreta-tion of (3) is to deem   log(S(W )) a pseudo wordcount of W , where more salient words have higherpseudo counts; this makes Z the total length of thepseudo document generated from the paper?s key-word profile.
We disregard actual word counts tomake the keyword profile language model directlyencode words?
salience.
Also, in the previous step,keyword profiling had already implicitly taken suchinformation into account, providing another justifi-cation for this design decision.
Table 3 shows aminiature example to illustrate how a keyword pro-file language model is built.
In this example, W5is automatically eliminated from the resulting lan-guage model because it has lowest salience in char-acterising the imaginary document.
Any word Swith salience value S(W ) close to but strictly lessthan 1.0 would still have a tiny pseudo probability inthe resulting keyword profile language model (e.g.,W4).
Words with low salience are not necessarilystop words (e.g., W4and W5), and neither is thereverse true: a content word can possibly be usedacross the document collection and thus evaluate toa very low salience (and so have a nul or low pseudogenerative probability in the resulting keyword pro-file language model) for the document under con-sideration.
For example, ?parsing?
would have a lowsalience for any paper in a collection on DependencyParsing.
It can be seen that our method amounts toa highly adaptive data driven term weighting frame-work.
For brevity, from now on, we use KPLM torefer to keyword profile language model.WordSalienceS(W )Pseudo count  log(S(W ))P (W |Mkp)W10.01 4.61 0.605W20.10 2.30 0.303W30.50 0.69 0.091W40.99 0.01 0.001W51.00 0.00 0.000Table 3: Keyword profile language model built for animaginary document consists of only 5 distinct words.Although implicitly conveyed in the formulationof KPLM above, it should be made clear that theKPLM is a pseudo language model that encodeswords?
salience in the form of pseudo generativeprobabilities, which functions as a language model,yet should not be interpreted as a true languagemodel under the traditional definition.
A traditionalunigram language model is constructed using theactual term frequencies in the document, the re-sulting model capturing generative probabilities.
Incontrast, the KPLM of a document is built usingpseudo term frequencies that directly encode words?salience in characterising a document?s contents,measured using a sophisticated quantitative statisti-cal procedure.
It can thus be interpreted as a proba-bilistic description of the document?s keywords withsignificantly boosted discriminative power.
Havingclarified the nature of KPLM, we treat it as a lan-guage model in the rest of the paper.3.3 KPLM Based Summarisation3.3.1 Sentence SelectionThe KPLM of a paper is a discriminative gen-erative model that incorporates words?
salience incharacterising a paper?s main contributions.
It thusrepresents an effective language model from whicha model citing sentence covering the paper?s maincontributions could be sampled from3.
So by mea-suring the statistical surprise between the realisticlanguage model estimated from each citing sentencewith the KPLM of a paper, we can select the setof citing sentences that conform best to the optimalmodel given by the the KPLM and build a sum-mary that well captures keywords.
More specifi-cally, we adopt the negative cross entropy retrievalmodel (Zhai, 2008), use the KPLM of a paper as the3A pseudo citing sentence sampled from KPLM in this man-ner would simply be a bag of words, not a grammatical sen-tence.
So here ?model?
has the favour of keywords coverage.127sole document model, and measure the cross entropyof multiple query models from it (one for each citingsentence in that paper?s citation summary).
Citingsentences whose Maximum Likelihood Estimation(MLE) language models are closest to the paper?sKPLM are taken as building blocks of the summary.Formally, let S be a citing sentence and letc(W,S) denote the number of occurrences of wordW in S. The MLE language modelMmleof S is therelative frequency of word W in S:P (W |Mmle)=c(W,S)|S|(4)Subsequently, the score for a citing sentence S isgiven by its negative cross entropy with the Mkp:Score(S)= H(Mmle||Mkp)=PW2VP (W |Mmle) log(P (W |Mkp)) (5)The larger a citing sentence?s score, the closer it isto the cited paper?s KPLM, thus the higher the citingsentence would be ranked.
To summarise a paper,one can just pick the top k ranked citing sentenceswhere k is the imposed summary length limit.We are not the first to cast the task of summarisa-tion as document retrieval.
Mei and Zhai (2008) pi-oneered in utilising language models and divergencebased IR to select sentences to build summaries.While similar in the fundamental methodology, ourapproach should be distinguished from this work.First, Mei and Zhai cast the task as ad-hoc retrieval,using the ?impact language model?
of a paper as solequery, while the paper?s sentences are treated as doc-uments whose Kullback-Leibler divergence (Kull-back and Leibler, 1951) with the query model ismeasured in turn.
Estimating reliable language mod-els for short documents is challenging due to datasparseness and thus requires prudent smoothing.
Wepurposefully reversed the roles of sentence modeland document model, using the shorter sentencesas queries and measuring their cross entropy witha sole document model (the KPLM)4.
This repre-sents a more natural formulation resulting in sim-pler language models that require fewer parame-ter estimations.
Second, while the impact languagemodel in (Mei and Zhai, 2008) is partially weighted4Kullback-Leibler divergence, used in (Mei and Zhai, 2008),is unsuitable to our task, as it is not formalised as ad-hoc re-trieval (i.e., single query, multiple documents).
Instead we com-pare multiple query models (MLE?s of citing sentences) to asingle document model (KPLM of the cited paper), making KL-divergence scores not comparable due to query specific entropyterms.
See (Zhai, 2008) for a detailed analysis.using citing paper authority and sentence proxim-ity to the citation anchor in the citing paper, it isstill largely based on actual word occurrences.
Incontrast, KPLM directly models words?
salience incharacterising a paper?s main contributions using itskeyword profile, with expectedly more discrimina-tive power.
Last, Mei and Zhai?s estimation of animpact language model for a paper assumes the reli-able estimation of its citing papers?
authority, whichcannot always be guaranteed, for example when apaper receives citations from new papers that them-selves have not been cited enough.
Furthermore,while a citation network can be unavailable, the es-timation of KPLM requires only the citation sum-maries of papers, which is arguably more robust.3.3.2 Top Sentence Re-rankingAs discussed in Section 3.2, a good summaryshould capture the most salient keywords of a pa-per, but also cover as many non-redundant keywordsas possible.
A summary built using our method islikely to contain citing sentences that concentrate onand repetitively cover salient keywords of the targetpaper, which may fall short in keywords diversity.Indeed, we can see in the top part of Table 4 thatthe summary of paper P05-1012 repetitively coversa single keyword, ?Minimum Spanning Tree?, whileit fails to capture other key concepts.To leverage the diversity in keywords captured ina summary, a simple heuristic is to select the nextsentence from a pool of top ranked sentences leastsimilar to the existing summary.
From an informa-tion theoretic point of view, this amounts to choos-ing the next sentence that carries the most extra in-formation (i.e., statistical surprise), w.r.t.
the currentcontents of the summary.
This formulation intu-itively suggests that cross entropy, as a natural mea-sure of statistical surprise, could again be employed.We first need to abstract a citing sentence and thecitation summary into probabilistic distributions be-fore their cross entropy can be measured.
Again weuse unigram language modelling.
Since both textsare small in size, data sparseness becomes a ma-jor issue, as nul dimensions in the MLE languagemodels would make cross entropy not measurable.Smoothing as a way to alleviate data sparseness isthus required.
Another issue that also arises fromthe texts?
small size is the non-negligible amount ofcross entropy contributed from non-content words inboth texts (English stop words plus the two pseudotokens: ?targetanchor?
and ?otheranchor?).
Wetherefore remove those non-content words prior to128language model construction to eliminate their noisein the cross entropy calculation.
Experiments didsupport this design decision, and better results havebeen achieved with non-content words removed.We perform Dirichlet Prior Smoothing (Zhai andLafferty, 2001) to both the citing sentence MLE andthe summary MLE using the KPLM of the paper asa background model using a Dirichlet Prior (DP) of20.
The choice of 20 has been based on the obser-vation that citing sentences are short (32 words onaverage) and a large DP is prone to generate overlysmoothed language models that are dominated bythe KPLM, thus lack discriminative power.
Here wechoose to use this empirically selected DP parameterwithout attempting to fine-tune it for best results.In summary, we implement a top sentence re-ranking heuristic that iteratively selects the next sen-tence to be appended to the existing summary whosesmoothed language model is with the largest crossentropy (so it contains most extra information) witha smoothed language mode built for the summaryat its current stage.
We shall demonstrate how ourtop sentence re-ranking method introduces a majorperformance boost in the next section.
For a quickinspection of the effectiveness of this method, com-pare the summaries constructed for paper P05-1012with and without sentence re-reranking in Table 4.It shows that the summary constructed with sen-tence re-ranking covers key facts more comprehen-sively.
The pseudo code for our re-ranking strategyis shown in Algorithm 1.
It adopts a straightforwardre-ranking approach that simply uses the top k+5 re-trieved citing sentences in the previous step as thecandidate pool; at each iteration, it selects the bestsentence based on its cross entropy with the sum-mary at the current stage.
A more sophisticated re-ranking method is to combine the two cross entropyscores in some way (e.g., Maximal Marginal Rele-vance (Carbonell and Goldstein, 1998)) so that thefinal score for a citing sentence reflects its value incapturing salient keywords that have not yet been in-cluded in the summary.
We leave the study of a moresophisticated re-ranking scheme for future work.4 Experimental Setup4.1 Evaluation MethodFollowing Qazvinian et al (2008; 2010), we use thepyramid method (Nenkova and Passonneau, 2004)at sentence level to evaluate our system?s perfor-mance.
The pyramid score is a fact-based eval-uation method that has been especially popular inevaluating extractive summarisation systems.
It hasAlgorithm 1 Top Sentence Re-ranking1: function TOPSENTENCERERANKER2: k  summary length limit3: top sent  top k plus 5 sents[0]4: es  top sent5: cp  top k plus 5 sents - top sent6: for s in cp do7: cp lms[s]  DPSmoothed(s)8: for i = 2 to k do9: es lm  DPSmoothed(es)10: s  argmaxs2cp(CE(cp lms||es lm))11: es  es + s12: cp  cp  s13: cp lms  cp lms  cp lms[s]return esbeen widely adopted because it incorporates bothfact coverage and fact importance into the scoringprocess, which resonates well with the goals of sum-marisation (Qazvinian et al, 2010).
More specifi-cally, the pyramid method scores a summary usingthe ratio between the total facts weights of the factsit covers and that of an optimal summary.
First a factweights pyramid is built using some facts weightingmethod and each fact is subsequently put into its per-spective pyramid tier.
Qazvinian et al (2008; 2010)built a weights pyramid for each paper and assignedeach humanly discovered fact into a tier accordingto the number of citing sentences the fact occurs inthat paper?s citation summary.
For example, fact fiappearing in |fi| citing sentences in the citation sum-mary of paper D is assigned to the tier T|fi|in D?sfact weights pyramid PD.
Let Fidenotes the num-ber of facts in the summary ES in tier Tiof PD.
Thetotal facts weights ES covers is calculated as:W (ES)=Pni=1i?Fi(6)where n is the highest tier of PD.
Let ESoptimalbe the optimal summary for D w.r.t.
the summarylength limit (ESoptimalcan be found using heuristic-driven exhaustive search).
The pyramid score forES is finally calculated as:Score(ES)=W (ES)/W (ESoptimal) (7)Note again that we used exactly the same corpusand evaluation method as in (Qazvinian and Radev,2008; Qazvinian et al, 2010), which makes our re-sults directly comparable to those described in thosepapers.
Furthermore, both papers report on perfor-mance of various baseline methods which are alsodirectly comparable to ours (see next section).
Wecompare our results with the current state-of-the-art; readers are encouraged to refer to (Qazvinian129Rank SummaryKPLM without sentence re-ranking (Pyramid score: 0.23)1 3.1 decoding mcdonald et al(2005b) use the chu-liuedmonds (cle) algorithm to solve the maximum spanning tree problem.2 thus far, the formulation follows mcdonald et al(2005b) and corresponds to the maximum spanning tree (mst) problem.3while we have presented signi cant improvements using additional constraints, one may won5even when cachingfeature extraction during training mcdonald et al(2005a) still takes approximately 10 minutes to train.4we have successfully replicated the state-of-the-art results for dependency parsing (mcdonald et al 2005a) for bothczech and english, using bayes point machines.5the search for the best parse can then be formalized as the search for the maximum spanning tree (mst)(mcdonald et al 2005b).KPLM with sentence re-ranking (Pyramid score: 0.73)1 3.1 decoding mcdonald et al(2005b) use the chu-liuedmonds (cle) algorithm to solve the maximum spanning tree problem.2to learn these structures we used online large-margin learning (mcdonald et al 2005) that empirically providesstate-of-the-art performance for czech.3while we have presented signi cant improvements using additional constraints, one may won5even when cachingfeature extraction during training mcdonald et al(2005a) still takes approximately 10 minutes to train.4mcdonald et al(2005a) introduce a dependency parsing framework which treats the task as searching for theprojective tree that maximises the sum of local dependency scores.5 we take as our starting point a re-implementation of mcdonald?s state-of-the-art dependency parser (mcdonald et al 2005a).Table 4: Summaries of paper P05-1012 produced using KPLM.
Key facts in citing sentences are highlighted and OCRand sentence segmentation errors have been retained as they originally appeared in the corpus.and Radev, 2008; Qazvinian et al, 2010) for cross-referencing results from a broader set of systems.4.2 Results and DiscussionTable 5 shows the pyramid score evaluation re-sults for the 25 papers.
To facilitate comparisonand cross-referencing, the table has been format-ted as close as possible to Table 7 in (Qazvinianand Radev, 2008) with figures in the Gold and C-LexRank columns directly copied over.
Note that aGold pyramid score less than 1 suggests that thereare more facts than can be covered using k sen-tences for that paper?s citation summary.
It canbe seen that KPLM based summarisation achievesquite comparable results (especially in terms of themedian score) with C-LexRank, even without topsentence re-ranking.
When the re-ranking is intro-duced, our system outperforms the current state-of-the-art C-LexRank by a measurable margin.
Al-beit the perceived differences in the results, a one-tailed Wilcoxon signed-rank test indicated that ourresults are not statistically superior at significancelevel 0.05 (Z=-1.22, P=0.11).
A power analysis re-veals that in order to achieve a statistically signif-icant result on this small sample of 25 papers, asystem would need to score a medium to large ef-fect size (Cohen?s d > 0.53), which is a challengingtask considering C-LexRank?s strong baseline per-formance.
We hope this analysis can inform futurestudies using Qazvinian?s 25 papers corpus.
Never-theless, it should be pointed out that our approachis not only substantially simpler than C-LexRank, italso yields more interpretable results.We know of a more recent set of results reportedin (Qazvinian et al, 2013), which again confirmedC-LexRank?s state-of-the-art status with a meanpyramid score of 0.799 (cf.
Table 6 in (Qazvinianet al, 2013)).
However those results are not com-parable with ours for the following reasons.
First,Qazvinian et al (2013) used a slightly different cor-pus with 30 papers (5 extra papers from the Condi-tional Random Field domain).
Second, results werebased on a summary length limit of 200 words, soroughly equivalent to 6.3 sentences per paper, giv-ing evaluations an extra edge.
Both changes boostedsystem performance in those evaluations, as evi-denced by comparing Table 7 in (Qazvinian andRadev, 2008) and Table 6 in (Qazvinian et al, 2013).Qazvinian et al (2010) used the same corpus andevaluation method as our work; however the re-sults have been presented as box plots (cf.
Figure 1in (Qazvinian et al, 2010)) fromwhich only the five-number summary (i.e., minimum, lower quartile,median, upper quartile and maximum) of the pyra-mid scores can be reconstructed and consequentlyno significance test can be performed.
Comparedwith the best performing variants of the system de-vised in (Qazvinian et al, 2010) based on unigrams,bigrams and trigrams, our system (KPLM+TSR)achieves a higher median score (0.86 vs. 0.80), aswell as a lower score variation across the 25 papers.An arbitrarily imposed constraint in the eval-uations is the summary length limit, which maybe changed to suit a specific application context.The summarisation task becomes increasingly morechallenging when summary length limit is furthertightened as this would require a summariser to pin-point the best sentences from a potentially large cita-130DomainPaperGoldC-LexRankKPLMKPLM+TSRDPC96-1058P97-1003P99-1065P05-1013P05-10121.001.000.941.000.950.730.400.670.670.620.330.790.620.660.230.560.790.760.660.73PBMTN03-1017W03-0301J04-4002N04-1033P05-10330.961.001.001.001.000.641.000.480.850.850.600.800.860.570.970.600.800.890.860.97SUMMA00-1043A00-2024C00-1072W00-0403W03-05101.001.001.001.001.000.950.600.930.700.830.500.600.870.811.000.500.600.930.541.00QAA00-1023W00-0603P02-1006D03-1017P03-10011.001.001.001.001.000.860.600.870.850.590.880.440.930.700.941.000.940.930.900.44TED04-9907H05-1047H05-1079W05-1203P05-10141.001.001.001.001.000.941.000.560.710.780.770.830.781.000.890.910.830.891.001.00Mean 0.99 0.75 0.73 0.80Median 1.00 0.73 0.79 0.86Table 5: Summary pyramid score evaluation results withsummary length limit k = 5.tion summary.
A desirable property of a good sum-mariser is thus the ability in maintaining its perfor-mance while the task becomes increasingly demand-ing.
To further evaluate KPLM?s performance un-der increasingly more stringent summary length lim-its, we gathered the pyramid scores with summarylength limit k decreasing from 5 to 1 and visualisedthe results in Figure 1.
We can see that KPLM?sperformance decays quite gracefully as more strin-gent limits are imposed.
Even under the harshestconstraint with the summary length limit sets to 1,our system still managed a mean pyramid score ofclose to 0.6 across the 25 papers.
Indeed, it canbe seen that the variance in pyramid scores gradu-ally spreads wider (the dark band in the figure marksout 95% confidence interval of the mean scores), butthis phenomenon is expected as the error margin alsoshrinks along with the summary length limit.5 Conclusion and Future WorkWe designed a statistical framework to summarisescientific papers, using methods rooted in quanti-tative statistics and information theory.
We firstbuilt a keyword profile for a paper using a quan-titative statistical method that captures its charac-?
?
??
?0.40.50.60.70.85 4 3 2 1Summary length limitPyramidscoreFigure 1: Pyramid scores of KPLM+TSR under differentsummary length limits.terising keywords that are both overly representedand relatively exclusively used in the paper?s cita-tion summary.
We then used the keyword profileof a paper to build a discriminative pseudo unigramlanguage model that directly incorporates words?salience in characterising a paper?s main contribu-tions into pseudo generative probabilities.
Based onthe fact that a paper?s KPLM represents an effec-tive language model from which pseudo citing sen-tences with good coverage of important keywordscould be sampled, we cast the task of summarisa-tion as language model divergence based IR.
Finally,we implemented an information-driven sentence re-ranking algorithm that can effectively leverage di-versity in keyword coverage in summaries produced.Experimental results show that our approach outper-forms the current state-of-the-art systems in scien-tific paper summarisation, which is also with goodresilience to more stringent summary length limits.In the future, we plan to extend our approach tohigher order n-grams and see whether larger infor-mation units (phrases) would help boost summarisa-tion performance.
We also plan to apply our methodto the problem of multi-document summarisation.
Inparticular, we are very interested to test our system?sperformance on automatically generating a technicalsurvey of a scientific paradigm, which thanks to theauthors of (Mohammad et al, 2009; Qazvinian et al,2013), has been established as a well-defined taskwith high-quality open data.
Finally, while we haveshown that our approach is effective in summarisinga scientific paper?s major contributions using its cita-tion summary text, further experiments are requiredto test our method?s effectiveness on more genericsummarisation tasks and texts genres.AcknowledgementWe thank Vahed Qazvinian for making the 25 pa-per summarisation corpus publicly available; with-out it, our formal evaluations would have been im-possible.
We also thank the anonymous reviewersfor their highly constructive comments.131ReferencesBrin, Sergey and Page, Larry.
1998 The anatomy of alarge-scale hypertextual web search engine.
In Pro-ceedings of the 7th International Conference on WorldWide Web, pp.
107?117.Carbonell, Jaime and Goldstein, Jade.
1998 The use ofMMR, diversity-based reranking for reordering docu-ments and producing summaries.
In Proceedings ofthe SIGIR, pp.
335?336.Elkiss, Aaron and Shen, Siwei and Fader, Anthony andErkan, G?unes?
and States, David and Radev, Dragomir.2008 Blind men and elephants: What do citation sum-maries tell us about a research article?.
Journal of theAmerican Society for Information Science and Tech-nology, vol.
59, no.
1, pp.
51?62.Kullback, S. and Leibler, R. A.
1951 On informationand sufficiency.
The Annals of Mathematical Statis-tics, vol.
22, no.
1, pp.
79?86.Mei, Qiaozhu and Zhai, Chengxiang.
2008 Generatingimpact-based summaries for scientific literature.
InProceedings of the ACL, pp.
816?824.Mohammad, Saif and Dorr, Bonnie and Egan, Melissaand Hassan, Ahmed and Muthukrishan, Pradeep andQazvinian, Vahed and Radev, Dragomir and Zajic,David.
2009 Using citations to generate surveysof scientific paradigms.
In Proceedings of the HLT-NAACL, pp.
584?592.Nenkova, Ani and Passonneau, Rebecca.
2004 Evaluat-ing content selection in summarization: The pyramidmethod.
In Proceedings of the HLT-NAACL, pp.
145?152.Qazvinian, Vahed and Radev, Dragomir.
2008 Scien-tific paper summarization using citation summary net-works.
In Proceedings of the 22nd International Con-ference on Computational Linguistics, pp.
689?696.Qazvinian, Vahed and Radev, Dragomir and?Ozg?ur, Arzu-can.
2010 Citation summarization through keyphraseextraction.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pp.
895?903.Dunning Ted.
1993 Accurate methods for the statisticsof surprise and coincidence.
Computational Linguis-tics, vol.
19, no.
1, pp.
61?74.Lin, Chin-Yew and Hovy, Eduard.
2000 The automatedacquisition of topic signatures for text summarization.In Proceedings of the 18th conference on Computa-tional Linguistics, pp.
495?501.Moore.
Robert C. 2004 On log-likelihood-ratios and thesignificance of rare events.
In Proceedings of EMNLP,pp.
333?340.Qazvinian, Vahed and Radev, Dragomir and Mohammad,Saif and Dorr, Bonnie and Zajic, David and Whidby,Michael and Moon, Taesun.
2013 Generating extrac-tive summaries of scientific paradigms.
Journal of Ar-tificial Intelligence Research (JAIR), vol.
46, pp.165?201.Radev, Dragomir and Pradeep, Muthukrishnan andQazvinian, Vahed.
2009 The ACL anthology networkcorpus.
In Proceedings of the ACL workshop on Nat-ural Language Processing and Information Retrievalfor Digital Libraries, pp.
54?61.Siddharthan, Advaith.
and Teufel, Simone.
2007 Whoseidea was this, and why does it matter?
In Proceedingsof the NAACL/HLT, pp.
316?323.Tomokiyo, Takashi and Hurst, Matthew.
2003 A lan-guage model approach to keyphrase extraction.
InProceedings of the ACL?03 workshop onMultiword ex-pressions, pp.
33?40.Zhai, Chengxiang.
2008 Statistical language modelsfor information retrieval a critical review.
Foundationsand Trends in Information Retrieval, vol.
2, no.
3, pp.137?213.Zhai, Chengxiang and Lafferty, John.
2001 A study ofsmoothing methods for language models applied to adhoc information retrieval.
In Proceedings of the 24thAnnual International ACM SIGIR Conference on Re-search and Development in Information Retrieval, pp.334?342.132
