Coling 2010: Poster Volume, pages 1301?1309,Beijing, August 2010Automatic Generation of Semantic Fields for Annotating Web Im-agesGang Wang?
?, Tat Seng Chua?, Chong-Wah Ngo?, Yong Cheng Wang??Shang Hai Jiao Tong University?School of Computing, National University of Singapore?Dept of Computer Science, City University of HongKong?Na Xun Hi-Tech Application Institutewanggang_sh@hotmail.com,chuats@comp.nus.edu.sg,cwngo@cs.cityu.edu.hk,ycwang@mail.sjtu.edu.cnAbstractThe overwhelming amounts of multi-media contents have triggered the needfor automatically detecting the semanticconcepts within the media contents.With the development of photo sharingwebsites such as Flickr, we are able toobtain millions of images with user-supplied tags.
However, user tags tendto be noisy, ambiguous and incomplete.In order to improve the quality of tagsto annotate web images, we propose anapproach to build Semantic Fields forannotating the web images.
The mainidea is that the images are more likely tobe relevant to a given concept, if severaltags to the image belong to the sameSemantic Field as the target concept.Semantic Fields are determined by a setof highly semantically associated termswith high tag co-occurrences in the im-age corpus and in different corpora andlexica such as WordNet and Wikipedia.We conduct experiments on the NUS-WIDE web image corpus and demon-strate superior performance on imageannotation as compared to the state-of-the-art approaches.1 IntroductionThe advancement in computer processor, sto-rage and the growing availability of low-costmultimedia recording devices has led to an ex-plosive growth of multimedia data.
In order toeffectively utilize such a huge amount of mul-timedia contents, we need provide tools to faci-litate their management and retrieval.
One ofthe most important tools is the automatic mediaconcept detectors, which aim to assign high-level semantic concepts such as ?bear?
to themultimedia data.
More formally, the conceptdetection for an web image is defined as: givena set of predefined concepts C : [C1, C2 ...Cn],we assign a semantic concept Ci to the image ifit appears visually in the image.
Traditionally,such concept detectors are built by the classifierapproaches.
The performance of such detectorsdepends highly on the quality of training data.However, preparing a set of high quality train-ing data usually needs a large amount of humanlabors.
On the other hand, the social web ischanging the way people create and use infor-mation.
For example, users started to developnovel strategies to annotate the massive amountof multimedia information from the web.
Inimage annotation, Kennedy et al (2006) ex-plored the trade-offs in acquiring training databy automated web image search as opposed tomanual human labeling.
Although the perfor-mance of systems with training data obtainedby manual human labeling is still better thanthose whose training data is acquired by auto-mated web search, the latter approaches haveattracted many researchers?
interest due to theirpotential in reducing human label efforts.
How-ever, the tags in the web images are known tobe ambiguous and overly personalized (Matu-siak 2006).Figure 1 gives four examples to illustrate therelationships between the visual concept ?bear?and the annotation tag ?bear?.
Generally speak-ing, there are four types of relationships:1301?
The relevant tag: The user-tag ?bear?properly reflects the content of an image,as shown in Figure 1(a).
While ?bear?
hasmultiple senses, the visual concept corres-ponds directly to the most common senseof ?bear?.?
The ambiguous tag: The user-tag ?bear?
isambiguously related to the visual content,as shown in Figure 1(b).
In this example,the visual content is related to anothersense of ?bear?
: ?a surly, uncouth, burly,or shambling person?
(Merriam-Websterdictionary, 2010).?
The noisy tag: The user-tag ?bear?
is anoisy tag, as shown in Figure 1(c).
In thisexample, the visual content is irrelevant tothe concept ?bear?.?
The incomplete tag: The user-tag ?bear?doesn?t occur in the tag list of Figure 1(d).However, many human annotators believethat the visual concept ?bear?
exist in theFigure 1(d).
Also, in Wikipeida, a panda isdefined as a kind of a bear.
(a) relevant                                (b)  ambiguous(c)  noisy                                      (d)  IncompleteFigure 1: The relationship between the tags andthe visual concept ?bear?
in NUS-Wide corpus.In this paper, we aim to assign relevant tagsto images in order to reduce the effects of am-biguous, noisy and incomplete tags.
To distin-guish relevant tags from other sense of tags, acommon practice is to perform word sense dis-ambiguation (WSD) to predict the right sense ofa tag.
Nevertheless, performing a WSD on anoisy and sparse set of tags, where the orderand position of tags do not matter, is by nomeans easy.
Most existing works on WSD, suchas Navigli (2009) are based on clean data andword neighborhood statistics.
They cannot bedirectly applied to address this problem.
Al-though there are some works such as Wang et al(2003) on capturing the semantics of noisy data,the problem of ambiguous words has not beenconsidered.
In addition, some semantic modelssuch as PLSA (Hofmann 1999), LDA (David etal.
2003) have been proposed to capture the se-mantics.
However, one challenge of employingsuch models is that there are many noisy tags inthe web image domain.
The reason for noisytags is that the purpose of tagging is not onlyfor content description, but also for other fac-tors such as getting attention and so on (Ameand Naaman, 2007, Bischoff et al 2008).Given a web image with a tag list, we pro-pose an approach to predict the ?SemanticField?
of the image.
Semantic Field (Jurafskyand Martin 2000) is designed to capture a moreintegrated relationship among the entire sets oftags.
In our work, we consider four differentcases of examples, as shown in Figure 1.
In 1(a),the concept ?bear?
will be assigned to the imagewith relatively high probability, because ?zoo?,?bear?, and ?polar?
provide clues that ?bear?
isthe major focus of the image.
In 1(b), the con-cept ?bear?
could possibly be disambiguated asnot related to ?animal?, the most common senseof ?bear?, by investigating other tags such as?men?,?guys?.
In 1(c), the image will not belabeled as ?bear?, since the surrounding tagssuch as ?dogs?, ?pups?
do not support the exis-tence of ?bear?
in the image.
In 1(d), althoughthe concept ?bear?
is missing, the image will bestill labeled as ?bear?
since the surrounding tagssuch as ?pandas?, ?animals?, and ?zoos?
jointlysuggest that ?bear?
appears in the image.
Thesignificance of user tags towards a target con-cept can be modeled from three differentsources: the statistics from the web image cor-pus, Wordnet and Wikipedia.
In summary, in-stead of directly matching the keywords andtags, we consider tags of an image collectivelyto predict the underlying semantic field.
Ideally,the semantic field can highlight the major visualconcepts in images so that we can assign thecorrect semantic labels to the images.In the rest of this paper, we discuss relatedwork in Section 2, while Section 3 reports thebuilding of Semantic Fields and its applicationto web image ranking.
Section 4 discusses theexperimental setup and results.
Finally, Section5 contains our concluding remarks.zoo, bear,polar,December,Viennamen, bear,hot, cubs,bears, fur,cub, hairy,guys, fuzzy,barebear, dogs,pups, pup-pies, cud-dle, daisyanimals,pandas,zoos13022 Related WorkIn this section, we report the works on SemanticField theory, text analysis in multimedia andthe existing systems for a web image corpus.2.1 Semantic FieldsSemantic Fields have been hotly debated in lin-guistics community (Grandy 1992, Garret1992).
Compared to lexical analysis, it consid-ers the entire sets of words instead of a singleword.
The FrameNet project (Baker et al 1998)is an attempt to realize the Semantic Field.However, the problem with FrameNet project isthat it needs extensive human efforts to definethe thematic roles for each domain and eachframe, and hence it is domain specific.2.2 Text Analysis in MultimediaIn multimedia, one of the important tasks isconcept detection, which attempts to find thevisual appearance of a concept such as ?bear?
inan image.
However, due to the large variationsin the low level visual feature space such ascolor, texture etc, in many cases, researchers arehardly able to capture the concept by visual in-formation alone.
Some researchers attempted toemploy natural language analysis to detect thevisual concept.
Rowe (1994) explored the syn-tax of images?
captions to infer the visual con-cepts present in images.
For example, he foundthat the primary subject noun phrase usuallydenotes the most significant information in themedia datum or its ?focus?.
He assumed thatboth visual and text features will describe thesame focus of the content.
Wang et al (2008)employed the similar idea to infer visual con-cepts in news video.
They first aligned text in-formation with visual information, and thencaptured the text focus to infer the visual con-cept.
These works suggest that we can transferthe problem of visual concept detection to thatof finding a text focus.In addition, researchers proposed statisticalmodels to combine text and visual features,such as the translation model (Duygulu et al2002, Jin et al 2005), cross media relevancemodel (Jeon et al 2003) and continuous relev-ance model (Lavrenko and Jeon, 2003).
How-ever, no matter what models we used, the anno-tation accuracy is still quite low, partially be-cause of the existence of noise in tags.
Jin et al(2005) provided a solution to tackle such a noi-sy tag problem.
They first investigated varioussemantic similarity measures between eachkeyword pairs in the tag list based on Wordnet.They then regarded non-correlated keywords asnoises and discarded them.
In this paper, thereare three major differences between our workand the above work.
First, because tags fromInternet are not always included in Wordnet, weemploy multi-resources of information to ana-lyze the semantics.
Second, we extend the anal-ysis of the word pair relationship to the Seman-tic Field analysis.
Third, since it is not easy toidentify the noise in the tag list directly, we on-ly analyze the tags which are highly relevant tothe concept with a specific sense.2.3 The State of the Art SystemsNUS-WIDE (Chua et al 2009) is a large scaleWeb image corpus.
It provides not only socialtags from the web, but also the ?gold?
labels (orground truth) for 81 concepts from large humanlabeling efforts.
As far as we know, there aretwo reported systems that used the whole NUS-WIDE corpus to test their proposed methods.
InChua et al (2009), the 81 concepts are detectedby k nearest neighbor using the visual featuresof: color moments, color auto-correlogram, col-or histogram, edge direction histogram, wavelettexture, and a bag of visual words.
The meanaverage precision (MAP) for the 81 conceptsreaches 0.1569.
Gao et al (2009) extended thek-NN approach to use both text tags and visualinformation.
For the tag information, they madeuse of the co-occurrence information to com-pute the probability of an image belonging to acontain concept.
They used the same visual fea-tures as in (Chua et al 2009).
In their work, thetaxonomy in WordNet is exploited to identifywhether a target concept is generic or specific.The co-occurrence tag analysis is employed forgeneric concepts, while visual analysis is usedfor specific concepts.
The MAP for this ap-proach reaches 0.2887.3 Building Semantic Fields for Annotat-ing Web ImagesIn this paper, we attempt to capture text seman-tics collectively from the tag list of images toannotate their visual contents.
Semantic Fieldsconsist of a selected subset of the tag list and1303the choice of these tags is based on their relev-ance to the contents of the targeted image witha specific sense.
There are three characteristicsin our Semantic Field model.
First, the Seman-tic Field is built by only a subset of tag list.
Forexample, the Semantic Field in Figure 1(a) is{zoo, bear, polar}.
It could partially reduce theeffect of the noise.
Second, because inferringthe visual concept of an image is more reliableby joint analysis of tags in the Semantic Field,rather than investigating one tag at a time in thewhole tag list, we analyze the whole SemanticField as a unit.
By utilizing the context informa-tion in Semantic Field, the problems of ambi-guous, noisy and incomplete tags are partiallytackled.
Third, we perform normalization toestimate the importance of Semantic Field,which is discussed in Section 3.1.
If the value islarge, it suggests that most of the tags in theimage support the Semantic Field; that is, theprobability that the target concept is the focusof the image is high, and vice versa.
Such a de-sign aims to minimize the effects of noisy andambiguous tags.3.1 A Probabilistic ModelWe denotexC as a target concept that appearsin the content of an image.
We want to deter-mine the set of tags that are related toxC fromthe user-supplied tags by building a SemanticFieldiSF for each image.
The probability ofthe appearance of concept )|( ix SFCP can becomputed as:)()()|()|(ixxiix SFPCPCSFPSFCP(1)For the purpose of collecting and annotatingimages and simplifying the model, we did notconsider the prior knowledge for each image.Thus, the prior probability P(Cx) can be viewedas a constant with respect to a concept Cx.
Inaddition, the range of the normalization factorP(SFi) is expected to be small, which will notaffect the annotation of web images.
This as-sumption is reasonable due to the fact that thereare a large number of different tags, and thesetags can be combined to form any SemanticField in an arbitrary manner.
The number ofcombinations is exponential to the number ofpossible tags available.
This is also evident bythe observation that most tag lists associatedwith the images are unique.
In other words, twoimages with the same Semantic Field are sel-dom found in reality.
With these in mind, Equa-tion (1) can be approximated and simplified to:)|()|( xiix CSFPSFCP  (2)Given a Semantic FieldiSF , it may include nrelated tagsnTTTT ,...,,, 321 .
Thus Equation (2) isexpanded to:)|,...,()|( 21 xnxi CTTTPCSFP  (3)Two obvious approaches to compute Equa-tion (3) are using the product of the individualterms or chain rule decomposition.
However,we consider the individual terms to be inter-dependent and the chain rule decomposition isnot easy to compute.
To simplify the model, weemploy the normalized linear fusion to expandEquation (3) as follows:TN)|()|,...,P(T 121nixixnCTPCTT (4)The normalization factor is the total number(TN) of tags in the image tag list.3.2 Using Multiple External SourcesTo estimate the probability of a tag Ti given atarget concept Cx, i.e., P(Ti|Cx), we considerboth the domain knowledge and general know-ledge acquired from Internet.
For the former,we utilize the co-occurrence statistics of tags inimages which can be computed offline fromany web image corpus.
For the latter, we em-ploy WordNet and Wikipedia for inferring therelatedness between tags and a target concept.Combining different knowledge sources, theprobability is estimated as:)|()|()|()|( ___ xcoixwikiixwdixi CTPCTPCTPCTP  (5)where Ti_wd, Ti_wiki, Ti_co represent the tag occur-rences in WordNet, Wikipedia and co-occurrence statistics, respectively.To compute Equation (5), we query differentinformation sources using the target concept Cx.In WordNet, because the sense of the conceptusually refers to the most common sense in ourcorpus, we choose the most common sense(noun) as the target.
Using Figure 2 as an ex-ample, the concept "bear" is defined in Word-Net as ?massive plantigrade carnivorous or om-nivorous mammals with long shaggy coats andstrong claws?.
In Wikipedia, with Figure 3 asan example, the related page is downloaded to1304describe the concept "bear".
For the co-occurrence statistics of the tag lists, we estimatetheir values from co-occurrence informationfrom the image corpus.
With the above know-ledge, we compute the conditional probabilityof a tag being related to Cx as:)(#),(#)|(xxjxj CCTCTP  (6)where j = {wd, wiki, co}, #(Tj, Cx) indicates thenumber of times the tag and the concept co-occur in an information source, and #(Cx) de-notes the number of times the concept Cx appearin the information source.
In addition, we em-ploy an add-one smoothing approach [Jurafskyand Martin 2000] to further process the results.Figure 2: The information in WordNetFigure 3: The information in Wikipedia.Given a concept with a special sense, for allthe tags in the corpus, we can obtain the condi-tional probabilities of each tag Ti based on Equ-ation (5).
We rank the tags according to)C|P(T xi .
To reduce computations, we selectthe top N (N=200) tags as the highly relatedtags to a given concept and place them in a dic-tionary.3.3 Building Semantic Field for Image An-notationWe now build the Semantic Fields to rank theimages with respect to concept Cx.
The detailedalgorithm is shown in Figure 4.Input:1) Given a target concept, we rank all the tagsin the corpus based on Equation (5).2) Given a web image, we have a list of anno-tation tags ( 121 ,..., nlll ).Step 1: Generate a dictionary (D) based on topN tagsStep 2: For (i=1; i<n1; i++)If ( Dli  ) then put il into the SemanticField for the image.Step 3: Annotate the images and compute theprobability of the occurrence of theconcept via Equation (4)Figure 4: The algorithm for building the Se-mantic Fields and annotating the im-ages.The algorithm comprises three steps:1. bear 2. bears 3. polar 4. species5.
panda 6. cubs 7. giant 8. grizzly9.teddy 10. pandas ?
?Table 1: The top 10 tags for concept ?bear?
inmost common sense.First, given a target concept with a specificsense, we generate a dictionary based on the topN candidate tags as discussed in Section 3.2.Table 1 shows the top 10 tags in the dictionaryfor the concept ?bear?
with the most commonsense.
As we want to distinguish single andplural noun for different visual concepts, we donot employ the stemming algorithm.
Althoughthe results are not ideal, we find that manyhighly related words are included in the dictio-nary.Second, we infer the annotation tags of theimage from the dictionary and use that to buildthe Semantic Fields.
Figure 1 demonstrates theresulting of Semantic Fields for images in Ta-ble 2.Third, we assign the tags to images based ontheir Semantic Fields.
Because most of the tagsin Figure 1(a) and 1(d) are highly relevant to?bear?
with the most common sense, we assignthe semantics to these two images with highprobabilities.
Thus, the problem of incompletetags is tackled in this case.
On the other hand,since most of the tags in Figure 1(b) and 1(c)fail to support the concept ?bear?
with the most1305common sense (the Semantic Field obtains lessthan 20% of tags?
support), we only assign thesemantics with very low probabilities.
Thus,the ambiguous and noisy problem can be par-tially tackled.Semantic Field forFigure 1 (a){zoo, bear, polar}Semantic Field forFigure 1 (b){bear, bears}Semantic Field forFigure 1 (c){bear}Semantic Field forFigure 1 (d){animals, pandas, zoos}Table 2: Semantic Fields of images in Figure 1.4 ExperimentsIn this section, we first introduce the test-bedand measurement of the experiments.
We thenreport the results and compare them with thestate-of-the-art systems tested on NUS-WIDEcorpus.The NUS-Wide corpus (Chua et al 2009) in-cludes 269,648 images with 5,018 user-provided tags, and the ground-truth for 81 con-cepts for the entire database.
These concepts aregrouped into six different categories: graph,program, scene /location, event/activities,people and object.
The choice of concepts isbased on the generality and popularity in Flickr,the distributions in different categories and thecommon interests of the multimedia community.This corpus includes two parts.
The first partcontains 161,789 images to be used for trainingand the second part contains 107,859 images isused for testing.The performance of the system is measuredusing the mean average precision (MAP) basedon all the test images for all the 81 concepts.This is the same as the evaluation used inTRECVID.
The MAP combines precision andrecall into one performance value.
Let},...,,{ 21 kk iiip  be a ranked version of theresulting set A.
At any given rank k, let kpR be the number of relevant images in the top k ofp, where |R| is the total number of relevant im-ages.
Then MAP for the 81 concepts Ci is de-fined as:)](||1[8111811kAkkCikpRRMAPi (6)where the indictor function 1)( ki if Rik  and0 otherwise.4.1 Comparison with the State-of-the-ArtSystemsWe compare our approach against the re-ported systems on NUS-WIDE corpus.MAPA visual based k-NN systemA visual and taginformation basedk-NN systemOur approachFigure 5: The comparison with the state-of-the-art systemIn our approach, we employ the SemanticField to annotate the images, which requiresneither training data nor visual analysis, and isrunning directly on the test data.
In contrast tothe two previous approaches in Section 2.3, theinput to Semantic Field is simply the tag list ofan image.
Figure 5 shows the performancecomparisons among the three tested approaches.As compared to (Chua et al 2009) and (Gao etal.
2009), which exhibit the best performanceon NUS-WIDE so far, Semantic Field achievesa MAP of 0.4198 which shows a 45.4% im-provement.The reason for the superior performance ofour approach is that there is insufficient trainingdata, which means that most learning-basedsystems could not perform well.
As seen inFigure 6(a), 44% of concepts have less than1,000 positive training data.
This is insufficientfor training the classifiers for the visual con-cepts.
Take the visual concept ?flag?
as the ex-ample.
Considering that there are at least 200national flags from different countries and re-gions, not to mention other types of flags suchas holiday flag, there are large variations inconcept "flag" as shown in Figure 6(b).
Henceit is difficult to train a classifier with visualanalysis by having only 214 positive trainingsamples.
This suggests that there may be a large1306gap between the training and test data.
On theother hand, because web images include notonly visual features but also text information,we could employ text analysis to infer the visu-al concept.
The advantages of our SemanticField approach are that we could analyze mul-tiple information sources to reduce the text var-iations and the performance of our approach isindependent of the training data and visual fea-tures.
With the increasing size of the corpus, theproblems of few positive training data and largevisual diversity between training and test datawill be exacerbated.
This is the reason why ourapproach is more robust than those based onvisual analysis and traditional learning-basedapproaches.
(a) The distribution of positive training data inNUS-Wide corpus.
(b) Different color and different shapes for the con-cept ?flag?
in NUS-Wide corpus.Figure 6: Various visual patterns need a lot oftraining data4.2 The Noisy, Ambiguous and IncompleteTag ProblemsWe design the second experiment to evaluatethe ability of our algorithm to tackle the noisyand ambiguous and incomplete tag problem inuser-supplied tags.
The baseline system is akeyword (tag) matching algorithm.
That is, ifthe image contains the keyword in the tag list,the algorithm will regard it as relevant to theconcept; otherwise, it is irrelevant.
The resultsare shown in Figure 7.We found that our approach achieves a rela-tive improvement of 38% as compared to thekeyword matching approach.
This is becausethe Semantic Field approach selects and analyz-es a group of tags as a whole, which providesessential context information and reduces theeffects of noisy, ambiguous and incomplete tags.Figure 7: Comparison with keyword matchingapproachFor completeness, we also evaluate the sys-tem using the Equations (7) and (8) accordingto the top k images (k=1000, 2000, 3000, 4000,5000).NAptagPNi ii1 )(#)(#)( (7)1#( )#( )( )Nii ipTR tagN(8)We use pi to represent the number of imageswith the target concept and Ai to represent thenumber of retrieved images for tag i. N denotesthe number of different detected concepts (tags)in the ground truth set.
In this corpus, the valueof the N is 81. iT is the number of the groundtruth for a certain target concept.Figure 8: Comparison in precision on top-k im-age ranking.
The x-axis indicates the value of k,while the y-axis shows the P(tag).1307Figure 9: Comparison in recall on top-k imageranking.
The x-axis indicates the value of k,while the y-axis shows the R(tag).Figures 8 and 9 report the performance inprecision and recall respectively.
From the re-sults, we find that our approach is better thanthat of the baseline system in both precision andrecall.
This is because on one hand the Seman-tic Field tackles the ambiguous and noisy tagproblems so that we could improve the preci-sion.
On the other hand, the Semantic Fieldanalysis includes many highly related tags,which tackle the incomplete tags problem sothat it could improve the performance in recall.4.3 Importance of Multi-source Informa-tionSemantic Fields combine three informationsources: WordNet, Wikipedia and the tag?s co-occurrence information in the NUS-Wide cor-pus.
We design the third experiment to evaluatethe contribution of each information source.The results are shown in Figure 10.Figure 10: The comparison between usingsingle information source and fusion ofmultiple information sources.From Figure 10, we find that theperformance of using WordNet alne obtainsthe worst result.
This is because the number oftags carries the most common sense is limitedand there are some noisy words in thedescription.
For example, in Figure 2, theoccurrence of the word ?long?
does not implythe occurrence of the concept ?bear?.
Due to thelack of further information, using WordNetalone can hardly remove the noisy tag "long".The test result shows that such noisyinformation significantly degrade theperformance of the system.
This suggests theimportance of incorperating other sources ofinformaiton to provide more completeinformation for the analysis.We can also observe that using Wikipedia ortag co-occurrence shows comparatively betterperformance.
This is because both informationsources include abundance information foranalysis.
Thus, compared to the keyword-basedapproach, the performance of the systemsshows around 17% improvement.
Finally,fusing the three information sources results inthe best MAP performance.
This is becauseinformation from different sourcescomplements each other and helps in reducingthe effects of the noisy, ambiguous andincomplete tags.5 ConclusionIn this paper, we proposed the use of SemanticField to annotate web images.
It could reducethe influences of noisy, ambiguous and incom-plete tags so that the quality of the tags assignedto the web image can be improved.
Our expe-riments showed that our approach is more ro-bust and could achieve 38% improvement inMAP as compared to the learning-based andvisual analysis approaches when there is suffi-cient text information.
Also the fusion of mul-tiple information sources could further boostthe performance of the system.The work is only the beginning.
Futureworks include the followings.
First, as multi-media data includes multiple modality features,how to fuse them to improve the performanceof the system is an important problem.
Second,current version of our algorithm only couldidentify one sense of the concept.
How to dis-tinguish among different senses of the conceptis also an urgent task.
Third, we will exploremore semantic relations from Wordnet, Wiki-pedia and so on.ReferencesM.
Ames and M. Naaman (2007), ?Why We Tag:Motivations for Annotation in Mobile and onlineMedia?.
In Proceedings of the SIGCHI confe-1308rence on Human factors in computing systems, pp.971 ?
980.C.
F. Baker and C. J. Fillmore and J.
B. Lowe (1998)?The Berkeley FrameNet Project?, Proceedings ofthe 36th Annual Meeting of the Association forComputational Linguistics pp.
86-90.K.
Bischoff, C. S. Firan, W. Nejdl, R. Paiu (2008),?Can All Tags be Used for Search?, In Proceed-ings of the 17th ACM conference on Informationand knowledge management, pp.
193-202.T.
S. Chua, J. H. Tang, R. C. Hong, H. J. Li, Z. P.Luo, and Y. T. Zheng (2009), "NUS-WIDE: AReal-World Web Image Database from NationalUniversity of Singapore", ACM InternationalConference on Image and Video Retrieval.B.
M. David, A. Y. Ng and M. I. Jordan (2003), ?La-tent Dirichlet Allocation?, Journal of MachineLearning Research 3: 993-1022.P.
Duygulu and K. Barnard (2002), ?Object recogni-tion as machine translation: learning a lexicon fora fixed image vocabulary?, In Proceedings of the7th European Conference on Computer Vision, 4:97-112.W.
A. Gale and K. Church and D. Yarowsky (1992),?A method for disambiguating word sense in acorpus?.
Computers and the Humanities.
26 pp.415-439.S.
H. Gao, L. T. Chia and X. G. Cheng, (2009) ?Un-derstanding Tag-Cloud and Visual Features forBetter Annotation of Concepts in NUS-Wide Da-taBase?
, In Proceedings of WSMC 2009.M.
F. Garrett (1992), ?Lexical Retrieval Processes:Semantic Filed Effects?, in Lehrer and Kittay Eds.Frames, Fields and Contrasts: New Essays in Se-mantic and Lexical Organization.
pp.
377-396Hillsdale: Lawrence Erlbaum.R.
E. Grandy (1992), ?Semantic Fields, Prototypes,and the Lexicon?, in Lehrer and Kittay Eds.Frames, Fields and Contrasts: New Essays in Se-mantic and Lexical Organization.
pp.
103-122Hillsdale: Lawrence Erlbaum.T.
Hofmann (1999), ?Probabilitic Latent SemanticIndexing?, In Proceedings of the 22rd Annual In-ternational SIGIR Conference on Research andDevelopment in Information Retrieval.J.
Jeon, V. Lavrenko, and R. Manmatha (2003),?Automatic Image annotation and retrieval usingcross-media relevance modes?, In Proceedings ofthe 26th Annual International ACM SIGIR Confe-rence on Research and Development in Informa-tion Retrieval, pp.
119-126.Y.
Jin, L. Khan, L. Wang and M. Awad (2005),?
Image Annnotations by Combining multipleEvidence & WordNet?, In Proceedings of theACM Multimedia Conference, pp.
706-715.D.
Jurafsky and J. H. Martin (2000), ?Speech andlanguage processing?, published by Prentice-HallInc.L.
S. Kennedy, S. F. Chang and I. V. Kozintsev(2006), ?To search or To Label?, In Proceedingsof MIR 2006, pp.
249-258.R.
M. V. Lavrenko and J. Jeon (2003), ?
A modelfor learning the semantic of pictures?, In Proceed-ings of the 17th Annual Conference on Neural In-formation Processing Systems.C.
Manning and H. Schutze (1999).
?Foundations ofStatistical Natural Language Processing?.
MITPress, Cambridge, MA.K.
Matusiak (2006), ?Towards user-centered index-ing in digitial image collections?, OCLC systemsand Services, 22(4): pp.
283-298.R.
Navigli (2009), ?Word Sense Disambiguation: ASurvey?, ACM Computing Surveys, Vol.
41, No.2.
Article 10.N.
C. Rowe (1994) ?Inferring depictions in naturallanguage captions for efficient access to picturedata?, Information Process & Management Vol.30 No 3. pp.
379-388.G.
Wang, T. S. Chua and Y. C. Wang (2003), ?Ex-tracting Key Semantic Terms from ChineseSpeech Query for Web Searches?.
In proceedingof 41st Annual Meeting of the Association forComputational Linguistics pp.
248-255.G.
Wang, T. S. Chua, M. Zhao (2008), "ExploringKnowledge of Sub-domain in a Multi-resolutionBootstrapping Framework for Concept Detectionin News Video", In Proceeding of the 16th ACMinternational Conference on Multimedia.
pp.
249-258.Merriam Webster Online dictionary (2010), Availa-ble at http://www.merriam-webster.com/1309
