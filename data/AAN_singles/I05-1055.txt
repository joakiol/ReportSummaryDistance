A Machine Learning Approach to SentenceOrdering for Multidocument Summarizationand Its EvaluationDanushka Bollegala, Naoaki Okazaki, and Mitsuru IshizukaUniversity of Tokyo, JapanAbstract.
Ordering information is a difficult but a important task fornatural language generation applications.
A wrong order of informationnot only makes it difficult to understand, but also conveys an entirelydifferent idea to the reader.
This paper proposes an algorithm that learnsorderings from a set of human ordered texts.
Our model consists of a setof ordering experts.
Each expert gives its precedence preference betweentwo sentences.
We combine these preferences and order sentences.
Wealso propose two new metrics for the evaluation of sentence orderings.Our experimental results show that the proposed algorithm outperformsthe existing methods in all evaluation metrics.1 IntroductionThe task of ordering sentences arises in many fields.
Multidocument Summa-rization (MDS) [5], Question and Answer (QA) systems and concept to textgeneration systems are some of them.
These systems extract information fromdifferent sources and combine them to produce a coherent text.
Proper orderingof sentences improves readability of a summary [1].
In most cases it is a trivialtask for a human to read a set of sentences and order them coherently.
Hu-mans use their wide background knowledge and experience to decide the orderamong sentences.
However, it is not an easy task for computers.
This paper pro-poses a sentence ordering algorithm and evaluate its performance with regardto MDS.MDS is the task of generating a human readable summary from a given set ofdocuments.
With the increasing amount of texts available in electronic format,automatic text summarization has become necessary.
It can be considered as atwo-stage process.
In the first stage the source documments are analyzed and aset of sentences are extracted.
However, the document set may contain repeatinginformation as well as contradictory information and these challenges shouldbe considered when extracting sentences for the summary.
Researchers havealready investigated this problem and various algorithms exist.
The second stageof MDS creates a coherent summary from this extract.
When summarizing asingle document, a naive strategy that arranges extracted sentences accordingto the appearance order may yield a coherent summary.
However, in MDS theextracted sentences belong to different source documents.
The source documentsR.
Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
624?635, 2005.c?
Springer-Verlag Berlin Heidelberg 2005A Machine Learning Approach to Sentence Ordering 625may have been written by various authors and on various dates.
Therefore wecannot simply order the sentences according to the position of the sentences inthe original document to get a comprehensible summary.This second stage of MDS has received lesser attention compared to thefirst stage.
Chronological ordering; ordering sentences according to the pub-lished date of the documents they belong to [6], is one solution to this problem.However, showing that this approach is insufficient, Barzilay [1] proposed anrefined algorithm which integrates chronology ordering with topical relatednessof documents.
Okazaki [7] proposes a improved chronological ordering algorithmusing precedence relations among sentences.
His algorithm searches for an orderwhich satisfies the precedence relations among sentences.
In addition to thesestudies which make use of chronological ordering, Lapata [3] proposes a prob-abilistic model of text structuring and its application to the sentence ordering.Her system calculates the conditional probabilities between sentences from acorpus and uses a greedy ordering algorithm to arrange sentences according tothe conditional probabilities.Even though these previous studies proposed different strategies to decide thesentence ordering, the appropriate way to combine these different methods toobtain more robust and coherent text remains unknown.
In addition to these ex-isting sentence ordering heuristics, we propose a new method which we shall callsuccession in this paper.
We then learn the optimum linear combination of theseheuristics that maximises readability of a summary using a set of human-madeorderings.
We then propose two new metrics for evaluating sentence orderings;Weighted Kendall Coefficient and Average Continuity.
Comparing with an in-trinsic evaluation made by human subjects, we perform a quantitative evaluationusing a number of metrics and discuss the possiblity of the automatic evaluationof sentence orderings.2 MethodFor sentences taken from the same document we keep the order in that docu-ment as done in single document summarization.
However, we have to be carefulwhen ordering sentences which belong to different documents.
To decide the or-der among such sentences, we implement five ranking experts: Chronological,Probabilistic, Topical relevance, Precedent and Succedent.
These experts returnprecedence preference between two sentences.
Cohen [2] proposes an elegantlearning model that works with preference functions and we adopt this learn-ing model to our task.
Each expert e generates a pair-wise preference functiondefined as following:PREFe(u, v, Q) ?
[0, 1].
(1)Where, u, v are two sentences that we want to order; Q is the set of sentenceswhich has been already ordered.
The expert returns its preference of u to v. Ifthe expert prefers u to v then it returns a value greater than 0.5.
In the extremecase where the expert is absolutely sure of preferring u to v it will return 1.0.On the other hand, if the expert prefers v to u it will return a value lesser than626 D. Bollegala, N. Okazaki, and M. Ishizuka0.5.
In the extreme case where the expert is absolutely sure of preferring v to uit will return 0.
When the expert is undecided of its preference between u and vit will return 0.5.The linear weighted sum of these individual preference functions is taken asthe total preference by the set of experts as follows:PREFtotal(u, v, Q) =?e?EwePREFe(u, v, Q).
(2)Therein: E is the set of experts and we is the weight associated to expert e ?
E.These weights are normalized so that the sum of them is 1.
We use the Hedgelearning algorithm to learn the weights associated with each expert?s preferencefunction.
Then we use the greedy algorithm proposed by Cohen [2] to get anordering that approximates the total preference.2.1 Chronological ExpertChronological expert emulates conventional chronological ordering [4,6] whicharranges sentences according to the dates on which the documents were publishedand preserves the appearance order for sentences in the same document.
Wedefine a preference function for the expert as follows:PREFchro(u, v, Q) =????
?1 T (u) < T (v)1 [D(u) = D(v)] ?
[N(u) < N(v)]0.5 [T (u) = T (v)] ?
[D(u) = D(v)]0 otherwise.
(3)Therein: T (u) is the publication date of sentence u; D(u) presents the uniqueidentifier of the document to which sentence u belongs; N(u) denotes the linenumber of sentence u in the original document.
Chronological expert gives 1(preference) to the newly published sentence over the old and to the prior overthe posterior in the same article.
Chronological expert returns 0.5 (undecided)when comparing two sentences which are not in the same article but have thesame publication date.2.2 Probabilistic ExpertLapata [3] proposes a probabilistic model to predict sentence order.
Her modelassumes that the position of a sentence in the summary depends only upon thesentences preceding it.
For example let us consider a summary T which hassentences S1, .
.
.
, Sn in that order.
The probability P (T ) of getting this order isgiven by:P (T ) =n?i=1P (Sn|S1, .
.
.
, Sn?i).
(4)She further reduces this probability using bi-gram approximation as follows.P (T ) =n?i=1P (Si|Si?1) (5)A Machine Learning Approach to Sentence Ordering 627She breaks each sentence into features and takes the vector product of featuresas follows:P (Si|Si?1) =?
(a<i,j>,a<i?1,k>)?Si?Si?1P (a<i,j>, a<i?1,k>).
(6)Feature conditional probabilities can be calculated using frequency counts offeatures as follows:P (a<i,j>|a<i?1,k>) =f(a<i,j>, a<i?1,k>)?a<i,j>f(a<i,j>, a<i?1,k>).
(7)Lapata [3] uses nouns,verbs and dependency structures as features.
Where asin our expert we implemented only nouns and verbs as features.
We performedback-off smoothing on the frequency counts in equation 7 as these values weresparse.
Once these conditional probabilities are calculated, for two sentences u,vwe can define the preference function for the probabilistic expert as follows:PREFprob(u, v, Q) ={1+P (u|r)?P (v|r)2 Q = 1+P (u)?P (v)2 Q = .
(8)Where, Q is the set of sentences ordered so far and r ?
Q is the lastly orderedsentence in Q.
Initially, Q is null and we prefer the sentence with higher absoluteprobability.
When Q is not null and u is preferred to v, i.e.
P (u|r) > P (v|r),according to definition 8 a preference value greater than 0.5 is returned.
If v ispreferred to u, i.e.
P (u|r) < P (v|r), we have a preference value smaller than 0.5.When P (u|r) = P (v|r), the expert is undecided and it gives the value 0.5.2.3 Topical Relevance ExpertIn MDS, the source documents could contain multiple topics.
Therefore, theextracted sentences could be covering different topics.
Grouping the extractedsentences which belong to the same topic, improves readability of the summary.Motivated by this fact, we designed an expert which groups the sentences whichbelong to the same topic.
This expert prefers sentences which are more similarto the ones that have been already ordered.
For each sentence l in the extractwe define its topical relevance, topic(l) as follows:topic(l) = maxq?Qsim(l, q).
(9)We use cosine similarity to calculate sim(l, q).
The preference function of thisexpert is defined as follows:PREFtopic(u, v, Q) ={0.5 [Q = ] ?
[topic(u) = topic(v)]1 [Q = ] ?
[topic(u) > topic(v)]0 otherwise.
(10)Where,  represents the null set, u,v are the two sentences under considera-tion and Q is the block of sentences that has been already ordered so far inthe summary.628 D. Bollegala, N. Okazaki, and M. Ishizuka37PQTFGTGF5GPVGPEGU.
5WOOCT[Fig.
1.
Topical relevance expert2.4 Precedent ExpertWhen placing a sentence in the summary it is important to check whether thepreceding sentences convey the necessary background information for this sen-tence to be clearly understood.
Placing a sentence without its context beingstated in advanced, makes an unintelligible summary.
As shown in figure 2, foreach extracted sentence l, we can compare the block of text that appears beforeit in its source document (P ) with the block of sentences which we have orderedso far in the summary (Q).
If P and Q matches well, then we can safely as-sume that Q contains the necessary background information required by l. Wecan then place l after Q.
Such relations among sentences are called precedencerelations.
Okazaki [7] proposes precedence relations as a method to improve thechronological ordering of sentences.
He considers the information stated in thedocuments preceding the extracted sentences to judge the order.
Based on thisidea, we define precedence pre(l) of the extracted sentence l as follows:pre(l) = maxp?P,q?Qsim(p, q).
(11)l2&QEWOGPV&5WOOCT[3Fig.
2.
Precedent expertHere, P is the set of sentences preceding the extract sentence l in the originaldocument.
We calculate sim(p, q) using cosine similarity.
The preference functionfor this expert can be written as follows:PREFpre(u, v, Q) ={0.5 [Q = ] ?
[pre(u) = pre(v)]1 [Q = ] ?
[pre(u) > pre(v)]0 otherwise.
(12)A Machine Learning Approach to Sentence Ordering 629&QEWOGPV&5WOOCT[3r-7PQTFGTGF5GPVGPEGU.lFig.
3.
Succedent expert2.5 Succedent ExpertWhen extracting sentences from source documents, sentences which are similar tothe ones that are already extracted, are usually ignored to prevent repetition ofinformation.
However, this information is valuable when ordering sentences.
Forexample, a sentence that was ignored by the sentence extraction algorithm mightturn out to be more suitable when ordering the extracted sentences.
However, weassume that the sentence ordering algorithm is independent from the sentence ex-traction algorithmand therefore does not possess this knowledge regarding the leftout candidates.
This assumption improves the compatibility of our algorithm as itcan be used to order sentences extracted by any sentence extraction algorithm.
Wedesign an expert which uses this information to order sentences.Let us consider the siuation depicted in Figure 3 where a block Q of text isorderd in the summary so far.
The lastly ordered setence r belongs to documentD in which a block K of sentences follows r. The author of this document assumesthat K is a natural consequence of r. However, the sentence selection algorithmmight not have selected any sentences from K because it already selected somesentences with this information from some other document.
Therefore, we searchthe extract L for a sentence that best matches with a sentence in K. We definesuccession as a measure of this agreement(13) as follows:succ(l) = maxk?Ksim(l, k).
(13)Here, we calculate sim(l, k) using cosine similarity.
Sentences with higher succes-sion values are preferred by the expert.
The preference function for this expertcan be written as follows:PREFsucc(u, v, Q) ={0.5 [Q = ] ?
[succ(u) = succ(v)]1 [Q = ] ?
[succ(u) > succ(v)]0 otherwise.
(14)2.6 Ordering AlgorithmUsing the five preference functions described in the previous sections, we computethe total preference function of the set of experts as defined by equation 2.
Sec-tion 2.7 explains the method that we use to calculate the weights assigned to eachexpert?s preference.
In this section we will consider the problem of finding an orderthat satisfies the total preference function.
Finding the optimal order for a given630 D. Bollegala, N. Okazaki, and M. Ishizukatotal preference function is NP-complete [2].
However, Cohen [2] proposes a greedyalgorithm that approximates the optimal ordering.
Once the unordered extract Xand total preference (equation 2) are given, this greedy algorithm can be used togenerate an approximately optimal ordering function ?
?.let V = Xfor each v ?
V do?
(v) =?u?VPREF(v, u, Q) ?
?u?VPREF(u, v, Q)while V is non-empty dolet t = arg maxu?V ?
(u)let ??
(t) = |V |V = V ?
{t}for each v ?
V do?
(v) = ?
(v) + PREF(t, u) ?
PREF(v, t)endwhile2.7 Learning AlgorithmCohen [2] proposes a weight allocation algorithm that learns the weights associ-ated with each expert in equation 2.
We shall explain this algorithm in regardto our model of five experts.Rate of learning ?
?
[0, 1], initial weight vector w1 ?
[0, 1]5, s.t.
?e?E w1e = 1.Do for t = 1, 2, .
.
.
, T where T is the number of training examples.1.
Get Xt; the set of sentences to be ordered.2.
Compute a total order ?
?t which approximates,PREFttotal(u, v, Q) =?e?EPREFte(u, v, Q).We used the greedy ordering algorithm described in section 2.6 to get ??t.3.
Order Xt using ??t.4.
Get the human ordered set F t of Xt.
Calculate the loss for each expert.Loss(PREFte, Ft) = 1 ?
1|F |?
(u,v)?FPREFte(u, v, Q) (15)5.
Set the new weight vector,wt+1e =wte?Loss(PREFte,Ft)Zt(16)where, Zt is a normalization constant, chosen so that,?e?E wt+1e = 1.A Machine Learning Approach to Sentence Ordering 631In our experiments we set ?
= 0.5 and w1i = 0.2.
To explain equation 15 let usassume that sentence u comes before sentence v in the human ordered summary.Then the expert must return the value 1 for PREF(u,v,Q).
However,if the expertreturns any value less than 1, then the difference is taken as the loss.
We do thisfor all such sentence pairs in F .
For a summary of length N we have N(N ?1)/2such pairs.
Since this loss is taken to the power of ?, a value smaller than 1, thenew weight of the expert gets changed according to the loss as in equation 16.3 EvaluationIn addition to Kendall?s ?
coefficient and Spearman?s rank correlation coefficientwhich are widely used for comparing two ranks, we use sentence continuity [7]as well as two metrics we propose; Weighted Kendall and Average Continuity.3.1 Weighted Kendall CoefficientThe Kendall?s ?
coefficient is defined as following:?
= 1 ?
2QnC2.
(17)Where, Q is the number of discordant pairs and nC2 is the number of combi-nations that can be generated from a set of n distinct elements by taking twoelements at a time with replacement.
However, one major drawback of this met-ric when evaluating sentence orderings is that, it does not take into considerationthe relative distance d between the discordant pairs.
However, when reading atext a human reader is likely to be more sensitive to a closer discordant pair thana discordant pair far apart.
Therefore, a closer discordant pair is more likely toharm the readability of the summary compared to a far apart discordant pair.
Inorder to reflect these differences in our metric, we use an exponentially decreasingweight function as follows:h(d) ={exp(1 ?
d) d ?
10 else.
(18)Here, d is the number of sentences that lie between the two sentences of thediscordant pair.
Going by the traditional Kendall?s ?
coefficient we defined ourweighted Kendall coefficient as following, so that it becomes a metric in [1, ?1]range.
?w = 1 ?2?d h(d)?ni=1 h(i)(19)3.2 Average ContinuityBoth Kendall?s ?
coefficient and the Weighted Kendall coefficient measure dis-cordants between ranks.
However, in the case of summaries, we need a metricwhich expresses the continuity of the sentences.
A summary which can be read632 D. Bollegala, N. Okazaki, and M. Ishizukacontinuously is better compared to a one that cannot.
If the ordered extractcontains most of the sentence blocks of the reference summary then we cansafely assume that it is far more readable and coherent to a one that is not.Sentence n-gram counts of continuous sentences give a rough idea of this kindof continuity.For a summary of length N there are N ?
n + 1 possible sentence n-gramsof length n. Therefore, we can define a precision Pn of continuity length n as:Pn =number of matched n-gramsN ?
n + 1 .
(20)Due to sparseness of higher order n-grams Pn decreases in an exponential-likecurve with n. Therefore, we define Average Continuity as the logrithmic averageof Pn as follows:Average Continuity = exp(134?n=2log(Pn)) (21)We add a small quantity ?
to numerator and denominator of Pn in equation20 so that the logarithm will not diverge when n-grams count is zero.
We used?
= 0.01 in our evaluations.
Experimental results showed that taking n-grams upto four gave contrasting results because the n-grams tend to be sparse for largern values.
BLEU(BiLingual Evaluation Understudy) proposed by Papineni [8]for the task of evaluating machine translations has an analogical form to ouraverage continuity.
In BLEU, a machine translation is compared against multiplereference translations and precision values are calculated using word n-grams.BLEU is then defined as the logarithmic average of these precision values.4 ResultsWe used the 3rd Text Summarization Challenge (TSC) corpus for our exper-iments.
TSC1 corpus contains news articles taken from two leading Japanesenewspapers; Mainichi and Yomiuri.
TSC-3 corpus contains human selected ex-tracts for 30 different topics.
However, in the TSC corpus the extracted sentencesare not ordered to make a readable summary.
Therefore, we first prepared 30summaries by ordering the extraction data of TSC-3 corpus by hand.
We thencompared the orderings by the proposed algorithm against these human orderedsummaries.
We used 10-fold cross validation to learn the weights assigned toeach expert in our proposed algorithm.
These weights are shown in table 1.According to table 1, succedent, chronology and precedent experts have thehighest weights among the five experts and therefore almost entirely control theprocess of ordering.
Whereas probabilistic and topical relevance experts havealmost no influence on their decisions.
However, we cannot directly compare La-pata?s [3] approach with our probabilistic expert as we do not use dependency1 http://lr-www.pi.titech.ac.jp/tsc/index-en.htmlA Machine Learning Approach to Sentence Ordering 633Table 1.
Weights learnedExpert Chronological Probabilistic Topical Relevance Precedent SuccedentWeights 0.327947 0.000039 0.016287 0.196562 0.444102Table 2.
Comparison with Human OrderingSpearman Kendall Continuity Weighted Kendall Average ContinuityRO -0.267 -0.160 -0.118 -0.003 0.024PO 0.062 0.040 0.187 0.013 0.029CO 0.774 0.735 0.629 0.688 0.511LO 0.783 0.746 0.706 0.717 0.546HO 1.000 1.000 1.000 1.000 1.000.1%121412TGEKUKQPPFig.
4.
Precision vs sentence n-gram lengthstructure in our probability calculations.
Moreover, Topical relevance, Precedentand Succedent experts require other experts to guide them at the start as theyare not defined when Q is null.
This inter-dependency among experts makes itdifficult to interpret the results in table 1.
However, we could approximatelyconsider the values of the weights in table 1 as expressing the reliability of eachexpert?s decisions.We ordered each extract by five methods: Random Ordering (RO); Proba-bilistic Ordering (PO); Chronological Ordering (CO); Learned Ordering (LO);and HO (Human-made Ordering) and evaluated the orderings.
The results areshown in table 2.
Continuity precision, defined in equation 20, against the lengthof continuity n, is shown in figure 4.According to table 2 LO outperforms RO,PO and CO in all metrics.
ANOVAtest of the results shows a statistically significant difference among the five meth-ods compared in table 2 under 0.05 confidence level.
However, we could notfind a statistically significant difference between CO and LO.
Topical relevance,Precedent and Succedent experts cannot be used stand-alone to generate a total634 D. Bollegala, N. Okazaki, and M. Ishizukaordering because these experts are not defined at the start, where Q is null.These experts need Chronological and Probabilistic experts to guide them atthe beginning.
Therefore we have not compared these orderings in table 2.According to figure 4, for sentence n-grams of length up to 6, LO has thehighest precision (defined by equation 20) among the compared orderings.
POdid not possess sentence n-grams for n greater than two.
Due to the sparsenessof the higher order n-grams, precision drops in an exponential-like curve withthe length of sentence continuity n. This justifies the logarithmic mean in thedefinition of average continuity in equation 21.
A similar tendency could beobserved for the BLEU metric [8].     7PCEEGRVCDNG2QQT#EEGRVCDNG2GTHGEV*1.1%141Fig.
5.
Human EvaluationWe also performed a human evaluation of our orderings.
We asked two humanjudges to grade the summaries into four categories.
The four grades are; perfect:no further adjustments are needed, acceptable: makes sense even though thereis some room for improvement, poor: requires minor amendments to bring it upto the acceptable level, unacceptable: requires overall restructuring rather thanpartial revision.
The result of the human evaluation of the 60 (2?30) summariesis shown in figure 5.
It shows that most of the randomly ordered summaries(RO) are unacceptable.
Although both CO and LO have same number of perfectsummaries, the acceptable to poor ratio is better in LO.
Over 60 percent of LOis either perfect or acceptable.
Kendall?s coefficient of concordance (W), whichassesses the inter-judge agreement of overall ratings, reports a higher agreementbetween judges with a value of W = 0.937.Although relatively simple in implementation, the chronological orderingsworks satisfactorily in our experiments.
This is mainly due to the fact that theTSC corpus only contains news paper articles.
Barzilay [1] shows chronologicalordering to work well with news summaries.
In news articles, events normallyoccur in a chronological order.
To evaluate the true power of the other expertsin our algorithm, we need to experiment using other genre of summaries otherthan news summaries.A Machine Learning Approach to Sentence Ordering 6355 ConclusionThis paper described a machine learning approach to sentence ordering for mul-tidocument summarization.
Our method integrated all the existing approachesto sentence ordering while proposing new techniques like succession.
The resultsof our experiments revealed that our algorithm for sentence ordering did con-tribute to summary readability.
We plan to do further study on the sentenceordering problem in future work, extending our algorithm to other natural lan-guage generation applications.References1.
Regina Barzilay, Noemie Elhadad, and Kathleen McKeown.
Inferring strategiesfor sentence ordering in multidocument news summarization.
Journal of ArtificialIntelligence Research, 17:35?55, 2002.2.
W. W. Cohen, R. E. Schapire, and Y.
Singer.
Learning to order things.
Journal ofArtificial Intelligence Research, 10:243?270, 1999.3.
Mirella Lapata.
Probabilistic text structuring: Experiments with sentence ordering.Proceedings of the annual meeting of ACL, 2003., pages 545?552, 2003.4.
C.Y.
Lin and E. Hovy.
Neats:a multidocument summarizer.
Proceedings of theDocument Understanding Workshop(DUC), 2001.5.
Inderjeet Mani and Mark T. Maybury, editors.
Advances in automatic text summa-rization.
The MIT Press, 2001.6.
Kathleen McKeown, Judith Klavans, Vasileios Hatzivassiloglou, Regina Barzilay,and Eleazar Eskin.
Towards multidocument summarization by reformulation:Progress and prospects.
AAAI/IAAI, pages 453?460, 1999.7.
Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishizuka.
An integrated summa-rization system with sentence ordering using precedence relation.
ACM-TALIP, toappear in 2005.8.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
Bleu:a methodfor automatic evaluation of machine translation.
Proceedings of the 40th An-nual Meeting of the Association for Computational Linguistics (ACL), pages311?318, 2002.
