Proceedings of ACL-08: HLT, pages 488?495,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsWord Clustering and Word Selection based Feature Reduction for MaxEntbased Hindi NERSujan Kumar SahaIndian Institute of TechnologyKharagpur, West BengalIndia - 721302sujan.kr.saha@gmail.comPabitra MitraIndian Institute of TechnologyKharagpur, West BengalIndia - 721302pabitra@gmail.comSudeshna SarkarIndian Institute of TechnologyKharagpur, West BengalIndia - 721302shudeshna@gmail.comAbstractStatistical machine learning methods are em-ployed to train a Named Entity Recognizerfrom annotated data.
Methods like Maxi-mum Entropy and Conditional Random Fieldsmake use of features for the training purpose.These methods tend to overfit when the avail-able training corpus is limited especially if thenumber of features is large or the number ofvalues for a feature is large.
To overcomethis we proposed two techniques for featurereduction based on word clustering and se-lection.
A number of word similarity mea-sures are proposed for clustering words forthe Named Entity Recognition task.
A fewcorpus based statistical measures are used forimportant word selection.
The feature reduc-tion techniques lead to a substantial perfor-mance improvement over baseline MaximumEntropy technique.1 IntroductionNamed Entity Recognition (NER) involves locat-ing and classifying the names in a text.
NER isan important task, having applications in informa-tion extraction, question answering, machine trans-lation and in most other Natural Language Process-ing (NLP) applications.
NER systems have been de-veloped for English and few other languages withhigh accuracy.
These belong to two main cate-gories based on machine learning (Bikel et al, 1997;Borthwick, 1999; McCallum and Li, 2003) and lan-guage or domain specific rules (Grishman, 1995;Wakao et al, 1996).In English, the names are usually capitalizedwhich is an important clue for identifying a name.Absence of capitalization makes the Hindi NER taskdifficult.
Also, person names are more diverse in In-dian languages, many common words being used asnames.A pioneering work on Hindi NER is by Li andMcCallum (2003) where they used Conditional Ran-dom Fields (CRF) and feature induction to auto-matically construct only the features that are impor-tant for recognition.
In an effort to reduce overfit-ting, they use a combination of a Gaussian prior andearly-stopping.In their Maximum Entropy (MaxEnt) based ap-proach for Hindi NER development, Saha et al(2008) also observed that the performance of theMaxEnt based model often decreases when hugenumber of features are used in the model.
This isdue to overfitting which is a serious problem in mostof the NLP tasks in resource poor languages whereannotated data is scarce.This paper is a study on effectiveness of wordclustering and selection as feature reduction tech-niques for MaxEnt based NER.
For clustering weuse a number of word similarities like cosine sim-ilarity among words and co-occurrence, along withthe k-means clustering algorithm.
The clusters arethen used as features instead of words.
For impor-tant word selection we use corpus based statisticalmeasurements to find the importance of the words inthe NER task.
A significant performance improve-ment over baseline MaxEnt was observed after usingthe above feature reduction techniques.The paper is organized as follows.
The MaxEnt488based NER system is described in Section 2.
Vari-ous approaches for word clustering are discussed inSection 3.
Next section presents the procedure forselecting the important words.
In Section 5 experi-mental results and related discussions are given.
Fi-nally Section 6 concludes the paper.2 Maximum Entropy Based Model forHindi NERMaximum Entropy (MaxEnt) principle is a com-monly used technique which provides probability ofbelongingness of a token to a class.
MaxEnt com-putes the probability p(o|h) for any o from the spaceof all possible outcomes O, and for every h fromthe space of all possible histories H .
In NER, his-tory can be viewed as all information derivable fromthe training corpus relative to the current token.
Thecomputation of probability (p(o|h)) of an outcomefor a token in MaxEnt depends on a set of featuresthat are helpful in making predictions about the out-come.
The features may be binary-valued or multi-valued.
Given a set of features and a training corpus,the MaxEnt estimation process produces a model inwhich every feature fi has a weight ?i.
We cancompute the conditional probability as (Berger et al,1996):p(o|h) =1Z(h)?i?ifi(h,o) (1)Z(h) =?o?i?ifi(h,o) (2)The conditional probability of the outcome is theproduct of the weights of all active features, normal-ized over the products of all the features.
For ourdevelopment we have used a Java based open-nlpMaxEnt toolkit1.
A beam search algorithm is usedto get the most probable class from the probabilities.2.1 Training CorpusThe training data for the Hindi NER task is com-posed of about 243K words which is collectedfrom the popular daily Hindi newspaper ?DainikJagaran?.
This corpus has been manually anno-tated and contains about 16,491 Named Entities(NEs).
In this study we have considered 4 types1http://sourceforge.net/projects/maxent/Type FeaturesWord wi, wi?1, wi?2, wi+1, wi+2NE Tag ti?1, ti?2Digit infor-mationContains digit, Only digit, Fourdigit, Numerical wordAffix infor-mationFixed length suffix, Suffix list,Fixed length prefixPOS infor-mationPOS of words, Coarse-grainedPOS, POS based binary featuresTable 1: Features used in the MaxEnt based Hindi NERsystemof NEs, these are Person (Per), Location (Loc),Organization (Org) and Date (Dat).
To recognizeentity boundaries each name class N has 4 typesof labels: N Begin, N Continue, N End andN Unique.
For example, Kharagpur is annotatedas Loc Unique and Atal Bihari Vajpeyi is annotatedas Per Begin Per Continue Per End.
Hence,there are a total of 17 classes including one class fornot-name.
The corpus contains 6298 person, 4696location, 3652 organization and 1845 date entities.2.2 Feature DescriptionWe have identified a number of candidate featuresfor the Hindi NER task.
Several experiments wereconducted with the identified features, individuallyand in combination.
Some of the features are men-tioned below.
They are summarized in Table 1.Static Word Feature: Recognition of NE ishighly dependent on contexts.
So the surroundingwords of a particular word (wi) are used as fea-tures.
During our experiments different combina-tions of previous 3 words (wi?3...wi?1) to next 3words (wi+1...wi+3) are treated as features.
This isrepresented by L binary features where L is the sizeof lexicon.Dynamic NE tag: NE tags of the previous words(ti?m...ti?1) are used as features.
During decoding,the value of this feature for a word (wi) is obtainedonly after the computation of the NE tag for the pre-vious word (wi?1).Digit Information: If a word (wi) containsdigit(s) then the feature ContainsDigit is set to 1.This feature is used with some modifications also.OnlyDigit, which is set to 1 if the word contains489Feature Id Feature Per Loc Org Dat TotalF1 wi, wi?1, wi+1 61.36 68.29 52.12 88.9 67.26F2 wi, wi?1, wi?2, wi+1, wi+2 64.10 67.81 58 92.30 69.09F3 wi, wi?1, wi?2, wi?3, wi+1,wi+2, wi+360.42 67.81 51.48 90.18 66.84F4 wi, wi?1, wi?2, wi+1, wi+2,ti?1, ti?2, Suffix66.67 73.36 58.58 89.09 71.2F5 wi, wi?1, wi+1, ti?1, Suffix 69.65 75.8 59.31 89.09 73.42F6 wi, wi?1, wi+1, ti?1, Prefix 66.67 71 58.58 87.8 70.02F7 wi, wi?1, wi+1, ti?1, Prefix,Suffix70.61 71 59.31 89.09 72.5F8 wi, wi?1, wi+1, ti?1, Suffix,Digit70.61 75.8 60.54 93.8 74.26F9 wi, wi?1, wi+1, ti?1, POS (28tags)64.25 71 60.54 89.09 70.39F10 wi, wi?1, wi+1, ti?1, POS(coarse grained)69.65 75.8 59.31 92.82 74.16F11 wi, wi?1, wi+1, Ti?1, Suffix,Digit, NomPSP72.26 78.6 61.36 92.82 75.6F12 wi, wi?1, wi+1, wi?2, wi+2,Ti?1, Prefix, Suffix, Digit,NomPSP65.26 78.01 52.12 93.33 72.65Table 2: F-values for different features in the MaxEnt based Hindi NER systemonly digits, 4Digit, which is set to 1 if the wordcontains only 4 digits, etc.
are some modificationsof the feature which are helpful.Numerical Word: For a word (wi) if it is a nu-merical word i.e.
word denoting a number (e.g.
eka2(one), do (two), tina (three) etc.)
then the featureNumWord is set to 1.Word Suffix: Word suffix information is helpfulto identify the NEs.
Two types of suffix featureshave been used.
Firstly a fixed length word suffix(set of characters occurring at the end of the word) ofthe current and surrounding words are used as fea-tures.
Secondly we compiled list of common suf-fixes of place names in Hindi.
For example, pura,bAda, nagara etc.
are location suffixes.
We usedbinary feature corresponding to the list - whether agiven word has a suffix from the list.Word Prefix: Prefix information of a word maybe also helpful in identifying whether it is a NE.
A2All Hindi words are written in italics using the ?Itrans?transliteration.fixed length word prefix (set of characters occur-ring at the beginning of the word) of current andsurrounding words are treated as features.
List ofimportant prefixes, which are used frequently in theNEs, are also effective.Parts-of-Speech (POS) Information: The POSof the current word and the surrounding words areused as feature for NER.
We have used a Hindi POStagger developed at IIT Kharagpur, India which hasan accuracy about 90%.
We have used the POS val-ues of the current and surrounding words as features.We realized that the detailed POS tagging is notvery relevant.
Since NEs are noun phrases, the nountag is very relevant.
Further the postposition follow-ing a name may give a clue to the NE type.
So we de-cided to use a coarse-grained tagset with only threetags - nominal (Nom), postposition (PSP) and other(O).The POS information is also used by defining sev-eral binary features.
An example is the NomPSPbinary feature.
The value of this feature is definedto be 1 if the current word is nominal and the next490word is a PSP.2.3 Performance of Hindi NER using MaxEntMethodThe performance of the MaxEnt based Hindi NERusing the above mentioned features is reported hereas a baseline.
We have evaluated the system us-ing a blind test corpus of 25K words.
The testcorpus contains 521 person, 728 location, 262 or-ganization and 236 date entities.
The accuraciesare measured in terms of the f-measure, which isthe weighted harmonic mean of precision and re-call.
Precision is the fraction of the correct anno-tations and recall is the fraction of the total NEsthat are successfully annotated.
The general formulafor measuring the f-measure or f-value is, F?
=(1+?2) .
(precision .
recall) \ (?2 .
precision +recall).
Here the value of ?
is taken as 1.
In Table 2we have shown the accuracy values for few featuresets.While experimenting with static word features,we have observed that a window of previous andnext two words (wi?2...wi+2) gives best result(69.09) using the word features only.
But whenwi?3and wi+3 are added with it, the f-value is reducedto 66.84.
Again when wi?2 and wi+2 are deductedfrom the feature set (i.e.
only wi?1 and wi+1 as fea-ture), the f-value is reduced to 67.26.
This demon-strates thatwi?2 andwi+2 are helpful features in NEidentification.When suffix, prefix and digit information areadded to the feature set, the f-value is increased upto74.26.
The value is obtained using the feature setF8 [wi, wi?1, wi+1, ti?1, Suffix, Digit].
It is ob-served that when wi?2 and wi+2 are added with thefeature, the accuracy decreases by 2%.
It contra-dicts the results using the word features only.
An-other interesting observation is that prefix informa-tion are helpful features in NE identification as theseincrease accuracy when separately added with theword features (F6).
Similarly the suffix informationhelps in increasing the accuracy.
But when both thesuffix and prefix information are used in combina-tion along with the word features, the f-value is de-creased.
From Table 2, a f-value of 73.42 is obtainedusing F5 [wi, wi?1, wi+1, ti?1, Suffix] but whenprefix information are added with it (F7), the f-valueis reduced to 72.5.POS information are important features in NER.In general it is observed that coarse grained POSinformation performs better than the finer grainedPOS information.
The best accuracy (75.6 f-value)of the baseline system is obtained using the binaryNomPSP feature along with word feature (wi?1,wi+1), suffix and digit information.
It is noted thatwhen wi?2, wi+2 and prefix information are addedwith the best feature, the f-value is reduced to 72.65.From the above discussion it is clear that the sys-tem suffers from overfitting if a large number of fea-tures are used to train the system.
Note that the sur-rounding word (wi?2, wi?1, wi+1, wi+2 etc.)
fea-tures can take any value from the lexicon and henceare of high dimensionality.
These cause the degra-dation of performance of the system.
However it isobvious that few words in the lexicon are importantin identification of NEs.To solve the problem of high dimensionality weuse clustering to group the words present in the cor-pus into much smaller number of clusters.
Thenthe word clusters are used as features instead ofthe word features (for surrounding words).
For ex-ample, our Hindi corpus contains 17,456 differentwords, which are grouped into N (say 100) clusters.Then for a particular word, it is assigned to a clusterand the corresponding cluster-id is used as feature.Hence the number of features is reduced to 100 in-stead of 17,456.Similarly, selection of important words can alsosolve the problem of high dimensionality.
As someof the words in the lexicon play important role inthe NE identification process, we aim to select theseparticular words.
Only these important words areused in NE identification instead of all words in thecorpus.3 Word ClusteringClustering is the process of grouping together ob-jects based on their similarity.
The measure of sim-ilarity is critical for good quality clustering.
Wehave experimented with some approaches to com-pute word-word similarity.
These are described indetails in the following section.4913.1 Cosine Similarity based on Sentence LevelCo-occurrenceA word is represented by a binary vector of dimen-sion same as the number of sentences in the cor-pus.
A component of the vector is 1 if the wordoccurs in the corresponding sentence and zero oth-erwise.
Then we measure cosine similarity betweenthe word vectors.
The cosine similarity between twoword vectors ( ~A and ~B) with dimension d is mea-sured as:CosSim( ~A, ~B) =?dAdBd(?dA2d)12 ?
(?dB2d)12(3)This measures the number of co-occurring sen-tences.3.2 Cosine Similarity based on ProximalWordsIn this measure a word is represented by a vectorhaving dimension same as the lexicon size.
Forease of implementation we have taken a dimen-sion of 2 ?
200, where each component of the vec-tor corresponds to one of the 200 most frequentpreceding and following words of a token word.List Prev containing the most frequent (top 200)previous words (wi?1 or wi?2 if wi is the first wordof a NE) and List Next contains 200 most frequentnext words (wi+1 or wi+2 if wi is the last word of aNE).
A particular word wk may occur several times(say n) in the corpus.
For each occurrence of wkfind if its previous word (wk?1 or wk?2) matchesany element of List Prev.
If matches, then set 1 tothe corresponding position of the vector and set zeroto all other positions related to List Prev.
Sim-ilarly check the next word (wk+1 or wk+2) in theList Next and find the values of the correspondingpositions.
The final word vector ~Wk is obtained bytaking the average of all occurrences of wk.
Thenthe cosine similarity is measured between the wordvectors.
This measures the similarity of the contextsof the occurrences of the word in terms of the prox-imal words.3.3 Similarity based on Proximity to NECategoriesHere, for each word (wi) in the corpus four binaryvectors are defined corresponding to two precedingand two following positions (i-1, i-2, i+1, i+2).
Eachbinary vector is of dimension five correspondingto four NE classes (Cj) and one for the not-nameclass.
For a particular word wk, find all the wordsoccur in a particular position (say, +1).
Measurethe fraction (Pj(wk)) of these words belonging to aclass Cj .
The component of the word vector ~Wk forthe position corresponding to Cj is Pj(wk).Pj(wk) =No.
of times wk+1 is a NE of class CjTotal occurrence of wk in corpusThe Euclidean distance is used to find the simi-larity between the above word vectors as a similar-ity measure.
Some of the word vectors for the +1position are given in Table 3.
In this table we havegiven the word vectors for a few Hindi words, whichare, sthita (located), shahara (city), jAkara (go), na-gara (township), gA.nva (village), nivAsI (resident),mishrA (a surname) and limiTeDa (ltd.).
From thetable we observe that the word vectors are close forsthita [0 0.478 0 0 0.522], shahara [0 0.585 0.0010.024 0.39], nagara [0 0.507 0.019 0 0.474] andgA.nva [0 0.551 0 0 0.449].
So these words are con-sidered as close.Word Per Loc Org Dat Notsthita 0 0.478 0 0 0.522shahara 0 0.585 0.001 0.024 0.39jAkara 0 0.22 0 0 0.88nagara 0 0.507 0.019 0 0.474gA.nva 0 0.551 0 0 0.449nivAsI 0.108 0.622 0 0 0.27mishrA 0.889 0 0 0 0.111limiTeDa 0 0 1 0 0Table 3: Example of some word vectors for next (+1)position (see text for glosses)3.4 K-means ClusteringUsing the above similarity measures we have usedthe k-means algorithm.
The seeds were randomlyselected.
The value of k (number of clusters) wasvaried till the best result is obtained.4 Important Word SelectionIt is noted that not all words are equally importantin determining the NE category.
Some of the words492in the lexicon are typically associated with a partic-ular NE category and hence have important role toplay in the classification process.
We describe be-low a few statistical techniques that has been used toidentify the important words.4.1 Class Independent Important WordSelectionWe define context words as those which occur inproximity of a NE.
In other words, context wordsare the words present in the wi?2, wi?1, wi+1or wi+2 position if wi is a NE.
Note that only asubset of the lexicon are context words.
For allthe context words, its N weight is calculated asthe ratio between the occurrence of the word as acontext word and its total number of occurrence inthe corpus.
The context words having the higherN weight are considered as important words forNER.
For our experiments we have considered top500 words as important words.N weight(wi) =Occurrence of wi as context wordTotal occurrence of wi in corpus4.2 Important Words for Each ClassSimilar to the class independent important word se-lection from the contexts, important words are se-lected for individual classes also.
This is an exten-sion of the previous context word considering onlyNEs of a particular class.
For person, location, or-ganization and date classes we have considered top150, 120, 50 and 50 words respectively as impor-tant words.
Four binary features are also defined forthese four classes.
These are defined as having value1 if any of the context words belongs to the impor-tant words list for a particular class.4.3 Important Words for Each PositionPosition based important words are also selectedfrom the corpus.
Here instead of context, particu-lar positions are considered.
Four lists are compiledfor two preceding and two following positions (-2,-1, +1 and +2).5 Evaluation of NE RecognitionThe following subsections contain the experimentalresults using word clustering and important word se-lection.
The results demonstrate the effectiveness ofk Per Loc Org Dat Total20 66.33 74.57 43.64 91.30 69.5450 64.13 76.35 52 93.62 71.780 66.33 74.57 53.85 93.62 72.08100 70.1 73.1 57.7 96.62 72.78120 66.15 73.43 54.9 93.62 71.52150 66.88 74.94 53.06 95.65 72.33200 66.09 73.82 52 92 71.13Table 4: Variation of MaxEnt based system accuracy de-pending on number of clusters (k)word clustering and important word selection overthe baseline MaxEnt model.5.1 Using Word ClustersTo evaluate the effectiveness of the clustering ap-proaches in Hindi NER, we have used cluster fea-tures instead of word features.
For the surroundingwords, corresponding cluster-ids are used as feature.Choice of k : We have already mentioned that,for k-means clustering number of classes (k) shouldbe determined initially.
To find suitable k we hadconducted the following experiments.
We have se-lected a feature F1 (mentioned in Table 2) and ap-plied the clusters with different k as features replac-ing the word features.
In Table 4 we have summa-rized the experimental results, in order to find a suit-able k for clustering, the word vectors obtained us-ing the procedure described in Section 3.3.
Fromthe table we observe that the best result is obtainedwhen k is 100.
We have used k = 100 for the sub-sequent experiments for comparing the effectivenessof the features.
Similarly when we deal with all thewords in the corpus (17,465 words), we got best re-sults when the words are clustered into 1100 clus-ters.
?The details of the comparison between the base-line word features and the reduced features obtainedusing clustering are given in Table 5.
In general itis observed that clustering has improved the perfor-mance over baseline features.
Using only clusterfeatures the system provides a maximum f-value of74.26 where the corresponding word features givef-value of 69.09.Among the various similarity measures of clus-tering, improved results are obtained using the clus-493Feature UsingWordFeaturesUsingClusters(C1)UsingClusters(C2)UsingClusters(C3)wi, window(-1, +1) 67.26 69.67 72.05 72.78wi, window(-2, +2) 69.09 71.52 72.65 74.26wi, window(-1, +1), Suffix 73.42 74.24 75.44 75.84wi, window(-1, +1), Prefix, Suffix 72.5 74.76 75.7 76.33wi, window(-1, +1), Prefix, Suffix, Digit 74.26 75.09 75.91 76.41wi, window(-1, +1), Prefix, Suffix, Digit,NomPSP75.6 77.2 77.39 77.61wi, window(-2, +2), Prefix, Suffix, Digit,NomPSP72.65 77.86 78.61 79.03Table 5: F-values for different features in a MaxEnt based Hindi NER with clustering based feature reduction[window(?m,+n) refers to the cluster or word features corresponding to previous m positions and next n posi-tions; C1 is the clusters which use sentence level co-occurrence based cosine similarity (3.1), C2 denotes the clusterswhich use proximal word based cosine similarity (3.2), C3 denotes the clusters for each positions related to NE (3.3)]ters which uses the similarity measurement based onproximity of the words to NE categories (defined inSection 3.3).Using clustering features the best f-value (79.03)is obtained using clusters for previous two and nexttwo words along with the suffix, prefix, digit andPOS information.It is observed that the prefix information increasesthe accuracy if applied along with suffix informa-tion when cluster features are used.
More interest-ingly, addition of cluster features for positions ?2and +2 over the feature [window(-1, +1), Suffix,Prefix, Digit, NomPSP] increase the f-value from77.61 to 79.03.
But in the baseline system additionof word features (wi?2 and wi+2) over the same fea-ture decrease the f-value from 75.6 to 72.65.5.2 Using Important Word SelectionThe details of the comparison between the word fea-ture and the reduced features based on importantword selection are given in Table 6.
For the sur-rounding word features, find whether the particularword (e.g.
at position -1, -2 etc.)
presents in theimportant words list (corresponding to the particu-lar position if position based important words areconsidered).
If the word occurs in the list then theword is used as features.
In general it is observedthat word selection also improves performance overbaseline features.
Among the different approaches,the best result is obtained when important words fortwo preceding and two following positions (definedin Section 4.3) are selected.
Using important wordbased features, the highest f-value of 79.85 is ob-tained by using the important words for previous twoand next two positions along with the suffix, prefix,digit and POS information.5.3 Relative Effectiveness of Clustering andWord SelectionIn most of the cases clustering based features per-form better then the important word based featurereduction.
But the best f-value (79.85) of the sys-tem (using the clustering based and important wordbased features separately) is obtained by using im-portant word based features.Next we have made an experiment by consider-ing both the clusters and important words combined.We have defined the combined feature as, if the word(wi) is in the corresponding important word list thenthe word is used as feature otherwise the correspond-ing cluster-id (in which wi belongs to) is consideredas feature.
Using the combined feature, we haveachieved further improvement.
Here we are able toachieve the highest f-value of 80.01.6 ConclusionA hierarchical word clustering technique, whereclusters are driven automatically from large unan-494Feature UsingWordFeaturesUsingWords(I1)UsingWords(I2)UsingWords(I3)wi, window(-1, +1) 67.26 66.31 67.53 66.8wi, window(-2, +2) 69.09 72.04 72.9 73.34wi, window(-1, +1), Suffix 73.42 73.85 73.12 74.61wi, window(-1, +1), Prefix, Suffix 72.5 73.52 73.94 74.87wi, window(-1, +1), Prefix, Suffix, Digit 74.26 73.97 74.13 74.7wi, window(-1, +1), Prefix, Suffix, Digit,NomPSP75.6 75.84 76.6 77.22wi, window(-2, +2), Prefix, Suffix, Digit,NomPSP72.65 76.69 77.42 79.85Table 6: F-values for different features in a MaxEnt based Hindi NER with important word based feature reduction[window(?m,+n) refers to the important word or baseline word features corresponding to previous m positions andnext n positions; I1 is the class independent important words (4.1), I2 denotes the important words for each class (4.2),I3 denotes the important words for each positions (4.3)]notated corpus, is used by Miller et al (2004) foraugmenting annotated training data.
Note that ourclustering approach is different, where the clustersare obtained using some statistics derived from theannotated corpus, and also the purpose is differentas we have used the clusters for feature reduction.In this paper we propose two feature reductiontechniques for Hindi NER based on word cluster-ing and word selection.
A number of word similar-ity measures are used for clustering.
A few statisti-cal approaches are used for the selection of impor-tant words.
It is observed that significant enhance-ment of accuracy over the baseline system which useword features is obtained.
This is probably due toreduction of overfitting.
This is more important fora resource poor languages like Hindi where there isscarcity in annotated training data and other NERresources (like, gazetteer lists).7 AcknowledgementThe work is partially funded by Microsoft ResearchIndia.ReferencesBerger A L, Pietra S D and Pietra V D 1996.
A Maxi-mum Entropy Approach to Natural Language Process-ing.
Computational Linguistic, 22(1):39?71.Bikel D M, Miller S, Schwartz R and W Ralph.
1997.Nymble: A High Performance Learning Name-finder.In Proceedings of the Fifth Conference on Applied Nat-ural Language Processing, pages 194?201.Borthwick A.
1999.
A Maximum Entropy Approach toNamed Entity Recognition.
Ph.D. thesis, ComputerScience Department, New York University.Grishman R. 1995.
The New York University SystemMUC-6 or Where?s the syntax?
In Proceedings of theSixth Message Understanding Conference.Li W and McCallum A.
2003.
Rapid Development ofHindi Named Entity Recognition using ConditionalRandom Fields and Feature Induction.
ACM Trans-actions on Asian Language Information Processing(TALIP), 2(3):290?294.McCallum A and Li W. 2003.
Early Results for NamedEntity Recognition with Conditional Random fields,feature induction and web-enhanced lexicons.
In Pro-ceedings of the Seventh Conference on Natural Lan-guage Learning at HLT-NAACL.Miller S, Guinness J and Zamanian A.
2004.
Name Tag-ging with Word Clusters and Discriminative Training.In Proceedings of the HLT-NAACL 2004, pages 337?342.Saha S K, Sarkar S and Mitra P. 2008.
A Hybrid Fea-ture Set based Maximum Entropy Hindi Named En-tity Recognition.
In Proceedings of the Third Interna-tional Joint Conference on Natural Language Process-ing (IJCNLP-08), pages 343?349.Wakao T, Gaizauskas R and Wilks Y.
1996.
Evaluationof an algorithm for the recognition and classificationof proper names.
In Proceedings of COLING-96.495
