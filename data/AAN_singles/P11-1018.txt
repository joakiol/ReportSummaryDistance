Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 171?179,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsMulti-Modal Annotation of Quest Games in Second LifeSharon Gower Small, Jennifer Stromer-Galley and Tomek StrzalkowskiILS InstituteState University of New York at AlbanyAlbany, NY 12222small@albany.edu, jstromer@albany.edu, tomek@albany.eduAbstractWe describe an annotation tool developed to as-sist in the creation of multimodal action-communication corpora from on-line massivelymulti-player games, or MMGs.
MMGs typicallyinvolve groups of players (5-30) who controltheir avatars1, perform various activities (quest-ing, competing, fighting, etc.)
and communicatevia chat or speech using assumed screen names.We collected a corpus of 48 group quests inSecond Life that jointly involved 206 playerswho generated over 30,000 messages in quasi-synchronous chat during approximately 140hours of recorded action.
Multiple levels of co-ordinated annotation of this corpus (dialogue,movements, touch, gaze, wear, etc) are requiredin order to support development of automatedpredictors of selected real-life social and demo-graphic characteristics of the players.
The anno-tation tool presented in this paper was developedto enable efficient and accurate annotation of alldimensions simultaneously.1 IntroductionThe aim of our project is to predict the real worldcharacteristics of players of massively-multiplayeronline games, such as Second Life (SL).
We soughtto predict actual player attributes like age or educa-tion levels, and personality traits including leader-ship or conformity.
Our task was to do so usingonly the behaviors, communication, and interactionamong the players produced during game play.
Todo so, we logged all players?
avatar movements,1 All avatar names seen in this paper have been changed toprotect players?
identities.
?touch events?
(putting on or taking off clothingitems, for example), and their public chat messages(i.e., messages that can be seen by all players in thegroup).
Given the complex nature of interpretingchat in an online game environment, we required atool that would allow annotators to have a synchro-nized view of both the event action as well as thechat utterances.
This would allow our annotators tocorrelate the events and the chat by marking themsimultaneously.
More importantly, being able toview game events enables more accurate chat anno-tation; and conversely, viewing chat utteranceshelps to interpret the significance of certain eventsin the game, e.g., one avatar following another.
Forexample, an exclamation of: ?I can?t do it!?
couldbe simply a response (rejection) to a request fromanother player; however, when the game action isviewed and the speaker is seen attempting to enter abuilding without success, another interpretationmay arise (an assertion, a call for help, etc.
).The Real World (RW) characteristics of SLplayers (and other on-line games) may be inferredto varying degrees from the appearance of theiravatars, the behaviors they engage in, as well asfrom their on-line chat communications.
For exam-ple, the avatar gender generally matches the genderof the owner; on the other hand, vocabulary choicesin chat are rather poor predictors of a player?s age,even though such correlation is generally seen inreal life conversation.Second Life2 was the chosen platform becauseof the ease of creating objects, controlling the playenvironment, and collecting players?
movement,chat, and other behaviors.
We generated a corpus ofchat and movement data from 48 quests comprisedof 206 participants who generated over 30,0002 An online Virtual World developed and launched in 2003, byLinden Lab, San Francisco, CA.
http://secondlife.com171messages and approximately 140 hours of recordedaction.
We required an annotation tool to help usefficiently annotate dialogue acts and communica-tion links in chat utterances as well as avatarmovements from such a large corpus.
Moreover,we required correlation between these two dimen-sions of chat and movement since movement andother actions may be both causes and effects ofverbal communication.
We developed a multi-modal event and chat annotation tool (called RAT,the Relational Annotation Tool), which will simul-taneously display a 2D rendering of all movementactivity recorded during our Second Life studies,synchronized with the chat utterances.
In this wayboth chat and movements can be annotated simul-taneously: the avatar movement actions can be re-viewed while making dialogue act annotations.This has the added advantage of allowing the anno-tator to see the relationships between chat, behav-ior, and location/movement.
This paper willdescribe our annotation process and the RAT tool.2 Related WorkAnnotation tools have been built for a variety ofpurposes.
The CSLU Toolkit (Sutton et al, 1998) isa suite of tools used for annotating spoken lan-guage.
Similarly, the EMU System (Cassidy andHarrington, 2001) is a speech database managementsystem that supports multi-level annotations.
Sys-tems have been created that allow users to readilybuild their own tools such as AGTK (Bird et al,2001).
The multi-modal tool DAT (Core and Al-len, 1997) was developed to assist testing of theDAMSL annotation scheme.
With DAT, annota-tors were able to listen to the actual dialogues aswell as view the transcripts.
While these tools areall highly effective for their respective tasks, ours isunique in its synchronized view of both event ac-tion and chat utterances.Although researchers studying online communi-cation use either off-the shelf qualitative data anal-ysis programs like Atlas.ti or NVivo, a few studieshave annotated chat using custom-built tools.
Oneapproach uses computer-mediated discourse analy-sis approaches and the Dynamic Topic Analysistool (Herring, 2003; Herring & Nix; 1997; Stromer-Galley & Martison, 2009), which allows annotatorsto track a specific phenomenon of online interactionin chat: topic shifts during an interaction.
TheVirtual Math Teams project (Stahl, 2009) created aated a tool that allowed for the simultaneous play-back of messages posted to a quasi-synchronousdiscussion forum with whiteboard drawings thatstudent math team members used to illustrate theirideas or visualize the math problem they were try-ing to solve (?akir, 2009).A different approach to data capture of complexhuman interaction is found in the AMI MeetingCorpus (Carletta, 2007).
It captures participants?head movement information from individual head-mounted cameras, which allows for annotation ofnodding (consent, agreement) or shaking (dis-agreement), as well as participants?
locations withinthe room; however, no complex events involvingseries of movements or participant proximity areconsidered.
We are unaware of any other tools thatfacilitate the simultaneous playback of multi-modesof communication and behavior.3 Second Life ExperimentsTo generate player data, we rented an island inSecond Life and developed an approximately twohour quest, the Case of the Missing Moonstone.
Inthis quest, small groups of 4 to 5 players, who werepreviously unacquainted, work their way togetherthrough the clues and puzzles to solve a murdermystery.
We recruited Second Life players in-gamethrough advertising and setting up a shop that inter-ested players could browse.
We also used Facebookads, which were remarkably effective.The process of the quest experience for playersstarted after they arrived in a starting area of theisland (the quest was open only to players whowere made temporary members of our island)where they met other players, browsed quest-appropriate clothing to adorn their avatars, and re-ceived information from one of the researchers.Once all players arrived, the main quest began,progressing through five geographic areas in theisland.
Players were accompanied by a ?trainingsergeant?, a researcher using a robot avatar, thatfollowed players through the quest and providedhints when groups became stymied along their in-vestigation but otherwise had little interaction withthe group.The quest was designed for players to encounterobstacles that required coordinated action, such asall players standing on special buttons to activate adoor, or the sharing of information between players,such as solutions to a word puzzle, in order to ad-vance to the next area of the quest (Figure 1).172Slimy Roastbeef: ?who?s got the square gear?
?Kenny Superstar: ?I do, but I?m stuck?Slimy Roastbeef: ?can you hand it to me?
?Kenny Superstar: ?i don?t know how?Slimy Roastbeef: ?open your inventory, clickand drag it onto me?Figure 1: Excerpt of dialogue during a coor-dination activityQuest activities requiring coordination among theplayers were common and also necessary to ensurea sufficient degree of movement and message traf-fic to provide enough material to test our predic-tions, and to allow us to observe particular socialcharacteristics of players.
Players answered a sur-vey before and then again after the quest, providingdemographic and trait information and evaluatingother members of their group on the characteristicsof interest.3.1 Data CollectionWe recorded all players?
avatar movements as theypurposefully moved avatars through the virtualspaces of the game environment, their public chat,and their ?touch events?, which are the actions thatbring objects out of player inventories, pick up ob-jects to put in their inventories, or to put objects,such as hats or clothes, onto the avatars, and thelike.
We followed Yee and Bailenson?s (2008)technical approach for logging player behavior.
Toget a sense of the volume of data generated, 206players generated over 30,000 messages into thegroup?s public chat from the 48 sessions.
We com-piled approximately 140 hours of recorded action.The avatar logger was implemented to record eachavatar?s location through their (x,y,z) coordinates,recorded at two second intervals.
This informationwas later used to render the avatar?s position on our2D representation of the action (section 4.1).4 RATThe Relational Annotation Tool (RAT) was built toassist in annotating the massive collection of datacollected during the Second Life experiments.
Atool was needed that would allow annotators to seethe textual transcripts of the chat while at the sametime view a 2D representation of the action.
Addi-tionally, we had a textual transcript for a select setof events: touch an object, stand on an object, at-tach an object, etc., that we needed to make avail-able to the annotator for review.These tool characteristics were needed forseveral reasons.
First, in order to fully understandthe communication and interaction occurring be-tween players in the game environment and accu-rately annotate those messages, we neededannotators to have as much information about thecontext as possible.
The 2D map coupled with theevents information made it easier to understand.For example, in the quest, players in a specificzone, encounter a dead, maimed body.
As annota-tors assigned codes to the chat, they would some-times encounter exclamations, such as ?ew?
or?gross?.
Annotators would use the 2D map and thelocation of the exclaiming avatar to determine if theexclamation was a result of their location (in thezone with the dead body) or because of somethingsaid or done by another player.
Location of avatarson the 2D map synchronized with chat was alsohelpful for annotators when attempting to disam-biguate communicative links.
For example, in onesubzone, mad scribblings are written on a wall.
Ifplayer A says ?You see that scribbling on thewall??
the annotator needs to use the 2D map to seewho the player is speaking to.
If player A andplayer C are both standing in that subzone, then theannotator can make a reasonable assumption thatplayer A is directing the question to player C, andnot player B who is located in a different subzone.Second, we annotated coordinated avatar move-ment actions (such as following each other into abuilding or into a room), and the only way to read-ily identify such complex events was through the2D map of avatar movements.The overall RAT interface, Figure 2, allowsthe annotator to simultaneously view all modes ofrepresentation.
There are three distinct panels inthis interface.
The left hand panel is the 2D repre-sentation of the action (section 4.1).
The upperright hand panel displays the chat and event tran-scripts (section 4.2), while the lower right hand por-tion is reserved for the three annotator sub-panels(section 4.3).173Figure 2: RAT interface4.1 The 2D Game RepresentationThe 2D representation was the most challenging ofthe panels to implement.
We needed to find theproper level of abstraction for the action, whilemaintaining its usefulness for the annotator.
Toocomplex a representation would cause cognitiveoverload for the annotator, thus potentially deterio-rating the speed and quality of the annotations.Conversely, an overly abstract representation wouldnot be of significant value in the annotation proc-ess.There were five distinct geographic areas on ourSecond Life Island: Starting Area, Mansion, TownCenter, Factory and Apartments.
An overview ofthe area in Second Life is displayed in Figure 3.
Wedecided to represent each area separately as eachgroup moves between the areas together, and it wastherefore never necessary to display more than onearea at a time.
The 2D representation of the Man-sion Area is displayed in Figure 4 below.
Figure 5is an exterior view of the actual Mansion in SecondLife.
Each area?s fixed representation was renderedusing Java Graphics, reading in the Second Life(x,y,z) coordinates from an XML data file.
We rep-resented the walls of the buildings as connectedsolid black lines with openings left for doorways.Key item locations were marked and labeled, e.g.Kitten, maid, the Idol, etc.
Even though annotatorsvisited the island to familiarize themselves with thelayout, many mansion rooms were labeled to helpthe annotator recall the layout of the building, andminimize error of annotation based on flawed re-call.
Finally, the exact time of the action that is cur-rently being represented is displayed in the lowerleft hand corner.Figure 3: Second Life overview map174Figure 4: 2D representation of Second Life actioninside the Mansion/ManorFigure 5: Second Life view of Mansion exteriorAvatar location was recorded in our log files as an(x,y,z) coordinate at a two second interval.
Avatarswere represented in our 2D panel as moving solidcolor circles, using the x and y coordinates.
A colorcoded avatar key was displayed below the 2D rep-resentation.
This key related the full name of everyavatar to its colored circle representation.
The zcoordinate was used to determine if the avatar wason the second floor of a building.
If the z valueindicated an avatar was on a second floor, their iconwas modified to include the number ?2?
for the du-ration of their time on the second floor.
Also loggedwas the avatar?s degree of rotation.
Using this wewere able to represent which direction the avatarwas looking by a small black dot on their coloredcircle.As the annotators stepped through the chat andevent annotation, the action would move forward,in synchronized step in the 2D map.
In this way atany given time the annotator could see the avataraction corresponding to the chat and event tran-scripts appearing in the right panels.
The annotatorhad the option to step forward or backward throughthe data at any step interval, where each step corre-sponded to a two second increment or decrement, toprovide maximum flexibility to the annotator inviewing and reviewing the actions and communica-tions to be annotated.
Additionally, ?Play?
and?Stop?
buttons were added to the tool so the anno-tator may simply watch the action play forward ra-ther than manually stepping through.4.2 The Chat & Event PanelAvatar utterances along with logged Second Lifeevents were displayed in the Chat and Event Panel(Figure 6).
Utterances and events were each dis-played in their own column.
Time was recorded forevery utterance and event, and this was displayed inthe first column of the Chat and Event Panel.
Allavatar names in the utterances and events werecolor coded, where the colors corresponded to theavatar color used in the 2D panel.
This panel wassynchronized with the 2D Representation panel andas the annotator stepped through the game action onthe 2D display, the associated utterances and eventspopulated the Chat and Event panel.175Figure 6: Chat & Event Panel4.3 The Annotator PanelsThe Annotator Panels (Figures 7 and 10) containsall features needed for the annotator to quicklyannotate the events and dialogue.
Annotators couldchoose from a number of categories to label eachdialogue utterance.
Coding categories includedcommunicative links, dialogue acts, and selectedmulti-avatar actions.
In the following we brieflyoutline each of these.
A more detailed descriptionof the chat annotation scheme is available in(Shaikh et al, 2010).4.3.1 Communicative LinksOne of the challenges in multi-party dialogue is toestablish which user an utterance is directed to-wards.
Users do not typically add addressing in-formation in their utterances, which leads toambiguity while creating a communication link be-tween users.
With this annotation level, we askedthe annotators to determine whether each utterancewas addressed to some user, in which case theywere asked to mark which specific user it was ad-dressed to; was in response to another prior utter-ance by a different user, which required markingthe specific utterance responded to; or a continua-tion of the user?s own prior utterance.Communicative link annotation allows for accu-rate mapping of dialogue dynamics in the multi-party setting, and is a critical component of trackingsuch social phenomena as disagreements and lead-ership.4.3.2 Dialogue ActsWe developed a hierarchy of 19 dialogue acts forannotating the functional aspect of the utterance inthe discussion.
The tagset we adopted is looselybased on DAMSL (Allen & Core, 1997) andSWBD (Jurafsky et al, 1997), but greatly reducedand also tuned significantly towards dialoguepragmatics and away from more surface character-istics of utterances.
In particular, we ask our anno-tators what is the pragmatic function of eachutterance within the dialogue, a decision that oftendepends upon how earlier utterances were classi-fied.
Thus augmented, DA tags become an impor-tant source of evidence for detecting language usesand such social phenomena as conformity.
Exam-ples of dialogue act tags include Assertion-Opinion,Acknowledge, Information-Request, and Confirma-tion-Request.Using the augmented DA tagset alo presents afairly challenging task to our annotators, who needto be trained for many hours before an acceptablerate of inter-annotator agreement is achieved.
Forthis reason, we consider our current DA tagging asa work in progress.4.3.3 Zone codingEach of the five main areas had a correspond-ing set of subzones.
A subzone is a building, aroom within a building, or any other identifiablearea within the playable spaces of the quest, e.g.
theMansion has the subzones: Hall, Dining Room,Kitchen, Outside, Ghost Room, etc.
The subzonewas determined based on the avatar(s) (x,y,z) coor-dinates and the known subzone boundaries.
Thisadditional piece of data allowed for statisticalanalysis at different levels: avatar, dialogue unit,and subzone.176Figure 7: Chat Annotation Sub-Panel4.3.4 Multi-avatar eventsAs mentioned, in addition to chat we also were in-terested in having the annotators record compositeevents involving multiple avatars over a span oftime and space.
While the design of the RAT toolwill support annotation of any event of interest withonly slight modifications, for our purposes, wewere interested in annotating two types of eventsthat we considered significant for our research hy-potheses.
The first type of event was the multi-avatar entry (or exit) into a sub-zone, including theorder in which the avatars moved.Figure 8 shows an example of a ?Moves intoSubzone?
annotation as displayed in the Chat &Event Panel.
Figure 9 shows the corresponding se-ries of progressive moments in time portraying en-try into the Bank subzone as represented in RAT.
Inthe annotation, each avatar name is recorded in or-der of its entry into the subzone (here, the Bank).Additionally, we record the subzone name and thetime the event is completed3.The second type of event we annotated was the?follow X?
event, i.e., when one or more avatarsappeared to be following one another within a sub-zone.
These two types of events were of particularinterest because we hypothesized that players whoare leaders are likely to enter first into a subzoneand be followed around once inside.In addition, support for annotation of other typesof composite events can be added as needed; forexample, group forming and splitting, or certain3 We are also able to record the start time of any event but forour purposes we were only concerned with the end time.joint activities involving objects, etc.
were fairlycommon in quests and may be significant for someanalyses (although not for our hypotheses).For each type of event, an annotation subpanel iscreated to facilitate speedy markup while minimiz-ing opportunities for error (Figure 10).
A ?MovesInto Subzone?
event is annotated by recording theordinal (1, 2, 3, etc.)
for each avatar.
Similarly, a?Follows?
event is coded as avatar group ?A?
fol-lows group ?B?, where each group will contain oneor more avatars.Figure 8: The corresponding annotation for Figure9 event, as displayed in the Chat & Event Panel5 The Annotation ProcessTo annotate the large volume of data generatedfrom the Second Life quests, we developed an an-notation guide that defined and described the anno-tation categories and decision rules annotators wereto follow in categorizing the data units (followingprevious projects (Shaikh et al, 2010).
Two stu-dents were hired and trained for approximately 60hours, during which time they learned how to usethe annotation tool and the categories and rules forthe annotation process.
After establishing a satisfac-tory level of interrater reliability (average Krippen-dorff?s alpha of all measures was <0.8.Krippendorff?s alpha accounts for the probability of177chance agreement and is therefore a conservativemeasure of agreement), the two students then anno-tated the 48 groups over a four-month period.
Ittook approximately 230 hours to annotate the ses-sions, and they assigned over 39,000 dialogue acttags.
Annotators spent roughly 7 hours marking upthe movements and chat messages per 2.5 hourquest session.Figure 9: A series of progressive moments in time portraying avatar entry into the Bank subzoneFigure 10: Event Annotation Sub-Panel, currently showing the ?Moves Into Subzone?
event fromfigure 9, as well as: ?Kenny follows Elliot in Vault?5.1 The Annotated CorpusThe current version of the annotated corpus consistsof thousands of tagged messages including: 4,294action-directives, 17,129 assertion-opinions, 4,116information requests, 471 confirmation requests,394 offer-commits, 3,075 responses to informationrequests, 1,317 agree-accepts, 215 disagree-rejects,and 2,502 acknowledgements, from 30,535 pre-split utterances (31,801 post-split).
We also as-signed 4,546 following events.6 ConclusionIn this paper we described the successful imple-mentation and use of our multi-modal annotationtool, RAT.
Our tool was used to accurately andsimultaneously annotate over 30,000 messages andapproximately 140 hours of action.
For each hourspent annotating, our annotators were able to tagapproximately 170 utterances as well as 36 minutesof action.The annotators reported finding the tool highlyfunctional and very efficient at helping them easilyassign categories to the relevant data units, and thatthey could assign those categories without produc-ing too many errors, such as accidentally assigningthe wrong category or selecting the wrong avatar.The function allowing for the synchronized play-back of the chat and movement data coupled withthe 2D map increased comprehension of utterances178and behavior of the players during the quest, im-proving validity and reliability of the results.AcknowledgementsThis research is part of an Air Force ResearchLaboratory sponsored study conducted by ColoradoState University, Ohio University, the University atAlbany, SUNY, and Lockheed Martin.ReferencesSteven Bird, Kazuaki Maeda, Xiaoyi Ma and HaejoongLee.
2001. annotation tools based on the annotationgraph API.
In Proceedings of ACL/EACL 2001Workshop on Sharing Tools and Resources for Re-search and Education.M.
P. ?akir.
2009.
The organization of graphical, narra-tive and symbolic interactions.
In Studying virtualmath teams (pp.
99-140).
New York, Springer.J.
Carletta.
2007.
Unleashing the killer corpus: experi-ences in creating the multi-everything AMI MeetingCorpus.
Language Resources and Evaluation Journal41(2): 181-190.Mark G. Core and James F. Allen.
1997.
Coding dia-logues with the DAMSL annotation scheme.
In Pro-ceedings of AAAI Fall 1997 Symposium.Steve Cassidy and Jonathan Harrington.
2001.
Multi-level annotation in the Emu speech database man-agement system.
Speech Communication, 33:61-77.S.
C. Herring.
2003.
Dynamic topic analysis of synchro-nous chat.
Paper presented at the New Research forNew Media: Innovative Research Symposium.
Min-neapolis, MN.S.
C. Herring and Nix, C. G. 1997.
Is ?serious chat?
anoxymoron?
Pedagogical vs. social use of internet re-lay chat.
Paper presented at the American Associationof Applied Linguistics, Orlando, FL.Samira Shaikh, Strzalkowski, T., Broadwell, A., Stro-mer-Galley, J., Taylor, S., and Webb, N. 2010.
MPC:A Multi-party chat corpus for modeling social phe-nomena in discourse.
Proceedings of the SeventhConference on International Language Resources andEvaluation.
Valletta, Malta: European Language Re-sources Association.G.
Stahl.
2009.
The VMT vision.
In G. Stahl, (Ed.
),Studying virtual math teams (pp.
17-29).
New York,Springer.Stephen Sutton, Ronald Cole,  Jacques DeVilliers,  Johan Schalkwyk,  Pieter Vermeulen,  MikeMacon,  Yonghong Yan,  Ed Kaiser,  Brian Run-Rundle,  Khaldoun Shobaki,  Paul Hosom,  AlexKain,  Johan Wouters,  Dominic Massaro,  MichaelCohen.
1998.
Universal Speech Tools: The CSLUtoolkit.
Proceedings of the 5th ICSLP, Australia.Jennifer Stromer-Galley and Martinson, A.
2009.
Coher-ence in political computer-mediated communication:Comparing topics in chat.
Discourse & Communica-tion, 3, 195-216.N.
Yee and Bailenson, J. N. 2008.
A method for longitu-dinal behavioral data collection in Second Life.
Pres-ence, 17, 594-596.179
