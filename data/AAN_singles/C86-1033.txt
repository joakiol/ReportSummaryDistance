A Stochast ic  Approach t O ParsingGeoffrey SampsonDepartment of L inguist ics  & Phonet icsUnivers i ty  of LeedsI.
S imulated anneal ing (e.g.K i rkpatr ick et al 1983, Br idle & Moore1984, Ackley et al 1985) is a stochast iccomputat ional  technique for f inding optimalsolut ions to combinator ia l  problems forwhich the combinator ia l  explos ion phenomenonrules out the poss ib i l i ty  of systemat ica l lyexamining each alternat ive.
It is current lybeing appl ied to the pract ical  problem ofopt imiz ing the physical  design of computercircuitry,  and to the theoret ical  problemsof resolv ing patterns of auditory and visualst imulat ion into meaningfu l  arrangements ofphonemes and three-d imensional  objects.Grammatica l  pars ing -- resolving unanalysedl inear sequences of words into meaningfulgrammatica l  structures -- can be regarded asa percept ion problem logical ly analogous tothose just cited, and s imulated anneal ingholds great promise as a parsing technique.S imulated anneal ing can mostd irect ly  be expla ined via a physicalanalogy.
Consider the logical space ofa l ternat iw~s in some large-scalecombinator ia l  problem as a chunk ofmountainous terrain, in which the a l t i tudeof any point corresponds to the relat ive"goodness" of that part icular  solut ion (thelower, the better).
We want to f ind thelowest point, but there are far too manypoints for each to be considered separately.We might try to locate the low point bydropping a bal l  onto the terr i tory at randomand hoping it wil l  roll down to the lowpoint.
This corresponds to randomlychoosing a part icu lar  overal l  solut ion tothe combinator ia l  problem, and thenconsider ing a series of modi f icat ions toindiv idual  components of the solution,adopt ing the modi f icat ions whenever theyimprove the overal l  solut ion and reject ingthem otherwise.
But the ball is veryunl ike ly  to reach the low point.
Much moreprobably, it wi l l  rol l  a short way downhi l land qome to rest in a "local minimum", aplace .where  all immediate moves are uphil leven though, some distance away, there areplaces much lower than the spot where thebal l  has halted.In this situation, a good way ofimproving hhe search technique would be topick up the landscape and shake it, so thatthe bal l  does not invar iably roll downhi l lbut sometimes bounces over obstruct ions.one would begin by shaking hard, so thateven the highest  peaks can be cleared, andthen gradual ly  reduce the ampl i tude ofshaking so that the ball searches forlowness in terms of progress ive ly  f inerdetail .
In computat ional  terms, rather thandec id ing whether  to adopt each of a seriesof modi f icat ions to the randomly-choseninit ial  posi t ion simply by reference towhether it y ields a gain or a loss, onedecides by reference to whether it yields aloss that is greater  than a number whosemagnitude tends to decrease as the processcontinues.
This is s imulated anneal ing.Not all combinator ia l  problems areamenable to the anneal ing technique.
If thedes i red low point in the mountainous terrainof the analogy happened to be at the bottomof a deep, narrow mineshaft  sunk from a highplace, anneal ing would not help to find it.But the logical geometry of many real- l i fecombinator ia l  phenomena is more like thegeometry of natural  mountains, where thereis a strong tendency for re lat ivelylow- ly ing points to be adjacent to manyother re lat ively low-ly ing points.
For suchphenomena, s imulated anneal ing can be aneff ic ient way of arr iv ing at optimalsolutions.2.
The appl icabi l i ty  of anneal ing asa parsing technique presupposes astat ist ical  approach to NL analysis whichwil l  itself be unfami l iar  to many readers.At this point I must therefore digress fromthe anneal ing concept in order br ief ly  todescr ibe the stat is t ics-based NL researchparadigm within which I am working, and towhich s imulated anneal ing appears to offeran important contr ibut ion.Much current work in parsing, asrepresented in books such as King (1983),Jones & Wilks (1983), analyses input bysystemat ica l ly  checking its propert iesagainst the var ious poss ib i l i t ies  a l lowed bya grammar which specif ies the language inquest ion as a wel l -def ined class ofsentences.
The grammar may be in the formof an ATN, a GPSG, or in some other form,and the checking process may operate in verydiverse ways; but all these approaches havein common the not ion of a c learcut boundarybetween a (probably very r ich and complex)class of wel l - formed inputs, and otherinputs which simply do not belong to thelanguage under analysis.
There are majord i f f icu l t ies  in making such parsers workadequately  with authentic,  unedited inputdrawn from unrestr ic ted domains.
NLs are soendless ly  d iverse and unpredictable  in theturns of phrase they display that manypeople f ind it extremely d i f f icu l t  tobel ieve that any sets of rules, no matterhow complex, can fully def ine them.
It151remains as true as it was sixty years agothat "All grammars leak" (Sapir 1921: 38).The Unit for Computer Research onthe Engl ish Language (UCREL) of theUnivers i ty  of Lancaster,  led by GeoffreyLeech and Roger Garside, with whom I remainassociated since my recent move to Leeds,has made considerable progress in recentyears in developing automatic rout ines whichsucceed in analys ing authent ic  text usingtechniques which do not assume the existenceof a c learcut grammat ica l /ungrammat ica ld ist inct ion (cf.
Garside et al:forthcoming).
The first major UCRELachievement was the CLAWS word-taggingsystem (see e.g.
Atwel l  et al 1984).
Since1982 CLAWS has been assigning part -o f -speechtags, drawn from a f inely-d i f ferent iated,134-member tag-set, to words of authent icrunning Engl ish text (which are oftengrammat ica l ly  many ways ambiguous inisolation), with a consistent  accuracy levelof 96-97% of words correct ly tagged -- af igure which we bel ieve can be furtherimproved by tuning the system in var iousways.
This is an achievement to which wehave been unable, despite extensiveenquir ies, to d iscover near rivals.The s igni f icant point about CLAWSis that it embodies no knowledge whatever  ofthe overal l  grammatical  archi tecture ofEngl ish  sentences.
Instead, it uses anempir ica l ly -der ived matr ix of relat ivet rans i t ion-probabi l i t ies  between pairs ofadjacent tags, together with informationenabl ing a set of candidate tags to beidenti f ied for any given word taken inisolat ion (using rules which ?efer to thelast letters of the word's spell ing,together with a list of c. 7200 exceptions).S impl i fy ing greatly, CLAWS works by formingall possible paths through sequences of tagswhich are candidates for the words of asentence, and choosing the path for whichthe product of the successive transi t ionprobabi l i t ies  is highest.
As a matter ofpolicy, no entr ies in the matr ix  oftag- t rans i t ion probabi l i t ies  are zero; weknow as well  as other l inguists that fai lureto observe a part icular  t ransi t ion in ourdata does not imply that the transi t ion is"ungrammatical" ,  and therefore evenunobserved transi t ions are ass igned a smallposit ive probabi l i ty.
Thus it is true tosay that the system "knows" nothing aboutEngl ish in the sense of drawing sharpd ist inct ions between grammatical  andungrammatical  sequences; it dealsexclusively in re lat ive l ikel ihoods.
Yetthis whol ly  "unintel l igent"  system worksextremely well.
It is easy to make CLAWSfail by inputt ing "trick" sentences of thekind often encountered in l inguist icstextbooks, but the lesson of CLAWS is thatsuch sentences are notably rare in reallife.We are current ly developing aCLAWS-l ike solut ion to the harder problem Ofgrammatical  parsing.
We have bui lt  up adatabase of manual ly -parsed sentences, from152which we extract stat ist ics that al low al ikel ihood measure to be determined for anylogical ly possib le non- leaf  const i tuent  of aparse-tree.
That is, given a pair ing of amother- label  with a sequence ofdaughter- labels ,  say the pair <J, NN JJ P>,the l ikel ihood funct ion wil l  return a f igUrefor the relat ive frequency with which (inthis case) an adject ive phrase consists ofs ingular common noun + adject ive +preposi t ional  phrase.
(In the case quotedthe l ikel ihood wil l  probably be low, but itought not to be zero: I selected theexample after encounter ing,  in a book openedat random, the adject ive phrase under l inedin "the value obtained must be a#s ignmen~compat ib le  with the ~fKP_q of  the var iable...".)
We assume, I believe- withjust i f icat ion,  that with only minor specialprovisos the l ikel ihood of a full parse-t reecan be ident i f ied with a simple funct ion ofthe l ikel ihood of each of its non- leafnodes.3.
The most d irect  way to imitate theCLAWS technique in the pars ing domain wouldbe to generate all poss ib le t ree-st ructuresfor a given sentence taken as a sequence ofword-tags,  and all possible label l ings ofeach of those structures, and choose thetree whose overal l  p laus ib i l i ty  f igure ishighest.
Unl ike in the case ofword-tagging,  however, for parsing thisapproach is whol ly  impractical.
The averagesentence in our database is about 22 wordslong, and the set of nonterminal  symbolsrecognized by our parsing scheme has a lmostthirty members; the number of a l ternat ivelog ica l ly -poss ib le  label led tree structureshaving 22 terminal  nodes is astronomical .
Ihave therefore begun to exper iment withs imulated anneal ing as  a solut ion to theproblem.
The grammatica l  stat ist ics in theexper iment descr ibed here are far cruderthan would be needed for a fu l l -scaleanneal ing parser, but init ial  results arenevertheless promising.4.
To apply the anneal ing techniqueto the parsing problem, it is necessary:(i) to state a t ree-evaluat ion function;(ii) to def ine a class of local changes totrees, such that any log ica l ly -poss ib le  treecan be converted to any other by apply ing aseries of changes drawn from the class (wecannot a l low the init ial  randomly-chosentree to el iminate the poss ib i l i ty  of everreaching some other tree which might be .thecorrect one); and (iii) to def ine ananneal ing schedule.Tree-eva luat ion  in my exper imentis based on stat ist ics ofconst i tuent -daughter  t rans i t ion frequencies:a const i tuent  label led A and havingdaughters label led B C D is g iven a valueder ived from the observed frequencies of thetransit ions A/\[B, A/BC, A/CD?
A/D\].
(Thefunctions which der ive node values fromdaughter - t rans i t ion  frequencies,  and treevalues from node values, are more complexthan simple averaging, which isunsat i s factory  because it too easi ly  al lowsan indiv idual  "bad" value in a candidateparse- t ree to be of fset  by several  "good"values e lsewhere in the tree.
I do not givedeta i ls  of the funct ions current ly  used.
)3in the experiment,  the stat ist icsreferred to a very small set ofbroad ly -def ined node- labels ,  compris ing 14nonterminal  labels and 30 word-c lass  labels.Our database uses a pars ing scheme whichrecognizes d is t inct ions  much finer than this-- we have seen that 134 word-c lasses  ared ist inguished,  and nonterminal  labels caninclude subcategory symbols which in theorypermit on the order of 60,000 d ist inctlabels~ However, most of this informationwas d iscarded for the sake of s impl i fy ingthe pi lot  exper iment.For point (ii) above, \[\[ def ine apossible move as fol lows.
Given aparse-tree~ select a node other than theroot at random.
Disconnect  it from itsmother?
It wi l l  then be located with in an"arch" of nodes whose left and r ight basesare respect ive ly  the last word before andthe f irst word after the d isconnectedconst i tuent,  and whose "keystone" is thelowest node dominat ing both of those words.Choose at random e i ther  any node located onthe arch other than the two bases, or anyl ink between two nodes in the arch?
In theformer case, attach the d isconnected node tothe chosen node as an extra daughter,  andre label  the new mother  of the d isconnectednode with a randomly-chosen label.
In thelatter case, create a new node on the chosenlink between exist ing nodes, label it with arandomly-chosen label, and attach thed isconnected node to it as a sister of thenode at the lower end of the chosen link.In either case, if the ex-mother  of thed isconnected  node is left with only onedaughter,  t l l en  delete the ex-mother bymerging its upward and downward l inks (ifthe ex-mother is the root node, itsremain ing (laughter becomes the new root andi s  accord ing ly  re label led S).
It is easy toshow that any tree can be der ived from anyother tree via a series of moves of thiskind.There is no "magic formula" todetermine the ideal anneal ing schedule for agiven class of combinator ia l  opt imizat ionproblems: this depends on the geometry ofthe logical space of poss ib i l i t ies ,  and hasto be d iscovered by experiment?
Therequi rements  are that anneal ing must beginat a high enough "temperature" for thesystem to be thoroughly  "melted" (that is,the factor by which the negat iv i ty  ofloca l ly -negat ive  moves is d iscounted must besuf f ic ient ly  large for moves to occur atrandom with no s igni f icant  bias towardsloca l ly -pos i t ive  moves),  and "cool ing" musttake place slowly enough for adequatesearching of the poss ib i l i ty -space to occur.
(If the init ial  temperature isunnecessar i ly  high, or cool ing unnecessar i lyprotracted,  a penalty wi l l  be paid in extraprocess ing for l i tt le or no gain in u l t imateaccuracy of search.)
"Temperature" might betreated as a constant f igure which is addedto the result  of subtract ing previousl ike l ihood-va lue from subsequentl ike l ihood-va lue in determin ing whether amove under cons iderat ion yields a net gainand is therefore adopted?
What is usual,however, is to strengthen the analogy withthermodynamics  by adding, not a constantfigure, but a f igure drawn randomly from aGauss ian distr ibut ion,  with "temperature"standing for the standard deviat ion of thed istr ibut ion.
Thus, even at a hightemperature it wil l  sometimes happen that as l ight ly loca l ly -negat ive move is rejected,and even at a low temperature it willoccas iona l ly  happen that a stronglyloca l ly -negat ive move is accepted.
(Local ly-pos i t ive and neutral  moves arealways accepted at any temperature?)5.
Let me i l lustrate by quot ing oneof the f irst anneal ing runs carr ied out bythe system, on a short sentence input asd j j n o v i d j n .Br ief  glosses for these symbols are: d,determiner;  j, adject ive; n, s ingular noun;o, modal verb or ~l_qo; v, main verb; i,preposit ion;  o, sentence- f ina l  punctuat ionmark.
Thus the sequence stands for asentence such as T~le ~ brown fox wil lj~m_~ over the laz Z ~\ ] i  This is of coursean art i f ic ia l ly  simple example, and ifauthent ic  language were commonly as"wel l -behaved" as this then the case forusing stochast ic  pars ing techniques would beweak.
However,  notice that the techniqueembodies no concept of a contrast  betweenwel l - formed and deviant  strings, so that inpr inc ip\ ]e  it should be as easy to set ananneal ing parser to see\]< the "leastimplaus ib le"  analysis of a highly deviantinput as to seek the correct analysis  of athoroughly  wel l - formed input.
The reasonfor beginning with a simple example is thatI ant ic ipate that the per formance of thesystem wil l  become more sensit ive to detai lsof the evaluat ion funct ion and anneal ingschedule as inputs become more complex andless wel l - formed,  and at present I have onlybegun to explore a l ternat ive evaluat ionfunct ions and anneal ing schedules.The schedule used for the run tobe i l lustrated was as fol lows.
Thestructure in i t ia l ly  ass igned to the str ingwas the "flat" structure in which each wordis an immediate const i tuent  of the root:\[S dj jnovidjn.
\] This tree is ass igned thevalue -2.26 by the t ree-evaluat ion function.The init ial  temperature (standard deviat ionof the Gaussian) was I.
The temperature wasreduced by 3% after every f i f t iethsuccess ive attempt to change the tree~ Thesystem was deemed to have "frozen" at thef irst temperature-drop at which each of the100 preceding attempts to change the treeei ther had been re jected or left the valueof the tree unchanged.153To give the reader a feeling forthe way an annealing run proceeds, I displaythe situation reached after every hundredthattempted tree-change.
On each line I showthe temperature reached immediately beforethe drop which occurs at that point, theproportion of the last hundred attemptedAttempts Temp.
Changes Value1002003004005O06007008009001000110012001300140015001600170018001900200021000.9700 .9130 ,8590 8080 ,7600 7150 6730 ,6330 ,5960 5610 ,5270 4960 4670 4390.4130.3890.36603440 3240,305028793 -1.3193 -1.5292 -1.0288 -2.3082 0.0097690 -0.53666 1.6469 -1.5173 -0.056275 -0.98458 1.5066 -0.18463 0.66854 0.32557 0.76018 4.938 5.2111 5.467 5.705 6.635 6.63changes which were accepted, the value ofthe current tree, and the tree itself.Nonterminal symbols are represented bycapitals written immediately after theopening bracket of the constituent theylabel; closing brackets are unlabelled.Current Tree\[Sdjj\[Rn\[Lo\[G\[Fvi\]\[J\[N\[Gdj\]n\].\]\]\]\]\]\[S\[Wd\[D\[Jjj\]no\]\]\[Fv\[Vidj\]\]\[Pn.\]\]\[S\[Pd\[Dj\[Jjn\]\]\[Nov\]i\[R\[Adj\]n\]\].\]\[Sd\[Dj\[V\[Fjn\]\[P\[V\[Gov\]\[Ji\[Ldj\]\]n\].\]\]\]\]\[S\[Vdj\]\[Njn\[W\[Tovidj\]n\]\].\]\[S\[N\[Jd\[T\[Pjj\]\[S\[N\[Tn\[Jov\]\]\[Tid\]\]j\]\]\]n\].\]\[S\[Sd\[Njj\[Nn\[No\[Vvi\]\]\]d\[Njn\]\]\].\]\[S\[N\[Nd\[Sjj\]\]no\]\[J\[A\[Tvi\]\[Tdjn\]\].\]\]\[S\[N\[D\[F\[Ad\[S\[Fjj\]\[Nno\]\]\]v\]\[Tid\]j\]n\].\]\[Sd\[D\[L\[Gj\[Njn\]\]\[Wov\]\]\[F\[F\[Lid\]jn\].\]\]\]\[S\[P\[Mdj\]\[Njn\]\[Jovi\]\[Ndj\]n\].\]\[S\[Ndjjn\]\[Novid\]j\[Mn.\]\]\[S\[N\[Dd\[Njj\]n\]ov\[Lidj\]n\].\]\[S\[Nd\[Njj\]n\]\[P\[Jo\[Rvi\[Sdj\]\]n\].\]\]\[S\[Nd\[V\[Njj\]n\]o\]\[S\[T\[Tvi\]\[Dd\[Njn\]\]\].\]\]\[S\[Ndjjn\[F\[Vov\]\[Pid\]\]\]\[Njn\].\]\[S\[Ndjjn\]\[F\[Vov\]\[Pi\[Nd\[Njn\]\]\]\].\]\[S\[N\[Ndjjn\]\[F\[Vov\]\[Pi\[Ndjn\]\]\]\].\]\[S\[Ndjjn\]\[F\[Vov\]\[Pi\[Ndjn\]\]\].\]\[S\[Ndjjn\]\[Vov\]\[Pi\[Ndjn\]\].\]\[S\[Ndjjn\]\[Vov\]\[Pi\[Ndjn\]\].\]On this run the system froze attemperature 0.287, after 2100 attemptedchanges of which 1173 were accepted.
Thestructure attained at freezing is thecorrect structure for the input sequence,according to our parsing scheme.
(Thesymbols N, P, V stand for noun phrase,prepositional phrase, and verb phrase -- thelatter in our terms referring to a sequenceof auxiliary and main verbs, not includingobject, complement, etc.
We recognize nointernal structure in a noun phrase such asthe ~uick brown fox.
)Not all runs of this pilot systemhave been as completely successful as this,though none have frozen on totally crazytrees.
Yet the range of possibilities outof which the system has winnowed the correctanalysis includes massive numbers of utterlycrazy structures: note for instance how inthe early stages of the run illustrated thesystem has considered a tree including agenitive phrase (G) consisting of a finiteclause (F) followed by an adjective phrase(J) -- a constituent which linguisticallymakes no sense at all.
Considering how manyalternative logically-possible solutions areavailable to the system, a few thousandsteps seems a small number by which to reachthe correct solution or even its vicinity.Although at present some mistakes are made,there is plenty of scope for improvingperformance by refining the grammaticalstatistics and evaluation function, andmodifying the annealing schedule.
At thisadmittedly very early stage I regard theprospects for parsing by annealing as highlypromising.1546.
Simulated annealing appealsstrongly to some writers (e.g.
Bridle &Moore 1984: 315) as a model of psychologicalperception mechanisms.
In the case ofgrammatical parsing, though, there is onerespect in which the model presented so faris quite implausible psychologically: itignores the left-to-right sequential mannerin which humans procesS written as well asspoken language.
There is a natural way toincorporate time into an annealing parserwhich not only is psychologically plausiblebut promises greatly to increase itsefficiency as a practical automatic system.Rather than a whole sentence beingsubmitted to the annealing system at once,in a "dynamic" annealing system parsingwould proceed in a series of rounds.
Theinput to the nth round would be an annealedparsing of the first n-1 words of thesentence, followed by the nth word;annealing would begin anew at meltingtemperature on this input.
The opportunityfor efficiency would arise from the factthat NL grammar only rarely forces thereader to backtrack -- the insight on whichMitchell Marcus's Parsifal system wasfounded (Marcus 1980).
Marcus's strategyinvolved a total exclusion of backtrackingfrom his central parsing system, with"garden path" sentences being handed over toa quite separate "higher level problemsolver" for processing.
However, Marcus'spredictions about a sharp categorization ofNL sentences into garden-paths andnon-garden-paths have provoked considerablecriticism.
In a dynamic annealing parser,all parts of the curreDtr tree would at allstages be available to revision, but therelative rarity of the need for backtrackingcould be exploited by adding a bias to thefunction which randomly selects nodes forreconsideration, so that nodes arereconsidered less frequently as they become"older".
Since the bulk of computing timein an annealing parser would undoubtedly beconsumed in calculating gains and losses forcandidate tree-changes, this system ofconcentrating the search for profitabletree-changes on the areas of the tree wheresuch changes are most likely to be foundcould be a good means of saving processingtime by reducing the total number of movesconsidered.7.
A problem that will not haveescaped the reader's attention is that Ihave discussed parsing purely in terms offinding surface parse-trees (which hapEensto be the task which the UCREL group areengaged on).
It is not obvious how toextend the annealing approach so as to yielddeep parses.
However, there is nothingabout simulated annealing that makes itintrinsically inapplicable to the task ofdeep parsing.
What needs to be done is todefine a class of logically-possible deepparse-trees and a class of moves betweenthem, and to find an evaluation functionwhich takes any pairing of a deep structurewith a surface word-sequence into alikelihood-value.
This task is verydifferent in kind from the work currentlydone by theoretical linguists and AIresearchers interested in underlying orlogical grammar, who tend to have littletime for statistical thinking, but that isnot to say that the task is necessarilysenseless or impossible.
Deep parsing, ifpossible at all, will presumably need toexploit semantic/"inferencing"consideratJons as well as information aboutgrammar in the narrow sense, but nothingsays that these matters might not be builtinto the evaluation function.8 Finaiiy, i t  may be that annealingis:useless as a parsing techn ique becausethe geometry of NL parsing space is wrong.Perhaps the space of English parse-trees(whether surface or deep) resembles theWitwatersrand rather than the Cotswolds,being an upland plateau riddled with deepgoldmines rather than a rolling landscapewhose treasures lie exposed in valleybottoms.
I conjecture that NLs areCotswold-like rather than Rand-like, andthat, if they were not, humans could notunderstand them.
Only empirical researchusing authentic data can settle thequestion.REFERENCESAckley, D.\[I., G.E.
Hinton, & T.J. Sejnowski1 985 "A learning algorithm forBoltzmann machines".
C_~nit iveScience 9.147-69.Atwell~ E.S., G.N.
Leech, & R.G.
Garside1984 "Analysis of the LOB Corpus:progress and prospects".
In J. Aarts& W. Meijs, eds., Cor~ ~ t i c s .Rodopi.Bridle, J.S.
& RoK.
Moore 1984 "Boltzmannmachines for speech patternprocessing".
Proceedings of theInstitute of Acoustics vol.
6 pt.
4pp.
315-22.Garside, R.G., G.N.
Leech, & G.R.
Sampson,eds.
Forthcoming.
The ComputationalAnalysis of Enqlish.
Longman.Jones, K.S.
& Y.A.
Wilks, eds.
1983Automatic Natural Language Parsinq.Ellis Horwood.King, M., ed.
1983 Parsinq NaturalL a n ~  Academic Press.Kirkpatrick, S., C.D.
Gelatt, & M.P.
Vecchi1983 "Optimization by simulatedannealing".
Science 220.671-80.Marcus, M.P.
1980 A Theor~ o_~f SyntaqticRecognition for Natural Language.
MITPress.Sapir, E. 1921.
La\[i u a ~  Harcourt, Brace& World.155
