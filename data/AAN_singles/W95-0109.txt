Automatic Construction of a Chinese Electronic Dictionary+Jing-Shin Chang, ?Yi-Chung Lin and ?
*Keh-Yih Su{ shin,lyc }@hermes.ee.nthu.edu.tw, kysu@bdc.com.tw+Department of Electrical EngineeringNatural Language Processing LabNational Tsing-Hua UniversityHsinchu, Taiwan 30043, ROC*Behavior Design Corporation2F, No.5, Industrial East Road IVScience-Based Industrial ParkHsinchu, Taiwan 30077, ROCABSTRACTIn this paper, an unsupervised approach for constructing a large-scale Chinese electronic dictionary issurveyed.
The main purpose is to enable cheap and quick acquisition of a large-scale dictionary from alarge untagged text corpus with the aid of the information i  a small tagged seed corpus.
The basic modelis based on a Viterbi reestimation technique.
During the dictionary construction process, it tries to optimizethe automatic segmentation and tagging process by repeatedly refining the set of parameters of theunderlying language model.
The refined parameters are then used to furtherget a better tagging result.
Inaddition, a two-class classifier, which is capable of classifying an n-gram either as a word or a non-word, isused in combination with the Viterbi training module to improve the system performance.Two different system configurations had been developed to construct he dictionary.
Theconfigurations include (1) a Viterbi word identification module followed by a Viterbi POS tagging moduleand (2) a two-class classification module as the postfilter for the above Viterbi word identification module.With a seed of 1,000 sentences and an untagged corpus of 311,591 sentences, the performance forbigram word identification is 56.88% in precision and 77.37% in recall when the two-class classifier isapplied to the word list suggested by the Viterbi word identification module.
The Viterbi part of speech tagreestimation stage gives the figures of 71.16% and 71.81% weighted precision rates and 73.42% and73.83% weighted recall rates for the 2 different configurations when using a seed corpus of 9676 sentences.1.
Introduction and System OverviewA large-scale lectronic dictionary is the fundamental component to many natural anguage andspoken language :processing applications uch as spelling correction, grammar checking, text-speechconversion, intelligent Chinese input methods and machine translation.
However, a large electronic dic-tionary for natural language processing may not be available.
This is true for the current Chinese languageprocessing community.
One possible way is to convert ageneral dictionary into its electronic form.
Firstly,however, ageneral dictionary may not be updated frequently to reflect he current status of language uses,and many new lexicon entries may not be available.
Secondly, a general dictionary may be lack of certainfield-specific terms such that the language processing system cannot make use of such unregistered wordsfor a domain-specific application.
Even with the term registered, it may not have the special syntactic orsemantic annotations for a particular domain.
For instance, a machine translation system for translatingcomputer manuals may need to update its lexicon frequently to catch up with the constantly changingcomputer technologies.
In this case, a general dictionary may not provide significant help in the particulardomain.Furthermore, the number of lexical entries in a practical electronic dictionary usually exceeds tens of107thousands.
Therefore, human involvement will be costly and time-consuming.
Even though somesupervised learning methods are possible for annotating text corpora (e.g., \[Chang 93\]), it is still a largeburden for tagging a large training corpus, and such approaches usually assume the existence of a largedictionary.
In addition, human involvement may introduce inconsistency in the lexicon entries.
Anautomatic and unsupervised dictionary construction approach is thus highly desirable.Because there are only a few manually constructed Chinese electronic dictionaries available forgeneral domain \[CKIP 90, BDC 93\], the techniques for automatic onstruction of large-scale Chineseelectronic dictionaries from text corpora will be exploited in this paper.
One particular difference betweenChinese text and English text is that there is no natural delimiters, like spaces, between Chinese words.
AChinese version of "This is a book.
", for instance, will look like "Thisisabook."
Therefore, extractingChinese lexicon entries and annotating the lexicon entries are much more difficult han other languages.
Anautomatic approach must be used first to segment the Chinese text corpus into segmented text for furtherprocessing.In this paper, an automatic approach to constructing an electronic dictionary, which contains lexicalentries and their possible parts of speech tags, is proposed.
In particular, we will use a reestimationtechnique, a small tagged seed corpus and a large untagged corpus to construct the dictionary.The word tokens embedded in a Chinese corpus can be acquired by segmenting the text corpus intoword tokens with a reestimation technique.
The reestimation technique, referred to as the Viterbi trainingprocedure for words (VTW), is used mainly to find possible word n-grams by maximizing the likelihoodof the segmentation patterns of the segmented text corpus.
However, more information may be used toidentify whether an n-gram is really a word entry.
In this paper, we thus also propose a two-classclassification (TCC) method for identifying the word entries; a character n-gram is classified as either aword or a non-word n-gram according to some useful features observed from the seed corpus and a largeunsegmented corpus.These two techniques, can be combined or used separately to form a system for automatic wordidentification.
In one configuration, we use a Viterbi reestimation algorithm to find out a list of candidatewords in the large, untagged text corpus.
We then use discriminative f atures, which provide intra-wordinformation and inter-character information for judging whether a candidate word is qualified as a trueword.The word segmentation patterns based on the dictionary extracted by the word reestimation process,or the two class classifier, or a concatenation f these two modules, are then automatically tagged with partof speech information with a part of speech reestimation method.
The reestimation process for POS taggingwill be referred to as a Viterbi Training process for Tags (VTT).2.
Automatic Construction of Electronic Dictionary with Reestimation ApproachThe fundamental building blocks for the above-mentioned automatic Chinese electronic dictionaryconstruction system contain the following modules: (i) automatic word extraction system, and (ii)automatic part-of-speech tagging system.
Figure 1 shows the block diagram of such a system, where theword extraction system is shown to be a word segmentation module implemented with the Viterbi Trainingprocedure for words.108Plain Text Corpus 1Segmented Seed Text \]SegmentationModul~(VTW) ~----I Segmentatlin Parameter~Reestimation |Segmented Text .
WordsI Part-of-SpeechTagging (VTT)lTagged TextCorpusTagged Seed CorpusTagging ParametersReestimation Tword-Tag PairsChineseElectronicDictionary(Word-Tag List)Figure 1 A Chinese Dictionary Construction SystemThe system reads a large untagged plain text and produces its segmented version based on asegmentation model (with or without TCC post-filtering).
The main purpose of the segmentation module isto segment the Chinese text corpus into words because there is no natural delimiter between Chinese wordsin a text.
After segmentation, each word in the segmented text is automatically tagged with its part ofspeech.
The possible parts of speech for each word in the segmented plain text are then collected to form aPOS annotated electronic dictionary.A Viterbi reestimation process, as outlined below, could be used both for the word segmentation a dPOS tagging tasks to optimize the tagging patterns (including segmentation patters and POS taggingpatterns) to a reasonable way.
The principle is to find a set of initial segmentation r tagging parameters firstfrom the small segmented ortagged seed corpus, and use this set of parameters to optimize the segmentationor POS tagging tasks.
After the task is done, the best tagging pattern is updated, and the set of parametersare reestimated based on the distribution of the new tagging patterns and the seed.
This process is repeateduntil a stopping criterion is met.Since only the best tagging pattern for each sentence is used for reestimating the parameters, such atraining procedure will be referred to as a Viterbi Training (VT) procedure, incontrast to an EM algorithm\[Dempster 77\], which considers all possible patterns and their expectations.
Since an EM version of thetraining procedure may require a long computation time, we will leave this option to future research.3.
Automatic Word Identification: Viterbi Training for Words (VTW)To compile an electronic dictionary (i.e., a word-tag list in the current task), we need to gather the word listwithin the corpus first.
Since there is no natural delimiter, like space, between Chinese words, all thecharacter n-grams in the text corpus are potential candidates for words.
The first lexicon acquisition task istherefore to identify appropriate words embedded in the text corpus which are not known to the seed corpus.This task could be, resolved by using a word segmentation model or a two-class classifier (to be describedin the next sections).109Rule-based approaches \[Ho 83, Chen 86, Yeh 91\] as well as probabilistic approaches \[Fan 88, Sproat90, Chang 91, Chiang 92\] to word segmentation had been proposed.
For a large-scale system, theprobabilistic approach is more practical when considering the capability of automatic training and cost.Practical probabilistic segmentation models can achieve quite satisfactory results \[Chang 91, Chiang 92\]provided that there is no unknown word to the system.A particular segmentation pattern can be expressed in terms of the words they have.
Given a string ofn Chinese characters c l , c2  .
.
.
.
.
c n , represented as c 1 , a Bayesian decision rule requires that we find theif' among all possible segmentation patterns Wj which maximizes the best word segmentation patternfollowing probability:~,, = argmaxP(Wj=w~:~J lc~)Wjwhere W j'mj ,\] are the mj words in the j-th alternative segmentation pattern W i .
In the current task, weassume that here is only a small segmented seed corpus available.
To reduce stimation error, we adopt hesimple model used in \[Chang 91\]:P(Wj= j,mi n'~ ~ mj w.  cl) , Pw..ri (which uses the product of word probabilities as the scoring function for segmentation.
Other morecomplicated segmentation models \[Chiang 92\] may get better esults.
However, amore complicated modelmight not be appropriate in the current unsupervised mode of learning since the estimation error for theparameters may be high due to the small seed corpus.
The following figure shows the block diagram of sucha system.UnSegmentedText CorpusIWord I .
WordSegmentation ~---I ProbabilityModule~ I I  P(W}~ Parameter '~ Segmented "Text Corpus It > t~lEstimation t _> 0Word List~\[ n-gram \] Seeqi,Frequency_> LBt=0Word LCandidate TMI d Corpust : iteration timeFigure 2 The block diagram of a Viterbi training model for word identificationNote the loop in re-estimating the word probabilities.
Initially, the n-grams embedded in theunsegmented corpus is gathered to form a word candidate list.
For practical purpose, we will only retainn-grams that are more frequent than a lower bound (LB=5), and only n-grams up to n=4 are considered(since most Chinese words are of length 1, 2, 3, or 4).
The frequency lower bound restriction is applied toreduce the number of possible word candidates; it also removes n-grams that are not sufficiently useful evenII0Ithough they are judged as word candidates.
Note that the words in the seed corpus are always included in thecandidate list.
In this sense, it plays the role of an initial dictionary.
Furthermore, all the characters(1-grams) are included to avoid the generation of 'unknown word regions' in the segmented patterns.Each word Candidate will be associated with a non-zero word probability; the various segmentationpatterns of the unsegmented corpus are then expanded in terms of such word candidates.
The path (i.e., thesegmentation pattern) with the highest score as evaluated according to the initial set of parameters (i.e.,word probabilities) is then marked as the best path for the current iteration.
A new set of parameters are thenre-estimated based on the best path.
This process repeats until the segmentation patterns no more change ora maximum number of iteration is reached.
We then derive the word list to be included in the electronicdictionary from the segmented text corpus.Initially, the word probability P(w.i,i ) is estimated from the small tagged seed corpus.
In thereestimation cycle, both the seed corpus and the segmented text corpus acquired in the previous iteration arejointly considered to get a better estimation for the word probabilities.4.
Automat ic  Word  Identi f icat ion: A Two-Class Classif ication (TCC)  ModelThe word list acquired through the above reestimation process is based on the optimization of thelikelihood value of the word segmentation pattern in a sentence, which implicitly takes the contextual wordsinto account.
However, it may not take into account he features for forming a word from characters.
It isdesirable, for instance, to take some "strength" measures for the chunks of characters into account in orderto know whether an n-gram is a word.
Therefore, an alternative approach, which could also be used tosupplement the VTW reestimation approach, is a Two-Class Classification model for classifying thecharacter n-grams into words and non-words.To identify whether an n-gram belongs to the word class (w) or the non-word class (w), each n-gramcould be associated with a feature vector ~ observed from the large untagged corpus.
It is then judged tosee whether it is more likely to be generated from a word model or a non-word model based on ~ .To simplify the the design of the classifier, we use a simple linear discrimination function forclassification:g (~,~) - -  ~ .~where Xs is the feature vector (or score vector) and Ws is a set of weights, acquired from the seedcorpus, for the various components of the score vector.
An n-gram will be classified as a word if theweighting sum of Ws and ~s is greater than zero (or larger than a threshold ~0)" (For better esults, ascore vector derived from a log-likelihood ratio test as in \[Su 94\] could be used.
Such an approach is beingstudied.
)For estimating the weights, the seed n-grams are firstly separated into the word and non-word classesby checking them against he known segmentation boundaries in the seed corpus.
The feature values for then-grams are estimated from the statistics of the n-grams in the large unsegmented corpus.
A set of initialweights are used to classify the word and non-word n-grams in the seed corpus according to their featurevalues.
The weights are then adjusted according to the misclassified instances in the word or non-wordn-grams until some optimization criteria for the classification results are achieved.
A probabilistic descentmethod is used for adjusting the weights \[Amari 67\].
In brief, the weights are adjusted in the direction whichis likely to decrease the risk, in terms of precision and recall, of the classifier.1115.
Features for ClassificationTo classify the character n-grams, we need to use some discriminative features for the classifier.
Inparticular, we found that the following features may be useful \[Wu 93, Su 94, Tung 94\].F requency.
Intuitively, a character n-gram is likely to be a word if it appears more frequently than theaverage.
Therefore, we use the frequency measure f (x i )  as the first feature for classification.Mutual Information.
In general, aword n-gram should contain characters that are strongly associated.
Onepossible measure to tell the strength of character association is the mutual information measure \[Church 90\]which had been applied successfully for measuring word association among 2-word compounds.
Thedefinition of mutual information for a bigram is defined as:e(x,y)l (x ,y)  = log p(x) xP(y)where P(x) and P(y) are the prior probabilities of the individual characters and P(x,y) is the joint probabilityfor the two characters to appear in the same 2-gram.
This measure is an indicator between the probabilityfor the individual characters to occur independently (denominator) and the probability for the characters toappear dependently (nominator).
If the mutual information measure is much larger than 0, then it tends tohave strong association.
To deal with n-grams with n greater than 2, such idea of dependent vs. independentwas extended to the following definition for the 3-gram mutual information:PD(x,Y, z) P(x,y,z)l (x,y,z)  = log pl(x,y,z  ) - log p1(x,y,z )Pt = P (x )P (y )P (z )  + P(x)P(y ,z )  + P(x ,y )P(z )In the above definition, the nominator PD means the probability for the three characters to occurdependently (i.e., the probability for the three characters to form a 3-character word), and the denominatorPl means the total probability (or average probability, to a scaling factor of 3) for the three characters toappear in the same 3-gram independently (i.e., by chance, possibly from two or three individual words).
Theextension could be made to other n-grams in a similar way.Entropy.
It is also desirable to know how the neighboring characters for an n-gram is distributed.
If thedistribution of the neighboring characters i random, it may suggest that the n-gram has a natural break atthe n-gram boundary, and thus suggest hat the n-gram is a potential word.
Therefore, we use the leftentropy H E and right entropy H R of an n-grammeasures are defined as follows \[Tung 94\]:HL(X ) =HR(X ) =as another feature for classification.
The left and right entropy-- :CP L( c i;x P L( C i;X )c i-~PR(X;Ci)IOgPR(X;Ci)Clwhere PL(Ci;X) are the probabilities of the left neighboring characters of the n-gram x, and PR(X;Ci) are theprobabilities of the right neighboring characters.
It is possible to use any function of the left and rightentropies for the classification task.
In this paper, the average of the left and right entropies is used as afeature.Furthermore, since the dynamic ranges of the frequencies and mutual information are very large, weused the log-scaled frequency, log-scaled mutual information and unsealed entropy measure as the featuresfor the two class classifier.
Without confusion, we will still use the terms of frequency and mutualinformation throughout the paper.
In other words, the score vector for the classifier is112\[Ys = \[log(f), log(1), H, 1\] .
(The constant '1' is used for training an appropriate threshold.)6.
Automatic Lexical Tagging: Viterbi Training for POS Tags (VTT)Once a word-segmented xt corpus is acquired, the segmented version can be annotated with parts ofspeech so as to: extract a POS annotated electronic dictionary.
The problem of POS tagging can beformulated as the problem of finding the best possible tagging pattern that maximizes the following lexicalscore \[Church 88, Lin 92\]:Sle x = e (z j lw  ) = P(t~\[w~)- -i /  P( W) x l'I P( t ilt i _ l )P( w ilt i )where Tj is the j-th possible set of lexical tags (parts of speech) for the segmentation pattern W. Thetagging process can thus be optimized based on the product of the POS tag transition probabilitiesP( t  ill i_ 1) and the distribution for P( w i l t / ) .The Viterbi training process for POS tagging based onthis optimization function is shown in Figure 3._~ Segmented SegmentedText Corpus \[ SeedACorpus ITagging t> 0~ ~t_>0Part-of-Speech |Probabi l i t ies Parameter iTagging ~-~ P(tilti.1) ~'- Estimation \[\[ iModule4' I\[&P(wilti) \[ t>0~ ~t_>0Tagged | | TaggedText Corpus Seed Corpus4,__~ Word-Tag t ?
iteration timeListFigure 3 Block Diagram for a Viterbi POS Tag Training SystemInitially, P( t i l t  i -1)  and P(  w ilti) are estimated from the small seed corpus.
Furthermore, achn-gram in the segmented text corpus will be assigned the most frequently encountered N POS tags in theseed corpus; in our experiments, N is selected as 10 since the most frequently used 10 POS tags alreadycover over 90% of the tags in the seed.During the training sessions, the various parts of speech sequences for the untagged text corpus areexpanded first, and the lexical score for each path is evaluated.
We then choose the path with the highestscore and the corresponding parts of speech of the path for re-estimating the required probabilities.
There-estimated probabilities are acquired from both the seed corpus and the highest-scored tagging results.This process repeats until the tagging results no more change or until a maximum number of iteration isreached.7.
Integrated Systems for Dictionary ConstructionThere are several ways to combine the above techniques to form an integrated automatic dictionary113construction system.
The following sections describe two such possibilities.
Their performances will becompared in the next chapter.7.1 Basic Model: Viterbi Training for Words + Viterbi Training for POS Tags (VTW + VTT)In the simplest topology, the Viterbi Training procedure for words is applied until the word segmentationparameters converge.
The segmented text thus acquired (and hence the word n-grams) is then labelled withPOS tags using the Viterbi Training procedure for POS tags.
As mentioned in Figure 2, the n-grams areacquired from the unsegmented text corpus; n-grams that are less frequent than a lower bound (LB) arefiltered out.
The remaining n-grams then form the word candidates for expanding the various egmentationpatterns.7.2 PostFiltering Model: Viterbi Training for Words + Two-Class Classifier PostFiltering + ViterbiTraining for Tags (VTW + TCC + VTT)In the Basic Model, all n-grams that occur more frequent than 5 times in the large text corpus are consideredpotential words.
Therefore, the number of possible segmentation patterns is extremely large.
In fact,however, only about 17% of bigrams, 3% of trigrams and 4% of 4-grams in the frequency-filtered wordcandidates are recognized as words in a human constructed dictionary of more than 80K entries.
Therefore,it is very difficult to find the best segmentation patterns, and thus the word list, with the Basic Model.
Torelieve the problem, the VTW module can be considered as a filter to the frequency-filtered wordcandidates, and we can further filter out inappropriate candidates by a TCC postfilter at the output end of theBasic Model.
Intuitively, the post-TCC module will have a better chance to find out real word candidatesfrom the output word list of the Basic Model, even though the VTW module may not perform well.
Theconfiguration is shown in Figure 4.UnSegmented ~ n-gramText CorpusFrequency_> LBWord FI .ISegmentation/~-I WordModule (VTW~\[ I CandidateSegmented WordText Corpus CandidateClassifier tWord List ~ g=Ws-X s_>O?WordSegmentationModuleI SegmentedText Corpus& Word ListPart-of-SpeeclrTaggingModule (V'IT~TaggedText CorpusWord-TagListFigure 4 VTW + TCC + VTT Configuration for Automatic Constructionof an Electronic DictionaryIn this topology, the Viterbi training procedure for words is applied first to acquire the possible wordlist which maximizes the likelihood of the segmentation patterns.
The two-class classifier is then used as apostfilter to confirm whether the candidates are real word n-grams.
The word n-grams thus acquired arethen used as the word candidates of a second word segmentation module to produce a segmented text114!corpus.
The segmented version is then labelled with POS tags using the Viterbi Training procedure for POStagging.8.
Experiment EnvironmentsIn our experiments, the untagged Chinese text corpus contains 311,591sentences (about 1,670,000words, 9 M byteS).
Its major domain is news articles and reports from the China Times daily news.
Thereare 246,036 distinct n-grams in this corpus, including 3,994 1-grams, 99,407 2-grams, 99,211 3-grams and43,424 -grams.
Since most Chinese words are not longer than 4 characters, only 1 -, 2-, 3- and 4-grams arein the word candidate list.A seed corpus of 9,676 sentences (127,052 words, about 415 K bytes) of computer domain isavailable.
A smaller seed of 1,000 sentences i  uniformly sampled from the above corpus.
This small seedcorpus contains 12,849 words (about 42K bytes).
The numbers of n-grams for n=l, 2, 3, 4 are 893, 7782,12289 and 12989, respectively.
Among these n-grams, only 1275 bigrams, 317 trigrams and 40 4-gramsare registered as words in a dictionary.Note that, since the numbers of word n-grams for n=3 and 4 are very small, the parameters (andperformances) estimated based on such n-grams will introduce large estimation errors.
Hence, the estimatedperformance will be very unreliable.
For this reason, the conclusions will be drawn from the 2-gramperformances; the performances for 3-gram and 4-gram will be listed for reference only.9.
Performance EvaluationTo get an estimation of the system performance automatically, the extracted dictionary is compared againsta manually constructed standard ictionary.
This is required because the extracted ictionary is large, andhuman verification will be both subjective and time-consuming.
The performance will be evaluated in termsof the word precision rate and recall rate for the VTW and the TCC modules.
The word precision rate is thenumber of n-grams common to the extracted word list and the standard ictionary divided by the numberof n-grams in the extracted word list; on the contrary, the recall is the number of common -grams dividedby the number of n-grams in the standard ictionary.
The VTT module will be estimated in terms of severalweighted tag precision and recall rate measures.The standard Word Dictionary to be compared with the extracted word list is acquired by merging theword lists of two electronically available dictionaries \[CKIP 90, BDC 93\] and the words included in theseed corpus.
It also excludes all n-grams which never appear in the 9767-sentence s ed corpus and theuntagged text corpus, because such n-grams will never be the input to the dictionary construction system.The merged ictionary, excluding entries that appear less frequently than the frequency lower bound (5),contains 17,005 bigram words, 2,524 trigram words and 1,612 4-gram words.The standard Word-Tag Dictionary to be compared with the extracted POSes is constructed from theBDC English-Chinese electronic dictionary \[BDC 93\].
The derived Word-Tag Dictionary contains 87,551entries, including 35,722 bigram words, 19,858 trigram words, and 24,092 4-gram words.
The tagset usedin this dictionary Contains 62 tags (including two punctuation tags).
Note that there are only 42 tags in thesmaller seed corpus of 1000 sentences, and the whole seed corpus of 9676 sentences contains only 47 POStags (including one punctuation tag).
Therefore, such missing tags will introduce some tag extracting errorsin the training processes.Since the Word Dictionary and Word-Tag Dictionary, which are used for comparison with theextracted ictionary, are constructed independently of the corpus from which the lexicon entries are115extracted, the reported performances could be greatly underestimated.
For instance, an n-gram which isidentified as a lexicon entry by the system but excluded from the Word Dictionary may not necessarily bea wrong word entry if it is judged by an expert lexicographer.
In the ideal case, the Word Dictionary andWord-Tag Dictionary should be constructed by an expert lexicographer based on the corpus for a faircomparison.
Unfortunately, we are unable to afford the man power for such an evaluation on the largecorpus.
Therefore, special attention should be taken when interpreting the performances reported in thefollowing sections.9.1 Performance for the Basic (VTW+VTT) TopologyTable 1 shows the performances in different stages for the Basic Model (columns 1-4) and thePostfiltering Model (columns 1-6) by using the small (1000-sentence) s ed corpus.
(Columns 1-4 are sharedbecause the Postfiltering is applied immediately after the Basic Model.)
The numerators in the parenthesesare the numbers of correctly identified n-grams; for precision, the denominators are the numbers of n-gramsin the extracted word lists; and for recall, they stand for the numbers of n-grams in the standard ictionary.The third column simply shows the initial precision and recall for the n-grams which are more frequentthan a frequency lower bound LB; such word candidates are the base for evaluating the effects of the VTWand TCC modules.
The Viterbi training process for extracting the word list goes through 4 iterations.
Withthe small seed corpus, it is observed that the precision for bigram words is improved from the initialprecision of 17.07% to 38.21%, corresponding to an increase of 21.14%, and the recall is dropped from100% to 89.87%, a decrease of 10.13%.
This shows that the Viterbi training procedure does provide asignificant improvement in precision while maintaining a reasonable r call.Note that, the precision for the initial (frequency-filterred) word candidates with respect o the dic-tionary is an indicator to the difficulty of the task.
It indicates how much percentage of word candidates arerecognized as words by the standard ictionary.
From the table, the initial word candidates in the largecorpus only include 3 to 4 % of the real word candidates which are recognized as words by a humanconstructed dictionary.
Furthermore, there are only 317 trigram words and 40 4-gram words in the trainingseed corpus.
As a result, it is difficult o spot such candidates from the large candidate list with a reasonableprecision and recall.
Hence, it is not surprising that the performance for the 3-grams and 4-grams is poor.For these reasons, we will make no further comments on the 3-gram and 4-gram performances which aretrained and observed under a very difficult training environment.
A few comments will be given on thesection for error analysis though.n- Processinggram StepPrecision2RecallPrecision3RecallPrecision4RecallFreq.
LB.
Two-Class Classifier VTW VTW-2 Filtering (LB=5) PostFiltering (TCC)17.07 (17,005/99,601) 38.21 (15,283/39,999) 56.80 (13,091/23,049) 56.88 (13,156/23,130)100.0 (17,005/ 17,005) 89.87 (15,283/17,005) 76.98 (13,091/17,005) 77.37 (13,156/17,005)2.54 (2,524/99,460) 6.01 (2,171/36,123) 6.02 (2,171/36,067) 6.12 (2,170/35,443)100.0 (2,524/2,524) 86.01 (2,171/2,524) 86.01 (2,171/2,524) 85.97 (2,170/2,524)3.71 (1,612/43,454) 5.81 (1,503/25,891) 6.21 (1,497/24,099) 6.31 (1,497/23,713)100.0 (1,612/1,612) 93.24 (1,503/1,612) 92.87 (1,497/1,612) 92.87 (1,497/1,612)Table 1.
Word Identification Performance for the VTW+VTT and VTW+TCC+VTT topologies(seed=1000 sentences)1169.2 Performance for the Postfiltering (VTW+TCC+VTT) TopologyThe performance of the PostFiltering model is shown in columns 1-6 of Table 1.
The two VTWmodules in Figure 4 are identical, and each VTW module goes through 4 training iterations.
With the smallseed corpus, the bigram performance is improved from 38.21% to 56.80% with a decrease of recall from89.87% to 76.98% after the post-filter is installed.
The global system achieves a precision rate of 56.88%at the recall rate of 77.37%.It is observed that, by using the large corpus (which is about en folds in size), the precisions are onlyslightly increased (by about 2%).
Therefore, the corpus size may not be a critical issue in this task.
A betterextraction model might be more likely to improve the system further.9.3 Error Analysis for Word Identification ModelsThe 3-gram and 4-gram precision rates are quite poor in the above tests.
An inspection of the entrieswhich are not recognized as words shows that some of the entries which should be considered words are notregistered in the standard general dictionary.
This means that the system does find some new words thatwere never seen by the standard ictionary, and thus are considered wrong.
Examples of such n-grams are:Some of the above examples are frequently encountered domain-specific terms in politics, economics,etc., which would be considered new words to a general dictionary.
Others include frequently encounteredproper names (company names, city names) or productive l xicon entries.
Although such terms may not beconsidered in constructing a general dictionary, it is useful to include such daily used high frequency termsin an electronic dictionary for practical processing purposes.
Therefore, the precision performance,estimated by comparing it with a general dictionary, is usually underestimated.Excluding such n-grams, the other incorrectly extracted n-grams have some special patterns whichsuggest that the extraction models might be refined by extracting or filtering out n-grams according to thesubstring patterns they have.
In particular, a3- gram (or 4-gram) may have the following relationships withits substrings:1. compositional: the n-gram can be decomposed into legal words (e.g., ~ ~ ~ ~ ("thisafternoon") - ~ l ~  ("announce ... today"), ~)k .~(" in tervene  the election")).. collocational: parts of the n-gram are legal words, the other parts are highly flexible (e.g.,"do not +VERBS" : ~ j~.~ - ~: :~1~ " ~ ~  ; "many + NOUNS" : ~ ~  - ;~ I~ i~ -~,-~/55 ; "not + ADJECTIVES" : qq~_3~z~ - ~\]J~l~ " ~2; t~) .3. idiomatic: none of the substrings are legal words, all single characters are highly flexible (e.g.,--~z~= ("cannot be enumerated one-by-one")).All the above patterns are related to the internal structure of the n-grams; our features and models,however, are more closely related to the intrinsic properties of the n-gram itself or the contextualinformation with the other n-grams.
This explains why some highly associated n-grams, which are not wordunits, are extracted as words by the system.
It also suggests that we could filter out some inappropriatecandidates which contain frequently encountered substrings and whose other parts show high entropy (or117similar measures.)
A few simple filtering rules based on such observation show that the precision could beincreased more effectively by refining the models in this way than increasing the seed corpus size.
A moreextensive survey is being studied.9.4 Tagging Accuracy: Weighted Tagging Recall and PrecisionBecause a word may be tagged ifferently under different context, a word identified by the VTW or TCCmodule may have more than one tag.
For the tagging accuracy, we use several measures to estimate theperformance.
Firstly, the number of word-tag pairs common to the extracted word-tag list and theWord-Tag Dictionary divided by the number of pairs in the extracted list is defined as the raw precisionrate; the raw recall rate is defined similarly as the number of common word-tag pairs divided by thenumber of word-tag pairs in the Word-Tag Dictionary.
With this measure, if a word in the extracted list hasM tags, then all the M word-tag pairs for the word are evaluated independently of the other pairs.Because the annotated tags for a word is usually considered as a whole when constructing a dictionaryentry, it may be desirable to define a per-word precision and per-word recall to measure how good the tagsfor a word is annotated, and then properly associate a weight o each word to evaluate the performance forthe whole system.The per-word precision for a word is defined as the number of tags commonly annotated in the dic-tionary entry and the extracted word-tag list for the word divided by the number of tags in the extractedword-tag entry for the word.
On the contrary, the number of common tags divided by the number of tags inthe corresponding dictionary entry is defined as the per-word recall for the word.
For instance, if a word istagged with the parts of speech\[n, v, a\] by the system, and it has the parts of speech \[n, adv\] in the standarddictionary, then the per-word recall will be 1/2 for this word and the per-word precision will be 1/3.Based on the per-word precision and recall, we define the average precision (resp.
recall) of thesystem as the sum of per-word precisions (resp.
recalls) divided by the number of words in the word list.Alternatively, we could take the frequencies ofthe n-grams into account so that more frequently used wordsare given a heavier weight on its per-word precision and recall.
Such weighted precision (or recall) isdefined as the sum of product of the per-word precision (or recall) and the word probability taken over eachword.9.5 Part-of-Speech Extraction PerformanceTo evaluate the performance of the Viterbi Part-of-Speech Tagging Module on the POS extractiontask, the words in the segmented and POS tagged text corpus are compared against the Word-TagDictionary mentioned in a previous ection.Since not all extracted words have a corresponding entry in the Word-Tag Dictionary, we onlyevaluate the performance of the POS extraction module over common entries in both the extracted ic-tionary and the standard ictionary.
The sizes of the common entries for the various models are around 8 to9 thousands entries.
On the average, each dictionary entry contains about 1.4 parts of speech, and each entryannotated by the Viterbi training module has about 1.7 parts of speech.Tables 2 shows the raw precision (Praw), average precision (Pavrg), weighted precision (Pwavg), andtheir corresponding recall rates.
(The left-hand side performance is acquired with a seed of 1000 sentences,and the right hand side with 9676 sentences.)
It seems that the performance is not significantly differentbetween the two different models.
This may imply that the segmented text corpus passed from the variousmodels do not have significant difference.
Furthermore, unlike in the word identification stage, the increase118Iin seed size does provide significant improvement on precision and recall.
With the large seed corpus, theweighted precision and recall are 71% and 73%.
Considering the fact that the parts of speech are optimizedfrom 10 parts of speech for each word, the results are reasonably acceptable.Basic Model Post-Filtering Basic Model Post-FilteringPraw 46.40 (7944/17119) 46.37 (7241/15615) 51.79 (8873/17132) 52.53 (8390/15973)Rraw 60.40 (7944/13153) 60.82 (7241/11906) 64.22 (8873/13816) 64.61 (8390/12986)Pavrg 53.07 53.17 60.21 61.25Ravrg 68.69 69.54 72.79 73.59Pwavrg 57.20 57.55 71.16 71.81Rwavrg 71.29 71.58 73.42 73.83Table 2.
Performance for Part-of-Speech Extraction of the Two Models(Seed=1000 and Seed = 9676, respectively)10.
Conc lud ing  RemarksIn this paper, we propose an unsupervised reestimation approach and a two-class classification methodto extract embedded words from a large unsegmented Chinese text, and assign possible parts of speech toeach word with a similar reestimation method.
An electronic dictionary with parts of speech informationcan thus be acquired automatically.It is observed that the system could acquire POS-tagged lexicon entries with a reasonably acceptableprecision and recall.
Since this approach adopts an unsupervised learning approach to construct the dic-tionary, its performance, in terms of precision and recall, is less satisfactory than a supervised learningstrategy, where a large tagged corpus and dictionary are used.
However, it requires little human interventionin the whole process, the cost to construct the dictionary, in terms of budget and time for pre-tagging, ismuch smaller than a supervised learning approach.
Therefore, it is worth while trading off the precisionrequirement with the cost of dictionary construction.
With the results of this preliminary study, it isexpected that the current echniques described here could form a good basis for constructing a better andautomatic dictionary construction system.References\[Amafi 67\] Amari, Shunichi, "A Theory of Adaptive Pattern Classifiers," IEEE Trans.
on Electronic Computers, Vol.EC-16, No.
3, pp.
299-307, 1967.\[BDC 93\] Behavior Design Corporation, "The BDC Chinese-English Electronic Dictionary: Version 2," Hsinchu,Taiwan, ROC, 1993.\[Chang 91\] Chang, Jyun-Sheng, C.-D. Chen and S.-D. Chen, "Chinese Word Segmentation through ConstraintSatisfaction and Statistical Optimization," (in Chinese) Proceedings of ROCLING-IV, ROC ComputationalLinguistics Conferences, pp.
147--165, National Chiao-Tung University, Hsinchu, Taiwan, ROC, 1991.\[Chang 93\] Chang, Chao-Huang and Cheng-Der Chen, "HMM-based Part-of-Speech Tagging for Chinese Corpora,"Proceedings of the Workshop on Very Large Corpora, WVLC-1, pp.
40-47, Ohio State University, 1993.\[Chen 86\] Chen, K,-J., C.-J.
Chen and L.-J.
Lee, "Analysis and Research in Chinese Sentence Segmentation a dConstruction,'!
Technical Report, TR-86--004, Taipei: Academia Sinica, 1986.\[Chiang 92\] Chiang, T.-H., J.-S. Chang, M.-Y.
Lin and K.-Y.
Su, "Statistical Models for Word Segmentation a d119Unknown Word Resolution", Proceedings of ROCL1NG V, pp.
121-146, National Taiwan University, Taiwan,ROC, 1992.\[Church 88\] Church, K., "A Stochastic Parts Program and Noun Phrase Parser for Unrestricted Text," ACL Proc.
2ndConf.
on Applied Natural Language Processing, pp.
136-143, Austin, Texas, USA, 9-12 Feb. 1988.\[Church 90\] Church, K. and P. Hanks, "Word Association Norms, Mutual Information, and Lexicography,"Computational Linguistics, vol.
16, pp.
22-29, Mar.
1990.\[CKIP 90\] Chinese Knowledge Information Processing Group, "The CKIP Electronic Dictionary," Academia Sinica,Taipei, Taiwan, ROC, 1990,\[Dempster 77\] Dempster, A. P., N. M. Laird and D. B. Rubin, "Maximum Likelihood from Incomplete Data via theEM Algorithm", Journal of the Royal Statistical Society, 39(B), pp.
1-38, 1977.\[Fan 88\] Fan, C.-K. and W.-H. Tsai, "Automatic Word Identification in Chinese Sentences by the RelaxationTechnique," Computer Processing of Chinese and Oriental Languages, vol.
4, no.
1, pp.
33--56, 1988.\[Ho 83\] Ho, W.-H., "Automatic Recognition of Chinese Words," master thesis, National Taiwan Institute ofTechnology, Taipei, Taiwan, 1983.\[Lin 92\] Lin, Y.-C., T.-H. Chiang and K.-Y.
Su, "Discrimination Oriented Probabilistic Tagging", Proceedings ofROCLING V, pp.
85-96, National Taiwan University, Taiwan, ROC, 1992.\[Sproat 90\] Sproat, R. and C. Shin, "A Statistical Method for Finding Word Boundaries in Chinese Text," ComputerProcessing of Chinese and Oriental Languages, vol.
4, no.
4, pp.
336--351, 1991.\[Su 94\] Su, K.-Y., M.-W. Wu and J.-S. Chang, "A Corpus-based Approach to Automatic Compound Extraction,"Proceedings of ACL 94, pp.
242-247, New Mexico State University, June, 1994.\[Tung 94\] Tung, Cheng-Huang and Hsi-Jian Lee, "Identification of Unknown Words from a Corpus," ComputerProcessing of Chinese & Oriental Languages, Vol.
8, pp.
131-145, (Proceedings of lCCPOL-94, pp.
412-417,Taejon, Korea,) Dec. 1994.\[Wu 93\] Wu, M.-W. and K.-Y.
Su, "Corpus-based Automatic Compound Extraction with Mutual Information andRelative Frequency Count," Proceedings ofROCLING VI, pp.
207-216, Nantou, Taiwan, ROC, Sep. 1993.\[Yeh 91\] Yeh, C.-L. and H.-J.
Lee, "Rule-Based Word Identification for Mandarin Chinese Sentences --- AUnification Approach," Computer Processing of Chinese and Oriental Languages, vol.
5, no.
2, pp.
97-- 118,March 1991.120
