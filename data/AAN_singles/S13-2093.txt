Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 554?561, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsBOUNCE: Sentiment Classification in Twitter using Rich Feature SetsNadin Ko?kciyan?, Arda C?elebi?, Arzucan O?zgu?r, Suzan U?sku?darl?Department of Computer EngineeringBogazici UniversityIstanbul, Turkey{nadin.kokciyan,arda.celebi,arzucan.ozgur,suzan.uskudarli}@boun.edu.trAbstractThe widespread use of Twitter makes it veryinteresting to determine the opinions and thesentiments expressed by its users.
The short-ness of the length and the highly informal na-ture of tweets render it very difficult to auto-matically detect such information.
This paperreports the results to a challenge, set forth bySemEval-2013 Task 2, to determine the posi-tive, neutral, or negative sentiments of tweets.Two systems are explained: System A for de-termining the sentiment of a phrase within atweet and System B for determining the senti-ment of a tweet.
Both approaches rely on richfeature sets, which are explained in detail.1 IntroductionTwitter consists of a massive number of posts on awide range of subjects, making it very interesting toextract information and sentiments from them.
Forexample, answering questions like ?What do Twitterusers feel about the brand X??
are quite interesting.The constrained length and highly informal natureof tweets presents a serious challenge for the auto-mated extraction of such sentiments.Twitter supports special tokens (i.e.
mentions andhashtags), which have been utilized to determine thesentiment of tweets.
In (Go et al 2009), emoticonsare used to label tweets.
In (Davidov et al 2010),Twitter emoticons as well as hashtags are used to la-bel tweets.
O?Connor et al(2010) demonstrateda correlation between sentiments identified in pub-lic opinion polls and those in tweets.
A subjectivity?
These authors contributed equally to this worklexicon was used to identify the positive and nega-tive words in a tweet.
In (Barbosa and Feng, 2010),subjective tweets are used for sentiment classifica-tion.
They propose the use of word specific (e.g.POS tags) and tweet specific (e.g.
presence of a link)features.
Most of these studies use their own anno-tated data sets for evaluation, which makes it diffi-cult to compare the performances of their proposedapproaches.Sentiment Analysis in Twitter 2013 (SemEval2013 Task 2) (Wilson et al 2013) presented a chal-lenge for exploring different approaches examin-ing sentiments conveyed in tweets: interval-level(phrase-level) sentiment classification (TaskA) andmessage-level sentiment classification (TaskB).
Sen-timent are considered as positive, negative, or neu-tral.
For TaskA, the goal is to determine the sen-timent of an interval (consecutive word sequence)within a tweet.
For TaskB, the goal is to determinesentiment of an entire tweet.
For example, let?s con-sider a tweet like ?Can?t wait until the DLC for ME3comes out tomorrow.
:-)?.
For TaskA, the interval0-1 (Can?t wait) is ?positive?
and the interval 10-10(:-)) is ?positive?.
For TaskB, this tweet is ?positive?.In this paper, we present two systems, one forTaskA and one for TaskB.
In both cases machinelearning methods were utilized with rich feature setsbased on the characteristics of tweets.
Our resultssuggest that our approach is promising for sentimentclassification in Twitter.2 ApproachThe task of detecting the sentiments of a tweet oran interval therein, is treated as a classification of554TaskATweetswithIntervalsPositiveClassifierTweetClassifierTaskAInterval ClassifierTaskBMultiple Binary ClassifierTaskBTweets+/-/0ClassifiedTweetsClassifiedTweet Intervals+/-/0Preprocessor+FeatureGeneratorNegativeClassifierLexiconsFigure 1: The Overview of BOUNCE Systemtweets into positive, negative, or neutral sets.
Fig-ure 1 gives the overview of our approach.
The Pre-processor module tokenizes the tweets that are usedby the Feature Generator.
At this stage, the tweetsare represented as feature vectors.
For TaskA, thefeature vectors are used by the Interval Classifierthat predicts the labels of the tweet intervals.
ForTaskB, the feature vectors are used by the PositiveClassifier and the Negative Classifier which reporton the positivity and negativity of the tweets.
TheTweet Classifier determines the tweet labels using arule-based method.
Each step is described in detailin the following subsections.2.1 LexiconsThe core of our approach to sentiment analysis relieson word lists that are used to determine the positiveand negative words or phrases.
Several acquired listsare used in addition to one that we curated.
AFINN(Nielsen, 2011) is the main sentiment word list in-cluding 2477 words rated between -5 to 5 for va-lence.
SentiWordNet (Baccianella et al 2010), de-rived from the Princeton English WordNet (Miller,1995), assigns positive, negative, or objective scoresto each synset in WordNet.
We considered the av-erage of a word?s synsets as its SentiWordNet score.Thus, synsets are disregarded and no disambiguationof the sense of a word in a given context is done.The SentiWordNet score of a word is not used if ithas objective synsets, since it indicates that the wordmight have been used in an objective sense.
We usea list of emotion words and categories that is createdby DeRose1.
Furthermore, a slang dictionary down-1http://derose.net/steve/resources/emotionwords/ewords.htmlloaded from the Urban Dictionary2 containing over16,000 phrases (with no sentiment) is used.
Finally,we curated a sentiment word list initiated with a listof positive and negative words obtained from Gen-eral Inquirer (Stone et al 1966), and refined by sen-timent emitting words from a frequency-based or-dered word list generated from the training data setof SemEval-2013 Task A.
Naturally, this list is morespecialized to the Twitter domain.2.2 PreprocessingPrior to feature generation, tweets were prepro-cessed to yield text with more common wording.For this, CMU?s Ark Tokenizer and Part-of-Speech(POS) Tagger (Gimpel et al 2011), which has beenspecifically trained for tweets, was used.
Tweets aretokenized and POS tagged.2.3 Feature SetsIn addition to the lexical or syntactic characteristics,the manner in which tweets are written may revealsentiment.
Orthogonal shapes of words (esp.
fullyor partially capitalized words), expressions of a sin-gle word or a phrase in the form of a hashtag, posi-tions of certain tokens in a tweet are prominent char-acteristics of tweets.
In addition to these, tweets mayconvey multiple sentiments.
This leads to sequence-based features, where we append features for eachsentiment emitted by a word or a phrase in a tweet.Moreover, since TaskA asks for sentiment of inter-vals in a tweet, we also engineer features to catchclues from the surrounding context of the interval,2http://www.urbandictionary.com555such as the sentiments and lengths of the neighbor-ing intervals.
For TaskB, the usage of hashtags andlast words in tweets were occasionally sentimental,thus we considered them as features as well.
We ex-plain all features in detail in Section 3.2.4 ClassificationMaximum entropy models (Berger et al 1996) havebeen used in sentiment analysis (Fei et al 2010).They model all given data and treat the remainder asuniform as possible making no assumptions aboutwhat is not provided.
For this, TaskA system usesthe MaxEnt tool (Zhang, 2011).Naive Bayes is a simple probabilistic model basedon Bayes?
Theorem that assumes independence be-tween features.
It has performed well in sentimentclassification of Twitter data (Go et al 2009; Bifetand Frank, 2010).
TaskB data was not evenly dis-tributed.
There were very few negative tweets com-pared to positive tweets.
Using a single classifierto distinguish the classes from each other resultedin poor performance in identifying negative tweets.Therefore, TaskB system utilizes multiple binaryclassifiers that use the one-vs-all strategy.
MaximumEntropy and Naive Bayes models were consideredand the model that performed best on the develop-ment set was chosen for each classifier.
As a result,the positive classifier (Bpos) is based on the Max-imum Entropy model, whereas the negative classi-fier (Bneg) is based on Naive Bayes.
TaskB systemuses the Natural Language Toolkit (Loper and Bird,2002).3 SystemsIn this section, TaskA and TaskB systems are ex-plained in detail.
All features used in the final ex-periments for both tasks are shown in Table 1.3.1 TaskA SystemTaskA is a classification task where we classify agiven interval as having positive, negative or neutralsentiment.
TaskA feature sets are shown in Table 1.lexical features: These features use directlywords (or tokens) from tweets as features.
single-word feature uses the word of the single-word inter-vals, whereas slang features are created for match-ing uni-grams and bi-grams from our slang dictio-nary.
We also use emoticons as features, as well asthe words or phrases that emit emotion according tothe lexicons described in Section 2.1.score-based features: These features use thescores obtained from the AFINN and SentiWordNet(SWN) lexicons.
We use separate scores for the pos-itive and negative sentiments, since one interval maycontain multiple words with opposite sentiment.
Incase of multiple positive or negative occurances, wetake the arithmetic mean of those.shape-based features: These features capture thelength of an interval, whether it contains a capital-ized word or all words are capitalized, whether itcontains a URL, or ends with an exclamation mark.tag-based features: In addition to numeric val-ues of sentiments, we use the tokens ?positive?
and?negative?
to express the type of sentiment.
Whenmultiple words emit a sentiment in a given interval,their corresponding tokens are appended to create asingle feature out of it, sequences.
Moreover, wehave another set of features which also contains thePOS tags of these sentiment words.indicator features: These features are used in or-der to expose how many sentiment emitting wordsfrom our currated large lexicon exist in a given inter-val.
hasNegation indicates the presence of a nega-tion word like not or can?t in the interval, whereasnumOfPosIndicators and numOfNegIndicators givesthe number of tokens that convey positive and nega-tive sentiment, respectively.context features: In addition to the features gen-erated from the given interval, these features capturethe context information from the neighboring inter-vals.
Feature surroundings combines the length ofthe interval along with the lengths of the intervals onboth sides, whereas surrounding-shape and extra-surrounding-shape features use number of positiveand negative sentiment indicators for the intervals.We also use their normalized forms (those startingwith norm-) where we divide the number of indi-cators by the length of the interval.
Features with-extra- use two adjacent intervals from both sides.Intervals that are not available are represented withNA.3.2 TaskB SystemTaskB is a classification task where we determinethe sentiment (positive, negative, or neutral) of atweet.
TaskB system uses a rule-based method to556Feature Set Feature Example Feature Instance used bylexical-basedsingle-word-* single-word-worst A, Bslang-* slang-shit A, Bposemoticons-* emoticons-:) Aemitted-emotions-* emitted-emotions-angry A, Bscore-basedafinn-positive:#, afinn-negative:# afinn-positive:4, afinn-negative:-2 A, Bswn-positive:#, swn-negative:# swn-positive:2, swn-negative:-3 Ashape-basedlength-# length-10 AhasAllCap-T/F hasAllCap-T AfullCap-T/F fullCap-T AhasURL-T/F hasURL-F A, BendsWExlamation-T/F endsWExlamation-T A, Bnegtag-basedour-seq-* our-seq-positive-positive A, Bour-tag-seq-*, swn-seq-*, swn-tag-seq-* afinn-seq-positive-a-positive-n Aafinn-seq-*, afinn-tag-seq-* afinn-seq-positive-a-negative-n AindicatorshasNegation-T/F hasNegation-F AnumOfPosIndicators-# numOfPosIndicators-2 AnumOfNegIndicators-# numOfNegIndicators-0 Acontextsurroundings-#-#-# surroundings-1-2-NA Asurr-shape-#-#-# surrounding-shape-NA-2-1 Aextra-surr-shape-#-#-#-#-# extra-surr-shape-NA-2-1-0-1 Anorm-surr-shape-#-#-# norm-surr-shape-0.5-0.2-0.0 Anorm-extra-surr-shape-#-#-#-#-# norm-extra-surr-shape-NA-0.5-0.2-0.0-0.2 Aleft-sentiment-*, right-sentiment-* left-sentiment-positive Atwitter-tagshasEmoticon-T/F hasEmoticon-T BhasMention-T/F hasMention-T BhasHashtag-T/F hasHashtag-F B[emoticon|mention|hash]-count-# mention-count-3 Brepetitionunigram-*n unigram-[no+] B$character-count-# o-count-7 Blastwordlastword-*n lastword-[OMG+] Blastwordshape-* lastwordshape-XXXX Bchat chatword-* for word ?gz?
: chatword-congratulations Binterjection interjection-*n interjection-[lo+l] Bnegationnegword-*n negword-never Bnegnegword-count-# negword-count-3 Bnegnegcapword-count-# negcapword-count-1 Bneghashhashword-* hashword-good Bhashtag-#* hashtag-#good Bhash-sentiment-[positive|negative] hash-sentiment-positive Blingemotion [noun|verb|adverb|adjective]-$emotion noun-fear Boursentfor tweet: a nice morning..
I hate work.. damn!oursent-* oursent-nice, oursent-hate, oursent-damn Boursent-longseq-* oursent-longseq-pnn Boursent-shortseq-* oursent-shortseq-pn Boursent-first-last-* oursent-first-last-pn Bafinn-phrasesphrase-firstsense-[positive|negative] phrase-firstsense-positive Bphrase-lastsense-[positive|negative] phrase-lastsense-negative Bafinnword-* afinnword-nice, afinnword-hate, afinnword-damn Bafinn-firstsense-[positive|negative] afinn-firstsense-positive Bafinn-lastsense-[positive|negative] afinn-lastsense-positive Bemo emo-pattern-* for =) : emo-pattern-HAPPY BTable 1: Feature sets used in TaskA and TaskB557Dataset Type Positive Negative Neutral+Objective Tot.
No.
of InstancesTaskATraining 5290 (5865) 2771(3120) 16118 (17943) 24179 (26928)Development 589 (648) 392 (430) 1993 (2202) 2974 (3280)Test 2734 1541 160 4435TaskBTraining 3274 (3640) 1291 (1458) 4155 (4586) 8720 (9684)Development 523 (575) 309 (340) 674 (739) 1506 (1654)Test 1572 601 1640 3813Table 2: Number of instances used in TaskA and TaskBdecide on the sentiment label of a tweet.
For eachtweet, the probabilities of belonging to the posi-tive class (Probpos) and negative class (Probneg)are computed by the Bpos and Bneg classifiers, re-spectively.
If Probpos is greater than Probneg, andgreater than a predefined threshold, then the tweetis classified as ?positive?, otherwise it is classifiedas ?neutral?.
On the other hand, if Probneg isgreater than Probpos, and greater than the prede-fined threshold, then the tweet is classified as ?neg-ative?, otherwise it is classified as ?neutral?.
Thethreshold is set to 0.45, since it gives the optimal F-score on the development set.
TaskB features alongwith examples are shown in Table 1.twitter-tags: hasEmoticon, hasMention, ha-sURL, and hasHashtag indicate whether the corre-sponding term (e.g.
mention) exists in the tweet.repetition: Words with repeating letters areadded as a feature ?n.
?n represents the normalizedversion (i.e., no repeating letters) of a word.
For ex-ample, ?nooooooo?
is shortened to [no+].
We alsokeep the count of the repeated character.wordshape: Shape of each word in a tweet is con-sidered.
For example, the shape of ?NOoOo!!?
is?XXxXx!!
?.lastword: The normalized form and the shape ofthe last word are used as features.
For example, ifthe lastword is ?OMGG?, then lastword ?
[OMG+]?and lastwordshape ?XXXX?
are used as features.chat: A list of chat abbreviations that express sen-timent is manually created.
Each abbreviation is re-placed by its corresponding word.interjection: An interjection is a word that ex-presses an emotion or sentiment (e.g.
hurraah,loool).
Interjection wordn is used as a feature.negation: We manually created a negation list ex-tended by word clusters from (Owoputi et al 2013).A negation word is represented by spellings suchas not, n0t, and naht.
Each negation wordn (e.gneve[r+]) is considered.
We keep the count of nega-tion words and all capitalized negation words.hash: If the hashtag is ?#good?
then #good andgood become hash features.
If the hashtag is a sen-timent expressing word according to our sentimentword list, then we keep the sentiment information.lingemotion: Nodebox Linguistics3 packagegives emotional values of words for expressions ofemotions such as fear and sadness.
POS augmentedexpression information is used as a feature.oursent: Each word in a tweet that exists in oursentiment word list is considered.
When multiplesentiment expressing words are found, a sentimentsequence feature is used.
oursent-longseq keepsthe long sequence, whereas oursent-shortseq keepssame sequence without repetitive sentiments.
Wealso consider the first and last sentiments emitted bya tweet.afinn: We consider each word that exists inAFINN.
If a negation exists before this word, theopposite sentiment is considered.
For example, if atweet contains the bigram ?not good?, then the senti-ment of the bigram is set to ?negative?.
The AFINNscores of the positive and negative words, as well asthe first and last sentiments emitted by the tweet areconsidered.phrases: Each n-gram (n > 1) of a tweet thatexists in our sentiment phrase list is considered.afinn-phrases: Phrases are retrieved using thephrases feature.
Each sentiment that appears ina phrase is kept, hence we obtain a sentiment se-quence.
The first and last sentiments of this se-quence are also considered.
Then, the phrases areremoved from the tweet text and the afinn feature isapplied.emo: We manually created an emoticon list where3http://nodebox.net/code/index.php/Linguistics558each term is associated with an emotion pattern suchas HAPPY.
These emotion patterns are used as a fea-ture.others: Bpos uses the slang feature from the lexi-cal feature set, and Bneg uses endsWExlamation fea-ture from the indicators feature set.4 Experiments and Results4.1 DataThe data set provided by the task organizers was an-notated by using Amazon Mechanical Turk4.
Theannotations of the tweets in the training and devel-opment sets were provided to the task participants.However, the tweets had to be downloaded fromTwitter by using the script made available by the or-ganizers.
We were unable to download all the tweetsin the training and development sets, since sometweets were deleted and others were not publiclyaccessible due to their updated authorization status.The number of actual tweets (numbers in parenthe-ses) and the number of collected tweets are shown inTable 2.
Almost 10% of the data for both tasks aremissing.
For the test data, however, the tweets weredirectly provided to the participants.4.2 Results on TaskAWe start our experiments with features generatedfrom lexicons and emoticons.
Called our baseline,it achieved an f-score of 47.8 on the devset in Ta-ble 3.
As we add other features at each step, wereach an average f-score of 81.6 on the devset atthe end.
Among those features, the most contribut-ing ones are lexical feature single-word, indicatorfeature hasNegation, and especially shape featurelength.
The success of the length feature is mostlydue to the nature of intervals, where the long onestend to be neutral, and the rest are mostly positiveor negative.
Another noteworthy result is that ourcurated word list contributed more compared to theothers.
When the final model is used on the test set,we get the results in Table 5.
Having low neutral f-score might be due to the fact that there were only afew neutral intervals in the test set, which might in-dicate that their characteristics may not be the sameas the ones in the devset.4https://www.mturk.com/mturk/Added Features Avg.
F-Scoreafinn-positive, afinn-negetive47.8swn-positive, swn-negative,emoticons, emitted-emotions+ hasAllCap, fullCap, hasURL,50.1endsWExclamation+ slang 51.5+ single-word 56.8+ afinn-seq, swn-seq, afinn-tag-seq,57.7swn-tag-seq+ our-seq, our-tag-seq 60.2+ hasNegation 64.8+ numOfPosIndicators,65.3numOfNegIndicators+ length 75.2+ left-sentiment, right-sentiment 76.5+ surroundings, surrounding-shape 78.9+ extra-surrounding-shape 80.6+ norm-surrounding-shape,81.6norm-extra-surrounding-shapeTable 3: Macro-averaged F-Score on the TaskA dev.
setAdded FeaturesAverageF-Scoreoursent (baseline) 58.59+ afinn-phrases 64.64+ tags + hash 65.43+ interjection + chat 65.53+ emo + lingemotion 65.92+ repetition + lastword 66.01+ negation + others 66.32Table 4: Macro-averaged F-Score on the TaskB dev.
set4.3 Results on TaskBThe baseline model is considered to include oursentfeature that gives an average f-score of 58.59.
Next,we added the afinn-phrases feature which increasedthe average f-score to 64.64.
This increase can beexplained by the sentiment scores and sequence pat-terns that afinn-phrases is based on.
Following thatmodel, the other added features slightly increasedthe average f-score to 66.32 as shown in Table 4.The final model is used over the test set of TaskB,where we obtained an f-score of 63.53 as shown inTable 5.559Class Precision Recall F-ScoreTestApositive 89.7 88.3 89.0negative 86.6 82.7 84.6neutral 10.7 18.1 13.4average(pos+neg) 88.15 85.5 86.8TestBpositive 82.3 55.6 66.4negative 48.7 80.2 60.6neutral 68.2 73.3 70.7average(pos+neg) 65.56 67.93 63.53Table 5: Results on the test sets for both tasks5 ConclusionWe presented two systems one for TaskA (a Maxi-mum Entropy model) and one for TaskB (MaximumEntropy + Naive Bayes models) based on using richfeature sets.
For Task A, we started with a baselinesystem that just uses ordinary features like sentimentscores of words.
As we added new features, we ob-served that lexical features and shape-based featuresare the ones that contribute most to the performanceof the system.
Including the context features and theindicator feature for negations led to considerableimprovement in performance as well.
For TaskB,we first created a baseline model that uses sentimentwords and phrases from the AFINN lexicon as fea-tures.
Each feature that we added to the system re-sulted in improvement in performance.
The nega-tion and endsWExclamation features only improvedthe performance of the negative classifier, whereasthe slang feature only improved the performance ofthe positive classifier.Our results show that using rich feature sets withmachine learning algorithms is a promising ap-proach for sentiment classification in Twitter.
OurTaskA system ranked 3rd among 23 systems andTaskB system ranked 4th among 35 systems partici-pating in SemEval 2013 Task 2.ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
SentiWordNet 3.0: An Enhanced Lex-ical Resource for Sentiment Analysis and OpinionMining.
In Proceedings of the Seventh Conferenceon International Language Resources and Evaluation(LREC?10), Valletta, Malta, May.
European LanguageResources Association (ELRA).Luciano Barbosa and Junlan Feng.
2010.
Robust senti-ment detection on twitter from biased and noisy data.In Proceedings of the 23rd International Conferenceon Computational Linguistics: Posters, COLING ?10,pages 36?44, Stroudsburg, PA, USA.
Association forComputational Linguistics.Adam L. Berger, Stephen A. Della Pietra, and VincentJ.
Della Pietra.
1996.
A maximum entropy approachto natural language processing.
Computational Lin-guistics, 22:39?71.Albert Bifet and Eibe Frank.
2010.
Sentiment knowl-edge discovery in twitter streaming data.
In Proceed-ings of the 13th international conference on Discov-ery science, DS?10, pages 1?15, Berlin, Heidelberg.Springer-Verlag.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,COLING ?10, pages 241?249, Stroudsburg, PA, USA.Association for Computational Linguistics.Xiaoxu Fei, Huizhen Wang, and Jingbo Zhu.
2010.
Sen-timent word identification using the maximum entropymodel.
In International Conference on Natural Lan-guage Processing and Knowledge Engineering (NLP-KE), pages 1?4.Kevin Gimpel, Nathan Schneider, Brendan O?Connor,Dipanjan Das, Daniel Mills, Jacob Eisenstein, MichaelHeilman, Dani Yogatama, Jeffrey Flanigan, andNoah A. Smith.
2011.
Part-of-speech tagging fortwitter: annotation, features, and experiments.
InProceedings of the 49th Annual Meeting of the Asso-ciation for Computational Linguistics: Human Lan-guage Technologies: short papers - Volume 2, HLT?11, pages 42?47.
Association for Computational Lin-guistics.Alec Go, Richa Bhayani, and Lei Huang.
2009.
Twit-ter sentiment classification using distant supervision.Technical report, Stanford University.Edward Loper and Steven Bird.
2002.
Nltk: the naturallanguage toolkit.
In Proceedings of the ACL-02 Work-shop on Effective tools and methodologies for teach-ing natural language processing and computationallinguistics - Volume 1, ETMTNLP ?02, pages 63?70,Stroudsburg, PA, USA.
Association for ComputationalLinguistics.George A. Miller.
1995.
Wordnet: A lexical database forenglish.
Communications of the ACM, 38:39?41.Finn A?.
Nielsen.
2011.
A new ANEW: Evaluation ofa word list for sentiment analysis in microblogs.
InProceedings of the ESWC2011 Workshop on ?MakingSense of Microposts?
: Big things come in small pack-ages.560Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.From Tweets to Polls: Linking Text Sentiment toPublic Opinion Time Series.
In Proceedings of theInternational AAAI Conference on Weblogs and SocialMedia.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A. Smith.
2013.Improved part-of-speech tagging for online conver-sational text with word clusters.
In Proceedings ofNAACL.Philip J.
Stone, Dexter C. Dunphy, Marshall S. Smith,and Daniel M. Ogilvie.
1966.
The General Inquirer:A Computer Approach to Content Analysis.
MITPress.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, SaraRosenthal, Veselin Stoyanov, and Alan Ritter.
2013.SemEval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the International Workshop on Se-mantic Evaluation, SemEval ?13, June.Le Zhang.
2011.
Maximum entropy modeling toolkit forpython and c++.
http://homepages.inf.ed.ac.uk/lzhang10/maxent_toolkit.html.Accessed: 2013-04-13.561
