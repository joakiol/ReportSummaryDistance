Proceedings of the Workshop on Statistical Machine Translation, pages 86?93,New York City, June 2006. c?2006 Association for Computational LinguisticsContextual Bitext-Derived Paraphrases in Automatic MT EvaluationKarolina Owczarzak Declan Groves Josef Van Genabith Andy WayNational Centre for Language TechnologySchool of ComputingDublin City UniversityDublin 9, Ireland{owczarzak,dgroves,josef,away}@computing.dcu.ieAbstractIn this paper we present a novel methodfor deriving paraphrases during automaticMT evaluation using only the source andreference texts, which are necessary forthe evaluation, and word and phrasealignment software.
Using target languageparaphrases produced through word andphrase alignment a number of alternativereference sentences are constructed auto-matically for each candidate translation.The method produces lexical and low-level syntactic paraphrases that are rele-vant to the domain in hand, does not useexternal knowledge resources, and can becombined with a variety of automatic MTevaluation system.1 IntroductionSince their appearance, BLEU (Papineni et al,2002) and NIST (Doddington, 2002) have been thestandard tools used for evaluating the quality ofmachine translation.
They both score candidatetranslations on the basis of the number of n-gramsit shares with one or more reference translationsprovided.
Such automatic measures are indispen-sable in the development of machine translationsystems, because they allow the developers to con-duct frequent, cost-effective, and fast evaluationsof their evolving models.These advantages come at a price, though: anautomatic comparison of n-grams measures onlythe string similarity of the candidate translation toone or more reference strings, and will penalizeany divergence from them.
In effect, a candidatetranslation expressing the source meaning accu-rately and fluently will be given a low score if thelexical choices and syntactic structure it contains,even though perfectly legitimate, are not present inat least one of the references.
Necessarily, thisscore would not reflect a much more favourablehuman judgment that such a translation would re-ceive.The limitations of string comparison are thereason why it is advisable to provide multiple ref-erences for a candidate translation in the BLEU- orNIST-based evaluation in the first place.
While(Zhang and Vogel, 2004) argue that increasing thesize of the test set gives even more reliable systemscores than multiple references, this still does notsolve the inadequacy of BLEU and NIST for sen-tence-level or small set evaluation.
On the otherhand, in practice even a number of references donot capture the whole potential variability of thetranslation.
Moreover, often it is the case that mul-tiple references are not available or are too difficultand expensive to produce: when designing a statis-tical machine translation system, the need for largeamounts of training data limits the researcher tocollections of parallel corpora like Europarl(Koehn, 2005), which provides only one reference,namely the target text; and the cost of creating ad-ditional reference translations of the test set, usu-ally a few thousand sentences long, often exceedsthe resources available.
Therefore, it would be de-sirable to find a way to automatically generate le-gitimate translation alternatives not present in thereference(s) already available.86In this paper, we present a novel method thatautomatically derives paraphrases using only thesource and reference texts involved in for theevaluation of French-to-English Europarl transla-tions produced by two MT systems: statisticalphrase-based Pharaoh (Koehn, 2004) and rule-based Logomedia.1 In using what is in fact a minia-ture bilingual corpus our approach differs from themainstream paraphrase generation based on mono-lingual resources.
We show that paraphrases pro-duced in this way are more relevant to the task ofevaluating machine translation than the use of ex-ternal lexical knowledge resources like thesauri orWordNet2, in that our paraphrases contain bothlexical equivalents and low-level syntactic vari-ants, and in that, as a side-effect, evaluation bitext-derived paraphrasing naturally yields domain-specific paraphrases.
The paraphrases generatedfrom the evaluation bitext are added to the existingreference sentences, in effect creating multiple ref-erences and resulting in a higher score for the can-didate translation.
Our hypothesis, confirmed bythe experiments in this paper, is that the scoresraised by additional references produced in thisway will correlate better with human judgmentthan the original scores.The remainder of this paper is organized as fol-lows: Section 2 describes related work; Section 3describes our method and presents examples ofderived paraphrases; Section 4 presents the resultsof the comparison between the BLUE and NISTscores for a single-reference translation and thesame translation using the paraphrases automati-cally generated from the bitext, as well as the cor-relations between the scores and human judgment;Section 5 discusses ongoing work; Section 6 con-cludes.22.1Related workWord and phrase alignmentSeveral researchers noted that the word andphrase alignment used in training translation mod-els in Statistical MT can be used for other purposesas well.
(Diab and Resnik, 2002) use second lan-guage alignments to tag word senses.
Working onan assumption that separate senses of a L1 word2.21 http://www.lec.com/2 http://wordnet.princeton.edu/can be distinguished by its different translations inL2, they also note that a set of possible L2 transla-tions for a L1 word may contain many synonyms.
(Bannard and Callison-Burch, 2005), on the otherhand, conduct an experiment to show that para-phrases derived from such alignments can be se-mantically correct in more than 70% of the cases.Automatic MT evaluationThe insensitivity of BLEU and NIST to per-fectly legitimate variation has been raised, amongothers, in (Callison-Burch et al, 2006), but thecriticism is widespread.
Even the creators of BLEUpoint out that it may not correlate particularly wellwith human judgment at the sentence level (Pap-ineni et al, 2002), a problem also noted by (Och etal., 2003) and (Russo-Lassner et al, 2005).
A sideeffect of this phenomenon is that BLEU is less re-liable for smaller data sets, so the advantage it pro-vides in the speed of evaluation is to some extentcounterbalanced by the time spent by developerson producing a sufficiently large test data set inorder to obtain a reliable score for their system.Recently a number of attempts to remedy theseshortcomings have led to the development of otherautomatic machine translation metrics.
Some ofthem concentrate mainly on the word reorderingaspect, like Maximum Matching String (Turian etal., 2003) or Translation Error Rate (Snover et al,2005).
Others try to accommodate both syntacticand lexical differences between the candidatetranslation and the reference, like CDER (Leuschet al, 2006), which employs a version of edit dis-tance for word substitution and reordering;METEOR (Banerjee and Lavie, 2005), which usesstemming and WordNet synonymy; and a linearregression model developed by (Russo-Lassner etal., 2005), which makes use of stemming, Word-Net synonymy, verb class synonymy, matchingnoun phrase heads, and proper name matching.A closer examination of these metrics suggeststhat the accommodation of lexical equivalence isas difficult as the appropriate treatment of syntacticvariation, in that it requires considerable externalknowledge resources like WordNet, verb class da-tabases, and extensive text preparation: stemming,tagging, etc.
The advantage of our method is that itproduces relevant paraphrases with nothing morethan the evaluation bitext and a widely availableword and phrase alignment software, and thereforecan be used with any existing evaluation metric.873 Contextual bitext-derived paraphrasesThe method presented in this paper rests on acombination of two simple ideas.
First, the compo-nents necessary for automatic MT evaluation likeBLEU or NIST, a source text and a reference text,constitute a miniature parallel corpus, from whichword and phrase alignments can be extractedautomatically, much like during the training for astatistical machine translation system.
Second, tar-get language words ei1, ?,  ein aligned as the likelytranslations to a source language word fi are oftensynonyms or near-synonyms of each other.
Thisalso holds for phrases: target language phrases epi1,?, epin aligned with a source language phrase fpiare often paraphrases of each other.
For example,in our experiment, for the French word questionthe most probable automatically aligned Englishtranslations are question, matter, and issue, whichin English are practically synonyms.
Section 3.2presents more examples of such equivalent expres-sions.3.13.2Experimental designFor our experiment, we used two test sets,each consisting of 2000 sentences, drawn ran-domly from the test section of the Europarl parallelcorpus.
The source language was French and thetarget language was English.
One of the test setswas translated by Pharaoh trained on 156,000French-English sentence pairs.
The other test setwas translated by Logomedia, a commerciallyavailable rule-based MT system.
Each test set con-sisted therefore of three files: the French sourcefile, the English translation file, and the Englishreference file.Each translation was evaluated by the BLEUand NIST metrics first with the single reference,then with the multiple references for each sentenceusing the paraphrases automatically generatedfrom the source-reference mini corpus.
A subset ofa 100 sentences was randomly extracted from eachtest set and evaluated by two independent humanjudges with respect to accuracy and fluency; thehuman scores were then compared to the BLEUand NIST scores for the single-reference and theautomatically generated multiple-reference files.Word alignment and phrase extractionWe used the GIZA++ word alignment soft-ware3 to produce initial word alignments for ourminiature bilingual corpus consisting of the sourceFrench file and the English reference file, and therefined word alignment strategy of (Och and Ney,2003; Koehn et al, 2003; Tiedemann, 2004) toobtain improved word and phrase alignments.For each source word or phrase fi that isaligned with more than one target words orphrases, its possible translations ei1, ..., ein wereplaced in a list as equivalent expressions (i.e.synonyms, near-synonyms, or paraphrases of eachother).
A few examples are given in (1).
(1) agreement - accordanceadopted - implementedmatter - lot - casefunds - moneyarms - weaponsarea - aspectquestion ?
issue ?
matterwe would expect - we cer-tainly expectbear on - are centredaroundAlignment divides target words andphrases into equivalence sets; each set correspondsto one source word/phrase that was originallyaligned with the target elements.
For example, forthe French word citoyens three English words weredeemed to be the most appropriate translations:people, public, and citizens; therefore these threewords constitute an equivalence set.
AnotherFrench word population was aligned with twoEnglish translations: population and people; so theword people appears in two equivalence set (thisgives rise to the question of equivalence transitiv-ity, which will be discussed in Section 3.3).
Fromthe 2000-sentence evaluation bitext we derived 769equivalence sets, containing in total 1658 words orphrases.
Each set contained on average two orthree elements.
In effect, we produced at least oneequivalent expression for 1658 English words orphrases.An advantage of our method is that the tar-get paraphrases and words come ordered with re-3 http://www.fjoch.com/GIZA++88spect to their likelihood of being the translation ofthe source word or phrase ?
each of them is as-signed a probability expressing this likelihood, sowe are able to choose only the most likely transla-tions, according to some experimentally estab-lished threshold.
The experiment reported here wasconducted without such a threshold, since the wordand phrase alignment was of a very high quality.3.33.43.5Domain-specific lexical and syntacticparaphrasesIt is important to notice here how the para-phrases produced are more appropriate to the taskat hand than synonyms extracted from a general-purpose thesaurus or WordNet.
First, our para-phrases are contextual - they are restricted to onlythose relevant to the domain of the text, since theyare derived from the text itself.
Given the contextprovided by our evaluation bitext, the word area in(1) turns out to be only synonymous with aspect,and not with land, territory, neighbourhood, divi-sion, or other synonyms a general-purpose thesau-rus or WordNet would give for this entry.
Thisallows us to limit our multiple references only tothose that are likely to be useful in the context pro-vided by the source text.
Second, the phrase align-ment captures something neither a thesaurus norWordNet will be able to provide: a certain amountof syntactic variation of paraphrases.
Therefore, weknow that a string such as we would expect in (1),with the sequence noun-aux-verb, might be para-phrased by we certainly expect, a sequence ofnoun-adv-verb.Open and closed class itemsOne important conclusion we draw fromanalysing the synonyms obtained through wordalignment is that equivalence is limited mainly towords that belong to open word classes, i.e.
nouns,verbs, adjectives, adverbs, but is unlikely to extendto closed word classes like prepositions or pro-nouns.
For instance, while the French preposition ?can be translated in English as to, in, or at, depend-ing on the context, it is not the case that these threeprepositions are synonymous in English.
The divi-sion is not that clear-cut, however: within the classof pronouns, he, she, and you are definitely notsynonymous, but the demonstrative pronouns thisand that might be considered equivalent for somepurposes.
Therefore, in our experiment we excludeprepositions and in future work we plan to examinethe word alignments more closely to decidewhether to exclude any other words.Creating multiple referencesAfter the list of synonyms and paraphrases isextracted from the evaluation bitext, for eachreference sentence a string search replaces everyeligible word or phrase with its equivalent(s) fromthe paraphrase list, one at a time, and the resultingstring is added to the array of references.
Theoriginal string is added to the array as well.
Thisprocess results in a different number of referencesentences for every test sentence, depending onwhether there was anything to replace in the refer-ence and how many paraphrases we have availablefor the original substring.
One example of thisprocess is shown in (2).
(2) Original reference:i admire the answer mrs parlygave this morning but we haveturned a blind eye to thatParaphrase 1:i admire the reply mrs parlygave this morning but we haveturned a blind eye to thatParaphrase 2:i admire the answer mrs parlygave this morning however wehave turned a blind eye tothatParaphrase 3:i admire the answer mrs parlygave this morning but we haveturned a blind eye to itTransitivityAs mentioned before, an interesting questionthat arises here is the potential transitivity of ourautomatically derived synonyms/paraphrases.
Itcould be argued that if the word people is equiva-lent to public according to one set from our list,and to the word population according to anotherset, then public can be thought of as equivalent topopulation.
In this case, the equivalence is not con-troversial.
However, consider the following rela-tion: if sure in one of the equivalence sets issynonymous to certain, and certain in a different89set is listed as equivalent to some, then treatingsure and some as synonyms is a mistake.
In ourexperiment we do not allow synonym transitivity;we only use the paraphrases from equivalence setscontaining the word/phrase we want to replace.Multiple simultaneous substitutionNote that at the moment the references we areproducing do not contain multiple simultaneoussubstitutions of equivalent expressions; for exam-ple, in (2) we currently do not produce the follow-ing versions:(3) Paraphrase 4:i admire the reply mrs parlygave this morning however wehave turned a blind eye tothatParaphrase 5:i admire the answer mrs parlygave this morning however wehave turned a blind eye to itParaphrase 6:i admire the reply mrs parlygave this morning but we haveturned a blind eye to itThis can potentially prevent higher n-grams beingsuccessfully matched if two or more equivalentexpressions find themselves within the range of n-grams being tested by BLEU and NIST.
To avoidcombinatorial problems, implementing multiplesimultaneous substitutions could be done using alattice, much like in (Pang et al, 2003).4 ResultsAs expected, the use of multiple referencesproduced by our method raises both the BLEU andNIST scores for translations produced by Pharaoh(test set PH) and Logomedia (test set LM).
Theresults are presented in Table 1.BLEU NISTPH single ref 0.2131 6.1625PH multi ref 0.2407 7.0068LM single ref 0.1782 5.5406LM multi ref 0.2043 6.3834Table 1.
Comparison of single-reference and multi-reference scores for test set PH and test set LMThe hypothesis that the multiple-referencescores reflect better human judgment is also con-firmed.
For 100-sentence subsets (Subset PH andSubset LM) randomly extracted from our test setsPH and LM, we calculated Pearson?s correlationbetween the average accuracy and fluency scoresthat the translations in this subset received fromtwo human judges (for each subset) and the single-reference and multiple-reference sentence-levelBLEU and NIST scores.There are two issues that need to be noted atthis point.
First, BLEU scored many of the sen-tences as zero, artificially leveling many of theweaker translations.4 This explains the low, al-though still statistically significant (p value <0.015) correlation with BLEU for both single andmultiple reference translations.
Using a version ofBLEU with add-one smoothing we obtain consid-erably higher correlations.
Table 2 shows Pear-son?s correlation coefficient for BLEU, BLEUwith add-one smoothing, NIST, and human judg-ments for Subsets PH.
Multiple paraphrase refer-ences produced by our method consistently lead toa higher correlation with human judgment forevery metric.6Subset PHMetricsinglerefmultirefH & BLEU 0.297 0.307H & BLEU smoothed 0.396 0.404H & NIST  0.323 0.355Table 2.
Pearson?s correlation between humanjudgment and single-reference and multiple-reference BLEU, smoothed BLEU, and NIST forsubset PH (of test set PH)The second issue that requires explanation isthe lower general scores Logomedia?s translationreceived on the full set of 2000 sentences, and theextremely low correlation of its automatic evalua-tion with human judgment, irrespective of thenumber of references.
It has been noticed (Calli-4 BLEU uses a geometric average while calculating the sen-tence-level score and will score a sentence as 0 if it does nothave at least one 4-gram.5 A critical value for Pearson?s correlation coefficient for thesample size between 90 and 100 is 0.267, with p < 0.01.6 The significance of the rise in scores was confirmed in aresampling/bootstrapping test, with p < 0.0001.90son-Burch et al, 2006) that BLEU and NIST fa-vour n-gram based MT models such as Pharaoh, sothe translation produced by Logomedia scoredlower on the automatic evaluation, even though thehuman judges rated Logomedia output higher thanPharaoh?s translation.
Both human judges consis-tently gave very high scores to most sentences insubset LM (Logomedia), and as a consequencethere was not enough variation in the scores as-signed by them to create a good correlation withthe BLEU and NIST scores.
The average humanscores for the subsets PH and LM and the coeffi-cients of variation are presented in Table 3.
It iseasy to see that Logomedia?s translation received ahigher mean score (on a scale 0 to 5) from the hu-man judges and with less variance than Pharaoh.Mean score  VariationSubset PH 3.815 19.1%Subset LM 4.005 16.25%Table 3.
Human judgment mean scores and coeffi-cients of variation for Subset PH and Subset LMAs a result of the consistently high human scoresfor Logomedia, none of the Pearson?s correlationscomputed for Subset LM is high enough to be sig-nificant.
The values are lower than the criticalvalue 0.164 corresponding to p < 0.10.Subset LMMetricsinglerefmultirefH & BLEU 0.046* 0.067*H & BLEU smoothed 0.163* 0.151*H & NIST  0.078* 0.116*Table 4.
Pearson?s correlation between humanjudgment and single-reference and multiple-reference BLEU, smoothed BLEU, and NIST forsubset LM (of test set LM).
* denotes values with p >0.10.5 Current and future workWe would like to experiment with the way inwhich the list of equivalent expressions is pro-duced.
One possible development would be to de-rive the expressions from a very large trainingcorpus used by a statistical machine translationsystem, following (Bannard and Callison-Burch,2005), for instance, and use it as an external wider-purpose knowledge resource (rather than a currentdomain-tailored resource as in our experiment),which would be nevertheless improve on a thesau-rus in that it would also include phrase equivalentswith some syntactic variation.
According to (Ban-nard and Callison-Burch, 2005), who derived theirparaphrases automatically from a corpus of over amillion German-English Europarl sentences, thebaseline syntactic and semantic accuracy of thebest paraphrases (those with the highest probabil-ity) reaches 48.9% and 64.5%, respectively.
Thatis, by replacing a phrase with its one most likelyparaphrase the sentence remained syntacticallywell-formed in 48.9% of the cases and retained itsmeaning in 65% of the cases.In a similar experiment we generated para-phrases from a French-English Europarl corpus of700,000 sentences.
The data contained a consid-erably higher level of noise than our previous ex-periment on the 2000-sentence test set, eventhough we excluded any non-word entities fromthe results.
Like (Bannard and Callison-Burch,2005), we used the product of probabilities p(fi|ei1)and p(ei2|fi) to determine the best paraphrase for agiven English word ei1.
We then compared the ac-curacy across four samples of data.
Each samplecontained 50 randomly drawn words/phrases andtheir paraphrases.
For the first two samples, theparaphrases were derived from the initial 2000-sentence corpus; for the second two, the para-phrases were derived from the 700,000-sentencecorpus.
For each corpus, one of the two samplescontained only one best paraphrase for each entry,while the other listed all possible paraphrases.
Wethen evaluated the quality of each paraphrase withrespect to its syntactic and semantic accuracy.
Interms of syntax, we considered the paraphrase ac-curate either if it had the same category as theoriginal word/phrase; in terms of semantics, werelied on human judgment of similarity.
Tables 5and 6 summarize the syntactic and semantic accu-racy levels in the samples.ParaphrasesDerived fromBest All2000-sent.
corpus 59% 60%700,000-sent.
corpus 70% 48%Table 5.
Syntactic accuracy of paraphrases91ParaphrasesDerived fromBest All2000-sent.
corpus 83% 74%700,000-sent.
corpus 76% 68%Table 6.
Semantic accuracy of paraphrasesAlthough it has to be kept in mind that thesepercentages were taken from relatively small sam-ples, an interesting pattern emerges from compar-ing the results.
It seems that the average syntacticaccuracy of all paraphrases decreases with in-creased corpus size, but the syntactic accuracy ofthe one best paraphrase improves.
This reflects theidea behind word alignment: the bigger the corpus,the more potential alignments there are for a givenword, but at the same time the better their order interms of probability and the likelihood to obtainthe correct translation.
Interestingly, the same pat-tern is not repeated for semantic accuracy, butagain, these samples are quite small.
In order toaddress this issue, we plan to repeat the experimentwith more data.Additionally, it should be noted that certainexpressions, although not completely correct syn-tactically, could be retained in the paraphrase listsfor the purposes of machine translation evaluation.Consider the case where our equivalence set lookslike this:(4) abandon ?
abandoning ?abandonedThe words in (4) are all inflected forms of the verbabandon, and although they would produce ratherungrammatical paraphrases, those ungrammaticalparaphrases still allow us to score our translationhigher in terms of BLEU or NIST if it contains oneof the forms of abandon than when it containssome unrelated word like piano instead.
This isexactly what other scoring metrics mentioned inSection 2 attempt to obtain with the use of stem-ming or prefix matching.6 ConclusionsIn this paper we present a novel combinationof existing ideas from statistical machine transla-tion and paraphrase generation that leads to thecreation of multiple references for automatic MTevaluation, using only the source and referencefiles that are required for the evaluation.
Themethod uses simple word and phrase alignmentsoftware to find possible synonyms and para-phrases for words and phrases of the target text,and uses them to produce multiple reference sen-tences for each test sentence, raising the BLEU andNIST evaluation scores and reflecting humanjudgment better.
The advantage of this methodover other ways to generate paraphrases is that (1)unlike other methods, it does not require extensiveparallel monolingual paraphrase corpora, but itextracts equivalent expressions from the miniaturebilingual corpus of the source and referenceevaluation files; (2) unlike other ways to accom-modate synonymy in automatic evaluation, it doesnot require external lexical knowledge sources likethesauri or WordNet; (3) it extracts only synonymsthat are relevant to the domain in hand; and (4) theequivalent expressions it produces include a certainamount of syntactic paraphrases.The method is general and it can be used withany automatic evaluation metric that supports mul-tiple references.
In our future work, we plan to ap-ply it to newly developed evaluation metrics likeCDER and TER that aim to allow for syntacticvariation between the candidate and the reference,therefore bringing together solutions for the twoshortcomings of automatic evaluation systems:insensitivity to allowable lexical differences andsyntactic variation.ReferencesSatanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgments.
Proceed-ings of the ACL 2005 Workshop on Intrinsic andExtrinsic Evaluation Measures for MT and/or Sum-marization: 65-73.Colin Bannard and Chris Callison-Burch.
2005.
Para-phrasing with Bilingual Parallel Corpora.
Proceed-ings of the 43rd Annual Meeting of the Association forComputational Linguistics (ACL 2005): 597-604.Chris Callison-Burch, Miles Osborne and PhilippKoehn.
2006.
Re-evaluating the role of BLEU inMachine Translation Research.
To appear in Pro-ceedings of EACL-2006.Mona Diab and Philip Resnik.
2002.
An unsupervisedMethod for Word Sense Tagging using Parallel Cor-pora.
Proceedings of the 40th Annual Meeting of the92Association for Computational Linguistics, Philadel-phia, PA.George Doddington.
2002.
Automatic Evaluation of MTQuality using N-gram Co-occurrence Statistics.
Pro-ceedings of Human Language Technology Confer-ence 2002: 138?145.Philipp Koehn, Franz Och and Daniel Marcu.
2003.Statistical Phrase-Based Translation.
Proceedings ofthe  Human Language Technology Conference (HLT-NAACL 2003): 48-54.Philipp Koehn.
2004.
Pharaoh: a beam search decoderfor phrase-based statistical machine translation mod-els.
Machine translation: From real users to re-search.
6th Conference of the Association forMachine Translation in the Americas (AMTA 2004):115-124.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
Proceedings of MTSummit 2005: 79-86.Gregor Leusch, Nicola Ueffing and Hermann Ney.2006.
CDER: Efficient MT Evaluation Using BlockMovements.
To appear in Proceedings of the 11thConference of the European Chapter of the Associa-tion for Computational Linguistics (EACL 2006).Franz Josef Och and Hermann Ney.
2003.
A SystematicComparison of Various Statistical Alignment Modes.Computational Linguistics, 29:19?51.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.Syntax for statistical machine translation.
Technicalreport, Center for Language and Speech Processing,John Hopkins University, Baltimore, MD.Bo Pang, Kevin Knight and Daniel Marcu.
2003.
Syn-tax-based Alignment of Multiple Translations: Ex-tracting Paraphrases and Generating New Sentences.Proceedings of Human Language Technology-NorthAmerican Chapter of the Association for Computa-tional Linguistics (HLT-NAACL) 2003: 181?188.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automaticevaluation of machine translation.
In Proceedings ofACL: 311-318.Grazia Russo-Lassner, Jimmy Lin, and Philip Resnik.2005.
A Paraphrase-based Approach to MachineTranslation Evaluation.
Technical Report LAMP-TR-125/CS-TR-4754/UMIACS-TR-2005-57, Uni-versity of Maryland, College Park, MD.Mathew Snover, Bonnie Dorr, Richard Schwartz, JohnMakhoul, Linnea Micciula and Ralph Weischedel.2005.
A Study of Translation Error Rate with Tar-geted Human Annotation.
Technical Report LAMP-TR-126, CS-TR-4755, UMIACS-TR-2005-58, Uni-versity of Maryland, College Park.
MD.J?rg Tiedemann.
2004.
Word to word alignment strate-gies.
Proceedings of the 20th International Confer-ence on Computational Linguistics (COLING 2004):212-218.Joseph P. Turian, Luke Shen, and I. Dan Melamed.2003.
Evaluation of Machine translation and ItsEvaluation.
Proceedings of MT Summit 2003: 386-393.Ying Zhang and Stephan Vogel.
2004.
Measuring con-fidence intervals for the machine translation evalua-tion metrics.
TMI-2004: Proceedings of the 10thConference on Theoretical and Methodological Is-sues in Machine Translation: 85-94.93
