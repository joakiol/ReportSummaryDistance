Proceedings of SSST, NAACL-HLT 2007 / AMTA Workshop on Syntax and Structure in Statistical Translation, pages 57?64,Rochester, New York, April 2007. c?2007 Association for Computational LinguisticsGeneration in Machine Translation from Deep Syntactic TreesKeith HallCenter for Language and Speech ProcessingJohns Hopkins UniversityBaltimore, MD 21218keith hall@jhu.eduPetr Ne?mecInstitute of Formal and Applied LinguisticsCharles UniversityPrague, Czech Republicnemec@ufal.mff.cuni.czAbstractIn this paper we explore a generativemodel for recovering surface syntax andstrings from deep-syntactic tree structures.Deep analysis has been proposed for anumber of language and speech process-ing tasks, such as machine translation andparaphrasing of speech transcripts.
In aneffort to validate one such formalism ofdeep syntax, the Praguian Tectogrammat-ical Representation (TR), we present amodel of synthesis for English which gen-erates surface-syntactic trees as well asstrings.
We propose a generative modelfor function word insertion (prepositions,definite/indefinite articles, etc.)
and sub-phrase reordering.
We show by way ofempirical results that this model is ef-fective in constructing acceptable Englishsentences given impoverished trees.1 IntroductionSyntactic models for language are being reintro-duced into language and speech processing sys-tems thanks to the success of sophisticated statisti-cal models of parsing (Charniak and Johnson, 2005;Collins, 2003).
Representing deep syntactic rela-tionships is an open area of research; examples ofsuch models are exhibited in a variety of grammat-ical formalisms, such as Lexical Functional Gram-mars (Bresnan and Kaplan, 1982), Head-drivenPhrase Structure Grammars (Pollard and Sag, 1994)and the Tectogrammatical Representation (TR) ofthe Functional Generative Description (Sgall et al,1986).
In this paper we do not attempt to analyze thedifferences of these formalisms; instead, we showhow one particular formalism is sufficient for au-tomatic analysis and synthesis.
Specifically, in thispaper we provide evidence that TR is sufficient forsynthesis in English.Augmenting models of machine translation (MT)with syntactic features is one of the main fronts ofthe MT research community.
The Hiero model hasbeen the most successful to date by incorporatingsyntactic structure amounting to simple tree struc-tures (Chiang, 2005).
Synchronous parsing mod-els have been explored with moderate success (Wu,1997; Quirk et al, 2005).
An extension to this workis the exploration of deeper syntactic models, suchas TR.
However, a better understanding of the syn-thesis of surface structure from the deep syntax isnecessary.This paper presents a generative model for surfacesyntax and strings of English given tectogrammati-cal trees.
Sentence generation begins by insertingauxiliary words associated with autosemantic nodes;these include prepositions, subordinating conjunc-tions, modal verbs, and articles.
Following this, thelinear order of nodes is modeled by a similar gen-erative process.
These two models are combined inorder to synthesize a sentence.The Amalgam system provides a similar modelfor generation from a logical form (Corston-Oliveret al, 2002).
The primary difference between ourapproach and that of the Amalgam system is thatwe focus on an impoverished deep structure (akin to57logical form); we restrict the deep analysis to con-tain only the features which transfer directly acrosslanguages; specifically, those that transfer directlyin our Czech-English machine translation system.Amalgam targets different issues.
For example,Amalgam?s generation of prepositions and subordi-nating conjunctions is severely restricted as most ofthese are considered part of the logical form.The work of Langkilde-Geary (2002) on the Halo-gen system is similar to the work we present here.The differences that distinguish their work fromours stem from the type of deep representation fromwhich strings are generated.
Although their syntac-tic and semantic representations appear similar tothe Tectogrammatical Representation, more explicitinformation is preserved in their representation.
Forexample, the Halogen representation includes mark-ings for determiners, voice, subject position, anddative position which simplifies the generation pro-cess.
We believe their minimally specified results arebased on input which most closely resembles the in-put from which we generate in our experiments.Amalgam?s reordering model is similar to the onepresented here; their model reorders constituents ina similar way that we reorder subtrees.
Both themodel of Amalgam and that presented here differconsiderably from the n-gram models of Langkildeand Knight (1998), the TAG models of Bangaloreand Rambow (2000), and the stochastic generationfrom semantic representation approach of Soricutand Marcu (2006).
In our work, we order the local-subtrees1 of an augmented deep-structure tree basedon the syntactic features of the nodes in the tree.
Byfactoring these decisions to be independent for eachlocal-subtree, the set of strings we consider is onlyconstrained by the projective strucutre of the inputtree and the local permutation limit described below.In the following sections we first provide a briefdescription of the Tectogrammatical Representationas used in our work.
Both manually annotated andsynthetic TR trees are utilized in our experiments;we present a description of each type of tree as wellas the motivation for using it.
We then describe thegenerative statistical process used to model the syn-thesis of analytical (surface-syntactic) trees based1A local subtree consists of a parent node (governor) and it?simmediate children.FORM:LEMM:FUNC:FORM:LEMM:FUNC:POS:  'NN'FORM:LEMM:FUNC:POS:  'RB'FORM:LEMM:FUNC:POS:  'VBN'T_M:  'SIM'_'IND'FORM:LEMM:FUNC:POS:  'NN'FORM:LEMM:FUNC:POS:  'NN'FORM:LEMM:FUNC:POS:  'NN'FORM:LEMM:FUNC:POS:  'JJ'#2#SENTnetworknetworkACTNownowTWHENopenedopenPREDbureaubureauPATnewsnewsRSTRcapitalcapitalLOCHungarianhungarianRSTRFigure 1: Example of a manually annotated, Synthetic TRtree (see Section 2.2).Reference: Now the network has opened a news bureau inthe Hungarian capitalEach sentence has an artificial root node labeled #.
Verbs con-tain their tense and mood (labeled T M).on the TR trees.
Details of the model?s featuresare presented in the following section.
Finally wepresent empirical results for experiments using boththe manually annotated and automatically generateddata.2 Tectogrammatical (Deep) SyntaxThe Tectogrammatical Representation (TR) comesout of the Praguian linguistic theory known asthe Functional Generative Description of language(Sgall et al, 1986).
TR attempts to capture deepsyntactic relationships based on the valency of pred-icates (i.e., function-argument structure) and modifi-cation of participants (i.e., nouns used as actors, pa-tients, etc.).
A key feature of TR is that dependencyrelationships are represented only for autosemanticwords (content words), meaning that synsemanticwords (syntactic function words) are encoded as fea-tures of the grammatical relationships rather than theactual words.
Abstracting away from specific syn-tactic lexical items allows for the representation tobe less language-specific making the representationattractive as a medium for machine translation andsummarization.Figure 1 shows an example TR tree, the nodes of58which represent the autosemantic words of the sen-tence.
Each node is labeled with a morphologicallyreduced word-form called the lemma and a functorthat describes the deep syntactic relationship to itsgovernor (function-argument form).
Additionally,the nodes are labeled with grammatemes that cap-ture morphological and semantic information asso-ciated with the autosemantic words.
For example,English verb forms are represented by the infinitiveform as the lemma and the grammatemes encodethe tense, aspect, and mood of the verb.
For a de-tailed description of the TR annotation scheme seeBo?hmova?
et al (2002).
In Figure 1 we show onlythose features that are present in the TR structuresused throughout this paper.Both the synsemantic nodes and the left-to-rightsurface order2 in the TR trees is under-specified.
Inthe context of machine translation, we assume theTR word order carries no information with the ex-ception of a single situation: the order of coordi-nated phrases is preserved in one of our models.2.1 Analytic RepresentationWhile it is not part of the formal TR description, theauthors of the TR annotation scheme have found ituseful to define an intermediate representation be-tween the sentence and the TR tree (Bo?hmova?
etal., 2002).
The analytical representation (AR) is asurface-syntactic dependency tree that encodes syn-tactic relationships between words (i.e., object, sub-ject, attribute, etc.).
Unlike the TR layer, the analyti-cal layer contains all words of the sentence and theirrelative ordering is identical to the surface order.2.2 Manually Annotated TRIn order to evaluate the efficacy of the generationmodel, we construct a dataset from both manuallyannotated data and automatically generated data.The information contained in the originally manu-ally annotated TR all but specifies the surface form.We have modified the annotated data by removingall features except those that could be directly trans-fered across languages.
Specifically, we preservethe following features: lemma, functor, verbal gram-2In a TR tree, a subtree is always between the nodes to theleft and right of its governor.
More specifically, all TR treesare projective.
For this reason, the relative ordering of subtreesimposes an absolute ordering for the tree.matemes, and part-of-speech tags.
The lemma isthe morphologically reduced form of the word; forverbs this is the infinitive form and for nouns this isthe singular form.
The functor is the deep-syntacticfunction of the node; for example, the deep functorindicates whether a node is a predicate, an actor, or apatient.
Modifiers can be labeled as locative, tempo-ral, benefactive, etc.
Additionally we include a ver-bal grammateme which encodes tense and mood aswell as a Penn Treebank style part-of-speech tag.3 Generative ProcessIn this section we describe the generative processthat inserts the synsemantic auxiliary words, re-orders the trees, and produces a sentence.
Our eval-uation will be on English data, so we describe themodels and the model features in the context of En-glish.
While the model is language independent, thespecific features and the size of the necessary condi-tioning contexts is a function of the language.Given a TR tree T , we wish to predict the cor-rect auxiliary nodes A and an ordering of the wordsassociated with {T ?
A}, defined by the functionf({T ?
A}).
The functions f determine the surfaceword order of the words associated with nodes of theauxiliary-inserted TR tree: N = {T ?A}.
The nodefeatures that we use from the nodes in the TR andAR trees are: the word lemma, the part-of-speech(POS) tag, and the functor.3 The objective of ourmodel is:argmaxA,fP (A, f |T )= argmaxA,fP (f |A,T )P (A|T ) (1)?
argmaxfP (f |T, argmaxAP (A|T )) (2)In Equation 2 we approximate the full model with agreedy procedure.
First, we predict the most likelyA according to the model P (A|T ).
Given A, wecompute the best ordering of the nodes of the tree,including those introduced in A.There is an efficient dynamic-programming solu-tion to the objective function in Equation 1; how-3The type of functor used (deep syntactic or surface-syntactic) depends on the tree to which we are applying themodel.
One form of the reordering model operates on AR treesand therefore uses surface syntactic functors.
The other modelis based on TR trees and uses deep-syntactic functors.59ever, in this work we experiment with the greedyapproximation.3.1 Insertion ModelThe specific English auxiliary nodes which are notpresent in TR include articles, prepositions, subor-dinating conjunctions, and modal verbs.4 For eachnode in the TR tree, the generative process predictswhich synsemantic word, if any, should be insertedas a dependent of the current node.
We make theassumption that these decisions are determined in-dependently.Let T = {w1, .
.
.
, wi, .
.
.
, wk} be the nodes ofthe TR tree.
For each node wi, we define the asso-ciated node aito be the auxiliary node that shouldbe inserted as a dependent of wi.
Given a tree T ,we wish to find the set of auxiliary nodes A ={a1, .
.
.
, ak} that should be inserted5:P (A|T )=?iP (ai|a1, .
.
.
, ai?1, T ) (3)?
?iP (ai|T ) (4)?
?iP (ai|wi, wg(i)) (5)Equation 3 is simply a factorization of the origi-nal model, Equation 4 shows the independence as-sumption, and in Equation 5 we make an additionalconditional independence assumption that in orderto predict auxiliary ai, we need only know the asso-ciated node wiand its governor wg(i).6We further divide the model into three compo-nents: one that models articles, such as the En-glish articles the and a; one that models preposi-tions and subordinating conjunctions; and one thatmodels modal verbs.
The first two models are of theform described by Equation 5.
The modal verb in-sertion model is a deterministic mapping based on4The function of synsemantic nodes are encoded by func-tors.
For example, the prepositions to, at, in, by, and on may beused to indicate time or location.
An autosemantic modifier willbe labeled as temporal or locative, but the particular prepositionis not specified.5Note that we include the auxiliary node labeled NOAUX tobe inserted, which in fact means a node is not inserted.6In the case of nodes whose governor is a coordinating con-junction, the governor information comes from the governor ofthe coordination node.grammatemes expressing the verb modality of themain verb.
Additionally, each model is independentof the other and therefore up to two insertions perTR node are possible (an article and another syntac-tic modifier).
In a variant of our model, we performa small set of deterministic transformations in caseswhere the classifier is relatively uncertain about thepredicted insertion node (i.e., the entropy of the con-ditional distribution is high).We note here that unlike the Amalgam system(Corston-Oliver et al, 2002), we do not address fea-tures which are determined (or almost completelydetermined) by the underlying deep-structure.
Forexample, the task of inserting prepositions is non-trivial given we only know a node?s functor (e.g.,the node?s valency role).3.2 Analytical Representation Tree GenerationWe have experimented with two paradigms for syn-thesizing sentences from TR trees.
The first tech-nique involves first generating AR trees (surfacesyntax).
In this model, we predict the node inser-tions, transform the functors from TR to AR func-tions (deep valency relationship to surface-syntacticrelationships), and then reorder the nodes.
In thesecond framework, we reorder the nodes directly inthe TR trees with inserted auxiliary nodes.3.3 Surface-order ModelThe node ordering model is used to determine a pro-jection of the tree to a string.
We assume the order-ing of the nodes in the input TR trees is arbitrary,the reordering model proposed here is based only onthe dependency structure and the node?s attributes(words, POS tags, etc.).
In a variant of the reorder-ing model, we assume the deep order of coordinatingconjunctions to be the surface order.Algorithm 1 presents the bottom-up node reorder-ing algorithm.
In the first part of the algorithm, wedetermine the relative ordering of child nodes.
Wemaximize the likelihood of a particular order via theprecedence operator ?.
If node ci?
ci+1, thenthe subtree of the word associated with ciimme-diately precedes the subtree of the word associatedwith ci+1in the projected sentence.In the second half of the algorithm (starting atline 13), we predict the position of the governorwithin the previously ordered child nodes.
Recall60Algorithm 1 Subtree Reordering Algorithmprocedure REORDER(T,A, O)  Result in ON ?bottomUp(T ?
A); O ?
{}for g ?
N dobestScore ?
0; og?
{}5: for C ?permutation of g?s children dofor i ?
1 .
.
.
|C| dos ?
s ?
P (ci?
ci+1|ci, ci+1, g)end forif s > bestScore then10: bestScore ?
s; og?
Cend ifend forbestScore ?
0; m ?
0for i ?
1 .
.
.
|bestOrder| do15: s ?
P (ci?
g ?
ci+1|ci, ci+1, g)if s > bestScore thens ?
bestScore ; m ?
iend ifend for20: Insert governor cgafter mth child in ogO ?
O ?
ogend forend procedurethat this is a dependency structure; knowing the gov-ernor does not tell us where it lies on the surfacewith respect to its children.
The model is similarto the general reordering model, except we consideran absolute ordering of three nodes (left child, gov-ernor, right child).
Finally, we can reconstruct thetotal ordering from the subtree ordering defined inO = {o1, .
.
.
, on}.The procedure described here is greedy; first wechoose the best child ordering and then we choosethe location of the governor.
We do this to minimizethe computational complexity of the algorithm.
Thecurrent algorithm?s runtime complexity isO(n!
), butthe complexity of the alternative algorithm for whichwe consider triples of child nodes is O(n!(n?
1)!
).The actual complexity is determined by the maxi-mum number of child nodes k = |C| and is O(nkk!
).3.4 Morphological GenerationIn order to produce true English sentences, we con-vert the lemma and POS tag to a word form.
Weuse John Carroll?s morphg tool7 to generate Englishword forms given lemma/POS tag pairs.
This isnot perfect, but it performs an adequate job at re-covering English inflected forms.
In the complete-system evaluation, we report scores based on gener-7Available on the web at:http://www.informatics.susx.ac.uk/research/nlp/carroll/morph.html.ated morphological forms.3.5 Insertion FeaturesFeatures for the insertion model come from the cur-rent node being examined and the node?s governor.When the governor is a coordinating conjunction,we use features from the governor of the conjunc-tion node.
The features used are the lemma, POStag, and functor for the current node, and the lemma,POS tag, and functor of the governor.
?iP (ai|wi, wg) (6)=?iP (ai|li, ti, fi, lg, tg, fg)The left-hand side of Equation 6 is repeated fromEquation 5 above.
Equation 6 shows the expandedmodel for auxiliary insertion where liis the lemma ,tiis the POS tag, and fiis the functor of node wi3.6 Reordering FeaturesOur reordering model for English is based primar-ily on non-lexical features.
We use the POS tagand functor from each node as features.
The twodistributions in our reordering model (used in Algo-rithm 1) are:P (ci?
ci+1|ci, ci+1, g) (7)= (ci?
ci+1|fi, ti, fi+1, ti+1, fg, tg)P (ci?
g ?
ci+1|ci, ci+1, g) (8)= P (ci?
g ?
ci+1|fi, ti, fi+1, ti+1, tg, fg)In both Equation 7 and Equation 8, only the func-tor and POS tag of each node is used.4 Empirical EvaluationWe have experimented with the above models onboth manually annotated TR trees and synthetictrees (i.e., automatically generated trees).
The datacomes from the PCEDT 1.0 corpus8, a version of thePenn WSJ Treebank that has been been translated toCzech and automatically transformed to TR in bothEnglish and Czech.
The English TR was automat-ically generated from the Penn Treebank?s manu-ally annotated surface syntax trees (English phrase-structure trees).
Additionally, a small set of 497 sen-tences were manually annotated at the TR level: 2488LDC catalog number: LDC2004T25.61Model Manual Data Synthetic DataIns.
Rules No Rules Ins.
Rules No RulesModel Articles Prep & SC Articles Prep & SC Articles Prep & SC Articles Prep & SCBaseline N/A N/A 77.93 76.78 N/A N/A 78.00 78.40w/o g. functor 87.29 89.65 86.25 89.31 88.07 91.83 87.34 91.06w/o g. lemma 86.77 89.48 85.68 89.02 87.53 90.95 86.55 91.16w/o g. POS 87.29 89.45 86.10 89.14 87.68 91.86 86.89 92.07w/o functor 86.10 85.02 84.86 84.56 86.01 85.60 84.79 85.65w/o lemma 81.34 89.02 80.88 88.91 81.28 91.03 81.42 91.33w/o POS 84.81 88.01 84.01 87.29 85.53 91.08 84.69 90.98All Features 87.49 89.68 86.45 89.28 87.87 91.83 87.24 92.02Table 1: Classification accuracy for insertion models on development data from PCEDT 1.0.
Article accuracy is computed overthe set of nouns.
Preposition and subordinating conjunction accuracy (P & SC) is computed over the set of nodes that appear onthe surface (excluding hidden nodes in the TR ?
these will not exist in automatically generated data).
Models are shown for allfeatures minus the specified feature.
Features with the prefix ?g.?
indicate governor features, otherwise the features are from thenode?s attributes.
The Baseline model is one which never inserts any nodes (i.e., the model which inserts the most probable value ?NOAUX).for development and 249 for evaluation; results arepresented for these two datasets.All models were trained on the PCEDT 1.0 dataset, approximately 49,000 sentences, of which 4,200were randomly selected as held-out training data, theremainder was used for training.
We estimate themodel distributions with a smoothed maximum like-lihood estimator, using Jelinek-Mercer EM smooth-ing (i.e., linearly interpolated backoff distributions).Lower order distributions used for smoothing are es-timated by deleting the rightmost conditioning vari-able (as presented in the above models).Similar experiments were performed at the 2002Johns Hopkins summer workshop.
The results re-ported here are substantially better than those re-ported in the workshop report (Hajic?
et al, 2002);however, the details of the workshop experimentsare not clear enough to ensure the experimental con-ditions are identical.4.1 Insertion ResultsFor each of the two insertion models (the articlemodel and the preposition and subordinating con-junction model), there is a finite set of values forthe dependent variable ai.
For example, the articlesare the complete set of English articles as collectedfrom the Penn Treebank training data (these havemanual POS tag annotations).
We add a dummyvalue to this set which indicates no article shouldbe inserted.9 The preposition and auxiliary model9In the classifier evaluation we consider the article a and anto be equivalent.assumes the set of possible modifiers to be all thoseseen in the training data that were removed whenmodifying the manual TR trees.The classification accuracy is the percentage ofnodes for which we predicted the correct auxiliaryfrom the set of candidate nodes for the auxiliarytype.
Articles are only predicted and evaluated fornouns (determined by the POS tag).
Prepositionsand subordinating conjunctions are predicted andevaluated for all nodes that appear on the surface.We do not report results for the modal verb inser-tion as it is primarily determined by the features ofthe verb being modified (accuracy is approximately100%).
We have experimented with different fea-tures sets and found that the model described inEquation 6 performs best when all features are used.In a variant of the insertion model, when the clas-sifier prediction is of low certainty (probability lessthan .5) we defer to a small set of deterministic rules.For infinitives, we insert ?to?
; for origin nouns, weinsert ?from?, for actors we insert ?of?, and we at-tach ?by?
to actors of passive verbs.
In the articleinsertion model, we do not insert anything if thereis another determiner (e.g., ?none?
or ?any?)
or per-sonal pronoun; we insert ?the?
if the word appearedwithin the previous four sentences or if there is asuggestive adjective attached to the noun.10Table 1 shows that the classifiers perform betteron automatically generated data (Synthetic Data),but also perform well on the manually annotated10Any adjective that is always followed by the definite articlein the training data.62Model Manual Data Synthetic DataCoord.
Rules No Rules Coord.
Rules No RulesAll Interior All Interior All Interior All InteriorBaseline N/A N/A 68.43 21.67 N/A N/A 69.00 21.42w/o g. functor 94.51 86.44 92.42 81.27 94.90 87.25 93.37 83.42w/o g. tag 93.43 83.75 90.89 77.50 93.82 84.56 91.64 79.12w/o c. functors 91.38 78.70 89.71 74.57 91.91 79.79 90.41 76.04w/o c. tags 88.85 72.44 82.29 57.36 88.91 72.29 83.04 57.60All Features 94.43 86.24 92.01 80.26 95.21 88.04 93.37 83.42Table 2: Reordering accuracy for TR trees on development data from PCEDT 1.0.
We include performance on the interior nodes(excluding leaf nodes) for the Manual data to show a more detailed analysis of the performance.
?g.?
are the governor features and?c.?
are the child features.
The baseline model sorts subtrees of each node randomly.data.
Prediction of articles is primarily dependent onthe lemma and the tag of the node.
The lemma andtag of the governing node and the node?s functor isimportant to a lesser degree.
In predicting the prepo-sitions and subordinating conjunctions, the node?sfunctor is the most critical factor.% Errors Reference?Hypothesis41 the ?
NULL19 a/an ?
NULL16 NULL ?
the11 a/an ?
the11 the ?
a/an2 NULL ?
a/anTable 3: Article classifier errors on development data.Manual SyntheticDet.
P & SC Det.
P & SC85.53 89.18 85.31 91.54Table 4: Accuracy of best models on the evaluation data.Table 3 presents a confusion set from the best ar-ticle classifier on the development data.
Our modelis relatively conservative, incurring 60% of the errorby choosing to insert nothing when it should have in-serted an article.
The model requires more informedfeatures as we are currently being overly conserva-tive.In Table 4 we report the overall accuracy on evalu-ation data using the model that performed best on thedevelopment data.
The results are consistent withthe results for the development data; however, thearticle model performs slightly worse on the evalua-tion set.4.2 Reordering ResultsEvaluation of the final sentence ordering was basedon predicting the correct words in the correct po-sitions.
We use the reordering metric described inHajic?
et al (2002) which computes the percentageof nodes for which all children are correctly ordered(i.e., no credit for partially correct orderings).Table 2 shows the reordering accuracy for thefull model and variants where a particular featuretype is removed.
These results are for orderingthe correct auxiliary-inserted TR trees (using deep-syntactic functors and the correctly inserted auxil-iaries).
In the model variant that preserves the deeporder of coordinating conjunctions, we see a signif-icant increase in performance.
The child node tagsare critical for the reordering model, followed by thechild functors.4.3 Combined System ResultsModel Manual SyntheticTR w/ Rules .4614 .4777TR w/o Rules .4532 .4657AR .2337 .2451Table 5: BLEU scores for complete generation system for TRtrees (with and without rules applied) and the AR trees.In order to evaluate the combined system, we usedthe multiple-translation dataset in the PCEDT cor-pus.
This data contains four retranslations fromCzech to English of each of the original English sen-tences in the development and evaluation datasets.In Table 5 we report the BLEU scores on develop-ment data for our TR generation model (includingthe morphological generation module) and the ARgeneration model.
Results for the system that usesAR trees as an intermediate stage are very poor; thisis likely due to the noise introduced when generatingAR trees.
Additionally, the results for the TR modelwith the additional rules are consistent with the pre-63vious results; the rules provide only a marginal im-provement.
Finally, we have run the complete sys-tem on the evaluation data and achieved a BLEUscore of .4633 on the manual data and .4750 onthe synthetic data.
These can be interpreted as theupper-bound for Czech-English translation systemsbased on TR tree transduction.5 ConclusionWe have provided a model for sentence synthesisfrom Tectogrammatical Representation trees.
Weprovide a number of models based on relatively sim-ple, local features that can be extracted from impov-erished TR trees.
We believe that further improve-ments will be made by allowing for more flexibleuse of the features.
The current model uses sim-ple linear interpolation smoothing which limits thetypes of model features used (forcing an explicit fac-torization).
The advantage of simple models of thetype presented in this paper is that they are robustto errors in the TR trees ?
which are expected whenthe TR trees are generated automatically (e.g., in amachine translation system).AcknowledgmentsThis work was partially supported by U.S.NSF grants IIS?9982329 and OISE?0530118; bythe project of the Czech Ministry of Educa-tion #LC536; by the Information Society ProjectNo.
1ET201120505 of the Grant Agency of theAcademy of Sciences of the Czech Republic; andGrant No.
352/2006 of the Grant Agency of CharlesUniversity.ReferencesSrinivas Bangalore and Owen Rambow.
2000.
Exploiting aprobabilistic hierarchical model for generation.
In Proceed-ings of the 18th International Conference on ComputationalLinguistics (COLING 2000), Saarbru?cken, Germany.Alena Bo?hmova?, Jan Hajic?, Eva Hajic?ova?, and Barbora Vidova?Hladka?.
2002.
The prague dependency treebank: Three-level annotation scenario.
In Anne Abeille, editor, In Tree-banks: Building and Using Syntactically Annotated Cor-pora.
Dordrecht, Kluwer Academic Publishers, The Neter-lands.Joan Bresnan and Ronald M. Kaplan.
1982.
Lexical-functionalgrammar: A formal system for grammatical representation.In The Mental Representation of Grammatical Relations.MIT Press.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and MaxEnt discriminative reranking.
In Pro-ceedings of the 43rd Annual Meeting of the Association forComputational Linguistics.David Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In Proceedings of the 43rdAnnual Meeting of the Association for Computational Lin-guistics, pages 263?270, Ann Arbor, MI.Michael Collins.
2003.
Head-driven statistical models fornatural language processing.
Computational Linguistics,29(4):589?637.Simon Corston-Oliver, Michael Gamon, Eric Ringger, andRobert Moore.
2002.
An overview of Amalgam: Amachine-learned generation module.
In Proceedings ofthe International Natural Language Generation Conference,pages 33?40, New York, USA.Jan Hajic?, Martin C?mejrek, Bonnie Dorr, Yuan Ding, JasonEisner, Dan Gildea, Terry Koo, Kristen Parton, DragomirRadev, and Owen Rambow.
2002.
Natural language genera-tion in the context of machine translation.
Technical report,Center for Language and Speech Processing, Johns HopkinsUniversity, Balitmore.
Summer Workshop Final Report.Irene Langkilde and Kevin Knight.
1998.
The practical value ofn-grams in generation.
In Proceedings of the InternationalNatural Language Generation Workshop.Irene Langkilde-Geary.
2002.
An empirical verification of cov-erage and correctness for a general-purpose sentence gener-ator.
In Proceedings of the International Natural LanguageGeneration Conference.Carl Pollard and Ivan A.
Sag.
1994.
Head-Driven PhraseStructure Grammar.
University of Chicago Press.Chris Quirk, Arul Menezes, and Colin Cherry.
2005.
De-pendency treelet translation: Syntactically informed phrasalSMT.
In Proceedings of the 43rd Annual Meeting of theAssociation for Computational Linguistics (ACL?05), pages271?279, Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.Petr Sgall, Eva Hajic?ova?, and Jarmila Panevova?.
1986.
TheMeaning of the Sentence in Its Semantic and Pragmatic As-pects.
Kluwer Academic, Boston.Radu Soricut and Daniel Marcu.
2006.
Stochastic languagegeneration using WIDL?expressions and its application inmachine translation and summarization.
In Proceedings ofthe 44th Annual Meeting of the Association for Computa-tional Linguistics.Dekai Wu.
1997.
Stochastic inversion transduction grammarsand bilingual parsing of parallel corpora.
ComputationalLinguistics, 23(3):377?404.64
