Proceedings of the 7th Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 39?47,Atlanta, Georgia, 13 June 2013. c?2013 Association for Computational LinguisticsCombining Word Reordering Methods on different Linguistic AbstractionLevels for Statistical Machine TranslationTeresa Herrmann, Jan Niehues, Alex WaibelInstitute for AnthropomaticsKarlsruhe Institute of TechnologyKarlsruhe, Germany{teresa.herrmann,jan.niehues,alexander.waibel}@kit.eduAbstractWe describe a novel approach to combin-ing lexicalized, POS-based and syntactic tree-based word reordering in a phrase-based ma-chine translation system.
Our results showthat each of the presented reordering meth-ods leads to improved translation quality on itsown.
The strengths however can be combinedto achieve further improvements.
We presentexperiments on German-English and German-French translation.
We report improvementsof 0.7 BLEU points by adding tree-based andlexicalized reordering.
Up to 1.1 BLEU pointscan be gained by POS and tree-based reorder-ing over a baseline with lexicalized reorder-ing.
A human analysis, comparing subjec-tive translation quality as well as a detailed er-ror analysis show the impact of our presentedtree-based rules in terms of improved sentencequality and reduction of errors related to miss-ing verbs and verb positions.1 IntroductionOne of the main difficulties in statistical machinetranslation (SMT) is presented by the different wordorders between languages.
Most state-of-the-artphrase-based SMT systems handle it within phrasepairs or during decoding by allowing words to beswapped while translation hypotheses are generated.An additional reordering model might be included inthe log-linear model of translation.
However, thesemethods can cover reorderings only over a very lim-ited distance.
Recently, reordering as preprocessinghas drawn much attention.
The idea is to detach thereordering problem from the decoding process andto apply a reordering model prior to translation inorder to facilitate a monotone translation.Encouraged by the improvements that can beachieved with part-of-speech (POS) reordering rules(Niehues and Kolss, 2009; Rottmann and Vogel,2007), we apply such rules on a different linguis-tic level.
We abstract from the words in the sentenceand learn reordering rules based on syntactic con-stituents in the source language sentence.
Syntac-tic parse trees represent the sentence structure andshow the relations between constituents in the sen-tence.
Relying on syntactic constituents instead ofPOS tags should help to model the reordering taskmore reliably, since sentence constituents are movedas whole blocks of words, thus keeping the sentencestructure intact.In addition, we combine the POS-based and syn-tactic tree-based reordering models and also add alexicalized reordering model, which is used in manystate-of-the-art phrase-based SMT systems nowa-days.2 Related WorkThe problem of word reordering has been addressedby several approaches over the last years.In a phrase-based SMT system reordering canbe achieved during decoding by allowing swaps ofwords within a defined window.
Lexicalized re-ordering models (Koehn et al 2005; Tillmann,2004) include information about the orientation ofadjacent phrases that is learned during phrase extrac-tion.
This reordering method, which affects the scor-ing of translation hypotheses but does not generatenew reorderings, is used e.g.
in the open source ma-39chine translation system Moses (Koehn et al 2007).Syntax-based (Yamada and Knight, 2001) orsyntax-augmented (Zollmann and Venugopal, 2006)MT systems address the reordering problem by em-bedding syntactic analysis in the decoding process.Hierarchical MT systems (Chiang, 2005) constructa syntactic hierarchy during decoding, which is in-dependent of linguistic categories.To our best knowledge Xia and McCord (2004)were the first to model the word reordering problemas a preprocessing step.
They automatically learnreordering rules for English-French translation fromsource and target language dependency trees.
After-wards, many followed these footsteps.
Earlier ap-proaches craft reordering rules manually based onsyntactic or dependency parse trees or POS tags de-signed for particular languages (Collins et al 2005;Popovic?
and Ney, 2006; Habash, 2007; Wang et al2007).
Later there were more and more approachesusing data-driven methods.
Costa-jussa` and Fonol-losa (2006) frame the word reordering problem asa translation task and use word class informationto translate the original source sentence into a re-ordered source sentence that can be translated moreeasily.
A very popular approach is to automaticallylearn reordering rules based on POS tags or syn-tactic chunks (Popovic?
and Ney, 2006; Rottmannand Vogel, 2007; Zhang et al 2007; Crego andHabash, 2008).
Khalilov et al(2009) present re-ordering rules learned from source and target sidesyntax trees.
More recently, Genzel (2010) proposedto automatically learn reordering rules from IBM1alignments and source side dependency trees.
InDeNero and Uszkoreit (2011) no parser is needed,but the sentence structure used for learning the re-ordering model is induced automatically from a par-allel corpus.
Among these approaches most are ableto cover short-range reorderings and some store re-ordering variants in a word lattice leaving the selec-tion of the path to the decoder.
Long-range reorder-ings are addressed by manual rules (Collins et al2005) or using automatically learned rules (Niehuesand Kolss, 2009).Motivated by the POS-based reordering modelsin Niehues and Kolss (2009) and Rottmann and Vo-gel (2007), we present a reordering model based onthe syntactic structure of the source sentence.
Weintend to cover both short-range and long-range re-ordering more reliably by abstracting to constituentsextracted from syntactic parse trees instead of work-ing only with morphosyntactic information on theword level.
Furthermore, we combine POS-basedand tree-based models and additionally include alexicalized reordering model.
Altogether we applyword reordering on three different levels: lexical-ized reordering model on the word level, POS-basedreordering on the morphosyntactic level and syntaxtree-based reordering on the constituent level.
Incontrast to previous work we use original syntacticparse trees instead of binarized parse trees or depen-dency trees.
Furthermore, our goal is to address es-pecially long-range reorderings involving verb con-structions.3 MotivationWhen translating from German to English differentword order is the most prominent problem.
Espe-cially the verb needs to be shifted over long dis-tances in the sentence, since the position of the verbdiffers in German and English sentences.
The finiteverbs in the English language are generally locatedat the second position in the sentence.
In Germanthis is only the case in a main clause.
In Germansubordinate clauses the verb is at the final positionas shown in Example 1.Example 1:Source: ..., nachdem ich eine Weile im Inter-net gesucht habe.Gloss: ... after I a while in-the internetsearched have.POS Reord.
: ..., nachdem ich habe eine Weile imInternet gesucht.POS Transl.
: ... as I have for some time on theInternet.The example shows first the source sentence andan English gloss.
POS Reord presents the reorderedsource sentence as produced by POS rules.
Thisshould be the source sentence according to targetlanguage word order.
POS Transl shows the trans-lation of the reordered sequence.
We can see thatsome cases remain unresolved.
The POS rules suc-ceed in putting the auxiliary habe/have to the rightposition in the sentence.
But the participle, carry-ing the main meaning of the sentence, is not shiftedtogether with the auxiliary.
During translation it is40dropped from the sentence, rendering it unintelligi-ble.A reason why the POS rules do not shift bothparts of the verb might be that the rules operate onthe word level only and treat every POS tag inde-pendently of the others.
A reordering model basedon syntactic constituents can help with this.
Addi-tional information about the syntactic structure ofthe sentence allows to identify which words belongtogether and should not be separated, but shifted asa whole block.
Abstracting from the word level tothe constituent level also provides the advantage thateven though reorderings are performed over longsentence spans, the rules consist of less reorderingunits (constituents which themselves consist of con-stituents or words) and can be learned more reliably.4 Tree-based ReorderingIn order to encourage linguistically meaningful re-orderings we learn rules based on syntactic tree con-stituents.
While the POS-based rules are flat andperform the reordering on a sequence of words, thetree-based rules operate on subtrees in the parse treeas shown in Figure 1.VPVVPPNPPTNEG?VPNPVVPPPTNEGFigure 1: Example reordering rule based on subtreesA syntactic parse tree contains both the word-level categories, i.e.
parts-of-speech and higher or-der categories, i.e.
constituents.
In this way it pro-vides information about the building blocks of a sen-tence that belong together and should not be takenapart by reordering.
Consequently, the tree-basedreordering operates both on the word level and onthe constituent level to make use of all available in-formation in the parse tree.
It is able to handle long-range reorderings as well as short-range reorder-ings, depending on how many words the reorderedconstituents cover.
The tree-based reordering rulesshould also be more stable and introduce less ran-dom word shuffling than the POS-based rules.The reordering model consists of two stages.
Firstthe rule extraction, where the rules are learned bysearching the training corpus for crossing align-ments which indicate a reordering between sourceand target language.
The second is the applicationof the learned reordering rules to the input text priorto translation.4.1 Rule ExtractionAs shown in Figure 4 we learn rules like this:VP PTNEG NP VVPP?
VP PTNEG VVPP NPwhere the first item in the rule is the head node ofthe subtree and the rest represent the children.
Inthe second part of the rule the children are indexedso that children of the same category cannot be con-fused.
Figure 2 shows an example for rule extrac-tion: a sentence in its syntactic parse tree representa-tion, the sentence in the target language and an auto-matically generated alignment.
A reordering occursbetween the constituents VVPP and NP.S1-nCS...VP2-5VVPP3-3gewa?hltNP4-5NN5-5SzenarienADJA4-4ku?nstlichePTNEG2-2nichtVAFIN2-2habenPPER1-1Wir1We2didn?t3choose4artificial5scenariosFigure 2: Example training sentence used to extract re-ordering rulesIn a first step the reordering rule has to be found.We extract the rules from a word aligned corpuswhere a syntactic parse tree is provided for eachsource side sentence.
We traverse the tree top downand scan each subtree for reorderings, i.e.
cross-ings of alignment links between source and targetsentence.
If there is a reordering, we extract arule that rearranges the source side constituents ac-cording to the order of the corresponding words on41the target side.
Each constituent in a subtree com-prises one or more words.
We determine the lowest(min) and highest (max) alignment point for eachconstituent ck and thus determine the range of theconstituent on the target side.
This can be formal-ized as min(ck) = min{j|fi ?
ck; ai = j} andmax(ck) = max{j|fi ?
ck; ai = j}.
To illustratethe process, we have annotated the parse tree in Fig-ure 2 with the alignment points (min-max) for eachconstituent.After defining the range, we check for the follow-ing conditions in order to determine whether to ex-tract a reordering rule.1.
all constituents have a non-empty range2.
source and target word order differFirst, for each subtree at least one word in each con-stituent needs to be aligned.
Otherwise it is not pos-sible to determine a conclusive order.
Second, wecheck whether there is actually a reordering, i.e.
thetarget language words are not in the same order asthe constituents in the source language: min(ck) >min(ck+1) and max(ck) > max(ck+1).Once we find a reordering rule to extract, we cal-culate the probability of this rule as the relative fre-quency with which such a reordering occurred in allsubtrees of the training corpus divided by the num-ber of total occurrences of this subtree in the corpus.We only store rules for reorderings that occur morethan 5 times in the corpus.4.1.1 Partial RulesThe syntactic parse trees of German sentences arequite flat, i.e.
a subtree usually has many children.When a rule is extracted, it always consists of thehead of the subtree and all its children.
The ap-plication requires that the applicable rule matchesthe complete subtree: the head and all its children.However, most of the time only some of the chil-dren are actually involved in a reordering.
Thereare also many different subtree variants that are quitesimilar.
In verb phrases or noun phrases, for exam-ple, modifiers such as prepositional phrases or ad-verbial phrases can be added nearly arbitrarily.
Inorder to generalize the tree-based reordering rules,we extend the rule extraction.
We do not only extractthe rules from the complete child sequence, but alsofrom any continuous child sequence in a constituent.This way, we extract generalized rules which canbe applied more often.
Formally, for each subtreeh ?
cn1 = c1c2...cn that matches the constraintspresented in Section 4.1, we modify the basic ruleextraction to: ?i, j1 ?
i < j ?
n : h ?
cji .
Itcould be argued that the partial rules might be notas reliable as the specific rules.
In Section 6 we willshow that such generalizations are meaningful andcan have a positive effect on the translation quality.4.2 Rule ApplicationDuring the training of the system all reordering rulesare extracted from the parallel corpus.
Prior to trans-lation the rules are applied to the original source text.Each rule is applied independently producing a re-ordering variant of that sentence.
The original sen-tence and all reordering variants are stored in a wordlattice which is later used as input to the decoder.The rules may be applied recursively to already re-ordered paths.
If more than one rule can be applied,all paths are added to the lattice unless the rules gen-erate the same output.
In this case only the rule withthe highest probability is applied.The edges in a word lattice for one sentence areassigned transition probabilities as follows.
In themonotone path with original word order all transi-tion probabilities are initially set to 1.
In a reorderedpath the first branching transition is assigned theprobability of the rule that generated the path.
Allother transition probabilities in this path are set to 1.Whenever a reordered path branches from the mono-tone path, the probability of the branching edge issubstracted from the probability of the monotoneedge.
However, a minimum probability of 0.05 isreserved for the monotone edge.
The score of thecomplete path is computed as the product of the tran-sition probabilities.
During decoding the best pathis searched for by including the score for the cur-rent path weighted by the weight for the reorderingmodel in the log-linear model.
In order to enableefficient decoding we limit the lattice size by onlyapplying rules with a probability higher than a pre-defined threshold.4.2.1 Recursive Rule ApplicationAs mentioned above, the tree-based rules may beapplied recursively.
That means, after one rule isapplied to the source sentence, a reordered path may42SSaus anderen Bundesla?ndernPPVAFINhabeVPVVPPbekommenviele AnfragenNPADVschonPPERichKOUSdassIch kann Ihnen nur sagen,...I may just tell you that I got already lots of requests from other federal statesFigure 3: Example parse tree with separated verb particlesbe reordered again.
The reason is the structure ofthe syntactic parse trees.
Verbs and their particlesare typically not located within the same subtree.Hence, they cannot be covered by one reorderingrule.
A separate rule is extracted for each subtree.Figure 3 demonstrates this in an example.
The twoparts that belong to the verb in this German sentence,namely bekommen and habe, are not located withinthe same constituent.
The finite verb habe forms aconstituent of its own and the participle bekommenforms part of the VP constituent.
In English the fi-nite verb and the participle need to be placed next toeach other.
In order to rearrange the source languagewords according to the target language word order,the following two reordering movements need to beperformed: the finite verb habe needs to be placedbefore the VP constituent and the participle bekom-men needs to be moved within the VP constituent tothe first position.
Only if both movements are per-formed, the right word order can be generated.However, the reordering model only considersone subtree at a time when extracting reorderingrules.
In this case two rules are learned, but if theyare applied to the source sentence separately, theywill end up in separate paths in the word lattice.
Thedecoder then has to choose which path to translate:the one where the finite verb is placed before the VPconstituent or the path where the participle is at thefirst position in the VP constituent.To counter this drawback the rules may be appliedrecursively to the new paths created by our reorder-ing rules.
We use the same rules, but newly createdpaths are fed back into the queue of sentences to bereordered.
However, we only apply the rules to partsof the reordered sentence that are still in the originalword order and restrict the recursion depth.5 Combining reordering methodsIn order to get a deeper insight into their individ-ual strengths we compare the reordering methods ondifferent linguistic levels and also combine them toinvestigate whether gains can be increased.
We ad-dress the word level using the lexicalized reordering,the morphosyntactic level by POS-based reorderingand the constituent level by tree-based reordering.5.1 POS-based and tree-based rulesThe training of the POS-based reordering is per-formed as described in (Rottmann and Vogel,2007) for short-range reordering rules, such asVVIMP VMFIN PPER ?
PPER VMFIN VVIMP.Long-range reordering rules trained accordingto (Niehues and Kolss, 2009) include gaps match-ing longer spans of arbitrary POS sequences(VAFIN * VVPP ?
VAFIN VVPP *).
The POS-based reordering used in our experiments always in-cludes both short and long-range rules.The tree-based rules are trained separately as de-scribed above.
First the POS-based rules are appliedto the monotone path of the source sentence and then43the tree-based rules are applied independently, pro-ducing separate paths.5.2 Rule-based and lexicalized reorderingAs described in Section 4.2 we create word latticesthat encode the reordering variants.
The lexical-ized reordering model stores for each phrase pairthe probabilities for possible reordering orientationsat the incoming and outgoing phrase boundaries:monotone, swap and discontinuous.
In order to ap-ply the lexicalized reordering model on lattices theoriginal position of each word is stored in the lat-tice.
While the translation hypothesis is generated,the reordering orientation with respect to the origi-nal position of the words is checked at each phraseboundary.
The probability for the respective orien-tation is included as an additional score.6 ResultsThe tree-based models are applied for German-English and German-French translation.
Results aremeasured in case-sensitive BLEU (Papineni et al2002).6.1 General System DescriptionFirst we describe the general system architecturewhich underlies all the systems used later on.
Weuse a phrase-based decoder (Vogel, 2003) that takesword lattices as input.
Optimization is performedusing MERT with respect to BLEU.
All POS-basedor tree-based systems apply monotone translationonly.
Baseline systems without reordering rules usea distance-based reordering model.
In addition, alexicalized reordering model as described in (Koehnet al 2005) is applied where indicated.
POS tagsand parse trees are generated using the Tree Tag-ger (Schmid, 1994) and the Stanford Parser (Raf-ferty and Manning, 2008).6.1.1 DataThe German-English system is trained on the pro-vided data of the WMT 2012. news-test2010 andnews-test2011 are used for development and test-ing.
The type of data used for training, developmentand testing the German-French system is similar toWMT data, except that 2 references are available.The training corpus for the reordering models con-sist of the word-aligned Europarl and News Com-mentary corpora where POS tags and parse trees aregenerated for the source side.6.2 German-EnglishWe built systems using POS-based and tree-basedreordering and show the impact of the individualmodels as well as their combination on the transla-tion quality.
The results are presented in Table 1.For each system, two different setups were evalu-ated.
First, with a distance-based reordering modelonly (noLexRM) and with an additional lexicalizedreordering model (LexRM).
The baseline systemwhich uses no reordering rules at all allows a re-ordering window of 5 in the decoder for both setups.For all systems where reordering rules are applied,monotone translation is performed.
Since the rulestake over the main reordering job, only monotonetranslation is necessary from the reordered word lat-tice input.
In this experiment, we compare the tree-based rules with and without recursion, and the par-tial rules.Rule TypeSystem noLexRM LexRMDev Test Dev TestBaseline (no Rules) 22.82 21.06 23.54 21.61POS 24.33 21.98 24.42 22.15Tree 24.01 21.92 24.24 22.01Tree rec.
24.37 21.97 24.53 22.19Tree rec.+ par.
24.31 22.21 24.65 22.27POS + Tree 24.57 22.21 24.91 22.47POS + Tree rec.
24.61 22.39 24.81 22.45POS + Tree rec.+ par.
24.80 22.45 24.78 22.70Table 1: German-EnglishCompared to the baseline system using distance-based reordering only, 1.4 BLEU points can begained by applying combined POS and tree-basedreordering.
The tree rules including partial rules andrecursive application alone achieve already a bet-ter performance than the POS rules, but using themall in combination leads to an improvement of 0.4BLEU points over the POS-based reordering alone.When lexicalized reordering is added, the relativeimprovements are similar: 1.1 BLEU points com-pared to the Baseline and 0.55 BLEU points over thePOS-based reordering.
We can therefore argue thatthe individual rule types as well as the lexicalized re-ordering model seem to address complementary re-ordering issues and can be combined successfully to44obtain an even better translation quality.We applied only tree rules with a probability of0.1 and higher.
Partial rules require a threshold of0.4 to be applied, since they are less reliable.
In or-der to prevent the lattices from growing too large,the recursive rule application is restricted to a max-imum recursion depth of 3.
These values were setaccording to the results of preliminary experimentsinvestigating the impact of the rule probabilities onthe translation quality.
Normal rules and partial rulesare not mixed during recursive application.With the best system we performed a final exper-iment on the official testset of the WMT 2012 andachieved a score of 23.73 which is 0.4 BLEU pointsbetter than the best constrained submission.6.3 Translation ExamplesExample 2 shows how the translation of the sen-tence presented above is improved by adding thetree-based rules.
We can see that using tree con-stituents in the reordering model indeed addressesthe problem of verb particles and especially missingverb parts in German.Example 2:Src: ..., nachdem ich eine Weile im Internetgesucht habe.Gloss: ..., after I a while in-the Internet search-ed have.POS: ... as I have for some time on the Inter-net.+Tree: ... after I have looked for a while on theInternet.Example 3 shows another aspect of how the tree-based rules work.
With the help of the tree-based re-ordering rules, it is possible to relocate the separatedprefix of German verbs and find the correct transla-tion.
The verb vorschlagen consist of the main verb(MV) schlagen (here conjugated as schla?gt) and theprefix (PX) vor.
Depending on the verb form andsentence type, the prefix must be separated from themain verb and is located in a different part of thesentence.
The two parts of the verb can also haveindividual meanings.
Although the translation ofthe verb stem were correct if it were the full verb,not recognizing the separated prefix and ignoring itin translation, corrupts the meaning of the sentence.With the help of the tree-based rules, the dependencybetween the main verb and its prefix is resolved andthe correct translation can be chosen.6.4 German-FrenchThe same experiments were tested on German-French translation.
For this language pair, similarimprovements could be achieved by combining POSand tree-based reordering rules and applying a lexi-calized reordering model in addition.
Table 2 showsthe results.
Up to 0.7 BLEU points could be gainedby adding tree rules and another 0.1 by lexicalizedreordering.Rule TypeSystem noLexRM LexRMDev Test Dev TestPOS 41.29 38.07 42.04 38.55POS + Tree 41.94 38.47 42.44 38.57POS + Tree rec.
42.35 38.66 42.80 38.71POS + Tree rec.+ par.
42.48 38.79 42.87 38.88Table 2: German-French6.5 Binarized Syntactic TreesEven though related work using syntactic parse treesin SMT for reordering purposes (Jiang et al 2010)have reported an advantage of binarized parse treesover standard parse trees, our model did not bene-fit from binarized parse trees.
It seems that the flathierarchical structure of standard parse trees enablesour reordering model to learn the order of the con-stituents most efficiently.7 Human Evaluation7.1 Sentence-based comparisonIn order to have an additional perspective of theimpact of our tree-based reordering, we also pro-vide a human evaluation of the translation outputof the German-English system without the lexical-ized reordering model.
250 translation hypotheseswere selected to be annotated.
For each input sen-tence two translations generated by different sys-tems were presented, one applying POS-based re-ordering only and the other one applying both POS-based and tree-based reordering rules.
The hypothe-ses were anonymized and presented in random order.Table 3 shows the BLEU scores of the analyzedsystems and the manual judgement of comparative,subjective translation quality.
In 50.8% of the sen-45Example 3:Src: Die RPG Byty schla?gt ihnen in den Schreiben eine Mieterho?hung von ca.
15 bis 38 Prozent vor.Gloss: The RPG Byty proposes-MV them in the letters a rent increase of ca.
15 to 38 percent proposes-PXPOS: The RPG Byty beats them in the letter, a rental increase of around 15 to 38 percent.+Tree: The RPG Byty proposes them in the letters a rental increase of around 15 to 38 percent.System BLEU wins %POS Rules 21.98 58 23.2POS + Tree Rules rec.
par.
22.45 127 50.8Table 3: Human Evaluation of Translation qualitytences, the translation generated by the system us-ing tree-based rules was judged to be better, whereasin 23.2% of the cases the system without tree-basedrules was rated better.
For 26% of the sentences thetranslation quality was very similar.
Consequently,in 76.8% of the cases the tree-based system pro-duced a translation that is either better or of the samequality as the system using POS rules only.7.2 Analysis of verbsSince the verbs in German-to-English translationwere one of the issues that the tree-based reorder-ing model should address, a more detailed analysiswas performed on the first 165 sentences.
We espe-cially investigated the changes regarding the verbsbetween the translations stemming from the systemusing the POS-based reordering only and the one us-ing both the POS and the tree-based model.
We ex-amined three aspects of the verbs in the two trans-lations.
Each change introduced by the tree-basedreordering model was first classified either as an im-provement or a degradation of the translation qual-ity.
Secondly, it was assigned to one of the followingcategories: exist, position or form.
In case of an im-provement, exist means a verb existed in the trans-lation due to the tree-based model, which did notexist before.
A degradation in this category meansthat a verb was removed from the translation whenincluding the tree-based reordering model.
An im-provement or degradation in the category positionor form means that the verb position or verb formwas improved or degraded, respectively.Table 4 shows the results of this analysis.
In total,48 of the verb changes were identified as improve-ments, while only 16 were regarded as degradationsof translation quality.
Improvements mainly concernType all exist position formImprovements 48 22 21 5Degradations 16 2 11 3Table 4: Manual Analysis of verbsimproved verb position in the sentence and verbsthat could be translated with the help of the tree-based rules that were not there before.
Even thoughalso degradations were introduced by the tree-basedreordering model, the improvements outweigh them.8 ConclusionWe have presented a reordering method based onsyntactic tree constituents to model long-range re-orderings in SMT more reliably.
Furthermore, wecombined the reordering methods addressing dif-ferent linguistic abstraction levels.
Experimentson German-English and German-French translationshowed that the best translation quality could beachieved by combining POS-based and tree-basedrules.
Adding a lexicalized reordering model in-creased the translation quality even further.
In totalwe could reach up to 0.7 BLEU points of improve-ment by adding tree-based and lexicalized reorder-ing compared to only POS-based rules.
Up to 1.1BLEU points were gained over to a baseline systemusing a lexicalized reordering model.A human evaluation showed a preference of thePOS+Tree-based reordering method in most cases.A detailed analysis of the verbs in the transla-tion outputs revealed that the tree-based reorderingmodel indeed addresses verb constructions and im-proves the translation of German verbs.AcknowledgmentsThis work was partly achieved as part of the QuaeroProgramme, funded by OSEO, French State agencyfor innovation.
The research leading to these resultshas received funding from the European Union Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement n?
287658.46ReferencesDavid Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proceedings ofthe 43rd Annual Meeting on Association for Computa-tional Linguistics, ACL ?05, pages 263?270, Strouds-burg, PA, USA.Michael Collins, Philipp Koehn, and Ivona Kuc?erova?.2005.
Clause Restructuring for Statistical MachineTranslation.
In Proceedings of ACL 2005, Ann Arbor,Michigan.Marta R. Costa-jussa` and Jose?
A. R. Fonollosa.
2006.Statistical Machine Reordering.
In Conference onEmpirical Methods on Natural Language Processing(EMNLP 2006), Sydney, Australia.Josep M. Crego and Nizar Habash.
2008.
Using ShallowSyntax Information to Improve Word Alignment andReordering for SMT.
In ACL-HLT 2008, Columbus,Ohio, USA.John DeNero and Jakob Uszkoreit.
2011.
Inducing sen-tence structure from parallel corpora for reordering.
InProceedings of EMNLP 2011, pages 193?203, Edin-burgh, Scotland, UK.Dmitriy Genzel.
2010.
Automatically learning source-side reordering rules for large scale machine transla-tion.
In Proceedings of COLING 2010, Beijing, China.Nizar Habash.
2007.
Syntactic preprocessing for statis-tical machine translation.
Proceedings of the 11th MTSummit.Jie Jiang, Jinhua Du, and Andy Way.
2010.
Improvedphrase-based smt with syntactic reordering patternslearned from lattice scoring.
In Proceedings of AMTA2010, Denver, CO, USA.M.
Khalilov, J.A.R.
Fonollosa, and M. Dras.
2009.A new subtree-transfer approach to syntax-based re-ordering for statistical machine translation.
In Proc.of the 13th Annual Conference of the European As-sociation for Machine Translation (EAMT?09), pages198?204, Barcelona, Spain.Philipp Koehn, Amittai Axelrod, Alexandra BirchMayne, Chris Callison-Burch, Miles Osborne, andDavid Talbot.
2005.
Edinburgh system description forthe 2005 iwslt speech translation evaluation.
In Inter-national Workshop on Spoken Language Translation.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran, RichardZens, et al2007.
Moses: Open source toolkit forstatistical machine translation.
In Annual meeting-association for computational linguistics, volume 45,page 2.Jan Niehues and Muntsin Kolss.
2009.
A POS-BasedModel for Long-Range Reorderings in SMT.
InFourth Workshop on Statistical Machine Translation(WMT 2009), Athens, Greece.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
Bleu: a Method for AutomaticEvaluation of Machine Translation.
Technical ReportRC22176 (W0109-022), IBM Research Division, T. J.Watson Research Center.Maja Popovic?
and Hermann Ney.
2006.
POS-basedWord Reorderings for Statistical Machine Translation.In International Conference on Language Resourcesand Evaluation (LREC 2006), Genoa, Italy.Anna N. Rafferty and Christopher D. Manning.
2008.Parsing three German treebanks: lexicalized and un-lexicalized baselines.
In Proceedings of the Workshopon Parsing German, Columbus, Ohio.Kay Rottmann and Stephan Vogel.
2007.
Word Reorder-ing in Statistical Machine Translation with a POS-Based Distortion Model.
In TMI, Sko?vde, Sweden.Helmut Schmid.
1994.
Probabilistic Part-of-Speech Tag-ging Using Decision Trees.
In International Con-ference on New Methods in Language Processing,Manchester, UK.Christoph Tillmann.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004: Short Papers, pages 101?104.Stephan Vogel.
2003.
SMT Decoder Dissected: WordReordering.
In Int.
Conf.
on Natural Language Pro-cessing and Knowledge Engineering, Beijing, China.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 737?745.Fei Xia and Michael McCord.
2004.
Improving a sta-tistical mt system with automatically learned rewritepatterns.
In Proceedings of COLING 2004, Geneva,Switzerland.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedings ofthe 39th Annual Meeting on Association for Computa-tional Linguistics, ACL ?01, pages 523?530, Strouds-burg, PA, USA.Yuqi Zhang, Richard Zens, and Hermann Ney.
2007.Chunk-Level Reordering of Source Language Sen-tences with Automatically Learned Rules for Statis-tical Machine Translation.
In HLT-NAACL Work-shop on Syntax and Structure in Statistical Transla-tion, Rochester, NY, USA.Andreas Zollmann and Ashish Venugopal.
2006.
Syntaxaugmented machine translation via chart parsing.
InProceedings of the Workshop on Statistical MachineTranslation, StatMT ?06, pages 138?141, Stroudsburg,PA, USA.47
