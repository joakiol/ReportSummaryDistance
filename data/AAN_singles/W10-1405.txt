Proceedings of the NAACL HLT 2010 First Workshop on Statistical Parsing of Morphologically-Rich Languages, pages 40?48,Los Angeles, California, June 2010. c?2010 Association for Computational LinguisticsModeling Morphosyntactic Agreementin Constituency-Based Parsing of Modern HebrewReut Tsarfaty?and Khalil Sima?anInstitute for Logic, Language and ComputationUniversity of Amsterdam{r.tsarfaty,k.simaan}@uva.nlAbstractWe show that na?
?ve modeling of morphosyn-tactic agreement in a Constituency-Based(CB) statistical parsing model is worse thannone, whereas a linguistically adequate wayof modeling inflectional morphology in CBparsing leads to improved performance.
Inparticular, we show that an extension of theRelational-Realizational (RR) model that in-corporates agreement features is superior toCB models that treat morphosyntax as state-splits (SP), and that the RR model benefitsmore from inflectional features.
We focus onparsing Hebrew and report the best result todate, F184.13 for parsing off of gold-taggedtext, 5% error reduction from previous results.1 IntroductionAgreement is defined by linguists as the system-atic covariance of the grammatical properties of onelinguistic element to reflect the semantic or formalproperties of another (Corbett, 2001).
Morpholog-ically marked agreement features such as gender,number and person are used to realize grammat-ical relations between syntactic constituents, andsuch patterns are abundantly found in (less- or) non-configurational languages (Hale, 1983) where theorder of words is known to be (relatively) free.Agreement features encompass information con-cerning the functional relations between constituentsin the syntactic structure, but whether incorporat-ing agreement features in a statistical parsing modelleads to improved performance has so far remainedan open question and saw contradictory results.
?The first author is currently a researcher at the departmentof Linguistics and Philology at Uppsala University.Taking Semitic languages as an example, it wasshown that an SVM-based shallow parser (Gold-berg et al, 2006) does not benefit from includ-ing agreement features for NP chunking in Hebrew.Phrase-structure based parsers for Arabic system-atically discard morphological features from theirlabel-set and never parametrize agreement explic-itly (Maamouri et al, 2008).
Models based on deepgrammars such as CCG (Hockenmaier and Steed-man, 2003) and HPSG (Miyao and Tsujii, 2008)could in principle use inflectional morphology, butthey currently rely on functional information mainly.For formalisms that do incorporate morphology,generative models are may leak probability due tounification failures (Abney, 1997).
Even resultsfrom dependency parsing remain inconclusive.
Itwas shown for dependency parsing that case, defi-niteness and animacy features are useful to enhanceparsing (e.g., (?vrelid and Nivre, 2007)), agreementpatterns are often excluded.
When agreement fea-tures were included as features in dependency parserfor Hebrew in (Goldberg and Elhadad, 2009) for He-brew they obtained tiny-to-no improvement.A question thus emerges whether there are anybenefits in explicitly incorporating morphosyntacticagreement patterns into our models.
This question isa manifestation of a greater issue, namely, whetherit is beneficial to represent complex patterns of mor-phology in the statistical parsing model, or whetherconfigurational information subsume the relevantpatterns, as it is commonly assumed in constituency-based parsing.
Here we claim that agreement fea-tures are useful for statistical parsing provided thatthey are represented and parametrized in a way thatreflects their linguistic substance; to express func-tional information orthogonal to configuration.40We do so by extending the Relational-Realizational (RR) model we presented in (Tsarfatyand Sima?an, 2008) to explicitly encode agreementfeatures in its native representation (RR-AGR).
Inthe RR model, a joint distribution over grammaticalrelations is firstly articulated in the projection phase.The grammatical relations may be spelled out bypositioning them with respect to one another in theconfiguration phase, through the use of morphologyin the realization phase, or both.
This paper showsthat, for Hebrew, this RR-AGR strategy signifi-cantly outperforms a constituency-based model thattreats agreement features as internally structurednon-terminal state-splits (SP-AGR).
As we accumu-late morphological features, the performance gapbetween the RR and SP models becomes larger.The best result we report for the RR-AGR model,F184.13, is the best result reported for Hebrew todate for parsing gold PoS-tagged segments, with5% error reduction from previous results.
Thisresult is also significantly higher than all parsingresults reported so far for Arabic, a Semitic lan-guage with similar morphosyntactic phenomena.1The RR approach is shown to be an adequate wayto model complex morphosyntactic patterns for im-proving constituency-based parsing of a morpholog-ically rich, free word order language.
Because theRR model is also proper and generative, it may alsoembed as a language model to enhance more com-plex NLP tasks, e.g., statistical Machine Translation.2 The DataThe grammar of nonconfigurational languages al-lows for freedom in word ordering and discontinu-ities of syntactic constituents (Hale, 1983).
Suchlanguages do not rely on configurational informationsuch as position and adjacency in marking grammat-ical relations such as subject and object, but insteadthey use word-level morphology.
One way to encodegrammatical relations in the form of words is by us-ing morphological case, that is, explicitly markingan argument (e.g.
nominative, accusative) with re-spect to its grammatical function.
In (Tsarfaty etal., 2009) we showed that incorporating case indeedleads to improved performance for constituency-based, Relational-Realizational parsing of Hebrew.1In (Maamouri et al, 2008), F178.1 for gold standard input.A more involved way to morphologically encodegrammatical relations is by making explicit refer-ence to the properties of multiple linguistic ele-ments.
This is the general pattern of agreement, i.e.,?
[A] systematic covariance between a se-mantic or a formal property of one ele-ment and a formal property of another.?
(Steele, adapted from (Corbett, 2001))Describing agreement patterns involves explicitreference to the following four components; the el-ement which determines the agreement propertiesis the Controller of the agreement, the elementwhose properties are determined by agreement isthe Target, the syntactic environment in which theagreement occurs is the Domain of agreement, andthe properties with respect to which they agree areagreement Features (Corbett, 2001).
Agreement isan inherently asymmetrical relation.
Combinationof features displayed by controllers has to be ac-commodated by the inflectional features of the tar-get, but there is no opposite requirement.
Let us il-lustrate the formal description of agreement throughSubject-Verb agreement familiar from English (1).
(1) a. Subject-Verb Agreement in English:Controller: NPTarget: VDomain: SFeatures: number, personb.
Example:i.
They like the paperii.
*They likes the paperThe agreement target (the verb) in English has arich enough inflectional paradigm that reflects theperson and number features inherent in controllers?
the nouns that realize subjects.
(But nouns in En-glish need not reflect, say, tense.)
Had the subjectbeen an NP, e.g., the phrase ?the committee?, theagreement pattern would have had to be determinedby the features of the entire NP, and in English thefeatures of the phrase would be determined by thelexical head ?committee?.
The controller of theagreement (noun) does not coincide with the head ofthe lexical dependency (the verb), which means thatthe direction of morphological dependencies neednot coincide with that of lexical dependencies.41The Semitic LanguageModernHebrew ModernHebrew, (henceforth, Hebrew) is a Semitic languagewith a flexible word order and rich morphologicalstructure.
Hebrew nouns morphologically reflecttheir inherent gender and number.
Pronouns alsoreflect person features.
Hebrew verbs are inflectedto reflect gender, number, person and tense.
Adjec-tives are inflected to reflect the inherent properties ofnouns, and both nouns and adjectives are inflectedfor definiteness.
The Hebrew grammar uses this ar-senal of properties to implement a wide variety ofagreement patterns realizing grammatical relations.Agreement in Hebrew S Domains Hebrew man-ifests different patterns of agreement in its S do-main.
Verbal predicates (the target) in matrix sen-tences (the domain) agree with their nominal sub-jects (the controller) on the agreement features gen-der, number and person.
This occurs regardless oftheir configurational positions, as illustrated in (2b).
(2) a.
Agreement in Verbal Sentences:Controller: NPTarget: VDomain: SFeatures: number, person, genderb.
i.
?????
????
???
??
?daniDani.3MSnatangave.3MSmatanapresentledinato-DinaDani gave a present to Dina (SVO)ii.
?????
???
???
???
?matanapresentnatangave.3MSdaniDani.3MSledinato-DinaDani gave a present to Dina (VI)Subject-Predicate agreement relations are notonly independent of surface positions, but are alsoorthogonal to the syntactic distributional type ofthe constituent realizing the predicate.
Semitic lan-guages allow for predicates to be realized as anNP, an ADJP or a PP clause (3b) lacking a verbaltogether.
(In the Hebrew treebank, such pred-icates are marked as PREDP).
In all such cases,agreement feature-bundles realized as pronominals,which (Doron, 1986) calls Pron, are optionallyplaced after the subject.
The position of Pron el-ement with respect to the subject and predicate isfixed.2 The role of these Pron elements is to indicatethe argument-structure of a nominal sentence that isnot projected by a verb.
In the Hebrew treebank theyare subsumed under predicative phrases (PREDPs).If a PREDP head is of type NP or ADJP it must beinflected to reflect the features of the subject con-troller, as is illustrated in examples (3b-i)?(3b-ii).
(3) a.
Agreement in Nominal Sentences:Controller: NPTarget: PronDomain: SFeatures: number, gender,personb.
i.
?????
(???)
???
?dinaDina.FS(hi)(Pron.3FS)cayeretpainter.FSDina is a painterii.
??????
(???)
???
?DinaDina.FS(hi)(Pron.3FS)muchsherettalented.FSDina is talentediii.
????
(???)
???
?DinaDina.FS(hi)(Pron.3FS)babayitin-the-houseDina is at homec.
i.
?????
????
*(???)
(hi)* dina cayeret(Pron.3FS)* Dina.FS painter.FSThe pronominal features gender, number, personare also a part the inflectional paradigm of the verb???
(be), which is extended to include tense features.These inflected elements are used as AUX whichfunction as co-heads together with the main (nom-inal or verbal) predicate.
AUX elements that take anominal predicate as in (4b) agree with their subject,and so do auxiliaries that take a verbal complement,e.g., the modal verb in (4c).
The nominal predicatein (4b) also agrees with the subject ?
and so doesthe modal verb in (4c).
Agreement of AUX with the2Doron (1986) shows that these Pron elements can not beconsidered the present tense supplements of AUX elements inHebrew since their position with respect to the subject and pred-icate is fixed, whereas AUX can change position, see (4) below.42verbal or nominal predicates is again independent oftheir surface positions.
(4) a. Subject-AUX Agreement in Hebrew:Controller: NPTarget: AUXDomain: SFeatures: number, person, genderb.
i.
?????
???
????
??
?hishe.3FShaytawas.3FSbe?avarin-pastcayeretpainter.FSShe was a painter in the pastii.
?????
???
????
??
?be?avarin-pasthaytawas.3FShishe.3FScayeretpainter.FSShe was a painter in the past?c.
i.
?
?? ?????
????
??
?hiShe.3FShaytawas.3FSamurasupposed.FSlehagi?ato-arriveShe was supposed to arriveii.
?
?? ????
?????
??
?hiShe.3FSamurasupposed.FShaytawas.3FSlehagi?ato-arriveShe was supposed to arriveAgreement in Construct State Nouns Semiticlanguages allow for the creation of noun compoundsby phonologically marking their lexical head andadding a genitive complement.
These constructionsare called Construct-State Nouns (CSN) (Danon,2008) and an example of a CSN is provided in (5a).3(5) a.
?????
?
?batchild.FS.CSNha-cayarDef-painter.MSThe painter?s daughter3Also known as iDaFa constructions in Arabic.In such cases, all the agreement features are takenfrom the head of the CSN, the noun ?daughter?
in (5).Since CSNs may be embedded in other CSNs, theconstructions may be arbitrarily long.
When shortor long, CSNs themselves may be modified by ad-jectives that agree with the CSN as a whole.
Thisgives rise to multiple patterns of agreement withina single complex CSN.
Consider, for instance, themodified CSN in (6a).
(6) a.
???????
?????
?
?batchild.FS.CSNha-cayarDef-painter.MSha-muchsheretDef-talented.FSThe talented daughter of the painterThe features Def, F, S of the adjective ?talented?agree with the inherent properties of the CSN head?child.FS?
and with the definiteness status of the em-bedded genitive Def-painter.
This phenomenon iscalled by Danon (2008) definiteness-spreading, andwhat is important about such spreading is to observethat it is not always the case that all agreement fea-tures of a phrase are contributed by its lexical head.4Interim Summary The challenges of model-ing agreement inside constituency-based statisticalmodels can be summarized as follows.
The modelsare required to assign probability mass to alternatingsequences of constituents while retaining equivalentfeature distributions that capture agreement.
Agree-ment is (i) orthogonal to the position of constituents(ii), orthogonal to their distributional types, and (iii)orthogonal to features?
distributions among domi-nated subconstituents.
Yet, from a functional pointof view their contribution is entirely systematic.3 The ModelsThe strong version of the well-known LexicalistHypothesis (LH) states that ?syntactic rules cannotmake reference to any aspect of word internal struc-ture?
(Chomsky, 1970).
Anderson (1982) arguesthat syntactic processes operating within configura-tional structures can often manipulate, or have ac-cess to, formal and inherent properties of individ-ual words.
Anderson (1982) argues that a model4Examples for non-overlapping contribution of features bymultiple dependencies can be found in (Guthmann et al, 2009).43that is well-equipped to capture such phenomena isone that retains a relaxed version of the LH, that is,one in which syntactic processes do not make refer-ence to aspects of word-internal structure other thanmorphologically marked inflectional features.
Whatkind of parsing model would allow us to implementthis relaxed version of the Lexicalist Hypothesis?The Morphosyntatctic State-Splits (SP) ModelOne way to maintain a relaxed version of the LHin syntax is to assume a constituency-based rep-resentation in which the morphological features ofwords are percolated to the level of constituencyin which they are syntactically relevant.
This ap-proach is characteristic of feature-based grammars(e.g., GPSG (Gazdar et al, 1985) and follow-upstudies).
These grammars assume a feature geom-etry that defines the internal structure of node labelsin phrase-structure trees.5Category-label state-splits can reflect the differentmorphosyntactic behavior of different non-terminalsof the same type.
Using such supervised, linguis-tically motivated, state-splits, based on the phrase-level marking of morphological information is onemay build an efficient implementation of a PCFG-based parsing model that takes into account mor-phological features.
State-split models were shownto obtain state-of-the-art performance with littlecomputational effort.
Supervised state-splits forconstituency-based unlexicalized parsing in (Kleinand Manning, 2003) in an accurate English parser.For the pair of Hebrew sentences (2b), the morpho-logical state-split context-free representation of thedomain S is as described at the top of figure 1.6The Relational-Realizational (RR) Model A dif-ferent way to implement a syntactic model that con-form to the relaxed LH is by separating the inflec-tional features of surface words from their grammat-ical functions in the syntactic representation and let-5While agreement patterns in feature-rich grammars giverise to re-entrancies that break context-freeness, GPSG showsthat using feature-percolation we can get quite far in modelingmorphosyntactic dependencies and retaining context-freeness.6Horizontal markovization a` la (Klein and Manning, 2003)would be self-defeating here.
Markovization of constituentsconditions inflectional features on configurational positions,which is inadequate for free word-order languages as Hebrew.This is already conjectured in the PhD thesis of Collins, and itis verified empirically for Hebrew in (Tsarfaty et al, 2009).ting the model learn systematic form-function corre-spondence patterns between them.The Relational-Realizational (RR) model (Tsar-faty and Sima?an, 2008) takes such a ?separational-ist?
approach which is constituent-based.
Grammat-ical relations are separated from their morphologi-cal or syntactic means of realization, which are inturn also distinguished.
The easiest way to describethe RR model is via a three-phase generative processencompassing the projection, configuration and re-alization phases.
In the projection phase, a clause-level syntactic category generates a Relational Net-work (RN), i.e., a set of grammatical function-labelsrepresenting the argument-structure of the clause.
Inthe configuration phase, linear ordering is generatedfor the function-labels and optional realization slotsare reserved for elements such as punctuation, auxil-iaries and adjuncts.
The realization phase spells outa rich morphosyntactic representation (MSR) ?
asyntactic label plus morphological features ?
real-izing each grammatical function and each of the re-served slots.
The process repeats as necessary untilMSRs of pre-terminals are mapped to lexical items.In (Tsarfaty et al, 2009) we have shown thatthe RR model makes beneficial use of morpholog-ical patterns involving case marking, but did notstudy the incorporation of inflectional agreementfeatures such as gender.
Since agreement featuressuch as gender, number and case-related informa-tion such accusativity, definiteness are determinedby non-overlapping subconstituents, it remains anopen question whether an addition of agreement fea-tures into the model can be down in a linguisticallyadequate and statistically sound way, and whether ornot they further improve performance.We claim that the Relational-Realizational modelof (Tsarfaty et al, 2009) has all the necessary ingre-dients to seamlessly migrate RR representations toones that encode agreement explicitely.
In order toexplain how we do so let us recapitulate the empir-ical facts.
Agreement is an asymmetric relation de-fined for a certain domain, in which the agreementproperties of a target co-vary with the inherent prop-erties of the controller.
Consider the two sentencesin (2b) in which the formal means to differentiate thesubject from the object is by the pattern of an agree-ing predicate.
The RR representations of the domainS are given at the bottom of figure 1.44The agreement targets and agreement controllersare easy to recognize; controllers are the syntac-tic constituents that realize subjects, parametrizedas Prealization(V B|PRD@S), and targets are theones that realize predicates, parametrized asPrealization(NP |SBJ@S).
Now, if we take thepredicted labels of controllers and targets to in-clude reference to inflectional features, we getthe following parameterization of the realizationparameters Prealization(V B?FEATSi?|PRD@S) andPrealization(NP ?FEATSj?|SBJ@S) with ?FEATSi?,?FEATSj?
the inflectional features indicated in theirmorphosyntactic representation.
Now, we only needto make sure that ?FEATSi?, ?FEATSj?
indeed agree,regardless of their position under S.We do so by explicitly marking the domainof agreement, the S category, with the featuresof the syntactically most prominent participant inthe situation, the subject (this is where the non-symmetrical nature of agreement comes into play).The realization distributions take the followingforms Prealization(V B?FEATSj?|PRD@S?FEATSi?
)and Prealization(NP ?FEATSi?|SBJ@S?FEATSi?).
Inthe former, NP ?FEATSi?
reflects the inherent fea-tures of the SBJ and in the latter V B?FEATSj?
re-flects the agreement features of the PRD.
Now, re-gardless of word order, and regardless of the inter-nal structure of NPs, the parameters capturing agree-ment would be the same for examples (2b i-ii).
Theonly parameters that differ are the configuration pa-rameters (boxed), reflecting word-order alternation.For the sake of completeness we include here alsothe SP vs. RR representation of S domains involv-ing auxiliaries in figure 2.
Here the sentences varyonly in the position of the AUX element relative tothe subject with which it agrees.
Subjects, predi-cates, and slots that have been reserved for AUXelements, all reflect the same pattern of agreementthrough their conditioning on the rich representa-tion of the domain.7 More parameters that vary here(boxed) are AUX placement and realization param-eters.
Since Pron elements endow PREDPs withagreement features, agreement with verbless (nomi-nal) predicates under S analogously follows.7In Hebrew, even some adverbial modifiers reflect pat-terns of agreement, e.g., ?????
(literally, ?I am still?, glossed?still.1S?).
This solution caters for all such patterns in whichnon-obligatory elements exhibit agreement.4 ExperimentsWe aim to examine whether the explicit incorpora-tion of agreement features helps Hebrew parsing,and if so, which of the two modeling strategies isbetter for utilizing the disambiguation cues providedby morphosyntactic agreement.Data We use the Hebrew treebank v2.0 with theextended annotation of (Guthmann et al, 2009),which adds inflectional properties to non-terminalcategories such as NP and VP.
We head-annotatethe corpus and systematically add the agreement fea-tures of Domains throughout the treebank.
We fur-ther distinguish finite from non-finite verb forms,and cliticized from non-cliticized nouns, as in(Goldberg and Tsarfaty, 2008; Tsarfaty et al, 2009).On top of the treebank labels SBJ subject, OBJ ob-ject, COM complement and CNJ conjunction weadd PRD predicates and IC infinitival complements.Procedure We devised a procedure to read-offtreebank grammars based on (i) GPSG-like, states-plit context-free parameters (SP-AGR), and (ii) RR-AGR parameters in which context-free rules capturethe projection, configuration and realization phases.In each model the multiplication provides the prob-ability of the generation.
We use relative frequencyestimates and exhaustively parse gold pos-tagged in-put8 using a general-purpose CKY parser.
We usethe same data split as in (Goldberg and Tsarfaty,2008; Tsarfaty et al, 2009) (training on sentences501-6000 and parsing sentences 1-500) and we con-vert all trees to the flat, coarse-grained, original tree-bank representation for the purpose of evaluation.Setup We experiment with bare constituent labels,grand-parent decorated labels (gp), and labels deco-rated with grand-parent and head-tag labels (gp,hd).We use increasingly richer subsets of the {gender,definiteness, accusativity} set.98This choice to parse gold-tagged sentences is meant to alle-viate the differences in the model?s morphological disambigua-tion capacity.
We want to evaluate the contribution of morpho-logical features for syntactic disambiguation, and if the modelswill disambiguate the morphological analyses differently, thesyntactic analysis will be assigned to different yields and theaccuracy results would be strictly incomparable.
But see (Gold-berg and Tsarfaty, 2008) for a way to combine the two.9We deliberately choose features that have non-overlappingbehavior, to see whether their contribution is accumulative.45SMSNPMS-SBJdaniDaniVPMS-PRDnatangaveNP-OBJmatanapresentPP-COMledianto-dinaSMSNP-OBJmatanapresentVPMS-PRDnatangaveNPMS-SBJdaniDaniPP-COMledianto-dinaP(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@SMSVPMSnatangaveOBJ@SMSNP-OBJmatanapresentCOM@SMSPP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanapresentPRD@SMSVPMSnatangaveSBJ@SMSNPMSdaniDaniCOM@SMSPP-COMledianto-dinaPprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )Pconfiguration(?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).SFSNPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveSFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).SMSNPMS-SBJdaniDaniVPMS-PRDn tangaveNP-OBJm tanapresentP-COMledianto-dinaSMSNP-OBJm tanapresentVPMS-PRDn tangaveNPMS-SBJdaniDaniP-COMledianto-dinaP(NPMS-SBJ,VPMS-PRD,NP-OBJ, P-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ, P-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@SMSVPMSn tangaveOBJ@SMSNP-OBJm tanapresentCOM@SMSP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJm tanapresentPRD@SMSVPMSn tangaveSBJ@SMSNPMSdaniDaniCOM@SMSP-COMledianto-dinaPprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )Pconfiguration(?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization( P | COM@ SMS ) Prealization( P | COM@ SMS )Figure 1: The SP-AGR (top) and R-AGR representations of sentences (2b-i) (left) and (2b-ii).SFSNPF -SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveSFSNPF -SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, P, PREDPFS-PRD?
| SFS ) P(?
P, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)Figure 2: The SP-AGR (top) and R-AGR representation of sentences (4c-i) (left) and (4c-ii).SMSNPMS-SBJdaniDaniVPMS-PRDnatangaveNP-OBJmatanapresentPP-COMledianto-dinaSMSNP-OBJmatanapresentVPMS-PRDnatangaveNPMS-SBJdaniDaniPP-COMledianto-dinaP(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@SMSVPMSnatangaveOBJ@SMSNP-OBJmatanapresentCOM@SMSPP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanapresentPRD@SMSVPMSnatangaveSBJ@SMSNPMSdaniDaniCOM@SMSPP-COMledianto-dinaPprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )Pconfiguration(?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).SFSNPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveSFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).SMSNPMS-SBJdaniDaniVPMS-PRDnatangaveNP-OBJmatanapresentPP-COMledianto-dinaSMSNP-OBJmatanapresentVPMS-PRDnatangaveNPMS-SBJdaniDaniPP-COMledianto-dinaP(NPMS-SBJ,VPMS PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@SMSVPMSnatangaveOBJ@SMSNP-OBJmatanapresentCOM@SMSPP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanapresentPRD@SMSVPMSnatangaveSBJ@SMSNPMSdaniDaniCOM@SMSPP-COMledianto-dinaprojection({SBJ,PRD,OBJ,COM} | SMS ) projection({SBJ,PRD,OBJ,COM} | SMS )Pconfiguration(?S,P ,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR repres tations of s n ences (2b- (left) and (2b-ii).SFSNPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveSFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| SFS ) (?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@SFSSBJ@ FSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPmurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD S+FSPPmurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arriveprojection({SBJ,PRD,COM} | SFS ) projection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )Prealization(AUX | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)Figure 2: The SP-AGR (top) and RR-AGR representation of s n ences (4c- (left) and (4c-ii).SMSNPMS-SBJdaniDaniVPMS-PRDnatangaveNP-OBJmatanapresentPP-COMledianto-dinaSMSNP-OBJmatanapresentVPMS-PRDatangaveNPMS-SBJdaniDaniPP-COMledianto-dinaP(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@SMSVPMSnatangaveOBJ@SMSNP-OBJmatanapresentCOM@SMSPP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanapresentPRD@SMSVPMSatangaveSBJ@SMSNPMSdaniDaniCOM@SMSPP-COMledianto-dinaPprojection({SBJ,PRD,OBJ,COM} | S S ) Pprojection({SBJ,PRD,OBJ,COM} | S S )Pconfiguration(?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).SFSNPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveSFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | SBJ@SFS ) Prealization(NPFS | SBJ@SFS )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).S SNP S-SBJdaniDaniVP S-PRDnatangaveNP-OBJmatanapresentPP-COledianto-dinaS SNP-OBJmatanapresentVP S-PRDnatangaveNP S-SBJdaniDaniPP-COledianto-dinaP( P S-SBJ, P S-PR , P- BJ,PP-C | S S ) P( P- BJ, P S-PR , P S-SBJ,PP-C | S S )S S{SBJ,PRD,OBJ,CO } S SSBJ S SNP SdaniDaniPRD S SVP SnatangaveOBJ S SNP-OBJmatanapresentCO S SPP-COledianto-dinaS S{SBJ,PRD,OBJ,CO } S SOBJ S SNP-OBJmatanapresentPRD S SVP SnatangaveSBJ S SNP SdaniDaniCO S SPP-COledianto-dinaPprojection({SBJ,PR , BJ,C } | S S ) Pprojection({SBJ,PR , BJ,C } | S S )Pconfiguration(?S,P, ,C?
| {SBJ,PR , BJ,C } S S) Pconfiguration(?
,P,S,C?
| {SBJ,PR , BJ,C } S S)Prealization( P S | SBJ S S ) Prealization( P S | SBJ S S )Prealization( B S | PR S S ) Prealization( B S | PR S S )Prealization( P | BJ S S ) Prealization( P | BJ S S )Prealization(PP | C S S ) Prealization(PP | C S S )igure 1: The SP- (top) and - representations of sentences (2b-i) (left) and (2b-ii).FSNPFS-SBJhisheAUXFShaytawasDFS-PRDamurasupposedVPINF-COMlehagiato-arriveFSNPFS-SBJhisheDFS-PRDamurasupposedAUXFShaytwasVPINF-COMlehagiato-arriveP(?
PFS-SBJ, FS, PP, PRE PFS-PR ?
| SFS ) P(?PP, FS, PFS-SBJ, PRE PFS-PR ?
| SFS )FS{SBJ,PRD,CO } FSSBJ FSNPFShisheSBJ:PRD FSAUXFShaytawasPRD S+FSPPamurasupposedCO SINFPREDPFSlehagiato-arriveFS{SBJ,PRD,CO } FSSBJ FSNPFShishePRD S+FSPPamurasupposedPRD:CO FSAUXFShaytawasCO SINFPREDPFSlehagiato-arrivePprojection({SBJ,PR ,C } | SFS ) Pprojection({SBJ,PR ,C } | SFS )Pconfiguration( ?SBJ, SBJ:PR , PR , C ?
| {SBJ,PR ,C } SFS) Pconfiguration( ?SBJ, PR , PR :C , C ?
| {SBJ,PR ,C } SFS)Prealization( PFS | SBJ SFS ) Prealization( PFS | SBJ SFS )Prealization( FS | SBJ:PR SFS ) Prealization( FS | PR :C SFS )Prealization( FS | PR SFS ) Prealization( FS | PR SFS )Prealization( P | C SFS) Prealization( P | C SFS)igure 2: The SP- (top) and - representation of sentences (4c-i) (left) and (4c-ii).SMSNPMS-SBJdaniDaniVPMS-PRDnatangaveNP-OBJmatanapresentPP-COMledianto-dinaSMSNP-OBJmatanapresentVPMS-PRDnatangaveNPMS-SBJdaniDaniPP-Cledianto-dinaP(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniP @SMSVPMSnatangaveBJ@SMSNP-OBJmatanapresentCOM@SMSPP-COledianto-dinaSM{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanap esentPRD@SMSVPMSnatangaveSBJ@SMSNPMSdaniDaniCOM@SMPP-COMledianto-dinaPprojection({S J,PRD,OBJ,COM} | SMS ) Pproject on({SBJ,PRD, J,CO } | SMS )Pconfiguration(?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfigur ti (?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealiz tion(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).NPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveSFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,P , }@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | BJ@SFS ) Prealizat on(NPFS | BJ@ FS )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@ FS )realization(VP | COM@SFS) Prealization(VP | COM@ FS)Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).SMSNPMS-SBJdaniDaniV -PRDn tangaveNP-OBJmatanapres ntP COMledianto-di aSMSNP-OBJmatanapresentVPMS-PRDnatangaveNPMS-SBJdaniDaniP-COMledianto-dinaP(NPMS-SBJ,V -PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS PRD,NP S-SBJ,PP-CO | SMS )S S{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@ MSVPMSnatangaveOBJ@SMSN -OBJmatanapresentCOM@SMSPP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanapresentPRD@SMSVPMSnatangaveSBJ S SNPMSd iDaniCOM@SMSPP-COMledianto-d naPprojection({SBJ,PRD,OBJ,CO } | S ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )Pconfiguration(?S,P,O,C?
| { BJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii .SFSNPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiato-arriveFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMle gito-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| S ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| S )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFSaytawasPRD@S+PPmurasupposedCOM@SINFPREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@ +FSPPamuraupposedP :COM@SFSAUXFShaytawasSINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfiguration( ?SBJ, J:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM COM?
| {SBJ,PRD,COM}@SFS)lization(NPFS | SBJ@S ) Prealization(N FS | SBJ@S )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PRD@SFS ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS) Prealization(VP | COM@SFS)Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii .SMSNPMS-SBJdaniDaniVPMS-PRDnatang veN -OBJmatanaprese tPP-COMledianto- naSMSNP-OBJmatanaprese tVPMS- RDtang veNPMS-SBJdaniDaniPP-COMledianto- naP(NPMS-SBJ,VPMS-PRD,N OBJ,PP-COM | SMS ) P(N -OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,O COM}@SMSBJ@SMSNPMSdaniDaniPRD@SMSVPMSnatang veOBJ@S SNP-OBJmatanaprese tCOM@SMSPP-COMledianto- naSMS{SBJ,PRD,O OM}@SMSOBJ@SMSNP-OBJmatanaprese tPRD@SMSVPMStang veBJ@ SNPMSdaniDaniCOM@SMSPP-COMledianto- naPprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )Pconfigurati (?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfigurati (?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@ MS ) Pre lization(NPMS | SBJ@ MS )Prealization(VB | PRD S ) Prealization(VB | PR S )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM ) Prealization(PP | CO )Figur 1: The SP-AGR (top) and R-AGR representatio of sentence (2b-i) (left) and (2b-ii).SFSNPFS-SBJhisheAUXFShaytwasMD -PRDamurasupposedVPINF-COMlehagiat -arriveSFSNPFS-SBJhisheMDFS-PRDamursupposedAUXFShaytawasVPIN -COMleh giato-arriveP(?NPFS-SBJ, AUXFS, P , PREDPFS-PR ?
| FS ) P(?PP, AUXFS, NPFS- BJ, PREDPFS-PR ?
| FS )SFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShisheSBJ:PRD@ FSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFREDPFSlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@ INFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PRD,COM} | SFS )Pconfigurati ( ?SBJ, SBJ:PRD, PRD, COM?
| {SBJ,PRD,COM}@SFS) Pconfigurati ( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)Prealization(NPFS | BJ@ FS ) Prealization(NPFS | BJ@ FS )Prealization(AUXFS | SBJ:PRD@SFS ) Prealization(AUXFS | PRD:COM@SFS )Prealization(MDFS | PR @SFS ) Prealization(MDFS | PR @SFS )Prealization(VP | COM@SFS) realization(VP | COM@SFS)Figur 2: The SP-AGR (top) and R-AGR representation of sentence (4c-i) (left) and (4c-ii).SMSNPMS-SBJdaniDaniVPMS-PRDnatangaveNP-OBJmatanapresentPP-COMledianto-dinaSMSNP-OBJmatanapresentVPMS-PRDnatangaveNPMS-SBJdaniDaniPP-COMledianto-dinaP(NPMS-SBJ,VPMS-PRD,NP-OBJ,PP-COM | SMS ) P(NP-OBJ,VPMS-PRD,NPMS-SBJ,PP-COM | SMS )SMS{SBJ,PRD,OBJ,COM}@SMSSBJ@SMSNPMSdaniDaniPRD@SMSVPMSnatangaveOBJ@SMSNP-OBJmatanapresentCOM@SMSPP-COMledianto-dinaSMS{SBJ,PRD,OBJ,COM}@SMSOBJ@SMSNP-OBJmatanapresentPRD@SMSVPMSnatangaveSBJ@SMSNPMSdaniDaniCOM@SMSPP-COMledianto-dinaPprojection({SBJ,PRD,OBJ,COM} | SMS ) Pprojection({SBJ,PRD,OBJ,COM} | SMS )Pconfiguration(?S,P,O,C?
| {SBJ,PRD,OBJ,COM}@SMS) Pconfiguration(?O,P,S,C?
| {SBJ,PRD,OBJ,COM}@SMS)Prealization(NPMS | SBJ@SMS ) Prealization(NPMS | SBJ@SMS )Prealization(VBMS | PRD@SMS ) Prealization(VBMS | PRD@SMS )Prealization(NP | OBJ@ SMS ) Prealization(NP | OBJ@ SMS )Prealization(PP | COM@ SMS ) Prealization(PP | COM@ SMS )Figure 1: The SP-AGR (top) and RR-AGR representations of sentences (2b-i) (left) and (2b-ii).SFSNPFS-SBJhisheAUXFShaytawasMDFS-PRDamurasupposedVPINF-COMlehagiao-arriveSFSNPFS-SBJhisheMDFS-PRDamurasupposedAUXFShaytawasVPINF-COMlehagiato-arriveP(?NPFS-SBJ, AUXFS, PP, PREDPFS-PRD?
| SFS ) P(?PP, AUXFS, NPFS-SBJ, PREDPFS-PRD?
| SFS )SFS{SBJ,PRD,COM}@ FSSBJ@SFSNPFShisheSBJ:PRD@SFSAUXFShaytawasPRD@S+FSPPamurasupposedCOM@SINFPREDPFlehagiato-arriveSFS{SBJ,PRD,COM}@SFSSBJ@SFSNPFShishePRD@S+FSPPamurasupposedPRD:COM@SFSAUXFShaytawasCOM@SINFPREDPFSlehagiato-arrivePprojection({SBJ,PRD,COM} | SFS ) Pprojection({SBJ,PR ,COM} | SFS )Pc nfiguration( ?SBJ, S J:PRD, PRD, C M?
| {SBJ,PRD,COM}@SFS) Pconfiguration( ?SBJ, PRD, PRD:COM, COM?
| {SBJ,PRD,COM}@SFS)realization(NPFS | SBJ@SFS ) Pre lization(NPFS | SBJ@SFS )Prealization(AUXFS | BJ:PRD SFS ) Prealization(AUXFS | PRD:C @SFS )Prealization(MDFS | F ) Prealization(MDFS | PRD@SFS )Prealization(VP | COM@SFS realization(VP | COM@ FS)Figure 2: The SP-AGR (top) and RR-AGR representation of sentences (4c-i) (left) and (4c-ii).46Model { gender def+acc gender+def+accSP-AGR 79.77 79.55 80.13 80.26(3942) (7594) (4980) (8933)RR-AGR 80.23 81.09 81.48 82.64(3292) (5686) (3772) (6516)SP-AGR (gp) 83.06 82.18 79.53 80.89(5914) (10765) (12700) (11028)RR-AGR (gp) 83.49 83.70 83.66 84.13(6688) (10063) (12383) (12497)SP-AGR (gp,hd) 76.61 64.07 75.12 61.69(10081) (16721) (11681) (18428)RR-AGR (gp,hd) 83.40 81.19 83.33 80.45(12497) (22979) (13828) (24934)Table 1: F-score (#params) measure for all models onthe Hebrew treebank dev-set for Sentences Length < 405 Results and DiscussionTable 1 shows the standard F1 scores (and #param-eters) for all models.
Throughout, the RR-AGRmodel outperforms the SP-AGR models that use thesame category set and the same morphological fea-tures as state splits.
For RR-AGR and RR-AGR (gp)models, adding agreement features to case featuresimproves performance.
The accumulative contribu-tion is significant.
For SP-AGR and SP-AGR (gp)models, adding more features either remains at thesame level of performance or becomes detrimental.Since the SP/RR-AGR and SP/RR-AGR (gp)models are of comparable size for each feature-set,it is unlikely that the differences in performance aredue to the lack of training data.
A more reason-able explanation if that the RR parameters repre-sent functional generalizations orthogonal to config-uration for which statistical evidence is more easilyfound in the data.
The robust realization distribu-tions which cut across ordering alternatives can steerthe disambiguation in the right direction.The RR-AGR (gp) +gen+def+acc model yieldsthe best result for parsing Hebrew to date (F1 84.13),improving upon our best model in (Tsarfaty et al,2009) (F1 83.33, underlined) in a pos-tagged set-ting.
For this setting, Arabic parsing results are F178.1.
Given the similar morphosyntactic phenomena(agreement, MaSDaR, iDaFa) it would be interest-ing to see if the model enhances parsing for Arabic.For (gp,hd) models (a configuration which wasshown to give the best results in (Tsarfaty et al,2009)) there is a significant decrease in accuracywith the gender feature, but there is a lesson to belearned.
Firstly, while the RR-AGR (gp,hd) modelshows moderate decrease with gender, the decreasein performance of SP-AGR (gp,hd) for the samefeature-set is rather dramatic, which is consistentwith the observation that the RR model is less vul-nerable to sparseness and that it makes better use ofthe statistics of functional relations in the data.Consulting the size of the different grammars, thecombination of RR-AGR (gp, hd) with gender fea-tures indeed results in substantially larger grammars,and it is possible that at this point we indeed need toincorporate smoothing.
At the same time there maybe an alternative explanation for the decreased per-formance.
It might be that the head-tag does not addinformative cues beyond the contribution of the fea-tures which are spread inside the constituent, and arealready specified.
This is a reasonable hypothesissince gender in Hebrew always percolates throughthe head as opposed to def/acc that percolate fromother forms.
Incorporating head-tag in (Tsarfaty etal., 2009) might have led to improvement only dueto the lack of agreement features which subsumethe relevant pattern.
This suggests that incorporat-ing all co-heads and functional elements that con-tribute morphological features spread inside the con-stituent, is more adequate for modeling morphosyn-tax than focusing on the features of a single head.6 ConclusionWe show that morphologically marked agreementfeatures can significantly improve parsing perfor-mance if they are represented and parametrized ina way that reflects their linguistic substance: relat-ing form-and-function in a non-linear fashion.
Wehave so far dealt with the adequacy of representa-tion and we plan to test whether more sophisticatedestimation (e.g., split-merge-smooth estimation as in(Petrov et al, 2006)) can obtain further improve-ments from the explicit representation of agreement.At the same time, the state-of-the-art results wepresent render the RR model promising for furtherexploration with morphologically rich languages.Acknowledgements The work of the first authorhas been funded by NWO, grant 017.001.271.
Wewish to thank Joakim Nivre and three anonymousreviewers for helpful comments on earlier drafts.47ReferencesSteven Abney.
1997.
Stochastic attribute-value gram-mars.
Computational Linguistics, 23(4):597?618.Stephen R. Anderson.
1982.
Where?s morphology?
Lin-guistic Inquiry.Noam Chomsky.
1970.
Remarks on nominalization.
InR.
Jacobs and P. Rosenbaum, editors, Reading in En-glish Transformational Grammar.
Waltham: Ginn.Greville G. Corbett.
2001.
Agreement: Terms andboundaries.
In SMG conference papers.Gabi Danon.
2008.
Definiteness spreading in the hebrewconstruct-state.
Lingua, 118(7):872?906.Edit Doron.
1986.
The pronominal ?copula?
as agree-ment clitic.
Syntax and Semantics, (19):313?332.Gerald Gazdar, Ewan Klein, Geoffrey K. Pullum, andIvan A.
Sag.
1985.
Generalised phrase structuregrammar.
Blackwell, Oxford, England.Yoav Goldberg and Michael Elhadad.
2009.
Hebrew de-pendency parsing: Initial results.
In Proceedings ofIWPT.Yoav Goldberg and Reut Tsarfaty.
2008.
A single frame-work for joint morphological segmentation and syn-tactic parsing.
In Proceedings of ACL.Yoav Goldberg, Meni Adler, and Michael Elhadad.
2006.Noun phrase chunking in hebrew: Influence of lex-ical and morphological features.
In Proceedings ofCOLING-ACL.Nomie Guthmann, Yuval Krymolowski, Adi Milea, andYoad Winter.
2009.
Automatic annotation of morpho-syntactic dependencies in a Modern Hebrew treebank.In Frank Van Eynde, Anette Frank, Koenraad DeSmedt, and Gertjan van Noord, editors, Proceedingsof TLT.Kenneth L. Hale.
1983.
Warlpiri and the grammar ofnon-configurational languages.
Natural Language andLinguistic Theory, 1(1).Julia Hockenmaier and Mark Steedman.
2003.
Parsingwith generative models of predicate-argument struc-ture.
In Proceedings of ACL.Dan Klein and Christopher D. Manning.
2003.
Accurateunlexicalized parsing.
In Proceedings of ACL.Mohamed Maamouri, Ann Bies, and Seth Kulick.
2008.Enhanced annotation and parsing of the arabic tree-bank.
In Proceedings of INFOS.Yusuke Miyao and Jun?ichi Tsujii.
2008.
Feature-forestmodels for probabilistic hpsg parsing.
ComputationalLinguistics, 34(1):35?80.Lilja ?vrelid and Joakim Nivre.
2007.
Swedish depen-dency parsing with rich linguistic features.
In Pro-ceeding of RANLP.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proceedings of ACL.Reut Tsarfaty and Khalil Sima?an.
2008.
Relational-realizational parsing.
In Proceedings of CoLing.Reut Tsarfaty, Khalil Sima?an, and Remko Scha.
2009.An alternative to head-driven approaches for parsing a(relatively) free word order language.
In Proceedingsof EMNLP.48
