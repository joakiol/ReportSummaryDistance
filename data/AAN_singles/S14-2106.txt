Proceedings of the 8th International Workshop on Semantic Evaluation (SemEval 2014), pages 605?609,Dublin, Ireland, August 23-24, 2014.Synalp-Empathic: A Valence Shifting Hybrid System for SentimentAnalysisAlexandre Denis, Samuel Cruz-Lara, Nadia Bellalem and Lotfi BellalemLORIA/University of LorraineNancy, France{alexandre.denis, samuel.cruz-lara, nadia.bellalem, lotfi.bellalem}@loria.frAbstractThis paper describes the Synalp-Empathicsystem that competed in SemEval-2014Task 9B Sentiment Analysis in Twitter.Our system combines syntactic-based va-lence shifting rules with a supervisedlearning algorithm (Sequential MinimalOptimization).
We present the system, itsfeatures and evaluate their impact.
Weshow that both the valence shifting mech-anism and the supervised model enable toreach good results.1 IntroductionSentiment Analysis (SA) is the determination ofthe polarity of a piece of text (positive, nega-tive, neutral).
It is not an easy task, as provenby the moderate agreement between human an-notators when facing this task.
Their agreementvaries whether considering document or sentencelevel sentiment analysis, and different domainsmay show different agreements as well (Berming-ham and Smeaton, 2009).As difficult the task is for human beings, it iseven more difficult for machines which face syn-tactic, semantic or pragmatic difficulties.
Considerfor instance irrealis phenomena such as ?if this isgood?
or ?it would be good if ?
that are both neu-tral.
Irrealis is also present in questions (?is thisgood??)
but presupposition of existence does mat-ter: ?can you fix this terrible printer??
would bepolarized while ?can you give me a good advice?
?would not.
Negation and irrealis interact as well,compare for instance ?this could be good?
(neutralor slightly positive) and ?this could not be good?
(clearly negative).
Other difficult phenomena in-clude semantic or pragmatic effects, such as pointThis work is licensed under a Creative Commons At-tribution 4.0 International Licence.
Page numbers and pro-ceedings footer are added by the organisers.
Licence details:http://creativecommons.org/licenses/by/4.0/of view (?Israel failed to defeat Hezbollah?, nega-tive for Israel, positive for Hezbollah), backgroundknowledge (?this car uses a lot of gas?
), seman-tic polysemy (?this vacuum cleaner sucks?
vs ?thismovie sucks?
), etc.From the start, machine learning has been thewidely dominant approach to sentiment analy-sis since it tries to capture these phenomena all-together (Liu, 2012).
Starting from simple n-grams (Pang et al., 2002), more recent approachestend to include syntactic contexts (Socher et al.,2011).
However these supervised approachesall require a training corpus.
Unsupervised ap-proaches such as the seminal paper of (Turney,2002) require training corpus as well but do notrequire annotations.
We propose in this paper tolook first at approaches that do not require anycorpus because annotating a corpus is in generalcostly, especially in sentiment analysis in whichseveral annotators are required to maintain a highlevel of agreement1.
Nevertheless supervised ma-chine learning can be useful to adapt the systemto a particular domain and we will consider it aswell.Hence, we propose in this paper to first considera domain independent sentiment analysis tool thatdoes not require any training corpus (section 2).Once the performance of this tool is assessed (sec-tion 2.4) we propose to consider how the systemcan be extended with machine learning in sec-tion 3.
We show the results on the SemEval 2013and 2014 corpora in section 4.2 Sentiment Analysis without CorpusWe present here a system that does sentiment anal-ysis without requiring a training corpus.
We do soin three steps: we first present a raw lexical base-line that naively considers average valence takingthe prior valence of words from polarity lexicons.1as done in SemEval2013 SA task (Nakov et al., 2013)605We then show how to adapt this baseline to theTwitter domain.
Finally, we describe a methodwich takes into account the syntactic context ofpolarized words.
All methods and strategies arethen evaluated.2.1 Raw Lexical BaselineThe raw lexical baseline is a simple system thatonly relies on polarity lexicons and takes the aver-age valence of all the words.
The valence is mod-eled using a continuous value in [0, 1], 0.5 beingneutral.
The algorithm is as follows:1. perform part of speech tagging of the inputtext using the Stanford CoreNLP tool suite,2.
for all words in the input text, retrieve theirpolarity from the lexicons using lemma andpart of speech information.
If the word isfound in several lexicons, return the averageof the found polarities.
Otherwise if the wordis not found, return 0.5.3. then for the tweet, simply compute the aver-age valence among all words.We tried several lexicons but ended with fo-cusing on the Liu?s lexicon (Hu and Liu, 2004)which proved to offer the best results.
HoweverLiu?s lexicon is missing slang or bad words.
Wetherefore extended the lexicon using the onlines-langdictionary.com website which provides a listof slang words expressing either positive or neg-ative properties.
We extracted around 100 wordsfrom this lexicon which we call urban lexicon.2.2 Twitter AdaptationsFrom this lexical base we considered several smallimprovements to adapt to the Twitter material.
Wefirst observed that the Stanford part of speech tag-ger had a tendency to mistag the first positionin the sentence as proper noun.
Since in tweetsthis position is often in fact a common noun, wesystematically retagged these words as commonnouns.
Second, we used a set of 150 hand writ-ten rules designed to handle chat colloquialismi.e., abbreviations (?wtf ??
?what the f***?, twit-ter specific expressions (?mistweet?
?
?regrettedtweet?
), missing apostrophe (?isnt?
?
?isn?t?
),and smileys.
Third, we applied hashtag splitting(e.g.
?#ihatemondays??
?i hate mondays?).
Fi-nally we refined the lexicon lookup strategy tohandle discrepancies between lexicon and part ofspeech tagger.
For instance, while the part ofspeech tagger may tag stabbed as an adjective withlemma stabbed, the lexicon might list it as a verbwith lemma stab.
To improve robustness we there-fore look first for the inflected form then for thelemma.2.3 Syntactic EnhancementsValence Shifting Valence shifting refers to thedifferential between the prior polarity of a word(polarity from lexicons) and its contextual po-larity (Polanyi and Zaenen, 2006).
Follow-ing (Moilanen and Pulman, 2007), we apply polar-ity rewriting rules over the parsing structure.
How-ever we differ from them in that we consider de-pendency rather than phrase structure trees.The algorithm is as follows:1. perform dependency parsing of the text (withStanford CoreNLP)2. annotate each word with its prior polarity asfound in polarity lexicons3.
rewrite prior polarities using dependencymatching, hand-crafted rules4.
return the root polarityTable 1 shows example rules.
Each rule is com-posed of a matching part and a rewriting part.
Bothparts have the form (N,LG, PG, LD, PD) whereN is the dependency name, LGand LDare re-spectively the lemmas of the governor and de-pendent words, PGand PDare the polarity ofthe governor and dependent words.
We write therules in short form by prefixing them with thename of the dependency and either the lemma orthe polarity for the arguments, e.g.
N(PG, PD).For instance, the inversion rule ?neg(PG, PD) ?neg(!PG, PD)?
inverts the polarity of the gover-nor PGfor dependencies named neg.
One impor-tant rule is the propagation rule ?N (0.5, PD) ?N (PD,PD)?
which propagates the polarity of thedependent word PDto the governor only if it isneutral.
Another useful rule is the overwrite rule?amod(1,0)?
amod(0,0)?
which erases for amoddependencies, the positive polarity of the governorgiven a negative modifier.The main algorithm for rule application consistsin testing all rules (in a fixed order) on all de-pendencies iteratively.
Whenever a rule fires, thewhole set of rules is tested again.
Potential looping606Rule Exampleneg(PG, PD) ?
neg(!PG, PD) he?s not happydet(PG, ?no?)
?
det(!PG,?no?)
there is no hateamod(1,0) ?
amod(0,0) a missed opportunitynsubj(0,1) ?
nsubj(0,0) my dreams are crushednsubj(1,0) ?
nsubj(1,1) my problem is fixedN (0.5, PD) ?
N (PD,PD) (propagation)Table 1: Excerpt of valence shifting rules.is prevented because (i) the dependency graph re-turned by the Stanford Parser is a directed acyclicgraph (de Marneffe and Manning, 2008) and (ii)the same rule cannot apply twice to the same de-pendency.For instance, in the sentence ?I do not think itis a missed opportunity?, the verb ?missed?
hasnegative polarity and the noun ?opportunity?
haspositive polarity.
The graph in Figure 1 shows dif-ferent rules application: first the overwrite rule (1.
)changes the positive polarity of ?opportunity?
to anegative polarity which is then transferred to themain verb ?think?
thanks to the propagation rule(2.).
Finally, the inversion rule (3.)
inverts the neg-ative polarity of think.
As a result, the polarity ofthe sentence is positive.Figure 1: Rules application example.Various Phenomena Several other phenomenaneed to be taken into account when consideringthe co-text.
Because of irrealis phenomena men-tioned in the introduction, we completely ignoredquestions.
We also ignored proper nouns (such asin ?u need 2 c the documentary The Devil Inside?
)which were a frequent source of errors.
These twophenomena are labeled Ignoring forms in Table 2.Finally since our approach is sentence-based weneed to consider valence of tweets with severalsentences and we simply considered the average.2.4 Results on SemEval2013We measure the performance of the differentstrategies on the 3270 tweets that we downloadedfrom the SemEval 2013 Task 2 (Nakov et al.,2013) test corpus2.
The used metrics is the same2Because of Twitter policy the test corpus is not dis-tributed by organizers but tweets must be downloaded usingthan SemEval 2013 one, an unweighted averagebetween positive and negative F-score.System F-score GainRaw lexical baseline 54.75+ Part of speech fix 55.00 +0.25+ Colloqualism rewriting 57.66 +2.66+ Hashtag splitting 57.80 +0.14+ Lexicon fetch strategy 58.25 +0.45+ Valence shifting 62.37 +4.12+ Ignoring forms 62.97 +0.60Table 2: Results of syntactic system.As shown in Table 2, the raw lexical baselinestarts at 54.75% F-score.
The two best improve-ments are Colloquialism rewriting (+2.66) thatseems to capture useful polarized elements andValence shifting (+4.12) which provides an accu-rate account for shifting phenomena.
Overall otherstrategies taken separately do not contribute muchbut enable to have an accumulated +1.44 gain ofF-score.
The final result is 62.97%, and we willrefer to this first system as the Syntactic system.3 Machine Learning OptimizationThe best F-score attained with the syntactic system(62.97%) is still below the best system that par-ticipated in SemEval2013 (69.02%)3.
To improveperformance, we input the valence computed bythe syntactic system as a feature in a supervisedmachine learning (ML) algorithm.
While there ex-ists other methods such as (Choi and Cardie, 2008)which incorporates syntax at the heart in the ma-chine algorithm, this approach has the advantageto be very simple and independent of any specificML algorithm.
We chose the Sequential MinimalOptimization (SMO) which is an optimization ofSupport Vector Machine (Platt, 1999) since it wasshown (Balahur and Turchi, 2012) to have goodresults that we observed ourselves.In addition to the valence output by our syntac-tic system, we considered the following additionallow level features:?
1-grams words: we observed lower resultswith n-grams (n > 1) and decided to keep1-grams only.
The words were lemmatizedand no tf-idf weighting was applied since itshowed lower results.?
polarity counts: it is interesting to in-clude low level polarity counts in case thetheir identifiers, resulting in discrepancies from the officialcampaign (3814 tweets).3Evaluated on full 3814 tweets corpus607syntactic system does not correctly cap-ture valence shifts.
We thus includedindependent features counting the numberof positive/negative/neutral words accord-ing to several lexicons: Liu?s lexicon (Huand Liu, 2004), our urban lexicon, Senti-Wordnet (Baccianella et al., 2010), QWord-net (Agerri and Garca-Serrano, 2010) andMPQA lexicon (Wilson et al., 2005).?
punctuation count: exclamation and interro-gation marks are important, so we have anindependent feature counting occurrences of??
?, ?!
?, ??!
?, ?!?
?.Thanks to the ML approach, we can obtain fora given tweet the different probabilities for eachclass.
We were then able to adapt each probabili-ties to favor the SemEval metrics by weighting theprobabilities thanks to the SemEval 2013 trainingand development corpus using 10-fold cross vali-dation (the weights were trained on 90% and eval-uated on 10%).
The resulting weights reduce theprobability to assign the neutral class to a giventweet while raising the positive/negative probabil-ities.
This optimization is called metrics weightingin Table 3.4 Optimization ResultsWe describe here the results of integrating the syn-tactic system as a feature of the SMO along withother low level features.
The SemEval 2014 goldtest corpus was not available at the time of thiswriting hence we detail the features only on theSemEval 2013 gold test corpus.4.1 On SemEval 2013The results displayed in Table 3 are obtained withthe SMO classifier trained using the WEKA li-brary (Hall et al., 2009) on our downloaded Se-mEval 2013 development and training corpora(7595 tweets).
As before, the given score is theaverage F-score computed on the SemEval 2013test corpus.
Note that the gain of each featuremust be interpreted in the context of other features(e.g.
Polarity counts needs to be understood asWords+Polarity Counts).The syntactic system feature, that is consider-ing only one training feature which is the valenceannotated by the syntactic system, starts very low(33.69%) since it appears to systematically fa-vor positive and neutral classes.
However addingFeatures F-score GainSyntactic system 33.69+ Words 63.03 +29.34+ Polarity counts 65.02 +1.99+ Punctuation 65.65 +0.63+ Metrics weighting 67.83 +2.18Table 3: Detailed results on SemEval 2013.the 1-gram lemmatized words raises the result to63.03%, slightly above the syntactic system alone(62.97%).
Considering polarity counts raises theF-score to 65.02% showing that the syntactic sys-tem does not capture correctly all valence shifts(or valence neutralizations).
Considering an inde-pendent feature for punctuation slightly raises theresult.
Metrics weighting, while not being a train-ing feature per se, provides an important boost forthe final F-score (67.83%).4.2 On SemEval 2014We participated to SemEval 2014 task B as theSynalp-Empathic team (Rosenthal et al., 2014).The results are 67.43% on the Twitter 2014dataset, 3.53 points below the best system.
In-terestingly the score obtained on Twitter 2014 isvery close to the score we computed ourselves onTwitter 2013 (67.83%) suggesting no overfitting toour training corpus.
However, we observed a bigdrop in the Twitter 2013 evaluation as carried outby organizers (63.65%), we assume that the differ-ence in results could be explained by difference indatasets coverage caused by Twitter policy.5 Discussion and ConclusionWe presented a two-steps approach for sentimentanalysis on Twitter.
We first developed a lexico-syntactic approach that does not require any train-ing corpus and enables to reach 62.97% on Se-mEval 2013.
We then showed how to adapt theapproach given a training corpus which enablesreaching 67.43% on SemEval 2014, 3.53 pointsbelow the best system.
We further showed thatthe approach is not sensitive to overfitting since itproved to be as efficient on the SemEval 2013 andthe SemEval 2014 test corpus.
In order to improvethe performance, it could be possible adapt thelexicons to the specific Twitter domain (Demirozet al., 2012).
It may also be possible to investi-gate how to learn automatically the valence shift-ing rules, for instance with Monte Carlo methods.608AcknowledgementsThis work was conducted in the context of theITEA2 1105 Empathic Products project, and issupported by funding from the French Services, In-dustry and Competitivity General Direction.
Wewould like to thank Christophe Cerisara for the in-sights regarding the machine learning system andClaire Gardent for her advices regarding the read-ability of the paper.ReferencesRodrigo Agerri and Ana Garca-Serrano.
2010.
Q-wordnet: Extracting polarity from wordnet senses.In Proceedings of the Seventh International Con-ference on Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).Stefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
Sentiwordnet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Proceedings of the Seventh InternationalConference on Language Resources and Evaluation(LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).Alexandra Balahur and Marco Turchi.
2012.
Mul-tilingual sentiment analysis using machine transla-tion?
In Proceedings of the 3rd Workshop in Com-putational Approaches to Subjectivity and SentimentAnalysis, WASSA ?12, pages 52?60, Stroudsburg,PA, USA.Adam Bermingham and Alan F. Smeaton.
2009.
Astudy of inter-annotator agreement for opinion re-trieval.
In James Allan, Javed A. Aslam, MarkSanderson, ChengXiang Zhai, and Justin Zobel, ed-itors, SIGIR, pages 784?785.
ACM.Yejin Choi and Claire Cardie.
2008.
Learning withcompositional semantics as structural inference forsubsentential sentiment analysis.
In Proceedings ofthe Conference on Empirical Methods in NaturalLanguage Processing, EMNLP ?08, pages 793?801,Stroudsburg, PA, USA.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
Stanford typed dependencies manual.Technical report.Gulsen Demiroz, Berrin Yanikoglu, Dilek Tapucu, andY?ucel Saygin.
2012.
Learning domain-specificpolarity lexicons.
In Proceedings of the 12th In-ternational Conference of Data Mining Workshops(ICDMW), pages 674?679, Dec.Mark Hall, Eibe Frank, Geoffrey Holmes, BernhardPfahringer, Peter Reutemann, and Ian H. Witten.2009.
The WEKA data mining software: An update.SIGKDD Explor.
Newsl., 11(1):10?18, November.Minqing Hu and Bing Liu.
2004.
Mining and summa-rizing customer reviews.
In Proceedings of the 10thInternational Conference on Knowledge Discoveryand Data Mining, pages 168?177.Bing Liu, 2012.
Sentiment Analysis and Opinion Min-ing.
Morgan & Claypool Publishers, May.Karo Moilanen and Stephen Pulman.
2007.
Sentimentcomposition.
In Proceedings of Recent Advances inNatural Language Processing (RANLP 2007), pages378?382, September 27-29.Preslav Nakov, Sara Rosenthal, Zornitsa Kozareva,Veselin Stoyanov, Alan Ritter, and Theresa Wilson.2013.
SemEval-2013 task 2: sentiment analysisin twitter.
In Proceedings of the 7th InternationalWorkshop on Semantic Evaluation.Bo Pang, Lillian Lee, and Shivakumar Vaithyanathan.2002.
Thumbs up?
Sentiment classication usingmachine learning techniques.
In Proceedings of the2002 Conference on Empirical Methods in NaturalLanguage Processing, pages 79?86, Philadelphia,PA.John C. Platt.
1999.
Fast training of support vectormachines using sequential minimal optimization.
InAdvances in Kernel Methods, pages 185?208.
MITPress, Cambridge, MA, USA.Livia Polanyi and Annie Zaenen.
2006.
Contextualvalence shifters.
In JamesG.
Shanahan, Yan Qu, andJanyce Wiebe, editors, Computing Attitude and Af-fect in Text: Theory and Applications, volume 20of The Information Retrieval Series, pages 1?10.Springer Netherlands.Sara Rosenthal, Preslav Nakov, Alan Ritter, andVeselin Stoyanov.
2014.
SemEval-2014 task 9:Sentiment analysis in twitter.
In Proceedings of the8th International Workshop on Semantic Evaluation.Richard Socher, Jeffrey Pennington, Eric H. Huang,Andrew Y. Ng, and Christopher D. Manning.
2011.Semi-supervised recursive autoencoders for predict-ing sentiment distributions.
In Proceedings of the2011 Conference on Empirical Methods in NaturalLanguage Processing, pages 151?161, Edinburgh.Peter Turney.
2002.
Thumbs up or thumbs down?
se-mantic orientation applied to unsupervised classica-tion of reviews.
In Proceedings of the 40th AnnualMeeting of the Association for Computational Lin-guistics, pages 417?424, Philadelphia, PA.Theresa Wilson, Janyce Wiebe, and Paul Hoff-mann.
2005.
Recognizing Contextual Polar-ity in Phrase-Level Sentiment Analysis.
In Pro-ceedings of Human Language Technologies Confer-ence/Conference on Empirical Methods in NaturalLanguage Processing (HLT/EMNLP 2005), Vancou-ver, CA.609
