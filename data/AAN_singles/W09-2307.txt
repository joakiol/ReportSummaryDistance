Proceedings of SSST-3, Third Workshop on Syntax and Structure in Statistical Translation, pages 51?59,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsDiscriminative Reordering with Chinese Grammatical Relations FeaturesPi-Chuan Changa, Huihsin Tsengb, Dan Jurafskya, and Christopher D. ManningaaComputer Science Department, Stanford University, Stanford, CA 94305bYahoo!
Inc., Santa Clara, CA 95054{pichuan,jurafsky,manning}@stanford.edu, huihui@yahoo-inc.comAbstractThe prevalence in Chinese of grammaticalstructures that translate into English in dif-ferent word orders is an important cause oftranslation difficulty.
While previous work hasused phrase-structure parses to deal with suchordering problems, we introduce a richer set ofChinese grammatical relations that describesmore semantically abstract relations betweenwords.
Using these Chinese grammatical re-lations, we improve a phrase orientation clas-sifier (introduced by Zens and Ney (2006))that decides the ordering of two phrases whentranslated into English by adding path fea-tures designed over the Chinese typed depen-dencies.
We then apply the log probabil-ity of the phrase orientation classifier as anextra feature in a phrase-based MT system,and get significant BLEU point gains on threetest sets: MT02 (+0.59), MT03 (+1.00) andMT05 (+0.77).
Our Chinese grammatical re-lations are also likely to be useful for otherNLP tasks.1 IntroductionStructural differences between Chinese and Englishare a major factor in the difficulty of machine trans-lation from Chinese to English.
The wide varietyof such Chinese-English differences include the or-dering of head nouns and relative clauses, and theordering of prepositional phrases and the heads theymodify.
Previous studies have shown that using syn-tactic structures from the source side can help MTperformance on these constructions.
Most of theprevious syntactic MT work has used phrase struc-ture parses in various ways, either by doing syntax-directed translation to directly translate parse treesinto strings in the target language (Huang et al,2006), or by using source-side parses to preprocessthe source sentences (Wang et al, 2007).One intuition for using syntax is to capture dif-ferent Chinese structures that might have the same(a) (ROOT  (IP     (LCP       (QP (CD ?
)(CLP (M ?
)))(LC ?
))(PU ?)
(NP       (DP (DT ??
))(NP (NN ??)))
(VP       (ADVP (AD ??
))(VP (VV ??)
(NP           (NP             (ADJP (JJ ??
))(NP (NN ??
)))(NP (NN ??
)))(QP (CD ?????
)(CLP (M ?
)))))(PU ?
)))(b) (ROOT  (IP     (NP       (DP (DT ??
))(NP (NN ??)))
(VP       (LCP         (QP (CD ?
)(CLP (M ?
)))(LC ?
))(ADVP (AD ??
))(VP (VV ??)
(NP           (NP             (ADJP (JJ ??
))(NP (NN ??
)))(NP (NN ??
)))(QP (CD ?????
)(CLP (M ?
)))))(PU ?)))???
??????
??
???
??????????
(three)?
(year)?
(over; in) ??
(city)??(complete)??
(collectively) ??
(invest) ?
(yuan)(these) ??(asset)??(fixed)?????
(12 billion)loc nsubj advmod dobj rangelobj det nnnummod amodnummodFigure 1: Sentences (a) and (b) have the same mean-ing, but different phrase structure parses.
Both sentences,however, have the same typed dependencies shown at thebottom of the figure.meaning and hence the same translation in English.But it turns out that phrase structure (and linear or-der) are not sufficient to capture this meaning rela-tion.
Two sentences with the same meaning can havedifferent phrase structures and linear orders.
In theexample in Figure 1, sentences (a) and (b) have thesame meaning, but different linear orders and dif-ferent phrase structure parses.
The translation ofsentence (a) is: ?In the past three years these mu-nicipalities have collectively put together investmentin fixed assets in the amount of 12 billion yuan.?
Insentence (b), ?in the past three years?
has moved its51position.
The temporal adverbial ??#u?
(in thepast three years) has different linear positions in thesentences.
The phrase structures are different too: in(a) the LCP is immediately under IP while in (b) itis under VP.We propose to use typed dependency parses in-stead of phrase structure parses.
Typed dependencyparses give information about grammatical relationsbetween words, instead of constituency informa-tion.
They capture syntactic relations, such as nsubj(nominal subject) and dobj (direct object) , but alsoencode semantic information such as in the loc (lo-calizer) relation.
For the example in Figure 1, if welook at the sentence structure from the typed depen-dency parse (bottom of Figure 1), ??#u?
is con-nected to the main verb q?
(finish) by a loc (lo-calizer) relation, and the structure is the same forsentences (a) and (b).
This suggests that this kindof semantic and syntactic representation could havemore benefit than phrase structure parses.Our Chinese typed dependencies are automati-cally extracted from phrase structure parses.
In En-glish, this kind of typed dependencies has been in-troduced by de Marneffe and Manning (2008) andde Marneffe et al (2006).
Using typed dependen-cies, it is easier to read out relations between words,and thus the typed dependencies have been used inmeaning extraction tasks.We design features over the Chinese typed depen-dencies and use them in a phrase-based MT sys-tem when deciding whether one chunk of Chinesewords (MT system statistical phrase) should appearbefore or after another.
To achieve this, we train adiscriminative phrase orientation classifier follow-ing the work by Zens and Ney (2006), and we usethe grammatical relations between words as extrafeatures to build the classifier.
We then apply thephrase orientation classifier as a feature in a phrase-based MT system to help reordering.2 Discriminative Reordering ModelBasic reordering models in phrase-based systemsuse linear distance as the cost for phrase move-ments (Koehn et al, 2003).
The disadvantage ofthese models is their insensitivity to the content ofthe words or phrases.
More recent work (Tillman,2004; Och et al, 2004; Koehn et al, 2007) has in-troduced lexicalized reordering models which esti-mate reordering probabilities conditioned on the ac-tual phrases.
Lexicalized reordering models havebrought significant gains over the baseline reorder-ing models, but one concern is that data sparsenesscan make estimation less reliable.
Zens and Ney(2006) proposed a discriminatively trained phraseorientation model and evaluated its performance as aclassifier and when plugged into a phrase-based MTsystem.
Their framework allows us to easily add inextra features.
Therefore we use it as a testbed to seeif we can effectively use features from Chinese typeddependency structures to help reordering in MT.2.1 Phrase Orientation ClassifierWe build up the target language (English) translationfrom left to right.
The phrase orientation classifierpredicts the start position of the next phrase in thesource sentence.
In our work, we use the simplestclass definition where we group the start positionsinto two classes: one class for a position to the left ofthe previous phrase (reversed) and one for a positionto the right (ordered).Let c j, j?
be the class denoting the movement fromsource position j to source position j?
of the nextphrase.
The definition is:c j, j?
={ reversed if j?
< jordered if j?
> jThe phrase orientation classifier model is in the log-linear form:p?N1 (c j, j?
| f J1 ,eI1, i, j)= exp(?Nn=1 ?nhn( f J1 ,eI1, i, j,c j, j?))?c?
exp(?Nn=1 ?nhn( f J1 ,eI1, i, j,c?
))i is the target position of the current phrase, and f J1and eI1 denote the source and target sentences respec-tively.
c?
represents possible categories of c j, j?
.We can train this log-linear model on lots of la-beled examples extracted from all of the aligned MTtraining data.
Figure 2 is an example of an alignedsentence pair and the labeled examples that can beextracted from it.
Also, different from conventionalMERT training, we can have a large number of bi-nary features for the discriminative phrase orienta-tion classifier.
The experimental setting will be de-scribed in Section 4.1.52(21) </s>(20) .
(19) world(18) outside(17) the(16) to(15) up(14) opening(13) of(12) policy(11) 's(10) China(9) from(8) arising(7) star(6) bright(5) a(4) become(3) already(2) has(1) Beihai(0) <s>(15)</s>(14)?(13)??(12)?(11)?(10)?(9)??(8)?(7)?5)?(6)?)?(4)??(3)??(2)?(1)??
(0)<s>ordered151420ordered14618ordered6516reversed5715reversed7810reversed8109ordered1098reversed9137ordered13126ordered12115ordered1134ordered323ordered211ordered100classj'jii jFigure 2: An illustration of an alignment grid between a Chinese sentence and its English translation along with thelabeled examples for the phrase orientation classifier.
Note that the alignment grid in this example is automaticallygenerated.The basic feature functions are similar to whatZens and Ney (2006) used in their MT experiments.The basic binary features are source words within awindow of size 3 (d ?
?1,0,1) around the currentsource position j, and target words within a windowof size 3 around the current target position i.
In theclassifier experiments in Zens and Ney (2006) theyalso use word classes to introduce generalization ca-pabilities.
In the MT setting it?s harder to incorpo-rate the part-of-speech information on the target lan-guage.
Zens and Ney (2006) also exclude word classinformation in the MT experiments.
In our workwe will simply use the word features as basic fea-tures for the classification experiments as well.
Asa concrete example, we look at the labeled example(i = 4, j = 3, j?
= 11) in Figure 2.
We include theword features in a window of size 3 around j and ias in Zens and Ney (2006), we also include wordsaround j?
as features.
So we will have nine wordfeatures for (i = 4, j = 3, j?
= 11):Src?1:.
Src0:??
Src1:?
)Src2?1:{ Src20: Src21:(Tgt?1:already Tgt0:become Tgt1:a2.2 Path Features Using Typed DependenciesAssuming we have parsed the Chinese sentence thatwe want to translate and have extracted the gram-matical relations in the sentence, we design featuresusing the grammatical relations.
We use the path be-tween the two words annotated by the grammaticalrelations.
Using this feature helps the model learnabout what the relation is between the two chunksof Chinese words.
The feature is defined as follows:for two words at positions p and q in the Chinese53Shared relations Chinese Englishnn 15.48% 6.81%punct 12.71% 9.64%nsubj 6.87% 4.46%rcmod 2.74% 0.44%dobj 6.09% 3.89%advmod 4.93% 2.73%conj 6.34% 4.50%num/nummod 3.36% 1.65%attr 0.62% 0.01%tmod 0.79% 0.25%ccomp 1.30% 0.84%xsubj 0.22% 0.34%cop 0.07% 0.85%cc 2.06% 3.73%amod 3.14% 7.83%prep 3.66% 10.73%det 1.30% 8.57%pobj 2.82% 10.49%Table 1: The percentage of typed dependencies in files1?325 in Chinese (CTB6) and English (English-ChineseTranslation Treebank)sentence (p < q), we find the shortest path in thetyped dependency parse from p to q, concatenate allthe relations on the path and use that as a feature.A concrete example is the sentences in Figure 3,where the alignment grid and labeled examples areshown in Figure 2.
The glosses of the Chinese wordsin the sentence are in Figure 3, and the English trans-lation is ?Beihai has already become a bright stararising from China?s policy of opening up to the out-side world.?
which is also listed in Figure 2.For the labeled example (i = 4, j = 3, j?
= 11),we look at the typed dependency parse to find thepath feature between ??
and .
The relevantdependencies are: dobj(?
?, ?h), clf (?h, ()and nummod( , ).
Therefore the path feature isPATH:dobjR-clfR-nummodR.
We also use the direc-tionality: we add an R to the dependency name if it?sgoing against the direction of the arrow.3 Chinese Grammatical RelationsOur Chinese grammatical relations are designed tobe very similar to the Stanford English typed depen-dencies (de Marneffe and Manning, 2008; de Marn-effe et al, 2006).3.1 DescriptionThere are 45 named grammatical relations, and a de-fault 46th relation dep (dependent).
If a dependencymatches no patterns, it will have the most genericrelation dep.
The descriptions of the 45 grammat-ical relations are listed in Table 2 ordered by theirfrequencies in files 1?325 of CTB6 (LDC2007T36).The total number of dependencies is 85748, andother than the ones that fall into the 45 grammaticalrelations, there are also 7470 dependencies (8.71%of all dependencies) that do not match any patterns,and therefore keep the generic name dep.3.2 Chinese Specific StructuresAlthough we designed the typed dependencies toshow structures that exist both in Chinese and En-glish, there are many other syntactic structures thatonly exist in Chinese.
The typed dependencies wedesigned also cover those Chinese specific struc-tures.
For example, the usage of ?{?
(DE) is onething that could lead to different English transla-tions.
In the Chinese typed dependencies, thereare relations such as cpm (DE as complementizer)or assm (DE as associative marker) that are usedto mark these different structures.
The Chinese-specific ???
(BA) construction also has a relationba dedicated to it.The typed dependencies annotate these Chinesespecific relations, but do not directly provide a map-ping onto how they are translated into English.
Itbecomes more obvious how those structures affectthe ordering when Chinese sentences are translatedinto English when we apply the typed dependenciesas features in the phrase orientation classifier.
Thiswill be further discussed in Section 4.4.3.3 Comparison with EnglishTo compare the distribution of Chinese typed de-pendencies with English, we extracted the Englishtyped dependencies from the translation of files 1?325 in the English Chinese Translation Treebank1.0 (LDC2007T02), which correspond to files 1?325in CTB6.
The English typed dependencies are ex-tracted using the Stanford Parser.There are 116,799 total English dependencies,and 85,748 Chinese ones.
On the corpus we use,there are 45 distinct dependency types (not includ-ing dep) in Chinese, and 50 in English.
The cov-erage of named relations is 91.29% in Chinese and90.48% in English; the remainder are the unnamedrelation dep.
We looked at the 18 shared relations54??
?
??
??
?
?
??
?
??
?
?
?
??
?nsubj nsubjpobj lccomp loc rcmoddobjclfnummodadvmodBeihai already become China to outside open during rising (DE) one measureword brightstar .prep cpmpunctFigure 3: A Chinese example sentence labeled with typed dependenciesbetween Chinese and English in Table 1.
Chinesehas more nn, punct, nsubj, rcmod, dobj, advmod,conj, nummod, attr, tmod, and ccomp while Englishuses more pobj, det, prep, amod, cc, cop, and xsubj,due mainly to grammatical differences between Chi-nese and English.
For example, some determinersin English (e.g., ?the?
in (1b)) are not mandatory inChinese:(1a)?
?=/import and export/total value(1b) The total value of imports and exportsIn another difference, English uses adjectives(amod) to modify a noun (?financial?
in (2b)) whereChinese can use noun compounds (??
?/finance?in (2a)).(2a)?u/Tibet??/finance?/system?
?/reform(2b) the reform in Tibet ?s financial systemWe also noticed some larger differences betweenthe English and Chinese typed dependency distribu-tions.
We looked at specific examples and providethe following explanations.prep and pobj English has much more uses of prepand pobj.
We examined the data and found threemajor reasons:1.
Chinese uses both prepositions and postposi-tions while English only has prepositions.
?Af-ter?
is used as a postposition in Chinese exam-ple (3a), but a preposition in English (3b):(3a)??/1997?
?/after(3b) after 19972.
Chinese uses noun phrases in some cases whereEnglish uses prepositions.
For example, ??-?
(period, or during) is used as a noun phrasein (4a), but it?s a preposition in English.(4a)??/1997t/to?
?/1998?- /period(4b) during 1997-19983.
Chinese can use noun phrase modification insituations where English uses prepositions.
Inexample (5a), Chinese does not use any prepo-sitions between ?apple company?
and ?newproduct?, but English requires use of either?of?
or ?from?.(5a)?*?
?/apple companyc?
?/new product(5b) the new product of (or from) AppleThe Chinese DE constructions are also oftentranslated into prepositions in English.cc and punct The Chinese sentences contain morepunctuation (punct) while the English translationhas more conjunctions (cc), because English usesconjunctions to link clauses (?and?
in (6b)) whileChinese tends to use only punctuation (?,?
in (6a)).
(6a) YJ/these?=/city??/social?
/economic0/development??/rapid??0/local?
/economic"?/strength?/clearly/enhance(6b) In these municipalities the social and economic de-velopment has been rapid, and the local economicstrength has clearly been enhancedrcmod and ccomp There are more rcmod andccomp in the Chinese sentences and less in the En-glish translation, because of the following reasons:1.
Some English adjectives act as verbs in Chi-nese.
For example, c (new) is an adjectivalpredicate in Chinese and the relation betweenc (new) and ??
(system) is rcmod.
But?new?
is an adjective in English and the En-glish relation between ?new?
and ?system?
isamod.
This difference contributes to more rc-mod in Chinese.
(7a)c/new{/(DE)X=/verify and write off(7b) a new sales verification system2.
Chinese has two special verbs (VC): 4 (SHI)and ?
(WEI) which English doesn?t use.
For55abbreviation short description Chinese example typed dependency counts percentagenn noun compound modifier q?
?e nn(?e,q?)
13278 15.48%punct punctuation 0:,??
punct(,?,?)
10896 12.71%nsubj nominal subject ??nsubj(,??)
5893 6.87%conj conjunct (links two conjuncts) ??Z?a?
conj(?a?,??)
5438 6.34%dobj direct object ???Y?
?G?G dobj(?Y,?G) 5221 6.09%advmod adverbial modifier \?
??
?G advmod(?
?,) 4231 4.93%prep prepositional modifier ?"B??Zq?
prep(q?,?)
3138 3.66%nummod number modifier ?G?G nummod(G,?) 2885 3.36%amod adjectival modifier J-??
amod(?
?,J-) 2691 3.14%pobj prepositional object ????
pobj(??,?)
2417 2.82%rcmod relative clause modifier X?t,{<Y rcmod(<Y,?t) 2348 2.74%cpm complementizer??{?
??
cpm(,{) 2013 2.35%assm associative marker ?{??
assm(?,{) 1969 2.30%assmod associative modifier ?{??
assmod(?
?,?) 1941 2.26%cc coordinating conjunction ??Z?a?
cc(?a?,Z) 1763 2.06%clf classifier modifier ?G?G clf(?G,G) 1558 1.82%ccomp clausal complement Uq??
Rzf~??
ccomp(?
?,Rz) 1113 1.30%det determiner YJ?
??
det(?
?,YJ) 1113 1.30%lobj localizer object ?#u lobj(u,?#) 1010 1.18%range dative object that is a quantifier phrase ?b ?7??
range(?b,?)
891 1.04%asp aspect marker ??
*~ asp(?,?)
857 1.00%tmod temporal modifier 1X?t, tmod(?t,1) 679 0.79%plmod localizer modifier of a preposition ?Y?yH?
plmod(?,?)
630 0.73%attr attributive ?4??7??
attr(?,??)
534 0.62%mmod modal verb modifier ?Czt?F mmod(zt,) 497 0.58%loc localizer 3??1?
loc(3,1?)
428 0.50%top topic O?4???
top(4,O?)
380 0.44%pccomp clausal complement of a preposition ??\???
pccomp(?,??)
374 0.44%etc etc modifier )?s?
etc(?s,) 295 0.34%lccomp clausal complement of a localizer ?)?i8?
?
{?h lccomp(?,8) 207 0.24%ordmod ordinal number modifier ?????
ordmod(?,??)
199 0.23%xsubj controlling subject Uq??
Rzf~??
xsubj(Rz,Uq) 192 0.22%neg negative modifier 1X?t, neg(?t,X) 186 0.22%rcomp resultative complement ????
rcomp(??,??)
176 0.21%comod coordinated verb compound modifier ?Y"q comod(?Y,"q) 150 0.17%vmod verb modifier ??|?i??0?
{*~ vmod(0?,|?)
133 0.16%prtmod particles such as?,1,u,????Rz{??
prtmod(Rz,?)
124 0.14%ba ?ba?
construction ?????5=?
ba(?5,?)
95 0.11%dvpm manner DE(?)
modifier ?H?3? dvpm(?H,?)
73 0.09%dvpmod a ?XP+DEV(?)?
phrase that modifies VP ?H?3? dvpmod(3?,?H) 69 0.08%prnmod parenthetical modifier ???-?
1990 ?
1995?
prnmod(?-, 1995) 67 0.08%cop copular ?4?{?
cop(?,4) 59 0.07%pass passive marker ????
?b? pass(??,?)
53 0.06%nsubjpass nominal passive subject 1??*S?{?
?
nsubjpass(?
*,1) 14 0.02%Table 2: Chinese grammatical relations and examples.
The counts are from files 1?325 in CTB6.example, there is an additional relation, ccomp,between the verb4/(SHI) and\?/reduce in(8a).
The relation is not necessary in English,since4/SHI is not translated.
(8a) /second4/(SHI)??#/1996?
)/ChinaLl?/substantially\?/reduce{/tariff(8b) Second, China reduced tax substantially in1996.conj There are more conj in Chinese than in En-glish for three major reasons.
First, sometimes onecomplete Chinese sentence is translated into sev-eral English sentences.
Our conj is defined for twogrammatical roles occurring in the same sentence,and therefore, when a sentence breaks into multipleones, the original relation does not apply.
Second,we define the two grammatical roles linked by theconj relation to be in the same word class.
However,words which are in the same word class in Chinesemay not be in the same word class in English.
Forexample, adjective predicates act as verbs in Chi-nese, but as adjectives in English.
Third, certain con-structions with two verbs are described differentlybetween the two languages: verb pairs are describedas coordinations in a serial verb construction in Chi-nese, but as the second verb being the complement56of the first verb in English.4 Experimental Results4.1 Experimental SettingWe use various Chinese-English parallel corpora1for both training the phrase orientation classifier, andfor extracting statistical phrases for the phrase-basedMT system.
The parallel data contains 1,560,071sentence pairs from various parallel corpora.
Thereare 12,259,997 words on the English side.
Chi-nese word segmentation is done by the Stanford Chi-nese segmenter (Chang et al, 2008).
After segmen-tation, there are 11,061,792 words on the Chineseside.
The alignment is done by the Berkeley wordaligner (Liang et al, 2006) and then we symmetrizedthe word alignment using the grow-diag heuristic.For the phrase orientation classifier experiments,we extracted labeled examples using the paralleldata and the alignment as in Figure 2.
We extracted9,194,193 total valid examples: 86.09% of them areordered and the other 13.91% are reversed.
To eval-uate the classifier performance, we split these exam-ples into training, dev and test set (8 : 1 : 1).
Thephrase orientation classifier used in MT experimentsis trained with all of the available labeled examples.Our MT experiments use a re-implementation ofMoses (Koehn et al, 2003) called Phrasal, whichprovides an easier API for adding features.
Weuse a 5-gram language model trained on the Xin-hua and AFP sections of the Gigaword corpus(LDC2007T40) and also the English side of all theLDC parallel data permissible under the NIST08rules.
Documents of Gigaword released during theepochs of MT02, MT03, MT05, and MT06 wereremoved.
For features in MT experiments, we in-corporate Moses?
standard eight features as well asthe lexicalized reordering features.
To have a morecomparable setting with (Zens and Ney, 2006), wealso have a baseline experiment with only the stan-dard eight features.
Parameter tuning is done withMinimum Error Rate Training (MERT) (Och, 2003).The tuning set for MERT is the NIST MT06 dataset, which includes 1664 sentences.
We evaluate theresult with MT02 (878 sentences), MT03 (919 sen-1LDC2002E18, LDC2003E07, LDC2003E14,LDC2005E83, LDC2005T06, LDC2006E26, LDC2006E85,LDC2002L27 and LDC2005T34.tences), and MT05 (1082 sentences).4.2 Phrase Orientation ClassifierFeature Sets #features Train.
Acc.
Train.
Dev DevAcc.
(%) Macro-F Acc.
(%) Macro-FMajority class - 86.09 - 86.09 -Src 1483696 89.02 71.33 88.14 69.03Src+Tgt 2976108 92.47 82.52 91.29 79.80Src+Src2+Tgt 4440492 95.03 88.76 93.64 85.58Src+Src2+Tgt+PATH 4691887 96.01 91.15 94.27 87.22Table 3: Feature engineering of the phrase orientationclassifier.
Accuracy is defined as (#correctly labeled ex-amples) divided by (#all examples).
The macro-F is anaverage of the accuracies of the two classes.The basic source word features described in Sec-tion 2 are referred to as Src, and the target wordfeatures as Tgt.
The feature set that Zens and Ney(2006) used in their MT experiments is Src+Tgt.
Inaddition to that, we also experimented with sourceword features Src2 which are similar to Src, but takea window of 3 around j?
instead of j.
In Table 3we can see that adding the Src2 features increasedthe total number of features by almost 50%, but alsoimproved the performance.
The PATH features addfewer total number of features than the lexical fea-tures, but still provide a 10% error reduction and1.63 on the macro-F1 on the dev set.
We use the bestfeature sets from the feature engineering in Table 3and test it on the test set.
We get 94.28% accuracyand 87.17 macro-F1.
The overall improvement ofaccuracy over the baseline is 8.19 absolute points.4.3 MT ExperimentsIn the MT setting, we use the log probability fromthe phrase orientation classifier as an extra feature.The weight of this discriminative reordering featureis also tuned by MERT, along with other Mosesfeatures.
In order to understand how much thePATH features add value to the MT experiments, wetrained two phrase orientation classifiers with differ-ent features: one with the Src+Src2+Tgt feature set,and the other one with Src+Src2+Tgt+PATH.
The re-sults are listed in Table 4.
We compared to twodifferent baselines: one is Moses8Features whichhas a distance-based reordering model, the other isBaseline which also includes lexicalized reorder-ing features.
From the table we can see that usingthe discriminative reordering model with PATH fea-tures gives significant improvement over both base-57Setting #MERT features MT06(tune) MT02 MT03 MT05Moses8Features 8 31.49 31.63 31.26 30.26Moses8Features+DiscrimRereorderNoPATH 9 31.76(+0.27) 31.86(+0.23) 32.09(+0.83) 31.14(+0.88)Moses8Features+DiscrimRereorderWithPATH 9 32.34(+0.85) 32.59(+0.96) 32.70(+1.44) 31.84(+1.58)Baseline (Moses with lexicalized reordering) 16 32.55 32.56 32.65 31.89Baseline+DiscrimRereorderNoPATH 17 32.73(+0.18) 32.58(+0.02) 32.99(+0.34) 31.80(?0.09)Baseline+DiscrimRereorderWithPATH 17 32.97(+0.42) 33.15(+0.59) 33.65(+1.00) 32.66(+0.77)Table 4: MT experiments of different settings on various NIST MT evaluation datasets.
All differences marked in boldare significant at the level of 0.05 with the approximate randomization test in (Riezler and Maxwell, 2005).???
??
?
?detevery level productnn?
?
??
?
?
?products of all level???
??
??
?
?
??
?
?
?whole city this year industry total output valuedet nngross industrial output value of the whole city this yearFigure 4: Two examples for the feature PATH:det-nn andhow the reordering occurs.lines.
If we use the discriminative reordering modelwithout PATH features and only with word features,we still get improvement over the Moses8Featuresbaseline, but the MT performance is not signifi-cantly different from Baseline which uses lexical-ized reordering features.
From Table 4 we see thatusing the Src+Src2+Tgt+PATH features significantlyoutperforms both baselines.
Also, if we compare be-tween Src+Src2+Tgt and Src+Src2+Tgt+PATH, thedifferences are also statistically significant, whichshows the effectiveness of the path features.4.4 Analysis: Highly-weighted Features in thePhrase Orientation ModelThere are a lot of features in the log-linear phraseorientation model.
We looked at some highly-weighted PATH features to understand what kindof grammatical constructions were informative forphrase orientation.
We found that many path fea-tures corresponded to our intuitions.
For example,the feature PATH:prep-dobjR has a high weight forbeing reversed.
This feature informs the model thatin Chinese a PP usually appears before VP, but inEnglish they should be reversed.
Other featureswith high weights include features related to theDE construction that is more likely to translate toa relative clause, such as PATH:advmod-rcmod andPATH:rcmod.
They also indicate the phrases aremore likely to be chosen in reversed order.
Anotherfrequent pattern that has not been emphasized in theprevious literature is PATH:det-nn, meaning that a[DT NP1NP2] in Chinese is translated into Englishas [NP2 DT NP1].
Examples with this feature arein Figure 4.
We can see that the important featuresdecided by the phrase orientation model are also im-portant from a linguistic perspective.5 ConclusionWe introduced a set of Chinese typed dependenciesthat gives information about grammatical relationsbetween words, and which may be useful in otherNLP applications as well as MT.
We used the typeddependencies to build path features and used them toimprove a phrase orientation classifier.
The path fea-tures gave a 10% error reduction on the accuracy ofthe classifier and 1.63 points on the macro-F1 score.We applied the log probability as an additional fea-ture in a phrase-based MT system, which improvedthe BLEU score of the three test sets significantly(0.59 on MT02, 1.00 on MT03 and 0.77 on MT05).This shows that typed dependencies on the sourceside are informative for the reordering component ina phrase-based system.
Whether typed dependen-cies can lead to improvements in other syntax-basedMT systems remains a question for future research.AcknowledgmentsThe authors would like to thank Marie-Catherine deMarneffe for her help on the typed dependencies,and Daniel Cer for building the decoder.
This workis funded by a Stanford Graduate Fellowship to thefirst author and gift funding from Google for theproject ?Translating Chinese Correctly?.58ReferencesPi-Chuan Chang, Michel Galley, and Christopher D.Manning.
2008.
Optimizing Chinese word segmen-tation for machine translation performance.
In Pro-ceedings of the Third Workshop on Statistical MachineTranslation, pages 224?232, Columbus, Ohio, June.Association for Computational Linguistics.Marie-Catherine de Marneffe and Christopher D. Man-ning.
2008.
The Stanford typed dependencies repre-sentation.
In Coling 2008: Proceedings of the work-shop on Cross-Framework and Cross-Domain ParserEvaluation, pages 1?8, Manchester, UK, August.
Col-ing 2008 Organizing Committee.Marie-Catherine de Marneffe, Bill MacCartney, andChristopher D. Manning.
2006.
Generating typed de-pendency parses from phrase structure parses.
In Pro-ceedings of LREC-06, pages 449?454.Liang Huang, Kevin Knight, and Aravind Joshi.
2006.Statistical syntax-directed translation with extendeddomain of locality.
In Proceedings of AMTA, Boston,MA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proc.of NAACL-HLT.Philipp Koehn, Hieu Hoang, Alexandra Birch Mayne,Christopher Callison-Burch, Marcello Federico,Nicola Bertoldi, Brooke Cowan, Wade Shen, Chris-tine Moran, Richard Zens, Chris Dyer, Ondrej Bojar,Alexandra Constantin, and Evan Herbst.
2007.Moses: Open source toolkit for statistical machinetranslation.
In Proceedings of the 45th Annual Meet-ing of the Association for Computational Linguistics(ACL), Demonstration Session.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In Proceedings of HLT-NAACL,pages 104?111, New York City, USA, June.
Associa-tion for Computational Linguistics.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine trans-lation.
In Proceedings of HLT-NAACL.Franz Josef Och.
2003.
Minimum error rate training forstatistical machine translation.
In ACL.Stefan Riezler and John T. Maxwell.
2005.
On somepitfalls in automatic evaluation and significance test-ing for MT.
In Proceedings of the ACL Workshop onIntrinsic and Extrinsic Evaluation Measures for Ma-chine Translation and/or Summarization, pages 57?64, Ann Arbor, Michigan, June.
Association for Com-putational Linguistics.Christoph Tillman.
2004.
A unigram orientation modelfor statistical machine translation.
In Proceedings ofHLT-NAACL 2004: Short Papers, pages 101?104.Chao Wang, Michael Collins, and Philipp Koehn.
2007.Chinese syntactic reordering for statistical machinetranslation.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing (EMNLP-CoNLL), pages 737?745, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Richard Zens and Hermann Ney.
2006.
Discriminativereordering models for statistical machine translation.In Proceedings on the Workshop on Statistical Ma-chine Translation, pages 55?63, New York City, June.Association for Computational Linguistics.59
