Extracting Hyponyms of Prespecified Hypernymsfrom Itemizations and Headings in Web DocumentsKeiji Shinzato Kentaro TorisawaJapan Advanced Institute of Science and Technology,1-1 Asahidai, Tatsunokuchi-machi, Nomi-gun, Ishikawa, 923-1292 JAPAN{skeiji,torisawa}@jaist.ac.jpAbstractThis paper describes a method to acquire hy-ponyms for given hypernyms from HTML doc-uments on the WWW.
We assume that a head-ing (or explanation) of an itemization (or list-ing) in an HTML document is likely to containa hypernym of the items in the itemization, andwe try to acquire hyponymy relations based onthis assumption.
Our method is obtained by ex-tending Shinzato?s method (Shinzato and Tori-sawa, 2004) where a common hypernym for ex-pressions in itemizations in HTML documents isobtained by using statistical measures.
By us-ing Japanese HTML documents, we empiricallyshow that our proposed method can obtain a sig-nificant number of hyponymy relations whichwould otherwise be missed by alternative meth-ods.1 IntroductionHyponymy relations can play a crucial role in var-ious NLP systems, and there have been many at-tempts to develop automatic methods to acquire hy-ponymy relations from text corpora (Hearst, 1992;Caraballo, 1999; Imasumi, 2001; Fleischman et al,2003; Morin and Jacquemin, 2003; Ando et al,2003).
Most of these techniques have relied on par-ticular linguistic patterns, such as ?NP such as NP.
?The frequencies of use for such linguistic patternsare relatively low, though, and there can be many ex-pressions that do not appear in such patterns evenif we look at large corpora.
The effort of searchingfor other clues indicating hyponymy relations is thussignificant.Our aim is to extract hyponyms of prespecified hy-pernyms from the WWW.
We use itemizations (orlists) in HTML documents, such as the one in Fig-ure 1(A), and their headings (?Car Company List?
inthe figure).
In a similar attempt, Shinzato and Tori-sawa proposed an automatic method to obtain a com-mon hypernym of expressions in the same itemiza-tions in HTML documents (Shinzato and Torisawa,2004) by using statistical measures such as documentfrequencies and inverse document frequencies.
Inthe following, we call this method the Algorithm forHyponymy Relation Acquisition from Itemizations(AHRAI).
On the other hand, the method we proposein this paper is called Hyponym Extraction Algorithm?
Car Company List?
Toyota?
Honda?
Nissan?
Car List?
Toyota?
Honda?
Nissan(A) (B)Figure 1: Examples of itemizationfrom Itemizations and Headings (HEAIH).The difference between AHRAI and HEAIH isthat HEAIH uses the headings attached to item-izations, while AHRAI obtains hypernyms withoutlooking at the headings.
This difference has a sig-nificant consequence in the acquisition of hyponymyrelations.
A hyponym tends to have more than onehypernym.
For instance, ?Toyota?
can have at leasttwo hypernyms, ?company?
and ?car.?
AHRAI maybe able to obtain ?company,?
for instance, from theitemizations presented in Figure 1(A), but it can-not simultaneously obtain ?car.?
Consider the item-ization in Figure 1(B).
Although the heading of theitemization says that the items in the itemizations arecars, AHRAI will provide ?company?
as a hypernymof the itemizations.
This is because AHRAI does notuse the headings as clues for finding hypernyms andthe itemizations in (A) and (B) are actually identi-cal.
Of course, the method could produce the hy-pernym ?car?
from different itemizations; this is un-likely, though, because the itemizations suggestingthat ?Toyota?
is a ?car?
are likely to again includethe names of other car manufactures such as ?Nis-san?
and ?Honda,?
so the itemization must be moreor less similar to the ones in the figure.
In such situa-tions, the procedure is likely to consistently produce?company?
instead of ?car.
?On the other hand, HEAIH can simultaneouslyrecognize ?Toyota?
as a hyponym of the two hy-pernyms by using the headings.
Given a set of hy-pernyms, for which we?d like to acquire their hy-ponyms, HEAIH finds the headings (or, more pre-cisely, candidates of headings) that include the givenhypernyms, and extracts the itemizations which arelocated near the headings.
The procedure then pro-duces hyponymy relations under the assumption thatthe expressions in the itemizations are hyponyms ofthe given hypernym.
For example, given ?car?
and?car company?
as hypernyms, the procedure findsdocuments including headings such as ?Car Com-pany List?
and ?Car List.?
If it is lucky enough, itfinds documents such as those in Figure 1, and ex-tracts the expressions ?Toyota?
?Honda,?
and ?Nis-san?
from the itemizations near the headings.
It willthen obtain that ?Toyota?
is a hyponym of ?car com-pany?
from document (A) in the figure, while it findsthat ?Toyota?
is a hyponym of ?car?
from (B).However, the task is not that simple.
A problem isthat we do not know how to identify the correspon-dence between itemizations and their headings pre-cisely.
One may think that, for instance, she can usethe distance between an itemization and (candidatesof) its heading in the HTML file as a clue for findingthe correspondence.
However, we empirically showthat this is not the case.
To solve this problem, weused a modified version of AHRAI.
This method canproduce a ranked list of hypernym candidates fromthe itemizations only.
We assume that if the top n el-ements of a ranked list produced by AHRAI includemany substrings of a given hypernym, the headingincluding the hypernym is attached to the itemiza-tion.Note that the original AHRAI produced the topelement of the ranked list as a hypernym, whileHEAIH recognizes a string as a hypernym if its sub-strings are included in the top n elements in theranked list.
This helps the HEAIH to acquire hy-ponymy relations that the AHRAI cannot.
Considerthe itemizations in Figure 1.
AHRAI may produce?company?
as the top element of a ranked list forboth (A) and (B).
But if ?car?
is in the top n ele-ments in the list as well, HEAIH can recognize ?car?as a hypernym for (B).This paper is organized as follows.
Section 2 de-scribes AHRAI.
Our proposed method, HEAIH, ispresented in Section 3.
The experimental results ob-tained by using Japanese HTML documents are pre-sented in Section 4, where we compared our methodand alternative methods.2 Previous Work: AHRAIThe Algorithm for Hyponymy Relation Acquisitionfrom Itemization (AHRAI) acquires hyponymy rela-tions from HTML documents according to three as-sumptions.Assumption A Expressions included in the sameitemization or listing in an HTML document arelikely to have a common hypernym.Assumption B Given a set of hyponyms that havea common hypernym, the hypernym appears inmany documents that include the hyponyms.Assumption C Hyponyms and their hypernyms aresemantically similar.We call expressions in an itemization hyponymcandidates.
A set of the hyponym candidates ex-tracted from a single itemization or list is called ahyponym candidate set (HCS).
For the itemizationin Figure 1 (A), we would treat Toyota, Honda, andNissan as hyponym candidates, and regard them asmembers of the same HCS.The procedure consists of the following four steps.Note that Steps 1-3 correspond to Assumptions A-C.Step 1 Extraction of hyponym candidates fromitemized expressions in HTML documents.Step 2 Selection of a hypernym candidate by us-ing document frequencies and inverse docu-ment frequencies.Step 3 Ranking of hypernym candidates and HCSsbased on semantic similarities between hyper-nym and hyponym candidates.Step 4 Application of a few additional heuristics toelaborate computed hypernym candidates andhyponym candidates.Step 1 is performed by using a rather simple al-gorithm operating on HTML tags.
See Shinzato andTorisawa, 2004, for more details.
The other steps aredescribed in the following.2.1 Step 2In Step 2, the procedure selects a common hyper-nym candidate for an HCS.
First, two sets of doc-uments are prepared.
The first set of documents isa large number of HTML documents that are ran-domly selected and downloaded.
This set of doc-uments is called a global document set, and is as-sumed to indicate the general tendencies of word fre-quencies.
Then the procedure downloads the docu-ments including each hyponym candidate in a givenHCS by using an existing search engine 1.
This doc-ument set is called a local document set, and is usedto determine the strength of the association of nounswith the hyponym candidates.Let us denote a given HCS as C, a local documentset obtained from all the items in C as LD(C), anda global document set as G. N is a set of the nounsthat can be hypernym candidates2 A hypernym can-didate, denoted as h(C), for C is obtained throughthe following formula.h(C) = argmaxn?N{hS(n,C)}hS(n,C) = df(n,LD(C)) ?
idf(n,G)df(n,D) is a document frequency, which is actuallythe number of documents including a noun n in adocument set D. idf(n,G) is an inverse documentfrequency, which is defined as log (|G|/df(n,G)).1As in Shinzato and Torisawa, 2004, we used the search en-gine ?goo.?
(http://www.goo.ne.jp).
Note that we enclosed thestrings to be searched by ??
so that the engine does not split themto words automatically.2We simply used the most frequent nouns observed in a largecorpora as N .The score hS has a large value for a noun that ap-pears in a large number of documents in the localdocument set and is found in a relatively small num-ber of documents in the global document set.
Thisreflects Assumption B given above.2.2 Step 3By Step 2, the procedure can produce pairs of a hy-pernym candidate and an HCS, which are denoted by{?h(Ci), Ci?}mi=1.
Here, Ci is an HCS, and h(Ci) isa common hypernym candidate for hyponym candi-dates in an HCS Ci.In Step 3, the similarity between hypernym candi-dates and hyponym candidates is considered to ex-clude non-hypernyms that are strongly associatedwith hyponym candidates from the hypernym can-didates obtained by h(C), according to AssumptionC.
For instance, non-hypernym ?price?
may be avalue of h({Toyota,Honda}) because it is stronglyassociated with the words Toyota and Honda inHTML documents.
Such non-hypernyms are ex-cluded based on the assumption that non-hypernymshave relatively low semantic similarities to the hy-ponym candidates, while the behavior of true hy-pernyms should be semantically similar to the hy-ponyms.
In the ?price?
example, the similarity be-tween ?price?
and ?Toyota?
is relatively low, and wecan expect that ?price?
is excluded from the output.The semantic similarities between hyponym can-didates in an HCS C and a hypernym candidate nare computed using a cosine measure between co-occurrence vectors:sim(n,C) = (ho(C) ?
hy(n))/(|ho(C)||hy(n)|).Here, ho(C) denotes a co-occurrence vector of hy-ponym candidates, while hy(n) is the co-occurrencevector of a hypernym candidate n. Assume thatall possible argument positions are denoted as{p1, ?
?
?
, pl} and {v1, ?
?
?
, vo} denotes a set of verbs.Then, the above vectors are defined as follows.ho(C) = ?fh(C, p1, v1), ?
?
?
, fh(C, pl, vo)?hy(n) = ?f(n, p1, v1), ?
?
?
, f(n, pl, vo)?Here, fh(C, p, v) denotes the frequency of the hy-ponym candidates in an HCS C occupying an argu-ment position p of a verb v in a local document setand f(n, p, v) is the frequency of a noun n occupyingan argument position p of a verb v in a large docu-ment set.The procedure sorts the hypernym-HCS pairs{?h(Ci), Ci?
}mi=1 using the valuesim(h(Ci), Ci) ?
hS(h(Ci), Ci).Then, the top elements of the sorted pairs are likelyto contain a hypernym candidate and an HCS thatare semantically similar to each other.
The final out-put of AHRAI is the top k pairs in this ranking af-ter some heuristic rules are applied to it in Step 4.Rule 1 If the number of documents that include a hypernymcandidate is less than the sum of the numbers of the documentsthat include an item in the HCS, then discard both the hypernymcandidate and the HCS from the output.Rule 2 If a hypernym candidate appears as substrings of anitem in its HCS and it is not a suffix of the item, then discardboth the hypernym candidate and the HCS from the output.
Ifa hypernym candidate is a suffix of its hyponym candidate, thenhalf of the members of an HCS must have the hypernym can-didate as their suffixes.
Otherwise, discard both the hypernymcandidate and its HCS from the output.Rule 3 If a hypernym candidate is an expression belongingto the category of place names, then replace it by ?place name.
?Recognition of place names was done by an existing morpho-logical analyzer.Figure 2: Heuristic rules of AHRAIIn other words, the procedure discards the remain-ing m ?
k pairs in the ranking because they tend toinclude erroneous hypernyms.2.3 Step 4The steps described up to now can produce a hy-pernym for hyponym candidates with a certain pre-cision.
However, Shinzato et al reported that therules shown in Figure 2 can contribute to higher ac-curacy.
In general, we can expect that a hypernym isused in a wider range of contexts than those of its hy-ponyms, and that the number of documents includ-ing the hypernym candidate should be larger than thenumber of web documents including hyponym can-didates.
This justifies Rule 1.
Rule 2 is effectivesince Japanese is a head final language, and seman-tic head of a complex noun phrase is the last noun.Rule 3 was justified by the observation that when aset of place names is given as an HCS, the proceduretends to produce the name of the region that includesall the places designated by the hyponym candidates.
(See Shinzato and Torisawa, 2004 for more details.
)Recall that in Step 3, the ranked pairs of an HCSand its common hypernym are obtained.
By apply-ing the above rules to these, some pairs are removedfrom the ranked pairs, or are modified.
For somegiven integer k, the top k pairs of the obtained rankedpairs become the final output of our procedure, asmentioned before.3 Proposed Method: HEAIHOur proposed method, Hyponym Extraction Algo-rithm from Itemizations and Headings (HEAIH),is obtained by using some steps of AHRAI.
TheHEAIH procedure is given a set of l hypernyms, de-noted by X = {xi}li=1, where xi is a hypernym,and finds hyponyms for the hypernyms.
The basicbehavior of the HEAIH is summarized as follows.First, it downloads the documents which are likely tocontain itemizations consisting of hyponyms of thegiven hypernyms.
This is done by generating possi-ble headings or explanations of the itemizations byusing prespecified linguistic patterns and by search-?X(?)???
(table of X) ?X(?)????
(guide to X)?X(?)?????
(category of X) ?X(?)????
(list of X)?X(?)???
(vote to X) ?X(?)?????
(menu of X)?X(?)??????
(ranking of X)X is a place holder that a given hypernym fills in.Figure 3: Patterns for generating headingsing the documents including the expressions with anexisting search engine.
Second, the procedure ap-plies Steps 1 and 2 of AHRAI and computes a rankedlist of hypernym candidates for each HCS extractedfrom the itemizations in the downloaded documents.The list is ranked in descending order of the hS scorevalues.
Note that the ranked list is generated inde-pendently from a given hypernym.We assume that a given hypernym is likely to bea true hypernym if the top elements of the rankedlist of hypernym candidates contain many substringsof the hypernym.
The procedure computes a scorevalue, which is designed so that it has a large valuewhen many substrings of the given hypernym areincluded in the list.
Then, the pairs of a given hy-pernym and a corresponding HCS are sorted by thescore value, and only the top k pairs are provided asthe output of the whole procedure.More precisely, HEAIH consists of Steps A-E,each of which are described below.Step A For each of the given hypernyms, denotedby xi, generate a set of strings which are typicallyused in headings, such as ?List of xi,?
by using theprespecified patterns listed in Figure 3.
The set ofgenerated strings for a hypernym xi is denoted byHd(xi).
Give each string in Hd(xi) to an existingsearch engine and pick up a string that has the maxi-mum hit count in Hd(xi).
Then, download the doc-uments in the ranking produced by the engine for thepicked up string.
In our experiments, we downloadedthe top 25 documents for each hypernym if the rank-ing contained more than 25 documents.
Otherwise,all the documents were downloaded.Step B Identify the itemizations in the downloadeddocuments and extract the expressions in them by us-ing Step 1 of AHRAI.
The results obtained in thisstep are denoted by B(X) = {?x?h, Ch?
}mh=1, wherex?h is one of the given hypernyms and Ch is an HCSextracted from a document downloaded for x?h.Step C Apply Step 2 of AHRAI to each HCSCh such that ?x?h, Ch?
?
B(X), and then obtain aranked list that contains the top p words according tothe hS values and is ranked by the values.
We denotethe list as HCList(Ch).Step D Sort the set B(X) = {?x?h, Ch?
}mh=1 in de-scending order of the hSC value, which is given be-low.hSC(x?h, Ch) = sim(x?h, Ch)?
?pj=1{sub(x?h, jth(HCList(Ch), j))?hS(jth(HCList(Ch), j), Ch)}jth(list, j) denotes the j-th element in list andsub(x, y) ={1 if y is a substring of x0 otherwise.In short, the score hSC is the sum of the score valueshS for the substrings of a given hypernym that wascontained in the top p elements of the ranked list pro-duced by Step 2 of AHRAI.
In our experiments, weassumed p = 10.
In addition, the score is weightedby the similarity measure sim(x,C)3.Step E Apply Rules 1 and 2 of AHRAI to each el-ement of the sorted list obtained in Step D, and pro-duce the top k pairs that survived the check by therules as the final output.
In our experiments, we as-sumed k = 200, while we obtained B(X) consistingof 2,034 pairs.Note that the weighting factor sim(x,C) in hSCcontributed to high accuracy in our experiments us-ing a development set.4 Experimental ResultsTo evaluate our procedure, we had to provide a set ofproper hypernyms for which HEAIH would find hy-ponyms.
This was a rather difficult task.
There aremany nouns that cannot be hypernyms.
We assumedthat the Japanese noun sequences or nouns that occu-pied the position of X in the patterns ?X???
(tableof X) ?X????
(guide to X) ????
X?
(succes-sive (or chronological list of) X) and ???X?
(well-known X) in corpora were appropriate as hypernyms.
(Despite this filtering, there were some inappropri-ate hypernyms in the set of hypernyms subjected tothe procedures in our experiments.
These inappro-priate hypernyms included expressions whose hy-ponyms change drastically according to the situa-tion in which the expressions are used.
Examplesare ?recommended products.?
One cannot determinethe possible hyponyms without knowing who is rec-ommending.
We judged any hyponymy relations in-cluding such hypernyms as being unacceptable.
)We downloaded 1.00?
106 Japanese HTML doc-uments (1.26 GB without tags), applied the abovepatterns and found 8,752 expressions.
Then, we ran-domly picked out 100 hypernym candidates from869 expressions that occurred with the above pat-terns more than three times, and 100 hypernym can-didates from the remaining 7,883 expressions.
These200 hypernym candidates became the input for ourprocedure.
As mentioned, we downloaded a maxi-mum of 25 pages for each hypernym, and extracted3In HEAIH, the hypernym x may not be included in the setof nouns for which we obtained a co-occurrence vector since x issimply given to the procedure from outside, and the proceduremay not be able to compute the sim values.
In that case, wereplace x with the longest suffix of x that is contained in the setof nouns for which co-occurrence vectors were obtained.
Thehead final characteristic of the Japanese language justifies thisreplacement.???
(laboratories, 34)*, ????
(health food/beverage, 18)*, ????
(welfare facilities, 13)*, ??
(functionalities, 12), ????
(parks in cities, 10)*,?
(stores/shops, 10)*,??
(emperors, 7)*,??
(districts, 6)*, ??
(businesses, 6), ??
(legacies, 6)*, ??????
(offered products, 5),????
(participant companies, 5),??
(worksof art, 5)*, ???
(parts of machines, 5), ??????
(Japan?s topthree something, 4), ??
(novels, 4)*, ???
(club activities, 3)*, ?????
(fortune telling websites, 3)*,????
(rules of business, 3),???????
(time attack, 3), ????
(commands, 3), ????
(recommended products, 2), ???
(producers,2), ?
(poems, 2)*, ?
(cities or markets, 2)*, ????
(alpine plants, 2)*, ????
(namesof teams, 2)*, ???????
(side businesses, 2), ???
(jobs, 2),??
(things, 1), ????
(Japanese versions, 1), ??
(animals, 1)*,??
(specialties, 1), ??
(introductions, 1), ???
(novelists, 1)*,??
(questions, 1), ??
(data, 1), ??????
(working at home,1), ?????
(students?
clubs, 1)*, ??
(venues, 1) ??
(namesof railway stations, 1)*, ????????
(dept.
of multimedia), ???????
(?Power Stone?
amulet, 1)*, ???
(bands/groups ofmusicians, 1)*,???
(chefs, 1) *,??????
(game programs, 1)*,???
(characters in games/movies/stories, 1)*, ????
(idols, 1)*,Figure 4: List of hypernyms in the HEAIH output3,211 itemizations from them.
(We restricted theitemizations to the ones containing less than or equalto 30 items.)
Then, we picked out 2,034 itemiza-tions and used them in our evaluation.
The choicewas made in the following manner.
First, for eachhypernym candidate, the itemizations were sorted inascending order of the distance between the occur-rence of the hypernym candidate and the itemizationin the downloaded page.
Then, the itemizations inthe top 65% were chosen for each hypernym.4.
Thisselection was made to eliminate the itemizations lo-cated extremely far from the given hypernyms andto keep the number of itemizations close to 2,000,which was the number of itemizations used in Shin-zato and Torisawa, 2004.Recall that HEAIH (and AHRAI) require two dif-ferent types of document sets: global document setsand local document sets.
As a global documentset, we used the downloaded 1.00 ?
106 HTMLdocuments used to obtain hypernyms given to theHEAIH.
As a local document set for each hyponymcandidate, we downloaded the top 100 documents inthe ranking produced by a search engine.
In addition,we used 5.72?106 Japanese HTML documents (6.27GB without tags) to obtain co-occurrence vectors tocalculate the semantic similarities between expres-sions.
To derive co-occurrence vectors, we parsedthe documents by using a downgraded version of anexisting parser (Kanayama et al, 2000) and collectedco-occurrences from the parsing results.As mentioned, we obtained 200 pairs of a hyper-nym and an HCS as the final HEAIH output.
Allthe hypernyms appearing in the output are listed inFigure 4 along with their English translations and4Particularly, when only one itemization was obtained for ahypernym, it was selected.hypernym HCS??
*?
?, *?
?, *?
?, *?
?, *?
?, *??
(emperor) (These are Chinese Emperors.)????
*????????
?,(welfare *????????
?,facilities) *???????????
(These are welfare facilities)????
*??????
?, *????
?,(health food/ *???
?, *????
?, *???
?beverage) (These are teas which are good for health.)?????
*????
?, *??????
(fortune telling *??????
?, *??????
?websites) (These are fortune telling websites.)???
???,?????,???????
(novelist) (These are novels.
)Figure 5: Examples of the acquired hyponymy rela-tionsthe number of HCSs that the procedure producedwith the hypernym.
In the 200 pairs, 48 hypernymsappeared.
The HCSs were taken from 119 distinctwebsites, and the maximum number of the HCSstaken from a single site was 7.
The resulting pairsof hypernym candidates and hyponym candidateswere checked by the authors according to thedefinition of the hypernym given in Miller et al,1990; i.e., we checked if the expression ?a hyponymcandidate is a kind of a hypernym candidate.?
isacceptable.
Figure 5 shows some examples of thehypernym-HCS pairs that were obtained by HEAIH.A hyponym candidates in the HCSs is marked by?*?
if it is a proper hyponym of the hypernym inthe pair.
We then computed the precision, whichwas the ratio of correct hypernym-hyponym pairsagainst all the pairs obtained from the top 200 pairsof an HCS and its hypernym candidate.
The graphin Figure 6 plots the precision obtained by HEAIH,along with the precisions of the alternative methodsas we explain later.
The x-axis of the graph indicatesthe number of hypernym-hyponym pairs obtainedfrom the top j pairs of an HCS and its hypernymcandidate, while the y-axis indicates the precision.More precisely, the curve plots the points denoted by?
?jh=1 |Ci|, (?jh=1 correct(Ch, x?h))/(?jh=1 |Ch|)?,where the output of the HEAIH is denoted by{?x?h, Ch?
}200h=1 and 1 ?
j ?
200. correct(Ch, x?h)indicates the number of hyponym candidates in Chthat are true hyponyms of the hypernym x?h.We compared the performances of the followingfive alternative methods with that of HEAIH.Alternative 1 Produce pairs consisting of a givenhypernym and a hyponym candidate in an HCS if thegiven hypernym is a suffix of the hyponym candi-date.
Note that Japanese is a head final language andthat suffixes of hyponym candidates are good candi-dates to be hypernyms.Alternative 2 Extract hyponymy relations by ap-plying lexicosyntactic patterns to the documents inthe local document sets for our method.
We used0204060801000  500  1000  1500  2000precision[%]# of hypernym hyponym pairsHEAIHAlternative 1Alternative 2Alternative 3Alternative 4Alternative 5Figure 6: Precision of hyponymy relationshypernym?hyponym?,hyponym, .*???
.
* hypernym,hyponym .*????
.
* hypernym,hyponym .*???
.
* hypernym,hyponym .*??
(?|?)?
hypernym,hyponym .*?????
.
* hypernym,hyponym .*?
(?
|?)?
.
* hypernym,hyponym .
* (?
|??)
.
* hypernymThe hypernym and hyponym may be bracketed by?
?or ?
?.Figure 7: Lexicosyntactic patternspatterns proposed in previous work (Imasumi, 2001;Ando et al, 2003) (Figure 7).
Note that these are reg-ular expressions and may overgenerate hyponymyrelations; however, they do not miss the relations ac-quired through more sophisticated methods such asthose with parsers.Alternative 3 Extract hyponymy relations by look-ing for lexicosyntactic patterns with an existingsearch engine.
The patterns used were basicallythe same as those used in Alternative 2.
However,the expression ?.*?
was eliminated from the pat-terns and the disjunctions ?|?
were expanded to sim-ple strings since the engine would not accept regu-lar expressions.
In addition, the pattern ?hypernym?hyponym??
was not used because the brackets ????
were not treated properly by the engine.Alternative 4 Original AHRAI.Alternative 5 Produce hypernym-hyponym pairsaccording to only the distance between the headingsincluding the hypernym and the itemizations includ-ing HCSs.
Recall that Hd(x) is the set of stringslikely to be headings of itemizations for a given hy-pernym x.
This alternative method computes the dis-tance in bytes between the position of a member ofHd(x) in a downloaded document and the positionof the itemization including an HCS.
The pairs of anitemization and a given hypernym are then sorted ac-cording to this distance to produce the 200 pairs withthe smallest distance as pairs of hypernyms and thecorresponding HCSs.
Note that we assumed a head-ing must appear before an HCS.0204060801000  500  1000  1500  2000  2500precision[%]# of hypernym hyponym pairsHEAIHAHRAI (full)HEAIH (restricted)Alternative 4Figure 8: Comparison between HEAIH and AHRAIWe checked if the above alternatives can acquirethe correct pairs of a hypernym and a hyponym ob-tained by HEAIH.
In other words, we counted howmany correct pairs produced by HEAIH were alsoacquired by the alternatives when using the samedocument set.
Note that all the alternative methodsexcept for Alternative 5 were applied only to the 200pairs of a hypernym and an HCS that were the finalHEAIH output.
The results are presented in Figure 6.The curves indicate the ratios of correct hyponymyrelations that are acquired by an alternative againstall the relations produced by HEAIH.
As for Alterna-tives 1-4, we plotted the graph assuming the pairs ofhypernym candidates and hyponym candidates weresorted in the same order as the order obtained by ourprocedure.
In the case of Alternative 5, the 2,034pairs of a hypernym candidates and an HCS, whichwere the results of Step B in HEAIH, were sorted ac-cording to the distance between headings and item-izations, and only the top 200 pairs were produced asthe final output.
The results suggest that our methodcan acquire a significant number of hyponymy rela-tions that the alternatives miss.We then conducted a fairer comparison betweenHEAIH and Alternative 4 (or AHRAI).
There aresome hypernyms that can never be produced byAHRAI since these hypernyms are not considered inAHRAI.
Recall that we computed the score hS forthe nouns in a set N , which contained the 155,345nouns most frequently observed in the downloaded5.72?
106 documents in our experiments.
If a givenhypernym was not included in N , AHRAI could notproduce that hypernym.
In addition, some of thegiven hypernyms are actually noun sequences (orcomplex nouns) and cannot be members of N .
Onthe other hand, HEAIH can acquire a hypernym notincluded in N if the hypernym contains substringsincluded in N .
Thus, we also compared the per-formance under the assumption that only the hyper-nyms included in N could be true hypernyms.
Theresults are presented in Figure 8.
?Alternative 4?refers to the performance of AHRAI, while ?HEAIH(restricted)?
indicates the performance of HEAIH0204060801000  200  400  600  800precision[%]# of hypernym hyponym pairsHEAIHAlternative 1Alternative 2Alternative 3Alternative 4Alternative 5Figure 9: Comparison with balanced datawhen the produced hypernyms were restricted to themembers of N .
They show that HEAIH still out-performed AHRAI.
In addition, the curve ?AHRAI(full)?
shows the performance of AHRAI when weaccept the hypernyms that were not given to theHEAIH and all the 2,034 pairs of a hypernym candi-date and an HCS were sorted according to the origi-nal score for AHRAI to produce the top 200 pairs.
Inthis case, AHRAI outperformed HEAIH, though thedifference is small.In the next set of experiments, we comparedHEAIH and Alternatives 1-5 in a slightly differentsetting.
Recall that Figure 4 gave the list of hy-pernyms in the HEAIH output and the number ofHCSs that the procedure produced with each hyper-nym.
The data was not balanced very evenly.
Whilethe procedure found 34 HCSs for laboratories, it pro-vided only one HCS for animals.
We tried to reeval-uate these methods by using more balanced data.From the data, we eliminated the pairs of a hyper-nym and an HCS that were not included in the topfive for each hypernym in the ranking of the HEAIHoutput.
In other words, each hypernym could have amaximum of only five HCSs in the evaluation data.This reduced the influence by dominant hypernyms.In addition, we removed problematic hypernymsfrom the evaluation data.
The preserved hypernymsare marked by ?*?
in Figure 4.
We preserved only thehypernyms that could have proper nouns, names ofspecies, or trade names as their hyponyms.5 In addi-tion, there are inappropriate hypernyms such as thosefor which we could not determine their hyponymswithout knowing the situation in which the hyper-nyms are used, as mentioned before.
We eliminated5Evidently, this condition was more restrictive than we ex-pected with regard to hypernyms, and some intuitively accept-able hypernyms were not preserved.
Examples are ?jobs?
and?business?
(For their Japanese translation, we could not find hy-ponyms which were either proper nouns, names of species, ortrade names).
We made this restriction simply to keep the condi-tion simple and to reduce borderline cases of proper hypernyms.Note that some of the eliminated hypernyms, such as ?jobs?
and?business?, were treated as proper hypernyms in the first com-parison in Figure 6.such hypernyms too.
We also removed ?things?
be-cause it was too general.
As a result of these changes,the evaluation data contained 73 pairs of a hyper-nym and an HCS.
The comparison using this datais shown in Figure 9.
HEAIH still acquired a largenumber of correct hyponymy relations that the alter-native methods miss.5 ConclusionsWe have presented a new method for acquiring hy-ponyms for prespecified hypernyms by using item-izations and their headings (or explanations.)
Thismethod was developed by modifying Shinzato?s al-gorithm to find hypernyms from itemizations inHTML documents.
The method could find a largenumber of hyponymy relations that alternative meth-ods, including the original Shinzato algorithm, couldnot.ReferencesMaya Ando, Satoshi Sekine, and Shun Ishizaki.2003.
Automatic extraction of hyponyms fromnewspaper using lexicosyntactic patterns.
In IPSJSIG Technical Report 2003-NL-157, pages 77?82.in Japanese.Sharon A. Caraballo.
1999.
Automatic construc-tion of a hypernym-labeled noun hierarchy fromtext.
In Proceedings of 37th Annual Meeting of theAssociation for Computational Linguistics, pages120?126.Michael Fleischman, Eduard Hovy, and AbdessamadEchihabi.
2003.
Offline strategies for onlinequestion answering: Answering questions beforethey are asked.
In Proceedings of the 41st AnnualMeeting of the Association for Computational Lin-guistics, pages 1?7.Marti A. Hearst.
1992.
Automatic acquistition ofhyponyms from large text corpora.
In Proceed-ings of the 14th International Conference on Com-putational Linguistics, pages 539?545.Kyosuke Imasumi.
2001.
Automatic acquisitionof hyponymy relations from coordinated nounphrases and appositions.
Master?s thesis, KyushuInstitute of Technology.Hiroshi Kanayama, Kentaro Torisawa, Yutaka Mit-suishi, and Jun?ichi Tsujii.
2000.
A hybridJapanese parser with hand-crafted grammar andstatistics.
In Proceedings of COLING 2000, pages411?417.Emmanuel Morin and Christian Jacquemin.
2003.Automatic acquisition and expansion of hyper-nym links.
In Computer and the Humanities 2003.forthcoming.Keiji Shinzato and Kentaro Torisawa.
2004.
Acquir-ing hyponymy relations from web documents.
InProceedings of HLT-NAACL 2004, pages 73?80.
