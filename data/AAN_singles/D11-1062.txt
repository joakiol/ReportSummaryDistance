Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 670?679,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsDivide and Conquer:Crowdsourcing the Creation of Cross-Lingual Textual Entailment CorporaMatteo NegriFBK-irstTrento, Italynegri@fbk.euLuisa BentivogliFBK-irstTrento, Italybentivo@fbk.euYashar MehdadFBK-irst and University of TrentoTrento, Italymehdad@fbk.euDanilo GiampiccoloCELCTTrento, Italygiampiccolo@celct.itAlessandro MarchettiCELCTTrento, Italyamarchetti@celct.itAbstractWe address the creation of cross-lingual tex-tual entailment corpora by means of crowd-sourcing.
Our goal is to define a cheap andreplicable data collection methodology thatminimizes the manual work done by expertannotators, without resorting to preprocess-ing tools or already annotated monolingualdatasets.
In line with recent works empha-sizing the need of large-scale annotation ef-forts for textual entailment, our work aims to:i) tackle the scarcity of data available to trainand evaluate systems, and ii) promote the re-course to crowdsourcing as an effective wayto reduce the costs of data collection withoutsacrificing quality.
We show that a complexdata creation task, for which even experts usu-ally feature low agreement scores, can be ef-fectively decomposed into simple subtasks as-signed to non-expert annotators.
The resultingdataset, obtained from a pipeline of differentjobs routed to Amazon Mechanical Turk, con-tains more than 1,600 aligned pairs for eachcombination of texts-hypotheses in English,Italian and German.1 IntroductionCross-lingual Textual Entailment (CLTE) has beenrecently proposed by (Mehdad et al, 2010; Mehdadet al, 2011) as an extension of Textual Entailment(Dagan and Glickman, 2004).
The task consists ofdeciding, given a text (T) and an hypothesis (H) indifferent languages, if the meaning of H can be in-ferred from the meaning of T. As in other NLP appli-cations, both for monolingual and cross-lingual TE,the availability of large quantities of annotated datais an enabling factor for systems development andevaluation.
Until now, however, the scarcity of suchdata on the one hand, and the costs of creating newdatasets of reasonable size on the other, have repre-sented a bottleneck for a steady advancement of thestate of the art.In the last few years, monolingual TE corpora forEnglish and other European languages have beencreated and distributed in the framework of sev-eral evaluation campaigns, including the RTE Chal-lenge1, the Answer Validation Exercise at CLEF2,and the Textual Entailment task at EVALITA3.
De-spite the differences in the design of the tasks, allthe released datasets were collected through simi-lar procedures, always involving expensive manualwork done by expert annotators.
Moreover, in thedata creation process, large amounts of hand-craftedT-H pairs often have to be discarded in order to re-tain only those featuring full agreement, in terms ofthe assigned entailment judgements, among multipleannotators.
The amount of discarded pairs is usuallyhigh, contributing to increase the costs of creatingtextual entailment datasets4.The issues related to the shortage of datasets andthe high costs for their creation are more evident1http://www.nist.gov/tac/2011/RTE/2http://nlp.uned.es/clef-qa/ave/3http://www.evalita.it/2009/tasks/te4For instance, in the first five RTE Challenges, the aver-age effort needed to create 1,000 pairs featuring full agreementamong 3 annotators was around 2.5 person-months.
Typically,around 25% of the original pairs had to be discarded during theprocess, due to low inter-annotator agreement (Bentivogli et al,2009).670in the CLTE scenario, where: i) the only datasetcurrently available is an English-Spanish corpus ob-tained by translating the RTE-3 corpus (Negri andMehdad, 2010), and ii) the application of the stan-dard methods adopted to build RTE pairs requiresproficiency in multiple languages, thus significantlyincreasing the costs of the data creation process.To address these issues, in this paper we devisea cost-effective methodology to create cross-lingualtextual entailment corpora.
In particular, we focuson the following problems:(1) Is it possible to collect T-H pairs minimizingthe intervention of expert annotators?
To addressthis question, we explore the feasibility of crowd-sourcing the corpus creation process.
As a contri-bution beyond the few works on TE/CLTE data ac-quisition, we define an effective methodology that:i) does not involve experts in the most complex (andcostly) stages of the process, ii) does not require pre-processing tools, and iii) does not rely on the avail-ability of already annotated RTE corpora.
(2) How can we guarantee good quality of the col-lected data at a low cost?
We address the qualitycontrol issue through the decomposition of a com-plex task (i.e.
creating and annotating entailmentpairs) into smaller sub-tasks.
Complex tasks are usu-ally hard to explain in a simple way understandableto non-experts, difficult to accomplish, and not suit-able for the application of the quality-check mecha-nisms provided by current crowdsourcing services.Our ?divide and conquer?
solution represents thefirst attempt to address a complex task involvingcontent generation and labelling through the defini-tion of a cheap and reliable pipeline of simple taskswhich are easy to define, accomplish, and control.
(3) Can we adapt such methodology to collectcross-lingual T-H pairs?
We tackle this questionby separating the problem of creating and annotatingTE pairs from the issues related to the multilingualdimension.
Our solution builds on the assumptionthat entailment annotations can be projected acrossaligned T-H pairs in different languages.
In thiscase, a complex multilingual task is reduced to a se-quence of simpler subtasks where the most difficultone, the generation of entailment pairs, is entirelymonolingual.
Besides ensuring cost-effectiveness,our solution allows us to overcome the problem offinding workers that are proficient in multiple lan-guages.
Moreover, since the core monolingual tasksof the process are carried out by manipulating En-glish texts, we are able to address the very largecommunity of English speaking workers, with aconsiderable reduction of costs and execution time.Finally, as a by-product of our method, the acquiredpairs are fully aligned for all language combinations,thus enabling meaningful comparisons between sce-narios of different complexity (monolingual TE, andCLTE between close or distant languages).We believe that, in the same spirit of recent workspromoting large-scale annotation efforts around en-tailment corpora (Sammons et al, 2010; Bentivogliet al, 2010), the proposed approach and the resultingdataset5 will contribute to meeting the strong needfor resources to develop and evaluate novel solutionsfor textual entailment.2 Related WorksCrowdsourcing services, such as Amazon Mechan-ical Turk6 (MTurk) and CrowdFlower7, have beenrecently used with success for a variety of NLP ap-plications (Callison-Burch and Dredze, 2010).
Theidea is that the acquisition and annotation of largeamounts of data needed to train and evaluate NLPtools can be carried out in a cost-effective mannerby defining simple Human Intelligence Tasks (HITs)routed to a crowd of non-expert workers (aka ?Turk-ers?)
hired through on-line marketplaces.As regards textual entailment, the first work ex-ploring the use of crowdsourcing services for dataannotation is described in (Snow et al, 2008), whichshows high agreement between non-expert annota-tions of the RTE-1 dataset and existing gold standardlabels assigned by expert labellers.Focusing on the actual generation of monolingualentailment pairs, (Wang and Callison-Burch, 2010)experiments the use of MTurk to collect facts andcounter facts related to texts extracted from an ex-isting RTE corpus annotated with named entities.Taking a step beyond the task of annotating exist-5The CLTE corpora described in this paper will be madefreely available for research purposes through the website ofthe funding EU Project CoSyne (http://www.cosyne.eu/).6https://www.mturk.com/7Although MTurk is directly accessible only to US citizens,the CrowdFlower service (http://crowdflower.com/) provides aninterface to MTurk for non-US citizens.671ing datasets, and showing the feasibility of involvingnon-experts also in the generation of TE pairs, thisapproach is more relevant to our objectives.
How-ever, at least two major differences with our workhave to be remarked.
First, they still use avail-able RTE data to obtain a monolingual TE corpus,whereas we pursue the more ambitious goal of gen-erating from scratch aligned CLTE corpora for dif-ferent language combinations.
To this aim, we donot resort to already annotated data, nor language-specific preprocessing tools.
Second, their approachinvolves qualitative analysis of the collected dataonly a posteriori, after manual removal of invalidand trivial generated hypotheses.
In contrast, ourapproach integrates quality control mechanisms atall stages of the data collection/annotation process,thus minimizing the recourse to experts to check thequality of the collected material.Related research in the CLTE direction is re-ported in (Negri and Mehdad, 2010), which de-scribes the creation of an English-Spanish corpusobtained from the RTE-3 dataset by translating theEnglish hypotheses into Spanish.
Translations havebeen crowdsourced adopting a methodology basedon translation-validation cycles, defined as separateHITs.
Although simplifying the CLTE corpus cre-ation problem, which is recast as the task of translat-ing already available annotated data, this solution isrelevant to our work for the idea of combining goldstandard units and ?validation HITS?
as a way tocontrol the quality of the collected data at runtime.3 Quality Control of Crowdsourced DataThe design of data acquisition HITs has to take intoaccount several factors, each having a considerableimpact on the difficulty of instructing the workers,the quality and quantity of the collected data, thetime and overall costs of the acquisition.
A majordistinction has to be made between jobs requiringdata annotation, and those involving content gener-ation.
In the former case, Turkers are presented withthe task of labelling input data referring to a fixedset of possible values (e.g.
making a choice betweenmultiple alternatives, assigning numerical scores torank the given data).
In the latter case, Turkers arefaced with creative tasks consisting in the productionof textual material (e.g.
writing a correct translation,or a summary of a given text).The ease of controlling the quality of the acquireddata depends on the nature of the job.
For annotationjobs, quality control mechanisms can be easily set upby calculating Turkers?
agreement, by applying vot-ing schemes, or by adding hidden gold units to thedata to be annotated8.
In contrast, the quality of theresults of content generation jobs is harder to assess,due to the fact that multiple valid results are accept-able (e.g.
the same content can be expressed, trans-lated, or summarized in different ways).
In such sit-uations the standard quality control mechanisms arenot directly applicable, and the detection of errorsrequires either costly manual verification at the endof the acquisition process, or more complex and cre-ative solutions integrating HITs for quality check.Most of the approaches to content generation pro-posed so far rely on post hoc verification to fil-ter out undesired low-quality data (Mrozinski et al,2008; Mihalcea and Strapparava, 2009; Wang andCallison-Burch, 2010).
The few solutions integrat-ing validation HITs address the translation of sin-gle sentences, a task that is substantially differentfrom ours (Negri and Mehdad, 2010; Bloodgood andCallison-Burch, 2010).
Compared to sentence trans-lation, the task of creating CLTE pairs is both harderto explain without recurring to notions that are dif-ficult to understand to non-experts (e.g.
?seman-tic equivalence?, ?unidirectional entailment?
), andharder to execute without mastering these notions.To tackle these issues the ?divide and conquer?
ap-proach described in the next section consists in thedecomposition of a difficult content generation jobinto easier subtasks that are: i) self-contained andeasy to explain, ii) easy to execute without any NLPexpertise, and iii) suitable for the integration of a va-riety of runtime control mechanisms (regional qual-ifications, gold units, ?validation HITs?)
able to en-sure a good quality of the collected material.8Both MTurk and CrowdFlower provide means to checkworkers?
reliability, and weed out untrusted ones without moneywaste.
These include different types of qualification mecha-nisms, the possibility of giving work only to known trustedTurkers (only with MTurk), and the possibility of adding hid-den gold standard units in the data to be annotated (offered as abuilt-in mechanism only by CrowdFlower).6724 CLTE Corpus Creation MethodologyOur approach builds on a pipeline of HITs routed toMTurk?s workforce through the CrowdFlower inter-face.
The objective is to collect aligned T-H pairsfor different language combinations, reproducing anRTE-like annotation style.
However, our annotationis not limited to the standard RTE framework, whereonly unidirectional entailment from T to H is con-sidered.
As a useful extension, we annotate any pos-sible entailment relation between the two text frag-ments, including: i) bidirectional entailment (i.e.semantic equivalence between T and H), ii) unidi-rectional entailment from T to H, and iii) unidirec-tional entailment from H to T. The resulting pairscan be easily used to generate not only standard RTEdatasets9, but also general-purpose collections fea-turing multi-directional entailment relations.4.1 Data Acquisition and AnnotationWe collect large amounts of CLTE pairs carrying outthe most difficult part of the process (the creation ofentailment-annotated pairs) at a monolingual level.Starting from a set of parallel sentences in n lan-guages, (e.g.
L1, L2, L3), n entailment corpora arecreated: one monolingual (L1/L1), and n-1 cross-lingual (L1/L2, and L1/L3).The monolingual corpus is obtained by modify-ing the sentences only in one language (L1).
Orig-inal and modified sentences are then paired and an-notated to form an entailment dataset for L1.
TheCLTE corpora are obtained by combining the mod-ified sentences in L1 with the original sentences inL2 and L3, and projecting to the multilingual pairsthe annotations assigned to the monolingual pairs.In principle, only two stages of the process re-quire crowdsourcing multilingual tasks, but do notconcern entailment annotations.
The first one, at thebeginning of the process, aims to obtain a set of par-allel sentences to start with, and can be done in dif-ferent ways (e.g.
crowdsourcing the translation ofa set of sentences).
The second one, at the end ofthe process, consists of translating the modified L1sentences into other languages (e.g.
L2) in order toextend the corpus to cover new language combina-9With the positive examples drawn from bidirectional andunidirectional entailments from T to H, and the negative onesdrawn from unidirectional entailments from H to T.tions (e.g.
L2/L2, L2/L3).The execution of the two ?multilingual?
stages isnot strictly necessary but depends on: i) the avail-ability of parallel sentences to start the process, andii) the actual objectives in terms of language combi-nations to be covered10.As regards the first stage, in this work we startedfrom a set of 467 English/Italian/German alignedsentences extracted from parallel documents down-loaded from the Cafebabel European Magazine11.Concerning the second multilingual stage, we per-formed only one round of translations from En-glish to Italian to extend the 3 combinations ob-tained without translations (ENG/ENG, ENG/ITA,and ENG/GER) with the new language combina-tions ITA/ITA, ITA/ENG, and ITA/GER.STEP1:?Sentence?modifica?on?(monolingual)?STEP3:?Transla?on?(mul?lingual)?GER?
ENG?ENG1?ITA?ITA1?
ITA?ENG?
ENG1?STEP2:?TE?annota?on?(monolingual)?Monolingual?TE?corpus?Cross-??lingual?TE?corpus?ENG1?GER?ENG1?ITA?TE?annota?ns?projec?n???ITA1?
GER?ITA1?
ENG?Figure 1: Corpus creation process.The main steps of our corpus creation process,depicted in Figure 1, can be summarized as follows:Step1: Sentence modification.
The originalEnglish sentences (ENG) are modified through(monolingual) generation HITs asking Turkers to:i) preserve the meaning of the original sentencesusing different surface forms, or ii) slightly changetheir meaning by adding or removing content.
Ourassumption, in line with (Bos et al, 2009), is that10Starting from parallel sentences in n languages, the n cor-pora obtained without recurring to translations can be aug-mented, by means of translation HITs, to create the full set oflanguage combinations.
Each round of translation adds 1 mono-lingual corpus, and n-1 CLTE corpora.11http://www.cafebabel.com/673another way to think about entailment is to considerwhether one text T1 adds new information to thecontent of another text T: if so, then T is entailed byT1.The result of this phase is a set of texts (ENG1)that can be of three types:1.
Paraphrases of the original ENG texts, that willbe used to create bidirectional entailment pairs(ENG?ENG1);2.
More specific sentences (the outcome ofcontent addition operations), used to createENG?ENG1 unidirectional entailment pairs;3.
More general sentences (the outcome ofcontent removal operations), used to createENG?ENG1 unidirectional entailment pairs.Step2: TE Annotation.
Entailment pairs com-posed of the original sentences (ENG) and the modi-fied ones (ENG1) are used as input of (monolingual)annotation HITs asking Turkers to decide which ofthe two texts contains more information.
As a re-sult, each ENG/ENG1 pair is annotated as an ex-ample of uni-/bidirectional entailment, and stored inthe monolingual English corpus.
Since the originalENG texts are aligned with the ITA and GER texts,the entailment annotations of ENG/ENG1 pairs canbe projected to the other language pairs and theITA/ENG1 and GER/ENG1 pairs are stored in theCLTE corpus.
The possibility of projecting TE an-notations is based on the assumption that the seman-tic information is mostly preserved during the trans-lation process.
This particularly holds at the deno-tative level (i.e.
regarding the truth values of thesentence) which is crucial to semantic inference.
Atother levels (e.g.
lexical) there might be slight se-mantic variations which, however, are very unlikelyto play a crucial role in determining entailment rela-tions.Step3: Translation.
The modified sentences(ENG1) are translated into Italian (ITA1) through(multilingual) generation HITs reproducing the ap-proach described in (Negri and Mehdad, 2010).
Asa result, three new datasets are produced by au-tomatically projecting annotations: the monolin-gual ITA/ITA1, and the cross-lingual ENG/ITA1 andGER/ITA1.Since the solution adopted for sentence transla-tion does not present novelty factors, the remainderof this paper will omit further details on it.
Instead,the following sections will focus on the more chal-lenging tasks of sentence modification and TE anno-tation.4.2 Crowdsourcing Sentence Modification andTE AnnotationSentence modification and TE annotation have beendecomposed into a pipeline of simpler monolingualEnglish sub-tasks.
Such pipeline, depicted in Figure2, involves several types of generation/annotationHITs designed to be easily understandable to non-experts.
Each HIT consists of: i) a set of instruc-tions for a specific task (e.g.
paraphrasing a text),ii) the data to be manipulated (e.g.
an English sen-tence), and iii) a test to check workers?
reliability.To cope with the quality control issues discussed inSection 3, such tests are realized using gold stan-dard units, either hidden in the data to be annotated(annotation HITs) or defined as test questions thatworkers must correctly answer (generation HITs).Moreover, regional qualifications are applied to allHITs.
As a further quality check, all the annotationHITs consider Turkers?
agreement as a way to filterout low quality results (only annotations featuringagreement among 4 out of 5 workers are retained).The six HITs defined for each subtask can be de-scribed as follows:1.
Paraphrase (generation).
Modify an En-glish text (ENG), in order to produce a semanticallyequivalent variant (ENG1).
As a reliability test, be-fore creating the paraphrase workers are asked tojudge if two English sentences contain the same in-formation.2.
Grammaticality (annotation).
Decide if anEnglish sentence is grammatically correct.
This val-idation HIT represents a quality check of the out-put of each generation task (i.e.
paraphrasing, andadd/remove information HITs).3.
Bidirectional Entailment (annotation).
De-cide whether two English sentences, the originalENG and the modified ENG1, contain the same in-formation (i.e.
are semantically equivalent).4a.
Add Information (generation).
Modify anEnglish text to create a more specific one by addingcontent.
As a reliability test, before generating the674Figure 2: Sentence modification and TE annotation pipeline.new sentence workers are asked to judge which oftwo given English sentences is more detailed.4b.
Remove Information (generation).
Mod-ify an English text to create a more general one byremoving part of its content.
As a reliability test, be-fore generating the new sentence workers are askedto judge which of two given English sentences is lessdetailed.5.
Unidirectional Entailment (annotation).
De-cide which of two English sentences (the originalENG, and a modified ENG1) provides more infor-mation.These HITs are combined in an iterative pro-cess that alternates text generation, grammaticalitycheck, and entailment annotation steps.
As a result,for each original ENG text we obtain multiple ENG1variants of the three types (paraphrases, more gen-eral texts, and more specific texts) and, in turn, a setof annotated monolingual (ENG/ENG1) TE pairs.As described in Section 4.1, the resulting mono-lingual English TE corpus (ENG/ENG1) is used tocreate the following mono/cross-lingual TE corpora:?
ITA/ENG1, and GER/ENG1 (by projecting TEannotations)?
ITA/ITA1, GER/ITA1, and ENG/ITA1 (bytranslating the ENG1 texts into Italian, and pro-jecting TE annotations)5 The Resulting CLTE CorporaThis section provides a quantitative and qualita-tive analysis of the results of our corpus creationmethodology, focusing on the collected ENG-ENG1monolingual dataset.
It has to be remarked that, asan effect of the adopted methodology, all the obser-vations and the conclusions drawn hold for the col-lected CLTE corpora as well.5.1 Quantitative AnalysisTable 1 provides some details about each step of thepipeline shown in Figure 2.
For each HIT the tablepresents: i) the number of items (sentences, or pairsof sentences) given in input, ii) the number of items(sentences or annotations) produced as output, iii)the number of items discarded when the agreementthreshold was not reached, iv) the number of entail-ment pairs added to the corpus, v) the time (days andhours) required by the MTurk workforce to completethe job, and vi) the cost of the job.In HIT-1 (Paraphrase) 1,414 paraphrases werecollected asking three different meaning-preservingmodifications of each of the 467 original sen-tences12.
From a practical point of view, such redun-dancy aims to ensure a sufficient number of gram-matically correct and semantically equivalent mod-ified sentences.
From a theoretical point of view,12Often, crowdsourced jobs return a number of output itemsthat is slightly larger than required, due to the labour distributionmechanism internal to MTurk.675HIT # Input items # Output items # Discarded items # Pairs to corpus MTurk time Cost ($)1.
Paraphrase 467 1,414 5d+10.5h 45.482.
Grammaticality 1,414 1,326 88 (6.22%) 1d+15h 56.883.
Bidirectional Ent.
1,326 1,213 113 (8.52%) 301 3d+2h 53.47(yes=1,205 no=8)4a.
Add Info 452 916 3d 37.024b.
Remove Info 452 923 2d+22h 29.732.
Grammaticality 1,839 1,749 90 (4.89%) 2d+5h 64.373.
Bidirectional Ent.
1,749 1,438 311 (17.78%) 148 3d+20.5h 70.52(yes=148 no=1,290)5.
Unidirectional Ent.
1,298 1,171 127 (9.78%) 1,171 8.5h 78.24(491 + 680)TOTAL 721 1,620 22d+11h 435.71Table 1: The monolingual dataset creation pipeline.collecting many variants of a small pool of origi-nal sentences aims to create pairs featuring differententailment relations with similar superficial forms.This, in principle, should allow to obtain a datasetwhich requires TE systems to focus more on deepersemantic phenomena than on the surface realizationof the pairs.The collected paraphrases were sent as input toHIT-2 (Grammaticality).
After this validation HIT,the number of acceptable paraphrases was reducedto 1,326 (with 88 discarded sentences, correspond-ing to 6.22% of the total).The retained paraphrases were paired with theircorresponding original sentences, and sent to HIT-3(Bidirectional Entailment) to be judged for semanticequivalence.
The pairs marked as bidirectional en-tailments (1,205) were divided in three groups: 25%of the pairs (301) were directly stored in the finalcorpus, while the ENG1 paraphrases of the remain-ing 75% (904) were equally distributed to the nextmodification steps.In both HIT-4a (Add Information) and HIT-4b(Remove information) two new modified sentenceswere asked for each of the 452 paraphrases receivedas input.
The sentences collected in these generationtasks were respectively 916 and 923.The new modified sentences were sent back toHIT-2 (Grammaticality) and HIT-3 (BidirectionalEntailment).
As a result 1,438 new pairs were cre-ated; out of these, 148 resulted to be bidirectionalentailments and were stored in the corpus.Finally, the 1,298 entailment pairs judged as non-bidirectional in the two previously completed HIT-3 (8+1,290) were given as input to HIT-5 (Unidi-rectional Entailment).
The pairs which passed theagreement threshold were classified according to thejudgement received, and stored in the corpus as uni-directional entailment pairs.The analysis of Table 1 allows to formulatesome considerations.
First, the percentage of dis-carded items confirms the effectiveness of decom-posing complex generation tasks into simpler sub-tasks that integrate validation HITs and qualitychecks based on non-experts?
agreement.
In fact, onaverage, around 9.5% of the generated items werediscarded without experts?
intervention13.
Second,the amount of discarded items gives evidence aboutthe relative difficulty of each HIT.
As expected,we observe lower rejection rates, corresponding tohigher inter-annotator agreement, for grammatical-ity HITs (5.55% on average) than for more complexentailment-related tasks (12.02% on average).Looking at costs and execution time, it is hardto draw definite conclusions due to several factorsthat influence the progress of the crowdsourced jobs(e.g.
the fluctuations of Turkers?
performances, thetime of the day at which jobs are posted, the dif-ficulty to set the optimal cost for a given HIT14).On the one hand, as expected, the more creative?Add Info?
task proved to be more demanding thanthe ?Remove Info?
: even though it was paid more,13Moreover, it is worthwhile noticing that around 20% of thecollected items were automatically rejected (and not paid) dueto failures on the gold standard controls created both for gener-ation and annotation tasks.14The payment for each HIT was set on the basis of a pre-vious feasibility study aimed at determining the best trade-offbetween cost and execution time.
However, replicating our ap-proach would not necessarily result in the same costs.676it still took little more time to be completed.
Onthe other hand, although the ?Unidirectional Entail-ment?
task was expected to be more difficult andthus rewarded more than the ?Bidirectional Entail-ment?
one, in the end it took notably less time tobe completed.
Nevertheless, the overall figures (435USD, and about 22.5 days of MTurk work to com-plete the process)15 clearly demonstrate the effec-tiveness of the approach.
Even considering the timeneeded for an expert to manage the pipeline (i.e.
oneweek to prepare gold units, and to handle the I/O ofeach HIT), these figures show that our methodologyprovides a cheaper and faster way to collect entail-ment data in comparison with the RTE average costsreported in Section 1.As regards the amount of data collected, the re-sulting corpus contains 1,620 pairs with the fol-lowing distribution of entailment relations: i) 449bidirectional entailments, ii) 491 ENG?ENG1 uni-directional entailments, and iii) 680 ENG?ENG1unidirectional entailments.It must be noted that our methodology does notlead to the creation of pairs where some informationis provided in one text and not in the other, and vice-versa, as Example 1 shows:Example 1.ENG: New theories were emerging in the field of psychology.ENG1: New theories were rising, which announced a kind ofveiled racism.These negative examples in both directions repre-sent a natural extension of the dataset, relevant alsofor specific application-oriented scenarios, and theircreation will be addressed in future work.Besides the achievement of our primary objec-tives, the adopted approach led to some interestingby-products.
First, the generated corpora are per-fectly suitable to produce entailment datasets simi-lar to those used in the traditional RTE evaluationframework.
In particular, considering any possibleentailment relation between two text fragments, ourannotation subsumes the one proposed in RTE cam-paigns.
This allows for the cost-effective genera-tion of RTE-like annotations from the acquired cor-15Although by projecting annotations the ENG1/ITA andENG1/GER CLTE corpora came for free, the ITA1/ITA,ITA1/ENG, and ITA1/GER combinations created by crowd-sourcing translations added 45 USD and approximately 5 daysto these figures.pora by combining ENG?ENG1 and ENG?ENG1pairs to form 940 positive examples (449+491),keeping the 680 ENG?ENG1 as negative exam-ples.
Moreover, by swapping ENG and ENG1 in theunidirectional entailment pairs, 491 additional nega-tive examples and 680 positive examples can be eas-ily obtained.Finally, the output of HITs 1-2-3 in Table 1 rep-resents per se a valuable collection of 1,205 para-phrases.
This suggests the great potential of crowd-sourcing for paraphrase acquisition.5.2 Qualitative AnalysisThrough manual verification of more than 50% ofthe corpus (900 pairs), a total number of 53 pairs(5.9%) were found incorrect.
The different errorswere classified as follows:Type 1: Sentence modification errors.
GenerationHITs are a minor source of errors, being responsiblefor 10 problematic pairs.
These errors are either in-troduced by generating a false statement (Example2), or by forming a not fully understandable, awk-ward, or non-natural sentence (Example 3).Example 2.ENG: Kosovo was the subject of major riots in 1989.ENG1: The Russian city of Kosovo was the subject of ...Example 3.ENG: Balat is the Kurdish-Armenian district of Instanbul.ENG1: Balat is a place, which is the Kurdish-Armenian ...Type 2: TE annotation errors.
The notion of con-taining more/less information, used in the ?Unidi-rectional Entailment?
HIT, can mostly be appliedstraightforwardly to the entailment definition.
How-ever, the concept of ?more/less detailed?, which gen-erally works for factual statements, in some cases isnot applicable.
In fact, the MTurk workers have reg-ularly interpreted the instructions about the amountof information as concerning the quantity of con-cepts contained in a sentence.
This is not always cor-responding to the actual entailment relation betweenthe sentences.
As a consequence, 43 pairs featur-ing wrong entailment annotations were encountered.These errors can be classified as follows:a) 13 pairs, where the added/removed informationchanges the meaning of the sentence.
In these cases,the modified sentence was judged more/less specific677than the original one, leading to unidirectional en-tailment annotation.
On the contrary, in terms ofthe standard entailment definition, the correct anno-tation is ?no entailment?
(as in Example 4, whichwas annotated as ENG?ENG1):Example 4.ENG: If you decide to live in Bulgaria, you have to likedifficulties because they are not difficulties, they are challenges.ENG1: You have to like difficulties as they are not difficulties,they are challenges.b) 10 pairs where the incorrect annotation is due toa coreference problem, as in:Example 5.ENG: John Smith is the new CEO of the company.ENG1: He is the new CEO of the company.These pairs were labelled as unidirectional entail-ments (in the example above ENG?ENG1), underthe assumption that a proper name is more specificand informative than a pronoun.
However, adher-ing to the TE definition, co-referring expressions areequivalent, and their realization does not play anyrole in the entailment decision.
This implies that thecorrect entailment annotation is ?bidirectional?.c) 9 pairs where the sentences are semanticallyequivalent, but contain a piece of information whichis explicit in one sentence, and implicit in the other.In these cases, Turkers judged the sentence contain-ing the explicit mention as more specific, and thusthe pair was annotated as unidirectional entailment.Example 6.ENG: I hear the click of the trigger and the burst of bulletsreach me immediately.ENG1: I hear the trigger and the burst of bullets reach meinstantly.In Example 6, the expression ?the trigger?
in ENG1implicitly means ?the click of the trigger?, mak-ing the two sentences equivalent, and the entailmentbidirectional (instead of ENG?ENG1).d) 7 pairs where the information removed from oradded to the sentence is not relevant to the entail-ment relation.
In these cases, the modified sen-tence was judged less/more specific than the origi-nal one (and thus considered as unidirectional entail-ment), even though the correct judgement is ?bidi-rectional?, as in:Example 7.ENG: At the same time, AKP is struggling with its approach tothe EU.ENG1: AKP is struggling with its approach to the EuropeanUnion.e) 4 pairs where the added/removed informationconcerns universally quantified general statements,about which the interpretation of ?more/less spe-cific?
given by Turkers resulted in the wrong anno-tation.Example 8.ENG: I think the success of multicultural couples depends onthe size of the cultural gap between the two partnersENG1: I believe the success of the couples depends on the sizeof the cultural gap between the 2 partners.In Example 8, the additional information (?mul-ticultural?)
restricts the set to which it refers(?couples?)
making ENG entailed by ENG1, andnot vice versa as resulted from Turkers?
annotation.In light of this analysis, we conclude that the sen-tence modification methodology proved to be suc-cessful, as the low number of Type 1 errors shows.Considering that the most expensive phase in thecreation of a TE dataset is the generation of thepairs, this is a significant achievement.
Differently,the entailment assessment phase appears to be moreproblematic, accounting for the majority of errors.As shown by Type 2 errors, this is due to a par-tial misalignment between the instructions given inour HITs, and the formal definition of textual en-tailment.
For this reason, further experimentationwill explore different ways to instruct workers (e.g.asking to consider proper names and pronouns asequivalent) in order to reduce the amount of errorsproduced.
As a final remark, considering that in thecreation of a TE dataset the manual check of the an-notated pairs represents a minor cost, even the in-volvement of experts to filter out wrong annotationswould not decrease the cost-effectiveness of the pro-posed methodology.6 ConclusionsThere is an increasing need of annotated data todevelop new solutions to the Textual Entailmentproblem, explore new entailment-related tasks, andset up experimental frameworks targeting real-worldapplications.
Following the recent trends promot-ing annotation efforts that go beyond the estab-lished RTE Challenge framework (unidirectional en-tailment between monolingual T-H pairs), in this678paper we addressed the multilingual dimension ofthe problem.
Our primary goal was the creation oflarge-scale collections of entailment pairs for differ-ent language combinations.
Besides that, we consid-ered cost effectiveness and replicability as additionalrequirements.
To achieve our objectives, we devel-oped a ?divide and conquer?
methodology based oncrowdsourcing.
Our approach presents several keyinnovations with respect to the related works on TEdata acquisition.
These include the decompositionof a complex content generation task in a pipelineof simpler subtasks accessible to a large crowd ofnon-experts, and the integration of quality controlmechanisms at each stage of the process.
The resultof our work is the first large-scale dataset contain-ing both monolingual and cross-lingual corpora forseveral combinations of texts-hypotheses in English,Italian, and German.
Among the advantages of ourmethod it is worth mentioning: i) the full alignmentbetween the created corpora, ii) the possibility toeasily extend the dataset to new languages, and iii)the feasibility of creating general-purpose corpora,featuring multi-directional entailment relations, thatsubsume the traditional RTE-like annotation.AcknowledgmentsThis work has been partially supported by the EC-funded project CoSyne (FP7-ICT-4-24853).
The au-thors would like to thank Emanuele Pianta for thehelpful discussions, and Giovanni Moretti for thevaluable support in the creation of the CLTE dataset.ReferencesLuisa Bentivogli, Ido Dagan, Hoa Trang Dang, DaniloGiampiccolo, and Bernardo Magnini.
2009.
The FifthPASCAL Recognizing Textual Entailment Challenge.Proceedings of TAC 2009.Luisa Bentivogli, Elena Cabrio, Ido Dagan, Danilo Gi-ampiccolo, Medea Lo Leggio, and Bernardo Magnini.2010.
Building Textual Entailment Specialized DataSets: a Methodology for Isolating Linguistic Phenom-ena Relevant to Inference.
Proceedings of LREC 2010.Michael Bloodgood and Chris Callison-Burch.
2010.Using Mechanical Turk to Build Machine TranslationEvaluation Sets.
Proceedings of the NAACL 2010Workshop on Creating Speech and Language DataWith Amazons Mechanical Turk.Johan Bos, Fabio Massimo Zanzotto, and Marco Pennac-chiotti.
2009.
Textual Entailment at EVALITA 2009.Proceedings of EVALITA 2009.Chris Callison-Burch and Mark Dredze.
2010.
Creat-ing Speech and Language Data With Amazons Me-chanical Turk.
Proceedings NAACL-2010 Workshopon Creating Speech and Language Data With AmazonsMechanical Turk.Ido Dagan and Oren Glickman.
2004.
Probabilistic tex-tual entailment: Generic applied modeling of languagevariability.
Proceedings of the PASCAL Workshop ofLearning Methods for Text Understanding and Min-ing.Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards Cross-Lingual Textual Entailment.Proceedings of NAACL-HLT 2010.Yashar Mehdad, Matteo Negri, and Marcello Federico.2011.
Using Bilingual Parallel Corpora for Cross-Lingual Textual Entailment.
Proceedings of ACL-HLT2011.Rada Mihalcea and Carlo Strapparava.
2009.
The LieDetector: Explorations in the Automatic Recognitionof Deceptive Language.
Proceedings of ACL 2009.Joanna Mrozinski, Edward Whittaker, and Sadaoki Furui.2008.
Collecting a Why-Question Corpus for Devel-opment and Evaluation of an Automatic QA-System.Proceedings of ACL 2008.Matteo Negri and Yashar Mehdad.
2010.
Creating a Bi-lingual Entailment Corpus through Translations withMechanical Turk: $100 for a 10-day Rush.
Proceed-ings of the NAACL 2010 Workshop on Creating Speechand Language Data With Amazons Mechanical Turk.Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.2010.
Ask Not What Textual Entailment Can Do forYou...
Proceedings of ACL 2010.Rion Snow, Brendan O?Connor, Daniel Jurafsky and An-drew Y. Ng.
2008.
Cheap and Fast - But is it Good?Evaluating Non-Expert Annotations for Natural Lan-guage Tasks.
Proceedings of EMNLP 2008.Rui Wang and Chris Callison-Burch.
2010.
Cheap Factsand Counter-Facts.
Proceedings of the NAACL 2010Workshop on Creating Speech and Language DataWith Amazons Mechanical Turk.679
