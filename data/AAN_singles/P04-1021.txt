A Joint Source-Channel Model for Machine TransliterationLi Haizhou, Zhang Min, Su JianInstitute for Infocomm Research21 Heng Mui Keng Terrace, Singapore 119613{hli,sujian,mzhang}@i2r.a-star.edu.sgAbstractMost foreign names are transliterated intoChinese, Japanese or Korean withapproximate phonetic equivalents.
Thetransliteration is usually achieved throughintermediate phonemic mapping.
Thispaper presents a new framework thatallows direct orthographical mapping(DOM) between two different languages,through a joint source-channel model, alsocalled n-gram transliteration model (TM).With the n-gram TM model, we automatethe orthographic alignment process toderive the aligned transliteration units froma bilingual dictionary.
The n-gram TMunder the DOM framework greatly reducessystem development effort and provides aquantum leap in improvement intransliteration accuracy over that of otherstate-of-the-art machine learningalgorithms.
The modeling framework isvalidated through several experiments forEnglish-Chinese language pair.1 IntroductionIn applications such as cross-lingual informationretrieval (CLIR) and machine translation, there isan increasing need to translate out-of-vocabularywords from one language to another, especiallyfrom alphabet language to Chinese, Japanese orKorean.
Proper names of English, French,German, Russian, Spanish and Arabic originsconstitute a good portion of out-of-vocabularywords.
They are translated through transliteration,the method of translating into another language bypreserving how words sound in their originallanguages.
For writing foreign names in Chinese,transliteration always follows  the originalromanization.
Therefore, any foreign name willhave only one Pinyin (romanization of Chinese)and thus in Chinese characters.In this paper, we focus on automatic Chinesetransliteration of foreign alphabet names.
Becausesome alphabet writing systems use variousdiacritical marks, we find it more practical to writenames containing such diacriticals as they arerendered in English.
Therefore, we refer allforeign-Chinese transliteration to English-Chinesetransliteration, or E2C.Transliterating English names into Chinese isnot straightforward.
However, recalling theoriginal from Chinese transliteration is even morechallenging as the E2C transliteration may havelost some original phonemic evidences.
TheChinese-English backward transliteration processis also called back-transliteration, or C2E (Knight& Graehl, 1998).In machine transliteration, the noisy channelmodel (NCM), based on a phoneme-basedapproach, has recently received considerableattention (Meng et al 2001; Jung et al 2000; Virga& Khudanpur, 2003; Knight & Graehl, 1998).
Inthis paper we discuss the limitations of such anapproach and address its problems by firstlyproposing a paradigm that allows directorthographic mapping (DOM), secondly furtherproposing a joint source-channel model as arealization of DOM.
Two other machine learningtechniques, NCM and ID3 (Quinlan, 1993)decision tree, also are implemented under DOM asreference to compare with the proposed n-gramTM.This paper is organized as follows: In section 2,we present the transliteration problems.
In section3, a joint source-channel model is formulated.
Insection 4, several experiments are carried out tostudy different aspects of proposed algorithm.
Insection 5, we relate our algorithms to otherreported work.
Finally, we conclude the study withsome discussions.2 Problems in transliterationTransliteration is a process that takes a characterstring in source language as input and generates acharacter string in the target language as output.The process can be seen conceptually as two levelsof decoding: segmentation of the source string intotransliteration units; and relating the sourcelanguage transliteration units with units in thetarget language, by resolving differentcombinations of alignments and unit mappings.
Aunit could be a Chinese character or a monograph,a digraph or a trigraph and so on for English.2.1 Phoneme-based approachThe problems of English-Chinese transliterationhave been studied extensively in the paradigm ofnoisy channel model (NCM).
For a given Englishname E as the observed channel output, one seeksa posteriori the most likely Chinese transliterationC that maximizes P(C|E).
Applying Bayes rule, itmeans to find C to maximizeP(E,C) = P(E | C)*P(C)                       (1)with equivalent effect.
To do so, we are left withmodeling two probability distributions: P(E|C), theprobability of transliterating C to E through a noisychannel, which is also called transformation rules,and P(C), the probability distribution of source,which reflects what is considered good Chinesetransliteration in general.
Likewise, in C2E back-transliteration, we would find E that maximizesP(E,C) = P(C | E)*P(E)                       (2)for a given Chinese name.In eqn (1) and (2), P(C) and P(E) are usuallyestimated using n-gram language models (Jelinek,1991).
Inspired by research results of grapheme-to-phoneme research in speech synthesis literature,many have suggested phoneme-based approachesto resolving P(E|C) and P(C|E), whichapproximates the probability distribution byintroducing a phonemic representation.
In this way,we convert the names in the source language, sayE, into an intermediate phonemic representation P,and then convert the phonemic representation intothe target language, say Chinese C. In E2Ctransliteration, the phoneme-based approach can beformulated as P(C|E) = P(C|P)P(P|E) andconversely we have P(E|C) = P(E|P)P(P|C) forC2E back-transliteration.Several phoneme-based techniques have beenproposed in the recent past for machinetransliteration using transformation-based learningalgorithm (Meng et al 2001; Jung et al 2000;Virga & Khudanpur, 2003) and using finite statetransducer that implements transformation rules(Knight & Graehl, 1998), where both handcraftedand data-driven transformation rules have beenstudied.However, the phoneme-based approaches arelimited by two major constraints, which couldcompromise transliterating precision, especially inEnglish-Chinese transliteration:1) Latin-alphabet foreign names are of differentorigins.
For instance, French has different phonicrules from those of English.
The phoneme-basedapproach requires derivation of proper phonemicrepresentation for names of different origins.
Onemay need to prepare multiple language-dependentgrapheme-to-phoneme (G2P) conversion systemsaccordingly, and that is not easy to achieve (TheOnomastica Consortium, 1995).
For example,/Lafontant/ is transliterated into ???
(La-Feng-Tang) while /Constant/ becomes ????(Kang-Si-Tan-Te)?
where syllable /-tant/ in the twonames are transliterated differently depending onthe names?
language of origin.2) Suppose that language dependent grapheme-to-phoneme systems are attainable, obtainingChinese orthography will need two further steps: a)conversion from generic phonemic representationto Chinese Pinyin; b) conversion from Pinyin toChinese characters.
Each step introduces a level ofimprecision.
Virga and Khudanpur (2003) reported8.3% absolute accuracy drops when convertingfrom Pinyin to Chinese characters, due tohomophone confusion.
Unlike Japanese katakanaor Korean alphabet, Chinese characters are moreideographic than phonetic.
To arrive at anappropriate Chinese transliteration, one cannot relysolely on the intermediate phonemic representation.2.2 Useful orthographic contextTo illustrate the importance of contextualinformation in transliteration, let?s take name/Minahan/ as an example, the correct segmentationshould be /Mi-na-han/, to be transliterated as ?-?-?
(Pinyin: Mi-Na-Han).English /mi- -na- -han/Chinese ?
?
?Pinyin Mi Nan HanHowever, a possible segmentation /Min-ah-an/could lead to an undesirable syllabication of ?-?-?
(Pinyin: Min-A-An).English /min- -ah- -an/Chinese ?
?
?Pinyin Min A AnAccording to the transliteration guidelines, awise segmentation can be reached only afterexploring the combination of the left and rightcontext of transliteration units.
From thecomputational point of view, this strongly suggestsusing a contextual n-gram as the knowledge basefor the alignment decision.Another example will show us how one-to-manymappings could be resolved by context.
Let?s takeanother name /Smith/ as an example.
Although wecan arrive at an obvious segmentation /s-mi-th/,there are three Chinese characters for each of /s-/,/-mi-/ and /-th/.
Furthermore, /s-/ and /-th/correspond to overlapping characters as well, asshown next.English /s- -mi- -th/Chinese 1 ?
?
?Chinese 2 ?
?
?Chinese 3 ?
?
?A human translator will use transliteration rulesbetween English syllable sequence  and Chinesecharacter sequence to obtain the best mapping ?-?-?, as indicated in italic in the table above.To address the issues in transliteration, wepropose a direct orthographic mapping (DOM)framework through a joint source-channel modelby fully exploring orthographic contextualinformation, aiming at alleviating the imprecisionintroduced by the multiple-step phoneme-basedapproach.3 Joint source-channel modelIn view of the close coupling of the source andtarget transliteration units, we propose to estimateP(E,C) by a joint source-channel model, or n-gramtransliteration model (TM).
For K alignedtransliteration units, we have)...,,...,(),( 2121 KK ccceeePCEP =),...,,,( 21 KcececeP ><><><= (3)?=?><><=Kkkk ceceP111 ),|,(which provides an alternative to the phoneme-based approach for resolving eqn.
(1) and (2) byeliminating the intermediate phonemicrepresentation.Unlike the noisy-channel model, the jointsource-channel model does not try to capture howsource names can be mapped to target names, butrather how source and target names can begenerated simultaneously.
In other words, weestimate a joint probability model that can beeasily marginalized in order to yield conditionalprobability models for both transliteration andback-transliteration.Suppose that we have an English namemxxx ...21=?
and a Chinese transliterationnyyy ...21=?
where ix are letters and jy areChinese characters.
Oftentimes, the number ofletters is different from the number of Chinesecharacters.
A Chinese character may correspond toa letter substring in English or vice versa.mii xxxxxxx ...... 21321 ++nj yyyy ......21where there exists an alignment  ?
with>=<>< 111 ,, yxce>=<>< 2322 ,, yxxce  ?and >=<>< nmK yxce ,, .
A transliteration unitcorrespondence >< ce,  is called a transliterationpair.
Then, the E2C transliteration can beformulated as),,(maxarg,?????
?P=  (4)and similarly the C2E back-transliteration as),,(maxarg,?????
?P=  (5)An n-gram transliteration model is defined as theconditional probability, or transliterationprobability, of a transliteration pair kce >< ,depending on its immediate n predecessor pairs:),,(),( ??
?PCEP =?=?+?><><=Kkknkk ceceP111),|,(        (6)3.1 Transliteration alignmentA bilingual dictionary contains entries mappingEnglish names to their respective Chinesetransliterations.
Like many other solutions incomputational linguistics, it is possible toautomatically analyze the bilingual dictionary toacquire knowledge in order to map new Englishnames to Chinese and vice versa.
Based on thetransliteration formulation above, a transliterationmodel can be built with transliteration unit?s n-gram statistics.
To obtain the statistics, thebilingual dictionary needs to be aligned.
Themaximum likelihood approach, through EMalgorithm (Dempster, 1977), allows us to infersuch an alignment easily as described in the tablebelow.The aligning process is different from that oftransliteration given in eqn.
(4) or (5) in that, herewe have fixed bilingual entries, ?
and ?
.
Thealigning process is just to find the alignmentsegmentation ?
between the two strings thatmaximizes the joint probability:),,(maxarg ????
?P=   (7)A set of transliteration pairs that is derived fromthe aligning process forms a transliteration table,which is in turn used in the transliterationdecoding.
As the decoder is bounded by this table,it is important to make sure that the trainingdatabase covers as much as possible the potentialtransliteration patterns.
Here are some examples ofresulting alignment pairs.
?|s  ?|l ?|t ?|d?|k ?|b ?|g ?|r?|ll ?|c  ?|ro  ?|ri?|man  ?|m  ?|p  ?|de?|ra  ?|le  ?|a  ?|ber?|la  ?|son  ?|ton  ?|tt?|re  ?|co  ?|o  ?|e?|ma  ?|ley  ?|li  ?|merKnowing that the training data set will never besufficient for every n-gram unit, differentsmoothing approaches are applied, for example, byusing backoff or class-based models, which can befound in statistical language modeling literatures(Jelinek, 1991).3.2 DOM: n-gram TM vs. NCMAlthough in the literature, most noisy channelmodels (NCM) are studied under phoneme-basedparadigm for machine transliteration, NCM canalso be realized under direct orthographic mapping(DOM).
Next, let?s look into a bigram case to seewhat n-gram TM and NCM present to us.
For E2Cconversion, re-writing eqn (1) and eqn (6) , wehave?=?
?Kkkkkk ccPcePP11)|()|(),,( ???
(8)),,( ??
?P ),|,( 11?=><><??
kkKkceceP   (9)The formulation of eqn.
(8) could be interpretedas a hidden Markov model with Chinese charactersas its hidden states and English transliteration unitsas the observations (Rabiner, 1989).
The numberof parameters in the bigram TM is potentially 2T ,while in the noisy channel model (NCM) it?s2CT + , where T  is the number of transliterationpairs and C is the number of Chinesetransliteration units.
In eqn.
(9), the currenttransliteration depends on both Chinese andEnglish transliteration history while in eqn.
(8), itdepends only on the previous Chinese unit.As 22 CTT +>> , an n-gram TM gives a finerdescription than that of NCM.
The actual size ofmodels largely depends on the availability oftraining data.
In Table 1, one can get an idea ofhow they unfold in a real scenario.
Withadequately sufficient training data, n-gram TM isexpected to outperform NCM in the decoding.
Aperplexity study in section 4.1 will look at themodel from another perspective.4 The experiments1We use a database from the bilingual dictionary?Chinese Transliteration of Foreign PersonalNames?
which was edited by Xinhua NewsAgency and was considered the de facto standardof personal name transliteration in today?s Chinesepress.
The database includes a collection of 37,694unique English entries and their official Chinesetransliteration.
The listing includes personal namesof English, French, Spanish, German, Arabic,Russian and many other origins.The database is initially randomly distributedinto 13 subsets.
In the open test, one subset iswithheld for testing while the remaining 12 subsetsare used as the training materials.
This process isrepeated 13 times to yield an average result, whichis called the 13-fold open test.
After experiments,we found that each of the 13-fold open tests gaveconsistent error rates with less than 1% deviation.Therefore, for simplicity, we randomly select oneof the 13 subsets, which consists of 2896 entries,as the standard open test set to report results.
In theclose test, all data entries are used for training andtesting.1 demo at http://nlp.i2r.a-star.edu.sg/demo.htmThe Expectation-Maximization algorithm1.
Bootstrap initial random alignment2.
Expectation: Update n-gram statistics toestimate probability distribution3.
Maximization: Apply the n-gram TM toobtain new alignment4.
Go to step 2 until the alignment converges5.
Derive a list transliteration units from finalalignment as transliteration table4.1 ModelingThe alignment of transliteration units is donefully automatically along with the n-gram TMtraining process.
To model the boundary effects,we introduce two extra units <s> and </s> for startand end of each name in both languages.
The EMiteration converges at 8th round when no furtheralignment changes are reported.
Next are somestatistics as a result of the model training:# close set bilingual entries (full data)  37,694# unique Chinese transliteration (close) 28,632# training entries for open test 34,777# test entries for open test 2,896# unique transliteration pairs  T 5,640# total transliteration pairs TW  119,364# unique English units E 3,683# unique Chinese units C 374# bigram TM ),|,( 1?><>< kk ceceP  38,655# NCM Chinese bigram )|( 1?kk ccP  12,742Table 1.
Modeling statisticsThe most common metric for evaluating an n-gram model is the probability that the modelassigns to test data, or perplexity (Jelinek, 1991).For a test set W composed of V names, where eachname has been aligned into a sequence oftransliteration pair tokens, we can calculate theprobability of test set?==VvvvvPWp1),,()( ???
by applying the n-grammodels to the token sequence.
The cross-entropy)(WH p  of a model on data W is defined as)(log1)( 2 WpWWHTp ?=  where TW is the totalnumber of aligned transliteration pair tokens in thedata W. The perplexity )(WPPp of a model is thereciprocal of the average probability assigned bythe model to each aligned pair in the test set Was )(2)( WHp pWPP = .Clearly, lower perplexity means that the modeldescribes better the data.
It is easy to understandthat closed test always gives lower perplexity thanopen test.TMopenNCMopenTMclosedNCMclosed1-gram 670 729 655 7162-gram 324 512 151 2103-gram 306 487 68 127Table 2.
Perplexity study of bilingual databaseWe have the perplexity reported in Table 2 onthe aligned bilingual dictionary, a database of119,364 aligned tokens.
The NCM perplexity iscomputed using n-gram equivalents of eqn.
(8) forE2C transliteration, while TM perplexity is basedon those of eqn (9) which applies to both E2C andC2E.
It is shown that TM consistently gives lowerperplexity than NCM in open and closed tests.
Wehave good reason to expect TM to provide bettertransliteration results which we expect to beconfirmed later in the experiments.The Viterbi algorithm produces the bestsequence by maximizing the overall probability,),,( ??
?P .
In CLIR or multilingual corpusalignment (Virga and Khudanpur, 2003), N-bestresults will be very helpful to increase chances ofcorrect hits.
In this paper, we adopted an N-beststack decoder (Schwartz and Chow, 1990) in bothTM and NCM experiments to search for N-bestresults.
The algorithm also allows us to applyhigher order n-gram such as trigram in the search.4.2 E2C transliterationIn this experiment, we conduct both open andclosed tests for TM and NCM models under DOMparadigm.
Results are reported in Table 3 andTable 4.open(word)open(char)closed(word)closed(char)1-gram 45.6% 21.1% 44.8% 20.4%2-gram 31.6% 13.6% 10.8% 4.7%3-gram 29.9% 10.8% 1.6% 0.8%Table 3.
E2C error rates for n-gram TM tests.open(word)open(char)closed(word)closed(char)1-gram 47.3% 23.9% 46.9% 22.1%2-gram 39.6% 20.0% 16.4% 10.9%3-gram 39.0% 18.8% 7.8% 1.9%Table 4.
E2C error rates for n-gram NCM testsIn word error report, a word is consideredcorrect only if an exact match happens betweentransliteration and the reference.
The charactererror rate is the sum of deletion, insertion andsubstitution errors.
Only the top choice in N-bestresults is used for error rate reporting.
Notsurprisingly, one can see that n-gram TM, whichbenefits from the joint source-channel modelcoupling both source and target contextualinformation into the model, is superior to NCM inall the test cases.4.3 C2E back-transliterationThe C2E back-transliteration is morechallenging than E2C transliteration.
Not manystudies have been reported in this area.
It iscommon that multiple English names are mappedinto the same Chinese transliteration.
In Table 1,we see only 28,632 unique Chinese transliterationsexist for 37,694 English entries, meaning that somephonemic evidence is lost in the process oftransliteration.
To better understand the task, let?scompare the complexity of the two languagespresented in the bilingual dictionary.Table 1 also shows that the 5,640 transliterationpairs are cross mappings between 3,683 Englishand 374 Chinese units.
In order words, on average,for each English unit, we have 1.53 = 5,640/3,683Chinese correspondences.
In contrast, for eachChinese unit, we have 15.1 = 5,640/374 Englishback-transliteration units!
Confusion is increasedtenfold going backward.The difficulty of back-transliteration is alsoreflected by the perplexity of the languages as inTable 5.
Based on the same alignmenttokenization, we estimate the monolinguallanguage perplexity for Chinese and Englishindependently using the n-gram language models)|( 1 1?+?knkk ccP  and )|(11?+?knkk eeP .
Withoutsurprise, Chinese names have much lowerperplexity than English names thanks to fewerChinese units.
This contributes to the success ofE2C but presents a great challenge to C2E back-transliteration.1-gram 2-gram 3-gramChinese 207/206 97/86 79/45English 710/706 265/152 234/67Table 5 language perplexity comparison(open/closed test)open(word)open(letter)closed(word)closed(letter)1 gram 82.3% 28.2% 81% 27.7%2 gram 63.8% 20.1% 40.4% 12.3%3 gram 62.1% 19.6% 14.7% 5.0%Table 6.
C2E error rate for n-gram TM testsE2CopenE2CclosedC2EopenC2Eclosed1-best 29.9% 1.6% 62.1% 14.7%5-best 8.2% 0.94% 43.3% 5.2%10-best 5.4% 0.90% 24.6% 4.8%Table 7.
N-best word error rates for 3-gram TMtestsA back-transliteration is considered correct if itfalls within the multiple valid orthographicallycorrect options.
Experiment results are reported inTable 6.
As expected, C2E error rate is muchhigher than that of E2C.In this paper, the n-gram TM model serves as thesole knowledge source for transliteration.However, if secondary knowledge, such as alookup table of valid target transliterations, isavailable, it can help reduce error rate bydiscarding invalid transliterations top-down the Nchoices.
In Table 7, the word error rates for bothE2C and C2E are reported which imply potentialerror reduction by secondary knowledge source.The N-best error rates are reduced significantly at10-best level as reported in Table 7.5 DiscussionsIt would be interesting to relate n-gram TM toother related framework.5.1 DOM: n-gram TM vs. ID3In section 4, one observes that contextualinformation in both source and target languages isessential.
To capture them in the modeling, onecould think of decision tree, another popularmachine learning approach.
Under the DOMframework, here is the first attempt to applydecision tree in E2C and C2E transliteration.With the decision tree, given a fixed sizelearning vector, we used top-down induction treesto predict the corresponding output.
Here weimplement ID3 (Quinlan, 1993) algorithm toconstruct the decision tree which containsquestions and return values at terminal nodes.Similar to n-gram TM, for unseen names in opentest, ID3 has backoff smoothing, which lies on thedefault case which returns the most probable valueas its best guess for a partial tree path according tothe learning set.In the case of E2C transliteration, we form alearning vector of 6 attributes by combining 2 leftand 2 right letters around the letter of focus ke  and1 previous Chinese unit 1?kc .
The process isillustrated in Table 8, where both English andChinese contexts are used to infer a Chinesecharacter.
Similarly, 4 attributes combining 1 left,1 centre and 1 right Chinese character and 1previous English unit are used for the learningvector in C2E test.
An aligned bilingual dictionaryis needed to build the decision tree.To minimize the effects from alignmentvariation, we use the same alignment results fromsection 4.
Two trees are built for two directions,E2C and C2E.
The results are compared with those3-gram TM  in Table 9.2?ke  1?ke  ke  1+ke  2+ke  1?kc   kc_ _ N I C _ > ?_ N I C E ?
> _N I C E _ _ > ?I C E _ _ ?
> _Table 8.
E2C transliteration using ID3 decisiontree  for transliterating Nice to??
(?|NI ?|CE)open  closedID3 E2C  39.1% 9.7%3-gram TM E2C 29.9% 1.6%ID3 C2E 63.3% 38.4%3-gram TM C2E 62.1% 14.7%Table 9.
Word error rate ID3 vs. 3-gram TMOne observes that n-gram TM consistentlyoutperforms ID3 decision tree in all tests.
Threefactors could have contributed:1) English transliteration unit size ranges from 1letter to 7 letters.
The fixed size windows in ID3obviously find difficult to capture the dynamics ofvarious ranges.
n-gram TM seems to have bettercaptured the dynamics of transliteration units;2) The backoff smoothing of n-gram TM is moreeffective than that of ID3;3) Unlike n-gram TM, ID3 requires a separatealigning process for bilingual dictionary.
Theresulting alignment may not be optimal for treeconstruction.
Nevertheless, ID3 presents anothersuccessful implementation of DOM framework.5.2 DOM vs. phoneme-based approachDue to lack of standard data sets, it is difficult tocompare the performance of the n-gram TM to thatof other approaches.
For reference purpose, we listsome reported studies on other databases of E2Ctransliteration tasks in Table 10.
As in thereferences, only character and Pinyin error ratesare reported, we only include our character andPinyin error rates for easy reference.
The referencedata are extracted from Table 1 and 3 of (Virga andKhudanpur 2003).
As we have not found any C2Eresult in the literature, only E2C results arecompared here.The first 4 setups by Virga et alall adopted thephoneme-based approach in the following steps:1) English name to English phonemes;2) English phonemes to Chinese Pinyin;3) Chinese Pinyin to Chinese characters.It is obvious that the n-gram TM comparesfavorably to other techniques.
n-gram TM presentsan error reduction of 74.6%=(42.5-10.8)/42.5% forPinyin over the best reported result, Huge MT (BigMT) test case, which is noteworthy.The DOM framework shows a quantum leap inperformance with n-gram TM being the mostsuccessful implementation.
The n-gram TM andID3 under direct orthographic mapping (DOM)paradigm simplify the process and reduce thechances of conversion errors.
As a result, n-gramTM and ID3 do not generate Chinese Pinyin asintermediate results.
It is noted that in the 374legitimate Chinese characters for transliteration,character to Pinyin mapping is unique while Pinyinto character mapping could be one to many.
Sincewe have obtained results in character already, weexpect less Pinyin error than character error shoulda character-to-Pinyin mapping be needed.System Training sizeTestsizePinyinerrorsCharerrorsMeng et al2,233 1,541 52.5% N/ASmall MT 2,233 1,541 50.8% 57.4%Big MT 3,625 250 49.1% 57.4%Huge MT(Big MT)309,0193,122 42.5% N/A3-gramTM/DOM34,777 2,896 < 10.8% 10.8%ID3/DOM 34,777 2,896 < 15.6% 15.6%Table 10.
Performance reference in recentstudies6 ConclusionsIn this paper, we propose a new framework(DOM) for transliteration.
n-gram TM is asuccessful realization of DOM paradigm.
Itgenerates probabilistic orthographic transformationrules using a data driven approach.
By skipping theintermediate phonemic interpretation, thetransliteration error rate is reduced significantly.Furthermore, the bilingual aligning process isintegrated into the decoding process in n-gram TM,which allows us to achieve a joint optimization ofalignment and transliteration automatically.
Unlikeother related work where pre-alignment is needed,the new framework greatly reduces thedevelopment efforts of machine transliterationsystems.
Although the framework is implementedon an English-Chinese personal name data set,without loss of generality, it well applies totransliteration of other language pairs such asEnglish/Korean and English/Japanese.It is noted that place and company names aresometimes translated in combination oftransliteration and meanings, for example,/Victoria-Fall/ becomes ?
?
?
?
?
?
(Pinyin:Wei Duo Li Ya Pu Bu).
As the proposedframework allows direct orthographical mapping,it can also be easily extended to handle such nametranslation.
We expect to see the proposed modelto be further explored in other related areas.ReferencesDempster, A.P., N.M. Laird and D.B.Rubin, 1977.Maximum likelihood from incomplete data viathe EM algorithm, J. Roy.
Stat.
Soc., Ser.
B. Vol.39, pp138Helen M. Meng, Wai-Kit Lo, Berlin Chen andKaren Tang.
2001.
Generate Phonetic Cognatesto Handle Name Entities in English-Chinesecross-language spoken document retrieval,ASRU 2001Jelinek, F. 1991, Self-organized languagemodeling for speech recognition, In Waibel, A.and Lee K.F.
(eds), Readings in SpeechRecognition, Morgan Kaufmann., San Mateo,CAK.
Knight and J. Graehl.
1998.
MachineTransliteration, Computational Linguistics 24(4)Paola Virga, Sanjeev Khudanpur, 2003.Transliteration of Proper Names in Cross-lingual Information Retrieval.
ACL 2003workshop MLNERQuinlan J. R. 1993, C4.5 Programs for machinelearning, Morgan Kaufmann , San Mateo, CARabiner, Lawrence R. 1989, A tutorial on hiddenMarkov models and selected applications inspeech recognition, Proceedings of the IEEE77(2)Schwartz, R. and Chow Y. L., 1990, The N-bestalgorithm: An efficient and Exact procedure forfinding the N most likely sentence hypothesis,Proceedings of ICASSP 1990, Albuquerque, pp81-84Sung Young Jung, Sung Lim Hong and EunokPaek, 2000, An English to KoreanTransliteration Model of Extended MarkovWindow, Proceedings of COLINGThe Onomastica Consortium, 1995.
TheOnomastica interlanguage pronunciationlexicon, Proceedings of EuroSpeech, Madrid,Spain, Vol.
1, pp829-832Xinhua News Agency, 1992, Chinesetransliteration of foreign personal names, TheCommercial Press
