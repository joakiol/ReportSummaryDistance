Proceedings of the SIGDIAL 2013 Conference, pages 102?106,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsModel-free POMDP optimisation of tutoring systems with echo-statenetworksLucie Daubigney1,3 Matthieu Geist11IMS-MaLIS ?
Supe?lec (Metz, France), 2UMI2958 ?
GeorgiaTech/CNRS (Metz, France)3Team project MaIA ?
Loria (Nancy, France)Olivier Pietquin1,2AbstractIntelligent Tutoring Systems (ITSs) arenow recognised as an interesting alter-native for providing learning opportuni-ties in various domains.
The Reinforce-ment Learning (RL) approach has beenshown reliable for finding efficient teach-ing strategies.
However, similarly to otherhuman-machine interaction systems suchas spoken dialogue systems, ITSs sufferfrom a partial knowledge of the interlocu-tor?s intentions.
In the dialogue case, en-gineering work can infer a precise state ofthe user by taking into account the uncer-tainty provided by the spoken understand-ing language module.
A model-free ap-proach based on RL and Echo State New-torks (ESNs), which retrieves similar in-formation, is proposed here for tutoring.1 IntroductionFor the last decades, Intelligent Tutoring Sys-tems (ITSs) have become powerful tools in variousdomains such as mathematics (Koedinger et al1997), physics (Vanlehn et al 2005; Litman andSilliman, 2004; Graesser et al 2005), computersciences (Corbett et al 1995), reading (Mostowand Aist, 2001), or foreign languages (Heift andSchulze, 2007; Amaral and Meurers, 2011).
Theirappeal relies on the fact that each student does nothave to follow an average teaching strategy, espe-cially as the one-to-one tutoring has been proventhe most efficent (Bloom, 1968).
The expertise ofa teacher relies on his capacity to advice at theright time the student to acquire new skills.
Todo so, the teacher is able to choose iteratively ped-agogical activities.
From this perspective, teach-ing is a sequential decision-making problem.
Tosolve it, the reinforcement learning (Sutton andBarto, 1998) approach and the Markov DecisionProcess (MDP) paradigm have been successfullyused (Iglesias et al 2009).
Given a situation, eachteacher?s decision is locally quantified by a re-ward.
However, the consequences of the teacher?sactions on the student?s cognition cannot be ex-actly determined, which introduce uncertainty.To find a solution, one can notice that spokendialogue management and tutoring are closely re-lated.
Both are humain-computer interactions inwhich the human user?s intentions are not per-fectly known.
In the spoken dialogue case, thepartial observability is due to the recognition er-rors introduced by the speech understanding mod-ule.
They are taken into account by using somehypotheses about how the language is constructed.Thus, accurate models to link observations fromthe user?s recognised utterances to the underlyingintentions can be set up.
For example, the HiddenInformation State paradigm (Young et al 2006;Young et al 2010) builds a state which is a sum-mary of the dialogue history (Gas?ic?
et al 2010;Daubigney et al 2011; Daubigney et al 2012).However, in the ITS case, such a state is harder todevelop since the cognition cannot be determinedby analysing a physical signal.
Thus, a model-freeapproach is preferred here.To do so, a memory of the past observationsand actions is built by means of a Recurrent Neu-ral Network (RNN) and more precisely an EchoState Network (ESN) (Jaeger, 2001).
The inter-nal state of the network can be shown (under someresonable conditions) to meet the Markov prop-erty (Szita et al 2006).
This internal state is thenused with a standard RL algorithm to estimate theoptimal solution.
It has already been applied to RLin (Szita et al 2006) in limited toy applicationsand it is, to our knowledge, the first attempt to useit in an interaction framework.
The proof of con-cept presented in Szita?s article uses the commonSARSA algorithm which is an on-line and on-policy algorithm.
Each improvement of the strat-102egy is directly tested.
In the case of teaching, test-ing poor decisions can be problematic.
Here, wethus propose the combination of an ESN with anoff-line and off-policy algorithm, namely the LeastSquare Policy Algorithm (LSPI) (Lagoudakis andParr, 2003), which is another original contribu-tion of this paper.
Indeed, learning the solutionwith Partially Observable MDPs in a batch andoff-policy manner is not common in the literature.2 Markov Decision Process andReinforcement LearningFormally, an MDP is a tuple {S,A, T,R, ?}
setup to describe the tutor environment.
The setS is the state space which represents the infor-mation about the student, A is the action spacewhich contains the tutor?s actions, T is a set oftransition probabilities defined such that T ={p(s?|s, a),?
(s?, s, a) ?
S ?
S ?
A}, R is thereward function, given according to the studentprogression for example, and ?
?
[0, 1] is thediscount factor which weights the future rewards.The set of transitions probabilities in the ITS caseis unknown: the evolution of the student intentionscannot be determined.
Solving the MPD consistsin finding the optimal strategy, called the optimalpolicy which brings the highest expected cumula-tive reward.However, in the ITS case, information about thestudent?s knowledge, represented by s, can onlybe known through observations.
Let O = {oi} bethe set of possible observations.
Yet, if only ob-servations are available, a memory of what hap-pened during previous interactions (the history)is necessary, because the process of observationsdoes not meet the Markov property.
The his-tory is the sequence of observation-action pairsencountered during a whole teaching phase.
LetH = {hi} be the set of all possible histories withhi = {o0, a0, o1, a1, ..., oi?1, ai?1, oi}.When the POMDP framework is used, the un-derlying state si is inferred from the history bymeans of a model of probabilities linking si tohi.
In the case of human-machine interactions, thismodel is not available.
It can be approximated butthe considered solutions are ad-hoc to a particularproblem, thus difficult to reuse.
Here, we proposean approach with as few assumptions as possibleabout the student cognitive model by using EchoStates Networks (ESNs).
This approach builds acompact representation of the history space H .u0u1u2Inputx0x1x2x3x4 x5x6x7 x8x9Reservoiry0y1y2y3Output1Figure 1: RNN structure (for sake of readability,all the connections do not appear).3 Echo State NetworksAn Echo State Network is represented by threelayers of neurons (Fig.
1): an input, a hidden andan output.
The number of neurons in the hiddenlayer is supposed to be large and each of themcan be connected to itself.
These recurrent con-nections are responsible for reusing the value ofthe neurons at a previous time step.
Consequently,a memory is built in the reservoir and trajectoriescan be encoded.
Only the connections from thehidden layer to the output one are learnt since allthe other connections are randomly and sparselyset.
The recurrent connections are defined so thatthe echo state property is met (Jaeger, 2001): ifafter a given number of updates of the input neu-rons, two internal states are exactly the same, thenthe input sequences which led to these two internalstates are identical.The connections of the ESN are presentedin Fig.
2, with uk ?
RNi , xk ?
RNh andyk ?
RNo , respectively representing the valuesof the input, hidden and output layers, Ni, Nhand No being the respective number of neuronsand W in ?
MNh?Ni , W hid ?
MNh?Nh andW out ?
MNo?Nh , matrix containing the synap-tic weights.
After a training, the output yk returnsa linear approximation of the internal state of thereservoir.
This output depends on the sequence ofinputs u0, ?
?
?
, uk and not only uk, through xk.Combining ESNs and RL is of interest.
Bymeans of the echo state property, a summary ofthe observations and decisions encountered duringthe tutoring phase is provided through the internalstate x.
In (Szita et al 2006), it has been provento meet the Markov property with high probabil-ity.
It thus can be used as a state for standardRL algorithms.
Here, more precisely, it representsthe basis function of an approximation of the Q-103Inputu(k)[?]MNi?1Hiddenx(k)???????...?????MNh?1Outputy(k)[?
]MNo?1WhidMNh?NhW inMNh?NiW outMNo?NhInputu(k)[?]MNi?1Hiddenx(k)???????...?????MNh?1Projectionx?(k)?????Mm?1Outputy(k)[?
]MNo?1WhidMNh?NhW inMNh?Ni?Mm?NhW outMNo?mFigure 2: Structure of an ESN.
For the example,Ni = 1 and No = 1.function.
This function is associated with a policypi, defined for each couple (s, a) ?
S ?
A suchthat Qpi(s, a) = E [?i ?iri|s0 = s, a0 = a] andquantifies the policy.
ESNs are used in the fol-lowing way to solve RL problems.
The networkis responsible for giving, from an observations okand an action ak at time step k, a linear estimationof the value of the Q-function Q??
(hk, ak) (withhk = {o0, a0, ..., ok?1, ak?1, ok}).
The state s isnot used in the estimation of the Q-function sinceit is unknown.
Instead, it is replaced by the historyhk.
The input of the ESN, uk, is thus the con-catenation of the observation ok and the action ak:uk = (ok, ak).
The internal state xk which com-ponent are in [?1, 1], is a summary of the historyhk and the action ak.
Thus, the estimation of theQ-function is Q??
(hk, ak) = ?>xk.
The values ofthe output connections are learnt by means of theLSPI algorithm.
With this algorithm, the optimalpolicy is learnt from a fixed set of data.4 Experimental settingsFor the experiments, we assume that the teachingcan be done by means of three actions.
First, a les-son can be presented to make the knowledge of thestudent increase.
The second and third actions areevaluations.
They can either be a simple questionor a final exam.
The final exam consists in ask-ing a hundred yes/no questions of equal complex-ity and on the same topic.
The student does nothave a feedback.
Once it is proposed, a new teach-ing episode starts.
Three observations are returnedto the ITS.
If a lesson is proposed to the user, theobservation is neutral: no feedback comes fromthe student since the direct influence of the lessonremains unknown.
The two other obervations ap-pear when a question is asked (yes or no).
Conse-quently, one observation is not enough to choosethe next action since no clue is given about howmany lessons have led to this result.
A non-null re-ward is only given when a final exam is proposed.In this case, it is proportional to the rate of cor-rect answers among all the answers given duringthe exam.
Thus, each improvement is taken intoaccount.
The ?
factor is set to 0.97.In this proof of concept, the results have beenobtained with simulated students from (Chang etal., 2006) to ensure the reproducibility of the ex-periments.
The simulation implements two abili-ties: answering a question and learning with a les-son.
Three groups of students have been set up.The first one, T1, is supposed to be able to learnvery efficiently, the second, T2, needs a few morelessons to provide good answers, and the third, T3,needs a lot of lessons to answer correctly.5 ResultsSeveral teaching strategies have been compared.As a lower bound baseline, a random strategy hasbeen tested.
With a probability (w.p.)
of 0.6, a les-son is proposed, w.p.
of 0.2 a question is chosen,and w.p.
of 0.2 a final exam is proposed.
The datagenerated with this random strategy have beenused by the LSPI algorithm and an informed statespace.
The second baseline proposed is the reac-tive policy learnt by LSPI (called reactive-LSPI),only from obervations.
Neither the informationabout the number of lessons proposed nor the in-ternal state of the ESN is used.
The third strategyis learnt by using the observations and a counterof lessons already given (called informed-LSPI).Thus, this state supposedly contains sufficient in-formation to take the decision.
For this case, sincethe numbers of observations and lessons are dis-crete thus countable, a tabular representation ischosen for the Q-function.
The fourth strategyuses the internal state of the ESN as basis functionfor the Q-function (called ESN-LSPI).
There are50 hidden neurons.
Different sizes of training datasets are tested.
Among the data, the three types ofstudents are represented in equal proportions.
Onehundred policies are learnt for each of the methodspresented, except for the ESN-LSPI.
For this one,10 ESNs are generated and 10 training sessions areperformed with each one of them.
The mean overthe average results of each of the 10 learnings ispresented in the results.
Each of the policies havebeen tested 1000 times.Fig.
3 shows a comparison of the learnt strate-gies.
The three types of students are used forthe training and test phases.
One can notice that1040.30.40.50.60.70  2000  4000  6000  8000  10000AveragesumofdiscountedrewardsNumber of transitionsRandomReactive-LSPIInformed-LSPIESN-LSPIFigure 3: Comparison of the different strategies.the standard deviation is larger when the ESN areused because uncertainty is added when generat-ing the ESN since the connections are randomlyset.
The random and the reactive policies givethe poorest results.
Yet, the average reward in-creases because of the data in the training set.
Forsmall sets, long sequences of lessons only have notbeen encountered.
Thus, larger rewards have notbeen encountered either.
For the two other curves,with a reasonable number of interactions (around8000), a good strategy is learnt by using informed-LSPI.
The strategies learnt with the ESN requirefewer transitions and allow a faster learning.
Inthis case, the optimum is reached with 2000 transi-tions while 8000 ones are needed to reach the samequality with the informed-LSPI strategy.
Around10000 samples, both policies give the same re-sults.
However, less information is given in theESN approach (only observations).
Thus, this ap-proach is more generic.
The counter informationmay not be sufficient for more complex problems.To compare the efficiency of the learnt policies,the informed-LSPI and ESN-LSPI are plotted foreach group of students in Fig.
4.
All the strate-gies are learnt with the same data sets than pre-viously, but only one type of students is tested ata time.
For the T2 and T3 types, the average re-sults are better with ESN-LSPI (especially for theT3 type).
For the T1 group, informed-LSPI re-turns slighlty better results.
A better insight ofthe behaviour of each policy is given in Fig.
5 byplotting the distribution of the actions used dur-ing the test phase.
A comparison reveals that thenumber of lessons is higher in the ESN-LSPI case(around 3) whereas only one lesson is given in av-erage with informed-LSPI.
This is of benefit tostudents of the third group and thus implicitly tothose of the first and second groups.
The numberof lessons is even larger for the third group than for00.10.20.30.40.50.60.70.80.90  2000  4000  6000  8000  10000AveragesumofdiscountedrewardsNumber of transitionsInformed-LSPI (StudentT1)Informed-LSPI (StudentT2)Informed-LSPI (StudentT3)ESN-LSPI (StudentT1)ESN-LSPI (StudentT2)ESN-LSPI (StudentT3)Figure 4: Results of the learnt policies for eachgroup of students.01234Lesson Question FinalExamAvg.numberofactionsActions proposed with Informed LSPIStudentT1StudentT2StudentT301234Lesson Question FinalExamAvg.numberofactionsActions proposed with ESN LSPIStudentT1StudentT2StudentT3Figure 5: Distribution of the actions (the size ofthe training dataset is 10000).the two others (0.5 more in average).
However, inthe informed-LSPI case, the learnt policy is onlyprofitable for those of the first group, who are al-ready skilled (this conclusion is consistent with theFig.
4).
Questions are very rarely asked becauseonce the number of lessons has been learnt, theybring no more information.6 ConclusionWe proposed a model-free approach which usesonly observations to find optimal teaching state-gies.
A summary of the history encountered isimplemented by means of an ESN.
This summaryhas been proven to be Markovian by (Szita et al2006).
A standard RL algorithm which can learnfrom already collected data, is then used to per-form the learning.
Preliminary experiments havebeen presented on simulated data.
In future works,we plan to apply this method to SDSs.AcknowledgmentsResults have been computed with the InterCellcluster funded by the Re?gion Lorraine.105ReferencesL.
Amaral and D. Meurers.
2011.
On using intelli-gent computer-assisted language learning in real-lifeforeign language teaching and learning.
ReCALL,23(1):4?24.B.
Bloom.
1968.
Learning for mastery.
Evaluationcomment, 1(2):1?5.K.
Chang, J. Beck, J. Mostow, and A. Corbett.
2006.
Abayes net toolkit for student modeling in intelligenttutoring systems.
In Intelligent Tutoring Systems,pages 104?113.
Springer.A.
Corbett, J. Anderson, and A. OBrien.
1995.
Studentmodeling in the act programming tutor.
Cognitivelydiagnostic assessment, pages 19?41.L.
Daubigney, M.
Gas?ic?, S. Chandramohan, M. Geist,O.
Pietquin, and S. Young.
2011.
Uncertaintymanagement for on-line optimisation of a POMDP-based large-scale spoken dialogue system.
In Pro-ceedings of Interspeech?11.L.
Daubigney, M. Geist, S. Chandramohan, andO.
Pietquin.
2012.
A Comprehensive Reinforce-ment Learning Framework for Dialogue Manage-ment Optimisation.
IEEE Journal of Selected Topicsin Signal Processing, 6(8):891?902.M.
Gas?ic?, F.
Jurc??
?c?ek, S. Keizer, F. Mairesse, B. Thom-son, K. Yu, and S. Young.
2010.
Gaussian pro-cesses for fast policy optimisation of POMDP-baseddialogue managers.
In Proceedings of SIGdial?10.A.
Graesser, P. Chipman, B. Haynes, and A. Olney.2005.
Autotutor: An intelligent tutoring systemwith mixed-initiative dialogue.
Education, IEEETransactions on, 48(4):612?618.T.
Heift and M. Schulze.
2007.
Errors and intelligencein computer-assisted language learning: Parsersand pedagogues, volume 2.
Psychology Press.Ana Iglesias, Paloma Mart?
?nez, Ricardo Aler, and Fer-nando Ferna?ndez.
2009.
Learning teaching strate-gies in an adaptive and intelligent educational sys-tem through reinforcement learning.
Applied Intel-ligence, 31(1):89?106.H.
Jaeger.
2001.
The ?echo state?
approach toanalysing and training recurrent neural networks.Technical report, Technical Report GMD Report148, German National Research Center for Informa-tion Technology.K.
Koedinger, J. Anderson, W. Hadley, M. Mark, et al1997.
Intelligent tutoring goes to school in the bigcity.
International Journal of Artificial Intelligencein Education (IJAIED), 8:30?43.M.
Lagoudakis and R. Parr.
2003.
Least-squares pol-icy iteration.
The Journal of Machine Learning Re-search, 4:1107?1149.D.
Litman and S. Silliman.
2004.
Itspoke: An intel-ligent tutoring spoken dialogue system.
In Demon-stration Papers at HLT-NAACL 2004, pages 5?8.
As-sociation for Computational Linguistics.J.
Mostow and G. Aist.
2001.
Evaluating tutors thatlisten: an overview of project listen.
In Smart ma-chines in education, pages 169?234.
MIT Press.R.
Sutton and A. Barto.
1998.
Reinforcement learning:An introduction.
The MIT press.I.
Szita, V. Gyenes, and A.
Lo?rincz.
2006.
Reinforce-ment learning with echo state networks.
ArtificialNeural Networks?ICANN 2006, pages 830?839.K.
Vanlehn, C. Lynch, K. Schulze, J. Shapiro,R.
Shelby, L. Taylor, D. Treacy, A. Weinstein, andM.
Wintersgill.
2005.
The andes physics tutoringsystem: Lessons learned.
International Journal ofArtificial Intelligence in Education, 15(3):147?204.S.
Young, J. Schatzmann, B. Thomson, H. Ye, andK.
Weilhammer.
2006.
The HIS dialogue manager.In Proceedings of IEEE/ACL Workshop on SpokenLanguage Technology (SLT?06).S.
Young, M. Gasic, S. Keizer, F. Mairesse, J. Schatz-mann, B. Thomson, and K. Yu.
2010.
The hid-den information state model: A practical frame-work for POMDP-based spoken dialogue manage-ment.
Computer Speech & Language, 24(2):150?174.106
