Getting More Mileage from Web Text Sources for ConversationalSpeech Language Modeling using Class-Dependent MixturesIvan Bulyko, Mari OstendorfDepartment of Electrical EngineeringUniversity of Washington, Seattle, WA 98195.
{bulyko,mo}@ssli.ee.washington.eduAndreas StolckeSRI InternationalMenlo Park, CA 94025.stolcke@speech.sri.comAbstractSources of training data suitable for languagemodeling of conversational speech are limited.In this paper, we show how training data can besupplemented with text from the web filtered tomatch the style and/or topic of the target recog-nition task, but also that it is possible to get big-ger performance gains from the data by usingclass-dependent interpolation of N-grams.1 IntroductionLanguage models constitute one of the key componentsin modern speech recognition systems.
Training an N-gram language model, the most commonly used type ofmodel, requires large quantities of text that is matchedto the target recognition task both in terms of style andtopic.
In tasks involving conversational speech the idealtraining material, i.e.
transcripts of conversational speech,is costly to produce, which limits the amount of trainingdata currently available.Methods have been developed for the purpose of lan-guage model adaptation, i.e.
the adaptation of an exist-ing model to new topics, domains, or tasks for whichlittle or no training material may be available.
Sinceout-of-domain data can contain relevant as well as irrele-vant information, various methods are used to identify themost relevant portions of the out-of-domain data prior tocombination.
Past work on pre-selection has been basedon word frequency counts (Rudnicky, 1995), probabil-ity (or perplexity) of word or part-of-speech sequences(Iyer and Ostendorf, 1999), latent semantic analysis (Bel-legarda, 1998), and information retrieval techniques (Ma-hajan et al, 1999; Iyer and Ostendorf, 1999).
Perplexity-based clustering has also been used for defining topic-specific subsets of in-domain data (Clarkson and Robin-son, 1997; Martin et al 1997), and test set perplexityhas been used to prune documents from a training corpus(Klakow, 2000).
The most common method for using theadditional text sources is to train separate language mod-els on a small amount of in-domain and large amountsof out-of-domain data and to combine them by interpola-tion, also referred to as mixtures of language models.
Thetechnique was reported by IBM in 1995 (Liu et al 1995),and has been used by many sites since then.
An alter-native approach involves decomposition of the languagemodel into a class n-gram for interpolation (Iyer and Os-tendorf, 1997; Ries, 1997), allowing content words to beinterpolated with different weights than filled pauses, forexample, which gives an improvement over standard mix-ture modeling for conversational speech.Recently researchers have turned to the World WideWeb as an additional source of training data for languagemodeling.
For ?just-in-time?
language modeling (Bergerand Miller, 1998), adaptation data is obtained by submit-ting words from initial hypotheses of user utterances asqueries to a web search engine.
Their queries, however,treated words as individual tokens and ignored functionwords.
Such a search strategy typically generates text ofa non-conversational style, hence not ideally suited forASR.
In (Zhu and Rosenfeld, 2001), instead of down-loading the actual web pages, the authors retrieved N-gram counts provided by the search engine.
Such an ap-proach generates valuable statistics but limits the set ofN-grams to ones occurring in the baseline model.In this paper, we present an approach to extracting ad-ditional training data from the web by searching for textthat is better matched to a conversational speaking style.We also show how we can make better use of this newdata by applying class-dependent interpolation.2 Collecting Text from the WebThe amount of text available on the web is enormous(over 3 billion web pages are indexed via Google alone)and continues to grow.
Most of the text on the web isnon-conversational, but there is a fair amount of chat-likematerial that is similar to conversational speech thoughoften omitting disfluencies.
This was our primary targetwhen extracting data from the web.
Queries submitted toGoogle were composed of N-grams that occur most fre-quently in the switchboard training corpus, e.g.
?I neverthought I would?, ?I would think so?, etc.
We weresearching for the exact match to one or more of theseN-grams within the text of the web pages.
Web pagesreturned by Google for the most part consisted of conver-sational style phrases like ?we were friends but we don?tactually have a relationship?
and ?well I actually I I reallyhaven?t seen her for years.
?We used a slightly different search strategy when col-lecting topic-specific data.
First we extended the base-line vocabulary with words from a small in-domain train-ing corpus (Schwarm and Ostendorf, 2002), and then weused N-grams with these new words in our web queries,e.g.
?wireless mikes like?, ?I know that recognizer?
fora meeting transcription task (Morgan et al 2001).
Webpages returned by Google mostly contained technical ma-terial related to topics similar to what was discussed in themeetings, e.g.
?we were inspired by the weighted countscheme...?, ?for our experiments we used the Bellman-Ford algorithm...?, etc.The retrieved web pages were filtered before their con-tent could be used for language modeling.
First westripped the HTML tags and ignored any pages with avery high OOV rate.
We then piped the text througha maximum entropy sentence boundary detector (Rat-naparkhi, 1996) and performed text normalization usingNSW tools (Sproat et al 2001).3 Class-dependent Mixture of LMsLinear interpolation is a standard approach to combin-ing language models, where the probability of a wordwi given history h is computed as a linear combinationof the corresponding N-gram probabilities from S dif-ferent models: p(wi|h) =?s?S ?sps(wi|h).
Depend-ing on how much adaptation data is available it may bebeneficial to estimate a larger number of mixture weights?s (more than one per data source) in order to handlesource mismatch, specifically letting the mixture weightdepend on the context h. One approach is to use a mixtureweight corresponding to the source posterior probability?s(h) = p(s|h) (Weintraub et al 1996).
Here, we insteadchoose to let the weight vary as a function of the previousword class, i.e.
p(wi|h) =?s?S ?s(c(wi?1))ps(wi|h),where classes c(wi?1) are part-of-speech tags except forthe 100 most frequent words which form their own indi-vidual classes.
Such a scheme can generalize across do-mains by tapping into the syntactic structure (POS tags),already shown to be useful for cross-domain languagemodeling (Iyer and Ostendorf, 1997), and at the sametime target conversational speech since the top 100 wordscover 70% of tokens in Switchboard training corpus.Combining several N-grams can produce a model witha very large number of parameters, which is costly in de-coding.
In such cases N-grams are typically pruned.
Herewe use entropy-based pruning (Stolcke, 1998) after mix-ing unpruned models, and reduce the model aggressivelyto about 15% of its original size.
The same pruning pa-rameters were applied to all models in our experiments.4 ExperimentsWe evaluated on two tasks: 1) Switchboard (Godfrey etal., 1992), specifically the HUB5 eval 2001 set having atotal of 60K words spoken by 120 speakers, and 2) anICSI Meeting recorder (Morgan et al 2001) eval set hav-ing a total of 44K words spoken by 25 speakers.
Bothsets featured spontaneous conversational speech.
Therewere 45K words of held-out data for each task.Text corpora of conversational telephone speech(CTS) available for training language models consistedof Switchboard, Callhome English, and Switchboard-cellular, a total of 3 million words.
In addition to that weused 150 million words of Broadcast News (BN) tran-scripts, and we collected 191 million words of ?con-versational?
text from the web.
For the Meetings task,there were 200K words of meeting transcripts availablefor training, and we collected 28 million words of ?topic-related?
text from the web.The experiments were conducted using the SRI largevocabulary speech recognizer (Stolcke et al 2000) inthe N-best rescoring mode.
A baseline bigram languagemodel was used to generate N-best lists, which were thenrescored with various trigram models.Table 1 shows word error rates (WER) on the HUB5test set, comparing performance of the class-based mix-ture against standard (i.e.
class-independent) interpola-tion.
The class-based mixture gave better results in allcases except when only CTS sources were used, probablybecause these sources are similar to each other and theclass-based mixture is mainly useful when data sourcesare more diverse.
We also obtained lower WER by usingthe web data instead of BN, which indicates that the webdata is better matched to our task (i.e.
it is more ?conver-sational?).
If training data is completely arbitrary, then itsbenefits to the recognition task are minimal, as shown byan example of using a 66M-word corpus collected fromrandom web pages.
The baseline Switchboard modelgave test set perplexity of 96, which is reduced to 87 witha standard mixture CTS and BN data, reduced further to83 by adding the web data, and to a best case of 82 withclass-dependent interpolation and the added web data.Increasing the amount of web training data from 61Mto 191M gave relatively small performance gains.
We?trimmed?
the 191M-word web corpus down to 61Mwords by choosing documents with lowest perplexityaccording to the combined CTS model, yielding the?Web2?
data source.
The model that used Web2 gavethe same WER as the one trained with the original 61Mweb corpus.
It could be that the web text obtainedwith ?Google?
filtering is fairly homogeneous, so littleis gained by further perplexity filtering.
Or, it could bethat when choosing better matched data, we also excludenew N-grams that may occur only in testing.Table 1: HUB5 (eval 2001) WER results using standardand class-based mixtures.LM Data Sources Std.
mix Class mixBaseline CTS 38.9% 38.9%+ 150M BN 37.9% 37.8%+ 66M Web (Random) 38.6% 38.3%+ 61M Web 37.7% 37.6%+ 191M Web 37.6% 37.4%+ 150M BN + 61M Web 37.7% 37.3%+ 150M BN + 191M Web 37.5% 37.2%+ 150M BN + 61M Web2 37.7% 37.3%Table 2: Meetings results (WER).LM Data Sources Std.
mix Class mixBaseline 38.2%+ 0.2M Meetings 37.2% 36.9%+ 28M Web (Topic) 36.9% 36.7%+ Meetings + Web (Topic) 36.2% 35.9%Results on the Meeting test set are shown in Table2, where the baseline model was trained on CTS andBN sources.
As in the HUB5 experiments, the class-based mixture outperformed standard interpolation.
Weachieved lower WER by using the web data instead ofthe meeting transcripts, but the best results are obtainedby using all data sources.
Language model perplexity isreduced from 122 for the baseline to a best case of 95.We also tried different class assignments for the class-based mixture on the HUB5 set and we found that usingautomatically derived classes instead of part-of-speechtags does not lead to performance degradation as longas we allocate individual classes for the top 100 words.Automatic class mapping can make class-based mixturesfeasible for other languages where part-of-speech tags aredifficult to derive.5 ConclusionsIn summary, we have shown that, if filtered, web textcan be successfully used for training language modelsof conversational speech, outperforming some other out-of-domain (BN) and small domain-specific (Meetings)sources of data.
We have also found that by combin-ing LMs from different domains with class-dependent in-terpolation (particularly when each of the top 100 wordsforms its own class), we achieve lower WER than if weuse the standard approach where mixture weights dependonly on the data source.
Recognition experiments show asignificant reduction in WER (1.3-2.3% absolute) due toadditional training data and class-based interpolation.ReferencesJ.
Bellegarda.
1998.
Exploiting both local and global con-straints for multispan statistical language modeling.
In Proc.ICASSP, pages II:677?680.A.
Berger and R. Miller.
1998.
Just-in-time language modeling.In Proc.
ICASSP, pages II:705?708.P.
Clarkson and A. Robinson.
1997.
Language model adapta-tion using mixtures and an exponentially decaying cache.
InProc.
ICASSP, pages II:799?802.J.
Godfrey, E. Holliman, and J. McDaniel.
1992.
Switchboard:telephone speech corpus for research and development.
InProc.
ICASSP, pages I:517?520.R.
Iyer and M. Ostendorf.
1997.
Transforming out-of-domainestimates to improve in-domain language models.
In Proc.Eurospeech, volume 4, pages 1975?1978.R.
Iyer and M. Ostendorf.
1999.
Relevance weighting forcombining multi-domain data for n-gram language model-ing.
Computer Speech and Language, 13(3):267?282.D.
Klakow.
2000.
Selecting articles from the language modeltraining corpus.
In Proc.
ICASSP, pages III:1695?1698.F.
Liu et al 1995.
IBM Switchboard progress and evaluationsite report.
In LVCSR Workshop, Gaithersburg, MD.
Na-tional Institute of Standards and Technology.M.
Mahajan, D. Beeferman, and D. Huang.
1999.
Improvedtopic-dependent language modeling using information re-trieval techniques.
In Proc.
ICASSP, pages I:541?544.S.
Martin et al 1997.
Adaptive topic-dependent languagemodeling using word-based varigrams.
In Proc.
Eurospeech,pages 3:1447?1450.N.
Morgan et al 2001.
The meeting project at ICSI.
In Proc.Conf.
on Human Language Technology, pages 246?252.A.
Ratnaparkhi.
1996.
A maximum entropy part-of-speech tag-ger.
In Proc.
Empirical Methods in Natural Language Pro-cessing Conference, pages 133?141.K.
Ries.
1997.
A class based approach to domain adaptationand constraint integration for empirical m-gram models.
InProc.
Eurospeech, pages 4:1983?1986.A.
Rudnicky.
1995.
Language modeling with limited domaindata.
In Proc.
ARPA Spoken Language Technology Work-shop, pages 66?69.S.
Schwarm and M. Ostendorf.
2002.
Text normalization withvaried data sources for conversational speech language mod-eling.
In Proc.
ICASSP, pages I:789?792.R.
Sproat et al 2001.
Normalization of non-standard words.Computer Speech and Language, 15(3):287?333.A.
Stolcke et al 2000.
The SRI March 2000 Hub-5 conver-sational speech transcription system.
In Proc.
NIST SpeechTranscription Workshop.A.
Stolcke.
1998.
Entropy-based pruning of backoff languagemodels.
In Proc.
DARPA Broadcast News Transcription andUnderstanding Workshop, pages 270?274.M.
Weintraub et al 1996.
LM95 Project Report: Fast trainingand portability.
Technical Report 1, Center for Language andSpeech Processing, Johns Hopkins University, Baltimore.X.
Zhu and R. Rosenfeld.
2001.
Improving trigram languagemodeling with the world wide web.
In Proc.
ICASSP, pagesI:533?536.
