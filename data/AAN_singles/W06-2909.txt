Proceedings of the 10th Conference on Computational Natural Language Learning (CoNLL-X),pages 61?68, New York City, June 2006. c?2006 Association for Computational LinguisticsSemantic Role Labeling via Tree Kernel Joint InferenceAlessandro Moschitti, Daniele Pighin and Roberto BasiliDepartment of Computer ScienceUniversity of Rome ?Tor Vergata?00133 Rome, Italy{moschitti,basili}@info.uniroma2.itdaniele.pighin@gmail.comAbstractRecent work on Semantic Role Labeling(SRL) has shown that to achieve highaccuracy a joint inference on the wholepredicate argument structure should be ap-plied.
In this paper, we used syntactic sub-trees that span potential argument struc-tures of the target predicate in tree ker-nel functions.
This allows Support Vec-tor Machines to discern between correctand incorrect predicate structures and tore-rank them based on the joint probabil-ity of their arguments.
Experiments on thePropBank data show that both classifica-tion and re-ranking based on tree kernelscan improve SRL systems.1 IntroductionRecent work on Semantic Role Labeling (SRL)(Carreras and Ma`rquez, 2005) has shown that toachieve high labeling accuracy a joint inference onthe whole predicate argument structure should beapplied.
For this purpose, we need to extract fea-tures from the sentence?s syntactic parse tree thatencodes the target semantic structure.
This task israther complex since we do not exactly know whichare the syntactic clues that capture the relation be-tween the predicate and its arguments.
For exam-ple, to detect the interesting context, the modelingof syntax/semantics-based features should take intoaccount linguistic aspects like ancestor nodes or se-mantic dependencies (Toutanova et al, 2004).A viable approach to generate a large number offeatures has been proposed in (Collins and Duffy,2002), where convolution kernels were used to im-plicitly define a tree substructure space.
The selec-tion of the relevant structural features was left to theVoted Perceptron learning algorithm.
Such success-ful experimentation shows that tree kernels are verypromising for automatic feature engineering, espe-cially when the available knowledge about the phe-nomenon is limited.In a similar way, we can model SRL systems withtree kernels to generate large feature spaces.
Morein detail, most SRL systems split the labeling pro-cess into two different steps: Boundary Detection(i.e.
to determine the text boundaries of predicatearguments) and Role Classification (i.e.
labelingsuch arguments with a semantic role, e.g.
Arg0 orArg1 as defined in (Kingsbury and Palmer, 2002)).The former relates to the detection of syntactic parsetree nodes associated with constituents that corre-spond to arguments, whereas the latter considers theboundary nodes for the assignment of the suitablelabel.
Both steps require the design and extractionof features from parse trees.
As capturing the tightlyinterdependent relations among a predicate and itsarguments is a complex task, we can apply tree ker-nels on the subtrees that span the whole predicateargument structure to generate the feature space ofall the possible subtrees.In this paper, we apply the traditional bound-ary (TBC) and role (TRC) classifiers (Pradhanet al, 2005a), which are based on binary predi-cate/argument relations, to label all parse tree nodescorresponding to potential arguments.
Then, we ex-61tract the subtrees which span the predicate-argumentdependencies of such arguments, i.e.
ArgumentSpanning Trees (AST s).
These are used in a treekernel function to generate all possible substructuresthat encode n-ary argument relations, i.e.
we carryout an automatic feature engineering process.To validate our approach, we experimented withour model and Support Vector Machines for the clas-sification of valid and invalid AST s. The resultsshow that this classification problem can be learnedwith high accuracy.
Moreover, we modeled SRL as are-ranking task in line with (Toutanova et al, 2005).The large number of complex features provided bytree kernels for structured learning allows SVMs toreach the state-of-the-art accuracy.The paper is organized as follows: Section 2 intro-duces the Semantic Role Labeling based on SVMsand the tree kernel spaces; Section 3 formally de-fines the AST s and the algorithm for their classifi-cation and re-ranking; Section 4 shows the compara-tive results between our approach and the traditionalone; Section 5 presents the related work; and finally,Section 6 summarizes the conclusions.2 Semantic Role LabelingIn the last years, several machine learning ap-proaches have been developed for automatic rolelabeling, e.g.
(Gildea and Jurasfky, 2002; Prad-han et al, 2005a).
Their common characteristic isthe adoption of attribute-value representations forpredicate-argument structures.
Accordingly, our ba-sic system is similar to the one proposed in (Pradhanet al, 2005a) and it is hereby described.We use a boundary detection classifier (for anyrole type) to derive the words compounding an ar-gument and a multiclassifier to assign the roles (e.g.Arg0 or ArgM) described in PropBank (Kingsburyand Palmer, 2002)).
To prepare the training data forboth classifiers, we used the following algorithm:1.
Given a sentence from the training-set, generatea full syntactic parse tree;2.
Let P and A be respectively the set of predicatesand the set of parse-tree nodes (i.e.
the potential ar-guments);3.
For each pair ?p, a?
?
P ?A:- extract the feature representation set, Fp,a;- if the subtree rooted in a covers exactly thewords of one argument of p, put Fp,a in the T+set (positive examples), otherwise put it in theT?
set (negative examples).The outputs of the above algorithm are the T+ andT?
sets.
These sets can be directly used to train aboundary classifier (e.g.
an SVM).
Regarding theargument type classifier, a binary labeler for a role r(e.g.
an SVM) can be trained on the T+r , i.e.
its pos-itive examples and T?r , i.e.
its negative examples,where T+ = T+r ?
T?r , according to the ONE-vs-ALL scheme.
The binary classifiers are then usedto build a general role multiclassifier by simply se-lecting the argument associated with the maximumamong the SVM scores.Regarding the design of features for predicate-argument pairs, we can use the attribute-values de-fined in (Gildea and Jurasfky, 2002) or tree struc-tures (Moschitti, 2004).
Although we focus onthe latter approach, a short description of the for-mer is still relevant as they are used by TBC andTRC.
They include the Phrase Type, PredicateWord, Head Word, Governing Category, Positionand Voice features.
For example, the Phrase Typeindicates the syntactic type of the phrase labeled asa predicate argument and the Parse Tree Path con-tains the path in the parse tree between the predicateand the argument phrase, expressed as a sequence ofnonterminal labels linked by direction (up or down)symbols, e.g.
V ?
VP ?
NP.A viable alternative to manual design of syntac-tic features is the use of tree-kernel functions.
Theseimplicitly define a feature space based on all possi-ble tree substructures.
Given two trees T1 and T2, in-stead of representing them with the whole fragmentspace, we can apply the kernel function to evaluatethe number of common fragments.Formally, given a tree fragment space F ={f1, f2, .
.
.
, f|F|}, the indicator function Ii(n)is equal to 1 if the target fi is rooted atnode n and equal to 0 otherwise.
A tree-kernel function over t1 and t2 is Kt(t1, t2) =?n1?Nt1?n2?Nt2 ?
(n1, n2), where Nt1 and Nt2are the sets of the t1?s and t2?s nodes, respectively.
Inturn ?
(n1, n2) =?|F|i=1 ?l(fi)Ii(n1)Ii(n2), where0 ?
?
?
1 and l(fi) is the height of the subtreefi.
Thus ?l(fi) assigns a lower weight to larger frag-62SNP VPPRPJohnVP CC VPVB NPandVB NPtookDT NNthe book readPRP$ NNits titleSentence Parse-TreeSNP VPPRPJohnVPVB NPtookDT NNthe booktook{ARG0, ARG1}SNP VPPRPJohnVPVB NPreadPRP$ NNits titleread{ARG0, ARG1}Figure 1: A sentence parse tree with two argument spanning trees (AST s)ments.
When ?
= 1, ?
is equal to the number ofcommon fragments rooted at nodes n1 and n2.
Asdescribed in (Collins and Duffy, 2002), ?
can becomputed in O(|Nt1 | ?
|Nt2 |).3 Tree kernel-based classification ofPredicate Argument StructuresTraditional semantic role labeling systems extractfeatures from pairs of nodes corresponding to apredicate and one of its argument, respectively.Thus, they focus on only binary relations to makeclassification decisions.
This information is poorerthan the one expressed by the whole predicate ar-gument structure.
As an alternative we can selectthe set of potential arguments (potential argumentnodes) of a predicate and extract features from them.The number of the candidate argument sets is ex-ponential, thus we should consider only those cor-responding to the most probable correct argumentstructures.The usual approach (Toutanova et al, 2005) usesa traditional boundary classifier (TBC) to select theset of potential argument nodes.
Such set can be as-sociated with a subtree which in turn can be classi-fied by means of a tree kernel function.
This func-tion intuitively measures to what extent a given can-didate subtree is compatible with the subtree of acorrect predicate argument structure.
We can use itto define two different learning problems: (a) thesimple classification of correct and incorrect pred-icate argument structures and (b) given the best mstructures, we can train a re-ranker algorithm able toexploit argument inter-dependencies.3.1 The Argument Spanning Trees (AST s)We consider predicate argument structures anno-tated in PropBank along with the correspondingTreeBank data as our object space.
Given the targetpredicate node p and a node subset s = {n1, .., nk}of the parse tree t, we define as the spanning treeroot r the lowest common ancestor of n1, .., nk andp.
The node set spanning tree (NST ) ps is the sub-tree of t rooted in r from which the nodes that areneither ancestors nor descendants of any ni or p areremoved.Since predicate arguments are associated withtree nodes (i.e.
they exactly fit into syntacticconstituents), we can define the Argument Span-ning Tree (AST ) of a predicate argument set,{p, {a1, .., an}}, as the NST over such nodes,i.e.
p{a1,..,an}.
An AST corresponds to the min-imal subtree whose leaves are all and only thewords compounding the arguments and the predi-cate.
For example, Figure 1 shows the parse treeof the sentence "John took the book and readits title".
took{Arg0,Arg1} and read{Arg0,Arg1}are two AST structures associated with the twopredicates took and read, respectively.
All the otherpossible subtrees, i.e.
NST s, are not valid AST sfor these two predicates.
Note that classifying ps inAST or NST for each node subset s of t is equiva-lent to solve the boundary detection problem.The critical points for the AST classification are:(1) how to design suitable features for the charac-terization of valid structures.
This requires a carefullinguistic investigation about their significant prop-erties.
(2) How to deal with the exponential numberof NST s.The first problem can be addressed by means oftree kernels over the AST s. Tree kernel spaces arean alternative to the manual feature design as thelearning machine, (e.g.
SVMs) can select the mostrelevant features from a high dimensional space.
Inother words, we can use a tree kernel function toestimate the similarity between two AST s (see Sec-63Figure 2: Two-step boundary classification.
a) Sentence tree; b) Two candidate ASTs; c) Extended AST -Ord labelingtion 2), hence avoiding to define explicit features.The second problem can be approached in twoways:(1) We can increase the recall of TBC to enlarge theset of candidate arguments.
From such set, we canextract correct and incorrect argument structures.
Asthe number of such structures will be rather small,we can apply the AST classifier to detect the cor-rect ones.
(2) We can consider the classification probabilityprovided by TBC and TRC (Pradhan et al, 2005a)and select the m most probable structures.
Then, wecan apply a re-ranking approach based on SVMs andtree kernels.The re-ranking approach is the most promisingone as suggested in (Toutanova et al, 2005) but itdoes not clearly reveal if tree kernels can be usedto learn the difference between correct or incorrectargument structures.
Thus it is interesting to studyboth the above approaches.3.2 NST ClassificationAs we cannot classify all possible candidate argu-ment structures, we apply the AST classifier just todetect the correct structures from a set of overlap-ping arguments.
Given two nodes n1 and n2 of anNST , they overlap if either n1 is ancestor of n2 orvice versa.
NST s that contain overlapping nodesare not valid AST s but subtrees of NSTs may bevalid ASTs.
Assuming this, we define s as the setof potential argument nodes and we create two nodesets s1 = s ?
{n1} and s2 = s ?
{n2}.
By classi-fying the two new NST s ps1 and ps2 with the ASTclassifier, we can select the correct structures.
Ofcourse, this procedure can be generalized to a set ofoverlapping nodes greater than 2.
However, consid-ering that the Precision of TBC is generally high,the number of overlapping nodes is usually small.Figure 2 shows a working example of the multi-stage classifier.
In Frame (a), TBC labels as po-tential arguments (circled nodes) three overlappingnodes related to Arg1.
This leads to two possiblenon-overlapping solutions (Frame (b)) but only thefirst one is correct.
In fact, according to the secondone the propositional phrase ?of the book?
would beincorrectly attached to the verbal predicate, i.e.
incontrast with the parse tree.
The AST classifier, ap-plied to the two NST s, is expected to detect thisinconsistency and provide the correct output.3.3 Re-ranking NST s with Tree KernelsTo implement the re-ranking model, we follow theapproach described in (Toutanova et al, 2005).First, we use SVMs to implement the boundaryTBC and role TRC local classifiers.
As SVMs donot provide probabilistic output, we use the Platt?salgorithm (Platt, 2000) and its revised version (Linet al, 2003) to trasform scores into probabilities.Second, we combine TBC and TRC probabil-ities to obtain the m most likely sequences s oftree nodes annotated with semantic roles.
As argu-ment constituents of the same verb cannot overlap,we generate sequences that respect such node con-straint.
We adopt the same algorithm described in(Toutanova et al, 2005).
We start from the leavesand we select the m sequences that respect the con-straints and at the same time have the highest jointprobability of TBC and TRC.Third, we extract the following feature represen-tation:(a) The AST s associated with the predicate argu-ment structures.
To make faster the learning processand to try to only capture the most relevant features,we also experimented with a compact version of the64AST which is pruned at the level of argument nodes.
(b) Attribute value features (standard features) re-lated to the whole predicate structure.
These includethe features for each arguments (Gildea and Juras-fky, 2002) and global features like the sequence ofargument labels, e.g.
?Arg0, Arg1, ArgM?.Finally, we prepare the training examples for there-ranker considering the m best annotations of eachpredicate structure.
We use the approach adoptedin (Shen et al, 2003), which generates all possiblepairs from the m examples, i.e.
(m2)pairs.
Each pairis assigned to a positive example if the first mem-ber of the pair has a higher score than the secondmember.
The score that we use is the F1 measureof the annotated structure with respect to the goldstandard.
More in detail, given training/testing ex-amples ei = ?t1i , t2i , v1i , v2i ?, where t1i and t2i are twoAST s and v1i and v2i are two feature vectors associ-ated with two candidate predicate structures s1 ands2, we define the following kernels:1) Ktr(e1, e2) = Kt(t11, t12) +Kt(t21, t22)?Kt(t11, t22)?Kt(t21, t12),where tji is the j-th AST of the pair ei, Kt is thetree kernel function defined in Section 2 and i, j ?
{1, 2}.2) Kpr(e1, e2) = Kp(v11, v12) +Kp(v21, v22)?Kp(v11, v22)?Kp(v21, v12),where vji is the j-th feature vector of the pair ei andKp is the polynomial kernel applied to such vectors.The final kernel that we use for re-ranking is thefollowing:K(e1, e2) = Ktr(e1, e2)|Ktr(e1, e2)| +Kpr(e1, e2)|Kpr(e1, e2)|Regarding tree kernel feature engineering, thenext section show how we can generate more effec-tive features given an established kernel function.3.4 Tree kernel feature engineeringConsider the Frame (b) of Figure 2, it shows twoperfectly identical NST s, consequently, their frag-ments will also be equal.
This prevents the algorithmto learn something from such examples.
To solve theproblem, we can enrich the NSTs by marking theirargument nodes with a progressive number, startingfrom the leftmost argument.
For example, in the firstNST of Frame (c), we mark as NP-0 and NP-1 thefirst and second argument nodes whereas in the sec-ond NST we trasform the three argument node la-bels in NP-0, NP-1 and PP-2.
We will refer to theresulting structure as a AST -Ord (ordinal number).This simple modification allows the tree kernel togenerate different argument structures for the aboveNST s. For example, from the first NST in Fig-ure 2.c, the fragments [NP-1 [NP][PP]], [NP[DT][NN]] and [PP [IN][NP]] are gener-ated.
They do not match anymore with the [NP-0[NP][PP]], [NP-1 [DT][NN]] and [PP-2[IN][NP]] fragments generated from the secondNST in Figure 2.c.Additionally, it should be noted that the semanticinformation provided by the role type can remark-ably help the detection of correct or incorrect predi-cate argument structures.
Thus, we can enrich the ar-gument node label with the role type, e.g.
the NP-0and NP-1 of the correct AST of Figure 2.c becomeNP-Arg0 and NP-Arg1 (not shown in the figure).We refer to this structure as AST -Arg.
Of course,to apply the AST -Arg classifier, we need that TRClabels the arguments detected by TBC.4 The experimentsThe experiments were carried out within the set-ting defined in the CoNLL-2005 Shared Task(Carreras and Ma`rquez, 2005).
In particular,we adopted the Charniak parse trees available atwww.lsi.upc.edu/?srlconll/ along with the of-ficial performance evaluator.All the experiments were performed withthe SVM-light-TK software available athttp://ai-nlp.info.uniroma2.it/moschitti/which encodes ST and SST kernels in SVM-light(Joachims, 1999).
For TBC and TRC, we used thelinear kernel with a regularization parameter (option-c) equal to 1.
A cost factor (option -j) of 10 wasadopted for TBC to have a higher Recall, whereasfor TRC, the cost factor was parameterized accord-ing to the maximal accuracy of each argument classon the validation set.
For the AST -based classifierswe used a ?
equal to 0.4 (see (Moschitti, 2004)).65Section 21 Section 23AST Class.
P. R. F1 P. R. F1?
69.8 77.9 73.7 62.2 77.1 68.9Ord 73.7 81.2 77.3 63.7 80.6 71.2Arg 73.6 84.7 78.7 64.2 82.3 72.1Table 1: AST , AST -Ord, and AST -Arg perfor-mance on sections 21 and 23.4.1 Classification of whole predicate argumentstructuresIn these experiments, we trained TBC on sections02-08 whereas, to achieve a very accurate role clas-sifier, we trained TRC on all sections 02-21.
Totrain the AST , AST -Ord (AST with ordinal num-bers in the argument nodes), and AST -Arg (ASTwith argument type in the argument nodes) clas-sifiers, we applied the TBC and TRC over sec-tions 09-20.
Then, we considered all the structureswhose automatic annotation showed at least an ar-gument overlap.
From these, we extracted 30,220valid AST s and 28,143 non-valid AST s, for a totalof 183,642 arguments.First, we evaluate the accuracy of the AST -basedclassifiers by extracting 1,975 AST s and 2,220 non-AST s from Section 21 and the 2,159 AST s and3,461 non-AST s from Section 23.
The accuracyderived on Section 21 is an upperbound for our clas-sifiers since it is obtained using an ideal syntacticparser (the Charniak?s parser was trained also onSection 21) and an ideal role classifier.Table 1 shows Precision, Recall and F1 mea-sures of the AST -based classifiers over the aboveNSTs.
Rows 2, 3 and 4 report the performance ofAST , AST -Ord, and AST -Arg classifiers, respec-tively.
We note that: (a) The impact of parsing ac-curacy is shown by the gap of about 6% points be-tween sections 21 and 23.
(b) The ordinal number-ing of arguments (Ord) and the role type informa-tion (Arg) provide tree kernels with more meaning-ful fragments since they improve the basic modelof about 4%.
(c) The deeper semantic informationgenerated by the Arg labels provides useful clues toselect correct predicate argument structures since itimproves the Ord model on both sections.Second, we measured the impact of the AST -based classifiers on the accuracy of both phases ofsemantic role labeling.
Table 2 reports the resultson sections 21 and 23.
For each of them, Precision,Recall and F1 of different approaches to bound-ary identification (bnd) and to the complete task,i.e.
boundary and role classification (bnd+class)are shown.
Such approaches are based on differ-ent strategies to remove the overlaps, i.e.
with theAST , AST -Ord and AST -Arg classifiers and usingthe baseline (RND), i.e.
a random selection of non-overlapping structures.
The baseline corresponds tothe system based on TBC and TRC1.We note that: (a) for any model, the boundary de-tection F1 on Section 21 is about 10 points higherthan the F1 on Section 23 (e.g.
87.0% vs. 77.9%for RND).
As expected the parse tree quality is veryimportant to detect argument boundaries.
(b) On thereal test (Section 23) the classification introduces la-beling errors which decrease the accuracy of about5% (77.9 vs 72.9 for RND).
(c) The Ord and Argapproaches constantly improve the baseline F1 ofabout 1%.
Such poor impact does not surprise asthe overlapping structures are a small percentage ofthe test set, thus the overall improvement cannot bevery high.Third, the comparison with the CoNLL 2005 re-sults (Carreras and Ma`rquez, 2005) can only becarried out with respect to the whole SRL task(bnd+class in table 2) since boundary detection ver-sus role classification is generally not provided inCoNLL 2005.
Moreover, our best global result, i.e.73.9%, was obtained under two severe experimentalfactors: a) the use of just 1/3 of the available train-ing set, and b) the usage of the linear SVM modelfor the TBC classifier, which is much faster than thepolynomial SVMs but also less accurate.
However,we note the promising results of the AST meta-classifier, which can be used with any of the bestfigure CoNLL systems.Finally, the overall results suggest that the treekernel model is robust to parse tree errors since pre-serves the same improvement across trees derivedwith different accuracy, i.e.
the semi-automatic treesof Section 21 and the automatic tree of Section 23.Moreover, it shows a high accuracy for the classi-fication of correct and incorrect AST s. This lastproperty is quite interesting as the best SRL systems1We needed to remove the overlaps from the baseline out-come in order to apply the CoNLL evaluator.66(Punyakanok et al, 2005; Toutanova et al, 2005;Pradhan et al, 2005b) were obtained by exploit-ing the information on the whole predicate argumentstructure.Next section shows our preliminary experimentson re-ranking using the AST kernel based approach.4.2 Re-ranking based on Tree KernelsIn these experiments, we used the output of TBCand TRC2 to provide an SVM tree kernel with aranked list of predicate argument structures.
More indetail, we applied a Viterbi-like algorithm to gener-ate the 20 most likely annotations for each predicatestructure, according to the joint probabilistic modelof TBC and TRC.
We sorted such structures basedon their F1 measure and used them to learn the SVMre-ranker described in 3.3.For training, we used Sections 12, 14, 15, 16and 24, which contain 24,729 predicate structures.For each of them, we considered the 5 annotationshaving the highest F1 score (i.e.
123,674 NST s)on the span of the 20 best annotations provided byViterbi algorithm.
With such structures, we ob-tained 294,296 pairs used to train the SVM-basedre-ranker.
As the number of such structures is verylarge the SVM training time was very high.
Thus,we sped up the learning process by using only theAST s associated with the core arguments.
From thetest sentences (which contain 5,267 structures), weextracted the 20 best Viterbi annotated structures,i.e.
102,343 (for a total of 315.531 pairs), whichwere used for the following experiments:First, we selected the best annotation (accordingto the F1 provided by the gold standard annotations)out of the 20 provided by the Viterbi?s algorithm.The resulting F1 of 88.59% is the upperbound of ourapproach.Second, we selected the top ranked annotation in-dicated by the Viterbi?s algorithm.
This provides ourbaseline F1 measure, i.e.
75.91%.
Such outcome isslightly higher than our official CoNLL result (Mos-chitti et al, 2005) obtained without converting SVMscores into probabilities.Third, we applied the SVM re-ranker to select2With the aim of improving the state-of-the-art, we appliedthe polynomial kernel for all basic classifiers, at this time.We used the models developed during our participation to theCoNLL 2005 shared task (Moschitti et al, 2005).the best structures according to the core roles.
Weachieved 80.68% which is practically equal to theresult obtained in (Punyakanok et al, 2005; Car-reras and Ma`rquez, 2005) for core roles, i.e.
81%.Their overall F1 which includes all the argumentswas 79.44%.
This confirms that the classification ofthe non-core roles is more complex than the otherarguments.Finally, the high computation time of the re-ranker prevented us to use the larger structureswhich include all arguments.
The major complexityissue was the slow training and classification timeof SVMs.
The time needed for tree kernel functionwas not so problematic as we could use the fast eval-uation proposed in (Moschitti, 2006).
This roughlyreduces the computation time to the one required bya polynomial kernel.
The real burden is therefore thelearning time of SVMs that is quadratic in the num-ber of training instances.
For example, to carry outthe re-ranking experiments required approximatelyone month of a 64 bits machine (2.4 GHz and 4GbRam).
To solve this problem, we are going to studythe impact on the accuracy of fast learning algo-rithms such as the Voted Perceptron.5 Related WorkRecently, many kernels for natural language applica-tions have been designed.
In what follows, we high-light their difference and properties.The tree kernel used in this article was proposedin (Collins and Duffy, 2002) for syntactic parsingre-ranking.
It was experimented with the VotedPerceptron and was shown to improve the syntac-tic parsing.
In (Cumby and Roth, 2003), a featuredescription language was used to extract structuralfeatures from the syntactic shallow parse trees asso-ciated with named entities.
The experiments on thenamed entity categorization showed that when thedescription language selects an adequate set of treefragments the Voted Perceptron algorithm increasesits classification accuracy.
The explanation was thatthe complete tree fragment set contains many irrel-evant features and may cause overfitting.
In (Pun-yakanok et al, 2005), a set of different syntacticparse trees, e.g.
the n best trees generated by theCharniak?s parser, were used to improve the SRLaccuracy.
These different sources of syntactic infor-mation were used to generate a set of different SRL67Section 21 Section 23bnd bnd+class bnd bnd+classAST Classifier RND AST Classifier RND AST Classifier RND AST Classifier RND- Ord Arg - Ord Arg - Ord Arg - Ord ArgP.
87.5 88.3 88.3 86.9 85.5 86.3 86.4 85.0 78.6 79.0 79.3 77.8 73.1 73.5 73.4 72.3R.
87.3 88.1 88.3 87.1 85.7 86.5 86.8 85.6 78.1 78.4 78.7 77.9 73.8 74.1 74.4 73.6F1 87.4 88.2 88.3 87.0 85.6 86.4 86.6 85.3 78.3 78.7 79.0 77.9 73.4 73.8 73.9 72.9Table 2: Semantic Role Labeling performance on automatic trees using AST -based classifiers.outputs.
A joint inference stage was applied to re-solve the inconsistency of the different outputs.
In(Toutanova et al, 2005), it was observed that thereare strong dependencies among the labels of the se-mantic argument nodes of a verb.
Thus, to approachthe problem, a re-ranking method of role sequenceslabeled by a TRC is applied.
In (Pradhan et al,2005b), some experiments were conducted on SRLsystems trained using different syntactic views.6 ConclusionsRecent work on Semantic Role Labeling has shownthat to achieve high labeling accuracy a joint in-ference on the whole predicate argument structureshould be applied.
As feature design for such task iscomplex, we can take advantage from kernel meth-ods to model our intuitive knowledge about the n-ary predicate argument relations.In this paper we have shown that we can exploitthe properties of tree kernels to engineer syntacticfeatures for the semantic role labeling task.
The ex-periments suggest that (1) the information relatedto the whole predicate argument structure is impor-tant as it can improve the state-of-the-art and (2)tree kernels can be used in a joint model to gen-erate relevant syntactic/semantic features.
The realdrawback is the computational complexity of work-ing with SVMs, thus the design of fast algorithm isan interesting future work.AcknowledgmentsThis research is partially supported by thePrestoSpace EU Project#: FP6-507336.ReferencesXavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction to theCoNLL-2005 shared task: Semantic role labeling.
In Pro-ceedings of CoNLL05.Michael Collins and Nigel Duffy.
2002.
New ranking algo-rithms for parsing and tagging: Kernels over discrete struc-tures, and the voted perceptron.
In ACL02.Chad Cumby and Dan Roth.
2003.
Kernel methods for re-lational learning.
In Proceedings of ICML03, Washington,DC, USA.Daniel Gildea and Daniel Jurasfky.
2002.
Automatic label-ing of semantic roles.
Computational Linguistic, 28(3):496?530.T.
Joachims.
1999.
Making large-scale SVM learning practical.In B. Scho?lkopf, C. Burges, and A. Smola, editors, Advancesin Kernel Methods - Support Vector Learning.Paul Kingsbury and Martha Palmer.
2002.
From Treebank toPropBank.
In Proceedings of LREC?02), Las Palmas, Spain.H.T.
Lin, C.J.
Lin, and R.C.
Weng.
2003.
A note on platt?sprobabilistic outputs for support vector machines.
Technicalreport, National Taiwan University.Alessandro Moschitti, Bonaventura Coppola, Daniele Pighin,and Roberto Basili.
2005.
Hierarchical semantic role label-ing.
In Proceedings of CoNLL05 shared task, Ann Arbor(MI), USA.Alessandro Moschitti.
2004.
A study on convolution kernelsfor shallow semantic parsing.
In Proceedings of ACL?04,Barcelona, Spain.Alessandro Moschitti.
2006.
Making tree kernels practicalfor natural language learning.
In Proceedings of EACL?06,Trento, Italy.J.
Platt.
2000.
Probabilistic outputs for support vector ma-chines and comparison to regularized likelihood methods.MIT Press.Sameer Pradhan, Kadri Hacioglu, Valeri Krugler, Wayne Ward,James H. Martin, and Daniel Jurafsky.
2005a.
Support vec-tor learning for semantic argument classification.
MachineLearning Journal.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, James Martin,and Daniel Jurafsky.
2005b.
Semantic role labeling usingdifferent syntactic views.
In Proceedings ACL?05.V.
Punyakanok, D. Roth, and W. Yih.
2005.
The necessity ofsyntactic parsing for semantic role labeling.
In Proceedingsof IJCAI 2005.Libin Shen, Anoop Sarkar, and Aravind Joshi.
2003.
Usingltag based features in parse reranking.
In Conference onEMNLP03, Sapporo, Japan.Kristina Toutanova, Penka Markova, and Christopher D. Man-ning.
2004.
The leaf projection path view of parse trees:Exploring string kernels for hpsg parse selection.
In In Pro-ceedings of EMNLP04.Kristina Toutanova, Aria Haghighi, and Christopher Manning.2005.
Joint learning improves semantic role labeling.
InProceedings of ACL05.68
