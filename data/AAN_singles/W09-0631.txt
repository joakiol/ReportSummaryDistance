Proceedings of the 12th European Workshop on Natural Language Generation, pages 185?186,Athens, Greece, 30 ?
31 March 2009. c?2009 Association for Computational LinguisticsGeneration of Referring Expression with an Individual ImprintBernd BohnetInternational Computer Science Institute1947 Center Street, CA 94704 Berkeleybohnet@icsi.berkeley.eduAbstractA major outcome of the last Shared Tasksfor Referring Expressions Generation wasthat each human prefers distinct proper-ties, syntax and lexical units for buildingreferring expressions.
One of the reasonsfor this seems to be that entities mightbe identified faster since the conversationpartner has already some knowledge abouthow his conversation partner builds refer-ring expressions.
Therefore, artificial re-ferring expressions should provide suchindividual preferences as well so that theybecome human like.
With this contribu-tion to the shared task, we follow this ideaagain.
For the development set, we got avery good DICE score of 0.88 for the fur-niture domain and of 0.79 for the peopledomain.1 IntroductionWe expect that the test set does not provide theinformation to which human a referring expres-sion belongs.
Therefore, we implemented a fallback strategy in order to get still acceptable DICEscores.
In such cases, we select among all sets theset of referring expressions which is most similarto all others.
We compute the similarity betweentwo sets as the average DICE score between all re-ferring expression of two sets.
The basis for ouralgorithm is an extended full brevity implementa-tion, cf.
(Bohnet and Dale, 2005).
IS-FP uses alsothe nearest neighbor technique like the IS-FBN al-gorithm that was introduced by Bohnet (2007).With the nearest neighbor technique, IS-FP se-lects the expressions which are most similar tothe referring expressions of the same human anda human that builds referring expressions similaror in the case that the human is unknown it usesthe most similar one to all others referring expres-sions.
The similarity is computed as the average ofall DICE scores between all combinations of theavailable trails for two humans.
From the resultof the nearest neighbor evaluation, FP selects theshortest and if still more than one expressions re-main then it computes the similarity among themand chooses the most typical and finally, if still al-ternatives remain, it selects one with the attributeshaving the highest frequency.
Table 1 shows theresults for IS-FP trained on the training set and ap-plied to the development set.Set Dice MASI Accuracy .Furniture 0.880 0.691 51.25%People 0.794 0.558 36.8%Total 0.837 0.625 44%Table 1: Results for the IS-FP algorithm2 IS-GT: Realization with GraphTransducersWe build the input dependency tree for the textgenerator due to the statistical information that wecollect from the training data for each person.
Thisprocedure is consistent with our referring expres-sion generator IS-FP that reproduces the individ-ual imprint in a referring expression for the targetperson.
We start with the realization of the refer-ring expressions from a surface syntactic depen-dency tree, cf.
(Mel?c?uk, 1988).
For the realiza-tion of the text, we use the Text Generator and Lin-guistic Environment MATE.1853 The Referring Expression ModelsAn algorithm learns a Referring Expression Modelfor each person that contributed referring expres-sion to the corpus.
The model contains the follow-ing information:(1) The lexicalization for the values of a attributesuch as couch for the value sofa, man forvalue person, etc.
(2) The preferred usage of determiners for thetype that can be definite (the), indefinite (a),no article.
(3) The syntactic preferences such as the top leftchair, the chair at the bottom to the left, etc.The information about the determiner and thelexicalization is collected from the annotated wordstring and the word string itself.
We collect themost frequent usage for each person in the corpus.In order to collect the preferred syntax, we anno-tated the word strings with syntactic dependencytrees.
Each of the dependency tress contains ad-ditional attributes, which describe the informationcontent of a branch outgoing from the root as wellas the possible value of the attribute at the nodeswhich carry the information.
The learning pro-gram cuts the syntactic tree at edges starting at theroot node and stores the branches in the referringexpression model for the person.4 RealizationFor the realization, we use a handcrafted grammarthat generates out of the dependency trees topo-logic graphs.
The main task of the grammar is todetermine the word order.
The system was devel-oped only by using the training data without anyconsideration of the development data.
We usedas guide for the optimization cross validation oftraining data.5 IS-FP-GT: The Combination ofAttribute Selection and RealizationFor the combination of the both methods, we com-bine the two procedure in a pipeline architecture.Table 2 shows the results.6 ConclusionThe IS-FP algorithm reproduces the imprint of hu-man referring expressions.
When the test set con-tains the reference to the human then the scores areexceptional high.Set Accuracy String ED Mean SED Blue 3Furniture 15 % 3,8625 0.3826 0.3684People 4,41 % 4,764 0.4817 0.2263Total 9,71 4,313 0.4321 0.297Table 2: Results for the TUNA-REG TaskReferencesB.
Bohnet and R. Dale.
2005.
Viewing referringexpression generation as search.
In IJCAI, pages1004?1009.B.
Bohnet.
2007.
IS-FBN, IS-FBS, IS-IAC: The Adap-tation of Two Classic Algorithms for the Generationof Referring Expressions in order to Produce Ex-pressions like Humans Do.
In MT Summit XI, UC-NLG+MT, pages 84?86.I.A.
Mel?c?uk.
1988.
Dependency Syntax: Theory andPractice.
State University of New York Press, Al-bany.186
