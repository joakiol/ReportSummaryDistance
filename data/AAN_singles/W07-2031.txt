Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 153?156,Prague, June 2007. c?2007 Association for Computational LinguisticsFUH (FernUniversita?t in Hagen):Metonymy Recognition UsingDifferent Kinds of Context for a Memory-Based LearnerJohannes LevelingIntelligent Information and Communication Systems (IICS)FernUniversita?t in Hagen (University of Hagen)johannes.leveling@fernuni-hagen.deAbstractFor the metonymy resolution task atSemEval-2007, the use of a memory-basedlearner to train classifiers for the identifica-tion of metonymic location names is inves-tigated.
Metonymy is resolved on differentlevels of granularity, differentiating betweenliteral and non-literal readings on the coarselevel; literal, metonymic, and mixed read-ings on the medium level; and a number ofclasses covering regular cases of metonymyon a fine level.
Different kinds of contextare employed to obtain different features:1) a sequence of n1 synset IDs represent-ing subordination information for nouns andfor verbs, 2) n2 prepositions, articles, modal,and main verbs in the same sentence, and 3)properties of n3 tokens in a context windowto the left and to the right of the locationname.Different classifiers were trained on theMascara data set to determine which valuesfor the context sizes n1, n2, and n3 yieldthe highest accuracy (n1 = 4, n2 = 3,and n3 = 7, determined with the leave-one-out method).
Results from these classifiersserved as features for a combined classifier.In the training phase, the combined classifierachieved a considerably higher precision forthe Mascara data.
In the SemEval submis-sion, an accuracy of 79.8% on the coarse,79.5% on the medium, and 78.5% on thefine level is achieved (the baseline accuracyis 79.4%).1 IntroductionMetonymy is typically defined as a figure of speechin which a speaker uses one entity to refer to an-other that is related to it (Lakoff and Johnson, 1980).The identification of metonymy becomes importantfor NLP tasks such as question answering (Stallard,1993) or geographic information retrieval (Levelingand Hartrumpf, 2006).For regular cases of metonymy for locations andorganizations, Markert and Nissim have proposeda set of metonymy classes.
Annotating a subset ofthe BNC (British National Corpus), they extracted aset of metonymic proper nouns from two categories:country names (Markert and Nissim, 2002) and or-ganization names (Nissim and Markert, 2003).In the metonymy resolution task at SemEval-2007, the goal was to identify metonymic names in asubset of the BNC.
The task consists of two subtasksfor company and country names, which are furtherdivided into classification on a coarse level (recog-nizing literal and non-literal readings), on a mediumlevel (differentiating non-literal readings into mixedand metonymic readings), and on a fine level (iden-tifying classes of regular metonymy, such as a namereferring to the population, place-for-people).
Thetask is described in more detail by Markert and Nis-sim (2007).2 System Description2.1 Tools and ResourcesThe following tools and resources are used for themetonymy classification:?
TiMBL 5.1 (Daelemans et al, 2004), amemory-based learner for classification is em-153ployed for training the classifiers (supervisedlearning).1?
Mascara 2.0 ?
Metonymy Annotation SchemeAnd Robust Analysis (Markert and Nissim,2003; Nissim and Markert, 2003; Markertand Nissim, 2002) contains annotated data formetonymic names from a subset of the theBNC.?
WordNet 2.0 (Fellbaum, 1998) serves as a lin-guistic resource for assigning synset IDs andfor looking up subordination information andfrequency of readings.?
The TreeTagger (Schmid, 1994) is utilized forsentence boundary detection, lemmatization,and part-of-speech tagging.
The English tag-ger was trained on the PENN treebank and usesthe English morphological database from theXTAG project (Karp et al, 1992).
The param-eter files were obtained from the web site.22.2 Different Kinds of ContextFollowing the assumption that metonymic locationnames can be identified from the context, there aredifferent kinds of context to consider.
At most, thecontext comprises a single sentence in this setup.Three kinds of context were employed to extract fea-tures for the memory-based learner TiMBL:?
C1: Subordination (hyponymy) information fornouns and verbs from the left and right contextof the possibly metonymic name.?
C2: The sentence context for modal verbs, mainverbs, prepositions, and articles.?
C3: A context window of tokens left and rightof the location name.The trial data provided (a subset of the Mascaradata) contained 188 non-literal location names (of925 samples total).
For a supervised learning ap-proach, this is too few data.
Therefore, the fullMascara data was converted to form training dataconsisting of feature values for context C1, C2, and1Peirsman (2006) also employs TiMBL for metonymy reso-lution, but trains a single classifier.2http://www.ims.uni-stuttgart.de/projek-te/corplex/TreeTagger/C3.
The training data contained 509 metonymic an-notations (of 2797 samples total).
Some cases inthe Mascara corpus are filtered during processing,including cases annotated as homonyms and caseswhose metonymy class could not be agreed upon.The test data had a majority baseline of 82.8% accu-racy for country names.2.3 FeaturesThe Mascara data was processed to extract the fol-lowing features (no hand-annotated data from Mas-cara was employed for feature values, i.e.
no gram-matical roles):?
For C1 (WordNet context): From a context ofn1 verbs and nouns in the same sentence, theirdistance to the location name is calculated.
Asequence of eight feature values of WordNetsynset IDs is obtained by iteratively looking upthe most frequent reading for a lemma inWord-Net and determining its synset ID.
Subordina-tion information between synsets is used to finda parent synset.
This process is repeated untila top-level parent synset is reached.
No actualword sense disambiguation is employed.?
For C2 (sentence context): Sentence bound-aries, part-of-speech tags, and lemmatizationare determined from the TreeTagger output.From a context window of n2 tokens, lemmaand distance are encoded as feature values forprepositions, articles, modal, and main verbs?
For C3 (word context): From a context of n3tokens to the left and to the right, the distancebetween token and location name, three pre-fix characters, three suffix characters, part-of-speech tag, case information (U=upper case,L=lower case, N=numeric, O=other), and wordlength are used as feature values.Table 1 and Table 2 show results for mem-ory based learners trained with TiMBL.
Perfor-mance measures were obtained with the leave-one-out method.
The classifiers were trained on fea-tures for different context sizes (ni ranging from 2to 7) to determine the setting for which the highestaccuracy is achieved (e.g.
1c, 2c, and 3c).
In thenext step, classifiers with a combined context were154Table 1: Results for training the classifiers on thecoarse location name classes (2797 instances, 509non-literal, leave-one-out) for the Mascara data (P =precision, R = recall, F = F-score).ID n1,n2,n3 coarse class P R F1c 4,0,0 literal 0.850 0.893 0.8711c 4,0,0 non-literal 0.377 0.289 0.3272c 0,3,0 literal 0.848 0.874 0.8602c 0,3,0 non-literal 0.342 0.295 0.3173c 0,0,7 literal 0.880 0.889 0.8853c 0,0,7 non-literal 0.478 0.455 0.4674c 4,3,0 literal 0.848 0.892 0.8964c 4,3,0 non-literal 0.368 0.282 0.3205c 4,0,7 literal 0.860 0.913 0.8855c 4,0,7 non-literal 0.459 0.332 0.3856c 0,3,7 literal 0.875 0.905 0.8896c 0,3,7 non-literal 0.496 0.420 0.4557c 4,3,7 literal 0.860 0.918 0.8887c 4,3,7 non-literal 0.473 0.332 0.3908c res.
of 1c?7c literal 0.852 0.968 0.9078c res.
of 1c?7c non-literal 0.639 0.248 0.357trained, selecting the setting with the highest accu-racy for a single context for the combination (e.g.4c, 5c, 6c, and 7c).
As an additional experiment, aclassifier was trained on classification results of theclassifiers described above (combination of 1?7, e.g.8c).
It was expected that the combination of featuresfrom different kinds of context would increase per-formance, and that the combination of classifier re-sults would increase performance.3 Evaluation ResultsTable 3 shows results for the official submission.Compared to results from the training phase onthe Mascara data (tested with the leave-one-outmethod), performance is considerably lower.
Forthis data, the combined classifier achieved a consid-erably higher precision (63.9% for non-literal read-ings; 57.3% for the fine class place-for-people andeven 83.3% for the rare class place-for-event).Performance may be affected by several reasons:A number of problems were encountered while pro-cessing the data.
The TreeTagger automatically to-kenizes its input and applies sentence boundary de-tection.
In some cases, the sentence boundary detec-tion did not work well, returning sentences of morethan 170 words.
Furthermore, the tagger output hadto be aligned with the test data again, as multi-wordTable 2: Excerpt from results for training the clas-sifiers on the fine location name classes (2797 in-stances, leave-one-out) for the Mascara data.ID n1,n2,n3 fine class P R F1f 4,0,0 literal 0.851 0.895 0.8731f 4,0,0 pl.-for-p. 0.366 0.280 0.3181f 4,0,0 pl.-for-e. 0.370 0.270 0.3122f 0,3,0 literal 0.848 0.876 0.8622f 0,3,0 pl.-for-p. 0.332 0.276 0.3012f 0,3,0 pl.-for-e. 0.222 0.270 0.2443f 0,0,7 literal 0.878 0.892 0.8853f 0,0,7 pl.-for-p. 0.463 0.424 0.4423f 0,0,7 pl.-for-e. 0.279 0.324 0.3004f 4,3,0 literal 0.851 0.899 0.8754f 4,3,0 pl.-for-p. 0.358 0.269 0.3074f 4,3,0 pl.-for-e. 0.435 0.270 0.3335f 4,0,7 literal 0.861 0.914 0.8875f 4,0,7 pl.-for-p. 0.452 0.322 0.3775f 4,0,7 pl.-for-e. 0.550 0.297 0.3866f 0,3,7 literal 0.871 0.906 0.8886f 0,3,7 pl.-for-p. 0.468 0.383 0.4226f 0,3,7 pl.-for-e. 0.400 0.324 0.3587f 4,3,7 literal 0.861 0.918 0.8897f 4,3,7 pl.-for-p. 0.459 0.323 0.3787f 4,3,7 pl.-for-e. 0.500 0.297 0.3738f res.
of 1f?7f literal 0.854 0.963 0.9058f res.
of 1f?7f pl.-for-p. 0.573 0.262 0.3608f res.
of 1f?7f pl.-for-e. 0.833 0.270 0.408names (e.g.
New York) were split into different to-kens.
In addition, the tag set of the tagger differssomewhat from the official PENN tag set and in-cludes additional tags for verbs.In earlier experiments on metonymy classifica-tion on a German corpus (Leveling and Hartrumpf,2006), the data was nearly evenly distributed be-tween literal and metonymic readings.
This seemsto make a classification task easier because there isno hidden bias in the classifier (i.e.
the baseline ofalways selecting the literal readings is about 50%).Features are obtained by shallow NLP methodsonly, not making use of a parser or chunker.
Thus,important syntactic or semantic information to de-cide on metonymy might be missing in the features.However, semantic features are more difficult to de-termine, because reliable automatic tools for seman-tic annotation are still missing.
This is also indi-cated by the fact that the grammatical roles (com-prising syntactic features) in Mascara data are hand-annotated.However, some linguistic phenomena are alreadyimplicitly represented by shallower features from155Table 3: Results for the coarse (908 samples: 721literal, 187 non-literal), medium (721 literal, 167metonymic, 20 mixed), and fine classification (721literal, 141 place-for-people, 10 place-for-event, 1place-for-product, 4 object-for-name, 11 othermet,20 mixed) of location names.class P R FFUH.location.coarse (0.798 accuracy)literal 0.812 0.971 0.884non-literal 0.543 0.134 0.214FUH.location.medium (0.795 accuracy)literal 0.810 0.970 0.883metonymic 0.500 0.132 0.208mixed 0.0 0.0 0.0FUH.location.fine (0.785 accuracy)literal 0.808 0.965 0.880place-for-people 0.386 0.120 0.183the surface level (given enough training instances).For instance, active/passive voice may be encodedby a combination of features for main verb/modalverbs.
If only a small training corpus is available,overall performance will be higher when utilizingexplicit syntactic or semantic features.Finally, the data may be too sparse for a super-vised memory-based learning approach.
The iden-tification of rare classes of metonymy (e.g.
place-for-event) would greatly benefit from a larger corpuscovering these classes.4 ConclusionEvaluation results on the training data were verypromising, indicating a boost of precision by com-bining classification results.
In the training phase,an accuracy of 83.7% was achieved on the coarselevel, compared to the majority baseline accuracy of81.8%.
For the submission for the metonymy res-olution task at SemEval-2007, accuracy is close tothe majority baseline (79.4%) on the coarse (79.8%),medium (79.5%), and fine (78.5%) level.In summary, using different context sizes for dif-ferent kinds of context and combining results of dif-ferent classifiers for metonymy resolution increasesperformance.
The general approach would profitfrom combining results of more diverse classifiers,i.e.
classifiers employing features extracted from thesurface, syntactic, and semantic context of a locationname.AcknowledgmentsThe research described was in part funded by theDFG (Deutsche Forschungsgemeinschaft) in theproject IRSAW (Intelligent Information Retrieval onthe Basis of a Semantically Annotated Web).ReferencesWalter Daelemans, Jakub Zavrel, Ko van der Sloot, andAntal van den Bosch.
2004.
TiMBL: Tilburg memorybased learner, version 5.1.
TR 04-02, ILK.Christiane Fellbaum, editor.
1998.
Wordnet.
An Elec-tronic Lexical Database.
MIT Press, Cambridge, Mas-sachusetts.Daniel Karp, Yves Schabes, Martin Zaidel, and DaniaEgedi.
1992.
A freely available wide coverage mor-phological analyzer for English.
In Proc.
of COLING-92, pages 950?955, Morristown, NJ.George Lakoff and Mark Johnson.
1980.
Metaphors WeLive By.
Chicago University Press.Johannes Leveling and Sven Hartrumpf.
2006.
Onmetonymy recognition for GIR.
In Proc.
of GIR-2006,the 3rd Workshop on Geographical Information Re-trieval (held at SIGIR 2006), Seattle, Washington.Katja Markert and Malvina Nissim.
2002.
Towards acorpus for annotated metonymies: The case of locationnames.
In Proc.
of LREC 2002, Las Palmas, Spain.Katja Markert and Malvina Nissim.
2003.
Corpus-basedmetonymy analysis.
Metaphor and symbol, 18(3).Katja Markert and Malvina Nissim.
2007.
Task 08:Metonymy resolution at SemEval-07.
In Proc.
of Sem-Eval 2007.Malvina Nissim and Katja Markert.
2003.
Syntacticfeatures and word similarity for supervised metonymyresolution.
In Proc.
of ACL-2003, Sapporo, Japan.Yves Peirsman.
2006.
Example-based metonymy recog-nition for proper nouns.
In Proc.
of the Student Re-search Workshop of EACL-2006, pages 71?78, Trento,Italy.Helmut Schmid.
1994.
Probabilistic part-of-speech tag-ging using decision trees.
In International Conferenceon NewMethods in Language Processing, Manchester,UK.David Stallard.
1993.
Two kinds of metonymy.
In Proc.of ACL-93, pages 87?94, Columbus, Ohio.156
