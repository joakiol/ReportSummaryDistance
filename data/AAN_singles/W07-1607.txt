Proceedings of the 4th ACL-SIGSEM Workshop on Prepositions, pages 45?50,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsAutomatically acquiring models of preposition useRachele De Felice and Stephen G. PulmanOxford University Computing LaboratoryWolfson Building, Parks Road, Oxford OX1 3QD, UK{rachele.defelice|stephen.pulman}@comlab.ox.ac.ukAbstractThis paper proposes a machine-learningbased approach to predict accurately, givena syntactic and semantic context, whichpreposition is most likely to occur in thatcontext.
Each occurrence of a preposition inan English corpus has its context representedby a vector containing 307 features.
Thevectors are processed by a voted perceptronalgorithm to learn associations between con-texts and prepositions.
In preliminary tests,we can associate contexts and prepositionswith a success rate of up to 84.5%.1 IntroductionPrepositions have recently become the focus ofmuch attention in the natural language processingcommunity, as evidenced for example by the ACLworkshops, a dedicated Sem-Eval task, and ThePreposition Project (TPP, Litkowski and Hargraves2005).
This is because prepositions play a key rolein determining the meaning of a phrase or sentence,and their correct interpretation is crucial for manyNLP applications: AI entities which require spatialawareness, natural language generation (e.g.
for au-tomatic summarisation, QA, MT, to avoid generat-ing sentences such as *I study at England), auto-matic error detection, especially for non-native En-glish speakers.
We present here an approach tolearning which preposition is most appropriate in agiven context by representing the context as a vectorpopulated by features referring to its syntactic andsemantic characteristics.
Preliminary tests on fiveprepositions - in, of, on, to, with - yield a successrate of between 71% and 84.5%.
In Section 2, we il-lustrate our motivations for using a vector-based ap-proach.
Section 3 describes the vector creation, andSection 4 the learning procedure.
Section 5 presentsa discussion of some preliminary results, and Sec-tion 6 offers an assessment of our method.2 Contextual featuresModelling preposition use is challenging because itis often difficult to explain why in two similar con-texts a given preposition is correct in one but not theother.
For example, we say A is similar to B, but dif-ferent from C, or we study in England, but at King?sCollege.
Nor can we rely on co-occurrence with par-ticular parts of speech (POS), as most prepositionshave a reasonably wide distribution.
Despite thisapparently idiosyncratic behaviour, we believe thatprepositional choice is governed by a combinationof several syntactic and semantic features.
Contextsof occurrence can be represented by vectors; a ma-chine learning algorithm trained on them can predictwith some confidence, given a new occurrence of acontext vector, whether a certain preposition is ap-propriate in that context or not.We consider the following macro-categories offeatures to be relevant: POS being modified; POS ofthe preposition?s complement; given a RASP-stylegrammatical relation output (GR; see e.g.
Briscoeet al 2006), what GRs the preposition occurs in;named entity (NE) information - whether the mod-ified or complement items are NEs; WordNet in-formation - to which of the WordNet lexicographer45classes1 the modified and complement nouns andverbs belong; immediate context - POS tags of ?2word window around the preposition.
For example,given a sentence such as John drove to Cambridge,we would note that this occurrence of the preposi-tion to modifies a verb, its complement is a locationNE noun, the verb it modifies is a ?verb of motion?,the tags surrounding it are NNP, VBD, NNP2, and itoccurs in the relation ?iobj?
with the verb, and ?dobj?with the complement noun.Our 307-feature set aims to capture all the salientelements of a sentence which we believe could be in-volved in governing preposition choice, and whichcan be accurately recognised automatically.
Ourchoice of features is provisional but based on a studyof errors frequently made by learners of English:however, when we spot a misused preposition, it of-ten takes some reflection to understand which ele-ments of the sentence are making that prepositionchoice sound awkward, and thus we have erred onthe side of generosity.
In some cases it is easier: weobserve that in the earlier example England is a loca-tion NE while King?s College is an organisation NE:this distinction may be the trigger for the differencein preposition choice.3 Vector constructionThe features are acquired from a version of theBritish National Corpus (BNC) processed by theC&C tools pipeline (Clark and Curran, to appear).The output of the C&C tools pipeline, which in-cludes stemmed words, POS tags, NER, GRs andCombinatory Categorial Grammar (CCG) deriva-tions of each sentence, is processed by a Pythonscript which, for each occurrence of a preposition ina sentence, creates a vector for that occurrence andpopulates it with 0s and 1s according to the absenceor presence of each feature in its context.
Each vec-tor therefore represents a corpus-seen occurrence ofa preposition and its context.
For each prepositionwe then construct a dataset to be processed by a ma-chine learning algorithm, containing all the vectorswhich do describe that preposition?s contexts, andan equal number of those which do not: our hypoth-1These are 41 broad semantic categories (e.g.
?noun denot-ing a shape?, ?verb denoting a cognitive process?)
to which allnouns and verbs in WordNet are assigned.2Penn Treebank tagset.esis is that these will be sufficiently different fromthe ?positive?
contexts that a machine learning algo-rithm will be able to associate the positive vectorsmore strongly to that preposition.4 Testing the approachTo test our approach, we first experimented witha small subset of the BNC, about 230,000 words(9993 sentences, of which 8997 contained at leastone preposition).
After processing we were left withover 33,000 vectors associated with a wide range ofprepositions.
Of course there is a certain amount ofnoise: since the vectors describe what the parser hastagged as prepositions, if something has been mis-tagged as one, then there will be a vector for it.
Thuswe find in our data vectors for things such as if andwhether, which are not generally considered prepo-sitions, and occasionally even punctuation items aremisanalysed as prepositions; however, these repre-sent only a small fraction of the total and so do notconstitute a problem.Even with a relatively large number of vectors,data sparseness is still an issue and for many prepo-sitions we did not find a large number of occurrencesin our dataset.
Because of this, and because thisis only a preliminary, small-scale exploration of thefeasibility of this approach, we decided to initiallyfocus on only 5 common prepositions3 : in (4278 oc-currences), of (7485), on (1483), to (48414), with(1520).
To learn associations between context vec-tors and prepositions, we use the Voted Perceptronalgorithm (Freund and Schapire 1999).
At this stagewe are only interested in establishing whether apreposition is correctly associated with a given con-text or not, so a binary classifier such as the VotedPerceptron is well-suited for our task.
At a laterstage we aim to expand this approach so that a noti-fication of error or inappropriateness is paired withsuggestions for other, more likely prepositions.
Apossible implementation of this is the output of a3These prepositions often occur in compound prepositionssuch as in front of ; their inclusion in the data could yield mis-leading results.
However out of 33,339 vectors, there were only463 instances of compound prepositions, so we do not find theirpresence skews the results.4Here to includes occurrences as an infinitival marker.
Thisis because the tagset does not distinguish between the two oc-currences; also, with a view to learner errors, its misuse as botha preposition and an infinitival marker is very common.46ranked list of the probability of each preposition oc-curring in the context under examination, especiallyas of course there are many cases in which morethan one preposition is possible (cf.
the folder onthe briefcase vs. the folder in the briefcase).We use the Weka machine learning package to runthe Voted Perceptron.
Various parameters can bemodified to obtain optimal performance: the num-ber of epochs the perceptron should go through, themaximum number of perceptrons allowed, and theexponent of the polynomial kernel function (whichallows a linear function such as the perceptron todeal with non-linearly separable data), as well as,of course, different combinations of vector features.We are experimenting with several permutations ofthese factors to ascertain which combination givesthe best performance.
Preliminary results obtainedso far show an average accuracy of 75.6%.5 Results and DiscussionWe present here results from two of the experiments,which consider two possible dimensions of varia-tion: the polynomial function exponent, d, and thepresence of differing subsets of features: WordNetor NE information and the ?2 POS tag window.Tests were run 10 times in 10-fold cross-validation.5.1 The effect of the d valueThe value of d is widely acknowledged in the litera-ture to play a key role in improving the performanceof the learning algorithm; the original experimentdescribed in Freund and Schapire (1999) e.g.
reportsresults using values of d from 1 to 6, with d=2 asthe optimal value.
Therefore our first investigationcompared performance with values for d set to d=1and d=2, with the other parameters set to 10 epochsand 10,000 as the maximum number of perceptronsallowed (Table 1).We can see that the results, as a first attempt atthis approach, are encouraging, achieving a successrate of above 80% in two cases.
Performance on onis somewhat disappointing, prompting the questionwhether this is because less data was available for it(although with, with roughly the same sized dataset,performs better), or if there is something intrinsic tothe syntactic and semantic properties of this prepo-sition that makes its use harder to pinpoint.
Theaverage performance of 75.6 - 77% is a promisingstarting point, and offers a solid base on which toproceed with a finer tuning of the various parame-ters, including the feature set, which could lead tobetter results.
The precision and recall support ourconfidence in this approach, as there are no great dif-ferences between the two in any dataset: this meansthat the good results we are achieving are not com-ing at the expense of one or the other measure.If we compare results for the two values of d, wenote that, contrary to expectations, there is no dra-matic improvement.
In most cases it is between lessthan 1% and just over that; only on shows a markedimprovement of 4%.
However, a positive trend isevident, and we will continue experimenting withvariations on this parameter?s value to determine itsoptimal setting.5.2 The effect of various feature categoriesAs well as variations on the learning algorithm it-self, we also investigate how different types of fea-tures affect performance.
This is interesting not onlyfrom a processing perspective - if some features arenot adding any useful information then they may bedisregarded, thus speeding up processing time - butalso from a linguistic one.
If we wish to use insightsfrom our work to assist in the description of preposi-tion use, an awareness of the extent to which differ-ent elements of language contribute to prepositionchoice is clearly of great importance.Here we present some results using datasets inwhich we have excluded various combinations of theNE, WordNet and POS tag features.
The WordNetand POS macrocategories of features are the largestsets - when both are removed, the vector is left withonly 31 features - so it is interesting to note how thisaffects performance.
Furthermore, the WordNet in-formation is in a sense the core ?lexical semantics?component, so its absence allows for a direct com-parison between a model ?with semantics?
and onewithout.
However, the WordNet data is also quitenoisy.
Many lexical items are assigned to severalcategories, because we are not doing any sense res-olution on our data.
The POS tag features represent?context?
in its most basic sense, detached from strictsyntactic and semantic considerations; it is useful toexamine the contribution this type of less sophisti-cated information can make.47d=1 d=2Preposition %correct Precision Recall F-score %correct Precision Recall F-scorein 76.30% 0.75 0.78 0.77 76.61% 0.77 0.77 0.77of 83.64% 0.88 0.78 0.83 84.47% 0.87 0.81 0.84on 65.66% 0.66 0.65 0.65 69.09% 0.69 0.69 0.69to 81.42% 0.78 0.87 0.82 82.43% 0.81 0.85 0.83with 71.25% 0.73 0.69 0.70 72.88% 0.73 0.72 0.73av.
75.65% 0.76 0.75 0.75 77.10% 0.77 0.77 0.77Table 1: The effect of the d valueAll features No W.Net No POS No NER No WN + POS GRs only% correct 83.64% 83.47% 81.46% 83.33% 81.00% 81.46%Precision 0.88 0.89 0.76 0.88 0.74 0.93Recall 0.78 0.76 0.91 0.77 0.94 0.68F-score 0.83 0.82 0.83 0.82 0.83 0.78Table 2: OF: the effect of various feature categories (d=1)Full results cannot be presented due to space re-strictions: we present those for ?of?, which are rep-resentative.
In almost case, the dataset with all fea-tures included is the one with the highest percentageof correct classifications, so all features do indeedplay a role in achieving the final result.
However,among the various sets variation is of just 1 or 2%,nor do f-scores vary much.
There are some interest-ing alternations in the precision and recall scores anda closer investigation of these might provide someinsight into the part played by each set of features:clearly there are some complex interactions betweenthem rather than a simple monotonic combination.Such small variations allow us to conclude thatthese sets of features are not hampering peformance(because their absence does not in general lead tobetter results), but also that they may not be a majordiscriminating factor in preposition choice: gram-matical relations seem to be the strongest feature -only 18 components of the vector!
This does notimply that semantics, or the immediate context of aword, play no role: it may just be that the way thisdata is captured is not the most informative for ourpurposes.
However, we must also consider if some-thing else in the feature set is impeding better perfor-mance, or if this is the best we can achieve with theseparameters, and need to identify more informativefeatures.
We are currently working on expandingthe feature set, considering e.g.
subcategorisationinformation for verbs, as well as experimenting withthe removal of other types of features, and using theWordNet data differently.
On the other hand, we alsoobserve that each macrocategory of features doescontribute something to the final result.
This couldsuggest that there is no one magic bullet-like featurewhich definitely and faultlessly identifies a preposi-tion but rather, as indeed we know by the difficultiesencountered in finding straightforward identificationcriteria for prepositions, this depends on a complexinterrelation of features each of which contributessomething to the whole.6 Evaluation and related work6.1 Error detection evaluationOne of our motivations in this work was to inves-tigate the practical utility of our context models inan error detection task.
The eventual aim is to beable, given a preposition context, to predict the mostlikely preposition to occur in it: if that differs fromthe one actually present, we have an error.
Usingreal learner English as testing material at our currentstage of development is too complex, however.
Thiskind of text presents several challenges for NLP andfor our task more specifically, such as spelling mis-takes - misspelled words would not be recognisedby WordNet or any other lexical item-based com-ponent.
Furthermore, often a learner?s error cannotsimply be described in terms of one word needingto be replaced by another, but has a more complexstructure.
Although it is our intention to be able toprocess these kinds of texts eventually, as an interimevaluation we felt that it was best to focus just ontexts where the only feature susceptible to error wasa preposition.
We therefore devised a simple artifi-cial error detection task using a corpus in which er-48rors are artificially inserted in otherwise correct text,for which we present interim results (the dataset iscurrently quite small) and we compare it against a?brute force?
baseline, namely using the recently re-leased Google n-gram data to predict the most likelypreposition.We set up a task aimed at detecting errors in theuse of of and to, for which we had obtained the bestresults in the basic classification tests reported ear-lier, and we created for this purpose a small corpususing BBC news articles, as we assume the presenceof errors there, spelling or otherwise, is extremelyunlikely.
Errors were created by replacing correctoccurrences of one of the prepositions with another,incorrect, one, or inserting of or to in place of otherprepositions.
All sentences contained at least onepreposition.
Together with a set of sentences wherethe prepositions were all correct, we obtained a setof 423 sentences for testing, consisting of 492 prepo-sition instances.
The aim was to replicate both kindsof errors one can make in using prepositions5 .We present here some results from this smallscale task; the data was classified by a model of thealgorithm trained on the BNC data with all featuresincluded, 10 epochs, and d=2.
If we run the task onthe vectors representing all occurrences of each ofthe prepositions, and ask the classifier to distinguishbetween correct and incorrect usages, we find thepercentage of correct classifications as follows:Prep Accuracy Precision Recallof 75.8 0.72 0.68to 81.35 0.76 0.74Average: 78.58 0.74 0.71These results show both high precision and highrecall, as do those for the dataset consisting of cor-rect occurrences of the preposition and use of an-other preposition instead of the right one: (of - 75%,to - 67% - these are accuracy figures only, as preci-sion and recall make no sense here.)
This small taskshows that it is possible to use our model to reliablycheck a text for preposition errors.However, these results need some kind of base-line for comparison.
The most obvious baselinewould be a random choice between positive and neg-ative (i.e.
the context matches or does not match the5A third, omitting it altogether, will be accounted for in fu-ture work.preposition) which we would expect to be success-ful 50% of the time.
Compared to that the observedaccuracies of 75% or more on all of these variousclassification tasks is clearly significant, represent-ing a 50% or more reduction in the error rate.However, we are also working on a more chal-lenging baseline consisting of a simple 3-gramlookup in the Google n-gram corpus (ca.
980 million3-grams).
For example, given the phrase y Paris,we could decide to use to rather than at because wefind 10,000 occurrences of y to Paris and hardlyany of y at Paris.
In a quick experiment, we ex-tracted 106 three-word sequences, consisting of oneword each side of the preposition, from a randomsample of the BBC dataset, ensuring each type of er-ror was equally represented.
For each sequence, wequeried the Google corpus for possible prepositionsin that sequence, selecting the most frequent one asthe answer.
Despite the very general nature of someof the 3-grams (e.g.
one of the), this method per-forms very well: the n-gram method scores 87.5%for of (vs. our 75.8%) and 72.5% for to (vs. our81.35%).
This is only a suggestive comparison, be-cause the datasets were not of the same size: by thetime of the workshop we hope to have a more rig-orous baseline to report.
Clearly, unless afflicted bydata sparseness, the raw word n-gram method willbe very hard to beat, since it will be based on fre-quently encountered examples of correct usage.
It istherefore encouraging that our method appears to beof roughly comparable accuracy even though we areusing no actual word features at all, but only moreabstract ones as described earlier.
An obvious nextstep, if this result holds up to further scrutiny, is toexperiment with combinations of both types of in-formation.6.2 Related workAlthough, as noted above, there is much research be-ing carried out on prepositions at the moment, to thebest of our knowledge there is no work which takesan approach similar to ours in the task of preposi-tion choice and error correction, i.e.
one that aims toautomate the process of context construction ratherthan relying on manually constructed grammars orother resources such as dictionaries (cf.
TPP).
Fur-thermore, much current research seems to have asits primary aim a semantic and functional descrip-49tion of prepositions.
While we agree this is a keyaspect of preposition use, and indeed hope at a laterstage of our research to derive some insights into thisbehaviour from our data, at present we are focusingon the more general task of predicting a prepositiongiven a context, regardless of semantic function.With regard to related work, as already men-tioned, there is no direct comparison we can makein terms of learning preposition use by a similarmethod.
One useful benchmark could be results ob-tained by others on a task similar to ours, i.e.
errordetection, especially in the language of non-nativespeakers.
In this case the challenge is finding workwhich is roughly comparable: there are a myriad ofvariables in this field, from the characteristics of thelearner (age, L1, education...) to the approach usedto the types of errors considered.
With this in mind,all we can do is mention some work which we feelis closest in spirit to our approach, but stress that thefigures are for reference only, and cannot be com-pared directly to ours.Chodorow and Leacock (2000) try to identify er-rors on the basis of context, as we do here, andmore specifically a ?2 word window around theword of interest, from which they consider func-tion words and POS tags.
Mutual information isused to determine more or less likely sequences ofwords, so that less likely sequences suggest the pres-ence of an error.
Unlike ours, their work focuses oncontent words rather than function words; they re-port a precision of 78% and a recall of 20%.
Ourprecision is comparable to this, and our recall ismuch higher, which is an important factor in errordetection: a user is likely to lose trust in a sys-tem which cannot spot his/her errors very often6.Izumi et al (2004) work with a corpus of En-glish spoken by Japanese students; they attempt toidentify errors using various contextual features andmaximum entropy based-methods.
They report re-sults for omission errors (precision 75.7%, recall45.67%) and for replacement errors (P 31.17%, R8%).
With the caveat that we are not working withspoken language, which presents several other chal-lenges, we note that in our task the errors, akin to re-placement errors, are detected with much more suc-6Although of course precision is a key measure: it is nothelpful for the user to be exposed to false alarms.cess.
Finally we can note the work done by Eeg-Olofsson and Knutsson (2003) on preposition errorsin L2 Swedish.
Their system uses manually craftedrules, unlike ours, and its performance is reported asachieving a recall of 25%.
On the basis of this briefand by no means exhaustive overview of the field,we claim that our results in the error detection taskare competitive, and we are working on fine-tuningvarious parameters to improve them further.7 ConclusionWe have presented an automated approach to learn-ing associations between sentence contexts andprepositions which does not depend on manuallycrafted grammars and achieves a success rate of upto 84.5%.
This model was tested on a small setof texts with artificially created preposition errors,and was found to be successful at detecting between76% and 81% of errors.
Ongoing work is focusingon how to further improve performance taking intoconsideration both the parameters of the voted per-ceptron algorithm and the feature set of the vectors.AcknowledgementsWe wish to thank Stephen Clark for stimulating discussions andthe anonymous reviewers for their helpful comments.
RacheleDe Felice is supported by an AHRC scholarship.ReferencesTed Briscoe, John Carroll, and Rebecca Watson.
2006.
Thesecond release of the RASP system.
In COLING/ACL-06Demo Session, Sydney, Australia.Martin Chodorow and Claudia Leacock.
2000.
An unsuper-vised method for detecting grammatical errors.
In NAACL-00, Seattle, Washington.Stephen Clark and James Curran.
To appear.
Wide-coverageEfficient Statistical Parsing with CCG and Log-linear Mod-els.Jens Eeg-Olofsson and Ola Knutsson.
2003.
Automatic gram-mar checking for second language learners - the use ofprepositions.
In Nodalida-03, Reykjavik, Iceland.Yoav Freund and Robert E. Schapire.
1999 Large margin clas-sification using the perceptron algorithm.
Machine Learning37:277-296Emi Izumi, Kiyotaka Uchimoto, and Hitoshi Isahara.
2004SST speech corpus of Japanese learners?
English and auto-matic detection of learners?
errors.
ICAME 28:31-48Ken Litkowski and Orin Hargraves.
2005.
The PrepositionProject.
In Second ACL-SIGSEM Prepositions Workshop,Colchester, UK.Guido Minnen, John Carroll, and Darren Pearce.
2001 Ap-plied Morphological Processing of English.
Natural Lan-guage Engineering 7(3):207-22350
