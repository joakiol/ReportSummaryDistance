Proceedings of the Ninth Workshop on Statistical Machine Translation, pages 150?156,Baltimore, Maryland USA, June 26?27, 2014. c?2014 Association for Computational LinguisticsStanford University?s Submissions to the WMT 2014 Translation TaskJulia Neidert?, Sebastian Schuster?, Spence Green,Kenneth Heafield, and Christopher D. ManningComputer Science Department, Stanford University{jneid,sebschu,spenceg,heafield,manning}@cs.stanford.eduAbstractWe describe Stanford?s participation inthe French-English and English-Germantracks of the 2014 Workshop on Statisti-cal Machine Translation (WMT).
Our sys-tems used large feature sets, word classes,and an optional unconstrained languagemodel.
Among constrained systems, oursperformed the best according to uncasedBLEU: 36.0% for French-English and20.9% for English-German.1 IntroductionPhrasal (Green et al., 2014b) is a phrase-based ma-chine translation system (Och and Ney, 2004) withan online, adaptive tuning algorithm (Green et al.,2013c) which allows efficient tuning of feature-rich translation models.
We improved upon thebasic Phrasal system with sparse features over wordclasses, class-based language models, and a web-scale language model.We submitted one constrained French-English(Fr-En) system, one unconstrained English-German(En-De) system with a huge language model, andone constrained English-German system without it.Each system was built using over 100,000 featuresand was tuned on over 10,000 sentences.
This paperdescribes our submitted systems and discusses howthe improvements affect translation quality.2 Data Preparation & Post-ProcessingWe used all relevant data allowed by the con-strained condition, with the exception of HindEn-Corp and Wiki Headlines, which we deemed toonoisy.
Specifically, our parallel data consists of theEuroparl version 7 (Koehn, 2005), parallel Com-monCrawl (Smith et al., 2013), French-English UN,Giga-FrEn, and News Commentary corpora pro-vided by the evaluation.
For monolingual data, we?These authors contributed equally.Sentences TokensEn-De 4.5M 222MFr-En 36.3M 2.1BTable 1: Gross parallel corpus statistics after pre-processing.Constrained LM Unconstrained LMGerman 1.7B 38.9 BEnglish 7.2B -Table 2: Number of tokens in pre-processed mono-lingual corpora used to estimate the language mod-els.
We split the constrained English data into twomodels: 3.7 billion tokens from Gigaword and 3.5billion tokens from all other sources.used the provided news crawl data from all years,English Gigaword version 5 (Parker et al., 2011),and target sides of the parallel data.
This includesEnglish from the Yandex, CzEng, and parallel Com-monCrawl corpora.
For parallel CommonCrawl,we concatenated the English halves for various lan-guage pairs and then deduplicated at the sentencelevel.In addition, our unconstrained English-Germansystem used German text extracted from the en-tire 2012, 2013, and winter 2013 CommonCrawl1corpora by Buck et al.
(2014).Tables 1 and 2 show the sizes of the pre-processed corpora of parallel text and monolingualtext from which our systems were built.2.1 Pre-ProcessingWe used Stanford CoreNLP to tokenize the Englishand German data according to the Penn Treebankstandard (Marcus et al., 1993).
The French sourcedata was tokenized similarly to the French Treebank1http://commoncrawl.org150(Abeill?
et al., 2003) using the Stanford Frenchtokenizer (Green et al., 2013b).We also lowercased the data and removed anycontrol characters.
Further, we filtered out all linesthat consisted mainly of punctuation marks, re-moved characters that are frequently used as bulletpoints and standardized white spaces and newlines.We additionally filtered out sentences longer than100 tokens from the parallel corpora in order tospeed up model learning.2.2 AlignmentFor both systems, we used the Berkeley Aligner(Liang et al., 2006) with default settings to alignthe parallel data.
We symmetrized the alignmentsusing the grow-diag heuristic.2.3 Language ModelsOur systems used up to three language models.2.3.1 Constrained Language ModelsFor En-De, we used lmplz (Heafield et al., 2013)to estimate a 5-gram language model on all WMTGerman monolingual data and the German side ofthe parallel Common Crawl corpus.
To query themodel, we used KenLM (Heafield, 2011).For the Fr-En system, we also estimated a 5-gramlanguage model from all the monolingual Englishdata and the English side of the parallel CommonCrawl, UN, Giga-FrEn, CzEng and Yandex corporausing the same procedure as above.
Additionally,we estimated a second language model from theEnglish Gigaword corpus.All of these language models used interpolatedmodified Kneser-Ney smoothing (Kneser and Ney,1995; Chen and Goodman, 1998).2.3.2 Unconstrained Language ModelOur unconstrained En-De submission used an ad-ditional language model trained on German webtext gathered by the Common Crawl Foundationand processed by Buck et al.
(2014).
This cor-pus was formed from the 2012, 2013, and winter2013 CommonCrawl releases, which consist of webpages converted to UTF-8 encoding with HTMLstripped.
Applying the Compact Language Detec-tor 2,22.89% of the data was identified as German,amounting to 1 TB of uncompressed text.
Aftersplitting sentences with the Europarl sentence split-ter (Koehn, 2005), the text was deduplicated at thesentence level to reduce the impact of boilerplate2https://code.google.com/p/cld2/Order 1 2 3 4 5Count 226 1,916 6,883 13,292 17,576Table 3: Number of unique n-grams, in millions,appearing in the Common Crawl German languagemodel.and pages that appeared in multiple crawls, discard-ing 78% of the data.
We treated the resulting dataas normal text, pre-processing it as described inSection 2.1 to yield 38.9 billion tokens.
We builtan unpruned interpolated modified Kneser-Ney lan-guage model with this corpus (Table 3) and addedit as an additional feature alongside the constrainedlanguage models.
At 38.9 billion tokens after dedu-plication, this monolingual data is almost 23 timesas large as the rest of the German monolingual cor-pus.
Since the test data was also collected from theweb, we cannot be sure that the test sentences werenot in the language model.
However, substantialportions of the test set are translations from otherlanguages, which were not posted online until after2013.2.3.3 Word-Class Language ModelWe also built a word-class language model for theEn-De system.
We trained 512 word classes onthe constrained German data using the predictiveone-sided class model of Whittaker and Woodland(2001) with the parallelized clustering algorithm ofUszkoreit and Brants (2008) by Green et al.
(2014a).All tokens were mapped to their word class; infre-quent tokens appearing fewer than 5 times weremapped to a special cluster for unknown tokens.Finally, we estimated a 7-gram language model onthe mapped corpus with SRILM (Stolcke, 2002)using Witten-Bell smoothing (Bell et al., 1990).2.4 Tuning and Test DataFor development, we tuned our systems on all13,573 sentences contained in the newstest2008-2012 data sets and tested on the 3,000 sentences ofthe newstest2013 data set.
The final system weightswere chosen among all tuning iterations using per-formance on the newstest2013 data set.2.5 Post-ProcessingOur post-processor recases and detokenizes sys-tem output.
For the English-German system, wecombined both tasks by using a Conditional Ran-dom Field (CRF) model (Lafferty et al., 2001) to151learn transformations between the raw output char-acters and the post-processed versions.
For eachtest dataset, we trained a separate model on 500,000sentences selected using the Feature Decay Algo-rithm for bitext selection (Bi?ici and Yuret, 2011).Features used include the character type of the cur-rent and surrounding characters, the token type ofthe current and surrounding tokens, and the positionof the character within its token.The English output was recased using a languagemodel based recaser (Lita et al., 2003).
The lan-guage model was trained on the English side of theFr-En parallel data using lmplz.3 Translation SystemWe built our translation systems using Phrasal.3.1 FeaturesOur translation model has 19 dense features thatwere computed for all translation hypotheses: thenine Moses (Koehn et al., 2007) baseline features,the eight hierarchical lexicalized reordering modelfeatures by Galley and Manning (2008), the logcount of each rule, and an indicator for unique rules.On top of that, the model uses the following addi-tional features of Green et al.
(2014a).Rule indicator features: An indicator feature foreach translation rule.
To combat overfitting, thisfeature fires only for rules that occur more than50 times in the parallel data.
Additional indicatorfeatures were constructed by mapping the words ineach rule to their corresponding word classes.Target unigram class: An indicator feature forthe class of each target word.Alignments: An indicator feature for each align-ment in a translation rule, including multi-wordalignments.
Again, class-based translation ruleswere used to extract additional indicator features.Source class deletion: An indicator feature forthe class of each unaligned source word in a trans-lation rule.Punctuation count ratio: The ratio of targetpunctuation tokens to source punctuation tokensfor each derivation.Functionword ratio: The ratio of target functionwords to source functionwords.
The functionwordsfor each language are the 35 most frequent wordson each side of the parallel data.
Numbers andpunctuation marks are not included in this list.Target-class bigram boundary: An indicatorfeature for the concatenation of the word class ofthe rightmost word in the left rule and the wordclass of the leftmost word in the right rule in eachadjacent rule pair in a derivation.Length features: Indicator features for the lengthof the source side and for the length of the targetside of the translation rule and an indicator featurefor the concatenation of the two lengths.Rule orientation features: An indicator featurefor each translation rule combined with its orienta-tion class (monotone, swap, or discontinuous).
Thisfeature also fires only for rules that occur more than50 times in the parallel data.
Again, class-basedtranslation rules were used to extract additional fea-tures.Signed linear distortion: The signed linear dis-tortion ?
for two rules a and b is ?
= r(a)?l(b)+1,where r(x) is the rightmost source index of rule xand l(x) is the leftmost source index of rule x. Eachadjacent rule pair in a derivation has an indicatorfeature for the signed linear distortion of this pair.Many of these features consider word classesinstead of the actual tokens.
For the target side, weused the same word classes as we used to train theclass-based language model.
For the source side,we trained word classes on all available data usingthe same method.3.2 TuningWe used an online, adaptive tuning algorithm(Green et al., 2013c) to learn the feature weights.The loss function is an online variant of expectedBLEU (Green et al., 2014a).
As a sentence-levelmetric, we used the extended BLEU+1 metric thatsmooths the unigram precision as well as the refer-ence length (Nakov et al., 2012).
For feature selec-tion, we used L1regularization.
Each tuning epochproduces a different set of weights; we tried all ofthem on newstest2013, which was held out from thetuning set, then picked the weights that producedthe best uncased BLEU score.3.3 System ParametersWe started off with the parameters of our systemsfor the WMT 2013 Translation Task (Green etal., 2013a) and optimized the L1-regularizationstrength.
Both systems used the following tuningparameters: a 200-best list, a learning rate of 0.02and a mini-batch size of 20.
The En-De system152Track Stanford Best RankEn-De constrained 19.9 20.1 3En-De unconstrained 20.0 20.6 5Fr-En constrained 34.5 35.0 3(a) cased BLEU (%)Track Stanford Best RankEn-De constrained 20.7 20.7 1En-De unconstrained 20.9 21.0 3Fr-En constrained 36.0 36.0 1(b) uncased BLEU (%)Table 4: Official results in terms of cased and uncased BLEU of our submitted systems compared to thebest systems for each track.
The ranks for the unconstrained system are calculated relative to all primarysubmissions for the language pair, whereas the ranks for the constrained systems are relative to only theconstrained systems submitted.used a phrase length limit of 8, a distortion limit of6 and a L1-regularization strength of 0.0002.
TheFr-En system used a phrase length limit of 9, a dis-tortion limit of 5 and a L1-regularization strengthof 0.0001.During tuning, we set the stack size for cube prun-ing to Phrasal?s default value of 1200.
To decodethe test set, we increased the stack size to 3000.4 ResultsTable 4 shows the official results of our systemscompared to other submissions to the WMT sharedtask.
Both our En-De and Fr-En systems achievedthe highest uncased BLEU scores among all con-strained submissions.
However, our recaser evi-dently performed quite poorly compared to othersystems, so our constrained systems ranked third bycased BLEU score.
Our unconstrained En-De sub-mission ranked third among all systems by uncasedBLEU and fifth by cased BLEU.To demonstrate the effectiveness of the individ-ual improvements, we show results for four differ-ent En-De systems: (1) A baseline that containsonly the 19 dense features, (2) a feature-rich trans-lation system with the additional rich features, (3)a feature-rich translation system with an additionalword class LM, and (4) a feature-rich translationsystem with an additional wordclass LM and a hugelanguage model.
For Fr-En we only built systems(1)-(3).
Results for all systems can be seen in Table5 and Table 6.
From these results, we can see thatboth language pairs benefitted from adding rich fea-tures (+0.4 BLEU for En-De and +0.5 BLEU forFr-En).
However, we only see improvements fromthe class-based language model in the case of theEn-De system (+0.4 BLEU).
For this reason our Fr-En submission did not use a class-based languagemodel.
Using additional data in the form of a hugelanguage model further improved our En-De sys-tem by almost 1% BLEU on the newstest2013 dataset.
However, we only saw 0.2 BLEU improvementon the newstest2014 data set.4.1 AnalysisGains from rich features are in line with the gainswe saw in the WMT 2013 translation task (Greenet al., 2013a).
We suspect that rich features wouldimprove the translation quality a lot more if we hadseveral reference translations to tune on.The word class language model seemed to im-prove only translations in our En-De system whileit had no effect on BLEU in our Fr-En system.
Oneof the main reasons seems to be that the 7-gramword class language model helped particularly withlong range reordering, which happens far more fre-quently in the En-De language pair compared to theFr-En pair.
For example, in the following transla-tion, we can see that the system with the class-basedlanguage model successfully translated the verb inthe second clause (set in italic) while the systemwithout the class-based language model did nottranslate the verb.Source: It became clear to me that this is my path.Feature-rich: Es wurde mir klar, dass das meinWeg.Word class LM: Es wurde mir klar, dass das meinWeg ist.We can also see that the long range of the wordclass language model improved grammaticality asshown in the following example:Source: Meanwhile, more than 40 percent of thepopulation are HIV positive.Feature-rich: Inzwischen sind mehr als 40Prozent der Bev?lkerung sind HIV positiv.153#iterations tune 2013 2013 cased 2014 2014 casedDense 8 16.9 19.6 18.7 20.0 19.2Feature-rich 10 20.1 20.0 19.0 20.0 19.2+ Word class LM 15 21.1 20.4 19.5 20.7 19.9+ Huge LM 9 21.0 21.3 20.3 20.9 20.1Table 5: En-De BLEU results.
The tuning set is newstest2008?2012.
Scores on newstest2014 werecomputed after the system submission deadline using the released references.#iterations tune 2013 2013 cased 2014 2014 casedDense 1 29.1 32.0 30.4 35.6 34.0Feature-rich 12 37.2 32.5 30.9 36.0 34.5+ Word class LM 14 35.7 32.3 30.7 ?
?Table 6: Fr-En BLEU results.
The tuning set is newstest2008?2012.
Scores on newstest2014 werecomputed after the system submission deadline using the released references.Word class LM: Unterdessen mehr als 40 Prozentder Bev?lkerung sind HIV positiv.In this example, the system without the class-based language model translated the verb twice.
Inthe second translation, the class-based languagemodel prevented this long range disagreement.
Ananalysis of the differences in the translation outputof our Fr-En systems showed that the word classlanguagemodelmainly led to different word choicesbut does not seem to help grammatically.4.2 CasingOur system performed comparatively poorly at cas-ing, as shown in Table 4.
In analysis after the eval-uation, we found many of these errors related towords with internal capitals, such as ?McCaskill?,because the limited recaser we used, which is basedon a language model, considered only all lowercase,an initial capital, or all uppercase words.
We ad-dressed this issue by allowing any casing seen in themonolingual data.
Some words were not seen at allin the monolingual data but, since the target side ofthe parallel data was included in monolingual data,these words must have come from the source sen-tence.
In such situations, we preserved the word?soriginal case.
Table 7 shows the results with the re-vised casing model.
We gained about 0.24% BLEUfor German recasing and 0.15% BLEU for Englishrecasing over our submitted systems.
In future work,we plan to compare with a truecased system.En-De Fr-EnUncased Oracle 20.71 36.05Conditional Random Field 19.85 ?Limited Recaser 19.82 34.51Revised Recaser 20.09 34.66Table 7: Casing results on newstest2014 performedafter the evaluation.
The oracle scores are uncasedBLEU (%) while all other scores are cased.
Sub-mitted systems are shown in italic.5 Negative ResultsWe experimented with several additions that did notmake it into the final submissions.5.1 PreorderingOne of the key challenges when translating fromEnglish to German is the long-range reordering ofverbs.
For this reason, we implemented a depen-dency tree based reordering system (Lerner andPetrov, 2013).
We parsed all source side sentencesusing the Stanford Dependency Parser (De Marn-effe et al., 2006) and trained the preordering systemon the entire bitext.
Then we preordered the sourceside of the bitext and the tuning and developmentdata sets using our preordering system, realignedthe bitext and tuned a machine translation systemusing the preordered data.
While preordering im-proved verb reordering in many cases, many otherparts of the sentences were often also reorderedwhich led to an overall decrease in translation qual-154ity.
Therefore, we concluded that this systemwill re-quire further development before it is useful withinour translation system.5.2 Minimum Bayes Risk DecodingWe further attempted to improve our output by re-ordering the best 1000 translations for each sentenceusing Minimum Bayes Risk decoding (Kumar andByrne, 2004) with BLEU as the distance measure.This in effect increases the score of candidates thatare ?closer?
to the other likely translations, where?closeness?
is measured by the BLEU score for thecandidate when the other translations are used as thereference.
Choosing the best translation followingthis reordering improved overall performance whentuned on the first half of the newstest2013 test set byonly 0.03 BLEU points for the English-German sys-tem and 0.005 BLEU points for the French-Englishsystem, so we abandoned this approach.6 ConclusionWe submitted three systems: one constrained Fr-Ensystem, one constrained En-De system, and one un-constrained En-De system.
Among all constrainedsystems, ours performed the best according to un-cased BLEU.
The key differentiating componentsof our systems are class-based features, word classlanguage models, and a huge web-scale languagemodel.
In ongoing work, we are investigating pre-ordering for En-De translation as well as improvedrecasing.AcknowledgementsWe thank Michael Kayser and Thang Luong forhelp with experiments.
This work was supportedby the Defense Advanced Research Projects Agency(DARPA) Broad Operational Language Translation(BOLT) program through IBM.
This work used theExtreme Science and Engineering Discovery Envi-ronment (XSEDE), which is supported by NationalScience Foundation grant number OCI-1053575.The authors acknowledge the Texas Advanced Com-puting Center (TACC) at The University of Texasat Austin for providing HPC resources that havecontributed to the research results reported withinthis paper.
Any opinions, findings, and conclusionsor recommendations expressed in this material arethose of the author(s) and do not necessarily reflectthe view of DARPA or the US government.ReferencesAnne Abeill?, Lionel Cl?ment, and Alexandra Kinyon,2003.
Building a treebank for French, chapter 10.Kluwer.Timothy C. Bell, John G. Cleary, and Ian H. Witten.1990.
Text compression.
Prentice-Hall.Ergun Bi?ici and Deniz Yuret.
2011.
Instance selec-tion for machine translation using feature decay al-gorithms.
In WMT.Christian Buck, Kenneth Heafield, and Bas van Ooyen.2014.
N-gram counts and language models from thecommon crawl.
In LREC.Stanley Chen and Joshua Goodman.
1998.
An empiri-cal study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Harvard Univer-sity, August.Marie-Catherine De Marneffe, Bill MacCartney,Christopher D Manning, et al.
2006.
Generatingtyped dependency parses from phrase structureparses.
In LREC.Michel Galley and Christopher D. Manning.
2008.
Asimple and effective hierarchical phrase reorderingmodel.
In EMNLP.Spence Green, Daniel Cer, Kevin Reschke, Rob Voigt,John Bauer, Sida Wang, et al.
2013a.
Feature-richphrase-based translation: Stanford University?s sub-mission to the WMT 2013 translation task.
In WMT.Spence Green, Marie-Catherine de Marneffe, andChristopher D. Manning.
2013b.
Parsing models foridentifying multiword expressions.
ComputationalLinguistics, 39(1):195?227.Spence Green, Sida Wang, Daniel Cer, and Christo-pher D. Manning.
2013c.
Fast and adaptive onlinetraining of feature-rich translation models.
In ACL.Spence Green, Daniel Cer, and Christopher D. Man-ning.
2014a.
An empirical comparison of featuresand tuning for phrase-based machine translation.
InWMT.Spence Green, Daniel Cer, and Christopher D. Man-ning.
2014b.
Phrasal: A toolkit for new directionsin statistical machine translation.
In WMT.Kenneth Heafield, Ivan Pouzyrevsky, Jonathan H.Clark, and Philipp Koehn.
2013.
Scalable modifiedKneser-Ney language model estimation.
In ACL.Kenneth Heafield.
2011.
KenLM: Faster and smallerlanguage model queries.
In WMT.Reinhard Kneser and Hermann Ney.
1995.
Improvedbacking-off for m-gram language modeling.
InICASSP.155Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,et al.
2007.
Moses: Open source toolkit for statisti-cal machine translation.
In ACL, Demonstration Ses-sion.Philipp Koehn.
2005.
Europarl: A parallel corpus forstatistical machine translation.
InProceedings ofMTSummit.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In HLT-NAACL.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labeling se-quence data.
In ICML.Uri Lerner and Slav Petrov.
2013.
Source-side classi-fier preordering for machine translation.
In EMNLP.Percy Liang, Ben Taskar, and Dan Klein.
2006.
Align-ment by agreement.
In NAACL.Lucian Vlad Lita, Abe Ittycheriah, Salim Roukos, andNanda Kambhatla.
2003. tRuEcasIng.
In ACL.Mitchell P Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large annotatedcorpus of English: The Penn Treebank.
Computa-tional Linguistics, 19:313?330.Preslav Nakov, Francisco Guzman, and Stephan Vogel.2012.
Optimizing for sentence-level BLEU+1 yieldsshort translations.
In COLING.Franz Josef Och and Hermann Ney.
2004.
The align-ment template approach to statistical machine trans-lation.
Computational Linguistics, 30(4):417?449.Robert Parker, David Graff, Junbo Kong, Ke Chen,and Kazuaki Maeda.
2011.
English gigawordfifth edition, june.
Linguistic Data Consortium,LDC2011T07.Jason Smith, Herv?
Saint-Amand, Magdalena Plamada,Philipp Koehn, Chris Callison-Burch, and AdamLopez.
2013.
Dirt cheap web-scale parallel textfrom the common crawl.
In ACL.
Association forComputational Linguistics, August.Andreas Stolcke.
2002.
SRILM?an extensible lan-guage modeling toolkit.
In ICLSP.Jakob Uszkoreit and Thorsten Brants.
2008.
Dis-tributed word clustering for large scale class-basedlanguage modeling in machine translation.
In ACL.Ed W. D. Whittaker and Philip C. Woodland.
2001.
Ef-ficient class-based language modelling for very largevocabularies.
In ICASSP.156
