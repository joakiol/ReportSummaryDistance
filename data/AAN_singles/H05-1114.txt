Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural LanguageProcessing (HLT/EMNLP), pages 907?914, Vancouver, October 2005. c?2005 Association for Computational LinguisticsA Semi-Supervised Feature Clustering Algorithmwith Application to Word Sense DisambiguationZheng-Yu Niu, Dong-Hong JiInstitute for Infocomm Research21 Heng Mui Keng Terrace119613 Singapore{zniu, dhji}@i2r.a-star.edu.sgChew Lim TanDepartment of Computer ScienceNational University of Singapore3 Science Drive 2117543 Singaporetancl@comp.nus.edu.sgAbstractIn this paper we investigate an applica-tion of feature clustering for word sensedisambiguation, and propose a semi-supervised feature clustering algorithm.Compared with other feature clusteringmethods (ex.
supervised feature cluster-ing), it can infer the distribution of classlabels over (unseen) features unavailablein training data (labeled data) by the use ofthe distribution of class labels over (seen)features available in training data.
Thus,it can deal with both seen and unseen fea-tures in feature clustering process.
Our ex-perimental results show that feature clus-tering can aggressively reduce the dimen-sionality of feature space, while still main-taining state of the art sense disambigua-tion accuracy.
Furthermore, when com-bined with a semi-supervised WSD algo-rithm, semi-supervised feature clusteringoutperforms other dimensionality reduc-tion techniques, which indicates that usingunlabeled data in learning process helps toimprove the performance of feature clus-tering and sense disambiguation.1 IntroductionThis paper deals with word sense disambiguation(WSD) problem, which is to assign an appropriatesense to an occurrence of a word in a given context.Many corpus based statistical methods have beenproposed to solve this problem, including supervisedlearning algorithms (Leacock et al, 1998; Towel andVoorheest, 1998), weakly supervised learning algo-rithms (Dagan and Itai, 1994; Li and Li, 2004; Mi-halcea, 2004; Niu et al, 2005; Park et al, 2000;Yarowsky, 1995), unsupervised learning algorithms(or word sense discrimination) (Pedersen and Bruce,1997; Schu?tze, 1998), and knowledge based algo-rithms (Lesk, 1986; McCarthy et al, 2004).In general, the most common approaches start byevaluating the co-occurrence matrix of features ver-sus contexts of instances of ambiguous word, givensense-tagged training data for this target word.
Asa result, contexts are usually represented in a high-dimensional sparse feature space, which is far fromoptimal for many classification algorithms.
Further-more, processing data lying in high-dimensional fea-ture space requires large amount of memory andCPU time, which limits the scalability of WSDmodel to very large datasets or incorporation ofWSD model into natural language processing sys-tems.Standard dimentionality reduction techniques in-clude (1) supervised feature selection and super-vised feature clustering when given labeled data, (2)unsupervised feature selection, latent semantic in-dexing, and unsupervised feature clustering whenonly unlabeled data is available.
Supervised fea-ture selection improves the performance of an ex-amplar based learning algorithm over SENSEVAL-2 data (Mihalcea, 2002), Naive Bayes and deci-sion tree over SENSEVAL-1 and SENSEVAL-2 data(Lee and Ng, 2002), but feature selection does notimprove SVM and Adaboost over SENSEVAL-1and SENSEVAL-2 data (Lee and Ng, 2002) forword sense disambiguation.
Latent semantic in-dexing (LSI) studied in (Schu?tze, 1998) improvesthe performance of sense discrimination, while un-supervised feature selection also improves the per-formance of word sense discrimination (Niu et al,2004).
But little work is done on using feature clus-tering to conduct dimensionality reduction for WSD.This paper will describe an application of feature907clustering technique to WSD task.Feature clustering has been extensively studiedfor the benefit of text categorization and documentclustering.
In the context of text categorization, su-pervised feature clustering algorithms (Baker andMcCallum, 1998; Bekkerman et al, 2003; Slonimand Tishby, 2001) usually cluster words into groupsbased on the distribution of class labels over fea-tures, which can compress the feature space muchmore aggressively while still maintaining state ofthe art classification accuracy.
In the context ofdocument clustering, unsupervised feature cluster-ing algorithms (Dhillon, 2001; Dhillon et al, 2002;Dhillon et al, 2003; El-Yaniv and Souroujon, 2001;Slonim and Tishby, 2000) perform word clusteringby the use of word-document co-occurrence matrix,which can improve the performance of documentclustering by clustering documents over word clus-ters.Supervised feature clustering algorithm groupsfeatures into clusters based on the distribution ofclass labels over features.
But it can not group un-seen features (features that do not occur in labeleddata) into meaningful clusters since there are noclass labels associated with these unseen features.On the other hand, while given labeled data, un-supervised feature clustering method can not uti-lize class label information to guide feature cluster-ing procedure.
While, as a promising classificationstrategy, semi-supervised learning methods (Zhou etal., 2003; Zhu and Ghahramani, 2002; Zhu et al,2003) usually utilize all the features occurring in la-beled data and unlabeled data.
So in this paper wepropose a semi-supervised feature clustering algo-rithm to overcome this problem.
Firstly, we try toinduce class labels for unseen features based on thesimilarity among seen features and unseen features.Then all the features (including seen features andunseen features) are clustered based on the distrib-ution of class labels over them.This paper is organized as follows.
First, wewill formulate a feature clustering based WSD prob-lem in section 2.
Then in section 3 we will de-scribe a semi-supervised feature clustering algo-rithm.
Section 4 will provide experimental resultsof various dimensionality reduction techniques withcombination of state of the art WSD algorithms onSENSEVAL-3 data.
Section 5 will provide a reviewof related work on feature clustering.
Finally we willconclude our work and suggest possible improve-ment in section 6.2 Problem SetupLet X = {xi}ni=1 be a set of contexts of occur-rences of an ambiguous word w, where xi repre-sents the context of the i-th occurrence, and n isthe total number of this word?s occurrences.
LetS = {sj}cj=1 denote the sense tag set of w. The firstl examples xg(1 ?
g ?
l) are labeled as yg (yg ?
S)and other u (l+u = n) examples xh(l+1 ?
h ?
n)are unlabeled.
The goal is to predict the sense of win context xh by the use of label information of xgand similarity information among examples in X .We use F?
to represent feature clustering resultinto NF?
clusters when F is a set of features.
Afterfeature clustering, any context xi in X can be repre-sented as a vector over feature clusters F?
.
Then wecan use supervised methods (ex.
SVM) (Lee andNg, 2002) or semi-supervised methods (ex.
labelpropagation algorithm) (Niu et al, 2005) to performsense disambiguation on unlabeled instances of tar-get word.3 Semi-Supervised Feature ClusteringAlgorithmIn supervised feature clustering process, F consistsof features occurring in the first l labeled examples,which can be denoted as FL.
But in the setting oftransductive learning, semi-supervised learning al-gorithms will utilize not only the features in labeledexamples (FL), but also unseen features in unlabeledexamples (denoted as FL).
FL consists of the fea-tures that occur in unlabeled data, but never appearin labeled data.Supervised feature clustering algorithm usuallyperforms clustering analysis over feature-class ma-trix, where each entry (i, j) in this matrix is the num-ber of times of the i-th feature co-occurring with thej-th class.
Therefore it can not group features in FLinto meaningful clusters since there are no class la-bels associated with these features.
We overcomethis problem by firstly inducing class labels for un-seen features based on the similarity among featuresin FL and FL, then clustering all the features (in-cluding FL and FL) based on the distribution of class908labels over them.This semi-supervised feature clustering algorithmis defined as follows:Input:Feature set F = FL?FL (the first |FL| featuresin F belong to FL, and the remaining |FL| featuresbelong to FL), context set X , the label informationof xg(1 ?
g ?
l), NF?
(the number of clusters in F?
);Output:Clustering solution F?
;Algorithm:1.
Construct |F | ?
|X| feature-example matrixMF,X , where entry MF,Xi,j is the number of times offi co-occurring with example xj (1 ?
j ?
n).2.
Form |F | ?
|F | affinity matrix W defined byWij = exp(?d2ij?2 ) if i 6= j and Wii = 0 (1 ?i, j ?
|F |), where dij is the distance (ex.
Euclid-ean distance) between fi (the i-th row in MF,X ) andfj (the j-th row in MF,X ), and ?
is used to controlthe weight Wij .3.
Construct |FL| ?
|S| feature-class matrixY FL,S , where the entry Y FL,Si,j is the number oftimes of feature fi (fi ?
FL) co-occurring withsense sj .4.
Obtain hard label matrix for features in FL(denoted as Y FL,Shard ) based on Y FL,S , where entryY F,Shard i,j = 1 if the hard label of fi is sj , otherwisezero.
Obtain hard labels for features in FL usinga classifier based on W and Y FL,Shard .
In this paperwe use label propagation (LP) algorithm (Zhu andGhahramani, 2002) to get hard labels for FL.5.
Construct |F | ?
|S| feature-class matrix Y F,Shard,where entry Y F,Shard i,j = 1 if the hard label of fi issj , otherwise zero.6.
Construct the matrix L = D?1/2WD?1/2 inwhich D is a diagonal matrix with its (i, i)-elementequal to the sum of the i-th row of W .7.
Label each feature in F as soft label Y?
F,Si , thei-th row of Y?
F,S , where Y?
F,S = (I ?
?L)?1Y F,Shard.8.
Obtain the feature clustering solution F?
byclustering the rows of Y?
F,Si into NF?
groups.
Inthis paper we use sequential information bottleneck(sIB) algorithm (Slonim and Tishby, 2000) to per-form clustering analysis.EndStep 3 ?
5 are the process to obtain hard la-bels for features in F , while the operation in step 6and 7 is a local and global consistency based semi-supervised learning (LGC) algorithm (Zhou et al,2003) that smooth the classification result of LP al-gorithm to acquire a soft label for each feature.At first sight, this semi-supervised feature cluster-ing algorithm seems to make little sense.
Since werun feature clustering in step 8, why not use LP algo-rithm to obtain soft label matrix Y FL,S for featuresin FL by the use of Y FL,S and W , then just applysIB directly to soft label matrix Y?
F,S (constructedby catenating Y FL,S and Y FL,S)?The reason for using LGC algorithm to acquiresoft labels for features in F is that in the contextof transductive learning, the size of labeled data israther small, which is much less than that of un-labeled data.
This makes it difficult to obtain re-liable estimation of class label?s distribution overfeatures from only labeled data.
This motivatesus to use raw information (hard labels of featuresin FL) from labeled data to estimate hard labelsof features in FL.
Then LGC algorithm is usedto smooth the classification result of LP algorithmbased on the assumption that a good classificationshould change slowly on the coherent structure ag-gregated by a large amount of unlabeled data.
Thisoperation makes our algorithm more robust to thenoise in feature-class matrix Y FL,S that is estimatedfrom labeled data.In this paper, ?
is set as the average distance be-tween labeled examples from different classes, andNF?
= |F |/10.
Latent semantic indexing technique(LSI) is used to perform factor analysis in MF,X be-fore calculating the distance between features in step2.4 Experiments and Results4.1 Experiment DesignFor empirical study of dimensionality reductiontechniques on WSD task, we evaluated five dimen-sionality reduction algorithms on the data in Englishlexical sample (ELS) task of SENSEVAL-3 (Mihal-cea et al, 2004)(including all the 57 English words) 1: supervised feature clustering (SuFC) (Baker andMcCallum, 1998; Bekkerman et al, 2003; Slonim1Available at http://www.senseval.org/senseval3909and Tishby, 2001), iterative double clustering (IDC)(El-Yaniv and Souroujon, 2001), semi-supervisedfeature clustering (SemiFC) (our algorithm), super-vised feature selection (SuFS) (Forman, 2003), andlatent semantic indexing (LSI) (Deerwester et.
al.,1990) 2.We used sIB algorithm 3 to cluster features inFL into groups based on the distribution of class la-bels associated with each feature.
This procedurecan be considered as our re-implementation of su-pervised feature clustering.
After feature clustering,examples can be represented as vectors over featureclusters.IDC is an extension of double clustering method(DC) (Slonim and Tishby, 2000), which performs it-erations of DC.
In the transductive version of IDC,they cluster features in F as distributions over classlabels (given by the labeled data) during the firststage of the IDC first iteration.
This phase results infeature clusters F?
.
Then they continue as usual; thatis, in the second phase of the first IDC iteration theygroup X into NX?
clusters, where X is representedas distribution over F?
.
Subsequent IDC iterationsuse all the unlabeled data.
This IDC algorithm canresult in two clustering solutions: F?
and X?
.
Follow-ing (El-Yaniv and Souroujon, 2001), the number ofiterations is set as 15, and NX?
= |S| (the number ofsenses of target word) in our re-implementation ofIDC.
After performing IDC, examples can be repre-sented as vectors over feature clusters F?
.Supervised feature selection has been extensivelystudied for text categorization task (Forman, 2003).Information gain (IG) is one of state of the art cri-teria for feature selection, which measures the de-crease in entropy when the feature is given vs. ab-sent.
In this paper, we calculate IG score for eachfeature in FL, then select top |F |/10 features withhighest scores to form reduced feature set.
Thenexamples can be represented as vectors over the re-duced feature set.LSI is an unsupervised factor analysis techniquebased on Singular Value Decomposition of a |X| ?|F | example-feature matrix.
The underlying tech-nique for LSI is to find an orthogonal basis for the2Following (Baker and McCallum, 1998), we use LSI as arepresentative method for unsupervised dimensionality reduc-tion.3Available at http://www.cs.huji.ac.il/?noamm/feature-example space for which the axes lie alongthe dimensions of maximum variance.
After usingLSI on the example-feature matrix, we can get vec-tor representation for each example in X in reducedfeature space.For each ambiguous word in ELS task ofSENSEVAL-3, we used three types of features tocapture contextual information: part-of-speech ofneighboring words with position information, un-ordered single words in topical context, and localcollocations (as same as the feature set used in (Leeand Ng, 2002) except that we did not use syntacticrelations).
We removed the features with occurrencefrequency (counted in both training set and test set)less than 3 times.We ran these five algorithms for each ambiguousword to reduce the dimensionality of feature spacefrom |F | to |F |/10 no matter which training data isused (ex.
full SENSEVAL-3 training data or sam-pled SENSEVAL-3 training data).
Then we can ob-tain new vector representation of X in new featurespace acquired by SuFC, IDC, SemiFC, and LSI orreduced feature set by SuFS.Then we used SVM 4 and LP algorithm to per-form sense disambiguation on vectors in dimension-ality reduced feature space.
SVM and LP were eval-uated using accuracy 5 (fine-grained score) on testset of SENSEVAL-3.
For LP algorithm, the test setin SENSEVAL-3 data was also used as unlabeleddata in tranductive learning process.We investigated two distance measures for LP: co-sine similarity and Jensen-Shannon (JS) divergence(Lin, 1991).
Cosine similarity measures the anglebetween two feature vectors, while JS divergencemeasures the distance between two probability dis-tributions if each feature vector is considered asprobability distribution over features.For sense disambiguation on SENSEVAL-3 data,we constructed connected graphs for LP algorithmfollowing (Niu et al, 2005): two instances u, v willbe connected by an edge if u is among v?s k nearestneighbors, or if v is among u?s k nearest neighbors4We used SV M light with linear kernel function, availableat http://svmlight.joachims.org/.5If there are multiple sense tags for an instance in trainingset or test set, then only the first tag is considered as correctanswer.
Furthermore, if the answer of the instance in test set is?U?, then this instance will be removed from test set.910as measured by cosine or JS distance measure.
k is5 in later experiments.4.2 Experiments on Full SENSEVAL-3 DataIn this experiment, we took the training set inSENSEVAL-3 as labeled data, and the test set as un-labeled data.
In other words, all of dimensionalityreduction methods and classifiers can use the labelinformation in training set, but can not access thelabel information in test set.
We evaluated differ-ent sense disambiguation processes using test set inSENSEVAL-3.We use features with occurrence frequency no lessthan 3 in training set and test set as feature set F foreach ambiguous word.
F consists of two disjointsubsets: FL and FL.
FL consists of features occur-ring in training set of target word in SENSEVAL-3,while FL consists of features that occur in test set,but never appear in training set.Table 1 lists accuracies of SVM and LPwithout or with dimensionality reduction on fullSENSEVAL-3 data.
From this table, we have somefindings as follows:(1) If without dimensionality reduction, the bestperformance of sense disambiguation is 70.3%(LPJS), while if using dimensionality reduction,the best two systems can achieve 69.8% (SuFS +LPJS) and 69.0% (SemiFC + LPJS) accuracies.It seems that feature selection and feature clusteringcan significantly reduce the dimensionality of fea-ture space while losing only about 1.0% accuracy.
(2) Furthermore, LPJS algorithm performs bet-ter than SVM when combined with the same dimen-sionality reduction technique (except IDC).
Noticethat LP algorithm uses unlabelled data during its dis-ambiguation phase while SVM doesn?t.
This indi-cates that using unlabeled data helps to improve theperformance of sense disambiguation.
(3) When using LP algorithm for sense disam-biguation, SemiFC performs better than other fea-ture clustering algorithms, such as SuFC, IDC.This indicates that clustering seen and unseen fea-tures can satisfy the requirement of semi-supervisedlearning algorithm, which does help the classifica-tion process.
(4) When using SuFC, IDC, SuFS, or SemiFC fordimensionality reduction, the performance of sensedisambiguation is always better than that using LSIas dimensionality reduction method.
SuFC, IDC,SuFS, and SemiFC use label information to guidefeature clustering or feature selection, while LSI isan unsupervised factor analysis method that can con-duct dimensionality reduction without the use of la-bel information from labeled data.
This indicatesthat using label information in dimensionality re-duction procedure can cluster features into bettergroups or select better feature subsets, which resultsin better representation of contexts in reduced fea-ture space.4.3 Additional Experiments on SampledSENSEVAL-3 DataFor investigating the performance of various dimen-sionality reduction techniques with very small train-ing data, we ran them with only lw examples fromtraining set of each word in SENSEVAL-3 as la-beled data.
The remaining training examples andall the test examples were used as unlabeled datafor SemiFC or LP algorithm.
Finally we evaluateddifferent sense disambiguation processes using testset in SENSEVAL-3.
For each labeled set size lw,we performed 20 trials.
In each trial, we randomlysampled lw labeled examples for each word fromtraining set.
If any sense was absent from the sam-pled labeled set, we redid the sampling.
lw is set asNw,train ?
10%, where Nw,train is the number ofexamples in training set of word w. Other settingsof this experiment is as same as that of previous onein section 4.2.In this experiment, feature set F is as same as thatin section 4.2.
FL consists of features occurring insampled training set of target word in SENSEVAL-3, while FL consists of features that occur in unla-beled data (including unselected training data and allthe test set), but never appear in labeled data (sam-pled training set).Table 2 lists accuracies of SVM and LP with-out or with dimensionality reduction on sampledSENSEVAL-3 training data 6.
From this table, wehave some findings as follows:(1) If without dimensionality reduction, the bestperformance of sense disambiguation is 54.9%(LPJS), while if using dimensionality reduction, the6We can not obtain the results of IDC over 20 trials since itcosts about 50 hours for each trial (Pentium 1.4 GHz CPU/1.0GB memory).911Table 1: This table lists the accuracies of SVM and LP without or with dimensionality reduction on fullSENSEVAL-3 data.
There is no result for LSI + LPJS , since the vectors obtained by LSI may containnegative values, which prohibits the application of JS divergence for measuring the distance between thesevectors.Without With various dimensionalitydimensionality reduction techniquesClassifier reduction SuFC IDC SuFS LSI SemiFCSVM 69.7% 66.4% 65.1% 65.2% 59.1% 64.0%LPcosine 68.4% 66.7% 64.9% 66.0% 60.7% 67.6%LPJS 70.3% 67.2% 64.0% 69.8% - 69.0%Table 2: This table lists the accuracies of SVM and LP without or with dimensionality reduction on sam-pled SENSEVAL-3 training data.
For each classifier, we performed paired t-test between the system usingSemiFC for dimensionality reduction and any other system with or without dimensionality reduction.
?
(or?)
means p-value ?
0.01, while > (or <) means p-value falling into (0.01, 0.05].
Both ?
(or ?)
and >(or <) indicate that the performance of current WSD system is significantly better (or worse) than that usingSemiFC for dimensionality reduction, when given same classifier.Without With various dimensionalitydimensionality reduction techniquesClassifier reduction SuFC SuFS LSI SemiFCSVM 53.4?1.1% (?)
50.4?1.1% (?)
52.2?1.2% (>) 49.8?0.8% (?)
51.5?1.0%LPcosine 54.4?1.2% (?)
49.5?1.1% (?)
51.1?1.0% (?)
49.8?1.0% (?)
52.9?1.0%LPJS 54.9?1.1% (?)
52.0?0.9% (?)
52.5?1.0% (?)
- 54.1?1.2%best performance of sense disambiguation is 54.1%(SemiFC + LPJS).
Feature clustering can signif-icantly reduce the dimensionality of feature spacewhile losing only 0.8% accuracy.
(2) LPJS algorithm performs better than SVMwhen combined with most of dimensionality reduc-tion techniques.
This result confirmed our previousconclusion that using unlabeled data can improvethe sense disambiguation process.
Furthermore,SemiFC performs significantly better than SuFC andSuFS when using LP as the classifier for sense dis-ambiguation.
The reason is that when given veryfew labeled examples, the distribution of class labelsover features can not be reliably estimated, whichdeteriorates the performance of SuFC or SuFS.
ButSemiFC uses only raw label information (hard labelof each feature) estimated from labeled data, whichmakes it robust to the noise in very small labeleddata.
(3) SuFC, SuFS and SemiFC perform better thanLSI no matter which classifier is used for sense dis-ambiguation.
This observation confirmed our previ-ous conclusion that using label information to guidedimensionality reduction process can result in bet-ter representation of contexts in feature subspace,which further improves the results of sense disam-biguation.5 Related WorkFeature clustering has been extensively studied forthe benefit of text categorization and document clus-tering, which can be categorized as supervised fea-ture clustering, semi-supervised feature clustering,and unsupervised feature clustering.Supervised feature clustering algorithms (Bakerand McCallum, 1998; Bekkerman et al, 2003;Slonim and Tishby, 2001) usually cluster words intogroups based on the distribution of class labels overfeatures.
Baker and McCallum (1998) apply super-vised feature clustering based on distributional clus-tering for text categorization, which can compressthe feature space much more aggressively while still912maintaining state of the art classification accuracy.Slonim and Tishby (2001) and Bekkerman et.
al.
(2003) apply information bottleneck method to findword clusters.
They present similar results with thework by Baker and McCallum (1998).
Slonim andTishby (2001) goes further to show that when thetraining sample is small, word clusters can yield sig-nificant improvement in classification accuracy.Unsupervised feature clustering algorithms(Dhillon, 2001; Dhillon et al, 2002; Dhillon et al,2003; El-Yaniv and Souroujon, 2001; Slonim andTishby, 2000) perform word clustering by the useof word-document co-occurrence matrix, which donot utilize class labels to guide clustering process.Slonim and Tishby (2000), El-Yaniv and Souroujon(2001) and Dhillon et.
al.
(2003) show that wordclusters can improve the performance of documentclustering.El-Yaniv and Souroujon (2001) present an itera-tive double clustering (IDC) algorithm, which per-forms iterations of double clustering (Slonim andTishby, 2000).
Furthermore, they extend IDC algo-rithm for semi-supervised learning when given bothlabeled and unlabeled data.Our algorithm belongs to the family of semi-supervised feature clustering techniques, which canutilize both labeled and unlabeled data to performfeature clustering.Supervised feature clustering can not group un-seen features (features that do not occur in labeleddata) into meaningful clusters since there are noclass labels associated with these unseen features.Our algorithm can overcome this problem by induc-ing class labels for unseen features based on the sim-ilarity among seen features and unseen features, thenclustering all the features (including both seen fea-tures and unseen features) based on the distributionof class labels over them.Compared with the semi-supervised version ofIDC algorithm, our algorithm is more efficient, sincewe perform feature clustering without iterations.The difference between our algorithm and unsu-pervised feature clustering is that our algorithm de-pends on both labeled and unlabeled data, but unsu-pervised feature clustering requires only unlabeleddata.O?Hara et.
al.
(2004) use semantic class-based collocations to augment traditional word-based collocations for supervised WSD.
Three sep-arate sources of word relatedness are used forthese collocations: 1) WordNet hypernym rela-tions; 2) cluster-based word similarity classes; and3) dictionary definition analysis.
Their systemachieved 56.6% fine-grained score on ELS task ofSENSEVAL-3.
In contrast with their work, our data-driven method for feature clustering based WSDdoes not require external knowledge resource.
Fur-thermore, our SemiFC+LPJS method can achieve69.0% fine-grained score on the same dataset, whichshows the effectiveness of our method.6 ConclusionIn this paper we have investigated feature clusteringtechniques for WSD, which usually group featuresinto clusters based on the distribution of class labelsover features.
We propose a semi-supervised fea-ture clustering algorithm to satisfy the requirementof semi-supervised classification algorithms for di-mensionality reduction in feature space.
Our ex-perimental results on SENSEVAL-3 data show thatfeature clustering can aggressively reduce the di-mensionality of feature space while still maintainingstate of the art sense disambiguation accuracy.
Fur-thermore, when combined with a semi-supervisedWSD algorithm, semi-supervised feature cluster-ing outperforms supervised feature clustering andother dimensionality reduction techniques.
Our ad-ditional experiments on sampled SENSEVAL-3 dataindicate that our semi-supervised feature clusteringmethod is robust to the noise in small labeled data,which achieves better performance than supervisedfeature clustering.In the future, we may extend our work by usingmore datasets to empirically evaluate this featureclustering algorithm.
This semi-supervised featureclustering framework is quite general, which can beapplied to other NLP tasks, ex.
text categorization.Acknowledgements We would like to thankanonymous reviewers for their helpful comments.Z.Y.
Niu is supported by A*STAR Graduate Schol-arship.ReferencesBaker L. & McCallum A.. 1998.
Distributional Clus-tering of Words for Text Classification.
ACM SIGIR9131998.Bekkerman, R., El-Yaniv, R., Tishby, N., & Winter, Y..2003.
Distributional Word Clusters vs.
Words forText Categorization.
Journal of Machine Learning Re-search, Vol.
3: 1183-1208.Dagan, I.
& Itai A.. 1994.
Word Sense Disambigua-tion Using A Second Language Monolingual Corpus.Computational Linguistics, Vol.
20(4), pp.
563-596.Deerwester, S.C., Dumais, S.T., Landauer, T.K., Furnas,G.W., & Harshman, R.A.. 1990.
Indexing by LatentSemantic Analysis.
Journal of the American Societyof Information Science, Vol.
41(6), pp.
391-407.Dhillon I.. 2001.
Co-Clustering Documents and WordsUsing Bipartite Spectral Graph Partitioning.
ACMSIGKDD 2001.Dhillon I., Mallela S., & Kumar R.. 2002.
EnhancedWord Clustering for Hierarchical Text Classification.ACM SIGKDD 2002.Dhillon I., Mallela S., & Modha, D.. 2003.
Information-Theoretic Co-Clustering.
ACM SIGKDD 2003.El-Yaniv, R., & Souroujon, O.. 2001.
Iterative Dou-ble Clustering for Unsupervised and Semi-SupervisedLearning.
NIPS 2001.Forman, G.. 2003.
An Extensive Empirical Study of Fea-ture Selection Metrics for Text Classification.
Journalof Machine Learning Research 3(Mar):1289?1305.Leacock, C., Miller, G.A.
& Chodorow, M.. 1998.
Us-ing Corpus Statistics and WordNet Relations for SenseIdentification.
Computational Linguistics, 24:1, 147?165.Lee, Y.K.
& Ng, H.T.. 2002.
An Empirical Evaluationof Knowledge Sources and Learning Algorithms forWord Sense Disambiguation.
EMNLP 2002, (pp.
41-48).Lesk M.. 1986.
Automated Word Sense DisambiguationUsing Machine Readable Dictionaries: How to Tell aPine Cone from an Ice Cream Cone.
ACM SIGDOC1986.Li, H. & Li, C.. 2004.
Word Translation DisambiguationUsing Bilingual Bootstrapping.
Computational Lin-guistics, 30(1), 1-22.Lin, J.
1991.
Divergence Measures Based on the Shan-non Entropy.
IEEE Transactions on Information The-ory, 37:1, 145?150.McCarthy, D., Koeling, R., Weeds, J., & Carroll, J..2004.
Finding Predominant Word Senses in UntaggedText.
ACL 2004.Mihalcea R.. 2002.
Instance Based Learning with Au-tomatic Feature Selection Applied to Word Sense Dis-ambiguation.
COLING 2002.Mihalcea R.. 2004.
Co-Training and Self-Training forWord Sense Disambiguation.
CoNLL 2004.Mihalcea R., Chklovski, T., & Kilgariff, A.. 2004.
TheSENSEVAL-3 English Lexical Sample Task.
SENSE-VAL 2004.Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2004.
Learning WordSenses With Feature Selection and Order IdentificationCapabilities.
ACL 2004.Niu, Z.Y., Ji, D.H., & Tan, C.L.. 2005.
Word SenseDisambiguation Using Label Propagation Based Semi-Supervised Learning.
ACL 2005.O?Hara, T., Bruce, R., Donner, J., & Wiebe, J..2004.
Class-Based Collocations for Word-Sense Dis-ambiguation.
SENSEVAL 2004.Park, S.B., Zhang, B.T., & Kim, Y.T.. 2000.
WordSense Disambiguation by Learning from UnlabeledData.
ACL 2000.Pedersen.
T., & Bruce, R.. 1997.
Distinguishing WordSenses in Untagged Text.
EMNLP 1997.Schu?tze, H.. 1998.
Automatic Word Sense Discrimina-tion.
Computational Linguistics, 24:1, 97?123.Slonim, N. & Tishby, N.. 2000.
Document ClusteringUsing Word Clusters via the Information BottleneckMethod.
ACM SIGIR 2000.Slonim, N. & Tishby, N.. 2001.
The Power of WordClusters for Text Classification.
The 23rd EuropeanColloquium on Information Retrieval Research.Towel, G. & Voorheest, E.M.. 1998.
DisambiguatingHighly Ambiguous Words.
Computational Linguis-tics, 24:1, 125?145.Yarowsky, D.. 1995.
Unsupervised Word Sense Disam-biguation Rivaling Supervised Methods.
ACL 1995,pp.
189-196.Zhou D., Bousquet, O., Lal, T.N., Weston, J., &Scho?lkopf, B.. 2003.
Learning with Local and GlobalConsistency.
NIPS 16,pp.
321-328.Zhu, X.
& Ghahramani, Z.. 2002.
Learning from La-beled and Unlabeled Data with Label Propagation.CMU CALD tech report CMU-CALD-02-107.Zhu, X., Ghahramani, Z., & Lafferty, J.. 2003.
Semi-Supervised Learning Using Gaussian Fields and Har-monic Functions.
ICML 2003.914
