Proceedings of the 4th International Workshop on Cross Lingual Information Access at COLING 2010, pages 26?34,Beijing, August 2010Multi-Word Expression-Sensitive Word AlignmentTsuyoshi Okita1, Alfredo Maldonado Guerra2, Yvette Graham3, Andy Way1{CNGL1, NCLT3} / School of Computing / Dublin City University,CNGL / School of Computer Science and Statistics / Trinity College Dublin2{tokita,ygraham,away}@computing.dcu.ie, maldonaa@scss.tcd.ieAbstractThis paper presents a new word align-ment method which incorporates knowl-edge about Bilingual Multi-Word Expres-sions (BMWEs).
Our method of wordalignment first extracts such BMWEs ina bidirectional way for a given corpus andthen starts conventional word alignment,considering the properties of BMWEs intheir grouping as well as their alignmentlinks.
We give partial annotation of align-ment links as prior knowledge to the wordalignment process; by replacing the max-imum likelihood estimate in the M-stepof the IBM Models with the Maximum APosteriori (MAP) estimate, prior knowl-edge about BMWEs is embedded in theprior in this MAP estimate.
In our exper-iments, we saw an improvement of 0.77Bleu points absolute in JP?EN.
Exceptfor one case, our method gave better re-sults than the method using only BMWEsgrouping.
Even though this paper doesnot directly address the issues in Cross-Lingual Information Retrieval (CLIR), itdiscusses an approach of direct relevanceto the field.
This approach could beviewed as the opposite of current trendsin CLIR on semantic space that incorpo-rate a notion of order in the bag-of-wordsmodel (e.g.
co-occurences).1 IntroductionWord alignment (Brown et al, 1993; Vogel etal., 1996; Och and Ney, 2003a; Graca et al,2007) remains key to providing high-quality trans-lations as all subsequent training stages rely on itsperformance.
It alone does not effectively cap-ture many-to-many word correspondences, but in-stead relies on the ability of subsequent heuristicphrase extraction algorithms, such as grow-diag-final (Koehn et al, 2003), to resolve them.Some aligned corpora include implicit partialalignment annotation, while for other corpora apartial alignment can be extracted by state-of-the-art techniques.
For example, implicit tagssuch as reference number within the patent cor-pus of Fujii et al (2010) provide (often many-to-many) correspondences between source and tar-get words, while statistical methods for extract-ing a partial annotation, like Kupiec et al (1993),extract terminology pairs using linguistically pre-defined POS patterns.
Gale and Church (1991)extract pairs of anchor words, such as num-bers, proper nouns (organization, person, title),dates, and monetary information.
Resnik andMelamed (1997) automatically extract domain-specific lexica.
Moore (2003) extracts named-entities.
In Machine Translation, Lambert andBanchs (2006) extract BMWEs from a phrase ta-ble, which is an outcome of word alignment fol-lowed by phrase extraction; this method does notalter the word alignment process.This paper introduces a new method of incorpo-rating previously known many-to-many word cor-respondences into word alignment.
A well-knownmethod of incorporating such prior knowledgein Machine Learning is to replace the likelihoodmaximization in the M-step of the EM algorithmwith either the MAP estimate or the MaximumPenalized Likelihood (MPL) estimate (McLach-26lan and Krishnan, 1997; Bishop, 2006).
Then, theMAP estimate allows us to incorporate the prior,a probability used to reflect the degree of prior be-lief about the occurrences of the events.A small number of studies have been carriedout that use partial alignment annotation for wordalignment.
Firstly, Graca et al (2007) introducea posterior regularization to employ the prior thatcannot be easily expressed over model parameterssuch as stochastic constraints and agreement con-straints.
These constraints are set in the E-step todiscard intractable alignments contradicting theseconstraints.
This mechanism in the E-step is in asimilar spirit to that in GIZA++ for IBM Model3 and 4 which only searches around neighbour-ing alignments around the Viterbi alignment.
Forthis reason, this algorithm is not intended to beused combined with IBM Models 3 and 4.
Al-though theoretically it is possible to incorporatepartial annotation with a small change in its code,Graca et al do not mention it.
Secondly, Tal-bot (2005) introduces a constrained EM methodwhich constrains the E-step to incorporate par-tial alignment into word alignment,1 which is ina similar manner to Graca et al (2007).
He con-ducted experiments using partial alignment anno-tation based on cognate relations, a bilingual dic-tionary, domain-specific bilingual semantic anno-tation, and numerical pattern matching.
He didnot incorporate BMWEs.
Thirdly, Callison-Burchet al (2004) replace the likelihood maximizationin the M-step with mixed likelihood maximiza-tion, which is a convex combination of negativelog likelihood of known links and unknown links.The remainder of this paper is organized as fol-lows: in Section 2 we define the anchor wordalignment problem.
In Section 3 we includea review of the EM algorithm with IBM Mod-els 1-5, and the HMM Model.
Section 4 de-scribes our own algorithm based on the combina-tion of BMWE extraction and the modified wordalignment which incorporates the groupings ofBMWEs and enforces their alignment links; weexplain the EM algorithm with MAP estimation1Although the code may be similar in practice to our PriorModel I, his explanation to modify the E-step will not beapplied to IBM Models 3 and 4.
Our view is to modify theM-step due to the same reason above, i.e.
GIZA++ searchesonly over the alignment space around the Viterbi alignment.pair GIZA++(no prior) Ours(with prior)EN-FR fin ini prior fin ini prioris NULL 1 .25 0 0 .25 .25rosy en 1 .5 0 0 .5 .2that .
1 .25 0 0 .25 .25life la 1 .25 0 0 .25 0. c?
1 .25 0 0 .25 .25that c?
0 .25 0 1 .25 .25is est 0 .25 0 1 .25 .25life vie 0 .5 0 1 .5 1rosy rose 0 .25 0 1 .25 .2Table 1: The benefit of prior knowledge of anchorwords.with three kinds of priors.
In Section 5 our exper-imental results are presented, and we conclude inSection 6.2 Anchor Word Alignment ProblemThe input to standard methods of word alignmentis simply the sentence-aligned corpus, whereasour alignment method takes in additionally a par-tial alignment.
We assume, therefore, the avail-ability of a partial alignment, for example via aMWE extraction tool.
Let e?
denote an Englishsentence, and e denote an English word, through-out this paper.
The anchor word alignment prob-lem is defined as follows:Definition 1 (Anchor Word Alignment Problem)Let (e?, f?)
= {(e?1, f?1), .
.
.
, (e?n, f?n)} be a parallelcorpus.
By prior knowledge we additionallyhave knowledge of anchor words (e?, f?)
={(senti, te1, tf1 , pose1, posf1 , lengthe, lengthf ),.
.
., (sentk, ten , tfn , posen , posfn , lengthe,lengthf )} where senti denotes sentence ID,posei denotes the position of tei in a sentence e?i,and lengthe (and lengthf ) denotes the sentencelength of the original sentence which includesei.
Under a given (e?, f?)
and (e?, f?
), our objectiveis to obtain word alignments.
It is noted that ananchor word may include a phrase pair whichforms n-to-m mapping objects.Table 1 shows two example phrase pairs forFrench to English c?est la vie and that is life, andla vie en rose and rosy life with the initial valuefor the EM algorithm, the prior value and the fi-27Statistical MWE extraction method97|||groupe socialiste|||socialist group|||26|||26101|||monsieur poettering|||mr poettering|||1|||4103|||monsieur poettering|||mr poettering|||1|||11110|||monsieur poettering|||mr poettering|||1|||9117|||explication de vote|||explanation of vote|||28|||26Heuristic-based MWE extraction method28|||the wheel 2|||??
?||| 25||| 528|||the primary-side fixed armature 13|||?
?
?
??
??
?
?
?||| 13||| 928|||the secondary-side rotary magnet 7|||?
?
?
??
?????
?||| 15||| 11Table 2: Example of MWE pairs in Europarl cor-pus (FR-EN) and NTCIR patent corpus (JP-EN).There are 5 columns for each term: sentence num-ber, source term, target term, source position, andtarget position.
The number appended to eachterm from the patent corpus (lower half) is a ref-erence number.
In this corpus, all the importanttechnical terms have been identified and annotatedwith reference numbers.nal lexical translation probability for Giza++ IBMModel 4 and that of our modified Giza++.
Ourmodified Giza++ achieves the correct result whenanchor words ?life?
and ?vie?
are used to assign avalue to the prior in our model.3 Word AlignmentWe review two models which address the prob-lem of word alignment.
The aim of word align-ment is to obtain the model parameter t amongEnglish and French words, ei and fj respectively.We search for this model parameter under somemodel M where M is chosen by IBM Models 1-5 and the HMM model.
We introduce the latentvariable a, which is an alignment function withthe hypothesis that each e and f correspond to thislatent variable.
(e, f, a) is a complete data set, and(e, f) is an incomplete data set.3.1 EM AlgorithmWe follow the description of the EM algorithm forIBM Models of Brown et al (1993) but introducethe parameter t explicitly.
In this model, the pa-rameter t represents the lexical translation proba-bilities t(ei|fj).
It is noted that we use e|f ratherthan f |e following the notation of Koehn (2010).One important remark is that the Viterbi align-ment of the sentence pair (e?, f?)
= (eJ1 , f I1 ), whichis obtained as in (1):Eviterbi : a?J1 = argmaxaJ1p??
(f, a|e) (1)provides the best alignment for a given log-likelihood distribution p??
(f, a|e).
Instead of sum-ming, this step simplifies the E-step.
However, un-der our modification of maximum likelihood esti-mate with MAP estimate, this simplification is nota correct approximation of the summation sinceour surface in the E-step is greatly perturbed bythe prior.
There is no guarantee that the Viterbialignment is within the proximity of the targetalignment (cf.
Table 1).Let z be the latent variable, t be the parameters,and x be the observations.
The EM algorithm isan iterative procedure repeating the E-step and theM-step as in (2):EEXH : q(z;x) =p(z|x; ?)
(2)MMLE : t?
= argmaxtQ(t, told)= argmaxt?x,zq(z|x) log p(x, z; t)In the E-step, our knowledge of the values of thelatent variables in a is given only by the poste-rior distribution p(a|e, f, t).
Hence, the (negativelog)-likelihood of complete data (e, f, a), whichwe denote by ?
log p(t|e, f, a), is obtained overall possible alignments a.
We use the current pa-rameter values told to find the posterior distribu-tion of the latent variables given by p(a|e, f, told).We then use this posterior distribution to find theexpectation of the complete data log-likelihoodevaluated for parameter value t. This expectationis given by?a p(a|e, f, told) log p(e, f, a|t).In the M-step, we use a maximal likelihood es-timation to minimize negative log-likelihood inorder to determine the parameter t; note that t isa lexical translation probability.
Instead of usingthe log-likelihood log p(a, e, f |t), we use the ex-pected complete data log-likelihood over all thepossible alignments a that we obtained in the E-28step, as in (3):MMLE : t?
= argmaxtQ(t, told) (3)= c(f |e; f, e)?e c(f |e; f, e)where an auxiliary function c(e|f ; e, f) for IBMModel 1 introduced by Brown et al is defined asc(f |e; f, e) =?ap(a|e, f)m?j=1?
(f, fj)?
(e, eaj )and where the Kronecker-Delta function ?
(x, y) is1 if x = y and 0 otherwise.
This auxiliary func-tion is convenient since the normalization factor ofthis count is also required.
We note that if we usethe MAP estimate, the E-step remains the same asin the maximum likelihood case, whereas in theM-step the quantity to be minimized is given byQ(t, told) + log p(t).
Hence, we search for thevalue of t which maximizes the following equa-tion:MMAP : t?
= argmaxtQ(t, told) + log p(t)3.2 HMMA first-order Hidden Markov Model (Vogel et al,1996) uses the sentence length probability p(J |I),the mixture alignment probability p(i|j, I), andthe translation probability, as in (4):p(f |e) = p(J |I)J?j=1p(fj|ei) (4)Suppose we have a training set of R observationsequences Xr, where r = 1, ?
?
?
, R, each of whichis labelled according to its class m, where m =1, ?
?
?
,M , as in (5):p(i|j, I) = r(i?
jIJ )?Ii?=1 r(i?
?
j IJ )(5)The HMM alignment probabilities p(i|i?, I) de-pend only on the jump width (i ?
i?).
Using a setof non-negative parameters s(i?
i?
), we have (6):p(i|i?, I) = s(i ?
i?
)?Il=1 s(l ?
i?
)(6)4 Our ApproachAlgorithm 1 Overall AlgorithmGiven: a parallel corpus,1.
Extract MWEs by Algorithm 2.2.
Based on the results of Step 1, specify a setof anchor word alignment links in the format ofanchor word alignment problem (cf.
Definition1 and Table 2).3.
Group MWEs in source and target text.4.
Calculate the prior in order to embed knowl-edge about anchor words.5.
Calculate lexical translation probabilitieswith the prior.6.
Obtain alignment probabilities.7.
Ungroup of MWEs in source and target text.Algorithm 1 consists of seven steps.
We use theModel I prior for the case where our prior knowl-edge is sparse and evenly distributed throughoutthe corpus, whereas we use the Model II priorwhen our prior knowledge is dense in a partialcorpus.
A typical example of the former caseis when we use partial alignment annotation ex-tracted throughout a corpus for bilingual terminol-ogy.
A typical example of the latter case is when asample of only a few hundred lines from the cor-pus have been hand-annotated.4.1 MWE ExtractionOur algorithm of extracting MWEs is a statisti-cal method which is a bidirectional version of Ku-piec (1993).
Firstly, Kupiec presents a method toextract bilingual MWE pairs in a unidirectionalmanner based on the knowledge about typicalPOS patterns of noun phrases, which is language-dependent but can be written down with some easeby a linguistic expert.
For example in French theyare N N, N prep N, and N Adj.
Secondly, we takethe intersection (or union) of extracted bilingualMWE pairs.22In word alignment, bidirectional word alignment by tak-ing the intersection or union is a standard method whichimproves its quality compared to unidirectional word align-ment.29Algorithm 2 MWE Extraction AlgorithmGiven: a parallel corpus and a set of anchorword alignment links:1.
We use a POS tagger (Part-Of-Speech Tag-ger) to tag a sentence on the SL side.2.
Based on the typical POS patterns for the SL,extract noun phrases on the SL side.3.
Count n-gram statistics (typically n =1, ?
?
?
, 5 are used) on the TL side which jointlyoccur with each source noun phrase extractedin Step 2.4.
Obtain the maximum likelihood counts ofjoint phrases, i.e.
noun phrases on the SL sideand n-gram phrases on the TL side.5.
Repeat the same procedure from Step 1 to 4reversing the SL and TL.6.
Intersect (or union) the results in both direc-tions.Let SL be the source language side and TL bethe target language side.
The procedure is shownin Algorithm 2.
We informally evaluated theMWE extraction tool following Kupiec (1993) bymanually inspecting the mapping of the 100 mostfrequent terms.
For example, we found that 93 ofthe 100 most frequent English terms in the patentcorpus were correctly mapped to their Japanesetranslation.Depending on the corpus, we can use moreprior knowledge about implicit alignment links.For example in some categories of patent andtechnical documents corpora,3 we can use heuris-tics to extract the ?noun phrase?
+ ?referencenumber?
from both sides.
This is due to the factthat terminology is often labelled with a uniquereference number, which is labelled on both theSL and TL sides.4.2 Prior Model IPrior for Exhaustive Alignment Space IBMModels 1 and 2 implement a prior for all possible3Unlike other language pairs, the availability ofJapanese?English parallel corpora is quite limited: the NT-CIR patent corpus (Fujii et al, 2010) of 3 million sentencepairs (the latest NTCIR-8 version) for the patent domain andJENAAD corpus (Utiyama and Isahara, 2003) of 150k sen-tence pairs for the news domain.
In this regard, the patentdomain is particularly important for this particular languagepair.Algorithm 3 Prior Model I for IBM Model 1Given: parallel corpus e?, f?
,anchor words biTerminitialize t(e|f ) uniformlydo until convergenceset count(e|f ) to 0 for all e,fset total(f) to 0 for all ffor all sentence pairs (e?s,f?s)prior(e|f)s = getPriorModelI(e?, f?
, biT erm)for all words e in e?stotals(e) = 0for all words f in f?stotals(e) += t(e|f )for all words e in e?sfor all words f in f?scount(e|f )+=t(e|f)/totals(e)?
prior(e|f)stotal(f) += t(e|f)/totals(e) ?
prior(e|f)sfor all ffor all et(e|f ) = count(e|f)/total(f)alignments exhaustively.
Such a prior requires thefollowing two conditions.
Firstly, partial knowl-edge about the prior that we use in our context isdefined as follows.
Let us denote a bilingual termlist T = {(s1, t1), .
.
.
, (sm, tm)}.
For examplewith IBM Model 1: Let us define the followingprior p(e|f, e, f ;T ) from Equation (4):p(e|f, e, f ;T ) =??
?1 (ei = si, fj = tj)0 (ei = si, fj 6= tj)0 (ei 6= si, fj = tj)uniform (ei 6= si, fj 6= tj)Secondly, this prior should be proper for the ex-haustive case and non-proper for the sampledalignment space where by proper we mean that theprobability is normalized to 1.
Algorithm 3 showsthe pseudo-code for Prior Model I.
Note that ifthe prior is uniform in the MAP estimation, this isequivalent to maximum likelihood estimation.Prior for Sampled Alignment (Function) SpaceDue to the exponential costs introduced by fertil-ity, null token insertion, and distortion probability,IBM Models 3 and 4 do not consider all (I + 1)Jalignments exhaustively, but rather a small subsetin the E-step.
Each iteration only uses the sub-set of all the alignment functions: this sampling30is not uniform, as it only includes the best possi-ble alignment with all its neighbouring alignmentswhich differ from the best alignment by one word(this can be corrected by a move operation) or twowords (this can be corrected by a swap operation).If we consider the neighbouring alignment viaa move or a swap operation, two issues arise.Firstly, the fact that these two neighbouring align-ments are drawn from different underlying distri-butions needs to be taken into account, and sec-ondly, that the application of a move and a swapoperation alters a row or column of a prior ma-trix (or indices of the prior) since either operationinvolves the manipulation of links.Algorithm 4 Pseudo-code for Prior Model II Ex-haustive Alignment Spacedef getPriorModelII(e?,f?
,biTerm):for i in sentence:for e in e?i:allWordsi = length of sentence e?for f in f?i:if (e, f ) in biTerm:n= num of anchor words in iuni(e|f)i = allWordsi?nallWordsiexpSum(e|f) += uni(e|f)i ?
nelse:countSum(e|f)i += ncountSum(e|f) += count(e|f)ifor e in alle:for f in allf :prior(e|f) = expSum(e|f) + countSum(e|f)return prior(e|f)Prior for Jump Width i?
One implementationof HMM is to use the forward-backward algo-rithm.
A prior should be embedded within theforward-backward algorithm.
From Equation (6),there are three cases which depend on whetherai and its neighbouring alignment ai?1 are deter-mined by our prior knowledge about anchor wordsor not.
When both ai and aj are determined, thisprobability is expressed as in (7):p(i?
i?
; I) =??
?0 (else) (7)1 (ei = si, fj = tj for ai) and(e?i = s?i, f ?j = t?j for aj)When either ai or aj is determined, this probabil-ity is expressed as in (8):4p(i?
i?
; I) =??????
?0 (condition 1) (8)1 (condition 2)1(m?#eai????
?#eai+m)(else)(uniform distribution)When neither ai nor aj is determined, this proba-bility is expressed as in (9): 5p(i?
i?
; I) =????????
?0 (condition 3) (9)1 (condition 4)m?i?(m?#eai????
?#eai+m)2(else)(Pascal?s triangle distribution)4.3 Prior Model IIPrior Model II assumes that we have prior knowl-edge only in some part of the training corpus.
Atypical example is when a small part of the corpushas a hand-crafted ?gold standard?
annotation.Prior for Exhaustive Alignment Space PriorModel II is used to obtain the prior probabilityp(e|f) over all possible combinations of e and f .In contrast to Prior Model I, which computes theprior probability p(e|f) for each sentence, PriorModel II computes the prior probability globallyfor all sentences in the corpus.
Algorithm 4 showsthe pseudo-code for Prior Model II ExhaustiveAlignment Space.4condition 1 is as follows:((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))?condition 2?
is as follows:((ei = si, fj 6= tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or((ei 6= si, fj = tj for ai) and (e?i = s?i, f ?j = t?j for aj)) or((ei = si, fj = tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or((ei = si, fj = tj for ai) and (e?i = s?i, f ?j 6= t?j for aj))5?condition 3?
is as follows:((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))?condition 4?
is as follows:((ei 6= si, fj 6= tj for ai) and (e?i 6= s?i, f ?j = t?j for aj)) or((ei 6= si, fj 6= tj for ai) and (e?i = s?i, f ?j 6= t?j for aj)) or((ei 6= si, fj = tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj)) or((ei = si, fj 6= tj for ai) and (e?i 6= s?i, f ?j 6= t?j for aj))31Prior for Sampled Alignment (Function) SpaceThis is identical to that of the Prior Model II ex-haustive alignment space with only a difference inthe normalization process.Prior for Jump Width i?
This categorization ofPrior Model II is the same as that of Prior Model Ifor for Jump Width i?
(see Section 4.2).
Note thatPrior Model II requires more memory comparedto the Prior Model I.65 Experimental SettingsThe baseline in our experiments is a standardlog-linear phrase-based MT system based onMoses.
The GIZA++ implementation (Och andNey, 2003a) of IBM Model 4 is used as the base-line for word alignment, which we compare toour modified GIZA++.
Model 4 is incrementallytrained by performing 5 iterations of Model 1, 5iterations of HMM, 5 iterations of Model 3, and5 iterations of Model 4.
For phrase extraction thegrow-diag-final heuristics are used to derive therefined alignment from bidirectional alignments.We then perform MERT while a 5-gram languagemodel is trained with SRILM.
Our implementa-tion is based on a modified version of GIZA++(Och and Ney, 2003a).
This modification is on thefunction that reads a bilingual terminology file,the function that calculates priors, the M-step inIBM Models 1-5, and the forward-backward algo-rithm in the HMM Model.
Other related softwaretools are written in Python and Perl: terminol-ogy concatenation, terminology numbering, andso forth.6 Experimental ResultsWe conduct an experimental evaluation on theNTCIR-8 corpus (Fujii et al, 2010) and on Eu-roparl (Koehn, 2005).
Firstly, MWEs are ex-tracted from both corpora, as shown in Table 3.In the second step, we apply our modified versionof GIZA++ in which we incorporate the results of6This is because it needs to maintain potentially an ?
?mmatrix, where ?
denotes the number of English tokens in thecorpus and m denotes the number of foreign tokens, even ifthe matrix is sparse.
Prior Model I only requires an ??
?
m?matrix where ??
is the number of English tokens in a sentenceand m?
is the number of foreign tokens in a sentence, whichis only needed until this information is incorporated in a pos-terior probability during the iterative process.corpus language size #unique #allMWEs MWEsstatistical methodNTCIR EN-JP 200k 1,121 120,070europarl EN-FR 200k 312 22,001europarl EN-ES 200k 406 16,350heuristic methodNTCIR EN-JP 200k 50,613 114,373Table 3: Statistics of our MWE extraction method.The numbers of MWEs are from 0.08 to 0.6 MWE/ sentence pair in our statistical MWE extractionmethods.MWE extraction.
Secondly, in order to incorpo-rate the extracted MWEs, they are reformatted asshown in Table 2.
Thirdly, we convert all MWEsinto a single token, i.e.
we concatenate them withan underscore character.
We then run the modi-fied version of GIZA++ and obtain a phrase andreordering table.
In the fourth step, we split theconcatenated MWEs embedded in the third step.Finally, in the fifth step, we run MERT, and pro-ceed with decoding before automatically evaluat-ing the translations.Table 4 shows the results where ?baseline?
in-dicates no BMWE grouping nor prior, and ?base-line2?
represents a BMWE grouping but withoutthe prior.
Although ?baseline2?
(BMWE group-ing) shows a drop in performance in the JP?EN/ EN?JP 50k sentence pair setting, Prior Model Iresults in an increase in performance in the samesetting.
Except for EN?ES 200k, our Prior ModelI was better than ?baseline2?.
For EN?JP NT-CIR using 200k sentence pairs, we obtained anabsolute improvement of 0.77 Bleu points com-pared to the ?baseline?
; for EN?JP using 50k sen-tence pairs, 0.75 Bleu points; and for ES?EN Eu-roparl corpus using 200k sentence pairs, 0.63 Bleupoints.
In contrast, Prior Model II did not workwell.
The possible reason for this is the misspec-ification, i.e.
the modelling by IBM Model 4 waswrong in terms of the given data.
One piece of ev-idence for this is that most of the enforced align-ments were found correct in a manual inspection.For EN?JP NTCIR using the same corpus of200k, although the number of unique MWEs ex-32size EN-JP Bleu JP-EN Bleu50k baseline 16.33 baseline 22.0150k baseline2 16.10 baseline2 21.7150k prior I 17.08 prior I 22.1150k prior II 16.02 prior II 20.02200k baseline 23.42 baseline 21.68200k baseline2 24.10 baseline2 22.32200k prior I 24.22 prior I 22.45200k prior II 23.22 prior II 21.00size FR-EN Bleu EN-FR Bleu50k baseline 17.68 baseline 17.8050k baseline2 17.76 baseline2 18.0050k prior I 17.81 prior I 18.0250k prior II 17.01 prior II 17.30200k baseline 18.40 baseline 18.20200k baseline2 18.80 baseline2 18.50200k prior I 18.99 prior I 18.60200k prior II 18.20 prior II 17.50size ES-EN Bleu EN-ES Bleu50k baseline 16.21 baseline 15.1750k baseline2 16.61 baseline2 15.6050k prior I 16.91 prior I 15.8750k prior II 16.15 prior II 14.60200k baseline 16.87 baseline 17.62200k baseline2 17.40 baseline2 18.21200k prior I 17.50 prior I 18.20200k prior II 16.50 prior II 17.10Table 4: Results.
Baseline is plain GIZA++ /Moses (without BMWE grouping / prior), base-line2 is with BMWE grouping, prior I / II are withBMWE grouping and prior.tracted by the statistical method and the heuris-tic method varies significantly, the total numberof MWEs by each method becomes comparable.The resulting Bleu score for the heuristic method(24.24 / 22.48 Blue points for 200k EN?JP / JP?EN) is slightly better than that of the statisticalmethod.
The possible reason for this is relatedto the way the heuristic method groups terms in-cluding reference numbers, while the statisticalmethod does not.
As a result, the complexity ofthe alignment model simplifies slightly in the caseof the heuristic method.7 ConclusionThis paper presents a new method of incorporat-ing BMWEs into word alignment.
We first de-tect BMWEs in a bidirectional way and then usethis information to do groupings and to enforcealready known alignment links.
For the latter pro-cess, we replace the maximum likelihood estimatein the M-step of the EM algorithm with the MAPestimate; this replacement allows the incorpora-tion of the prior in the M-step of the EM algo-rithm.
We include an experimental investigationinto incorporating extracted BMWEs into a wordaligner.
Although there is some work which incor-porates BMWEs in groupings, they do not enforcealignment links.There are several ways in which this work canbe extended.
Firstly, although we assume that oura priori partial annotation is reliable, if we extractsuch MWEs automatically, we cannot avoid erro-neous pairs.
Secondly, we assume that the rea-son why our Prior Model II did not work was dueto the misspecification (or wrong modelling).
Wewould like to check this by discriminative mod-elling.
Thirdly, although here we extract BMWEs,we can extend this to extract paraphrases and non-literal expressions.8 AcknowledgmentsThis research is supported by the Science Foun-dation Ireland (Grant 07/CE/I1142) as part ofthe Centre for Next Generation Localisation(http://www.cngl.ie) at Dublin City Uni-versity and Trinity College Dublin.
We would alsolike to thank the Irish Centre for High-End Com-puting.ReferencesBishop, Christopher M. 2006.
Pattern Recognitionand Machine Learning.
Springer.
Cambridge, UKBrown, Peter F., Vincent .J.D Pietra, StephenA.D.Pietra, Robert L. Mercer.
1993.
The Mathe-matics of Statistical Machine Translation: Param-eter Estimation.
Computational Linguistics.
19(2),pp.
263?311.Callison-Burch, Chris, David Talbot and Miles Os-borne.
2004.
Statistical Machine Translation with33Word- and Sentence-Aligned Parallel Corpora.
Pro-ceedings of the 42nd Annual Meeting of the As-sociation for Computational Linguistics (ACL?04),Main Volume.
Barcelona, Spain, pp.
175?182.Fujii, Atsushi, Masao Utiyama, Mikio Yamamoto,Takehito Utsuro, Terumasa Ehara, Hiroshi Echizen-ya, Sayori Shimohata.
2010.
Overview of thePatent Translation Task at the NTCIR-8 Workshop.Proceedings of the 8th NTCIR Workshop Meet-ing on Evaluation of Information Access Technolo-gies: Information Retrieval, Question Answeringand Cross-lingual Information Access, pp.
293?302.Graca, Joao de Almeida Varelas, Kuzman Ganchev,Ben Taskar.
2007.
Expectation Maximizationand Posterior Constraints.
In Neural InformationProcessing Systems Conference (NIPS), Vancouver,BC, Canada, pp.
569?576.Gale, William, and Ken Church.
1991.
A Program forAligning Sentences in Bilingual Corpora.
In Pro-ceedings of the 29th Annual Meeting of the Associ-ation for Computational Linguistics.
Berkeley CA,pp.
177?184.Koehn, Philipp, Franz Och, Daniel Marcu.
2003.
Sta-tistical Phrase-Based Translation.
In Proceedingsof the 2003 Human Language Technology Confer-ence of the North American Chapter of the Asso-ciation for Computational Linguistics.
Edmonton,Canada.
pp.
115?124.Koehn, Philipp.
2005.
Europarl: A Parallel Corpusfor Statistical Machine Translation.
In ConferenceProceedings: the tenth Machine Translation Sum-mit.
Phuket, Thailand, pp.79-86.Koehn, Philipp, H. Hoang, A. Birch, C. Callison-Burch, M. Federico, N. Bertoldi, B. Cowan,W.
Shen, C. Moran, R. Zens, C. Dyer, O. Bojar,A.
Constantin, and E. Herbst, 2007.
Moses: Opensource toolkit for Statistical Machine Translation.Proceedings of the 45th Annual Meeting of the As-sociation for Computational Linguistics CompanionVolume Proceedings of the Demo and Poster Ses-sions, Prague, Czech Republic, pp.
177?180.Koehn, Philipp.
2010.
Statistical Machine Transla-tion.
Cambridge University Press.
Cambridge, UK.Kupiec, Julian.
1993.
An Algorithm for finding NounPhrase Correspondences in Bilingual Corpora.
InProceedings of the 31st Annual Meeting of Associa-tion for Computational Linguistics.
Columbus.
OH.pp.
17?22.Lambert, Patrik and Rafael Banchs.
2006.
Group-ing Multi-word Expressions According to Part-Of-Speech in Statistical Machine Translation.
In Pro-ceedings of the EACL Workshop on Multi-Word-Expressions in a Multilingual Context.
Trento, Italy,pp.
9?16.McLachlan, Geoffrey J. and Thriyambakam Krishnan,1997.
The EM Algorithm and Extensions.
WileySeries in probability and statistics.
New York, NY.Moore, Robert C.. 2003.
Learning Translations ofNamed-Entity Phrases from Parallel Corpora.
InProceedings of the 11th Conference of the EuropeanChapter of the Association for Computational Lin-guistics.
Budapest, Hungary.
pp.
259?266.Moore, Robert C.. 2004.
On Log-Likelihood-Ratiosand the Significance of Rare Events.
In Proceedingsof the 2004 Conference on Empirical Methods inNatural Language Processing (EMNLP).
Barcelona,Spain, pp.
333?340.Och, Franz and Herman Ney.
2003.
A SystematicComparison of Various Statistical Alignment Mod-els.
Computational Linguistics.
29(1), pp.
19?51.Resnik, Philip and I. Dan Melamed, 1997.
Semi-Automatic Acquisition of Domain-Specific Transla-tion Lexicons.
Proceedings of the 5th Applied Nat-ural Language Processing Conference.
Washington,DC., pp.
340?347.Talbot, David.
2005.
Constrained EM for parallel textalignment, Natural Language Engineering, 11(3):pp.
263?277.Utiyama, Masao and Hitoshi Isahara.
2003.
ReliableMeasures for Aligning Japanese-English News Arti-cles and Sentences, In Proceedings of the 41st An-nual Meeting of the Association for ComputationalLinguistics.
Sapporo, Japan, pp.
72?79.Vogel, Stephan, Hermann Ney, Christoph Tillmann1996.
HMM-Based Word Alignment in Statisti-cal Translation.
In Proceedings of the 16th Inter-national Conference on Computational Linguistics.Copenhagen, Denmark, pp.
836?841.34
