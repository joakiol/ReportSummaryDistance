Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 408?413, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsECNUCS: A Surface Information Based System Description of SentimentAnalysis in Twitter in the SemEval-2013 (Task 2)Tian Tian ZHU and Fang Xi ZHANG and Man LAN?Department of Computer Science and TechnologyEast China Normal University51111201046,51111201041@ecnu.edu.cn; mlan@cs.ecnu.edu.cnAbstractThis paper briefly reports our submissionsto the two subtasks of Semantic Analysis inTwitter task in SemEval 2013 (Task 2), i.e.,the Contextual Polarity Disambiguation task(an expression-level task) and the MessagePolarity Classification task (a message-leveltask).
We extract features from surface infor-mation of tweets, i.e., content features, Micro-blogging features, emoticons, punctuation andsentiment lexicon, and adopt SVM to buildclassifier.
For subtask A, our system on twit-ter data ranks 2 on unconstrained rank and onSMS data ranks 1 on unconstrained rank.1 IntroductionMicro-blogging today has become a very popularcommunication tool among Internet users.
Millionsof messages are appearing daily in popular web sitesthat provide services for Micro-blogging and onepopularly known is Twitter1.
Through the twitterplatform, users share either information or opin-ions about personalities, politicians, products, com-panies, events (Prentice and Huffman, 2008) etc.
Asa result of the rapidly increasing number of tweets,mining sentiments expressed in tweets has attractedmore and more attention, which is also one of thebasic analysis utility functions needed by various ap-plications.The task of Sentiment Analysis in Twitter isto identify the sentiment of tweets and get a bet-ter understanding of how sentiment is conveyed in1http://www.twitter.comtweets and texts, which consists of two sub-tasks,i.e., the Contextual Polarity Disambiguation task(an expression-level task) and the Message PolarityClassification task (a message-level task).
The con-textual polarity disambiguation task (subtask A) isto determine whether a given message containing amarked instance of a word or a phrase is positive,negative or neutral in that context.
The messagepolarity classification task (subtask B) is to decidewhether a given message is of positive, negative, orneutral sentiment and for messages conveying botha positive and negative sentiment, whichever is thestronger sentiment should be chosen (Wilson et al2013).
We participate in these two tasks.In recent years, many researchers have proposedmethods to analyze sentiment in twitter.
For exam-ple, (Pak and Paroubek, 2010) used a Part of Speech(POS) tagger on the tweets and found that some POStaggers can help identify the sentiment of tweets.They found that objective tweets often contain morenouns than subjective tweets.
However, subjectivetweets may carry more adjectives and adverbs thanobjective tweets.
Besides, (Davidov et al 2010)proved that emoticon and punctuation like excla-mation mark are good features when distinguishingthe sentiment of tweets.
In addition, some senti-ment lexicons like SentiWordNet (Baccianella et al2010) and MPQA Subjectivity Lexicon (Wilson etal., 2009) have been adopted to calculate the senti-ment score of tweets (Zirn et al 2011).The rest of this paper is organized as follows.
Sec-tion 2 describes our approach for subtask 1, i.e.,the Contextual Polarity Disambiguation task.
Sec-tion 3 describes our approach for subtask 2, i.e., the408message polarity classification task.
Concluding re-marks is in Section 4.2 System Description of ContextualPolarity DisambiguationFor the Contextual Polarity Disambiguation task,we first extract features from multiple aspects, i.e.,punctuation, emoticons, POS tags, instance lengthand sentiment lexicon features.
Then we adopt poly-nomial SVM to build classification models.
Accord-ing to the definition of this task, the given instancehas been marked by a start position and an end posi-tion rather than a whole tweet.
So we first record thefrequency of the first three kinds of features in thisgiven instance.
To avoid interference from the num-ber of words in given instance, we then normalizethe feature values by the length of instance.2.1 PreprocessingTypically, most tweets contain informal languageexpressions, with creative spelling and punctuation,misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, such as,?RT?
for ?re-tweet?
and #hashtags, which are a typeof tagging for Twitter messages.
Therefore, workingwith these informal text genres presents challengesfor natural language processing beyond those typ-ically encountered when working with more tradi-tional text genres, such as newswire data.
So weperform text preprocessing in order to remedy asmany informal texts as possible.
Firstly, we per-form normalization to convert creative spelling andmisspelling into its right spelling.
For example, anyrepetition of more than 3 continuous letters are re-duced back to 1 letter (e.g.
?noooo?
is reduced to?no?).
In addition, according to the Internet slangdictionary2, we convert each slang to its completeform, for example, ?aka?
is rewritten as ?also knownas?.
After that, we use the Stanford parser3 for to-kenization and the Stanford POS Tagger4 for POStagging.
Finally, Natural Language Toolkit5 is usedfor WordNet based Lemmatization.2http://www.noslang.com3http://nlp.stanford.edu/software/lex-parser.shtml4http://nlp.stanford.edu/software/tagger.shtml5http://nltk.org/2.2 Features2.2.1 PunctuationTypically, punctuation may express user?s senti-ment to a certain extent.
For example, many excla-mation marks (!)
in tweet may indicate strong feel-ings or high volume (shouting).
Therefore, givena marked instance, we record the frequency of thefollowing four types of punctuation: (1) exclama-tion mark (!
), (2) question mark (?
), (3) double orsingle quotation marks( ?
and ??
), (4) sum of theabove three punctuation.
Then the punctuation fea-ture value is normalized by the length of instance.2.2.2 EmoticonsWe create two features that capture the number ofpositive and negative emoticons.
Table 1 lists thetwo types of emoticons.
We also use the union ofthe two emoticon sets as a feature.
In total, we havethree emoticon features.Positive Emoticons Negative Emoticons:-) : ) :D :-D =) ;) :( :-( : ( ;(;-) ; ) ;D ;-D (; :) ;-( ; ( )::-P ;-P XD (-: (-; :o) ;o) -/ :/ ;-/ ;/:0) ;0) ?
?
T T T0T ToTTable 1: List of emoticons2.2.3 POSAccording to the finding of (Pak and Paroubek,2010), POS taggers help to identify the sentimentof tweets.
Therefore, we record the frequency ofthe following four POS features, i.e., noun (?NN?,?NNP?, ?NNS?
and ?NNPS?
POS tags are groupedinto noun feature), verb (?VB?, ?VBD?, ?VBG?,?VBN?, ?VBP?
and ?VBZ?
POS tags are groupedinto verb feature), adjective (?JJ?, ?JJR?
and ?JJS?POS tags are grouped into adjective feature) andadverb (?RB?, ?RBR?
and ?RBS?
POS tags aregrouped into adverb feature).
Then we normalizethem by the length of given instance.2.2.4 Sentiment lexicon FeaturesFor each word in a given instance, we use threesentiment lexicons to identify its sentiment polarityand calculate its sentiment weight, i.e., SentiWord-Net (Baccianella et al 2010), MPQA SubjectivityLexicon (Wilson et al 2009) and an Unigram Lex-icon made from the Large Movie Review Dataset409v1.0 (Maas et al 2011).
To calculate the sentimentscore for this instance, we use the following formulato sum up the sentiment score of each word:Senti(I) =?w?INum(w) ?
Senti weightLength(I)(1)where I represents the given instance and w repre-sents each word in I .
The Senti weight is calcu-lated based on the word in the instance and the cho-sen sentiment lexicon.
That is, for each word in theinstance, we have different Senti weight values forit since we use different sentiment lexicons.
Belowwe describe the calculation of Senti weight valuesfor a word in three sentiment lexicons.
Note thatNum(w) is always 1 since most words appear onetime in a instance.SentiWordNet.
SentiWordNet is a lexical resourcefor sentiment analysis, which assigns each synset ofWordNet (Stark and Riesenfeld, 1998) three senti-ment scores: positivity, negativity, objectivity (e.g.living#a#3, positivity: 0.5, negativity: 0.125, ob-jectivity: 0.375), where sum of these three scoresis always 1.
For one concept, if its positive scoreand negative score are all 0, we treat it as objectiveconcept; otherwise, we treat it as subjective concept.And we take the first sense as the concept of eachword.We extract three features from SentiWordNet, i.e.,SUBWordNet, POSWordNet and NEGWordNet.The Senti weight of SUBWordNet recordswhether a word is subjective.
If it is subjective,we set Senti weight as 1, otherwise 0.
Similarly,the Senti weight values of POSWordNet andNEGWordNet indicate the positive score and thenegative score of the given word.
Consideringsome negation terms may reverse the sentimentorientation of instance, we manually generate anegation term list (e.g.
?not?, ?never?, etc.,) and if anegation term appears in the instance, we switch thePOSWordNet to NEGWordNet and vice versa.
Be-sides, we adopt another feature to record the ratio ofPOSWordNet/NEGWordNet.
If the denominator is0, i.e., NEGWordNet = 0, that means, the word hasthe strongest positive sentiment orientation, then weset 10*POSWordNet as its feature value.MPQA.
The MPQA Subjectivity Lexicon containsabout 8, 000 subjective words.
Each word in thelexicon has two types of sentiment strength: strongsubjective and weak subjective, and four kinds ofsentiment polarity: positive, negative, both (positiveand negative) and neutral.
Therefore we calculatethree features from this lexicon, i.e., SUBMPQA,POSMPQA and NEGMPQA.
For the SUBMPQAfeature, if the word has strong or weak subjective,we set its Senti weight as 1 or 0.5 accordingly.For the POSMPQA (NEGMPQA) feature, we setSenti weight as 1, or 0.5 or 0 if the word has strongpositive (negative), or weak positive (negative) orneutral.
We also reverse the sentiment orientationof POSMPQA and NEGMPQA if a negation termappears.Unigram Lexicon.
Unlike the above two lexiconsin themselves which provide sentiment polarity andsentiment strength for each word, we also utilize thethird lexicon to calculate the sentiment informationstatistically.
Therefore we generate an unigram lex-icon by ourselves from a large Movie Review dataset(Maas et al 2011) which contains 25, 000 posi-tive and 25, 000 negative movie reviews.
We calcu-late the Senti weight of each word appears in thedata set as the ratio of the frequency of this wordin positive reviews to that in negative reviews andrecord this feature as SentiUL.Clearly, since we use additional data set to de-velop a sentiment lexicon which is used to generatethis SentiUL feature, this feature is worked with allother features to train the unconstrained system.2.2.5 Other featuresIn addition, we collect three other features: (1)length of instance, (2) uppercase word (e.g.
?WTO?or ?Machine Learning?
), (3) URL.
For the uppercaseword and URL features, we record the frequency ofthem and then normalize them by the instance lengthas well.2.3 Experiment and Results2.3.1 Classification AlgorithmWe adopt LibSVM6 to build polynomial kernel-based SVM classifiers.
We have also tried linear ker-nel but get no improvement.
To obtain the optimalparameters for SVM, such as c and g, we performgrid search with 10-fold cross validation on training6http://www.csie.ntu.edu.tw/ cjlin/libsvm/410data.2.3.2 Results and DiscussionIn section 2, we obtained 22 features in total.
Totrain the constrained model, we used the above de-scribed 21 features (except SentiUL) and used allabove 22 features to train the unconstrained model.We combined the provided training and develop-ment data by the organizers as our final trainingdata.
And we should apologize for our misunder-standing of the definitions of the constrained andunconstrained condition.
As the official definitionof unconstrained model, participates are allowedto add other data to expand the training data sets,but our unconstrained model only adds one fea-ture (SentiUL) which is got from other data set.Therefore, we actually submitted two results of con-strained model.
But we still refer this model trainedon all features as unconstrained model for it ap-peared in the unconstrained list of official results.There are two kinds of test data: 4, 435 twitter in-stances and 2, 334 SMS message instances.
Table2 list the F-score and averaged F-score of positive,negative and neutral class of each test data set.On one hand, from the table we can see thatwhether on constrained or unconstrained model, theresults on twitter data are slightly better than thoseof SMS data.
However, this difference is not signifi-cant.
This indicates that the model trained on twitterdata performs well on SMS data.
And it also showsthat twitter data and SMS data are linguistically sim-ilar with each other in nature.
On the other hand, wefind that on each test data set, there is little differ-ence between the constrained model and the uncon-strained model, which indicates the SentiUL featuredoes not have discriminating power by itself.
How-ever, since we had not used other labeled or unla-beled data to extend the training data set, we cannotdraw a conclusion on this.
Besides, our results con-tain no neutral items even though the classifier weused is multivariate.
One reason may be the neutralinstances in training data is too sparse for the classi-fier to learn.On twitter data, our system ranks 2 under un-constrained model and ranks 10 under constrainedmodel.
On SMS data, our system ranks first underunconstrained model and ranks 7 under constrainedmodel.3 System Description of Message PolarityClassificationUnlike the previous subtask, the Message Polarityclassification task focuses on the whole tweet ratherthan a marked sequence of given instance.
Firstly,we perform text preprocessing as Task A. Besidesthe previous described features, we also extract fol-lowing features.3.1 Features3.1.1 Micro-blogging featuresWe adopted three tweet domain-specific features,i.e., #hashtags, @USERS, URLs.
We calculate thefrequency of the three features and normalize themby the length of instance.3.1.2 n-gram featuresWe used unigrams to capture the content oftweets.3.2 Classification AlgorithmWe adopted two different classifiers in preliminaryexperiments, i.e., maximum entropy and SVM.
Weused the Mallet tool (McCallum, 2002) to performMaximum Entropy classification and LibSVM7 witha linear kernel, where the default setting is adoptedin all experiments.3.3 Results on Training DataIn the first experiment, we used only content fea-tures and LibSVM classifier to do our experiments.The results were listed in Table 3.
From Table 3,we found that the system with unigram without re-moving stop words performs the best.
The possiblereason was that Microblogs are always short (con-strained in 140 words) and removing stop wordswould cause information missing in such a shorttext.
In addition, although bigrams improved theperformance to some extern, they added the featurespace many more and might affect other features.
Soin our final systems, we used only unigram featureand did not remove stop words.In the second experiment, we compared all fea-tures described before with two learning algorithms.The results were shown in Table 4, where 1 indi-cates unigram, 2 indicates micro-blog, 3 indicates7http://www.csie.ntu.edu.tw/ cjlin/libsvm/411System F-pos F-neg F-neu average F(pos and neg)twitter-constrained 0.8506 0.7390 0.0 0.7948twitter-unconstrained 0.8561 0.7468 0.0 0.8015SMS-constrained 0.7727 0.7611 0.0 0.7669SMS-unconstrained 0.7645 0.7824 0.0 0.7734Table 2: Results of our systems on subtask A test datafeatures F-pos F-neg F-neu average F(pos and neg) acc(%)unigrams 0.6356 0.3381 0.7122 0.4869 63.75unigrams(remove stop words) 0.6046 0.3453 0.6988 0.4750 62.13bigrams 0.5186 0.0196 0.6625 0.2691 55.85unigrams+bigrams 0.6234 0.3724 0.7043 0.4979 63.18Table 3: Results of our systems on on subtask B training data using content featuresfeatures F-pos F-neg F-neu average F(pos and neg) acc(%)MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM MaxEnt SVM1 0.6178 0.6356 0.3696 0.3381 0.6848 0.7122 0.4937 0.4869 61.56 63.751+2 0.6403 0.6339 0.4207 0.4310 0.6990 0.7184 0.5305 0.5324 63.75 64.891+2+3 0.6328 0.6512 0.4051 0.4371 0.6975 0.7232 0.5190 0.5442 63.18 65.751+2+3+4 0.6488 0.6593 0.4587 0.4481 0.7083 0.7288 0.5538 0.5537 64.89 66.412+3+4 0.5290 0.5201 0.2897 0.2643 0.6503 0.6411 0.4093 0.3922 55.85 54.80Table 4: Results of our systems on subtask B training data using all features and two learning algorithmspunctuation, 4 indicates sentiment lexicon features.From Table 4, the best performance was obtainedby using all these features.
Since the performanceof Maximum Entropy and SVM in terms of F-scorewas comparable to each other, we finally chose SVMsince it achieved a better accuracy than MaxEnt.3.4 Results on Test DataWe combined the provided training and develop-ment data by the organizers as our final training data.There were two kinds of test data: 3, 813 tweets and2, 094 SMS messages .
Table 5 listed the results ofour final systems on the tweet and SMS data sets byusing all above described features and SVM algo-rithm.From Table 5, on one hand, we can see that theoverall performance of SMS test data is inferior totwitter data, for the reason may be that the domainof features are all based on twitter data, and maybenot quite suitable for SMS data.
However, this dif-ferent is not significant.
On the other hand, we alsocan find that there is no obvious distinction betweenthe constrained and the unconstrained model on eachtest data.Also from Table 5, the F-score for positiveinstances is higher than negative instances, and itis interesting that most of other participants?systemsresults show the same consequence.
One of the rea-son may be the positive instance in training data aremore than negative instances both in training dataand test data.Our result on twitter message is 0.5842 , whileon SMS is 0.5477.
Compared with the highest av-erage F-score 0.6902 in twitter data and 0.6848 inSMS data, our system does not perform very well.On the one hand , pre-processing was roughly , thenfeatures extracted were not suited in classificationstage.
On the other hand, in classification stage allparameters were default when used LibSVM.
Thesemight cause low performance.
In future, we mayovercome the insufficient described above and takehashtags?
sentiment inclination and the source filesof URLs into consideration to enhance the perfor-mance.412System F-pos F-neg F-neu average F(pos and neg)twitter-constrained 0.6671 0.4338 0.7124 0.5505twitter-unconstrained 0.6775 0.4908 0.7204 0.5842SMS-constrained 0.5796 0.4846 0.7801 0.5321SMS-unconstrained 0.5818 0.5137 0.7612 0.5477Table 5: Results of our systems on subtask B test data4 ConclusionIn this work we extracted features from four aspects,including surface information of twitters and senti-ment lexicons like SentiWordNet and MPQA Lexi-con.
On the contextual polarity disambiguation task,our system ranks 2 on twitter (unconstrained) rankand ranks 1 on SMS (unconstrained) rank.AcknowledgementsThe authors would like to thank the organizers andreviewers for this interesting task and their helpfulsuggestions and comments, which improves the fi-nal version of this paper.
This research is supportedby grants from National Natural Science Foundationof China (No.60903093), Shanghai Pujiang TalentProgram (No.09PJ1404500), Doctoral Fund of Min-istry of Education of China (No.
20090076120029)and Shanghai Knowledge Service Platform Project(No.
ZF1213).ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Se-bastiani.
2010.
Sentiwordnet 3.0: An enhancedlexical resource for sentiment analysis and opinionmining.
In Nicoletta Calzolari (Conference Chair),Khalid Choukri, Bente Maegaard, Joseph Mariani,Jan Odijk, Stelios Piperidis, Mike Rosner, and DanielTapias, editors, Proceedings of the Seventh Interna-tional Conference on Language Resources and Evalu-ation (LREC?10), Valletta, Malta, may.
European Lan-guage Resources Association (ELRA).Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced sentiment learning using twitter hashtagsand smileys.
In Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters,pages 241?249.
Association for Computational Lin-guistics.Andrew L. Maas, Raymond E. Daly, Peter T. Pham, DanHuang, Andrew Y. Ng, and Christopher Potts.
2011.Learning word vectors for sentiment analysis.
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 142?150, Portland, Oregon, USA,June.
Association for Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Alexander Pak and Patrick Paroubek.
2010.
Twitter as acorpus for sentiment analysis and opinion mining.
InProceedings of LREC, volume 2010.Sara Prentice and Ethan Huffman.
2008.
Social mediasnew role in emergency management.
Idaho NationalLaboratory, pages 1?5.Michael M Stark and Richard F Riesenfeld.
1998.
Word-net: An electronic lexical database.
In Proceedings of11th Eurographics Workshop on Rendering.
Citeseer.Theresa Wilson, Janyce Wiebe, and Paul Hoffmann.2009.
Recognizing contextual polarity: An explo-ration of features for phrase-level sentiment analysis.Computational linguistics, 35(3):399?433.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.Ca?cilia Zirn, Mathias Niepert, Heiner Stuckenschmidt,and Michael Strube.
2011.
Fine-grained sentimentanalysis with structural features.
In Proceedings ofthe 5th international Joint conference on natural Lan-guage Processing (iJcnLP-2011), volume 167.413
