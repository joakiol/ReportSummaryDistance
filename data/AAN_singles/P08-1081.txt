Proceedings of ACL-08: HLT, pages 710?718,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUsing Conditional Random Fields to Extract Contexts and Answers ofQuestions from Online ForumsShilin Ding ?
?
Gao Cong?
?
Chin-Yew Lin?
Xiaoyan Zhu?
?Department of Computer Science and Technology, Tsinghua University, Beijing, China?Department of Computer Science, Aalborg University, Denmark?Microsoft Research Asia, Beijing, Chinadingsl@gmail.com gaocong@cs.aau.dkcyl@microsoft.com zxy-dcs@tsinghua.edu.cnAbstractOnline forum discussions often contain vastamounts of questions that are the focuses ofdiscussions.
Extracting contexts and answerstogether with the questions will yield not onlya coherent forum summary but also a valu-able QA knowledge base.
In this paper, wepropose a general framework based on Con-ditional Random Fields (CRFs) to detect thecontexts and answers of questions from forumthreads.
We improve the basic framework bySkip-chain CRFs and 2D CRFs to better ac-commodate the features of forums for betterperformance.
Experimental results show thatour techniques are very promising.1 IntroductionForums are web virtual spaces where people can askquestions, answer questions and participate in dis-cussions.
The availability of vast amounts of threaddiscussions in forums has promoted increasing in-terests in knowledge acquisition and summarizationfor forum threads.
Forum thread usually consistsof an initiating post and a number of reply posts.The initiating post usually contains several ques-tions and the reply posts usually contain answers tothe questions and perhaps new questions.
Forumparticipants are not physically co-present, and thusreply may not happen immediately after questionsare posted.
The asynchronous nature and multi-participants make multiple questions and answers?This work was done when Shilin Ding was a visiting stu-dent at the Microsoft Research Asia?This work was done when Gao Cong worked as a re-searcher at the Microsoft Research Asia.<context id=1>S1: Hi I am looking for a pet friendlyhotel in Hong Kong because all of my family is go-ing there for vacation.
S2: my family has 2 sonsand a dog.</context> <question id=1>S3: Is thereany recommended hotel near Sheung Wan or TsingSha Tsui?</question><context id=2,3>S4: We alsoplan to go shopping in Causeway Bay.</context><question id=2>S5: What?s the traffic situa-tion around those commercial areas?</question><question id=3>S6: Is it necessary to take ataxi?</question>.
S7: Any information would be ap-preciated.<answer qid=1>S8: The Comfort Lodge nearKowloon Park allows pet as I know, and usually fitswell within normal budget.
S9: It is also conve-niently located, nearby the Kowloon railway stationand subway.</answer><answer qid=2,3> S10: It?s very crowd in those ar-eas, so I recommend MTR in Causeway Bay becauseit is cheap to take you around </answer>Figure 1: An example thread with question-context-answer annotatedinterweaved together, which makes it more difficultto summarize.In this paper, we address the problem of detectingthe contexts and answers from forum threads for thequestions identified in the same threads.
Figure 1gives an example of a forum thread with questions,contexts and answers annotated.
It contains threequestion sentences, S3, S5 and S6.
Sentences S1and S2 are contexts of question 1 (S3).
Sentence S4is the context of questions 2 and 3, but not 1.
Sen-tence S8 is the answer to question 3.
(S4-S5-S10) isone example of question-context-answer triple thatwe want to detect in the thread.
As shown in the ex-ample, a forum question usually requires contextualinformation to provide background or constraints.710Moreover, it sometimes needs contextual informa-tion to provide explicit link to its answers.
Forexample, S8 is an answer of question 1, but theycannot be linked with any common word.
Instead,S8 shares word pet with S1, which is a context ofquestion 1, and thus S8 could be linked with ques-tion 1 through S1.
We call contextual informationthe context of a question in this paper.A summary of forum threads in the form ofquestion-context-answer can not only highlight themain content, but also provide a user-friendly orga-nization of threads, which will make the access toforum information easier.Another motivation of detecting contexts and an-swers of the questions in forum threads is that itcould be used to enrich the knowledge base ofcommunity-based question and answering (CQA)services such as Live QnA and Yahoo!
Answers,where context is comparable with the question de-scription while question corresponds to the questiontitle.
For example, there were about 700,000 ques-tions in the Yahoo!
Answers travel category as ofJanuary 2008.
We extracted about 3,000,000 travelrelated questions from six online travel forums.
Onewould expect that a CQA service with large QA datawill attract more users to the service.
To enrich theknowledge base, not only the answers, but also thecontexts are critical; otherwise the answer to a ques-tion such as How much is the taxi would be uselesswithout context in the database.However, it is challenging to detecting contextsand answers for questions in forum threads.
We as-sume the questions have been identified in a forumthread using the approach in (Cong et al, 2008).Although identifying questions in a forum thread isalso nontrivial, it is beyond the focus of this paper.First, detecting contexts of a question is importantand non-trivial.
We found that 74% of questions inour corpus, which contain 1,064 questions from 579forum threads about travel, need contexts.
However,relative position information is far from adequate tosolve the problem.
For example, in our corpus 63%of sentences preceding questions are contexts andthey only represent 34% of all correct contexts.
Toeffectively detect contexts, the dependency betweensentences is important.
For example in Figure 1,both S1 and S2 are contexts of question 1.
S1 couldbe labeled as context based on word similarity, but itis not easy to link S2 with the question directly.
S1and S2 are linked by the common word family, andthus S2 can be linked with question 1 through S1.The challenge here is how to model and utilize thedependency for context detection.Second, it is difficult to link answers with ques-tions.
In forums, multiple questions and answerscan be discussed in parallel and are interweaved to-gether while the reply relationship between posts isusually unavailable.
To detect answers, we need tohandle two kinds of dependencies.
One is the depen-dency relationship between contexts and answers,which should be leveraged especially when ques-tions alone do not provide sufficient information tofind answers; the other is the dependency betweenanswer candidates (similar to sentence dependencydescribed above).
The challenge is how to modeland utilize these two kinds of dependencies.In this paper we propose a novel approach for de-tecting contexts and answers of the questions in fo-rum threads.
To our knowledge this is the first workon this.We make the following contributions:First, we employ Linear Conditional RandomFields (CRFs) to identify contexts and answers,which can capture the relationships between con-tiguous sentences.Second, we also found that context is very im-portant for answer detection.
To capture the depen-dency between contexts and answers, we introduceSkip-chain CRF model for answer detection.
Wealso extend the basic model to 2D CRFs to modeldependency between contiguous questions in a fo-rum thread for context and answer identification.Finally, we conducted experiments on forum data.Experimental results show that 1) Linear CRFs out-perform SVM and decision tree in both contextand answer detection; 2) Skip-chain CRFs outper-form Linear CRFs for answer finding, which demon-strates that context improves answer finding; 3)2D CRF model improves the performance of LinearCRFs and the combination of 2D CRFs and Skip-chain CRFs achieves better performance for contextdetection.The rest of this paper is organized as follows:The next section discusses related work.
Section 3presents the proposed techniques.
We evaluate ourtechniques in Section 4.
Section 5 concludes thispaper and discusses future work.7112 Related WorkThere is some research on summarizing discussionthreads and emails.
Zhou and Hovy (2005) seg-mented internet relay chat, clustered segments intosubtopics, and identified responding segments ofthe first segment in each sub-topic by assumingthe first segment to be focus.
In (Nenkova andBagga, 2003; Wan and McKeown, 2004; Rambowet al, 2004), email summaries were organized byextracting overview sentences as discussion issues.Carenini et al(2007) leveraged both quotation re-lation and clue words for email summarization.
Incontrast, given a forum thread, we extract questions,their contexts, and their answers as summaries.Shrestha and McKeown (2004)?s work on emailsummarization is closer to our work.
They usedRIPPER as a classifier to detect interrogative ques-tions and their answers and used the resulting ques-tion and answer pairs as summaries.
However, it didnot consider contexts of questions and dependencybetween answer sentences.We also note the existing work on extractingknowledge from discussion threads.
Huang etal.
(2007) used SVM to extract input-reply pairs fromforums for chatbot knowledge.
Feng et al (2006a)used cosine similarity to match students?
query withreply posts for discussion-bot.
Feng et al (2006b)identified the most important message in onlineclassroom discussion board.
Our problem is quitedifferent from the above work.Detecting context for question in forums is relatedto the context detection problem raised in the QAroadmap paper commissioned by ARDA (Burger etal., 2006).
To our knowledge, none of the previouswork addresses the problem of context detection.The method of finding follow-up questions (Yanget al, 2006) from TREC context track could beadapted for context detection.
However, the follow-up relationship is limited between questions whilecontext is not.
In our other work (Cong et al, 2008),we proposed a supervised approach for question de-tection and an unsupervised approach for answer de-tection without considering context detection.Extensive research has been done in question-answering, e.g.
(Berger et al, 2000; Jeon et al,2005; Cui et al, 2005; Harabagiu and Hickl, 2006;Dang et al, 2007).
They mainly focus on con-structing answer for certain types of question from alarge document collection, and usually apply sophis-ticated linguistic analysis to both questions and thedocuments in the collection.
Soricut and Brill (2006)used statistical translation model to find the appro-priate answers from their QA pair collections fromFAQ pages for the posted question.
In our scenario,we not only need to find answers for various typesof questions in forum threads but also their contexts.3 Context and Answer DetectionA question is a linguistic expression used by a ques-tioner to request information in the form of an an-swer.
The sentence containing request focus iscalled question.
Context are the sentences contain-ing constraints or background information to thequestion, while answer are that provide solutions.
Inthis paper, we use sentences as the detection segmentthough it is applicable to other kinds of segments.Given a thread and a set of m detected questions{Qi}mi=1, our task is to find the contexts and an-swers for each question.
We first discuss using Lin-ear CRFs for context and answer detection, and thenextend the basic framework to Skip-chain CRFs and2D CRFs to better model our problem.
Finally, wewill briefly introduce CRF models and the featuresthat we used for CRF model.3.1 Using Linear CRFsFor ease of presentation, we focus on detecting con-texts using Linear CRFs.
The model could be easilyextended to answer detection.Context detection.
As discussed in Introductionthat context detection cannot be trivially solved byposition information (See Section 4.2 for details),and dependency between sentences is important forcontext detection.
Recall that in Figure 1, S2 couldbe labeled as context of Q1 if we consider the de-pendency between S2 and S1, and that between S1and Q1, while it is difficult to establish connectionbetween S2 and Q1 without S1.
Table 1 shows thatthe correlation between the labels of contiguous sen-tences is significant.
In other words, when a sen-tence Yt?s previous Yt?1 is not a context (Yt?1 6= C)then it is very likely that Yt (i.e.
Yt 6= C) is also not acontext.
It is clear that the candidate contexts are notindependent and there are strong dependency rela-712Contiguous sentences yt = C yt 6= Cyt?1 = C 901 1,081yt?1 6= C 1,081 47,190Table 1: Contingency table(?2 = 9,386,p-value<0.001)tionships between contiguous sentences in a thread.Therefore, a desirable model should be able to cap-ture the dependency.The context detection can be modeled as a clas-sification problem.
Traditional classification tools,e.g.
SVM, can be employed, where each pair ofquestion and candidate context will be treated as aninstance.
However, they cannot capture the depen-dency relationship between sentences.To this end, we proposed a general framework todetect contexts and answers based on ConditionalRandom Fields (Lafferty et al, 2001) (CRFs) whichare able to model the sequential dependencies be-tween contiguous nodes.
A CRF is an undirectedgraphical model G of the conditional distributionP (Y|X).
Y are the random variables over the la-bels of the nodes that are globally conditioned on X,which are the random variables of the observations.
(See Section 3.4 for more about CRFs)Linear CRF model has been successfully appliedin NLP and text mining tasks (McCallum and Li,2003; Sha and Pereira, 2003).
However, our prob-lem cannot be modeled with Linear CRFs in thesame way as other NLP tasks, where one node has aunique label.
In our problem, each node (sentence)might have multiple labels since one sentence couldbe the context of multiple questions in a thread.Thus, it is difficult to find a solution to tag contextsentences for all questions in a thread in single pass.Here we assume that questions in a given threadare independent and are found, and then we canlabel a thread with m questions one-by-one in m-passes.
In each pass, one question Qi is selectedas focus and each other sentence in the thread willbe labeled as context C of Qi or not using LinearCRF model.
The graphical representations of Lin-ear CRFs is shown in Figure2(a).
The linear-chainedges can capture the dependency between two con-tiguous nodes.
The observation sequence x = <x1,x2,...,xt>, where t is the number of sentences in athread, represents predictors (to be described in Sec-tion 3.5), and the tag sequence y=<y1,...,yt>, whereyi ?
{C,P}, determines whether a sentence is plaintext P or context C of question Qi.Answer detection.
Answers usually appear in theposts after the post containing the question.
Thereare also strong dependencies between contiguousanswer segments.
Thus, position and similarity in-formation alone are not adequate here.
To copewith the dependency between contiguous answersegments, Linear CRFs model are employed as incontext detection.3.2 Leveraging Context for Answer DetectionUsing Skip-chain CRFsWe observed in our corpus 74% questions lack con-straints or background information which are veryuseful to link question and answers as discussed inIntroduction.
Therefore, contexts should be lever-aged to detect answers.
The Linear CRF model cancapture the dependency between contiguous sen-tences.
However, it cannot capture the long distancedependency between contexts and answers.One straightforward method of leveraging contextis to detect contexts and answers in two phases, i.e.to first identify contexts, and then label answers us-ing both the context and question information (e.g.the similarity between context and answer can beused as features in CRFs).
The two-phase proce-dure, however, still cannot capture the non-local de-pendency between contexts and answers in a thread.To model the long distance dependency betweencontexts and answers, we will use Skip-chain CRFmodel to detect context and answer together.
Skip-chain CRF model is applied for entity extractionand meeting summarization (Sutton and McCallum,2006; Galley, 2006).
The graphical representationof a Skip-chain CRF given in Figure2(b) consistsof two types of edges: linear-chain (yt?1 to yt) andskip-chain edges (yi to yj).Ideally, the skip-chain edges will establish theconnection between candidate pairs with high prob-ability of being context and answer of a question.To introduce skip-chain edges between any pairs ofnon-contiguous sentences will be computationallyexpensive, and also introduce noise.
To make thecardinality and number of cliques in the graph man-ageable and also eliminate noisy edges, we wouldlike to generate edges only for sentence pairs withhigh possibility of being context and answer.
This is713(a) Linear CRFs (b) Skip-chain CRFs (c) 2D CRFsFigure 2: CRF ModelsSkip-Chain yv = A yv 6= Ayu = C 4,105 5,314yu 6= C 3,744 9,740Table 2: Contingence table(?2=615.8,p-value < 0.001)achieved as follows.
Given a question Qi in post Pjof a thread with n posts, its contexts usually occurwithin post Pj or before Pj while answers appear inthe posts after Pj .
We will establish an edge betweeneach candidate answer v and one condidate contextin {Pk}jk=1 such that they have the highest possibil-ity of being a context-answer pair of question Qi:u = argmaxu?
{Pk}jk=1sim(xu, Qi).sim(xv, {xu, Qi})here, we use the product of sim(xu, Qi) andsim(xv, {xu, Qi} to estimate the possibility of be-ing a context-answer pair for (u, v) , where sim(?, ?
)is the semantic similarity calculated on WordNet asdescribed in Section 3.5.
Table 2 shows that yu andyv in the skip chain generated by our heuristics in-fluence each other significantly.Skip-chain CRFs improve the performance ofanswer detection due to the introduced skip-chainedges that represent the joint probability conditionedon the question, which is exploited by skip-chainfeature function: f(yu, yv, Qi,x).3.3 Using 2D CRF ModelBoth Linear CRFs and Skip-chain CRFs label thecontexts and answers for each question in separatepasses by assuming that questions in a thread are in-dependent.
Actually the assumption does not holdin many cases.
Let us look at an example.
As in Fig-ure 1, sentence S10 is an answer for both questionQ2 and Q3.
S10 could be recognized as the answerof Q2 due to the shared word areas and Causewaybay (in Q2?s context, S4), but there is no direct re-lation between Q3 and S10.
To label S10, we needconsider the dependency relation between Q2 andQ3.
In other words, the question-answer relation be-tween Q3 and S10 can be captured by a joint mod-eling of the dependency among S10, Q2 and Q3.The labels of the same sentence for two contigu-ous questions in a thread would be conditioned onthe dependency relationship between the questions.Such a dependency cannot be captured by both Lin-ear CRFs and Skip-chain CRFs.To capture the dependency between the contigu-ous questions, we employ 2D CRFs to help contextand answer detection.
2D CRF model is used in(Zhu et al, 2005) to model the neighborhood de-pendency in blocks within a web page.
As shownin Figure2(c), 2D CRF models the labeling task forall questions in a thread.
For each thread, there arem rows in the grid, where the ith row correspondsto one pass of Linear CRF model (or Skip-chainmodel) which labels contexts and answers for ques-tion Qi.
The vertical edges in the figure representthe joint probability conditioned on the contiguousquestions, which will be exploited by 2D featurefunction: f(yi,j , yi+1,j , Qi, Qi+1,x).
Thus, the in-formation generated in single CRF chain could bepropagated over the whole grid.
In this way, contextand answer detection for all questions in the threadcould be modeled together.3.4 Conditional Random Fields (CRFs)The Linear, Skip-Chain and 2D CRFs can be gen-eralized as pairwise CRFs, which have two kinds ofcliques in graph G: 1) node yt and 2) edge (yu, yv).The joint probability is defined as:p(y|x)= 1Z(x) exp{?k,t?kfk(yt,x)+?k,t?kgk(yu, yv,x)}714where Z(x) is the normalization factor, fk is thefeature on nodes, gk is on edges between u and v,and ?k and ?k are parameters.Linear CRFs are based on the first order Markovassumption that the contiguous nodes are dependent.The pairwise edges in Skip-chain CRFs representthe long distance dependency between the skippednodes, while the ones in 2D CRFs represent the de-pendency between the neighboring nodes.Inference and Parameter Estimation.
For LinearCRFs, dynamic programming is used to compute themaximum a posteriori (MAP) of y given x. How-ever, for more complicated graphs with cycles, ex-act inference needs the junction tree representationof the original graph and the algorithm is exponen-tial to the treewidth.
For fast inference, loopy BeliefPropagation (Pearl, 1988) is implemented.Given the training Data D = {x(i),y(i)}ni=1, theparameter estimation is to determine the parame-ters based on maximizing the log-likelihood L?
=?ni=1 log p(y(i)|x(i)).
In Linear CRF model, dy-namic programming and L-BFGS (limited memoryBroyden-Fletcher-Goldfarb-Shanno) can be used tooptimize objective function L?, while for compli-cated CRFs, Loopy BP are used instead to calculatethe marginal probability.3.5 Features used in CRF modelsThe main features used in Linear CRF models forcontext detection are listed in Table 3.The similarity feature is to capture the word sim-ilarity and semantic similarity between candidatecontexts and answers.
The word similarity is basedon cosine similarity of TF/IDF weighted vectors.The semantic similarity between words is computedbased on Wu and Palmer?s measure (Wu and Palmer,1994) using WordNet (Fellbaum, 1998).1 The simi-larity between contiguous sentences will be used tocapture the dependency for CRFs.
In addition, tobridge the lexical gaps between question and con-text, we learned top-3 context terms for each ques-tion term from 300,000 question-description pairsobtained from Yahoo!
Answers using mutual infor-mation (Berger et al, 2000) ( question descriptionin Yahoo!
Answers is comparable to contexts in fo-1The semantic similarity between sentences is calculated asin (Yang et al, 2006).Similarity features:?
Cosine similarity with the question?
Similarity with the question using WordNet?
Cosine similarity between contiguous sentences?
Similarity between contiguous sentences using WordNet?
Cosine similarity with the expanded question using the lexicalmatching wordsStructural features:?
The relative position to current question?
Is its author the same with that of the question??
Is it in the same paragraph with its previous sentence?Discourse and lexical features:?
The number of Pronouns in the question?
The presence of fillers, fluency devices (e.g.
?uh?, ?ok?)?
The presence of acknowledgment tokens?
The number of non-stopwords?
Whether the question has a noun or not??
Whether the question has a verb or not?Table 3: Features for Linear CRFs.
Unless otherwisementioned, we refer to features of the sentence whose la-bel to be predictedrums), and then use them to expand question andcompute cosine similarity.The structural features of forums provide strongclues for contexts.
For example, contexts of a ques-tion usually occur in the post containing the questionor preceding posts.We extracted the discourse features from a ques-tion, such as the number of pronouns in the question.A more useful feature would be to find the entity insurrounding sentences referred by a pronoun.
Wetried GATE (Cunningham et al, 2002) for anaphoraresolution of the pronouns in questions, but the per-formance became worse with the feature, which isprobably due to the difficulty of anaphora resolutionin forum discourse.
We also observed that questionsoften need context if the question do not contain anoun or a verb.In addition, we use similarity features betweenskip-chain sentences for Skip-chain CRFs and simi-larity features between questions for 2D CRFs.4 Experiments4.1 Experimental setupCorpus.
We obtained about 1 million threadsfrom TripAdvisor forum; we randomly selected 591threads and removed 22 threads which has more than40 sentences and 6 questions; the remaining 579 fo-rum threads form our corpus 2.
Each thread in our2TripAdvisor (http://www.tripadvisor.com/ForumHome) isone of the most popular travel forums; the list of 579 urls is715Model Prec(%) Rec(%) F1(%)Context DetectionSVM 75.27 68.80 71.32C4.5 70.16 64.30 67.21L-CRF 75.75 72.84 74.45Answer DetectionSVM 73.31 47.35 57.52C4.5 65.36 46.55 54.37L-CRF 63.92 58.74 61.22Table 4: Context and Answer Detectioncorpus contains at least two posts and on averageeach thread consists of 3.87 posts.
Two annotatorswere asked to tag questions, their contexts, and an-swers in each thread.
The kappa statistic for identi-fying question is 0.96, for linking context and ques-tion given a question is 0.75, and for linking answerand question given a question is 0.69.
We conductedexperiments on both the union and intersection ofthe two annotated data.
The experimental results onboth data are qualitatively comparable.
We only re-port results on union data due to space limitation.The union data contains 1,064 questions, 1,458 con-texts and 3,534 answers.Metrics.
We calculated precision, recall,and F1-score for all tasks.
All the experimentalresults are obtained through the average of 5 trialsof 5-fold cross validation.4.2 Experimental resultsLinear CRFs for Context and Answer Detection.This experiment is to evaluate Linear CRF model(Section 3.1) for context and answer detection bycomparing with SVM and C4.5(Quinlan, 1993).
ForSVM, we use SVMlight(Joachims, 1999).
We triedlinear, polynomial and RBF kernels and report theresults on polynomial kernel using default param-eters since it performs the best in the experiment.SVM and C4.5 use the same set of features as Lin-ear CRFs.
As shown in Table 4, Linear CRF modeloutperforms SVM and C4.5 for both context and an-swer detection.
The main reason for the improve-ment is that CRF models can capture the sequen-tial dependency between segments in forums as dis-cussed in Section 3.1.given in http://homepages.inf.ed.ac.uk/gcong/acl08/; Removingthe 22 long threads can greatly reduce the training and test time.position Prec(%) Rec(%) F1(%)Context DetectionPrevious One 63.69 34.29 44.58Previous All 43.48 76.41 55.42Anwer DetectionFollowing One 66.48 19.98 30.72Following All 31.99 100 48.48Table 5: Using position information for detectionContext Prec(%) Rec(%) F1(%)No context 63.92 58.74 61.22Prev.
sentence 61.41 62.50 61.84Real context 63.54 66.40 64.94L-CRF+context 65.51 63.13 64.06Table 6: Contextual Information for Answer Detection.Prev.
sentence uses one previous sentence of the currentquestion as context.
RealContext uses the context anno-tated by experts.
L-CRF+context uses the context foundby Linear CRFsWe next report a baseline of context detectionusing previous sentences in the same post with itsquestion since contexts often occur in the questionpost or preceding posts.
Similarly, we report a base-line of answer detecting using following segments ofa question as answers.
The results given in Table 5show that location information is far from adequateto detect contexts and answers.The usefulness of contexts.
This experiment is toevaluate the usefulness of contexts in answer de-tection, by adding the similarity between the con-text (obtained with different methods) and candi-date answer as an extra feature for CRFs.
Table 6shows the impact of context on answer detectionusing Linear CRFs.
Linear CRFs with contextualinformation perform better than those without con-text.
L-CRF+context is close to that using real con-text, while it is better than CRFs using the previoussentence as context.
The results clearly shows thatcontextual information greatly improves the perfor-mance of answer detection.Improved Models.
This experiment is to evaluatethe effectiveness of Skip-Chain CRFs (Section 3.2)and 2D CRFs (Section 3.3) for our tasks.
The resultsare given in Table 7 and Table 8.In context detection, Skip-Chain CRFs have simi-716Model Prec(%) Rec(%) F1(%)L-CRF+Context 75.75 72.84 74.45Skip-chain 74.18 74.90 74.422D 75.92 76.54 76.412D+Skip-chain 76.27 78.25 77.34Table 7: Skip-chain and 2D CRFs for context detectionlar results as Linear CRFs, i.e.
the inter-dependencycaptured by the skip chains generated using theheuristics in Section 3.2 does not improve the con-text detection.
The performance of Linear CRFs isimproved in 2D CRFs (by 2%) and 2D+Skip-chainCRFs (by 3%) since they capture the dependency be-tween contiguous questions.In answer detection, as expected, Skip-chainCRFs outperform L-CRF+context since Skip-chainCRFs can model the inter-dependency between con-texts and answers while in L-CRF+context the con-text can only be reflected by the features on the ob-servations.
We also observed that 2D CRFs improvethe performance of L-CRF+context due to the de-pendency between contiguous questions.
In contrastwith our expectation, the 2D+Skip-chain CRFs doesnot improve Skip-chain CRFs in terms of answer de-tection.
The possible reason could be that the struc-ture of the graph is very complicated and too manyparameters need to be learned on our training data.Evaluating Features.
We also evaluated the con-tributions of each category of features in Table 3to context detection.
We found that similarity fea-tures are the most important and structural featurethe next.
We also observed the same trend for an-swer detection.
We omit the details here due to spacelimitation.As a summary, 1) our CRF model outperformsSVM and C4.5 for both context and answer detec-tions; 2) context is very useful in answer detection;3) the Skip-chain CRF method is effective in lever-aging context for answer detection; and 4) 2D CRFmodel improves the performance of Linear CRFs forboth context and answer detection.5 Discussions and ConclusionsWe presented a new approach to detecting contextsand answers for questions in forums with good per-formance.
We next discuss our experience not cov-ered by the experiments, and future work.Model Prec(%) Rec(%) F1(%)L-CRF+context 65.51 63.13 64.06Skip-chain 67.59 71.06 69.402D 65.77 68.17 67.342D+Skip-chain 66.90 70.56 68.89Table 8: Skip-chain and 2D CRFs for answer detectionSince contexts of questions are largely unexploredin previous work, we analyze the contexts in ourcorpus and classify them into three categories: 1)context contains the main content of question whilequestion contains no constraint, e.g.
?i will visit NY atOct, looking for a cheap hotel but convenient.
Any goodsuggestion?
?
; 2) contexts explain or clarify part ofthe question, such as a definite noun phrase, e.g.
?Weare going on the Taste of Paris.
Does anyone know if it isadvisable to take a suitcase with us on the tour., wherethe first sentence is to describe the tour; and 3) con-texts provide constraint or background for questionthat is syntactically complete, e.g.
?We are inter-ested in visiting the Great Wall(and flying from London).Can anyone recommend a tour operator.?
In our corpus,about 26% questions do not need context, 12% ques-tions need Type 1 context, 32% need Type 2 contextand 30% Type 3.
We found that our techniques oftendo not perform well on Type 3 questions.We observed that factoid questions, one of fo-cuses in the TREC QA community, take less than10% question in our corpus.
It would be interestingto revisit QA techniques to process forum data.Other future work includes: 1) to summarize mul-tiple threads using the triples extracted from indi-vidual threads.
This could be done by clusteringquestion-context-answer triples; 2) to use the tradi-tional text summarization techniques to summarizethe multiple answer segments; 3) to integrate theQuestion Answering techniques as features of ourframework to further improve answer finding; 4) toreformulate questions using its context to generatemore user-friendly questions for CQA services; and5) to evaluate our techniques on more online forumsin various domains.AcknowledgmentsWe thank the anonymous reviewers for their detailedcomments, and Ming Zhou and Young-In Song fortheir valuable suggestions in preparing the paper.717ReferencesA.
Berger, R. Caruana, D. Cohn, D. Freitag, and V. Mit-tal.
2000.
Bridging the lexical chasm: statistical ap-proaches to answer-finding.
In Proceedings of SIGIR.J.
Burger, C. Cardie, V. Chaudhri, R. Gaizauskas,S.
Harabagiu, D. Israel, C. Jacquemin, C. Lin,S.
Maiorano, G. Miller, D. Moldovan, B. Ogden,J.
Prager, E. Riloff, A. Singhal, R. Shrihari, T. Strza-lkowski16, E. Voorhees, and R. Weishedel.
2006.
Is-sues, tasks and program structures to roadmap researchin question and answering (qna).
ARAD: AdvancedResearch and Development Activity (US).G.
Carenini, R. Ng, and X. Zhou.
2007.
Summarizingemail conversations with clue words.
In Proceedingsof WWW.G.
Cong, L. Wang, C.Y.
Lin, Y.I.
Song, and Y.
Sun.
2008.Finding question-answer pairs from online forums.
InProceedings of SIGIR.H.
Cui, R. Sun, K. Li, M. Kan, and T. Chua.
2005.
Ques-tion answering passage retrieval using dependency re-lations.
In Proceedings of SIGIR.H.
Cunningham, D. Maynard, K. Bontcheva, andV.
Tablan.
2002.
Gate: A framework and graphicaldevelopment environment for robust nlp tools and ap-plications.
In Proceedings of ACL.H.
Dang, J. Lin, and D. Kelly.
2007.
Overview of thetrec 2007 question answering track.
In Proceedings ofTREC.C.
Fellbaum, editor.
1998.
WordNet: An Electronic Lex-ical Database (Language, Speech, and Communica-tion).
The MIT Press, May.D.
Feng, E. Shaw, J. Kim, and E. Hovy.
2006a.
An intel-ligent discussion-bot for answering student queries inthreaded discussions.
In Proceedings of IUI.D.
Feng, E. Shaw, J. Kim, and E. Hovy.
2006b.
Learningto detect conversation focus of threaded discussions.In Proceedings of HLT-NAACL.M.
Galley.
2006.
A skip-chain conditional random fieldfor ranking meeting utterances by importance.
In Pro-ceedings of EMNLP.S.
Harabagiu and A. Hickl.
2006.
Methods for using tex-tual entailment in open-domain question answering.In Proceedings of ACL.J.
Huang, M. Zhou, and D. Yang.
2007.
Extracting chat-bot knowledge from online discussion forums.
In Pro-ceedings of IJCAI.J.
Jeon, W. Croft, and J. Lee.
2005.
Finding similarquestions in large question and answer archives.
InProceedings of CIKM.T.
Joachims.
1999.
Making large-scale support vectormachine learning practical.
MIT Press, Cambridge,MA, USA.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proceedingsof ICML.A.
McCallum and W. Li.
2003.
Early results for namedentity recognition with conditional random fields, fea-ture induction and web-enhanced lexicons.
In Pro-ceedings of CoNLL-2003.A.
Nenkova and A. Bagga.
2003.
Facilitating emailthread access by extractive summary generation.
InProceedings of RANLP.J.
Pearl.
1988.
Probabilistic reasoning in intelligent sys-tems: networks of plausible inference.
Morgan Kauf-mann Publishers Inc., San Francisco, CA, USA.J.
Quinlan.
1993.
C4.5: programs for machine learn-ing.
Morgan Kaufmann Publishers Inc., San Fran-cisco, CA, USA.O.
Rambow, L. Shrestha, J. Chen, and C. Lauridsen.2004.
Summarizing email threads.
In Proceedings ofHLT-NAACL.F.
Sha and F. Pereira.
2003.
Shallow parsing with condi-tional random fields.
In HLT-NAACL.L.
Shrestha and K. McKeown.
2004.
Detection ofquestion-answer pairs in email conversations.
In Pro-ceedings of COLING.R.
Soricut and E. Brill.
2006.
Automatic question an-swering using the web: Beyond the Factoid.
Informa-tion Retrieval, 9(2):191?206.C.
Sutton and A. McCallum.
2006.
An introduction toconditional random fields for relational learning.
InLise Getoor and Ben Taskar, editors, Introduction toStatistical Relational Learning.
MIT Press.
To appear.S.
Wan and K. McKeown.
2004.
Generating overviewsummaries of ongoing email thread discussions.
InProceedings of COLING.Z.
Wu and M. S. Palmer.
1994.
Verb semantics and lexi-cal selection.
In Proceedings of ACL.F.
Yang, J. Feng, and G. Fabbrizio.
2006.
A datadriven approach to relevancy recognition for contex-tual question answering.
In Proceedings of the Inter-active Question Answering Workshop at HLT-NAACL2006.L.
Zhou and E. Hovy.
2005.
Digesting virtual ?geek?culture: The summarization of technical internet relaychats.
In Proceedings of ACL.J.
Zhu, Z. Nie, J. Wen, B. Zhang, and W. Ma.
2005.
2dconditional random fields for web information extrac-tion.
In Proceedings of ICML.718
