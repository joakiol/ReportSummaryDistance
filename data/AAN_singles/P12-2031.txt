Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 156?160,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsCrowdsourcing Inference-Rule EvaluationNaomi ZeichnerBar-Ilan UniversityRamat-Gan, Israelzeichner.naomi@gmail.comJonathan BerantTel-Aviv UniversityTel-Aviv, Israeljonatha6@post.tau.ac.ilIdo DaganBar-Ilan UniversityRamat-Gan, Israeldagan@cs.biu.ac.ilAbstractThe importance of inference rules to semanticapplications has long been recognized and ex-tensive work has been carried out to automat-ically acquire inference-rule resources.
How-ever, evaluating such resources has turned outto be a non-trivial task, slowing progress in thefield.
In this paper, we suggest a frameworkfor evaluating inference-rule resources.
Ourframework simplifies a previously proposed?instance-based evaluation?
method that in-volved substantial annotator training, makingit suitable for crowdsourcing.
We show thatour method produces a large amount of an-notations with high inter-annotator agreementfor a low cost at a short period of time, withoutrequiring training expert annotators.1 IntroductionInference rules are an important component in se-mantic applications, such as Question Answering(QA) (Ravichandran and Hovy, 2002) and Informa-tion Extraction (IE) (Shinyama and Sekine, 2006),describing a directional inference relation betweentwo text patterns with variables.
For example, to an-swer the question ?Where was Reagan raised??
aQA system can use the rule ?X brought up in Y?Xraised in Y?
to extract the answer from ?Reagan wasbrought up in Dixon?.
Similarly, an IE system canuse the rule ?X work as Y?X hired as Y?
to ex-tract the PERSON and ROLE entities in the ?hiring?event from ?Bob worked as an analyst for Dell?.The significance of inference rules has led to sub-stantial effort into developing algorithms that au-tomatically learn inference rules (Lin and Pantel,2001; Sekine, 2005; Schoenmackers et al, 2010),and generate knowledge resources for inference sys-tems.
However, despite their potential, utilization ofinference rule resources is currently somewhat lim-ited.
This is largely due to the fact that these al-gorithms often produce invalid rules.
Thus, evalu-ation is necessary both for resource developers aswell as for inference system developers who want toasses the quality of each resource.
Unfortunately, asevaluating inference rules is hard and costly, there isno clear evaluation standard, and this has become aslowing factor for progress in the field.One option for evaluating inference rule resourcesis to measure their impact on an end task, as that iswhat ultimately interests an inference system devel-oper.
However, this is often problematic since infer-ence systems have many components that addressmultiple phenomena, and thus it is hard to assess theeffect of a single resource.
An example is the Recog-nizing Textual Entailment (RTE) framework (Daganet al, 2009), in which given a text T and a textualhypothesis H, a system determines whether H canbe inferred from T. This type of evaluation was es-tablished in RTE challenges by ablation tests (seeRTE ablation tests in ACLWiki) and showed that re-sources?
impact can vary considerably from one sys-tem to another.
These issues have also been notedby Sammons et al (2010) and LoBue and Yates(2011).
A complementary application-independentevaluation method is hence necessary.Some attempts were made to let annotators judgerule correctness directly, that is by asking them tojudge the correctness of a given rule (Shinyama etal., 2002; Sekine, 2005).
However, Szpektor et al(2007) observed that directly judging rules out ofcontext often results in low inter-annotator agree-ment.
To remedy that, Szpektor et al (2007) and156Bhagat et al (2007) proposed ?instance-based eval-uation?, in which annotators are presented with anapplication of a rule in a particular context andneed to judge whether it results in a valid inference.This simulates the utility of rules in an applicationand yields high inter-annotator agreement.
Unfortu-nately, their method requires lengthy guidelines andsubstantial annotator training effort, which are timeconsuming and costly.
Thus, a simple, robust andreplicable evaluation method is needed.Recently, crowdsourcing services such as Ama-zon Mechanical Turk (AMT) and CrowdFlower(CF)1 have been employed for semantic inferenceannotation (Snow et al, 2008; Wang and Callison-Burch, 2010; Mehdad et al, 2010; Negri et al,2011).
These works focused on generating and an-notating RTE text-hypothesis pairs, but did not ad-dress annotation and evaluation of inference rules.In this paper, we propose a novel instance-basedevaluation framework for inference rules that takesadvantage of crowdsourcing.
Our method substan-tially simplifies annotation of rule applications andavoids annotator training completely.
The nov-elty in our framework is two-fold: (1) We simplifyinstance-based evaluation from a complex decisionscenario to two independent binary decisions.
(2)We apply methodological principles that efficientlycommunicate the definition of the ?inference?
rela-tion to untrained crowdsourcing workers (Turkers).As a case study, we applied our method to evalu-ate algorithms for learning inference rules betweenpredicates.
We show that we can produce many an-notations cheaply, quickly, at good quality, whileachieving high inter-annotator agreement.2 Evaluating Rule ApplicationsAs mentioned, in instance-based evaluation individ-ual rule applications are judged rather than rules inisolation, and the quality of a rule-resource is thenevaluated by the validity of a sample of applicationsof its rules.
Rule application is performed by findingan instantiation of the rule left-hand-side in a cor-pus (termed LHS extraction) and then applying therule on the extraction to produce an instantiation ofthe rule right-hand-side (termed RHS instantiation).For example, the rule ?X observe Y?X celebrate Y?1https://www.mturk.com and http://crowdflower.comcan be applied on the LHS extraction ?they observeholidays?
to produce the RHS instantiation ?they cel-ebrate holidays?.The target of evaluation is to judge whether eachrule application is valid or not.
Following the stan-dard RTE task definition, a rule application is con-sidered valid if a human reading the LHS extrac-tion is highly likely to infer that the RHS instanti-ation is true (Dagan et al, 2009).
In the aforemen-tioned example, the annotator is expected to judgethat ?they observe holidays?
entails ?they celebrateholidays?.
In addition to this straightforward case,two more subtle situations may arise.
The first isthat the LHS extraction is meaningless.
We regarda proposition as meaningful if a human can easilyunderstand its meaning (despite some simple gram-matical errors).
A meaningless LHS extraction usu-ally occurs due to a faulty extraction process (e.g.,Table 1, Example 2) and was relatively rare in ourcase study (4% of output, see Section 4).
Such ruleapplications can either be extracted from the sam-ple so that the rule-base is not penalized (since theproblem is in the extraction procedure), or can beused as examples of non-entailment, if we are in-terested in overall performance.
A second situationis a meaningless RHS instantiation, usually causedby rule application in a wrong context.
This case istagged as non-entailment (for example, applying therule ?X observe Y?X celebrate Y?
in the context ofthe extraction ?companies observe dress code?
).Each rule application therefore requires an answerto the following three questions: 1) Is the LHS ex-traction meaningful?
2) Is the RHS instantiationmeaningful?
3) If both are meaningful, does theLHS extraction entail the RHS instantiation?3 CrowdsourcingPrevious works using crowdsourcing noted someprinciples to help get the most out of the ser-vice(Wang et al, 2012).
In keeping with these find-ings we employ the following principles: (a) Simpletasks.
The global task is split into simple sub-tasks,each dealing with a single aspect of the problem.
(b)Do not assume linguistic knowledge by annota-tors.
Task descriptions avoid linguistic terms suchas ?tense?, which confuse workers.
(c) Gold stan-dard validation.
Using CF?s built-in methodology,157Phrase Meaningful Comments1) Doctors be treat Mary Yes Annotators are instructed to ignore simple inflectional errors2) A player deposit an No Bad extraction for the rule LHS ?X deposit Y?3) humans bring in bed No Wrong context, result of applying ?X turn in Y?X bring in Y?
on ?humans turn in bed?Table 1: Examples of phrase ?meaningfulness?
(Note that the comments are not presented to Turkers).gold standard (GS) examples are combined with ac-tual annotations to continuously validate annotatorreliability.We split the annotation process into two tasks,the first to judge phrase meaningfulness (Questions1 and 2 above) and the second to judge entailment(Question 3 above).
In Task 1, the LHS extrac-tions and RHS instantiations of all rule applicationsare separated and presented to different Turkers in-dependently of one another.
This task is simple,quick and cheap and allows Turkers to focus onthe single aspect of judging phrase meaningfulness.Rule applications for which both the LHS extrac-tion and RHS instantiation are judged as meaningfulare passed to Task 2, where Turkers need to decidewhether a given rule application is valid.
If not forTask 1, Turkers would need to distinguish in Task 2between non-entailment due to (1) an incorrect rule(2) a meaningless RHS instantiation (3) a meaning-less LHS extraction.
Thanks to Task 1, Turkers arepresented in Task 2 with two meaningful phrases andneed to decide only whether one entails the other.To ensure high quality output, each example isevaluated by three Turkers.
Similarly to Mehdad etal.
(2010) we only use results for which the confi-dence value provided by CF is greater than 70%.We now describe the details of both tasks.
Oursimplification contrasts with Szpektor et al (2007),whose judgments for each rule application are simi-lar to ours, but had to be performed simultaneouslyby annotators, which required substantial training.Task 1: Is the phrase meaningful?In keeping with the second principle above, the taskdescription is made up of a short verbal explana-tion followed by positive and negative examples.The definition of ?meaningfulness?
is conveyed viaexamples pointing to properties of the automaticphrase extraction process, as seen in Table 1.Task 2: Judge if one phrase is true given another.As mentioned, rule applications for which both sideswere judged as meaningful are evaluated for entail-ment.
The challenge is to communicate the defini-tion of ?entailment?
to Turkers.
To that end the taskdescription begins with a short explanation followedby ?easy?
and ?hard?
examples with explanations,covering a variety of positive and negative entail-ment ?types?
(Table 2).Defining ?entailment?
is quite difficult when deal-ing with expert annotators and still more with non-experts, as was noted by Negri et al (2011).
Wetherefore employ several additional mechanisms toget the definition of entailment across to Turkersand increase agreement with the GS.
We run aninitial small test run and use its output to improveannotation in two ways: First, we take examplesthat were ?confusing?
for Turkers and add them tothe GS with explanatory feedback presented whena Turker answers incorrectly.
(E.g., the pair (?Theowner be happy to help drivers?, ?The owner assistdrivers?)
was judged as entailing in the test run butonly achieved a confidence value of 0.53).
Second,we add examples that were annotated unanimouslyby Turkers to the GS to increase its size, allowingCF to better estimate Turker?s reliability (followingCF recommendations, we aim to have around 10%GS examples in every run).
In Section 4 we showthat these mechanisms improved annotation quality.4 Case StudyAs a case study, we used our evaluation methodol-ogy to compare four methods for learning entailmentrules between predicates: DIRT (Lin and Pantel,2001), Cover (Weeds and Weir, 2003), BInc (Szpek-tor and Dagan, 2008) and Berant et al (2010).
Tothat end, we applied the methods on a set of onebillion extractions (generously provided by Faderet al (2011)) automatically extracted from theClueWeb09 web crawl2, where each extraction com-prises a predicate and two arguments.
This resultedin four learned inference rule resources.2http://lemurproject.org/clueweb09.php/158Example Entailed Explanation given to TurkersLHS: The lawyer sign the contract Yes There is a chance the lawyer has not read the contract, butmost likely that as he signed it, he must have read it.RHS: The lawyer read the contractLHS: John be related to Jerry No The LHS can be understood from the RHS, but not theother way around as the LHS is more general.RHS: John be a close relative of JerryLHS: Women be at increased risk of cancer No Although the RHS is correct, it cannot be understood fromthe LHS.RHS: Women die of cancerTable 2: Examples given in the description of Task 2.We randomly sampled 5,000 extractions, and foreach one sampled four rules whose LHS matches theextraction from the union of the learned resources.We then applied the rules, which resulted in 20,000rule applications.
We annotated rule applicationsusing our methodology and evaluated each learn-ing method by comparing the rules learned by eachmethod with the annotation generated by CF.In Task 1, 281 rule applications were annotated asmeaningless LHS extraction, and 1,012 were anno-tated as meaningful LHS extraction but meaninglessRHS instantiation and so automatically annotated asnon-entailment.
8,264 rule applications were passedon to Task 2, as both sides were judged meaning-ful (the remaining 10,443 discarded due to low CFconfidence).
In Task 2, 5,555 rule applications werejudged with a high confidence and supplied as out-put, 2,447 of them as positive entailment and 3,108as negative.
Overall, 6,567 rule applications (datasetof this paper) were annotated for a total cost of$1000.
The annotation process took about one week.In tests run during development we experimentedwith Task 2 wording and GS examples, seeking tomake the definition of entailment as clear as pos-sible.
To do so we randomly sampled and manu-ally annotated 200 rule applications (from the initial20,000), and had Turkers judge them.
In our initialtest, Turkers tended to answer ?yes?
comparing toour own annotation, with 0.79 agreement betweentheir annotation and ours, corresponding to a kappascore of 0.54.
After applying the mechanisms de-scribed in Section 3, false-positive rate was reducedfrom 18% to 6% while false-negative rate only in-creased from 4% to 5%, corresponding to a highagreement of 0.9 and kappa of 0.79.In our test, 63% of the 200 rule applications wereannotated unanimously by the Turkers.
Importantly,all these examples were in perfect agreement withour own annotation, reflecting their high reliability.For the purpose of evaluating the resources learnedby the algorithms we used annotations with CF con-fidence ?
0.7 for which kappa is 0.99.Lastly, we computed the area under the recall-precision curve (AUC) for DIRT, Cover, BInc andBerant et al?s method, resulting in an AUC of 0.4,0.43, 0.44, and 0.52 respectively.
We used the AUCcurve, with number of recall-precision points in theorder of thousands, to avoid tuning a threshold pa-rameter.
Overall, we demonstrated that our evalua-tion framework allowed us to compare four differentlearning methods in low costs and within one week.5 DiscussionIn this paper we have suggested a crowdsourcingframework for evaluating inference rules.
We haveshown that by simplifying the previously-proposedinstance-based evaluation framework we are able totake advantage of crowdsourcing services to replacetrained expert annotators, resulting in good qualitylarge scale annotations, for reasonable time and cost.We have presented the methodological principles wedeveloped to get the entailment decision across toTurkers, achieving very high agreement both withour annotations and between the annotators them-selves.
Using the CrowdFlower forms we providewith this paper, the proposed methodology can bebeneficial for both resource developers evaluatingtheir output as well as inference system developerswanting to assess the quality of existing resources.AcknowledgmentsThis work was partially supported by the IsraelScience Foundation grant 1112/08, the PASCAL-2 Network of Excellence of the European Com-munity FP7-ICT-2007-1-216886, and the Euro-pean Communitys Seventh Framework Programme(FP7/2007-2013) under grant agreement no.
287923(EXCITEMENT).159ReferencesJonathan Berant, Ido Dagan, and Jacob Goldberger.2010.
Global learning of focused entailment graphs.In Proceedings of the annual meeting of the Associa-tion for Computational Linguistics (ACL).Rahul Bhagat, Patrick Pantel, and Eduard Hovy.
2007.LEDIR: An unsupervised algorithm for learning di-rectionality of inference rules.
In Proceedings of the2007 Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL).Ido Dagan, Bill Dolan, Bernardo Magnini, and Dan Roth.2009.
Recognizing textual entailment: Rational, eval-uation and approaches.
Natural Language Engineer-ing, 15(Special Issue 04):i?xvii.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open informationextraction.
In Proceedings of the Conference ofEmpirical Methods in Natural Language Processing(EMNLP ?11).Dekang Lin and Patrick Pantel.
2001.
DIRT - discov-ery of inference rules from text.
In Proceedings of theACM SIGKDD Conference on Knowledge Discoveryand Data Mining.Peter LoBue and Alexander Yates.
2011.
Types ofcommon-sense knowledge needed for recognizing tex-tual entailment.
In Proceedings of the 49th AnnualMeeting of the Association for Computational Linguis-tics: Human Language Technologies (ACL-HLT).Yashar Mehdad, Matteo Negri, and Marcello Federico.2010.
Towards cross-lingual textual entailment.
InHuman Language Technologies: The 2010 AnnualConference of the North American Chapter of the As-sociation for Computational Linguistics (ACL).Matteo Negri, Luisa Bentivogli, Yashar Mehdad, DaniloGiampiccolo, and Alessandro Marchetti.
2011.
Di-vide and conquer: Crowdsourcing the creation ofcross-lingual textual entailment corpora.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing (EMNLP ?11).Deepak Ravichandran and Eduard Hovy.
2002.
Learningsurface text patterns for a question answering system.In Proceedings of the annual meeting of the Associa-tion for Computational Linguistics (ACL).Mark Sammons, V. G. Vinod Vydiswaran, and Dan Roth.2010.
?ask not what textual entailment can do foryou...?.
In Proceedings of the annual meeting of theAssociation for Computational Linguistics (ACL).Stefan Schoenmackers, Oren Etzioni Jesse Davis, andDaniel S. Weld.
2010.
Learning first-order hornclauses from web text.
In Proceedings of the 2010Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP ?10).Satoshi Sekine.
2005.
Automatic paraphrase discoverybased on context and keywords between ne pairs.
InProceedings of the Third International Workshop onParaphrasing (IWP2005).Yusuke Shinyama and Satoshi Sekine.
2006.
Preemp-tive information extraction using unrestricted relationdiscovery.
In Proceedings of the main conferenceon Human Language Technology Conference of theNorth American Chapter of the Association of Com-putational Linguistics (HLT-NAACL ?06).Yusuke Shinyama, Satoshi Sekine, and Kiyoshi Sudo.2002.
Automatic paraphrase acquisition from newsarticles.
In Proceedings of the second internationalconference on Human Language Technology Research(HLT ?02).Rion Snow, Brendan O?Connor, Daniel Jurafsky, and An-drew Y. Ng.
2008.
Cheap and fast?but is it good?
:evaluating non-expert annotations for natural languagetasks.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing (EMNLP?08).Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In Proceedings of the22nd International Conference on Computational Lin-guistics (Coling 2008).Idan Szpektor, Eyal Shnarch, and Ido Dagan.
2007.Instance-based evaluation of entailment rule acquisi-tion.
In Proceedings of the annual meeting of the As-sociation for Computational Linguistics (ACL).Rui Wang and Chris Callison-Burch.
2010.
Cheap factsand counter-facts.
In Proceedings of the NAACL HLT2010 Workshop on Creating Speech and LanguageData with Amazon?s Mechanical Turk.Aobo Wang, Cong Duy Vu Hoang, and Min-Yen Kan.2012.
Perspectives on crowdsourcing annotations fornatural language processing.
Journal of Language Re-sources and Evaluation).Julie Weeds and David Weir.
2003.
A general frame-work for distributional similarity.
In Proceedings ofthe Conference on Empirical Methods in Natural Lan-guage Processing (EMNLP 2003).160
