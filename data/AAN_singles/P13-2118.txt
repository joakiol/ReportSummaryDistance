Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 671?677,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsJoint Apposition Extraction with Syntactic and Semantic ConstraintsWill Radford and James R. Currane-lab, School of Information TechnologiesUniversity of SydneyNSW, 2006, Australia{wradford,james}@it.usyd.edu.auAbstractAppositions are adjacent NPs used to addinformation to a discourse.
We proposesystems exploiting syntactic and seman-tic constraints to extract appositions fromOntoNotes.
Our joint log-linear modeloutperforms the state-of-the-art Favre andHakkani-Tu?r (2009) model by ?10% onBroadcast News, and achieves 54.3% F-score on multiple genres.1 IntroductionAppositions are typically adjacent coreferent nounphrases (NP) that often add information aboutnamed entities (NEs).
The apposition in Figure 1consists of three comma-separated NPs ?
the firstNP (HEAD) names an entity and the others (ATTRs)supply age and profession attributes.
Attributescan be difficult to identify despite characteristicpunctuation cues, as punctuation plays many rolesand attributes may have rich substructure.While linguists have studied apposition in de-tail (Quirk et al, 1985; Meyer, 1992), most appo-sition extraction has been within other tasks, suchas coreference resolution (Luo and Zitouni, 2005;Culotta et al, 2007) and textual entailment (Rothand Sammons, 2007).
Extraction has rarely beenintrinsically evaluated, with Favre and Hakkani-Tu?r?s work a notable exception.We analyze apposition distribution inOntoNotes 4 (Pradhan et al, 2007) and com-pare rule-based, classification and parsingextraction systems.
Our best system uses a jointmodel to classify pairs of NPs with featuresthat faithfully encode syntactic and semanticrestrictions on appositions, using parse trees andWordNet synsets.
{John Ake}h , {48}a , {a former vice-presidentin charge of legal compliance at American CapitalManagement & Research Inc., in Houston,}a , .
.
.Figure 1: Example apposition from OntoNotes 4Our approach substantially outperforms Favreand Hakkani-Tu?r on Broadcast News (BN) at54.9% F-score and has state-of-the-art perfor-mance 54.3% F-score across multiple genres.
Ourresults will immediately help the many systemsthat already use apposition extraction components,such as coreference resolution and IE.2 BackgroundApposition is widely studied, but ?grammariansvary in the freedom with which they apply theterm ?apposition??
(Quirk et al, 1985).
They areusually composed of two or more adjacent NPs,hierarchically structured, so one is the head NP(HEAD) and the rest attributes (ATTRs).
They areoften flagged using punctuation in text and pausesin speech.
Pragmatically, they allow an author tointroduce new information and build a shared con-text (Meyer, 1992).Quirk et al propose three tests for apposition: i)each phrase can be omitted without affecting sen-tence acceptability, ii) each fulfils the same syntac-tic function in the resultant sentences, iii) extralin-guistic reference is unchanged.
Strict interpreta-tions may exclude other information-bearing caseslike pseudo-titles (e.g.
({President}a {Bush}h)NP),but include some adverbial phrases (e.g.
{(JohnSmith)NP}h, {(formerly (the president)NP)AP}a).
Weadopt the OntoNotes guidelines?
relatively strictinterpretation: ?a noun phrase that modifies animmediately-adjacent noun phrase (these may beseparated by only a comma, colon, or parenthe-sis).?
(BBN, 2004?2007).671Unit TRAINF DEVF TESTF TRAIN DEV TESTSents.
9,595 976 1,098 48,762 6,894 6,896Appos.
590 64 68 3,877 502 490Table 1: Sentence and apposition distributionApposition extraction is a common componentin many NLP tasks: coreference resolution (Luoand Zitouni, 2005; Culotta et al, 2007; Bengt-son and Roth, 2008; Poon and Domingos, 2008),textual entailment (Roth and Sammons, 2007;Cabrio and Magnini, 2010), sentence simplifica-tion (Miwa et al, 2010; Candido et al, 2009;Siddharthan, 2002) and summarization (Nenkovaet al, 2005).
Comma ambiguity has been studiedin the RTE (Srikumar et al, 2008) and generationdomains (White and Rajkumar, 2008).Despite this, few papers to our knowledge ex-plicitly evaluate apposition extraction.
Moreover,apposition extraction is rarely the main researchgoal and descriptions of the methods used are of-ten accordingly terse or do not match our guide-lines.
Lee et al (2011) use rules to extract appo-sitions for coreference resolution, selecting onlythose that are explicitly flagged using commas orparentheses.
They do not separately mark HEADand ATTR and permit relative clauses as an ATTR.While such differences capture useful informationfor coreference resolution, these methods wouldbe unfairly disadvantaged in a direct evaluation.Favre and Hakkani-Tu?r (2009, FHT) directlyevaluate three extraction systems on OntoNotes2.9 news broadcasts.
The first retrains the Berke-ley parser (Petrov and Klein, 2007) on trees la-belled with appositions by appending the HEADand ATTR suffix to NPs ?
we refer to this as a La-belled Berkeley Parser (LBP).
The second is a CRFlabelling words using an IOB apposition scheme.Token, POS, NE and BP-label features are used,as are presence of speech pauses.
The final sys-tem classifies parse tree phrases using an Adaboostclassifier (Schapire and Singer, 2000) with similarfeatures.The LBP, IOB and phrase systems score 41.38%,32.76% and 40.41%, while their best uses LBP treelabels as IOB features, scoring 42.31%.
Their fo-cus on BN automated speech recognition (ASR)output, which precludes punctuation cues, doesnot indicate how well the methods perform on tex-tual genres.
Moreover all systems use parsers orparse-label features and do not completely evalu-ate non-parser methods for extraction despite in-cluding baselines.Form # % Reverse form # % ?%H t A 2109 55.9 A t H 724 19.2 75.1A H 482 12.8 H A 205 5.4 93.3H , A 1843 48.9 A , H 532 14.1 63.0A H 482 12.9 H A 205 5.4 81.3H ( A 146 3.9 A ( H 16 0.4 85.6A : H 94 2.5 H : A 23 0.6 88.7H -- A 66 1.8 A -- H 35 0.9 91.4A - H 31 0.8 H - A 21 0.6 92.8Table 2: Apposition forms in TRAIN with abstract(top) and actual (bottom) tokens, e.g., H t A in-dicates an HEAD, one token then an ATTR.3 DataWe use apposition-annotated documents from theEnglish section of OntoNotes 4 (Weischedel et al,2011).
We manually adjust appositions that do nothave exactly one HEAD and one or more ATTR1.Some appositions are nested, and we keep only?leaf?
appositions, removing the higher-level ap-positions.We follow the CoNLL-2011 scheme to selectTRAIN, DEV and TEST datasets (Pradhan et al,2011).
OntoNotes 4 is made up of a wide vari-ety of sources: broadcast conversation and news,magazine, newswire and web text.
Appositionsare most frequent in newswire (one per 192 words)and least common in broadcast conversation (oneper 645 words) with the others in between (aroundone per 315 words).We also replicate the OntoNotes 2.9 BN dataused by FHT, selecting the same sentences fromOntoNotes 4 (TRAINF/DEVF/TESTF).
We do not?speechify?
our data and take a different approachto nested apposition.
Table 1 shows the distri-bution of sentences and appositions (HEAD-ATTRpairs).3.1 AnalysisMost appositions in TRAIN have one ATTR(97.4%) with few having two (2.5%) or three(0.1%).
HEADs are typically shorter (median 5tokens, 95% < 7) than ATTRs (median 7 tokens,95% < 15).
Table 2 shows frequent appositionforms.
Comma-separated apposition is the mostcommon (63%) and 93% are separated by zero orone token.
HEADs are often composed of NEs:52% PER and 13% ORG, indicating an entity aboutwhich the ATTR adds information.1Available at http://schwa.org/resources672Pattern and Example P R F{ne:PER}h # {pos:NP (pos:IN ne:LOC|ORG|GPE)?
}a #?
{Jian Zhang}h, {the head of Chinese delegation}a,?
73.1 21.9 33.7{pos:DT gaz:role|relation}a #?
{ne:PER}h?
{his new wife}a {Camilla}h?
45.9 9.5 15.8{ne:ORG|GPE}h # {pos:DT pos:NP}a #?
{Capetronic Inc.}h, {a Taiwan electronics maker}a,?
60.4 6.0 10.9{pos:NP}a # {ne:PER}h #?
{The vicar}a, {W.D.
Jones}h,?
33.7 4.5 7.9{ne:PER}h # {pos:NP pos:POS pos:NP}a #?
{Laurence Tribe}h, {Gore ?s attorney}a,?
82.0 4.0 7.7Table 3: The top-five patterns by recall in the TRAIN dataset.
?#?
is a pause (e.g., punctuation), ?|?
adisjunction and ???
an optional part.
Patterns are used to combine tokens into NPs for pos:NP.4 Extracting AppositionsWe investigate different extraction systems usinga range of syntactic information.
Our systems thatuse syntactic parses generate candidates (pairs ofNPs: p1 and p2) that are then classified as apposi-tion or not.This paper contributes three complementarytechniques for more faithfully modelling apposi-tion.
Any adjacent NPs, disregarding interveningpunctuation, could be considered candidates, how-ever stronger syntactic constraints that only allowsibling NP children provide higher precision can-didate sets.
Semantic compatibility features en-coding that an ATTR provides consistent informa-tion for its HEAD.
A joint classifier models thecomplete apposition rather than combining sepa-rate phrase-wise decisions.
Taggers and parsersare trained on TRAIN and evaluated on DEV orTEST.
We use the C&C tools (Curran and Clark,2003) for POS and NE tagging and the and theBerkeley Parser (Petrov and Klein, 2007), trainedwith default parameters.Pattern POS, NE and lexical patterns are usedto extract appositions avoiding parsing?s compu-tational overhead.
Rules are applied indepen-dently to tokenized and tagged sentences, yield-ing HEAD-ATTR tuples that are later deduplicated.The rules were manually derived from TRAIN2 andTable 3 shows the top five of sixteen rules by re-call over TRAIN.
The ?role?
gazetteer is the transi-tive closure of hyponyms of the WordNet (Miller,1995) synset person.n.01 and ?relation?
man-ually constructed (e.g., ?father?, ?colleague?).
Tu-ples are post-processed to remove spurious appo-2There is some overlap between TRAIN and DEVF/TESTFwith appositions from the latter used in rule generation.sitions such as comma-separated NE lists3.Adjacent NPs This low precision, high recallbaseline assumes all candidates, depending ongeneration strategy, are appositions.Rule We only consider HEADs whose syntactichead is a PER, ORG, LOC or GPE NE.
We formalisesemantic compatibility by requiring the ATTR headto match a gazetteer dependent on the HEAD?s NEtype.
To create PER, ORG and LOC gazetteers,we identified common ATTR heads in TRAIN andlooked for matching WordNet synsets, selectingthe most general hypernym that was still seman-tically compatible with the HEAD?s NE type.Gazetteer words are pluralized using pattern.en(De Smedt and Daelemans, 2012) and normalised.We use partitive and NML-aware rules (Collins,1999; Vadas and Curran, 2007) to extract syntacticheads from ATTRs.
These must match the type-appropriate gazetteer, with ORG and LOC/GPEfalling back to PER (e.g., ?the champion, Apple?
).Extracted tuples are post-processed as for Pat-tern and reranked by the OntoNotes specificityscale (i.e., NNP > PRO > Def.
NP > Indef.
NP> NP), and the more specific unit is assignedHEAD.
Possible ATTRs further to the left or rightare checked, allowing for cases such as Figure 1.Labelled Berkeley Parser We train a LBP onTRAIN and recover appositions from parsed sen-tences.
Without syntactic constraints this is equiv-alent to FHT?s LBP system (LBPF) and indicated by?
in Tables.Phrase Each NP is independently classified asHEAD, ATTR or None.
We use a log-linear modelwith a SGD optimizer from scikit-learn (Pedregosa3Full description: http://schwa.org/resources673Model Full system -syn -sem -both +goldPattern 44.8 34.9 39.2 - - - - - - - - - 52.2 39.6 45.1Adj NPs 11.6 58.0 19.3 3.6 65.1 6.8 - - - - - - 16.0 85.3 27.0Rule 65.3 46.8 54.5 43.7 50.0 46.7 - - - - - - 79.1 62.0 69.5LBP 66.3 52.2 58.4 47.8 53.0 ?50.3 - - - - - - - - -Phrase 73.2 45.6 56.2 77.7 41.0 53.7 73.2 44.6 55.4 77.7 40.8 ?53.5 89.0 58.2 70.4Joint 66.3 49.0 56.4 68.5 48.6 56.9 70.4 47.0 56.4 68.9 48.0 56.6 87.9 69.5 77.6Joint LBP 69.6 51.0 58.9 69.6 49.6 57.9 71.5 49.0 58.2 68.3 48.6 56.8 - -Table 4: Results over DEV: each column shows precision, recall and F-score.
-syn/-sem/-both show theimpact of removing constraints/features, +gold shows the impact of parse and tagging errors.et al, 2011).
The binary features are calculatedfrom a generated candidate phrase (p) and are thesame as FHT?s phrase system (PhraseF), denoted?
in Tables.
In addition, we propose the fea-tures below and to decode classifications, adjacentapposition-classified NPs are re-ordered by speci-ficity.?
p precedes/follows punctuation/interjection?
p starts with a DT or PRP$ (e.g., ?{thedirector}a?
or ?
{her husband}a?)?
p?s syntactic head matches a NE-specific se-mantic gazetteer (e.g., ?
{the famous actor}a??
PER, ?
{investment bank}a??
ORG)?
p?s syntactic head has the POS CD (e.g.,?
{John Smith}h, {34}a, .
.
.
?)?
p?s NE type (e.g., ?
{John Smith}h??
PER)?
Specificity rankJoint The final system classifies pairs of phrases(p1, p2) as: HEAD-ATTR, ATTR-HEAD or None.The system uses the phrase model features asabove as well as pairwise features:?
the cross-product of selected features for p1and p2: gazetteer matches, NE type, speci-ficity rank.
This models the compatibility be-tween p1 and p2.
For example, if the HEADhas the NE type PER and the ATTR has thesyntactic head in the PER gazetteer, for ex-ample ?
{Tom Cruise}h, {famous actor}a,??
(p1: PER, p2: PER-gaz)?
If semantic features are found in p1 and p2?
p1/p2 specificity (e.g., equal, p1 > p2)?
whether p1 is an acronym of p2 or vice-versa5 ResultsWe evaluate by comparing the extracted HEAD-ATTR pairs against the gold-standard.
Correctpairs match gold-standard bounds and label.
Wereport precision (P), recall (R) and F1-score (F).Table 4 shows our systems?
performance on themulti-genre DEV dataset, the impact of remov-ing syntactic constraints, semantic features andparse/tag error.
Pattern performance is reasonableat 39.2% F-score given its lack of full syntacticinformation.
All other results use parses and, al-though it has a low F-score, the Adjacent NPs?65.1% recall, without syntactic constraints, is theupper bound for the parse-based systems.
Statis-tical models improve performance, with the jointmodels better than the higher-precision phrasemodel as the latter must make two independentlycorrect classification decisions.
Our best systemhas an F-score of 58.9% using a joint model overthe de-labelled trees produced by the LBP.
Thisindicates that although our model does not usethe apposition labels from the tree, the tree is amore suitable structure for extraction.
This sys-tem substantially improves on our implementationof FHT?s LBPF (?)
and PhraseF (?)
systems by 8.6%and 5.4%4.Removing syntactic constraints mostly reducesperformance in parse-based systems as the systemmust consider lower-quality candidates.
The F-score increase is driven by higher precision at min-imal cost to recall.
Removing semantic featureshas less impact and removing both is most detri-mental to performance.
These features have lessimpact on joint models; indeed, joint performanceusing BP trees increases without the features, per-haps as joint models already model the syntacticcontext.We evaluate the impact of parser and taggererror by using gold-standard resources.
Gold-standard tags and trees improve recall in all casesleading to F-score improvements (+gold).
Thepattern system is reasonably robust to automatictagging errors, but parse-based models suffer con-siderably from automatic parses.
To compare theimpact of tagging and parsing error, we configurethe joint system to use gold parses and automaticNE tags and vice versa.
Using automatic tags doesnot greatly impact performance (-1.3%), whereas4We do not implement the IOB or use LBP features forTRAIN as these would require n-fold parser training.674Model P R FLBPF ?
53.1 46.9 49.8PhraseF ?
71.5 30.2 42.5Pattern 44.8 34.3 38.8LBP 63.9 45.1 52.9Joint LBP 66.9 45.7 54.3Table 5: Results over TEST: FHT?s (top) and our(bottom) systems.Error BP LBP ?PP Attachment 5,585 5,396 -189NP Internal Structure 1,483 1,338 -145Other 3,164 3,064 -100Clause Attachment 3,960 3,867 -93Modifier Attachment 1,523 1,700 177Co-ordination 3,095 3,245 150NP Attachment 2,615 2,680 65Total 30,189 29,859 -330Table 6: Selected BP/LBP parse error distribution.using automatic parses causes a drop of around20% to 57.7%, demonstrating that syntactic infor-mation is crucial for apposition extraction.We compare our work with Favre and Hakkani-Tu?r (2009), training with TRAINF and evaluatingover TESTF?
exclusively BN data.
Our implemen-tations of their systems, PhraseF and LBPF, per-form at 43.6% and 44.1%.
Our joint LBP systemis substantially better, scoring 54.9%.Table 5 shows the performance of our best sys-tems on the TEST dataset and these follow thesame trend as DEV.
Joint LBP performs the bestat 54.3%, 4.5% above LBPF.Finally, we test whether labelling appositionscan help parsing.
We parse DEV trees with LBPand BP, remove apposition labels and analysethe impact of labelling using the Berkeley ParserAnalyser (Kummerfeld et al, 2012).
Table 6shows the LBP makes fewer errors, particularlyNP internal structuring, PP and clause attachmentclasses at the cost of modifier attachment and co-ordination errors.
Rather than increasing parsingdifficulty, apposition labels seem complementary,improving performance.6 ConclusionWe present three apposition extraction techniques.Linguistic tests for apposition motivate strict syn-tactic constraints on candidates and semantic fea-tures encode the addition of compatible informa-tion.
Joint models more faithfully capture apposi-tion structure and our best system achieves state-of-the-art performance of 54.3%.
Our results willimmediately benefit the large number of systemswith apposition extraction components for coref-erence resolution and IE.AcknowledgementsThe authors would like to thank the anonymous re-viewers for their suggestions.
Thanks must also goto Benoit Favre for his clear writing and help an-swering our questions as we replicated his datasetand system.
This work has been supported byARC Discovery grant DP1097291 and the Capi-tal Markets CRC Computable News project.ReferencesBBN.
2004?2007.
Co-reference guidelines for en-glish ontonotes.
Technical Report v6.0, BBNTechnologies.Eric Bengtson and Dan Roth.
2008.
Understand-ing the value of features for coreference resolu-tion.
In Proceedings of the 2008 Conference onEmpirical Methods in Natural Language Pro-cessing, pages 294?303.
Association for Com-putational Linguistics, Honolulu, Hawaii.Elena Cabrio and Bernardo Magnini.
2010.
To-ward qualitative evaluation of textual entailmentsystems.
In Coling 2010: Posters, pages 99?107.
Coling 2010 Organizing Committee, Bei-jing, China.Arnaldo Candido, Erick Maziero, Lucia Specia,Caroline Gasperin, Thiago Pardo, and SandraAluisio.
2009.
Supporting the adaptation oftexts for poor literacy readers: a text simplifi-cation editor for brazilian portuguese.
In Pro-ceedings of the Fourth Workshop on InnovativeUse of NLP for Building Educational Applica-tions, pages 34?42.
Association for Computa-tional Linguistics, Boulder, Colorado.Michael Collins.
1999.
Head-Driven StatisticalModels for Natural Language Parsing.
Ph.D.thesis, University of Pennsylvania.Aron Culotta, Michael Wick, and Andrew Mc-Callum.
2007.
First-order probabilistic mod-els for coreference resolution.
In Human Lan-guage Technologies 2007: The Conference ofthe North American Chapter of the Associationfor Computational Linguistics; Proceedings ofthe Main Conference, pages 81?88.
Association675for Computational Linguistics, Rochester, NewYork.James Curran and Stephen Clark.
2003.
Languageindependent ner using a maximum entropy tag-ger.
In Walter Daelemans and Miles Osborne,editors, Proceedings of the Seventh Conferenceon Natural Language Learning at HLT-NAACL2003, pages 164?167.Tom De Smedt and Walter Daelemans.
2012.
Pat-tern for python.
Journal of Machine LearningResearch, 13:2013?2035.Benoit Favre and Dilek Hakkani-Tu?r.
2009.Phrase and word level strategies for detectingappositions in speech.
In Proceedings of Inter-speech 2009, pages 2711?2714.
Brighton, UK.Jonathan K. Kummerfeld, David Hall, James R.Curran, and Dan Klein.
2012.
Parser show-down at the wall street corral: An empirical in-vestigation of error types in parser output.
InProceedings of the 2012 Joint Conference onEmpirical Methods in Natural Language Pro-cessing and Computational Natural LanguageLearning, pages 1048?1059.
Jeju Island, SouthKorea.Heeyoung Lee, Yves Peirsman, Angel Chang,Nathanael Chambers, Mihai Surdeanu, and DanJurafsky.
2011.
Stanford?s multi-pass sievecoreference resolution system at the conll-2011 shared task.
In Proceedings of theCoNLL-2011 Shared Task.
URL pubs/conllst2011-coref.pdf.Xiaoqiang Luo and Imed Zitouni.
2005.
Multi-lingual coreference resolution with syntacticfeatures.
In Proceedings of Human Lan-guage Technology Conference and Conferenceon Empirical Methods in Natural LanguageProcessing, pages 660?667.
Association forComputational Linguistics, Vancouver, BritishColumbia, Canada.Charles F. Meyer.
1992.
Apposition in Contem-porary English.
Cambridge University Press,Cambridge, UK.George A. Miller.
1995.
Wordnet: A lexicaldatabase for english.
Communications of theACM, 38:39?41.Makoto Miwa, Rune S?tre, Yusuke Miyao, andJun?ichi Tsujii.
2010.
Entity-focused sentencesimplification for relation extraction.
In Pro-ceedings of the 23rd International Conferenceon Computational Linguistics (Coling 2010),pages 788?796.
Coling 2010 Organizing Com-mittee, Beijing, China.Ani Nenkova, Advaith Siddharthan, and Kath-leen McKeown.
2005.
Automatically learn-ing cognitive status for multi-document sum-marization of newswire.
In Proceedings ofHuman Language Technology Conference andConference on Empirical Methods in NaturalLanguage Processing, pages 241?248.
Associa-tion for Computational Linguistics, Vancouver,British Columbia, Canada.F.
Pedregosa, G. Varoquaux, A. Gramfort,V.
Michel, B. Thirion, O. Grisel, M. Blondel,P.
Prettenhofer, R. Weiss, V. Dubourg, J. Van-derplas, A. Passos, D. Cournapeau, M. Brucher,M.
Perrot, and E. Duchesnay.
2011.
Scikit-learn: Machine Learning in Python .
Journalof Machine Learning Research, 12:2825?2830.Slav Petrov and Dan Klein.
2007.
Learning and in-ference for hierarchically split PCFGs.
In Pro-ceedings of the 22nd AAAI Conference of Artifi-cial Intelligence, pages 1642?1645.
Vancouver,Canada.Hoifung Poon and Pedro Domingos.
2008.Joint unsupervised coreference resolution withMarkov Logic.
In Proceedings of the 2008Conference on Empirical Methods in NaturalLanguage Processing, pages 650?659.
Associ-ation for Computational Linguistics, Honolulu,Hawaii.Sameer Pradhan, Lance Ramshaw, Mitchell Mar-cus, Martha Palmer, Ralph Weischedel, andNianwen Xue.
2011.
CoNLL-2011 sharedtask: Modeling unrestricted coreference inOntoNotes.
In Proceedings of the FifteenthConference on Computational Natural Lan-guage Learning: Shared Task, pages 1?27.Portland, OR USA.Sameer S. Pradhan, Eduard Hovy, Mitch Marcus,Martha Palmer, Lance Ramshaw, and RalphWeischedel.
2007.
OntoNotes: A unified rela-tional semantic representation.
In Proceedingsof the International Conference on SemanticComputing, pages 517?526.
Washington, DCUSA.Randolph Quirk, Sidney Greenbaum, GeoffreyLeech, and Jan Svartvik.
1985.
A Comprehen-sive Grammar of the English Language.
Gen-eral Grammar Series.
Longman, London, UK.676Dan Roth and Mark Sammons.
2007.
Seman-tic and logical inference model for textual en-tailment.
In Proceedings of the ACL-PASCALWorkshop on Textual Entailment and Para-phrasing, pages 107?112.
Association for Com-putational Linguistics, Prague.Robert E. Schapire and Yoram Singer.
2000.
Boos-texter: A boosting-based systemfor text catego-rization.
Machine Learning, 39(2-3):135?168.Advaith Siddharthan.
2002.
Resolving attachmentand clause boundary ambiguities for simplify-ing relative clause constructs.
In Proceedings ofthe ACL Student Research Workshop (ACLSRW2002), pages 60?65.
Association for Computa-tional Linguistics, Philadelphia.Vivek Srikumar, Roi Reichart, Mark Sammons,Ari Rappoport, and Dan Roth.
2008.
Extractionof entailed semantic relations through syntax-based comma resolution.
In Proceedings ofACL-08: HLT, pages 1030?1038.
Columbus,OH USA.David Vadas and James R. Curran.
2007.
Pars-ing internal noun phrase structure with collins?models.
In Proceedings of the AustralasianLanguage Technology Workshop 2007, pages109?116.
Melbourne, Australia.Ralph Weischedel, Martha Palmer, Mitchell Mar-cus, Eduard Hovy, Sameer Pradhan, LanceRamshaw, Nianwen Xue, Ann Taylor, JeffKaufman, Michelle Franchini, Mohammed El-Bachouti, Robert Belvin, and Ann Houston.2011.
OntoNotes Release 4.0.
Technical re-port, Linguistic Data Consortium, Philadelphia,PA USA.Michael White and Rajakrishnan Rajkumar.
2008.A more precise analysis of punctuation forbroad-coverage surface realization with CCG.In Coling 2008: Proceedings of the workshopon Grammar Engineering Across Frameworks,pages 17?24.
Coling 2008 Organizing Commit-tee, Manchester, England.677
