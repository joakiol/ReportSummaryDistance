Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 707?714,Sydney, July 2006. c?2006 Association for Computational LinguisticsTranslating HPSG-style Outputs of a Robust Parserinto Typed Dynamic LogicManabu Sato?
Daisuke Bekki?
Yusuke Miyao?
Jun?ichi Tsujii???
Department of Computer Science, University of TokyoHongo 7-3-1, Bunkyo-ku, Tokyo 113-0033, Japan?
Center for Evolutionary Cognitive Sciences, University of TokyoKomaba 3-8-1, Meguro-ku, Tokyo 153-8902, Japan?School of Informatics, University of ManchesterPO Box 88, Sackville St, Manchester M60 1QD, UK?SORST, JST (Japan Science and Technology Corporation)Honcho 4-1-8, Kawaguchi-shi, Saitama 332-0012, Japan?
{sa-ma, yusuke, tsujii}@is.s.u-tokyo.ac.jp?
bekki@ecs.c.u-tokyo.ac.jpAbstractThe present paper proposes a methodby which to translate outputs of a ro-bust HPSG parser into semantic rep-resentations of Typed Dynamic Logic(TDL), a dynamic plural semantics de-fined in typed lambda calculus.
Withits higher-order representations of con-texts, TDL analyzes and describesthe inherently inter-sentential nature ofquantification and anaphora in a strictlylexicalized and compositional manner.The present study shows that the pro-posed translation method successfullycombines robustness and descriptive ad-equacy of contemporary semantics.
Thepresent implementation achieves highcoverage, approximately 90%, for thereal text of the Penn Treebank corpus.1 IntroductionRobust parsing technology is one result of therecent fusion between symbolic and statisticalapproaches in natural language processing andhas been applied to tasks such as informationextraction, information retrieval and machinetranslation (Hockenmaier and Steedman, 2002;Miyao et al, 2005).
However, reflecting thefield boundary and unestablished interfaces be-tween syntax and semantics in formal theoryof grammar, this fusion has achieved less insemantics than in syntax.For example, a system that translates theoutput of a robust CCG parser into seman-tic representations has been developed (Bos etal., 2004).
While its corpus-oriented parser at-tained high coverage with respect to real text,the expressive power of the resulting semanticrepresentations is confined to first-order predi-cate logic.The more elaborate tasks tied to discourseinformation and plurality, such as resolutionof anaphora antecedent, scope ambiguity, pre-supposition, topic and focus, are required torefer to ?deeper?
semantic structures, such asdynamic semantics (Groenendijk and Stokhof,1991).However, most dynamic semantic theoriesare not equipped with large-scale syntax thatcovers more than a small fragment of targetlanguages.
One of a few exceptions is Min-imal Recursion Semantics (MRS) (Copestakeet al, 1999), which is compatible with large-scale HPSG syntax (Pollard and Sag, 1994)and has affinities with UDRS (Reyle, 1993).For real text, however, its implementation, asin the case of the ERG parser (Copestakeand Flickinger, 2000), restricts its target to thestatic fragment of MRS and yet has a lowercoverage than corpus-oriented parsers (Baldwin,to appear).The lack of transparency between syntax anddiscourse semantics appears to have created atension between the robustness of syntax andthe descriptive adequacy of semantics.In the present paper, we will introducea robust method to obtain dynamic seman-tic representations based on Typed DynamicLogic (TDL) (Bekki, 2000) from real textby translating the outputs of a robust HPSGparser (Miyao et al, 2005).
Typed Dy-namic Logic is a dynamic plural seman-tics that formalizes the structure underlyingthe semantic interactions between quantifica-tion, plurality, bound variable/E-type anaphora707re????
?e7?t xi1 ?
?
?xin ?
?G(i7?e)7?t .?
gi7?e.g ?
G?
r?gx1, .
.
.
,gxm??
?
prop ?
?G(i7?e)7?t .?
gi7?e.g ?
G??
?hi7?e.h ?
?G???
prop...?
prop??
?
?G(i7?e)7?t .(?
?
?
?
(?G))re f?xi?[?
prop] [?
prop] ?
?G(i7?e)7?t .??
?i f G?x = ?G?xthen ?
gi 7?e.g ?
?G?
G?x = ?G?xotherwise unde f ined?????
?where prop ?
((i 7?
e) 7?
t) 7?
(i 7?
e) 7?
tg?
?
G?
7?t ?
GgG(i7?e) 7?t.xi ?
?
de.
?gi7?e.g ?
G?gx = d??
?Figure 1: Propositions of TDL (Bekki, 2005)and presuppositions.
All of this complexdiscourse/plurality-related information is encap-sulated within higher-order structures in TDL,and the analysis remains strictly lexical andcompositional, which makes its interface withsyntax transparent and straightforward.
This isa significant advantage for achieving robustnessin natural language processing.2 Background2.1 Typed Dynamic LogicFigure 1 shows a number of propositions de-fined in (Bekki, 2005), including atomic pred-icate, negation, conjunction, and anaphoric ex-pression.
Typed Dynamic Logic is described intyped lambda calculus (G?del?s System T) withfour ground types: e(entity), i(index), n(naturalnumber), and t(truth).
While assignment func-tions in static logic are functions in meta-language from type e variables (in the case offirst-order logic) to objects in the domain De,assignment functions in TDL are functions inobject-language from indices to entities.
TypedDynamic Logic defines the notion context asa set of assignment functions (an object oftype (i 7?
e) 7?
t) and a proposition as a func-tion from context to context (an object of type((i 7?
e) 7?
t) 7?
(i 7?
e) 7?
t).
The conjunctionsof two propositions are then defined as com-posite functions thereof.
This setting conformsto the view of ?propositions as informationflow?, which is widely accepted in dynamicsemantics.Since all of these higher-order notions aredescribed in lambda terms, the path for compo-sitional type-theoretic semantics based on func-tional application, functional composition andtype raising is clarified.
The derivations ofTDL semantic representations for the sentences?A boy ran.
He tumbled.?
are exemplified inFigure 2 and Figure 3.
With some instantia-tion of variables, the semantic representationsof these two sentences are simply conjoinedand yield a single representation, as shown in(1).????
?boy0x1s1run0e1s1agent 0e1x1re f (x2) [ ]?tumble0e2s2agent 0e2x2??????
(1)The propositions boy0x1s1, run0e1s1 andagent 0e1x1 roughly mean ?the entity referredto by x1 is a boy in the situation s1?, ?theevent referred to by e1 is a running event inthe situation s1?, and ?the agent of event e1is x1?, respectively.The former part of (1) that corresponds tothe first sentence, filtering and testing the inputcontext, returns the updated context schema-tized in (2).
The updated context is thenpassed to the latter part, which corresponds tothe second sentence as its input.?
?
?
x1 s1 e1 ?
?
?john situation1 running1john situation2 running2.........(2)This mechanism makes anaphoric expressions,such as ?He?
in ?He tumbles?, accessible to itspreceding context; namely, the descriptions oftheir presuppositions can refer to the precedingcontext compositionally.
Moreover, the refer-ents of the anaphoric expressions are correctlycalculated as a result of previous filtering andtesting.708?a??
ni7?i7?p7?p.
?wi 7?i7?i7?p7?p.?
ei.?
si.?
?
p.nx1s?wx1es???boy??
xi.?
si.?
?
p.?boy0xs???wi7?i7?i7?p7?p.?
ei.?
si.?
?
p.?boy0x1swx1es???ran??
sb j(i7?i 7?i7?p7?p)7?i 7?i7?p7?p.sb j??
xi.?
ei.?
si.?
?
p."run0esagent 0ex?#!?
ei.?
si.?
?
p.??
?boy0x1s1run0esagent 0ex1???
?Figure 2: Derivation of a TDL semantic representation of ?A boy ran?.?he??wi7?i7?i7?p7?p.?
ei.?
si.?
?
p.re f ?x2?
[ ]?wx2es???tumbled??
sb j(i7?i7?i7?p7?p)7?i7?i7?p7?p.sb j??
xi.?
ei.?
si.?
?
p."tumble0esagent 0ex?#!?
ei.?
si.?
?
p.re f ?x2?
[ ]?tumble0e2s2agent 0e2x2?Figure 3: Derivation of TDL semantic representation of ?He tumbled?.Although the antecedent for x2 is not de-termined in this structure, the possible candi-dates can be enumerated: x1, s1 and e1, whichprecede x2.
Since TDL seamlessly representslinguistic notions such as ?entity?, ?event?
and?situation?, by indices, the anaphoric expres-sions, such as ?the event?
and ?that case?, canbe treated in the same manner.2.2 Head-driven Phrase StructureGrammarHead-driven Phrase Structure Grammar (Pollardand Sag, 1994) is a kind of lexicalized gram-mar that consists of lexical items and a smallnumber of composition rules called schema.Schemata and lexical items are all describedin typed feature structures and the unificationoperation defined thereon.?????
?PHON ?boy?SYNSEM??????
?HEAD?nounMOD h i?VAL"SUBJ h iCOMPS h iSPR hdeti#SLASH h i?????????????
(3)Figure 4 is an example of a parse tree,where the feature structures marked with thesame boxed numbers have a shared struc-ture.
In the first stage of the derivation ofthis tree, lexical items are assigned to eachof the strings, ?John?
and ?runs.?
Next, themother node, which dominates the two items,??
?PHON ?John runs?HEAD 1SUBJ h iCOMPS h i?????
?PHON ?John?HEAD nounSUBJ h iCOMPS h i???
: 2???
?PHON ?runs?HEAD verb : 1SUBJ h 2 iCOMPS h i???
?John runsFigure 4: An HPSG parse treeis generated by the application of Subject-HeadSchema.
The recursive application of these op-erations derives the entire tree.3 MethodIn this section, we present a method to de-rive TDL semantic representations from HPSGparse trees, adopting, in part, a previousmethod (Bos et al, 2004).
Basically, we firstassign TDL representations to lexical items thatare terminal nodes of a parse tree, and thencompose the TDL representation for the en-tire tree according to the tree structure (Figure5).
One problematic aspect of this approach isthat the composition process of TDL semanticrepresentations and that of HPSG parse treesare not identical.
For example, in the HPSG709?
?PHON ?John runs?HEAD 1SUBJ h iCOMPS h i?
?Subject-Head Schema* ?
e.?
s.?
?
.re f (x1) [John0x1s1]"run0esagent 0ex1?#?run_empty_+Composition Rulesnormal compositionword formationnonlocal applicationunary derivation?
?PHON ?John?HEAD nounSUBJ h iCOMPS h i??
: 2??
?PHON ?runs?HEAD verb : 1SUBJ h 2 iCOMPS h i??
?Assignment Rules?
?w.?
e.?
s.?
?
.re f (x1) [John0x1s1] [wx1es?
]?John_empty_?
* ?
sb j.sb j??
x.?
e.?
s.?
?
.
"run0esagent 0ex?#!
?run_empty_+John runs John runsFigure 5: Example of the application of the rulesparser, a compound noun is regarded as twodistinct words, whereas in TDL, a compoundnoun is regarded as one word.
Long-distancedependency is also treated differently in thetwo systems.
Furthermore, TDL has an opera-tion called unary derivation to deal with emptycategories, whereas the HPSG parser does nothave such an operation.In order to overcome these differences andrealize a straightforward composition of TDLrepresentations according to the HPSG parsetree, we defined two extended compositionrules, word formation rule and non-localapplication rule, and redefined TDL unaryderivation rules for the use in the HPSGparser.
At each step of the composition, onecomposition rule is chosen from the set ofrules, based on the information of the schemataapplied to the HPSG tree and TDL represen-tations of the constituents.
In addition, we de-fined extended TDL semantic representations,referred to as TDL Extended Structures (TD-LESs), to be paired with the extended compo-sition rules.In summary, the proposed method is com-prised of TDLESs, assignment rules, composi-tion rules, and unary derivation rules, as willbe elucidated in subsequent sections.3.1 Data StructureA TDLES is a tuple hT, p,ni, where T is anextended TDL term, which can be either aTDL term or a special value ?
.
Here, ?is a value used by the word formation rule,which indicates that the word is a word modi-fier (See Section 3.3).
In addition, p and n arethe necessary information for extended compo-sition rules, where p is a matrix predicate in Tand is used by the word formation rule, andn is a nonlocal argument, which takes eithera variable occurring in T or an empty value.This element corresponds to the SLASH fea-ture in HPSG and is used by the nonlocalapplication rule.The TDLES of the common noun ?boy?
isgiven in (4).
The contents of the structureare T , p and n, beginning at the top.
In(4), T corresponds to the TDL term of ?boy?in Figure 2, p is the predicate boy, which isidentical to a predicate in the TDL term (theidentity relation between the two is indicatedby ???).
If either T or p is changed, the otherwill be changed accordingly.
This mechanismis a part of the word formation rule, whichoffers advantages in creating a new predicatefrom multiple words.
Finally, n is an emptyvalue.
* ?
x.?
s.?
?
.??boy0xs??
?boy_empty_+(4)3.2 Assignment RulesWe define assignment rules to associate HPSGlexical items with corresponding TDLESs.
Forclosed class words, such as ?a?, ?the?
or?not?, assignment rules are given in the formof a template for each word as exemplifiedbelow.
"PHON ?a?HEAD detSPEC hnouni#?
* ?
x.?
s.?
?
.?
?
n.?w.?
e.?
s.?
?
.nx1s?wx1es??
?_empty__empty_+(5)710Shown in (5) is an assignment rule for theindefinite determiner ?a?.
The upper half of(5) shows a template of an HPSG lexical itemthat specifies its phonetic form as ?a?, wherePOS is a determiner and specifies a noun.
ATDLES is shown in the lower half of the fig-ure.
The TDL term slot of this structure isidentical to that of ?a?
in Figure 2, while slotsfor the matrix predicate and nonlocal argumentare empty.For open class words, such as nouns, verbs,adjectives, adverbs and others, assignment rulesare defined for each syntactic category.?????
?PHON PHEAD nounMOD hiSUBJ hiCOMPS hiSPR hdeti???????
* ?
x.?
s.?
?
.??P0xs??
?P_empty_+(6)The assignment rule (6) is for common nouns.The HPSG lexical item in the upper half of (6)specifies that the phonetic form of this item isa variable, P, that takes no arguments, doesnot modify other words and takes a specifier.Here, POS is a noun.
In the TDLES assignedto this item, an actual input word will be sub-stituted for the variable P, from which the ma-trix predicate P0 is produced.
Note that we canobtain the TDLES (4) by applying the rule of(6) to the HPSG lexical item of (3).As for verbs, a base TDL semantic represen-tation is first assigned to a verb root, and therepresentation is then modified by lexical rulesto reflect an inflected form of the verb.
Thisprocess corresponds to HPSG lexical rules forverbs.
Details are not presented herein due tospace limitations.3.3 Composition RulesWe define three composition rules: the func-tion application rule, the word formationrule, and the nonlocal application rule.Hereinafter, let SL = hTL, pL,nLi and SR =hTR, pR,nRi be TDLESs of the left and theright daughter nodes, respectively.
In addition,let SM be TDLESs of the mother node.Function application rule: The compositionof TDL terms in the TDLESs is performed byfunction application, in the same manner as inthe original TDL, as explained in Section 2.1.Definition 3.1 (function application rule).
IfType?TL?= ?
and Type?TR?= ?
7?
?
thenSM =* TRTLpRunion?nL,nR?+Else if Type?TL?= ?
7?
?
and Type?TR?= ?
thenSM =* TLTRpLunion?nL,nR?+In Definition 3.1, Type(T ) is a functionthat returns the type of TDL term T , andunion(nL,nR) is defined as:union?nL,nR?=????
?empty i f nL = nR = _empty_n i f nL = n, nR = _empty_n i f nL = _empty_, nR = nunde f ined i f nL 6= _empty_, nR 6= _empty_This function corresponds to the behavior ofthe union of SLASH in HPSG.
The composi-tion in the right-hand side of Figure 5 is anexample of the application of this rule.Word formation rule: In natural language,it is often the case that a new word is cre-ated by combining multiple words, for exam-ple, ?orange juice?.
This phenomenon is calledword formation.
Typed Dynamic Logic andthe HPSG parser handle this phenomenon indifferent ways.
Typed Dynamic Logic doesnot have any rule for word formation and re-gards ?orange juice?
as a single word, whereasmost parsers treat ?orange juice?
as the sepa-rate words ?orange?
and ?juice?.
This requiresa special composition rule for word formationto be defined.
Among the constituent words ofa compound word, we consider those that arenot HPSG heads as word modifiers and definetheir value for T as ?
.
In addition, we applythe word formation rule defined below.Definition 3.2 (word formation rule).
IfType?TL?= ?
thenSM =* TRconcat?pL, pR?nR+Else if Type?TR?= ?
thenSM =* TLconcat?pL, pR?nL+711concat (pL, pR) in Definition 3.2 is a func-tion that returns a concatenation of pL and pR.For example, the composition of a word mod-ifier ?orange?
(7) and and a common noun?juice?
(8) will generate the TDLES (9).?
?orange_empty_?
(7)* ?
x.?
s.?
?
.??
juice0xs???
juice_empty_+(8)* ?
x.?
s.?
?
.?
?orange_ juice0xs??
?orange_ juice_empty_+(9)Nonlocal application rule: Typed DynamicLogic and HPSG also handle the phenomenonof wh-movement differently.
In HPSG, a wh-phrase is treated as a value of SLASH, andthe value is kept until the Filler-Head Schemaare applied.
In TDL, however, wh-movementis handled by the functional composition rule.In order to resolve the difference betweenthese two approaches, we define the nonlocalapplication rule, a special rule that introducesa slot relating to HPSG SLASH to TDLESs.This slot becomes the third element of TD-LESs.
This rule is applied when the Filler-Head Schema are applied in HPSG parse trees.Definition 3.3 (nonlocal application rule).If Type?TL?= (?
7?
? )
7?
?
, Type?TR?= ?
,Type?nR?= ?
and the Filler-Head Schema are appliedin HPSG, thenSM =*TL??
nR.TR?pL_empty_+3.4 Unary Derivation RulesIn TDL, type-shifting of a word or a phrase isperformed by composition with an empty cat-egory (a category that has no phonetic form,but has syntactic/semantic functions).
For ex-ample, the phrase ?this year?
is a noun phraseat the first stage and can be changed into averb modifier when combined with an emptycategory.
Since many of the type-shifting rulesare not available in HPSG, we defined unaryderivation rules in order to provide an equiva-lent function to the type-shifting rules of TDL.These unary rules are applied independentlywith HPSG parse trees.
(10) and (11) illus-trate the unary derivation of ?this year?.
(11)Table 1: Number of implemented rulesassignment rulesHPSG-TDL template 51for closed words 16for open words 35verb lexical rules 27composition rulesbinary composition rules 3function application ruleword formation rulenonlocal application ruleunary derivation rules 12is derived from (10) using a unary derivationrule.?
?w.?
e.?
s.?
?
.re f ?x1???year0x1s1??wx1es???year_empty_?
(10)* ?
v.?
e.?
s.?
?
.re f?x1???year0x1s1?
?ves?mod 0ex1???
?year_empty_+(11)4 ExperimentThe number of rules we have implemented isshown in Table 1.
We used the Penn Treebank(Marcus, 1994) Section 22 (1,527 sentences) todevelop and evaluate the proposed method andSection 23 (2,144 sentences) as the final testset.We measured the coverage of the construc-tion of TDL semantic representations, in themanner described in a previous study (Boset al, 2004).
Although the best method forstrictly evaluating the proposed method is tomeasure the agreement between the obtainedsemantic representations and the intuitions ofthe speaker/writer of the texts, this type ofevaluation could not be performed because ofinsufficient resources.
Instead, we measuredthe rate of successful derivations as an indica-tor of the coverage of the proposed system.The sentences in the test set were parsed bya robust HPSG parser (Miyao et al, 2005),and HPSG parse trees were successfully gen-erated for 2,122 (98.9%) sentences.
The pro-posed method was then applied to these parsetrees.
Table 2 shows that 88.3% of the un-712Table 2: Coverage with respect to the test setcovered sentences 88.3 %uncovered sentences 11.7 %assignment failures 6.2 %composition failures 5.5 %word coverage 99.6 %Table 3: Error analysis: the development set# assignment failures 103# unimplemented words 61# TDL unsupporting words 17# nonlinguistic HPSG lexical items 25# composition failures 72# unsupported compositions 20# invalid assignments 36# nonlinguistic parse trees 16seen sentences are assigned TDL semantic rep-resentations.
Although this number is slightlyless than 92.3%, as reported by Bos et al,(2004), it seems reasonable to say that the pro-posed method attained a relatively high cover-age, given the expressive power of TDL.The construction of TDL semantic represen-tations failed for 11.7% of the sentences.
Weclassified the causes of the failure into twotypes.
One of which is application failure ofthe assignment rules (assignment failure); thatis, no assignment rules are applied to a num-ber of HPSG lexical items, and so no TD-LESs are assigned to these items.
The otheris application failure of the composition rules(composition failure).
In this case, a type mis-match occurred in the composition, and so aTDLES was not derived.Table 3 shows further classification of thecauses categorized into the two classes.
Wemanually investigated all of the failures in thedevelopment set.Assignment failures are caused by three fac-tors.
Most assignment failures occurred due tothe limitation in the number of the assignmentrules (as indicated by ?unimplemented words?in the table).
In this experiment, we did notimplement rules for infrequent HPSG lexicalitems.
We believe that this type of failurewill be resolved by increasing the number ofref($1)[][lecture($2,$3) &past($3) &agent($2,$1) &content($2,$4) &ref($5)[][every($6)[ball($6,$4)][see($7,$4) &present($4) &agent($7,$5) &theme($7,$6) &tremendously($7,$4) &ref($8)[][ref($9)[groove($9,$10)][be($11,$4) &present($4) &agent($11,$8) &in($11,$9) &when($11,$7)]]]]]Figure 6: Output for the sentence: ?Whenyou?re in the groove, you see every balltremendously,?
he lectured.assignment rules.
The second factor in thetable, ?TDL unsupported words?, refers to ex-pressions that are not covered by the currenttheory of TDL.
In order to resolve this type offailure, the development of TDL is required.The third factor, ?nonlinguistic HPSG lexicalitems?
includes a small number of cases inwhich TDLESs are not assigned to the wordsthat are categorized as nonlinguistic syntacticcategories by the HPSG parser.
This problemis caused by ill-formed outputs of the parser.The composition failures can be further clas-sified into three classes according to theircausative factors.
The first factor is the ex-istence of HPSG schemata for which we havenot yet implemented composition rules.
Thesefailures will be fixed by extending of the def-inition of our composition rules.
The sec-ond factor is type mismatches due to the un-intended assignments of TDLESs to lexicalitems.
We need to further elaborate the as-signment rules in order to deal with this prob-lem.
The third factor is parse trees that arelinguistically invalid.The error analysis given above indicates thatwe can further increase the coverage throughthe improvement of the assignment/compositionrules.Figure 6 shows an example of the outputfor a sentence in the development set.
Thevariables $1, .
.
.
,$11 are indices that713represent entities, events and situations.
Forexample, $3 represents a situation and $2represents the lecturing event that existsin $3.
past($3) requires that the sit-uation is past.
agent($2,$1) requiresthat the entity $1 is the agent of $2.content($2,$4) requires that $4 (as aset of possible worlds) is the content of$2.
be($11,$4) refers to $4.
Finally,every($6)[ball($6,$4)][see($7,$4)...] represents a generalized quantifier?every ball?.
The index $6 serves as anantecedent both for bound-variable anaphorawithin its scope and for E-type anaphora out-side its scope.
The entities that correspond tothe two occurrences of ?you?
are representedby $8 and $5.
Their unification is left asan anaphora resolution task that can be easilysolved by existing statistical or rule-basedmethods, given the structural information ofthe TDL semantic representation.5 ConclusionThe present paper proposed a method by whichto translate HPSG-style outputs of a robustparser (Miyao et al, 2005) into dynamic se-mantic representations of TDL (Bekki, 2000).We showed that our implementation achievedhigh coverage, approximately 90%, for realtext of the Penn Treebank corpus and that theresulting representations have sufficient expres-sive power of contemporary semantic theoryinvolving quantification, plurality, inter/intra-sentential anaphora and presupposition.In the present study, we investigated thepossibility of achieving robustness and descrip-tive adequacy of semantics.
Although previ-ously thought to have a trade-off relationship,the present study proved that robustness anddescriptive adequacy of semantics are not in-trinsically incompatible, given the transparencybetween syntax and discourse semantics.If the notion of robustness serves as a cri-terion not only for the practical usefulness ofnatural language processing but also for thevalidity of linguistic theories, then the compo-sitional transparency that penetrates all levelsof syntax, sentential semantics, and discoursesemantics, beyond the superficial difference be-tween the laws that govern each of the levels,might be reconsidered as an essential principleof linguistic theories.ReferencesTimothy Baldwin, John Beavers, Emily M. Bender,Dan Flickinger, Ara Kim and Stephan Oepen (toappear) Beauty and the Beast: What running abroad-coverage precision grammar over the BNCtaught us about the grammar ?
and the cor-pus, In Linguistic Evidence: Empirical, Theoreti-cal, and Computational Perspectives, Mouton deGruyter.Daisuke Bekki.
2000.
Typed Dynamic Logic forCompositional Grammar, Doctoral Dissertation,University of Tokyo.Daisuke Bekki.
2005.
Typed Dynamic Logic andGrammar: the Introduction, manuscript, Univer-sity of Tokyo,Johan Bos, Stephen Clark, Mark Steedman, JamesR.
Curran and Julia Hockenmaier.
2004.
Wide-Coverage Semantic Representations from a CCGParser, In Proc.
COLING ?04, Geneva.Ann Copestake, Dan Flickinger, Ivan A.
Sag andCarl Pollard.
1999.
Minimal Recursion Seman-tics: An introduction, manuscript.Ann Copestake and Dan Flickinger.
2000.An open-source grammar development environ-ment and broad-coverage English grammar usingHPSG In Proc.
LREC-2000, Athens.Jeroen Groenendijk and Martin Stokhof.
1991.
Dy-namic Predicate Logic, In Linguistics and Philos-ophy 14, pp.39-100.Julia Hockenmaier and Mark Steedman.
2002.
Ac-quiring Compact Lexicalized Grammars from aCleaner Treebank, In Proc.
LREC-2002, Las Pal-mas.Mitch Marcus.
1994.
The Penn Treebank: Arevised corpus design for extracting predicate-argument structure.
In Proceedings of the ARPAHuman Language Technolog Workshop, Prince-ton, NJ.Yusuke Miyao, Takashi Ninomiya and Jun?ichi Tsu-jii.
2005.
Corpus-oriented Grammar Develop-ment for Acquiring a Head-driven Phrase Struc-ture Grammar from the Penn Treebank, in IJC-NLP 2004, LNAI3248, pp.684-693.
Springer-Verlag.Carl Pollard and Ivan A.
Sag.
1994.
Head-DrivenPhrase Structure Grammar, Studies in Contem-porary Linguistics.
University of Chicago Press,Chicago, London.Uwe Reyle.
1993.
Dealing with Ambiguities byUnderspecification: Construction, Representationand Deduction, In Journal of Semantics 10,pp.123-179.714
