Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 87?91,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsLearning to Order Natural Language TextsJiwei Tana, b, Xiaojun Wana* and Jianguo XiaoaaInstitute of Computer Science and Technology, The MOE Key Laboratory of Computa-tional Linguistics, Peking University, ChinabSchool of Information Science and Technology, Beijing Normal University, Chinatanjiwei8@gmail.com, {wanxiaojun,jgxiao}@pku.edu.cnAbstractOrdering texts is an important task for manyNLP applications.
Most previous works onsummary sentence ordering rely on the contex-tual information (e.g.
adjacent sentences) ofeach sentence in the source document.
In thispaper, we investigate a more challenging taskof ordering a set of unordered sentences with-out any contextual information.
We introducea set of features to characterize the order andcoherence of natural language texts, and usethe learning to rank technique to determine theorder of any two sentences.
We also proposeto use the genetic algorithm to determine thetotal order of all sentences.
Evaluation resultson a news corpus show the effectiveness ofour proposed method.1 IntroductionOrdering texts is an important task in many natu-ral language processing (NLP) applications.
It istypically applicable in the text generation field,both for concept-to-text generation and text-to-text generation (Lapata, 2003), such as multipledocument summarization (MDS), question an-swering and so on.
However, ordering a set ofsentences into a coherent text is still a hard andchallenging problem for computers.Previous works on sentence ordering mainlyfocus on the MDS task (Barzilay et al, 2002;Okazaki et al, 2004; Nie et al, 2006; Ji andPulman, 2006; Madnani et al, 2007; Zhang et al,2010; He et al, 2006; Bollegala et al, 2005; Bol-legala et al, 2010).
In this task, each summarysentence is extracted from a source document.The timestamp of the source documents and theadjacent sentences in the source documents canbe used as important clues for ordering summarysentences.In this study, we investigate a more challeng-ing and more general task of ordering a set ofunordered sentences (e.g.
randomly shuffle the* Xiaojun Wan is the corresponding author.sentences in a text paragraph) without any con-textual information.
This task can be applied toalmost all text generation applications withoutrestriction.In order to address this challenging task, wefirst introduce a few useful features to character-ize the order and coherence of natural languagetexts, and then propose to use the learning torank algorithm to determine the order of two sen-tences.
Moreover, we propose to use the geneticalgorithm to decide the overall text order.
Evalu-ations are conducted on a news corpus, and theresults show the prominence of our method.
Eachcomponent technique or feature in our methodhas also been validated.2 Related WorkFor works taking no use of source document,Lapata (2003) proposed a probabilistic modelwhich learns constraints on sentence orderingfrom a corpus of texts.
Experimental evaluationindicated the importance of several learned lexi-cal and syntactic features.
However, the modelonly works well when using single feature, butunfortunately, it becomes worse when multiplefeatures are combined.
Barzilay and Lee (2004)investigated the utility of domain-specific con-tent model for representing topic and topic shiftsand the model performed well on the five se-lected domains.
Nahnsen (2009) employed fea-tures which were based on discourse entities,shallow syntactic analysis, and temporal prece-dence relations retrieved from VerbOcean.
How-ever, the model does not perform well on data-sets describing the consequences of events.3 Our Proposed Method3.1 OverviewThe task of text ordering can be modeled like(Cohen et al, 1998), as measuring the coherenceof a text by summing the association strength ofany sentence pairs.
Then the objective of a textordering model is to find a permutation whichcan maximize the summation.87Formally, we define an association strengthfunction PREF( , ) Ru v ?
to measure how strongit is that sentence u  should be arranged beforesentence v  (denoted as u v; ).
We then definefunction AGREE( ,PREF)?
as:, : ( ) ( )AGREE( ,PREF) = PREF( , )u v u vu v?
??>?
(1)where ?
denotes a sentence permutation and( ) ( )u v?
?>  means u v;  in the permutation ?
.Then the objective of finding an overall order ofthe sentences becomes finding a permutation ?to maximize AGREE( ,PREF)?
.The main framework is made up of two parts:defining a pairwise order relation and determin-ing an overall order.
Our study focuses on boththe two parts by learning a better pairwise rela-tion and proposing a better search strategy, asdescribed respectively in next sections.3.2 Pairwise Relation LearningThe goal for pairwise relation learning is defin-ing the strength function PREF for any sentencepair.
In our method we define the function PREFby combining multiple features.Method: Traditionally, there are two mainmethods for defining a strength function: inte-grating features by a linear combination (He etal., 2006; Bollegala et al, 2005) or by a binaryclassifier (Bollegala et al, 2010).
However, thebinary classification method is very coarse-grained since it considers any pair of sentenceseither ?positive?
or ?negative?.
Instead we pro-pose to use a better model of learning to rank tointegrate multiple features.In this study, we use Ranking SVM imple-mented in the svmrank toolkit (Joachims, 2002;Joachims, 2006) as the ranking model.
The ex-amples to be ranked in our ranking model aresequential sentence pairs like u v; .
The featurevalues for a training example are generated by afew feature functions ( , )if u v , and we will intro-duce the features later.
We build the training ex-amples for svmrank  as follows:For a training query, which is a paragraph withn  sequential sentences as 1 2 ... ns s s; ; ; , wecan get 2 ( 1)nA n n= ?
training examples.
Forpairs like ( 0)a a ks s k+ >;  the target rank valuesare set to n k?
, which means that the longer thedistance between the two sentences is, the small-er the target value is.
Other pairs like a k as s+ ;are all set to 0.
In order to better capture the or-der information of each feature, for every sen-tence pair u v; , we derive four feature valuesfrom each function ( , )if u v , which are listed asfollows:,1 ( , )iiV f u v=  (2),21 / 2, if ( , ) ( , ) 0( , ), otherwise( , ) ( , )i ii ii if u v f v uV f u vf u v f v u+ =?
?= ??
+?
(3),31 / if ( , ) 0( , ) / ( , ), otherwiseiy S y uii iy S y uS f u yVf u v f u y?
?
??
?
??
=?= ??????
(4),41 / if ( , ) 0( , ) / ( , ), otherwiseix S x vii ix S x vS f x vVf u v f x v?
?
??
?
??
=?= ??????
(5)where S  is the set of all sentences in a paragraphand S  is the number of sentences in S .
Thethree additional feature values of (3) (4) (5) aredefined to measure the priority of u v;  to v u; ,u v;  to { , }u y S u v?
?
?
;  and u v;  to{ , }x S u v v?
?
?
;  respectively, by calculatingthe proportion of ( , )if u v  in respective summa-tions.The learned model can be used to predict tar-get values for new examples.
A paragraph of un-ordered sentences is viewed as a test query, andthe predicted target value for u v;  is set asPREF( , )u v .Features: We select four types of features tocharacterize text coherence.
Every type of fea-tures is quantified with several functions distin-guished by i  in the formulation of ( , )if u v  andnormalized to [0,1] .
The features and definitionsof ( , )if u v  are introduced in Table 1.Type Descriptionsim( , )u vSimilaritysim(latter( ),former( ))u voverlap ( , ) / min(| |,| |)j u v u vOverlap overlap (latter( ),former( ))overlap ( , )jju vu vNumber ofcoreference chains CoreferenceNumber ofcoreference wordsNounVerbVerb & noun dependencyProbabilityModelAdjective & adverbTable 1: Features used in our model.88As in Table 1, function sim( , )u v  denotes thecosine similarity of sentence u  and v ; latter( )uand former( )v  denotes the latter half part of uand  the former part of v  respectively, which areseparated by the most centered comma (if exists)or word (if no comma exits); overlap ( , )j u v  de-notes the number of mutual words of u  and v ,for 1,2,3j =  representing lemmatized noun,verb and adjective or adverb respectively; | |u  isthe number of words of sentence u .
The valuewill be set to 0 if the denominator is 0.For the coreference features we use the ARK-ref 1  tool.
It can output the coreference chainscontaining words which represent the same entityfor two sequential sentences u v; .The probability model originates from (Lapata,2003), and we implement the model with fourfeatures of lemmatized noun, verb, adjective oradverb, and verb and noun related dependency.3.3 Overall Order DeterminationCohen et al (1998) proved finding a permutation?
to maximize AGREE( ,PREF)?
is NP-complete.
To solve this, they proposed a greedyalgorithm for finding an approximately optimalorder.
Most later works adopted the greedysearch strategy to determine the overall order.However, a greedy algorithm does not alwayslead to satisfactory results, as our experimentshows in Section 4.2.
Therefore, we propose touse the genetic algorithm (Holland, 1992) as thesearch strategy, which can lead to better results.Genetic Algorithm: The genetic algorithm(GA) is an artificial intelligence algorithm foroptimization and search problems.
The key pointof using GA is modeling the individual, fitnessfunction and three operators of crossover, muta-tion and selection.
Once a problem is modeled,the algorithm can be constructed conventionally.In our method we set a permutation ?
as anindividual encoded by a numerical path, for ex-ample a permutation 2 1 3s s s; ;  is encoded as (21 3).
Then the function AGREE( ,PREF)?
is justthe fitness function.
We adopt the order-basedcrossover operator which is described in (Davis,1985).
The mutation operator is a random inver-sion of two sentences.
For selection operator wetake a tournament selection operator which ran-domly selects two individuals to choose the onewith the greater fitness value AGREE( ,PREF)?
.1 http://www.ark.cs.cmu.edu/ARKref/After several generations of evolution, the indi-vidual with the greatest fitness value will be aclose solution to the optimal result.4 Experiments4.1 Experiment SetupData Set and Evaluation Metric: We con-ducted the experiments on the North AmericanNews Text Corpus2.
We trained the model on 80thousand paragraphs and tested with 200 shuffledparagraphs.
We use Kendall?s ?
as the evalua-tion metric, which is based on the number of in-versions in the rankings.Comparisons: It is incomparable with othermethods for summary sentence ordering basedon special summarization corpus, so we imple-mented Lapata?s probability model for compari-son, which is considered the state of the art forthis task.
In addition, we implemented a randomordering as a baseline.
We also tried to use aclassification model in place of the ranking mod-el.
In the classification model, sentence pairs like1a as s +;  were viewed as positive examples andall other pairs were viewed as negative examples.When deciding the overall order for either rank-ing or classification model we used three searchstrategies: greedy, genetic and exhaustive (orbrutal) algorithms.
In addition, we conducted aseries of experiments to evaluate the effect ofeach feature.
For each feature, we tested in twoexperiments, one of which only contained thesingle feature and the other one contained all theother features.
For comparative analysis of fea-tures, we tested with an exhaustive search algo-rithm to determine the overall order.4.2 Experiment ResultsThe comparison results in Table 2 show that ourRanking SVM based method improves the per-formance over the baselines and the classifica-tion based method with any of the search algo-rithms.
We can also see the greedy search strat-egy does not perform well and the genetic algo-rithm can provide a good approximate solution toobtain optimal results.Method Greedy Exhaustive GeneticBaseline -0.0127Probability 0.1859Classification 0.5006 0.5360 0.5264Ranking 0.5191 0.5768 0.5747Table 2: Average ?
of different methods.2 The corpus is available fromhttp://www.ldc.upenn.edu/Catalog/catalogEntry.jsp?catalogId=LDC98T3089Ranking vs.
Classification: It is not surpris-ing that the ranking model is better, becausewhen using a classification model, an exampleshould be labeled either positive or negative.
It isnot very reasonable to label a sentence pair like( 1)a a ks s k+ >;  as a negative example, nor a pos-itive one, because in some cases, it is easy toconclude one sentence should be arranged afteranother but hard to decide whether they shouldbe adjacent.
As we see in the function AGREE ,the value of PREF( , )a a ks s +  also contributes tothe summation.
In a ranking model, this informa-tion can be quantified by the different prioritiesof sentence pairs with different distances.Single Feature Effect: The effects of differ-ent types of features are shown in Table 3.
Probdenotes Lapata?s probability model with differ-ent features.Feature Only RemovedSimilarity 0.0721 0.4614Overlap 0.1284 0.4631Coreference 0.0734 0.4704Probnoun 0.3679 0.3932Probverb 0.0615 0.4544Probadjective&adverb 0.2650 0.4258Probdependency 0.2687 0.4892All 0.5768Table 3: Effects of different features.It can be seen in Table 3 that all these featurescontribute to the final result.
The two features ofnoun probability and dependency probabilityplay an important role as demonstrated in (La-pata, 2003).
Other features also improve the finalperformance.
A paragraph which is ordered en-tirely right by our method is shown in Figure 1.Sentences which should be arranged togethertend to have a higher similarity and overlap.
Likesentence (3) and (4) in Figure 1, they have ahighest cosine similarity of 0.2240 and mostoverlap words of ?Israel?
and ?nuclear?.
How-ever, the similarity or overlap of the two sen-tences does not help to decide which sentenceshould be arranged before another.
In this casethe overlap and similarity of half part of the sen-tences may help.
For example latter((3)) andformer((4)) share an overlap of ?Israel?
whilethere is no overlap for latter((4)) and former((3)).Coreference is also an important clue for or-dering natural language texts.
When we use apronoun to represent an entity, it always has oc-curred before.
For example when conductingcoreference resolution for (1) (2); , it will befound that ?He?
refers to ?Vanunu?.
Otherwisefor (2) (1); , no coreference chain will be found.4.3 Genetic AlgorithmThere are three main parameters for GA includ-ing the crossover probability (PC), the mutationprobability (PM) and the population size (PS).There is no definite selection for these parame-ters.
In our study we experimented with a widerange of parameter values to see the effect ofeach parameter.
It is hard to traverse all possiblecombinations so when testing a parameter wefixed the other two parameters.
The results areshown in Table 4.ValuePara Avg Max Min StddevPS 0.5731 0.5859 0.5606 0.0046PC 0.5733 0.5806 0.5605 0.0038PM 0.5741 0.5803 0.5337 0.0045Table 4: Results of GA with different parameters.As we can see in Table 4, when adjusting thethree parameters the average ?
values are allclose to the exhaustive result of 0.5768 and theirstandard deviations are low.
Table 4 shows thatin our case the genetic algorithm is not very sen-sible to the parameters.
In the experiments, weset PS to 30, PC to 0.5 and PM to 0.05, andreached a value of 0.5747, which is very close tothe theoretical upper bound of 0.5768.5 Conclusion and DiscussionIn this paper we propose a method for orderingsentences which have no contextual informationby making use of Ranking SVM and the geneticalgorithm.
Evaluation results demonstrate thegood effectiveness of our method.In future work, we will explore more featuressuch as semantic features to further improve theperformance.AcknowledgmentsThe work was supported by NSFC (61170166),Beijing Nova Program (2008B03) and NationalHigh-Tech R&D Program (2012AA011101).
(1) Vanunu, 43, is serving an 18-year sentence fortreason.
(2) He was kidnapped by Israel's Mossad spyagency in Rome in 1986 after giving The Sun-day Times of London photographs of the in-side of the Dimona reactor.
(3) From the photographs, experts determinedthat Israel had the world's sixth largest stock-pile of nuclear weapons.
(4) Israel has never confirmed or denied that ithas a nuclear capability.Figure 1: A right ordered paragraph.90ReferencesDanushka Bollegala, Naoaki Okazaki, Mitsuru Ishi-zuka.
2005.
A machine learning approach to sen-tence ordering for multi-document summarizationand its evaluation.
In Proceedings of the Second in-ternational joint conference on Natural LanguageProcessing (IJCNLP '05), 624-635.Danushka Bollegala, Naoaki Okazaki, and MitsuruIshizuka.
2010.
A bottom-up approach to sentenceordering for multi-document summarization.
Inf.Process.
Manage.
46, 1 (January 2010), 89-109.John H. Holland.
1992.
Adaptation in Natural andArtificial Systems: An Introductory Analysis withApplications to Biology, Control and Artificial In-telligence.
MIT Press, Cambridge, MA, USA.Lawrence Davis.
1985.
Applying adaptive algorithmsto epistatic domains.
In Proceedings of the 9th in-ternational joint conference on Artificial intelli-gence - Volume 1 (IJCAI'85), Aravind Joshi (Ed.),Vol.
1.
Morgan Kaufmann Publishers Inc., SanFrancisco, CA, USA, 162-164.Mirella Lapata.
2003.
Probabilistic text structuring:experiments with sentence ordering.
InProceedingsof the 41st Annual Meeting on Association forComputational Linguistics - Volume 1(ACL '03),Vol.
1.
Association for Computational Linguistics,Stroudsburg, PA, USA, 545-552.Naoaki Okazaki, Yutaka Matsuo, and Mitsuru Ishi-zuka.
2004.
Improving chronological sentence or-dering by precedence relation.
In Proceedings ofthe 20th international conference on Computa-tional Linguistics (COLING '04).
Association forComputational Linguistics, Stroudsburg, PA,USA, , Article 750 .Nitin Madnani, Rebecca Passonneau, Necip FazilAyan, John M. Conroy, Bonnie J. Dorr, Judith L.Klavans, Dianne P. O'Leary, and Judith D. Schle-singer.
2007.
Measuring variability in sentence or-dering for news summarization.
In Proceedings ofthe Eleventh European Workshop on Natural Lan-guage Generation (ENLG '07), Stephan Busemann(Ed.).
Association for Computational Linguistics,Stroudsburg, PA, USA, 81-88.Paul D. Ji and Stephen Pulman.
2006.
Sentence order-ing with manifold-based classification in multi-document summarization.
In Proceedings of the2006 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP '06).
Associationfor Computational Linguistics, Stroudsburg, PA,USA, 526-533.Regina Barzilay, Noemie Elhadad, and KathleenMcKeown.
2002.
Inferring strategies for sentenceordering in multidocument news summarization.Journal of Artificial Intelligence Research, 17:35?55.Regina Barzilay and Lillian Lee.
2004.
Catching thedrift: Probabilistic content models, with applica-tions to generation and summarization.
In HLT-NAACL2004: Proceedings of the Main Conference,pages 113?120.Renxian Zhang, Wenjie Li, and Qin Lu.
2010.
Sen-tence ordering with event-enriched semantics andtwo-layered clustering for multi-document newssummarization.
In Proceedings of the 23rd Interna-tional Conference on Computational Linguistics:Posters (COLING '10).
Association for Computa-tional Linguistics, Stroudsburg, PA, USA, 1489-1497.Thade Nahnsen.
2009.
Domain-independent shallowsentence ordering.
In Proceedings of Human Lan-guage Technologies: The 2009 Annual Conferenceof the North American Chapter of the Associationfor Computational Linguistics, Companion Volume:Student Research Workshop and Doctoral Consor-tium (SRWS '09).
Association for ComputationalLinguistics, Stroudsburg, PA, USA, 78-83.Thorsten Joachims.
2002.
Optimizing search enginesusing click through data.
In Proceedings of theeighth ACM SIGKDD international conference onKnowledge discovery and data mining (KDD '02).ACM, New York, NY, USA, 133-142.Thorsten Joachims.
2006.
Training linear SVMs inlinear time.
In Proceedings of the 12th ACMSIGKDD international conference on Knowledgediscovery and data mining (KDD '06).
ACM, NewYork, NY, USA, 217-226.William W. Cohen, Robert E. Schapire, and YoramSinger.
1998.
Learning to order things.
InProceed-ings of the 1997 conference on Advances in neuralinformation processing systems 10(NIPS '97), Mi-chael I. Jordan, Michael J. Kearns, and Sara A.Solla (Eds.).
MIT Press, Cambridge, MA, USA,451-457.Yanxiang He, Dexi Liu, Hua Yang, Donghong Ji,Chong Teng, and Wenqing Qi.
2006.
A hybrid sen-tence ordering strategy in multi-document summa-rization.
In Proceedings of the 7th internationalconference on Web Information Systems (WISE'06),Karl Aberer, Zhiyong Peng, Elke A. Rundensteiner,Yanchun Zhang, and Xuhui Li (Eds.).
Springer-Verlag, Berlin, Heidelberg, 339-349.Yu Nie, Donghong Ji, and Lingpeng Yang.
2006.
Anadjacency model for sentence ordering in multi-document summarization.
In Proceedings of theThird Asia conference on Information RetrievalTechnology (AIRS'06), 313-322.91
