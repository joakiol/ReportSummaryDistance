Proceedings of the Joint 5th Workshop on Statistical Machine Translation and MetricsMATR, pages 384?391,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsTaming Structured Perceptrons on Wild Feature VectorsRalf D. BrownCarnegie Mellon University Language Technologies Institute5000 Forbes Avenue, Pittsburgh PA 15213 USAralf+@cs.cmu.eduAbstractStructured perceptrons are attractive dueto their simplicity and speed, and havebeen used successfully for tuning theweights of binary features in a machinetranslation system.
In attempting to applythem to tuning the weights of real-valuedfeatures with highly skewed distributions,we found that they did not work well.
Thispaper describes a modification to the up-date step and compares the performanceof the resulting algorithm to standard min-imum error-rate training (MERT).
In ad-dition, preliminary results for combiningMERT or structured-perceptron tuning ofthe log-linear feature weights with coordi-nate ascent of other translation system pa-rameters are presented.1 IntroductionStructured perceptrons are a relatively recent(Collins, 2002) update of the classic perceptronalgorithm which permit the prediction of vec-tors of values.
Initially developed for part ofspeech taggers, they have been applied to tuningthe weights of the features in the log-linear mod-els used by statistical machine translation (Arunand Koehn, 2007), and found to have performancesimilar to the Margin-Infused Relaxed Algorithm(MIRA) by Crammer and Singer (2003; 2006) andMinimum-Error Rate Training (MERT) by Och(2003).
Parameter tuning is an important aspect ofcurrent data-driven machine translation systems,as an improper selection of feature weights candramatically reduce scores on evaluation metricssuch as BLEU (Papineni et al, 2002) or METEOR(Banerjee and Lavie, 2005).When we recently added new features to theCMU-EBMT translation system (Brown, 1996;Brown, 2008)1, in addition to splitting a number ofcomposite features into their components, our pre-vious method of parameter tuning via coordinateascent2 became impractical.
With now more than50 features partaking in the scoring model, MERTno longer seemed a good choice, as the commonwisdom is that it is not able to reliably optimizemore than about 20 features (Chiang et al, 2008).We had been using coordinate ascent because ofa need to tune a substantial number of parameterswhich are not directly part of the log-linear modelwhich can be tuned by MERT or similar methods.Our system generates a translation lattice by run-time lookup in the training corpus rather than us-ing a precomputed phrase table, so important pa-rameters include?
the size of the sample of retrieved traininginstances for a given input phrase which arealigned,?
the weight of source features for rankingtraining instances during sampling, and?
the minimum alignment score to accept atranslation instanceDecoder parameters which are important to tune,but which are generally not mentioned in the liter-ature include?
how many alternative translations of a phraseto consider during decoding,?
the size of the reordering window, and?
the rank of the language model (4-gram, 5-gram, etc.
)In addition, it is desirable to tune parameters suchas beam width to minimize translation time with-out degrading performance.1Source code for CMU-EBMT is available fromhttp://cmu-ebmt.sourceforge.net.2Coordinate ascent is described in more detail in Sec-tion 7.384As a result of the non-model parameters, a fullsystem tuning will involve multiple runs of thetuning algorithm for the feature weights, since theother parameters will affect the optimal weights.Thus, speed is an important consideration for anymethod to be used in this setting.
The structuredperceptron algorithm is ideally suited due to itsspeed, provided that it can produce competitive re-sults.2 Related WorkThe perceptron algorithm (Rosenblatt, 1958) itselfis over 50 years old, but variations such as votedand averaged perceptrons have gained popularityin the past ten years.
In particular, Collins (2002)adapted the perceptron algorithm to structuredprediction tasks such as part of speech tagging andnoun phrase chunking.
Arun and Koehn (2007)subsequently applied Collins?
structured percep-tron algorithm to the task of tuning feature weightsin a statistical machine translation system, demon-strating the extreme scalability of the algorithm byapplying it to vectors containing four to six mil-lion binary features.
However, their work left openthe question of how well structured perceptronswould deal with continuous-valued features.
Theywere unable to apply a language model due to thelack of continuous-valued features and hence hadto compare performance against a standard statis-tical machine translation (SMT) system which hadbeen stripped of its language model, with a conse-quent loss of several BLEU points in performance.During the same period, Crammer et al(2003;2006) developed a number of ?ultraconservative?learning algorithms, including MIRA, the Margin-Infused Relaxed Algorithm (which was also ap-plied to large binary feature vectors by Arun andKoehn) and variations of what they referred to asPassive-Aggressive algorithms including PA-I andPA-II.
These algorithms have in common the no-tion of updating a weight vector ?just enough?
toaccount for a new training instance which is in-correctly predicted by the existing weight vector.In contrast, the perceptron algorithm aggressivelyupdates the weight vector and relies on averagingeffects over the whole of the training set.3 Structured PerceptronsThe structured perceptron algorithm can be ap-plied to tasks where the goal is to select the bestamong competing hypotheses, where each hypoth-esis has an associated vector of feature values andthe score for a hypothesis is a linear combinationof its feature values.Beginning with a zero vector for the featureweights, the structured perceptron algorithm it-erates through each element of the training set,updating the weight vector after processing eachtraining instance.
The training set is processed re-peatedly (each pass is known as a training epoch)until convergence.
The update step is very sim-ple: if the best hypothesis according to the prod-uct of feature vector and weight vector is not thecorrect answer, add the difference between the fea-ture vectors of the correct answer and the model?sselected answer to the weight vector.Thus, the entire algorithm may be summarizedwith just two equations:~w ?
0 (1)~w ?
~w + (?oracle ?
?top1) (2)where ?x is the feature vector (?1, ?2, ..., ?n) forhypothesis x.Repeated application of Equation 2 results ina weight vector which reflects the relative impor-tance (on average) of each feature to making thecorrect selection.
Since selecting the best hypoth-esis is an arg max operation, the absolute mag-nitudes of the weights are not important.4 More Conservative Updates forStructured PerceptronsOne issue which arises in using learning algo-rithms for machine translation is that there is noone correct answer.
In addition, it may not evenbe possible for the MT system to generate thereference translation at all.
This is commonlyaddressed by using the highest-scoring (by somemetric such as BLEU) translation which the sys-tem can generate as a pseudo-oracle.Our initial implementation closely followed thedescription in (Arun and Koehn, 2007), includ-ing the refinement of using the objective-functionscore of the pseudo-oracle translation from the n-best list to modulate the learning rate of the updatestep, i.e.~w ?
~w + S?oracle ?
(?oracle ?
?top1) (3)As can be seen, the difference between Equa-tions 2 and 3 is simply the additional factor ofS?oracle .385While we initially used sentence-levelsmoothed BLEU as the objective function,we found it to perform very poorly (the full BLEUscores on the Haitian Creole tuning set were wellbelow 0.10), and instead adopted the Rouge-S(skip bigrams) metric by Lin and Och (2004a)with a maximum skip distance of four words,which was found to best correlate with humanquality judgements (Lin and Och, 2004b).In early testing, we found that both the featureweights and performance as measured by the av-erage objective score over the tuning set oscillatedwildly.
Analyzing the results, it became appar-ent that the update function was overly aggres-sive.
Unlike the binary features used in (Arunand Koehn, 2007), our continuous-valued featureshave different operating ranges for each feature,e.g.
the total distance moved as a result of reorder-ing could reach 100 on a long sentence, while theproportion of training instances with at least sixwords of adjacent context in the bilingual corpusis unlikely to exceed 0.05, even where samplingis biased toward training instances with adjacentcontext.The first attempt to address the disparity in op-erating ranges was to perform feature-wise nor-malization on the update.
Instead of taking thesimple difference in feature vectors between then-best entry with the highest log-linear score andthe one with the highest objective score, we con-struct ?diff such that?i(diff)?(?i(oracle)?
?i(top1))r2(4)wherer ?
max(0.01,maxj |?i(j)|) (5)i.e.
we estimate the operating range by finding then-best entry with the highest magnitude value ofthe feature, and then divide by the square of thatmagnitude since large feature values also magnifythe effects of weight changes.
Normalization islimited by clipping the normalization factor to beat least 0.01 so that features whose values are al-ways very near zero do not dominate the overallscore.While the feature-wise normalization didlargely control the wild swings in feature weights,it did not curb the oscillations in the objectivescores and produced only a minor improvement intuning results.We next looked at MIRA and related workon so-called Passive-Aggressive algorithms, andin particular at the update functions described in(Crammer et al, 2006).
We decided on their PA-II update rule (PA-II being akin to 1-best MIRA),with which the learning step becomes~w ?
~w + ?
?
(?oracle ?
?top1) (6)whereloss?
S?oracle ?
S?top1 (7)?
?loss||?oracle ?
?top1||2 + 12C(8)with C an ?aggressiveness?
parameter.This version of the update function producedthe desired smooth changes in feature weightsfrom iteration to iteration, though objective scoresstill do not converge.
Allowing multiple passesthrough the tuning set before re-decoding with up-dated feature weights now frequently results inweights where the pseudo-oracle is the top-rankedtranslation in 80 to 90 percent of all sentences.None of our previous experiments had achievedeven a fraction of this level due to the erratic be-havior of the feature weights.
However, as the ex-treme overfitting necessary to achieve such highrankings of the oracle translation results in poorBLEU scores, we have since used only one passover the tuning set before re-decoding with up-dated weights.5 The Final AlgorithmAfter the various attempts at taming the behav-ior of the structured perceptron approach just de-scribed, the final algorithm used for the experi-ments described below was1.
Structured perceptron, with2.
passive-aggressive updates,3.
run in semi-batch mode,4.
using sentence-level modified Rouge-S4 asthe objective functionSemi-batch mode here means that while the per-ceptron algorithm updates the weight vector af-ter each sentence, those updates are not commu-nicated to the decoder until the end of a completepass through the tuning set.
An exception is madefor the very first iteration, as it starts with uniformweights of 10?9 (rather than the conventional zero,which would cause problems with decoding).
This386permits the exact determination of the overall ob-jective score for the weight vector which is even-tually returned as the tuned optimal weights, andpermits parallelization of the decoding (though thelatter has not yet been implemented).We slightly modified the Rouge-S scoring func-tion to use the generalized F-measureF?
=(1 + ?2)?
precision?
recall?2 ?
precision+ recall(9)instead of the standard F1, allowing us to givemore weight to recall over precision by increas-ing ?
above 1.0.
This change was prompted bythe observation that the tuning process stronglyfavored shorter outputs, resulting in substantialbrevity penalties from BLEU.6 ExperimentsWe present the results of experiments on threedata sets in the next section.
The data setsare English-to-Haitian, French-to-English, andCzech-to-English.The English-to-Haitian system was built usingthe data released by Carnegie Mellon University(2010).
It consists of a medical phrasebook, aglossary, and a modest amount of newswire text,each available as a set of sentence pairs in En-glish and Haitian Creole.
For training, we usedall of the glossary, all but the last 300 phrase pairsof the medical phrasebook (these had previouslybeen used for development and testing of a ?toy?system), and the first 12,500 sentence pairs of thenewswire text.
Tuning was performed using thenext 217 sentence pairs of the newswire text, andthe test set consisted of the final 800 sentence pairsof the newswire text.
The target language modelwas built solely from the target half of the trainingcorpus, as we did not have any additional HaitianCreole text.The French-to-English system was built usingthe Europarl (Koehn, 2005) version 3 data forFrench and English.
As is usual practice, text fromthe fourth quarter of 2000 was omitted from thetraining set.
Tuning was performed using 200 sen-tences from the ?devtest2006?
file and all 2000sentences of ?test2007?
were used as the final testset.
Two target language models were built andinterpolated during decoding; the first was trainedon the target half of the bilingal corpus, and thesecond was built using the Canadian Hansards textreleased by ISI (Natural Language Group, 2001).The Czech-to-English system was built us-ing the parallel data made available for the2010 Workshop on Statistical Machine Transla-tion (WMT10).
The target language model wasbuilt from the target half of the bilingual trainingcorpus.
Tuning was performed on a 200-sentencesubset of the ?news-2008-test?
data, and all 2525sentences of the ?news-2009-test?
data were usedas unseen test data.
As these experiments werethe very first time that the CMU-EBMT systemwas applied to Czech, there are undoubtedly nu-merous pre-processing and training improvementswhich will increase scores above the values pre-sented here.Parameter tuning was performed using CMERT0.5, the reimplemented MERT program includedwith recent releases of the MOSES translationsystem (specifically, the version included withthe 2010-04-01 release), the annealing-based op-timizer included with Cunei (Phillips and Brown,2009; Phillips, 2010), and the Structured Percep-tron optimizer.
Feature weights were initializedto a uniform value of 1.0 for MERT and 10?9for annealing and Perceptron (since the usual zerocauses problems for the decoder).
Both versionsof MERT were permitted to run for 15 iterationsor until features weights converged and remained(nearly) unchanged from one iteration to the next,using merged n-best lists from the current and thethree most recent prior iterations.
Annealing wasrun with gamma values from 0.25 to 4.0, skippingthe entropy phase.
The Structured Perceptron wasallowed to run for 18 iterations and to choose theweights from the iteration which resulted in thehighest average Rouge-S score for the top trans-lation in the n-best list.
For French-English, thisproved to be the sixth iteration, while for English-Haitian it was the twelfth.
We have found that theobjective score increases for the first six to eightiterations of SP, after which it fluctuates with notrend up or down (but occasionally setting a newhigh, which is why we decided to run 18 itera-tions).For French-English, we determined the bestvalue of ?
for the Rouge-S scoring to be 1.5,and the best value of the aggressiveness parame-ter C to be 0.1, using a 40-sentence subset of theFrench-English tuning set, and then applied thosevalue for the full tuning set.
For English-Haitian,we used ?
= 1.2 and C = 0.01 (lower valuesof C provide more smoothing and overall smaller387updates, which is necessary for sparse or noisydata).
Due to limited time prior to submission, theEnglish-Haitian values for ?
and C were re-usedfor Czech, with no attempt at tuning.7 Combining Log-Linear Tuning withCoordinate AscentAs noted in the introduction, translation systemsusing SMT-style decoders incorporate various fea-tures that affect performance (and/or speed), butwhich do not contribute directly to the log-linearscoring model.
Thus, neither MERT nor the struc-tured perceptron training presented in this paper isa complete solution for parameter tuning.The CMU-EBMT system has long used a coor-dinate ascent approach to parameter tuning.
Eachparameter is varied in turn, with the MT systemperforming a translation for each setting, and thevalue which produces the best score is retainedwhile the next parameter is varied.
If the bestscoring value is the highest or lowest in the list ofvalues to be checked, the range is extended; like-wise, unless the interval between adjacent valuesis already very small, the intervals on each sideof the highest-scoring value (which is not one ofthe extremes) is divided in half and the two addi-tional points are evaluated.
This process continuesuntil convergence (cycling through all parameterswithout changing any of them) or until a pre-setmaximum number of parameter combinations isscored.
Naturally, the approach becomes sloweras the number of parameters increases, but it wasstill (barely) practical with 20 to 25 parameters.A recent change in the internals of CMU-EBMTled to a decomposition of multiple compositescores and the addition of numerous others, bal-looning the total number of tunable parameters tomore than 60.
Fortunately, most of the tunableparameters are feature weights, which can all betreated as a unit, leaving only about a dozen fea-tures for coordinate ascent.The tuning program operates by calling an eval-uation script which in turn invokes the machinetranslation on a modified configuration file pro-vided by the tuner and returns the score corre-sponding to the given parameter settings.
Whengiven an optional flag, the evaluation script firstinvokes either MERT or SP to further adjust theparameters before performing the actual evalua-tion, and modifies the given configuration file ac-cordingly.
The tuner reads the modified parame-ters from the configuration file and stores then forfurther use.Both MERT and SP can produce settings whichactually decrease the resulting BLEU score, sincethey are optimizing toward a surrogate metric.
Ifthe evaluation score after an invocation of MERTor SP is less than 0.98 times the previous bestscore, the parameter settings are rolled back; oth-erwise, the best score is set to the evaluation score.This permits MERT/SP to move the parametersto a different space if necessary, without allowingthem to substantially degrade overall scores.There was time for only one experiment involv-ing complete tuning, as summarized in Table 4.Starting with the Haitian-Creole feature weightsfound for the results in Table 1, the tuner ran-domly perturbed the non-feature-weight parame-ters by a small amount (up to 2% relative) twentytimes, then started coordinate ascent from the best-scoring of those 20 trials.
The tuner requested aMERT/SP run before ascending on the first pa-rameter, and after every fourth parameter was pro-cessed thereafter.
Because both MERT and SPstarted from previously-tuned feature weights, thenumber of iterations was reduced from 15 to 4 forMERT and from 18 to 5 for SP.
The maximumnumber of parameter combinations for coordinateascent was set to 750, which is approximately fourcycles through all parameters (the exact number ofcombinations per cycle varies, as the tuner can addnew combinations by extended the range which issearched or adding intermediate points around amaximum).In Table 4, the three different Perceptron en-tries refer to the results starting from the pre-vious experiment?s feature weights (?Perceptron1?
), starting from the results of the complete tun-ing (?Perceptron 2?
), and starting from uniformfeature weights (?Perceptron 3?).
The third runwas stopped before convergence due to the loom-ing submission deadline.8 ResultsTables 1, 2, and 3 present the results of running thetuning methods on the English-Haitian, French-English, and Czech-English data sets, respectively.Performance is shown both in terms of the time re-quired to perform a tuning run as well as the BLEUscore achieved using the resulting feature weights.Structured perceptrons are the clear winner forspeed, thanks to the simplicity of the algorithm.388Method Run-Time Iter BLEU (dev) BLEU (test) #words / ratioCMERT 0.5 73m 5 0.0993 ?new MERT 58m 3 0.0964 ?CMERT 0.51 138m 15 0.1073 0.0966 22298 / 1.213xnew MERT1 187m 15 0.1516 0.1347 17375 / 0.945xPerceptron 22m 18 0.1619 0.1534 15565 / 0.847x1 omitting several unused features, as noted in the textTable 1: English-to-Haitian tuning performanceMethod Run-Time Iter BLEU (dev) BLEU (test) #words / ratioCMERT 0.5 3h53m 15 0.12952 0.13927 100875 / 1.709xnew MERT 5h52m 15 0.22533 0.23315 60354 / 1.023xAnnealing 6h46m - 0.25017 0.25943 58518 / 0.992xPerceptron 1h23m 18 0.24214 0.26048 57408 / 0.973xTable 2: French-to-English tuning performanceWhile MERT takes two to three times as long toprocess ten random starting points as it does todecode the test set, SP is three orders of magni-tude faster than decoding.
As a result, SP tuningrequires one-third or less of the time that MERTdoes, even though we used 18 iterations of SPcompared to 15 for MERT.
Note that the time dif-ference between the two versions of MERT is inpart due to different amounts of time spent decod-ing as a result of the different feature weights.MERT unexpectedly has considerable difficultywith our new feature set, as can be seen byits much lower BLEU scores, particularly in thecase of CMERT.
An analysis of the actual fea-ture weights produced by MERT shows that itplaces nearly all of the mass on a single feature,and that the feature receiving the bulk of the masschanges from iteration to iteration.
In contrast, SPproduces BLEU scores consistent with those pro-duced by pure coordinate ascent prior to the pro-liferation of features.We believe that the difference in performancebetween the two versions of MERT is due pri-marily to the simple difference in output format:CMERT 0.5 prints its tuned weights using a fixed-point format having six digits after the decimalpoint, while the new MERT program prints us-ing scientific notation.
Because the tuned weightvector is highly skewed, most features have lowweights after L1 normalization, and thus CMERTtruncated many weights to zero (and indeed, losessignificant digits for any features assigned weightsless than 0.1), including such critical weights aslength features and language model scores.
Wesuspect that this preservation of significant digitscontributes substantially to the improved BLEUscores Bertoldi et al(2009) reported for the newimplementation compared to CMERT.The features which, at one time or another, re-ceive the bulk of the mass have one thing in com-mon: for most translations, they have a defaultvalue, and in a small proportion of cases they havea value which varies from the default by only asmall amount.
Initially, most such features hada default value of zero in CMU-EBMT, but thismeant that the line optimization in MERT had ab-solutely no constraint on raising the weight of thefeature, and thus obtaining feature vectors whereone feature has 1018 or even 1020 times the weightof any other feature.
The same problem occurswith features that are unused but have a small jit-ter in their values due to rounding errors, for ex-ample, if there are no document boundaries (as isthe case for the Haitian data described previously),the document-similarity score may be 1.000000for 99% of the arcs in the translation lattices and0.999999 for the remainder.
Offsetting the mostly-zero features so that their default value is 1 or -1(depending on the sense of the feature) and elim-inating unused features mitigated but did not en-tirely solve the problem.
In Table 1, two results areshown for both CMERT and new MERT; the firstincludes all 52 features while the second excludesfive features which are not used in a baseline-trained CMU-EBMT system.
In the former case,both programs placed all the mass on a single fea-389Method Run-Time Iter BLEU (dev) BLEU (test)new MERT 56m 15 0.0584 0.0743Perceptron 14m 18 0.0830 0.1163Table 3: Czech-to-English tuning performanceMethod Run-Time BLEU (dev) BLEU (test) length rationew MERT 48h 0.1821 0.1633 0.942Perceptron 1 25h 0.1675 0.1547 0.833Perceptron 2 38h 0.1738 0.1597 0.837Perceptron 3 12h?
0.1705 0.1647 0.939?
truncated run (see text)Table 4: English-to-Haitian tuning performance (including coordinate ascent)ture and left all the others at 10?14 or less (dis-played as 0.000000 in the case of CMERT).The full tuning runs summarized in Table 4show that SP is often competitive with MERTwhile running more quickly, but still requires fur-ther analysis to determine the causes of variabilityin its performance.
One initial conclusion fromexamining the logs of the SP runs is that weightupdates are perhaps too conservative when ap-plied in conjunction with coordinate ascent.
WhileMERT frequently shifted settings in response tochanges in the non-feature parameters, SP rarelydoes so, typically preferring to retain the exist-ing feature weights as the best setting encounteredduring the five iterations performed at each invo-cation.
The ?Perceptron 3?
run starting with smalluniform feature weights resulted from the obser-vation that a first, buggy attempt at integrationreached tuning-set BLEU scores in excess of 0.18before early termination.
The bug in question wasthat many of the feature weights were initially readin from the configuration file as zero rather thanthe correct value.As shown in the rightmost column of Tables 1,2 and 4, the Perceptron algorithm tends towardshort output, yielding translations which are about97% as long as the reference translation in French-English, a mere 85% as long for English-Haitian,and even shorter than that in two of three Czechruns.
This tendency towards short translationsprompted the inclusion of the ?
parameter ?the French-English output was originally muchshorter, but ?
has little effect on Haitian given thesparse training data.
The extremely long outputfor CMERT on French-English is due to a largenumber of zero weights, including those for lengthfeatures.9 Conclusion and Future WorkStructured perceptrons with passive-aggressiveupdates are a viable alternative to the usual MERTfeature-weight tuning, particularly where the num-ber of features exceeds that which MERT canreliably handle, or when some of the featureshave characteristics which confuse MERT.
Struc-tured perceptrons are also a good alternative wherespeed is important, such as in a hybrid tuningscheme which alternates between (re-)tuning thelog-linear model and performing coordinate ascenton parameters which do not directly contributeweight to the log-linear model.We have thus far implemented two objectivefunctions which operate on individual sentenceswithout regard for choices made on other sen-tences.
When the final evaluation metric incorpo-rates global statistics, however, an objective func-tion which takes them into account is desirable.For example, when using BLEU, it makes a bigdifference whether individual sentences are bothlonger and shorter than the reference or system-atically shorter than the reference, but these twocases can not be distinguished by single-sentenceobjective functions.
Our plan is to implement awindowed or moving-average version of BLEU asin (Chiang et al, 2008).We also plan to further speed up the tuning pro-cess by parallelizing the decoding of the sentencesin the tuning set.
As we have used a semi-batchupdate method which leaves the decoder?s weightsunchanged for an entire pass through the tuningset, there is no data dependency between individ-ual sentences, allowing them to be decoded in par-390allel.
The perceptron algorithm itself remains se-quential, but as it is three orders of magnitudefaster than the decoding, this will have negligibleimpact on overall speedup factors until hundredsof CPUs are used for simultaneous decoding.ReferencesAbhishek Arun and Phillip Koehn.
2007.
OnlineLearning Methods for Discriminative Training ofPhrase Based Statistical Machine Translation.
InProceedings of the Eleventh Machine TranslationSummit (MT Summit XI).Satanjeev Banerjee and Alon Lavie.
2005.
METEOR:An Automatic Metric for MT Evaluation with Im-proved Correlation with Human Judgements.
InProceedings of the Workshop on Intrinsic and Ex-trinsic Evaluation Measures for MT and/or Summa-rization at the 43rd Annual Meeting of the Asso-ciation for Computational Linguistics (ACL-2005),June.Nicola Bertoldi, Barry Haddow, and Jean-BaptisteFouet.
2009.
Improved Minimum Error Rate Train-ing in Moses.
The Prague Bulletin of MathematicalLinguistics, pages 1?11, February.Ralf D. Brown.
1996.
Example-Based MachineTranslation in the PANGLOSS System.
In Proceed-ings of the Sixteenth International Conference onComputational Linguistics, pages 169?174, Copen-hagen, Denmark.
http://www.cs.cmu.edu-/?ralf/papers.html.Ralf D. Brown.
2008.
Exploiting Document-Level Context for Data-Driven Machine Trans-lation.
In Proceedings of the Eighth Con-ference of the Association for Machine Trans-lation in the Americas (AMTA-2008), Octo-ber.
http://www.amtaweb.org/papers/-2.02 Brown.pdf.Carnegie Mellon University.
2010.
Public release ofhaitian-creole language data, January.
http://-www.speech.cs.cmu.edu/haitian/text.David Chiang, Yuval Marton, and Philis Resnik.
2008.Online Large-Margin Training of Syntactic andStructural Translation Features.
In Proceedings ofthe Conference on Empirical Methods in NaturalLangauge Processing (EMNLP-2008), pages 224?233, October.Michael Collins.
2002.
Discriminative TrainingMethods for Hidden Markov Models: Theory andExperiments with Perceptron Algorithms.
In Pro-ceedings of EMNLP-2002.
http://people.-csail.mit.edu/mcollins/papers/-tagperc.pdf.Koby Crammer, Ofer Deke, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
OnlinePassive-Aggressive Algorithms.
The Journal of Ma-chine Learning Research, 7:551?585, December.Koby Cranmer and Yoram Singer.
2003.
Ultraconser-vative Online Algorithms for Multiclass Problems.The Journal of Machine Learning Research, 3:951?991, March.Philipp Koehn.
2005.
Europarl: A Parallel Corpus forStatistical Machine Translation.
In Proceedings ofthe Tenth Machine Translation Summit (MT SummixX), pages 79?86.Chin-Yew Lin and Franz Joseph Och.
2004a.
Au-tomatic Evaluation of Machine Translation Qual-ity using Longest Common Subsequence and Skip-Bigram Statistics.
In Proceedings of ACL-2004.Chin-Yew Lin and Franz Joseph Och.
2004b.
OR-ANGE: A Method for Evaluating Automatic Evalu-ation Metrics for Machine Translation.
In Proceed-ings of the 20th International Conference on Com-putational Linguistics (COLING 2004).USC Information Sciences Institute Natural LanguageGroup.
2001.
Aligned Hansards of the 36th Par-liament of Canada, Release 2001-1a.
http://-www.isi.edu/natural-language/-download/hansard/.Franz Josef Och.
2003.
Minimum Error Rate Trainingin Statistical Machine Translation.
In Proceedingsof the 41st Meeting of the Association for Computa-tional Linguistics (ACL-2003), Sapporo, Japan, July6?7.Kishore Papineni, Salim Roukos, Todd Ward, andWei-Jing Zhu.
2002.
BLEU: a Method for Au-tomatic Evaluation of Machine Translation.
InProceedings of the 40th Annual Meeting of theAssociation for Computational Linguistics, July.http://acl.ldc.upenn.edu/P/P02/.Aaron B. Phillips and Ralf D. Brown.
2009.
CuneiMachine Translation Platform: System Descrip-tion.
In Proceedings of the Third Workshop onExample-Based Machine Translation, Dublin, Ire-land, November.Aaron B. Phillips.
2010.
The Cunei Machine Trans-lation Platform for WMT?10.
In Proceedings of theACL 2010 Joint Fifth Workshop on Statistical Ma-chine Translation and Metrics MATR, July.F.
Rosenblatt.
1958.
The Perceptron: A ProbabilisticModel for Information Storage and Organization inthe Brain.
Psychological Review, 65:386?408.391
