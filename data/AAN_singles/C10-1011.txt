Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 89?97,Beijing, August 2010Very High Accuracy and Fast Dependency Parsing is not a ContradictionBernd BohnetUniversity of StuttgartInstitut fu?r Maschinelle Sprachverarbeitungbernd.bohnet@ims.uni-stuttgart.deAbstractIn addition to a high accuracy, short pars-ing and training times are the most impor-tant properties of a parser.
However, pars-ing and training times are still relativelylong.
To determine why, we analyzed thetime usage of a dependency parser.
We il-lustrate that the mapping of the featuresonto their weights in the support vectormachine is the major factor in time com-plexity.
To resolve this problem, we im-plemented the passive-aggressive percep-tron algorithm as a Hash Kernel.
TheHash Kernel substantially improves theparsing times and takes into account thefeatures of negative examples built dur-ing the training.
This has lead to a higheraccuracy.
We could further increase theparsing and training speed with a paral-lel feature extraction and a parallel parsingalgorithm.
We are convinced that the HashKernel and the parallelization can be ap-plied successful to other NLP applicationsas well such as transition based depen-dency parsers, phrase structrue parsers,and machine translation.1 IntroductionHighly accurate dependency parsers have high de-mands on resources and long parsing times.
Thetraining of a parser frequently takes several daysand the parsing of a sentence can take on averageup to a minute.
The parsing time usage is impor-tant for many applications.
For instance, dialogsystems only have a few hundred milliseconds toanalyze a sentence and machine translation sys-tems, have to consider in that time some thousandtranslation alternatives for the translation of a sen-tence.Parsing and training times can be improvedby methods that maintain the accuracy level, ormethods that trade accuracy against better parsingtimes.
Software developers and researchers areusually unwilling to reduce the quality of their ap-plications.
Consequently, we have to consider atfirst methods to improve a parser, which do not in-volve an accuracy loss, such as faster algorithms,faster implementation of algorithms, parallel al-gorithms that use several CPU cores, and featureselection that eliminates the features that do notimprove accuracy.We employ, as a basis for our parser, the secondorder maximum spanning tree dependency pars-ing algorithm of Carreras (2007).
This algorithmfrequently reaches very good, or even the best la-beled attachment scores, and was one of the mostused parsing algorithms in the shared task 2009of the Conference on Natural Language Learning(CoNLL) (Hajic?
et al, 2009).
We combined thisparsing algorithm with the passive-aggressive per-ceptron algorithm (Crammer et al, 2003; McDon-ald et al, 2005; Crammer et al, 2006).
A parserbuild out of these two algorithms provides a goodbaseline and starting point to improve upon theparsing and training times.The rest of the paper is structured as follows.
InSection 2, we describe related work.
In section 3,we analyze the time usage of the components of89the parser.
In Section 4, we introduce a new Ker-nel that resolves some of the bottlenecks and im-proves the performance.
In Section 5, we describethe parallel parsing algorithms which nearly al-lowed us to divide the parsing times by the num-ber of cores.
In Section 6, we determine the opti-mal setting for the Non-Projective ApproximationAlgorithm.
In Section 7, we conclude with a sum-mary and an outline of further research.2 Related WorkThe two main approaches to dependency parsingare transition based dependency parsing (Nivre,2003; Yamada and Matsumoto., 2003; Titov andHenderson, 2007) and maximum spanning treebased dependency parsing (Eisner, 1996; Eisner,2000; McDonald and Pereira, 2006).
Transitionbased parsers typically have a linear or quadraticcomplexity (Nivre et al, 2004; Attardi, 2006).Nivre (2009) introduced a transition based non-projective parsing algorithm that has a worst casequadratic complexity and an expected linear pars-ing time.
Titov and Henderson (2007) combineda transition based parsing algorithm, which used abeam search with a latent variable machine learn-ing technique.Maximum spanning tree dependency basedparsers decomposes a dependency structure intoparts known as ?factors?.
The factors of the firstorder maximum spanning tree parsing algorithmare edges consisting of the head, the dependent(child) and the edge label.
This algorithm has aquadratic complexity.
The second order parsingalgorithm of McDonald and Pereira (2006) uses aseparate algorithm for edge labeling.
This algo-rithm uses in addition to the first order factors: theedges to those children which are closest to the de-pendent.
The second order algorithm of Carreras(2007) uses in addition to McDonald and Pereira(2006) the child of the dependent occurring in thesentence between the head and the dependent, andthe an edge to a grandchild.
The edge labeling isan integral part of the algorithm which requiresan additional loop over the labels.
This algorithmtherefore has a complexity of O(n4).
Johanssonand Nugues (2008) reduced the needed number ofloops over the edge labels by using only the edgesthat existed in the training corpus for a distincthead and child part-of-speech tag combination.The transition based parsers have a lower com-plexity.
Nevertheless, the reported run times inthe last shared tasks were similar to the maxi-mum spanning tree parsers.
For a transition basedparser, Gesmundo et al (2009) reported run timesbetween 2.2 days for English and 4.7 days forCzech for the joint training of syntactic and se-mantic dependencies.
The parsing times wereabout one word per second, which speeds upquickly with a smaller beam-size, although the ac-curacy of the parser degrades a bit.
Johansson andNugues (2008) reported training times of 2.4 daysfor English with the high-order parsing algorithmof Carreras (2007).3 Analysis of Time UsageWe built a baseline parser to measure the time us-age.
The baseline parser resembles the architec-ture of McDonald and Pereira (2006).
It consistsof the second order parsing algorithm of Carreras(2007), the non-projective approximation algo-rithm (McDonald and Pereira, 2006), the passive-aggressive support vector machine, and a featureextraction component.
The features are listed inTable 4.
As in McDonald et al (2005), the parserstores the features of each training example in afile.
In each epoch of the training, the featurefile is read, and the weights are calculated andstored in an array.
This procedure is up to 5 timesfaster than computing the features each time anew.But the parser has to maintain large arrays: forthe weights of the sentence and the training file.Therefore, the parser needs 3GB of main memoryfor English and 100GB of disc space for the train-ing file.
The parsing time is approximately 20%faster, since some of the values did not have to berecalculated.Algorithm 1 illustrates the training algorithm inpseudo code.
?
is the set of training exampleswhere an example is a pair (xi, yi) of a sentenceand the corresponding dependency structure.
?
?wand ?
?v are weight vectors.
The first loop ex-tracts features from the sentence xi and maps thefeatures to numbers.
The numbers are groupedinto three vectors for the features of all possibleedges ?h,d, possible edges in combination withsiblings ?h,d,s and in combination with grandchil-90te+s tr tp ta rest total te pars.
train.
sent.
feat.
LAS UASChinese 4582 748 95 - 3 846 3298 3262 84h 22277 8.76M 76.88 81.27English 1509 168 12.5 20 1.5 202 1223 1258 38.5h 39279 8.47M 90.14 92.45German 945 139 7.7 17.8 1.5 166 419 429 26.7h 36020 9.16M 87.64 90.03Spanish 3329 779 36 - 2 816 2518 2550 16.9h 14329 5.51M 86.02 89.54Table 1: te+s is the elapsed time in milliseconds to extract and store the features, tr to read the featuresand to calculate the weight arrays, tp to predict the projective parse tree, ta to apply the non-projectiveapproximation algorithm, rest is the time to conduct the other parts such as the update function, train.
isthe total training time per instance (tr + tp + ta+rest ), and te is the elapsed time to extract the features.The next columns illustrate the parsing time in milliseconds per sentence for the test set, training timein hours, the number of sentences in the training set, the total number of features in million, the labeledattachment score of the test set, and the unlabeled attachment score.Algorithm 1: Training ?
baseline algorithm?
= {(xi, yi)}Ii=1 // Training data?
?w = 0,?
?v = 0?
= E ?
I // passive-aggresive update weightfor i = 1 to Itss+e; extract-and-store-features(xi); tes+e;for n = 1 to E // iteration over the training epochsfor i = 1 to I // iteration over the training examplesk ?
(n?
1) ?
I + i?
= E ?
I ?
k + 2 // passive-aggressive weighttsr,k; A = read-features-and-calc-arrays(i,?
?w ) ; ter,ktsp,k; yp = predicte-projective-parse-tree(A);tep,ktsa,k; ya = non-projective-approx.
(yp ,A); tea,kupdate ?
?w , ?
?v according to ?
(yp, yi) and ?w = v/(E ?
I) // averagedren ?h,d,g where h, d, g, and s are the indexesof the words included in xi.
Finally, the methodstores the feature vectors on the hard disc.The next two loops build the main part of thetraining algorithm.
The outer loop iterates overthe number of training epochs, while the innerloop iterates over all training examples.
The on-line training algorithm considers a single trainingexample in each iteration.
The first function in theloop reads the features and computes the weightsA for the factors in the sentence xi.
A is a set ofweight arrays.A = {?
?w ?
?
?f h,d,?
?w ??
?f h,d,s,?
?w ??
?f h,d,g}The parsing algorithm uses the weight arraysto predict a projective dependency structure yp.The non-projective approximation algorithm hasas input the dependency structure and the weightarrays.
It rearranges the edges and tries to in-crease the total score of the dependency structure.This algorithm builds a dependency structure ya,which might be non-projective.
The training al-gorithm updates ?
?w according to the differencebetween the predicted dependency structures yaand the reference structure yi.
It updates ?
?v aswell, whereby the algorithm additionally weightsthe updates by ?.
Since the algorithm decreases?
in each round, the algorithm adapts the weightsmore aggressively at the beginning (Crammer etal., 2006).
After all iterations, the algorithm com-putes the average of ?
?v , which reduces the effectof overfitting (Collins, 2002).We have inserted into the training algorithmfunctions to measure the start times ts and theend times te for the procedures to compute andstore the features, to read the features, to pre-dict the projective parse, and to calculate the non-projective approximation.
We calculate the aver-age elapsed time per instance, as the average overall training examples and epochs:tx =?E?Ik=1 tex,k?tsx,kE?I .We use the training set and the test set of theCoNLL shared task 2009 for our experiments.
Ta-ble 1 shows the elapsed times in 11000 seconds(milliseconds) of the selected languages for theprocedure calls in the loops of Algorithm 1.
Wehad to measure the times for the feature extractionin the parsing algorithm, since in the training al-gorithm, the time can only be measured togetherwith the time for storing the features.
The tablecontains additional figures for the total trainingtime and parsing scores.1The parsing algorithm itself only required, toour surprise, 12.5 ms (tp) for a English sentence1We use a Intel Nehalem i7 CPU 3.33 Ghz.
With turbomode on, the clock speed was 3.46 Ghz.91on average, while the feature extraction needs1223 ms. To extract the features takes about100 times longer than to build a projective depen-dency tree.
The feature extraction is already im-plemented efficiently.
It uses only numbers to rep-resent features which it combines to a long integernumber and then maps by a hash table2 to a 32bitinteger number.
The parsing algorithm uses theinteger number as an index to access the weightsin the vectors ?
?w and ?
?v .The complexity of the parsing algorithm is usu-ally considered the reason for long parsing times.However, it is not the most time consuming com-ponent as proven by the above analysis.
There-fore, we investigated the question further, askingwhat causes the high time consumption of the fea-ture extraction?In our next experiment, we left out the mappingof the features to the index of the weight vectors.The feature extraction takes 88 ms/sentence with-out the mapping and 1223 ms/sentence with themapping.
The feature?index mapping needs 93%of the time to extract the features and 91% of thetotal parsing time.
What causes the high time con-sumption of the feature?index mapping?The mapping has to provide a number as an in-dex for the features in the training examples and tofilter out the features of examples built, while theparser predicts the dependency structures.
The al-gorithm filters out negative features to reduce thememory requirement, even if they could improvethe parsing result.
We will call the features builtdue to the training examples positive features andthe rest negative features.
We counted 5.8 timesmore access to negative features than positive fea-tures.We now look more into the implementation de-tails of the used hash table to answer the pre-viously asked question.
The hash table for thefeature?index mapping uses three arrays: one forthe keys, one for the values and a status array toindicate the deleted elements.
If a program storesa value then the hash function uses the key to cal-culate the location of the value.
Since the hashfunction is a heuristic function, the predicted lo-cation might be wrong, which leads to so-called2We use the hash tables of the trove library:http://sourceforge.net/projects/trove4j.hash misses.
In such cases the hash algorithmhas to retry to find the value.
We counted 87%hash misses including misses where the hash hadto retry several times.
The number of hash misseswas high, because of the additional negative fea-tures.
The CPU cache can only store a smallamount of the data from the hash table.
Therefore,the memory controller has frequently to transferdata from the main memory into the CPU.
Thisprocedure is relatively slow.
We traced down thehigh time consumption to the access of the keyand the access of the value.
Successive accessesto the arrays are fast, but the relative random ac-cesses via the hash function are very slow.
Thelarge number of accesses to the three arrays, be-cause of the negative features, positive featuresand because of the hash misses multiplied by thetime needed to transfer the data into the CPU arethe reason for the high time consumption.We tried to solve this problem with Bloom fil-ters, larger hash tables and customized hash func-tions to reduce the hash misses.
These techniquesdid not help much.
However, a substantial im-provement did result when we eliminated the hashtable completely, and directly accessed the weightvectors ?
?w and ?
?v with a hash function.
This ledus to the use of Hash Kernels.4 Hash KernelA Hash Kernel for structured data uses a hashfunction h : J ?
{1...n} to index ?, cf.
Shi etal.
(2009).
?
maps the observations X to a fea-ture space.
We define ?
(x, y) as the numeric fea-ture representation indexed by J .
Let ?k(x, y) =?j(x, y) the hash based feature?index mapping,where h(j) = k. The process of parsing a sen-tence xi is to find a parse tree yp that maximizesa scoring function argmaxyF (xi, y).
The learningproblem is to fit the function F so that the errorsof the predicted parse tree y are as low as possible.The scoring function of the Hash Kernel isF (x, y) = ?
?w ?
?
(x, y)where ?
?w is the weight vector and the size of ?
?w isn.Algorithm 2 shows the update function of theHash Kernel.
We derived the update functionfrom the update function of MIRA (Crammer et92Algorithm 2: Update of the Hash Kernel// yp = arg maxyF (xi, y)update(??w,?
?v , xi, yi, yp, ?)?
= ?
(yi, yp) // number of wrong labeled edgesif ?
> 0 then?
?u ?
(?
(xi, yi)?
?
(xi, yp))?
= ??
(F (xt,yi)?F (xi,yp))||?
?u ||2?
?w ?
?
?w + ?
?
??u?
?v ?
~v + ?
?
?
?
?
?ureturn ?
?w , ?
?val., 2006).
The parameters of the function arethe weight vectors ?
?w and ?
?v , the sentence xi,the gold dependency structure yi, the predicteddependency structure yp, and the update weight?.
The function ?
calculates the number ofwrong labeled edges.
The update function up-dates the weight vectors, if at least one edge is la-beled wrong.
It calculates the difference ?
?u of thefeature vectors of the gold dependency structure?
(xi, yi) and the predicted dependency structure?
(xi, yp).
Each time, we use the feature represen-tation ?, the hash function h maps the features tointeger numbers between 1 and |?
?w |.
After thatthe update function calculates the margin ?
andupdates ?
?w and ?
?v respectively.Algorithm 3 shows the training algorithm forthe Hash Kernel in pseudo code.
A main dif-ference to the baseline algorithm is that it doesnot store the features because of the required timewhich is needed to store the additional negativefeatures.
Accordingly, the algorithm first extractsthe features for each training instance, then mapsthe features to indexes for the weight vector withthe hash function and calculates the weight arrays.Algorithm 3: Training ?
Hash Kernelfor n?
1 to E // iteration over the training epochsfor i?
1 to I // iteration over the training exmaplesk ?
(n?
1) ?
I + i?
?
E ?
I ?
k + 2 // passive-aggressive weighttse,k; A?
extr.-features-&-calc-arrays(i,?
?w ) ; tee,ktsp,k; yp?
predicte-projective-parse-tree(A);tep,ktsa,k; ya?
non-projective-approx.
(yp ,A); tea,kupdate ?
?w , ?
?v according to ?
(yp, yi) and ?w = v/(E ?
I) // averageFor different j, the hash function h(j) mightgenerate the same value k. This means that thehash function maps more than one feature to thesame weight.
We call such cases collisions.
Col-lisions can reduce the accuracy, since the weightsare changed arbitrarily.
This procedure is similarto randomization of weights (features), whichaims to save space by sharing values in the weightvector (Blum., 2006; Rahimi and Recht, 2008).The Hash Kernel shares values when collisionsoccur that can be considered as an approximationof the kernel function, because a weight mightbe adapted due to more than one feature.
If theapproximation works well then we would needonly a relatively small weight vector otherwisewe need a larger weight vector to reduce thechance of collisions.
In an experiments, wecompared two hash functions and different hashsizes.
We selected for the comparison a standardhash function (h1) and a custom hash function(h2).
The idea for the custom hash function h2 isnot to overlap the values of the feature sequencenumber and the edge label with other values.These values are stored at the beginning of a longnumber, which represents a feature.h1 ?
|(l xor(l ?
0xffffffff00000000 >> 32))% size|3h2 ?
|(l xor ((l >> 13) ?
0xffffffffffffe000) xor((l >> 24) ?
0xffffffffffff0000) xor((l >> 33) ?
0xfffffffffffc0000) xor((l >> 40) ?
0xfffffffffff00000)) % size |vector size h1 #(h1) h2 #(h2)411527 85.67 0.41 85.74 0.413292489 87.82 3.27 87.97 3.2810503061 88.26 8.83 88.35 8.7721006137 88.19 12.58 88.41 12.5342012281 88.32 12.45 88.34 15.27115911564?
88.32 17.58 88.39 17.34179669557 88.34 17.65 88.28 17.84Table 2: The labeled attachment scores for differ-ent weight vector sizes and the number of nonzerovalues in the feature vectors in millions.
?
Not aprime number.Table 2 shows the labeled attachment scores forselected weight vector sizes and the number ofnonzero weights.
Most of the numbers in Table2 are primes, since they are frequently used to ob-tain a better distribution of the content in hash ta-3>> n shifts n bits right, and % is the modulo operation.93bles.
h2 has more nonzero weights than h1.
Nev-ertheless, we did not observe any clear improve-ment of the accuracy scores.
The values do notchange significantly for a weight vector size of 10million and more elements.
We choose a weightvector size of 115911564 values for further exper-iments since we get more non zero weights andtherefore fewer collisions.te tp ta r total par.
trai.Chinese 1308 - 200 3 1511 1184 93hEnglish 379 21.3 18.2 1.5 420 354 46hGerman 209 12 15.3 1.7 238 126 24hSpanish 1056 - 39 2 1097 1044 44hTable 3: The time in milliseconds for the featureextraction, projective parsing, non-projective ap-proximation, rest (r), the total training time perinstance, the average parsing (par.)
time in mil-liseconds for the test set and the training time inhours01230 5000 10000 15000SpanishFigure 1: The difference of the labeled attachmentscore between the baseline parser and the parserwith the Hash Kernel (y-axis) for increasing largetraining sets (x-axis).Table 3 contains the measured times for theHash Kernel as used in Algorithm 2.
The parserneeds 0.354 seconds in average to parse a sen-tence of the English test set.
This is 3.5 timesfaster than the baseline parser.
The reason for thatis the faster feature mapping of the Hash Kernel.Therefore, the measured time te for the feature ex-traction and the calculation of the weight arraysare much lower than for the baseline parser.
Thetraining is about 19% slower since we could nolonger use a file to store the feature indexes ofthe training examples because of the large numberof negative features.
We counted about twice thenumber of nonzero weights in the weight vector ofthe Hash Kernel compared to the baseline parser.For instance, we counted for English 17.34 Mil-lions nonzero weights in the Hash Kernel and 8.47Millions in baseline parser and for Chinese 18.28Millions nonzero weights in the Hash Kernel and8.76 Millions in the baseline parser.
Table 6 showsthe scores for all languages of the shared task2009.
The attachment scores increased for all lan-guages.
It increased most for Catalan and Span-ish.
These two corpora have the smallest trainingsets.
We searched for the reason and found thatthe Hash Kernel provides an overproportional ac-curacy gain with less training data compared toMIRA.
Figure 1 shows the difference between thelabeled attachment score of the parser with MIRAand the Hash Kernel for Spanish.
The decreasingcurve shows clearly that the Hash Kernel providesan overproportional accuracy gain with less train-ing data compared to the baseline.
This providesan advantage for small training corpora.However, this is probably not the main rea-son for the high improvement, since for languageswith only slightly larger training sets such as Chi-nese the improvement is much lower and the gra-dient at the end of the curve is so that a hugeamount of training data would be needed to makethe curve reach zero.5 ParallelizationCurrent CPUs have up to 12 cores and we willsee soon CPUs with more cores.
Also graphiccards provide many simple cores.
Parsing algo-rithms can use several cores.
Especially, the tasksto extract the features and to calculate the weightarrays can be well implemented as parallel algo-rithm.
We could also successful parallelize theprojective parsing and the non-projective approx-imation algorithm.
Algorithm 4 shows the paral-lel feature extraction in pseudo code.
The mainmethod prepares a list of tasks which can be per-formed in parallel and afterwards it creates thethreads that perform the tasks.
Each thread re-moves from the task list an element, carries outthe task and stores the result.
This procedure isrepeated until the list is empty.
The main methodwaits until all threads are completed and returnsthe result.
For the parallel algorithms, Table 5shows the elapsed times depend on the number of94# Standard Features # Linear Features Linear G. Features Sibling Features1 l,hf ,hp,d(h,d) 14 l,hp,h+1p,dp,d(h,d) 44 l,gp,dp,d+1p,d(h,d) 99 l,sl,hp,d(h,d)?r(h,d)2 l,hf ,d(h,d) 15 l,hp,d-1p,dp,d(h,d) 45 l,gp,dp,d-1p,d(h,d) 100 l,sl,dp,d(h,d)?r(h,d)3 l,hp,d(h,d) 16 l,hp,dp,d+1p,d(h,d) 46 l,gp,g+1p,d-1p,dp,d(h,d) 101 l,hl,dp,d(h,d)?r(h,d)4 l,df ,dp,d(h,d) 17 l,hp,h+1p,d-1p,dp,d(h,d) 47 l,g-1p,gp,d-1p,dp,d(h,d) 102 l,dl,sp,d(h,d)?r(h,d)5 l,hp,d(h,d) 18 l,h-1p,h+1p,d-1p,dp,d(h,d) 48 l,gp,g+1p,dp,d+1p,d(h,d) 75 l,?dm,?sm,d(h,d)6 l,dp,d(h,d) 19 l,hp,h+1p,dp,d+1p,d(h,d) 49 l,g-1p,gp,dp,d+1p,d(h,d) 76 l,?hm,?sm,d(h,s)7 l,hf ,hp,df ,dp,d(h,d) 20 l,h-1p,hp,dp,d-1p,d(h,d) 50 l,gp,g+1p,hp,d(h,d) Linear S. Features8 l,hp,df ,dp,d(h,d) Grandchild Features 51 l,gp,g-1p,hp,d(h,d) 58 l,sp,s+1p,hp,d(h,d)9 l,hf ,df ,dp,d(h,d) 21 l,hp,dp,gp,d(h,d,g) 52 l,gp,hp,h+1p,d(h,d) 59 l,sp,s-1p,hp,d(h,d)10 l,hf ,hp,df ,d(h,d) 22 l,hp,gp,d(h,d,g) 53 l,gp,hp,h-1p,d(h,d) 60 l,sp,hp,h+1p,d(h,d)11 l,hf ,df ,hp,d(h,d) 23 l,dp,gp,d(h,d,g) 54 l,gp,g+1p,h-1p,hp,d(h,d) 61 l,sp,hp,h-1p,d(h,d)12 l,hf ,df ,d(h,d) 24 l,hf ,gf ,d(h,d,g) 55 l,g-1p,gp,h-1p,hp,d(h,d) 62 l,sp,s+1p,h-1p,d(h,d)13 l,hp,dp,d(h,d) 25 l,df ,gf ,d(h,d,g) 56 l,gp,g+1p,hp,h+1p,d(h,d) 63 l,s-1p,sp,h-1p,d(h,d)77 l,hl,hp,d(h,d) 26 l,gf ,hp,d(h,d,g) 57 l,g-1p,gp,hp,h+1p,d(h,d) 64 l,sp,s+1p,hp,d(h,d)78 l,hl,d(h,d) 27 l,gf ,dp,d(h,d,g) Sibling Features 65 l,s-1p,sp,hp,h+1p,d(h,d)79 l,hp,d(h,d) 28 l,hf ,gp,d(h,d,g) 30 l,hp,dp,sp,d(h,d) ?r(h,d) 66 l,sp,s+1p,dp,d(h,d)80 l,dl,dp,d(h,d) 29 l,df ,gp,d(h,d,g) 31 l,hp,sp,d(h,d)?r(h,d) 67 l,sp,s-1p,dp,d(h,d)81 l,dl,d(h,d) 91 l,hl,gl,d(h,d,g) 32 l,dp,sp,d(h,d)?r(h,d) 68 sp,dp,d+1p,d(h,d)82 l,dp,d(h,d) 92 l,dp,gp,d(h,d,g) 33 l,pf ,sf ,d(h,d)?r(h,d) 69 sp,dp,d-1p,d(h,d)83 l,dl,hp,dp,hl,d(h,d) 93 l,gl,hp,d(h,d,g) 34 l,pp,sf ,d(h,d)?r(h,d) 70 sp,s+1p,d-1p,dp,d(h,d)84 l,dl,hp,dp,d(h,d) 94 l,gl,dp,d(h,d,g) 35 l,sf ,pp,d(h,d)?r(h,d) 71 s-1p,sp,d-1p,dp,d(h,d)85 l,hl,dl,dp,d(h,d) 95 l,hl,gp,d(h,d,g) 36 l,sf ,dp,d(h,d)?r(h,d) 72 sp,s+1p,dp,d+1p,d(h,d)86 l,hl,hp,dp,d(h,d) 96 l,dl,gp,d(h,d,g) 37 l,sf ,dp,d(h,d)?r(h,d) 73 s-1p,sp,dp,d+1p,d(h,d)87 l,hl,dl,hp,d(h,d) 74 l,?dm,?gm,d(h,d) 38 l,df ,sp,d(h,d)?r(h,d) Special Feature88 l,hl,dl,d(h,d) Linear G. Features 97 l,hl,sl,d(h,d)?r(h,d) 39 ?l,hp,dp,xpbetween h,d89 l,hp,dp,d(h,d) 42 l,gp,g+1p,dp,d(h,d) 98 l,dl,sl,d(h,d)?r(h,d)41 l,?hm,?dm,d(h,d) 43 l,gp,g-1p,dp,d(h,d)Table 4: Features Groups.
l represents the label, h the head, d the dependent, s a sibling, and g agrandchild, d(x,y,[,z]) the order of words, and r(x,y) the distance.used cores.
The parsing time is 1.9 times fasteron two cores and 3.4 times faster on 4 cores.
Hy-per threading can improve the parsing times againand we get with hyper threading 4.6 faster parsingtimes.
Hyper threading possibly reduces the over-head of threads, which contains already our singlecore version.Algorithm 4: Parallel Feature ExtractionA // weight arraysextract-features-and-calc-arrays(xi)data-list?
{} // thread-save data listfor w1 ?
1 to |xi|for w2 ?
1 to |xi|data-list?
data-list ?
{(w1, w2)}c?
number of CPU coresfor t?
1 to cTt ?
create-array-thread(t, xi,data-list)start array-thread Tt// start thread tfor t?
1 to cjoin Tt// wait until thread t is finishedA?
A ?
collect-result(Tt)return A//array-thread Td?
remove-first-element(data-list)if d is empty then end-thread... // extract features and calculate part d of ACores te tp ta rest total pars.
train.1 379 21.3 18.2 1.5 420 354 45.8h2 196 11.7 9.2 2.1 219 187 23.9h3 138 8.9 6.5 1.6 155 126 16.6h4 106 8.2 5.2 1.6 121 105 13.2h4+4h 73.3 8.8 4.8 1.3 88.2 77 9.6hTable 5: Elapsed times in milliseconds for differ-ent numbers of cores.
The parsing time (pars.
)are expressed in milliseconds per sentence andthe training (train.)
time in hours.
The last rowshows the times for 8 threads on a 4 core CPUwith Hyper-threading.
For these experiment, weset the clock speed to 3.46 Ghz in order to havethe same clock speed for all experiments.6 Non-Projective ApproximationThresholdFor non-projective parsing, we use the Non-Projective Approximation Algorithm of McDon-ald and Pereira (2006).
The algorithm rearrangesedges in a dependency tree when they improvethe score.
Bohnet (2009) extended the algorithmby a threshold which biases the rearrangement ofthe edges.
With a threshold, it is possible to gaina higher percentage of correct dependency links.We determined a threshold in experiments forCzech, English and German.
In the experiment,we use the Hash Kernel and increase the thresh-95System Average Catalan Chinese Czech English German Japanese SpanishTop CoNLL 09 85.77(1) 87.86(1) 79.19(4) 80.38(1) 89.88(2) 87.48(2) 92.57(3) 87.64(1)Baseline Parser 85.10 85.70 76.88 76.93 90.14 87.64 92.26 86.12this work 86.33 87.45 76.99 80.96 90.33 88.06 92.47 88.13Table 6: Top LAS of the CoNLL 2009 of (1) Gesmundo et al (2009), (2) Bohnet (2009), (3) Che etal.
(2009), and (4) Ren et al (2009); LAS of the baseline parser and the parser with Hash Kernel.
Thenumbers in bold face mark the top scores.
We used for Catalan, Chinese, Japanese and Spanish theprojective parsing algorithm.old at the beginning in small steps by 0.1 and laterin larger steps by 0.5 and 1.0.
Figure 2 showsthe labeled attachment scores for the Czech, En-glish and German development set in relation tothe rearrangement threshold.
The curves for alllanguages are a bit volatile.
The English curveis rather flat.
It increases a bit until about 0.3and remains relative stable before it slightly de-creases.
The labeled attachment score for Ger-man and Czech increases until 0.3 as well and thenboth scores start to decrease.
For English a thresh-old between 0.3 and about 2.0 would work well.For German and Czech, a threshold of about 0.3is the best choice.
We selected for all three lan-guages a threshold of 0.3.74767880828486880 1 2 3 4 5Czech English GermanFigure 2: English, German, and Czech labeled at-tachment score (y-axis) for the development set inrelation to the rearrangement threshold (x-axis).7 Conclusion and Future WorkWe have developed a very fast parser with ex-cellent attachment scores.
For the languages ofthe 2009 CoNLL Shared Task, the parser couldreach higher accuracy scores on average than thetop performing systems.
The scores for Catalan,Chinese and Japanese are still lower than the topscores.
However, the parser would have rankedsecond for these languages.
For Catalan andChinese, the top results obtained transition-basedparsers.
Therefore, the integration of both tech-niques as in Nivre and McDonald (2008) seemsto be very promising.
For instance, to improvethe accuracy further, more global constrains cap-turing the subcategorization correct could be inte-grated as in Riedel and Clarke (2006).
Our fasteralgorithms may make it feasible to consider fur-ther higher order factors.In this paper, we have investigated possibilitiesfor increasing parsing speed without any accuracyloss.
The parsing time is 3.5 times faster on a sin-gle CPU core than the baseline parser which hasan typical architecture for a maximum spanningtree parser.
The improvement is due solely to theHash Kernel.
The Hash Kernel was also a prereq-uisite for the parallelization of the parser becauseit requires much less memory bandwidth which isnowadays a bottleneck of parsers and many otherapplications.By using parallel algorithms, we could furtherincrease the parsing time by a factor of 3.4 on a4 core CPU and including hyper threading by afactor of 4.6.
The parsing speed is 16 times fasterfor the English test set than the conventional ap-proach.
The parser needs only 77 millisecond inaverage to parse a sentence and the speed willscale with the number of cores that become avail-able in future.
To gain even faster parsing times, itmay be possible to trade accuracy against speed.In a pilot experiment, we have shown that it ispossible to reduce the parsing time in this way toas little as 9 milliseconds.
We are convinced thatthe Hash Kernel can be applied successful to tran-sition based dependency parsers, phrase structureparsers and many other NLP applications.
44We provide the Parser and Hash Kernel as open sourcefor download from http://code.google.com/p/mate-tools.96ReferencesAttardi, G. 2006.
Experiments with a MultilanguageNon-Projective Dependency Parser.
In Proceedingsof CoNLL, pages 166?170.Blum., A.
2006.
Random Projection, Margins, Ker-nels, and Feature-Selection.
In LNCS, pages 52?68.Springer.Bohnet, B.
2009.
Efficient Parsing of Syntactic andSemantic Dependency Structures.
In Proceedingsof the 13th Conference on Computational NaturalLanguage Learning (CoNLL-2009).Carreras, X.
2007.
Experiments with a Higher-orderProjective Dependency Parser.
In EMNLP/CoNLL.Che, W., Li Z., Li Y., Guo Y., Qin B., and Liu T. 2009.Multilingual Dependency-based Syntactic and Se-mantic Parsing.
In Proceedings of the 13th Confer-ence on Computational Natural Language Learning(CoNLL-2009).Collins, M. 2002.
Discriminative Training Methodsfor Hidden Markov Models: Theory and Experi-ments with Perceptron Algorithms.
In EMNLP.Crammer, K., O. Dekel, S. Shalev-Shwartz, andY.
Singer.
2003.
Online Passive-Aggressive Algo-rithms.
In Sixteenth Annual Conference on NeuralInformation Processing Systems (NIPS).Crammer, K., O. Dekel, S. Shalev-Shwartz, andY.
Singer.
2006.
Online Passive-Aggressive Al-gorithms.
Journal of Machine Learning Research,7:551?585.Eisner, J.
1996.
Three New Probabilistic Models forDependency Parsing: An Exploration.
In Proceed-ings of the 16th International Conference on Com-putational Linguistics (COLING-96), pages 340?345, Copenhaen.Eisner, J., 2000.
Bilexical Grammars and their Cubic-time Parsing Algorithms, pages 29?62.
KluwerAcademic Publishers.Gesmundo, A., J. Henderson, P. Merlo, and I. Titov.2009.
A Latent Variable Model of Syn-chronous Syntactic-Semantic Parsing for MultipleLanguages.
In Proceedings of the 13th Confer-ence on Computational Natural Language Learning(CoNLL-2009), Boulder, Colorado, USA., June 4-5.Hajic?, J., M. Ciaramita, R. Johansson, D. Kawahara,M.
Anto`nia Mart?
?, L. Ma`rquez, A. Meyers, J. Nivre,S.
Pado?, J.
?Ste?pa?nek, P. Stran?a?k, M. Surdeanu,N.
Xue, and Y. Zhang.
2009.
The CoNLL-2009Shared Task: Syntactic and Semantic Dependenciesin Multiple Languages.
In Proceedings of the 13thCoNLL-2009, June 4-5, Boulder, Colorado, USA.Johansson, R. and P. Nugues.
2008.
Dependency-based Syntactic?Semantic Analysis with PropBankand NomBank.
In Proceedings of the Shared TaskSession of CoNLL-2008, Manchester, UK.McDonald, R. and F. Pereira.
2006.
Online Learningof Approximate Dependency Parsing Algorithms.In In Proc.
of EACL, pages 81?88.McDonald, R., K. Crammer, and F. Pereira.
2005.
On-line Large-margin Training of Dependency Parsers.In Proc.
ACL, pages 91?98.Nivre, J. and R. McDonald.
2008.
Integrating Graph-Based and Transition-Based Dependency Parsers.In ACL-08, pages 950?958, Columbus, Ohio.Nivre, J., J.
Hall, and J. Nilsson.
2004.
Memory-Based Dependency Parsing.
In Proceedings of the8th CoNLL, pages 49?56, Boston, Massachusetts.Nivre, J.
2003.
An Efficient Algorithm for Pro-jective Dependency Parsing.
In 8th InternationalWorkshop on Parsing Technologies, pages 149?160,Nancy, France.Nivre, J.
2009.
Non-Projective Dependency Parsing inExpected Linear Time.
In Proceedings of the 47thAnnual Meeting of the ACL and the 4th IJCNLP ofthe AFNLP, pages 351?359, Suntec, Singapore.Rahimi, A. and B. Recht.
2008.
Random Featuresfor Large-Scale Kernel Machines.
In Platt, J.C.,D.
Koller, Y.
Singer, and S. Roweis, editors, Ad-vances in Neural Information Processing Systems,volume 20.
MIT Press, Cambridge, MA.Ren, H., D. Ji Jing Wan, and M. Zhang.
2009.
Pars-ing Syntactic and Semantic Dependencies for Mul-tiple Languages with a Pipeline Approach.
In Pro-ceedings of the 13th Conference on ComputationalNatural Language Learning (CoNLL-2009), Boul-der, Colorado, USA., June 4-5.Riedel, S. and J. Clarke.
2006.
Incremental Inte-ger Linear Programming for Non-projective Depen-dency Parsing.
In Proceedings of the 2006 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 129?137, Sydney, Australia, July.Association for Computational Linguistics.Shi, Q., J. Petterson, G. Dror, J. Langford, A. Smola,and S.V.N.
Vishwanathan.
2009.
Hash Kernels forStructured Data.
In Journal of Machine Learning.Titov, I. and J. Henderson.
2007.
A Latent VariableModel for Generative Dependency Parsing.
In Pro-ceedings of IWPT, pages 144?155.Yamada, H. and Y. Matsumoto.
2003.
Statistical De-pendency Analysis with Support Vector Machines.In Proceedings of IWPT, pages 195?206.97
