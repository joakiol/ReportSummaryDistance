R. Dale et al (Eds.
): IJCNLP 2005, LNAI 3651, pp.
519 ?
529, 2005.?
Springer-Verlag Berlin Heidelberg 2005Web-Based Unsupervised Learning for QueryFormulation in Question AnsweringYi-Chia Wang1, Jian-Cheng Wu2, Tyne Liang1, and Jason S. Chang21Dep.
of Computer and Information Science, National Chiao Tung University,1001 Ta Hsueh Rd., Hsinchu, Taiwan 300, R.O.C.rhyme.cis92g@nctu.edu.tw, tliang@cis.nctu.edu.tw2Dep.
of Computer Science, National Tsing Hua University,101, Section 2 Kuang Fu Road, Hsinchu, Taiwan 300, R.O.C.d928322@oz.nthu.edu.tw, jschang@cs.nthu.edu.twAbstract.
Converting questions to effective queries is crucial to open-domainquestion answering systems.
In this paper, we present a web-basedunsupervised learning approach for transforming a given natural-languagequestion to an effective query.
The method involves querying a search enginefor Web passages that contain the answer to the question, extracting patternsthat characterize fine-grained classification for answers, and linking thesepatterns with n-grams in answer passages.
Independent evaluation on a set ofquestions shows that the proposed approach outperforms a naive keyword-based approach in terms of mean reciprocal rank and human effort.1   IntroductionAn automated question answering (QA) system receives a user?s natural-languagequestion and returns exact answers by analyzing the question and consulting a largetext collection [1, 2].
As Moldovan et al [3] pointed out, over 60% of the QA errorscan be attributed to ineffective question processing, including query formulation andquery expansion.A naive solution to query formulation is using the keywords in an input question asthe query to a search engine.
However, it is possible that the keywords may not appearin those answer passages which contain answers to the given question.
For example,submitting the keywords in ?Who invented washing machine??
to a search engine likeGoogle may not lead to retrieval of answer passages like ?The inventor of the automaticwasher was John Chamberlain.?
In fact, by expanding the keyword set (?invented?,?washing?, ?machine?)
with ?inventor of,?
the query to a search engine is effective inretrieving such answer passages as the top-ranking pages.
Hence, if we can learn how toassociate a set of questions (e.g.
(?who invented ???)
with effective keywords orphrases (e.g.
?inventor of?)
which are likely to appear in answer passages, the searchengine will have a better chance of retrieving pages containing the answer.In this paper, we present a novel Web-based unsupervised learning approach tohandling question analysis for QA systems.
In our approach, training-data questionsare first analyzed and classified into a set of fine-grained categories of question520 Y.-C. Wang et alpatterns.
Then, the relationships between the question patterns and n-grams in answerpassages are discovered by employing a word alignment technique.
Finally, the bestquery transforms are derived by ranking the n-grams which are associated with aspecific question pattern.
At runtime, the keywords in a given question are extractedand the question is categorized.
Then the keywords are expanded according thecategory of the question.
The expanded query is the submitted to a search engine inorder to bias the search engine to return passages that are more likely to containanswers to the question.
Experimental results indicate the expanded query indeedoutperforms the approach of directly using the keywords in the question.2   Related WorkRecent work in Question Answering has attempted to convert the original inputquestion into a query that is more likely to retrieve the answers.
Hovy et al [2] utilizedWordNet hypernyms and synonyms to expand queries to increase recall.
Hildebrandt etal.
[4] looked up in a pre-compiled knowledge base and a dictionary to expand adefinition question.
However, blindly expanding a word using its synonyms ordictionary gloss may cause undesirable effects.
Furthermore, it is difficult to determinewhich of many related word senses should be considered when expanding the query.Radev et al [5] proposed a probabilistic algorithm called QASM that learns the bestquery expansion from a natural language question.
The query expansion takes theform of a series of operators, including INSERT, DELETE, REPLACE, etc., toparaphrase a factual question into the best search engine query by applyingExpectation Maximization algorithm.
On the other hand, Hermjakob et al [6]described an experiment to observe and learn from human subjects who were given aquestion and asked to write queries which are most effective in retrieving the answerto the question.
First, several randomly selected questions are given to users to?manually?
generate effective queries that can bias Web search engines to returnanswers.
The questions, queries, and search results are then examined to derive sevenquery reformulation techniques that can be used to produce queries similar to the onesissued by human subjects.In a study closely related to our work, Agichtein et al [7] presented Tritus systemthat automatically learns transforms of wh-phrases (e.g.
expanding ?what is?
to?refers to?)
by using FAQ data.
The wh-phrases are restricted to sequences offunction word beginning with an interrogative, (i.e.
who, what, when, where, why,and how).
These wh-phrases tend to coarsely classify questions into a few types.Tritus uses heuristic rules and thresholds of term frequencies to learn transforms.In contrast to previous work, we rely on a mathematical model trained on a set ofquestions and answers to learn how to transform the question into an effective query.Transformations are learned based on a more fine-grained question classificationinvolving the interrogative and one or more content words.3   Transforming Question to QueryThe method is aimed at automatically learning of the best transforms that turn a givennatural language question into an effective query by using the Web as corpus.
To thatWeb-Based Unsupervised Learning for Query Formulation in Question Answering 521end, we first automatically obtain a collection of answer passages (APs) as thetraining corpus from the Web by using a set of (Q, A) pairs.
Then we identify thequestion pattern for each Q by using statistical and linguistic information.
Here, aquestion pattern Qp is defined as a question word plus one or two keywords that arerelated to the question word.
Qp represents the question intention and it can be treatedas a preference indicative for fine-grained type of named entities.
Finally, we decidethe transforms Ts for each Qp by choosing those phrases in the APs that arestatistically associated with Qp and adjacent to the answer A.Table 1.
An example of converting a question (Q) with its answer (A) to a SE query andretrieving answer passages (AP)(Q, A) APBungalow For Rent in Islamabad, CapitalPakistan.
Beautiful Big House For ?
What is the capital of Pakistan?
Answer:( Islamabad)(k1, k2,?, kn, A)Islamabad is the capital of Pakistan.
Currenttime, ?capital, Pakistan, Islamabad ?the airport which serves Pakistan's capital Islamabad, ?3.1   Search the Web for Relevant Answer PassagesFor training purpose, a large amount of question/answer passage pairs are mined fromthe Web by using a set of question/answer pairs as seeds.More formally, we attempt to retrieve a set of (Q, AP) pairs on the Web for trainingpurpose, where Q stands for a natural language question, and AP is a passagecontaining at least one keyword in Q and A (the answer to Q).
The seed data (Q, A)pairs can be acquired from many sources, including trivia game Websites, TREC QATrack benchmarks, and files of Frequently Asked Questions (FAQ).
The output ofthis training-data gathering process is a large collection of (Q, AP) pairs.
We describethe procedure in details as follows:1.
For each (Q, A) pair, the keywords k1, k2,?, kn are extracted from Q by removingstopwords.2.
Submit (k1, k2,?, kn, A) as a query to a search engine SE.3.
Download the top n summaries returned by SE.4.
Separate sentences in the summaries, and remove HTML tags, URL, specialcharacter references (e.g., ?&lt;?).5.
Retain only those sentences which contain A and some ki.Consider the example of gathering answer passages from the Web for the (Q, A)pair where Q = ?What is the capital of Pakistan??
and A = ?Islamabad.?
See Table 1for the query submitted to a search engine and potential answer passages returned.3.2   Question AnalysisThis subsection describes the presented identification of the so-called ?questionpattern?
which is critical in categorizing a given question and transforming thequestion into a query.522 Y.-C. Wang et alFormally, a ?question pattern?
for any question is defined as following form:question-word  head-word+where ?question-word?
is one of the interrogatives (Who/What/Where/When/How)and the ?head-word?
represents the headwords in the subsequent chunks that tend toreflect the intended answer more precisely.
If the first headword is a light verb, anadditional headword is needed.
For instance, ?who had hit?
is a reasonable questionpattern for ?Who had a number one hit in 1984 with ?Hello??
?, while ?who had?seems to be too coarse.In order to determine the appropriate question pattern for each question, weexamined and analyzed a set of questions which are part-of-speech (POS) tagged andphrase-chunked.
With the help of a set of simple heuristic rules based on POS andchunk information, fine-grained classification of questions can be carried outeffectively.Question Pattern ExtractionAfter analyzing recurring patterns and regularity in quizzes on the Web, we designeda simple procedure to recognize question patterns.
The procedure is based on a smallset of prioritized rules.The question word which is one of the wh-words (?who,?
?what,?
?when,??where,?
?how,?
or ?why?)
tagged as determiner or adverbial question word.According to the result of POS tagging and phrase chunking, we further decide themain verb and the voice of the question.
Then, we apply the following expanded rulesto extract words to form question patterns:Rule 1: Question word in a chunk of length more than one (see Example (1) in Table 2).Qp = question word + headword in the same chunkRule 2: Question word followed by a light verb and Noun Phrase(NP) orPrepositional Phrase(PP) chunk (Example (2)).Qp = question word + light verb +headword in the following NP or PP chunkRule 3: Question word followed immediately by a verb (Example (3)).Qp = question word + headword in the following Verb Phrase(VP) or NP chunkRule 4: Question word followed by a passive VP (Example (4)).Qp = Question word + ?to be?
+ headword in the passive VP chunkRule 5: Question word followed by the copulate ?to be?
and an NP (Example (5)).Qp = Question word + ?to be?
+ headword in the next NP chunkRule 6: If none of the above rules are applicable, the question pattern is the questionword.By exploiting linguistic information of POS and chunks, we can easily form thequestion pattern.
These heuristic rules are intuitive and easy to understand.
Moreover,the fact that these patterns which tend to recur imply that they are general and it iseasy to gather training data accordingly.
These question patterns also indicate apreference for the answer to be classified with a fine-grained type of proper nouns.
InWeb-Based Unsupervised Learning for Query Formulation in Question Answering 523the next section, we describe how we exploit these patterns to learn the best question-to-query transforms.Table 2.
Example questions and question patterns (of words shown in bold)(1) Which female singer performed the first song on Top of the Pops?
(2) Who in 1961 made the first space flight?
(3) Who painted ?The Laughing Cavalier??
(4) What is a group of geese called?
(5) What is the second longest river in the world?3.3   Learning Best TransformsThis section describes the procedure for learning transforms Ts which convert thequestion pattern Qp into bigrams in relevant APs.Word Alignment Across Q and APWe use word alignment techniques developed for statistical machine translation tofind out the association between question patterns in Q and bigrams in AP.
The reasonwhy we use bigrams in APs instead of unigrams is that bigrams tend to have moreunique meaning than single words and are more effective in retrieving relevantpassages.We use Competitive Linking Algorithm [8] to align a set of (Q, AP) pairs.
Themethod involves preprocessing steps for each (Q, AP) pair so as to filter uselessinformation:1.
Perform part-of-speech tagging on Q and AP.2.
Replace all instances of A with the tag <ANS> in APs to indicate the location ofthe answers.3.
Identify the question pattern, Qp and keywords which are not a named entity.
Wedenote the question pattern and keywords as q1, q2, ..., qn.4.
Convert AP into bigrams and eliminate bigrams with low term frequency (tf) orhigh document frequency (df).
Bigrams composed of two function words are alsoremoved, resulting in bigrams a1, a2, ..., am.We then align q?s and a?s via Competitive Linking Algorithm (CLA) procedure asfollows:Input: A collection C of (Q; A) pairs, where (Q; A) = (q1 = Qp , q2, q3, ..., qn ; a1, a2,..., am)Output: Best alignment counterpart a?s for all q?s in C1.
For each pair of (Q; A) in C and for all qi and aj in each pair of C, calculate LLR(qi,aj), logarithmic likelihood ratio (LLR) between qi and aj, which reflects theirstatistical association.2.
Discard (q, a) pairs with a LLR value lower than a threshold.524 Y.-C. Wang et al3.
For each pair of (Q; A) in C and for all qi and aj therein, carry out Steps 4-7:4.
Sort list of (qi, aj) in each pair of (Q ; A) by decreasing LLR value.5.
Go down the list and select a pair if it does not conflict with previous selection.6.
Stop when running out of pairs in the list.7.
Produce the list of aligned pairs for all Qs and APs.8.
Tally the counts of aligning (q, a).9.
Select top k bigrams, t1, t2, ..., tk, for every question pattern or keyword q.The LLR statistics is generally effective in distinguishing related terms fromunrelated ones.
However, if two terms occur frequently in questions, their alignmentcounterparts will also occur frequently, leading to erroneous alignment due to indirectassociation.
CLA is designed to tackle the problem caused by indirect association.Therefore, if we only make use of the alignment counterpart of the question pattern,we can keep the question keywords in Q so as to reduce the errors caused by indirectassociation.
For instance, the question ?How old was Bruce Lee when he died??
Ourgoal is to learn the best transforms for the question pattern ?how old.?
In other words,we want to find out what terms are associated with ?how old?
in the answer passages.However, if we consider the alignment counterparts of ?how old?
without consideringthose keyword like ?died,?
we run the risk of getting ?died in?
or ?is dead?
rather than?years old?
and ?age of.?
If we have sufficient data for a specific question pattern like?how long,?
we will have more chances to obtain alignment counterparts that areeffective terms for query expansion.Distance Constraint and Proximity RanksIn addition to the association strength implied with alignment counts and co-occurrence, the distance of the bigrams to the answer should also be considered.
Weobserve that terms in the answer passages close to the answers intuitively tend to beuseful in retrieving answers.
Thus, we calculate the bigrams appearing in a window ofthree words appearing on both sides of the answers to provide additional constraintsfor query expansion.Combing Alignment and Proximity RanksThe selection of the best bigrams as the transforms for a specific question pattern isbased on a combined rank of alignment count and proximity count.
It takes theaverage of these two counts to re-rank bigrams.
The average rank of a bigram b isRankavg (b) = (Rankalign (b)+ Rankprox (b))/2,where Rankalign (b) is the rank of b?s alignment count and Rankprox (b) is the rank ofb?s proximity count.
The n top-ranking bigrams  for a specific type of question will bechosen to transform the question pattern into query terms.
For the question pattern?how old,?
the candidate bigrams with alignment ranks, co-occurring ranks, andaverage ranks are shown in Table 3.Web-Based Unsupervised Learning for Query Formulation in Question Answering 525Table 3.
Average rank calculated from for the bigram counterparts of ?how old?Bigrams Alignment Rank Proximity Rank Avg.
Rank Final Rankage of 1 1 1 1years old 2 2 2 2ascend the 3 - - -throne in 4 3 3.5 3the youngest 3 - - -?
?
?
?
?3.4   Runtime Transformation of QuestionsAt runtime, a given question Q submitted by a user is converted into one or morekeywords and a question pattern, which is subsequently expanded in to a sequence ofquery terms based on the transforms obtained at training.We follow the common practice of keyword selection in formulating Q into aquery:?
Function words are identified and discarded.?
Proper nouns that are capitalized or quoted are treated as a single search term withquotes.Additionally, we expand the question patterns based on alignment and proximityconsiderations:?
The question pattern Qp is identified according to the rules (in Section 3.2) and isexpanded to be a disjunction (sequence of ORs) of Qp?s headword and n top-ranking bigrams (in section 3.3)?
The query will be a conjunction (sequence of ANDs) of expanded Qp, propernames, and remaining keywords.
Except for the expanded Qp, all other propernames and keywords will be in the original order in the given question for the bestresults.Table 4.
An example of transformation from question into queryQuestionHow old was Bruce Lee when he died?Question pattern Proper noun Keywordhow oldTransformationage of, years old?Bruce Lee?
diedExpanded queryBoolean query: ( ?old?
OR ?age of?
OR ?years old? )
AND ?Bruce Lee?
AND ?died?Equivalent Google query: (old || ?age of?
|| ?years old?)
?Bruce Lee?
died526 Y.-C. Wang et alFor example, formulating a query for the question ?How old was Bruce Lee whenhe died??
will result in a question pattern ?how old.?
Because there is a proper noun?Bruce Lee?
in the question and a remaining keyword ?died,?
the query becomes?
( ?old?
OR ?age of?
OR ?years old? )
AND ?Bruce Lee?
AND ?died.??
Table 4 lists thequery formulating for the example question.4   Experiments and EvaluationThe proposed method is implemented by using the Web search engine, Google, as theunderlying information retrieval system.
The experimental results are also justifiedwith assessing the effectiveness of question classification and query expansion.We used a POS tagger and chunker to perform shallow parsing of the questionsand answer passages.
The tagger was developed using the Brown corpus andWordNet.
The chunker is built from the shared CoNLL-2000 data provided byCoNLL-2000.
The shared task CoNLL-2000 provides a set of training and test datafor chunks.
The chunker we used produces chunks with an average precision rate ofabout 94%.4.1   Evaluation of Question PatternsThe 200 questions from TREC-8 QA Track provide an independent evaluation of howwell the proposed method works for question pattern extraction works.
We will alsogive an error analysis.Table 5.
Evaluation results of question pattern extractionTwo ?good?
labels At least one ?good?
labelPrecision (%) 86 96Table 6.
The first five questions with question patterns and judgmentQuestion Question pattern JudgmentWho is the author of the book, "The IronLady: A Biography of Margaret Thatcher"?
Who-author goodWhat was the monetary value of the NobelPeace Prize in 1989?
What value goodWhat does the Peugeot company manufacture?
What domanufacture goodHow much did Mercury spend on advertisingin 1993?
How much goodWhat is the name of the managing director ofApricot Computer?
What name badWeb-Based Unsupervised Learning for Query Formulation in Question Answering 527Two human judges both majoring in Foreign Languages were asked to assess theresults of question pattern extraction and give a label to each extracted questionpattern.
A pattern will be judged as ?good?
if it clearly expresses the answerpreference of the question; otherwise, it is tagged as ?bad.?
The precision rate ofextraction for these 200 questions is shown in Table 5.
The second column indicatesthe precision rate when both of two judges agree that an extracted question pattern is?good.?
In addition, the third column indicates the rate of those question patterns thatare found to be ?good?
by either judge.
The results imply that the proposed patternextraction rules are general, since they are effective even for questions independent ofthe training and development data.
Table 6 shows evaluation results for ?two ?good?labels?
of the first five questions.We summarize the reasons behind these bad patterns:?
Incorrect part-of-speech tagging and chunking?
Imperative questions such as ?Name the first private citizen to fly in space.??
Question patterns that are not specific enoughFor instance, the system produces ?what name?
for ?What is the name of thechronic neurological autoimmune disease which ?
?
?, while the judges suggestedthat ?what disease.?.
Indeed, some of the patterns extracted can be modified to meetthe goal of being more fine-grained and indicative of a preference to a specific type ofproper nouns or terminology.4.2   Evaluation of Query ExpansionWe implemented a prototype of the proposed method called Atlas (AutomaticTransform Learning by Aligning Sentences of question and answer).
To develop thesystem of Atlas, we gathered seed training data of questions and answers from a triviagame website, called QuizZone1.
We collected the questions posted in June, 2004 onQuizZone and obtained 3,851 distinct question-answer pairs.
We set aside the first 45questions for testing and used the rest for training.
For each question, we form a querywith question keywords and the answer and submitted the query to Google to retrievetop 100 summaries as the answer passages.
In all, we collected 95,926 answer passages.At training time, we extracted a total of 338 distinct question patterns from 3,806questions.
We aligned these patterns and keywords with bigrams in the 95,926 answerpassages, identified the locations of the answers, and obtained the bigrams appearingwithin a distance of 3 of the answers.
At runtime, we use the top-ranking bigram toexpand each question pattern.
If no such bigrams are found, we use only the keywordin the question patterns.
The expanded terms for question pattern are placed at thebeginning of the query.We submitted forty-five keyword queries and the same number of expandedqueries generated by Atlas for the test questions to Google and obtained ten returnedsummaries for evaluation.
For the evaluation, we use three indicators to measure theperformance.
The first indicator is the mean reciprocal rank (MRR) of the firstrelevant document (or summary) returned.
If the r-th document (summary) returned isthe one with the answer, then the reciprocal rank of the document (summary) is 1/r.1QuizZone (http://www.quiz-zone.co.uk)528 Y.-C. Wang et alThe mean reciprocal rank is the average reciprocal rank of all test questions.
Thesecond indicator of effective query is the recall at R document retrieved (Recall at R).The last indicator measures the human effort (HE) in finding the answer.
HE isdefined as the least number of passages needed to be viewed for covering all theanswers to be returned from the system.The average length of these test questions is short.
We believe the proposedquestion expansion scheme helps those short sentences, which tend to be lesseffective in retrieving answers.
We evaluated the expanded queries against the samemeasures for summaries returned by simple keyword queries.
Both batches ofreturned summaries for the forty-five questions were verified by two human judges.As shown in Table 7, the MRR produced by keyword-based scheme is slightly lowerthan the one yielded by the presented query expansion scheme.
Nevertheless, suchimprovement is encouraging by indicating the effectiveness of the proposed method.Table 8 lists the comparisons in more details.
It is found that our method iseffective in bringing the answers to the top 1 and top 2 summaries as indicated by thehigh Recall of 0.8 at R = 2.
In addition, Table 8 also shows that less user?s efforts areneeded by using our approach.
That is, for each question, the average of summariesrequired to be viewed by human beings goes down from 2.7 to 2.3.In the end, we found that those bigrams containing a content word and a functionword  turn out to be very effective.
For instance, our method tends to favor transformsTable 7.
Evaluation results of MRRPerformances MRRGO (Direct keyword query for Google) 0.64AT+GO (Atlas expanded query for Google) 0.69Table 8.
Evaluation Result of Recall at R and Human EffortRank count Recall at R Rank GO AT+GO GO AT+GO1 25 26 0.56 0.582 6 10 0.69 0.803 5 3 0.80 0.874 0 1 0.80 0.895 1 1 0.82 0.916 2 0 0.87 0.917 1 0 0.89 0.918 2 0 0.93 0.919 0 1 0.93 0.9310 0 0 0.93 0.93No answers 3 3Human Effort 122 105# of questions 45 45HE per question 2.7 2.3Web-Based Unsupervised Learning for Query Formulation in Question Answering 529such as ?who invented?
to bigrams such as ?invented by,?
?invent the,?
and ?inventorof.?
This contrasts to conventional wisdom of using a stoplist of mostly functionwords and excluding them from consideration in a query.
Our experiment also showsa function word as part of a phrasal term seems to be very effective, for it indicate animplied relation with the answer.5   Conclusion and Future WorkIn this paper, we introduce a method for learning query transformations that improvesthe ability to retrieve passages with answers using the Web as corpus.
The methodinvolves question classification and query transformations using a learning-basedapproach.
We also describe the experiment with over 3,000 questions indicates thatsatisfactory results were achieved.
The experimental results show that the proposedmethod provides effective query expansion that potentially can lead to performanceimprovement for a question answering system.A number of future directions present themselves.
First, the patterns learned fromanswer passages acquired on the Web can be refined and clustered to derive ahierarchical classification of questions for more effective question classification.
Second,different question patterns, like ?who wrote?
and ?which author?, should be treated as thesame in order to cope with data sparseness and improve system performance.
On theother hand, an interesting direction is the generating pattern transformations that containthe answer extraction patterns for different types of questions.References1.
Ittycheriah, A., Franz, M., Zhu, W.-J., and Rathaparkhi, A.
2000.
IBM?s statisticalquestion answering system.
In Proceedings of the TREC-9 Question Answering Track,Gaithersburg, Maryland.2.
Hovy, E., Gerber, L., Hermjakob, U., Junk, M., and Lin, C.-Y.
2000.
Question answeringin Webclopedia.
In Proceedings of the TREC-9 Question Answering Track, Gaithersburg,Maryland.3.
Moldovan D., Pasca M., Harabagiu S., & Surdeanu M. 2002.
Performance Issues and errorAnalysis in an Open-Domain Question Answering System.
In Proceedings of the 40thAnnual Meeting of ACL, Philadelphia, Pennsylvania.4.
Hildebrandt, W., Katz, B., & Lin, J.
2004.
Answering definition questions with multipleknowledge sources.
In Proceedings of the 2004 Human Language Technology Conferenceand the North American Chapter of the Association for Computational.5.
Radev, D. R., Qi, H., Zheng, Z., Blair-Goldensohn, S., Fan, Z.
Z. W., and Prager, J. M.2001.
Mining the web for answers to natural language questions.
In Proceedings of theInternational Conference on Knowledge Management (CIKM-2001), Atlanta, Georgia.6.
Hermjakob, U., Echihabi, A., and Marcu, D. 2002.
Natural Language BasedReformulation Resource and Web Exploitation for Question Answering.
In Proceeding ofTREC-2002, Gaithersburg, Maryland.7.
Agichtein, E., Lawrence, S., and Gravano, L. Learning to find answers to questions on theWeb.
2003.
In ACM Transactions on Internet Technology (TOIT), 4(2):129-162.8.
Melamed, I. D. 1997.
A Word-to-Word Model of Translational Equivalence.
InProceedings of the 35st Annual Meeting of ACL, Madrid, Spain.9.
Yi-Chia Wang, Jian-Cheng Wu, Tyne Liang, and Jason S. Chang.
2004.
Using the Web asCorpus for Un-supervised Learning in Question Answering, Proceedings of Rocling 2004,Taiwan.
