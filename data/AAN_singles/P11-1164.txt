Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1636?1644,Portland, Oregon, June 19-24, 2011. c?2011 Association for Computational LinguisticsExtracting Comparative Entities and Predicates from Texts UsingComparative Type ClassificationSeon Yang Youngjoong KoDepartment of Computer Engineering, Department of Computer Engineering,Dong-A University, Dong-A University,Busan, Korea Busan, Koreaseony.yang@gmail.com yjko@dau.ac.krAbstractThe automatic extraction of comparative in-formation is an important text miningproblem and an area of increasing interest.In this paper, we study how to build aKorean comparison mining system.
Ourwork is composed of two consecutive tasks:1) classifying comparative sentences intodifferent types and 2) mining comparativeentities and predicates.
We perform variousexperiments to find relevant features andlearning techniques.
As a result, we achieveoutstanding performance enough forpractical use.1 IntroductionAlmost every day, people are faced with a situationthat they must decide upon one thing or the other.To make better decisions, they probably attempt tocompare entities that they are interesting in.
Thesedays, many web search engines are helping peoplelook for their interesting entities.
It is clear thatgetting information from a large amount of webdata retrieved by the search engines is a muchbetter and easier way than the traditional surveymethods.
However, it is also clear that directlyreading each document is not a perfect solution.
Ifpeople only have access to a small amount of data,they may get a biased point of view.
On the otherhand, investigating large amounts of data is a time-consuming job.
Therefore, a comparison miningsystem, which can automatically provide asummary of comparisons between two (or more)entities from a large quantity of web documents,would be very useful in many areas such asmarketing.We divide our work into two tasks to effectivelybuild a comparison mining system.
The first task isrelated to a sentence classification problem and thesecond is related to an information extractionproblem.Task 1.
Classifying comparative sentences intoone non-comparative class and sevencomparative classes (or types); 1) Equality, 2)Similarity, 3) Difference, 4) Greater or lesser, 5)Superlative, 6) Pseudo, and 7) Implicitcomparisons.
The purpose of this task is toefficiently perform the following task.Task 2.
Mining comparative entities andpredicates taking into account the characteristicsof each type.
For example, from the sentence?Stock-X is worth more than stock-Y.?
belongingto ?4) Greater or lesser?
type, we extract ?stock-X?
as a subject entity (SE), ?stock-Y?
as anobject entity (OE), and ?worth?
as a comparativepredicate (PR).These tasks are not easy or simple problems asdescribed below.Classifying comparative sentences (Task 1): Forthe first task, we extract comparative sentencesfrom text documents and then classify theextracted comparative sentences into seven1636comparative types.
Our basic idea is a keywordsearch.
Since Ha (1999a) categorized dozens ofKorean comparative keywords, we easily build aninitial keyword set as follows:?
?ling = {??
([gat]: same)?, ???
([bo-da]: than)?,???
([ga-jang]: most)?, ?
}In addition, we easily match each of thesekeywords to a particular type anchored to Ha?sresearch, e.g., ??
([gat]: same)?
to ?1) Equality?,???
([bo-da]: than)?
to ?4) Greater or lesser?.However, any method that depends on just theselinguistic-based keywords has obvious limitationsas follows:1)  ?ling is insufficient to cover all of the actualcomparison expressions.2) There are many non-comparative sentencesthat contain some elements of ?ling.3) There is no one-to-one relationship betweenkeyword types and sentence types.Mining comparative entities and predicates(Task 2): Our basic idea for the second task isselecting candidates first and finding answers fromthe candidates later.
We regard each of noun wordsas a candidate for SE/OE, and each of adjective (orverb) words as a candidate for PR.
However, thiscandidate detection has serious problems asfollows:4) There are many actual SEs, OEs, and PRs thatconsist of multiple words.5) There are many sentences with no OE,especially among superlative sentences.
Itmeans that the ellipsis is frequently occurred insuperlative sentences.We focus on solving the above five problems.We perform various experiments to find relevantfeatures and proper machine learning techniques.The final experimental results in 5-fold crossvalidation show the overall accuracy of 88.59% forthe first task and the overall accuracy of 86.81%for the second task.The remainder of the paper is organized asfollows.
Section 2 briefly introduces related work.Section 3 and Section 4 describe our first task andsecond task in detail, respectively.
Section 5reports our experimental results and finally Section6 concludes.2 Related WorkLinguistic researchers focus on defining the syntaxand semantics of comparative constructs.
Ha(1999a; 1999b) classified the structures of Koreancomparative sentences into several classes andarranged comparison-bearing words from alinguistic perspective.
Since he summarized themodern Korean comparative studies, his researchhelps us have a linguistic point of view.
We alsorefer to Jeong (2000) and Oh (2004).
Jeongclassified adjective superlatives using certainmeasures, and Oh discussed the gradability ofcomparatives.In computer engineering, we found five previousstudies related to comparison mining.
Jindal andLiu (2006a; 2006b) studied to mine comparativerelations from English text documents.
They usedcomparative and superlative POS tags, and someadditional keywords.
Their methods applied ClassSequential Rules and Label Sequential Rules.Yang and Ko (2009; 2011) studied to extractcomparative sentences in Korean text documents.Li et al (2010) studied to mine comparable entitiesfrom English comparative questions that usersposted online.
They focused on finding a set ofcomparable entities given a user?s input entity.Opinion mining is also related to our workbecause many comparative sentences also containthe speaker?s opinion/sentiment.
Lee et al (2008)surveyed various techniques that have beendeveloped for the key tasks of opinion mining.Kim and Hovy (2006) introduced a methodologyfor analyzing judgment opinion.
Riloff and Wiebe(2003) presented a bootstrapping process thatlearns linguistically rich extraction patterns forsubjective expressions.In this study, three learning techniques areemployed: the maximum entropy method (MEM)as a representative probabilistic model, the supportvector machine (SVM) as a kernel model, andtransformation-based learning (TBL) as a rule-based model.
Berger et al (1996) presented aMaximum Entropy Approach to natural languageprocessing.
Joachims (1998) introduced SVM fortext classification.
Various TBL studies have beenperformed.
Brill (1992; 1995) first introduced TBLand presented a case study on part-of-speech1637tagging.
Ramshaw and Marcus (1995) appliedTBL for locating chunks in tagged texts.
Black andVasilakopoulos (2002) used a modified TBLtechnique for Named Entity Recognition.3 Classifying Comparative Sentences(Task 1)We first classify the sentences into comparativesand non-comparatives by extracting onlycomparatives from text documents.
Then weclassify the comparatives into seven types.3.1 Extracting comparative sentences fromtext documentsOur strategy is to first detect ComparativeSentence candidates (CS-candidates), and theneliminate non-comparative sentences from thecandidates.
As mentioned in the introductionsection, we easily construct a linguistic-basedkeyword set, ?ling.
However, we observe that ?lingis not enough to capture all the actual comparisonexpressions.
Hence, we build a comparison lexiconas follows:?
Comparison Lexicon = ?ling U {Additionalkeywords that are frequently used for actualcomparative expressions}This lexicon is composed of three parts.
The firstpart includes the elements of ?ling and theirsynonyms.
The second part consists of idioms.
Forexample, an idiom ?X?
??
???
[X-ga meon-jeou-seot-da]?
commonly means ?The winner is X?while it literally means ?X laughed first?.
The lastpart consists of long-distance-words sequences,e.g., ?<X?
[X-neun], ??
[ji-man], Y?
[Y-neun], ?[da]>?.
This sequence means that the sentence isformed as < S(X) + V + but + S(Y) + V > inEnglish (S: subject phrase; V: verb phrase; X, Y:proper nouns).
We could regard a word, ???
([ji-man]: but),?
as a single keyword.
However, thisword also captures numerous non-comparativesentences.
Namely, the precision value can fall toomuch due to this word.
By using long-distance-words sequences instead of single keywords, wecan keep the precision value from droppingseriously low.The comparison lexicon finally has a total of177 elements.
We call each element ?CK?hereafter.
Note that our lexicon does not includecomparative/superlative POS tags.
Unlike English,there is no Korean comparative/superlative POStag from POS tagger commonly.
Our lexiconcovers 95.96% of the comparative sentences in ourcorpus.
It means that we successfully defined acomparison lexicon for CS-candidate detection.However, the lexicon shows a relatively lowprecision of 68.39%.
While detecting CS-candidates, the lexicon also captures many non-comparative sentences, e.g., following Ex1:?
Ex1.
????
???
??
?
??.?
([nai-il-eun ju-sik-i o-reul-geot gat-da]: I think stock price willrise tomorrow.
)This sentence is a non-comparative sentence eventhough it contains a CK, ??[gat].?
This CKgenerally means ?same,?
but it often expresses?conjecture.?
Since it is an adjective in both cases,it is difficult to distinguish the difference.To effectively filter out non-comparativesentences from CS-candidates, we use thesequences of ?continuous POS tags within a radiusof 3 words from each CK?
as features.
Each wordin the sequence is replaced with its POS tag inorder to reflect various expressions.
However, asCKs play the most important role, they arerepresented as a combination of their lexicalizationand POS tag, e.g., ??/pa1.?
Finally, the feature hasthe form of ?X ?
y?
(?X?
means a sequence and?y?
means a class; y1: comparative, y2: non-comparative).
For instance, ?<pv etm nbn ?/pa efsf 2  >?
y2?
is one of the features from Ex1sentence.
Finally, we achieved an f1-score of90.23% using SVM.3.2 Classifying comparative sentences intoseven typesAs we extract comparative sentences successfully,the next step is to classify the comparatives intodifferent types.
We define seven comparative typesand then employ TBL for comparative sentenceclassification.We first define six broad comparative typesbased on modern Korean linguistics: 1) Equality,2) Similarity, 3) Difference, 4) Greater or lesser,5) Superlative, 6) Pseudo comparisons.
The firstfive types can be understood intuitively, whereas1 The POS tag ?pa?
means ?the stem of an adjective?.2 The labels such as ?pv?, ?etm?
are Korean POS Tags.1638the sixth type needs more explanation.
?6) Pseudo?comparison includes comparative sentences thatcompare two (or more) properties of one entitysuch as ?Smartphone-X is a computer rather than aphone.?
This type of sentence is often classifiedinto ?4) Greater or lesser.?
However, since thispaper focuses on comparisons between differententities, we separate ?6) Pseudo?
type from ?4)Greater or lesser?
type.The seventh type is ?7) Implicit?
comparison.
Itis added with the goal of covering literally?implicit?
comparisons.
For example, the sentence?Shopping Mall X guarantees no fee full refund,but Shopping Mall Y requires refund-fee?
does notdirectly compare two shopping malls.
It implicitlygives a hint that X is more beneficial to use than Y.It can be considered as a non-comparative sentencefrom a linguistic point of view.
However, weconclude that this kind of sentence is as importantas the other explicit comparisons from anengineering point of view.After defining the seven comparative types, wesimply match each sentences to a particular typebased on the CK types; e.g., a sentence whichcontains the word ???
([ga-jang]: most)?
ismatched to ?Superlative?
type.
However, a methodthat uses just the CK information has a seriousproblem.
For example, although we easily matchthe CK ???
([bo-da]: than)?
to ?Greater or lesser?without doubt, we observe that the type of CKitself does not guarantee the correct type of thesentence as we can see in the following threesentences:?
Ex2.
?X?
???
Y??
???
????
??.?
([X-eui pum-jil-eun Y-bo-da jo-chi-do na-ppeu-ji-do an-ta]: The quality of X is neither better nor worsethan that of Y.)
?
It can be interpreted as ?Thequality of X is similar to that of Y.?
(Similarity)?
Ex3.
?X?
Y??
???
??.?
([X-ga Y-bo-da pum-jil-I jo-ta]:  The quality of X is better than that ofY.)
?
It is consistent with the CK type(Greater or lesser)?
Ex4.
?X?
??
??
?????
???
??.?
([X-neun  da-reun eo-tteon ka-me-ra-bo-da pum-jil-i  jo-ta]: X is better than any other cameras inquality.)
?
It can be interpreted as ?X is thebest camera in quality.?
(Superlative)If we only rely on the CK type, we should label theabove three sentences as ?Greater or lesser?.However, each of these three sentences belongs toa different type.
This fact addresses that many CKscould have an ambiguity problem just like the CKof ???
([bo-da]: than).
?To solve this ambiguity problem, we employTBL.
We first roughly annotate the type ofsentences using the type of CK itself.
After thisinitial annotating, TBL generates a set of error-driven transformation rules, and then a scoringfunction ranks the rules.
We define our scoringfunction as Equation (1):Score(ri) = Ci - Ei                      (1)Here, ri is the i-th transformation rule, Ci is thenumber of corrected sentences after ri is applied,and Ei is the number of the opposite case.
Theranking process is executed iteratively.
Theiterations stop when the scoring function reaches acertain threshold.
We finally set up the thresholdvalue as 1 after tuning.
This means that we useonly the rules whose score is 2 or more.4 Mining Comparative Entities andPredicates (Task 2)This section explains how to extract comparativeentities and predicates.
Our strategy is to firstdetect Comparative Element candidates (CE-candidates), and then choose the answer among thecandidates.In this paper, we only present the results of twotypes: ?Greater or lesser?
and ?Superlative.?
Aswe will see in the experiment section, these twotypes cover 65.8% of whole comparative sentences.We are still studying the other five types and planto report their results soon.4.1 Comparative elementsWe extract three kinds of comparative elements inthis paper: SE, OE and PR?
Ex5.
?X???
Y????
??
???.?
([X-pa-i-gaY-pa-i-bo-da ssa-go mas-it-da]: Pie X is cheaperand more delicious than Pie Y.)?
Ex6.
???
???
?
Z ?
??
?????.?
([dai-seon hu-bo-deul jung Z-ga ga-jang mit-eum-jik-ha-da]: ?Z is the most trustworthy among thepresidential candidates.?
)1639In Ex5 sentence, ?X??
(Pie X)?
is a SE, ?Y??
(Pie Y)?
is an OE, and ???
???
(cheaper andmore delicious)?
is a PR.
In Ex6 sentence, ?Z?
is aSE, ???
???
(the presidential candidates)?
is anOE, and ??????
(trustworthy)?
is a PR.Note that comparative elements are not limitedto just one word.
For example, ???
???
(cheaper and more delicious)?
and ???
???
(thepresidential candidates)?
are composed of multiplewords.
After investigating numerous actualcomparison expressions, we conclude that SEs,OEs, and PRs should not be limited to a singleword.
It can miss a considerable amount ofimportant information to restrict comparativeelements to only one word.
Hence, we define asfollows:?
Comparative elements (SE, OE, and PR) arecomposed of one or more consecutive words.It should also be noted that a number of superlativesentences are expressed without OE.
In our corpus,the percentage of the Superlative sentences withoutany OE is close to 70%.
Hence, we define asfollows:?
OEs can be omitted in the Superlative sentences.4.2 Detecting CE-candidatesAs comparative elements are allowed to havemultiple words, we need some preprocessing stepsfor easy detection of CE-candidates.
We thus applysome simplification processes.
Through thesimplification processes, we represent potentialSEs/OEs as one ?N?
and potential PRs as one ?P?.The following process is one of the simplificationprocesses for making ?N?- Change each noun (or each noun compound) toa symbol ?N?.And, the following two example processes are for?P?.- Change ?pa (adjective)?
and ?pv (verb)?
to asymbol ?P?.- Change ?P + ecc (a suffix whose meaning is?and?)
+ P?
to one ?P?, e.g., ?cheaper andmore delicious?
is tagged as one ?P?.In addition to the above examples, severalprocesses are performed.
We regard all the ?N?s asCE-candidates for SE/OE and all the ?P?s as CE-candidates for PR.
It is possible that a moreanalytic method is used instead of thissimplification task, e.g., by a syntactic parser.
Weleave this to our future work.4.3 Finding final answersWe now generate features.
The patterns thatconsist of POS tags, CKs, and ?P?/?N?
sequenceswithin a radius of 4 POS tags from each ?N?
or?P?
are considered as features.Originalsentence?X???
Y????
??
???.?
(Pie X is cheaper and moredelicious than Pie Y.
)After POStaggingX?
?/nq + ?/jcs + Y?
?/nq +?
?/jca + ?/pa + ?/ecc + ?
?/pa +?/ef +./sfAftersimplificationprocessX?
?/N(SE) + ?/jcs +Y?
?/N(OE) + ?
?/jca +????
?/P(PR) + ./sfPatterns forSE<N(SE), jcs, N, ?
?/jca,P>, ?,<N(SE), jcs>Patterns forOE<N, jcs, N(OE), ?
?/jca,P, sf>, ?,<N(OE), ?
?/jca >Patterns forPR<N, jcs, N, ?
?/jca,P(PR), sf>, ?,<P(PR), sf>Table 1: Feature examples for mining comparativeelementsTable 1 lists some examples.
Since the CKs playan important role, they are represented as acombination of their lexicalization and POS tag.After feature generation, we calculate eachprobability value of all CE-candidates using SVM.For example, if a sentence has three ?P?s, one ?P?with the highest probability value is selected as theanswer PR.5 Experimental Evaluation5.1 Experimental SettingsThe experiments are conducted on 7,384 sentencescollected from the web by three trained humanlabelers.
Firstly, two labelers annotated the corpus.A Kappa value of 0.85 showed that it was safe tosay that the two labelers agreed in their judgments.1640Secondly, the third labeler annotated theconflicting part of the corpus.
All three labelersdiscussed any conflict, and finally reached anagreement.
Table 2 lists the distribution of thecorpus.ComparativeTypesSentencePortionNon-comparative: 5,001 (67.7%)Comparative: 2,383 (32.3%)Total (Corpus) 7,384 (100%)AmongComparativeSentences1) Equality 3.6%2) Similarity 7.2%3) Difference 4.8%4) Greater or lesser 54.5%5) Superlative 11.3%6) Pseudo  1.3%7) Implicit 17.5%Total (Comparative) 100%Table 2: Distribution of the corpus5.2 Classifying comparative sentencesOur experimental results for Task 1 showed an f1-score of 90.23% in extracting comparativesentences from text documents and an accuracy of81.67% in classifying the comparative sentencesinto seven comparative types.The integrated results showed an accuracy of88.59%.
Non-comparative sentences were regardedas an eighth comparative type in this integratedresult.
It means that we classify entire sentencesinto eight types (seven comparative types and onenon-comparative type).5.2.1   Extracting comparative sentences.Before evaluating our proposed method forcomparative sentence extraction, we conductedfour experiments with all of the lexical unigramsand bigrams using MEM and SVM.
Among thesefour cases, SVM with lexical unigrams showed thehighest performance, an f1-score of 79.49%.
Weregard this score as our baseline performance.Next, we did experiments using all of thecontinuous lexical sequences and using all of thePOS tags sequences within a radius of n wordsfrom each CK as features (n=1,2,3,4,5).
Amongthese ten cases, ?the POS tags sequences within aradius of 3?
showed the best performance.
Besides,as SVM showed the better performance than MEMin overall experiments, we employ SVM as ourproposed learning technique.
Table 3 summarizesthe overall results.Systems Precision Recall F1-scorebaseline 87.86 72.57 79.49comparison lexicononly68.39 95.96 79.87comparison lexicon& SVM(proposed)92.24 88.31 90.23Table 3: Final results in comparative sentenceextraction (%)As given above, we successfully detected CS-candidates with considerably high recall by usingthe comparison lexicon.
We also successfullyfiltered the candidates with high precision whilestill preserving high recall by applying machinelearning technique.
Finally, we could achieve anoutstanding performance, an f1-score of 90.23%.5.2.2   Classifying comparative sentences intoseven types.Like the previous comparative sentence extractiontask, we also conducted experiments for typeclassification using the same features (continuousPOS tags sequences within a radius of 3 wordsfrom each CK) and the same learning technique(SVM).
Here, we achieved an accuracy of 73.64%.We regard this score as our baseline performance.Next, we tested a completely different technique,the TBL method.
TBL is well-known to berelatively strong in sparse problems.
We observedthat the performance of type classification can beinfluenced by very subtle differences in manycases.
Hence, we think that an error-drivenapproach can perform well in comparative typeclassification.
Experimental results showed thatTBL actually performed better than SVM or MEM.In the first step, we roughly annotated the typeof a sentence using the type of the CK itself.
Then,we generated error-driven transformation rulesfrom the incorrectly annotated sentences.Transformation templates we defined are given inTable 4.
Numerous transformation rules weregenerated on the basis of the templates.
Forexample, ?Change the type of the current sentencefrom ?Greater or lesser?
to ?Superlative?
if thissentence holds the CK of ???
([bo-da]: than)?,1641and the second preceding word of the CK is taggedas mm?
is a transformation rule generated by thethird template.Change the type of the current sentence from x to y ifthis sentence holds the CK of k, and ?1.
the preceding word of k is tagged z.2.
the following word of k is tagged z.3.
the second preceding word of k is tagged z.4.
the second following word of k is tagged z.5.
the preceding word of k is tagged z, and thefollowing word of k is tagged w.6.
the preceding word of k is tagged z, and thesecond preceding word of k is tagged w.7.
the following word of k is tagged z, and thesecond following word of k is tagged w.Table 4: Transformation templatesFor evaluation of threshold values, weperformed experiments with three options as givenin Table 5.Threshold 0 1 2Accuracy 79.99 81.67 80.04Table 5: Evaluation of threshold option (%);Threshold n means that the learning iterations continues whileCi-Ei ?
n+1We achieved the best performance with thethreshold option 1.
Finally, we classifiedcomparative sentences into seven types using TBLwith an accuracy of 81.67%.5.2.3   Integrated results of Task 1We sum up our proposed method for Task 1 as twosteps as follows;1) The comparison lexicon detects CS-candidatesin text documents, and then SVM eliminatesthe non-comparative sentences from thecandidates.
Thus, all of the sentences aredivided into two classes: a comparative classand a non-comparative class.2) TBL then classifies the sentences placed in thecomparative class in the previous step intoseven comparative types.The integrated results showed an overall accuracyof 88.59% for the eight-type classification.
Toevaluate the effectiveness of our two-stepprocessing, we performed one-step processingexperiments using SVM and TBL.
Table 6 shows acomparison of the results.Processing AccuracyOne-stepprocessing(classifying eighttypes at a time)comparisonlexicon & SVM75.64comparisonlexicon & TBL72.49Two-step processing(proposed)88.59Table 6: Integrated results for Task 1 (%)As shown above, Task 1 was successfully dividedinto two steps.5.3 Mining comparative entities andpredicatesFor the mining task of comparative entities andpredicates, we used 460 comparative sentences(Greater or lesser: 300, Superlative: 160).
Aspreviously mentioned, we allowed multiple-wordcomparative elements.
Table 7 lists the portion ofmultiple-word comparative elements.Multi-word rate SE OE PRGreater or lesser 30.0 31.3 8.3Superlative 24.49.4(32.6)8.1Table 7: Portion (%) of multiple-word comparativeelementsAs given above, each multiple-word portion,especially in SEs and OEs, is quite high.
This factproves that it is absolutely necessary to allowmultiple-word comparative elements.
Relativelylower rate of 9.4% in Superlative-OEs is caused bya number of omitted OEs.
If sentences that do nothave any OEs are excluded, the portion ofmultiple-words becomes 32.6% as written inparentheses.Table 8 shows the effectiveness of simplificationprocesses.
We calculated the error rates of CE-candidate detection before and after simplificationprocesses.1642SimplificationprocessesSE OE PRGreater orlesserBefore 34.7 39.3 10.0After 4.7 8.0 1.7SuperlativeBefore 26.385.0(38.9)9.4After 1.975.6(6.3)1.3Table 8: Error rate (%) in CE-candidate detectionHere, the first value of 34.7% means that the realSEs of 104 sentences (among total 300 Greater orlesser sentences) were not detected by CE-candidate detection before simplification processes.After the processes, the error rate decreased to4.7%.
The significant differences between beforeand after indicate that we successfully detect CE-candidates through the simplification processes.Although the Superlative-OEs still show theseriously high rate of 75.6%, it is also caused by anumber of omitted OEs.
If sentences that do nothave any OEs are excluded, the error rate is only6.3% as written in parentheses.The final results for Task 2 are reported in Table9.
We calculated each probability of CE-candidatesusing MEM and SVM.
Both MEM and SVMshowed outstanding performance; there was nosignificant difference between the two machinelearning methods (SVM and MEM).
Hence, weonly report the results of SVM.
Note that manysentences do not contain any OE.
To identify suchsentences, if SVM tagged every ?N?
in a sentenceas ?not OE?, we tagged the sentence as ?no OE?.Final Results SE OE PRGreater or lesser 86.00 89.67 92.67Superlative 84.38 71.25 90.00Total 85.43 83.26 91.74Table 9: Final results of Task 2 (Accuracy, %)As shown above, we successfully extracted thecomparative entities and predicates withoutstanding performance, an overall accuracy of86.81%.6 Conclusions and Future WorkThis paper has studied a Korean comparisonmining system.
Our proposed system achieved anaccuracy of 88.59% for classifying comparativesentences into eight types (one non-comparativetype and seven comparative types), and anaccuracy of 86.81% for mining comparativeentities and predicates.
These results demonstratedthat our proposed method could be used effectivelyin practical applications.
Since the comparisonmining is an area of increasing interest around theworld, our study can contribute greatly to textmining research.In our future work, we have the following plans.Our first plan is to complete the mining process onall the types of sentences.
The second one is toconduct more experiments for obtaining betterperformance.
The final one is about an integratedsystem.
Since we perform Task 1 and Task 2separately, we need to build an end-to-end system.AcknowledgmentThis research was supported by Basic ScienceResearch Program through the National ResearchFoundation of Korea (NRF) funded by theMinistry of Education, Science and Technology(2010-0015613)ReferencesAdam L. Berger, Stephen A. Della Pietra and Vicent J.Della Pietra.
1996.
A Maximum Entropy Approachto Natural Language Processing.
ComputationalLinguistics, 22(1):39-71.William J.
Black and Argyrios Vasilakopoulos.
2002.Language-Independent named Entity Classificationby modified Transformation-based Learning and byDecision Tree Induction.
In Proceedings ofCoNLL?02, 24:1-4.Eric Brill.
1992.
A simple rule-based part of speechtagger.
In Proceedings of ANLP?92, 152-155.Eric Brill.
1995.
Transformation-based Error-DrivenLearning and Natural language Processing: A CaseStudy in Part-of-Speech tagging.
ComputationalLinguistics, 543-565.Gil-jong Ha.
1999a.
Korean Modern ComparativeSyntax, Pijbook Press, Seoul, Korea.Gil-jong Ha.
1999b.
Research on Korean EqualityComparative Syntax, Association for KoreanLinguistics, 5:229-265.In-su Jeong.
2000.
Research on Korean AdjectiveSuperlative Comparative Syntax.
Korean Han-min-jok Eo-mun-hak, 36:61-86.1643Nitin Jindal and Bing Liu.
2006.
IdentifyingComparative Sentences in Text Documents, InProceedings of SIGIR?06, 244-251.Nitin Jindal and Bing Liu.
2006.
Mining ComparativeSentences and Relations, In Proceedings of AAAI?06,1331-1336.Thorsten Joachims.
1998.
Text Categorization withSupport Vector Machines: Learning with Manyrelevant Features.
In Proceedings of ECML?98, 137-142Soomin Kim and Eduard Hovy.
2006.
AutomaticDetection of Opinion Bearing Words and Sentences.In Proceedings of ACL?06.Dong-joo Lee, OK-Ran Jeong and Sang-goo Lee.
2008.Opinion Mining of Customer Feedback Data on theWeb.
In Proceedings of ICUIMC?08, 247-252.Shasha Li, Chin-Yew Lin, Young-In Song and ZhoujunLi.
2010.
Comparable Entity Mining fromComparative Questions.
In Proceedings of ACL?10,650-658.Kyeong-sook Oh.
2004.
The Difference between ?Man-kum?
Comparative and ?Cheo-rum?
Comparative.Society of Korean Semantics, 14:197-221.Lance A. Ramshaw and Mitchell P. Marcus.
1995.
TextChunking using Transformation-Based Learning.
InProceedings of NLP/VLC?95, 82-94.Ellen Riloff and Janyce Wiebe.
2003.
LearningExtraction Patterns for Subjective Expressions.
InProceedings of EMNLP?03.Seon Yang and Youngjoong Ko.
2009.
ExtractingComparative Sentences from Korean TextDocuments Using Comparative Lexical Patterns andMachine Learning Techniques.
In Proceedings ofACL-IJNLP:Short Papers, 153-156Seon Yang and Youngjoong Ko.
2011.
Finding relevantfeatures for Korean comparative sentence extraction.Pattern Recognition Letters, 32(2):293-2961644
