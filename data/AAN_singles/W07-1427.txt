Proceedings of the Workshop on Textual Entailment and Paraphrasing, pages 165?170,Prague, June 2007. c?2007 Association for Computational LinguisticsLearning Alignments and Leveraging Natural LogicNathanael Chambers, Daniel Cer, Trond Grenager, David Hall, Chloe KiddonBill MacCartney, Marie-Catherine de Marneffe, Daniel RamageEric Yeh, Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{natec,dcer,grenager,dlwh,loeki,wcmac,mcdm,dramage,yeh1,manning}@stanford.eduAbstractWe describe an approach to textual infer-ence that improves alignments at both thetyped dependency level and at a deeper se-mantic level.
We present a machine learningapproach to alignment scoring, a stochas-tic search procedure, and a new tool thatfinds deeper semantic alignments, allowingrapid development of semantic features overthe aligned graphs.
Further, we describe acomplementary semantic component basedon natural logic, which shows an added gainof 3.13% accuracy on the RTE3 test set.1 IntroductionAmong the many approaches to textual inference,alignment of dependency graphs has shown utilityin determining entailment without the use of deepunderstanding.
However, discovering alignmentsrequires a scoring function that accurately scoresalignment and a search procedure capable of approx-imating the optimal mapping within a large searchspace.
We address the former requirement througha machine learning approach for acquiring lexicalfeature weights, and we address the latter with anapproximate stochastic approach to search.Unfortunately, the most accurate aligner can-not capture deeper semantic relations between twopieces of text.
For this, we have developed a tool,Semgrex, that allows the rapid development of de-pendency rules to find specific entailments, such asfamilial or locative relations, a common occurencein textual entailment data.
Instead of writing code byhand to capture patterns in the dependency graphs,we develop a separate rule-base that operates overaligned dependency graphs.
Further, we describe aseparate natural logic component that complementsour textual inference system, making local entail-ment decisions based on monotonic assumptions.The next section gives a brief overview of the sys-tem architecture, followed by our proposal for im-proving alignment scoring and search.
New coref-erence features and the Semgrex tool are then de-scribed, followed by a description of natural logic.2 System OverviewOur system is a three stage architecture that con-ducts linguistic analysis, builds an alignment be-tween dependency graphs of the text and hypothesis,and performs inference to determine entailment.Linguistic analysis identifies semantic entities, re-lationships, and structure within the given text andhypothesis.
Typed dependency graphs are passedto the aligner, as well as lexical features such asnamed entities, synonymity, part of speech, etc.
Thealignment stage then performs dependency graphalignment between the hypothesis and text graphs,searching the space of possible alignments for thehighest scoring alignment.
Improvements to thescorer, search algorithm, and automatically learnedweights are described in the next section.The final inference stage determines if the hy-pothesis is entailed by the text.
We construct a setof features from the previous stages ranging fromantonyms and polarity to graph structure and seman-tic relations.
Each feature is weighted according to aset of hand-crafted or machine-learned weights over165the development dataset.
We do not describe the fea-tures here; the reader is referred to de Marneffe et al(2006a) for more details.
A novel component thatleverages natural logic is also used to make the finalentailment decisions, described in section 6.3 Alignment ModelWe examine three tasks undertaken to improve thealignment phase: (1) the construction of manu-ally aligned data which enables automatic learningof alignment models, and effectively decouples thealignment and inference development efforts, (2) thedevelopment of new search procedures for findinghigh-quality alignments, and (3) the use of machinelearning techniques to automatically learn the pa-rameters of alignment scoring models.3.1 Manual Alignment AnnotationWhile work such as Raina et al (2005) has triedto learn feature alignment weights by credit assign-ment backward from whether an item is answeredcorrectly, this can be very difficult, and here we fol-low Hickl et al (2006) in using supervised gold-standard alignments, which help us to evaluate andimprove alignment and inference independently.We built a web-based tool that allows annotatorsto mark semantic relationships between text and hy-pothesis words.
A table with the hypothesis wordson one axis and the text on the other allows re-lationships to be marked in the corresponding ta-ble cell with one of four options.
These relation-ships include text to hypothesis entailment, hypothe-sis to text entailment, synonymy, and antonymy.
Ex-amples of entailment (from the RTE 2005 dataset)include pairs such as drinking/consumption, coro-navirus/virus, and Royal Navy/British.
By distin-guishing between these different types of align-ments, we can capture some limited semantics in thealignment process, but full exploitation of this infor-mation is left to future work.We annotated the complete RTE2 dev andRTE3 dev datasets, for a total of 1600 alignedtext/hypothesis pairs (the data is available athttp://nlp.stanford.edu/projects/rte/).3.2 Improving Alignment SearchIn order to find ?good?
alignments, we define both aformal model for scoring the quality of a proposedalignment and a search procedure over the alignmentspace.
Our goal is to build a model that maximizesthe total alignment score of the full datasetD, whichwe take to be the sum of the alignment scores for allindividual text/hypothesis pairs (t, h).Each of the text and hypothesis is a semantic de-pendency graph; n(h) is the set of nodes (words)and e(h) is the set of edges (grammatical relations)in a hypothesis h. An alignment a : n(h) 7?
n(t) ?
{null} maps each hypothesis word to a text wordor to a null symbol, much like an IBM-style ma-chine translation model.
We assume that the align-ment score s(t, h, a) is the sum of two terms, the firstscoring aligned word pairs and the second the matchbetween an edge between two words in the hypoth-esis graph and the corresponding path between thewords in the text graph.
Each of these is a sum, overthe scoring function for individual word pairs sw andthe scoring function for edge path pairs se:s(t, h, a) =?hi?n(h)sw(hi, a(hi))+?
(hi,hj)?e(h)se((hi, hj), (a(hi), a(hj)))The space of alignments for a hypothesis with mwords and a text with n words contains (n + 1)mpossible alignments, making exhaustive search in-tractable.
However, since the bulk of the alignmentscore depends on local factors, we have exploredseveral search strategies and found that stochasticlocal search produces better quality solutions.Stochastic search is inspired by Gibbs samplingand operates on a complete state formulation of thesearch problem.
We initialize the algorithm with thecomplete alignment that maximizes the greedy wordpair scores.
Then, in each step of the search, weseek to randomly replace an alignment for a singlehypothesis word hi.
For each possible text word tj(including null), we compute the alignment score ifwe were to align hi with tj .
Treating these scores aslog probabilities, we create a normalized distributionfrom which we sample one alignment.
This Gibbssampler is guaranteed to give us samples from theposterior distribution over alignments defined im-plicitly by the scoring function.
As we wish to find amaximum of the function, we use simulated anneal-ing by including a temperature parameter to smooth166the sampling distribution as a function of time.
Thisallows us to initially explore the space widely, butlater to converge to a local maximum which is hope-fully the global maximum.3.3 Learning Alignment ModelsLast year, we manually defined the alignment scor-ing function (de Marneffe et al, 2006a).
However,the existence of the gold standard alignments de-scribed in section 3.1 enables the automatic learningof a scoring function.
For both the word and edgescorers, we choose a linear model where the score isthe dot product of a feature and a weight vector:sw(hi, tj) = ?w ?
f(hi, tj), andse((hi, hj), (tk, t`)) = ?e ?
f((hi, hj), (tk, t`)).Recent results in machine learning show the ef-fectiveness of online learning algorithms for struc-ture prediction tasks.
Online algorithms update theirmodel at each iteration step over the training set.
Foreach datum, they use the current weight vector tomake a prediction which is compared to the correctlabel.
The weight vector is updated as a functionof the difference.
We compared two different up-date rules: the perceptron update and the MIRA up-date.
In the perceptron update, for an incorrect pre-diction, the weight vector is modified by adding amultiple of the difference between the feature vectorof the correct label and the feature vector of the pre-dicted label.
We use the adaptation of this algorithmto structure prediction, first proposed by (Collins,2002).
TheMIRA update is a proposed improvementthat attempts to make the minimal modification tothe weight vector such that the score of the incorrectprediction for the example is lower than the score ofthe correct label (Crammer and Singer, 2001).We compare the performance of the perceptronand MIRA algorithms on 10-fold cross-validationon the RTE2 dev dataset.
Both algorithms improvewith each pass over the dataset.
Most improve-ment is within the first five passes.
Table 1 showsruns for both algorithms over 10 passes through thedataset.
MIRA consistently outperforms perceptronlearning.
Moreover, scoring alignments based on thelearned weights marginally outperforms our hand-constructed scoring function by 1.7% absolute.A puzzling problem is that our overall per-formance decreased 0.87% with the addition ofPerfectly alignedIndividual words Text/hypothesis pairsPerceptron 4675 271MIRA 4775 283Table 1: Perceptron and MIRA results on 10-fold cross-validation on RTE2 dev for 10 passes.RTE3 dev alignment data.
We believe this is dueto a larger proportion of ?irrelevant?
and ?relation?pairs.
Irrelevant pairs are those where the text andhypothesis are completely unrelated.
Relation pairsare those where the correct entailment judgment re-lies on the extraction of relations such as X worksfor Y, X is located in Y, or X is the wife of Y. Bothof these categories do not rely on alignments for en-tailment decisions, and hence introduce noise.4 CoreferenceIn RTE3, 135 pairs in RTE3 dev and 117 inRTE3 test have lengths classified as ?long,?
with642 personal pronouns identified in RTE3 dev and504 in RTE3 test.
These numbers suggest that re-solving pronomial anaphora plays an important rolein making good entailment decisions.
For exam-ple, identifying the first ?he?
as referring to ?Yunus?in this pair from RTE3 dev can help alignment andother system features.P: Yunus, who shared the 1.4 million prize Friday with theGrameen Bank that he founded 30 years ago, pioneered the con-cept of ?microcredit.
?H: Yunus founded the Grameen Bank 30 years ago.Indeed, 52 of the first 200 pairs from RTE3 devwere deemed by a human evaluator to rely on ref-erence information.
We used the OpenNLP1 pack-age?s maximum-entropy coreference utility to per-form resolution on parse trees and named-entity datafrom our system.
Found relations are stored andused by the alignment stage for word similarity.We evaluated our system with and without coref-erence over RTE3 dev and RTE3 test.
Results areshown in Table 3.
The presence of reference infor-mation helped, approaching significance on the de-velopment set (p < 0.1, McNemar?s test, 2-tailed),but not on the test set.
Examination of alignmentsand features between the two runs shows that thealignments do not differ significantly, but associated1http://opennlp.sourceforge.net/167weights do, thus affecting entailment threshold tun-ing.
We believe coreference needs to be integratedinto all the featurizers and lexical resources, ratherthan only with word matching, in order to make fur-ther gains.5 Semgrex LanguageA core part of an entailment system is the ability tofind semantically equivalent patterns in text.
Pre-viously, we wrote tedious graph traversal code byhand for each desired pattern.
As a remedy, wewrote Semgrex, a pattern language for dependencygraphs.
We use Semgrex atop the typed dependen-cies from the Stanford Parser (de Marneffe et al,2006b), as aligned in the alignment phase, to iden-tify both semantic patterns in a single text and overtwo aligned pieces of text.
The syntax of the lan-guage was modeled after tgrep/Tregex, query lan-guages used to find syntactic patterns in trees (Levyand Andrew, 2006).
This speeds up the process ofgraph search and reduces errors that occur in com-plicated traversal code.5.1 Semgrex FeaturesRather than providing regular expression match-ing of atomic tree labels, as in most tree patternlanguages, Semgrex represents nodes as a (non-recursive) attribute-value matrix.
It then uses regularexpressions for subsets of attribute values.
For ex-ample, {word:run;tag:/?NN/} refers to anynode that has a value run for the attribute word anda tag that starts with NN, while {} refers to any nodein the graph.However, the most important part of Semgrex isthat it allows you to specify relations between nodes.For example, {} <nsubj {} finds all the depen-dents of nsubj relations.
Logical connectives canbe used to form more complex patterns and nodenaming can help retrieve matched nodes from thepatterns.
Four base relations, shown in figure 1, al-low you to specify the type of relation between twonodes, in addition to an alignment relation (@) be-tween two graphs.5.2 Entailment PatternsA particularly useful application of Semgrex is tocreate relation entailment patterns.
In particular, theIE subtask of RTE has many characteristics that areSemgrex RelationsSymbol #Description{A} >reln {B} A is the governor of a reln relationwith B{A} <reln {B} A is the dependent of a reln relationwith B{A} >>reln {B} A dominates a node that is thegovernor of a reln relation with B{A} <<reln {B} A is the dependent of a node that isdominated by B{A} @ {B} A aligns to BFigure 1: Semgrex relations between nodes.not well suited to the core alignment features of oursystem.
We began integrating Semgrex into our sys-tem by creating semantic alignment rules for theseIE tasks.T: Bill Clinton?s wife Hillary was in Wichita today, continuingher campaign.H: Bill Clinton is married to Hillary.
(TRUE)Pattern:({}=1<nsubjpass ({word:married} >pp to {}=2))@ ({} >poss ({lemma:/wife/} >appos {}=3))This is a simplified version of a pattern that looksfor marriage relations.
If it matches, additional pro-grammatic checks ensure that the nodes labeled 2and 3 are either aligned or coreferent.
If they are,then we add a MATCH feature, otherwise we add aMISMATCH.
Patterns included other familial rela-tions and employer-employee relations.
These pat-terns serve both as a necessary component of an IEentailment system and as a test drive of Semgrex.5.3 Range of ApplicationOur rules for marriage relations correctly matchedsix examples in the RTE3 development set and onein the test set.
Due to our system?s weaker per-formance on the IE subtask of the data, we ana-lyzed 200 examples in the development set for Sem-grex applicability.
We identified several relationalclasses, including the following:?
Work: works for, holds the position of?
Location: lives in, is located in?
Relative: wife/husband of, are relatives?
Membership: is an employee of, is part of?
Business: is a partner of, owns?
Base: is based in, headquarters inThese relations make up at least 7% of the data, sug-gesting utility from capturing other relations.1686 Natural LogicWe developed a computational model of naturallogic, the NatLog system, as another inference en-gine for our RTE system.
NatLog complements ourcore broad-coverage system by trading lower recallfor higher precision, similar to (Bos and Markert,2006).
Natural logic avoids difficulties with translat-ing natural language into first-order logic (FOL) byforgoing logical notation and model theory in favorof natural language.
Proofs are expressed as incre-mental edits to natural language expressions.
Editsrepresent conceptual contractions and expansions,with truth preservation specified natural logic.
Forfurther details, we refer the reader to (Sa?nchez Va-lencia, 1995).We define an entailment relation v betweennouns (hammer v tool), adjectives (deafening vloud), verbs (sprint v run), modifiers, connectivesand quantifiers.
In ordinary (upward-monotone)contexts, the entailment relation between compoundexpressions mirrors the entailment relations be-tween their parts.
Thus tango in Paris v dancein France, since tango v dance and in Paris v inFrance.
However, many linguistic constructions cre-ate downward-monotone contexts, including nega-tion (didn?t sing v didn?t yodel), restrictive quanti-fiers (few beetles v few insects) and many others.NatLog uses a three-stage architecture, compris-ing linguistic pre-processing, alignment, and entail-ment classification.
In pre-processing, we define alist of expressions that affect monotonicity, and de-fine Tregex patterns that recognize each occurrenceand its scope.
This monotonicity marking can cor-rectly account for multiple monotonicity inversions,as in no soldier without a uniform, and marks eachtoken span with its final effective monotonicity.In the second stage, word alignments from ourRTE system are represented as a sequence of atomicedits over token spans, as entailment relationsare described across incremental edits in NatLog.Aligned pairs generate substitution edits, unalignedpremise words yield deletion edits, and unalignedhypothesis words yield insertion edits.
Where pos-sible, contiguous sequences of word-level edits arecollected into span edits.In the final stage, we use a decision-tree classi-fier to predict the elementary entailment relation (ta-relation symbol in terms of v RTEequivalent p = h p v h, h v p yesforward p < h p v h, h 6v p yesreverse p = h h v p, p 6v h noindependent p # h p 6v h, h 6v p noexclusive p | h p v ?h, h v ?p noTable 2: NatLog?s five elementary entailment relations.
The lastcolumn indicates correspondences to RTE answers.ble 2) for each atomic edit.
Edit features includethe type, effective monotonicity at affected tokens,and their lexical features, including syntactic cate-gory, lemma similarity, and WordNet-derived mea-sures of synonymy, hyponymy, and antonymy.
Theclassifier was trained on a set of 69 problems de-signed to exercise the feature space, learning heuris-tics such as deletion in an upward-monotone contextyields<, substitution of a hypernym in a downward-monotone context yields =, and substitution of anantonym yields |.To produce a top-level entailment judgment, theatomic entailment predictions associated with eachedit are composed in a fairly obvious way.
If r is anyentailment relation, then (= ?
r) ?
r, but (# ?
r) ?#.
< and= are transitive, but (< ?
=) ?
#, and soon.We do not expect NatLog to be a general-purposesolution for RTE problems.
Many problems dependon types of inference that it does not address, suchas paraphrase or relation extraction.
Most pairs havelarge edit distances, and more atomic edits meansa greater chance of errors propagating to the finaloutput: given the entailment composition rules, thesystem can answer yes only if all atomic-level pre-dictions are either< or =.
Instead, we hope to makereliable predictions on a subset of the RTE problems.Table 3 shows NatLog performance on RTE3.
Itmakes positive predictions on few problems (18%on development set, 24% on test), but achieves goodprecision relative to our RTE system (76% and 68%,respectively).
For comparison, the FOL-based sys-tem reported in (Bos and Markert, 2006) attained aprecision of 76% on RTE2, but made a positive pre-diction in only 4% of cases.
This high precision sug-gests that superior performance can be achieved byhybridizing NatLog with our core RTE system.The reader is referred to (MacCartney and Man-169ID Premise(s) Hypothesis Answer518 The French railway company SNCF is cooperating inthe project.The French railway company is called SNCF.
yes601 NUCOR has pioneered a giant mini-mill in which steelis poured into continuous casting machines.Nucor has pioneered the first mini-mill.
noTable 4: Illustrative examples from the RTE3 test suiteRTE3 Development Set (800 problems)System % yes precision recall accuracyCore +coref 50.25 68.66 66.99 67.25Core -coref 49.88 66.42 64.32 64.88NatLog 18.00 76.39 26.70 58.00Hybrid, bal.
50.00 69.75 67.72 68.25Hybrid, opt.
55.13 69.16 74.03 69.63RTE3 Test Set (800 problems)System % yes precision recall accuracyCore +coref 50.00 61.75 60.24 60.50Core -coref 50.00 60.25 58.78 59.00NatLog 23.88 68.06 31.71 57.38Hybrid, bal.
50.00 64.50 62.93 63.25Hybrid, opt.
54.13 63.74 67.32 63.62Table 3: Performance on the RTE3 development and test sets.% yes indicates the proportion of yes predictions made by thesystem.
Precision and recall are shown for the yes label.ning, 2007) for more details on NatLog.7 System ResultsOur core systemmakes yes/no predictions by thresh-olding a real-valued inference score.
To constructa hybrid system, we adjust the inference score by+x if NatLog predicts yes, ?x otherwise.
x is cho-sen by optimizing development set accuracy whenadjusting the threshold to generate balanced predic-tions (equal numbers of yes and no).
As anotherexperiment, we fix x at this value and adjust thethreshold to optimize development set accuracy, re-sulting in an excess of yes predictions.
Results forthese two cases are shown in Table 3.
Parametervalues tuned on development data yielded the bestperformance.
The optimized hybrid system attainedan absolute accuracy gain of 3.12% over our RTEsystem, corresponding to an extra 25 problems an-swered correctly.
This result is statistically signifi-cant (p < 0.01, McNemar?s test, 2-tailed).The gain cannot be fully attributed to NatLog?ssuccess in handling the kind of inferences aboutmonotonicity which are the staple of natural logic.Indeed, such inferences are quite rare in the RTEdata.
Rather, NatLog seems to have gained primarilyby being more precise.
In some cases, this precisionworks against it: NatLog answers no to problem 518(table 4) because it cannot account for the insertionof called.
On the other hand, it correctly rejects thehypothesis in problem 601 because it cannot accountfor the insertion of first, whereas the less-precisecore system was happy to allow it.AcknowledgementsThis material is based upon work supported inpart by the Disruptive Technology Office (DTO)?sAQUAINT Phase III Program.ReferencesJohan Bos and Katja Markert.
2006.
When logical inferencehelps determining textual entailment (and when it doesn?t).In Proceedings of the Second PASCAL RTE Challenge.Michael Collins.
2002.
Discriminative training methods forhidden markov models: Theory and experiments with per-ceptron algorithms.
In Proceedings of EMNLP-2002.Koby Crammer and Yoram Singer.
2001.
Ultraconservativeonline algorithms for multiclass problems.
In Proceedingsof COLT-2001.Marie-Catherine de Marneffe, Bill MacCartney, Trond Grena-ger, Daniel Cer, Anna Rafferty, and Christopher D. Manning.2006a.
Learning to distinguish valid textual entailments.
InSecond Pascal RTE Challenge Workshop.Marie-Catherine de Marneffe, Bill MacCartney, and Christo-pher D. Manning.
2006b.
Generating typed dependencyparses from phrase structure parses.
In 5th Int.
Conferenceon Language Resources and Evaluation (LREC 2006).Andrew Hickl, John Williams, Jeremy Bensley, Kirk Roberts,Bryan Rink, and Ying Shi.
2006.
Recognizing textual entail-ment with LCC?s GROUNDHOG system.
In Proceedings ofthe Second PASCAL RTE Challenge.Roger Levy and Galen Andrew.
2006.
Tregex and Tsurgeon:tools for querying and manipulating tree data structures.
InProceedings of the Fifth International Conference on Lan-guage Resources and Evaluation.Bill MacCartney and Christopher D. Manning.
2007.
Natu-ral logic for textual inference.
In ACL Workshop on TextualEntailment and Paraphrasing.Rajat Raina, Andrew Y. Ng, and Christopher D. Manning.
2005.Robust textual inference via learning and abductive reason-ing.
In AAAI 2005, pages 1099?1105.Victor Sa?nchez Valencia.
1995.
Parsing-driven inference: Nat-ural logic.
Linguistic Analysis, 25:258?285.170
