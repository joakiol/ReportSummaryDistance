Coling 2008: Proceedings of the workshop on Multi-source Multilingual Information Extraction and Summarization, page 1Manchester, August 2008Generating Image Captions using Topic Focused Multi-documentSummarizationRobert GaizauskasNatural Language Processing GroupDepartment of Computer Science, University of SheffieldRegent Court, 211 Portobello, Sheffield, S1 4DP, UKR.Gaizauskas@sheffield.ac.ukIn the near future digital cameras will comestandardly equipped with GPS and compass andwill automatically add global position and direc-tion information to the metadata of every picturetaken.
Can we use this information, together withinformation from geographical information sys-tems and the Web more generally, to caption im-ages automatically?This challenge is being pursued in the TRIPODproject (http://tripod.shef.ac.uk/) and in this talkI will address one of the subchallenges this topicraises: given a set of toponyms automatically gen-erated from geo-data associated with an image, canwe use these toponyms to retrieve documents fromthe Web and to generate an appropriate caption forthe image?We begin assuming the toponyms name the prin-cipal objects or scene contents in the image.
Usingweb resources (e.g.
Wikipedia) we attempt to de-termine the types of these things ?
is this a pictureof church?
a mountain?
a city?
We have con-structed a taxonomy of such image content typesusing on-line collections of captioned images andfor each type in the taxonomy we have constructedseveral collections of texts describing that type.For example, we have a collection of captions de-scribing churches and a collection of Wiki pagesdescribing churches.
The intuition here is thatthese collections are examples of, e.g.
the sortsof things people say in captions or in descriptionsof churches.
These collections can then be used toderive models of objects or scene types which inturn can be used to bias or focus multi-documentsummaries of new images of things of the samec?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.type.In the talk I report results of work we havecarried out to explore the hypothesis underlyingthis approach, namely that brief multi-documentsummaries generated as image captions by usingmodels of object/scene types to bias or focus con-tent selection will be superior to generic multi-document summaries generated for this purpose.I describe how we have constructed an image con-tent taxonomy, how we have derived text collec-tions for object/scene types, how we have derivedobject/scene type models from these collectionsand how these have been used in multi-documentsummarization.
I also discuss the issue of how toevaluate the resulting captions and present prelim-inary results from one sort of evaluation.1
