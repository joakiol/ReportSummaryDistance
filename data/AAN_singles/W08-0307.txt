Proceedings of the Third Workshop on Statistical Machine Translation, pages 53?61,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUsing Shallow Syntax Informationto Improve Word Alignment and Reordering for SMTJosep M. CregoTALP Research CenterUniversitat Polite`cnica de Catalunya08034 Barcelona, Spainjmcrego@gps.tsc.upc.eduNizar HabashCenter for Computational Learning SystemsColumbia UniversityNew York, NY 10115, USAhabash@ccls.columbia.eduAbstractWe describe two methods to improve SMTaccuracy using shallow syntax information.First, we use chunks to refine the set of wordalignments typically used as a starting point inSMT systems.
Second, we extend an N -gram-based SMT system with chunk tags to betteraccount for long-distance reorderings.
Exper-iments are reported on an Arabic-English taskshowing significant improvements.
A humanerror analysis indicates that long-distance re-orderings are captured effectively.1 IntroductionMuch research has been done on using syntactic in-formation in statistical machine translation (SMT).In this paper we use chunks (shallow syntax infor-mation) to improve an N -gram-based SMT system.We tackle both the alignment and reordering prob-lems of a language pair with important differencesin word order (Arabic-English).
These differenceslead to noisy word alignments, which lower the ac-curacy of the derived translation table.
Addition-ally, word order differences, especially those span-ning long distances and/or including multiple levelsof reordering, are a challenge for SMT decoding.Two improvements are presented here.
First, wereduce the number of noisy alignments by using theidea that chunks, like raw words, have a transla-tion correspondence in the source and target sen-tences.
Hence, word links are constrained (i.e.,noisy links are pruned) using chunk information.Second, we introduce rewrite rules which can han-dle both short/medium and long distance reorder-ings as well as different degrees of recursive applica-tion.
We build our rules with two different linguisticannotations, (local) POS tags and (long-spanning)chunk tags.
Despite employing an N -gram-basedSMT system, the methods described here can alsobe applied to any phrase-based SMT system.
Align-ment and reordering are similarly used in both ap-proaches.In Section 2 we discuss previous related work.
InSection 3, we discuss Arabic linguistic issues andmotivate some of our decisions.
In Section 4, wedescribe the N -gram based SMT system which weextend in this paper.
Sections 5 and 6 detail the maincontributions of this work.
In Section 7, we carry outevaluation experiments reporting on the accuracy re-sults and give details of a human evaluation erroranalysis.2 Related WorkIn the SMT community, it is widely accepted thatthere is a need for structural information to accountfor differences in word order between different lan-guage pairs.
Structural information offers a greaterpotential to learn generalizations about relationshipsbetween languages than flat-structure models.
Theneed for these ?mappings?
is specially relevant whenhandling language pairs with very different word or-der, such as Arabic-English or Chinese-English.Many alternatives have been proposed on usingsyntactic information in SMT systems.
They rangefrom those aiming at harmonizing (monotonizing)the word order of the considered language pairs bymeans of a set of linguistically-motivated reorder-ing patterns (Xia and McCord, 2004; Collins etal., 2005) to others considering translation a syn-chronous parsing process where reorderings intro-duced in the overall search are syntactically moti-vated (Galley et al, 2004; Quirk et al, 2005).
Thework presented here follows the word order harmo-nization strategy.53Collins et al (2005) describe a technique for pre-processing German to look more like English syn-tactically.
They used six transformations that areapplied on German parsed text to reorder it beforepassing it on to a phrase-based system.
They show amoderate statistically significant improvement.
Ourwork differs from theirs crucially in that our pre-processing rules are learned automatically.
Xia andMcCord (2004) describe an approach for transla-tion from French to English, where reordering rulesare acquired automatically using source and targetparses and word alignment.
The reordering rulesthey use are in a context-free constituency represen-tation with marked heads.
The rules are mostly lexi-calized.
Xia and McCord (2004) use source and tar-get parses to constrain word alignments used for ruleextraction.
Their results show that there is a positiveeffect on reordering when the decoder is run mono-tonically (i.e., without additional distortion-basedreordering).
The value of reordering is diminishedif the decoder is run in a non-monotonic way.Recently, Crego and Marin?o (2007b) employ POStags to automatically learn reorderings in train-ing.
They allow all possible learned reorderingsto be used to create a lattice that is input to thedecoder, which further improves translation accu-racy.
Similarly, Costa-jussa` and Fonollosa (2006)use statistical word classes to generalize reorder-ings, which are learned/introduced in a transla-tion process that transforms the source languageinto the target language word order.
Zhang et al(2007) describe a similar approach using unlexi-calized context-free chunk tags (XPs) to learn re-ordering rules for Chinese-English SMT.
Crego andMarin?o (2007c) extend their previous work usingsyntax trees (dependency parsing) to learn reorder-ings on a Chinese-English task.
Habash (2007)applies automatically-learned syntactic reorderingrules (for Arabic-English SMT) to preprocess the in-put before passing it to a phrase-based SMT decoder.As in (Zhang et al, 2007), (Costa-jussa` andFonollosa, 2006) and (Crego and Marin?o, 2007b),we employ a word graph for a tight coupling be-tween reordering and decoding.
However, we differon the language pair (Arabic-English) and the rulesemployed to learn reorderings.
Rules are built usingboth POS tags and chunk tags in order to balancethe higher generalization power of chunks with thehigher accuracy of POS tags.
Additionally, we in-troduce a method to use chunks for refining wordalignments employed in the system.3 Arabic Linguistic IssuesArabic is a morpho-syntactically complex languagewith many differences from English.
We describehere three prominent syntactic features of Arabicthat are relevant to Arabic-English translation andthat motivate some of our decisions in this work.First, Arabic words are morphologically complexcontaining clitics whose translations are representedseparately in English and sometimes in a differentorder.
For instance, possessive pronominal encli-tics are attached to the noun they modify in Ara-bic but their translation precedes the English trans-lation of the noun: kitAbu+hu1 ?book+his ?
hisbook?.
Other clitics include the definite article Al+?the?, the conjunction w+ ?and?
and the prepositionl+ ?of/for?, among others.
We use the Penn Ara-bic Treebank tokenization scheme which splits threeclasses of clitics only.
This scheme is compatiblewith the chunker we use (Diab et al, 2004).Secondly, Arabic verb subjects may be: pro-dropped (verb conjugated), pre-verbal (SVO), orpost-verbal (VSO).
The VSO order is quite challeng-ing in the context of translation to English.
For smallnoun phrases (NP), small phrase pairs in a phrase ta-ble and some degree of distortion can easily movethe verb to follow the NP.
But this becomes muchless likely with very long NPs that exceed the sizeof phrases in a phrase table.Finally, Arabic adjectival modifiers typically fol-low their nouns (with a small exception of some su-perlative adjectives).
For example, rajul Tawiyl (lit.man tall) translates as ?a tall man?.These three syntactic features of Arabic-Englishtranslation are not independent of each other.
As wereorder the verb and the subject NP, we also have toreorder the NP?s adjectival components.
This bringsnew challenges to previous implementations of N -gram based SMT which had worked with languagepairs that are more similar than Arabic and English,e.g., Spanish and English.
Although Spanish is likeArabic in terms of its noun-adjective order; Spanishis similar to English in terms of its subject-verb or-der.
Spanish morphology is more complex than En-glish but not as complex as Arabic: Spanish is likeArabic in terms of being pro-drop but has a smaller1All Arabic transliterations in this paper are provided in theBuckwalter transliteration scheme (Buckwalter, 2004).54number of clitics.
We do not focus on morphologyissues in this work.
Table 1 illustrates these dimen-sions of variations.
The more variations, the harderthe translation.Morph.
Subj-Verb Noun-AdjAR hard VSO, SVO, pro-drop N-A, A-NES medium SVO, pro-drop N-AEN simple SVO A-NTable 1: Arabic (AR), Spanish (ES) and English (EN)linguistic features.4 N-gram-based SMT SystemThe baseline translation system described in thispaper implements a log-linear combination of sixmodels: a translation model, a surface target lan-guage model, a target tag language model, a wordbonus model, a source-to-target lexicon model, anda target-to-source lexicon model.
In contrast to stan-dard phrase-based approaches, the translation modelis expressed in tuples, bilingual translation units,and is estimated as an N -gram language model(Marin?o et al, 2006).4.1 Translation UnitsTranslation units (or tuples) are extracted after re-ordering source words following the unfold methodfor monotonizing word alignments (Crego et al,2005).
Figure 1 shows an example of tuple extrac-tion with the original source-side word order result-ing in one tuple (regular); and after reordering thesource words resulting in three tuples (unfold).Figure 1: Regular Vs. Unfold translation units.In general, the unfold extraction method out-performs the regular method because it producessmaller, less sparse and more reusable units, whichis specially relevant for languages with very dif-ferent word order.
On the other hand, the unfoldmethod needs the input source words to be reorderedduring decoding similarly to how source words werereordered in training.
If monotonic decoding wereused with unfolded units, translation hypotheseswould follow the source language word order.4.2 Reordering FrameworkIn training time, a set of reordering rules are au-tomatically learned from word alignments.
Theserules are used in decoding time to provide the de-coder with a set of reordering hypotheses in the formof a reordering input graph.Rule ExtractionFollowing the unfold technique, source side re-orderings are introduced into the training corpus inorder to harmonize the word order of the source andtarget sentences.
For each reordering produced inthis step a record is taken in the form of a reorder-ing rule: ?s1, ..., sn ?
i1, ..., in?, where ?s1, ..., sn?is a sequence of of source words, and ?i1, ..., in?
isa sequence of index positions into which the sourcewords (left-hand side of the rule) are reordered.
It isworth noticing that translation units and reorderingrules are tightly coupled.The reordering rules described so far can onlyhandle reorderings of word sequences already seenin training.
In order to improve the generalizationpower of these rules, linguistic classes (POS tags,chunks, syntax trees, etc.)
can be used instead of rawwords in the left-hand side of the rules.
For example,the reordering introduced to unfold the alignmentsof the regular tuple ?AEln Almdyr AlEAm ?AlEAm Almdyr AEln?
in Figure 1 can producethe rule: ?V BD NN JJ ?
2 1 0?, wherethe left-hand side of the rule contains the sequenceof POS tags (?verb noun adjective?)
belonging to thesource words involved in reordering.Search Graph ExtensionIn decoding, the input sentence is handled as aword graph.
A monotonic search graph containsa single path, composed of arcs covering the inputwords in the original word order.
To allow for re-ordering, the graph is extended with new arcs, cov-ering the source words in the desired word order.
Fora given test sentence, any sequence of input tags ful-filling a left-hand side reordering rule leads to the55Figure 2: Linguistic information, reordering graph and translation composition of an Arabic sentence.addition of a reordering path.
Figure 2 shows an ex-ample of an input search graph extension (middle).The monotonic search graph is expanded followingthree different reordering rules.5 Rules with Chunk InformationThe generalization power of POS-based reorderingrules is somehow limited to short rules (less sparse)which fail to capture many real examples.
Longerrules are needed to model reorderings between full(linguistic) phrases, which are not restricted to anysize.
In order to capture such long-distance reorder-ings, we introduce rules with tags referring to arbi-trary large sequences of words: chunk tags.
Chunk-based rules allow the introduction of chunk tags inthe left-hand side of the rule.
For instance, therule: ?V P NP ?
1 0?
indicates that a verbphrase ?V P ?
preceding a noun phrase ?NP ?
are tobe swapped.
That is, the sequence of words com-posing the verb phrase are reordered at the end ofthe sequence of words composing the noun phrase.In training, like POS-based rules, a record is takenin the form of a rule whenever a source reordering isintroduced by the unfold technique.
To account forchunk-based rules, a chunk tag is used instead of thecorresponding POS tags when the words composingthe phrase remain consecutive (not necessarily in thesame order) after reordering.
Notice that rules arebuilt using POS tags as well as chunk tags.
Sinceboth approaches are based on the same reorderingsintroduced in training, both POS-based and chunk-based rules collect the same number of training ruleinstances.Figure 3 illustrates the process of POS-based andchunk-based rule extraction.
Here, the reorderingFigure 3: POS-based and chunk-based Rule extrac-tion: word-alignments, chunk and POS information (top),translation units (middle) and reordering rules (bottom)are shown.rule is applied over the sequence ?s2 s3 s4 s5 s6?,which is transformed into ?s6 s3 s4 s5 s2?.
Asfor the chunk rule, the POS tags ?p3 p4 p5?
of thePOS rule are replaced by the corresponding chunktag ?c2?
since words within the phrase remain con-secutive after being reordered.
The vocabulary ofchunk tags is typically smaller than that of POS tags.Hence, in order to increase the accuracy of the rules,we always use the POS tag instead of the chunk tagfor single word chunks.
In the example in Figure 3,the resulting chunk rule contains the POS tag ?p6?instead of the corresponding chunk tag ?c3?.Any sequence of input POS/chunk tags fulfillinga left-hand side reordering rule entails the exten-sion of the permutation graph with a new reorder-ing path.
Figure 2 shows the permutation graph(middle) computed for an Arabic sentence (top) af-56ter applying three reordering rules.
The best pathis drawn in bold arcs.
It is important to notice thatrules are recursively applied on top of sequences ofalready reordered words.
Chunk rules are appliedover phrases (sequences of words) which may needadditional reorderings.
Larger rules are applied be-fore shorter ones in order to allow for an easy im-plementation of recursive reordering.
Rules are al-lowed to match any path of the permutation graphconsisting of a sequence of words in the original or-der.
For example, the sequence ?Almdyr AlEAm?
isreordered into ?AlEAm Almdyr?
following the rule?NN JJ ?
1 0?
on top of the monotonic path aswell as on top of the path previously reordered byrule ?V P NP PP PP NP ?
1 2 3 4 0?.
In Fig-ure 2, the best reordering path (bold arcs) could notbe hypothesized without recursive reorderings.6 Refinement of Word AlignmentsAs stated earlier, the Arabic-English language pairpresents important word order disparities.
Thesestrong differences make word alignment a very dif-ficult task, typically producing a large number ofnoisy (wrong) alignments.
The N -gram-based SMTapproach suffers highly from the presence of noisyalignments since translation units are extracted outof single alignment-based segmentations of trainingsentences.
Noisy alignments lead to large translationunits, which cause a loss of translation informationand add to sparseness problems.We propose an alignment refinement method toreduce the number of wrong alignments.
Themethod employs two initial alignment sets: one withhigh precision, the other with high recall.
We usethe Intersection and Union (Och and Ney, 2000)of both alignment directions2 as the high precisionand high recall alignment sets, respectively.
Wewill study the effect of various initial alignment sets(such as grow-diag-final instead of Union) in thefuture.
The method is based on the fact that linguis-tic phrases (chunks), like raw words, have transla-tion correspondences and can therefore be aligned.We use chunk information to reduce the numberof allowed alignments for a given word.
The sim-ple idea that words in a source chunk are typicallyaligned to words in a single possible target chunk isused to discard alignments which link words from2We use IBM-1 to IBM-5 models (Brown et al, 1993) im-plemented with GIZA++ (Och and Ney, 2003).distant chunks.
Since limiting alignments to one-to-one chunk links is perhaps too strict, we extend thenumber of allowed alignments by permitting wordsin a chunk to be aligned to words in a target range ofwords.
This target range is computed as a projectionof the source chunk under consideration.
The re-sulting refined set contains all the Intersection align-ments and some of the Union.t1       t2       t3       t4       t5       t6      t7       t8s3      s4      s5 s6 s7      s8      s9s1     s2c2?c2c1 c3 c4c1?c3?
c4?Figure 4: Chunk projection: solid link are Intersectionlinks and all links (solid and dashed) are Union links.We outline the algorithm next.
The method canbe decomposed in two steps.
In the first step, usingthe Intersection set of alignments and source-sidechunks, each chunk is projected into the target side.Figure 4 shows an example of word alignment re-finement.
The projection c?k of the chunk ck is com-posed of the sequence of consecutive target words[tleft, tright] which can be determined as follows:?
All target words tj contained in Intersectionlinks (si, tj) with source word si within ck areconsidered projection anchors.
In the exam-ple in Figure 4, source words of chunk (c2) arealigned into the target side by means of two In-tersection alignments, (s3, t3) and (s4, t5), andproducing two anchors (t3 and t5).?
For each source chunk ck, tleft/tright is set byextending its leftmost/rightmost anchor in theleft/right direction up to the word before thenext anchor (or the first/last word if at sentenceedge).
In the example in Figure 4, c?1, c?2, c?3and c?4 are respectively [t4, t4], [t2, t6], [t1, t2]and [t6, t8].In the second step, for every alignment of theUnion set, the alignment is discarded if it links a57source word si to a target word tj that falls out of theprojection of the chunk containing the source word.Notice that all the Intersection links are containedin the resulting refined set.
In the example in Fig-ure 4, the link (s1, t2) is discarded as t2 falls out ofthe projection of chunk c1 ([t4, t4]).A further refinement can be done using the chunksof the target side.
The same technique is applied byswitching the role of source and target words/chunksin the algorithm described above and using the out-put of the basic source-based refinement (describedabove) as the high-recall alignment set, i.e., insteadof Union.7 Evaluation7.1 Experimental FrameworkAll of the training data used here is available fromthe Linguistic Data Consortium (LDC).3 We use anArabic-English parallel corpus4 consisting of 131Ksentence pairs, with approximately 4.1M Arabic to-kens and 4.4M English tokens.
Word alignment isdone with GIZA++ (Och and Ney, 2003).
All evalu-ated systems use the same surface trigram languagemodel, trained on approximately 340 million wordsof English newswire text from the English Giga-word corpus (LDC2003T05).
Additionally, we usea 5-gram language model computed over the POStagged English side of the training corpus.
Languagemodels are implemented using the SRILM toolkit(Stolcke, 2002).For Arabic tokenization, we use the Arabic Tree-Bank tokenization scheme: 4-way normalized seg-ments into conjunction, particle, word and pronom-inal clitic.
For POS tagging, we use the collapsedtagset for PATB (24 tags).
Tokenization and POStagging are done using the publicly available Mor-phological Analysis and Disambiguation of Arabic(MADA) tool (Habash and Rambow, 2005).
Forchunking Arabic, we use the AMIRA (ASVMT)toolkit (Diab et al, 2004).
English preprocessingsimply included down-casing, separating punctua-tion from words and splitting off ??s?.
The Englishside is POS-tagged with TNT(Brants, 2000) andchunked with the freely available OpenNlp5 tools.3http://www.ldc.upenn.edu4The parallel text includes Arabic News (LDC2004T17),eTIRR (LDC2004E72), English translation of Arabic Treebank(LDC2005E46), and Ummah (LDC2004T18).5http://opennlp.sourceforge.net/We use the standard four-reference NIST MTE-val data sets for the years 2003, 2004 and 2005(henceforth MT03, MT04 and MT05, respectively)for testing and the 2002 data set for tuning.6 BLEU-4 (Papineni et al, 2002), METEOR (Banerjee andLavie, 2005) and multiple-reference Word ErrorRate scores are reported.
SMT decoding is done us-ing MARIE,7 a freely available N -gram-based de-coder implementing a beam search strategy with dis-tortion/reordering capabilities (Crego and Marin?o,2007a).
Optimization is done with an in-house im-plementation of the SIMPLEX (Nelder and Mead,1965) algorithm.7.2 ResultsIn this section we assess the accuracy results of thetechniques introduced in this paper for alignment re-finement and word reordering.Alignment Refinement ExperimentWe contrast three systems built from differentword alignments: (a.)
the Union alignment setof both translation directions (U); (b.)
the refinedalignment set, detailed in Section 6, employing onlysource-side chunks (rS); (c.) the refined alignmentset employing source as well as target-side chunks(rST).
For this experiment, the system employs an n-gram bilingual translation model (TM) with n = 3and n = 4.
We also vary the use of a 5-gram target-tag language model (ttLM).
The reordering graph isbuilt using POS-based rules restricted to a maximumsize of 6 tokens (POS tags in the left-hand side of therule).
The results are shown in Table 2.Results from the refined alignment (rS) systemclearly outperform the results from the alignmentunion (U) system.
All measures agree in all test sets.Results further improve when we employ target-sidechunks to refine the alignments (rST), although notstatistically significantly.
BLEU 95% confidenceintervals for the best configuration (last row) are?.0162, ?.0210 and ?.0135 respectively for MT03,MT04 and MT05.As anticipated, the N -gram system suffers un-der high reordering needs when noisy alignmentsproduce long (sparse) tuples.
This can be seen bythe increase in translation unit counts when refinedlinks are used to alleviate the sparseness problem.The number of links of each alignment set over all6http://www.nist.gov/speech/tests/mt/7http://gps-tsc.upc.es/veu/soft/soft/marie/58Align TM ttLM BLEU mWER METEORMT03U 3 - .4453 51.94 .6356rS 3 - .4586 50.67 .6401rST 3 - .4600 50.64 .6416rST 4 - .4610 50.20 .6401rST 4 5 .4689 49.36 .6411MT04U 3 - .4244 50.12 .6055rS 3 - .4317 49.89 .6085rST 3 - .4375 49.69 .6109rST 4 - .4370 49.07 .6093rST 4 5 .4366 48.70 .6092MT05U 3 - .4366 50.40 .6306rS 3 - .4447 49.77 .6353rST 3 - .4484 49.09 .6386rST 4 - .4521 48.69 .6377rST 4 5 .4561 48.07 .6401Table 2: Evaluation results for experiments on transla-tion units, alignment and modeling.training data is 5.5M (U), 4.9M (rS) and 4.6M(rST).
Using the previous sets, the number of uniqueextracted translation units is 265.5K (U), 346.3K(rS) and 407.8K (rST).
Extending the TM to order4 and introducing the ttLM seems to further boostthe accuracy results for all sets in terms of mWERand for MT03 and MT05 only in terms of BLEU.Chunk Reordering ExperimentWe compare POS-based reordering rules withchunk-based reordering rules under different max-imum rule-size constraints.
Results are obtained us-ing TM n = 4, ttLM n=5 and rST refinement align-ment.
BLEU scores are shown in Table 3 for all testsets and rule sizes.
Rule size 7R indicates that chunkrules are used with recursive reorderings.BLEU 2 3 4 5 6 7 8 7RMT03POS .4364 .4581 .4656 .4690 .4689 .4686 .4685 -Chunk .4426 .4637 .4680 .4698 .4703 .4714 .4714 .4725MT04POS .4105 .4276 .4332 .4355 .4366 .4362 .4368 -Chunk .4125 .4316 .4358 .4381 .4373 .4372 .4373 .4364MT05POS .4206 .4465 .4532 .4549 .4561 .4562 .4565 -Chunk .4236 .4507 .4561 .4571 .4574 .4575 .4575 .4579Table 3: BLEU scores according to the maximum size ofrules employed.Table 4 measures the impact of introducing re-ordering rules limited to a given size (Y axis) onthe permutation graphs of input sentences from theMT03 data set (composed of 663 sentences contain-ing 18, 325 words).
Column Total shows the num-ber of additional (extended) paths introduced intothe test set permutation graph (i.e., 2, 971 additionalpaths of size 3 POS tags were introduced).
Columns3 to 8 show the number of moves made in the 1-besttranslation output according to the size of the movein words (i.e., 1, 652 moves of size 2 words appearedwhen considering POS rules of up to size 3 words).The rows in Table 4 correspond to the columns as-sociated with MT03 in Table 3.
Notice that a chunktag may refer to multiple words, which explains, forinstance, how 42 moves of size 4 appear using chunkrules of size 2.
Overall, short-size reorderings are farmore abundant than larger ones.Size Total 2 3 4 [5,6] [7,8] [9,14]POS rules2 8, 142 2, 129 - - - - -3 +2, 971 1, 652 707 - - - -4 +1, 628 1, 563 631 230 - - -5 +964 1, 531 615 210 82 - -6 +730 1, 510 604 200 123 - -7 +427 1, 497 600 191 121 24 -8 +159 1, 497 599 191 120 26 -Chunk rules2 9, 201 2, 036 118 42 20 1 03 +4, 977 1, 603 651 71 42 5 24 +1, 855 1, 542 593 200 73 7 05 +1, 172 1, 514 578 187 118 15 16 +760 1, 495 573 178 130 20 57 +393 1, 488 568 173 129 27 108 +112 1, 488 568 173 129 27 107R +393 1, 405 546 179 152 54 25Table 4: Reorderings hypothesized and employed in the1-best translation output according to their size.Differences in BLEU (Table 3) are very smallacross the alternative configurations (POS/chunk).
Itseems that larger reorderings, size 7 to 14, (shownin Table 4) introduce very small accuracy variationswhen measured using BLEU.
POS rules are able toaccount for most of the necessary moves (size 2 to6).
However, the presence of the larger moves whenconsidering chunk-based rules (together with accu-racy improvements) show that long-size reorderingscan only be captured by chunk rules.
The largestmoves taken by the decoder using POS rules con-sist of 2 sequences of 8 words (Table 4, column 7,row 9 minus row 8).
The increase in the number of59long moves when considering recursive chunks (7R)means that longer chunk rules provide only valid re-ordering paths if further (recursive) reorderings arealso considered.
The corresponding BLEU score(Table 3, last column) indicates that the new set ofmoves improves the resulting accuracy.
The gen-eral lower scores and inconsistent behavior of MT04compared to MT03/MT05 may be a result of MT04being a mix of genres (newswire, speeches and edi-torials).7.3 Error AnalysisWe conducted a human error analysis by compar-ing the best results from the POS system to thoseof the best chunk system.
We used a sample of 155sentences from MT03.
In this sample, 25 sentences(16%) were actually different between the two an-alyzed systems.
The differences were determinedto involve 30 differing reorderings.
In all of thesecases, the chunk system made a move, but the POSsystem only moved (from source word order) in 60%of the cases.
We manually judged the relative qual-ity of the move (or lack thereof).
We found that47% of the time, chunk moves were superior to POSchoice.
In 27% of the time POS moves were better.In the rest of the time, the two systems were equallygood or bad.
The main challenge for chunk reorder-ing seems to be the lack of syntactic constraints: inmany cases of errors the chunk reordering did not gofar enough or went too far, breaking up NPs or pass-ing multiple NPs, respectively.
Additional syntacticfeatures to constrain the reordering model may beneeded.8 Conclusions and Future WorkIn this work we have described two methods toimprove SMT accuracy using shallow syntax in-formation.
First, alignment quality has been im-proved (in terms of translation accuracy) by prun-ing out noisy links which do not respect a chunk-to-chunk alignment correspondence.
Second, rewriterules built with two different linguistic annotations,(local) POS tags and (long-spanning) chunk tags,can handle both short/medium and long distance re-orderings as well as different degrees of recursiveapplication.
In order to better assess the suitabilityof chunk rules we carried out a human error analy-sis which confirmed that long reorderings were ef-fectively captured by chunk rules.
However, the er-ror analysis also revealed that additional syntacticfeatures to constrain the reordering model may beneeded.
In the future, we plan to introduce weightsinto the permutations graph to more accurately drivethe search process as well as extend the rules withfull syntactic information (parse trees).AcknowledgmentsThe first author has been partially funded by theSpanish Government under the AVIVAVOZ project(TEC2006-13694-C03) the Catalan Government un-der BE-2007 grant and the Universitat Polite`cnica deCatalunya under UPC-RECERCA grant.
The sec-ond author was funded under the DARPA GALEprogram, contract HR0011-06-C-0023.ReferencesS.
Banerjee and A. Lavie.
2005.
METEOR: An Auto-matic Metric for MT Evaluation with Improved Cor-relation with Human Judgments.
In Proceedings ofthe Association for Computational Linguistics (ACL)Workshop on Intrinsic and Extrinsic Evaluation Mea-sures for Machine Translation and/or Summarization.T.
Brants.
2000.
TnT ?
a statistical part-of-speech tagger.In Proceedings of the 6th Applied Natural LanguageProcessing Conference (ANLP?2000).P.
Brown, S. Della Pietra, V. Della Pietra, and R. Mercer.1993.
The mathematics of statistical machine trans-lation: Parameter estimation.
Computational Linguis-tics, 19(2):263?311.T.
Buckwalter.
2004.
Buckwalter Arabic Morphologi-cal Analyzer Version 2.0.
Linguistic Data Consortium,University of Pennsylvania, 2002.
LDC Cat alog No.
:LDC2004L02, ISBN 1-58563-324-0.M.
Collins, P. Koehn, and I. Kucerova.
2005.
ClauseRestructuring for Statistical Machine Translation.
InProceedings of ACL?05.M.R.
Costa-jussa` and J.A.R.
Fonollosa.
2006.
Statisticalmachine reordering.
Proceedings of the 2006 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP).J.M.
Crego and J.B. Marin?o.
2007a.
Extending marie: ann-gram-based smt decoder.
Proceedings of ACL?07).J.M.
Crego and J.B. Marin?o.
2007b.
Improving statisti-cal mt by coupling reordering and decoding.
MachineTranslation, 20(3):199?215.J.M.
Crego and J.B. Marin?o.
2007c.
Syntax-enhancedN-gram-based SMT.
In Proceedings of the MachineTranslation Summit (MT SUMMIT XI).J.M.
Crego, J.B. Marin?o, and A. de Gispert.
2005.
Re-ordered search and tuple unfolding for ngram-basedsmt.
Proceedings of MT Summit X.60M.
Diab, K. Hacioglu, and D. Jurafsky.
2004.
Automatictagging of arabic text: From raw text to base phrasechunks.
In Proceedings of HLT-NAACL?04.M.
Galley, M. Hopkins, K. Knight, and D. Marcu.
2004.What?s in a translation rule?
In Proceedings of HLT-NAACL?04.N.
Habash and O. Rambow.
2005.
Arabic Tokeniza-tion, Part-of-Speech Tagging and Morphological Dis-ambiguation in One Fell Swoop.
In Proceedings ofACL?05.N.
Habash.
2007.
Syntactic Preprocessing for StatisticalMT.
In Proceedings of MT SUMMIT XI.J.B.
Marin?o, R.E.
Banchs, J.M.
Crego, A. de Gispert,P.
Lambert, J.A.R.
Fonollosa, and M.R.
Costa-jussa`.2006.
N-gram based machine translation.
Computa-tional Linguistics, 32(4):527?549.J.A.
Nelder and R. Mead.
1965.
A simplex methodfor function minimization.
The Computer Journal,7:308?313.F.
J. Och and H. Ney.
2000.
Improved statistical align-ment models.
In Proceedings of ACL.F.
Och and H. Ney.
2003.
A Systematic Comparison ofVarious Statistical Alignment Models.
ComputationalLinguistics, 29(1):19?52.K.
Papineni, S. Roukos, T. Ward, and W. Zhu.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
In Proceedings of ACL?02C.
Quirk, A. Menezes, and C. Cherry.
2005.
DependencyTreelet Translation: Syntactically Informed PhrasalSMT.
In Proceedings of ACL?05A.
Stolcke.
2002.
SRILM - an Extensible LanguageModeling Toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing (ICSLP).F.
Xia and M. McCord.
2004.
Improving a statistical mtsystem with automatically learned rewrite patterns.
InProceedings of the 20th International Conference onComputational Linguistics (COLING 2004).Y.
Zhang, R. Zens, and H. Ney.
2007.
Chunk-level re-ordering of source language sentences with automati-cally learned rules for statistical machine translation.In Proceedings of HLT-NAACL Workshop on Syntaxand Structure in Statistical Translation.61
