Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 474?478,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsSpecifying and Annotating Reduced Argument SpanVia QA-SRLGabriel Stanovsky Meni Adler Ido DaganComputer Science Department, Bar-Ilan University{gabriel.satanovsky,meni.adler}@gmail.comdagan@cs.biu.ac.ilAbstractProminent semantic annotations take aninclusive approach to argument span an-notation, marking arguments as full con-stituency subtrees.
Some works, how-ever, showed that identifying a reduced ar-gument span can be beneficial for vari-ous semantic tasks.
While certain practi-cal methods do extract reduced argumentspans, such as in Open-IE , these solutionsare often ad-hoc and system-dependent,with no commonly accepted standards.
Inthis paper we propose a generic argumentreduction criterion, along with an anno-tation procedure, and show that it can beconsistently and intuitively annotated us-ing the recent QA-SRL paradigm.1 IntroductionRepresentations of predicate-argument structureneed to determine the span of predicates and theircorresponding arguments.
Surprisingly, there areno accepted NLP-standards which specify whatthe ?right?
span of an argument should be.Semantic representations typically take an in-clusive (or maximal) approach: PropBank anno-tation (Palmer et al, 2005), for example, marksarguments as full constituency subtrees.
Froman application perspective, this maximal approachensures that all arguments are indeed embeddedwithin the annotated span, yet it is often not trivialhow to accurately recover them.In contrast to this maximal-span approach,Open-IE systems (Etzioni et al, 2008; Fader et al,2011) put emphasis on extracting readable stand-alone propositions, typically producing shorter ar-guments (see examples in Section 2.1).
Severalrecent works have exploited this property, usingOpen-IE extractions as an intermediate represen-tation within a larger framework.Angeli et al (2015) built an Open-IE systemwhich focuses on shorter argument spans.
Theyhypothesize that ?shorter arguments [are] morelikely to be useful for downstream applications?,and demonstrate this by using their system to ex-tract facts about predefined entities in a state-of-the-art Knowledge Base Population system.Further, Stanovsky et al (2015) compared theperformance of several off-the-shelf parsers in dif-ferent semantic tasks.
Most relevant to this workis the comparison between Open-IE and SRL.Specifically, they suggest that SRL?s longer argu-ments introduce noise which hurts performancefor downstream tasks.
This is sustained empiri-cally by showing that extractions from Open-IE41significantly outperform ClearNLP?s SRL (Choi,2012) in textual similarity, analogies, and readingcomprehension tasks.2While Open-IE extractors do provide a reduc-tion of argument span, they lack consistency andprincipled rigor ?
there is no clear definition forthe desired argument span, which is defined de-facto by the different implementations.
This lackof a common system-independent definition, letalone an annotation methodology, hinders the cre-ation of gold standard argument-span annotation.In this work we propose a concrete argumentspan reduction criterion and an accompanyingannotation procedure, based on the recent QA-SRL paradigm (He et al, 2015).
We show thatthis criterion can be consistently annotated withhigh agreement, and that it is intuitive enough tobe obtained through crowd-sourcing.As future work, we intend to apply the reductioncriterion to other types of predicates (e.g., nomi-1http://knowitall.github.io/openie2Open IE-4 is based on ClearNLPs SRL, allowing for adirect comparison.474nal and adjectival predication).
Subsequently, wewould like to create a comprehensive annotatedresource, as a benchmark for the detection of re-duced argument spans.2 Background2.1 Argument SpanAs discussed in the Introduction, PropBank takesan inclusive approach to annotating arguments, bymarking them as full constituency subtrees.
Forexample, given the sentence ?Obama, the newlyelected president, flew to Russia?, PropBank willmark ?Obama, the newly elected president?
as ARG0 ofthe predicate flew.However, in certain applications, such as ques-tion answering or abstractive summarization, a re-duced argument is preferred (i.e., ?Obama?).
No-tably, different implementations of Open-IE pro-vide an applicable generic way to reduce argumentspan.
Since there are no common guidelines forthis task, each Open-IE extractor produces differ-ent argument spans.
We cover briefly some of themain differences in a few prominent Open-IE sys-tems.ReVerb (Fader et al, 2011) uses part-of-speech-based regular expressions to decide whether aword should be included within an argument span.For example, they move certain light verb com-pliments and prepositions from the argument tothe predicate slot (e.g., ?gave a talk at?).
OLLIE(Mausam et al, 2012) learns lexical-syntactic pat-terns and splits extractions across certain prepo-sitions.
For example, given ?I flew from Paris toBerlin?, OLLIE yields (I; flew; from Paris) and(I; flew; to Berlin).
More recently, (Angeli et al,2015) used natural logic to remove non-integralparts of arguments (e.g., removing the underlinednon-restrictive prepositional phrase in ?Heinz Fis-cher of Austria?
).2.2 QA-SRLSRL is typically perceived as answering argu-ment role questions, such as who, what, to whom,when, or where, regarding a target predicate.
Forinstance, PropBank?s ARG0 for the predicate sayanswers the question ?who said something?
?.QA-SRL (He et al, 2015) follows this per-spective, and suggests that answering explicit rolequestions is an intuitive means to solicit predicate-argument structures from non-expert annotators.Annotators are presented with a sentence in whicha target predicate3was marked, and are requestedto annotate argument role questions (from a re-stricted grammar) and corresponding answers.For example, given the previous sentence andthe target predicate flew, an annotator can intu-itively provide the following QA pairs: (1) Whoflew somewhere?
Obama, and (2) Where didsomeone fly?
Russia.The annotation guidelines further solicit multi-ple shorter answers, each typically embedded inthe span of a maximal PropBank-style argument,while providing a different answer to the (same)argument role question.In Section 4 we make use of QA-SRL?s frame-work in order to produce annotations by our re-duction argument criterion, which is defined in thenext section.3 Argument ReductionIn this section, we propose annotation criteria andprocess for obtaining minimal argument spans.Given an original, non-reduced argument, we aimto reduce it to a set of (one or more) smaller argu-ments, which jointly specify the same answer tothe argument?s role question.Formally, given a non-reduced argument a ={w1, ..., wn}, along with its role question Q(a)with respect to predicate p in sentence s, weseek to find a set of minimally-scoped arguments,M(a), such that:(1) Each m ?
M(a) is a proper subset of a.
(2) Each m ?
M(a) provides a different, inde-pendently interpreted answer to Q(a).
(3) M(a) is equivalent to a, in the sense thatwhen taken jointly, M(a) specifies the sameanswers as a does for Q(a).
(4) Each m ?
M(a) is minimal, meaning it can-not be further reduced without violating theequivalence criterion (3).Note that this definition relies on human judg-ments, which are used to decide whether two ar-guments provide the same or different answers.Generally speaking, a non-minimal argument acan be reduced in one of two ways:(a) Removal of tokens from a, forming a smallerargument.3Currently these consist of automatically annotated verbs.475(b) Splitting a, yielding multiple arguments.In our context, we would like to apply thesetwo operations as long as they maintain the equiv-alence criterion (3).
We empirically observe thatthe first case (removal) corresponds to the omis-sion of non-restrictive modifiers, that is, modifiersfor which the content of the modifier presents aseparate, parenthetical unit of information aboutthe NP (Huddleston et al, 2002).
For example, re-visiting the sentence: ?Obama, the newly electedpresident, flew to Russia.
?, the non-reduced argu-ment ?Obama, the newly elected president?
can be re-duced to the minimal argument ?Obama?, as bothspecify the same answer to the role question ?whoflew to Russia?
?.In contrast, a restrictive modifier is an integralpart of the meaning of the containing NP, andhence should not be removed, as in ?She wore thenecklace that her mother gave her?.The second reduction operation (splitting) cor-responds to decoupling distributive coordinations,that is, cases in which a predicate applies sepa-rately to all of the elements in the coordination.For example, in: ?Obama and Clinton were bornin America.
?, the non-reduced PropBank-style ar-gument ?Obama and Clinton?
can be reduced to twoarguments {?Obama?, ?Clinton?}.
Each of these ar-guments independently answers the role question?Who was born in America?
?, while jointly theycorrespond to the longer, non-reduced argument.Note that splitting a shorter distributive argu-ment does not necessarily produce disjoint argu-ments.
For example, consider: ?The tall boys andgirls were born in America.
?, in which ?The tall boysand girls?
would reduce to two overlapping argu-ments: {?The tall boys?, ?The tall girls?
}.In contrast, non-distributive conjuncts cannot besplit.
These are cases in which the predicate ap-plies to the conjuncts taken together, while apply-ing it separately to each element changes the inter-pretation of the clause.
Consider for example thereciprocal structure of: ?Obama and Putin met inMoscow?, in which we cannot split the argument?Obama and Putin?
since the predicate met impliesthat Obama and Putin met with each other, whichwill be lost if we split the argument to two inde-pendent answers.Based on these two operations, a set of mini-mal arguments, M(a), can be obtained from a ina top-down manner: first apply removal, if possi-ble; then splitting, if possible.4Next, apply recur-sively to each of the smaller arguments, stoppingwhen none of the two reduction operations can beapplied.This annotation process might yield differentsets of minimal arguments by different annotators,depending on their decisions regarding the reduc-tion steps.
As we show empirically in the next sec-tion, high agreement levels can be obtained, sup-porting the validity of our proposed criterion.4 Annotation ExperimentIn this section we describe the compilation andanalysis of a small-scale expert annotation corpus.Creating such corpus serves 3 goals: (1) It allowsus to test the applicability of the argument reduc-ing procedure, (2) By comparing it with Propbankwe can examine how often, and in which cases, wereduce arguments (Section 4.1), and (3) We can as-sess the plausibility of crowd-sourcing argumentspan annotation (Sections 4.2 and 4.3).In order to achieve these goals, we sample 100predicates of the Propbank corpus, which covered260 arguments.
To allow comparisons, we samplepredicates which were annotated by QA-SRL andwhose arguments were aligned by (He et al, 2015)with a matching Propbank argument.5Two expert annotators used the QA-SRL?s inter-face to re-answer the original QA-SRL annotatedquestions with minimally-scoped arguments, ac-cording to the procedure described in Section 3.Prior to annotating the expert dataset, the annota-tors discussed the process and resolved conflictson a separate development set of 20 predicates.Annotator agreement From an argument per-spective, the annotators fully agreed on the spanof 94.6% of the arguments.Looking into the word token level, we foundthat for a given PropBank argument a =(w1, ..., wn), the respective reduced arguments al-ways constitute a subset of a.
This allows us tolook at the annotation process as a list of n map-ping decisions ?
for each wi, an annotator decideswhether he (1) Maps it to one or more of the argu-4This order is arbitrary, chosen solely to provide a deter-ministic process.
Alternating the steps would yield an identi-cal set.5An annotated answer is judged to match the PropBankargument if either (1) the gold argument head is within theannotated answer span, or (2) the gold argument head is apreposition and at least one of its children is within the an-swer span.476ments of M(a), or (2) Deletes it.
The completeannotation required each annotator to make 985such mappings decisions.
Word level agreementbetween the annotators was calculated as the per-cent of the decisions on which they agreed, andfound to be 97.1%.Overall, the annotators achieved a high level ofagreement, suggesting that the reduction criterioncan be consistently applied by trained annotators.An analysis of the few disagreements revealed thatthe deviations between the annotators stem fromsemantic ambiguities, where two legitimate read-ings of the sentence led to different span annota-tions.6Finally, we compose the expert annotationdataset from 247 arguments on which both anno-tators fully agreed.4.1 Comparison with PropbankComparing our annotation with PropBank showedthat we reduced roughly 24% of the arguments:19% of the arguments were reduced by omittingnon-restrictive modifications and 5% of the argu-ments were split across distributive co-ordinations(see discussion on both types of reductions in Sec-tion 3).The average reduced argument shrunk byroughly 58%.
In general, these numbers sug-gest that our annotation scheme targets commonlyrecurring phenomena, and significantly deviatesfrom PropBank?s annotation of arguments.4.2 CrowdsourcingWe created an Amazon Mechanical Turk7projectto investigate the possible scalability of our anno-tation using non-trained annotators.Similarly to the setting used by the expert an-notators, turkers were presented with a sentence,followed by a list of questions regarding a targetpredicate.
The sentences, predicates and questionswere taken from the expert corpus, which alignsbetween QA-SRL and Propbank.8The guidelines for annotators refined those ofHe et al (2015), soliciting answers which follow6For example, in ?The American Stock Exchange said aseat was sold for $ 160,000 , down $ 5,000 from the previ-ous sale last Friday .
?, one annotator did not reduce ARG1,while the second annotator chose to restrict the span of theargument to ?a seat was sold for $ 160,00?, interpreting theremaining part of the clause as an addition by the author.7https://www.mturk.com8To be clear, the annotators saw only the raw text andquestions from QA-SRL and were not exposed to the Prop-Bank annotations.Annotation Argument WordExpert - IAA 94.6% 97.1%QA-SRL - Expert 80% 88.5%Our Crowdsourcing - Expert 89.1% 93.5%Table 1: Agreement levels between the differentannotations: (1) IAA - Inter-Annotator agreementbetween the expert annotators (2) Agreement ofQA-SRL corpus with our expert annotation and(3) Our Crowdsourcing - agreement of the Ama-zon Mechanical Turk annotations with our expertannotation.
See Section 4.our formal criterion.
In cases of multiple answersreferring to the same entity, annotators are asked toprovide the most specific answer, otherwise (if theanswers refer to different entities), the annotatorsare asked to list all of the answers.
Furthermore,the annotators are requested to provide the shortestanswer they can, while preserving its correctness.We chose annotations which were agreed uponby at least two annotators.
In cases where the threeannotators gave different answers (26% of thetime), we used a fourth annotator to arbitrate, andcalculated agreement using the same metrics dis-cussed above.
Cases where annotators disagreedwere mostly semantically ambigouos.
For exam-ple, given the sentence ?Our pilot simply laughed ,fired up the burner and with another blast of flamelifted us , oh , a good 12 - inches above the waterlevel .?
and the question ?how much did someonelift someone?
?, one annotator replied 12 - incheswhile another replied a good 12 - inches.We found that the crowdsourcing annotationsto be of high quality, reaching 89.1% argumentagreement and 93.5% word agreement with ourexpert annotation.
These results suggest that theannotation of argument span is efficiently andaccurately attainable using crowd-sourcing tech-niques, with only subtle refinements over the orig-inal QA-SRL guidelines.4.3 Comparison with QA-SRLFinally, we want to compare our crowdsourcingannotation versus that of QA-SRL, with respect toargument span.
Using the previously mentionedagreement metric, we find that QA-SRL agreeswith our expert dataset on 80% of the argumentsand 88.5% of the word-level decisions.
Althoughit is outperformed by our crowdsourcing annota-477tion project, QA-SRL still manages to capture sig-nificant amounts of the minimally-reduced argu-ments.
This is interesting, as the QA-SRL guide-lines did not address this issue specifically, but in-stead solicited annotators to provide ?as many an-swers as possible?.
This suggests that the questionanswering format intuitively prompts human an-notators to reduce the span of their answers.To conclude this section, the entire comparisonmeasurements are summarized in Table 1.5 Conclusion and Future WorkIn this work we proposed a concrete criterion forspecifying minimally-scoped arguments.
Whilethis issue was applicably addressed by previouswork, it was not consistently defined or anno-tated.
Following this definition, we created anexpert annotation dataset over texts from Prop-Bank, using the QA-SRL paradigm.
This annota-tion achieved high levels of inter-annotator agree-ment, and was shown to be intuitive enough sothat it can be scaled to crowdsourcing annotation.As future work, we plan to extend this annotationproject to larger volumes of text, and to additionaltypes of (non-verbal) predications, which will al-low to develop learning-based methods that iden-tify minimally-reduced argument span.AcknowledgmentsWe would like to thank Luheng He and LukeZettlemoyer for the fruitful discussions, and theanonymous reviewers for their helpful comments.This work was supported in part by grants fromthe MAGNET program of the Israeli Office of theChief Scientist (OCS), the Israel Science Founda-tion grant 880/12, and the German Research Foun-dation through the German-Israeli Project Cooper-ation (DIP, grant DA 1600/1-1).ReferencesGabor Angeli, Melvin Johnson Premkumar, andChristopher D. Manning.
2015.
Leveraging lin-guistic structure for open domain information ex-traction.
In Proceedings of the 53rd Annual Meet-ing of the Association for Computational Linguistics(ACL 2015).Jinho D. Choi.
2012.
Optimization of NaturalLanguage Processing Components for Robustnessand Scalability.
Ph.D. thesis, Boulder, CO, USA.AAI3549172.Oren Etzioni, Michele Banko, Stephen Soderland, andDaniel S Weld.
2008.
Open information extrac-tion from the Web.
Communications of the ACM,51(12):68?74.Anthony Fader, Stephen Soderland, and Oren Etzioni.2011.
Identifying relations for open information ex-traction.
In Proceedings of the Conference on Em-pirical Methods in Natural Language Processing,pages 1535?1545.
Association for ComputationalLinguistics.Luheng He, Mike Lewis, and Luke Zettlemoyer.
2015.Question-answer driven semantic role labeling: Us-ing natural language to annotate natural language.In the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP).Rodney Huddleston, Geoffrey K Pullum, et al 2002.The cambridge grammar of english.
Language.Cambridge: Cambridge University Press.Mausam, Michael Schmitz, Stephen Soderland, RobertBart, and Oren Etzioni.
2012.
Open language learn-ing for information extraction.
In Proceedings ofthe 2012 Joint Conference on Empirical Methodsin Natural Language Processing and ComputationalNatural Language Learning, pages 523?534, JejuIsland, Korea, July.
Association for ComputationalLinguistics.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition bank: An annotated cor-pus of semantic roles.
Computational linguistics,31(1):71?106.Gabriel Stanovsky, Ido Dagan, and Mausam.
2015.Open IE as an intermediate structure for semantictasks.
In Proceedings of the 53rd Annual Meeting ofthe Association for Computational Linguistics (ACL2015).478
