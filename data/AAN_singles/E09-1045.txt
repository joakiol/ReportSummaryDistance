Proceedings of the 12th Conference of the European Chapter of the ACL, pages 389?397,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsAn Empirical Study on Class-based Word Sense Disambiguation?Rube?n Izquierdo & Armando Sua?rezDeparment of Software and Computing SystemsUniversity of Alicante.
Spain{ruben,armando}@dlsi.ua.esGerman RigauIXA NLP Group.EHU.
Donostia, Spaingerman.rigau@ehu.esAbstractAs empirically demonstrated by the lastSensEval exercises, assigning the appro-priate meaning to words in context has re-sisted all attempts to be successfully ad-dressed.
One possible reason could be theuse of inappropriate set of meanings.
Infact, WordNet has been used as a de-factostandard repository of meanings.
How-ever, to our knowledge, the meanings rep-resented by WordNet have been only usedfor WSD at a very fine-grained sense levelor at a very coarse-grained class level.
Wesuspect that selecting the appropriate levelof abstraction could be on between bothlevels.
We use a very simple method forderiving a small set of appropriate mean-ings using basic structural properties ofWordNet.
We also empirically demon-strate that this automatically derived set ofmeanings groups senses into an adequatelevel of abstraction in order to performclass-based Word Sense Disambiguation,allowing accuracy figures over 80%.1 IntroductionWord Sense Disambiguation (WSD) is an inter-mediate Natural Language Processing (NLP) taskwhich consists in assigning the correct semanticinterpretation to ambiguous words in context.
Oneof the most successful approaches in the last yearsis the supervised learning from examples, in whichstatistical or Machine Learning classification mod-els are induced from semantically annotated cor-pora (Ma`rquez et al, 2006).
Generally, super-vised systems have obtained better results thanthe unsupervised ones, as shown by experimentalwork and international evaluation exercises such?This paper has been supported by the European Unionunder the projects QALL-ME (FP6 IST-033860) and KY-OTO (FP7 ICT-211423), and the Spanish Government underthe project Text-Mess (TIN2006-15265-C06-01) and KNOW(TIN2006-15049-C03-01)as Senseval1.
These annotated corpora are usu-ally manually tagged by lexicographers with wordsenses taken from a particular lexical semantic re-source ?most commonly WordNet2 (WN) (Fell-baum, 1998).WN has been widely criticized for being a senserepository that often provides too fine?grainedsense distinctions for higher level applicationslike Machine Translation or Question & Answer-ing.
In fact, WSD at this level of granularityhas resisted all attempts of inferring robust broad-coverage models.
It seems that many word?sensedistinctions are too subtle to be captured by auto-matic systems with the current small volumes ofword?sense annotated examples.
Possibly, build-ing class-based classifiers would allow to avoidthe data sparseness problem of the word-based ap-proach.
Recently, using WN as a sense reposi-tory, the organizers of the English all-words taskat SensEval-3 reported an inter-annotation agree-ment of 72.5% (Snyder and Palmer, 2004).
In-terestingly, this result is difficult to outperform bystate-of-the-art sense-based WSD systems.Thus, some research has been focused on deriv-ing different word-sense groupings to overcomethe fine?grained distinctions of WN (Hearst andSchu?tze, 1993), (Peters et al, 1998), (Mihalceaand Moldovan, 2001), (Agirre and LopezDeLa-Calle, 2003), (Navigli, 2006) and (Snow et al,2007).
That is, they provide methods for groupingsenses of the same word, thus producing coarserword sense groupings for better disambiguation.Wikipedia3 has been also recently used to over-come some problems of automatic learning meth-ods: excessively fine?grained definition of mean-ings, lack of annotated data and strong domain de-pendence of existing annotated corpora.
In thisway, Wikipedia provides a new very large sourceof annotated data, constantly expanded (Mihalcea,2007).1http://www.senseval.org2http://wordnet.princeton.edu3http://www.wikipedia.org389In contrast, some research have been focused onusing predefined sets of sense-groupings for learn-ing class-based classifiers for WSD (Segond et al,1997), (Ciaramita and Johnson, 2003), (Villarejoet al, 2005), (Curran, 2005) and (Ciaramita andAltun, 2006).
That is, grouping senses of differentwords into the same explicit and comprehensivesemantic class.Most of the later approaches used the origi-nal Lexicographical Files of WN (more recentlycalled SuperSenses) as very coarse?grained sensedistinctions.
However, not so much attention hasbeen paid on learning class-based classifiers fromother available sense?groupings such as WordNetDomains (Magnini and Cavaglia`, 2000), SUMOlabels (Niles and Pease, 2001), EuroWordNetBase Concepts (Vossen et al, 1998), Top Con-cept Ontology labels (Alvez et al, 2008) or Ba-sic Level Concepts (Izquierdo et al, 2007).
Obvi-ously, these resources relate senses at some levelof abstraction using different semantic criteria andproperties that could be of interest for WSD.
Pos-sibly, their combination could improve the overallresults since they offer different semantic perspec-tives of the data.
Furthermore, to our knowledge,to date no comparative evaluation has been per-formed on SensEval data exploring different levelsof abstraction.
In fact, (Villarejo et al, 2005) stud-ied the performance of class?based WSD com-paring only SuperSenses and SUMO by 10?foldcross?validation on SemCor, but they did not pro-vide results for SensEval2 nor SensEval3.This paper empirically explores on the super-vised WSD task the performance of differentlevels of abstraction provided by WordNet Do-mains (Magnini and Cavaglia`, 2000), SUMO la-bels (Niles and Pease, 2001) and Basic Level Con-cepts (Izquierdo et al, 2007).
We refer to this ap-proach as class?based WSD since the classifiersare created at a class level instead of at a senselevel.
Class-based WSD clusters senses of differ-ent words into the same explicit and comprehen-sive grouping.
Only those cases belonging to thesame semantic class are grouped to train the clas-sifier.
For example, the coarser word grouping ob-tained in (Snow et al, 2007) only has one remain-ing sense for ?church?.
Using a set of Base LevelConcepts (Izquierdo et al, 2007), the three sensesof ?church?
are still represented by faith.n#3,building.n#1 and religious ceremony.n#1.The contribution of this work is threefold.
Weempirically demonstrate that a) Basic Level Con-cepts group senses into an adequate level of ab-straction in order to perform supervised class?based WSD, b) that these semantic classes canbe successfully used as semantic features to boostthe performance of these classifiers and c) thatthe class-based approach to WSD reduces dramat-ically the required amount of training examples toobtain competitive classifiers.After this introduction, section 2 presents thesense-groupings used in this study.
In section 3 theapproach followed to build the class?based systemis explained.
Experiments and results are shown insection 4.
Finally some conclusions are drawn insection 5.2 Semantic ClassesWordNet (Fellbaum, 1998) synsets are organizedin forty five Lexicographer Files, more recetlycalled SuperSenses, based on open syntactic cat-egories (nouns, verbs, adjectives and adverbs) andlogical groupings, such as person, phenomenon,feeling, location, etc.
There are 26 basic cate-gories for nouns, 15 for verbs, 3 for adjectives and1 for adverbs.WordNet Domains4 (Magnini and Cavaglia`,2000) is a hierarchy of 165 Domain Labels whichhave been used to label all WN synsets.
Informa-tion brought by Domain Labels is complementaryto what is already in WN.
First of all a Domain La-bels may include synsets of different syntactic cat-egories: for instance MEDICINE groups togethersenses from nouns, such as doctor and hospital,and from Verbs such as to operate.
Second, a Do-main Label may also contain senses from differ-ent WordNet subhierarchies.
For example, SPORTcontains senses such as athlete, deriving from lifeform, game equipment, from physical object, sportfrom act, and playing field, from location.SUMO5 (Niles and Pease, 2001) was created aspart of the IEEE Standard Upper Ontology Work-ing Group.
The goal of this Working Group isto develop a standard upper ontology to promotedata interoperability, information search and re-trieval, automated inference, and natural languageprocessing.
SUMO consists of a set of concepts,relations, and axioms that formalize an upper on-tology.
For these experiments, we used the com-plete WN1.6 mapping with 1,019 SUMO labels.4http://wndomains.itc.it/5http://www.ontologyportal.org/390Basic Level Concepts6 (BLC) (Izquierdo et al,2007) are small sets of meanings representing thewhole nominal and verbal part of WN.
BLC canbe obtained by a very simple method that uses ba-sic structural WN properties.
In fact, the algorithmonly considers the relative number of relations ofeach synset alng the hypernymy chain.
The pro-cess follows a bottom-up approach using the chainof hypernymy relations.
For each synset in WN,the process selects as its BLC the first local maxi-mum according to the relative number of relations.The local maximum is the synset in the hypernymychain having more relations than its immediatehyponym and immediate hypernym.
For synsetshaving multiple hypernyms, the path having thelocal maximum with higher number of relationsis selected.
Usually, this process finishes havinga number of preliminary BLC.
Obviously, whileascending through this chain, more synsets aresubsumed by each concept.
The process finisheschecking if the number of concepts subsumed bythe preliminary list of BLC is higher than a cer-tain threshold.
For those BLC not representingenough concepts according to the threshold, theprocess selects the next local maximum followingthe hypernymy hierarchy.
Thus, depending on thetype of relations considered to be counted and thethreshold established, different sets of BLC can beeasily obtained for each WN version.In this paper, we empirically explore the perfor-mance of the different levels of abstraction pro-vided by Basic Level Concepts (BLC) (Izquierdoet al, 2007).Table 1 presents the total number of BLC andits average depth for WN1.6, varying the thresholdand the type of relations considered (all relationsor only hyponymy).Thres.
Rel.
PoS #BLC Av.
depth.0all Noun 3,094 7.09Verb 1,256 3.32hypo Noun 2,490 7.09Verb 1,041 3.3120all Noun 558 5.81Verb 673 1.25hypo Noun 558 5.80Verb 672 1.2150all Noun 253 5.21Verb 633 1.13hypo Noun 248 5.21Verb 633 1.10Table 1: BLC for WN1.6 using all or hyponym relations6http://adimen.si.ehu.es/web/BLCClassifier Examples # of exampleschurch.n#2 (sense approach) church.n#2 58church.n#2 58building.n#1 48hotel.n#1 39building, edifice (class approach) hospital.n#1 20barn.n#1 17....... ......TOTAL= 371 examplesTable 2: Examples and number of them in Semcor, forsense approach and for class approach3 Class-based WSDWe followed a supervised machine learning ap-proach to develop a set of class-based WSD tag-gers.
Our systems use an implementation of a Sup-port Vector Machine algorithm to train the clas-sifiers (one per class) on semantic annotated cor-pora for acquiring positive and negative examplesof each class and on the definition of a set of fea-tures for representing these examples.
The systemdecides and selects among the possible semanticclasses defined for a word.
In the sense approach,one classifier is generated for each word sense, andthe classifiers choose between the possible sensesfor the word.
The examples to train a single clas-sifier for a concrete word are all the examples ofthis word sense.
In the semantic?class approach,one classifier is generated for each semantic class.So, when we want to label a word, our programobtains the set of possible semantic classes forthis word, and then launch each of the semanticclassifiers related with these semantic categories.The most likely category is selected for the word.In this approach, contrary to the word sense ap-proach, to train a classifier we can use all examplesof all words belonging to the class represented bythe classifier.
In table 2 an example for a senseof ?church?
is shown.
We think that this approachhas several advantages.
First, semantic classes re-duce the average polysemy degree of words (someword senses are grouped together within the sameclass).
Moreover, the well known problem of ac-quisition bottleneck in supervised machine learn-ing algorithms is attenuated, because the numberof examples for each classifier is increased.3.1 The learning algorithm: SVMSupport Vector Machines (SVM) have beenproven to be robust and very competitive in manyNLP tasks, and in WSD in particular (Ma`rquez etal., 2006).
For these experiments, we used SVM-Light (Joachims, 1998).
SVM are used to learnan hyperplane that separates the positive from the391negative examples with the maximum margin.
Itmeans that the hyperplane is located in an interme-diate position between positive and negative ex-amples, trying to keep the maximum distance tothe closest positive example, and to the closestnegative example.
In some cases, it is not possi-ble to get a hyperplane that divides the space lin-early, or it is better to allow some errors to obtain amore efficient hyperplane.
This is known as ?soft-margin SVM?, and requires the estimation of a pa-rameter (C), that represent the trade-off allowedbetween training errors and the margin.
We haveset this value to 0.01, which has been proved as agood value for SVM in WSD tasks.When classifying an example, we obtain thevalue of the output function for each SVM clas-sifier corresponding to each semantic class for theword example.
Our system simply selects the classwith the greater value.3.2 CorporaThree semantic annotated corpora have been usedfor training and testing.
SemCor has been usedfor training while the corpora from the Englishall-words tasks of SensEval-2 and SensEval-3has been used for testing.
We also consid-ered SemEval-2007 coarse?grained task corpusfor testing, but this dataset was discarded becausethis corpus is also annotated with clusters of wordsenses.SemCor (Miller et al, 1993) is a subset of theBrown Corpus plus the novel The Red Badge ofCourage, and it has been developed by the samegroup that created WordNet.
It contains 253 textsand around 700,000 running words, and more than200,000 are also lemmatized and sense-tagged ac-cording to Princeton WordNet 1.6.SensEval-27 English all-words corpus (here-inafter SE2) (Palmer et al, 2001) consists on 5,000words of text from three WSJ articles represent-ing different domains from the Penn TreeBank II.The sense inventory used for tagging is WordNet1.7.
Finally, SensEval-38 English all-words cor-pus (hereinafter SE3) (Snyder and Palmer, 2004),is made up of 5,000 words, extracted from twoWSJ articles and one excerpt from the Brown Cor-pus.
Sense repository of WordNet 1.7.1 was usedto tag 2,041 words with their proper senses.7http://www.sle.sharp.co.uk/senseval28http://www.senseval.org/senseval33.3 Feature typesWe have defined a set of features to represent theexamples according to previous works in WSDand the nature of class-based WSD.
Featureswidely used in the literature as in (Yarowsky,1994) have been selected.
These features arepieces of information that occur in the context ofthe target word, and can be organized as:Local features: bigrams and trigrams thatcontain the target word, including part-of-speech(PoS), lemmas or word-forms.Topical features: word?forms or lemmas ap-pearing in windows around the target word.In particular, our systems use the following ba-sic features:Word?forms and lemmas in a window of 10words around the target wordPoS: the concatenation of the preced-ing/following three/five PoSBigrams and trigrams formed by lemmas andword-forms and obtained in a window of 5 words.We use of all tokens regardless their PoS to buildbi/trigrams.
The target word is replaced by Xin these features to increase the generalization ofthem for the semantic classifiersMoreover, we also defined a set of SemanticFeatures to explode different semantic resourcesin order to enrich the set of basic features:Most frequent semantic class calculated overSemCor, the most frequent semantic class for thetarget word.Monosemous semantic classes semanticclasses of the monosemous words arround thetarget word in a window of size 5.
Several typesof semantic classes have been considered to createthese features.
In particular, two different setsof BLC (BLC20 and BLC509), SuperSenses,WordNet Domains (WND) and SUMO.In order to increase the generalization capabil-ities of the classifiers we filter out irrelevant fea-tures.
We measure the relevance of a feature10.
ffor a class c in terms of the frequency of f. For eachclass c, and for each feature f of that class, we cal-culate the frequency of the feature within the class(the number of times that it occurs in examples9We have selected these set since they represent differentlevels of abstraction.
Remember that 20 and 50 refer to thethreshold of minimum number of synsets that a possible BLCmust subsume to be considered as a proper BLC.
These BLCsets were built using all kind of relations.10That is, the value of the feature, for example a featuretype can be word-form, and a feature of that type can be?houses?392of the class), and also obtain the total frequencyof the feature, for all the classes.
We divide bothvalues (classFreq / totalFreq) and if the result isnot greater than a certain threshold t, the featureis removed from the feature list of the class c11.In this way, we ensure that the features selectedfor a class are more frequently related with thatclass than with others.
We set this threshold t to0.25, obtained empirically with very preliminaryversions of the classifiers on SensEval3 test.4 Experiments and ResultsTo analyze the influence of each feature type in theclass-based WSD, we designed a large set of ex-periments.
An experiment is defined by two sets ofsemantic classes.
First, the semantic class type forselecting the examples used to build the classifiers(determining the abstraction level of the system).In this case, we tested: sense12, BLC20, BLC50,WordNet Domains (WND), SUMO and Super-Sense (SS).
Second, the semantic class type usedfor building the semantic features.
In this case, wetested: BLC20, BLC50, SuperSense, WND andSUMO.
Combining them, we generated the set ofexperiments described later.Test pos Sense BLC20 BLC50 WND SUMO SSSE2 N 4.02 3.45 3.34 2.66 3.33 2.73V 9.82 7.11 6.94 2.69 5.94 4.06SE3 N 4.93 4.08 3.92 3.05 3.94 3.06V 10.95 8.64 8.46 2.49 7.60 4.08Table 3: Average polysemy on SE2 and SE3Table 3 presents the average polysemy on SE2and SE3 of the different semantic classes.4.1 BaselinesThe most frequent classes (MFC) of each wordcalculated over SemCor are considered to be thebaselines of our systems.
Ties between classes ona specific word are solved obtaining the global fre-quency in SemCor of each of these tied classes,and selecting the more frequent class over thewhole training corpus.
When there are no occur-rences of a word of the test corpus in SemCor (weare not able to calculate the most frequent class ofthe word), we obtain again the global frequencyfor each of its possible semantic classes (obtained11Depending on the experiment, around 30% of the origi-nal features are removed by this filter.12We included this evaluation for comparison purposessince the current system have been designed for class-basedevaluation only.from WN) over SemCor, and we select the mostfrequent.4.2 ResultsTables 4 and 5 present the F1 measures (harmonicmean of recall and precision) for nouns and verbsrespectively when training our systems on Sem-Cor and testing on SE2 and SE3.
Those resultsshowing a statistically significant13 positive dif-ference when compared with the baseline are inmarked bold.
Column labeled as ?Class?
refers tothe target set of semantic classes for the classifiers,that is, the desired semantic level for each exam-ple.
Column labeled as ?Sem.
Feat.?
indicatesthe class of the semantic features used to train theclassifiers.
For example, class BLC20 combinedwith Semantic Feature BLC20 means that this setof classes were used both to label the test exam-ples and to define the semantic features.
In orderto compare their contribution we also performeda ?basicFeat?
test without including semantic fea-tures.As expected according to most literature inWSD, the performances of the MFC baselines arevery high.
In particular, those corresponding tonouns (ranging from 70% to 80%).
While nom-inal baselines seem to perform similarly in bothSE2 and SE3, verbal baselines appear to be con-sistently much lower for SE2 than for SE3.
InSE2, verbal baselines range from 44% to 68%while in SE3 verbal baselines range from 52% to79%.
An exception is the results for verbs con-sidering WND: the results are very high due tothe low polysemy for verbs according to WND.As expected, when increasing the level of abstrac-tion (from senses to SuperSenses) the results alsoincrease.
Finally, it also seems that SE2 task ismore difficult than SE3 since the MFC baselinesare lower.As expected, the results of the systems increasewhile augmenting the level of abstraction (fromsenses to SuperSenses), and almost in every case,the baseline results are reached or outperformed.This is very relevant since the baseline results arevery high.Regarding nouns, a very different behaviour isobserved for SE2 and SE3.
While for SE3 noneof the system presents a significant improvementover the baselines, for SE2 a significant improve-ment is obtained by using several types of seman-13Using the McNemar?s test.393tic features.
In particular, when using WordNetDomains but also BLC20.
In general, BLC20 se-mantic features seem to be better than BLC50 andSuperSenses.Regarding verbs, the system obtains significantimprovements over the baselines using differenttypes of semantic features both in SE2 and SE3.In particular, when using again WordNet Domainsas semantic features.In general, the results obtained by BLC20 arenot so much different to the results of BLC50(in a few cases, this difference is greater than2 points).
For instance, for nouns, if we con-sider the number of classes within BLC20 (558classes), BLC50 (253 classes) and SuperSense (24classes), BLC classifiers obtain high performancerates while maintaining much higher expressivepower than SuperSenses.
In fact, using Super-Senses (40 classes for nouns and verbs) we canobtain a very accurate semantic tagger with per-formances close to 80%.
Even better, we can useBLC20 for tagging nouns (558 semantic classesand F1 over 75%) and SuperSenses for verbs (14semantic classes and F1 around 75%).Obviously, the classifiers using WordNet Do-mains as target grouping obtain very high per-formances due to its reduced average polysemy.However, when used as semantic features it seemsto improve the results in most of the cases.In addition, we obtain very competitive classi-fiers at a sense level.4.3 Learning curvesWe also performed a set of experiments for mea-suring the behaviour of the class-based WSD sys-tem when gradually increasing the number oftraining examples.
These experiments have beencarried for nouns and verbs, but only noun resultsare shown since in both cases, the trend is verysimilar but more clear for nouns.The training corpus has been divided in portionsof 5% of the total number of files.
That is, com-plete files are added to the training corpus of eachincremental test.
The files were randomly selectedto generate portions of 5%, 10%, 15%, etc.
of theSemCor corpus14.
Then, we train the system oneach of the training portions and we test the sys-tem on SE2 and SE3.
Finally, we also compare the14Each portion contains also the same files than the previ-ous portion.
For example, all files in the 25% portion are alsocontained in the 30% portion.Class Sem.
Feat.
SensEval2 SensEval3Poly All Poly AllSensebaseline 59.66 70.02 64.45 72.30basicFeat 61.13 71.20 65.45 73.15BLC20 61.93 71.79 65.45 73.15BLC50 61.79 71.69 65.30 73.04SS 61.00 71.10 64.86 72.70WND 61.13 71.20 65.45 73.15SUMO 61.66 71.59 65.45 73.15BLC20baseline 65.92 75.71 67.98 76.29basicFeat 65.65 75.52 64.64 73.82BLC20 68.70 77.69 68.29 76.52BLC50 68.83 77.79 67.22 75.73SS 65.12 75.14 64.64 73.82WND 68.97 77.88 65.25 74.24SUMO 68.57 77.60 64.49 73.71BLC50baseline 67.20 76.65 68.01 76.74basicFeat 64.28 74.57 66.77 75.84BLC20 69.72 78.45 68.16 76.85BLC50 67.20 76.65 68.01 76.74SS 65.60 75.52 65.07 74.61WND 70.39 78.92 65.38 74.83SUMO 71.31 79.58 66.31 75.51WNDbaseline 78.97 86.11 76.74 83.8basicFeat 70.96 80.81 67.85 77.64BLC20 72.53 81.85 72.37 80.79BLC50 73.25 82.33 71.41 80.11SS 74.39 83.08 68.82 78.31WND 78.83 86.01 76.58 83.71SUMO 75.11 83.55 73.02 81.24SUMObaseline 66.40 76.09 71.96 79.55basicFeat 68.53 77.60 68.10 76.74BLC20 65.60 75.52 68.10 76.74BLC50 65.60 75.52 68.72 77.19SS 68.39 77.50 68.41 76.97WND 68.92 77.88 69.03 77.42SUMO 68.92 77.88 70.88 78.76SSbaseline 70.48 80.41 72.59 81.50basicFeat 69.77 79.94 69.60 79.48BLC20 71.47 81.07 72.43 81.39BLC50 70.20 80.22 72.92 81.73SS 70.34 80.32 65.12 76.46WND 73.59 82.47 70.10 79.82SUMO 70.62 80.51 71.93 81.05Table 4: Results for nounsresulting system with the baseline computed overthe same training portion.Figures 1 and 2 present the learning curves overSE2 and SE3, respectively, of a class-based WSDsystem based on BLC20 using the basic featuresand the semantic features built with WordNet Do-mains.Surprisingly, in SE2 the system only improvesthe F1 measure around 2% while increasing thetraining corpus from 25% to 100% of SemCor.In SE3, the system again only improves the F1measure around 3% while increasing the trainingcorpus from 30% to 100% of SemCor.
That is,most of the knowledge required for the class-basedWSD system seems to be already present on asmall part of SemCor.Figures 3 and 4 present the learning curves overSE2 and SE3, respectively, of a class-based WSDsystem based on SuperSenses using the basic fea-tures and the semantic features built with WordNetDomains.Again, in SE2 the system only improves the F1394Class Sem.
Feat.
SensEval2 SensEval3Poly All Poly AllSensebaseline 41.20 44.75 49.78 52.88basicFeat 42.01 45.53 54.19 57.02BLC20 41.59 45.14 53.74 56.61BLC50 42.01 45.53 53.6 56.47SS 41.80 45.34 53.89 56.75WND 42.01 45.53 53.89 56.75SUMO 42.22 45.73 54.19 57.02BLC20baseline 50.21 55.13 54.87 58.82basicFeat 52.36 57.06 57.27 61.10BLC20 52.15 56.87 56.07 59.92BLC50 51.07 55.90 56.82 60.60SS 51.50 56.29 57.57 61.29WND 54.08 58.61 57.12 60.88SUMO 52.36 57.06 57.42 61.15BLC50baseline 49.78 54.93 55.96 60.06basicFeat 53.23 58.03 58.07 61.97BLC20 52.59 57.45 57.32 61.29BLC50 51.72 56.67 57.01 61.01SS 52.59 57.45 57.92 61.83WND 55.17 59.77 58.52 62.38SUMO 52.16 57.06 57.92 61.83WNDbaseline 84.80 90.33 84.96 92.20basicFeat 84.50 90.14 78.63 88.92BLC20 84.50 90.14 81.53 90.42BLC50 84.50 90.14 81.00 90.15SS 83.89 89.75 78.36 88.78WND 85.11 90.52 84.96 92.20SUMO 85.11 90.52 80.47 89.88SUMObaseline 54.24 60.35 59.69 64.71basicFeat 56.25 62.09 61.41 66.21BLC20 55.13 61.12 61.25 66.07BLC50 56.25 62.09 61.72 66.48SS 53.79 59.96 59.69 64.71WND 55.58 61.51 61.56 66.35SUMO 54.69 60.74 60.00 64.98SSbaseline 62.79 68.47 76.24 79.07basicFeat 66.89 71.95 75.47 78.39BLC20 63.70 69.25 74.69 77.70BLC50 63.70 69.25 74.69 77.70SS 63.70 69.25 74.84 77.84WND 66.67 71.76 77.02 79.75SUMO 64.84 70.21 74.69 77.70Table 5: Results for verbsmeasure around 2% while increasing the trainingcorpus from 25% to 100% of SemCor.
In SE3,the system again only improves the F1 measurearound 2% while increasing the training corpusfrom 30% to 100% of SemCor.
That is, with only25% of the whole corpus, the class-based WSDsystem reaches a F1 close to the performance us-ing all corpus.
This evaluation seems to indicatethat the class-based approach to WSD reduces dra-matically the required amount of training exam-ples.In both cases, when using BLC20 or Super-Senses as semantic classes for tagging, the be-haviour of the system is similar to MFC baseline.This is very interesting since the MFC obtains highresults due to the way it is defined, since the MFCover the total corpus is assigned if there are no oc-currences of the word in the training corpus.
With-out this definition, there would be a large numberof words in the test set with no occurrences whenusing small training portions.
In these cases, therecall of the baselines (and in turn F1) would be626466687072747678805  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100F1% corpusSystem SV2MFC SV2Figure 1: Learning curve of BLC20 on SE26264666870727476785  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100F1% corpusSystem SV3MFC SV3Figure 2: Learning curve of BLC20 on SE3much lower.5 Conclusions and discussionWe explored on the WSD task the performanceof different levels of abstraction and sense group-ings.
We empirically demonstrated that BaseLevel Concepts are able to group word senses intoan adequate medium level of abstraction to per-form supervised class?based disambiguation.
Wealso demonstrated that the semantic classes pro-vide a rich information about polysemous wordsand can be successfully used as semantic fea-tures.
Finally we confirm the fact that the class?based approach reduces dramatically the requiredamount of training examples, opening the way tosolve the well known acquisition bottleneck prob-lem for supervised machine learning algorithms.In general, the results obtained by BLC20 arenot very different to the results of BLC50.
Thus,we can select a medium level of abstraction, with-out having a significant decrease of the perfor-3956870727476788082845  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100F1% corpusSystem SV2MFC SV2Figure 3: Learning curve of SuperSense on SE2707274767880825  10  15  20  25  30  35  40  45  50  55  60  65  70  75  80  85  90  95  100F1% corpusSystem SV3MFC SV3Figure 4: Learning curve of SuperSense on SE3mance.
Considering the number of classes, BLCclassifiers obtain high performance rates whilemaintaining much higher expressive power thanSuperSenses.
However, using SuperSenses (46classes) we can obtain a very accurate semantictagger with performances around 80%.
Even bet-ter, we can use BLC20 for tagging nouns (558 se-mantic classes and F1 over 75%) and SuperSensesfor verbs (14 semantic classes and F1 around75%).As BLC are defined by a simple and fully au-tomatic method, they can provide a user?definedlevel of abstraction that can be more suitable forcertain NLP tasks.Moreover, the traditional set of features used forsense-based classifiers do not seem to be the mostadequate or representative for the class-based ap-proach.
We have enriched the usual set of fea-tures, by adding semantic information from themonosemous words of the context and the MFCof the target word.
With this new enriched set offeatures, we can generate robust and competitiveclass-based classifiers.To our knowledge, the best results for class?based WSD are those reported by (Ciaramita andAltun, 2006).
This system performs a sequencetagging using a perceptron?trained HMM, usingSuperSenses, training on SemCor and testing onSensEval3.
The system achieves an F1?score of70.54, obtaining a significant improvement froma baseline system which scores only 64.09.
Inthis case, the first sense baseline is the SuperSenseof the most frequent synset for a word, accordingto the WN sense ranking.
Although this result isachieved for the all words SensEval3 task, includ-ing adjectives, we can compare both results sincein SE2 and SE3 adjectives obtain very high per-formance figures.
Using SuperSenses, adjectivesonly have three classes (WN Lexicographic Files00, 01 and 44) and more than 80% of them belongto class 00.
This yields to really very high perfor-mances for adjectives which usually are over 90%.As we have seen, supervised WSD systems arevery dependent of the corpora used to train andtest the system.
We plan to extend our system byselecting new corpora to train or test.
For instance,by using the sense annotated glosses from Word-Net.ReferencesE.
Agirre and O. LopezDeLaCalle.
2003.
Clusteringwordnet word senses.
In Proceedings of RANLP?03,Borovets, Bulgaria.J.
Alvez, J. Atserias, J. Carrera, S. Climent, E. Laparra,A.
Oliver, and G. Rigau G. 2008.
Complete andconsistent annotation of wordnet using the top con-cept ontology.
In 6th International Conference onLanguage Resources and Evaluation LREC, Mar-rakesh, Morroco.M.
Ciaramita and Y. Altun.
2006.
Broad-coveragesense disambiguation and information extractionwith a supersense sequence tagger.
In Proceedingsof the Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?06), pages 594?602,Sydney, Australia.
ACL.M.
Ciaramita and M. Johnson.
2003.
Supersense tag-ging of unknown nouns in wordnet.
In Proceedingsof the Conference on Empirical methods in naturallanguage processing (EMNLP?03), pages 168?175.ACL.J.
Curran.
2005.
Supersense tagging of unknownnouns using semantic similarity.
In Proceedings ofthe 43rd Annual Meeting on Association for Compu-tational Linguistics (ACL?05), pages 26?33.
ACL.396C.
Fellbaum, editor.
1998.
WordNet.
An ElectronicLexical Database.
The MIT Press.M.
Hearst and H. Schu?tze.
1993.
Customizing a lexi-con to better suit a computational task.
In Proceed-ingns of the ACL SIGLEX Workshop on Lexical Ac-quisition, Stuttgart, Germany.R.
Izquierdo, A. Suarez, and G. Rigau.
2007.
Explor-ing the automatic selection of basic level concepts.In Galia Angelova et al, editor, International Con-ference Recent Advances in Natural Language Pro-cessing, pages 298?302, Borovets, Bulgaria.T.
Joachims.
1998.
Text categorization with supportvector machines: learning with many relevant fea-tures.
In Claire Ne?dellec and Ce?line Rouveirol, edi-tors, Proceedings of ECML-98, 10th European Con-ference on Machine Learning, number 1398, pages137?142, Chemnitz, DE.
Springer Verlag, Heidel-berg, DE.B.
Magnini and G. Cavaglia`.
2000.
Integrating subjectfield codes into wordnet.
In Proceedings of LREC,Athens.
Greece.Ll.
Ma`rquez, G. Escudero, D.
Mart?
?nez, and G. Rigau.2006.
Supervised corpus-based methods for wsd.
InE.
Agirre and P. Edmonds (Eds.)
Word Sense Disam-biguation: Algorithms and applications., volume 33of Text, Speech and Language Technology.
Springer.R.
Mihalcea and D. Moldovan.
2001.
Automatic gen-eration of coarse grained wordnet.
In Proceding ofthe NAACL workshop on WordNet and Other Lex-ical Resources: Applications, Extensions and Cus-tomizations, Pittsburg, USA.R.
Mihalcea.
2007.
Using wikipedia for automaticword sense disambiguation.
In Proceedings ofNAACL HLT 2007.G.
Miller, C. Leacock, R. Tengi, and R. Bunker.
1993.A Semantic Concordance.
In Proceedings of theARPA Workshop on Human Language Technology.R.
Navigli.
2006.
Meaningful clustering of senseshelps boost word sense disambiguation perfor-mance.
In ACL-44: Proceedings of the 21st Inter-national Conference on Computational Linguisticsand the 44th annual meeting of the Association forComputational Linguistics, pages 105?112, Morris-town, NJ, USA.
Association for Computational Lin-guistics.I.
Niles and A. Pease.
2001.
Towards a standard upperontology.
In Proceedings of the 2nd InternationalConference on Formal Ontology in Information Sys-tems (FOIS-2001), pages 17?19.
Chris Welty andBarry Smith, eds.M.
Palmer, C. Fellbaum, S. Cotton, L. Delfs, andH.
Trang Dang.
2001.
English tasks: All-words and verb lexical sample.
In Proceedingsof the SENSEVAL-2 Workshop.
In conjunction withACL?2001/EACL?2001, Toulouse, France.W.
Peters, I. Peters, and P. Vossen.
1998.
Automaticsense clustering in eurowordnet.
In First Interna-tional Conference on Language Resources and Eval-uation (LREC?98), Granada, Spain.F.
Segond, A. Schiller, G. Greffenstette, and J. Chanod.1997.
An experiment in semantic tagging using hid-den markov model tagging.
In ACL Workshop onAutomatic Information Extraction and Building ofLexical Semantic Resources for NLP Applications,pages 78?81.
ACL, New Brunswick, New Jersey.R.
Snow, Prakash S., Jurafsky D., and Ng A.
2007.Learning to merge word senses.
In Proceedings ofJoint Conference on Empirical Methods in NaturalLanguage Processing and Computational NaturalLanguage Learning (EMNLP-CoNLL), pages 1005?1014.B.
Snyder and M. Palmer.
2004.
The english all-wordstask.
In Rada Mihalcea and Phil Edmonds, edi-tors, Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysisof Text, pages 41?43, Barcelona, Spain, July.
Asso-ciation for Computational Linguistics.L.
Villarejo, L. Ma`rquez, and G. Rigau.
2005.
Ex-ploring the construction of semantic class classi-fiers for wsd.
In Proceedings of the 21th AnnualMeeting of Sociedad Espaola para el Procesamientodel Lenguaje Natural SEPLN?05, pages 195?202,Granada, Spain, September.
ISSN 1136-5948.P.
Vossen, L. Bloksma, H. Rodriguez, S. Climent,N.
Calzolari, A. Roventini, F. Bertagna, A. Alonge,and W. Peters.
1998.
The eurowordnet base con-cepts and top ontology.
Technical report, Paris,France, France.D.
Yarowsky.
1994.
Decision lists for lexical ambigu-ity resolution: Application to accent restoration inspanish and french.
In Proceedings of the 32nd An-nual Meeting of the Association for ComputationalLinguistics (ACL?94).397
