Proceedings of the 15th Conference on Computational Natural Language Learning: Shared Task, pages 117?121,Portland, Oregon, 23-24 June 2011. c?2011 Association for Computational LinguisticsA Machine Learning-Based Coreference Detection System For OntoNotesYaqin YangComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USAyaqin@brandeis.eduNianwen XueComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USAxuen@brandeis.eduPeter AnickComputer Science DepartmentBrandeis UniversityWaltham, Massachusetts, USApeter anick@yahoo.comAbstractIn this paper, we describe the algorithms andexperimental results of Brandeis Universityin the participation of the CoNLL Task 2011closed track.
We report the features used inour system, and describe a novel cluster-basedchaining algorithm to improve performance ofcoreference identification.
We evaluate thesystem using the OntoNotes data set and de-scribe our results.1 IntroductionThis paper describes the algorithms designed andexperiments finished in the participation of theCoNLL Task 2011.
The goal of the Task is to designefficient algorithms for detecting entity candidatesand identifying coreferences.
Coreference identifi-cation is an important technical problem.
Its impor-tance in NLP applications has been observed in pre-vious work, such as that of Raghunathan et al, Prad-han et al, Bergsma et al, Haghighi et al, and Ng etal..
While most of the existing work has evaluatedtheir systems using the ACE data set, in this workwe present our experimental results based on theOntoNotes data set used in the CoNLL 2011 SharedTask.
We detail a number of linguistic featuresthat are used during the experiments, and highlighttheir contribution in improving coreference identi-fication performance over the OntoNotes data set.We also describe a cluster-based approach to multi-entity chaining.
Finally, we report experimental re-sults and summarize our work.2 Data PreparationWe divide the CoNLL Task into three steps.
First,we detect entities from both the training data andthe development data.
Second, we group related en-tities into entity-pairs.
Finally, we use the gener-ated entity-pairs in the machine learning-based clas-sifier to identify coreferences.
In this section, wedescribe how we extract the entities and group theminto pairs.2.1 Generating Entity CandidatesWe use the syntactic parse tree to extract four typesof entities, including noun phrase, pronoun, pre-modifier and verb (Pradhan et al, 2007).
Thismethod achieves 94.0% (Recall) of detection accu-racy for gold standard trees in the development data.When using the automatic parses, not surprisingly,the detection accuracy becomes lower, with a per-formance drop of 5.3% (Recall) compared with thatof using the gold standard trees.
Nevertheless, thismethod can still cover 88.7% of all entities existingin the development data, thus we used it in our algo-rithm.2.2 Generating Entity-Pairs From IndividualEntitiesIn the annotated training documents, an entity hasbeen marked in a coreference chain that includes allcoreferential entities.
In our algorithm, we only de-tect the closest antecedent for each entity, insteadof all coreferences, of each entity.
Specifically, wedefine each training and testing instance as a pairof entities.
During the training process, for eachentity encountered by the system, we create a pos-itive instance by pairing an entity with its closestantecedent (Soon et al, 2001).
In addition, a setof negative instances are also created by pairing theentity with any preceding entities that exist betweenits closest antecedent and the entity itself (note thatthe antecedent must be a coreference of the currententity, whereas preceding entities may not be coref-erential).
For example, in the entity sequence ?A,B, C, D, E?, let us assume that ?A?
is the closestantecedent of ?D?.
Then, for entity ?D?, ?A-D?
isconsidered a positive instance, whereas ?B-D?
and?C-D?
are two negative instances.To generate testing data, every entity-pair withinthe same sentence is considered to form positive ornegative instances, which are then used to form test-ing data.
Since occasionally the distance between anentity and its closest antecedent can be far apart, wehandle considerably distant coreferences by consid-117ering each entity-pair that exists within the adjacentN sentences.
During our experiments, we observedthat the distance between an entity and its closest an-tecedent could be as far as 23 sentences.
Therefore,in the classification process, we empirically set N as23.3 Machine Learning-Based ClassificationAfter labeling entity pairs, we formalize the corefer-ence identification problem as a binary classificationproblem.
We derive a number of linguistic featuresbased on each entity-pair, i and j, where i is the po-tential antecedent and j the anaphor in the pair (Soonet al, 2001).
Generally, we select a set of featuresthat have been proved to be useful for the corefer-ence classification tasks in previous work, includ-ing gender, number, distance between the antecedentand the anaphor, and WordNet (WordNet, 2010).
Inaddition, we design additional features that couldbe obtained from the OntoNotes data, such as thespeaker or author information that is mainly avail-able in Broadcast Conversation and Web Log data(Pradhan et al, 2007).
Moreover, we extract appo-sition and copular structures and used them as fea-tures.
The features we used in the system are de-tailed below.?
Independent feature: 1) if a noun phrase is defi-nite; 2) if a noun phrase is demonstrative; 3) genderinformation of each entity; 4) number informationof each entity; 5) the entity type of a noun phrase;6) if an entity is a subject; 7) if an entity is an object;8) if an noun phrase is a coordination, the numberof entities it has; 9) if a pronoun is preceded by apreposition; 10) if a pronoun is ?you?
or ?me?
; 11)if a pronoun is ?you?
and it is followed by the word?know?.?
Name entity feature: 1) i-j-same-entity-type-etype=True, if i and j have the same entity type;2) i-j-same-etype-subphrase=True, if i and j havethe same entity type and one is the subphrase of theother.?
Syntactic feature: 1) i-j-both-subject=True, if iand j are both subjects; 2) if i and j are in the samesentence, record the syntactic path between i and j,e.g.
i-j-syn-path=PRP?NP!PRP; 3) i-j-same-sent-diff-clause=True, if i and j are in the same sentencebut in different clauses.?
Gender and number feature: 1) i-j-same-gender=True/False, by comparing if i and j have thesame gender; 2) i-j-same-num=True/False, by com-paring if i and j have the same number; 3) i-j-same-num-modifier=True/False, by comparing if i and jhave the same number modifier, e.g.
?two coun-tries?
and ?they both?
have the same number mod-ifier; 4) i-j-same-family=True/False, we designedseven different families for pronouns, e.g.
?it?, ?its?and ?itself?
are in one family while ?he?, ?him?,?his?
and ?himself?
are in another one.?
Distance feature: 1) i-j-sent-dist, if the sentencedistance between i and j is smaller than three, usetheir sentence distance as a feature; 2) i-j-sent-dist=medium/far: if the sentence distance is largerthan or equal to three, set the value of i-j-sent-distto ?medium?, otherwise set it to ?far?
combinedwith the part-of-speech of the head word in j.?
String and head word match feature: 1) i-j-same-string=True, if i and j have the same string;2) i-j-same-string-prp=True, if i and j are thesame string and they are both pronouns; 3) i-j-sub-string=True, if one is the sub string of the other,and neither is a pronoun; 4) i-j-same-head=True,if i and j have the same head word; 5) i-j-prefix-head=True, if the head word of i or j is the pre-fix of the head word of the other; 6) i-j-loose-head,the same as i-j-prefix-head, but comparing only thefirst four letters of the head word.?
Apposition and copular feature: for each nounphrase, if it has an apposition or is followed bya copular verb, then the apposition or the subjectcomplement is used as an attribute of that nounphrase.
We also built up a dictionary where thekey is the noun phrase and the value is its apposi-tion or the subject?s complement to define features.1) i-appo-j-same-head=True, if i?s apposition andj have the same head word; 2) i-j-appo-same-head=True, if j?s apposition has the same head wordas i; we define the similar head match features forthe noun phrase and its complement; Also, if an ior j is a key in the defined dictionary, we get thehead word of the corresponding value for that keyand compare it to the head word of the other entity.?
Alias feature: i-j-alias=True, if one entity is aproper noun, then we extract the first letter of eachword in the other entity.
( The extraction processskips the first word if it?s a determiner and also skipsthe last one if it is a possessive case).
If the propernoun is the same as the first-letter string, it is thealias of the other entity.?
Wordnet feature: for each entity, we used Wordnetto generate all synsets for its head word, and foreach synset, we get al hypernyms and hyponyms.1) if i is a hypernym of j, then i-hyper-j=True; 2)if i is a hyponym of j, then i-hypo-j=True.?
Speaker information features: In a conversation,a speaker usually uses ?I?
to refer to himself/herself,and most likely uses ?you?
to refer to the nextspeaker.
Since speaker or author name informa-tion is given in Broadcast Conversation and WebLog data, we use such information to design fea-tures that represent relations between pronouns and118speakers.
1) i-PRP1-j-PRP2-same-speaker=True,if both i and j are pronouns, and they have the samespeaker; 2) i-I-j-I-same-speaker=True, if both i andj are ?I?, and they have the same speaker; 3) i-I-j-you-same-speaker=True, if i is ?I?
and j is ?you?,and they have the same speaker; 4) if i is ?I?, jis ?you?
and the speaker of j is right after that ofi, then we have feature i-I-j-you&itarget=jspeaker;5) if i is ?you?, j is ?I?
and the speaker of j isright after that of i, then we have feature i-you-j-I-itarget=jspeaker; 6) if both i and j are ?you?,and they followed by the same speaker, we consider?you?
as a general term, and this information is usedas a negative feature.?
Other feature: i-j-both-prp=True, if both i and jare pronouns.4 Chaining by Using ClustersAfter the classifier detects coreferential entities,coreference detection systems usually need to chainmultiple coreferential entity-pairs together, forminga coreference chain.
A conventional approach isto chain all entities in multiple coreferential entity-pairs if they share the same entities.
For example, if?A-B?, ?B-C?, and ?C-D?
are coreferential entity-pairs, then A, B, C, and D would be chained to-gether, forming a coreference chain ?A-B-C-D?.One significant disadvantage of this approach isthat it is likely to put different coreference chains to-gether in the case of erroneous classifications.
Forexample, suppose in the previous case, ?B-C?
is ac-tually a wrong coreference detection, then the coref-erence chain created above will cause A and D to bemistakenly linked together.
This error can propagateas coreference chains become larger.To mitigate this issue, we design a cluster-basedchaining approach.
This approach is based on theobservation that some linguistic rules are capable ofdetecting coreferential entities with high detectionprecision.
This allows us to leverage these rules todouble-check the coreference identifications, and re-ject chaining entities that are incompatible with rule-based results.To be specific, we design two lightweight yet ef-ficient rules to cluster entities.?
Rule One.
For the first noun phrase (NP) encoun-tered by the system, if 1) this NP has a name entityon its head word position or 2) it has a name en-tity inside and the span of this entity includes thehead word position, a cluster is created for this NP.The name entity of this NP is also recorded.
Foreach following NP with a name entity on its headword position, if there is a cluster that has the samename entity, this NP is considered as a coreferenceto other NPs in that cluster, and is put into that clus-ter.
If the system cannot find such a cluster, a newcluster is created for the current NP.?
Rule Two.
In Broadcast Conversation or Web Logdata, a speaker or author would most likely use ?I?to refer to himself/herself.
Therefore, we used itas the other rule to cluster all ?I?
pronouns and thesame speaker information together.Given the labeled entity pairs, we then link them indifferent coreference chains by using the cluster in-formation.
As the Maximum Entropy classifier notonly labels each entity-pair but also returns a con-fidence score of that label, we sort all positive pairsusing their possibilities.
For each positive entity-pairin the sorted list, if the two entities are in differentclusters, we consider this to be a conflict, and with-draw this positive entity-pair; if one entity belongs toone cluster whereas the other does not belong to anycluster, the two entities will be both included in thatcluster.
This process is repeated until no more enti-ties can be included in a cluster.
Finally, we chainthe rest of entity pairs together.5 Results and DiscussionTo evaluate the features and the chaining approachdescribed in this paper, we design experiments de-scribed as follows.
Since there are five differentdata types in the provided OntoNotes coreferencedata set, we create five different classifiers to pro-cess each of the data types.
We used the featuresdescribed in Section 3 to train the classifiers, anddid the experiments using a Maximum Entropy clas-sifier trained with the Mallet package (McCallum,2002).
We use the gold-standard data in the trainingset to train the five classifiers and test the classifierson both gold and automatically-parsed data in thedevelopment data set.
The MUC metric provided bythe Task is used to evaluate the results.5.1 Performance without ClusteringFirst, we evaluate the system by turning the clus-tering technique off during the process of creatingcoreference chains.
For entity detection, we ob-serve that for all five data types, i.e.
Broadcast(BC), Broad news (BN), Newswire (NW), Magazine(MZ), and Web blog (WB), the NW and WB datatypes achieve relatively lower F1-scores, whereasthe BC, BN, and MZ data types achieve higher per-119BC BN NW MZ WBWithout ClusteringGold 57.40 (64.92/51.44) 59.45 (63.53/55.86) 52.01 (59.71/46.07) 55.59 (62.90/49.80) 49.53 (61.16/41.62)Auto 54.00 (61.28/48.26) 55.40 (59.05/52.17) 48.44 (55.32/43.09) 52.21 (59.78/46.33) 47.02 (58.33/39.39)With ClusteringGold 57.44 (64.12/52.03) 56.56 (58.10/55.09) 51.37 (56.64/46.99) 54.26 (60.07/49.47) 49.00 (60.09/41.36)Auto 54.19 (60.82/48.87) 52.69 (54.07/51.37) 48.01 (52.74/44.05) 50.82 (56.76/46.01) 46.86 (57.49/39.55)Table 1: Performance comparison of coreference identification between using and without using the clustering tech-nique in chaining.
Note that the results are listed in sequence of F1-scores (Recalls/Precisions).
The results shown arebased on MUC.formance.
Due to limited space, the performancetable of entity detection is not included in this paper.For coreference identification, as shown in Ta-ble 1, we observe pretty similar performance gapsamong different data types.
The NW and WB datatypes achieve the lowest F1-scores (i.e.
52.01%and 49.53% for gold standard data, and 48.44% and47.02% for automatically-parsed data) among all thefive data types.
This can be explained by seeing thatthe entity detection performance of these two datatypes are also relatively low.
The other three typesachieves more than 55% and 52% F1-scores for goldand auto data, respectively.These experiments that are done without usingclustering techniques tend to indicate that the perfor-mance of entity detection has a positive correlationwith that of coreference identification.
Therefore, inthe other set of experiments, we enable the cluster-ing technique to improve coreference identificationperformance by increasing entity detection accuracy.Metric Recall Precision F1MUC 59.94 45.38 51.65BCUBED 72.07 53.65 61.51CEAF (M) 45.67 45.67 45.67CEAF (E) 29.43 42.54 34.79BLANC 70.86 60.55 63.37Table 2: Official results of our system in the CoNLL Task2011.
Official score is 49.32.
((MUC + BCUBED +CEAF (E))/3)5.2 Performance with ClusteringAfter enabling the clustering technique, we observean improvement in entity detection performance.This improvement occurs mainly in the cases of theNW and WB data types, which show low entitydetection performance when not using the cluster-ing technique.
To be specific, the performance ofthe NW type on both the gold standard and auto-matic data improves by about 0.5%, and the perfor-mance of the WB type on the automatic data im-proves about 0.1%.
In addition, the performance ofthe BC type on both the gold standard and automaticdata also increases about 0.2% to 0.6%.Although the clustering technique succeeds in im-proving entity detection performance for multipledata types, there is no obvious improvement gainedwith respect to coreference identification.
This isquite incompatible with our observation in the ex-periments that do not utilize the clustering tech-nique.
Currently, we attribute this issue to the lowaccuracy rates of the clustering operation.
For ex-ample, ?H.
D.
Ye.?
and ?Ye?
can be estimated cor-rectly to be coreferential by the Maxtent classifier,but the clustering algorithm puts them into differentclusters since ?H.
D.
Ye.?
is a PERSON type nameentity while ?Ye?
is a ORG type name entity.
There-fore, the system erroneously considers them to be aconflict and rejects them.
We plan to investigate thisissue further in our future work.The official results of our system in the CoNLLTask 2011 are summarized in Table 2.6 ConclusionIn this paper, we described the algorithm design andexperimental results of Brandeis University in theCoNLL Task 2011.
We show that several linguisticfeatures perform well in the OntoNotes data set.ReferencesAndrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.120Sameer S. Pradhan, Lance Ramshaw, Ralph Weischedel,Jessica MacBride, and Linnea Micciulla.
2007.
Unre-stricted Coreference: Identifying Entities and Eventsin OntoNotes.
International Conference on SemanticComputing (ICSC 2007), pages 446?453, September.W.M.
Soon, H.T.
Ng, and D.C.Y.
Lim.
2001.
A ma-chine learning approach to coreference resolution ofnoun phrases.
Computational Linguistics, 27(4):521?544.WordNet.
2010.
Princeton University ?AboutWordNet.?
WordNet.
Princeton University.
2010.http://wordnet.princeton.edu.121
