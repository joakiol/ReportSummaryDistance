Proceedings of the SIGDIAL 2013 Conference, pages 344?348,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsA Semi-supervised Approach for Natural Language Call RoutingTatiana GasanovaInstitute of Communications Engineer-ing, Ulm University, Germanytatiana.gasanova@uni-ulm.deEugene ZhukovInstitute of Computer Science andTelecommunications, Siberian StateAerospace University, Russiazhukov.krsk@gmail.comRoman SergienkoInstitute of Computer Science andTelecommunications, Siberian StateAerospace University, Russiaromaserg@list.ruEugene SemenkinInstitute of Computer Science andTelecommunications, Siberian StateAerospace University, Russiaeugenesemenkin@yandex.comWolfgang MinkerInstitute of Communications Engineer-ing, Ulm University, Germanywolfgang.minker@uni-ulm.deAbstractNatural Language call routing remains a com-plex and challenging research area in machineintelligence and language understanding.
Thispaper is in the area of classifying user utter-ances into different categories.
The focus is ondesign of algorithm that combines supervisedand unsupervised learning models in order toimprove classification quality.
We have shownthat the proposed approach is able to outper-form existing methods on a large dataset anddo not require morphological and stop-wordfiltering.
In this paper we present a new for-mula for term relevance estimation, which is amodification of fuzzy rules relevance estima-tion for fuzzy classifier.
Using this formulaand only 300 frequent words for each class, weachieve an accuracy rate of 85.55% on the da-tabase excluding the ?garbage?
class (it in-cludes utterances that cannot be assigned toany useful class or that can be assigned tomore than one class).
Dividing the ?garbage?class into the set of subclasses by agglomera-tive hierarchical clustering we achieve about9% improvement of accuracy rate on thewhole database.1 IntroductionNatural language call routing can be treated as aninstance of topic categorization of documents(where the collection of labeled documents isused for training and the problem is to classifythe remaining set of unlabeled test documents)but it also has some differences.
For instance, indocument classification there are much moreterms in one object than in single utterance fromcall routing task, where even one-word utteranc-es are common.A number of works have recently been publishedon natural language call classification.
B. Car-penter, J. Chu-Carroll, C.-H. Lee and H.-K. Kuoproposed approaches using a vector-based in-formation retrieval technique, the algorithms de-signed by A. L. Gorin, G. Riccardi, and J. H.Wright use a probabilistic model with salientphrases.
R. E. Schapire and Y.
Singer focused ona boosting-based system for text categorization.The most similar work has been done by A.Albalate, D. Suendermann, R. Pieraccini, A.Suchindranath, S. Rhinow, J. Liscombe, K.Dayanidhi, and W. Minker.
They have workedon the data with the same structure: the focuswas on the problem of big part of non-labeleddata and only few labeled utterances for eachclass, methods of matching the obtained clustersand the given classes have also been considered;they provided the comparison of several classifi-cation methods that are able to perform on thelarge scale data.The information retrieval approach for call rout-ing is based on the training of the routing matrix,which is formed by statistics of appearances of344words and phrases in a training set (usually aftermorphological and stop-word filtering).
The newcaller request is represented as a feature vectorand is routed to the most similar destination vec-tor.
The most commonly used similarity criterionis the cosine similarity.
The performance of sys-tems, based on this approach, often depends onthe quality of the destination vectors.In this paper we propose a new term relevanceestimation approach based on fuzzy rules rele-vance for fuzzy classifier (H. Ishibuchi, T.Nakashima, and T.
Murata., 1999) to improverouting accuracy.
We have also used a decisionrule different from the cosine similarity.
We as-sign relevancies to every destination (class), cal-culate the sums of relevancies of words from thecurrent utterance and choose the destination withthe highest sum.The database for training and performance eval-uation consists of about 300.000 user utterancesrecorded from caller interactions with commer-cial automated agents.
The utterances were man-ually transcribed and classified into 20 classes(call reasons), such as appointments, operator,bill, internet, phone or video.
Calls that cannot berouted certainly to one reason of the list are clas-sified to class _TE_NOMATCH.A significant part of the database (about 27%)consists of utterances from the ?garbage?
class(_TE_NOMATCH).
Our proposed approach de-composes the routing task into two steps.
On thefirst step we divide the ?garbage?
class into theset of subclasses by one of the clustering algo-rithms and on the second step we define the callreason considering the ?garbage?
subclasses asseparate classes.
We apply genetic algorithmswith the whole numbers alphabet, vector quanti-zation network and hierarchical agglomerativeclustering in order to divide ?garbage?
class intosubclasses.
The reason to perform such a cluster-ing is due to simplify the detection of the classwith non-uniform structure.Our approach uses the concept of salient phrases:for each call reason (class) only 300 words withthe highest term relevancies are chosen.
It allowsus to eliminate the need for the stop and ignoreword filtering.
The algorithms are implementedin C++.As a baseline for results comparison we havetested some popular classifiers from RapidMiner,which we have applied to the whole database andthe database with decomposition.This paper is organized as follows: In Section II,we describe the problem and how we perform thepreprocessing.
Section III describes in detail theway of the term relevance calculating and thepossible rules of choosing the call class.
In Sec-tion IV we present the clustering algorithmswhich we apply to simplify the ?garbage?
classdetection.
Section V reports on the experimentalresults.
Finally, we provide concluding remarksin Section VI.2 Problem Description and Data Pre-processingThe data for testing and evaluation consists ofabout 300.000 user utterances recorded fromcaller interactions with commercial automatedagents.
Utterances from this database are manu-ally labeled by experts and divided into 20 clas-ses (_TE_NOMATCH, appointments, operator,bill, internet, phone etc).
Class _TE_NOMATCHincludes utterances that cannot be put into anoth-er class or can be put into more than one class.The database is also unbalanced, some classesinclude much more utterances than others (thelargest class _TE_NOMATCH includes 6790 ut-terances and the smallest one consists of only 48utterances).The initial database has been preprocessed to bea binary matrix with rows representing utterancesand columns representing the words from thevocabulary.
An element from this binary matrix,aij, equals to 1 if in utterance i the word j appearsand equals to 0 if it does not appear.Utterance duplicates were removed.
The prepro-cessed database consisting of 24458 utteranceswas divided into train (22020 utterances,90,032%) and test set (2438 utterances, 9,968%)such that the percentage of classes remained thesame in both sets.
The size of the dictionary ofthe whole database is 3464 words, 3294 wordsappear in training set, 1124 words appear in testset, 170 words which appear only in test set anddo not appear in training set (unknown words),33 utterances consisted of only unknown words,and 160 utterances included at least one un-known word.3 Term Relevance EstimationFor each term we assign a real number term rele-vance that depends on the frequency in utteranc-es.
Term relevance is calculated using a modifiedformula of fuzzy rules relevance estimation forfuzzy classifier.
Membership function has beenreplaced by word frequency in the current class.The details of the procedure are:Let L be the number of classes; ni is the numberof utterances of the ith class; Nij is the number of345jth word occurrence in all utterances of the ithclass; Tji=Nji/ni is the relative frequency of jthword occurrence in the ith class.Rj=maxi Tji, Sj=arg(maxi Tji) is the number ofclass which we assign to jth word;The term relevance, Cj, is given byCj is higher if the word occurs often in few clas-ses than if it appears in many classes.The learning phase consists of counting the Cvalues for each term, it means that this algorithmuses the statistical information obtained fromtrain set.
We have tested several differentdecision rules defined in Table 1.Decision rulesRCFor each class i wecalculate AiThen we find the num-ber of class whichachieves maximum ofAiRC maxCC withlimitRTable 1.
Decision RulesThe best obtained accuracies is achieved with thedecision rule C, where the destination is chosenthat has the highest sum of word relevanciesfrom the current utterance.
In Table 2 we showthe obtained results on the whole database anddatabase without ?garbage?
class.Train TestWith class ?garbage?
0,614 0,551Without class ?garbage?
0,887 0,855Table 2.
Performance of the new TRE approach4 Clustering methodsAfter the analysis of the performances of stand-ard classification algorithms on the given data-base, we can conclude that there exists one spe-cific class (class _TE_NOMATCH) where allstandard techniques perform worse.
Due to thenon-uniform structure of the ?garbage?
class it isdifficult to detect the whole class by the pro-posed procedure.
If we apply this procedure di-rectly we achieve only 55% of accuracy rate onthe test data (61% on the train data).
We suggestto divide the ?garbage?
class into the set of sub-classes using one of the clustering methods andthen recount the values of Cj taking into accountthat there are 19 well defined classes and that theset of the ?garbage?
subclasses can be consideras separate classes.In this paper the following clustering methodsare used: a genetic algorithm with integers, vec-tor quantization networks trained by a geneticalgorithm, hierarchical agglomerative clusteringwith different metrics.4.1 Genetic AlgorithmThe train set accuracy is used as a fitness func-tion.
Each individual is the sequence of nonnega-tive integer numbers (each number correspondsto the number of ?garbage?
subclass).
The lengthof this sequence is the number of utterances fromtrain set which belong to the ?garbage?
class.We apply this genetic algorithm to find directlythe optimal clustering using different numbers ofclusters and we can conclude that with increasingthe clusters number (in the ?garbage?
class) weget better classification accuracy on the wholedatabase.
We have used the following parametersof GA: population size = 50, number of genera-tion = 50, weak mutation, tournament selection,uniform crossover, averaged by 50 runs.
Apply-ing this method we achieve about 7% improve-ment of accuracy rate on train data and about 5%on test data.4.2 Vector Quantization NetworkWe have also implemented vector quantizationnetwork.
For a given number of subclasses wesearch for the set of code vectors (the number ofcode vectors is equal to the number of sub-classes).
These code vectors are optimized usinggenetic algorithm where as a fitness function weuse the classification quality on the train set.Each code vector corresponds to a certain ?gar-bage?
subclass.
The object belongs to the sub-class if the distance between it and the corre-sponding code vector is smaller than the distanc-es between the object and all other code vectors.Applying this algorithm to the given database weobtain results similar to the results of the geneticalgorithm.4.3 Hierarchical Agglomerative ClusteringIn this work we consider hierarchical agglomera-tive binary clustering where we set each utter-ance to one subclass and then we consequentlygroup classes into pairs until there is only one).11(111??
?==?
?=LSiijijLijijjTLRTC?==iSjjjijCRA:)maxarg(iiAwinner =?==iSjjjijCRA:max?==iSjjijCA:?>==constCiSjjijjCA:?==iSjjijRA:346class containing all utterances or until weachieve a certain number of classes.
The perfor-mance of hierarchical clustering algorithms de-pends on the metric (the way to calculate the dis-tance between objects) and the criterion for clus-ters union.
In this work we use Hamming metricand Ward criterion (J.
Ward.
1963).5 Experimental resultsThe approach described above has been appliedon the preprocessed corpus which has been pro-vided by Speech Cycle company.
We proposethat only terms with highest value of RC (prod-uct of R and C) are contributed to the total sum.We have investigated the dependence of the newTRE approach on the frequent words number(Figure 1).
The best accuracy rate was obtainedwith more than 300 frequent words.
By usingonly limited set of words we eliminated the needof stop and ignore words filtering.
This alsoshows that the method works better if utteranceincludes terms with high C values.
This approachrequires informative well-defined classes andenough data for statistical model.Figure 1.
New TRE approach with different numbersof frequent words (x-axis: number of frequent words;y-axis: accuracy)Figure 2.
Overall accuracyFigure 3.
Comparison of decision rules (x-axis: deci-sion rule; y-axis: accuracy)We have tested standard classification algorithms(k-nearest neighbors algorithms, Bayes classifi-ers, Decision Stump, Rule Induction, perceptron)and the proposed approach on the database with?garbage?
class and on the database without it(Figure 2).
The proposed algorithm outperformsall other methods with has an accuracy rate of85.55%.
Figure 3 provides accuracies of differentdecision rules.
Applying the proposed formula tothe whole database we obtain 61% and 55% ofclassification quality on train and test data.
Weshould also mention that the common tf.idf ap-proach gives us on the given data 45% and 38%of accuracy rate on the train and test data.
Theproposed approach performs significantly betteron this kind of data.Using the agglomerative hierarchical clusteringwe achieve about 9% improvement.
The bestclassification quality is obtained with 35 sub-classes on the train data (68.7%) and 45 sub-classes on the test data (63.9%).
Clustering into35 subclasses gives 63.7% of accuracy rate onthe test data.6 ConclusionThis paper reported on call classification experi-ments on large corpora using a new term rele-vance estimation approach.
We propose to splitthe classification task into two steps: 1) cluster-ing of the ?garbage?
class in order to simplify itsdetection; 2) further classification into meaning-ful classes and the set of ?garbage?
subclasses.The performance of the proposed algorithm iscompared to several standard classification algo-rithms on the database without the ?garbage?class and found to outperform them with the ac-curacy rate of 85.55%.Dividing the ?garbage?
class into the set of sub-classes by genetic algorithm and vector quantiza-tion network we obtain about 5% improvementof accuracy rate and by agglomerative hierar-chical clustering we achieve about 9% improve-ment of accuracy rate on the whole database.0,60,650,70,750,80,850,90 20 50 100 150 200 300Train set accuracy Test set accuracy00,20,40,60,8RC RC max C C withlimitRTrain Set Accuracy Test Set Accuracy347ReferencesA.
Albalate, D. Suendermann, R. Pieraccini, and W.Minker.
2009.
Mathematical Analysis of Evolution,Information, and Complexity, Wiley, Hoboken,USA.A.
Albalate, D. Suendermann D., and W. Minker.2011.
International Journal on Artificial Intelli-gence Tools, 20(5).A.
Albalate, A. Suchindranath, D. Suendermann, andW.
Minker.
2010.
Proc.
of the Interspeech 2010,11th Annual Conference of the InternationalSpeech Communication Association, Makuhari, Ja-pan.A.
Albalate, S. Rhinow, and D. Suendermann.
2010.Proc.
of the ICAART 2010, 2nd International Con-ference on Agents and Artificial Intelligence, Va-lencia, Spain.A.L.
Gorin, G. Riccardi, and J. H. Wright.
1997.Speech Commun., vol.
23, pp.
113?127.B.
Carpenter and J. Chu-Carroll.
1998.
Proc.
ICSLP-98, pp.
2059?2062.C.-H. Lee, B. Carpenter, W. Chou, J. Chu-Carroll, W.Reichl, A. Saad, and Q. Zhou.
2000.
SpeechCommun., vol.
31, no.
4, pp.
309?320.D.
Suendermann, J. Liscombe, K. Dayanidhi, and R.Pieraccini.
2009.
Proc.
of the SIGDIAL 2009, Lon-don, UK.H.
Ishibuchi, T. Nakashima, and T. Murata.
1999.Trans.
on Systems, Man, and Cybernetics, vol.
29,pp.
601-618.H.-K. Kuo and C.-H. Lee.
2000.
Proc.
of ICSLP?00.J.
Chu-Carroll and B. Carpenter.
1999.
Comput.
Lin-guist., vol.
25, no.
3, pp.
361- 388.J.
Ward.
1963.
Journal of the American StatisticalAssociation, 58 (301): 236-244.J.
H. Wright, A. L. Gorin, and G. Riccardi.
1997.Proc.
Eurospeech-97, pp.
1419?1422.K.
Evanini, D. Suendermann, and R. Pieraccini.
2007.Proc.
of the ASRU 2007, Kyoto, Japan.R.
E. Schapire and Y.
Singer.
2000.
Mach.
Learn.,vol.
39, no.
2/3, pp.
135?168.348
