?
1" )" A Met;hod for Ac(:elerati lg CFG-l msing/)y \[Js\]ng \])ependency\]nformationHideo \?atmJalmIBM lh'scarch, '.l.
'okyo lh~'sc'arch Lal)oral:ory\]623-1d Shimotsuruma, Ymnato, \](anagawa 242-8502, Jalmnwatanabt~((0trl.ilml.
(:o.j i)Abst rac t'\].
'hi.q lmlmr d(,scrib(;s an algorithnl for accc'lerat-ing l;h(; CF(~'qm.rsing t)ro(:t;ss by using (lel)(;nd(;ncy(or modifier-nlodifie(; relationship) infornmtion givenby, for insi,an(:e, d('.llcnd('alcy cstimal,ion l)rogramsSll(:h as sl;o(:\]ulsl;i(: 1)arsers~ llSCl';,q Jltdic;d;ion in aninl;(,ra(;tiv(', al)t)li(:al;il)n ,mM \]inguisl;ic mmotal;i(nm;t(hh:(1 in a sour(:(' l;(.'xl;.
This is a ml;l;hod for ('.n-\]mn(:ing exi,%ing grmnmard/as(',d CF(\]-l)arsing sys-Wan by using dc'tmnden(;y informal;ion._1.
Introduct ionTh(' parsing sysl;O.lil is ;i, key co111t)o11(111|; \]'or 11at-tual language, ai)i/lications uch as machine trans-lal;ion, informal;ion rel;rJ(wal, l;cxt ,'-;unllnariz;ll;ion,and its l)(;rfornlml(:(; (\])roct;ssintt; speed and act:u-racy) is very inq)orl;ant o l;h(!
success of l;lms(' ap-pli,::ations.Tim umm\] CF(Ltmrsin/~ algorithlns \[3, 6\] k(,.
(!p allinterm('.dJat(; l)ossibiliti(~s which may or may not t)ctls(xl ill tim tinal pm:se r(;,qults.
Tlmr(~for(!, we usu-ally reduce 1;hi;s(; illl;(.
'l:lllcdial;o, l)ossibiliti('s whichare unlikely to t)(; used as tinal results in the nlid-die ()f l;he process l)y using s(;vt;ral l)rmling t('x:h-ni(luCS.
One good information ,sour(:('~ for pruningis d(;t)end(;n('y information |)c.tw('.cn words.
It hasnol; l)(;(m so easy l;o gel; such d('l)('n(h'.ncy informa-tion until a. few years a.go, but, th(; sil;uat;ion hasret:(ml;ly chang(,,d.Recent intensive studies on statistical alll)roach\[7, 1, 2\] a(lvanccxl statistical parsing systems, andwc can gel, relatively correct dct)en(h'ncy informa-tion using these systems, leurthc'r, if we SUl)t)oscan interactive NLP system, then there aa(, sore(,types of user intera(:tions which can b(; consideredto determilm 1;11(; modifice c;mdidatc.
I11 addition,recent studies on the linguistic infi)rnmtion mmo-(;a.l;ion \[10, 4, 12, 1.3\] provid(; tools l/y which a usercan (;asily annotate  l inguist ic  intbrnmtion (si /ecialXML markup tags) into source texts ,  and we canOX\[)(X;I; |;0 ,qtX) a,ll increase of tho 11111111)(11 of l;exi;swil;h linguistic information.
This linguistic infor-nlai;ion usually includes dependtmc,y infornml;ion.For instmmc, the following example shows m~ &llllO-(;al;ion ('xaml/h' by Linguistic Annol;ation lmnguag(;described in \[12, 13\], and the id and rood atl;l'i})ll|;c,qinside tal:w (,hmmifl;s pc.ci\[\[y word dependencies.IIe (lal:w id=" 1" )saw(/lal:w) a man (lal:w,nod="1"  )wi th( / la l :w)  a tc'l(',scolm.in this (;xanll)h', the word "with" modifi(;s l;he wordAs shown in l;hc, above (~xample,% we can nowget depc.ndcncy inlbrnmtion more easily than a ti;wyears ago.
This paper describes an algorithni foraccelcrnting CFG-lmrsing systems by using su(:hd(;pcnd(;ncy (or modifier-moditi(;e r(~lationship) in-formation.
Th(; prot)oscd algorithm does not as-sume all words are given dctmndency int'ormation~ratht;r it works in case such that some of words arepartia.lly given dep(;ndt',ncy infl)rnm.tion.2 Ol)t imiz ing A lgor i thm Using De-pendency  InfornmtionWe use a. nornml CFG lmrsing sysi;('m with one'(;xl;(;nsion that for (m.t'h ru\]c (here, must he.
o11c righl;-h;md sid(' (or \]{ITS) t('rm I mark(,d as a h(,ad, andth('.
informati(m (if a head term is trmlsJhrr('.
(l Lo l;hc.lc.ft-hmM side (or H IS)  tenn.
In this lmtmr, a CIeGrule is (hmol;ed as follows:{x -~ ~q ... ~ .
.
.
.
~.
;,} (,,, > 0)In tim above, notation, X is dm left-lmnd side(or LHS) term, mM I5- are right-hand side (or l lH$)terms, mM a RHS term followed by an asterisk '*' isa head term.
The l;ypical usage of the head is thatthe LHS t(;nn shares many features of the headterm in the RHS.
For instmme, a matching word ofthe the LHS tcnn becomes the same as the one ofthe head term in the RHS.For each rule, an arc is constructed over a wordsegment in a.n input sentence.
An aa'c is d('alot(,dusing terms of its base rule as follows:I x -~ ~q ... E - .
~1+,* ... 5, \ ]1A term expresses a non-terminal symbol in IAIS, an(1 ~'~non-terminld or a terminal symbol in l/.IIS.913The LHS term of an arc nmans the LHS termof the base rule of the arc, and RHS terms of anarc means RHS terms of the base rule of the arc.In the above notation, a single dot indicates thai;RHS terms located to the left of a dot are inactive,that is, they already match the LHS term of someother arcs.
Three dots are used to ret)resent zero orany number of terms.
An arc whose RHS terms areall inactive is called an inactive arc, otherwise it iscalled an active arc.
An arc covers a segment ofinput words; the start point of an arc is the indexof the first word in the covering segment, and theend point of an arc is 1 plus the index of the lastword in the covering segment.Basically, a standard CFG parsing algorithm suchas \[3, 6\] consists of the following three operations.Initialization: For each word, arcs are generatedfroln rules such that the leftmost RHS termmatches it.Operation A: For each inactive arc A, an arc isgenerated fl'om A and a rule R such that theleftmost RHS term of R ,natchcs the LHSterm of A.Operation B: For each inactive arc A, an arc isgenerated from A and another active arc Bsuch that the leftinost active RHS term of Bmatches the LHS term of A and the end t)ointof B is the stone as tile start point of A.We assume that some dependency information1)etween words are given, and such det)endency in-formation is denoted as follows:w.~w,The first of the above examples represents thata word I/V u modifies another word I~(~, attd W~, pre-cedes 14~j, while the second one represents that aword Rq, modifies another word H~j and W,, pre-cedes 1/1~/.Given this kind of dependency information, thefollowing conditions are imposed on Operation Aand Operation B.Condit ions for Operation A:Condition A1 (when the leftmost RHS term ofa rule is a head term):Given an inactive arc Arc1 denoted by\[A ~ ...\] and a rule which has two ormore RHS terms and the leftmost RHSterm is a head denoted by {X -+ A *B ...}, Operation A is executed only ifthere is dependency information 144,lYb where 1,14~ is a word matching theLHS term A of Arci and lVb is a wordlocated anywhere to the right of the endI)oint of Arc1.
{X->A*  B ...}Wa WD- .
.
.
.
.
.
.
.
.
.
.
.
.
?
- - "Figure 1: Condition A1Figure 1 shows the above condition.
In this fig-ure, a thick arc ret)resents an inactive arc, a linerepresents a matching to be tried in this ot)eration,a dotted line represents a matching betweeu a termin an arc and a word, and a dotted arrow representsdependency infbrmation.
In this case, this type ofrule implies that a word matching the LHS termof the arc to be matched with the leftmost erm ofthe rule must be modified by any word which is lo-cated after the end t)oint of the arc, since the headterm is the left;most erm of the rule.
Therefore, ifthe A1 condition does not hold, Operation A is notrequired to be executed.Condition A2 (when the leftmost RHS term ofa rule is not a head term):Given an inactive arc Arc1 denoted by\[A --+ ...\] and a rule which has two ormore RHS terins and the leftmost I{HSterm is not a head denoted by {X --+ A... D* ...}, Operation A is executedonly if there is a dependency informa-tion 14~ ~ 1?~ where 1/1~ is a wordInatching the LHS term A of Arc1 andWv is a word located anywhere after theend point of Arc1.Figure 2 shows the above condition.
In this case,this type of rule ilnplies that a word matching tileLHS term of the arc to be matched with the left-most term of the rule Inust inodify any word whichis located after the end point of the arc, since thehead terin is not tile leftmost erm of the rule.Condit ions for Operation B:Condition B1 (when the leftmost active RHSterm of an active arc is the head term):Given an active arc Area denoted by\[X --+ Ao ... A~ .
B .
.
.
.
\ ]  and anin-active arc Arc1 denoted by \[B -+ ...\]914{X-> A ... D*  .
.
.
}Wa WbFigure 2: Condition A2such that the end point of Area is thesanto as the start point of Arc r, Ol)era-tion B is executed only if, for each l,lz,~(0 < i < n) which is a word match-ing the RItS term A~ of AreA, thereis dependency information I'Vai => I'Vb,where Wi, is a word matching the LHSterm B of Arc,.ix->.,: : -, V / \Wao ... Wa,, Wb" " - -  .
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.
.-"l?igure 3: Condition B1Figure 3 shows the above condition.
Ill this fig-ure, ~ dotted thick arc represellts an active m:c. hithis case, this type of active arc implies that wordsmatching inactive terms before' the head term ofthe active art: must modify a. word matdfing tileLHS term of the inactive arc.Condition B2 (when the head term is on the leftside of the leftinost active RHS term of an activearc):Given an active mc AreA denoted by\[X ~ ... A* ... B ...\] and all in-active arc Arc1 denoted by \[B --+ ...\]such that the end point of Area is thesame as the start point of Arc1, Oper-ation B is executed only if there is de-t)endency information W,, ~ Wb where144~ is a word matching the RHS termA of AreA, and Wv is a word matchingthe LItS terln B of Arc1.. .
.
.
.IX ->,  "l"IWa WbFigure 4: Condition B2Figure 4 shows tile above condition.
I n  this case,this type of active arc implies that a word lnatchingthe LIIS term of the inactive arc nmst modi~ aword matching the head tcrin of the active arc.Condition B3 (when the head term is on theright side of the leftlnost active RItS term of anactive arc):Given an actiw; arc Area denoted by\[X ~ A .
B ... C* ...\] and an inactivearc Arc1 denoted by \[/3 -+ ...\] such thatthe end point of Area is the same as thestart point of Arc,, Operation B is exe-cuted only if there is dependency infofnmtion Wb ~ 14~ where Wf, is a wordmatching the.
LItS term B of Arcl, andl'14: is a word on the right side of the,end point of Arci.Wb ,.
I," WcFigure 5: Condition B3Figure 5 shows the above condition.
In this case,this type of active arc implies that a word matchingthe.
LHS term of tile inactive arc must modify aword after the end point of tile inactive arc.The dependency information is not necessarilygiven to all words.
If there is any source word ex-cept for the root word of a sentence such that there915is no del)endency information originating fl:om it:then a set of such del)endeney inibrmation is calledpartial, otherwise, it is called total.
If the given de-1)endency informatioll is partial, the A\] conditioncan not be used, since, even if there is no det)en-dency information targeting I.V,,, we eanllot know ifsuch del)endency information does not really (,xist,or if such delmndency inlbrmation is llot Sul)plied.For other conditions, we check them only when allsource words for dependency checking have depen-dency information.
On the other hand, if the givendependency information ix total, all conditions arechecked.3 ExperimentWe have imt)lemented the 1)reposed algorithminto an existing English CFG-parser we have devel-oped for a machine translation t)roduct \[8, 9, 11\] e, and conducted an experinmnt to know the effec-tiveness of this algorithm.We selected 280 test sentences rmxdomly froma sentence set created by .\]EIDA :~ for ewfluatingtranslation systen L and made the correct dei)en--(lency relation data for these selected test sentences.We collected the number of inactive arcs, the num-b(;r of active arcs, and the t)rocessing time for casessuch that C modifiee candidates (one of which isthe correct modifiee) are given to a word.
4 If C:=Ithen it; corresponds to the best case for a parsersuch that only one correct modifiee is given fin' eachword, while if C is 3 or 4 then it; corresponds to theapproximation of using a statistical modifiee esti-ination program for getting candidate modifiees.The graphs in Figure 6 indicate the reductionratios of active arcs, inactive arcs, and 1)recessingtime for using conditions for total dependency in-formation and conditions tbr partial del)endeney in-formation.
The de'nominators for calculating theser&tios are the numbers of ar(:s and the processingtime (seconds) in case of the parser without this al-gorithm.
In these graphs, C=X indicates that X isthe maxilnunl nulnber of moditlee candidates givento a word.From these gratlhs, we can so(; that the morewords in a sentence, the better the 1)erformance.In a real domain, most sentences consist of morethan ten words.
Therefore, looking at values foraround 10 in the X axis, we can see that inactivearcs are reduced by about 40% and 25%, active arcs2This parser is used in a Web page translation softwarecalled "lnternet King of 3t'anslation" released from IBM.laI)an.a.lal)all Electronic Industry \])evcloi)ment Association4Modifiee candidates are selected randomly except forthe correct oi1o.are reduced by about 65% and 35%, and t)rocessingtime is reduced by about ~15% and 15%, for theideal case (C-1)  and more practical cases (C=3or 4), respectively, in the (:as('.
of total del)endencyinformation.
Please note that, since the 1)arser inwhich this algorithm ix impleumnted has alreadyseveral pruning mechanisms, we can expect morereduction (or pertbrmance gain) for generic CFGpars(',rs.4 DiscussionAs a study for accelerating the parsing tu'ocessusing dependency information, Imaichi\[5\] reportedan algoritlnn for Japanese language.
The condi-tions introduced by hnaichi are described by usingthe notation in this paper as ~bllows:Condition MI:Given an active arc Area denoted by\[X -~ A .
13.\] and an inactive arc Arc1denoted by \[B -+  ...\] such that the endpoint of Area is the same as the startpoint of Arc j, Operation B is executedonly if there is dependency infl)rmat.ionI<1~, -=> lYb where 1'15~ is a word matchingthe RIIS term A of AreA, and lVt, isa word mat(:hing the I~HS term ,r3 ofmrcl.Con(lition M2:Given an inactive ar(: Arc1 denoted 1)y\[A -+ ...\] and a rule denoted by {X --~A ...}, Operatioil A is execllted onlyif there is no det)endency iifl'ormationWt.
=> l'l/-,~ where 1.
'I~ ix a word match-ing the LHS term A of Arc1 and lYt.
isa word loca.ted before the start point ofA~rc l  .The condition M1 correspouds to B 1.
Since hnaiehi'salgorithln considers only .Japmmse in which all wordsother than the last; word modifies one of the suc-ceeding words, it does not deal with cases usuallyseen in Eurot)eall languages where a word moditiesone of the preceding words.
Therefore, it is notapplicable to any language other than Jat)anese ingeneral.
Fnrthcr, since a CFG rule is restricted tobe in Chomsky normal form, hnaichi's algorithm islimited in terms of at)plicability.Since the algorithm proposed in this pal)er doesnot have any restrictions on the dependency direc-tion and the CFG rule format, it can be applicableto any CFG-parsers ill any languages.916Reductiof Ratio o\[ Inactive Arcsfor Total Dependency Inf0.6085o'~\[ 40OqJ1004 5 6 -!
8 9 10 11 12NUt'rl 01" WOIL~;O0Reduction I~atio of Active Arcsfor loLal Dependency Info.8070~ 60 i i'~0J ,50ft-=40 o~: 20104 5 fi 7 8 ?
10 11 1\[J\]J G=1I~ 0=2\[ \ ]  O~-3I~ 0:-4Nl.lm of 'lil'~l'Orl\[}~O0\[\]\]J O=l.~ 0--2\ [ \ ]  0=3\[ \ ]  0=460Reduction Ratio of limefor Total Dependency Info.~.5O'~ 40 ~0~30,\]3~clO4 5 6 7 8 g 10 11 12Nurl'l of "~l'l,l'Orlj4,.
1:\ [ \ ]  0=1\ [ \ ]  O--2\ [ \ ]  0--3\ [ \ ]  O=4Reduction Raito of Inactive .Arcs\[or Partial Dependency In\[o.60~5o0-P 400~20E 104 5 6 1 8 9 10 11NLIFr'I Elf 'l,ll,?Oi"l.~-:(t,)t21 O=2O=3F_1 0=412Reduction Ratio ot Active Arcsfor-Partial Dependency Info5O~40oo"~ 20"U j4 5 6 7 8 g 10 11NU~'\[\] ,:If ),ll,tClrl~:~:(d)~ O=lO=2O=4125040C IoailOReduction Ratio of l imefor Partial Dependency Info.4 5 6 7 8 9 10 11 12Num ,:,f W0rd~.
(,;) (f)~\] 0=1U O=2 ~ G=36=4Figure.
6: l{educl;ion ratios o\[ inaci.ive arcs, acl~ive arcs, a.ml processing Lime9175 Conc lus ionWe developed an algorithm for accelerating theperformance of the CFG t)arsing process if we aregiven dependency information.
From an experi-ment, we can show the effectiveness of this algo-rithm.By using this algorithm, we can enhance xist-ing grammar-based parsers using dependency in-formation given by stochastic parsers, interactivesystems, and texts created by linguistic annotationsystems.References\[1\] M. Collins.
A new statistical parser based onbigram lexical dependencies.
In Proc.
of 3~fl~AC?, pages 184-191, 1996.\[2\] M. Collins.
Three generative, lexicalized modelsfbr statistical parsing.
In P~vc.
of 35th A CL,pages 16-23, 1997.\[3\] J. Earley.
An efficient context-free parsing al-gorithm.
In Readings in Natural Language Pro-cessin9.
Morgan Kauflnan, 1969.\[4\] K. Hashida, K. Nagao, et al, Progress andProspect of Global Document Annotation.
(inJapanese) In P~vc.
of ~th Annual Meeting ofthe Association of Natural Language Process-ing, pp.
618-621, 1998.\[5\] O. Imaichi, Y. Matsumoto, and M. Fujio.
Anintegrated parsing method using stochastic in-formation and grammatical constraints.
Jour-nal of Natural Language Prvcessing, 5(3):67-83,1998.\[6\] M. Kay.
Algorithm schemata nd data struc-ture in syntactic processing.
Technical Report;CSL-80-12, Xerox PARC, 1980.\[7\] D. M. Magerman.
Statistical decision-tree mod-els fi)r parsing.
In Prvc.
of 33rd A CL, pages276-283, 1995.\[8\] K. Takeda.
Pattern-based context-free gram-inars for machine translation.
In Proc.
of 3~thACL, pages 144-151, 1996.\[9\] K. Takeda.
Pattern-based machine translation.In Proc.
of 16th Coling, volume 2, pages 11551158, 1996.\[10\] Text Encoding Initiative(http://www.uic.edu:80/orgs/tei/)\[11\] H. Watanabe and K. Takeda.
A pattern-based machine translation system extended byexample-based processing.
In Proc.
of 17th Col-ing (Coling-ACL'98), volume 2, pages 1369-1373, 1998.\[12\] H. Watanabe, Linguistic Annotation Lan-guage - The Markup Language for Assist-ing NLP Programs -.
IBM Research ReportI/T0334, 1999.\[13\] H. Watanabe, K. Nagao, et al, Linguistic An-notation System for Improving the Performanceof Natural Language Processing Programs.
InProc.
of 6th Annual Meeting of The Associationfor NLP (in Japanese), pp.
171-174, 2000.918
