Coling 2010: Poster Volume, pages 801?809,Beijing, August 2010Dimensionality Reduction for Text using Domain KnowledgeYi Mao and Krishnakumar Balasubramanian and Guy LebanonGeorgia Institute of TechnologyAbstractText documents are complex high dimen-sional objects.
To effectively visualizesuch data it is important to reduce its di-mensionality and visualize the low dimen-sional embedding as a 2-D or 3-D scatterplot.
In this paper we explore dimension-ality reduction methods that draw upondomain knowledge in order to achieve abetter low dimensional embedding and vi-sualization of documents.
We considerthe use of geometries specified manuallyby an expert, geometries derived automat-ically from corpus statistics, and geome-tries computed from linguistic resources.1 IntroductionVisual document analysis systems such as IN-SPIRE have demonstrated their applicability inmanaging large text corpora, identifying topicswithin a document and quickly identifying a setof relevant documents by visual exploration.
Thesuccess of such systems depends on several fac-tors with the most important one being the qual-ity of the dimensionality reduction.
This is ob-vious as visual exploration can be made possibleonly when the dimensionality reduction preservesthe structure of the original space, i.e., documentsthat convey similar topics are mapped to nearbyregions in the low dimensional 2D or 3D space.Standard dimensionality reduction methodssuch as principal component analysis (PCA), lo-cally linear embedding (LLE) (Roweis and Saul,2000), or t-distributed stochastic neighbor embed-ding (t-SNE) (van der Maaten and Hinton, 2008)take as input a set of feature vectors such as bagof words.
An obvious drawback is that such meth-ods ignore the textual nature of documents and in-stead consider the vocabulary words v1, .
.
.
, vn asabstract orthogonal dimensions.In this paper we introduce a framework for in-corporating domain knowledge into dimensional-ity reduction for text documents.
Our techniquedoes not require any labeled data, therefore iscompletely unsupervised.
In addition, it appliesto a wide variety of domain knowledge.We focus on the following type of non-Euclidean geometry where the distance betweendocument x and y is defined asdT (x, y) =?(x?
y)>T (x?
y).
(1)Here T ?
Rn?n is a symmetric positive semidef-inite matrix, and we assume that documents x, yare represented as term-frequency (tf) columnvectors.
Since T can always be written as H>Hfor some matrix H ?
Rn?n, an equivalent butsometimes more intuitive interpretation of (1) isto compose the mapping x 7?
Hx with the Eu-clidean geometrydT (x, y) = dI(Hx,Hy) = ?Hx?Hy?2.
(2)We can view T as encoding the semantic similar-ity between pairs of words and H as smoothingthe tf vector by mapping observed words to re-lated but unobserved words.
Therefore, the geom-etry realized by (1) or (2) may be used to derivenovel dimensionality reduction methods that arecustomized to text in general and to specific textdomains in particular.
The main challenge is toobtain the matrices H or T that describe the rela-tionship among vocabulary words appropriately.We consider three general ways of obtainingH or T using domain knowledge.
The first cor-responds to manually specifying H or T basedon the semantic relationship among words (de-termined by domain expert).
The second corre-sponds to constructing H or T by analyzing re-lationships between different words using corpusstatistics.
The third is based on knowledge ob-tained from linguistic resources.
Whether to spec-ify H directly or indirectly by specifying T =801H>H depends on the knowledge type and is dis-cussed in detail in Section 4.We investigate the performance of the proposeddimensionality reduction methods for three textdomains: sentiment visualization for movie re-views, topic visualization for newsgroup discus-sion articles, and visual exploration of ACL pa-pers.
In each of these domains we evaluate thedimensionality reduction using several differentquantitative measures.
All the techniques men-tioned in this paper are unsupervised, making useof labels only for evaluation purposes.Our take home message is that all three ap-proaches mentioned above improves dimension-ality reduction for text upon standard embedding(H = I).
Furthermore, geometries obtainedfrom corpus statistics are superior to manuallyconstructed geometries and to geometries derivedfrom standard linguistic resources such as Word-Net.
Combining heterogenous types of knowl-edge provides the best results.2 Related WorkDespite having a long history, dimensionality re-duction is still an active research area.
Broadlyspeaking, dimensionality reduction methods maybe classified as projective or manifold based(Burges, 2009).
The first projects data onto alinear subspace (e.g., PCA and canonical corre-lation analysis) while the second traces a low di-mensional nonlinear manifold on which data lies(e.g., multidimensional scaling, isomap, Lapla-cian eigenmaps, LLE and t-SNE).
The use of di-mensionality reduction for text documents is sur-veyed by Thomas and Cook (2005) who also de-scribe current homeland security applications.Dimensionality reduction is closely related tometric learning.
Xing et al (2003) is one of theearliest papers that focus on learning metrics ofthe form (1).
In particular they try to learn ma-trix T in an supervised way by expressing rela-tionships between pairs of samples.
A representa-tive paper on unsupervised metric learning for textdocuments is Lebanon (2006) which learns a met-ric on the simplex based on the geometric volumeof the data.We focus in this paper on visualizing a cor-pus of text documents using a 2-D scatter plot.While this is perhaps the most popular and prac-tical text visualization technique, other methodssuch as Spoerri (1993), Hearst (1997), Havre etal.
(2002), Paley (2002), Blei et al (2003), Maoet al (2007) exist.
Techniques developed in thispaper may be ported to enhance these alternativevisualization methods as well.3 Non-Euclidean GeometriesDimensionality reduction methods often assume,either explicitly or implicitly, Euclidean geome-try.
For example, PCA minimizes the reconstruc-tion error for a family of Euclidean projections.LLE uses the Euclidean geometry as a local met-ric.
t-SNE is based on a neighborhood structure,determined again by the Euclidean geometry.
Thegeneric nature of the Euclidean geometry makesit somewhat unsuitable for visualizing text docu-ments as the relationship between words conflictswith Euclidean orthogonality.
We consider in thispaper several alternative geometries of the form(1) or (2) which are more suited for text and com-pare their effectiveness in visualizing documents.As mentioned in Section 1, H smooths the tfvector x by mapping the observed words into ob-served and non-observed (but related) words.
Incase H is nonnegative, it can be further decom-posed into a product of a non-negative columnnormalized matrix R ?
Rn?n and a non-negativediagonal matrix D ?
Rn?n.
The decompositionH = RD shows that H has two key roles.
Itsmooths related vocabulary words (realized by R)and it emphasizes some words over others (real-ized by D).
Setting Rij to a high value if wi, wjare similar and 0 if they are unrelated maps anobserved word to a probability vector over re-lated words in the vocabulary.
The value Dii cap-tures the importance of vi and therefore should behigher for important content words than for lessimportant words or stop-words1.It is instructive to examine the matrices R andD in the case where the vocabulary words clus-ter in some meaningful way.
Figure 1 givesan example where vocabulary words form twoclusters.
The matrix R may become block-diagonal with non-zero elements occupying di-agonal blocks representing within-cluster word1The nonnegativity assumption of H is useful when con-structing H by domain experts such as the method A in Sec-tion 4.
In general, H needs not to be nonnegative for dimen-sionality reduction as in (2).802?????
?0.8 0.1 0.1 0 00.1 0.8 0.1 0 00.1 0.1 0.8 0 00 0 0 0.9 0.10 0 0 0.1 0.9???????????
?5 0 0 0 00 5 0 0 00 0 5 0 00 0 0 3 00 0 0 0 3?????
?Figure 1: An example of a decomposition H = RD inthe case of two word clusters {v1, v2, v3}, {v4, v5}.
Theblock diagonal elements in R represent the fact that wordsare mostly mapped to themselves, but sometimes are mappedto other words in the same cluster.
The diagonal matrix indi-cates that the first cluster is more important than the secondcluster for the purposes of dimensionality reduction.blending, i.e., words within each cluster are in-terchangeable to some degree.
The diagonal ma-trix D represents the importance of different clus-ters.
The word clusters are formed with respectto the visualization task at hand.
For example,in the case of visualizing the sentiment contentof reviews we may have word clusters labeled as?positive sentiment words?, ?negative sentimentwords?
and ?objective words?.In general, the matrices R,D may be definedbased on the language or may be specific to docu-ment domain and visualization purpose.
It is rea-sonable to expect that the words emphasized forvisualizing topics in news stories might be dif-ferent than the words emphasized for visualizingwriting styles or sentiment content.Applying the geometry (1) or (2) to dimen-sionality reduction is easily accomplished by firstmapping document tf vectors x 7?
Hx and pro-ceeding with standard dimensionality reductiontechniques such as PCA or t-SNE.
The resultingdimensionality reduction is Euclidean in the trans-formed space but non-Euclidean in the originalspace.
In many cases, the vocabulary containstens of thousands of words or more making thespecification of T or H a complicated and errorprone task.
We describe in the next section severaltechniques for specifying these matrices in prac-tice.4 Domain KnowledgeMethod A: Manual SpecificationIn this method, a domain expert manually spec-ifies H = RD by specifying (R,D) based onthe perceived relationship among the vocabularywords.
More specifically, the user first constructsa hierarchical word clustering that may depend onthe current text domain, and then specifies the ma-trices (R,D) based on the clustering.Denoting the clusters byC1, .
.
.
, Cr (a partitionof {v1, .
.
.
, vn}), R is set toRij ?
{?a, i = j, vi ?
Ca?ab, i 6= j, vi ?
Ca, vj ?
Cb.The values ?ab, a 6= b capture the semantic simi-larity between two clusters and the value ?aa cap-tures the similarity of two different words withinthe cluster a.
These values may be set manu-ally by domain expert or automatically computedbased on the clustering hierarchy (for example ?abcan be the inverse of the minimal number of treeedges traversed in moving from a to b).
To main-tain a probabilistic interpretation, the matrix Rshould be normalized so that its columns sum to1.
The diagonal matrix D is specified by settingthe valuesDii = da, vi ?
Caaccording to the importance of word cluster Ca tothe current visualization task.We emphasize that as with the rest of the meth-ods in this paper, the manual specification is donewithout access to labeled data.
Since manual clus-tering assumes some form of human intervention,it is reasonable to also consider cases where theuser specifiesH or T in an interactive manner.
Forexample, the expert specifies an initial clusteringof words and values for (R,D), views the result-ing embeddings and adjusts the selection interac-tively until reaching a satisfactory embedding.Method B: Contextual DiffusionAn alternative to manually specifying T =DR>RD is to construct it based on similarity be-tween the contextual distributions of the vocabu-lary words.
The contextual distribution of word vis defined asqv(w) = p(w appears in x|v appears in x) (3)where x is a randomly drawn document.
In otherwords qv is the distribution governing the wordsappearing in the context of word v.803A natural similarity measure between distribu-tions is the Fisher diffusion kernel proposed byLafferty and Lebanon (2005).
Applied to contex-tual distributions as in Dillon et al (2007) we ar-rive at the following similarity matrixT (u, v) = exp(?c arccos2(?w?qu(w)qv(w))).where c > 0.
Intuitively, the word u will be dif-fused into v depending on the geometric diffusionbetween the distributions of likely contexts.We use the following formula to estimate thecontextual distribution from a corpusqv(w) =?x?p(w, x?|v) =?x?p(w|x?, v)p(x?|v)=?x?tf(w, x?)
tf(v, x?)?x??
tf(v, x??
)(4)=( 1?x?
tf(v, x?
))(?x?tf(w, x?
)tf(v, x?
))where tf(w, x) is the number of times word w ap-pears in document x divided by the length of thedocument x.
The contextual distribution qv or dif-fusion matrix T above may be computed in an un-supervised manner without labels.Method C: Web n-GramsIn method B the contextual distribution is com-puted using a large external corpus that is similarto the text being analyzed.
An alternative that isespecially useful when such a corpus is not eas-ily available is to use generic resources to esti-mate the contextual distribution (3)-(4).
One op-tion is to use the publicly available Google n-gramdataset (Brants and Franz, 2006) to estimate T .More specifically, we compute the contextual dis-tribution by considering the proportion of timestwo words appear together within the n-gramse.g., for n = 2 we haveqv(w) =# of bigrams containing both w and v# of bigrams containing v .Method D: Word-NetIn the last method, we consider using Word-Net,a standard linguistic resource, to specify T .
ThisVocabularySportsOthersCanoeingcatchboxinginningssoccerTeamNamePlacesEU AsiaMid eastUSArizonafranciscocarolinaatlantaaustinOthersFigure 2: Manually specified hierarchical word clusteringfor the 20 newsgroup domain.
The words in the frames areexamples of words belonging to several bottom level clusters.is similar to manual specification (method A) inthat it builds upon experts?
knowledge rather thancorpus statistics.
In contrast to method A, how-ever, Word-Net is a carefully built resource con-taining more accurate and comprehensive linguis-tic information such as synonyms, hyponyms andholonyms.
On the other hand, its generality putsit at a disadvantage as method A may be adaptedto a specific text domain.We follow Budanitsky and Hirst (2001) whocompared five similarity measures between wordsbased on Word-Net.
In our experiments we usethe measure of Jiang and Conrath (1997) (see alsoJurafsky and Martin (2008))T (u, v) = log p(u)p(v)2p(lcs(u, v))as it was shown to outperform the others.
Above,lcs stands for the lowest common subsumer, i.e.,the lowest node in the hierarchy that subsumes (isa hypernym of) both u and v. The quantity p(u)is the probability that a randomly selected wordin a corpus is an instance of the synonym set thatcontains word u.Combination of MethodsIn addition to individual methods we also considertheir convex combinationsH?
=?i?iHi s.t.
?i ?
0,?i?i = 1 (5)where Hi are matrices from methods A-D (ob-tained implicitly by specifying R and D formethod A and T for methods B-D).
Doing so al-lows us to combine heterogeneous types of do-main knowledge including experts?
knowledge804and corpus statistics, leverage their diverse natureand potentially achieve better performance thanany of the methods on its own.5 ExperimentsWe evaluate the proposed methods by experiment-ing on two text datasets where domain knowledgeis relatively easy to obtain (especially for methodA and B).
Preprocessing includes lower-casing,stop words removal, stemming, and selecting themost frequent 2000 words for both datasets.The first is the Cornell sentiment scale datasetof movie reviews from 4 critics (Pang and Lee,2004).
The visualization in this case focuses onthe sentiment quantity of either 1 (very bad) or 4(very good) (Pang et al, 2002).
For method A,we use the General Inquirer resource2 to partitionthe vocabulary into three clusters conveying pos-itive, negative or neutral sentiment.
While visu-alizing documents from one particular author, therest of the reviews from other three authors can beused as an estimate of contextual distribution formethod B.The second text dataset is the 20 newsgroups.It consists of newsgroup articles from 20 distinctnewsgroups and is meant to demonstrate topic vi-sualization.
In this case one of the authors de-signed a hierarchical clustering of the vocabularywords based on general knowledge of English lan-guage (see Figure 2 for a partial clustering hier-archy) without access to labels.
The contextualdistribution for method B is estimated from theReuters RCV1 dataset (Lewis et al, 2004) whichconsists of news articles from Reuters.com in theyear 1996 and 1997.Method C uses Google n-gram which providesa massive scale resource for estimating the con-textual distribution.
In the case of Word-Net(method D) we used Pedersen?s implementationof Jiang and Conrath?s similarity measure3.
Note,for these two methods, the obtained informationis not domain specific but rather represents gen-eral semantic relationships between words.In our experiments belowwe focused on two di-mensionality reduction methods: PCA and t-SNE.PCA is a well known classical method while t-SNE (van der Maaten and Hinton, 2008) is a re-2http://www.wjh.harvard.edu/?inquirer/3http://wn-similarity.sourceforge.net/cent dimensionality reduction technique for visu-alization purposes.
The use of t-SNE is motivatedby the fact that it was shown to outperform LLE,CCA, MVU, Isomap, and Laplacian eigenmapswhen the dimensionality of the data is reduced totwo or three.To measure the dimensionality reduction qual-ity, we visualize the data as a scatter plot with dif-ferent data groups (topics, sentiments) displayedwith different markers and colors.
Our quantita-tive evaluation of the visualization is based on thefact that documents belonging to different groups(topics, sentiments) should be spatially separatedin the 2-D space.
Specifically, we used the follow-ing indices:(i) The weighted intra-inter criteria is a standardclustering quality index that is invariant tonon-singular linear transformations of theembedded data.
It equals tr(S?1T SW ) whereSW is the within-cluster scatter matrix, ST =SW + SB is the total scatter matrix, and SBis the between-cluster scatter matrix (Duda etal., 2001).
(ii) The Davies Bouldin index is an alternativeto (i) that is similarly based on the ratioof within-cluster scatter to between-clusterscatter (Davies and Bouldin, 2000).
(iii) Classification error rate of a k-NN classifierthat applies to data groups in the 2-D em-bedded space.
Despite the fact that we arenot interested in classification per se (other-wise we would classify in the original highdimensional space), it is an intuitive and in-terpretable measure of cluster separation.
(iv) An alternative to (iii) is to project the em-bedded data onto a line which is the direc-tion returned by applying Fisher?s linear dis-criminant analysis to the embedded data.
Theprojected data from each group is fitted to aGaussian whose separation is used as a proxyfor visualization quality.
In particular, wesummarize the separation of the two Gaus-sians by measuring the overlap area.
While(iii) corresponds to the performance of a k-NN classifier, method (iv) corresponds to theperformance of Fisher?s LDA classifier.Labeled data is not used during the dimensionalityreduction stage but it is used in each of the abovemeasures for evaluation purposes.805Figure 3 displays both qualitative and quanti-tative evaluation of PCA and t-SNE for the senti-ment and newsgroup domains forH = I (left col-umn), manual specification (middle column) andcontextual distribution (right column).
In generalfor both domains, methods A and B perform bet-ter both qualitatively and quantitatively (indicat-ing by the numbers in the top two rows) than theoriginal dimensionality reduction with method Boutperforming method A.Tables 1-2 compare evaluation measures (i)and (iii) for different types of domain knowl-edge.
Table 1 corresponds to the sentiment do-main where we conducted separate experimentsfor four movie critics.
Table 2 corresponds tothe newsgroup domain where two tasks wereconsidered.
The first involves three newsgroups(comp.sys.mac.hardware vs. rec.sports.hockeyvs.
talk.politics.mideast) and the second involvesfour newsgroups (rec.autos vs. rec.motorcyclesvs.
rec.sports.baseball vs. rec.sports.hockey).
It isclear from these two tables that the contextual dif-fusion, Google n-gram, and Word-Net generallyoutperform the original H = I matrix.
The bestmethod varies from task to task but the contextualdiffusion and Google n-gram in general result ingood performance.PCA (1) PCA (2) t-SNE (1) t-SNE (2)H = I 1.5391 1.4085 1.1649 1.1206B 1.2570 1.3036 1.2182 1.2331C 1.2023 1.3407 0.7844 1.0723D 1.4475 1.3352 1.1762 1.1362PCA (1) PCA (2) t-SNE (1) t-SNE (2)H = I 0.8461 0.5630 0.9056 0.7281B 0.7381 0.6815 0.9110 0.6724C 0.8420 0.5898 0.9323 0.7359D 0.8532 0.5868 0.9013 0.7728Table 2: Quantitative evaluation of dimensionality reduc-tion for visualization for two tasks in the news article domain.The numbers in the top five rows correspond to measure (i)(lower is better), and the numbers in the bottom five rowscorrespond to measure (iii) (k = 5) (higher is better).
Weconclude that contextual diffusion (B), Google n-gram (C),and Word-Net (D) tend to outperform the original H = I .We also examined convex combinations?1HA + ?2HB + ?3HC + ?4HD (6)with?
?i = 1 and ?i ?
0.
Table 3 displaysquantitative results using evaluation measures (i),(ii) and (iii) where k is chosen to be 5 for (iii).The first four rows correspond to method A, B, C(?1, ?2, ?3, ?4) (i) (ii) (iii) (k=5)(1,0,0,0) 0.5756 -3.9334 0.7666(0,1,0,0) 0.5645 -4.6966 0.7765(0,0,1,0) 0.5155 -5.0154 0.8146(0,0,0,1) 0.6035 -3.1154 0.8245(0.3,0.4,0.1,0.2) 0.4735 -5.1154 0.8976Table 3: Three evaluation measures (i), (ii), and (iii) (seethe beginning of the section for description) for convex com-binations (6) using different values of ?.
The first four rowsrepresent methods A, B, C, and D. The bottom row repre-sents a convex combination whose coefficients were obtainedby searching for the minimizer of measure (ii).
Interestinglythe minimizer also performs well on measure (i) and moreimpressively on the labeled measure (iii).and D and the bottom row corresponds to a convexcombination found which minimizes the unsuper-vised evaluation measure (ii) (i.e.
the search forthe optimal combination is based on (ii) that doesnot require labeled data).
Note that the convexcombination also outperforms method A, B, C,and D for measure (i) and more impressively formeasure (iii) which is a supervised measure thatuses labeled data.
In general, by combining het-erogeneous types of domain knowledge, we mayfurther improve the quality of dimensionality re-duction for visualization, and the search for sucha combination may be accomplished without theuse of labeled data.Finally, we demonstrate the effect of domainknowledge on a new dataset that consists of alloral papers appearing in ACL 2001 ?
2009.
Forthe purpose of manual specification, we obtain1545 unique words from paper titles, and as-sign for each word relatedness scores for thefollowing clusters: morphology/phonology, syn-tax/parsing, semantics, discourse/dialogue, gen-eration/summarization, machine translation, re-trieval/categorization and machine learning.
Thescore takes value from 0 to 2, where 2 representsthe most relevant.
The score information is thenused to generate the transformation matrix R. Wealso assign for each word an importance valueranging from 0 to 3 (larger the value, more impor-tant the word).
This information is used to gener-ate the diagonal matrix D.Figure 4 shows the projection of all 2009 pa-pers using t-SNE (papers from 2001 to 2008 areused to estimate contextual diffusion).
Using Eu-clidean geometry H = I (Figure 4 left) results ina Gaussian like distribution which does not pro-vide much insight into the data.
Using a manually806(a) 0.3284 (b) 0.1794 (c) 0.1385(d) 0.3008 (e) 0.2295 (f) 0.1093Figure 3: Qualitative evaluation of dimensionality reduction for the sentiment domain (top two rows) and the newsgroupdomain (bottom two rows).
The first and the third rows display PCA reduction while the second and the fourth display t-SNE.The left column correspond to no domain knowledge (H = I) reverting PCA and t-SNE to their original form.
The middlecolumn corresponds to manual specification (method A).
The right column corresponds to contextual diffusion (method B).Different groups (sentiment labels or newsgroup labels) are marked with different colors and marks.In the sentiment case (top two rows) the graphs were rotated such that the direction returned by applying Fisher lineardiscriminant onto the projected 2D coordinates aligns with the positive x-axis.
The bell curves are Gaussian distributionsfitted from the x-coordinates of the projected data points (after rotation).
The numbers displayed in each sub-figure arecomputed from measure (iv).807Dennis Schwartz James Berardinelli Scott Renshaw Steve RhodesPCA t-SNE PCA t-SNE PCA t-SNE PCA t-SNEH = I 1.8625 1.8781 1.4704 1.5909 1.8047 1.9453 1.8013 1.8415A 1.8474 1.7909 1.3292 1.4406 1.6520 1.8166 1.4844 1.6610B 1.4254 1.5809 1.3140 1.3276 1.5133 1.6097 1.5053 1.6145C 1.6868 1.7766 1.3813 1.4371 1.7200 1.8605 1.7750 1.7979H = I 0.6404 0.7465 0.8481 0.8496 0.6559 0.6821 0.6680 0.7410A 0.6011 0.7779 0.9224 0.8966 0.7424 0.7411 0.8350 0.8513B 0.8831 0.8554 0.9188 0.9377 0.8215 0.8332 0.8124 0.8324C 0.7238 0.7981 0.8871 0.9093 0.6897 0.7151 0.6724 0.7726Table 1: Quantitative evaluation of dimensionality reduction for visualization in the sentiment domain.
Each of the fourcolumns corresponds to a different movie critic from the Cornell dataset (see text).
The top five rows correspond to measure(i) (lower is better) and the bottom five rows correspond to measure (iii) (k = 5, higher is better).
Results were averaged over40 cross validation iterations.
We conclude that all methods outperform the original H = I with the contextual diffusion andmanual specification generally outperforming the others.47081464912851032077504892931375610510442116981496959364419299751097936625512282 7110166679030104371211155395931848073108197210778971199453658311610291241122832262510063886475868118110638  54334057766184511710643114113601527347488698942171158211123120355247081464912851032077504892931375610510442116981496959364419299751097936625512287271101666790301043712111553959318480731081972107789711994536583116102912411228322625100638864758681181106385433405776611845117106431141136015273474886989421711582111231203552470814649128510320775048929313756105 104421169814969593 64419299751097936625512287271101 66679030104371211155395931848073108197210778971199453658311610291241 22832262510063884758 681181106385433405776611845117106431141130  1527347488698942171158211123120552Figure 4: Qualitative evaluation of dimensionality reduction for the ACL dataset using t-SNE.
Left: no domain knowledge(H = I); Middle: manual specification (method A); Right: contextual diffusion (method B).
Each document is labeled by itsassigned id from ACL anthology.
See text for more details.specified H (Figure 4 left) we get two clear clus-ters, the smaller containing papers dealing withmachine translation and multilingual tasks.
Inter-estingly, the contextual diffusion results in a one-dimensional manifold.
Investigating the papersalong the curve (from bottom to top) we find thatit starts with papers discussing semantics and dis-course (south), continues to structured predictionand segmentation (east), continues to parsing andmachine learning (north), and then moves to senti-ment prediction, summarization and IR (west) be-fore returning to the center.
Another interestinginsight that we can derive is the relative disconti-nuity between the bottom part (semantics and dis-course) and the rest of the curve.
It seems spatialseparability is higher in that area than in the otherareas where the curve nicely traverses different re-gions continuously.6 DiscussionIn this paper we introduce several ways of incor-porating domain knowledge into dimensionalityreduction for visualizing text documents.
The pro-posed methods all outperform in general the base-line H = I , which is the one currently used inmost text visualization systems.The answer to the question of which method isbest depends on both the domain and the task athand.
For small tasks with limited vocabulary,manual specification could achieve best results.A large vocabulary size makes manual specifica-tion less accurate and effective.
In cases wherewe have access to a large external corpus that issimilar to the one we are interested in visualizing,contextual diffusion is an excellent choice.
Lack-ing such a domain specific dataset estimating thecontextual distribution using the generic Googlen-gram is a good substitute.
Word-Net capturesrelationships (such as synonyms and hyponyms)other than occurrence statistics between vocabu-lary words, and could be useful for certain tasks.Finally, the effectiveness of dimensionality reduc-tion methods can be increased further by carefullycombining different types of domain knowledgeranging from semantic similarity to occurrencestatistics.808ReferencesBlei, D., A. Ng, , and M. Jordan.
2003.
Latent dirich-let alocation.
Journal of Machine Learning Re-search, 3:993?1022.Brants, T. and A. Franz.
2006.
Web 1T 5-gram Ver-sion 1.Budanitsky, A. and G. Hirst.
2001.
Semantic distancein wordnet: An experimental, application-orientedevaluation of five measures.
In NAACL Workshopon WordNet and other Lexical Resources.Burges, C. 2009.
Dimension reduction: A guidedtour.
Technical Report MSR-TR-2009-2013, Mi-crosoft Research.Davies, D. L. and D. W. Bouldin.
2000.
A clusterseparation measure.
IEEE Transactions on PatternAnalysis and Machine Intelligence, 1(4):224?227.Dillon, J., Y. Mao, G. Lebanon, and J. Zhang.
2007.Statistical translation, heat kernels, and expecteddistances.
In Uncertainty in Artificial Intelligence,pages 93?100.
AUAI Press.Duda, R. O., P. E. Hart, and D. G. Stork.
2001.
Patternclassification.
Wiley New York.Havre, S., E. Hetzler, P. Whitney, and L. Nowell.
2002.Themeriver: Visualizing thematic changes in largedocument collections.
IEEE Transactions on Visu-alization and Computer Graphics, 8(1).Hearst, M. A.
1997.
Texttiling: Segmenting text intomulti-paragraph subtopic passages.
ComputationalLinguistics, 23(1):33?64.Jiang, J. J. and D. W. Conrath.
1997.
Semantic sim-ilarity based on corpus statistics and lexical tax-onomy.
In International Conference Research onComputational Linguistics (ROCLING X).Jurafsky, D. and J. H. Martin.
2008.
Speech and Lan-guage Processing.
Prentice Hall.Lafferty, J. and G. Lebanon.
2005.
Diffusion kernelson statistical manifolds.
Journal of Machine Learn-ing Research, 6:129?163.Lebanon, G. 2006.
Metric learning for text docu-ments.
IEEE Transactions on Pattern Analysis andMachine Intelligence, 28(4):497?508.Lewis, D., Y. Yang, T. Rose, and F. Li.
2004.
RCV1:A new benchmark collection for text categorizationresearch.
Journal of Machine Learning Research,5:361?397.Mao, Y., J. Dillon, and G. Lebanon.
2007.
Sequen-tial document visualization.
IEEE Transactions onVisualization and Computer Graphics, 13(6):1208?1215.Paley, W. B.
2002.
TextArc: Showing word frequencyand distribution in text.
In IEEE Symposium on In-formation Visualization Poster Compendium.Pang, B. and L. Lee.
2004.
A sentimental eduction:sentiment analysis using subjectivity summarizationbased on minimum cuts.
In Proc.
of the Associationof Computational Linguistics.Pang, B., L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learn-ing techniques.
In Proc.
of the Conference on Em-pirical Methods in Natural Language Processing.Roweis, S. and L. Saul.
2000.
Nonlinear dimensional-ity reduction by locally linear embedding.
Science,290:2323?2326.Spoerri, A.
1993.
InfoCrystal: A visual tool for infor-mation retrieval.
In Proc.
of IEEE Visualization.Thomas, J. J. and K. A. Cook, editors.
2005.
Illu-minating the Path: The Research and DevelopmentAgenda for Visual Analytics.
IEEE Computer Soci-ety.van der Maaten, L. and G. Hinton.
2008.
Visualiz-ing data using t-sne.
Journal of Machine LearningResearch, 9:2579?2605.Xing, E., A. Ng, M. Jordan, and S. Russel.
2003.
Dis-tance metric learning with applications to clusteringwith side information.
In Advances in Neural Infor-mation Processing Systems, 15.809
