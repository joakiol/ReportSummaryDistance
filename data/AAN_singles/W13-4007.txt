Proceedings of the SIGDIAL 2013 Conference, pages 51?60,Metz, France, 22-24 August 2013. c?2013 Association for Computational LinguisticsAutomatic Prediction of Friendship via Multi-model Dyadic FeaturesZhou Yu, David Gerritsen, Amy Ogan, Alan W Black, Justine CassellSchool of Computer Science, Carnegie Mellon University{zhouyu, dgerrits, aeo, awb, justine }@cs.cmu.eduAbstractIn this paper we focus on modelingfriendships between humans as a way ofworking towards technology that can initiateand sustain a lifelong relationship with users.We do this by predicting friendship status in adyad using a set of automatically harvestedverbal and nonverbal features from videos ofthe interaction of students in a peer tutoringstudy.
We propose a new computationalmodel used to model friendship status in ourdata, based on a group sparse model (GSM)with L2,1 norm which is designed toaccommodate the sparse and noisy propertiesof the multi-channel features.
Our GSM modelachieved the best overall performancecompared to a non-sparse linear model (NLM)and a regular sparse linear model (SLM), aswell as outperforming human raters.
Dyadicfeatures, such as number and length ofconversational turns and mutual gaze, inaddition to low level features such as F0 andgaze at task, were found to be good predictorsof friendship status.1 Introduction and Related WorkWhile significant advances have been made indetecting the speech and nonverbal social signalsemitted by individuals (see Vinciarelli, Pantic &Bourlard, 2009, for a review), and research hasaddressed the social roles and states ofindividuals in groups (see Gatica-Perez, 2009,for a review), considerably less computationalwork has focused on the automatic detection ofspeech or nonverbal correlates of specificallydyadic states, such as rapport.
And yet rapporthas been shown to have important effects oninteractions as diverse as survey interviewing(Berg, 1989), sales (Brooks, 1989), and health(Harrigan et al 1985).
If we are to buildinteractive systems that are successful, then, webelieve that the ability to build rapport with ahuman user will be essential.Rapport can be instantaneous and can alsobuild over time.
Granovetter (1973) describes thestrength of an interpersonal ?tie?
as a function ofthe time, emotional intensity, and reciprocity thataccumulates between people.
These ties mediateeffects in myriad domains such as learning(Azmitia & Montgomery, 1993) and healthcare(Harrigan & Rosenthal, 1983).Accordingly, analysis of initial exchanges andthose after many years of interaction suggeststhat the behavioral signals that indicate rapportchange over time.
For example, in Tickle-Degnen and Rosenthal?s highly cited model(1990), rapport consists of mutual attention,positivity, and coordination.
High levels ofpositivity between conversational partners arecommon in the initial phases of a relationship,but positivity has been shown to decline, withouta loss in rapport, as the number of interactionsincreases.
In fact, Ogan et al(2012) gaveevidence that the use of playful rudenessbetween friends during peer tutoring correlates togreater learning.
This leads to an associatedchallenge of spoken dialogue systemdevelopment: creating systems that can developsocial ties, and increase rapport with the userover repeated interactions to maximize beneficialoutcomes.While little work has addressed automaticdetection, some prior work has addressed theproblem of emitting signals to build rapport indialogue and agent systems (Stronks et al 2002;Bickmore & Picard, 2005; Gratch et al 2006;Cassell et al 2007; Bickmore et al 2011), andwe turn to this research for what cues might beimportant in rapport.
The majority of this priorwork, however, has addressed harmony ?
orinstant rapport ?
rather than rapport over time.For those systems that have addressed friendshipor the growth of rapport, most commonly thenumber of interactions has been used as a meterof relationship progression, instigating changesin the dialogue system as the social odometerscrolls onward (Cassell & Bickmore, 2003;Vardoulakis et al 2012).
Counting the times adyad has interacted is a crude approximation of arelationship state, however; being able to detectthe behavioral signals that people actually use toindicate relationship status would be superior.In our own prior work (Cassell et al2007) welooked at particular hand-annotated nonverbalsignals (such as nodding and mutual gaze) asoperationalizations of rapport, and found thatfriends and non-friends indeed show differingdistributions of each signal as a function ofrelationship state.
In the current study, we moveto the next step and automatically harvest a set ofmultimodal dyadic and time contingent featuresto identify those features that play a significantrole in predicting friendship state.
A major51challenge for predicting relational states such asthese is to construct a compact feature space thatcaptures only reliable rapport signals and alsogeneralizes across different users.
To providestrength to our model (as well as to fit themultimodal nature of embodied conversationalagents), we look at both acoustic and visualfeatures.
Such an approach takes advantage ofthe fact that multimodal aspects ofcommunication are not redundant, but oftencomplementary (Cassell, 2000).However, dyadic behaviors such asconversational turns, mutual/non-mutual smile,mutual/non-mutual gaze, and mutual/non-mutuallean forward provide an additional challenge inmodeling; no matter how important, they appearrelatively rarely in conversational data.
Thusstandard non-sparse linear models, normallytrained on high frequency factors, might assigntoo much weight to low frequency (i.e., sparse)features.
In order to address issues of this sortYuan and Lin (2007) introduced the grouplasso.
To address the sparse nature of ourfeatures in real-world data and the noise thatoccurs from different production sources, wepropose an extension to this genre of techniquein the form of a Group Sparse Model (GSM)which enforces sparsity with a L2,1 norm insteadof the group lasso penalty (Chen, et  al., 2011),due to the relatively efficient optimizationprocess of L2,1 norms (Liu, et al 2009).
Unlikea straightforward sparse linear model (SLM)(Yang et al 2010), which treats each featureindependently, GSMs group features which sharethe same production source in the optimizationprocess.
In the GSM linear model, the removal ofthe assumption of independence betweenfeatures means that the penalty is on group ratherthan individual features.
Thus the model hasgeneral robustness to noise, since groupingfeatures from the same production source canincrease the overall confidence of the featuregroup.Our contributions in this work, then, are three-fold: we (1) designed and implemented a methodfor automatic dyadic feature extraction which isbased on low level features, and which yieldsstrong predictive power of friendship status, (2)propose a new Group Sparse Model (GSM) withL2,1 norm, that deals with the noisy and sparsenature of the feature sets, and (3) illuminate,from this model, the nature of verbal andnonverbal behavior between friends and non-friends in a peer tutoring setting.The remainder of the paper is organized asfollows.
We first describe the data set andintroduce the features used in our experiments.We then describe the performance of the threecomputational models we evaluated.
Finally, wediscuss the contributions of different features tofriendship prediction and provide an erroranalysis of our proposed model.2 The Data SetFigure 1: Camera View 1 and Camera View 2We collected data from dyads of studentsengaged in a reciprocal peer tutoring task.
Wechose peer tutoring as it is a domain in whichfriendship has been shown to have a positiveeffect on student learning (see e.g.
Ogan et al2012).
In addition, tutoring systems that rely ondialogue are common, and peer tutoring dialoguesystems are increasingly common.
Thus, beingable to assess friendship state in this domain is auseful step on the path to creating a peer tutoringagent that can use rapport to increase learninggains.Each dyad consisted of two American Englishspeakers with a mean age of 13.3 years (range =12 ?
15).
We collected data from 12 dyads, ofwhich 6 dyads were already friends.
Dyads wereeither both girls or both boys, and each conditioncontained 3 boy dyads and 3 girl dyads.Each dyad came to the lab for 3 sessions, withan average interval between visits of 4.6 days(SD = 3.1), totaling 36 sessions across all dyads.Each session consisted of about 90 minutes ofinteraction recorded from three camera views (afrontal view of each participant and a side viewof the two participants).
With close talkmicrophones, we also recorded the participants?speech in separate audio channels for the purposeof automatic dyadic acoustic feature extraction.The setting is shown in Figure 1.Each session began with a short period of timefor participants to become acquainted.
After that,using a standard reciprocal tutoring procedure(see Fantuzzo et al 1989), participants tutoredeach other on procedural and conceptual aspectsof an algebra topic in which both participantswere relatively novice.
Order of seating andassignment of tutoring roles (tutor or tutee) wasdetermined in the first session by alphabeticalorder of participant name.
Tutoring rolesalternated from that point on, such that bothparticipants had the opportunity to take on therole of ?expert?
during each session.
After aperiod of individual study time to familiarize52themselves with the material, the first tutoringperiod began and lasted approximately 25minutes.
This was followed by a 5 minute break,after which students?
tutoring roles were reversedfor a second tutoring period of 25 minutes.Finally, each student answered a survey aboutthe interaction.The current study examines only the tutoringsections of each session, which were divided into30-second clips or ?thin slices?
(Ambady et al2006).
In total, the data points used for modelingcomprise 2259 clips from the 12 dyads.3 Multimodal InformationIn our analyses, low-level audio and visualfeatures were automatically extracted using threeoff-the-shelf toolkits.
Dyadic features, which area second order derivative of the low levelfeatures, and which capture the interaction of twoparticipants, are also automatically produced.Taken together, analysis of these features allowsus to determine if the verbal and nonverbalbehaviors of the participants index theirfriendship status in any significant way.3.1 Low Level Audio Features (LA)Type # of FeaturesProsodic FeaturesF0 72Energy 38Duration 154Voice Quality FeaturesJitter 68Shimmer 34Voicing 38Spectral FeaturesMFCC 570Total 974Table 1: Acoustic Feature GroupsFor acoustic feature extraction, a large set ofacoustic low-level descriptors (LLD) andderivatives of LLDs combined with appropriatestatistical functionals, i.e., maxPos (the absoluteposition of the maximum value in frames),minPos (the absolute position of the minimumvalue in frames), amean (The arithmetic mean ofthe contour), etc., were extracted for each of thesplit channel recordings.
The ?INTERSPEECH2010 Paralinguistic Challenge Feature Set?
in theopenSMILE toolkit (Schuller et al 2012) wasused as our basic acoustic feature set.
Forspectral features, Mel Spectrum and LSP wereexcluded due to the possible overlap withMFCC.
The set contained 974 features whichresulted from a base of 32 low-level descriptors(LLD) with 32 corresponding delta coefficients,and 21 functionals applied to each of these 68LLD contours.
In addition, 19 functionals wereapplied to the 4 pitch-based LLD and their fourdelta coefficient contours.
Finally the number ofpitch onsets (pseudo syllables) and the totalduration of the input were included.
Thedimension of each feature group is shown inTable 1.3.2 Low Level Vision Features (LV)Type # of FeaturesFace Position Feature 1038 Face Interest Points 114Gaze Features 3Face Direction  Features 4Mouth and Eye Openness 6Smile Intensity 1Discretized Smile 1Total 139Table 2: Vision Feature GroupsSince participants were facing the cameradirectly most of the time, as seen in Fig 1,current technology for facial tracking canefficiently be applied to our dataset.
OMRON?sOKAO Vision System was used in facedetection, facial feature extraction, and basic facerelated features extrapolation.
For each frame,the vision software returns a smile intensity (0-100) and the gaze direction, using bothhorizontal and vertical angles expressed indegrees.
Apart from gaze direction, the softwarealso provides information about head orientation:horizontal, vertical, and roll (in or out).
38additional face interest points, position andconfidence, were also extracted.
These werenormalized to pixel coordinates, which turnedout to lead to quite noisy data, and hence todiminished utility of these 38 points (in thefuture we will consider normalizing to facecoordinates).
We also calculated the openness ofthe left eye, right eye, mouth, and the location ofthe face.
Details are shown in Table 2.
Similar toour audio feature extraction method, one staticfeature vector per 30 second video clip wasproduced.
All the features were computed at thesame rate as the original videos: 30 Hz.Altogether, 139 dimensions were extracted ineach frame from each camera view.3.3 Dyadic Features (DF)All of the features discussed above are low-levelacoustic and visual features, extracted with53respect to individual participants.
Whileindividual behavior may index friendship state,we posit that patterns of interaction will be moreeffective.
For example, prior research (Baker etal., 2008) suggests that the number and length ofconversational turns (Cassell et al 2007),presence of mutual smiles and non-mutual smiles(Prepin et al 2012), mutual gaze and non-mutual gaze (Nakano et al 2010), as well asposture shifting (Cassell, et al 2001; Tickle-Degnen & Rosenthal, 1990), are importantfeatures to investigate in dyadic data.
Whileother features such as gestures and mutual pitchshift may also play a role in indexing relationshipstate, these are not yet a part of the dyadicfeatures we address here.3.3.1 Number and Average Length ofConversational TurnsWe recorded individual audio channels for eachparticipant, which makes the automaticextraction of conversational turns possible.
First,we extracted intervals of silence with toolboxSoX which produced speech chunks, and thenidentified the speaker by comparing the speechenergy (loudness) in each audio channel, asspeech from each speaker is carried by theother?s microphone.
After that we combined thespeech chunks and speaker ID to approximateconversational turns.
The approximation qualityis not perfect, given the variability of the audiorecording, but noise can be mediated duringmodel building.3.3.2 Mutual Smile and Non Mutual SmilePrepin et al(2012) describe the role of mutualsmiles (smiles that occur during the same timeperiod) in ?stance alignment?
and make the pointthat interactional alignment of this behaviorreflects synchronization of internal states.
Suchsynchrony predicts mutual understanding andincreased quality of interaction, and as such is afundamental quality in the formation ofadolescent friendships (Youniss, 1982).
Cappella& Pelachaud (2002) likewise describe?mutuality?
as the precondition for how smilesfunction in contingent ways in a dyad.
Smiles areclearly therefore important to assess in data suchas ours.
We defined a maximum window of 500milliseconds between the end of one participant?ssmile and the beginning of the next for smiles tobe considered mutual.3.3.3 Mutual Gaze and Non-mutual GazeNakano & Ishii (2010) describe eye gaze as aclue to engagement, and integrate mutual gazeinto their conversational agents.
There is nofeature for direct gaze at partner provided in theOKAO vision toolkit.
Mutual gaze was thereforeapproximated by annotating a gaze ?in front,?achieved by combining the information fromthree directions of gaze: vertical, horizontal, anddepth.
Gaze ?in front?, or at the partner, wasrecorded only if the participant gaze had lessthan a 15 degree angle from straight forward inall of these three directions.
A maximum windowof 500 milliseconds for gaze to be consideredmutual was also employed here.3.3.4 Mutual Lean Forward and Non-MutualLean ForwardForward leaning has been shown to be asignificant predictor of the ability to establishrapport in a dyad (Harrigan et al 1985).
In fact,friends who lean in are seen as more sociallycompetent, while strangers are seen as lesssocially competent when they lean in (Burgoon& Hale, 1988).
For our study, lean forward wasapproximated by detecting the smooth trend offace enlargement within the video frame.
Inorder to improve precision of the feature, thesegments with high confidence in face detectionwere processed.
Furthermore, posture shifting,i.e., forward leaning, is not as quickly executedas changes in gaze or smile.
We therefore used a1 second sample window for lean forward, ratherthan a 500 millisecond window.3.3.5 Mutual Gaze followed by Mutual SmileMutual gaze followed by mutual smile is alsoapproximated using a similar approach as above.It is a relatively dense feature compared to all theother possible combinations of nonverbalbehaviors, thus it is the only combination that isincluded in the feature set in this paper.
Thewindow within which mutual gaze is consideredto be followed by mutual smile is set to be within2 seconds.4 Computational ModelWe formulate friendship prediction as a set ofbinary classifications.
In order to have the leastvariance and make sure no participant appearedin both the training and testing set, a leave-one-out cross-validation setting was adopted in all ofour experiments.
Each session had approximately180 30-second video clips, totaling 2259 datapoints.
Z-score normalization by dyad was usedto scale all the features into the same range.Early fusion, which is simple concatenation offeature vectors, was adopted throughout ourexperiments to combine different features.
Weevaluated our group sparse model (GSM), alongwith a non-sparse linear model (NLM) andsparse linear model (SLM).544.1 Non-sparse Linear Model (NLM)We began with a standard non-sparse linearmodel (NLM), which is a Support VectorMachine (SVM) (Cortes & Vapnik, 1995) with alinear kernel.
The libsvm (Fan et al 2008)package was used in our experiment, and theparameter, the slack value of SVM that controlsthe scale of the soft margin, was obtained bycross validation.4.2 Sparse Linear Model (SLM)In order to prevent over-fitting on rare dyadicfeatures, a sparse sensitive model SLM wasintroduced.
As well as preventing over-fitting,through weight shrinkage the sparse model canalso exclude redundant features.
In ourexperiment, an L2,1 norm sparse model withlinear kernel (Yang et al 2012) was selected asour baseline sparse model.4.3 Group Sparse Model (GSM)Based on the SLM, we propose a group-sparsemodel (GSM) with the novel use of an L2,1norm.
Instead of assuming every feature isuncorrelated to other features, the GSM groupssome of the features together and utilizes theircorrelated information to mediate the noise of thedata.
For an arbitrary matrix        , itsis defined as?
?
?Suppose that we have n training data indicatedby            and sampled from c classes.
Inour setting, c = 2, friends or non-friends.
{   }          is the corresponding label.The total scatter matrix    and between classscatter matrix    are defined as follows.?
?where ?
is the mean of all samples,    is themean of samples in the i-th class.
is thenumber of samples in the i-th class,.G is the scaled label matrix.
A well-knownmethod to utilize discriminate information is tofind a low dimensional subspace in which     ismaximized while    is minimized (Fukunaga etal., 1990).
So the object function could be easilywritten as follows(() )The optimization of the above object functionwas introduced in Yang et al(2012).
It is anadaptation of iterative singular valuedecomposition.
In GSM, a block-wise constraintis imposed on the diagonal matrix (D) which isthe intermediate result of the iterative singlevalue decomposition.
()W in the equation is the weight function,    isthe ith feature group in W, and there are a totalnumber of G sub diagonal matricescorresponding to G groups of features.For acoustic features, Steidl et al (2012)designed a grouping schema which consists ofProsodic Features, Voice Quality Features andSpectral features which we adopted.
For visualfeatures, based on our observation of the highlyunstable performance of the 38 feature points ofthe face, we introduced group bondage for theentire group to prevent single face features over-fitting the classifier.
Detailed group informationis shown in Table 1 and Table 2.5 Human BaselineFigure 2: Boxplot of human rating accuracy withrespect to gender.In order to establish a baseline of the difficultyof predicting friendship, we conducted anexperiment with humans, rating whether twopeople in a video were friends or not, afterwatching a 30-second video/audio clip takenfrom the first session of tutoring (in which thebehaviors of strangers are most likely to bedistinct from friends).
We recruited 14 peopleand screened out participants with priortheoretical knowledge of nonverbal behavior,gesture, friendship, and rapport, or who rated all12 clips in under 8 minutes, leaving 10participants, half male, with an average age of 23(SD 4.8).
Each participant was asked to watchone 30-second clip per dyad, taken from 3minutes after tutoring began.
The mean accuracyof their friendship prediction was 0.717 (SD0.119), which is significantly lower than our bestGSM model (trained on all three sessions)applied to those same 12 clips, with a55performance of 0.837 (t(11) = -2.1381 p.<.05).When we split the ratings by gender, we foundfemales on average were more accurate thanmales (see Figure 2).
According to Hall et al(1979) females are generally better decoders ofnonverbal behaviors, which may lead to betterjudgment of friendship.6 Results: ModelsHuman NLM SLM GSMLV  0.743 0.768 0.792*LA  0.674 0.664 0.682*LV+DF  0.752 0.769 0.801*LA+DF  0.679 0.681 0.683LV+LA  0.744 0.780 0.803*LV+LA+DF  0.717 0.749 0.782 0.814#Table 3: The classification accuracy of the threealgorithms on different features sets.
Feature setswere combined with early fusion (+).
Values marked* are significantly better (p<.05, pairwise t-test) thanother results in the same row.
Values marked # aresignificantly better (p<.001, pairwise t-test) than otherresults in the same column.Our group sparse model (GSM) along with thenon-sparse linear model (NLM) and sparse linearmodel (SLM) were evaluated on differentcombinations of three sets of features: low-levelvision features (LV), low-level audio features(LA) and dyadic features (DF), and theirperformance is presented in Table 3.
We did notevaluate dyadic features (DF) alone due to theirsparse nature.In particular, we found that adding theautomatically extracted DF to LV and LA withearly fusion improved the performance (t(2258)=-3.12,p<.001) of the GSM model.
When usingfewer modalities, our newly proposed GSMoutperformed NLM and SLM (t(2258)=-1.65,p<0.05).
However, when the number of featuresets increased, there was no statistical differencein performance between GSM and the other twomodels.
We suspect that when features areabundant, the information that the featuresprovide reaches a ceiling.
The advantage of theGSM was gained by mediating the noise andsparseness of the data, which resulted in betterweight assignment for each feature.
Alternatively,when features are abundant, even NLM can havea comparative weight assignment by performinga greedy high dimensional feature space search.Thus there is limited room for furtherimprovement by better weight assignment amongthe group features which GSM assumes.When we looked at the top features selectedby NLM using the vision modality alone, two(out of 38) face features, which had an unstablenature, appeared high in rank, which suggests thepossibility of NLM over-fitting the noise of thesefeatures.
Surprisingly, when more modalities areadded, NLM stops picking single face features astop informative features.
In GSM, none of the 38face features are listed in the top ranked featuresfor any of the modalities, which demonstrates itsability in noise mediation.In real world applications, data sets whichproduce ideal, abundant, and accurate featuresare rarely encountered.
We often end up withdata that are poor in video quality, e.g.
with nosplit channels for each participant or no frontalface view.
Our newly proposed GSM maytherefore be more robust when features are noisyor certain modalities are not available.7 Results: Contributions of FeaturesFeature Name WeightNumber of Conversational Turns &Average Length of Turns0.041Gaze Down -0.036Mutual Gaze 0.014F0 0.013Non-mutual Gaze -0.013Voicing 0.014MFCC -0.007Non-mutual Smile 0.004Non-mutual Lean Forward 0.004Mutual Gaze followed by MutualSmile0.001Table 4:  The top 10 informative features and theirweights as trained by GSM.
Positive weight isassociated with friends while negative weight isassociated with non-friends.After building the model and ranking thefeatures, we looked into the weights learned foreach feature.
This weight comprises not only themagnitude, which tells us if the feature isimportant, but also the polarity.
A detailed list ofthe most informative features and their weights isshown in Table 4.The strongest feature is number and length ofconversational turns which is grouped in thetable and should be interpreted as meaning thatfriends have more and shorter conversationalturns.
This is consistent with previous researchon direction giving (Cassell et al 2007), andmirrors the fact that friends are more likely tointerrupt one another.We expected that unfamiliar participants,seated about two feet across from one another,would maintain a low level of eye contact(Argyle & Dean, 1965).
In fact we found thatnon-friends tend to gaze down more often.
Inthis context, non-friends spend more time56looking down at their study materials.
In turn,mutual gaze is higher among friends.Among the audio features, F0, which capturespitch related information such as range andmean, has been shown to differ betweenconversational and non-conversational speech(Bolinger, 1986).
Here, friends show that moreconversational style in their speech, despite thetutoring nature of the interaction.In order to further examine the lessons to belearned from this GSM model about verbal andnonverbal behavior in friends and strangers, wealso ran a repeated measures ANOVA, includingboth gender and friendship status as factors.There were no significant effects for gender,however, and so that factor was collapsed forfurther analysis.
The four features describedabove were all significantly different betweenfriends and strangers (although gaze down wassimply a trend, at p<.08).The following features were also important tothe model, but did not show significance in theANOVA, perhaps because of their sparse naturein our data.
MFCC (Mel-Frequency CepstralCoefficients) was associated with strangers andthe similar audio feature of voicing wasassociated with friends.
Both of these featureshave been described as approximating speechstyle ?
voicing, for example, may indicate morebackchannels, such as ?uh huh?
and ?hmm?
(Ward, 2006).In Nakano et al(2003), listener gaze at thespeaker is interpreted as evidence of non-understanding.
We found similar results wherebynon-friends were more likely to engage in non-mutual gaze ?
looking at one another when theother person was not looking back.
Mutual smiledid not distinguish between friends and non-friends, while non-mutual smile, on the otherhand, provided indicative strength, in spite of itssparse nature, for friendship.
This may relate toour prior work (Ogan et al 2012) which foundsignificant teasing and other behavior wherebyfriends appear comfortable enjoying themselvesat the expense of the other.Mutual lean forward lacked predictive powerin our model, while non-mutual lean forwardwas more salient between friends.
We oftenfound, for example, that friends maintained verydifferent postures, with a tutor leaning backmuch of the time, leaning forward only to answera direct question from the tutee.
Non-friends, onthe other hand, tended to remain fixed on thestudy material.
This may have been a display offormality, where a casual attitude would havebeen perceived as either impolite orinappropriate.
In either relationship state, thetutee tended to sit hunched over the worksheet,and since we did not enter tutor state into themodel, this may have washed out some tutor-specific results.For the time contingent feature, mutual gazefollowed by mutual smile is informative andpredictive of friends.8 Error Analysis and DiscussionDyadIDLA+DF LV+DF LA+LV+DF1 0.732 0.809 0.8192* 0.703 0.793 0.8043* 0.574 0.771 0.7784* 0.713 0.708 0.7625 0.653 0.879 0.8806 0.728 0.827 0.8357 0.624 0.873 0.8828* 0.712 0.861 0.8529* 0.698 0.820 0.83010 0.606 0.834 0.85411* 0.700 0.682 0.74312 0.749 0.780 0.785Table 5: The average accuracy of classification ineach dyad using the group sparse model (GSM) withdifferent combination of feature sets.
Dyads markedwith * are friendsWe performed an error analysis to understand thecontexts under which our model failed toaccurately predict friendship states, and here wediscuss the implications of these examples forour work.
Table 5 shows the average accuracy ofeach dyad using audio, visual, and dyadicfeatures to predict friendship.
Dyads 2, 3, 4, 8, 9and 11 are friend dyads, and the rest arestrangers.Dyad 3 (friends) showed very low accuracy inaudio and dyadic features alone, which might beexplained by the fact that in one early session forthis dyad, most of the 30-second clips containvery sparse numbers of low-level audio features(LA).
An examination of the audio recordingreveals that one of the participants was moreaggressive than in the other sessions.
The studenttold his friend, ?Just be quiet?I am trying towork,?
and ?Shh, you don?t understand, so Ibasically have to teach you how to work that, butI'm trying to work.?
At this point in theinteraction, his partner stopped participating inthe task and said virtually nothing for the rest ofthe session.
This lack of speech led to a lowernumber of turns ?
a pattern with a closerresemblance to strangers than friends.It seems that such rude behavior would bemore likely between friends than strangers,meaning that ultimately our model will need to57be sensitive to this kind of variance.
With morepairs of friends, different styles of friendship canbe further distinguished.
However, this specificphenomenon signals that in the future, lexicalinformation which could be obtained byautomatic speech recognition could furtherimprove performance.Dyad 11 also showed low relative accuracy inpredication, particularly when the model usedvision features.
We found that one of theparticipants often tilted her head, which partiallyblocked the frontal camera view of the otherparticipant, thus resulting in less confidence inautomatically extracted visual features.
In thefuture we will set our cameras in a better positionin order to reach higher feature extractionaccuracy.When we combined all our features, theprediction accuracy of Dyad 3 and 11 improved,further demonstrating that multimodalinformation improves friendship modeling.9 Conclusion and Future WorkAs a first step towards predicting the state offriendship between two interlocutors, weanalyzed a set of automatically harvested low-level and dyadic features from dialogues in apeer-tutoring task.
Both low level features anddyadic features were shown to be useful indiscriminating between those who are friendsand those who are not.To perform the analysis, we introduced a newcomputational group sparse model (GSM) inorder to accommodate the sparse and noisyproperties of multi-channel features.
GSMoutperformed a baseline of human raters whomake these types of social judgments ineveryday interactions.
GSM also statisticallyoutperformed a non-sparse linear model (NLM)and a sparse linear model (SLM) when theanalysis used only a single set of low levelfeatures or single set of low level featurescombined with dyadic features.
When allfeatures were used, the distinctions betweenmodels decreased, since in a huge multimodalfeature space, even a na?ve model could greedysearch for a good weight assignment.
Thus ournewly proposed model did not significantlyoutperform the others in this scenario.
And ingeneral, more features produced more accurateprediction.Based on the outcomes of the GSM model, weinvestigated differences between verbal andnonverbal behavior cues as a function ofdifferent friendship states.
While much researchon rapport detection and building in ECAs hasfocused on low level features, we found thatdyadic features provided some of the mostdistinguishing differences between friends andnon-friends.
For example, mutual gaze and non-mutual gaze were both indicative, as friends arecomfortable looking directly at one another whilenon-friends may have used direct gaze only tosignal non-understanding.
This comfort betweenfriends was also notable in other salient dyadicfeatures; i.e., while non-friends often work inconcert looking down at the task, friends wererelaxed such that one partner could lean back,interrupt to take more conversational turns, andsmile at the other without needing to reciprocatethe smile each time.In future work we will look at temporalcontingency more closely, examining whetherparticipants?
actions are contingent on thebehavior of their partner.
We will also examinewhether the behavior of friends and strangerschanges over multiple sessions.
In this context,we include one suggestive graph, which showsthat strangers increase their mutual gaze oversessions but friends decrease it.
We are currentlycollecting further sessions for each dyad so as tobe able to further analyze the nature of theserelationships over time.Figure 3: Weight of the mutual gaze in eachsession, by friendship statusTo date we have found that the inclusion ofautomatically extracted dyadic features can leadto better prediction of friendship state.
Bothverbal and nonverbal behaviors were discoveredthat distinguish between different friendshipstatus and that suggest how to design embodieddialogue systems that intend to spend a lifetimeon the job.AcknowledgementsThanks to Angela Ng, Rachel Marino and MarissaCross for data collection, Giota Stratou for visualfeature extraction, Yi Yang, Louis-Philippe Morency,Shoou-I Yu, William Wang, and Eric Xing forvaluable discussions, and the NSF IIS for generousfunding.ReferencesAmbady, N., Krabbenhoft, M. A.
& Hogan, D.(2006).
The 30-sec sale: Using thin-slice02461 2 3Session NumberMutual gaze per clipstrangersfriends58judgments to evaluate sales effectiveness.Journal of Consumer Psychology, 16(1), 4?13.Argyle, M. & Dean, J.
(1965).
Eye-contact, distanceand affiliation.
Sociometry, 28(3), 289?304.Azmitia, M. & Montgomery, R. (1993).
Friendship,transactive dialogues, and the development ofscientific reasoning.
Social Development, 2(3),202?221.Baker, R. E., Gill, A. J., & Cassell, J.
(2008).
Reactiveredundancy and listener comprehension indirection-giving.
In Proceedings of the 9thSIGdial Workshop on Discourse and Dialogue(pp.
37?45).Brooks, M. (1989).
Instant rapport (p. 205).
NewYork: Warner Books.Berg, B. L. (1989).
Qualitative research methods forthe social sciences.
Boston: Allyn and Bacon.Bernieri, F. J.
(1988).
Coordinated movement andrapport in teacher-student interactions.
Journalof Nonverbal Behavior, 12(2), 120?138.Bickmore, T. W. & Picard, R. W. (2005).
Establishingand maintaining long-term human-computerrelationships.
ACM Transactions on Computer-Human Interaction, 12(2), 293?327.Bickmore, T. W., Pfeifer, L., & Schuman, D. (2011).Relational agents improve engagement andlearning in science museum visitors.
InIntelligent Virtual Agents (pp.
55?67).Reykjavik.Bolinger, D. (1986).
Intonation and its parts: Melodyin spoken English.
Stanford, CA: StanfordUniversity Press.Burgoon, J. K. & Hale, J. L. (1988).
Nonverbalexpectancy violations: Model elaboration andapplication to immediacy behaviors.Communications Monographs, (May 2013), 37?41.Cappella, J. N.  & Pelachaud, C. (2002).
Rules forresponsive robots: Using human interactions tobuild virtual interactions.
In Reis, Firzpatrick, &Vangelisti (Eds.
), Stability and change inrelationships.
New York, NY: CambridgeUniversity Press.Cassell, J.
(2000).
Nudge nudge wink wink: Elementsof face-to-face conversation for embodiedconversational agents.
In J. Cassell, J. Sullivan,S.
Prevost, & E. Churchill (Eds.
), Embodiedconversational agents (pp.
1?27).
MIT Press.Cassell, J., Gill, A. J., & Tepper, P. A.
(2007).Coordination in conversation and rapport.Proceedings of the ACL Workshop on EmbodiedNatural Language, 40 ?50.Cassell, J., Bickmore, T. W., Campbell, L.,Vilhj?lmsson, H. H., & Yan, H. (2001).
Morethan just a pretty face: Conversational protocolsand the affordances of embodiment.
Knowledge-Based Systems, 14(1-2), 55?64.Cassell, J.
& Bickmore, T. W. (2003).
Negotiatedcollusion: Modeling social language and itsrelationship effects in intelligent agents.
UserModeling and User-Adapted Interaction, 13(1),89?132.Chen, X., Lin, Q., Kim, S., Carbonell, J. G., & Xing,E.
P. (2012).
Smoothing proximal gradientmethod for general structured sparse regression.The Annals of Applied Statistics, 6(2), 719?752.Cortes, C. & Vapnik, V. (1995).
Support-vectornetworks.
Machine Learning, 20(3), 273?297.Fan, R., Chang, K., Hsieh, C., Wang, X., & Lin, C.(2008).
LIBLINEAR: A library for large linearclassification.
The Journal of Machine LearningResearch, 9, 1871?1874.Fantuzzo, J., Riggio, R., Connelly, S., & Dimeff, L.(1989).
Effects of reciprocal peer tutoring onacademic achievement and psychologicaladjustment: A component analysis.
Journal ofEducational Psychology, 81(2), 173-177.Fukunaga, K. (1990).
Introduction to StatisticalPattern Recognition, Second Edition (2nd ed., p.592).
San Diego, CA: Academic Press.Gatica-Perez, D. (2009).
Automatic nonverbalanalysis of social interaction in small groups: Areview.
Image and Vision Computing, 27(12),1775?1787.Granovetter, M. S. (1973).
The strength of weak ties.American Journal of Sociology, 78(6), 1360?1380.Gratch, J., Okhmatovskaia, A., Lamothe, F., Marsella,S.,  Morales, M., van der Werf, R. J., &Morency, L.-P. (2006).
Virtual rapport.
InIntelligent Virtual Agents (pp.
14?27).
SpringerBerlin/Heidelberg.Hall, J.
A., DiMatteo, M. R., Rogers, P. L., & Archer,D.
(1979).
Sensitivity to nonverbalcommunication: The PONS test.
Baltimore, MD:Johns Hopkins University Press.Harrigan, J.
A., Oxman, T. E., & Rosenthal, R.(1985).
Rapport expressed through nonverbalbehavior.
Journal of Nonverbal Behavior, 9,95?110.Harrigan, J.
A.
& Rosenthal, R. (1983).
Physicians?head and body positions as determinants ofperceived rapport.
Applied Social Psychology,13(6), 496?509.Liu, J., Ji, S., & Ye, J.
(2009).
Multi-task featurelearning via efficient l2, 1-norm minimization.In Proceedings of the Twenty-Fifth Conferenceon Uncertainty in Artificial Intelligence (pp.339?348).
AUAI Press.Nakano, Y. I., Reinstein, G., Stocky, T., & Cassell, J.(2003).
Towards a model of face-to-facegrounding.
In Proceedings of the 41st AnnualMeeting on Association for ComputationalLinguistics.
ACL?03 (Vol.
1, pp.
553?561).Sapporo: Association for ComputationalLinguistics.Nakano, Y. I.
& Ishii, R. (2010).
Estimating user?sengagement from eye-gaze behaviors in human-agent conversations.
In Proceedings of the 15thinternational conference on Intelligent user59interfaces.
IUI?10 (pp.
139?148).
Hong Kong:ACM Press.Ogan, A., Finkelstein, S., Walker, E., Carlson, R., &Cassell, J.
(2012).
Rudeness and rapport: Insultsand learning gains in peer tutoring.
InProceedings of the 11 International Conferenceon Intelligence Tutoring Systems (ITS 2012).Prepin, K., Ochs, M., & Pelachaud, C. (2012).
Mutualstance building in dyad of virtual agents: Smilealignment and synchronisation.
In Privacy,Security, Risk and Trust (PASSAT), 2012International Conference on Social Computing(SocialCom) (pp.
938?943).Schuller, B., Steidl, S., Batliner, A., N?th, E.,Vinciarelli, A., Burkhardt, F., ?
Weiss, B.(2012).
The INTERSPEECH 2012 speaker traitchallenge.
In Proceedings of the 13th AnnualConference of the International SpeechCommunication Association (INTERSPEECH2012).
Portland, OR: ISCA.Steidl, S., Polzehl, T., Bunnell, H. T., Dou, Y.,Muthukumar, P. K., Perry, ?
Metze, F. (2012).Emotion identification for evaluation ofsynthesized emotional speech.
In Proceedings ofthe 6th International Conference on SpeechProsody 2012 (pp.
4?7).
Shanghai: TongjiUniversity Press.Stronks, B., Nijholt, A., van Der Vet, P., Heylen, D.,& Machado, A.
(2002).
Designing forfriendship: Becoming friends with your ECA.
InA.
Marriott, C. Pelachaud, T. Rist, Z. M.Ruttkay, & H. Vilhjalmsson (Eds.
), Workshopon Embodied Conversational Agents - Let?sspecify and evaluate them!, AMAAS 2002 (pp.91?96).
Bologna: AMAAS.Tickle-Degnen, L. & Rosenthal, R. (1990).
The natureof rapport and its nonverbal correlates.Psychological Inquiry, 1(4), 285?293.Vardoulakis, L. P., Ring, L., Barry, B., Sidner, C. L.,& Bickmore, T. W. (2012).
Designing relationalagents as long term social companions for olderadults.
In Y. Nakano, M. Neff, A. Paiva, & M.Walker (Eds.
), Intelligent Virtual Agents (pp.289?302).
Santa Cruz, CA: Springer BerlinHeidelberg.Vinciarelli, A., Pantic, A., Bourlard, H. (2009) Socialsignal processing: Survey of an emergingdomain.
Image and Vision Computing, (27)12,1743-1759.Ward, N. (2006).
Non-Lexical Conversational Soundsin American English.
Pragmatics andCognition, (14)1, 113-184.Wang, W. Y., Finkelstein, S., Ogan, A., Black, A. W.,& Cassell, J.
(2012).
?Love ya, jerkface?
: Usingsparse log-linear models to build positive (andimpolite) relationships with teens.
InProceedings of the 13th Annual SIGDIALMeeting on Discourse and Dialogue (pp.
20?29).
Seoul, South Korea.Yang, Y., Shen, H. T., Ma, Z., Huang, Z., & Zhou, X.(2010).
l2,1-regularized discriminative featureselection for unsupervised learning.
InProceedings of the Twenty-Second InternationalJoint Conference on Artificial Intelligence (pp.1589?1594).
AAAI Press.Youniss, J.
(1982).
Parents and peers in socialdevelopment: A Sullivan-Piaget perspective.University of Chicago Press.Yuan, M. & Lin, Y.
(2007), Model selection andestimation in regression with grouped variables.Journal of the Royal Statistical Society, Series B68(1), 49-67.60
