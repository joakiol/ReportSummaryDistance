Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 836?845,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsKeyphrase Extraction Using Deep Recurrent Neural Networks on TwitterQi Zhang, Yang Wang, Yeyun Gong, Xuanjing HuangShanghai Key Laboratory of Data ScienceSchool of Computer Science, Fudan UniversityShanghai, P.R.
China{qz, ywang14, yygong12, xjhuang}@fudan.edu.cnAbstractKeyphrases can provide highly condensedand valuable information that allows users toquickly acquire the main ideas.
The task ofautomatically extracting them have receivedconsiderable attention in recent decades.Different from previous studies, which areusually focused on automatically extractingkeyphrases from documents or articles, inthis study, we considered the problem ofautomatically extracting keyphrases fromtweets.
Because of the length limitationsof Twitter-like sites, the performances ofexisting methods usually drop sharply.
Weproposed a novel deep recurrent neuralnetwork (RNN) model to combine keywordsand context information to perform thisproblem.
To evaluate the proposed method,we also constructed a large-scale datasetcollected from Twitter.
The experimentalresults showed that the proposed methodperforms significantly better than previousmethods.1 IntroductionKeyphrases are usually the selected phrases that cancapture the main topics described in a given docu-ment (Turney, 2000).
They can provide users withhighly condensed and valuable information, andthere are a wide variety of sources for keyphrases,including web pages, research articles, books, andeven movies.
In contrast to keywords, keyphrasesusually contain two or more words.
Normally, themeaning representations of these phrases are moreprecise than those of single words.
Moreover, alongwith the increasing development of the internet,this kind of summarization has received continuousconsideration in recent years from both the academicand entiprise communities (Witten et al, 1999; Wanand Xiao, 2008; Jiang et al, 2009; Zhao et al, 2011;Tuarob et al, 2015).Because of the enormous usefulness ofkeyphrases, various studies have been conducted onthe automatic extraction of keyphrases usingdifferent methods, including rich linguisticfeatures (Barker and Cornacchia, 2000; Paukkeriet al, 2008), supervised classification-basedmethods (Witten et al, 1999; Wu et al, 2005;Wang et al, 2006), ranking-based methods (Jianget al, 2009), and clustering-based methods (Moriet al, 2007; Danilevsky et al, 2014).
Thesemethods usually focus on extracting keyphrasesfrom a single document or multiple documents.Typically, a large number of words exist in even adocument of moderate length, where a few hundredwords or more is common.
Hence, statistical andlinguistic features can be considered to determinethe importance of phrases.In addition to the previously mentioned methods,a few researchers have studied the problem ofextracting keyphrases from collections of tweets(Zhao et al, 2011; Bellaachia and Al-Dhelaan,2012).
In contrast to traditional web applications,Twitter-like services usually limit the content lengthto 140 characters.
In (Zhao et al, 2011), the context-sensitive topical PageRank method was proposedto extract keyphrases by topic from a collectionof tweets.
NE-Rank was also proposed to rankkeywords for the purpose of extracting topical836keyphrases (Bellaachia and Al-Dhelaan, 2012).
Be-cause multiple tweets are usually organized bytopic, many document-level approaches can alsobe adopted to achieve the task.
In contrast withthe previous methods, Marujo et al (2015) focusedon the task of extracting keywords from singletweets.
They used several unsupervised methods andword embeddings to construct features.
However,the proposed method worked on the word level.In this study, we investigated the problem ofautomatically extracting keyphrases from singletweets.
Compared to the problem of identifyingkeyphrases from documents containing hundreds ofwords, the problem of extracting keyphrases from asingle short text is generally more difficult.
Manylinguistic and statistical features (e.g., the numberof word occurrences) cannot be determined andused.
Moreover, the standard steps of keyphraseextraction usually include keyword ranking, candi-date keyphrase generation, and keyphrase ranking.Previous works usually used separate methods tohandle these steps.
Hence, the error of each stepis propagated, which may highly impact the finalperformance.
Another challenge of keyphrase ex-traction on Twitter is the lack of training and eval-uation data.
Manual labelling is a time-consumingprocedure.
The labelling consistency of differentlabellers cannot be easily controlled.To meet these challenges, in this paper, wepropose a novel deep recurrent neural network(RNN) model for the joint processing of the key-word ranking, keyphrase generation, and keyphraseranking steps.
The proposed RNN model containstwo hidden layers.
In the first hidden layer, wecapture the keyword information.
Then, in thesecond hidden layer, we extract the keyphrasesbased on the keyword information using a sequencelabelling method.
In order to train and evaluate theproposed method, we also proposed a novel methodto construct a dataset that contained a large numberof tweets with golden standard keyphrases.
Theproposed dataset construction method was based onthe hashtag definitions in Twitter and how thesewere used in specific tweets.The main contributions of this work can besummarized as follows:?
We proposed a two-hidden-layer RNN-basedmethod to jointly model the keyword ranking,keyphrase generation, and keyphrase rankingsteps.?
To train and evaluate the proposed method, weproposed a novel method for constructing alarge dataset, which consisted of more than onemillion words.?
Experimental results demonstrated that the pro-posed method could achieve better results thanthe current state-of-the-art methods for thesetasks.2 Proposed MethodsIn this paper, we will first describe the deep recur-rent neural network (RNN).
Then, we will discussthe proposed joint-layer recurrent neural networkmodel, which jointly processes the keyword ranking,keyphrase generation, and keyphrase ranking.2.1 Deep Recurrent Neural NetworksOne way to capture the contextual information ofa word sequence is to concatenate neighboringfeatures as input features for a deep neural net-work.
However, the number of parameters rapidlyincreases according to the input dimension.
Hence,the size of the concatenating window is limited.A recurrent neural network (RNN) can be con-sidered to be a deep neural network (DNN) withan indefinite number of layers, which introducesthe memory from previous time steps.
A potentialweakness of a RNN is its lack of hierarchicalprocessing for the input at the current time step.To further provide hierarchical information throughmultiple time scales, deep recurrent neural networks(DRNNs) are explored (Hermans and Schrauwen,2013).
Fig.
1 (a) shows an L intermediate layerDRNN with full temporal connections (called astacked RNN (sRNN) in (Pascanu et al, 2013)).2.2 Joint-layer Recurrent Neural NetworksThe proposed joint-layer recurrent neural network(joint-layer RNN) is a variant of an sRNN with twohidden layers.
The joint-layer RNN has two outputlayers, which are combined into a objective layer.Suppose there is an L intermediate layer sRNN thathas an output layer for each hidden layer.
The l-th837(a) (b)Figure 1: Deep recurrent neural network (DRNN) architectures: arrows represent connection matrices; white, black, and grey circlesrepresent input frames, hidden states, and output frames, respectively; (a): L intermediate layer DRNN with recurrent connectionsat all levels (called stacked RNN); (b): joint-layer RNN folded out in time.
Each hidden layer can be interpreted to be an RNN thatreceives the time series of the previous layer as input, where the hidden layer transforms into an output layer.
Two output layers arecombined via linear superposition into the objective function.hidden activation is defined as:hlt = fh(hl?1t ,hlt?1)= ?l(Ulhlt?1 + Wlhl?1t ), (1)where hlt is the hidden state of the l-th layer attime t. Ul and Wl are the weight matrices forthe hidden activation at time t ?
1 and the lowerlevel activation hl?1t , respectively.
When l = 1,the hidden activation is computed using h0t = xt.
?l is an element-wise non-linear function, such asthe sigmoid function.
The l-th output activation isdefined as:y?lt = fo(hlt)= ?l(Vlhlt), (2)where Vl is the weight matrix for the l-th hiddenlayer hlt.
?l is also an element-wise non-linearfunction, such as the softmax function.A joint-layer recurrent neural network is anextension of a stacked RNN with two hidden layers.At time t, the training input, xt, of the network isthe concatenation of features from a mixture withina window.
We use word embedding as a feature inthis paper.
The output targets, y1t and y2t , and outputpredictions, y?1t and y?2t , of the network indicatewhether the current word is a keyword and part of akeyphrase, respectively.
y?1t just has two values Trueand False indicating whether the current word iskeyword.
y?2t has 5 values Single, Begin, Middle,End and Not indicating the current word is a singlekeyword, the beginning of a keyphrase, the middle(neither beginning nor ending) of a keyphrase, theending of a keyphrase or not a part of a keyphrase.Since our goal is to extract a keyphrase from aword sequence, we adopt a framework to simul-taneously model keyword finding and keyphraseextraction.
Figure 1 (b) shows the architecture of ourmodel.
The hidden layer formulation is defined as:h1t = fh(xt,h1t?1) (3)h2t = fh(h1t ,h2t?1).
(4)The output layer formulation is defined as:y?1t = fo(h1t ) (5)y?2t = fo(h2t ).
(6)8382.3 TrainingIn this work, we joined learning the parameters ?
inthe deep neural network.?
= {X,W1,W2,U1,U2,V1,V2},where X are the words embeddings, the otherparameters are defined before.
Once give a la-beled sentence we can know both the keyword andkeyphrase (keyphrase is made of keywords).
At thefirst output layer we use our model to discriminatekeyword and at the second output layer we useour model to discriminate keyphrase.
Then wecombine these two sub-objective which at differentdiscrimination level into the final objective.
The finalobjection is defined as:J(?)
= ?J1(?)
+ (1?
?)J2(?
), (7)where ?
is linear weighted factor.
Given N trainingsequences D ={(xt,y1t ,y2t)Tnt=1}Nn=1, the sub-objective formulation is defined as:J1(?)
=1NN?n=1Tn?t=1d(y?1t ,y1t ) (8)J2(?)
=1NN?n=1Tn?t=1d(y?2t ,y2t ), (9)where d(a,b) is a predefined divergence measurebetween a and b, such as Euclidean distance orcross-entropy.Eq.
(8) and Eq.
(9) show that we discover keywordand extract keyphrase at different level simulta-neously.
The experimental results will show thatcombination of different granularity discriminationcan significantly improve the performance.To minimize the objective function, we optimizeour models by back-propagating the gradients withrespect to the training objectives.
The stochasticgradient descent (SGD) algorithm is used to train themodels.
The update rule for the i-th parameter ?i atepoch e is as follows:?e,i = ?e?1,i ?
?ge,i, (10)where the ?
is a global learning rate shared by alldimensions.
ge is the gradient of the parameters atthe e-th iteration.
We select the best model accordingto the validation set.#tweets W T N?w N?t41,644,403 147,377 112,515 13.22 1.0Table 1: Statistical information of dataset.
W , T , N?w, and N?tare the vocabulary of words, number of tweets with hashtags,average number of words in each tweet, and average number ofhashtags in each tweet, respectively.3 Experiments3.1 Data ConstructionTo analyze the effectiveness of our model forkeyphrase extraction on Twitter, we constructed anevaluation dataset.
We crawled a large number oftweets.
Generally, for each user, we gathered about3K tweets, with a final total of more than 41 milliontweets.From analyzing these tweets, we found thatsome of the hashtags can be considered as thekeyphrases of the tweet.
For example: ?The Warriorstake Game 1 of the #NBAFinals 104-89 behinda playoff career-high 20 from Shaun Livingston.?.
?NBA Finals?
can be considered as the keyphraseof the twitter.
Based on this intuition, to constructthe dataset, we firstly filtered out all non-Latintweets using regular expressions.
Then, we removedany URL links from the tweets since we werefocusing on the textual content.
Tweets that startwith the ?@username?
are generally consideredreplies and have a conversational nature more thantopical nature.
Therefore, we also removed anytweets that start with ?@username?
to focus ontopical tweets only.
Moreover, we designed somerules about the hashtags in tweets to filter theremaining tweets.
First, one tweet could have onlyone hashtag.
Second, the position of the hashtag hadto be inside the tweet because we needed the hashtagand tweet to be semantically inseparable.
When ahashtag appears inside a tweet, it is most likely tobe an inseparable semantical part of the tweet andhas important meaning.
Therefore, we regarded thishashtag as a keyphrase of the tweet.Each hashtag was split into keywords if it en-compassed more than one word, for example ?Old-StockCanadians?
for ?Old Stock Canadians?.
Afteran effort to filter the tweets we finally had 110Ktweets with the hashtags which could meet our839Algorithm 1 Twitter Dataset ConstructionRequire: Tweets list tListEnsure: Filtered Tweets and hashtags1: resultList?
?2: while t in tList do3: if t not contains latin letters then4: continue5: end if6: if t starts with ?@username?
then7: continue8: end if9: removed any URL links from t10: if t not exactly contains one hashtag then11: continue12: end if13: get hashtag from t14: split hashtag into keywords15: resultList.append((t, hashtag))16: end while17: return resultListneeds.
The pseudocode is defined in Alg.
1.
Thestatistical information of the dataset can be seenin Table 1.
To evaluate the quality of the tweetsin our dataset, we randomly selected 1000 tweetsfrom our dataset and chose three volunteers.
Everytweet was assigned a score of 2 (perfectly suitable),1 (suitable), or 0 (unsuitable) to indicate whether thehashtag of the tweet was a good keyphrase for it.The results showed that 90.2% were suitable and66.1% were perfectly suitable.
This demonstratedthat our constructed dataset was good for keyphraseextraction on Twitter.3.2 Experiment ConfigurationsTo perform an experiment on extracting keyphrases,we used 70% as a training set, 10% as a developmentset, and 20% as a testing set.
For evaluation metrics,we used the precision (P), recall (R), and F1-score(F1) to evaluate the performance.
The precision wascalculated based on the percentage of keyphrasestruly identified among the keyphrases labeled bythe system.
Recall was calculated based on thekeyphrases truly identified among the golden stan-dard keyphrases.In the experiments, we use word embeddings asinput to the neural network.
The word embeddingswe used in this work were pre-trained vectors trainedon part of a Google News dataset (about 100 billionwords).
A skip-gram model (Mikolov et al, 2013)was used to generate these 300-dimensional vectorsfor 3 million words and phrases.
We used the wordembeddings to initialize our word weight matrix.The matrix was updated in the training process.The default parameters of our model are asfollows: The window size is 3, number of neuronsin the hidden layer is 300, and ?
is 0.5, which werechosen based on the performance using the valid set.3.3 Methods for ComparisonSeveral algorithms were implemented and used toevaluate the validity of the proposed approach.Among these algorithms, CRF, RNN, LSTM, andR-CRF treat the keyphrase extraction task as asequence labelling task.
Automatic keyword ex-traction on Twitter (AKET) uses an unsupervisedmethod to extract keywords on Twitter.?
CRF: The keyphrase extraction task can beformalized as a sequence labeling task thatinvolves the algorithmic assignment of a cat-egorical label to each word of a tweet.
CRF is atype of discriminative undirected probabilisticgraphical model and can process a sequencelabeling task.
Hence, we applied CRF to extractkeyphrases on Twitter.?
RNN: A recurrent neural network (RNN) is atype of artificial neural network where the con-nections between units form a directed cycle.This creates an internal state of the networkthat allows it to exhibit dynamic temporalbehavior.
In an RNN model, word embeddingis introduced to represent the semantics ofwords.?
LSTM: Long short-term memory (LSTM) isa recurrent neural network (RNN) architecture.Unlike traditional RNNs, an LSTM network iswell-suited to learn from experience to classify,process, and predict time series when there arevery long time lags of unknown size betweenimportant events.?
R-CRF: A recurrent conditional random field(R-CRF)(Yao et al, 2014) is a mixture model840P R F1CRF 72.37% 71.82% 72.09%RNN 78.65% 70.08% 74.14%LSTM 77.52% 71.19% 74.22%R-CRF 79.29% 73.15% 76.10%AKET 11.00% 46.10% 17.80%Joint-layer RNN 80.74% 81.19% 80.97%Table 2: Keyphrase Extraction on Twittercombining an RNN and a CRF.
This model hasthe advantages of both the CRF and RNN.
Theprevious work showed that the performance ofR-CRF can be significantly improved.?
AKET (Automatic Keyword Extraction onTwitter) (Marujo et al, 2015): Several unsuper-vised methods and word embeddings were usedto construct features to obtain keyword.3.4 Experiment ResultsTable 2 shows the performances of different meth-ods on the dataset for keyphrase extraction.
Fromthe results, we observe that the joint-layer RNNachieved a better performance than the state-of-the-art methods.
The relative improvement in the F-score of the joint-layer RNN over the second bestresult was 6.1%.
AKET performed the worst.
Thiswas because AKET worked on the word level.
Of theother methods, CRF performed the worst, RNN andLSTM were almost the same but better than CRF,and R-CRF was the best of these methods, with theexception of our joint-layer RNN.
The results can beexplained by the word embedding and long short-term memory cell providing some benefits.
The bestresult was found with our joint-layer RNN.
Thisindicated that the joint processing of the keywordfinding and keyphrase extraction worked well andcould to some degree demonstrate the effectivenessof our model in keyphrase extraction on Twitter.To further analyze the keyword extraction resultson Twitter, we compared AKET and our method.In Table 3, we can see that except for the recall,AKET is a little better than our method, but ourmethod performed significantly better than AKETin the precision and F-score.
This indicates that ourP R F1AKET 20.68% 87.56% 33.46%Joint-layer RNN 87.45% 85.38% 86.40%Table 3: Keyword Extraction on Twittermodel indeed has better performance in keywordfinding.In summary, the experimental results conclusivelydemonstrated that the proposed joint-layer RNNmethod is superior to the state-of-the-art methodswhen measured using commonly accepted perfor-mance metrics on Twitter.To analysis the sensitivity of the hyper-parametersof the joint-layer RNN, we conducted several empir-ical experiments on the dataset.Fig.2(a) shows the performances of the joint-layer RNN with different numbers of neurons in thehidden layers.
To simplify, we made hidden layer1 and hidden layer 2 have the same number ofneurons.
In the figure, the x-axis denotes the numberof neurons, and the y-axis denotes the precision,recall, and F-score.
The data used for constructingthe test set were the same as we used in the previoussection.
From the figure, we can observe that thenumber of neurons in the hidden layers do not highlyaffect the final performance.
Three performanceindicators of the joint-layer RNN change stably withdifferent numbers of neurons.Fig.2(b) shows the performances of the joint-layerRNN with different window sizes.
In the figure, thex-axis denotes the different window size, and the y-axis denotes the precision, recall, and F-score.
Fromthe figure, we observe that when the window sizeis one, the three performance indicators of joint-layer RNN perform badly.
Then, as the window sizeincreases, the three performance indicators changestably.
The main reason may possibly be that whenthe window size is one, the model just uses thecurrent word information.
When the window sizeincreases, the model uses the context informationof the current word but the most important contextinformation is nearby the current word.Fig.2(c) shows the performances of the joint-layerRNN with different ?
values.
In the figure, the x-axis denotes the value of ?
used for training, andthe y-axis denotes the precision, recall, and F-score.841PrecisionRecall F1-score60657075808590# of Neurons50 100 150 200 250 300 350 400PrecisionRecall F1-score60657075808590Window Size1 3 5 7 9PrecisionRecall F1-score60657075808590Alpha0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9(a) (b) (c)Figure 2: (a): Performance with varying number of neurons in the hidden layer; (b): Performance with varying window size; (c):Performance with varying ?.P R F1WEU 80.74% 81.19% 80.97%WENU 74.10% 69.30% 71.62%REU 79.01% 79.75% 79.38%RENU 78.16% 64.55% 70.70%Table 4: Effects of embedding on performance.
WEU, WENU,REU and RENU represent word embedding update, wordembedding without update, random embedding update andrandom embedding without update respectively.We can see that the best performance is obtainedwhen ?
is around 0.5.
This indicates that our modelemphasizes the combination of keyword finding andkeyphrase extraction.Table 4 lists the effects of word embedding.
Wecan see that the performance when updating theword embedding is better than when not updating,and the performance of word embedding is a littlebetter than random word embedding.
The mainreason is that the vocabulary size is 147,377, but thenumber of words from tweets that exist in the wordembedding trained on the Google News dataset isjust 35,133.
This means that 76.2% of the words aremissing.
This also confirms that the proposed joint-layer RNN is more suitable for keyphrase extractionon Twitter.Fig.3(a) shows the performances of the joint-layerRNN with different percentages of training data.In the figure, the x-axis denotes the percentagesof data used for training, and the y-axis denotesthe precision, recall, and F-score.
From the figure,we observe that as the amount of training dataincreases, the three performance indicators of thejoint-layer RNN consequently improve.
When thepercentage of training data is greater than 60% ofthe whole dataset, the performance indicators slowlyincrease.
The main reason may possibly be that thenumber concepts included in these data sets aresmall.
However, on the other hand, we can say thatthe proposed joint-layer RNN method can achieveacceptable results with a few ground truths.
Hence,it can be easily adopted for other data sets.Since the keyphrase extraction training processis solved using an iterative procedure, we alsoevaluated its convergence property.
Fig.3 (b) showsthe precision, recall, and F-score performances ofthe joint-layer RNN.
In the figure, the x-axis denotesthe number of epochs for optimizing the model,and the y-axis denotes the precision, recall, and F-score.
From the figure, we observe that the joint-layer RNN can coverage with less than six iterations.This means that the joint-layer RNN can achieve astable and superior performance under a wide rangeof parameter values.4 Related WorkIn general, keyphrase extraction methods can beroughly divided into two groups: supervised ma-chine learning approaches and unsupervised rankingapproaches.In the supervised line of research, keyphraseextraction is treated as a classification problem,in which a candidate must be classified as eithera keyphrase (i.e., keyphrases) or not (i.e., non-842PrecisionRecall F1-score60657075808590Percents of Training Data20% 40% 60% 80% 100%PrecisionRecall F1-score60657075808590Number of Epochs1 2 3 4 5 6 7 8 9 10(a) (b)Figure 3: (a): Effects of train size on performance; (b): Effectsof the number of epochs on performance.keyphrases).
A classifier needs to be trained usingannotated training data.
The trained model is thenapplied to documents for which keyphrases areto be identified.
For example (Frank et al, 1999)developed a system called KEA that used twofeatures: tf-idf and first occurrence of the term andused them as input to Naive Bayes (Hulth, 2003)used linguistic knowledge (i.e., part-of-speech tags)to determine candidate sets: potential pos-patternswere used to identify candidate phrases from thetext.
Tang et al (2004) applied Bayesian decisiontheory for keyword extraction.
Medelyan and Wittenextended the KEA to KEA++, which uses semanticinformation on terms and phrases extracted from adomain specific thesaurus, thus enhances automatickeyphrase extraction (Medelyan and Witten, 2006).In the unsupervised line of research, keyphraseextraction is formulated as a ranking problem.
Awell-known approach is the Term Frequency In-verse Document Frequency (TF-IDF) (Sparck Jones,1972; Zhang et al, 2007; Lee and Kim, 2008).Measures like term frequencies (Wu and Giles,2013; Rennie and Jaakkola, 2005; Kireyev, 2009),inverse document frequencies, topic proportions,etc.
and knowledge of specific domain are appliedto rank terms in documents which are aggregated toscore the phrases.
The ranking based on tf-idf hasbeen shown to work well in practice (Hasan and Ng,2010).
Mihalcea et al proposed the TextRank, whichconstructs keyphrases using the PageRank valuesobtained on a graph based ranking model for graphsextracted from texts (Mihalcea and Tarau, 2004).
Liuet al proposed to extract keyphrases by adoptinga clustering-based approach, which ensures thatthe document is semantically covered by thesekeyphrases (Liu et al, 2009).
Ali Mehri et al putforward a method for ranking the words in texts,which can also be used to classify the correlationrange between word-type occurrences in a text,by using non-extensive statistical mechanics (Mehriand Darooneh, 2011).Recurrent neural networks(RNNs) (Elman, 1990)has been applied to many sequential predictiontasks, which is an important class of naturally deeparchitecture.
In NLP, RNNs deal with a sentenceas a sequence of tokens and have been successfullyapplied to various tasks like spoken language under-standing (Mesnil et al, 2013) and language model-ing (Mikolov et al, 2011).
Classical recurrent neuralnetworks incorporate information from preceding,there are kinds of variants, bidirectional RNNs arealso useful for NLP tasks, especially when making adecision on the current token, information providedby the following tokens is generally useful.5 ConclusionIn this work, we proposed a novel deep recurrentneural network (RNN) model to combine keywordsand context information to perform the keyphraseextraction task.
The proposed model can jointlyprocess the keyword ranking and keyphrase gener-ation task.
It has two hidden layers to discriminatekeywords and classify keyphrases, and these twosub-objectives are combined into a final objectivefunction.
We evaluated the proposed method on adataset filtered from ten million crawled tweets.
Theproposed method can achieve better results thanthe state-of-the-art methods.
The experimental re-sults demonstrated the effectiveness of the proposedmethod for keyphrase extraction on single tweets.6 AcknowledgementThe authors wish to thank the anonymous reviewersfor their helpful comments.
This work was partiallyfunded by National Natural Science Foundation ofChina (No.
61532011, 61473092, and 61472088),the National High Technology Research and Devel-opment Program of China (No.
2015AA015408).ReferencesKen Barker and Nadia Cornacchia.
2000.
Using nounphrase heads to extract document keyphrases.
InAdvances in Artificial Intelligence.843Abdelghani Bellaachia and Mohammed Al-Dhelaan.2012.
Ne-rank: A novel graph-based keyphraseextraction in twitter.
In Proceedings of IEEE CS.Marina Danilevsky, Chi Wang, Nihit Desai, Xiang Ren,Jingyi Guo, and Jiawei Han.
2014.
Automaticconstruction and ranking of topical keyphrases oncollections of short documents.
In Proceedings ofSDM.Jeffrey L Elman.
1990.
Finding structure in time.Cognitive science.Eibe Frank, Gordon W Paynter, Ian H Witten, CarlGutwin, and Craig G Nevill-Manning.
1999.
Domain-specific keyphrase extraction.Kazi Saidul Hasan and Vincent Ng.
2010.
Conundrumsin unsupervised keyphrase extraction: making sense ofthe state-of-the-art.
In Proceedings of COLING.Michiel Hermans and Benjamin Schrauwen.
2013.Training and analysing deep recurrent neural net-works.
In Proceedings of NIPS.Anette Hulth.
2003.
Improved automatic keywordextraction given more linguistic knowledge.
InProceedings of EMNLP.Xin Jiang, Yunhua Hu, and Hang Li.
2009.
A rankingapproach to keyphrase extraction.
In Proceedings ofSIGIR.Kirill Kireyev.
2009.
Semantic-based estimation of terminformativeness.
In Proceedings of NAACL.Sungjick Lee and Han-joon Kim.
2008.
News keywordextraction for topic tracking.
In Proceedings of NCM.Zhiyuan Liu, Peng Li, Yabin Zheng, and Maosong Sun.2009.
Clustering to find exemplar terms for keyphraseextraction.
In Proceedings of EMNLP.Luis Marujo, Wang Ling, Isabel Trancoso, Chris Dyer,Alan W Black, Anatole Gershman, David Martins deMatos, Joa?o Neto, and Jaime Carbonell.
2015.
Auto-matic keyword extraction on twitter.
In Proceedingsof ACL.Olena Medelyan and Ian H Witten.
2006.
Thesaurusbased automatic keyphrase indexing.
In Proceedingsof JCDL.Ali Mehri and Amir H Darooneh.
2011.
Keywordextraction by nonextensivity measure.
PhysicalReview E.Gre?goire Mesnil, Xiaodong He, Li Deng, and YoshuaBengio.
2013.
Investigation of recurrent-neural-network architectures and learning methods for spokenlanguage understanding.
In Proceedings of INTER-SPEECH.Rada Mihalcea and Paul Tarau.
2004.
Textrank:Bringing order into texts.
In Proceedings of ACL.Toma?s?
Mikolov, Stefan Kombrink, Luka?s?
Burget,Jan Honza C?ernocky`, and Sanjeev Khudanpur.
2011.Extensions of recurrent neural network languagemodel.
In Proceedings of ICASSP.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representationsof words and phrases and their compositionality.
InProceedings of NIPS.Junichiro Mori, Mitsuru Ishizuka, and Yutaka Matsuo.2007.
Extracting keyphrases to represent relations insocial networks from web.
In Proceedings of IJCAI.Razvan Pascanu, Caglar Gulcehre, Kyunghyun Cho, andYoshua Bengio.
2013.
How to construct deeprecurrent neural networks.
arXiv.Mari-Sanna Paukkeri, Ilari T Nieminen, Matti Po?lla?,and Timo Honkela.
2008.
A language-independentapproach to keyphrase extraction and evaluation.
InProceedings of COLING.Jason DM Rennie and Tommi Jaakkola.
2005.
Usingterm informativeness for named entity detection.
InProceedings of SIGIR.Karen Sparck Jones.
1972.
A statistical interpretation ofterm specificity and its application in retrieval.
JDoc.Jie Tang, Juan-Zi Li, Ke-Hong Wang, and Yue-Ru Cai.2004.
Loss minimization based keyword distillation.In Advanced Web Technologies and Applications.Suppawong Tuarob, Wanghuan Chu, Dong Chen, andConrad S Tucker.
2015.
Twittdict: Extracting socialoriented keyphrase semantics from twitter.
IJCNLP.Peter D Turney.
2000.
Learning algorithms forkeyphrase extraction.
Information Retrieval.Xiaojun Wan and Jianguo Xiao.
2008.
Single documentkeyphrase extraction using neighborhood knowledge.In Proceedings of AAAI.Jiabing Wang, Hong Peng, and Jing-song Hu.
2006.Automatic keyphrases extraction from document usingneural network.
In Advances in Machine Learning andCybernetics.Ian H Witten, Gordon W Paynter, Eibe Frank, CarlGutwin, and Craig G Nevill-Manning.
1999.Kea: Practical automatic keyphrase extraction.
InProceedings of DL.Zhaohui Wu and C Lee Giles.
2013.
Measuring terminformativeness in context.
In Proceedings of NAACL.Yi-fang Brook Wu, Quanzhi Li, Razvan Stefan Bot,and Xin Chen.
2005.
Domain-specific keyphraseextraction.
In Proceedings of CIKM.Kaisheng Yao, Baolin Peng, Geoffrey Zweig, Dong Yu,Xiaolong Li, and Feng Gao.
2014.
Recurrentconditional random field for language understanding.In Proceedings of ICASSP.Yongzheng Zhang, Evangelos Milios, and Nur Zincir-Heywood.
2007.
A comparative study on keyphrase extraction methods in automatic web sitesummarization.
JDIM.Wayne Xin Zhao, Jing Jiang, Jing He, Yang Song,Palakorn Achananuparp, Ee-Peng Lim, and Xiaoming844Li.
2011.
Topical keyphrase extraction from twitter.In Proceedings of ACL.845
