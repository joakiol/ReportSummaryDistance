Proceedings of the ACL-IJCNLP 2009 Conference Short Papers, pages 317?320,Suntec, Singapore, 4 August 2009. c?2009 ACL and AFNLPGraph Ranking for Sentiment TransferQiong Wu1,2, Songbo Tan1 and Xueqi Cheng11Institute of Computing Technology, Chinese Academy of Sciences, China2 Graduate University of Chinese Academy of Sciences, China{wuqiong,tansongbo}@software.ict.ac.cn, cxq@ict.ac.cnAbstractWith the aim to deal with sentiment-transferproblem, we proposed a novel approach,which integrates the sentiment orientations ofdocuments into the graph-ranking algorithm.We apply the graph-ranking algorithm usingthe accurate labels of old-domain documentsas well as the ?pseudo?
labels of new-domaindocuments.
Experimental results show thatproposed algorithm could improve the per-formance of baseline methods dramatically forsentiment transfer.1 IntroductionWith the rapid growth of reviewing pages, sen-timent classification is drawing more and moreattention (Bai et al, 2005; Pang and Lee, 2008).Generally speaking, sentiment classification canbe considered as a special kind of traditional textclassification (Tan et al, 2005; Tan, 2006).
Inmost cases, supervised learning methods can per-form well (Pang et al, 2002).
But when trainingdata and test data are drawn from different do-mains, supervised learning methods always pro-duce disappointing results.
This is so-calledcross-domain sentiment classification problem(or sentiment-transfer problem).Sentiment transfer is a new study field.
In re-cent years, only a few works are conducted onthis field.
They are generally divided into twocategories.
The first one needs a small amount oflabeled training data for the new domain (Aueand Gamon, 2005).
The second one needs nolabeled data for the new domain (Blitzer et al,2007; Tan et al, 2007; Andreevskaia and Bergler,2008; Tan et al, 2008; Tan et al, 2009).
In thispaper, we concentrate on the second categorywhich proves to be used more widely.Graph-ranking algorithm has been success-fully used in many fields (Wan et al, 2006; Esuliand Sebastiani, 2007), whose idea is to give anode high score if it is strongly linked with otherhigh-score nodes.
In this work, we extend thegraph-ranking algorithm for sentiment transferby integrating the sentiment orientations of thedocuments, which could be considered as a sen-timent-transfer version of the graph-ranking al-gorithm.
In this algorithm, we assign a score forevery unlabelled document to denote its extent to?negative?
or ?positive?, then we iteratively cal-culate the score by making use of the accuratelabels of old-domain data as well as the ?pseudo?labels of new-domain data, and the final scorefor sentiment classification is achieved when thealgorithm converges, so we can label the new-domain data based on these scores.2 The Proposed Approach2.1 OverviewIn this paper, we have two document sets: thetest data DU = {d1,?,dn} where di is the termvector of the ith text document and each di?DU(i= 1,?,n) is unlabeled; the training data DL ={dn+1,?dn+m} where dj represents the term vectorof the jth text document and each dj?DL(j =n+1,?,n+m) should have a label from a categoryset C = {negative, positive}.
We assume thetraining dataset DL is from the related but differ-ent domain with the test dataset DU.
Our objec-tive is to maximize the accuracy of assigning alabel in C to di?DU (i = 1,?,n) utilizing thetraining data DL in another domain.The proposed algorithm is based on the fol-lowing presumptions:(1) Let WL denote the word space of old do-main, WU denote the word space of new domain.WL?WU??.
(2) The labels of documents appear both in thetraining data and the test data should be the same.Based on graph-ranking algorithm, it isthought that if a document is strongly linked withpositive (negative) documents, it is probablypositive (negative).
And this is the basic idea oflearning from a document?s neighbors.Our algorithm integrates the sentiment orienta-tions of the documents into the graph-rankingalgorithm.
In our algorithm, we build a graph317whose nodes denote documents and edges denotethe content similarities between documents.
Weinitialize every document a score (?1?
denotespositive, and ?-1?
denotes negative) to representits degree of sentiment orientation, and we call itsentiment score.
The proposed algorithm calcu-lates the sentiment score of every unlabelleddocument by learning from its neighbors in bothold domain and new domain, and then iterativelycalculates the scores with a unified formula.
Fi-nally, the algorithm converges and each docu-ment gets its sentiment score.
When its sentimentscore falls in the range [0, 1] (or [-1, 0]], thedocument should be classified as ?positive (ornegative)?.
The closer its sentiment score is near1 (or -1), the higher the ?positive (or negative)?degree is.2.2 Score DocumentsScore Documents Using Old-domain Informa-tionWe build a graph whose nodes denote documentsin both DL and DU and edges denote the contentsimilarities between documents.
If the contentsimilarity between two documents is 0, there isno edge between the two nodes.
Otherwise, thereis an edge between the two nodes whose weightis the content similarity.
The content similaritybetween two documents is computed with thecosine measure.
We use an adjacency matrix Uto denote the similarity matrix between DU andDL.
U=[Uij]nxm is defined as follows:mnnjniddddUjijiij ++==?
?= ,...,1,,...,1,    (1)The weight associated with term t is computedwith tftidft where tft is the frequency of term t inthe document and idft is the inverse documentfrequency of term t, i.e.
1+log(N/nt), where N isthe total number of documents and nt is the num-ber of documents containing term t in a data set.In consideration of convergence, we normal-ize U to U?
by making the sum of each row equalto 1:1 1, 0?0,m mij ij ijj jijU U if UUotherwise= =?
?
?= ????
?
(2)In order to find the neighbors (in another word,the nearest documents) of a document, we sortevery row of U?
to U% in descending order.
That is:U% ij?
U% ik (i = 1,?n; j,k = 1,?m; k?j).Then for di?DU (i = 1,?,n), U% ij (j = 1,?,K )corresponds to K neighbors in DL.
So we can getits K neighbors.
We use a matrix [ ]ij n KN N ?=to denote the neighbors of DU in old domain,with Nij corresponding to the jth nearest neighborof di.At last, we can calculate sentiment score si (i= 1,?,n) using the scores of the di?s neighbors asfollows:nisUsijNjkijki ,...,1,)?
()1()( =?= ????
(3)where ?i means the ith row of a matrix and)(kis denotes the is at the kth iteration.Score Documents Using New-domain Infor-mationSimilarly, a graph is built, in which each nodecorresponds to a document in DU and the weightof the edge between any different documents iscomputed by the cosine measure.
We use an ad-jacency matrix V=[Vij]nxn to describe the similar-ity matrix.
And V is similarly normalized to V?
tomake the sum of each row equal to 1.
Then wesort every row of V?
to V% in descending order,thus we can get K neighbors of di?DU (i =1,?,n) from V% ij (j = 1,?K), and we use a matrix[ ]ij n KM M ?=  to denote the neighbors of DU inthe new domain.
Finally, we can calculate si us-ing the sentiment scores of the di?s neighbors asfollows:????
=?=ijiMjkijk nisVs ,...,1),?
( )1()(          (4)2.3 Sentiment Transfer AlgorithmInitializationFirstly, we classify the test data DU to get theirinitial labels using a traditional classifier.
Forsimplicity, we use prototype classification algo-rithm (Tan et al, 2005) in this work.Then, we give ?-1?
to si(0) if di?s label is?negative?, and ?1?
if ?positive?.
So we obtainthe initial sentiment score vector S(0) for bothdomain data.At last, si(0) (i = 1,?,n) is normalized as fol-lows to make the sum of positive scores of DUequal to 1, and the sum of negative scores of DUequal to -1:nisifsssifsssiDjjiiDjjiUposUnegi,...,10,0,)()0()0()0()0()0()0()0( =??????
?><?= ????
(5)318where UnegD andUposD denote the negative andpositive document set of DU respectively.
Thesame as (5), sj (0) (j =n+1,?,n+m) is normalized.Algorithm IntroductionIn our algorithm, we label DU by making use ofinformation of both old domain and new domain.We fuse equations (3) and (4), and get the itera-tive equation as follows:nisVsUsihijiMhkihNjkijk ,...,1,)?()?
( )1()1()( =?+?= ????
????
??
(6)where 1?
?+ = , and ?
and?
show the relativeimportance of old domain and new domain to thefinal sentiment scores.
In consideration of theconvergence, S(k) (S at the kth iteration) is normal-ized after each iteration.Here is the complete algorithm:1.
Classify DU with a traditional classifier.Initialize the sentiment score si of di?DU?DL (i = 1,?n+m) and normalize it.2.
Iteratively calculate the S(k) of DU andnormalize it until it achieves the conver-gence:nisVsUsihijiMhkihNjkijk ,...,1,)?()?
( )1()1()( =?+?= ????
????
?
?nisifsssifssskiDjkjkikiDjkjkikiUposUneg ,...,10,0,)()()()()()()()( =??????
?><?= ????3.
According to si?
S (i = 1,?,n), assigneach di?DU (i = 1,?n) a label.
If si is be-tween -1 and 0, assign di the label ?nega-tive?
; if si is between 0 and 1, assign di thelabel ?positive?.3 EXPERIMENTS3.1 Data PreparationWe prepare three Chinese domain-specific datasets from on-line reviews, which are: ElectronicsReviews (Elec, from http://detail.zol.com.cn/),Stock Reviews (Stock, from http://blog.sohu.com/stock/) and Hotel Reviews (Hotel, fromhttp://www.ctrip.com/).
And then we manuallylabel the reviews as ?negative?
or ?positive?.The detailed composition of the data sets areshown in Table 1, which shows the name of thedata set (DataSet), the number of negative re-views (Neg), the number of positive reviews(Pos), the average length of reviews (Length),the number of different words (Vocabulary) inthis data set.DataSet Neg Pos Length VocabularyElec 554 1,054 121 6,200Stock 683 364 460 13,012Hotel 2,000 2,000 181 11,336Table 1.
Data sets compositionWe make some preprocessing on the datasets.First, we use ICTCLAS (http://ictclas.org/), aChinese text POS tool, to segment these Chinesereviews.
Second, the documents are representedby vector space model.3.2 Evaluation SetupIn our experiment, we use prototype classifica-tion algorithm (Tan et al, 2005) and SupportVector Machine experimenting on the three datasets as our baselines separately.
The SupportVector Machine is a state-of-the-art supervisedlearning algorithm.
In our experiment, we useLibSVM (www.csie.ntu.edu.tw/~cjlin/libsvm/) with alinear kernel and set al options by default.We also compare our algorithm to StructuralCorrespondence Learning (SCL) (Blitzer et al,2007).
SCL is a state-of-the-art sentiment-transfer algorithm which automatically inducescorrespondences among features from differentdomains.
It identifies correspondences amongfeatures from different domains by modelingtheir correlations with pivot features, which arefeatures that behave in the same way for dis-criminative learning in both domains.
In our ex-periment, we use 100 pivot features.3.3 Overall PerformanceIn this section, we conduct two groups of ex-periments where we separately initialize the sen-timent scores in our algorithm by prototype clas-sifier and Support Vector Machine.There are two parameters in our algorithm, Kand ?
( ?
can be calculated by 1-?
).
We set theparameters K and ?
with 150 and 0.7 respec-tively, which indicates we use 150 neighbors andthe contribution from old domain is a little moreimportant than that from new domain.
It isthought that the algorithm achieves the conver-gence when the changing between the sentimentscore si computed at two successive iterations forany di?DU (i = 1,?n) falls below a giventhreshold, and we set the threshold 0.00001 inthis work.Table 2 shows the accuracy of Prototype,LibSVM, SCL and our algorithm when trainingdata and test data belong to different domains.319Our algorithm is separately initialized by Proto-type and LibSVM.Baseline Proposed AlgorithmPrototype LibSVMSCL Prototype+OurApproachLibSVM+OurApproachElec->Stock 0.6652 0.6478 0.7507 0.7326 0.7304Elec->Hotel 0.7304 0.7522 0.7750 0.7543 0.7543Stock->Hotel 0.6848 0.6957 0.7683 0.7435 0.7457Stock->Elec 0.7043 0.6696 0.8340 0.8457 0.8435Hotel->Stock 0.6196 0.5978 0.6571 0.7848 0.7848Hotel->Elec 0.6674 0.6413 0.7270 0.8609 0.8609Average 0.6786 0.6674 0.7520 0.7870 0.7866Table 2.
Accuracy comparison of different methodsAs we can observe from Table 2, our algo-rithm can dramatically increase the accuracy ofsentiment-transfer.
Seen from the 2nd column andthe 5th column, every accuracy of the proposedalgorithm is increased comparing to Prototype.The average increase of accuracy over all the 6problems is 10.8%.
Similarly, the accuracy ofour algorithm is higher than LibSVM in everyproblem and the average increase of accuracy is11.9%.
The great improvement comparing withthe baselines indicates that the proposed algo-rithm performs very effectively and robustly.Seen from Table 2, our result about SCL is inaccord with that in (Blitzer et al, 2007) on thewhole.
The average accuracy of SCL is higherthan both baselines, which convinces that SCL iseffective for sentiment-transfer.
However, ourapproach outperforms SCL: the average accuracyof our algorithm is about 3.5 % higher than SCL.This is caused by two reasons.
First, SCL is es-sentially based on co-occurrence of words (thewindow size is the whole document), so it is eas-ily affected by low frequency words and the sizeof data set.
Second, the pivot features of SCL aretotally dependent on experts in the field, so thequality of pivot features will seriously affect theperformance of SCL.
This improvement con-vinces us of the effectiveness of our algorithm.4 Conclusion and Future WorkIn this paper, we propose a novel sentiment-transfer algorithm.
It integrates the sentimentorientations of the documents into the graph-ranking based method for sentiment-transferproblem.
The algorithm assigns a score for everydocument being predicted, and it iteratively cal-culates the score making use of the accurate la-bels of old-domain data, as well as the ?pseudo?labels of new-domain data, finally it labels thenew-domain data as ?negative?
or ?positive?
bas-ing on this score.
The experiment results showthat the proposed approach can dramatically im-prove the accuracy when transferred to a newdomain.In this study, we find the neighbors of a givendocument using cosine similarity.
This is toogeneral, and perhaps not so proper for sentimentclassification.
In the next step, we will try othermethods to calculate the similarity.
Also, ourapproach can be applied to multi-task learning.5 AcknowledgmentsThis work was mainly supported by two funds, i.e.,0704021000 and 60803085, and one another project,i.e., 2004CB318109.ReferencesB.
Pang and L. Lee.
2008.
Opinion mining and senti-ment analysis.
Foundations and Trends in Infor-mation Retrieval, 2008S.
Tan, X. Cheng, M. Ghanem, B. Wang and H. Xu.2005.
A Novel Refinement Approach for TextCategorization.
In Proceedings of CIKM 2005.S.
Tan.
2006.
An Effective Refinement Strategy forKNN Text Classifier.
Expert Systems With Appli-cations.
Elsevier.
30(2): 290-298.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
Sentiment classification using machine learn-ing techniques.
In Proceedings of EMNLP, 2002.X.
Bai, R. Padman and E. Airoldi.
2005.
On learningparsimonious models for extracting consumeropinions.
In Proceedings of HICSS 2005.A.
Aue and M. Gamon.
2005.
Customizing sentimentclassifiers to new domains: a case study.
InProceedings of RANLP 2005.J.
Blitzer, M. Dredze, and F. Pereira.
2007.
Biogra-phies, Bollywood, Boom-boxes and Blenders:Domain adaptation for sentiment classification.
InProceedings of ACL 2007.S.
Tan, G. Wu, H. Tang and X. Cheng.
2007.
A novelscheme for domain-transfer problem in the contextof sentiment analysis.
In Proceedings of CIKM2007.S.
Tan, Y. Wang, G. Wu and X. Cheng.
2008.
Usingunlabeled data to handle domain-transfer problemof semantic detection.
In Proceedings of SAC 2008.S.
Tan, X. Cheng, Y. Wang, H. Xu.
2009.
AdaptingNaive Bayes to Domain Adaptation for SentimentAnalysis.
In Proceedings of ECIR 2009.A.
Esuli, F. Sebastiani.
2007.
Random-walk modelsof term semantics: An application to opinion-related properties.
In Proceedings of LTC 2007.X.
Wan, J. Yang and J. Xiao.
2006.
Using Cross-Document Random Walks for Topic-FocusedMulti-Document Summarization.
In Proceedingsof WI 2006.A.
Andreevskaia and S. Bergler.
2008.
When Special-ists and Generalists Work Together: OvercomingDomain Dependence in Sentiment Tagging.
InProceedings of ACL 2008.320
