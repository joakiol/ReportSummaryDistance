Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1142?1151,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPOn the Role of Lexical Features in Sequence LabelingYoav Goldberg?and Michael ElhadadBen Gurion University of the NegevDepartment of Computer SciencePOB 653 Be?er Sheva, 84105, Israel{yoavg|elhadad}@cs.bgu.ac.ilAbstractWe use the technique of SVM anchoring todemonstrate that lexical features extractedfrom a training corpus are not necessary toobtain state of the art results on tasks suchas Named Entity Recognition and Chunk-ing.
While standard models require asmany as 100K distinct features, we derivemodels with as little as 1K features thatperform as well or better on different do-mains.
These robust reduced models in-dicate that the way rare lexical featurescontribute to classification in NLP is notfully understood.
Contrastive error analy-sis (with and without lexical features) in-dicates that lexical features do contributeto resolving some semantic and complexsyntactic ambiguities ?
but we find thiscontribution does not generalize outsidethe training corpus.
As a general strat-egy, we believe lexical features should notbe directly derived from a training corpusbut instead, carefully inferred and selectedfrom other sources.1 IntroductionCommon NLP tasks, such as Named EntityRecognition and Chunking, involve the identifi-cation of spans of words belonging to the samephrase.
These tasks are traditionally reduced toa tagging task, in which each word is to be clas-sified as either Beginning a span, Inside a span,or Outside of a span.
The decision is based onthe word to be classified and its neighbors.
Fea-tures supporting the classification usually include?Supported by the Lynn and William Frankel Center forComputer Sciences, Ben Gurion Universitythe word forms themselves and properties derivedfrom the word forms, such as prefixes, suffixes,capitalization information, and parts-of-speech.While early approaches to the NP-chunking task(Cardie and Pierce, 1998) relied on part-of-speechinformation alone, it is widely accepted that lexi-cal information (word forms) is crucial for build-ing accurate systems for these tasks.
Indeed,all the better-performing systems in the CoNLLshared tasks competitions for Chunking (Sang andBuchholz, 2000) and Named Entity Recognition(Tjong Kim Sang, 2002; Tjong Kim Sang andDe Meulder, 2003) make extensive use of suchlexical information.Is this belief justified?
In this paper, we showthat the influence of lexical features on such se-quence labeling tasks is more complex than is gen-erally assumed.
We find that exact word formsaren?t necessary for accurate classification.
Thisobservation is important because relying on theexact word forms that appear in a training corpusleads to over-fitting, as well as to larger models.In this work, we focus on learning with SupportVector Machines (SVMs) (Vapnik, 1995).
SVMclassifiers can handle very large feature spaces,and produce state-of-the-art results for NLP ap-plications (see e.g.
(Kudo and Matsumoto, 2000;Nivre et al, 2006)).
Alas, when trained on prunedfeature sets, in which rare lexical items are re-moved, SVM models suffer a loss in classifica-tion accuracy.
It would seem that rare lexicalitems are indeed crucial for SVM classificationperformance.
However, in Goldberg and Elhadad(2007), we suggested that the SVM learner is us-ing the rare lexical features for singling out hardcases rather than for learning meaningful general-izations.
We provide further evidence to supportthis claim in this paper.1142We show that by using a variant of SVM ?Anchored SVM Learning (Goldberg and Elhadad,2007) with a polynomial kernel, one can learnaccurate models for English NP-chunking (Mar-cus and Ramshaw, 1995), base-phrase chunking(CoNLL 2000), and Dutch Named Entity Recog-nition (CoNLL 2002), on a heavily pruned featurespace.
Our models make use of only a fractionof the lexical features available in the training set(less than 1%), and yet provide highly-competitiveaccuracies.For the Chunking and NP-Chunking tasks, themost heavily pruned experiments, in which weconsider only features appearing at least 100 timesin the training corpus, do show a small but signif-icant drop in accuracy on the testing corpus com-pared to the non-pruned models exposed to allavailable features in the training data.
We pro-vide detailed error analysis of a development setin Section 6, revealing the causes for these differ-ences.
We suggest one additional binary featurein order to account for some of the performancegap.
Moreover, we show that the differences inaccuracy vanish when the lexicalized and unlexi-calized models are tested on text from slightly dif-ferent sources than the training corpus (Section 7).This goes to show that with an appropriatelearning method, orthographic and structural (inthe form of POS tag sequences) information is suf-ficient for achieving state-of-the-art performanceon these kind of sequence labeling tasks.
Thisdoes not mean semantic information is not neededfor these tasks.
It does mean that current modelscapture only a tiny amount of such semantic in-formation through rare lexical features, and in amanner that does not generalize well.We believe this data motivates a different strat-egy to incorporate lexical features into classifica-tion models: instead of collecting the raw lexicalforms appearing in a training corpus, we should at-tempt to actively construct a feature space includ-ing lexical features derived from external sources.The feature representation of (Collobert and We-ston, 2008) could be a step in that direction.
Wealso believe that hard cases for sequence labeling(POS ambiguity, coordination, long syntactic con-structs) could be directly approached with special-ized classifiers.1.1 Related WorkThis work complements a similar line of resultsfrom the parsing literature.
While it was ini-tially believed that lexicalization of PCFG parsers(Collins, 1997; Charniak, 2000) is crucial forobtaining good parsing results, Gildea (2001)demonstrated that the lexicalized Model-1 parserof Collins (1997) does not benefit from bilexicalinformation when tested on a new text domain,and only marginally benefits from such informa-tion when tested on the same text domain as thetraining corpora.
This was followed by (Bikel,2004) who showed that bilexical-information isused in only 1.49% of the decisions in Collins?Model-2 parser, and that removing this informa-tion results in ?an exceedingly small drop in per-formance?.
However, uni-lexical information wasstill considered crucial.
Klein and Manning (2003)bridged the gap between lexicalized and unlexi-calized parsing performance, providing a compet-itive unlexicalized parsing model, relying on lex-ical information for only a few closed-class lex-ical items.
This was recently followed by (Mat-suzaki et al, 2005; Petrov et al, 2006) who intro-duce state-of-the-art nearly unlexicalized PCFGparsers.Similarly for discriminative dependency pars-ing, state-of-the-art parsers (McDonald, 2006;Nivre et al, 2006) are highly lexicalized.
How-ever, the model analysis in (McDonald, 2006)reveals that bilexical features hardly contributeto the performance of a discriminative MST-based dependency parser, while Kawahara andUchimoto (2007) demonstrate that minimally-lexicalized shift-reduce based dependency parserscan produce near state-of-the-art accuracy.In this work, we address the same question ofdetermining the impact of lexical features on a dif-ferent family of tasks: sequence labeling, as illus-trated by named entity recognition and chunking.As discussed above, all state-of-the-art publishedmethods rely on lexical features for such tasks(Zhang et al, 2001; Sha and Pereira, 2003; Finkelet al, 2005; Ratinov and Roth, 2009).
Sequencelabeling includes both a structural aspect (bracket-ing the chunks) and a tagging aspect (classifyingthe chunks).
While we expect the structural aspectcan benefit from techniques similar to those usedin the parsing literature, it is unclear whether thetagging component could perform well withoutdetailed lexical information.
We demonstrate inthis work that, indeed, lexical features are not nec-essary to obtain competitive performance.
Our ap-proach consists in performing a detailed analysis1143of the role played by rare lexical features in SVMmodels.
We distinguish the information brought tothe model by such features from the role they playin a specific learning method.2 Learning with Less FeaturesWe adopt the common feature representation inwhich each data-point is represented as a sparseD dimensional binary-valued vector f .
Each ofthe D possible features fiis an indicator func-tion.
The indicator functions look at properties ofthe current or neighbouring words.
An exampleof such function fiis 1 iff the previousword-form is DOG, 0 otherwise.
Thelexical (word-form) features result in extremelyhigh-dimensional (yet very sparse) feature vectors?
each word-form in the vocabulary of the trainingset correspond to (at-least) one indicator function.Due to the Zipfian distribution of language data,many of the lexical features are very rare, and ap-pear only a couple times in the training set.
Ide-ally, we would like our classifiers to learn onlyfrom robust features: consider only features thatappear at least k times in the training data (rare-feature pruning).
These features are more likely toappear in unseen test data, and thus such featurescan support more robust generalization.However, we find empirically that performingsuch feature pruning prior to learning SVM mod-els hurts the performance of the learned models.Our intuition is that this sensitivity to rare lexi-cal features is not explained by the richness of in-formation such rare features bring to the model.Instead, we believe that rare lexical features helpthe classifier because they make the data artifi-cially more separable.
To demonstrate this claim,we experiment with anchored SVM, which intro-duces artificial mechanical anchors into the modelto achieve separability, and make rare lexical fea-tures unnecessary.3 Learning MethodSVM are discriminative, max-margin, linear clas-sifiers (Vapnik, 1995), which can be kernelized.For the formulation of SVMs in the context ofNLP applications, see (Kudo and Matsumoto,2001).
SVMs with a polynomial kernel of degree2 were shown to provide state-of-the-art perfor-mance in many NLP application, see for example(Kudo and Matsumoto, 2000; Nivre et al, 2006;Isozaki and Kazawa, 2002; Goldberg et al, 2006).SVMs cope with inseparable data by introduc-ing a soft-margin ?
allowing some of the traininginstances to be classified incorrectly subject to apenalty, controlled by a parameter C.Anchored SVM As we show in Section 5, thesoft-margin heuristic performs sub-optimally forNLP tasks when the data is inseparable.
We use in-stead the Anchored Learning heuristic, introducedin (Goldberg and Elhadad, 2007).
The idea behindanchored learning is that some training instancesare inherently ambiguous.
This ambiguity stemsfrom ambiguity in language structure, which can-not be resolved with a given feature representa-tion.
When a data-point cannot be classified, itmight be due to missing information, which is notavailable in the data representation.
Instead of al-lowing ambiguous items to be misclassified duringtraining, we make the training data artificially sep-arable.
This is achieved by adding a unique featureto each training example (an anchor).
These an-chor features cause each data-point to be slightlymore similar to itself than to any other data point.At test time, we remove anchor features.In terms of kernel-based learning, anchored learn-ing can be achieved by redefining the dot productbetween two vectors to take into account the iden-tity of the vectors: xi?ancxj= xi?
xj+ ?ij.The classifier learned over the anchored datatakes into account the fine interactions betweenthe various inseparable data points.
In our ex-periments, SVM models over anchored data havemany more support vectors than soft-margin SVMmodels.
However, the anchored models generalizemuch better when less features are available.Relation to L2 SVM The classic soft-marginSVM formulation uses L1-penalty for misclassi-fied instances.
Specifically, the objective of thelearner is to minimize12||w||2+ C?i?isubjectto some margin constraints, where w is a weightvector to be learned and ?iis the misclassificationerror for instance i.
This is equivalent to maximiz-ing the dual problem:?Mi=1?i?12?i,j?i?jyiyjK(xi, xj)Another variant is L2-penalty SVM (Koshibaand Abe, 2003), in which there is a quadraticpenalty for misclassified instances.Here, the learning objective is to minimize:12||w||2+12C?i?2ior alternatively maximize thedual:?i?i?12?i,j?i?jyiyj(K(xi, xj) +?ijC).Interestingly, for the linear kernel, SVM-1144anchoring reduces to L2-SVM with C=1.
How-ever, for the case of non-linear kernels, anchoredand L2-SVM produce different results, as the an-choring is applied prior to the kernel expansion.Specifically for the case of the second-degreepolynomial kernel, L2-SVM aims to maximize:?i?i?12?i,j?i?jyiyj((xi?
xj+ 1)2+?ijC),while the anchored-SVM variant would maxi-mizes:?i?i?12?i,j?i?jyiyj(xi?xj+?ij+1)2.In our experiments, as discussed in Section5.4, we find that anchored-SVM and soft-marginSVM with tuned C value both reach good re-sults when we reduce the amount of lexical fea-tures.
Anchored-SVM, however, does not requirefine-tuning of the error-parameter C since it in-sures separability.
As a result, we learn anchored-SVM models quickly (few hours) as opposed toseveral days per model for C-tuned soft-marginSVM.
Anchored-SVMs also provide an easy ex-planation of the role of features in terms of sepa-rability.
Therefore, we use anchored-SVMs in ourexperiments as the learning method, but we expectthat other learning methods are capable of learningwith the same reduced feature sets.4 Experiment SetupHow important are the rare lexical features forlearning accurate NLP models?
To investigatethis question, we experiment with 3 different NLPsequence-labeling tasks.
For each task, we train asequence of polynomial kernel (d=2) SVM classi-fiers, using both soft-margin (C=1) and anchoredSVM.
Each classifier is trained on a pruned fea-ture set, in which only features appearing at leastk times in the training data are kept.
We vary thepruning parameter k. Pruning is performed overall the features in the model, but lexical featuresare most affected by it.For all the models, we use the B-I-O represen-tation, and perform multiclass classification usingpairwise-voting.
For our features, we considerproperties of tokens in a 5-token window centeredaround the token to be classified, as well as thetwo previous classifier predictions.
Results are re-ported as F-measure over labeled identified spans.Polynomial vs.
Linear models The polynomialkernel of degree 2 allows us to efficiently and im-plicitly include in our models all feature pairs.Syntactic structure information as captured bypairs of POS-tags and Word-POS pairs is certainlyimportant for such syntactic tasks as Chunkingand NER, as demonstrated by the many systemsdescribed in (Sang and Buchholz, 2000; TjongKim Sang, 2002).
By using the polynomial ker-nel, we can easily make use of this informationwithout intensive feature-tuning for the most suc-cessful feature pairs.L1-SVM, L2-SVM and the choice of the C pa-rameter Throughout our experiments, we use the?standard?
variant of SVM, L1-penalty soft mar-gin SVM, as implemented by the TinySVM1soft-ware package, with the default C value of 1.
Thissetting is shown to produce good results for se-quence labeling tasks in previous work (Kudo andMatsumoto, 2000), and is what most end-users ofSVM classifiers are likely to use.
As we showin Sect.5.4, fine-tuning the C parameter reachesbetter accuracy than L1-SVM with C=1.
How-ever, as this fine-tuning is computationally expen-sive, we first report the comparison L1-SVM/C=1vs.
anchored-SVM, which consistently reachedthe best results, and was the quickest to train.Feature Pruning vs.
Feature Selection Our aimin this set of experiments is not to find the optimalset of lexical features, but rather to demonstratethat most lexical items are not needed for accurateclassification in sequence labeling tasks.
To thisend, we perform very crude frequency based fea-ture pruning.
We believe better motivated featureselection technique taking into account linguistic(e.g.
prune only open-class words) or statistic in-formation could result in slightly more accuratemodels with even fewer lexical items.5 Experiments and Results5.1 Named Entity Recognition (NER)We use the Dutch data set from the CoNLL 2002shared task (Tjong Kim Sang, 2002).
The aim is toidentify named entities (persons, locations, orga-nizations and miscellaneous) in text.
The task hastwo stages: identification of the entities, and clas-sification of the identified entities into their corre-sponding types.
We focus here on the identifica-tion task.Features: We use the following properties foreach of the relevant tokens: word-form, POS,ORT, prefix1, prefix2, prefix3, suffix1, suffix2,suffix3.
The ORT feature can take one of the fol-lowing values: {number, contains-digit, contains-hyphen, capitalized, all-capitalized, URL, punctu-ation, regular}.1http://chasen.org/?taku/software/TinySVM/1145PRUNING #FEATURES SOFT-MARGIN ANCHORED0 186,421 90.92 90.78100 5,804 90.73 90.751000 1,207 88.56 90.101500 821 85.92 89.29Table 1: Named Entity Identification results (F-score) on dev set, with various pruning thresholds.Results are presented in Table 1.
Without fea-ture pruning, we achieve an F-score of 90.9.
Thisdataset proved to be quite resilient to feature prun-ing.
Pruning features appearing less than 100times results in just a slight decrease in F-score.Extremely aggressive pruning, keeping only fea-tures appearing more than 1,000 or 1,500 times inthe training data, results in a big drop in F-scorefor the soft-margin SVM (from about 91 to 86).Much less so for the Anchored-SVM.
Using An-chored SVM we achieve an F-score of 90.1 afterpruning with k = 1, 000.
This model has 1207 ac-tive features, and 27 unique active lexical forms.5.2 NP ChunkingThe goal of this task (Marcus and Ramshaw, 1995)is the identification of non-recursive NPs.
We usethe data from the CoNLL 2000 shared task: NPchunks are extracted from Sections 15-18 (train)and 20 (test) of the Penn WSJ corpus.
POS taggedare automatically assigned by the Brill Tagger.Features: We consider the POS and word-form ofeach token.PRUNING #FEATURES SOFT-MARGIN ANCHORED0 92,805 94.12 94.081 46,527 93.78 94.092 32,583 93.58 94.005 18,092 93.42 94.0110 10,812 93.00 93.9820 5,952 92.48 93.9250 2,436 92.33 93.96100 1,168 91.94 93.83Table 2: NP-Chunking results (F-score), with var-ious pruning thresholds.Results are presented in Table 2.
Without fea-ture pruning (k = 0), the soft-margin SVM per-forms slightly better than the Anchored-SVM.
Ei-ther of the results are state-of-the-art for this task.However, even modest pruning (k = 2) hurtsthe soft-margin model significantly.
Not so forthe anchored-SVM.
Even with relatively aggres-sive pruning (k = 100), the anchored model stillachieves an impressive F-score of 93.83.
Remark-ably, in that last model, there are only 1,168 activefeatures, and only 209 unique active lexical forms.5.3 ChunkingThe goal of the Chunking task (Sang and Buch-holz, 2000) is the identification of an assortmentof linguistic base-phrases.
We use the data fromthe CoNLL 2000 shared task.Features: We perform two experiments.
In thefirst experiment, we consider the POS and word-form of each token.
In this setting, feature pruningresulted in a bigger loss in performance than inthe two previous tasks.
Preliminary error analysisrevealed that many errors are due to tagger errors,especially of the present participle forms.
This ledus to the second experiment, in which we added asfeatures the 2- and 3- letter suffixes for the word tobe classified (but not for the surrounding words).Results are presented in Tables 3 and 4.
In thefirst experiment (POS + Word), the non-prunedsoft-margin model is the same system as the top-performing system in the original shared task,and yields state-of-the-art results.
Unlike the NP-chunking case, here feature pruning has a rela-tively large impact on the results even for the an-chored models.
However, the anchored modelsare still far more robust than the soft-margin ones.With k = 100 pruning, the soft-margin model suf-fers a drop of 2.5 F points, while the anchoredmodel suffers a drop of only 0.84 F points.
Evenafter this drop, the anchored k = 100 model stillperforms above the top-third system in the CoNLL2000 shared task.
This anchored k = 100 modelhas 1,180 active features, and only 209 unique ac-tive lexical features.The second experiment (POS + word-form +suffixes for main word) adds crude morphologicalinformation to the learner, helping it to avoid com-mon tagger mistakes.
This additional informationis helpful: pruning with k = 100 leads to an ac-curate anchored model (93.12 F) with only 209unique lexical items.
Note that with the additionof the suffix features, the pruned model k = 20beats the purely lexical model (no suffix features)with no pruning (93.51 vs. 93.44) with 10 timesless features.
When we combine suffixes and alllexical forms, we still see a slight advantage tothe lexical model (93.73 vs. 93.12 with pruningat k = 100).Even less lexicalization How robust are the suf-fixes?
We performed a third experiment, in which1146PRUNING #FEATURES SOFT-MARGIN ANCHORED0 92,837 93.44 93.401 46,557 93.20 93.322 32,614 93.10 93.315 18,126 92.89 93.2910 10,834 92.73 93.2320 5,975 92.18 93.1650 2,463 91.80 92.89100 1,180 90.94 92.56Table 3: Chunking results (F), with various prun-ing thresholds.
Experiment 1.
Features: POS,Word.PRUNING #FEATURES SOFT-MARGIN ANCHORED0 104,304 93.73 93.691 72,228 93.56 93.682 57,578 93.50 93.645 37,210 93.35 93.6210 23,968 93.26 93.5620 14,060 92.84 93.5150 6,326 92.28 93.37100 3,340 91.83 93.12Table 4: Chunking results (F), with various prun-ing thresholds.
Experiment 2.
Features: POS,Word, {Suff2, Suff3} of main Word.we replaced any explicit word-forms by 2- and 3-letter suffixes.
This gives us the complete wordform of many function words, and a reasonableamount of morphological marking.
Results arepresented in Table 5.
Surprisingly, this infor-mation proves to be quite robust.
Without fea-ture pruning, both the anchored and soft-marginmodel achieve near state-of-the-art performanceof 93.25F.
Pruning with k = 100 hurts the re-sult of the soft-margin model, but the anchoredmodel remains robust with an F-score of 93.18.This last model has 2,563 active features.
Withfurther pruning (k = 250), the result of the an-chored model drops to 92.87F (still 3rd place inthe CoNLL shared task), with only 1,508 activefeatures in the model.5.4 Fine-tuned soft-margin SVMsFor the sake of completeness, and to serve as a bet-ter comparison to the soft-margin SVM, we reportresults of some experiments with both L1 and L2SVMs, with tuned C values.
NP-chunking perfor-mance with tuned C values and various pruningthresholds is presented in Table 6.For these results, the C parameter was tunedon a development set using Brent?s 1-dimensionminimization method (Brent, 1973).
While tak-ing about 40 hours of computation to fit, the fi-PRUNING #FEATURES SOFT-MARGIN ANCHORED0 19,910 93.25 93.23100 2,563 92.87 93.18250 1,508 92.40 92.87Table 5: Chunking results (F), with various prun-ing thresholds.
Experiment 3.
Features: POS ,Suff2, Suff3 .K L1 (C) L2 (C) ANCHORED0 94.12 (1.0001) 94.09 (2.6128) 94.0850 93.79 (0.0524) 93.71 (0.0082) 93.96100 93.72 (0.0567) 93.59 (0.0072) 93.83Table 6: NP-Chunking results (F), with variouspruning thresholds K, for L1 and L2 SVMs withtuned C valuesnal results catch up with those of the anchored-SVM but still remain slightly lower.
This furtherhighlights our main point: accurate models canbe achieved also with mostly unlexicalized mod-els, and the lexical features do not contribute sub-stantial semantic information, but rather affect theseparability of the data.
This is nicely demon-strated by SVM-anchoring, in which lexical infor-mation is practically replaced by artificial seman-tically void indexes, but similar performance canalso be achieved by fine-tuning other learning pa-rameters.6 Error AnalysisOur experiments so far indicate that very aggres-sive feature pruning hurts performance slightly (byabout 0.5F point).
The feature-pruned models arestill accurate, indicating that lexical features con-tribute little to the classification accuracy.
We nowinvestigate the differences between the lexicalizedand pruned models, in order to characterize thekind of information that is available to the lexi-calized models but missing from the pruned ones.In the next section, we also verify that pruned-models are more stable than the fully lexicalizedones when tested over different text genres and do-mains.We focus our analysis on the chunking task, whichis a superset of the NP-chunking task.
We comparethe fully lexicalized soft-margin SVM model withthe POS+suffix2+suffix3 anchored-SVM modelwith k = 100 pruning.
We analyze the mod-els?
respective performance on section 05 of theWSJ corpus.
This dataset is different than the of-ficial test set.
It is, however, part of the same an-notated corpus as both the training and test sets.On this dataset, the fully lexicalized SVM model1147achieves an F-score of 93.24, vs. 92.59 for thesuffix-based pruned anchored-SVM model.
(Thepruned anchored-SVM model (k = 100) from ex-periment 2, achieve a slightly higher F-score of92.84)We investigate only those chunks which areidentified correctly by one model but not by theother.
Overall, there are 440 chunks (363 unique)which are identified correctly only by the lexical-ized model, and 258 chunks (232 unique) only bythe pruned model.Where the pruned model is always wrongSome errors are unique to the pruned model.Over 45 of the cases that are identified correctlyonly in the lexicalized model (more than 10%) aredue to the words ?including?
(18 cases) and ?If?
(9 cases), as well as other -ing forms such as ?fol-lowing?, ?according?, ?rising?
and ?suspecting.
?The word ?including?
appears 80 times in thetraining data, always tagged as VBG and func-tioning as a PP chunk, which is an odd chunkfor VBGs.
The lexicalized model easily pickedup on this behaviour, while the pruned modelcouldn?t.
Similarly, the word ?following/VBG?appears 32 times, 20 of which as PP, and theword ?according/VBG?
53 times, all of them asPP.
The pruned model could not distinguish thosefrom the rest of the VBGs and tagged them asVPs.
What seems to happen in these cases, isthat certain verbal forms participate in idiomaticconstructions and behave syntactically as preposi-tions.
The POS tagger does not pick this ambigu-ity in function and contributes only the most likelytag for the words (VBG).
Lexical models learn thatcertain VBGs are ?becoming?
prepositions in theobserved dataset.
These words do not appear asspecific features in the pruned models, and hencethese usage shifts are often misclassified.
Interest-ingly, the pruned model did learn that verbal formscan sometimes be PPs: it made use of that infor-mation by mis-identifying 11 verbal VBGs and 6verbal VBNs as PPs.The word ?If/IN?, unlike most prepositions, italways starts an SBAR rather than a PP chunk inthe corpus.
The pruned model learned this be-haviour correctly for the lower-cased ?if/IN?, butmissed the upper-cased version appearing in 79sentence initial locations in the corpus.These cases are caused by a mismatch betweenthe POS tag and the syntactic function observed inthe chunked dataset.Additional cases include the adverbs (Already,Nearby, Soon, Maybe, Perhaps, once, Then): theyare sometimes not chunked as ADVP but are leftoutside of any chunk.
Some one-word ADJPchunks being chunked as NPs (short, general, sure,worse, .
.
. )
(6 cases) and some are chunked asADVPs (hard, British-born, .
.
. )
(4 cases).There are 10 cases where the pruned modelsplits an NP into ADVP and NP, such as:[later] [this week], [roughly][18 more U.S. stores].
Inaddition, the pruned model failed to learn the con-struction ?typical of?, resulting in 2 NP chunkssuch as: [The more intuitive approach typical].Some mistakes of the pruned model seemlike mistakes/pecularities of the annotated corpus,which the lexicalized model found a way to workaround.
Consider the following gold-standardcases from the annotated corpus:- [ VP seems ] [ ADVP rarely ] [ VP to cut ]- [ ADVP just ] [ PP after ]- [ VP is ] [ NP anything ] [ O but ] [ VP fixing ]- [ ADJP as high ] [ PP as ] [ NP 8.3 % ]- [ ADJP less ] [ PP than ] [ ADJP rosy ]- [ NP 40 % ] [ PP to ] [ NP 45 % ]Which were each identified as a single chunk bythe pruned model.
It can be argued these are mis-takes in the tagged dataset.Where the lexical model is sometimes betterBoth models fail on conjunctions, but the lexical-ized model do slightly better.
Conjunction errortypes come in two main varieties, either chunking[x][and][y] instead of [x and y] (pruned: 21 cases,lex: 14 cases) or chunking [x and y] instead of[x][and][y] (pruned: 26 cases, lex: 24 cases).Joining VP and NP into an NP, dueto a verb/adj ambiguity.
For exam-ple chunking [NP fired six executives] in-stead of [VP fired] [NP six executives],or [NP keeping viewers] instead of[VP keeping] [NP viewers].
12 such cases areresolved correctly only by the lexicalized model,and 5 only by the pruned one.SBAR/PP confusion for words such as:?as?,?after?,?with?,?since?
(both ways).
13 casesfor the pruned model, 6 for lexicalized one.Where both model are similarMerging back-to-back NPs: Both models tendto erroneously join back-to-back NPs to a sin-gle NP, e.g.
: [NP Westinghouse this year], or[NP themselves fashion enterprises].
No model is bet-ter than the other on these cases, each model failed1148on 16 cases the other model succeeded on.Joining NP and VP into an NP due toVerb/Noun ambiguity and tagger mistakes:- [NP the weekend] [VP making] ?
[NP the weekend making]- [NP the competition] [VP can] ?
[NP the competition can](lexicalized: 6 errors, pruned: 8 errors)And splitting some NPs to VP+NP due to thesame reasons:- [VP operating] [NP profit]- [VP improved] [NP average yield](lexicalized: 5 errors, pruned: 7 errors)The word ?that?
is confused between SBAR andNP (5 mistakes for each model)Erroneously splitting range NPs, e.g.
:- [about $115][to][$125] (2 cases for each model).Where the pruned model is betterThere are some cases where the pruned models isdoing better than the lexicalized one:VP wrongly split into VP and ADJP:- [remains] [banned]4 mistakes for lexicalized, 1 for prunedVP wrongly split into VP and VP:- [were scheduled] [to meet]- [used] [to complain]3 mistakes for lexicalized, 1 for prunedVP wrongly split into ADVP and VP:- [largly][reflecting]- [selectively][leaking]6 mistakes for lexicalized, 1 for prunedPP and SBAR confusion:- of, with, As, after9 mistakes for lex, 5 for prunedVP chunked as NP due to tagger mistake:- [NP ruling], [NP drives], [NP cuts]6 mistakes for lex, 2 for pruned?that?
tagged as NP instead of SBAR:2 mistakes for lex, 0 for prunedTo concludeBoth the pruned and the fully lexicalized modelshave problems dealing with non-local phenomenasuch as coordination and relative clauses, as wellas verb/adjective ambiguities and VBG/Noun am-biguities.
They also perform poorly on embededsyntactic constructions (such as an NP containingan ADJP), and on identification of back-to-backNPs, which often requires semantic knowledge.Both models suffer from tagging mistakes of theunderlying tagger and systematic ambiguity be-tween the morphological tag assigned by the tag-ger and the syntactic tag in which the word oper-ates (e.g., ?including?
used as a preposition).The main advantage of the fully lexcializedmodel is in dealing with:?
Some coordinated constructions.?
Some cases of verb/adjective ambiguities.?
Specific function words not seen much intraining.?
Idiomatic usages of some VBG/VBN formsfunctioning as prepositions.The first two items are semantic in nature, and hintthat lexical features do capture some semantic in-formation.
While this might be true on the spe-cific corpus, we believe that such corpus-derivedsemantic knowledge is very restricted, is not gen-eralizable, and will not transfer well to other cor-pora, even on the same genre.
We provide evi-dence for this claim in Section 7.The last two items are syntactic.
We addressthem by introducing a slightly modified featuremodel.6.1 Another chunking ExperimentBased on the observations from the error analy-sis, we performed another pruned-chunking exper-iment, with the following features:?
Word and POS for a -2,+2 window aroundthe current token, and 2-and-3-letter suffixesof the token to be classified (same as Experi-ment 2 in Section 5.2 above).?
Features of words appearing as a preposi-tion (IN) anywhere in the training set arenot pruned (this result in a model with 310unique lexical items after k = 100 pruning).?
An additional binary feature indicating foreach token whether it can function as a PP.The list of possible-PP forms is generated byconsidering all tokens seen inside a PP in thetraining corpus.
It can be easily extended ifadditional lexicographic resources are avail-able, without retraining the model.This last proposed feature incorporates importantlexical knowledge without relying on features forspecific lexical forms, and is more generalizable.The accuracy of this new model on the develop-ment and test set with various pruning thresholdsis presented in Table 7.The addition of the CanBePrep feature im-proves the fully-lexicalized model accuracy on thedevelopment set (93.24 to 93.68), and does not af-fect fully lexicalized result on the test set (93.711149CORPUS SOURCE CONTENT #TOKENSWSJ 4 articles from wsj.com business Magazine, business 2,671Jaguar Wikipedia page on Jaguar Well edited text, animals 5,396FreeWill Wikipedia page on Free Will Well edited text, philosophy 9,428LJ-Life 4 LiveJournal posts Noisy teenage writing, life 870Table 8: Corpus Variation Text SourcesPRUNING #FEATURES SOFT-MARGIN ANCHOREDDev Set0 92,989 93.71 ?100 4,066 ?
93.22Test Set0 92,989 93.68 ?100 4,066 ?
93.26Table 7: Chunking results (F), with various prun-ing thresholds.
Experiment 4.
Features: POS,Word , Suff2, Suff3 for main word, CanBePrep .vs.
93.73).
The pruned model performance im-proves in both cases, more so on the developmentset (93.12 to 93.22 on the test set, 92.84 to 93.26on the development set).
The new model helpsbridging the gap between the fully lexicalized andthe pruned model, yet we still observe a lead of0.4F for the fully lexicalized model.
We now turnto explore how meaningful this difference is inreal-world situation in which one does not operateon the Penn-WSJ corpus.7 Corpus Variation and ModelPerformanceWhen tested on the exact same resource as themodels are trained on, the fully lexicalized modelstill has a slight edge over the pruned ones.
Howwell does this lexical knowledge transfer to dif-ferent text genres?
We compare the models?
per-formance on text from various genres, rangingfrom very similar to the training material (re-cent articles from the WSJ Business section) to awell-edited but different domain text (?Featured-content?
wikipedia pages) to a non-edited noisytext (live-journal blog posts from the ?life?
cate-gory).
As we do not have gold-annotated data forthese text genres, we analyze the few differencesbetween the models, manually inspecting the in-stances on which the models disagree.Table 8 describes our test corpora for this ex-periment.
We applied the fully-lexicalized andthe pruned (k = 100) anchored models describedin Section 6.1 to these texts, and compared thechunking results.
The results are presented in Ta-ble 9.When moving outside of the canonic trainingTEXT #DIFF PRUNED LEX BOTHCORRECT CORRECT WRONGWSJ 13 9 4 0Jaguar 45 20 20 7FreeWill 118 51 38 29LJ-Life 15 8 6 1Table 9: Comparison of Models?
performance ondifferent text genrescorpus, the fully lexicalized model have no advan-tage over the heavily pruned one.
On the contrary,the pruned models seem to have a small advantagein most cases (though it is hard to tell if the differ-ences are significant).
This is true even for textsin the very same domain, genre and editing guide-lines as the training corpus was derived from.8 DiscussionFor all the sequence labeling tasks we analyzed,the anchored-SVM proved to be robust to featurepruning.
The experiments support the claim thatrare lexical features do not provide substantial in-formation to the model, but instead play a role inmaintaining separability.
When this role is takenover by anchoring, we can obtain the same levelof performance with very few robust lexical fea-tures.
Yet, we cannot conclude that lexical infor-mation is not needed.
There is a significant differ-ence between the pruned and non-pruned modelsfor the chunking task.
We showed that this dif-ference can be bridged to some extent by a binaryfeature relating to idiomatic word usage, and thatthe difference vanishes when testing outside of theannotated corpus.
The high classification accura-cies achieved with the heavily pruned anchored-SVM models sheds new light on the actual roleof lexical features, and indicating that there is stilla lot to be learned regarding the effective incor-poration of lexical and semantic information intoour models.
It is our view that semantic knowl-edge should not be expected to be learned by in-spection of raw lexical counts from an annotatedtext corpus, but instead collected from sources ex-ternal to the annotated corpora ?
either based ona very large unannotated corpora, or on manuallyconstructed lexical resources.1150ReferencesDaniel M. Bikel.
2004.
Intricacies of collins?
parsingmodel.
Computational Linguistics, 30(4).Richard P. Brent, 1973.
Algorithms for Minimizationwithout Derivatives, chapter 4.
Prentice-Hall.Claire Cardie and David Pierce.
1998.
Error-drivenpruning of treebank grammars for base noun phraseidentification.
In ACL-1998.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proc of NAACL.Michael Collins.
1997.
Three generative, lexicalizedmodels for statistical parsing.
In Proc of EACL.Ronan Collobert and Jason Weston.
2008.
A unifiedarchitecture for natural language processing: Deepneural networks with multitask learning.
In Proc.
ofICML.Jenny Rose Finkel, Trond Grenager, and ChristopherManning.
2005.
Incorporating non-local informa-tion into information extraction systems by gibbssampling.
In Proc of ACL.Daniel Gildea.
2001.
Corpus variation and parser per-formance.
In Proc of EMNLP.Yoav Goldberg and Michael Elhadad.
2007.
SVMModel Tampering and Anchored Learning: A CaseStudy in Hebrew.
NP Chunking.
In ACL2007.Yoav Goldberg, Meni Adler, and Michael Elhadad.2006.
Noun Phrase Chunking in Hebrew: Influenceof Lexical and Morphological Features.
In COL-ING/ACL2006.Hideki Isozaki and Hideto Kazawa.
2002.
EfficientSupport Vector Classifiers For Named Entity Recog-nition.
In COLING2002.Daisuke Kawahara and Kiyotaka Uchimoto.
2007.Miniamlly lexicalized dependency parsing.
In Procof ACL (Short papers).Dan Klein and Christopher D. Manning.
2003.
Accu-rate unlexicalized parsing.
In Proc.
of ACL.Yoshiaki Koshiba and Shigeo Abe.
2003.
Comparisonof L1 and L2 support vector machines.
In Proc.
ofthe International Joint Conference on Neural Net-works, volume 3.Taku Kudo and Yuji Matsumoto.
2000.
Use of Sup-port Vector Learning for Chunk Identification.
InCoNLL-2000.Taku Kudo and Yuji Matsumoto.
2001.
Chunking withsupport vector machines.
In NAACL ?01.Mitch P. Marcus and Lance A. Ramshaw.
1995.Text Chunking Using Transformation-Based Learn-ing.
In 3rd ACL Workshop on Very Large Corpora.Takuya Matsuzaki, Yusuke Miyao, and Jun?ichi Tsujii.2005.
Probabilistic cfg with latent annotations.
InProc of ACL.Ryan McDonald.
2006.
Discriminative Training andSpanning Tree Algorithms for Dependency Parsing.Ph.D.
thesis, University of Pennsylvania.Joakim Nivre, Johan Hall, and Jens Nillson.
2006.MaltParser: A Data-Driven Parser-Generator forDependency Parsing.
In LREC2006.Slav Petrov, Leon Barrett, Romain Thibaux, and DanKlein.
2006.
Learning accurate, compact, and inter-pretable tree annotation.
In Proc of ACL.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InProc of CONLL.Erik F. Tjong Kim Sang and S. Buchholz.
2000.
Intro-duction to the CoNLL-2000 shared task: chunking.In CoNLL-2000.Fei Sha and Fernando Pereira.
2003.
Shallow parsingwith conditional random fields.
In Proc of NAACL.Erik F. Tjong Kim Sang and Fien De Meulder.2003.
Introduction to the CoNLL-2003 SharedTask: Language-Independent Named Entity Recog-nition.
In CoNLL-2003.Erik F. Tjong Kim Sang.
2002.
Introduction to theCoNLL-2002 Shared Task: Language-IndependentNamed Entity Recognition.
In CoNLL-2002.Vladimir Vapnik.
1995.
The nature of statistical learn-ing theory.
Springer-Verlag New York, Inc.Tong Zhang, Fred Damerau, and David Johnson.
2001.Text chunking using regularized winnow.
In Proc ofACL.1151
