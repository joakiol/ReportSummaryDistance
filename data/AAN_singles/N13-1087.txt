Proceedings of NAACL-HLT 2013, pages 727?732,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsSemi-Supervised Discriminative Language Modelingwith Out-of-Domain Text DataArda C?elebi1 and Murat Sarac?lar21Department of Computer Engineering2Department of Electrical and Electronics EngineeringBog?azic?i University, Istanbul, Turkey{arda.celebi, murat.saraclar}@boun.edu.trAbstractOne way to improve the accuracy of auto-matic speech recognition (ASR) is to use dis-criminative language modeling (DLM), whichenhances discrimination by learning wherethe ASR hypotheses deviate from the utteredsentences.
However, DLM requires largeamounts of ASR output to train.
Instead,we can simulate the output of an ASR sys-tem, in which case the training becomes semi-supervised.
The advantage of using simu-lated hypotheses is that we can generate asmany hypotheses as we want provided that wehave enough text material.
In typical scenar-ios, transcribed in-domain data is limited butlarge amounts of out-of-domain (OOD) datais available.
In this study, we investigate howsemi-supervised training performs with OODdata.
We find out that OOD data can yield im-provements comparable to in-domain data.1 IntroductionDiscriminative language modeling (DLM) helpsASR systems to discriminate between acousticallysimilar word sequences in the process of choos-ing the most accurate transcription of an utterance.DLM characterizes and learns from ASR errors bycomparing the reference transcription of the utter-ance and the candidate hypotheses generated by theASR system.
Although previous studies based onthis supervised setting have been successful (Roarket al 2007; Ar?soy et al 2009; Ar?soy et al 2012;Sak et al 2012), they require large amounts of tran-scribed speech data and a well-trained in-domainASR system, both of which are hard to obtain.
Toovercome this difficulty, instead of training with thereal ASR output, we can use simulated output, inwhich case the training becomes semi-supervised.Semi-supervised training for discriminative lan-guage modeling has been shown to achieve as goodword error rate (WER) reduction as the training donewith real ASR output (Sagae et al 2012; C?elebi etal., 2012).
In this approach, first a confusion model(CM) is estimated from supervised data.
This CMcontains all seen confusions and their occurrenceprobabilities in hypotheses generated by an ASRsystem.
Then, the CM is used to generate a num-ber of alternative-but-incorrect hypotheses, or simu-lated hypotheses, for a given sentence.
Since the CMcharacterizes the errors that the ASR system makes,simulated hypotheses carry these characteristics.
Atthe end, the DLM is trained on the reference sen-tences and their simulated hypotheses.
Although be-ing able to simulate the output of the ASR systemallows us to generate as much output as we needfor the DLM training, there is not always enoughtext data that is in the same domain as the ASR sys-tem.
Yet, it is easier to find large amounts of out-of-domain (OOD) text data.
In this study, we extend theprevious studies where in-domain text data was usedfor hypothesis simulation.
Instead of using limitedin-domain data, we experiment with larger amountsof OOD data for hypothesis simulation.The rest of the paper is organized as follows.
InSection 2, we summarize the related work.
In Sec-tion 3, we explain the methods to simulate the hy-potheses and to train the DLM.
We give the exper-imental results in Section 4 before concluding withSection 5.7272 Related WorkThe earliest work on hypothesis simulation for DLMwas done by Kurata et al(2009; 2012).
They gen-erate the probable n-best lists that an ASR systemmay output for a hypothetical input utterance givena word sequence.
In another study, Tan et al(2010)propose a system for channel modeling of ASRfor simulating the ASR corruption using a phrase-based machine translation system trained betweenthe reference and output phoneme sequences froma phoneme recognizer.
Jyothi and Fosler-Lussier(2010) also model the phonetic confusions using aconfusion matrix that takes into account word-basedphone confusion log likelihoods and distances be-tween the phonetic acoustic models.
This modelis then used to generate confusable word graphsfor training a DLM using the perceptron algorithm.Xu et al(2009) propose the concept of cohortsand report significant WER improvement for self-supervised DLM.
Similarly, Sagae et al(2012) usephrasal cohorts to simulate ASR output and the per-ceptron algorithm for training.
They observe half ofthe WER reduction that the fully supervised meth-ods achieve.
In another parallel study, C?elebi et al(2012) work on a Turkish ASR system and considervarious confusion models at four different granular-ities (word, morph, syllable, and phone) and differ-ent sampling methods to choose from a large list ofsimulated hypotheses.
They observe that the strat-egy that matches the word error (WE) distributionof the simulated hypotheses to the WE distributionof the ASR outputs yields the best WER reduction.While the previous studies use in-domain datasets for simulation, it is quite common to collectlarge amounts of OOD text data from the web.
How-ever, given the nature of web data, some kind of se-lection mechanism is needed to ensure quality.
Bu-lyko et al(2007) use perplexity-based filtering toselect a relevant subset from vast amounts of webdata in order to increase the training data of the gen-erative LM used by the ASR system.
There are alsostudies that use a relative-entropy based selectionmechanism in order to match the n-gram distributionof the selected data against the in-domain data bySethy et al(2006; 2009).
In this study, we considerthe perplexity-based selection method for a start.3 Method3.1 Sentence Selection from OOD DataIn order to select sentences from the OOD data,we use three methods in addition to random selec-tion.
We calculate the perplexity of each sentencewith SRILM toolkit, which gives normalized scoreswith respect to the length of the sentence.
Then,we order sentences based on their perplexity scoresin increasing order.
Perplexity is calculated by aLM trained on in-domain data.
After ordering, thetop of the list contains those sentences that resem-ble the in-domain data the most whereas the sen-tences at the bottom resemble the in-domain data theleast.
We apply the three methods on this orderedlist of sentences.
The first two methods, TOP-N andBOTTOM-N , simply get the top and bottom N sen-tences, respectively.
The third method, RC-NxM ,picks uniformly separated N clusters of M consec-utive sentences, while making sure that top and bot-tom M sentences are among the selected ones.3.2 Hypothesis SimulationSemi-supervised DLM training uses artificially gen-erated hypotheses which mimic the ASR systemoutput.
To generate the hypotheses, we followthe three-step finite state transducer based pipelinegiven in C?elebi et al(2012) and summarized by thefollowing composition sequence:sample(N -best(prune(W?LW?CM)?LM?1?GM))In the first step of the pipeline, we use the confusionmodel transducer (CM) to generate all possible con-fusions that the ASR system can make for a givenreference sentenceW .
We consider syllable, morphand word based confusion models, and convert Wto these units using the lexicon LW .
The generatedalternatives are pruned for efficiency reasons.As the output of the first step may include manyimplausible sequences, the second step convertsthem to morphs using LM?1 and reweights themwith a morph-based language model GM to favorthe meaningful sequences.
For this, we use three ap-proaches.
The first approach is to use the LM that isused by the ASR system, called GEN-LM.
The sec-ond LM called ASR-LM is trained from the outputof the ASR system, whereas the third approach isnot to use any language model, denoted by NO-LM,728in which case we just use the scores coming fromthe confusion model in the first step.
A large list ofof N -best (N = 1000) hypotheses are produced atthis stage.The third step, called sampling, involves picking asubset of the hypotheses from a larger set with broadvariety.
This step is done in order to pick samples soas to make sure that they include error variety in-stead of just high scoring hypotheses.
As done byC?elebi et al(2012), we use four sampling meth-ods to pick 50 hypotheses out of the highest scoring1000 hypotheses.
The simplest of them is Top50,where we select the highest scoring 50 hypotheses.Another method is Uniform Sampling (US) whichselects instances from the WER-ordered list in uni-form intervals.
Third method, called RC5x10, forms5 clusters separated uniformly, each containing 10hypotheses.
Lastly, ASRdist-50 selects 50 hypothe-ses in such a way that the WE distribution of selectedhypotheses resembles the WE distribution of the realASR output as much as it can.
We accomplish thisby filling the WE bins with the hypotheses havingrequired number of WEs.3.3 DLM EstimationThe training of the DLM involves representing thetraining data as feature vectors and processing viaa discriminative learning algorithm.
We representthe simulated N -best lists using unigram features asdescribed by Dikici et al(2012).
As the learningalgorithm, we apply the WER-sensitive perceptronalgorithm proposed by Sak et al(2011b), which hasbeen shown to perform better for reranking ASR hy-potheses as it minimizes an objective function basedon the WER rather than the number of misclassifi-cations.4 Experiments4.1 Experimental SetupWe employ DLM on a Turkish broadcast news tran-scription data set (Ar?soy et al 2009), which com-prises disjoint training (105356 sentences), held-out(1947 sentences) and test (1784 sentences) subsetsconsisting of ASR outputs represented as N -bestlists.
We use Morfessor (Creutz and Lagus, 2005)to obtain the morph level word segmentations fromwhich we build the LMs.
For semi-supervised ex-periments, we use the first half of the training sub-set (t1: 53992 sentences, 965K morphs) to learnthe confusion models, and the reference transcrip-tions of the second half (t2: 51364 sentences, 935Kmorphs) to generate in-domain simulated n-best liststo be compared against OOD simulated ones.
Forthis setup, the generative baseline WER and oracleWER on the held-out set are 22.9% and 14.2% andon the test set are 22.4% and 13.9%, respectively.When we use ASR 50-best from t1 for DLM train-ing, WERs drop to 22.2% and 21.8% on the held-outand the test sets, respectively.For OOD data, we use a data set of 10.8Msentences (140M morphs) from newspaper articlesdownloaded from the Internet (Sak et al 2011a).To calculate the perplexity of OOD sentences for se-lection, we use a language model trained over thereference transcripts and 50-best lists of t1 and t2.4.2 Results on Out-of-Domain DataWe start our experiments with 500K randomly se-lected OOD sentences, or RAND-500K.
We runthe simulation pipeline with four sampling methods,three confusion and three language models, giving36 experiments in total.
We choose among the pro-posed sampling approaches and confusion modelsusing a rank-based comparison as done by Dikici etal.
(2012).We look at which sampling method performs thebest by first dividing experiments into 9 groups, eachhaving 4 results from all sampling methods.
Withineach group, we rank the sampling methods based onthe WER they achieve in increasing order and takethe average of assigned ranks.
ASRdist-50 gets thelowest average rank of 1.8, while RC5x10, US-50,and TOP-50 come after with the averages of 2.1, 2.4,and 3.4, respectively.
This shows that ASRdist-50gives the best WER reduction on OOD data, whichis also true for in-domain data (C?elebi et al 2012).Doing the same rank-based comparison for theCMs this time, we observe that the syllable andmorph-based models have the same average rank of1.5, whereas the word-based model has 2.8.
How-ever, a closer look reveals that the syllable-basedCM paired with NO-LM is an outlier because NO-LM approach allows variety at the output but whenthe unit of the confusion model is as small as syl-lables, it produces too much variety that deterio-729rates the discriminative model.
If we don?t considerthe ranks coming from NO-LM, the average rank ofsyllable- and morph-based models become 1.1 and1.8, respectively.
Thus, we use syllable-based mod-els over the others for the rest of the experiments.Knowing that the ASRdist-50 sampling methodand syllable-based CM together give the best re-sults for RAND-500K, we experiment with threemore sentence selection methods described in Sec-tion 3.1.
Table 1 shows all the results obtained fromfour 500K OOD data sets.OOD Data sets GEN-LM ASR-LM NO-LMTOP-500K 22.6 22.6 22.6BOTTOM-500K 22.4 22.2 22.5RAND-500K 22.2 22.5 22.6RC-5x100K 22.4 22.6 22.5Table 1: WER (%) on held-out set obtained with syllable-based CMs and ASRdist-50 sampling methodAccording to Table 1, the highest WER reduc-tion is achieved with BOTTOM-500K+ASR-LMand RAND-500K+GEN-LM combinations.
WhileASR-LM exceeds the other two LMs only in thecase of BOTTOM-500K, for other three OOD datasets GEN-LM gives the best results.
More interest-ingly, using OOD sentences resembling in-domaindata (or TOP-500K) is outperformed in all cases,especially by BOTTOM-500K.
To understand this,we look at the number of morphs in each data setgiven in Table 2.
Even though each OOD data sethas 500K sentences, BOTTOM-500K has the high-est number of morphs (?6.5M) and TOP-500K hadthe lowest (?3.5M), while the other two have around5.5M morphs.
We also look at the morph unigramdistribution (M) of all four data sets and calculat-ing the KL divergence KL(M || U)1 of each M touniform distribution (U).
We observe that the uni-gram morph distribution of the TOP-500K data setis the least uniform with KL distance of 6.6, whereasBOTTOM-500K has KL distance of 2.7 and theother two have KL distances of around 4.3.
Inother words, this shows that TOP-500K has the low-est content variation, especially when compared toBOTTOM-500K.
Note also the slightly high valueof KL distance for t2, which can be attributed to the1KL(M || U) =?i pilog(pi1/V ) = log(V ) ?
H(p), whereV = 61294 and H(p) is the entropy of p.relatively low number of unique morphs (types).Data set KLD Types Tokenst2 (50K) 4.65 22,107 935,137TOP-500K 6.63 20,689 3,519,012BOTTOM-500K 2.71 54,458 6,474,385RAND-500K 4.36 50,422 5,559,763RC-5x100K 4.35 50,561 5,343,342Table 2: KL distance, KL(M || U), between uniform dis-tribution (U) and unigram morph distribution (M); num-ber of unique morphs and tokens.4.3 Out-of-Domain vs In-Domain DataIn this section, we compare the results for in-domaindata with the results for four OOD data sets inTable 3.
In order to see how the size of OODdata set affects the WER reduction, we start with50K sentences and increase the size gradually upto 500K.
The first row of Table 3 shows the WERobtained with the in-domain data t2, containing ap-proximately 50K sentences.Data 50K 100K 200K 500Kt2 22.4 - - -TOP 22.8 22.7 22.7 22.6BOTTOM 22.6 22.4 22.3 22.2RAND 22.5 22.3 22.3 22.2RC-5 22.5 22.5 22.3 22.4Table 3: WER (%) on held-out set for in-domain(Syllable+ASR-LM+ASRdist-50) and four OOD datasets in increasing sizesAccording to Table 3, even though 50K OOD sen-tences yield worse results than the same amount ofin-domain sentences, as the size of OOD data setincreases, the amount of WER reduction increasesand surpasses the level obtained by using in-domaindata.
What is more interesting is that RAND outper-forms in-domain data starting from 100K, whereasBOTTOM starts at a higher WER but drops rela-tively fast, leveling with RAND starting at 200K.Note that the best WER achieved with the simulateddata matches the supervised DLM performance us-ing ASR 50-best from t1, reported in Section 4.1.Then we go one step further and expand the BOT-TOM data set to 1M sentences and we observe WERof 22.1% on the held-out set.
This further supports730the observation that the more OOD data we use, thelower WER we can achieve.As a side observation, when we calculate theWER of five 100K-blocks from the RAND-500Kset, we find that the standard deviation of WER is0.06%, which gives and idea about the significancelevel of the WER differences.4.4 Merging Real and Simulated HypothesesWe also evaluate whether merging simulated hy-potheses with real ASR hypotheses yields furtherWER reductions.
The result of merging the real hy-potheses from t1 with the simulated ones from in-domain and OOD data are shown in Table 4.
Thefirst row shows the WER of the combination withthe simulated hypotheses from in-domain data t2.Real Simulated WER (%)t1 t2 (50K) 22.0t1 TOP-500K 22.3t1 BOTTOM-500K 22.1t1 RAND-500K 22.0t1 RC-5x100K 22.1t1 BOTTOM-1M 21.9Table 4: WER (%) on held-out set obtained by mergingreal and simulated hypothesesWhen combined with the real hypotheses from t1,RAND500K achieves the same level of WER re-duction as the simulated hypotheses from t2 on theheldout set.
The results on the test set are also sim-ilar.
On the test set, the combination of the realhypotheses from t1 and the simulated hypothesesfrom t2 achieve 21.5% WER, whereas the WER is21.6% when the simulated hypotheses from t2 arereplaced by those from RAND500K.
This indicatesthat enough OOD data can replace the in-domaindata and yield similar performance, even in combi-nation with in-domain real data.Moreover, we further expand the OOD data to 1Mfor BOTTOM, however even though it reduces theWER on the heldout set, it achieves slightly higherWER on the test set (21.7%).Next, we combine the in-domain real hypothesesfrom t1, simulated hypotheses from t2 and simulatedones from the OOD data sets.
However, comparedto the combination of t1 and t2, adding extra 500KOOD hypotheses on top of those two gives similarWERs on the held-out set while WERs on the testset increases slightly.
From another point of view,adding in-domain simulated hypotheses from t2 ontop of real ones from t1 and 500K OOD data (rows2-5 in Table 4) provides slight WER improvementon the held-out set but not on the test set.5 ConclusionIn this study, we investigate whether we canachieve the same level of WER reduction for semi-supervised DLM with the large amounts of OODdata instead of in-domain data.
We observe thatASRdist-50 sampling method and syllable-basedCMs yield the best results with the OOD data.
More-over, selecting OOD sentences randomly rather thanusing perplexity-based methods is enough to achievethe best WER reduction.
We also observe that sim-ulated hypotheses from the OOD data is almost asgood as in-domain simulated hypotheses or even realones.
As a future work, we will increase the size ofthe OOD data and examine other methods like rela-tive entropy based OOD selection.AcknowledgmentsThis research is supported in part by TU?BI?TAK un-der the project number 109E142.ReferencesEbru Ar?soy, Dog?an Can, S?dd?ka Parlak, Has?im Sak,and Murat Sarac?lar.
2009.
Turkish broadcast newstranscription and retrieval.
IEEE Transactions on Au-dio, Speech, and Language Processing, 17(5):874?883, July.Ebru Ar?soy, Murat Sarac?lar, Brian Roark, and IzhakShafran.
2012.
Discriminative language modelingwith linguistic and statistically derived features.
IEEETransactions on Audio, Speech, and Language Pro-cessing, 20(2):540?550, February.Ivan Bulyko, Mari Ostendorf, Man-Hung Siu, Tim Ng,Andreas Stolcke, and O?zgu?r C?etin.
2007.
Webresources for language modeling in conversationalspeech recognition.
ACM Transactions on Speech andLanguage Processing, 5(1):1?25, December.Arda C?elebi, Has?im Sak, Erinc?
Dikici, Murat Sarac?lar,Maider Lehr, Emily T. Prud?hommeaux, Puyang Xu,Nathan Glenn, Damianos Karakos, Sanjeev Khudan-pur, Brian Roark, Kenji Sagae, Izhak Shafran, DanielBikel, Chris Callison-Burch, Yuan Cao, Keith Hall,731Eva Hasler, Philipp Koehn, Adam Lopez, Matt Post,and Darcey Riley.
2012.
Semi-supervised discrimi-native language modeling for Turkish ASR.
In Proc.ICASSP, pages 5025?5028.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using morfessor 1.0.
Technical re-port, Helsinki University of Technology.
Publicationsin Computer and Information Science Report A81.Erinc?
Dikici, Arda C?elebi, and Murat Sarac?lar.
2012.Performance comparison of training algorithms forsemi-supervised discriminative language modeling.
InProc.
Interspeech, Oregon, Portland, September.Preethi Jyothi and Eric Fosler-Lussier.
2010.
Discrimi-native language modeling using simulated ASR errors.In Proc.
Interspeech, pages 1049?1052.Gakuto Kurata, Nobuyasu Itoh, and MasafumiNishimura.
2009.
Acoustically discriminativetraining for language models.
In Proc.
ICASSP, pages4717?4720.Gakuto Kurata, Abhinav Sethy, Bhuvana Ramabhad-ran, Ariya Rastrow, Nobuyasu Itoh, and MasafumiNishimura.
2012.
Acoustically discriminative lan-guage model training with pseudo-hypothesis.
SpeechCommunication, 54(2):219?228.Brian Roark, Murat Sarac?lar, and Michael Collins.
2007.Discriminative n-gram language modeling.
ComputerSpeech and Language, 21(2):373?392, April.Kenji Sagae, Maider Lehr, Emily T. Prud?hommeaux,Puyang Xu, Nathan Glenn, Damianos Karakos, San-jeev Khudanpur, Brian Roark, Murat Sarac?lar, IzhakShafran, Daniel Bikel, Chris Callison-Burch, YuanCao, Keith Hall, Eva Hasler, Philipp Koehn, AdamLopez, Matt Post, and Darcey Riley.
2012.
Halluci-nated n-best lists for discriminative language model-ing.
In Proc.
ICASSP.Has?im Sak, Tunga Gu?ngo?r, and Murat Sarac?lar.
2011a.Resources for turkish morphological processing.
Lan-guage Resources and Evaluation, 45(2):249?261.Has?im Sak, Murat Sarac?lar, and Tunga Gu?ngo?r.
2011b.Discriminative reranking of ASR hypotheses withmorpholexical and n-best-list features.
In Proc.
ASRU,pages 202?207.Has?im Sak, Murat Sarac?lar, and Tunga Gu?ngo?r.
2012.Morpholexical and discriminative language models forTurkish automatic speech recognition.
IEEE Trans-actions on Audio, Speech, and Language Processing,20(8):2341?2351, October.Abhinav Sethy, Panayiotis G. Georgiou, and ShrikanthNarayanan.
2006.
Text data acquisition for domain-specific language models.
In EMNLP ?06 Proceedingsof the 2006 Conference on Empirical Methods in Nat-ural Language Processing, pages 382?389.Abhinav Sethy, Panayiotis G. Georgiou, Bhuvana Ram-abhadran, and Shrikanth Narayanan.
2009.
An itera-tive relative entropy minimization-based data selectionapproach for n-gram model adaptation.
IEEE Trans-actions on Audio, Speech and Language Processing,17(1):13?23, January.Qun Feng Tan, Kartik Audhkhasi, Panayiotis G. Geor-giou, Emil Ettelaie, and Shrikanth Narayanan.
2010.Automatic speech recognition system channel model-ing.
In Proc.
Interspeech, pages 2442?2445.Puyang Xu, Damianos Karakos, and Sanjeev Khudanpur.2009.
Self-supervised discriminative training of statis-tical language models.
In Proc.
ASRU, pages 317?322.732
