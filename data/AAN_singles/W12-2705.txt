NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages 37?40,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsUnsupervised Vocabulary Adaptation for Morph-based Language ModelsAndre?
Mansikkaniemi and Mikko KurimoAalto University School of ScienceDepartment of Information and Computer SciencePO BOX 15400, 00076 Aalto, Finland{andre.mansikkaniemi,mikko.kurimo}@aalto.fiAbstractModeling of foreign entity names is an im-portant unsolved problem in morpheme-basedmodeling that is common in morphologicallyrich languages.
In this paper we present anunsupervised vocabulary adaptation methodfor morph-based speech recognition.
Foreignword candidates are detected automaticallyfrom in-domain text through the use of lettern-gram perplexity.
Over-segmented foreignentity names are restored to their base forms inthe morph-segmented in-domain text for eas-ier and more reliable modeling and recogni-tion.
The adapted pronunciation rules are fi-nally generated with a trainable grapheme-to-phoneme converter.
In ASR performance theunsupervised method almost matches the abil-ity of supervised adaptation in correctly rec-ognizing foreign entity names.1 IntroductionForeign entity names (FENs) are difficult to rec-ognize correctly in automatic speech recognition(ASR).
Pronunciation rules that cover native wordsusually give incorrect pronunciation for foreignwords.
More often the foreign entity names encoun-tered in speech are out-of-vocabulary words, previ-ously unseen words not present in neither the lexiconnor background language model (LM).An in-domain LM trained on a smaller corpus re-lated to the topic of the speech, can be used to adaptthe background LM to give more suitable probabil-ities to rare or unseen foreign words.
Proper pro-nunciation rules for foreign entity names are neededto increase the probability of their correct recogni-tion.
These can either be obtained from a hand-madelexicon or by generating pronunciation rules auto-matically using for example a trainable grapheme-to-phoneme (G2P) converter.In morph-based speech recognition words aresegmented into sub-word units called morphemes.When using statistical morph-segmentation algo-rithms such as Morfessor (Creutz and Lagus, 2005)new foreign entity names encountered in in-domaintext corpora are often over-segmented (e.g.
mcdow-ell ?
mc do well).
To guarantee reliable pronuncia-tion modeling, it?s preferable to keep the lemma in-tact.
Restoring over-segmented foreign entity namesback in to their base forms is referred to as mor-pheme adaptation in this paper.This work describes an unsupervised approach tolanguage and pronunciation modeling of foreign en-tity names in morph-based speech recognition.
Wewill study an adaptation framework illustrated belowin Figure 1.In-domain textFind FENsStemmerMorphemeadaptationLM adaptationMorphsegmentationBackgroundcorpusAdapted LMFEN lexiconG2P converterAdaptedlexiconFigure 1: Adaptation framework.37The adaptation framework is centered around thefollowing automated steps: 1.
Find foreign words inadaptation texts, 2.
Convert foreign word candidatesinto their base forms, 3.
Generate pronunciationvariants for the retrieved foreign entity name can-didates using a G2P converter.
Additionally, to fa-cilitate easier and more reliable pronunciation adap-tation, the foreign entity names are restored to theirbase forms in the segmented in-domain text.The adaptation framework will be compared to asupervised method where the adaptation steps aredone manually.
The evaluation will be done onFinnish radio news segments.2 Methods2.1 Foreign Word DetectionUnsupervised detection of foreign words in texthas previously been implemented for English usingword n-ngram models (Ahmed, 2005).Finnish has a rich morphology and using word n-gram models or dictionaries for the detection of for-eign words would not be practical.
Many of the for-eign words occurring in written Finnish texts couldbe identified from unusual letter sequences that arenot common in native words.
A letter n-gram modeltrained on Finnish words could be used to identifyforeign words by calculating the average perplexityof the letter sequence in a word normalized by itslength.A two-step algorithm is implemented for the au-tomatic detection of foreign words.
First, all wordsstarting in uppercase letters in the unprocessed adap-tation text are held out as potential foreign entitynames.
The perplexity for each foreign word candi-date is calculated using a letter-ngram model trainedon Finnish words.
Words with the highest perplexityvalues are the most probable foreign entity names.
Apercentage threshold T for the top perplexity wordscan be determined from prior information.The most likely foreign words are fi-nally converted into their base forms usinga Finnish stemming algorithm (Snowball -http://snowball.tartarus.org/).2.2 Lexicon AdaptationFor Finnish ASR systems the pronunciation dictio-nary can easily be constructed for arbitrary wordsby mapping letters directly to phonemes.
Foreignnames are often pronounced according to their orig-inal languages, which can have more complicatedpronunciation rules.
These pronunciation rulescan either be manually added to a lookup dictio-nary or generated automatically with a grapheme-to-phoneme converter.
Constructing a foreign wordlexicon through manual input involves a lot of te-dious work and it will require a continuous effort tokeep it updated.In this work Sequitur G2P is used, a data-driven grapheme-to-phoneme converter based onjoint-sequence models (Bisani and Ney, 2008).
Apronunciation model is trained on a manually con-structed foreign word lexicon consisting of 2000 for-eign entity names with a manually given pronuncia-tion hand-picked from a Finnish newswire text col-lection.
The linguistic origins of the foreign wordsare mixed but Germanic and Slavic languages arethe most common.The pronunciation model is used to generate themost probable pronunciation variants for the foreignentity name candidates found in the adaptation text.2.3 Morpheme AdaptationIn current state of the art Finnish language model-ing words are segmented into sub-word units (mor-phemes) (Hirsima?ki et.
al, 2009).
This allows thesystem to cover a large number of words which re-sult from the highly agglutinative word morphology.Over-segmentation usually occurs for previouslyunseen words found in adaptation texts.
To en-sure reliable pronunciation modeling of foreignentity names it?s preferable to keep the lemmaintact.
Mapping a whole word pronunciationrule onto separate morphemes is a non-trivialtask for non-phonetic languages such as English.The morphemes in the in-domain corpus will beadapted such that all foreign words are restoredinto their base forms and the base forms are addedto the morpheme vocabulary.
Below is an exam-ple.
Word boundaries are labeled with the <w>-tag.<w> oilers <w> ha?visi <w> edmonton in <w> com monwe al th <w> sta dium illa <w>?<w> oilers <w> ha?visi <w> edmonton in <w> com-monwealth <w> stadium illa <w>382.4 Language Model AdaptationThe in-domain adaptation text is segmented differ-ently depending on the foreign entity name can-didates that are included.
A separate in-domainLM Pi(w|h) is trained for each segmentation of thetext.
Linear interpolation is used to the adapt thebackground LM PB(w|h) with the in-domain LMPi(w|h).Padapi(w|h) = ?Pi(w|h) + (1?
?
)PB(w|h) (1)3 Experiments3.1 Speech DataEvaluation data consisted of two sets of Finnish ra-dio news segments in 16 kHz audio.
All of therecordings were collected in 2011-2012 from YLERadio Suomi news and sports programs.The first data set consisted of 32 general newssegments.
The total transcription length was 8271words.
4.8% of the words were categorized as for-eign entity names (FEN).
The second data set con-sisted of 43 sports news segments.
The total tran-scription length 6466 was words.
7.9% of the wordswere categorized as foreign entity names.3.2 System and ModelsAll speech recognition experiments were run on theAalto speech recognizer (Hirsima?ki et.
al, 2009).The background LM was trained on the Kieli-pankki corpus (70 million words).
A lexicon of 30kmorphs and a model of morph segmentation waslearnt from the same corpus as the LM using Mor-fessor (Creutz and Lagus, 2005).
The baseline lex-icon was adapted with a manually transcribed pro-nunciation dictionary of 2000 foreign entity namesfound in Finnish newswire texts.
A Kneser-Neysmoothed varigram LM (n=12) was trained on thesegmented corpus with the variKN language model-ing toolkit (Siivola et al, 2007).LM adaptation data was manually collected fromthe Web.
On average 2-3 articles were gathered pertopic featured in the evaluation data sets.
120 000words of text were gathered for LM adaptation onthe general news set.
60 000 words were gatheredfor LM adaptation on the sports news set.The foreign word detection algorithm and a lettertrigram model trained on the Kielipankki word listwere used to automatically find foreign entity namesin the adaptation texts and convert them into theirbase forms.
Different values were used as percent-age threshold T (30, 60, and 100%).The adaptation texts were segmented into morphswith the segmentation model learnt from the back-ground corpus.
Morpheme adaptation was per-formed by restoring the foreign entity name candi-dates into their base forms.
Separate in-domain vari-gram LMs (n=6) were trained for adaptation datasegmented into morphs using each choice of T inthe foreign name detection.
The background LMwas adapted with each in-domain LM separately us-ing linear interpolation with weight ?
= 0.1 chosenbased on preliminary experiments.A pronunciation model was trained with SequiturG2P on the manually constructed foreign word lexi-con.
The number of the most probable pronunciationvariants m for one word to be used in lexicon adap-tation, was tested with different values (1, 4, and 8).4 ResultsThe word error rate (WER), letter error rate (LER),and the foreign entity name error rate (FENER) arereported in the results.
All the results are presentedin Table 1.The first experiment was run on the baseline sys-tem.
The average WER is 21.7% for general newsand 34.0% for sports.
The average FENER is signif-icantly higher for both (76.6% and 80.7%).Supervised vocabulary adaptation was imple-mented by manually retrieving the foreign entitynames from the adaptation text and adding their pro-nunciation rules to the lexicon.
Morpheme adapta-tion was also applied.
Compared to only using linearinterpolation (?
= 0.1) supervised vocabulary adap-tation reduces WER by 4% (general news) and 6%(sports news).
Recognition of foreign entity namesis also improved with FENER reductions of 18%and 24%.Unsupervised vocabulary adaptation was imple-mented through automatic retrieval and pronuncia-tion generation of foreign entity names.
The pa-rameters of interest are the foreign name percentagethreshold T, determining how many foreign wordcandidates are included for lexicon and morphemeadaptation and m, the number of pronunciation vari-39Adaptation method ResultsLM Lexicon General News Sports NewsAdaptation T[%] m WER[%] LER[%] FENER[%] WER[%] LER[%] FENER[%]Background Baseline 21.7 5.7 76.6 34.0 11.4 80.7Background + AdaptationBaseline 20.6 5.3 67.8 32.0 10.7 69.4Supervised - 1 19.8 5.0 55.7 30.1 9.8 53.1Unsupervised301 20.4 5.2 64.0 31.5 10.4 64.14 20.2 5.2 58.7 31.6 10.4 60.48 20.4 5.3 56.9 31.5 10.4 56.8601 20.7 5.3 63.7 32.3 10.4 63.74 20.7 5.3 59.4 31.1 9.9 59.88 21.1 5.5 58.2 31.0 9.9 55.61001 21.1 5.4 62.7 33.2 10.7 66.14 21.2 5.5 58.2 32.6 10.4 60.78 22.1 5.9 59.2 33.2 10.6 57.0Table 1: Results of adaptation experiments on the two test sets.
Linear interpolation is tested with supervised andunsupervised vocabulary adaptation.
T is the top percentage of foreign entity name candidates used in unsupervisedvocabulary adaptation, and m is the number of pronunciation variants for each word.ants generated for each word.
The best performanceis reached on the general news set with T = 30% andm = 4 (WER = 20.2%, FENER = 58.7%), and on thesports news set with T = 60% and m = 8 (WER =31.0%, FENER = 55.6%).5 Conclusion and DiscussionIn this work we presented an unsupervised approachto pronunciation and language modeling of foreignentity names in morph-based speech recognition.In the context of LM adaptation, foreign en-tity name candidates were retrieved from in-domaintexts using a foreign word detection algorithm.
Pro-nunciation variants were generated for the foreignword candidates using a grapheme-to-phoneme con-verter.
Morpheme adaptation was also applied byrestoring the foreign entity names into their baseforms in the morph-segmented adaptation texts.The results indicate that unsupervised pronun-ciation and language modeling of foreign entitynames is feasible.
The unsupervised approach al-most matches supervised adaptation in correctly rec-ognizing foreign entity names.
Average WER is alsovery close to the supervised adaptation one despitethe increased acoustic confusability when introduc-ing more pronunciation variants.
The percentage offoreign word candidates included for adaptation af-fects performance of the algorithm.
Including allwords starting in uppercase letters significantly de-grades ASR results.
The optimal threshold valueis dependent on the adaptation text and its foreignword frequency and similarity to the evaluation data.The composition of likely pronunciations of for-eign names by Finnish speakers is not a straight-forward task.
While the native pronunciation of thename is the favored one, the origin of the name is notalways clear, nor the definition of the pronunciation.Additionally, the mapping of the native pronuncia-tion to the phoneme set used by the Finnish ASRsystem can only be an approximation, as well as thepronunciations that the Finnish speakers are able toproduce.
In future work we will study new methodsto model the pronunciation of the foreign names andperform evaluations also in speech retrieval wherethe recognition of names have particular importance.ReferencesB.
Ahmed.
2005.
Detection of Foreign Words and Namesin Written Text.
Doctoral thesis, Pace University.M.
Bisani and H. Ney.
2008.
Joint-Sequence Models forGrapheme-to-Phoneme Conversion.
Speech Commu-nication, vol.
50, Issue 5, pp.
434-451.M.
Creutz and K. Lagus.
2005.
Unsupervised Mor-pheme Segmentation and Morphology Induction fromText Corpora using Morfessor 1.0.
Technical ReportA81, Publications in Computer and Information Sci-ence, Helsinki University of Technology.T.
Hirsima?ki, J. Pylkko?nen, and M. Kurimo 2009.
Im-portance of High-order N-gram Models in Morph-based Speech Recognition.
IEEE Trans.
Audio,Speech and Lang., pp.
724-732, vol.
17.V.
Siivola, T. Hirsima?ki and S. Virpioja.
2007.
On Grow-ing and Pruning Kneser-Ney Smoothed N-Gram Mod-els.
IEEE Trans.
Audio, Speech and Lang., Vol.
15,No.
5.40
