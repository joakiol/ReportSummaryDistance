Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 344?355, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsName Phylogeny: A Generative Model of String VariationNicholas Andrews and Jason Eisner and Mark DredzeDepartment of Computer Science and Human Language Technology Center of ExcellenceJohns Hopkins University3400 N. Charles St., Baltimore, MD 21218 USA{noa,eisner,mdredze}@jhu.eduAbstractMany linguistic and textual processes involve transduc-tion of strings.
We show how to learn a stochastic trans-ducer from an unorganized collection of strings (ratherthan string pairs).
The role of the transducer is to orga-nize the collection.
Our generative model explains simi-larities among the strings by supposing that some stringsin the collection were not generated ab initio, but were in-stead derived by transduction from other, ?similar?
stringsin the collection.
Our variational EM learning algorithmalternately reestimates this phylogeny and the transducerparameters.
The final learned transducer can quickly linkany test name into the final phylogeny, thereby locatingvariants of the test name.
We find that our method caneffectively find name variants in a corpus of web stringsused to refer to persons inWikipedia, improving over stan-dard untrained distances such as Jaro-Winkler and Leven-shtein distance.1 IntroductionSystematic relationships between pairs of stringsare at the core of problems such as transliteration(Knight and Graehl, 1998), morphology (Dreyer andEisner, 2011), cross-document coreference resolu-tion (Bagga and Baldwin, 1998), canonicalization(Culotta et al2007), and paraphrasing (Barzilay andLee, 2003).
Stochastic transducers such as proba-bilistic finite-state transducers are often used to cap-ture such relationships.
They model a conditionaldistribution p(y | x), and are ordinarily trained oninput-output pairs of strings (Dreyer et al2008).In this paper, we are interested in learning froman unorganized collection of strings, some of whichmight have been derived from others by transforma-tive linguistic processes such as abbreviation, mor-phological derivation, historical sound or spellingchange, loanword formation, translation, transliter-ation, editing, or transcription error.
We assume thateach string was derived from at most one parent, butmay give rise to any number of children.The difficulty is that most or all of these parent-child relationships are unobserved.
We must recon-struct this evolutionary phylogeny.
At the same time,we must fit the parameters of a model of the relevantlinguistic process p(y | x), which says what sort ofchildren y might plausibly be derived from parent x.Learning this model of p(y | x) helps us organize thetraining collection by reconstructing its phylogeny,and also permits us to generalize to new forms.We will focus on the problem of name varia-tion.
We observe a collection of person names?fullnames, nicknames, abbreviated or misspelled names,etc.
Some of these names can refer to the same per-son; we hope to detect this.
It would be an unlikelycoincidence if two mentions of John Jacob Jingle-heimer Schmidt referred to different people, sincethis is a long and unusual name.
Similarly, John Ja-cob Jingelhimer Smith andDr.
J. J. Jingleheimermayalso be related names for this person.
That is, thesenames may be derived from one another, via unseenrelationships, although we cannot be sure.Readers may be reminded of unsupervised clus-tering, in which ?suspiciously similar?
points can beexplained as having been generated by the same clus-ter.
Since each name is linked to at most one parent,our setting resembles single-link clustering?with alearned, asymmetric distance measure p(y | x).We will propose a generative process that makesexplicit assumptions about how strings are copiedwith mutation.
It is assumed to have generated all thenames in the collection, in an unknown order.
Givenlearned parameters, we can ask the model whether aname Dr. J. J. Jingelheimer in the collection is morelikely to have been generated from scratch, or derivedfrom some previous name.1.1 Related WorkSeveral previous papers have also considered learn-ing transducers or other models of word pairs when344the pairing between inputs and outputs is not given.Most commonly, one observes parallel or compa-rable corpora in two languages, and must recon-struct a matching from one language?s words to theother?s before training on the resulting pairs (Schafer,2006b; Klementiev and Roth, 2006; Haghighi et al2008; Snyder et al2010; Sajjad et al2011).Hall and Klein (2010) extend this setting to morethan two languages, where the phylogenetic tree isknown.
A given lexeme (abstract word) can be re-alized in each language by at most one word (stringtype), derived from the parent language?s realizationof the same lexeme.
The system must match wordsthat share an underlying lexeme (i.e., cognates), cre-ating a matching of each language?s vocabulary to itsparent language?s vocabulary.
A further challenge isthat the parent words are unobserved ancestral forms.Similarly, Dreyer and Eisner (2011) organizewords into morphological paradigms of a givenstructure.
Again words with the same underlying lex-eme (i.e., morphemes) must be identified.
A lexemecan be realized in each grammatical inflection (suchas ?first person plural present?)
by exactly one wordtype, related to other inflected forms of the same lex-eme, which as above may be unobserved.
Their in-ference setting is closer to ours because the input isan unorganized collection of words?input words arenot tagged with their grammatical inflections.
Thiscontrasts with the usual multilingual setting whereeach word is tagged with its true language.In one way, our problem differs significantly fromthe above problems.
We are interested in randomvariation that may occur within a language as wellas across languages.
A person name may have un-boundedly many different variants.
This is unlikethe above problems, in which a lexeme has at mostK realizations, where K is the (small) number oflanguages or inflections.1 We cannot assign the ob-served strings to positions in an existing structurethat is shared across all lexemes, such as a given phy-logenetic tree whose K nodes represent languages,or a given inflectional grid whose K cells representgrammatical inflections.
Rather, we must organize1In the above problems, one learns a set ofO(K) orO(K2)specialized transducers that relate Latin to Italian, singular toplural, etc.
We instead use one global mutation model that ap-plies to all names?but see footnote 14 on incorporating special-ized transductions (Latin to Italian) within our mutation model.them into a idiosyncratic phylogenetic tree whosenodes are the string types or tokens themselves.Names and words are not the only non-biologicalobjects that are copied with mutation.
Documents,database records, bibliographic entries, code, andimages can evolve in the same way.
Reconstructingthese relationships has been considered by a numberof papers on authorship attribution, near-duplicatedetection, deduplication, record linkage, and plagia-rism detection.
A few such papers reconstruct a phy-logeny, as in the case of chain letters (Bennett etal., 2003), malware (Karim et al2005), or images(Dias et al2012).
In fact, the last of these uses thesame minimum spanning tree method that we applyin ?5.3.
However, these papers do not train a similar-ity measure as we do.
To our knowledge, these twotechniques have not been combined outside biology.In molecular evolutionary analysis, phylogenetictechniques have often been combined with estima-tion of some parametric model of mutation (Tamuraet al2011).
However, names mutate differentlyfrom biological sequences, and our mutation modelfor names (?4, ?8) reflects that.
We also posit a spe-cific process (?3) that generates the name phylogeny.2 An ExampleA fragment of a phylogeny for person names isshown in Figure 1.
Our procedure learned this auto-matically from a collection of name tokens, withoutobserving any input/output pairs.
The nodes of thephylogeny are the observed name types,2 each oneassociated with a count of observed tokens.Each arrow corresponds to a hypothesized mu-tation.
These mutations reflect linguistic processessuch as misspelling, initialism, nicknaming, translit-eration, etc.
As an exception, however, each ar-row from the distinguished root node ?
generatesan initial name for a new entity.
The descendants ofthis initial name are other names that subsequentlyevolved for that entity.
Thus, the child subtrees of ?give a partition of the name types into entities.Thanks to the phylogeny, the seemingly disparatenames Ghareeb Nawaz and Muinuddin Chishti areseen to refer to the same entity.
They may be tracedback to their common ancestor Khawaja Gharib-2We cannot currently hypothesize unobserved intermediateforms, e.g., common ancestors of similar strings.
See ?6.2.345Khawaja Gharibnawaz Muinuddin Hasan ChistyKhwaja Gharib NawazKhwaja Muin al-Din ChishtiGhareeb Nawaz Khwaja Moinuddin ChishtiKhwaja gharibnawaz Muinuddin ChishtiThomas Ruggles Pynchon, Jr.Thomas Ruggles Pynchon Jr.Thomas R. Pynchon, Jr.Thomas R. Pynchon Jr.Thomas R. Pynchon Thomas Pynchon, Jr.Thomas Pynchon Jr.Figure 1: A portion of a spanning tree found by our model.nawaz Muinuddin Hasan Chisty, from which bothwere derived via successive mutations.Not shown in Figure 1 is our learned family p ofconditional probability distributions, which modelsthe likely mutations in this corpus.
Our EM learn-ing procedure found p jointly with the phylogeny.Specifically, it alternated between improving p andimproving the distribution over phylogenies.
At theend, we extracted the single best phylogeny.Together, the learned p and the phylogeny in Fig-ure 1 form an explanation of the observed collectionof names.
What makes it more probable than otherexplanations?
Informally, two properties:?
Each node in the tree is plausibly derived fromits parent.
More precisely, the product ofthe edge probabilities under p is comparativelyhigh.
A different p would have reduced theprobability of the events in this phylogeny.
Adifferent phylogeny would have involved a moreimprobable collection of events, such as replac-ing Chishti with Pynchon, or generating manyunrelated copies of Pynchon directly from ?.?
In the phylogeny, the parent names tend to beused often enough that it is plausible for variantsof these names to have emerged.
Our modelsays that new tokens are derived from previ-ously generated tokens.
Thus?other thingsequal?Barack Obama is more plausibly a vari-ant of Barack Obama, Jr. than of BarackObama, Sr. (which has fewer tokens).3 A Generative Model of TokensOur model should reflect the reasons that name vari-ation exists.
A named entity has the form y = (e, w)where w is a string being used to refer to entity e. Asingle entity e may be referred to on different occa-sions by different name strings w. We suppose thatthis is the result of copying the entity with occasionalmutation of its name (as in asexual reproduction).Thus, we assume the following simple generativeprocess that produces an ordered sequence of tokensy1, y2, .
.
., where yi = (ei, wi).?
After the first k tokens y1, .
.
.
yk have been gen-erated, the author responsible for generating yk+1must choose whom to talk about next.
She is likelyto think of someone she has heard about often in thepast.
So to make this choice, she selects one of theprevious tokens yi uniformly at random, each havingprobability 1/(k + ?
); or else she selects ?, withprobability ?/(k + ?).?
If the author selected a previous token yi, thenwith probability 1 ?
?
she copies it faithfully, soyk+1 = yi.
But with probability ?, she instead drawsa mutated token yk+1 = (ek+1, wk+1) from the mu-tation model p(?
| yi).
This preserves the entity(ek+1 = ei with probability 1), but the new namewk+1 is a stochastic transduction of wi drawn fromp(?
| wi).3 For example, in referring to ei, the authormay shorten and respellwi = Khwaja Gharib Nawazinto wk+1 = Ghareeb Nawaz (Figure 1).?
If the author selected?, she must choose a freshentity yk+1 = (ek+1, wk+1) to talk about.
So shesets ek+1 to a newly created entity, sampling its namewk+1 from the distribution p(?
| ?).
For example,wk+1 = Thomas Ruggles Pynchon, Jr. (Figure 1).Nothing prevents wk+1 from being a name that is al-ready in use for another entity (i.e., wk+1 may equalwj for some j ?
k).3Straightforward extensions are to allow a variable mutationrate ?
(yi) that depends on properties of yi, and to allow wk+1to depend on known properties of ei.
See footnote 14 for furtherdiscussion of enriched tokens.3463.1 Relationship to other modelsIf we ignore the name strings, we can see that thesequence of entities e1, e2, .
.
.
eN is being generatedfrom a Chinese restaurant process (CRP) with con-centration parameter ?.
To the extent that ?
is low(so that  is rarely used), a few randomly chosen en-tities will dominate the corpus.The CRP is equivalent to sampling e1, e2, .
.
.
IIDfrom an unknown distribution that was itself drawnfrom a Dirichlet process with concentration ?.
Thisis indeed a standard model of a distribution over en-tities.
For example, Hall et al2008) use it to modelvenues in bibliographic entries.From this characterization of the CRP, one can seethat any permutation of this entity sequence wouldhave the same probability.
That is, our distributionover sequences of entities e is exchangeable.However, our distribution over sequences ofnamed entities y = (e, w) is non-exchangeable.It assigns different probabilities to different order-ings of the same tokens.
This is because our modelposits that later authors are influenced by earlier au-thors, copying entity names from them with muta-tion.
So ordering is important.
The mutation processis not symmetric?for example, Figure 1 reflects atendency to shorten rather than lengthen names.Non-exchangeability is one way that our presentmodel differs from (parametric) transformationmod-els (Eisner, 2002) and (non-parametric) transforma-tion processes (Andrews and Eisner, 2011).
Thesetoo are defined using mutation of strings or othertypes.
From a transformation process, one can drawa distribution over types, from which the tokens arethen sampled IID.
This results in an exchangeablesequence of tokens, just as in the Dirichlet process.We avoid transformation models here for threereasons.
(1) Inference is more expensive.
(2) Atransformation process seems less realistic as amodel of authorship.
It constructs a distribution overderivational paths, similar to the paths in Figure 1.It effectively says that each token is generated by re-capitulating some previously used path from ?, butwith some chance of deviating at each step.
For anauthor to generate a name token this way, she wouldhave to know the whole derivational history of theprevious name she was adapting.
Our present modelinstead allows an author simply to select a name shepreviously saw and copy or mutate its surface form.
(3) One should presumably prefer to explain a novelname y as a mutation of a frequent name x, otherthings equal (?2).
But surprisingly, inference underthe transformation process does not prefer this.4Another view of our present model comes fromthe literature on random graphs (e.g., for modelingsocial networks or the link structure of the web).
Ina preferential attachment model, a graph?s verticesare added one by one, and each vertex selects someprevious vertices as its neighbors.
Our phylogenyis a preferential attachment tree, a random directedgraph in which each vertex selects a single previousvertex as its parent.
Specifically, it is a random recur-sive tree (Smythe and Mahmoud, 1995) whose ver-tices are the tokens.5 To this simple random topol-ogy we have added a random labeling process withmutation.
The first ?
vertices are labeled with ?.4 A Mutation Model for StringsOur model in ?3 samples the next token y, when it isnot simply a faithful copy, from p(y | x) or p(y | ?
).The key step there is to sample the name string wyfrom p(wy | wx) or p(wy | ?
).Our model of these distributions could easily in-corporate detailed linguistic knowledge of the muta-tion process (see ?8).
Here we describe the specificmodel that we use in our experiments.
Like manysuch models, it can be regarded as a stochastic finite-state string-to-string transducer parameterized by ?.There is much prior work on stochastic models ofedit distance (Ristad andYianilos, 1998; Bilenko andMooney, 2003; Oncina and Sebban, 2006; Schafer,2006a; Bouchard-C?t?
et al2008; Dreyer et al2008, among others).
For the present experiments,we designed a moderately simple one that employs(1) conditioning on one character of right context,(2) latent ?edit?
and ?no-edit?
regions to capture thefact that groups of edits are often made in close prox-imity, and (3) some simple special handling for thedistribution conditioned on the root p(wy | ?
).We assume a stochastic mutation process which,when given an input string wx, edits it from left to4The very fact that x has been frequently observed demon-strates that it has often chosen to stop mutating.
This impliesthat it is likely to choose stop again rather than mutate into y.5This is not the tree shown in Figure 1, whose vertices aretypes rather than tokens.347right into an output string wy.
Then p(wy | wx) isthe total probability of all operation sequences onwxthat would produce wy.
This total can be computedin time O(|wx| ?
|wy|) by dynamic programming.Our process has four character-level edit opera-tions: copy, substitute, insert, delete.
It also has adistinguished no-edit operation that behaves exactlylike copy.
At each step, the process first randomlychooses whether to edit or no-edit, conditioned onlyon whether the previous operation was an edit.
If itchooses to edit, it chooses a random edit type withsome probability conditioned on the next input char-acter.
In the case of insert or substitute, it then ran-domly chooses an output character, conditioned onthe type of edit and the next input character.It is common to mutate a name by editing con-tiguous substrings (e.g., words).
Contiguous regionsof copying versus editing can be modeled by a lowprobability of transitioning between no-edit and editregions.6 Note that an edit region may include somecopy edits (or substitute edits that replace a charac-ter with itself) without leaving the edit region.
Thisis why we distinguish copy from no-edit.Input and output strings are augmented with atrailing eos (?end-of-string?)
symbol that is seen bythe single-character lookahead.
If the next characteris eos, the only available edit is insert.
Alternatively,if the process selects no-edit, then eos is copied tothe output string and the process terminates.In the case of p(wy | ?
), the input string is empty,and both input and output are augmented with a trail-ing eos?
character that behaves like eos.
Then wyis generated by a sequence of insertions followed bya copy.
These are conditioned as usual on the nextcharacter, here eos?, so the model can learn to insertmore or different characters when the input is ?.The parameters ?
determining the conditionalprobabilities of the different operations and charac-ters are estimated with backoff smoothing.5 InferenceThe input to inference is a collection of named entitytokens y.
Most are untagged tokens of the form y =(?, w).
In a semi-supervised setting, however, some6This somewhat resembles the traditional affine gap penaltyin computational biology (Gusfield, 1997), which makes dele-tions or insertions cheaper if they are consecutive.
We insteadmake consecutive edits cheaper regardless of the edit type.of the tokens may be tagged tokens of the form y =(e, w), whose true entity is known.
The entity tagsplace a constraint on the phylogeny, since each childsubtree of ?
must correspond to exactly one entity.5.1 An unrealistically supervised settingSuppose we were lucky enough to fully observe thesequence of named entity tokens yi = (ei, wi) pro-duced by our generative model.
That is, suppose alltokens were tagged and we knew their ordering.Yet there would still be something to infer: whichtokens were derived from which previous tokens.This phylogeny is described by a spanning tree overthe tokens.
Let us see how to infer it.For each potential edge x ?
y between namedentity tokens, define ?
(y | x) to be the probability ofchoosing x and copying it (possibly with mutation)to obtain y.
So?
(yj | ?)
= ?
p(yj | ?)
(1)?
(yj | yi) = ?
p(yj | yi) + (1?
?
)1(yj = yi) (2)except that if i ?
j or if ei 6= ej , then ?
(yj | yi) = 0(since yj can only be derived from an earlier tokenyi with the same entity).Now the prior probability of generating y1, .
.
.
yNwith a given phylogenetic tree is easily seen to be aproduct over all tree edges,?j ?
(yj | pa(yj)) wherepa(yj) is the parent of yj .
As a result, it is knownthat the following are efficient to compute from the(N + 1)?
(N + 1) matrix of ?
values (see ?5.3):(a) the max-probability spanning tree(b) the total probability of all spanning trees(c) the marginal probability of each edge, under theposterior distribution on spanning trees(a) is our single best guess of the phylogeny.
We usethis during evaluation.
(b) gives the model likeli-hood, i.e., the total probability of the observed datay1, .
.
.
yN .
To locally maximize the model likeli-hood, (c) can serve as the E step of our EM algorithm(?6) for tuning our mutation model.
The M step thenretrains the mutation model?s parameters ?
on input-output pairs wi ?
wj , weighting each pair by itsedge?s posterior marginal probability (c), since thatis the expected count of a wi ?
wj mutation.
Thiscomputation is iterated.3485.2 The unsupervised settingNow we turn to a real setting?fully unsuperviseddata.
Two issues will force us to use an approximateinference algorithm.
First, we have an untagged cor-pus: a token?s entity tag e is never observed.
Second,the order of the tokens is not observed, so we do notknow which other tokens are candidate parents.Our first approximation is to consider only phylo-genies over types rather than tokens.7 The type phy-logeny in Figure 1 represents a set of possible tokenphylogenies.
Each node of Figure 1 represents anuntagged name type y = (?, w).
By grouping all nytokens of this type into a single node, we mean thatthe first token of y was derived by mutation from theparent node, while each later token of y was derivedby copying an (unspecified) earlier token of y.A token phylogeny cannot be represented in thisway if two or more tokens of y were created by mu-tations.
In that case, their name strings are equal onlyby coincidence.
They may have different parents(perhaps of different entities), whereas the y node ina type phylogeny can have only one parent.We argue, however, that these unrepresentable to-ken phylogenies are comparatively unlikely a poste-riori and can be reasonably ignored during inference.The first token of y is necessarily amutation, but latertokens are much more likely to be copies.
The prob-ability of generating a later token y by copying someprevious token is at least(1?
?
)/(N + ?
),while the probability of generating it in some otherway is at mostmax(?
p(y | ?
), ?
maxx?Yp(y | x))where Y is the set of observed types.
The secondprobability is typically much smaller: an author isunlikely to invent exactly the observed string y, cer-tainly from ?
but even by mutating a similar stringx (especially when the mutation rate ?
is small).How do we evaluate a type phylogeny?
Con-sider the probability of generating untagged tokens7Working over types improves the quality of our second ap-proximation, and also speeds up the spanning tree algorithms.
?6 explains how to regard this approximation as variational EM.y1, .
.
.
yN in that order and respecting the phylogeny:(N?k=11k + ?
)?y?Yg(y | pa(y))?
?ny?1?i=1i (1?
?)??
(3)where g(y | pa(y)) is a factor for generating the firsttoken of y from its parent pa(y), defined byg(y | ?)
= ?
?
p(y | ?)
(4)g(y | x) = ?
?
(# tokens of x precedingfirst token of y) ?
p(y | x) (5)But we do not actually know the token order: byassumption, our input corpus is only an unorderedbag of tokens.
So we must treat the hidden order-ing like any other hidden variable and maximize themarginal likelihood, which sums (3) over all possi-ble orderings (permutations).
This sum can be re-garded as the number of permutations N !
(which isfixed given the corpus) times the expectation of (3)for a permutation chosen uniformly at random.This leads to our second approximation.
We ap-proximate this expectation of the product (3) with aproduct of expectations of its individual factors.8 Tofind the expectation of (5), observe that the expectednumber of tokens of x that precede the first token ofy is nx/(ny+1), since each of the nx tokens of x hasa 1/(ny + 1) chance of falling before all ny tokensof y.
It follows that the approximated probability ofgenerating all tokens in some order, with our giventype parentage, is proportional to?y?Y?
(y | pa(y)) (6)where?
(y | ?)
= ?
?
p(y | ?)
(7)?
(y | x) = ?
?
p(y | x) ?
nx/(ny + 1) (8)and the constant of proportionality depends on thecorpus.The above equations are analogous to those in?5.1.
Again, the approximate posterior probabilityof a given type parentage tree is edge-factored?it isthe product of individual edge weights defined by ?.Thus, we are again eligible to use the spanning treealgorithms in ?5.3 below.8In general this is an overestimate for each phylogeny.349Notice that the ratio ?/?
controls the preferencefor an entity to descend from ?
versus an existingentity.
Thus, by tuning this ratio, we can controlthe number of entities inferred by our method, whereeach entity corresponds to one of the child subtreesof ?.Also note that nx in the numerator of (8) meansthat y?s parent is more likely to be frequent.
Also,ny +1 in the denominator means that a frequent y isnot as likely to have any parent x 6= ?, because itsfirst token probably falls early in the sequence wherethere are fewer available parents x 6= ?.5.3 Spanning tree algorithmsDefine a complete directed graphG over the verticesY ?
{?}.
The weight of an edge x ?
y is definedby ?
(y | x).
The (approximate) posterior probabilityof a given phylogeny given our evidence, is propor-tional to the product of the ?
values of its edges.Formally, let T?
(G) denote the set of spanningtrees of G rooted at ?, and define the weight of aparticular spanning tree T ?
T?
(G) to be the prod-uct of the weights of its edges:w(T ) =?(x?y)?T?
(y | x) (9)Then the posterior probability of spanning tree T isp?
(T ) =w(T )Z(G)(10)where Z(G) =?T?T?
(G)w(T ) is the partitionfunction, i.e.
the total probability of generating thedataG via any spanning tree of the formwe consider.This distribution is determined by the parameters ?of the transducer p?, along with the ratio ?/?.There exist several algorithms to find the sin-glemaximum-probability spanning tree, notably Tar-jan?s implementation of the Chu-Liu-Edmonds algo-rithm, which runs in O(m log n) for a sparse graphor O(n2) for a dense graph (Tarjan, 1977).
Figure 1shows a spanning tree found by our model using Tar-jan?s algorithm.
Here n is the number of vertices(in our case, types and ), whilem is the number ofedges (which we can keep small by pruning, ?6.1).6 Training the Transducer with EMOur inference algorithm assumes that we know thetransducer parameters ?.
We now explain how to op-timize ?
to maximize the marginal likelihood of thetraining data.
This marginal likelihood sums over allthe other latent variables in the model?the spanningtree, the alignments between strings, and the hiddentoken ordering.The EMprocedure repeats the following until con-vergence:E-step: Given ?, compute the posterior marginalprobabilities cxy of all possible phylogenyedges.M-step Given all cxy, retrain ?
to assign a highconditional probability to the mutations on theprobable edges.We actually use a variational EM algorithm: ourE step approximates the true distribution q over allphylogenies with the closest distribution p that as-signs positive probability only to type-based phylo-genies.
This distribution is given by (10) and min-imizes KL(p || q).
We argued in section ?5.2 thatit should be a good approximation.
The posteriormarginal probability of a directed edge from vertexx to vertex y, according to (10), iscxy =?T?T?(G):(x?y)?Tp?
(T ) (11)The probability cxy is a ?pseudocount?
for the ex-pected number of mutations from x to y.
This is atmost 1 under our assumptions.Calculating cxy requires summing over all span-ning trees of G, of which there are nn?2 for a fullyconnected graph with n vertices.
Fortunately, Tutte(1984) shows how to compute this sum by the fol-lowing method, which extends Kirchhoff?s classi-cal matrix-tree theorem to weighted directed graphs.This result has previously been employed in non-projective dependency parsing (Koo et al2007;Smith and Smith, 2007).Let L ?
Rn?n denote the Laplacian ofG, namelyL ={ ?x?
?
(y | x?)
if x = y??
(y | x) if x 6= y(12)Tutte?s theorem relates the determinant of the Lapla-cian to the spanning trees in graph G. In particular,the cofactor L0,0 is equal to the sum of the weights350of all directed spanning trees rooted at 0, which (sup-posing?
is indexed at 0) yields the partition functionZ(G).The edge marginals of interest are related to thelog partition function bycxy =?Z(G)??
(y | x)(13)which has the closed-form solutioncxy ={?
(y | ?
)L?1yy if x = y?
(y | x)(L?1xx ?
L?1xy ) if x 6= y(14)Thus, the problem of computing edge marginals re-duces to that of computing a matrix inverse, whichmay be done in O(n3) time.At the M step, we retrain the mutation model pa-rameters ?
to maximize?xy cxy log p(wy | wx).This is tantamount to maximum conditional likeli-hood training on a supervised collection of (wx, wy)pairs that are respectively weighted by cxy.The M step is nontrivial because the term p(wy |wx) sums over a hidden alignment between twostrings.
It may be performed by an inner loop of EM,where the E step uses dynamic programming to ef-ficiently consider all possible alignments, as in (Ris-tad and Yianilos, 1996).
In practice, we have found iteffective to take only a single step of this inner loop.Such a Generalized EM procedure enjoys the sameconvergence properties as EM, but may reach a localoptimum faster (Dempster et al1977).6.1 Pruning the graphFor large graphs, it is essential to prune the numberof edges to avoid considering all n(n ?
1) input-output pairs.
To prune the graph, we eliminate alledges between strings that do not share any commontrigrams (case- and diacritic-insensitive), by settingtheir matrix entries to 0.
As a result, the graph Lapla-cian is a sparse matrix, which often allows fastermatrix inversion using preconditioned iterative algo-rithms.
Furthermore, pruned edges do not appear inany spanning tree, so the E step will find that theirposterior marginal probabilities are 0.
This meansthat the input-output pairs corresponding to theseedges can be ignored when re-estimating the trans-ducer parameters in the M step.
We found that prun-ing significantly improves training time with no ap-preciable loss in performance.96.2 Training with unobserved tokens?A deficiency of our method is that it assumes thatauthors of our corpus have only been exposed to pre-vious tokens in our corpus.
In principle, one couldalso train with U additional tokens (e, w) where weobserve neither e nor w, for very large U .
This is the?universe of discourse?
in which our authors oper-ate.10 In this case, we would need (expensive) newalgorithms to reconstruct the strings w. However,this model could infer a more realistic phylogeny bypositing unobserved ancestral or intermediate formsthat relate the observed tokens, as in transformationmodels (Eisner, 2002; Andrews and Eisner, 2011).7 Experimental Evaluation7.1 Data preparationScraping Wikipedia.
Wikipedia documents manyvariant names for entities.
As a result, it has fre-quently been used as a source for mining name vari-ations, both within and across languages (Parton etal., 2008; Cucerzan, 2007).
We used Wikipedia tocreate a list of name aliases for different entities.Specifically, we mined English Wikipedia11 for allredirects: page names that lead directly to anotherpage.
Redirects are created by Wikipedia users forresolving common name variants to the correct page.For example, the pages titled Barack Obama Ju-nior and Barack Hussein Obama automatically redi-rect to the page titled Barack Obama.
This redirec-tion implies that the first two are name variants ofthe third.
Collecting all such links within EnglishWikipedia yields a large number of aliases for eachpage.
However, many redirects are for topics otherthan individual people, and these would be poor ex-amples of name variation.
In addition, some phrases9For instance, on a dataset of approximately 6000 distinctnames, pruning reduced the number of outgoing edges at eachvertex to fewer than 100 per vertex.10Notice that theN observed tokens would be approximatelyexchangeable in this setting: they are unlikely to depend on oneanother when N  U , and hence their order no longer mattersmuch.
In effect, generating theU hidden tokens constructs a richdistribution (analogous to a sample from the Dirichlet process)from which the N observed tokens are then sampled IID.11Using a Wikipedia dump from February 2, 2011.351Ho Chi Minh, Ho chi mihn, Ho-Chi Minh, Ho Chih-minhGuy Fawkes, Guy fawkes, Guy faux, Guy Falks, Guy Faukes, Guy Fawks, Guy foxe, Guy FalkesNicholas II of Russia, Nikolai Aleksandrovich Romanov, Nicholas Alexandrovich of Russia, Nicolas IIBill Gates, Lord Billy, Bill Gates, BillGates, Billy Gates, William Gates III, William H. GatesWilliam Shakespeare, William shekspere, William shakspeare, Bill ShakespearBill Clinton, Billll Clinton, William Jefferson Blythe IV, Bill J. Clinton, William J ClintonFigure 2: Sample alias lists scraped from Wikipedia.
Note that only partial alias lists are shown for space reasons.that redirect to an entity are descriptions rather thannames.
For example, 44th President of the UnitedStates also links to Barack Obama, but it is not aname variant.Freebase filtering.
To improve data quality we usedFreebase, a structured knowledge base that incorpo-rates information from Wikipedia.
Among its struc-tured information are entity types, including the type?person.?
We filtered the Wikipedia redirect col-lection to remove pairs where the target page wasnot listed as a person in Freebase.
Additionally, toremove redirects that were not proper names (44thPresident of the United States), we applied a seriesof rule based filters to remove bad aliases: removingnumerical names, parentheticals after names, quota-tion marks, and names longer than 5 tokens, sincewe found that these long names were rarely personnames (e.g.
United States Ambassador to the Eu-ropean Union, Success Through a Positive MentalAttitude which links to the author Napoleon Hill.
)While not perfect, these modifications dramaticallyimproved quality.
The result was a list of 78,079 dif-ferent person entities, each with one or more knownnames or aliases.
Some typical names are shown inFigure 2.Estimating empirical type counts.
Our method isreally intended to be run on a corpus of string to-kens.
However, for experimental purposes, we in-stead use the above dataset of string types becausethis allows us to use the ?ground truth?
given bythe Wikipedia redirects.
To synthesize token counts,empirical token frequencies for each type were esti-mated from the LDC Gigaword corpus,12 which isa corpus of newswire text spanning several years.Wikipedia name types that did not appear in Giga-word were assigned a ?backoff count?
of one.
Notethat by virtue of the domain, many misspellings will12LDC Catalog No.
LDC2003T05.not appear; however, edges ?popular?
names (whichmay be canonical names) will be assigned higherweight.7.2 ExperimentsWe begin by evaluating the generalization ability of atransducer trained using a transformation model.
Todo so, we measure log-likelihood on held-out entitytitle and alias pairs.
We then verify that the general-ization ability according to log-likelihood translatesinto gains for a name matching task.
For the experi-ments in this section, we use ?
= 0.9 and ?
= 0.1.13Held-out log-likelihood.
We construct pairs of en-tity title (input) and alias (output) names from theWikipedia data.
For different amounts of superviseddata, we trained the transformation model on thetraining set, and plotted the log-likelihood of held-out test data for the transducer parameters at each it-eration of EM.
The held-out test set is constructedfrom a disjoint set of Wikipedia entities, the samenumber of entities as in the training set.
We useddifferent corpora of 1000 and 1500 entities for trainand test.Name matching.
For each alias a in a test set (notseen at training time), we produce a ranking of testentity titles t according to transducer probabilitiesp?
(a | t).
A good transducer should assign highprobability to transformations from the correct ti-tle for the alias.
Mean reciprocal rank (MRR) is acommonly used metric to estimate the quality of aranking, which we report in Figure 4.
The reportedmean is over all aliases in the test data.
In addition toevaluating the ranking for different initializations ofour transducer, we compare to two baselines: Lev-enshtein distance and Jaro-Winkler similarity.
Jaro-Winkler is a measure on strings that was specificallydesigned for record linkage (Winkler, 1999).
The13We did not find these parameters to be sensitive.3520 1 2 3 4 5 6 7 8 9EM iteration15000014000013000012000011000010000090000Held out log-likelihoodsup=0sup=5sup=25sup=100sup=250(a) 1000 entities.0 1 2 3 4 5 6 7 8 9EM iteration240000230000220000210000200000190000180000170000160000150000Held out log-likelihoodsup=0sup=5sup=25sup=100sup=250(b) 1500 entities.Figure 3: Learning curves for different initializations of the transducer parameters.
Above, ?sup=100?
(for instance)means that 100 entities were used as training data to initialize the transducer parameters (constructing pairs betweenall title-alias pairs for those Wikpedia entities).15000.600.650.700.750.800.85MRRjwinklevsup10semi10unsupsupFigure 4: Mean reciprocal rank (MRR) results for differ-ent training conditions: ?sup10?
means that 10 entities(roughly 40 name pairs) were used as training data forthe transducer; ?semi10?
means that the ?sup10?
modelwas used as initialization before re-estimating the param-eters using our model; ?unsup?
is the transducer trainedusing our model without any initial supervision; ?sup?
istrained on all 1500 entities in the training set (an upperbound on performance); ?jwink?
and ?lev?
correspond toJaro-Winkler and Levenshtein distance baselines.matching experiments were performed on a corpusof 1500 entities (with separate corpora of the samesize for training and test).8 Conclusions and Future WorkWe have presented a new unsupervised method forlearning string-to-string transducers.
It learns froma collection of related strings whose relationships areunknown.
The key idea is that some strings are mu-tations of common strings that occurred earlier.
Wecompute a distribution over the unknown phyloge-netic tree that relates these strings, and use it to rees-timate the transducer parameters via EM.One direction for future work would be more so-phisticated transduction models than the one we de-veloped in ?4.
For names, this could include learn-ing common nicknames (nonparametrically); explic-itly modeling abbreviation processes such as initials;conditioning on name components such as title andmiddle name; and transliterating across languages.14In other domains, one could model bibliographic en-try propagation, derivational morphology, or histor-ical sound change (again using language tags).Another future direction would be to incorporatethe context of tokens in order to help reconstructwhich tokens are coreferent.
For example, we mightextend the generative story to generate a context fortoken (e, w) conditioned on e. Combining contex-tual similarity with string similarity has previouslyproved very useful for identifying cognates (Schaferand Yarowsky, 2002; Schafer, 2006b; Bergsma andVan Durme, 2011).
In our setting it would help todistinguish people with identical names, as well asdetermining whether two people with similar namesare really the same.14These last two points suggest that the mutation modelshould operate not on simple (entity, string) pairs, but on richerrepresentations in which the name has been parsed into its com-ponents (Eisenstein et al2011), labeled with a language ID,and perhaps labeled with a phonological pronunciation.
Theseadditional properties of a named entity may be either observedor latent in training data.
For example, if wy and `y denote thestring and language of name y, then define p(y | x) = p(`y |`x) ?
p(wy | `y, `x, wx).
The second factor captures translitera-tion from language `x to language `y , e.g., by using ?4?s modelwith an (`x, `y)-specific parameter setting.353ReferencesNicholas Andrews and Jason Eisner.
2011.
Transformation pro-cess priors.
In NIPS 2011 Workshop on Bayesian Nonpara-metrics: Hope or Hype?, Sierra Nevada, Spain, December.Extended abstract (3 pages).A.
Bagga and B. Baldwin.
1998.
Algorithms for scoring coref-erence chains.
In LREC.Regina Barzilay and Lillian Lee.
2003.
Learning to para-phrase: an unsupervised approach using multiple-sequencealignment.
In Proc.
of NAACL-HLT, pages 16?23, Strouds-burg, PA, USA.C.
H. Bennett, M. Li, , and B. Ma.
2003.
Chain lettersand evolutionary histories.
Scientific American, 288(3):76?81, June.
More mathematical version available at http://www.cs.uwaterloo.ca/~mli/chain.html.Shane Bergsma and Benjamin Van Durme.
2011.
Learningbilingual lexicons using the visual similarity of labeled webimages.
In Proc.
of IJCAI, pages 1764?1769, Barcelona,Spain.Mikhail Bilenko and Raymond J. Mooney.
2003.
Adaptiveduplicate detection using learnable string similarity mea-sures.
In Proc.
of ACM SIGKDD International Conferenceon Knowledge Discovery and Data Mining, KDD ?03, pages39?48, New York, NY, USA.
ACM.Alexandre Bouchard-C?t?, Percy Liang, Thomas Griffiths, andDan Klein.
2008.
A probabilistic approach to languagechange.
In Proc.
of NIPS, pages 169?176.S.
Cucerzan.
2007.
Large-scale named entity disambiguationbased on Wikipedia data.
In Proc.
of EMNLP.Aron Culotta, Michael Wick, Robert Hall, Matthew Marzilli,and Andrew McCallum.
2007.
Canonicalization of databaserecords using adaptive similarity measures.
In Proc.
of ACMSIGKDD International Conference on Knowledge Discoveryand Data Mining, KDD ?07, pages 201?209.A.
P. Dempster, N. M. Laird, and D. B. Rubin.
1977.
Maxi-mum likelihood from incomplete data via the EM algorithm.Journal of the Royal Statistical Society.
Series B (Method-ological), 39(1):1?38.Z.
Dias, A. Rocha, and S. Goldenstein.
2012.
Image phy-logeny by minimal spanning trees.
IEEE Trans.
on Informa-tion Forensics and Security, 7(2):774?788, April.Markus Dreyer and Jason Eisner.
2011.
Discovering morpho-logical paradigms from plain text using a Dirichlet processmixture model.
In Proc.
of EMNLP, pages 616?627.
Sup-plementary material (9 pages) also available.Markus Dreyer, Jason Smith, and Jason Eisner.
2008.
Latent-variable modeling of string transductions with finite-statemethods.
In Proc.
of EMNLP, pages 1080?1089, Honolulu,Hawaii, October.
Association for Computational Linguistics.Jacob Eisenstein, Tae Yano, William Cohen, Noah Smith, andEric Xing.
2011.
Structured databases of named entitiesfromBayesian nonparametrics.
InProc.
of the First workshopon Unsupervised Learning in NLP, pages 2?12, Edinburgh,Scotland, July.
Association for Computational Linguistics.Jason Eisner.
2002.
Transformational priors over grammars.In Proceedings of the Conference on Empirical Methods inNatural Language Processing (EMNLP), Philadelphia, July.Dan Gusfield.
1997.
Algorithms on Strings, Trees, andSequences?Computer Science and Computational Biology.Cambridge University Press.Aria Haghighi, Percy Liang, Taylor Berg-Kirkpatrick, and DanKlein.
2008.
Learning bilingual lexicons from monolingualcorpora.
In Proc.
of ACL-08: HLT, pages 771?779.David Hall and Dan Klein.
2010.
Finding cognates using phylo-genies.
In Association for Computational Linguistics (ACL).Rob Hall, Charles Sutton, and Andrew McCallum.
2008.
Un-supervised deduplication using cross-field dependencies.
InProc.
of the ACM SIGKDD International Conference OnKnowledge Discovery and Data Mining, KDD ?08, pages310?317.Md.
Enamul.
Karim, Andrew Walenstein, Arun Lakhotia, andLaxmi Parida.
2005.
Malware phylogeny generation usingpermutations of code.
Journal in Computer Virology, 1(1?2):13?23.Alexandre Klementiev and Dan Roth.
2006.
Weakly supervisednamed entity transliteration and discovery from multilingualcomparable corpora.
In Proc.
of COLING-ACL, pages 817?824.K.
Knight and J. Graehl.
1998.
Machine transliteration.
Com-putational Linguistics, 24:599?612.Terry Koo, Amir Globerson, Xavier Carreras, and MichaelCollins.
2007.
Structured prediction models via the matrix-tree theorem.
In Proc.
of EMNLP-CoNLL, pages 141?150.Jose Oncina and Marc Sebban.
2006.
Using learned conditionaldistributions as edit distance.
In Proc.
of the 2006 Joint IAPRinternational Conference on Structural, Syntactic, and Statis-tical Pattern Recognition, SSPR?06/SPR?06, pages 403?411.Kristen Parton, Kathleen R. McKeown, James Allan, and En-rique Henestroza.
2008.
Simultaneous multilingual searchfor translingual information retrieval.
In Proceeding of theACM conference on Information and Knowledge Manage-ment, CIKM ?08, pages 719?728.Eric Sven Ristad and Peter N. Yianilos.
1996.
Learning stringedit distance.
Technical Report CS-TR-532-96, PrincetonUniversity, Department of Computer Science.Eric Sven Ristad and Peter N. Yianilos.
1998.
Learning stringedit distance.
IEEE Transactions on Pattern Recognition andMachine Intelligence, 20(5):522?532, May.Hassan Sajjad, Alexander Fraser, and Helmut Schmid.
2011.An algorithm for unsupervised transliteration mining with anapplication to word alignment.
In Proc.
of ACL, pages 430?439.Charles Schafer and David Yarowsky.
2002.
Inducing transla-tion lexicons via diverse similarity measures and bridge lan-guages.
In Proc.
of CONLL, pages 146?152.Charles Schafer.
2006a.
Novel probabilistic finite-state transduc-ers for cognate and transliteration modeling.
In 7th BiennialConference of the Association for Machine Translation in theAmericas (AMTA).Charles Schafer.
2006b.
Translation Discovery Using DiverseSmilarity Measures.
Ph.D. thesis, Johns Hopkins University.David A. Smith and Noah A. Smith.
2007.
Probabilistic mod-els of nonprojective dependency trees.
In Proc.
of EMNLP-CoNLL, pages 132?140.354R.
T. Smythe and H. M. Mahmoud.
1995.
A survey of recur-sive trees.
Theory of Probability andMathematical Statistics,51(1?27).Benjamin Snyder, Regina Barzilay, and Kevin Knight.
2010.
Astatistical model for lost language decipherment.
In Proc.
ofACL, pages 1048?1057.Koichiro Tamura, Daniel Peterson, Nicholas Peterson, GlenStecher, Masatoshi Nei, and Sudhir Kumar.
2011.
Mega5:Molecular evolutionary genetics analysis using maximumlikelihood, evolutionary distance, and maximum parsimonymethods.
Molecular Biology and Evolution, 28(10):2731?2739.R E Tarjan.
1977.
Finding optimum branchings.
Networks,7(1):25?35.W.
Tutte.
1984.
Graph Theory.
Addison-Wesley.William E. Winkler.
1999.
The state of record linkage and cur-rent research problems.
Technical report, Statistical ResearchDivision, U.S. Census Bureau.355
