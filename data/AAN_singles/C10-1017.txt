Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 143?151,Beijing, August 2010End-to-End Coreference Resolution via Hypergraph PartitioningJie Cai and Michael StrubeNatural Language Processing GroupHeidelberg Institute for Theoretical Studies gGmbH(jie.cai|michael.strube)@h-its.orgAbstractWe describe a novel approach to coref-erence resolution which implements aglobal decision via hypergraph partition-ing.
In constrast to almost all previ-ous approaches, we do not rely on sep-arate classification and clustering steps,but perform coreference resolution glob-ally in one step.
Our hypergraph-basedglobal model implemented within an end-to-end coreference resolution system out-performs two strong baselines (Soon et al,2001; Bengtson & Roth, 2008) using sys-tem mentions only.1 IntroductionCoreference resolution is the task of groupingmentions of entities into sets so that all mentionsin one set refer to the same entity.
Most recentapproaches to coreference resolution divide thistask into two steps: (1) a classification step whichdetermines whether a pair of mentions is corefer-ent or which outputs a confidence value, and (2)a clustering step which groups mentions into enti-ties based on the output of step 1.The classification steps of most approachesvary in the choice of the classifier (e.g.
decisiontree classifiers (Soon et al, 2001), maximum en-tropy classification (Luo et al, 2004), SVM clas-sifiers (Rahman & Ng, 2009)) and the number offeatures used (Soon et al (2001) employ a set oftwelve simple but effective features while e.g., Ng& Cardie (2002) and Bengtson & Roth (2008) de-vise much richer feature sets).The clustering step exhibits much more varia-tion: Local variants utilize a closest-first decision(Soon et al, 2001), where a mention is resolved toits closest possible antecedent, or a best-first deci-sion (Ng & Cardie, 2002), where a mention is re-solved to its most confident antecedent (based onthe confidence value returned by step 1).
Globalvariants attempt to consider all possible cluster-ing possibilites by creating and searching a Belltree (Luo et al, 2004), by learning the optimalsearch strategy itself (Daume?
III & Marcu, 2005),by building a graph representation and applyinggraph clustering techniques (Nicolae & Nicolae,2006), or by employing integer linear program-ming (Klenner, 2007; Denis & Baldridge, 2009).Since these methods base their global clusteringstep on a local pairwise model, some global infor-mation which could have guided step 2 is alreadylost.
The twin-candidate model (Yang et al, 2008)replaces the pairwise model by learning prefer-ences between two antecedent candidates in step1 and applies tournament schemes instead of theclustering in step 2.There is little work which deviates from thistwo-step scheme.
Culotta et al (2007) introduce afirst-order probabilistic model which implementsfeatures over sets of mentions and thus operatesdirectly on entities.In this paper we describe a novel approach tocoreference resolution which avoids the divisioninto two steps and instead performs a global deci-sion in one step.
We represent a document as a hy-pergraph, where the vertices denote mentions andthe edges denote relational features between men-tions.
Coreference resolution is performed glob-ally in one step by partitioning the hypergraph intosubhypergraphs so that all mentions in one subhy-pergraph refer to the same entity.
Our model out-143performs two strong baselines, Soon et al (2001)and Bengtson & Roth (2008).Soon et al (2001) developed an end-to-endcoreference resolution system for the MUC data,i.e., a system which processes raw documentsas input and produces annotated ones as output.However, with the advent of the ACE data, manysystems either evaluated only true mentions, i.e.mentions which are included in the annotation,the so-called key, or even received true informa-tion for mention boundaries, heads of mentionsand mention type (Culotta et al, 2007, inter alia).While these papers report impressive results it hasbeen concluded that this experimental setup sim-plifies the task and leads to an unrealistic surro-gate for the coreference resolution problem (Stoy-anov et al, 2009, p.657, p660).
We argue thatthe field should move towards a realistic settingusing system mentions, i.e.
automatically deter-mined mention boundaries and types.
In this pa-per we report results using our end-to-end coref-erence resolution system, COPA, without relyingon unrealistic assumptions.2 Related WorkSoon et al (2001) transform the coreference res-olution problem straightforwardly into a pairwiseclassification task making it accessible to standardmachine learning classifiers.
They use a set oftwelve powerful features.
Their system is basedsolely on information of the mention pair anaphorand antecedent.
It does not take any informationof other mentions into account.
However, it turnedout that it is difficult to improve upon their re-sults just by applying a more sophisticated learn-ing method and without improving the features.We use a reimplementation of their system as firstbaseline.
Bengtson & Roth (2008) push this ap-proach to the limit by devising a much more in-formative feature set.
They report the best resultsto date on the ACE 2004 data using true mentions.We use their system combined with our prepro-cessing components as second baseline.Luo et al (2004) perform the clustering stepwithin a Bell tree representation.
Hence theirsystem theoretically has access to all possibleoutcomes making it a potentially global system.However, the classification step is still based ona pairwise model.
Also since the search space inthe Bell tree is too large they have to apply searchheuristics.
Hence, their approach loses much ofthe power of a truly global approach.Culotta et al (2007) introduce a first-orderprobabilistic model which implements featuresover sets of mentions.
They use four features fortheir first-order model.
The first is an enumerationover pairs of noun phrases.
The second is the out-put of a pairwise model.
The third is the clustersize.
The fourth counts mention type, number andgender in each cluster.
Still, their model is basedmostly on information about pairs of mentions.They assume true mentions as input.
It is not clearwhether the improvement in results translates tosystem mentions.Nicolae & Nicolae (2006) describe a graph-based approach which superficially resembles ourapproach.
However, they still implement a twostep coreference resolution approach and applythe global graph-based model only to step 2.
Theyreport considerable improvements over state-of-the-art systems including Luo et al (2004).
How-ever, since they not only change the clusteringstrategy but also the features for step 1, it is notclear whether the improvements are due to thegraph-based clustering technique.
We, instead,describe a graph-based approach which performsclassification and clustering in one step.
We com-pare our approach with two competitive systemsusing the same feature sets.3 COPA: Coreference PartitionerThe COPA system consists of learning moduleswhich learn hyperedge weights from the trainingdata, and resolution modules which create a hy-pergraph representation for the testing data andperform partitioning to produce subhypergraphs,each of which represents an entity.
An exampleanalysis of a short document involving the two en-tities, BARACK OBAMA and NICOLAS SARKOZYillustrates how COPA works.
[US President Barack Obama] came to Toronto today.
[Obama] discussed the financial crisis with [PresidentSarkozy].
[He] talked to him [him] about the recent downturn of theEuropean markets.
[Barack Obama] will leave Toronto tomorrow.144A hypergraph (Figure (1a)) is built for thisdocument based on three features.
Two hyper-edges denote the feature partial string match,{US President Barack Obama, Barack Obama,Obama} and {US President Barack Obama, Pres-ident Sarkozy}.
One hyperedge denotes the fea-ture pronoun match, {he, him}.
Two hyperedgesdenote the feature all speak, {Obama, he} and{President Sarkozy, him}.On this initial representation, a spectral clus-tering technique is applied to find two partitionswhich have the strongest within-cluster connec-tions and the weakest between-clusters relations.The cut found is called Normalized Cut, whichavoids trivial partitions frequently output by themin-cut algorithm.
The two output subhyper-graphs (Figure (1b)) correspond to two resolvedentities shown on both sides of the bold dashedline.
In real cases, recursive cutting is appliedto all the subhypergraphs resulting from previoussteps, until a stopping criterion is reached.Figure 1: Hypergraph-based representation3.1 HyperEdgeLearnerCOPA needs training data only for computing thehyperedge weights.
Hyperedges represent fea-tures.
Each hyperedge corresponds to a featureinstance modeling a simple relation between twoor more mentions.
This leads to initially overlap-ping sets of mentions.
Hyperedges are assignedweights which are calculated based on the train-ing data as the percentage of the initial edges (asillustrated in Figure (1a)) being in fact coreferent.The weights for some of Soon et al (2001)?s fea-tures learned from the ACE 2004 training data aregiven in Table 1.Edge Name WeightAlias 0.777StrMatch Pron 0.702Appositive 0.568StrMatch Npron 0.657ContinuousDistAgree 0.403Table 1: Hyperedge weights for ACE 2004 data3.2 Coreference Resolution ModulesUnlike pairwise models, COPA processes a docu-ment globally in one step, taking care of the pref-erence information among all the mentions at thesame time and clustering them into sets directly.A raw document is represented as a single hyper-graph with multiple edges.
The hypergraph re-solver partitions the simple hypergraph into sev-eral subhypergraphs, each corresponding to oneset of coreferent mentions (see e.g.
Figure (1b)which contains two subhypergraphs).3.2.1 HGModelBuilderA single document is represented in a hyper-graph with basic relational features.
Each hyper-edge in a graph corresponds to an instance of oneof those features with the weight assigned by theHyperEdgeLearner.
Instead of connecting nodeswith the target relation as usually done in graphmodels, COPA builds the graph directly out of aset of low dimensional features without any as-sumptions for a distance metric.3.2.2 HGResolverIn order to partition the hypergraph we adopta spectral clustering algorithm.
Spectral cluster-ing techniques use information obtained from theeigenvalues and eigenvectors of the graph Lapla-cian to cluster the vertices.
They are simple to im-plement and reasonably fast and have been shownto frequently outperform traditional clustering al-gorithms such as k-means.
These techniques have145Algorithm 1 R2 partitionerNote: { L = I ?Dv?12HWDe?1HTDv?12 }Note: { Ncut(S) := vol?S( 1volS + 1volSc )}input: target hypergraph HG, predefined ?
?Given a HG, construct its Dv , H , W and DeCompute L for HGSolve the L for the second smallest eigenvector V2for each splitting point in V2 docalculate Ncutiend forChoose the splitting point with mini(Ncuti)Generate two subHGsif mini(Ncuti) < ??
thenfor each subHG doBi-partition the subHG with the R2 partitionerend forelseOutput the current subHGend ifoutput: partitioned HGmany applications, e.g.
image segmentation (Shi& Malik, 2000).We adopt two variants of spectral clustering,recursive 2-way partitioning (R2 partitioner) andflat-K partitioning.
Since flat-K partitioning didnot perform as well we focus here on recursive 2-way partitioning.
In contrast to flat-K partitioning,this method does not need any information aboutthe number of target sets.
Instead a stopping cri-terion ??
has to be provided.
??
is adjusted ondevelopment data (see Algorithm 1).In order to apply spectral clustering to hyper-graphs we follow Agarwal et al (2005).
All ex-perimental results are obtained using symmetricLaplacians (Lsym) (von Luxburg, 2007).Given a hypergraph HG, a set of matrices isgenerated.
Dv and De denote the diagonal matri-ces containing the vertex and hyperedge degreesrespectively.
|V | ?
|E| matrix H represents theHG with the entries h(v, e) = 1 if v ?
e and 0otherwise.
HT is the transpose of H .
W is thediagonal matrix with the edge weights.
S is oneof the subhypergraphs generated from a cut in theHG, where Ncut(S) is the cut?s value.Using Normalized Cut does not generate sin-gleton clusters, hence a heuristic singleton detec-tion strategy is used in COPA.
We apply a thresh-old ?
to each node in the graph.
If a node?s degreeis below the threshold, the node will be removed.3.3 Complexity of HGResolverSince edge weights are assigned using simple de-scriptive statistics, the time HGResolver needs forbuilding the graph Laplacian matrix is insubstan-tial.
For eigensolving, we use an open source li-brary provided by the Colt project1which imple-ments a Householder-QL algorithm to solve theeigenvalue decomposition.
When applied to thesymmetric graph Laplacian, the complexity of theeigensolving is given by O(n3), where n is thenumber of mentions in a hypergraph.
Since thereare only a few hundred mentions per document inour data, this complexity is not an issue (spectralclustering gets problematic when applied to mil-lions of data points).4 FeaturesThe HGModelBuilder allows hyperedges with adegree higher than two to grow throughout thebuilding process.
This type of edge is mergeable.Edges with a degree of two describe pairwise rela-tions.
Thus these edges are non-mergeable.
Thisway any kind of relational features can be incor-porated into the hypergraph model.Features are represented as types of hyperedges(in Figure (1b) the two hyperedges marked by ?????
are of the same type).
Any realized edge is aninstance of the corresponding edge type.
All in-stances derived from the same type have the sameweight, but they may get reweighted by the dis-tance feature (Section 4.4).In the following Subsections we describe thefeatures used in our experiments.
We use the en-tire set for obtaining the final results.
We restrictourselves to Soon et al (2001)?s features when wecompare our system with theirs in order to assessthe impact of our model regardless of features (weuse features 1., 2., 3., 6., 7., 11., 13.
).4.1 Hyperedges With a Degree > 2High degree edges are the particular property ofthe hypergraph which allows to include all typesof relational features into our model.
The edgesare built through pairwise relations and, if consis-tent, get incrementally merged into larger edges.1http://acs.lbl.gov/?hoschek/colt/146High degree edges are not sensitive to positionalinformation from the documents.
(1) StrMatch Npron & (2) StrMatch Pron:After discarding stop words, if the strings of men-tions completely match and are not pronouns, theyare put into edges of the StrMatch Npron type.When the matched mentions are pronouns, theyare put into the StrMatch Pron type edges.
(3) Alias: After discarding stop words, if men-tions are aliases of each other (i.e.
proper nameswith partial match, full names and acronyms oforganizations, etc.
), they are put into edges of theAlias type.
(4) Synonym: If, according to WordNet, men-tions are synonymous, they are put into an edge ofthe Synonym type.
(5) AllSpeak: Mentions which appear within awindow of two words of a verb meaning to sayform an edge of the AllSpeak type.
(6) Agreement: If mentions agree in Gender,Number and Semantic Class they are put in edgesof the Agreement type.
Because Gender, Num-ber and Semantic Class are strong negative coref-erence indicators ?
in contrast to e.g.
StrMatch ?and hence weak positive features, they are com-bined into the one feature Agreement.4.2 Hyperedges With a Degree = 2Features which have been used by pairwise mod-els are easily integrated into the hypergraph modelby generating edges with only two vertices.
Infor-mation sensitive to relative distance is representedby pairwise edges.
(7) Apposition & (8) RelativePronoun: If twomentions are in a appositive structure, they are putin an edge of type Apposition.
If the latter mentionis a relative pronoun, the mentions are put in anedge of type RelativePronoun.
(9) HeadModMatch: If the syntactic heads oftwo mentions match, and if their modifiers do notcontradict each other, the mentions are put in anedge of type HeadModMatch.
(10) SubString: If a mention is the substringof another one, they are put into an edge of typeSubString.4.3 MentionType and EntityTypeIn our model (11) mention type can only reason-ably be used when it is conjoined with other fea-tures, since mention type itself describes an at-tribute of single mentions.
In COPA, it is con-joined with other features to form hyperedges, e.g.the StrMatch Pron edge.
We use the same strat-egy to represent (12) entity type.4.4 Distance WeightsOur hypergraph model does not have any obvi-ous means to encode distance information.
How-ever, the distance between two mentions playsan important role in coreference resolution, es-pecially for resolving pronouns.
We do not en-code distance as feature, because this would intro-duce many two-degree-hyperedges which wouldbe computationally very expensive without muchgain in performance.
Instead, we use distance toreweight two-degree-hyperedges, which are sen-sitive to positional information.We experimented with two types of distanceweights: One is (13) sentence distance as used inSoon et al (2001)?s feature set, while the other is(14) compatible mentions distance as introducedby Bengtson & Roth (2008).5 ExperimentsWe compare COPA?s performance with two im-plementations of pairwise models.
The first base-line is the BART (Versley et al, 2008) reimple-mentation of Soon et al (2001), with few but ef-fective features.
Our second baseline is Bengtson& Roth (2008), which exploits a much larger fea-ture set while keeping the machine learning ap-proach simple.
Bengtson & Roth (2008) showthat their system outperforms much more sophis-ticated machine learning approaches such as Cu-lotta et al (2007), who reported the best resultson true mentions before Bengtson & Roth (2008).Hence, Bengtson & Roth (2008) seems to be a rea-sonable competitor for evaluating COPA.In order to report realistic results, we neitherassume true mentions as input nor do we evalu-ate only on true mentions.
Instead, we use an in-house mention tagger for automatically extractingmentions.1475.1 DataWe use the MUC6 data (Chinchor & Sund-heim, 2003) with standard training/testing divi-sions (30/30) as well as the MUC7 data (Chin-chor, 2001) (30/20).
Since we do not have ac-cess to the official ACE testing data (only avail-able to ACE participants), we follow Bengtson &Roth (2008) for dividing the ACE 2004 Englishtraining data (Mitchell et al, 2004) into training,development and testing partitions (268/76/107).We randomly split the 252 ACE 2003 trainingdocuments (Mitchell et al, 2003) using the sameproportions into training, development and testing(151/38/63).
The systems were tuned on develop-ment and run only once on testing data.5.2 Mention TaggerWe implement a classification-based mention tag-ger, which tags each NP chunk as ACE mention ornot, with neccessary post-processing for embed-ded mentions.
For the ACE 2004 testing data, wecover 75.8% of the heads with 73.5% accuracy.5.3 Evaluation MetricsWe evaluate COPA with three coreference resolu-tion evaluation metrics: the B3-algorithm (Bagga& Baldwin, 1998), the CEAF-algorithm (Luo,2005), and, for the sake of completeness, theMUC-score (Vilain et al, 1995).Since the MUC-score does not evaluate single-ton entities, it only partially evaluates the perfor-mance for ACE data, which includes singletonentities in the keys.
The B3-algorithm (Bagga& Baldwin, 1998) addresses this problem of theMUC-score by conducting calculations based onmentions instead of coreference relations.
How-ever, another problematic issue emerges whensystem mentions have to be dealt with: B3 as-sumes the mentions in the key and in the responseto be identical, which is unlikely when a men-tion tagger is used to create system mentions.The CEAF-algorithm aligns entities in key andresponse by means of a similarity metric, whichis motivated by B3?s shortcoming of using oneentity multiple times (Luo, 2005).
However, al-though CEAF theoretically does not require tohave the same number of mentions in key andresponse, the algorithm still cannot be directlyapplied to end-to-end coreference resolution sys-tems, because the similarity metric is influencedby the number of mentions in key and response.Hence, both the B3- and CEAF-algorithmshave to be extended to deal with system mentionswhich are not in the key and true mentions notextracted by the system, so called twinless men-tions (Stoyanov et al, 2009).
Two variants ofthe B3-algorithm are proposed by Stoyanov et al(2009), B3all and B30 .
B3all tries to assign intu-itive precision and recall to the twinless systemmentions and twinless key mentions, while keep-ing the size of the system mention set and the keymention set unchanged (which are different fromeach other).
For twinless mentions, B3all discardstwinless key mentions for precision and twinlesssystem mentions for recall.
Discarding parts ofthe key mentions, however, makes the fair com-parison of precision values difficult.
B30 producescounter-intuitive precision by discarding all twin-less system mentions.
Although it penalizes therecall of all twinless key mentions, so that the F-scores are balanced, it is still too lenient (for fur-ther analyses see Cai & Strube (2010)).We devise two variants of the B3- and CEAF-algorithms, namely B3sys and CEAFsys.
For com-puting precision, the algorithms put all twinlesstrue mentions into the response even if they werenot extracted.
All twinless system mentions whichwere deemed not coreferent are discarded.
Onlytwinless system mentions which were mistakenlyresolved are put into the key.
Hence, the systemis penalized for resolving mentions not found inthe key.
For recall the algorithms only considermentions from the original key by discarding allthe twinless system mentions and putting twin-less true mentions into the response as singletons(algorithm details, simulations and comparison ofdifferent systems and metrics are provided in Cai& Strube (2010)).
For CEAFsys, ?3 (Luo, 2005)is used.
B3sys and CEAFsys report results for end-to-end coreference resolution systems adequately.5.4 BaselinesWe compare COPA?s performance with two base-lines: SOON ?
the BART (Versley et al, 2008)reimplementation of Soon et al (2001) ?
and148SOON COPA with R2 partitionerR P F R P F ??
?MUC MUC6 59.4 67.9 63.4 62.8 66.4 64.5 0.08 0.03MUC7 52.3 67.1 58.8 55.2 66.1 60.1 0.05 0.01ACE 2003 56.7 75.8 64.9 60.8 75.1 67.2 0.07 0.03ACE 2004 50.4 67.4 57.7 54.1 67.3 60.0 0.05 0.04B3sys MUC6 53.1 78.9 63.5 56.4 76.3 64.1 0.08 0.03MUC7 49.8 80.0 61.4 53.3 76.1 62.7 0.05 0.01ACE 2003 66.9 87.7 75.9 71.5 83.3 77.0 0.07 0.03ACE 2004 64.7 85.7 73.8 67.3 83.4 74.5 0.07 0.03CEAFsys MUC6 56.9 53.0 54.9 62.2 57.5 59.8 0.08 0.03MUC7 57.3 54.3 55.7 58.3 54.2 56.2 0.06 0.01ACE 2003 71.0 68.7 69.8 71.1 68.3 69.7 0.07 0.03ACE 2004 67.9 65.2 66.5 68.5 65.5 67.0 0.07 0.03Table 3: SOON vs. COPA R2 (SOON features, system mentions, bold indicates significant improvementin F-score over SOON according to a paired-t test with p < 0.05)SOON B&RR P F R P FB3sys 64.7 85.7 73.8 66.3 85.8 74.8Table 2: Baselines on ACE 2004B&R ?
Bengtson & Roth (2008)2.
All systemsshare BART?s preprocessing components and ourin-house ACE mention tagger.In Table 2 we report the performance of SOONand B&R on the ACE 2004 testing data usingthe BART preprocessing components and our in-house ACE mention tagger.
For evaluation we useB3sys only, since Bengtson & Roth (2008)?s sys-tem does not allow to easily integrate CEAF.B&R considerably outperforms SOON (we can-not compute statistical significance, because wedo not have access to results for single documentsin B&R).
The difference, however, is not as bigas we expected.
Bengtson & Roth (2008) re-ported very good results when using true men-tions.
For evaluating on system mentions, how-ever, they were using a too lenient variant of B3(Stoyanov et al, 2009) which discards all twinlessmentions.
When replacing this with B3sys the dif-ference between SOON and B&R shrinks.5.5 ResultsIn both comparisons, COPA uses the same fea-tures as the corresponding baseline system.2http://l2r.cs.uiuc.edu/?cogcomp/asoftware.php?skey=FLBJCOREF5.5.1 COPA vs. SOONIn Table 3 we compare the SOON-baseline withCOPA using the R2 partitioner (parameters ??
and?
optimized on development data).
Even thoughCOPA and SOON use the same features, COPAconsistently outperforms SOON on all data setsusing all evaluation metrics.
With the exception ofthe MUC7, the ACE 2003 and the ACE 2004 dataevaluated with CEAFsys, all of COPA?s improve-ments are statistically significant.
When evaluatedusing MUC and B3sys, COPA with the R2 parti-tioner boosts recall in all datasets while losing inprecision.
This shows that global hypergraph par-titioning models the coreference resolution taskmore adequately than Soon et al (2001)?s localmodel ?
even when using the very same features.5.5.2 COPA vs. B&RIn Table 4 we compare the B&R system (using ourpreprocessing components and mention tagger),and COPA with the R2 partitioner using B&R fea-tures.
COPA does not use the learned featuresfrom B&R, as this would have implied to embed apairwise coreference resolution system in COPA.We report results for ACE 2003 and ACE 2004.The parameters are optimized on the ACE 2004data.
COPA with the R2 partitioner outperformsB&R on both datasets (we cannot compute statisti-cal significance, because we do not have access toresults for single documents in B&R).
Bengtson &Roth (2008) developed their system on ACE 2004data and never exposed it to ACE 2003 data.
Wesuspect that the relatively poor result of B&R onACE 2003 data is caused by overfitting to ACE149B&R COPA with R2 partitionerR P F R P FB3sys ACE 2003 56.4 97.3 71.4 70.3 86.5 77.5ACE 2004 66.3 85.8 74.8 68.4 84.4 75.6Table 4: B&R vs. COPA R2 (B&R features, system mentions)2004.
Again, COPA gains in recall and losesin precision.
This shows that COPA is a highlycompetetive system as it outperforms Bengtson &Roth (2008)?s system which has been claimed tohave the best performance on the ACE 2004 data.5.5.3 Running TimeOn a machine with 2 AMD Opteron CPUs and 8GB RAM, COPA finishes preprocessing, trainingand partitioning the ACE 2004 dataset in 15 min-utes, which is slightly faster than our duplicatedSOON baseline.6 Discussion and OutlookMost previous attempts to solve the coreferenceresolution task globally have been hampered byemploying a local pairwise model in the classifi-cation step (step 1) while only the clustering steprealizes a global approach, e.g.
Luo et al (2004),Nicolae & Nicolae (2006), Klenner (2007), De-nis & Baldridge (2009), lesser so Culotta et al(2007).
It has been also observed that improve-ments in performance on true mentions do notnecessarily translate into performance improve-ments on system mentions (Ng, 2008).In this paper we describe a coreference reso-lution system, COPA, which implements a globaldecision in one step via hypergraph partitioning.COPA looks at the whole graph at once which en-ables it to outperform two strong baselines (Soonet al, 2001; Bengtson & Roth, 2008).
COPA?shypergraph-based strategy can be taken as a gen-eral preference model, where the preference forone mention depends on information on all othermentions.We follow Stoyanov et al (2009) and arguethat evaluating the performance of coreferenceresolution systems on true mentions is unrealis-tic.
Hence we integrate an ACE mention tag-ger into our system, tune the system towards thereal task, and evaluate only using system men-tions.
While Ng (2008) could not show that su-perior models achieved superior results on sys-tem mentions, COPA was able to outperformBengtson & Roth (2008)?s system which has beenclaimed to achieve the best performance on theACE 2004 data (using true mentions, Bengtson &Roth (2008) did not report any comparison withother systems using system mentions).An error analysis revealed that there were somecluster-level inconsistencies in the COPA output.Enforcing this consistency would require a globalstrategy to propagate constraints, so that con-straints can be included in the hypergraph parti-tioning properly.
We are currently exploring con-strained clustering, a field which has been veryactive recently (Basu et al, 2009).
Using con-strained clustering methods may allow us to in-tegrate negative information as constraints insteadof combining several weak positive features to onewhich is still weak (e.g.
our Agreement feature).For an application of constrained clustering to therelated task of database record linkage, see Bhat-tacharya & Getoor (2009).Graph models cannot deal well with positionalinformation, such as distance between mentionsor the sequential ordering of mentions in a doc-ument.
We implemented distance as weights onhyperedges which resulted in decent performance.However, this is limited to pairwise relations andthus does not exploit the power of the high de-gree relations available in COPA.
We expect fur-ther improvements, once we manage to includepositional information directly.Acknowledgements.
This work has beenfunded by the Klaus Tschira Foundation, Hei-delberg, Germany.
The first author has beensupported by a HITS PhD.
scholarship.
We wouldlike to thank Byoung-Tak Zhang for bringinghypergraphs to our attention and `Eva Mu?jdricza-Maydt for implementing the mention tagger.Finally we would like to thank our colleagues inthe HITS NLP group for providing us with usefulcomments.150ReferencesAgarwal, Sameer, Jonwoo Lim, Lihi Zelnik-Manor, PietroPerona, David Kriegman & Serge Belongie (2005).
Be-yond pairwise clustering.
In Proceedings of the IEEEComputer Society Conference on Computer Vision andPattern Recognition (CVPR?05), Vol.
2, pp.
838?845.Bagga, Amit & Breck Baldwin (1998).
Algorithms for scor-ing coreference chains.
In Proceedings of the 1st Inter-national Conference on Language Resources and Evalu-ation, Granada, Spain, 28?30 May 1998, pp.
563?566.Basu, Sugato, Ian Davidson & Kiri L. Wagstaff (Eds.)(2009).
Constrained Clustering: Advances in Algorithms,Theory, and Applications.
Boca Raton, Flo.
: CRC Press.Bengtson, Eric & Dan Roth (2008).
Understanding the valueof features for coreference resolution.
In Proceedings ofthe 2008 Conference on Empirical Methods in NaturalLanguage Processing, Waikiki, Honolulu, Hawaii, 25-27October 2008, pp.
294?303.Bhattacharya, Indrajit & Lise Getoor (2009).
Collective re-lational clustering.
In S. Basu, I. Davidson & K.
Wagstaff(Eds.
), Constrained Clustering: Advances in Algorithms,Theory, and Applications, pp.
221?244.
Boca Raton, Flo.
:CRC Press.Cai, Jie & Michael Strube (2010).
Evaluation metrics forend-to-end coreference resolution systems.
In Proceed-ings of the SIGdial 2010 Conference: The 11th AnnualMeeting of the Special Interest Group on Discourse andDialogue, Tokyo, Japan, 24?25 September 2010.
To ap-pear.Chinchor, Nancy (2001).
Message Understanding Confer-ence (MUC) 7.
LDC2001T02, Philadelphia, Penn: Lin-guistic Data Consortium.Chinchor, Nancy & Beth Sundheim (2003).
Message Under-standing Conference (MUC) 6.
LDC2003T13, Philadel-phia, Penn: Linguistic Data Consortium.Culotta, Aron, Michael Wick & Andrew McCallum (2007).First-order probabilistic models for coreference resolu-tion.
In Proceedings of Human Language Technologies2007: The Conference of the North American Chapter ofthe Association for Computational Linguistics, Rochester,N.Y., 22?27 April 2007, pp.
81?88.Daume?
III, Hal & Daniel Marcu (2005).
A large-scale ex-ploration of effective global features for a joint entity de-tection and tracking model.
In Proceedings of the HumanLanguage Technology Conference and the 2005 Confer-ence on Empirical Methods in Natural Language Process-ing, Vancouver, B.C., Canada, 6?8 October 2005, pp.
97?104.Denis, Pascal & Jason Baldridge (2009).
Global joint modelsfor coreference resolution and named entity classification.Procesamiento del Lenguaje Natural, 42:87?96.Klenner, Manfred (2007).
Enforcing consistency on coref-erence sets.
In Proceedings of the International Confer-ence on Recent Advances in Natural Language Process-ing, Borovets, Bulgaria, 27?29 September 2007, pp.
323?328.Luo, Xiaoqiang (2005).
On coreference resolution perfor-mance metrics.
In Proceedings of the Human LanguageTechnology Conference and the 2005 Conference on Em-pirical Methods in Natural Language Processing, Van-couver, B.C., Canada, 6?8 October 2005, pp.
25?32.Luo, Xiaoqiang, Abe Ittycheriah, Hongyan Jing, NandaKambhatla & Salim Roukos (2004).
A mention-synchronous coreference resolution algorithm based onthe Bell Tree.
In Proceedings of the 42nd Annual Meet-ing of the Association for Computational Linguistics,Barcelona, Spain, 21?26 July 2004, pp.
136?143.Mitchell, Alexis, Stephanie Strassel, Shudong Huang &Ramez Zakhary (2004).
ACE 2004 Multilingual TrainingCorpus.
LDC2005T09, Philadelphia, Penn.
: LinguisticData Consortium.Mitchell, Alexis, Stephanie Strassel, Mark Przybocki,JK Davis, George Doddington, Ralph Grishman, AdamMeyers, Ada Brunstain, Lisa Ferro & Beth Sundheim(2003).
TIDES Extraction (ACE) 2003 MultilingualTraining Data.
LDC2004T09, Philadelphia, Penn.
: Lin-guistic Data Consortium.Ng, Vincent (2008).
Unsupervised models for corefer-ence resolution.
In Proceedings of the 2008 Conferenceon Empirical Methods in Natural Language Processing,Waikiki, Honolulu, Hawaii, 25-27 October 2008, pp.
640?649.Ng, Vincent & Claire Cardie (2002).
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the 40th Annual Meeting of the Associationfor Computational Linguistics, Philadelphia, Penn., 7?12July 2002, pp.
104?111.Nicolae, Cristina & Gabriel Nicolae (2006).
BestCut: Agraph algorithm for coreference resolution.
In Proceed-ings of the 2006 Conference on Empirical Methods in Nat-ural Language Processing, Sydney, Australia, 22?23 July2006, pp.
275?283.Rahman, Altaf & Vincent Ng (2009).
Supervised modelsfor coreference resolution.
In Proceedings of the 2009Conference on Empirical Methods in Natural LanguageProcessing, Singapore, 6-7 August 2009, pp.
968?977.Shi, Jianbo & Jitendra Malik (2000).
Normalized cuts andimage segmentation.
IEEE Transactions on Pattern Anal-ysis and Machine Intelligence, 22(8):888?905.Soon, Wee Meng, Hwee Tou Ng & Daniel Chung YongLim (2001).
A machine learning approach to coreferenceresolution of noun phrases.
Computational Linguistics,27(4):521?544.Stoyanov, Veselin, Nathan Gilbert, Claire Cardie & EllenRiloff (2009).
Conundrums in noun phrase coreferenceresolution: Making sense of the state-of-the-art.
In Pro-ceedings of the Joint Conference of the 47th Annual Meet-ing of the Association for Computational Linguistics andthe 4th International Joint Conference on Natural Lan-guage Processing, Singapore, 2?7 August 2009, pp.
656?664.Versley, Yannick, Simone Paolo Ponzetto, Massimo Poesio,Vladimir Eidelman, Alan Jern, Jason Smith, XiaofengYang & Alessandro Moschitti (2008).
BART: A mod-ular toolkit for coreference resolution.
In CompanionVolume to the Proceedings of the 46th Annual Meetingof the Association for Computational Linguistics, Colum-bus, Ohio, 15?20 June 2008, pp.
9?12.Vilain, Marc, John Burger, John Aberdeen, Dennis Connolly& Lynette Hirschman (1995).
A model-theoretic corefer-ence scoring scheme.
In Proceedings of the 6th MessageUnderstanding Conference (MUC-6), pp.
45?52.
San Ma-teo, Cal.
: Morgan Kaufmann.von Luxburg, Ulrike (2007).
A tutorial on spectral clustering.Statistics and Computing, 17(4):395?416.Yang, Xiaofeng, Jian Su & Chew Lim Tan (2008).
A twin-candidate model for learning-based anaphora resolution.Computational Linguistics, 34(3):327?356.151
