Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 180?187,Atlanta, Georgia, June 13 2013. c?2013 Association for Computational LinguisticsNative Language Identification with PPMVictoria BobicevTechnical University of Moldova168, Stefan Cel Mare bvd.Chisin?u, MD2004 Republic of Moldovavictoria_bobicev@rol.mdAbstractThis paper reports on our work in the NLIshared task 2013 on Native Language Identi-fication.
The task is to automatically detectthe native language of the TOEFL essays au-thors in a set of given test documents in Eng-lish.
The task was solved by a system thatused the PPM compression algorithm basedon an n-gram statistical model.
We submittedfour runs; word-based PPMC algorithm withnormalization and without, character-basedPPMC algorithm with normalization andwithout.
The worst result was obtained ontraining and testing data during the evaluationprocedure using the character-based PPMmethod and normalization: accuracy = 31.9%;the best one was macroaverage F-measure =0.708 with the word-based PPMC algorithmwithout normalization.1 IntroductionWith the emergence of user-generated web con-tent, text author profiling is being increasinglystudied by the NLP community.
Various worksdescribe experiments aiming to automatically dis-cover hidden attributes of text which reveal au-thor?s gender, age, personality and others.
WhileEnglish remains one of the main global languagesused for communication, interchange of infor-mation and ideas, English texts written by differentlanguage speakers differ considerably.
This is yetanother characteristic of the author that can belearned from a text.
While a great number of workshave presented investigations in this area there wasno common ground to evaluate different tech-niques and approaches to Native Language Identi-fication.
NLI shared task 2013 on Native LanguageIdentification provides a playground and a corpusfor such an evaluation.We participated in this shared task with the PPMcompression algorithm based on a character-basedand word-based n-gram statistical model.2 Related workThe task of Native Language Identification is toautomatically detect text?s author?s native lan-guage when having only English text written bythis author.
It is generally a sub-task of text classi-fication or, more closely, text author profilingwhen various stylometric text features are used forcertain author?s characteristics (gender, age, educa-tion, cultural background, etc.)
detection (Bergsmaet al 2012; Argamon et al 2009).This task is mostly solved by machine-learningalgorithms, such as SVM (Witten and Frank,2005).
However, the algorithm itself is not themost influential choice for better performance butrather the set of features used for learning.
This setcan consist of character, word and PoS n-grams,functional words, punctuation, specific errors, syn-tactic structures, and others.
Some works investi-gate the influence of thousands of features of verydifferent types (Koppel et al 2011; Abbasi andChen, 2008).
Extraction of all these features re-quires a substantial amount of text processingwork.
We, instead, concentrated on an easiermethod, namely, PPM, a statistical model used fortext compression which almost needs no text pre-processing.Several approaches that apply compression modelsto text classification have been presented in Eibe et180al.
(2000); Thaper (1996).
The underlying idea ofusing compression methods for text classificationwas their ability to create a language modeladapted to particular texts.
It was hypothesized thatthis model captures individual features of the textbeing modelled.
Theoretical background to thisapproach was given in Teahan and Harper (2001).3 System descriptionDetection of the English text author?s native lan-guage can be viewed as a type of classificationtask.
Such tasks are solved using learning methods.There are different types of text classification.
Au-thorship attribution, spam filtering, dialect identifi-cation are just several of the purposes of textcategorization.
It is natural that for different typesof categorization different methods are pertinent.The most common type is the content-based cate-gorization which classifies texts by their topic andrequires the most common classification methodsbased on classical set of features.
More specificmethods are necessary in cases when classificationcriterions are not so obvious, for example, in thecase of author identification.In this paper the application of the PPM (Predic-tion by Partial Matching) model for automatic textclassification is explored.
Prediction by partialmatching (PPM) is an adaptive finite-contextmethod for text compression that is a back-offsmoothing technique for finite-order Markov mod-els (Bratko et al 2006).
It obtains all informationfrom the original data, without feature engineering,is easy to implement and relatively fast.
PPM pro-duces a language model and can be used in a prob-abilistic text classifier.PPM is based on conditional probabilities of theupcoming symbol given several previous symbols(Cleary and Witten, 1984).
The PPM techniqueuses character context models to build an overallprobability distribution for predicting upcomingcharacters in the text.
A blending strategy for com-bining context predictions is to assign a weight toeach context model, and then calculate theweighted sum of the probabilities:mP(x) = ?
?i pi(x), (1)i=1where?i and pi are weights and probabilities assignedto each order i (i=1?m).For example, the probability of character 'm' incontext of the word 'algorithm' is calculated as asum of conditional probabilities dependent on dif-ferent context lengths up to the limited maximallength:PPPM('m') = ?5 ?
P( 'm' | 'orith') + ?4 ?
P( 'm' | 'rith')+ ?3 ?
P( 'm' | 'ith') + ?2 ?
P( 'm' | 'th') ++ ?1 ?
P( 'm' | 'h') + + ?0 ?
P( 'm' ) ++ ?-1 ?
P( ?esc?
), (2)where?i (i = 1?5) is the normalization weight;5 - maximal length of the context;P( ?esc? )
?
?escape?
probability, the proba-bility of an unknown character.PPM is a special case of the general blending strat-egy.
The PPM models use an escape mechanism tocombine the predictions of all character contexts oflength m, where m is the maximum model order;the order 0 model predicts symbols based on theirunconditioned probabilities, the default order -1model ensures that a finite probability (howeversmall) is assigned to all possible symbols.
ThePPM escape mechanism is more practical to im-plement than weighted blending.
There are severalversions of the PPM algorithm depending on theway the escape probability is estimated.
In our im-plementation, we used the escape method C (Bellet al 1989), named PPMC.
Treating a text as astring of characters, a character-based PPM avoidsdefining word boundaries; it deals with differenttypes of documents in a uniform way.
It can workwith texts in any language and be applied to di-verse types of classification; more details can befound in Bobicev (2007).
Our utility function fortext classification was cross-entropy of the testdocument:nHd m - = ?
pm(xi) log pm(xi), (3)i=1wheren is the number of symbols in a text d,Hd m ?
entropy of the text d obtained by model m,pm(xi) is a probability of a symbol xi in the text d.Hd m was estimated by the modelling part of thecompression algorithm.Usually, the cross-entropy is greater than theentropy, because the probabilities of symbols indiverse texts are different.
The cross-entropy canbe used as a measure for document similarity; thelower cross-entropy for two texts is, the more simi-181lar they are.
Hence, if several statistical models hadbeen created using documents that belong to dif-ferent classes and cross-entropies are calculated foran unknown text on the basis of each model, thelowest value of cross-entropy will indicate theclass of the unknown text.
In this way cross-entropy is used for text classification.On the training step, we created PPM modelsfor each class of documents; on the testing step, weevaluated cross-entropy of previously unseen textsusing models for each class.
The lowest value ofcross-entropy indicates the class of the unknowntext.The maximal length of a context equal to 5 inPPM model was proven to be optimal for textcompression (Teahan, 1998).
In other experiments,length of character n-grams used for text classifica-tion varied from 2 (Kukushkina et al 2001) to 4(Koppel et al 2011) or a combination of severallengths (Keselj et al 2003).
Stamatatos (2009)pointed out that the best length of character n-grams depends on different conditions and variesfor different texts.
In all our experiments withcharacter-based PPM model we used maximallength of a context equal to 5; thus our method isPPMC5.The character-based PPM models were used forspam detection, source-based text classificationand classification of multi-modal data streams thatincluded texts.
In Bratko et al(2006), the charac-ter-based PPM models were used for spam detec-tion.
In this task there existed two classes only:spam and legitimate email (ham).
The createdmodels showed strong performance in the TextRetrieval Conference competition, indicating thatdata-compression models are well suited to thespam filtering problem.
In Teahan (2000), a PPM-based text model and minimum cross-entropy as atext classifier were used for various tasks; one ofthem was an author detection task for the wellknown Federalist Papers.
In Bobicev and Sokolova(2008), the PPM algorithm was applied to textcategorization in two ways: on the basis of charac-ters and on the basis of words.
Character-basedmethods performed almost as well as SVM, thebest method among several machine learningmethods compared in Debole and Sebastiani(2004) for the Reuters-21578 corpus.Usually, PPM models are character-based.However, word-based models were also used forvarious purposes.
For example, if texts are classi-fied by the contents, they are better characterizedby words and word combinations than by frag-ments consisting of five letters.
For some taskswords can be more indicative text features thancharacter sequences.
That?s why we decided to useboth character-based and word-based models forPPM text classification.
In the case of word-basedPPM, the context is only one word and an examplefor formula (1) looks like the following:PPPM( ' wordi ') = ?1 ?
P( ' wordi ' | ' wordi-1 ') ++ ?0?
P( ' wordi ' ) + ?-1 ?
P( ?esc?
),wherewordi is the current word;wordi-1 is the previous word.This model is coded as PPMC1 because of thesame C escape method and one length context usedfor probability estimation.Training and testing data is distributed quite un-evenly in many tasks, for example, in Reuters-21578 corpus.
This imbalance drastically affectedthe results of the classification experiments; theclassification was biased towards classes with alarger volume of data for training.
Such imbalanceclass distribution problems were mentioned in Bo-bicev and Sokolova (2008), Stamatatos (2009),Narayanan et al(2012).
Considering the fact thatunbalanced data affected classification results insuch a substantial way we used a normalizationprocedure for balancing entropies of the statisticaldata models.The first step of our algorithm was training.
Inthe process of training, statistical models for eachclass of texts were created.
This meant that prob-abilities of text elements were estimated.
The nextstep after training was calculation of entropies oftest documents on the basis of each class model.We obtained a matrix of entropies ?class statisticalmodels x test documents?.
The columns were en-tropies for the class statistical models and rowswere entropies for a given test documents.
Afterthis step the normalization procedure was applied.The procedure consisted of several steps.
(1) Mean entropy for each class of texts wascalculated on the base of the matrix;(2) Each value in the matrix was divided by themean entropy for this class.
Thereby we obtainedmore balanced values and classification improvedconsiderably.Although the application of PPM model to thedocument classification is not new, PPM was never182applied to the task of English text author?s nativelanguage detection.In order to evaluate the PPM classificationmethod for English text author?s native languageidentification a number of experiments were per-formed.
The aim of the experiments was twofold:- to evaluate the quality of PPM-based docu-ment classification;- to compare letter-based and word-based PPMclassification.4 EvaluationThree sets of experiments were carried out duringthe NLI shared task event.
The first one was per-formed on the training and development data re-leased in January.
The second set consisted ofevaluation runs on test data released in March andthe results for these experiments were provided bythe organizers.
The third set was 10-fold cross-validation on training + development data request-ed by the organizers.4.1 The First set of experimentsThe first set of experiments was carried out on thefirst set of data released by the organizers: TOEFLessays written by 11 native languages speakers.9,900 essays of this set were sequestered as thetraining data and 1,100 were for the developmentset.
Thus, we trained our model on 900 files foreach native language speakers, for each class.Next, we attributed classes to 1,100 developmenttexts.
We carried out four experiments.
The firsttwo were done on the basis of the character-basedPPMC5 method with and without the normaliza-tion procedure described earlier.
The second twoexperiments were done with the word-basedPPMC1 method with and without the normaliza-tion.
The Precision, Recall and F-measure for thesefour experiments are presented in Table 1.
Tables 2and 3 are confusion tables for the worst and for thebest cases of the four experiments.Model Microaverage F-score Precision RecallMacroaverage F-scoreCharacter-based PPMC5 method withoutnormalization 0.382 0.384 0.382 0.383Character-based PPMC5 method withnormalization 0.362 0.363 0.362 0.3625Word-based PPMC1 method without nor-malization 0.701 0.715 0.701 0.708Word-based PPMC1 method with normali-zation 0.687 0.702 0.687 0.695Table 1.
Results obtained on character-based and letter-based PPM models with and without normalization.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURARA 26 7 9 3 6 5 14 6 8 12 4CHI 3 32 8 7 3 3 20 13 4 4 3FRE 6 4 32 8 9 13 7 3 4 8 6GER 1 6 10 36 3 10 8 7 6 5 8HIN 2 3 4 5 36 7 6 3 1 29 4ITA 5 3 16 6 2 45 1 4 10 4 4JPN 3 14 2 3 2 6 49 13 5 1 2KOR 2 6 5 5 2 3 21 42 1 8 5SPA 3 4 8 8 3 19 13 5 25 9 3TEL 1 5 0 4 18 2 4 4 0 60 2TUR 5 9 9 9 8 5 17 11 3 9 15Table 2.
Confusion table for 1,100 development files for the first PPMC5 character-based experiment with normali-zation.183ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURARA 46 2 3 6 8 7 2 5 8 5 8CHI 1 67 1 2 1 0 7 9 3 1 8FRE 0 2 77 9 1 3 1 0 4 0 3GER 0 0 3 90 1 2 0 0 2 0 2HIN 0 0 1 2 69 0 0 0 2 26 0ITA 1 1 6 3 0 82 0 0 3 0 4JPN 1 7 1 5 0 0 65 15 1 1 4KOR 1 3 0 2 0 0 20 67 2 1 4SPA 1 1 7 10 2 9 1 1 62 0 6TEL 0 0 0 0 31 0 0 1 0 68 0TUR 0 0 2 7 7 0 2 0 2 2 78Table 3.
Confusion table for 1,100 development files for the first PPMC1 word-based experiment without normali-zation.4.2 The second set of experimentsThe second set of experiments was done on the1,100 test files during the evaluation phase of thechallenge.
The results of these experiments wereprovided by the organizers.
Again, we carried outfour experiments: character-based PPMC5 methodwith and without normalization and word-basedPPMC1 method with and without normalization.Confusion tables 4 and 5 presents the worst and thebest results.The overall accuracies for these experiments are:Character-based PPMC5 method without nor-malization - 37.4%;Character-based PPMC5 method with normali-zation - 31.9%;Word-based PPMC1 method without normaliza-tion - 62.5%;Word-based PPMC1 method with normalization- 62.2%.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 7 4 16 5 3 17 10 25 0 8 5 43.8% 7.0% 12.1%CHI 1 31 8 5 1 9 19 23 0 2 1 38.8% 31.0% 34.4%FRE 0 1 55 5 2 17 6 10 0 0 4 28.4% 55.0% 37.4%GER 2 2 18 33 2 15 8 15 0 3 2 40.7% 33.0% 36.5%HIN 0 6 20 9 15 7 15 14 0 11 3 36.6% 15.0% 21.3%ITA 1 1 16 3 1 58 7 8 2 1 2 32.8% 58.0% 41.9%JPN 0 2 7 0 0 8 57 24 1 0 1 29.2% 57.0% 38.6%KOR 2 15 8 0 1 4 27 37 1 2 3 18.5% 37.0% 24.7%SPA 0 8 21 9 1 18 19 14 8 1 1 66.7% 8.0% 14.3%TEL 1 5 8 6 13 6 12 10 0 35 4 55.6% 35.0% 42.9%TUR 2 5 17 6 2 18 15 20 0 0 15 36.6% 15.0% 21.3%Table 4.
Confusion table for 1,100 test files for the PPMC5 character-based experiment with normalization.The overall accuracy is 31.9%.184ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TUR Precision Recall F-measureARA 39 2 7 9 6 1 3 1 14 7 11 75.0% 39.0% 51.3%CHI 3 65 3 5 1 0 8 4 2 0 9 72.2% 65.0% 68.4%FRE 1 0 67 10 1 11 1 0 4 0 5 60.9% 67.0% 63.8%GER 0 0 4 92 1 0 0 0 2 0 1 63.4% 92.0% 75.1%HIN 0 1 3 2 64 0 0 1 12 11 6 58.7% 64.0% 61.2%ITA 1 1 10 10 0 71 0 0 4 0 3 70.3% 71.0% 70.6%JPN 1 4 1 1 2 1 66 15 1 1 7 63.5% 66.0% 64.7%KOR 2 9 3 2 3 0 22 50 2 0 7 61.0% 50.0% 54.9%SPA 1 2 9 12 2 15 0 4 51 1 3 48.1% 51.0% 49.5%TEL 1 3 0 0 27 0 1 0 8 54 6 73.0% 54.0% 62.1%TUR 3 3 3 2 2 2 3 7 6 0 69 54.3% 69.0% 60.8%Table 5.
Confusion table for 1,100 test files for the PPMC1 word-based experiment without normalization.The overall accuracy is 62.5%.Model Microaverage F-score Precision RecallMacroaverageF-scoreCharacter-based PPMC5 method without normaliza-tion 0.366 0.368 0.366 0.367Character-based PPMC5 method with normalization 0.353 0.366 0.353 0.359Word-based PPMC1 method without normalization 0.649 0.660 0.649 0.655Word-based PPMC1 method with normalization 0.640 0.652 0.640 0.640Table 6.
Results obtained on character-based and letter-based PPM models with and without normalization on thebasis of training + development data.ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURARA 22 7 13 1 1 11 18 10 7 6 4CHI 1 29 7 2 1 8 22 22 2 2 4FRE 6 4 40 8 4 9 10 7 7 2 3GER 3 3 15 26 3 15 14 9 4 4 4HIN 5 3 6 3 31 6 7 5 4 26 4ITA 4 4 10 9 3 42 15 6 4 0 3JPN 1 9 4 6 1 3 49 17 3 3 4KOR 1 7 7 2 5 4 37 29 3 1 4SPA 6 5 12 3 6 21 14 8 20 1 4TEL 5 1 5 2 16 6 9 9 1 43 3TUR 4 3 14 7 3 7 22 8 5 2 25Table 7.
Confusion table for the worst case in the third set of experiments; 10-fold cross-validation, fold 9, PPMC5character-based, with normalization.185ARA CHI FRE GER HIN ITA JPN KOR SPA TEL TURARA 40 3 9 5 5 7 5 4 8 4 10CHI 2 73 1 1 2 2 6 10 2 0 1FRE 0 2 70 9 2 4 1 2 6 1 3GER 0 0 2 87 3 1 0 1 5 0 1HIN 1 0 2 3 69 0 0 1 3 15 6ITA 0 1 11 10 3 72 1 0 2 0 0JPN 0 6 0 1 2 2 68 16 3 0 2KOR 1 5 3 1 3 0 16 63 5 0 3SPA 2 1 8 4 4 5 1 6 65 0 4TEL 1 1 0 1 25 0 1 1 2 66 2TUR 1 1 3 4 6 1 0 0 10 1 73Table 8.
Confusion table for the best case in the third set of experiments; 10-fold cross-validation, fold 3, PPMC1word-based, without normalization.4.3 The third set of experimentsThe third set of the experiments was done at theorganizers?
request on the basis of training + de-velopment data.
10-fold cross-validation was madeon this data with exactly the same splitting used inTetreault et al(2012).
The results of these experi-ments are presented in Table 6.
Tables 7 and 8 areconfusion tables for the worst and the best casesamong all 10 folds and four experiments.5 ConclusionThe task of identifying the native language of awriter based solely on a sample of their Englishwriting is an exiting and intriguing task.
It is a typeof text classification task; however it requires taskspecific features.
The PPM method presented inthis paper uses two types of features: (1) charactersequences of length from 5 characters and shorter,(2) words and bigrams of words.
This methodachieved lower results than methods which usedcarefully selected and adjusted feature sets.
Theadvantage of this method is its relative simplicityof use and ability to work with any text.Two interesting and surprising conclusions wehave drawn from these experiments: (1) normaliza-tion did not improve the results for this data; (2)word-based method performed much better thancharacter-based.
In most previous experimentswith PPM-based classification (Bobicev, 2007;Bobicev and Sokolova, 2008) we obtained inverseresults: character-based methods were much betterthan word-based.
The author recognition experi-ments showed the same, much better performanceof character-based methods.
The possible explana-tion is that the data for this experiment was cleanedand tokenized whereas the data in other experi-ments was much noisier which created problemsfor the word-based method.The same was with normalization.
The organizersprepared very well balanced data and there was noneed of normalization which helped to gain anoth-er 20-25% of accuracy on other data.ReferencesAbbasi A. and Chen H. 2008.
Writeprints: A stylometricapproach to identity-level identification and similar-ity detection in cyberspace, ACM Trans.
Inf.
Syst.,vol.
26, no.
2, pp.
7:1?7:29.Argamon S., Koppel M., Pennebaker J. W., and SchlerJ.
2009.
Automatically profiling the author of ananonymous text, Commun.
ACM, vol.
52, no.
2, pp.119?123.Bell, T., Witten, I. and Cleary, J.
1989.
Modeling fortext compression, ACM Comput.
Surv.
21(4):557?591.Bergsma, S., Post, M., and Yarowsky, D. 2012.Stylometric analysis of scientific articles, 2012 Con-ference of the North American Chapter of the Asso-ciation for Computational Linguistics: HumanLanguage Technologies, pages 327?337, Montr?al,Canada.
Association for Computational Linguistics.Bobicev, V. 2007.
Comparison of word-based and let-ter-based text classification, RANLP?07, 76?80.Bobicev V., Sokolova M. 2008.
An Effective and RobustMethod for Short Text Classification, Association forthe Advancement of Artificial Intelligence (AAAI-2008), Cohn (ed), AAAI Press, Chicago, USA.186Bratko, A., Cormack, G. V., Filipic, B., Lynam, T. R.,and Zupan, B.
2006.
Spam filtering using statisticaldata compression models, Journal of Machine Learn-ing Research 7:2673?2698.Cleary, J., and Witten, I.
1984.
Data compression usingadaptive coding and partial string matching, IEEETrans.
Commun.
32(4):396?402.Debole F.  and Sebastiani F. 2004.
An Analysis of theRelative Hardness of Reuters-21578 Subsets, Journalof the American Society for Information Science andTechnology, vol.
56, pp.
971?974.Eibe Frank, Chang Chui and Ian H. Witten.
2000.
Textcategorisation using compression models, DCC-00,IEEE Data Compression Conference.Keselj V., Peng F., Cercone N., and Thomas C. 2003.N-gram-based author profiles for authorship attribu-tion, PACLING ?03, Halifax, pp.
255?264.Koppel M., Schler J., and Argamon S. 2011.
Authorshipattribution in the wild, Lang Resources & Evaluation,vol.
45, no.
1, pp.
83?94.Kukushkina O. V., Polikarpov A.
A., and Khmelev D.V., 2001.
Using Literal and Grammatical Statisticsfor Authorship Attribution, Probl.
Inf.
Transm., vol.37, no.
2, pp.
172?184.Narayanan A., Paskov H., Gong N. Z., Bethencourt J.,Stefanov E., Shin E. C. R., and Song D. 2012.
On theFeasibility of Internet-Scale Author Identification, in2012 IEEE Symposium on Security and Privacy(SP), pp.
300 ?314.Stamatatos E. 2009.
A survey of modern authorshipattribution methods, J.
Am.
Soc.
Inf.
Sci.
Technol.,vol.
60, no.
3, pp.
538?556.Teahan W. J.
1998.
Modelling English text, PhD Thesis,University of Waikato, New Zealand.Teahan W. J., McNab R., Wen Y., and Witten I. H.2000.
A compression-based algorithm for Chineseword segmentation, Comput.
Linguist., vol.
26, no.
3,pp.
375?393.Teahan W. J. and Harper D. J.
2001.
Using compres-sion based language models for text categorization,in J. Callan, B. Croft and J. Lafferty, editors, Work-shop on Language Modeling and Information Re-trieval, pages 83-88.
ARDA, Carnegie MellonUniversity.Tetreault J., Blanchard D., Cahill A., Chodorow M.2012.
Native Tongues, Lost and Found, Resourcesand Empirical Evaluations in Native Language Iden-tification, COLING 2012.Thaper N. 1996.
Using Compression For Source BasedClassification Of Text.
Bachelor of Technology(Computer Science and Engineering), Indian Instituteof Technology, Delhi, India.187
