Proceedings of NAACL HLT 2009: Short Papers, pages 41?44,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsTowards Automatic Image Region Annotation - Image Region TextualCoreference ResolutionEmilia ApostolovaCollege of Computing and Digital MediaDePaul UniversityChicago, IL 60604, USAemilia.aposto@gmail.comDina Demner-FushmanCommunications Engineering BranchNational Library of MedicineBethesda, MD 20894, USAddemner@mail.nih.govAbstractDetailed image annotation necessary for reli-able image retrieval involves not only annotat-ing the image as a single artifact, but also an-notating specific objects or regions within theimage.
Such detailed annotation is a costly en-deavor and the available annotated image dataare quite limited.
This paper explores the fea-sibility of using image captions from scientificjournals for the purpose of automatically an-notating image regions.
Salient image clues,such as an object location within the image oran object color, together with the associatedexplicit object mention, are extracted and clas-sified using rule-based and SVM learners.1 IntroductionThe profusion of digitally available images has nat-urally led to an interest in the field of automatic im-age annotation and retrieval.
A number of studiesattempt to associate image regions with the corre-sponding concepts.
In (Duygulu et al, 2002), forexample, the problem of annotation is treated as atranslation from a set of image segments (or blobs)to a set of words.
Modeling the association betweenblobs and words for the purpose of automated an-notation has also been proposed by (Barnard et al,2003; Jeon et al, 2003).A recurring hindrance that appears in studies aim-ing at automatic image region annotation is the lackof an appropriate dataset.
All of the above studiesuse the Corel image dataset that consists of 60,000images annotated with 3 to 5 keywords.
The needfor an image dataset with annotated image regionshas been recognized by many researchers.
For ex-ample, Russell et al(2008) have developed a tooland a general purpose image database designed todelineate and annotate objects within image scenes.The need for an image dataset with annotated ob-ject boundaries appears to be especially pertinent inthe biomedical field.
Organizing and using for re-search the available medical imaging data proved tobe a challenge and a goal of the ongoing research.Rubin et al(2008), for example, propose an ontol-ogy and annotation tool for semantic annotation ofimage regions in radiology.However, creating a dataset of image regionsmanually annotated and delineated by domain ex-perts, is a costly enterprise.
Any attempts to auto-mate or semi-automate the process would be of asubstantial value.This work proposes an approach towards auto-matic annotation of regions of interest in imagesused in scientific publications.
Publications abun-dant in image data are an untapped source of an-notated image data.
Due to publication standards,meaningful image captions are almost always pro-vided within scientific articles.
In addition, imageRegions of Interest (ROIs) are commonly referred towithin the image caption.
Such ROIs are also com-monly delineated with some kind of an overlay thathelps locating the ROI.
This is especially true forhard to interpret scientific images such as radiologyimages.
ROIs are also described in terms of locationwithin the image, or by the presence of a particularcolor.
Identifying ROI mentions within image cap-tions and visual clues pinpointing the ROI within theimage would be the first step in building an object411.
Object Location - explicit ROI location, e.g.
front row, back-ground, top, bottom, left, right.Shells of planktonic animals called formainifera record cli-matic conditions as they are formed.
This one, Globigeri-noides ruber, lives year-round at the surface of the Sargasso Sea.The form of the live animal is shown at right, and its shell, whichis actually about the size of a fine grain of sand, at left.2.
Object Color - presence of a distinct color that identifies aROI.Anterior SSD image shows an elongated splenorenal varix (bluearea).
The varix travels from the splenic hilar region inferiorlyalong the left flank, down into the pelvis, and eventually back up tothe left renal vein via the left gonadal vein.
The kidney is encodedyellow, the portal system is encoded magenta, and the spleen isencoded tan.3.
Overlay Marker - an overlay marker used to pinpoint the loca-tion of the ROI, e.g.
arrows, asterisks, bounding boxes, or circles.Transverse sonograms obtained with a 7.5-MHz linear trans-ducer in the subareolar region.
The straight arrowsshow a dilated tubular structure.
The curved arrow indicatesan intraluminal solid mass.4.
Overlay Label - an overlay label used to pinpoint the locationof the ROI, e.g.
numbers, letters, words, abbreviations.Location of the calf veins.
Transverse US image justabove ankle demonstrates the paired posterior tibial veins (V)and posterior tibial artery (A) imaged from a posteromedial ap-proach.
Note there is inadequate venous flow velocity to visualizewith color Doppler without flow augmentation.Table 1: Image Markers divided into four categories, followed bya sample image caption1 in which Image Markers are marked in bold,Image Marker Referents are underlined.delineated and annotated image dataset.2 Problem DefinitionThe goal of this research is to locate visually salientimage region characteristics in the text surroundingscientific images that could be used to facilitate thedelineation of the image object boundaries.
Thistask could be broken down into two related subtasks- 1) locating and classifying textual clues for visu-ally salient ROI features (Image Markers), and 2) lo-cating the corresponding ROI text mentions (ImageMarker Referents).
Table 1 gives a classification ofImage Markers including examples of Image Mark-ers and Image Marker Referents.
Figure 1 shows thefrequency of Image Marker occurrences.1The captions were extracted from Radiology and Ra-diographics c?
Radiological Society of North America andOceanus c?Woods Hole Oceanographic Institution.3 Related WorkCohen et al(2003) attempt to identify what theyrefer to as ?image pointers?
within captions inbiomedical publications.
The image pointers of in-terest are, for example, image panel labels, or lettersand abbreviations used as an overlay within the im-age, similar to the Overlay Labels described in Table1.
They developed a set of hand-crafted rules, and alearning method involving Boosted Wrapper Induc-tion on a dataset consisting of biomedical articlesrelated to fluorescence microscope images.Deschacht and Moens (2007) analyze text sur-rounding images in news articles trying to identifypersons and objects in the text that appear in thecorresponding image.
They start by extracting per-sons?
names and visual objects using Named EntityRecognition (NER) tools.
Next, they measure the?salience?
of the extracted named entities within thetext with the assumption that more salient named en-tities in the text will also be present in the accompa-nying image.Davis et al(2003) develop a NER tool to iden-tify references to a single art object (for example aspecific building within an image) in text related toart images for the purpose of automatic catalogingof images.
They take a semi-supervised approach tolocating the named entities of interest by first provid-ing an authoritative list of art objects of interest andthen seeking to match variants of the seed named en-tities in related text.4 Experimental Methods and Results4.1 DatasetFigure 1: Distribution of ImageMarker types across 400 annotatedimage captions.The chosen date-set contains morethan 60,000 imagestogether with their as-sociated captions fromthree online life andearth sciences jour-nals1.
400 randomlyselected image cap-tions were manuallyannotated by a singleannotator with theirImage Markers and Image Marker Referents andused for testing and for cross-validation respectively42in the two methods described below.4.2 Rule Based ApproachFirst, we developed a two-stage rule-based, boot-strapping algorithm for locating the image markersand their coreferents from unannotated data.
The al-gorithm is based on the observation that textual im-age markers commonly appear in parentheses andare usually closely related semantic concepts.
Thusthe seed for the algorithm consists of:1.
The predominant syntactic pattern - parenthe-ses, as in ?hooking of the soft palate (arrow)?.
Thispattern could easily be captured by a regular expres-sion and doesn?t require sentence parsing.2.
A dozen seed phrases (e.g ?left?, ?circle?, ?as-terisk?, ?blue?)
identified by initially annotating asmall subset of the data (20 captions).
Wordnet wasused to look up and prepare a list of their corre-sponding inherited hypernyms.
This hypernym listcontains concepts such as ?a spatially limited lo-cation?, ?a two-dimensional shape?, ?a written orprinted symbol?, ?a visual attribute of things thatresults from the light they emit or transmit or re-flect?.
Best results were achieved when inherited hy-pernyms up to the third parent were used.In the first stage of the algorithm, all image cap-tions were searched for parenthesized expressionsthat share the seed hypernyms.
This step of the al-gorithm will result in high precision, but a low re-call since image markers do not necessarily appearin parentheses.
To increase recall, in stage 2 a fulltext search was performed for the stemmed versionsof the expressions identified in stage 1.A baseline measure was also computed for theidentification of the Image Marker Referents using asimple heuristic - the coreferent of the Image Markeris usually the closest Noun Phrase (NP).
In the caseof parenthesized image markers, it is the closest NPto the left of the image marker; in the case of non-parenthesized image markers, the referent is usuallythe complement of the verb; and in the case of pas-sive voice, the NP preceding the verb phrase.
TheStanford parser was used to parse the sentences.Table 2 summarizes the results validated againstthe annotated dataset (excluding the 20 captionsused to identify the seed phrases).
It appears that therelatively low accuracy for Image Marker Referentidentification was mostly due to parsing errors sincePrecision Recall F1-scoreImage Marker 87.70 68.10 76.66Image Marker Referent Accuracy 59.10Table 2: Rule-based approach results for Image Marker and Im-age Marker Referent identification.
Image Marker Referent results arereported as accuracy because the algorithm involves locating an ImageMarker Referent for each Image Marker.
Referent identification accu-racy was computed for all annotated Image Markers.Kind k-5 .
.
.
k0 .
.
.
k+5Orth o-5 .
.
.
o0 .
.
.
o+5Stem s-5 .
.
.
s0 .
.
.
s+5Hypernym h-5 .
.
.
h0 .
.
.
h+5Dep Path d-5 .
.
.
d0 .
.
.
d+5Category [c0]Table 3: Features from a surrounding token window are used toclassify the current token into category [c0].
Best results were achievedwith a five-token window.the syntactic structure of the image caption texts isquite distinct from the Penn Treebank dataset usedfor training the Stanford parser.4.3 Support Vector MachinesNext we explored the possibility of improving therule-based method results by applying a machinelearning technique on the set of annotated data.
Sup-port Vector Machines (SVM) (Vapnik, 2000) wasthe approach taken because it is a state-of-the-artclassification approach proven to perform well onmany NLP tasks.In our approach, each sentence was tokenized,and tokens were classified as Beginning, Inside, orOutside an Image Marker type or Image Marker Ref-erent.
Image Marker Referents are not related to Im-age Markers and creating a classifier trained on thistask is planned as future work.
SVM classifiers weretrained for each of these categories, and combinedvia ?one-vs-all?
classification (the category of theclassifier with the largest output was selected).
Fea-tures of the surrounding context are used as shownin Table 3 and Table 4.Table 5 summarizes the results of a 10-fold cross-validation.
SVM performed well overall for iden-tifying Image Markers, Location being the hardestbecause of higher variability of expressing ROI posi-tion.
Image Marker Referents are harder to classify,43Token Kind The general type of the sentence to-ken (Word, Number, Symbol, Punctuation,White space).Orthography Orthographic categorization of the token(Upper initial, All capitals, Lower case,Mixed case).Stem The stem of the token, extracted with thePorter stemmer.Wordnet Super-classWordnet hypernyms (nouns, verbs); the hy-pernym of the derivationally related form(adjectives); the superclass of the pertanym(adverbs).POS Category POS categories extracted using Brill?s tag-ger.DependencyPath*The smallest sentence parse subtree includ-ing both the current token and the anno-tated image marker(s), encoded as an undi-rected path across POS categories.Table 4: Orthographic, semantic, and grammatical classificationfeatures computed for each token (*Dependency Path is used only forclassifying Image Marker Referents).as deeper syntactic knowledge is necessary.
Idiosyn-cratic syntactic structures in image captions posea problem for the general-purpose trained Stanfordparser and performance is hindered by the accuracyof computing Dependency Path feature.5 Conclusion and Future WorkWe explored the feasibility of determining the con-tent of ROIs in images from scientific publicationsusing image captions.
We developed a two-stagerule-based approach that utilizes WordNet to findROI pointers (Image Markers) and their referents.We also explored a supervised machine learning ap-proach.
Both approaches are promising.
The rule-based approach seeded with a small manually an-notated set resulted in 78.7% precision and 68.1%recall for Image Markers recognition.
The SVM ap-proach (which requires a greater annotation effort)outperformed the rule based approach (p=93.6%,r=87.7%).
Future plans include training SVMs onthe results of the rule-based annotation.
Furtherwork is also needed in improving Image MarkerReferent identification and co-reference resolution.We also plan to involve two annotators in orderto collect a more robust dataset based on inter-annotator agreement.ReferencesK.
Barnard, P. Duygulu, D. Forsyth, N. de Freitas, D.M.Blei, and M.I.
Jordan.
2003.
Matching words andPrecision Recall F1-scoreLocation 60.93 45.15 51.86Color 100.00 51.32 67.82Overlay Marker 97.43 95.39 96.39Overlay Label 85.74 87.69 86.70Overall 93.64 87.69 90.56Image Marker Referent Accuracy 61.15Table 5: SVM classification results for the four types of ImageMarkers, and for Image Marker Referents.
LibSVM software was used(3-degree polynomial kernel, cost parameter = 1, ?
= 0.6 empiricallydetermined).pictures.
The Journal of Machine Learning Research,3:1107?1135.W.W.
Cohen, R. Wang, and R.F.
Murphy.
2003.
Un-derstanding captions in biomedical publications.
InProceedings of the 9th ACM SIGKDD internationalconference on Knowledge discovery and data mining,pages 499?504.
ACM New York, NY, USA.P.T.
Davis, D.K.
Elson, and J.L.
Klavans.
2003.
Methodsfor precise named entity matching in digital collec-tions.
In Proceedings of the 3rd ACM/IEEE-CS jointconference on Digital libraries, pages 125?127.
IEEEComputer Society Washington, DC, USA.K.
Deschacht and M. Moens.
2007.
Text analysis for au-tomatic image annotation.
In Proceedings of the 45thAnnual ACL Meeting, pages 1000?1007.
ACL.P.
Duygulu, K. Barnard, JFG de Freitas, and D.A.Forsyth.
2002.
Object Recognition as Machine Trans-lation: Learning a Lexicon for a Fixed Image Vocab-ulary.
LECTURE NOTES IN COMPUTER SCIENCE,pages 97?112.J.
Jeon, V. Lavrenko, and R. Manmatha.
2003.
Au-tomatic image annotation and retrieval using cross-media relevance models.
In Proceedings of the 26thannual international ACM SIGIR conference on Re-search and development in informaion retrieval, pages119?126.
ACM New York, NY, USA.D.
Rubin, P. Mongkolwat, V. Kleper, K. Supekar, andD.
Channin.
2008.
Medical imaging on the SemanticWeb: Annotation and image markup.
In AAAI SpringSymposium Series, Semantic Scientific Knowledge In-tegration.B.C.
Russell, A. Torralba, K.P.
Murphy, and W.T.
Free-man.
2008.
LabelMe: A Database and Web-BasedTool for Image Annotation.
International Journal ofComputer Vision, 77(1):157?173.V.N.
Vapnik.
2000.
The Nature of Statistical LearningTheory.
Springer.44
