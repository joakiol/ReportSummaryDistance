Proceedings of the 8th International Conference on Computational Semantics, pages 116?127,Tilburg, January 2009. c?2009 International Conference on Computational SemanticsAn Extensible Toolkit for Computational SemanticsDan Garrettedhgarrette@gmail.comEwan KleinUniversity of Edinburghewan@inf.ed.ac.uk1 IntroductionIn this paper we focus on the software for computational semantics pro-vided by the Python-based Natural Language Toolkit (nltk).
The seman-tics modules in nltk are inspired in large part by the approach developed inBlackburn and Bos (2005) (henceforth referred to as B&B).
Since Blackburnand Bos have also provided a software suite to accompany their excellenttextbook, one might ask what the justification is for the nltk offering, whichis similarly slanted towards teaching computational semantics.This question can be answered in a number of ways.
First, we believethere is intrinsic merit in the availability of different software tools for se-mantic analysis, even when there is some duplication of coverage; and thiswill become more true as computational semantics starts to be as widelystudied as computational syntax.
For example, one rarely hears the ob-jection that too many implementations of syntactic parsers are available.Moreover, the nltk software significantly goes beyond B&B in providingan implementation of Glue Semantics.Second, whatever the relative merits of Prolog vs. Python as program-ming languages, there is surely an advantage in offering students and in-structors a choice in this respect.
Given that many students have eitheralready been exposed to Java, or else have had no programming experienceat all, Python offers them the option of accomplishing interesting resultswith only a shallow learning curve.Third, nltk is a rapidly developing, open source project1with a broadcoverage of natural language processing (nlp) tools; see Bird et al (2008)for a recent overview.
This wide functionality has a number of benefits,most notably that lexical, syntactic and semantic processing can be carriedout within a uniform computational framework.
As a result, nltk makes itmuch easier to include some computational semantics subject matter in abroad course on natural language analysis, rather than having to devote awhole course exclusively to the topic.1See http://www.nltk.org116Fourth, nltk is accompanied by a substantial collection of corpora, pluseasy-to-use corpus readers.
This collection, which currently stands at over50 corpora and trained models, includes parsed, POS-tagged, plain text, cat-egorized text, and lexicons.
The availability of corpora can help encouragestudents to go beyond writing toy grammars, and instead to start grapplingwith the complexities of semantically analysing realistic bodies of text.Fifth, nltk is not just for students.
Although Python is slower thanlanguages like Java and C++, its suitability for rapid prototyping makes itan attractive addition to the researcher?s inventory of resources.
Buildingan experimental set-up in nltk to test a hypothesis or explore some data isstraightforward and quick, and the rich variety of existing nlp components inthe toolkit allows rapid assembly of quite sophisticated processing pipelines.2 OverviewTheorem ProvingParsingFeature-based CFG +Earley parserStatistical dependencygrammar parserUnderspecified LFsHole SemanticsLinear Logic GlueSemanticsLogical FormsFOL + ?DRS + ?Model CheckingTableau TPModel BuildingMaceNon-monotonic TPProver9Like B&B, we assume thatone of the most importanttasks for the teacher is toground students in the ba-sic concepts of first orderlogic and the lambda cal-culus, model-theoretic in-terpretation and inference.This provides a basis forexploring more modern ap-proaches like Discourse Rep-resentation Theory (drt;Kamp and Reyle (1993))and underspecification.In the accompanying fig-ure, we give a diagram-matic overview of the mainsemantics-related function-ality that is currently avail-able in nltk.
Logicalforms (lfs) can be inducedas result of syntactic pars-ing, using either feature-based grammars that areprocessed with an Earleychart parser, or else by as-sociating lfs with the output of a broad-coverage dependency parser.
Ourbasic lfs are expressions of first order logic, supplemented with the lambda117operator.
However, we also admit Discourse Representation Structures(drss) as lfs, and underspecified lfs can be built using either Hole Se-mantics (Blackburn and Bos, 2005) or Glue Semantics (Dalrymple et al,1999).
Once we have constructed lfs, they can be evaluated in a first ordermodel (Klein, 2006), tested for equivalence and validity in a variety of the-orem provers, or tested for consistency in a model builder.
The latter twotasks are aided by nltk interfaces to third-party inference tools, currentlyProver9 and Mace4 (McCune, 2008).We do not have space in this paper to discuss all of these components,but will try to present some of the key aspects, and along the way notingcertain points of difference vis-a`-vis B&B.3 Logical Form3.1 First Order Predicate Logic with Lambda CalculusFrom a pedagogical point of view, it is usually important to ensure that stu-dents have some grasp of the language of first order predicate logic (fol),and can also manipulate ?-abstraction.
The nltk.sem.logic module con-tains an object-oriented approach to representing fol plus ?-abstraction.Logical formulas are typically fed to the logic parser as strings, and thenrepresented as instances of various subclasses of Expression, as we will seeshortly.An attractive feature of Python is its interactive interpreter, which allowsthe user to enter Python expressions and statements for evaluation.
In theexample below and subsequently, >>> is the Python interpreter?s prompt.1 >>> from nltk.sem import logic2 >>> lp = logic.LogicParser()3 >>> e = lp.parse(?all x.
(girl(x) -> exists y.
(dog(y) & chase(x,y)))?
)4 >>> e5 <AllExpression all x.
(girl(x) -> exists y.
(dog(y) & chase(x,y)))>As illustrated, the result of parsing the formula at line 3 is an object e be-longing to the class AllExpression, itself a subclass of Expression.
Allsuch subclasses have numerous methods that implement standard logicaloperations.
For example, the simplify() method carries out ?-conversion;the free() method finds all the free variables in an expression; and for quan-tified expressions (such as AllExpressions), there is an alpha convert()method.
The logic module will ?-convert automatically when appropri-ate to avoid name-clashes in the replace() method.
Let?s illustrate thesemethods with a formula involving ?-abstraction, namely \x.P(x)(y); weuse \ to represent ?.
(Since \ is a special character in Python, we add ther prefix to strings containing it to preclude additional escape characters.
)118>>> from nltk.sem import Variable>>> e1 = lp.parse(r?\x.P(x)(y)?
)>>> print e1.simplify()P(y)>>> e2 = lp.parse(?all x.P(x,a,b)?
)>>> print e2.free()set([<Variable(?a?
), Variable(?b?
)])>>> print e2.alpha_convert(Variable(?z?
))all z.P(z,a,b)>>> e3 = lp.parse(?x?
)>>> print e2.replace(Variable(?b?
), e3)all z1.P(z1,a,x)Allowing students to build simple first order models, and evaluate expres-sions in those models, can be useful for helping them clarify their intuitionsabout quantification.
In the next example, we show one of the availablemethods in nltk for specifying a model and using it to determine the set ofsatisfiers of the open formula ?x.
(girl(y) ?
chase(x, y)).2,3>>> from nltk.sem import parse_valuation, Model, Assignment>>> v = """... suzie => s... fido => f... rover => r... girl => {s}... chase => {(f, s), (r, s), (s, f)}... """>>> val = parse_valuation(v) #create a Valuation>>> m = Model(val.domain, val) #initialize a Model>>> g = Assignment(val.domain) #initialize an Assignment>>> e4 = lp.parser(?exists y.
(girl(y) & chase(x, y))?
)>>> m.satisfiers(e4, ?x?, g) #check satisfiers of e4 wrt to xset([?r?, ?f?
])In B&B, ?-abstracts are second-class citizens, used exclusively as a ?glue?mechanism for composing meaning representations.
Although we use ?-abstracts as glue too, abstracts over individual variables are semanticallyinterpreted in nltk, namely as characteristic functions.Expressions in nltk can be optionally typed (using Montague-styletypes) by passing the parameter type check=True to LogicParser.
Apartfrom allowing the user to display the Expression?s type with type, typechecking will raise an exception for non-well typed expressions:2The triple quotes """ in Python allow us to break a logical line across several physicallines.3Given a valuation val, the property val.domain returns the set of all domain indi-viduals specified in the valuation.119>>> tlp = logic.LogicParser(type_check=True)>>> a = tlp.parse(r?\x y.see(x,y)?
)>>> b = tlp.parse(r?\x.man(x)?
)>>> a.type, b.type(<e,<e,t>>, <e,t>)>>> tlp.parse(r?\x y.see(x,y)(\x.man(x))?
)Traceback (most recent call last):.
.
.TypeException: The function ?\x y.see(x,y)?
is of type ?<e,<e,t>>?and cannot be applied to ?\x.man(x)?
of type ?<e,t>?.
Its argumentmust match type ?e?.3.2 Discourse Representation TheoryAs mentioned earlier, nltk contains an extension to the logic module forworking with Discourse Representation Theory (drt) (Kamp and Reyle,1993).
The nltk.sem.drt module introduces a DRS() constructor whichtakes lists of discourse referents and conditions as initialization parameters:(1) DRS([j,d],[John(j), dog(d), sees(j,d)])On top of the functionality available for fol expressions, drt expres-sions have a ?drs-concatenation?
operator, represented as the + symbol.
Theconcatenation of two drss is a single drs containing the merged discoursereferents and the conditions from both arguments.
drs-concatenation auto-matically ?-converts bound variables to avoid name-clashes.
The + symbolis overloaded so that drt expressions can be added together easily.
Thenltk.sem.drt parser allows drss to be specified succinctly as strings.>>> from nltk.sem import drt>>> dp = drt.DrtParser()>>> d1 = dp.parse(?
([x],[walk(x)]) + ([y],[run(y)])?
)>>> print d1(([x],[walk(x)]) + ([y],[run(y)]))>>> print d1.simplify()([x,y],[walk(x), run(y)])>>> d2 = dp.parse(?
([x,y],[Bill(x), Fred(y)])?
)>>> d3 = dp.parse("""([],[([u],[Porsche(u), own(x,u)])... -> ([v],[Ferrari(v), own(y,u)])])""")>>> d4 = d2 + d3>>> print d4.simplify()([x,y],[Bill(x), Fred(y),(([u],[Porsche(u), own(x,u)]) -> ([v],[Ferrari(v), own(y,u)]))])drt expressions can be converted to their first order predicate logic equiva-lents using the toFol() method and can be graphically rendered on screenwith the draw() method.120>>> print d1.toFol()(exists x.walk(x) & exists y.run(y))>>> d4.simplify().draw()Figure 1: DRS ScreenshotSince the ?
operator can be combinedwith drt expressions, the nltk.sem.drt mod-ule can be used as a plug-in replacement fornltk.sem.logic in building compositional se-mantics.4 Scope Ambiguity and UnderspecificationTwo key questions in introducing students to computational semantics are:Q1: How are semantic representations constructed from input sentences?Q2: What is scope ambiguity and how is it captured?A standard pedagogical approach is to address (Q1) with a simple syntax-driven induction of logical forms which fails to deal with scope ambiguity,while (Q2) is addressed by introducing underspecified representations whichare resolved to produce different readings of ambiguous sentences.nltk includes a suite of parsing tools, amongst which is a chart parserfor context free grammars augmented with feature structures.
A ?semantics?feature sem allows us to compose the contributions of constituents to builda logical form for a complete sentence.
To illustrate, the following minimalgrammar sem1.fcfg handles quantification and intransitive verbs (wherevalues such as ?subj and ?vp are unification variables, while P and Q are?-bound object language variables):S[sem = <?subj(?vp)>] -> NP[sem=?subj] VP[sem=?vp]VP[sem=?v] -> IV[sem=?v]NP[sem=<?det(?n)>] -> Det[sem=?det] N[sem=?n]Det[sem=<\P.\Q.exists x.
(P(x) & Q(x))>] -> ?a?N[sem=<\x.dog(x)>] -> ?dog?IV[sem=<\x.bark(x)>] -> ?barks?Using sem1.fcfg, we can parse A dog barks and view its semantics.
Theload earley() method takes an optional parameter logic parser whichspecifies the logic-parser for processing the value of the sem feature, thusallowing different kinds of logical forms to be constructed.>>> from nltk.parse import load_earley>>> parser = load_earley(?grammars/sem1.fcfg?, trace=0)>>> trees = parser.nbest_parse(?a dog barks?.split())>>> print trees[0].node[?sem?
].simplify()exists x.
(dog(x) & bark(x))121Underspecified logical forms allow us to loosen the relation between syn-tactic and semantic representations.
We consider two approaches to under-specification, namely Hole Semantics and Glue Semantics.
Since the formerwill be familiar from B&B, we devote most of our attention to presentingGlue Semantics.4.1 Hole SemanticsHole Semantics in nltk is handled by the nltk.sem.hole module, whichuses a context free grammar to generate an underspecified logical form.Since the latter is itself a formula of first order logic, we can continue to usethe sem feature in the context free grammar:N[sem=<\x h l.(PRED(l,dog,x) & LEQ(l,h) & HOLE(h) & LABEL(l))>]-> ?dog?The Hole Semantics module uses a standard plugging algorithm to derivethe sentence?s readings from the underspecified lf.>>> from nltk.sem import hole>>> readings = hole.hole_readings(?every girl chases a dog?
)>>> for r in reading: print rexists z1.
(dog(z1) & all z2.
(girl(z2) -> chase(z1,z2)))all z2.
(girl(z2) -> exists z1.
(dog(z1) & chase(z1,z2)))4.2 Glue SemanticsGlue Semantics (Dalrymple et al, 1999), or Glue for short, is an approach tocompositionality that tries to handle semantic ambiguity by using resource-sensitive logic to assemble meaning expressions.
The approach builds proofsover ?meaning constructors?
; these are of the form M : G, where M is ameaning representation and G is a term of linear logic.
The linear logicterm G dictates how the meaning expression M can be combined.
Eachdistinct proof that can be derived reflects a different semantic reading of theentire sentence.The variant of linear logic that we use has (linear) implication (i.e., ()as its only operator, so the primary operation during the proof is ModusPonens.
Linear logic is an appropriate logic to serve as ?glue?
because itis resource-sensitive.
This means that when Modus Ponens combines twoterms to create a new one, the two original terms are ?consumed?, and cannotbe used again in the proof; cf.
(2) vs. (3).
Additionally, every premise mustbe used for the proof to be valid; cf.
(4).
This resource-sensitivity dictatesthat each word contributes its meaning exactly once to the meaning of thewhole.122(2) A, (A( B) ` B(3) A, (A( B) 0 A,B(4) A,A, (A( B) 0 Bnltk?s nltk.gluesemantics.linearlogic module contains an implemen-tation of linear logic.The primary rule for composing Glue formulas is (5).
Function-argumentapplication of meaning expressions is reflected (via the Curry-Howard iso-morphism) by the application of Modus Ponens in a linear logic proof.
Notethat A and B are meta-variables over constants of linear logic; these con-stants represent ?attachment points?
for meaning expressions in some kind ofsyntactically-derived representation (such as an LFG f -structure).
It is (5)which allows Glue to guide the construction of complex meaning expressions.
(5) ?
: A, ?
: (A( B) ` ?(?)
: BThe nltk modules gluesemantics.glue and gluesemantics.drt glueimplement Glue for fol and drt meaning expressions, respectively.4Thefollowing example shows how Glue formulas are created and combined toderive a logical form for John walks:>>> from nltk.gluesemantics.glue import GlueFormula>>> john = GlueFormula(?john?, ?g?
)>>> walks = GlueFormula(r?\x.walk(x)?, ?
(g -o f)?
)>>> john_walks = walks.applyto(john)>>> print john_walks.meaning.simplify()walk(john)Thus, the non-logical constant john is associated with the Glue term g,while the meaning expression ?x.walk(x) is associated with (g ( f) sinceit is a function that takes g as input and returns the meaning expression f ,corresponding to the whole sentence.
Consequently, a proof of f from thepremises is a derivation of a meaning representation for the sentence.Scope ambiguity, resulting, for example, from quantifiers, requires theuse of variables in the Glue terms.
Such variables may be instantiated to anylinear logic constant, so long as this is carried out uniformly.
Let?s assumethat the quantified noun phrase every girl has the meaning constructor (6)(where G is a linear logic variable):(6) ?Q.?x.
(girl(x) ?
Q(x)) : ((g( G)( G)4See http://nltk.googlecode.com/svn/trunk/doc/contrib/sem/index.html formore details.123Then the Glue derivation shown below correctly generates two readings forthe sentence Every girl chases a dog :>>> from nltk.gluesemantics.glue import GlueFormula, Glue>>> a = GlueFormula(r?\Q.all x.
(girl(x) -> Q(x))?, ?
((g -o G) -o G)?
)>>> b = GlueFormula(r?\x y.chase(x,y)?, ?
(g -o (h -o f))?
)>>> c = GlueFormula(r?\Q.exists x.
(dog(x)&Q(x))?, ?
((h -o H) -o H)?
)>>> glue = Glue()>>> for reading in glue.get_readings(glue.gfl_to_compiled([a,b,c])):... print reading.simplify()exists x.
(dog(x) & all z13.
(girl(z13) -> chase(z13,x)))all x.
(girl(x) -> exists z14.
(dog(z14) & chase(x,z14)))5 Inference toolsIn order to perform inference over semantic representations, nltk can callboth theorem provers and model builders.
The library includes a purePython tableau-based first order theorem prover; this is intended to allowstudents to study tableau methods for theorem proving, and provides anopportunity for experimentation.
In addition, nltk provides interfaces totwo off-the-shelf tools, namely the theorem prover Prover9, and the modelbuilder Mace4 (McCune, 2008).The get_prover(G, A) method by default calls Prover9, and takes asparameters a proof goal G and a list A of assumptions.
Here, we verify thatif every dog barks, and Rover is a dog, then it is true that Rover barks:>>> from nltk.inference import inference>>> a = lp.parse(?all x.
(dog(x) -> bark(x))?
)>>> b = lp.parse(?dog(rover)?
)>>> c = lp.parse(?bark(rover)?
)>>> prover = inference.get_prover(c, [a,b])>>> prover.prove()TrueA theorem prover can also be used to check the logical equivalence ofexpressions.
For two expressions A and B, we can pass (A ??
B) intoa theorem prover and know that the theorem will be proved if and only ifthe expressions are logically equivalent.
nltk?s standard equality operatorfor Expressions (==) is able to handle situations where two expressions areidentical up to ?-conversion.
However, it would be impractical for nltkto invoke a wider range of logic rules every time we checked for equality oftwo expressions.
Consequently, both the logic and drt modules in nltkhave a separate method, tp equals, for checking ?equality?
up to logicalequivalence.124>>> a = lp.parse(?all x.walk(x)?
)>>> b = lp.parse(?all y.walk(y)?
)>>> a == bTrue>>> c = lp.parse(?-(P(x) & Q(x))?
)>>> d = lp.parse(?-P(x) | -Q(x)?
)>>> c == dFalse>>> c.tp_equals(d)True6 Discourse Processingnltk contains a discourse processing module, nltk.inference.discourse,similar to the curt program presented in B&B.
This module processessentences incrementally, keeping track of all possible threads when there isambiguity.
For simplicity, the following example ignores scope ambiguity.>>> from nltk.inference.discourse import DiscourseTester as DT>>> dt = DT([?A student dances?, ?Every student is a person?
])>>> dt.readings()s0 readings:------------------------------s0-r0: exists x.
(student(x) & dance(x))s1 readings:------------------------------s1-r0: all x.
(student(x) -> person(x))When a new sentence is added to the current discourse, setting the parameterconsistchk=True causes consistency to be checked by invoking the modelchecker for each ?thread?, i.e., discourse sequence of admissible readings.
Inthis case, the user has the option of retracting the sentence in question.>>> dt.add_sentence(?No person dances?, consistchk=True)Inconsistent discourse d0 [?s0-r0?, ?s1-r0?, ?s2-r0?
]:s0-r0: exists x.
(student(x) & dance(x))s1-r0: all x.
(student(x) -> person(x))s2-r0: -exists x.
(person(x) & dance(x))>>> dt.retract_sentence(?No person dances?, quiet=False)Current sentences ares0: A student dancess1: Every student is a personIn a similar manner, we use informchk=True to check whether the new sen-tence is informative relative to the current discourse (by asking the theoremprover to derive it from the discourse).125>>> dt.add_sentence(?A person dances?, informchk=True)Sentence ?A person dances?
under reading ?exists x.
(person(x) &dance(x))?
:Not informative relative to thread ?d0?It is also possible to pass in an additional set of assumptions as backgroundknowledge and use these to filter out inconsistent readings.The discourse module can accommodate semantic ambiguity and filterout readings that are not admissable.
By invoking both Glue Semantics anddrt, the following example processes the two-sentence discourse Every dogchases a boy.
He runs.
As shown, the first sentence has two possible read-ings, while the second sentence contains an anaphoric pronoun, indicated asPRO(x).>>> from nltk.inference.discourse import DrtGlueReadingCommand as RC>>> dt = DT([?Every dog chases a boy?, ?He runs?
], RC())>>> dt.readings()s0 readings:------------------------------s0-r0: ([],[(([x],[dog(x)]) -> ([z15],[boy(z15), chase(x,z15)]))])s0-r1: ([z16],[boy(z16), (([x],[dog(x)]) -> ([],[chase(x,z16)]))])s1 readings:------------------------------s1-r0: ([x],[PRO(x), run(x)])When we examine the two threads d0 and d1, we see that that readings0-r0, where every dog out-scopes a boy, is deemed inadmissable becausethe pronoun in the second sentence cannot be resolved.
By contrast, inthread d1 the pronoun (relettered to z24) has been bound via the equation(z24 = z20).>>> dt.readings(show_thread_readings=True)d0: [?s0-r0?, ?s1-r0?]
: INVALID: AnaphoraResolutionExceptiond1: [?s0-r1?, ?s1-r0?]
: ([z20,z24],[boy(z20), (([x],[dog(x)]) ->([],[chase(x,z20)])), (z24 = z20), run(z24)])7 Conclusions and Future Worknltk?s semantics functionality has been written with extensibility in mind.The logic module?s LogicParser employs a basic parsing template andcontains hooks that an extending module can use to supplement or sub-stitute functionality.
Moreover, the base Expression class in logic, aswell as any derived classes, can be extended, allowing variants to reuse theexisting functionality.
For example, the drt and linear logic modules areimplemented as extensions to logic.py.126The theorem prover and model builder code has also been carefully archi-tected to allow extensions and the nltk.inference.api library exposes theframework for the inference architecture.
The library therefore provides agood starting point for creating interfaces with other theorem provers andmodel builders in addition to Prover9, Mace4, and the tableau prover.nltk already includes the beginnings of a framework for ?recognizingtextual entailment?
; access to the rte data sets is provided and we are in thecourse of developing a few simple modules to demonstrate rte techniques.For example, a Logical Entailment rte tagger based on Bos and Markert(2005) begins by building a semantic representation of both the text andthe hypothesis in drt.
It then runs a theorem prover with the text as theassumption and the hypothesis as the goal in order to check whether thetext entails the hypothesis.The tagger is also capable of adding backgroundknowledge via an interface to the WordNet dictionary in nltk.wordnet asa first step in making the entailment checking more robust.ReferencesSteven Bird, Ewan Klein, Edward Loper, and Jason Baldridge.
Multidisciplinaryinstruction with the Natural Language Toolkit.
In Proceedings of the ThirdWorkshop on Issues in Teaching Computational Linguistics, Columbus, Ohio,USA, June 2008.Patrick Blackburn and Johan Bos.
Representation and Inference for Natural Lan-guage: A First Course in Computational Semantics.
CSLI Publications, NewYork, 2005.Johan Bos and Katja Markert.
Recognising textual entailment with logical infer-ence.
In Proceedings of the conference on Human Language Technology and Em-pirical Methods in Natural Language Processing, Vancouver, British Columbia,Canada, 2005.Mary Dalrymple, V. Gupta, John Lamping, and V. Saraswat.
Relating resource-based semantics to categorial semantics.
In Mary Dalrymple, editor, Semanticsand syntax in Lexical Functional Grammar: the resource logic approach, pages261?280.
MIT Press, Cambridge, MA, 1999.Hans Kamp and Uwe Reyle.
From Discourse to the Lexicon: Introduction to Mod-eltheoretic Semantics of Natural Language, Formal Logic and Discourse Repre-sentation Theory.
Kluwer Academic Publishers, 1993.Ewan Klein.
Computational semantics in the Natural Language Toolkit.
In Pro-ceedings of the Australasian Language Technology Workshop, pages 26?33, 2006.William McCune.
Prover9: Automated theorem prover for first-orderand equational logic, 2008. http://www.cs.unm.edu/~mccune/mace4/manual-examples.html.127
