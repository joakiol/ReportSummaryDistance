Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 104?113,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsBetter Word Representations with Recursive Neural Networks forMorphologyMinh-Thang Luong Richard Socher Christopher D. ManningComputer Science Department Stanford University, Stanford, CA, 94305{lmthang, manning}@stanford.edu richard@socher.orgAbstractVector-space word representations havebeen very successful in recent years at im-proving performance across a variety ofNLP tasks.
However, common to mostexisting work, words are regarded as in-dependent entities without any explicit re-lationship among morphologically relatedwords being modeled.
As a result, rare andcomplex words are often poorly estimated,and all unknown words are representedin a rather crude way using only one ora few vectors.
This paper addresses thisshortcoming by proposing a novel modelthat is capable of building representationsfor morphologically complex words fromtheir morphemes.
We combine recursiveneural networks (RNNs), where each mor-pheme is a basic unit, with neural languagemodels (NLMs) to consider contextualinformation in learning morphologically-aware word representations.
Our learnedmodels outperform existing word repre-sentations by a good margin on word sim-ilarity tasks across many datasets, includ-ing a new dataset we introduce focused onrare words to complement existing ones inan interesting way.1 IntroductionThe use of word representations or word clusterspretrained in an unsupervised fashion from lots oftext has become a key ?secret sauce?
for the suc-cess of many NLP systems in recent years, acrosstasks including named entity recognition, part-of-speech tagging, parsing, and semantic role label-ing.
This is particularly true in deep neural net-work models (Collobert et al 2011), but it is alsotrue in conventional feature-based models (Koo etal., 2008; Ratinov and Roth, 2009).Deep learning systems give each word adistributed representation, i.e., a dense low-dimensional real-valued vector or an embedding.The main advantage of having such a distributedrepresentation over word classes is that it can cap-ture various dimensions of both semantic and syn-tactic information in a vector where each dimen-sion corresponds to a latent feature of the word.
Asa result, a distributed representation is compact,less susceptible to data sparsity, and can implicitlyrepresent an exponential number of word clusters.However, despite the widespread use of wordclusters and word embeddings, and despite muchwork on improving the learning of word repre-sentations, from feed-forward networks (Bengioet al 2003) to hierarchical models (Morin, 2005;Mnih and Hinton, 2009) and recently recurrentneural networks (Mikolov et al 2010; Mikolov etal., 2011), these approaches treat each full-formword as an independent entity and fail to cap-ture the explicit relationship among morphologi-cal variants of a word.1 The fact that morphologi-cally complex words are often rare exacerbates theproblem.
Though existing clusterings and embed-dings represent well frequent words, such as ?dis-tinct?, they often badly model rare ones, such as?distinctiveness?.In this work, we use recursive neural networks(Socher et al 2011b), in a novel way to modelmorphology and its compositionality.
Essentially,we treat each morpheme as a basic unit in theRNNs and construct representations for morpho-logically complex words on the fly from their mor-phemes.
By training a neural language model(NLM) and integrating RNN structures for com-plex words, we utilize contextual information in1An almost exception is the word clustering of (Clark,2003), which does have a model of morphology to encour-age words ending with the same suffix to appear in the sameclass, but it still does not capture the relationship between aword and its morphologically derived forms.104an interesting way to learn morphemic semanticsand their compositional properties.
Our modelhas the capability of building representations forany new unseen word comprised of known mor-phemes, giving the model an infinite (if still in-complete) covered vocabulary.Our learned representations outperform pub-licly available embeddings by a good margin onword similarity tasks across many datasets, whichinclude our newly released dataset focusing onrare words (see Section 5).
The detailed analysisin Section 6 reveals that our models can blend wellsyntactic information, i.e., the word structure, andthe semantics in grouping related words.22 Related WorkNeural network techniques have found success inseveral NLP tasks recently such as sentiment anal-ysis at the sentence (Socher et al 2011c) anddocument level (Glorot et al 2011), languagemodeling (Mnih and Hinton, 2007; Mikolov andZweig, 2012), paraphrase detection (Socher et al2011a), discriminative parsing (Collobert, 2011),and tasks involving semantic relations and compo-sitional meaning of phrases (Socher et al 2012).Common to many of these works is use of adistributed word representation as the basic inputunit.
These representations usually capture lo-cal cooccurrence statistics but have also been ex-tended to include document-wide context (Huanget al 2012).
Their main advantage is that theycan both be learned unsupervisedly as well as betuned for supervised tasks.
In the former trainingregiment, they are evaluated by how well they cancapture human similarity judgments.
They havealso been shown to perform well as features forsupervised tasks, e.g., NER (Turian et al 2010).While much work has focused on different ob-jective functions for training single and multi-word vector representations, very little work hasbeen done to tackle sub-word units and how theycan be used to compute syntactic-semantic wordvectors.
Collobert et al(2011) enhanced wordvectors with additional character-level featuressuch as capitalization but still can not recovermore detailed semantics for very rare or unseenwords, which is the focus of this work.This is somewhat ironic, since working out cor-2The rare word dataset and trained word vectors can befound at http://nlp.stanford.edu/?lmthang/morphoNLM.rect morphological inflections was a very centralproblem in early work in the parallel distributedprocessing paradigm and criticisms of it (Rumel-hart and McClelland, 1986; Plunkett and March-man, 1991), and later work developed more so-phisticated models of morphological structure andmeaning (Gasser and Lee, 1990; Gasser, 1994),while not providing a compositional semantics norworking at the scale of what we present.To the best of our knowledge, the work clos-est to ours in terms of handing unseen words arethe factored NLMs (Alexandrescu and Kirchhoff,2006) and the compositional distributional seman-tic models (DSMs) (Lazaridou et al 2013).
Inthe former work, each word is viewed as a vec-tor of features such as stems, morphological tags,and cases, in which a single embedding matrix isused to look up all of these features.3 Thoughthis is a principled way of handling new words inNLMs, the by-product word representations, i.e.the concatenations of factor vectors, do not en-code in them the compositional information (theyare stored in the NN parameters).
Our work doesnot simply concatenate vectors of morphemes, butrather combines them using RNNs, which cap-tures morphological compositionality.The latter work experimented with differentcompositional DSMs, originally designed to learnmeanings of phrases, to derive representations forcomplex words, in which the base unit is the mor-pheme similar to ours.
However, their models canonly combine a stem with an affix and does notsupport recursive morpheme composition.
It is,however, interesting to compare our neural-basedrepresentations with their DSM-derived ones andcross test these models on both our rare wordsimilarity dataset and their nearest neighbor one,which we leave as future work.Mikolov et al(2013) examined existing wordembeddings and showed that these representationsalready captured meaningful syntactic and seman-tic regularities such as the singular/plural relationthat xapple - xapples ?
xcar - xcars.
However,we believe that these nice relationships will nothold for rare and complex words when their vec-tors are poorly estimated as we analyze in Sec-tion 6.
Our model, on the other hand, explicitlyrepresents these regularities through morphologi-cal structures of words.3(Collobert et al 2011) used multiple embeddings, oneper discrete feature type, e.g., POS, Gazeteer, etc.105 Figure 1: Morphological Recursive Neural Net-work.
A vector representation for the word ?un-fortunately?
is constructed from morphemic vec-tors: unpre, fortunatestm, lysuf.
Dotted nodes arecomputed on-the-fly and not in the lexicon.3 Morphological RNNsOur morphological Recursive Neural Network(morphoRNN) is similar to (Socher et al 2011b),but operates at the morpheme level instead of atthe word level.
Specifically, morphemes, the mini-mum meaning-bearing unit in languages, are mod-eled as real-valued vectors of parameters, and areused to build up more complex words.
We assumeaccess to a dictionary of morphemic analyses ofwords, which will be detailed in Section 4.Following (Collobert and Weston, 2008), dis-tinct morphemes are encoded by column vectorsin a morphemic embedding matrix We ?
Rd?|M|,where d is the vector dimension and M is an or-dered set of all morphemes in a language.As illustrated in Figure 1, vectors of morpho-logically complex words are gradually built upfrom their morphemic representations.
At any lo-cal decision (a dotted node), a new parent wordvector (p) is constructed by combining a stem vec-tor (xstem) and an affix vector (xaffix) as follow:p = f(Wm[xstem;xaffix] + bm) (1)Here, Wm ?
Rd?2d is a matrix of morphemic pa-rameters while bm ?
Rd?1 is an intercept vector.We denote an element-wise activation function asf , such as tanh.
This forms the basis of our mor-phoRNNmodels with ?
= {We,Wm, bm} beingthe parameters to be learned.3.1 Context-insensitive Morphological RNNOur first model examines how well morphoRNNscould construct word vectors simply from the mor-phemic representation without referring to anycontext information.
Input to the model is a refer-ence embedding matrix, i.e.
word vectors trainedby an NLM such as (Collobert and Weston, 2008)and (Huang et al 2012).
By assuming that thesereference vectors are right, the goal of the modelis to construct new representations for morpholog-ically complex words from their morphemes thatclosely match the corresponding reference ones.Specifically, the structure of the context-insensitive morphoRNN (cimRNN) is the same asthe basic morphoRNN.
For learning, we first de-fine a cost function s for each word xi as thesquared Euclidean distance between the newly-constructed representation pc(xi) and its refer-ence vector pr(xi): s (xi) = ?pc(xi) ?
pr(xi)?22.The objective function is then simply the sum ofall individual costs over N training examples, plusa regularization term, which we try to minimize:J(?)
=N?i=1s (xi) +?2 ??
?22 (2)3.2 Context-sensitive Morphological RNNThe cimRNN model, though simple, is interestingto attest if morphemic semantics could be learnedsolely from an embedding.
However, it is lim-ited in several aspects.
Firstly, the model hasno chance of improving representations for rarewords which might have been poorly estimated.For example, ?distinctness?
and ?unconcerned?are very rare, occurring only 141 and 340 timesin Wikipedia documents, even though their corre-sponding stems ?distinct?
and ?concern?
are veryfrequent (35323 and 26080 respectively).
Tryingto construct exactly those poorly-estimated wordvectors might result in a bad model with parame-ters being pushed in wrong directions.Secondly, though word embeddings learnedfrom an NLM could, in general, blend well boththe semantic and syntactic information, it wouldbe useful to explicitly model another kind of syn-tactic information, the word structure, as we trainour embeddings.
Motivated by these limitations,we propose a context-sensitive morphoRNN (csm-RNN) which integrates RNN structures into NLMtraining, allowing for contextual information be-ing taken into account in learning morphemiccompositionality.
Specifically, we adopt the NLMtraining approach proposed in (Collobert et al2011) to learn word embeddings, but build rep-resentations for complex words from their mor-phemes.
During learning, updates at the top levelof the neural network will be back-propagated allthe way till the morphemic layer.106  	  	  Figure 2: Context-sensitive morphological RNNhas two layers: (a) themorphological RNN, whichconstructs representations for words from theirmorphemes and (b) the word-based neural lan-guage which optimizes scores for relevant ngrams.Structure-wise, we stack the NLM on top of ourmorphoRNN as illustrated in Figure 2.
Complexwords like ?unfortunately?
and ?closed?
are con-structed from their morphemic vectors, unpre +fortunatestm + lysuf and closestm + dsuf, whereassimple words4, i.e.
stems, and affixes could belooked up from the morphemic embedding ma-trix We as in standard NLMs.
Once vectors of allcomplex words have been built, the NLM assignsa score for each ngram ni consisting of wordsx1, .
.
.
, xn as follows:s (ni) = ?
?f(W [x1; .
.
.
;xn] + b)Here, xj is the vector representing the word xj .We follow (Huang et al 2012) to use a sim-ple feed-forward network with one h-dimensionalhidden layer.
W ?
Rh?nd, b ?
Rh?1, and?
?
Rh?1 are parameters of the NLM, and f isan element-wise activation function as in Eq.
(1).We adopt a ranking-type cost in defining our ob-jective function to minimize as below:J(?)
=N?i=1max{0, 1?
s (ni) + s (ni)} (3)Here, N is the number of all available ngrams inthe training corpus, whereas ni is a ?corrupted?ngram created from ni by replacing its last wordwith a random word similar in spirit to (Smithand Eisner, 2005).
Our model parameters are?
= {We,Wm, bm,W , b,?
}.Such a ranking criterion influences the modelto assign higher scores to valid ngrams than to4?fortunate?, ?the?, ?bank?, ?was?, and ?close?.invalid ones and has been demonstrated in (Col-lobert et al 2011) to be both efficient and effectivein learning word representations.3.3 LearningOur models alternate between two stages: (1) for-ward pass ?
recursively construct morpheme trees(cimRNN, csmRNN) and language model struc-tures (csmRNN) to derive scores for training ex-amples and (2) back-propagation pass ?
computethe gradient of the corresponding object functionwith respect to the model parameters.For the latter pass, computing the objective gra-dient amounts to estimating the gradient for eachindividual cost ?s(x)??
, where x could be either aword (cimRNN) or an ngram (csmRNN).
We havethe objective gradient for the cimRNN derived as:?J(?)??
=N?i=1?s (xi)??
+ ?
?In the case of csmRNN, since the objectivefunction in Eq.
(3) is not differentiable, we use thesubgradient method (Ratliff et al 2007) to esti-mate the objective gradient as:?J(?)??
=?i:1?s(ni)+s(ni)>0?
?s (ni)??
+?s (ni)?
?Back-propagation through structures (Gollerand Ku?chler, 1996) is employed to compute thegradient for each individual cost with similar for-mulae as in (Socher et al 2010).
Unlike theirRNN structures over sentences, where each sen-tence could have an exponential number of deriva-tions, our morphoRNN structure per word is, ingeneral, deterministic.
Each word has a singlemorphological tree structure which is constructedfrom the main morpheme (the stem) and gradu-ally appended affixes in a fixed order (see Sec-tion 4 for more details).
As a result, both ourforward and backward passes over morphologi-cal structures are efficient with no recursive callsimplementation-wise.4 Unsupervised MorphologicalStructuresWe utilize an unsupervised morphological seg-mentation toolkit, named Morfessor by Creutz andLagus (2007), to obtain segmentations for wordsin our vocabulary.
Morfessor segments words in107two stages: (a) it recursively splits words to min-imize an objective inspired by the minimum de-scription length principle and (b) it labels mor-phemes with tags pre (prefixes), stm (stems),and suf (suffixes) using hidden Markov models.Morfessor captures a general word structure ofthe form (pre?
stm suf?
)+, which is handyfor words in morphologically rich languages likeFinnish or Turkish.
However, such general form iscurrently unnecessary in our models as the mor-phoRNNs assume input of the form pre?
stmsuf?
for efficient learning of the RNN structures:a stem is always combined with an affix to yield anew stem.5 We, thus, postprocess as follows:(1) Restrict segmentations to the form pre?stm{1, 2} suf?
: allow us to capture compounds.
(2) Split hyphenated words A-B as Astm Bstm.
(3) For a segmentation with two stems, pre?Astm Bstm suf?, we decide if one could be a mainstem while the other could functions as an affix.6Otherwise, we reject the segmentation.
This willprovide us with more interesting morphemes suchas alpre in Arabic names (al-jazeera, al-salem) andrelatedsuf in compound adjectives (health-related,government-related).
(4) To enhance precision, we reject a segmen-tation if it has either an affix or an unknown stem(not a word by itself) whose type count is below apredefined threshold7.The final list of affixes produced is given in Ta-ble 1.
Though generally reliable, our final seg-mentations do contain errors, most notably non-compositional ones, e.g.
depre faultstm edsuf orrepre turnstm ssuf.
With a sufficiently large num-ber of segmentation examples, we hope that themodel would be able to pick up general trendsfrom the data.
In total, we have about 22K com-plex words out of a vocabulary of 130K words.Examples of words with interesting affixes aregiven in Table 2.
Beside conventional affixes, non-conventional ones like ?0?
or ?mc?
help furthercategorize rare or unknown words into meaningfulgroups such as measurement words or names.5When multiple affixes are present, we use a simpleheuristic to first merge suffixes into stems and then combineprefixes.
Ideally, we would want to learn and generate anorder for such combination, which we leave for future work.6We first aggregate type counts of pairs (A, left) and (B,right) across all segmentations with two stems.
Once done,we label A as stm and B as suf if count (B, right) > 2 ?count (A, left), and conversely, we label them as Apre Bstm ifcount (A, left) > 2 ?
count(B, right).
Our rationale was thatPrefixes Suffixes0 al all anti auto cocounter cross de diselectro end ex first fivefocus four half high hy-per ill im in inter ir janjean long low market mcmicro mid multi neuronewly no non off oneover post pre pro re sec-ond self semi seven shortsix state sub super thirdthree top trans two ununder uni wellable al ally american anceate ation backed bankbased born controlled ddale down ed en er es fieldford free ful general headia ian ible ic in ing isationise ised ish ism ist ity iveization ize ized izing landled less ling listed ly mademaking man ment ness offon out owned related s shipshire style ton town up usville woodTable 1: List of prefixes and suffixes discovered ?conventional affixes in English are italicized.Affix Words0 0-acre, 0-aug, 0-billion, 0-centistokeanti anti-immigrant, antipsychoticscounter counterexample, counterinsurgencyhyper hyperactivity, hypercholesterolemiamc mcchesney, mcchord, mcdevittbank baybank, brockbank, commerzbankford belford, blandford, carlingfordland adventureland, bodoland, bottomlandless aimlessly, artlessness, effortlesslyowned bank-owned, city-owned disney-ownedTable 2: Sample affixes and corresponding words.5 ExperimentsAs our focus is in learning morphemic seman-tics, we do not start training from scratch, butrather, initialize our models with existing wordrepresentations.
In our experiments, we makeuse of two publicly-available embeddings (50-dimensional) provided by(Collobert et al 2011)(denoted as C&W)8 and Huang et al(2012) (re-ferred as HSMN)9.Both of these representations are trained onWikipedia documents using the same ranking-typecost function as in Eq.
(3).
The latter further uti-lizes global context and adopts a multi-prototypeapproach, i.e.
each word is represented by mul-tiple vectors, to better capture word semantics invarious contexts.
However, we only use theirsingle-prototype embedding10 and as we train, weaffixes occur more frequently than stems.7Set to 15 and 3 for affixes and stems respectively.8http://ronan.collobert.com/senna/.9http://www-nlp.stanford.edu/?ehhuang/.10The embedding obtained just before the clustering stepto build multi-prototype representation.108do not consider the global sentence-level contextinformation.
It is worth to note that these aspectsof the HSMN embedding ?
incorporating globalcontext and maintaining multiple prototypes ?
areorthogonal to our approach, which would be inter-esting to investigate in future work.For the context-sensitive morphoRNN model,we follow Huang et al(2012) to use the April2010 snapshot of the Wikipedia corpus (Shaouland Westbury, 2010).
All paragraphs containingnon-roman characters are removed while the re-maining text are lowercased and then tokenized.The resulting clean corpus contains about 986 mil-lion tokens.
Each digit is then mapped into 0, i.e.2013 will become 0000.
Other rare words not inthe vocabularies of C&W and HSMN are mappedto an UNKNOWN token, and we use <s> and</s> for padding tokens representing the begin-ning and end of each sentence.Follow (Huang et al 2012)?s implementation,which our code is based on initially, we use 50-dimensional vectors to represent morphemic andword embeddings.
For cimRNN, the regulariza-tion weight ?
is set to 10?2.
For csmRNN, we use10-word windows of text as the local context, 100hidden units, and no weight regularization.5.1 Word Similarity TaskSimilar to (Reisinger and Mooney, 2010) and(Huang et al 2012), we evaluate the quality of ourmorphologically-aware embeddings on the popu-lar WordSim-353 dataset (Finkelstein et al 2002),WS353 for short.
In this task, we compare corre-lations between the similarity scores given by ourmodels and those rated by human.To avoid overfitting our models to a singledataset, we benchmark our models on a vari-ety of others including MC (Miller and Charles,1991), RG (Rubenstein and Goodenough, 1965),SCWS?11 (Huang et al 2012), and our new rareword (RW) dataset (details in ?5.1.1).
Informationabout these datasets are summarized in Table 3We also examine these datasets from the?rareness?
aspect by looking at distributions ofwords across frequencies as in Table 4.
The firstbin counts unknown words in each dataset, whilethe remaining bins group words based on their11SCWS?
is a modified version of the Stanford?s contex-tual word similarities dataset.
The original one utilizes sur-rounding contexts in judging word similarities and includespairs of identical words, e.g.
financial bank vs. river bank.We exclude these pairs and ignore the provided contexts.pairs type raters scale Complex wordstoken typeWS353 353 437 13-16 0-10 24 17MC 30 39 38 0-4 0 0RG 65 48 51 0-4 0 0SCWS?
1762 1703 10 0-10 190 113RW (new) 2034 2951 10 0-10 987 686Table 3: Word similarity datasets and theirstatistics: number of pairs/raters/type counts aswell as rating scales.
The number of complexwords are shown as well (both type and tokencounts).
RW denotes our new rare word dataset.frequencies extracted from Wikipedia documents.It is interesting to observe that WS353, MC, RGcontain very frequent words and have few complexwords (only WS353 has).12 SCWS?
and RW havea more diverse set of words in terms of frequenciesand RW has the largest number of unknown andrare words, which makes it a challenging dataset.All words Complex wordsWS353 0 | 0 / 9 / 87 / 341 0 | 0 / 1 / 6 / 10MC 0 | 0 / 1 / 17 / 21 0 | 0 / 0 / 0 / 0RG 0 | 0 / 4 / 22 / 22 0 | 0 / 0 / 0 / 0SCWS?
26 | 2 / 140 / 472 / 1063 8 | 2 / 22 / 44 / 45RW 801 | 41 / 676 / 719 / 714 621 | 34 / 311 / 238 / 103Table 4: Word distribution by frequencies ?
dis-tinct words in each dataset are grouped based onfrequencies and counts are reported for the fol-lowing bins : unknown | [1, 100] / [101, 1000] /[1001, 10000] / [10001, ?).
We report counts forall words in each dataset as well as complex ones.5.1.1 Rare Word DatasetAs evidenced in Table 4, most existing word sim-ilarity datasets contain frequent words and few ofthem possesses enough rare or morphologicallycomplex words that we could really attest the ex-pressiveness of our morphoRNN models.
In fact,we believe a good embedding in general should beable to learn useful representations for not just fre-quent words but also rare ones.
That motivates usto construct another dataset focusing on rare wordsto complement existing ones.Our dataset construction proceeds in threestages: (1) select a list of rare words, (2) for eachof the rare words, find another word (not neces-sarily rare) to form a pair, and (3) collect humanjudgments on how similar each pair is.12All these counts are with respect to the vocabulary list inthe C&W embedding (we obtain similar figures for HSMN).109(5, 10] (10, 100] (100, 1000]un- untracked unrolls undissolved unrehearsed unflagging unfavourable unprecedented unmarried uncomfortable-al apocalyptical traversals bestowals acoustical extensional organismal directional diagonal spherical-ment obtainment acquirement retrenchments discernment revetment rearrangements confinement establishment managementword1 untracked unflagging unprecedented apocalyptical organismal diagonal obtainment discernment confinementword2 inaccessible constant new prophetic system line acquiring knowing restraintTable 5: Rare words (top) ?
word1 by affixes and frequencies and sample word pairs (bottom).Rare word selection: our choices of rare words(word1) are based on their frequencies ?
based onfive bins (5, 10], (10, 100], (100, 1000], (1000,10000], and the affixes they possess.
To create adiverse set of candidates, we randomly select 15words for each configuration (a frequency bin, anaffix).
At the scale of Wikipedia, a word withfrequency of 1-5 is most likely a junk word, andeven restricted to words with frequencies abovefive, there are still many non-English words.
Tocounter such problems, each word selected is re-quired to have a non-zero number of synsets inWordNet(Miller, 1995).Table 5 (top) gives examples of rare words se-lected and organized by frequencies and affixes.
Itis interesting to find out that words like obtainmentand acquirement are extremely rare (not in tradi-tional dictionaries) but are perfectly understand-able.
We also have less frequent words like revet-ment from French or organismal from biology.Pair construction: following (Huang et al2012), we create pairs with interesting relation-ships for each word1 as follow.
First, a Word-Net synset of word1 is randomly selected, and weconstruct a set of candidates which connect to thatsynset through various relations, e.g., hypernyms,hyponyms, holonyms, meronyms, and attributes.A word2 is then randomly selected from these can-didates, and the process is repeated another timeto generate a total of two pairs for each word1.Sample word pairs are given in Table 5 in whichword2 includes mostly frequent words, implyinga balance of words in terms of frequencies in ourdataset.
We collected 3145 pairs after this stageHuman judgment: we use Amazon Mechani-cal Turk to collect 10 human similarity ratings ona scale of [0, 10] per word pair.13 Such procedurehas been demonstrated by Snow et al(2008) inreplicating ratings for the MC dataset, achievingclose inter-annotator agreement with expert raters.Since our pairs contain many rare words which are13We restrict to only US-based workers with 95% approvalrate and ask for native speakers to rate 20 pairs per hit.challenging even to native speakers, we ask ratersto indicate for each pair if they do not know thefirst word, the second word, or both.
We use suchinformation to collect reliable ratings by either dis-card pairs which many people do not know or col-lect additional ratings to ensure we have 10 rat-ings per pair.14 As a result, only 2034 pairs areretained.5.2 ResultsWe evaluate the quality of our morphoRNN em-beddings through the word similarity task dis-cussed previously.
The Spearman?s rank correla-tion is used to gauge how well the relationship be-tween two variables, the similarity scores given bythe NLMs and the human annotators, could be de-scribed using a monotonic function.Detailed performance of the morphoRNN em-beddings trained from either the HSMN or theC&W embeddings are given in Table 7 forall datasets.
We also report baseline results(rows HSMN, C&W) using these initial embed-dings alone, which interestingly reveals strengthsand weaknesses of existing embeddings.
WhileHSMN is good for datasets with frequent words(WS353, MC, and RG), its performances for thosewith more rare and complex words (SCWS?
andRW) are much inferior than those of C&W, andvice versa.
Additionally, we consider two slightlymore competitive baselines (rows +stem) basedon the morphological segmentation of unknownwords: instead of using a universal vector repre-senting all unknown words, we use vectors rep-resenting the stems of unknown words.
Thesebaselines yield slightly better performance for theSCWS?
and RW datasets while the trends we men-tioned earlier remain the same.Our first model, the context-insensitive mor-phoRNN (cimRNN), outperforms its correspond-ing baseline significantly over the rare word14In our later experiments, an aggregated rating is derivedfor each pair.
We first discard ratings not within one standarddeviation of the mean, and then estimate a new mean fromthe remaining ones to use as an aggregated rating.110Words C&W C&W + cimRNN C&W + csmRNNcommenting insisting insisted focusing hinted republishing accounting expounding commented comments criticizingcomment commentary rant statement remark commentary rant statement remark rant commentary statement anecdotedistinctness morphologies pesawat clefts modality indistinct tonality spatiality indistinct distinctiveness largeness uniquenessdistinct different distinctive broader narrower different distinctive broader divergent divergent diverse distinctive homogeneousunaffected unnoticed dwarfed mitigated disaffected unconstrained uninhibited undesired unhindered unrestrictedaffected caused plagued impacted damaged disaffected unaffected mitigated disturbed complicated desired constrained reasonedunaffect ?
affective affecting affectation unobserved affective affecting affectation restrictiveaffect exacerbate impacts characterize affects affectation exacerbate characterize decrease arise complicate exacerbateheartlessness ?
fearlessness vindictiveness restlessness depersonalization terrorizes sympathizesheartless merciless sadistic callous mischievous merciless sadistic callous mischievous sadistic callous merciless hideousheart death skin pain brain life blood death skin pain brain life blood death brain blood skin lung mouthsaudi-owned avatar mohajir kripalani fountainhead saudi-based somaliland al-jaber saudi-based syrian-controlled syrian-backedshort-changed kindled waylaid endeared peopled conformal conformist unquestionable short-termism short-positions self-sustainableTable 6: Nearest neighbors.
We showmorphologically related words and their closest words in differentrepresentations (?unaffect?
is a pseudo-word; ?
marks no results due to unknown words).WS353 MC RG SCWS?
RWHSMN 62.58 65.90 62.81 32.11 1.97+stem 62.58 65.90 62.81 32.11 3.40+cimRNN 62.81 65.90 62.81 32.97 14.85+csmRNN 64.58 71.72 65.45 43.65 22.31C&W 49.77 57.37 49.30 48.59 26.75+stem 49.77 57.37 49.30 49.05 28.03+cimRNN 51.76 57.37 49.30 47.00 33.24+csmRNN 57.01 60.20 55.40 48.48 34.36Table 7: Word similarity task ?
shown are Spear-man?s rank correlation coefficient (?
?
100) be-tween similarity scores assigned by neural lan-guage models and by human annotators.
stem in-dicates baseline systems in which unknown wordsare represented by their stem vectors.
cimRNN andcsmRNN refer to our context insensitive and sensi-tive morphological RNNs respectively.dataset.
The performance is constant for MC andRG (with no complex words) and modestly im-proved for MC (with some complex words ?
seeTable 4).
This is expected since the cimRNNmodel only concerns about reconstructing theoriginal embedding (while learning word struc-tures), and the new representation mostly differsat morphologically complex words.
For SCWS?,the performance, however, decreases when train-ing with C&W, which perhaps is due to: (a) thebaseline performance of C&W for SCWS?
is com-petitive and (b) the model trades off between learn-ing syntactics (the word structure) and capturingsemantics, which requires context information.On the other hand, the context-sensitive mor-phoRNN (csmRNN) consistently improves corre-lations over the cimRNN model for all datasets,demonstrating the effectiveness of using surround-ing contexts in learning both morphological syn-tactics and semantics.
It also outperforms thecorresponding baselines by a good margin for alldatasets (except for SCWS?).
This highlights thefact that our method is reliable and potentially ap-plicable for other embeddings.6 AnalysisTo gain a deeper understanding of how our mor-phoRNN models have ?moved?
word vectorsaround, we look at nearest neighbors of sev-eral complex words given by various embed-dings, where cosine similarity is used as a dis-tance metric.
Examples are shown in Table 6for three representations: C&W and the context-insensitive/sensitive morphoRNN models trainedon the C&W embedding.15Syntactically, it is interesting to observe thatthe cimRNN model could well enforce structuralagreement among related words.
For example, itreturns V-ing as nearest neighbors for ?comment-ing?
and similarly, JJ-ness for ?fearlessness?, anunknown word that C&W cannot handle.
How-ever, for those cases, the nearest neighbors arebadly unrelated.On the semantic side, we notice that whenstructural agreement is not enforced, the cimRNNmodel tends to cluster words sharing the samestem together, e.g., rows with words of the formaffect .16 This might be undesirable when wewant to differentiate semantics of words sharingthe same stem, e.g.
?affected?
and ?unaffected?.The csmRNN model seems to balance well be-tween the two extremes (syntactic and seman-tic) by taking into account contextual information15Results of HSMN-related embeddings are not shown, butsimilar trends follow.16?unaffect?
is a pseudo-word that we inserted.111when learning morphological structures.
It returnsneighbors of the same structure un ed for ?unaf-fected?, but does not include any negation of ?af-fected?
in the top 10 results when ?affected?
isqueried.17 Even better, the answers for ?distinct-ness?
have blended well both types of results.7 ConclusionThis paper combines recursive neural networks(RNNs) and neural language models (NLMs) ina novel way to learn better word representa-tions.
Each of these components contributes tothe learned syntactic-semantic word vectors in aunique way.
The RNN explicitly models the mor-phological structures of words, i.e., the syntacticinformation, to learn morphemic compositional-ity.
This allows for better estimation of rare andcomplex words and a more principled way of han-dling unseen words, whose representations couldbe constructed from vectors of knownmorphemes.The NLMs, on the other hand, utilize surround-ing word contexts to provide further semanticsto the learned morphemic representations.
Asa result, our context-sensitive morphoRNN em-beddings could significantly outperform existingembeddings on word similarity tasks for manydatasets.
Our analysis reveals that the model couldblend well both the syntactic and semantic infor-mation in clustering related words.
We have alsomade available a word similarity dataset focusingon rare words to complement existing ones whichtend to include frequent words.Lastly, as English is still considered limitedin terms of morphology, our model could poten-tially yield even better performance when appliedto other morphologically complex languages suchas Finnish or Turkish, which we leave for futurework.
Also, even within English, we expect ourmodel to be value to other domains, such as bio-NLP with complicated but logical taxonomy.AcknowledgementsWe thank the anonymous reviewers for their feed-back and Eric Huang for making his various piecesof code available to us as well as answering ourquestions on different datasets.
Stanford Uni-versity gratefully acknowledges the support ofthe Defense Advanced Research Projects Agency(DARPA) Deep Exploration and Filtering of Text17?disaffected?
is ranked 5th for the first query while ?af-fecting?
occurs at position 8 for the latter.
(DEFT) Program under Air Force Research Lab-oratory (AFRL) contract no.
FA8750-13-2-0040and the DARPA Broad Operational LanguageTranslation (BOLT) program through IBM.
Anyopinions, findings, and conclusion or recommen-dations expressed in this material are those of theauthors and do not necessarily reflect the view ofthe DARPA, AFRL, or the US government.ReferencesAndrei Alexandrescu and Katrin Kirchhoff.
2006.Factored neural language models.
In NAACL.Y.
Bengio, R. Ducharme, P. Vincent, and C. Janvin.2003.
A neural probabilistic language model.
Jour-nal of Machine Learning Research, 3:1137?1155.Alexander Clark.
2003.
Combining distributional andmorphological information for part of speech induc-tion.
In EACL.R.
Collobert and J. Weston.
2008.
A unified archi-tecture for natural language processing: deep neuralnetworks with multitask learning.
In ICML.R.
Collobert, J. Weston, L. Bottou, M. Karlen,K.
Kavukcuoglu, and P. Kuksa.
2011.
Natural lan-guage processing (almost) from scratch.
Journal ofMachine Learning Research, 12:2493?2537.R.
Collobert.
2011.
Deep learning for efficient dis-criminative parsing.
In AISTATS.Mathias Creutz and Krista Lagus.
2007.
Unsuper-vised models for morpheme segmentation and mor-phology learning.
ACM Transactions on Speech andLanguage Processing, 4:3:1?3:34.Lev Finkelstein, Evgeniy Gabrilovich, Yossi Matias,Ehud Rivlin, Zach Solan, Gadi Wolfman, and Ey-tan Ruppin.
2002.
Placing search in context: theconcept revisited.
ACM Transactions on Informa-tion Systems, 20(1):116?131.Michael Gasser and Chan-Do Lee.
1990.
A short-term memory architecture for the learning of mor-phophonemic rules.
In NIPS.Michael Gasser.
1994.
Acquiring receptive morphol-ogy: A connectionist model.
In ACL.X.
Glorot, A. Bordes, and Y. Bengio.
2011.
Domainadaptation for large-scale sentiment classification: Adeep learning approach.
In ICML.C.
Goller and A. Ku?chler.
1996.
Learning task-dependent distributed representations by backprop-agation through structure.
IEEE Transactions onNeural Networks, 1:347?352.112E.
H. Huang, R. Socher, C. D. Manning, and A. Y. Ng.2012.
Improving word representations via globalcontext and multiple word prototypes.
In AnnualMeeting of the Association for Computational Lin-guistics (ACL).Terry Koo, Xavier Carreras, and Michael Collins.2008.
Simple semi-supervised dependency parsing.In ACL.Angeliki Lazaridou, Marco Marelli, Roberto Zampar-elli, and Marco Baroni.
2013.
Compositional-lyderived representations of morphologically complexwords in distributional semantics.
In ACL.Tomas Mikolov and Geoffrey Zweig.
2012.
Contextdependent recurrent neural network language model.In SLT.Tomas Mikolov, Martin Karafia?t, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2010.
Recur-rent neural network based language model.
In IN-TERSPEECH.Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky?, and Sanjeev Khudanpur.
2011.
Exten-sions of recurrent neural network language model.In ICASSP.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.2013.
Linguistic regularities in continuous spaceword representations.
In NAACL-HLT.George A. Miller and Walter G. Charles.
1991.
Con-textual correlates of semantic similarity.
Languageand Cognitive Processes, 6(1):1?28.G.A.
Miller.
1995.
WordNet: A Lexical Database forEnglish.
Communications of the ACM.Andriy Mnih and Geoffrey Hinton.
2007.
Three newgraphical models for statistical language modelling.In ICML.Andriy Mnih and Geoffrey Hinton.
2009.
A scalablehierarchical distributed language model.
In NIPS.Frederic Morin.
2005.
Hierarchical probabilistic neu-ral network language model.
AIstats?05.
In AIS-TATS.K.
Plunkett and V. Marchman.
1991.
U-shaped learn-ing and frequency effects in a multi-layered percep-tron: implications for child language acquisition.Cognition, 38(1):43?102.Lev Ratinov and Dan Roth.
2009.
Design challengesand misconceptions in named entity recognition.
InCoNLL.Nathan D. Ratliff, J. Andrew Bagnell, and Martin A.Zinkevich.
2007.
Online subgradient methods forstructured prediction.Joseph Reisinger and Raymond J. Mooney.
2010.Multi-prototype vector-space models of word mean-ing.
In NAACL.Herbert Rubenstein and John B. Goodenough.
1965.Contextual correlates of synonymy.
Communica-tions of the ACM, 8(10):627?633.David E. Rumelhart and James L. McClelland.
1986.On learning the past tenses of English verbs.
In J. L.McClelland, D. E. Rumelhart, and PDP ResearchGroup, editors, Parallel Distributed Processing.
Vol-ume 2: Psychological and Biological Models, pages216?271.
MIT Press.Cyrus Shaoul and Chris Westbury.
2010.
The West-bury lab wikipedia corpus.
Edmonton, AB: Univer-sity of Alberta.Noah A. Smith and Jason Eisner.
2005.
Contrastiveestimation: training log-linear models on unlabeleddata.
In ACL.Rion Snow, Brendan O?Connor, Daniel Jurafsky, andAndrew Y. Ng.
2008.
Cheap and fast?but is itgood?
: Evaluating non-expert annotations for nat-ural language tasks.
In EMNLP.Richard Socher, Christopher Manning, and AndrewNg.
2010.
Learning continuous phrase represen-tations and syntactic parsing with recursive neuralnetworks.
In NIPS*2010 Workshop on Deep Learn-ing and Unsupervised Feature Learning.R.
Socher, E. H. Huang, J. Pennington, A. Y. Ng, andC.
D. Manning.
2011a.
Dynamic pooling and un-folding recursive autoencoders for paraphrase detec-tion.
In NIPS.R.
Socher, Cliff C. Lin, A. Y. Ng, and C. D. Manning.2011b.
Parsing natural scenes and natural languagewith recursive neural networks.
In ICML.R.
Socher, J. Pennington, E. H. Huang, A. Y. Ng, andC.
D. Manning.
2011c.
Semi-supervised recursiveautoencoders for predicting sentiment distributions.In EMNLP.R.
Socher, B. Huval, C. D. Manning, and A. Y. Ng.2012.
Semantic compositionality through recursivematrix-vector spaces.
In EMNLP.J.
Turian, L. Ratinov, and Y. Bengio.
2010.
Word rep-resentations: a simple and general method for semi-supervised learning.
In ACL.113
