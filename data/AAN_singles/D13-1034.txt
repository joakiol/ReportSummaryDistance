Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 345?356,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAdaptor Grammars for Learning Non-Concatenative MorphologyJan A. Botha and Phil BlunsomDepartment of Computer ScienceUniversity of OxfordOxford, OX1 3QD, UK{jan.botha,phil.blunsom}@cs.ox.ac.ukAbstractThis paper contributes an approach forexpressing non-concatenative morphologicalphenomena, such as stem derivation inSemitic languages, in terms of a mildlycontext-sensitive grammar formalism.
Thisoffers a convenient level of modelling ab-straction while remaining computationallytractable.
The nonparametric Bayesian frame-work of adaptor grammars is extended to thisricher grammar formalism to propose a prob-abilistic model that can learn word segmenta-tion and morpheme lexicons, including oneswith discontiguous strings as elements, fromunannotated data.
Our experiments on He-brew and three variants of Arabic data findthat the additional expressiveness to captureroots and templates as atomic units improvesthe quality of concatenative segmentation andstem identification.
We obtain 74% accuracyin identifying triliteral Hebrew roots, whileperforming morphological segmentation withan F1-score of 78.1.1 IntroductionUnsupervised learning of morphology is the taskof acquiring, from unannotated data, the intra-wordbuilding blocks of a language and the rules by whichthey combine to form words.
This task is of interestboth as a gateway for studying language acquisitionin humans and as a way of producing morphologicalanalyses that are of practical use in a variety of nat-ural language processing tasks, including machinetranslation, parsing and information retrieval.A particularly interesting version of the morphol-ogy learning problem comes from languages thatuse templatic morphology, such as Arabic, He-brew and Amharic.
These Semitic languages de-rive verb and noun stems by interspersing abstractroot morphemes into templatic structures in a non-concatenative way.
For example, the Arabic rootk?t?b can combine with the template (i-a) to derivethe noun stem kitab (book).
Established morpho-logical analysers typically ignore this process andsimply view the derived stems as elementary units(Buckwalter, 2002), or their account of it coincideswith a requirement for extensive linguistic knowl-edge and hand-crafting of rules (Finkel and Stump,2002; Schneider, 2010; Altantawy et al 2010).
Theformer approach is bound to suffer from vocabu-lary coverage issues, while the latter clearly doesnot transfer easily across languages.
The practicalappeal of unsupervised learning of templatic mor-phology is that it can overcome these shortcomings.Unsupervised learning of concatenative morphol-ogy has received extensive attention, partly drivenby the MorphoChallenge (Kurimo et al 2010) in re-cent years, but that is not the case for root-templaticmorphology (Hammarstro?m and Borin, 2011).In this paper we present a model-based methodthat learns concatenative and root-templatic mor-phology in a unified framework.
We build on twodisparate strands of work from the literature: Firstly,we apply simple Range Concatenating Grammars(SRCGs) (Boullier, 2000) to parse contiguous anddiscontiguous morphemes from an input string.These grammars are mildly-context sensitive (Joshi,1985), a superset of context-free grammars thatretains polynomial parsing time-complexity.
Sec-ondly, we generalise the nonparametric Bayesianlearning framework of adaptor grammars (Johnsonet al 2007) to SRCGs.1 This should also be rel-1Our formulation is in terms of SRCGs, which are equiv-alent in power to linear context-free rewrite systems (Vijay-Shanker et al 1987) and multiple context-free grammars (Sekiet al 1991), all of which are weaker than (non-simple) rangeconcatenating grammars (Boullier, 2000).345evant to other applications of probabilistic SRCGs,e.g.
in parsing (Maier, 2010), translation (Kaesham-mer, 2013) and genetics (Kato et al 2006).In addition to unannotated data, our method re-quires as input a minimal set of high-level grammarrules that encode basic intuitions of the morphology.This is where there would be room to become verylanguage specific.
Our aim, however, is not to obtaina best-published result in a particular language, butrather to create a method that is applicable acrossa variety of morphological processes.
The specificrules used in our empirical evaluation on Arabic andHebrew therefore contain hardly any explicit lin-guistic knowledge about the languages and are ap-plicable across the family of Semitic languages.2 A powerful grammar for morphologyConcatenative morphology lends itself well to ananalysis in terms of finite-state transducers (FSTs)(Koskenniemi, 1984).
With some additional effort,FSTs can also encode non-concatenative morphol-ogy (Kiraz, 2000; Beesley and Karttunen, 2003;Cohen-Sygal and Wintner, 2006; Gasser, 2009).
De-spite this seeming adequacy of regular languages todescribe morphology, we see two main shortcom-ings that motivate moving further up the Chom-sky hierarchy of formal languages: first is the is-sue of learning.
We are not aware of successful at-tempts at inducing FST-based morphological analy-sers in an unsupervised way, and believe the chal-lenge lies in the fact that FSTs do not offer a conve-nient way of expressing prior linguistic intuitions toguide the learning process.
Secondly, an FST com-posed of multiple machines might capture morpho-logical processes well and excel at analysis, but in-terpretability of its internal operations are limited.These shortcomings are overcome for concate-native morphology by context-free adaptor gram-mars, which allowed diverse segmentation modelsto be formulated and investigated within a singleframework (Johnson et al 2007; Johnson, 2008;Sirts and Goldwater, 2013).
In principle, that cov-ers a wide range of phenomena (typical examplelanguage in parentheses): affixal inflection (Czech)and derivation (English), agglutinative derivation(Turkish, Finnish), compounding (German).
Ouragenda here is to extend that approach to includenon-concatenative processes such as root-templaticderivation (Arabic), infixation (Tagalog) and cir-cumfixation (Indonesian).
In this pursuit, an ab-straction that permits discontiguous constituents isa highly useful modelling tool, but requires lookingbeyond context-free grammars.An idealised generative grammar that would cap-ture all the aforementioned phenomena could looklike this:Word?
(Pre?
Stem Suf?
)+ (1)e.g.
English un+accept+ableStem |Pre |Suf?
Morph (2)Stem?
intercal (Root,Template) (3)e.g.
Arabic derivation k?t?b + i?a?
kitab (book)Stem?
infix (Stem, Infix) (4)e.g.
Tagalog sulat (write)?
sumulat (wrote)Stem?
circfix (Stem,Circumfix) (5)e.g.
Indonesian percaya (to trust)?
kepercayaan (belief)where the symbols (excluding Word and Stem) im-plicitly expand to the relevant terminal strings.
Thebold-faced ?functions?
combine the potentially dis-contiguous yields of the argument symbols into sin-gle contiguous strings, e.g.
infix(s?ulat, um) pro-duces stem sumulat.Taken by themselves, the first two rules are sim-ply a CFG that describes word formation as theconcatenation of stems and affixes, a formulationthat matches the underlying grammar of Morfessor(Creutz and Lagus, 2007), a well-studied unsuper-vised model.The key aim of our extension is that we want thegrammar to capture a discontiguous string like k?t?bas a single constituent in a parse tree.
This leads towell-understood problems in probabilistic grammars(e.g.
what is this rule?s probability?
), but also corre-sponds to the linguistic consideration that k?t?b is aproper morpheme of the language (Prunet, 2006).3 Simple range concatenating grammarsIn this section we define SRCGs formally andillustrate how they can be used to model non-concatenative morphology.
SRCGs define lan-guages that are recognisable in polynomial time, yetcan capture discontiguous elements of a string un-der a single category (Boullier, 2000).
An SRCG-346rule operates on vectors of ranges in contrast to theway a CFG-rule operates on single ranges (spans).In other words, a non-terminal symbol in an SRCG(CFG) derivation can dominate a subset (substring)of terminals in an input string.3.1 FormalismAn SRCG G is a tuple (N,T, V, P, S), withfinite sets of non-terminals (N ), termi-nals (T ) and variables (V ), with a start sym-bol S ?
N .
A rewrite rule p ?
P of rankr = ?
(p) ?
0 has the form A(?1, .
.
.
, ??
(A)) ?B1(?1,1, .
.
.
, ?1,?
(B1)) .
.
.
Br(?r,1, .
.
.
, ?r,?
(Br)),where each ?, ?
?
(T ?
V )?, and ?
(A) is thenumber of arguments a non-terminal A has, calledits arity.
By definition, the start symbol has arity 1.Any variable v ?
V appearing in a given rulemust be used exactly once on each side of therule.
Terminating rules are written with  as theright-hand side and thus have rank 0.A range is a pair of integers (i, j) denoting thesubstring wi+1 .
.
.
wj of a string w = w1 .
.
.
wn.A non-terminal becomes instantiated when its vari-ables are bound to ranges through substitution.
Vari-ables within an argument imply concatenation andtherefore have to bind to adjacent ranges.An instantiated non-terminal A?
is said to de-rive  if the consecutive application of a sequenceof instantiated rules rewrite it as .
A string w iswithin the language defined by a particular SRCGiff the start symbol S, instantiated with the exhaus-tive range (0, wn), derives .An important distinction with regard to CFGs isthat, due to the instantiation mechanism, the order-ing of non-terminals on the right-hand side of anSRCG rule is irrelevant, i.e.
A(ab) ?
B(a)C(b)and A(ab) ?
C(b)B(a) are the same rule.2 Con-sequently, the isomorphisms of any given SRCGderivation tree all encode the same string, which isuniquely defined through the instantiation process.3.2 Application to morphological analysisA fragment of the idealised grammar schema fromthe previous section (?2) can be rephrased as anSRCG by writing the rules in the newly introduced2Certain ordering restrictions over the variables within anargument need to hold for an SRCG to indeed be a simple RCG(Boullier, 2000).Word(wakitabi)Suf(i)iStm(kitab)Template(i,a)Root(k,t,b)Pre(wa)aw k i t a bFigure 1: Example derivation for wakitabi (and mybook) using the SRCG fragment from ?3.2.
CFGscannot capture such crossing branches.notation, and supplying a definition of the intercalfunction as simply another rule of the grammar, withinstantiation for w = kitab shown below:Word(abc)?
Pre(a) Stem(b) Suf(c)Stem(abcde)?
Root(a, c, e) Template(b, d),Stem(?0..1?, ?1..2?, ?2..3?, ?3..4?, ?4..5?)?
Root(?0..1?, ?2..3?, ?4..5?
)Template(?1..2?, ?3..4?
)Given an appropriate set of grammar rules (as wepresent in ?5), we can parse an input string to ob-tain a tree as shown in Figure 1.
The overlappingbranches of the tree demonstrate that this grammarcaptures something a CFG could not.
From the parsetree one can read off the word?s root morpheme andthe template used.Although SRCGs specify mildly context-sensitivegrammars, each step in a derivation is context-free ?a node?s expansion does not depend on other partsof the tree.
This property implies that a recogni-tion/parsing algorithm can have a worst-case timecomplexity that is polynomial in the input length n,O(n(?+1)?)
for arity ?
and rank ?, which reducesto O(n3?)
for a binarised grammar.
To capture themaximal case of a root with k ?
1 characters andk discontiguous templatic characters forming a stemwould require a grammar that has arity ?
= k. ForArabic, which has up to quadriliteral roots (k = 5),the time complexity would be O(n15).3 This is adaunting proposition for parsing, but we are careful3The trade-off between arity and rank with respect to pars-ing complexity has been characterised (Gildea, 2010), and theappropriate refactoring may bring down the complexity for ourgrammars too.347to set up our application of SRCGs in such a waythat this is not too big an obstacle:Firstly, our grammars are defined over the char-acters that make up a word, and not over words thatmake up a sentence.
As such, the input length nwould tend to be shorter than when parsing full sen-tences from a corpus.Secondly, we do type-based morphological analy-sis, a view supported by evidence from Goldwater etal.
(2006), so each unique word in a dataset is onlyever parsed once with a given grammar.
The set ofword types attested in the data sources of interesthere is fairly limited, typically in the tens of thou-sands.
For these reasons, our parsing and inferencetasks turn out to be tractable despite the high timecomplexity.4 Learning4.1 Probabilistic SRCGThe probabilistic extension of SRCGs is similar tothe probabilistic extension of CFGs, and has beenused in other guises (Kato et al 2006; Maier, 2010).Each rule r ?
P has an associated probability ?rsuch that?r?PA?r = 1.
A random string inthe language of the grammar can then be obtainedthrough a generative procedure that begins with thestart symbol S and iteratively expands it until deriv-ing : At each step for some current symbol A, arewrite rule r is sampled randomly from PA in ac-cordance with the distribution over rules and usedto expand A.
This procedure terminates when nofurther expansions are possible.
Of course, expan-sions need to respect the range concatenating and or-dering constraints imposed by the variables in rules.The expansions imply a chain of variable bindingsgoing down the tree, and instantiation happens onlywhen rewriting into s but then propagates back upthe tree.The probability P (w, t) of the resulting tree t andterminal string w is the product?r ?r over the se-quence of rewrite rules used.
This generative proce-dure is a conceptual device; in practice, one wouldcare about parsing some input string under this prob-abilistic grammar.4.2 PYSRCAGA central property of the generative procedure un-derlying probabilistic SRCGs is the fact that eachexpansion happens independently, both of the otherexpansions in the tree under construction and of anyother trees.
To some extent, this flies in the face ofthe reality of estimating a grammar from text, whereone would expect certain sub-trees to be used repeat-edly across different input strings.Adaptor grammars weaken this independence as-sumption by allowing whole subtrees to be reusedduring expansion.
Informally, they act as a cache oftree fragments whose tendency to be reused duringexpansion is governed by the choice of adaptor func-tion.
Following earlier applications of adaptor gram-mars (Johnson et al 2007; Huang et al 2011), weemploy the Pitman-Yor process (Pitman, 1995; Pit-man and Yor, 1997) as adaptor function.A Pitman-Yor Simple Range Concatenat-ing Adaptor Grammar (PYSRCAG) is a tupleG = (GS ,M,a, b,?
), where GS is a probabilisticSRCG as defined before and M ?
N is a setof adapted non-terminals.
The vectors a and b,indexed by the elements of M , are the discountand concentration parameters for each adapted non-terminal, with a ?
[0, 1], b ?
0. ?
are parameters toDirichlet priors on the rule probabilities ?.PYSRCAG defines a generative process over a setof trees T .
Unadapted non-terminals A?
?
N \Mare expanded as before (?4.1).
For each adaptednon-terminal A ?
M , a cache CA is maintainedfor storing the terminating tree fragments expandedfrom A earlier in the process, and we denote thefragment corresponding to the i-th expansion of Aas zi.
In other words, the sequence of indices ziis the assignment of a sequence of expansions ofA to particular tree fragments.
Given a cache CAthat has n previously generated trees comprisingm unique trees each used n1, .
.
.
, nm times (wheren =?k nk), the tree fragment for the next expan-sion of A, zn+1, is sampled conditional on the pre-vious assignments z< according tozn+1|z< ?
{nk?an+b if zn+1 = k ?
[1,m]ma+bn+b if zn+1 = m+ 1,where a and b are those elements of a and b cor-responding to A.
The first case denotes the situa-tion where a previously cached tree is reused for thisn + 1-th expansion of A; to be clear, this expandsA with a fully terminating tree fragment, meaningthat none of the nodes descending from A in the348tree being generated are subject to further expan-sion.
The second case by-passes the cache and ex-pandsA according to the rules PA and rule probabil-ities ?A of the underlying SRCG GS .
Other cachesCB(B ?
M) may come into play during thoseexpansions of the descendants of A; thus a PYS-RCAG can define a hierarchical stochastic process.Both cases eventually result in a terminating tree-fragment for A, which is then added to the cache,updating the counts n, nzn+1 and potentially m.The adaptation does not affect the string languageof GS , but it maps the distribution over trees to onethat is distributed according to the PYP.The invariance of SRCGs trees under isomor-phism would make the probabilistic model deficient,but we side-step this issue by requiring that grammarrules are specified in a canonical way that ensuresa one-to-one correspondence between the order ofnodes in a tree and of terminals in the yield.4.3 Inference under PYSRCAGThe inference procedure under our model is verysimilar to that of CFG PY-adaptor grammars, so werestate the central aspects here but refer the readerto the original article by Johnson et al(2007) forfurther details.
First, one may integrate out theadaptors to obtain a single distribution over the setof trees generated from a particular non-terminal.Thus, the joint probability of a particular sequence zfor the adapted non-terminal A with cached counts(n1, .
.
.
, nm) isPY (z|a, b) =?mk=1 (a(k ?
1) + b)?nk?1j=1 (j ?
a)?n?1i=0 (i+ b).
(6)Taking all the adapted non-terminals into account,the joint probability of a set of full trees T under thegrammar G isP (T |a, b,?)
=?A?MB(?A + fA)B(?A)PY (z(T )|a, b),(7)where fA is a vector of the usage counts of rulesr ?
PA across T , and B is the Euler beta function.The posterior distribution over a set of stringsw is obtained by marginalising (7) over all treesthat have w as their yields.
This is intractable tocompute directly, so instead we use MCMC tech-niques to obtain samples from that posterior using acomponent-wise Metropolis-Hastings sampler.
Thesampler works by visiting each string w in turn anddrawing a new tree for it under a proposal grammarGQ and randomly accepting that as the new analysisfor w according to the Metropolis-Hastings accept-reject probability.
As proposal grammar, we use theanalogous approximation of our G as Johnson et alused for PCFGs, namely by taking a static snapshotGQ of the adaptor grammar where additional rulesrewrite adapted non-terminals as the terminal stringsof their cached trees.
Drawing a sample from theproposal distribution is then a matter of drawing arandom tree from the parse chart of w under GQ.Lastly, the adaptor hyperparameters a and bare modelled by placing flat Beta(1, 1) and vagueGamma(10, 0.1) priors on them, respectively, andinferring their values using slice sampling (Johnsonand Goldwater, 2009).5 Modelling root-templatic morphologyWe start with a CFG-based adaptor grammar4 thatmodels words as a stem and any number of prefixesand suffixes:Word?
Pre?
Stem Suf?
(8)Pre | Stem | Suf?
Char+ (9)This fragment can be seen as building on the stem-and-affix adaptor grammar presented in (Johnson etal., 2007) for morphological analysis of English, ofwhich a later version also covers multiple affixes(Sirts and Goldwater, 2013).
In the particular case ofArabic, multiple affixes are required to handle the at-tachment of particles and proclitics onto base words.To extend this to complex stems consisting of aroot with three radicals we have rules like the fol-lowing:Stem(abcdefg)?
R3(b, d, e) T4(a, c, e, g) (10)Stem(abcdef)?
R3(a, c, e) T3(b, d, f) (11)Stem(abcde)?
R3(a, c, e) T2(b, d) (12)Stem(abcd)?
R3(a, c, d) T1(b) (13)Stem(abc)?
R3(a, b, c) (14)4Adapted non-terminals are indicated by underlining andwe use the following abbreviations: X ?
Y+ means oneor more instances of Y and encodes the rules X ?
Ys andYs ?
Ys Y | Y.
Similarly, X ?
Y?
Z allows zero or moreinstances of Y and encodes the rules X ?
Z and X ?
Y+ Z.Further relabelling is added as necessary to avoid cycles amongadapted non-terminals.349The actual rules include certain permutations ofthese, e.g.
rule (13) has a variant R3(a, b, d)T1(c).In unvocalised text, the standard written form ofModern Standard Arabic (MSA), it may happen thatthe stem and the root of a word form are one and thesame.
So while rule (14) may look trivial, it ensuresthat in such cases the radicals are still captured as de-scendants of the non-terminal category R3, therebymaking their appearance in the cache.A discontiguous non-terminal An is rewrittenthrough recursion on its arity down to 1, i.e.An(v1, .
.
.
, vn)?
Al(v1, .
.
.
, vn?1) Char(vn) withbase case A1(v) ?
Char(v), where Char rewritesall individual terminals as , vi are variables andl = n?1.5 Note that although we provide the modelwith two sets of discontiguous non-terminals R andT, we do not specify their mapping onto the actualterminal strings; no subdivision of the alphabet intovowels and consonants is hard-wired.6 ExperimentsWe evaluate our model on standard Arabic, QuranicArabic and Hebrew in terms of segmentation qualityand lexicon induction ability.
These languages sharevarious properties, including morphology and lexi-cal cognates, but are sufficiently different so as torequire manual intervention when transferring rule-based morphological analysers across languages.
Akey question in this evaluation is therefore whetheran appropriate instantiation of our model success-fully generalises across related languages.6.1 Data setsOur models are unsupervised and therefore learnfrom raw text, but their evaluation requires anno-tated data as a gold-standard and these were derived6as follows:Arabic (MSA) We created the dataset BW by syn-thesising 50k morphotactically correct word typesfrom the morpheme lexicons and consistency rulessupplied with the Buckwalter Arabic Morphological5Including the arity as part of the non-terminal symbolnames forms part of our convention here to ensure that thegrammar contains no cycles, a situation which would compli-cate inference under PYSRCAG.6Our data preprocessing scripts are obtainable fromhttp://github.com/bothameister/pysrcag-data.Types Stems Roots m/w c/wBW 48428 24197 4717 2.3 6.4BW?48428 30891 4707 2.3 10.7QU?18808 12021 1270 1.9 9.9HEB 5231 3164 492 2.1 6.7Table 1: Corpus statistics, including average numberof morphemes (m/w) and characters (c/w) per word,and total surface-realised roots of length 3 or 4.Analyser (BAMA).7 This allowed control over theword shapes, which is important to focus the evalu-ation, while yielding reliable segmentation and rootannotations.
BW has no vocalisation; we denote thecorresponding vocalised dataset as BW?.Quranic Arabic We extracted the roughly 18kword types from a morphologically analysed versionof the Quran (Dukes and Habash, 2010).
As an ad-ditional challenge, we left all given diacritics intactfor this dataset, QU?.Hebrew We leveraged the Hebrew CHILDESdatabase as an annotated resource (Albert et al2013) and were able to extract 5k word types thatfeature at least one affix to use as dataset HEB.
Thecorrected versions of words marked as non-standardchild language were used, diacritics were dropped,and we conflated stressed and unstressed vowels toovercome inconsistencies in the source data.6.2 ModelsWe consider two classes of models.
The firstis the strictly context-free adaptor grammar formorphemes as sequences of characters usingrules (8)-(9), which we denote as Concat andMConcat, where the latter allows multiple pre-fixes/suffixes in a word.
These serve as baselines forthe second class in which non-concatenative rulesare added.
MTpl and Tpl denote the canonical ver-7We used version 2.0, LDC2004L02, and sampled wordtypes having a single stem and at most one prefix, suffix or both,according to the following random procedure: Sample a shape(stem: 0.1, pre+stem: 0.25 stem+suf: 0.25, pre+stem+suf: 0.4).Sample uniformly at random (with replacement) a stem fromthe BAMA stem lexicon, and affix(es) from the ones consis-tent with the chosen stem.
The BAMA lexicons contain affixesand their legitimate concatenations, so some of the generatedwords would permit a linguistic segmentation into multiple pre-fixes/suffixes.
Nonetheless, we take as gold-standard segmenta-tion precisely the items used by our procedure.350sions with stems as shown in the set of rules above,and we experiment with a variant Tpl3Ch that al-lows the non-terminal T1 to be rewritten as up tothree Char symbols, since the data indicate there arecases where multiple characters intervene betweenthe radicals of a root.These models exclude rule (10), which we includeonly in the variant Tpl+T4.
Lastly, TplR4 is the ex-tension of Tpl+T4 to include a stem-forming rulethat uses R4.As external baseline model we used Morfessor(Creutz and Lagus, 2007), which performs decentlyin morphological segmentation of a variety of lan-guages, but only handles concatenation.6.3 MethodThe MCMC samplers converged within a few hun-dred iterations and we collected 100 posterior sam-ples after 900 iterations of burn-in.
Collected sam-ples, each of which is a set of parse trees of the inputword types, are used in two ways:First, by averaging over the samples we can es-timate the joint probability of a word type w and aparse tree t under the adaptor grammar, conditionalon the data and the model?s hyperparameters.
Wetake the most probable parse of each word type andevaluate the implied segmentation against the goldstandard segmentation.
Likewise, we evaluate theimplied lexicon of stems, affixes and roots againstthe corresponding reference sets.
It should be em-phasised that using this maximally probable analy-sis is aimed at simplifying the evaluation set-up; onecould also extract multiple analyses of a word sincethe model defines a distribution over them.The second method abstracts away from individ-ual word-types and instead averages over the unionof all samples to obtain an estimate of the probabil-ity of a string s being generated by a certain category(non-terminal) of the grammar.
In this way we canobtain a lexicon of the morphemes in each category,ranked by their probability under the model.6.4 Inducing Morpheme LexiconsThe quality of each induced lexicon is measuredwith standard set-based precision and recall with re-spect to the corresponding gold lexicon.
The resultsare summarised by balanced F-scores in Table 2.The main result is that all our models capable offorming complex stems obtain a marked improve-ment in F-scores over the baseline concatenativeadaptor grammar, and the margin of improvementgrows along with the expressivity of the complex-stem models tested.
This applies across prefix, stemand suffix categories and across our datasets, withthe exception of QU?, which we elaborate on in ?6.5.Stem lexicons of Arabic were learnt with rel-atively constant precision (?70%), but modellingcomplex stems broadened the coverage by about3000 stems over the concatenative model (against areference set of 24k stems).
On vocalised Arabic,the improvements for stems are along both dimen-sions.
In contrast, affix lexicons for both BW andBW?are noisy and the models all generate greedilyto obtain near perfect recall but low precision.On our Hebrew data, which comprises only 5kwords, the gains in lexicon quality from modellingcomplex stems tend to be larger than on Arabic.
Thisis consistent with our intuition that an appropriate,richer Bayesian prior helps overcome data sparsity.Extracting a lexicon of roots is rendered challeng-ing by the unsupervised nature of the model as thelabelling of grammar symbols is ultimately arbitrary.Our simple approach was to regard a character tupleparsed under category R3 as a root.
This had mixedsuccess, as demonstrated by the outlier scores in Ta-ble 2.
In the one case where it was obvious that T3had been been co-opted for the role, we report theF-score obtained on the union of R3 and T3 strings.Soft decisions The preceding set-based evaluationimposes hard decisions about category membership.But adaptor grammars are probabilistic by definitionand should thus also be evaluated in terms of prob-abilistic ability.
One method is to turn the modelpredictions into a binary classifier of strings us-ing Receiver-Operator-Characteristic (ROC) theory.We plot the true positive rate versus the false pos-itive rate for each prediction lexicon L?
containingstrings that have probability greater than ?
under themodel (for a grammar category of interest).
A per-fect classifier would rank all true positives (e.g.
stemstrings) above false positives (e.g.
non-stem strings),corresponding to a curve in the upper left corner ofthe ROC plot.
A random guesser would trace a di-agonal line.
The area under the curves (AUC) isthe probability that the classifier would discriminatecorrectly.351Vocalised Arabic (BW?)
Unvocalised Arabic (BW) Hebrew (HEB)Pre Stem Suf R3 Pre Stem Suf R3 Pre Stem Suf R3Concat 15.0 20.2 25.4 - 32.8 44.1 40.3 - 18.7 20.9 29.2 -Tpl 24.7 39.4 35.2 ?42.4 45.9 54.7 47.9 62.7 35.1 59.6 52.9 34.8Tpl3Ch 28.4 36.0 36.5 5.2 50.3 55.1 48.5 62.4 38.6 61.5 56.6 7.1Tpl+T4 29.0 44.8 41.0 3.9 46.2 54.2 47.7 62.3 32.5 59.6 53.0 36.4TplR4 37.8 60.3 47.0 5.2 53.0 57.7 51.9 62.4 38.0 62.4 55.2 34.7Table 2: Morpheme lexicon induction quality.
F1-scores for lexicons induced from the most probable parseof each different dataset under each models.
?42.4 was obtained by taking the union of R3 and T3 items tomatch the way the model used them (see ?6.4).BW?BW QU?HEBMorfessor 55.57 40.04 44.34 24.20Concat 47.36 64.22 19.64 60.05Tpl 60.42 71.91 22.53 77.26Tpl3Ch 60.52 72.20 25.72 77.41Tpl+T4 64.49 71.59 24.81 77.14TplR4 74.54 73.66 - 78.14Table 3: Segmentation quality in SBF1.
The QU?results are for the corresponding M* models .Our models with complex stem formation im-prove over the baseline on the AUC metric too.
Weinclude the ROC plots for Hebrew stem and root in-duction in Figure 2, along with the roots the modelwas most confident about (Table 4).6.5 Morphological Analysis per Word TypeIn this section we turn to the analyses our modelsassign to each word type.
Two aspects of interest arethe segmentation into sequential morphemes and theidentification of the root.Our intercalating adaptor grammars consistentlyobtain large gains in segmentation accuracy over thebaseline concatenative model, across all our datasets(Table 3).
We measure segmentation quality as seg-ment border F1-score (SBF) (Sirts and Goldwater,2013), which is the F-score over word-internal seg-mentation points of the predicted analysis with re-spect to the gold segmentation.Of the two MSA datasets, the vocalised versionBW?presents a more difficult segmentation task asits words are on average longer and feature 31kunique contiguous morphemes, compared to the 24kin BW for the same number of words.
It should thusbenefit more from additional model expressivity, asis reflected in the increase of 10 SBF when addingthe TplR4 rule to the other triliteral ones.The best triliteral root identification accuracy (ona per-word basis) was found for HEB (74%) and BW(67%).8,9 Refer to Figure 3 for example analyses.An interesting aspect of these results is that tem-platic rules may aid segmentation quality withoutnecessarily giving perfect root identification.
Mod-elling stem substructure allows any regularities thatgive rise to a higher data likelihood to be picked up.The low performance on the Quran demands fur-ther explanation.
All our adaptor grammars severelyoversegmented this data, although the mistakes werenot uniformly distributed.
Most of the performanceloss is on the 79% of words that have 1-2 mor-phemes.
On the remaining words (having 3-5 mor-phemes), our models recover and approach the Mor-fessor baseline (MConcat: 32.7 , MTpl3Ch: 38.6).Preliminary experiments on BW had indicatedthat adaptation of (single) affix categories is crucialfor good performance.
Our multi-affixing modelsused on QU?lacked a further level of adaptation forcomposite affixes, which we suspect as a contribut-ing factor to the lower performance on that dataset.This remains to be confirmed in future experiments,but would be consistent with other observations onthe role of hierarchical adaptation in adaptor gram-mars (Sirts and Goldwater, 2013).
The trend thatintercalated rules improve segmentation (comparedto the concatenative grammar) remains consistent8When excluding cases where root equals stem, root identi-fication on BW is 55%.
Those cases are still not trivial, sincewords without roots also exist.9By way of comparison, Rodrigues and C?avar (2007)presented an unsupervised statistics-based root identificationmethod that obtained precision ranging between 50-75%, thehigher requiring vocalised words.3520.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0False Positive Rate0.00.10.20.30.40.50.60.70.80.91.0TruePositiveRateTplR4 (0.84)Tpl+T4 (0.83)Concat (0.63)random (0.5)(a) Stems0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0False Positive Rate0.00.10.20.30.40.50.60.70.80.91.0TruePositiveRateTplR4 (0.77)Tpl+T4 (0.77)random (0.5)(b) Triliteral rootsFigure 2: ROC curves for predicting the stem and root lexicons for the HEB dataset.
The area under eachcurve (AUC), as computed with the trapezium rule, is given in parentheses.across datasets, despite the lower absolute perfor-mance on QU?.The performance of the Morfessor baseline wasquite mixed.
Contrary to our expectations, it per-forms best on the ?harder?
BW?, worst on the ar-guably simpler HEB and struggled less than theadaptor grammars on QU?.One factor here is that it learns according toa grammar with multiple consecutive affixes andstems, whereas all our experiments (except on QU?
)presupposed single affixes.
This biases the evalua-tion slightly in our favour, but works in Morfessor?sfavour on the QU?data which is annotated with mul-tiple affixes.7 Related workThe distinctive feature of our morphological modelis that it jointly addresses root identification andmorpheme segmentation, and our results demon-strate the mutual benefit of this.In contrast, earlier unsupervised approaches tendto focus on these tasks in isolation.In unsupervised Arabic segmentation, the para-metric Bayesian model of (Lee et al 2011) achievesF1-scores in the high eighties by incorporating sen-tential context and inferred syntactic categories,both of which our model forgoes, although theirs hasno account of discontiguous root morphemes.Root Example instances1.
spr X G s?ap?ar?.ti te.s?ap?r?
ye.s?ap?r?.uB sipur.im hixs?tap?xar?
t2.
lbs X G l?ab?aS?.t li.l?b?oS?
ti.l?b?eS?.iB le hax l?b?iS?
ti tx l?ab?S?.i3.
ptx X G p?at?ax?.ti ti.p?t?ex?.iB li.p?t?oax?
nixp?t?ax?.at5.
!al ?
B ya.
!al.u m?ax!
?al?.a !
?ac?l?xan itTable 4: Top Hebrew roots hypothesised by Tpl+T4.Numbers indicate position when ranked by modelprobability.
(G)ood and (B)ad instances fromthe corpus are given with morpheme boundariesmarked: true positive (.
), false negative ( ) and falsepositive (x).
Hypothesised root characters are bold-faced, while accent (?)
marks gold root characters.Previous approaches to Arabic root identifica-tion that sought to use little supervision typicallyconstrain the search space of candidate characterswithin a word, leveraging pre-existing dictionar-ies (Darwish, 2002; Boudlal et al 2009) or ruleconstraints (Elghamry, 2005; Rodrigues and C?avar,2007; Daya et al 2008).In contrast to these approaches, our model re-quires no dictionary, and while our grammar ruleseffect some constraints on what could be a root, theyare specified in a convenient and flexible manner that353WordSufk mStems t A rPrew l >wl >stAr km XPre(w l) ...
Stem ... Suf(k m)X2 s t ?
rR3 s ?
t ?
rR1rR2 s ?
tR1tR1sT2 > ?
AT1AT1>(a) Concat & Tpl+T4, ?wl>stArkm?
(BW)WordStemn o t i l l APrel i d ali danotillA XPre(l i) ...
Stem(danotill) ... Suf(A)T4 o ?
a ?
i ?
lT1lT3 o ?
a ?
iT1iT2 o ?
aT1aT1oR4 d ?
n ?
t ?
lR1lR3 d ?
n ?
tR1tR2 d ?
nR1nR1d(b) Concat & TplR4, ?lidanotillA?
(BW?
)Figure 3: Parse trees produced for words in the two standard Arabic datasets that were incorrectly segmentedby the baseline grammar.
The templatic grammars correctly identified the triliteral and quadriliteral roots,also fixing the segmentation of (a).
In (b), the templatic grammar improved over the baseline by findingthe correct prefix but falsely posited a suffix.
Unimportant subtrees are elided for space, while the yields ofdiscontiguous constituents are indicated next to their symbols, with dots marking gaps.
Crossing branchesare not drawn but should be inferrable.
Root characters are bold-faced in the reference analysisX.
The non-terminal X2 in (a) is part of a number of implementation-specific helper rules that ensure the appropriatehandling of partly contiguous roots.makes experimentation with other phenomena easy.Recent work by Fullwood and O?Donnell (2013)goes some way toward jointly dealing with non-concatenative and concatenative morphology in theunsupervised setting, but their focus is limited to in-flected stems and does not handle multiple consecu-tive affixes.
They analyse the Arabic verb stem (e.g.kataba ?he wrote?)
into a templatic bit-string denot-ing root and non-root characters (e.g.
r-r-r-) alongwith a root morpheme (e.g.
ktb) and a so-calledresidue morpheme (e.g.
aaa).
Their nonparamet-ric Bayesian model induces lexicons of these en-tities and achieves very high performance on tem-plates.
The explicit formulation of templates allevi-ates the labelling ambiguity that hampered our eval-uation (?6.4), but we believe their method of anal-ysis can be simulated in our framework using theappropriate SRCG-rules.Learning root-templatic morphology is loosely re-lated to morphological paradigm induction (Clark,2001; Dreyer and Eisner, 2011; Durrett and DeN-ero, 2013).
Our models do not represent templaticparadigms explicitly, but it is interesting to note thatpreliminary experiments with German indicate thatour adaptor grammars pick up on the past participleforming circumfix in ab+ge+spiel+t (played back).8 Conclusion and OutlookWe presented a new approach to modelling non-concatenative phenomena in morphology using sim-ple range concatenating grammars and extendedadaptor grammars to this formalism.
Our experi-ments show that this richer model improves morpho-logical segmentation and morpheme lexicon induc-tion on different languages in the Semitic family.Various avenues for future work present them-selves.
Firstly, the lightly-supervised, meta-grammar approach to adaptor grammars (Sirts andGoldwater, 2013) can be extended to this morepowerful formalism to lessen the burden of defin-ing the ?right?
grammar rules by hand, and possi-bly boost performance.
Secondly, the discontigu-ous constituents learnt with our framework can beused as features in other downstream applications.Especially in low-resource languages, the ability tomodel non-concatenative phenomena (e.g.
circum-fixing, ablaut, etc.)
can play an important role in re-ducing data sparsity for tasks like word alignmentand language modelling.
Finally, the PYSRCAGpresents another way of learning SRCGs in general,which can thus be employed in other applications ofSRCGs, including syntactic parsing and translation.AcknowledgementsWe thank the anonymous reviewers for their valu-able comments.
Our PYSRCAG implementationleveraged the adaptor grammar code released byMark Johnson, whom we thank, along with the in-dividuals who contributed to the public data sourcesthat enabled the empirical elements of this paper.354ReferencesAviad Albert, Brian MacWhinney, Bracha Nir, and ShulyWintner.
2013.
The Hebrew CHILDES corpus: tran-scription and morphological analysis.
Language Re-sources and Evaluation, pages 1?33.Mohamed Altantawy, Nizar Habash, Owen Rambow, andIbrahim Saleh.
2010.
Morphological Analysis andGeneration of Arabic Nouns: A Morphemic Func-tional Approach.
In Proceedings of LREC, pages 851?858.Kenneth R Beesley and Lauri Karttunen.
2003.
Fi-nite state morphology, volume 18.
CSLI publicationsStanford.Abderrahim Boudlal, Rachid Belahbib, AbdelhakLakhouaja, Azzeddine Mazroui, Abdelouafi Meziane,and Mohamed Bebah.
2009.
A Markovian approachfor Arabic Root Extraction.
The International ArabJournal of Information Technology, 8(1):91?98.Pierre Boullier.
2000.
A cubic time extension of context-free grammars.
Grammars, 3(2-3):111?131.Tim Buckwalter.
2002.
Arabic Morphological Ana-lyzer.
Technical report, Linguistic Data Consortium,Philedelphia.Alexander Clark.
2001.
Learning Morphology with PairHidden Markov Models.
In Proceedings of the ACLStudent Workshop, pages 55?60.Yael Cohen-Sygal and Shuly Wintner.
2006.
Finite-state registered automata for non-concatenative mor-phology.
Computational Linguistics, 32(1):49?82.Mathias Creutz and Krista Lagus.
2007.
Unsupervisedmodels for morpheme segmentation and morphologylearning.
ACM Transactions on Speech and LanguageProcessing, 4(1):1?34.Kareem Darwish.
2002.
Building a shallow Arabicmorphological analyzer in one day.
In Proceedingsof the ACL Workshop on Computational Approachesto Semitic Languages, pages 47?54.
Association forComputational Linguistics.Ezra Daya, Dan Roth, and Shuly Wintner.
2008.Identifying Semitic Roots: Machine Learning withLinguistic Constraints.
Computational Linguistics,34(3):429?448.Markus Dreyer and Jason Eisner.
2011.
DiscoveringMorphological Paradigms from Plain Text Using aDirichlet Process Mixture Model.
In Proceedings ofEMNLP, pages 616?627, Edinburgh, Scotland.Kais Dukes and Nizar Habash.
2010.
Morphological An-notation of Quranic Arabic.
In Proceedings of LREC.Greg Durrett and John DeNero.
2013.
Supervised Learn-ing of Complete Morphological Paradigms.
In Pro-ceedings of NAACL-HLT, pages 1185?1195, Atlanta,Georgia, June.
Association for Computational Lin-guistics.Khaled Elghamry.
2005.
A Constraint-based Algorithmfor the Identification of Arabic Roots.
In Proceed-ings of the Midwest Computational Linguistics Collo-quium.
Indiana University.
Bloomington, IN.Raphael Finkel and Gregory Stump.
2002.
GeneratingHebrew verb morphology by default inheritance hier-archies.
In Proceedings of the ACL Workshop on Com-putational Approaches to Semitic Languages.
Associ-ation for Computational Linguistics.Michelle A. Fullwood and Timothy J. O?Donnell.
2013.Learning non-concatenative morphology.
In Proceed-ings of the Workshop on Cognitive Modeling and Com-putational Linguistics, pages 21?27, Sofia, Bulgaria.Association for Computational Linguistics.Michael Gasser.
2009.
Semitic morphological analysisand generation using finite state transducers with fea-ture structures.
In Proceedings of EACL, pages 309?317.
Association for Computational Linguistics.Daniel Gildea.
2010.
Optimal Parsing Strategies for Lin-ear Context-Free Rewriting Systems.
In Proceedingsof NAACL, pages 769?776.
Association for Computa-tional Linguistics.Sharon Goldwater, Thomas L. Griffiths, and Mark John-son.
2006.
Interpolating Between Types and Tokensby Estimating Power-Law Generators.
In Advances inNeural Information Processing Systems, Volume 18.Harald Hammarstro?m and Lars Borin.
2011.
Unsuper-vised Learning of Morphology.
Computational Lin-guistics, 37(2):309?350.Yun Huang, Min Zhang, and Chew Lim Tan.
2011.Nonparametric Bayesian Machine Transliteration withSynchronous Adaptor Grammars.
In Proceedings ofACL (Short papers), pages 534?539.Mark Johnson and Sharon Goldwater.
2009.
Improvingnonparameteric Bayesian inference: Experiments onunsupervised word segmentation with adaptor gram-mars.
In Proceedings of NAACL-HLT, pages 317?325.Association for Computational Linguistics.Mark Johnson, Thomas L. Griffiths, and Sharon Gold-water.
2007.
Adaptor Grammars: A Framework forSpecifying Compositional Nonparametric BayesianModels.
In Advances in Neural Information Process-ing Systems, volume 19, page 641.
MIT.Mark Johnson.
2008.
Unsupervised word segmentationfor Sesotho using Adaptor Grammars.
In Proceedingsof ACL Special Interest Group on Computational Mor-phology and Phonology (SigMorPhon), pages 20?27.Association for Computational Linguistics.Aravind K. Joshi.
1985.
Tree adjoining grammars: Howmuch context-sensitivity is required to provide reason-able structural descriptions?
In D.R.
Dowty, L. Kart-tunen, and A.M. Zwicky, editors, Natural LanguageParsing, chapter 6, pages 206?250.
Cambridge Uni-versity Press.355Miriam Kaeshammer.
2013.
Synchronous LinearContext-Free Rewriting Systems for Machine Trans-lation.
In Proceedings of the Workshop on Syntax, Se-mantics and Structure in Statistical Translation, pages68?77, Atlanta, Georgia.
Association for Computa-tional Linguistics.Yuki Kato, Hiroyuki Seki, and Tadao Kasami.
2006.Stochastic Multiple Context-Free Grammar for RNAPseudoknot Modeling.
In Proceedings of the Inter-national Workshop on Tree Adjoining Grammar andRelated Formalisms, pages 57?64.George Anton Kiraz.
2000.
Multitiered Nonlinear Mor-phology Using Multitape Finite Automata: A CaseStudy on Syriac and Arabic.
Computational Linguis-tics, 26(1):77?105, March.Kimmo Koskenniemi.
1984.
A general computationalmodel for word-form recognition and production.
InProceedings of the 10th international conference onComputational Linguistics, pages 178?181.
Associa-tion for Computational Linguistics.Mikko Kurimo, Sami Virpioja, Ville T. Turunen,Graeme W. Blackwood, and William Byrne.
2010.Overview and Results of Morpho Challenge 2009.
InMultilingual Information Access Evaluation I.
Text Re-trieval Experiments, volume 6241 of Lecture Notes inComputer Science, pages 578?597.
Springer Berlin /Heidelberg.Yoong Keok Lee, Aria Haghighi, and Regina Barzilay.2011.
Modeling syntactic context improves morpho-logical segmentation.
In Proceedings of CoNLL.Wolfgang Maier.
2010.
Direct Parsing of Discon-tinuous Constituents in German.
In Proceedings ofthe NAACL-HLT Workshop on Statistical Parsing ofMorphologically-Rich Languages, pages 58?66.
Asso-ciation for Computational Linguistics.Jim Pitman and Marc Yor.
1997.
The Two-ParameterPoisson-Dirichlet Distribution Derived from a StableSubordinator.
The Annals of Probability, 25(2):855?900.Jim Pitman.
1995.
Exchangeable and partially exchange-able random partitions.
Probability Theory and Re-lated Fields, 102:145?158.Jean-Franc?ois Prunet.
2006.
External Evidence and theSemitic Root.
Morphology, 16(1):41?67.Paul Rodrigues and Damir C?avar.
2007.
Learning ArabicMorphology Using Statistical Constraint-SatisfactionModels.
In Elabbas Benmamoun, editor, Perspectiveson Arabic Linguistics: Proceedings of the 19th Ara-bic Linguistics Symposium, pages 63?75, Urbana, IL,USA.
John Benjamins Publishing Company.Nathan Schneider.
2010.
Computational CognitiveMorphosemantics: Modeling Morphological Compo-sitionality in Hebrew Verbs with Embodied Construc-tion Grammar.
In Proceedings of the Annual Meetingof the Berkeley Linguistics Society, Berkeley, CA.Hiroyuki Seki, Takashi Matsumura, Mamoru Fujii, andTadao Kasami.
1991.
On multiple context-free gram-mars.
Theoretical Computer Science, 88(2):191?229.Kairit Sirts and Sharon Goldwater.
2013.
Minimally-Supervised Morphological Segmentation using Adap-tor Grammars.
Transactions of the ACL.K.
Vijay-Shanker, David J. Weir, and Aravind K. Joshi.1987.
Characterizing structural descriptions producedby various grammatical formalisms.
In Proceedings ofACL, pages 104?111.356
