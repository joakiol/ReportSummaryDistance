Proceedings of the ACL 2007 Demo and Poster Sessions, pages 221?224,Prague, June 2007. c?2007 Association for Computational LinguisticsAutomatic Part-of-Speech Tagging for Bengali: An Approach forMorphologically Rich Languages in a Poor Resource ScenarioSandipan Dandapat, Sudeshna Sarkar, Anupam BasuDepartment of Computer Science and EngineeringIndian Institute of Technology KharagpurIndia 721302{sandipan,sudeshna,anupam.basu}@cse.iitkgp.ernet.inAbstractThis paper describes our work on build-ing Part-of-Speech (POS) tagger forBengali.
We have use Hidden MarkovModel (HMM) and Maximum Entropy(ME) based stochastic taggers.
Bengali isa morphologically rich language and ourtaggers make use of morphological andcontextual information of the words.Since only a small labeled training set isavailable (45,000 words), simple stochas-tic approach does not yield very good re-sults.
In this work, we have studied theeffect of using a morphological analyzerto improve the performance of the tagger.We find that the use of morphology helpsimprove the accuracy of the tagger espe-cially when less amount of tagged cor-pora are available.1 IntroductionPart-of-Speech (POS) taggers for natural lan-guage texts have been developed using linguisticrules, stochastic models as well as a combinationof both (hybrid taggers).
Stochastic models (Cut-ting et al, 1992; Dermatas et al, 1995; Brants,2000) have been widely used in POS tagging forsimplicity and language independence of themodels.
Among stochastic models, bi-gram andtri-gram Hidden Markov Model (HMM) arequite popular.
Development of a high accuracystochastic tagger requires a large amount of an-notated text.
Stochastic taggers with more than95% word-level accuracy have been developedfor English, German and other European Lan-guages, for which large labeled data is available.Our aim here is to develop a stochastic POS tag-ger for Bengali but we are limited by lack of alarge annotated corpus for Bengali.
SimpleHMM models do not achieve high accuracywhen the training set is small.
In such cases, ad-ditional information may be coded into theHMM model to achieve higher accuracy (Cuttinget al, 1992).
The semi-supervised model de-scribed in Cutting et al (1992), makes use ofboth labeled training text and some amount ofunlabeled text.
Incorporating a diverse set ofoverlapping features in a HMM-based tagger isdifficult and complicates the smoothing typicallyused for such taggers.
In contrast, methods basedon Maximum Entropy (Ratnaparkhi, 1996),Conditional Random Field (Shrivastav, 2006)etc.
can deal with diverse, overlapping features.1.1 Previous Work on Indian LanguagePOS TaggingAlthough some work has been done on POS tag-ging of different Indian languages, the systemsare still in their infancy due to resource poverty.Very little work has been done previously onPOS tagging of Bengali.
Bengali is the mainlanguage spoken in Bangladesh, the second mostcommonly spoken language in India, and thefourth most commonly spoken language in theworld.
Ray et al (2003) describes a morphology-based disambiguation for Hindi POS tagging.System using a decision tree based learning algo-rithm (CN2) has been developed for statisticalHindi POS tagging (Singh et al, 2006).
A rea-sonably good accuracy POS tagger for Hindi hasbeen developed using Maximum EntropyMarkov Model (Dalal et al, 2007).
The systemuses linguistic suffix and POS categories of aword along with other contextual features.2 Our ApproachThe problem of POS tagging can be formallystated as follows.
Given a sequence of words w1?
wn, we want to find the corresponding se-quence of tags t1 ?
tn, drawn from a set of tags T.We use a tagset of 40 tags1.
In this work, we ex-plore supervised and semi-supervised bi-gram1 http://www.mla.iitkgp.ernet.in/Tag.html221HMM and a ME based model.
The bi-gram as-sumption states that the POS-tag of a word de-pends on the current word and the POS tag of theprevious word.
An ME model estimates the prob-abilities based on the imposed constraints.
Suchconstraints are derived from the training data,maintaining some relationship between featuresand outcomes.
The most probable tag sequencefor a given word sequence satisfies equation (1)and (2) respectively for HMM and ME model:11... 1,( | ) ( | )arg max i i i it tn i nS P w t P t t ?== ?
(1)1 11,( ... | ... ) ( | )n n i ii np t t w w p t h== ?
(2)Here, hi is the context for word wi.
Since the ba-sic bigram model of HMM as well as the equiva-lent ME models do not yield satisfactory accu-racy, we wish to explore whether other availableresources like a morphological analyzer can beused appropriately for better accuracy.2.1 HMM and ME based TaggersThree taggers have been implemented based onbigram HMM and ME model.
The first tagger(we shall call it HMM-S) makes use of the su-pervised HMM model parameters, whereas thesecond tagger (we shall call it HMM-SS) usesthe semi supervised model parameters.
The thirdtagger uses ME based model to find the mostprobable tag sequence for a given sequence ofwords.In order to further improve the tagging accuracy,we use a Morphological Analyzer (MA) and in-tegrate morphological information with the mod-els.
We assume that the POS-tag of a word w cantake values from the set TMA(w), where TMA(w) iscomputed by the Morphological Analyzer.
Notethat the size of TMA(w) is much smaller than T.Thus, we have a restricted choice of tags as wellas tag sequences for a given sentence.
Since thecorrect tag t for w is always in TMA(w) (assumingthat the morphological analyzer is complete), it isalways possible to find out the correct tag se-quence for a sentence even after applying themorphological restriction.
Due to a much re-duced set of possibilities, this model is expectedto perform better for both the HMM (HMM-Sand HMM-SS) and ME models even when only asmall amount of labeled training text is available.We shall call these new models HMM-S+MA,HMM-SS+ MA and ME+MA.Our MA has high accuracy and coverage but itstill has some missing words and a few errors.For the purpose of these experiments we havemade sure that all words of the test set are pre-sent in the root dictionary that an MA uses.While MA helps us to restrict the possible choiceof tags for a given word, one can also use suffixinformation (i.e., the sequence of last few charac-ters of a word) to further improve the models.For HMM models, suffix information has beenused during smoothing of emission probabilities,whereas for ME models, suffix information isused as another type of feature.
We shall denotethe models with suffix information with a ?+suf?marker.
Thus, we have ?
HMM-S+suf, HMM-S+suf+MA, HMM-SS+suf etc.2.1.1 Unknown Word Hypothesis in HMMThe transition probabilities are estimated by lin-ear interpolation of unigrams and bigrams.
Forthe estimation of emission probabilities add-onesmoothing or suffix information is used for theunknown words.
If the word is unknown to themorphological analyzer, we assume that thePOS-tag of that word belongs to any of the openclass grammatical categories (all classes ofNoun, Verb, Adjective, Adverb and Interjection).2.1.2 Features of the ME ModelExperiments were carried out to find out themost suitable binary valued features for the POStagging in the ME model.
The main features forthe POS tagging task have been identified basedon the different possible combination of theavailable word and tag context.
The features alsoinclude prefix and suffix up to length four.
Weconsidered different combinations from the fol-lowing set for obtaining the best feature set forthe POS tagging task with the data we have.
{ }11 2 2 1 2, , , , , , , 4, 4ii i i i i iF w w w w w t t pre suf+?
?
+ ?
?= ?
?Forty different experiments were conducted tak-ing several combinations from set ?F?
to identifythe best suited feature set for the POS taggingtask.
From our empirical analysis we found thatthe combination of contextual features (currentword and previous tag), prefixes and suffixes oflength ?
4 gives the best performance for the MEmodel.
It is interesting to note that the inclusionof prefix and suffix for all words gives betterresult instead of using only for rare words as isdescribed in Ratnaparkhi (1996).
This can beexplained by the fact that due to small amount ofannotated data, a significant number of instances222are not found for most of the word of thelanguage vocabulary.3 ExperimentsWe have a total of 12 models as described insubsection 2.1 under different stochastic taggingschemes.
The same training text has been used toestimate the parameters for all the models.
Themodel parameters for supervised HMM and MEmodels are estimated from the annotated textcorpus.
For semi-supervised learning, the HMMlearned through supervised training is consideredas the initial model.
Further, a larger unlabelledtraining data has been used to re-estimate themodel parameters of the semi-supervised HMM.The experiments were conducted with three dif-ferent sizes (10K, 20K and 40K words) of thetraining data to understand the relative perform-ance of the models as we keep on increasing thesize of the annotated data.3.1 Training DataThe training data includes manually annotated3625 sentences (approximately 40,000 words)for both supervised HMM and ME model.
Afixed set of 11,000 unlabeled sentences (ap-proximately 100,000 words) taken from CIILcorpus 2  are used to re-estimate the model pa-rameter during semi-supervised learning.
It hasbeen observed that the corpus ambiguity (meannumber of possible tags for each word) in thetraining text is 1.77 which is much larger com-pared to the European languages (Dermatas etal., 1995).3.2 Test DataAll the models have been tested on a set of ran-domly drawn 400 sentences (5000 words) dis-joint from the training corpus.
It has been notedthat 14% words in the open testing text are un-known with respect to the training set, which isalso a little higher compared to the Europeanlanguages (Dermatas et al, 1995)3.3 ResultsWe define the tagging accuracy as the ratio ofthe correctly tagged words to the total number ofwords.
Table 1 summarizes the final accuraciesachieved by different learning methods with thevarying size of the training data.
Note that thebaseline model (i.e., the tag probabilities depends2 A part of the EMILE/CIIL corpus developed at Cen-tral Institute of Indian Languages (CIIL), Mysore.only on the current word) has an accuracy of76.8%.Accuracy Method10K 20K 40KHMM-S 57.53 70.61 77.29HMM-S+suf 75.12 79.76 83.85HMM-S+MA 82.39 84.06 86.64HMM-S+suf+MA 84.73 87.35 88.75HMM-SS 63.40 70.67 77.16HMM-SS+suf 75.08 79.31 83.76HMM-SS+MA 83.04 84.47 86.41HMM-SS+suf+MA 84.41 87.16 87.95ME 74.37 79.50 84.56ME+suf 77.38 82.63 86.78ME+MA 82.34 84.97 87.38ME+suf+MA 84.13 87.07 88.41Table 1: Tagging accuracies (in %) of differentmodels with 10K, 20K and 40K training data.3.4 ObservationsWe find that in both the HMM based models(HMM-S and HMM-SS), the use of suffix in-formation as well as the use of a morphologicalanalyzer improves the accuracy of POS taggingwith respect to the base models.
The use of MAgives better results than the use of suffix infor-mation.
When we use both suffix information aswell as MA, the results is even better.HMM-SS does better than HMM-S when verylittle tagged data is available, for example, whenwe use 10K training corpus.
However, the accu-racy of the semi-supervised HMM models areslightly poorer than that of the supervised HMMmodels for moderate size training data and use ofsuffix information.
This discrepancy arises dueto the over-fitting of the supervised models in thecase of small training data; the problem is allevi-ated with the increase in the annotated data.As we have noted already the use of MA and/orsuffix information improves the accuracy of thePOS tagger.
But what is significant to note is thatthe percentage of improvement is higher whenthe amount of training data is less.
The HMM-S+suf model gives an improvement of around18%, 9% and 6% over the HMM-S model for10K, 20K and 40K training data respectively.Similar trends are observed in the case of thesemi-supervised HMM and the ME models.
Theuse of morphological restriction (HMM-S+MA)gives an improvement of 25%, 14% and 9% re-spectively over the HMM-S in case of 10K, 20K223and 40K training data.
As the improvement dueto MA decreases with increasing data, it mightbe concluded that the use of morphological re-striction may not improve the accuracy when alarge amount of training data is available.
Fromour empirical observations we found that bothsuffix and morphological restriction (HMM-S+suf+MA) gives an improvement of 27%, 17%and 12% over the HMM-S model respectivelyfor the three different sizes of training data.The Maximum Entropy model does better thanthe HMM models for smaller training data.
Butwith higher amount of training data the perform-ance of the HMM and ME model are compara-ble.
Here also we observe that suffix informationand MA have positive effect, and the effect ishigher with poor resources.Furthermore, in order to estimate the relative per-formance of the models, experiments were car-ried out with two existing taggers: TnT (Brants,2000) and ACOPOST3.
The accuracy achievedusing TnT are 87.44% and 87.36% respectivelywith bigram and trigram model for 40K trainingdata.
The accuracy with ACOPOST is 86.3%.This reflects that the higher order Markov mod-els do not work well under the current experi-mental setup.3.5 Assessment of Error TypesTable 2 shows the top five confusion classes forHMM-S+MA model.
The most common types oferrors are the confusion between proper nounand common noun and the confusion betweenadjective and common noun.
This results fromthe fact that most of the proper nouns can beused as common nouns and most of the adjec-tives can be used as common nouns in Bengali.ActualClass(frequency)PredictedClass% of totalerrors% ofclasserrorsNP(251) NN 21.03 43.82JJ(311) NN 5.16 8.68NN(1483) JJ 4.78 1.68DTA(100) PP 2.87 15.0NN(1483) VN 2.29 0.81Table 2: Five most common types of errorsAlmost all the confusions are wrong assignmentdue to less number of instances in the trainingcorpora, including errors due to long distancephenomena.3 http://maxent.sourceforge.net4 ConclusionIn this paper we have described an approach forautomatic stochastic tagging of natural languagetext for Bengali.
The models described here arevery simple and efficient for automatic taggingeven when the amount of available annotatedtext is small.
The models have a much higheraccuracy than the na?ve baseline model.
How-ever, the performance of the current system isnot as good as that of the contemporary POS-taggers available for English and other Europeanlanguages.
The best performance is achieved forthe supervised learning model along with suffixinformation and morphological restriction on thepossible grammatical categories of a word.
Infact, the use of MA in any of the models dis-cussed above enhances the performance of thePOS tagger significantly.
We conclude that theuse of morphological features is especially help-ful to develop a reasonable POS tagger whentagged resources are limited.ReferencesA.
Dalal, K. Nagaraj, U. Swant, S. Shelke and P.Bhattacharyya.
2007.
Building Feature Rich POSTagger for Morphologically Rich Languages: Ex-perience in Hindi.
ICON, 2007.A.
Ratnaparkhi, 1996.
A maximum entropy part-of-speech tagger.
EMNLP 1996. pp.
133-142.D.
Cutting, J. Kupiec, J. Pederson and P. Sibun.
1992.A practical part-of-speech tagger.
In Proc.
of the3rd Conference on Applied NLP, pp.
133-140.E.
Dermatas and K. George.
1995.
Automatic stochas-tic tagging of natural language texts.
Computa-tional Linguistics, 21(2): 137-163.M.
Shrivastav, R. Melz, S. Singh, K. Gupta andP.
Bhattacharyya, 2006.
Conditional RandomField Based POS Tagger for Hindi.
In Pro-ceedings of the MSPIL, pp.
63-68.P.
R. Ray, V. Harish, A. Basu and S. Sarkar, 2003.Part of Speech Tagging and Local Word GroupingTechniques for Natural Language Processing.ICON 2003.S.
Singh, K. Gupta, M. Shrivastav and P. Bhat-tacharyya, 2006.
Morphological Richness OffsetResource Demand ?
Experience in constructing aPOS Tagger for Hindi.
COLING/ACL 2006, pp.779-786.T.
Brants.
2000.
TnT ?
A statistical part-of-sppechtagger.
In Proc.
of the 6th Applied NLP Conference,pp.
224-231.224
