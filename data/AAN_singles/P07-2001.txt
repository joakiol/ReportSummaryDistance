Proceedings of the ACL 2007 Demo and Poster Sessions, pages 1?4,Prague, June 2007. c?2007 Association for Computational LinguisticsMIMUS: A Multimodal and Multilingual Dialogue System for the HomeDomainJ.
Gabriel AmoresJulietta Research GroupUniversidad de Sevillajgabriel@us.esGuillermo Pe?rezJulietta Research GroupUniversidad de Sevillagperez@us.esPilar Mancho?nJulietta Research GroupUniversidad de Sevillapmanchon@us.esAbstractThis paper describes MIMUS, a multimodaland multilingual dialogue system for the in?home scenario, which allows users to con-trol some home devices by voice and/orclicks.
Its design relies on Wizard of Oz ex-periments and is targeted at disabled users.MIMUS follows the Information State Up-date approach to dialogue management, andsupports English, German and Spanish, withthe possibility of changing language on?the?fly.
MIMUS includes a gestures?enabledtalking head which endows the system witha human?like personality.1 IntroductionThis paper describes MIMUS, a multimodal andmultilingual dialogue system for the in?home sce-nario, which allows users to control some home de-vices by voice and/or clicks.
The architecture ofMIMUS was first described in (Pe?rez et al, 2006c).This work updates the description and includes alife demo.
MIMUS follows the Information StateUpdate approach to dialogue management, and hasbeen developed under the EU?funded TALK project(Talk Project, 2004).
Its architecture consists of aset of OAA agents (Cheyer and Martin, 1972) linkedthrough a central Facilitator, as shown in figure 1:The main agents in MIMUS are briefly describedhereafter:?
The system core is the Dialogue Manager,which processes the information coming fromthe different input modality agents by means ofa natural language understanding module andprovides output in the appropriate modality.?
The main input modality agent is the ASRManager, which is obtained through an OAAFigure 1: MIMUS Architecturewrapper for Nuance.
Currently, the system sup-ports English, Spanish and German, with thepossibility of changing languages on?the?flywithout affecting the dialogue history.?
The HomeSetup agent displays the house lay-out, with all the devices and their state.
When-ever a device changes its state, the HomeSetupis notified and the graphical layout is updated.?
The Device Manager controls the physical de-vices.
When a command is sent, the DeviceManager notifies it to the HomeSetup and theKnowledge Manager, guaranteeing coherencein all the elements in MIMUS.?
The GUI Agents control each of the device?specific GUIs.
Thus, clicking on the telephoneicon, a telephone GUI will be displayed, and soon for each type of service.?
The Knowledge Manager connects all theagents to the common knowledge resource by1means of an OWL Ontology.?
The Talking Head.
MIMUS virtual charac-ter is synchronized with Loquendo?s TTS, andhas the ability to express emotions and playsome animations such as nodding or shakingthe head.2 WoZ ExperimentsMIMUS has been developed taking into accountwheel?chair bound users.
In order to collect first?hand information about the users?
natural behaviorin this scenario, several WoZ experiments were firstconducted.
A rather sophisticated multilingual WoZexperimental platform was built for this purpose.The set of WoZ experiments conducted was de-signed in order to collect data.
In turn, thesedata helped determine the relevant factors to con-figure multimodal dialogue systems in general, andMIMUS in particular.A detailed description of the results obtained afterthe analysis of the experiments and their impact onthe overall design of the system may be found in(Mancho?n et al, 2007).3 ISU?based Dialogue Management inMIMUSAs pointed out above, MIMUS follows the ISUapproach to dialogue management (Larsson andTraum, 2000).
The main element of the ISU ap-proach in MIMUS is the dialogue history, repre-sented formally as a list of dialogue states.
Dia-logue rules update this information structure eitherby producing new dialogue states or by supplyingarguments to existing ones.3.1 Multimodal DTAC structureThe information state in MIMUS is represented as afeature structure with four main attributes: DialogueMove, Type, Arguments and Contents.?
DMOVE: Identifies the kind of dialogue move.?
TYPE: This feature identifies the specific dia-logue move in the particular domain at hand.?
ARGS: The ARGS feature specifies the argu-ment structure of the DMOVE/TYPE pair.Modality and Time features have been added inorder to implement fusion strategies at dialoguelevel.3.2 Updating the Information State in MIMUSThis section provides an example of how the In-formation State Update approach is implementedin MIMUS.
Update rules are triggered by dialoguemoves (any dialogue move whose DTAC structureunifies with the Attribute?Value pairs defined in theTriggeringCondition field) and may require addi-tional information, defined as dialogue expectations(again, those dialogue moves whose DTAC structureunify with the Attribute?Value pairs defined in theDeclareExpectations field).Consider the following DTAC, which representsthe information state returned by the NLU modulefor the sentence switch on:?????????
?DMOVE specifyCommandTYPE SwitchOnARGS[Location, DeviceType]META INFO ??
?MODALITY VOICETIME INIT 00:00:00TIME END 00:00:30CONFIDENCE 700????????????
?Consider now the (simplified) dialogue rule?ON?, defined as follows:RuleID: ON;TriggeringCondition:(DMOVE:specifyCommand,TYPE:SwitchOn);DeclareExpectations: {Location,DeviceType }ActionsExpectations: {[DeviceType] =>{NLG(DeviceType);} }PostActions: {ExecuteAction(@is-ON); }The DTAC obtained for switch on triggers thedialogue rule ON.
However, since two declaredexpectations are still missing (Location and De-viceType), the dialogue manager will activate theActionExpectations and prompt the user for thekind of device she wants to switch on, by meansof a call to the natural language generation mod-ule NLG(DeviceType).
Once all expectations have2been fulfilled, the PostActions can be executed overthe desired device(s).4 Integrating OWL in MIMUSInitially, OWL Ontologies were integrated inMIMUS in order to improve its knowledge manage-ment module.
This functionality implied the imple-mentation of a new OAA wrapper capable of query-ing OWL ontologies, see (Pe?rez et al, 2006b) fordetails.4.1 From Ontologies to Grammars: OWL2GraOWL ontologies play a central role in MIMUS.
Thisrole is limited, though, to the input side of the sys-tem.
The domain?dependent part of multimodal andmultilingual production rules for context?free gram-mars is semi?automatically generated from an OWLontology.This approach has achieved several goals: it lever-ages the manual work of the linguist, and ensurescoherence and completeness between the DomainKnowledge (Knowledge Manager Module) and theLinguistic Knowledge (Natural Language Under-standing Module) in the application.
A detailed ex-planation of the algorithm and the results obtainedcan be found in (Pe?rez et al, 2006a)4.2 From OWL to the House LayoutMIMUS home layout does not consist of a pre?defined static structure only usable for demonstra-tion purposes.
Instead, it is dynamically loaded atexecution time from the OWL ontology where allthe domain knowledge is stored, assuring the coher-ence of the layout with the rest of the system.This is achieved by means of an OWL?RDQLwrapper.
It is through this agent that the Home Setupenquires for the location of the walls, the label of therooms, the location and type of devices per room andso forth, building the 3D graphical image from thesedata.5 Multimodal Fusion StrategiesMIMUS approach to multimodal fusion involvescombining inputs coming from different multimodalchannels at dialogue level (Pe?rez et al, 2005).
Theidea is to check the multimodal input pool beforelaunching the actions expectations while waiting foran ?inter?modality?
time.
This strategy assumesthat each individual input can be considered as anindependent dialogue move.
In this approach, themultimodal input pool receives and stores all in-puts including information such as time and modal-ity.
The Dialogue Manager checks the input poolregularly to retrieve the corresponding input.
Ifmore than one input is received during a certain timeframe, they are considered simultaneous or pseudo?simultaneous.
In this case, further analysis is neededin order to determine whether those independentmultimodal inputs are truly related or not.
Another,improved strategy has been proposed at (Mancho?net al, 2006), which combines the advantages of thisone, and those proposed for unification?based gram-mars (Johnston et al, 1997; Johnston, 1998).6 Multimodal Presentation in MIMUSMIMUS offers graphical and voice output to theusers through an elaborate architecture composed ofa TTS Manager, a HomeSetup and GUI agents.
Themultimodal presentation architecture in MIMUSconsists of three sequential modules.
The currentversion is a simple implementation that may be ex-tended to allow for more complex theoretical issueshereby proposed.
The main three modules are:?
Content Planner (CP): This module decideson the information to be provided to the user.As pointed out by (Wahlster et al, 1993), theCP cannot determine the content independentlyfrom the presentation planner (PP).
In MIMUS,the CP generates a set of possibilities, fromwhich the PP will select one, depending ontheir feasibility.?
Presentation Planner (PP): The PP receives theset of possible content representations and se-lects the ?best?
one.?
Realization Module (RM): This module takesthe presentation generated and selected bythe CP?PP, divides the final DTAC structureand sends each substructure to the appropriateagent for rendering.7 The MIMUS Talking HeadMIMUS virtual character is known as Ambrosio.Endowing the character with a name results in per-3sonalization, personification, and voice activation.Ambrosio will remain inactive until called for duty(voice activation); each user may name their per-sonal assistant as they wish (Personalization); andthey will address the system at personal level, re-inforcing the sense of human?like communication(Personification).
The virtual head has been imple-mented in 3D to allow for more natural and realis-tic gestures and movements.
The graphical engineused is OGRE (OGRE, 2006), a powerful, free andeasy to use tool.
The current talking head is inte-grated with Loquendo, a high quality commercialsynthesizer that launches the information about thephonemes as asynchronous events, which allows forlip synchronization.
The dialogue manager controlsthe talking head, and sends the appropriate com-mands depending of the dialogue needs.
Through-out the dialogue, the dialogue manager may see itfit to reinforce the communication channel with ges-tures and expressions, which may or may not implysynthesized utterances.
For instance, the head mayjust nod to acknowledge a command, without utter-ing words.8 Conclusions and Future WorkIn this paper, an overall description of the MIMUSsystem has been provided.MIMUS is a fully multimodal and multilingual di-alogue system within the Information State Updateapproach.
A number of theoretical and practical is-sues have been addressed successfully, resulting in auser?friendly, collaborative and humanized system.We concluded from the experiments that ahuman?like talking head would have a significantpositive impact on the subjects?
perception and will-ingness to use the system.Although no formal evaluation of the system hastaken place, MIMUS has already been presentedsuccessfully in different forums, and as expected,?Ambrosio?
has always made quite an impression,making the system more appealing to use and ap-proachable.ReferencesAdam Cheyer and David Martin.
2001.
The openagent architecture.
Journal of Autonomous Agents andMulti?Agent Systems, 4(12):143?148.Michael Johnston, Philip R. Cohen, David McGee,Sharon L. Oviatt, James A. Pitman and Ira A. Smith.1997.
Unification?based Multimodal Integration ACL281?288.Michael Johnston.
1998.
Unification?based MultimodalParsing Coling?ACL 624?630.Staffan Larsson and David Traum.
2000.
InformationState and dialogue management in the TRINDI Dia-logue Move Engine Toolkit.
Natural Language Engi-neering, 6(34): 323-340.Pilar Mancho?n, Guillermo Pe?rez and Gabriel Amores.2006.
Multimodal Fusion: A New Hybrid Strategyfor Dialogue Systems.
Proceedings of InternationalCongress of Multimodal Interfaces (ICMI06), 357?363.
ACM, New York, USA.Pilar Mancho?n, Carmen Del Solar, Gabriel Amores andGuillermo Pe?rez.
2007.
Multimodal Event Analysisin the MIMUS Corpus.
Multimodal Corpora: SpecialIssue of the International Journal JLRE, submitted.OGRE.
2006.
Open Source Graphics Engine.www.ogre3d.orgGuillermo Pe?rez, Gabriel Amores and Pilar Mancho?n.2005.
Two Strategies for multimodal fusion.
E.V.Zudilova?Sainstra and T. Adriaansen (eds.)
Proceed-ings of Multimodal Interaction for the Visualizationand Exploration of Scientific Data, 26?32.
Trento,Italy.Guillermo Pe?rez, Gabriel Amores, Pilar Mancho?n andDavid Gonza?lez Maline.
2006.
Generating Multilin-gual Grammars from OWL Ontologies.
Research inComputing Science, 18:3?14.Guillermo Pe?rez, Gabriel Amores, Pilar Mancho?n, Fer-nando Go?mez and Jesu?s Gonza?lez.
2006.
IntegratingOWL Ontologies with a Dialogue Manager.
Proce-samiento del Lenguaje Natural 37:153?160.Guillermo Pe?rez, Gabriel Amores and Pilar Mancho?n.2006.
A Multimodal Architecture For Home Con-trol By Disabled Users.
Proceedings of the IEEE/ACL2006 Workshop on Spoken Language Technology,134?137.
IEEE, New York, USA.Talk Project.
Talk and Look: Linguistic Tools for Am-bient Linguistic Knowledge.
2004.
6th FrameworkProgramme.
www.talk-project.orgWolfgang Wahlster, Elisabeth Andre?, Wolfgang Finkler,Hans?Ju?rgen Profitlich and Thomas Rist.
1993.
Plan?Based integration of natural language and graphicsgeneration.
Artificial intelligence, 63:287?247.4
