Proceedings of the 7th Workshop on Statistical Machine Translation, pages 133?137,Montre?al, Canada, June 7-8, 2012. c?2012 Association for Computational LinguisticsMorpheme- and POS-based IBM1 scores and language model scores fortranslation quality estimationMaja Popovic?German Research Center for Artificial Intelligence (DFKI)Language Technology (LT), Berlin, Germanymaja.popovic@dfki.deAbstractWe present a method we used for the qualityestimation shared task of WMT 2012 involvingIBM1 and language model scores calculatedon morphemes and POS tags.
The IBM1 scorescalculated on morphemes and POS-4grams ofthe source sentence and obtained translationoutput are shown to be competitive with theclassic evaluation metrics for ranking of trans-lation systems.
Since these scores do not re-quire any reference translations, they can beused as features for the quality estimation taskpresenting a connection between the sourcelanguage and the obtained target language.
Inaddition, target language model scores of mor-phemes and POS tags are investigated as esti-mates for the obtained target language quality.1 IntroductionAutomatic quality estimation is a topic of increas-ing interest in machine translation.
Different fromevaluation task, quality estimation does not rely onany reference translations ?
it relies only on infor-mation about the input source text, obtained targetlanguage text, and translation process.
Being a newtopic, it still does not have well established base-lines, datasets or standard evaluation metrics.
Theusual approach is to use a set of features which areused to train a classifier in order to assign a predic-tion score to each sentence.In this work, we propose a set of features basedon the morphological and syntactic properties of in-volved languages thus abstracting away from wordsurface particularities (such as vocabulary and do-main).
This approach is shown to be very useful forevaluation task (Popovic?, 2011; Popovic?
et al, 2011;Callison-Burch et al, 2011).
The features investi-gated in this work are based on the language model(LM) scores and on the IBM1 lexicon scores (Brownet al, 1993).The inclusion of IBM1 scores in translation sys-tems has shown experimentally to improve transla-tion quality (Och et al, 2003).
They also have beenused for confidence estimation for machine transla-tion (Blatz et al, 2003).
The IBM1 scores calcu-lated on morphemes and POS-4grams are shown tobe competitive with the classic evaluation metricsbased on comparison with given reference transla-tions (Popovic?
et al, 2011; Callison-Burch et al,2011).
To the best of our knowledge, these scoreshave not yet been used for translation quality esti-mation.
The LM scores of words and POS tags areused for quality estimation in previous work (Spe-cia et al, 2009), and in our work we investigate thescores calculated on morphemes and POS tags.At this point, only preliminary experiments havebeen carried out in order to determine if the pro-posed features are promising at all.
We did not useany classifier, we used the obtained scores to rankthe sentences of a given translation output from thebest to the worst.
The Spearman?s rank correlationcoefficients between our ranking and the ranking ob-tained using human scores are then computed on theprovided manually annotated data sets.2 Morpheme- and POS-based featuresA number of features for quality estimation havebeen already investigated in previous work (Speciaet al, 2009).
In this paper, we investigate two sets of133features which do not depend on any aspect of trans-lation process but only on the morphological andsyntactic structures of the involved languages: theIBM1 scores and the LM scores calculated on mor-phemes and POS tags.
The IBM1 scores describethe correspondences between the structures of thesource and the target language, and the LM scoresdescribe the structure of the target language.
In ad-dition to the input source text and translated targetlanguage hypothesis, a parallel bilingual corpus forthe desired language pair and a monolingual corpusfor the desired target language are required in or-der to learn IBM1 and LM probabilities.
AppropriatePOS taggers and tools for splitting words into mor-phemes are necessary for each of the languages.
ThePOS tags cannot be only basic but must have all de-tails (e.g.
verb tenses, cases, number, gender, etc.
).2.1 IBM1 scoresThe IBM1 model is a bag-of-word translation modelwhich gives the sum of all possible alignment proba-bilities between the words in the source sentence andthe words in the target sentence.
Brown et al (1993)defined the IBM1 probability score for a translationpair fJ1 and eI1 in the following way:P (fJ1 |eI1) =1(I + 1)JJ?j=1I?i=0p(fj |ei) (1)where fJ1 is the source language sentence of lengthJ and eI1 is the target language sentence of length I .As it is a conditional probability distribution, weinvestigated both directions as quality scores.
In or-der to avoid frequent confusions about what is thesource and what the target language, we defined ourscores in the following way:?
source-to-hypothesis (sh) IBM1 score:IBM1sh =1(H + 1)SS?j=1H?i=0p(sj |hi) (2)?
hypothesis-to-source (hs) IBM1 score:IBM1hs =1(S + 1)HH?i=1S?j=0p(hi|sj) (3)where sj are the units of the original source lan-guage sentence, S is the length of this sentence, hiare the units of the target language hypothesis, andH is the length of this hypothesis.The units investigated in this work are morphemesand POS-4grams, thus we have the following fourIBM1 scores:?
MIBM1sh and MIBM1hs:IBM1 scores of word morphemes in each direc-tion;?
P4IBM1sh and P4IBM1hs:IBM1 scores of POS 4grams in each direction.2.2 Language model scoresThe n-gram language model score is defined as:P (eI1) =I?i=1p(ei|ei...ei?n) (4)where ei is the current target language word andei...ei?n is the history, i.e.
the preceeding n words.In this paper, the two following language modelscores are explored:?
MLM6:morpheme-6gram language model score;?
PLM6:POS-6gram language model score.3 Experimental set-upThe IBM1 probabilities necessary for the IBM1scores are learnt using the WMT 2010 NewsCommentary Spanish-English, French-English andGerman-English parallel texts.
The language mod-els are trained on the corresponding target parts ofthis corpus using the SRI language model tool (Stol-cke, 2002).
The POS tags for all languages were pro-duced using the TreeTagger1, and the morphemesare obtained using the Morfessor tool (Creutz andLagus, 2005).
The tool is corpus-based andlanguage-independent: it takes a text as input andproduces a segmentation of the word forms observedin the text.
The obtained results are not strictly1http://www.ims.uni-stuttgart.de/projekte/corplex/TreeTagger/134linguistic, however they often resemble a linguisticmorpheme segmentation.
Once a morpheme seg-mentation has been learnt from some text, it canbe used for segmenting new texts.
In our experi-ments, the splitting are learnt from the training cor-pus used for the IBM1 lexicon probabilities.
Theobtained segmentation is then used for splitting thecorresponding source texts and hypotheses.
Detailedcorpus statistics are shown in Table 1.Using the obtained probabilities, the scores de-scribed in Section 2 are calculated for the pro-vided annotated data: the English-Spanish data fromWMT 2008 consisting of four translation outputsproduced by four different systems (Specia et al,2010), the French-English and English-Spanish datafrom WMT 2010 (Specia, 2011), as well as for anadditional WMT 2011 German-English and English-German annotated data.
The human quality scoresfor the first two data sets range from 1 to 4, and forthe third data set from 1 to 3.
The interpretation ofhuman scores is:1. requires complete retranslation (bad)2. post-editing quicker than retranslation (edit?
);this class was omitted for the third data set3.
little post-editing needed (edit+)4. fit for purpose (good)As a first step, the arithmetic means and standarddeviations are calculated for each feature and eachclass in order to see if the features are at all possiblecandidates for quality estimation, i.e.
if the valuesfor different classes are distinct.After that, the main test is carried out: for eachof the features, the Spearman correlation coefficient?
with the human ranking are calculated for eachdocument.
In total, 9 correlation coefficients are ob-tained for each score ?
four Spanish outputs from theWMT 2008 task, one Spanish and one English outputfrom the WMT 2010 as well as one English and twoGerman outputs from the WMT 2011 task.The obtained correlation results were then sum-marised into the following two values:?
meana correlation coefficient averaged over all trans-lation outputs;?
rank>percentage of translation outputs where the par-ticular feature has better correlation than theother investigated features.4 Results4.1 Arithmetic meansThe preliminary experiments consisted of compar-ing arithmetic means of scores for each feature andeach class.
The idea is: if the values are distinctenough, the feature is a potential candidate for qual-ity estimation.
In addition, standard deviations werecalculated in order to estimate the overlapping.For most translation outputs, all of our featureshave distinct arithmetic means for different classesand decent standard deviations, indicating that theyare promising for further investigation.
On all WMT2011 outputs annotated with three classes, the dis-tinction is rather clear, as well as for the majority ofthe four class outputs.However, on some of the four class translationoutputs, the values of the bad translation class wereunexpected in the following two ways:?
the bad class overlaps with the edit?
class;?
the bad class overlaps with the edit+ class.The first overlapping problem occured on two trans-lation outputs of the 2011 set, and the second one onthe both outputs of the 2010 set.Examples for the PLM6 and P4IBM1sh featuresare shown in Table 2.
First two rows present threeclass and four class outputs with separated arith-metic means, the first problem is shown in the thirdrow, and the second (and more serious) problem ispresented in the last row.These overlaps have not been investigated furtherin the framework of this work, however this shouldbe studied deeply (especially the second problem) inorder to better understand the underlying phenom-ena and improve the features.4.2 Spearman correlation coefficientsAs mentioned in the previous section, Spearmanrank correlation coefficients are calculated for eachtranslation output and for each feature, and sum-marised into two values described in Section 3, i.e.135Spanish English French English German Englishsentences 97122 83967 100222running words 2661344 2338495 2395141 2042085 2475359 2398780vocabulary:words 69620 53527 56295 50082 107278 54270morphemes 14178 13449 12004 12485 22211 13499POS tags 69 44 33 44 54 44POS-4grams 135166 121182 62177 114555 114314 123550Table 1: Statistics of the corpora for training IBM1 lexicon models and language models.feature output / class ok edit+ edit?
badPLM6 de-en 13.5 / 7.3 23.7 / 13.6 33.0 / 19.7es-en4 10.9 / 5.0 20.7 / 8.7 34.6 / 16.4 49.0 / 23.7es-en3 18.5 / 11.0 30.2 / 15.6 38.4 / 17.4 37.9 / 18.9fr-en 15.2 / 8.8 26.2 / 13.7 34.5 / 18.4 21.7 / 11.3P4IBM1sh de-en 50.5 / 38.4 109.7 / 75.6 161.8 / 108.3es-en4 37.9 / 25.0 88.7 / 48.7 165.8 / 89.0 241.5 / 127.4es-en3 77.0 / 56.7 139.8 / 82.5 186.4 / 94.6 185.2 / 102.0fr-en 53.5 / 44.3 110.0 / 69.3 151.8 / 90.9 90.8 / 59.0Table 2: Arithmetic means with standard deviations of PLM6 and P4IBM1sh scores for four translation outputs: firsttwo rows present decently separated classes, third row illustrates the overlap problem concerning the bad and the edit?class, the last row illustrates the overlap problem concerning the bad and the edit+ class.mean and rank>.
The results are shown in Table 3.In can be seen that the best individual features arePOS IBM1 scores followed by POS LM score.The next step was to investigate combinations ofthe individual features.
First, we calculated arith-metic mean of POS based features only, since theyare more promising than the morpheme based ones,however we did not yield any improvements overthe individual mean values.
As a next step, we in-troduced weights to the features according to theirmean correlations, i.e.
we did not omit the mor-pheme features but put more weight on the POSbased ones.
Nevertheless, this also did not resultin an improvement.
Furthermore, we tried a sim-ple arithmetic mean of all features, and this resultedin a better Spearman correlation coefficients.Following all these observations, we decided tosubmit the arithmetic mean of all features to theWMT 2012 quality estimation task.
Our submissionconsisted only of sentence ranking without scores,since we did not convert our scores to the inter-val [1,5].
Therefore we did not get any MAE orRMSE results, only DeltaAvg and Spearman corre-lation coefficients which were both 0.46.
The high-est scores in the shared task were 0.63, the lowestabout 0.15, and for the ?baseline?
system which usesa set of well established features with an SVM clas-sifier about 0.55.5 Conclusions and outlookThe results presented in this article show that theIBM1 and the LM scores calculated on POS tags andmorphemes have the potential to be used for theestimation of translation quality.
These results arevery preliminary, offering many directions for futurework.
The most important points are to use a classi-fier, as well as to combine the proposed features withalready established features.
Furthermore, the badclass overlapping problem described in Section 4.1should be further investigated and understood.AcknowledgmentsThis work has been partly developed within theTARAXU?
project financed by TSB Technologies-136mean rank>0.449 P4IBM1sh 70.4 P4IBM1sh0.445 P4IBM1hs 68.5 P4IBM1hs0.444 PLM6 61.1 PLM60.430 MLM6 27.7 MLM60.426 MIBM1sh 20.3 MIBM1sh0.420 MIBM1hs 9.2 MIBM1hs0.450 arithmetic mean 83.3 arithmetic meanTable 3: Features sorted by average correlation (column 1) and rank> value (column 2).
The most promising scoreis the arithmetic mean of all individual features.
The most promising individual features are POS-4gram IBM1 scoresfollowed by POS-6gram language model score.tiftung Berlin ?
Zukunftsfonds Berlin, co-financedby the European Union ?
European fund for regionaldevelopment.ReferencesJohn Blatz, Erin Fitzgerald, George Foster, Simona Gan-drabur, Cyril Goutte, Alex Kulesza, Alberto Sanchis,and Nicola Ueffing.
2003.
Confidence estimation formachine translation.
Final report, JHU/CLSP SummerWorkshop.Peter F. Brown, Stephen A. Della Pietra, Vincent J. DellaPietra, and Robert L. Mercer.
1993.
The mathemat-ics of statistical machine translation: Parameter esti-mation.
Computational Linguistics, 19(2):263?311,June.Chris Callison-Burch, Philipp Koehn, Christof Monz,and Omar Zaidan.
2011.
Findings of the 2011 Work-shop on Statistical Machine Translation.
In Proceed-ings of the Sixth Workshop on Statistical MachineTranslation (WMT 2011), pages 22?64, Edinburgh,Scotland, July.Mathias Creutz and Krista Lagus.
2005.
Unsupervisedmorpheme segmentation and morphology inductionfrom text corpora using morfessor 1.0.
Technical Re-port Report A81, Computer and Information Science,Helsinki University of Technology, Helsinki, Finland,March.Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Katherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2003.
Syn-tax for statistical machine translation.
Technical re-port, Johns Hopkins University 2003 Summer Work-shop on Language Engineering, Center for Languageand Speech Processing, Baltimore, MD, USA, August.Maja Popovic?, David Vilar Torres, Eleftherios Avramidis,and Aljoscha Burchardt.
2011.
Evaluation withoutreferences: IBM1 scores as evaluation metrics.
In Pro-ceedings of the Sixth Workshop on Statistical MachineTranslation (WMT 2011), pages 99?103, Edinburgh,Scotland, July.Maja Popovic?.
2011.
Morphemes and POS tags forn-gram based evaluation metrics.
In Proceedings ofthe Sixth Workshop on Statistical Machine Translation(WMT 2011), pages 104?107, Edinburgh, Scotland,July.Lucia Specia, Marco Turchi, Zhuoran Wang, JohnShawe-Taylor, and Craig Saunders.
2009.
Improv-ing the confidence of machine translation quality es-timates.
In Machine Translation Summit XII, Ottawa,Canada.Lucia Specia, Nicola Cancedda, and Marc Dymetman.2010.
A Dataset for Assessing Machine TranslationEvaluation Metrics.
In Proceedings of the Seventhconference on International Language Resources andEvaluation (LREC?2010), pages 3375?3378, Valletta,Malta, May.Lucia Specia.
2011.
Exploiting Objective Annotationsfor Measuring Translation Post-editing Effort.
In Pro-ceedings of the 15th Annual Conference of the Euro-pean Association for Machine Translation (EAMT 11),pages 73?80, Leuven, Belgium, May.Drahom?
?ra ?Johanka?
Spoustova?, Jan Hajic?, Jan Raab,and Miroslav Spousta.
2009.
Semi-Supervised Train-ing for the Averaged Perceptron POS Tagger.
InProceedings of the 12th Conference of the EuropeanChapter of the ACL (EACL 2009), pages 763?771,Athens, Greece, March.Andreas Stolcke.
2002.
SRILM ?
an extensible languagemodeling toolkit.
In Proceedings of the 7th Inter-national Conference on Spoken Language Processing(ICSLP 02), volume 2, pages 901?904, Denver, CO,September.137
