Proceedings of NAACL HLT 2007, Companion Volume, pages 49?52,Rochester, NY, April 2007. c?2007 Association for Computational LinguisticsDocument Similarity Measures to DistinguishNative vs. Non-Native Essay WritersOlga GurevichEducational Testing ServiceRosedale & Carter Roads,Turnbull 11RPrinceton, NJ 08541ogurevich@ets.orgPaul DeaneEducational Testing ServiceRosedale & Carter Roads,Turnbull 11RPrinceton, NJ 08541pdeane@ets.orgAbstractThe ability to distinguish statistically dif-ferent populations of speakers or writerscan be an important asset in many NLPapplications.
In this paper, we describe amethod of using document similaritymeasures to describe differences in be-havior between native and non-nativespeakers of English in a writing task.11 IntroductionThe ability to distinguish statistically differentpopulations of speakers or writers can be an impor-tant asset in many NLP applications.
In this paper,we describe a method of using document similaritymeasures to describe differences in behavior be-tween native and non-native speakers of English ina prompt response task.We analyzed results from the new TOEFL inte-grated writing task, described in the next section.All task participants received the same set ofprompts and were asked to summarize them.
Theresulting essays are all trying to express the same?gist?
content, so that any measurable differencesbetween them must be due to differences in indi-vidual language ability and style.
Thus the task isuniquely suited to measuring differences in linguis-tic behavior between populations.Our measure of document similarity, describedin section 3, is a combination of word overlap andsyntactic similarity, also serving as a measure ofsyntactic variability.
The results demonstrate sig-nificant differences between native and non-native1 This research was funded while the first author was a Re-search Postdoctoral Fellow at ETS in Princeton, NJ.speakers that cannot be attributed to any demo-graphic factor other than the language ability itself.2 TOEFL Integrated Writing Task andScoringThe Test of English as a Foreign Language(TOEFL) is administered to foreign students wish-ing to enroll in US or Canadian universities.
Itaims to measure the extent to which a student hasacquired English; thus native speakers should onaverage perform better on the test regardless oftheir analytical abilities.
The TOEFL now includesa writing component, and pilot studies were con-ducted with native as well as non-native speakers.One of the writing components is an IntegratedWriting Task.
Students first read an expositorypassage, which remains on the screen throughoutthe task.
Students then hear a segment of a lectureconcerning the same topic.
However, the lecturecontradicts and complements the information con-tained in the reading.
The lecture is heard once;students then summarize the lecture and the read-ing and describe any contradictions between them.The resulting essays are scored by human raterson a scale of 0 to 5, with 5 being the best possiblescore2.
The highest-scoring essays express ideasfrom both the lecture and the reading using correctgrammar; the lowest-scoring essays rely on onlyone of the prompts for information and havegrammatical problems; and the scores in betweenshow a combination of both types of deficiencies.The test prompt contained passages about theadvantages and disadvantages of working ingroups; the reading was 260 words long, the lec-ture 326 words.
540 non-native speakers and 9502 Native speaker essays were initially scored with possiblehalf-grades such as 2.5.
For purposes of comparison, thesewere rounded down to the nearest integer.49native speakers were tested by ETS in 2004.
ETSalso collected essential demographic data such asnative language, educational level, etc., for eachstudent.
For later validation, we excluded 1/3 ofeach set, selected at random, thus involving 363non-native speakers and 600 native speakers.Percent score frequencies051015202530351 2 3 4 5Sco reNon-nativeNativeFigure 1.
Relative score distributions.Among the non-native speakers, the mostcommon score was 1 (see Fig.
1 for a histogram).By contrast, native speaker scores centered around3 and showed a normal-type distribution.
The dif-ference in distributions confirms that the task iseffective at separating non-native speakers by skilllevel, and is easier for native speakers.
The poten-tial sources of difficulty include comprehension ofthe reading passage, listening ability and memoryfor the lecture, and the analytical ability to findcommonalities and differences between the contentof the reading and the lecture.3 Document Similarity MeasureDue to the design of the TOEFL task, the contentof the student essays is highly constrained.
Theaim of the computational measures is to extractgrammatical and stylistic differences between dif-ferent essays.
We do this by comparing the essaysto the reading and lecture prompts.
Our end goal isto determine to what extent speakers diverge fromthe prompts while retaining the content.The prediction is that native speakers are muchmore likely to paraphrase the prompts while keep-ing the same gist, whereas non-native speakers arelikely to either repeat the prompts close to verba-tim, or diverge from them in ways that do not pre-serve the gist.
This intuition conforms to previousstudies of native vs. non-native speakers?
textsummarization (cf.
Campbell 1987), although weare not aware of any related computational work.We begin by measuring lexico-grammaticalsimilarity between each essay and the two prompts.An essay is represented as a set of features derivedfrom its lexico-grammatical content, as describedbelow.
The resulting comparison measure goesbeyond simple word or n-gram overlap by provid-ing a measure of structural similarity as well.
Inessence, our method measures to what extent theessay expresses the content of the prompt in thesame words, used in the same syntactic positions.3.1 C-rater tuplesIn order to get a measure of syntactic similarity, werelied on C-rater (Leacock & Chodorow 2003), anautomatic scoring engine developed at ETS.
C-rater includes several basic NLP components, in-cluding POS tagging, morphological processing,anaphora resolution, and shallow parsing.
Theparsing produces tuples for each clause, which de-scribe each verb and its syntactic arguments (1).
(1) CLAUSE: the group spreads responsibil-ity for a decision to all the membersTUPLE: :verb: spread :subj: the group :obj:responsible :pp.for: for a decide :pp.to: to allC-rater does not produce full-sentence trees orprepositional phrase attachment.
However, thetuples are reasonably accurate on non-native input.3.2 Lexical and Syntactic FeaturesC-rater produces tuples for each document, oftenseveral per sentence.
For the current experiment,we used the main verb, its subject and object.
Wethen converted each tuple into a set of features,which included the following:?
The verb, subject (pro)noun, and object(pro)noun as individual words;?
All of the words together as a single feature;?
The verb, subject, and object words withtheir argument roles.Each document can now be represented as a setof tuple-derived features, or feature vectors.3.3 Document ComparisonTwo feature vectors derived from tuples can becompared using a cosine measure (Salton 1989).The closer to 1 the cosine, the more similar the twofeature sets.
To compensate for different frequen-cies of the features and for varying documentlengths, the feature vectors are weighted usingstandard tf*idf techniques.50In order to estimate the similarity between twodocuments, we use the following procedure.
Foreach tuple vector in Document A, we find the tuplein Document B with the maximum cosine to thetuple in Document A.
The maximum cosine val-ues for each tuple are then averaged, resulting in asingle scalar value for Document A.
We call thismeasure Average Maximum Cosine (AMC).We calculated AMCs for each student responseversus the reading, the lecture, and the reading +lecture combined.
This procedure was performedfor both native and non-native essays.
A detailedexamination of the resulting trends is in section 4.3.4 Other Measures of Document SimilarityWe also performed several measures of documentsimilarity that did not include syntactic features.Content Vector AnalysisThe student essays and the prompts were comparedusing Content Vector Analysis (CVA), where eachdocument was represented as a vector consisting ofthe words in it (Salton 1989).
The tf*idf-weightedvectors were compared by a cosine measure.For non-native speakers, there was a noticeabletrend.
At higher score levels (where the score isdetermined by a human rater), student essaysshowed more similarity to both the reading and thelecture prompts.
Both the reading and lecturesimilarity trends were significant (linear trend; F=MSlinear trend/MSwithin-subjects=63 for the reading; F=71for the lecture at 0.05 significance level3).
Thus,the rate of vocabulary retention from both promptsincreases with higher English-language skill level.Native speakers showed a similar pattern of in-creasing cosine similarity between the essay andthe reading (F=35 at 0.05 significance for thetrend), and the lecture (F=35 at the 0.05 level).BLEU scoreIn order to measure the extent to which wholechunks of text from the prompt are reproduced inthe student essays, we used the BLEU score,known from studies of machine translation (Pap-ineni et al 2002).
We used whole essays as sec-tions of text rather than individual sentences.For non-native speakers, the trend was similarto that found with CVA: at higher score levels, the3 All statistical calculations were performed as ANOVA-styletrend analyses using SPSS.overlap between the essays and both prompts in-creased (F=52.4 at the 0.05 level for the reading;F=53.6 for the lecture).Native speakers again showed a similar pattern,with a significant trend towards more similarity tothe reading (F=35.6) and the lecture (F=31.3).These results were confirmed by a simple n-gramoverlap measure.4 Results4.1 Overall similarity to reading and lectureThe AMC similarity measure, which relies on syn-tactic as well as lexical similarity, produced some-what different results from simpler bag-of-word orn-gram measures.
In particular, there was a differ-ence in behavior between native and non-nativespeakers: non-native speakers showed increasedstructural similarity to the lecture with increasingscores, but native speakers did not.For non-native speakers, the trend of increasedAMC between the essay and the lecture was sig-nificant (F=10.9).
On the other hand, there was nosignificant increase in AMC between non-nativeessays and the reading (F=3.4).
Overall, for non-native speakers the mean AMC was higher for thereading than for the lecture (0.114 vs. 0.08).Native speakers, by contrast, showed no sig-nificant trends for either the reading or the lecture.Overall, the average AMCs for the reading and thelecture were comparable (0.08 vs. 0.075).We know from results of CVA and BLEUanalyses that for both groups of speakers, higher-scoring essays are more lexically similar to theprompts.
Thus, the lack of a trend for nativespeakers must be due to lack of increase in struc-tural similarity between higher-scoring essays andthe prompts.
Since better essays are presumablybetter at expressing the content of the prompts, wecan hypothesize that native speakers paraphrase thecontent more than non-native speakers.4.2 Difference between lecture and readingThe most informative measure of speaker behaviorwas the difference between the Average MaximumCosine with the reading and the lecture, calculatedby subtracting the lecture AMC from the readingAMC.
Here, non-native speakers showed a sig-nificant downward linear trend with increasing51score (F=6.5; partial eta-squared 0.08), whereas thenative speakers did not show any trend (F=1.5).The AMC differences are plotted in Figure 3.AMC difference between reading andlecture-0.0500.050.10.150 1 2 3 4 5ScoreNon-nativeNativeFigure 2 - AMC difference between reading andlectureNon-native speakers with lower scores relymostly on the reading to produce their response,whereas speakers with higher scores rely some-what more on the lecture than on the reading.
Bycontrast, native speakers show no correlation be-tween score and reading vs. lecture similarity.Thus, there is a significant difference in the overalldistribution and behavior between native and non-native speaker populations.
This difference alsoshows that human raters rely on information otherthan simple verbatim similarity to the lecture inassigning the overall scores.4.3 Other parameters of variationFor non-native speakers, the best predictor of thehuman-rated score is the difference in AMC be-tween the reading and the lecture.As demonstrated in the previous section, theAMC difference does not predict the score for na-tive speakers.
We analyzed native speaker demo-graphic data in order to find any other possiblepredictors.
The students?
overall listening score,their status as monolingual vs. bilingual, their par-ents?
educational levels all failed to predict the es-say scores.5 Discussion and Future WorkThe Average Maximum Cosine measure as de-scribed in this paper successfully characterizes thebehavior of native vs. non-native speaker popula-tions on an integrated writing task.
Less skillfulnon-native speakers show a significant trend ofrelying on the easier, more available prompt (thereading) than on the harder prompt (the lecture),whereas more skillful readers view the lecture asmore relevant and rely on it more than on the read-ing.
This difference can be due to better listeningcomprehension for the lecture and/or better mem-ory.
By contrast, native speakers rely on both thereading and the lecture about the same, and showno significant trend across skill levels.
Nativespeakers seem to deviate more from the structureof the original prompts while keeping the samecontent, signaling better paraphrasing skills.While not a direct measure of gist similarity,this technique represents a first step toward detect-ing paraphrases in written text.
In the immediatefuture, we plan to extend the set of features to in-clude non-verbatim similarity, such as synonymsand words derived by LSA-type comparison (Lan-dauer et al 1998).
In addition, the syntactic fea-tures will be expanded to include frequentgrammatical alternations such as active / passive.A rather simple measure such as AMC has al-ready revealed differences in population distribu-tions for native vs. non-native speakers.Extensions of this method can potentially be usedto determine if a given essay was written by a na-tive or a non-native speaker.
For instance, a statis-tical classifier can be trained to distinguish featuresets characteristic for different populations.
Such aclassifier can be useful in a number of NLP-relatedfields, including information extraction, search,and, of course, educational measurement.ReferencesCampbell, C. 1987.
Writing with Others?
Words: Nativeand Non-Native University Students?
Use of Infor-mation from a Background Reading Text in Aca-demic Compositions.
Technical Report, UCLACenter for Language Education and Research.Landauer, T.; Foltz, P. W; and Laham.
D. 1998.
Intro-duction to Latent Semantic Analysis.
DiscourseProcesses 25: 259-284.Leacock, C., & Chodorow, M. 2003.
C-rater: Scoring ofshort-answer questions.
Computers and the Humani-ties, 37(4), 389-405.Papineni, K; Roukos, S.; Ward, T. and Zhu, W-J.
2002.BLEU: a Method for Automatic Evaluation of Ma-chine Translation.
ACL ?02, p. 311-318.Salton, G. 1989.
Automatic Text Processing: The Trans-formation, Analysis, and Retrieval of Information byComputer.
Reading, MA: Addison-Weley.52
