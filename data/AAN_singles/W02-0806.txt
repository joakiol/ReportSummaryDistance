Assessing System Agreement and Instance Difficultyin the Lexical Sample Tasks of SENSEVAL-2Ted PedersenDepartment of Computer ScienceUniversity of MinnesotaDuluth, MN, 55812 USAtpederse@d.umn.eduAbstractThis paper presents a comparative evalua-tion among the systems that participatedin the Spanish and English lexical sam-ple tasks of SENSEVAL-2.
The focus ison pairwise comparisons among systemsto assess the degree to which they agree,and on measuring the difficulty of the testinstances included in these tasks.1 IntroductionThis paper presents a post-mortem analysis ofthe English and Spanish lexical sample tasks ofSENSEVAL-2.
Two closely related questions areconsidered.
First, to what extent did the compet-ing systems agree?
Did systems tend to be redun-dant and have success with many of the same test in-stances, or were they complementary and able to dis-ambiguate different portions of the instance space?Second, how much did the difficulty of the test in-stances vary?
Are there test instances that provedunusually difficult to disambiguate relative to otherinstances?We address the first question via a series of pair-wise comparisons among the participating systemsthat measures their agreement via the kappa statis-tic.
We also introduce a simple measure of the de-gree to which systems are complementary called op-timal combination.
We analyze the second questionby rating the difficulty of test instances relative to thenumber of systems that were able to disambiguatethem correctly.Nearly all systems that received official scoresin the Spanish and English lexical sample tasks ofSENSEVAL-2 are included in this study.
There are23 systems included from the English lexical sam-ple task and eight from the Spanish.
Table 1 lists thesystems and shows the number of test instances thateach disambiguated correctly, both by part of speechand in total.2 Pairwise System AgreementAssessing agreement among systems sheds light onwhether their combined performance is potentiallymore accurate than that of any of the individual sys-tems.
If several systems are largely in agreement,then there is little benefit in combining them sincethey are redundant and will simply reinforce one an-other.
However, if some systems disambiguate in-stances that others do not, then they are complemen-tary and it may be possible to combine them to takeadvantage of the different strengths of each systemto improve overall accuracy.The kappa statistic (Cohen, 1960) is a measure ofagreement between multiple systems (or judges) thatis scaled by the agreement that would be expectedjust by chance.
A value of 1.00 suggests completeagreement, while 0.00 indicates pure chance agree-ment.
Negative values indicate agreement less thanwhat would be expected by chance.
(Krippendorf,1980) points out that it is difficult to specify a par-ticular value of kappa as being generally indicativeof agreement.
As such we simply use kappa as atool for comparison and relative ranking.
A detaileddiscussion on the use of kappa in natural languageprocessing is presented in (Carletta, 1996).July 2002, pp.
40-46.
Association for Computational Linguistics.Disambiguation: Recent Successes and Future Directions, Philadelphia,Proceedings of the SIGLEX/SENSEVAL Workshop on Word SenseTable 1: Lexical Sample Systemssystem correct instancesname noun verb adj total (%)English1754 1806 768 4328 (1.00)jhu final 1196 1022 562 2780 (0.64)smuls 1219 1016 528 2763 (0.64)kunlp 1171 1040 513 2724 (0.63)cs224n 1198 945 527 2670 (0.62)lia 1177 966 510 2653 (0.61)talp 1149 927 495 2571 (0.59)duluth3 1137 840 497 2473 (0.57)umcp 1081 891 487 2459 (0.57)ehu all 1069 891 480 2440 (0.56)duluth4 1065 806 476 2346 (0.54)duluth2 1056 795 483 2334 (0.54)lesk corp 960 804 454 2218 (0.51)duluthB 1004 729 467 2200 (0.51)uned ls t 987 699 469 2155 (0.50)common 880 728 453 2061 (0.48)alicante 427 866 486 1779 (0.41)uned ls u 781 519 437 1736 (0.40)clr ls 602 393 272 1267 (0.29)iit2 541 348 166 1054 (0.24)iit1 516 337 182 1034 (0.24)lesk 467 328 182 977 (0.23)lesk def 438 159 108 704 (0.16)random 303 153 155 611 (0.14)Spanish799 745 681 2225 (1.00)jhu 560 478 546 1584 (0.71)cs224 520 443 526 1489 (0.67)umcp 482 435 479 1396 (0.63)duluth8 494 382 494 1369 (0.62)duluth7 470 374 480 1324 (0.60)duluth9 445 359 446 1250 (0.56)duluthY 411 325 434 1170 (0.53)alicante 269 381 468 1118 (0.50)To study agreement we have made a series of pair-wise comparisons among the systems included in theEnglish and Spanish lexical sample tasks.
Each pair-wise combination is represented in a 2  2 contin-gency table, where one cell represents the number oftest instances that both systems disambiguate cor-rectly, one cell represents the number of instanceswhere both systems are incorrect, and there are twocells to represent the counts when only one system iscorrect.
Agreement does not imply accuracy, sincetwo systems may get a large number of the same in-stances incorrect and have a high rate of agreement.Tables 2 and 3 show the system pairs in theEnglish and Spanish lexical sample tasks that ex-hibit the highest level of agreement according to thekappa statistic.
The values in the both?one?zero col-umn indicate the percentage of instances where bothsystems are correct, where only one is correct, andwhere neither is correct.
The top 15 pairs are shownfor nouns and verbs, and the top 10 for adjectives.A complete list would include about 250 pairs foreach part of speech for English and 24 such pairs forSpanish.The utility of kappa agreement is confirmed inthat system pairs known to be very similar have cor-respondingly high measures.
In Table 2, duluth2and duluth3 exhibit a high kappa value for all partsof speech.
This is expected since duluth3 is an en-semble approach that includes duluth2 as one of itsmembers.
The same relationship exists between du-luth7 and duluth8 in the Spanish lexical sample, andcomparable behavior is seen in Table 3.A more surprising case is the even higher level ofagreement between the most common sense base-line and the lesk corpus baseline shown in Table 2.This is not necessarily expected, and suggests thatlesk corpus may not be finding a significant numberof matches between the Senseval contexts and theWordNet glosses (as the lesk algorithm would hopeto do) but instead may be relying on a simple defaultin many cases.In previous work (Pedersen, 2001) we propose a50-25-25 rule that suggests that about half of the in-stances in a supervised word sense disambiguationevaluation will be fairly easy for most systems toresolve, another quarter will be harder but possiblefor at least some systems, and that the final quar-ter will be very difficult for any system to resolve.Table 2: Pairwise Agreement Englishsystem pair both-one-zero kappaNounscommon lesk corp 0.49 0.06 0.44 0.87duluth2 duluth3 0.60 0.08 0.32 0.82lesk corp umcp 0.53 0.11 0.36 0.78duluth2 duluthB 0.54 0.14 0.32 0.70iit1 iit2 0.24 0.13 0.63 0.69duluth3 duluthB 0.56 0.15 0.29 0.68common umcp 0.48 0.16 0.36 0.68ehu all umcp 0.55 0.17 0.29 0.64uned ls t uned ls u 0.41 0.19 0.40 0.63duluth3 duluth4 0.55 0.18 0.27 0.61duluth2 duluth4 0.52 0.19 0.29 0.60duluth4 duluthB 0.51 0.19 0.29 0.59ehu all lesk corp 0.50 0.20 0.30 0.59cs224n duluth3 0.58 0.19 0.24 0.58cs224n duluth4 0.55 0.19 0.25 0.58Verbscommon lesk corp 0.39 0.06 0.55 0.88duluth2 duluth3 0.43 0.07 0.50 0.85duluth3 duluth4 0.39 0.14 0.47 0.72duluth2 duluth4 0.38 0.15 0.47 0.69lesk corp umcp 0.38 0.17 0.44 0.65common umcp 0.36 0.18 0.46 0.65cs224n duluth3 0.40 0.20 0.40 0.60cs224n duluth4 0.38 0.21 0.41 0.59cs224n duluth2 0.38 0.21 0.40 0.57uned ls t uned ls u 0.24 0.20 0.55 0.56duluth3 lia 0.39 0.23 0.38 0.54lesk corp talp 0.36 0.23 0.40 0.54cs224n lia 0.41 0.24 0.35 0.53common talp 0.34 0.24 0.42 0.52kunlp talp 0.42 0.24 0.33 0.51Adjectivescommon lesk corp 0.59 0.00 0.41 0.99duluth2 duluth3 0.63 0.03 0.34 0.93lesk corp umcp 0.58 0.07 0.35 0.86duluth2 duluthB 0.59 0.07 0.35 0.86duluth3 duluthB 0.60 0.07 0.34 0.86common umcp 0.58 0.07 0.35 0.86duluth4 duluthB 0.55 0.14 0.32 0.71duluth3 duluth4 0.57 0.14 0.30 0.71cs224n duluth3 0.60 0.13 0.27 0.70cs224n duluth2 0.59 0.14 0.27 0.70Table 3: Pairwise Agreement Spanishsystem pair both-one-zero kappaNounsduluth7 duluth8 0.57 0.11 0.32 0.76umcp duluth9 0.50 0.17 0.33 0.65duluth7 duluthY 0.49 0.21 0.30 0.57umcp duluthY 0.48 0.21 0.31 0.56duluth8 duluthY 0.50 0.21 0.29 0.56umcp duluth8 0.50 0.23 0.27 0.51umcp duluth7 0.50 0.23 0.27 0.51cs224 umcp 0.51 0.24 0.25 0.49duluth9 duluthY 0.44 0.26 0.30 0.47duluth8 duluth9 0.47 0.27 0.27 0.45cs224 duluth9 0.47 0.27 0.26 0.44cs224 jhu 0.55 0.25 0.20 0.43cs224 duluth8 0.51 0.27 0.22 0.42jhu umcp 0.51 0.29 0.21 0.38jhu duluth8 0.53 0.28 0.19 0.37Verbsduluth7 duluth8 0.48 0.08 0.44 0.84duluth8 duluth9 0.44 0.14 0.42 0.72umcp duluth8 0.48 0.14 0.37 0.71umcp duluth9 0.46 0.16 0.38 0.69duluth7 duluth9 0.42 0.16 0.42 0.68umcp duluth7 0.47 0.16 0.37 0.68duluth8 duluthY 0.44 0.16 0.39 0.67duluth9 duluthY 0.42 0.18 0.40 0.65duluth7 duluthY 0.43 0.18 0.39 0.64umcp duluthY 0.46 0.19 0.35 0.61cs224 umcp 0.49 0.19 0.32 0.61cs224 duluth8 0.44 0.25 0.32 0.50alicante umcp 0.42 0.25 0.33 0.50cs224 duluth7 0.43 0.26 0.32 0.48cs224 jhu 0.49 0.25 0.26 0.48Adjectivesduluth7 duluth8 0.69 0.06 0.25 0.85duluth7 duluthY 0.60 0.14 0.26 0.68umcp duluthY 0.60 0.15 0.26 0.67umcp duluth9 0.61 0.14 0.25 0.67duluth8 duluthY 0.61 0.15 0.24 0.67umcp duluth8 0.64 0.15 0.21 0.64duluth9 duluthY 0.56 0.18 0.26 0.60duluth8 duluth9 0.61 0.17 0.22 0.59umcp duluth7 0.62 0.17 0.21 0.59duluth7 duluth9 0.58 0.20 0.22 0.54This same idea could also be expressed by statingthat the kappa agreement between two word sensedisambiguation systems will likely be around 0.50.In fact this is a common result in the full set of pair-wise comparisons, particularly for overall results notbroken down by part of speech.
Tables 2 and 3 onlylist the largest kappa values, but even there kappaquickly reduces towards 0.50.
These same tablesshow that it is rare for two systems to agree on morethan 60% of the correctly disambiguated instances.3 Optimal CombinationAn optimal combination is the accuracy that couldbe attained by a hypothetical tool called an optimalcombiner that accepts as input the sense assignmentsfor a test instance as generated by several differentsystems.
It is able to select the correct sense fromthese inputs, and will only be wrong when none ofthe sense assignments is the correct one.
Thus, thepercentage accuracy of an optimal combiner is equalto one minus the percentage of instances that no sys-tem can resolve correctly.Of course this is only a tool for thought experi-ments and is not a practical algorithm.
An optimalcombiner can establish an upper bound on the accu-racy that could reasonably be attained over a partic-ular sample of test instances.Tables 4 and 5 list the top system pairs rankedby optimal combination (1.00 - value in zero col-umn) for the English and Spanish lexical samples.Kappa scores are also shown to illustrate the inter-action between agreement and optimal combination.Optimal combination is maximized when the per-centage of instances where both systems are wrongis minimized.
Kappa agreement is maximized byminimizing the percentage of instances where oneor the other system (but not both) is correct.
Thus,the only way a system pair could have a high mea-sure of kappa and a high measure of optimal com-bination is if they were very accurate systems thatdisambiguated many of the same test instances cor-rectly.System pairs with low measures of agreementare potentially quite interesting because they are themost likely to make complementary errors.
For ex-ample, in Table 5 under nouns, the alicante systemhas a low level of agreement with all of the otherTable 4: Optimal Combination Englishsystem pair both-one-zero kappaNounskunlp smuls 0.49 0.39 0.12 0.11smuls talp 0.48 0.39 0.13 0.11cs224n kunlp 0.48 0.39 0.13 0.11ehu all smuls 0.47 0.40 0.13 0.10cs224n talp 0.48 0.39 0.14 0.13jhu final kunlp 0.49 0.37 0.14 0.16smuls umcp 0.45 0.41 0.14 0.10kunlp lia 0.48 0.38 0.14 0.14lia talp 0.47 0.39 0.14 0.13jhu final talp 0.48 0.38 0.14 0.15duluth3 kunlp 0.47 0.38 0.15 0.15cs224n ehu all 0.48 0.38 0.15 0.16ehu all lia 0.47 0.38 0.15 0.15ehu all jhu final 0.48 0.36 0.16 0.19duluth3 talp 0.47 0.38 0.16 0.17Verbsjhu final kunlp 0.34 0.46 0.20 0.06ehu all jhu final 0.31 0.44 0.21 0.07ehu all smuls 0.31 0.44 0.21 0.07ehu all kunlp 0.33 0.41 0.22 0.13kunlp smuls 0.36 0.43 0.22 0.13cs224n ehu all 0.29 0.45 0.22 0.05ehu all lia 0.30 0.44 0.22 0.08cs224n kunlp 0.33 0.44 0.23 0.11alicante ehu all 0.26 0.47 0.23 0.03kunlp lia 0.34 0.42 0.23 0.14jhu final talp 0.32 0.44 0.24 0.12duluth3 ehu all 0.26 0.46 0.24 0.05ehu all talp 0.30 0.41 0.24 0.13alicante jhu final 0.30 0.45 0.24 0.09jhu final umcp 0.30 0.45 0.24 0.09Adjectivesalicante jhu final 0.46 0.37 0.08 0.03alicante smuls 0.41 0.41 0.09 -0.04alicante cs224n 0.42 0.40 0.09 -0.01alicante kunlp 0.41 0.39 0.11 0.03alicante lia 0.41 0.39 0.11 0.03alicante duluth3 0.40 0.40 0.11 0.02alicante talp 0.40 0.41 0.11 0.02alicante ehu all 0.41 0.39 0.11 0.05alicante umcp 0.39 0.40 0.12 0.04alicante duluth2 0.39 0.40 0.12 0.03Table 5: Optimal Combination Spanishsystem pair both-one-zero kappaNounsalicante jhu 0.29 0.32 0.11 0.06alicante duluth7 0.27 0.34 0.12 0.03alicante duluthY 0.25 0.35 0.12 0.01alicante duluth8 0.28 0.32 0.13 0.08alicante cs224 0.28 0.32 0.13 0.09alicante umcp 0.26 0.33 0.14 0.06alicante duluth9 0.26 0.31 0.16 0.14jhu duluthY 0.46 0.36 0.18 0.24jhu duluth7 0.51 0.29 0.19 0.35jhu duluth8 0.53 0.28 0.19 0.37cs224 jhu 0.55 0.25 0.20 0.43jhu duluth9 0.46 0.34 0.20 0.29jhu umcp 0.51 0.29 0.21 0.38cs224 duluth7 0.49 0.30 0.22 0.36cs224 duluth8 0.51 0.27 0.22 0.42Verbsjhu duluthY 0.39 0.38 0.23 0.23jhu umcp 0.48 0.27 0.25 0.44jhu duluth9 0.39 0.36 0.26 0.29jhu duluth8 0.42 0.32 0.26 0.35cs224 jhu 0.49 0.25 0.26 0.48jhu duluth7 0.42 0.32 0.26 0.36alicante jhu 0.45 0.26 0.29 0.47cs224 duluthY 0.43 0.27 0.30 0.46alicante cs224 0.41 0.28 0.31 0.44alicante duluthY 0.35 0.34 0.31 0.32cs224 umcp 0.49 0.19 0.32 0.61cs224 duluth7 0.43 0.26 0.32 0.48cs224 duluth8 0.44 0.25 0.32 0.50cs224 duluth9 0.41 0.27 0.32 0.47alicante umcp 0.42 0.25 0.33 0.50Adjectivesjhu duluth8 0.66 0.22 0.12 0.39jhu duluth7 0.64 0.24 0.12 0.36jhu duluthY 0.56 0.31 0.12 0.25alicante jhu 0.62 0.26 0.13 0.33jhu duluth9 0.59 0.29 0.13 0.29cs224 jhu 0.70 0.16 0.13 0.51jhu umcp 0.64 0.23 0.13 0.38alicante cs224 0.61 0.24 0.15 0.39cs224 duluth8 0.66 0.19 0.16 0.50cs224 duluth7 0.64 0.20 0.16 0.49Table 6: Difficulty of Instances# noun verb adj totalEnglish0 59 (16) 174 (6) 29 (8) 262 (8)1 51 (15) 116 (10) 26 (14) 193 (12)2 59 (18) 122 (12) 41 (21) 222 (15)3 64 (19) 117 (16) 29 (23) 210 (18)4 84 (17) 102 (16) 28 (18) 214 (17)5 76 (23) 76 (18) 24 (20) 176 (21)6 53 (28) 61 (30) 23 (31) 137 (29)7 51 (29) 65 (22) 23 (34) 139 (27)8 62 (27) 58 (34) 18 (31) 138 (30)9 47 (32) 69 (28) 17 (26) 133 (29)10 62 (28) 61 (32) 18 (30) 141 (30)11 55 (39) 56 (26) 21 (38) 132 (34)12 80 (40) 61 (41) 22 (35) 163 (40)13 86 (58) 56 (34) 21 (45) 163 (48)14 125 (65) 62 (49) 33 (51) 220 (59)15 131 (77) 125 (99) 36 (60) 292 (84)16 141 (83) 107 (117) 61 (70) 309 (92)17 133 (75) 100 (162) 86 (74) 319 (101)18 92 (73) 80 (203) 102 (80) 274 (113)19 97 (68) 59 (170) 49 (77) 205 (100)20 65 (66) 38 (192) 30 (49) 133 (96)21 42 (68) 15 (155) 17 (47) 74 (79)22 29 (70) 15 (73) 7 (39) 51 (67)23 10 (49) 11 (52) 7 (38) 28 (47)Spanish0 50 (16) 126 (12) 52 (24) 228 (16)1 81 (18) 63 (17) 32 (36) 176 (21)2 63 (24) 69 (18) 42 (50) 174 (28)3 63 (27) 55 (23) 39 (81) 157 (39)4 74 (32) 47 (23) 43 (101) 164 (47)5 94 (35) 49 (28) 35 (77) 178 (42)6 87 (40) 61 (39) 57 (90) 205 (53)7 182 (47) 94 (46) 88 (93) 364 (58)8 105 (44) 181 (62) 293 (166) 579 (111)Table 7: Difficulty of English Word Typesword-pos (test) mean word-pos (test) meancollaborate-v (30) 20.2 circuit-n (85) 10.7solemn-a (25) 18.3 sense-n (53) 10.6holiday-n (31) 17.7 authority-n (92) 10.5dyke-n (28) 17.5 replace-v (45) 10.4graceful-a (29) 17.3 restraint-n (45) 10.3vital-a (38) 16.7 live-v (67) 10.2detention-n (32) 16.5 treat-v (44) 10.1faithful-a (23) 16.5 free-a (82) 10.0yew-n (28) 16.1 nature-n (46) 10.0chair-n (69) 16.0 simple-a (66) 9.8ferret-v (1) 16.0 dress-v (59) 9.7blind-a (55) 15.7 cool-a (52) 9.7lady-n (53) 15.5 bar-n (151) 9.5spade-n (33) 15.3 stress-n (39) 9.5hearth-n (32) 15.1 channel-n (73) 9.2face-v (93) 15.1 match-v (42) 9.0green-a (94) 14.9 natural-a (103) 9.0fatigue-n (43) 14.9 serve-v (51) 8.8oblique-a (29) 14.3 train-v (63) 8.7nation-n (37) 14.0 post-n (79) 8.7church-n (64) 13.8 fine-a (70) 8.6local-a (38) 13.6 drift-v (32) 7.7fit-a (29) 13.4 leave-v (66) 7.7use-v (76) 13.4 play-v (66) 7.5child-n (64) 13.0 wash-v (12) 7.4wander-v (50) 12.9 keep-v (67) 7.4begin-v (280) 12.6 work-v (60) 7.0bum-n (45) 12.5 drive-v (42) 6.8feeling-n (51) 11.4 develop-v (69) 6.6facility-n (58) 11.1 carry-v (66) 6.3colorless (35) 11.1 see-v (69) 6.3grip-n (51) 11.1 strike-v (54) 5.9day-n (145) 11.0 call-v (66) 5.8mouth-n (60) 11.0 pull-v (60) 5.7material-n (69) 11.0 turn-v (67) 5.0art-n (98) 10.7 draw-v (41) 4.7find-v (68) 4.2Table 8: Difficulty of Spanish Word Typesword-pos (test) mean word-pos (test) meanclaro-a (66) 7.6 verde-a (33) 5.3local-a (55) 7.4 canal-n (41) 5.3popular-a (204) 7.1 clavar-v (44) 5.1partido-n (57) 7.0 masa-n (41) 5.1bomba-n (37) 6.8 apuntar-v (49) 4.9brillante-a (87) 6.7 autoridad-n (34) 4.9usar-v (56) 6.5 tocar-v (74) 4.8tabla-n (41) 6.3 explotar-v (41) 4.7vencer-v (65) 6.3 programa-n (47) 4.7simple-a (57) 6.2 circuito-n (49) 4.3hermano-n (57) 6.1 copiar-v (53) 4.3apoyar-v (73) 6.0 actuar-v (55) 4.2vital-a (79) 5.9 operacion-n (47) 4.2gracia-n (61) 5.9 pasaje-n (41) 4.1organo-n (81) 5.8 saltar-v (37) 4.1corona-n (40) 5.5 tratar-v (70) 3.9ciego-a (42) 5.5 natural-a (58) 3.9corazon-n (47) 5.5 grano-n (22) 3.9coronar-v (74) 5.4 conducir-v (54) 3.8naturaleza-n (56) 5.4systems.
However, the measure of optimal combi-nation is quite high, reaching 0.89 (1.00 - 0.11) forthe pair of alicante and jhu.
In fact, all seven of theother systems achieve their highest optimal combi-nation value when paired with alicante.This combination of circumstances suggests thatthe alicante system is fundamentally different thanthe other systems, and is able to disambiguate a cer-tain set of instances where the other systems fail.
Infact the alicante system is different in that it is theonly Spanish lexical sample system that makes useof the structure of Euro-WordNet, the source of thesense inventory.4 Instance DifficultyThe difficulty of disambiguating word senses canvary considerably.
A word with multiple closely re-lated senses is likely to be more difficult than onewith a few starkly drawn differences.
In supervisedlearning, a particular sense of a word can be diffi-cult to disambiguate if there are a small number oftraining examples available.Table 6 shows the distribution of the number ofinstances that are successfully disambiguated by aparticular number of systems in both the Englishand Spanish lexical samples.
The value under the# column shows the number of systems that are ableto disambiguate the number of noun, verb, adjec-tive and total instances shown in the row.
The aver-age number of training examples available for thecorrect answers associated with these instances isshown in parenthesis.
For example, the first lineshows that there were 59 noun instances that no sys-tem (of 23) could disambiguate, and that there wereon average 16 training examples available for eachof the correct senses for these 59 instances.Two very clear trends emerge.
First, there are asubstantial number of instances that are not disam-biguated correctly by any system (262 in English,228 in Spanish) and there are a large number of in-stances that are disambiguated by just a handful ofsystems.
In the English lexical sample, there are1,277 test instances that are correctly disambiguatedby five or fewer of the 23 systems.
This is nearly30% of the test data, and confirms that this was avery challenging set of test instances.There is also a very clear correlation between thenumber of training examples available for a particu-lar sense of a word and the number of systems thatare able to disambiguate instances of that word cor-rectly.
For example, Table 6 shows that there were174 English verb instances that no system disam-biguated correctly.
On average there were only 6training examples for the correct senses of these in-stances.
However, there were 28 instances that all23 English systems were able to disambiguate.
Forthese instances an average of 47 training exampleswere available for each correct sense.This correlation between instance difficulty andnumber of training examples may suggest that futureSENSEVAL exercises provide a minimum number oftraining examples for each sense, or adjust the scor-ing to reflect the difficulty of disambiguating a sensewith very few training examples.Finally, we assess the difficulty associated withword types by calculating the average number ofsystems that were able to disambiguate the instancesassociated with that type.
This information is pro-vided for the English and Spanish lexical samples inTables 7 and 8.
Each word is shown with its part ofspeech, the number of test instances, and the averagenumber of systems that were able to disambiguateeach of the test instances.The verb collaborate is the easiest according tothis metric in the English lexical sample.
It has 30test instances that were disambiguated correctly byan average of 20.2 of the 23 systems.
The verb findproves to be the most difficult, with 68 test instancesdisambiguated correctly by an average of 4.2 sys-tems.
A somewhat less extreme range of values isobserved for the Spanish lexical sample in Table 8.The adjective claro had 66 test instances that weredisambiguated correctly by an average of 7.6 of the8 systems.
The most difficult word was the verb con-ducir, which has 54 test instances that were disam-biguated correctly by an average of 3.8 systems.5 ConclusionThis paper presents an analysis of the results fromthe English and Spanish lexical sample tasks ofSENSEVAL-2.
The analysis is based on the kappastatistic and a measure known as optimal combina-tion.
It also assesses the difficulty of the test in-stances in these lexical samples.
We find that thereare a significant number of test instances that werenot disambiguated correctly by any system, and thatthere is some correlation between instance difficultyand the number of available training examples.6 AcknowledgmentsThis work has been partially supported by a NationalScience Foundation Faculty Early CAREER Devel-opment award (#0092784).ReferencesJ.
Carletta.
1996.
Assessing agreement on classificationtasks: the kappa statistic.
Computational Linguistics,22(2).J.
Cohen.
1960.
A coefficient of agreement for nominalscales.
Educ.
Psychol.
Meas., 20:37?46.K.
Krippendorf.
1980.
Content Analysis: An Introduc-tion to its Methodology.
Sage Publications, ThousandOaks, CA.T.
Pedersen.
2001.
Machine learning with lexical fea-tures: The Duluth approach to SENSEVAL-2.
In Pro-ceedings of the Senseval-2 Workshop, Toulouse, July.
