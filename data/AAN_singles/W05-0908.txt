Proceedings of the ACL Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translationand/or Summarization, pages 57?64, Ann Arbor, June 2005. c?2005 Association for Computational LinguisticsOn Some Pitfalls in Automatic Evaluation and Significance Testing for MTStefan Riezler and John T. Maxwell IIIPalo Alto Research Center3333 Coyote Hill Road, Palo Alto, CA 94304AbstractWe investigate some pitfalls regarding thediscriminatory power of MT evaluationmetrics and the accuracy of statistical sig-nificance tests.
In a discriminative rerank-ing experiment for phrase-based SMT weshow that the NIST metric is more sensi-tive than BLEU or F-score despite their in-corporation of aspects of fluency or mean-ing adequacy into MT evaluation.
In anexperimental comparison of two statisticalsignificance tests we show that p-valuesare estimated more conservatively by ap-proximate randomization than by boot-strap tests, thus increasing the likelihoodof type-I error for the latter.
We pointout a pitfall of randomly assessing signif-icance in multiple pairwise comparisons,and conclude with a recommendation tocombine NIST with approximate random-ization, at more stringent rejection levelsthan is currently standard.1 IntroductionRapid and accurate detection of result differences iscrucial in system development and system bench-marking.
In both situations a multitude of systemsor system variants has to be evaluated, so it is highlydesirable to employ automatic evaluation measuresfor detection of result differences, and statistical hy-pothesis tests to assess the significance of the de-tected differences.
When evaluating subtle differ-ences between system variants in development, orwhen benchmarking multiple systems, result differ-ences may be very small in magnitude.
This imposesstrong requirements on both automatic evaluationmeasures and statistical significance tests: Evalua-tion measures are needed that have high discrimi-native power and yet are sensitive to the interestingaspects of the evaluation task.
Significance tests arerequired to be powerful and yet accurate, i.e., if thereare significant differences they should be able to as-sess them, but not if there are none.In the area of statistical machine translation(SMT), recently a combination of the BLEU evalua-tion metric (Papineni et al, 2001) and the bootstrapmethod for statistical significance testing (Efron andTibshirani, 1993) has become popular (Och, 2003;Kumar and Byrne, 2004; Koehn, 2004b; Zhang etal., 2004).
Given the current practice of reportingresult differences as small as .3% in BLEU score,assessed at confidence levels as low as 70%, ques-tions arise concerning the sensitivity of the em-ployed evaluation metrics and the accuracy of theemployed significance tests, especially when resultdifferences are small.
We believe that is important toaccurately detect such small-magnitude differencesin order to understand how to improve systems andtechnologies, even though such differences may notmatter in current applications.In this paper we will investigate some pitfalls thatarise in automatic evaluation and statistical signifi-cance testing in MT research.
The first pitfall con-cerns the discriminatory power of automatic eval-uation measures.
In the following, we compare thesensitivity of three intrinsic evaluation measures thatdiffer with respect to their focus on different aspects57of translation.
We consider the well-known BLEUscore (Papineni et al, 2001) which emphasizes flu-ency by incorporating matches of high n-grams.
Fur-thermore, we consider an F-score measure that isadapted from dependency-based parsing (Crouch etal., 2002) and sentence-condensation (Riezler et al,2003).
This measure matches grammatical depen-dency relations of parses for system output and ref-erence translations, and thus emphasizes semanticaspects of translational adequacy.
As a third mea-sure we consider NIST (Doddington, 2002), whichfavors lexical choice over word order and does nottake structural information into account.
On an ex-perimental evaluation on a reranking experiment wefound that only NIST was sensitive enough to de-tect small result differences, whereas BLEU and F-score produced result differences that were statisti-cally not significant.
A second pitfall addressed inthis paper concerns the relation of power and ac-curacy of significance tests.
In situations where theemployed evaluation measure produces small resultdifferences, the most powerful significance test isdemanded to assess statistical significance of the re-sults.
However, accuracy of the assessments of sig-nificance is seldom questioned.
In the following,we will take a closer look at the bootstrap test andcompare it with the related technique of approxi-mate randomization (Noreen (1989)).
In an exper-imental evaluation on our reranking data we foundthat approximate randomization estimated p-valuesmore conservatively than the bootstrap, thus increas-ing the likelihood of type-I error for the latter test.Lastly, we point out a common mistake of randomlyassessing significance in multiple pairwise compar-isons (Cohen, 1995).
This is especially relevant ink-fold pairwise comparisons of systems or systemvariants where k is high.
Taking this multiplicityproblem into account, we conclude with a recom-mendation of a combination of NIST for evaluationand the approximate randomization test for signifi-cance testing, at more stringent rejection levels thanis currently standard in the MT literature.
This is es-pecially important in situations where multiple pair-wise comparisons are conducted, and small resultdifferences are expected.2 The Experimental Setup: DiscriminativeReranking for Phrase-Based SMTThe experimental setup we employed to compareevaluation measures and significance tests is a dis-criminative reranking experiment on 1000-best listsof a phrase-based SMT system.
Our system is are-implementation of the phrase-based system de-scribed in Koehn (2003), and uses publicly avail-able components for word alignment (Och and Ney,2003)1, decoding (Koehn, 2004a)2, language mod-eling (Stolcke, 2002)3 and finite-state processing(Knight and Al-Onaizan, 1999)4.
Training and testdata are taken from the Europarl parallel corpus(Koehn, 2002)5.Phrase-extraction follows Och et al (1999) andwas implemented by the authors: First, the wordaligner is applied in both translation directions, andthe intersection of the alignment matrices is built.Then, the alignment is extended by adding immedi-ately adjacent alignment points and alignment pointsthat align previously unaligned words.
From thismany-to-many alignment matrix, phrases are ex-tracted according to a contiguity requirement thatstates that words in the source phrase are alignedonly with words in the target phrase, and vice versa.Discriminative reranking on a 1000-best list oftranslations of the SMT system uses an `1 regu-larized log-linear model that combines a standardmaximum-entropy estimator with an efficient, in-cremental feature selection technique for `1 regu-larization (Riezler and Vasserman, 2004).
Trainingdata are defined as pairs {(sj , tj)}mj=1 of source sen-tences sj and gold-standard translations tj that aredetermined as the translations in the 1000-best listthat best match a given reference translation.
Theobjective function to be minimized is the conditionallog-likelihood L(?)
subject to a regularization termR(?
), where T (s) is the set of 1000-best translationsfor sentence s, ?
is a vector or log-parameters, and1http://www.fjoch.com/GIZA++.html2http://www.isi.edu/licensed-sw/pharaoh/3http://www.speech.sri.com/projects/srilm/4http://www.isi.edu/licensed-sw/carmel/5http://people.csail.mit.edu/people/koehn/publications/europarl/58Table 1: NIST, BLEU, F-scores for reranker and baseline on development setNIST BLEU Fbaseline 6.43 .301 .385reranking 6.58 .298 .383approxrand p-value < .0001 .158 .424bootstrap p-value < .0001 .1 -f is a vector of feature functions:L(?)
+ R(?)
= ?
logm?j=1p?
(tj |sj) + R(?
)= ?m?j=1loge?
?f(tj)?t?T (sj)e?
?f(t)+ R(?
)The features employed in our experiments con-sist of 8 features corresponding to system compo-nents (distortion model, language model, phrase-translations, lexical weights, phrase penalty, wordpenalty) as provided by PHARAOH, together with amultitude of overlapping phrase features.
For exam-ple, for a phrase-table of phrases consisting of max-imally 3 words, we allow all 3-word phrases and 2-word phrases as features.
Since bigram features canoverlap, information about trigrams can be gatheredby composing bigram features even if the actual tri-gram is not seen in the training data.Feature selection makes it possible to employ andevaluate a large number of features, without con-cerns about redundant or irrelevant features hamper-ing generalization performance.
The `1 regularizer isdefined by the weighted `1-norm of the parametersR(?)
= ?||?||1 = ?n?i=1|?i|where ?
is a regularization coefficient, and n is num-ber of parameters.
This regularizer penalizes overlylarge parameter values in their absolute values, andtends to force a subset of the parameters to be ex-actly zero at the optimum.
This fact leads to a naturalintegration of regularization into incremental featureselection as follows: Assuming a tendency of the `1regularizer to produce a large number of zero-valuedparameters at the function?s optimum, we start withall-zero weights, and incrementally add features tothe model only if adjusting their parameters awayfrom zero sufficiently decreases the optimization cri-terion.
Since every non-zero weight added to themodel incurs a regularizer penalty of ?|?i|, it onlymakes sense to add a feature to the model if thispenalty is outweighed by the reduction in negativelog-likelihood.
Thus features considered for selec-tion have to pass the following test:?????L(?)??i????
> ?This gradient test is applied to each feature and ateach step the features that pass the test with maxi-mum magnitude are added to the model.
This pro-vides both efficient and accurate estimation withlarge feature sets.Work on discriminative reranking has been re-ported before by Och and Ney (2002), Och et al(2004), and Shen et al (2004).
The main purpose ofour reranking experiments is to have a system thatcan easily be adjusted to yield system variants thatdiffer at controllable amounts.
For quick experimen-tal turnaround we selected the training and test datafrom sentences with 5 to 15 words, resulting in atraining set of 160,000 sentences, and a developmentset of 2,000 sentences.
The phrase-table employedwas restricted to phrases of maximally 3 words, re-sulting in 200,000 phrases.3 Detecting Small Result Differences byIntrinsic Evaluations MetricsThe intrinsic evaluation measures used in our ex-periments are the well-known BLEU (Papineni etal., 2001) and NIST (Doddington, 2002) metrics,and an F-score measure that adapts evaluation tech-niques from dependency-based parsing (Crouch etal., 2002) and sentence-condensation (Riezler et al,2003) to machine translation.
All of these measures59Set c = 0Compute actual statistic of score differences |SX ?
SY| on test dataFor random shuffles r = 0, .
.
.
, RFor sentences in test setShuffle variable tuples between system X and Y with probability 0.5Compute pseudo-statistic |SXr ?
SYr | on shuffled dataIf |SXr ?
SYr | ?
|SX ?
SY|c + +p = (c + 1)/(R + 1)Reject null hypothesis if p is less than or equal to specified rejection level.Figure 1: Approximate Randomization Test for Statistical Significance Testingevaluate document similarity of SMT output againstmanually created reference translations.
The mea-sures differ in their focus on different entities inmatching, corresponding to a focus on different as-pects of translation quality.BLEU and NIST both consider n-grams in sourceand reference strings as matching entities.
BLEUweighs all n-grams equally whereas NIST puts moreweight on n-grams that are more informative, i.e.,occur less frequently.
This results in BLEU favor-ing matches in larger n-grams, corresponding to giv-ing more credit to correct word order.
NIST weighslower n-grams more highly, thus it gives more creditto correct lexical choice than to word order.F-score is computed by parsing reference sen-tences and SMT outputs, and matching grammaticaldependency relations.
The reported value is the har-monic mean of precision and recall, which is definedas (2?
precision ?
recall )/( precision + recall ).Precision is the ratio of matching dependency re-lations to the total number of dependency relationsin the parse for the system translation, and recall isthe ratio of matches to the total number of depen-dency relations in the parse for the reference trans-lation.
The goal of this measure is to focus on as-pects of meaning in measuring similarity of systemtranslations to reference translations, and to allowfor meaning-preserving word order variation.Evaluation results for a comparison of rerank-ing against a baseline model that only includes fea-tures corresponding to the 8 system components areshown in Table 1.
Since the task is a comparisonof system variants for development, all results arereported on the development set of 2,000 exam-ples of length 5-15.
The reranking model achievesan increase in NIST score of .15 units, whereasBLEU and F-score decrease by .3% and .2% respec-tively.
However, as measured by the statistical sig-nificance tests described below, the differences inBLEU and F-scores are not statistically significantwith p-values exceeding the standard rejection levelof .05.
In contrast, the differences in NIST scoreare highly significant.
These findings correspond toresults reported in Zhang et al (2004) showing ahigher sensitivity of NIST versus BLEU to small re-sult differences.
Taking also the results from F-scorematching in account, we can conclude that similar-ity measures that are based on matching more com-plex entities (such as BLEU?s higher n-grams or F?sgrammatical relations) are not as sensitive to smallresult differences as scoring techniques that are ableto distinguish models by matching simpler entities(such as NIST?s focus on lexical choice).
Further-more, we get an indication that differences of .3%in BLEU score or .2% in F-score might not be largeenough to conclude statistical significance of resultdifferences.
This leads to questions of power and ac-curacy of the employed statistical significance testswhich will be addressed in the next section.4 Assessing Statistical Significance ofSmall Result DifferencesThe bootstrap method is an example for a computer-intensive statistical hypothesis test (see, e.g., Noreen(1989)).
Such tests are designed to assess resultdifferences with respect to a test statistic in caseswhere the sampling distribution of the test statistic60Set c = 0Compute actual statistic of score differences |SX ?
SY| on test dataCalculate sample mean ?B = 1B?Bb=0 |SXb ?
SYb | over bootstrap samples b = 0, .
.
.
, BFor bootstrap samples b = 0, .
.
.
, BSample with replacement from variable tuples for systems X and Y for test sentencesCompute pseudo-statistic |SXb ?
SYb | on bootstrap dataIf |SXb ?
SYb | ?
?B (+?)
?
|SX ?
SY|c + +p = (c + 1)/(B + 1)Reject null hypothesis if p is less than or equal to specified rejection level.Figure 2: Bootstrap Test for Statistical Significance Testingis unknown.
Comparative evaluations of outputs ofSMT systems according to test statistics such as dif-ferences in BLEU, NIST, or F-score are examplesof this situation.
The attractiveness of computer-intensive significance tests such as the bootstrapor the approximate randomization method lies intheir power and simplicity.
As noted in standardtextbooks such as Cohen (1995) or Noreen (1989)such tests are as powerful as parametric tests whenparametric assumptions are met and they outper-form them when parametric assumptions are vio-lated.
Because of their generality and simplicity theyare also attractive alternatives to conventional non-parametric tests (see, e.g., Siegel (1988)).
The powerof these tests lies in the fact that they answer only avery simple question without making too many as-sumptions that may not be met in the experimen-tal situation.
In case of the approximate random-ization test, only the question whether two sam-ples are related to each other is answered, with-out assuming that the samples are representative ofthe populations from which they were drawn.
Thebootstrap method makes exactly this one assump-tion.
This makes it formally possible to draw in-ferences about population parameters for the boot-strap, but not for approximate randomization.
How-ever, if the goal is to assess statistical significanceof a result difference between two systems the ap-proximate randomization test provides the desiredpower and accuracy whereas the bootstrap?s advan-tage to draw inferences about population parameterscomes at the price of reduced accuracy.
Noreen sum-marizes this shortcoming of the bootstrap techniqueas follows: ?The principal disadvantage of [the boot-strap] method is that the null hypothesis may be re-jected because the shape of the sampling distributionis not well-approximated by the shape of the boot-strap sampling distribution rather than because theexpected value of the test statistic differs from thevalue that is hypothesized.?
(Noreen (1989), p. 89).Below we describe these two test procedures in moredetail, and compare them in our experimental setup.4.1 Approximate RandomizationAn excellent introduction to the approximate ran-domization test is Noreen (1989).
Applications ofthis test to natural language processing problems canbe found in Chinchor et al (1993).In our case of assessing statistical significance ofresult differences between SMT systems, the teststatistic of interest is the absolute value of the differ-ence in BLEU, NIST, or F-scores produced by twosystems on the same test set.
These test statistics arecomputed by accumulating certain count variablesover the sentences in the test set.
For example, incase of BLEU and NIST, variables for the length ofreference translations and system translations, andfor n-gram matches and n-gram counts are accumu-lated over the test corpus.
In case of F-score, vari-able tuples consisting of the number of dependency-relations in the parse for the system translation, thenumber of dependency-relations in the parse for thereference translation, and the number of matchingdependency-relations between system and referenceparse, are accumulated over the test set.Under the null hypothesis, the compared systemsare not different, thus any variable tuple produced byone of the systems could have been produced just as61Table 2: NIST scores for equivalent systems under bootstrap and approximate randomization tests.compared systems 1:2 1:3 1:4 1:5 1:6NIST difference .031 .032 .029 .028 .036approxrand p-value .03 .025 .05 .079 .028bootstrap p-value .014 .013 .028 .039 .013likely by the other system.
So shuffling the variabletuples between the two systems with equal probabil-ity, and recomputing the test statistic, creates an ap-proximate distribution of the test statistic under thenull hypothesis.
For a test set of S sentences thereare 2S different ways to shuffle the variable tuplesbetween the two systems.
Approximate randomiza-tion produce shuffles by random assignments insteadof evaluating all 2S possible assignments.
Signifi-cance levels are computed as the percentage of trialswhere the pseudo statistic, i.e., the test statistic com-puted on the shuffled data, is greater than or equal tothe actual statistic, i.e., the test statistic computed onthe test data.
A sketch of an algorithm for approxi-mate randomization testing is given in Fig.
1.4.2 The BootstrapAn excellent introduction to the technique is thetextbook by Efron and Tibshirani (1993).
In contrastto approximate randomization, the bootstrap methodmakes the assumption that the sample is a repre-sentative ?proxy?
for the population.
The shape ofthe sampling distribution is estimated by repeatedlysampling (with replacement) from the sample itself.A sketch of a procedure for bootstrap testing isgiven in Fig.
2.
First, the test statistic is computed onthe test data.
Then, the sample mean of the pseudostatistic is computed on the bootstrapped data, i.e.,the test statistic is computed on bootstrap samplesof equal size and averaged over bootstrap samples.In order to compute significance levels based onthe bootstrap sampling distribution, we employ the?shift?
method described in Noreen (1989).
Here itis assumed that the sampling distribution of the nullhypothesis and the bootstrap sampling distributionhave the same shape but a different location.
Thelocation of the bootstrap sampling distribution isshifted so that it is centered over the location wherethe null hypothesis sampling distribution should becentered.
This is achieved by subtracting from eachvalue of the pseudo-statistic its expected value ?Band then adding back the expected value ?
of thetest statistic under the null hypothesis.
?B can be es-timated by the sample mean of the bootstrap sam-ples; ?
is 0 under the null hypothesis.
Then, similarto the approximate randomization test, significancelevels are computed as the percentage of trials wherethe (shifted) pseudo statistic is greater than or equalto the actual statistic.4.3 Power vs.
Type I ErrorsIn order to evaluate accuracy of the bootstrap and theapproximate randomization test, we conduct an ex-perimental evaluation of type-I errors of both boot-strap and approximate randomization on real data.Type-I errors indicate failures to reject the null hy-pothesis when it is true.
We construct SMT systemvariants that are essentially equal but produce su-perficially different results.
This can be achieved byconstructing reranking variants that differ in the re-dundant features that are included in the models, butare similar in the number and kind of selected fea-tures.
The results of this experiment are shown in Ta-ble 2.
System 1 does not include irrelevant features,whereas systems 2-6 were constructed by adding aslightly different number of features in each step,but resulted in the same number of selected features.Thus competing features bearing the same informa-tion are exchanged in different models, yet overallthe same information is conveyed by slightly dif-ferent feature sets.
The results of Table 2 show thatthe bootstrap method yields p-values < .015 in 3out of 5 pairwise comparisons whereas the approx-imate randomization test yields p-values ?
.025 inall cases.
Even if the true p-value is unknown, wecan say that the approximate randomization test es-timates p-values more conservatively than the boot-strap, thus increasing the likelihood of type-I errorfor the bootstrap test.
For a restrictive significancelevel of 0.15, which is motivated below for multiple62comparisons, the bootstrap would assess statisticalsignificance in 3 out of 5 cases whereas statisticalsignificance would not be assessed under approxi-mate randomization.
Assuming equivalence of thecompared system variants, these assessments wouldcount as type-I errors.4.4 The Multiplicity ProblemIn the experiment on type-I error described above, amore stringent rejection level than the usual .05 wasassumed.
This was necessary to circumvent a com-mon pitfall in significance testing for k-fold pairwisecomparisons.
Following the argumentation given inCohen (1995), the probability of randomly assess-ing statistical significance for result differences ink-fold pairwise comparisons grows exponentially ink.
Recall that for a pairwise comparison of systems,specifying that p < .05 means that the probability ofincorrectly rejecting the null hypothesis that the sys-tems are not different be less than .05.
Caution hasto be exercised in k-fold pairwise comparisons: Fora probability pc of incorrectly rejecting the null hy-pothesis in a specific pairwise comparison, the prob-ability pe of at least once incorrectly rejecting thisnull hypothesis in an experiment involving k pair-wise comparisons ispe ?
1?
(1?
pc)kFor large values of k, the probability of concludingresult differences incorrectly at least once is unde-sirably high.
For example, in benchmark testing of15 systems, 15(15 ?
1)/2 = 105 pairwise compar-isons will have to be conducted.
At a per-comparisonrejection level pc = .05 this results in an experi-mentwise error pe = .9954, i.e., the probability ofat least one spurious assessment of significance is1?
(1?
.05)105 = .9954.
One possibility to reducethe likelihood that one ore more of differences as-sessed in pairwise comparisons is spurious is to runthe comparisons at a more stringent per-comparisonrejection level.
Reducing the per-comparison rejec-tion level pc until an experimentwise error rate peof a standard value, e.g., .05, is achieved, will favorpe over pc.
In the example of 5 pairwise compar-isons described above, a per-comparison error ratepc = .015 was sufficient to achieve an experimen-twise error rate pe ?
.07.
In many cases this tech-nique would require to reduce pc to the point wherea result difference has to be unrealistically large tobe significant.
Here conventional tests for post-hoccomparisons such as the Scheffe?
or Tukey test haveto be employed (see Cohen (1995), p.
185ff.
).5 ConclusionSituations where a researcher has to deal with subtledifferences between systems are common in systemdevelopment and large benchmark tests.
We haveshown that it is useful in such situations to trade inexpressivity of evaluation measures for sensitivity.For MT evaluation this means that recording differ-ences in lexical choice by the NIST measure is moreuseful than failing to record differences by employ-ing measures such as BLEU or F-score that incorpo-rate aspects of fluency and meaning adequacy intoMT evaluation.
Similarly, in significance testing, itis useful to trade in the possibility to draw inferencesabout the sampling distribution for accuracy andpower of the test method.
We found experimentalevidence confirming textbook knowledge about re-duced accuracy of the bootstrap test compared to theapproximate randomization test.
Lastly, we pointedout a well-known problem of randomly assessingsignificance in multiple pairwise comparisons.
Tak-ing these findings together, we recommend for mul-tiple comparisons of subtle differences to combinethe NIST score for evaluation with the approximaterandomization test for significance testing, at morestringent rejection levels than is currently standardin the MT literature.ReferencesNancy Chinchor, Lynette Hirschman, and David D.Lewis.
1993.
Evaluating message understanding sys-tems: An analysis of the third message understand-ing conference (MUC-3).
Computational Linguistics,19(3):409?449.Paul R. Cohen.
1995.
Empirical Methods for ArtificialIntelligence.
The MIT Press, Cambridge, MA.Richard Crouch, Ronald M. Kaplan, Tracy H. King, andStefan Riezler.
2002.
A comparison of evaluationmetrics for a broad-coverage stochastic parser.
In Pro-ceedings of the ?Beyond PARSEVAL?
Workshop at the3rd International Conference on Language Resourcesand Evaluation (LREC?02), Las Palmas, Spain.George Doddington.
2002.
Automatic evaluation of ma-chine translation quality using n-gram co-occurrence63statistics.
In Proceedings of the ARPA Workshop onHuman Language Technology.Bradley Efron and Robert J. Tibshirani.
1993.
An In-troduction to the Bootstrap.
Chapman and Hall, NewYork.Kevin Knight and Yaser Al-Onaizan.
1999.
A primer onfinite-state software for natural language processing.Technical report, USC Information Sciences Institute,Marina del Rey, CA.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of the Human Language Technology Conferenceand the 3rd Meeting of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL?03), Edmonton, Cananda.Philipp Koehn.
2002.
Europarl: A multilingual corpusfor evaluation of machine translation.
Technical re-port, USC Information Sciences Institute, Marina delRey, CA.Philipp Koehn.
2004a.
PHARAOH.
a beam search de-coder for phrase-based statistical machine translationmodels.
user manual.
Technical report, USC Informa-tion Sciences Institute, Marina del Rey, CA.Philipp Koehn.
2004b.
Statistical significance tests formachine translation evaluation.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?04), Barcelona, Spain.Shankar Kumar and William Byrne.
2004.
MinimumBayes-risk decoding for statistical machine transla-tion.
In Proceedings of the Human Language Technol-ogy conference / North American chapter of the Asso-ciation for Computational Linguistics annual meeting(HLT/NAACL?04), Boston, MA.Eric W. Noreen.
1989.
Computer Intensive Methodsfor Testing Hypotheses.
An Introduction.
Wiley, NewYork.Franz Josef Och and Hermann Ney.
2002.
Discrimina-tive training and maximum entropy models for statis-tical machine translation.
In Proceedings of the 40thAnnual Meeting of the Association for ComputationalLinguistics (ACL?02), Philadelphia, PA.Franz Josef Och and Hermann Ney.
2003.
A system-atic comparison of various statistical alignment mod-els.
Computational Linguistics, 29(1):19?51.Franz Josef Och, Christoph Tillmann, and Hermann Ney.1999.
Improved alignment models for statistical ma-chine translation.
In Proceedings of the 1999 Confer-ence on Empirical Methods in Natural Language Pro-cessing (EMNLP?99).Franz Josef Och, Daniel Gildea, Sanjeev Khudanpur,Anoop Sarkar, Kenji Yamada, Alex Fraser, ShankarKumar, Libin Shen, David Smith, Ketherine Eng,Viren Jain, Zhen Jin, and Dragomir Radev.
2004.
Asmorgasbord of features for statistical machine transla-tion.
In Proceedings of the Human Language Technol-ogy conference / North American chapter of the Asso-ciation for Computational Linguistics annual meeting(HLT/NAACL?04), Boston, MA.Franz Josef Och.
2003.
Minimum error rate train-ing in statistical machine translation.
In Proceedingsof the Human Language Technology Conference andthe 3rd Meeting of the North American Chapter ofthe Association for Computational Linguistics (HLT-NAACL?03), Edmonton, Cananda.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2001.
Bleu: a method for automaticevaluation of machine translation.
Technical ReportIBM Research Division Technical Report, RC22176(W0190-022), Yorktown Heights, N.Y.Stefan Riezler and Alexander Vasserman.
2004.
Incre-mental feature selection and `1 regularization for re-laxed maximum-entropy modeling.
In Proceedings ofthe 2004 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP?04), Barcelona, Spain.Stefan Riezler, Tracy H. King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensationusing ambiguity packing and stochastic disambigua-tion methods for lexical-functional grammar.
In Pro-ceedings of the Human Language Technology Confer-ence and the 3rd Meeting of the North American Chap-ter of the Association for Computational Linguistics(HLT-NAACL?03), Edmonton, Cananda.Libin Shen, Anoop Sarkar, and Franz Josef Och.
2004.Discriminative reranking for machine translation.
InProceedings of the Human Language Technology con-ference / North American chapter of the Associa-tion for Computational Linguistics annual meeting(HLT/NAACL?04), Boston, MA.Sidney Siegel.
1988.
Nonparametric Statistics for theBehavioral Sciences.
Second Edition.
MacGraw-Hill,Boston, MA.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, Denver,CO.Ying Zhang, Stephan Vogel, and Alex Waibel.
2004.Interpreting BLEU/NIST scores: How much improve-ment do we need to have a better system?
In Proceed-ings of the 4th International Conference on LanguageResources and Evaluation (LREC?04), Lisbon, Portu-gal.64
