Semi-Automatic Practical Ontology Construction by Using aThesaurus, Computational Dictionaries, and Large CorporaSin-Jae Kang and Jong-Hyeok LeeDiv.
of Electrical and Computer Engineering, Pohang University of Science and TechnologySan 31 Hyoja-Dong, Nam-Gu, Pohang 790-784Republic of KOREAsjkang@postech.ac.kr, jhlee@postech.ac.krAbstractThis paper presents the semi-automaticconstruction method of a practicalontology by using various resources.
Inorder to acquire a reasonably practicalontology in a limited time and with lessmanpower, we extend the Kadokawathesaurus by inserting additionalsemantic relations into its hierarchy,which are classified as case relationsand other semantic relations.
Theformer can be obtained by convertingvalency information and case framesfrom previously-built computationaldictionaries used in machine translation.The latter can be acquired from conceptco-occurrence information, which isextracted automatically from largecorpora.
The ontology stores richsemantic constraints among 1,110concepts, and enables a naturallanguage processing system to resolvesemantic ambiguities by makinginferences with the concept network ofthe ontology.
In our practical machinetranslation system, our ontology-basedword sense disambiguation methodachieved an 8.7% improvement overmethods which do not use an ontologyfor Korean translation.1 IntroductionAn ontology is a knowledge base withinformation about concepts existing in the worldor domain, their properties, and how they relateto each other.
The principal reasons to use anontology in machine translation (MT) are toenable source language analyzers and targetlanguage generators to share knowledge, to storesemantic constraints, and to resolve semanticambiguities by making inferences using theconcept network of the ontology (Mahesh, 1996;Nirenburg et al, 1992).
An ontology is differentfrom a thesaurus in that it contains onlylanguage independent information and manyother semantic relations, as well as taxonomicrelations.In general, to build a high-quality semanticknowledge base, manual processing isindispensable.
Previous attempts were mostlyperformed manually, or were developed withoutconsidering the context of a practical situation(Mahesh, 1996; Lenat et al, 1990).
Therefore, itis difficult to construct a practical ontology withlimited time and manpower resources.
To solvethis problem, we propose a semi-automaticontology construction method, which takes fulladvantage of already existing knowledgeresources and practical usages in large corpora.First, we define our ontology representationlanguage (ORL) by modifying the most suitableamong previously developed ORLs, and thendesign a language-independent and practical(LIP) ontology structure based on the definedORL.
Afterwards, we construct a practicalontology by the semi-automatic constructionmethod given below.We extend the existing Kadokawa thesaurus(Ohno & Hamanishi, 1981) by insertingadditional semantic relations into the hierarchyof the thesaurus.
Uramoto (1996) and Tokunaga(1997) propose thesaurus extension methods forpositioning unknown words in an existingthesaurus.
Our approach differs in that theobjects inserted are not words but semanticrelations.Additional semantic relations can beclassified as case relations and other semanticrelations.
The former can be obtained byconverting the established valency informationin bilingual dictionaries of COBALT-J/K(Collocation-Based Language Translator fromJapanese to Korean) and COBALT-K/J(Collocation-Based Language Translator fromKorean to Japanese) (Moon & Lee, 2000)  MTsystems, as well as from the case frame in theSejong electronic dictionary1.
The latter can beacquired from concept co-occurrenceinformation, which is extracted automaticallyfrom a corpus (Li et al, 2000).The remainder of this paper is organized asfollows.
We describe the principles of ontologydesign and an ORL used to represent our LIPontology in the next section.
In Section 3, wedescribe the semi-automatic ontologyconstruction methodology in detail.
Anontology-based word sense disambiguation(WSD) algorithm is given in Section 4.Experimental results are presented and analyzedin Section 5.
Finally, we make a conclusion andindicate the direction of our future work inSection 6.2 Ontology Design2.1    Basic PrinciplesAlthough no formal principles exist to determinethe structure or content of our ontology, we cansuggest some principles underlying ourmethodology.
Firstly, an ontology for naturallanguage processing (NLP) must provideconcepts for representing word meanings in thelexicon and store selectional constraints ofconcepts, which enable inferences using thenetwork of an ontology (Onyshkevych, 1997).These inferences can assist in metaphor andmetonymy processing, as well as word sensedisambiguation.
For these reasons, an ontologybecomes an essential knowledge source for highquality NLP, although it is difficult and time-consuming to construct.
Secondly, an ontologycan be effortlessly shared by any application andin any domain (Gruber, 1993; Karp et al, 1999;Kent, 1999).
More than two different ontologiesin a certain domain can produce a semanticmismatch problem between concepts.
Further, if1 The Sejong electronic dictionary has been developed byseveral Korean linguistic researchers, funded by Ministryof Culture and Tourism, Republic of Korea.
(http://www.sejong.or.kr)you wish to apply an existing ontology to a newapplication, it will often be necessary to convertthe structure of the ontology to a new one.Thirdly, an ontology must support languageindependent features, because constructingontologies for each language is inefficient.Fourthly, an ontology must have capabilities forusers to easily understand, search, and browse.Therefore, we define a suitable ORL to supportthese principles.2.2    Ontology Representation LanguageMany knowledge representation languages arebuilt specifically to share knowledge amongdifferent knowledge representation systems.Five types of ORLs were reviewed, such asFRAMEKIT (Nirenburg et al, 1992),Ontolingua (Gruber, 1993), CycL (Lenat et al,1990), XOL (Karp et al, 1999), and OntologyMarkup Language (OML) (Kent, 1999).According to their semantics, FRAMEKIT andXOL adopt frame representation, CycL andOntolingua use an extended first order predicatecalculus, and the OML is based on conceptualgraphs (CGs).
Excepting FRAMEKIT and CycL,the other ORLs have not yet been applied tobuild any large-scale ontology.Among this variety of ORLs, we chose thesimplified OML as the ORL of our LIP ontology,which is based on Extensible Markup Language(XML) and CGs.
Since XML has a well-established syntax, it is reasonably simple toparse, and XML will be widely used, because ithas many software tools for parsing andmanipulating, and a human readablerepresentation.
We intend to leave room forimprovement by adopting the semantics of CGs,because the present design of our LIP ontologyis for the specific purpose of disambiguatingword senses.
In future, however, we must extendits structure and content to build an interlingualmeaning representation during semantic analysisin machine translation.
Sowa's CGs (1984) is awidely-used knowledge representation language,consisting of logic structures with a graphnotation and several features integrated fromsemantic net and frame representation.
Globally,many research teams are working on theextension and application of CGs in manydomains.3 Ontology ConstructionMany ontologies are developed for purelytheoretical purposes, or are constructed aslanguage-dependent computational resources,such as WordNet and EDR.
However, they areseldom constructed as a language-independentcomputational resource.To construct a language-independent andpractical ontology, we developed two strategies.First, we introduced the same number and grainsize of concepts of the Kadokawa thesaurus andits taxonomic hierarchy into the LIP ontology.The thesaurus has 1,110 Kadokawa semanticcategories and a 4-level hierarchy as ataxonomic relation (see Figure 1).
This approachis a moderate shortcut to construct a practicalontology which easily enables us to utilize itsresults, since some resources are readilyavailable, such as bilingual dictionaries ofCOBALT-J/K and COBALT-K/J.
In thesebilingual dictionaries, nominal and verbal wordsare already annotated with concept codes fromthe Kadokawa thesaurus.
By using the samesense inventories of these MT systems, we caneasily apply and evaluate our LIP ontologywithout additional lexicographic works.
Inaddition, the Kadokawa thesaurus has proven tobe useful for providing a fundamentalfoundation to build lexical disambiguationknowledge in COBALT-J/K and COBALT-K/JMT systems (Li et al, 2000).The second strategy to construct a practicalontology is to extend the hierarchy of theKadokawa thesaurus by inserting additionalsemantic relations into its hierarchy.
Theadditional semantic relations can be classified ascase relations and other semantic relations.
Thusfar, case relations have been used occasionallyto disambiguate lexical ambiguities in the formof valency information and case frame, but othersemantic relations have not, because of theproblem of discriminating them from each other,making them difficult to recognize.
We define atotal of 30 semantic relation types for WSD byreferring mainly to the Sejong electronicdictionary and the Mikrokosmos ontology(Mahesh, 1996), as shown in Table 1.
Thesesemantic relation types cannot express allpossible semantic relations existing amongconcepts, but experimental results demonstratedtheir usefulness for WSD.Two approaches are used to obtain theseadditional semantic relations, which will beinserted into the LIP ontology.
The first importsrelevant semantic information from existingcomputational dictionaries.
The second appliesthe semi-automatic corpus analysis method (Liet al, 2000).
Both approaches are explained inSection 3.1 and 3.2, respectively.Figure 2 displays the overall constructingflow of the LIP ontology.
First, we build aninitial LIP ontology by importing the existingKadokawa thesaurus.
Each concept inserted intothe initial ontology has a Kadokawa code, aKorean name, an English name, a timestamp,and a concept definition.
Although concepts canbe uniquely identified by the Kadokawa conceptcodes, their Korean and English names areRoot conceptnature    character  change action     feeling     human  disposition  society institute things0           1   2   3   4      5     6     7       8       9astro- calen- wea- geog- sight plant  ani- physi- subs- pheno-nomy    dar    ther  raphy                   mal  ology tance  mena00      01       02     03     04       05     06     07     08 09goods drugs food clothes buil- furni- statio- mark tools mach-ding   ture    nary                       ine90      91       92     93     94       95     96     97     98 99orga- ani- fish insect organ foot&  sin- intes- egg   sexnism  mal                               tail   ews   tine060    061  062   063   064    065   066   067   068   069supp- writing- count- binder  toy  doll  recreation- sports- music- belllies      tool      book                             thing    tool  instrument960     961      962     963   964  965     966           967 968       969?????????
?Figure 1.
Concept hierarchy of the KadokawathesaurusTable 1.
Sematic relation types in the LIPontologyTypes Relation ListsTaxonomicrelationis-aCase relation agent, theme, experiencer,accompanier, instrument,location, source, destination,reason, appraisee, criterion,degree, recipientOtherSemanticrelationhas-member, has-element,contains, material-of, headed-by, operated-by,controls, owner-of, represents,symbol-of, name-of,producer-of, composer-of,inventor-of, make, measured-ininserted for the readability and convenience ofthe ontology developer.3.1    Dictionary Resources UtilizationCase relations between concepts can beprimarily derived from semantic information inthe Sejong electronic dictionary 2  and thebilingual dictionaries of MT systems, which areCOBALT-J/K and COBALT-K/J.We obtained 7,526 case frames from verb andadjective sub-dictionaries, which contain 3,848entries.
Automatically converting lexical wordsin the case frame into the Kadokawa conceptcodes by using COBALT-K/J (see Figure 33),we extracted a total of 6,224 case relationinstances.The bilingual dictionaries, which contain20,580 verb and adjective entries, have 16,567instances of valency information.
Semi-automatically converting syntactic relations intosemantic relations by using specific rules andhuman intuition (see Figure 4), we generated15,956 case relation instances.
The specific rules,as shown in Figure 5, are inferred from trainingsamples, which are explained in Section 4.1.These obtained instances may overlap eachother, but all instances are inserted only onceinto the initial LIP ontology.2 The Sejong electronic dictionary has sub-dictionaries,such as noun, verb, pronoun, adverb, and others.3 The Yale Romanization is used to represent Koreanlexical words.3.2    Corpus AnalysisFor the automatic construction of a sense-taggedcorpus, we used the COBALT-J/K, which is ahigh-quality practical MT system developed byPOSTECH in 1996.
The entire system has beenused successfully at POSCO (Pohang Iron andSteel Company), Korea, to translate patentmaterials on iron and steel subjects.
Weperformed a slight modification on COBALT-J/K so that it can produce Korean translationsfrom Japanese texts with all nominal and verbalwords tagged with the specific concept codes ofthe Kadokawa thesaurus.
As a result, a Koreansense-tagged corpus, which has two hundred andfifty thousand sentences, can be obtained fromJapanese texts.
Unlike English, the Koreanlanguage has almost no syntactic constraints onword order as long as a verb appears in the finalposition.
So we defined 12 local syntacticpatterns (LSPs) using syntactically-relatedwords in a sentence.
Frequently co-occurringwords in a sentence have no syntactic relationsto homographs but may control their meaning.Such words are retrieved as unordered co-occurring words (UCWs).
Case relations areobtained from LSPs, and other semanticCase framesCilmwunha-ta (question) agent Salam(person)Cungkasikh-ta (increase) theme Kakyek (price)Kyeyhoykha-ta (plan) theme Pepan (bill)Seywu-ta (construct) theme Saep (business)346 agent 5743 theme 171344 theme 419394 theme 369Case relationsFigure 3.
Example of conversion from caseframes in the Sejong dictionary381 (exercise) ul/lul (object) 449 (right)071 (live) wa/kwa (adverb) 06|05|5 (living thing)217 (soar) lo/ulo (adverb) 002 (sky)712 (join) i/ka (subject) 5 (person)381 theme 449071 accompanier 06|05|5217 destination 002712 agent 5Valency informationCase relationsFigure 4.
Example of conversion fromvalency information in the bilingualdictionariesStep 1: From KadokawathesaurusStep 2: From existingcomputational dictionariesLIP OntologyKadokawaThesaurus??
?Importconcepts& taxonomicrelationsSejongElectronic Dic.
(Case frame)K-J & J-KBilingual Dic.
(Valency Info.
)ImportcaserelationsSemi-AutomaticRelations or WordsMappingStep 3: From a corpusJapanese Raw CorpusCOBALT J/KJapanese-to-Korean TranslationSense TaggedKorean CorpusPartial Parsing& Statistical ProcessingGeneralized ConceptCo-occurrence InformationImport case& other semanticrelationsSemi-AutomaticRelations MappingFigure 2.
Ovreall constructing flow of the LIPontologyrelations are acquired from UCWs.
Concept co-occurrence information (CCI), which iscomposed of LSPs and UCWs, can be extractedby partial parsing and scanning.
To select themost probable concept types, Shannon's entropymodel is adopted to define the noise of a concepttype to discriminate the homograph.
Although itprocesses for concept type discrimination, manyco-occurring concept types, which must befurther selected, remain in each LSP and UCW.To solve this problem, some statisticalprocessing was automatically applied (Li et al,2000).
Finally, manual processing wasperformed to generate the ontological relationinstances from the generalized CCI, similar tothe previous valency information.
The resultsobtained include approximately about 3,701case relations and 1,650 other semantic relationsfrom 9,245 CCI, along with their frequencies.The obtained instances are inserted into theinitial LIP ontology.
Table 2 shows the numberof relation instances imported into the LIPontology from the Kadokawa thesaurus,computational dictionaries, and a corpus.4 Ontology ApplicationThe LIP ontology is applicable to many NLPapplications.
In this paper, we propose to use theontology to disambiguate word senses.
Allapproaches to WSD make use of words in asentence to mutually disambiguate each other.The distinctions between various approaches liein the source and type of knowledge made bythe lexical units in a sentence.Our WSD approach is a hybrid method,which combines the advantages of corpus-basedand knowledge-based methods.
We use the LIPontology as an external knowledge source andsecured dictionary information as contextinformation.
Figure 6 shows our overall WSDalgorithm.
First, we apply the previously-secured dictionary information to select correctsenses of some ambiguous words with highprecision, and then use the LIP ontology todisambiguate the remaining ambiguous words.The following are detailed descriptions of theprocedure for applying the LIP ontology toWSD work.4.1    Measure of Concept AssociationTo measure concept association, we use anassociation ratio based on the informationtheoretic concept of mutual information (MI),which is a natural measure of the dependence071 (life)072 (upbringing)073 (disease)YESNOagent / theme295 (influence)370 (giving & receiving)YESNOrecipient49 (joy & sorrow)62 (figure)YESNOexperiencer201 (stable)202 (vibration)1 (natural condition)07 (physiology)YESNOtheme2 (change)3 (action)YESNOagentManual mapping by human intuitionFigure 5.
Example of subject relationmapping rules with governer concept codesApply secured dictionary information with high precisionVerb?s valency informationSuccess?Local syntactic patternsYESNOSuccess?
YESNOUnordered co-occurring words patternsSuccess?Infer with the LIP ontologyYESNOSuccess?AnswerYESNOSet the answer to the most frequently appearing senseFigure 6.
The proposed WSD algorithmTable 2.
Imported relation instancesTypes NumberTaxonomic relations 1,100Case relations 19,459Other semantic relations 1,650Total 22,209between random variables (Church & Hanks,1989).
Resnik (1995) suggested a measure ofsemantic similarity in an IS-A taxonomy, basedon the notion of information content.
However,his method differs from ours in that we considerall semantic relations in the ontology, nottaxonomy relations only.
To implement this idea,we bind source concepts (SC) and semanticrelations (SR) into one entity, since SR is mainlyinfluenced by SC, not the destination concepts(DC).
Therefore, if two entities, < SC, SR>, andDC have probabilities P(<SC, SR>) and P(DC),then their mutual information I(<SC, SR>, DC)is defined as:????????
+><><=>< 1)(),(),,(log),,( 2 DCPSRSCPDCSRSCPDCSRSCIThe MI between concepts in the LIP ontologymust be calculated before using the ontology asknowledge for disambiguating word senses.Figure 7 shows the construction process fortraining data in the form of <SC (governer), SR,DC (dependent), frequency> and the calculationof MI between the LIP ontology concepts.
Weperformed a slight modification on COBALT-K/J and COBALT-J/K to enable them toproduce sense-tagged valency informationinstances with the specific concept codes of theKadokawa thesaurus.
After producing theinstances, we converted syntactic relations intosemantic relations using the specific rules (seeFigure 5) and human intuition.
As a result, weextracted sufficient training data from theKorean raw corpus: KIBS (Korean InformationBase System, '94-'97) is a large-scale corpus of70 million words, and the Japanese raw corpus,which has eight hundred and ten thousandsentences.
During this process, more specificsemantic relation instances are obtained whencompared with previous instances obtained inSection 3.
Since such specific instances reflectthe context of a practical situation, they are alsoimported into the LIP ontology.
Table 3 showsthe final number of semantic relations insertedinto the LIP ontology.Table 3.
Final relation instances in the LIPontologyTypes NumberTaxonomic relations 1,100Case relations 112,746Other semantic relations 2,093Total 115,9394.2    Locate the Least Weighted Path fromOne Ontology Concept to Other ConceptIf we regard MI as a weight between ontologyconcepts, we can treat the LIP ontology as agraph with weighted edges.
All edge weights arenon-negative and weights are converted intopenalties by the below formula Pe.
c indicate aconstant, maximum MI between concepts of theLIP ontology.
),,(),,( DCSRSCIcDCSRSCPe ><?=><So we use the formula below to locate theleast weighted path from one concept to theother concept.
The score function S is definedas:( )( )???????????????+????><==?
?.,),(),(min,),,(min,1),(}{jRkjijkkiCCCjRijijpipjijiCCandCCifCCSCCSCCandCCifCRCPeCCifCCSpjikpHere C and R indicate concepts and semanticrelations, respectively.
By applying this formula,we can verify how well selectional constraintsbetween concepts are satisfied.
In addition, ifthere is no direct semantic relation betweenconcepts, this formula provides a relaxationprocedure, which enables it to approximate theirsemantic relations.
This characteristic enables usApply valency information with high precisionJapaneseRaw CorpusCOBALT J/KJapanese-to-KoreanTranslationApplied ValencyPatterns<SC, synRel, DC, frequency>Semi-Automatic Relation MappingCalculating MIbtw <SC, SR> & DCSemantic Relation Instances<SC, SR, DC, frequency>KoreanRaw CorpusCOBALT K/JKorean-to-JapaneseTranslationLIP OntologyImporting semanticrelation instancesFigure 7.
Construction flow of ontologytraining datato obtain hints toward resolving metaphor andmetonymy expressions.
For example, whenthere is no direct semantic relation betweenconcepts such as ?school?
and ?inform,?
theinferring process is as follows.
The concept?school?
is a ?facility?, and the ?facility?
has?social human?
as its members.
The concept?inform?
has ?social human?
as its agent.
Figure8 presents an example of the best path betweenthese concepts, which is shown with bold lines.To locate the best path, the search mechanism ofour LIP ontology applies heuristics as follows.Firstly, a taxonomic relation must be treated asexceptional from other semantic relations,because they inherently lack frequenciesbetween parent and child concepts.
So we assigna fixed weight to those edges experimentally.Secondly, the weight given to an edge issensitive to the context of prior edges in the path.Therefore, our mechanism restricts the numberof times that a particular relation can betraversed in one path.
Thirdly, this mechanismavoids an excessive change in the gradient.5 Experimental EvaluationFor experimental evaluation, eight ambiguousKorean nouns were selected, along with a totalof 404 test sentences in which one of thehomographs appears.
The test sentences wererandomly selected from the KIBS.
Out ofseveral senses for each ambiguous word, weconsidered only two senses that are mostfrequently used in the corpus.
We performedthree experiments: The first experiment, BASE,is the case where the most frequently usedsenses are always taken as the senses of testwords.
The purpose of this experiment is toshow the baseline for WSD work.
The second,PTN, uses only secured dictionary information,such as the selectional restriction of verbs, localsyntactic patterns, and unordered co-occurringword patterns in disambiguating word senses.This is a general method without an ontology.The third, LIP, shows the results of our WSDmethod using the LIP ontology.
Theexperimental results are shown in Table 4.
Inthese experiments, the LIP method achieved an8.7% improvement over the PTN method forKorean analysis.
The main reason for theseresults is that, in the absence of secureddictionary information (see Figure 7) about anambiguous word, the ontology provides ageneralized case frame (i.e.
semantic restriction)by the concept code of the word.
In addition,when there is no direct semantic restrictionbetween concepts, our search mechanismprovides a relaxation procedure (see Figure 8).Therefore, the quality and usefulness of the LIPontology were proved indirectly by these results.6 ConclusionIn this paper we have proposed a semi-automaticconstruction method of the LIP ontology and anontology-based WSD algorithm.
The LIPTable 4.
Experimental results of WSD (%)Homograph Sense BASE PTN LIPfather &child Pwucarich man76.9 69.2 86.0liver Kancangsoy sauce67.3 87.8 91.8houseworkKasa words ofsong48.1 88.5 96.1shoeKwutwu word ofmouth79.6 85.7 95.9eye Nwunsnow82.0 96.0 92.0courage Yongkicontainer62.0 74.0 82.0expenses Kyengpidefense74.5 78.4 90.2times Kyeongkimatch52.9 80.4 95.6Average Precision 67.9 82.5 91.2RootConceptnature0character1change2action3feeling4human5disposition6society7institute8things9school722facility72is-ais-ais-asocialhuman507person50is-ais-a hasmemberinform750report75is-ais-aagent?????
?Figure 8.
Example of the best path betweenconcepts ?school?
and ?inform?
in the LIPontologyontology includes substantial semantic relationsbetween concepts, and differs from many of theresources in that there is no language-dependentknowledge in the resource, which is a networkof concepts, not words.
Semantic relations of theLIP ontology are generated by considering twodifferent languages, Korean and Japanese.
Inaddition, we can easily apply the ontologywithout additional lexicographic works, sincelarge-scale bilingual dictionaries have wordsalready annotated with concept codes of the LIPontology.
Therefore, our LIP ontology is alanguage independent and practical knowledgebase.
You can apply this ontology for otherlanguages, if one merely inserts Kadokawaconcept codes for each entry into the dictionary.Our ontology construction method requiresmanual processing, i.e., mapping from syntacticrelations to semantic relations by specific rulesand human intuition.
However, this is necessaryfor building a high-quality semantic knowledgebase.
Our construction method is quite effectivein comparison with other methods.We plan further research on how toeffectively divide the grain size of ontologyconcepts to best express the whole worldknowledge, and how to utilize the LIP ontologyin a full semantic analysis process.AcknowledgementsThe authors would like to thank the Ministry ofEducation of Korea for its financial supporttoward the Electrical and Computer EngineeringDivision at POSTECH through its BK21program.ReferencesChurch, K. and P. Hanks.
1989.
Word associationnorms, mutual information, and lexicography.
InProceedings of the 27th Annual Meeting of theAssociation for Computational Linguistics, pages76-83, Vancouver, Canada.Gruber, Thomas R. 1993.
A Translation Approach toPortable Ontology Specification.
KnowledgeAcquisition 5(2):199-220.Karp, P. D., V. K. Chaudhri, and J. F. Thomere.
1999.XOL: An XML-Based Ontology ExchangeLanguage.
Technical Note 559, AI Center, SRIInternational, July.Kent, Robert E. 1999.
Conceptual KnowledgeMarkup Language: The Central Core.
In theElectronic Proceedings of the Twelfth Workshopon Knowledge Acquisition, Modeling andManagement(KAW`99).
Banff, Alberta, Canada,October.Lenat, D. B. et al 1990.
Cyc: toward programs withcommon sense.
Communications of the ACM,33(8):30-49.Li, Hui-Feng et al 2000.
Lexical Transfer AmbiguityResolution Using Automatically-ExtractedConcept Co-occurrence Information.
InternationalJournal of Computer Processing of OrientalLanguages, World Scientific Pub., 13(1):53-68.Mahesh, Kavi.
1996.
Ontology Development forMachine Translation: Ideology and Methodology.Technical Report MCCS 96-292, ComputingResearch Laboratory, New Mexico StateUniversity, Las Cruces, NM.Moon, Kyunghi and Jong-Hyeok Lee.
2000.Representation and Recognition Method forMulti-Word Translation Units in Korean-to-Japanese MT System.
COLING 2000, pages 544-550, Germany.Nirenburg, Sergei, Jaime Carbonell, Masaru Tomita,and Kenneth Goodman.
1992.
MachineTranslation: A Knowledge-Based Approach,Morgan Kaufmann Pub., San Mateo, California.Ohno, S. and M. Hamanishi.
1981.
New SynonymsDictionary, Kadogawa Shoten, Tokyo.
(Written inJapanese).Onyshkevych, Boyan A.
1997.
An Ontological-Semantic Framework for Text Analysis.
Ph.D.dissertation, Program in Language andInformation Technologies, School of ComputerScience, Carnegie Mellon University, CMU-LTI-97-148.Resnik, Philip.
1995.
Using Information Content toEvaluate Semantic Similarity in a Taxonomy.
InProceedings of IJCAI-95, 1995, pages 448-453,Montreal, Canada.Sowa, John F. 1984.
Conceptual Structures:Information Processing in Mind and Machine,Addison-Wesley Pub., MA.Takenobu, Tokunaga et al 1997.
Extending athesaurus by classifying words.
In Proceedings ofthe ACL-EACL Workshop on AutomaticInformation Extraction and Building of LexicalSemantic Resources, pages 16-21, Madrid, Spain.Uramoto, Naohiko.
1996.
Positioning UnknownWord in a Thesaurus by using InformationExtracted from a Corpus.
In Proceedings ofCOLING-96, pages 956-961, Copenhagen,Denmark.
