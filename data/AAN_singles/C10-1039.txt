Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 340?348,Beijing, August 2010Opinosis: A Graph-Based Approach to Abstractive Summarization ofHighly Redundant OpinionsKavita Ganesan and ChengXiang Zhai and Jiawei HanDepartment of Computer ScienceUniversity of Illinois at Urbana-Champaign{kganes2,czhai,hanj}@cs.uiuc.eduAbstractWe present a novel graph-based summa-rization framework (Opinosis) that generatesconcise abstractive summaries of highly re-dundant opinions.
Evaluation results on sum-marizing user reviews show that Opinosissummaries have better agreement with hu-man summaries compared to the baseline ex-tractive method.
The summaries are readable,reasonably well-formed and are informativeenough to convey the major opinions.1 IntroductionSummarization is critically needed to help usersbetter digest the large amounts of opinions ex-pressed on the web.
Most existing work in Opin-ion Summarization focus on predicting sentimentorientation on an entity (Pang et al, 2002) (Pangand Lee, 2004) or attempt to generate aspect-basedratings for that entity (Snyder and Barzilay, 2007)(Lu et al, 2009)(Lerman et al, 2009)(Titov andMcdonald, 2008).
Such summaries are very infor-mative, but it is still hard for a user to understandwhy an aspect received a particular rating, forcinga user to read many, often highly redundant sen-tences about each aspect.
To help users further di-gest the opinions in each aspect, it is thus desirableto generate a concise textual summary of such re-dundant opinions.Indeed, in many scenarios, we will face theproblem of summarizing a large number of highlyredundant opinions; other examples include sum-marizing the ?tweets?
on Twitter or commentsmade about a blog or news article.
Due to the sub-tle variations of redundant opinions, typical extrac-tive methods are often inadequate for summarizingsuch opinions.
Consider the following sentences:1.
The iPhone?s battery lasts long, only had tocharge it once every few days.2.
iPhone?s battery is bulky but it is cheap..3. iPhone?s battery is bulky but it lasts long!With extractive summarization, no matter whichsingle sentence of the three is chosen as a sum-mary, the generated summary would be biased.In such a case, an abstractive summary such as?iPhone?s battery is cheap, lasts long but is bulky?is a more complete summary, conveying all thenecessary information.
Extractive methods alsotend to be verbose and this is especially problem-atic when the summaries need to be viewed onsmaller screens like on a PDA.
Thus, an informa-tive and concise abstractive summary would be abetter solution.Unfortunately, abstractive summarization isknown to be difficult.
Existing work in abstractivesummarization has been quite limited and can becategorized into two categories: (1) approaches us-ing prior knowledge (Radev and McKeown, 1998)(Finley and Harabagiu, 2002) (DeJong, 1982) and(2) approaches using Natural Language Genera-tion (NLG) systems (Saggion and Lapalme, 2002)(Jing and McKeown, 2000).
The first line of workrequires considerable amount of manual effort todefine schemas such as frames and templates thatcan be filled with the use of information extractiontechniques.
These systems were mainly used tosummarize news articles.
The second category ofwork uses deeper NLP analysis with special tech-niques for text regeneration.
Both approaches ei-ther heavily rely on manual effort or are domaindependent.In this paper, we propose a novel flexible sum-marization framework, Opinosis, that uses graphsto produce abstractive summaries of highly redun-dant opinions.
In contrast with the previous work,Opinosis assumes no domain knowledge and usesshallow NLP, leveraging mostly the word order inthe existing text and its inherent redundancies togenerate informative abstractive summaries.
Thekey idea of Opinosis is to first construct a tex-tual graph that represents the text to be summa-rized.
Then, three unique properties of this graphare used to explore and score various subpathsthat help in generating candidate abstractive sum-maries.Evaluation results on a set of user reviews showthat Opinosis summaries have reasonable agree-ment with human summaries.
Also, the gener-340ated summaries are readable, concise and fairlywell-formed.
Since Opinosis assumes no do-main knowledge and is highly flexible, it canbe potentially used to summarize any highly re-dundant content and could even be ported toother languages.
(All materials related to thiswork including the dataset and demo software canbe found at http://timan.cs.uiuc.edu/downloads.html.
)2 Opinosis-GraphOur key idea is to use a graph data structure (calledOpinosis-Graph) to represent natural language textand cast this abstractive summarization problemas one of finding appropriate paths in the graph.Graphs have been commonly used for extractivesummarization (e.g., LexRank (Erkan and Radev,2004) and TextRank (Mihalcea and Tarau, 2004)),but in these works the graph is often undirectedwith sentences as nodes and similarity as edges.Our graph data structure is different in that eachnode represents a word unit with directed edgesrepresenting the structure of sentences.
Moreover,we also attach positional information to nodes aswill be discussed later.Algorithm 1 (A1): OpinosisGraph(Z)1: Input: Topic related sentences to be summarized: Z = {zi}ni=12: Output: G = (V,E)3: for i = 1 to n do4: w ?
Tokenize(zi)5: sent size?
SizeOf(w)6: for j = 1 to sent size do7: LABEL?
wj8: PID ?
j9: SID ?
i10: ifExistsNode(G,LABEL) then11: vj ?
GetExistingNode(G,LABEL)12: PRIvj ?
PRIvj ?
(SID, PID)13: else14: vj ?
CreateNewNode(G,LABEL)15: PRIvj ?
(SID, PID)16: end if17: if notExistsEdge(vj?1 ?
vj , G) then18: AddEdge(vj?1 ?
vj , G)19: end if20: end for21: end forOur graph representation is closer to that used byBarzilay and Lee (Barzilay and Lee, 2003) for thetask of paraphrasing, wherein each node in thegraph represents a unique word.
However, in theirwork, such a graph is used to identify regions ofcommonality and variability amongst similar sen-tences.
Thus, the positional information is not re-quired nor is it maintained.
In contrast, we main-tain positional information at each node as this iscritical for the selection of candidate paths.Algorithm A1 outlines the steps involved inbuilding an Opinosis-Graph.
We start with a setof sentences relevant to a specific topic, which canbe obtained in different ways depending on the ap-plication.
For example, they may be all sentencesrelated to the battery life of the iPod Nano.
We de-note these sentences as Z = {zi}ni=1 where each ziis a sentence containing part-of-speech (POS) an-notations.
(A1:4) Each zi ?
Z is split into a setof word units, where each unit, wj consists of aword and its corresponding POS annotation (e.g.
?service:nn?, ?good:adj?).
(A1:7-9) Each uniquewj will form a node, vj , in the Opinosis-Graph,with wj being the label.
Also, since we only haveone node per unique word unit, each node keepstrack of all sentences that it is a part of using a sen-tence identifier (SID) along with its position of oc-currence in that sentence (PID).
(A1:10-16) Eachnode will thus carry a Positional Reference Infor-mation (PRI) which is a list of {SID:PID} pairsrepresenting the node?s membership in a sentence.
(A1:17-19) The original structure of a sentence isrecorded with the use of directed edges.
Figure 1shows a resulting Opinosis-Graph based on foursentences.The Opinosis-Graph has some unique proper-ties that are crucial in generating abstractive sum-maries.
We highlight some of the core propertiesby drawing examples from Figure 1:Property 1.
(Redundancy Capture).
Highly re-dundant discussions are naturally captured by sub-graphs.Figure 1 shows that although the phrase ?great de-vice?
was mentioned in different parts of sentences(1) and (3), this phrase forms a relatively heavysub-path in the resulting graph.
This is a good in-dication of salience.Property 2.
(Gapped Subsequence Capture).
Ex-isting sentence structures introduce lexical linksthat facilitate the discovery of new sentences or re-inforce existing ones.The main point conveyed by sentences (2) and (3)in Figure 1 is that calls drop frequently.
However,this is expressed in slightly different ways and isreflected in the resulting subgraph.
Since sentence(2) introduces a lexical link between ?drop?
and?frequently?, the word ?too?
can be ignored for sen-tence (3) as the same amount of information is re-tained.
This is analogous to capturing a repetitivegapped subsequence where similar sequences withminor variations are captured.
With this, the sub-graph calls drop frequently can be considered re-dundant.Property 3.
(Collapsible Structures).
Nodes thatresemble hubs are possibly collapsible.In Figure 1 we see that the subgraph ?the iPhoneis?, is fairly heavy and the ?is?
node acts like a341myphonecallsfrequentlytoo {3:8}withdrop iphoneisamy {2:1}phone{2:2}calls{2:3, 3:6}frequently{2:5, 3:9}with {2:6}thedrop{2:4, 3:7}great{1:5, 3:1}{1:2, 2:8, 4:2}{1:3,4:3}{1:4}.
{1:7,2:9,3:10}{1:1, 2:7, 3:5,4:1,4:5}worthprice{4:6}{,}, {3:3}but {3:4}{1:7,2:9,3:10}worth {4:4} nodelabelSID:PIDpairsdevice{1:6, 3:2}Input:SID:1.TheiPhoneis agreat device.SID:2.Myphonecallsdrop frequently withtheiPhone.SID:3.Greatdevice, but the callsdrop too frequently.p ,pqySID:4.The iPhoneisworththeprice.Figure 1: Sample Opinosis-Graph.
Thick edgesindicate salient paths.?hub?
where it connects to various other nodes.Such a structure is naturally captured by theOpinosis-Graph and is a good candidate for com-pression to generate a summary such as ?TheiPhone is a great device and is worth the price?.Also, certain word POS (e.g.
linking verbs like?is?
and ?are?)
often carry hub-like properties thatcan be used in place of the outlink information.3 Opinosis Summarization FrameworkIn this section, we describe a general frameworkfor generating abstractive summaries using theOpinosis-Graph.
We also describe our implemen-tation of the components in this framework.At a high level, we generate an abstractive sum-mary by repeatedly searching the Opinosis graphfor appropriate subgraphs that both encode a validsentence (thus meaningful sentences) and havehigh redundancy scores (thus representative of themajor opinions).
The sentences encoded by thesesubgraphs would then form an abstractive sum-mary.Going strictly by the definition of true abstrac-tion (Radev et al, 2002), our problem formula-tion is still more extractive than abstractive be-cause the generated summary can only containwords that occur in the text to be summarized;our problem definition may be regarded as a word-level (finer granularity) extractive summarization.However, compared to the conventional sentence-level extractive summarization, our formulationhas flavors of abstractive summarization whereinwe have elements of fusion (combining extractedportions) and compression (squeezing out unim-portant material from a sentence).
Hence, the sen-tences in the generated summary are generally notthe same as any original sentence.
Such a ?shal-low?
abstractive summarization problem is moretractable, enabling us to develop a general solutionto the problem.
We now describe each componentin such a summarization framework.3.1 Valid PathA valid path intuitively refers to a path that corre-sponds to a meaningful sentence.Definition 1.
(Valid Start Node - VSN).
A node vqis a valid start node if it is a natural starting pointof a sentence.We use the positional information of a node to de-termine if it is a VSN.
Specifically, we check ifAverage(PIDvq) ?
?vsn, where ?vsn is a pa-rameter to be empirically set.
With this, we onlyqualify nodes that tend to occur early on in a sen-tence.Definition 2.
(Valid End Node - VEN).
A node vsis a valid end point if it completes a sentence.We use the natural ending points in the text to besummarized as hints to which node may be a validend point of a path (i.e., a sentence).
Specifically,a node is a valid end node if (1) the node is apunctuation such as period and comma or (2) thenode is any coordinating conjunction (e.g., ?but?and ?yet?
).Definition 3.
(Valid Path).
A path W = {vq...vs}is valid if it is connected by a set of directed edgessuch that (1) vq is a VSN, (2) vs is a VEN, and(3) W satisfies a set of well-formedness POS con-straints.Since not every path starting with a VSN and end-ing at a VEN encodes a meaningful sentence, wefurther require a valid path to satisfy the followingPOS constraints (expressed in regular-expression)to ensure that a valid path encodes a well-formedsentence:1. .
?
(/nn) + .
?
(/vb) + .
?
(/jj) + .?2.
.
?
(/jj) + .
?
(/to) + .
?
(/vb).?3.
.
?
(/rb) ?
.
?
(/jj) + .
?
(/nn) + .?4.
.
?
(/rb) + .
?
(/in) + .
?
(/nn) + .
?This also provides a way (if needed) for the appli-cation to generate only specific type of sentenceslike comparative sentences or strictly opinionatedsentences.
These rules are thus application spe-cific.3.2 Path ScoringIntuitively, to generate an abstractive summary, weshould select a valid path that can represent most ofthe redundant opinions well.
We would thus favora valid path with a high redundancy score.Definition 4.
(Path Redundancy).
Let W ={vq...vs} be a path from an Opinosis-Graph.
Thepath redundancy of W , r(q, s), is the number ofoverlapping sentences covered by this path, i.e.,342r(q, s) = nq??nq+1...?
?ns,where ni = PRIvi and ??
is the intersection be-tween two sets of SIDs such that the difference be-tween the corresponding PIDs is no greater than?gap, and ?gap > 0 is a parameter.Path redundancies provide good indication of howmany sentences discuss something similar at eachpoint in the path.
The ?gap parameter controls themaximum allowed gaps in discovering these re-dundancies.
Thus, a common sentence X betweennodes vq and vr, will be considered a valid inter-sect if (PIDvrx ?
PIDvqx) ?
?gap.Based on path redundancy, we propose severalways to score a path for the purpose of selecting agood path to include in the summary:1.
Sbasic(W ) = 1|W |?sk=i+1,i r(i, k)2.
Swt len(W ) = 1|W |?sk=i+1,i |vi, vk| ?
r(i, k)3.
Swt loglen(W ) = 1|W |(r(i, i+ 1) +?sk=i+2,i+1 log2|vi, vk| ?
r(i, k))vi is the first node in the path being scored and vsis the last node.
|vi, vk| is the length from node vito vk.
|W | is the length of the entire path beingscored.
The Sbasic scoring function scores a pathpurely based on the level of redundancy.
One couldalso argue that high redundancy on a longer path isintuitively more valuable than high redundancy ona shorter path as the former would provide bettercoverage than the latter.
This intuition is factoredin by the Swt len and Swt loglen scoring functionswhere the level of redundancy is weighted by thepath length.
Swt loglen is similar to Swt len onlythat it scales down the path length so that it doesnot entirely dominate.3.3 Collapsed pathsIn some cases, paths in the Opinosis-Graph may becollapsible (as explained in Section 2).
In such acase, the collapse operation is performed and thenthe path scores are computed.
We will now ex-plain a few concepts related to collapsible struc-tures.
Let W?
= {vi...vk} be a path from theOpinosis-Graph.Definition 5.
(Collapsible Node).
Node vk is acandidate for collapse if its POS is a verb.We only attempt to collapse nodes that are verbsdue to the heavy usage of verbs in opinion text andthe ease with which the structures can be combinedto form a new sentence.
However, as mentionedearlier other properties like the outlink informationcan be used to determine if a node is collapsible.Definition 6.
(Collapsed Candidates, Anchor).Let vk be a collapsible node.
The collapsed can-didates of vk (denoted by CC = {cci}mi=1) are theCanchor CC Connectora.
the sound quality is cc1 : really good andcc2 : clearb.
the iphone is cc1 : great butcc2 : expensiveTable 1: Example of anchors, collapsed candidatesand suitable connectorsremaining paths after vk in all the valid paths go-ing through vi...vk.
The prefix vi...vk is called theanchor, denoted as Canchor = {vi...vk}.
Eachpath {vi...vn}, where vn is the last node in eachcci ?
CC, is an individually valid path.Table 1 shows a simplistic example of anchors andcorresponding collapsed candidates.
Once the an-chor and collapsed candidates have been identified,the task is then to combine all of these to form anew sentence.Definition 7.
(Stitched Sentence) A stitched sen-tence is one that combines Canchor and CC toform a combined, logical sentence.We will now describe the stitching procedure thatwe use, by drawing examples from Table 1.
Sincewe are dealing with verbs, Canchor can be com-bined with the corresponding CC with commasto separate each cci ?
CC with one exception -the correct sentence connector has to be used forthe last cci.
For Canchora , the phrases really goodand clear can be connected by ?and?
due to thesame sentiment orientation.
For Canchorb , the col-lapsed candidate phrases are well connected by theword ?but?.
We use the existing Opinosis-Graphto determine the most appropriate connector.
Wedo this by looking at all coordinating conjunction(e.g.
?but?, ?yet?)
nodes (vcconj) that are connectedto the first node of the last collapsed candidate,ccm.
This would be the node labeled ?clear?
forCanchora and ?expensive?
for Canchorb .
We denotethese nodes as v0,ccm .
The vcconj , with the high-est path redundancy with v0,ccm , will be selectedas the connector.Definition 8.
(Collapsed Path Score) The finalpath score after the entire collapse operation is theaverage across path scores computed from vi to thelast node in each cci ?
CC.The collapsed path score essentially involves com-puting the path scores of the individual sentencesassuming that they are not collapsed and then av-eraging them.3.4 Generation of summaryOnce we can score all the valid paths as well as allthe collapsed paths, the generation of an abstrac-tive summary can be done in two steps: First, werank all the paths (including the collapsed paths)in descending order of their scores.
Second, we343eliminate duplicated (or extremely similar) pathsby using a similarity measure (in our experiments,we used Jaccard).
We then take the top few re-maining paths as the generated summary, with thenumber of paths to be chosen controlled by a pa-rameter ?ss, which represents summary size.Although conceptually we enumerate all thevalid paths, in reality we can use a redundancyscore threshold, ?r to prune many non-promisingpaths.
This is reasonable because we are only in-terested in paths with high redundancy scores.4 Summarization AlgorithmAlgorithms A2 and A3 describe the steps involvedin Opinosis Summarization.
A2 is the startingpoint of the Opinosis Summarization and A3 is asubroutine where path finding takes place, invokedfrom within A2.Algorithm 2 (A2): OpinosisSummarization(Z)1: Input: Topic related sentences to be summarized: Z = {zi}ni=12: Output: O ={Opinosis Summaries}3: g ?
OpinosisGraph(Z)4: node size?
SizeOf(g)5: for j = 1 to node size do6: if V SN(vj) then7: pathLen?
18: score?
09: cList?
CreateNewList()10: Traverse(cList, vj , score, PRIvj , labelvj , pathLen)11: candidates?
{candidates ?
cList}12: end if13: end for14: C ?
EliminateDuplicates(candidates)15: C ?
SortByPathScore(C)16: for i = 1 to ?ss do17: O = {O ?
PickNextBestCandidate(C)}18: end for(A2:3) Opinosis Summarization starts with theconstruction of the Opinosis-Graph, described indetail in Section 2.
This is followed by the depthfirst traversal of this graph to locate valid pathsthat become candidate summaries.
(A2:6-12) Toachieve this, each node vj in the Opinosis-Graphis examined to determine if it is a VSN and, if itis, path finding will start from this node by invok-ing subroutine A3.
A3 takes the following as in-put: list - a list to hold candidate summaries; vi- the node to continue traversal from; score - theaccumulated path score; PRIoverlap - the intersectbetween PRIs of all nodes visited so far (see Defi-nition 4); sentence - the summary sentence formedso far; len - the current path length.
(A2:7-10) Be-fore invoking A3 from A2, the path length is set to?1?, path score is set to ?0?
and a new list is cre-ated to store candidate summaries generated fromnode vj .
(A2:11) All candidate summaries gener-ated from vj will be stored in a common pool ofcandidate summaries.Algorithm 3 (A3): Traverse(...)1: Input: list, vk ?
V , score, PRIoverlap, sentence, len2: Output: A set of candidate summaries3: redundancy ?
SizeOf(PRIoverlap)4: if redundancy ?
?r then5: if V EN(vk) then6: if V alidSentence(sentence) then7: finalScore?
scorelen8: AddCandidate(list, sentence, finalScore)9: end if10: end if11: for vn ?
Neighborsvk do12: PRInew ?
PRIoverlap ??
PRIvn13: redundancy ?
SizeOf(PRInew)14: newSent?
Concat(sentence, labelvn )15: L?
len+ 116: newScore?
score+ PathScore(redundancy, L)17: if Collapsible(vn) then18: Canchor ?
newSent19: tmp?
CreateNewList()20: for vx ?
Neighborsvn do21: Traverse(tmp, vx, 0, PRInew, labelvx , L)22: CC ?
EliminateDuplicates(tmp)23: CCPathScore?
AveragePathScore(CC)24: finalScore?
newScore+ CCPathScore25: stitchedSent?
Stitch(Canchor, CC)26: AddCandidate(list, stitchedSent, finalScore)27: end for28: else29: Traverse(list, vn, newScore, PRInew, newSent, L)30: end if31: end for32: end if(A3:3-4) Algorithm A3 starts with a check toensure that the minimum path redundancy require-ment is satisfied (see definition 4).
For the veryfirst node sent from A2, the path redundancy is thesize of the raw PRI .
(A3:5-10) If the redundancyrequirement is satisfied, a few checks are done todetermine if a valid path has been found.
If it has,then the resulting sentence and its final score areadded to the list of candidate summaries.
(A3:11-31) Traversal proceeds recursivelythrough the exploration of all neighboring nodesof the current node, vk.
(A3:12-16) For everyneighboring node, vn the PRI overlap information,path length, summary sentence and path scoreare updated before the next recursion.
(A3:29)If a vn is not collapsible, then a regular traver-sal takes place.
(A3:17-27) However, if vn iscollapsible, the updated sentence in A3:14, willnow serve as an anchor in A3:18.
(A3:21) A3will then attempt to start a recursive traversalfrom all neighboring nodes of vn in order to findcorresponding collapsed candidates.
(A3:22-26)After this, duplicates are eliminated from thecollapsed candidates and the collapsed path scoreis computed.
The resulting stitched sentence andits final score are then added to the original list ofcandidate summaries.
(A2:14-18) Once all paths have been explored344for candidate generation, duplicate candidates areremoved and the remaining are sorted in descend-ing order of their path scores.
The best ?ss candi-dates are ?picked?
as final Opinosis summaries.5 Experimental SetupWe evaluate this abstractive summarization taskusing reviews of hotels, cars and various prod-ucts1.
Based on these reviews, 2 humans wereasked to construct ?opinion seeking?
queries whichwould consist of an entity name and a topic of in-terest.
Example of such queries are: Amazon Kin-dle:buttons, Holiday Inn, Chicago: staff, and soon.
We compiled a set of 51 such queries.
We cre-ate one review document per query by collectingall review sentences that contain the query wordsfor the given entity.
Each review document thusconsists of a set of unordered, redundant reviewsentences related to the query.
There are approxi-mately 100 sentences per review document.We use ROUGE (Lin, 2004b) to quantitativelyassess the agreement of Opinosis summaries withhuman composed summaries.
ROUGE is based onan n-gram co-occurrence between machine sum-maries and human summaries and is a widely ac-cepted standard for evaluation of summarizationtasks.
In our experiments, we use ROUGE-1,ROUGE-2 and ROUGE-SU4 measures.
ROUGE-1 and ROUGE-2 have been shown to have mostcorrelation with human summaries (Lin and Hovy,2003) and higher order ROUGE-N scores (N > 1)estimate the fluency of summaries.We use multiple reference (human) summariesin our evaluation since it can achieve better cor-relation with human judgment (LIN, 2004a).
Weleverage Amazon?s Online Workforce2 to get 5 dif-ferent human workers to summarize each reviewdocument.
The workers were asked to be conciseand were asked to summarize the major opinions inthe review document presented to them.
We manu-ally reviewed each set of reference summaries anddropped summaries that had little or no correlationwith the majority.
This left us with around 4 refer-ence summaries for each review document.To allow performance comparison between hu-mans, Opinosis and the baseline method, we im-plemented a Jackknifing procedure where, given Kreferences, the ROUGE score is computed over Ksets of K-1 references.
With this, average humanperformance is computed by treating each refer-ence summary as a ?system?
summary, computingROUGE scores over the remaining K-1 reference1Reviews collected from Tripadvisor, Amazon, Edmunds2https://www.mturk.comsummaries.Due to the limited work in abstractive sum-marization, no natural baseline could be used forcomparison.
The existing work in this area ismostly domain dependent and requires too muchmanual effort (explained in Section 1).
The nextbest baseline is to use a state of the art extractivemethod.
Thus, we use MEAD (Radev et al, 2000)as our baseline.
MEAD is an extractive summa-rizer based on cluster centroids.
It uses a collectionof the most important words from the whole clus-ter to select the best sentences for summarization.By default, the scoring of sentences in MEAD isbased on 3 parameters - minimum sentence length,centroid, and position in text.
MEAD was idealfor our task because a good summary in our casewould be one that could capture the most essentialinformation.
This is exactly what centroid-basedsummarization aims to achieve.
Also, since the po-sition in text parameter is irrelevant in our case, wecould easily turn this off with MEAD.We introduce a readability test to understand ifOpinosis summaries are in fact readable.
Supposewe have N sentences from a system-generatedsummary and M sentences from correspondinghuman summaries.
We mix all these sentencesand then ask a human assessor to pick at most Nsentences that are least readable as the predictionof system summary.readability(O) = 1?
#CorrectPickNIf the human assessor often picks out system gen-erated summaries as being least readable, then thereadability of system summaries is poor.
If not,then the system generated summaries are no dif-ferent from human summaries.6 ResultsThe baseline method (MEAD) selects 2 most rep-resentative sentences as summaries.
To give a faircomparison, we fix the Opinosis summary size,?ss = 2.
We also fix ?vsn = 15.
The best Opinosisconfiguration with ?ss = 2 and ?vsn = 15 iscalled Opinosisbest (?gap = 4, ?r = 2, Swt loglen).ROUGE scores reported are with the use of stem-ming and stopword removal.Performance comparison between humans,Opinosis and baseline.
Table 2 shows the perfor-mance comparison between humans, Opinosisbestand the baseline method.
First, we see that thebaseline method has very high recall scores com-pared to Opinosis.
This is because extractive meth-ods that just ?select?
sentences tend to be muchlonger resulting in higher recall.
However, thesesummaries tend to carry information that may notbe significant and is clearly reflected by the poor345RecallROUGE-1 ROUGE-2 ROUGE-SU4 Avg # WordsHuman 0.3184 0.1106 0.1293 17Opinosis 0.2831 0.0853 0.0851 15Baseline 0.4932 0.1058 0.2316 75PrecisionROUGE-1 ROUGE-2 ROUGE-SU4 Avg # WordsHuman 0.3434 0.1210 0.1596 17Opinosis 0.4482 0.1416 0.2261 15Baseline 0.0916 0.0184 0.0102 75F-scoreROUGE-1 ROUGE-2 ROUGE-SU4 Avg # WordsHuman 0.3088 0.1069 0.1142 17Opinosis 0.3271 0.0998 0.1027 15Baseline 0.1515 0.0308 0.0189 75Table 2: Performance comparison between Hu-mans, Opinosisbest and Baseline.0.11003300.1000.110E-SU40.3100.330GE-10.0900.1000.110ROUGbasic0.2900.3100.330ROUbasic00700.0800.0900.1000.110basicwt_loglenwt_len0.2700.2900.3100.330basicwt_loglenwtlen0.0700.0800.0900.1000.11012345basicwt_loglenwt_len0.2500.2700.2900.3100.33012345basic wt_loglenwt_len?gap?gap0.0700.0800.0900.1000.11012345basicwt_loglenwt_len0.2500.2700.2900.3100.33012345basic wt_loglenwt_len?gap?gapFigure 2: ROUGE scores (f-measure) at differentlevels of ?gap, ?r = 2.precision scores.Next, we see that humans have reasonableagreement amongst themselves given that these areindependently composed summaries.
This agree-ment is especially clear with the ROUGE-2 re-call score where the recall is better than Opinosisbut comparable to the baseline even though thesummaries are much shorter.
It is also clear thatOpinosis is closer in performance to humans thanto the baseline method.
The recall scores ofOpinosis summaries are slightly lower than thatachieved by humans, while the precision scores arehigher (Wilcoxon test shows that the increase inprecision is statistically more significant than thedecrease in recall).
In terms of f-scores, Opinosishas the best ROUGE-1 score and its ROUGE-2 andROUGE-SU4 scores are comparable with humanperformance.
The baseline method has the low-est f-scores.
The difference between the f-scoresof Opinosis and that of humans is statistically in-significant.Comparison of scoring functions.
Next, we lookinto the performance of the three scoring func-tions, Sbasic, Swt len and Swt loglen described inSection 3.
Figure 2 shows ROUGE scores of thesescoring methods at varying levels of ?gap.
First,0.30ROUGE-10.090.10ROUGE-SU40.200.250.30ROUGE-10.050.060.070.080.090.10R2R3R4ROUGE-SU40.200.250.30R2R3R4ROUGE-1basicwt_loglenwt_basic0.050.060.070.080.090.10R2R3R4ROUGE-SU4basicwt_loglenwt_basicFigure 3: ROUGE scores (f-measure) at differentlevels of ?r averaged across ?gap ?
[1, 5]0.230.28PRECISIONprecision-recall curveROUGE-SU40.400.450.50PRECISIONprecision-recall curveROUGE-1x collapsex dupelim x collapse+dupelimOpinosis(baseline)+llx collapsex dupelim xcollapse+dupelimOpinosis(baseline)0.180.230.28 0.060.070.080.090.1RECALLprecision-recall curveROUGE-SU40.350.400.450.50 0.220.240.260.280.3RECALLprecision-recall curveROUGE-1x collapsex dupelim x collapse+dupelimOpinosis(baseline)+collapsex dupelimx collapsex dupelim xcollapse+dupelimOpinosis(baseline) +collapse x dupelimFigure 4: Precision-Recall comparison with differ-ent Opinosis features turned off.it can be observed that Swt basic which does notuse path length information, performs the worst.This is due to the effect of heavily favoring re-dundant paths over longer but reasonably redun-dant ones that can provide more coverage.
We alsosee that Swt len and Swt loglen are similar in per-formance with Swt loglen marginally outperform-ing Swt len when ?gap > 2.
Since Swt len usesthe raw path length in its scoring function, it maybe inflating the path scores of long but insignifi-cant paths.
Swt loglen scales down the path length,thus providing a reasonable tradeoff between re-dundancy and the length of the selected path.
Thethree scoring functions are not influenced by dif-ferent levels of ?r as shown in Figure 3.Effect of gap setting (?gap).
Now, we will ex-amine the effect of ?gap on the generated sum-maries.
Based on Figure 2, we see that setting?gap=1 yields in relatively low performance.
Thisis because ?gap=1 implies immediate adjacencybetween the PIDs of two nodes and such strict ad-jacency enforcements prevent redundancies frombeing discovered.
When ?gap is increased to 2,there is a big jump in performance, after whichimprovements are observed in smaller amounts.
Avery large gap setting could increase the possibilityof generating ill-formed sentences, thus we recom-mend that ?gap is set between 2-5.Effect of redundancy requirement (?r) .
Fig-ure 3 shows the ROUGE scores at different levelsof ?r.
It is clear that when ?r > 2, the quality ofsummaries is negatively impacted.
Since we onlyhave about 100 sentences per review document,?r > 2 severely restricts the number of paths thatcan be explored, yielding in lower ROUGE scores.Since the scoring function can account for the levelof redundancy, ?r should be set according to thesize of the input data.
For our dataset, ?r = 2 wasideal.346?Aboutfoodat Holiday Inn, London?Humansummaries:[1]Food wasexcellent withawide rangeofchoicesand good services.
[2]Thefoodisgood, the servicegreat.
Verygoodselection of foodfor breakfastbuffet.
?What isfreeatBestwesternInn, SanFrancisco?Humansummaries:[1]Thereisfree WiFiinternet access availableinall the rooms.. From5-6p.m.
thereisfreewine tasting andappetizersavailabletoallthe guests.
[2]Evening winereceptionand freecoffee in themorning.
Free internet, freeparking andfreemassageOpinosisabstractive summary:Thefoodwasexcellent,good anddelicious.
Very good selectionoffood.Baselineextractive summary:Within200 yards of leavingthe hotel andheadingtothe TubeStationyou have anumberoffast foodoutlets,highstreetRestautants, Pastryshops andsupermarketssoifyoudidwishtoliveinyourhotelroomforthedurationofyourfree massage.Opinosisabstractive summary:Freewinereceptioninevening.
Free coffeeand biscotti andwine.Baselineextractive summary:Thefree wineand nibblesserved between 5pmand 6pmwerealovelytouch.
There's freecoffeeteasatbreakfasttimewithlittlebiscottiandbestofallfrom5till6pmyougetafreesupermarkets, so if youdid wishtolive in your hotel roomfor the duration of yourstay,you coulddo.......coffee,teas at breakfasttime withlittlebiscotti and, best of all,from5 till 6pmyou geta freewine 'tasting' reception which,aslongasyou don't take?
?Figure 5: Sample results comparing Opinosis summaries with human and baseline summaries.Effect of collapsed structures and duplicateelimination.
So far, it has been assumed that allfeatures used in Opinosis are required to gener-ate reasonable summaries.
To test this hypothesis,we use Opinosisbest as a baseline and then we turnoff different features of Opinosis.
We turn off theduplicate elimination feature, then the collapsi-ble structure feature, and finally both.
Figure 4shows the resulting precision-recall curve.
Fromthis graph, we see that without duplicate elimina-tion and when collapsing is turned off, the preci-sion is highest but recall is lowest.
No collaps-ing implies shorter sentences and thus lower recall,which is clearly reflected in Figure 4.
On top ofthis, if duplicates are allowed, the overall informa-tion coverage is low, further affecting the recall.Notice that the presence of duplicates with the col-lapse feature turned on results in very high recall(even higher than the baseline).
This is caused bythe presence of similar phrases that were not elim-inated from the collapsed candidates, resulting inlong sentences that artificially boost recall.
TheOpinosis baseline which uses duplicate elimina-tion and the collapsible structure feature, offers areasonable tradeoff between precision and recall.Readability of Summaries.
To test the readabilityof Opinosis summaries, we conducted a readabil-ity test (described in Section 5) using summariesgenerated from Opinosisbest.
A human assessorpicked the 2 least readable sentences from each ofthe 51 test sets (based on 51 summaries).
Collec-tively, there were 565 sentences out of which 102were Opinosis generated.
Out of these, the hu-man assessor picked only 34 of the sentences asbeing least readable, resulting in an average read-ability score of 0.67.
This shows that more than60% of the generated sentences are indistinguish-able from human composed sentences.
Of the 34sentences with problems, 11 contained no informa-tion or were incomprehensible, 12 were incompletepossibly due to false positives when the sentencevalidity check was done, and 8 had conflicting in-formation such as ?the hotel room is clean anddirty?.
This happens due to mixed feelings aboutthe same topic and can be resolved using sentimentanalysis.
The remaining 3 sentences were foundto contain poor grammar, possibly caused by thegaps allowed in finding redundant paths.Sample Summaries.
Finally, in Figure 5 we showtwo sample summaries on two different topics.Notice that the Opinosis summaries are concise,fairly well-formed and have closer resemblance tohuman summaries than to the baseline summaries.7 ConclusionIn this paper, we described a novel summarizationframework (Opinosis) that uses textual graphs togenerate abstractive summaries of highly redun-dant opinions.
Evaluation results on a set of reviewdocuments show that Opinosis summaries havebetter agreement with human summaries com-pared to the baseline extractive method.
TheOpinosis summaries are concise, reasonably well-formed and communicate essential information.Our readability test shows that more than 60% ofthe generated sentences are no different from hu-man composed sentences.Opinosis is a flexible framework in that manyof its modules can be easily improved or replacedwith other suitable implementation.
Also, sinceOpinosis is domain independent and relies on min-imal external resources, it can be used with anycorpus containing high amounts of redundancies.Our graph representation naturally ensures thecoherence of a summary, but such a graph empha-sizes too much on the surface order of words.
As aresult, it cannot group sentences at a deep seman-tic level.
To address this limitation, we can use asimilar idea to overlay parse trees and this wouldbe a very interesting future research.8 AcknowledgmentsWe thank the anonymous reviewers for their use-ful comments.
This paper is based upon work sup-ported in part by an IBM Faculty Award, an AlfredP.
Sloan Research Fellowship, an AFOSR MURIGrant FA9550-08-1-0265, and by the National Sci-ence Foundation under grants IIS-0347933, IIS-0713581, IIS-0713571, and CNS-0834709.347References[Barzilay and Lee2003] Barzilay, Regina and LillianLee.
2003.
Learning to paraphrase: an unsuper-vised approach using multiple-sequence alignment.In NAACL ?03: Proceedings of the 2003 Confer-ence of the North American Chapter of the Associ-ation for Computational Linguistics on Human Lan-guage Technology, pages 16?23, Morristown, NJ,USA.
Association for Computational Linguistics.
[DeJong1982] DeJong, Gerald F. 1982.
An overview ofthe FRUMP system.
In Lehnert, Wendy G. and Mar-tin H. Ringle, editors, Strategies for Natural Lan-guage Processing, pages 149?176.
Lawrence Erl-baum, Hillsdale, NJ.
[Erkan and Radev2004] Erkan, Gu?nes and Dragomir R.Radev.
2004.
Lexrank: graph-based lexical central-ity as salience in text summarization.
J. Artif.
Int.Res., 22(1):457?479.
[Finley and Harabagiu2002] Finley, Sanda Harabagiuand Sanda M. Harabagiu.
2002.
Generating sin-gle and multi-document summaries with gistexter.
InProceedings of the workshop on automatic summa-rization, pages 30?38.
[Jing and McKeown2000] Jing, Hongyan and Kath-leen R. McKeown.
2000.
Cut and paste basedtext summarization.
In Proceedings of the 1st NorthAmerican chapter of the Association for Computa-tional Linguistics conference, pages 178?185, SanFrancisco, CA, USA.
Morgan Kaufmann PublishersInc.
[Lerman et al2009] Lerman, Kevin, Sasha Blair-Goldensohn, and Ryan Mcdonald.
2009.
Sentimentsummarization: Evaluating and learning user prefer-ences.
In 12th Conference of the European Chapterof the Association for Computational Linguistics(EACL-09).
[Lin and Hovy2003] Lin, Chin-Yew and Eduard Hovy.2003.
Automatic evaluation of summaries using n-gram co-occurrence statistics.
In Proc.
HLT-NAACL,page 8 pages.
[LIN2004a] LIN, Chin-Yew.
2004a.
Looking for a fewgood metrics : Rouge and its evaluation.
proc.
of the4th NTCIR Workshops, 2004.
[Lin2004b] Lin, Chin-Yew.
2004b.
Rouge: a pack-age for automatic evaluation of summaries.
In Pro-ceedings of the Workshop on Text SummarizationBranches Out (WAS 2004), Barcelona, Spain.
[Lu et al2009] Lu, Yue, ChengXiang Zhai, and NeelSundaresan.
2009.
Rated aspect summarization ofshort comments.
In 18th International World WideWeb Conference (WWW2009), April.
[Mihalcea and Tarau2004] Mihalcea, R. and P. Tarau.2004.
TextRank: Bringing order into texts.
In Pro-ceedings of EMNLP-04and the 2004 Conference onEmpirical Methods in Natural Language Processing,July.
[Pang and Lee2004] Pang, Bo and Lillian Lee.
2004.A sentimental education: Sentiment analysis usingsubjectivity summarization based on minimum cuts.In Proceedings of the ACL, pages 271?278.
[Pang et al2002] Pang, Bo, Lillian Lee, and Shivaku-mar Vaithyanathan.
2002.
Thumbs up?
Sentimentclassification using machine learning techniques.
InProceedings of the 2002 Conference on EmpiricalMethods in Natural Language Processing (EMNLP),pages 79?86.
[Radev and McKeown1998] Radev, DR and K. McKe-own.
1998.
Generating natural language summariesfrom multiple on-line sources.
Computational Lin-guistics, 24(3):469?500.
[Radev et al2000] Radev, Dragomir, Hongyan Jing, andMalgorzata Budzikowska.
2000.
Centroid-basedsummarization of multiple documents: Sentence ex-traction, utility-based evaluation, and user studies.In In ANLP/NAACL Workshop on Summarization,pages 21?29.
[Radev et al2002] Radev, Dragomir R., Eduard Hovy,and Kathleen McKeown.
2002.
Introduction to thespecial issue on summarization.
[Saggion and Lapalme2002] Saggion, Horacio and GuyLapalme.
2002.
Generating indicative-informativesummaries with sumum.
Computational Linguistics,28(4):497?526.
[Snyder and Barzilay2007] Snyder, Benjamin andRegina Barzilay.
2007.
Multiple aspect rankingusing the good grief algorithm.
In In Proceedingsof the Human Language Technology Conferenceof the North American Chapter of the Associationof Computational Linguistics (HLT-NAACL, pages300?307.
[Titov and Mcdonald2008] Titov, Ivan and Ryan Mc-donald.
2008.
A joint model of text and aspect rat-ings for sentiment summarization.
In Proceedingsof ACL-08: HLT, pages 308?316, Columbus, Ohio,June.
Association for Computational Linguistics.348
