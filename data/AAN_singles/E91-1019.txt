AUTOMATIC LEARNING OF WORD TRANSDUCERSFROM EXAMPLESMichel GillouxCentre National d'l~tudes des T616communlcationsLAA/SLC/AtARoute de Tr6gastel, BP 40F-22301 Lannlon Cedex, FRANCEe-mail: gfl loux@lannion.cnet .frABBTRACTThis paper  descr ibes the application ofmarkovian teaming methods to the infer-ence of word t ransducers .
We show howthe proposed method d ispenses from thedifficult design of hand-crafted rules, al-lows the use of weighed non determinist ict ransducers  and is able to translate wordsby taking into account  their whole ratherthan by making decisions locally.
These ar-guments  are i l lustrated on two examples:morphological analysis and grapheme-to-phoneme transcription.I N T R O D U C T I O NSeveral tasks  associated with elec-tronic lexicons may be viewed as t ransduc-t ions between character  strings.
This maybe the decomposit ion of words into mor-phemes  in morphology or  the grapheme-to-phoneme transcript ion in phonology.
In thefirst case.
one has  for example to decom-pose the French word "chronom~trage" intothe sequence of affixes "chrono+m~tre+er?-age".
In the second, "abstenlr" should betranslated into "abstoniR",or "apstoniR" I.Most of the proposed methods in theIThese two tasks are in fact closely relatedin that {I) the correct phoneme transcriptionmay mirror an underlying morphological struc-ture, like for "asoc/a/" whose phonemic form is"asos ja l"  rather than "azosja l"  due to the de-composition "a+soclal", and (2) the surface formof a derived word may depend on the pronuncl-aUon of its component morphemes, llke for"d~+harnacher" which results in "d~harnacher"and not "d~sharnachet".domain (Catach 1984; Danlos et al 1986;Koskenniemi 1983; Laporte 1988; Ritchleet al 1987, Tufts 1989; V6ronls 1988) arebased on the availability of local ruleswhose combination, either through directinterpretation or by being compiled, formthe target transducer.Although these methods  make it pos-sible - at least in theory - to design suitablet ransducers ,  provided that the rule de-scrlpUon language has  the right expressivepower, they are complex to use because  ofthe difficulty of writing down rules.
More-over, for a given rule language, there maynot exist an algorithm for compil ing rulesinto a form better  suited to the translat ionprocess.
Lastly, in numerous  cases, thetranslat ion procedures are improperly de-terministic as shown by the example of"ab-stcnl f  so that it is not  possible to considerseveral competing hypotheses  in parallelnot  to speak of ranking them according tosome certainty factor.We have designed a program whichallows to construct  ransducers  without  re-taming the above shortcomings.
It is nolonger necessary to write down translat ionrules since the t ransducer  is obtained asthe result  of an automat ic  learning over aset of examples.
The t ransducer  is repre-sented into the language of probabil lst ic fi-nite state automata  (Markov models\] sothat its use  is straightforward.
Lastly, ttproduces results  which are assigned aprobabil ity and makes  it possible to llstthem by decreasing order of likelihood.After stating the problem of characterstrings translat ion and defining the few- 107  -central not ions of markovian learnJng, thispaper  descr ibes their adaptat ion to theword translat ion problem in the learningand translat ing phases.
This adaptat ion isi l lustrated through two applications: mor-phological analysis  and grapheme-to-pho-neme transcription.THE TRANSDUCTION PROBLEMIn the context of character  str ingstransduct ion,  we look for an application f:C* --> C'* which t ransforms certaJn wordsbui lt  over the a lphabet  C Into words overthe a lphabet  C'.
For example, In the case ofgrapheme-to -phoneme transcript ion, C isthe set of graphemes and C' that  of pho-nemes.It may  be appropriate,  for example inmorphology, to use  an auxil iary lexicon(Ritchle et al 1987; Ritchie 1989) which al-lows to d iscard certain translat ion results.For example, the decomposit ion "sage" -~"ser+age" would not be allowed because"sef is not  a verb in the French lexicon, al-though this is a correct result  with respectto the splitt ing of word forms into affixes.The method we propose in this paper  is onlyconcerned with descr ibing this last type ofregularit ies leaving aside all non regularphenomena better  descr ibed on a case-by-case bas is  such as  through a lexicon.MARKOV MODELSA Markou model is a probabi l ist ic fl-nlte state automaton  M = (S, T, A, s I, s F, g)where S is a finite set of states,  A is a finitealphabet,  s x E S and s F ~ S are two distln-gulshed states called respectively the/nit /a/state and the final state, T is a finite set oftransit ions,  and g Is a funct ion g: t E T -->(O(t), D(t), S(t), p(t)) ~ SxS  ?A?
\[0, I\] suchthatV(se  S), ~ p(t) = l{tl O(t)= s\]where p(t) is the probabfl l ty of reachingstate D(t) while generat ing symbol  S(t)start ing from state O(t).In general, the transit ion probablIi-ties p(t} are mutual ly  independent.
Yet, insome contexts, it may  be useful  to havetheft values depend on others transit ions.In this respect, it is possible to define a one-to-one correspondence x , {t I O(t) = s'}{t I O(t) = s} such  tha~Sp(t) is equal  top(~s,(t}).
States s and s' are then said to beFor every word w = a I "'" an ~ A*, theset of partWl paths compatible wflh w till CPathl(w}, is the set of sequences  of I transi-t ions t I ... t l such  that  O(t 1) = % D(~} =O(.tj+l), for J  = 1 .
.
.
.
.
1-1 and S(tj) = aj, lo r J= I  .
.
.
.
.
1.The set of complete paths compatiblewith w, Path(w), is in turn the set of ele-ments  in Pathlwl(W}, where I wl = n, thelength of word @, such  that  D(t n) = SF.The probabi l i ty for the model  M ofemitt ing the word w isProbM(m) = ~ l'I p(t)path q Path(w) t e pathA Markov model  for which there existat most  one complete path  for a given wordis said to be  un/fl/ar.
In this case, the aboveprobabil i ty is reduced toPr?bM(W) = l' I  p(t), if Path(w) = patht e pathProbM(w ) = O, if Path(w) = OThus  the probabi l i ty PM{W) may begeneral ly computed  by adding the probabi l -ities observed along every path  compatiblewith w. In practice, this renders  computa -tionally expensive the algorithm for com-put ing PM(W) and it is tempt ing to assumethat  the model is unlfllar.
Practical studieshave shown that this sub-opt imal  methodis appl icable wi thout  great loss (Bahl et al1983).Under  this hypothesis ,  the probabil i -ty PM(W) may be computed  through the Vlt-erbi dynamic programming algorithm.
In-deed, the probabi l i ty PM(w, 1, s}, maximalprobabi l i ty of reaching state s with the 1first t ransit ions in a path compat ib le with w- 108 -iswhere (path  ~ {t l .
.
.
t i  E Pathl(w) \[ D(t I) = s l  )PM(W, O, sl) = I' PM(W, O, s) = O, ff (S ~ el)thereforePlvi(w, 1 + 1, s) = maXpath (P(tl + 1).
PM(w, 1, D(tl)))where path  e {t l .
.
.
t l+ 1 E Pathl+l(W) \[D(tl+ 1 ) = s}wherebyPM(W, I + I, s) = max t (p(t) ?
PM(W,  I, O(t)))where (te {qD(t) = sand  S(t) = ai+ll )with= = \ [ I  p(t) PrObM(W ) PM(W, \[w\[, SF) t e MaxPath(w)It is therefore possible to computePM(W, 1, s) recurslvely for t = 1 .
.
.
.
.
n unti lPrObM(W).Automat ic  learn ing  o f  Markovmode lsGiven a training set "IS made of wordsin A* and a number  N > 2 of states, that  isthe set S, learning a Markov model consistsin finding a set T of transit ions such thatthe Joint probabi l i ty P of the examples inthe training setP(TS) = l'I PM (w)we  TSis maximal.In general, the set T Is composed apriori of all possible transi t ions betweenstates in S producing a symbol  in A.
The de-termination of probabi l i t ies p assoc iatedwith these transi t ions Is equivalent o therestriction of T to e lements with non nullprobabil i ty which induces the st ructure ofthe associated automaton.
In this case, themodel is said to be hidden because  it ishard to attach a meaning to the states in S.On the contrary, it is possible to force thosestates to have a clear-cut interpretation bydefining them, for example, as n-gramswhich are sequences  of n e lements in Awhich encode the last n symbols  producedby the model to reach the state.
It is clearthat then only some transit ions are mean-ingful.
In dealing with problems like thosestudied in the present  paper  it Is preferableto use  hidden models  which allow states tos tand for arbitrari ly complex predicates.The learning algorithm {Bahl et al1983) is based  upon the following remark:given a model M whose transit ions proba-btlltles are known a priori, the a postertoriprobabi l i ty of a transit ion t may be estimat-ed by the relative f requency with which t isused on a training set.The number  of t imes a transit ion t isused on "IS isfreq(t) = ~ ~ 8(t, t ')wGTS t' e MaxPath(w)where 8(t, t ' )= l  f i t=t  ?.
0 otherwiseThe relative f requency of us ing t on"IS isre l - f req( t )  =freq(t)( ~' freq(t')){t'l (o(t')= o(t)) }The learning algorithm consists  thenin sett ing randomly the probabtltty distri-but ion  p(t) and adjust ing lteratively its val-ues  through the above formula unti l  the ad-Jus tment  is small enough to consider  thedistr ibution as stat ionary.
It has  beenshown (Bahl et al 1983) that this algorithmdoes converge towards a stat ionary value ofthe p(t} which maximizes locally 1 the prob-abil ity P of the training set depending onthe initial random probabi l i ty distr ibution.\]In order to find a global optimum, we useda kind of simulated annealing technique (Kirk-patrick et al 1983) during the learning process.10~ -The stat ionary distr ibution defines theMarkov model induced from the examplesin TS i.TRANSDUCTION MODELTo be applied in both il lustrative x-amples, the general s t ructure of Markovmodels should be related, by means  of ashi~ in representat ion, to the problem ofstrings translat ion.
The model of two-levelmorphological nalysis (Koskenniemi 1983)suggests the nature  of this shift.
Indeed,this method, which was successful ly ap-plied to morphologically rich natura l  lan-guages (Koskenniemi 1983), Is based upona two-level rule formal ism for which thereexist a way to compile them into the lan-guage of finite state automata  (FSA) (Ritchie1989).
This result  val idates the idea thatFSAs are reasonable candidates for repre-senting transduct Jon rules, at least in thecase of morphology 2.The shift in representat ion is de-signed so as to define the alphabet A as theset of pairs c:-  or -:c' where c e C and cC', - s tanding Ior the null  character,  - ~ C,- ?
C'.
The mapping between the transduc-er f and the associated Markov model M isnow straightforward:lln practice, the number N = Card(S) ofstates for the model to be learned on a trainingset is not known.
When N is small, the modelhas a tendency to generate much more charac-ter strings that were in "IS due to an overgener-alllation.
At the other end of the spectrum, whenN is large, the learned model will describe the ex-amples in TS and them only.
So.
it is among theintermediate values of N that an optimum has tobe looked for,2Rltchle (1989) has even shown that thegenerative power of two-level morphological n-alyzers is strictly bound by that of finite stateautomata.
He proved that all languages L gener-ated by these analyzers are such that wheneverE^, E 3 and EIE2E3E.
belong to L, then E2E 3belongs to L.  a ough tins point was notconsidered in the present study, we may sup-pose that constraining the learned automaton torespect this last property, for example by meansof tying states, would improve the overall resultsby augmenting in a sound way the generaliza-tion from examples.w'  e f{w) iff3 x = x I ... x n E (C u {-}}*,Y = Yl "" Yn  E (C' u {-}}*such  thatxi:Y i is of the fo rm -:c' or c:-,fori = 1 ..... n,ProbM(Xl.T1 ... xn.Tn) 40,w = delete(x) and w' = delete(y)where the funct ion delete is defined asdelete(M = ~., (~.
is the empty  string),delete(-Z} = delete(Z) anddelete{zZ} = z.delete(Z} if z e C or z e C'Given a training set TS = {<w, w'> l wC*, w' ~ C'*}, the problem is thus  to findthe model M that  maximizes the probabil ityP= \]I max- -  .Prob..{x. : yl ...Xn: yn) (w, w~ ~e T s ~x,y~ M" iwhere delete(x) = w anddelete(y) = w'This formula makes  it clear what  isthe new difficulty with this type of learning,namely the indeterminat ion f words x andy, that  is of the a l ignment induced by thembetween w and its translat ion w'.
The no-t ions of partial and complete compatiblepaths  should thus  be redefined in order totake this into account.The partial paths  compatible with wand w' till t and J are now the set of se-quences t 1 ... tl+ !
, Pathlj(W, w') such thatO(t 1) = sl, D(t k) =O(tk+l), 'Jfor k= 1 .
.
.
.
.
l+J-1, S(tk)= Xk.Tk, for k = 1 .
.
.
.
.
t+J, dele-te(xl...xl+ 1}= wl .
.
.w I and  delete(Yl...Yl+j) =W'l.
.
.w I.
 partial path  is also complete assoon aS 1 = \[wl, J = Iw'\[ and D(t b~+ I~ l  ) = SF.As before, we can define the probabil-ity PM(W, 1, w', J, s) of reaching state s alonga partial path compatible with w and w' andgenerating the first I symbols in w and J  firstsymbols in w'.PM(W, i, w',J, s) = maxt l  ...ti +Jk <l~l +J P(tk)where (t 1...tt+ j e {Patht,j(w, w')\[ D(tl+j) = s I )PM(W, O, w', O, sl) = IPM(W, O, w', O, s) = O, if s~s  I- l lO -Here again, this probabil ity is suchthat PrObM(W, w') = PM(W, \]wl, w', \[w'l, Sv} andmay be computed (firough dynamic pro-gramming according to the formulaPM(W, i + I, w',J + I, s) =max ( maXt lPM(W' i 'w" J  + I"O ( t l ) )I maxt2PM(W , I + I, w',J, O (t2))where ( t l~  {t~'I~ D(t) = set  S(t) = w l+ l : - l )and ( t2a  {taT \ [D( t )  = set  S(t) =- :w j :+ l \ ] )It is now possible to compute for everytraining example the optimal path corre-sponding to a given probabil i ty distr ibutionp(t).
This path not only defines the crossedstates but  also the al ignment between wand w'.
The learning algorithm applicable togeneral markovian models  remains valid foradjust ing iteratively the probabil it ies p(t).EXPERIMENTSMorpho log ica l  ana lys i sAs a prel iminary experiment, themorphological analysis automaton  waslearned on a set of 738 French words end-ing with the morpheme "/sme" and :associ-ated with their decomposit ion i to two mor-phemes,  the first being a noun or anadjective.
For example, we had the pair<"athl~ttsme","athl~te+isme">.
With a 400states only automaton,  the correct decom-posit ion was found amongst  the 10 mostprobable outputs  for 97.6% of the trainingexamples !.Grapheme- to  -phonemet ranscr ip t ionThe case of grapheme-to-phonemetranscription is a straightforward applica-tion of the t ransduct ion model.
String w isthe graphetnic form, e.g.
"absten/r" and w'lWe are  aware  that  a more  prec ise  assess -ment  of the  method wou ld  use  a tes t  set  d i f fe rentfi 'om the t ra in ing  set .
We p lan  to per fo rm such  atest in the near future.is its transcript ion into phonemes,  e.g.
"ap-s ten iR"  o r  "absten iR" .
Here the trainingset may feature such pairs as <w, w'> and<w, w"> where w' ~ w".The automaton was learned on a setof 1170 acronyms associated to their pho-nemic form which was descr ibed in acoarse phonemic a lphabet  where, for exam-ple, open or closed /o /  are not dlstin-guished.
Acronyms raise an interestingproblem in that some should be spelled let-ter by letter ("ACL") whereas others may bepronounced ("COLING").
This experimentwas thus  intended to show that the modelmay take into account  its input  as a whole.With a 400 states only automaton,  morethan 50% of the training examples werecorrectly transcr ibed when only the mostprobable output  was considered.
This fig-ure may be improved by augment ing thenumber  of states in which case the learningphase becomes much longer.CONCLUSIONWe have proposed a method for leam-Ing t ransducers  for the tasks  of morpholog-ical analysis and grapheme-to-phonemetranscription.
This method may be favor-ably compared to others solut ions basedupon writing rules in the sense that it doesnot oblige to identify rules, it provides a re-sult  which is directly usab le  as a t ransduc-er and it allows to l is t /~anslat ions accord-ing to a decreasing order of probabil ity.
Yet,the learned automaton  does not lend itselfto an interpretation i  the form of symbolicrules - provided that such rules exist -.Moreover, some learning parameters  areset only as the results of empirical or ran-dom choices: number  of states, initial prob-ability distr ibution, etc.
Yet, other advan-tages weigh for the proposed method.
Theautomaton may take into account  thewhole word to be translated rather than alimited part of it - this Justifies that a set ofequivalent symbolic rules is hard to obtain-.
For example, the grapheme-to-phonemetranscript ion may recognize the originallanguage of a word while translating It(Oshlka et al 1988): the "French" nouns"meeting" and "carpacclo" have kept respec-tively their original English and Italian form- II1 -and pronunciation, etc.
The learned autom-aton is symmetrical, thus it Is also revers-ible.
In other words, the morphologicalanalysis automaton may also be used as agenerator and the grapheme-to-phonemeautomaton may become a phoneme-to-grapheme transducer.
Another emark ts inorder: since the automaton is reversible, itmay be composed with its inverse to form,for example, a grapheme-to-graphemetranslator that keeps the phonemic formconstant without actually computing it.Now, it has been shown elsewhere (Reapeand Thompson 1988) that the transducerthat would result is also describable in theformalism of finite state automata nd thatits number  of states has a upper boundwhich is the square of the number of statesin the base automaton.
(Reape and Thomp-son 1988) also describes an algorithm forcomputing the resulting automaton.
Lastly,other functions than morphological naly-sis or grapheme-to-phoneme transcriptionmay be envisioned like, for example, the de-composition of words into syllables or thecomputation of abbreviations by contrac-tion.REFERENCESBahl, L. R.; Jelinek, F.; and Mercer, R. L.1983.
"A Maximum Likelihood Approachto Continuous Speech Recognition," IEEETrans.
on Pattern Analysis and MachineIntelligence, 5(2\]: 179-190.Catach, N.
1984.
"La phon~tisation automa-Uque du fran~ais," CNRS.Danlos, L.; Laporte, E.; and Emerard, F.1986 "Synthesis of Spoken Messagesfrom Semantic Representations (Seman-tlc-Representation-to-Speech-System),"Proc.
of the 11 th Intern.
Conf.
on Computa-tional Linguistics, 599-604.Kirkpatrlck, S.; Gelatt, C. D.; and Vecchl,M.
P. 1983.
"Optimization by SimulatedAnnealing," Science, 220:671-680.Koskenniemi, K. 1983.
'~I'wo-Level Modelfor Morphological Analysis", Proc.
of theEighth Intern.
Joint Conf.
onArti.flcial Intel-ligence, 683-685.Laporte, E. 1988.
M(~thodes algorlthmlqueset lexlcales de phon~tlsation de textes,These de doctorat, Universit6 Paris 7.Oshlka, B. T.; Evans, B.; Machi, F.; andTom, J.
1988.
"Computational Tech-niques for Improved Name Search," Proc.of the Second Conf.
on Applied NaturalLang~J_oge Processing, 203-210.Reape, W. and Thompson, H. 1988.
"Paral-lel Intersection and Serial Composition ofFinite State: Transducers", Proc.
of COL-ING'88, 535-539.Ritchie G. D.; Pulman, S. G.; Black, A. W.;and Russell, G. J .
1987.
"A Computation~al Framework for Lexical Description",Computational Linguistics, 13(3-4):290-307.Ritchie, G. 1989.
"On the Generative Powerof Two-Level Morphological Rules,"Fourth Conf.
of the European Chapter ofthe ACL, 51-57.Tufts D. 1989.
"It Would Be Much Easier IfWENT Were GOED," Proc.
of the FourthConf.
of the European Chapter of the ACL,145-152.V6ronis, J.
1988.
"Correction of Phono-graphic Errors in Natural Language In-terfaces", 1 lth ACM-SIGIR Conf., I01-115.- 112-
