Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1056?1065,Singapore, 6-7 August 2009.c?2009 ACL and AFNLPA Compact Forest for Scalable Inferenceover Entailment and Paraphrase RulesRoy Bar-Haim?, Jonathan Berant?, Ido Dagan?
?Computer Science Department, Bar-Ilan University, Ramat Gan 52900, Israel{barhair,dagan}@cs.biu.ac.il?The Blavatnik School of Computer Science, Tel-Aviv University, Tel-Aviv 69978, Israeljonatha6@post.tau.ac.ilAbstractA large body of recent research has beeninvestigating the acquisition and applica-tion of applied inference knowledge.
Suchknowledge may be typically captured asentailment rules, applied over syntacticrepresentations.
Efficient inference withsuch knowledge then becomes a funda-mental problem.
Starting out from a for-malism for entailment-rule application wepresent a novel packed data-structure anda corresponding algorithm for its scalableimplementation.
We proved the validity ofthe new algorithm and established its effi-ciency analytically and empirically.1 IntroductionApplied semantic inference is concerned with de-riving target meanings from texts.
In the textualentailment framework, this is reduced to infer-ring a textual statement (the hypothesis h) froma source text (t).
Traditional formal semanticsapproaches perform such inferences over logi-cal forms derived from the text.
By contrast,most practical NLP applications operate over shal-lower representations such as parse trees, possiblysupplemented with limited semantic informationabout named entities, semantic roles etc.Most commonly, inference over such represen-tations is made by applying some kind of transfor-mations or substitutions to the tree or graph rep-resenting the text.
Such transformations may begenerally viewed as entailment (inference) rules,which capture semantic knowledge about para-phrases, lexical relations such as synonyms andhyponyms, syntactic variations etc.
Such knowl-edge is either composed manually, e.g.
WordNet(Fellbaum, 1998), or learned automatically.A large body of work has been dedicated tolearning paraphrases and entailment rules, e.g.
(Lin and Pantel, 2001; Shinyama et al, 2002;Szpektor et al, 2004; Bhagat and Ravichandran,2008), identifying appropriate contexts for theirapplication (Pantel et al, 2007) and utilizing themfor inference (de Salvo Braz et al, 2005; Bar-Haim et al, 2007).
Although current avail-able rule bases are still quite noisy and incom-plete, the progress made in recent years suggeststhat they may become increasingly valuable fortext understanding applications.
Overall, appliedknowledge-based inference is a prominent line ofresearch gaining much interest, with recent exam-ples including the series of workshops on Knowl-edge and Reasoning for Answering Questions(KRAQ)1and the planned evaluation of knowledgeresources in the forthcoming 5thRecognizing Tex-tual Entailment challenge (RTE-5)2.While many applied systems utilize semanticknowledge via such inference rules, their use istypically limited, application-specific, and quiteheuristic.
Formalizing these practices seems im-portant for applied semantic inference research,analogously to the role of well-formalized mod-els in parsing and machine translation.
Bar-Haimet al (2007) made a step in this direction by in-troducing a generic formalism for semantic infer-ence over parse trees.
Their formalism uses entail-ment rules as a unifying representation for varioustypes of inference knowledge, allowing unified in-ference as well.
In this formalism, rule applicationhas a clear, intuitive interpretation as generating anew sentence parse (a consequent), semanticallyentailed by the source sentence.
The inferred con-sequent may be subject to further rule applications1http://www.irit.fr/recherches/ILPL/kraq09.html2http://www.nist.gov/tac/2009/RTE/1056and so on.
In their implementation, each conse-quent was generated explicitly as a separate tree.Following this line of work, our long-term re-search goal is to investigate effective utilizationof entailment rules for inference.
While the for-malism of Bar-Haim et al provides a princi-pled framework for modeling such inferences,its implementation using explicit generation ofconsequents raises severe efficiency issues, sincethe number of consequents may grow exponen-tially in the number of rule applications.
Con-sider, for example, the sentence ?Children arefond of candies.
?, and the following entailmentrules: ?children?kids?, ?candies?sweets?, and ?Xis fond of Y?X likes Y?.
The number of derivablesentences (including the source sentence) wouldbe 23as each rule can either be applied or not, in-dependently.
Indeed, we found that this exponen-tial explosion leads to poor scalability in practice.Intuitively, we would like that each rule applica-tion would add just the entailed part (e.g.
kids) to apacked sentence representation.
Yet, we still wantthe resulting structure to represent a set of entailedsentences, rather than some mixture of sentencefragments whose semantics is unclear.As discussed in section 5, previous work pro-posed only partial solutions to this problem.
In thispaper we present a novel data structure, termedcompact forest, and a corresponding inference al-gorithm, which efficiently generate and representall consequents while preserving the identity ofeach individual one (section 3).
Our work isinspired by previous work on packed represen-tations in other fields, such as parsing, genera-tion and machine translation (section 5).
Aswe follow a well-defined formalism, we couldprove that all inference operations of Bar-Haimet al are equivalently applied over the compactforest.
We compare inference cost over compactforests to explicit consequent generation both the-oretically (section 3.4), illustrating an exponential-to-linear complexity ratio, and empirically (sec-tion 4), showing improvement by orders of magni-tude.
These results suggest that our data-structureand algorithm are both valid and scalable, open-ing up the possibility to investigate large-scale en-tailment rule application within a well-formalizedframework.2 Inference FrameworkThis section briefly presents a (simplified) descrip-tion of the tree transformations inference formal-ism of Bar-Haim et al (2007).
Given a source text,syntactically parsed, and a set of entailment rulesrepresenting tree transformations, the formalismdefines the set of consequents derivable from thetext using the rules.
Each consequent is obtainedthrough a sequence of rule applications, each gen-erates an intermediate parse tree, similar to a proofprocess in logic.More specifically, sentences are represented asdependency trees, where nodes are annotated withlemma and part-of-speech, and edges are anno-tated with dependency relation.
A rule ?L ?
R?is primarily composed of two templates, termedleft-hand-side (L), and right-hand-side (R).
Tem-plates are dependency subtrees which may con-tain POS-tagged variables, matching any lemma.Figure 1(a) shows passive-to-active transforma-tion rule, and (b) illustrates its application.A rule application generates a derived tree dfrom a source tree s through the following steps:L matching: First, a match of L in the sourcetree s is sought.
In our example, the variable V ismatched in the verb see, N1 is matched in Maryand N2 is matched in John.R instantiation: Next, a copy of R is generatedand its variables are instantiated according to theirmatching node in L. In addition, a rule may spec-ify alignments, defined as a partial function fromL nodes to R nodes.
An alignment indicates thatfor each modifier m of the source node that is notpart of the rule structure, the subtree rooted at mshould also be copied as a modifier of the targetnode.
In addition to defining alignments explic-itly, each variable in L is implicitly aligned to itscounterpart in R. In our example, the alignmentbetween the V nodes implies that yesterday (mod-ifying see) should be copied to the generated sen-tence, and similarly beautiful (modifying Mary) iscopied for N1.Derived tree generation: Let r be the instanti-ated R, along with its descendants copied from Lthrough alignment, and l be the subtree matchedby L. The formalism has two methods for gen-erating the derived tree d: substitution and intro-duction, as specified by the rule type.
Substitutionrules specify modification of a subtree of s, leav-ing the rest of s unchanged.
Thus, d is formed bycopying s while replacing l (and the descendants1057LV VERBobjssffffffffffbeby++XXXXXXXXXXRV VERBsubjssffffffffffobj++XXXXXXXXXXN1 NOUN be VERB by PREPpcomp?nN2 NOUN N1 NOUNN2 NOUN(a) Passive to active transformation (substitution rule)ROOTisee VERBobjqqccccccccccccccccccccbessffffffffffbymod++XXXXXXXXXXMary NOUNmodbe VERB by PREPpcomp?nyesterday NOUNbeautiful ADJ John NOUNROOTisee VERBsubjrreeeeeeeeeeeobjmod,,YYYYYYYYYYYJohn NOUN Mary NOUNmodyesterday NOUNbeautiful ADJSource: Beautiful Mary was seen by John yesterday.
Derived: John saw beautiful Mary yesterday.
(b) Application of passive to active transformationFigure 1: An inference rule application.
POS and relation labels are based on Minipar (Lin, 1998)of l?s nodes) with r. This is the case for the pas-sive rule, as well as for lexical rules such as ?buy?
purchase?.
By contrast, introduction rules areused to make inferences from a subtree of s, whilethe other parts of s are ignored and do not affect d.A typical example is inferring a proposition em-bedded as a relative clause in s. In this case, thederived tree d is simply taken to be r.In addition to inference rules, the formalism in-cludes annotation rules which add features to ex-isting parse tree nodes.
These rules have been usedfor identifying contexts that affect the polarity ofpredicates.As shown by Bar-Haim et al, this concise, welldefined formalism allows unified representation ofdiverse types of knowledge which are commonlyused for applied semantic inference.3 Efficient Inference over Compact ParseForestsAs shown in the introduction, explicit genera-tion of consequents (henceforth explicit inference)leads to an exponential explosion of the numberof generated trees.
In this section we present ourefficient implementation for this formalism.
Ourimplementation is based on a novel data structure,termed compact forest (Section 3.1), which com-pactly represents a large set of trees.
Each ruleapplication generates explicitly only the nodes ofthe rule right-hand-side while the rest of the con-sequent tree is shared with the source, which alsoreduces the number of redundant rule applications.As we shall see, this novel representation is basedprimarily on disjunction edges, an extension ofdependency edges that specify a set of alterna-tive edges of multiple trees.
Section 3.2 presentsan efficient algorithm for inference over compactforests, followed by a discussion of its correctnessand complexity (sections 3.3 and 3.4).3.1 The Compact Forest Data StructureA compact forestF represents a set of dependencytrees.
Figure 2 shows an example of a compactforest, containing both the source and derived sen-tences of Figure 1.
We first define a more generaldata structure for directed graphs, and then narrowthe definition to the case of trees.A Compact Directed Graph (cDG) is a pairG = (V, E) where V is a set of nodes and E is aset of disjunction edges (d-edges).
Let D be aset of dependency relations.
A d-edge d is a triple(Sd, reld, Td), where Sdand Tdare disjoint setsof source nodes and target nodes; reld: Sd?
Dis a function specifying the dependency relationcorresponding to each source node.
Graphically,d-edges are shown as point nodes, with incomingedges from source nodes and outgoing edges totarget nodes.
For instance, let d be the bottom-most d-edge in Figure 3.
Then Sd= {of, like},Td= {candy, sweet}, rel(of ) = pcomp-n, andrel(like) = obj.A d-edge represents, for each si?
Sd, a set of1058ROOTiJohnseeby objbe modbypcomp-nbeautifulMarymodbe yesterdayseesubjobjmodFigure 2: A compact forest containing both thesource and derived sentences of Figure 1.
Partsof speech are omitted.alternative directed edges {(si, tj) : tj?
Td}, allof which are labeled with the same relation givenby reld(si).
Each of these edges, termed embed-ded edge (e-edge), would correspond to a differ-ent graph represented in G. In the previous exam-ple, the e-edges are likeobj??
?candy, likeobj???sweet,ofpcomp?n??????
?candy and ofpcomp?n??????
?sweet (noticethat the definition implies that all source nodes inSdhave the same set of alternative target nodesTd).
d is called an outgoing d-edge of a node v ifv ?
Sdand an incoming d-edge of v if v ?
Td.A Compact Directed Acyclic Graph (cDAG) is acDG that contains no cycles of e-edges.A DAG G rooted in a node v ?
V of a cDAGG is embedded in G if it can be derived as follows:we initialize G with v alone; then, we expand vby choosing exactly one target node t ?
Tdfromeach outgoing d-edge d of v, and adding t and thecorresponding e-edge (v, t) to G. This expansionprocess is repeated recursively for each new nodeadded to G.Each such set of choices results in a differentDAG with v as its only root.
In Figure 2, we maychoose to connect the root either to the left see,resulting in the source passive sentence, or to theright see, resulting in the derived active sentence.A Compact Forest F is a cDAG with a singleroot r (i.e.
r has no incoming d-edges) where allthe embedded DAGs rooted in r are trees.
This setof trees, termed embedded trees, comprise the setof trees represented by F .Figure 3 shows another example for a compactROOTichildbepredfondsubjmodofpcomp-ncandylikesubjobjkidsweetFigure 3: A compact forest representing the 23sentences derivable from the sentence ?childrenare fond of candies?
using the following threerules: ?children?kids?, ?candies?sweets?, and ?Xis fond of Y?X likes Y?.forest efficiently representing the 23sentences re-sulting from three independently-applied rules.3.2 The Inference ProcessNext, we describe the algorithm implementing theinference process of Section 2 over the compactforest (henceforth, compact inference), illustratingit through Figures 1 and 2.Forest initialization F is initialized with theset of dependency trees representing the text sen-tences, with their roots connected under the forestroot as the target nodes of a single d-edge.
Depen-dency edges are transformed trivially to d-edgeswith a single source and target.
Annotation rulesare applied at this stage to the initial F .
The blackpart of Figure 2 corresponds to the initial forest.Rule application comprises the following steps:L matching: L is matched in F if there existsan embedded tree t in F such that L is matchedin t, as in Section 2.
We denote by l the subtreeof t in which L was matched.
As in section 2, thematch in our example is (V,N1, N2)=(see, Mary,John).
Notice that this definition does not allow lto be scattered over multiple embedded trees.As the target nodes of a d-edge specify alterna-tives for the same position in the tree, their parts-of-speech are expected to be of substitutable types.1059In this paper we further assume that all targetnodes of the same d-edge have the same part-of-speech3.
Consequently, variables that are leaves inL and may match a certain target node of a d-edged are mapped to the whole set of target nodes Tdrather than to a single node.
This yields a compactrepresentation of multiple matches, and preventsredundant rule applications.
For instance, givena compact representation of ?
{Children/kids} arefond of {candies/sweeets}?
(cf.
Figure 3), the rule?X is fond of Y?X likes Y?
will be matched andapplied only once, rather than four times (for eachcombination of matching X and Y ).Derived tree generation: A template r consist-ing of R while excluding variables that are leavesof both L and R (termed dual leaf-variables)4isgenerated and inserted into F .
In case of a substi-tution rule (as in our example), r is set as an alter-native to l by adding r?s root to Td, where d is theincoming d-edge of l?s root.
In case of an intro-duction rule, it is set as an alternative to the othertrees in the forest by adding r?s root to the targetnode set of the forest root?s outgoing d-edge.
Inour example, r is the gray node (still labeled withthe variable V ) , and it becomes an additional tar-get node of the d-edge entering the original (left)see.Variable instantiation: Each variable in r (i.e.a non-dual leaf) is instantiated according to itsmatch in L (as in Section 2), e.g.
V is instantiatedwith see.
As specified above, if the variable is aleaf inL is not a dual leaf then it is matched in a setof nodes, and hence each of them should be instan-tiated in r. This is decomposed into a sequenceof simpler operations: first, r is instantiated with arepresentative from the set, and then we apply (ad-hoc) lexical substitution rules for creating a newnode for each other node in the set5.Alignment sharing: Modifiers of aligned nodesare shared (rather than copied) as follows.
Givena node nLin l aligned to a node nRin r, and anoutgoing d-edge d of nLwhich is not part of l, weshare d between nLand nRby adding nRto Sd3This is the case in our current implementation, which isbased on the coarse tag-set of Minipar (Lin, 1998).4With the following exceptions: variables that are theonly node in R (and hence are both the root and a leaf), andvariables with additional alignments (other than the implicitalignment between their occurrences in L and R) are not con-sidered dual-leaves.5Notice that these nodes, in addition to the usual align-ment with their source nodes in l, share the same daughtersin r.and setting reld(nR) = reld(nL).
This is illus-trated by the sharing of yesterday in Figure 2.
Wealso copy annotation features from nLto nR.We note at this point that the instantiation ofvariables that are not dual leaves (e.g.
V in ourexample) cannot be shared because they typicallyhave different modifiers at the two sides of therule.
Yet, their modifiers which are not part ofthe rule are shared through the alignment opera-tion (recall that common variables are always con-sidered aligned).
Dual leaf variables, on the otherhand, might be shared, as described next, since therule doesn?t specify any modifiers for them.Dual leaf variable sharing: This final step isperformed analogously to alignment sharing.
Sup-pose that a dual leaf variable X is matched in anode v in l whose incoming d-edge is d. Thenwe simply add the parent p of X in r to Sdandset reld(p) to the relation between p and X (inR).
Since v itself is shared, its modifiers becomeshared as well, implicitly implementing the align-ment operation.
The subtrees beautiful Mary andJohn are shared this way for variablesN1 andN2.Applying the rule in our example added onlya single node and linked it to four d-edges, com-pared to duplicating the whole tree in explicit in-ference.3.3 CorrectnessIn this section we present two theorems, whichprove that the inference algorithm is a valid imple-mentation of the inference formalism of Section 2.Due to space limitations, the proofs themselvesare omitted, and instead we outline their generalscheme.We first argue that performing any sequence ofrule applications over the set of initial trees resultsin a compact forest:Theorem 1: The compact inference processgenerates a compact forest.Proof scheme: We prove by induction on thenumber of rule applications.
Initialization gen-erates a single-rooted cDAG, whose embeddedDAGs are all trees, as required.
We then prove thatif applying a rule on a compact forest creates a cy-cle or an embedded DAG that is not a tree, thensuch a cycle or a non-tree DAG already existedprior to rule application, in contradiction with theinductive assumption.
A crucial observation forthis proof is that for any path from a node u to anode v that passes through r, where u and v are1060outside r, there is also an analogous path from uto v that passes through l instead, QED.Next, we argue that the inference process over acompact forest is complete and sound, i.e., it gen-erates the set of consequents derivable from a textaccording to the inference formalism.Theorem 2: Given a rule base R and a set ofinitial trees T , a tree t is embedded in a compactforest derivable from T by the compact inferenceprocess?
t is a consequent of T according to theinference formalism.Proof scheme: We first show completeness byinduction on the number of explicit rule applica-tions.
Let tn+1be a tree derived from a tree tnusing the rule rnaccording to the inference for-malism.
The inductive assumption asserts that tnis embedded in some derivable compact forest F .It is easy to verify that applying rnto F will yielda compact forest F?in which tn+1is embedded.Next, we show soundness by induction on thenumber of rule applications over the compact for-est.
Let tn+1be a tree represented in some derivedcompact forest Fn+1.
Fn+1was derived from thecompact forest Fn, using the rule rn.
It can beshown that Fnrepresents a tree tn, such that ap-plying rnon tnwill yield tn+1according to theformalism.
The inductive assumption asserts thattnis a consequent in the inference formalism andtherefore tn+1is a consequent as well, QED.These two theorems guarantee that the compactinference process is valid - i.e., it yields a compactforest that represents the set of consequents deriv-able from a given text by a given rule set.3.4 ComplexityIn this section we explain why compact inferenceexponentially reduces the time and space com-plexity in typical scenarios.We consider a set of rule matches in a tree Tindependent if their matched left-hand-sides (ex-cluding dual-leaf variables) do not overlap in T ,and their application over T can be chained in anyorder.
For example, the three rule matches pre-sented in Figure 3 are independent.Let us consider explicit inference first.
Assumewe start with a single tree T with k independentrules matched.
Applying k rules will yield 2ktrees, since any subset of the rules might be ap-plied to T .
Therefore, the time and space com-plexity of applying k independent rule matches is?(2k).
Applying more rules on the newly derivedCompact Explicit RatioTime (msec) 61 24,184 396Rule applications 12 123 10Node count 69 5,901 86Edge endpoints 141 11,552 82Table 1: Compact vs. explicit inference, us-ing generic rules.
Results are averaged per text-hypothesis pair.consequents behaves in a similar manner.Next, we examine compact inference.
Apply-ing a rule using compact inference adds the right-hand-side of the rule and shares with it existingd-edges.
Since that the size of the right-hand-sideand the number of outgoing d-edges per node arepractically bounded by low constants, applying krules on a tree T yields a linear increase in the sizeof the forest.
Thus, the resulting size isO(|T |+k),as we can see from Figure 3.The time complexity of rule application is com-posed of matching the rule in the forest and apply-ing the matched rule.
Applying a matched rule islinear in its size.
Matching a rule of size r in aforest F takes O(|F|r) time even when perform-ing an exhaustive search for matches in the forest.Since r tends to be quite small and can be boundedby a low constant, this already gives polynomialtime complexity.
In practice, indexing the forestnodes, as well as the typical low connectivity ofthe forest, result in a very fast matching procedure,as illustrated in the empirical evaluation, describednext.4 Empirical EvaluationThis section reports empirical evaluation of the ef-ficiency of compact inference, tested in the recog-nizing textual entailment setting using the RTE-3and RTE-4 datasets (Giampiccolo et al, 2007; Gi-ampiccolo et al, 2009).
These datasets consist of(text, hypothesis) pairs, which need to be classi-fied as entailing/non entailing.
Our first experi-ment shows, using a small rule set, that compactinference outperforms explicit inference by ordersof magnitude (Section 4.1).
The second experi-ment shows that compact inference scales well toa full-blown RTE setting with several large-scalerule bases, where up to hundreds of rules are ap-plied for a text (Section 4.2).10614.1 Compact vs.
Explicit InferenceTo compare explicit and compact inference werandomly sampled 100 pairs from the RTE-3 de-velopment set, and parsed the text in each pairusing Minipar (Lin, 1998).
We used a set ofmanually-composed entailment rules for inferenceover generic linguistic phenomena such as pas-sive, conjunction, relative clause, apposition, pos-sessives, and determiners, which contains a fewdozens of rules.
To make a fair comparison, weaimed to make the explicit inference implementa-tion reasonably efficient, e.g.
by preventing gen-eration of the same tree by different permutationsof the same rule applications.
Both configurationsperform rule application iteratively, until no newmatches are found.
In each iteration we first findall rule matches and then apply all matching rules.We compare run time, number of rule applications,and the overall generated size of nodes and edges,where edge size is represented by the sum of itsendpoints.The results are summarized in Table 1.
As ex-pected, the results show that compact inference isby orders of magnitude more efficient than explicitinference.
To avoid memory overflow, inferencewas terminated after reaching 100,000 nodes.
3out of the 100 pairs reached that limit with explicitinference, while the maximal node count for com-pact inference was only 268.
The number of ruleapplications is reduced thanks to the sharing ofcommon subtrees in the compact forest, by whicha single rule application operates simultaneouslyover a large number of embedded trees.
The re-sults suggest that scaling to larger rule bases andlonger inference chains would be feasible for com-pact inference, but prohibitive for explicit infer-ence.4.2 Application to an RTE SystemExperimental setting The goal of the secondexperiment was to assess that compact inferencescales well for broad entailment rule bases.
Inthis experiment we used the Bar-Ilan RTE system(Bar-Haim et al, 2009).
The system operates intwo primary stages: Inference, in which entail-ment rules are applied to the initial compact forestF , aiming to bring it closer to the hypothesis H,and Classification, in which a set of features is ex-tracted from the resulting F and from H and fedinto an SVM classifier, which determines entail-ment.The classification setting and its features arequite typical for the RTE literature.
They includelexical and structural measures for the coverage ofH by F , where high coverage is assumed to cor-relate with entailment, as well as features aimingto detect inconsistencies between F and H suchas incompatible arguments for the same predicateor incompatible verb polarity (see below).
For acomplete feature description, see (Bar-Haim et al,2009).Rule Bases In addition to the generic rules de-scribed in Section 4.1, the following large-scalesources for entailment rules were used: Wikipeda:We used the lexical rulebase of Shnarch et al(2009), who extracted rules such as ?Janis Joplin?
singer?
from Wikipedia based on both its meta-data (e.g.
links and redirects) and text defini-tions, using patterns such as ?X is a Y?.
Word-Net: We extracted from WordNet (Fellbaum,1998) lexical rules based on synonyms, hyper-nyms and derivation relations.
DIRT: The DIRTalgorithm (Lin and Pantel, 2001) learns from acorpus entailment rules between binary predicates,e.g.
?X is fond of Y?X likes Y?.
We used theversion described in (Szpektor and Dagan, 2007),which learns canonical rule forms.
Argument-MappedWordNet (AmWN):A resource for entail-ment rules between verbal and nominal predicates(Szpektor and Dagan, 2009), including their argu-ment mapping, based on WordNet and NomLex-plus (Meyers et al, 2004), verified statisticallythrough intersection with the unary-DIRT algo-rithm (Szpektor and Dagan, 2008).
In total, theserule bases represent millions of rules.
Polarity An-notation Rules: We compiled a small set of anno-tation rules for marking the polarity of predicatesas negative or unknown due to verbal negation,modal verbs, conditionals etc.
(Bar-Haim et al,2009).Search In this work we focus on efficient rep-resentation of the search space, leaving for futurework the complementary problem of devising ef-fective search heuristics over our representation.In the current experiment we implemented a sim-ple search strategy, in the spirit of (de Salvo Brazet al, 2005): first, we applied three exhaustive iter-ations of generic rules.
Since these rules have lowfan-out (few possible right-hand-sides for a givenleft-hand-side) it is affordable to apply and chainthem more freely.
We then perform a single itera-tion of all other lexical and lexical-syntactic rules,1062applying them only if their L part was matched inF and their R part was matched inH.The system was trained over the RTE-3 devel-opment set, and tested on both RTE-3 test set andRTE-4 (which includes only a test set).Results Table 2 provides statistics on rule appli-cation using all rule bases, over the RTE-3 devel-opment set and the RTE-4 dataset6.
Overall, theprimary result is that the compact forest indeed ac-commodates well extensive rule application fromlarge-scale rule bases.
The resulting forest size iskept small, even in the maximal cases which werecausing memory overflow for explicit inference.The accuracies obtained in this experiment andthe overall contribution of rule-based inference areshown in Table 3.
The results on RTE-3 are quitecompetitive: compared to our 66.4%, only 3 teamsout of the 26 who participated in RTE-3 scoredhigher than 67%, and three more systems scoredbetween 66% and 67%.
The results for RTE4 rank9-10 out of 26, with only 6 teams scoring higher bymore than 1%.
Overall, these results validate thatthe setting of our experiment represents a state-of-the-art system.Inference over the rule bases utilized in ourexperiment improved the accuracy on both testsets.
The contribution was more prominent forthe RTE-4 dataset.
These results illustrate a typ-ical contribution of current knowledge sources forcurrent RTE systems.
This contribution is likelyto increase with current and near future research,on topics such as extending and improving knowl-edge resources, applying them only in seman-tically suitable contexts, improved classificationfeatures and broader search strategies.
As for ourcurrent experiment, we may conclude that the goalof assessing the compact forest scalability in astate-of-the-art setting was achieved7.Finally, Tables 4 and 5 illustrate the usage andcontribution of individual rule bases.
Table 4shows the distribution of rule applications over thevarious rule bases.
Table 5 presents ablation studyshowing the marginal accuracy gain for each rulebase.
These results show that each of the rulebases is applicable for a large portion of the pairs,and contributes to the overall accuracy.6Running time is omitted since most of it was dedicatedto rule fetching, which was rather slow for our available im-plementation of some resources.
The elapsed time was a fewseconds per (t, h) pair.7We note that common RTE research issues, such as im-proving accuracy, fall out of the scope of the current paper.RTE3-Dev RTE4Avg.
Max.
Avg.
Max.Rule applications 14 275 15 110Node count 71 606 80 357Edge endpoints 155 1,741 173 1,062Table 2: Application of compact inference to theRTE-3 Dev.
and RTE-4 datasets, using all ruletypes.AccuracyTest set No inference Inference ?RTE3 64.6% 66.4% 1.8%RTE4 57.5% 60.6% *3.1%Table 3: Inference contribution to RTE perfor-mance.
The system was trained on the RTE-3 de-velopment set.
* indicates statistically significantdifference (at level p < 0.02, using McNemar?stest).Rule base RTE3-Dev RTE4Rules App Rules AppWordNet 0.6 1.2 0.6 1.1AmWN 0.3 0.4 0.3 0.4Wikipedia 0.6 1.7 0.6 1.3DIRT 0.5 0.7 0.5 1.0Generic 4.7 10.4 5.4 11.5Polarity 0.2 0.2 0.2 0.2Table 4: Average number of rule applications per(t, h) pair, for each rule base.
App counts each ruleapplication, while Rules ignores multiple matchesof the same rule in the same iteration.Rule base ?Accuracy (RTE4)WordNet 0.8%AmWN 0.7%Wikipedia 1.0%DIRT 0.9%Generic 0.4%Polarity 0.9%Table 5: Contribution of various rule bases.
Re-sults show accuracy loss on RTE-4, obtained forremoving each rule base (ablation tests).5 Related WorkThis section discusses related work on applyingknowledge-based transformations within RTE sys-tems, as well as on using packed representations inother NLP tasks.RTE Systems Previous RTE systems usually re-stricted both the type of allowed transformationsand the search space.
Systems based on lexical(word-based or phrase-based) matching of h in ttypically applied only lexical rules (without vari-1063ables), where both sides of the rule are matcheddirectly in t and h (Haghighi et al, 2005; Mac-Cartney et al, 2008).
The inference formalismwe use is more expressive, allowing also syntac-tic and lexical-syntactic transformations as well asrule chaining.Hickl (2008) derived from a given (t, h) paira small set of discourse commitments, which arequite similar to the kind of consequents we deriveby our syntactic and lexical-syntactic rules.
Thecommitments were generated by several differenttools and techniques, compared to our generic uni-fied inference process, and commitment genera-tion efficiency was not discussed.Braz et al (2005) presented a semantic infer-ence framework which ?augments?
the text repre-sentation with only the right-hand-side of an ap-plied rule, and in this respect is similar to ours.However, in their work, both rule application andthe semantics of the resulting ?augmented?
struc-ture were not fully specified.
In particular, the dis-tinction between individual consequents was lostin the augmented graph.
By contrast, our com-pact inference is fully formalized and is provablyequivalent to an expressive, well-defined formal-ism operating over individual trees, and each in-ferred consequent can be recovered from the com-pact forest.Packed representations Packed representationsin various NLP tasks share common principles,which also underly our compact forest: factor-ing out common substructures and representingchoice as local disjunctions.
Applying this gen-eral scheme to individual problems typically re-quires specific representations and algorithms, de-pending on the type of alternatives that should berepresented and the specified operations for creat-ing them.
In our work, alternatives are created byrule application, where a newly derived subtree isset as an alternative to existing subtrees.
Alterna-tives are specified locally using d-edges.Packed chart representations for parse forestswere introduced in classical parsing algorithmssuch as CYK and Earley (Jurafsky and Martin,2008), and have been extended in later workfor various purposes (Maxwell III and Kaplan,1991; Kay, 1996).
Alternatives in the parse chartstem from syntactic ambiguities, and are speci-fied locally as the possible decompositions of eachphrase into its sub-phrases.Packed representations have been utilized alsoin transfer-based machine translation.
Emele andDorna (1998) translated packed source languagerepresentation to packed target language represen-tation while avoiding unnecessary unpacking dur-ing transfer.
Unlike our rule application, in theirwork transfer rules preserve ambiguity stemmingfrom source language, rather than generating newalternatives.
Mi et al(2008) applied statistical ma-chine translation to a source language parse forest,rather than to the 1-best parse.
Their transfer rulesare tree-to-string, contrary to our tree-to-tree rules,and chaining is not attempted (rules are applied ina single top-down pass over the source forest), andthus their representation and algorithms are quitedifferent from ours.6 ConclusionThis work addresses the efficiency of entailmentand paraphrase rule application.
We presented anovel compact data structure and a rule applicationalgorithm for it, which are provably a valid imple-mentation of a generic inference formalism.
We il-lustrated inference efficiency both analytically andempirically.
Beyond entailment inference, we sug-gest that the compact forest would also be use-ful in generation tasks (e.g.
paraphrasing).
Ourefficient representation of the consequent searchspace opens the way to future investigation of thebenefit of larger-scale rule chaining, and to the de-velopment of efficient search strategies required tosupport such inferences.AcknowledgmentsThis work was partially supported by thePASCAL-2 Network of Excellence of the Eu-ropean Community FP7-ICT-2007-1-216886, theFBK-irst/Bar-Ilan University collaboration andthe Israel Science Foundation grant 1112/08.
Thesecond author is grateful to the Azrieli Foundationfor the award of an Azrieli Fellowship.
The au-thors would like to thank Yonatan Aumann, MarcoPennacchiotti and Marc Dymetman for their valu-able feedback on this work.ReferencesRoy Bar-Haim, Ido Dagan, Iddo Greental, and EyalShnarch.
2007.
Semantic inference at the lexical-syntactic level.
In Proceedings of AAAI.Roy Bar-Haim, Jonathan Berant, Ido Dagan, IddoGreental, Shachar Mirkin, Eyal Shnarch, and Idan1064Szpektor.
2009.
Efficient semantic deduction andapproximate matching over compact parse forests.In Proceedings of the First Text Analysis Conference(TAC 2008).Rahul Bhagat and Deepak Ravichandran.
2008.
Largescale acquisition of paraphrases for learning surfacepatterns.
In Proceedings of ACL-08: HLT.Rodrigo de Salvo Braz, Roxana Girju, Vasin Pun-yakanok, Dan Roth, and Mark Sammons.
2005.
Aninference model for semantic entailment in naturallanguage.
In Proceedings of AAAI.Martin C. Emele and Michael Dorna.
1998.
Ambi-guity preserving machine translation using packedrepresentations.
In Proceedings of Coling-ACL.Christiane Fellbaum, editor.
1998.
WordNet: An Elec-tronic Lexical Database.
Language, Speech andCommunication.
MIT Press.Danilo Giampiccolo, Bernardo Magnini, Ido Dagan,and Bill Dolan.
2007.
The Third PASCAL Recog-nizing Textual Entailment Challenge.
In Proceed-ings of the ACL-PASCAL Workshop on Textual En-tailment and Paraphrasing.Danilo Giampiccolo, Hoa Trang Dang, BernardoMagnini, Ido Dagan, and Bill Dolan.
2009.
TheFourth PASCAL Recognizing Textual EntailmentChallenge.
In Proceedings of the First Text Analy-sis Conference (TAC 2008).Aria D. Haghighi, Andrew Y. Ng, and Christopher D.Manning.
2005.
Robust textual inference via graphmatching.
In Proceedings of EMNLP.Andrew Hickl.
2008.
Using discourse commitmentsto recognize textual entailment.
In Proceedings ofCOLING.Daniel Jurafsky and James H. Martin.
2008.
Speechand Language Processing: An Introduction to Nat-ural Language Processing, Computational Linguis-tics and Speech Recognition.
Prentice Hall, secondedition.Martin Kay.
1996.
Chart generation.
In Proceedingsof ACL.Dekang Lin and Patrick Pantel.
2001.
Discovery of in-ference rules for question answering.
Natural Lan-guage Engineering, 7(4):343?360.Dekang Lin.
1998.
Dependency-based evaluation ofminipar.
In Proceedings of the Workshop on Evalu-ation of Parsing Systems at LREC.Bill MacCartney, Michel Galley, and Christopher D.Manning.
2008.
A phrase-based alignment modelfor natural language inference.
In Proceedings ofEMNLP.John T. Maxwell III and Ronald M. Kaplan.
1991.A method for disjunctive constraint satisfaction.
InMasaru Tomita, editor, Current Issues in ParsingTechnology.
Kluwer Academic Publishers.A.
Meyers, R. Reeves, Catherine Macleod, RachelSzekeley, Veronkia Zielinska, and Brian Young.2004.
The cross-breeding of dictionaries.
In Pro-ceedings of LREC.Haitao Mi, Liang Huang, and Qun Liu.
2008.
Forest-based translation.
In Proceedings of ACL-08: HLT.Patrick Pantel, Rahul Bhagat, Bonaventura Coppola,Timothy Chklovski, and Eduard Hovy.
2007.
ISP:Learning inferential selectional preferences.
In Pro-ceedings of NAACL-HLT.Yusuke Shinyama, Satoshi Sekine, Kiyoshi Sudo, andRalph Grishman.
2002.
Automatic paraphrase ac-quisition from news articles.
In Proceedings of Hu-man Language Technology Conference (HLT 2002),San Diego, USA.Eyal Shnarch, Libby Barak, and Ido Dagan.
2009.
Ex-tracting lexical reference rules from Wikipedia.
InProceedings of ACL-IJCNLP.Idan Szpektor and Ido Dagan.
2007.
Learning canon-ical forms of entailment rules.
In Proceedings ofRANLP.Idan Szpektor and Ido Dagan.
2008.
Learning entail-ment rules for unary templates.
In Proceedings ofCOLING.Idan Szpektor and Ido Dagan.
2009.
AugmentingWordNet-based inference with argument mapping.In Proceedings of ACL-IJCNLP Workshop on Ap-plied Textual Inference (TextInfer).Idan Szpektor, Hristo Tanev, Ido Dagan, and Bonaven-ture Coppola.
2004.
Scaling web based acquisitionof entailment patterns.
In Proceedings of EMNLP.1065
