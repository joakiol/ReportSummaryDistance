Proceedings of the Eighth Workshop on Statistical Machine Translation, pages 213?218,Sofia, Bulgaria, August 8-9, 2013 c?2013 Association for Computational LinguisticsThe CNGL-DCU-Prompsit Translation Systems for WMT13Raphael Rubino?, Antonio Toral?, Santiago Cort?s Va?llo?,Jun Xie?, Xiaofeng Wu?, Stephen Doherty[, Qun Liu?
?NCLT, Dublin City University, Ireland?Prompsit Language Engineering, Spain?ICT, Chinese Academy of Sciences, China?,[CNGL, Dublin City University, Ireland?,?
{rrubino, atoral, xfwu, qliu}@computing.dcu.ie?santiago@prompsit.com?junxie@ict.ac.cn[stephen.doherty@dcu.ieAbstractThis paper presents the experiments con-ducted by the Machine Translation groupat DCU and Prompsit Language Engineer-ing for the WMT13 translation task.
Threelanguage pairs are considered: Spanish-English and French-English in both direc-tions and German-English in that direc-tion.
For the Spanish-English pair, the useof linguistic information to select paral-lel data is investigated.
For the French-English pair, the usefulness of the small in-domain parallel corpus is evaluated, com-pared to an out-of-domain parallel datasub-sampling method.
Finally, for theGerman-English system, we describe ourwork in addressing the long distance re-ordering problem and a system combina-tion strategy.1 IntroductionThis paper presents the experiments conductedby the Machine Translation group at DCU1 andPrompsit Language Engineering2 for the WMT13translation task on three language pairs: Spanish-English, French-English and German-English.For these language pairs, the language and trans-lation models are built using different approachesand datasets, thus presented in this paper in sepa-rate sections.In Section 2, the systems built for the Spanish-English pair in both directions are described.
Weinvestigate the use of linguistic information to se-lect parallel data.
In Section 3, we present the sys-tems built for the French-English pair in both di-1http://www.nclt.dcu.ie/mt/2http://www.prompsit.com/rections.
The usefulness of the small in-domainparallel corpus is evaluated, compared to an out-of-domain parallel data sub-sampling method.
InSection 4, for the German-English system, aimingat exploring the long distance reordering problem,we first describe our efforts in a dependency tree-to-string approach, before combining different hi-erarchical systems with a phrase-based system andshow a significant improvement over three base-line systems.2 Spanish-EnglishThis section describes the experimental setup forthe Spanish-English language pair.2.1 SettingOur setup uses the MOSES toolkit, version1.0 (Koehn et al 2007).
We use a pipelinewith the phrase-based decoder with standard pa-rameters, unless noted otherwise.
The decoderuses cube pruning (-cube-pruning-pop-limit 2000-s 2000), MBR (-mbr-size 800 -mbr-scale 1) andmonotone at punctuation reordering.Individual language models (LMs), 5-gram andsmoothed using a simplified version of the im-proved Kneser-Ney method (Chen and Goodman,1996), are built for each monolingual corpus usingIRSTLM 5.80.01 (Federico et al 2008).
TheseLMs are then interpolated with IRSTLM usingthe test set of WMT11 as the development set.
Fi-nally, the interpolated LMs are merged into oneLM preserving the weights using SRILM (Stol-cke, 2002).We use all the parallel corpora available forthis language pair: Europarl (EU), News Com-mentary (NC), United Nations (UN) and CommonCrawl (CC).
Regarding monolingual corpora, weuse the freely available monolingual corpora (Eu-213roparl, News Commentary, News 2007?2012) aswell as the target side of several parallel corpora:Common Crawl, United Nations and 109 French?English corpus (only for English as target lan-guage).
Both the parallel and monolingual dataare tokenised and truecased using scripts from theMOSES toolkit.2.2 Data selectionThe main contribution in our participation regardsthe selection of parallel data.
We follow theperplexity-based approach to filter monolingualdata (Moore and Lewis, 2010) extended to filterparallel data (Axelrod et al 2011).
In our case, wedo not measure perplexity only on word forms butalso using different types of linguistic information(lemmas and named entities) (Toral, 2013).We build LMs for the source and target sidesof the domain-specific corpus (in our case NC)and for a random subset of the non-domain-specific corpus (EU, UN and CC) of the same size(number of sentences) of the domain-specific cor-pus.
Each parallel sentence s in the non-domain-specific corpus is then scored according to equa-tion 1 where PPIsl(s) is the perplexity of s inthe source side according to the domain-specificLM and PPOsl(s) is the perplexity of s in thesource side according to the non-domain-specificLM.
PPItl(s) and PPOtl(s) contain the corre-sponding values for the target side.score(s) = 12 ?
(PPIsl(s)?
PPOsl(s))+(PPItl(s)?
PPOtl(s)) (1)Table 1 shows the results obtained using fourmodels: word forms (forms), forms and named en-tities (forms+nes), lemmas (lem) and lemmas andnamed entities (lem+nes).
Details on these meth-ods can be found in Toral (2013).For each corpus we selected two subsets (see inbold in Table 1), the one for which one methodobtained the best perplexity (top 5% of EU us-ing forms, 2% of UN using lemmas and 50% ofCC using forms and named entities) and a big-ger one used to compare the performance in SMT(top 14% of EU using lemmas and named entities(lem+nes), top 12% of UN using forms and namedentities and the whole CC).
These subsets are usedas training data in our systems.As we can see in the table, the use of lin-guistic information allows to obtain subsets withlower perplexity than using solely word forms, e.g.1057.7 (lem+nes) versus 1104.8 (forms) for 14%of EU.
The only exception to this is the subset thatcomprises the top 5% of EU, where perplexity us-ing word forms (957.9) is the lowest one.corpus size forms forms+nes lem lem+nesEU 5% 957.9 987.2 974.3 1005.514% 1104.8 1058.7 1111.6 1057.7UN 2% 877.1 969.6 866.6 962.212% 1203.2 1130.9 1183.8 1131.6CC 50% 573.0 547.2 574.5 546.4100% 560.1 560.1 560.1 560.1Table 1: Perplexities in data selection2.3 ResultsTable 2 presents the results obtained.
Note thatthese were obtained during development and thusthe systems are tuned on WMT?s 2011 test set andtested on WMT?s 2012 test set.All the systems share the same LM.
The firstsystem (no selection) is trained with the whole NCand EU.
The second (small) and third (big) sys-tems use as training data the whole NC and sub-sets of EU (5% and 14%, respectively), UN (2%and 12%, respectively) and CC (50% and 100%,respectively), as shown in Table 1.System #sent.
BLEU BLEUcasedno selection 2.1M 31.99 30.96small 1.4M 33.12 32.05big 3.8M 33.49 32.43Table 2: Number of sentences and BLEU scoresobtained on the WMT12 test set for the differentsystems on the EN?ES translation task.The advantage of data selection is clear.
Thesecond system, although smaller in size comparedto the first (1.4M sentence pairs versus 2.1M),takes its training from a more varied set of data,and its performance is over one absolute BLEUpoint higher.When comparing the two systems that rely ondata selection, one might expect the one that usesdata with lower perplexity (small) to perform bet-ter.
However, this is not the case, the third system(big) performing around half an absolute BLEUpoint higher than the second (small).
This hintsat the fact that perplexity alone is not an optimalmetric for data selection, but size should also beconsidered.
Note that the size of system 3?s phrasetable is more than double that of system 2.2143 French-EnglishThis section describe the particularities of the MTsystems built for the French-English language pairin both directions.
The goal of the experimen-tal setup presented here is to evaluate the gain ofadding small in-domain parallel data into a trans-lation system built on a sub-sample of the out-of-domain parallel data.3.1 Data Pre-processingAll the available parallel and monolingual data forthe French-English language pair, including thelast versions of LDC Gigaword corpora, are nor-malised and special characters are escaped usingthe scripts provided by the shared task organisers.Then, the corpora are tokenised and for each lan-guage a true-case model is built on the concatena-tion of all the data after removing duplicated sen-tences, using the scripts included in MOSES dis-tribution.
The corpora are then true-cased beforebeing used to build the language and the transla-tion models.3.2 Language ModelTo build our final language models, we first buildLMs on each corpus individually.
All the monolin-gual corpora are considered, as well as the sourceor target side of the parallel corpora if the dataare not already in the monolingual data.
We buildmodified Kneser-Ney discounted 5-gram LMs us-ing the SRILM toolkit for each corpus and sepa-rate the LMs in three groups: one in-domain (con-taining news-commentary and news crawl cor-pora), another out-of-domain (containing Com-mon Crawl, Europarl, UN and 109 corpora), andthe last one with LDC Gigaword LMs (the dataare kept separated by news source, as distributedby LDC).
The LMs in each group are linearly in-terpolated based on their perplexities obtained onthe concatenation of all the development sets fromprevious WMT translation tasks.
The same devel-opment corpus is used to linearly interpolate thein-domain and LDC LMs.
We finally obtain twoLMs, one containing out-of-domain data which isonly used to filter parallel data, and another onecontaining in-domain data which is used to filterparallel data, tuning the translation model weightsand at decoding time.
Details about the number ofn-grams in each language model are presented inTable 3.French Englishout in out in1-gram 4.0 3.3 4.2 10.72-gram 43.0 44.0 48.2 161.93-gram 54.2 61.8 63.4 256.84-gram 99.7 119.2 103.2 502.75-gram 136.4 165.0 125.4 680.7Table 3: Number of n-grams (in millions) for thein-domain and out-of-domain LMs in French andEnglish.3.3 Translation ModelTwo phrase-based translation models are builtusing MGIZA++ (Gao and Vogel, 2008) andMOSES3, with the default alignment heuris-tic (grow-diag-final) and bidirectional reorderingmodels.
The first translation model is in-domain,built with the news-commentary corpus.
The sec-ond one is built on a sample of all the other paral-lel corpora available for the French-English lan-guage pair.
Both corpora are cleaned using thescript provided with Moses, keeping the sentenceswith a length below 80 words.
For the secondtranslation model, we used the modified Moore-Lewis method based on the four LMs (two perlanguage) presented in section 3.2.
The sum ofthe source and target perplexity difference is com-puted for each sentence pair of the corpus.
We setan acceptance threshold to keep a limited amountof sentence pairs.
The kept sample finally con-tains ?
3.7M sentence pairs to train the translationmodel.
Statistics about this data sample and thenews-commentary corpus are presented in Table 4.The test set of WMT12 translation task is used tooptimise the weights for the two translation mod-els with the MERT algorithm.
For this tuning step,the limit of target phrases loaded per source phraseis set to 50.
We also use a reordering constraintaround punctuation marks.
The same parametersare used during the decoding of the test set.news-commentary sampletokens FR 4.7M 98.6Mtokens EN 4.0M 88.0Msentences 156.5k 3.7MTable 4: Statistics about the two parallel corpora,after pre-processing, used to train the translationmodels.3Moses version 1.02153.4 ResultsThe two translation models presented in Sec-tion 3.3 allow us to design three translation sys-tems: one using only the in-domain model, oneusing only the model built on the sub-sample ofthe out-of-domain data, and one using both mod-els by giving two decoding paths to Moses.
Forthis latter system, the MERT algorithm is also usedto optimise the translation model weights.
Resultsobtained on the WMT13 test set, measured withthe official automatic metrics, are presented in Ta-ble 5.
The submitted system is the one built onthe sub-sample of the out-of-domain parallel data.This system was chosen during the tuning step be-cause it reached the highest BLEU scores on thedevelopment corpus, slightly above the combina-tion of the two translation models.News-Com.
Sample Comb.FR-ENBLEUdev 26.9 30.0 29.9BLEU 27.0 30.8 30.4BLEUcased 26.1 29.8 29.3TER 62.9 58.9 59.3EN-FRBLEUdev 27.1 29.7 29.6BLEU 26.6 29.6 29.4BLEUcased 25.8 28.7 28.5TER 65.1 61.8 62.0Table 5: BLEU and TER scores obtained by oursystems.
BLEUdev is the score obtained on thedevelopment set given by MERT, while BLEU,BLEUcased and TER are obtained on the test setgiven by the submission website.For both FR-EN and EN-FR tasks, the best re-sults are reached by the system built on the sub-sample taken from the out-of-domain parallel data.Using only News-Commentary to build a trans-lation model leads to acceptable BLEU scores,with regards to the size of the training corpus.When the sub-sample of the out-of-domain par-allel data is used to build the translation model,adding a model built on News-Commentary doesnot improve the results.
The difference betweenthese two systems in terms of BLEU score (bothcased sensitive and insensitive) indicates that sim-ilar results can be achieved, however it appearsthat the amount of sentence pairs in the sampleis large enough to limit the impact of the smallin-domain corpus parallel.
Further experimentsare still required to determine the minimum sam-ple size needed to outperform both the in-domainsystem and the combination of the two translationmodels.4 German-EnglishIn this section we describe our work on Germanto English subtask.
Firstly we describe the De-pendency tree to string method which we tried butunfortunately failed due to short of time.
Secondlywe discuss the baseline system and the preprocess-ing we performed.
Thirdly a system combinationmethod is described.4.1 Dependency Tree to String MethodOur original plan was to address the long distancereordering problem in German-English transla-tion.
We use Xie?s Dependency tree to stringmethod(Xie et al 2011) which obtains good re-sults on Chinese to English translation and ex-hibits good performance at long distance reorder-ing as our decoder.We use Stanford dependency parser4 to parsethe English side of the data and Mate-Tool5 forthe German side.
The first set of experiments didnot lead to encouraging results and due to insuffi-cient time, we decide to switch to other decoders,based on statistical phrase-based and hierarchicalapproaches.4.2 Baseline SystemIn this section we describe the three baseline sys-tem we used as well as the preprocessing technolo-gies and the experiments set up.4.2.1 Preprocessing and CorpusWe first use the normalisation scripts provided byWMT2013 to normalise both English and Ger-man side.
Then we escape special characters onboth sides.
We use Stanford tokeniser for Englishand OpenNLP tokeniser6 for German.
Then wetrain a true-case model using with Europarl andNews-Commentary corpora, and true-case all thecorpus we used.
The parallel corpus is filteredwith the standard cleaning scripts provided with4http://nlp.stanford.edu/software/lex-parser.shtml5http://code.google.com/p/mate-tools/6http://opennlp.sourceforge.net/models-1.5/216MOSES.
We split the German compound wordswith jWordSplitter7.All the corpus provided for the shared task areused for training our translation models, whileWMT2011 and WMT2012 test sets are used totune the models parameters.
For the LM, weuse all the monolingual data provided, includingLDC Gigaword.
Each LM is trained with theSRILM toolkit, before interpolating all the LMsaccording to their weights obtained by minimiz-ing the perplexity on the tuning set (WMT2011and WMT2012 test sets).
As SRILM can onlyinterpolate 10 LMs, we first interpolate a LM withEuroparl, News Commentary, News Crawl (2007-2012, each year individually, 6 separate parts),then we interpolate a new LM with this interpo-lated LM and LDC Gigawords (we kept the Gi-gaword subsets separated according to the newssources as distributed by LDC, which leads to 7corpus).4.2.2 Three baseline systemsWe use the data set up described by the formersubsection and build up three baseline systems,namely PB MOSES (phrase-based), Hiero MOSES(hierarchical) and CDEC (Dyer et al 2010).
Themotivation of choosing Hierarchical Models is toaddress the German-English?s long reorder prob-lem.
We want to test the performance of CDEC andHiero MOSES and choose the best.
PB MOSES isused as our benchmark.
The three results obtainedon the development and test sets for the three base-line system and the system combination are shownin the Table 6.Development TestPB MOSES 22.0 24.0Hiero MOSES 22.1 24.4CDEC 22.5 24.4Combination 23.0 24.8Table 6: BLEU scores obtained by our systems onthe development and test sets for the German toEnglish translation task.From the Table 6 we can see that on develop-ment set, CDEC performs the best, and its muchbetter than MOSES?s two decoder, but on testset, Hiero MOSES and CDEC performs as well aseach other, and they both performs better than PBModel.7http://www.danielnaber.de/jwordsplitter/4.3 System CombinationWe also use a word-level combination strat-egy (Rosti et al 2007) to combine the three trans-lation hypotheses.
To combine these systems, wefirst use the Minimum Bayes-Risk (MBR) (Kumarand Byrne, 2004) decoder to obtain the 5 best hy-pothesis as the alignment reference for the Con-fusion Network (CN) (Mangu et al 2000).
Wethen use IHMM (He et al 2008) to choose thebackbone build the CN and finally search for andgenerate the best translation.We tune the system parameters on developmentset with Simple-Simplex algorithm.
The param-eters for system weights are set equal.
Other pa-rameters like language model, length penalty andcombination coefficient are chosen when we see agood improvement on development set.5 ConclusionThis paper presented a set of experiments con-ducted on Spanish-English, French-English andGerman-English language pairs.
For the Spanish-English pair, we have explored the use of linguisticinformation to select parallel data and use this asthe training for SMT.
However, the comparison ofthe performance obtained using this method andthe purely statistical one (i.e.
perplexity on wordforms) remains to be carried out.
Another openquestion regards the optimal size of the selecteddata.
As we have seen, minimum perplexity alonecannot be considered an optimal metric since us-ing a larger set, even if it has higher perplexity,allowed us to obtain notably higher BLEU scores.The question is then how to decide the optimal sizeof parallel data to select.For the French-English language pair, we inves-tigated the usefulness of the small in-domain par-allel data compared to out-of-domain parallel datasub-sampling.
We show that with a sample con-taining ?
3.7M sentence pairs extracted from theout-of-domain parallel data, it is not necessary touse the small domain-specific parallel data.
Fur-ther experiments are still required to determine theminimum sample size needed to outperform boththe in-domain system and the combination of thetwo translation models.Finally, for the German-English language pair,we presents our exploitation of long orderingproblem.
We compared two hierarchical modelswith one phrase-based model, and we also use asystem combination strategy to further improve217the translation systems performance.AcknowledgmentsThe research leading to these results has re-ceived funding from the European Union SeventhFramework Programme FP7/2007-2013 undergrant agreement PIAP-GA-2012-324414 (Abu-MaTran) and through Science Foundation Irelandas part of the CNGL (grant 07/CE/I1142).ReferencesAmittai Axelrod, Xiaodong He, and Jianfeng Gao.2011.
Domain adaptation via pseudo in-domaindata selection.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, EMNLP ?11, pages 355?362, Stroudsburg, PA,USA.
Association for Computational Linguistics.Stanley F. Chen and Joshua Goodman.
1996.
An em-pirical study of smoothing techniques for languagemodeling.
In Proceedings of the 34th annual meet-ing on Association for Computational Linguistics,ACL ?96, pages 310?318, Stroudsburg, PA, USA.Association for Computational Linguistics.Chris Dyer, Jonathan Weese, Hendra Setiawan, AdamLopez, Ferhan Ture, Vladimir Eidelman, Juri Gan-itkevitch, Phil Blunsom, and Philip Resnik.
2010.cdec: A decoder, alignment, and learning frameworkfor finite-state and context-free translation models.In Proceedings of the ACL 2010 System Demonstra-tions, pages 7?12.
Association for ComputationalLinguistics.Marcello Federico, Nicola Bertoldi, and Mauro Cet-tolo.
2008.
IRSTLM: an open source toolkit forhandling large scale language models.
In INTER-SPEECH, pages 1618?1621.
ISCA.Qin Gao and Stephan Vogel.
2008.
Parallel implemen-tations of word alignment tool.
In Software Engi-neering, Testing, and Quality Assurance for NaturalLanguage Processing, pages 49?57.
Association forComputational Linguistics.Xiaodong He, Mei Yang, Jianfeng Gao, PatrickNguyen, and Robert Moore.
2008.
Indirect-hmm-based hypothesis alignment for combining outputsfrom machine translation systems.
In Proceedingsof the Conference on Empirical Methods in Natu-ral Language Processing, pages 98?107.
Associa-tion for Computational Linguistics.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan, Wade Shen, Christine Moran,Richard Zens, Chris Dyer, Ondr?ej Bojar, AlexandraConstantin, and Evan Herbst.
2007.
Moses: opensource toolkit for statistical machine translation.
InProceedings of the 45th Annual Meeting of the ACLon Interactive Poster and Demonstration Sessions,ACL ?07, pages 177?180, Stroudsburg, PA, USA.Association for Computational Linguistics.Shankar Kumar and William Byrne.
2004.
Minimumbayes-risk decoding for statistical machine transla-tion.
In Proceedings of HLT-NAACL, pages 169?176.Lidia Mangu, Eric Brill, and Andreas Stolcke.
2000.Finding consensus in speech recognition: word er-ror minimization and other applications of confu-sion networks.
Computer Speech & Language,14(4):373?400.Robert C. Moore and William Lewis.
2010.
Intelli-gent selection of language model training data.
InProceedings of the ACL 2010 Conference Short Pa-pers, ACLShort ?10, pages 220?224, Stroudsburg,PA, USA.
Association for Computational Linguis-tics.Antti-Veikko I Rosti, Necip Fazil Ayan, Bing Xiang,Spyros Matsoukas, Richard Schwartz, and BonnieDorr.
2007.
Combining outputs from multiplemachine translation systems.
In Human LanguageTechnologies 2007: The Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics, pages 228?235.Andreas Stolcke.
2002.
Srilm - an extensible lan-guage modeling toolkit.
In John H. L. Hansen andBryan L. Pellom, editors, INTERSPEECH.
ISCA.Antonio Toral.
2013.
Hybrid Selection of LanguageModel Training Data Using Linguistic Informationand Perplexity.
In Proceedings of the Second Work-shop on Hybrid Approaches to Machine Translation(HyTra), ACL 2013.Jun Xie, Haitao Mi, and Qun Liu.
2011.
A noveldependency-to-string model for statistical machinetranslation.
In Proceedings of the Conference onEmpirical Methods in Natural Language Process-ing, pages 216?226.
Association for ComputationalLinguistics.218
