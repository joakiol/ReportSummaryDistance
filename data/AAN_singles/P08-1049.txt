Proceedings of ACL-08: HLT, pages 425?433,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsUnsupervised Translation Induction for Chinese Abbreviationsusing Monolingual CorporaZhifei Li and David YarowskyDepartment of Computer Science and Center for Language and Speech ProcessingJohns Hopkins University, Baltimore, MD 21218, USAzhifei.work@gmail.com and yarowsky@cs.jhu.eduAbstractChinese abbreviations are widely used inmodern Chinese texts.
Compared withEnglish abbreviations (which are mostlyacronyms and truncations), the formation ofChinese abbreviations is much more complex.Due to the richness of Chinese abbreviations,many of them may not appear in available par-allel corpora, in which case current machinetranslation systems simply treat them as un-known words and leave them untranslated.
Inthis paper, we present a novel unsupervisedmethod that automatically extracts the relationbetween a full-form phrase and its abbrevia-tion from monolingual corpora, and inducestranslation entries for the abbreviation by us-ing its full-form as a bridge.
Our method doesnot require any additional annotated data otherthan the data that a regular translation systemuses.
We integrate our method into a state-of-the-art baseline translation system and showthat it consistently improves the performanceof the baseline system on various NIST MTtest sets.1 IntroductionThe modern Chinese language is a highly abbrevi-ated one due to the mixed use of ancient single-character words with modern multi-character wordsand compound words.
According to Chang and Lai(2004), approximately 20% of sentences in a typicalnews article have abbreviated words in them.
Ab-breviations have become even more popular alongwith the development of Internet media (e.g., onlinechat, weblog, newsgroup, and so on).
While En-glish words are normally abbreviated by either theirFull-form Abbreviation Translation&?
?
??
Hong Kong Governor?\ ?/?
???
Security CouncilFigure 1: Chinese Abbreviations Examplesfirst letters (i.e.
acronyms) or via truncation, the for-mation of Chinese abbreviations is much more com-plex.
Figure 1 shows two examples for Chinese ab-breviations.
Clearly, an abbreviated form of a wordcan be obtained by selecting one or more charactersfrom this word, and the selected characters can be atany position in the word.
In an extreme case, thereare even re-ordering between a full-form phrase andits abbreviation.While the research in statistical machine trans-lation (SMT) has made significant progress, mostSMT systems (Koehn et al, 2003; Chiang, 2007;Galley et al, 2006) rely on parallel corpora to extracttranslation entries.
The richness and complexnessof Chinese abbreviations imposes challenges to theSMT systems.
In particular, many Chinese abbrevi-ations may not appear in available parallel corpora,in which case current SMT systems treat them asunknown words and leave them untranslated.
Thisaffects the translation quality significantly.To be able to translate a Chinese abbreviation thatis unseen in available parallel corpora, one may an-notate more parallel data.
However, this is veryexpensive as there are too many possible abbrevia-tions and new abbreviations are constantly created.Another approach is to transform the abbreviation425into its full-form for which the current SMT systemknows how to translate.
For example, if the baselinesystem knows that the translation for ?&???
is?Hong Kong Governor?, and it also knows that ????
is an abbreviation of ?&?
??
, then it cantranslate ????
to ?Hong Kong Governor?.Even if an abbreviation has been seen in parallelcorpora, it may still be worth to consider its full-form phrase as an additional alternative to the ab-breviation since abbreviated words are normally se-mantically ambiguous, while its full-form containsmore context information that helps the MT systemchoose a right translation for the abbreviation.Conceptually, the approach of translating an ab-breviation by using its full-form as a bridge in-volves four components: identifying abbreviations,learning their full-forms, inducing their translations,and integrating the abbreviation translations into thebaseline SMT system.
None of these components istrivial to realize.
For example, for the first two com-ponents, we may need manually annotated data thattags an abbreviation with its full-form.
We also needto make sure that the baseline system has at leastone valid translation for the full-form phrase.
Onthe other hand, integrating an additional componentinto a baseline SMT system is notoriously tricky asevident in the research on integrating word sensedisambiguation (WSD) into SMT systems: differentways of integration lead to conflicting conclusionson whether WSD helps MT performance (Chan etal., 2007; Carpuat and Wu, 2007).In this paper, we present an unsupervised ap-proach to translate Chinese abbreviations.
Our ap-proach exploits the data co-occurrence phenomenaand does not require any additional annotated dataexcept the parallel and monolingual corpora that thebaseline SMT system uses.
Moreover, our approachintegrates the abbreviation translation componentinto the baseline system in a natural way, and thus isable to make use of the minimum-error-rate training(Och, 2003) to automatically adjust the model pa-rameters to reflect the change of the integrated sys-tem over the baseline system.
We carry out experi-ments on a state-of-the-art SMT system, i.e., Moses(Koehn et al, 2007), and show that the abbreviationtranslations consistently improve the translation per-formance (in terms of BLEU (Papineni et al, 2002))on various NIST MT test sets.2 Background: Chinese AbbreviationsIn general, Chinese abbreviations are formed basedon three major methods: reduction, elimination andgeneralization (Lee, 2005; Yin, 1999).
Table 1presents examples for each category.Among the three methods, reduction is the mostpopular one, which generates an abbreviation byselecting one or more characters from each of thewords in the full-form phrase.
The selected char-acters can be at any position of the word.
Table 1presents examples to illustrate how characters at dif-ferent positions are selected to generate abbrevia-tions.
While the abbreviations mostly originate fromnoun phrases (in particular, named entities), othergeneral phrases are also abbreviatable.
For example,the second example ?Save Energy?
is a verb phrase.In an extreme case, reordering may happen betweenan abbreviation and its full-form phrase.
For exam-ple, for the seventh example in Table 1, a monotoneabbreviation should be ?X?
?, however, ?X??
is a more popular ordering in Chinese texts.In elimination, one or more words of the origi-nal full-form phrase are eliminated and the rest partsremain as an abbreviation.
For example, in the full-form phrase ?8?L?
?, the word ?L??
is elim-inated and the remaining word ?8??
alone be-comes the abbreviation.In generalization, an abbreviation is createdby generalizing parallel sub-parts of the full-formphrase.
For example, ?
?3 (three preventions)?
inTable 1 is an abbreviation for the phrase ?3?3x3b//?
(fire prevention, theft prevention,and traffic accident prevention)?.
The character ?3(prevention)?
is common to the three sub-parts of thefull-form, so it is being generalized.3 Unsupervised Translation Induction forChinese AbbreviationsIn this section, we describe an unsupervised methodto induce translation entries for Chinese abbrevia-tions, even when these abbreviations never appear inthe Chinese side of the parallel corpora.
Our basicidea is to automatically extract the relation betweena full-form phrase and its abbreviation (we refer therelation as full-abbreviation) from monolingual cor-pora, and then induce translation entries for the ab-breviation by using its full-form phrase as a bridge.426Category Full-form Abbreviation TranslationReduction ??
L?
?L Peking University??
?
? Save Energy&?
?
??
Hong Kong Governorib \?
i?
Foreign Minister|?
??
??
People?s Police?\ ?/?
???
Security Council? X ??
X?
No.1 Nuclear Energy Power PlantElimination 8?
L?
8?
Tsinghua UniversityGeneralization 3?3x3b//?
?3 Three PreventionsTable 1: Chinese Abbreviation: Categories and ExamplesOur approach involves five major steps:?
Step-1: extract a list of English entities fromEnglish monolingual corpora;?
Step-2: translate the list into Chinese using abaseline translation system;?
Step-3: extract full-abbreviation relations fromChinese monolingual corpora by treating theChinese translations obtained in Step-2 as full-form phrases;?
Step-4: induce translation entries for Chineseabbreviations by using their full-form phrasesas bridges;?
Step-5: augment the baseline system withtranslation entries obtained in Step-4.Clearly, the main purpose of Step-1 and -2 is toobtain a list of Chinese entities, which will be treatedas full-form phrases in Step-3.
One may use a namedentity tagger to obtain such a list.
However, this re-lies on the existence of a Chinese named entity tag-ger with high-precision.
Moreover, obtaining a listusing a dedicated tagger does not guarantee that thebaseline system knows how to translate the list.
Onthe contrary, in our approach, since the Chinese en-tities are translation outputs for the English entities,it is ensured that the baseline system has translationsfor these Chinese entities.Regarding the data resource used, Step-1, -2, and-3 rely on the English monolingual corpora, paral-lel corpora, and the Chinese monolingual corpora,respectively.
Clearly, our approach does not re-quire any additional annotated data compared withthe baseline system.
Moreover, our approach uti-lizes both Chinese and English monolingual datato help MT, while most SMT systems utilizes onlythe English monolingual data to build a languagemodel.
This is particularly interesting since we nor-mally have enormous monolingual data, but a smallamount of parallel data.
For example, in the transla-tion task between Chinese and English, both the Chi-nese and English Gigaword have billions of words,but the parallel data has only about 30 million words.Step-4 and -5 are natural ways to integrate the ab-breviation translation component with the baselinetranslation system.
This is critical to make the ab-breviation translation get performance gains over thebaseline system as will be clear later.In the remainder of this section, we will present aspecific instantiation for each step.3.1 English Entity Extraction from EnglishMonolingual CorporaThough one can exploit a sophisticated named-entitytagger to extract English entities, in this paper weidentify English entities based on the capitalizationinformation.
Specifically, to be considered as an en-tity, a continuous span of English words must satisfythe following conditions:?
all words must start from a capital letter exceptfor function words ?of?, ?the?, and ?and?;?
each function word can appear only once;?
the number of words in the span must besmaller than a threshold (e.g., 10);?
the occurrence count of this span must begreater than a threshold (e.g., 1).4273.2 English Entity TranslationFor the Chinese-English language pair, most MT re-search is on translation from Chinese to English, buthere we need the reverse direction.
However, sincemost of statistical translation models (Koehn et al,2003; Chiang, 2007; Galley et al, 2006) are sym-metrical, it is relatively easy to train a translationsystem to translate from English to Chinese, exceptthat we need to train a Chinese language model fromthe Chinese monolingual data.It is worth pointing out that the baseline systemmay not be able to translate all the English enti-ties.
This is because the entities are extracted fromthe English monolingual corpora, which has a muchlarger vocabulary than the English side of the par-allel corpora.
Therefore, we should remove all theChinese translations that contain any untranslatedEnglish words before proceeding to the next step.Moreover, it is desirable to generate an n-best listinstead of a 1-best translation for the English entity.3.3 Full-abbreviation Relation Extraction fromChinese Monolingual CorporaWe treat the Chinese entities obtained in Section 3.2as full-form phrases.
To identify their abbreviations,one can employ an HMM model (Chang and Teng,2006).
Here we propose a much simpler approach,which is based on the data co-occurrence intuition.3.3.1 Data Co-occurrenceIn a monolingual corpus, relevant words tend toappear together (i.e., co-occurrence).
For example,Bill Gates tends to appear together with Microsoft.The co-occurrence may imply a relationship (e.g.,Bill Gates is the founder of Microsoft).
By inspec-tion of the Chinese text, we found that the dataco-occurrence phenomena also applies to the full-Title ?????*R?<?
?Text c???2?9??(V?c??)?20?????{?*R?h?
-10?t8??????.??t*y ?{?
?Table 2: Data Co-occurrence Example for the Full-abbreviation Relation (????,???)
meaning?winter olympics?abbreviation relation.
Table 2 shows an example,where the abbreviation ?????
appears in the titlewhile its full-form ??????
appears in the textof the same document.
In general, the occurrencedistance between an abbreviation and its full-formvaries.
For example, they may appear in the samesentence, or in the neighborhood sentences.3.3.2 Full-abbreviation Relation ExtractionAlgorithmBy exploiting the data co-occurrence phenom-ena, we identify possible abbreviations for full-formphrases.
Figure 2 presents the pseudocode of thefull-abbreviation relation extraction algorithm.Relation-Extraction(Corpus ,Full-list)1 contexts ?
NIL2 for i ?
1 to length[Corpus]3 sent1 ?
Corpus[i ]4 contexts ?
UPDATE(contexts ,Corpus , i)5 for full in sent16 if full in Full-list7 for sent2 in contexts8 for abbr in sent29 if RL(full , abbr ) = TRUE10 Count[abbr , full]++11 return CountFigure 2: Full-abbreviation Relation ExtractionGiven a monolingual corpus and a list of full-formphrases (i.e., Full-list, which is obtained in Sec-tion 3.2), the algorithm returns a Count that con-tains full-abbreviation relations and their occurrencecounts.
Specifically, the algorithm linearly scansover the whole corpus as indicated by line 1.
Alongthe linear scan, the algorithm maintains contexts ofthe current sentence (i.e., sent1), and the contextsremember the sentences from where the algorithmidentifies possible abbreviations.
In our implemen-tation, the contexts include current sentence, the ti-tle of current document, and previous and next sen-tence in the document.
Then, for each ngram (i.e.,full) of the current sentence (i.e., sent1) and for eachngram (i.e., abbr) of a context sentence (i.e., sent2),the algorithm calls a function RL, which decideswhether the full-abbreviation relation holds betweenfull and abbr.
If RL returns TRUE, the count table428(i.e., Count) is incremented by one for this relation.Note that the filtering through the full-form phraseslist (i.e., Full-list) as shown in line 6 is the key tomake the algorithm efficient enough to run throughlarge-size monolingual corpora.In function RL, we run a simple alignment algo-rithm that links the characters in abbr with the wordsin full.
In the alignment, we assume that there is noreordering between full and abbr.
To be consideredas a valid full-abbreviation relation, full and abbrmust satisfy the following conditions:?
abbr must be shorter than full by a relativethreshold (e.g., 1.2);?
each character in abbr must be aligned to full;?
each word in full must have at least one charac-ter aligned to abbr;?
abbr must not be a continuous sub-part of full;Clearly, due to the above conditions, our approachmay not be able to handle all possible abbreviations(e.g., the abbreviations formed by the generalizationmethod described in Section 2).
One can modifythe conditions and the alignment algorithm to handlemore complex full-abbreviation relations.With the count table Count, we can calculate therelative frequency and get the following probability,P (full|abbr) = Count[abbr, full]?Count[abbr, ?]
(1)3.4 Translation Induction for ChineseAbbreviationsGiven a Chinese abbreviation and its full-form, weinduce English translation entries for the abbrevia-tion by using the full-form as a bridge.
Specifically,we first generate n-best translations for each full-form Chinese phrase using the baseline system.1 Wethen post-process the translation outputs such thatthey have the same format (i.e., containing the sameset of model features) as a regular phrase entry in1In our method, it is guaranteed that each Chinese full-formphrase will have at least one English translation, i.e., the En-glish entity that has been used to produce this full-form phrase.However, it does not mean that this English entity is the besttranslation that the baseline system has for the Chinese full-form phrase.
This is mainly due to the asymmetry introducedby the different LMs in different translation directions.the baseline phrase table.
Once we get the transla-tion entries for the full-form, we can replace the full-form Chinese with its abbreviation to generate trans-lation entries for the abbreviation.
Moreover, to dealwith the case that an abbreviation may have severalcandidate full-form phrases, we normalize the fea-ture values using the following equation,?j(e, abbr) = ?j(e, full)?
P (full|abbr) (2)where e is an English translation, and ?j is the j-thmodel feature indexed as in the baseline system.3.5 Integration with Baseline TranslationSystemSince the obtained translation entries for abbrevia-tions have the same format as the regular transla-tion entries in the baseline phrase table, it is rela-tively easy to add them into the baseline phrase ta-ble.
Specifically, if a translation entry (signatured byits Chinese and English strings) to be added is not inthe baseline phrase table, we simply add the entryinto the baseline table.
On the other hand, if the en-try is already in the baseline phrase table, then wemerge the entries by enforcing the translation prob-ability as we obtain the same translation entry fromtwo different knowledge sources (one is from par-allel corpora and the other one is from the Chinesemonolingual corpora).Once we obtain the augmented phrase table, weshould run the minimum-error-rate training (Och,2003) with the augmented phrase table such that themodel parameters are properly adjusted.
As will beshown in the experimental results, this is critical toobtain performance gain over the baseline system.4 Experimental Results4.1 CorporaWe compile a parallel dataset which consists of var-ious corpora distributed by the Linguistic Data Con-sortium (LDC) for NIST MT evaluation.
The paral-lel dataset has about 1M sentence pairs, and about28M words.
The monolingual data we use includesthe English Gigaword V2 (LDC2005T12) and theChinese Gigaword V2 (LDC2005T14).4.2 Baseline System TrainingUsing the toolkit Moses (Koehn et al, 2007), webuilt a phrase-based baseline system by following429the standard procedure: running GIZA++ (Och andNey, 2000) in both directions, applying refinementrules to obtain a many-to-many word alignment, andthen extracting and scoring phrases using heuristics(Och and Ney, 2004).
The baseline system has eightfeature functions (see Table 8).
The feature func-tions are combined under a log-linear framework,and the weights are tuned by the minimum-error-ratetraining (Och, 2003) using BLEU (Papineni et al,2002) as the optimization metric.To handle different directions of translation be-tween Chinese and English, we built two tri-gram language models with modified Kneser-Neysmoothing (Chen and Goodman, 1998) using theSRILM toolkit (Stolcke, 2002).4.3 Statistics on Intermediate StepsAs described in Section 3, our approach involvesfive major steps.
Table 3 reports the statistics foreach intermediate step.
While about 5M English en-tities are extracted and 2-best Chinese translationsare generated for each English entity, we get only4.7M Chinese entities.
This is because many of theEnglish entities are untranslatable by the baselinesystem.
The number of full-abbreviation relations2extracted from the Chinese monolingual corpora is51K.
For each full-form phrase we generate 5-bestEnglish translations, however only 210k (<51K?5)translation entries are obtained.
This is because thebaseline system may have less than 5 unique trans-lations for some of the full-form phrases.
Lastly, thenumber of translation entries added due to abbrevi-ations is very small compared with the total numberof translation entries (i.e., 50M).Measure Valuenumber of English entities 5Mnumber of Chinese entities 4.7Mnumber of full-abbreviation relations 51Knumber of translation entries added 210Ktotal number of translation entries 50MTable 3: Statistics on Intermediate Steps2Note that many of the ?abbreviations?
extracted by our al-gorithm are not true abbreviations in the linguistic sense, insteadthey are just continuous-span of words.
This is analogous to theconcept of ?phrase?
in phrase-based MT.4.4 Precision on Full-abbreviation RelationsTable 4 reports the precision on the extracted full-abbreviation relations.
We classify the relations intoseveral classes based on their occurrence counts.
Inthe second column, we list the fraction of the rela-tions in the given class among all the relations wehave extracted (i.e., 51K relations).
For each class,we randomly select 100 relations, manually tag themas correct or wrong, and then calculate the precision.Intuitively, a class that has a higher occurrence countshould have a higher precision, and this is generallytrue as shown in the fourth column of Table 4.
Incomparison, Chang and Teng (2006) reports a preci-sion of 50% over relations between single-word full-forms and single-character abbreviations.
One canimagine a much lower precision on general relations(e.g., the relations between multi-word full-formsand multi-character abbreviations) that we considerhere.
Clearly, our results are very competitive3.Count Fraction (%) Precision (%)Baseline Ours(0, 1] 35.2 8.9 42.6(1, 5] 33.8 7.8 54.4(5, 10] 10.7 8.9 60.0(10, 100] 16.5 7.6 55.9(100,+?)
3.8 12.1 59.9Average Precision (%) 8.4 51.3Table 4: Full-abbreviation Relation Extraction PrecisionTo further show the advantage of our relation ex-traction algorithm (see Section 3.3), in the third col-umn of Table 4 we report the results on a simplebaseline.
To create the baseline, we make use of thedominant abbreviation patterns shown in Table 5,which have been reported in Chang and Lai (2004).The abbreviation pattern is represented using theformat ?
(bit pattern|length)?
where the bit patternencodes the information about how an abbreviatedform is obtained from its original full-form word,and the length represents the number of characters inthe full-form word.
In the bit pattern, a ?1?
indicatesthat the character at the corresponding position ofthe full-form word is kept in the abbreviation, whilea ?0?
means the character is deleted.
Now we dis-3However, it is not a strict comparison because the dataset isdifferent and the recall may also be different.430Pattern Fraction (%) Example(1|1) 100 (?,?
)(10|2) 87 (??,?
)(101|3) 44 (?/?,??
)(1010|4) 56 (?
?=?,?=)Table 5: Dominant Abbreviation Patterns reported inChang and Lai (2004)cuss how to create the baseline.
For each full-formphrase in the randomly selected relations, we gener-ate a baseline hypothesis (i.e., abbreviation) as fol-lows.
We first generate an abbreviated form for eachword in the full-form phrase by using the dominantabbreviation pattern, and then concatenate these ab-breviated words to form a baseline abbreviation forthe full-form phrase.
As shown in Table 4, the base-line performs significantly worse than our relationextraction algorithm.
Compared with the baseline,our relation extraction algorithm allows arbitrary ab-breviation patterns as long as they satisfy the align-ment constraints.
Moreover, our algorithm exploitsthe data co-occurrence phenomena to generate andrank hypothesis (i.e., abbreviation).
The above tworeasons explain the large performance gain.It is interesting to examine the statistics on abbre-viation patterns over the relations automatically ex-tracted by our algorithm.
Table 6 reports the statis-tics.
We obtain the statistics on the relations thatare manually tagged as correct before, and there arein total 263 unique words in the corresponding full-form phrases.
Note that the results here are highlybiased to our relation extraction algorithm (see Sec-tion 3.3).
For the statistics on manually collectedexamples, please refer to Chang and Lai (2004).4.5 Results on Translation Performance4.5.1 Precision on Translations of ChineseFull-form PhrasesFor the relations manually tagged as correct inSection 4.4, we manually look at the top-5 transla-tions for the full-form phrases.
If the top-5 transla-tions contain at least one correct translation, we tagit as correct, otherwise as wrong.
We get a precisionof 97.5%.
This precision is extremely high becausethe BLEU score (precision with brevity penalty) thatone obtains for a Chinese sentence is normally be-tween 30% to 50%.
Two reasons explain such a highPattern Fraction (%) Example(1|1) 100 (?,?
)(10|2) 74.3 (??,?
)(01|2) 7.6 (??,?
)(11|2) 18.1 ( j, j)(100|3) 58.5 (n.,)(010|3) 3.1 (qu?,u)(001|3) 4.6 (???,?
)(110|3) 13.8 (???,??
)(101|3) 3.1 (?/?,??
)(111|3) 16.9 ()?,)?)Table 6: Statistics on Abbreviation Patternsprecision.
Firstly, the full-form phrase is short com-pared with a regular Chinese sentence, and thus it iseasier to translate.
Secondly, the full-form phrase it-self contains enough context information that helpsthe system choose a right translation for it.
In fact,this shows the importance of considering the full-form phrase as an additional alternative to the ab-breviation even if the baseline system already hastranslation entries for the abbreviation.4.5.2 BLEU on NIST MT Test SetsWe use MT02 as the development set4 for mini-mum error rate training (MERT) (Och, 2003).
TheMT performance is measured by lower-case 4-gramBLEU (Papineni et al, 2002).
Table 7 reports the re-sults on various NIST MT test sets.
As shown in thetable, our Abbreviation Augmented MT (AAMT)systems perform consistently better than the base-line system (described in Section 4.2).Task Baseline AAMTNo MERT With MERTMT02 29.87 29.96 30.46MT03 29.03 29.23 29.71MT04 29.05 29.88 30.55Average Gain +0.52 +1.18Table 7: MT Performance measured by BLEU ScoreAs clear in Table 7, it is important to re-run MERT(on MT02 only) with the augmented phrase tablein order to get performance gains.
Table 8 reports4On the dev set, about 20K (among 210K) abbreviationtranslation entries are matched in the Chinese side.431the MERT weights with different phrase tables.
Onemay notice the change of the weight in word penaltyfeature.
This is very intuitive in order to prevent thehypothesis being too long due to the expansion ofthe abbreviations into their full-forms.Feature Baseline AAMTlanguage model 0.137 0.133phrase translation 0.066 0.023lexical translation 0.061 0.078reverse phrase translation 0.059 0.103reverse lexical translation 0.112 0.090phrase penalty -0.150 -0.162word penalty -0.327 -0.356distortion model 0.089 0.055Table 8: Weights obtained by MERT5 Related WorkThough automatically extracting the relations be-tween full-form Chinese phrases and their abbrevi-ations is an interesting and important task for manynatural language processing applications (e.g., ma-chine translation, question answering, informationretrieval, and so on), not much work is availablein the literature.
Recently, Chang and Lai (2004),Chang and Teng (2006), and Lee (2005) have in-vestigated this task.
Specifically, Chang and Lai(2004) describes a hidden markov model (HMM) tomodel the relationship between a full-form phraseand its abbreviation, by treating the abbreviation asthe observation and the full-form words as states inthe model.
Using a set of manually-created full-abbreviation relations as training data, they reportexperimental results on a recognition task (i.e., givenan abbreviation, the task is to obtain its full-form, orthe vice versa).
Clearly, their method is supervisedbecause it requires the full-abbreviation relations astraining data.5 Chang and Teng (2006) extends thework in Chang and Lai (2004) to automatically ex-tract the relations between full-form phrases andtheir abbreviations.
However, they have only con-sidered relations between single-word phrases andsingle-character abbreviations.
Moreover, the HMMmodel is computationally-expensive and unable toexploit the data co-occurrence phenomena that we5However, the HMM model aligns the characters in the ab-breviation to the words in the full-form in an unsupervised way.have exploited efficiently in this paper.
Lee (2005)gives a summary about how Chinese abbreviationsare formed and presents many examples.
Manualrules are created to expand an abbreviation to its full-form, however, no quantitative results are reported.None of the above work has addressed the Chi-nese abbreviation issue in the context of a machinetranslation task, which is the primary goal in thispaper.
To the best of our knowledge, our work isthe first to systematically model Chinese abbrevia-tion expansion to improve machine translation.The idea of using a bridge (i.e., full-form) to ob-tain translation entries for unseen words (i.e., abbre-viation) is similar to the idea of using paraphrases inMT (see Callison-Burch et al (2006) and referencestherein) as both are trying to introduce generaliza-tion into MT.
At last, the goal that we aim to exploitmonolingual corpora to help MT is in-spirit similarto the goal of using non-parallel corpora to help MTas aimed in a large amount of work (see Munteanuand Marcu (2006) and references therein).6 ConclusionsIn this paper, we present a novel method thatautomatically extracts relations between full-formphrases and their abbreviations from monolingualcorpora, and induces translation entries for these ab-breviations by using their full-form as a bridge.
Ourmethod is scalable enough to handle large amountof monolingual data, and is essentially unsupervisedas it does not require any additional annotated datathan the baseline translation system.
Our methodexploits the data co-occurrence phenomena that isvery useful for relation extractions.
We integrate ourmethod into a state-of-the-art phrase-based baselinetranslation system, i.e., Moses (Koehn et al, 2007),and show that the integrated system consistently im-proves the performance of the baseline system onvarious NIST machine translation test sets.AcknowledgmentsWe would like to thank Yi Su, Sanjeev Khudan-pur, Philip Resnik, Smaranda Muresan, Chris Dyerand the anonymous reviewers for their helpful com-ments.
This work was partially supported by the De-fense Advanced Research Projects Agency?s GALEprogram via Contract No?HR0011-06-2-0001.432ReferencesChris Callison-Burch, Philipp Koehn, and Miles Os-borne, 2006.
Improved Statistical Machine TranslationUsing Paraphrases.
In Proceedings of NAACL 2006,pages 17-24.Marine Carpuat and Dekai Wu.
2007.
Improving Statis-tical Machine Translation using Word Sense Disam-biguation.
In Proceedings of EMNLP 2007, pages 61-72.Yee Seng Chan, Hwee Tou Ng, and David Chiang.
2007.Word Sense Disambiguation Improves Statistical Ma-chine Translation.
In Proceedings of ACL 2007, pages33-40.Jing-Shin Chang and Yu-Tso Lai.
2004.
A preliminarystudy on probabilistic models for Chinese abbrevia-tions.
In Proceedings of the 3rd SIGHAN Workshop onChinese Language Processing, pages 9-16.Jing-Shin Chang and Wei-Lun Teng.
2006.
MiningAtomic Chinese Abbreviation Pairs: A ProbabilisticModel for Single Character Word Recovery.
In Pro-ceedings of the 5rd SIGHAN Workshop on ChineseLanguage Processing, pages 17-24.Stanley F. Chen and Joshua Goodman.
1998.
An empiri-cal study of smoothing techniques for language mod-eling.
Technical Report TR-10-98, Harvard UniversityCenter for Research in Computing Technology.David Chiang.
2007.
Hierarchical phrase-based transla-tion.
Computational Linguistics, 33(2):201-228.Michel Galley, Jonathan Graehl, Kevin Knight, DanielMarcu, Steve DeNeefe, Wei Wang, and IgnacioThayer.
2006.
Scalable inference and training ofcontext-rich syntactic translation models.
In Proceed-ings of COLING/ACL 2006, pages 961-968.Philipp Koehn, Hieu Hoang, Alexandra Birch, ChrisCallison-Burch, Marcello Federico, Nicola Bertoldi,Brooke Cowan,Wade Shen, Christine Moran, RichardZens, Chris Dyer, Ondrej Bojar, Alexandra Con-strantin, and Evan Herbst.
2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of ACL, Demonstration Session, pages 177-180.Philipp Koehn, Franz Josef Och, and Daniel Marcu.2003.
Statistical phrase-based translation.
In Proceed-ings of NAACL 2003, pages 48-54.H.W.D Lee.
2005.
A study of automatic expansion ofChinese abbreviations.
MA Thesis, The University ofHong Kong.Dragos Stefan Munteanu and Daniel Marcu.
2006.
Ex-tracting Parallel Sub-Sentential Fragments from Non-Parallel Corpora.
In Proceedings of ACL 2006, pages81-88.Franz Josef Och.
2003.
Minimum error rate training instatistical machine translation.
In Proceedings of ACL2003, pages 160-167.Franz Josef Och and Hermann Ney.
2000.
Improvedstatistical alignment models.
In Proceedings of ACL2000, pages 440-447.Franz Josef Och and Hermann Ney.
2004.
The alignmenttemplate approach to statistical machine translation.Computational Linguistics, 30:417-449.Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu.
2002.
BLEU: a method for automatic eval-uation of machine translation.
In Proceedings of ACL2002, pages 311-318.Andreas Stolcke.
2002.
SRILM - an extensible languagemodeling toolkit.
In Proceedings of the InternationalConference on Spoken Language Processing, pages901-904.Z.P.
Yin.
1999.
Methodologies and principles of Chi-nese abbreviation formation.
In Language Teachingand Study, 2:73-82.433
