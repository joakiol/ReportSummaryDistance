A PROPOSAL FOR LEXICAL DISAMBIGUATIONGeorge  A.  M i l l e r  Dan ie l  A.  Te ibe lP r inceton  Un ivers i ty  Cogn i t ive  Sc ience  Laboratory221 Nassau  St reetP r inceton ,  New Je rsey  08542ABSTRACTA method of sense resolution is proposed that is based on WordNet, anon-line lexical database that incorporates semantic relations (synonymy,antonymy, hyponymy, meronymy, causal and troponymic entailment) aslabeled pointers between word senses.
With WordNet, it is easy toretrieve sets of semantically rehted words, a facility that will be used forsense resolution during text processing, as follows.
When a word withmultiple senses is encountered, one of two procedures will be followed.Either, (1) words related in meaning to the alternative senses of thepolysemous word will be retrieved; new strings will be derived by substi-tuting these related words into the context of the polysemous word; alarge textual corpus will then be searched for these derived strings; andthat sense will be chosen that corresponds to the derived string that isfound most often in the corpus.
Or, (2) the context of the polysemousword will be used as a key to search a large corpus; all words found tooccur in that context will be neted; Word.Net will then be used to estimatethe semantic distance from those words to the alternative senses of thepolysemous word; and that sense will be chosen that is closest in meaningto other words occurring in the same contexL If successful, this pro-cedure could have practical applications to problems of informationretrieval, mechan/cal translation, intelligent tutoring systems, and else-where.BACKGROUNDAn example can set the problem.
Suppose that an automatic tran-scription device were to recognize the string of phonemes/ralt/ inthe flow of speech and could correctly identify it as an Englishword; the device would still have to decide whether the wordshould be spelled right, write, or rite.
And if the result were thensent to a language understanding system, there would be a furtherproblem of deciding which sense of the word the speakerintended to communicate.
These decisions, which are maderapidly and unconsciously by human listeners, are difficult toaccomplish computationally.
Ordinarily, anyone who reads andwrites English will be able to listen to the context of a string like/rait/and quickly decide which sense is appropriate.
The task isso easy, in fact, that laymen unfamiliar with these matters find ithard to understand what the problem is.
But computers havetrouble using context to make such apparently simple lexiealdecisions.
How those troubles might be overcome is the subjectof this paper.The process under consideration here is called "lexical disambi-guation," although that terminology can be misleading.
In theeveryday use of linguistic communication, true ambiguity isremarkably rare.
In the present context, however, "ambiguity"has taken on a special meaning that derives, apparently, from theclaim by Katz and Fodor (1963) that semantics, like syntax,should be restricted to sentences, "without reference to inforrna-tion about seaings" (p. 174).
Many sentences are indeed ambi-guous when viewed in a contextual vacuum.
More to the point,as Katz and Fodor emphasized, most words, when taken in isola-tion, are ambiguous in just this sense; they convey differentmeanings when used in different linguistic settings.
Hence, lexi-cal disambiguation is the process (either psychological or compu-tational) that reduces this putative ambiguity--that results in theselection of the appropriate sense of a polysemous word.
"Senseresolution" might be a better term, but by now "disambigua-t ion" is firmly established in the technical literature.Much attention has been given to lexical disarnbiguation by stu-dents of language tmderstanding.
In an excellent survey, Hirst(1987) distinguishes three types of lexical ambiguity: categorical,homonyrnous, and polysemous.
A word is categorically ambigu-ous if it can be used in different syntactic ategories; right, forexample, can be used as a noun, a verb, an adjective, or anadverb.
A word is a homonym if it has two clearly differentsenses; as an adjective, for example, right can mean the oppositeof wrong or the opposite of left.
A word is polysemous if it hassenses that are different but closely related; as a noun, for exam-ple, right can mean something that is morally approved, or some-thing that is factually correct, or something that is due one.
Sodefined, the distinction between homonymy and pelysernybecomes a matter of degree that is often difficult to draw; somelexicographers have tried to draw it on etymological grounds.
Forthe present discussion, however, the distinction will be ignored;homonymy and polysemy will be referred to together simply aspolysemy.Categorical ambiguity, however, is of a different kind and isresolved in a different way.
For the purposes of the presentpaper, it will be assumed that only content words are at issue, andthat the syntactic ategory of all content words in the text that isunder study can be determined automatically (Church, 1988;DeRose, 1988).
The problem is simply to decide which sense ofa content word--noun, verb, adjective, or adverb---is appropriatein a given linguistic context.
It will also be assumed that senseresolution for individual words can be accomplished on the basisof information about the irnrnediate linguistic context.
It shouldbe noted that this is an important simplification.
Inferences basedon knowledge of the world are frequently required to resolveuncertainties about a speaker's intentions in uttering a particularsentence, and endowing computers with such general knowledgein a usuable form is a particularly formidable problem.
Senseresolution for individual words, however, promises to be moremanageable.395Illustrating Multiple Senses of an English WordPhonology Orthography Syntax Semanticsrite Noun -Verb //rait/ , rightwrite mritual, ceremonymorally approved behaviorfactually correct statementsomething one is duethe right side of somethingjustify, vindicateavenge, redressrestore to the uprightsuitable, appropriatecorrecton the righthand sideprecisely, "r ight here"immediately, "r ight now"transcribe, inscribedraft, composeVerb ~ underwritecommunicate by writingHirst (1987) has reviewed various attempts o program computersto use linguistic ontexts in order to perform lexical disambigua-tion; that information eed not be repeated here.
It should benoted, however, that there are two contrasting ways to thinkabout linguistic ontexts, one based on co-oocurrence and theother on substitutability (Charles and Miller, 1989; Miller andCharles, 1991), usually referred to (Jenkins, 1954) as the syntag-marie and paradigmatic views.
The eo-eceurronce or syntagmafieapproach olds the target word constant and compares the con-texts in which it can appear; the substitutability or paradigmaticapproach olds the context constant and compares the words thatcan appear in it.
According to the co-occurrence approach, asso-ciations are formed between a word and the other words thatoccur with it in the same phrases and sentences; most psycho-linguists assume that syntagrnatie word associations are a conse-quence of temporal and spatial contiguity.
Many attempts toautomate lexical disambiguation have exploited these co-occurrence associations.
Lesk (1986) provides an elegantly sim-ple example: each sense of the polysemous word is retrievedfrom an on-line dictionary and compared with the target son-tence; the sense is chosen that has the most words in commonwith the target sentence.
According to the substitutability view,on the other hand, associations are formed between words thatcan be substituted into similar contexts; most psycholinguistsassume that paradigmatic word associations between words aremediated by their common contexts.
Syntactic categories ofwords, for example, must be learned on the basis of their inter-substitutability; Miller and Charles (1991) have argued thatsemantic similarities between words are also learned on the basisof inter-substitutability.The present proposal is an attempt to exploit paradigmatic associ-ations for the purpose of lexical disambiguation.
That is to say, itis proposed to use the substitutability of semantically similarwords in order to determine which sense of a polysemous word isappropriate.
In order to explain where the semantically similarwords might come from, however, it is necessary to describe thelexical database that is a central component of the present propo-sal for lexical disambiguation.WordNetStandard alphabetical procedures for organizing lexical informa-tion put together words that are spelled alike and scatter wordswith related meanings haphazardly through the list.
WordNet(Miller, 1990) is an attempt to use computers in order to achievea more efficient organization f the lexicon of English.
Inasmuchas it instantiates hypotheses based on results of psycholinguisficresearch, it can be said to be a dictionary based on psycholinguis-tic principles.
One obvious difference from a conventional on-line dictionary is that WordNet divides the lexicon into four syn-tactic categories: nouns, verbs, modifiers, and fimction words.
Infact, WordNet contains only nouns, verbs, and adjectives.Adverbs are omitted on the assumption that most of them dupli-cate adjectives; the relatively small set of English function wordsis omitted on the assumption that they are stored separately aspart of the syntactic omponent.
The most ambitious feature,however, is the attempt to organize lexical information i  termsof word meanings, rather than word forms.
In that respect,WordNet resembles a thesaurus.
It is not merely an on-linethesaurus, however.
In order to appreciate what more has beenattempted, it is necessary tounderstand the basic design.Lexical semantics begins with a recognition that a word is a con-ventional association between a lexicalizad concept and an utter-ance that plays a syntactic role.
The basic structure of any lexi-396con is a many:many mapping between word forms and wordsenses (Miller, 1986), with syntactic ategory as a parameter.When a particular word form can be used to express two or moreword senses it is said to be polysemous; when a particular wordsense can be expressed by two or more word forms they are saidto be synonymous (relative to a context).
Initially, WordNet wasto be concerned solely with the relations between word senses,but as the work proceeded it became increasingly clear that ques-tions of relations between word forms could not be ignored.
Forlexical disambiguation, however, word meanings are crucial, sothis description will focus on semantic relations.How word senses are to be represented is a central question forany theory of lexical semantics.
In WordNet, a lexicalizad con-cept is represented by simply listing the word forms that can (inan appropriate context) be used to express it: {W l, W 2 .
.
.
.
}.
(The curly brackets are used to surround sets of synonyms, orsynsets.)
For example, board can signify either a piece of lumberor a group of people assembled for some purpose; these twosenses can be represented by the synsets {board, plank} and{board, committee}.
These synsets do not explain what the con-cepts are; they serve merely to signal that two different conceptsexist.
People who know English are assumed to have alreadyacquired the concepts and are expected to recognize them fromthe words listed in the synsets.The mapping between word forms and word senses, therefore,can be represented asa mapping between written words and syn-sets.
Since English is rich in synonyms, synsets are oftensufficient for differentiation, but sometimes an appropriatesynonym is not available.
In thai case, the lexicalized conceptcan be represented by a short gloss, e.g., {board, (a person'smeals, provided regularly for money)}.
The gloss is not intendedfor use in constructing a new lexical concept, and differs from asynonym in that it is not used to gain access to stored informa-tion.
Its purpose is simply to enable users to distinguish thissense from others with which it could be confused.WordNet is organized by semantic relations.
A great variety ofsemantic relations could be defined, of course, but this work waslimited not only to relations that lay persons can appreciatewithout advanced training in linguistics, but also to relations thathave broad application throughout the lexicon.
In that way it washoped to capture the gross semantic structure of the English lexi-con, even though particular semantic domains (e.g., kin terms,color terms) may not be optimally analyzed.Since a semantic relation is a relation between meanings, andsince meanings are here represented by synsets, it is natural tothink of semantic relations as labeled pointers between synsets.
Inthe case of synonymy and antonymy, however, the semantic rela-tion is a relation between words.Synonymy: The most important semantic relation in WordNetis synonymy, since it allows the formation of synsets to representsenses.
According to one definition (usually attributed to Leib-niz) two expressions are synonymous if the substitution of onefor the other never changes the truth value of a statement inwhich the substitution is made.
By that definition, true synonymsare rare in natural languages.
A weakened version of thedefinition would make synonymy relative to a context: twoexpressions are synonymous in a context C if the substitution ofone for the other in C does not alter the truth value.
For example,the substitution of plank for board will seldom alter the truthvalue in carpentry contexts, although in other contexts that substi-tution might be totally inappropriate.
Note that this definition ofsynonymy in terms of substitutability makes it necessary toparti-tion the lexicon into nouns, adjectives, and verbs.
That is to say,if concepts are represented by synsets, and if synonyms must beinter-substitutable, then words in different syntactic ategoriescannot form synsets because they are not inter-substitutable.Antonymy: Another familiar semantic relation is antonymy.Like synonymy, antonymy is a semantic relation between words,not between concepts.
For example, the meanings {rise, ascend}and {fall, descend} may be conceptual opposites, but they are notantonyms; rise/fail are antonyms, and ascend/descend are anto-nyms, but most people hesitate and look thoughtful when askedwhether ise and descend, or fall and ascend, are antonyms.Antonymy provides the central organizing relation for adjectives:every predicable adjective ither has a direct antonym or is simi-lar to another adjective that has a direct antonym.
Moist, forexample, does not have a direct antonym, but it is similar to wet,which has the antonym dry; thus, dry is an indirect antonym ofmoist.Hyponymy: Hypenymy is a semantic relation between mean-ings: e.g., {map/e} is a hyponym of {tree}, and {tree} is a hypo-nym of {plant}.
Considerable attention has been devoted tohyponymy/hypemyrny (variously calledsubordinatiordsuperordination, subset/superset, or the ISA rela-tion).
Hyponymy is transitive and asymmetrical, nd, since thereis normally a single superordinate, it generates a hierarchicalsemantic structure, or tree.
Such hierarchical representations arewidely used in information retrieval systems, where they areknown as inheritance systems (Touretsky, 1986); a hyponyminherits all of the features of is superordinates.
Hypenymy pro-vides the central organizing principle for nouns.Meronymy: The part/whole (or rtASA) relation is known to lexi-cal semanticists as meronymy/holonymy.
One concept x is amemnym of another concept y if native speakers accept such con-structions as An x is a part of  y or y has x as a part.
If x is ameronym of y, then it is also a meronym of all hyponyms ofy.Entailment: A variety of entailment relations hold betweenverbs.
For example, the semantic relation between kill and die isone of causal entailment; tokill is to cause to die.
Similarly, thesemantic relation between march and walk is troponymy, anentailment of manner; to march is to walk in a certain manner.Other types of entailment hold between marry and divorce; adivorce entails a prior marriage.
These entailments, along withsynonymy and antonymy, provide the central organizing princi-ples for the verb lexicon.It should be obvious that, given this semantic organization of thelexical database, it is a simple matter to retrieve sets of words thathave similar senses.
The next step is to consider how suchrelated words can be used for lexical disambiguation.397THE PROPOSED SYSTEMIt is assumed that a grammatical text is to be processed, and thatthe processor is expected to use the textual context o determinethe appropriate sense of each successive content word.
Then, inbrief outline, the present proposal envisions a processor that willperform three operations:(1) Take a content word from the text and look it up in the lexicaldatabase; if a single sense is found, the problem is solved.
Ifmore than one sense is found, continue.
(2) Determine the syntactic ategory of each sense.
If a singlecategory is involved, go to operation three.
If more than onesyntactic ategory is found, use a "parts" program to deter-mine the appropriate eatagory.
If the word has only one senseas a member of that category, the problem is solved.
If theword has more than one sense in the appropriate syntacticcategory, continue.
(3) Determine which sense of the polysemous word is appropri-ate to the text.
If the word is a noun, determine which sensecan serve as an argument of the verb, or can be modified byan accompanying adjective.
If the word is verb or adjective,determine which sense can be combined with an accompany-ing noun phrase.The final operation is the critical step, of course, but beforedescribing how it might be implemented, a simplified examplewill help to make the central idea clear.
Suppose the processorencounters the sentence, the baby is in the pen, and tries to assignthe appropriate s nse to the noun pen.
It would first generalize thegiven context (e.g., with respect to number and tense), then findwords that are semantically related to the various senses of penand substitute them into the generalized context.
It would thenundertake a comparison of:(a/the baby is/was)/(the/some babies are/were) in a/the:(a) fountain pen~pencil~quill~crayon~stylus(b) sty/coop/cage/fold~pound(e ) playpen/playroomlnursery(d) prison~penitentiary/jail/brig~ dungeon( e ) swan/cygnet~goose~duck~owlIn order to decide that one of these is acceptable and the othersare unlikely, the processor might search an extensive corpus forstrings of the form "(a/the baby is/was)/(the babies are/were) inthe X," where X is one of the closely related words listed above.If the playpen/playroom~nursery expressions significantly out-number the others, the conventionally correct choice can bemade.
In other words, the processor will interrogate a corpusmuch the way a linguist might ask a native informant: "Can yousay this in your language?
"That is the basic strategy.
Words related in meaning to the dif-ferent senses of the polysemous word will be retrieved; newexpressions will be derived by substituting these related wordsinto the generalized context of the polysemous word; a large tex-tual corpus will then be searched for these derived expressions;that sense will be chosen that corresponds to the derived expres-sion that is found most often in the corpus.
(Alternatively, allcontexts of the semantically related words could be collected andtheir similarity to the target context could be estimated.
)We assume that the similarity of this strategy to the theory ofspreading activation (Quilliam 1968, 1969) is obvious.
Ofcourse, in order even to approach the best possible implementa-tion, a variety of possibilities will have to be explored.
Forexample, how much context should be preserved?
Too short, andit will not discriminate between different senses; too long and noinstances will be found in the corpus.
Should the grammaticalintegriw of the contexts be preserved?
Or, again, how large acorpus will be required?
Too small, and no instances will befound; too large and the system will be unacceptably large or theresponse unacceptably slow.
Fortunately, most of thepolysemous words occur relatively frequently in everyday usage,so a corpus of several million words should be adequate.
Or, stillagain, how closely related should the semantically related wordsbe?
Can superordinate rms be substituted?
How far can thecontexts be generalized?
Experience should quickly guide thechoice of sensible answers.As described so far, the proessor begins with WordNet in order tofind semantically related words that can be searched for in acorpus.
Obviously, it could all be done in the reverse order.
Thatis to say, the processor could begin by searching the corpus forthe given generalized context.
In the above example, it mightsearch for "(a/the baby is/was)/(the babies are/were) in the Y,"where Y is any word at all.
Then, given the set of Y words,WordNet could be used to estimate the semantic distance fromthese words to the alternative s nses of the polysemous word.
Asimilarity metric could easily be constructed by simply countingthe number of pointers between terms.
That sense would bechosen that was closest in meaning to the other words that werefound to occur in the same context.Whether WordNet is used to provide related words or to measuresemandc similarity, amajor component of the present proposal isthe search of a large textual corpus.
Since the corpus would notneed to be continually updated, it should be practical to developan inverted index, i.e., to divide the corpus into sentence itemsthat can be keyed by the content words in WordNet, then to com-pute hash codes and write inverted files (Lesk, 1978).
In 'thisway, a small file of relevant sentences could be rapidly assembledfor more careful examination, sothe whole process could be con-dueted on-line.
Even if response times were satisfactorily short,however, one feels that once a particular context has been used todisambiguate a polysemous word, it should never have to be doneagain.
That thought opens up possibilities for enlarging WordNetthat we will not speculate about at the present time.SOME OBVIOUS APPLICATIONSSeveral practical applications could result from a refiable lexicaldisambiguation device.
The fact that people see concepts wherecomputers see strings of characters i a major obstacle to human-machine interaction.Consider this situation.
A young student who is reading anassignment encounters an unfamifiar word.
When a dictionary isconsulted it turns out that the word has several senses.
The stu-dent reconsiders the original context, testing each definitionalgloss in turn, and eventually chooses a best fit.
It is a slow pro-398cess and a serious interruption of the student's task of under-standing the text.
Now compare this alternative.
A computer ispreumfing a reading assignment to the same studant when anunfamiliar word appears.
The student points to the word and thecomputer, which is able solve the polysemy problem, presents tothe student only the meaning that is appropriate in the givencontext--as if a responsive teacher were sitting at the student'sside.
The desired information is presented rapidly and the realtask of understanding is not interrupted.Or think of having a lexical disambiguator in your word process-ing system.
As you write, it could flag for you every word inyour text that it could not disambiguate on the basis of the con-text you have provided.
It might even suggest alternative word-ings.The application to mechanical translation is also obvious.
Apolysemous word in the source language must be disambiguatedbefore an appropriate word in the target language can be selected.The feasibility of multilingual WordNets has not been explored.Finally, consider the importance of disambiguation for informa-tion retrieval systems.
If, say, you were a radar engineer lookingfor articles about antennas and you were to ask an informationretrieval system for every article it had with antenna in the title orabstract., you might receive unwanted articles about insects anderustaceans---the so-called problem of false drops.
So you reviseyour descriptor to, say, metal antenna nd try again.
Now youhave eliminated the animals, but you have also eliminated articlesabout metal antennas that did not bother to include the wordmetal in the title or abstract--the so-called problem of misses.False drops and misses are the Scylla and Charybdis of informa-tion relrieval; anything that reduces one tends to increase theother.
But note that a lexical disambiguator could increase theprobability of selecting only those titles and abstracts in whichthe desired sense was appropriate; the efficiency of informationretrieval would be significantly increased.In short, a variety of practical advances could be implemented ifit were possible to solve the problem of lexical ambiguity insome tidy and reliable way.
The problem lies at the heart of theprocess of turning word forms into word meanings.
But the veryreason lexical disambiguation is important is also the reason thatit is difficult.REFERENCESCharles, W. O., and Miller, G. A.
"Contexts of antonymous adjectives,"AppliedPsycholin&uistics, Vol.10, 1989, pp.
357-375.Church, IC "A stochastic parts program and noun phrase parser forunrestricted text," Second Conference on @plied NaturalLanguage Processing, Austin, Texas, 1988.DeRose, S., "Grammatical category disambiguation by statistical organi-zation," Computational Linguistics, VoL 14, 1988, pp.
31-39.Hirst, G., Semantic Interpretation a d the Resolution o/Ambiguity.
Cam-bridge University Press, Cambridge, 1987.Jenkins, J. J., ?
'Traniitional organization: Association techniques."
InC.Osgood and T. A. Sebeok (Eds.
), Psycholinguistica: A Survey ofTheory and R~earch Problems.
Supplement, Journal of Abnor-real and Social Psychology, Vol.
52, 1954, pp.
112-118.Katz, J. J., and Fodor, J.
A.
"The structure of a semantic theory,"Language, Vol.
39, 1963, pp.
170-210.Lesk, M. E. "Some applications of inverted indexes on the Unix sys-tent," Unix Programmer's Manual, VoL 2a, Bell Laboratories,Murray Hill, NJ, 1978.Lesk, M. E. "Automatic sense discrimination: How to tell a pine conefrom an ice cream cone," manuscript, 1986.Miller, G. A.
"Dictionaries in the mind," Language and CognitiveProcesses, Vol.
1, 1986, 171-185.Miller, G. A.
(E&) Five papers on WordNet, International Journal ofl.~?ico&raphy, Vol.
3, No.
4, 1990, 235-312.Miller, G. A., and Charles, W. G. *'Contextual correlates of semanticsimilarity," Language and Cognitive Processes, Vol.
6, 1991.Qulllian, M.R.
"Semantic memory."
In Minsky, M. L (Ed.)
SemanticInformation Processing.
MIT Press, Cambridge, MA, 1968.Quillian, M. R. "The teachable anguage comprehender.
A simulationprogram and theory of language."
Communications ofthe ACM,VoL 12, 1969, 459-476.Touretzky, D. S. The Mathematics of Inheritance Systems.
MorganKaufman, Los Altos, California, 1986.399
