Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 855?859,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsIdentifying Sentiment WordsUsing an Optimization-based Model without Seed WordsHongliang Yu 1, Zhi-Hong Deng 2?, Shiyingxue Li 3Key Laboratory of Machine Perception (Ministry of Education),School of Electronics Engineering and Computer Science,Peking University, Beijing 100871, China1yuhongliang324@gmail.com2zhdeng@cis.pku.edu.cn3rachellieinspace@gmail.comAbstractSentiment Word Identification (SWI) is abasic technique in many sentiment analy-sis applications.
Most existing research-es exploit seed words, and lead to low ro-bustness.
In this paper, we propose a noveloptimization-based model for SWI.
Unlikeprevious approaches, our model exploitsthe sentiment labels of documents insteadof seed words.
Several experiments on re-al datasets show that WEED is effectiveand outperforms the state-of-the-art meth-ods with seed words.1 IntroductionIn recent years, sentiment analysis (Pang et al2002) has become a hotspot in opinion mining andattracted much attention.
Sentiment analysis is toclassify a text span into different sentiment polar-ities, i.e.
positive, negative or neutral.
Sentimen-t Word Identification (SWI) is a basic techniquein sentiment analysis.
According to (Ku et al2006)(Chen et al 2012)(Fan et al 2011), SWIcan be applied to many fields, such as determin-ing critics opinions about a given product, tweeterclassification, summarization of reviews, and mes-sage filtering, etc.
Thus in this paper, we focus onSWI.Here is a simple example of how SWI is appliedto comment analysis.
The sentence below is anmovie review in IMDB database:?
Bored performers and a lackluster plot andscript, do not make a good action movie.In order to judge the sentence polarity (thus we canlearn about the preference of this user), one mustrecognize which words are able to express senti-ment.
In this sentence, ?bored?
and ?lackluster?are negative while ?good?
should be positive, yet?Corresponding authorits polarity is reversed by ?not?.
By such analy-sis, we then conclude such movie review is a nega-tive comment.
But how do we recognize sentimentwords?To achieve this, previous supervised approach-es need labeled polarity words, also called seedwords, usually manually selected.
The wordsto be classified by their sentiment polarities arecalled candidate words.
Prior works study the re-lations between labeled seed words and unlabeledcandidate words, and then obtain sentiment polar-ities of candidate words by these relations.
Thereare many ways to generate word relations.
Theauthors of (Turney and Littman, 2003) and (Kajiand Kitsuregawa, 2007) use statistical measures,such as point wise mutual information (PMI), tocompute similarities in words or phrases.
Kanaya-ma and Nasukawa (2006) assume sentiment word-s successively appear in the text, so one couldfind sentiment words in the context of seed words(Kanayama and Nasukawa, 2006).
In (Hassan andRadev, 2010) and (Hassan et al 2011), a Markovrandom walk model is applied to a large word re-latedness graph, constructed according to the syn-onyms and hypernyms in WordNet (Miller, 1995).However, approaches based on seed words hasobvious shortcomings.
First, polarities of seedwords are not reliable for various domains.
Asa simple example, ?rise?
is a neutral word mostoften, but becomes positive in stock market.
Sec-ond, manually selection of seed words can be verysubjective even if the application domain is deter-mined.
Third, algorithms using seed words havelow robustness.
Any missing key word in the setof seed words could lead to poor performance.Therefore, the seed word set of such algorithmsdemands high completeness (by containing com-mon polarity words as many as possible).Unlike the previous research work, we identi-fy sentiment words without any seed words in thispaper.
Instead, the documents?
bag-of-words in-855formation and their polarity labels are exploited inthe identification process.
Intuitively, polarities ofthe document and its most component sentimen-t words are the same.
We call such phenomenonas ?sentiment matching?.
Moreover, if a word isfound mostly in positive documents, it is very like-ly a positive word, and vice versa.We present an optimization-based model, calledWEED, to exploit the phenomenon of ?sentimen-t matching?.
We first measure the importance ofthe component words in the labeled documents se-mantically.
Here, the basic assumption is that im-portant words are more sentiment related to thedocument than those less important.
Then, weestimate the polarity of each document using it-s component words?
importance along with theirsentiment values, and compare the estimation tothe real polarity.
After that, we construct an op-timization model for the whole corpus to weighthe overall estimation error, which is minimizedby the best sentiment values of candidate words.Finally, several experiments demonstrate the ef-fectiveness of our approach.
To the best of ourknowledge, this paper is the first work that identi-fies sentiment words without seed words.2 The Proposed Approach2.1 PreliminaryWe formulate the sentiment word identificationproblem as follows.
Let D = {d1, .
.
.
, dn} denotedocument set.
Vector l?
=???l1...ln???
represents theirlabels.
If document di is a positive sample, thenli = 1; if di is negative, then li = ?1.
We use thenotation C = {c1, .
.
.
, cV } to represent candidateword set, and V is the number of candidate words.Each document is formed by consecutive words inC. Our task is to predict the sentiment polarity ofeach word cj ?
C.2.2 Word ImportanceWe assume each document di ?
D is presentedby a bag-of-words feature vector f?i =???fi1...fiV??
?,where fij describes the importance of cj to di.
Ahigh value of fij indicates word cj contributes alot to document di in semantic view, and vice ver-sa.
Note that fij > 0 if cj appears in di, whilefij = 0 if not.
For simplicity, every f?i is normal-ized to a unit vector, such that features of differentdocuments are relatively comparable.There are several ways to define the wordimportance, and we choose normalized TF-IDF(Jones, 1972).
Therefore, we have fij ?TF?IDF (di, cj), and ?f?i?
= 1.2.3 Polarity ValueIn the above description, the sentiment polarity hasonly two states, positive or negative.
We extendboth word and document polarities to polarity val-ues in this section.Definition 1 Word Polarity Value: For each wordcj ?
C, we denote its word polarity value asw(cj).
w(cj) > 0 indicates cj is a positive word,while w(cj) < 0 indicates cj is a negative word.|w(cj)| indicates the strength of the belief of cj?spolarity.
Denote w(cj) as wj , and the word polar-ity value vector w?
=???w1...wV??
?.For example, ifw(?bad?)
< w(?greedy?)
< 0, wecan say ?bad?
is more likely to be a negative wordthan ?greedy?.Definition 2 Document Polarity Value: For eachdocument di, document polarity value isy(di) = cosine(f?i, w?)
=f?iT ?
w??w??
.
(1)We denote y(di) as yi for short.Here, we can regard yi as a polarity estimatefor di based on w?.
To explain this, Table 1 showsan example.
?MR1?, ?MR2?
and ?MR3?
arethree movie review documents, and ?compelling?and ?boring?
are polarity words in the vocabu-lary.
we simply use TF to construct the documentfeature vectors without normalization.
In the ta-ble, these three vectors, f?1, f?2 and f?3, are (3, 1),(2, 1) and (1, 3) respectively.
Similarly, we can getw?
= (1,?1), indicating ?compelling?
is a positiveword while ?boring?
is negative.
After normaliz-ing f?1, f?2 and f?3, and calculating their cosine sim-ilarities with w?, we obtain y1 > y2 > 0 > y3.These inequalities tell us the first two reviews arepositive, while the last review is negative.
Further-more, we believe that ?MR1?
is more positive than?MR2?.856?compelling?
?boring?MR1 3 1MR2 2 1MR3 1 3w 1 -1Table 1: Three rows in the middle shows the fea-ture vectors of three movie reviews, and the lastrow shows the word polarity value vector w?.
Forsimplicity, we use TF value to represent the wordimportance feature.2.4 Optimization ModelAs mentioned above, we can regard yi as a polari-ty estimate for document di.
A precise predictionmakes the positive document?s estimator close to1, and the negative?s close to -1.
We define thepolarity estimate error for document di as:ei = |yi ?
li| = |f?iT ?
w??w??
?
li|.
(2)Our learning procedure tries to decrease ei.
Weobtain w?
by minimizing the overall estimation er-ror of all document samplesn?i=1e2i .
Thus, the op-timization problem can be described asminw?n?i=1( f?iT ?
w??w??
?
li)2.
(3)After solving this problem, we not only obtain thepolarity of each word cj according to the sign ofwj , but also its polarity belief based on |wj |.2.5 Model SolutionWe use normalized vector x?
to substitute w??w??
, andderive an equivalent optimization problem:minx?E(x?)
=n?i=1(f?iT ?
x??
li)2s.t.
?x??
= 1.
(4)The equality constraint of above model makesthe problem non-convex.
We relax the equalityconstraint to ?x??
?
1, then the problem becomesconvex.
We can rewrite the objective functionas the form of least square regression: E(x?)
=?F ?
x?
?
l?
?2, where F is the feature matrix, andequals to????f?1T...f?nT???
?.Now we can solve the problem by convex op-timization algorithms (Boyd and Vandenberghe,2004), such as gradient descend method.
In eachiteration step, we update x?
by ?x?
= ?
?
(?
?E) =2?
?
(F T l?
?
F TF x?
), where ?
> 0 is the learningrate.3 Experiment3.1 Experimental SetupWe leverage two widely used document dataset-s.
The first dataset is the Cornell Movie ReviewData 1, containing 1,000 positive and 1,000 nega-tive processed reviews.
The other is the StanfordLarge Dataset 2 (Maas et al 2011), a collectionof 50,000 comments from IMDB, evenly dividedinto training and test sets.The ground-truth is generated with the help ofa sentiment lexicon, MPQA subjective lexicon 3.We randomly select 20% polarity words as theseed words, and the remaining are candidate ones.Here, the seed words are provided for the baselinemethods but not for ours.
In order to increase thedifficulty of our task, several non-polarity wordsare added to the candidate word set.
Table 2 showsthe word distribution of two datasets.Dataset Word Set pos neg non totalCornell seed 135 201 - 336candidate 541 806 1232 2579Stanford seed 202 343 - 545candidate 808 1370 2566 4744Table 2: Word DistributionIn order to demonstrate the effectiveness of ourmodel, we select two baselines, SO-PMI (Turneyand Littman, 2003) and COM (Chen et al 2012).Both of them need seed words.3.2 Top-K TestIn face of the long lists of recommended polaritywords, people are only concerned about the top-ranked words with the highest sentiment value.
Inthis experiment we consider the accuracy of thetop K polarity words.
The quality of a polarityword list is measured by p@K = Nright,KK , whereNright,K is the number of top-K words which arecorrectly recommended.1http://www.cs.cornell.edu/people/pabo/movie-review-data/2http://ai.stanford.edu/ amaas/data/sentiment/3http://www.cs.pitt.edu/mpqa/857WEED SO-PMI COMpositive words negative words positive words negative words positive words negative wordsgreat excellent bad stupid destiny lush cheap worst best great ridiculous badperfect perfectly worst mess brilliant skillfully ridiculous annoying will star plot evilterrific best boring ridiculous courtesy courtesy damn pathetic bad fun star garishtrue wonderfully awful plot gorgeous magnificent inconsistencies fool better plot dreadfully stupidbrilliant outstanding worse terrible temptation marvelously desperate giddy love horror pretty funTable 3: Case Study(a) Cornell Dataset(b) Stanford Dataset0.30.40.50.60.70.80.91p@10 p@20 p@50 p@100WEEDSO_PMICOM0.30.40.50.60.70.80.91p@10 p@20 p@50 p@100WEEDSO_PMICOMFigure 1: Top-K TestFigure 1 shows the final result of p@K, whichis the average score of the positive and negativelist.
We can see that in both datasets, our approachhighly outperforms two baselines, and the preci-sion is 14.4%-33.0% higher than the best baseline.p@10s of WEED for Cornell and Stanford dataset-s reach to 93.5% and 89.0%, and it shows the top10 words in our recommended list is exceptionallyreliable.
As the size of K increases, the accuracyof all methods falls accordingly.
This shows threeapproaches rank the most probable polarity wordsin the front of the word list.
Compared with thesmall dataset, we obtain a better result with largeK on the Stanford dataset.3.3 Case StudyWe conduct an experiment to illustrate the char-acteristics of three methods.
Table 3 shows top-10 positive and negative words for each method,where the bold words are the ones with correc-t polarities.
From the first two columns, we cansee the accuracy of WEED is very high, wherepositive words are absolutely correct and negativeword list makes only one mistake, ?plot?.
The oth-er columns of this table shows the baseline meth-ods both achieve reasonable results but do not per-form as well as WEED.Our approach is able to identify frequently usedsentiment words, which are vital for the applica-tions without prior sentiment lexicons.
The sen-timent words identified by SO-PMI are not sorepresentative as WEED and COM.
For example,?skillfully?
and ?giddy?
are correctly classified butthey are not very frequently used.
COM tends toassign wrong polarities to the sentiment words al-though these words are often used.
In the 5th and6th columns of Table 3, ?bad?
and ?horror?
arerecognized as positive words, while ?pretty?
and?fun?
are recognized as negative ones.
These con-crete results show that WEED captures the gener-ality of the sentiment words, and achieves a higheraccuracy than the baselines.4 Conclusion and Future WorkWe propose an effective optimization-based mod-el, WEED, to identify sentiment words from thecorpus without seed words.
The algorithm exploit-s the sentiment information provided by the docu-ments.
To the best of our knowledge, this paper isthe first work that identifies sentiment words with-out any seed words.
Several experiments on realdatasets show that WEED outperforms the state-of-the-art methods with seed words.Our work can be considered as the first stepof building a domain-specific sentiment lexicon.Once some sentiment words are obtained in a cer-tain domain, our future work is to improve WEEDby utilizing these words.858AcknowledgmentsThis work is partially supported by National Nat-ural Science Foundation of China (Grant No.61170091).ReferencesS.
Boyd and L. Vandenberghe.
2004.
Convex optimiza-tion.
Cambridge university press.L.
Chen, W. Wang, M. Nagarajan, S. Wang, and A.P.Sheth.
2012.
Extracting diverse sentiment expres-sions with target-dependent polarity from twitter.
InProceedings of the Sixth International AAAI Confer-ence on Weblogs and Social Media (ICWSM), pages50?57.Wen Fan, Shutao Sun, and Guohui Song.
2011.Probability adjustment na?
?ve bayes algorithm basedon nondomain-specific sentiment and evaluationword for domain-transfer sentiment analysis.
InFuzzy Systems and Knowledge Discovery (FSKD),2011 Eighth International Conference on, volume 2,pages 1043?1046.
IEEE.A.
Hassan and D. Radev.
2010.
Identifying text po-larity using random walks.
In Proceedings of the48th Annual Meeting of the Association for Compu-tational Linguistics, pages 395?403.
Association forComputational Linguistics.A.
Hassan, A. Abu-Jbara, R. Jha, and D. Radev.
2011.Identifying the semantic orientation of foreign word-s.
In Proceedings of the 49th Annual Meeting of theAssociation for Computational Linguistics: HumanLanguage Technologies, pages 592?597.K.S.
Jones.
1972.
A statistical interpretation of termspecificity and its application in retrieval.
Journal ofdocumentation, 28(1):11?21.N.
Kaji and M. Kitsuregawa.
2007.
Building lexiconfor sentiment analysis from massive collection ofhtml documents.
In Proceedings of the joint confer-ence on empirical methods in natural language pro-cessing and computational natural language learn-ing (EMNLP-CoNLL), pages 1075?1083.H.
Kanayama and T. Nasukawa.
2006.
Fully automat-ic lexicon expansion for domain-oriented sentimentanalysis.
In Proceedings of the 2006 Conference onEmpirical Methods in Natural Language Process-ing, pages 355?363.
Association for ComputationalLinguistics.Lun-Wei Ku, Yu-Ting Liang, and Hsin-Hsi Chen.2006.
Opinion extraction, summarization and track-ing in news and blog corpora.
In Proceedings ofAAAI-2006 spring symposium on computational ap-proaches to analyzing weblogs, volume 2001.A.L.
Maas, R.E.
Daly, P.T.
Pham, D. Huang, A.Y.
Ng,and C. Potts.
2011.
Learning word vectors for sen-timent analysis.
In Proceedings of the 49th annu-al meeting of the association for computational Lin-guistics (acL-2011).Miller.
1995.
Wordnet: a lexical database for english.Communications of the ACM, 38(11):39?41.B.
Pang, L. Lee, and S. Vaithyanathan.
2002.
Thumbsup?
: sentiment classification using machine learn-ing techniques.
In Proceedings of the ACL-02 con-ference on Empirical methods in natural languageprocessing-Volume 10, pages 79?86.
Association forComputational Linguistics.P.
Turney and M.L.
Littman.
2003.
Measuring praiseand criticism: Inference of semantic orientationfrom association.859
