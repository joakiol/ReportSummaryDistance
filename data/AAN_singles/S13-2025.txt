Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 138?143, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsSemEval-2013 Task 4: Free Paraphrases of Noun CompoundsIris HendrickxRadboud University Nijmegen &Universidade de Lisboairis@clul.ul.ptPreslav NakovQCRI, Qatar Foundationpnakov@qf.org.qaStan SzpakowiczUniversity of Ottawa &Polish Academy of Sciencesszpak@eecs.uottawa.caZornitsa KozarevaUniversity of Southern Californiakozareva@isi.eduDiarmuid O?
Se?aghdhaUniversity of Cambridgedo242@cam.ac.ukTony VealeUniversity College Dublintony.veale@ucd.ieAbstractIn this paper, we describe SemEval-2013 Task4: the definition, the data, the evaluation andthe results.
The task is to capture some of themeaning of English noun compounds via para-phrasing.
Given a two-word noun compound,the participating system is asked to producean explicitly ranked list of its free-form para-phrases.
The list is automatically comparedand evaluated against a similarly ranked listof paraphrases proposed by human annota-tors, recruited and managed through Ama-zon?s Mechanical Turk.
The comparison ofraw paraphrases is sensitive to syntactic andmorphological variation.
The ?gold?
rankingis based on the relative popularity of para-phrases among annotators.
To make the rank-ing more reliable, highly similar paraphrasesare grouped, so as to downplay superficial dif-ferences in syntax and morphology.
Threesystems participated in the task.
They all beata simple baseline on one of the two evalua-tion measures, but not on both measures.
Thisshows that the task is difficult.1 IntroductionA noun compound (NC) is a sequence of nounswhich act as a single noun (Downing, 1977), as inthese examples: colon cancer, suppressor protein,tumor suppressor protein, colon cancer tumor sup-pressor protein, etc.
This type of compounding ishighly productive in English.
NCs comprise 3.9%and 2.6% of all tokens in the Reuters corpus and theBritish National Corpus (BNC), respectively (Bald-win and Tanaka, 2004).The frequency spectrum of compound types fol-lows a Zipfian distribution (O?
Se?aghdha, 2008), somany NC tokens belong to a ?long tail?
of low-frequency types.
More than half of the two-nountypes in the BNC occur exactly once (Kim and Bald-win, 2006).
Their high frequency and high produc-tivity make robust NC interpretation an importantgoal for broad-coverage semantic processing of En-glish texts.
Systems which ignore NCs may give upon salient information about the semantic relation-ships implicit in a text.
Compositional interpretationis also the only way to achieve broad NC coverage,because it is not feasible to list in a lexicon all com-pounds which one is likely to encounter.
Even forrelatively frequent NCs occurring 10 times or morein the BNC, static English dictionaries provide only27% coverage (Tanaka and Baldwin, 2003).In many natural language processing applicationsit is important to understand the syntax and seman-tics of NCs.
NCs often are structurally similar,but have very different meaning.
Consider caffeineheadache and ice-cream headache: a lack of caf-feine causes the former, an excess of ice-cream ?
thelatter.
Different interpretations can lead to differentinferences, query expansion, paraphrases, transla-tions, and so on.
A question answering system mayhave to determine whether protein acting as a tumorsuppressor is an accurate paraphrase for tumor sup-pressor protein.
An information extraction systemmight need to decide whether neck vein thrombosisand neck thrombosis can co-refer in the same doc-ument.
A machine translation system might para-phrase the unknown compound WTO Geneva head-quarters as WTO headquarters located in Geneva.138Research on the automatic interpretation of NCshas focused mainly on common two-word NCs.
Theusual task is to classify the semantic relation under-lying a compound with either one of a small numberof predefined relation labels or a paraphrase from anopen vocabulary.
Examples of the former take onclassification include (Moldovan et al 2004; Girju,2007; O?
Se?aghdha and Copestake, 2008; Tratz andHovy, 2010).
Examples of the latter include (Nakov,2008b; Nakov, 2008a; Nakov and Hearst, 2008; But-nariu and Veale, 2008) and a previous NC paraphras-ing task at SemEval-2010 (Butnariu et al 2010),upon which the task described here builds.The assumption of a small inventory of prede-fined relations has some advantages ?
parsimony andgeneralization ?
but at the same time there are lim-itations on expressivity and coverage.
For exam-ple, the NCs headache pills and fertility pills wouldbe assigned the same semantic relation (PURPOSE)in most inventories, but their relational semanticsare quite different (Downing, 1977).
Furthermore,the definitions given by human subjects can involverich and specific meanings.
For example, Down-ing (1977) reports that a subject defined the NCoil bowl as ?the bowl into which the oil in the en-gine is drained during an oil change?, compared towhich a minimal interpretation bowl for oil seemsvery reductive.
In view of such arguments, linguistssuch as Downing (1977), Ryder (1994) and Coulson(2001) have argued for a fine-grained, essentiallyopen-ended space of interpretations.The idea of working with fine-grained para-phrases for NC semantics has recently grown in pop-ularity among NLP researchers (Butnariu and Veale,2008; Nakov and Hearst, 2008; Nakov, 2008a).
Task9 at SemEval-2010 (Butnariu et al 2010) was de-voted to this methodology.
In that previous work,the paraphrases provided by human subjects wererequired to fit a restrictive template admitting onlyverbs and prepositions occurring between the NC?sconstituent nouns.
Annotators recruited throughAmazon Mechanical Turk were asked to provideparaphrases for the dataset of NCs.
The gold stan-dard for each NC was the ranked list of paraphrasesgiven by the annotators; this reflects the idea that acompound?s meaning can be described in differentways, at different levels of granularity and capturingdifferent interpretations in the case of ambiguity.For example, a plastic saw could be a saw madeof plastic or a saw for cutting plastic.
Systems par-ticipating in the task were given the set of attestedparaphrases for each NC, and evaluated according tohow well they could reproduce the humans?
ranking.The design of this task, SemEval-2013 Task 4,is informed by previous work on compound anno-tation and interpretation.
It is also influenced bysimilar initiatives, such as the English Lexical Sub-stitution task at SemEval-2007 (McCarthy and Nav-igli, 2007), and by various evaluation exercises inthe fields of paraphrasing and machine translation.We build on SemEval-2010 Task 9, extending thetask?s flexibility in a number of ways.
The restric-tions on the form of annotators?
paraphrases was re-laxed, giving us a rich dataset of close-to-freeformparaphrases (Section 3).
Rather than ranking a set ofattested paraphrases, systems must now both gener-ate and rank their paraphrases; the task they performis essentially the same as what the annotators wereasked to do.
This new setup required us to innovatein terms of evaluation measures (Section 4).We anticipate that the dataset and task will be ofbroad interest among those who study lexical se-mantics.
We believe that the overall progress in thefield will significantly benefit from a public-domainset of free-style NC paraphrases.
That is why ourprimary objective is the challenging endeavour ofpreparing and releasing such a dataset to the re-search community.
The common evaluation taskwhich we establish will also enable researchers tocompare their algorithms and their empirical results.2 Task descriptionThis is an English NC interpretation task, which ex-plores the idea of interpreting the semantics of NCsvia free paraphrases.
Given a noun-noun compoundsuch as air filter, the participating systems are askedto produce an explicitly ranked list of free para-phrases, as in the following example:1 filter for air2 filter of air3 filter that cleans the air4 filter which makes air healthier5 a filter that removes impurities from the air.
.
.139Such a list is then automatically compared andevaluated against a similarly ranked list of para-phrases proposed by human annotators, recruitedand managed via Amazon?s Mechanical Turk.
Thecomparison of raw paraphrases is sensitive to syn-tactic and morphological variation.
The rankingof paraphrases is based on their relative popular-ity among different annotators.
To make the rank-ing more reliable, highly similar paraphrases aregrouped so as to downplay superficial differences insyntax and morphology.3 Data collectionWe used Amazon?s Mechanical Turk service tocollect diverse paraphrases for a range of ?gold-standard?
NCs.1 We paid the workers a small fee($0.10) per compound, for which they were asked toprovide five paraphrases.
Each paraphrase shouldcontain the two nouns of the compound (in sin-gular or plural inflectional forms, but not in an-other derivational form), an intermediate non-emptylinking phrase and optional preceding or followingterms.
The paraphrasing terms could have any partof speech, so long as the resulting paraphrase was awell-formed noun phrase headed by the NC?s head.We gave the workers feedback during data col-lection if they appeared to have misunderstood thenature of the task.
Once raw paraphrases had beencollected from all workers, we collated them into aspreadsheet, and we merged identical paraphrasesin order to calculate their overall frequencies.
Ill-formed paraphrases ?
those violating the syntacticrestrictions described above ?
were manually re-moved following a consensus decision-making pro-cedure; every paraphrase was checked by at leasttwo task organizers.
We did not require that theparaphrases be semantically felicitous, but we per-formed minor edits on the remaining paraphrases ifthey contained obvious typos.The remaining well-formed paraphrases weresorted by frequency separately for each NC.
Themost frequent paraphrases for a compound are as-signed the highest rank 0, those with the next-highest frequency are given a rank of 1, and so on.1Since the annotation on Mechanical Turk was going slowly,we also recruited four other annotators to do the same work,following exactly the same instructions.Total Min / Max / AvgTrial/Train (174 NCs)paraphrases 6,069 1 / 287 / 34.9unique paraphrases 4,255 1 / 105 / 24.5Test (181 NCs)paraphrases 9,706 24 / 99 / 53.6unique paraphrases 8,216 21 / 80 / 45.4Table 1: Statistics of the trial and test datasets: the totalnumber of paraphrases with and without duplicates, andthe minimum / maximum / average per noun compound.Paraphrases with a frequency of 1 ?
proposed fora given NC by only one annotator ?
always occupythe lowest rank on the list for that compound.We used 174+181 noun-noun compounds fromthe NC dataset of O?
Se?aghdha (2007).
The trialdataset, which we initially released to the partici-pants, consisted of 4,255 human paraphrases for 174noun-noun pairs; this dataset was also the trainingdataset.
The test dataset comprised paraphrases for181 noun-noun pairs.
The ?gold standard?
contained9,706 paraphrases of which 8,216 were unique forthose 181 NCs.
Further statistics on the datasets arepresented in Table 1.Compared with the data collected for theSemEval-2010 Task 9 on the interpretation of nouncompounds, the data collected for this new task havea far greater range of variety and richness.
For ex-ample, the following (selected) paraphrases for workarea vary from parsimonious to expansive:?
area for work?
area of work?
area where work is done?
area where work is performed?
.
.
.?
an area cordoned off for persons responsible forwork?
an area where construction work is carried out?
an area where work is accomplished and done?
area where work is conducted?
office area assigned as a work space?
.
.
.1404 ScoringNoun compounding is a generative aspect of lan-guage, but so too is the process of NC interpretation:human speakers typically generate a range of possi-ble interpretations for a given compound, each em-phasizing a different aspect of the relationship be-tween the nouns.
Our evaluation framework reflectsthe belief that there is rarely a single right answerfor a given noun-noun pairing.
Participating systemsare thus expected to demonstrate some generativityof their own, and are scored not just on the accu-racy of individual interpretations, but on the overallbreadth of their output.For evaluation, we provided a scorer imple-mented, for good portability, as a Java class.
Foreach noun compound to be evaluated, the scorercompares a list of system-suggested paraphrasesagainst a ?gold-standard?
reference list, compiledand rank-ordered from the paraphrases suggestedby our human annotators.
The score assigned toeach system is the mean of the system?s performanceacross all test compounds.
Note that the scorer re-moves all determiners from both the reference andthe test paraphrases, so a system is neither punishedfor not reproducing a determiner or rewarded forproducing the same determiners.The scorer can match words identically or non-identically.
A match of two identical words Wgoldand Wtest earns a score of 1.0.
There is a partialscore of (2 |P | / (|PWgold| + |PWtest|))2 for amatch of two words PWgold and PWtest that arenot identical but share a common prefix P , |P | > 2,e.g., wmatch(cutting, cuts) = (6/11)2 = 0.297.Two n-grams Ngold = [GW1, .
.
.
, GWn] andNtest = [TW1, .
.
.
, TWn] can be matched ifwmatch(GWi, TWi) > 0 for all i in 1..n. Thescore assigned to the match of these two n-grams isthen?i wmatch(GWi, TWi).
For every n-gramNtest = [TW1, .
.
.
, TWn] in a system-generatedparaphrase, the scorer finds a matching n-gramNgold = [GW1, .
.
.
, GWn] in the reference para-phrase Paragold which maximizes this sum.The overall n-gram overlap score for a referenceparaphrase Paragold and a system-generated para-phrase Paratest is the sum of the score calculatedfor all n-grams in Paratest, where n ranges from 1to the size of Paratest.This overall score is then normalized by dividingby the maximum value among the n-gram overlapscore for Paragold compared with itself and the n-gram overlap score for Paratest compared with it-self.
This normalization step produces a paraphrasematch score in the range [0.0 ?
1.0].
It punishes aparaphrase Paratest for both over-generating (con-taining more words than are found in Paragold)and under-generating (containing fewer words thanare found in Paragold).
In other words, Paratestshould ideally reproduce everything in Paragold,and nothing more or less.The reference paraphrases in the ?gold standard?are ordered by rank; the highest rank is assigned tothe paraphrases which human judges suggested mostoften.
The rank of a reference paraphrase mattersbecause a good participating system will aim to re-produce the top-ranked ?gold-standard?
paraphrasesas produced by human judges.
The scorer assignsa multiplier of R/(R + n) to reference paraphrasesat rank n; this multiplier asymptotically approaches0 for the higher values of n of ever lower-rankedparaphrases.
We choose a default setting of R = 8,so that a reference paraphrase at rank 0 (the highestrank) has a multiplier of 1, while a reference para-phrase at rank 5 has a multiplier of 8/13 = 0.615.When a system-generated paraphrase Paratest ismatched with a reference paraphrase Paragold, theirnormalized n-gram overlap score is scaled by therank multiplier attaching to the rank of Paragold rel-ative to the other reference paraphrases provided byhuman judges.
The scorer automatically chooses thereference paraphrase Paragold for a test paraphraseParatest so as to maximize this product of normal-ized n-gram overlap score and rank multiplier.The overall score assigned to each system fora specific compound is calculated in two differ-ent ways: using isomorphic matching of suggestedparaphrases to the ?gold-standard?s?
reference para-phrases (on a one-to-one basis); and using non-isomorphic matching of system?s paraphrases to the?gold-standard?s?
reference paraphrases (in a poten-tially many-to-one mapping).Isomorphic matching rewards both precision andrecall.
It rewards a system for accurately reproduc-ing the paraphrases suggested by human judges, andfor reproducing as many of these as it can, and inmuch the same order.141In isomorphic mode, system?s paraphrases arematched 1-to-1 with reference paraphrases on a first-come first-matched basis, so ordering can be crucial.Non-isomorphic matching rewards only preci-sion.
It rewards a system for accurately reproducingthe top-ranked human paraphrases in the ?gold stan-dard?.
A system will achieve a higher score in a non-isomorphic match if it reproduces the top-ranked hu-man paraphrases as opposed to lower-ranked humanparaphrases.
The ordering of system?s paraphrasesis thus not important in non-isomorphic matching.Each system is evaluated using the scorer in bothmodes, isomorphic and non-isomorphic.
Systemswhich aim only for precision should score highlyon non-isomorphic match mode, but poorly in iso-morphic match mode.
Systems which aim for pre-cision and recall will face a more substantial chal-lenge, likely reflected in their scores.A na?
?ve baselineWe decided to allow preposition-only paraphrases,which are abundant in the paraphrases suggestedby human judges in the crowdsourcing MechanicalTurk collection process.
This abundance means thatthe top-ranked paraphrase for a given compound isoften a preposition-only phrase, or one of a smallnumber of very popular paraphrases such as used foror used in.
It is thus straightforward to build a na?
?vebaseline generator which we can expect to scorereasonably on this task, at least in non-isomorphicmatching mode.
For each test compound M H,the baseline system generates the following para-phrases, in this precise order: H of M, H in M, Hfor M, H with M, H on M, H about M, H has M, H toM, H used for M, H used in M.This na?
?ve baseline is truly unsophisticated.
Noattempt is made to order paraphrases by their corpusfrequencies or by their frequencies in the trainingdata.
The same sequence of paraphrases is generatedfor each and every test compound.5 ResultsThree teams participated in the challenge, and alltheir systems were supervised.
The MELODI sys-tem relied on semantic vector space model builtfrom the UKWAC corpus (window-based, 5 words).It used only the features of the right-hand head nounto train a maximum entropy classifier.Team isomorphic non-isomorphicSFS 23.1 17.9IIITH 23.1 25.8MELODI-Primary 13.0 54.8MELODI-Contrast 13.6 53.6Naive Baseline 13.8 40.6Table 2: Results for the participating systems; the base-line outputs the same paraphrases for all compounds.The IIITH system used the probabilities of thepreposition co-occurring with a relation to identifythe class of the noun compound.
To collect statis-tics, it used Google n-grams, BNC and ANC.The SFS system extracted templates and fillersfrom the training data, which it then combined witha four-gram language model and a MaxEnt reranker.To find similar compounds, they used Lin?s Word-Net similarity.
They further used statistics from theEnglish Gigaword and the Google n-grams.Table 2 shows the performance of the partici-pating systems, SFS, IIITH and MELODI, and thena?
?ve baseline.
The baseline shows that it is rela-tively easy to achieve a moderately good score innon-isomorphic match mode by generating a fixedset of paraphrases which are both common andgeneric: two of the three participating systems,SFS and IIITH, under-perform the na?
?ve baselinein non-isomorphic match mode, but outperform itin isomorphic mode.
The only system to surpassthis baseline in non-isomorphic match mode is theMELODI system; yet, it under-performs against thesame baseline in isomorphic match mode.
No par-ticipating team submitted a system which would out-perform the na?
?ve baseline in both modes.6 ConclusionsThe conclusions we draw from the experience of or-ganizing the task are mixed.
Participation was rea-sonable but not large, suggesting that NC paraphras-ing remains a niche interest ?
though we believe itdeserves more attention among the broader lexicalsemantics community and hope that the availabil-ity of our freeform paraphrase dataset will attract awider audience in the future.142We also observed a varied response from our an-notators in terms of embracing their freedom to gen-erate complex and rich paraphrases; there are manypossible reasons for this including laziness, timepressure and the fact that short paraphrases are oftenvery appropriate paraphrases.
The results obtainedby our participants were also modest, demonstratingthat compound paraphrasing is both a difficult taskand a novel one that has not yet been ?solved?.AcknowledgmentsThis work has partially supported by a small but ef-fective grant from Amazon; the credit allowed usto hire sufficiently many Turkers ?
thanks!
And athank-you to our additional annotators Dave Carter,Chris Fournier and Colette Joubarne for their com-plete sets of paraphrases of the noun compounds inthe test data.ReferencesTimothy Baldwin and Takaaki Tanaka.
2004.
Transla-tion by machine of complex nominals: Getting it right.Proc.
ACL04 Workshop on Multiword Expressions: In-tegrating Processing, Barcelona, Spain, 24-31.Cristina Butnariu and Tony Veale.
2008.
A concept-centered approach to noun-compound interpretation.Proc.
22nd International Conference on Computa-tional Linguistics (COLING-08), Manchester, UK, 81-88.Cristina Butnariu, Su Nam Kim, Preslav Nakov, Diar-muid O?
Se?aghdha, Stan Szpakowicz, and Tony Veale.2010.
SemEval-2010 Task 9: The interpretation ofnoun compounds using paraphrasing verbs and prepo-sitions.
Proc.
5th International ACL Workshop on Se-mantic Evaluation, Uppsala, Sweden, 39-44.Seana Coulson.
2001.
Semantic Leaps: Frame-Shiftingand Conceptual Blending in Meaning Construction.Cambridge University Press, Cambridge, UK.Pamela Downing.
1977.
On the creation and use of En-glish compound nouns.
Language, 53(4): 810-842.Roxana Girju.
2007.
Improving the interpretation ofnoun phrases with cross-linguistic information.
Proc.45th Annual Meeting of the Association of Computa-tional Linguistics, Prague, Czech Republic, 568-575.Su Nam Kim and Timothy Baldwin.
2006.
Interpretingsemantic relations in noun compounds via verb seman-tics.
Proc.
ACL-06 Main Conference Poster Session,Sydney, Australia, 491-498.Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
Proc.Fourth International Workshop on Semantic Evalua-tions (SemEval-2007), Prague, Czech Republic, 48-53.Dan Moldovan, Adriana Badulescu, Marta Tatu, DanielAntohe, and Roxana Girju.
2004.
Models for the se-mantic classification of noun phrases.
Dan Moldovanand Roxana Girju, eds., HLT-NAACL 2004: Workshopon Computational Lexical Semantics, Boston, MA,USA, 60-67.Preslav Nakov and Marti Hearst.
2008.
Solving rela-tional similarity problems using the Web as a corpus.Proc.
46th Annual Meeting of the Association for Com-putational Linguistics ACL-08, Columbus, OH, USA,452-460.Preslav Nakov.
2008a.
Improved statistical machinetranslation using monolingual paraphrases.
Proc.
18thEuropean Conference on Artificial Intelligence ECAI-08, Patras, Greece, 338-342.Preslav Nakov.
2008b.
Noun compound interpretationusing paraphrasing verbs: Feasibility study.
Proc.13th International Conference on Artificial Intelli-gence: Methodology, Systems, Applications AIMSA-08, Varna, Bulgaria, Lecture Notes in Computer Sci-ence 5253, Springer, 103-117.Diarmuid O?
Se?aghdha.
2007.
Designing and Evaluatinga Semantic Annotation Scheme for Compound Nouns.In Proceedings of the 4th Corpus Linguistics Confer-ence, Birmingham, UK.Diarmuid O?
Se?aghdha.
2008.
Learning compoundnoun semantics.
Ph.D. thesis, Computer Laboratory,University of Cambridge.
Published as Universityof Cambridge Computer Laboratory Technical Report735.Diarmuid O?
Se?aghdha and Ann Copestake.
2009.
Usinglexical and relational similarity to classify semantic re-lations.
Proc.
12th Conference of the European Chap-ter of the Association for Computational LinguisticsEACL-09, Athens, Greece, 621-629.Diarmuid O?
Se?aghdha and Ann Copestake.
2008.
Se-mantic classification with distributional kernels.
InProc.
22nd International Conference on Computa-tional Linguistics (COLING-08), Manchester, UK.Mary Ellen Ryder.
1994.
Ordered Chaos: The Interpre-tation of English Noun-Noun Compounds.
Universityof California Press, Berkeley, CA, USA.Takaaki Tanaka and Tim Baldwin.
2003.
Noun-nouncompound machine translation: A feasibility studyon shallow processing.
Proc.
ACL-2003 Workshopon Multiword Expressions: Analysis, Acquisition andTreatment, Sapporo, Japan, 17-24.Stephen Tratz and Eduard Hovy.
2010.
A taxonomy,dataset, and classifier for automatic noun compoundinterpretation.
Proc.
48th Annual Meeting of the As-sociation for Computational Linguistics ACL-10, Up-psala, Sweden, 678-687.143
