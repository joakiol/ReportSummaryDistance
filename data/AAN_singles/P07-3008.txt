Proceedings of the ACL 2007 Student Research Workshop, pages 43?48,Prague, June 2007. c?2007 Association for Computational LinguisticsLimitations of Current Grammar Induction AlgorithmsBart CramerSchool of Behavioral and Cognitive NeurosciencesUniversity of GroningenGroningen, the Netherlandsbart.cramer@gmail.comAbstractI review a number of grammar inductionalgorithms (ABL, Emile, Adios), and testthem on the Eindhoven corpus, resulting indisappointing results, compared to the usu-ally tested corpora (ATIS, OVIS).
Also, Ishow that using neither POS-tags inducedfrom Biemann?s unsupervised POS-taggingalgorithm nor hand-corrected POS-tags asinput improves this situation.
Last, I arguefor the development of entirely incrementalgrammar induction algorithms instead of theapproaches of the systems discussed before.1 IntroductionGrammar induction is a task within the field of nat-ural language processing that attempts to construct agrammar of a given language solely on the basis ofpositive examples of this language.
If a successfulmethod is found, this will have both practical appli-cations and considerable theoretical implications.Concerning the practical side, this will make theengineering of NLP systems easier, especially forless widely studied languages.
One can conceivesuccessful GI algorithms as an inspiration for sta-tistical machine translation systems.Theoretically, grammar induction is important aswell.
One of the main assertions in the nativist?sposition is the Poverty of the Stimulus argument,which means that the child does not perceive enoughpositive examples of language throughout his earlyyouth to have learned the grammar from his parents,without the help of innate knowledge (or: UniversalGrammar), that severely constrains the number ofhypotheses (i.e.
grammars) that he can learn.
Provedmore strictly for formal grammars, Gold?s (1967)work showed that one cannot learn any type of su-perfinite grammar (e.g.
regular languages, context-free languages), if one only perceives (an unlim-ited amount of) positive examples.
After, say, n ex-amples, there is always more than 1 grammar thatwould be able to explain the seen examples, thusthese grammar might give different judgments on ann + 1th example, of which it is impossible to say inadvance which judgment is the correct one.But, given this is true, isn?t the grammar inductionpursuit deemed to fail?
Not really.
First, there arehints that children do receive negative information,and that they use it for grammar acquisition.
Also,the strictness required by Gold is not needed, and anapproximation in the framework of PAC (ProbablyApproximately Correct) or VC (Vapnis and Cher-vonenkis) could then suffice.
This, and other argu-ments favouring the use of machine learning tech-niques in linguistic theory testing, are very well re-viewed in Lappin and Shieber (2007).Several attempts have been made to create suchsystems.
The authors of these systems reportedpromising results on the ATIS and OVIS treebanks.
Itried to replicate these findings on the more compli-cated Eindhoven treebank, which turned out to yielddisappointing results, even inferior to very simplebaselines.
As an attempt to ameliorate this, and asan attempt to confirm Klein and Manning?s (2002)and Bod?s (2006) thesis that good enough unsuper-vised POS-taggers exist to justify using POS-tagsinstead of words in evaluating GI systems, I pre-43sented the algorithms with both POS-tags that wereinduced from Biemann?s unsupervised POS-taggingalgorithm and hand-corrected POS-tags.
This didnot lead to improvement.2 Current Grammar Induction Models2.1 AlgorithmsGrammar induction models can be split up into twotypes: tag-based and word-based grammar induc-tion.
The key feature that distinguishes betweenthese two is the type of input.
Tag-based systemsreceive part-of-speech tags as their input (i.e.
thewords are already labelled), and only induce rulesusing the given tags.
This kind of work is doneby, for instance, Klein and Manning (2005).
On theother hand, word-based models accept plain text asits input, and have to extract both the categories andthe syntactic rules from given input.Recently, several word-based grammar inductionalgorithms have been developed: Alignment-BasedLearning (van Zaanen, 2002), Adios (Solan et al,2005), Emile (Adriaans, 1992; Adriaans and Ver-voort, 2002) and GraSp1 (Henrichsen, 2002).
Al-though the means of computation and underlyingaims differ, they all rely to a certain extent on Har-ris?
principle (1951): if two word groups constitutethe same category, then they can be interchanged inany sentence, without damaging the grammaticalityof that sentence.
Hence, these GI system depend onthe inverse: if two word groups appear to occur inthe same contexts, they probably possess the samesyntactic characteristics.The most prominent example of this principle isAlignment-Based Learning, or ABL, (van Zaanen,2002).
This algorithm consists of two stages.
First,all sentences are aligned such that it finds a sharedand a distinct part of all pairs of sentences, sug-gesting that the distinct parts have the same type.For example, consider the pair ?I saw the man?
and?I saw John?.
Here, ?John?
and ?the man?
are cor-rectly identified as examples of the same type (NP?sin this case).
The second step, that takes the samecorpus as input, tries to identify the constituents inthat sentence.
Because the generated constituentsfound in the previous step might overlap, the correct1As there was no current working version of this system, Idid not include it in this project.John(.)Pat(.)Jim(.
)walks x xtalks x xsmiles x xTable 1: An example of some context/expressionpairs to show the workings of EMILE.
Note that, un-der standard settings, a rule covering this entire tablewill be inferred, causing a phrase like ?John talks?
tobe accepted, although there was no such input sen-tence.ones have to be selected.
Simple heuristics are usedto achieve this, for example to take the constituentthat was generated first (ABL-first) or to take theconstituent with the highest score on some proba-bilistic function (ABL-leaf).
For details, I refer tovan Zaanen (2000).
Because ABL compares all sen-tences in the corpus with all other sentences, the al-gorithm is quadratic in the number of sentences, buthas low memory demands.
Interestingly, ABL doesnot come up with an explicit grammar, but generatesjust a bracketed version of the corpus instead.Adios (Solan et al, 2005) uses Harris?
principleas well, although it attempts to create a grammar(either context-free or context-sensitive) more ex-plicitly.
The algorithm represents language as a di-rected pseudograph2 , with equivalence classes (ini-tially single words) as nodes.
Input sentences canbe regarded as ?snakes?
over the nodes in the graph.If enough support is found, words are merged intoequivalence classes, or frequently occurring edgesare put in a path (a rule in usual grammatical terms).This generalisation process is done iteratively, untilconvergence is reached.Emile (Adriaans, 1992; Adriaans and Vervoort,2002) is the system that to a greater extent tries topinpoint its reasons to accept a linguistic hypothe-sis.
Each rule is divided into expressions and types,where types should be the interchangeable part oftwo sentences.
Instead of explicitly comparing eachsentence with all other sentences, it incrementallybuilds up a table of type/expression pairs, and on thebasis of this table rules are extracted.
An example isgiven in table 1.
This incrementality has two major2This is a graph that allows for loops and multiple edges.44consequences: it makes the system vastly more effi-cient in terms of time, at the cost of rising memorydemands, and it models time linearly, in contrast toABL and Adios.2.2 EvaluationDifferent methods of evaluation are used in GI.
Oneof them is visual inspection (Henrichsen, 2002).This is not a reproducible and independent evalua-tion measure, and it does certainly not suffice as anassessment of the quality of the results.
However,Roberts and Atwell (2003) argue that this evaluationshould still be included in GI discussions.A second evaluation method is shown by Solanet al (2005), in which Adios had to carry out a testthat is available on the Internet: English as a SecondLanguage (ESL).
This test shows three sentences, ofwhich the examinee has to say which sentence is thegrammatical one.
Adios answers around 60% cor-rect on these questions, which is considered as inter-mediate for a person who has had 6 years of Englishlessons.
Although this sounds impressive, no exam-ples of test sentences are given, and the website isnot available anymore, so we are not able to assessthis result.A third option is to have sentences generated bythe induced grammar judged on their naturalness,and compare this average with the average of thesentences of the original corpus.
Solan et al (2005)showed that the judgments of Adios generated sen-tences were comparable to the sentences in their cor-pus.
However, the algorithm might just generatesoverly simple utterances, and will receive relativelyhigh scores that it doesn?t deserve.The last option for evaluation is to compare theparses with hand-annotated treebanks.
This givesthe most quantifiable and detailed view on the per-formance of a GI system.
An interesting compara-tive study between Emile and ABL using this eval-uation method is available in van Zaanen and Adri-aans (2001) where F-scores of 41.4% (Emile) and61.7% (ABL) are reported on the OVIS (OpenbaarVervoer Informatie Systeem3; Dutch) corpus, and25.4% and 39.2% on the ATIS (Air Traffic Informa-tion System; English) corpus.3This acronym means Public Transport Information System.3 Experiment 13.1 MotivationA major choice in evaluating GI systems is to decidewhich corpus to train the algorithm on.
The cre-ators of ABL and Emile chose to test on the ATISand OVIS corpus, which is, I believe, an unfortu-nate choice.
These corpora contain sentences thatare spoken to a computer, and represent a very lim-ited subset of language.
Deep recursion, one of theaspects that is hard to catch in grammar induction,does not occur often.
The average sentence lengthsare 7.5 (ATIS) and 4.4 (OVIS).
If we want to knowwhether a system is truly capable of bootstrappingknowledge about language, there is only one way totest it: by using natural language that is unlimitedin its expressive power.
Therefore, I will test ABL,Adios and Emile on the Eindhoven corpus, that con-tains 7K sentences, with an average length of ap-proximately 20 tokens.
This is, as far as I know, thefirst attempt to train and test word-based GI algo-rithms on such a complicated corpus.3.2 MethodThe Eindhoven corpus has been automatically anno-tated by Alpino (Bouma et al, 2000; van der Beeket al, 2002), a wide-coverage hand-written parserfor Dutch, with around 90% dependency triple ac-curacy.
Afterwards, this treebank has been manu-ally corrected.
The treebank does not literally con-tain trees, but graphs: some nodes can be copied, sothat linguistic structure can be analyzed in more de-tail.
However, by removing all double nodes it is stillpossible to retrieve a list of bracket-tuples from thesegraphs.
The graphs are also non-concatenative,meaning that a constituent can span word groups thatare not contiguous.
Therefore, if a sentence containsa constituent wi...wjwk...wl, with k ?
j > 1, threebracket-tuples are generated: (i, j), (k, l) and (i, l).Evaluation of the algorithm is done according toPARSEVAL, except for a few changes that are alsoproposed by Klein and Manning (2002).
The set ofbracket-pairs that is found in the Alpino treebankare called facts, and those from a grammar induc-tion algorithm predictions.
The intersection of thefacts and predictions are called hits.
From these wecan compute the unlabeled precision, recall and F-score.
The subtleties adopted from Klein and Man-45ning are the following: constituents of length 0 or 1,constituents that span the whole sentence and con-stituents just excluding punctuation are not takeninto account, as these are obvious predictions.Three baselines were created: an algorithm thatalways branches left4, idem for right-branching andan algorithm that performs binary branching on ran-dom points in the sentence.
Note that left-branchingand right-branching yield the maximum number ofpredictions.3.3 ResultsFrom the results in table 2, it can be seen that ABLscores best: it is the only one that is able to slightlyoutperform the random baseline.
This is surpris-ing, because it is the least complicated system of thethree.
Adios and Emile performed poorly.
It ap-pears that, with larger sentences, the search spacebecome too sparse to actually induce any meaning-ful structure.
This is expressed in the low number ofpredictions per sentence that Adios (1.5) and Emile(0.7) make.
Adjusting support parameters, to makethe algorithm accept more hypotheses, did not havethe intended effect.
Still, notice that Emile has a rel-atively high precision.In sum, none of the systems is convincingly ableto outperform the very simple baselines.
Neitherdid visual inspection give the impression that mean-ingful information was derived.
Therefore, it canbe concluded that current word-based GI algorithmsare not equipped to derive syntactic structure fromcorpora as complicated as the Eindhoven corpus.4 Experiment 24.1 MotivationThe second experiment deals with the differencebetween tag-based and word-based systems.
Intu-itively, the latter task seems to be more challenging.Still, Klein and Manning (2002) and Bod (2006)stick to tag-based models.
Their argumentation istwofold.First, Bod assumes that unsupervised POS-tagging can be done successfully, without explic-itly showing results that can confirm this.
Kleinand Manning did tag their text using a simple un-supervised POS-tagging algorithm, and this mod-4For example: [ [ [ I saw ] the ] large ] house.erately harmed their performance: their Context-Constituent Model?s F-score on Wall Street Journaltext fell from 71.1% to 63.2%.Second, Klein and Manning created context vec-tors for a number of non-terminals (NP, VP, PP), andextracted the two principal components from thesevectors.
They did the same with contexts of con-stituents and distituents.
The distribution of thesevectors suggest that the non-terminals were easierto distinguish from each other than the constituentsfrom the distituents, suggesting that POS-tagging iseasier than finding syntactic rules.
However, thisresult would be more convincing if this is true forPOS-tags as well.4.2 MethodIn order to test the argument above, and as an at-tempt to improve the results from the previous ex-periment, POS-tags were induced using Biemann?sunsupervised POS-tagger (Biemann, 2006).
Be-cause that algorithm needs at least 50M words towork reliably, it was trained on the concatenation ofthe Eindhoven corpus and the CLEF corpus (70Mwords, also newspaper text).
The tags of the Eind-hoven corpus are then used as input for the GI al-gorithms, both under same settings as experiment 1.The evaluation was done the same way as in experi-ment 1.The same method was carried out using hand-corrected tags.
Large and equal improvements willimply the justification for tag-based grammar in-duction.
If the models only improve on the hand-corrected tags, this will suggest the opposite.4.3 ResultsThe results can be found in table 3.
Generally, morepredictions were made with respect to experiment 1,due to the denser search space.
Only a convergenceto the baseline was achieved, especially by Adiosand Emile, that were very low in predictions in thefirst experiment.
Again, none of the tested systemswas able to clearly outperform the baselines.Because using neither induced nor hand-correctedmade the systems work more reliably, there seems tobe no strong evidence in favor or against Bod?s andKlein and Manning?s conjecture.
Therefore, there isno sound justification for tag-based grammar induc-tion yet.46Method Hits/Predictions Precision Recall F-scoreLeft 5.8K / 119K 4.9% 9.2% 6.4%Right 4.4K / 119K 3.6% 6.9% 4.8%Random 11K / 93K 11.7% 17.3% 14.0%ABL-leaf 4.0K / 24K 16.9% 6.4% 9.3%ABL-first 13K / 113K 11.6% 20.8% 14.9%Adios 319 / 11K 2.8% 0.5% 0.9%Emile 912 / 5.2K 17.3% 1.5% 2.7%Table 2: This table shows the results of experiment 1.
Left, Right and Random are baseline scores.
The twovariants of ABL differ in the selection phase.
62.9K facts were found in the Alpino treebank.Induced tags Hand-corrected tagsMethod Hits/Pred.
?s Precision Recall F-score Hits/Pred.
?s Precision Recall F-scoreABL-leaf 5K / 30K 16.8% 8.1% 10.9% 7.0K / 34K 21.0% 11.2% 14.6%ABL-first 11K / 125K 9.2% 18.2% 12.2% 12.6K / 123K 10.3% 20.0% 13.6%Adios 2.7K / 24K 11.2% 4.3% 6.3% 2.2K / 20K 11.0% 3.5% 5.3%Emile 1.8K / 16K 11.2% 2.9% 4.6% 1.7K / 19K 8.9% 2.7% 4.1%Table 3: This table shows the results of experiment 2.
The baseline scores are identical to the ones inexperiment 1.5 DiscussionThe results from experiment 1 and 2 clearly showthat ABL, Adios and Emile have severe shortcom-ings, and that they cannot derive meaningful struc-ture from language as complicated as the Eindhovencorpus.
An important reason for this is that a cor-pus with only 7K sentences is not able to sufficientlycover the search space.
This can be seen from thevery low number of predictions made by Adios andEmile: there was not enough support to accept hy-potheses.But how should we proceed?
Any algorithmbased solely on Harris?
principle can be either incre-mental (Emile) or non-incremental (ABL, Adios).The previous experiments show that very large cor-pora are needed to mitigate the very sparse searchspace, leading me to conclude that non-incrementalsystems are not suitable for the problem of gram-mar induction.
Also, incremental systems have theadvantage of an intuitive notion of time: it is al-ways clear which working hypothesis of a grammaris maintained.Emile retains a Boolean table with all combina-tions of types and expressions it has encountered upuntil a given moment.
This means that very infre-quent words demand a disproportionally large partof the memory.
Therefore, all found words and rulesshould be divided into three groups: pivotal, nor-mal and infrequent.
Initially, all encountered wordsare infrequent.
Transitions to the normal and piv-otal stage occur when an estimator of the relativefrequency is high enough, for example by taking thelower bound of the confidence interval (Mikheev,1997).
Ultimately, the number of words in the nor-mal and pivotal stage will converge to a constant.For example, if the relative frequency of a wordshould be larger than 0.01 to become pivotal, therecan only be 100 of these words.
Because one candefine upper limits for pivotal and normal words,the size of the bookkeeping table is limited as well.Also, when the system starts inducing syntactic cate-gories of words, very infrequent words should not beparsed as a separate category initially, but as a mem-ber of another open-class category.
This connects tothe cross-linguistic tendency that infrequent wordsgenerally have simple complementation patterns.One very important question remains: what in-tuitions should this imaginary system use to inducerules?
First, all sentences should be sorted by length.Then, for each sentence, the following steps aretaken:47?
Update the bookkeeping tables.?
Parse the sentence as deeply as possible.?
If the sentence cannot be parsed completely,induce all possible rules that would make theparse complete.
Add all these rules to the book-keeping tables.The last step deserves some extra attention.
Ifthe algorithm encounters the sentence ?he is such a(.
)?, we can safely infer that the unknown word at(.)
is a noun.
Inducing complementation patternsshould be possible as well.
Imagine that the algo-rithm understands NP?s and transitive verbs.
Thenconsider the following: ?John gave Tim a book?.It will parse ?John gave Tim?
as a sentence, and ?abook?
as a noun phrase.
Because these two shouldbe connected, a number of hypotheses are generated,for example: ?a book?
is a complement of ?Tim?
; ?abook?
is a complement of ?John gave Tim?
; ?a book?is a second complement of ?gave?.
Naturally, onlythe last hypothesis is correct.
All three inductionsare included, but only the last is likely to be repro-duced in later sentences in the corpus, because sen-tences of the form ?(.)
gave (.)
(.)?
are more likelythan ?John gave Tim (.)?
and ?Tim (.
)?.6 AcknowledgmentsI would like to thank Jennifer Spenader, Gertjan vanNoord and the anonymous reviewers for providingme their invaluable comments.ReferencesPieter W. Adriaans and Mark R. Vervoort.
2002.
TheEMILE 4.1 grammar induction toolbox.
In Proceed-ings of the 6th International Colloquium on Gram-mar Induction (ICGI), pages 293?295, Amsterdam,the Netherlands.Pieter W. Adriaans.
1992.
Language learning from a cat-egorial perspective.
Ph.D. thesis, University of Ams-terdam, NL.Chris Biemann.
2006.
Unsupervised part-of-speech tag-ging employing efficient graph clustering.
In Proceed-ings of ACL/COLING-2006 Students Research Work-shop, pages 7?12, Sydney, Australia.Rens Bod.
2006.
An all-subtrees approach to unsuper-vised parsing.
In Proceedings of ACL/COLING-2006,pages 865?872, Sydney, Australia.Gosse Bouma, Gertjan van Noord, and Robert Malouf.2000.
Alpino: wide-coverage computational analysisof Dutch.
In Proceedings of Computational Linguis-tics in the Netherlands (CLIN), pages 45?59, Tilburg,the Netherlands.E.
Mark Gold.
1967.
Language identification in thelimit.
Information and Control, 16:447?474.Zellig S. Harris.
1951.
Methods in Structural Linguis-tics.
University of Chicago Press, Chicago.Peter J. Henrichsen.
2002.
GraSp: Grammar learningfrom unlabelled speech corpora.
In Proceedings ofCoNLL-2002, pages 22?28, Pennsylvania, PA, USA.Dan Klein and Christopher D. Manning.
2002.
A gener-ative Constituent-Context Model for improved gram-mar induction.
In Proceedings of ACL-2001, pages128?135, Toulouse, France.Dan Klein and Christopher D. Manning.
2005.
Nat-ural language grammar induction with a genera-tive constituent-context model.
Pattern Recognition,9(38):1407?1419.Shalom Lappin and Stuart M. Shieber.
2007.
Machinelearning theory and practice as a source of insight intouniversal grammar.
Computational Linguistics, 43:1?34.Andrei Mikheev.
1997.
Automatic rule induction forunknown-word guessing.
Computational Linguistics,23(3):405?423.Andrew Roberts and Eric Atwell.
2003.
The use of cor-pora for automatic evaluation of grammar inferencesystems.
In Proceedings of the Corpus Linguistics2003 conference, pages 657?661, Lancaster, UnitedKingdom.Zach Solan, David Horn, Eytan Ruppin, and ShimonEdelman.
2005.
Unsupervised learning of natural lan-guages.
Proceedings of the National Academy of Sci-ences, 102(33):11629?11634.Leonoor van der Beek, Gosse Bouma, Robert Malouf,and Gertjan van Noord.
2002.
The Alpino depen-dency treebank.
In Proceedings of Computational Lin-guistics in the Netherlands (CLIN) 2001, pages 8?22,Enschede, the Netherlands.Menno van Zaanen and Pieter W. Adriaans.
2001.Alignment-Based Learning versus EMILE: A compar-ison.
In Proceedings of the 13th Dutch-Belgian Artifi-cial Intelligence Conference (BNAIC), pages 315?322,Amsterdam, the Netherlands.Menno van Zaanen.
2002.
Implementing Alignment-Based Learning.
In Proceedings of the 6th Interna-tional Colloquium on Grammatical Inference (ICGI),pages 312?314, Amsterdam, the Netherlands.48
