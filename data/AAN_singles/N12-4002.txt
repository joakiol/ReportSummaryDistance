T2: Structured Sparsity in Natural LanguageProcessing: Models, Algorithms andApplicationsAndr?
F. T. Martins, M?rio A. T. Figueiredo, and Noah A. SmithABSTRACTThis tutorial will cover recent advances in sparse modeling with diverse applications innatural language processing (NLP).
A sparse model is one that uses a relatively smallnumber of features to map an input to an output, such as a label sequence or parsetree.
The advantages of sparsity are, among others, compactness and interpretability; infact, sparsity is currently a major theme in statistics, machine learning, and signalprocessing.
The goal of sparsity can be seen in terms of earlier goals of featureselection and therefore model selection (Della Pietra et al, 1997; Guyon and Elisseeff,2003; McCallum, 2003).This tutorial will focus on methods which embed sparse model selection into theparameter estimation problem.
In such methods, learning is carried out by minimizing aregularized empirical risk functional composed of two terms: a "loss term," whichcontrols the goodness of fit to the data (e.g., log loss or hinge loss), and a "regularizerterm," which is designed to promote sparsity.
The simplest example is L1-normregularization (Tibshirani, 2006), which penalizes weight components individually, andhas been explored in various NLP applications (Kazama and Tsujii, 2003; Goodman,2004; Gao, 2007).
More sophisticated regularizers, those that use mixed norms andgroups of weights, are able to promote "structured" sparsity: i.e., they promote sparsitypatterns that are compatible with a priori knowledge about the structure of the featurespace.
These kind of regularizers have been proposed in the statistical and signalprocessing literature (Yuan and Lin, 2006; Zhao et al, 2009; Kim et al, 2010; Bach etal., 2011) and are a recent topic of research in NLP (Eisenstein et al, 2011; Martins etal, 2011, Das and Smith, 2012).
Sparsity-inducing regularizers require the use ofspecialized optimization routines for learning (Wright et al, 2009; Xiao, 2009; Langfordet al, 2009).The tutorial will consist of three parts: (1) how to formulate the problem, i.e., how tochoose the right regularizer for the kind of sparsity pattern intended; (2) how to solve theoptimization problem efficiently; and (3) examples of the use of sparsity within naturallanguage processing problems.OUTLINE1.
Introduction(30 minutes)o What is sparsity?o Why sparsity is often desirable in NLPo Feature selection: wrappers, filters, and embedded methodso What has been done in other areas: the Lasso and group-Lasso,compressive sensing, and recovery guaranteeso Theoretical and practical limitations of previous methods to typical NLPproblemso Beyond cardinality: structured sparsity2.
Group-Lasso and Mixed-Norm Regularizers(45 minutes)o Selecting columns in a grid-shaped feature spaceo Examples: multiple classes, multi-task learning, multiple kernel learningo Mixed L2/L1 and Linf/L1 norms: the group Lassoo Non-overlapping groupso Example: feature template selectiono Tree-structured groupso The general case: a DAGo Coarse-to-fine regularization3.
Coffee Break(15 minutes)4.
Optimization Algorithms(45 minutes)o Non-smooth optimization: limitations of subgradient algorithmso Quasi-Newton methods: OWL-QNo Proximal gradient algorithms: iterative soft-thresholding, forward-backwardand other splittingso Computing proximal stepso Other algorithms: FISTA, Sparsa, ADMM, Bregman iterationso Convergence rateso Online algorithms: limitations of stochastic subgradient descento Online proximal gradient algorithmso Managing general overlapping groupso Memory footprint, time/space complexity, etc.o The "Sparseptron" algorithm and debiasing5.
Applications(30 minutes):o Sociolinguistic association discoveryo Sequence problems: named entity recognition, chunkingo Multilingual dependency parsingo Lexicon expansion6.
Closing Remarks and Discussion(15 minutes)BIOSAndr?
F. T. MartinsInstituto de Telecomunica?
?es, Instituto Superior T?cnicoAv.
Rovisco Pais, 1, 1049-001 Lisboa, Portugal,and Priberam Inform?ticaAl.
Afonso Henriques, 41 - 2., 1000-123 Lisboa, Portugalafm--AT--cs.cmu.eduA.
Martins is a final year Ph.D. student in Carnegie Mellon's School of ComputerScience and the Instituto de Telecomunica?
?es at Instituto Superior T?cnico, where heis working on a degree in Language Technologies.
Martins' research interests includenatural language processing, machine learning, convex optimization, and sparsemodeling.
His dissertation focuses on new models and algorithms for structuredprediction with non-local features.
His paper "Concise Integer Linear ProgrammingFormulations for Dependency Parsing" received a best paper award at ACL 2009.M?rio A. T. FigueiredoInstituto de Telecomunica?
?es, Instituto Superior T?cnicoAv.
Rovisco Pais, 1, 1049-001 Lisboa, Portugalmario.figueiredo--AT--lx.it.ptM.
Figueiredo is a professor of electrical and computer engineering at Instituto SuperiorT?cnico (the engineering school of the Technical University of Lisbon) and his mainresearch interests include machine learning, statistical signal processing, andoptimization.
He recently guest co-edited a special issue of the IEEE Journal on SpecialTopics in Signal Processing devoted to compressive sensing (one of the central areasof research on sparsity) and gave several invited talks (and a tutorial at ICASSP 2012)on optimization algorithms for problems involving sparsity.Noah A. SmithSchool of Computer Science, Carnegie Mellon University5000 Forbes Avenue, Pittsburgh, PA 15213, USAnasmith--AT--cs.cmu.eduN.
Smith is Finmeccanica associate professor in language technologies and machinelearning at CMU.
His research interests include statistical natural language processing,especially unsupervised methods, machine learning for structured data, andapplications of natural language processing, including machine translation andstatistical modeling of text and quantitative social data.
He recently published a book,Linguistic Structure Prediction, about many of these topics; in 2009 he gave a tutorial atICML about structured prediction in NLP.ACKNOWLEDGMENTSThis tutorial was enabled by support from the following organizations:?
National Science Foundation (USA), CAREER grant IIS-1054319.?
Funda?
?o para a Ci?ncia e Tecnologia (Portugal), grant PEst-OE/EEI/LA0008/2011.?
Funda?
?o para a Ci?ncia e Tecnologia and Information and CommunicationTechnologies Institute (Portugal/USA), through the CMU-Portugal Program.?
QREN/POR Lisboa (Portugal), EU/FEDER programme, Discooperio project,contract 2011/18501.
