Proceedings of BioNLP Shared Task 2011 Workshop, pages 74?82,Portland, Oregon, USA, 24 June, 2011. c?2011 Association for Computational LinguisticsOverview of the Protein Coreference task in BioNLP Shared Task 2011Ngan NguyenUniversity of TokyoHongo 7-3-1, Bunkyoku, Tokyonltngan@is.s.u-tokyo.ac.jpJin-Dong KimDatabase Center for Life ScienceYayoi 2-11-16, Bunkyo-ku, Tokyojdkim@dbcls.rois.ac.jpJun?ichi TsujiiMicrosoft Research Asia5 Dan Ling Street, Haiian District, Beijingjtsujii@microsoft.comAbstractThis paper summarizes the Protein Coref-erence Resolution task of BioNLP SharedTask 2011.
After 7 weeks of system devel-opment period, the task received final sub-missions from 6 teams.
Evaluation resultsshow that state-of-the-art performance on thetask can find 22.18% of protein coreferenceswith the precision of 73.26%.
Analysis ofthe submissions shows that several types ofanaphoric expressions including definite ex-pressions, which occupies a significant part ofthe problem, have not yet been solved.1 IntroductionWhile named entity recognition (NER) and relationor event extraction are regarded as standard tasksof information extraction (IE), coreference resolu-tion (Ng, 2010; Bejan and Harabagiu, 2010) is moreand more recognized as an important component ofIE for a higher performance.
Without coreferenceresolution, the performance of IE is often substan-tially limited due to an abundance of coreferencestructures in natural language text, i.e.
informationpieces written in text with involvement of a corefer-ence structure are hard to be captured (Miwa et al,2010).
There have been several attempts for coref-erence resolution, particularly for newswire texts(Strassel et al, 2008; Chinchor, 1998).
It is also oneof the lessons from BioNLP Shared Task (BioNLP-ST, hereafter) 2009 that coreference structures inbiomedical text substantially hinder the progress offine-grained IE (Kim et al, 2009).To address the problem of coreference resolutionin molecular biology literature, the Protein Corefer-ence (COREF) task is arranged in BioNLP-ST 2011as a supporting task.
While the task itself is notan IE task, it is expected to be a useful compo-nent in performing the main IE tasks more effec-tively.
To establish a stable evaluation and to observethe effect of the results of the task to the main IEtasks, the COREF task particularly focuses on find-ing anaphoric protein references.The benchmark data sets for developing and test-ing coreference resolution system were developedbased on various manual annotations made to theGenia corpus (Ohta et al, 2002).
After 7 weeks ofsystem development phase, for which training anddevelopment data sets with coreference annotationwere given, six teams submitted their prediction ofcoreferences for the test data.
The best system ac-cording to our primary evaluation criteria is evalu-ated to find 22.18% of anaphoric protein referencesat the precision of 73.26%.This paper presents overall explanation of theCOREF task, which includes task definition (Sec-tion 2), data preparation (Section 4), evaluationmethods (Section 5), results (Section 7), and thor-ough analyses (Section 8) to figure out what areremaining problems for coreference resolution inbiomedical text.2 Problem DefinitionThis section provides an explanation of the corefer-ence resolution task in our focus, through examples.Figure 1 shows an example text segmented intofour sentences, S2 - S5, where anaphoric corefer-ences are illustrated with colored extends and ar-rows.
In the figure, protein names are highlighted inpurple, T4 - T10, and anaphoric protein references,e.g.
pronouns and definite noun phrases, are high-lighted in red, T27, T29, T30, T32, of which the an-74Figure 1: Protein coreference annotationtecedents are indicated by arrows if found in the text.In the example, the definite noun phrase (NP), thistranscription factor (T32), is a coreference to p65(T10).
Without knowing the coreference structure,it becomes hard to capture the information writtenin the phrase, nuclear exclusion of this transcriptionfactor, which is localization of p65 (out of nucleus)according to the framework of BioNLP-ST.A standard approach would include a step to findcandidate anaphoric expressions that may refer toproteins.
In this task, pronouns, e.g.
it or they, anddefinite NPs that may refer to proteins, e.g.
the tran-scription factor or the inhibitor are regarded as can-didates of anaphoric protein references.
This stepcorresponds to markable detection and anaphoric-ity determination steps in the jargon of MUC.
Thenext step would be to find the antecedents of theanaphoric expressions.
This step corresponds toanaphora resolution in the jargon of MUC.3 Task SettingIn the task, the training, development and test datasets are provided in three types of files: the text, theprotein annotation, and the coreference annotationfiles.
The text files contain plain texts which are tar-get of annotation.
The protein annotation files pro-vide gold annotation for protein names in the texts,and the coreference annotation files provide gold an-notation for anaphoric references to those proteinnames.
The protein annotation files are given to theparticipants, together with all the training, develop-ment and test data sets.
The coreference annotationfiles are not given with the test data set, and the taskfor the participants is to produce them automatically.In protein annotation files, annotations for proteinnames are given in a stand-off style encoding.
Forexample, those highlighted in purple in Figure 1 areprotein names, which are given in protein annotationfiles as follows:T4 Protein 275 278 p65T5 Protein 294 297 p50T6 Protein 367 372 v-relT7 Protein 406 409 p65T8 Protein 597 600 p50T9 Protein 843 848 MAD-3T10 Protein 879 882 p65The first line indicates there is a protein referencein the span that begins at 275th character and endsbefore 278th character, of which the text is ?p65?,and the annotation is identified by the id, ?T4?The coreference annotation files include three sortof annotations.
First, annotations for anaphoric pro-tein references are given.
For example, those in redin Figure 1 are anaphoric protein references:T27 Exp 179 222 the N.. 215 222 complexT29 Exp 307 312 whichT30 Exp 459 471 this .. 464 471 complexT32 Exp 1022 1047 this .. 1027 1047 tra..The first line indicates that there is an anaphoricprotein reference in the specified span, of which thetext is ?the NF-kappa B transcription factor com-plex?
(truncated due to limit of space), and that itsminimal expression is ?complex?.
Second, nounphrases that are antecedents of the anaphoric refer-ences are also given in the coreference annotationfile.
For example, T28 and T31 (highlighted in blue)are antecedents of T29 and T32, respectively, andthus given in the file:T28 Exp 264 297 NF-ka..T31 Exp 868 882 NF-ka..Third, the coreference relation between theanaphoric expressions and their antecedents aregiven in predicate-argument expressions1:R1 Coref Ana:T29 Ant:T28 [T5, T4]R2 Coref Ana:T30 Ant:T27R3 Coref Ana:T32 Ant:T31 [T10]The first line indicates there is a coreference rela-tion, R1, of which the anaphor is T29 and the an-tecedent is T28, and the relation involves two proteinnames, T5 and T4.Note that, sometimes, an anaphoric expression,e.g.
which (T29), is connected to more than oneprotein names, e.g.
p65 (T4) and p50 (T5).
Some-times, coreference structures do not involve any spe-cific protein names, e.g.
T30 and T27.
In order1Due to limitation of space, argument names are abbrevi-ated, e.g.
?Ana?
for ?Anaphora?, and ?Ant?
for ?Antecedent?75to establish a stable evaluation, our primary evalu-ation will focus only on coreference structures thatinvolve specific protein names, e.g.
T29 and T28,and T32 and T31.
Among the three, only two, R1and R3, involves specific protein references, T4 andT5, and T10.
Thus, finding of R2 will be ignoredin the primary evaluation.
However, those not in-volving specific protein references are also providedin the training data to help system development,and will be considered in the secondary evaluationmode.
See section 5 for more detail.4 Data PreparationThe data sets for the COREF task are producedbased on three resources: MedCO coreference an-notation (Su et al, 2008), Genia event annotation(Kim et al, 2008), and Genia Treebank (Tateisi etal., 2005).
Although the three have been developedindependently from each other, they are annotationsmade to the same corpus, the Genia corpus (Kim etal., 2008).
Since COREF was focused on findinganaphoric references to proteins (or genes), only rel-evant annotations were extracted from the MedCOcorpus though the following process:1.
From MedCo annotation, coreference entities thatwere pronouns and definite base NPs were ex-tracted, which became candidate anaphoric expres-sions.
The base NPs were determined by consultingGenia Tree Bank.2.
Among the candidate anaphoric expressions, thosethat could not be protein references were filteredout.
This process was done by checking the headnoun of NPs.
For example, definite NPs with ?cell?as their head noun were filtered out.
The remainingones became candidate protein coreferences.3.
The candidate protein coreferences and their an-tecedents according to MedCo annotation were in-cluded in the data files for COREF task.4.
The protein name annotations from Genia eventannotation were added to the data files to deter-mine which coreference expressions involve proteinname references.Table 1 summarizes the coreference entities in thetraining, development, and test sets for COREF task.In the table, the anaphoric entities are classified intofour types as follows:RELAT indicates relative pronouns or relative adjec-tives, e.g.
that, which, or whose.PRON indicates pronouns, e.g.
it.Type Train Dev TestRELAT 1193 254 349PRON 738 149 269Anaphora DNP 296 58 91APPOS 9 1 3N/C 11 1 2Antecedent 2116 451 674TOTAL 4363 914 1388Table 1: Statistics of coreference entities in COREF datasets: N/C = not-classified.DNP indicates definite NPs or demonstrative NPs, e.g.NPs that begin with the, this, etc.APPOS indicates coreferences in apposition.5 EvaluationThe coreference resolution performance is evaluatedin two modes.The Surface coreference mode evaluates the per-formance of finding anaphoric protein referencesand their antecedents, regardless whether the an-tecedents actually embed protein names or not.
Inother words, it evaluates the ability to predict thecoreference relations as provided in the gold coref-erence annotation file, which we call surface coref-erence links.The protein coreference mode evaluates the per-formance of finding anaphoric protein referenceswith their links to actual protein names (proteincoreference links).
In the implementation of theevaluation, the chain of surface coreference linkesis traced until an antecedent embedding a proteinname is found.
If a protein-name-embedding an-tecedent is connected to an anaphora through onlyone surfs link, we call the antecedent a direct pro-tein antecedent.
If a protein-name-embedding an-teceden is connected to an anaphora through morethan one surface link, we call it an indirect proteinantecedent, and the antecedents in the middle of thechain intermediate antecedents.
The performanceevaluated in this mode may be directly connectedto the potential performance in main IE tasks: themore the (anaphoric) protein references are found,the more the protein-related events may be found.For this reason, the protein coreference mode is cho-sen as the primary evaluation mode.Evaluation results for both evaluation modes are76given in traditional precision, recall and f-score,which are similar to (Baldwin, 1997).5.1 Surface coreferenceA response expression is matched with a gold ex-pression following partial match criterion.
In par-ticular, a response expression is considered cor-rect when it covers the minimal boundary, and isincluded in the maximal boundary of expression.Maximal boundary is the span of expression anno-tation, and minimal boundary is the head of ex-pression, as defined in MUC annotation schemes(Chinchor, 1998).
A response link is correct whenits two argument expressions are correctly matchedwith those of a gold link.5.2 Protein coreferenceThis is the primary evaluation perspective of the pro-tein coreference task.
In this mode, we ignore coref-erence links that do not reference to proteins.
Inter-mediate antecedents are also ignored.Protein coreference links are generated from thesurface coreference links.
A protein coreference linkis composed of an anaphoric expression and a pro-tein reference that appears in its direct or indirectantecedent.
Below is an example.Example:R1 Coref Ana:T29 Ant:T28 [T5, T4]R2 Coref Ana:T30 Ant:T27R3 Coref Ana:T32 Ant:T31 [T10]R4 Coref Ana:T33 Ant:T32In this example, supposing that there are four surfacelinks in the coreference annotation file (T29,T28),(T30,T27), (T32,T31), and (T33, T32), in whichT28 contains two protein mentions T5, T4, and T31contains one protein mention T10; thus, the proteincoreference links generated from these surface linksare (T29,T4), (T29,T5), (T32,T10), and (T33, T10).Notice that T33 is connected with T10 through theintermediate expression T32.Response expressions and generated response re-sult links are matched with gold expressions andlinks correspondingly in a way similar to the surfacecoreference evaluation mode.6 ParticipationWe received submissions from six teams.
Each teamwas requested to submit a brief description of theirteam, which was summarized in Table 2.Team Member Approach & ToolsUU 1 NLP ML (Yamcha SVM,Reconcile)UZ 5 NLP RB (-)CU 2 NLP RB (-)UT 1 biochemist ML (SVM-Light)US 2 AI ML (SVM-Light)UC 3 NLP, 1 BioNLP ML (Weka SVM)Table 2: Participation.
UU = UofU, UZ = UZH,CU=ConcordU, UT = UTurku, UZ = UZH, US =Uszeged, UC = UCD SCI, RB = Rule-based, ML = Ma-chine learning-based.TEAM RESP C P R FUU 86 63 73.26 22.18 34.05UZ 110 61 55.45 21.48 30.96CU 87 55 63.22 19.37 29.65UT 61 41 67.21 14.44 23.77US 259 9 3.47 3.17 3.31UC 794 2 0.25 0.70 0.37Table 3: Protein coreference results.
Total num-ber of gold link = 284.
RESP=response, C=correct,P=precision, R=recall, F=fscoreThe tool column shows the external tools usedin resolution processing.
Among these tools,there is only one team used an external coref-erence resolution framework, Reconcile, whichachieved the state-of-the-art performance for super-vised learning-based coreference resolution (Stoy-anov et al, 2010b).7 Results7.1 Protein coreference resultsEvaluation results in the protein coreference modeare shown in Table 3.
The UU team got the high-est f-score 34.05%.
The UZ and CU teams arethe second- and third-best teams with 30.96% and29.65% f-score correspondingly, which are compa-rable to each other.
Unfortunately, two teams, USand UC could not produce meaningful results, andthe other four teams show performance optimizedfor high precision.
It was expected that the 22.18%of protein coreferences may contribute to improvethe performance on main task, which was not ob-served this time, unfortunately.The first ranked system by UU utilized Recon-77TEAM RESP C P R FUU 360 43 11.94 20.48 15.09UZ 736 51 6.93 24.29 10.78CU 365 36 9.86 17.14 12.52UT 452 50 11.06 23.81 15.11US 259 4 1.54 1.90 1.71UC 797 1 0.13 0.48 0.20Table 4: Surface coreference results.
Total num-ber of gold link = 210.
RESP=response, C=correct,P=precision, R=recall, F=fscoreUU UTS-correct & P-missing 8 29S-missing & P-correct 16 5Table 5: Count of anaphors that have different status indifferent evaluation modes.
S = surface coreference eval-uation mode, P = protein coreference evaluation modecile which was originally developed for newswiredomain.
It supports the hypothesis that machinelearning-based coreference resolution tool trainedon different domains can be helpful for the bio med-ical domain; however, it still requires some adapta-tions.7.2 Surface coreference resultsTable 4 shows the evaluation results in the surfacelink mode.
The overall performances of all the sys-tems are low, in which recalls are much higher thanthe precisions.
One possible reason of the low re-sults is because most of the teams focus on resolv-ing pronominal coreference; however, they failed tosolve some difficult types of pronoun such as ?it?,?its?, ?these?, ?them?, and ?which?, which occupythe majority of anaphoric pronominal expressions(Table 1).
Definite anaphoric expressions were ig-nored by almost all of the systems (except one sub-mission).The results show that the protein coreference res-olution is not a trivial task; and many parts remainschallenging.
In next section, we analyze about po-tential reason of the low results, and discuss possibledirections for further improvement.Ex 1 GOLDT5 DQalpha and DQbeta trans heterodimericHLA-DQ moleculesT6 such trans-dimersT7 whichR1 T6 T5 [T3, T4]R2 T7 T6RESPT5 such trans-dimersT6 whichR1 T6 T5Ex 2 GOLDT18 Five members of this family(MYC, SCL, TAL-2, LYL-1 and E2A)T20 theirR3 T20 T18 [T3, T2, T5, T4]RESPT19 Five membersT20 theirR2 T20 T19Table 6: Example of surface-correct & protein-missingcases.
Protein names are underlined, and the min-valuesare in italic.8 Analysis8.1 Why the rankings based on the twoevaluation methods are not the same?Comparing with the protein coreference mode, wecan see the rankings based on two evaluation meth-ods are different.
In order to find out what led tothis interesting difference, we further analyzed thesubmissions from the two teams UT and UU.
TheUT team achieved the highest f-score in the surfaceevaluation mode, but was in the fourth rank in theprotein evaluation mode.
Meanwhile, the score ofUU team was slightly less than the UT team in theformer mode, but got the highest in the later (Table3 and Table 4).
In other words, there is no clear cor-relation between the two evaluation results.Because the two precisions in surface evaluationmode are not much different, the recalls were themain contribution in the difference of f-score.
An-alyzing the correct and missing examples in bothevaluation modes, we found that there are anaphorswhose surface links are correct, while the proteinlinks with the same anaphors are evaluated as miss-ing; and vice versa with missing surface links andcorrect protein links.
Counts of anaphors of each78type are shown in Table 5.
In this table, the cellat column UT and row S-correct and P-missing canbe interpreted as following.
There are 29 anaphorsin the UT response whose surface links are correctbut protein links are missing, which contributes pos-itively to the recall in surface coreference mode, andnegatively to that in protein coreference mode.Table 6 shows two examples of S-correct andP-missing.
In the first example, we can see thatthe gold antecedent proteins are contained in an in-direct antecedent.
Therefore, when the interme-diate antecedent is correctly detected by the sur-face link R1, but the indirect antecedent is not de-tected, the anaphor is not linked to it antecedentproteins ?DQalpha?
and ?DQbeta?.
Another reasonis because response antecedents do not include an-tecedent proteins.
This is actually the problem ofexpression boundary detection.
An example of thisis example 2 (Table 6), in which the response sur-face link R2 is correct, but the protein links to thefour proteins are not detected, because the responseantecedent ?five members?
does not include the pro-tein mentions ?SCL, TAL-2, LYL-1 and E2A?.
How-ever, the response antecedent expression is correctbecause it contains the minimal boundary ?mem-bers?.For S-missing and P-correct, we found thatanaphors are normally directly linked to antecedentproteins.
In other words, expression boundary issame as protein boundary.
Another case is that re-sponse antecedents contain the antecedent proteins,but are evaluated as incorrect because the expres-sion boundary of the response expression is largerthan the gold expression.
An example is shown inTable 7 where the response expression ?a secondGCR, termed GCRbeta?
includes the gold expres-sion ?GCRbeta?.
Therefore, although the surfacelink is incorrect because the response expression isevaluated as incorrect, the protein coreference linkreceives a full score .The difference reflects the characteristics of thetwo evaluation methods.
The analysis result alsoshows the affect of markable detection or expressiondetection on the resolution evaluation result.8.2 Protein coreference analysisWe want to see how well each system performs oneach type of anaphor.
However, the type informationEx 3 GOLDT17 GCRbetaT18 whichR2 T18 T17 [T4]RESPT16 a second GCR, termed GCRbetaT19 whichR2 T19 T16Table 7: Examples of S-missing and P-correctis not explicitly included in the response, so it hasto be induced automatically.
We done this by find-ing the first word of anaphoric expression; then, wecombine it with 1 if the expression is a single-wordexpression, or 2 if the expression is multi-word, tocreate a sub type value for each anaphor of bothgold and response anaphors.
After that, subtypes aremapped with the anaphor types specified in Section4 using the mapping in Table 10.Protein coreference resolution results by sub typeare given in Table 9 and 8.
It can be easily seen inTable 9 which team performed well on which typeof anaphor.
In particular, the CU system was good atresolving the RELAT, APPOS and other types.
TheUU team performed well on the DNP type.
And forthe PRON type, UZ was the best team.
In theory,knowing this, we can combine strengths of the teamsto tackle all the types.We analyzed false positive protein anaphora linksto see what types of anaphora are solved by eachsystem.
The recalls in Table 11 are calculated basedon the anaphor type information manually annotatedin the gold data.
Comparing with those in Table 9,there is a small difference due to the automatic in-duction of anaphoric types based on sub types.
Itcan be seen in the table 11 that only 77.5 percent ofRELAT-typed anaphora links were resolved (by CUteam), although this type is supposed to be the eas-iest type.
Examining the output data, we found thatthe system tends to choose the nearest expressionas the antecedent of a relative pronoun; however,this is not always correct, as in the following exam-ples from the UofU submission: ?We also identifiedfunctional Aiolos-binding sites1a in the Bcl-2 pro-moter1b, which1 are able to activate the luciferasereporter gene.
?, and ?Furthermore, the analysis ofIkappaBalpha turnover demonstrated an increased79PRON P- P- P- P- P- P- DNP D- RELAT R-both-2 it-1 its-1 one-2 that-1 their-1 these-2 this-2 those-1 which-1 whose-1 N/CUU 36.4 64.4 2 13.3 18.2 62 5 30.8UZ 46.2 35.7 53.3 7.1 12.5 5.4 59 66.7 15.4CU 62 70.9 5 42.1UT 9.5 36.8 10 34.6 9.5 5 30.8US 13.9 22.9UC 28.6 9.1Table 8: Fine-grained results (f-score, %)Team PRON P- P- DNP D- D- RELAT R- R- Others O- O-P R F P R F P R F P R FUU 79.0 11.5 20.1 66.7 5.9 10.8 71.3 56.0 62.7 100.0 18.3 30.8UZ 62.9 16.9 26.7 12.5 4.4 6.5 71.4 46.7 56.5 50.0 9.1 15.4CU 64.6 68.0 66.2 50.0 36.4 42.1UT 72.7 12.3 21.1 14.3 1.5 2.7 73.3 29.3 41.9 100.0 18.2 30.8US 27.3 6.9 11.0UC 9.1 1.5 2.6Table 9: Protein coreference results by coreference type (fscore, %).
P = precision, R = recall, F = f-score.
O = Others.TEAM A R D P OUU 0.0 62.0 5.7 11.1 0.0UZ 0.0 49.3 4.3 17.0 0.0CU 0.0 77.5 0.0 0.0 0.0UT 0.0 32.4 1.4 11.9 14.3US 0.0 0.0 0.0 6.7 0.0UC 0.0 0.0 1.4 0.7 0.0Table 11: Exact recalls by anaphor type, based on man-ual type annotation.
A=APPOS, R=RELAT, D=DNP,P=PRON, O=OTHERdegradation of IkappaBalpha2a in HIV-1-infectedcells2b that2 may account for the constitutive DNAbinding activity.?.
Expressions with the same indexare coreferential expressions.
The a subscript indi-cates correct antecedent, and b subscript indicatesthe wrong one.
In these examples, the relative pro-noun that and which are incorrectly linked with thenearest expression, which is actually part of post-modifier or the correct antecedent expression.For the DNP type, recall of the best system is lessthan 6 percent (Table 11), although it is an impor-tant type which occupies almost one fifth of all pro-tein links (Table 1).
There is only one team, the UCteam, attempted to tackle the anaphor; however, itresulted in many spurious links.
The other teamsdid not make any prediction on this type.
A possi-ble reason of this is because there are much morenon-anaphoric definite noun phrases than anaphoricones, which making it difficult to train an effectiveclassier for anaphoricity determination.
We have toseek for a better method for solving the DNP links,in order to significantly improve protein coreferenceresolution system.Concerning the PRON type, Table 8 shows thatexcept for that-1, no other figures are higher than50 percent f-score.
This is an interesting obser-vation because pronominal anaphora problem hasbeen reported with much higher results on otherdomains(Raghunathan et al, 2010), and also onother bio data (hsiang Lin and Liang, 2004).
Oneof the reasons for the low recall is because targetanaphoric pronouns in the bio domain are neutral-gender and third-person pronouns(Nguyen and Kim,2008), which are difficult to resolve than other typesof pronouns(Stoyanov et al, 2010a).8.3 Protein coreference analysis - IntermediateantecedentAs mentioned in the task setting, anaphors can di-rectly link to their antecedent, or indirectly link viaone or more intermediate antecedents.
We countedthe numbers of correct direct and indirect proteincoreference links in each submission (Table 12).80Sub type Type Count Sub type Type Count Sub type Type Countboth 1 PRON 2 both 2 PRON 4 either 1 PRON 0it 1 PRON 17 its 1 PRON 61 one 2 PRON 1such 2 DNP 2 that 1 RELAT 37 the 2 DNP 20their 1 PRON 27 them 1 PRON 1 these 1 PRON 1these 2 DNP 26 they 1 PRON 5 this 1 PRON 1this 2 DNP 20 those 1 PRON 9 which 1 RELAT 37whose 1 RELAT 1 whose 2 RELAT 0 (others) N/C 11Table 10: Mapping from sub type to coreference type.
Count = number of anaphorsTEAM A R R D D P P ODi Di In Di In Di In DiUU 44 4 15UZ 35 2 1 23CU 54 1UT 22 1 1 16 1US 8 1UC 1 1Total 1 64 7 65 5 126 9 7Table 12: Numbers of correct protein coreference linksby anaphor type and by number of antecedents, based onmanual type annotation.
A=APPOS, R=RELAT, D=DNP,P=PRON, O=Others.
Di=direct, In=indirect.APPOS and Others types do not have any intermedi-ate antecedent, thus there is only one column markedwith D (direct protein coreference link).
We cansee in this table that very few indirect links weredetected.
Therefore, there is place to improve ourresolution system by focusing on detection of suchlinks.8.4 Surface coreference resultsBecause inclusion of all expressions was not a re-quirement of shared task submission, the submittedresults may not contain expressions that do not in-volve in any coreference links.
Therefore, it is un-fair to evaluate expression detection based on the re-sponse expressions.Evaluation results for anaphoricity determinationare shown in Table 13.
The calculation is performedas following.
Supposing that every anaphor has aresponse link, the number of anaphors is numberof distinct anaphoric expressions inferred from theresponse links, which is given in the first column.The total number of gold anaphors are also calcu-lated in similar way.
Since response expressionsare lined with gold expressions before evaluation,Team Resp Align P R FUU 360 94.2 19.4 33.3 24.6UZ 736 75.8 22.0 77.1 34.2CU 365 89.6 15.3 26.7 19.5UT 452 92.0 18.1 39.0 24.8US 259 9.3 6.2 7.6 6.8UC 797 6.8 1.1 4.3 1.8Table 13: Anaphoricity determination results.
Total num-ber of gold anaphors = 210.
Resp = number of responseanchors, Align = alignment rate(%), P = precision (%), R= recall (%), F = f-score (%)we provided the alignment rate for reference in thesecond column of the table.
The third and forthcolumns show the precisions and recalls.
In theory,low anaphoricity determination precision results inmany spurious response links, while low recall be-comes the bottle neck for the overall coreferenceresolution recall.
Therefore, we can conclude thatthe low performance of anaphoricity determinationcontribute to the low coreference evaluation results(Table 4, Table 3).9 ConclusionThe coreference resolution supporting task ofBioNLP Shared Task 2011 has drawn attention fromresearchers of different interests.
Although the over-all results are not good enough to be helpful for themain shared tasks as expected, the analysis results inthis paper shows the coreference types which haveand have not yet been successfully solved.
Tack-ling the remained problems in expression bound-ary detection, anaphoricity determination and reso-lution algorithms for difficult types of anaphors suchas definite noun phrases should be the future work.Then, it would be interesting to see how much coref-erence can contribute to event extraction.81ReferencesB.
Baldwin.
1997.
Cogniac: High precision with limitedknowledge and linguistic resources.
In Proceedings ofthe ACL?97/EACL?97 Workshop on Operational Fac-tors in Practical, Robust Anaphora Resolution, pages38?45, Madrid, Spain.Cosmin Bejan and Sanda Harabagiu.
2010.
Unsuper-vised event coreference resolution with rich linguis-tic features.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguistics,pages 1412?1422, Uppsala, Sweden, July.
Associationfor Computational Linguistics.Nancy Chinchor.
1998.
Overview of MUC-7/MET-2.In Message Understanding Conference (MUC-7) Pro-ceedings.Yu hsiang Lin and Tyne Liang.
2004.
Pronominal andsortal anaphora resolution for biomedical literature.In In Proceedings of ROCLING XVI: Conference onComputational Linguistics and Speech Processing.Jin-Dong Kim, Tomoko Ohta, and Jun?ichi Tsujii.
2008.Corpus annotation for mining biomedical events fromlterature.
BMC Bioinformatics, 9(1):10.Jin-Dong Kim, Tomoko Ohta, Sampo Pyysalo, Yoshi-nobu Kano, and Jun?ichi Tsujii.
2009.
Overviewof BioNLP?09 Shared Task on Event Extraction.In Proceedings of Natural Language Processing inBiomedicine (BioNLP) NAACL 2009 Workshop, pages1?9.Makoto Miwa, Rune S?tre, Jin-Dong Kim, and Jun?ichiTsujii.
2010.
Event extraction with complex eventclassification using rich features.
Journal of Bioinfor-matics and Computational Biology (JBCB), 8(1):131?146, February.Vincent Ng.
2010.
Supervised noun phrase coreferenceresearch: The first fifteen years.
In Proceedings of theACL, pages 1396?1411.Ngan Nguyen and Jin-Dong Kim.
2008.
Exploring do-main differences for the design of a pronoun resolutionsystem for biomedical texts.
In Proceedings of 22ndInternational Conference on Computational Linguis-tics (COLING-2008).T Ohta, Y Tateisi, H Mima, and J Tsujii.
2002.
Ge-nia corpus: an annotated research abstract corpus inmolecular biology domain.
Proceedings of the Hu-man Language Technology Conference (HLT 2002),San Diego, California, pages 73?77.Karthik Raghunathan, Heeyoung Lee, Sudarshan Ran-garajan, Nate Chambers, Mihai Surdeanu, Dan Juraf-sky, and Christopher Manning.
2010.
A multi-passsieve for coreference resolution.
In Proceedings ofthe 2010 Conference on Empirical Methods in Natu-ral Language Processing, pages 492?501, October.V.
Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,and D. Hysom.
2010a.
Coreference resolution withreconcile.
In Proceedings of the Conference of the48th Annual Meeting of the Association for Compu-tational Linguistics (ACL 2010).V.
Stoyanov, C. Cardie, N. Gilbert, E. Riloff, D. Buttler,and D. Hysom.
2010b.
Reconcile: A coreference res-olution platform.
In Tech Report - Cornell University.Stephanie Strassel, Mark Przybocki, Kay Peterson, ZhiyiSong, and Kazuaki Maeda.
2008.
Linguistic Re-sources and Evaluation Techniques for Evaluation ofCross-Document Automatic Content Extraction.
InProceedings of the 6th International Conference onLanguage Resources and Evaluation (LREC 2008).Jian Su, Xiaofeng Yang, Huaqing Hong, Yuka Tateisi,and Jun?ichi Tsujii.
2008.
Coreference Resolution inBiomedical Texts: a Machine Learning Approach.
InOntologies and Text Mining for Life Sciences?08.Yuka Tateisi, Akane Yakushiji, Tomoko Ohta, andJun?ichi Tsujii.
2005.
Syntax annotation for the geniacorpus.
In International Joint Conference on Natu-ral Language Processing, pages 222?227, Jeju Island,Korea, October.82
