Error-Driven Pruning of Treebank Grammarsfor Base Noun Phrase Identif icationCla i re  Card ie  and  Dav id  P ie rceDepar tment  of Computer  ScienceCornel l  Univers i tyI thaca,  NY 14853cardie, pierce@cs.cornel l .eduAbst rac tFinding simple, non-recursive, base noun phrases isan important subtask for many natural languageprocessing applications.
While previous empiricalmethods for base NP identification have been rathercomplex, this paper instead proposes a very simplealgorithm that is tailored to the relative simplicityof the task.
In particular, we present a corpus-basedapproach for finding base NPs by matching part-of-speech tag sequences.
The training phase of the al-gorithm is based on two successful techniques: firstthe base NP grammar is read from a "treebank" cor-pus; then the grammar is improved by selecting ruleswith high "benefit" scores.
Using this simple algo-rithm with a naive heuristic for matching rules, weachieve surprising accuracy in an evaluation on thePenn Treebank Wall Street Journal.1 In t roduct ionFinding base noun phrases is a sensible first stepfor many natural anguage processing (NLP) tasks:Accurate identification of base noun phrases is ar-guably the most critical component of any partialparser; in addition, information retrieval systemsrely on base noun phrases as the main source ofmulti-word indexing terms; furthermore, the psy-cholinguistic studies of Gee and Grosjean (1983) in-dicate that text chunks like base noun phrases playan important role in human language processing.
Inthis work we define base NPs to be simple, nonre-cursive noun phrases - -  noun phrases that do notcontain other noun phrase descendants.
The brack-eted portions of Figure 1, for example, show the baseNPs in one sentence from the Penn Treebank WallStreet Journal (WSJ) corpus (Marcus et al, 1993).Thus, the string the sunny confines of resort townslike Boca Raton and Hot Springs is too complex tobe a base NP; instead, it contains four simpler nounphrases, each of which is considered a base NP: thesunny confines, resort towns, Boca Raton, and HotSprings.Previous empirical research has addressed theproblem of base NP identification.
Several algo-rithms identify "terminological phrases" - -  certainWhen \[it\] is \[time\] for \[their biannual powwow\] ,\[the nation\] 's \[manufacturing titans\] typicallyjet off to \[the sunny confines\] of \[resort towns\]like \[Boca Raton\] and \[Hot Springs\].Figure 1: Base NP Examplesbase noun phrases with initial determiners and mod-ifiers removed: Justeson & Katz (1995) look forrepeated phrases; Bourigault (1992) uses a hand-crafted noun phrase grammar in conjunction withheuristics for finding maximal length noun phrases;Voutilainen's NPTool (1993) uses a handcrafted lex-icon and constraint grammar to find terminologicalnoun phrases that include phrase-final prepositionalphrases.
Church's PARTS program (1988), on theother hand, uses a probabilistic model automati-cally trained on the Brown corpus to locate corenoun phrases as well as to assign parts of speech.More recently, Ramshaw & Marcus (In press) ap-ply transformation-based l arning (Brill, 1995) tothe problem.
Unfortunately, it is difficult to directlycompare approaches.
Each method uses a slightlydifferent definition of base NP.
Each is evaluated ona different corpus.
Most approaches have been eval-uated by hand on a small test set rather than by au-tomatic omparison to a large test corpus annotatedby an impartial third party.
A notable exception isthe Ramshaw & Marcus work, which evaluates theirtransformation-based l arning approach on a baseNP corpus derived from the Penn Treebank WSJ,and achieves precision and recall levels of approxi-mately 93%.This paper presents a new algorithm for identi-fying base NPs in an arbitrary text.
Like some ofthe earlier work on base NP identification, ours isa trainable, corpus-based algorithm.
In contrast oother corpus-based approaches, however, we hypoth-esized that the relatively simple nature of base NPswould permit heir accurate identification using cor-respondingly simple methods.
Assume, for example,that we use the annotated text of Figure 1 as ourtraining corpus.
To identify base NPs in an unseen218text, we could simply search for all occurrences of thebase NPs seen during training - -  it, time, their bian-nual powwow, .
.
.
,  Hot Springs - -  and mark themas base NPs in the new text.
However, this methodwould certainly suffer from data sparseness.
Instead,we use a similar approach, but back off from lexicalitems to parts of speech: we identify as a base NPany string having the same part-of-speech tag se-quence as a base NP from the training corpus.
Thetraining phase of the algorithm employs two previ-ously successful techniques: like Charniak's (1996)statistical parser, our initial base NP grammar isread from a "treebank" corpus; then the grammaris improved by selecting rules with high "benefit"scores.
Our benefit measure is identical to that usedin transformation-based learning to select an orderedset of useful transformations (Brill, 1995).Using this simple algorithm with a naive heuristicfor matching rules, we achieve surprising accuracyin an evaluation on two base NP corpora of varyingcomplexity, both derived from the Penn TreebankWSJ.
The first base NP corpus is that used in theRamshaw & Marcus work.
The second espouses aslightly simpler definition of base NP that conformsto the base NPs used in our Empire sentence ana-lyzer.
These simpler phrases appear to be a goodstarting point for partial parsers that purposely de-lay all complex attachment decisions to later phasesof processing.Overall results for the approach are promising.For the Empire corpus, our base NP finder achieves94% precision and recall; for the Ramshaw & Marcuscorpus, it obtains 91% precision and recall, which is2% less than the best published results.
Ramshaw& Marcus, however, provide the learning algorithmwith word-level information in addition to the part-of-speech information used in our base NP finder.By controlling for this disparity in available knowl-edge sources, we find that our base NP algorithmperforms comparably, achieving slightly worse preci-sion (-1.1%) and slightly better ecall (+0.2%) thanthe Ramshaw & Marcus approach.
Moreover, ourapproach offers many important advantages thatmake it appropriate for many NLP tasks:* Training is exceedingly simple..
The base NP bracketer is very fast, operatingin time linear in the length of the text..
The accuracy of the treebank approach is goodfor applications that require or prefer fairly sim-ple base NPs..
The learned grammar is easily modified for usewith corpora that differ from the training texts.Rules can be selectively added to or deletedfrom the grammar without worrying about or-dering effects.
* Finally, our benefit-based training phase offersa simple, general approach for extracting ram-mars other than noun phrase grammars fromannotated text.Note also that the treebank approach to base NPidentification obtains good results in spite of a verysimple algorithm for "parsing" base NPs.
This is ex-tremely encouraging, and our evaluation suggests atleast two areas for immediate improvement.
First,by replacing the naive match heuristic with a proba-bilistic base NP parser that incorporates lexical pref-erences, we would expect a nontrivial increase in re-call and precision.
Second, many of the remainingbase NP errors tend to follow simple patterns; thesemight be corrected using localized, learnable repairrules.The remainder of the paper describes the specificsof the approach and its evaluation.
The next sectionpresents the training and application phases of thetreebank approach to base NP identification i moredetail.
Section 3 describes our general approach forpruning the base NP grammar as well as two instan-tiations of that approach.
The evaluation and a dis-cussion of the results appear in Section 4, along withtechniques for reducing training time and an initialinvestigation i to the use of local repair heuristics.2 The  Treebank  ApproachFigure 2 depicts the treebank approach to base NPidentification.
For training, the algorithm requiresa corpus that has been annotated with base NPs.More specifically, we assume that the training corpusis a sequence of words wl, w2,...,  along with a set ofbase NP annotations b(il&), b(i~j~),.
.
.
,  where b(ij)indicates that the NP brackets words i through j:\[NP Wi, .
.
.
,  W j\].
The goal of the training phase is tocreate a base NP grammar from this training corpus:1.
Using any available part-of-speech tagger, as-sign a part-of-speech tag ti to each word wi inthe training corpus.2.
Extract from each base noun phrase b(ij) in thetraining corpus its sequence of part-of-speechtags tl .
.
.
.
, t j  to form base NP rules, one ruleper base NP.3.
Remove any duplicate rules.The resulting "grammar" can then be used to iden-tify base NPs in a novel text.1.2.Assign part-of-speech tags tl, t2,.. ,  to the inputwords wl, w2, ?
?
?Proceed through the tagged text from leftto right, at each point matching the NPrules against he remaining part-of-speech tagst i , t i+ l , .
.
,  in the text.219Training PhaseTraining CorpusWhen lit\] is \[time\] for \[their biannual powwowl.\[ the nation I's I manufacturing titans I typically jetoffto \[the sunny confinesl of Ireson townsl like\[Boca Ratonl and IHot Springs\[.Tagged TextWhen/W'RB \[it/PRP\] is/VBZ \[time/NN\] for/IN \[their/PRP$biannual/JJ powwow/NN\] ./.
\[the/DT nation/NN\] 's/POSImanufacmring/VBG titans/NNSI typically/RB jet/VBPoff/RP to/TO Ithe/DT snnny/JJ confines/NNSI of/INI resort/NN towns/NNS \] like/IN I Boca/NNP Raton/NNPIand/CC IHot/NNP Spring~NNPI.~lP Ru les<PRP><NN><PRP$ JJ NN><DT NN><VBG NNS><DT JJ NNS><NN NNS><NNP NNP>Application PhaseNovel Text ,Not this year.
National Association of Manufacturers settledon the Hoosier capital of Indianapolis for its next meeting.And the city decided to treat its guests more like royalty orrock sta~ than factory owners.Tagged TextNot/RB this/DT year/NN J. National/NNPAssociation/NNP of/IN ManufacturerffNNP settled/VBDon/IN the/DT Hoosier/NNP capital/NN of/INlndianapoli~NNP for/IN its/PRP$ nexV'JJ meeting/NN J.And/CC the/DT city/NN decided/VBD to/TO treaV'VBits/PRP$ guesl.,;/NNS more/J JR like/IN royahy/NN or/CCrock/NN star,4NNS than/IN factory/NN owners/NNS ./.NP Bracketed TextNot \[this year\].
INational Association \] of I Manufacturers Isettled on Ithe Hoosier capitall of \[Indianapolisl for l its nextmeetingl.
And Ithe cityl decided to treat \[its guestsl morelike \[royaltyl or/rock starsl than \[factory ownerq.Figure 2: The Treebank Approach to Base NP Identification3.
If there are multiple rules that match beginningat ti, use the longest matching rule R. Add thenew base noun phrase b(i,i+\]R\[-1) to the set ofbase NPs.
Continue matching at ti+lR\[.With the rules stored in an appropriate data struc-ture, this greedy "parsing" of base NPs is very fast.In our implementation, for example, we store therules in a decision tree, which permits base NP iden-tification in time linear in the length of the taggedinput text when using the longest match heuristic.Unfortunately, there is an obvious problem withthe algorithm described above.
There will be manyunhelpful rules in the rule set extracted from thetraining corpus.
These "bad" rules arise from foursources: bracketing errors in the corpus; tagging er-rors; unusual or irregular linguistic constructs (suchas parenthetical expressions); and inherent ambigu-ities in the base NPs - -  in spite of their simplicity.For example, the rule (VBG NNS), which was ex-tracted from manufacturing/VBG titans/NNS in theexample text, is ambiguous, and will cause erroneousbracketing in sentences such as The execs queezedin a few meetings before \[boarding/VBG buses/NNS~again.
In order to have a viable mechanism for iden-tifying base NPs using this algorithm, the grammarmust be improved by removing problematic rules.The next section presents two such methods for au-tomatically pruning the base NP grammar.3 P run ing  the  Base  NP  GrammarAs described above, our goal is to use the base NPcorpus to extract and select a set of noun phraserules that can be used to accurately identify baseNPs in novel text.
Our general pruning procedure isshown in Figure 3.
First, we divide the base NP cor-pus into two parts: a training corpus and a pruningcorpus.
The initial base NP grammar is extractedfrom the training corpus as described in Section 2.Next, the pruning corpus is used to evaluate the setof rules and produce a ranking of the rules in termsof their utility in identifying base NPs.
More specif-ically, we use the rule set and the longest matchheuristic to find all base NPs in the pruning corpus.Performance of the rule set is measured in terms oflabeled precision (P):p _- # of correct proposed NPs# of proposed NPsWe then assign to each rule a score that denotesthe "net benefit" achieved by using the rule duringNP parsing of the improvement corpus.
The ben-efit of rule r is given by B~ = C, - E,  where C~220TrainingCorpusPruningCorpusImprovedRule SetFinal Rule SetFigure 3: Pruning the Base NP Grammaris the number of NPs correctly identified by r, andE~ is the number of precision errors for which r isresponsible.
1 A rule is considered responsible for anerror if it was the first rule to bracket part of a refer-ence NP, i.e., an NP in the base NP training corpus.Thus, rules that form erroneous bracketings are notpenalized if another ule previously bracketed partof the same reference NP.For example, suppose the fragment containingbase NPs Boca Raton, Hot Springs, and Palm Beachis bracketed as shown below.resort towns like\[NP1 Boca/NNP Raton/NNP,  Hot/NNP\]\[NP2 Springs/NNP\], and\[NP3 Palm/NNP Beach/NNP\]Rule (NNP NNP , NNP) brackets NP1; (NNP /brackets NP2; and (NNP NNP / brackets NP~.
Rule(NNP NNP , NNP / incorrectly identifies Boca Ra-ton, Hot as a noun phrase, so its score is -1.
Rule(NNP) incorrectly identifies Springs, but it is notheld responsible for the error because of the previ-ous error by (NNP NNP,  NNP / on the same originalNP Hot Springs: so its score is 0.
Finally, rule (NNPNNP) receives a score of 1 for correctly identifyingPalm Beach as a base NP.The benefit scores from evaluation on the pruningcorpus are used to rank the rules in the grammar.With such a ranking, we can improve the rule setby discarding the worst rules.
Thus far, we haveinvestigated two iterative approaches for discardingrules, a thresholding approach and an incrementalapproach.
We describe ach, in turn, in the subsec-tions below.1 This same benefit measure is also used in the R&M study,but it is used to rank transformations rather than to rank NPrules.3.1 Thresho ld  P run ingGiven a ranking on the rule set, the threshold algo-rithm simply discards rules whose score is less thana predefined threshold R. For all of our experiments,we set R = 1 to select rules that propose more cor-rect bracketings than incorrect.
The process of eval-uating, ranking, and discarding rules is repeated un-til no rules have a score less than R. For our evalua-tion on the WSJ corpus, this typically requires onlyfour to five iterations.3.2  Inc rementa l  P run ingThresholding provides a very coarse mechanism forpruning the NP grammar.
In particular, becauseof interactions between the rules during bracketing,thresholding discards rules whose score might in-crease in the absence of other rules that are also be-ing discarded.
Consider, for example, the Boca Ra-ton fragments given earlier.
In the absence of (NNPNNP , NNP), the rule (NNP NNP / would have re-ceived a score of three for correctly identifying allthree NPs.As a result, we explored a more fine-grainedmethod of discarding rules: Each iteration of incre-mental pruning discards the N worst rules, ratherthan all rules whose rank is less than some thresh-old.
In all of our experiments, we set N = 10.
Aswith thresholding, the process of evaluating, rank-ing, and discarding rules is repeated, this time untilprecision of the current rule set on the pruning cor-pus begins to drop.
The rule set that maximizedprecision becomes the final rule set.3.3  Human Rev iewIn the experiments below, we compare the thresh-olding and incremental methods for pruning the NPgrammar to a rule set that was pruned by hand.When the training corpus is large, exhaustive re-view of the extracted rules is not practical.
Thisis the case for our initial rule set, culled from theWSJ corpus, which contains approximately 4500base NP rules.
Rather than identifying and dis-carding individual problematic rules, our revieweridentified problematic lasses of rules that could beremoved from the grammar automatically.
In partic-ular, the goal of the human reviewer was to discardrules that introduced ambiguity or corresponded tooverly complex base NPs.
Within our partial parsingframework, these NPs are better identified by moreinformed components of the NLP system.
Our re-viewer identified the following classes of rules as pos-sibly troublesome: rules that contain a preposition,period, or colon; rules that contain WH tags; rulesthat begin/end with a verb or adverb; rules that con-tain pronouns with any other tags; rules that containmisplaced commas or quotes; rules that end withadjectives.
Rules covered under any of these classes221were omitted from the human-pruned rule sets usedin the experiments of Section 4.4 Eva luat ionTo evaluate the treebank approach to base NP iden-tification, we created two base NP corpora.
Eachis derived from the Penn Treebank WSJ.
The firstcorpus attempts to duplicate the base NPs used theRamshaw & Marcus (R&M) study.
The second cor-pus contains slightly less complicated base NPs - -base NPs that are better suited for use with oursentence analyzer, Empire.
2 By evaluating on bothcorpora, we can measure the effect of noun phrasecomplexity on the treebank approach to base NPidentification.
In particular, we hypothesize that thetreebank approach will be most appropriate whenthe base NPs are sufficiently simple.For all experiments, we derived the training, prun-ing, and testing sets from the 25 sections of WallStreet Journal distributed with the Penn TreebankII.
All experiments employ 5-fold cross validation.More specifically, in each of five runs, a different foldis used for testing the final, pruned rule set; three ofthe remaining folds comprise the training corpus (tocreate the initial rule set); and the final partition isthe pruning corpus (to prune bad rules from the ini-tial rule set).
All results are averages across the fivefolds.
Performance is measured in terms of precisionand recall.
Precision was described earlier - -  it is astandard measure of accuracy.
Recall, on the otherhand, is an attempt o measure coverage:# of correct proposed NPs P =# of proposed NPs# of correct proposed NPsR =# of NPs in the annotated textTable 1 summarizes the performance of the tree-bank approach to base NP identification on theR&M and Empire corpora using the initial andpruned rule sets.
The first column of results showsthe performance of the initial, unpruned base NPgrammar.
The next two columns show the perfor-mance of the automatically pruned rule sets.
Thefinal column indicates the performance of rule setsthat had been pruned using the handcrafted pruningheuristics.
As expected, the initial rule set performsquite poorly.
Both automated approaches providesignificant increases in both recall and precision.
Inaddition, they outperform the rule set pruned usinghandcrafted pruning heuristics.2Very briefly, the Empire sentence analyzer relies on par-tial parsing to find simple const i tuents like base NPs andverb groups.
Machine learning algorithms then operate onthe output  of the partial parser to perform all at tachment  de-cisions.
The ult imate output  of the parser is a semantic aseframe representation of the functional structure of the inputsentence.R&M (1998) \]" R&M (1998)with \[ withoutlexical templates lexical templates93.1P/93.5R ~ 90.5P/90.7RTreebank \]Approach89.4p/9o.9a \]Table 2: Comparison of Treebank Approach withRamshaw & Marcus (1998) both With and WithoutLexical Templates, on the R&M CorpusThroughout he table, we see the effects of baseNP complexity - -  the base NPs of the R&M cor-pus are substantially more difficult for our approachto identify than the simpler NPs of the Empire cor-pus.
For the R&M corpus, we lag the best pub-lished results (93.1P/93.5R) by approximately 3%.This straightforward comparison, however, is not en-tirely appropriate.
Ramshaw & Marcus allow theirlearning algorithm to access word-level informationin addition to part-of-speech tags.
The treebank ap-proach, on the other hand, makes use only of part-of-speech tags.
Table 2 compares Ramshaw & Marcus'(In press) results with and without lexical knowl-edge.
The first column reports their performancewhen using lexical templates; the second when lexi-cal templates are not used; the third again shows thetreebank approach using incremental pruning.
Thetreebank approach and the R&M approach withoutlecial templates are shown to perform comparably(-1.1P/+0.2R).
Lexicalization of our base NP finderwill be addressed in Section 4.1.Finally, note the relatively small difference be-tween the threshold and incremental pruning meth-ods in Table 1.
For some applications, this minordrop in performance may be worth the decrease intraining time.
Another effective technique to speedup training is motivated by Charniak's (1996) ob-servation that the benefit of using rules that onlyoccurred once in training is marginal.
By discard-ing these rules before pruning, we reduce the size ofthe initial grammar - -  and the time for incrementalpruning - -  by 60%, with a performance drop of only-0.3P/-0.1R.4.1 Er rors  and Local  Repa i r  Heur is t icsIt is informative to consider the kinds of errorsmade by the treebank approach to bracketing.
Inparticular, the errors may indicate options for incor-porating lexical information into the base NP finder.Given the increases in performance achieved byRamshaw & Marcus by including word-level cues, wewould hope to see similar improvements by exploit-ing lexical information in the treebank approach.For each corpus we examined the first 100 or soerrors and found that certain linguistic constructsconsistently cause trouble.
(In the examples thatfollow, the bracketing shown is the error.
)222Base NP I Initial I Threshold Incremental I HumanCorpus Rule Set Pruning Pruning ReviewEmpire I 23.OP/46.5RI 91.2P/93.1R 92.TP/93.7RI 90.3P/9O.5RR&M 19.0P/36.1R 87.2P/90.0R 89.4P/90.9R 81.6P/g5.0RTable h Evaluation of the Treebank Approach Using the Mitre Part-of-Speech Tagger (P = precision; R =recall)BaseNP I Threshold I Threshold I Incremental I Incremental ICorpus Improvement T Local Repair Improvement + Local RepairEmpire \[ 91.2P/93.1R 92.8P/93.7R 92.7P/93.7R 93.7P/94.0R87.2P/90.0R I 89.2P/gO.6R I 89"4P/90"gR I 90.7P/91.IR I R&M ITable 3: Effect of Local Repair Heuristics* Conjunctions.
Conjunctions were a major prob-lem in the R&M corpus.
For the Empirecorpus, conjunctions of adjectives proved dif-ficult: \[record/N2~ \[third-quarter/JJ and/CCnine-month/JJ results/NN5~.?
Gerunds.
Even though the most difficult VBGconstructions such as manufacturing titans wereremoved from the Empire corpus, there wereothers that the bracketer did not handle, like\[chiej~ operating \[officer\].
Like conjunctions,gerunds posed a major difficulty in the R&Mcorpus.?
NPs Containing Punctuation.
Predictably, thebracketer has difficulty with NPs containing pe-riods, quotation marks, hyphens, and parenthe-ses.?
Adverbial Noun Phrases.
Especially temporalNPs such as last month in at \[83.6~\] of\[capacitylast month\].?
Appositives.
These are juxtaposed NPs such asof \[colleague Michael Madden\] that the brack-eter mistakes for a single NP.?
Quantified NPs.
NPs that look like PPs area problem: at/IN \[least/JJS~ \[the/DT right/JJjobs/NNS~; about/IN \[25/CD million/CD\].Many errors appear to stem from four underly-ing causes.
First, close to 20% can be attributedto errors in the Treebank and in the Base NP cor-pus, bringing the effective performance of the algo-rithm to 94.2P/95.9R and 91.5P/92.TR for the Em-pire and R&M corpora, respectively.
For example,neither corpus includes WH-phrases as base NPs.When the bracketer correctly recognizes these NPs,they are counted as errors.
Part-of-speech taggingerrors are a second cause.
Third, many NPs aremissed by the bracketer because it lacks the appro-priate rule.
For example, household products busi-ness is bracketed as \[household/NN products/NNS~\[business/Nh~.
Fourth, idiomatic and specialized ex-pressions, especially time, date, money, and numericphrases, also account for a substantial portion of theerrors.These last two categories of errors can often be de-tected because they produce ither ecognizable pat-terns or unlikely linguistic constructs.
ConsecutiveNPs, for example, usually denote bracketing errors,as in \[household/NN products/NNS~ \[business/Nh~.Merging consecutive NPs in the correct contextswould fix many such errors.
Idiomatic and special-ized expressions might be corrected by similarly localrepair heuristics.
Typical examples might includechanging \[effective/JJ Monday/NNP\] to effective\[Monday\]; changing \[the/DT balance/NN due/J \] to\[the balance\] due; and changing were/VBP \[n't/RBthe/DT only/RS losers/NNS~ to were n't \[the onlylosers\].Given these observations, we implemented threelocal repair heuristics.
The first merges consecutiveNPs unless either might be a time expression.
Thesecond identifies two simple date expressions.
Thethird looks for quantifiers preceding of NP.
The firstheuristic, for example, merges \[household products\]\[business\] to form \[household products business\], butleaves increased \[15 ~ \[last Friday\] untouched.
Thesecond heuristic merges \[June b~ , \[1995\] into \[June5, 1995\]; and \[June\], \[1995\] into \[June, 1995\].
Thethird finds examples like some of\[the companies\] andproduces \[some\] of\[the companies\].
These heuristicsrepresent an initial exploration into the effectivenessof employing lexical information in a post-processingphase rather than during grammar induction andbracketing.
While we are investigating the latterin current work, local repair heuristics have the ad-vantage of keeping the training and bracketing algo-rithms both simple and fast.The effect of these heuristics on recall and preci-sion is shown in Table 3.
We see consistent improve-ments for both corpora and both pruning methods,223achieving approximately 94P/R for the Empire cor-pus and approximately 91P/R for the R&M corpus.Note that these are the final results reported in theintroduction and conclusion.
Although these xperi-ments represent only an initial investigation i to theusefulness of local repair heuristics, we are very en-couraged by the results.
The heuristics uniformlyboost precision without harming recall; they helpthe R&M corpus even though they were designed inresponse to errors in the Empire corpus.
In addi-tion, these three heuristics alone recover 1/2 to 1/3of the improvements we can expect o obtain fromlexicalization based on the R&M results.5 ConclusionsThis paper presented a new method for identifyingbase NPs.
Our treebank approach uses the simpletechnique of matching part-of-speech tag sequences,with the intention of capturing the simplicity of thecorresponding syntactic structure.
It employs twoexisting corpus-based techniques: the initial nounphrase grammar is extracted irectly from an an-notated corpus; and a benefit score calculated fromerrors on an improvement corpus selects the bestsubset of rules via a coarse- or fine-grained pruningalgorithm.The overall results are surprisingly good, espe-cially considering the simplicity of the method.
Itachieves 94% precision and recall on simple baseNPs.
It achieves 91% precision and recall on themore complex NPs of the Ramshaw & Marcus cor-pus.
We believe, however, that the base NP findercan be improved further.
First, the longest-matchheuristic of the noun phrase bracketer could be re-placed by more sophisticated parsing methods thataccount for lexical preferences.
Rule application, forexample, could be disambiguated statistically usingdistributions induced during training.
We are cur-rently investigating such extensions.
One approachclosely related to ours - -  weighted finite-state trans-ducers (e.g.
(Pereira nd Riley, 1997)) - -  might pro-vide a principled way to do this.
We could thenconsider applying our error-driven pruning strategyto rules encoded as transducers.
Second, we haveonly recently begun to explore the use of local re-pair heuristics.
While initial results are promising,the full impact of such heuristics on overall perfor-mance can be determined only if they are system-atically learned and tested using available trainingdata.
Future work will concentrate on the corpus-based acquisition of local repair heuristics.In conclusion, the treebank approach to base NPsprovides an accurate and fast bracketing method,running in time linear in the length of the taggedtext..
The approach is simple to understand, im-plement, and train.
The learned grammar is easilymodified for use with new corpora, as rules can beadded or deleted with minimal interaction problems.Finally, the approach provides a general frameworkfor developing other treebank grammars (e.g., forsubject/verb/object identification) in addition tothese for base NPs.Acknowledgments.
This work was supported inpart by NSF (\]rants IRI-9624639 and GER-9454149.We thank Mitre for providing their part-of-speech tag-ger.Re ferencesD.
Bourigault.
1992.
Surface Grammatical Anal-ysis for the Extraction of Terminological NounPhrases.
In Proceedings, COLING-92, pages 977-981.Eric Brill.
1995.
Transformation-Based Error-Driven Learning and Natural Language Process-ing: A Case Study in Part-of-Speech Tagging.Computational Linguistics, 21(4):543-565.E.
Charniak.
1996.
Treebank Grammars.
In Pro-ceedings of the Thirteenth National Conference onArtificial Intelligence, pages 1031-1036, Portland,OR.
AAAI Press / MIT Press.K.
Church.
1988.
A Stochastic Parts Program andNoun Phrase Parser for Unrestricted Text.
In Pro-ceedings of the Second Conference on Applied Nat-ural Language Processing, pages 136-143.
Associ-ation for Computational Linguistics.J.
P. Gee and F. Grosjean.
1983.
Performance struc-tures: A psycholinguistic and linguistic appraisal.Cognitive Psychology, 15:411-458.John S. Justeson and Slava M. Katz.
1995.
Techni-cal Terminology: Some Linguistic Properties andan Algorithm for Identification i  Text.
NaturalLanguage Engineering, 1:9-27.M.
Marcus, M. Marcinkiewicz, and B. Santorini.1993.
Building a Large Annotated Corpus of En-glish: The Penn Treebank.
Computational Lin-guistics, 19(2):313-330.Fernando C. N. Pereira and Michael D. Riley.
1997.Speech Recognition by Composition of WeightedFinite Automata.
In Emmanuel Roche and YvesSchabes, editors, Finite-State Language Process-ing.
MIT Press.Lance A. Ramshaw and Mitchell P. Marcus.
Inpress.
Text chunking using transformation-basedlearning.
In Natural Language Processing UsingVery Large Corpora.
Kluwer.
Originally appearedin WVLC95, 82-94.A.
Voutilainen.
1993.
NPTool, A Detector of En-glish Noun Phrases.
In Proceedings of the Work-shop on Very Large Corpora, pages 48-57.
Asso-ciation for Computational Linguistics.224
