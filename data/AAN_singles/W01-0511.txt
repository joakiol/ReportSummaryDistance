Classifying the Semantic Relations in Noun Compounds via aDomain-Specific Lexical HierarchyBarbara Rosario and Marti HearstSchool of Information Management & SystemsUniversity of California, BerkeleyBerkeley, CA 94720-4600{rosario,hearst}@sims.berkeley.eduAbstractWe are developing corpus-based techniques for iden-tifying semantic relations at an intermediate level ofdescription (more specific than those used in caseframes, but more general than those used in tra-ditional knowledge representation systems).
In thispaper we describe a classification algorithm for iden-tifying relationships between two-word noun com-pounds.
We find that a very simple approach usinga machine learning algorithm and a domain-specificlexical hierarchy successfully generalizes from train-ing instances, performing better on previously un-seen words than a baseline consisting of training onthe words themselves.1 IntroductionWe are exploring empirical methods of determin-ing semantic relationships between constituents innatural language.
Our current project focuses onbiomedical text, both because it poses interestingchallenges, and because it should be possible to makeinferences about propositions that hold between sci-entific concepts within biomedical texts (Swansonand Smalheiser, 1994).One of the important challenges of biomedicaltext, along with most other technical text, is theproliferation of noun compounds.
A typical articletitle is shown below; it consists a cascade of fournoun phrases linked by prepositions:Open-labeled long-term study of the effi-cacy, safety, and tolerability of subcuta-neous sumatriptan in acute migraine treat-ment.The real concern in analyzing such a title is in de-termining the relationships that hold between differ-ent concepts, rather than on finding the appropriateattachments (which is especially difficult given thelack of a verb).
And before we tackle the prepo-sitional phrase attachment problem, we must finda way to analyze the meanings of the noun com-pounds.Our goal is to extract propositional informationfrom text, and as a step towards this goal, we clas-sify constituents according to which semantic rela-tionships hold between them.
For example, we wantto characterize the treatment-for-disease relation-ship between the words of migraine treatment ver-sus the method-of-treatment relationship betweenthe words of aerosol treatment.
These relations areintended to be combined to produce larger proposi-tions that can then be used in a variety of interpreta-tion paradigms, such as abductive reasoning (Hobbset al, 1993) or inductive logic programming (Ng andZelle, 1997).Note that because we are concerned with the se-mantic relations that hold between the concepts, asopposed to the more standard, syntax-driven com-putational goal of determining left versus right as-sociation, this has the fortuitous effect of changingthe problem into one of classification, amenable tostandard machine learning classification techniques.We have found that we can use such algorithms toclassify relationships between two-word noun com-pounds with a surprising degree of accuracy.
Aone-out-of-eighteen classification using a neural netachieves accuracies as high as 62%.
By taking ad-vantage of lexical ontologies, we achieve strong re-sults on noun compounds for which neither word ispresent in the training set.
Thus, we think this is apromising approach for a variety of semantic label-ing tasks.The reminder of this paper is organized as follows:Section 2 describes related work, Section 3 describesthe semantic relations and how they were chosen,and Section 4 describes the data collection and on-tologies.
In Section 5 we describe the method forautomatically assigning semantic relations to nouncompounds, and report the results of experimentsusing this method.
Section 6 concludes the paperand discusses future work.2 Related WorkSeveral approaches have been proposed for empiri-cal noun compound interpretation.
Lauer and Dras(1994) point out that there are three components tothe problem: identification of the compound fromwithin the text, syntactic analysis of the compound(left versus right association), and the interpreta-tion of the underlying semantics.
Several researchershave tackled the syntactic analysis (Lauer, 1995;Pustejovsky et al, 1993; Liberman and Sproat,1992), usually using a variation of the idea of find-ing the subconstituents elsewhere in the corpus andusing those to predict how the larger compounds arestructured.We are interested in the third task, interpretationof the underlying semantics.
Most related work re-lies on hand-written rules of one kind or another.Finin (1980) examines the problem of noun com-pound interpretation in detail, and constructs acomplex set of rules.
Vanderwende (1994) uses a so-phisticated system to extract semantic informationautomatically from an on-line dictionary, and thenmanipulates a set of hand-written rules with hand-assigned weights to create an interpretation.
Rind-flesch et al (2000) use hand-coded rule based sys-tems to extract the factual assertions from biomed-ical text.
Lapata (2000) classifies nominalizationsaccording to whether the modifier is the subject orthe object of the underlying verb expressed by thehead noun.1In the related sub-area of information extraction(Cardie, 1997; Riloff, 1996), the main goal is to findevery instance of particular entities or events of in-terest.
These systems use empirical techniques tolearn which terms signal entities of interest, in orderto fill in pre-defined templates.
Our goals are moregeneral than those of information extraction, andso this work should be helpful for that task.
How-ever, our approach will not solve issues surroundingpreviously unseen proper nouns, which are often im-portant for information extraction tasks.There have been several efforts to incorporate lex-ical hierarchies into statistical processing, primar-ily for the problem of prepositional phrase (PP)attachment.
The current standard formulation is:given a verb followed by a noun and a prepositionalphrase, represented by the tuple v, n1, p, n2, deter-mine which of v or n1 the PP consisting of p andn2 attaches to, or is most closely associated with.Because the data is sparse, empirical methods thattrain on word occurrences alone (Hindle and Rooth,1993) have been supplanted by algorithms that gen-eralize one or both of the nouns according to class-membership measures (Resnik, 1993; Resnik andHearst, 1993; Brill and Resnik, 1994; Li and Abe,1998), but the statistics are computed for the par-ticular preposition and verb.It is not clear how to use the results of such anal-ysis after they are found; the semantics of the rela-1Nominalizations are compounds whose head noun is anominalized verb and whose modifier is either the subject orthe object of the verb.
We do not distinguish the NCs on thebasis of their formation.tionship between the terms must still be determined.In our framework we would cast this problem asfinding the relationship R(p, n2) that best character-izes the preposition and the NP that follows it, andthen seeing if the categorization algorithm deter-mines their exists any relationship R?
(n1, R(p, n2))or R?
(v,R(p, n2)).The algorithms used in the related work reflect thefact that they condition probabilities on a particularverb and noun.
Resnik (1993; 1995) use classes inWordnet (Fellbaum, 1998) and a measure of concep-tual association to generalize over the nouns.
Brilland Resnik (1994) use Brill?s transformation-basedalgorithm along with simple counts within a lexi-cal hierarchy in order to generalize over individualwords.
Li and Abe (1998) use a minimum descrip-tion length-based algorithm to find an optimal treecut over WordNet for each classification problem,finding improvements over both lexical association(Hindle and Rooth, 1993) and conceptual associa-tion, and equaling the transformation-based results.Our approach differs from these in that we are us-ing machine learning techniques to determine whichlevel of the lexical hierarchy is appropriate for gen-eralizing across nouns.3 Noun Compound RelationsIn this work we aim for a representation that is in-termediate in generality between standard case roles(such as Agent, Patient, Topic, Instrument), and thespecificity required for information extraction.
Wehave created a set of relations that are sufficientlygeneral to cover a significant number of noun com-pounds, but that can be domain specific enough tobe useful in analysis.
We want to support relation-ships between entities that are shown to be impor-tant in cognitive linguistics, in particular we intendto support the kinds of inferences that arise fromTalmy?s force dynamics (Talmy, 1985).
It has beenshown that relations of this kind can be combined inorder to determine the ?directionality?
of a sentence(e.g., whether or not a politician is in favor of, or op-posed to, a proposal) (Hearst, 1990).
In the medicaldomain this translates to, for example, mapping asentence into a representation showing that a chem-ical removes an entity that is blocking the passageof a fluid through a channel.The problem remains of determining what the ap-propriate kinds of relations are.
In theoretical lin-guistics, there are contradictory views regarding thesemantic properties of noun compounds (NCs).
Levi(1978) argues that there exists a small set of se-mantic relationships that NCs may imply.
Downing(1977) argues that the semantics of NCs cannot beexhausted by any finite listing of relationships.
Be-tween these two extremes lies Warren?s (1978) tax-onomy of six major semantic relations organized intoa hierarchical structure.We have identified the 38 relations shown in Ta-ble 1.
We tried to produce relations that correspondto the linguistic theories such as those of Levi andWarren, but in many cases these are inappropriate.Levi?s classes are too general for our purposes; forexample, she collapses the ?location?
and ?time?relationships into one single class ?In?
and there-fore field mouse and autumnal rain belong to thesame class.
Warren?s classification schema is muchmore detailed, and there is some overlap betweenthe top levels of Warren?s hierarchy and our setof relations.
For example, our ?Cause (2-1)?
forflu virus corresponds to her ?Causer-Result?
of hayfever, and our ?Person Afflicted?
(migraine patient)can be thought as Warren?s ?Belonging-Possessor?of gunman.
Warren differentiates some classes alsoon the basis of the semantics of the constituents,so that, for example, the ?Time?
relationship is di-vided up into ?Time-Animate Entity?
of weekendguests and ?Time-Inanimate Entity?
of Sunday pa-per.
Our classification is based on the kind of re-lationships that hold between the constituent nounsrather than on the semantics of the head nouns.For the automatic classification task, we used onlythe 18 relations (indicated in bold in Table 1) forwhich an adequate number of examples were foundin the current collection.
Many NCs were ambigu-ous, in that they could be described by more thanone semantic relationship.
In these cases, we sim-ply multi-labeled them: for example, cell growth isboth ?Activity?
and ?Change?, tumor regression is?Ending/reduction?
and ?Change?
and bladder dys-function is ?Location?
and ?Defect?.
Our approachhandles this kind of multi-labeled classification.Two relation types are especially problematic.Some compounds are non-compositional or lexical-ized, such as vitamin k and e2 protein; others defyclassification because the nouns are subtypes of oneanother.
This group includes migraine headache,guinea pig, and hbv carrier.
We placed all these NCsin a catch-all category.
We also included a ?wrong?category containing word pairs that were incorrectlylabeled as NCs.2The relations were found by iterative refinementbased on looking at 2245 extracted compounds (de-scribed in the next section) and finding commonal-ities among them.
Labeling was done by the au-thors of this paper and a biology student; the NCswere classified out of context.
We expect to con-tinue development and refinement of these relation-ship types, based on what ends up clearly being use-2The percentage of the word pairs extracted that were nottrue NCs was about 6%; some examples are: treat migraine,ten patient, headache more.
We do not know, however, howmany NCs we missed.
The errors occurred when the wronglabel was assigned by the tagger (see Section 4).ful ?downstream?
in the analysis.The end goal is to combine these relationships inNCs with more that two constituent nouns, like inthe example intranasal migraine treatment of Sec-tion 1.4 Collection and Lexical ResourcesTo create a collection of noun compounds, we per-formed searches from MedLine, which contains ref-erences and abstracts from 4300 biomedical journals.We used several query terms, intended to span acrossdifferent subfields.
We retained only the titles andthe abstracts of the retrieved documents.
On thesetitles and abstracts we ran a part-of-speech tagger(Cutting et al, 1991) and a program that extractsonly sequences of units tagged as nouns.
We ex-tracted NCs with up to 6 constituents, but for thispaper we consider only NCs with 2 constituents.The Unified Medical Language System (UMLS)is a biomedical lexical resource produced andmaintained by the National Library of Medicine(Humphreys et al, 1998).
We use the MetaThe-saurus component to map lexical items into uniqueconcept IDs (CUIs).3 The UMLS also has a map-ping from these CUIs into the MeSH lexical hier-archy (Lowe and Barnett, 1994); we mapped theCUIs into MeSH terms.
There are about 19,000unique main terms in MeSH, as well as additionalmodifiers.
There are 15 main subhierarchies (trees)in MeSH, each corresponding to a major branchof medical ontology.
For example, tree A cor-responds to Anatomy, tree B to Organisms, andso on.
The longer the name of the MeSH term,the longer the path from the root and the moreprecise the description.
For example migraine isC10.228.140.546.800.525, that is, C (a disease), C10(Nervous System Diseases), C10.228 (Central Ner-vous System Diseases) and so on.We use the MeSH hierarchy for generalizationacross classes of nouns; we use it instead of the otherresources in the UMLS primarily because of MeSH?shierarchical structure.
For these experiments, weconsidered only those noun compounds for whichboth nouns can be mapped into MeSH terms, re-sulting in a total of 2245 NCs.5 Method and ResultsBecause we have defined noun compound relationdetermination as a classification problem, we canmake use of standard classification algorithms.
Inparticular, we used neural networks to classify acrossall relations simultaneously.3In some cases a word maps to more than one CUI; for thework reported here we arbitrarily chose the first mapping inall cases.
In future work we will explore how to make use ofall of the mapped terms.Name N ExamplesWrong parse (1) 109 exhibit asthma, ten drugs, measure headacheSubtype (4) 393 headaches migraine, fungus candida, hbv carrier,giant cell, mexico city, t1 tumour, ht1 receptorActivity/Physical process (5) 59 bile delivery, virus reproduction, bile drainage,headache activity, bowel function, tb transmissionEnding/reduction 8 migraine relief, headache resolutionBeginning of activity 2 headache induction, headache onsetChange 26 papilloma growth, headache transformation,disease development, tissue reinforcementProduces (on a genetic level) (7) 47 polyomavirus genome, actin mrna, cmv dna, protein geneCause (1-2) (20) 116 asthma hospitalizations, aids death, automobile accidentheat shock, university fatigue, food infectionCause (2-1) 18 flu virus, diarrhoea virus, influenza infectionCharacteristic (8) 33 receptor hypersensitivity, cell immunity,drug toxicity, gene polymorphism, drug susceptibilityPhysical property 9 blood pressure, artery diameter, water solubilityDefect (27) 52 hormone deficiency, csf fistulas, gene mutationPhysical Make Up 6 blood plasma, bile vomitPerson afflicted (15) 55 aids patient, bmt children, headache group, polio survivorsDemographic attributes 19 childhood migraine, infant colic, women migraineurPerson/center who treats 20 headache specialist, headache center, diseases physicians,asthma nurse, children hospitalResearch on 11 asthma researchers, headache study, language researchAttribute of clinical study (18) 77 headache parameter, attack study, headache interview,biology analyses, biology laboratory, influenza epidemiologyProcedure (36) 60 tumor marker, genotype diagnosis, blood culture,brain biopsy, tissue pathologyFrequency/time of (2-1) (22) 25 headache interval, attack frequency,football season, headache phase, influenza seasonTime of (1-2) 4 morning headache, hour headache, weekend migraineMeasure of (23) 54 relief rate, asthma mortality, asthma morbidity,cell population, hospital survivalStandard 5 headache criteria, society standardInstrument (1-2) (33) 121 aciclovir therapy, chloroquine treatment,laser irradiation, aerosol treatmentInstrument (2-1) 8 vaccine antigen, biopsy needle, medicine ginsengInstrument (1) 16 heroin use, internet use, drug utilizationObject (35) 30 bowel transplantation, kidney transplant, drug deliveryMisuse 11 drug abuse, acetaminophen overdose, ergotamine abuserSubject 18 headache presentation, glucose metabolism, heat transferPurpose (14) 61 headache drugs, hiv medications, voice therapy,influenza treatment, polio vaccineTopic (40) 38 time visualization, headache questionnaire, tobacco history,vaccination registries, health education, pharmacy databaseLocation (21) 145 brain artery, tract calculi, liver cell, hospital bedsModal 14 emergency surgery, trauma methodMaterial (39) 28 formaldehyde vapor, aloe gel, gelatin powder, latex glove,Bind 4 receptor ligand, carbohydrate ligandActivator (1-2) 6 acetylcholine receptor, pain signalsActivator (2-1) 4 headache trigger, headache precipitantInhibitor 11 adrenoreceptor blockers, influenza preventionDefect in Location (21 27) 157 lung abscess, artery aneurysm, brain disorderTable 1: The semantic relations defined via iterative refinement over a set of noun compounds.
The relationsshown in boldface are those used in the experiments reported on here.
Relation ID numbers are shown inparentheses by the relation names.
The second column shows the number of labeled examples for each class;the last row shows a class consisting of compounds that exhibit more than one relation.
The notation (1-2)and (2-1) indicates the directionality of the relations.
For example, Cause (1-2) indicates that the first nouncauses the second, and Cause (2-1) indicates the converse.flu vaccinationModel 2 D 4 G 3Model 3 D 4 808 G 3 770Model 4 D 4 808 54 G 3 770Model 5 D 4 808 54 79 G 3 770 670Model 6 D 4 808 54 79 429 G 3 770 670 310Table 2: Different lengths of the MeSH descriptorsfor the different modelsModel Feature Vector2 423 3154 6875 9506 1111Lexical 1184Table 3: Length of the feature vectors for differentmodels.We ran the experiments creating models that useddifferent levels of the MeSH hierarchy.
For example,for the NC flu vaccination, flu maps to the MeSHterm D4.808.54.79.429.154.349 and vaccination toG3.770.670.310.890.
Flu vaccination for Model 4would be represented by a vector consisting of theconcatenation of the two descriptors showing onlythe first four levels: D4.808.54.79 G3.770.670.310(see Table 2).
When a word maps to a general MeSHterm (like treatment, Y11) zeros are appended to theend of the descriptor to stand in place of the missingvalues (so, for example, treatment in Model 3 is Y11 0, and in Model 4 is Y 11 0 0, etc.
).The numbers in the MeSH descriptors are cate-gorical values; we represented them with indicatorvariables.
That is, for each variable we calculatedthe number of possible categories c and then repre-sented an observation of the variable as a sequence ofc binary variables in which one binary variable wasone and the remaining c ?
1 binary variables werezero.We also used a representation in which the wordsthemselves were used as categorical input variables(we call this representation ?lexical?).
For this col-lection of NCs there were 1184 unique nouns andtherefore the feature vector for each noun had 1184components.
In Table 3 we report the length of thefeature vectors for one noun for each model.
The en-tire NC was described by concatenating the featurevectors for the two nouns in sequence.The NCs represented in this fashion were used asinput to a neural network.
We used a feed-forwardnetwork trained with conjugate gradient descent.Model Acc1 Acc2 Acc3Lexical: Log Reg 0.31 0.58 0.62Lexical: NN 0.62 0.73 0.782 0.52 0.65 0.723 0.58 0.70 0.764 0.60 0.70 0.765 0.60 0.72 0.786 0.61 0.71 0.76Table 4: Test accuracy for each model, where the modelnumber corresponds to the level of the MeSH hierarchyused for classification.
Lexical NN is Neural Network onLexical and Lexical: Log Reg is Logistic Regression onNN.
Acc1 refers to how often the correct relation is thetop-scoring relation, Acc2 refers to how often the correctrelation is one of the top two according to the neural net,and so on.
Guessing would yield a result of 0.077.The network had one hidden layer, in which a hy-perbolic tangent function was used, and an outputlayer representing the 18 relations.
A logistic sig-moid function was used in the output layer to mapthe outputs into the interval (0, 1).The number of units of the output layer was thenumber of relations (18) and therefore fixed.
Thenetwork was trained for several choices of numbers ofhidden units; we chose the best-performing networksbased on training set error for each of the models.We subsequently tested these networks on held-outtesting data.We compared the results with a baseline in whichlogistic regression was used on the lexical features.Given the indicator variable representation of thesefeatures, this logistic regression essentially forms atable of log-odds for each lexical item.
We also com-pared to a method in which the lexical indicator vari-ables were used as input to a neural network.
Thisapproach is of interest to see to what extent, if any,the MeSH-based features affect performance.
Notealso that this lexical neural-network approach is fea-sible in this setting because the number of uniquewords is limited (1184) ?
such an approach wouldnot scale to larger problems.In Table 4 and in Figure 1 we report the resultsfrom these experiments.
Neural network using lex-ical features only yields 62% accuracy on averageacross all 18 relations.
A neural net trained onModel 6 using the MeSH terms to represent thenouns yields an accuracy of 61% on average acrossall 18 relations.
Note that reasonable performance isalso obtained for Model 2, which is a much more gen-eral representation.
Table 4 shows that both meth-ods achieve up to 78% accuracy at including the cor-rect relation among the top three hypothesized.Multi-class classification is a difficult problem(Vapnik, 1998).
In this problem, a baseline in which2 3 4 5 600.10.20.30.40.50.60.70.80.91Testing set performance on the best models for each  MeSH levelLevels of the MeSH HierarchyAccuracyontestsetAccuracy for the largest NN outputwithin 2 largest NN outputwithin 3 largest NN outputFigure 1: Accuracies on the test sets for all the models.The dotted line at the bottom is the accuracy of guess-ing (the inverse of the number of classes).
The dash-dotline above this is the accuracy of logistic regression onthe lexical data.
The solid line with asterisks is the ac-curacy of our representation, when only the maximumoutput value from the network is considered.
The solidline with circles if the accuracy of getting the right an-swer within the two largest output values from the neuralnetwork and the last solid line with diamonds is the ac-curacy of getting the right answer within the first threeoutputs from the network.
The three flat dashed linesare the corresponding performances of the neural net-work on lexical inputs.the algorithm guesses yields about 5% accuracy.
Wesee that our method is a significant improvementover the tabular logistic-regression-based approach,which yields an accuracy of only 31 percent.
Addi-tionally, despite the significant reduction in raw in-formation content as compared to the lexical repre-sentation, the MeSH-based neural network performsas well as the lexical-based neural network.
(And weagain stress that the lexical-based neural network isnot a viable option for larger domains.
)Figure 2 shows the results for each relation.MeSH-based generalization does better on some re-lations (for example 14 and 15) and Lexical on others(7, 22).
It turns out that the test set for relation-ship 7 (?Produces on a genetic level?)
is dominatedby NCs containing the words alleles and mrna andthat all the NCs in the training set containing thesewords are assigned relation label 7.
A similar situa-tion is seen for relation 22, ?Time(2-1)?.
In the testset examples the second noun is either recurrence,season or time.
In the training set, these nouns ap-pear only in NCs that have been labeled as belongingto relation 22.On the other hand, if we look at relations 14 and15, we find a wider range of words, and in some cases1 4 5 7 8 14 15 18 20 21 22 23 27 33 35 36 39 40 202700.10.20.30.40.50.60.70.80.91Performance of each class  for the LEXICAL modelClassesAccuraciesonthetestsetfor thebest modelMeSHLexicalFigure 2: Accuracies for each class.
The numbers at thebottom refer to the class numbers in Table 1.
Note thevery high accuracy for the ?mixed?
relationship 20-27(last bar on the right).the words in the test set are not present in the train-ing set.
In relationship 14 (?Purpose?
), for example,vaccine appears 6 times in the test set (e.g., varicellavaccine).
In the training set, NCs with vaccine init have also been classified as ?Instrument?
(anti-gen vaccine, polysaccharide vaccine), as ?Object?
(vaccine development), as ?Subtype of?
(opv vac-cine) and as ?Wrong?
(vaccines using).
Other wordsin the test set for 14 are varicella which is presentin the trainig set only in varicella serology labeledas ?Attribute of clinical study?, drainage which isin the training set only as ?Location?
(gallbladderdrainage and tract drainage) and ?Activity?
(biledrainage).
Other test set words such as immunisa-tion and carcinogen do not appear in the trainingset at all.In other words, it seems that the MeSHk-basedcategorization does better when generalization is re-quired.
Additionally, this data set is ?dense?
in thesense that very few testing words are not present inthe training data.
This is of course an unrealisticsituation and we wanted to test the robustness ofthe method in a more realistic setting.
The resultsreported in Table 4 and in Figure 1 were obtainedsplitting the data into 50% training and 50% testingfor each relation and we had a total of 855 trainingpoints and 805 test points.
Of these, only 75 ex-amples in the testing set consisted of NCs in whichboth words were not present in the training set.We decided to test the robustness of the MeSH-based model versus the lexical model in the case ofunseen words; we are also interested in seeing therelative importance of the first versus the secondnoun.
Therefore, we split the data into 5% training(73 data points) and 95% testing (1587 data points)Model All test 1 2 3 4Lexical: NN 0.23 0.54 0.14 0.33 0.082 0.44 0.62 0.25 0.53 0.383 0.41 0.62 0.18 0.47 0.354 0.42 0.58 0.26 0.39 0.385 0.46 0.64 0.28 0.54 0.406 0.44 0.64 0.25 0.50 0.39Table 5: Test accuracy for the four sub-partitions ofthe test set.and partitioned the testing set into 4 subsets as fol-lows (the numbers in parentheses are the numbersof points for each case):?
Case 1: NCs for which the first noun was notpresent in the training set (424)?
Case 2: NCs for which the second noun was notpresent in the training set (252)?
Case 3: NCs for which both nouns were presentin the training set (101)?
Case 4: NCs for which both nouns were notpresent in the training set (810).Table 5 and Figures 3 and 4 present the accuraciesfor these test set partitions.
Figure 3 shows thatthe MeSH-based models are more robust than thelexical when the number of unseen words is high andwhen the size of training set is (very) small.
In thismore realistic situation, the MeSHmodels are able togeneralize over previously unseen words.
For unseenwords, lexical reduces to guessing.4Figure 4 shows the accuracy for the MeSH based-model for the the four cases of Table 5.
It is interest-ing to note that the accuracy for Case 1 (first nounnot present in the training set) is much higher thanthe accuracy for Case 2 (second noun not present inthe training set).
This seems to indicate that thesecond noun is more important for the classificationthat the first one.6 ConclusionsWe have presented a simple approach to corpus-based assignment of semantic relations for nouncompounds.
The main idea is to define a set of rela-tions that can hold between the terms and use stan-dard machine learning techniques and a lexical hi-erarchy to generalize from training instances to newexamples.
The initial results are quite promising.In this task of multi-class classification (with 18classes) we achieved an accuracy of about 60%.These results can be compared with Vanderwende4Note that for unseen words, the baseline lexical-basedlogistic regression approach, which essentially builds a tabularrepresentation of the log-odds for each class, also reduces torandom guessing.2 3 4 5 600.10.20.30.40.50.60.70.80.91Testing set  performances for different partitions on the test setLevels of the MeSH HierarchyAccuracyontestsetAccuracy for MeSH for the entire testAccuracy for MeSH for Case 4Accuracy for Lexical  for the entire testAccuracy for Lexical  for Case 4GuessingFigure 3: The unbroken lines represent the MeSH mod-els accuracies (for the entire test set and for case 4) andthe dashed lines represent the corresponding lexical ac-curacies.
The accuracies are smaller than the previouscase of Table 4 because the training set is much smaller,but the point of interest is the difference in the perfor-mance of MeSH vs. lexical in this more difficult setting.Note that lexical for case 4 reduces to random guessing.2 3 4 5 600.10.20.30.40.50.60.70.80.91Testing set  performances for different partitions on the test set for the MeSH?based modelLevels of the MeSH HierarchyAccuracyontestsetAccuracy for the entire testCase 3Case 1Case 2Case 4Figure 4: Accuracy for the MeSH based-model for thethe four cases.
All these curves refer to the case of get-ting exactly the right answer.
Note the difference inperformance between case 1 (first noun not present inthe training set) and case 2 (second noun not present intraining set).
(1994) who reports an accuracy of 52% with 13classes and Lapata (2000) whose algorithm achievesabout 80% accuracy for a much simpler binary clas-sification.We have shown that a class-based representationperformes as well as a lexical-based model despitethe reduction of raw information content and de-spite a somewhat errorful mapping from terms toconcepts.
We have also shown that representing thenouns of the compound by a very general represen-tation (Model 2) achieves a reasonable performanceof aout 52% accuracy on average.
This is particu-larly important in the case of larger collections witha much bigger number of unique words for whichthe lexical-based model is not a viable option.
Ourresults seem to indicate that we do not lose muchin terms of accuracy using the more compact MeSHrepresentation.We have also shown how MeSH-besed models outperform a lexical-based approach when the num-ber of training points is small and when the testset consists of words unseen in the training data.This indicates that the MeSH models can generalizesuccessfully over unseen words.
Our approach han-dles ?mixed-class?
relations naturally.
For the mixedclass Defect in Location, the algorithm achieved anaccuracy around 95% for both ?Defect?
and ?Loca-tion?
simultaneously.
Our results also indicate thatthe second noun (the head) is more important indetermining the relationships than the first one.In future we plan to train the algorithm to allowdifferent levels for each noun in the compound.
Wealso plan to compare the results to the tree cut algo-rithm reported in (Li and Abe, 1998), which allowsdifferent levels to be identified for different subtrees.We also plan to tackle the problem of noun com-pounds containing more than two terms.AcknowledgmentsWe would like to thank Nu Lai for help with theclassification of the noun compound relations.
Thiswork was supported in part by NSF award numberIIS-9817353.ReferencesEric Brill and Philip Resnik.
1994.
A rule-basedapproach to prepositional phrase attachment dis-amibuation.
In Proceedings of COLING-94.Claire Cardie.
1997.
Empirical methods in informa-tion extraction.
AI Magazine, 18(4).Douglass R. Cutting, Julian Kupiec, Jan O. Peder-sen, and Penelope Sibun.
1991.
A practical part-of-speech tagger.
In The 3rd Conference on Ap-plied Natural Language Processing, Trento, Italy.P.
Downing.
1977.
On the creation and use of en-glish compound nouns.
Language, (53):810?842.Christiane Fellbaum, editor.
1998.
WordNet: AnElectronic Lexical Database.
MIT Press.Timothy W. Finin.
1980.
The Semantic Interpreta-tion of Compound Nominals.
Ph.d. dissertation,University of Illinois, Urbana, Illinois.Marti A. Hearst.
1990.
A hybrid approach to re-stricted text interpretation.
In Paul S. Jacobs, ed-itor, Text-Based Intelligent Systems: Current Re-search in Text Analysis, Information Extraction,and Retrieval, pages 38?43.
GE Research & De-velopment Center, TR 90CRD198.Donald Hindle and Mats Rooth.
1993.
Structualambiguity and lexical relations.
ComputationalLinguistics, 19(1).Jerry R. Hobbs, Mark Stickel, Douglas Appelt, andPaul Martin.
1993.
Interpretation as abduction.Artificial Intelligence, 63(1-2).L.
Humphreys, D.A.B.
Lindberg, H.M. Schoolman,and G. O. Barnett.
1998.
The unified medicallanguage system: An informatics research collab-oration.
Journal of the American Medical Infor-matics Assocation, 5(1):1?13.Maria Lapata.
2000.
The automatic interpretationof nominalizations.
In AAAI Proceedings.Mark Lauer and Mark Dras.
1994.
A probabilisticmodel of compound nouns.
In Proceedings of the7th Australian Joint Conference on AI.Mark Lauer.
1995.
Corpus statistics meet the com-pound noun.
In Proceedings of the 33rd Meetingof the Association for Computational Linguistics,June.Judith Levi.
1978.
The Syntax and Semantics ofComplex Nominals.
Academic Press, New York.Hang Li and Naoki Abe.
1998.
Generalizing caseframes using a thesaurus and the MDI principle.Computational Linguistics, 24(2):217?244.Mark Liberman and Richard Sproat.
1992.
Thestress and structure of modified noun phrases inenglish.
In I.l Sag and A. Szabolsci, editors, Lex-ical Matters.
CSLI Lecture Notes No.
24, Univer-sity of Chicago Press.Henry J. Lowe and G. Octo Barnett.
1994.
Un-derstanding and using the medical subject head-ings (MeSH) vocabulary to perform literaturesearches.
Journal of the American Medical Asso-cation (JAMA), 271(4):1103?1108.Hwee Tou Ng and John Zelle.
1997.
Corpus-basedapproaches to semantic interpretation in naturallanguage processing.
AI Magazine, 18(4).James Pustejovsky, Sabine Bergler, and Peter An-ick.
1993.
Lexical semantic techniques for corpusanalysis.
Computational Linguistics, 19(2).Philip Resnik and Marti A. Hearst.
1993.
Structuralambiguity and conceptual relations.
In Proceed-ings of the ACL Workshop on Very Large Corpora,Columbus, OH.Philip Resnik.
1993.
Selection and Information:A Class-Based Approach to Lexical Relationships.Ph.D.
thesis, University of Pennsylvania, Decem-ber.
(Institute for Research in Cognitive Sciencereport IRCS-93-42).Philip Resnik.
1995.
Disambiguating noun group-ings with respect to WordNet senses.
In ThirdWorkshop on Very Large Corpora.
Association forComputational Linguistics.Ellen Riloff.
1996.
Automatically generating ex-traction patterns from untagged text.
In Pro-ceedings of the Thirteenth National Conference onArtificial Intelligence and the Eighth InnovativeApplications of Artificial Intelligence Conference,Menlo Park.
AAAI Press / MIT Press.Thomas Rindflesch, Lorraine Tanabe, John N. We-instein, and Lawrence Hunter.
2000.
Extractionof drugs, genes and relations from the biomedicalliterature.
Pacific Symposium on Biocomputing,5(5).Don R. Swanson and N. R. Smalheiser.
1994.
As-sessing a gap in the biomedical literature: Mag-nesium deficiency and neurologic disease.
Neuro-science Research Communications, 15:1?9.Len Talmy.
1985.
Force dynamics in language andthought.
In Parasession on Causatives and Agen-tivity, University of Chicago.
Chicago LinguisticSociety (21st Regional Meeting).Lucy Vanderwende.
1994.
Algorithm for automaticinterpretation of noun sequences.
In Proceedingsof COLING-94, pages 782?788.V.
Vapnik.
1998.
Statistical Learning Theory.
Ox-ford University Press.Beatrice Warren.
1978.
Semantic Patterns of Noun-Noun Compounds.
Acta Universitatis Gothobur-gensis.
