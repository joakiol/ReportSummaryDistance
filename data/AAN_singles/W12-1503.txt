INLG 2012 Proceedings of the 7th International Natural Language Generation Conference, pages 3?11,Utica, May 2012. c?2012 Association for Computational LinguisticsLearning Preferences for Referring Expression Generation:Effects of Domain, Language and AlgorithmRuud KoolenTilburg UniversityP.O.
Box 901355000 LE TilburgThe Netherlandsr.m.f.koolen@uvt.nlEmiel KrahmerTilburg UniversityP.O.
Box 901355000 LE TilburgThe Netherlandse.j.krahmer@uvt.nlMarie?t TheuneUniversity of TwenteP.O.
Box 2177500 AE EnschedeThe Netherlandsm.theune@utwente.nlAbstractOne important subtask of Referring Expres-sion Generation (REG) algorithms is to se-lect the attributes in a definite description fora given object.
In this paper, we study howmuch training data is required for algorithmsto do this properly.
We compare two REG al-gorithms in terms of their performance: theclassic Incremental Algorithm and the morerecent Graph algorithm.
Both rely on a notionof preferred attributes that can be learned fromhuman descriptions.
In our experiments, pref-erences are learned from training sets that varyin size, in two domains and languages.
Theresults show that depending on the algorithmand the complexity of the domain, training ona handful of descriptions can already lead to aperformance that is not significantly differentfrom training on a much larger data set.1 IntroductionMost practical NLG systems include a dedicatedmodule for Referring Expression Generation (REG)in one form or another (Mellish et al, 2006).
Onecentral problem a REG module needs to address isdeciding on the contents of a description.
Jordanand Walker (2005), for example, studied human-produced descriptions in a furniture scenario, andfound that speakers can refer to a target in many dif-ferent ways (?the yellow rug?, ?the $150 rug?, etc.
).The question, then, is how speakers decide which at-tributes to include in a description, and how this de-cision process can be modeled in a REG algorithm.When we focus on the generation of distinguish-ing descriptions (which is often done in REG), it isusually assumed that some attributes are more pre-ferred than others: when trying to identify a chair,for example, its colour is probably more helpful thanits size.
It is precisely this intuition of preferred at-tributes which is incorporated in the Incremental Al-gorithm (Dale and Reiter, 1995), arguably one of themost influential REG algorithms to date.
The Incre-mental Algorithm (IA) assumes the existence of acomplete, ordered list of preferred attributes.
Thealgorithm basically iterates through this list, addingan attribute (e.g., COLOUR) to the description underconstruction if its value (e.g., yellow) helps rulingout one or more of the remaining distractors.Even though the IA is exceptional in that it re-lies on a complete ordering of attributes, most cur-rent REG algorithms make use of preferences insome way (Fabbrizio et al, 2008; Gerva?s et al,2008; Kelleher, 2007; Spanger et al, 2008; Viethenand Dale, 2010).
The graph-based REG algorithm(Krahmer et al, 2003), for example, models prefer-ences in terms of costs, where cheaper is more pre-ferred.
Contrary to the IA, the graph-based algo-rithm assumes that preferences operate at the levelof attribute-value pairs (or properties) rather than atthe level of attributes; in this way it becomes pos-sible to prefer a straightforward size (large) over asubtle colour (mauve, taupe).
Moreover, the graph-based algorithm looks for the cheapest overall de-scription, and may opt for a description with a sin-gle, relatively dispreferred property (?the man withthe blue eyes?)
when the alternative would be tocombine many, relatively preferred properties (?thelarge, balding man with the bow tie and the stripedtuxedo?).
This flexibility is arguably one of the3reasons why the graph-based REG approach workswell: it was the best performing system in the mostrecent REG Challenge (Gatt et al, 2009).But where do the preferences used in the algo-rithms come from?
Dale and Reiter point out thatpreferences are domain dependent, and that deter-mining them for a given domain is essentially anempirical question.
Unfortunately, they do not spec-ify how this particular empirical question should beanswered.
The general preference for colour oversize is experimentally well-established (Pechmann,1989), but for most other cases experimental dataare not readily available.
An alternative would beto look at human data, preferably in a ?semanticallytransparent?
corpus (van Deemter et al, 2006), thatis: a corpus that contains the attributes and values ofall domain objects, together with the attribute-valuepairs actually included in a target reference.
Suchcorpora are typically collected using human partic-ipants, who are asked to produce referring expres-sions for targets in controlled visual scenes.
Oneexample is the TUNA corpus, which is a publiclyavailable data set containing 2280 human-produceddescriptions in total, and which formed the basis ofvarious REG Challenges.
Clearly, building a corpussuch as TUNA is a time consuming and labour in-tensive exercise, so it will not be surprising that onlya handful of such corpora exists (and often only forEnglish).This raises an important question: how manyhuman-produced references are needed to make agood estimate of which attributes and properties arepreferred?
Do we really need hundreds of instances,or is it conceivable that a few of them (collected in asemantically transparent way) will do?
This is not aneasy matter, since various factors might play a role:from which data set are example references sampled,what are the domains of interest, and, perhaps mostimportantly, which REG algorithm is considered?
Inthis paper, we address these questions by systemati-cally training two REG algorithms (the IncrementalAlgorithm and the graph-based REG algorithm) onsets of human-produced descriptions of increasingsize and evaluating them on a held-out test set; wedo this for two different domains (people and furni-ture descriptions) and two data sets in two differentlanguages (TUNA and D-TUNA, the Dutch versionof TUNA).That size of the training set may have an impacton the performance of a REG algorithm was alreadysuggested by Theune et al (2011), who used the En-glish TUNA corpus to determine preferences (costs)for the graph-based algorithm using a similar learn-ing curve set-up as we use here.
However, the cur-rent paper expands on Theune et al (2011) in threemajor ways.
Firstly, and most importantly, whereTheune et al reported results for only one algorithm(the graph-based one), we directly compare the per-formance of the graph-based algorithm and the In-cremental Algorithm (something which, somewhatsurprisingly, has not been done before).
Secondly,we test whether these algorithms perform differentlyin two different languages (English and Dutch), andthirdly, we use eight training set sizes, which is morethan the six set sizes that were used by Theune et alBelow we first explain in more detail which algo-rithms (Section 2) and corpora (Section 3) we usedfor our experiments.
Then we describe how we de-rived costs and orders from subsets of these corpora(Section 4), and report the results of our experimentsfocusing on effects of domain, language and sizeof the training set (Section 5).
We end with a dis-cussion and conclusion (Section 6), where we alsocompare the performance of the IA trained on smallset sizes with that of the classical Full Brevity andGreedy algorithms (Dale and Reiter, 1995).2 The AlgorithmsIn this section we briefly describe the two algo-rithms, and their settings, used in our experiment.For details about these algorithms we refer to theoriginal publications.The Incremental Algorithm (IA) The basicassumption underlying the Incremental Algorithm(Dale and Reiter, 1995) is that speakers ?prefer?certain attributes over others when referring toobjects.
This intuition is formalized in the notionof a list of attributes, ranked in order of preference.When generating a description for a target, the al-gorithm iterates through this list, adding an attributeto the description under construction if its valuehelps rule out any of the distractors not previouslyruled out.
There is no backtracking in the IA, whichmeans that a selected attribute is always realized in4the final description, even if the inclusion of laterattributes renders it redundant.
In this way, the IA iscapable of generating overspecified descriptions, inaccordance with the human tendency to mention re-dundant information (Pechmann, 1989; Engelhardtet al, 2006; Arts et al, 2011).
The TYPE attribute(typically realized as the head noun) has a specialstatus in the IA.
After running the algorithm it ischecked whether TYPE is in the description; if not,it is added, so that TYPE is always included even ifit does not rule out any distractors.To derive preference orders from human-produced descriptions we proceeded as follows:given a set of n descriptions sampled from alarger corpus (where n is the set size, a variablewe systematically control in our experiment), wecounted the number of times a certain attributeoccurred in the n descriptions.
The most frequentlyoccurring attribute was placed at the first position ofthe preferred attributes list, followed by the secondmost frequent attribute, etc.
In the case of a tie (i.e.,when two attributes occurred equally often, whichtypically is more likely to happen in small trainingsets), the attributes were ordered alphabetically.
Inthis way, we made sure that all ties were treated inthe same, comparable manner, which resulted in acomplete ranking of attributes, as required by the IA.The Graph-based Algorithm (Graph) In thegraph-based algorithm (Krahmer et al, 2003),which we refer to as Graph, information aboutdomain objects is represented as a labelled directedgraph, and REG is modeled as a graph-searchproblem.
The output of the algorithm is thecheapest distinguishing subgraph, given a particularcost function assigning costs to properties (i.e.,attribute-value pairs).
By assigning zero costs tosome properties Graph is also capable of generatingoverspecified descriptions, including redundantproperties.
To ensure that the graph search does notterminate before the free properties are added, thesearch order must be explicitly controlled (Viethenet al, 2008).
To ensure a fair comparison with theIA, we make sure that if the target?s TYPE propertywas not originally selected by the algorithm, it isadded afterwards.In this study, both the costs and orders requiredby Graph are derived from corpus data.
We basethe property order on the frequency with which eachattribute-value pair is mentioned in a training cor-pus, relative to the number of target objects withthis property.
The properties are then listed in or-der of decreasing frequency.
Costs can be derivedfrom the same corpus frequencies; here, followingTheune et al (2011), we adopt a systematic way ofderiving costs from frequencies based on k-meansclustering.
Theune and colleagues achieved the bestperformance with k = 2, meaning that the prop-erties are divided in two groups based on their fre-quency.
The properties in the group with the high-est frequency get cost 0.
These ?free?
properties arealways included in the description if they help dis-tinguish the target.
The properties in the less fre-quent group get cost 1; of these properties, the al-gorithm only adds the minimum number necessaryto achieve a distinguishing description.
Ties due toproperties occurring with the same frequency neednot be resolved when determining the cost function,since Graph does not assume the existence of a com-plete ordering.
Properties that did not occur in atraining corpus were automatically assigned cost 1.Like we did for the IA, we listed attribute-value pairswith the same frequency in alphabetical order.3 CorporaTraining and test data for our experiment weretaken from two corpora of referring expressions,one English (TUNA) and one Dutch (D-TUNA).TUNA The TUNA corpus (Gatt et al, 2007)is a semantically transparent corpus consisting ofobject descriptions in two domains (furniture andpeople).
The corpus was collected in an on-lineproduction experiment, in which participants werepresented with visual scenes containing one targetobject and six distractor objects.
These objects wereordered in a 5 ?
3 grid, and the participants wereasked to describe the target in such a way that itcould be uniquely distinguished from its distractors.Table 1 shows the attributes and values that wereannotated for the descriptions in the two domains.There were two experimental conditions: inthe +LOC condition, the participants were freeto describe the target using any of its properties,including its location on the screen (represented5FurnitureAttribute Possible valuesTYPE chair, desk, sofa, fanCOLOUR green, red, blue, grayORIENTATION front, back, left, rightSIZE large, smallX-DIMENSION 1, 2, 3, 4, 5Y-DIMENSION 1, 2, 3PeopleAttribute Possible valuesTYPE personAGE old, youngHAIRCOLOUR light, darkORIENTATION front, left, rightHASBEARD true, falseHASGLASSES true, falseHASSHIRT true, falseHASSUIT true, falseHASTIE true, falseX-DIMENSION 1, 2, 3, 4, 5Y-DIMENSION 1, 2, 3Table 1: Attributes and values in the furniture and peopledomains.
X- and Y-DIMENSION refer to an object?s hori-zontal and vertical position in a scene grid and only occurin the English TUNA corpus.in Table 1 as the X- and Y-DIMENSION), whereasin the -LOC condition they were discouraged (butnot prevented) from mentioning object locations.However, some descriptions in the -LOC conditioncontained location information anyway.D-TUNA For Dutch, we used the D-TUNAcorpus (Koolen and Krahmer, 2010).
This corpususes the same visual scenes and annotation schemeas the TUNA corpus, but consists of Dutch insteadof English target descriptions.
Since the D-TUNAexperiment was performed in laboratory conditions,its data is relatively ?cleaner?
than the TUNA data,which means that it contains fewer descriptions thatare not fully distinguishing and that its descriptionsdo not contain X- and Y-DIMENSION attributes.
Al-though the descriptions in D-TUNA were collectedin three different conditions (written, spoken, andface-to-face), we only use the written descriptionsin this paper, as this condition is most similar to thedata collection in TUNA.4 MethodTo find out how much training data is requiredto achieve an acceptable attribute selection perfor-mance for the IA and Graph, we derived orders andcosts from different sized training sets.
We thenevaluated the algorithms, using the derived ordersand costs, on a test set.
Training and test sets weretaken from TUNA and D-TUNA.As Dutch training data, we used 160 furniture and160 people items, randomly selected from the tex-tual descriptions in the D-TUNA corpus.
The re-maining furniture and people descriptions (40 itemseach) were used for testing.
As English trainingdata, we took all -LOC data from the training setof the REG Challenge 2009 (Gatt et al, 2009): 165furniture and 136 people descriptions.
As Englishtest data we used all -LOC data from the REG 2009development set: 38 furniture and 38 people descrip-tions.
We only used -LOC data to increase compa-rability to the Dutch data.From the Dutch and English furniture and peopletraining data, we selected random subsets of 1, 5,10, 20, 30, 40 and 50 descriptions.
Five differentsets of each size were created, since the accidentalcomposition of a training set could strongly influ-ence the results.
All training sets were built up in acumulative fashion, starting with five randomly se-lected sets of size 1, then adding 4 items to each ofthem to create five sets of size 5, and so on, for eachcombination of language and domain.
We used thesedifferent training sets to derive preference orders ofattributes for the IA, and costs and property ordersfor Graph, as outlined above.We evaluated the performance of the derived pref-erence orders and cost functions on the test data forthe corresponding domain and language, using thestandard Dice and Accuracy metrics for evaluation.Dice measures the overlap between attribute sets,producing a value between 1 and 0, where 1 standsfor a perfect match and 0 for no overlap at all.
Ac-curacy is the percentage of perfect matches betweenthe generated attribute sets and the human descrip-tions in the test set.
Both metrics were used in theREG Generation Challenges.6English furnitureIA GraphSet size Dice Acc.
(%) Dice Acc.
(%)1 0.764 36.8 0.693 24.75 0.829 55.3 0.756 33.710 0.829 55.3 0.777 39.520 0.829 55.3 0.788 40.530 0.829 55.3 0.782 40.540 0.829 55.3 0.793 45.350 0.829 55.3 0.797 45.8All 0.829 55.3 0.810 50.0Dutch furnitureIA GraphSet size Dice Acc.
(%) Dice Acc.
(%)1 0.925 63.0 0.876 44.55 0.935 67.5 0.917 62.010 0.929 68.5 0.923 66.020 0.930 65.5 0.923 64.030 0.931 67.0 0.924 65.540 0.931 67.0 0.931 67.550 0.929 66.0 0.929 67.0All 0.926 65.0 0.929 67.5Table 2: Performance for each set size in the furnituredomain.
For sizes 1 to 50, means over five sets are given.The full sets are 165 English and 160 Dutch descriptions.Note that the scores of the IA for the English sets of sizes1 to 30 were also reported in Theune et al (2011).5 Results5.1 Overall analysisTo determine the effect of domain and language onthe performance of REG algorithms, we applied re-peated measures analyses of variance (ANOVA) tothe Dice and Accuracy scores, using set size (1, 5,10, 20, 30, 40, 50, all) and domain (furniture, peo-ple) as within variables, and algorithm (IA, Graph)and language (English, Dutch) as between variables.The results show main effects of domain (Dice:F(1,152) = 56.10, p < .001; Acc.
: F(1,152) = 76.36,p < .001) and language (Dice: F(1,152) = 30.30,p < .001; Acc.
: F(1,152) = 3.380, p = .07).
Regard-ing the two domains, these results indicate that boththe IA and the Graph algorithm generally performedbetter in the furniture domain (Dice: M = .86, SD =.01; Acc.
: M = .56, SD = .03) than in the people do-main (Dice: M = .72, SD = .01; Acc.
: M = .20, SD =.02).
Regarding the two languages, the results showthat both algorithms generally performed better onEnglish peopleIA GraphSet size Dice Acc.
(%) Dice Acc.
(%)1 0.519 7.4 0.558 12.65 0.605 15.8 0.617 14.510 0.682 21.1 0.683 20.020 0.710 22.1 0.716 24.730 0.682 15.3 0.716 26.840 0.716 26.3 0.723 26.350 0.718 27.9 0.727 26.3All 0.724 31.6 0.730 28.9Dutch peopleIA GraphSet size Dice Acc.
(%) Dice Acc.
(%)1 0.626 4.5 0.682 17.55 0.737 16.0 0.738 21.010 0.738 12.5 0.741 19.520 0.765 12.5 0.778 25.530 0.762 14.5 0.789 25.040 0.763 11.5 0.792 25.050 0.764 10.5 0.798 26.0All 0.775 12.5 0.812 32.5Table 3: Performance for each set size in the people do-main.
For sizes 1 to 50, means over five sets are given.The full sets are 136 English and 160 Dutch descriptions.Note that the scores of the IA for the English sets of sizes1 to 30 were also reported in Theune et al (2011).the Dutch data (Dice: M = .84, SD = .01; Acc.
: M= .41, SD = .03) than on the English data (Dice: M= .74, SD = .01; Acc.
: M = .34, SD = .03).
Thereis no main effect of algorithm, meaning that over-all, the two algorithms had an equal performance.However, this is different when we look separatelyat each domain and language, as we do below.5.2 Learning curves per domain and languageGiven the main effects of domain and language de-scribed above, we ran separate ANOVAs for the dif-ferent domains and languages.
For these four analy-ses, we used set size as a within variable, and algo-rithm as a between variable.
To determine the effectsof set size, we calculated the means of the scoresof the five training sets for each set size, so that wecould compare them with the scores of the entire set.The results are shown in Tables 2 and 3.We made planned post hoc comparisons to testwhich is the smallest set size that does not performsignificantly different from the entire training set in7terms of Dice and Accuracy scores (we call this the?ceiling?).
We report results both for the standardBonferroni method, which corrects for multiplecomparisons, and for the less strict LSD methodfrom Fisher, which does not.
Note that with theBonferroni method we are inherently less likely tofind statistically significant differences between theset sizes, which implies that we can expect to reacha ceiling earlier than with the LSD method.
Table 4shows the ceilings we found for the algorithms, perdomain and language.The furniture domain Table 2 shows the Diceand Accuracy scores in the furniture domain.
Wefound significant effects of set size for both theEnglish data (Dice: F(7,518) = 15.59, p < .001;Acc.
: F(7,518) = 17.42, p < .001) and the Dutch data(Dice: F(7,546) = 5.322, p < .001; Acc.
: F(7,546)= 5.872, p < .001), indicating that for both lan-guages, the number of descriptions used for traininginfluenced the performance of both algorithms interms of both Dice and Accuracy.
Although wedid not find a main effect of algorithm, suggestingthat the two algorithms performed equally well, wedid find several interactions between set size andalgorithm for both the English data (Dice: F(7,518) =1.604, ns; Acc.
: F(7,518) = 2.282, p < .05) and theDutch data (Dice: F(7,546) = 3.970, p < .001; Acc.
:F(7,546) = 3.225, p < .01).
For the English furnituredata, this interaction implies that small set sizeshave a bigger impact for the IA than for Graph.For example, moving from set size 1 to 5 yielded aDice improvement of .18 for the IA, while this wasonly .09 for Graph.
For the Dutch furniture data,however, a reverse pattern was observed; movingfrom set size 1 to 5 yielded an improvement of .01(Dice) and .05 (Acc.)
for the IA, while this was .11(Dice) and .18 (Acc.)
for Graph.Post hoc tests showed that small set sizes weregenerally sufficient to reach ceiling performance:the general pattern for both algorithms and bothlanguages was that the scores increased with the sizeof the training set, but that the increase got smalleras the set sizes became larger.
For the Englishfurniture data, Graph reached the ceiling at set size10 for Dice (5 with the Bonferroni test), and at setsize 40 for Accuracy (again 5 with Bonferroni),while this was the case for the IA at set size 5 forEnglish furniture Dutch furnitureDice Accuracy Dice AccuracyIA 5 (5) 5 (5) 1 (1) 1 (1)Graph 10 (5) 40 (5) 5 (1) 5 (1)English people Dutch peopleDice Accuracy Dice AccuracyIA 10 (10) 40 (1) 20 (5) 1 (1)Graph 20 (10) 20 (1) 30 (20) 5 (1)Table 4: Ceiling set sizes computed using LSD, withBonferroni between brackets.both Dice and Accuracy (also 5 with Bonferroni).For the Dutch furniture data, Graph reached theceiling at set size 5 for both Dice and Accuracy(and even at 1 with the Bonferroni test), while thiswas at set size 1 for the IA (again 1 with Bonferroni).The people domain Table 3 shows the Diceand Accuracy scores in the people domain.
Again,we found significant effects of set size for both theEnglish data (Dice: F(7,518) = 39.46, p < .001;Acc.
: F(7,518) = 11.77, p < .001) and the Dutch data(Dice: F(7,546) = 33.90, p < .001; Acc.
: F(7,546)= 3.235, p < .01).
Again, this implies that forboth languages, the number of descriptions usedfor training influenced the performance of bothalgorithms in terms of both Dice and Accuracy.Unlike we did in the furniture domain, we foundno interactions between set size and algorithm, butwe did find a main effect of algorithm for the Dutchpeople data (Dice: F(1,78) = .751, ns; Acc.
: F(1,78)= 5.099, p < .05), showing that Graph generatedDutch descriptions that were more accurate thanthose generated by the IA.As in the furniture domain, post hoc tests showedthat small set sizes were generally sufficient to reachceiling performance.
For the English data, Graphreached the ceiling at set size 20 for both Dice andAccuracy (with Bonferroni: 10 for Dice, 1 for Accu-racy), while this was the case for the IA at set size 10for Dice (also 10 with Bonferroni), and at set size 40for Accuracy (and even at 1 with Bonferroni).
Forthe Dutch data, Graph reached the ceiling at set size30 for Dice (20 with Bonferroni), and at set size 5for Accuracy (1 with Bonferroni).
For the IA, ceil-ing was reached at set size 20 for Dice (Bonferroni:5), and already at 1 for Accuracy (Bonferroni: 1).86 Discussion and ConclusionOur main goal was to investigate how many human-produced references are required by REG algo-rithms such as the Incremental Algorithm and thegraph-based algorithm to determine preferences (orcosts) for a new domain, and to generate ?human-like?
descriptions for new objects in these domains.Our results show that small data sets can be usedto train these algorithms, achieving results that arenot significantly different from those derived froma much larger training set.
In the simple furnituredomain even one training item can already be suffi-cient, at least for the IA.
As shown in Table 4, on thewhole the IA needed fewer training data than Graph(except in the English people domain, where Graphonly needed a set size of 10 to hit the ceiling forDice, while the IA needed a set size of 20).Given that the IA ranks attributes, while thegraph-based REG algorithm ranks attribute-valuepairs, the difference in required training data isnot surprising.
In any domain, there will be moreattribute-value pairs than attributes, so determiningan attribute ranking is an easier task than determin-ing a ranking of attribute-value pairs.
Another ad-vantage of ranking attributes rather than attribute-value pairs is that it is less vulnerable to the problemof ?missing data?.
More specifically, the chance thata specific attribute does not occur in a small train-ing set is much smaller than the chance that a spe-cific attribute-value pair does not occur.
As a conse-quence, the IA needs fewer data to obtain completeattribute orderings than Graph needs to obtain costsfor all attribute-value pairs.Interestingly, we only found interactions betweentraining set size and algorithm in the furniture do-main.
In the people domain, there was no signifi-cant difference between the size of the training setsrequired by the algorithms.
This could be explainedby the fact that the people domain has about twice asmany attributes as the furniture domain, and fewervalues per attribute (see Table 1).
This means thatfor people the difference between the number of at-tributes (IA) and the number of attribute-value pairs(Graph) is not as big as for furniture, so the two al-gorithms are on more equal grounds.Both algorithms performed better on furniturethan on people.
Arguably, the people pictures in theTUNA experiment can be described in many moredifferent ways than the furniture pictures can, so itstands to reason that ranking potential attributes andvalues is more difficult in the people than in the fur-niture domain.
In a similar vein, we might expectGraph?s flexible generation strategy to be more use-ful in the people domain, where more can be gainedby the use of costs, than in the furniture domain,where there are relatively few options anyway, and asimple linear ordering may be quite sufficient.This expectation was at least partially confirmedby the results: although in most cases the differencesare not significant, Graph tends to perform numeri-cally better than the IA in the people domain.
Herewe see the pay-off of Graph?s more fine-grainedpreference ranking, which allows it to distinguishbetween more and less salient attribute values.
In thefurniture domain, most attribute values appear to bemore or less equally salient (e.g., none of the coloursgets notably mentioned more often), but in the peo-ple domain certain values are clearly more salientthan others.
In particular, the attributes HASBEARDand HASGLASSES are among the most frequent at-tributes in the people domain when their value istrue (i.e., the target object can be distinguished byhis beard or glasses), but they hardly get mentionedwhen their value is false.
Graph quickly learns thisdistinction, assigning low costs and a high rankingto <HASBEARD, true> and <HASGLASSES, true>while assigning high costs and a low ranking to<HASBEARD, false> and <HASGLASSES, false>.The IA, on the other hand, does not distinguish be-tween the values of these attributes.Moreover, the graph-based algorithm is arguablymore generic than the Incremental Algorithm, as itcan straightforwardly deal with relational propertiesand lends itself to various extensions (Krahmer etal., 2003).
In short, the larger training investmentrequired for Graph in simple domains may be com-pensated by its versatility and better performanceon more complex domains.
To test this assump-tion, our experiment should be repeated using datafrom a more realistic and complex domain, e.g., ge-ographic descriptions (Turner et al, 2008).
Unfortu-nately, currently no such data sets are available.Finally, we found that the results of both algo-rithms were better for the Dutch data than for theEnglish ones.
We think that this is not so much an ef-9fect of the language (as English and Dutch are highlycomparable) but rather of the way the TUNA and D-TUNA corpora were constructed.
The D-TUNA cor-pus was collected in more controlled conditions thanTUNA and as a result, arguably, it contains trainingdata of a higher quality.
Also, because the D-TUNAcorpus does not contain any location properties (X-and Y-DIMENSION) its furniture and people domainsare slightly less complex than their TUNA counter-parts, making the attribute selection task a bit easier.One caveat of our study is that so far we haveonly used the standard automatic metrics on REGevaluation (albeit in accordance with many otherstudies in this area).
However, it has been foundthat these do not always correspond to the results ofhuman-based evaluations, so it would be interestingto see whether the same learning curve effectsare obtained for extrinsic, task based evaluationsinvolving human subjects.
Following Belz andGatt (2008), this could be done by measuringreading times, identification times or error rates as afunction of training set size.Comparing IA with FB and GR We have shownthat small set sizes are sufficient to reach ceiling forthe IA.
But which preference orders (PO?s) do wefind with these small set sizes?
And how does theIA?s performance with these orders compare to theresults obtained by alternative algorithms such asDale and Reiter?s (1995) classic Full Brevity (FB)and Greedy Algorithm (GR)?
?
a question explicitlyasked by van Deemter et al (2012).
In the furnituredomain, all five English training sets of size 5 yielda PO for which van Deemter et al showed that itcauses the IA to significantly outperform FB andGR (i.e., either C(olor)O(rientation)S(ize) or CSO;note that here we abstract over TYPE which vanDeemter and colleagues do not consider).
Whenwe look at the English people domain and considerset size 10 (ceiling for Dice), we find that fourout of five sets have a preference order whereHAIRCOLOUR, HASBEARD and HASGLASSES arein the top three (again disregarding TYPE); one ofthese is the best performing preference order foundby van Deemter and colleagues (GBH), anotherperforms slightly less well but still significantlybetter than FB and GR (BGH); the other two scorestatistically comparable to the classical algorithms.The fifth people PO includes X- and Y-DIMENSIONin the top three, which van Deemter et al ignore.
Insum: in almost all cases, small set sizes (5 and 10respectively) yield POs with which the IA performsat least as well as the FB and GR algorithms, and inmost cases significantly better.Conclusion We have shown that with few traininginstances, acceptable attribute selection results canbe achieved; that is, results that do not significantlydiffer from those obtained using a much largertraining set.
Given the scarcity of resources inthis field, we feel that this is an important resultfor researchers working on REG and NaturalLanguage Generation in general.
We found that lesstraining data is needed in simple domains with fewattributes, such as the furniture domain, and more inrelatively more complex domains such as the peopledomain.
The data set being used is also of influence:better results were achieved with D-TUNA thanwith the TUNA corpus, which probably not so muchreflects a language difference, but a difference inthe way the corpora were collected.We found some interesting differences betweenthe IA and Graph algorithms, which can be largelyexplained by the fact that the former ranks attributes,and the latter attribute-value pairs.
The advantageof the former (coarser) approach is that overall,fewer training items are required, while the latter(more fine-grained) approach is better equipped todeal with more complex domains.
In the furnituredomain both algorithms had a similar performance,while in the people domain Graph did slightly betterthan the IA.
It has to be kept in mind that theseresults are based on the relatively simple furnitureand people domains, and evaluated in terms of alimited (though standard) set of evaluation met-rics.
We hope that in the near future semanticallytransparent corpora for more complex domains willbecome available, so that these kinds of learningcurve experiments can be replicated.Acknowledgments Krahmer and Koolen re-ceived financial support from The NetherlandsOrganization for Scientific Research (NWO Vicigrant 27770007).
We thank Albert Gatt for allowingus to use his implementation of the IA, and SanderWubben for help with k-means clustering.10ReferencesAnja Arts, Alfons Maes, Leo Noordman, and CarelJansen.
2011.
Overspecification facilitates objectidentification.
Journal of Pragmatics, 43(1):361?374.Anja Belz and Albert Gatt.
2008.
Intrinsic vs. extrinsicevaluation measures for referring expression genera-tion.
In Proceedings of the 46th Annual Meeting of theAssociation for Computational Linguistics (ACL?08),pages 197?200.Robert Dale and Ehud Reiter.
1995.
Computational in-terpretation of the Gricean maxims in the generation ofreferring expressions.
Cognitive Science, 19(2):233?263.Paul E. Engelhardt, Karl G.D Bailey, and Fernanda Fer-reira.
2006.
Do speakers and listeners observe theGricean Maxim of Quantity?
Journal of Memory andLanguage, 54:554?573.Giuseppe Di Fabbrizio, Amanda Stent, and SrinivasBangalore.
2008.
Trainable speaker-based refer-ring expression generation.
In Twelfth Conference onComputational Natural Language Learning (CoNLL-2008), pages 151?158.Albert Gatt, Ielka van der Sluis, and Kees van Deemter.2007.
Evaluating algorithms for the generation of re-ferring expressions using a balanced corpus.
In Pro-ceedings of the 11th European Workshop on NaturalLanguage Generation (ENLG 2007), pages 49?56.Albert Gatt, Anja Belz, and Eric Kow.
2009.
The TUNA-REG Challenge 2009: Overview and evaluation re-sults.
In Proceedings of the 12th European Workshopon Natural Language Generation (ENLG 2009), pages174?182.Pablo Gerva?s, Raquel Herva?s, and Carlos Le?on.
2008.NIL-UCM: Most-frequent-value-first attribute selec-tion and best-scoring-choice realization.
In Proceed-ings of the 5th International Natural Language Gener-ation Conference (INLG 2008), pages 215?218.Pamela W. Jordan and Marilyn Walker.
2005.
Learningcontent selection rules for generating object descrip-tions in dialogue.
Journal of Artificial Intelligence Re-search, 24:157?194.John Kelleher.
2007.
DIT - frequency based incremen-tal attribute selection for GRE.
In Proceedings of theMT Summit XI Workshop Using Corpora for NaturalLanguage Generation: Language Generation and Ma-chine Translation (UCNLG+MT), pages 90?92.Ruud Koolen and Emiel Krahmer.
2010.
The D-TUNAcorpus: A Dutch dataset for the evaluation of refer-ring expression generation algorithms.
In Proceedingsof the 7th international conference on Language Re-sources and Evaluation (LREC 2010).Emiel Krahmer, Sebastiaan van Erk, and Andre?
Verleg.2003.
Graph-based generation of referring expres-sions.
Computational Linguistics, 29(1):53?72.Chris Mellish, Donia Scott, Lynn Cahill, Daniel Paiva,Roger Evans, and Mike Reape.
2006.
A referencearchitecture for natural language generation systems.Natural Language Engineering, 12:1?34.Thomas Pechmann.
1989.
Incremental speech pro-duction and referential overspecification.
Linguistics,27:98?110.Ehud Reiter and Anja Belz.
2009.
An investigationinto the validity of some metrics for automaticallyevaluating NLG systems.
Computational Linguistics,35(4):529?558.Philipp Spanger, Takehiro Kurosawa, and TakenobuTokunaga.
2008.
On ?redundancy?
in selecting at-tributes for generating referring expressions.
In COL-ING 2008: Companion volume: Posters, pages 115?118.Marie?t Theune, Ruud Koolen, Emiel Krahmer, andSander Wubben.
2011.
Does size matter ?
How muchdata is required to train a REG algorithm?
In Pro-ceedings of the 49th Annual Meeting of the Associa-tion for Computational Linguistics: Human LanguageTechnologies, pages 660?664, Portland, Oregon, USA.Ross Turner, Somayajulu Sripada, Ehud Reiter, and Ian P.Davy.
2008.
Using spatial reference frames to gener-ate grounded textual summaries of georeferenced data.In Proceedings of the 5th International Natural Lan-guage Generation Conference (INLG), pages 16?24.Kees van Deemter, Ielka van der Sluis, and Albert Gatt.2006.
Building a semantically transparent corpus forthe generation of referring expressions.
In Proceed-ings of the 4th International Natural Language Gener-ation Conference (INLG 2006), pages 130?132.Kees van Deemter, Albert Gatt, Ielka van der Sluis, andRichard Power.
2012.
Generation of referring expres-sions: Assessing the Incremental Algorithm.
Cogni-tive Science, to appear.Jette Viethen and Robert Dale.
2010.
Speaker-dependentvariation in content selection for referring expressiongeneration.
In Proceedings of the 8th AustralasianLanguage Technology Workshop, pages 81?89.Jette Viethen, Robert Dale, Emiel Krahmer, Marie?t Theu-ne, and Pascal Touset.
2008.
Controlling redundancyin referring expressions.
In Proceedings of the SixthInternational Conference on Language Resources andEvaluation (LREC 2008), pages 239?246.11
