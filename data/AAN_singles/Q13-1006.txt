Transactions of the Association for Computational Linguistics, 1 (2013) 63?74.
Action Editor: Brian Roark.Submitted 9/2012; Published 3/2013.
c?2013 Association for Computational Linguistics.Unsupervised Dependency Parsing with Acoustic CuesJohn K Pate?
?j.k.pate@sms.ed.ac.ukSharon Goldwater?sgwater@inf.ed.ac.uk?ILCC, School of Informatics ?Department of ComputingUniversity of Edinburgh Macquarie UniversityEdinburgh, EH8 9AB, UK Sydney, NSW 2109, AustraliaAbstractUnsupervised parsing is a difficult task thatinfants readily perform.
Progress has beenmade on this task using text-based models, butfew computational approaches have consideredhow infants might benefit from acoustic cues.This paper explores the hypothesis that wordduration can help with learning syntax.
We de-scribe how duration information can be incor-porated into an unsupervised Bayesian depen-dency parser whose only other source of infor-mation is the words themselves (without punc-tuation or parts of speech).
Our results, evalu-ated on both adult-directed and child-directedutterances, show that using word duration canimprove parse quality relative to words-onlybaselines.
These results support the idea thatacoustic cues provide useful evidence aboutsyntactic structure for language-learning in-fants, and motivate the use of word durationcues in NLP tasks with speech.1 IntroductionUnsupervised learning of syntax is difficult for NLPsystems, yet infants perform this task routinely.
Pre-vious work in NLP has focused on using the implicitsyntactic information available in part-of-speech(POS) tags (Klein and Manning, 2004), punctuation(Seginer, 2007; Spitkovsky et al 2011b; Ponvert etal., 2011), and syntactic similarities between relatedlanguages (Cohen and Smith, 2009; Cohen et al2011).
However, these approaches likely use the datain a very different way from children: neither POStags nor punctuation are observed during languageacquisition (although see Spitkovsky et al(2011a)and Christodoulopoulos et al(2012) for encourag-ing results using unsupervised POS tags), and manychildren learn in a broadly monolingual environment.This paper explores a possible source of informationthat NLP systems typically ignore: word duration, orthe length of time taken to pronounce each word.There are good reasons to think that word dura-tion might be useful for learning syntax.
First, thewell-established Prosodic Bootstrapping hypothesis(Gleitman and Wanner, 1982) proposes that infantsuse acoustic-prosodic cues (such as word duration)to help them identify syntactic structure, becauseprosodic and syntactic structures sometimes coincide.More recently, we proposed (Pate and Goldwater,2011) that infants might use word duration as a di-rect cue to syntactic structure (i.e., without requir-ing intermediate prosodic structure), because wordsin high-probability syntactic structures tend to bepronounced more quickly (Gahl and Garnsey, 2004;Gahl et al 2006; Tily et al 2009).Like most recent work on unsupervised parsing,we focus on learning syntactic dependencies.
Ourwork is based on Headden et al(2009)?s Bayesianversion of the Dependency Model with Valence(DMV) (Klein and Manning, 2004), using interpo-lated backoff techniques to incorporate multiple infor-mation sources per token.
However, whereas Head-den et alused words and POS tags as input, weuse words and word duration information, presentingthree variants of their model that use this informationin slightly different ways.11By using neither gold-standard nor learned POS tags asinput, our work differs from nearly all previous work on unsuper-vised dependency parsing.
While learned tags might be plausible63To our knowledge, this is the first work to incor-porate acoustic cues into an unsupervised system forlearning full syntactic parses.
The methods in thispaper were inspired by our previous approach (Pateand Goldwater, 2011), which showed that word dura-tion measurements could improve the performanceof an unsupervised lexicalized syntactic chunker overa words-only baseline.
However, that work was lim-ited to HMM-like sequence models, tested on adult-directed speech (ADS) only, and none of the modelsoutperformed uniform-branching baselines.
Here, weextend our results to full dependency parsing, andexperiment on transcripts of both spontaneous ADSand child-directed speech (CDS).
Our models us-ing word duration outperform words-only baselines,along with the Common Cover Link parser of Seginer(2007), and the Unsupervised Partial Parser of Pon-vert et al(2011), unsupervised lexicalized parsersthat have obtained state-of-the-art results on standardnewswire treebanks (though their performance hereis worse, as our input lacks punctuation).
We alsooutperform uniform-branching baselines.2 Syntax and Word DurationBefore presenting our models and experiments, wefirst discuss why word duration might be a useful cueto syntax.
This section reviews the two possible rea-sons mentioned above: duration as a cue to prosodicstructure, or as a cue to predictability.2.1 Prosodic BootstrappingProsody is the structure of speech as conveyed byrhythm and intonation, which are, in turn, conveyedby such measurable phenomena as variation in fun-damental frequency, word duration, and spectral tilt.Prosodic structure is typically analyzed as imposinga shallow, hierarchical grouping structure on speech,with the ends of prosodic phrases (constituents) be-ing cued in part by lengthening the last word of thephrase (Beckman and Pierrehumbert, 1986).The Prosodic Bootstrapping hypothesis (Gleit-man and Wanner, 1982) points out that prosodicphrases are often also syntactic phrases, and proposesthat language-acquiring infants exploit this correla-tion.
Specifically, if infants can learn about prosodicphrase structure using word duration (and fundamen-in a model of language acquisition, gold tags certainly are not.tal frequency), they may be able to identify syntacticphrases more easily using word strings and prosodictrees than using word strings alone.Several behavioral experiments support the con-nection between prosody and syntax and the prosodicbootstrapping hypothesis specifically.
For example,there is evidence that adults use prosodic informationfor syntactic disambiguation (Millotte et al 2007;Price et al 1991) and to help in learning the syntaxof an artificial language (Morgan et al 1987), whileinfants can use acoustic-prosodic cues for utterance-internal clause segmentation (Seidl, 2007).On the computational side, we are aware of onlyour previous HMM-based chunkers (Pate and Gold-water, 2011), which learned shallow syntax fromwords, words and word durations, or words and hand-annotated prosody.
Using these chunkers, we foundthat using words plus prosodic annotation workedbetter than just words, and words plus word durationworked even better.
While these results are consistentwith the prosodic bootstrapping hypothesis, we sug-gested that predictability bootstrapping (see below)might be a more plausible explanation.Other computational work has combined prosodywith syntax, but only in supervised systems, and typi-cally using hand-annotated prosodic information.
Forexample, Huang and Harper (2010) used annotatedprosodic breaks as a kind of punctuation in a su-pervised PCFG, while prosodic breaks learned in asemi-supervised way have been used as features forparse reranking (Kahn et al 2005) or PCFG state-splitting (Dreyer and Shafran, 2007).
In contrast tothese methods, our approach observes neither parsetrees nor prosodic annotations.2.2 Predictability BootstrappingOn the basis of our HMM chunkers, we introducedthe predictability bootstrapping hypothesis (Pate andGoldwater, 2011): the idea that word durations couldbe a useful cue to syntactic structure not (or not only)because they provide information about prosodicstructure, but because they are a direct cue to syntac-tic predictability.
It is well-established that talkerstend to pronounce words more quickly when theyare more predictable, as measured by, e.g., wordfrequency, n-gram probability, or whether or not theword has been previously mentioned (Aylett and Turk,2004; Bell et al 2009).
However, syntactic proba-64you threw it right at the basketFigure 1: Example unlabeled dependency parse.bility also seems to matter, with studies showing thatverbs tend to be pronounced more quickly when theyare in their preferred syntactic frame?transitive vs.intransitive or direct object vs. sentential comple-ment (Gahl and Garnsey, 2004; Gahl et al 2006;Tily et al 2009).
While this syntactic evidence isonly for verbs, together with the evidence that effectsof other notions of predictability, it suggests that suchsyntactic effects may also be widespread.
If so, theduration of a word could give clues as to whether itis being used in a high-probability or low-probabilitystructure, and thus what the correct structure is.We found that our syntactic chunkers benefitedmore from duration information than prosodic an-notations, providing some preliminary evidence infavor of predictability bootstrapping, but not rulingout prosodic bootstrapping.
So, we are left with twoplausible mechanisms by which word duration couldhelp with learning syntax.
Slow pronunciations maycue the end of a prosodic phrase, which is sometimesalso the end of a syntactic phrase.
Alternatively, slowpronunciations may indicate that the hidden syntacticstructure is low probability, facilitating the induc-tion of a probabilistic grammar.
This paper will notseek to determine which mechanism is useful, insteadtaking the presence of two possible mechanisms asencouraging for the prospect of incorporating wordduration into unsupervised parsing.3 Models2As mentioned, we will be incorporating word dura-tion into unsupervised dependency parsing, produc-ing analyses like the one in Figure 1.
Each arc isbetween two words, with the head at the non-arrowend of the arc, and the dependent at the arrow end.One word, the root, depends on no word, and allother words depend on exactly one word.
Followingprevious work on unsupervised dependency parsing,we will not label the arcs.2The implementation of these models is available athttp://github.com/jpate/predictabilityParsing3.1 Dependency Model with ValenceAll of our models are ultimately based on the De-pendency Model with Valence (DMV) of Klein andManning (2004), a generative, probabilistic modelfor projective (i.e.
no crossing arcs), unlabeled de-pendency parses, such as the one in Figure 1.The DMV generates dependency parses usingthree probability distributions, which together com-prise model parameters ?.
First, the root of thesentence is drawn from Proot .
Second, we decidewhether to stop generating dependents of the headh in direction dir ?
{left, right} with probabilityPstop(?|h, dir , v), where v is T if h has a dir-warddependent and F otherwise.
If we decide to stop,then h takes no more dependents in the direction ofdir.
If we don?t stop, we use the third probabilitydistribution Pchoose(d|h, dir) to determine which de-pendent d to generate.
The second and third steprepeat for each generated word until all words havestopped generating in both directions.The DMV was the first unsupervised parsingmodel to outperform a uniform-branching baselineon the Wall Street Journal corpus.
It was trainedusing EM to obtain a maximum-likelihood estimateof the parameters ?, and learned from POS tags toavoid rare events.
However, all work on syntacticpredictability effects on word duration has been lexi-calized (looking at, e.g., the transitivity bias of par-ticular verbs).
In addition, it is unlikely that childrenhave access to the correct parts of speech when firstlearning syntactic structure.
Thus, we want a DMVvariant that learns from words rather than POS tags.We therefore adopt several extensions to the DMVdue to Headden et al(2009), described next.3.2 The DMV with BackoffHeadden et al(2009) sought to improve the DMV byincorporating lexical information in addition to POStags.
However, arcs between particular words arerare, so they modified the DMV in two ways to dealwith this sparsity.
First, they switched from MLE to aBayesian approach, estimating a probability distribu-tion over model parameters ?
and dependency treesT given the training corpus C and a prior distribution?
over models: P (T, ?|C,?
).Headden et alavoided overestimating the proba-bility of rare events that happen to occur in the train-65ing data by picking ?
to assign low probability tomodels ?
which give high probability to rare events.Accordingly, models that overcommit to rare eventswill contribute little to the final average over models.Specifically, Headden et aluse Dirichlet priors, with?
being the Dirichlet hyperparameters.Headden et als second innovation was to adapt in-terpolated backoff methods from language modelingwith n-grams, where one can estimate the probabil-ity of word wn given word wn?1 by interpolatingbetween unigram and bigram probability estimates:P?
(wn|wn?1) = ?P (wn|wn?1) + (1?
?
)P (wn)with ?
?
[0, 1].
Ideally, ?
should be large whenwn?1is frequent, and small when wn?1 is rare.
Headden etal.
(2009) apply this method to the DMV by backingoff from Choose and Stop distributions that conditionon both head word and POS to distributions thatcondition on only the head POS.In the equation above, ?
is a scalar parameter.However, it actually specifies a probability distri-bution over the decision to back off (B) or not backoff (?B), and we can use different notation to reflectthis view.
Specifically, ?stop(?)
and ?choose(?)
willrepresent our backoff distributions for the Stop andChoose decision, respectively.
Using hp and dp torepresent head and dependent POS tag and hw anddw to represent head and dependent word, one of themodels Headden et alexplored estimates:P?
choose(dp|hw, hp, dir , val) =?choose(?B|hw, hp, dir)Pchoose(dp|hw, hp, dir)+?choose(B|hw, hp, dir)Pchoose(dp|hp, dir) (1)with an analogous backoff for Pstop .
We can seefrom Equation 1 that P?choose backs off from a dis-tribution that conditions on hw to a distribution thatmarginalizes out hw, and that the extent of backoffvaries across hw; we can use this to back off morewhen we have less evidence about hw.
This modelonly conditions on words; it does not generate themin the dependents.
This means it is actually a condi-tional, rather than fully generative, model of observedPOS tags and unobserved syntax conditioned on theobserved words.Since identifying the true posterior distributionP (T, ?|C,?)
is intractable, Headden et aluse Mean-field Variational Bayes (Kurihara and Sato, 2006;Johnson, 2007), which finds an approximation to theposterior using an iterative EM-like algorithm.
In theE-step of VBEM, expected countsE(ri) are gatheredfor each latent variable using the Inside-Outside algo-rithm, exactly as in the E-step of traditional EM.
TheMaximization step differs from the M-Step of EM intwo ways.
First, the expected counts for each valueof the latent variable ri are incremented by the hy-perparameter ?i.
Second, the numerator and denom-inator are scaled by the function exp(?(?
)), whichreduces the probability of rare events.
Specifically,the Pchoose distribution is estimated using expecta-tions for each arc adp,h,dir from head h to dependentPOS tag dp in direction dir, and the update equationfor Pchoose from iteration n to n+ 1 is:P?n+1choose(dp|h, dir) =exp(?
(En(adp,h,dir ) + ?dp,h,dir ))exp(?
(?c(En(ac,h,dir ) + ?c,h,dir )))(2)where h is the head POS tag for the backoff distri-bution, and the head (word, POS) pair for the nobackoff distribution.
The update equation for Pstopis analogous.Now consider the update equations for ?choose :?
?n+1choose(?B|hw, hp, dir) =exp(?(?
?B +?c(En(ac,hw,hp,dir ))))exp(?
(?B + ?
?B +?c(En(ac,hw,hp,dir ))))?
?n+1choose(B|hw, hp, dir) =exp(?(?B))exp(?
(?B + ?
?B +?c(En(ac,hw,hp,dir ))))Only the ?B numerator includes the expected counts,so as we see hw in direction dir more often, the ?Bnumerator will swamp the B numerator.
By picking?B larger than ?
?B, we can bias our ?
distribution toprefer backing off until we expect at least ?B ?
?
?Barcs out of hw with tag hp in the direction of dir.To obtain good performance, Headden et alre-placed each word that appeared fewer than 100 timesin the training data with the token ?UNK.?
We willalso use such an UNK cutoff.3.3 DMV with DurationWe explore three models.
One is a straightforwardapplication of the DMV with Backoff to words and66(quantized) word duration, and the other two are fully-generative variants.
We also consider using wordsand POS tags as input to these models.
Backoff mod-els are given two streams of information, providingtwo of word identity, POS tag, or word duration foreach observed token.
We call one stream the ?back-off?
stream, and the other the ?extra?
stream.
Backoffmodels learn a probability distribution conditioningon both streams, backing off to condition on only thebackoff stream.Our first words and duration model takes the du-ration as the extra stream and the word identity asthe backoff stream, and, using ha to represent theacoustic information for the head, defines:P?
choose(dw|hw, ha, dir) =?choose(?B|hw, ha, dir)Pchoose(dw|hw, ha, dir)+?choose(B|hw, ha, dir)Pchoose(dw|hw, dir) (3)with an analogous backoff scheme for Pstop .
We willrefer to this conditional model as ?Cond.?
in ourexperiments.
This equation is similar to Equation 1,except it uses words and duration instead of wordsand POS tags, and backs off to, not away from, words.We back off to the sparse words, rather than the lesssparse duration, because duration provides almost noinformation about syntax in isolation.3Directly modelling the extra stream among thedependents may allow us to capture selectional re-strictions in POS and words models, or exploit ef-fects of syntactic predictability on dependent dura-tion.
We therefore explore variants that generate bothstreams in the dependents.
First, we examine a model(?Joint?)
that generates them jointly:P?choose(dw, da|hw, hp, dir) =?choose(?B|hw, ha, dir)Pchoose(dw, da|hw, ha, dir)+?choose(B|hw, ha, dir)Pchoose(dw, da|hw, dir) (4)However, this joint model will have a very large state-space and may suffer from the same data sparsity, sowe also explore a model (?Indep.?)
that generates the3Preliminary dev-set experiments confirmed this intuition, asmodels that backed off to word duration performed poorly.extra and backoff independently:P?choose(dw, da|hw, hp, dir) =?choose(?B|hw, ha, dir)Pchoose backoff (dw|hw, ha, dir)Pchoose extra(da|hw, ha, dir)+ ?choose(B|hw, ha, dir)Pchoose backoff (dw|hw, dir)Pchoose extra(da|hw, dir) (5)We also modified the DMV with Backoff to handleheavily lexicalized models.
In Headden et al(2009),arcs between words that never appear in the samesentence are given probability mass only by virtueof the backoff distribution to POS tags, which allappear in the same sentence at least once.
We want toavoid relying on POS tags, and we also want to useheld-out development and test sets to avoid implicitlyoverfitting the data when exploring different modelstructures.
To this end, we add one extra ?UNK hyper-parameter to the Dirichlet prior of Pchoose for eachcombination of conditioning events.
This hyperpa-rameter reserves probability mass for a head h to takea word dw as a dependent if h and dw never appearedtogether in the training data.
The amount of probabil-ity mass reserved decreases as we see hw more often.This is implemented in training by adding ?UNK tothe denominator of the Pchoose update equation foreach h and dir.
At test time, if a word dw appearsas an unseen dependent for head h, h takes dw as adependent with probability:P?
choose(dw|h, dir) = (6)exp(?(?UNK))exp(?
(?UNK +?c(Elast(rc,h,dir ) + ?c,h,dir )))Here, h may be a word, (word, POS) pair, or (word,duration) pair.
Since this event by definition neveroccurs in the training data, ?UNK does not appear inthe numerator during training.
4Finally, the conditional model ignores the extrastream in Proot , and the generative models estimate4Note also that ?UNK is different from a global UNK cutoff,which is imposed in preprocessing, and so effects every occur-rence of an an UNK?d word in the model.
?UNK affects onlydependents in Pchoose , and treats a dependent as UNK iff it didnot occur on that particular side of that particular head word inany sentence.
We used both global UNK cutoffs (optimized onthe dev set) and these ?UNK hyperparameters.67Train Dev Testwsj10 Word tokens 42,505 1,765 2,571Word types 7,804 818 1,134Sentences 6,007 233 357swbdnxt10 Word tokens 24,998 2,980 3,052Word types 2,647 760 767Sentences 3,998 488 491brent Word tokens 20,954 2,127 2,206Word types 1,390 482 488Sentences 6,249 424 449Table 1: Statistics for our three corpora.Proot over both streams jointly and independently,respectively.4 Experimental Setup4.1 DatasetsWe evaluate on three datasets: wsj10, sentences oflength 10 or less from the Wall Street Journal por-tion of the Penn Treebank; swbdnxt10, sentencesof length 10 or less from the Switchboard datasetof ADS used by Pate and Goldwater (2011); andbrent, part of the Brent corpus of CDS (Brent andSiskind, 2001).
Table 1 presents corpus statistics.4.1.1 wsj10We present a new evaluation of the DMV withBackoff on wsj10, which does not have any acous-tic information, simply to verify that ?UNK performssensibly on a standard corpus.
Additionally, Headdenet al(2009) use an intensive initializer that relies ondozens of random restarts, and so, strictly speaking,only show that the backoff technology is useful forgood initializations.
Our new evaluation will showthat the backoff technology provides a substantialbenefit even for harmonic initialization.wsj10 was created in the standard way; all punc-tuation and traces were removed, and sentences con-taining more than ten tokens were discarded.
Forour fully lexicalized version of wsj10, all wordswere lowercased, and numbers were replaced withthe token ?NUMBER.
?5 Following standard practice,we used sections 2-21 for training, section 22 fordevelopment, and section 23 for test.
wsj10 con-tains hand-annotated constituency parses, not depen-dency parses, so we used the standard ?constituency-5Numbers were treated in this way only in wsj10.to-dependency?
conversion tool of Johansson andNugues (2007) to obtain high-quality CoNLL-styledependency parses.4.1.2 swbdnxt10Next, we evaluate on swbdnxt10, which con-tains all sentences up to length 10 from the samesections of the swbdnxt version of Switchboardused by Pate and Goldwater (2011).
Short sentencesare usually formulaic discourse responses (e.g.
?ohok?
), so this dataset al excludes sentences shorterthan three words.
As our models successfully useword durations, this evaluation provides an importantreplication of the basic result from Pate and Goldwa-ter (2011) with a different kind of syntactic model.swbdnxt10 has a forced alignment of adictionary-based phonetic transcription of each ut-terance to audio, providing our word duration infor-mation.
As a very simple model of hyper-articulationand hypo-articulation, we classify a word as in thelongest third duration, shortest third, or middle third.To minimize effects of word form, this classificationwas based on vowel count (counting a diphthong asone vowel): each word with n vowels is classified asin the shortest, longest, or middle tercile of durationamong words with n vowels.Like wsj10, swbdnxt10 is annotated onlywith constituency parses, so to provide approximate?gold-standard?
dependencies, we used the sameconstituency-to-dependency conversion tool as forwsj10.
We evaluated 200 randomly-selected sen-tences to check the accuracy of the conversion tool,which was designed for newspaper text.
Excludingarcs involving words with no clear role in depen-dency structure (such as ?um?
), about 86% of thearcs were correct.
While this rate is uncomfortablylow, it is still much higher than unsupervised depen-dency parsers typically achieve, and so may providea reasonable measure of relative dependency parsequality among competing systems.4.1.3 brentWe also evaluated our models on the ?Large Brent?dataset introduced in Rytting et al(2010), a por-tion of the Brent corpus of child-directed speech(Brent and Siskind, 2001).
We call this corpusbrent.
It consists of utterances from four of themothers in Brent and Siskind?s (2001) study, and, like68swbdnxt10, has a forced alignment from which weobtain duration terciles.
Rytting et al(2010) useda 90%/10% train/test partition.
We extracted everyninth utterance from the original training partition tocreate a dev set, producing an 80%/10%/10% parti-tion.
We also separated clitics from their base word.This dataset only has 186 sentences longer than tenwords, with a maximum length of 22 words, so wediscarded only sentences shorter than three wordsfrom the evaluation sets.The Brent corpus is distributed via CHILDES(MacWhinney, 2000) with automatic dependency an-notations.
However, these are not hand-corrected,and rely on a different tokenization of the datasetthan is present on the transcription tier.
To produce areliable gold-standard,6 we annotated all sentences oflength 2 or greater from the development and test setswith dependencies drawn from the Stanford TypedDependency set (de Marneffe and Manning, 2008)using the annotation tool used for the CopenhagenDependency Treebank (Kromann, 2003).4.2 ParametersIn all experiments, hyperparameters for Proot , Pstop ,and Pchoose (and their backed-off distributions, andincluding ?UNK) were 1, ?B was 10, and ?
?B was 1.VBEM was run on the training set until the datalog-likelihood changed by less than 0.001%, andthen the parameters were held fixed and used toobtain Viterbi parses for the evaluation sentences.Finally, we explored different global UNK cutoffs,replacing each word that appeared less than c timeswith the token UNK.
We ran each model for eachc ?
{0, 1, 25, 50, 100}, and picked the best-scoringc on the development set for running on the test setand presentation here.
We used a harmonic initializersimilar to the one in Klein and Manning (2004).4.3 EvaluationIn addition to evaluating the various incarnations ofthe DMV with backoff and input types, we compareto uniform branching baselines, the Common CoverLink (CCL) parser of Seginer (2007), and the Unsu-pervised Partial Parser (UPP) of Ponvert et al(2011).The UPP produces a constituency parse from wordsand punctuation using a series of finite-state chun-6Available at http://homepages.inf.ed.ac.uk/s0930006/brentDep/kers; we use the best-performing (Probabilistic RightLinear Grammar) version.
The CCL parser producesa constituency parse using a novel ?Cover Link?
rep-resentation, scoring these links heuristically.
BothCCL and UPP rely on punctuation (though accordingto Ponvert et al(2011), UPP less so), which our in-put is missing.
The left-headed ?LH?
(right-headed?RH?)
baseline assumes that each word takes the firstword to its right (left) as a dependent, and corre-sponds to a uniform right-branching (left-branching)constituency baseline.We evaluate the output of all models in termsof both constituency scores and dependency accu-racy.
Our wsj10 and swbdnxt10 corpora areoriginally annotated for constituency structure, withthe dependency gold standard derived as describedabove, while our brent corpus is originally anno-tated for dependency structure, with the constituencygold standard derived by defining a constituent tospan a head and each of its dependents (ignoringany one-word ?constituents?).
As the CCL andUPP parsers don?t produce dependencies, only con-stituency scores are provided.For constituency scores, we present the standardunlabeled Precision, Recall, and F-measure scores.For dependency scores, we present Directed attach-ment accuracy, Undirected attachment accuracy, andthe ?Neutral Edge Detection?
(NED) score intro-duced by Schwartz et al(2011).
Directed attachmentaccuracy counts an arc as a true positive if it correctlyidentifies both a head and a dependent, whereas undi-rected attachment accuracy ignores arc direction incounting true positives.
NED counts an arc as a truepositive if it would be a true positive under the Undi-rected attachment score, or if the proposed head isthe gold-standard grandparent of the proposed depen-dent.
This avoids penalizing parses for flipping anarc, such as making determiners, rather than nouns,the head of noun phrases.To assess statistical significance, we carried outstratified shuffling tests, with 10, 000 random shuf-fles, for all measures.
Tables indicate significancedifferences between the backoff models and the mostcompetitive baseline model on that measure, indi-cated by an italic score.
A star (?)
indicates p < 0.05,and a dagger (?)
indicates p < 0.01.
To see the di-rection of a significant difference (i.e.
whether thebackoff model is better or worse than the baseline),69wsj10 swbdnxt10Dependency Constituency Dependency ConstituencyUNK Dir.
Undir.
NED P R F UNK Dir.
Undir.
NED P R FEMWds 25 32.5 52.5 67.0 49.5 48.5 49.0 25 30.6 50.9 66.8 45.4 47.1 46.3POS ?
46.4 63.8 78.1 59.2 58.1 58.6 ?
53.0 65.0 76.8 52.5 52.9 52.7VB Wds 25 29.4 52.4 70.5 51.3 52.6 52.0 25 36.1 54.9 72.7 49.0 50.0 49.5POS ?
43.5 61.9 77.3 59.7 57.1 58.4 ?
51.3 62.5 74.3 47.1 46.6 46.8Wds+POS Cond.
50 49.9?
66.1?
79.6?
64.2?
61.9?
63.0?
100 45.5?
62.4?
77.8 58.4?
58.9?
58.7?Joint 50 46.0 63.7 79.0 62.0?
59.1 60.5?
1 49.4?
63.7 79.6?
60.0?
52.9 56.3?Indep.
25 52.5?
68.0?
83.5?
63.5?
61.5?
62.5?
100 55.7?
65.8 74.6?
61.5?
57.9?
59.6?LH ?
26.0 55.8 74.3 53.1 69.6 60.3 ?
24.1 50.8 72.7 60.8 82.5 70.0RH ?
31.2 56.4 61.4 25.8 33.8 29.3 ?
29.2 52.0 57.9 22.2 30.1 25.5CCL ?
?
?
?
50.8 40.7 45.2 ?
?
?
?
53.6 47.4 50.3UPP ?
?
?
?
52.8 37.2 43.7 ?
?
?
?
60.0 46.6 52.4Table 2: Performance on wsj10 and swbdnxt10 for models using words and POS tags only.
Bold scores indicatethe best performance of all models and baselines on that measure.?
Significantly different from best non-uniform baseline (italics) by a stratified shuffling test, p < 0.01; ?
: p < 0.05.look to the scores themselves.5 ResultsIn all results, when a model sees only one kind ofinformation, that is expressed by writing out the ab-breviation for the relevant stream: ?Wds?
for words,?POS?
for Part-Of-Speech, ?Dur?
for word duration.For baseline models that see two streams, the abbre-viations are joined by a ???
symbol (as they treatinput pairs as atoms drawn in the cross-product of thetwo streams?
vocabulary).
For the backoff models,the abbreviations are joined by a ?+?
symbol (as theycombine the information sources with a weightedsum), with the ?extra?
stream name first.5.1 Results: wsj10The left half of Table 2 presents results on wsj10.For the baseline models, the first column with hori-zontal text indicates the input, while for the backoff(Wds+POS) models, the first column with horizontaltext indicates whether and how the extra stream ismodeled in dependents (as described in Section 3.3).The EM model with POS input is largely a repli-cation of the original DMV, differing in the use ofseparate train, dev, and test sets, and possibly thedetails of the harmonic initializer.
Our replicationachieves an undirected attachment score of 63.8 onthe test set, similar to the score of 64.5 reported byKlein and Manning (2004) when training and evalu-ating on all of wsj10.
Cohen et al(2008) use thesame train/dev/test partition that we do, and reporta directed attachment score of 45.8, similar to ourdirected attachment score of 46.4.The VB model which learns from POS tags doesnot outperform the EM model which learns from POStags, suggesting that data sparsity does not hurt theDMV when using POS tags.
As expected, the words-only models perform much worse than both the POSinput models and the uniform LH baseline.
VB doesimprove the words-only constituency performance.The Cond.
and Indep.
backoff models outperformthe POS-only baseline on all measures, but the Jointbackoff model does not demonstrate a clear advan-tage over the POS-only baseline on any measure.
Thesuccess of the Indep.
model indicates that modellingdependent word identity does provide enough infor-mation to justify the increase in sparsity.
The failureof the Joint model to provide a further improvementindicates that the extra information in the full jointover dependents does not justify the large increasein parameters.
We also see that several models out-perform the LH baseline on dependencies, but theadvantage is much less in F-Score, underscoring theloss of information in the conversion of dependen-cies to constituencies.
Finally, all models outperformCCL and UPP on F-score, emphasizing their relianceon the punctuation we removed.70Dependency ConstituencyUNK Dir.
Undir.
NED P R FEMWds 25 30.6 50.9 66.8 45.4 47.1 46.3Wds?Dur 25 26.1 46.5 62.0 45.6 48.7 47.1VB Wds 25 36.4 55.1 73.0 49.1 50.0 49.6Wds?Dur 25 31.8 51.7 71.3 49.2 55.9 52.3Dur+Wds Cond.
25 32.6?
55.1 74.5?
59.1?
71.4?
64.7?Joint 50 31.8?
51.8?
70.8?
54.4?
60.5?
57.3?Indep.
50 40.3?
59.1?
76.0?
56.1?
61.7?
58.8?LH ?
24.1 50.8 72.7 60.8 82.5 70.0RH ?
29.2 52.0 57.9 22.2 30.1 25.5CCL ?
?
?
?
53.6 47.4 50.3UPP ?
?
?
?
60.0 46.6 52.4ll45 50 55 6045505560657075Switchboard Model PerformanceUndirected Attachment ScoreConstituencyF?scorellWdsWdsxDurCond.JointIndep.LHTable 3: Performance on swbdnxt10 for models using words and duration.
The scatterplot includes a subset of theinformation in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.Bold, italics, and significance annotations as in Table 2.5.2 Results: swbdnxt10The right half of Table 2 presents performance fig-ures on swbdnxt10 for input involving words andPOS tags.
As expected, the EM and VB baselinesperform best when learning from gold-standard POStags, and we again see no benefit for the VB POS-only model compared to the EM POS-only model.The POS-only baselines far outperform the uniform-attachment baselines on the dependency measures; toour knowledge this is the first demonstration outsidethe newspaper domain that the DMV outperforms auniform branching strategy on these measures.The other comparisons among systems listed inTable 2 are largely inconclusive.
Models do com-paratively well on either the constituency or depen-dency evaluation, but not both.
The backoff mod-els outperform the baseline POS-only models in theconstituency evaluation, but underperform or matchthose same models in the dependency evaluation.Conversely, most models outperform the LH base-line in the dependency evaluation, but not in theconstituency evaluation.
There are probably twocauses for the ambiguity in these results.
First, thenoise in the dependency gold-standard may have over-whelmed any advantage from backoff.
Second, as wesaw with wsj10, the conversion from dependenciesto constituencies removes information, which mayexplain the failure of any model to outperform theLH baseline in the constituency evaluation.Table 3 presents performance figures onswbdnxt10 for input involving words and duration,including a scatter-plot of Undirected attachmentagainst constituency F-Score for the interestingcomparisons.
In the scatter-plot, models up andto the right performed better, and we see that thenegative correlation between the dependency andconstituency evaluations persists in words and dura-tion input.
VB substantially outperforms EM in thebaselines, indicating that good smoothing is helpfulwhen learning from words.
Other comparisonsare again ambiguous; the dependency evaluationis noisy, and backoff models outperform baselinemodels on the constituency evaluation but not theLH baseline.
Still, the backoff models outperformall words-only baselines in constituency score, withtwo performing slightly worse in dependency scoreand one performing much better.
So there is someevidence that word duration is useful, but we willfind clearer evidence on the brent corpus.5.3 Results: brentTable 4 presents results on the brent dataset.
VBis even more effective than in the other datasets forimproving performance among baseline models, lead-ing to double-digit improvements on some measures.Moreover, the best dev-set UNK cutoff drops to 1for all VB models, indicating that, on this dataset,VB provides good smoothing even in models withoutbackoff.
This difference between datasets is likelyrelated to differences in vocabulary diversity; the71Dependency ConstituencyUNK Dir.
Undir.
NED P R FEMWds 25 36.9 56.3 70.7 52.4 69.5 59.8Wds?Dur 25 31.3 51.1 66.9 50.7 64.7 56.9VB Wds 1 51.2 64.2 77.3 63.3 68.1 66.0Wds?Dur 1 47.0 60.5 74.0 66.2 64.9 65.5Dur+Wds Cond.
1 53.1?
65.5?
78.7?
65.4 68.6 67.0?Joint 1 50.7 63.0 76.3 65.6 65.4?
65.5Indep.
1 53.2 66.7?
79.6?
61.5?
67.9 64.5LH ?
28.3 53.6 78.3 47.9 85.6 61.4RH ?
27.2 48.8 61.1 26.2 46.8 33.6CCL ?
?
?
?
41.7 58.8 48.8UPP ?
?
?
?
56.8 63.8 60.1ll50 55 60 65 70606264666870Brent Model PerformanceUndirected Attachment ScoreConstituencyF?scorellWdsWdsxDurCond.JointIndep.LHTable 4: Performance on brent for models using words and duration.
The scatterplot includes a subset of theinformation in the table: F-score and undirected attachment accuracy for backoff models and VB and LH baseline.Bold, italics, and significance annotations as in Table 2.type:token ratio in the brent training set is about1:15, compared to 1:5 and 1:9 in the wsj10 andswbdnxt10 training sets, respectively.More importantly for our main hypothesis, allthree backoff models using words and duration out-perform the words-only baselines (including CCLand UPP) on all dependency measures?the mostaccurate measures on this corpus, which has hand-annotated dependencies?and the Cond.
model alsowins on F-score.6 ConclusionIn this paper, we showed how to use the DMV withBackoff and two fully-generative variants to explorethe utility of word duration in fully lexicalized un-supervised dependency parsing.
Although other re-searchers have incorporated features beyond wordsand POS tags into DMV-like models (e.g., semantics:Naseem and Barzilay (2011); morphology: Berg-Kirkpatrick et al(2009)), we believe this is the firstexample based on Headden et al(2009)?s backoffmethod.
As far as we know, our work is also the firsttest of a DMV-based model on transcribed conver-sational speech and the first to outperform uniform-branching baselines without using either POS tags orpunctuation in the input.
Our results show that fully-lexicalized models can do well if they are smoothedproperly and exploit multiple cues.Our experiments also suggest that CDS is espe-cially easy to learn from.
Model performance onthe brent dataset was generally higher than onswbdnxt10, with a much lower UNK threshold.This latter point, and the fact that brent has a muchlower word type/token ratio than the other datasets,suggest that CDS provides more and clearer evidenceabout words?
syntactic behavior.Finally, our results provide more evidence, usinga different, more powerful syntactic model than thatof Pate and Goldwater (2011), that word durationis a useful cue for unsupervised parsing.
We foundthat several ways of incorporating duration were use-ful, although the extra sparsity of Joint emissionswas not justified in any of our investigations.
Ourresults are consistent with both the prosodic and pre-dictability bootstrapping hypotheses of language ac-quisition, providing the first computational supportfor these using a full syntactic parsing model andtested on child-directed speech.
While our models donot provide a mechanistic account of how childrenmight use duration information to help with learningsyntax, they do show that this information is usefulin principle, even without any knowledge of latentprosodic structure or its relationship to syntax.
In ad-dition, our results suggest it may be useful to exploreusing word duration to enrich NLP tasks in speech-related technologies, such as syntactically-inspiredlanguage models for text-to-speech generation.
Inthe future, we also hope to investigate why durationis helpful, designing experiments to tease apart therole of prosody and predictability in learning syntax.72ReferencesMatthew Aylett and Alice Turk.
2004.
The smooth signalredundancy hypothesis: A functional explanation for re-lationships between redundancy, prosodic prominence,and duration in spontaneous speech.
Language andSpeech, 47(1):31?56.Mary Beckman and Janet Pierrehumbert.
1986.
Intona-tional structure in Japanese and English.
PhonologyYearbook, 3:255?309.Alan Bell, Jason M Brenier, Michelle Gregory, CynthiaGirand, and Dan Jurafsky.
2009.
Predictability effectson durations of content and function words in conver-sational English.
Journal of Memory and Language,60:92?111.Taylor Berg-Kirkpatrick, Alexandre Bouchard-Co?te?, JohnDeNero, and Dan Klein.
2009.
Painless unsupervisedlearning with features.
In Proceedings of NAACL.Michael R Brent and Jeffrey M Siskind.
2001.
The roleof exposure to isolated words in early vocabulary de-velopment.
Cognition, 81:31?44.Christos Christodoulopoulos, Sharon Goldwater, andMark Steedman.
2012.
Turning the pipeline into a loop:Iterated unsupervised dependency parsing and PoS in-duction.
In Proceedings of the NAACL-HLT Workshopon the Induction of Linguistic Structure, pages 96?99,Montre?al, Canada, June.
Association for ComputationalLinguistics.Shay B Cohen and Noah A Smith.
2009.
Shared lo-gistic normal distributions for soft parameter tying inunsupervised grammar induction.
In Proceedings ofNAACL.Shay B Cohen, Kevin Gimpel, and Noah A Smith.
2008.Logistic normal priors for unsupervised probabilisticgrammar induction.
In Advances in Neural InformationProcessing Systems 22.Shay B Cohen, Dipanjan Das, and Noah A Smith.
2011.Unsupervised structure prediction with non-parallelmultilingual guidance.
In Proceedings of EMNLP.Marie-Catherine de Marneffe and Christopher D Manning.2008.
Stanford typed dependencies manual.
Technicalreport.Markus Dreyer and Izhak Shafran.
2007.
Exploitingprosody for PCFGs with latent annotations.
In Pro-ceedings of Interspeech, Antwerp, Belgium, August.Susanne Gahl and Susan M Garnsey.
2004.
Knowledge ofgrammar, knowledge of usage: Syntactic probabilitiesaffect pronunciation variation.
Language, 80:748?775.Susanne Gahl, Susan M Garnsey, Cynthia Fisher, andLaura Matzen.
2006.
?That sounds unlikely?
: Syntac-tic probabilities affect pronunciation.
In Proceedingsof the 27th meeting of the Cognitive Science Society.Lila Gleitman and Eric Wanner.
1982.
Language acqui-sition: The state of the art.
In Eric Wanner and LilaGleitman, editors, Language acquisition: The state ofthe art, pages 3?48.
Cambridge University Press, Cam-bridge, UK.Will Headden, Mark Johnson, and David McClosky.
2009.Improved unsupervised dependency parsing with richercontexts and smoothing.
In Proceedings of NAACL-HLT.Zhongqiang Huang and Mary Harper.
2010.
Appropri-ately handled prosodic breaks help PCFG parsing.
InProceedings of NAACL-HLT, pages 37?45, Los Ange-les, California, June.
Association for ComputationalLinguistics.Richard Johansson and Pierre Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProceedings of NODALIDA 2007.Mark Johnson.
2007.
Why doesn?t EM find good HMMPOS-taggers.
In Proceedings of EMNLP-CoNLL, pages296?305.Jeremy G Kahn, Matthew Lease, Eugene Charniak, MarkJohnson, and Mari Ostendorf.
2005.
Effective use ofprosody in parsing conversational speech.
In Proceed-ings of HLT-EMNLP, pages 233?240.Dan Klein and Christopher D. Manning.
2004.
Corpus-based induction of syntactic structure: Models of de-pendency and constituency.
In Proceedings of ACL,pages 479?486.Matthias Trautner Kromann.
2003.
The Danish Depen-dency Treebank and the DTAG treebank tool.
In Pro-ceedings of the Second Workshop on Treebanks andLinguistic Theories, pages 217?220.Kenichi Kurihara and Taisuke Sato.
2006.
VariationalBayesian grammar induction for natural language.
InProceedings of the International Colloquium on Gram-matical Inference, pages 84?96.Brian MacWhinney.
2000.
The CHILDES project: Toolsfor analyzing talk.
Lawrence Erlbaum Associates, Mah-wah, NJ, third edition.Se?verine Millotte, Roger Wales, and Anne Christophe.2007.
Phrasal prosody disambiguates syntax.
Lan-guage and Cognitive Processes, 22(6):898?909.James L Morgan, Richard P Meier, and Elissa L Newport.1987.
Structural packaging in the input to languagelearning: contributions of prosodic and morphologi-cal marking of phrases to the acquisition of language.Cognitive Psychology, 19:498?550.Tahira Naseem and Regina Barzilay.
2011.
Using seman-tic cues to learn syntax.
In Proceedings of AAAI.John K Pate and Sharon Goldwater.
2011.
Unsupervisedsyntactic chunking with acoustic cues: computationalmodels for prosodic bootstrapping.
In Proceedingsof the 2nd ACL workshop on Cognitive Modeling andComputational Linguistics.73Elias Ponvert, Jason Baldridge, and Katrin Erk.
2011.Simple unsupervised grammar induction from raw textwith cascaded finite state models.
In Proceedings ofACL-HLT.Patti J Price, Mari Ostendorf, Stefanie Shattuck-Hufnagel,and Cynthia Fong.
1991.
The use of prosody in syntac-tic disambiguation.
In Proceedings of the HLT work-shop on Speech and Natural Language, pages 372?377,Morristown, NJ, USA.
Association for ComputationalLinguistics.C Anton Rytting, Chris Brew, and Eric Fosler-Lussier.2010.
Segmenting words from natural speech: subseg-mental variation in segmental cues.
Journal of ChildLanguage, 37(3):513?543.Roy Schwartz, Omri Abend, Roi Reichart, and Ari Rap-poport1.
2011.
Neutralizing linguistically problematicannotations in unsupervised dependency parsing evalu-ation.
In Proceedings of the 49th ACL, pages 663?672.Yoav Seginer.
2007.
Fast unsupervised incremental pars-ing.
In Proceedings of ACL.Amanda Seidl.
2007.
Infants?
use and weighting ofprosodic cues in clause segmentation.
Journal of Mem-ory and Language, 57(1):24?48.Valentin I Spitkovsky, Hiyan Alshawi, Angel X Chang,and Daniel Jurafsky.
2011a.
Unsupervised dependencyparsing without gold part-of-speech tags.
In Proceed-ings of EMNLP.Valentin I Spitkovsky, Hiyan Alshawi, and Daniel Jurafsky.2011b.
Punctuation: Making a point in unsuperviseddependency parsing.
In Proceedings of CoNLL.Harry Tily, Susanne Gahl, Inbal Arnon, Neal Snider,Anubha Kothari, and Joan Bresnan.
2009.
Syntacticprobabilities affect pronunciation variation in sponta-neous speech.
Language and Cognition, 1(2):147?165.74
