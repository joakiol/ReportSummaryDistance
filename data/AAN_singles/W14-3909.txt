Proceedings of The First Workshop on Computational Approaches to Code Switching, pages 80?86,October 25, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsThe CMU Submission for the Shared Task on Language Identification inCode-Switched DataChu-Cheng Lin Waleed Ammar Lori Levin Chris DyerLanguage Technologies InstituteCarnegie Mellon UniversityPittsburgh, PA, 15213, USA{chuchenl,wammar,lsl,cdyer}@cs.cmu.eduAbstractWe describe the CMU submission forthe 2014 shared task on language iden-tification in code-switched data.
Weparticipated in all four language pairs:Spanish?English, Mandarin?English,Nepali?English, and Modern StandardArabic?Arabic dialects.
After describingour CRF-based baseline system, wediscuss three extensions for learning fromunlabeled data: semi-supervised learning,word embeddings, and word lists.1 IntroductionCode switching (CS) occurs when a multilingualspeaker uses more than one language in the sameconversation or discourse.
Automatic idenefica-tion of the points at which code switching occursis important for two reasons: (1) to help sociolin-guists analyze the frequency, circumstances andmotivations related to code switching (Gumperz,1982), and (2) to automatically determine whichlanguage-specific NLP models to use for analyz-ing segments of text or speech.CS is pervasive in social media due to its in-formal nature (Lui and Baldwin, 2014).
The firstworkshop on computational approaches to codeswitching in EMNLP 2014 organized a shared task(Solorio et al., 2014) on identifying code switch-ing, providing training data of multilingual tweetswith token-level language-ID annotations.
See?2 for a detailed description of the shared task.This short paper documents our submission in theshared task.We note that constructing a CS data set that isannotated at the token level requires remarkablemanual effort.
However, collecting raw tweets iseasy and fast.
We propose leveraging both labeledand unlabeled data in a unified framework; condi-tional random field autoencoders (Ammar et al.,2014).
The CRF autoencoder framework consistsof an encoding model and a reconstruction model.The encoding model is a linear-chain conditionalrandom field (CRF) (Lafferty et al., 2001) whichgenerates a sequence of labels, conditional on atoken sequence.
Importantly, the parameters ofthe encoding model can be interpreted in the sameway a CRF model would.
This is in contrary togenerative model parameters which explain boththe observation sequence and the label sequence.The reconstruction model, on the other hand, inde-pendently generates the tokens conditional on thecorresponding labels.
Both labeled and unlabeleddata can be efficiently used to fit parameters of thismodel, minimizing regularized log loss.
See ?4.1for more details.After modeling unlabeled token sequences, weexplore two other ways of leveraging unlabeleddata: word embeddings and word lists.
The wordembeddings we use capture monolingual distribu-tional similarities and therefore may be indicativeof a language (see ?4.2).
A word list, on the otherhand, is a collection of words which have beenmanually or automatically constructed and sharesome property (see ?4.3).
For example, we extractthe set of surface forms in monolingual corpora.In ?5, we describe the experiments and discussresults.
According to the results, modeling unla-beled data using CRF autoencoders did not im-prove prediction accuracy.
Nevertheless, more ex-periments need to be run before we can concludethis setting.
On the positive side, word embed-dings and word lists have been shown to improveCS prediction accuracy, provided they have decentcoverage of tokens in the test set.2 Task DescriptionThe shared task training data consists of code?switched tweets with token-level annotations.The data is organized in four language pairs:English?Spanish (En-Es), English?Nepali (En-80Ne), Mandarin?English (Zh-En) and ModernStandard Arabic?Arabic dialects (MSA-ARZ).Table 1 shows the size of the data sets providedfor the shared task in each language pair.For each tweet in the data set, the user ID, tweetID, and a list of tokens?
start offset and end offsetare provided.
Each token is annotated with oneof the following labels: lang1, lang2, ne (i.e.,named entities), mixed (i.e., mixed parts of lang1and lang2), ambiguous (i.e., cannot be identifiedgiven context), and other.Two test sets were used to evaluate each sub-mission for the shared task in each language pair.The first test set consists of Tweets, similar to thetraining set.
The second test set consists of tokensequences from a surprise genre.
Since partici-pants were not given the test sets, we only reportresults on a Twitter test set (a subset of the dataprovided for shared task participants).
Statisticsof our train/test data splits are given in Table 5.lang.
pair split tweets tokens usersEn?Ne all 9, 993 146, 053 18train 7, 504 109, 040 12test 2, 489 37, 013 6En?Es all 11, 400 140, 738 9train 7, 399 101, 451 6test 4, 001 39, 287 3Zh?En all 994 17, 408 995train 662 11, 677 663test 332 5, 731 332MSA?ARZ all 5, 862 119, 775 7train 4, 800 95, 352 6test 1, 062 24, 423 1Table 1: Total number of tweets, tokens, and Twit-ter user IDs for each language pair.
For each lan-guage pair, the first line represents all data pro-vided to shared task participants.
The second andthird lines represent our train/test data split for theexperiments reported in this paper.
Since Twit-ter users are allowed to delete their tweets, thenumber of tweets and tokens reported in the thirdand fourth columns may be less than the numberof tweets and tokens originally annotated by theshared task organizers.3 Baseline SystemWe model token-level language ID as a sequenceof labels using a linear-chain conditional ran-dom field (CRF) (Lafferty et al., 2001) describedin ?3.1 with the features in ?3.2.3.1 ModelA linear-chain CRF models the conditional proba-bility of a label sequence y given a token sequencex and given extra context ?, as follows:p(y | x,?)
=exp?>?|x|i=1f(x, yi, yi?1,?
)?y?exp?>?|x|i=1f(x, y?i, y?i?1,?
)where ?
is a vector of feature weights, and f isa vector of local feature functions.
We use ?
toexplicitly represent context information necessaryto compute the feature functions described below.In a linear-chain structure, yionly depends onobserved variables x,?
and the neighboring labelsyi?1and yi+1.
Therefore, we can use dynamicprogramming to do inference in run time that isquadratic in the number of unique labels and lin-ear in the sequence length.
We use L-BFGS tolearn the feature weights ?, maximizing the L2-regularized log-likelihood of labeled examples L:``supervised(?)
=cL2||?||22+??x,y?
?Llog p(y | x,?
)After training the model, we use again use dy-namic programming to find the most likely labelsequence, for each token sequence in the test set.3.2 FeaturesWe use the following features in the baseline sys-tem:?
character n-grams (loweredcased tri- and quad-grams)?
prefixes and suffixes of lengths 1, 2, 3 and 4?
unicode page of the first character1?
case (first-character-uppercased vs. all-characters-uppercased vs. all-characters-alphanumeric)?
tweet-level language ID predictions from twooff-the-shelf language identifiers: cld22andldig31http://www.unicode.org/charts/2https://code.google.com/p/cld2/3https://github.com/shuyo/ldig81encodingreconstructionxyi-1 yi yi+1xi-1 xi xi+1 ?
?
?
?Figure 1: A diagram of the CRF autoencoder4 Using Unlabeled DataIn ?3, we learn the parameters of the CRF modelparameters in a standard fully supervised fashion,using labeled examples in the training set.
Here,we attempt to use unlabeled examples to improveour system?s performance in three ways: model-ing unlabeled token sequences in the CRF autoen-coder framework, word embeddings, and wordlists.4.1 CRF AutoencodersA CRF autoencoder (Ammar et al., 2014) consistsof an input layer, an output layer, and a hiddenlayer.
Both input and output layer represent theobserved token sequence.
The hidden layer rep-resents the label sequence.
Fig.
1 illustrates themodel dependencies for sequence labeling prob-lems with a first-order Markov assumption.
Con-ditional on an observation sequence x and side in-formation ?, a traditional linear-chain CRF modelis used to generate the label sequence y. Themodel then generates x?
which represents a recon-struction of the original observation sequence.
El-ements of this reconstruction (i.e., x?i) are then in-dependently generated conditional on the corre-sponding label yiusing simple categorical distri-butions.The parametric form of the model is given by:p(y, x?
| x,?)
=|x|?i=1?x?i|yi?exp?>?|x|i=1f(x, yi?1, yi, i,?
)?y?exp?>?|x|i=1f(x, y?i?1, y?i, i,?
)where ?
is a vector of CRF feature weights, f is avector of local feature functions (we use the samefeatures described in ?3.2), and ?x?i|yiare categor-ical distribution parameters of the reconstructionmodel representing p(x?i| yi).We can think of a label sequence as a low-cardinality lossy compression of the correspond-ing token sequence.
CRF autoencoders explic-itly model this intuition by creating an informationbottleneck where label sequences are required toregenerate the same token sequence despite theirlimited capacity.
Therefore, when only unlabeledexamples U are available, we train CRF autoen-coders by maximizing the regularized likelihoodof generating reconstructions x?, conditional on x,marginalizing values of label sequences y:``unsupervised(?,?)
= cL2||?||22+RDirichlet(?, ?)+??x,x??
?Ulog?y:|y|=|x|p(y, x?
| x)where RDirichletis a regularizer based on a vari-ational approximation of a symmetric Dirichletprior with concentration parameter ?
for the re-construction parameters ?.Having access to labeled examples, it is easy tomodify this objective to learn from both labeledand unlabeled examples as follows:``semi(?,?)
= cL2||?||22+RDirichlet(?, ?)+cunlabeled???x,x??
?Ulog?y:|y|=|x|p(y, x?
| x)+clabeled???x,y?
?Llog p(y | x)We use block coordinate descent to optimizethis objective.
First, we use cemiterations ofthe expectation maximization algorithm to opti-mize the ?-block while the ?-block is fixed, thenwe optimize the ?-block with clbfgsiterations ofL-BFGS (Liu et al., 1989) while the ?-block isfixed.44.2 Unsupervised Word EmbeddingsFor many NLP tasks, using unsupervisedword representations as features improvesaccuracy (Turian et al., 2010).
We useword2vec (Mikolov et al., 2013) to train100?dimensional word embeddings from alarge Twitter corpus of about 20 million tweetsextracted from the live stream, in multiple lan-guages.
We define an additional feature function4An open source efficient c++ imple-mentation of our method can be found athttps://github.com/ldmt-muri/alignment-with-openfst82in the CRF autoencoder model ?4.1 for each ofthe 100 dimensions, conjoined with the label yi.The feature value is the corresponding dimensionfor xi.
A binary feature indicating the absence ofword embeddings is fired for out-of-vocabularywords (i.e., words for which we do not have wordembeddings).
The token-level coverage of theword embeddings for each of the languages ordialects used in the training data is reported inTable 2.4.3 Word List FeaturesWhile some words are ambiguous, many wordsfrequently occur in only one of the two lan-guages being considered.
An easy way to iden-tify the label of such unambiguous words is tocheck whether they belong to the vocabulary ofeither language.
Moreover, named entity recog-nizers typically rely on gazetteers of named enti-ties to improve their performance.
We generalizethe notion of using monolingual vocabularies andgazetteers of named entities to general word lists.Using K word lists {l1, .
.
.
, lK}, when a token xiis labeled with yi, we fire a binary feature that con-joins ?yi, ?(xi?
l1), .
.
.
, ?(xi?
lK)?, where ?
isan indicator boolean function.
We use the follow-ing word lists:?
Hindi and Nepali Wikipedia article titles?
multilingual named entities from the JRCdataset5and CoNLL 2003 shared task?
word types in monolingual corpora in MSA,ARZ, En and Es.?
set difference between the following pairs ofword lists: MSA-ARZ, ARZ-MSA, En-Es, Es-En.Transliteration from Devanagari The Nepali?English tweets in the dataset are romanized.
Thisrenders our Nepali word lists, which are basedon the Devanagari script, useless.
Therefore, wetransliterate the Hindi and Nepali named entitieslists using a deterministic phonetic mapping.
Weromanize the Devanagari words using the IASTscheme.6We then drop all accent marks on thecharacters to make them fit into the 7?bit ASCIIrange.5http://datahub.io/dataset/jrc-names6http://en.wikipedia.org/wiki/International_Alphabet_of_Sanskrit_Transliterationembeddings word listslanguage coverage coverageARZ 30.7% 68.8%En 73.5% 55.7%MSA 26.6% 76.8%Ne 14.5% 77.0%Es 62.9% 78.0%Zh 16.0% 0.7%Table 2: The type-level coverage of annotated dataaccording to word embeddings (second column)and according to word lists (third column), per lan-guage.5 ExperimentsWe compare the performance of five models foreach language pair, which correspond to the fivelines in Table 3.
The first model, ?CRF?
is thebaseline model described in ?3.
The second ?CRF+ Utest?
and the third ?CRF + Uall?
are CRF au-toencoder models (see ?4.1) with two sets of un-labeled data: (1) Utestwhich only includes the testset,7and (2) Uallwhich includes the test set as wellas all tweets by the set of users who contributedany tweets in L. The fourth model ?CRF + Uall+emb.?
is a CRF autoencoder which uses word em-bedding features (see ?4.2), as well as the featuresdescribed in ?3.2.
Finally, the fifth model ?CRF +Uall+ emb.
+ lists?
further adds word list features(see ?4.3).
In all but the ?CRF?
model, we adopt atransductive learning setup.Since the CRF baseline is used as the encodingpart of the CRF autoencoder model, we use thesupervisedly-trained CRF parameters to initializethe CRF autoencoder models.
The categorical dis-tributions of the reconstruction model are initial-ized with discrete uniforms.
We set the weightof the labeled data log-likelihood clabeled= 0.5,the weight of the unlabeled data log-likelihoodcunlabeled= 0.5, the L2regularization strengthcL2= 0.3, the concentration parameter of theDirichlet prior ?
= 0.1, the number of L-BFGSiterations cLBFGS= 4, and the number of EM iter-ations cEM= 4.8We stop training after 50 itera-tions of block coordinate descent.7Utestis potentially useful when the test set belongs to adifferent domain than the labeled examples, which is oftenreferred to as ?domain adaptation?.
However we were unableto test this hypothesis since all the CS annotations we hadaccess to are from Twitter.8Hyper-parameters cL2and ?
were tuned using cross-validation.
The remaining hyper-parameters were not tuned.83config En?Ne MSA?ARZ En?Es Zh?EnCRF 95.2% 80.5% 94.6% 94.9%+Ttest95.2% 80.6% 94.6% 94.9%+Tall95.2% 80.7% 94.6% 94.9%+emb.
95.3% 81.3% 95.1% 95.0%+lists 97.0% 81.2% 96.7% 95.3%Table 3: Token level accuracy results for each ofthe four language pairs.label predicted predictedMSA ARZtrue MSA 93.9% 5.3%true ARZ 32.1% 65.2%Table 4: Confusion between MSA and ARZ in theBaseline configuration.Results.
The CRF baseline results are reportedin the first line in Table 3.
For three languagepairs, the overall token-level accuracy ranges be-tween 94.6% and 95.2%.
In the fourth languagepair, MSA-ARZ, the baseline accuracy is 80.5%which indicates the relative difficulty of this task.The second and third lines in Table 3 show theresults when we use CRF autoencoders with theunlabeled test set (Utest), and with all unlabeledtweets (Uall), respectively.
While semi-supervisedlearning did not hurt accuracy on any of the lan-guages, it only resulted in a tiny increase in accu-racy for the Arabic dialects task.The fourth line in Table 3 extends the CRF au-toencoder model (third line) by adding unsuper-vised word embedding features.
This results inan improvement of 0.6% for MSA-ARZ, 0.5% forEn-Es, 0.1% for En-Ne and Zh-En.The fifth line builds on the fourth line by addingword list features.
This results in an improvementof 1.7% in En-Ne, 1.6% in En-Es, 0.4% in Zh-En,and degradation of 0.1% in MSA-ARZ.Analysis and Discussion The baseline perfor-mance in the MSA-ARZ task is considerablylower than those of the other tasks.
Table 4 illus-trates how the baseline model confuses lang1 andlang2 in the MSA-ARZ task.
While the baselinesystem correctly labels 93.9% of MSA tokens, itonly correctly labels 65.2% of ARZ tokens.Although the reported semi-supervised resultsdid not significantly improve on the CRF baseline,more work needs to be done in order to concludethese results:lang.
pair |Utest| |Uall| |L|En?Ne 2489 6230 7504MSA?ARZ 1062 2520 4800Zh?En 332 332 663En?Es 4001 7177 7399Table 5: Number of tweets inL, UtestandUallusedfor semi-supervised learning of CRF autoencodersmodels.?
Use an out-of-domain test set where some adap-tation to the test set is more promising.?
Vary the number of labeled examples |L| andthe number of unlabeled examples |U|.
Table 5gives the number of labeled and unlabeled ex-amples used for training the model.
It is pos-sible that semi-supervised learning would havebeen more useful with a smaller |L| and a larger|U|.?
Tune clabeledand cunlabeled.?
Split the parameters ?
into two subsets: ?labeledand ?unlabeled; where ?labeledare the parameterswhich have a non-zero value for any input x inL and ?unlabeledare the remaining parameters in?
which only have non-zero values with unla-beled examples but not with the labeled exam-ples.?
Use a richer reconstruction model.?
Reconstruct a transformation of the token se-quences instead of their surface forms.?
Train a token-level language ID model trainedon a large number of languages, as opposed todisambiguating only two languages at a time.Word embeddings improve the results for alllanguage pairs, but the largest improvement is inMSA-ARZ and En-Es.
Looking into the word em-beddings coverage of those languages (i.e., MSA,ARZ, Es, En in Table 2), we find that they are bet-ter covered than the other languages (Ne, Zh).
Weconclude that further improvements on En-Ne andZh-En may be expected if they are better repre-sented in the corpus used to learn word embed-dings.As for the word lists, the largest improvementwe get is the romanized word lists of Nepali,which have a 77.0% coverage and improve theaccuracy by 1.7%.
This shows that our translit-erated word lists not only cover a lot of tokens,and are also useful for language ID.
The Spanish84Config lang1 lang2 ne+lists 84.1% 76.5% 73.7%-lists 84.2% 77.1% 71.5%Table 6: F?Measures of two Arabic configura-tions.
lang1 is MSA.
lang2 is ARZ.word lists also have a wide coverage, improvingthe overall accuracy by 1.6%.
The overall accu-racy of the Arabic dialects slightly degrades withthe addition of the word lists.
Closer inspectionin table 6 reveals that it improves the F?Measureof the named entities at the expense of both MSA(lang1) and ARZ (lang2).6 Related WorkPrevious work on identifying languages in a mul-tilingual document includes (Singh and Gorla,2007; King and Abney, 2013; Lui et al., 2014).Their goal is generally more about identifying thelanguages that appear in the document than intra?sentential CS points.Previous work on computational models ofcode?switching include formalism (Joshi, 1982)and language models that encode syntactic con-straints from theories of code?switching, such as(Li and Fung, 2013; Li and Fung, 2014).
Theserequire the existence of a parser for the languagesunder consideration.
Other work on predictionof code?switching points, such as (Elfardy et al.,2013; Nguyen and Dogruoz, 2013) and ours, donot depend upon such NLP infrastructure.
Both ofthe aforementioned use basic character?level fea-tures and dictionaries on sequence models.7 ConclusionWe have shown that a simple CRF baseline witha handful of feature templates obtains strong re-sults for this task.
We discussed three methodsto improve over the supervised baseline using un-labeled data: (1) modeling unlabeleld data usingCRF autoencoders, (2) using pre-trained word em-beddings, and (3) using word list features.We show that adding word embedding featuresand word lists features is useful when they havegood coverage of words in a data set.
While mod-est improvements are observed due to modelingunlabeled data with CRF autoenocders, we iden-tified possible directions to gain further improve-ments.While bilingual disambiguation was a good firststep for identifying code switching, we suggest areformulation of the task such that each label cantake on one of many languages.AcknowledgmentsWe thank Brendan O?Connor who helped assem-ble the Twitter dataset.
We also thank the work-shop organizers for their hard work, and the re-viewers for their comments.
This work wassponsored by the U.S. Army Research Labora-tory and the U.S. Army Research Office undercontract/grant number W911NF-10-1-0533.
Thestatements made herein are solely the responsibil-ity of the authors.ReferencesWaleed Ammar, Chris Dyer, and Noah A. Smith.
2014.Conditional random field autoencoders for unsuper-vised structured prediction.
In Proc.
of NIPS.Heba Elfardy, Mohamed Al-Badrashiny, and MonaDiab.
2013.
Code switch point detection in ara-bic.
In Natural Language Processing and Informa-tion Systems, pages 412?416.
Springer.John J. Gumperz.
1982.
Discourse Strategies.
Studiesin Interactional Sociolinguistics.
Cambridge Univer-sity Press.Aravind K. Joshi.
1982.
Processing of sentences withintra-sentential code-switching.
In Proceedings ofthe 9th Conference on Computational Linguistics -Volume 1, COLING ?82, pages 145?150, Czechoslo-vakia.
Academia Praha.Ben King and Steven Abney.
2013.
Labeling the lan-guages of words in mixed-language documents us-ing weakly supervised methods.
Proceedings of the2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: Hu-man Language Technologies, pages 1110?1119.
As-sociation for Computational Linguistics.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: Prob-abilistic models for segmenting and labeling se-quence data.
In Proc.
of ICML.Ying Li and Pascale Fung.
2013.
Improved mixed lan-guage speech recognition using asymmetric acous-tic model and language model with code-switch in-version constraints.
In Acoustics, Speech and Sig-nal Processing (ICASSP), 2013 IEEE InternationalConference on, pages 7368?7372, May.Ying Li and Pascale Fung.
2014.
Code switch lan-guage modeling with functional head constraint.
InAcoustics, Speech and Signal Processing (ICASSP),2014 IEEE International Conference on, pages4913?4917, May.85D.
C. Liu, J. Nocedal, and C. Dong.
1989.
On the lim-ited memory bfgs method for large scale optimiza-tion.
Mathematical Programming.Marco Lui and Timothy Baldwin.
2014.
Accuratelanguage identification of twitter messages.
In Pro-ceedings of the 5th Workshop on Language Analysisfor Social Media (LASM), pages 17?25, Gothenburg,Sweden, April.
Association for Computational Lin-guistics.Marco Lui, Han Jey Lau, and Timothy Baldwin.
2014.Automatic detection and language identification ofmultilingual documents.
Transactions of the Asso-ciation of Computational Linguistics, 2:27?40.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word represen-tations in vector space.
In Proc.
of ICLR.Dong Nguyen and Seza A. Dogruoz.
2013.
Word levellanguage identification in online multilingual com-munication.
Proceedings of the 2013 Conference onEmpirical Methods in Natural Language Processing,pages 857?862.
Association for Computational Lin-guistics.Anil Kumar Singh and Jagadeesh Gorla.
2007.
Identi-fication of languages and encodings in a multilingualdocument.
In Building and Exploring Web Corpora(WAC3-2007): Proceedings of the 3rd Web as Cor-pus Workshop, Incorporating Cleaneval, volume 4,page 95.Thamar Solorio, Elizabeth Blair, Suraj Maharjan, SteveBethard, Mona Diab, Mahmoud Gonheim, AbdelatiHawwari, Fahad AlGhamdi, Julia Hirshberg, AlisonChang, and Pascale Fung.
2014.
Overview for thefirst shared task on language identification in code-switched data.
In Proceedings of the First Workshopon Computational Approaches to Code-Switching.Joseph Turian, Lev Ratinov, and Yoshua Bengio.
2010.Word representations: A simple and general methodfor semi-supervised learning.
In Proceedings of the48th Annual Meeting of the Association for Com-putational Linguistics, ACL ?10, pages 384?394,Stroudsburg, PA, USA.
Association for Computa-tional Linguistics.86
