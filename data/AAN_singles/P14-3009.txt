Proceedings of the ACL 2014 Student Research Workshop, pages 64?70,Baltimore, Maryland USA, June 22-27 2014.c?2014 Association for Computational LinguisticsMulti-Document Summarization Using Distortion-Rate RatioUlukbek AttokurovDepartment of Computer EngineeringIstanbul Technical Universityattokurov@itu.edu.trUlug BayazitDepartment of Computer EngineeringIstanbul Technical Universityulugbayazit@itu.edu.trAbstractThe current work adapts the optimal treepruning algorithm(BFOS) introduced byBreiman et al(1984) and extended byChou et al(1989) to the multi-documentsummarization task.
BFOS algorithm isused to eliminate redundancy which is oneof the main issues in multi-document sum-marization.
Hierarchical AgglomerativeClustering algorithm(HAC) is employedto detect the redundancy.
The tree de-signed by HAC algorithm is successivelypruned with the optimal tree pruning al-gorithm to optimize the distortion vs. ratecost of the resultant tree.
Rate parameter isdefined to be the number of the sentencesin the leaves of the tree.
Distortion is thesum of the distances between the represen-tative sentence of the cluster at each nodeand the other sentences in the same clus-ter.
The sentences assigned to the leaves ofthe resultant tree are included in the sum-mary.
The performance of the proposedsystem assessed with the Rouge-1 metricis seen to be better than the performanceof the DUC-2002 winners on DUC-2002data set.1 IntroductionNowadays, the massive amount of informationavailable in the form of digital media over the in-ternet makes us seek effective ways of accessingthis information.
Textual documents, audio andvideo materials are uploaded every second.
For in-stance, the number of Google?s indexed web pageshas exceeded 30 billion web pages in the last twoyears.
Extraction of the needed information froma massive information pool is a challenging task.The task of skimming all the documents in theirentirety before deciding which information is rel-evant is very time consuming.One of the well known and extensively studiedmethods for solving this problem is summariza-tion.
Text summarization produces a short ver-sion of a document that covers the main topics init (Mani and Hahn, 2000).
It enables the readerto determine in a timely manner whether a givendocument satisfies his/her needs or not.A single document summarization system pro-duces a summary of only one document whereasa multi-document summarization system producesa summary based on multiple documents on thesame topic.
Summarization systems can also becategorized as generic or query-based .
A genericsummary contains general information about par-ticular documents.
It includes any informationsupposed to be important and somehow linked tothe topics of the document set.
In contrast, a querybased summary comprises information relevant tothe given query.
In this case, query is a rule ac-cording to which a summary is to be generated.Summarization systems can be also classifiedas extractive or abstractive.
In extractive systems,a summary is created by selecting important sen-tences from a document.
Here, only sentencescontaining information related to the main topicsof the document are considered to be important.These sentences are added to the summary with-out any modification.
On the other hand, abstrac-tive systems can modify the existing sentences oreven generate new sentences to be included in thesummary.
Therefore, abstractive summarizationis typically more complex than extractive summa-rization.The main goal in multi-document summariza-tion is redundancy elimination.
Since the docu-ments are related to the same topics, similar textunits(passages, sentences etc.)
are encounteredfrequently in different documents.
Such text unitsthat indicate the importance of the topics discussedwithin them should be detected in order to re-duce the redundancy.
Some of the well-known ap-64proaches that address this problem are briefly ex-plained in the following section.Although much work has been done to elim-inate the redundancy in multi-document summa-rization, the problem is still actual and addressedin the current work as well.
The current workproposes to integrate the generalized BFOS algo-rithm (Breiman et al, 1984) adopted by Chou et.al(1989) for pruned tree structured quantizer designwith the HAC (Hierarchical Agglomerative Clus-tering) algorithm.
The two main parameters (dis-tortion and rate) in the latter work are adopted tothe multi-document summarization task.
Distor-tion can be succinctly defined as the informationloss in the meaning of the sentences due to theirrepresentation with other sentences.
More specif-ically, in the current context, distortion contribu-tion of a cluster is taken to be the sum of the dis-tances between the vector representations of thesentences in the cluster and representative sen-tence of that cluster.
Rate of a summary is de-fined to be the number of sentences in the sum-mary, but more precise definitions involving wordor character counts are also possible.
BFOS basedtree pruning algorithm is applied to the tree builtwith the HAC algorithm.
HAC algorithm is usedfor clustering purposes since BFOS algorithm getstree structured data as an input.
It is found thatthe suggested approach yields better results interms of the ROUGE-1 Recall measure (Lin etal., 2003) when compared to 400 word extractivesummaries(400E) included in DUC-2002 data set.Also, the results with the proposed method arehigher than the ones obtained with the best sys-tems of DUC-2002 in terms of sentence recall andprecision(Harabagiu, 2002; Halteren, 2002).2 Related WorksTerm frequency (Luhn, 1958), lexical chains(Barzilay and Elhadad, 1997), location of the sen-tences (Edmundson, 1969) and the cue phrases(Teufel et al, 1997) are used to determine the im-portant lexical units.
Goldstein et al (2000) pro-posed a measure named Maximal Marginal Rel-evance which assigns a high priority to the pas-sages relevant to the query and has minimal sim-ilarity to the sentences in the summary.
Radevet al (2001) developed a system called MEADbased on the centroid of the cluster.
The wordsthat are most relevant to the main topics are in-cluded in the centroid.
Lin et al designed astatistic-based summarization system (Summarist)which incorporated NLP(Natural Language Pro-cessing) and IR(Information Retrieval) methods.LSA(Latent Semantic Analysis) (Landauer et al,1998) has also been used extensively in recentyears for multi-document summarization.
By ap-plying SVD(Singular Value Decomposition) to theterm-document matrix, it determines the most im-portant topics and represents the term and docu-ments in the reduced space (Murray et al, 2005;Steinberger and Jezek , 2004; Geiss, 2011).
RachitArora et al (2008) combined LDA(Latent Dirich-let Allocation) and SVD.
In this approach, LDA isused to detect topics and SVD is applied to selectthe sentences representing these topics.Clustering of the sentences has also been usedto determine the redundant information.
In thisapproach, the sentences are first clustered.
Thesentences in each cluster share common informa-tion about the main topics of the documents to besummarized.
Then a sentence is selected (Radevet al, 2004) or generated (McKeown et al, 1999)from each cluster that represents the sentences inthe cluster.
Finally, selected sentences are addedto the summary until a predetermined length is ex-ceeded (Aliguliyev, 2006; Hatzivassiloglou et al,1999; Hatzivassiloglou et al, 2001).3 Background3.1 Generalized BFOS AlgorithmLet us assume that we have a tree T with the set ofleaves?T .
Also let us denote a sub-tree of T rootedat any node of T as S. The leaves of the sub-treesmay happen to be the inner nodes of T .
If the rootnode of the sub-tree S is not identical to the rootnode of T and the set of leaves?S is a sub-set of?T then S is called a branch.
But if the sub-tree Sis rooted at the root node of T then S is named apruned sub-tree of T .
Function defined on the treeT and on any sub-tree S is called a tree functional.Monotonic tree functional is a class of functionalwhere it increases or decreases depending on thetree size.
In our case, tree size is the number of thenodes of T .Two main tree functionals(u1 and u2) need tobe defined in the generalized BFOS algorithm.They are adapted to the problem under considera-tion.
In regression trees, u1 is the number of theleaves and u2 is the mean squared distortion er-ror.
In TSVQ(Tree Structured Vector Quantiza-tion), u1 and u2 are the length of the code and65the expected distortion, respectively.
In the cur-rent context, distortion(D) and rate(R) defined inthe next section are used as the tree functionals u1and u2.As shown in Chou et al, the set of distortion andrate points of the pruned sub-trees of T generate aconvex hull if distortion is an increasing and rateis a decreasing function.
Also it is stated that ifthe tree T is pruned off until the root node remains,then it is possible to generate the sub-trees whichcorrespond to the vertices on the lower boundaryof the convex hull.
Thus it is sufficient to considerthe sub-trees corresponding to the vertices of theboundary to trade off between rate and distortion.A parameter ?
= ?
?D?Rmay be used to locatethe vertices on the lower boundary of the convexhull.
?D and ?R indicate the amount of distor-tion increase and rate decrease when branch sub-tree S is pruned off.
It can be shown that a stepon the lower boundary can be taken by pruning offat least one branch sub-tree rooted at a particularinner node.
The ?
value of this sub-tree is mini-mal among all the other branch sub-trees rooted atvarious inner nodes of T , because it is a slope ofthe lower boundary.
At each pruning iteration, thealgorithm seeks the branch sub-tree rooted at aninner node with the minimal lambda and prunesit off the tree.
After each pruning step, the in-ner node at which the pruned branch sub-tree isrooted becomes a leaf node.
The pruning itera-tions continue until the root node remains or thepruned sub-tree meets a certain stopping criterion.4 The Proposed Summarization SystemIn the current work, BFOS and HAC algorithmwere incorporated to the multi-document sum-marization system.
Generalized version of theBFOS algorithm discussed in the work of Chouet al (1989) with previous applications to TSVQ,speech recognition etc.
was adapted for the pur-pose of pruning the large tree designed by theHAC algorithm.
Generalized BFOS algorithmwas preferred in the current context because it isbelieved that the generated optimal trees yield thebest trade-off between the semantic distortion andrate (the summary length in terms of number ofsentences).The proposed system consists of the followingstages: preprocessing, redundancy detection, re-dundancy elimination and the summary genera-tion.In preprocessing stage, the source documentsare represented in the vector space.
Towards thisend, the sentences are parsed, stemmed and a fea-ture set is created (terms (stems or words, n-gramsetc.)
that occur in more than one document areextracted).
The sentences of the document set arethen represented by a sentence X term matrix withn columns and m rows, where n is the number ofthe sentences and m is the number of the terms inthe feature set.
TF-IDF is used to determine thevalues of the matrix elements.
TF-IDF assigns avalue according to the importance of the terms inthe collection of the sentences.
If the term t occursfrequently in the current document but the oppo-site is true for other documents then tf-idf value oft is high.TF ?
IDF = TF ?
logNDF(1)where TF is the term frequency, DF is the docu-ment frequency and N is the number of sentences.Term frequency is the number of the occurrencesof the term in the sentence.
Document frequencyis the number of the sentences in which the term isfound.Redundancy detection is facilitated by applyingthe Hierarchical Agglomerative Clustering(HAC)algorithm.
Initially, individual sentences are con-sidered to be singletons in the HAC algorithm.The most similar clusters are then successivelymerged to form a new cluster that contains theunion of the sentences in the merged clusters.
Ateach step, a new (inner) node is created in the treeas the new cluster appears and contains all the sen-tences in the union of the merged clusters.
HACmerge operations continue until a single cluster re-mains.
The tree built after HAC operation is re-ferred to as the HAC tree.The third stage is the redundancy elimination.To this end, generalized BFOS algorithm dis-cussed previously is applied to the HAC tree.
Inorder to adapt the generalized BFOS algorithm tothe current context, distortion contribution of eachcluster (node) is defined as follows:D =?s?clusterd(rs, s) (2)where d is the distance between the representativesentence(rs) and a sentence(s) in the cluster.By definition, the distortion contribution of eachleaf node of the HAC tree is zero.66Rate is defined to be the number of sentencesin the leaves of the tree.
A branch sub-tree isremoved at each pruning step of the generalizedBFOS algorithm.
Correspondingly, the sentencesat the leaf nodes of the pruned branch subtree areeliminated.
As a result, the rate decreases to thenumber of leaf nodes remaining after pruning.The centroid of the cluster can be used as therepresentative sentence of the cluster.
Centroidcan be constituted of the important (with TF-IDFvalues exceeding a threshold) words of the cluster(Radev et al, 2004) or can be generated using Nat-ural language processing techniques (McKeown etal., 1999).
In the current work, the simpler ap-proach of selecting the sentence from the clusteryielding the minimal distortion as the representa-tive sentence is employed.?
parameter is used to determine the branchsub-trees that are successively pruned.
In eachpruning step, the branch sub-tree with minimum?
is identified to minimize the increase in totaldistortion(?D) per discarded sentence(?R).In accordance with the definition of rate givenabove, ?R is the change in the number of sen-tences in the summary before and after the prun-ing of the branch sub-tree.
It also equals to thenumber of pruned leaf nodes, because rate equalsto the number of the sentences stored in the leafnodes of the current tree.
For instance, let us as-sume that the number of sentences before pruningis 10 and a sub-tree A is cut off.
If A has 4 leafnodes, than 3 of them is eliminated and one is leftto represent the cluster of sentences correspondingto the sub-tree A.
Since 3 leaf nodes are removedand each leaf node is matched to the certain sen-tence, the current rate equals to 7.
The increase intotal distortion is written as?D = Dpost?Dprev(3)where Dprevis set equal to the sum of distortionsin the leaves of the tree before pruning and Dpostis set equal to the sum of distortions in the leavesof the tree after pruning.The application of the generalized BFOS algo-rithm to the HAC tree can be recapped as follows.At the initial step, a representative sentence is se-lected for each inner node and ?
is determined foreach inner node.
At each generic pruning step, thenode with the minimum lambda value is identified,the sub-tree rooted at that node is pruned, the rootnode of the sub-tree is converted to a leaf node.After each pruning step, the ?
values of the ances-tor nodes of this new leaf node are updated.
Wesummarize the generalized BFOS algorithm witha pseudocode in Algorithm 1.Algorithm 1: PRUNING THE TREE.
Prunes atree T created by using Hierarchical Agglom-erative Clustering AlgorithmInput: A tree T produced by usingHierarchical Clustering AlgorithmOutput: Optimal sub-tree O obtained bypruning T1 For each leaf node,???,distortion(D)?
02 For each inner node calculate ?
=?D?R,where ?D and ?R are change indistortion(D) and rate(R) respectively3 rate(R)?
the number of the leaves of T4 while the number of the nodes > 1 do5 find a node A with minimum ?
valueamong the inner nodes6 prune the sub-tree S rooted at the node A7 convert the pruned inner node A to theleaf node containing the representativesentence of the sub-tree S8 update the ancestor nodes of the node A:update ?D, ?R and ?9 update rate(R)10 return OA summary of desired length can be created byselecting a threshold based on rate (the number ofremaining sentences after pruning, the number ofleaf nodes of the pruned tree).
Another possibil-ity for the choice of the stopping criterion maybe based on the ?
parameter which monotonicallyincreases with pruning iterations.
When a largeenough ?
value is reached, it may be assumed thatshortening the summary further eliminates infor-mative sentences.The proposed method of summarization has afew drawbacks.
The main problem is that thepruning algorithm is highly dependent on the dis-tortion measure.
If the distortion measure is notdefined appropriately, the representative sentencecan be selected incorrectly.
Another issue is theinclusion of the irrelevant sentences into the sum-mary.
This problem may occur if the sentencesremaining after pruning operation are included inthe summary without filtering.675 EvaluationThe testing of the system performed on DUC-2002data set (Document Understanding Conference,2002) since the proposed system is designed toproduce a generic summary without specified in-formation need of users or predefined user profile.This data set contains 59 document sets.
For eachdocument set extraction based summaries with thelength 200 and 400 words are provided.
Documentsets related to the single event are used for testingpurposes.Evaluation of the system is carried out usingROUGE package (Lin C, 2004).
Rouge is a sum-mary evaluation approach based on n-gram co-occurrence , longest common subsequence andskip bigram statistics (Lin et al, 2003).
The per-formance of the summarizing system is measuredwith Rouge-1 Recall, Rouge-1 Precision and F1measure(Table 1).
400E stood for the extrac-tive 400 word summary provided by DUC-2002data set.
It was created manually as an extrac-tive summary for evaluation purposes.
Candidatesummary(CS) was produced by the proposed sys-tem.
Both summaries were compared against a200 word abstractive summary included in DUC-2002 data set.
200 word abstractive summarywas considered as the model summary in ROUGEpackage.
As shown, the summary of the proposedsystem gives better results in Rouge-1 recall mea-sure.
However, the highest precision is achievedin the 400E summary.
Generally, the proposedsystem outperforms the 400E summary, since F1-score which takes into account precision and recallis higher.In addition, the performance of the system wascompared with the best systems(BEST) of DUC-2002(Halteren, 2002; Harabagiu, 2002)(Table 2).The results of the best systems(BEST) in termsof sentence recall and sentence precision are pro-vided by DUC-2002.
Sentence recall and sentenceprecision of the candidate summary(produced bythe proposed system) were calculated by using 400word extract based summary(provided by DUC-2002) and a candidate summary.
Sentence recalland sentence precision are defined as follows:sentence recall =MB(4)sentence precision =MC(5)where M is the number of the sentences includedsummary P R F1400E 0.313 0.553 0.382candidate 0.3 0.573 0.394Table 1: ROUGE-1 Results.
Candidate sum-mary(produced by the proposed system) and 400Esummary provided by DUC 2002 are comparedwith 200 word abstract created manually.in both of the summaries(a candidate and 400word summary provided by DUC-2002(400E)),C,B are the number of the sentences in the can-didate summary and in a 400E summary, respec-tively.summary SentencePrecisionSentenceRecallBEST 0.271 0.272candidate 0.273 0.305Table 2: Results.
The best systems of DUC-2002results and the results of the proposed system.
Pro-posed system is compared with 400 word extractsprovided by DUC-2002.As shown, the proposed system performs bet-ter than the best systems of DUC-2002 in termsof sentence recall.
We are more interested in sen-tence recall because it states the ratio of the impor-tant sentences contained in the candidate summaryif the sentences included in the 400E summary aresupposed to be important ones.
Furthermore, sen-tence precision is affected from the length of thecandidate summary.Figure 1: The relationship between distortion andrate.
While rate is decreasing distortion is increas-ing.Summarizing the text can be considered as thecompression of the text.
Thus it is possible to de-pict the graph of dependence of distortion on rate(Figure 1).
The graph shows that as rate decreasesdistortion increases monotonically.
Therefore, ifdistortion is assumed to be the information loss oc-68curred when the original text is summarized thenthe summaries of different quality can be producedby restricting rate (the number of sentences).Another graph shows the change of the lambdavalue(Figure 2).
The iteration number of the prun-ing is on X axis and lambda value is on Y one.
If ?value of the pruned points are sorted in ascendingorder and then the graph of ordered ?
values is de-picted according to their order then the graph iden-tical to the one shown below is obtained(Figure 2).This indicates that the node with minimal lambdavalue is selected in each iteration.
Consequently,the sentences are eliminated so that increase in dis-tortion is minimal for decrease in rate.Figure 2: ?
value of the pruned node.
The changeof ?
value has upward tendency.All in all, the quantitative analyses show that theproposed system can be used as one of the redun-dancy reduction methods.
However, in order toachieve the good results, the parameters of BFOSalgorithm have to be set appropriately.6 ConclusionIn this paper , the combination of tree pruning andclustering is explored for the purpose of multi-document summarization.
Redundancy in the textdetected by the HAC algorithm is eliminated bythe generalized BFOS algorithm.
It is shown thatif the parameters(distortion and rate) are set prop-erly, generalized BFOS algorithm can be used toreduce the redundancy in the text.
The depictedgraph (Figure 1) shows that the proposed defi-nitions of distortion and rate are eligible for themulti-document summarization purpose.The performance evaluation results in terms ofROUGE-1 metric suggest that the proposed sys-tem can perform better with additional improve-ments (combining with LSI).
Also it is stated thatdistance measure selection and noisy sentence in-clusion have significance impact on the summa-rization procedure.Future research will deal with the abstraction.
Anew sentence will be created(not extracted) whentwo clusters are merged.
It will represent the clus-ter of sentences as well as summarize the othersentences in the same cluster.AcknowledgmentsWe thank Google for travel and conference sup-port for this paper.ReferencesAliguliyev R. 2006.
A Novel Partitioning-Based Clus-tering Method and Generic Document Summariza-tion.
In WI-IATW 06: Proceedings of the 2006IEEE/WIC/ACM international conference on WebIntelligence and Intelligent Agent Technology,pages626?629,Washington,DC, USA.Arora R. and Ravindran B.
2008.
Latent Dirichlet Al-location Based Multi-Document Summarization.
InProceedings of the Second Workshop on Analyticsfor Noisy Unstructured Text Data (AND 2008),91-97.Barzilay R. and Elhadad M. 1997.
Using LexicalChains for Text Summarization.
In Proceedings ofthe ACL/EACL?97 Workshop on Intelligent ScalableText Summarization,pages 10-17.Barzilay R. 2003.
Information fusion for multi-document summarization: Paraphrasing and gener-ation, PhD thesis, DigitalCommons@Columbia.Breiman L., Friedman J.H., Olshen R.A., andStone C.J.
1984.
Classification and RegressionTrees.
The Wadsworth Statistics/Probability Se-ries,Belmont, CA: Wadsworth.Chou A. Philip, Tom Lookabaugh, and Gray M.Robert.
1989.
Optimal Pruning with Applica-tions to Tree-Structured Source Coding and Model-ing.
IEEE transactions on information theory, vol-ume 35, no 2.DUC?2002.
2002.
Document Understanding Confer-ence.Edmundson H. P. 1969.
New methods in automaticextracting.
Journal of the ACM,16:264-285.Goldstein J., Mittal V., Carbonell J., and Kantrowitz M.2000.
Multi-document summarization by sentenceextraction.
In Proceedings of the ANLP/NAACLWorkshop on Automatic Summarization,pages 40-48.H.
van Halteren.
2002.
Writing style recognition andsentence extraction.
In Proceedings of the workshopon automatic summarization,pages 66?70.Harabagiu S.M.
and Lacatusu F. 2002.
Generatingsingle and multi-document summaries with gistex-ter.
In Proceedings of the workshop on automaticsummarization,pages 30?38.69Hatzivassiloglou V., Klavans J. L., Holcombe M.L., Barzilay R., Kan M.-Y., and McKeown K. R.1999.
Detecting text similarity over short pas-sages: Exploring Linguistic Feature Combinationsvia Machine Learning.
In Proceedings of the 1999Joint SIGDAT Conference on empirical Methods inNatural Language Processing and very large cor-pora,pages 203-212.
College Park, MD, USA.Hatzivassiloglou V., Klavans J. L., Holcombe M. L.,Barzilay R., Kan M.-Y., and McKeown K. R. 2001.SIMFINDER: A Flexible Clustering Tool for Sum-marization.
In NAACL Workshop on AutomaticSummarization,pages 41-49.
Pittsburgh, PA, USA.Hahn U. and Mani I.
2000.
Computer.
Thechallenges of automatic summarization.
IEEEComputer,33(11),29?36.Hovy E. and Lin C.Y.
1999.
Automated Text Sum-marization in SUMMARIST.
Mani I and May-bury M (eds.
), Advances in Automatic Text Summa-rization,pages 81?94.
The MIT Press.Johanna Geiss.
2011.
Latent semantic sentence clus-tering for multi-document summarization, PhD the-sis.
Cambridge University.
?Towards Multidocument Summarization by Refor-mulation: Progress and Prospects?,Kathleen McKeown, Judith Klavans, Vasilis Hatzivas-siloglou, Regina Barzilay, Eleazar Eskin.
1999.
To-wards Multidocument Summarization by Reformu-lation: Progress and Prospects.
In Proceedings ofAAAI,Orlando, Florida.Landauer T.K., Foltz P.W., and Laham D. 1998.
In-troduction to Latent Semantic Analysis.
DiscourseProcesses,25,pages 259?284.Lin C.Y.
and Hovy E. 2003.
Automatic Evaluationof Summaries Using N-gram Co-occurrence Statis-tics.
In North American Chapter of the Associationfor Computational Linguistics on Human LanguageTechnology (HLTNAACL- 2003),pages 71-78.Lin C?Y.
2004.
Rouge: A package for automaticevaluation of summaries.
In Proceedings of Work-shop on Text Summarization Branches Out, Post-Conference Workshop of ACL 2004.Luhn H.P.
1958.
The Automatic Creation ofLiterature Abstracts.
IBM Journal of ResearchDevelopment,2(2):159-165.Murray G., Renals S., and Carletta J.
2005.
Extrac-tive summarization of meeting recordings.
In Pro-ceedings of the 9th European Conference on SpeechCommunication and Technology.Radev D. R., Jing H., and Budzikowska M. 2000.Centroid-based summarization of multiple docu-ments: sentence extraction, utility-based evaluation,and user studies, pages 21-29.
In ANLP/NAACLWorkshop on Summarization, Morristown, NJ, USA.Radev R., Blair?goldensohn S,Zhang Z.
2001.
Exper-iments in Single and Multi-Docuemtn Summariza-tion using MEAD.
InFirst Document Under- stand-ing Conference,New Orleans,LA.Radev D. R., Jing H., Stys M., and Tam D.2004.
Centroid-based summarization of mul-tiple documents.
Information Processing andManagement,40:919-938.Scott Deerwester, Dumais T. Susan, Furnas W George ,Landauer Thomas K., and Richard Harshman.
1990.Indexing by latent semantic analysis.
Journal of theAmerican Society of Information Science,41(6):391-407.Steinberger J. and Jezek K. 2004.
Using Latent Se-mantic Analysis in Text Summarization and Sum-mary Evaluation.
Proceedings of ISIM ?04, pages93-100.Teufel, Simone, and Marc Moens.
1997.
Sentenceextraction as a classification task.
ACL/EACLworkshop on Intelligent and scalable Textsummarization,58-65.70
