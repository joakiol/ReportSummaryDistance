Can Subcategorisation Probabil it ies Help a Statist ical Parser?John Carroll and Guido MinnenSchool of Cognitive and Computing SciencesUniversity of Sussex, Brighton, BN1 9QH, UK{j ohnca ,  gu idomi}@cogs ,  susx.
ac.
ukTed BriscoeComputer Laboratory, University of CambridgePembroke Street, Cambridge CB2 3QG, UKejb,~cl, cam.
ac.
ukAbst ractResearch into the automatic acquisition of lex-ical information from corpora is starting toproduce large-scale computational lexicons con-taining data on the relative frequencies of sub-categorisation alternatives for individual verbalpredicates.
However, the empirical question ofwhether this type of frequency information canin practice improve the accuracy of a statisti-cal parser has not yet been answered.
In thispaper we describe an experiment with a wide-coverage statistical grammar and parser for En-glish and subcategorisation frequencies acquiredfrom ten million words of text which shows thatthis information can significantly improve parseaccuracy 1 .1 IntroductionRecent work on the automatic acquisition oflexical information from substantial amounts ofmachine-readable text (e.g.
Briscoe & Carroll,1997; Gahl, 1998; Carroll & Rooth, 1998) hasopened up the possibility of producing large-scale computational lexicons containing dataon the relative frequencies of subcategorisa-tion alternatives for individual verbal predi-cates.
However, although Resnik (1992), Sch-abes (1992), Carroll & Weir (1997) and othershave proposed 'lexicalised' probabilistic gram-mars to improve the accuracy of parse rank-~This work was funded by UK EPSRC projectGR/L53175 'PSET: Practical Simplification of EnglishText', CEC Telematics Applications Programme projectLE1-2111 'SPARKLE: Shallow PARsing and Knowledgeextraction for Language Engineering', and by an EPSRCAdvanced Fellowship to the first author.
Some of thework was carried out while the first author was a visitorat the Tanaka Laboratory, Department of Computer Sci-ence, Tokyo Institute of Technology, and at CSLI, Stan-ford University; the author wishes to thank researchersat these institutions for many stimulating conversations.ing, no wide-coverage parser has yet been con-structed which explicitly incorporates probabil-ities of different subcategorisation alternativesfor individual predicates.
It is therefore an openquestion whether this type of information canactually improve parser accuracy in practice.In this paper we address this issue, describingan experiment with an existing wide-coveragestatistical grammar and parser for English (Car-roll & Briscoe, 1996) in conjunction with sub-categorisation frequencies acquired from 10 mil-lion words of text from the British NationalCorpus (BNC; Leech, 1992).
Our results showconclusively that this information can improveparse accuracy.2 Background2.1 Subcategor isat ion Acquis i t ionSeveral substantial machine-readable subcate-gorisation dictionaries exist for English, eitherbuilt semi-automatically from machine-readableversions of conventional learners' dictionaries,or manually by (computational) linguists (e.g.the Alvey NL Tools (ANLT) dictionary, Bogu-raev et al (1987); the COMLEX Syntax dic-tionary, Grishman, Macleod & Meyers (1994)).However, since these efforts were not carried outin tandem with rigorous large-scale classifica-tion of corpus data, none of the resources pro-duced provide useful information on the relativefrequency of different subcategorisation frames.Systems which are able to acquire a smallnumber of verbal subcategorisation classes au-tomatically from corpus text have been de-scribed by Brent (1991, 1993), and Ushiodaet al (1993).
Ushioda et al also derive rel-ative subcategorisation frequency informationfor individual predicates.
In this work theyutilise a part-of-speech (PoS) tagged corpus andfinite-state NP parser to recognise and calculate118the relative frequency of six subcategorisationclasses.
They report that for 32 out of 33 verbstested their system correctly predicts the mostfrequent class, and for 30 verbs it correctly pre-dicts the second most frequent class, if there wasone.Manning (1993) reports a larger experiment,also using a PoS tagged corpus and a finite-stateNP parser, attempting to recognise sixteen dis-tinct complementation patterns--although notwith relative frequencies.
In a comparison be-tween entries for 40 common verbs acquiredfrom 4.1 million words of text and the entriesgiven in the Ozford Advanced Learner's Dictio-nary off Current English (Hornby, 1989) Man-ning's system achieves a precision of 90% and arecall of 43%.Gahl (1998) presents an extraction tool foruse with the BNC that is able to create sub-corpora containing different subcategorisationframes for verbs, nouns and adjectives, giventhe frames expected for each predicate.
Thetool is based on a set of regular expressionsover PoS tags, lemmas, morphosyntactic tagsand sentence boundaries, effectively performingthe same function as a chunking parser (c.f.
Ab-ney, 1996).
The resulting subcorpora can beused to determine the (relative) frequencies ofthe frames.Carroll & Rooth (1998) use an iterative ap-proach to estimate the distribution of subcat-egorisation frames given head words, startingfrom a manually-developed context-free gram-mar (of English).
First, a probabilistic ver-sion of the grammar is trained from a text cor-pus using the expectation-maximisation (EM)algorithm, and the grammar is lexicalised onrule heads.
The EM algorithm is then runagain to calculate the expected frequencies of ahead word accompanied by a particular frame.These probabilities can then be fed back intothe grammar for the next iteration.
Carroll &Rooth report encouraging results for three verbsbased on applying the technique to text fromthe BNC.Briscoe & Carroll (1997) describe a systemcapable of distinguishing 160 verbal subcate-gorisation classes--a superset of those found inthe ANLT and COMLEX Syntax dictionaries--returning relative frequencies for each framefound for each verb.
The classes also incorpo-rate information about control of predicative ar-guments and alternations such as particle move-ment and extraposition.
The approach uses arobust statistical parser which yields completethough 'shallow' parses, a comprehensive sub-categorisation class classifier, and a priori esti-mates of the probability of membership of theseclasses.
For a sample of seven verbs with multi-ple subcategorisation possibilities the system'sfrequency rankings averaged 81% correct.
(Wetalk about this system further in section 3.2 be-low, describing how we used it to provide fre-quency data for our experiment).2.2 Lexical ised Stat is t ica l  Pars ingCarroll & Weir (1997)--without actually build-ing a parsing system--address the issue of howfrequency information can be associated withlexicalised grammar formalisms, using Lexical-ized Tree Adjoining Grammar (Joshi & Schabes,1991) as a unifying framework.
They considersystematically a number of alternative probaobilistic formulations, including those of Resnik(1992) and Schabes (1992) and implementedsystems based on other underlying rammati-cal frameworks, evaluating their adequacy fromboth a theoretical and empirical perspective interms of their ability to model particular distri-butions of data that occur in existing treebanks.Magerman (1995), Collins (1996), Ratna-parkhi (1997), Charniak (1997) and others de-scribe implemented systems with impressive ac-curacy on parsing unseen data from the PennTreebank (Marcus, Santorini & Marcinkiewicz,1993).
These parsers model probabilisticallythe strengths of association between heads ofphrases, and the configurations in which theselexical associations occur.
The accuracies re-ported for these systems are substantially bet-ter than their (non-lexicalised) probabilisticcontext-free grammar analogues, demonstrat-ing clearly the value of lexico-statistical infor-mation.
However, since the grammatical de-scriptions are induced from atomic-labeled con-stituent structures in the training treebank,rather than coming from an explicit generativegrammar, these systems do not make contactwith traditional notions of argument structure(i.e.
subcategorisation, selectional preferences ofpredicates for complements) in any direct sense.So although it is now possible to extract at leastsubcategorisation data from large corpora 2 with2Grishman & Sterling (1992), Poznanski & Sanfilippo(1993), Resnik (1993), Pdbas (1994), McCarthy (1997)and others have shown that it is possible also to a?-119some degree of reliability, it would be difficultto integrate the data into this type of parsingsystem.Briscoe & Carroll (1997) present a small-scaleexperiment in which subcategorisation class fre-quency information for individual verbs w~us in-tegrated into a robust statistical (non-lexicalis-ed) parser.
The experiment used a test corpusof 250 sentences, and used the standard GEIGbracket precision, recall and crossing measures(Grishman, Macleod & Sterling, 1992) for eval-uation.
While bracket precision and recall werevirtually unchanged, the crossing bracket scorefor the lexicalised parser showed a 7% improve-ment.
However, this difference turned out notto be statistically significant at the 95% level:some analyses got better while others got worse.We have performed a similar, but much largerscale experiment, which we describe below.
Weused a larger test corpus, acquired data froman acquisition corpus an order of magnitudelarger, and used a different quantitative evalua-tion measure that we argue is more sensitive toargument/adjunct and attachment distinctions.We summarise the main features of the 'base-line' parsing system in section 3.1, describe howwe lexicalised it (section 3.2), present the resultsof the quantitative evaluation (section 3.3), givea qualitative analysis of the analysis errors made(section 3.4), and conclude with directions forfuture work.3 The  Exper iment3.1 The  Basel ine ParserThe baseline parsing system comprises:?
an HMM part-of-speech tagger (Elworthy,1994), which produces either the singlehighest-ranked tag for each word, or multi-ple tags with associated forward-backwardprobabilities (which are used with a thresh-old to prune lexical ambiguity);?
a robust finite-state lemmatiser for En-glish, an extended and enhanced versionof the University of Sheffield GATE sys-tem morphological nalyser (Cunninghamet al, 1995);?
a wide-coverage unification-based 'phrasal'grammar of English PoS tags and punctu-ation;quire selection preferences automatically from (partially)parsed data.?
a fast generalised Lit parser using thisgrammar, taking the results of the tagger asinput, and performing disambiguation us-ing a probabilistic model similar to that ofBriscoe & Carroll (1993); and?
training and test treebanks (of 4600 and500 sentences respectively) derived semi-automatically from the SUSANNE corpus(Sampson, 1995);The grammar consists of 455 phrase struc-ture rule schemata in the format accepted bythe parser (a syntactic variant of a DefiniteClause Grammar with iterative (Kleene) op-erators).
It is 'shallow' in that no attemptis made to fully analyse unbounded ependen-cies.
However, the distinction between argu-ments and adjuncts is expressed, following X-bar theory, by Chomsky-adjunction to maximalprojections of adjuncts (XP  ~ XP  Adjunct)as opposed to 'government' of arguments (i.e.arguments are sisters within X1 projections;X1 ~ XO Argl  ... ArgN) .
Furthermore, allanalyses are rooted (in S) so the grammar as-signs global, shallow and often 'spurious' analy-ses to many sentences.
Currently, the coverageof this grammar--the proportion of sentencesfor which at least one analysis is found--is 79%when applied to the SUSANNE corpus, a 138Kword treebanked and balanced subset of theBrown corpus.Inui et al (1997) have recently proposed anovel model for probabilistic LR parsing whichthey justify as theoretically more consistent andprincipled than the Briscoe & Carroll (1993)model.
We use this new model since we havefound that it indeed also improves disambigua-tion accuracy.The 500-sentence t st corpus consists only ofin-coverage sentences, and contains a mix ofwritten genres: news reportage (general andsports), belles lettres, biography, memoirs, andscientific writing.
The mean sentence length is19.3 words (including punctuation tokens).3.2 Incorporat ing  Acqu i redSubcategor i sa t ion  I fo rmat ionThe test corpus contains a total of 485 distinctverb lemmas.
We ran the Briscoe & Carroll(1997) subcategorisation acquisition system onthe first 10 million words of the BNC, for each ofthese verbs saving the first 1000 cases in whicha possible instance of a subcategorisation frame120AP NP_PP_PP PP_WHPPNONE NP_SCOMP PP_WHSNP NP_WHPP PP_WHVPNP_AP PP SCOMPNP_NP PP_AP S INFNP_NP_SCOMP PP_PP SINGNP_PP PP_SCOMP SING_PPNP_PPOF PP_VPINF VPBSEVPINFVPINGVPING_PPVPPRTWHPPTable h VSUBCAT values in the grammar.was identified.
For each verb the acquisitionsystem hypothesised a set of lexical entries cor-responding to frames for which it found enoughevidence.
Over the complete set of verbs weended up with a total of 5228 entries, each withan associated frequency normalised with respectto the total number of frames for all hypothe-sised entries for the particular verb.In the experiment each acquired lexical en-try was assigned a probability based on its nor-malised frequency, with smoothing--to allow forunseen events--using the (comparatively crude)add-1 technique.
We did not use the lexical en-tries themselves during parsing, since missingentries would have compromised coverage.
In-stead, we factored in their probabilities duringparse ranking at the end of the parsing process.We ranked complete derivations based on theproduct of (1) the (purely structural) deriva-tion probability according to the probabilisticLR model, and (2) for each verb instance inthe derivation the probability of the verbal lex-ical entry that would be used in the particu-lar analysis context.
The entry was located viathe VSUBCATvalue assigned to the verb in theanalysis by the immediately dominating verbalphrase structure rule in the grammar: VSUB-CATvalues are also present in the lexical entriessince they were acquired using the same gram-mar.
Table 1 lists the VSUBCAT values.
Thevalues are mostly self-explanatory; however, ex-amples of some of the less obvious ones are givenin (1).
(1) They made (NP_WHPP) a great fuss aboutwhat to do.They admitted (PP~COMP) to the authori-ties that they had entered illegally.It dawned (PP_WHS) on him what he shoulddo.121Some VSUBCATvalues correspond to several ofthe 160 subcategorisation classes distinguishedby the acquisition system.
In these cases thesum of the probabilities of the correspondingentries was used.
The finer distinctions stemfrom the use by the acquisition system of ad-ditional information about classes of specificprepositions, particles and other function wordsappearing within verbal frames.
In this experi-ment we ignored these distinctions.In taking the product of the derivation andsubcategorisation probabilities we have lostsome of the properties of a statistical languagemodel.
The product is no longer strictly a prob-ability, although we do not attempt to use itas such: we use it merely to rank competinganalyses.
Better integration of these two sets ofprobabilities is an area which requires furtherinvestigation.3.3 Quant i ta t ive  Eva luat ion3.3.1 Bracket ingWe evaluated parser accuracy on the unseentest corpus with respect to the phrasal brack-eting annotation standard described by Carrollet al (1997) rather than the original SUSANNEbracketings, since the analyses assigned by thegrammar and by the corpus differ for manyconstructions 3.
However, with the exception ofSUSANNE 'verb groups' our annotation standardis bracket-consistent with the treebank analy-ses (i.e.
no 'crossing brackets').
Table 2 showsthe baseline accuracy of the parser with respectto (unlabelled) bracketings, and also with thismodel when augmented with the extracted sub-categorisation i formation.
Briefly, the evalu-ation metrics compare unlabelled bracketingsderived from the test treebank with those de-rived from parses, computing recall, the ratioof matched brackets over all brackets in thetreebank; precision, the ratio of matched brack-ets over all brackets found by the parser; meancrossings, the number of times a bracketed se-quence output by the parser overlaps with onefrom the treebank but neither is properly con-tained in the other, averaged over all sentences;SOur previous attempts to produce SUSANNE annota-tion scheme analyses were not entirely successful, sinceSUSANNE does not have an underlying rammar, or evena formal description of the possible bracketing configu-rations.
Our evaluation results were often more sensitiveto the exact mapping we used than to changes we madeto the parsing system itself.Zero Mean Bracket Bracketcrossing:3 crossings recall precision(% sents,) per sent.
(%) (%)'Baseline' 57.2 1.11 82.5 83.0With subcat 56.6 1.10 83.1 83.1Table 2: Bracketing evaluation measures, before and after incorporation of subcat informationand zero crossings, the percentage of sentencesfor which the analysis returned has zero cross-ings (see Grishman, Macleod & Sterling, 1992).Since the test corpus contains only in-coverage sentences our results are relative to the80?70 or so of sentences that can be parsed.
Inexperiments measuring the coverage of our sys-tem (Carroll & Briscoe, 1996), we found thatthe mean length of failing sentences was lit-tle different o that of successfully parsed ones.We would therefore argue that the remaining20% of sentences are not significantly more com-plex, and therefore our results are not skeweddue to parse failures.
Indeed, in these experi-ments a fair proportion of unsuccessfully parsedsentences were elliptical noun or prepositionalphrases, fragments from dialogue and so forth,which we do not attempt o cover.On these measures, there is no significant dif-ference between the baseline and lexicalised ver-sions of the parser.
In particular, the meancrossing rates per sentence are almost identical.This is in spite of the fact that the two versionsreturn different highest-ranked analyses for 30%of the sentences in the test corpus.
The reasonfor the similarity in scores appears to be that theannotation scheme and evaluation measures arerelatively insensitive to argument/adjunct andattachment distinctions.
For example, in thesentence (2) from the test corpus(2) Salem (AP)  - the statewide meeting ofwar mothers Tuesday in Salem will hear agreeting from Gov.
Mark HaYfield.the phrasal analyses returned by the baselineand lexicalised parsers are, respectively (3a) and(3b).
(3) a ... (VP will hear (NP a greeting) (PPfrom (NP Gov.
Mark garfield))) ...b ... (VP will hear (NP a greeting (PPfrom (YP Gov.
Mark Hatfield)))) ...122The latter is correct, but the former, incor-rectly taking the PP  to be an argument of theverb, is penalised only lightly by the evalua-tion measures: it has zero crossings, and 75%recall and precision.
This type of annotationand evaluation scheme may be appropriate fora phrasal parser, such as the baseline version ofthe parser, which does not have the knowledgeto resolve such ambiguities.
Unfortunately, itmasks differences between such a phrasal parserand one which can use lexical information tomake informed decisions between complemen-tation and modification possibilities 4.3.3.2 Grammat ica l  Re la t ionWe therefore also evaluated the baseline andlexicalised parser against the 500 test sentencesmarked up in accordance with a second, gram-matical relation-based (GR) annotation scheme(described in detail by Carroll, Briscoe ~ San-fil!ppo, 1998).In general, grammatical relations (GRs) areviewed as specifying the syntactic dependencywhich holds between a head and a dependent.The set of GRs form a hierarchy; the ones we areconcerned with are shown in figure 1.
Subj(ect)GRs divide into clausal (zsubj/csubj), and non-clausal (ncsubj) relations.
Comp(lement) GRsdivide into clausal, and into non-clausal directobject (dobj), second (non-clausal) complementin ditransitive constructions (obj2), and indi-rect object complement introduced by a prepo-sition (iobj).
In general the parser returns themost specific (leaf) relations in the GR hier-archy, except when it is unable to determinewhether clausal subjects/objects are controlledfrom within or without (i.e.
csubj vs. zsubj, andccomp vs. zcomp respectively), in which case it4Shortcomings of this combination of annotation andevaluation scheme have been noted previously by Lin(1996), Carpenter & Manning (1997) and others.
Car-roll, Briscoe & Sanfilippo (1998) summarise the variouscriticisms that have been made.dependentrood arg..mod a~ncmod xmod c m o ~ncsu3Zs~ubbjcsubj ~ausa ldobj obj2 iobj xcomp ccompFigure 1: Portions of GR hierarchy used.
(Relations in italics are not returned by the parser).returns ubj or clausal as appropriate.
Each re-lation is parameterised with a head (lemma)and a dependent (lemma)--also ptionally atype and/or specification of grammatical func-tion.
For example, the sentence (4a) would bemarked up as in (4b).
(4) a Paul intends to leave IBM.b ncsubj (intend, Paul,_)xcomp (to, intend, leave)ncsubj (leave, Paul,_)do bj (leave, IBM,_)Carroll, Briscoe & Sanfilippo (1998) justify thisnew evaluation annotation scheme and compareit with others (constituent- and dependency-based) that have been proposed in the litera-ture.The relatively large size of the test corpushas meant hat to date we have in some casesnot distinguished between c/zsubj and betweenc/zcomp, and we have not marked up modifi-cation relations; we thus report evaluation withrespect o argument relations only (but includ-ing the relation arg_mod--a semantic argumentwhich is syntactically realised as a modifier,such as the passive 'by-phrase').
The meannumber of GRs per sentence in the test corpusis 4.15.When computing matches between the GRsproduced by the parser and those in the corpusannotation, we allow a single level of subsump-tion: a relation from the parser may be onelevel higher in the GR hierarchy than the ac-tual correct relation.
For example, if the parserreturns clausal, this is taken to match both themore specific zcomp and ccomp.
Also, an un-specified filler (_) for the type slot in the iobjand clausal relations uccessfully matches anyactual specified filler.
The head slot fillers are inall cases the base forms of single head words, sofor example, 'multi-component' heads, such asthe names of people, places or organisations arereduced to one word; thus the slot filler corre-sponding to Mr. Bill Clinton would be Clinton.For real-world applications this might not be thedesired behaviour---one might instead want thetoken Mr._BiILClinton.
This could be achievedby invoking a processing phase similar to theconventional 'named entity' identification taskin information extraction.Considering the previous example (2), butthis time with respect o GRs, the sets returnedby the baseline and lexicalised parsers are (5a)and (Sb), respectively.
(5) a ncsubj (hear, meeting,_)dobj (hear, greeting,_)io bj (.from, hear, Hatfield)b ncsubj (hear, meeting,_)dobj (hear, greeting,_)The latter is correct, but the former, incor-rectly taking the PP to be an argument of theverb, hear, is penalised more heavily than in thebracketing annotation and evaluation schemes:it gets only 67% recall.
There is also no mis-leadingly low crossing score since there is noanalogue to this in the GR scheme.Table 3 gives the result of evaluating the base-line and lexicalised versions of the parser on theGR annotation.
The measures compare the setof GRs in the annotated test corpus with thosereturned by the parser, in terms of recall, thepercentage of GRs correctly found by the parserout of all those in the treebank; and precision;123Recall Precision(%) (%)'Baseline' 88.6 79.2With subcat 88.1 88.2Table 3: GR evaluation measures, before andafter incorporation of subcategorisation i for-mation.
Argument relations only.ComplementationModificationCo-ordinationTextualMisbracketingNumber124134303040Table 5: Numbers of errors of each type madeby the lexicalised parser.the percentage of GRs returned by the parserthat are actually correct.
In the evaluation, GRrecall of the lexicalised parser drops by 0.5%compared with the baseline, while precision in-creases by 9.0%.
The drop in recall is not statis-tically significant at the 95% level (paired t-test,1.46, 499 dr, p > 0.1), whereas the increase inprecision is significant even at the 99.95% level(paired t-test, 5.14, 499 dr, p < 0.001).Table 4 gives the number of each type of GRreturned by the two models, compared with thecorrect numbers in the test corpus.
The base-line parser returns a mean of 4.65 relations persentence, whereas the lexicalised parser returnsonly 4.15, the same as the test corpus.
Thisis further, indirect evidence that the lexicalisedprobabilistic system models the data more ac-curately.3.4 D iscuss ionIn addition to the quantitative analysis of parseraccuracy reported above, we have also per-formed a qualitative analysis of the errors made.We looked at each of the errors made by the lexi-calised version of the parser on the 500-sentencetest corpus, and categorised them into errorsconcerning: complementation, modification, co-ordination, structural attachment of textual ad-juncts, and phrase-internal misbracketing.
Ofcourse, multiple errors within a given sentencemay interact, in the sense that one error may sodisrupt the structure of an analysis that it nec-essarily leads to one or more other errors beingmade.
In all cases, though, we considered allof the errors and did not attempt o determinewhether or not one of them was the 'root cause'.Table 5 summarises the number of errors of eachtype over the test corpus.Typical examples of the five error types iden-tified are:complementat ion  ... decried the high rate ofunemployment in the state misanalysed asdecry followed by an NP and a PP  comple-ment;modi f i cat ion  in ... surveillance of the pricingpractices of the concessionaires for the pur-pose of keeping the prices reasonable, thePP modifier for the purpose of...  attached'low' to concessionaires rather than 'high'to surveillance;co-ord inat ion  the NP priests, soldiers, andother members of the party misanalysed asjust two conjuncts, with the first conjunctcontaining the first two words in apposi-tion;textua l  in But you want a job guaranteed whenyou return, I continued my attack, the (tex-tual) adjunct I ... attack attached to theVP guaranteed ... return rather than the SBut ... return; andmisbracket ing  Nowhere in Isfahan is this richaesthetic life of the Persians ... has of mis-analysed as a particle, with the Persiansbecoming a separate NP.There are no obvious trends within each typeof error, although some particularly numeroussub-types can be identified.
In 8 of the 30 casesof textual misanalysis, a sentential textual ad-junct preceded by a comma was attached toolow.
The most common type of modification er-ror was--in 20 of the 134 cases--misattachmentof a PP  modifier of N to a higher VP.
The ma-jority of the complementation errors were ver-bal, accounting for 115 of the total of 124.
In15 cases of incorrect verbal complementation apassive construction was incorrectly analysed asactive, often with a following 'by' prepositionalphrase erroneously taken to be a complement.Other shortcomings of the system were ev-ident in the treatment of co-ordinated verbal124arg_mod ccomp clausal csubj dobj io~ ncsu~ obj2 subj xcomp'Baseline' 16 39 202 4 415 327 1054 53 14 202With subcat 9 20 138 3 429 172 1058 39 15 195Correct 32 16 136 2 428 160 1064 23 13 203Table 4: Numbers of each type of grammatical relation.heads, and of phrasal verbs.
The grammaticalrelation extraction module is currently unableto return GRs in which the verbal head aloneappears in the sentence as a conjunct--as in theVP ... to challenge and counter-challenge theauthentication.
This can be remedied fairly eas-ily.
Phrasal verbs, such as to consist of are iden-tified as such by the subcategorisation acquisi-tion system.
The grammar used by the shal-low parser analyses phrasal verbs in two stages:firstly the verb itself and the following parti-cle are combined to form a sub--constituent, andthen phrasal complements are attached.
Thesimple mapping from VSUBCAT values to sub-categorisation classes cannot cope with the sec-ond level of embedding of phrasal verbs, so theseverbs do not pick up any lexical information atparse time.4 Conc lus ionsWe surveyed recent work on automatic acquisi-tion from corpora of subcategorisation a d as-sociated frequency information.
We describedan experiment with a wide-coverage statisticalgrammar and parser for English and subcate-gorisation frequencies acquired from 10 millionwords of text which shows that this informationcan significantly improve the accuracy of recov-ery of grammatical relation specifications froma test corpus of 500 sentences covering a numberor different genres.Future work will include: investigating moreprincipled probabilistic models; addressing im-mediate lower-level shortcomings in the currentsystem as discussed in section 3.4 above; addingmod(ification) GR annotations to the test cor-pus and extending the parser to also returnthese; and working on incorporating selectionalpreference information that we are acquiring inother, related work (McCarthy, 1997).Re ferencesAbney, S. (1996).
Partial parsing via finite-State125cascades.
Natural Language Engineering, 2(4), 337-344.Boguraev, B., Briscoe, E., Carroll, J., Carter,D.
& Grover, C. (1987).
The derivation of agrammatically-indexed lexicon from the LongmanDictionary of Contemporary English.
In Proceed-ings of the 25th Annual Meeting of the Associationfor Computational Linguistics, 193-200.
Stanford,CA.Brent, M. (1991).
Automatic acquisition of subcat-egorization frames from untagged text.
In Proceed-ings of the 29th Annual Meeting of the Associationfor Computational Linguistics, 209-214.
Berkeley,CA.Brent, M. (1993).
From grammar to lexicon: unsu-pervised learning of lexical syntax.
ComputationalLinguistics, 19(3), 243-262.Briscoe, E. & Carroll, J.
(1993).
Generalized proba-bilistic LR parsing for unification-based grammars.Computational Linguistics, 19(1), 25-60.Briscoe, E. & Carroll, J.
(1997).
Automatic extrac-tion of subcategorization from corpora.
In Proceed-ings of the 5th ACL Conference on Applied NaturalLanguage Processing.
Washington, DC.Carpenter, B.
& Manning, C. (1997).
Probabilisticparsing using left corner language models.
In Pro-ceedings of the 5th ACL/SIGPARSE InternationalWorkshop on Parsing Technologies.
MIT, Cam-bridge, MA.Carroll, J.
& Briscoe, E. (1996).
Apportioning de-velopment effort in a probabilistic LR parsing sys-tem through evaluation.
In Proceedings of the 1stACL/SIGDAT Conference on Empirical Methods inNatural Language Processing, 92-100.
University ofPennsylvania, Philadelphia, PA.Carroll, J., Briscoe, E., Calzolari, N., Federici,S., Montemagni, S., Pirrelli, V., Grefenstette, G.,Sanfilippo, A., Carroll, G. & Rooth, M. (1997).SPARKLE WP1 specification off phrasal parsing.<http://www.ilc.pi.cnr.it/sparkle.html>.Carroll, J., Briscoe, E. & Sanfilippo, A.
(1998).Parser evaluation: a survey and a new proposal.In Proceedings of the Ist International Conferenceon Language Resources and Evaluation, 447-454.Granada, Spain.Carroll, J.
& Weir, D. (1997).
Encoding frequencyinformation i  lexicalized grammars.
In Proceedingsof the 5th A CL/SIGPARSE International Workshopon Parsing Technologies (IWPT-97), 8-17.
MIT,Cambridge, MA.Carroll, G. & Rooth, M. (1998).
Valence induc-tion with a head-lexicalized PCFG.
In Proceedingsof the 3rd Conference on Empirical Methods in Nat-ural Language Processing.
Granada, Spain.Charniak, E. (1997).
Statistical parsing with ac0ntext-free grammar and word statistics.
In .Pro-ceedings of the 14th National Conference on Artifi-cial Intelligence (AAAI-97), 598-603.
Providence,RI.Collins, M. (1996).
A new statistical parser based onbigram lexical dependencies.
In Proceedings of the34th Meeting of the Association for ComputationalLinguistics, 184-191.
Santa Cruz, CA.Cunningham, H., Gaizauskas, R. & Wilks, Y.(1995).
A general architecture for text engineering(GATE) - a new approach to language R~D.
Re-search memo CS-95-21, Department of ComputerScience, University of Sheffield, UK.Elworthy, D. (1994).
Does Baum-Welch re-estimation help taggers?.
In Proceedings of the 4thACL Conference on Applied Natural Language Pro-cessing.
Stuttgart, Germany.Gahl, S. (1998).
Automatic extraction of sub-corpora based on subcategorization frames from apart-of-speech tagged corpus.
In Proceedings of theCOLING-A CL '98.
Montreal, Canada.Grishman, R., Macleod, C. & Meyers, A.
(1994).Comlex syntax: building a computational lexicon.In Proceedings of the 15th International Conferenceon Computational Linguistics (COLING-94), 268-272.
Kyoto, Japan.Grishman, R., Macleod, C. & Sterling, J.
(1992).Evaluating parsing strategies using standardizedparse files.
In Proceedings of the 3rd ACL Confer-ence on Applied Natural Language Processing, 156-161.
Trento, Italy.Grishman, R. & Sterling, J.
(1992).
Acquisition ofselectional patterns.
In Proceedings of the 14th In-ternational Conference on Computational Linguis-tics (COLING-92), 658-664.
Nantes, France.Hornby, A.
(1989).
Ozford Advanced Learner's Dic-tionary of Current English.
Oxford, UK: OUP.Inui, K., Sornlertlamvanich, V. Tanaka, H. & Toku-naga, T. (1997).
A new formalization of proba-bilistic GLR parsing.
In Proceedings of the 5thA CL/SIGPARSE International Workshop on Pars-ing Technologies (IWPT-97}, 123-134.
Cambridge,MA.Joshi, A.
& Schabes, Y.
(1991).
Tree-adjoininggrammars and lexicalized grammars.
In M. Nivat& A. Podelski (Eds.
), Definability and Recognizabil-ity of Sets of Trees.
Elsevier.Leech, G. (1992).
100 million words of English: theBritish National Corpus.
Language Research, 28(1),1-13.Lin, D. (1996).
Dependency-based parser evalua-tion: a study with a software manual co.rpus.
In R.Sutcliffe, H-D. Koch & A. McElligott (Eds.
), Indus-trial Parsing of Software Manuals, 13-24.
Amster-dam, The Netherlands: Rodopi.Magerman, D. (1995).
Statistical decision-tree mod-els for parsing.
In Proceedings of the 33rd AnnualMeeting of the Association for Computational Lin-guistics.
Boston, MA.Manning, C. (1993).
Automatic acquisition of alarge subcategorisation dictionary from corpora.
InProceedings of the 31st Annual Meeting of the As-sociation for Computational Linguistics, 235-242.Columbus, Ohio.Marcus, M., Santorini, B.
& Marcinkiewicz (1993).Building a large annotated corpus of English: ThePenn 'IYeebank.
Computational Linguistics, 19(2),313-330.McCarthy, D. (1997).
Word sense disambigua-tion for acquisition of selectional preferences.
InProceedings of the ACL/EACL'97 Workshop Auto-matic Information Extraction and Building of Lex-ical Semantic Resources for NLP Applications, 52-61.
Madrid, Spain.Poznanski, V. & Sanfilippo, A.
(1993).
Detectingdependencies between semantic verb subclasses andsubcategorization frames in text corpora.
In B.Boguraev & J. Pustejovsky (Eds.
), SIGLEX ACLWorkshop on the Acquisition of Lexical Knowledgefrom Tezt.
Columbus, Ohio.Ratnaparkhi, A.
(1997).
A linear observed time sta-tistical parser based on maximum entropy models.In Proceedings of the 2nd Conference on Empiri-cal Methods in Natural Language Processing.
BrownUniversity, Providence, l:~_I.Resnik, P. (1992).
Probabilistic tree-adjoining gram-mar as a framework for statistical natural lan-gfiage processing.
In Proceedings of the 14th Inter-national Conference on Computational Linguistics(COLING-g2), 418-424.
Nantes, France.Resnik, P. (1993).
Selection and information: aclass-based approach to lezical relationships.
Uni-versity of Pennsylvania, CIS Dept, PhD thesis.Ribas, P. (1994).
An experiment on learning appro-priate selection restrictions from a parsed corpus.In Proceedings of the I 5th International Conferenceon Computational Linguistics (COLING-9~).
Ky-oto, Japan.Sampson, G. (1995).
English for the computer.
Ox-ford, UK: Oxford University Press.Schabes, Y.
(1992).
Stochastic lexicalized tree-adjoining grammars.
In Proceedings of the l~th In-ternational Conference on Computational Linguis-tics (COLING-92), 426-432.
Nantes, France.Ushioda, A., Evans, D., Gibson, T. & Waibel, A.(1993).
The automatic acquisition of frequencies ofverb subcategorization frames from tagged corpora.In B. Boguraev & J. Pustejovsky (Eds.
), SIGLEXA CL Workshop on the Acquisition of Lezical Knowl-edge from Tea, 95-106.
Columbus, Ohio.126
