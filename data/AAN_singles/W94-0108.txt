The automatic construction of a symbolic parser viastatistical techniquesShyam KapurDepar tment  of Computer  ScienceJames  Cook UniversityTownsvi l le QLD 4811 (Austral ia)kaput@cora l ,  cs .
j cu.
edu.
auRobin ClarkDepartment of LinguisticsUniversity of PennsylvaniaPhiladelphia PA 19104rclark@babel, l ing .
upenn, eduAbst ractWe report on the development of a robust pars-ing device which aims to provide a partial expla-nation for child language acquisition and help inthe construction of better natural language pro-cessing systems.
The backbone of the new ap-proach is the synthesis of statistical and symbolicapproaches to natural anguage.Mot ivat ionWe report on the progress we have made towardsdeveloping a robust 'self-constructing' parsing de-vice that uses indirect negative vidence (Kapur,1992) to set its parameters.
Generally, by param-eter, we mean any point of variation at which twolanguages may differ.
Thus, the relative place-ment of all object with respect o the verb, a de-terminer with respect o a noun, the difference be-tween prepositional nd postpositional languages,and the presence of long distance anaphors l ikeJapanese "zibun" and Icelandic "sig" are all pa-rameters.
The device would be exposed to an in-put text consisting of simple unpreprocessed sen-tences.
Oil the basis of this text, the device wouldinduce indirect negative evidence in support ofsome one parsing device located in the parameterspace.The development of a self-constructing pars-ing system would have a number of practical andtheoretical benefits.
First, such a parsing de-vice would reduce the development costs of newparsers.
At the moment, grammars must be de-veloped by hand, a technique which requires asignificant investment in money and man-hours.If a b~mic parser could be developed automati-cally, costs would be reduced significantly, evenif the parser requires some fine-tuning after theinitial automatic learning procedure.
Second, aparser capable of self-modification is potentiallymore robust when confronted with novel or semi-grammatical input.
This type of parser wouldhaw~ applications in information retrieval as wellas language instruction and grammar correction.Finally, the development of a parser capable o fself-modification would give us considerable in-sight into the formal properties of complex sys-tems as well as the twin problems of languagelearnability and language acquisition.Given a linguistic parameter space, the prob-lem of locating a target language somewhere inthe space on the basis of a text consisting of onlygrammatical sentences i far from trivial.
Clark(1990, 1992) has shown that the complexity ofthe problem is potentially exponential becausethe relationship between the points of variationand the actual data can he quite indirect andtangled.
Since, given n parameters, there are2 n possible parsing devices, enumerative searchthrough the space is clearly impossible.
Becauseeach datum may be successfully parsed by a num-ber of different parsing devices within the spaceand because the surface properties of grammati-cal strings underdetermine the properties of theparsing device which must be fixed by the learningalgorithm, standard deductive machine learningtechniques are as complex as a brute enumera-tive search (Clark, 1992, 1994).
In order to solvethis problem, robust echniques which can rapidlyeliminate inferior hypotheses must be developed.We propose a learning procedure which unitessymbolic omputation with statistical tools.
His-torically, symbolic techniques have proven to bea versatile tool in natural language processing.These techniques have the disadvantage of be-ing both brittle (easily broken by new input orby user error) and costly (as grammars are ex-tended to handle new constructions, developmentbecomes more difficult due to the complexity ofrule interactions within the grammar).
Statisti-cal techniques have the advantage of robustness,although the resulting grammars may lack theintuitive clarity found in symbolic systems.
Wepropose to fuse the symbolic and the statisticaltechniques, a development which we view as in-evitable; the resulting system will use statistical60learning techniques to output a symbolic parsingdevice.
We view this development to provide anice middle ground between the problems of over-training versus undertraining.
That is, statisticalapproaches to learning often tend to overfit thetraining set of data.
Symbolic approaches, onthe other hand, tend to behave as though theywere undertrained (breaking down on novel input)since the grammar tends to be compact.
Combin-ing statistical techniques with symbolic parsingwould give the advantage of obtaining relativelycompact descriptions (symbolic processing) withrobustness (statistical learning) that is not over-tuned to the training set.P re l iminar iesNaturally, a necessary preliminary for our workis to specify a set of parameters which will serveas a testing ground for the learning algorithm.This set of parameters must be embedded in aparsing system so that the learning algorithm canbe tested against data sets that approximate thekind of input that parsing devices are likely to en-counter in real world applications.In this section,we first list some parameters that gives some ideaof the kinds of variations between languages thatour system is hoped to be capable of handling.We then illustrate why parameter setting is dif-ficult by standard methods.
This provides omeadditional explanation for the failure so far in de-veloping a truly universal parameterized parser.L ingu is t ic  ParametersOur goal will be to first develop a prototype.
Wedo not require that the prototype accept any arbi-trarily selected language nor that the coverage ofthe prototype parser be complete in any given lan-guage.
Instead, we will develop a prototype withcoverage that extends to some basic structuresthat any language learning device must accountfor, plus some structures that have proven dif-ficult for various learning theories.
In particular,given an already existing parser, we will extend itscoverage by parameterizing it, as described below.Our initial set of parameters will include thefollowing other points of variation:1.
Relat ive order  of specifiers and heads:This parameter covers the placement of deter-miners relative to nouns, relative position ofthe subject and the placement of certain VP-modifying adverbs.Relat ive order  of  heads and comple-ments: This parameter deals with the po-sition of objects relative to the verb (VO orOV orders), placement of nominal and adjecti-val complements a  well as the choice betweenprepositions and postpositions..613.
Scrambling: Some language.~ allow (rein.tively) free word order.
For examph', Germallhas rules for displacing definite N Ps and clan.yesout of their canonical positions.
Japanese al-lows relatively free ordering of NPs and post-positional phrases o long as the verbal ~'om-plex remains clause final.
Other languages al-low even freer word orders.
We will focus onGerman and Japanese scrambling, bearing illmind that the model should be extendible toother types of scrambling.4.
Relat ive placement of negative mark-ers and verbs: Languages vary as to wherethey place negative markers like English not.English places its negative marker after thefirst tensed auxiliary, thus forcing do insertionwhen there is no other auxiliary, while Italianplaces negation after the tensed verb.
Frenchuses discontinuous elements like ne...pas..,  orne...plus.., which are wrapped around thetensed verb or which occur as continuous el-ements in infinitivals.
Italian differs from bothEnglish and French in placing its negativemarker before the first verb, whether tensed orinfinitive.
The proper treatment of negationwill require several parameters, given the rangeof variation.5.
Root  word order changes: In general, lan-guages allow for certain word order changes inroot clauses but not in embedded clauses.
Anexample of a root word order change is subject-auxiliary inversion in English which occurs illroot questions (Did John leave?
vs. *I wonderdid John leave?).
Another example Would beinversion of the subject clitic with the tensedverb in French ( Quelle pomme a-t-il mangle\["which apple did he eat?
"\]) and process ofsubject postposition and PP preposition in En-glish ( A man walked into the room vs.
Into theroom walked a man).6.
Rightward dislocation: This includes extra-position structures in English ( That John is lateamazes me.
vs.
It amazes me that John islate.
), presentational there structures (A manwas in the park.
vs.
There was a man in thepark.
), and stylistic inversion in French (Quellepiste Marie a-t-elle choisie?
\["What path hasMarie chosen?"\]).
Each of these constructionspresent unique problems o that the entire dataset is best handled by a system of interactingparameters.7.
Wh-movement  versus wh-in situ: Lan-guages vary in the way they encode wh-questions.
English obligatorily places one andonly one wh-phrase (for example, who or whichpicture) in first position.
In French the wh-phrase may remain in place (in silu) althoughit may also form wh-questions as in English.Polish allows wh-phrases to be stacked at thebeginning of the question.8.
Exceptional Case Marking, StructuralCase Marking: These parameters have lit-tle obvious effect on word order, but involvethe treatment ofinfinitival complements.
Thus,exceptional case marking and structural casemarking allow for the generation of the orderV\[+t~.,d NP VPl-ten,e\], where "V\[+tense\]" is atensed verb and "VPl-tense\]" is a VP headedby a verb in the infinitive.
Both parametersinvolve the semantic relations between the NPand the infinitival VP as well as the treatmentof case marking.
These relations are reflectedin constituent structure rather than word or-der and thus pose an interesting problem forthe learning algorithm.9.
Raising and control: In the case of raisingverbs and control verbs, the learner must cor-rectly categorize verbs which occur in the samesyntactic frame into two distinct groups basedon scmantic relations as reflected in the distri-bution of elements (for example, idiom chunks)around the verbs.10.
Long and short distance anaphora: Shortdistance anaphors, like "himself" in Englishmust be related to a coreferential NP withina constrained local domain.
Long distanceanaphors (Japanese "zibun", Korean "caki")must also be related to a coreferential NP, buttiffs N P need not be contained within the sametype of local domain as in the short distancecase.The above sampling of parameters has the virtueof being both small (and, therefore, possible toimplement relatively quickly) and posing interest-ing learnability problems which will appropriatelytest our learning algorithm.
Although the abovelist can be described succinctly, the set of possi-ble targets will be large and a simple enumerativesearch through the possible targets will not beefficient.Complex i t ies  o f  Parameter  Set t ingTheories based on the principles and parame-ters (POP) paradigm hypothesize that languagesshare a central core of universal properties andthat language variation can be accounted for byappeal to a finite number of points of variation,the so-called parameters.
The parameters them-selves may take on only a finite number of pos-sibh, values, prespecified by Universal Grammar.A fully spooled I'~:~i' theory would account forI;mguagc acquisition by hypothesizing that theh'aruer sets parameters to the appropriate val-ues by monitoring the input stream for "trigger-ing data"; triggers are sentences which cause the62learner to set a particular parameter to a partic-ular value.
For example, the imperative in (1) isa trigger for the order "V(erb) O(bject)":(1) Kiss grandma.under the hypothesis that the learner analyzesgrandma as the patient of kissing and is predis-posed to treat patients as structural objects.Notice that trigger-based parameter settingpresupposes that, for each parameter p and eachvalue v, the learner can identify the appropriatetrigger in the input stream.
This is the problemof trigger detection.
That is, given a particularinput item, the learner must be able to recognizewhether or not it is a trigger and, if so, what pa-rameter and value it is a trigger for.
Similarly, thelearner must be able to recognize that a particularinput datum is not a trigger for a certain param-eter even though it may share many propertieswith a trigger.
In order to make the discussionmore concrete, consider the following example:(2) a. John: thinks that Marylikes ~aim i.b.
*John thinks that Maryjlikes herj.English allows pronouns to be coreferent with ac-commanding ominal just in case that nominalis not contained within the same local syntacticdomain as the pronoun; this is a universal prop-erty of pronouns and would seem to present littleproblem to the learner.Notice, however, that some languages, includ-ing Chinese, Icelandic, Japanese andKorean, al-low for long distance anaphors.
These are ele-ments which are obligatorily coreferent with an-other nominal in the sentence, but which maybe separated from that nominal by several clauseboundaries.
Thus, the following example fromIcelandic is grammatical even though the anaphorsig is separated from its antecedent JSn by aclause boundary (Anderson, 1986):(3) J6n i segir ad MariaJohn says that Maryelski sigi/hann iloves self/himJohn says that Mary loves him.Thus, UG includes aparameter which allows omelanguages to have long distance anaphors andwhich, perhaps, fixes certain other properties ofthis class of anaphora.Notice that the example in (3) is of the samestructure as the pronominal example in (2a).
Alearner whose target is English must not take ex-amples like (2a) as a trigger for the long distanceanaphor parameter; what prevents the learnerfrom being deceived?
Why doesn't the learnerconclude that English him is comparable to Ice-landic sig?
We would argue that the learner issensitive to distributional evidence.
For example,the learner is aware of examples like (4):(4) John i likes himj.where the pronoun is not coreferential with any-thing else in the sentence.
The existence of (4)implies that him cannot be a pure anaphor, longdistance or otherwise.
Once the learner is awareof this distributional property of him, he or shecan correctly rule out (2a) as a potential triggerfor the long distance anaphor parameter.Distributional evidence, then, is crucial for pa-rameter setting; no theory of parameter settingcan avoid statistical properties of the input text.How far can we push the statistical component ofparameter setting?
In this paper, we suggest thatstatistically-based algorithms can be exploited toset parameters involving phenomena s diverseas word order, particularly verb second construc-tions, and cliticization, the difference between freepronouns and proclitics.
The work reported herecan be viewed as providing the basis for a theoryof trigger detection; it seeks to establish a theoryof the connection between the raw input text andthe process of parameter setting.Parameter Setting ProposalLet us suppose that there are n binary parameterseach of which can take one of two values ( '+' or' - ' )  in a particular natural anguage.
The coreof a natural anguage is uniquely defined once allthe n parameters have been assigned a value)Consider a random division of the parametersinto some m groups.
Let us call these groupsP1, P~,..., Pro.
The Parameter Setting Machinefirst goes about setting all the parameters withinthe first group Px concurrently as sketched below.After these parameters have been fixed, the ma-chine next tries to set the parameters in group P2in a similar fashion, and so on.a Parameters can be looked at as fixed points ofvariation among languages, From a computationalpoint of view, two different values of a parametermay simply correspond to two different bits of code inthe parser.
We are not committed to any particularscheme for the translation from a tuple of parametervalues to the corresponding language.
However, thesorts of parameters we consider have been listed inthe previous ection.631.
All parameters are unset initially, i.t,., l.h,,r,, arcno preset values.
The parser' is organized toonly obey all the universal principles.
At.
thisstage, utterances from any possible natural an-guage are accommodated with equal ea.s,~, butno sophisticated structure can be built.2.
Both the values of each of the parameters pl EP1 are 'competing' to establish themselves.3.
Corresponding to Pi, a pair of hypotheses aregenerated, say H~.
and Hi_.4.
Next, these hypotheses are tested on the basisof input evidence.5.
If H~.
fails or H~.
succeeds, et Pi'S value to '+'.Otherwise, set pi's value to ' - ' .Formal  Ana lys i s  o f  the  ParameterSet t ing  Mach ineWe next consider a particular instantiation of thehypotheses and their testing.
The way wc hart,in mind involves constructing suitable window-sizes during which the algorithm is sensitive tooccurrence as well as non-occurrence of specificphenomena.
Regular failure of a particular phe-nomenon to occur in a suitable window is one nat-ural, robust kind of indirect negative vidence.For example, the pair of hypotheses may be1.
Hypothesis H~: Expect not to observe phe-nomena from a fixed set Oi  of phenomenawhich support he parameter value ' - ' .2.
Hypothesis H~_: Expect not to observe phe-nomena from a fixed set O~.
of phenomenawhich support he parameter value '+'.Let wi and ki be two small numbers.
Testingthe hypothesis H~ involves the following proce-dure:1.
A window of size wi sentences i  constructedand a record is maintained whether or not aphenomenon from within the set O~_ occurredamong those wi sentences.2.
This construction of the window is repeated kidifferent imes and a tally ci is made of thefraction of times the phenomena occurred atleast once in the duration of the window.3.
The hypothesis H+ succeeds if and only if theratio of ci to kl is less than 0.5.Note that the phenomena under scrutiny areassumed to be such that the parser is always ca-pable of analyzing (to whatever extent necessary)the input.
This is because in our view the parserconsists of a fixed, core program whose behaviorcan be modified by selecting from among a finiteset of 'flags' (the parameters).
Therefore, evenif not all of the flags have been set to the cor-rect values, the parser is such that it can at leastpartially represent the input.
Thus, the parser is?
always capable of analyzing the input.
Also, thereis no need to explicitly store any input evidence.Saitable window-sizes can be constructed uringwhich the algorithm is sensitive to occurrence aswell as non-occurrence of specific phenomena.
Byusing windows, just the relevant bit of informa-tion from the input is extracted and maintained.
(For detailed argumentation that this is a rea-sonable theoretica!
argument, see Kaput (1992,1993).)
Notice also that we have only sketchedand analyzed a particular, simple version of ouralgorithm.
In general, a whole range of window-sizes may be used and this may be governed bythe degree to which the different hypotheses haveearned corroboration.
(For some ideas along thisdirection in a more general setting, see Kaput(199l, 1992).
)Order  in wh ich  parameters  get  setNotice that in our approach certain parametersget set quicker than others.
These are the onesthat are expressed very frequently.
It is possi-ble that these parameters also make the informa-tion extraction more efficient quicker, for exam-pie, by enabling structure building so that otherparameters can be set.
If our proposal is right,then, for example, the word order parameterswhich are presumably the very first ones to beset must be set based on a very primitive parsercapable of handling any natural language.
Atthis early stage, it may be that word and ut-terance boundaries cannot be reliably recognizedand the lexicon is quite rudimentary.
Further-more, the only accessible property in the inputstream may be the linear word order.
Anotherparticular difficulty with setting word-order pa-rameters is that the surface order of constituentsin the input does not necessarily reflect the un-derlying word-order.
For example, even thoughDutch and German are SOV languages, there isa preponderance of SVO forms in the input dueto the V2 (verb-second) phenomenon.
The finiteverb in root clauses moves to the second positionand then the first position can be occupied by thesubject, objects (direct or indirect), adverbials orprepositional phrases.
As we shall see, it is impor-t;rot to note that if the subject is not in the firstposition in a V2 language, it is most likely in thefirst position to the right of the verb.
Finally, ithas been shown by Gibson and Wexler (1992) thatthe parameter space created by the head-directionparameters along with the V2 parameter has lo-cal maxima, thai.
is, incorrect parameter settingsfront which the learner can never escape.Computat iona l  Ana lys i s  o f  theParameter  Set t ing  Mach ineV2 parameter  In this section, we summarizeresults we have obtained which show that word or-64der parameters can plausibly be set in our model.
2The key concept we use is that of entropy, aninformation-theoretic statistical measure of ran-domness of a random variable.
The entropy H(X)of a random variable X, measured in bits, is- ~x p(z)logp(z).
To give a concrete exam-ple, the outcome of a fair coin has an entropyof -( .5 * log(.5) + .5 * log(.5)) = 1 bit.
If thecoin is not fair and has .9 chance of heads and.
1chance of tails, then the entropy is around .5 bits.There is less uncertainty with the unfair coin--itis most likely going to turn up heads.
Entropycan also be thought of as the number of bits onthe average required to describe a random vari-able.
Entropy of one variable, say X, conditionedon another, say Y, denoted as H(X\]Y) is a mea-sure of how much better the first variable can bepredicted when the value of the other variable isknown.Descriptively, verb second (V2) languages placethe tensed verb in a position that immediatelyfollows the first constituent of the sentence.
Forexample, German is V2 in root clauses, as shownin (refex:v2-root), but not in embedded clauses,as shown in (telex:embedding): 3(5) a. Hans hat MariaH.
has M.getroffen.met"Hans has met Maria."b.
Hans wird MariaH.
will M.getroffen haben.met has"Hans will have metMaria.
"(o) a. well Hans Mariabecause H. M.getroffen, hat.met has"Hans has met Maria."b.
well Hans Mariabecause H. M.getroffen haben wird.met has will"because Hans will havemet Maria.
"In the examples in (5), a constituent, XP, has2Preliminary results obtained with Eric Brill werepresented at the 1993 Georgetown Roundtable onLanguage and Linguistics: Pre-session on Corpus-based Linguistics.3See the papers collected in Haider & Prinzhorn(1985) for a genera\] discussion of V2 constructions.been moved into the Specifier position of CP, trig-gering movement of the finite verb to C o .
Thisresults in the structure shown in (7).
Notice thatthe constituent XP can be of any category, maybe extracted from an embedded clause ormay bean adverbial; thus, the XP need not be related tothe finite verb via selectional restrictions or sub-categorization:(7) \[CP XPi \[C O Vj\] .
.
.
t i .
.
.
tj\]where Vj is a finite verb.The V2 parameter (or set of parameters) wouldregulate the movement of a constituent o theSpecifier of CP, forcing movement of the finiteverb to C O as well as determining whether the V2structures are restricted to the root clause or mayoccur in embedded clauses.We considered the possibility that by investi-gating the behavior of the entropy of positions inthe neighborhood of verbs in a language, wordorder characteristics of that language may bediscovered.
4 For a V2 language, we expect thatthere will be more entropy to the left of the verbthan to its right, i.e., the position to the \[eft willbe less predictable than the one to the right.
Thisis because the first position need not be relatedto the verb in any systematic way while the posi-tion following the verb will be drawn from a morerestricted class of elements (it will either be thesubject or an element internal to the VP); hence,there is more uncertainty (higher entropy) aboutthe first position than about the position follow-ing the verb.
We first show that using a simpledistributional nalysis technique based on the fiveverbs the algorithm is assumed to know, anotherfifteen words most of which turn out to be verbscan readily be obtained.Consider text as generating tuples of the form(v ,d ,w) ,  where v is one of the top twenty words(most of which are verbs), d is either the positionto the left of the verb or to the right, and w isthe word at that position.
~ V, D and W are thecorresponding random variables.The procedure for setting the V2 parameter is4In the competition model for language acquisition(MacWhinney, 1987), the child considers cues to de-termine properties of the language but while thesecues are reinforced in a statistical sense, the cuesthemselves axe not information-theoretic in the waythat ours are.
In some redent discussion of trigger-ing, Niyogi and Berwick (1993) formalize parametersetting as a Maxkov process.
Crucially, there againthe statistical assumption, on the input is merely usedto ensure that convergence is likely, and triggers aresimple sentences.SWe thank Steve Abney for suggesting this formu-lation to us.65the following:I f  U(WIV, D = left(L)) >right(It)) then+V2 else-V2., (WlV ,  z> =LanguageEnglishFrenchItalianPolishTamilTurkishDutchDanishGermanH(WIV, O = L) HCW\[V,I) = R5.554.22 4.263.91 5.094.91 5.334.09 5.784.01 \[ 5.04I 3.69 i 4.914.84 3.614.42 4.244.97Table 1.
Entropy in the Neighborhood of VerbsOn each of the 9 languages on which it hasbeen possible to test our algorithm, the correctresult was obtained.
(Only the last three lan-guages in the table are V2 languages.)
Further-more, in almost all cases, it was also shown to bestatistically significant.
The amount (only 3000utterances) and the quality of the input (unstruc-tured unannotated input caretaker speech subcor-pus from the CHILDES database (MacWhinney,1991)), and the computational resources neededfor parameter setting to succeed are psycholog-ically plausible.
Further tests were successfullyconducted in order to establish both the robust-ness and the simplicity of this learning algorittun.It is also clear that once the value of the V2 pa-rameter has been correctly set, the input is farmore revealing with regard to other word orderparameters and they too can be set using similartechniques.In order to make clear how this procedure litsinto our general parameter setting proposal, wespell out what the hypotheses are.
In the caseof the V2 parameter, the two hypotheses are notseparately necessary since one hypothesis is theexact complement of the other.
So the hypothesisH+ may be as shown.Hypothesis H+: Expect not to observe that theentropy to the left of the verbs is lower than thatto the right.The window size that may be used could bearound 300 utterances and the nmnber of repeti-tions need to be around 10.
Our previous resultsprovide empirical support hat this should suflh:e.By assuming that besides knowing a fcw verbs,as before, the algorithm also recognizes ome ofthe first and second person pronouns of the lan-guage, we can not only detcrmine aspects uf thupronoun system (see below) but also get infor-mation about the V2 parameter.
The first stepof learning is same as above; that is, the learneracquires additional verbs based on distributionalanalysis.
We expect that in the V2 languages(Dutch and German), the pronouns will appearmore often immediately to the right of the verbthan to the left.
For French, English and Ital-ian exactly the reverse is predicted.
Our results(2 to 1 or better ratio in the predicted irection)confirm these predictions:Cl i t ic  p ronouns  We now show that our tech-niques can lead to straightforward i entificationand classification of clitic pronouns7 Briefly,clitic pronouns are phonologically reduced ele-ments which obligatorily attach to another ele-,,,ent.
Syntactic litics have a number of syntacticconsequences including special word order prop-crties and an inability to participate in conjunc-t.ions and disjunctions.
For example, in French,,fldl direct objects occur after the lexical verb butaccusative clitics appear before the verb:(s) a. Jean a vu lesJ.
has seen thefilles.girls"Jean saw the girls."b.
Jean les a rues.J.
clitic has seen"Jean saw them.
"Restricting our attention, for the moment toFrench, we should note that clitic pronouns mayoccur in sequences, in which case there are a num-ber of restrictions on their relative order.
Thus,nominative clitics (eg., "je", "tu", "il", etc.)
oc-cur first, followed by the negative lement "ne",fi)llowed by accusative clitics (eg., "la", "me","re") and dative clitics ("lui"), followed, at last,I)y the first element of the verbal sequence (anauxiliary or the main verb).
There are further or-dering constraints within the accusative and da-tive elites based on the person of the clitic; seePerlmutter (1971) for an exhaustive descriptionof clitic pronouns in French.In order to correctly set the parameters govern-ing the syntax of pronominals, the learner mustdistinguish clitic pronouns from free and weakpronouns as well as sort all pronoun systems ac-cording to their proper case system (e.g., nomi-natiw' pronouns, accusal.iw, pronouns).
Further-r Wc also vcrilicd that tile object clitics in Frenchwere not primarily responsible for the correct result.7preliminary results were presented at the Berneworkshop on L|- and \[,2-acquisition ofclause-internalrules: scrambling and cliticization in January, 1994.more, the learner must have some reliable methodfor identifying the presence of clitic pronouns inthe input stream.
The above considerations sug-gest that free pronouns occur in a wider rangeof syntactic environments than clitic pronounsand, so, should carry less information about thesyntactic nature of the positions that surroundthem.
Clitic pronouns, on the other hand, occurin a limited number of environments and, hence,carry more information about the surroundingpositions.
Furthermore, since there are system-atic constraints on the relative ordering of clitics,we would expect them to fall into distributionclasses depending on the information they carryabout the positions that surround them.
The al-gorithm we report, which is also based on theobservation of entropies of positions in the neigh-borhood of pronouns, not only distinguishes accu-rately between clitic and free-standing pronouns,but also successfully sorts clitic pronouns into lin-guistically natural classes.It is assumed that the learner knows a set offirst and second person pronouns.
The learningalgorithm computes the entropy profile for threepositions to the left and right of the pronouns(H(W\]P = p) for the six different positions),where ps are the individual pronouns.
These pro-files are then compared and those pronouns whichhave similar profiles are clustered together.
Inter-estingly, it turns out that the clusters are syntac-tically appropriate categories.In French, for example, based on the Pearsoncorrelation coefficients we could deduce that theobject clitics "me" and "te", the subject clitics"je" and "tu", the non-clitics "moi" and "toi",and the ambiguous pronouns "nous" and "vons"are most closely related only to the other elementin their own class.Table 2.
Correlation Matrix for the FrenchPronounsVOOS 1TOI 0 .62  1MOI 0 .57  0 .98  1RE 0 .86  0 .24  0 .17  13E 0 .28  0 .89  0 .88  -0 .02'I'D" 0 .41  0 .94  0 .94  0 .09'rE 0 .88  0 .39  0 .30  0 .95NOOS 0 .91  0 .73  0 .68  0 .8210 .97  10 .16  0 .24  10 .53  0 .64  0 .87  1VOUS TOI MOI lie ~ TU TE IOUS66In fact, the entropy signature for the ambiguouspronouns can be analyzed as a mathematical com-bination of the signatures for the conflated forms.To distinguish clitics from non-clitics, we use themeasure of stickiness (proportion of times theyare sticking to the verbs compared to the timesthey are two or three positions away).
These re-suits are quite good.
The stickiness is as highas 54-55% for the subject clitics; non-clitics havestickiness no more than 17%.The Dutch clitic system is far more complicatedthan the French pronoun system.
(See for ex-ample, Zwart (1993).)
Even so, our entropy cal-culations made some headway towards classify-ing the pronouns.
We are able to distinguish theweak and strong subject pronouns.
Since even thestrong subject pronouns in Dutch tend to stick totheir verbs very closely and two clitics can comenext to each other, the raw stickiness measureseems to be inappropriate.
Although the Dutchcase is problematic due to the effects of V2 andscrambling, we are in the process of treating thesephenomena and anticipate that the pronoun cal-culations in Dutch will sort out properly once theinfluence of these other word order processes arefactored in appropriately.ConclusionsIt needs to be emphasized that in our statisti-cal procedure there is a mechanism available tothe learning mechanism by which it can deter-mine when it has seen enough input to reliablydetermine the value of a certain parameter.
(Suchmeans are non-existent in any trigger-based rror-driven learning theory.)
In principle at least, thelearning mechanism can determine the variance inthe quantity of interest as a function of the textsize and then know when enough text has beenseen to be sure that a certain parameter has tobe set in a particular way.We are currently extending the results we haveobtained to other parameters and other lan-guages.
We are convinced that the word or-der parameters (for example, those in (1-2) inthe section Preliminaries) should be fairly easyto set and amenable to an information-theoreticanalysis along the lines sketched earlier.
Scram-bling also provides a case where calculations ofentropy should provide an immediate solution tothe parameter-setting problem.
Notice howeverthat both scrambling and V2 interact in an in-teresting way with the basic word order parame-ters; a learner may be potentially misled by bothscrambling and V2 into mis-setting the basic wordorder parameters since both parameters can al-ter the relationship between heads, their comple-ments and their specifiers.Parameters involving adverb placement, extra-position and wh-movement should be relativelymore challenging to the learning algorithm giventhe relatively low frequency with which adverbsare found in adult speech to children.
These casesprovide good examples which motivate the useof multiple trials by the learner.
The interac-tion between adverb placement and head move-67meat, then, will pose an interesting problem forthe learner since the two parameters are interde-pendent; what the learner assumes about adverbplacement is contingent on what it assumes abouthead placement and vice versa.ReferencesAnderson, S. 1986.
The typology of anaphoricdependencies: Icelandic (and other) reflexives inL.
Hellan & K. Christensen (eds) Topics in Scan-dinavian Syntax.
D. Reidel Publishing Company,l)ordrecht, the Netherlands, pp.
65-88.Robin Clark.
1990.
Papers on learnability andnatural selection.
Technical Report 1, Universit6de Gen6ve, D~partement deLinguistique g6n~raleet de linguistique framjaise, Facult6 des Lettres,CH-1211, Gen~ve 4, 1990.
Technical Reports inFormal and Computational Linguistics.Robin Clark.
1992.
The selection of syntacticknowledge.
Language Acquisition, 2(2):83-149.Robin Clark.
1994.
Hypothesis formation asadaptation to an environment: Learnability andnatural selection.
In Barbara Lust, Magui Suffer,and Gabriella Hermon, editors, Syntactic Theoryand First Language Acquisition: CrosslinguisticPerspectives.
Lawrence Erlbaum Assoc.. Pre-sented at the 1992 symposium on 'Syntactic The-ory and First Language Acquisition: Cross Lin-guistic Pcrspectives' at Cornell University.Huber Ilaider and Martin Prinzhorn (eds).1985.
Verb Second Phenomena in GermanicLanguages.
Forts Publications, Dordrecht, theNetherlands.Edward Gibson and Kenneth Wexler.
1992.Triggers.
Presented at GLOW.Shyam Kapur.
1991.
Computational Learn-ing of Languages.
PhD thesis, Cornell University.Computer Science Department Technical Report91-1234.Shyam Kapur.
1993.
How much of what?
Isthis what underlies parameter setting?
In Pro-cccdings of the 25th Stanford University ChildLanguage Research Forum.
Also in Cognition.
('lb appear.
)Shyam Kapur.
1994.
Some applications of for-real learning theory results to natural anguageacquisition.
In Barbara Lust, Magui Suffer, andGabriella Hermon, editors, Syntactic Theory andb'irst Language Acquisition: Crosslinguistic Per-.spectives.
Lawrence Erlbaum Assoc.. Presentedat the 1992 symposium on 'Syntactic Theory andFirst Language Acquisition: Cross Linguistic Per-spectives' at Cornell University.Shyaru Kaput and Gianfranco Bilardi.
1992.Iml,guage learning from stochastic input.
In Pro-cccdin.qs of lhr fifth conference on Computational68Learning Theory.
Morgan-Kaufman.Brian MacWhinney.
1987.
The competitionmodel.
In Brian MacWhinney, editor, Mecha-nisms of Language Acquisition.
Lawrence Erl-baum Assoc..Brian MacWhinney.
1991.
The CHILDESProject: Tools for analyzing Talk.
L.
ErlbaumAssoc., Hillsdale, New Jersey.Partha Niyogi and Robert C. Berwick.
1993.Formalizing triggers: A learning model for finitespaces.
Technical Report A.I.
Memo No.
1449,Massachusetts Institute of Technology.
AlsoCenter for Biological Computational Learning,Whitaker College Paper No.
86.David Perlmutter.
(1971).
Deep and SurfaceConstraints in Syntax.
Holt, Reinhart and Win-ston, New York.C.
Jan-Wouter Zwart.
1993.
Notes on cliticsin dutch.
In Lars Hellan, editor, Clitics in Ger-manic and Slavic, pages 119-155.
Eurotyp work-ing papers, Theme Group 8, Vol.
4, University ofTilhurg.
