Est imators  for Stochast ic "Unif ication-Based" Grammars*Mark  JohnsonCognitive and Linguistic SciencesBrown UniversityS tuar t  GemanApplied MathematicsBrown UniversityS tephen CanonCognitive and Linguistic SciencesBrown UniversityZhiyi ChiDept.
of StatisticsThe University of ChicagoSte fan  R iez le rInst i tut  fiir Maschinelle SprachverarbeitungUniversit~t StuttgartAbstractLog-linear models provide a statistically soundframework for Stochastic "Unification-Based"Grammars (SUBGs) and stochastic versions ofother kinds of grammars.
We describe twocomputationally-tractable ways of estimatingthe parameters of such grammars from a train-ing corpus of syntactic analyses, and applythese to estimate a stochastic version of Lexical-Functional Grammar.1 Introduct ionProbabilistic methods have revolutionized com-putational inguistics.
They can provide asystematic treatment of preferences in pars-ing.
Given a suitable estimation procedure,stochastic models can be "tuned" to reflect theproperties of a corpus.
On the other hand,"Unification-Based" Grammars (UBGs) can ex-press a variety of linguistically-important syn-tactic and semantic onstraints.
However, de-veloping Stochastic "Unification-based" Gram-mars (SUBGs) has not proved as straight-forward as might be hoped.The simple "relative frequency" estimatorfor PCFGs yields the maximum likelihood pa-rameter estimate, which is to say that itminimizes the Kulback-Liebler divergence be-tween the training and estimated istributions.On the other hand, as Abney (1997) pointsout, the context-sensitive dependencies that"unification-based" constraints introduce ren-der the relative frequency estimator suboptimal:in general it does not maximize the likelihoodand it is inconsistent.
* This research was supported by the National ScienceFoundation (SBR,-9720368), the US Army Research Of-fice (DAAH04-96-BAA5), and Office of Naval Research(N00014-97-1-0249).Abney (1997) proposes a Markov RandomField or log linear model for SUBGs, and themodels described here are instances of Abney'sgeneral framework.
However, the Monte-Carloparameter estimation procedure that Abneyproposes eems to be computationally imprac-tical for reasonable-sized grammars.
Sections 3and 4 describe two new estimation procedureswhich are computationally tractable.
Section 5describes an experiment with a small LFG cor-pus provided to us by Xerox PAaC.
The loglinear framework and the estimation proceduresare extremely general, and they apply directlyto stochastic versions of HPSG and other theo-ries of grammar.2 Features  in SUBGsWe follow the statistical literature in using theterm feature to refer to the properties that pa-rameters are associated with (we use the word"attribute" to refer to the attributes or featuresof a UBG's feature structure).
Let ~ be theset of all possible grammatical or well-formedanalyses.
Each feature f maps a syntactic anal-ysis w E ~ to a real value f(w).
The form ofa syntactic analysis depends on the underlyinglinguistic theory.
For example, for a PCFG wwould be parse tree, for a LFG w would be atuple consisting of (at least) a c-structure, an f-structure and a mapping from c-structure nodesto f-structure lements, and for a Chomskyiantransformational grammar w would be a deriva-tion.Log-linear models are models in which thelog probability is a linear combination of fea-ture values (plus a constant).
PCFGs, Gibbsdistributions, Maximum-Entropy distributionsand Markov Random Fields are all examples oflog-linear models.
A log-linear model associateseach feature fj with a real-valued parameter Oj.535A log-linear model with m features is one inwhich the likelihood P(w) of an analysis w is:PO(CO) -- 1 eEj= 1 ...... ojlj(~o)ZoZo ----- Z eZJ=l ...... Ojfj(oJ)w'E~While the estimators described below makeno assumptions about the range of the .fi, inthe models considered here the value of eachfeature fi(w) is the number of times a particu-lar structural arrangement or configuration oc-curs in the analysis w, so f i(w) ranges over thenatural numbers.For example, the features of a PCFG areindexed by productions, i.e., the value f i(w)of feature fi is the number of times theith production is used in the derivation w.This set of features induces a tree-structureddependency graph on the productions whichis characteristic of Markov Branching Pro-cesses (Pearl, 1988; Frey, 1998).
This treestructure has the important consequence thatsimple "relative-frequencies" yield maximum-likelihood estimates for the Oi.Extending a PCFG model by adding addi-tional features not associated with productionswill in general add additional dependencies, de-stroy the tree structure, and substantially com-plicate maximum likelihood estimation.This is the situation for a SUBG, even if thefeatures are production occurences.
The uni-fication constraints create non-local dependen-cies among the productions and the dependencygraph of a SUBG is usually not a tree.
Conse-quently, maximum likelihood estimation is nolonger a simple matter of computing relativefrequencies.
But the resulting estimation proce-dures (discussed in detail, shortly), albeit morecomplicated, have the virtue of applying to es-sentially arbitrary features--of the productionor non-production type.
That is, since estima-tors capable of finding maximum-likelihood pa-rameter estimates for production features in aSUBG will also find maximum-likelihood esti-mates for non-production features, there is nomotivation for restricting features to be of theproduction type.Linguistically there is no particular reasonfor assuming that productions are the best fea-tures to use in a stochastic language model.For example, the adjunct attachment ambigu-ity in (1) results in alternative syntactic struc-tures which use the same productions the samenumber of times in each derivation, so a modelwith only production features would necessarilyassign them the same likelihood.
Thus modelsthat use production features alone predict thatthere should not be a systematic preference forone of these analyses over the other, contrary tostandard psycholinguistic results.1.a Bill thought Hillary \[vp\[vP left \] yesterday \]1.b Bill \[vP\[vP thought Hillary left \] yesterday \]There are many different ways of choosingfeatures for a SUBG, and each of these choicesmakes an empirical claim about possible distri-butions of sentences.
Specifying the features ofa SUBG is as much an empirical matter as spec-ifying the grammar itself.
For any given UBGthere are a large (usually infinite) number ofSUBGs that can be constructed from it, differ-ing only in the features that each SUBG uses.In addition to production features, thestochastic LFG models evaluated below usedthe following kinds of features, guided by theprinciples proposed by Hobbs and Bear (1995).Adjunct and argument features indicate adjunctand argument attachment respectively, and per-mit the model to capture a general argumentattachment preference.
In addition, there arespecialized adjunct and argument features cor-responding to each grammatical function usedin LFG (e.g., SUB J, OBJ, COMP, XCOMP,ADJUNCT, etc.).
There are features indi-cating both high and low attachment (deter-mined by the complexity of the phrase beingattached to).
Another feature indicates non-right-branching nonterminal nodes.
There isa feature for non-parallel coordinate structures(where parallelism is measured in constituentstructure terms).
Each f-structure attribute-atomic value pair which appears in any featurestructure is also used as a feature.
We also usea number of features identifying syntactic struc-tures that seem particularly important in thesecorpora, such as a feature identifying NPs thatare dates (it seems that date interpretations ofNPs are preferred).
We would have liked tohave included features concerning specific lex-ical items (to capture head-to-head dependen-cies), but we felt that our corpora were so small536that the associated parameters could not be ac-curately estimated.3 A pseudo- l i ke l ihood  es t imator  forlog l inear  mode lsSuppose ~ = Wl,..-,Wn is a training cor-pus of n syntactic analyses.
Letting f j(~) =~i=l,...,n fJ (wi), the log likelihood of the corpusand its derivatives are:logL0(~) = ~ Oj f j (~) -n logZo(2)j=l, .
.
.
,m0 log L0 (~)- - nEd/ j )  (3)oo jwhere Eo(fj) is the expected value of fj underthe distribution determined by the parameters0.
The maximum-likelihood estimates are the 0which maximize log Lo(~).
The chief difficultyin finding the maximum-likelihood estimates icalculating E0 (fj), which involves umming overthe space of well-formed syntactic structures ft.There seems to be no analytic or efficient nu-merical way of doing this for a realistic SUBG.Abney (1997) proposes a gradient ascent,based upon a Monte Carlo procedure for esti-mating E0(fj).
The idea is to generate randomsamples of feature structures from the distribu-tion P~i(w), where 0 is the current parameterestimate, and to use these to estimate E~(fj),and hence the gradient of the likelihood.
Sam-ples are generated as follows: Given a SUBG,Abney constructs a covering PCFG based uponthe SUBG and 0, the current estimate of 0.
Thederivation trees of the PCFG can be mappedonto a set containing all of the SUBG's syn-tactic analyses.
Monte Carlo samples from thePCFG are comparatively easy to generate, andsample syntactic analyses that do not map towell-formed SUBG syntactic structures are thensimply discarded.
This generates a stream ofsyntactic structures, but not distributed accord-ing to P~(w) (distributed instead according tothe restriction of the PCFG to the SUBG).
Ab-ney proposes using a Metropolis acceptance-rejection method to adjust the distribution ofthis stream of feature structures to achieve de-tailed balance, which then produces a streamof feature structures distributed according toPo(w).While this scheme is theoretically sound, itwould appear to be computationally impracti-cal for realistic SUBGs.
Every step of the pro-posed procedure (corresponding to a single stepof gradient ascent) requires a very large numberof PCFG samples: samples must be found thatcorrespond to well-formed SUBGs; many suchsamples are required to bring the Metropolis al-gorithm to (near) equilibrium; many samplesare needed at equilibrium to properly estimateE0(Ij).The idea of a gradient ascent of the likelihood(2) is appealing--a simple calculation revealsthat the likelihood is concave and therefore freeof local maxima.
But the gradient (in partic-ular, Ee(fj)) is intractable.
This motivates analternative strategy involving a data-based esti-mate of E0(fj):Ee(fj) = Ee(Ee(fj(w)ly(w))) (4)1= - ~ Ea(fj(w)ly(w) =yd(5)72 i=l,...,nwhere y(w) is the yield belonging to the syn-tactic analysis w, and Yi = y(wi) is the yieldbelonging to the i'th sample in the training cor-pus.The point is that Ee(fj(w)ly(w ) = Yi) is gen-erally computable.
In fact, if f~(y) is the set ofwell-formed syntactic structures that have yieldy (i.e., the set of possible parses of the string y),thenEo(fj( o)ly( ,) = =Ew'Ef~(yi) f J(w') e~-~k=x ...... Ok$1,(w')Hence the calculation of the conditional expec-tations only involves umming over the possiblesyntactic analyses or parses f~(Yi) of the stringsin the training corpus.
While it is possible toconstruct UBGs for which the number of pos-sible parses is unmanageably high, for manygrammars it is quite manageable to enumeratethe set of possible parses and thereby directlyevaluate Eo(f j(w)ly(w ) = Yi).Therefore, we propose replacing the gradient,(3), byfj(w) - ~ Eo(fj(w)lY(W) = Yi) (6)i=l,...,nand performing a gradient ascent.
Of course (6)is no longer the gradient of the likelihood func-537tion, but fortunately it is (exactly) the gradientof (the log of) another criterion:PLo(~) = I I  Po(w = wily(w) = yi) (7)i=l , .
.
.
,nInstead of maximizing the likelihood of the syn-tactic analyses over the training corpus, wemaximize the conditional likelihood of theseanalyses given the observed yields.
In our exper-iments, we have used a conjugate-gradient op-timization program adapted from the one pre-sented in Press et al (1992).Regardless of the pragmatic (computational)motivation, one could perhaps argue that theconditional probabilities Po(wly ) are as use-ful (if not more useful) as the full probabili-ties P0(w), at least in those cases for whichthe ultimate goal is syntactic analysis.
Bergeret al (1996) and Jelinek (1997) make this samepoint and arrive at the same estimator, albeitthrough a maximum entropy argument.The problem of estimating parameters forlog-linear models is not new.
It is especially dif-ficult in cases, such as ours, where a large sam-ple space makes the direct computation of ex-pectations infeasible.
Many applications in spa-tial statistics, involving Markov random fields(MRF), are of this nature as well.
In hisseminal development of the MRF approach tospatial statistics, Besag introduced a "pseudo-likelihood" estimator to address these difficul-ties (Besag, 1974; Besag, 1975), and in fact ourproposal here is an instance of his method.
Ingeneral, the likelihood function is replaced by amore manageable product of conditional likeli-hoods (a pseudo-likelihood--hence the designa-tion PL0), which is then optimized over the pa-rameter vector, instead of the likelihood itself.In many cases, as in our case here, this sub-stitution side steps much of the computationalburden without sacrificing consistency (more onthis shortly).What are the asymptotics of optimizing apseudo-likelihood function?
Look first at thelikelihood itself.
For large n:1 logL0(~) 1 log I I  Po(wi)n n i=l,... ,n1 ~ logp0(w dF& i=l,... ,nf Poo(w)logPo(w)dw (8)where 0o is the true (and unknown) parame-ter vector.
Up to a constant, (8) is the nega-tive of the Kullback-Leibler divergence betweenthe true and estimated istributions of syntac-tic analyses.
As sample size grows, maximizinglikelihood amofints to minimizing divergence.As for pseudo-likelihood:1- log PL0(~)nl l?g IX Po(w wi{y(w)=yi)n i=l,.
.
.
,n_-- _1 ~ logPo(w=wily(  w )=Yi )n i=l,... ,nEOo \ [ f  P0o (wly) log P0 (wly)dw\]So that maximizing pseudo-likelihood (at largesamples) amounts to minimizing the average(over yields) divergence between the true andestimated conditional distributions of analysesgiven yields.Maximum likelihood estimation is consistent:under broad conditions the sequence of dis-tributions P0 , associated with the maximumr~likelihood estimator for 0o given the samplesWl,...wn, converges to P0o.
Pseudo-likelihoodis also consistent, but in the present implemen-tation it is consistent for the conditional dis-tributions P0o (w\[y(w)) and not necessarily forthe full distribution P0o (see Chi (1998)).
It isnot hard to see that pseudo-likelihood will notalways correctly estimate P0o- Suppose thereis a feature fi which depends only on yields:fi(w) = fi(y(w)).
(Later we will refer to suchfeatures as pseudo-constant.)
In this case, thederivative of PL0 (~) with respect o Oi is zero;PL0(~) contains no information about Oi.
Infact, in this case any value of Oi gives the sameconditional distribution Po(wly(w)); Oi is irrele-vant to the problem of choosing ood parses.Despite the assurance of consistency, pseudo-likelihood estimation is prone to over fittingwhen a large number of features is matchedagainst a modest-sized training corpus.
Oneparticularly troublesome manifestation of overfitting results from the existence of featureswhich, relative to the training set, we mightterm "pseudo-maximal": Let us say that afeature f is pseudo-maximal for a yield y iff538Vw' E ~)(y)f(w) ~ f ( J )  where w is any cor-rect parse of y, i.e., the feature's value on everycorrect parse w of y is greater than or equalto its value on any other parse of y. Pseudo-minimal features are defined similarly.
It is easyto see that if f j is pseudo-maximal on each sen-tence of the training corpus then the param-eter assignment Oj = co maximizes the cor-pus pseudo-likelihood.
(Similarly, the assign-ment Oj = -oo  maximizes pseudo-likelihood iff j  is pseudo-minimal over the training corpus).Such infinite parameter values indicate that themodel treats pseudo-maximal features categori-cally; i.e., any parse with a non-maximal featurevalue is assigned a zero conditional probability.Of course, a feature which is pseudo-maximalover the training corpus is not necessarilypseudo-maximal for all yields.
This is an in-stance of over fitting, and it can be addressed,as is customary, by adding a regularization termthat promotes mall values of 0 to the objec-tive function.
A common choice is to add aquadratic to the log-likelihood, which corre-sponds to multiplying the likelihood itself bya normal distribution.
In our experiments, wemultiplied the pseudo-likelihood bya zero-meannormal in 01,... Om, with diagonal covariance,and with standard eviation aj for 0j equal to7 times the maximum value of fj found in anyparse in the training corpus.
(We experimentedwith other values for aj, but the choice seems tohave little effect).
Thus instead of maximizingthe log pseudo-likelihood, wechoose 0 to maxi-mize /3z 2log PL0(~) - ~ 2avJ2 (9)j=l,...,m J4 A max imum cor rect  es t imator  forlog l inear  mode lsThe pseudo-likelihood estimator described inthe last section finds parameter values whichmaximize the conditional probabilities of theobserved parses (syntactic analyses) given theobserved sentences (yields) in the training cor-pus.
One of the empirical evaluation measureswe use in the next section measures the num-ber of correct parses selected from the set ofall possible parses.
This suggests another pos-sible objective function: choose ~ to maximizethe number Co (~) of times the maximum likeli-hood parse (under 0) is in fact the correct parse,in the training corpus.Co(~) is a highly discontinuous function of 0,and most conventional optimization algorithmsperform poorly on it.
We had the most suc-cess with a slightly modified version of the sim-ulated annealing optimizer described in Presset al (1992).
This procedure ismuch more com-putationally intensive than the gradient-basedpseudo-likelihood procedure.
Its computationaldifficulty grows (and the quality of solutions de-grade) rapidly with the number of features.5 Empirical evaluationRon Kaplan and Hadar Shemtov at Xerox PArtCprovided us with two LFG parsed corpora.
TheVerbmobil corpus contains appointment plan-ning dialogs, while the Homecentre corpus isdrawn from Xerox printer documentation.
Ta-ble 1 summarizes the basic properties of thesecorpora.
These corpora contain packed c/f-structure representations (Maxwell III and Ka-plan, 1995) of the grammatical parses of eachsentence with respect to Lexical-Functionalgrammars.
The corpora also indicate which ofthese parses is in fact the correct parse (thisinformation was manually entered).
Becauseslightly different grammars were used for eachcorpus we chose not to combine the two corpora,although we used the set of features described insection 2 for both in the experiments describedbelow.
Table 2 describes the properties of thefeatures used for each corpus.In addition to the two estimators describedabove we also present results from a baseline s-timator in which all parses are treated as equallylikely (this corresponds tosetting all the param-eters Oj to zero).We evaluated our estimators using held-outtest corpus ~test.
We used two evaluationmeasures.
In an actual parsing application aSUBG might be used to identify the correctparse from the set of grammatical parses, soour first evaluation measure counts the numberCo(~test) of sentences in the test corpus ~testwhose maximum likelihood parse under the es-timated model 0 is actually the correct parse.If a sentence has 1 most likely parses (i.e., all1 parses have the same conditional probability)and one of these parses is the correct parse, thenwe score 1/l for this sentence.The second evaluation measure is the pseudo-539Number of sentencesNumber of ambiguous sentencesNumber of parses of ambiguous sentencesVerbmobi l  corpus Homecent re  corpus540 980314 4813245 3169Table 1: Properties of the two corpora used to evaluate the estimators.Verbmobi l  corpus Homecent re  corpusNumber of features 191 227Number of rule features 59 57Number of pseudo-constant features 19 41Number of pseudo-maximal features 12 4Number of pseudo-minimal features 8 5Table 2: Properties of the features used in the stochastic LFG models.
The numbers of pseudo-maximal and pseudo-minimal features do not include pseudo-constant features.likelihood itself, PL~(wtest) .
The pseudo-likelihood of the test corpus is the likelihood ofthe correct parses given their yields, so pseudo-likelihood measures how much of the probabil-ity mass the model puts onto the correct anal-yses.
This metric seems more relevant o ap-plications where the system needs to estimatehow likely it is that the correct analysis lies ina certain set of possible parses; e.g., ambiguity-preserving translation and human-assisted dis-ambiguation.
To make the numbers more man-ageable, we actually present he negative loga-rithm of the pseudo-likelihood rather than thepseudo-likelihood itself--so smaller is better.Because of the small size of our corpora weevaluated our estimators using a 10-way cross-validation paradigm.
We randomly assignedsentences of each corpus into 10 approximatelyequal-sized subcorpora, each of which was usedin turn as the test corpus.
We evaluated on eachsubcorpus the parameters that were estimatedfrom the 9 remaining subcorpora that served asthe training corpus for this run.
The evalua-tion scores from each subcorpus were summedin order to provide the scores presented here.Table 3 presents the results of the empiri-cal evaluation.
The superior performance ofboth estimators on the Verbmobil corpus prob-ably reflects the fact that the non-rule fea-tures were designed to match both the gram-mar and content of that corpus.
The pseudo-likelihood estimator performed better than thecorrect-parses timator on both corpora un-der both evaluation metrics.
There seems tobe substantial over learning in all these mod-els; we routinely improved performance by dis-carding features.
With a small number offeatures the correct-parses timator typicallyscores better than the pseudo-likelihood estima-tor on the correct-parses valuation metric, butthe pseudo-likelihood estimator always scoresbetter on the pseudo-likelihood evaluation met-ric.6 Conc lus ionThis paper described a log-linear model forSUBGs and evaluated two estimators for suchmodels.
Because stimators that can estimaterule features for SUBGs can also estimate otherkinds of features, there is no particular reason tolimit attention to rule features in a SUBG.
In-deed, the number and choice of features tronglyinfluences the performance of the model.
Theestimated models are able to identify the cor-rect parse from the set of all possible parses ap-proximately 50% of the time.We would have liked to introduce featurescorresponding to dependencies between lexicalitems.
Log-linear models are well-suited for lex-ical dependencies, but because of the large num-ber of such dependencies substantially largercorpora will probably be needed to estimatesuch models.
11Alternatively, it may be possible to use a simplernon-SUBG model of lexical dependencies timated froma much larger corpus as the reference distribution with540Baseline estimatorPseudo-likelihood estimatorCorrect-parses stimatorVerbmobi l  corpus Homecent re  corpusC(~test) -logPL(~test) C(~test) - logPL(~test)9.7% 533 15.2% 65558.7% 396 58.8% 58353.7% 469 53.2% 604Table 3: An empirical evaluation of the estimators.
C(~test) is the number of maximum likelihoodparses of the test corpus that were the correct parses, and - log  PL(wtest) is the negative logarithmof the pseudo-likelihood f the test corpus.However, there may be applications whichcan benefit from a model that performs even atthis level.
For example, in a machine-assistedtranslation system a model like ours couldbe used to order possible translations o thatmore likely alternatives are presented before lesslikely ones.
In the ambiguity-preserving trans-lation framework, a model ike this one could beused to choose between sets of analyses whoseambiguities cannot be preserved in translation.ReferencesSteven P. Abney.
1997.
Stochastic Attribute-Value Grammars.
Computational Linguis-tics, 23(4):597-617.Adam~L.
Berger, Vincent J. Della Pietra,and Stephen A. Della Pietra.
1996.
Amaximum entropy approach to natural lan-guage processing.
Computational Linguistics,22(1):39-71.J.
Besag.
1974.
Spatial interaction and the sta-tistical analysis of lattice systems (with dis-cussion).
Journal of the Royal Statistical So-ciety, Series D, 36:192-236.J.
Besag.
1975.
Statistical analysis of non-lattice data.
The Statistician, 24:179-195.Zhiyi Chi.
1998.
Probability Models for Com-plex Systems.
Ph.D. thesis, Brown University.Brendan J. Frey.
1998.
Graphical Models forMachine Learning and Digital Communica-tion.
The MIT Press, Cambridge, Mas-sachusetts.Jerry R. Hobbs and John Bear.
1995.
Twoprinciples of parse preference.
In AntonioZampolli, Nicoletta Calzolari, and MarthaPalmer, editors, Linguistica Computazionale:Current Issues in Computational Linguistics:In Honour of Don Walker, pages 503-512.Kluwer.Frederick Jelinek.
1997.
Statistical Methods forSpeech Recognition.
The MIT Press, Cam-bridge, Massachusetts.John T. Maxwell III and Ronald M. Kaplan.1995.
A method for disjunctive constraintsatisfaction.
In Mary Dalrymple, Ronald M.Kaplan, John T. Maxwell III, and AnnieZaenen, editors, Formal Issues in Lexical-Functional Grammar, number 47 in CSLILecture Notes Series, chapter 14, pages 381-481.
CSLI Publications.Judea Pearl.
1988.
Probabalistic Reasoning inIntelligent Systems: Networks of PlausibleInference.
Morgan Kaufmann, San Mateo,California.William H. Press, Saul A. Teukolsky,William T. Vetterling, and Brian P. Flannery.1992.
Numerical Recipies in C: The Art ofScientific Computing.
Cambridge UniversityPress, Cambridge, England, 2nd edition.respect o which the SUBG model is defined, as describedin Jelinek (1997).541
