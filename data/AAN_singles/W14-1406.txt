Proceedings of the EACL 2014 Workshop on Type Theory and Natural Language Semantics (TTNLS), pages 46?54,Gothenburg, Sweden, April 26-30 2014.c?2014 Association for Computational LinguisticsA Type-Driven Tensor-Based Semantics for CCGJean MaillardUniversity of CambridgeComputer Laboratoryjm864@cam.ac.ukStephen ClarkUniversity of CambridgeComputer Laboratorysc609@cam.ac.ukEdward GrefenstetteUniversity of OxfordDepartment of Computer Scienceedward.grefenstette@cs.ox.ac.ukAbstractThis paper shows how the tensor-based se-mantic framework of Coecke et al.
canbe seamlessly integrated with Combina-tory Categorial Grammar (CCG).
The inte-gration follows from the observation thattensors are linear maps, and hence canbe manipulated using the combinators ofCCG, including type-raising and compo-sition.
Given the existence of robust,wide-coverage CCG parsers, this opens upthe possibility of a practical, type-drivencompositional semantics based on distri-butional representations.1 IntoductionIn this paper we show how tensor-based distribu-tional semantics can be seamlessly integrated withCombinatory Categorial Grammar (CCG, Steed-man (2000)), building on the theoretical discus-sion in Grefenstette (2013).
Tensor-based distribu-tional semantics represents the meanings of wordswith particular syntactic types as tensors whose se-mantic type matches that of the syntactic type (Co-ecke et al., 2010).
For example, the meaning of atransitive verb with syntactic type (S\NP)/NP isa 3rd-order tensor from the tensor product spaceN ?
S ?
N .
The seamless integration with CCGarises from the (somewhat trivial) observation thattensors are linear maps ?
a particular kind offunction ?
and hence can be manipulated usingCCG?s combinatory rules.Tensor-based semantics arises from the desire toenhance distributional semantics with some com-positional structure, in order to make distribu-tional semantics more of a complete semantic the-ory, and to increase its utility in NLP applica-tions.
There are a number of suggestions for howto add compositionality to a distributional seman-tics (Clarke, 2012; Pulman, 2013; Erk, 2012).One approach is to assume that the meanings ofall words are represented by context vectors, andthen combine those vectors using some operation,such as vector addition, element-wise multiplica-tion, or tensor product (Clark and Pulman, 2007;Mitchell and Lapata, 2008).
A more sophisticatedapproach, which is the subject of this paper, is toadapt the compositional process from formal se-mantics (Dowty et al., 1981) and attempt to builda distributional representation in step with the syn-tactic derivation (Coecke et al., 2010; Baroni et al.,2013).
Finally, there is a third approach using neu-ral networks, which perhaps lies in between thetwo described above (Socher et al., 2010; Socheret al., 2012).
Here compositional distributed rep-resentations are built using matrices operating onvectors, with all parameters learnt through a su-pervised learning procedure intended to optimiseperformance on some NLP task, such as syntac-tic parsing or sentiment analysis.
The approachof Hermann and Blunsom (2013) conditions thevector combination operation on the syntactic typeof the combinands, moving it a little closer to themore formal semantics-inspired approaches.The remainder of the Introduction gives a shortsummary of distributional semantics.
The rest ofthe paper introduces some mathematical notationfrom multi-linear algebra, including Einstein nota-tion, and then shows how the combinatory rules ofCCG, including type-raising and composition, canbe applied directly to tensor-based semantic rep-resentations.
As well as describing a tensor-basedsemantics for CCG, a further goal of this paper is topresent the compositional framework of Coecke etal.
(2010), which is based on category theory, to acomputational linguistics audience using only themathematics of multi-linear algebra.1.1 Distributional SemanticsWe assume a basic knowledge of distributional se-mantics (Grefenstette, 1994; Sch?utze, 1998).
Re-46cent inroductions to the topic include Turney andPantel (2010) and Clark (2014).A potentially useful distinction for this paper,and one not commonly made, is between distri-butional and distributed representations.
Distri-butional representations are inherently contextual,and rely on the frequently quoted dictum fromFirth that ?you shall know a word from the com-pany it keeps?
(Firth, 1957; Pulman, 2013).
Thisleads to the so-called distributional hypothesis thatwords that occur in similar contexts tend to havesimilar meanings, and to various proposals forhow to implement this hypothesis (Curran, 2004),including alternative definitions of context; alter-native weighting schemes which emphasize theimportance of some contexts over others; alterna-tive similarity measures; and various dimension-ality reduction schemes such as the well-knownLSA technique (Landauer and Dumais, 1997).
Aninteresting conceptual question is whether a sim-ilar distributional hypothesis can be applied tophrases and larger units: is it the case that sen-tences, for example, have similar meanings if theyoccur in similar contexts?
Work which does ex-tend the distributional hypothesis to larger unitsincludes Baroni and Zamparelli (2010), Clarke(2012), and Baroni et al.
(2013).Distributed representations, on the other hand,can be thought of simply as vectors (or possiblyhigher-order tensors) of real numbers, where thereis no a priori interpretation of the basis vectors.Neural networks can perhaps be categorised in thisway, since the resulting vector representations aresimply sequences of real numbers resulting fromthe optimisation of some training criterion on atraining set (Collobert and Weston, 2008; Socheret al., 2010).
Whether these distributed represen-tations can be given a contextual interpretation de-pends on how they are trained.One important point for this paper is that thetensor-based compositional process makes no as-sumptions about the interpretation of the tensors.Hence in the remainder of the paper we make noreference to how noun vectors or verb tensors,for example, can be acquired (which, for the caseof the higher-order tensors, is a wide open re-search question).
However, in order to help thereader who would prefer a more grounded dis-cussion, one possibility is to obtain the noun vec-tors using standard distributional techniques (Cur-ran, 2004), and learn the higher-order tensors us-ing recent techniques from ?recursive?
neural net-works (Socher et al., 2010).
Another possibilityis suggested by Grefenstette et al.
(2013), extend-ing the learning technique based on linear regres-sion from Baroni and Zamparelli (2010) in which?gold-standard?
distributional representations areassumed to be available for some phrases andlarger units.2 Mathematical PreliminariesThe tensor-based compositional process relies ontaking dot (or inner) products between vectors andhigher-order tensors.
Dot products, and a numberof other operations on vectors and tensors, can beconveniently written using Einstein notation (alsoreferred to as the Einstein summation convention).In the rest of the paper we assume that the vectorspaces are over the field of real numbers.2.1 Einstein NotationThe squared amplitude of a vector v ?
Rnis givenby:|v|2=n?i=1viviSimilarly, the dot product of two vectors v,w ?Rnis given by:v ?w =n?i=1viwiDenote the components of an m?n real matrixA by Aijfor 1 ?
i ?
m and 1 ?
j ?
n. Thenthe matrix-vector product of A and v ?
Rngivesa vector Av ?
Rmwith components:(Av)i=n?j=1AijvjWe can also multiply an n?mmatrixA and anm ?
o matrix B to produce an n ?
o matrix ABwith components:(AB)ij=m?k=1AikBkjThe previous examples are some of the mostcommon operations in linear algebra, and they allinvolve sums over repeated indices.
They can besimplified by introducing the Einstein summationconvention: summation over the relevant rangeis implied on every component index that occurs47twice.
Pairs of indices that are summed over areknown as contracted, while the remaining indicesare known as free.
Using this convention, theabove operations can be written as:|v|2= viviv ?w = viwi(Av)i= Aijvj, i.e.
the contraction of v withthe second index of A(AB)ij= AikBkj, i.e.
the contraction of thesecond index of A with the first of BNote how the number of free indices is alwaysconserved between the left- and right-hand sides inthese examples.
For instance, while the last equa-tion has two indices on the left and four on theright, the two extra indices on the right are con-tracted.
Hence counting the number of free indicescan be a quick way of determining what type ofobject is given by a certain mathematical expres-sion in Einstein notation: no free indices meansthat an operation yields a scalar number, one freeindex means a vector, two a matrix, and so on.2.2 TensorsLinear Functionals Given a finite-dimensionalvector space Rnover R, a linear functional is alinear map a : Rn?
R.Let a vector v have components viin a fixed ba-sis.
Then the result of applying a linear functionala to v can be written as:a(v) = a1v1+?
?
?+anvn=(a1?
?
?
an)???v1...vn??
?The numbers aiare the components of the lin-ear functional, which can also be pictured as a rowvector.
Since there is a one-to-one correspondencebetween row and column vectors, the above equa-tion is equivalent to:v(a) = a1v1+?
?
?+anvn=(v1?
?
?
vn)???a1...an??
?Using Einstein convention, the equations abovecan be written as:a(v) = viai= v(a)Thus every finite-dimensional vector is a linearfunctional, and vice versa.
Row and column vec-tors are examples of first-order tensors.Definition 1 (First-order tensor).
Given a vectorspace V over the field R, a first-order tensor Tcan be defined as:?
an element of the vector space V ,?
a linear map T : V ?
R,?
a |V |-dimensional array of numbers Ti, for1 ?
i ?
|V |.These three definitions are all equivalent.
Givena first-order tensor described using one of thesedefinitions, it is trivial to find the two other de-scriptions.Matrices An n?mmatrixA over R can be rep-resented by a two-dimensional array of real num-bers Aij, for 1 ?
i ?
n and 1 ?
j ?
m.Via matrix-vector multiplication, the matrix Acan be seen as a linear map A : Rm?
Rn.
Itmaps a vector v ?
Rmto a vector???A11?
?
?
A1m.........An1?
?
?
Anm??????v1...vm??
?,with componentsA(v)i= Aijvj.We can also contract a vector with the first indexof the matrix, which gives us a map A : Rn?Rm.
This corresponds to the operation(w1?
?
?
wn)???A11?
?
?
A1m.........An1?
?
?
Anm??
?,resulting in a vector with components(wTA)i= Ajiwj.We can combine the two operations and see amatrix as a map A : Rn?
Rm?
R, defined by:wTAv =(w1?
?
?
wn)???A11?
?
?
A1m.........An1?
?
?
Anm??????v1...vm??
?In Einstein notation, this operation can be writ-ten aswiAijvj,48which yields a scalar (constant) value, consistentwith the fact that all the indices are contracted.Finally, matrices can also be characterised interms of Kronecker products.
Given two vectorsv ?
Rnand w ?
Rm, their Kronecker productv ?w is a matrixv ?w =???v1w1?
?
?
v1wm.........vnw1?
?
?
vnwm??
?,with components(v ?w)ij= viwj.It is a general result in linear algebra that anyn ?
m matrix can be written as a finite sum ofKronecker products?kx(k)?
y(k)of a set ofvectors x(k)and y(k).
Note that the sum over kis written explicitly as it would not be implied byEinstein notation: this is because the index k doesnot range over vector/matrix/tensor components,but over a set of vectors, and hence that index ap-pears in brackets.An n ?
m matrix is an element of the tensorspace Rn?Rm, and it can also be seen as a linearmap A : Rn?
Rm?
R. This is because, givena matrix B with decomposition?kx(k)?
y(k),the matrix A can act as follows:A(B) = Aij?kx(k)iy(k)j=?k(x(k)1?
?
?
x(k)n)???A11?
?
?
A1m.........An1?
?
?
Anm??????y(k)...y(k)m??
?= AijBij.Again, counting the number of free indices in thelast line tells us that this operation yields a scalar.Matrices are examples of second-order tensors.Definition 2 (Second-order tensor).
Given vectorspaces V,W over the field R, a second-order ten-sor T can be defined as:?
an element of the vector space V ?W ,?
a |V | ?
|W |-dimensional array of numbersTij, for 1 ?
i ?
|V | and 1 ?
j ?
|W |,?
a (multi-) linear map:?
T : V ?W ,?
T : W ?
V ,?
T : V ?W ?
R or T : V ?W ?
R.Again, these definitions are all equivalent.
Mostimportantly, the four types of maps given in thedefinition are isomorphic.
Therefore specifyingone map is enough to specify all the others.Tensors We can generalise these definitions tothe more general concept of tensor.Definition 3 (Tensor).
Given vector spacesV1, .
.
.
, Vkover the field R, a kth-order tensor Tis defined as:?
an element of the vector space V1?
?
?
??Vk,?
a |V1| ?
?
?
?
?
|Vk|, kth-dimensional array ofnumbers Ti1??
?ik, for 1 ?
ij?
|Vj|,?
a multi-linear map T : V1?
?
?
?
?
Vk?
R.3 Tensor-Based CCG SemanticsIn this section we show how CCG?s syntactic typescan be given tensor-based meaning spaces, andhow the combinator?s employed by CCG to com-bine syntactic categories carry over to those mean-ing spaces, maintaining what is often describedas CCG?s ?transparent interface?
between syntaxand semantics.
Here are some example syntactictypes, and the corresponding tensor spaces con-taining the meanings of the words with those types(using the notation syntactic type : semantic type).We first assume that all atomic types havemeanings living in distinct vector spaces:?
noun phrases, NP : N?
sentences, S : SThe recipe for determining the meaning spaceof a complex syntactic type is to replace eachatomic type with its corresponding vector spaceand the slashes with tensor product operators:?
Intransitive verb, S\NP : S?
N?
Transitive verb, (S\NP)/NP : S?
N?
N?
Ditransitive verb, ((S\NP)/NP)/NP :S?
N?
N?
N?
Adverbial modifier, (S\NP)\(S\NP) :S?
N?
S?
N?
Preposition modifying NP , (NP\NP)/NP :N?
N?
N49Hence the meaning of an intransitive verb, forexample, is a matrix in the tensor product spaceS ?
N. The meaning of a transitive verb is a?cuboid?, or 3rd-order tensor, in the tensor productspace S?N?N.
In the same way that the syntac-tic type of an intransitive verb can be thought of asa function ?
taking an NP and returning an S ?the meaning of an intransitive verb is also a func-tion (linear map) ?
taking a vector in N and re-turning a vector in S. Another way to think of thisfunction is that each element of the matrix spec-ifies, for a pair of basis vectors (one from N andone from S), what the result is on the S basis vec-tor given a value on the N basis vector.Now we describe how the combinatory rulescarry over to the meaning spaces.3.1 ApplicationThe function application rules of CCG are forward(>) and backward (<) application:X/Y Y =?
X (>)Y X\Y =?
X (<)In a traditional semantics for CCG, if functionapplication is applied in the syntax, then functionapplication applies also in the semantics (Steed-man, 2000).
This is also true of the tensor-basedsemantics.
For example, the meaning of a subjectNP combines with the meaning of an intransitiveverb via matrix multiplication, which is equivalentto applying the linear map corresponding to thematrix to the vector representing the meaning ofthe NP .
Applying (multi-)linear maps in (multi-)linear algebra is equivalent to applying tensorcontraction to the combining tensors.
Here is thecase for an intransitive verb:Pat walksNP S\NPN S?
NLet Pat be assigned a vector P ?
N and walksbe assigned a second-order tensor W ?
S ?
N.Using the backward application combinator cor-responds to feeding P , an element of N, into W ,seen as a function N?
S. In terms of tensor con-traction, this is the following operation:WijPj.Here we use the convention that the indicesmaintain the same order as the syntactic type.Therefore, in the tensor of an object of type X/Y ,the first index corresponds to the type X and thesecond to the type Y .
That is why, when perform-ing the contraction corresponding to Pat walks,P ?
N is contracted with the second index ofW ?
S ?
N, and not the first.1The first indexof W is then the only free index, telling us that theabove operation yields a first-order tensor (vector).Since this index corresponds to S, we know thatapplying backward application to Pat walks yieldsa meaning vector in S.Forward application is performed in the samemanner.
Consider the following example:Pat kisses SandyNP (S\NP)/NP NPN S?
N?
N Nwith corresponding tensors P ?
N for Pat, K ?S?
N?
N for kisses and Y ?
N for Sandy.The forward application deriving the type ofkisses Sandy corresponds toKijkYk,where Y is contracted with the third index of Kbecause we have maintained the order defined bythe type (S\NP)/NP : the third index then corre-sponds to an argument NP coming from the right.Counting the number of free indices in theabove expression tells us that it yields a second-order tensor.
Looking at the types correspondingto the free indices tells us that this second-ordertensor is of type S?N, which is the semantic typeof a verb phrase (or intransitive verb), as we havealready seen in the walks example.3.2 CompositionThe forward (>B) and backward (<B) composi-tion rules are:X/Y Y/Z =?
X/Z (>B)Y \Z X\Y =?
X\Z (<B)Composition in the semantics also reduces to aform of tensor contraction.
Consider the followingexample, in which might can combine with kissusing forward composition:Pat might kiss SandyNP (S\NP)/(S\NP) (S\NP)/NP NPN S?
N?
S?
N S?
N?
N N1The particular order of the indices is not important, aslong as a convention such as this one is decided upon andconsistently applied to all types (so that tensor contractioncontracts the relevant tensors from each side when a combi-nator is used).50with tensors M ?
S ?
N ?
S ?
N for might andK ?
S?N?N for kiss.
Combining the meaningsof might and kiss corresponds to the following op-eration:MijklKklm,yielding a tensor in S ?
N ?
N, which is thecorrect semantic type for a phrase with syntactictype (S\NP)/NP .
Backward composition is per-formed analogously.3.3 Backward-Crossed CompositionEnglish also requires the use of backward-crossedcomposition (Steedman, 2000):X/Y Z\X =?
Z/Y (<B?
)In tensor terms, this is the same as forward com-position; we just need to make sure that the con-traction matches up the correct parts of each ten-sor correctly.
Consider the following backward-crossed composition:(S\NP)/NP (S\NP)\(S\NP) ?<B?
(S\NP)/NPLet the two items on the left-hand side be rep-resented by tensors A ?
S ?
N ?
N and B ?S ?
N ?
S ?
N. Then, combining them withbackward-crossed composition in tensor terms isBijklAklm,resulting in a tensor in S ?
N ?
N (correspond-ing to the indices i, j and m).
Note that we havereversed the order of tensors in the contraction tomake the matching of the indices more transpar-ent; however, tensor contraction is commutative(since it corresponds to a sum over products) sothe order of the tensors does not affect the result.3.4 Type-raisingThe forward (>T) and backward (<T) type-raising rules are:X =?
T/(T\X) (>T)X =?
T\(T/X) (<T)where T is a variable ranging over categories.Suppose we are given an item of atomic type Y ,with corresponding vector A ?
Y.
If we applyforward type-raising to it, we get a new tensor oftype A??
T ?
T ?
Y.
Now suppose the item oftype Y is followed by another item of type X\Y ,with tensor B ?
X ?
Y.
A phrase consisting oftwo words with types Y and X\Y can be parsedin two different ways:?
Y X\Y ?
X , by backward application;?
Y X\Y ?TX/(X\Y ) X\Y , by forwardtype-raising, and X/(X\Y ) X\Y ?
X , byforward application.Both ways of parsing this sentence yield an itemof type X , and crucially the meaning of the result-ing item should be the same in both cases.2Thisproperty of type-raising provides an avenue intodetermining what the tensor representation for thetype-raised category should be, since the tensorrepresentations must also be the same:AjBij= A?ijkBjk.Moreover, this equation must hold for all items,B.
As a concrete example, the requirement saysthat a subject NP combining with a verb phraseS\NP must produce the same meaning for thetwo alternative derivations, irrespective of the verbphrase.
This is equivalent to the requirement thatAjBij= A?ijkBjk, ?B ?
X?
Y.So to arrive at the tensor representation, we sim-ply have to solve the tensor equation above.
Westart by renaming the dummy index j on the left-hand side:AkBik= A?ijkBjk.We then insert a Kronecker delta (?ij= 1 if i = jand 0 otherwise):AkBjk?ij= A?ijkBjk.Since the equation holds for allB, we are left withA?ijk= ?ijAk,which gives us a recipe for performing type-raising in a tensor-based model.
The recipe is par-ticularly simple and elegant: it corresponds to in-serting the vector being type-raised into the 3rd-order tensor at all places where the first two in-dices are equal (with the rest of the elements inthe 3rd-order tensor being zero).
For example, totype-raise a subject NP , its meaning vector in N isplaced in the 3rd-order tensor S?S?N at all placeswhere the indices of the two S dimensions are thesame.
Visually, the 3rd-order tensor correspond-ing to the meaning of the type-raised category is2This property of CCG resulting from the use of type-raising and composition is sometimes referred to as ?spuriousambiguity?.51a cubiod in which the noun vector is repeated anumber of times (once for each sentence index),resulting in a series of ?steps?
progressing diag-onally from the bottom of the cuboid to the top(assuming a particular orientation).The discussion so far has been somewhat ab-stract, so to finish this section we include somemore examples with CCG categories, and showthat the tensor contraction operation has an intu-itive similarity with the ?cancellation law?
of cat-egorial grammar which applies in the syntax.First consider the example of a subject NPwith meaning A, combining with a verb phraseS\NP with meaning B, resulting in a sentencewith meaning C. In the syntax, the two NPs can-cel.
In the semantics, for each basis of the sentencespace S we perform an inner product between twovectors in N:Ci= AjBijHence, inner products in the tensor space corre-spond to cancellation in the syntax.This correspondence extends to complex argu-ments, and also to composition.
Consider the sub-ject type-raising case, in which a subject NP withmeaning A in S ?
S ?
N combines with a verbphrase S\NP with meaning B, resulting in a sen-tence with meaning C. Again we perform innerproduct operations, but this time the inner productis between two matrices:3Ci= AijkBjkNote that two matrices are ?cancelled?
for eachbasis vector of the sentence space (i.e.
for eachindex i in Ci).As a final example, consider the forward com-position from earlier, in which a modal verb withmeaningA in S?N?S?N combines with a tran-sitive verb with meaning B in S ?
N ?
N to givea transitive verb with meaning C in S ?
N ?
N.Again the cancellation in the syntax correspondsto inner products between matrices, but this timewe need an inner product for each combination of3 indices:Cijk= AijlmBlmk3To be more precise, the two matrices can be thought ofas vectors in the tensor space S ?
N and the inner product isbetween these vectors.
Another way to think of this opera-tion is to ?linearize?
the two matrices into vectors and thenperform the inner product on these vectors.For each i, j, k, two matrices ?
corresponding tothe l,m indices above ?
are ?cancelled?.This intuitive explanation extends to argumentswith any number of slashes.
For example, acomposition where the cancelling categories are(N /N )/(N /N ) would require inner products be-tween 4th-order tensors in N?
N?
N?
N.4 Related WorkThe tensor-based semantics presented in this pa-per is effectively an extension of the Coecke et al.
(2010) framework to CCG, re-expressing in Ein-stein notation the existing categorical CCG exten-sion in Grefenstette (2013), which itself buildson an earlier Lambek Grammar extension to theframework by Coecke et al.
(2013).This work also bears some similarity to thetreatment of categorial grammars presented by Ba-roni et al.
(2013), which it effectively encompassesby expressing the tensor contractions described byBaroni et al.
as Einstein summations.
However,this paper also covers CCG-specific operations notdiscussed by Baroni et al., such as type-raising andcomposition.One difference between this paper and the orig-inal work by Coecke et al.
(2010) is that they usepregroups as the syntactic formalism (Lambek,2008), a context-free variant of categorial gram-mar.
In pregroups, cancellation in the syntax isalways between two atomic categories (or moreprecisely, between an atomic category and its ?ad-joint?
), whereas in CCG the arguments in complexcategories can be complex categories themselves.To what extent this difference is significant re-mains to be seen.
For example, one area where thismay have an impact is when non-linearities areadded after contractions.
Since the CCG contrac-tions with complex arguments happen ?in one go?,whereas the corresponding pregroup cancellationin the semantics would be a series of contractions,many more non-linearities would be added in thepregroup case.Krishnamurthy and Mitchell (2013) is based ona similar insight to this paper ?
that CCG providescombinators which can manipulate functions op-erating over vectors.
Krishnamurthy and Mitchellconsider the function application case, whereas wehave shown how the type-raising and compositionoperators apply naturally in this setting also.525 ConclusionThis paper provides a theoretical framework forthe development of a compositional distributionalsemantics for CCG.
Given the existence of ro-bust, wide-coverage CCG parsers (Clark and Cur-ran, 2007; Hockenmaier and Steedman, 2002),together with various techniques for learning thetensors, the opportunity exists for a practical im-plementation.
However, there are significant engi-neering difficulties which need to be overcome.Consider adapting the neural-network learningtechniques of Socher et al.
(2012) to this prob-lem.4In terms of the number of tensors, the lexi-con would need to contain a tensor for every word-category pair; this is at least an order of magnitudemore tensors then the number of matrices learnt inexisting work (Socher et al., 2012; Hermann andBlunsom, 2013).
Furthermore, the order of thetensors is now higher.
Syntactic categories such as((N /N )/(N /N ))/((N /N )/(N /N )) are not un-common in the wide-coverage grammar of Hock-enmaier and Steedman (2007), which in this casewould require an 8th-order tensor.
This combina-tion of many word-category pairs and higher-ordertensors results in a huge number of parameters.As a solution to this problem, we are investigat-ing ways to reduce the number of parameters, forexample using tensor decomposition techniques(Kolda and Bader, 2009).
It may also be possi-ble to reduce the size of some of the complex cat-egories in the grammar.
Many challenges remainbefore a type-driven compositional distributionalsemantics can be realised, similar to the work ofBos for the model-theoretic case (Bos et al., 2004;Bos, 2005), but in this paper we have set out thetheoretical framework for such an implementation.Finally, we repeat a comment made earlier thatthe compositional framework makes no assump-tions about the underlying vector spaces, or howthey are to be interpreted.
On the one hand, thisflexibility is welcome, since it means the frame-work can encompass many techniques for buildingword vectors (and tensors).
On the other hand, itmeans that a description of the framework is nec-essarily abstract, and it leaves open the question4Non-linear transformations are inherent to neural net-works, whereas the framework in this paper is entirely linear.However, as hinted at earlier in the paper, non-linear transfor-mations can be applied to the output of each tensor, turningthe linear networks in this paper into extensions of those inSocher et al.
(2012) (extensions in the sense that the tensorsin Socher et al.
(2012) do not extend beyond matrices).of what the meaning spaces represent.
The lat-ter question is particularly pressing in the case ofthe sentence space, and providing an interpretationof such spaces remains a challenge for the distri-butional semantics community, as well as relatingdistributional semantics to more traditional topicsin semantics such as quantification and inference.AcknowledgmentsJean Maillard is supported by an EPSRC MPhilstudentship.
Stephen Clark is supported by ERCStarting Grant DisCoTex (306920) and EPSRCgrant EP/I037512/1.
Edward Grefenstette is sup-ported by EPSRC grant EP/I037512/1.
We wouldlike to thank Tamara Polajnar, Laura Rimell, NalKalchbrenner and Karl Moritz Hermann for usefuldiscussion.ReferencesM.
Baroni and R. Zamparelli.
2010.
Nounsare vectors, adjectives are matrices: Representingadjective-noun constructions in semantic space.
InConference on Empirical Methods in Natural Lan-guage Processing (EMNLP-10), Cambridge, MA.M.
Baroni, R. Bernardi, and R. Zamparelli.
2013.Frege in space: A program for compositional dis-tributional semantics (to appear).
Linguistic Issuesin Language Technologies.Johan Bos, Stephen Clark, Mark Steedman, James R.Curran, and Julia Hockenmaier.
2004.
Wide-coverage semantic representations from a CCGparser.
In Proceedings of COLING-04, pages 1240?1246, Geneva, Switzerland.Johan Bos.
2005.
Towards wide-coverage seman-tic interpretation.
In Proceedings of the Sixth In-ternational Workshop on Computational Semantics(IWCS-6), pages 42?53, Tilburg, The Netherlands.Stephen Clark and James R. Curran.
2007.
Wide-coverage efficient statistical parsing with CCGand log-linear models.
Computational Linguistics,33(4):493?552.Stephen Clark and Stephen Pulman.
2007.
Combiningsymbolic and distributional models of meaning.
InProceedings of AAAI Spring Symposium on Quan-tum Interaction, Stanford, CA.
AAAI Press.Stephen Clark.
2014.
Vector space models of lexicalmeaning (to appear).
In Shalom Lappin and ChrisFox, editors, Handbook of Contemporary Semanticssecond edition.
Wiley-Blackwell.Daoud Clarke.
2012.
A context-theoretic frame-work for compositionality in distributional seman-tics.
Computational Linguistics, 38(1):41?71.53B.
Coecke, M. Sadrzadeh, and S. Clark.
2010.
Math-ematical foundations for a compositional distribu-tional model of meaning.
In J. van Bentham,M.
Moortgat, and W. Buszkowski, editors, Linguis-tic Analysis (Lambek Festschrift), volume 36, pages345?384.Bob Coecke, Edward Grefenstette, and MehrnooshSadrzadeh.
2013.
Lambek vs. Lambek: Functorialvector space semantics and string diagrams for Lam-bek calculus.
Annals of Pure and Applied Logic.R.
Collobert and J. Weston.
2008.
A unified architec-ture for natural language processing: Deep neuralnetworks with multitask learning.
In InternationalConference on Machine Learning, ICML, Helsinki,Finland.James R. Curran.
2004.
From Distributional to Seman-tic Similarity.
Ph.D. thesis, University of Edinburgh.D.R.
Dowty, R.E.
Wall, and S. Peters.
1981.
Introduc-tion to Montague Semantics.
Dordrecht.Katrin Erk.
2012.
Vector space models of word mean-ing and phrase meaning: a survey.
Language andLinguistics Compass, 6(10):635?653.J.
R. Firth.
1957.
A synopsis of linguistic theory 1930-1955.
In Studies in Linguistic Analysis, pages 1?32.Oxford: Philological Society.Edward Grefenstette, Georgiana Dinu, YaoZhongZhang, Mehrnoosh Sadrzadeh, and Marco Baroni.2013.
Multistep regression learning for composi-tional distributional semantics.
In Proceedings ofthe 10th International Conference on ComputationalSemantics (IWCS-13), Potsdam, Germany.Gregory Grefenstette.
1994.
Explorations in Auto-matic Thesaurus Discovery.
Kluwer.Edward Grefenstette.
2013.
Category-TheoreticQuantitative Compositional Distributional Modelsof Natural Language Semantics.
Ph.D. thesis, Uni-versity of Oxford.Karl Moritz Hermann and Phil Blunsom.
2013.
Therole of syntax in vector space models of composi-tional semantics.
Proceedings of ACL, Sofia, Bul-garia, August.
Association for Computational Lin-guistics.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with Combi-natory Categorial Grammar.
In Proceedings of the40th Meeting of the ACL, pages 335?342, Philadel-phia, PA.Julia Hockenmaier and Mark Steedman.
2007.
CCG-bank: a corpus of CCG derivations and dependencystructures extracted from the Penn Treebank.
Com-putational Linguistics, 33(3):355?396.T.
G. Kolda and B. W. Bader.
2009.
Tensor decompo-sitions and applications.
SIAM Review, 51(3):455?500.Jayant Krishnamurthy and Tom M. Mitchell.
2013.Vector space semantic parsing: A framework forcompositional vector space models.
In Proceed-ings of the 2013 ACL Workshop on Continuous Vec-tor Space Models and their Compositionality, Sofia,Bulgaria.Joachim Lambek.
2008.
From Word to Sentence.A Computational Algebraic Approach to Grammar.Polimetrica.T.
K. Landauer and S. T. Dumais.
1997.
A solu-tion to Plato?s problem: the latent semantic analysistheory of acquisition, induction and representationof knowledge.
Psychological Review, 104(2):211?240.Jeff Mitchell and Mirella Lapata.
2008.
Vector-basedmodels of semantic composition.
In Proceedings ofACL-08, pages 236?244, Columbus, OH.Stephen Pulman.
2013.
Distributional semantic mod-els.
In Sadrzadeh Heunen and Grefenstette, editors,Compositional Methods in Physics and Linguistics.Oxford University Press.Hinrich Sch?utze.
1998.
Automatic word sense dis-crimination.
Computational Linguistics, 24(1):97?124.Richard Socher, Christopher D. Manning, and An-drew Y. Ng.
2010.
Learning continuous phraserepresentations and syntactic parsing with recursiveneural networks.
In Proceedings of the NIPS DeepLearning and Unsupervised Feature Learning Work-shop, Vancouver, Canada.Richard Socher, Brody Huval, Christopher D. Man-ning, and Andrew Y. Ng.
2012.
Semantic composi-tionality through recursive matrix-vector spaces.
InProceedings of the Conference on Empirical Meth-ods in Natural Language Processing, pages 1201?1211, Jeju, Korea.Mark Steedman.
2000.
The Syntactic Process.
TheMIT Press, Cambridge, MA.Peter D. Turney and Patrick Pantel.
2010.
Fromfrequency to meaning: Vector space models of se-mantics.
Journal of Artificial Intelligence Research,37:141?188.54
