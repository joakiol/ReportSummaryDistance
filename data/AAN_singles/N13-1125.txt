Proceedings of NAACL-HLT 2013, pages 1051?1060,Atlanta, Georgia, 9?14 June 2013. c?2013 Association for Computational LinguisticsDependency-based empty category detection via phrase structure treesNianwen XueBrandeis UniversityWaltham, MA, USAxuen@brandeis.eduYaqin YangBrandeis UniversityWaltham, MA, USAyaqin@brandeis.eduAbstractWe describe a novel approach to detectingempty categories (EC) as represented in de-pendency trees as well as a new metric formeasuring EC detection accuracy.
The newmetric takes into account not only the positionand type of an EC, but also the head it is adependent of in a dependency tree.
We alsointroduce a variety of new features that aremore suited for this approach.
Tested on a sub-set of the Chinese Treebank, our system im-proved significantly over the best previouslyreported results even when evaluated with thismore stringent metric.1 IntroductionIn modern theoretical linguistics, empty categories(ECs) are an important piece of machinery in repre-senting the syntactic structure of a sentence and theyare used to represent phonologically null elementssuch as dropped pronouns and traces of dislocatedelements.
They have also found their way into large-scale treebanks which have played an important rolein advancing the state of the art in syntactic parsing.In phrase-structure treebanks, ECs have been used toindicate long-distance dependencies, discontinuousconstituents, and certain dropped elements (Marcuset al 1993; Xue et al 2005).
Together with la-beled brackets and function tags, they make up thefull syntactic representation of a sentence.The use of ECs captures some cross-linguisticcommonalities and differences.
For example, whileboth the Penn English TreeBank (PTB) (Marcus etal., 1993) and the Chinese TreeBank (CTB) (Xueet al 2005) use traces to represent the extractionsite of a dislocated element, dropped pronouns (rep-resented as *pro*s) are much more widespread inthe CTB.
This is because Chinese is a pro-drop lan-guage (Huang, 1984) that allows the subject to bedropped in more contexts than English does.
Whiledetecting and resolving traces is important to the in-terpretation of the syntactic structure of a sentence inboth English and Chinese, the prevalence of droppednouns in Chinese text gives EC detection added sig-nificance and urgency.
They are not only an impor-tant component of the syntactic parse of a sentence,but are also essential to a wide range of NLP appli-cations.
For example, any meaningful tracking ofentities and events in natural language text wouldhave to include those represented by dropped pro-nouns.
If Chinese is translated into a different lan-guage, it is also necessary to render these droppedpronouns explicit if the target language does not al-low pro-drop.
In fact, Chung and Gildea (2010) re-ported preliminary work that has shown a positiveimpact of automatic EC detection on statistical ma-chine translation.Some ECs can be resolved to an overt element inthe same text while others only have a generic ref-erence that cannot be linked to any specific entity.Still others have a plausible antecedent in the text,but are not annotated due to annotation limitations.A common practice is to resolve ECs in two separatestages (Johnson, 2002; Dienes and Dubey, 2003b;Dienes and Dubey, 2003a; Campbell, 2004; Gab-bard et al 2006; Schmid, 2006; Cai et al 2011).The first stage is EC detection, where empty cate-gories are first located and typed.
The second stage1051is EC resolution, where empty categories are linkedto an overt element if possible.In this paper we describe a novel approach to de-tecting empty categories in Chinese, using the CTBas training and test data.
More concretely, EC de-tection involves (i) identifying the position of theEC, relative to some overt word tokens in the samesentence, and (ii) determining the type of EC, e.g.,whether it is a dropped pronoun or a trace.
We fo-cus on EC detection here because most of the ECsin the Chinese Treebank are either not resolved toan overt element or linked to another EC.
For ex-ample, dropped pronouns (*pro*) are not resolved,and traces (*T*) in relative clauses are linked to anempty relative pronoun (*OP*).In previous work, ECs are either represented lin-early, where ECs are indexed to the following word(Yang and Xue, 2010) or attached to nodes in aphrase structure tree (Johnson, 2002; Dienes andDubey, 2003b; Gabbard et al 2006).
In a linearrepresentation where ECs are indexed to the follow-ing word, it is difficult to represent consecutive ECsbecause that will mean more than one EC will beindexed to the same word (making the classificationtask more complicated).
While in English consecu-tive ECs are relatively rare, in Chinese this is verycommon.
For example, it is often the case that anempty relative pronoun (*OP*) is followed imme-diately by a trace (*T*).
Another issue with the lin-ear representation of ECs is that it leaves unspecifiedwhere the EC should be attached, and crucial depen-dencies between ECs and other elements in the syn-tactic structure are not represented, thus limiting theutility of this task.In a phrase structure representation, ECs are at-tached to a hierarchical structure and the problemof multiple ECs indexed to the same word token canbe avoided because linearly consecutive ECs may beattached to different non-terminal nodes in a phrasestructure tree.
In a phrase structure framework, ECsare evaluated based on their linear position as wellas on their contribution to the overall accuracy ofthe syntactic parse (Cai et al 2011).In the present work, we propose to look at ECdetection in a dependency structure representation,where we define EC detection as (i) determining itslinear position relative to the following word token,(ii) determining its head it is a dependent of, and (iii)determining the type of EC.
Framing EC detectionthis way also requires a new evaluation metric.
AnEC is considered to be correctly detected if its linearposition, its head, and its type are all correctly de-termined.
We report experimental results that showeven using this more stringent measure, our EC de-tection system achieved performance that improvedsignificantly over the state-of-the-art results.The rest of the paper is organized as follows.
InSection 2, we will describe how to represent ECsin a dependency structure in detail and present ourapproach to EC detection.
In Section 3, we describehow linguistic information is encoded as features.In Section 4, we discuss our experimental setup andpresent our results.
In Section 5, we describe relatedwork.
Section 6 concludes the paper.2 ApproachIn order to detect ECs anchored in a dependencytree, we first convert the phrase structure trees in theCTB into dependency trees.
After the conversion,each word token in a dependency tree, including theECs, will have one and only one head (or parent).We then train a classifier to predict the position andtype of ECs in the dependency tree.
Let W be a se-quence of word tokens in a sentence, and T is syn-tactic parse tree for W , our task is to predict whetherthere is a tuple (h, t, e), such that h and t are word to-kens in W , e is an EC, h is the head of e, and t imme-diately follows e. When EC detection is formulatedas a classification task, each classification instanceis thus a tuple (h, t).
The input to our classifier isT , which can either be a phrase structure tree or adependency tree.
We choose to use a phrase struc-ture tree because phrase structure parsers trained onthe Chinese Treebank are readily available, and wealso hypothesize that phrase structure trees have aricher hierarchical structure that can be exploited asfeatures for EC detection.2.1 Empty categories in the Chinese TreebankAccording to the CTB bracketing guidelines (Xueand Xia, 2000), there are seven different types ofECs in the CTB.
Below is a brief description of theempty categories:1.
*pro*: small pro, used to represent droppedpronouns.10522.
*PRO*: big PRO, used to represent shared el-ements in control structures or elements thathave generic references.3.
*OP*: null operator, used to represent emptyrelative pronouns.4.
*T*: trace left by movement such as topical-ization and relativization.5.
*RNR*: right node raising.6.
*: trace left by passivization and raising.7.
*?
*: missing elements of unknown category.An example parse tree with ECs is shown inFigure 1.
In the example, there are two ECs, anempty relative pronoun (*OP*) and a trace (*T*), acommon syntactic pattern for relative clauses in theCTB.ShanghaiPudongrecentlyissue*OP*involveNNDECdocumentNR NRADVVVVNNDECNPADVPNPNPNPWHNPVPIPCPCPNPVPVPIP"Shanghai Pudong recently enacted 71 regulatory documents involvingthe enconomic field.
"ASPAST*NNQPCDMCLPADJPJJ	regulatoryeconomicfieldMFigure 1: Empty categories in a phrase structure tree2.2 Converting phrase structure to dependencystructureWe convert the phrase structure parses in the CTBto dependency trees using the conversion tool thatgenerated the Chinese data sets for the CoNLL 2009Shared Task on multilingual dependency parsingand semantic role labeling (Hajic?
et al 2009)1.While the Chinese data of CoNLL 2009 Shared Taskdoes not include ECs, the tool has an option of pre-serving the ECs in the conversion process.
As an ex-ample, the dependency tree in Figure 2 is convertedfrom the phrase structure tree in Figure 1, with theECs preserved.1The tool can be downloaded athttp://www.cs.brandeis.edu/ clp/ctb/ctb.html.In previous work EC detection has been formu-lated as a classification problem with the target ofthe classification being word tokens (Yang and Xue,2010; Chung and Gildea, 2010), or constituents ina parse tree (Gabbard et al 2006).
When word to-kens are used as the target of classification, the taskis to determine whether there is an EC before eachword token, and what type EC it is.
One shortcom-ing with that representation is that more than one ECcan precede the same word token, as is the case inthe example in Figure 1, where both *OP* and *T*precede ??
(?involve?).
In fact, (Yang and Xue,2010) takes the last EC when there is a sequence ofECs and as a result, some ECs will never get thechance to be detected.
Notice that this problem canbe avoided in a dependency structure representationif we make the target of classification a tuple thatconsists of the following word token and the head ofthe EC.
From Figure 2, it should be clear that while*OP* and *T* both precede the same word token??
(?involve?
), they have different heads, which are?
(DE) and??
respectively.Dependency-based EC detection also has othernice properties.
For ECs that are arguments of theirverbal head, when they are resolved to some overtelement, the dependency between the referent ofthe EC and its head will be naturally established.This can be viewed as an alternative to the approachadopted by Levy and Manning (2004), where phrasestructure parses are augmented to recover non-localdependencies.
Dependency structures are also easilydecomposable into head/dependency pairs and thismakes the evaluation more straightforward.
Eachclassification instance can be evaluated indepen-dently of other parts of the dependency structure.2.3 One pass vs two passesWith pairs of tokens (h, t) as the classification tar-get, all possible pairs in a sentence will have to beconsidered and there will be a large number of (h,t) tuples that are not associated with an EC, leadingto a highly imbalanced data set.
One can conceivea two-pass scenario where we first make a binarydecision of whether there is an empty category as-sociated with the head in the first pass and then de-termine whether there is an EC associated with thetuple as well as the EC type in the second pass.
Thealternative is to have a one-pass model in which we1053Figure 2: Empty categories in a dependency structure treeadd a NONE category indicating there is no EC as-sociated with the tuple.
With the seven EC typespresented earlier in this section, this will be an eight-way classification problem.
There are reasons for ei-ther model: the one-pass model is simpler but in thetwo-pass model we can bring different sources of in-formation to bear on each sub-problem.
Ultimatelywhich model leads to better accuracy is an empiricalquestion.
We experimented with both models and itturned out that they led to very similar results.
Inthis paper, we report results from the simpler one-pass model.3 FeaturesWe explored a wide range of features, all derivedfrom the phrase structure parse tree (T ).
With eachclassification instance being a tuple (h, t), the ?piv-ots?
for these features are h the head, t the wordtoken following the EC, and p, the word token pre-ceding the EC.
The features we tried fall into sixbroad groups that are all empirically confirmed tohave made a positive contribution to our classifica-tion task.
These are (i) horizontal features, (ii) ver-tical features, (iii) targeted grammatical construc-tions, (iv) head information, (v) transitivity features,and (vi) semantic role features.
We obviously havelooked at features used in previous work on ChineseEC detection, most notably (Yang and Xue, 2010),which has also adopted a classification-based ap-proach, but because we frame our classification taskvery differently, we have to use very different fea-tures.
However, there is a subset of features we usedhere that has at least a partial overlap with their fea-tures, and such features are clearly indicated with ?.3.1 Horizontal featuresThe first group of features we use can be describedas horizontal features that exploit lexical context ofthe head (h), the word token following the EC (t),and the word token before the EC (p) .
These in-clude different combinations of h, t and p, as wellas their parts-of-speech.
They also include variouslinear distance features between h and t. Below isthe full list of lexical features:1.
?The token string representation of h, t and p,as well as their part-of-speech tag (POS).2.
?The POS combination of h and t, the POScombination of t and p.3.
The normalized word distance between h andt, with the values of this feature being same,immediately before, immediately after,near before, and near after, and other.4.
The verb distance between h and t, defined asthe number of verbs that occur between h andt.5.
The comma distance between h and t, definedas the number of commas that occur between hand t.3.2 Vertical featuresVertical features are designed to exploit the hierar-chical structure of the syntactic tree.
Our hierar-chical features are based on the following observa-tions.
An empty category is always located betweenits left frontier and right frontier, anchored by t andp.
Given the lowest common ancestor A of p andt, the right frontier is the path from t to A and theleft frontier is the path from the p to A.
We alsodefine a path feature from h to t, which constrainsthe distance between the EC and its head, just as itconstrains the distance between a predicate and itsargument in the semantic role labeling task (Gildeaand Jurafsky, 2002).
Given the lowest common an-cestor A?
of h and t, the path from h to t is the pathfrom h to A?
and from A?
to t.In Figure 3, assuming that t is ??
(?rapidly?
)and h is ??
(?take off?
), the vertical features ex-1054??capital??structure?DE??optimization?make??Qingdao?one?CL??enterprise*PRO*??rapidly?
?take offNP VPDNP NPIPNPDEGNNVVNPIPNN NNNPNPQPQCLPVPNPADVP VPVVADNRMNN"Theoptimization of the capitalstructure has led to the rapidtake-off ofa host ofenterprises in Qingdao.
"Figure 3: Empty category on the right frontiertracted include:1.
The string representation of the right frontier,AD?ADVP?VP?IP?VP2.
The path from the head t to h,AD?ADVP?VP?VP?VV3.
The path from the head h to A,VV?VP?VP?IP?VP.
Notice there is notalways a path from h to A.The vertical features are really a condensed rep-resentation of a certain syntactic configuration thathelps to predict the presence or absence of an emptycategory as well as the empty category type.
Forexample, the right frontier of *PRO* in Figure3 AD?ADVP?VP?IP?VP represents a subjectlessIP.
Had there been an overt subject in the placeof the *PRO*, the right frontier would have beenAD?ADVP?VP?IP.
Therefore, the vertical featuresare discriminative features that can help detect thepresence or absence of an empty category.3.3 Targeted grammatical constructionsThe third group of features target specific, linguisti-cally motivated grammatical constructions.
The ma-jority of features in this group hinge on the immedi-ate IP (roughly corresponds to S in the PTB) ances-tor of t headed by h. These features are only invokedwhen t starts (or is on the left edge of) the immedi-ate IP ancestor, and they are designed to capture thecontext in which the IP ancestor is located.
This con-text can provide discriminative clues that may helpidentify the types of empty category.
For example,both *pro*s and *PRO*s tend to occur in the sub-ject position of an IP, but the larger context of theIP often determines the exact empty category type.In Figure 3, the IP that has a *PRO* subject is thecomplement of a verb in a canonical object-controlconstruction.
An IP can also be a sentential subject,the complement of a preposition or a localizer (alsocalled postposition in the literature), or the comple-ment in a CP (roughly SBAR in the PTB), etc.
Thesedifferent contexts tend to be associated with differ-ent types of empty categories.
The full list of fea-tures that exploit these contexts include:1.
?Whether t starts an IP2.
?Whether t starts a subjectless IP3.
The left sisters of the immediate IP parent thatt starts4.
The right sisters of the immediate IP parent thatt starts5.
The string representation of the governing verbof the immediate IP parent that t starts6.
Whether the IP started by t is the complementof a localizer phrase7.
Whether the immediate IP parent that t starts isa sentential subject3.4 Head informationMost ECs have a verb as its head, but when there is acoordination VP structure where more than one VPshare an EC subject, only one such verb can be thehead of this EC.
The phrase structure to dependencystructure conversion tool designates the first verb asthe head of the coordinated VP and thus the head ofthe EC subject in the dependency structure.
Otherverbs have no chance of being the head.
We use aVP head feature to capture this information.
It isa binary feature indicating whether a verb can be ahead.3.5 Transitivity featuresA transitivity lexicon has been extracted from theChinese Treebank and it is used to determine thetransitivity value of a word.
A word can betransitive, intransitive, or unknown if it is not averb.
Ditransitive verbs are small in number and arefolded into transitive verbs.
Transitivity features aredefined on h and constrained by word distance: it isonly used when h immediately precedes t. This fea-ture category is intended to capture transitive verbsthat are missing an object.10553.6 Semantic role featuresThere are apparent connections between semanticrole labeling and EC detection.
The task of seman-tic role labeling is typically defined as one of detect-ing and classifying arguments for verbal or nomi-nal predicates, with more work done so far on ver-bal than nominal predicates.
Although empty cat-egories are annotated as arguments to verbal pred-icates in linguistic resources such as the English(Palmer et al 2005) and Chinese (Xue and Palmer,2009) Propbanks, they are often left out in seman-tic role labeling systems trained on these resources.This is because the best performing semantic role la-beling systems rely on syntactic features extractedfrom automatic parses (Gildea and Palmer, 2002;Punyakanok et al 2005) and the parsers that pro-duce them do not generally reproduce empty cate-gories.
As a result, current semantic role labelingsystems can only recover explicit arguments.
How-ever, assuming that all the explicit arguments to apredicate are detected and classified, one can inferthe empty arguments of a predicate from its explicitarguments, given a list of expected arguments forthe predicate.
The list of expected arguments canbe found in the ?frame files?
that are used to guideprobank annotation.
We defined a semantic role fea-ture category on h when it is a verb and the value ofthis feature is the semantic role labels for the EC ar-guments.
Like transitivity features, this feature cate-gory is also constrained by word distance.
It is onlyused when h immediately precedes t.To extract semantic role features, we retrained aChinese semantic role labeling system on the Chi-nese Propbank.
We divided the Chinese Propbankdata into 10 different subsets, and automatically as-signed semantic roles to each subset with a systemtrained on the other nine subsets.
Using the framefiles for the Chinese Propbank, we are able to inferthe semantic roles for the missing arguments and usethem as features.4 Experimental Results4.1 Experimental setupOur EC detection models are trained and evaluatedon a subset of the Chinese TreeBank 6.0.
The train-ing/development/test data split in our experimentsis recommended in the CTB documentation.
TheCTB file IDs for training, development and testingare listed in Table 1.
The development data is usedfor feature selection and tuning, and results are re-ported on the test set.Train Dev Test81-325, 400-454, 500-554 41-80 1-40590-596, 600-885, 900 901-931Table 1: Data set division.As discussed in Section 2, the gold standard de-pendency structure parses are converted from theCTB parse trees, with the ECs preserved.
Fromthese gold standard parse trees, we extract triples of(e, h, t) where e is the EC type, h is (the position of)the head of the EC, and t is (the position of) the wordtoken following the EC.
During the training phrase,features are extracted from automatic phrase struc-ture parses and paired with these triples.
The au-tomatic phrase structure parses are produced by thethe Berkeley parser2 with a 10-fold cross-validation,which each fold parsed using a model trained on theother nine folds.
Measured by the ParsEval met-ric (Black et al 1991), the parsing accuracy onthe CTB test set stands at 83.63% (F-score), witha precision of 85.66% and a recall of 81.69%.
Wechose to train a Maximum Entropy classifier usingthe Mallet toolkit3 (McCallum, 2002) to detect ECs.4.2 Evaluation metricWe use standard metrics of precision, recall and F-measure in our evaluation.
In a dependency struc-ture representation, evaluation is very straightfor-ward because individual arcs from the dependencytree can be easily decomposed.
An EC is consideredto be correctly detected if it is attached to the correcthead h, correctly positioned relative to t, and cor-rectly typed.
This is a more stringent measure thanmetrics proposed in previous work, which evaluatesEC detection based on its position and type withoutconsidering the head it is a dependent of.4.3 ResultsThere are 1,838 total EC instances in the test set, andif we follow (Yang and Xue, 2010) and collapse all2http://code.google.com/p/berkeleyparser3http://mallet.cs.umass.edu1056consecutive ECs before the same word token to one,we will end up with a total EC count of 1,352, andthis is also the EC count used by (Cai et al 2011)in their evaluation.
On the dependency-based repre-sentation adopted here, after collapsing all consecu-tive ECs before the same word token AND attachedto the same head to one, we end up with a total ECcount of 1,765.
The distribution of the ECs in thetest set are presented in Table 2, with the EC countper type from (Yang and Xue, 2010) in parenthesisif it is different.
The number of *OP*s, in particular,has increased dramatically from 134 to 527, and thisis because a null relative pronoun (*OP*) immedi-ately followed by a trace (*T*) in the subject posi-tion of a relative clause is a very common pattern inthe Chinese Treebank, as illustrated in Figure 2.
In(Yang and Xue, 2010), the *OP*-*T* sequences arecollapsed into one, and only the *T*s are counted.That leads to the much smaller count of *OP*s.type count type count*pro* 298 (290) *PRO* 305 (299)*OP* 527 (134) *T* 584 (578)* 19 *RNR* 32*?
* 0 total (1352)/1765/(1838)Table 2: EC distribution in the CTB test setOur results are shown in Table 3.
These resultsare achieved by using the full feature set presentedin Section 3.
The overall accuracy by F1-measure is0.574 if we assume there can only be one EC asso-ciated with a given (h, t) tuple and hence the totalEC count in the gold standard is 1,765, or 0.561 ifwe factor in all the EC instances and use the highertotal count of 1,838, which lowers the recall.
If in-stead we use the total EC count of 1,352 that wasused in previous work (Yang and Xue, 2010; Cai etal., 2011), then the F1-measure is 0.660 because thelower total count greatly improves the recall.
Thisis a significant improvement over the best previousresult reported by Cai et al2011), which is an F1measure of 0.586 on the same test set but based ona less stringent metric of just comparing the EC po-sition and type, without considering whether the ECis attached to the correct head.There are several observations worth noting fromthese results.
One is that our method performs par-ticularly well on null relative pronouns (*OP*) andclass correct prec rec F1*pro* 46 .397 .154 .222*PRO* 162 .602 .531 .564*OP* 344 .724 .653 .687*T* 331 .673 .567 .615* 0 0 0 0*RNR* 20 .714 .625 .667all 903 .653.512 .574(.491) (.561)(.668) (.660)CCG .660 .545 .586Table 3: EC detection results on the CTB test set andcomparison with (Cai et al 2011) [CCG]traces (*T*), indicating that our features are effec-tive in capturing information from relative clauseconstructions.
This accounts for most of the gaincompared with previous approaches.
The *OP* cat-egory, in particular, benefits most from the depen-dency representation because it is collapsed to theimmediately following *T* in previous approachesand does not even get a chance to be detected.
Onthe other hand, our model did poorly on droppedpronouns (*pro*).
One possible explanation is that*pro*s generally occupy subject positions in a sen-tence and is attached as an immediate child of anIP, which is the top-level structure of a sentencethat an automatic parser tends to get wrong.
Unlike*PRO*, it is not constrained to well-defined gram-matical constructions such as subject- and object-control structures.To evaluate the effectiveness of our features, wealso did an ablation study on the contribution of dif-ferent feature groups.
The most effective featuresare the ones when taken out lead to the most drop inaccuracy.
As should be clear from Table 4, the mosteffective features are the horizontal features, fol-lowed by vertical structures.
Features extracted fromtargeted grammatical constructions and features rep-resenting whether h is the head of a coordinated VPlead to modest improvement.
Transitivity and se-mantic role features make virtually no difference atall.
We believe it is premature to conclude that theyare not useful.
Possible explanations for their lackof effectiveness is that they are used in very limitedcontext and the accuracy of the semantic role label-1057ing system is not sufficient to make a difference.class correct prec rec F1all 903 .653 .512 .574 (.561)-Horizontal 827 .627 .469 .536 (.524)-Vertical 865 .652 .490 .559 (.547)-Gr Cons 887 .646 .483 .565 (.552)-V head 891 .651 .505 .569 (.556)-Trans 899 .654 .509 .573 (.560)-SRL 900 .657 .510 .574 (.561)Table 4: Contribution of feature groups5 Related WorkThe work reported here follows a fruitful line of re-search on EC detection and resolution, mostly inEnglish.
Empty categories have initially been leftbehind in research on syntactic parsing (Collins,1999; Charniak, 2001) for efficiency reasons, butmore recent work has shown that EC detection canbe effectively integrated into the parsing process(Schmid, 2006; Cai et al 2011).
In the meantime,both pre-processing and post-processing approacheshave been explored in previous work as alternatives.Johnson (2002) has showed that empty categoriescan be added to the skeletal parses with reasonableaccuracy with a simple pattern-matching algorithmin a postprocessing step.
Dienes and Dubey (2003b;2003a) achieved generally superior accuracy using amachine learning framework without having to referto the syntactic structure in the skeletal parses.
Theydescribed their approach as a pre-processing step forparsing because they only use as features morpho-syntactic clues (passives, gerunds and to-infinitives)that can be found in certain function words and part-of-speech tags.
Even better results, however, wereobtained by Campbell (2004) in a postprocessingstep that makes use of rules inspired by work in theo-retical linguistics.
Gabbard et al2006) reported fur-ther improvement largely by recasting the Campbellrules as features to seven different machine learningclassifiers.We adopted a machine-learning based postpro-cessing approach based on insights gained fromprior work in English and on Chinese-specific con-siderations.
All things being equal, we believe thata machine learning approach that can exploit partialinformation is more likely to succeed than determin-istic rules that have to make reference to morpho-syntactic clues such as to-infinitives and gerunds thatare largely non-existent in Chinese.
Without theseclues, we believe a preprocessing approach that doesnot take advantage of skeletal parses is unlikely tosucceed either.
The work we report here also buildson emerging work in Chinese EC detection.
Yangand Xue (2010) reported work on detecting just thepresence and absence of empty categories withoutfurther classifying them.
Chung and Gildea (2010)reported work on just detecting just a small subsetof the empty categories posited in the Chinese Tree-Bank.
Kong and Zhou (2010) worked on Chinesezero anaphora resolution, where empty category de-tection is a subtask.
More recently, Cai et al2011)has successfully integrated EC detection into phrase-structure based syntactic parsing and reported state-of-the-art results in both English and Chinese.6 Conclusions and Future WorkWe described a novel approach to detecting emptycategories (EC) represented in dependency trees anda new metric for measuring EC detection accuracy.The new metric takes into account not only the po-sition and type of an EC, but also the head it is adependent of in a dependency structure.
We alsoproposed new features that are more suited for thisnew approach.
Tested on a subset of the ChineseTreebank, we show that our system improved signif-icantly over the best previously reported results de-spite using a more stringent evaluation metric, withmost of the gain coming from an improved represen-tation.
In the future, we intend to work toward re-solving ECs to their antecedents when EC detectioncan be done with adequate accuracy.
We also plan totest our approach on the Penn (English) Treebank,with the first step being converting the Penn Tree-bank to a dependency representation with the ECspreserved.AcknowledgmentsThis work is supported by the National Sci-ence Foundation via Grant No.
0910532 enti-tled?Richer Representations for Machine Transla-tion?.
All views expressed in this paper are thoseof the authors and do not necessarily represent the1058view of the National Science Foundation.ReferencesE.
Black, S. Abney, D. Flickinger, C. Gdaniec, R. Gr-ishman, P. Harrison, D. Hindle, R. Ingria, F. Jelinek,J.
Klavans, M. Liberman, M. Marcus, S. Roukos,B.
Santorini, and T. Strzalkowski.
1991.
A proce-dure for quantitively comparing the syntactic coverageof English grammars.
In Proceedings of the DARPASpeech and Natural Language Workshop, pages 306?311.Shu Cai, David Chiang, and Yoav Goldberg.
2011.Language-independent parsing with empty elements.In Proceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics: Human Lan-guage Technologies, pages 212?216, Portland, Ore-gon, USA, June.
Association for Computational Lin-guistics.Richard Campbell.
2004.
Using linguistic principlesto recover empty categories.
In Proceedings of the42nd Annual Meeting on Association For Computa-tional Linguistics.E.
Charniak.
2001.
Immediate-head Parsing for Lan-guage Models.
In ACL-01.Tagyoung Chung and Daniel Gildea.
2010.
Effects ofempty categories on machine translation.
In Proceed-ings of the 2010 Conference on Empirical Methods inNatural Language Processing, pages 636?645, Cam-bridge, MA.Michael Collins.
1999.
Head-driven Statistical Modelsfor Natural Language Parsing.
Ph.D. thesis, Univer-sity of Pennsylvania.Pe?ter Dienes and Amit Dubey.
2003a.
Antecendant Re-covery: Experiments with a Trace Tagger.
In Proceed-ings of the Conference on Empirical Methods in Natu-ral Language Processing , Sapporo, Japan.Pe?ter Dienes and Amit Dubey.
2003b.
Deep syntacticprocessing by combining shallow methods.
In Pro-ceedings of the 41st Annual Meeting of the Associationfor Computational Linguistics, Sapporo, Japan.Ryan Gabbard, Seth Kulick, and Mitchell Marcus.
2006.Fully parsing the penn treebank.
In Proceedings ofHLT-NAACL 2006, pages 184?191, New York City.D.
Gildea and D. Jurafsky.
2002.
Automatic label-ing for semantic roles.
Computational Linguistics,28(3):245?288.Dan Gildea and Martha Palmer.
2002.
The Necessityof Parsing for Predicate Argument Recognition.
InProceedings of the 40th Meeting of the Association forComputational Linguistics, Philadelphia, PA.Jan Hajic?, Massimiliano Ciaramita, Richard Johans-son, Daisuke Kawahara, Maria Anto`nia Mart?
?, Llu?
?sMa`rquez, Adam Meyers, Joakim Nivre, SebastianPado?, Jan S?te?pa?nek, Pavel Stran?a?k, Mihai Surdeanu,Nianwen Xue, and Yi Zhang.
2009.
The CoNLL-2009 shared task: Syntactic and semantic depen-dencies in multiple languages.
In Proceedings ofthe 13th Conference on Computational Natural Lan-guage Learning (CoNLL-2009), June 4-5, Boulder,Colorado, USA.James C.T.
Huang.
1984.
On the distribution and refer-ence of empty pronouns.
Linguistics Inquiry, 15:531?574.Mark Johnson.
2002.
A simple pattern-matching al-gorithm for recovering empty nodes and their an-tecedents.
In Proceedings of the 40th Annual Meetingof the Association for Computational Linguistics.Fang Kong and Guodong Zhou.
2010.
A Tree Kernel-based unified framework for Chinese zero anaphoraresolution.
In Proceedings of the 2010 Conference onEmpirical Methods in Natural Language Processing,MIT, Massachusetts.Roger Levy and Christopher Manning.
2004.
Deep de-pendencies from context-free statistical parsers: cor-recting the surface dependency approximation.
In Pro-ceedings of the ACL.M.
Marcus, B. Santorini, and M. A. Marcinkiewicz.1993.
Building a Large Annotated Corpus of English:the Penn Treebank.
Computational Linguistics.Andrew Kachites McCallum.
2002.
Mal-let: A machine learning for language toolkit.http://mallet.cs.umass.edu.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated corpus ofsemantic roles.
Computational Linguistics, 31(1):71?106.Vasin Punyakanok, Dan Roth, and W. Yih.
2005.
TheNecessity of Syntactic Parsing for Semantic Role La-beling.
In Proceedings of IJCAI-2005, pages 1124?1129, Edinburgh, UK.Helmut Schmid.
2006.
Trace prediction and recoverywith unlexicalized PCFGs and slash features.
In Procof ACL.Nianwen Xue and Martha Palmer.
2009.
Adding seman-tic roles to the Chinese Treebank.
Natural LanguageEngineering, 15(1):143?172.Nianwen Xue and Fei Xia.
2000.
The Bracketing Guide-lines for Penn Chinese Treebank Project.
TechnicalReport IRCS 00-08, University of Pennsylvania.Nianwen Xue, Fei Xia, Fu dong Chiou, and MarthaPalmer.
2005.
The Penn Chinese TreeBank: PhraseStructure Annotation of a Large Corpus.
Natural Lan-guage Engineering, 11(2):207?238.Yaqin Yang and Nianwen Xue.
2010.
Chasing the Ghost:Recovering Empty Categories in the Chinese Tree-1059bank.
In Proceedings of the 23rd International Con-ference on Computational Linguistics (COLING), Bei-jing, China.1060
