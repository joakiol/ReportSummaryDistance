Proceedings of the Fourth Linguistic Annotation Workshop, ACL 2010, pages 20?28,Uppsala, Sweden, 15-16 July 2010. c?2010 Association for Computational LinguisticsAnnotation Scheme for Social Network Extraction from TextApoorv AgarwalComputer Science DepartmentColumbia UniversityNew York, U.S.A.apoorv@cs.columbia.eduOwen RambowCCLSColumbia UniversityNew York, U.S.A.rambow@ccls.columbia.eduRebecca J. PassonneauCCLSColumbia UniversityNew York, U.S.A.becky@cs.columbia.eduAbstractWe are interested in extracting social net-works from text.
We present a novel an-notation scheme for a new type of event,called social event, in which two peopleparticipate such that at least one of themis cognizant of the other.
We compareour scheme in detail to the ACE scheme.We perform a detailed analysis of inter-annotator agreement, which shows thatour annotations are reliable.1 IntroductionOur task is to extract a social network from writtentext.
The extracted social network can be used forvarious applications such as summarization, ques-tion answering, or the detection of main charac-ters in a story.
We take a ?social network?
to bea network consisting of individual human beingsand groups of human beings who are connectedto each other through various relationships by thevirtue of participating in events.
A text can de-scribe a social network in two ways: explicitly, bystating the type of relationship between two indi-viduals (Example ??
); or implicitly, by describ-ing an event which creates or perpetuates a so-cial relationship (Example 2).
We are interested inthe implicit description of social relations throughevents.
We will call these types of events so-cial events.
Crucially, many social relations aredescribed in text largely implicitly, or even en-tirely implicitly.
This paper presents an annotationproject for precisely such social events.To introduce the terminology and conventionswe use throughout the paper, consider the follow-ing Example 2.
In this example, there are twoentities: Iraqi officials and Timothy McVeigh.These entities are present in text as nominaland named entity mentions respectively (within[.
.
.]).
Furthermore, these entities are related byan event, whose type we call INR.NONVERBAL-NEAR (a non-verbal interaction that occurs inphysical proximity), and whose textual mention isthe extent (or span of text) provided money andtraining.1(1) [[Sharif]?s {wife} Tahari Shad Tabussum],27, (.
.
.)
made no application for bail at thecourt, according to local reports.
PER-SOC(2) The suit claims [Iraqi officials] {providedmoney and training} to [convicted bomberTimothy McVeigh] (.
.
.)
INR.Nonverbal-NearOne question that immediately comes to mindis how would these annotations be useful?
Letus consider the problem of finding the hierarchyof people in the Enron Email corpus (Klimt andYang, 2004; Diesner et al, 2005).
Much work tosolve this problem has focused on using social net-work analysis algorithms for calculating the graphtheoretical quantities (like degree centrality, clus-tering coefficient (Wasserman and Faust, 1994))of people in the email sender-receiver network(Rowe et al, 2007).
Attempts have been made toincorporate the content of emails usually by us-ing topic modeling techniques (McCallum et al,2007; Pathak et al, 2008).
These techniques con-sider a distribution of words in emails to classifythe interaction between people into topics and thencluster together people that talk about the sametopic.
Researchers also map relationships amongindividuals based on their patterns of word usein emails (Keila and Skillicorn, 2005).
But thesetechniques do not attempt to create an accurate so-cial network in terms of interaction or cognitivestates of people.
In comparison, our data allows1Throughout this paper we will follow this representationscheme for examples ?
entity mentions will be enclosed insquare brackets [.
.
.]
and relation mentions will be enclosedin set brackets {.
.
.}20Sender?
Receiver Email contentKate?
Sam [Jacob], the City attorney had a couple of questions which [I] will {attempt torelay} without having a copy of the documents.Sam?
Kate, Mary Can you obtain the name of Glendale?s bond counsel (lawyer?s name, phonenumber, email, etc.)?Kate?
Sam Glendale?s City Attorney is Jacob.
Please let [me] {know} if [you] need any-thing else.Mary?
Sam I do not see a copy of an opinion in the file nor have we received one since [I]{sent} the execution copies of the ISDA to [Jacob].Kate?
Jacob Jacob, could you provide the name, phone number, etc.
of your bond councilfor our attorney, Sam?Kate?
Sam [I] will {work on this for} [you] - and will be in touch.Figure 1: An email thread from the Enron Email Corpus.
(For space concerns some part of the conversation is removed.
Themissing conversation does not affect our discussion.
)KateSamMaryJacobFigure 2: Network formed by considering email exchangesas links.
Identical color or shape implies structural equiva-lence.
Only Sam and Mary are structurally equivalentfor such a technique to be created.
This is becauseour annotations capture interactions described inthe content of the email such as face-to-face meet-ings, physical co-presence and cognizance.To explore if this is useful, we analyzed an En-ron thread which is presented in Figure 1.
Fig-ure 2 shows the network formed when only theemail exchange is considered.
It is easy to seethat Sam and Mary are structurally equivalent andthus have the same role and position in the so-cial network.
When we analyze the content of thethread, a link gets added between Mary and Ja-cob since Mary in her email to Sam talks aboutsending something to Jacob.
This link changesthe roles and positions of people in the network.
Inthe new network, Figure 3, Kate and Mary appearstructurally equivalent to each other, as do Samand Jacob.
Furthermore, Mary now emerges asa more important player than the email exchangeon its own suggests.
This rather simple example isan indication of the degree to which a link may af-fect the social network analysis results.
In emailswhere usually a limited number of people are in-volved, getting an accurate network seems to becrucial to the hierarchal analysis.There has been much work in the past on an-KateSamMaryJacobFigure 3: Network formed by augmenting the email ex-change network above with links that occur in the content ofthe emails.
Now, Kate and Mary are structurally equivalent,as are Sam and Jacob.notating entities, relations and events in free text,most notably the ACE effort (Doddington et al,2004).
We intend to leverage this work as muchas possible.
The task of social network extrac-tion can be broadly divided into 3 tasks: 1) en-tity extraction; 2) social relation extraction; 3) so-cial event extraction.
We are only interested in thethird task, social event extraction.
For the first twotasks, we can simply use the annotation guidelinesdeveloped by the ACE effort.
Our social events,however, do not clearly map to the ACE events:we introduce a comprehensive set of social eventswhich are very different from the event annotationthat already exists for ACE.
This paper is about theannotation of social events.The structure of the paper is as follows.
In Sec-tion 2 we present a list of social relations that weannotate.
We also talk about some design deci-sions and explain why we took them.
We com-pare this annotation to existing annotation, notablythe ACE annotation, in Section 3.
In section 4we present the procedure of annotation.
Section 5gives details of our inter-annotator agreement cal-culation procedure and shows the inter-annotatoragreement on our task.
We conclude in section 621and mention future direction of research.2 Social Event AnnotationIn this section we define the social events that theannotators were asked to annotate.
Here, we areinterested in the meaning of the annotation; de-tails of the annotation procedure can be found inSection 4.
Note that in this annotation effort, wedo not consider issues related to the truth of theclaims made in the text we are analyzing ?
weare interested in finding social events whether theyare claimed as being true, presented as specula-tion, or presented as wishful thinking.
We assumethat other modules will be able to determine thefactive status of the described social events, andthat social events do not differ from other types ofevents in this respect.A social event is an event in which two or moreentities relate, communicate or are associated suchthat for at least one participant, the interaction isdeliberate and conscious.
Put differently, at leastone participant must be aware of relating to theother participant.
In this definition, what consti-tutes a social relation is an aspect of cognitivestate: an agent is aware of being in a particular re-lation to another agent.
While two people passingeach other on a street without seeing each othermay be a nice plot device in a novel, it is not asocial event in our sense, since it does not entail asocial relation.Following are the four types of social events thatwere annotated:2Interaction event (INR): When both entitiesparticipating in an event have each other in theircognitive state (i.e., are aware of the social re-lation) we say they have an INR relation.
Therequirement is actually deeper: it extends to thetransitive closure under mutual awareness, what inthe case of belief is called ?mutual belief?.
AnINR event could either be of sub-type VERBAL orNONVERBAL.
Note that a verbal interaction eventdoes not mean that all participants must activelycommunicate verbally, it is enough if one partic-ipant communicates verbally and the others areaware of this communication.3 Furthermore, theinteraction can be in physical proximity or from adistance.
Therefore, we have further subtypes of2Details of the annotation guidelines can be found in theunpublished annotation manual, which we will refer to in thefinal version of the paper.3For this reason we explicitly annotate legal events asVERBAL because legal interactions usually involve wordsINR relation: NEAR and FAR.
In all, INR hasfour subtypes: VERBAL-NEAR, VERBAL-FAR,NONVERBAL-NEAR, NONVERBAL-FAR.
Con-sider the following Example (3).
In this sen-tence, our annotators recorded an INR.VERBAL-FAR between entities Toujan Faisal and the com-mittee.
(3) [Toujan Faisal], 54, {said} [she] was{informed} of the refusal by an [Inte-rior Ministry committee] overseeing electionpreparations.
INR.Verbal-FarAs is intuitive, if one person informs the otherabout something, both have to be cognizant ofeach other and of the informing event.
Also, theevent of informing involves words, therefore, it is averbal interaction.
From the context it is not clearif Toujan was informed personally, in which caseit would be a NEAR relation, or not.
We decidedto default to FAR in case the physical proximity isunclear from the context.
We decided this because,on observation, we found that if the author of thenews article was reporting an event that occurredin close proximity, the author would explicitly sayso or give an indication.
INR is the only relationwhich is bi-directional.Cognition event (COG): When only one person(out of the two people that are participating in anevent) has the other in his or her cognitive state,we say there exists a cognition relationship be-tween entities.
Consider the aforementioned Ex-ample (3).
In this sentence, the event said marksa COG relation between Toujan Faisal and thecommittee.
This is because, when one persontalks about the other person, the other person mustbe present in the first person?s cognitive state.COG is a directed event from the entity whichhas the other entity in its cognitive state to theother entity.
In the example under consideration,it would be from Toujan Faisal to the committee.There are no subtypes of this relation.Physical Proximity event (PPR): We record aPPR event when both the following conditionshold: 1) exactly one entity has the other entity intheir cognitive state (this is the same requirementas that for COG) and 2) both the entities arephysically proximate.
Consider the followingExample (4).
Here, one can reasonably assumethat Asif Muhammad Hanif was aware of beingin physical proximity to the three people killed,while the inverse was not necessarily true.22(4) [Three people] were killed when (.
.
.
), [AsifMuhammad Hanif], (.
.
.
), {detonated explo-sives strapped to [his] body} PPRPPR is a directed event like COG.
There are nosubtypes of this relation.
Note that if there existsa PPR event then of course there would also bea COG event.
In such cases, the PPR event sub-sumes COG, and we do not separately record aCOG event.Perception event (PCR): The Perception Rela-tionship is the distant equivalent of the Physi-cal Proximity event.
The point is not physicaldistance; rather, the important ingredient is theawareness required for PPR, except that physicalproximity is not required, and in fact physical dis-tance is required.
This kind of relationship usuallyexists if one entity is watching the other entity onTV broadcast, listening to him or her on the radioor using a listening device, or reading about theother entity in a newspaper or magazine etc.
Con-sider the following Example (5).
In this example,we record a PCR relation between the pair andthe Nepalese babies.
This is because, the babiesare of course not aware of the pair.
Moreover, thepair heard about the babies so there is no physicalproximity.
It is not COG because there was an ex-plicit external information source which broughtthe babies to the attention of the pair.
(5) [The pair] flew to Singapore last year af-ter {hearing} of the successful surgery on[Nepalese babies] [Ganga] and [JamunaShrestha], (.
.
.).
PCRPCR is a directed event like COG.
There are nosubtypes of this relation.
Note that if there existsa PCR event then we do not separately record aCOG event.Figure 4 represents the series of decisions thatan annotator is required to take before reaching aterminal node (or an event annotation label).
Theinterior nodes of the tree represent questions thatannotators answer to progress downwards in thetree.
Each question has a binary answer.
For ex-ample, the first question the annotators answer toget to the type and subtype of an event is: ?Isthe relation directed (1-way) or bi-directional (2-way)??
Depending on the answer, they move tothe left or the right in the tree respectively.
If its a2-way relation, then it has to one of the sub-typesof INR because only INR requires that both enti-ties be aware of each other.?Event?Present?Event?Absent?Verbal?2-??Way?Nonverbal?1-??Way?Mind?Far??????Near?Near?
Far?Near???
?Far?Figure 4: Tree representation of decision points for select-ing an event type/subtype out of the list of social events.
Eachdecision point is numbered for easy reference.
We refer tothese number later when we present our results.
The num-bers in braces ([.
.
.])
are the number of examples that reach adecision point.3 Comparison Between Social Eventsand ACE AnnotationsIn this section, we compare our annotationswith existing annotation efforts.
To the best ofour knowledge, no annotation effort has beengeared towards extracting social events, or to-wards extracting expressions that convey socialrelations in text.
The Automated Content Ex-traction (ACE) annotations are the most similarto ours because ACE also annotates Person Enti-ties (PER.Individual, PER.Group), Relations be-tween people (PER-SOC), and various types ofEvents.
Our annotation scheme is different, how-ever, because the focus of our event annotation ison events that occur only between people.
Fur-thermore, we annotate text that expresses the cog-nitive states of the people involved, or allows theannotator to infer it.
Therefore, at the top levelof classification we differentiate between eventsin which only one entity is cognizant of the otherversus events when both entities are cognizant ofeach other.
This distinction is, we believe, novelin event or relation annotation.
In the remainderof this section, we will present statistics and de-tailed examples to highlight differences betweenour event annotations and the ACE event annota-tions.The statistics we present are based on 62 docu-ments from the ACE-2005 corpus that one of ourannotator also annotated.4 Since our event typesand subtypes are not directly comparable to the4Due to space constraints we do not give statistics for theother annotator.23ACE event types, we say there is a ?match?
whenboth the following conditions hold:1.
The span of text that represents an event inthe ACE event annotations overlap with ours.2.
The entities participating in the ACE eventare same as the entities participating in ourevent.5Our annotator recorded a total of 212 eventsin 62 documents.
We found a total of 63 can-didate ACE events that had at least two Per-son entities involved.
Out of these 63 candi-date events, 54 match both the aforementionedconditions and hence our annotations.
A clas-sification of all of the events (those found byour annotators and the ACE events involving atleast two persons) into our social event categoriesand into the ACE categories is given in Fig-ure 5.
The figure shows that the majority of so-cial events that match the ACE events are of typeINR.VERBAL-NEAR.
On analysis, we found thatmost of these correspond to the ACE type/subtypeCONTACT.MEET.
It should be noted, how-ever, our type/subtype INR.VERBAL-NEAR has abroader definition than ACE type/subtype CON-TACT.MEET, as will become apparent later in thissection.
In the following, we discuss the 9 ACEevents that are not social events, and then we dis-cuss the 158 social events that are not ACE events.Out of the nine candidate ACE events which didnot match our social event annotation, we foundfive are our annotation errors, i.e.
when we an-alyzed manually and looked for ACE events thatdid not correspond to our annotations, we foundthat our annotator missed these events.
The re-maining four, in contrast, are useful for our dis-cussion because they highlight the differences inACE and our annotation perspectives.
This willbecome clearer with the following example:(6) In central Baghdad, [a Reuters cameraman]and [a cameraman for Spain?s Telecinco]died when an American tank fired on thePalestine HotelACE has annotated the above example as anevent of type CONFLICT-ATTACK in which thereare two entities that are of type person: theReuters cameraman and the cameraman for5Recall that our event annotations are between exactlytwo entities of type PER.Individual or PER.Group.Spain?s Telecinco, both of which are argumentsof type ?Victim?.
Being an event that has two per-son entities involved makes the above sentence avalid candidate (or potential) ACE event that wematch with our annotations.
However, it fails tomatch our annotations, since we do not annotatean event in this sentence.
The reason is that thisexample does not reveal the cognitive states of thetwo entities ?
we do not know whether one wasaware of the other.We now discuss social events that are not ACEevents.
From Figure 5 we see that most of theevents that did not overlap with ACE event anno-tations were Cognition (COG) social events.
Inthe following, our annotator records a COG rela-tion between Digvijay Singh and Abdul Kalam(also Atal Behari Vajpayee and Varuna).
Thereason is that by virtue of talking about the twoentities, Digvijay Singh?s cognitive state containsthose entities.
However, the sentence does not re-veal the cognitive states of the other two entitiesand therefore it is not an INR event.
In contrast,ACE does not have any event annotation for thissentence.
(7) The Times of India newspaper quoted [Digvi-jay Singh] as {saying} that [Prime MinisterAtal Behari Vajpayee] and [President AbdulKalam] had offended [the Hindu rain GodVaruna] by remaining bachelors.
COGIt is easy to see why COG relations are not usu-ally annotated as ACE events.
But it is counter-intuitive for INR social events not to be annotatedas ACE events.
We explain this using Example (3)in Section 2.
Our annotator recorded an INR re-lation between Toujan Faisal and the commit-tee (event span: informed).
ACE did not recordany event between the two entities.6 This exam-ple highlights the difference between our defini-tion of Interaction events and ACE?s definition ofContact events.
For this reason, in Figure 5, 51 ofour INR relations do not overlap with ACE eventcategories.4 Annotation ProcedureWe used Callisto (a configurable workbench) (Dayet al, 2004) to annotate the ACE-2005 corpus for6The ACE event annotated in the sentence is of type?Personell-Elect?
(span election) which is not recorded as anevent between two or more entities and is not relevant here.2462 Documents Conflict (5) Contact (32) Justice-* (13) Life (7) Transaction (2) Not FoundAttack Meet Phone-Write Die Divorce Injure Transfer-MoneyINRVerbal  Near (66) 0 26 0 9 0 0 0 0 31 Far (17) 0 0 3 3 0 1 0 0 10NonVerbal  Near (14) 3 0 0 0 2 0 0 1 8 Far (3) 0 0 0 0 0 0 0 1 2COG (109) 2 0 0 0 1 0 0 0 106PPR (2)  0  0  0  0  1  0  1  0  0PCR (1)  0  0  0  0  0  0  0  0  1Errors  0  3  0  1  1  0  0  0Figure 5: This table maps the type and subtype of ACE events to our types and subtypes of social events.
The columns haveACE event types and sub-types.
The rows represent our social event types and sub-types.
The last column is the number of ourevents that are not annotated as ACE events.
The last row has the number of social events that our annotator missed but areACE events.the social events we defined earlier.
The ACE-2005 corpus has already been annotated for enti-ties as part of the ACE effort.
The entity anno-tation is therefore not part of this annotation ef-fort.
We hired two annotators.
Annotators openedACE-2005 files one by one in Callisto.
They couldsee the whole document at one time (top screenof Figure 6) with entities highlighted in blue (bot-tom screen of Figure 6).
These entities were onlyof type PER.Individual and PER.Group and be-longed to class SPC.
All other ACE entity annota-tions were removed.
The annotators were requiredto read the whole document (not just the part thathas entities) and record a social event span (high-lighted in dark blue in Figure 6), social event type,subtype and the two participating entities in theevent.The span of a event mention is the minimumspan of text that best represents the presence of thetype of event being recorded.
It can also be viewedas the span of text that evokes the type of event be-ing recorded.
The span may be a word, a phraseor the whole sentence.
For example, the span inExample (4) in Section 2 includes strapped to hisbody because that confirms the physical proximityof the two entities.
We have, however, not paidmuch attention to the annotation of the span, andwill not report inter-annotator agreement on thispart of the annotation.
The reason for this is thatwe are interested in annotating the underlying se-mantics; we will use machine learning to find thelinguistics clues to each type of social event, ratherthan relying on the annotators?
ability to deter-mine these.
Also note that we did not give preciseinstructions on which entity mentions to choosein case of multiple mentions of the same entity.Again, this is because we are interested in anno-tating the underlying semantics, and we will relyon later analysis to determine which mentions par-ticipate in signaling the annotated social events.Figure 6: Snapshot of Callisto.
Top screen has the textfrom a document.
Bottom screen has tabs for Entities, EntityMentions etc.
An annotator selected text said, highlightedin dark blue, as an event of type COG between Entities withentity ID E1 and E9.Both our annotators annotated 46 common doc-uments.
Out these, there was one document thathad no entity annotations, implying no social eventannotation.
The average number of entities in theremaining 45 documents was 6.82 per document,and the average number of entity mentions perdocument was 23.78.
The average number of so-cial events annotated per document by one anno-25tator was 3.43, whereas for the other annotator itwas 3.69.
In the next section we present our inter-annotator agreement calculations for these 45 doc-uments.5 Inter-annotator AgreementAnnotators consider all sentences that contain atleast two person entities (individuals or group),but do not always consider all possible labels, orannotation values.
As represented in the decisiontree in Figure 5, many of the labels are conditional.At each next depth of the tree, the number of in-stances can become considerably pruned.
Due tothe novelty of the annotation task, and the condi-tional nature of the labels, we want to assess thereliability of the annotation of each decision point.For this, we report Cohen?s Kappa (Cohen, 1960)for each independent decision.
We use the stan-dard formula for Cohen?s Kappa given by:Kappa =P (a)?
P (e)1?
P (e)where P (a) is probability of agreement and P (e)is probability of chance agreement.
These proba-bilities can be calculated from the confusion ma-trix represented as follows:In addition, we present the confusion matrix foreach decision point to show the absolute numberof cases considered, and F-measure to show theproportion of cases agreed upon.
For most de-cision points, the Kappa scores are at or abovethe 0.67 threshold recommended by Krippen-dorff (1980) with F-measures above 0.90.
WhereKappa is low, F-measure remains high.
As dis-cussed below, we conclude that the annotationschema is reliable.We note that in the ACE annotation effort, inter-annotator agreement (IAA) was measured by asingle number, but this number did not take chanceagreement into account: it simply used the eval-uation metric to compare systems against a goldstandard.
Furthermore, this metric is composedof distinct parts which were weighted in accor-dance with research goals from year to year, mean-ing that the results of applying the metric changedfrom year to year.
We have also performed anACE-style IAA evaluation, which we report at theend of this section.Figure 7 shows the results for the seven binarydecision points, considered separately.
The num-ber of the decision point in the table correspondsto the decision points in Figure 4.
The (flattened)confusion matrices in column two present annota-tor two?s choices by annotator one?s, with positiveagreement in the upper left (cell A) and negativeagreement in the lower right (cell D).
In all casesthe cell values on the agreement diagonal (A, D)are much higher than the cells for disagreement(B, C).
The upper left cell (A) of the matrix fordecision 1 represents the positive agreements onthe presence of a social event (N=133), and theseare the cases considered for decision 2.
For theremaining decisions, agreement is always unbal-anced towards agreement on the positive cases,with few negative cases.
In the case of decision4, for example, this reflects the inherent unlike-lihood of the NONVERBAL-FAR event.
In othercases, it reflects a property of the genre.
For ex-ample, when we apply this annotation schema tofiction, we find a much higher frequency of phys-ically proximate events (PPR), corresponding tothe lower left cell (D) of the confusion matrix fordecision 6.For decision 4 (NONVERBAL-NEAR) and 7(PCR/COG), kappa scores are low but the con-fusion matrices and high F-measures demonstratethat the absolute agreement is very high.
Kappameasures the amount of agreement that would nothave occurred by chance, with values in [-1,1].
Forbinary data and two annotators, values of -1 canoccur, indicating that the annotators have perfectlynon-random disagreements.
The probability of anannotation value is estimated by its frequency inthe data (the marginals of the confusion matrix).It does not measure the actual amount of agree-ment among annotators, as illustrated by the rowsfor decisions 4 and 7.
Because NONVERBAL-FAR is chosen so rarely by either annotator (neverby annotator 2), the likelihood that both annota-tors will agree on NONVERBAL-NEAR is close toone.
In this case, there is little room for agreementabove chance, hence the Kappa score of zero.
Weshould point out, however, that this skewness wasrevealed from the annotated corpus.
We did notbias our annotators to look for a particular type ofrelation.The five cases of high Kappa and high F-26measure indicate aspects of the annotation whereannotators generally agree, and where the agree-ment is unlikely to be accidental.
We conclude thatthese aspects of the annotation can be carried outreliably as independent decisions.
The two casesof low Kappa and high F-measure indicate aspectsof the annotation where, for this data, there is rel-atively little opportunity for disagreement.Decision Point Confusion Matrix Kappa F1A B C D1 (+/- Relation) 133 31 34 245 0.68 0.802 (1 or 2 way) 51 8 1 73 0.86 0.913 (Verbal/NonV) 40 4 0 7 0.73 0.954 (NonV-Near/Far) 6 0 1 0 0.00 0.925 (Verbal-Near/Far) 30 1 2 7 0.77 0.956 (+/- PPR) 71 0 1 1 0.66 0.997 (PCR/COG) 69 1 1 0 -0.01 0.98Figure 7: This table presents the Inter-annotator agreementmeasures.
Column 1 is the decision point corresponding tothe decision tree.
Column 2 represents a flattened confusionmatrix where A corresponds to top left corner, D correspondsto the bottom right corner, B corresponds to top right cornerand C corresponds to the bottom left corner of the confusionmatrix.
We present values for Cohen?s Kappa in column 3and F-measure in the last column.Now, we present a measure of % agreementfor our annotators by using the ACE evaluationscheme.7 We considered one annotator to be thegold standard and the other to be a system beingevaluated against the gold standard.
For the cal-culation of this measure we first take the union ofall event spans.
As in the ACE evaluation scheme,we associate penalties with each wrong decisionannotators take about the entities participating inan event, type and sub-type of an event.
Sincethese penalties are not public, we assign our ownpenalties.
We choose penalties that are not biasedtowards any particular event type or subtype.
Wedecide the penalty based on the number of optionsan annotator has to consider before taking a cer-tain decision.
For example, we assign a penaltyof 0.5 if one annotator records an event which theother annotator does not.
If annotators disagreeon the relation type, the penalty is 0.25 becausethere are four options to select from (INR, COG,PPR, PCR).
Similarly, we assign a penalty of 0.27http://www.itl.nist.gov/iad/mig//tests/ace/2007/doc/ace07-evalplan.v1.3a.pdfif the annotators disagree on the relation sub-types(VERBAL-NEAR, VERBAL-FAR, NONVERBAL-NEAR, NONVERBAL-FAR, No sub-type).
We as-sign a penalty of 0.5 if the annotators disagree onthe participating entities (incorporating the direc-tionality in directed relations).
Using these penal-ties, we get % agreement of 69.74%.
This is a highagreement rate as compared to that of ACE?s eventannotation, which was reported to be 31.5% at theACE 2005 meeting.6 Conclusion and Future WorkWe have presented a new annotation scheme forextracting social networks from text.
We haveargued, social network created by the sender -receiver links in Enron Email corpus can ben-efit from social event links extracted from thecontent of emails where people talk about their?implicit?
social relations.
Our annotation taskis novel in that we are interested in the cogni-tive states of people: who is aware of interact-ing with whom, and who is aware of whom with-out interacting.
Though the task requires detec-tion of events followed by conditional classifica-tion of events into four types and subtypes, weachieve high Kappa (0.66-0.86) and F-measure(0.8-0.9).
We also achieve a high global agree-ment of 69.74% which is inspired by AutomatedContent Extraction (ACE) inter-annotator agree-ment measure.
These measures indicate that ourannotations are reliable.In future work, we will apply our annotationeffort to other genres, including fiction, and totext from which larger social networks can beextracted, such as extended journalistic reportingabout a group of people.Please contact the second author of the paperabout the availability of the corpus.AcknowledgmentsThis work was funded by NSF grant IIS-0713548.We thank Dr. David Day for help with adaptingthe annotation interface (Callisto) to our require-ments.
We would like to thank David Elson forextremely useful discussions and feedback on theannotation manual and the inter-annotator calcu-lation scheme.
We would also like to thank theNatural Language Processing group at ColumbiaUniversity for their feedback on our classificationof social events.27ReferencesJacob Cohen.
1960.
A coeffiecient of agreementfor nominal scales.
Educational and PsychologicalMeasurement, 20:37?46.David Day, Chad McHenry, Robyn Kozierok, and Lau-rel Riek.
2004.
Callisto: A configurable annotationworkbench.
International Conference on LanguageResources and Evaluation.Jana Diesner, Terrill L Frantz, and Kathleen M Car-ley.
2005.
Communication networks from the enronemail corpus it?s always about the people.
enron isno different.
Computational & Mathematical Orga-nization Theory, 11(3):201?228.G Doddington, A Mitchell, M Przybocki, L Ramshaw,S Strassel, and R Weischedel.
2004.
The auto-matic content extraction (ace) program?tasks, data,and evaluation.
LREC, pages 837?840.P S Keila and D B Skillicorn.
2005.
Structure in theenron email dataset.
Computational & Mathemati-cal Organization Theory, 11 (3):183?199.Bryan Klimt and Yiming Yang.
2004.
Introducingthe enron corpus.
In First Conference on Email andAnti-Spam (CEAS).Klaus Krippendorff.
1980.
Content Analysis: An In-troduction to Its Methodology.
Sage Publications.Andrew McCallum, Xuerui Wang, and AndresCorrada-Emmanuel.
2007.
Topic and role discoveryin social networks with experiments on enron andacademic email.
Journal of Artificial IntelligenceResearch, 30 (1):249?272.Nishith Pathak, Colin DeLong, Arindam Banerjee, andKendric Erickson.
2008.
Social topic models forcommunity extraction.
Proceedings of SNA-KDD.Ryan Rowe, German Creamer, Shlomo Hershkop, andSalvatore J Stolfo.
2007.
Automated social hi-erarchy detection through email network analysis.Proceedings of the 9th WebKDD and 1st SNA-KDD2007 workshop on Web mining and social networkanalysis, pages 109?117.Stanley Wasserman and Katherine Faust.
1994.
SocialNetwork Analysis: Methods and Applications.
NewYork: Cambridge University Press.28
