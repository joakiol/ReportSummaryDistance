The Importance of Discourse Contextfor Statistical Natural Language GenerationCassandre CreswellDepartment of LinguisticsUniversity of Torontocreswell@cs.toronto.eduElsi KaiserCenter for Language SciencesUniversity of Rochesterekaiser@ling.rochester.eduAbstractSurface realization in statistical natural lan-guage generation is based on the idea that whenthere are many ways to say the same thing, themost frequent option based on corpus countsis the best.
Based on data from English andFinnish, we argue instead that all options arenot equivalent, and the most frequent one canbe incoherent in some contexts.
A statisticalNLG system where word order choice is basedonly on frequency counts of forms cannot cap-ture the contextually-appropriate use of wordorder.
We describe an alternative method forword order selection and show how it outper-forms a frequency-only approach.1 IntroductionThe purpose of a natural language generation (NLG) sys-tem is to encode semantic content in a linguistic form eas-ily understood by humans in order to communicate it tothe user of the system.
Ideally, this content should beencoded in strings that are both grammatical and contex-tually appropriate.Human speakers of all natural languages have manyways to encode the same truth-conditional meaning be-sides a single ?canonical?
word order, even when encod-ing one predicate and its arguments as a main clause.
Hu-mans choose contextually-appropriate options from thesemany ways with little conscious effort and with rather ef-fective communicative results.
Statistical approaches tonatural language generation are based on the assumptionthat often many of these options will be equally good,e.g.
(Bangalore and Rambow, 2000).In this paper, we argue that, in fact, not all optionsare equivalent, based on linguistic data both from En-glish, a language with relatively static word order, andfrom Finnish, a language with much more flexible wordorder.
We show that a statistical NLG algorithm basedonly on counts of trees cannot capture the appropriateuse of word order.
We provide an alternative methodwhich has been implemented elsewhere and show thatit dramatically outperforms the statistical approach.
Fi-nally, we explain how the alternative method could beused to augment present statistical approaches and drawsome lessons for future development of statistical NLG.2 Statistical NLG: a brief summaryIn recent years, a new approach to NLG has emerged,which hopes to build on the success of the use ofprobabilistic models in natural language understanding(Langkilde and Knight, 1998; Bangalore and Rambow,2000; Ratnaparkhi, 2000).
Building an NLG system ishighly labor-intensive.
For the system to be robust, largeamounts of world and linguistic knowledge must be hand-coded.
The goal of statistical approaches is to minimizehand-coding and instead rely upon information automat-ically extracted from linguistic corpora when selecting alinguistic realization of some conceptual representation.The underlying concept of these statistical approachesis that the form generated to express a particular mean-ing should be selected on the basis of counts of that form(either strings or trees) in a corpus.
In other words, ingenerating a form f to express an input, one wants tomaximize the probability of the form, P (f), with respectto some gold-standard corpus, and thus express the in-put in a way that resembles the realizations in the corpusmost closely (Bangalore and Rambow, 2000).
Bangaloreand Rambow?s algorithm for generating a string in theFERGUS system begins with an underspecified concep-tual representation which is mapped to a dependency treewith unordered sibling nodes.
To convert the dependencytree into a surface form, a syntactic structure is chosen foreach node.
In FERGUS, this structure is an elementarytree in a tree-adjoining grammar.
The choice of a tree isstochastic, based on a tree model derived from 1,000,000words of the Wall Street Journal.
For example, the treechosen for a verb V will be the most frequently foundtree in the corpus headed by V .3 Where counting forms failsThis section provides evidence from English and Finnishthat word order affects meaning and acceptability.
Foreach phenomenon we show how a statistical generationtechnique based only on the probability of forms in a cor-pus will fail to capture this distinction in meaning.Speakers can use a particular form to indicate theirassumptions about the status of entities, properties, andevents in the discourse model.
For example, referencesto entities may appear as full NPs, pronouns, or be miss-ing entirely, depending on whether speakers regard themas new or old to the hearer or the discourse or as particu-larly salient (Gundel et al, 1993; Prince, 1992).
Not justthe lexical form of referential expressions, but also theirposition or role within the clause may vary depending onthe information status of its referent (Birner and Ward,1998).
An example of this in English is ditransitive verbs,which have two variants, the to-dative (I gave the book tothe manager) and the double-object (I gave the managerthe book).
Without a context both forms are equally ac-ceptable, and in context native speakers may be unableto consciously decide which is more appropriate.
How-ever, the use of the forms is highly systematic and almostentirely predictable from the relative information statusand the relative size of the object NPs (Snyder, 2003).
Ingeneral, older, lighter NPs precede newer, heavier NPs.Generating the appropriate ditransitive form basedonly on their relative frequencies is impossible, as canbe seen in the behavior of the ditransitive give in a corpusof naturally occurring written and spoken English (Sny-der, 2003).1 Of the 552 tokens of give where the indirectand direct objects are full NPs,2 152 (27.5%) are the to-dative and 400 (72.5%) are the double object construc-tion.
Given this ratio, only the double object constructionwould be generated.
If the distribution of relative infor-mation status and heaviness of direct and indirect objectsis the same in the domain of generation as in the sourcecorpus, then on average, the construction chosen as a sur-face realization will be inappropriate 3 times out of 10.Compared to English, the evidence for the importanceof word order from a free word order language likeFinnish is even more striking.
When word order is usedto encode the information status and discourse functionof NP referents, native speakers will judge the use of thewrong form infelicitous and odd, and a text incorporating1This corpus consists of two novels, the Switchboard corpus,and a corpus of online newsgroup texts.2She omits pronominal NPs because their ordering is af-fected by additional phonological factors related to cliticization.several wrong forms in succession rapidly becomes in-coherent (cf.
Kruijff-Korbayova?
et al (2002) on Czech,Russian, and Bulgarian).Although Finnish is regarded as canonically subject-verb-object (SVO), all six permutations of these three el-ements are possible, and corpus studies reveal that SVOorder only occurs in 56% of sentences (Hakulinen andKarlsson, 1980).
Different word order variants in Finnishrealize different pragmatic structurings of the conveyedinformation.
For example, Finnish has no definite or in-definite article, and the SVO/OVS variation is used to en-code the distinction between already-mentioned entitiesand new entities (e.g.
Chesterman (1991)).
OVS ordertypically marks the object as given, and the subject asnew.
SVO order is more flexible.
It can be used whenthe subject is given, and the object is new, and also whenboth are old or both are new.
In orders with more than onepreverbal argument (SOV, OSV), as well as verb-initialorders (VOS, VSO), the initial constituent is interpretedas being contrastive (Vilkuna (1995); and others).Because different orders have different discourse prop-erties, use of an inappropriate order can lead to severemisunderstandings, including difficulty in interpretingNPs.
For example, if a speaker uses canonical SVO orderin a context where the subject is discourse-new informa-tion but the object has already been mentioned, the hearerwill tend to have difficulty interpreting the NPs becauseOVS?not SVO?is the order that usually marks the ob-ject as discourse-old and subject as discourse-new.
Psy-cholinguistic evidence from sentence processing experi-ments shows that humans are very sensitive to the given-new information carried by word order (Kaiser, 2003).Hence, it is an important factor in the quality of linguisticoutput of a NLG system.Attempts to choose the appropriate word order inFinnish will encounter the same problem found with En-glish ditransitives.
Table 1 illustrates the frequency of thedifferent word orders in a 10,000 sentence corpus usedby Hakulinen and Karlsson (1980).
The most frequentorder is SV(X), where X is any non-subject, non-verbalconstituent, and so this order should always be the oneselected by a statistical algorithm.
Based on the countsthen, assuming that the proportion of discourse contextsis roughly similar within a domain, in only 56% of con-texts will the choice of SV(X) order actually match thediscourse conditions in which it is used.Order SV(X) XVS SXV XSV OtherN 5674 1139 60 348 2928% 56 11 1 3 29Table 1: Finnish word order frequencyThe point here is not that statistical approaches to NLGare entirely flawed.
Attempting to generate natural lan-guage by mimicking a corpus of naturally-occurring lan-guage may be the most practical strategy for designing ro-bust, scalable NLG systems.
However, human languageis not just a system for concatenating words (or assem-bling trees) to create grammatical outputs.
Speakers donot put constituents in a certain order simply because thewords they are using to express the constituents have beenfrequently put in that order in the past.
Constituents (andthereby words) appear in particular orders because thoseorders can reliably indicate the content speakers wish tocommunicate.
Because of the lucky coincidence that sta-tistical NLG has been primarily based on English, wherethe effects of word order variation are subtle, the prob-lems with selecting a form f based only on a calcula-tion of P (f) are not obvious.
It might seem as if themost frequent tree can express a given proposition ad-equately.
However, given the English word order phe-nomenon shown above, a model based on P (f) is prob-lematic.
Moreover, in languages like Finnish, even thegeneration of simple transitive clauses may result in out-put which is confusing for human users.NLG must take into account not just grammaticalitybut contextual appropriateness, and so statistical algo-rithms need to be provided with an augmented represen-tation from which to learn?not just strings or trees, butpairings of linguistic forms, contexts, and meanings.
Theprobability we need to maximize for NLG is the probabil-ity that f is used given a meaning to be expressed and thecontext in which f will be used, P (f |meaning,context).4 An alternative approachThis section describes a very simple example of how aprobability like P (f |meaning,context) could be utilizedas part of a surface realization algorithm for English di-transitives, in particular for the verb give.
This exampleis only a small subset of the larger problem of surfacerealization, but it illustrates well the improvement in per-formance of using P (f |meaning,context) vs. P (f), whenevaluated against actual corpus data.First, the corpus from which the probabilities are be-ing taken must be annotated with the additional mean-ing information conditioning the use of the form.
Forditransitives, this is the information status of the indirectobject NP, in particular whether it is hearer-new.
Hearer-status can be quickly and reliably annotated and has beenwidely used in corpus-based pragmatic studies (Birnerand Ward, 1998).
It could be applied as an additionalmarkup of a corpus to be used as input to a statistical gen-eration algorithm, like the Penn Treebank, such that eachNP indirect object of a ditransitive verb would be givenan additional tag marking its hearer status.
Here we usethe corpus counts presented in Snyder (2003) for the verbgive as our training data.
Table 2 shows the frequency ofthe properties of hearer-newness and relative heavinessof indirect objects (IOs) and direct objects (DOs) withrespect to the two ditransitive alternations.IO STATUS TO-DATIVE DOUBLE OBJECTHearer-new ?
60 0IO heavier 79 31Hearer-old DO heavier 7 357IO=DO 6 12Totals 152 400Table 2: Corpus freq.
of ditransitives (Snyder, 2003)To demonstrate the performance of an approach whichcounts only form, we use the equation P (f) to determinethe choice of double-object vs. to-dative.
The relativeprobabilities of each order in the Snyder (2003) corpusare .725 and .275 for double object and to-dative, respec-tively.
As such, this method will always select the doubleobject form, yielding an error rate of 27.5% on the train-ing data, as shown in the row labeled P (f) of Table 3.An algorithm which incorporates more informationthan just raw frequencies will proceed as follows: if theIO is hearer-new, generate a to-dative because the prob-ability in the corpus of finding a to-dative given that theindirect object is hearer-new is 1 (60 out of 552 tokens).In all other cases (i.e.
all other information statuses ofIO and DO), the probability of finding a to-dative is now92/400, or 18.6%, so generate a double object.
Thismethod results in 92 incorrect forms (all cases where thedouble object is generated instead of a to-dative), an errorrate of 16.7% on the training data.If the generation algorithm is further augmented to takeinto account information about the relative heaviness ofthe direct and indirect object NPs?possible in a systemwhere NPs are generated separately from sentences as awhole, the error rate can be reduced even more.
This al-gorithm will be as follows, if the IO is hearer-new, theform chosen is a to-dative.
If the IO is not hearer-new,the IO and DO are compared with respect to number ofsyllables.
If the IO is longer, generate a to-dative; if theDO is longer, generate a double object.
As before, thefirst rule applies to the 60 tokens where the IO is hearer-new.
Out of the remaining 492 tokens, 474 have IOs andDOs of different heaviness.
In 357 of the 388 double ob-jects, the DO is heavier, and in 79 of the 86 to-datives,the IO is heavier.
This leaves 38 of 474 tokens not cov-ered by the heaviness rule, along with 18 tokens wherethe IO and DO are equal.
For these 56 cases, we gener-ate the more probable overall form, the double object.
Intotal then, this augmented generation rule will yield 139to-datives (60 cases where the IO is hearer-new and 79cases where the IO is heavier).
With this algorithm, only13 actual to-datives will be generated wrongly as double-objects when compared to their actual form in the corpus,an error rate of only 2.4%DO-IO IO-DO ErrorActual counts 152 400 ?P (f) 0 552 27.5%Hearer-status, P (f) 60 492 16.7%Hearer-status, heaviness, P (f) 139 413 2.4%Table 3: Error rates with respect to choice of word orderThis example shows that for some arbitrary generationof a surface realization of the predicate GIVE, simply in-cluding the hearer-status of the recipient as a condition onthe choice of form yields the order that matches the ?goldstandard?
of human behavior in a meaningful way about80% of the time vs. only 70% for an approach based oncounts of trees including give alone.
By including addi-tional information about the relative size of the NPs, thesurface realization will match the gold standard over 97%of the time, a highly human-like output.5 Implementation & implications for NLGThe approach argued for above is one where discoursecontext and meaning must be taken into account when se-lecting a construction for NLG purposes.
Admittedly, thedemonstration of the error rate here is not derived froman actual system.
However, functioning NLG systemshave been implemented where exactly such informationconditions the algorithm for choice of main clause wordorder (Stone et al, 2001; Kruijff-Korbayova?
et al, 2002).Additionally, an approach like Bangalore and Rambow?scould easily be extended by annotating their corpus forhearer-status of NPs.
The necessary information couldalso possibly be extracted automatically from a corpuslike the Prague Dependency Treebank which includesdiscourse-level information relevant to word order.
Forphenomena which have not been as closely studied as En-glish ditransitives, machine learning could be used to findcorrelations between context and forms in corpora whichcould be incorporated into statistical NLG algorithms.The primary implication of our argument here is thatcounting words and trees is not enough for statisticalNLG.
Meaning, semantic and pragmatic, is a crucialcomponent of natural language generation.
Despite thedesire to lessen the need for labeled data in statisticalNLP, such data remain crucial.
Efforts to create multi-level corpora which overlay semantic annotation on topof syntactic annotation, such as the Propbank (Kingsburyand Palmer, 2002), should be expanded to include anno-tations of pragmatic and discourse information and usedin the development of statistical NLG methods.
We can-not generate forms by ignoring their meaning and expectto get meaningful output.
In other words, if the input toan NLG system does lack distinctions that play a crucialrole in human language comprehension, the system willnot be able to overcome this lack of quality and generatehigh-quality output.In addition, in the effort to push the boundaries of sta-tistical techniques, limiting the scope of research to En-glish may give falsely promising results.
If one of theprimary benefits of statistical techniques is robust porta-bility to other languages, presentation of results based onexperimentation on a small subset of human languagesmust be accompanied by a typologically-informed exam-ination of the assumptions underlying such experiments.ReferencesBangalore, S., and O. Rambow.
2000.
Exploiting a proba-bilistic hierarchical model for generation.
In COLING.Birner, B., and G. Ward.
1998.
Information status andnoncanonical word order in English.
Amsterdam:John Benjamins.Chesterman, A.
1991.
On definiteness.
Cambridge: CUP.Gundel, J., N. Hedberg, and R. Zacharski.
1993.
Cogni-tive status and the form of referring expressions.
Lan-guage 69:274?307.Hakulinen, A., and F. Karlsson.
1980.
Finnish syntax intext.
Nordic Journal of Linguistics 3:93?129.Kaiser, E. 2003.
The quest for a referent: A crosslinguis-tic look at reference resolution.
Doctoral Dissertation,University of Pennsylvania.Kingsbury, P., and M. Palmer.
2002.
From Treebank toPropbank.
In LREC-02.
Las Palmas, Spain.Kruijff-Korbayova?, I., G. J. Kruijff, and J. Bateman.2002.
Generation of contextually appropriate word or-der.
In Information sharing, 193?222.
CSLI.Langkilde, I., and K. Knight.
1998.
Generation that ex-ploits corpus-based statistical knowledge.
In COLING-ACL.Prince, E. F. 1992.
The ZPG letter: subjects, definiteness,and information-status.
In Discourse description.
Am-sterdam: John Benjamins.Ratnaparkhi, A.
2000.
Trainable methods for surface nat-ural language generation.
In ANLPC 6?NAACL 1.Snyder, K. 2003.
On ditransitives.
Doctoral Dissertation,University of Pennsylvania.Stone, M., C. Doran, B. Webber, T. Bleam, andM.
Palmer.
2001.
Communicative-intent-based mi-croplanning: the Sentence Planning Using Descriptionsystem.
Rutgers University.Vilkuna, M. 1995.
Discourse configurationality inFinnish.
In Discourse configurational languages, ed.K.
Kiss, 244?268.
New York: Oxford University Press.
