Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 658?668,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsJoint Chinese Word Segmentation and POS Tagging onHeterogeneous Annotated Corpora with Multiple Task LearningXipeng Qiu, Jiayi Zhao, Xuanjing HuangFudan University, 825 Zhangheng Road, Shanghai, Chinaxpqiu@fudan.edu.cn, zjy.fudan@gmail.com, xjhuang@fudan.edu.cnAbstractChinese word segmentation and part-of-speech tagging (S&T) are fundamentalsteps for more advanced Chinese languageprocessing tasks.
Recently, it has at-tracted more and more research intereststo exploit heterogeneous annotation cor-pora for Chinese S&T.
In this paper, wepropose a unified model for Chinese S&Twith heterogeneous annotation corpora.We first automatically construct a looseand uncertain mapping between two rep-resentative heterogeneous corpora, PennChinese Treebank (CTB) and PKU?s Peo-ple?s Daily (PPD).
Then we regard theChinese S&T with heterogeneous corporaas two ?related?
tasks and train our modelon two heterogeneous corpora simultane-ously.
Experiments show that our methodcan boost the performances of both of theheterogeneous corpora by using the sharedinformation, and achieves significant im-provements over the state-of-the-art meth-ods.1 IntroductionCurrently, most of statistical natural languageprocessing (NLP) systems rely heavily on manu-ally annotated resources to train their statisticalmodels.
The more of the data scale, the betterthe performance will be.
However, the costs areextremely expensive to build the large scale re-sources for some NLP tasks.
Even worse, the ex-isting resources are often incompatible even for asame task and the annotation guidelines are usu-ally different for different projects, since thereare many underlying linguistic theories whichexplain the same language with different per-spectives.
As a result, there often exist multi-ple heterogeneous annotated corpora for a sametask with vastly different and incompatible an-notation philosophies.
These heterogeneous re-sources are waste on some level if we cannot fullyexploit them.However, though most of statistical NLPmethods are not bound to specific annota-tion standards, almost all of them cannot dealsimultaneously with the training data withdifferent and incompatible annotation.
Theco-existence of heterogeneous annotation datatherefore presents a new challenge to utilizethese resources.The problem of incompatible annotation stan-dards is very serious for many tasks in NLP,especially for Chinese word segmentation andpart-of-speech (POS) tagging (Chinese S&T).
InChinese S&T, the annotation standards are of-ten incompatible for two main reasons.
One isthat there is no widely accepted segmentationstandard due to the lack of a clear definitionof Chinese words.
Another is that there are nomorphology for Chinese word so that there aremany ambiguities to tag the parts-of-speech forChinese word.
For example, the two commonly-used corpora, PKU?s People?s Daily (PPD) (Yuet al 2001) and Penn Chinese Treebank (CTB)(Xia, 2000), use very different segmentation andPOS tagging standards.For example, in Table 1, it is very differentto annotate the sentence ???????????
(Liu Xiang reaches the national final inChina)?
with guidelines of CTB and PDD.
PDDbreaks some phrases, which are single words in658Liu Xiang reachs China finalCTB ?
?/NR ?
?/VV ??
?/NN ??
?/NNPDD ?/nrf ?/nrg ?
?/v ?
?/ns ?/n ?/b ?
?/vnTable 1: Incompatible word segmentation and POS tagging standards between CTB and PDDCTB, into two words.
The POS tagsets are alsosignificantly different.
For example, PDD givesdiverse tags ?n?
and ?vn?
for the noun, whileCTB just gives ?NN?.
For proper names, theymay be tagged as ?nr?, ?ns?, etc in PDD, whilethey are just tagged as ?NR?
in CTB.Recently, it has attracted more and more re-search interests to exploit heterogeneous anno-tation data for Chinese word segmentation andPOS tagging.
(Jiang et al 2009) presented apreliminary study for the annotation adapta-tion topic.
(Sun and Wan, 2012) proposed astructure-based stacking model to fully utilizeheterogeneous word structures.
They also re-ported that there is no one-to-one mapping be-tween the heterogeneous word classification andthe mapping between heterogeneous tags is veryuncertain.These methods usually have a two-step pro-cess.
The first step is to train the preliminarytaggers on heterogeneous annotations.
The sec-ond step is to train the final taggers by usingthe outputs of the preliminary taggers as fea-tures.
We call these methods as ?pipeline-based?
methods.In this paper, we propose a method for jointChinese word segmentation and POS taggingwith heterogeneous annotation corpora.
We re-gard the Chinese S&T with heterogeneous cor-pora as two ?related?
tasks which can improvethe performance of each other.
Since it is impos-sible to establish an exact mapping between twoannotations, we first automatically construct aloose and uncertain mapping the heterogeneoustagsets of CTB and PPD.
Thus we can tag a sen-tence in one style with the help of the ?related?information in another heterogeneous style.
Theproposed method can improve the performancesof joint Chinese S&T on both corpora by usingthe shared information of each other, which isproven effective by experiments.There are three main contributions of ourmodel:?
First, we regard these two joint S&T taskson different corpora as two related taskswhich have interdependent and peer rela-tionship.?
Second, different to the pipeline-basedmethods, our model can be trained simul-taneously on the heterogeneous corpora.Thus, it can also produce two differentstyles of POS tags.?
Third, our model do not depend on theexactly correct mappings between the twoheterogeneous tagsets.
The correct map-ping relations can be automatically built intraining phase.The rest of the paper is organized as follows:We first introduce the related works in section 2and describe the background of character-basedmethod for joint Chinese S&T in section 3.
Sec-tion 4 presents an automatic method to buildthe loose mapping function.
Then we proposeour method on heterogeneous corpora in 5 and6.
The experimental results are given in section7.
Finally, we conclude our work in section 8.2 Related WorksThere are some works to exploit heteroge-neous annotation data for Chinese S&T.
(Gao et al 2004) described a transformation-based converter to transfer a certain annotation-style word segmentation result to another style.However, this converter need human designedtransformation templates, and is hard to be gen-eralized to POS tagging.
(Jiang et al 2009) proposed an automaticadaptation method of heterogeneous annotationstandards, which depicts a general pipeline to in-tegrate the knowledge of corpora with different659TaggerPPDTaggerCTBInput: xOutput: f(x)Output: CTB-style Tagsz=f(x)y=h(x,f(x))Figure 1: Traditional Pipeline-based Strategy forHeterogeneous POS Taggingunderling annotation guidelines.
They furtherproposed two optimization strategies, iterativetraining and predict-self re-estimation, to fur-ther improve the accuracy of annotation guide-line transformation (Jiang et al 2012).
(Sun and Wan, 2012) proposed a structure-based stacking model to fully utilize heteroge-neous word structures.These methods regard one annotation as themain target and another annotation as the com-plementary/auxiliary purposes.
For example, intheir solution, an auxiliary tagger TaggerPPDis trained on a complementary corpus PPD, toassist the target CTB-style TaggerCTB.
To re-fine the character-based tagger, PPD-style char-acter labels are directly incorporated as newfeatures.
The brief sketch of these methods isshown in Figure 1.The related work in machine learning liter-ature is multiple task learning (Ben-David andSchuller, 2003), which learns a problem togetherwith other related problems at the same time,using a shared representation.
This often leadsto a better model for the main task, becauseit allows the learner to use the commonalityamong the tasks.
Multiple task learning hasbeen proven quite successful in practice and hasbeen also applied to NLP (Ando and Zhang,2005).
We also preliminarily verified that mul-tiple task learning can improve the performanceon this problem in our previous work (Zhao etal., 2013), which is a simplified case of the workin this paper and has a relative low complexity.Different with the multiple task learning,whose tasks are actually different labels in thesame classification task, our model utilizes theshared information between the real differenttasks and can produce the corresponding differ-ent styles of outputs.3 Joint Chinese Word Segmentationand POS TaggingCurrently, the mainstream method of Chi-nese POS tagging is joint segmentation & tag-ging with character-based sequence labelingmodels(Lafferty et al 2001), which can avoidthe problem of segmentation error propagationand achieve higher performance on both sub-tasks(Ng and Low, 2004; Jiang et al 2008; Sun,2011; Qiu et al 2012).The label of each character is the cross-product of a segmentation label and a tagginglabel.
If we employ the commonly used label set{B, I, E, S} for the segmentation part of cross-labels ({B, I, E} represent Begin, Inside, End ofa multi-node segmentation respectively, and Srepresents a Single node segmentation), the la-bel of character can be in the form of {B-T}(Trepresents POS tag).
For example, B-NN indi-cates that the character is the begin of a noun.4 Automatically Establishing theLoose Mapping Function for theLabels of CharactersTo combine two human-annotated corpora,the relationship of their guidelines should befound.
A mapping function should be estab-lished to represent the relationship between twodifferent annotation guidelines.
However, theexact mapping relations are hard to establish.As reported in (Sun and Wan, 2012), there isno one-to-one mapping between their heteroge-neous word classification, and the mapping be-tween heterogeneous tags is very uncertain.Fortunately, there is a loose mappingcan be found in CTB annotation guide-line1 (Xia, 2000).
Table 2 shows some1Available at http://www.cis.upenn.edu/ ?chi-660CTB?s Tag PDD?
Tag1Total tags 33 26verbal noun NN v[+nom]proper noun NR n?
(shi4) VC v?
(you3) VE, VV vconjunctions CC, CS cother verb VV, VA v, a, znumber CD, OD m1 The tag set of PDD just includes the 26 broadcategories in the mapping table.
The whole tag setof PDD has 103 sub categories.Table 2: Examples of mapping between CTB andPDD?s tagsetmapping relations in CTB annotation guide-line.
These loose mapping relations aremany-to-many mapping.
For example, themapping may be ?NN/CTB?{n,nt,nz}/PDD?,?NR/CTB?
{nr,ns}/PDD?, ?v/PDD?{VV,VA}/CTB?
and so on.We define T1 and T2 as the tag sets for twodifferent annotations, and t1 ?
T1 and t2 ?
T2are the corresponding tags in two tag sets re-spectively.We first establish a loose mapping functionm : T1 ?
T2 ?
{0, 1} between the tags of CTBand PDD.m(t1, t2) ={1 if t1 and t2 have mapping relation0 else(1)The mapping relations are automaticallybuild from the CTB guideline (Xia, 2000).
Dueto the fact that the tag set of PPD used inthe CTB guideline is just broad categories, weexpand the mapping relations to include thesub categories.
If a PPD?s tag is involvedin the mapping, all its sub categories shouldbe involved.
For example, for the mapping?NR/CTB?nr/PDD?, the relation of NR andnrf/nrg should be added in the mapping rela-tions too (nrf/nrg belong to nr).Since we use the character-based joint S&Tmodel, we also need to find the mapping func-tion between the labels of characters.nese/posguide.3rd.ch.pdfIn this paper, we employ the commonly usedlabel set {B, I, E, S} for the segmentation partof cross-labels and the label of character can bein the form of {B-T}(T represents POS tag).Thus, each mapping relation t1 ?
t2 can beautomatically transformed to four forms: B-t1 ?B-t2, I-t1 ?I-t2, E-t1 ?E-t2 and S-t1 ?S-t2.
(?B-NR/CTB?{B-nr,B-ns}/PPD?
for ex-ample).Beside the above transformation, we alsogive a slight modification to adapt the dif-ferent segmentation guidelines.
For in-stance, the person name ???
(Mo Yan)?is tagged as ?B-NR, E-NR?
in CTB but?S-nrf, S-nrg?
in PPD.
So, some spe-cial mappings may need to be added like?B-NR/CTB?S-nrf/PPD?, ?E-NR/CTB?
{S-nrg, E-nrg}/PPD?, ?M-NR/CTB?
{B-nrg, M-nrg}/PPD?
and so on.
Although these spe-cial mappings are also established automaticallywith an exhaustive solution.
In fact, we give seg-mentation alignment only to proper names dueto the limitation of computing ability.Thus, we can easily build the loose bidirec-tional mapping function m?
for the labels ofcharacters.
An illustration of our constructionflowchart is shown in Figure 2.Finally, total 524 mappings relationships areestablished.5 Joint Chinese S&T withHeterogeneous Data with MultipleTask LearningInspired by the multiple task learning (Ben-David and Schuller, 2003), we can regard thejoint Chinese S&T with heterogeneous data astwo ?related?
tasks, which can improve theperformance of each other simultaneously withshared information.5.1 Sequence Labeling ModelWe first introduce the commonly used se-quence labeling model in character-based jointChinese S&T.Sequence labeling is the task of assigning la-bels y = y1, .
.
.
, yn(yi ?
Y) to an input sequencex = x1, .
.
.
, xn.
Y is the set of labels.661PPD-styleCTB-style NRnrNRnrf nrgB-NR S-NR...B-nrf B-nrg S-nrgS-nrg...mapping function m()between tagsmapping function m()between labels~Figure 2: An Illustration of Automatically Establishing the Loose Mapping FunctionGiven a sample x, we define the feature?(x,y).
Thus, we can label x with a score func-tion,y?
= arg maxyS(w,?
(x,y)), (2)where w is the parameter of score function S(?
).The feature vector ?
(x,y) consists of lots ofoverlapping features, which is the chief benefit ofdiscriminative model.
Different algorithms varyin the definition of S(?)
and the correspondingobjective function.
S(?)
is usually defined as lin-ear or exponential family function.For first-order sequence labeling, the featurecan be denoted as ?k(x, yi?1:i), where i standsfor the position in the sequence and k stands forthe number of feature templates.
For the linearclassifier, the score function can be rewritten indetail asy?
= arg maxyL?i=1(?u, f(x, yi)?+ ?v,g(x, yi?1:i)?)
,(3)where yi:j denotes label subsequenceyiyi+1 ?
?
?
yj ; f and g denote the state andtransition feature vectors respectively, u and vare their corresponding weight vectors; L is thelength of x.5.2 The Proposed ModelDifferent to the single task learning, the het-erogeneous data have two sets of labels Y andZ.The heterogeneous datasets Ds and Ds con-sist of {xi,yi}(i = 0, ?
?
?
,m) and {xi, zi}(i =0, ?
?
?
, n) respectively.For a sequence x = x1, .
.
.
, xL with lengthL.
, there may have two output sequence labelsy = y1, .
.
.
, yL and z = z1, .
.
.
, zL, where yi ?
Yand zi ?
Z.We rewrite the loose mapping function m?
be-tween two label sets into the following forms,?
(y) = {z|m?
(y, z) = 1}, (4)?
(z) = {y|m?
(y, z) = 1}, (5)where ?
(z) ?
Y and ?
(y) ?
Z are the subsetsof Y and Z.
Give a label y(or z) in an annota-tion, the loose mapping function ?
returns thecorresponding mapping label set in another het-erogeneous annotation.Our model for heterogeneous sequence label-ing can be write asy?
= arg maxy,yi?YL?i=1(?u, f(x, yi)?+ ?s,?z??
(yi)h(x, z)?+ ?v1,g1(x, yi?1:i)?+ ?v2,?zi?1??(yi?1)zi??
(yi)g2(x, zi?1:i)?
), (6)andz?
= arg maxz,zi?ZL?i=1(?u,?y??
(zi)f(x, y)?+?s,h(x, zi)?+ ?v1,?yi?1??(zi?1)yi??
(zi)g1(x, yi?1:i)?+ ?v2,g2(x, zi?1:i)?
), (7)where f and h represent the state feature vectorson two label sets Y and Z respectively.In Eq.
(6) and (7), the score of the label ofevery character is decided by the weights of thecorresponding mapping labels and itself.662Input sequence: xOutput: PPD-style TagsTaggerPPD TaggerCTBSharedInformationOutput: CTB-style TagsFigure 3: Our model for Heterogeneous POS TaggingThe main challenge of our model is the effi-ciency of decoding algorithm, which is similar tostructured learning with latent variables(Lianget al 2006) (Yu and Joachims, 2009).
Mostmethods for structured learning with latent vari-ables have not expand all possible mappings.In this paper, we also only expand the map-ping that with highest according to the currentmodel.Our model is shown in Figure 3 and theflowchart is shown in Algorithm 1.
If given theoutput type of label T , we only consider the la-bels in T to initialize the Viterbi matrix, andthe score of each node is determined by all theinvolved heterogeneous labels according to theloose mapping function.input : character sequence x1:Lloose mapping function ?output type: T (T ?
{Ty, Tz})output: label sequence lsif T == Ty thencalculate ls using Eq.
(6);else if T == Tz thencalculate ls using Eq.
(7) ;elsereturn null;endreturn lsAlgorithm 1: Flowchart of the Tagging pro-cess of the proposed model6 TrainingWe use online Passive-Aggressive (PA) algo-rithm (Crammer and Singer, 2003; Crammer etal., 2006) to train the model parameters.
Fol-lowing (Collins, 2002), the average strategy isused to avoid the overfitting problem.For the sake of simplicity, we merge the Eq.
(6)and (7) into a unified formula.Given a sequence x and the expect type oftags T , the merged model isy?
= arg maxyt(y)=T?w,?z??(y)?
(x, z)?, (8)where t(y) is a function to judge the type ofoutput tags; ?
(y) represents the set {?
(y1) ??
(y2) ?
?
?
?
?
?
(yL)} ?
{y}, where ?
meansCartesian product; w = (uT , sT ,vT1 ,vT2 )T and?
= (fT ,hT ,gT1 ,gT2 )T .We redefine the score function asS(w,x,y) = ?w,?z??(y)?
(x, z)?.
(9)Thus, we rewrite the model into a unified for-mulay?
= arg maxyt(y)=TS(w,x,y).
(10)Given an example (x,y), y?
is denoted as theincorrect label sequence with the highest scorey?
= arg maxy?
?=yt(y?
)=t(y)S(w,x, y?).
(11)The margin ?
(w; (x,y)) is defined as?
(w; (x,y)) = S(w,x,y)?
S(w,x, y?).
(12)Thus, we calculate the hinge loss?
(w; (x,y)), (abbreviated as ?w) by?w ={0, ?
(w; (x,y)) > 11?
?
(w; (x,y)), otherwise(13)In round k, the new weight vector wk+1 iscalculated bywk+1 = arg minw12 ||w?wk||2 + C ?
?,s.t.
?
(w; (xk,yk)) <= ?
and ?
>= 0 (14)663where ?
is a non-negative slack variable, and Cis a positive parameter which controls the influ-ence of the slack term on the objective function.Following the derivation in PA (Crammer etal., 2006), we can get the update rule,wk+1 = wk + ?kek, (15)whereek =?z??(yk)?
(xk, z)??z??(y?k)?
(xk, z),?k = min(C,?wk?ek?2).As we can see from the Eq.
(15), when we up-date the weight vector, the update informationincludes not only the features extracted fromcurrent input, but also that extracted from theloose mapping sequence of input.
For each fea-ture, the weights of its corresponding relatedfeatures derived from the loose mapping func-tion will be updated with the same magnitudeas well as itself.Our method regards two annotations to be in-terdependence and peer relationship.
Therefore,the two heterogeneous annotated corpora can besimultaneously used as the input of our trainingalgorithm.
Because of the tagging and trainingalgorithm, the weights and tags of two corporacan be used separately with the only dependentpart built by the loose mapping function.Our training algorithm based on PA is shownin Algorithm 2.6.1 AnalysisAlthough our mapping function between twoheterogeneous annotations is loose and uncer-tain, our online training method can automat-ically increase the relative weights of featuresfrom the beneficial mapping relations and de-crease the relative weights of features from theunprofitable mapping relations.Consider an illustrative loose mapping re-lation ?NN/CTB?n,nt,nz/PDD?.
For an in-put sequence x and PDD-style output is ex-pected.
If the algorithm tagging a charac-ter as ?n/PDD?
(with help of the weight of?NN/CTB?)
and the right tag isn?t one ofinput : mixed heterogeneous datasets:(xi,yi), i = 1, ?
?
?
, N ;parameters: C,K;loose mapping function: ?
;output: wKInitialize: wTemp?
0,w?
0;for k = 0 ?
?
?K ?
1 dofor i = 1 ?
?
?N doreceive an example (xi,yi);predict: y?i with Eq.
(11);if hinge loss ?w > 0 thenupdate w with Eq.
(15);endendwTemp = wTemp + w ;endwK = wTemp/K ;Algorithm 2: Training Algorithm?n,nt,nz/PDD?, the weight of ?NN/CTB?
willalso be decreased, which is reasonable sinceit is beneficial to distinguish the right tag.And if the right tag is one of ?n,nt,nz/PDD?but not ?n/PDD?
(for example, ?nt/PDD?
),which means it is a ?NN/CTB?, the weight of?NN/CTB?
will remain unchanged according tothe algorithm (updating ?n/PDD?
changes the?NN/CTB?, but updating ?nt/PDD?
changes itback).Therefore, after multiple iterations, useful fea-tures derived from the mapping function aretypically receive more updates, which take rela-tively more responsibility for correct prediction.The final model has good parameter estimatesfor the shared information.We implement our method based on Fu-danNLP(Qiu et al 2013).7 Experiments7.1 DatasetsWe use the two representative corpora men-tioned above, Penn Chinese Treebank (CTB)and PKU?s People?s Daily (PPD) in our ex-periments.664Dataset Partition Sections WordsCTB-5Training 1?270 0.47M400?9311001?1151Develop 301?325 6.66KTest 271?300 7.82KCTB-S Training 0.64MTest - 59.96KPPD Training - 1.11MTest - 0.16MTable 3: Data partitioning for CTB and PD7.1.1 CTB DatasetTo better comparison with the previousworks, we use two commonly used criterions topartition CTB dataset into the train and testsets.?
One is the partition criterion used in (Jinand Chen, 2008; Jiang et al 2009; Sun andWan, 2012) for CTB 5.0.?
Another is the CTB dataset from thePOS tagging task of the Fourth Interna-tional Chinese Language Processing Bake-off (SIGHAN Bakeoff 2008)(Jin and Chen,2008).7.1.2 PPD DatasetFor the PPD dataset, we use the PKU datasetfrom SIGHAN Bakeoff 2008.The details of all datasets are shown in Table3.
Our experiment on these datasets may lead toa fair comparison of our system and the relatedworks.7.2 SettingWe conduct two experiments on CTB-5 +PPD and CTB-S + PPD respectively.The form of feature templates we used isshown in Table 7.2, where C represents a Chi-nese character, and T represents the character-based tag.
The subscript i indicates its positionrelated to the current character.Our method can be easily combined withsome other complicated models, but we only usethe simple one for the purpose of observing theCi, T0(i = ?2,?1, 0, 1, 2)Ci, Ci+1, T0(i = ?1, 0)T?1, T0Table 4: Feature Templatessole influence of our unified model.
The parame-ter C is tested on develop dataset, and we foundthat it just impact the speed of convergence andhave no effect on the accuracy.
Moreover, sincewe use the averaged strategy, we wish more iter-ations to avoid overfitting and set a small value0.01 to it.
The maximum number of iterationsK is 50.The F1 score is used for evaluation, which isthe harmonic mean of precision P (percentage ofpredict phrases that exactly match the referencephrases) and recall R (percentage of referencephrases that returned by system).7.3 Evaluation on CTB-5 + PPDThe experiment results on the heterogeneouscorpora CTB-5 + PPD are shown in Table5.
Our method obtains an error reductions of24.08% and 90.8% over the baseline on CTB-5and PDD respectively.Our method also gives better performancethan the pipeline-based methods on heteroge-neous corpora, such as (Jiang et al 2009) and(Sun and Wan, 2012).The reason is that our model can utilize theinformation of both corpora effectively, whichcan boost the performance of each other.Although the loose mapping function are bidi-rectional between two annotation tagsets, wemay also use unidirectional mapping.
Therefore,we also evaluate the performance when we useunidirectional mapping.
We just use the map-ping function ?PDD?CTB, which means we ob-tain the PDD-style output without the informa-tion from CTB in tagging stage.
Thus, in train-ing stage, there are no updates for the weights ofCTB-features for the instances from PDD cor-pus, while instances from CTB corpus can resultto updates for PDD-features.Surprisedly, we find that the one-way map-ping can also improve the performances of bothcorpora.
The results are shown in Table 7.
The665Method Training Dataset Test Dataset P R F1(Jiang et al 2009) CTB-5, PDD CTB-5 - - 94.02(Sun and Wan, 2012) CTB-5, PDD CTB-5 94.42 94.93 94.68Our Model CTB-5 CTB-5 93.28 93.35 93.31Our Model PDD PDD 89.41 88.58 88.99Our Model CTB-5, PDD CTB-5 94.74 95.11 94.92Our Model CTB-5, PDD PDD 90.25 89.73 89.99Table 5: Performances of different systems on CTB-5 and PPD.Method Training Dataset Test Dataset P R F1Our Model CTB-S CTB-S 89.11 89.16 89.13Our Model PDD PDD 89.41 88.58 88.99Our Model CTB-S, PDD CTB-S 89.86 90.02 89.94Our Model CTB-S, PDD PDD 90.5 89.82 90.16Table 6: Performances of different systems on CTB-S and PPD.modelPPD?CTB obtains an error reductions of14.63% and 6.12% over the baseline on CTB-5and PDD respectively.Method P R F1ModelS on CTB-5 93.86 94.73 94.29ModelS on PDD 90.05 89.28 89.66?ModelS?
is the model which is trained on both CTB-5 and PDD training datasets with just just using theunidirectional mapping function ?PDD?CTB.Table 7: Performances of unidirectional PPD?CTBmapping on CTB-5 and PPD.7.4 Evaluation on CTB-S + PPDTable 6 shows the experiment results on theheterogeneous corpora CTB-S + PPD.
Ourmethod obtains an error reductions of 7.41% and10.59% over the baseline on CTB-S and PDD re-spectively.7.5 AnalysisAs we can see from the above experiments,our proposed unified model can improve theperformances of the two heterogeneous corporawith unidirectional or bidirectional loose map-ping functions.
Different to the pipeline-basedmethods, our model can use the shared infor-mation between two heterogeneous POS tag-gers.
Although the mapping function is looseand uncertain, it is still can boost the perfor-mances.
The features derived from the wrongmapping function take relatively less responsi-bility for prediction after multiple updates oftheir weights in training stage.
The final modelhas good parameter estimates for the shared in-formation.Another phenomenon is that the performanceof one corpus can gains when the data size of an-other corpus increases.
In our two experiments,the training set?s size of CTB-S is larger thanCTB-5, so the performance of PDD is higher inlatter experiment.8 ConclusionWe proposed a method for joint Chinese wordsegmentation and POS tagging with heteroge-neous annotation data.
Different to the previouspipeline-based works, our model is learned onheterogeneous annotation data simultaneously.Our method also does not require the exactcorresponding relation between the standardsof heterogeneous annotations.
The experimen-tal results show our method leads to a signif-icant improvement with heterogeneous annota-tions over the best performance for this task.Although our work is for a specific task on jointChinese word segmentation and POS, the keyidea to leverage heterogeneous annotations isvery general and applicable to other NLP tasks.666In the future, we will continue to refine theproposed model in two ways: (1) We wish to usethe unsupervised method to extract the loosemapping relation between the different annota-tion standards, which is useful to the corporawithout loose mapping guideline.
(2) We willanalyze the shared information (weights of thefeatures derived from the tags which have themapping relation) in detail and propose a moreeffective model.
Besides, we would also like toinvestigate for other NLP tasks which have dif-ferent annotation-style corpora.AcknowledgmentsWe would like to thank the anonymous re-viewers for their valuable comments.
Thiswork was funded by NSFC (No.61003091), KeyProjects in the National Science & Technol-ogy Pillar Program (2012BAH18B01), Shang-hai Municipal Science and Technology Com-mission (12511504500), Shanghai Leading Aca-demic Discipline Project (B114) and 973 Pro-gram (No.2010CB327900).ReferencesRie Kubota Ando and Tong Zhang.
2005.
A frame-work for learning predictive structures from mul-tiple tasks and unlabeled data.
J. Mach.
Learn.Res., 6:1817?1853, December.S.
Ben-David and R. Schuller.
2003.
Exploiting taskrelatedness for multiple task learning.
LearningTheory and Kernel Machines, pages 567?580.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In Proceedingsof the 2002 Conference on Empirical Methods inNatural Language Processing.K.
Crammer and Y.
Singer.
2003.
Ultraconservativeonline algorithms for multiclass problems.
Journalof Machine Learning Research, 3:951?991.Koby Crammer, Ofer Dekel, Joseph Keshet, ShaiShalev-Shwartz, and Yoram Singer.
2006.
Onlinepassive-aggressive algorithms.
Journal of MachineLearning Research, 7:551?585.J.
Gao, A. Wu, M. Li, C.N.
Huang, H. Li, X. Xia,and H. Qin.
2004.
Adaptive chinese word segmen-tation.
In Proceedings of ACL-2004.W.
Jiang, L. Huang, Q. Liu, and Y. Lu.
2008.
A cas-caded linear model for joint Chinese word segmen-tation and part-of-speech tagging.
In In Proceed-ings of the 46th Annual Meeting of the Associationfor Computational Linguistics.
Citeseer.W.
Jiang, L. Huang, and Q. Liu.
2009.
Automaticadaptation of annotation standards: Chinese wordsegmentation and POS tagging: a case study.
InProceedings of the Joint Conference of the 47thAnnual Meeting of the ACL and the 4th Inter-national Joint Conference on Natural LanguageProcessing, pages 522?530.Wenbin Jiang, Fandong Meng, Qun Liu, and Ya-juan L?.
2012.
Iterative annotation transfor-mation with predict-self reestimation for Chineseword segmentation.
In Proceedings of the 2012Joint Conference on Empirical Methods in Natu-ral Language Processing and Computational Nat-ural Language Learning, pages 412?420, Jeju Is-land, Korea, July.
Association for ComputationalLinguistics.C.
Jin and X. Chen.
2008.
The fourth interna-tional Chinese language processing bakeoff: Chi-nese word segmentation, named entity recognitionand Chinese pos tagging.
In Sixth SIGHAN Work-shop on Chinese Language Processing, page 69.John D. Lafferty, Andrew McCallum, and FernandoC.
N. Pereira.
2001.
Conditional random fields:Probabilistic models for segmenting and labelingsequence data.
In Proceedings of the EighteenthInternational Conference on Machine Learning.Percy Liang, Alexandre Bouchard-C?t?, Dan Klein,and Ben Taskar.
2006.
An end-to-end discrimi-native approach to machine translation.
In Pro-ceedings of the 21st International Conference onComputational Linguistics and the 44th annualmeeting of the Association for Computational Lin-guistics, pages 761?768.
Association for Computa-tional Linguistics.H.T.
Ng and J.K. Low.
2004.
Chinese part-of-speechtagging: one-at-a-time or all-at-once?
word-basedor character-based.
In Proceedings of EMNLP,volume 4.Xipeng Qiu, Feng Ji, Jiayi Zhao, and XuanjingHuang.
2012.
Joint segmentation and taggingwith coupled sequences labeling.
In Proceedingsof COLING 2012, pages 951?964, Mumbai, India,December.
The COLING 2012 Organizing Com-mittee.Xipeng Qiu, Qi Zhang, and Xuanjing Huang.
2013.FudanNLP: A toolkit for Chinese natural languageprocessing.
In Proceedings of ACL.Weiwei Sun and Xiaojun Wan.
2012.
Reducingapproximation and estimation errors for Chineselexical processing with heterogeneous annotations.In Proceedings of the 50th Annual Meeting of the667Association for Computational Linguistics, pages232?241.W.
Sun.
2011.
A stacked sub-word model for jointChinese word segmentation and part-of-speechtagging.
In Proceedings of the 49th Annual Meet-ing of the Association for Computational Lin-guistics: Human Language Technologies, pages1385?1394.F.
Xia, 2000.
The part-of-speech tagging guidelinesfor the penn Chinese treebank (3.0).Chun-Nam John Yu and Thorsten Joachims.
2009.Learning structural svms with latent variables.
InProceedings of the 26th Annual International Con-ference on Machine Learning, pages 1169?1176.ACM.S.
Yu, J. Lu, X. Zhu, H. Duan, S. Kang, H. Sun,H.
Wang, Q. Zhao, and W. Zhan.
2001.
Process-ing norms of modern Chinese corpus.
Technicalreport, Technical report.Jiayi Zhao, Xipeng Qiu, and Xuanjing Huang.
2013.A unified model for joint chinese word segmen-tation and pos tagging with heterogeneous anno-tation corpora.
In International Conference onAsian Language Processing, IALP.668
