Proceedings of the Third Joint Conference on Lexical and Computational Semantics (*SEM 2014), pages 1?11,Dublin, Ireland, August 23-24 2014.More or less supervised supersense tagging of TwitterAnders Johannsen, Dirk Hovy, H?ector Mart?
?nez Alonso, Barbara Plank, Anders S?gaardCenter for Language TechnologyUniversity of Copenhagen, DenmarkNjalsgade 140ajohannsen@hum.ku.dk, dirk@cst.dk, alonso@hum.ku.dkplank@cst.dk, soegaard@hum.ku.dkAbstractWe present two Twitter datasets annotatedwith coarse-grained word senses (super-senses), as well as a series of experimentswith three learning scenarios for super-sense tagging: weakly supervised learn-ing, as well as unsupervised and super-vised domain adaptation.
We show that(a) off-the-shelf tools perform poorly onTwitter, (b) models augmented with em-beddings learned from Twitter data per-form much better, and (c) errors can bereduced using type-constrained inferencewith distant supervision from WordNet.1 IntroductionSupersense tagging (SST, Ciaramita and Altun,2006) is the task of assigning high-level ontolog-ical classes to open-class words (here, nouns andverbs).
It is thus a coarse-grained word sense dis-ambiguation task.
The labels are based on the lexi-cographer file names for Princeton WordNet (Fell-baum, 1998).
They include 15 senses for verbsand 26 for nouns (see Table 1).
While WordNetalso provides catch-all supersenses for adjectivesand adverbs, these are grammatically, not seman-tically motivated, and do not provide any higher-level abstraction (recently, however, Tsvetkov etal.
(2014) proposed a semantic taxonomy for ad-jectives).
They will not be considered in this paper.Coarse-grained categories such as supersensesare useful for downstream tasks such as question-answering (QA) and open relation extraction (RE).SST is different from NER in that it has a larger setof labels and in the absence of strong orthographiccues (capitalization, quotation marks, etc.).
More-over, supersenses can be applied to any of the lex-ical parts of speech and not only proper names.Also, while high-coverage gazetteers can be foundfor named entity recognition, the lexical resourcesavailable for SST are very limited in coverage.Twitter is a popular micro-blogging service,which, among other things, is used for knowledgesharing among friends and peers.
Twitter posts(tweets) announce local events, say talks or con-certs, present facts about pop stars or program-ming languages, or simply express the opinions ofthe author on some subject matter.Supersense tagging is relevant for Twitter, be-cause it can aid e.g.
QA and open RE.
If someoneposts a message saying that some LaTeX modulenow supports ?drawing trees?, it is important toknow whether the post is about drawing naturalobjects such as oaks or pines, or about drawingtree-shaped data representations.This paper is, to the best of our knowledge, thefirst work to address the problem of SST for Twit-ter.
While there exist corpora of newswire andliterary texts that are annotated with supersenses,e.g., SEMCOR (Miller et al., 1994), no data isavailable for microblogs or related domains.
Thispaper introduces two new data sets.Furthermore, most, if not all, of previous workon SST has relied on gold standard part-of-speech(POS) tags as input.
However, in a domain suchas Twitter, which has proven to be challengingfor POS tagging (Foster et al., 2011; Ritter etal., 2011), results obtained under the assumptionof available perfect POS information are almostmeaningless for any real-life application.In this paper, we instead use predicted POS tagsand investigate experimental settings in which oneor more of the following resources are available tous:?
a large corpus of unlabeled Twitter data;?
Princeton WordNet (Fellbaum, 1998);?
SEMCOR (Miller et al., 1994); and?
a small corpus of Twitter data annotated withsupersenses.We approach SST of Twitter using various de-grees of supervision for both learning and domainadaptation (here, from newswire to Twitter).
In1weakly supervised learning, only unlabeled dataand the lexical resource WordNet are available tous.
While the quality of lexical resources varies,this is the scenario for most languages.
We presentan approach to weakly supervised SST based ontype-constrained EM-trained second-order HMMs(HMM2s) with continuous word representations.In contrast, when using supervised learning, wecan distinguish between two degrees of supervi-sion for domain adaptation.
For some languages,e.g., Basque, English, Swedish, sense-annotatedresources exist, but these corpora are all limitedto newswire or similar domains.
In such lan-guages, unsupervised domain adaptation (DA)techniques can be used to exploit these resources.The setting does not presume labeled data fromthe target domain.
We use discriminative mod-els for unsupervised domain adaptation, trainingon SEMCOR and testing on Twitter.Finally, we annotated data sets for Twitter, mak-ing supervised domain adaptation (SU) exper-iments possible.
For supervised domain adapta-tion, we use the annotated training data sets fromboth the newswire and the Twitter domain, as wellas WordNet.For both unsupervised domain adaptation andsupervised domain adaptation, we use structuredperceptron (Collins, 2002), i.e., a discriminativeHMM model, and search-based structured predic-tion (SEARN) (Daume et al., 2009).
We aug-ment both the EM-trained HMM2, discrimina-tive HMMs and SEARN with type constraints andcontinuous word representations.
We also exper-imented with conditional random fields (Laffertyet al., 2001), but obtained worse or similar resultsthan with the other models.Contributions In this paper, we present twoTwitter data sets with manually annotated su-persenses, as well as a series of experimentswith these data sets.
These experiments coverexisting approaches to related tasks, as well assome new methods.
In particular, we presenttype-constrained extensions of discriminativeHMMs and SEARN sequence models with con-tinuous word representations that perform well.We show that when no in-domain labeled datais available, type constraints improve modelperformance considerably.
Our best modelsachieve a weighted average F1 score of 57.1 overnouns and verbs on our main evaluation dataset, i.e., a 20% error reduction over the mostfrequent sense baseline.
The two annotated Twit-ter data sets are publicly released for downloadat https://github.com/coastalcph/supersense-data-twitter.n.Tops n.object v.cognitionn.act n.person v.communicationn.animal n.phenomenon v.competitionn.artifact n.plant v.consumptionn.attribute n.possession v.contactn.body n.process v.creationn.cognition n.quantity v.emotionn.communication n.relation v.motionn.event n.shape v.perceptionn.feeling n.state v.possessionn.food n.substance v.socialn.group n.time v.stativen.location v.body v.weathern.motive v.changeTable 1: The 41 noun and verb supersenses inWordNet2 More or less supervised modelsThis sections covers the varying degree of super-vision of our systems as well as the usage of typeconstraints as distant supervision.2.1 Distant supervisionDistant supervision in these experiments was im-plemented by only allowing a system to predicta certain supersense for a given word if that su-persense had either been observed in the trainingdata, or, for unobserved words, if the sense wasthe most frequent sense in WordNet.
If the worddid not appear in the training data nor in WordNet,no filtering was applied.
We refer to the distant-supervision strategy as type constraints.Distant supervision was implemented differ-ently in SEARN and the HMM model.
SEARNdecomposes sequential labelling into a series ofbinary classifications.
To constrain the labels wesimply pick the top-scoring sense for each tokenfrom the allowed set.
Structured perceptron usesViterbi decoding.
Here we set the emission prob-abilities for disallowed senses to negative infinityand decode as usual.2.2 Weakly supervised HMMsThe HMM2 model is a second-order hiddenMarkov model (Mari et al., 1997; Thede andHarper, 1999) using logistic regression to estimateemission probabilities.
In addition we constrain2w1t1t2P(t2|t1)P(w1|t1)t3w2w3Figure 1: HMM2 with continuous word represen-tationsthe inference space of the HMM2 tagger usingtype-level tag constraints derived from WordNet,leading to roughly the model proposed by Li etal.
(2012), who used Wiktionary as a (part-of-speech) tag dictionary.
The basic feature modelof Li et al.
(2012) is augmented with continuousword representation features as shown in Figure 1,and our logistic regression model thus works overa combination of discrete and continuous variableswhen estimating emission probabilities.
We do 50passes over the data as in Li et al.
(2012).We introduce two simplifications for the HMM2model.
First, we only use the most frequent senses(k = 1) in WordNet as type constraints.
Themost frequent senses seem to better direct the EMsearch for a local optimum, and we see dramaticdrops in performance on held-out data when weinclude more senses for the words covered byWordNet.
Second, motivated by computationalconcerns, we only train and test on sequences of(predicted) nouns and verbs, leaving out all otherword classes.
Our supervised models performedslightly worse on shortened sequences, and it is anopen question whether the HMM2 models wouldperform better if we could train them on full sen-tences.2.3 Structured perceptron and SEARNWe use two approaches to supervised sequen-tial labeling, structured perceptron (Collins, 2002)and search-based structured prediction (SEARN)(Daume et al., 2009).
The structured perceptronis a in-house reimplementation of Ciaramita andAltun (2006).1SEARN performed slightly betterthan structured perceptron, so we use it as our in-house baseline in the experiments below.
In thissection, we briefly explain the two approaches.1https://github.com/coastalcph/rungsted2.3.1 Structured perceptron (HMM)Structured perceptron learning was introduced inCollins (2002) and is an extension of the onlineperceptron learning algorithm (Rosenblatt, 1958)with averaging (Freund and Schapire, 1999) tostructured learning problems such as sequence la-beling.In structured perceptron for sequential labeling,where we learn a function from sequences of datapoints x1.
.
.
xnto sequences of labels y1.
.
.
yn,we begin with a random weight vector w0initial-ized to all zeros.
This weight vector is used toassign weights to transitions between labels, i.e.,the discriminative counterpart of P (yi+1| yi), andemissions of tokens given labels, i.e., the counter-part of P (xi| yi).
We use Viterbi decoding to de-rive a best path?y through the correspondingm?nlattice (with m the number of labels).
Let the fea-ture mapping ?
(x,y) be a function from a pairof sequences ?x,y?
to all the features that firedto make y the best path through the lattice for x.Now the structured update for a sequence of datapoints is simply ?(?(x,y)??
(x,?y)), i.e., a fixedpositive update of features that fired to produce thecorrect sequence of labels, and a fixed negative up-date of features that fired to produce the best pathunder the model.
Note that if y =?y, no featuresare updated.2.3.2 SEARNSEARN is a way of decomposing structured pre-diction problems into search and history-basedclassification.
In sequential labeling, we decom-pose the sequence of m tokens into m classifica-tion problems, conditioning our labeling of the ithtoken on the history of i ?
1 previous decisions.The cost of a mislabeling at training time is de-fined by a cost function over output structures.
Weuse Hamming loss rather than F1as our cost func-tion, and we then use stochastic gradient descentwith quantile loss as a our cost-sensitive learningalgorithm.
We use a publicly available implemen-tation.23 ExperimentsWe experiment with weakly supervised learning,unsupervised domain adaptation, as well as su-pervised domain adaptation, i.e., where our mod-els are induced from hand-annotated newswireand Twitter data.
Note that in all our experiments,2http://hunch.net/?vw/3we use predicted POS tags as input to the system,in order to produce a realistic estimate of SST per-formance.3.1 DataOur experiments rely on combinations of availableresources and newly annotated Twitter data setsmade publicly available with this paper.3.1.1 Available resourcesPrinceton WordNet (Fellbaum, 1998) is the mainresource for SST.
The lexicographer file namesprovide the label alphabet of the task, and the tax-onomy defined therein is used not only in the base-lines, but also as a feature in the discriminativemodels.
We use the WordNet 3.0 distribution.SEMCOR (Miller et al., 1994) is a sense-annotated corpus composed of 80% newswire and20% literary text, using the sense inventory fromWordNet.
SEMCOR comprises 23k distinct lem-mas in 234k instances.
We use the texts whichhave full annotations, leaving aside the verb-onlytexts (see Section 6).We use a distributional semantic model in orderto incorporate distributional information as fea-tures in our system.
In particular, we use theneural-network based models from (Mikolov etal., 2013), also referred as word embeddings.
Thismodel makes use of skip-grams (n-grams that donot need to be consecutive) within a word windowto calculate continuous-valued vector representa-tions from a recurrent neural network.
These dis-tributional models have been able to outperformstate of the art in the SemEval-2012 Task 2 (Mea-suring degrees of relational similarity).
We calcu-late the embeddings from an in-house corpus of57m English tweets using a window size 5 andyielding vectors of 100 dimensions.We also use the first 20k tweets of the 57mtweets to train our HMM2 models.3.1.2 AnnotationWhile an annotated newswire corpus and a high-quality lexical resource already enable us to train,we also need at least a small sample of anno-tated tweets data to evaluate SST for Twitter.
Fur-thermore, if we want to experiment with super-vised SST, we also need sufficient annotated Twit-ter data to learn the distribution of sense tags.This paper presents two data sets: (a) super-sense annotations for the POS+NER-annotateddata set described in Ritter et al.
(2011), which weuse for training, development and evaluation, us-ing the splits proposed in Derczynski et al.
(2013),and (b) supersense annotations for a sample of 200tweets, which we use for additional, out-of-sampleevaluation.
We call these data sets RITTER-{TRAIN,DEV,EVAL} and IN-HOUSE-EVAL, re-spectively.
The IN-HOUSE-EVAL dataset wasdownloaded in 2013 and is a sample of tweets thatcontain links to external homepages but are other-wise unbiased.
It was previously used (with part-of-speech annotation) in (Plank et al., 2014).
Bothdata sets are made publicly available with this pa-per.Supersenses are annotated with in spans definedby the BIO (Begin-Inside-Other) notation.
To ob-tain the Twitter data sets, we carried out an an-notation task.
We first pre-annotated all data setswith WordNet?s most frequent senses.
If the wordwas not in WordNet and a noun, we assigned it thesense n.person.
All other words were labeled O.Chains of nouns were altered to give every ele-ment the sense of the head noun, and the BI tagsadjusted, i.e.
:Empire/B-n.loc State/B-n.loc Building/B-n.artifactwas changed toEmpire/B-n.artifact State/I-n.artifact Building/I-n.artifactFor the RITTER data, three paid student an-notators worked on different subsets of the pre-annotated data.
They were asked to correct mis-takes in both the BIO notation and the assignedsupersenses.
They were free to chose from the fulllabel set, regardless of the pre-annotation.
Whilethe three annotators worked on separate parts, theyoverlapped on a small part of RITTER-TRAIN (841tokens).
On this subset, we computed agreementscores and annotation difficulties.
The averageraw agreement was 0.86 and Cohen?s ?
0.77.
Themajority of tokens received the O label by all an-notators; this happended in 515 out of 841 cases.Excluding these instances to evaluate the perfor-mance on the more difficult content words, rawagreement dropped to 0.69 and Cohen?s ?
to 0.69.The IN-HOUSE-EVAL data set was annotatedby two different annotators, namely two of the au-thors of this article.
Again, for efficiency reasonsthey worked on different subsets of the data, withan overlapping portion.
Their average raw agree-ment was 0.65 and their Cohen?s ?
0.62.
For thisdata set, we also compute F1, defined as usual asthe harmonic mean of recall and precision.
To4compute this, we set one of the annotators as golddata and the other as predicted data.
However,since F1is symmetrical, the order does not mat-ter.
The annotation F1gives us another estimateof annotation difficulty.
We present the figures inTable 3.3.2 BaselinesFor most word sense disambiguation studies, pre-dicting the most frequent sense (MFS) of a wordhas been proven to be a strong baseline.
Follow-ing this, our MFS baseline simply predicts the su-persense of the most frequent WordNet sense fora tuple of a word and a part of speech.
We usethe part of speech predicted by the LAPOS tagger(Tsuruoka et al., 2011).
Any word not in Word-Net is labeled as noun.person, which is the mostfrequent sense overall in the training data.
Aftertagging, we run a script to correct the BI tag pre-fixes, as described above for the annotation ask.We also compare to the performance of exist-ing SST systems.
In particular we use Sense-Learner (Mihalcea and Csomai, 2005) as a base-line, which produces estimates of the WordNetsense for each word.
For these predictions, weretrieve the corresponding supersense.
Finally,we use a publicly available reimplementation ofCiaramita and Altun (2006) by Michael Heilman,which reaches comparable performance on gold-tagged SEMCOR.33.3 Model parametersWe use the feature model of Paa?
and Reichartz(2009) in all our models, except the weakly su-pervised models.
For the structured perceptron weset the number of passes over the training data onthe held-out development data.
The weakly super-vised models use the default setting proposed inLi et al.
(2012).
We have used the standard onlinesetup for SEARN, which only takes one pass overthe data.The type of embedding is the same in all ourexperiments.
For a given word the embedding fea-ture is a 100 dimensional vector, which combinesthe embedding of the word with the embedding ofadjacent words.
The feature combination fefor aword wtis calculated as:fe(wt) =12(e(wt?1) + e(wt+1))?
2e(wt),3http://www.ark.cs.cmu.edu/mheilman/questions/SupersenseTagger-10-01-12.tar.gzwhere the factor of two is chosen heurestically togive more weight to the current word.We also set a parameter k on development datafor using the k-most frequent senses inWordNetas type constraints.
Our supervised models aretrained on SEMCOR+RITTER-TRAIN or simplyRITTER-TRAIN, depending on what gave us thebest performance on the held-out data.4 ResultsThe results are presented in Table 2.
We dis-tinguish between three settings with various de-grees of supervision: weakly supervised, whichuses no domain annotated information, but solelyrelies on embeddings trained on unlabeled Twit-ter data; unsupervised domain adaptation (DA),which uses SemCor for supervised training; andsupervised domain adaptation (SU), which usesannotated Twitter data in addition to the SemCordata for training.In each of the two domain adaptation settings,SEARN and HMM are evaluated with type con-straints as distant supervision, and without forcomparison.
SEARN without embeddings or dis-tant supervision serves as an in-house baseline.In Table 3 we present the WordNet token cov-erage of predicted nouns and verbs in the devel-opment and evaluation data, as well as the inter-annotator agreement F1scores.All the results presented in Table 2 are(weighted averaged) F1measures obtained on pre-dicted POS tags.
Note that these results are con-siderably lower than results on supersense taggingnewswire (up to 80 F1) that assume gold standardPOS tags (Ciaramita and Altun, 2006; Paa?
andReichartz, 2009).The re-implementation of the state-of-the-artsystem improves slightly upon the most frequentsense baseline.
SenseLearner does not seem tocapture the relevant information and does notreach baseline performance.
In other words, thereis no off-the-shelf tool for supersense tagging ofTwitter that does much better than assigning themost frequent sense to predicted nouns and verbs.Our weakly supervised model performs worsethan the most frequent sense baseline.
This is anegative result.
It is, however, well-known fromthe word sense disambiguation literature that theMFS is a very strong baseline.
Moreover, the EMlearning problem is hard because of the large la-bel set and weak distributional evidence for super-5RITTER IN-HOUSEDEV EVAL EVALWordnet noun-verbtoken coverage 83.72 70.22 41.18Inter-annotatoragreement (F1) 81.01 69.15 61.57Table 3: Properties of dataset.senses.The unsupervised domain adaptation and fullysupervised systems perform considerably betterthan this baseline across the board.
In the unsuper-vised domain adaptation setup, we see huge im-provements from using type constraints as distantsupervision.
In the supervised setup, we only seesignificant improvements adding type constraintsfor the structured perceptron (HMM), but not forsearch-based structured prediction (SEARN).For all the data sets, there is still a gap betweenmodel performance and human inter-annotatoragreement levels (see Table 3), leaving some roomfor improvements.
We hope that the release of thedata sets will help further research into this.4.1 Coarse-grained evaluationWe also experimented with the more coarse-grained classes proposed by Yuret and Yatbaz(2010).
Here our best model obtained an F1scorefor mental concepts (nouns) of 72.3%, and 62.6%for physical concepts, on RITTER-DEV.
The over-all F1score for verbs is 85.6%.
The overall F1is75.5%.
Note that this result is not directly com-parable to the figure (72.9%) reported in Yuretand Yatbaz (2010), since they use different datasets, exclude verbs and make different assump-tions, e.g., relying on gold POS tags.5 Error analysisWe have seen that inter-annotator agreements onsupersense annotation are reliable at above .60but far from perfect.
The Hinton diagram in Ta-ble 2 presents the confusion matrix between ourannotators on IN-HOUSE-EVAL.Errors in the prediction primarily stem fromtwo sources: out-of-vocabulary words and incor-rect POS tags.
Figure 3 shows the distribution ofsenses over the words that were not contained ineither the training data, WordNet, or the Twitterdata used to learn the embeddings.
The distribu-tion follows a power law, with the most frequentsense being noun.person, followed by noun.group,and noun.artifact.
The first two are related to NERcategories, namely PER and ORG, and can be ex-pected, since Twitter users frequently talk aboutnew actors, musicians, and bands.
Nouns of com-munication are largely related to films, but also in-clude Twitter, Facebook, and other forms of socialmedia.
Note that verbs occur only towards the tailend of the distribution, i.e., there are very few un-known verbs, even in Twitter.Overall, our models perform best on labels withlow lexical variability, such as quantities, statesand times for nouns, as well as consumption, pos-session and stative for verbs.
This is unsurprising,since these classes have lower out-of-vocabularyrates.With regards to the differences between source(SEMCOR) and target (Twitter) domains, we ob-serve that the distribution of supersenses is al-ways headed by the same noun categories likenoun.person or noun.group, but the frequency ofout-of-vocabulary stative verbs plummets in thetarget domain, as some semantic types are moreclosed class than others.
There are for instancefewer possibilities for creating new time units(noun.time) or stative verbs like be than people orcompany names (noun.person or noun.group, re-spectively).The weakly supervised model HMM2 hashigher precision (57% on RITTER-DEV) than re-call (48.7%), which means that it often predictswords to not belong to a semantic class.
Thissuggests an alternative strategy, which is to traina model on sequences of purely non-O instances.This would force the model to only predict O onwords that do not appear in the reduced sequences.One important source of error seems to be un-reliable part-of-speech tagging.
In particular wepredict the wrong POS for 20-35% of the verbsacross the data sets, and for 4-6.5% of the nouns.In the SEMCOR data, for comparability, we havewrongly predicted tags for 6-8% of the anno-tated tokens.
Nevertheless, the error propaga-tion of wrongly predicted nouns and verbs is par-tially compensated by our systems, since they aretrained on imperfect input, and thus it becomespossible for the systems to predict a noun super-sense for a verb and viceversa.
In our data we havefound e.g.
that the noun Thanksgiving was incor-rectly tagged as a verb, but its supersense was cor-rectly predicted to be noun.time, and that the verbguess had been mistagged as noun but the system6Resources ResultsToken-level Type-level RITTER IN-HOUSESemCor Twitter Embeddings Type constraints DEV EVAL EVALGeneral baselinesMFS - - - + 47.54 44.98 38.65SENSELEARNER + - - - 14.61 26.24 22.81HEILMAN + - - - 48.96 45.03 39.65Weakly supervised systemsHMM2 - - - + 47.09 42.12 26.99Unsupervised domain adaptation systems (DA)SEARN (Baseline) + - - - 48.31 42.34 34.30SEARN + - + - 52.45 48.30 40.22SEARN + - + + 56.59 50.89 40.50HMM + - + - 52.40 47.90 40.51HMM + - + + 57.14 50.98 41.84Supervised domain adaptation systems (SU)SEARN (Baseline) + + - - 58.30 52.12 36.86SEARN + + + - 63.05 57.09 42.37SEARN + + + + 62.72 57.14 42.42HMM + + + - 57.20 49.26 39.88HMM + + + + 60.66 51.40 41.60Table 2: Weighted F1 average over 41 supersenses.7Figure 2: Inter-annotator confusion matrix on TWITTER-EVAL.00.10.20.30.4noun.person noun.groupnoun.artifactnoun.communicationnoun.eventnoun.locationnoun.time noun.actnoun.foodnoun.attributenoun.relationverb.cognitionverb.creationverb.emotionverb.motionverb.perceptionverb.stativeFigure 3: Sense distribution of OOV words.8still predicted the correct verb.cognition as super-sense.6 Related WorkThere has been relatively little previous work onsupersense tagging, and to the best of our knowl-edge, all of it has been limited to English newswireand literature (SEMCOR and SENSEVAL).The task of supersense tagging was first intro-duced by Ciaramita and Altun (2006), who useda structured perceptron trained and evaluated onSEMCOR via 5-fold cross validation.
Their eval-uation included a held-out development set oneach fold that was used to estimate the number ofepochs.
They used additional training data con-taining only verbs.
More importantly, they reliedon gold standard POS tags.
Their overall F1scoreon SEMCOR was 77.1.
Reichartz and Paa?
(Re-ichartz and Paa?, 2008; Paa?
and Reichartz, 2009)extended this work, using a CRF model as wellas LDA topic features.
They report an F1scoreof 80.2, again relying on gold standard POS fea-tures.
Our implementation follows their setup andfeature model, but we rely on predicted POS fea-tures, not gold standard features.Supersenses provide information similar tohigher-level distributional clusters, but more in-terpretable, and have thus been used as high-level features in various tasks, such as preposi-tion sense disambiguation, noun compound inter-pretation, and metaphor detection (Ye and Bald-win, 2007; Tratz and Hovy, 2010; Tsvetkov et al.,2013).
Princeton WordNet only provides a fullydeveloped taxonomy of supersenses for verbs andnouns, but Tsvetkov et al.
(2014) have recentlyproposed an extension of the taxonomy to coveradjectives.
Outside of English, supersenses havebeen annotated for Arabic Wikipedia articles bySchneider et al.
(2012).In addition, a few researchers have tried tosolve coarse-grained word sense disambiguationproblems that are very similar to supersense tag-ging.
Kohomban and Lee (2005) and Kohom-ban and Lee (2007) also propose to use lexicogra-pher file identifers from Princeton WordNet senses(supersenses) and, in addition, discuss how to re-trieve fine-grained senses from those predictions.They evaluate their model on all-words data fromSENSEEVAL-2 and SENSEEVAL-3.
They use aclassification approach rather than structured pre-diction.Yuret and Yatbaz (2010) present a weakly unsu-pervised approach to this problem, still evaluatingon SENSEVAL-2 and SENSEVAL-3.
They focusonly on nouns, relying on gold part-of-speech, butalso experiment with a coarse-grained mapping,using only three high level classes.For Twitter, we are aware of little previous workon word sense disambiguation.
Gella et al.
(2014)present lexical sample word sense disambiguationannotation of 20 target nouns on Twitter, but noexperimental results with this data.
There has alsobeen related work on disambiguation to Wikipediafor Twitter (Cassidy et al., 2012).In sum, existing work on supersense taggingand coarse-grained word sense disambiguation forEnglish has to the best of our knowledge all fo-cused on newswire and literature.
Moreover, theyall rely on gold standard POS information, makingprevious performance estimates rather optimistic.7 ConclusionIn this paper, we present two Twitter data sets withmanually annotated supersenses, as well as a se-ries of experiments with these data sets.
The datais publicly available for download.In this article we have provided, to the bestof our knowledge, the first supersense tagger forTwitter.
We have shown that off-the-shelf toolsperform poorly on Twitter, and we offer twostrategies?namely distant supervision and the us-age of embeddings as features?that can be com-bined to improve SST for Twitter.We propose that distant supervision imple-mented as type constraints during decoding is aviable method to limit the mispredictions of su-persenses by our systems, thereby enforcing pre-dicted senses that a word has in WordNet.
This ap-proach compensates for the size limitations of thetraining data and mitigates the out-of-vocabularyeffect, but is still subject to the coverage of Word-Net; which is far from perfect for words comingfrom high-variability sources such as Twitter.Using distributional semantics as features inform of word embeddings also improves the pre-diction of supersenses, because it provides seman-tic information for words, regardless of whetherthey have been observed the training data.
Thismethod does not require a hand-created knowl-edge base like WordNet, and is a promising tech-nique for domain adaptation of supersense tag-ging.9ReferencesTaylor Cassidy, Heng Ji, Lev-Arie Ratinov, Arkaitz Zu-biaga, and Hongzhao Huang.
2012.
Analysis andenhancement of wikification for microblogs withcontext expansion.
In COLING, volume 12, pages441?456.Massimiliano Ciaramita and Yasemin Altun.
2006.Broad-coverage sense disambiguation and informa-tion extraction with a supersense sequence tagger.
InProc.
of EMNLP, pages 594?602, Sydney, Australia,July.Michael Collins.
2002.
Discriminative training meth-ods for hidden markov models: Theory and experi-ments with perceptron algorithms.
In EMNLP.Hal Daume, John Langford, and Daniel Marcu.
2009.Search-based structured prediction.
Machine Learn-ing, pages 297?325.Leon Derczynski, Alan Ritter, Sam Clark, and KalinaBontcheva.
2013.
Twitter part-of-speech taggingfor all: overcoming sparse and noisy data.
InRANLP.Christiane Fellbaum.
1998.
WordNet: an electroniclexical database.
MIT Press USA.Jennifer Foster, Ozlem Cetinoglu, Joachim Wagner,Josef Le Roux, Joakim Nivre, Deirde Hogan, andJosef van Genabith.
2011.
From news to comments:Resources and benchmarks for parsing the languageof Web 2.0.
In IJCNLP.Yoav Freund and Robert Schapire.
1999.
Large marginclassification using the perceptron algorithm.
Ma-chine Learning, 37:277?296.Spandana Gella, Paul Cook, and Timothy Baldwin.2014.
One sense per tweeter and other lexical se-mantic tales of Twitter.
In EACL.Upali Kohomban and Wee Lee.
2005.
Learning se-mantic classes for word sense disambiguation.
InACL.Upali Kohomban and Wee Lee.
2007.
Optimizingclassifier performance in word sense disambiguationby redefining word sense classes.
In IJCAI.John Lafferty, Andrew McCallum, and FernandoPereira.
2001.
Conditional random fields: prob-abilistic models for segmenting and labeling se-quence data.
In ICML.Shen Li, Jo?ao Grac?a, and Ben Taskar.
2012.
Wiki-lysupervised part-of-speech tagging.
In EMNLP.Jean-Francois Mari, Jean-Paul Haton, and AbdelazizKriouile.
1997.
Automatic word recognition basedon second-order hidden Markov models.
IEEETransactions on Speech and Audio Processing,5(1):22?25.Rada Mihalcea and Andras Csomai.
2005.
Sense-learner: Word sense disambiguation for all words inunrestricted text.
In Proceedings of the ACL 2005on Interactive poster and demonstration sessions,pages 53?56.
Association for Computational Lin-guistics.Tomas Mikolov, Ilya Sutskever, Kai Chen, GregoryCorrado, and Jeffrey Dean.
2013.
Distributed rep-resentations of words and phrases and their compo-sitionality.
In NIPS.George A. Miller, Martin Chodorow, Shari Landes,Claudia Leacock, and Robert G. Thomas.
1994.Using a semantic concordance for sense identifica-tion.
In Proceedings of the workshop on HumanLanguage Technology, pages 240?243.
Associationfor Computational Linguistics.Gerhard Paa?
and Frank Reichartz.
2009.
Exploit-ing semantic constraints for estimating supersenseswith CRFs.
In Proc.
of the Ninth SIAM Interna-tional Conference on Data Mining, pages 485?496,Sparks, Nevada, May.Barbara Plank, Dirk Hovy, and Anders S?gaard.
2014.Learning part-of-speech taggers with inter-annotatoragreement loss.
In Proceedings of EACL.Frank Reichartz and Gerhard Paa?.
2008.
EstimatingSupersenses with Conditional Random Fields.
InProceedings of ECMLPKDD.Alan Ritter, Sam Clark, Mausam, and Oren Etzioni.2011.
Named entity recognition in tweets: an ex-perimental study.
In EMNLP.Frank Rosenblatt.
1958.
The perceptron: a probabilis-tic model for information storage and organizationin the brain.
Psychological Review, 65(6):386?408.Nathan Schneider, Behrang Mohit, Kemal Oflazer, andNoah A Smith.
2012.
Coarse lexical semantic an-notation with supersenses: an arabic case study.
InProceedings of the 50th Annual Meeting of the As-sociation for Computational Linguistics, pages 253?258.
Association for Computational Linguistics.Scott Thede and Mary Harper.
1999.
A second-orderhidden Markov model for part-of-speech tagging.
InACL.Stephen Tratz and Eduard Hovy.
2010.
Isi: automaticclassification of relations between nominals using amaximum entropy classifier.
In Proceedings of the5th International Workshop on Semantic Evaluation,pages 222?225.
Association for Computational Lin-guistics.Yoshimasa Tsuruoka, Yusuke Miyao, and Jun?ichiKazama.
2011.
Learning with lookahead: canhistory-based models rival globally optimized mod-els?
In CoNLL.10Yulia Tsvetkov, Elena Mukomel, and Anatole Gersh-man.
2013.
Cross-lingual metaphor detection us-ing common semantic features.
Meta4NLP 2013,page 45.Yulia Tsvetkov, Nathan Schneider, Dirk Hovy, ArchnaBhatia, Manaal Faruqui, and Chris Dyer.
2014.Augmenting english adjective senses with super-senses.
In Proc.
of LREC.Patrick Ye and Timothy Baldwin.
2007.
Melb-yb:Preposition sense disambiguation using rich seman-tic features.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, pages 241?244.Association for Computational Linguistics.Deniz Yuret and Mehmet Yatbaz.
2010.
The noisychannel model for unsupervised word sense disam-biguation.
Computational Linguistics, 36:111?127.11
