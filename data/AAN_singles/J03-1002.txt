c?
2003 Association for Computational LinguisticsA Systematic Comparison of VariousStatistical Alignment ModelsFranz Josef Och?
Hermann Ney?University of Southern California RWTH AachenWe present and compare various methods for computing word alignments using statistical orheuristic models.
We consider the five alignment models presented in Brown, Della Pietra, DellaPietra, and Mercer (1993), the hidden Markov alignment model, smoothing techniques, andrefinements.
These statistical models are compared with two heuristic models based on the Dicecoefficient.
We present different methods for combining word alignments to perform a symmetriza-tion of directed statistical alignment models.
As evaluation criterion, we use the quality of theresulting Viterbi alignment compared to a manually produced reference alignment.
We evaluatethe models on the German-English Verbmobil task and the French-English Hansards task.
Weperform a detailed analysis of various design decisions of our statistical alignment system andevaluate these on training corpora of various sizes.
An important result is that refined align-ment models with a first-order dependence and a fertility model yield significantly better resultsthan simple heuristic models.
In the Appendix, we present an efficient training algorithm for thealignment models presented.1.
IntroductionWe address in this article the problem of finding the word alignment of a bilingualsentence-aligned corpus by using language-independent statistical methods.
There isa vast literature on this topic, and many different systems have been suggested tosolve this problem.
Our work follows and extends the methods introduced by Brown,Della Pietra, Della Pietra, and Mercer (1993) by using refined statistical models forthe translation process.
The basic idea of this approach is to develop a model of thetranslation process with the word alignment as a hidden variable of this process, toapply statistical estimation theory to compute the ?optimal?
model parameters, andto perform alignment search to compute the best word alignment.So far, refined statistical alignment models have in general been rarely used.
Onereason for this is the high complexity of these models, which makes them difficultto understand, implement, and tune.
Instead, heuristic models are usually used.
Inheuristic models, the word alignments are computed by analyzing some associationscore metric of a link between a source language word and a target language word.These models are relatively easy to implement.In this article, we focus on consistent statistical alignment models suggested in theliterature, but we also describe a heuristic association metric.
By providing a detaileddescription and a systematic evaluation of these alignment models, we give the readervarious criteria for deciding which model to use for a given task.?
Information Science Institute (USC/ISI), 4029 Via Marina, Suite 1001, Marina del Rey, CA 90292.?
Lehrstuhl fu?r Informatik VI, Computer Science Department, RWTH Aachen?University of Technology,D-52056 Aachen, Germany.20Computational Linguistics Volume 29, Number 1Figure 1Example of a word alignment (VERBMOBIL task).We propose to measure the quality of an alignment model by comparing the qual-ity of the most probable alignment, the Viterbi alignment, with a manually producedreference alignment.
This has the advantage of enabling an automatic evaluation to beperformed.
In addition, we shall show that this quality measure is a precise and reli-able evaluation criterion that is well suited to guide designing and training statisticalalignment models.The software used to train the statistical alignment models described in this articleis publicly available (Och 2000).1.1 Problem DefinitionWe follow Brown, Della Pietra, Della Pietra, and Mercer (1993) to define alignmentas an object for indicating the corresponding words in a parallel text.
Figure 1 showsan example.
Very often, it is difficult for a human to judge which words in a giventarget string correspond to which words in its source string.
Especially problematicis the alignment of words within idiomatic expressions, free translations, and missingfunction words.
The problem is that the notion of ?correspondence?
between wordsis subjective.
It is important to keep this in mind in the evaluation of word alignmentquality.
We shall deal with this problem in Section 5.The alignment between two word strings can be quite complicated.
Often, analignment includes effects such as reorderings, omissions, insertions, and word-to-phrase alignments.
Therefore, we need a very general representation of alignment.Formally, we use the following definition for alignment in this article.
We are givena source (French) string f J1 = f1, .
.
.
, fj, .
.
.
, fJ and a target language (English) stringeI1 = e1, .
.
.
, ei, .
.
.
, eI that have to be aligned.
We define an alignment between the twoword strings as a subset of the Cartesian product of the word positions; that is, an21Och and Ney Comparison of Statistical Alignment Modelsalignment A is defined asA ?
{(j, i): j = 1, .
.
.
, J; i = 1, .
.
.
, I} (1)Modeling the alignment as an arbitrary relation between source and target languagepositions is quite general.
The development of alignment models that are able to dealwith this general representation, however, is hard.
Typically, the alignment models pre-sented in the literature impose additional constraints on the alignment representation.Typically, the alignment representation is restricted in a way such that each sourceword is assigned to exactly one target word.
Alignment models restricted in this wayare similar to the concept of hidden Markov models (HMMs) in speech recognition.The alignment mapping in such models consists of associations j ?
i = aj from sourceposition j to target position i = aj.
The alignment aJ1 = a1, .
.
.
, aj, .
.
.
, aJ may containalignments aj = 0 with the ?empty?
word e0 to account for source words that arenot aligned with any target word.
Constructed in such a way, the alignment is nota relation between source and target language positions, but only a mapping fromsource to target language positions.In Melamed (2000), a further simplification is performed that enforces a one-to-onealignment for nonempty words.
This means that the alignment mapping aJ1 must beinjective for all word positions aj > 0.
Note that many translation phenomena cannotbe handled using restricted alignment representations such as this one.
Especially,methods such as Melamed?s are in principle not able to achieve a 100% recall.
Theproblem can be reduced through corpus preprocessing steps that perform groupingand splitting of words.Some papers report improvements in the alignment quality of statistical methodswhen linguistic knowledge is used (Ker and Chang 1997; Huang and Choi 2000).
Inthese methods, the linguistic knowledge is used mainly to filter out incorrect align-ments.
In this work, we shall avoid making explicit assumptions concerning the lan-guage used.
By avoiding these assumptions, we expect our approach to be applicableto almost every language pair.
The only assumptions we make are that the paralleltext is segmented into aligned sentences and that the sentences are segmented intowords.
Obviously, there are additional implicit assumptions in the models that areneeded to obtain a good alignment quality.
For example, in languages with a veryrich morphology, such as Finnish, a trivial segmentation produces a high number ofwords that occur only once, and every learning method suffers from a significant datasparseness problem.1.2 ApplicationsThere are numerous applications for word alignments in natural language processing.These applications crucially depend on the quality of the word alignment (Och andNey 2000; Yarowsky and Wicentowski 2000).
An obvious application for word align-ment methods is the automatic extraction of bilingual lexica and terminology fromcorpora (Smadja, McKeown, and Hatzivassiloglou 1996; Melamed 2000).Statistical alignment models are often the basis of single-word-based statisticalmachine translation systems (Berger et al 1994; Wu 1996; Wang and Waibel 1998;Nie?en et al 1998; Garc?
?a-Varea, Casacuberta, and Ney 1998; Och, Ueffing, and Ney2001; Germann et al 2001).
In addition, these models are the starting point for re-fined phrase-based statistical (Och and Weber 1998; Och, Tillmann, and Ney 1999)or example-based translation systems (Brown 1997).
In such systems, the quality ofthe machine translation output directly depends on the quality of the initial wordalignment (Och and Ney 2000).22Computational Linguistics Volume 29, Number 1Another application of word alignments is in the field of word sense disambigua-tion (Diab 2000).
In Yarowsky, Ngai, and Wicentowski (2001), word alignment is usedto transfer text analysis tools such as morphologic analyzers or part-of-speech taggersfrom a language, such as English, for which many tools already exist to languages forwhich such resources are scarce.1.3 OverviewIn Section 2, we review various statistical alignment models and heuristic models.We present a new statistical alignment model, a log-linear combination of the bestmodels of Vogel, Ney, and Tillmann (1996) and Brown, Della Pietra, Della Pietra, andMercer (1993).
In Section 3, we describe the training of the alignment models andpresent a new training schedule that yields significantly better results.
In addition,we describe how to deal with overfitting, deficient models, and very small or verylarge training corpora.
In Section 4, we present some heuristic methods for improvingalignment quality by performing a symmetrization of word alignments.
In Section 5,we describe an evaluation methodology for word alignment methods dealing withthe ambiguities associated with the word alignment annotation based on generalizedprecision and recall measures.
In Section 6, we present a systematic comparison of thevarious statistical alignment models with regard to alignment quality and translationquality.
We assess the effect of training corpora of various sizes and the use of aconventional bilingual dictionary.
In the literature, it is often claimed that the refinedalignment models of Brown, Della Pietra, Della Pietra, and Mercer (1993) are notsuitable for small corpora because of data sparseness problems.
We show that this isnot the case if these models are parametrized suitably.
In the Appendix, we describesome methods for efficient training of fertility-based alignment models.2.
Review of Alignment Models2.1 General ApproachesWe distinguish between two general approaches to computing word alignments: sta-tistical alignment models and heuristic models.
In the following, we describe bothtypes of models and compare them from a theoretical viewpoint.The notational convention we employ is as follows.
We use the symbol Pr(?
)to denote general probability distributions with (almost) no specific assumptions.
Incontrast, for model-based probability distributions, we use the generic symbol p(?
).2.1.1 Statistical Alignment Models.
In statistical machine translation, we try to modelthe translation probability Pr(f J1 | eI1), which describes the relationship between asource language string f J1 and a target language string eI1.
In (statistical) alignmentmodels Pr(f J1, aJ1 | eI1), a ?hidden?
alignment aJ1 is introduced that describes a mappingfrom a source position j to a target position aj.
The relationship between the translationmodel and the alignment model is given byPr(f J1 | eI1) =?aJ1Pr(f J1, aJ1 | eI1) (2)The alignment aJ1 may contain alignments aj = 0 with the empty word e0 to accountfor source words that are not aligned with any target word.In general, the statistical model depends on a set of unknown parameters ?
that islearned from training data.
To express the dependence of the model on the parameter23Och and Ney Comparison of Statistical Alignment Modelsset, we use the following notation:Pr(f J1, aJ1 | eI1) = p?
(fJ1, aJ1 | eI1) (3)The art of statistical modeling is to develop specific statistical models that capturethe relevant properties of the considered problem domain.
In our case, the statisticalalignment model has to describe the relationship between a source language stringand a target language string adequately.To train the unknown parameters ?, we are given a parallel training corpus con-sisting of S sentence pairs {(fs, es) : s = 1, .
.
.
, S}.
For each sentence pair (fs, es), thealignment variable is denoted by a = aJ1.
The unknown parameters ?
are determinedby maximizing the likelihood on the parallel training corpus:??
= argmax?S?s=1?ap?
(fs, a | es) (4)Typically, for the kinds of models we describe here, the expectation maximization (EM)algorithm (Dempster, Laird, and Rubin 1977) or some approximate EM algorithm isused to perform this maximization.
To avoid a common misunderstanding, however,note that the use of the EM algorithm is not essential for the statistical approach, butonly a useful tool for solving this parameter estimation problem.Although for a given sentence pair there is a large number of alignments, we canalways find a best alignment:a?J1 = argmaxaJ1p??
(fJ1, aJ1 | eI1) (5)The alignment a?J1 is also called the Viterbi alignment of the sentence pair (fJ1, eI1).
(Forthe sake of simplicity, we shall drop the index ?
if it is not explicitly needed.
)Later in the article, we evaluate the quality of this Viterbi alignment by comparingit to a manually produced reference alignment.
The parameters of the statistical align-ment models are optimized with respect to a maximum-likelihood criterion, whichis not necessarily directly related to alignment quality.
Such an approach, however,requires training with manually defined alignments, which is not done in the researchpresented in this article.
Experimental evidence shows (Section 6) that the statisticalalignment models using this parameter estimation technique do indeed obtain a goodalignment quality.In this paper, we use Models 1 through 5 described in Brown, Della Pietra, DellaPietra, and Mercer (1993), the hidden Markov alignment model described in Vogel,Ney, and Tillmann (1996) and Och and Ney (2000), and a new alignment model, whichwe call Model 6.
All these models use a different decomposition of the probabilityPr(f J1, aJ1 | eI1).2.1.2 Heuristic Models.
Considerably simpler methods for obtaining word alignmentsuse a function of the similarity between the types of the two languages (Smadja, Mc-Keown, and Hatzivassiloglou 1996; Ker and Chang 1997; Melamed 2000).
Frequently,variations of the Dice coefficient (Dice 1945) are used as this similarity function.
Foreach sentence pair, a matrix including the association scores between every word atevery position is then obtained:dice(i, j) =2 ?
C(ei, fj)C(ei) ?
C(fj)(6)24Computational Linguistics Volume 29, Number 1C(e, f ) denotes the co-occurrence count of e and f in the parallel training corpus.
C(e)and C(f ) denote the count of e in the target sentences and the count of f in the sourcesentences, respectively.
From this association score matrix, the word alignment is thenobtained by applying suitable heuristics.
One method is to choose as alignment aj = ifor position j the word with the largest association score:aj = argmaxi{dice(i, j)} (7)A refinement of this method is the competitive linking algorithm (Melamed 2000).In a first step, the highest-ranking word position (i, j) is aligned.
Then, the correspond-ing row and column are removed from the association score matrix.
This procedure isiteratively repeated until every source or target language word is aligned.
The advan-tage of this approach is that indirect associations (i.e., words that co-occur often butare not translations of each other) occur less often.
The resulting alignment containsonly one-to-one alignments and typically has a higher precision than the heuristicmodel defined in equation (7).2.1.3 A Comparison of Statistical Models and Heuristic Models.
The main advan-tage of the heuristic models is their simplicity.
They are very easy to implement andunderstand.
Therefore, variants of the heuristic models described above are widelyused in the word alignment literature.One problem with heuristic models is that the use of a specific similarity functionseems to be completely arbitrary.
The literature contains a large variety of differentscoring functions, some including empirically adjusted parameters.
As we show inSection 6, the Dice coefficient results in a worse alignment quality than the statisticalmodels.In our view, the approach of using statistical alignment models is more coherent.The general principle for coming up with an association score between words resultsfrom statistical estimation theory, and the parameters of the models are adjusted suchthat the likelihood of the models on the training corpus is maximized.2.2 Statistical Alignment Models2.2.1 Hidden Markov Alignment Model.
The alignment model Pr(f J1, aJ1 | eI1) can bestructured without loss of generality as follows:Pr(f J1, aJ1 | eI1) = Pr(J | eI1) ?J?j=1Pr(fj, aj | f j?11 , aj?11 , eI1) (8)= Pr(J | eI1) ?J?j=1Pr(aj | f j?11 , aj?11 , eI1) ?
Pr(fj | fj?11 , aj1, eI1) (9)Using this decomposition, we obtain three different probabilities: a length probabilityPr(J | eI1), an alignment probability Pr(aj | fj?11 , aj?11 , eI1) and a lexicon probabilityPr(fj | f j?11 , aj1, eI1).
In the hidden Markov alignment model, we assume a first-orderdependence for the alignments aj and that the lexicon probability depends only on theword at position aj:Pr(aj | f j?11 , aj?11 , eI1) = p(aj | aj?1, I) (10)Pr(fj | f j?11 , aj1, eI1) = p(fj | eaj) (11)25Och and Ney Comparison of Statistical Alignment ModelsLater in the article, we describe a refinement with a dependence on eaj?1 in thealignment model.
Putting everything together and assuming a simple length modelPr(J | eI1) = p(J | I), we obtain the following basic HMM-based decomposition ofp(f J1 | eI1):p(f J1 | eI1) = p(J | I) ?
?aJ1J?j=1[p(aj | aj?1, I) ?
p(fj | eaj)] (12)with the alignment probability p(i | i?, I) and the translation probability p(f | e).To make the alignment parameters independent of absolute word positions, weassume that the alignment probabilities p(i | i?, I) depend only on the jump width(i?
i?).
Using a set of non-negative parameters {c(i?
i?
)}, we can write the alignmentprobabilities in the formp(i | i?, I) = c(i ?
i?)?Ii?
?=1 c(i??
?
i?
)(13)This form ensures that the alignment probabilities satisfy the normalization constraintfor each conditioning word position i?, i?
= 1, .
.
.
, I.
This model is also referred to as ahomogeneous HMM (Vogel, Ney, and Tillmann 1996).
A similar idea was suggestedby Dagan, Church, and Gale (1993).In the original formulation of the hidden Markov alignment model, there is noempty word that generates source words having no directly aligned target word.
Weintroduce the empty word by extending the HMM network by I empty words e2II+1.The target word ei has a corresponding empty word ei+I (i.e., the position of the emptyword encodes the previously visited target word).
We enforce the following constraintson the transitions in the HMM network (i ?
I, i?
?
I) involving the empty word e0:1p(i + I | i?, I) = p0 ?
?
(i, i?)
(14)p(i + I | i?
+ I, I) = p0 ?
?
(i, i?)
(15)p(i | i?
+ I, I) = p(i | i?, I) (16)The parameter p0 is the probability of a transition to the empty word, which has to beoptimized on held-out data.
In our experiments, we set p0 = 0.2.Whereas the HMM is based on first-order dependencies p(i = aj | aj?1, I) for thealignment distribution, Models 1 and 2 use zero-order dependencies p(i = aj | j, I, J):?
Model 1 uses a uniform distribution p(i | j, I, J) = 1/(I + 1):Pr(f J1, aJ1 | eI1) =p(J | I)(I + 1)J?J?j=1p(fj | eaj) (17)Hence, the word order does not affect the alignment probability.?
In Model 2, we obtainPr(f J1, aJ1 | eI1) = p(J | I) ?J?j=1[p(aj | j, I, J) ?
p(fj | eaj)] (18)1 ?
(i, i?)
is the Kronecker function, which is one if i = i?
and zero otherwise.26Computational Linguistics Volume 29, Number 1To reduce the number of alignment parameters, we ignore thedependence on J in the alignment model and use a distribution p(aj | j, I)instead of p(aj | j, I, J).2.3 Fertility-Based Alignment ModelsIn the following, we give a short description of the fertility-based alignment modelsof Brown, Della Pietra, Della Pietra, and Mercer (1993).
A gentle introduction can befound in Knight (1999b).The fertility-based alignment models (Models 3, 4, and 5) (Brown, Della Pietra,Della Pietra, and Mercer 1993) have a significantly more complicated structure thanthe simple Models 1 and 2.
The fertility ?i of a word ei in position i is defined as thenumber of aligned source words:?i =?j?
(aj, i) (19)The fertility-based alignment models contain a probability p(?
| e) that the target worde is aligned to ?
words.
By including this probability, it is possible to explicitly describethe fact that for instance the German word u?bermorgen produces four English words(the day after tomorrow).
In particular, the fertility ?
= 0 is used for prepositionsor articles that have no direct counterpart in the other language.To describe the fertility-based alignment models in more detail, we introduce,as an alternative alignment representation, the inverted alignments, which define amapping from target to source positions rather than the other way around.
We allowseveral positions in the source language to be covered; that is, we consider alignmentsB of the formB: i ?
Bi ?
{1, .
.
.
, j, .
.
.
, J}.
(20)An important constraint for the inverted alignment is that all positions of the sourcesentence must be covered exactly once; that is, the Bi have to form a partition of theset {1, .
.
.
, j, .
.
.
, J}.
The number of words ?i = |Bi| is the fertility of the word ei.
In thefollowing, Bik refers to the kth element of Bi in ascending order.The inverted alignments BI0 are a different way to represent normal alignmentsaJ1.
The set B0 contains the positions of all source words that are aligned with theempty word.
Fertility-based alignment models use the following decomposition andassumptions:2Pr(f J1, aJ1 | eI1) = Pr(fJ1, BI0 | eI1) (21)= Pr(B0 | BI1) ?I?i=1Pr(Bi | Bi?11 , eI1) ?
Pr(fJ1 | BI0, eI1) (22)= p(B0 | BI1) ?I?i=1p(Bi | Bi?1, ei) ?I?i=0?j?Bip(fj | ei) (23)As might be seen from this equation, we have tacitly assumed that the set B0 of wordsaligned with the empty word is generated only after the nonempty positions have2 The original description of the fertility-based alignment models in Brown, Della Pietra, Della Pietra,and Mercer (1993) includes a more refined derivation of the fertility-based alignment models.27Och and Ney Comparison of Statistical Alignment Modelsbeen covered.
The distribution p(Bi | Bi?1, ei) is different for Models 3, 4, and 5:?
In Model 3, the dependence of Bi on its predecessor Bi?1 is ignored:p(Bi | Bi?1, ei) = p(?i | ei) ?i!
?j?Bip(j | i, J) (24)We obtain an (inverted) zero-order alignment model p(j | i, J).?
In Model 4, every word is dependent on the previous aligned word andon the word classes of the surrounding words.
First, we describe thedependence of alignment positions.
(The dependence on word classes isfor now ignored and will be introduced later.)
We have two (inverted)first-order alignment models: p=1(?j | ?
?
?)
and p>1(?j | ?
?
?).
Thedifference between this model and the first-order alignment model in theHMM lies in the fact that here we now have a dependence along thej-axis instead of a dependence along the i-axis.
The model p=1(?j | ?
?
?)
isused to position the first word of a set Bi, and the model p>1(?j | ?
?
?)
isused to position the remaining words from left to right:p(Bi | Bi?1, ei) = p(?i | ei) ?p=1(Bi1 ?B?
(i) | ?
?
?
)?i?k=2p>1(Bik ?Bi,k?1 | ?
?
?)
(25)The function i ?
i?
= ?
(i) gives the largest value i?
< i for which |Bi?
| > 0.The symbol B?
(i) denotes the average of all elements in B?(i).?
Both Model 3 and Model 4 ignore whether or not a source position hasbeen chosen.
In addition, probability mass is reserved for sourcepositions outside the sentence boundaries.
For both of these reasons, theprobabilities of all valid alignments do not sum to unity in these twomodels.
Such models are called deficient (Brown, Della Pietra, DellaPietra, and Mercer 1993).
Model 5 is a reformulation of Model 4 with asuitably refined alignment model to avoid deficiency.
(We omit thespecific formula.
We note only that the number of alignment parametersfor Model 5 is significantly larger than for Model 4.
)Models 3, 4, and 5 define the probability p(B0 | BI1) as uniformly distributed for the?0!
possibilities given the number of words aligned with the empty word ?0 = |B0|.Assuming a binomial distribution for the number of words aligned with the emptyword, we obtain the following distribution for B0:p(B0 | BI1) = p(?0 |I?i=1?i)?
1?0!
(26)=(J ?
?0?0)(1 ?
p1)J?2?0 p?01 ?1?0!
(27)The free parameter p1 is associated with the number of words that are aligned withthe empty word.
There are ?0!
ways to order the ?0 words produced by the emptyword, and hence, the alignment model of the empty word is nondeficient.
As we will28Computational Linguistics Volume 29, Number 1see in Section 3.2, this creates problems for Models 3 and 4.
Therefore, we modifyModels 3 and 4 slightly by replacing ?0!
in equation (27) with J?0 :p(B0 | BI1) =(J ?
?0?0)(1 ?
p1)J?2?0 p?01 ?1J?0(28)As a result of this modification, the alignment models for both nonempty words andthe empty word are deficient.2.3.1 Model 6.
As we shall see, the alignment models with a first-order dependence(HMM, Models 4 and 5) produce significantly better results than the other alignmentmodels.
The HMM predicts the distance between subsequent source language po-sitions, whereas Model 4 predicts the distance between subsequent target languagepositions.
This implies that the HMM makes use of locality in the source language,whereas Model 4 makes use of locality in the target language.
We expect to achievebetter alignment quality by using a model that takes into account both types of de-pendencies.
Therefore, we combine HMM and Model 4 in a log-linear way and callthe resulting model Model 6:p6(f, a | e) =p4(f, a | e)?
?
pHMM(f, a | e)?a?,f?
p4(f?, a?
| e)?
?
pHMM(f?, a?
| e)(29)Here, the interpolation parameter ?
is employed to weigh Model 4 relative to thehidden Markov alignment model.
In our experiments, we use Model 4 instead ofModel 5, as it is significantly more efficient in training and obtains better results.In general, we can perform a log-linear combination of several models pk(f, a | e),k = 1, .
.
.
, K byp6(f, a | e) =?Kk=1 pk(f, a | e)?k?a?,f?
?Kk=1 pk(f?, a?
| e)?k(30)The interpolation parameters ?k are determined in such a way that the alignmentquality on held-out data is optimized.We use a log-linear combination instead of the simpler linear combination be-cause the values of Pr(f, a | e) typically differ by orders of magnitude for HMM andModel 4.
In such a case, we expect the log-linear combination to be better than a linearcombination.2.3.2 Alignment Models Depending on Word Classes.
For HMM and Models 4 and5, it is straightforward to extend the alignment parameters to include a dependenceon the word classes of the surrounding words (Och and Ney 2000).
In the hiddenMarkov alignment model, we allow for a dependence of the position aj on the classof the preceding target word C(eaj?1): p(aj | aj?1, I, C(eaj?1)).
Similarly, we can includedependencies on source and target word classes in Models 4 and 5 (Brown, DellaPietra, Della Pietra, and Mercer 1993).
The categorization of the words into classes(here: 50 classes) is performed automatically by using the statistical learning proceduredescribed in Kneser and Ney (1993).2.3.3 Overview of Models.
The main differences among the statistical alignment mod-els lie in the alignment model they employ (zero-order or first-order), the fertilitymodel they employ, and the presence or absence of deficiency.
In addition, the modelsdiffer with regard to the efficiency of the E-step in the EM algorithm (Section 3.1).Table 1 offers an overview of the properties of the various alignment models.29Och and Ney Comparison of Statistical Alignment ModelsTable 1Overview of the alignment models.Model Alignment model Fertility model E-step DeficientModel 1 uniform no exact noModel 2 zero-order no exact noHMM first-order no exact noModel 3 zero-order yes approximative yesModel 4 first-order yes approximative yesModel 5 first-order yes approximative noModel 6 first-order yes approximative yes2.4 Computation of the Viterbi AlignmentWe now develop an algorithm to compute the Viterbi alignment for each alignmentmodel.
Although there exist simple polynomial algorithms for the baseline Models 1and 2, we are unaware of any efficient algorithm for computing the Viterbi alignmentfor the fertility-based alignment models.For Model 2 (also for Model 1 as a special case), we obtaina?J1 = argmaxaJ1Pr(f J1, aJ1 | eI1) (31)= argmaxaJ1??
?p(J | I) ?J?j=1[p(aj | j, I) ?
p(fj | eaj)]???
(32)=[argmaxaj{p(aj | j, I) ?
p(fj | eaj)}]Jj=1(33)Hence, the maximization over the (I+1)J different alignments decomposes into J max-imizations of (I + 1) lexicon probabilities.
Similarly, the Viterbi alignment for Model 2can be computed with a complexity of O(I ?
J).Finding the optimal alignment for the HMM is more complicated than for Model 1or Model 2.
Using a dynamic programming approach, it is possible to obtain the Viterbialignment for the HMM with a complexity of O(I2 ?
J) (Vogel, Ney, and Tillmann 1996).For the refined alignment models, however, namely, Models 3, 4, 5, and 6, max-imization over all alignments cannot be efficiently carried out.
The correspondingsearch problem is NP-complete (Knight 1990a).
For short sentences, a possible so-lution could be an A* search algorithm (Och, Ueffing, and Ney 2001).
In the workpresented here, we use a more efficient greedy search algorithm for the best align-ment, as suggested in Brown, Della Pietra, Della Pietra, and Mercer (1993).
The basicidea is to compute the Viterbi alignment of a simple model (such as Model 2 or HMM).This alignment is then iteratively improved with respect to the alignment probabilityof the refined alignment model.
(For further details on the greedy search algorithm,see Brown, Della Pietra, Della Pietra, and Mercer [1993].)
In the Appendix, we presentmethods for performing an efficient computation of this pseudo-Viterbi alignment.3.
Training3.1 EM AlgorithmIn this section, we describe our approach to determining the model parameters ?.Every model has a specific set of free parameters.
For example, the parameters ?
for30Computational Linguistics Volume 29, Number 1Model 4 consist of lexicon, alignment, and fertility parameters:?
= {{p(f | e)}, {p=1(?j | ?
?
?
)}, {p>1(?j | ?
?
?
)}, {p(?
| e)}, p1} (34)To train the model parameters ?, we use a maximum-likelihood approach, as describedin equation (4), by applying the EM algorithm (Baum 1972).
The different models aretrained in succession on the same data; the final parameter values of a simpler modelserve as the starting point for a more complex model.In the E-step of Model 1, the lexicon parameter counts for one sentence pair (e, f)are calculated:c(f | e; e, f) =?e,fN(e, f)?aPr(a | e, f)?j?
(f , fj)?
(e, eaj) (35)Here, N(e, f) is the training corpus count of the sentence pair (f, e).
In the M-step, thelexicon parameters are computed:p(f | e) =?s c(f | e; fs, es)?s,f c(f | e; fs, es)(36)Similarly, the alignment and fertility probabilities can be estimated for all other align-ment models (Brown, Della Pietra, Della Pietra, and Mercer 1993).
When bootstrappingfrom a simpler model to a more complex model, the simpler model is used to weigh thealignments, and the counts are accumulated for the parameters of the more complexmodel.In principle, the sum over all (I+1)J alignments has to be calculated in the E-step.Evaluating this sum by explicitly enumerating all alignments would be infeasible.Fortunately, Models 1 and 2 and HMM have a particularly simple mathematical formsuch that the EM algorithm can be implemented efficiently (i.e., in the E-step, it ispossible to efficiently evaluate all alignments).
For the HMM, this is referred to as theBaum-Welch algorithm (Baum 1972).Since we know of no efficient way to avoid the explicit summation over all align-ments in the EM algorithm in the fertility-based alignment models, the counts arecollected only over a subset of promising alignments.
For Models 3 to 6, we performthe count collection only over a small number of good alignments.
To keep the trainingfast, we consider only a small fraction of all alignments.
We compare three differentmethods for using subsets of varying sizes:?
The simplest method is to perform Viterbi training using only the bestalignment found.
As the Viterbi alignment computation itself is verytime consuming for Models 3 to 6, the Viterbi alignment is computedonly approximately, using the method described in Brown, Della Pietra,Della Pietra, and Mercer (1993).?
Al-Onaizan et al (1999) suggest using as well the neighboringalignments of the best alignment found.
(For an exact definition of theneighborhood of an alignment, the reader is referred to the Appendix.)?
Brown, Della Pietra, Della Pietra, and Mercer (1993) use an even largerset of alignments, including also the pegged alignments, a large set ofalignments with a high probability Pr(f J1, aJ1 | eI1).
The method forconstructing these alignments (Brown, Della Pietra, Della Pietra, andMercer 1993) guarantees that for each lexical relationship in everysentence pair, at least one alignment is considered.31Och and Ney Comparison of Statistical Alignment ModelsIn Section 6, we show that by using the HMM instead of Model 2 in bootstrap-ping the fertility-based alignment models, the alignment quality can be significantlyimproved.
In the Appendix, we present an efficient training algorithm of the fertility-based alignment models.3.2 Is Deficiency a Problem?When using the EM algorithm on the standard versions of Models 3 and 4, we observethat during the EM iterations more and more words are aligned with the empty word.This results in a poor alignment quality, because too many words are aligned to theempty word.
This progressive increase in the number of words aligned with the emptyword does not occur when the other alignment models are used.
We believe that thisis due to the deficiency of Model 3 and Model 4.The use of the EM algorithm guarantees that the likelihood increases for eachiteration.
This holds for both deficient and nondeficient models.
For deficient models,however, as the amount of deficiency in the model is reduced, the likelihood increases.In Models 3 and 4 as defined in Brown, Della Pietra, Della Pietra, and Mercer (1993),the alignment model for nonempty words is deficient, but the alignment model forthe empty word is nondeficient.
Hence, the EM algorithm can increase likelihood bysimply aligning more and more words with the empty word.3Therefore, we modify Models 3 and 4 slightly, such that the empty word also hasa deficient alignment model.
The alignment probability is set to p(j | i, J) = 1/J for eachsource word aligned with the empty word.
Another remedy, adopted in Och and Ney(2000), is to choose a value for the parameter p1 of the empty-word fertility and keepit fixed.3.3 SmoothingTo overcome the problem of overfitting on the training data and to enable the modelsto cope better with rare words, we smooth the alignment and fertility probabilities.
Forthe alignment probabilities of the HMM (and similarly for Models 4 and 5), we performan interpolation with a uniform distribution p(i | j, I) = 1/I using an interpolationparameter ?:p?
(aj | aj?1, I) = (1 ?
?)
?
p(aj | aj?1, I) + ?
?1I(37)For the fertility probabilities, we assume that there is a dependence on the numberof letters g(e) of e and estimate a fertility distribution p(?
| g) using the EM algorithm.Typically, longer words have a higher fertility.
By making this assumption, the modelcan learn that the longer words usually have a higher fertility than shorter words.Using an interpolation parameter ?, the fertility distribution is then computed asp?(?
| e) =(1 ?
??
+ n(e))?
p(?
| e) + ??
+ n(e)?
p(?
| g(e)) (38)Here, n(e) denotes the frequency of e in the training corpus.
This linear interpolationensures that for frequent words (i.e., n(e)  ?
), the specific distribution p(?
| e) dom-inates, and that for rare words (i.e., n(e)?
), the general distribution p(?
| g(e))dominates.The interpolation parameters ?
and ?
are determined in such a way that thealignment quality on held-out data is optimized.3 This effect did not occur in Brown, Della Pietra, Della Pietra, and Mercer (1993), as Models 3 and 4were not trained directly.32Computational Linguistics Volume 29, Number 13.4 Bilingual DictionaryA conventional bilingual dictionary can be considered an additional knowledge sourcethat can be used in training.
We assume that the dictionary is a list of word strings(e, f).
The entries for each language can be a single word or an entire phrase.To integrate a dictionary into the EM algorithm, we compare two differentmethods:?
Brown, Della Pietra, Della Pietra, Goldsmith, et al (1993) developed amultinomial model for the process of constructing a dictionary (by ahuman lexicographer).
By applying suitable simplifications, the methodboils down to adding every dictionary entry (e, f) to the training corpuswith an entry-specific count called effective multiplicity, expressed as?
(e, f):?
(e, f) =?
(e) ?
p(f | e)1 ?
e?
(e)?p(f|e) (39)In this section, ?
(e) is an additional parameter describing the size of thesample that is used to estimate the model p(f | e).
This count is thenused instead of N(e, f) in the EM algorithm as shown in equation (35).?
Och and Ney (2000) suggest that the effective multiplicity of a dictionaryentry be set to a large value ?+  1 if the lexicon entry actually occursin one of the sentence pairs of the bilingual corpus and to a low valueotherwise:?
(e, f) ={?+ if e and f co-occur??
otherwise(40)As a result, only dictionary entries that indeed occur in the trainingcorpus have a large effect in training.
The motivation behind this is toavoid a deterioration of the alignment as a result of out-of-domaindictionary entries.
Every entry in the dictionary that does co-occur in thetraining corpus can be assumed correct and should therefore obtain ahigh count.
We set ??
= 0.4.
SymmetrizationIn this section, we describe various methods for performing a symmetrization of ourdirected statistical alignment models by applying a heuristic postprocessing step thatcombines the alignments in both translation directions (source to target, target tosource).The baseline alignment model does not allow a source word to be aligned withmore than one target word.
Therefore, lexical correspondences like that of the Germancompound word Zahnarzttermin with the English dentist?s appointment cause problems,because a single source word must be mapped to two or more target words.
Therefore,the resulting Viterbi alignment of the standard alignment models has a systematic lossin recall.To solve this problem, we perform training in both translation directions (source totarget, target to source).
As a result, we obtain two alignments aJ1 and bI1 for each pairof sentences in the training corpus.
Let A1 = {(aj, j) | aj > 0} and A2 = {(i, bi) | bi > 0}denote the sets of alignments in the two Viterbi alignments.
To increase the qualityof the alignments, we combine A1 and A2 into one alignment matrix A using the33Och and Ney Comparison of Statistical Alignment Modelsfollowing combination methods:?
Intersection: A = A1 ?
A2.?
Union: A = A1 ?
A2.?
Refined method: In a first step, the intersection A = A1 ?
A2 isdetermined.
The elements of this intersection result from both Viterbialignments and are therefore very reliable.
Then, we extend thealignment A iteratively by adding alignments (i, j) occurring only in thealignment A1 or in the alignment A2 if neither fj nor ei has an alignmentin A, or if both of the following conditions hold:?
The alignment (i, j) has a horizontal neighbor (i ?
1, j), (i + 1, j)or a vertical neighbor (i, j ?
1), (i, j + 1) that is already in A.?
The set A ?
{(i, j)} does not contain alignments with bothhorizontal and vertical neighbors.Obviously, the intersection of the two alignments yields an alignment consisting ofonly one-to-one alignments with a higher precision and a lower recall than eitherone separately.
The union of the two alignments yields a higher recall and a lowerprecision of the combined alignment than either one separately.
Whether a higherprecision or a higher recall is preferred depends on the final application for whichthe word alignment is intended.
In applications such as statistical machine translation(Och, Tillmann, and Ney 1999), a higher recall is more important (Och and Ney 2000),so an alignment union would probably be chosen.
In lexicography applications, wemight be interested in alignments with a very high precision obtained by performingan alignment intersection.5.
Evaluation MethodologyIn the following, we present an annotation scheme for single-word-based alignmentsand a corresponding evaluation criterion.It is well known that manually performing a word alignment is a complicatedand ambiguous task (Melamed 1998).
Therefore, in performing the alignments forthe research presented here, we use an annotation scheme that explicitly allows forambiguous alignments.
The persons conducting the annotation are asked to specifyalignments of two different kinds: an S (sure) alignment, for alignments that are un-ambiguous, and a P (possible) alignment, for ambiguous alignments.
The P label isused especially to align words within idiomatic expressions and free translations andmissing function words (S ?
P).The reference alignment thus obtained may contain many-to-one and one-to-manyrelationships.
Figure 2 shows an example of a manually aligned sentence with S andP labels.The quality of an alignment A = {(j, aj) | aj > 0} is then computed by appropriatelyredefined precision and recall measures:recall =|A ?
S||S| , precision =|A ?
P||A| (41)and the following alignment error rate (AER), which is derived from the well-knownF-measure:AER(S, P; A) = 1 ?
|A ?
S|+ |A ?
P||A|+ |S| (42)34Computational Linguistics Volume 29, Number 1Figure 2A manual alignment with S (filled squares) and P (unfilled squares) connections.These definitions of precision, recall and the AER are based on the assumptionthat a recall error can occur only if an S alignment is not found and a precision errorcan occur only if the found alignment is not even P.The set of sentence pairs for which the manual alignment is produced is randomlyselected from the training corpus.
It should be emphasized that all the training of themodels is performed in a completely unsupervised way (i.e., no manual alignmentsare used).
From this point of view, there is no need to have a test corpus separate fromthe training corpus.Typically, the annotation is performed by two human annotators, producing setsS1, P1, S2, P2.
To increase the quality of the resulting reference alignment, the anno-tators are presented with the mutual errors and asked to improve their alignmentswhere possible.
(Mutual errors of the two annotators A and B are the errors in thealignment of annotator A if we assume the alignment of annotator B as reference andthe errors in the alignment of annotator B if we assume the alignment of annotator Aas reference.)
From these alignments, we finally generate a reference alignment thatcontains only those S connections on which both annotators agree and all P connec-tions from both annotators.
This can be accomplished by forming the intersection ofthe sure alignments (S = S1?S2) and the union of the possible alignments (P = P1?P2),respectively.
By generating the reference alignment in this way, we obtain an alignmenterror rate of 0 percent when we compare the S alignments of every single annotatorwith the combined reference alignment.6.
ExperimentsWe present in this section results of experiments involving the Verbmobil and Hansardstasks.
The Verbmobil task (Wahlster 2000) is a (German-English) speech translation task35Och and Ney Comparison of Statistical Alignment ModelsTable 2Corpus characteristics of the Verbmobil task.German EnglishTraining corpus Sentences 34,446 ?
34KWords 329,625 343,076Vocabulary 5,936 3,505Singletons 2,600 1,305Bilingual dictionary Entries 4,404Words 4,758 5,543Test corpus Sentences 354Words 3,233 3,109Table 3Corpus characteristics of the Hansards task.French EnglishTraining corpus Sentences 1470KWords 24.33M 22.16MVocabulary 100,269 78,332Singletons 40,199 31,319Bilingual dictionary Entries 28,701Words 28,702 30,186Test corpus Sentences 500Words 8,749 7,946in the domain of appointment scheduling, travel planning, and hotel reservation.
Thebilingual sentences used in training are correct transcriptions of spoken dialogues.However, they include spontaneous speech effects such as hesitations, false starts, andungrammatical phrases.
The French-English Hansards task consists of the debates inthe Canadian parliament.
This task has a very large vocabulary of about 100,000 Frenchwords and 80,000 English words.4Statistics for the two corpora are shown in Tables 2 and 3.
The number of runningwords and the vocabularies are based on full-form words and the punctuation marks.We produced smaller training corpora by randomly choosing 500, 2,000 and 8,000sentences from the Verbmobil task and 500, 8,000, and 128,000 sentences from theHansards task.For both tasks, we manually aligned a randomly chosen subset of the trainingcorpus.
From this subset of the corpus, the first 100 sentences are used as the de-velopment corpus to optimize the model parameters that are not trained via the EM4 We do not use the Blinker annotated corpus described in Melamed (1998), since the domain is veryspecial (the Bible) and a different annotation methodology is used.36Computational Linguistics Volume 29, Number 1Table 4Comparison of alignment error rate percentages for various training schemes (Verbmobil task;Dice+C: Dice coefficient with competitive linking).Size of training corpusModel Training scheme 0.5K 2K 8K 34KDice 28.4 29.2 29.1 29.0Dice+C 21.5 21.8 20.1 20.4Model 1 15 19.3 19.0 17.8 17.0Model 2 1525 27.7 21.0 15.8 13.5HMM 15H5 19.1 15.4 11.4 9.2Model 3 152533 25.8 18.4 13.4 10.315H533 18.1 14.3 10.5 8.1Model 4 15253343 23.4 14.6 10.0 7.715H543 17.3 11.7 9.1 6.515H53343 16.8 11.7 8.4 6.3Model 5 15H54353 17.3 11.4 8.7 6.215H5334353 16.9 11.8 8.5 5.8Model 6 15H54363 17.2 11.3 8.8 6.115H5334363 16.4 11.7 8.0 5.7algorithm (e.g., the smoothing parameters).
The remaining sentences are used as thetest corpus.The sequence of models used and the number of training iterations used for eachmodel is referred to in the following as the training scheme.
Our standard train-ing scheme on Verbmobil is 15H5334363.
This notation indicates that five iterationsof Model 1, five iterations of HMM, three iterations of Model 3, three iterationsof Model 4, and three iterations of Model 6 are performed.
On Hansards, we use15H10334363.
This training scheme typically gives very good results and does not leadto overfitting.
We use the slightly modified versions of Model 3 and Model 4 describedin Section 3.2 and smooth the fertility and the alignment parameters.
In the E-step ofthe EM algorithm for the fertility-based alignment models, we use the Viterbi align-ment and its neighborhood.
Unless stated otherwise, no bilingual dictionary is usedin training.6.1 Models and Training SchemesTables 4 and 5 compare the alignment quality achieved using various models andtraining schemes.
In general, we observe that the refined models (Models 4, 5, and 6)yield significantly better results than the simple Model 1 or Dice coefficient.
Typically,the best results are obtained with Model 6.
This holds across a wide range of sizesfor the training corpus, from an extremely small training corpus of only 500 sentencesup to a training corpus of 1.5 million sentences.
The improvement that results fromusing a larger training corpus is more significant, however, if more refined models areused.
Interestingly, even on a tiny corpus of only 500 sentences, alignment error ratesunder 30% are achieved for all models, and the best models have error rates somewhatunder 20%.We observe that the alignment quality obtained with a specific model heavilydepends on the training scheme that is used to bootstrap the model.37Och and Ney Comparison of Statistical Alignment ModelsTable 5Comparison of alignment error rate percentages for various training schemes (Hansards task;Dice+C: Dice coefficient with competitive linking).Size of training corpusModel Training scheme 0.5K 8K 128K 1.47MDice 50.9 43.4 39.6 38.9Dice+C 46.3 37.6 35.0 34.0Model 1 15 40.6 33.6 28.6 25.9Model 2 1525 46.7 29.3 22.0 19.5HMM 15H5 26.3 23.3 15.0 10.8Model 3 152533 43.6 27.5 20.5 18.015H533 27.5 22.5 16.6 13.2Model 4 15253343 41.7 25.1 17.3 14.115H53343 26.1 20.2 13.1 9.415H543 26.3 21.8 13.3 9.3Model 5 15H54353 26.5 21.5 13.7 9.615H5334353 26.5 20.4 13.4 9.4Model 6 15H54363 26.0 21.6 12.8 8.815H5334363 25.9 20.3 12.5 8.7Figure 3Comparison of alignment error rate (in percent) for Model 1 and Dice coefficient (left: 34KVerbmobil task, right: 128K Hansards task).6.2 Heuristic Models versus Model 1We pointed out in Section 2 that from a theoretical viewpoint, the main advantageof statistical alignment models in comparison to heuristic models is the well-foundedmathematical theory that underlies their parameter estimation.
Tables 4 and 5 showthat the statistical alignment models significantly outperform the heuristic Dice coef-ficient and the heuristic Dice coefficient with competitive linking (Dice+C).
Even thesimple Model 1 achieves better results than the two Dice coefficient models.It is instructive to analyze the alignment quality obtained in the EM training ofModel 1.
Figure 3 shows the alignment quality over the iteration numbers of Model 1.We see that the first iteration of Model 1 achieves significantly worse results than theDice coefficient, but by only the second iteration, Model 1 gives better results than theDice coefficient.38Computational Linguistics Volume 29, Number 1Table 6Effect of using more alignments in training fertility models on alignment error rate (Verbmobiltask).
Body of table presents error rate percentages.Size of training corpusTraining scheme Alignment set 0.5K 2K 8K 34KViterbi 17.8 12.6 8.6 6.615H5334363 +neighbors 16.4 11.7 8.0 5.7+pegging 16.4 11.2 8.2 5.7Viterbi 24.1 16.0 11.6 8.61525334353 +neighbors 22.9 14.2 9.8 7.6+pegging 22.0 13.3 9.7 6.9Table 7Effect of using more alignments in training fertility models on alignment error rate (Hansardstask).
Body of table presents error rate percentages.Size of training corpusTraining scheme Alignment set 0.5K 8K 128KViterbi 25.8 20.3 12.615H10334363 +neighbors 25.9 20.3 12.5+pegging 25.8 19.9 12.6Viterbi 41.9 25.1 17.61525334353 +neighbors 41.7 24.8 16.1+pegging 41.2 23.7 15.86.3 Model 2 versus HMMAn important result of these experiments is that the hidden Markov alignment modelachieves significantly better results than Model 2.
We attribute this to the fact that theHMM is a homogeneous first-order alignment model, and such models are able tobetter represent the locality and monotonicity properties of natural languages.
Bothmodels have the important property of allowing an efficient implementation of theEM algorithm (Section 3).
On the largest Verbmobil task, the HMM achieves an im-provement of 3.8% over Model 2.
On the largest Hansards task, the improvement is8.7%.
Interestingly, this advantage continues to hold after bootstrapping more refinedmodels.
On Model 4, the improvement is 1.4% and 4.8%, respectively.We conclude that it is important to bootstrap the refined alignment models withgood initial parameters.
Obviously, if we use Model 2 for bootstrapping, we eventuallyobtain a poor local optimum.6.4 The Number of Alignments in TrainingIn Tables 6 and 7, we compare the results obtained by using different numbers ofalignments in the training of the fertility-based alignment models.
We compare thethree different approaches described in Section 3: using only the Viterbi alignment,using in addition the neighborhood of the Viterbi alignment, and using the peggedalignments.
To reduce the training time, we restrict the number of pegged alignmentsby using only those in which Pr(f, a | e) is not much smaller than the probability of theViterbi alignment.
This reduces the training time drastically.
For the large Hansards39Och and Ney Comparison of Statistical Alignment ModelsTable 8Computing time on the 34K Verbmobil task (on 600 MHz Pentium III machine).Seconds per iterationAlignment set Model 3 Model 4 Model 5Viterbi 48.0 251.0 248.0+neighbors 101.0 283.0 276.0+pegging 129.0 3,348.0 3,356.0Table 9Effect of smoothing on alignment error rate (Verbmobil task, Model 6).
Body of table presentserror rate percentages.Size of training corpusSmoothing method 0.5K 2K 8K 34KNone 19.7 14.9 10.9 8.3Fertility 18.4 14.3 10.3 8.0Alignment 16.8 13.2 9.1 6.4Alignment and fertility 16.4 11.7 8.0 5.7corpus, however, there still is an unacceptably large training time.
Therefore, we reportthe results for only up to 128,000 training sentences.The effect of pegging strongly depends on the quality of the starting point usedfor training the fertility-based alignment models.
If we use Model 2 as the startingpoint, we observe a significant improvement when we use the neighborhood align-ments and the pegged alignments.
If we use only the Viterbi alignment, the results aresignificantly worse than using additionally the neighborhood of the Viterbi alignment.If we use HMM as the starting point, we observe a much smaller effect.
We concludethat using more alignments in training is a way to avoid a poor local optimum.Table 8 shows the computing time for performing one iteration of the EM algo-rithm.
Using a larger set of alignments increases the training time for Model 4 andModel 5 significantly.
Since using the pegging alignments yields only a moderateimprovement in performance, all following results are obtained by using the neigh-borhood of the Viterbi alignment without pegging.6.5 Effect of SmoothingTables 9 and 10 show the effect on the alignment error rate of smoothing the alignmentand fertility probabilities.
We observe a significant improvement when we smooththe alignment probabilities and a minor improvement when we smooth the fertilityprobabilities.
An analysis of the alignments shows that smoothing the fertility proba-bilities significantly reduces the frequently occurring problem of rare words forming?garbage collectors?
in that they tend to align with too many words in the otherlanguage (Brown, Della Pietra, Della Pietra, Goldsmith, et al 1993).Without smoothing, we observe early overfitting: The alignment error rate in-creases after the second iteration of HMM, as shown in Figure 4.
On the Verbmobiltask, the best alignment error rate is obtained in the second iteration.
On the Hansardstask, the best alignment error rate is obtained in the sixth iteration.
In iterations sub-sequent to the second on the Verbmobil task and the sixth on the Hansards task, thealignment error rate increases significantly.
With smoothing of the alignment param-40Computational Linguistics Volume 29, Number 1Figure 4Overfitting on the training data with the hidden Markov alignment model using varioussmoothing parameters (top: 34K Verbmobil task, bottom: 128K Hansards task).41Och and Ney Comparison of Statistical Alignment ModelsTable 10Effect of smoothing on alignment error rate (Hansards task, Model 6).
Body of table presentserror rate percentages.Size of training corpusSmoothing method 0.5K 8K 128K 1470KNone 28.6 23.3 13.3 9.5Fertility 28.3 22.5 12.7 9.3Alignment 26.5 21.2 13.0 8.9Alignment and fertility 25.9 20.3 12.5 8.7Table 11Effect of word classes on alignment error rate (Verbmobil task).
Body of table presents errorrate percentages.Size of training corpusWord classes 0.5K 2K 8K 34KNo 16.5 11.7 8.0 6.3Yes 16.4 11.7 8.0 5.7Table 12Effect of word classes on alignment error rate (Hansards task).
Body of table presents errorrate percentages.Size of training corpusWord classes 0.5K 8K 128K 1470KNo 25.5 20.7 12.8 8.9Yes 25.9 20.3 12.5 8.7eters, we obtain a lower alignment error rate, overfitting occurs later in the process,and its effect is smaller.6.6 Alignment Models Depending on Word ClassesTables 11 and 12 show the effects of including a dependence on word classes in thealignment model, as described in Section 2.3.
The word classes are always trainedon the same subset of the training corpus as is used for the training of the align-ment models.
We observe no significant improvement in performance as a resultof including dependence on word classes when a small training corpus is used.
Apossible reason for this lack of improvement is that either the word classes them-selves or the resulting large number of alignment parameters cannot be estimatedreliably using a small training corpus.
When a large training corpus is used, however,there is a clear improvement in performance on both the Verbmobil and the Hansardstasks.6.7 Using a Conventional Bilingual DictionaryTables 13 and 14 show the effect of using a conventional bilingual dictionary in trainingon the Verbmobil and Hansards tasks, respectively.
We compare the two methods forusing the dictionary described in Section 3.4.
We observe that the method with a fixed42Computational Linguistics Volume 29, Number 1Table 13Effect of using a conventional dictionary on alignment error rate (Verbmobil task).
Body oftable presents error rate percentages.Size of training corpusBilingual dictionary 0.5K 2K 8K 34KNo 16.4 11.7 8.0 5.7Yes/?
var.
10.9 9.0 6.9 5.1Yes/?+ = 8 9.7 7.6 6.0 5.1Yes/?+ = 16 10.0 7.8 6.0 4.6Yes/?+ = 32 10.4 8.5 6.4 4.7Table 14Effect of using a conventional dictionary on alignment error rate (Hansards task).
Body oftable presents error rate percentages.Size of training corpusBilingual dictionary 0.5K 8K 128K 1470KNo 25.9 20.3 12.5 8.7Yes/?
var.
23.3 18.3 12.3 8.6Yes/?+ = 8 22.7 18.5 12.2 8.6Yes/?+ = 16 23.1 18.7 12.1 8.6Yes/?+ = 32 24.9 20.2 11.7 8.3threshold of ?+ = 16 gives the best results.
The method with a varying ?
gives worseresults, but this method has one fewer parameter to be optimized on held-out data.On small corpora, there is an improvement of up to 6.7% on the Verbmobil taskand 3.2% on the Hansards task, but when a larger training corpus is used, the im-provements are reduced to 1.1% and 0.4%, respectively.
Interestingly, the amountof the overall improvement contributed by the use of a conventional dictionary issmall compared to the improvement achieved through the use of better alignmentmodels.6.8 Generalized AlignmentsIn this section, we compare the results obtained using different translation directionsand using the symmetrization methods described in Section 4.
Tables 15 and 16 showprecision, recall, and alignment error rate for the last iteration of Model 6 for bothtranslation directions.
In this experiment, we use the conventional dictionary as well.Particularly for the Verbmobil task, with the language pair German-English, we ob-serve that for German as the source language the alignment error rate is much higherthan for English as source language.
A possible reason for this difference in the align-ment error rates is that the baseline alignment representation as a vector aJ1 does notallow German word compounds (which occur frequently) to be aligned with morethan one English word.The effect of merging alignments by forming the intersection, the union, or therefined combination of the Viterbi alignments in both translation directions is shown inTables 17 and 18.
Figure 5 shows the corresponding precision/recall graphs.
By usingthe refined combination, we can increase precision and recall on the Hansards task.
Thelowest alignment error rate on the Hansards task is obtained by using the intersection43Och and Ney Comparison of Statistical Alignment ModelsTable 15Effect of training corpus size and translation direction on precision, recall, and alignment errorrate (Verbmobil task + dictionary).
All figures are percentages.English ?
German German ?
EnglishCorpus size Precision Recall AER Precision Recall AER0.5K 87.6 93.1 10.0 77.9 80.3 21.12K 90.5 94.4 7.8 88.1 88.1 11.98K 92.7 95.7 6.0 90.2 89.1 10.334K 94.6 96.3 4.6 92.5 89.5 8.8Table 16Effect of training corpus size and translation direction on precision, recall, and alignment errorrate (Hansards task + dictionary).
All figures are percentages.English ?
French French ?
EnglishCorpus size Precision Recall AER Precision Recall AER0.5K 73.0 83.8 23.1 68.5 79.1 27.88K 77.0 88.9 18.7 76.0 88.5 19.5128K 84.5 93.5 12.1 84.6 93.3 12.21470K 89.4 94.7 8.6 89.1 95.2 8.6Table 17Effect of alignment combination on precision, recall, and alignment error rate (Verbmobil task+ dictionary).
All figures are percentages.Intersection Union Refined methodCorpus size Precision Recall AER Precision Recall AER Precision Recall AER0.5K 97.5 76.8 13.6 74.8 96.1 16.9 87.8 92.9 9.92K 97.2 85.6 8.6 84.1 96.9 10.6 91.3 94.2 7.48K 97.5 86.6 8.0 87.0 97.7 8.5 92.8 96.0 5.834K 98.1 87.6 7.2 90.6 98.4 6.0 94.0 96.9 4.7Table 18Effect of alignment combination on precision, recall, and alignment error rate (Hansards task +dictionary).
All figures are percentages.Intersection Union Refined methodCorpus size Precision Recall AER Precision Recall AER Precision Recall AER0.5K 91.5 71.3 18.7 63.4 91.6 29.0 75.5 84.9 21.18K 95.6 82.8 10.6 68.2 94.4 24.2 83.3 90.0 14.2128K 96.7 90.0 6.3 77.8 96.9 16.1 89.4 94.4 8.71470K 96.8 92.3 5.2 84.2 97.6 11.3 91.5 95.5 7.044Computational Linguistics Volume 29, Number 1Figure 5Effect of various symmetrization methods on precision and recall for different training corpussizes (top: Verbmobil task, bottom: Hansards task).45Och and Ney Comparison of Statistical Alignment Modelsmethod.
By forming a union or intersection of the alignments, we can obtain very highrecall or precision values on both the Hansards task and the Verbmobil task.6.9 Effect of Alignment Quality on Translation QualityAlignment models similar to those studied in this article have been used as a start-ing point for refined phrase-based statistical machine translation systems (Alshawi,Bangalore, and Douglas 1998; Och, Tillmann, and Ney 1999; Ney et al 2000).
In Ochand Ney (2000), the overall result of the experimental evaluation has been that animproved alignment quality yields an improved subjective quality of the statisticalmachine translation system as well.7.
ConclusionIn this article, we have discussed in detail various statistical and heuristic word align-ment models and described various modifications and extensions to models known inthe literature.
We have developed a new statistical alignment model (Model 6) that hasyielded the best results among all the models we considered in the experiments wehave conducted.
We have presented two methods for including a conventional bilin-gual dictionary in training and described heuristic symmetrization algorithms thatcombine alignments in both translation directions possible between two languages,producing an alignment with a higher precision, a higher recall, or an improved align-ment error rate.We have suggested measuring the quality of an alignment model using the qualityof the Viterbi alignment compared to that achieved in a manually produced referencealignment.
This quality measure has the advantage of automatic evaluation.
To pro-duce the reference alignment, we have used a refined annotation scheme that reducesthe problems and ambiguities associated with the manual construction of a wordalignment.We have performed various experiments to assess the effect of different alignmentmodels, training schemes, and knowledge sources.
The key results of these experi-ments are as follows:?
Statistical alignment models outperform the simple Dice coefficient.?
The best results are obtained with our Model 6.
In general, veryimportant ingredients of a good model seem to be a first-orderdependence between word positions and a fertility model.?
Smoothing and symmetrization have a significant effect on the alignmentquality achieved by a particular model.?
The following methods have only a minor effect on the quality ofalignment achieved by a particular model:?
adding entries of a conventional bilingual dictionary to thetraining data.?
making the alignment models dependent on word classes (as inModels 4 and 5).?
increasing the number of alignments used in the approximationof the EM algorithm for the fertility-based alignment models.Further improvements in alignments are expected to be produced through theadoption of cognates (Simard, Foster, and Isabelle 1992) and from statistical alignment46Computational Linguistics Volume 29, Number 1models based on word groups rather than single words (Och, Tillmann, and Ney1999).
The use of models that explicitly deal with the hierarchical structures of naturallanguage is very promising (Wu 1996; Yamada and Knight 2001).We plan to develop structured models for the lexicon, alignment, and fertility prob-abilities using maximum-entropy models.
This is expected to allow an easy integrationof more dependencies, such as in a second-order alignment model, without runninginto the problem of the number of alignment parameters getting unmanageably large.Furthermore, it will be important to verify the applicability of the statistical align-ment models examined in this article to less similar language pairs such as Chinese-English and Japanese-English.Appendix: Efficient Training of Fertility-Based Alignment ModelsIn this Appendix, we describe some methods for efficient training of fertility-basedalignment models.
The core idea is to enumerate only a small subset of good align-ments in the E-step of the EM algorithm instead of enumerating all (I + 1)J align-ments.
This small subset of alignments is the set of neighboring alignments of thebest alignment that can be found by a greedy search algorithm.
We use two operatorsto transform alignments: The move operator m[i,j](a) changes aj := i, and the swapoperator s[j1,j2](a) exchanges aj1 and aj2 .
The neighborhood N (a) of an alignment a isthen defined as the set of all alignments that differ by one move or one swap fromalignment a:N (a) = {a?
: ?i,j : a?
= m[i,j](a) ?
?j1,j2 : a?
= s[j1,j2](a)} (43)For one step of the greedy search algorithm, we define the following hill-climbingoperator (for Model 3), which yields for an alignment a the most probable alignmentb(a) in the neighborhood N (a):b(a) = argmaxa?
?N (a)p3(a?
| e, f) (44)Similarly, we define a hill-climbing operator for the other alignment models.Straightforward ImplementationA straightforward count collection procedure for a sentence pair (f,e) following thedescription in Brown, Della Pietra, Della Pietra, and Mercer (1993) is as follows:51.
Calculate the Viterbi alignment of Model 2: a0 := argmaxa p2(f, a | e),n := 0.2.
While in the neighborhood N (an) an alignment a?
exists withp3(a?
| e, f) > p3(an | e, f):(a) Set an+1 to the best alignment in the neighborhood.
(b) n := n + 1.3.
Calculates :=?a?N (an)Pr(f, a | e) (45)5 To simplify the description, we ignore the process known as pegging, which generates a bigger numberof alignments considered in training.47Och and Ney Comparison of Statistical Alignment Models4.
For each alignment a in the neighborhood N (an)(a) Calculatep := Pr(a | e, f) (46)=Pr(f, a | e)s(47)(b) For each j := 1 to J: Increase alignment countsc(j | aj, m, l; e, f) := c(j | aj, m, l; e, f) + p (48)(c) For each i := 1 to I: Increase the fertility counts with p:c(?i | ei; e, f) := c(?i | ei; e, f) + p (49)(d) Increase the counts for p1:c(1; e, f) := c(1; e, f) + p ?
?0 (50)A major part of the time in this procedure is spent on calculating the probabilityPr(a?
| e, f) of an alignment a?.
In general, this takes about (I + J) operations.
Brown,Della Pietra, Della Pietra, and Mercer (1993) describe a method for obtaining Pr(a?
|e, f) incrementally from Pr(a | e, f) if alignment a differs only by moves or swaps fromalignment a?.
This method results in a constant number of operations that is sufficientto calculate the score of a move or the score of a swap.Refined Implementation: Fast Hill ClimbingAnalyzing the training program reveals that most of the time is spent on the compu-tation of the costs of moves and swaps.
To reduce the number of operations requiredin such computation, these values are cached in two matrices.
We use one matrix forthe scores of a move aj := i:Mij =Pr(m[i,j](a) | e, f)Pr(a | e, f) ?
(1 ?
?
(aj, i)) (51)and an additional matrix for the scores of a swap of aj and aj?
:Sjj ?
=???Pr(s[j,j?
](a) | e, f)Pr(a | e, f) ?
(1 ?
?
(aj, aj?))
if j < j?0 otherwise(52)During the hill climbing, it is sufficient, after making a move or a swap, to updateonly those rows or columns in the matrix that are affected by the move or swap.
Forexample, when performing a move aj := i, it is necessary to?
update in matrix M the columns j?
with aj?
= aj or aj?
= i.?
update in matrix M the rows aj and i.?
update in matrix S the rows and the columns j?
with aj?
= aj or aj?
= i.Similar updates have to be performed after a swap.
In the count collection (step 3), itis possible to use the same matrices as obtained in the last hill-climbing step.By restricting in this way the number of matrix entries that need to be updated,it is possible to reduce the number of operations in hill climbing by about one orderof magnitude.48Computational Linguistics Volume 29, Number 1Refined Implementation: Fast Count CollectionThe straightforward algorithm given for performing the count collection has the dis-advantage of requiring that all alignments in the neighborhood of alignment a beenumerated explicitly.
In addition, it is necessary to perform a loop over all targetsand a loop over all source positions to update the lexicon/alignment and the fertil-ity counts.
To perform the count collection in an efficient way, we use the fact thatthe alignments in the neighborhood N (a) are very similar.
This allows the sharing ofmany operations in the count collection process.To efficiently obtain the alignment and lexicon probability counts, we introduce thefollowing auxiliary quantities that use the move and swap matrices that are availableafter performing the hill climbing described above:?
probability of all alignments in the neighborhood N (a):Pr(N (a) | e, f) =?a?
?N (a)Pr(a?
| e, f) (53)= Pr(a | e, f) ??
?1 +?i,jMij +?j,j?Sjj ???
(54)?
probability of all alignments in the neighborhood N (a) that differ inposition j from alignment a:Pr(Nj(a) | e, f) =?a?
?N (a)Pr(a?
| e, f)(1 ?
?
(aj, a?j)) (55)= Pr(a | e, f)??
?iMij +?j?
(Sjj ?
+ Sj?j)??
(56)For the alignment counts c(j | i; e, f) and the lexicon counts c(f | e; e, f), we havec(j | i; e, f) ={Pr(N (a) | e, f)?Pr(Nj(a) | e, f) if i=ajPr(a | e, f)(Mij +?j?
?(aj?
, i)?
(Sjj ?+Sj?j))if i =aj(57)c(f | e; e, f) =?i?jc(j | i; e, f) ?
?
(f , fj) ?
?
(e, ei) (58)To obtain the fertility probability counts and the count for p1 efficiently, we intro-duce the following auxiliary quantities:?
probability of all alignments that have an increased fertility for position i:Pr(N+1i (a) | e, f) = Pr(a | f, e)??
?j(1 ?
?
(aj, i)) ?
Mij??
(59)?
probability of all alignments that have a decreased fertility for position i:Pr(N?1i (a) | e, f) = Pr(a | e, f)???j?
(aj, i)?i?Mi?j??
(60)49Och and Ney Comparison of Statistical Alignment Models?
probability of all alignments that have an unchanged fertility for posi-tion i:Pr(N+0i (a) | e, f) = Pr(N (a) | e, f)?
Pr(N+1i (a) | e, f)?
Pr(N?1i (a) | e, f) (61)These quantities do not depend on swaps, since a swap does not change the fertilitiesof an alignment.
For the fertility counts, we have:c(?
| e; e, f) =?i?
(e, ei)?kPr(N+ki (a) | e, f)?
(?i + k,?)
(62)For p1, we have:c(1; e, f) =?kPr(N+k0 (a) | e, f)(?0 + k) (63)Using the auxiliary quantities, a count collection algorithm can be formulated thatrequires about O(max(I, J)2) operations.
This is one order of magnitude faster than thestraightforward algorithm described above.
In practice, we observe that the resultingtraining is 10?20 times faster.AcknowledgmentsThis work has been partially supported aspart of the Verbmobil project (contractnumber 01 IV 701 T4) by the GermanFederal Ministry of Education, Science,Research and Technology and as part of theEuTrans project (project number 30268) bythe European Union.
In addition, this workhas been partially supported by theNational Science Foundation under grantno.
IIS-9820687 through the 1999 Workshopon Language Engineering, Center forLanguage and Speech Processing, JohnsHopkins University.
All work for this paperwas done at RWTH Aachen.ReferencesAl-Onaizan, Yaser, Jan Curin, Michael Jahr,Kevin Knight, John D. Lafferty, I. DanMelamed, David Purdy, Franz J. Och,Noah A. Smith, and David Yarowsky.1999.
Statistical machine translation.
FinalReport, JHU Workshop.
Available athttp://www.clsp.jhu.edu/ws99/projects/mt/final report/mt-final-report.ps.Alshawi, Hiyan, Srinivas Bangalore, andShona Douglas.
1998.
Automaticacquisition of hierarchical transductionmodels for machine translation.
InCOLING?ACL ?98: 36th Annual Meeting ofthe Association for Computational Linguisticsand 17th International Conference onComputational Linguistics, volume 1,pages 41?47, Montreal, Canada, August.Baum, L. E. 1972.
An inequality andassociated maximization technique instatistical estimation for probabilisticfunctions of Markov processes.Inequalities, 3:1?8.Berger, Adam L., Peter F. Brown, Stephen A.Della Pietra, Vincent J. Della Pietra,John R. Gillett, John D. Lafferty, HarryPrintz, and Lubos Ures.
1994.
TheCandide system for machine translation.In Proceedings of the ARPA Workshop onHuman Language Technology,pages 157?162, Plainsboro, New Jersey,March.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, M. J. Goldsmith,J.
Hajic, R. L. Mercer, and S. Mohanty.1993.
But dictionaries are data too.
InProceedings of the ARPA Workshop on HumanLanguage Technology, pages 202?205,Plainsboro, New Jersey, March.Brown, Peter F., Stephen A. Della Pietra,Vincent J. Della Pietra, and R. L. Mercer.1993.
The mathematics of statisticalmachine translation: Parameterestimation.
Computational Linguistics,19(2):263?311.Brown, Ralf D. 1997.
Automated dictionaryextraction for ?knowledge-free?example-based translation.
In SeventhInternational Conference on Theoretical andMethodological Issues in Machine Translation(TMI-97), pages 111?118, Santa Fe, NewMexico, July.Dagan, Ido, Kenneth W. Church, and50Computational Linguistics Volume 29, Number 1William A. Gale.
1993.
Robust bilingualword alignment for machine aidedtranslation.
In Proceedings of the Workshopon Very Large Corpora, pages 1?8,Columbus, Ohio, June.Dempster, A. P., N. M. Laird, and D. B.Rubin.
1977.
Maximum likelihood fromincomplete data via the EM algorithm.Journal of the Royal Statistical Society,Series B, 39(1):1?22.Diab, Mona.
2000.
An unsupervised methodfor multilingual word sense tagging usingparallel corpora: A preliminaryinvestigation.
In ACL-2000 Workshop onWord Senses and Multilinguality, pages 1?9,Hong Kong, October.Dice, Lee R. 1945.
Measures of the amountof ecologic association between species.Journal of Ecology, 26:297?302.Garc?
?a-Varea, Ismael, Francisco Casacuberta,and Hermann Ney.
1998.
An iterative,DP-based search algorithm for statisticalmachine translation.
In Proceedings of theInternational Conference on Spoken LanguageProcessing (ICSLP?98), pages 1235?1238,Sydney, Australia, November.Germann, Ulrich, Michael Jahr, KevinKnight, Daniel Marcu, and Kenji Yamada.2001.
Fast decoding and optimal decodingfor machine translation.
In Proceedings ofthe 39th Annual Meeting of the Association forComputational Linguistics (ACL),pages 228?235, Toulouse, France, July.Huang, Jin-Xia and Key-Sun Choi.
2000.Chinese-Korean word alignment based onlinguistic comparison.
In Proceedings of the38th Annual Meeting of the Association forComputational Linguistics (ACL),pages 392?399, Hong Kong, October.Ker, Sue J. and Jason S. Chang.
1997.
Aclass-based approach to word alignment.Computational Linguistics, 23(2):313?343.Kneser, Reinhard and Hermann Ney.
1993.Improved clustering techniques forclass-based statistical language modelling.In European Conference on SpeechCommunication and Technology,pages 973?976, Berlin, Germany,September.Knight, Kevin.
1999a.
Decoding complexityin word-replacement translation models.Computational Linguistics, 25(4):607?615.Knight, Kevin.
1999b.
A Statistical MTTutorial Workbook.
Available athttp://www.isi.edu/natural-language/mt/wkbk.rtf.Melamed, I. Dan.
1998.
Manual annotationof translational equivalence: The blinkerproject.
Technical Report 98-07, Institutefor Research in Cognitive Science,Philadelphia.Melamed, I. Dan.
2000.
Models oftranslational equivalence among words.Computational Linguistics, 26(2):221?249.Ney, Hermann, Sonja Nie?en, Franz J. Och,Hassan Sawaf, Christoph Tillmann, andStephan Vogel.
2000.
Algorithms forstatistical translation of spoken language.IEEE Transactions on Speech and AudioProcessing, 8(1):24?36.Nie?en, Sonja, Stephan Vogel, HermannNey, and Christoph Tillmann.
1998.
ADP-based search algorithm for statisticalmachine translation.
In COLING-ACL ?98:36th Annual Meeting of the Association forComputational Linguistics and 17thInternational Conference on ComputationalLinguistics, pages 960?967, Montreal,Canada, August.Och, Franz J.
2000.
Giza++: Training ofstatistical translation models.
Available athttp://www-i6.informatik.rwth-aachen.de/?och/software/GIZA++.html.Och, Franz J. and Hermann Ney.
2000.
Acomparison of alignment models forstatistical machine translation.
In COLING?00: The 18th International Conference onComputational Linguistics, pages 1086?1090,Saarbru?cken, Germany, August.Och, Franz J., Christoph Tillmann, andHermann Ney.
1999.
Improved alignmentmodels for statistical machine translation.In Proceedings of the Joint SIGDATConference on Empirical Methods in NaturalLanguage Processing and Very Large Corpora,pages 20?28, University of Maryland,College Park, June.Och, Franz J., Nicola Ueffing, and HermannNey.
2001.
An efficient A* searchalgorithm for statistical machinetranslation.
In Data-Driven MachineTranslation Workshop, pages 55?62,Toulouse, France, July.Och, Franz J. and Hans Weber.
1998.Improving statistical natural languagetranslation with categories and rules.
InCOLING-ACL ?98: 36th Annual Meeting ofthe Association for Computational Linguisticsand 17th International Conference onComputational Linguistics, pages 985?989,Montreal, Canada, August.Simard, M., G. Foster, and P. Isabelle.
1992.Using cognates to align sentences inbilingual corpora.
In Fourth InternationalConference on Theoretical and MethodologicalIssues in Machine Translation (TMI-92),pages 67?81, Montreal, Canada.Smadja, Frank, Kathleen R. McKeown, andVasileios Hatzivassiloglou.
1996.Translating collocations for bilinguallexicons: A statistical approach.Computational Linguistics, 22(1):1?38.51Och and Ney Comparison of Statistical Alignment ModelsVogel, Stephan, Hermann Ney, andChristoph Tillmann.
1996.
HMM-basedword alignment in statistical translation.In COLING ?96: The 16th InternationalConference on Computational Linguistics,pages 836?841, Copenhagen, Denmark,August.Wahlster, Wolfgang, editor.
2000.
Verbmobil:Foundations of speech-to-speech translations.Springer Verlag, Berlin.Wang, Ye-Yi and Alex Waibel.
1998.
Fastdecoding for statistical machinetranslation.
In Proceedings of theInternational Conference on Speech andLanguage Processing, pages 1357?1363,Sydney, Australia, November.Wu, Dekai.
1996.
A polynomial-timealgorithm for statistical machinetranslation.
In Proceedings of the 34thAnnual Conference of the Association forComputational Linguistics (ACL ?96),pages 152?158, Santa Cruz, California,June.Yamada, Kenji and Kevin Knight.
2001.
Asyntax-based statistical translation model.In Proceedings of the 39th Annual Meeting ofthe Association for Computational Linguistics(ACL), pages 523?530, Toulouse, France,July.Yarowsky, David, Grace Ngai, and RichardWicentowski.
2001.
Inducing multilingualtext analysis tools via robust projectionacross aligned corpora.
In HumanLanguage Technology Conference, pages109?116, San Diego, California, March.Yarowsky, David and Richard Wicentowski.2000.
Minimally supervisedmorphological analysis by multimodalalignment.
In Proceedings of the 38th AnnualMeeting of the Association for ComputationalLinguistics (ACL), pages 207?216, HongKong, October.
