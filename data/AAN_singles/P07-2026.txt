Proceedings of the ACL 2007 Demo and Poster Sessions, pages 101?104,Prague, June 2007. c?2007 Association for Computational LinguisticsMinimum Bayes Risk Decoding for BLEUNicola Ehling and Richard Zens and Hermann NeyHuman Language Technology and Pattern RecognitionLehrstuhl fu?r Informatik 6 ?
Computer Science DepartmentRWTH Aachen University, D-52056 Aachen, Germany{ehling,zens,ney}@cs.rwth-aachen.deAbstractWe present a Minimum Bayes Risk (MBR)decoder for statistical machine translation.The approach aims to minimize the expectedloss of translation errors with regard to theBLEU score.
We show that MBR decodingon N -best lists leads to an improvement oftranslation quality.We report the performance of the MBRdecoder on four different tasks: the TC-STAR EPPS Spanish-English task 2006, theNIST Chinese-English task 2005 and theGALE Arabic-English and Chinese-Englishtask 2006.
The absolute improvement of theBLEU score is between 0.2% for the TC-STAR task and 1.1% for the GALE Chinese-English task.1 IntroductionIn recent years, statistical machine translation(SMT) systems have achieved substantial progressregarding their perfomance in international transla-tion tasks (TC-STAR, NIST, GALE).Statistical approaches to machine translation wereproposed at the beginning of the nineties and foundwidespread use in the last years.
The ?standard?
ver-sion of the Bayes decision rule, which aims at a min-imization of the sentence error rate is used in vir-tually all approaches to statistical machine transla-tion.
However, most translation systems are judgedby their ability to minimize the error rate on the wordlevel or n-gram level.
Common error measures arethe Word Error Rate (WER) and the Position Inde-pendent Word Error Rate (PER) as well as evalua-tion metric on the n-gram level like the BLEU andNIST score that measure precision and fluency of agiven translation hypothesis.The remaining part of this paper is structured asfollows: after a short overview of related work inSec.
2, we describe the MBR decoder in Sec.
3.
Wepresent the experimental results in Sec.
4 and con-clude in Sec.
5.2 Related WorkMBR decoder for automatic speech recognition(ASR) have been reported to yield improvementover the widely used maximum a-posteriori prob-ability (MAP) decoder (Goel and Byrne, 2003;Mangu et al, 2000; Stolcke et al, 1997).For MT, MBR decoding was introduced in (Ku-mar and Byrne, 2004).
It was shown that MBR ispreferable over MAP decoding for different evalu-ation criteria.
Here, we focus on the performanceof MBR decoding for the BLEU score on varioustranslation tasks.3 Implementation of Minimum Bayes RiskDecoding for the BLEU Score3.1 Bayes Decision RuleIn statistical machine translation, we are given asource language sentence fJ1 = f1 .
.
.
fj .
.
.
fJ ,which is to be translated into a target language sen-tence eI1 = e1 .
.
.
ei .
.
.
eI .
Statistical decision the-ory tells us that among all possible target languagesentences, we should choose the sentence whichminimizes the Bayes risk:e?I?1 = argminI,eI1{?I?,e?I?1Pr(e?I?1 |fJ1 ) ?
L(eI1, e?I?1 )}Here, L(?, ?)
denotes the loss function under con-sideration.
In the following, we will call this deci-sion rule the MBR rule (Kumar and Byrne, 2004).101Although it is well known that this decision rule isoptimal, most SMT systems do not use it.
The mostcommon approach is to use the MAP decision rule.Thus, we select the hypothesis which maximizes theposterior probability Pr(eI1|fJ1 ):e?I?1 = argmaxI,eI1{Pr(eI1|fJ1 )}This decision rule is equivalent to the MBR crite-rion under a 0-1 loss function:L0?1(eI1, e?I?1 ) ={1 if eI1 = e?I?10 elseHence, the MAP decision rule is optimal for thesentence or string error rate.
It is not necessarilyoptimal for other evaluation metrics as for examplethe BLEU score.
One reason for the popularity ofthe MAP decision rule might be that, compared tothe MBR rule, its computation is simpler.3.2 Baseline SystemThe posterior probability Pr(eI1|fJ1 ) is modeled di-rectly using a log-linear combination of severalmodels (Och and Ney, 2002):Pr(eI1|fJ1 ) =exp(?Mm=1 ?mhm(eI1, fJ1 ))?I?,e?I?1exp(?Mm=1 ?mhm(e?I?1 , fJ1 ))(1)This approach is a generalization of the source-channel approach (Brown et al, 1990).
It has theadvantage that additional models h(?)
can be easilyintegrated into the overall system.The denominator represents a normalization fac-tor that depends only on the source sentence fJ1 .Therefore, we can omit it in case of the MAP de-cision rule during the search process.
Note that thedenominator affects the results of the MBR decisionrule and, thus, cannot be omitted in that case.We use a state-of-the-art phrase-based translationsystem similar to (Matusov et al, 2006) includingthe following models: an n-gram language model,a phrase translation model and a word-based lex-icon model.
The latter two models are used forboth directions: p(f |e) and p(e|f).
Additionally,we use a word penalty, phrase penalty and a distor-tion penalty.
The model scaling factors ?M1 are opti-mized with respect to the BLEU score as describedin (Och, 2003).3.3 BLEU ScoreThe BLEU score (Papineni et al, 2002) measuresthe agreement between a hypothesis eI1 generated bythe MT system and a reference translation e?I?1.
It isthe geometric mean of n-gram precisions Precn(?, ?
)in combination with a brevity penalty BP(?, ?)
for tooshort translation hypotheses.BLEU(eI1, e?I?1) = BP(I, I?)
?4?n=1Precn(eI1, e?I?1)1/4BP(I, I?)
={1 if I?
?
Iexp (1 ?
I/I?)
if I?
< IPrecn(eI1, e?I?1) =?wn1min{C(wn1 |eI1), C(wn1 |e?I?1)}?wn1C(wn1 |eI1)Here, C(wn1 |eI1) denotes the number of occur-rences of an n-gram wn1 in a sentence eI1.
The de-nominator of the n-gram precisions evaluate to thenumber of n-grams in the hypothesis, i.e.
I ?n+1.As loss function for the MBR decoder, we use:L[eI1, e?I?1] = 1 ?
BLEU(eI1, e?I?1) .While the original BLEU score was intended to beused only for aggregate counts over a whole test set,we use the BLEU score at the sentence-level duringthe selection of the MBR hypotheses.
Note that wewill use this sentence-level BLEU score only duringdecoding.
The translation results that we will reportlater are computed using the standard BLEU score.3.4 Hypothesis SelectionWe select the MBR hypothesis among the N besttranslation candidates of the MAP system.
For eachentry, we have to compute its expected BLEU score,i.e.
the weighted sum over all entries in the N -bestlist.
Therefore, finding the MBR hypothesis has aquadratic complexity in the size of the N -best list.To reduce this large work load, we stop the summa-tion over the translation candidates as soon as therisk of the regarded hypothesis exceeds the currentminimum risk, i.e.
the risk of the current best hy-pothesis.
Additionally, the hypotheses are processedaccording to the posterior probabilities.
Thus, wecan hope to find a good candidate soon.
This allowsfor an early stopping of the computation for each ofthe remaining candidates.1023.5 Global Model Scaling FactorDuring the translation process, the different sub-models hm(?)
get different weights ?m.
These scal-ing factors are optimized with regard to a specificevaluation criteria, here: BLEU.
This optimizationdescribes the relation between the different modelsbut does not define the absolute values for the scal-ing factors.
Because search is performed using themaximum approximation, these absolute values arenot needed during the translation process.
In con-trast to this, using the MBR decision rule, we per-form a summation over all sentence probabilitiescontained in the N -best list.
Therefore, we use aglobal scaling factor ?0 > 0 to modify the individ-ual scaling factors ?m:?
?m = ?0 ?
?m ,m = 1, ...,M.For the MBR decision rule the modified scaling fac-tors ?
?m are used instead of the original model scal-ing factors ?m to compute the sentence probabilitiesas in Eq.
1.
The global scaling factor ?0 is tuned onthe development set.
Note that under the MAP deci-sion rule any global scaling factor ?0 > 0 yields thesame result.
Similar tests were reported by (Manguet al, 2000; Goel and Byrne, 2003) for ASR.4 Experimental Results4.1 Corpus StatisticsWe tested the MBR decoder on four translationtasks: the TC-STAR EPPS Spanish-English task of2006, the NIST Chinese-English evaluation test setof 2005 and the GALE Arabic-English and Chinese-English evaluation test set of 2006.
The TC-STAREPPS corpus is a spoken language translation corpuscontaining the verbatim transcriptions of speechesof the European Parliament.
The NIST Chinese-English test sets consists of news stories.
The GALEproject text track consists of two parts: newswire(?news?)
and newsgroups (?ng?).
The newswire partis similar to the NIST task.
The newsgroups partcovers posts to electronic bulletin boards, Usenetnewsgroups, discussion groups and similar forums.The corpus statistics of the training corpora areshown in Tab.
1 to Tab.
3.
To measure the trans-lation quality, we use the BLEU score.
With ex-ception of the TC-STAR EPPS task, all scores arecomputed case-insensitive.
As BLEU measures ac-curacy, higher scores are better.Table 1: NIST Chinese-English: corpus statistics.Chinese EnglishTrain Sentences 9MWords 232M 250MVocabulary 238K 412KNIST 02 Sentences 878Words 26 431 24 352NIST 05 Sentences 1 082Words 34 908 36 027GALE 06 Sentences 460news Words 9 979 11 493GALE 06 Sentences 461ng Words 9 606 11 689Table 2: TC-Star Spanish-English: corpus statistics.Spanish EnglishTrain Sentences 1.2MWords 35M 33MVocabulary 159K 110KDev Sentences 1 452Words 51 982 54 857Test Sentences 1 780Words 56 515 58 2954.2 Translation ResultsThe translation results for all tasks are presentedin Tab.
4.
For each translation task, we tested thedecoder on N -best lists of size N=10 000, i.e.
the10 000 best translation candidates.
Note that in somecases the list is smaller because the translation sys-tem did not produce more candidates.
To analyzethe improvement that can be gained through rescor-ing with MBR, we start from a system that has al-ready been rescored with additional models like ann-gram language model, HMM, IBM-1 and IBM-4.It turned out that the use of 1 000 best candidatesfor the MBR decoding is sufficient, and leads to ex-actly the same results as the use of 10 000 best lists.Similar experiences were reported by (Mangu et al,2000; Stolcke et al, 1997) for ASR.We observe that the improvement is larger forTable 3: GALE Arabic-English: corpus statistics.Arabic EnglishTrain Sentences 4MWords 125M 124MVocabulary 421K 337Knews Sentences 566Words 14 160 15 320ng Sentences 615Words 11 195 14 493103Table 4: Translation results BLEU [%] for the NIST task, GALE task and TC-STAR task (S-E: Spanish-English; C-E: Chinese-English; A-E: Arabic-English).TC-STAR S-E NIST C-E GALE A-E GALE C-Edecision rule test 2002 (dev) 2005 news ng news ngMAP 52.6 32.8 31.2 23.6 12.2 14.6 9.4MBR 52.8 33.3 31.9 24.2 13.3 15.4 10.5Table 5: Translation examples for the GALE Arabic-English newswire task.Reference the saudi interior ministry announced in a report the implementation of the death penaltytoday, tuesday, in the area of medina (west) of a saudi citizen convicted of murdering afellow citizen.MAP-Hyp saudi interior ministry in a statement to carry out the death sentence today in the area ofmedina (west) in saudi citizen found guilty of killing one of its citizens.MBR-Hyp the saudi interior ministry announced in a statement to carry out the death sentence todayin the area of medina (west) in saudi citizen was killed one of its citizens.Reference faruq al-shar?a takes the constitutional oath of office before the syrian presidentMAP-Hyp farouk al-shara leads sworn in by the syrian presidentMBR-Hyp farouk al-shara lead the constitutional oath before the syrian presidentlow-scoring translations, as can be seen in the GALEtask.
For an ASR task, similar results were reportedby (Stolcke et al, 1997).Some translation examples for the GALE Arabic-English newswire task are shown in Tab.
5.
The dif-ferences between the MAP and the MBR hypothesesare set in italics.5 ConclusionsWe have shown that Minimum Bayes Risk decod-ing on N -best lists improves the BLEU score con-siderably.
The achieved results are promising.
Theimprovements were consistent among several eval-uation sets.
Even if the improvement is sometimessmall, e.g.
TC-STAR, it is statistically significant:the absolute improvement of the BLEU score is be-tween 0.2% for the TC-STAR task and 1.1% for theGALE Chinese-English task.
Note, that MBR de-coding is never worse than MAP decoding, and istherefore promising for SMT.
It is easy to integrateand can improve even well-trained systems by tun-ing them for a particular evaluation criterion.AcknowledgmentsThis material is partly based upon work supportedby the Defense Advanced Research Projects Agency(DARPA) under Contract No.
HR0011-06-C-0023,and was partly funded by the European Union un-der the integrated project TC-STAR (Technologyand Corpora for Speech to Speech Translation, IST-2002-FP6-506738, http://www.tc-star.org).ReferencesP.
F. Brown, J. Cocke, S. A. Della Pietra, V. J. Della Pietra,F.
Jelinek, J. D. Lafferty, R. L. Mercer, and P. S. Roossin.1990.
A statistical approach to machine translation.
Com-putational Linguistics, 16(2):79?85, June.V.
Goel and W. Byrne.
2003.
Minimum bayes-risk automaticspeech recognition.
Pattern Recognition in Speech and Lan-guage Processing.S.
Kumar and W. Byrne.
2004.
Minimum bayes-risk decod-ing for statistical machine translation.
In Proc.
Human Lan-guage Technology Conf.
/ North American Chapter of theAssoc.
for Computational Linguistics Annual Meeting (HLT-NAACL), pages 169?176, Boston, MA, May.L.
Mangu, E. Brill, and A. Stolcke.
2000.
Finding consensusin speech recognition: Word error minimization and otherapplications of confusion networks.
Computer, Speech andLanguage, 14(4):373?400, October.E.
Matusov, R. Zens, D. Vilar, A. Mauser, M. Popovic?,S.
Hasan, and H. Ney.
2006.
The RWTH machine trans-lation system.
In Proc.
TC-Star Workshop on Speech-to-Speech Translation, pages 31?36, Barcelona, Spain, June.F.
J. Och and H. Ney.
2002.
Discriminative training and max-imum entropy models for statistical machine translation.
InProc.
40th Annual Meeting of the Assoc.
for ComputationalLinguistics (ACL), pages 295?302, Philadelphia, PA, July.F.
J. Och.
2003.
Minimum error rate training in statistical ma-chine translation.
In Proc.
41st Annual Meeting of the As-soc.
for Computational Linguistics (ACL), pages 160?167,Sapporo, Japan, July.K.
Papineni, S. Roukos, T. Ward, and W. J. Zhu.
2002.
Bleu: amethod for automatic evaluation of machine translation.
InProc.
40th Annual Meeting of the Assoc.
for ComputationalLinguistics (ACL), pages 311?318, Philadelphia, PA, July.A.
Stolcke, Y. Konig, and M. Weintraub.
1997.
Explicit worderror minimization in N-best list rescoring.
In Proc.
Eu-ropean Conf.
on Speech Communication and Technology,pages 163?166, Rhodes, Greece, September.104
