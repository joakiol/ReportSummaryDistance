Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 376?383,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsA Multimodal Interface for Access to Content in the HomeMichael JohnstonAT&T LabsResearch,Florham Park,New Jersey, USAjohnston@research.att.comLuis Fernando D?HaroUniversidad Polit?cnicade Madrid,Madrid, Spainlfdharo@die.upm.esMichelle LevineAT&T LabsResearch,Florham Park,New Jersey, USAmfl@research.att.comBernard RengerAT&T LabsResearch,Florham Park,New Jersey, USArenger@research.att.comAbstractIn order to effectively access the rapidlyincreasing range of media content availablein the home, new kinds of more natural in-terfaces are needed.
In this paper, we ex-plore the application of multimodal inter-face technologies to searching and brows-ing a database of movies.
The resultingsystem allows users to access movies usingspeech, pen, remote control, and dynamiccombinations of these modalities.
An ex-perimental evaluation, with more than 40users, is presented contrasting two variantsof the system: one combining speech withtraditional remote control input and a sec-ond where the user has a tablet displaysupporting speech and pen input.1 IntroductionAs traditional entertainment channels and theinternet converge through the advent of technolo-gies such as broadband access, movies-on-demand,and streaming video, an increasingly large range ofcontent is available to consumers in the home.However, to benefit from this new wealth of con-tent, users need to be able to rapidly and easily findwhat they are actually interested in, and do so ef-fortlessly while relaxing on the couch in their liv-ing room ?
a location where they typically do nothave easy access to the keyboard, mouse, andclose-up screen display typical of desktop webbrowsing.Current interfaces to cable and satellite televi-sion services typically use direct manipulation of agraphical user interface using a remote control.
Inorder to find content, users generally have to eithernavigate a complex, pre-defined, and often deeplyembedded menu structure or type in titles or otherkey phrases using an onscreen keyboard or tripletap input on a remote control keypad.
These inter-faces are cumbersome and do not scale well as therange of content available increases (Berglund,2004; Mitchell, 1999).Figure 1 Multimodal interface on tabletIn this paper we explore the application of multi-modal interface technologies (See Andr?
(2002)for an overview) to the creation of more effectivesystems used to search and browse for entertain-ment content in the home.
A number of previoussystems have investigated the addition of unimodalspoken search queries to a graphical electronicprogram guide (Ibrahim and Johansson, 2002376(NokiaTV); Goto et al, 2003; Wittenburg et al,2006).
Wittenburg et alexperiment with unre-stricted speech input for electronic program guidesearch, and use a highlighting mechanism to pro-vide feedback to the user regarding the ?relevant?terms the system understood and used to make thequery.
However, their usability study results showthis complex output can be confusing to users anddoes not correspond to user expectations.
Othershave gone beyond unimodal speech input andadded multimodal commands combining speechwith pointing (Johansson, 2003; Portele et al2006).
Johansson (2003) describes a movie re-commender system MadFilm where users can usespeech and pointing to accept/reject recommendedmovies.
Portele et al(2006) describe the Smart-Kom-Home system which includes multimodalelectronic program guide on a tablet device.In our work we explore a broader range of inter-action modalities and devices.
The system providesusers with the flexibility to interact using spokencommands, handwritten commands, unimodalpointing (GUI) commands, and multimodal com-mands combining speech with one or more point-ing gestures made on a display.
We compare twodifferent interaction scenarios.
The first utilizes atraditional remote control for direct manipulationand pointing, integrated with a wireless micro-phone for speech input.
In this case, the onlyscreen is the main TV display (far screen).
In thesecond scenario, the user also has a second graphi-cal display (close screen) presented on a mobiletablet which supports speech and pen input, includ-ing both pointing and handwriting (Figure 1).
Ourapplication task also differs, focusing on searchand browsing of a large database of movies-on-demand and supporting queries over multiple si-multaneous dimensions.
This work also differs inthe scope of the evaluation.
Prior studies have pri-marily conducted qualitative evaluation with smallgroups of users (5 or 6).
A quantitative and qualita-tive evaluation was conducted examining the inter-action of 44 na?ve users with two variants of thesystem.
We believe this to be the first broad scaleexperimental evaluation of a flexible multimodalinterface for searching and browsing large data-bases of movie content.In Section 2, we describe the interface and illus-trate the capabilities of the system.
In Section 3,we describe the underlying multimodal processingarchitecture and how it processes and integratesuser inputs.
Section 4 describes our experimentalevaluation and comparison of the two systems.Section 5 concludes the paper.2 Interacting with the systemThe system described here is an advanced user in-terface prototype which provides multimodal ac-cess to databases of media content such as moviesor television programming.
The current databaseis harvested from publicly accessible web sourcesand contains over 2000 popular movie titles alongwith associated metadata such as cast, genre, direc-tor, plot, ratings, length, etc.The user interacts through a graphical interfaceaugmented with speech, pen, and remote controlinput modalities.
The remote control can be used tomove the current focus and select items.
The pencan be used both for selecting items (pointing atthem) and for handwritten input.
The graphicaluser interface has three main screens.
The mainscreen is the search screen (Figure 2).
There is alsoa control screen used for setting system parametersand a third comparison display used for showingmovie details side by side (Figure 4).
The user canselect among the screens using three icons in thenavigation bar at the top left of the screen.
The ar-rows provide ?Back?
and ?Next?
for navigationthrough previous searches.
Directly below, there isa feedback window which indicates whether thesystem is listening and provides feedback onspeech recognition and search.
In the tablet vari-ant, the microphone and speech recognizer are ac-tivated by tapping on ?CLICK TO SPEAK?
withthe pen.
In the remote control version, the recog-nizer can also be activated using a button on theremote control.
The main section of the searchdisplay (Figure 2) contains two panels.
The rightpanel (results panel) presents a scrollable list ofthumbnails for the movies retrieved by the currentsearch.
The left panel (details panel) provides de-tails on the currently selected title in the resultspanel.
These include the genre, plot summary,cast, and director.The system supports a speech modality, a hand-writing modality, pointing (unimodal GUI) modal-ity, and composite multimodal input where the userutters a spoken command which is combined withpointing ?gestures?
the user has made towardsscreen icons using the pen or the remote control.377Figure 2 Graphical user interfaceSpeech: The system supports speech search overmultiple different dimensions such as title, genre,cast, director, and year.
Input can be more tele-graphic with searches such as ?Legally Blonde?,?Romantic comedy?, and ?Reese Witherspoon?, ormore verbose natural language queries such as?I?m looking for a movie called Legally Blonde?and ?Do you have romantic comedies?.
An impor-tant advantage of speech is that it makes it easy tocombine multiple constraints over multiple dimen-sions within a single query (Cohen, 1992).
For ex-ample, queries can indicate co-stars: ?movies star-ring Ginger Rogers and Fred Astaire?, or constraingenre and cast or director at the same time: ?MegRyan Comedies?, ?show drama directed by WoodyAllen?
and ?show comedy movies directed byWoody Allen and starring Mira Sorvino?.Handwriting: Handwritten pen input can also beused to make queries.
When the user?s pen ap-proaches the feedback window, it expands allow-ing for freeform pen input.
In the example in Fig-ure 3, the user requests comedy movies with BruceWillis using unimodal handwritten input.
This is animportant input modality as it is not impacted byambient noise such as crosstalk from other viewersor currently playing content.Figure 3 Handwritten queryNavigation Bar Feedback WindowPointing/GUI:  In addition to the recognition-based modalities, speech and handwriting, the in-terface also supports more traditional graphicaluser interface (GUI) commands.
In the detailspanel, the actors and directors are presented as but-tons.
Pointing at (i.e., clicking on) these buttonsresults in a search for all of the movies with thatparticular actor or director, allowing users toquickly navigate from an actor or director in a spe-cific title to other material they may be interestedin.
The buttons in the results panel can be pointedat (clicked on) in order to view the details in theleft panel for that particular title.Actor/Director Buttons Details ResultsFigure 4 Comparison screenComposite multimodal input: The system alsosupports true composite multimodality when spo-ken or handwritten commands are integrated withpointing gestures made using the pen (in the tabletversion) or by selecting items (in the remote con-trol version).
This allows users to quickly executemore complex commands by combining the easeof reference of pointing with the expressiveness ofspoken constraints.
While by unimodally pointingat an actor button you can search for all of the ac-tor?s movies, by adding speech you can narrow thesearch to, for example, all of their comedies bysaying: ?show comedy movies with THIS actor?.Multimodal commands with multiple pointing ges-tures are also supported, allowing the user to ?glue?together references to multiple actors or directorsin order to constrain the search.
For example, theycan say ?movies with THIS actor and THIS direc-tor?
and point at the ?Alan Rickman?
button andthen the ?John McTiernan?
button in turn (Figure2).
Comparison commands can also be multimo-378dal; for example, if the user says ?compare THISmovie and THIS movie?
and clicks on the two but-tons on the right display for ?Die Hard?
and the?The Fifth Element?
(Figure 2), the resulting dis-play shows the two movies side-by-side in thecomparison screen (Figure 4).3 Underlying multimodal architectureThe system consists of a series of componentswhich communicate through a facilitator compo-nent (Figure 5).
This develops and extends uponthe multimodal architecture underlying theMATCH system (Johnston et al, 2002).Multimodal UI ASRServerASRServerMultimodalNLUMultimodalNLUMovie DB(XML)NLUModelGrammar TemplateASRModelWordsGesturesSpeechClientSpeechClientMeaningGrammarCompilerGrammarCompilerFACILITATORHandwritingHandwritingRecognitionFigure 5 System architectureThe underlying database of movie information isstored in XML format.
When a new database isavailable, a Grammar Compiler component ex-tracts and normalizes the relevant fields from thedatabase.
These are used in conjunction with a pre-defined multimodal grammar template and anyavailable corpus training data to build a multimo-dal understanding model and speech recognitionlanguage model.The user interacts with the multimodal user in-terface client (Multimodal UI), which provides thegraphical display.
When the user presses ?CLICKTO SPEAK?
a message is sent to the Speech Cli-ent, which activates the microphone and ships au-dio to a speech recognition server.
Handwritteninputs are processed by a handwriting recognizerembedded within the multimodal user interfaceclient.
Speech recognition results, pointing ges-tures made on the display, and handwritten inputs,are all passed to a multimodal understanding serverwhich uses finite-state multimodal language proc-essing techniques (Johnston and Bangalore, 2005)to interpret and integrate the speech and gesture.This model combines alignment of multimodalinputs, multimodal integration, and language un-derstanding within a single mechanism.
The result-ing combined meaning representation (representedin XML) is passed back to the multimodal userinterface client, which translates the understandingresults into an XPATH query and runs it againstthe movie database to determine the new series ofresults.
The graphical display is then updated torepresent the latest query.The system first attempts to find an exact matchin the database for all of the search terms in theuser?s query.
If this returns no results, a back offand query relaxation strategy is employed.
First thesystem tries a search for movies that have all of thesearch terms, except stop words, independent ofthe order (an AND query).
If this fails, then itbacks off further to an OR query of the searchterms and uses an edit machine, using Levenshteindistance, to retrieve the most similar item to theone requested by the user.4 EvaluationAfter designing and implementing our initial proto-type system, we conducted an extensive multimo-dal data collection and usability study with the twodifferent interaction scenarios: tablet versus remotecontrol.
Our main goals for the data collection andstatistical analysis were three-fold: collect a largecorpus of natural multimodal dialogue for this me-dia selection task, investigate whether future sys-tems should be paired with a remote control or tab-let-like device, and determine which types ofsearch and input modalities are more or less desir-able.4.1 Experimental set upThe system evaluation took place in a conferenceroom set up to resemble a living room (Figure 6).The system was projected on a large screen acrossthe room from a couch.An adjacent conference room was used for datacollection (Figure 7).
Data was collected in soundfiles, videotapes, and text logs.
Each subject?s spo-ken utterances were recorded by three micro-phones: wireless, array and stand alone.
The wire-less microphone was connected to the systemwhile the array and stand alone microphones were379around 10 feet away.1 Test sessions were recordedwith two video cameras ?
one captured the sys-tem?s screen using a scan converter while the otherrecorded the user and couch area.
Lastly, the user?sinteractions and the state of the system were cap-tured by the system?s logger.
The logger is an addi-tional agent added to the system architecture forthe purposes of the evaluation.
It receives log mes-sages from different system components as interac-tion unfolds and stores them in a detailed XML logfile.
For the specific purposes of this evaluation,each log file contains: general information aboutthe system?s components, a description and time-stamp for each system event and user event, namesand timestamps for the system-recorded soundfiles, and timestamps for the start and end of eachscenario.Figure 6 Data collection environmentForty-four subjects volunteered to participate inthis evaluation.
There were 33 males and 11 fe-males, ranging from 20 to 66 years of age.
Eachuser interacted with both the remote control andtablet variants of the system, completing the sametwo sets of scenarios and then freely interactingwith each system.
For counterbalancing purposes,half of the subjects used the tablet and then the re-mote control and the other half used the remote1 Here we report results for the wireless microphone only.Analysis of the other microphone conditions is ongoing.control and then the tablet.
The scenario set as-signed to each version was also counterbalanced.Figure 7 Data collection roomEach set of scenarios consisted of seven definedtasks, four user-specialized tasks and five open-ended tasks.
Defined tasks were presented in chartform and had an exact answer, such as the movietitle that two specified actors/actresses starred in.For example, users had to find the movie in thedatabase with Matthew Broderick and DenzelWashington.
User-specialized tasks relied on thespecific user?s preferences, such as ?What type ofmovie do you like to watch on a Sunday evening?Find an example from that genre and write downthe title?.
Open-ended tasks prompted users tosearch for any type of information with any inputmodality.
The tasks in the two sets paralleled eachother.
For example, if one set of tasks asked theuser to find the highest ranked comedy movie withReese Witherspoon, the other set of tasks asked theuser to find the highest ranked comedy movie withWill Smith.
Within each task set, the defined tasksappeared first, then the user-specialized tasks andlastly the open-ended tasks.
However, for each par-ticipant, the order of defined tasks was random-ized, as well as the order of user-specialized tasks.At the beginning of the session, users read ashort tutorial about the system?s GUI, the experi-ment, and available input modalities.
Before inter-acting with each version, users were given a man-ual on operating the tablet/remote control.
Tominimize bias, the manuals gave only a generaloverview with few examples and during the ex-periment users were alone in the room.At the end of each session, users completed auser-satisfaction/preference questionnaire and thena qualitative interview.
The questionnaire consisted380of 25 statements about the system in general, thetwo variants of the system, input modality optionsand search options.
For example, statementsranged from ?If I had [the system], I would use thetablet with it?
to ?If my spoken request was mis-understood, I would want to try again with speak-ing?.
Users responded to each statement with a 5-point Likert scale, where 1 = ?I strongly agree?, 2 =?I mostly agree?, 3 = ?I can?t say one way or theother?, 4 = ?I mostly do not agree?
and 5 = ?I do notagree at all?.
The qualitative interview allowed formore open-ended responses, where users coulddiscuss reasons for their preferences and their likesand dislikes regarding the system.4.2 ResultsData was collected from all 44 participants.
Due totechnical problems, five participants?
logs or soundfiles were not recorded in parts of the experiment.All collected data was used for the overall statisticsbut these five participants had to be excluded fromanalyses comparing remote control to tablet.Spoken utterances: After removing emptysound files, the full speech corpus consists of 3280spoken utterances.
Excluding the five participantssubject to technical problems, the total is 3116 ut-terances (1770 with the remote control and 1346with the tablet).The set of 3280 utterances averages 3.09 wordsper utterance.
There was not a significant differ-ence in utterance length between the remote con-trol and tablet conditions.
Users?
averaged 2.97words per utterance with the remote control and3.16 words per utterance with the tablet, paired t(38) = 1.182, p = n.s.
However, users spoke sig-nificantly more often with the remote control.
Onaverage, users spoke 34.51 times with the tabletand 45.38 times with the remote control, paired t(38) = -3.921, p < .01.ASR performance: Over the full corpus of3280 speech inputs, word accuracy was 44% andsentence accuracy 38%.
In the tablet condition,word accuracy averaged 46% and sentence accu-racy 41%.
In the remote control condition, wordaccuracy averaged 41% and sentence accuracy38%.
The difference across conditions was onlysignificant for word accuracy, paired t (38) =2.469, p < .02.
In considering the ASR perform-ance, it is important to note that 55% of the 3280speech inputs were out of grammar, and perhapsmore importantly 34% were out of the functional-ity of the system entirely.
On within functionalityinputs, word accuracy is 62% and sentence accu-racy 57%.
On the in grammar inputs, word accu-racy is 86% and sentence accuracy 83%.
The vo-cabulary size was 3851 for this task.
In the corpus,there are a total of 356 out-of-vocabulary words.Handwriting recognition: Performance was de-termined by manual inspection of screen capturevideo recordings.
2   There were a total of 384handwritten requests with overall 66% sentenceaccuracy and 76% word accuracy.Task completion:  Since participants had to re-cord the task answers on a paper form, task com-pletion was calculated by whether participantswrote down the correct answer.
Overall, users hadlittle difficulty completing the tasks.
On average,participants completed 11.08 out of the 14 definedtasks and 7.37 out of the 8 user-specialized tasks.The number of tasks completed did not differacross system variants.
3  For the seven definedtasks within each condition, users averaged 5.69with the remote control and 5.40 with the tablet,paired t (34) = -1.203, p = n.s.
For the four user-specialized task within each condition, users aver-aged 3.74 on the remote control and 3.54 on thetablet, paired t (34) = -1.268, p = n.s.Input modality preference: During the inter-view, 55% of users reported preferring the pointing(GUI) input modality over speech and multimodalinput.
When asked about handwriting, most userswere hesitant to place it on the list.
They also dis-cussed how speech was extremely important, andgiven a system with a low error speech recognizer,using speech for input probably would be their firstchoice.
In the questionnaire, the majority of users(93%) ?strongly agree?
or ?mostly agree?
with theimportance of making a pointing request.
The im-portance of making a request by speaking had thenext highest average, where 57% ?strongly agree?or ?mostly agree?
with the statement.
The impor-tance of multimodal and handwriting requests hadthe lowest averages, where 39% agreed with theformer and 25% for the latter.
However, in theopen-ended interview, users mentioned handwrit-ing as an important back-up input choice for caseswhen the speech recognizer fails.2 One of the 44 participants videotape did not record and so isnot included in the statistics.3 Four participants did not properly record their task answersand had to be eliminated from the 39 participants being usedin the remote control versus tablet statistics.381Further support for input modality preference wasgathered from the log files, which showed that par-ticipants mostly searched using unimodal speechcommands and GUI buttons.
Out of a total of6082 user inputs to the systems, 48% were unimo-dal speech and 39% were unimodal GUI (pointingand clicking).
Participants requested informationwith composite multimodal commands 7% of thetime and with handwriting 6% of the time.Search preference: Users most strongly agreedwith movie title being the most important way tosearch.
For searching by title, more than half theusers chose ?strongly agree?
and 91% of userschose ?strongly agree?
or ?mostly agree?.
Slightlymore than half chose ?strongly agree?
with search-ing by actor/actress and slightly less than halfchose ?strongly agree?
with the importance ofsearching by genre.
During the open ended inter-view, most users reported title as the most impor-tant means for searching.Variant preference:  Results from the qualita-tive interview indicate that 67% of users preferredthe remote control over the tablet variant of thesystem.
The most common reported reasons werefamiliarity, physical comfort and ease of use.
Re-mote control preference is further supported fromthe user-preference questionnaire, where 68% ofparticipants ?mostly agree?
or ?strongly agree?
withwanting to use the remote control variant of thesystem, compared to 30% of participants choosing?mostly agree?
or ?strongly agree?
with wanting touse the tablet version of the system.5 ConclusionWith the range of entertainment content availableto consumers in their homes rapidly expanding, thecurrent access paradigm of direct manipulation ofcomplex graphical menus and onscreen keyboards,and remote controls with way too many buttons isincreasingly ineffective and cumbersome.
In orderto address this problem, we have developed ahighly flexible multimodal interface that allowsusers to search for content using speech, handwrit-ing, pointing (using pen or remote control), anddynamic multimodal combinations of input modes.Results are presented in a straightforward graphicalinterface similar to those found in current systemsbut with the addition of icons for actors and direc-tors that can be used both for unimodal GUI andmultimodal commands.
The system allows users tosearch for movies over multiple different dimen-sions of classification (title, genre, cast, director,year) using the mode or modes of their choice.
Wehave presented the initial results of an extensivemultimodal data collection and usability study withthe system.Users in the study were able to successfully usespeech in order to conduct searches.
Almost half oftheir inputs were unimodal speech (48%) and themajority of users strongly agreed with the impor-tance of using speech as an input modality for thistask.
However, as also reported in previous work(Wittenburg et al2006), recognition accuracy re-mains a serious problem.
To understand the per-formance of speech recognition here, detailed erroranalysis is important.
The overall word accuracywas 44% but the majority of errors resulted fromrequests from users that lay outside the functional-ity of the underlying system, involving capabilitiesthe system did not have or titles/cast absent fromthe database (34% of the 3280 spoken and multi-modal inputs).
No amount of speech and languageprocessing can resolve these problems.
This high-lights the importance of providing more detailedhelp and tutorial mechanisms in order to appropri-ately ground users?
understanding of system capa-bilities.
Of the remaining 66% of inputs (2166)which were within the functionality of the system,68% were in grammar.
On the within functionalityportion of the data, the word accuracy was 62%,and on in grammar inputs it is 86%.
Since this wasour initial data collection, an un-weighted finite-state recognition model was used.
The perform-ance will be improved by training stochastic lan-guage models as data become available and em-ploying robust understanding techniques.
One in-teresting issue in this domain concerns recognitionof items that lie outside of the current database.Ideally the system would have a far larger vocabu-lary than the current database so that it would beable to recognize items that are outside the data-base.
This would allow feedback to the user to dif-ferentiate between lack of results due to recogni-tion or understanding problems versus lack ofitems in the database.
This has to be balancedagainst degradation in accuracy resulting from in-creasing the vocabulary.In practice we found that users, while acknowl-edging the value of handwriting as a back-upmode, generally preferred the more relaxed andfamiliar style of interaction with the remote con-trol.
However, several factors may be at play here.382The tablet used in the study was the size of a smalllaptop and because of cabling had a fixed locationon one end of the couch.
In future, we would liketo explore the use of a smaller, more mobile, tabletthat would be less obtrusive and more conducive toleaning back on the couch.
Another factor is thatthe in-lab data collection environment is somewhatunrealistic since it lacks the noise and disruptionsof many living rooms.
It remains to be seenwhether in a more realistic environment we mightsee more use of handwritten input.
Another factorhere is familiarity.
It may be that users have morefamiliarity with the concept of speech input thanhandwriting.
Familiarity also appears to play a rolein user preferences for remote control versus tablet.While the tablet has additional capabilities suchhandwriting and easier use of multimodal com-mands, the remote control is more familiar to usersand allows for a more relaxed interaction sincethey can lean back on the couch.
Also many usersare concerned about the quality of their handwrit-ing and may avoid this input mode for that reason.Another finding is that it is important not to un-derestimate the importance of GUI input.
39% ofuser commands were unimodal GUI (pointing)commands and 55% of users reported a preferencefor GUI over speech and handwriting for input.Clearly, the way forward for work in this area is todetermine the optimal way to combine more tradi-tional graphical interaction techniques with themore conversational style of spoken interaction.Most users employed the composite multimodalcommands, but they make up a relatively smallproportion of the overall number of user inputs inthe study data (7%).
Several users commented thatthey did not know enough about the multimodalcommands and that they might have made moreuse of them if they had understood them better.This, along with the large number of inputs thatwere out of functionality, emphasizes the need formore detailed tutorial and online help facilities.The fact that all users were novices with the sys-tem may also be a factor.
In future, we hope toconduct a longer term study with repeat users tosee how previous experience influences use ofnewer kinds of inputs such as multimodal andhandwriting.Acknowledgements Thanks to Keith Bauer, Simon Byers,Harry Chang, Rich Cox, David Gibbon, Mazin Gilbert,Stephan Kanthak, Zhu Liu, Antonio Moreno, and BehzadShahraray for their help and support.
Thanks also to the Di-recci?n General de Universidades e Investigaci?n - Consejer?ade Educaci?n - Comunidad de Madrid, Espa?a for sponsoringD?Haro?s visit to AT&T.ReferencesElisabeth Andr?.
2002.
Natural Language in Multimodaland Multimedia systems.
In Ruslan Mitkov (ed.)
Ox-ford Handbook of Computational Linguistics.
OxfordUniversity Press.Aseel Berglund.
2004.
Augmenting the Remote Control:Studies in Complex Information Navigation for Digi-tal TV.
Link?ping Studies in Science and Technol-ogy, Dissertation no.
872.
Link?ping University.Philip R. Cohen.
1992.
The Role of Natural Language ina Multimodal Interface.
In Proceedings of ACMUIST Symposium on User Interface Software andTechnology.
pp.
143-149.Jun Goto, Kazuteru Komine, Yuen-Bae Kim and Nori-yoshi Uratan.
2003.
A Television Control Systembased on Spoken Natural Language Dialogue.
InProceedings of 9th International Conference on Hu-man-Computer Interaction.
pp.
765-768.Aseel Ibrahim and Pontus Johansson.
2002.
MultimodalDialogue Systems for Interactive TV Applications.
InProceedings of 4th IEEE International Conferenceon Multimodal Interfaces.
pp.
117-222.Pontus Johansson.
2003.
MadFilm - a Multimodal Ap-proach to Handle Search and Organization in aMovie Recommendation System.
In Proceedings ofthe 1st Nordic Symposium on Multimodal Communi-cation.
Helsing?r, Denmark.
pp.
53-65.Michael Johnston, Srinivas Bangalore, Guna Vasireddy,Amanda Stent, Patrick Ehlen, Marilyn Walker, SteveWhittaker, Preetam Maloor.
2002.
MATCH: An Ar-chitecture for Multimodal Dialogue Systems.
In Pro-ceedings of the 40th ACL.
pp.
376-383.Michael Johnston and Srinivas Bangalore.
2005.
Finite-state Multimodal Integration and Understanding.Journal of Natural Language Engineering 11.2.Cambridge University Press.
pp.
159-187.Russ Mitchell.
1999.
TV?s Next Episode.
U.S. Newsand World Report.
5/10/99.Thomas Portele, Silke Goronzy, Martin Emele, AndreasKellner, Sunna Torge, and J?ergen te Vrugt.
2006.SmartKom?Home: The  Interface to Home Enter-tainment.
In Wolfgang Wahlster (ed.)
SmartKom:Foundations of Multimodal Dialogue Systems.Springer.
pp.
493-503.Kent Wittenburg, Tom Lanning, Derek Schwenke, HalShubin and Anthony Vetro.
2006.
The Prospects forUnrestricted Speech Input for TV Content Search.
InProceedings of  AVI?06.
pp.
352-359.383
