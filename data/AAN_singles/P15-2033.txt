Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 199?204,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsDistributional Neural Networks forAutomatic Resolution of Crossword PuzzlesAliaksei Severyn?, Massimo Nicosia, Gianni Barlacchi, Alessandro Moschitti?DISI - University of Trento, Italy?Qatar Computing Research Institute, Hamad Bin Khalifa University, Qatar?Google Inc.{aseveryn,gianni.barlacchi,m.nicosia,amoschitti}@gmail.comAbstractAutomatic resolution of Crossword Puz-zles (CPs) heavily depends on the qual-ity of the answer candidate lists producedby a retrieval system for each clue of thepuzzle grid.
Previous work has shownthat such lists can be generated using In-formation Retrieval (IR) search algorithmsapplied to the databases containing previ-ously solved CPs and reranked with treekernels (TKs) applied to a syntactic treerepresentation of the clues.
In this pa-per, we create a labelled dataset of 2 mil-lion clues on which we apply an innovativeDistributional Neural Network (DNN) forreranking clue pairs.
Our DNN is com-putationally efficient and can thus take ad-vantage of such large datasets showing alarge improvement over the TK approach,when the latter uses small training data.
Incontrast, when data is scarce, TKs outper-form DNNs.1 IntroductionAutomatic solvers of CPs require accurate list ofanswer candidates to find good solutions in littletime.
Candidates can be retrieved from the DBsof previously solved CPs (CPDBs) since clues areoften reused, and thus querying CPDBs with thetarget clue allows us to recuperate the same (orsimilar) clues.In this paper, we propose for the first time theuse of Distributional Neural Networks to improvethe ranking of answer candidate lists.
Most im-portantly, we build a very large dataset for clueretrieval, composed of 2,000,493 clues with theirassociated answers, i.e., this is a supervised cor-pus where large scale learning models can be de-veloped and tested.
This dataset is an interesting?Work done when student at University of Trentoresource that we make available to the researchcommunity1.
To assess the effectiveness of ourDNN model, we compare it with the current stateof the art model (Nicosia et al, 2015) in rerank-ing CP clues, where tree kernels (Moschitti, 2006)are used to rerank clues according to their syntac-tic/semantic similarity with the query clue.The experimental results on our dataset demon-strate that:(i) DNNs are efficient and can greatly benefitfrom large amounts of data;(ii) when DNNs are applied to large-scale data,they largely outperform traditional feature-based rerankers as well as kernel-based mod-els; and(iii) if limited training data is available for train-ing, tree kernel-based models are more accu-rate than DNNs2 Clue Reranking Models for CPsIn this section, we briefly introduce the generalidea of CP resolution systems and the state-of-the-art models for reranking answer candidates.2.1 CP resolution systemsThe main task of a CP resolution system is thegeneration of candidate answer lists for each clueof the target puzzle (Littman et al, 2002).
Thena solver for Probabilistic-Constraint SatisfactionProblems, e.g., (Pohl, 1970), tries combinationsof letters that satisfy the crossword constraints.The combinations are derived from words foundin dictionaries or in the lists of answer candidates.The latter can be generated using large crossworddatabases as well as several expert modules ac-cessing domain-specific databases (e.g., movies,writers and geography).
WebCrow, one of the1http://ikernels-portal.disi.unitn.it/projects/webcrow/199Rank Clue Answer1 Actress Pflug who played Lt.
Dish in ?MASH?
Jo Ann2 Actress Pflug who played in ?MASH?
(1970) Jo Ann3 Actress Jo Ann Pflug4 MASH Actress Jo Ann Pflug5 MASH CrushTable 1: Candidate list for the query clue: Jo Annwho played Lt. ?Dish?
in 1970?s ?MASH?
(an-swer: Pflug)best systems (Ernandes et al, 2005), incorporatesknowledge sources and an effective clue retrievalmodel from DB.
It carries out basic linguistic anal-ysis such as part-of-speech tagging and lemmati-zation and takes advantage of semantic relationscontained in WordNet, dictionaries and gazetteers.It also uses a Web module constituted by a searchengine (SE), which can retrieve text snippets re-lated to the clue.Clearly, lists of better quality, i.e., many correctcandidates in top positions, result in higher accu-racy and speed of the solver.
Thus the design ofeffective answer rankers is extremely important.2.2 Clue retrieval and rerankingOne important source of candidate answers is theDB of previously solved clues.
In (Barlacchi etal., 2014a), we proposed the BM25 retrieval modelto generate clue lists, which were further refinedby applying our reranking models.
The latter pro-mote the most similar, which are probably asso-ciated with the same answer of the query clue, tothe top.
The reranking step is important becauseSEs often fail to retrieve the correct clues in thefirst position.
For example, Table 1 shows the firstfive clues retrieved for the query clue: Jo Ann whoplayed Lt. ?Dish?
in 1970?s ?MASH?.
BM25 re-trieved the wrong clue, Actress Pflug who playedLt.
Dish in ?MASH?, at the top since it has a largerbag-of-words overlap with the query clue.2.3 Reranking with KernelsWe applied our reranking framework for questionanswering systems (Moschitti, 2008; Severyn andMoschitti, 2012; Severyn et al, 2013a; Severynet al, 2013b; Severyn and Moschitti, 2013).
Thisretrieves a list of related clues by using the tar-get clue as a query in an SE (applied to the Webor to a DB).
Then, both query and candidates arerepresented by shallow syntactic structures (gen-erated by running a set of NLP parsers) and tradi-tional similarity features which are fed to a kernel-based reranker.
Hereafter, we give a brief descrip-tion of our models for clue reranking whereas thereader can refer to our previous work (Barlacchiet al, 2014a; Nicosia et al, 2015; Barlacchi et al,2014b) for more specific details.Given a query clue qcand two retrieved cluesc1, c2, we can rank them by using a classifi-cation approach: the two clues c1and c2arereranked by comparing their classification scores:SVM(?q, c1?)
and SVM(?q, c2?).
The SVM classi-fier uses the following kernel applied to two pairsof query/clues, p = ?q, ci?
and p?= ?q?, c?j?
:K(p, p?)
= TK(q, q?)
+ TK(ci, c?j)+FV (q, ci) ?
FV (q?, c?j),where TK can be any tree kernel, e.g., the syntac-tic tree kernel (STK) also called SST by Moschitti(2006), and FV is the feature vector representationof the input pair, e.g., ?q, ci?
or ?q?, c?j?.
STK mapstrees into the space of all possible tree fragmentsconstrained by the rule that the sibling nodes fromtheir parents cannot be separated.
It enables theexploitation of structural features, which can beeffectively combined with more traditional fea-tures (described hereafter).Feature Vectors (FV).
We compute the followingsimilarity features between clues: (i) tree kernelsimilarity applied to intra-pairs, i.e., between thequery and the retrieved clues; (ii) DKPro Simi-larity, which defines features used in the contextof the Semantic Textual Similarity (STS) chal-lenge (B?ar et al, 2013); and (iii) WebCrow fea-tures (WC), which are the similarity measurescomputed on the clue pairs by WebCrow (usingthe Levenshtein distance) and the SE score.3 Distributional models for cluererankingThe architecture of our distributional matchingmodel for measuring similarity between clues ispresented in Fig.
1.
Its main components are:(i) sentence matrices sci?
Rd?|ci|obtained bythe concatenation of the word vectors wj?Rd(with d being the size of the embeddings)of the corresponding words wjfrom the inputclues ci;(ii) a distributional sentence modelf : Rd?|ci|?
Rmthat maps the sentence200Figure 1: Distributional sentence matching model for computing similarity between clues.matrix of an input clue cito a fixed-sizevector representations xciof size m;(iii) a layer for computing the similarity betweenthe obtained intermediate vector representa-tions of the input clues, using a similarity ma-trix M ?
Rm?m?
an intermediate vectorrepresentation xc1of a clue c1is projected toa x?c1= xc1M, which is then matched withxc2(Bordes et al, 2014), i.e., by computing adot-product x?c1xc2, thus resulting in a singlesimilarity score xsim;(vi) a set of fully-connected hidden layers thatmodels the similarity between clues usingtheir vector representations produced by thesentence model (also integrating the singlesimilarity score from the previous layer); and(v) a softmax layer that outputs probabilityscores reflecting how well the clues matchwith each other.The choice of the sentence model plays a cru-cial role as the resulting intermediate representa-tions of the input clues will affect the successivestep of computing their similarity.
Recently, dis-tributional sentence models, where f(s) is rep-resented by a sequence of convolutional-poolingfeature maps, have shown state-of-the-art resultson many NLP tasks, e.g., (Kalchbrenner et al,2014; Kim, 2014).
In this paper, we opt for a sim-ple solution where f(sci) =?iwi/|ci|, i.e., theword vectors, are averaged to a single fixed-sizedvector x ?
Rd.
Our preliminary experiments re-vealed that this simpler model works just as wellas more complicated single or multi-layer convo-lutional architectures.
We conjecture that this islargely due to the nature of the language used inclues, which is very dense and where the syntacticinformation plays a minor role.Considering recent deep learning models formatching sentences, our network is most similarto the models in Hu et al (2014) applied for com-puting sentence similarity and in Yu et al(2014)(answer sentence selection in Question Answer-ing) with the following differences:(i) In contrast to more complex convolutionalsentence models explored in (Hu et al, 2014)and in (Yu et al, 2014), our sentence modelis composed of a single averaging operation.
(ii) To compute the similarity between the vec-tor representation of the input sentences, ournetwork uses two methods: (i) computing thesimilarity score obtained by transforming oneclue into another using a similarity matrix M(explored in (Yu et al, 2014)), and (ii) di-rectly modelling interactions between inter-mediate vector representations of the input201clues via fully-connected hidden layers (usedby (Hu et al, 2014)).4 ExperimentsOur experiments compare different ranking mod-els, i.e., BM25 as the IR baseline, and severalrerankers, and our distributional neural network(DNN) for the task of clue reranking.4.1 Experimental setupData.
We compiled our crossword corpus combin-ing (i) CPs downloaded from the Web2and (ii) theclue database provided by Otsys3.
We removedduplicates, fill-in-the-blank clues (which are bettersolved by using other strategies) and clues repre-senting anagrams or linguistic games.We collected over 6.3M pairs of clue/answerand after removal of duplicates, we obtained acompressed dataset containing 2M unique andstandard clues, with associated answers, which wecalled CPDB.
We used these clues to build a SmallDataset (SD) and a Large Dataset (LD) for rerank-ing.
The two datasets are based on pairs of clues:query and retrieved clues.
Such clues are retrievedusing a BM25 model on CPDB.For creating SD, we used 8k clues that (i) wererandomly extracting from CPDB and (ii) satisfy-ing the property that at least one correct clue (i.e.,having the same answer of the query clue) is inthe first retrieved 10 clues (of course the queryclue is eliminated from the ranked list providedby BM25).
In total we got about 120K examples,84,040 negative and 35,960 positive clue4.For building LD, we collected 200k clues withthe same property above.
More precisely weobtained 1,999,756 pairs (10?200k minus fewproblematic examples) with 599,025 positive and140,0731 negative pairs of queries with their re-trieved clues.
Given the large number of examples,we only used such dataset in classification modal-ity, i.e., we did not form reranking examples (pairsof pairs).2http://www.crosswordgiant.com3http://www.otsys.com/clue4A true reranker should be built using pairs of clue pairs,where the positive pairs are those having the correct pair asthe first member.
This led to form 127,109 reranking exam-ples, with 66,011 positive and 61,098 negative pairs.
How-ever, in some experiments, which we do not report in thepaper, we observed that the performance both of the simpleclassifier as well as the true reranker were similar, thus wedecided to use the simpler classifier.Structural model.
We use SVM-light-TK5,which enables the use of structural kernels (Mos-chitti, 2006).
We applied structural kernels to shal-low tree representations and a polynomial kernelof degree 3 to feature vectors (FV).Distributional neural network model.
We pre-initialize the word embeddings by running theword2vec tool (Mikolov et al, 2013) on the En-glish Wikipedia dump.
We opt for a skipgrammodel with window size 5 and filtering words withfrequency less than 5.
The dimensionality of theembeddings is set to 50.
The input sentences aremapped to fixed-sized vectors by computing theaverage of their word embeddings.
We use a sin-gle non-linear hidden layer (with rectified linear(ReLU) activation function) whose size is equal tothe size of the previous layer.The network is trained using SGD with shuf-fled mini-batches using the Adagrad updaterule (Duchi et al, 2011).
The batch size is set to100 examples.
We used 25 epochs with early stop-ping, i.e., we stop the training if no update to thebest accuracy on the dev set (we create the devset by allocating 10% of the training set) is madefor the last 5 epochs.
The accuracy computed onthe dev set is the Mean Average Precision (MAP)score.
To extract the DNN features we simply takethe output of the hidden layer just before the soft-max.Evaluation.
We used standard metrics widelyused in QA: the Mean Reciprocal Rank (MRR)and Mean Average Precision (MAP).4.2 ResultsTable 2 summarizes the results of our differentreranking models trained on a small dataset (SD)of 120k examples and a large dataset (LD) with2M examples.The first column reports the BM25 result; thesecond column shows the performance of SVMperf (SVMp), which is a very fast variant of SVM,using FV; the third column reports the state-of-the-art model for crossword clue reranking (Nicosia etal., 2015), which uses FV vector and tree kernels,i.e., SVM(TK).Regarding the other systems: DNNMSDis theDNN model trained on the small data (SD) of120k training pairs; SVMp(DNNFLD) is SVMperf trained with (i) the features derived from5http://disi.unitn.it/moschitti/Tree-Kernel.htm202Training classifiers with the Small Dataset (SD) (120K instances)BM25 SVMp SVM(TK) DNNMSDSVMp(DNNFLD) SVM(DNNFLD,TK)MRR 37.57 41.95 43.59 40.08 46.12 45.50MAP 27.76 30.06 31.79 28.25 33.75 33.71Training classifiers with the Large Dataset (LD) (2 million instances)BM25 SVMp SVM(TK) DNNMLDSVMp(DNNFLD,?FV) SVMp(DNNFLD)MRR 37.57 41.47 ?
46.10 46.36 46.27MAP 27.76 29.95 ?
33.81 34.07 33.86Table 2: SVM models and DNN trained on 120k (small dataset) and 2 millions (large dataset) examples.Feature vectors are used with all models except when indicated by ?FVDNN trained on a large clue dataset LD and (ii)the FV; and finally, SVM(DNNFLD,TK) is SVMusing DNN features (generated from LD), FV andTK.
It should be noted that:(i) SVMpis largely improved by TK;(ii) DNNMSDon relatively small data deliversan accuracy lower than FV;(iii) if SVMpis trained with DNNMLD, i.e., fea-tures derived from the dataset of 2M clues,the accuracy greatly increases; and(iv) finally, the combination with TK, i.e.,SVM(DNNFLD,TK), does not significantlyimprove the previous results.In summary, when a dataset is relatively smallDNNM fails to deliver any noticeable improve-ment over the SE baseline even when combinedwith additional similarity features.
SVM andTK models generalize much better on the smallerdataset.Additionally, it is interesting to see that trainingan SVM on a small number of examples enrichedwith the features produced by a DNN trained onlarge data gives us the same results of DNN trainedon the large dataset.
Hence, it is desired to uselarger training collections to build an accuratedistributional similarity matching model that canbe then effectively combined with other feature-based or tree kernel models, although at the mo-ment the combination does not significantly im-prove TK models.Regarding the LD training setting it can be ob-served that:(i) the second column shows that adding moretraining examples to SVMpdoes not increaseaccuracy (compared with SD result);(ii) DNNMLDdelivers high accuracy suggestingthat a large dataset is essential to its training;and(iii) again SVMpusing DNN features deliverstate-of-the-art accuracy independently of us-ing or not additional features (i.e., see ?FV,which excludes the latter).5 ConclusionsIn this paper, we have explored various rerankermodels to improve automatic CP resolution.
Themost important finding is that our distributionalneural network model is very effective in estab-lishing similarity matching between clues.
Wecombine the features produced by our DNN modelwith other rerankers to greatly improve over theprevious state-of-the-art results.
Finally, we col-lected a very large dataset composed of 2 millionsclue/answer pairs that can be useful to the NLPcommunity for developing semantic textual simi-larity models.Future research will be devoted to find modelsto effectively combine TKs and DNN.
In partic-ular, our previous model exploiting Linked OpenData in QA (Tymoshenko et al, 2014) seems verypromising to find correct answer to clues.
This aswell as further research will be integrated in ourCP system described in (Barlacchi et al, 2015).AcknowledgmentsThis work was supported by the EC projectCogNet, 671625 (H2020-ICT-2014-2).
The firstauthor was supported by the Google Europe Doc-toral Fellowship Award 2013.203ReferencesDaniel B?ar, Torsten Zesch, and Iryna Gurevych.
2013.DKPro similarity: An open source framework fortext similarity.
In Proceedings of ACL (SystemDemonstrations).Gianni Barlacchi, Massimo Nicosia, and AlessandroMoschitti.
2014a.
Learning to rank answer can-didates for automatic resolution of crossword puz-zles.
In Proceedings of the Eighteenth Conferenceon Computational Natural Language Learning.
As-sociation for Computational Linguistics.Gianni Barlacchi, Massimo Nicosia, and AlessandroMoschitti.
2014b.
A retrieval model for automaticresolution of crossword puzzles in italian language.In The First Italian Conference on ComputationalLinguistics CLiC-it 2014.Gianni Barlacchi, Massimo Nicosia, and AlessandroMoschitti.
2015.
SACRY: Syntax-based automaticcrossword puzzle resolution system.
In Proceed-ings of 53nd Annual Meeting of the Association forComputational Linguistics: System Demonstrations,Beijing, China, July.
Association for ComputationalLinguistics.Antoine Bordes, Sumit Chopra, and Jason Weston.2014.
Question answering with subgraph embed-dings.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Process-ing (EMNLP), pages 615?620, Doha, Qatar, Octo-ber.
Association for Computational Linguistics.John Duchi, Elad Hazan, and Yoram Singer.
2011.Adaptive subgradient methods for online learningand stochastic optimization.
J. Mach.
Learn.
Res.,12:2121?2159.Marco Ernandes, Giovanni Angelini, and Marco Gori.2005.
Webcrow: A web-based system for crosswordsolving.
In In Proc.
of AAAI 05, pages 1412?1417.Menlo Park, Calif., AAAI Press.Baotian Hu, Zhengdong Lu, Hang Li, and QingcaiChen.
2014.
Convolutional neural network archi-tectures for matching natural language sentences.
InNIPS.Nal Kalchbrenner, Edward Grefenstette, and Phil Blun-som.
2014.
A convolutional neural network formodelling sentences.
Proceedings of the 52nd An-nual Meeting of the Association for ComputationalLinguistics, June.Yoon Kim.
2014.
Convolutional neural networks forsentence classification.
Doha, Qatar.Michael L. Littman, Greg A. Keim, and Noam Shazeer.2002.
A probabilistic approach to solving crosswordpuzzles.
Artificial Intelligence, 134(12):23 ?
55.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositional-ity.
In Advances in Neural Information ProcessingSystems 26, pages 3111?3119.Alessandro Moschitti.
2006.
Efficient convolution ker-nels for dependency and constituent syntactic trees.In ECML, pages 318?329.Alessandro Moschitti.
2008.
Kernel methods, syn-tax and semantics for relational text categorization.In Proceedings of the 17th ACM Conference on In-formation and Knowledge Management, CIKM ?08,pages 253?262, Napa Valley, California, USA.Massimo Nicosia, Gianni Barlacchi, and AlessandroMoschitti.
2015.
Learning to rank aggregated an-swers for crossword puzzles.
In Allan Hanbury,Gabriella Kazai, Andreas Rauber, and Norbert Fuhr,editors, Advances in Information Retrieval - 37thEuropean Conference on IR Research, ECIR, Vi-enna, Austria.
Proceedings, volume 9022 of LectureNotes in Computer Science, pages 556?561.Ira Pohl.
1970.
Heuristic search viewed as path findingin a graph.
Artificial Intelligence, 1(34):193 ?
204.Aliaksei Severyn and Alessandro Moschitti.
2012.Structural relationships for large-scale learning ofanswer re-ranking.
In Proceedings of ACM SIGIR,New York, NY, USA.Aliaksei Severyn and Alessandro Moschitti.
2013.
Au-tomatic feature engineering for answer selection andextraction.
In Proceedings of the 2013 Conferenceon Empirical Methods in Natural Language Pro-cessing, pages 458?467, Seattle, Washington, USA,October.
Association for Computational Linguistics.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013a.
Building structures from clas-sifiers for passage reranking.
In CIKM.Aliaksei Severyn, Massimo Nicosia, and AlessandroMoschitti.
2013b.
Learning adaptable patterns forpassage reranking.
In Proceedings of the Seven-teenth Conference on Computational Natural Lan-guage Learning, pages 75?83, Sofia, Bulgaria, Au-gust.
Association for Computational Linguistics.Kateryna Tymoshenko, Alessandro Moschitti, and Ali-aksei Severyn.
2014.
Encoding semantic resourcesin syntactic structures for passage reranking.
InProceedings of the 14th Conference of the Euro-pean Chapter of the Association for ComputationalLinguistics, pages 664?672, Gothenburg, Sweden,April.
Association for Computational Linguistics.Lei Yu, Karl Moritz Hermann, Phil Blunsom, andStephen Pulman.
2014.
Deep learning for answersentence selection.
CoRR.204
