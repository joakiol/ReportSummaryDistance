Automatic Classification of English Verbs Using Rich Syntactic FeaturesLin Sun and Anna KorhonenComputer LaboratoryUniversity of Cambridge15 JJ Thomson AvenueCambridge CB3 0FD, UKls418,alk23@cl.cam.ac.ukYuval KrymolowskiDepartment of Computer ScienceUniversity of Haifa31905, HaifaIsraelyuvalkry@gmail.comAbstractPrevious research has shown that syntacticfeatures are the most informative featuresin automatic verb classification.
We exper-iment with a new, rich feature set, extractedfrom a large automatically acquired subcate-gorisation lexicon for English, which incor-porates information about arguments as wellas adjuncts.
We evaluate this feature set us-ing a set of supervised classifiers, most ofwhich are new to the task.
The best classi-fier (based on Maximum Entropy) yields thepromising accuracy of 60.1% in classifying204 verbs to 17 Levin (1993) classes.
Wediscuss the impact of this result on the state-of-art, and propose avenues for future work.1 IntroductionRecent research shows that it is possible, using cur-rent natural language processing (NLP) and machinelearning technology, to automatically induce lex-ical classes from corpus data with promising ac-curacy (Merlo and Stevenson, 2001; Korhonen etal., 2003; Schulte im Walde, 2006; Joanis et al,2007).
This research is interesting, since lexi-cal classifications, when tailored to the applicationand domain in question, can provide an effectivemeans to deal with a number of important NLPtasks (e.g.
parsing, word sense disambiguation, se-mantic role labeling), as well as enhance perfor-mance in many applications (e.g.
information ex-traction, question-answering, machine translation)(Dorr, 1997; Prescher et al, 2000; Swier and Steven-son, 2004; Dang, 2004; Shi and Mihalcea, 2005).Lexical classes are useful because they capturegeneralizations over a range of (cross-)linguisticproperties.
Being defined in terms of similar mean-ing components and (morpho-)syntactic behaviourof words (Jackendoff, 1990; Levin, 1993) theygenerally incorporate a wider range of propertiesthan e.g.
classes defined solely on semantic grounds(Miller, 1990).
They can be used to build a lexicalorganization which effectively captures generaliza-tions and predicts much of the syntax and semanticsof a new word by associating it with an appropriateclass.
This can help compensate for lack of data forindividual words in NLP.Large-scale exploitation of lexical classes in real-world or domain-sensitive tasks has not been pos-sible because existing manually built classificationsare incomprehensive.
They are expensive to extendand do not incorporate important statistical infor-mation about the likelihood of different classes forwords.
Automatic classification is a better alterna-tive.
It is cost-effective and gathers statistical infor-mation as a side-effect of the acquisition process.Most work on automatic classification has fo-cussed on verbs which are typically the main pred-icates in sentences.
Syntactic features have provedthe most informative in verb classification.
Exper-iments have been reported using both (i) deep syn-tactic features (e.g.
subcategorization frames (SCFs))extracted using parsers and subcategorisation acqui-sition systems (Schulte im Walde, 2000; Korhonenet al, 2003; Schulte im Walde, 2006) and (ii) shal-low ones (e.g.
NPs/PPs preceding/following verbs)extracted using taggers and chunkers (Merlo andStevenson, 2001; Joanis et al, 2007).769(i) correspond closely with features used formanual classification (Levin, 1993).
They haveproved successful in the classification of German(Schulte im Walde, 2006) and English verbs (Ko-rhonen et al, 2003).
Yet promising results have alsobeen reported when using (ii) for English verb clas-sification (Merlo and Stevenson, 2001; Joanis et al,2007).
This may indicate that (i) are optimal for thetask when combined with additional syntactic infor-mation from (ii).We investigate this matter by experimenting witha new, rich feature set which incorporates informa-tion about SCFs (arguments) as well as adjuncts.
Itwas extracted from VALEX, a large automaticallyacquired SCF lexicon for English (Korhonen et al,2006).
We evaluate the feature set thoroughly us-ing set of supervised classifiers, most of which arenew in verb classification.
The best performing clas-sifier (Maximum Entropy) yields the accuracy of60.1% on classifying 204 verbs into 17 Levin (1993)classes.
This result is good, considering that we per-formed no sophisticated feature engineering or se-lection based on the properties of the target classi-fication (Joanis et al, 2007).
We propose variousavenues for future work.We introduce our target classification in section 2and syntactic features in section 3.
The classifica-tion techniques are presented in section 4.
Detailsof the experimental evaluation are supplied in sec-tion 5.
Section 6 provides discussion and concludeswith directions for future work.2 Test Verbs and ClassesWe adopt as a target classification Levin?s (1993)well-known taxonomy where verbs taking similardiathesis alternations are assumed to share meaningcomponents and are organized into a semanticallycoherent class.
For instance, the class of ?BreakVerbs?
(class 45.1) is partially characterized by itsparticipation in the following alternations:1.
Causative/inchoative alternation:Tony broke the window ?
The window broke2.
Middle alternation:Tony broke the window ?
The window broke easily3.
Instrument subject alternation:Tony broke the window with the hammer ?
The hammerbroke the windowLEVIN CLASS EXAMPLE VERBS9.1 PUT bury, place, install, mount, put10.1 REMOVE remove, abolish, eject, extract, deduct11.1 SEND ship, post, send, mail, transmit13.5.1 GET win, gain, earn, buy, get18.1 HIT beat, slap, bang, knock, pound22.2 AMALGAMATE contrast, match, overlap, unite, unify29.2 CHARACTERIZE envisage, portray, regard, treat, enlist30.3 PEER listen, stare, look, glance, gaze31.1 AMUSE delight, scare, shock, confuse, upset36.1 CORRESPOND cooperate, collide, concur, mate, flirt37.3 MANNER OF shout, yell, moan, mutter, murmurSPEAKING37.7 SAY say, reply, mention, state, report40.2 NONVERBAL smile, laugh, grin, sigh, gasEXPRESSION43.1 LIGHT EMISSION shine, flash, flare, glow, blaze45.4 CHANGE OF STATE soften, weaken, melt, narrow, deepen47.3 MODES OF BEING quake, falter, sway, swirl, teeterWITH MOTION51.3.2 RUN swim, fly, walk, slide, runTable 1: Test classes and example verbsAlternations are expressed as pairs of SCFs.
Addi-tional properties related to syntax, morphology andextended meanings of member verbs are specifiedwith some classes.
The taxonomy provides a classi-fication of 4,186 verb senses into 48 broad and 192fine-grained classes according to their participationin 79 alternations involving NP and PP complements.We selected 17 fine-grained classes and 12 mem-ber verbs per class (table 2) for experimentation.The small test set enabled us to evaluate our resultsthoroughly.
The classes were selected to (i) includeboth syntactically and semantically similar and dif-ferent classes (to vary the difficulty of the classifi-cation task), and to (ii) have enough member verbswhose predominant sense belongs to the class inquestion (we verified this according to the methoddescribed in (Korhonen et al, 2006)).
As VALEXwas designed to maximise coverage most test verbshad 1000-9000 occurrences in the lexicon.3 Syntactic FeaturesWe employed as features distributions of SCFs spe-cific to given verbs.
We extracted them from the re-cent VALEX (Korhonen et al, 2006) lexicon whichprovides SCF frequency information for 6,397 En-glish verbs.
VALEX was acquired automaticallyfrom five large corpora and the Web (using up to10,000 occurrences per verb) using the subcatego-rization acquisition system of Briscoe and Carroll(1997).
The system incorporates RASP, a domain-independent robust statistical parser (Briscoe and770Carroll, 2002), and a SCF classifier which iden-tifies 163 verbal SCFs.
The basic SCFs abstractover lexically-governed particles and prepositionsand predicate selectional preferences.We used the noisy unfiltered version of VALEXwhich includes 33 SCFs per verb on average1.
Someare genuine SCFs but some express adjuncts (e.g.I sang in the party could be SCF PP).
A lexicalentry for each verb and SCF combination providese.g.
the frequency of the entry (in active and passive)in corpora, the POS tags of verb tokens, the argumentheads in argument positions, and the prepositions inPP slots.
We experimented with three feature sets:1.
Feature set 1: SCFs and their frequencies2.
Feature set 2: Feature set 1 with two high frequencyPP frames parameterized for prepositions: the simple PP(e.g.
they apologized to him) and NP-PP (e.g.
he removedthe shoes from the bag) frames.3.
Feature set 3: Feature set 2 with three additional highfrequency PP frames parameterized for prepositions: theNP-FOR-NP (e.g.
he bought a book for him), NP-TO-NP(e.g.
he gave a kiss to her), and OC-AP, EQUI, AS (e.g.
hecondemned him as stupid) frames.In feature sets 2 and 3, 2-5 PP SCFs were refined ac-cording to the prepositions provided in the VALEXSCF entries (e.g.
PP at, PP on, PP in) because Levinspecifies prepositions with some SCFs / classes.
Thescope was restricted to the 3-5 highest ranked PPSCFs to reduce the effects of sparse data.4 Classification4.1 Preparing the DataA feature vector was constructed for each verb.VALEX includes 107, 287 and 305 SCF types for fea-ture sets 1, 2, and 3, respectively.
Each feature corre-sponds to a SCF type, and its value is the relative fre-quency of the SCF with the verb in question.
Someof the feature values are zero, because most verbstake only a subset of the possible SCFs.4.2 Machine Learning MethodsWe implemented three methods for classification:the K nearest neighbours (KNN), support vector ma-chines (SVM), and maximum entropy (ME).
To ourknowledge, only SVM has been previously used for1The SCF accuracy of this lexicon is 23.7 F-measure, see(Korhonen et al, 2006) for details.verb classification.
The free parameters were opti-mised for each feature set by (i) defining the valuerange (as explained below), and by (ii) searching forthe optimal value on the training data using 10 foldcross validation (section 5.2).4.2.1 K Nearest NeighboursKNN is a memory-based classification methodbased on the distances between verbs in the featurespace.
For each verb in the test data, we measureits distance to each verb in the training data.
Theverb class label is the most frequent label in thetop K closest training verbs.
We use the entropy-based Jensen-Shannon (JS) divergence as the dis-tance measure:JS(P,Q) = 12?D(P?P+Q2 ) +D(Q?P+Q2 )?The range of the parameter K is 2-20.4.2.2 Support Vector MachinesSVM (Vapnik, 1995) tries to find a maximal mar-gin hyperplane to separate between two groups ofverb feature vectors.
In practice, a linear hyperplanedoes not always exist.
SVM uses a kernel functionto map the original feature vectors to higher dimen-sion space.
The ?maximal margin?
optimizes ourchoice of dimensionality to avoid over-fitting.
Weuse Chang and Lin (2001) ?s LIBSVM library to im-plement the SVM.
Following Hsu et al (2003), weuse the radial basis function as the kernel function:K(xi, xj) = exp (?
?||xi ?
xj ||2), ?
> 0?
and the cost of the error term C (the penalty formargin errors) are optimized.
The search ranges ofHsu et al (2003) are used:C = 2?5, 2?3, .
.
.
, 215, 217 ; ?
= 2?17, 2?15, .
.
.
, 21, 234.2.3 Maximum EntropyME constructs a probabilistic model that maxi-mizes entropy on test data subject to a set of featureconstraints.
If verb x is in class 10.1 and takes theSCF 49 (NP-PP) with the relative frequency of 0.6 infeature function f , we havef(x, y) = 0.6 if y = 10.1 and x = 49The expected value of a feature f with respect to theempirical distribution (training data) isE?
(f) ?Px,y p?
(x, y)f(x, y)The expected value of the feature f (on test data)with respect to the model p(y|x) is771E(f) ?Px,y p?
(x)p(y|x)f(x, y)p?
(x) is the empirical distribution of x in the train-ing data.
We constrain E(f) to be the same as E?
(f)E(f) = E?
(f)The model must maximize the entropy H(Y |X)H(Y |X) ?
?Px,y p?
(x)p(y|x) log p(y|x)The constraint-optimization problem is solved bythe Lagrange multiplier (Pietra et al, 1997).
Weused Zhang (2004)?s maximum entropy toolkit forimplementation.
The number of iterations i (5-50)of the parameter estimation algorithm is optimised.5 Experiments5.1 MethodologyWe split the data into training and test sets using twomethods.
The first is ?leave one out?
cross-validationwhere one verb in each class is held out as test data,and the remaining N-1 (i.e.
11) verbs are used astraining data.
The overall accuracy is the averageaccuracy of N rounds.
The second method is re-sampling.
For each class, 3 verbs are selected ran-domly as test data, and 9 are used as training data.The process is repeated 30 times, and the averageresult is recorded.5.2 MeasuresThe methods are evaluated using first accuracy ?
thepercentage of correct classifications out of all theclassifications:Accuracy = truePositivestruePositives+falseNegativesWhen evaluating the performance at class level, pre-cision and recall are calculated as follows:Precision = truePositivestruePositives+falsePositivesRecall = truePositivestruePositives+falseNegativesF-score is the balance over recall and precision.
Wereport the average F-score over the 17 classes.
Giventhere are 17 classes in the data, the accuracy of ran-domly assigning a verb into one of the 17 classes is1/17 ?
5.8%.5.3 Results from Quantitative EvaluationTable 2 shows the average performance of each clas-sifier and feature set according to ?leave one out?cross-validation2.
Each classifier performs consid-erably better than the random baseline.
The simple2Recall is not shown as it is identical here with accuracy.KNN method produces the lowest accuracy (44.1-54.9) and SVM and ME the best (47.1-57.9 and 47.5-59.3, respectively).The performance of all methods improves sharplywhen moving from the feature set 1 to the refinedfeature set 2: both accuracy and F-measure improveby over 10%.
When moving from feature set 2 tothe sparser feature set 3 (which includes a highernumber of low frequency PP features) KNN worsensclearly (c. 5% in accuracy and F-measure) while theimprovement in other methods is very small.
Thissuggests that KNN deals worse than other methodswith sparse data.The resampling results in table 3 reveal that someclassifiers perform worse than others when lesstraining data is available3.
KNN produces consid-erably lower results, particularly with the sparsefeature set 3: 28.2 F-measure vs. 48.2 with cross-validation.
Also SVM performs worse with fea-ture set 3: 54.6 F-measure vs. 58.2 with cross-validation.
ME thus appears the most robust methodwith smaller training data, producing results compa-rable with those in cross-validation.Figure 1 shows the F-measure for 17 individualclasses when the methods are used with feature set3.
Levin classes 40.2, 29.2, and 37.3 (see table 2)(the ones taking fewer prepositions with higher fre-quency) have the best average performance (65% ormore), and classes 47.3, 45.4 and 18.1 the worst(40% or less).
ME outperforms SVM with 9 of the17 classes.5.4 Qualitative EvaluationWe did some qualitative analysis to trace the ori-gin of error types produced by ME with feature set3.
Examination of the worst performing class 47.3(MODES OF BEING INVOLVING MOTION verbs) il-lustrates well the various error types.
10 of the 12verbs in this class are classified incorrectly:?
3 in class 43.1 (LIGHT EMISSION verbs): Verbs in 47.3and 43.1 describe intrinsic properties of their subjects(e.g.
a jewel sparkles, a flag flutters).
Their similar al-ternations and PP SCFs make it difficult to separate themon syntactic grounds.?
2 in class 51.3.2 (RUN verbs): 47.3 and 51.3.2 share themeaning component of motion.
Their members take sim-ilar alternations and SCFs, which causes the confusion.3Recall that the amount of training data is smaller with re-sampling evaluation, see section 5.2.772Feature set 1 Feature set 2 Feature set 3ACC P F ACC P F ACC P FRAND 5.8 5.8 5.8KNN 44.1 48.4 44.0 54.9 56.9 53.9 49.5 47.0 48.2ME 47.5 49.4 47.6 59.3 61.4 59.9 59.3 61.9 60.0SVM 47.1 50.4 47.8 57.8 59.4 57.9 57.8 60.1 58.2Table 2: ?Leave one out?
cross-validation results for KNN, ME, and SVMFeature set 1 Feature set 2 Feature set 3ACC P F ACC P F ACC P FRAND 5.8 5.8 5.8KNN 37.3 39.9 36.5 42.7 47.2 42.6 27.1 34.2 28.2ME 47.1 47.3 47.0 58.1 59.1 58.1 60.1 60.5 59.8SVM 47.3 50.2 47.7 56.8 59.5 57.1 54.4 56.5 54.6Table 3: Re-sampling results for KNN, ME, and SVM?
2 in class 37.7 (SAY verbs) and 1 in class 37.3 (MANNEROF SPEAKING verbs): 47.3 differs in semantics and syn-tax from 37.7 and 37.3.
The confusion is due to idiosyn-cratic properties of individual verbs (e.g.
quake, wiggle).?
1 in class 36.1 (CORRESPOND verbs): 47.3 and 36.1 aresemantically very different, but their members take simi-lar intransitive and PP SCFs with high frequency.?
1 in class 45.4 (OTHER CHANGE OF STATE verbs):Classes 47.3 and 45.3 are semantically different.
Theirsimilar PP SCFs explains the misclassification.Most errors concern classes which are in fact se-mantically related.
Unfortunately there is no goldstandard which would comprehensively capture thesemantic relatedness of Levin classes.
Other er-rors concern semantically unrelated but syntacticallysimilar classes ?
cases which we may be able to ad-dress in the future with careful feature engineering.Some errors relate to syntactic idiosyncracy.
Theseshow the true limits of lexical classification - the factthat the correspondence between the syntax and se-mantics of verbs is not always perfect.6 Discussion and ConclusionOur best results (e.g.
60.1 accuracy and 59.8 F-measure of ME) are good, considering that no so-phisticated feature engineering / selection based onthe properties of the target classification was per-formed in these experiments.
The closest compari-son point is the recent experiment reported by Joaniset al (2007) which involved classifying 835 Englishverbs to 14 Levin classes using SVM.
Features werespecifically selected via analysis of alternations thatare used to characterize Levin classes.
Both shal-low syntactic features (syntactic slots obtained us-ing a chunker) and deep ones (SCFs extracted usingBriscoe and Carroll?s system) were used.
The accu-racy was 58% with the former and only 38% withthe latter.
This experiment is not directly compa-rable with ours as we classified a smaller numberof verbs (204) to a higher number of Levin classes(17) (i.e.
we had less training data) and did not se-lect the optimal set of features using Levin?s alter-nations.
We nevertheless obtained better accuracywith our best performing method, and better accu-racy (47%) with the same method (SVM) when thecomparable feature set 1 was acquired using the verysame subcategorization acquisition system.It is likely that using larger and noisier SCF dataexplains the better result, suggesting that rich syn-tactic features incorporating information about botharguments and adjuncts are ideal for verb classifica-tion.
Further experiments are required to determinethe optimal set of features.
In the future, we planto experiment with different (noisy and filtered) ver-sions of VALEX and add to the comparison a shal-lower set of features (e.g.
NP and PP slots in VALEXregardless of the specific SCFs).
We will also im-prove the features e.g.
by enriching them with addi-tional syntactic information available in VALEX lex-ical entries.AcknowledgementThis work was partially supported by the Royal So-ciety, UK.773Figure 1: Class level F-score for feature set 3 (cross-validation)ReferencesE.
J. Briscoe and J. Carroll.
1997.
Automatic extractionof subcategorization from corpora.
In Proceedings ofthe 5th ACL Conference on Applied Natural LanguageProcessing, pages 356?363, Washington DC.E.
J. Briscoe and J. Carroll.
2002.
Robust accurate statis-tical annotation of general text.
In Proceedings of the3rd LREC, pages 1499?1504, Las Palmas, Gran Ca-naria.C.
Chang and J. Lin, 2001.
LIBSVM: a library for sup-port vector machines.H.
T. Dang.
2004.
Investigations into the Role of Lexi-cal Semantics in Word Sense Disambiguation.
Ph.D.thesis, CIS, University of Pennsylvania.B.
J. Dorr.
1997.
Large-scale dictionary constructionfor foreign language tutoring and interlingual machinetranslation.
Machine Translation, 12(4):271?322.W.
Hsu, C. Chang, and J. Lin.
2003.
A practical guide tosupport vector classification.R.
Jackendoff.
1990.
Semantic Structures.
MIT Press,Cambridge, Massachusetts.E.
Joanis, S. Stevenson, and D. James.
2007.
A generalfeature space for automatic verb classification.
Natu-ral Language Engineering, Forthcoming.A.
Korhonen, Y. Krymolowski, and Z. Marx.
2003.Clustering polysemic subcategorization frame distri-butions semantically.
In Proceedings of the 41st An-nual Meeting on Association for Computational Lin-guistics, pages 64?71.A.
Korhonen, Y. Krymolowski, and T. Briscoe.
2006.A large subcategorization lexicon for natural languageprocessing applications.
In Proceedings of LREC.B.
Levin.
1993.
English Verb Classes and Alternations.Chicago University Press, Chicago.P.
Merlo and S. Stevenson.
2001.
Automatic verb clas-sification based on statistical distributions of argumentstructure.
Computational Linguistics, 27(3):373?408.G.
A. Miller.
1990.
WordNet: An on-line lexi-cal database.
International Journal of Lexicography,3(4):235?312.S.
D. Pietra, J. D. Pietra, and J. D. Lafferty.
1997.
Induc-ing features of random fields.
IEEE Transactions onPattern Analysis and Machine Intelligence, 19(4):380?393.D.
Prescher, S. Riezler, and M. Rooth.
2000.
Using aprobabilistic class-based lexicon for lexical ambiguityresolution.
In 18th International Conference on Com-putational Linguistics, pages 649?655, Saarbru?cken,Germany.S.
Schulte im Walde.
2000.
Clustering verbs semanti-cally according to their alternation behaviour.
In Pro-ceedings of COLING, pages 747?753, Saarbru?cken,Germany.S.
Schulte im Walde.
2006.
Experiments on the au-tomatic induction of german semantic verb classes.Computational Linguistics, 32(2):159?194.L.
Shi and R. Mihalcea.
2005.
Putting pieces together:Combining FrameNet, VerbNet and WordNet for ro-bust semantic parsing.
In Proceedings of the Sixth In-ternational Conference on Intelligent Text Processingand Computational Linguistics, Mexico City, Mexico.R.
Swier and S. Stevenson.
2004.
Unsupervised seman-tic role labelling.
In Proceedings of the 2004 Confer-ence on Empirical Methods in Natural Language Pro-cessing, pages 95?102, Barcelona, Spain, August.V.
N. Vapnik.
1995.
The nature of statistical learningtheory.
Springer-Verlag New York, Inc., New York,NY, USA.L.
Zhang, 2004.
Maximum Entropy Modeling Toolkit forPython and C++, December.774
