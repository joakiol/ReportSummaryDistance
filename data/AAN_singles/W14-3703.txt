Proceedings of TextGraphs-9: the workshop on Graph-based Methods for Natural Language Processing, pages 15?24,October 29, 2014, Doha, Qatar.c?2014 Association for Computational LinguisticsMulti-document Summarization Using Bipartite GraphsDaraksha Parveen and Michael StrubeHeidelberg Institute for Theoretical Studies gGmbHSchloss-Wolfsbrunnenweg 3569118 Heidelberg, Germany(daraksha.parveen|michael.strube)@h-its.orgAbstractIn this paper, we introduce a novel graphbased technique for topic based multi-document summarization.
We transformdocuments into a bipartite graph whereone set of nodes represents entities and theother set of nodes represents sentences.
Toobtain the summary we apply a rankingtechnique to the bipartite graph which isfollowed by an optimization step.
We testthe performance of our method on severalDUC datasets and compare it to the state-of-the-art.1 IntroductionTopic-based multi-document summarization aimsto create a single summary from a set of givendocuments while considering the topic of inter-est.
The input documents can be created by query-ing an information retrieval or search engine for aparticular topic and retaining highly ranked docu-ments, or by clustering documents of a large col-lection and then using each cluster as a set of inputdocuments (Galanis et al., 2012).
Here, each clus-ter of the set of documents contains a representa-tive topic.A summary extracted from a set of input doc-uments must be related to the topic of that set.If textual units (or sentences) extracted fromdifferent documents convey the same informa-tion, then those units are called redundant.
Ide-ally, the multi-document summary should be non-redundant.
Hence each textual unit in a summaryshould convey unique information.
Still, all ex-tracted textual units should be related to the topic.They should also make up a coherent summary.When building summaries from multiple docu-ments belonging to different sets, a system shouldattempt to optimize these three basic properties:1.
Relevance: A summary should contain onlythose textual units which are relevant to thetopic and provide useful information.2.
Non-redundancy: A summary should notcontain the same information twice.3.
Readability: A summary should have goodreadability (syntactically well formed, nodangling pronouns, coherent, .
.
.
).Generally, multi-document summarization sys-tems differ from each other on the basis of docu-ment representation, sentence selection method oron the requirements for the output summary.
Pop-ular methods for document representation includegraph-based representations (e.g.
LexRank (Erkanand Radev, 2004) and TextRank (Mihalcea and Ta-rau, 2004)) and tf-idf vector-based representations(Luhn, 1958; Nenkova and Vanderwende, 2005;Goldstein et al., 2000).
These document represen-tations act as input for the next phase and provideinformation about the importance of individualsentences.
Sentence selection is the crucial phaseof the summarizer where sentence redundancymust be handled in an efficient way.
A widelyused technique is the greedy approach introducedby Carbonell and Goldstein (1998) and Goldsteinet al.
(2000).
They compute a relevance score forall sentences with regard to the topic, start by ex-tracting the most relevant sentence, and then itera-tively extract further sentences which are relevantto the topic and at the same time most dissimilarto already extracted sentences.
Later more fun-damental optimization methods have been widelyused in multi-document summarization, e.g.
Inte-ger Linear Programming (ILP) (McDonald, 2007;Gillick et al., 2009; Nishikawa et al., 2010; Gala-nis et al., 2012).
Unlike most other approaches(Galanis et al., 2012) has also taken into accountthe readability of the final summary.In this work, we introduce an extractivetopic based multi-document summarization sys-tem which represents documents graphically and15optimizes the importance of sentences and non-redundancy.
The importance of sentences is ob-tained by means of applying the Hubs and Author-ities ranking algorithm (Kleinberg, 1999) on theunweighted bipartite graph whereas redundancy inthe final summary is dealt with entities in a graph.In Section 2 we introduce the state-of-the-art intopic based multi-document summarizaton.
Sec-tion 3 provides a detailed description of our ap-proach.
Experiments are described in Section 4where we also briefly describe the datasets usedand the results.
Section 5 discusses the results ofour approach, and in Section 6 we finally give con-clusions.2 Related workA graph-based representation of documents forsummarization is adopted by various approaches.For instance, TextRank by Mihalcea and Tarau(2004) applies the PageRank algorithm (Brin andPage, 1998) to extract important sentences for sin-gle document summarization.
This ranking algo-rithm proclaims the importance of a sentence byconsidering the global information which is com-puted recursively from the entire graph.
Later,the graph is converted into a weighted graph inwhich the weights are calculated by measuring thesimilarity of sentences (Mihalcea, 2004).
Simi-larly, in the LexRank approach (Erkan and Radev,2004), documents are represented as a similaritygraph in which the sentences are nodes and thesesentences are then ranked according to centralitymeasures.
The three centrality measures used aredegree, LexRank with threshold and continuousLexRank.
LexRank is a measure to calculate ranksusing the similarity graph of sentences.
It is alsoknown as lexical PageRank.
The summarizationapproach developed by Gong and Liu (2001) isalso based on ranking sentences where importantsentences are selected using a relevance measureand latent semantic analysis.Later, for better performance, sentences areclassified according to their existence in their finalsummary in binary format i.e.
1 (belongs to sum-mary) and 0 (doesn?t belong to summary) (Shen etal., 2007; Gong and Liu, 2001).
Here, the sen-tences are projected as feature vectors and con-ditional random fields are used to classify them.During document processing, most informativesentences are selected by the summarizer (Shenet al., 2007).
Fattah and Ren (2009) also consid-ers summarization as two class classification prob-lem.
They use a genetic algorithm and mathemati-cal regression to select appropriate weights for thefeatures and used different classification techniquefor e.g.
feed forward neural network, probablisticneural network and Gaussian mixture models.In the summarization task, optimization of thethree properties discussed in Section 1, relevance,non-redundancy and readability, is required.
Thisis a global inference problem, which can be solvedby two approaches.
Firstly, relevance and redun-dancy can be optimized simultaneously.
For in-stance, Goldstein et al.
(2000) developed a met-ric named MMR-MD (influenced by the Max-imum Marginal Relevance (MMR) approach ofCarbonell and Goldstein (1998)) and applied it toclusters of passages.
Similarly, influenced by theSumBasic system (Nenkova and Vanderwende,2005), Yih et al.
(2007) developed a system whichassigns a score to each term on the basis of po-sition and frequency information and selects thesentence having highest score.
Other approachesare based on an estimate of word importance (e.g.Lin and Hovy (2000)) or the log likelihood ratiotest which identifies the importance of words usinga supervised model that considers a rich set of fea-tures (Hong and Nenkova, 2014).
Finally, Barzi-lay and Elhadad (1999) extract sentences whichare strongly connected by lexical chains for sum-marization.
The second approach deals with rel-evance and redundancy seperately.
For instance,McKeown et al.
(1999) create clusters of similarsentences and pick the representative one from ev-ery cluster.
The representative sentence of a clus-ter of sentences takes care of the requirement toextract relevant information whereas clustering re-duces the redundancy.McDonald (2007) proposes a new ILP opti-mization method for extractive summarization.
Heintroduces an objective function which maximizesthe importance of sentences and minimizes thesimilarity of sentences.
ILP methods for optimiza-tion have also been adopted by Berg-Kirkpatricket al.
(2011),Woodsend and Lapata (2012) andGalanis et al.
(2012).
Until now, Galanis etal.
(2012) have reported the highest scores formulti-document summarization on DUC2005 andDUC2007.
However, their approach is not com-pletely unsupervised.163 Our methodThis section describes the technique, which weadopted for summarization.
We start by discussingthe graphical representation of the text followedby a description how to quantify the importanceof sentences in the input texts.
We then discussthe ILP technique which optimizes the importanceof sentences and redundancy.3.1 Graphical representation of textThe graphical representation of a text makes itmore expressive than a traditional tf-idf depictionfor summarization.
A graph can easily capturethe essence of the whole text without leading tohigh computational complexity.
Guinaudeau andStrube (2013) introduced a bipartite graph repre-sentation of text based on the entity grid (Barzilayand Lapata, 2008) representation of text.
The pro-jection of this bipartite graph representation hasbeen used for calculating the local coherence ofa text (Guinaudeau and Strube, 2013).
The basicintuition to use a bipartite graph for summariza-tion is that it contains entity transitions similar tolexical chains (Barzilay and Elhadad, 1999).
Anappropriate measure to determine the importanceof sentences by considering strong entity transi-tions indicates the information central to a text bet-ter than simply giving scores on the basis of mostfrequent words.
The unweighted bipartite graphG = (Vs, Ve, L) contains two sets of nodes, Vscorresponding to the sentences from the input textand Vecorresonding to the entities, and a set ofedges represented by L. Figure 1 shows a modelsummary from the DUC 2006 data, which is trans-formed into an entity grid in Figure 2 (Barzilayand Lapata, 2008; Elsner and Charniak, 2011).Here, cells are filled with the syntactic role a men-tion of an entity occupies in a sentence.
Subjectsare denoted by S, objects by O and all other rolesby X.
If an entity is not mentioned in a sentencethen the corresponding cell contains ?-?.
In thecorresponding bipartite graph (Figure 3), edges arecreated between a sentence and an entity only ifthe entity is mentioned in a sentence (the cell inentity grid is not ?-?).
Since this is a dyadic graph,there are no edges between nodes of the same set.3.2 Ranking the importance of sentencesA graph based ranking algorithm is used to cal-culate the importance of a sentence representedas a node in the graph discussed above.
In con-trast to the local information specific to a ver-tex, graphical ranking algorithms take (graph-)global information to calculate the rank of a node.The Hyperlink-Induced Topic Search algorithm(HITS, also known as Hubs and Authorities) byKleinberg (1999) is used to rank sentences in ourmethod.
This algorithm considers two types ofnodes, hence it is well suited to rank sentences inour bipartite graph.
Entities are considered as hubnodes, and sentences are considered as authoritynodes.
The importance of a sentence is calculatedin two steps:?
Hub update rule: Update each node?s hubscore to be equal to the sum of the author-ity scores of each node that it points to.
It canbe written as:HubScore = A ?AuthorityScore (1)Here, A is an adjacency matrix which representsthe connection between the nodes in a graph.?
Authority update rule: In this step, each au-thority node is updated by equating them tothe sum of the hub scores of each node, whichis pointing to that authority node.
It can bewritten as:AuthorityScore = AT?HubScore (2)Hence, the authority weight is high if it ispointed at by a hub having high weights.Given some intial ranks to all nodes in a graph,the hub and authority update rules are applied un-til convergence.
After applying this algorithm, therank of every node is obtained.
The rank is consid-ered as importance of the node within the graph.We normalize the ranks of sentences according tosentence length to avoid assigning high ranks tolong sentences.To incorporate important information from doc-uments, ranks of entities are incremented byRank+tfdoc?idfdocin every iteration, where tfdocshows the importance of an entity in a documentby calculating the frequency whereas idfdocis aninverse document frequency from the current clus-ter.
Rank+ tfdoc?
idfdocis used in calculating theAuthorityScore.
Initially, theRank can be any nu-merical value but after every iteration of the HITSalgorithm it will be updated accordingly.17S1The treatment of osteoarthritis includes a number of non-steroidal anti-inflammatory drugs such asaspirin, acetaminophen, and ibuprofen.S2These drugs, however, cause liver damage and gastrointestinal bleeding and contribute to thousandsof hospitalizations and deaths per year.S3New cox-2 inhibitor drugs are proven as effective against pain, with fewer gastrointestinal sideeffects.S4The two together appeared to reduce knee pain after 8 weeks.Figure 1: Model summary from DUC 2006TREATMENT(e1)OSTEOARTHRITIS(e2)NUMBER(e3)DRUGS(e4)ASPIRIN(e5)ACETAMINOPHEN(e6)IBUPROFEN(e7)DAMAGE(e8)BLEEDING(e9)THOUSANDS(e10)DEATHS(e11)YEAR(e12)PAIN(e13)EFFECTS(e14)TWO(e15)WEEKS(e16)S1S X O X X X X - - - - - - - - -S2- - - S - - - O O X X X - - - -S3- - - S - - - - - - - - X X - -S4- - - - - - - - - - - - O - S XFigure 2: Entity grid of the model summary from Figure 1Figure 3: Bipartite graph derived from the entity grid from Figure 23.3 Optimization algorithmIn topic-based multi-document summarization,the final summary should be non-redundant.
Atthe same time it should contain the important in-formation from the documents.
To achieve thesetwo conditions, we employ integer linear program-ming (ILP) to obtain an optimal solution.
In ILPwe maximize an objective function.
Our objectivefunction, given in Equation 3, has two parts: theimportance of a summary and the non-redundancyof a summary.
The values obtained after rankingby the HITS algorithm are used as the importanceof sentences for ILP.
Non-redundancy can not becalculated for a single sentence.
Instead, it has tobe evaluated with respect to other sentences.
Wecalculate non-redundancy by the number of un-shared entities, i.e.
entities which are not sharedby other sentences, after appending a sentence toa summary.
The least redundant sentence will in-crease the number of entities in the final summary.max(?1n?i=1(Rank(si) + topicsim(si))?xi+?2m?j=1yj)(3)Equation 3 is the objective function where m is18Topic Documents per topic Human Summaries Word limit in final summaryDUC 2005 50 25-50 4-9 250DUC 2006 50 25 4 250DUC 2007 45 25 4 250Table 1: Document Statisticsthe number of entities in a document and n is thenumber of sentences in a document.
xiand yjarebinary variables for sentences and entities respec-tively.
?1and ?2are tuning parameters.
Rank(si)is a rank of a sentence siobtained by applying theHITS algorithm.
Since, we work on topic-basedmulti-document summarization, we include topicinformation by calculating topicsim(si), whichcaptures the cosine similarity of a sentence siwiththe corresponding topic.
If the topic contains morethan one sentence then we take an average of co-sine similarity with a sentence si.
The constraintson the variables are shown in Equations 4-6:n?i=1Len(si) ?
xi?
Len(summary) (4)Here, Len(si) and Len(summary) are thenumber of words in a sentence siand in the fi-nal summary, respectively.
This constraint doesnot allow the length of final summary to exceed itsmaximum length.
The maximum length varies de-pending on the datasets discussed in Section 4.1.?jEiyj?
Entities(si), for i = 1, .
.
.
, n (5)In constraint 5, Eiis a set of entities present ina sentence si.
The number of entities present in asentence is represented as Entities(si).
If a sen-tence siis selected then the entities present in asentence are also selected(?yj= Entities(si)).Whereas, if a sentence siis not selected thensome of its entities can also be selected becausethey may appear in already selected sentences(Entities(si) = 0, ??yj?
0).
In both thecases, constraint 5 is not violated.?iSjxi?
yj, forj = 1, .
.
.
,m (6)In constraint 6, Sjis a set of sentences contain-ing entity yj.
This constraint shows that, if an en-tity yjis selected then at least one sentence is se-lected which contains it (yj= 1, ??xi?
1).
Ifan entity yjis not selected, then it is possible thatnone of the sentences which contain it may not beselected (yj= 0, ?
?xi= 0).
Also, constraint 4holds in either of the cases.4 ExperimentsWe perform experiments on various DUC datasetsto compare the results with state-of-the-art sys-tems.4.1 DatasetsDatasets used for our experiments are DUC2005(Dang, 2005), DUC2006 (Dang, 2006) andDUC20071.
Each dataset contains group of re-lated documents.
Each group of documents con-tains one related topic or a query consisting of afew sentences.
In DUC, the final summary shouldrespond to the corresponding topic.
Also, the sum-mary cannot exceed the maximum allowed length.For instance, in DUC2005, 250 words are allowedin the final summary.
Every document cluster hascorresponding human summaries for evaluatingsystem summaries on the basis of ROUGE scores(Lin, 2004).
The sources of DUC datasets are LosAngeles Times, Financial Times of London, As-sociated Press, New York Times and Xinhua newsagency.
We employ ROUGE SU4 and ROUGE 2as evaluation metrics.
ROUGE returns recall, pre-cision and F-score of a system, but usually only re-call is used in for evaluating automatic summariza-tion systems, because the final summary does notcontain many words.
Hence, if the recall is highthen the summarization system is working well.Document statistics is provided in Table 1.4.2 Experimental setupWe use raw documents from the various DUCdatasets as input for our system.
We remove non-alphabetical characters from the documents.
Thenwe obtain a clean sentence split by means of theStanford parser (Klein and Manning, 2003) so thatthe sentences are compatible with the next steps.1http://www-nlpir.nist.gov/projects/duc/index.html19ROUGE-2 ROUGE-SU4?1= 0.5 & ?2= 0.5 0.07950 0.14060?1= 0.6 & ?2= 0.4 0.07956 0.14071?1= 0.7 & ?2= 0.3 0.07975 0.14105?1= 0.8 & ?2= 0.2 0.07976 0.14106?1= 0.9 & ?2= 0.1 0.07985 0.14107Table 2: Results on different ?
?s on DUC 2005We use the Brown coherence toolkit (Elsner andCharniak, 2011) to convert the documents into theentity grid representation from which the bipar-tite graph is constructed (Guinaudeau and Strube,2013).
Entities in the graph correspond to headnouns of noun phrase mentioned in the sentences.The ranking algorithm from Section 3.2 is appliedto this graph and returns the importance score ofa sentence as required by the objective functiongiven in Equation 3.
Next optimization using ILPis performed as described in Section 3.3.
We useGUROBI Optimizer2for performing ILP.
ILP re-turns a binary value, i.e., if a sentence should beincluded in the summary it returns 1, if not it re-turns 0.
We set ?1= 0.7 and ?2= 0.3 forall datasets.
We did not choose the optimal val-ues, but rather opted for ones which favor impor-tance over non-redundancy.
We did not observesignificant differences between different ?
valuesas long as ?1> ?2(see Table 2).
The sentences inthe output summary are ordered according to theirranks.
If the output summary contains pronouns,we perform pronoun resolution in the source doc-uments using the coreference resolution system byMartschat (2013).
If pronoun and antecedent oc-cur in the same sentence, we leave the pronoun.If the antecedent occurs in an earlier sentence, wereplace the pronoun in the summary by the firstelement of the coreference chain the pronoun be-longs to.
Except for setting ?1and ?2on DUC2005, our approach is unsupervised, as there is notraning data required.
The recall (ROUGE) scoreson different datasets are shown in Table 3.Table 3 shows that our system would have per-formed very well in the DUC 2005 and DUC 2006competitions with ranks in the top 3 and well inthe DUC 2007 competition.
Since the compe-titions date a while back, we compare in addi-tion to the current state-of-art in multi-documentsummarization.
To our knowledge Galanis et al.2Gurobi Optimization, Inc., http://www.gurobi.comDataset ROUGE-2 ROUGE-SU4DUC 2005 (32) 0.07975 (1) 0.14105 (1)DUC 2006 (35) 0.08969 (3) 0.15070 (2)DUC 2007 (32) 0.10928 (6) 0.16735 (5)Table 3: System performance (and rank) on theDUC 2005, 2006 and 2007 (main) data.
The num-ber in parenthesis after the DUC year indicates thenumber of competing systems.
(2012) report the best results on DUC 2005 data.While their ROUGE-2 score is slightly better thanours, we outperform them in terms of ROUGE-SU4 (0.14105 vs. 0.13640), where, to our knowl-edge, our results are the highest reported so far.However, their results on DUC 2007 (ROUGE-20.12517 and ROUGE-SU4 0.17603) are still quitea bit better than our results.
On the DUC 2006data we outperform the HIERSUM system byHaghighi and Vanderwende (2009) on ROUGE-2 (0.08969 vs. 0.086) as well as on ROUGE-SU4 (0.15070 vs. 0.143).
On the DUC 2007data, our results are worse than theirs on ROUGE-2 (0.10928 vs. 0.118) and on par on ROUGE-SU4 (0.16735 vs. 0.167).
The system which wonthe DUC 2007 task, PYTHY by Toutanova et al.
(2007), performs similar to HIERSUM and henceslightly better than our system on these data.
Therecent work by Suzuki and Fukumoto (2014) eval-uates also on DUC 2007 but reports only ROUGE-1 scores.
We obtain a ROUGE-1 score of 0.448 onDUC 2007 which is better than Suzuki and Fuku-moto (2014) (0.438) as well as PYTHY (0.426).The best ROUGE-1 score reported to date hasbeen reported by Celikyilmaz and Hakkani-T?ur(2010) with 0.456.
The difference between thisscore and our score of 0.448 is rather small.5 DiscussionSeveral approaches have been proposed for topicbased multi-document summarization on the DUCdatasets we use for our experiments.
The best re-sults to date have been obtained by supervised andsemi-supervised systems.
The results of our sys-tem are mostly on par with these systems thoughour system is unsupervised (as mentioned in Sec-tion 4 the values for ?1and ?2in the objectivefunction (Equation 3) were not tuned for optimalROUGE scores but rather set for favoring impor-tance over non-redundancy).We compared our results with various state-of-20S1What is being learned from the study of deep water, seabeds, and deep water life?S2What equipment and techniques are used?S3What are plans for future related activity?Figure 4: Topic containing interrogative words from DUC 2007S1I?ve started to use irrigation hoses called ?leaky pipe?.S2Soil?s usually best to water the target area a few days before I plan to dig.S3If I don?t place element in the root zone , element can?t be added later when the plants are growing.S4The new composts were much lighter and more suitable for container plants in garden centres andthrough these were rapidly introduced to gardeners.Figure 5: Sentences containing dangling first person pronoun from DUC 2005the-art systems, and our system is giving compet-itive results in both ROUGE-2 and ROUGE-SU4scores.
However, the ROUGE-2 score of Galaniset al.
(2012) on DUC 2005 is slightly better thanour score.
This might be because they use bigraminformation for redundancy reduction.
However,they need training data for sentence importance.Hence their system has to be classified as super-vised while ours is unsupervised.We have also calculated the ROUGE-1 scoreon DUC 2007 and compared it with state-of-the-art approaches.
HybHsum (Celikyilmaz andHakkani-T?ur, 2010) has obtained the top ROUGE-1 score on DUC 2007 with 0.456.
However,HybHsum is a semi-supervised approach whichrequires a labeled training data.
The differencebetween our ROUGE-1 score of 0.448 and HybH-sum ROUGE-1 score on DUC2007 is not signif-icant (to be fair, achieving significant improve-ments in ROUGE scores on DUC data is very dif-ficult).
In contrast to HybHsum, our approach isunsupervised.Our method computes importance on the basisof a bipartite graph.
We believe that our bipartitegraph captures more information than the generalgraphs used in earlier graph-based approaches toautomatic summarization.
Entity transition infor-mation present in the bipartite graph of a docu-ment, helps us in finding the salient sentences.
Ourapproach works well if the graph is not sparse.We observed a couple of problems in the out-put of our system which we plan to address infuture work.
If topics contain interrogative pro-nouns as shown in Figure 4 the mapping betweentopic and sentences from the documents does notwork well.
We need to resolve which entities theinterrogative pronouns refer to.
Another problemoccurs, because the coreference resolution systememployed does not resolve first person pronouns.Hence, we end up with summaries containing dan-gling first person pronouns as shown in Figure 5.However, our system appears to work reasonablywell in other cases where the summaries are co-herent and readable and also have a high ROUGEscore as shown in the summary from DUC 2007data in Figure 6.6 ConclusionsIn this paper, we have presented an unsuper-vised graph based approach for topic based multi-document summarization.
Our graph based ap-proach provides state-of-the-art results on variousdatasets taken from DUC competitions.
The graphbased representation of a document makes com-putation very efficient and less complex.
In futurework, we incorporate the syntactic roles of enti-ties, to provide more information in the method.AcknowledgmentsThis work has been funded by Klaus TschiraFoundation, Heidelberg, Germany.
The first au-thor has been supported by a Heidelberg Institutefor Theoretical Studies Ph.D. scholarship.21The European Parliament , angered by Turkey ?s human rights record , voted Thursday to freeze hundredsof millions of US dollars in aid to Turkey for setting up a customs union with the EU.
Since then , theEU has been trying to patch up the relationship , with several leaders of member countries insistingthat Turkey ?s place is in the union.
The special aid is part of the agreement between the EuropeanUnion EU and Turkey on the establishment of a customs union between the two sides.
?
The EuropeanUnion , without renouncing its principles , ?
will have to decide in December to allow Turkey to becomea formal candidate for EU membership.
ANKARA , February 27 Xinhua Turkey today welcomed theEuropean Union ?s attitude toward its dispute with Greece and urged the EU to release financial assistanceimmediately despite Greek efforts to block it.
After the decision in December to exclude Turkey fromthe first wave of enlargement talks , Turkey put its relations with the 15 member union on hold.
DuringSolana stay here , Turkish leaders reiterated their position to link the expansion of the NATO with Turkey?s entry into the European Union.
The European Union , European Union Ankara wants to join , ispressing Turkey to find a peaceful solution to the war.
The statement added that Greece , despite itsattempts , was unable to get the support of the other 14 European Union members in getting a statementthat would express solidarity with Greece and condemn Turkey.
Both the European Union and the UnitedStates criticized Turkey for jailing Birdal.Figure 6: Output summary from DUC 2007AcknowledgmentsThis work has been funded by Klaus TschiraFoundation, Heidelberg, Germany.
The first au-thor has been supported by a Heidelberg Institutefor Theoretical Studies Ph.D. scholarship.ReferencesRegina Barzilay and Michael Elhadad.
1999.
Us-ing lexical chains for text summarization.
In Inder-jeet Mani and Mark T. Maybury, editors, Advancesin Automatic Text Summarization, pages 111?121.Cambridge, Mass.
: MIT Press.Regina Barzilay and Mirella Lapata.
2008.
Modelinglocal coherence: An entity-based approach.
Compu-tational Linguistics, 34(1):1?34.Taylor Berg-Kirkpatrick, Dan Gillick, and Dan Klein.2011.
Jointly learning to extract and compress.
InProceedings of the 49th Annual Meeting of the As-sociation for Computational Linguistics, Portland,Oreg., 19?24 June 2011, pages 481?490.Sergey Brin and Lawrence Page.
1998.
Theanatomy of a large-scale hypertextual web searchengine.
Computer Networks and ISDN Systems,30(1?7):107?117.Jaime G. Carbonell and Jade Goldstein.
1998.
The useof MMR, diversity-based reranking for reorderingdocuments and producing summaries.
In Proceed-ings of the 21st Annual International ACM-SIGIRConference on Research and Development in Infor-mation Retrieval, Melbourne, Australia, 24?28 Au-gust 1998, pages 335?336.Asli Celikyilmaz and Dilek Hakkani-T?ur.
2010.
A hy-brid hierarchical model for multi-document summa-rization.
In Proceedings of the 48th Annual Meet-ing of the Association for Computational Linguis-tics, Uppsala, Sweden, 11?16 July 2010, pages 815?824.Hoa Trang Dang.
2005.
Overview of DUC 2005.
InProceedings of the 2005 Document UnderstandingConference held at the Human Language Technol-ogy Conference and Conference on Empirical Meth-ods in Natural Language Processing, Vancouver,B.C., Canada, 9?10 October 2005.Hoa Trang Dang.
2006.
Overview of DUC 2006.
InProceedings of the 2006 Document UnderstandingConference held at the Human Language Technol-ogy Conference of the North American Chapter ofthe Association for Computational Linguistics, NewYork, N.Y., 8?9 June 2006.Micha Elsner and Eugene Charniak.
2011.
Extendingthe entity grid with entity-specific features.
In Pro-ceedings of the ACL 2011 Conference Short Papers,Portland, Oreg., 19?24 June 2011, pages 125?129.G?unes?
Erkan and Dragomir R. Radev.
2004.
LexRank:Graph-based lexical centrality as salience in textsummarization.
Journal of Artificial IntelligenceResearch, 22:457?479.Mohamed Abdel Fattah and Fuji Ren.
2009.
GA,MR, FFNN, PNN and GMM based models for au-tomatic text summarization.
Computer Speech andLanguage, 23(1):126?144.Dimitrios Galanis, Gerasimos Lampouras, and Ion An-droutsopoulos.
2012.
Extractive multi-documentsummarization with integer linear programming andsupport vector regression.
In Proceedings of the24th International Conference on ComputationalLinguistics, Mumbai, India, 8?15 December 2012,pages 911?926.22Daniel Gillick, Korbinian Riedhammer, Benoit Favre,and Dilek Hakkani-T?ur.
2009.
A global optimiza-tion framework for meeting summarization.
In Pro-ceedings of the 2009 IEEE International Conferenceon Acoustics, Speech, and Signal Processing, Taipei,Taiwan, 19?24 June 2009, pages 4769?4772.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andMark Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Proceedingsof the Workshop on Automatic Summarization atANLP/NAACL 2000, Seattle, Wash., 30 April 2000,pages 40?48.Yihong Gong and Xin Liu.
2001.
Generic text summa-rization using relevance measure and latent semanticanalysis.
In Proceedings of the 24th Annual Inter-national ACM SIGIR Conference on Research andDevelopment in Information Retrieval New Orleans,Louis., 9?12 September 2001, pages 19?25.Camille Guinaudeau and Michael Strube.
2013.Graph-based local coherence modeling.
In Proceed-ings of the 51st Annual Meeting of the Associationfor Computational Linguistics, Sofia, Bulgaria, 4?9August 2013, pages 93?103.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing content models for multi-document summariza-tion.
In Proceedings of Human Language Technolo-gies 2009: The Conference of the North AmericanChapter of the Association for Computational Lin-guistics, Boulder, Col., 31 May ?
5 June 2009, pages362?370.Kai Hong and Ani Nenkova.
2014.
Improvingthe estimation of word importance for news multi-document summarization.
In Proceedings of the14th Conference of the European Chapter of theAssociation for Computational Linguistics, Gothen-burg, Sweden, 26?30 April 2014, pages 712?721.Dan Klein and Christopher D. Manning.
2003.
Ac-curate unlexicalized parsing.
In Proceedings of the41st Annual Meeting of the Association for Com-putational Linguistics, Sapporo, Japan, 7?12 July2003, pages 423?430.Jon M. Kleinberg.
1999.
Authoritative sources ina hyperlinked environment.
Journal of the ACM,46(5):604?632.Chin-Yew Lin and Eduard Hovy.
2000.
The auto-mated acquisition of topic signatures for automaticsummarization.
In Proceedings of the 18th Inter-national Conference on Computational Linguistics,Saarbr?ucken, Germany, 31 July ?
4 August 2000,pages 495?501.Chin-Yew Lin.
2004.
ROUGE: A package for auto-matic evaluation of summaries.
In Proceedings ofthe Text Summarization Branches Out Workshop atACL ?04, Barcelona, Spain, 25?26 July 2004, pages74?81.H.P.
Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal of Research and Develop-ment, 2:159?165.Sebastian Martschat.
2013.
Multigraph clustering forunsupervised coreference resolution.
In Proceed-ings of the Student Research Workshop at the 51stAnnual Meeting of the Association for Computa-tional Linguistics, Sofia, Bulgaria, 5?7 August 2013,pages 81?88.Ryan McDonald.
2007.
A study of global inference al-gorithms in multi-document summarization.
In Pro-ceedings of the European Conference on Informa-tion Retrieval, Rome, Italy, 2-5 April 2007.Kathleen R. McKeown, Judith L. Klavans, VassileiosHatzivassiloglou, Regina Barzilay, and Eleazar Es-kin.
1999.
Towards multidocument summarizationby reformulation: Progress and prospects.
In Pro-ceedings of the 16th National Conference on Arti-ficial Intelligence, Orlando, Flo., 18?22 July 1999,pages 453?460.Rada Mihalcea and Paul Tarau.
2004.
TextRank:Bringing order into texts.
In Proceedings of the2004 Conference on Empirical Methods in NaturalLanguage Processing, Barcelona, Spain, 25?26 July2004, pages 404?411.Rada Mihalcea.
2004.
Graph-based ranking algo-rithms for sentence extraction, applied to text sum-marization.
In Companion Volume to the Proceed-ings of the 42nd Annual Meeting of the Associationfor Computational Linguistics, Barcelona, Spain,21?26 July 2004, pages 170?173.Ani Nenkova and Lucy Vanderwende.
2005.
The im-pact of frequency on summarization.
Technical Re-port MSR-TR-2005-101, Microsoft Research.Hitoshi Nishikawa, Takaaki Hasegawa, Yoshihiro Mat-suo, and Genichiro Kikui.
2010.
Opinion summa-rization with integer linear programming formula-tion for sentence extraction and ordering.
In Pro-ceedings of the 23rd International Conference onComputational Linguistics, Beijing, China, 23?27August 2010, pages 910?918.Dou Shen, Jian-Tao Sun, Hua Li, Qiang Yang, andZheng Chen.
2007.
Document summarization us-ing conditional random fields.
In Proceedings ofthe 20th International Joint Conference on ArtificialIntelligence, Hyderabad, India, 6?12 January 2007,pages 2862?2867.Yoshimi Suzuki and Fumiyo Fukumoto.
2014.
De-tection of topic and its extrinsic evaluation throughmulti-document summarization.
In Proceedings ofthe ACL 2014 Conference Short Papers, Baltimore,Md., 22?27 June 2014, pages 241?246.Kristina Toutanova, Chris Brockett, Michael Gamon,Jagadeesh Jagarlamudi, Hisami Suzuki, and LucyVanderwende.
2007.
The PYTHY summariza-tion system: Microsoft Research at DUC 2007.23In Proceedings of the 2007 Document Understand-ing Conference held at the Human Language Tech-nology Conference of the North American Chapterof the Association for Computational Linguistics,Rochester, N.Y., 26?27 April 2007.Kristian Woodsend and Mirella Lapata.
2012.
Mul-tiple aspect summarization using integer linear pro-gramming.
In Proceedings of the 2012 Conferenceon Empirical Methods in Natural Language Pro-cessing and Natural Language Learning, Jeju Is-land, Korea, 12?14 July 2012, pages 233?242.Wen-tau Yih, Joshua Goodman, Lucy Vanderwende,and Hisami Suzuki.
2007.
Multi-document summa-rization by maximizing informative content-words.In Proceedings of the 20th International Joint Con-ference on Artificial Intelligence, Hyderabad, India,6?12 January 2007, pages 1776?1782.24
