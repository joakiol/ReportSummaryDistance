Proceedings of the NAACL HLT Workshop on Software Engineering, Testing, and Quality Assurance for Natural Language Processing, pages 65?73,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsModular resource development and diagnostic evaluation framework forfast NLP system improvementGae?l de Chalendar, Damien NouvelCEA, LIST, Multilingual Multimedia Knowledge Engineering Laboratory,F-92265 Fontenay-aux-Roses, France.
{Gael.de-Chalendar,Damien.Nouvel}@cea.frAbstractNatural Language Processing systems arelarge-scale softwares, whose development in-volves many man-years of work, in terms ofboth coding and resource development.
Givena dictionary of 110k lemmas, a few hundredsyntactic analysis rules, 20k ngrams matricesand other resources, what will be the impacton a syntactic analyzer of adding a new pos-sible category to a given verb?
What will bethe consequences of a new syntactic rules ad-dition?
Any modification may imply, besideswhat was expected, unforeseeable side-effectsand the complexity of the system makes it dif-ficult to guess the overall impact of even smallchanges.
We present here a framework de-signed to effectively and iteratively improvethe accuracy of our linguistic analyzer LIMAby iterative refinements of its linguistic re-sources.
These improvements are continu-ously assessed by evaluating the analyzer per-formance against a reference corpus.
Our firstresults show that this framework is really help-ful towards this goal.1 Introduction1.1 The evaluation frameworkIn Natural Language Processing (NLP), robustnessand reliability of linguistic analyzers becomes aneveryday more addressed issue, given the increas-ing size of resources and the amount of code im-plied by the implementation of such systems.
Be-yond choosing a sound technology, one must nowhave efficients and user-friendly tools around thesystem itself, for evaluating its accuracy.
As shownby (Chatzichrisafis et al, 2008), where developersreceive daily reports of system?s performance forimproving their system, systematic evaluation withregression testing has shown to be gainful to accel-erate grammar engineering.Evaluation campaigns, where several participantsevaluate their system?s performance on a specifictask against other systems, are a good mean tosearch for directions in which a system may be ableto improve its performance.
Often, these evaluationcampaigns also give possibility for participants torun their analyzer on test data and retrieve evalua-tion results.
In this context, parsers authors may relyon evaluation campaigns to provide performance re-sults, but they should also be able to continuouslyevaluate and improve their analyzers between evalu-ation campaigns.
We aim at providing such a genericevaluation tool, using evaluation data to assess sys-tems accuracy, this software will be referenced asthe ?Benchmarking Tool?.Approaches concerning Natural Language Pro-cessing involve everyday more and more resourcedata for analyzing texts.
These resources havegrown enough (in terms of volume and diversity),that it now becomes a challenge to manipulate them,even for experienced users.
Moreover, it is neededto have non-developers being able to work on theseresources: it is necessary to develop accessible toolsthrough intuitive graphical user interfaces.
Such aresource editing GUI tool represent the second partof our contribution, called the ?Resource Tool?.The overall picture is to build a diagnostic frame-work enabling a language specialist, such as a lin-guist, to status, almost in real-time, how modifica-65tions impact our analyzer on as much test data aspossible.
For analyzers, each resource may have aneffect on the final accuracy of the analysis.
It is of-ten needed to iterate over tests before understandingwhat resource, what part of the code needs to be im-proved.
This is especially the case with grammarengineering, where it is difficult to predict the con-sequences of modifying a single rule.
Ideally, ourframework would allow the manipulator to slightlyalter a resource, trigger an evaluation and, almost in-stantaneously, view results and interpret them.
Withthis framework, we expect a large acceleration in theprocess of improving our analyzer.In the remaining of this introduction, we willdescribe our analyzer and Passage, a collabora-tive project including an evaluation campaign andthe production of a reference treebank for Frenchthrough a voting procedure.
Section 2 will describeour evaluation framework; its architecture, its twomain modules and our first results using it.
Section3 describes some related works.
We conclude in sec-tion 4 by describing the next steps of our work.1.2 The LIMA linguistic analyzerOur linguistic analyzer LIMA (LIc2m MultilingualAnalyzer, (Besancon and de Chalendar, 2005)), isimplemented as a pipeline of independent modulesapplied successively on a text.
It implements a de-pendency grammar (Kahane, 2000) in the sense thatproduced analysis are exclusively represented as bi-nary dependency relations between tokens.The analyzer includes, among other modules, atokenizer segmenting the text based on punctuationmarks, a part of speech tagger, short and long dis-tance dependencies extractors based on finite-stateautomata defined by contextualized rules.
The latterrules express successions of categories, augmentedwith constraints (on words inflexion, existence ofother dependencies, etc.).
The analyzer also in-cludes modules to find idiomatic expressions andnamed entities that, once recognized, are mergedinto a single token, thus allowing grammar rules toapply on those.
Furthermore, modules may be spe-cialized in processing language-specific phenomena,e.g.
Chinese tokenization, German compounds, etc.Currently, the analyzer is able to process more orless deeply ten languages, including English, Span-ish, Chinese, Arab, French and German.1.3 The Passage ProjectOur work is part of the Passage project (Clergerieet al, 2008b).
The objectives of this project aretwofold.
Firstly, it organizes two evaluation cam-paigns of syntactic analyzers (around 15 participat-ing systems) for the French language.
Secondly, itaims at producing a large scale reference treebankfor French by merging the output of all the partic-ipating parsers, using a Rover (Recognizer OutputVoting Error Reduction) (Fiscus, 1997) approach.Within this project, syntactic annotations are pro-duced in a common format, rich enough to representall necessary linguistic features and simple enoughto allow participating parsers (using very differentparsing approaches) to represent their analysis inthis format.
It is an evolution of the EASy cam-paign format, mixing simple non recursive chunksand dependency relations between chunks or tokens.It respects two proposed ISO specifications: MAF(ISO 24611) and SynAF (ISO 24615).
The chunksand dependencies types are issued from the ISO datacategory registry, DCR1, currently using the Frenchlanguage section names.
The syntactic analysis ofa corpus in the Passage format provides informationabout:?
Segmentation of the corpus into sentences?
Segmentation of sentences into forms?
Non-recursive typed (listed in Table 1) chunksembedding forms?
Labeled typed (listed in Table 2) dependenciesthat are anchored by either forms or chunksType ExplanationGN Nominal ChunkNV Verbal KernelGA Adjectival ChunkGR Adverbial ChunkGP Prepositional ChunkPV Prepositional non-tensed Verbal KernelTable 1: Chunks typesWithin the EASy project, parsers have been eval-uated against a reference, which itself was a smallsubset of the available corpora.
The reference was1http://www.isocat.org66Type ExplanationSUJ-V Subject-verbAUX-V Aux-verbCOD-V Direct objectsCPL-V Other verb arguments/complementsMOD-V Verb modifiers (e.g.
adverbs)COMP Subordinate sentencesATB-SO Verb attributeMOD-N Noun modifierMOD-A Adjective modifierMOD-R Adverb modifierMOD-P Preposition modifierCOORD CoordinationAPPOS AppositionJUXT JuxtapositionTable 2: Dependencies typescreated by human annotation of random sentenceswithin the corpora.
Thus, once this evaluation cam-paign had been finished, the annotated corpora ref-erence was released for participants to test and im-prove their parser.
Currently, we use this referencefor benchmarking our analyzer.1.4 Metrics for parsing evaluationWe are constantly recalled that evaluation metricsand methodologies evolve and are subject to intenseresearch and innovation (Carroll et al, 2002).
Dis-cussing these metrics is not in the scope of this pa-per, we only need to be able to work out as manymetrics as possible on the entire corpus or on anypart of it.
The evaluation is supposed, for each doc-ument d and for each type (of chunk or of depen-dency) t within all types set T , to return followingcounts:?
Number of items found and correct - fc(d, t)?
Number of items found - f(d, t)?
Number of items correct - c(d, t)With this approach, we are able to compute com-mon Information Retrieval (IR) metrics (Rijsbergen,1979): precision, recall, f-measure.
We also intro-duce a new metric that gives us indications aboutwhat types are the most lowering overall perfor-mance, called ?Type error ratio?
:f(d, t) + c(d, t)?
2.fc(d, t)?
t?T f(d, t) + c(d, t)?
2.fc(d, t) (1)This metric counts the number of errors andmisses for a given type reported to the total numberof errors and misses.
It allows us to quantify howmuch an improvement on a given type will improvethe overall score.
In our case, scores are computedfor chunks on the one hand, and for dependencieson the other hand.
For instance, we have noticesthat GN errors represent 34.6% of the chunks errors,whereas PV only represent 2.2%: we are thus muchmore interested in improving detection of GN thanPV regarding current evaluation campaign.2 The evaluation framework2.1 ArchitectureWe need our framework to be portable and to be im-plemented using an agile approach: each new ver-sion should be fully functional while adding somemore features.
It also must be user-friendly, allow-ing to easily add eye-candy features.
Consequently,we have chosen to implement these tools in C++,using the Qt 4.5 library2.
This library satisfies ourrequirements and will allow to rely on stable andopen source (LGPL) tools, making it feasible for usto possibly deliver our framework as a free software.This approach allows us to quickly deliver work-ing software while continuously testing and devel-oping it.
Iterations of this process are still occurringbut the current version, with its core functions, al-ready succeeded in running benchmarks and in be-ginning the improvement of our linguistic resourceswhile regularly delivering upgraded versions of ourframework.
First results of this work will be pre-sented below in this paper.The open architecture we have chosen implies touse externals tools, for analysis and evaluation onthe one hand, for compiling and installing resourceson the other hand.
These tools may then be con-sidered as black boxes, being externals commandscalled with convenient parameters.
In particular, theBenchmarking Tool relies on two commands: theanalyzer command, receiving input file as a param-eter and producing the analyzed file, the evaluationcommand, receiving the analyzed file and the ref-erence file as parameters and outputting counts offound, correct, found and correct items for each di-mension.
This allows, for example, to replace our2http://www.qtsoftware.com/67analyzer with another one, by just wrapping the lat-ter in a thin conversion layer to convert its inputs andits outputs.2.2 Benchmarking ToolThe Benchmarking Tool, which architecture is de-picted in Figure 1, is responsible of executing anal-ysis and evaluation on pairs of data and referencefiles, using commands stored in benchmarking con-figuration.
For each pair of files, the registered anal-ysis command is executed followed by the evalua-tion one.
In our case, those commands apply to thetask of annotating files for syntactic chunks and de-pendencies.Figure 1: Benchmarking Tool data flowWe may consider the type of chunks and depen-dencies as dimensions of an evaluation.
To a certainextent, these may be associated to linguistics phe-nomena which are tested, as proposed within theTSNLP project (Balkan et al, 1994) or, more re-cently, for Q/A systems by (Paiva et al, 2008).
Butin these projects, focus is also made on the evalua-tion tool, where we do not implement the evaluationtool but rely on an external program to provide ac-curacy of analysis.The pairs of data and reference files are insertedinside a structure implemented as a pipeline, whichmay be modified (adding, removing, reorderingunits) with common GUI interfaces.
After creationof the pipeline, the user may trigger a benchmark-ing (progress is shown by coloring pipeline units),which may be suspended, resumed or restarted atany moment.
For note, the current version of theframework uses the local machine?s processors toanalyze pipeline units in parallel, but we intend todistribute the analyzes on the available nodes of acluster soon.
As soon as results are received, tablesand graphics are updated on screen within a viewshowing previous and current results for each eval-uated dimension.
To refine diagnosis, the user maychoose what dimensions are displayed, what met-rics should be computed, and what pipeline units areused.
Finally, any evaluation may be deleted if thecorresponding modification did not increase perfor-mance and should be reverted.Upon demand, the tool saves current benchmark-ing configuration and results as an XML file.
Con-versely, it loads a pipeline and results from file, soas to resume or switch between evaluations.
Theparsed output of the evaluator tool is recorded foreach pipeline unit and for each dimension, so thatmetrics based on those quantities are computed foreach pipeline unity or for the overall corpus.
Be-sides, the date and a user comment for each evalua-tion are also saved for these records.
Writing com-ments has proved to be very helpful to keep trackof what changes have been made on code, linguisticresources, configuration, parameters, etc.As an example within the Passage project, run-ning evaluation with the Benchmarking Tool al-lowed us to notice that we had difficulties in rec-ognizing verb-auxiliary dependencies.
Consideringprevious results, we detected that this issue appearedafter having introduced a set of idioms concerningpronominal verbs.
Unit testing showed that the anal-ysis of past perfect progressive for pronominal verbswas buggy.
Patching the code gave us a 10 points f-measure gain for AUX-V dimension and 0.3 for alldependencies dimensions (AUX-V having a 2.6%global error rate within dependencies).
Thus, bench-marking results have been saved with appropriatecomment and other improvements or deficienciescould be examined.With these features, the tool offers the possibilityto have an overall view on evaluation results and ontheir evolution across time, given multiple data, di-mensions of analysis and computed metrics.
There-fore, it helps us, without any complex manipulation,to get a visual report on what implication on evalu-ation results has a modification to the analysis pro-cess.
Furthermore, those tests allow to search forerrors in resources as well as in code, so as to findhow to enrich our linguistic resources or to identifydeficiencies in our code.Figure 2 shows a benchmarking using a set of 24evaluation files (left part) to improve the analyzer?s68Figure 2: Chunks (CONSTS), dependencies (RELS), nominal chunks (GN) and direct objects dependencies (COD V)f-measure results evolution through 4 evaluations on a 24 files corpusresults.
The central table shows the measures corre-sponding to 4 successive evaluations, displaying re-sults for the dimensions selected on the top most part(check-boxes).
The right-hand side shows graph-ically the same data, successive evaluations beingdisplayed as its abscissa and measures as its ordi-nate.2.3 Resource ToolThe Resource Tool, which modular design is de-picted in Figure 3, aims at making resources edit-ing accessible for people who have neither a deepknowledge of the system internals nor computer pro-gramming skills.
Enriching our resources implieshaving people, either specialized in linguistics or intesting to interact with the resources, even if not ac-customed to our specific storage format for each re-source.In its current version, the Resource Tool allow toedit the following resources:?
Dictionary: items and their categories?
Syntactic rules: syntactic dependency detectionFigure 3: Resource Tool modular design?
Part-of-speech tagger learning corpus: taggedexamples of ngrams disambiguation matrices?
Idioms: language dependent fixed expressionsThose resources are presented in a tabbed view,each having a dedicated interface and access func-tions.
Within each resource, a search feature is im-plemented, which has shown to be really useful, es-pecially for dictionary.
The tool also provides sim-ple means to save, compile and install resources,once they have been modified.
This has to be verytransparent for the user and we just provide a ?Save?button and another ?Compile and install?
button.The current version of Resource Tool is quite ba-69Figure 4: Viewing and editing disambiguation matrices: probabilities and examples for articles followed by nounssic in terms of edition capacities.
Dictionary has adedicated interface for editing words and their cat-egories, but ngrams, syntactic rules and idioms re-sources may yet only be changed through a basictext editor.Figure 4 shows the resource tool interface for theannotated corpus that allows to build part-of-speechdisambiguation matrices.
The top most tabs allowto switch between resources among editable ones.The data table shows the computed 3-grams (fromour own tag set).
The left part text field shows alist of sentences, where occurrences of the ngramsselected in the above table appear.
The right parttext field shows correspondences between two tagsets.
Eventually, the ?Edit corpus file?
button opensan editor for the user to add sentences or to modifysentences in the tagged corpus.The Resource Tool and the Benchmarking Toolcommunicate together through two signals: on theone hand when resources are installed, the ResourceTool may trigger an evaluation in the BenchmarkingTool, on the other hand when the evaluation has fin-ished, the Resource Tool is notified and warns theuser.
Being aware of their respective status, we alsowarn the user for dangerous operations, like whentrying to install resources while a benchmarking isstill running, or when quitting the application beforelast benchmark is finished.While these two applications are connected to beaware of benchmarking and resource installation sta-tus, no more interaction has been implemented forthe moment to link evaluation and resource editiontogether.
We have considered implementing a fea-ture making possible to automatically do unit testingresource modifications, but, from our point of view,this has to be implemented with following restric-tions: the Benchmarking Tool should remain generic(modifying configuration and resources should notbe part of the tool) ; amount of required disk spaceshould remain minimal (only differences betweenevaluations should be stored).2.4 Preliminary resultsWe recently finished the first implementation itera-tion.
The evaluator itself is provided by a partnerlaboratory.
Its measurement methodology is deeplypresented in (Paroubekr, 2006).
From our point ofview, we are only concerned in the fact that these70Chunks Dependencies ModificationsF P R F P R72.6 72.0 73.2 45.9 54.2 39.8 Initial evaluation76.3 76.2 76.3 47.5 56.1 41.1 Code reengineering / debugging76.7 76.7 76.7 47.6 56.2 41.3 New set of syntactic rules76.9 76.9 76.9 47.8 56.7 41.4 Specified preposition detection rulesTable 3: Benchmarking results, f-measure (F), precision (P), recall (R)measures are relevant for improving the quality ofanalysis produced by our parser.We applied our resource improvement methodol-ogy on a small annotated corpus of approximately80.000 words, delivered after the EASy campaign,among 27 thematic files.
For information, the wholeprocess (analysis and evaluation for each file) is 5minutes long on a bi-processor: this allows the soft-ware to be used intensively on a personal computer.Results in Table 3 show that the use of our frame-work already allowed us to introduce modificationsof the linguistic resources with the Resource Tool;these changes lead to a slight improvement of theoverall score of the system.First, we obtained confirmation that some codereengineering and some debugging was required.These tasks, associated with iterative evaluation,have allowed us to detect parts of the code whichdid not give entire satisfaction, especially in the steptransforming output from our analyzer to the ex-pected Passage format.
We also found a bug withinthe evaluation scripts, which, once corrected, forcedto restart evaluation measures from the beginning:this shows the importance of having a stable en-vironment apart analyzer (evaluation process, validdata and reference file).
These results show that iter-ating over time and saving history may help to revealpotential weaknesses of the code and to detect whatgoes wrong.Secondly, these tools where well-suited for eval-uating the impact of a new set of syntactic rules,for which we did not have opportunities to do pre-cise evaluation before.
For this set of 20 rules,we systematically tried each rule separately, thenkept the combination of the rules increasing scores.This improvement may appear as minimal, but theserules where written in the context of an ongoingwork on our grammar.
It gave an intuitive idea thatthis approach is not a dead-end and may be furtherexplored.
Besides, methodologies have been sug-gested to test the impact of each rule in the entireset of rules by systematically testing combinationsof rules.
But, currently, this is beyond our goal.Finally, we also introduced some ?syntacticsugar?, by grouping some expressions within rules,and successfully obtained insurance that these mod-ification did not lower scores.
This is an importantresult for us in the sense that we ensure that the sameset of rules expressed differently (with rules moreconcise thus more readable) do not introduce regres-sions.3 Related worksWe have previously described the test suite ap-proach, along with the TSNLP project.
This ap-proach was concerned with identifying and system-atically testing linguistic phenomena.
As a conclu-sion of TSNLP, (Oepen et al, 1998) points out thenecessity ?to assess the impact of individual contri-butions, regularly evaluate the quality of the overallgrammar, and compare it to previous versions?.
Thisproject thus showed how positive it is to identify de-ficiencies and improve grammars by iterating testsover time.
This is the goal we intend to reach withour framework.More recently, in biomedical domain, (Baum-gartner et al, 2008) describes implementation ofa framework and, although it is applied to a textmining task, the approach remains quite close inits foundations (evaluation oriented, iterative testing,modular framework, open source, corpora based,etc.)
to ours and encourages these kind of initiativeby showing the importance of continuous evaluationwhile coding parser and engineering grammar.
Thiswork present the interest to rely on the UIMA frame-work, thus allowing a good modularity.
In the future,we should study the interest to give the ability to ourframework to integrate UIMA-ready modules.71Close to our Benchmarking Tool, some projectsaim at building frameworks for text analysis, an-notation and evaluation, which projects encouragepeople to use a common architecture, as openNLPor GATE.
Those may also be used for benchmark-ing and evaluation tasks (Cunningham et al, 2002)as part of their process.
But, while these frame-work often provide evaluation and regression test-ing tools, they are rarely well-suited for only imple-menting specific diagnostic tasks.
We would appre-ciate that such frameworks focusing on evaluating,benchmarking and diagnosing, as generic as possi-ble across IR tasks, become more widely available.If our Benchmarking Tool appears to be appropri-ate for other systems evaluations, we will considermaking it available for the IR community.4 Conclusions and future workFrom our first use of the framework, we are con-vinced of the importance of diagnostic for acceler-ating the improvement of our analyzer, by makinglinguistic resources accessible and by iterating testsand comparing results obtained over time.
We alsoconcluded that this generic framework would be use-ful in other tasks, such as Information Retrieval.
Es-pecially, image retrieval is a very active and growingfield of research, and we currently consider apply-ing the Benchmarking Tool for accelerating the im-provement of the image retrieval system developedin our laboratory (Joint et al, 2004).This work also emphasizes the great distinc-tion between performance evaluation and diagnos-tic evaluation.
In our case, the association of theBenchmarking Tool and the Resource Tool used inconjunction with unit and regression testings helpsto identify what part of the analysis process is con-cerned and, for grammar engineering, what rule orset of rules have to be questioned in order to improvethe overall system performance.Future directions of our work include the paral-lelization of the analysis on a cluster, so as to re-trieve evaluation results as quickly as possible.
Thisshould allow us to use evaluation results from alarger annotated corpus.
We also intend to focus onvisualization of results for better identification andinterpretation of errors, in order to access directly er-roneous analysis and involved resources.
A seconddevelopment iteration will include the developmentof more user friendly resources editors.We also plan to work on automatic syntactic rulesinference, based on previous work in our laboratory(Embarek and Ferret, 2008).
For this goal, contin-uous benchmarking will be even more important asthe system will rely on experts tuning parameters forlearning rules, the syntactic rules themselves beingnot necessarily edited nor viewable for the expert.AcknowledgmentsThis work was partly funded by the French NationalResearch Agency (ANR), MDCA program 2006.ReferencesLorna Balkan, Klaus Netterz, Doug Arnold, Siety Meijer,1994.
Test Suites for Natural Language Processing.Proceedings of the Language Engineering Convention(LEC?94), 17?22.William A Baumgartner, Kevin Bretonnel Cohen,Lawrence Hunter, 2008.
An open-source frameworkfor large-scale, flexible evaluation of biomedical textmining systems.
Journal of Biomedical Discovery andCollaboration 2008, Vol.
3, pp 1.Romaric Besanc?on, Gae?l de Chalendar, 2005.L?analyseur syntaxique de LIMA dans la campagned?valuation EASY.
Actes des Ateliers de la 12e Con-frence annuelle sur le Traitement Automatique desLangues Naturelles (TALN 2005), Vol.
2, pp 21.John Carroll, Anette Frank, Dekang Lin, Detlef Prescher,Hans Uszkoreit, 2002.
Proceedings of the workshopbeyond parseval - toward improved evaluation mea-sures for parsing systems.
Proceedings of the 3rdInternational Conference on Language Resources andEvaluation (LREC?02).Nikos Chatzichrisafis, Dick Crouch, Tracy HollowayKing, Rowan Nairn, Manny Rayner, Marianne Santa-holma, 2007.
Regression Testing For Grammar-BasedSystems.
Proceedings of the GEAF07 Workshop, pp128?143.Eric V. de la Clergerie, Olivier Hamon, Djamel Mostefa,Christelle Ayache, Patrick Paroubek, Anne Vilnat,2008.
PASSAGE: from French Parser Evaluationto Large Sized Treebank.
Proceedings of the SixthInternational Language Resources and Evaluation(LREC?08).Eric V. de la Clergerie, Christelle Ayache, Gae?l deChalendar, Gil Francopoulo, Claire Gardent, PatrickParoubek, 2008.
Large scale production of syntactic72annotations for French.
In Proceedings of the interna-tional workshop on Automated Syntactic Annotationsfor Interoperable Language Resources, Hong-Kong.Hamish Cunningham, Diana Maynard, KalinaBontcheva, Valentin Tablan, 2002.
GATE: Aframework and graphical development environmentfor robust NLP tools and applications.
Proceedings ofthe 40th Anniversary Meeting of the ACL, 2002.Mehdi Embarek, Olivier Ferret, 2008.
Learning patternsfor building resources about semantic relations in themedical domain.
6th Conference on Language Re-sources and Evaluation (LREC?08), Marrakech, Mo-rocco.Jonathan G. Fiscus, 1997.
A Post-Processing System toYield Reduced Word Error Rates: Recognizer OutputVoting Error Reduction (ROVER).
Proceedings IEEEWorkshop on Automatic Speech Recognition and Un-derstanding (ASRU97), pp 347?352.Magali Joint, Pierre-Alain Moellic, Patrick Hede, Pas-cal Adam, 2004.
PIRIA: a general tool for indexing,search, and retrieval of multimedia content.
Proceed-ings of SPIE, Vol.
5298, 116 (2004), San Jose, CA,USA.Sylvain Kahane, 2000.
Les grammaires de dpendance.Traitement Automatique des Langues, Vol.
41.Stephan Oepen, Daniel P. Flickinger, 1998.
Towards sys-tematic grammar profiling.
Test suite technology tenyears after.
Special Issue on Evaluation 12, 411?436.Valeria de Paiva, Tracy Holloway King, 2008.
Design-ing Testsuites for Grammar-based Systems in Appli-cations.
Proceedings of the GEAF08 Workshop, pp49?56.Patrick Paroubek, Isabelle Robba, Anne Vilnat, ChristelleAyache, 2006.
Data, Annotations and Measures inEASY, the Evaluation Campaign for Parsers of French.5th Conference on Language Resources and Evalua-tion (LREC?06), Genoa, Italy.C.
J. van Rijsbergen, 1979.
Information Retrieval, 2ndedition.73
