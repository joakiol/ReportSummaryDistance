Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1422?1427,Denver, Colorado, May 31 ?
June 5, 2015.c?2015 Association for Computational LinguisticsTemplate Kernels for Dependency ParsingHillel Taub-TabibHebrew UniversityJerusalem, Israel{hillel.t,yoav.goldberg}@gmail.comYoav GoldbergBar-Ilan UniversityRamat-Gan, IsraelAmir GlobersonHebrew UniversityJerusalem, Israelgamir@cs.huji.ac.ilAbstractA common approach to dependency parsingis scoring a parse via a linear function of aset of indicator features.
These features aretypically manually constructed from templatesthat are applied to parts of the parse tree.
Thetemplates define which properties of a partshould combine to create features.
Existingapproaches consider only a small subset ofthe possible combinations, due to statisticaland computational efficiency considerations.In this work we present a novel kernel whichfacilitates efficient parsing with feature rep-resentations corresponding to a much largerset of combinations.
We integrate the kernelinto a parse reranking system and demonstrateits effectiveness on four languages from theCoNLL-X shared task.11 IntroductionDependency parsing is the task of labeling a sen-tence x with a syntactic dependency tree y ?
Y (x),where Y (x) denotes the space of valid trees over x.Each word in x is represented as a list of linguis-tic properties (e.g.
word form, part of speech, baseform, gender, number, etc.).
In the graph based ap-proach (McDonald et al, 2005b) parsing is cast as astructured linear prediction problem:hv(x) = argmaxy?Y(x)vT?
?
(x, y) (1)where ?
(x, y) ?
Rdis a feature representation de-fined over a sentence and its parse tree, and v ?
Rdis a vector of parameters.To construct an effective representation, ?
(x, y)is typically decomposed into local representations1See https://bitbucket.org/hillel/templatekernels for imple-mentation.over parts p of the tree y:?
(x, y) =?p?y?
(x, p)Standard decompositions include different types ofparts: arcs, sibling arcs, grandparent arcs, etc.
Fea-ture templates are then applied to the parts to con-struct the local representations.
The templates de-termine how the linguistic properties of the wordsin each part should combine to create features (seeSection 2).Substantial effort has been dedicated to the man-ual construction of feature templates (McDonald etal., 2005b; Carreras, 2007; Koo and Collins, 2010).Still, for both computational and statistical reasons,existing templates consider only a small subset ofthe possible combinations of properties.
From acomputational perspective, solving Eq.
1 involvesapplying the templates to y and calculating a dotproduct in the effective dimension of ?.
The useof many templates thus quickly leads to computa-tional infeasibility (the dimensionality of v, as wellas the number of non-zero features in ?, becomevery large).
From a statistical perspective, the useof a large number of feature templates can lead tooverfitting.Several recent works have proposed solutions tothe above problem.
Lei et al, (2014) representedthe space of all possible property combinations inan arc-factored model as a third order tensor andlearned the parameter matrix for the tensor undera low rank assumption.
In the context of transi-tion parsers, Chen and Manning (2014) have im-plemented a neural network that uses dense repre-sentations of words and parts of speech as its in-put and implicitly considers combinations in its in-ner layers.
Earlier work on transition-based depen-dency parsing used SVM classifiers with 2nd orderpolynomial kernels to achieve similar effects (Hall1422Figure 1: Feature template over the second order con-secutive siblings part type.
The part type contains slotsfor the head (h), sibling (s) and modifier (m) words, aswell as for the two edges (e1 and e2).
Each slot is asso-ciated with a set of properties.
The directed path skipsover the edge properties and defines the partial template<h-cpos=?
; s-cpos=?
; m-gender=?>.et al, 2006).
While training greedy transition-basedparsers such as the ones used in (Chen and Manning,2014) and (Hall et al, 2006) amounts to training amulticlass classifier, the graph-based parsing frame-work explored in (Lei et al, 2014) and in the presentwork is a more involved structured-learning task.In this paper we present a kernel based approachto automated feature generation in the context ofgraph-based parsing.
Compared to tensors and neu-ral networks, kernel methods have the attractiveproperties of a convex objective and well understoodgeneralization bounds (Shawe-Taylor and Cristian-ini, 2004).
We introduce a kernel that allows us tolearn the parameters for a representation similar tothe tensor representation in (Lei et al, 2014) butwithout the low rank assumption, and without ex-plicitly instantiating the exponentially many possi-ble features.In contrast to previous works on parsing with ker-nels (Collins and Duffy, 2002), in which the ker-nels are defined over trees and count the numberof shared subtrees, our focus is on feature combi-nations.
In that sense our work is more closely re-lated to work on tree kernels for relation extraction(Zelenko et al, 2003; Culotta and Sorensen, 2004;Reichartz et al, 2010; Sun and Han, 2014), but thekernel we propose is designed to generate combi-nations of properties within selected part types anddoes not involve the all-subtrees representation.2 Template KernelsFor simplicity, we begin with the case where partsp correspond to head modifier pairs (h,m) (i.e.
allparts belong to the ?arc?
part type).
The features in?
(x, p) can then depend on any property of h,m andthe sentence x.
We denote properties related to h us-ing the prefix h- (e.g., h-pos corresponds to the part-of-speech of the head), and similarly for m-.
Wealso use e- to denote properties related to the tripletsh,m, x (e.g., the surface distance between h,m in xis denoted by e-dist).Templates defined over the ?arc?
part type willthen combine different properties of h,m and e,to create features.
e.g.
the template <h-form=?;e-dist=?
; m-form=?,m-pos=?>, when applied toa dependency arc, may yield the feature: <h-form=dog;e-dist=1;m-form=black,m-pos=JJ>.More generally, a parse tree part can be seen asordered lists of slots that contain properties (differ-ent part types will contain different lists of slots).The feature templates defined over them select oneproperty from each slot (possibly skipping someslots to produce partial templates).
A template canthus be thought of as a directed path between theproperties it selects in the different slots.
Clearly, thenumber of possible templates in a given part type isexponential in the number of its slots.
Figure 1 de-picts the process for sibling parts.As discussed in Section 1, manually constructedfeature templates consider only a small subset of thecombinations of properties (i.e.
a small number of?good?
paths is manually identified and selected).Our goal is to introduce a kernel that allows us torepresent all possible paths for a part type in poly-nomial time.Formally, let ?
(x, y) =?p?y?
(x, p) be a featurerepresentation which associates a feature with anydistinct combination of properties in any of the treeparts in the training set.
For a given part p, the effec-tive dimensionality of ?
(x, p) is thus O (ms) wheres is the number of slots in p, and m is the maximalnumber of properties in a slot.Explicitly representing ?
(x, y) is therefore oftenimpractical.
However, the well known ?kernel trick?
(Shawe-Taylor and Cristianini, 2004) implies thatlinear classifiers depend only on dot products be-tween feature vectors and not on the feature vectors1423themselves.
In the context of reranking (see Sec-tion 3), it means we can learn classifiers if we cancalculate dot products K (y1, y2) = ?T(x1, y1) ??
(x2, y2) for two sentences and candidate parses.2We first note that such dot products can be ex-pressed as sum of dot products over parts:K (y1, y2) =?p?y1?p?
?y2k(p, p?
)where k (p, p?)
= ?
(x1, p) ?
?
(x2, p).To calculate k (p, p?)
we?ll assume for simplic-ity that p and p?are of the same type (otherwisek(p, p?)
= 0).
Let pijand p?ijbe the values of thei?th property in the j?th slot in p, p?
(e.g., for a sec-ond order sibling part as in Figure 1, p1,4will cor-respond to the label of the edge e2 in p) , and letCp?p??
{0, 1}m?sbe a binary matrix comparing pand p?such that[Cp?p?
]ij= 1 when pij= p?ijand0 otherwise.
Simple algebra yields that:k(p, p?)=?j~1T?[Cp?p?
]:,jThat is, calculating k (p, p?)
amounts to multiply-ing the sums of the columns in C.3The runtime ofk (p, p?)
is then O (m?
s) which means the overallruntime of K (y1, y2) is O (|y1| ?
|y2| ?
|s| ?
|m|),where |y1| , |y2| are the number of parts in y1and y2.Finally, note that adding 1 to one of the columncounts ofC corresponds to a slot that can be skippedto produce a partial template (this simulates a wildcard property that is always on).3 Kernel RerankerWe next show how to use the template kernels withina reranker.
In the reranking approach (Collins andKoo, 2005; Charniak and Johnson, 2005), a baseparser produces a list of k-best candidate parses foran input sentence and a separately trained rerankingmodel is used to select the best one.2For brevity we?ll omit x from the kernel parameters and useK (y1, y2) instead of K ((x1, y1), (x2, y2)).3We omit the proof, but intuitively, the product of columnsums is equal to the number of 1 valued paths between elementsin the different columns of C. Each such path corresponds to apath in p and p?where all the properties have identical values.i.e.
it corresponds to a feature that is active in both ?
(x1, p) and?
(x2, p?)
and thus contributes 1 towards the dot product.Features: Our feature vector will have two parts.One, ?g(x, y) ?
Rd1, consists of features obtainedfrom manually constructed templates.
The other,?k(x, y) ?
Rd2, corresponds to our kernel features.We will not evaluate or store it, but rather use thekernel trick for implicitly learning with it, as ex-plained below.
The score of a candidate parse yfor sentence x is calculated via the following linearfunction:?
(x, y) = [?g(x, y) ,?k(x, y)]hv(x, y) = v ?
?
(x, y) (2)Learning For learning we use the passive-aggressive algorithm (Crammer et al, 2006; Mc-Donald et al, 2005a), and adapt it to use with ker-nels.
Formally, let S = {(xi,K (xi))}ni=1be a train-ing set of size n such that K (xi) = {yi1, .
.
.
, yik}is the set of k-best candidate trees produced for thesentence xi.
Assume that yi1is the optimal tree interms of Hamming distance to the gold tree.A key observation to make is that the v generatedby the PA algorithm will depend on two parameters.One is a weight vector w ?
Rd1, in the manuallyconstructed ?gfeature space.
The other is a set ofweights ?ijwith i = 1, .
.
.
, n and j = 1, .
.
.
, k cor-responding to the jthcandidate for the ithsample.4The score is then given by:fw,?
(x, y) = v ??
(x, y) = w ?
?g(x, y)+f?
(x, y)where:f?
(x, y) =?i,j?ij?
(K (yi1, y)?K (yij, y))We can now rewrite the updates of the PA algo-rithm using w, ?, as described in Alg 1.54 ImplementationThe classifier depends on parameters ?ij, which areupdated using the PA algorithm.
In the worst case,all nk of these may be non-zero.
For large datasets,this may slow down both learning and prediction.4This follows from tracing the steps of PA and noting theirdependence on dot products.5The denominator in line 5 is equal to?
?g(xi, yij)?
?g(xi, yi1)?2+K (yij, yij)?2K (yij, yi1)+K (yi1, yi1) so it can be calculated efficiently using the kernel.?yi1?
yij?1is the hamming distance between yi1and yij.The updates for ??
are equivalent to averaging over all alphas initerations 1, ..., T .
We use this form to save space.1424Below we discuss implementation techniques to mit-igate this problem.
To facilitate the discussion werewrite the dot-product computation as follows:f?
(x, y) =?p??y?f?
(x, p?)(3)where:?f?
(x, p?)=?i,j?ij??
?p?yi1k(p, p?)?
?p?yijk(p, p?)?
?Reducing Prediction Runtime From Equation 3we note several facts.
First, prediction involves cal-culating k (p, p?)
for every combination of a partp from the support instances (i.e., those for which?ij> 0) and part p?from the instances in the k-bestlist.
Our implementation thus maintains its supportas a set of parts rather than a set of instances.Second, parts that appear in both yi1and yijdonot affect the result of f?
(x, y) since they can-cel each other out.
Our implementation thus onlyupdates the support set with parts that belong ex-clusively to either yi1or yij.
This improves per-formance significantly since the number of non-overlapping parts in yi1and yijis typically muchsmaller than the total number of parts therein.Another important performance gain is obtainedby caching the results of?f?
(x, p?)
when calculatingf?
(x, y) for the different instances in the k-best list.This avoids recalculating the summation for partsthat occur multiple times in the k-best list.
Onceagain, this amounts to a considerable gain, as thenumber of distinct parts in the k-best list is muchsmaller than the total number of parts therein.Reducing Training Runtime We greatly improvetraining speed by caching the results of f?
(xi, yij)between training iterations so that on each repeat-ing invocation of the function, only the support partsadded since the previous iteration need to be consid-ered.
Since the predictions of the learning algorithmbecome increasingly more accurate, the number ofadded support parts decreases sharply between iter-ations6, and so does the runtime.
In practice, all iter-ations from the 3rd onwards have negligible runtimecompared to the first and second iterations.
Thistechnique allows us to comfortably train the kernel6On correct predictions, ?tat line 5 of Alg 1 is 0, so noupdate is taking place and no support parts are added.Algorithm 1 PA Algorithm for Template KernelsInput: S = {(xi,K (xi))}ni=1, NumIters, Ag-gressiveness parameter C1: ?i, j ?ij?
0, ??ij?
0;T ?
n?NumIters2: for t = 1 to T do3: i?
t mod n4: j ?
argmaxj: yij?K(xi)fw,?
(xi, yij)5: ?t?min{C,fw,?(x,yij)?fw,?(x,yi1)+?yi1?yij?1??(xi,yij)??
(xi,yi1)?2}6: ?ij?
?ij+ ?t7: ??ij?
?
?ij+ ?t(T ?
t+ 1)8: w(t+1)?w(t)+?t(?g(xi, yi1)?
?g(xi, yij))9: end for10: ?i, j, ??ij??
?ijT,?w =1TT?t=1w(t)Output: predictor: argmaxy?K(x)(f?w,??
(x, y))predictor on large datasets.5 Experimental SetupDatasets We test our system on 4 languages fromthe CoNLL 2006 shared task, all with rich mor-phological features.7The properties provided foreach word in these datasets are its form, part ofspeech (pos), coarse part of speech (cpos), lemmaand morph features (number, gender, person, etc.around 10-20 feats in total).
We use 20-fold jack-knifing to create the k-best lists for the reranker(Collins and Duffy, 2002).Base Parser The base parser used in experimentswas the sampling parser of Lei et al (2014), aug-mented to produce the k-best trees encountered dur-ing sampling.
The parser was set to use feature tem-plates over third order part types, but its tensor com-ponent and global templates were deactivated.Features The manual features ?gwere based onfirst to third order templates from Lei et al (2014).For the kernel features ?kwe annotated the nodesand edges in each tree with the properties in Table 1.We used a first order template kernel to train a modelusing all the the possible combinations of head, edgeand modifier properties.
Our kernel also produces allthe property combinations of the head and modifierwords (disregarding the edge properties).7Our property combination approach is less relevant for tree-banks that do not specify morphological properties.
This is the1425Node Unigram properties:formposcpos?i morphiform?1pos?1cpos?1form+1pos+1cpos+1Node Bigram properties: Edge prop:pos?1, pospos, pos+1pos, form?i pos, morphilabellen, distalways onTable 1: Linguistic properties for nodes and edges.Results For each language we train a KernelReranker by running Alg 1 for 10 iterations over thetraining set, using k-best lists of size 25 and C set toinfinity.
As baseline, we train a Base Reranker in thesame setup but with kernel features turned off.
Table2 shows the results for the two systems.
Even thoughthey use the same feature set, the base-reranker lagsbehind the base-parser.
We attribute this to the factthat the reranker explores a much smaller fractionof the search space, and that the gold parse treemay not be available to it in either train or testtime.
However, the kernel-reranker significantly im-proves over the base-reranker.
In Bulgarian andDanish, the kernel-reranker outperforms the base-parser.
This is not the case for Slovene and Ara-bic, which we attribute to the low oracle accuracy ofthe k-best lists in these languages.
As is common inreranking (Jagarlamudi and Daum?e III, 2012), ourfinal system incorporates the scores assigned to sen-tences by the base parser: i.e.
scorefinal(x, y) =?scorebase(x, y) + scorereranker(x, y).
?
is tunedper language on a development set.8Our final sys-tem outperforms the base parser, as well as Tur-boParser (Martins et al, 2013), a parser based onmanually constructed feature templates over up tothird order parts.
The system lags slightly behindthe sampling parser of Zhang et al (2014) which ad-ditionally uses global features (not used by our sys-tem) and a tensor component for property combi-nations.
Another important difference between thesystems is that our search is severely restricted bythe use of a reranker.
It is likely that using our ker-nel in a graph-based parser will further improve itsreason we did not select the English treebank.8To obtain a development set we further split the rerankertraining sets into tuning training and a development sets (90/10).We then tune ?
per language on the respective development setsby selecting the best value from a list of {0, 0.05, .
.
.
, 3}Arabic Slovene Danish BulgarianBase Parser 80.15 86.13 90.76 92.98Base Reranker 79.46 84.61 90.36 92.27Kernel Reranker 79.48 85.25 91.04 93.28Final System 80.19 86.44 91.56 93.4Turbo Parser 79.64 86.01 91.48 93.1Zhang et al 80.24 86.72 91.86 93.72Table 2: System Performance (UAS excluding punctua-tion).
TurboParser is (Martins et al, 2013), Zhang et alis (Zhang et al, 2014)Arabic Slovene Danish BulgarianSentences 1,460 1,534 5,190 12,823Avg.
Sent Len 37 19 18 15Support Parts 15,466 10,101 31,627 58,842Training Time 6m 7m 31m 57mtokens/sec 551 432 223 99Table 3: Runtime statistics, measured on a standard Mac-book Pro 2.8 GHz Core i7 using 8 threads.accuracy.Performance Table 3 lists the performance met-rics of our system on the four evaluation treebanks.While training times are reasonable even for largedatasets, the increase in support size causes predic-tion to become slow for medium and large trainingsets.
The number of support instances is a gen-eral problem with kernel methods.
It has been ad-dressed using techniques like feature maps (Rahimiand Recht, 2007; Lu et al, 2014) and bounded on-line algorithms (Dekel et al, 2008; Zhao et al,2012).
The application of these techniques to tem-plate kernels is a topic for future research.6 ConclusionsWe present a kernel approach to graph based de-pendency parsing.
The proposed method facilitatesglobally optimal parameter estimation in a high di-mensional feature space, corresponding to the fullset of property combinations.
We implemented oursolution as part of a parse reranking system, demon-strating state of the art results.
Future work will fo-cus on performance improvements, using the kernelon higher order parts, and integrating the kernel di-rectly into a graph based dependency parser.Acknowledgments This work is supported by theUS-Israel Binational Science Foundation (BSF, Grant No2012330) and by the Intel Collaborative Research Insti-tute for Computational Intelligence (ICRI-CI).
We thankthe NAACL HLT reviewers for their comments.1426ReferencesXavier Carreras.
2007.
Experiments with a higher-orderprojective dependency parser.
In EMNLP-CoNLL,pages 957?961.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-best parsing and maxent discriminative rerank-ing.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages173?180.Danqi Chen and Christopher D Manning.
2014.
Afast and accurate dependency parser using neural net-works.
In Proceedings of the 2014 Conference onEmpirical Methods in Natural Language Processing(EMNLP), volume 1, pages 740?750.Michael Collins and Nigel Duffy.
2002.
New rankingalgorithms for parsing and tagging: Kernels over dis-crete structures, and the voted perceptron.
In Proceed-ings of the 40th Annual Meeting of the Association forComputational Linguistics, pages 263?270.
Associa-tion for Computational Linguistics.Michael Collins and Terry Koo.
2005.
Discrimina-tive reranking for natural language parsing.
Compu-tational Linguistics, 31(1):25?70.Koby Crammer, Ofer Dekel, Joseph Keshet, Shai Shalev-Shwartz, and Yoram Singer.
2006.
Online passive-aggressive algorithms.
The Journal of Machine Learn-ing Research, 7:551?585.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedings ofthe 42nd Annual Meeting on Association for Compu-tational Linguistics, page 423.
Association for Com-putational Linguistics.Ofer Dekel, Shai Shalev-Shwartz, and Yoram Singer.2008.
The forgetron: A kernel-based perceptron ona budget.
SIAM Journal on Computing, 37(5):1342?1372.Johan Hall, Joakim Nivre, and Jens Nilsson.
2006.
Dis-criminative classifiers for deterministic dependencyparsing.
In Proceedings of the COLING/ACL on Mainconference poster sessions, pages 316?323.Jagadeesh Jagarlamudi and Hal Daum?e III.
2012.
Low-dimensional discriminative reranking.
In Proceedingsof the 2012 Conference of the North American Chap-ter of the Association for Computational Linguistics:Human Language Technologies, pages 699?709.Terry Koo and Michael Collins.
2010.
Efficient third-order dependency parsers.
In Proceedings of the 48thAnnual Meeting of the Association for ComputationalLinguistics, pages 1?11.
Association for Computa-tional Linguistics.Tao Lei, Yu Xin, Yuan Zhang, Regina Barzilay, andTommi Jaakkola.
2014.
Low-rank tensors for scor-ing dependency structures.
In Proceedings of the 52ndAnnual Meeting of the Association for ComputationalLinguistics, volume 1, pages 1381?1391.Zhiyun Lu, Avner May, Kuan Liu, Alireza BagheriGarakani, Dong Guo, Aur?elien Bellet, Linxi Fan,Michael Collins, Brian Kingsbury, Michael Picheny,et al 2014.
How to scale up kernel methodsto be as good as deep neural nets.
arXiv preprintarXiv:1411.4000.Andr?e FT Martins, Miguel Almeida, and Noah A Smith.2013.
Turning on the turbo: Fast third-order non-projective turbo parsers.
In Proceedings of the 51stAnnual Meeting of the Association for ComputationalLinguistics, pages 617?622.Ryan McDonald, Koby Crammer, and Fernando Pereira.2005a.
Online large-margin training of dependencyparsers.
In Proceedings of the 43rd Annual Meeting ofthe Association for Computational Linguistics, pages91?98.Ryan McDonald, Fernando Pereira, Kiril Ribarov, andJan Haji?c.
2005b.
Non-projective dependency pars-ing using spanning tree algorithms.
In Proceedings ofthe conference on Human Language Technology andEmpirical Methods in Natural Language Processing,pages 523?530.
Association for Computational Lin-guistics.Ali Rahimi and Benjamin Recht.
2007.
Random featuresfor large-scale kernel machines.
In Advances in neuralinformation processing systems, pages 1177?1184.Frank Reichartz, Hannes Korte, and Gerhard Paass.2010.
Semantic relation extraction with kernels overtyped dependency trees.
In Proceedings of the 16thACM SIGKDD international conference on Knowl-edge discovery and data mining, pages 773?782.John Shawe-Taylor and Nello Cristianini.
2004.
Kernelmethods for pattern analysis.
Cambridge universitypress.Le Sun and Xianpei Han.
2014.
A feature-enriched treekernel for relation extraction.
In The 52nd AnnualMeeting of the Association for Computational Linguis-tics, volume 2, pages 61?67.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relationextraction.
The Journal of Machine LearningResearch, 3:1083?1106.Yuan Zhang, Tao Lei, Regina Barzilay, and TommiJaakkola.
2014.
Greed is good if randomized: Newinference for dependency parsing.
In Proceedings ofthe 2014 Conference on Empirical Methods in NaturalLanguage Processing (EMNLP), pages 1013?1024.Peilin Zhao, Jialei Wang, Pengcheng Wu, Rong Jin, andSteven CH Hoi.
2012.
Fast bounded online gradi-ent descent algorithms for scalable kernel-based onlinelearning.
arXiv preprint arXiv:1206.4633.1427
