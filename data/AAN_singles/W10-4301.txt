Proceedings of SIGDIAL 2010: the 11th Annual Meeting of the Special Interest Group on Discourse and Dialogue, pages 1?8,The University of Tokyo, September 24-25, 2010. c?2010 Association for Computational LinguisticsTowards Incremental Speech Generation in Dialogue SystemsGabriel SkantzeDept.
of Speech Music and HearingKTH, Stockholm, Swedengabriel@speech.kth.seAnna HjalmarssonDept.
of Speech Music and HearingKTH, Stockholm, Swedenannah@speech.kth.seAbstractWe present a first step towards a model ofspeech generation for incremental dialoguesystems.
The model allows a dialogue systemto incrementally interpret spoken input, whilesimultaneously planning, realising and self-monitoring the system response.
The modelhas been implemented in a general dialoguesystem framework.
Using this framework, wehave implemented a specific application andtested it in a Wizard-of-Oz setting, comparingit with a non-incremental version of the samesystem.
The results show that the incrementalversion, while producing longer utterances,has a shorter response time and is perceivedas more efficient by the users.1 IntroductionSpeakers in dialogue produce speech in a piece-meal fashion and on-line as the dialogue pro-gresses.
When starting to speak, dialogue partici-pants typically do not have a complete plan ofhow to say something or even what to say.
Yet,they manage to rapidly integrate informationfrom different sources in parallel and simultane-ously plan and realize new dialogue contribu-tions.
Moreover, interlocutors continuously self-monitor the actual production processes in orderto facilitate self-corrections (Levelt, 1989).
Con-trary to this, most spoken dialogue systems use asilence threshold to determine when the user hasstopped speaking.
The user utterance is thenprocessed by one module at a time, after which acomplete system utterance is produced and real-ised by a speech synthesizer.This paper has two purposes.
First, to presentan initial step towards a model of speech genera-tion that allows a dialogue system to incremen-tally interpret spoken input, while simultaneouslyplanning, realising and self-monitoring the sys-tem response.
The model has been implementedin a general dialogue system framework.
This isdescribed in Section 2 and 3.
The second purposeis to evaluate the usefulness of incrementalspeech generation in a Wizard-of-Oz setting, us-ing the proposed model.
This is described in Sec-tion 4.1.1 MotivationA non-incremental dialogue system waits untilthe user has stopped speaking (using a silencethreshold to determine this) before starting toprocess the utterance and then produce a systemresponse.
If processing takes time, for examplebecause an external resource is being accessed,this may result in a confusing response delay.
Anincremental system may instead continuouslybuild a tentative plan of what to say as the user isspeaking.
When it detects that the user?s utter-ance has ended, it may start to asynchronouslyrealise this plan while processing continues, withthe possibility to revise the plan if needed.There are many potential reasons for why dia-logue systems may need additional time forprocessing.
For example, it has been assumedthat ASR processing has to be done in real-time,in order to avoid long and confusing responsedelays.
Yet, if we allow the system to startspeaking before input is complete, we can allowmore accurate (and time-consuming) ASR proc-essing (for example by broadening the beam).
Inthis paper, we will explore incremental speechgeneration in a Wizard-of-oz setting.
A commonproblem in such settings is the time it takes forthe Wizard to interpret the user?s utteranceand/or decide on the next system action, resultingin unacceptable response delays (Fraser & Gil-bert, 1991).
Thus, it would be useful if the sys-tem could start to speak as soon as the user hasfinished speaking, based on the Wizard?s actionsso far.11.2 Related workIncremental speech generation has been studiedfrom different perspectives.
From a psycholin-guistic perspective, Levelt (1989) and othershave studied how speakers incrementally pro-duce utterances while self-monitoring the output,both overtly (listening to oneself speaking) andcovertly (mentally monitoring what is about tobe said).
As deviations from the desired output isdetected, the speaker may initiate  self-repairs.
Ifthe item to be repaired has already been spoken,an overt repair is needed (for example by usingan editing term, such as ?sorry?).
If not, the ut-terance plan may be altered to accommodate therepair, a so-called covert repair.
Central to theconcept of incremental speech generation is thatthe realization of overt speech can be initiatedbefore the speaker has a complete plan of what tosay.
An option for a speaker who does not knowwhat to say (but wants to claim the floor) is touse hesitation phenomena such as filled pauses(?eh?)
or cue phrases such as ?let?s see?.A dialogue system may not need to self-monitor its output for the same reasons as hu-mans do.
For example, there is no risk of articu-latory errors (with current speech synthesis tech-nology).
However, a dialogue system may utilizethe same mechanisms of self-repair and hesita-tion phenomena to simultaneously plan and real-ise the spoken output, as there is always a riskfor revision in the input to an incremental mod-ule (as described in Section 2.1).There is also another aspect of self-monitoringthat is important for dialogue systems.
In a sys-tem with modules operating asynchronously, thedialogue manager cannot know whether the in-tended output is actually realized, as the usermay interrupt the system.
Also, the timing of thesynthesized speech is important, as the user maygive feedback in the middle of a system utter-ance.
Thus, an incremental, asynchronous systemsomehow needs to self-monitor its own output.From a syntactic perspective, Kempen &Hoenkamp (1987) and Kilger & Finkler (1995)have studied how to syntactically formulate sen-tences incrementally under time constraints.Dohsaka & Shimazu (1997) describes a systemarchitecture for incremental speech generation.However, there is no account for revision of theinput (as discussed in Section 2.1) and there is noevaluation with users.
Skantze & Schlangen(2009) describe an incremental system that partlysupports incremental output and that is evaluatedwith users, but the domain is limited to numberdictation.In this study, the focus is not on syntactic con-struction of utterances, but on how to build prac-tical incremental dialogue systems within limiteddomains that can handle revisions and produceconvincing, flexible and varied speech output inon-line interaction with users.2 The Jindigo frameworkThe proposed model has been implemented inJindigo ?
a Java-based open source frameworkfor implementing and experimenting with incre-mental dialogue systems (www.jindigo.net).
Wewill here briefly describe this framework and themodel of incremental dialogue processing that itis based on.2.1 Incremental unitsSchlangen & Skantze (2009) describes a general,abstract model of incremental dialogue process-ing, which Jindigo is based on.
In this model, asystem consists of a network of processing mod-ules.
Each module has a left buffer, a processor,and a right buffer, where the normal mode ofprocessing is to receive input from the leftbuffer, process it, and provide output in the rightbuffer, from where it is forwarded to the nextmodule?s left buffer.
An example is shown inFigure 1.
Modules exchange incremental units(IUs), which are the smallest ?chunks?
of infor-mation that can trigger connected modules intoaction (such as words, phrases, communicativeacts, etc).
IUs are typically part of larger units:individual words are parts of an utterance; con-cepts are part of the representation of an utter-ance meaning.
This relation of being part of thesame larger unit is recorded through same-levellinks.
In the example below, IU2 has a same-levellink to IU1 of type PREDECESSOR, meaning thatthey are linearly ordered.
The information thatwas used in creating a given IU is linked to it viagrounded-in links.
In the example, IU3 isgrounded in IU1 and IU2, while IU4 is groundedin IU3.IU1 IU2IU1 IU2IU3 IU3IU3IU4IU4Module AModule Bleft buffer processor right bufferleft buffer processor right bufferFigure 1: Two connected modules.2A challenge for incremental systems is to han-dle revisions.
For example, as the first part of theword ?forty?
is recognised, the best hypothesismight be ?four?.
As the speech recogniser re-ceives more input, it might need to revise its pre-vious output, which might cause a chain of revi-sions in all subsequent modules.
To cope withthis, modules have to be able to react to threebasic situations: that IUs are added to a buffer,which triggers processing; that IUs that were er-roneously hypothesized by an earlier module arerevoked, which may trigger a revision of a mod-ule?s own output; and that modules signal thatthey commit to an IU, that is, won?t revoke itanymore.Jindigo implements an efficient model forcommunicating these updates.
In this model, IUsare associated with edges in a graph, as shown inTable 1.
The graph may be incrementallyamended without actually removing edges orvertices, even if revision occurs.
At each time-step, a new update message is sent to the con-suming module.
The update message contains apair of pointers [C, A]: (C) the vertex from whichthe currently committed hypothesis can be con-structed, and (A) the vertex from which the cur-rently best tentative hypothesis can be con-structed.
In Jindigo, all modules run as threadswithin a single Java process, and therefore haveaccess to the same memory space.2.2 A typical architectureA typical Jindigo system architecture is shown inFigure 2.
The word buffer from the Recognizermodule is parsed by the Interpreter modulewhich tries to find an optimal sequence of topphrases and their semantic representations.
Thesephrases are then interpreted in light of the currentdialogue context by the Contextualizer moduleand are packaged as Communicative Acts (CAs).As can be seen in Figure 2, the Contextualizeralso self-monitors Concepts from the system asthey are spoken by the Vocalizer, which makes itpossible to contextually interpret user responsesto system utterances.
This also makes it possiblefor the system to know whether an intended ut-terance actually was produced, or if it was inter-rupted.
The current context is sent to the ActionManager, which generates a SpeechPlan that issent to the Vocalizer.
This is described in detailin the next section.Figure 2: A typical Jindigo system architecture.String Right buffer Updatemessaget1: one w1 one w2[w1, w2]t2: one five w1 one w2 five w3[w1, w3]t3: one w1 one w2 five w3[w1, w2]t4: one four five w1 one w2 five w3five w5four w4[w1, w5]t5: [commit] w1 one w2 five w3five w5four w4[w5,w5]Table 1: The right buffer of an ASR module, and up-date messages at different time-steps.Figure 3: Incremental Units at different levels of processing.
Some grounded-in relations are shown with dottedlines.
W=Word, SS=SpeechSegment, SU=SpeechUnit, CA=Communicative Act.InterpreterVADASRAction ManagerVocalizerContextualizerSpeechPlanSpeechSegmentSU SU SU SU SU SUSelfDelayOtherDelayW W W W W W W W WPCASSConceptCAResponseToSSC Phrase ConceptUtterance UtteranceUtteranceSegmentUS USUser SystemSSUserVocalizerSpeechSpeechInterpreterWordContextualizerActionManagerUtteranceSegmentASRSpeechPlanContextPhraseConcept33 Incremental speech generation3.1 Incremental units of speechIn order for user and system utterances to be in-terpreted and produced incrementally, they needto be decomposed into smaller units of process-ing (IUs).
This decomposition is shown in Figure3.
Using a standard voice activity detector(VAD) in the ASR, the user?s speech is chunkedinto Utterance-units.
The Utterance bounda-ries determine when the ASR hypothesis iscommitted.
However, for the system to be able torespond quickly, the end silence threshold ofthese Utterances are typically too long.
Thereforesmaller units of the type UtteranceSegment(US) are detected, using a much shorter silencethreshold of about 50ms.
Such short silencethresholds allow the system to give very fast re-sponses (such as backchannels).
Informationabout US boundaries is sent directly from theASR to the Vocalizer.
As Figure 3 illustrates, thegrounded-in links can be followed to derive thetiming of IUs at different levels of processing.The system output is also modelled using IUsat different processing levels.
The widest-spanning IU on the output side is theSpeechPlan.
The rendering of a SpeechPlanwill result in a sequence of SpeechSegment?s,where each SpeechSegment represents a con-tinuous audio rendering of speech, either as asynthesised string or a pre-recorded audio file.For example, the plan may be to say ?okay, a reddoll, here is a nice doll?, consisting of three seg-ments.
Now, there are two requirements that weneed to meet.
First, the output should be varied:the system should not give exactly the same re-sponse every time to the same request.
But, aswe will see, the output in an incremental systemmust also be flexible, as speech plans are incre-mentally produced and amended.
In order to re-lieve the Action Manager of the burden of vary-ing the output and making time-critical adjust-ments, we model the SpeechPlan as a directedgraph, where each edge is associated with aSpeechSegment, as shown in Figure 4.
Thus, theAction Manager may asynchronously plan (a setof possible) responses, while the Vocalizer se-lects the rendering path in the graph and takescare of time-critical synchronization.
To controlthe rendering, each SpeechSegment has theproperties optional, committing, selfDelayand otherDelay, as described in the next sec-tion.
It must also be possible for an incrementalsystem to interrupt and make self-repairs in themiddle of a SpeechSegment.
Therefore, eachSpeechSegment may also be decomposed into anarray of SpeechUnit?s, where each SpeechUnitcontains pointers to the audio rendering in theSpeechSegment.3.2 Producing and consuming SpeechPlansThe SpeechPlan does not need to be completebefore the system starts to speak.
An example ofthis is shown in Figure 4.
As more words arerecognised by the ASR, the Action Manager mayadd more SpeechSegment?s to the graph.
Thus,the system may start to say ?it costs?
before itknows which object is being talked about.w1 how w2 much w3 is w4 the w5 doll w6ehwells1you can have it forit costslet?s say s3 s640 crownsFigure 4: The right buffer of an ASR (top) and theSpeechPlan that is incrementally produced (bottom).Vertex s1 is associated with w1, s3 with w3, etc.
Op-tional, non-committing SpeechSegment?s are markedwith dashed outline.The SpeechPlan has a pointer calledfinalVertex.
When the Vocalizer reaches thefinalVertex, the SpeechPlan is completelyrealised.
If finalVertex is not set, it means thatthe SpeechPlan is not yet completely con-structed.
The SpeechSegment propertyoptional tells whether the segment needs to berealised or if it could be skipped if thefinalVertex is in sight.
This makes it possibleto insert floor-keeping SpeechSegment?s (suchas ?eh?)
in the graph, which are only realised ifneeded.
The Vocalizer also keeps track of whichSpeechSegment?s it has realised before, so that itcan look ahead in the graph and realise a morevaried output.
Each SpeechSegment may carry asemantic representation of the segment (aConcept).
This is sent by the Vocalizer to theContextualizer as soon as the segment has beenrealised.The SpeechSegment properties selfDelayand otherDelay regulate the timing of the out-put (as illustrated in Figure 3).
They specify thenumber of milliseconds that should pass beforethe Vocalizer starts to play the segment, depend-ing on the previous speaker.
By setting theotherDelay of a segment, the Action Managermay delay the response depending on how cer-tain it is that it is appropriate to speak, for exam-ple by considering pitch and semantic complete-ness.
(See Raux & Eskenazi (2008) for a study4on how such dynamic delays can be derived us-ing machine learning.
)If the user starts to speak (i.e., a newUtteranceSegment is initiated) as the system isspeaking, the Vocalizer pauses (at a SpeechUnitboundary) and waits until it has received a newresponse from the Action Manager.
The ActionManager may then choose to generate a new re-sponse or simply ignore the last input, in whichcase the Vocalizer continues from the point ofinterruption.
This may happen if, for example,the UtteranceSegment was identified as a back-channel, cough, or similar.3.3 Self-repairsAs Figure 3 shows, a SpeechPlan may begrounded in a user CA (i.e., it is a response tothis CA).
If this CA is revoked, or if theSpeechPlan is revised, the Vocalizer may initial-ize a self-repair.
The Vocalizer keeps a list of theSpeechSegment?s it has realised so far.
If theSpeechPlan is revised when it has been partlyrealised, the Vocalizer compares the history withthe new graph and chooses one of the differentrepair strategies shown in Table 2.
In the bestcase, it may smoothly switch to the new planwithout the user noticing it (covert repair).
Incase of a unit repair, the Vocalizer searches for azero-crossing point in the audio segment, close tothe boundary pointed out by the SpeechUnit.covertsegmentrepairyou are right it is blueyou are right they are blueovertsegmentrepairyou are right it is blueyou are wrong it is redsorrycovertunitrepairyou are right it is blueyou are wrong it is redovertunitrepairyou are right it is blueyou are wrong it is redsorryTable 2: Different types of self-repairs.
The shadedboxes show which SpeechUnit?s have been realised,or are about to be realised, at the point of revision.The SpeechSegment property committingtells whether it needs to be repaired if theSpeechPlan is revised.
For example, a filledpause such as ?eh?
is not committing (there is noneed to insert an editing term after it), while arequest or an assertion usually is.
If (parts of) acommitting segment has already been realisedand it cannot be part of the new plan, an overtrepaired is made with the help of an editing term(e.g., ?sorry?).
When comparing the history withthe new graph, the Vocalizer searches the graphand tries to find a path so that it may avoid mak-ing an overt repair.
For example if the graph inFigure 4 is replaced with a corresponding onethat ends with ?60 crowns?, and it has so farpartly realised ?it costs?, it may choose the cor-responding path in the new SpeechPlan, makinga covert repair.4 A Wizard-of-Oz experimentA Wizard-of-Oz experiment was conducted totest the usefulness of the model outlined above.All modules in the system were fully functional,except for the ASR, since not enough data hadbeen collected to build language models.
Thus,instead of using ASR, the users?
speech wastranscribed by a Wizard.
As discussed in section1.1, a common problem is the time it takes forthe Wizard to transcribe incoming utterances,and thus for the system to respond.
Therefore,this is an interesting test-case for our model.
Inorder to let the system respond as soon as theuser finished speaking, even if the Wizard hasn?tcompleted the transcription yet, a VAD is used.The setting is shown in Figure 5 (compare withFigure 2).
The Wizard may start to type as soonas the user starts to speak and may alter whateverhe has typed until the return key is pressed andthe hypothesis is committed.
The word buffer isupdated in exactly the same manner as if it hadbeen the output of an ASR.User VADVocalizerSpeechSpeechInterpreterWordContextualizerActionManagerUtteranceSegmentWizardFigure 5: The system architecture used in the Wizard-of-Oz experiment.For comparison, we also configured a non-incremental version of the same system, wherenothing was sent from the Wizard until he com-5mitted by pressing the return key.
Since we didnot have mature models for the Interpreter either,the Wizard was allowed to adapt the transcrip-tion of the utterances to match the models, whilepreserving the semantic content.4.1 The DEAL domainThe system that was used in the experiment wasa spoken dialogue system for second languagelearners of Swedish under development at KTH,called DEAL (Hjalmarsson et al, 2007).
Thescene of DEAL is set at a flea market where atalking agent is the owner of a shop selling usedgoods.
The student is given a mission to buyitems at the flea market getting the best possibleprice from the shop-keeper.
The shop-keeper cantalk about the properties of goods for sale andnegotiate about the price.
The price can be re-duced if the user points out a flaw of an object,argues that something is too expensive, or offerslower bids.
However, if the user is too persistenthaggling, the agent gets frustrated and closes theshop.
Then the user has failed to complete thetask.For the experiment, DEAL was re-implemented using the Jindigo framework.
Fig-ure 6 shows the GUI that was shown to the user.Figure 6: The user interface in DEAL.
The object onthe table is the one currently in focus.
Example ob-jects are shown on the shelf.
Current game score,money and bought objects are shown on the right.4.2 Speech segments in DEALIn a previous data collection of human-humaninteraction in the DEAL domain (Hjalmarsson,2008) it was noted that about 40% of the speakerturns were initiated with standardized lexical ex-pressions (cue phrases) or filled pauses.
Suchspeech segments commit very little semanticallyto the rest of the utterance and are therefore veryuseful as initiations of utterances, since suchspeech segments can be produced immediatelyafter the user has stopped speaking, allowing theWizard to exploit the additional time to tran-scribe the rest of the utterance.The DEAL corpus was used to create utter-ance initial speech segments for the experiment.The motivation to use speech segments derivedfrom human recordings was to make the systemsound convincing in terms of both lexical choiceand intonation.
In particular, we wanted a reper-toire of different types of filled pauses and feed-back expression such as ?eh?
and ?mm?
in orderto avoid a system that sounds monotone and re-petitive.
First, a number of feedback expressionsuch as ?ja?, ?a?, ?mm?
(Eng: ?yes?
), filledpauses such as ?eh?, ?ehm?
and expressions usedto initiate different domain specific speech acts(for example ?it costs?
and ?let me see?)
wereextracted.
The segments were re-synthesizedusing Expros, a tool for experimentation withprosody in diphone voices (Gustafson & Edlund,2008).
Based on manual transcriptions and soundfiles, Expros automatically extracts pitch, dura-tion and intensity from the human voice and cre-ates a synthetic version using these parameters.In the speech plan, these canned segments weremixed with generated text segments (for examplereferences to objects, prices, etc) that were syn-thesized and generated on-line with the samediphone voice.An example interaction with the incrementalversion of the system is shown in Table 3.
S.11exemplifies a self-correction, where the systemprepares to present another bid, but then realizesthat the user?s bid is too low to even consider.
Avideo (with subtitles) showing an interactionwith one of the users can be seen athttp://www.youtube.com/watch?v=cQQmgItIMvs.S.1 [welcome] [how may I help you]U.2 I want to buy a dollS.3 [eh] [here is] [a doll]U.4 how much is it?S.5 [eh] [it costs] [120 crowns]U.6 that is too expensivehow much is the teddy bear?S.7 [well] [you can have it for] [let?s see][40 crowns]U.8 I can give you 30 crownsS.9 [you could have it for] [37 crowns]U.10 I can give you 10 crownsS.11 [let?s say] [or, I mean] [that is way toolittle]Table 3: An example DEAL dialogue (translated fromSwedish).
Speech segments are marked in brackets.64.3 Experimental setupIn order to compare the incremental and non-incremental versions of the system, we con-ducted an experiment with 10 participants, 4male and 6 female.
The participants were given amission: to buy three items (with certain charac-teristics) in DEAL at the best possible price fromthe shop-keeper.
The participants were furtherinstructed to evaluate two different versions ofthe system, System A and System B. However,they were not informed how the versions dif-fered.
The participants were lead to believe thatthey were interacting with a fully working dia-logue system and were not aware of the Wizard-of-Oz set up.
Each participant interacted with thesystem four times, first two times with each ver-sion of the system, after which a questionnairewas completed.
Then they interacted with thetwo versions again, after which they filled out asecond questionnaire with the same questions.The order of the versions was balanced betweensubjects.The mid-experiment questionnaire was used tocollect the participants?
first opinions of the twoversions and to make them aware of what type ofcharacteristics they should consider when inter-acting with the system the second time.
Whenfilling out the second questionnaire, the partici-pants were asked to base their ratings on theiroverall experience with the two system versions.Thus, the analysis of the results is based on thesecond questionnaire.
In the questionnaires, theywere requested to rate which one of the two ver-sions was most prominent according to 8 differ-ent dimensions: which version they preferred;which was more human-like, polite, efficient, andintelligent; which gave a faster response and bet-ter feedback; and with which version it was eas-ier to know when to speak.
All ratings were doneon a continuous horizontal line with System A onthe left end and System B on the right end.
Thecentre of the line was labelled with ?no differ-ence?.The participants were recorded during their in-teraction with the system, and all messages in thesystem were logged.4.4 ResultsFigure 7 shows the difference in response timebetween the two versions.
As expected, the in-cremental version started to speak more quickly(M=0.58s, SD=1.20) than the non-incrementalversion (M=2.84s, SD=1.17), while producinglonger utterances.
It was harder to anticipatewhether it would take more or less time for theincremental version to finish utterances.
Bothversions received the final input at the sametime.
On the one hand, the incremental versioninitiates utterances with speech segments thatcontain little or no semantic information.
Thus, ifthe system is in the middle of such a segmentwhen receiving the complete input from theWizard, the system may need to complete thissegment before producing the rest of the utter-ance.
Moreover, if an utterance is initiated andthe Wizard alters the input, the incremental ver-sion needs to make a repair which takes addi-tional time.
On the other hand, it may also startto produce speech segments that are semanticallyrelevant, based on the incremental input, whichallows it to finish the utterance more quickly.
Asthe figure shows, it turns out that the averageresponse completion time for the incrementalversion (M=5.02s, SD=1.54) is about 600msfaster than the average for non-incremental ver-sion (M=5.66s, SD=1.50), (t(704)=5.56,p<0.001).0,001,002,003,004,005,006,00start end lengthSecondsincnonFigure 7: The first two column pairs show the averagetime from the end of the user?s utterance to the startof the system?s response, and from the end of theuser?s utterance to the end of the system?s response.The third column pair shows the average total systemutterance length (end minus start).In general, subjects reported that the systemworked very well.
After the first interaction withthe two versions, the participants found it hard topoint out the difference, as they were focused onsolving the task.
The marks on the horizontalcontinuous lines on the questionnaire weremeasured with a ruler based on their distancefrom the midpoint (labelled with ?no difference?
)and normalized to a scale from -1 to 1, each ex-treme representing one system version.
A Wil-coxon Signed Ranks Test was carried out, usingthese rankings as differences.
The results areshown in Table 4.
As the table shows, the twoversions differed significantly in three dimen-sions, all in favour of the incremental version.7Hence, the incremental version was rated asmore polite, more efficient, and better at indicat-ing when to speak.diff z-value p-valuepreferred 0.23 -1.24 0.214human-like 0.15 -0.76 0.445polite 0.40 -2.19 0.028*efficient 0.29 -2.08 0.038*intelligent 0.11 -0.70 0.484faster response 0.26 -1.66 0.097feedback 0.08 -0.84 0.400when to speak 0.35 -2.38 0.017*Table 4: The results from the second questionnaire.All differences are positive, meaning that they are infavour of the incremental version.A well known phenomena in dialogue is thatof entrainment (or adaptation or alignment), thatis, speakers (in both human-human and human-computer dialogue) tend to adapt the conversa-tional behaviour to their interlocutor (e.g., Bell,2003).
In order to examine whether the differentversions affected the user?s behaviour, we ana-lyzed both the user utterance length and user re-sponse time, but found no significant differencesbetween the interactions with the two versions.5 Conclusions & Future workThis paper has presented a first step towards in-cremental speech generation in dialogue systems.The results are promising: when there are delaysin the processing of the dialogue, it is possible toincrementally produce utterances that make theinteraction more efficient and pleasant for theuser.As this is a first step, there are several ways toimprove the model.
First, the edges in theSpeechPlan could have probabilities, to guidethe path planning.
Second, when the user hasfinished speaking, it should (in some cases) bepossible to anticipate how long it will take untilthe processing is completed and thereby choose amore optimal path (by taking the length of theSpeechSegment?s into consideration).
Third, alot of work could be done on the dynamic gen-eration of SpeechSegment?s, considering syntac-tic and pragmatic constraints, although thiswould require a speech synthesizer that was bet-ter at convincingly produce conversationalspeech.The experiment also shows that it is possibleto achieve fast turn-taking and convincing re-sponses in a Wizard-of-Oz setting.
We think thatthis opens up new possibilities for the Wizard-of-Oz paradigm, and thereby for practical develop-ment of dialogue systems in general.6 AcknowledgementsThis research was funded by the Swedish researchcouncil project GENDIAL (VR #2007-6431).ReferencesBell, L. (2003).
Linguistic adaptations in spoken hu-man-computer dialogues.
Empirical studies of userbehavior.
Doctoral dissertation, Department ofSpeech, Music and Hearing, KTH, Stockholm.Dohsaka, K., & Shimazu, A.
(1997).
System architec-ture for spoken utterance production in collaborativedialogue.
In Working Notes of IJCAI 1997 Work-shop on Collaboration, Cooperation and Conflict inDialogue Systems.Fraser, N. M., & Gilbert, G. N. (1991).
Simulatingspeech systems.
Computer Speech and Language,5(1), 81-99.Gustafson, J., & Edlund, J.
(2008).
expros: a toolkitfor exploratory experimentation with prosody incustomized diphone voices.
In Proceedings of Per-ception and Interactive Technologies for Speech-Based Systems (PIT 2008) (pp.
293-296).
Ber-lin/Heidelberg: Springer.Hjalmarsson, A., Wik, P., & Brusk, J.
(2007).
Dealingwith DEAL: a dialogue system for conversationtraining.
In Proceedings of SigDial (pp.
132-135).Antwerp, Belgium.Hjalmarsson, A.
(2008).
Speaking without knowingwhat to say... or when to end.
In Proceedings ofSIGDial 2008.
Columbus, Ohio, USA.Kempen, G., & Hoenkamp, E. (1987).
An incrementalprocedural grammar for sentence formulation.
Cog-nitive Science, 11(2), 201-258.Kilger, A., & Finkler, W. (1995).
Incremental Gen-eration for Real-Time Applications.
Technical Re-port RR-95-11, German Research Center for Artifi-cial Intelligence.Levelt, W. J. M. (1989).
Speaking: From Intention toArticulation.
Cambridge, Mass., USA: MIT Press.Raux, A., & Eskenazi, M. (2008).
Optimizing end-pointing thresholds using dialogue features in aspoken dialogue system.
In Proceedings of SIGdial2008.
Columbus, OH, USA.Schlangen, D., & Skantze, G. (2009).
A general, ab-stract model of incremental dialogue processing.
InProceedings of EACL-09.
Athens, Greece.Skantze, G., & Schlangen, D. (2009).
Incrementaldialogue processing in a micro-domain.
In Proceed-ings of EACL-09.
Athens, Greece.8
