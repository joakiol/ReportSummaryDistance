Proceedings of the ACL-HLT 2011 Student Session, pages 30?35,Portland, OR, USA 19-24 June 2011. c?2011 Association for Computational LinguisticsA Latent Topic Extracting Method based on Events in a Documentand its ApplicationRisa KitajimaOchanomizu Universitykitajima.risa@is.ocha.ac.jpIchiro KobayashiOchanomizu Universitykoba@is.ocha.ac.jpAbstractRecently, several latent topic analysis methodssuch as LSI, pLSI, and LDA have been widelyused for text analysis.
However, those meth-ods basically assign topics to words, but do notaccount for the events in a document.
Withthis background, in this paper, we propose alatent topic extracting method which assignstopics to events.
We also show that our pro-posed method is useful to generate a documentsummary based on a latent topic.1 IntroductionRecently, several latent topic analysis methods suchas Latent Semantic Indexing (LSI) (Deerwesteret al, 1990), Probabilistic LSI (pLSI) (Hofmann,1999), and Latent Dirichlet Allocation (LDA) (Bleiet al, 2003) have been widely used for text analy-sis.
However, those methods basically assign top-ics to words, but do not account for the events in adocument.
Here, we define a unit of informing thecontent of document at the level of sentence as an?Event?
1, and propose a model that treats a docu-ment as a set of Events.
We use LDA as a latenttopic analysis method, and assign topics to Eventsin a document.
To examine our proposed method?sperformance on extracting latent topics from a doc-ument, we compare the accuracy of our method tothat of the conventional methods through a commondocument retrieval task.
Furthermore, as an appli-cation of our method, we apply it to a query-biaseddocument summarization (Tombros and Sanderson,1For the definition of an Event, see Section 3.1998; Okumura and Mochizuki, 2000; Berger andMittal, 2000) to verify that the method is useful forvarious applications.2 Related StudiesSuzuki et al (2010) proposed a flexible latent top-ics inference in which topics are assigned to phrasesin a document.
Matsumoto et al (2005) showedthat the accuracy of document classification will beimproved by introducing a feature dealing with thedependency relationships among words.In case of assigning topics to words, it is likelythat two documents, which have the same word fre-quency in themselves, tend to be estimated as theyhave the same topic probablistic distribution withoutconsidering the dependency relation among words.However, there are many cases where the relation-ship among words is regarded as more importantrather than the frequency of words as the featureidentifying the topics of a document.
For example,in case of classifying opinions to objects in a doc-ument, we have to identify what sort of opinion isassigned to the target objects, therefore, we have tofocus on the relationship among words in a sentence,not only on the frequent words appeared in a docu-ment.
For this reason, we propose a method to as-sign topics to Events instead of words.As for studies on document summarization, thereare various methods, such as the method based onword frequency (Luhn, 1958; Nenkova and Van-derwende, 2005), and the method based on a graph(Radev, 2004; Wan and Yang, 2006).
Moreover,several methods using a latent topic model havebeen proposed (Bing et al, 2005; Arora and Ravin-30dran, 2008; Bhandari et al, 2008; Henning, 2009;Haghighi and Vanderwende, 2009).
In those stud-ies, the methods estimate a topic distribution on eachsentence in the same way as the latent semantic anal-ysis methods normally do that on each document,and generate a summary based on the distribution.We also show that our proposed method is useful forthe document summarization based on extracting la-tent topics from sentences.3 Topic Extraction based on EventsIn this study, since we deal with a document as aset of Events, we extract Events from each docu-ment; define some of the extracted Events as the in-dex terms for the whole objective documents; andthen make an Event-by-document matrix consistingof the frequency of Events to the documents.
A la-tent topic distribution is estimated based on this ma-trix.3.1 Definition of an EventIn this study, we define a pair of words in depen-dent relation which meets the following conditions:(Subject, Predicate) or (Predicate1, Predicate2) , asan Event.
A noun and unknown words correspondto Subject, while a verb, adjective and adjectiveverb correspond to Predicate.
To extract these pairs,we analyze the dependency structure of sentencesin a document by a Japanese dependency structureanalyzer, CaboCha 2.
The reason why we define(Predicete1, Predicate2) as an Event is because werecognized the necessity of such type of an Event byinvestigating the extracted pairs of words and com-paring them with the content of the target documentin preliminary experiments, and could not extractany Event in case of extracting an Event from thesentences without subject.3.2 Making an Event-by-Document MatrixIn making a word-by-document matrix, high-frequent words appeared in any documents, and ex-tremely infrequent words are usually not included inthe matrix.
In our method, high-frequent Events likethe former case were not observed in preliminary ex-periments.
We think the reason for this is because anEvent, a pair of words, can be more meaningful than2http://chasen.org/ taku/software/cabocha/a single word, therefore, an Event is particularly agood feature to express the meaning of a document.Meanwhile, the average number of Events per sen-tence is 4.90, while the average number of words persentence is 8.93.
A lot of infrequent Events were ob-served in the experiments because of the nature of anEvent, i.e., a pair of words.
This means that the sameprocess of making a word-by-document matrix can-not be applied to making an Event-by-document ma-trix because the nature of an Event as a feature ex-pressing a document is different from that of a word.In concrete, if the events, which once appear in doc-uments, would be removed from the candidates tobe a part of a document vector, there might be a casewhere the constructed document vector does not re-flect the content of the original documents.
Consid-ering this, in order to make the constructed docu-ment vector reflect the content of the original doc-uments, we do not remove the Event only itself ex-tracted from a sentence, even though it appears onlyonce in a document.3.3 Estimating a Topic DistributionAfter making an Event-by-document matrix, a la-tent topic distribution of each Event is estimated bymeans of Latent Dirichlet Allocation.
Latent Dirich-let Allocation is a generative probabilistic model thatallows multiple topics to occur in a document, andgets the topic distribution based on the idea thateach topic emerges in a document based on a certainprobability.
Each topic is expressed as a multinomialdistribution of words.In this study, since a topic is assigned to an Event,each topic is expressed as a multinomial distributionof Events.
As a method to estimate a topic distri-bution, while a variational Bayes method (Blei etal., 2003) and its application (Teh et al, 2006) havebeen proposed, in this study we use Gibbs samplingmethod (Grififths and Steyvers, 2004).
Furthermore,we define a sum of topic distributions of the eventsin a query as the topic distribution of the query.4 Performance Evaluation ExperimentThrough a common document retrieval task, wecompare our method with the conventional methodand evaluate both of them.
In concrete, we regardthe documents which have a similar topic distribu-31tion to a query?s topic distribution as the result ofretrieval, and then examine whether or not the esti-mated topic distribution can represent the latent se-mantics of each document based on the accuracy ofretrieval results.
Henceforth, we call the conven-tional word-based LDA ?wordLDA?
and our pro-posed event-based LDA ?eventLDA?.4.1 Measures for Topic DistributionAs measures for identifying the similarity oftopic distribution, we adopt Kullback-Leibler Di-vergence (Kullback and Leibler, 1951), SymmetricKullback-Leibler Divergence (Kullback and Leibler,1951), Jensen-Shannon Divergence (Lin, 2002), andcosine similarity.
As for wordLDA, Henning (2009)has reported that Jensen-Shannon Divergence showsthe best performance among the above measures interms of estimating the similarity between two sen-tences.
We also compare the performance of theabove measures when using eventLDA.4.2 Experimental SettingsAs for the documents used in the experiment, we usea set of data including users?
reviews and their eval-uations for hotels and their facilities, provided byRakuten Travel3.
Each review has five-grade eval-uations of a hotel?s facilities such as room, location,and so on.
Since the data hold the relationships be-tween objects and their evaluations, therefore, it issaid that they are appropriate for the performanceevaluation of our method because the relationship isusually expressed in a pair of words, i.e., an Event.The query we used in the experiment was ?a room isgood?.
The total number of documents is 2000, con-sisting of 1000 documents randomly selected fromthe users?
reviews whose evaluation for ?a room?
is1 (bad) and 1000 documents randomly selected fromthe reviews whose evaluation is 5 (good).
The latter1000 documents are regarded as the objective doc-uments in retrieval.
Because of this experiment de-sign, it is clear that the random choice for retrieving?good?
vs. ?bad?
is 50%.
As for the evaluation mea-sure, we adopt 11-point interpolated average preci-sion.In this experiment, a comparison between theboth methods, i.e., wordLDA and eventLDA, is con-3http://travel.rakuten.co.jp/ducted from the viewpoints of the proper numberof topics and the most useful measure to estimatesimilarity.
At first, we use Jensen-Shannon Diver-gence as the measure to estimate the similarity oftopic distribution, changing the number of topics kin the following, k = 5, k = 10, k = 20, k = 50,k = 100, and k = 200.
Next, the number of topicsis fixed based on the result of the first process, andthen it is decided which measure is the most usefulby applying each measure to estimate the similarityof topic distributions.
Here, the iteration count ofGibbs Sampling is 200.
The number of trials is 20,and all trials are averaged.
The same experiment isconducted for wordLDA to compare both results.4.3 ResultTable 1 shows the retrieval result examined by 11-point interpolated average precision, changing thenumber of topics k. High accuracy is shown at k = 5in eventLDA, and k = 50 in wordLDA, respectively.Overall, we see that eventLDA keeps higher accu-racy than wordLDA.number of topics wordLDA eventLDA5 0.5152 0.625610 0.5473 0.574420 0.5649 0.587450 0.5767 0.5740100 0.5474 0.5783200 0.5392 0.5870Table 1: Result based on the number of topics.Table 2 shows the retrieval result examined by11-point interpolated average precision under vari-ous measures.
The number of topics k is k = 50in wordLDA and k = 5 in eventLDA respectively,based on the above result.
Under any measures,we see that eventLDA keeps higher accuracy thanwordLDA.similarity measure wordLDA eventLDAKullback-Leibler 0.5009 0.5056Symmetric Kullback-Leibler 0.5695 0.6762Jensen-Shannon 0.5753 0.6754cosine 0.5684 0.6859Table 2: Performance under various measures.4.4 DiscussionsThe result of the experiment shows that eventLDAprovides a better performance than wordLDA, there-32fore, we see our method can properly treat the latenttopics of a document.
In addition, as for a prop-erty of eventLDA, we see that it can provide detailclassification with a small number of topics.
As thereason for this, we think that a topic distribution ona feature is narrowed down to some extent by usingan Event as the feature instead of a word, and thenas a result, the possibility of generating error topicsdecreased.On the other hand, a proper measure for ourmethod is identified as cosine similarity, althoughcosine similarity is not a measure to estimate prob-abilistic distribution.
It is unexpected that the mea-sures proper to estimate probabilistic distribution gotthe result of lower performance than cosine similar-ity.
From this, there are some space where we needto examine the characteristics of topic distribution asa probabilistic distribution.5 Application to SummarizationHere, we show multi-document summarization asan application of our proposed method.
We makea query-biased summary, and show the effectivenessof our method by comparing the accuracy of a gener-ated summary by our method with that of summariesby the representative summarization methods oftenused as benchmark methods to compare.5.1 Extracting Sentences by MMR-MDIn extracting important sentences, considering onlysimilarity to a given query, we may generate a redun-dant summary.
To avoid this problem, a measure,MMR-MD (Maximal Marginal Relevance Multi-Document), was proposed (Goldstein et al, 2000).This measure is the one which prevents extractingsimilar sentences by providing penalty score thatcorresponds to similarity between a newly extractedsentence and the previously extracted sentences.
Itis defined by Eq.
1 (Okumura and Nanba, 2005).MMR-MD ?
argmaxCi?R\S [?Sim1(Ci,Q)?(1??
)maxCj?SSim2(Ci,Cj)] (1)We aim to choose sentences whose content is sim-ilar to query?s content based on a latent topic, whilereducing the redundancy of choosing similar sen-tences to the previously chosen sentences.
There-fore, we adopt the similarity of topic distributionsCi ?
sentence in the document setsQ ?
queryR ?
a set of sentences retrieved by Q from the document setsS ?
a set of sentences in R already extracted?
?
weighting parameterfor Sim1 which estimates similarity between a sen-tence and a query, and adopt cosine similarity basedon Events as a feature unit for Sim2 which estimatesthe similarity with the sentences previously chosen.As the measures to estimate topic distribution simi-larity, we use the four measures explained in Section4.1.
Here, as for the weighting parameter ?, we set?
= 0.5.5.2 Experimental SettingsIn the experiment, we use a data set provided at NT-CIR4 (NII Test Collection for IR Systems 4) TSC3(Text Summarization Challenge 3) 4 .The data consists of 30 topic sets of documentsin which each set has about 10 Japanese newspaperarticles, and the total number of the sentences in thedata is 3587.
In order to make evaluation for the re-sult provided by our method easier, we compile a setof questions, provided by the data sets for evaluatingthe result of summarization, as a query, and then useit as a query for query-biased summarization.
As anevaluation method, we adopt precision and coverageused at TSC3 (Hirao et al, 2004), and the numberof extracted sentences is the same as used in TSC3.Precision is an evaluation measure which indicatesthe ratio of the number of correct sentences to thatof the sentences generated by the system.
Coverageis an evaluation measure which indicates the degreeof how the system output is close to the summarygenerated by a human, taking account of the redun-dancy.Moreover, to examine the characteristics of theproposed method, we compare both methods interms of the number of topics and the proper mea-sure to estimate similarity.
The number of trials is20 at each condition.
5 sets of documents selectedat random from 30 sets of documents are used in thetrials, and all the trials are totally averaged.
As atarget for comparison with the proposed method, wealso conduct an experiment using wordLDA.4http://research.nii.ac.jp/ntcir/index-en.html335.3 ResultAs a result, there is no difference among the fourmeasures ?
the same result is obtained by thefour measures.
Table 3 shows comparison betweeneventLDA and wordLDA in terms of precision andcoverage.
The number of topics providing the high-est accuracy is k = 5 for wordLDA, and k = 10 foreventLDA, respectively.number of topics wordLDA eventLDAPrecision Coverage Precision Coverage5 0.314 0.249 0.404 0.32310 0.264 0.211 0.418 0.34020 0.261 0.183 0.413 0.32550 0.253 0.171 0.392 0.319Table 3: Comparison of the number of topics.Furthermore, Table 4 shows comparison betweenthe proposed method and representative summa-rization methods which do not deal with latenttopics.
As representative summarization methodsto compare our method, we took up the Leadmethod (Brandow et al, 1995) which is effectivefor document sumarization of newspapers, and theimportant sentence extraction-based summarizationmethod using TF-IDF.method Precision CoverageLead 0.426 0.212TF-IDF 0.454 0.305wordLDA (k=5) 0.314 0.249eventLDA (k=10) 0.418 0.340Table 4: Comparison of each method.5.4 DiscussionsUnder any condition, eventLDA provides a higheraccuracy than wordLDA.
We see that the proposedmethod is useful for estimating a topic on a sentence.As the reason for that the accuracy does not dependon any kinds of similarity measures, we think thatan estimated topic distribution is biased to a particu-lar topic, therefore, there was not any influence dueto the kinds of similarity measures.
Moreover, theproper number of topics of eventLDA is bigger thanthat of wordLDA.
We consider the reason for thisis because we used newspaper articles as the objec-tive documents, so it can be thought that the top-ics onto the words in the articles were specific tosome extent; in other words, the words often usedin a particular field are often used in newspaper ar-ticles, therefore, we think that wordLDA can clas-sify the documents with the small number of top-ics.
In comparison with the representative methods,the proposed method takes close accuracy to theiraccuracy, therefore, we see that the performance ofour method is at the same level as those representa-tive methods which directly deal with words in doc-uments.
In particular, as for coverage, our methodshows high accuracy.
We think the reason for thisis because a comprehensive summary was made bylatent topics.6 ConclusionIn this paper, we have defined a pair of words withdependency relationship as ?Event?
and proposed alatent topic extracting method in which the contentof a document is comprehended by assigning latenttopics onto Events.
We have examined the abilityof our proposed method in Section 4, and as its ap-plication, we have shown a document summariza-tion using the proposed method in Section 5.
Wehave shown that eventLDA has higher ability thanwordLDA in terms of estimating a topic distribu-tion on even a sentence or a document; furthermore,even in case of assigning a topic on an Event, we seethat latent topics can be properly estimated.
Sincean Event can hold a relationship between a pair ofwords, it can be said that our proposed method, i.e.,eventLDA, can comprehend the content of a docu-ment more deeper and proper than the conventionalmethod, i.e., wordLDA.
Therefore, eventLDA canbe effectively applied to various document data setsrather than wordLDA can be.
We have also shownthat another feature other than a word, i.e., an Eventis also useful to estimate latent topics in a document.As future works, we will conduct experiments withvarious types of data and query, and further investi-gate the characteristic of our proposed method.AcknowledgmentsWe would like to thank Rakuten, Inc. for permissionto use the resources of Rakuten Travel, and thankthe National Institute of Informatics for providingNTCIR data sets.34ReferencesAdam Berger and Vibhu O. Mittal.
2000.
Query-relevantsummarization using FAQs.
In ACL ?00 Proceedingsof the 38th Annual Meeting on Association for Com-putational Linguistics:294?301.Anastasios Tombros and Mark Sanderson.
1998.
Ad-vantages of query biased summaries in information re-trieval.
In Proceedings of the 21st Annual Interna-tional ACM-SIGIR Conference on Research and De-velopment in Information Retrieval:2?10.Ani Nenkova and Lucy Vanderwende.
2005.
The Im-pact of Frequency on Summarization.
Technical re-port, Microsoft Research.Aria Haghighi and Lucy Vanderwende.
2009.
Explor-ing Content Models for Multi-Document Summariza-tion.
In Human Language Technologies: The 2009 An-nual Conference of the North American Chapter of theACL:362?370.David M. Blei, Andrew Y. Ng, and Michael I. Jordan.2003.
Latent Dirichlet Allocation.
Journal of Ma-chine Learning Research,3:993?1022.Dragomir R. Radev.
2004.
Lexrank: graph-based cen-trality as salience in text summarization.
Journal ofArtificial Intelligence Research (JAIR.Harendra Bhandari, Masashi Shimbo, Takahiko Ito, andYuji Matsumoto.
2008.
Generic Text SummarizationUsing Probabilistic Latent Semantic Indexing.
In Pro-ceedings of the 3rd International Joint Conference onNatural Langugage Proceeding:133-140.H.
P. Luhn.
1958.
The automatic creation of literatureabstracts.
IBM Journal of Research and Development.Jade Goldstein, Vibhu Mittal, Jaime Carbonell, andMark Kantrowitz.
2000.
Multi-document sum-marization by sentence extraction.
In Proceedingsof the 2000 NAALP-ANLP Workshop on AutomaticSummarization:40?48.Jianhua Lin.
2002.
Divergence Measures based on theShannon Entropy.
IEEE Transactions on InformationTheory, 37(1):145?151.Leonhard Henning.
2009.
Topic-based Multi-DocumentSummarization with Probabilistic Latent SemanticAnalysis.
Recent Advances in Natural LanguageProcessing:144?149.Manabu Okumura and Eiji Nanba.
2005.
Sci-ence of knowledge: Automatic Text Summarization.
(inJapanese) ohmsha.Manabu Okumura and Hajime Mochizuki.
2000.
Query-Biased Summarization Based on Lexical Chaining.Computational Intelligence,16(4):578?585.Qin Bing, Liu Ting, Zhang Yu, and Li Sheng.
2005.
Re-search on Multi-Document Summarization Based onLatent Semantic Indexing.
Journal of Harbin Instituteof Technology,12(1):91?94.Rachit Arora and Balaraman Ravindran.
2008.
Latentdirichlet alocation based multi-document summariza-tion.
In Proceedings of the 2nd Workshop on Analyticsfor Noisy Unstructured Text Data.Ronald Brandow, Karl Mitze, and Lisa F. Rau.
1995.Automatic condensation of electronic publications bysentence selection.
Information Processing and Man-agement: an International Journal - Special issue:summarizing text,31(5):675?685.Scott Deerwester, Susan T. Dumais, George W. Furnas,Thomas K. Landauer, and Richard Harshman.
1990.Indexing by Latent Semantic Analysis.
Journal of theAmerican Society of Information Science, 41(6):391?407.Shotaro Matsumoto, Hiroya Takamura, and ManabuOkumura.
2005.
Sentiment Classification Us-ing Word Sub-sequences and Dependency Sub-trees.In Proceedings of the 9th Pacific-Asia Interna-tional Conference on Knowledge Discovery and DataMining:301?310.Solomon Kullback and Richard A. Leibler.
1951.
OnInformation and Sufficiency.
Annuals of MathematicalStatistics, 22:49?86.Thomas L. Grififths and Mark Steyvers.
2004.
Find-ing scientific topics.
In Proceedings of the Na-tional Academy of Sciences of the United States ofAmerica,101:5228?5235.Thomas Hofmann.
1999.
Probabilistic Latent Seman-tic Indexing.
In Proceedings of the 22nd Annual In-ternational ACM-SIGIR Conference on Research andDevelopment in Information Retrieval:50?57.Tsutomu Hirao, Takahiro Fukusima, Manabu Okumura,Chikashi Nobata, and Hidetsugu Nanba.
2004.
Cor-pus and evaluation measures for multiple documentsummarization with multiple sources.
In Proceed-ings of the 20th International Conference on Compu-tational Linguistics:535?541.Xiaojun Wan and Jianwu Yang.
2006.
Improved affinitygraph based multi-document summarization.
In Pro-ceedings of the Human Language Technology Confer-ence of the NAACL, Companion Volume: Short PapersYasuhiro Suzuki, Takashi Uemura, Takuya Kida, and Hi-roki Arimura.
2010.
Extension to word phrase on la-tent dirichlet alocation.
Forum on Data Engineeringand Information Management,i-6.Yee W. Teh, David Newman, and Max Welling.
2006.A Collapsed Variational Bayesian Inference Algorithmfor Latent Dirichlet Allocation.
Advances in NeuralInformation Processing Systems Conference,19:1353?1360.35
