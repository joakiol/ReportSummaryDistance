A Web-based Demonstrator of a Multi-lingual Phrase-basedTranslation SystemRoldano Cattoni, Nicola Bertoldi, Mauro Cettolo, Boxing Chen and Marcello FedericoITC-irst - Centro per la Ricerca Scientifica e Tecnologica38050 Povo - Trento, Italy{surname}@itc.itAbstractThis paper describes a multi-lingualphrase-based Statistical Machine Transla-tion system accessible by means of a Webpage.
The user can issue translation re-quests from Arabic, Chinese or Spanishinto English.
The same phrase-based sta-tistical technology is employed to realizethe three supported language-pairs.
Newlanguage-pairs can be easily added to thedemonstrator.
The Web-based interface al-lows the use of the translation system toany computer connected to the Internet.1 IntroductionAt this time, Statistical Machine Translation(SMT) has empirically proven to be the mostcompetitive approach in international competi-tions like the NIST Evaluation Campaigns1 andthe International Workshops on Spoken LanguageTranslation (IWSLT-20042 and IWSLT-20053).In this paper we describe our multi-lingualphrase-based Statistical Machine Translation sys-tem which can be accessed by means of a Webpage.
Section 2 presents the general log-linearframework to SMT and gives an overview ofour phrase-based SMT system.
In section 3the software architecture of the demo is out-lined.
Section 4 focuses on the currently supportedlanguage-pairs: Arabic-to-English, Chinese-to-English and Spanish-to-English.
In section 5 theWeb-based interface of the demo is described.1http://www.nist.gov/speech/tests/mt/2http://www.slt.atr.jp/IWSLT2004/3http://www.is.cs.cmu.edu/iwslt2005/2 SMT System Description2.1 Log-Linear ModelGiven a string f in the source language, the goal ofthe statistical machine translation is to select thestring e in the target language which maximizesthe posterior distribution Pr(e | f).
By introduc-ing the hidden word alignment variable a, the fol-lowing approximate optimization criterion can beapplied for that purpose:e?
= arg maxePr(e | f)= arg maxe?aPr(e,a | f)?
arg maxe,aPr(e,a | f)Exploiting the maximum entropy (Berger etal., 1996) framework, the conditional distribu-tion Pr(e,a | f) can be determined throughsuitable real valued functions (called features)hr(e, f ,a), r = 1 .
.
.
R, and takes the parametricform:p?
(e,a | f) ?
exp{R?r=1?rhr(e, f ,a)}The ITC-irst system (Chen et al, 2005) isbased on a log-linear model which extends theoriginal IBM Model 4 (Brown et al, 1993)to phrases (Koehn et al, 2003; Federico andBertoldi, 2005).
In particular, target strings e arebuilt from sequences of phrases e?1 .
.
.
e?l.
For eachtarget phrase e?
the corresponding source phrasewithin the source string is identified through threerandom quantities: the fertility ?, which estab-lishes its length; the permutation pii, which setsits first position; the tablet f?
, which tells its wordstring.
Notice that target phrases might have fer-tility equal to zero, hence they do not translate any91source word.
Moreover, uncovered source posi-tions are associated to a special target word (null)according to specific fertility and permutation ran-dom variables.The resulting log-linear model applies eight fea-ture functions whose parameters are either esti-mated from data (e.g.
target language models,phrase-based lexicon models) or empirically fixed(e.g.
permutation models).
While feature func-tions exploit statistics extracted from monolingualor word-aligned texts from the training data, thescaling factors ?
of the log-linear model are esti-mated on the development data by applying a min-imum error training procedure (Och, 2004).2.2 Decoding StrategyThe translation of an input string is performed bythe SMT system in two steps.
In the first pass abeam search algorithm (decoder) computes a wordgraph of translation hypotheses.
Hence, eitherthe best translation hypothesis is directly extractedfrom the word graph and output, or an N-best listof translations is computed (Tran et al, 1996).
TheN-best translations are then re-ranked by applyingadditional features and the top ranking translationis finally output.The decoder exploits dynamic programming,that is the optimal solution is computed by expand-ing and recombining previously computed partialtheories.
A theory is described by its state which isthe only information needed for its expansion.
Ex-panded theories sharing the same state are recom-bined, that is only the best scoring one is storedfor further expansions.
In order to output a wordgraph of translations, backpointers to all expandedtheories are mantained, too.To cope with the large number of generated the-ories some approximations are introduced duringthe search: less promising theories are pruned off(beam search) and a new source position is se-lected by limiting the number of vacant positionson the left-hand and the distance from the left mostvacant position (re-ordering constraints).2.3 Phrase extraction and model trainingTraining of the phrase-based translation modelrequires a parallel corpus provided with word-alignments in both directions, i.e.
from sourceto target positions, and viceversa.
This pre-processing step can be accomplished by applyingthe GIZA++ toolkit (Och and Ney, 2003) that pro-vides Viterbi alignments based on IBM Model-4.Starting from the parallel training corpus, pro-vided with direct and inverted alignments, the so-called union alignment (Och and Ney, 2003) iscomputed.Phrase-pairs are extracted from each sentence pairwhich correspond to sub-intervals of the sourceand target positions, J and I , such that the unionalignment links all positions of J into I and allpositions of I into J .
In general, phrases are ex-tracted with maximum length in the source and tar-get defined by the parameters Jmax and Imax.
Allsuch phrase-pairs are efficiently computed by analgorithm with complexity O(lImaxJ2max) (Cet-tolo et al, 2005).Given all phrase-pairs extracted from the train-ing corpus, lexicon probabilities and fertility prob-abilities are estimated.Target language models (LMs) used by the de-coder and rescoring modules are, respectively,estimated from 3-gram and 4-gram statisticsby applying the modified Kneser-Ney smoothingmethod (Goodman and Chen, 1998).
LMs are es-timated with an in-house software toolkit whichalso provides a compact binary representation ofthe LM which is used by the decoder.3 Demo ArchitectureFigure 1 shows the two-layer architecture of thedemo.
At the bottom lie the programs that providethe actual translation services: for each language-pair a wrapper coordinates the activity of a special-ized pre-processing tool and a MT decoder.
Thetranslation programs run on a grid-based clusterof high-end PCs to optimize the processing speed.All the wrappers communicate with the MT front-end whose main task is to forward translation re-quests to the appropriate language-pair wrapperand to report an error in case of wrong requests(e.g.
unsupported language-pair).
It is worthnoticing here that a new language-pair can be eas-ily added to the system with a minimal interven-tion on the code of the MT front-end.At the top of the architecture are the programsthat provide the interface with the user.
This layeris separated from the translation layer (hosted byinternal machines only) by means of a firewall.The user interface is implemented as a Web pagein which a translation request (a source sentenceand a language-pair) is input by means of anHTML form.
The cgi script invocated by the formmanages the interaction with the MT front-end.92Web Page(form)scriptCGIlang 1wrapperprepro?cessingMTdecoderprepro?cessingMTdecoderwrapperlang 2prepro?cessingMTdecoderwrapperlang N...MTfront?endfirewallexternal hostinternal hostsfast machinesFigure 1: Architecture of the demo.
For eachlanguage-pair a set of programs (in particular theMT decoder) provides the translation service.
Therequest issued by the user on the Web page issent by the cgi script to the MT front-end.
Thetranslation is then performed on the appropriatelanguage-pair service and the output sent back tothe Web browser.When a user issues a translation request afterfilling the form fields, the cgi script sends the re-quest to the MT front-end and waits for its reply.The input sentence is then forwarded to the wrap-per of the appropriate language-pair.
After a pre-processing step, the actual translation is performedby the specific MT decoder.
The output in the tar-get language is then sent back to the user?s Webbrowser through the chain in the reverse order.From a technical point of view, the inter-processcommunication is realized by means of standardTCP-IP sockets.
As far as the encoding of texts isconcerned, all the languages are encoded in UTF-8: this allows to manage the processing phase inan uniform way and to render graphically differentcharacter sets.4 The supported language-pairsAlthough there is no theoretical limit to the num-ber of supported language-pairs, the current ver-sion of the demo provides translations to Englishfrom three source languages: Arabic, Chinese andSpanish.
For demonstration purpose, three differ-ent application domains are covered too.Arabic-to-English (Tourism)The Arabic-to-English system has been trainedwith the data provided by the International Work-shop on Spoken Language Translation 2005 Thecontext is that of the Basic Traveling Expres-sion Corpus (BTEC) task (Takezawa et al, 2002).BTEC is a multilingual speech corpus which con-tains sentences coming from phrase books fortourists.
Training set includes 20k sentences con-taining 159K Arabic and 182K English runningwords; vocabulary size is 18K for Arabic, 7K forEnglish.Chinese-to-English (Newswire)The Chinese-to-English system has been trainedwith the data provided by the NIST MT EvaluationCampaign 2005 , large-data condition.
In this caseparallel data are mainly news-wires provided bynews agencies.
Training set includes 71M Chineseand 77M English running words; vocabulary sizeis 157K for Chinese, 214K for English.Spanish-to-English (European Parliament)The Spanish-to-English system has been trainedwith the data provided by the Evaluation Cam-paign 2005 of the European integrated project TC-STAR4.
The context is that of the speeches ofthe European Parliament Plenary sessions (EPPS)from April 1996 to October 2004.
Training set forthe Final Text Edition transcriptions includes 31MSpanish and 30M English running words; vocabu-lary size is 140K for Spanish, 94K for English.5 The Web-based InterfaceFigure 2 shows a snapshot of the Web-based in-terface of the demo ?
the URL has been removedto make this submission anonymous.
In the upperpart of the page the user provides the two informa-tion required for the translation: the source sen-tence can be input in a 80x5 textarea html struc-ture, while the language-pair can be selected bymeans of a set a radio-buttons.
The user can resetthe input area or send the translation request bymeans of standard reset and submit buttons.
Someexamples of bilingual sentences are provided inthe lower part of the page.4http://www.tc-star.org93Figure 2: A snapshot of the Web-based interface.The user provides the sentence to be translatedin the desired language-pair.
Some examples ofbilingual sentences are also available to the user.The output of a translation request is simple: therequested source sentence, the translation in thetarget language and the selected language-pair arepresented to the user.
Figure 3 shows an exampleof an Arabic sentence translated into English.We plan to extend the interface with the pos-sibility for the user to ask additional informationabout the translation ?
e.g.
the number of exploredtheories or the score of the first-best translation.6 AcknowledgementsThis work has been funded by the European Unionunder the integrated project TC-STAR - Technol-ogy and Corpora for Speech to Speech Translation- (IST-2002-FP6-506738, http://www.tc-star.org).ReferencesA.L.
Berger, S.A. Della Pietra, and V.J.
Della Pietra.1996.
A Maximum Entropy Approach to NaturalLanguage Processing.
Computational Linguistics,22(1):39?71.P.
F. Brown, S. A. Della Pietra, V. J. Della Pietra, andR.
L. Mercer.
1993.
The Mathematics of StatisticalMachine Translation: Parameter Estimation.
Com-putational Linguistics, 19(2):263?313.Figure 3: Example of an Arabic sentence trans-lated into English.Mauro Cettolo, Marcello Federico, Nicola Bertoldi,Roldano Cattoni, and Boxing Chen.
2005.
A lookinside the itc-irst smt system.
In Proceedings of the10th Machine Translation Summit, pages 451?457,Phuket, Thailand, September.B.
Chen, R. Cattoni, N. Bertoldi, M. Cettolo, andM.
Federico.
2005.
The ITC-irst SMT System forIWSLT-2005.
In Proceedings of the IWSLT 2005,Pittsburgh, USA.M.
Federico and N. Bertoldi.
2005.
A Word-to-PhraseStatistical Translation Model.
ACM Transactions onSpeech and Language Processing.
to appear.J.
Goodman and S. Chen.
1998.
An empirical study ofsmoothing techniques for language modeling.
Tech-nical Report TR-10-98, Harvard University, August.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proceedings of HLT-NAACl 2003, pages 127?133, Edmonton, Canada.F.
J. Och and H. Ney.
2003.
A systematic comparisonof various statistical alignment models.
Computa-tional Linguistics, 29(1):19?51.F.J.
Och.
2004.
Minimum Error Rate Training inStatistical Machine Translation.
In Proceedings ofACL, Sapporo, Japan.T.
Takezawa, E. Sumita, F. Sugaya, H. Yamamoto, andS.
Yamamoto.
2002.
Toward a Broad-CoverageBilingual Corpus for Speech Translation of TravelConversations in the Real World.
In Proceedings of3rd LREC, pages 147?152, Las Palmas, Spain.B.
H. Tran, F. Seide, and V. Steinbiss.
1996.
A WordGraph based N-Best Search in Continuous SpeechRecognition.
In Proceedings of ICLSP, Philadel-phia, PA, USA.94
