A Computat ional  Account o f  SomeConstraints on LanguageMitchell MarcusMIT Artificial intelligence LaboratoryIn a series of papers over the last several years,Noam Chomsky has argued for several specific properties oflanguage which he claims are universal to all humanlanguages \[Chomsky 78, 76, 76\].
These properties, whichform one of the cornerstones of his current linguistic theory,are embodied in a set of constraints on language, a set  ofrestr ict ions on the operation of rules of grammar.This paper will outline two arguments presented atlength in \[Marcus 77\] demonstrating that important sub-cases of two of these constraints, the Subjacency Principleand the Specified Subject Constraint, fall out naturally fromthe structure "of a grammar interpreter called PARSIFAL,whose structure is in turn based upon the hypothesis that anatural language parser needn't simulate a nondeterministicmachine.
This "Determinism Hypothesis" claims that naturallanguage can be parsed by a computationally simplemechanism that uses neither backtracking nor pseudo-parallelism, and in which all grammatical structure createdby the parser is "indelible" in that it must all be output aspart of the structural analysis of the parser's input.
Oncebuilt, no grammatical structure can be discarded or alteredin the course of the parsing process.In particular, this paper will show that thestructure of the grammar interpreter constrains its operationin such a way that, by and large, grammar rules cannotparse sentences which violate either .the Specified SubjectConstraint or the Subjacency.
Principle.
The component ofthe grammar interpreter upon which this result principallydepends is motivated by the Determinism Hypothesis; thisresult thus provides indirect evidence for the hypothesis.This result also depends upon the use within acompLitational framework of the closely related notions ofannotated surface structure and trace theory, which alsoderive from Chomsky's recent work.
(It should be noted that these constraints are farfrom universally accepted.
They are currently the source ofmuch controversy; for various critiques of Chomsky'sposition see \[Postal 74; Bresnan 76\].
However, what ispresented below does not argue for these constraints, perse, but rather provides a different sort of explanation,based on a processing model, of why the sorts of sentenceswhich these constraints forbid are bad.
While the exactformulation of these constraints is controversial, the factthat some set of constraints is needed to account for thisrange of data is generally agreed upon by most generative(jrammarians.
The account which I will present below iscrucially linked to Chomsky's, however, in that trace theoryis at the heart of this account.
)Because of space limitations, this paper deals onlywith those grammatical processes characterized by thecompetence rule "MOVE NP"; the constraints imposed bythe grammar interl)reter upon those processescharacter ized by the rule "MOVE WH-phrase" are discussedat length in \[Marcus 77\] where I show that the behaviorcharacter ized by Ross's Complex NP Constraint \[Ross 67 \ ]itself  follows directly from the structure of the grammarinterpreter  for rather different reasons than the behaviorconsidered in this section.
Also because of spacelimitations, I will not attempt to show that the twoconstraints I will deal with here necessari ly follow from thegrammar interpreter, but rather only .that they natural lyfollow from the interpreter, in particular from a simple,natural formulation of a rule for passivization which itselfdepends heavily upon the structure of the interpreter.Again, necessity is argued for in detail in \[Marcus 77\].This pal)er will first outline the structure of thegrammar interpreter, then present the PASSIVE rule, andtho.n finally show how Chomsky% constraints "fall out" ofthe formulation of PASSIVE.Before proceeding with the body of this paper, twoother important properties of the parser should bementioned which will not be discussed here.
Both arediscussed at lengtll in \[Marcus 77\]; the first is sketchedas well in \[Marcus 781:1) Simple rules of grammar can by written for thisinterpreter which elegantly capture the signif icantgeneralizations behind not only passivization, but also suchconstructions as yes/no questions, imperatives, andsentences with existential there.
These rules arereminiscent of the sorts of rules proposed within theframework of the theory of generative grammar, despite thefact  that the rules presented here must recover underlyingstructure given only the terminal string of the surface formof the sentence.2)The grammar interpreter provides a simplee?planation for the difficulty caused by "garden path"sentences, such as "The cotton clothing is made of grows inMississippi."
Rules can be written for this interpreter to236reso lve local structural ambiguities which might seem torequire nondeterministic parsing; the power of such rules,however ,  depends upon a parameter of the mechanism.Most structural  ambiguities can be resolved, given anappropr iate sett ing of this parameter, but those whichtyp ica l ly  cause garden paths cannot.The Structure  o f  PARSIFALPARSIFAL maintains two major data structures: apushdown stack of incomplete donstituents called the activenode stack, and a small three-place constituent buffer whichcontains const ituents which are complete, but whose higherlevel  grammatical function is as yet  uncertain.Figure 1 below shows a snapshot of the parser 'sdata structures taken while parsing the sentence "Johnshould have scheduled -the meeting.".
Note that the act ivenode stack in shown growing downward, so that thestructure of the stack ref lects the structure of theemerging parse tree.
At the bottom of the stack is anauxi l iary node labelled with the features modal, past, etc.,which has as a daughter the modal "should".
Above thebottom of the stack is an S node with an NP as a daughter,dominating the word "John".
There are two words in t imbuffer,  the verb "have" in the first buffer cell and the word"scheduled"  in the second.
The two words "the meeting"have not yet  come to the attention of the parser.
(Thestructures of form "(PARSE-AUX CPOOL)" and the like will beexpla ined below.
)The Active Node Stack$1 (S DECL MAJOR S) / (PARSE-AUX CPOOL)NP : (John)AUX1 (MODAL PAST VSPL AUX) / (BUILD-AUX)MODAL : (should)1 ;2:The BufferWORD8 (~HAVE VERB TNSLESS AUXVERB PRESV-eS) : (have)WORD4 ("SCHEDULE COMP-OBJ VERB INF-OBJV-eS ED=EN EN PART PAST ED) : (scheduled)Yet unseen words: the meeting.Figure 1 - PARSIFAL's two major data structures.The constituent bt,ffer is the heart of the grammarinterpreter ;  it is the central feature that distinguishes thisparser from all others.
The words that make up the parser 'sinput first come to its attention when they appear at theend of this buffer after morphological analysis.
Triggeredby the words at the beginning of the buffer, the parser maydecide to create  a new grammatical constituent, create  anew node at the bottom of the act ive node stack, and thenbegin to attach the constituents in the buffer to it.
Afterthis new const ituent is completed, the parser will then popthe f~ew constituent from the act ive node stack; if thegrammatical role of this larger structure is as yetundetermined, the parser will insert it into the first cell ofthe buffer.
The parser is free to examine the const i tuentsin the buffer, to act upon them, and to otherwise use thebuffer  as a workspace.While the buffer allows the parser to examinesome of the context  surrounding a given constituent, it doesnot al low arbitrary Iook-allead.
The length of the buffer  iss t r ic t ly  limited; in the version of the parser presented here,the buffer  has only three cells.
(The buffer must beextended to f ive cells to allow the parser to build NPs in anlanner which is transparent to the "clause level"  grammarrules which will be presented in this paper.
This extendedparser still has a window of only three cells, but theef fec t ive  start  of the buffer can be changed through an"at tent ion  ~hifting mechanism" whenever the parser isbuilding an NP.
In  ef fect ,  this extended parser  has two" logical" buffers of length three, one for NPs and anotherfor clauses, witl l  these two buffers implemented by allowingan over lap in one larger buffer.
For details, see \[Marcus77\].
)Note t l lat  each of the three cells in the buffer canhold a grammatical consHtuent of any type, .where aconst i tuent  is any tree that the parser has constructedunder a single root node.
The size of the structureunderneath the node is immaterial; both "that"  and " thatthe big green cookie monster's toe got stubbed" areper fec t ly  good constituents once the parser hasconstructed a subordinate clause from the latter phrase,The constituent buffer and the act ive node stackare acted upon by a grammar which is made up ofpat tern /ac t ion  rules; this grammar can be v iewed as anaugmented form o f  Newell and Simon's production systems\ [Newel l  & Simon 72\].
Each rule is made up of a pattern,which is matched against some subset of the const i tuentsof the buffer and the accessible nodes in the act ive nodestack (about which more will be said below), and an action,a sequence of operations which acts on these constituents.Each rule is assigned a numerical priority, which thegrammar interpreter  uses to arbitrate simultaneous matches.The grammar as a whole is structured into rulepackets, clumps of grammar rules which can be act ivatedand deac( ivated  as a group; the grammar interpreter  onlyat tempts  to match rules in packets that have beenact ivated  by the grammar.
Any grammar rule can act ivate  apacket  by associating that packet with the const ituent atthe bottom of the act ive node stack.
As long as that nodeis at the bottom of the stack, the packets associated withit are act ive;  when that node is pushed into the stack, thepackets  remain associated with it, but become act ive againonly when that node reaches the bottom of the stack.
Forexample,  in figure 1 above, the packet BUILD-AUX isassoc iated with the bottom of the stack, and is thus act ive,while the packet  PARSE-AUX is associated with the S nodeabove the auxil iary.The grammar rules themselves are wr i t ten in alanguage called PIDGIN, an English-like formal language thatis t ranslated into LISP by a simple grammar translator basedon the notion of top-down operator precedence \ [Pratt  73\] .This use of pseudo-English is similar to ti le use of pseudo-English in the grammar for Sager's STRING parser \ [Sager73.\].
Figure 2 below gives a schematic overv iew of theorganization of the grammar, and exhibits some of the rulesthat  make up the packet PARSE-AUX.A few comments on the grammar notation itself  are237in order.
The general form of each grammar rule is:{Rule (name)  priority: <priority) in (packet )<pattern)  --> <action)}Each pattern is of tl~e form :\ [ (descr ipt ion of 1st buffer const i tuent) \ ]  \ [<2nd) \ ]\ [<3rd>\]The symbol "=", used only in pattern descriptions, is to beread as "has the feature(s)" .
Features of the form"*<word>" mean "has time root <word)",  e.g.
"*have" means"has the root "have"" .
The tokens "1st" ,  "2nd",  "3rd"  and"C" (or "c" )  refer to the constituents in the 1st, 2nd, and3rd buffer  positions and the current act ive node (i.e.
thebottom of the stack), respectively.
The PIDGIN code of therule pat terns  should otherwise be fairly se l f -exp lanatory .Priority1st5: \[ \]10: \[ \]10: \[ \]10: \[15: (Pattern ActionDescription of:2nd 3rd The StackPACKET1\[ \] \[ \] --> ACTION1\[ \] - - )  ACTION2\[ \] \[ \] \[ \] - - )  ACTION8PACKET2\[ \] --> ACTION4\[ \] - - )  ACTION5(a) - Time structure of time grammar.
{RULE START-AUX PRIORITY: 10.
IN PARSE-AUX\ [=verb \ ]  -->Create a new aux node.Label C with the meet of the features of 1st and pres,past, future, tnsless.Act ivate  bui ld-aux.
}{RULE TO-INFINITIVE PRIORITY: 10.
IN PARSE-AUX\[=~to, auxverb \ ]  \ [=tnsless\]  - - )Label a new aux node inf.Attach 1st to C as to.Act ivate  bui ld-aux.
}(b) - Some grammar rules that initiate auxil iaries.Figure 2Time parser (i.e.
time grammar interpreterinterpret ing some grammar) operates by attachingconst i tuents  which are in the buffer to the const i tuent atthe bottom of the stack; functionally, a constituent is in thes tack  when the parser is attempting to find its daughters,and in the buffer when the parser is attempting to find itsmother.
Once a constituent in the buffer has beenat tached,  the grammar interpreter will automatical ly removeit from the buffer, filling in the'gap by shifting to the le f t  theconst i tuents  formerly to its right.
When the parser hascompleted the constituent at the bottom of the stack, itpops that  constituent from the act ive node stack;  theconst i tuent  either remains attached to its parent, if it wasa t tached to some larger constituent When it was created,  orelse it falls into the first cell of the constituent buffer,shift ing time buffer to time riglmt to create a gap (and causingaim error if the buffer was already full).
If the const i tuentsin the buffer  provide suff icient evidence that a const i tuentof a given type  should be initiated, a new node of that  typecan be created and pushed onto time stack; this new nodecan also be attached to the node at the bottom of thes tack before the stack is pushed, if time grammaticalfunction of the new constituent is clear when it is created.This structure is motivated by several  prOpertieswhich, as is argued in \[Marcus 77\],  any "non-nondeterminist ic"  grammar interpreter must embody.
Theseprinciples, and their embodiment in PARSIFAL, are as fol lows:1 ) A deterministic parser must be at least partially datadriven.
A grammar for PARSIFAL is made up ofpattern/act iOn rules which are tr iggered whenconst ituents which fulfill specif ic descript ionsappear in the buffer.2) A deterministic parser must be able to reflectexpectations that follow from the partial structuresbuilt up during the parsing process.
Packets  ofrules can be act ivated and deact ivated  bygrammar rules to ref lect  the properties of theconst i tuents in the act ive node stack.3) A deterministic parser must have some sort ofconstrained look-ahead facility.
PARSIFAL's bufferprovides this constrained look-ahead.
Because thebuffer can hold several constituents, a grammarrule can examine the context  that follows the f irstconst i tuent in the buffer before deciding whatgrammatical role it fills in a higher level structure.The key idea is that the size of the buffer can besharply constrained if each location in the buffercan hold a single complete constituent, regardlessof that  constituent's size.
It must be stressed thatthis look-ahead ability must be constrained in somemanner, as it is here by l imiting the length of thebuffer~ otherwise the "determinism" claim isVACUOUS.The General  Gran~matical F ramework  - TracesThe form of the structures that time currentgrammar builds is based on the notion of Annotated SurfaceStructure.
This term has been used in two di f ferent  sensesby Winograd I-Winograd 71\]  and CImomsky \[Cl lomsky 7,3\];the usage of time term here can be thought of as asynthesis  of the two concepts.
Following Winograd, tlmisterm will be used to refer to a notion of surface structureannotated by the addition of a set of features to each nodein a parse tree.
Following Chomsky, time term will be used torefer  to a notion of surface structure annotated by theaddit ion of aim element called trace to indicate the"underly ing position" of "shifted" NPs.In current linguistic theory, a trace is essent ia l ly  a"phonological ly null" NP in the surface structurerepresentat ion of a sentence that has 11o daughters but is"bound" to the NP that filled that position at some level  ofunderlying structure.
In a sense, a trace can be v iewed asa "dummy" NP that serves as a placeholder for the NP thatear l ier  fi l led that  position; in the same sense, the trace's238binding can be v iewed as simply a pointer to that  NP.
Itshould be stressed at the outset, however, that a trace isindistinguishable from a normal NP in terms of normalgrammatical processes; a trace is an NP, even though it isan NP that dominates no lexical material.There are several  reasons for choosing a properlyannotated surface structure as a primary outputrepresentat ion for syntact ic analysis.
While a deeperanalysis is needed to recover the predicate/argumentstructure of a sentence (either in terms of Fillmore caserelations \[Fillmore 68\ ]  or Gruber/Jackendoff "thematicre lat ions" \[Gruber 65; Jackendoff 72\]),  phenomena such asfocus, theme, pronominal reference, scope of quantif ication,and the like can be recovered only from the surfacestructure of a sentence.
By means of proper annotation, itis possible to encode in the surface structure the "deep"syntactic i.formation necessary to recover underlyingpredicate/argument relations, and tht,s to encode in thesame formalism both deep syntactic relations and thesurface order needed for pronominal reference and theother  phenomena listed above.Some examples of the use of trace are given inFigure 3 immediately below.
( la )  lJhat d id  John g ive  to Sue?
( lb}  IJhat d id  John g ive  t to Sue?I I( l c )  John gave what to Sue.
(2a) The meet ing Has scheduled fo r  Wednesdag.
(2b} The meet ing  uas scheduled t fo r  Wednesdag.I I(2c} V scheduled ameeting fo r  Wednesdag.
(3a) John gas be l ieved  to be happg.
(3b}John gas be l ieved  \[S t to be happg\ ] .I IF igure  3 - Some examples of  the use of  t race .One use of trace is to indicate the underlyingposition of the wh-head of a question or relat ive clause.Thus, the structure built by the parser for ,3.1a wouldinclude the trace shown in 8.1b, with ti le t race's  bindingshown by the line under the sentence.
The position of thet race indicates that ,3.1a has an underlying structureanaloqous to the overt  surface structure of 3.1c.Another use of trace is to indicate the underlyingposition of the surface subject of a passivized clause.
Forexample,  8.2a will be parsed into a structure that includes at race as shown as 8.2b; this trace indicates that  thesuh ject  of the passive has the underlying position shown in3.2e.
The symbol "V" signifies the fact that the sub jectposition of (2c) is filled by an NP that dominates no lexicalstructure.
(Following Chomsky, I assume that a passivesentence  in fact has no underlying subject, that  anagent ive  "by NP" prepositional phrase originates as such inunderlying strl ,cture.)
The trace in (,3b) indicates that  thephrase "to be happy", which the brackets show is real ly anembedded clause, has an underlying subject which isidentical  wit l l  the sorface subject of the matrix S, theclause that  dominates the embedded complement.
Notethat  what  is conceptual ly tile underlying subject  of theembedded clause has been passivized into subject  positionof the matr ix S, a phenomenon commonly called "raising".The analysis of tiffs phenomenon assumed here derives from\[Chomsky 73\] ;  it is an alternative to the classic analysiswhich involves "raising" the subject of the embeddedclause into object  position of the matrix S beforepassivizat ion (for details of this later analysis see \[Postal74\]) .The Passive RuleIn this section and the next,  I will brief ly sketch asolution to the phenomena of passivization and "raising" inthe context  of a grammar for PARSIFAl_.
This sect ion willpresent  the Passive rule; the next  section will show howthis rule, without alteration, handles the "raising" cases.Let us begin with the parser in the state shown inf igure 4 below, in the midst of parsing 0.2a above.
Theanalysis process for the sentence prior to this point isessent ia l ly  parallel to the analysis of any simple dec larat ivewith one exception= the rvle PASSIVE-AUX in packet  BUILD-AUX has decoded the passive morphology in the auxi l iaryand given the auxil iary the feature passive (although thisfeature  is not visible in figure 4).
At the point we begin ourexample,  t i le packet  SUBJ-VERB is active.C:The Active Node Stack ( 1. deep)$21 (S DECL MAJOR) / (SS-FINAL)NP : (The meeting)AUX : (was)VP:?VPI 7 (VP) / (SUBJ-VERB)VERB : (scheduled)2 :The BufferPP14 (PP) : (for Wednesday)WORD162 (".
FINALPUNC PUNC) : (.
)Figure 4 - Partial analysis of a passive sentence:after  the verb has been attached.Tile packet  SUBJ-VERB contains, among other rules, the rulePASSIVE, shown in figure 5 below.
The pattern of this ruleis fulf i l led if the auxil iary of the S node dominating thecurrent  act ive node (which will always be a VP node ifpacket  SUBJ-VERB is act ive) has the feature passive, andthe S node has not yet  been labelled np-preposed.
(Thenotat ion ""~ C" indicates that this rule matches against thetwo accessible nodes in the stack, not against the contentsof the buffer.)
The action of the rule PASSIVE simplyc reates  a trace, sets the binding of the trace to thesub jec t  of the dominating S node, and then drops the newt race into the buffer.239{RULE PASSIVE IN SUBJ-VERB\[~" c; the aux of the s above c is passive;the s above c is not np-preposed\] -->Label the s above c np-preposed.Create a new np node labelled trace.Set  the binding of c to the np of the s above c.Drop c.}Figure 5 - Six lines of code captures np-preposing.The state  of the parser after this rule has been executed ,with the parser previously in the state in figure 4 above,  isshown in figure 6 below.
$21 is now labelled with thefeaturenp-preposed, and there is a trace, NP53, in the f irstbuf fer  position.
NP53, as a trace, has no daughters, but isbound to the subject of $21.C:The Active Node Stack ( 1. deep)$21 (NP-PREPOSED S DECL MAJOR) / (SS-FINAL)NP : (The meeting)AU?
: (was)VP:~VP17 (VP) / (SUBJ-VERB)VERB : (scheduled)1 :2:3:The BufferNP53 (NP TRACE) : bound to: (The meeting)PP14 (PP) : (for Wednesday)WORD162 (*.
FtNALPUNC PUNC) : (.
)Figure 6 - After PASSIVE has been executed.Now rules will rut\] which will act ivate  the 'twopackets  SS-VP and INF-COMP, given that the verb of VP17is "schedule".
These two packets contain rules for parsingsimple ob jects  of non-embedded Ss, and inf init ivecomplements, respect ively.
Two such rules, each of whichutil ize an NP immediately following a verb, are given in f igure7 helow.
The rule OBJECTS, in packet SS-VP, picks up anNP af ter  the verb and attaches it to the VP node an asimple object .
The rule INF-S-STARTI,in packet INF-COMP,tr iggers when an NP is followed by "to" and a tenselessverb;  it init iates an infinitive complement and attaches theNP as its subject.
(An example of such a sentence is "Wewanted  John to give a seminar next  week". )
The rule INF-S-START1 must have a higher priority than OBJECTSbecause tl ie pattern of OBJECTS is fulfilled by any situationthat  fulfills the pattern of INF-S-START1; if both rules are inac t ive  packets  and match, the higher priority of INF-S-START1 will cause it to be run instead of OBJECTS.
{RULE OBJECTS PRIORITY: 10 IN SS-VP\ [=np\]  -->Attach 1st to c as np.
}{RULE INF-S-START1 PRIORITY: 5.
IN INF-COMP\ [=np\ ]  [ :~to ,auxverb \ ]  \ [ : tns less \ ]  -->Label a new s node sac, inf-s.Attach 1st to c as np.Act ivate  parse-aux.
}Figure 7 - Two rules which utilize an NP following a verb.While there is not space to continue the examplehere in detail, l lote that the rule OBJECTS will tr igger withthe parser in the state shown in figure 6 above, and willa t tach NP53 as the object  of the verb "schedule.
OBJECTSis thus total ly indifferent both to the fact  that NP53 wasnot a regular NP, but rather a trace, and the fact that  NP53did not originate in the input string, but was placed into thebuffer  by grammatical processes.
Whether or not this ruleis executed  is absolutely unaffected by d i f ferencesbetween an act ive sentence and its passive form; theanalysis process for either is identical as of this point in theparsing process.
Thus, the an'alysis process will be exact lyparal lel  in both cases after the PASSIVE rule has beenexecuted .
(I remind the reader that the analysis of passiveassumed above, following Chomsky, does not assume aprocess of "agent deletion", "subject  postposing" or thelike.
)Passives in Embedded Complements - "Ra is ing"The reader =nay have wondered why PASSIVEdrops the trace it creates into the buffer rather thanimmediately attaching the new trace to the VP node.
As wewill nee below, such a formulation of PASSIVE also correct lyanalyzes passives like 3.3a above which involve "raising",but with no additional complexity added to the grammar,correct ly  capturing an important generalization aboutEnglish.
To show the range of the generalization, theexample which we will invest igate in this section, sentence( t )  in f ig .
re  8 below, is yet  a level more complex than ,3.3aabove;  its analysis is shown schematically in 8.2.
In thisexample  there are two traces: the first, the subject  of theembedded clause, is bound to the subject of the majorclause, the second, t i le object  of the embedded S, is boundto the first trace, and is thus ultimately bound to thesub ject  of the higher S as well.
Thus the underlyingposition of ti le NP "the meeting" can be v iewed as beingthe ob ject  position of the embedded S, as shown in 8.3.
( 1 )The meeting was bel ieved to have been scheduled forWednesday.
(2)The meeting was bel ieved \[s t to have been scheduledt for Wednesday\](3) v bel ieved \[s v to have scheduled the meeting forWednesday\] .Figure 8 - This example shows simple passive and raising.We begin our example, once again, right a f ter"be l ieved"  has been attached to VP20, the current act ivenode, as shown in figure 9 below.
Note that the AUX nodehas been labelled passive, although this feature is notshown here.240C:The Active Node Stack ( 1. deep)$22 (S DECL MAJOR) / (SS-FINAL)NP : (The meeting)AUX : (was)VP :$VP20 (VP) / (SUBJ-VERB)VERB : (bel ieved)1 :2:The BufferWORD166 (~TO PREP AUXVERB) : (to)WORD107 ("HAVE VERB TNSLESS AUXVERBPRES ...) : (have)Figure 9 - After the verb has been attached.The packet  SUB J-VERB is now act ive; the PASSIVErule, contained in this packet now matches and is executed .This rule, as s tated above, creates a trace, binds it to thesub ject  of the current clause, and drops ti le trace into thefirst cell in the buffer.
The resulting state is shown inf igure 10 below.C:The Active Node Stack ( 1. deep)$22 (NP-PREPOSED S DECL MAJOR) / (SS-FINAL)NP : (The meeting)AUX : (was)VP :$VP20 (VP) / (SUBJ-VERB)VERB : (bel ieved)1 :2:3 :The BufferNP55 (NP TRACE) : bound to: (The meeting)WORD166 (~TO PREP AUXVERB) : (to)WORD167 (~HAVE VERB TNSLESS AUXVERBPRES ...) : (have)Yet unseen words: been scheduled for Wednesday.Figure 10 - After PASSIVE has been executed.Again, rules will now be executed which willac t ivate  the packet SS-VP (which contains the ruleOBJECTS) and, since "bel ieve" takes infinitive complements,the packet  INF-COMP (which contains INF-S-START1),among others.
(These rules will also deact ivate  the packetSUI',J-VERB.)
Now the patterns of OBJECTS and INF-S-START1 will botll match, and INF-S-START1, shown above inf igure 7, will be executed by the interpreter since it hast i le higher priority.
(Note once again that a trace is aper fec t ly  normal NP from the point v iew of t i le patternmatching process.)
This rule now creates a new S nodelabel led infinitive and attaches the trace NP,.55 to the newinf init ive as its subject.
The resulting state is shown inf igure 11 below.C:Tile Active Node Stack ( 2. deep)$22 (NP-PREPOSED S DECL MAJOR) / (SS-FINAL)NP : (The meeting)AUX : (was)VP :$VP20 (VP) / (SS-VP THAT-COMP INF-COMP)VERB : (bel ieved)$2,3 (SEC INF-S S) / (PARSE-AUX)NP : bound to: (The meeting)1 :2:Tile BufferWORD166 (*TO PREP AUXVERB) : (to)WORD167 (*HAVE VERB TNSLESS AUXVERBPRES ...) : (have)Yet unseen words: been scheduled for Wednesday.Figure 11 o After INF-S-START1 has been executed.We are now well on our way to the desiredanalysis.
An embedded infinitive has been initiated, and at race bound to the subject of the dominating S has beena t tached as its subject, although no rule has expl ic i t ly" lowered"  the trace from one clause into the other.The parser will now proceed exact ly  as in theprevious example.
It will build tile auxiliary, attach it, andat tach  t i le verb "scheduled" to a new VP node.
Once againPASSIVE will match and be executed,  creating a trace,binding it to the subject of the clause (in this case itself  at race) ,  and dropping the new trace into the buffer.
Againthe rule OBJECTS will attach the trace NP67 as the ob jectof VP21, and the parse will then be completed bygrammatical processes which will not be discussed here.
Aned i t ted  form of the tree structure which results is shown inf igure 12 below.
A trace is indicated in this tree by givingthe terminal string of its ultimate binding in parentheses.
(NP-PREPOSED S DECL MAJOR)NP: (MODIBLE NP DEF DET NP)The meetingAUX: (PASSIVE PAST V18S AUX)WaSVP: (VP)VERB: bel ievedNP: (NP COMP)S: (NP-PREPOSED sEC INF-S S)NP: (NP TRACE) (bound* to: The meeting)AUX: (PASSIVE PERF INF AUX)to have beenVP: (VP)VERB: scheduledNP: (NP TRACE) (bound* to: The meeting)PP: (PP)PREP: forNP: (NP TIME DOW)WednesdayFigure 1 2 -  Tile final tree structure.This example demonstrates that the simpleformulation of the PASSIVE rule presented above,interact ing with other simply formulated grammatical rules241for parsing objects and initiating embedded infinitives,al lows a trace to be attached either as the object  of a verbor as the subject  of an embedded infinitive, whichever  isthe appropriate analysis for a given grammatical situation.Because the PASSIVE rule is formulated in such a way  thatit drops the trace it creates into the buffer, later rules,a l ready formulated to trigger on an NP in the buffer, willanalyze sentences with NP-preposing exact ly  the same asthose without a preposed subject.
Thus, we see that  theavai labi l i ty  of the buffer mechanism is crucial to capturingthis generalization; such a generalization can only bes ta ted  by a parser with a mechanism much like the bufferused here.The Grammar  In terpreter  and Chomsky 's  ConstraintsBefore turning now to a sketch of a computationalaccount  of Chomsky's constraints, there are severa limportant limitations of this work which must be enumerated.First of all, while two of Chomsky's constraintsseem to fall out of the grammar interpreter, there seems tobe no app.arent account of a third, the Propositional IslandConstraint, in terms of this mechanism.Second, Chomsky's formulation of theseconstraints is intended to apply to all rules of grammar, bothsyntact ic  rules (i.e.
transformations) and those rules ofsemantic interpretat ion which Chomsky calls "rules ofconstrual",  a set  of shallow semantic rules which governanaphoric processes \[Chomsky 77\].
The discussion herewill only touch on purely syntact ic phenomena; the questionof how rules of semantic interpretatiqn can be meshed withthe framework presented in this document has yet  to beinvest igated.Third, the arguments presented below deal onlywith English, and in fact  depend strongly upon several  factsabout English syntax,  most crucially upon the fact  thatEnglish is subject- init ial .
Whether these arguments can besuccessful ly  extended to other language types is an openquestion, and to this extent  this work must be considerede x ploratory.And finally, I will not show that these constraintsmust be true without exception; as we will see, there arevarious situations in which the constraints imposed by thegrammar interpreter  can be circumvented.
Most of thesesituations, though, will be shown to demand much morecomplex grammar formulations than those typical ly neededin the grammar so far constructed.
This is quite in keepingwith the suggestion made by Chomsky \[Chomsky 77"\] thatthe constraints are not necessari ly without except ion,  butrather that except ions will be "highly marked" andtherefore  will count heavily against any grammar thatincludes them.The Specified Subject ConstraintThe Specif ied Subject Constraint (SSC), s ta tedinformally, says that no rule may involve two const i tuentsthat  are Dominated by different cyclic nodes unless thelower of the two is the subject of an S or NP.
Thus, no rulemay involve constituents X and Y in tl~e structure shown inf igure 13 below, if cl and ~ are cyclic nodes and Z is thesub ject  of c(, Z distinct from X.\ [e .
.
.Y .
.
.
\ [~  z , , .x .
.
.
\ ] .
.
.Y .
.
.
\ ]Figure 18 - SSC:No rule can involve X and Y in this structure.The SSC explains why the surface subject  positionof verbs like "seems" and "is certain" which have nounderlying subject  can be filled only by the subject  and notthe ob ject  of the embedded S: The rule "MOVE NP" is f reeto shift  any NP into the empty subject position, but isconstrained by the SSC so that the ob ject  of the embeddedS cannot be moved out o f  that clause.
This explains why(a) in f igure 14 below, but not 14b, can be der ived from14c; the derivation of 14b from 14c would violate the SSC.
(a) John seems to like Mary.
(b)*Mary seems John to like.
(c) v seems is John to like Mary\]Figure 14 - Some examples illustrating the 8S(3.In essence,  then, the Specified Subject Constraintconstrains the rule "MOVE NP" in such a way that  only thesub jec t  of a clause can be moved out of that clause into aposition in a higher S. Thus, if a trace in an annotatedsurface structure is bound to an NP Dominated by a higherS, that t race must fill the subject position of the lowerclause.In the remainder of this section I will show that  thegrammar interpreter  constrains grammatical processes insuch a way  that annotated surface structures constructedby thp.
grammar interpreter will have this same property,given the formulation of the PASSIVE rule presented above.In torms of the parsing process, this means that if a t race is" lowered"  from one clause to another as a result of a"MOVE NP"-type operation during the parsing process, thenit will be at tached as the subject of the second clause.
Tobe more precise, if a trace is attached so that  it isDominated by some S node $1', and the trace is bound to anNP Dominated by some other S node $2, then that t race willnecessar i ly  be attached so that it fills the subject  positionof $1.
This is depicted in figure 15 below.C:The Active Node StackS2 ... / ...NP2s l  ... / ...NP: NP1 (NP TRACE) : bound to NP2Figure 15 - NP1 must be attached as the subject  of $1since it is bound to an NP Dominated by a higher S.Looking back at ti le complex passive exampleinvolving "raising" presented above, we see that  t i leparsing process results in a structure exact ly  like thatshown above.
The original point of the example, of course,was that  the rather simple PASSIVE rule handles this casewi thout  the need for some mechanism to expl ic i t ly  lower theNP.
The PASSIVE rule captures this general ization by242dropping the trace it creates into the buffer (a f te rappropr iate ly  binding the trace), thus allowing other ruleswr i t ten to handle normal NPs (e.g.
OBJECTS and INF-S-START1) to correct ly place the trace.This statement of PASSIVE does more, however,than simply capture a generalization about a specif icconstruction.
As I will argue in detail below, the behaviorspeci f ied by both the Specified Subject Constraint andSubjacency follows almost immediately from this formulation.In \[Marcus 77\],  I argue that this formulation of PASSIVE isthe only simple, non-ad hoc, formulation of this rule possible,and that  all other rules characterized by the competencerule "MOVE NP" must operate similarly; here, however,  I willonly show that these constraints follow natural ly.from thisformulation of PASSIVE, leaving the question of necess i tyaside.
I will also assume one additional constraint below,the Left-to-Right Constraint, which will be briefly mot ivatedlater  in this paper as a natural condition on the formulationof a grammar for this mechanism.The Left-to-Right Constraint: ti le constituents in thebuffer are (almost always) attached to higher levelconstituents in left - to-r ight order, i.e.
the f irstconst itnent in the buffer is (almost a lways)at tached before the second constituent.I will now show that a trace created by PASSIVEwhich is bound to an NP in one clause can only serve as thesub ject  of a clause dominated by that first clause.Given ti le formulation of PASSIVE, a trace can be" lowered"  into one clause from another only by the indirectroute of dropping it into the buffer before the subordinateclause node is created, which is exact ly  how the PASSIVErule operates.
This means that the ordering of theoperation.~ is crucially: 1) create a trace and drop it intot i le buffer, 2) create a subordinate S node, 3) attach thet race to the newly created S node.
Tile key point is that  att i le time that  the subordinate clause node is created andbecomes ti le current active node, ti le trace must be sittingin the buffer, filling one of the three buffer positions.
Thus,t i le parser will be in the state shown in figure 16 below,with the trace, in fact, most likely in t i le first buffer~)osition.The Active Node StackC: S123 (S SEC ...) / ...Tile BufferNP123 (NP TRACE) : bound to NP in S above $123Figure 16-  Parser state after embedded S created.Now, given the L-to-R Constraint, a trace which ism the buffer at the time that an embedded S node is f irstc reated must be one of the first several  const i tuentsa t tached to the S node or its daughter nodes.
From thestructure of English, we know that t i le leftmost threeconst i tuents  of an embedded S node, ignoring topical izedconst i tuents,  must either beCOMP NP AUXorNP AUX \[vP VERB ...\].
(The COMP node will dominate flags like " that"  or "for" thatmark the I)eginning of a complement clause.)
But then, if atrace,  i tself  an NP, is one of ti le first several  const i tuentsa t tached to an embedded clause, the only position it can fillwill be t i le subject  of the clause, exactly the empiricalconsequence of Cbomsky's Specified Subject Constraint insuch cases as explained above,The L - to -R  ConstraintLet us now return to the motivation for the L-to-RConstraint.
Again, I will not attempt to prove that  thisconstraint  must be true, but merely to show why it isplausible.Empirically, ti le Left-to-Right Constraint Seems tohold for the most part; for the grammar of English discussedin this paper, and, it would seem, for any grammar of Englishthat  attempts to capture the same range of generalizationsas this grammar, the constituents in the buffer are uti l izedin le f t - to - r ig l l t  order, with a small range of exceptions.
Thisusage is clearly not enforced by the grammar interpreter aspresent ly  implemented; it is quite possible to write a set  ofgrammar rules that specif ical ly ignores a constituent in thebuffer  until some arbitrary point in the clause, though such aset  of rL,les would be highly adhoc.
However, there rare lyseems to be a need to remove other than ti le f i rstconst i tuent  in the buffer.The one except ion to the L-to-R Constraint seemsto be that  a constituent C~ may be attached before theconst i tuent  to its left, Ci.
I, if Ci does not appear in surfacestructure in its underlying position (or, if one prefers, in itsunmarked position) and if its removal from the bufferreestabl ishes the unmarked order of the remainingconst i tuents,  as in the case of the AUX-INVERSION rulediscussed earlier in this paper.
To capture this notion, theL-to-R Constraint can be restated as follows: Allconst i tuents must be attached to higher level const i tuentsaccording'  to t i le left - to-r ight order of constituents in theunmarked case of that constituent's structure.This reformulation is interesting in that it would bea natural consequence of the operation of the grammarinterpreter  if packets were associated with the phrasestructure rules of an expl icit  "base component", and theserules were  used as templates to build up the structureassigned by ti le grammar interpreter.
A packet of grammarrules would tl len be expl ic it ly associated with each symboloil the right hand side of each phrase structure rule.
Aconst i tuent  of a given type would then be constructed byact ivat ing the packets associated with each node type  ofthe appropriate phrase structure rule in lef t - to-r ight  order.Since these base rules would ref lect the unmarked I - to-rorder of constituents, the constraint suggested here wouldthen simply falt out of the interpreter mechanism.SubjacencyBefore turning to the Subjacency Principle, a fewauxi l iary technical terms need to be defined: If we can243t race  a path up the tree from a given node X to a givennode Y, then we say X is dominated by Y, or equivalent ly,  Ydominates X.
If Y dominates X, and no other nodes in tervene(i.e.
X is a daughter of Y), then Y immediately (or directly)dominates X.
\[Akmajian & Heny 7,5\].
One non-standarddefinit ion will prove useful: I will say that if Y dominates X,and Y is a cycl ic  node, i.e.
an S or NP node, and there is noother  cycl ic  node Z such that Y dominates Z and Zdominates 'X (i.e.
there is no intervening cycl ic  node Zbetween Y and X) then Y Dominates X.The principle of Subjacency, informally s ta ted ,says  that no rule can involve const i tuents that  areseparated  by more than one Cyclic node.
Let us say  that  anode X is subjacent to a node Y if there is at most onecyc l ic  node, i.e.
at most one NP or S node, between thecyc l ic  node that  Dominates Y and the node X.
Given thisdefinit ion, the Subjacency principle says that no rule caninvolve const i tuents  that are not subjacent.The Sul) jacency principle implies that movementrules are constrained so that they can move a const i tuentonly into positions that  the constituent was sub jacent  to,i.e.
only within the clause (or NP) in which it or iginates, orinto the clause (or NP) that Dominates that clause (...).
Thismeans that  if c~, ~, and ( in figure 17 are cycl ic  nodes, norl, le can move a const ituent from position X to either of  theposit ions Y, where \[~...X...\] is distinct from \[~X\] .\[,...Y..-\[/3...\[~...X...\]...\].-.Y..-\]Figure 17 - Subjacency:No nile can involve X and Y in this structure.Suh jacency implies that if a const i tuent is to be" l i f ted"  up more than one level in constituent structure,  thisoperat ion must he done by repeated operations.
Thus, touse one of Chomsky's examples, t i le sentence given inf igure 18a, with a deep structure analogous to 18b, must beder ived  as fol lows (assuming that '*is certain",  like "seems" ,has no sub jec t  in underlying structure):  The deep structuremust f i rst  undergo a movement operation that results in as t ructure  analogous to 18c, and then another movementoperat ion that results in 18(I, each of these movementsleaving a t race as shown.
That 18c is in fac t  anintermediate  structure is supported by the ex is tence  ofsentences  such as 18e, which purportedly result when theV in the matrix S is replaced by the lexical  item " i t" ,  andthe embedded S is tensed rather than infinitival.
Thes t ructure  given in 18f is ruled out as a possible annotatedsur face  structure,  because the single t race could only bele f t  if the NP "John" was moved in one fell swoop from itsunderlying position to its position in surface structure,  whichwould v io late Subjacency.
(a) John seems to be certain to win.
(b) V seems IS V to be certain Is John to w in \ ] \ ](c) v seems Is John to be certain \[s t to w in \ ] \ ](d) John seems Is t to be certain \[s t to w in \ ] \ ](e) It seems that John is certain to win.
(f) John seems \[s V to be certain \[S t to w in \ ] \ ]Figure 18 - An example demonstrating Subjacency.Having s tated Subjacency in terms of the abst rac tcompetence  theory of generat ive grammar, I now will showthat  a parsing correlate of Subjacency follows from thest ructure  of the grammar interpreter.
Specif ical ly,  I willshow that there are only limited cases in which a t racegenerated  by a "MOVE-NP" process can be " lowered"  morethan one clause, i.e.
that a trace created and bound whileany given S is current must almost always be a t tachede i ther  to that  S or to an S which is Dominated by that S.Let us begin by examining what it would mean tolower a t race more than one clause.
Given that a t race canonly be " lowered"  by dropping it into the buffer  and thencreat ing a suhordinate S node, as discussed above,lowering a t race more than one clause necessar i ly  impliesthe fol lowing sequence of events,  dep icted in f igure 19below: First, a trace NP1 must (a) be created with some Snode, $1, as the current S, (b) bound to some NP Dominatedby that  S and then (c) dropped into the buffer.
Byrlefinition, it will be inserted into the first cell in the buffer.
(This is shown in figure lga)Then  a second S, $2, must bec reated ,  stq~planting $1 as the ct=rrent S, and then yet  athird S, $3, must he created,  becoming the current S. Duringall these  steps,  the trace NP1 remains sitt ing in the buffer .Finally, NP1 is at tached ?,nder $3 (fig.
19b).
By theSpec i f ied  Subject  Constraint, NP1 must then at tach  to $3as its subject .The Act ive Node StackC: $1 ... / ...1 st :The BufferNP1 (NP TRACE) : bound to NP Dominated by $1(a) - NP1 is dropped into the bufferwhile $1 is the current S.C:Tile Act ive Node StackS1 ... / ...S2 .
.
.
/ .
.
.S3  .
.
.
/ .
.
.NP1 (NP TRACE) : bound to NP Dominated by  S1(b )  - After 52 and $3 are created,NP1 is a t tached to $3 as its subject  (by the SSC).Figl,re lg  - Lowering a trace more than 1 clauseBut this sequence of events  is highly unlikely.
Theessence  of the argument is this:Nothing in t i le buffer can change between t i le timethat  $2 is c reated  and $3 is created if NP1 remains in thebuffer.
NP1, like any other node that is dropped from theac t ive  node stack into the buffer, is inserted into the f i rstbuf fer  position.
But then, by the L-to-R Constraint, nothingto the right of NP1 can be at tached to a higher levelconst i tuent  until NP1 is attached.
(One can show that  it ismost unlikely that any const ituents will enter  to the lef t  ofNP1 af ter  it is dropped into ti le buffer, but I will suppressthis detai l  here;  t i le full argument is included in \ [Marcus77\ ] .
)244But if the contents of the buffer do not changebetween the creation of $2 and $3, then what can possiblymot ivate the creation of both $2 and $3?
The contents ofthe buffer  mr,st necessari ly provide clear evidence thatbotl\] of these clauses are present, since, by theDeterminism Hypothesis, the parser must be correct if itin it iates a constittient.
Thus, the same three const ituents int im buffer  must provide convincing evidence not only forthe creat ion of $2 hut also for $3.
Furthermore, if NP1 is tobecome ti le subject  of $3, and if $2 Dominates $3, then itwoL,Id seem that ti le constituents that follow NP1 in thebuffer  nit=st also be constituents of $3, since $3 must becompleted before it is dropped from the act ive node stackand const i tuents can then be attached to $2.
But then $2must be created entirely on the basis of evidence providedby the const ituents of another clatise (unless Sa has lessthan three constituents).
Thus, it would seem that thecontents  of the buffer cannot provide evidence for thepresence of both clauses unless ti le presence of $3, byitself ,  is enough to provide confirming evidence for thepresence of $2.
Tiffs would be the case only if there were,say, a clausal construction that could only appear (perhapsin a particular environment) as ti le initial constituent of ahigher clause.
In tiffs case, if there are such constructions,a violation of Subjacency should be possible.With the one exception just mentioned, there is nomotivation for creating two clauses in such a situation, andtl lus the initiation of only one such clause can be motivated.But if only one clause is initiated before NP1 is attached,then NP1 must he attached to this clause, and this clause isnecessar i ly  sub jacent to  the clause which Dominates the NPto which it is bound.
Thus, the grammar interpreter  willbehave  as if it enforces the Subjacency Constraint.As a concluding point, it is worthy of note thatwil l ie the grammar interpreter appears to behave exact ly  asif it were  consD:ained by the Subjacency principle, it is infact  constrained by a version of the Clausemate Constraint!
(The Clausemate Constraint, long tacit ly assumed bylinguists but first expl ic it ly stated, I bel ieve, by Postal\ [Postal  64\] ,  s tates that a transformation can only involveconst i tuents that are Dominated by the same cyclic node.This constraint is at the heart of Postal's attack on theconstraints that are discussed above and his argument for a"raising" analysis.)
The grammar interpreter, as was s ta tedabove,  limits grammar rules from examining any node in theact ive  node stack higher than tile current cyclic node,which is to say that it can only examine clausemates.
Thetr ick is that a trace is created and bound while it is a"c lausemate"  of the NP to which it is bound in that  thecurrent cycl ic node at that time is the node to which thatNP is attached.
The trace is then dropped into the bufferand another S node is created, thereby destroying theclausemate relationship.
The trace is then attached to thisnew 5 node.
Thus, in a sense, the trace is lowered fromone clause to another.
The crucial point is that while thislowering goes on as a result of the operation of ti le grammarinterpreter ,  it is only implicitly lowered in that 1) the t racewas never  attached to the higher S and 2) it is not droppedinto t i le buffer because of any realization that it must be" lowered" ;  in fact  it may end up attached as a c lausemateof the NP to which it is bound - as the passive examplespre~ented earl ier make clear.
Tl~e trace is simply droppedinto the buffer because its grammatical function is not clear,and the creation of ti le second S follows from otheri f ldependent ly motivated grammatical processes.
From thepoint of v iew of this processing theory, we can have ourcake and eat  it too; to the extent  that it makes sense tomap results from the realm of processing into the realm ofcompetence,  in a sense both the clausemate/"rais ing" andthe Subjacency positions are correct.Evidence for the Determinism HypothesisIn closing, I would like to show that the propert iesof the grammar interpreter crucial to capturing the behaviorof Chomsky's constraints were originally mot ivated 'by  theDeterminism Hypothesis, and thus, to some extent ,  theDeterminism Hypotlmsis explains Chomsky's constraints.Tile strongest form of such an argument, of course,woukl  he to show that (a) either (i) the grammar interpreteraccounts for all of Chomsky's constraints in a manner whichis conclusively ?miversal or (ii) the constraints that it willnot account for are wrong and that (b) the properties of thegrammar interpreter  which were crucial for this proof werefo rced  by the Determinism Hypothesis.
If such an argumentcotdd be made, it would show that ti le DeterminismHypothesis provides a natural processing account of thel inguistic data characterized by Chomsky's constraints,giving strong confirmation to the Determinism Hypothesis.I have shown none Of the above, and thus' myclaims must be proport ionately more modest.
I have arguedonly that important sub-cases of Chomsky's Constraintsfol low from the grammar interpreter, and while I can showthat  the Determinism Hypothesis strongly motivates themechanisms from which these arguments follow, I cannotshow necessity.
The extent  to which this argumentprovides evidence for ti le Determinism Hypothesis must thusbe left  to the reader; no object ive measure exists  for suchmatters.The abil ity to drop a trace into the buffer is at theheart  of the arguments presented here for Subjacency andthe 55C as consequences of the functioning of the grammarinterpreter ;  this is the central operation upon which theabove arguments are based.
But the buffer itself, and thefact  that a const ituent can be dropped into the buffer if itsgrammatical function is uncertain, are direct ly motivated bythe Determinism Hypothesis.
Given tiffs, it is fair to claimthat  if Chomsky's constraints follow from the operat ion ofthe grammar interpreter,  then they are strongly linked to theDeterminism Hypothesis.
If Chomsky's constraints are Infac t  true, then the arguments presented in this paperprovide solid evidence in support of the DeterminismHypothesis.AcknowledgmentsThis paper summarizes one result presented in myPh.D.
thesis; I would like to express my gratitude to themany people who Contributed to the technical content  oft l lat  work: Jon Alien, my thesis advisor, to whom I owe aspecial  debt of tllanks, Ira Goldstein, Seymour Papert, Bill245Martin, Bob Moore, Chuck Rieger, Mike Genesereth, GerrySussman, Mike Brady, Craig Thiersch, Beth Levin, CandyBullwinkle, Kl,rt VanLehn, Dave McDonald, and Chuck Rich.This paper describes research done at theArtificial Intelligence Laboratory of the MassachusettsInstitute of Technology.
Support for the laboratory'sartificial intelligence research is provided in part by theAdvanced .Research Projects Agency of the Department ofDefence under Office of Naval Research Contract N00014-75-C-0043.BIBLIOGRAPHYAkmajian, A. and F. Heny \[1975\] An Introduction to thePrinciples of Transformational Syntax, MIT Press,Cambridge, Mass.Bresnan, d. W. \[1976\] "Evidence for a Theory of UnboundedTransformations", Linguistic Analysis 2:853.Chomsky, N. \[1973\] "Conditions on Transformations", in S.Anderson and P. Kiparsky, ads., A Festschrift forMorris Halle, Itolt, Rinehart and Winston, N.Y.Chomsky, N. \[1975\] Reflections on Language, Pantheon, N.Y.Chomsky, N. \[1976\] "Conditions on Rules ofjGrammar",LingUistic Analysis 2:803.Chomsky, N. \[1977\] "Oil Wh-Movement", in A. Akmajian, P.Culicover, and T. Wasow, eds., Formal Syntax,Academic Press, N.Y.Fillmore, C. J.
\[1968\] "The Case for Case" in Universals inLinguistic Theory, E. Bach and R..T. Harms, eds., Holt,Rinehart, and Winston, N.Y.Gruber, J. S. \[196,5\] Studies in Lexical Relations,unpublished Ph.D. thesis, MIT.Jackendoff, R. S. \[1972\] Semantic Interpretation inGenerative Grammar, MIT Press, Cambridge, Mass.Marcus, M. P. \[1977\] A Theory of Syntactic Recognition forNatural Language, unpublished Ph.D. thesis, MIT.Marcus, M. P. \[1078\] "Capturing Linguistic Generalizationsin a Parser for English", in the proceedings of The 2ndNational Conference of the Canadian Society forComputational Studies of Intelligence, Toronto, Ca.nada.Newell, A. and H.A.
Simon \[1972\] Human Problem Solving,Prentice-Hall, Englewood Cliffs, N.J.Postal, P. M. \[1974\] On Raising, MIT Press, Cambridge,Mass.Pratt, V. R. \[1973\] "Top-Down Operator Precedence", in theproceedings of The SIGACT/SIGPLAN Symposium onPrinciples of Programming Languages, Boston, Mass.Sager, N. \[197,3\] "The String Parser for ScientificLiterature", in \[Rustin 73\].Winograd, T. \[1971\] Procedures as a Representation forData in a Computer Program for Understanding NaturalLanguage, Project MAC-TR 84, MIT, Cambridge, Mass.Woods, W. A.
\[1970\] "Transition Network Grammars forNatural Language Analysis", Communications of the ACM13:591.246
