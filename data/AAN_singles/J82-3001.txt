Computational Complexity andLexicaI-Functional GrammarRobert C. BerwickArtificial Intelligence LaboratoryMassachusetts Institute of TechnologyCambridge, Massachusetts 021391.
In t roduct ionAn important goal of modern linguistic theory is tocharacterize as narrowly as possible the class of natu-ral languages.
One classical approach to this charac-terization has been to investigate the generative capac-ity of grammatical systems pecifiable within particularlinguistic theories.
Formal results along these lineshave already been obtained for certain kinds of Trans-formational Generative Grammars: for example, Petersand Ritchie 1973a showed that the theory of Transfor-mational Grammar presented in Chomsky's Aspects ofthe Theory of Syntax 1965 is powerful enough to allowthe specification of grammars for generating any re-cursively enumerable language, while Rounds1973,1975 extended this work by demonstrating thatmoderately restricted Transformational Grammars(TGs) can generate languages whose recognition timeis provably exponential.
1These moderately restricted theories of Transfor-mational Grammar generate languages whose recogni-tion is widely considered to be computationally in-tractable.
Whether this "worst case" complexity anal-ysis has any real import for actual linguistic study hasbeen the subject of some debate (for discussion, seeChomsky 1980, Berwick and Weinberg 1982).
Re-suits on generative capacity provide only a worst-casebound on the computational resources required tol In Rounds 's  proof, t ransformat ions are subject to a"terminal length non-decreasing" condition, as suggested by Petersand Myhill (cited in Rounds 1975).
A similar " terminal  lengthincreasing" constraint (to the author's knowledge first proposed byPetrick 1965) when coupled with a condition on recoverability ofdeletions, yields languages that are recursive but not necessaryrecognizable in exponential time.2 Usually, the recognition procedures presented actually re-cover the structural description of sentences in the process of rec-ognition, so that in fact they actually parse sentences, rather thansimply recognize them.recognize the sentences pecified by a linguistic theo-ry.
2 But a sentence processor might not have to ex-plicitly reconstruct deep structures in an exact (butinverse) mimicry of a transformation derivation, oreven recognize every sentence generable by a particu-lar transformational theory.
For example, as suggestedby Fodor, Bever and Garrett 1974, the human sen-tence processor could simply obey a set of heuristicprinciples and recover the right representations speci-fied by a linguistic theory, but not according to therules of that theory.
To say this much is to simplyrestate a long-standing view that a theory of linguisticperformance could well differ from a theory of linguis-tic competence - and that the relation between thetwo could vary from one of near isomorphism to themuch weaker input/output equivalence implied by theFodor, Bever, and Garrett position.
3In short, the study of generative capacity furnishesa mathematical characterization of the computationalcomplexity of a linguistic system.
Whether this mathe-matical characterization is cognitively relevant is arelated, but distinct, question.
Still, the determinationof the computational complexity of a linguistic systemis an important undertaking.
For one thing, it gives aprecise description of the class of languages that the3 The phrase " input /output  equivalence" simply means thatthe two systems - the linguistic grammar and the heuristic princi-ples - produce the same (surface string, underlying structure) pairs.Note that the "internal constitut ion" of the two systems could bewildly different.
The intuitive notion of "embedding a linguistictheory into a model of language use" as it is generally construed ismuch stronger than this, since it implies that the parsing systemfollows some (perhaps all) of the same operating principles as thelinguistic system, and makes reference in its operation to the samesystem of rules.
This intuitive description can be sharpened consid-erably.
See Berwick and Weinberg 1983 for a more detailed discus-sion of " t ransparency" as it relates to the embeddabil ity of a lin-guistic theory in a model of language use, in this case, a model ofparsing.Copyright 1982 by the Association for Computational  Linguistics.
Permission to copy without fee all or part of this material is grantedprovided that the copies are not made for direct commercial advantage and the Journal reference and this copyright notice are included onthe first page.
To copy otherwise, or to republish, requires a fee and/or  specific permission.0362-613X/82/030097-13 $0 .00American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 97Robert C. Berwick Computational Complexity and LexicaI-Functional Grammarsystem can generate, and so can tell us whether thelinguistic system is in principle descriptively adequate.This method of argument was used in Chomsky's orig-inal rejection of finite-state languages as an adequatecharacterization of human linguistic competence.
Sec-ond, as mentioned, the resource bound on recognitiongiven by a complexity-theoretic analysis tells us howlong recognition will take in the worst possible case.Since unrestricted TGs can generate computational-ly "hard"  languages, then plainly, in order to makeTGs efficiently parsable, one must supply additionalrestrictions.
These could be either modifications tothe theory of TG itself, or constraints on the parsingmechanism.
For example, the current theory of TG(see Chomsky 1981) contains several restrictions onthe way in which displaced constituents such aswh-phrases may be linked to their "canonical" posi-tion in predicate-argument structure.
(E.g., Who inWho did Bill kiss is assumed to be linked to a canoni-cal argument position after the verb kiss.)
As an ex-ample of a constraint on the parsing mechanism, onecould proceed as did Marcus 1980, and posit const-raints dictating that TG-generated languages must haveparsers that meet certain "locality condit ions".
4 Forinstance, the Marcus constraints amount to an exten-sion of Knuth's  1965 LR(k) locality condition to a(restricted) version of a two-stack deterministic push-down automaton.
5Recently, a new theory of grammar has been ad-vanced with the explicitly stated aim of meeting thedual demands of learnabil ity and parsabil ity - theLexical-Functional Grammars  (LFGs) of Kaplan andBresnan 1981.
The theory of Lexical-FunctionalGrammar  is claimed to be at least as descriptivelyadequate as Transformational  Grammar,  if not moreso.
Moreover, it is claimed to have none of TG's com-4 It is important not to confuse the requirement hat TG-generated languages have parsers that meet certain constraints withthe claim that such parsers transparently embed TGs.
As stated,the only requirement is one of weak input/output equivalence - i.e.,that the parser construct he same (surface string, underlying repre-sentation) pairs as the TG.
Actually, one can show that a modifiedMarcus parsing system goes beyond this requirement and operatesaccording to the same principles as the recent transformationaltheory of Chomsky.
That is, such a modified Marcus parser makesreference to the same base constraints and representational units asthe linguistic theory.
Since it abides by the same rules and repre-sentations as TG, one is justified in claiming that the model embedsa TG.
Note that the Marcus parser does not mimic earlier theoriesof TG (as presented in Aspects of the Theory of Syntax); there is norule-for-rule correspondence between an Aspects grammar and therules of the Marcus parser.
But neither is there a rule-for-rulecorrespondence between modern theories of TG and the Aspectstheory.
For example, there is no longer a distinct rule of "passive"or "dative movement".
A detailed demonstration of this claimwould go far beyond the purpose of this paper.
See Berwick andWeinberg forthcoming.5 The possible need for LR(k)-l ike restrictions in order toensure efficient processability was also suggested by Rounds 1973.putational unruliness, in the sense that it is claimedthat there is a "natural"  embedding of an LFG into aparsing mechanism (a performance model) that ac-counts for human sentence processing behavior.
InLFG, there are no transformations (as classically de-scribed); the work formerly ascribed to transforma-tions such as "passive" is shouldered by informationstored in lexical entries associated with lexical items.The underlying representation of surface strings that isbuilt is also different from the deep structures of clas-sical transformational theory; the representation makesreference to functionally defined notions of grammati-cal terms like "Subject" ,  rather than defining themstructurally, as was done in classical t ransformationtheory.
The elimination of transformational  powerand the use of a different kind of underlying repre-sentation for sentences naturally gives rise to the hopethat a lexical-functional system would be computation-ally simpler than a transformational one.An interesting question then is to determine, as hasalready been done for the case of certain brands ofTransformational  Grammar,  just what the "worstcase" computational complexity for the recognition ofLFG languages is.
If the recognition time complexityfor languages generated by the basic LFG theory canbe as complex as that for languages generated by amoderately restricted transformational  system, thenpresumably LFG will also have to add additional const-raints, beyond those provided in its basic theory, inorder to ensure efficient parsability.
Just as withtransformational  theories, these could be constraintson either the theory or its performance model realiza-tion.The main result of this paper is to show that cer-tain Lexical-Functional Grammars  can generate lan-guages whose recognition time is very likely computa-tionally intractable, at least according to our currentunderstanding of algorithmic complexity.
Briefly, thedemonstrat ion proceeds by showing how a problemthat is widely conjectured to be computationally diffi-cult - namely, whether there exists an assignment ofl ' s  and O's (or "T ' "s  and "F ' "s )  to the atoms of aBoolean formula in conjunctive normal form thatmakes the formula evaluate to "1"  (or " t rue")  - canbe re-expressed as the problem of recognizing whethera particular string is or is not a member of the lan-guage generated by a certain Lexical-FunctionalGrammar.
This "reduct ion" shows that in the worstcase the recognition of LFG languages can be just ashard as the original Boolean satisfiability problem.Since it is widely conjectured that there cannot be apolynomial-t ime algorithm for satisfiability (the prob-lem is NP-complete),  there cannot be a polynomial-time recognition algorithm for LFGs in general either.Note that this results sharpens that in Kaplan andBresnan 1981; there it is shown only that LFGs98 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982Robert C. Berwick Computational Complexity and LexicaI-Functional Grammar(weakly) generate some subset of the class of context-sensitive languages, and, therefore, in the worst case,exponential time is known to be sufficient (though notnecessary) to recognize any LFG language.
The resultin Kaplan and Bresnan 1981 therefore does not ad-dress the question of how much time, in the worstcase, is necessary to recognize LFG languages.
6 Theresult of this paper indicates that in the worst casemore than polynomial time will probably be necessary.
(The reason for the hedge "probab ly"  will becomeapparent below; it hinges upon the central unsolvedconjecture of current complexity theory.)
In shortthen, this result places the LFG languages more pre-cisely in the complexity hierarchy of languages.It also turns out to be instructive to inquire intojust why a lexical-functional approach can turn out tobe computational ly difficult, and how computationaltractability may be guaranteed.
Advocates of lexical-functional theories may have thought (and some haveexplicitly stated) that the banishment of transforma-tions is a computationally wise move because transfor-mations are computational ly costly.
El iminate thetransformations, o this causal argument goes, and onehas eliminated all computational problems.
Intriguing-ly though, when one examines the proof to be givenbelow, the ability to express co-occurrence constraintsover arbitrary distances across terminal tokens in astring (as in Subject-Verb number agreement), whencoupled with the possibility of alternative lexical en-tries, seems to be all that is required to make the rec-ognition of LFG languages intractable.This leaves the question posed in the opening para-graph: just what sorts of constraints on natural lan-guages are required in order to ensure efficient parsa-bility?
As it turns out, even though general LFGs maywell be computationally intractable, it is easy to imag-ine a variety of additional constraints for LFG theorythat provide a way to avoid this problem.
All of theseadditional restrictions amount to making the LFG the-ory more restricted, in such a way that the reductionargument cannot be made to work.
For example, oneeffective restriction is to stipulate that there can onlybe a finite stock of features with which to label lexicalitems.
In any case, the moral of the story is an unsur-prising one: specificity and constraints can absolve atheory of grammar from computational intractability.What may be more surprising is that the requisitelocality constraints eem to be useful for a variety oftheories of grammar, from Transformational Grammarto Lexical-Functional Grammar.2.
A Rev iew of Reduct ion ArgumentsThe demonstration of the computational complexityof LFGs relies upon the standard complexity-theoretictechnique of reduction.
Because this method may beunfamiliar to many readers, a short review is presentedimmediately below; this is followed by a sketch of thereduction proper.The idea behind the reduction technique is to takea difficult problem, in this case the problem of deter-mining the satisfiability of Boolean formulas in con-junctive normal form (CNF), and show that the prob-lem can be quickly transformed into the problemwhose complexity remains to be determined, in thiscase the problem of deciding whether a given string isin the language generated by a given Lexical-Functional Grammar.
Before the reduction proper isreviewed, some definitional groundwork must be pres-ented.
A Boolean formula in conjunctive normal formis a conjunction or disjunction of literals, where aliteral is just an atom (like Xi) or the negation of anatom (Xi).
A formula is satisfiable just in case thereexists some assignment of T's and F's (or l 's  and O's)to the atoms of a formula that forces the evaluation ofthe entire formula to be T (true); otherwise, the for-mula is said to be unsatisfiable.
For example, the fol-lowing formula is satisfiable:(X2VX3VX7)A(X1VX2VX4)A(X3VX1VX7)since the assignment of X2=T,  X3=F,  X7=F,  X I=T,and X4=F makes the whole formula evaluate to "T" .The reduction in the proof below uses a somewhatmore restricted format where every term comprises thedisjunction of exactly three literals, so-called 3-CNF(or "3-SAT").
7How does a reduction show that the LFG recogni-tion problem must be at least as hard (computationallyspeaking) as the original problem of Boolean satisfia-bility?
The answer is that any decision procedure forLFG recognition could be used as a correspondinglyfast decision procedure for 3-CNF, as follows:(1) Given an instance of a 3-CNF problem (the ques-tion of whether there exists a satisfying assignmentfor a given formula in 3-CNF), apply the transfor-mational algorithm provided by the reduction; thisalgorithm is itself assumed to execute quickly, inpolynomial time or less.
The algorithm outputs acorresponding LFG decision problem, namely: (i)a Lexical-Functional Grammar and (ii) a string tobe tested for membership in the language generat-ed by the LFG.
The LFG recognition problemrepresents or mimics the decision problem for3-CNF in the sense that the "yes"  and "no"  an-swers to both satisfiability problem and member-6 This result can also be established by showing that LFGscan generate at least all the indexed languages as defined by Aho1968.
See Berwick 1981 for details.7 This restriction entails no loss of generality (see Hopcroftand Ullman 1979, Chapter 12), since this restricted format can beeasily shown to have the power to express any CNF formula.American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 99Robert C. Berwick Computational Complexity and LexicaI-Functional Grammarship problem must coincide (if there is a satisfy-ing assignment, hen the corresponding LFG deci-sion problem should give a "yes"  answer, etc.).
(2) Solve the LFG decision problem - the string-LFGpair - output by Step 1.
If the string is in theLFG language, the original formula is satisfiable;if not, it is unsatisfiable.
8To see how a reduction can tell us something aboutthe "worst case" time or space complexity required torecognize whether a string is or is not in an LFG lan-guage, suppose for example that the decision proce-dure for determining whether a string is in an LFGlanguage took only polynomial time (that is, takes timen k on a deterministic Turing machine, for some integerk, where n is the length of the input string).
Then,since the composit ion of two polynomial algorithmscan be readily shown to take only polynomial time(see Hopcrof t  and Ullman 1979, Chapter  12), theentire process sketched above, from input of the CNFformula to the decision about its satisfiability, will takeonly polynomial time.However,  CNF (or 3-CNF) has no known polynomi-al time algorithm, and indeed, it is consideredexceedingly unlikely that one could exist.
Therefore, itis just as unlikely that LFG recognition could be done(in general) in polynomial time.
What the reductionshows is that LFG recognition is at least as hard as theproblem of CNF.
Since the latter problem is widelyconsidered to be difficult, the former inherits the diffi-culty.The theory of computat ional  complexity has amuch more compact erm for problems like CNF: CNFis NP-complete.
This label is easily deciphered:(1) CNF satisfiability is in the class NP.
That is, theproblem of determining whether an arbitrary CNFformula is satisfiable can be computed by anon-determinist ic Turing machine in polynomialtime.
(Hence the abbreviation "NP" ,  for "non-deterministic polynomial".
To see that CNF isindeed in the class NP, note that one can simplyguess all possible combinations of truth assign-ments to literals, and check each guess in polyno-mial time.
)(2) CNF is complete.
That is, all other problems in theclass NP can be quickly reduced to some CNFformula.
(Roughly, one shows that Boolean for-8 Note that the grammar and string so constructed ependupon just what formula is under analysis; that is, for each differentCNF formula, the procedure presented above outputs a differentLFG grammar and string combination.
In the LFG case it isimportant to remember that "grammar" eally means "grammar pluslexicon" - as one might expect in a lexically-based theory.
S.Peters has observed that a slightly different reduction allows one tokeep most of the grammar fixed across' all possible input formulas,constructing only different-sized lexicons for each different CNFformula.
Details are provided below.mulas can be used to "s imulate" any valid compu-tation of a non-deterministic Turning machine.
)Since the class of problems solvable in polynomialtime on a deterministic Turing machine (conventional-ly notated, P) is trivially contained in the class sosolved by a non-determinist ic Turing machine, theclass P must be a subset of the class NP.
A well-known, well-studied, and still open question is whetherthe class P is a proper subset of the class NP.
In otherwords, are there problems solvable in non-deterministic polynomial time that cannot be solved indeterministic polynomial time?
Because all of theseveral thousand NP-complete problems now cata-logued have so far proved recalcitrant o deterministicpolynomial time solution, it is widely held that P mustindeed be a proper subset of NP, and therefore thatthe best possible algorithms for solving NP-completeproblems must take more than polynomial time.
(Ingeneral, the algorithms now known for such problemsinvolve exponential combinatorial search, in one fash-ion or another; these are essentially methods that dono better than to brutally simulate - deterministically,of course - a non-determinist ic machine that"guesses" possible answers.
)To repeat the force of the reduction argument hen,if all LFG recognition problems were solvable in po-lynomial time, then the ability to quickly reduce CNFformulas to LFG recognit ion problems would implythat all NP-complete problems would be solvable inpolynomial time, and that the class P = the class NP.This possibility seems extremely remote.
Hence, ourassumption that there is a fast (general) procedure forrecognizing whether a string is or is not in the lan-guage generated by an arbitrary LFG must be false.
Inthe terminology of complexity theory, LFG recognitionmust be NP-hard - "as hard as" any other NP prob-lem, including the NP-complete problems.
This meansonly that LFG recognition is at least as hard as otherNP-complete problems - it could still be more difficult(lie in some class that contains the class NP).
If onecould also show that the languages generated by LFGsare in the class NP, then LFGs would be shown to beNP-complete.
This paper stops short of proving thislast claim, but simply conjectures that LFGs are in theclass NP.3.
A Sketch  of the Reduct ionTo carry out this demonstration i detail one mustexplicitly describe the transformation procedure thattakes as input a formula in CNF and outputs a corre-sponding LFG decision problem - a string to be testedfor membership in a LFG language and the LFG itself.One must also show that this can be done quickly, in anumber of steps proportional to (at most) the lengthof the original formula to some polynomial power.100 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982Robert C. Berwick Computational Complexity and LexicaI-Functional GrammarOne caveat is in order before embarking on a proofsketch of this reduction.
The grammar that is outputby the reduction procedure will not look very muchlike a grammar for a natural language, although thegrammatical devices that will be employed will in ev-ery way be those that are an essential part of the LFGtheory.9 In other words, although it is most unlikelythat any natural language would encode the satisfiabil-ity problem (and hence be intractable) in just themanner outlined below, no "exot ic" LFG machinery isused in the reduction.
Indeed, some of the more pow-erful LFG notational formalisms - long-distance bind-ing, existential and negative feature operators -  havenot been exploited.
(An earlier proof made use of anexistential operator in the feature machinery of LFG,but the reduction presented here does not.
)To make good this demonstration one must set outjust what the satisfiability problem is and what thedecision problem for membership in an LFG languageis.
Recall that a formula in conjunctive normal form issatisfiable just in case every conjunctive term evalu-ates to true, that is, at least one literal in each term istrue.
The satisfiability problem is to find an assign-ment of T's and F's to the atoms at the bottom (notethat complements of atoms are also permitted) suchthat the root node at the top gets the value "T"  (fortrue).
How can we get a Lexical-Functional Grammarto represent this problem?
What we want is forsatisfying assignments to correspond to well-formedsentences of some corresponding Lexical-FunctionalGrammar,  and non-satisfying assignments to corre-spond to sentences that are not well-formed, accordingto the LFG, as indicated in Figure 1.
Since one wantsthe satisfying/non-satisfying assignments of ahay par-ticular formula to map over into wel l - formed/i l l -formed sentences, one must obviously exploit the LFGmachinery for capturing well-formedness conditions onsentences.
To make the discussion clear to the readerwill require a brief account of the LFG theory itself.satisfiable non-satisfiableformula formulasentence w IS sentence w IS NOTin LFG language L(G) in LFG language L(G)Figure 1.
A reduction preserves olutions to the original problem.Just as in a transformational theory, a Lexical-Functional Grammar  associates with each generablesurface string (sentence) a number of distinct repre-9 These include feature agreement, the lexical analog of Sub-ject or Object "control",  lexical ambiguity, and a garden varietycontext-free base grammar.sentations.
For our purposes here we need to focus onjust two of these: the constituent structure of a sen-tence (its "c-structure",  roughly, a labeled bracketingof the surface string, annotated with certain featurecomplexes); and the functional structure of a sentence(its " f -st ructure" ,  roughly, a representat ion of theunderlying predicate-argument structure of a sentence,described in terms of grammatical relations such asSubject and Object.)
Unlike a TransformationalGrammar,  however, a Lexical-Functional Grammardoes not generate surface sentences by first specifyingan explicit, context-free deep structure followed by aseries of categorial ly-based transformations.
"Ca-tegorial ly-based" simply means that the transforma-tions move constituents defined in terms of categories,like NP or PP.)
Rather, predicate-argument structureis mapped directly into c-structure, on the basis ofpredicates that are grounded upon grammatical rela-tions (like Subject and Object).
The conditions forthis mapping are provided by a set of so-called func-tional equations associated with the context-free rulesfor generating permissible c-structures, along with aset of conventions that in effect convert the functionalequations into wel l - formedness predicates for c-structures.In more detail, an LFG c-structure is generated bya base context-free ~rammar.
A necessary conditionfor a sentence (considered as a string) to be in thelanguage generated by a Lexical-Functional Grammaris that it can be generated by this base grammar; sucha sentence is then said to have a well-formed constitu-ent structure.
For example, if the base rules includedS=>NP VP; VP=>V NP, then (glossing over detailsof Noun Phrase rules) the sentence John kissed thebaby would be well-formed but John the baby kissedwould not.
Note that this assumes, as usual, the exist-ence of a lexicon that provides a categorization foreach terminal item, e.g., that baby is of the categoryN, kissed is a V, etc.
Important ly then, this well-formedness condition requires us to provide at leastone legitimate parse tree for the candidate sentencethat shows how it may be derived from the underlyingLFG base context-free grammar.
(There could bemore than one legitimate tree if the underlying gram-mar is ambiguous.)
Note further that the choice ofcategorization for a lexical item may be crucial.
Ifbaby was assumed to be of category V, then bothsentences above would be ill-formed.Since the base grammar is context-free, there arewell-known algorithms for checking the well-formedness of the strings it can generate in polynomialtime.
Intractability cannot arise on this score, then.A Lexical-Functi0nal Grammar  consists of morethan just a base context-free grammar, however.
Asmentioned, a second major component  of the LFGtheory is the provision for adding a set of so-calledAmerican Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 101Robert C. Berwick Computational Complexity and LexicaI-Functional Grammarfunctional equations to the base context-free rules.The functional equations define an implicit f-structureassociated with every c-structure, and this f-structuremust itself be well-formed.
Part of the linguistic roleof f-structures is to account for the co-occurrencerestrictions that are an obvious part of natural lan-guages (e.g., Subject-Verb agreement).How exactly do the functional equations work?Their job is to specify how the f-structure of a sen-tence gets built.
This is done by associating possiblycomplex features with lexical entries and with the non-terminals of specified context-free rules; these featureshave values.
The features are pasted together underthe direction of the functional equations to form f-structures associated with the sub-consIituents of thesentence; these (now possibly complex) f-structuresare in turn assembled to form a master f-structureassociated with the root node of the sentence.Note then that in this theory a " feature"  can besomething as simple as an atomic object that is binaryvalued; for example, a Subiect feature could be eitherplural or singular in value.
But denominators can alsohave a range of values, and - more crucial for thepurposes of the demonstrat ion here - a feature canitself be a complex, hierarchically structured objectthat contains other features as sub-constituents.
Forexample, the " feature"  that eventually becomes associ-ated with the root node of a sentence is in fact anf-structure that represents the full propositional struc-ture of the sentence.
Thus if the surface string wasthe sentence, The girl promised to kiss the baby, thenthe f-structure associated with the root node of thesentence is a complex " feature"  that itself contains anembedded f-structure corresponding to the embeddedproposition the girl to kiss the baby.As mentioned, well-formedness i also determinedby functional equations, dictating (according to certainconventions) how feature complexes are to be assem-bled.
By and large the f-structure complex at a nodeX is assembled composit ionally in terms of the f-structure complexes of the nodes below it in the con-stituent structure tree.
For example, the root node ofa sentence will have an associated f-structure withSubject and Predicate sub-features.
These structuresare themselves complex - the entire Subject NP andVerb-Verb Complement structures, respectively.
Forinstance, the Subject NP in turn has a sub-featureNumber; the Predicate contains complex sub-featurescorresponding to the Verb and Verb Complements.The basic assembly directive is the notation (4=4) .
1?When attached to a particular node X, it states thatthe f-structure of the node above X is to share all thef-structure of the nodes be low X.
The effect is tomerge and "pass up" all the f-structure values of thenodes below X to the node above X.
One can alsopass along just particular subfields of the f-structurebelow X by specifying a subfield on the right-handside of the expansion rule.
As an example, the notion(+=+Number)  attached to a node X states that thef-structure of the node above X is to contain at leastthe value of the Number feature.
(This "value" mayitself be an f-structure.)
Similarly, a particular sub-field of the f-structure above a node X may be speci-fied by providing a subfield label on the left-hand sideof the arrow notation.
For  example, the notation,(+Sub ject=4)  means that the Subject subfield of thef-structure built at the node above X must contain thef-structure built below X.A basic constraint on f-structures is that the f-structure assembled at X must be uniquely determined;that is, it cannot contain a feature F 1 with conflictingvalues.
This entails, for example, that the Subjectsub-f-structure that is built at a root-S node cannothave a Number  sub-field that is filled in from oneplace beneath with the value Singular and from anoth-er place with the value Plural.
More generally, thisrestriction means that two or more f-structures thatare "passed up" from below according to the dictatesof an arrow notation at a single node above must beun i f iab le  - any common sub-fields, no matter howhierarchically complex, must be mergeable withoutconflict.For example, consider Subject-Verb agreement andthe sentence the baby is kissin~ John.
The lexicalentry for baby (considered as a Noun) might have theNumber feature, with the value singular.
The lexicalentry for is might assert that the number feature of theSubiect above it in the parse tree must  have the valuesingular, via the annotation (+Subject=singular)  atta-ched to the verb.
Meanwhile, the feature values forSubject are automatical ly found by the annotat ion(+Sub ject=4)  associated with the Noun Phrase por-tion of S=>NP VP) that grabs whatever features itfinds below the NP node and copies them up above tothe S node.
Thus the S node gets the Subject featurewith whatever value it has passed from baby below -namely, the value singular; this accords with the dic-tates of the verb is___z, and all is well.
In contrast, in thesentence, th_.__~e boys in the band is kissin~ John, boyspasses up the number value plural, and this clasheswith the verb's constraint; as a result this sentence isjudged ill-formed, as Figure 2 shows.10 More generally, the assembly directive is specified via thenotation (+featl=Sfeat2), where feat1 and feat2 are meta-variables pecifying a subfield of the f-structure immediately aboveor below the node to which the the annotation is attached.
If nofield is given, then the entire f-structure is assumed.
For example,the notation (+ Subject Number= 4) attached to a node X meansthat the Number subfield of the Subject subfield of the f-structureassociated with the node above X is to be filled in with the value ofthe entire f-structure below X.102 American Journal of Computational Linguistics, Volume 8, Number 3-4, Ju ly -December 1982Robert C. Berwick Computational Complexity and LexicaI-Functional GrammarS Subject \[Number:Singular or Plural?\] / \=NP VP\[Number:plural\] \[Number: singular\]L-  /the boys in the band is kissing JohnFigure 2.
Co-occurrence r strictions are enforced by feature checking in a Lexical-Functional Grammar.It is important o note that the feature compatibili-ty check requires (1) a particular constituent structuretree (a parse tree); and (2) an assignment of terminalitems (words) to lexical categories - e.g., in the firstSubject-Verb agreement example above, baby wasassigned to the category N, a Noun.
The tree is obvi-ously required because the feature-checking machinerypropagates values according to the links specified bythe derivation tree; the assignment of terminal items tocategories is crucial because in most cases the valuesof features are derived from those listed in the lexicalentry for an item (as the value of the number featurewas derived from the lexical entry for the Noun formof baby).
One and the same terminal item can havetwo distinct lexical entries, corresponding to distinctlexical categorizations; for example, baby can be botha Noun and a Verb.
If we had picked baby to be aVerb, and hence had adopted whatever features areassociated with the Verb entry for baby to be propa-gated up the tree, then the string that was previouslywell-formed, th.__~e baby is kissin 8 John, would now beconsidered eviant.
If a string is ill-formed under allpossible derivation trees and assignments of featuresfrom possible lexical categorizations, than that string isnot in the language generated by the LFG.
The abilityto have multiple derivation trees and lexical categori-zations for one and the same terminal item plays acrucial role in the reduction proof: it is intended tocapture the satisfiability problem of deciding whetherto given an atom X i a value of "T"  or "F" .Finally, LFG also provides a way to express thefamiliar patterning of grammatical relations (e.g.,"Subject"  and "Object" )  found in natural language.For example, transitive verbs must have objects.
Thisfact of life (expressed in an Aspects-style Transforma-tional Grammar by subcategorization restrictions) iscaptured in LFG by specifying a so-called PRED (forpredicate) feature with a Verb; the PRED can describewhat grammatical relations like "Subject"  and"Object"  must be filled in after feature passing hastaken place in order for the analysis to be well-formed.
For instance, a transitive verb like kiss mighthave the pattern, k iss<(Subject)(Obiect)> , and thusdemand that the Subject and Object (now consideredto be "features")  have some value in the final analysis.The values for Subject and Object might of course beprovided from some other branch of the parse tree, asprovided by the feature propagation machinery; forexample, the Object feature could be filled in from theNoun Phrase part of the VP expansion.
See Figure 3.But if the Object were not filled in, then the analysis isdeclared functionally incomplete, and is ruled out.
Thisdevice is used to cast out sentences uch as th._ee babykissed.So much for the LFG machinery that is required forthe reduction proof.
(There are additional capabilitiesin the LFG theory, such as long-distance binding, butthese will not be called upon in the demonstrationbelow.
)What then does the LFG representation of the CNFsatisfiability problem look like?
Basically, there arethree parts to the satisfiability problem that must bemimicked by the LFG: (1) the assignment of values toatoms, e.g., X2=>"T" ;  X4=>"F" ;  (2) the consisten-cy of value assignments in the formula; e.g., the atomX 2 can appear in several different terms, but one isnot allowed to assign it the value "T"  in one term andthe value "F"  in another; and (3) the preservation ofCNF satisfiability, in that a string will be in the LFGlanguage to be defined just in case its associated CNFformula is satisfiable.
Let us now go over how thesecomponents may be reproduced in an LFG, one byone.
(1) Assignments: The input string to be testedfor membership in the LFG will simply be the originalformula, sans parentheses and the operators A and V;the terminal items are thus just a string of Xi's.
Recallthat the job of checking the string for well-formednessinvolves finding a derivation tree for the string, solvingthe ancillary co-occurrence equations (by feature pro-pagation), and checking for functional completeness.Now, the context-free grammar constructed by thetransformation procedure will be set up so as to gener-ate a virtual copy of the associated formula, down tothe point where literals X i are assigned their value of"T"  or "F" .
If the original 3-CNF form had n terms,then denoting each by the symbol Ep, p=l ,  ..., n, thispart of the grammar would look like the following:US => E 1 E 2 ... E nEp=> Y iY jYkAmerican Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 103Robert C. Berwiek Computational Complexity and LexicaI-Functional GrammarS ~ features:NP VPI /NSue V NPI Ikiss JohnSubject : SuePred ?
'k iss< (Subject) (Object) >'Object : JohnFigure 3.
Predicate templates can demand that a subject or object be filled in.
(p= 1,2,...,n)The subscripts i, j, and k correspond to the actualsubscripts in the original formula.
Further, the Yi arenot terminal items, but are non-terminals that will beexpanded into one of the non-terminals T i o r  Fi.12Note that so far there are no rules to extend theparse tree down to the level of terminal items, namethe X i.
The next step does this and at the same timeadds the power to choose between "T"  and "F"  as-signments to atoms.
One adds to the context- freebase grammar two productions deriving each terminalitem X i, namely, T i=>X i and Fi - ->Xi,  correspondingto an assignment of "T"  or "F"  to the atoms of theformula (it is important not to get confused here be-tween the atoms of the formula - these are terminalelements in the Lexical-Functional Grammar - and thenon-terminals of the grammar.)
Plainly, one must alsoadd the rules Y i=>Ti \ ]  Fi, for each i, and rules corre-sponding to the assignment of truth-values to the neg-ations of literals, T i=>X i and Fi--->X i.
Note thatthese are not "exot ic"  LFG rules: exactly the samesort of rule is required in the baby case, i.e.,N=>baby or V=>baby,  corresponding to whetherbaby is a Noun or a Verb.
Now, the lexical entries forthe "T i"  categorization of X i will look very differentfrom the "Fi"  categorization of Xi, just as one mightexpect the N and V forms for baby to be different.Here is what the entries for the two categorizations ofX i look like:Xi: T i (+Truth-assignment) =T(+Assign X i) =T11 The context-free base that is built depends upon the origi-nal CNF  formula that is input, since the number of terms, n, variesfrom formula to formula.
In Stanley Peters's improved version ofthe reduction proof \[personal communication\], the context-free baseis fixed for all formulas with the rules:S=>S S"S '=>T T T or T T F or T F F or T F T or ...(remaining twelve triples containing at least one "T")The Peters grammar works by recursing until the right number ofterms is generated (any sentences that are too long or too shortcannot be matched to the input formula).
Thus, the number ofterms in the original CNF  formula need not be explicitly encodedinto the base grammar.Xi: F i (+Assign Xi) =FPutting aside for the moment the "Truth-ass ignment"feature in this entry, the feature assignments for thenegation of the literal X i must be the complement ofthis entry:Xi: T i (+Truth-assignment) =T(+Assign X i) =FXi: F i (+Assign X i) =TThe upward-directed arrows in the entries reflectthe LFG feature propagation machine.
Remember  thatT i and F i are just non-terminal categories, like Nounand Verb.
For example, if the T i categorization for X iis selected, the entry says to "make theTruth-assignment feature of the node above T i havethe value T, and make the X i portion of the Assignfeature of the node above have the value T ."
Thisfeature propagation device reproduces the assignmentof T's and F's to the CNF literals.
If we have a tripleof such elements, and at least one of them is expandedout to Ti, then the feature propagation machinery ofLFG will merge the common feature names into onelarge structure for the node above, reflecting the as-signments made; moreover, the term will get a filled-intruth assignment value just in case at least one of theexpansions elected a T i path.
This is depicted in Fig-ure 4.
Features are passed transparently through theintervening Yi nodes via the LFG "copy"  device,(+ = +); this simply means that all the features of thenode below the node to which the "copy"  up-and-down arrows are attached are to be the same as thoseof the node above the up-and-down arrows.It should be plain that this mechanism mimics theassignment of values to literals required by the satisfi-ability problem.
(2) Coordination of assignments: One must alsoguarantee that the X i value assigned at one place inthe tree is not contradicted by the value of an X i or X ielsewhere.
To ensure this, we use the LFG co-12 This grammar will have to be slightly modified in order forthe reduction to work, as will become apparent shortly104 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982Robert C. Berwick Computational Complexity and LexicaI-Functional Grammarterminalstring:ures:Yii i  ~J ~ kX i Xj X kITruth-Assignment _=T \[Xi=T\] l \ ]\[Xj=F1 IIl Assign\[Xg=F\]JJFigure 4.
The LFG feature propagation machinery is used to percolate f ature assignments from the lexicon.occurrence agreement machinery: the Assign feature-bundle is passed up from each term to the highestnode in the parse tree (one simply adds the (+=4,)notation to each E i rule in order to indicate this).
TheAssign feature at this node will contain the union of allassign feature bundles passed up by all terms.
If anyX i values conflict, then the resulting structure isjudged ill-formed.
Thus, only compatible X i assign-ments are well-formed.
Figure 5 depicts this situation.
(3) Preservation of satisfying assignments: Final-ly, one has to reproduce the conjunctive character ofthe 3-CNF problem - that is, a sentence is satisfiable(well-formed) if and only if each term has at least oneliteral assigned the value "T" .
Part of the disjunctivecharacter of the problem has already been encoded inthe feature propagation machinery presented so far; ifat least one X i in a term E 1 expands to the lexicalcategorization Ti, then the Truth-assignment featuregets the value T. This is just as desired.
If one, two,or three of the literals X i in a term select Ti, then El'STruth-assignment feature is T, and the analysis is well-formed.
But how do we rule out the case where allthree Xi's in a term select the "F"  path, Fi?
And howdo we ensure that all terms have at least one T belowthem?Both of these problems can be solved by resortingto the LFG functional completeness constraint.
Thetrick is to add a Pred feature to a "dummy" node atta-ched to each term; the sole purpose of this feature willbe to refer to the feature Truth-assisnment, just as thepredicate template for the transitive verb kiss mentionsthe feature Obiect.
Since an analysis is not well-formed if the "grammatical relations" a Pred mentionsare not filled in from somewhere, this will have theeffect of forcing the Truth-assignment feature to getfilled in every term.
Since the "F"  lexical entry doesnot have a Truth-assignment value, if all the Xi's in aterm triple select the F i path (all the literals are "F" ) ,then no Truth-assignment feature is ever picked upfrom the lexical entries, and that term never gets avalue for the Truth-assignment feature.
This violateswhat the predicate template demands, and so thewhole analysis is thrown out.
(The ill-formedness isexactly analogous to the case where a transitive verbnever gets an Object.)
Since this condition is appliedto each term, we have now guaranteed that each termmust have at least one literal below it that selects the"T"  path - just as desired.
To actually add the newpredicate template, one simply adds a new (but dum-my) branch to each term El, with the appropriatepredicate constraint attached to it.
See Figure 6.S features: / \~1 Ejk~7 ~77 F7 \X7 X7~Axssign 17=T or F?
dClash!
(+Assign XT=T ) (+Assign XT=F )Figure 5.
The feature compatibility machinery ofLFG can force assignments to be co-ordinated across terms.American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 105Robert C. Berwick Computational Complexity and LexicaI-Functional Grammarlexical entryfor 'dummy2':( ?
Pred) ='dummy2<(?Truth-ass ignment)  > 'E lDummy2 y, y~ y<~ i Fj F kXifeatures: \ [Pred='dummy2 < ( ?
Truth-assignment) > '\]"\[Truth-assignment = T\]( + Truth-assignment) = TFigure 6.
Predicates can be used to force at least one "T" per term.There is one final subtle point here: one must alsoprevent the Pred and Truth-assignment features foreach term from being passed up to the head "S"  node.The reason is that if these features were passed up,then, since the LFG machinery automatical ly mergesthe values of any features with the same name at thetopmost node of the parse tree, the LFG machinerywould force the union of the feature values for Predand Truth-assignment over all terms in the analysistree.
The result would be that if any term had at leastone "T"  (hence satisfying the Truth-assignment predi-cate template in at least one term), then the Pred andTruth-assignment features would get filled in at thetopmost node as well.
The string below would bewell-formed if at least one term were "T" ,  and thiswould amount to a disjunction of disjunctions (an"OR"  of "OR 's ) ,  not quite what is sought.
To elimi-nate this possibility, one must add a final trick: eachterm E 1 is given separate Pred, Truth-assignment, andAssign features, but only the Assign feature is propa-gated to the highest node in the parse tree as such.
Incontrast, the Pred and Truth-assignment features foreach term are kept "protected" from merger by storingthem under separate feature headings labeled E 1 ..... E n.The means by which just the Assign feature bundle islifted out is the LFG analogue of the natural languagephenomenon of Subject or Object "control" ,  wherebyjust the features of the Subject or Object of a lowerclause are lifted out of the lower clause to become theSubject of Object of a matrix sentence; the remainingfeatures stay unmergeable because they stay protectedbehind the individually labeled terms.To actually " implement"  this in an LFG, one canadd two new branches to each term expansion in thebase context-free grammar, as well as two "control"equation specifications that do the actual work of lift-ing the features from a lower clause to the matrix sen-tence.
A natural language example of this phenome-non is the following (from Kaplan and Bresnan 1981,pp.
43-45):The girl persuaded the baby to go.
(part of the)lexical entry forpersuaded: V (?
Vcomp Subject) = (?
Object)According to this lexical entry, the Object featurestructure of a root sentence containing a verb likepersuade is to be the same as the feature structure ofthe Subject of the Complement  of persuade - a"control"  equation.
Since this Subject is the baby,this means that the features associated with the NP thebaby are shared with the features of the Object of thematrix sentence.The satisfiability analogue of this machinery isquite similar to this; see Figure 7.As Figure 7 shows, a "control  equation" should beattached to the A i node that forces the Assign featurebundle from the C i side to be lifted up and ultimatelymerged into the Assign feature bundle of the E 1 node(and then, in turn, to become merged at the topmostnode of the tree by the usual full copy up-and-downarrows):( + CiAssign )= ?
Assign)The satisfiability analogue is just like the sharing ofthe Subject features of a Verb Complement with theObject position of a matrix clause.To finish off the reduction argument, it must beshown that, given any 3-CNF formula, the correspond-ing LFG grammar and string as just described can beconstructed in a time that is a polynomial function ofthe length of the original input formula.
This is not adifficult task, and only an informal sketch of how itcan be done will be given.
All one has to do is scanthe original formula from left to right, outputting anappropriate cluster of base rules as each triple of liter-als is scanned: E i=>AiC i ;  C i=>Dummy2 YiYjYk;Y i=>Ti lF i  (similarly for Yj and Yk); T i=>Xi ,F i=>X i (similarly for Tj and Tk).
Note that for eachtriple of literals in the original input formula the ap-propriate grammar ules can be output in an amount oftime that is just a constant times n. In addition, onemust also maintain a counter to keep track of the106 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982Robert C. Berwick Computational Complexity and LexicaI-Functional GrammarPhrase structure tree:Dummy2 Yi  Y j  YkFigure 7.
Phrase structure tree required to implement control .equations in CNF  analogue of natural anguage case.number of triples so far encountered.
This adds atmost a logarithmic factor, to do the actual counting.At the end of processing the input formula, one mustalso output the rule S=>E1E2,.. .
,Em, where m is thenumber of triples in the CNF formula.
Since rn is lessthan n, this procedure too is easily seen to take timethat is a polynomial function of the length of the origi-nal input formula.
Finally, one must also construct helexical entry for each X i and X i.
This too can be doneas the input formula is scanned left to right.
The onlydifficulty here is that one must check to see if theentry for each X i has been previously constructed.
Inthe worst case, this involves rescanning the list of lexi-cal entries built so far.
Since there are at most n suchentries, and since the time to actually output a singleentry is constant, at worst the time spent constructinga single lexical entry could be proportional to n. Thusfor n entries the total time spent in construction couldbe at most of order n 2.
Since the time to construct heentire grammar is just the sum of the times spent inconstructing its production rules and its lexicon, thetotal time to transform the input formula is boundedabove by some constant imes n 2.4.
Relevance of Complexity Results and ConclusionsThe demonstrat ion of the previous section showsthat LFGs have enough power to "simulate" a proba-bly computationally intractable problem.
But what arewe to make of this result?
On the positive side, acomplexity result such as this one places the LFG the-ory more precisely in the complexity hierarchy.
If oneconjectures, as seems reasonable, that LFG languagerecognition is actually in the class NP (that is, LFGrecognition can be done by a non-deterministic Turingmachine in polynomial time), then the LFG languagesare NP-complete.
This is a plausible conjecture be-cause a non-determinist ic Turing machine should beable to "guess" all candidate feature propagation solu-tions using its non-deterministic power, including any" long-distance" binding solutions (an LFG device notdiscussed in this paper).
Since checking candidatesolutions is quite rapid - it can be done in n 2 time orless, as described by Kaplan and Bresnan 1981 - rec-ognition should be possible in polynomial time on sucha machine.
Comparing this result to other knownlanguage classes, note that context-sensitive languagerecognition is in the class of polynomial space("PSPACE"),  since (non-determinist ic) linear spacebounded automata generate exactly the class ofcontext-sensitive languages.
(As is well known, forpolynomial space the deterministic and non-deterministic classes collapse together because ofSavitch's results (see Hopcroft  and Ullman 1979) thatany function computable in non-deterministic space Ncan be computed in deterministic space N2.)
Further-more, the class NP is clearly a subset of PSPACE(since if a function uses space N it must use at leasttime N) and it is suspected, but not known for certain,that NP is a proper subset of PSPACE (this being theP=NP question once again).
Our conclusion is that itis likely that LFGs can generate only a proper subset ofthe context-sensit ive languages.
This speculation ishighly suggestive, in that several other "natural"  ex-tensions of the context-free languages - notably theclass of languages generated by the so-called "indexedgrammars"  - also generate strict subsets of thecontext-sensit ive languages, including those strictlycontext-sensitive languages hown to be generable byLFGs by Kaplan and Bresnan 1981.
The class of in-dexed languages is also known to be NP-complete (seeRounds 1973).
Indeed, a cursory look at the power ofindexed grammars uggests that they might subsumethe machinery of the LFG theory, t3 On the other sideof the coin, how might one restrict the LFG theoryfurther so as to avoid potential intractability?
Severalescape hatches come to mind; these will simply belisted here.
Note that all of these "f ixes" have theeffect of supplying additional constraints to furtherrestrict the LFG theory.
In this respect, the LFG com-plexity demonstration presented here plays the samerole as, say Peters and Ritchie's earlier result aboutTransformational Grammars: it shows that the theory13 For a formal discussion of this possibil ity, see Berwick1981.
Note added in proof: This can be shown to be false, howev-er; LFGs can generate non- indexed languages.American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982 107Robert C. Berwick Computational Complexity and LexicaI-Functional Grammarmust be tightened i f  one wants to avoid computationalintractability.1.
Rule out "worst case" languages as linguistically(that is, empirically) irrelevant.The probable computational intractability of LFG rec-ognition arises because co-occurrence restrictions(compatible X i assignments) can be expressed acrossarbitrary stretches of the terminal string in conjunctionwith potential categorial ambiguity for each terminalitem.
If some device can be found in natural lan-guages that always filters out or removes uch ambigu-ity locally (so that the choice of whether an item is"T"  or "F"  never depends on other items arbitrarilyfar away in the terminal string), or if natural languagesnever employ such kinds of co-occurrence restrictions,then the reduction is theoretically valid but linguisti-cally irrelevant.
Note that such a finding would be apositive result, since one would be able to further nar-row the LFG theory in its attempt to characterize alland only the natural languages.
This discovery wouldbe on a par with, for example, Peters and Ritchie'sobservation that although the context-sensitive phrasestructure rules formally advanced in linguistic theoryhave the power to generate non-context- f ree lan-guages, this power has apparently never been used inthe grammars that linguists have designed (see Petersand Ritchie 1973b).2.
Add "locality principles" for recognition (or parsing).One could simply stipulate that LFG languages mustmeet additional conditions known to ensure efficientparsabil ity, e.g., Knuth's  LR(k) restriction, suitablyextended to handle the LFG case.
This approach istypified by Marcus's 1980 work, which hypothesizedthat people normally construct only a single derivationfor any given sentence, and proposed other conditionsthat turn out to guarantee that Knuth's LR(k) restric-tion will hold.
(See Berwick 1982 for further discus-sion.)3.
Restrict the lexicon.The reduction argument crucially depends upon havingan infinite stock of lexical items and an infinite num-ber of features with which to label them.
This is nec-essary because as CNF formulas grow larger and larg-er, the number of distinct literals can grow arbitrarilylarge, and one requires an arbitrarily large number ofdistinct X i features to check for co-occurrence condi-tions.
If, for whatever reason, the stock of lexicalitems or feature labels is finite, then the reductionmethod works for only finite-sized problems.
Thisrestriction seems ad hoc in the case of lexical items(why can't  there be an infinite number of words?)
butless so in the case of features.
(If features required"grounding" in terms of other sub-systems of knowl-edge, e.g, if a feature had to be in the spanning set ofa finite number of some hypothetical cognitive orsensory-motor basis elements, then the total numberof features would be finite.)
t4Of course, constraints may be drawn from all threeof these general classes in order to make the LFG the-ory computationally tractable.
Even then, it remainsto be seen what additional constraints would be re-quired in order to guarantee that LFG recognitiontakes only a small amount of polynomial time - e.g,cubic time or less, as for context-free language recog-nition.
Here it may well turn out to be the case thatsomething like the LR(k) restrictions uffice.AcknowledgmentsI would like to thank Ron Kaplan, Ray Perrault,Christos Papadimitriou, and Stanley Peters for variousdiscussions about the contents of this paper.This report describes research done at the ArtificialIntelligence Laboratory of the Massachusetts Instituteof Technology.
Support for the Laboratory 's  artificialintelligence research is provided in part by the Officeof Naval Research under ONR contract N00014-80C-0505.ReferencesAho, A.
1968 Indexed grammars - an extension of context-freegrammars.
Journal of the ACM 15:4 647-671.Berwick, R. 1981 The formal language theory of Lexical-Functional Grammar.
Talk given at the Linguistic Society ofAmerica Annual Meeting, December 1981.Berwick, R. 1982 Locality Principles and the Acquisition of Syn-tactic Knowledge.
Ph.D. dissertation.
Cambridge, MA: MITDepartment of Computer Science and Electrical Engineering.Berwick, R. and Weinberg, A.
1982 Parsing efficiency, computa-tional complexity, and the evaluation of grammatical theories.Linguistic Inquiry 13:1 165-191.Berwick R. and Weinberg A.
1983 The role of grammars in mod-els of language use.
Cognition, 13:1, 1-61.Berwick, R. and Weinberg, A. forthcoming The Grammatical Basisof Linguistic Performance: Language Use and Acquisition.
Cam-bridge, MA: MIT Press.Chomsky, N. 1965 Aspects of the Theory of Syntax.
Cambridge,MA: MIT Press.Chomsky, N. 1980 Rules and Representations.
New York: Colum-bia University Press.Chomsky, N. 1981 Lectures on Government and Binding.
Dor-drecht: Foris Publications.Fodor, J., Bever, T., and Garrett, M. 1974 The Psychology ofLanguage.
New York: McGraw-Hill.Hopcroft, J. and Ullman, J.
1979 Introduction to Automata Theory,Languages, and Computation.
Reading, MA: Addison-Wesley.Kaplan, R. and Bresnan, J.
1981 Lexical-functional Grammar: AFormal System for Grammatical Representation.
Cambridge,MA: MIT Center for Cognitive Science Occasional Paper #13.
(Also forthcoming in Bresnan, J., ed., The Mental Representationof Grammatical Relations.
Cambridge, MA: MIT Press.
)14 See for example Pinker 1980, where this claim is made forLexical-Functional Grammar.
Still, it is most natural to assume thatthere is a potentially unbounded number of lexical items - all that isrequired for the reduction.108 American Journal of Computational Linguistics, Volume 8, Number 3-4, July-December 1982Robert C. Berwick Computational Complexity and LexicaI-Functional GrammarKnuth, D. 1965 On the translation of languages from left to right.Information and Control 8 607-639.Marcus, M. 1980 A Theory of Syntactic Recognition for NaturalLanguage.
Cambridge, MA: MIT Press.Peters, S. and Ritchie, R. 1973a On the generative power oftransformational grammars.
Information Sciences 6 49-83.Peters, S. and Ritchie, R. 1973b Context-sensitive immediateconstituent analysis: context-free languages revisited.
Mathemat-ical Systems Theory 6:4 324-333.Petrick, S. 1965 A Recognition Procedure for TransformationalGrammar.
Ph.D. dissertation.
Cambridge, MA: MIT Depart-ment of Linguistics.Pinker, S. 1980 A Theory of the Acquisition of Lexical-Interpretive Grammars.
Cambridge, MA: MIT Center forCognitive Science Occasional Paper #6.
(Also forthcoming inBresnan, J., ed., The Mental Representation of GrammaticalRelations.
Cambridge, MA: MIT Press.Rounds, W. 1973 Complexity of recognition in intermediate-levellanguages.
Proceedings of the 14th Annual Symposium on Switch-ing Theory and Automata.
145-158.Rounds, W. 1975 A grammatical characterization f exponential-time languages.
Proceedings of the 16th Annual Symposium onSwitching Theory and Automata.
135-143.American Journal of Computational Linguistics, Volume 8, Number 3-4, Ju ly-December 1982 109
