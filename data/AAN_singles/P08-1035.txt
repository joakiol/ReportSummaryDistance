Proceedings of ACL-08: HLT, pages 299?307,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsA Generic Sentence Trimmer with CRFsTadashi NomotoNational Institute of Japanese Literature10-3, Midori TachikawaTokyo, 190-0014, Japannomoto@acm.orgAbstractThe paper presents a novel sentence trimmerin Japanese, which combines a non-statisticalyet generic tree generation model and Con-ditional Random Fields (CRFs), to addressimproving the grammaticality of compres-sion while retaining its relevance.
Experi-ments found that the present approach out-performs in grammaticality and in relevancea dependency-centric approach (Oguro et al,2000; Morooka et al, 2004; Yamagata et al,2006; Fukutomi et al, 2007)?
the only line ofwork in prior literature (on Japanese compres-sion) we are aware of that allows replicationand permits a direct comparison.1 IntroductionFor better or worse, much of prior work on sentencecompression (Riezler et al, 2003; McDonald, 2006;Turner and Charniak, 2005) turned to a single cor-pus developed by Knight and Marcu (2002) (K&M,henceforth) for evaluating their approaches.The K&M corpus is a moderately sized corpusconsisting of 1,087 pairs of sentence and compres-sion, which account for about 2% of a Ziff-Daviscollection from which it was derived.
Despite itslimited scale, prior work in sentence compressionrelied heavily on this particular corpus for establish-ing results (Turner and Charniak, 2005; McDonald,2006; Clarke and Lapata, 2006; Galley and McKe-own, 2007).
It was not until recently that researchersstarted to turn attention to an alternative approachwhich does not require supervised data (Turner andCharniak, 2005).Our approach is broadly in line with prior work(Jing, 2000; Dorr et al, 2003; Riezler et al, 2003;Clarke and Lapata, 2006), in that we make use ofsome form of syntactic knowledge to constrain com-pressions we generate.
What sets this work apartfrom them, however, is a novel use we make ofConditional Random Fields (CRFs) to select amongpossible compressions (Lafferty et al, 2001; Sut-ton and McCallum, 2006).
An obvious benefit ofusing CRFs for sentence compression is that themodel provides a general (and principled) proba-bilistic framework which permits information fromvarious sources to be integrated towards compress-ing sentence, a property K&M do not share.Nonetheless, there is some cost that comes withthe straightforward use of CRFs as a discriminativeclassifier in sentence compression; its outputs areoften ungrammatical and it allows no control overthe length of compression they generates (Nomoto,2007).
We tackle the issues by harnessing CRFswith what we might call dependency truncation,whose goal is to restrict CRFs to working with can-didates that conform to the grammar.Thus, unlike McDonald (2006), Clarke and Lap-ata (2006) and Cohn and Lapata (2007), we do notinsist on finding a globally optimal solution in thespace of 2n possible compressions for an n wordlong sentence.
Rather we insist on finding a mostplausible compression among those that are explic-itly warranted by the grammar.Later in the paper, we will introduce an approachcalled the ?Dependency Path Model?
(DPM) fromthe previous literature (Section 4), which purports toprovide a robust framework for sentence compres-299sion in Japanese.
We will look at how the presentapproach compares with that of DPM in Section 6.2 A Sentence Trimmer with CRFsOur idea on how to make CRFs comply with gram-mar is quite simple: we focus on only those la-bel sequences that are associated with grammati-cally correct compressions, by making CRFs lookat only those that comply with some grammaticalconstraints G, and ignore others, regardless of howprobable they are.1 But how do we find compres-sions that are grammatical?
To address the issue,rather than resort to statistical generation models asin the previous literature (Cohn and Lapata, 2007;Galley and McKeown, 2007), we pursue a particularrule-based approach we call a ?dependency trunca-tion,?
which as we will see, gives us a greater controlover the form that compression takes.Let us denote a set of label assignments for S thatsatisfy constraints, by G(S).2 We seek to solve thefollowing,y?
= arg maxy?G(S)p(y|x;?).
(2)There would be a number of ways to go about theproblem.
In the context of sentence compression, alinear programming based approach such as Clarkeand Lapata (2006) is certainly one that deserves con-sideration.
In this paper, however, we will explore amuch simpler approach which does not require asinvolved formulation as Clarke and Lapata (2006)do.We approach the problem extentionally, i.e.,through generating sentences that are grammatical,or that conform to whatever constraints there are.1Assume as usual that CRFs take the form,p(y|x) ?expPk,j ?jfj(yk, yk?1,x) +Pi ?igi(xk, yk,x)!= exp[w?f(x,y)](1)fj and gi are ?features?
associated with edges and vertices, re-spectively, and k ?
C, where C denotes a set of cliques in CRFs.
?j and ?i are the weights for corresponding features.
w and fare vector representations of weights and features, respectively(Tasker, 2004).2Note that a sentence compression can be represented as anarray of binary labels, one of themmarking words to be retainedin compression and the other those to be dropped.SVN PNVNA D JN PNV NFigure 1: Syntactic structure in JapaneseConsider the following.
(3) Mushoku-nounemployedJohnJohn-gaSBJtakaiexpensivekurumacar-woACCkat-ta.buy PAST?John, who is unemployed, bought anexpensive car.
?whose grammatically legitimate compressionswould include:(4) (a) John -ga takai kuruma -wo kat-ta.
?John bought an expensive car.?
(b) John -ga kuruma -wo kat-ta.
?John bought a car.?
(c) Mushoku-no John -ga kuruma -wo kat-ta.
?John, who is unemployed, bought a car.
(d) John -ga kat-ta.
?John bought.?
(e) Mushoku-no John -ga kat-ta.
?John, who is unemployed, bought.?
(f) Takai kuruma-wo kat-ta.?
Bought an expensive car.?
(g) Kuruma-wo kat-ta.?
Bought a car.?
(h) Kat-ta.?
Bought.
?This would give us G(S)={a, b, c, d, e, f, g, h}, forthe input 3.
Whatever choice we make for compres-sion among candidates in G(S), should be gram-matical, since they all are.
One linguistic feature300B S 2B S 4B S 5B S 3B S 1N P VSFigure 2: Compressing an NP chunkCDEBAFigure 3: Trimming TDPsof the Japanese language we need to take into ac-count when generating compressions, is that the sen-tence, which is free of word order and verb-final,typically takes a left-branching structure as in Fig-ure 1, consisting of an array of morphological unitscalled bunsetsu (BS, henceforth).
A BS, which wemight regard as an inflected form (case marked in thecase of nouns) of verb, adjective, and noun, couldinvolve one or more independent linguistic elementssuch as noun, case particle, but acts as a morpholog-ical atom, in that it cannot be torn apart, or partiallydeleted, without compromising the grammaticality.3Noting that a Japanese sentence typically consistsof a sequence of case marked NPs and adjuncts, fol-lowed by a main verb at the end (or what wouldbe called ?matrix verb?
in linguistics), we seek tocompress each of the major chunks in the sentence,leaving untouched the matrix verb, as its removal of-ten leaves the sentence unintelligible.
In particular,starting with the leftmost BS in a major constituent,3Example 3 could be broken into BSs: / Mushuku -no / John-ga / takai / kuruma -wo / kat-ta /.we work up the tree by pruning BSs on our way up,which in general gives rise to grammatically legiti-mate compressions of various lengths (Figure 2).More specifically, we take the following steps toconstruct G(S).
Let S = ABCDE.
Assume thatit has a dependency structure as in Figure 3.
Webegin by locating terminal nodes, i.e., those whichhave no incoming edges, depicted as filled circlesin Figure 3, and find a dependency (singly linked)path from each terminal node to the root, or a nodelabeled ?E?
here, which would give us two pathsp1 = A-C-D-E and p2 = B-C-D-E (call them ter-minating dependency paths, or TDPs).
Now createa set T of all trimmings, or suffixes of each TDP,including an empty string:T (p1) = {<A C D E>, <C D E>, <D E>, <E>, <>}T (p2) = {<B C D E>, <C D E>, <D E>, <E>, <>}Then we merge subpaths from the two sets in everypossible way, i.e., for any two subpaths t1 ?
T (p1)and t2 ?
T (p2), we take a union over nodes in t1 andt2; Figure 4 shows how this might done.
We removeduplicates if any.
This would give us G(S)={{A B CD E}, {A C D E}, {B C D E}, {C D E}, {D E}, {E},{}}, a set of compressions over S based on TDPs.What is interesting about the idea is that creatingG(S) does not involve much of anything that is spe-cific to a given language.
Indeed this could be doneon English as well.
Take for instance a sentence atthe top of Table 1, which is a slightly modified leadsentence from an article in the New York Times.
As-sume that we have a relevant dependency structureas shown in Figure 5, where we have three TDPs,i.e., one with southern, one with British and one withlethal.
Then G(S) would include those listed in Ta-ble 1.
A major difference from Japanese lies in thedirection in which a tree is branching out: right ver-sus left.4Having said this, we need to address some lan-guage specific constraints: in Japanese, for instance,we should keep a topic marked NP in compressionas its removal often leads to a decreased readability;and also it is grammatically wrong to start any com-pressed segment with sentence nominalizers such as4We stand in a marked contrast to previous ?grafting?
ap-proaches which more or less rely on an ad-hoc collectionof transformation rules to generate candidates (Riezler et al,2003).301Table 1: Hedge-clipping EnglishAn official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on Britishtroops in southern IraqAn official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on Britishtroops in IraqAn official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on BritishtroopsAn official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacks on troopsAn official was quoted yesterday as accusing Iran of supplying explosive technology used in lethal attacksAn official was quoted yesterday as accusing Iran of supplying explosive technology used in attacksAn official was quoted yesterday as accusing Iran of supplying explosive technologyAn official was quoted yesterday as accusing Iran of supplying technology< A C D E >< B C D E >< C D E >< D E >< E >< >{A B C D E }{A C D E }{A C D E }{A C D E }{A C D E }< D E >< B C D E >< C D E >< D E >< E >< >{B C D E }{C D E }{D E }{D E }{D E }< >< B C D E >< C D E >< D E >< E >< >{B C D E }{C D E }{D E }{E }{}< C D E >< B C D E >< C D E >< D E >< E >< >{B C D E }{C D E }{C D E }{C D E }{C D E }< E >< B C D E >< C D E >< D E >< E >< >{B C D E }{C D E }{D E }{E }{E }Figure 4: Combining TDP suffixes-koto and -no.
In English, we should keep a prepo-sition from being left dangling, as in An official wasquoted yesterday as accusing Iran of supplying tech-nology used in.
In any case, we need some extrarules on G(S) to take care of language specific is-sues (cf.
Vandeghinste and Pan (2004) for English).An important point about the dependency truncationis that for most of the time, a compression it gener-ates comes out reasonably grammatical, so the num-ber of ?extras?
should be small.Finally, in order for CRFs to work with the com-pressions, we need to translate them into a sequenceof binary labels, which involves labeling an elementtoken, bunsetsu or a word, with some label, e.g., 0for ?remove?
and 1 for ?retain,?
as in Figure 6.i ns o u t h e rnIr a qt r o o p sBritis hona t t a c k sle t h ali nu s e dFigure 5: An English dependency structure and TDPsConsider following compressions y1 to y4 forx = ?1?2?3?4?5?6.
?i denotes a bunsetsu (BS).?0?
marks a BS to be removed and ?1?
that to be re-tained.
?1 ?2 ?3 ?4 ?5 ?6y1 0 1 1 1 1 1y2 0 0 1 1 1 1y3 0 0 0 0 0 1y4 0 0 1 0 0 0Assume that G(S) = {y1,y2,y3}.
Because y4is not part of G(S), it is not considered a candidatefor a compression for y, even if its likelihood mayexceed those of others in G(S).
We note that theapproach here does not rely on so much of CRFsas a discriminative classifier as CRFs as a strategyfor ranking among a limited set of label sequenceswhich correspond to syntactically plausible simpli-fications of input sentence.Furthermore, we could dictate the length of com-pression by putbting an additional constraint on out-302S0 0010 001Figure 6: Compression in binary representation.put, as in:y?
= arg maxy?G?(S)p(y|x;?
), (5)where G?
(S) = {y : y ?
G(S), R(y,x) = r}.R(y,x) denotes a compression rate r for which y isdesired, where r = # of 1 in ylength of x .
The constraint forcesthe trimmer to look for the best solution among can-didates that satisfy the constraint, ignoring those thatdo not.5Another point to note is thatG(S) is finite and rel-atively small ?
it was found, for our domain, G(S)usually runs somewhere between a few hundred andten thousand in length ?
so in practice it sufficesthat we visit each compression in G(S), and selectone that gives the maximum value for the objectivefunction.
We will have more to say about the size ofthe search space in Section 6.3 Features in CRFsWe use an array of features in CRFs which are ei-ther derived or borrowed from the taxonomy thata Japanese tokenizer called JUMAN and KNP,6 aJapanese dependency parser (aka Kurohashi-NagaoParser), make use of in characterizing the outputthey produce: both JUMAN and KNP are part of thecompression model we build.Features come in three varieties: semantic, mor-phological and syntactic.
Semantic features are usedfor classifying entities into semantic types such asname of person, organization, or place, while syn-tactic features characterize the kinds of dependency5It is worth noting that the present approach can be recastinto one based on ?constraint relaxation?
(Tromble and Eisner,2006).6http://nlp.kuee.kyoto-u.ac.jp/nl-resource/top-e.htmlrelations that hold among BSs such as whether a BSis of the type that combines with the verb (renyou),or of the type that combines with the noun (rentai),etc.A morphological feature could be thought of assomething that broadly corresponds to an EnglishPOS, marking for some syntactic or morphologicalcategory such as noun, verb, numeral, etc.
Alsowe included ngram features to encode the lexi-cal context in which a given morpheme appears.Thus we might have something like: for somewords (morphemes) w1, w2, and w3, fw1?w2(w3) =1 if w3 is preceded by w1, w2; otherwise, 0.
In ad-dition, we make use of an IR-related feature, whosejob is to indicate whether a given morpheme in theinput appears in the title of an associated article.The motivation for the feature is obviously to iden-tify concepts relevant to, or unique to the associ-ated article.
Also included was a feature on tfidf,to mark words that are conceptually more importantthan others.
The number of features came to around80,000 for the corpus we used in the experiment.4 The Dependency Path ModelIn what follows, we will describe somewhat indetail a prior approach to sentence compressionin Japanese which we call the ?dependency pathmodel,?
or DPM.
DPM was first introduced in(Oguro et al, 2000), later explored by a number ofpeople (Morooka et al, 2004; Yamagata et al, 2006;Fukutomi et al, 2007).7DPM has the form:h(y) = ?f(y) + (1 ?
?
)g(y), (6)where y = ?0, ?1, .
.
.
, ?n?1, i.e., a compressionconsisting of any number of bunsetsu?s, or phrase-like elements.
f(?)
measures the relevance of con-tent in y; and g(?)
the fluency of text.
?
is to providea way of weighing up contributions from each com-ponent.We further define:f(y) =n?1?i=0q(?i), (7)7Kikuchi et al (2003) explore an approach similar to DPM.303d i s a p p e a r e dd o g sf r o mT hr e e l e g g e ds i gh tFigure 7: A dependency structureandg(y) = maxsn?2?i=0p(?i, ?s(i)).
(8)q(?)
is meant to quantify how worthy of inclusionin compression, a given bunsetsu is; and p(?i, ?j)represents the connectivity strength of dependencyrelation between ?i and ?j .
s(?)
is a linking functionthat associates with a bunsetsu any one of those thatfollows it.
g(y) thus represents a set of linked edgesthat, if combined, give the largest probability for y.Dependency path length (DL) refers to the num-ber of (singly linked) dependency relations (oredges) that span two bunsetsu?s.
Consider the de-pendency tree in Figure 7, which corresponds toa somewhat contrived sentence ?Three-legged dogsdisappeared from sight.?
Take an English word for abunsetsu here.
We haveDL(three-legged, dogs) = 1DL(three-legged, disappeared) = 2DL(three-legged, from) = ?DL(three-legged, sight) = ?Since dogs is one edge away from three-legged, DLfor them is 1; and we have DL of two for three-legged and disappeared, as we need to cross twoedges in the direction of arrow to get from the for-mer to the latter.
In case there is no path betweenwords as in the last two cases above, we take the DLto be infinite.DPM takes a dependency tree to be a set oflinked edges.
Each edge is expressed as a triple< Cs(?i), Ce(?j),DL(?i, ?j) >, where ?i and ?jrepresent bunsestu?s that the edge spans.
Cs(?)
de-notes the class of a bunsetsu where the edge startsand Ce(?)
that of a bunsetsu where the edge ends.What we mean by ?class of bunsetsu?
is some sort ofa classificatory scheme that concerns linguistic char-acteristics of bunsetsu, such as a part-of-speech ofthe head, whether it has an inflection, and if it does,what type of inflection it has, etc.
Moreover, DPMuses two separate classificatory schemes for Cs(?
)and Ce(?
).In DPM, we define the connectivity strength p by:p(?i, ?j) ={logS(t) if DL(?i, ?j) ?= ???
otherwise (9)where t =< Cs(?i), Ce(?j),DL(?i, ?j) >, andS(t) is the probability of t occurring in a compres-sion, which is given by:S(t) = # of t?s found in compressions# of triples found in the training data(10)We complete the DPM formulation with:q(?)
= log pc(?)
+ tfidf(?)
(11)pc(?)
denotes the probability of having bunsetsu ?in compression, calculated analogously to Eq.
10,8and tfidf(?)
obviously denotes the tfidf value of ?.In DPM, a compression of a given sentence can beobtained by finding argmaxy h(y), where y rangesover possible candidate compressions of a particularlength one may derive from that sentence.
In theexperiment described later, we set ?
= 0.1 for DPM,following Morooka et al (2004), who found the bestperformance with that setting for ?.5 Evaluation SetupWe created a corpus of sentence summaries basedon email news bulletins we had received over fiveto six months from an on-line news provider calledNikkei Net, which mostly deals with finance andpolitics.9 Each bulletin consists of six to seven newsbriefs, each with a few sentences.
Since a news briefcontains nothing to indicate what its longer version8DPM puts bunsetsu?s into some groups based on linguis-tic features associated with them, and uses the statistics of thegroups for pc rather than that of bunsetsu?s that actually appearin text.9http://www.nikkei.co.jp304Table 2: The rating scale on fluencyRATING EXPLANATION1 makes no sense2 only partially intelligible/grammatical3 makes sense; seriously flawed in gram-mar4 makes good sense; only slightly flawedin grammar5 makes perfect sense; no grammar flawsmight look like, we manually searched the news sitefor a full-length article that might reasonably be con-sidered a long version of that brief.We extracted lead sentences both from the briefand from its source article, and aligned them, us-ing what is known as the Smith-Waterman algorithm(Smith and Waterman, 1981), which produced 1,401pairs of summary and source sentence.10 For theease of reference, we call the corpus so produced?NICOM?
for the rest of the paper.
A part of our sys-tem makes use of a modeling toolkit called GRMM(Sutton et al, 2004; Sutton, 2006).
Throughout theexperiments, we call our approach ?Generic Sen-tence Trimmer?
or GST.6 Results and DiscussionWe ran DPM and GST on NICOM in the 10-foldcross validation format where we break the data into10 blocks, use 9 of them for training and test on theremaining block.
In addition, we ran the test at threedifferent compression rates, 50%, 60% and 70%, tolearn how they affect the way the models perform.This means that for each input sentence in NICOM,we have three versions of its compression created,corresponding to a particular rate at which the sen-tence is compressed.
We call a set of compressionsso generated ?NICOM-g.?In order to evaluate the quality of outputs GSTand DPM generate, we asked 6 people, all Japanesenatives, to make an intuitive judgment on how eachcompression fares in fluency and relevance to gold10The Smith-Waterman algorithm aims at finding a bestmatch between two sequences which may include gaps, suchas A-C-D-E and A-B-C-D-E.
The algorithm is based on an idearather akin to dynamic programming.Table 3: The rating scale on content overlapRATING EXPLANATION1 no overlap with reference2 poor or marginal overlap w. ref.3 moderate overlap w. ref.4 significant overlap w. ref.5 perfect overlap w. ref.standards (created by humans), on a scale of 1 to 5.To this end, we conducted evaluation in two sepa-rate formats; one concerns fluency and the other rel-evance.
The fluency test consisted of a set of com-pressions which we created by randomly selecting200 of them from NICOM-g, for each model at com-pression rates 50%, 60%, and 70%; thus we have200 samples for each model and each compressionrate.11 The total number of test compressions cameto 1,200.The relevance test, on the other hand, consisted ofpaired compressions along with the associated goldstandard compressions.
Each pair contains compres-sions both from DPM and from GST at a given com-pression rate.
We randomly picked 200 of them fromNICOM-g, at each compression rate, and asked theparticipants to make a subjective judgment on howmuch of the content in a compression semanticallyoverlap with that of the gold standard, on a scale of1 to 5 (Table 3).
Also included in the survey are 200gold standard compressions, to get some idea of howfluent ?ideal?
compressions are, compared to thosegenerated by machine.Tables 4 and 5 summarize the results.
Table 4looks at the fluency of compressions generated byeach of the models; Table 5 looks at how much ofthe content in reference is retained in compressions.In either table, CR stands for compression rate.
Allthe results are averaged over samples.We find in Table 4 a clear superiority of GST overDPM at every compression rate examined, with flu-ency improved by as much as 60% at 60%.
How-ever, GST fell short of what human compressionsachieved in fluency ?
an issue we need to address11As stated elsewhere, by compression rate, we mean r =# of 1 in ylength of x .305Table 4: Fluency (Average)MODEL/CR 50% 60% 70%GST 3.430 3.820 3.810DPM 2.222 2.372 2.660Human ?
4.45 ?Table 5: Semantic (Content) Overlap (Average)MODEL/CR 50% 60% 70%GST 2.720 3.181 3.405DPM 2.210 2.548 2.890in the future.
Since the average CR of gold standardcompressions was 60%, we report their fluency atthat rate only.Table 5 shows the results in relevance of con-tent.
Again GST marks a superior performance overDPM, beating it at every compression rate.
It is in-teresting to observe that GST manages to do wellin the semantic overlap, despite the cutback on thesearch space we forced on GST.As for fluency, we suspect that the superior per-formance of GST is largely due to the depen-dency truncation the model is equipped with; andits performance in content overlap owes a lot toCRFs.
However, just how much improvement GSTachieved over regular CRFs (with no truncation) influency and in relevance is something that remainsto be seen, as the latter do not allow for variablelength compression, which prohibits a straightfor-ward comparison between the two kinds of models.We conclude the section with a few words on thesize of |G(S)|, i.e., the number of candidates gener-ated per run of compression with GST.Figure 8 shows the distribution of the numbers ofcandidates generated per compression, which lookslike the familiar scale-free power curve.
Over 99%of the time, the number of candidates or |G(S)| isfound to be less than 500.7 ConclusionsThis paper introduced a novel approach to sentencecompression in Japanese, which combines a syntac-tically motivated generation model and CRFs, in or-Number of CandidatesFrequency0 500 1500 250004008001200Figure 8: The distribution of |G(S)|der to address fluency and relevance of compres-sions we generate.
What distinguishes this workfrom prior research is its overt withdrawal from asearch for global optima to a search for local optimathat comply with grammar.We believe that our idea was empirically borneout, as the experiments found that our approach out-performs, by a large margin, a previously knownmethod called DPM, which employs a global searchstrategy.
The results on semantic overlap indicatesthat the narrowing down of compressions we searchobviously does not harm their relevance to refer-ences.An interesting future exercise would be to explorewhether it is feasible to rewrite Eq.
5 as a linear inte-ger program.
If it is, the whole scheme of ours wouldfall under what is known as ?Linear ProgrammingCRFs?
(Tasker, 2004; Roth and Yih, 2005).
What re-mains to be seen, however, is whether GST is trans-ferrable to languages other than Japanese, notably,English.
The answer is likely to be yes, but detailshave yet to be worked out.ReferencesJames Clarke and Mirella Lapata.
2006.
Constraint-based sentence compression: An integer programming306approach.
In Proceedings of the COLING/ACL 2006,pages 144?151.Trevor Cohn and Mirella Lapata.
2007.
Large marginsynchronous generation and its application to sentencecompression.
In Proceedings of the 2007 Joint Confer-ence on Empirical Methods in Natural Language Pro-cessing and Computational Natural Language Learn-ing, pages 73?82, Prague, June.Bonnie Dorr, David Zajic, and Richard Schwartz.
2003.Hedge trimmer: A parse-and-trim approach to head-line generataion.
In Proceedings of the HLT-NAACLText Summarization Workshop and Document Under-standing Conderence (DUC03), pages 1?8, Edmon-ton, Canada.Satoshi Fukutomi, Kazuyuki Takagi, and KazuhikoOzeki.
2007.
Japanese Sentence Compression usingProbabilistic Approach.
In Proceedings of the 13thAnnual Meeting of the Association for Natural Lan-guage Processing Japan.Michel Galley and Kathleen McKeown.
2007.
Lexical-ized Markov grammars for sentence compression.
InProceedings of the HLT-NAACL 2007, pages 180?187.Hongyan Jing.
2000.
Sentence reduction for automatictext summarization.
In Proceedings of the 6th Confer-ence on Applied Natural Language Processing, pages310?315.Tomonori Kikuchi, Sadaoki Furui, and Chiori Hori.2003.
Two-stage automatic speech summarization bysentence extraction and compaction.
In Proceedingsof ICASSP 2003.Kevin Knight and Daniel Marcu.
2002.
Summariza-tion beyond sentence extraction: A probabilistic ap-proach to sentence compression.
Artificial Intelli-gence, 139:91?107.John Lafferty, Andrew MacCallum, and FernandoPereira.
2001.
Conditional random fields: Probabilis-tic models for segmenting and labeling sequence data.In Proceedings of the 18th International Conferenceon Machine Learning (ICML-2001).Ryan McDonald.
2006.
Discriminative sentence com-pression with soft syntactic evidence.
In Proceedingsof the 11th Conference of EACL, pages 297?304.Yuhei Morooka, Makoto Esaki, Kazuyuki Takagi, andKazuhiko Ozeki.
2004.
Automatic summarization ofnews articles using sentence compaction and extrac-tion.
In Proceedings of the 10th Annual Meeting ofNatural Language Processing, pages 436?439, March.
(In Japanese).Tadashi Nomoto.
2007.
Discriminative sentence com-pression with conditional random fields.
InformationProcessing and Management, 43:1571 ?
1587.Rei Oguro, Kazuhiko Ozeki, Yujie Zhang, and KazuyukiTakagi.
2000.
An efficient algorithm for Japanesesentence compaction based on phrase importanceand inter-phrase dependency.
In Proceedings ofTSD 2000 (Lecture Notes in Artificial Intelligence1902,Springer-Verlag), pages 65?81, Brno, Czech Re-public.Stefan Riezler, Tracy H. King, Richard Crouch, and An-nie Zaenen.
2003.
Statistical sentence condensationusing ambiguity packing and stochastic disambigua-tion methods for lexical functional grammar.
In Pro-ceedings of HLT-NAACL 2003, pages 118?125, Ed-monton.Dan Roth and Wen-tau Yih.
2005.
Integer linear pro-gramming inference for conditional random fields.
InProceedings of the 22nd International Conference onMachine Learning (ICML 05).T.
F. Smith and M. S. Waterman.
1981.
Identification ofcommon molecular subsequence.
Journal of Molecu-lar Biology, 147:195?197.Charles Sutton and Andrew McCallum.
2006.
An in-troduction to conditional random fields for relationallearning.
In Lise Getoor and Ben Taskar, editors,Introduction to Statistical Relational Learning.
MITPress.
To appear.Charles Sutton, Khashayar Rohanimanesh, and AndrewMcCallum.
2004.
Dynamic conditional randomfields: Factorized probabilistic labeling and segment-ing sequence data.
In Proceedings of the 21st In-ternational Conference on Machine Learning, Banff,Canada.Charles Sutton.
2006.
GRMM: A graphical modelstoolkit.
http://mallet.cs.umass.edu.Ben Tasker.
2004.
Learning Structured Prediction Mod-els: A Large Margin Approach.
Ph.D. thesis, StanfordUniversity.Roy W. Tromble and Jason Eisner.
2006.
A fast finite-state relaxation method for enforcing global constrainton sequence decoding.
In Proceeings of the NAACL,pages 423?430.Jenie Turner and Eugen Charniak.
2005.
Supervised andunsupervised learning for sentence compression.
InProceedings of the 43rd Annual Meeting of the ACL,pages 290?297, Ann Arbor, June.Vincent Vandeghinste and Yi Pan.
2004.
Sentence com-pression for automatic subtitling: A hybrid approach.In Proceedings of the ACL workshop on Text Summa-rization, Barcelona.Kiwamu Yamagata, Satoshi Fukutomi, Kazuyuki Takagi,and Kzauhiko Ozeki.
2006.
Sentence compressionusing statistical information about dependency pathlength.
In Proceedings of TSD 2006 (Lecture Notes inComputer Science, Vol.
4188/2006), pages 127?134,Brno, Czech Republic.307
