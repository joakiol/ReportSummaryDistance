Towards Building Contextual Representations ofWord Senses Using Statistical ModelsClaudia Leacock, l Geoffrey Towdl ~ and Ellen Voorhees 2I Peinceton UniversityleacockOclarity.princeton, edu2Siemens Corporate Research, Inc.towellOlearning.scr.siemenJ, com, ellenOlearning.scr.siemenJ, comAbst ractAutomatic orpus-based sense resolution, or sense dlsambiguation, techniques tend tofocus either on very local context or on topical context.
Both components axe neededfor word sense resolution.
A contextual representation f a word sense consists of top-ical context and local context.
Our goal is to construct contextual representations byautomatically extracting topical and local information from textual corpora.
We re-view an experiment evaluating three statistical classifiers that automatically extracttopical context.
An experiment designed to examine human subject performancewith similar input is described.
Finally, we investigate a method for automaticallyextracting local context from a corpus.
Preliminary results show improved perfor-ms,  nce .1 Contextual RepresentationsThe goal of automat ic  sense resolution is to acquire a conteztual representation of wordsenses.
A contextual representation, as defined by Miller and Charles \[7\], is a characteri-zation of the l inguistic ontexts in which a word can be used.
We look at two componentsof contextual representations that can be automatical ly  extracted from textual corporausing statist ical methods.
These are topical contezt and local contezt.Topical contezt is comprised of substantive words that are likely to co-occur with agiven sense of a target word.
If, for example, the polysemous word line occurs in a sentencewith poetry and we/re, it is probably being used to express a different sense of line thanif it occurred with stand and wait.
Topical context is relatively insensitive to the orderof words or their grammatical  inflections; the focus is on the meanings of the open-classwords that  are used together in the same sentences.Local contezt includes information on word order, distance and syntactic structure.
Forexample, a line from does not suggest he same sense as in line .for.
Order and inflectionare critical clues for local information, which is not restricted to open-class words.In the next section, we briefly review an experiment using three statist ical classifiersdesigned for sense resolution, and show that they are effective in extracting topical con-text.
Section 3 describes an experiment that was performed to establish the upper boundof performance for these classifiers.
Section 4 presents ome techniques that we are devel-oping to extract local context.2 Acquiring Topical ContextOf the two types of context features, topical ones seem easier to identify.
The idea issimple: for any topic there is a sub-vocabulary of terms that are appropr iate for discussing10it.
The task is to identify the topic, then to select that sense of the polysemous word thatbest fits the topic.
For example, if the topic is writing, then sheet probably refers to apiece of paper; if the topic is sleeping, then it probably refers to bed linen; if the topic issailing, it could refer to a sail; and so on.Instead of using topics to discover senses, one can use senses to discover topics.
Thatis to say, if the senses are known in advance for a textual corpus, it is possible to searchfor words that are likely to co-occur with each sense.
This strategy requires two steps.It is necessary (1) to partition a sizeable number of occurrences of a polysemous wordaccording to its senses, and then (2) to use the resulting sets of instances to search forco-occurring words that are diagnostic of each sense.
That was the strategy followed withconsiderable success by Gale, Church, and Yarowsky \[1\], who used a bilingual corpus for(1), and a Bayesian decision system for (2).To understand this and other statistical systems better, we posed a very specific prob-lem: given a set of contexts, each containing the noun line in a known sense, constructa classifier that selects the correct sense of line for new contexts.
To see how the degreeof polysemy affects performance, we ran three- and six-sense tasks.
A full descriptionof the three-sense task is reported in Voorhees, et.
al.
\[11\], and the six-sense task inLeacock, et.
al \[5\].
These experiments are reviewed briefly below.We tested three corpus-based statistical sense resolution methods which attempt toinfer the correct sense of a polysemous word by using knowledge about patterns of wordco-occurrences.
The first technique, developed by Gale et.
al.
\[1\] at AT&T Bell Labora-tories, is based on Bayesian decision theory, the second is based on neural network withback propagation \[9\], and the third is based on content vectors as used in informationretrieval \[10\].
The only information used by the three classifiers is co-occurrence of char-acter strings in the contexts.
They use no other cues, such as syntactic tags or wordorder, nor do they require any augmentation of the training and testing data that is notfully automatic.
The Bayesian classifier uses all of the information in the sentence xceptword order.
That is, it uses punctuation, upper/lower case distinctions, and inflectionalendings.
The other two classifiers remove punctuation and convert all characters to lowercase.
In addition, they remove a list of stop words, a set of about 570 very high frequencywords that includes most function words as well as some content words.
The remainingstrings are stemmed."
suffixes are removed to conflate across morphological distinctions.For example, the strings computer(s), computing, computcdion(al), etc.
are conflated tothe stem comput.2.1  Methodo logyThe training and testing contexts were taken from the 1987-89 Wall Street Journal corpusand from the APHB corpus.
1 Sentences containing line(s) and Line(s) were extractedand manually assigned a single sense from WordNet.
2 Sentences with proper names con-taining Line, such as Japan Air Lines, were removed from the set of sentences.
Sentencescontaining collocations that have a single sense in WordNet, such as product line and lineof products, were also excluded since the collocations are not ambiguous.1 The 25 million word corpus, obtained from the American Printing House for the Blind, is arc.hlved atIBM's T.J. Watson Research Center; it consists of stories and articles from books and general circulationmagazines.2WordNet is a lexical database developed by George Miller and his colleagues at Princeton Univer-sity \[6\].11Typically, experiments have used a fixed number of words or characters on eitherside of the target word as the context.
In these experiments, we used linguistic units -sentences - instead.
Since the target word is often used anaphorical ly to refer back to theprevious sentence, as in:That  was the last t ime Bell ever talked on the phone.
He couldn't  get his wifeoff the line.we chose to use two-sentence contexts: the sentence containing line and the precedingsentence.
However, if the sentence containing line was the first sentence in the article,then the context consists of one sentence.
If the preceding sentence also contained line inthe same sense, then an addit ional  preceding sentence was added to the context, creatingcontexts three or more sentences long.
The average size of the training and testing contextswas 44.5 words.The sense resolution task used the following six senses of the noun line:1. a product: ... a new line of midsized cars ...2. a formation of people or things: People waited patient ly in long lines ...3. spoken or written tezt: One winning line from that speech ...4. a thin, flexible object; cord: With a line tied to his foot, ...5. an abstract division: ... the Amish draw no line between work and religion and life.6.
a telephone connection: One key to WordPerfect's growth was its toll-free help lineThe classifiers were run three times each on randomly selected training sets.
The set ofcontexts for each sense was randomly permuted, with each permutat ion correspondingto one trial.
For each trial, the first 200 contexts of each sense were selected as trainingcontexts.
The next 149 contexts were selected as test contexts.
The remaining contextswere not used in that trial.
The 200 training contexts for each sense were combined toform a final training set of size 1200.
The final test set contained the 149 test contextsfrom each sense, for a total  of 894 contexts.
To test the effect that  the number of trainingexamples has on classifier performance, smaller training sets of 50 and 100 contexts wereextracted from the 200 context training set.2 .2  Resu l tsAll of the classifiers performed best with the largest number (200) of training contexts,and the percent correct results reported here are averaged over the three trials with 200training contexts.
On the six-sense task, the Bayesian classifier averaged 71% correctanswers, the content vector classifier 72%, and the neural networks 76%.
None of thesedifferences are stat ist ical ly significant due to the l imited sample size of three trials.The ten most heavily weighted tokens for each sense for each classifier appear inTable 1.
The words on the list seem, for the most part,  indicative of the target senseand are reasonable indicators of topical context.
However, there are some consistentdifferences among the methods.
For example, while the Bayesian method is sensitiveto proper nouns, the neural network appears to have no such preference.
To test thehypothesis that  the methods have different response patterns, we performed the X 2 test forcorrelated proportions.
This test measures how consistently the methods treat individualtest contexts by determining whether the classifiers are making the same classificationerrors in each of the senses.12Bayesian"ChryslerworkstationsDigitalintroducedmodelsIBMCompaclsellagreement,,,computersProduct FormationVector Network Bayesian Vector NetworkcomputibmproduccorpflalemodelsellintroducbrandmainframecomputsellminicomputmodelintroducextendacquirlaunchcontinuquaknightcheckoutwaitgasolineoutsidewaitingfoodhourslongdriverwaltlongcheckoutparkmrairportshopcountpeoplcanadwaltlongstandcheckoutparkhourformshortcustomshopText CordBayesian Vector Network Bayesian Vector NetworkspeechwritmrbushadspeakreaddukakbidenpoemfishfishingbowdeckseaboatwaterclothesfastenedshipBidenadBushopeningfamousDolespeechDukakisfunnyspeechesfishboatwathookwashfloatmendivecagerodfamiliarwritadremembdelivfamespeakfunnymoviereadhapfishwashpullboatropebreakhookexerciscryDivision PhoneBayesian Vector Network Bayesian Vector Networkphones telephontoll phonporn callBellsouth accessgab dialtelephone gabBell bellbillion servicPacific tollcalls pornblurredwalkingcrossedethicsnarrowfineclassbetweenwalkdrawdraw drawfine privblur hugcross blurwalk crossnarrow finemr thintread functfaction geniusthin narrowtelephonphondeadcheerhearhendersonminutcallbillsilentTable 1: Topical Context.
The ten most heavily weighted tokens for each sense of line forthe Bayesian, content vector and neural network classifiers.13The results of the X 2 test for a three-sense resolution task (product, formation andtezt), 3 indicate that the response pattern of the content vector classifier is significantlydifferent from the patterns of both the Bayesian and neural network classifiers, but theBayesian response pattern is significantly different from the neural network pattern forthe product sense only.
In the six-sense disambiguation task, the X 2 results indicatethat the Bayesian and neural network classifiers' response patterns are not significantlydifferent for any sense.
The neural network and Bayesian classifiers' response patternsare significantly different from the content vector classifier only in the formation and teztsenses.
Therefore, with the addit ion of three senses, the classifiers' response patternsappear to be converging.A pilot two-sense distinction task (between product and formation) yielded over 90%correct answers.
4 In the three-sense distinction task, the three classifiers had a mean of76% correct, yielding a sharp degradation with the addit ion of a third sense.
Therefore, wehypothesized degree of polysemy to be a major  factor for performance.
We were surprisedto find that in the six-sense task, all three classifiers degraded only slightly from the three-sense task, with a mean of 73% correct.
Although the addit ion of three new senses tothe task caused consistent degradation, the degradation is relatively slight.
Hence, weconclude that some senses are harder to resolve than others, and it appears that overallaccuracy is a function of the difficulty of the sense rather than being strictly a function ofthe degree of polysemy.
The hardest sense for all three classifiers to learn was tezt, followedby formation, followed by division.
The difficulty in training for the product, phone, andcord senses varied among the classifiers, but they were the three 'easiest'  senses acrossthe classifiers.
To test our conclusion that the difficulty involved in learning individualsenses is a greater factor for performance than degree of polysemy, we ran a three-wayexperiment on the three 'easy'  senses.
On this task, the content vector classifier achieved90% accuracy and neural network classifier 92% accuracy.The convergence of the response patterns for the three methods suggests that eachof the classifiers is extracting as much data as is available in word co-occurrences inthe training contexts.
If this is the case, any technique that uses only word counts willnot be significantly more accurate than the techniques tested here.
Although the degreeof polysemy does affect the difficulty of the sense resolution task, a greater factor ~ forperformance is the difficulty of resolving individual senses.
From inspection of the contextsfor the various senses, it appears that the senses of line that were easy to learn tend tobe surrounded by a lot of topical context.
Wi th  the senses that were hard to learn, thecrucial d issmbiguat ing information tends to be very local, so that a greater proport ionof the context is noise.
Although it is recognized that local information is more reliablethan distant information, the classifiers make no use of locality.
Figure 1 shows somerepresentative contexts for each sense of line used in the study.
The product, phoneand cord senses contain a lot of topical context, while the other senses have l itt le or noinformation that is not very local.The three classifiers are doing a good job finding topical context.
However, s implyknowing which words are likely to co-occur in the same sentences when a part icular topicis under discussion is not sufficient for sense resolution.3'Prainlng and test sets for these senses are identical to those in the six-sense resolution task.aThls task was only run with the content vector and neural network clarsifiers.141.
text:  In a warmly received speech that seemingly sought to distance him fromReagan administration civil-rights policies, Mr. Bush outlined what he called a"positive civil-rights agenda," and promised to have "minority men and womenof excellence as full-scale partners" during his presidency.
One winning l ine fromthat speech: "Whenever racism rears its ugly head-Howard Beach, Forsyth County,wherever-we must be there to cut it off."2.
fo rmat ion:  On the way to work one morning, he stops at the building to tell Mr.Arkhipov: "Don't forget the drains today."
Back in his office, the l lne of peoplewaiting to see him has dwindled, so Mr. Goncharov stops in to see the mayor, YuriKhivrich.3.
division: Thus, some families are probably buying take-out food from grocerystores-such as barbecued chicken-but aren't classifying it as such.
The l ine betweengroceries and take-out food may have become blurred.4.
cord: Larry ignored the cries and came swooping in.
The fisherman's nylon llne,taut and glistening with drops of seawater, suddenly went slack as Larry's boardrode over it.5.
phone:  "Hello, Weaver," he said and then to put her on the defensive, "what's allthe gabbing on the house phones?
I couldn't get an open llne to you."6.
product :  International Business Machines Corp., seeking to raise the return onits massive research and development investments, aid it will start charging moremoney to license its 32,000 patents around the world.
In announcing the change,IBM also said that it's willing to license patents for its PS/2 l ine of personal com-puters.Figure i: Representative contexts for the six senses of line used in the study.153 An  Upper  Bound For Classif ier Per fo rmanceIn an effort to establish an upper bound for performance on corpus-based statistical senseresolution methods, we decided to see how humans would perform on a sense resolutiontask using the same input that drives the statistical classifiers \[4\].
An experiment wasdesigned to answer the following questions:1.
How do humans perform in a sense resolution task when given the same testinginput as the statistical classifiers?2.
Are the contexts that are hard/easy for the statistical classifiers also hard/easy forpeople?The three-sense task was replicated using human subjects.
For each of the three sensesof line (product, tezt, and formation), we selected 10 easy contexts (contexts that werecorrectly classified by the three statistical methods) and 10 haed contexts (contexts thatwere misclassified by the three methods), for a total of 60 contexts.
These contexts wereprepared in three formats: (1) a sentential form (as they originally appeared in the corpus),(2) a long list format (as was used by the Bayesian classifier), and (3) a shor~ list format(as was used by the content vector and neural network classifiers).
In order to mimicthe fact that the classifiers do not use word order, collocations, or syntactic structure,the latter two contexts were presented to the subjects as word lists in reverse alphabeticalorder.
36 subjects each saw 60 contexts, 20 in each of the three formats, and were asked tochoose the appropriate sense of line.
The order in which the formats were presented wascounter-balanced across subjects.
No subject saw the same context twice.
The subjectswere Princeton undergraduates who were paid for their participation.Human subjects performed almost perfectly on the sentential formats and had abouta 32% error rate on the list formats.
There was no significant difference between the twolist formats - indicating that function words are of no use for sense resolution when wordorder is lost.
They made significantly more errors on the contexts that were hard for thestatistical classifiers, and fewer errors on the contexts that were easy for the classifiers.Not all the senses were equally difficult for human subjects: there were significantly fewererrors for the product sense of line than for the tczt and fformgtion senses.
Error rates forthe subjects on the list formats were almost 50% for the hard contexts (contexts wherethe classifiers performed with 100% error), so subjects performed much better than theclassifiers on these contexts.
However, on the easy contexts, where the classifiers made noerrors, the students howed an error rate of approximately 15%.When subjects see the original sentences and therefore have access to all cues, bothtopical and local, they resolve the senses of line with 98% accuracy.
When they are giventhe contexts in a list format, and are getting only topical cues, their performance dropsto about 70% accuracy.
Although their performance was significantly better than theclassifiers (which all performed at 50% accuracy on this sample) human subjecrts are notable to disamhiguate effectively using only topical context.
From this result we concludethat in order to improve the performance of automatic lassifiers, we need to incorporatelocal information into the statistical methods.164 Acquiring Local ContextKelly and Stone \[3\] pioneered research in finding local context by creating algorithms forautomatic sense resolution.
Over a period of seven years in the early 1970s, they (andsome 30 students) hand coded sets of ordered rules for disambiguating 671 words.
Therules include syntactic markers (part of speech, position within the sentence, punctuation,inflection), semantic markers and selectional restrictions, and words occurring within aspecified istance before and/or after the target.
An obvious hortcoming of this approachis the amount of work involved.Recently there has been much interest in automatic and semi-automatic a quisition oflocal context (Hearst \[2\], Resnik \[8\], Yarowsky \[13\]).
These systems are all plagued withthe same problem, excellent precision but low recall.
That is, if the local information thatthe methods learn is also present in a novel context, then that information is very reliable.However, quite frequently no local context match is found in a novel context.
Given thesparseness of the local data, we hope to look for both local and topical context, and wehave begun experimenting with various ways of acquiring the local context.Local context can be derived from a variety of sources, including WordNet.
The nounsin WordNet are organized in a hierarchical tree structure based on hypernomy/hyponomy.The hypernym of a noun is its superordinate, and the/s  a kind o/relation exists betweena noun and its hypernym.
For example, line is a hypernym of conga line, which is tosay that a conga line is a kind of line.
Conversely, cong~ line is a hyponym of line.Polysemous words tend to have hyponyms that are monosemous collocations incorporatingthe polysemous word: product line is a monosemous hyponym of the merchandise sense ofline; any occurrence of product line can be recognized immediately as an instance of thatsense.
Similarly, phone line is a hyponym of the telephone connection sense of line, actor'sline is a hyponym of the text sense of line, etc.
These collocational hyponyms provide aconvenient starting point for the construction of local contexts for polysemous words.We are also experimenting with template matching, suggested by Weiss as one ap-proach to using local context o resolve word senses \[12\].
In template matching, specificword patterns recognized as being indicative of a particular sense (the templates) areused to select a sense when a template is contained in the novel context; otherwise wordco-occurrence within the context (topical context) is used to select a sense.
Weiss initiallyused templates that were created by hand, and later derived templates automatically fromhis dataset.
Unfortunately, the datasets available to Weiss at the time were very small,and his results are inconclusive.
We are investigating a similar approach using the linedata: training contexts are used to both automatically extract indicative templates andcreate topical sense vectors.To create the templates, the system extracts contiguous ubsets of tokens includingthe target word and up to two tokens on either side of the target as candidate templates,The system keeps a count of the number of times each candidate template occurs in allof the training contexts.
A candidate is selected as a template if it occurs in at least n ofthe training contexts and one sense accounts for at least m% of its total occurrences.
Forexample, Figure 2 shows the templates formed when this process is used on a training setof 200 contexts for each of six senses when n = 10 and m = 75.
The candidate templateblurs the line is not selected as a template with these parameter settings because it doesnot occur frequently enough in the training corpus; the candidate template line o/is notSin the templ6te l arning phase, tokens include ptmetuation a d Jiop wordm.
No stemmingls performedand case distinctions are significant.17cordhis lined iv i s iona fine line between, line line between, a line line, line lineline between the, the line between, line betweendraw the line, over the linefo rmat iona long line of, long line of, a long line, long line, long linesin line for, wait in line, in linephonetelephone linesaccess linesl i l le wa4lproducta new line of, a new line, new line of, new lineFigure 2: Templates formed for a training set of 200 contexts for each of six senses whena template must occur at least 10 times and at least 75% of the occurrences must be forone sense.
No templates were learned for the tezt sense.selected because it appears too frequently in both the product line and formation contexts.Wi th  the exception of his line (cord) and line was (phone), these templates readilysuggest their corresponding sense.
The n = 10 and m = 75 parameter  settings arerelatively stringent criteria for template formation, so not many templates are formed,but  those templates that are formed tend to be highly indicative of the sense.Prel iminary results show template matching improves the performance of the contentvector classifier.
The six-sense xperiment was repeated using a simple decision tree toincorporate the templates: The sense corresponding to the longest emplate contained in atest context was selected for that  context; if the context contained no template,  the sensechosen by the vector classifer was selected.
The templates were automat ica l ly  createdfrom the same training set as was used to create the content vectors.
To be selected as atemplate,  a candidate had to appear at least 3 t imes for the training sets that  included50 of each sense, 5 t imes for the 100 each training sets, and 10 times for the 200 eachtraining sets.
In all cases, a single sense had to account for at least 75% of a candidate'soccurrences.
This hybrid approach was more accurate than the content vector classifieralone on each of the 9 trials.
The average accuracy when trained using 200 contexts ofeach sense was 75% for the hybrid approach compared to 72% for the content vectorsalone.Other researchers have also suggested methods for incorporating local information intoa classifier.
Yarowsky found collocations 8 to be such powerful sense indicators that hesuggests choosing a sense by matching on a set of collocations and choosing the mostfrequent sense if no collocation matches \[13\].
To resolve syntactic ambiguities, Resnlkeyarowsky uses the term collocation to denote constructs similar to what we have called templates.18investigated four different methods for combining three sources of information \[8\].
The"backing off" strategy, in which the three sources of information were tried in order frommost reliable to least reliable until some match was found (no resolution was done if nomethod matched), maintained high precision (81%) and produced substantially higherrecall (95%) than any single method.Our plans for incorporating templates into the content vector classifier include inves-tigating the significance of the tradeoff between the reliability of the templates and thenumber of templates that are formed.
When stringent criteria are used for template for-mation, and the templates are thought to be highly reliable sense indicators, the sensecorresponding to a matched template will always be selected, and the sense vectors willbe used only when no template match occurs.
When the templates are thought o be lessreliable, the choice of sense will be a function of the uniqueness of a matched template(if any) and the sense vector similarities.
By varying the relative importance of a tem-plate match and sense vector similarity we will be able to incorporate different amountsof topical and local information into the template classifier.5 Conc lus ionThe capacity to determine the intended sense of an ambiguous word is an importantcomponent of any general system for language understanding.
We believe that, in orderto accomplish this task, we need contextual representations of word senses containing bothtopical and local context.
Initial experiments focused on methods that are able to extracttopical context.
These methods are effective, but topical context alone is not sufficientfor sense resolution tasks.
The human subject experiment shows that even people are notvery good at resolving senses when given only topical context.
Currently we are testingmethods for learning local context for word senses.
Preliminary results show that theaddition of template matching on local context improves performance.AcknowledgmentsThis work was supported in part by Grant No.
N00014-91-1634 from the Defense Ad-vanced Research Projects Agency, Information and Technology Office, by the Office ofNaval Research, and by the James S. McDonnell Foundation.
We are indebted to GeorgeA.
Miller and Martin S. Chodorow for valuable comments on an earlier version of thispaper.References\[1\] William Gale, Kenneth W. Church, and David Yarowsky.
A method for disam-biguating word senses in a large corpus.
Statistical Research Report 104, AT&T BellLaboratories, 1992.\[2\] Marti A. Hearst.
Noun homograph disambiguation using local context in large textcorpora.
In Seventh Annual Conference of the UW Centre for the New OED andTezt Research: Using Corpora, pages 1-22, Oxford, 1991.
UW Centre for the NewOED and Text Research.19\[3\] Edward Kelly and Philip Stone.
Computer Recognition of English Word Senses.North-Holland, Amsterdam, 1975.\[4\] Claudia Leacock, Shari Landes, and Martin Chodorow.
Comparison of sense reso-lution by statistical classifiers and human subjects.
Cognitive Science LaboratoryReport, Princeton University, in preparation.\[5\] Claudia Leacock, Geoffrey Towell, and Ellen M. Voorhees.
Corpus-based statisti-cal sense resolution.
In Proceedings off the ARPA Workshop on Human LanguageTechnology, 1993.\[6\] George Miller.
Special Issue, WordNet: An on-line lexical database.
InternationalJournal off Lezicography, 3(4), 1990.\[7\] George A. Miller and Walter G. Charles.
Contextual correlates of semantic similarity.Language and Cognitive Processes, 6(1), 1991.\[8\] Philip Resnik.
Semantic lasses and syntactic ambiguity.
In Proceedings of the ARPAWorkshop on Human Language Technology, 1993.\[9\] D. E. Rumelhart, G. E. Hinton, and R. J. Williams.
Learning internal representationsby error propagation.
In D. E. Rumelhart and J. L. McClelland, editors, ParallelDistributed Processing: Ezplorations in the Microstrncture of Cognition, Volume 1:Foundatior~s, pages 318-363.
MIT Press, Cambridge, 1986.\[10\] G. Salton, A. Wong, and C.S.
Yang.
A vector space model for automatic indexing.Communications of the ACM, 18(11):613-620, 1975.\[11\] Ellen M. Voorhees, Claudia Leacock, and Geoffrey Towell.
Learning context o dis-smbiguate word senses.
In Proceedings off the 3rd Computational Leaerdng Theoryand Natural Learning Systems Conference-lags, Cambridge, to appear.
MIT Press.\[12\] Stephen Weiss.
Learning to disambiguate.
Infformation Storage and Retrieval, 9:33-41, 1973.\[13\] David Yarowsky.
One sense per collocation.
In Proceedings off the ARPA Workshopon Human Language Technology, 1993.20THIS  PAGE INTENTION~T,LY  LEFT  BLANK21
