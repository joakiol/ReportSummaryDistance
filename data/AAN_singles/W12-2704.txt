NAACL-HLT 2012 Workshop: Will We Ever Really Replace the N-gram Model?
On the Future of Language Modeling for HLT, pages 29?36,Montre?al, Canada, June 8, 2012. c?2012 Association for Computational LinguisticsA Challenge Set for Advancing Language ModelingGeoffrey Zweig and Chris J.C. BurgesMicrosoft ResearchRedmond, WA 98052AbstractIn this paper, we describe a new, publiclyavailable corpus intended to stimulate re-search into language modeling techniqueswhich are sensitive to overall sentence coher-ence.
The task uses the Scholastic AptitudeTest?s sentence completion format.
The testset consists of 1040 sentences, each of whichis missing a content word.
The goal is to selectthe correct replacement from amongst five al-ternates.
In general, all of the options are syn-tactically valid, and reasonable with respect tolocal N-gram statistics.
The set was gener-ated by using an N-gram language model togenerate a long list of likely words, given theimmediate context.
These options were thenhand-groomed, to identify four decoys whichare globally incoherent, yet syntactically cor-rect.
To ensure the right to public distribution,all the data is derived from out-of-copyrightmaterials from Project Gutenberg.
The testsentences were derived from five of ConanDoyle?s Sherlock Holmes novels, and we pro-vide a large set of Nineteenth and early Twen-tieth Century texts as training material.1 IntroductionPerhaps beginning with Claude Shannon?s use ofN-gram statistics to compute the perplexity of let-ter sequences (Shannon and Weaver, 1949), N-grammodels have grown to be the most commonly usedtype of language model in human language tech-nologies.
At the word level, N-gram modeling tech-niques have been extensively refined, with state-of-the-art techniques based on smoothed N-gramcounts (Kneser and Ney, 1995; Chen and Good-man, 1999), multi-layer perceptrons (Schwenk andGauvain, 2002; Schwenk, 2007) and maximum-entropy models (Rosenfeld, 1997; Chen, 2009a;Chen, 2009b).
Trained on large amounts of data,these methods have proven very effective in bothspeech recognition and machine translation applica-tions.Concurrent with the refinement of N-grammodel-ing techniques, there has been an important streamof research focused on the incorporation of syntac-tic and semantic information (Chelba and Jelinek,1998; Chelba and Jelinek, 2000; Rosenfeld et al,2001; Yamada and Knight, 2001; Khudanpur andWu, 2000; Wu and Khudanpur, 1999).
Since in-tuitively, language is about expressing meaning ina highly structured syntactic form, it has come assomething of a surprise that the improvements fromthese methods have been modest, and the methodshave yet to be widely adopted in non-research sys-tems.One explanation for this is that the tasks to whichlanguage modeling has been most extensively ap-plied are largely soluble with local information.
Inthe speech recognition application, there is a fun-damental confluence of acoustic and linguistic in-formation, and the language model can be thoughtof as resolving ambiguity only between acousticallyconfusable words (Printz and Olsen, 2002).
Sincewords which are acoustically similar, e.g.
?bill?
and?spill?
usually appear in very different textual con-texts, the local information of an N-gram languagemodel may be adequate to distinguish them.
To alesser degree, in a machine translation application,291.
One of the characters in Milton Murayama?snovel is considered because he deliber-ately defies an oppressive hierarchical society.
(A) rebellious (B) impulsive (C) artistic (D)industrious (E) tyrannical2.
Whether substances are medicines or poisonsoften depends on dosage, for substances that arein small doses can be in large.
(A) useless .. effective(B) mild .. benign(C) curative .. toxic(D) harmful .. fatal(E) beneficial .. miraculousFigure 1: Sample sentence completion questions(Educational-Testing-Service, 2011).the potential phrase translations may be similar inmeaning and local information may again suffice tomake a good selection.In this paper, we present a language processingcorpus which has been explicitly designed to be non-solvable using purely N-gram based methods, andwhich instead requires some level of semantic pro-cessing.
To do this, we draw inspiration from thestandardized testing paradigm, and propose a sen-tence completion task along the lines of that foundin the widely used Scholastic Aptitude Test.
In thistype of question, one is given a sentence with one ortwo words removed, and asked to select from amonga set of five possible insertions.
Two examples ofSAT test questions are shown in Figure 1.As can be seen, the options available all makesense from the local N-gram point of view, and areall syntactically valid; only semantic considerationsallow the correct answer to be distinguished.
Webelieve this sort of question is useful for two keyreasons: first, its full solution will require languagemodeling techniques which are qualitatively differ-ent than N-grams; and secondly, the basic task for-mulation has been externally determined and is awidely used method for assessing human abilities.Unfortunately, to date no publicly available corpusof such questions has been released.The contribution of this work is to release a publiccorpus of sentence completion questions designed tostimulate research in language modeling technologywhich moves beyond N-grams to explicitly addressglobal sentence coherence.
The corpus is basedpurely on out-of-copyright data from Project Guten-berg, thus allowing us to distribute it.
The test ques-tions consist of sentences taken from five SherlockHolmes novels.
In each, a word has been removed,and the task is to choose from among five alterna-tives.
One of the options is the original word, and theother four ?decoys?
have been generated from an N-gram language model using local context.
Samplingfrom an N-gram model is done to generate alternateswhich make sense locally, but for which there is noother reason to expect them to make sense globally.To ensure that synonyms of the correct answer arenot present, and that the options are syntacticallyreasonable, the decoys have been hand selected fromamong a large number of possibilities suggested bythe N-gram model.
The training data consists ofapproximately 500 out-of-copyright Nineteenth andearly Twentieth century novels, also from ProjectGutenberg.We expect that the successful development ofmodels of global coherence will be useful in a va-riety of tasks, including:?
the interactive generation of sentence comple-tion questions for vocabulary tutoring applica-tions;?
proof-reading;?
automated grading of essays and other studentwork; and?
sentence generation in free-form dialog appli-cations.The remainder of this paper is organized as fol-lows.
In Section 2, we describe the process by whichwe made the corpus.
Section 3 provides guidanceas to the proper use of the data.
In Section 4, wepresent baseline results using several simple auto-mated methods for answering the questions.
Finally,in Section 5, we discuss related work.2 The Question Generation ProcessQuestion generation was done in two steps.
First,a candidate sentence containing an infrequent word30was selected, and alternates for that word were auto-matically determined by sampling with an N-gramlanguage model.
The N-gram model used the im-mediate history as context, thus resulting in wordsthat may ?look good?
locally, but for which thereis no a-priori reason to expect them to make senseglobally.
In the second step, we eliminated choiceswhich are obviously incorrect because they consti-tute grammatical errors.
Choices requiring semanticknowledge and logical inference were preferred, asdescribed in the guidelines, which we give in Sec-tion 3.
Note that an important desideratum guid-ing the data generation process was requiring thata researcher who knows exactly how the data wascreated, including knowing which data was used totrain the language model, should nevertheless not beable to use that information to solve the problem.We now describe the data that was used, and thendescribe the two steps in more detail.2.1 Data UsedSeed sentences were selected from five of Co-nan Doyle?s Sherlock Holmes novels: The Sign ofFour (1890), The Hound of the Baskervilles (1892),The Adventures of Sherlock Holmes (1892), TheMemoirs of Sherlock Holmes (1894), and The Val-ley of Fear (1915).
Once a focus word withinthe sentence was selected, alternates to that wordwere generated using an N-gram language model.This model was trained on approximately 540 textsfrom the Project Gutenberg collection, consistingmainly of 19th century novels.
Of these 522 hadadequate headers attesting to lack of copyright,and they are now available at the Sentence Com-pletion Challenge website http://research.microsoft.com/en-us/projects/scc/.2.2 Automatically Generating AlternatesAlternates were generated for every sentence con-taining an infrequent word.
A state-of-the-art class-based maximum entropy N-gram model (Chen,2009b) was used to generate the alternates.
Ide-ally, these alternates would be generated accordingto P (alternate|remainder of sentence).
This canbe done by computing the probability of the com-pleted sentence once for every possible vocabularyword, and then normalizing and sampling.
However,the normalization over all words is computationallyexpensive, and we have used a procedure based onsampling based on the preceding two word historyonly, and then re-ordering based on a larger context.The following procedure was used:1.
Select a focus word with overall frequency lessthan 10?4.
For example, we might select ?ex-traordinary?
in ?It is really the most extraordi-nary and inexplicable business.?2.
Use the two-word history immediately preced-ing the selected focus word to predict alter-nates.
We sampled 150 unique alternates at thisstage, requiring that they all have frequencyless than 10?4.
For example, ?the most?
pre-dicts ?handsome?
and ?luminous.?3.
If the original (correct) sentence has a betterscore than any of these alternates, reject thesentence.4.
Else, score each option according to how well itand its immediate predecessor predict the nextword.
For example, the probability of ?and?following ?most handsome?
might be 0.012.5.
Sort the predicted words according to thisscore, and retain the top 30 options.In step 3, omitting questions for which the correctsentence is the best makes the set of options moredifficult to solve with a language model alone.
How-ever, by allowing the correct sentence to potentiallyfall below the set of alternates retained, an oppositebias is created: the language model will tend to as-sign a lower score to the correct option than to thealternates (which were chosen by virtue of scoringwell).
We measured the bias by performing a test onthe 1,040 test sentences using the language model,and choosing the lowest scoring candidate as the an-swer.
This gave an accuracy of 26% (as opposed to31%, found by taking the highest scoring candidate:recall that a random choice would give 20% in ex-pectation).
Thus although there is some remainingbias for the answer to be low scoring, it is small.When a language model other than the precise oneused to generate the data is used, the score reversaltest yielded 17% correct.
The correct polarity gave39%.
If, however, just the single score used to dothe sort in the last step is used (i.e.
the probability31of the immediate successor alone), then the lowestscoring alternate is correct about 38% of the time -almost as much as the language model itself.
Theuse of the word score occurring two positions af-ter the focus also achieves 38%, though a positivepolarity is beneficial here.
Combined, these scoresachieve about 43%.
Neither is anywhere close tohuman performance.
We are currently evaluatinga second round of test questions, in which we stillsample options based on the preceding history, butre-order them according the the total sentence prob-ability P (w1 .
.
.
wN ).The overall procedure has the effect of providingoptions which are both well-predicted by the imme-diate history, and predictive of the immediate future.Since in total the procedure uses just four consec-utive words, it cannot be expected to provide glob-ally coherent alternates.
However, sometimes it doesproduce synonyms to the correct word, as well assyntactically invalid options, which must be weededout.
For this, we examine the alternates by hand.2.3 Human GroomingThe human judges picked the best four choices ofimpostor sentences from the automatically gener-ated list of thirty, and were given the following in-structions:1.
All chosen sentences should be grammaticallycorrect.
For example: He dances while he atehis pipe would be illegal.2.
Each correct answer should be unambiguous.In other words, the correct answer should al-ways be a significantly better fit for that sen-tence than each of the four impostors; it shouldbe possible to write down an explanation as towhy the correct answer is the correct answer,that would persuade most reasonable people.3.
Sentences that might cause offense or contro-versy should be avoided.4.
Ideally the alternatives will require somethought in order to determine the correct an-swer.
For example:?
Was she his [ client | musings | discomfi-ture | choice | opportunity ] , his friend ,or his mistress?would constitute a good test sentence.
In orderto arrive at the correct answer, the student mustnotice that, while ?musings?
and ?discomfi-ture?
are both clearly wrong, the terms friendand mistress both describe people, which there-fore makes client a more likely choice thanchoice or opportunity.5.
Alternatives that require understanding proper-ties of entities that are mentioned in the sen-tence are desirable.
For example:?
All red-headed men who are above the ageof [ 800 | seven | twenty-one | 1,200 |60,000 ] years , are eligible.requires that the student realize that a man can-not be seven years old, or 800 or more.
How-ever, such examples are rare: most often, arriv-ing at the answer will require thought, but notdetailed entity knowledge, such as:?
That is his [ generous | mother?s | suc-cessful | favorite | main ] fault , but onthe whole he?s a good worker.6.
Dictionary use is encouraged, if necessary.7.
A given sentence from the set of five novelsshould only be used once.
If more than onefocus word has been identified for a sentence(i.e.
different focuses have been identified, indifferent positions), choose the set of sentencesthat generates the best challenge, according tothe above guidelines.Note that the impostors sometimes constitute aperfectly fine completion, but that in those cases, thecorrect completion is still clearly identifiable as themost likely completion.2.4 Sample QuestionsFigure 2 shows ten examples of the Holmesderived questions.
The full set is availableat http://research.microsoft.com/en-us/projects/scc/.3 Guidelines for UseIt is important for users of this data to realize the fol-lowing: since the test data was taken from five 19thcentury novels, the test data itself is likely to occur in321) I have seen it on him , and could to it.a) write b) migrate c) climb d) swear e) contribute2) They seize him and use violence towards him in order to make him sign some papers to makeover the girl?s of which he may be trustee to them.a) appreciation b) activity c) suspicions d) administration e) fortune3) My morning?s work has not been , since it has proved that he has the very strongestmotives for standing in the way of anything of the sort.a) invisible b) neglected c) overlooked d) wasted e) deliberate4) It was furred outside by a thick layer of dust , and damp and worms had eaten through the wood, so that a crop of livid fungi was on the inside of it.a) sleeping b) running c) resounding d) beheaded e) growing5) Presently he emerged , looking even more than before.a) instructive b) reassuring c) unprofitable d) flurried e) numerous6) We took no to hide it.a) fault b) instructions c) permission d) pains e) fidelity7) I stared at it , not knowing what was about to issue from it.a) afterwards b) rapidly c) forever d) horror-stricken e) lightly8) The probability was , therefore , that she was the truth , or , at least , a part of the truth.a) addressing b) telling c) selling d) surveying e) undergoing9) The furniture was scattered about in every direction , with dismantled shelves and open drawers, as if the lady had hurriedly them before her flight.a) warned b) rebuked c) assigned d) ransacked e) taught10) The sun had set and was settling over the moor.a) dusk b) mischief c) success d) disappointment e) laughterFigure 2: The first ten questions from the Holmes Corpus.the index of most Web search engines, and in otherlarge scale data-sets that were constructed from webdata (for example, the Google N-gram project).
Forexample, entering the string That is his fault , but onthe whole he?s a good worker (one of the sentenceexamples given above, but with the focus word re-moved) into the Bing search engine results in thecorrect (full) sentence at the top position.
It is im-portant to realize that researchers may inadvertentlyget better results than truly warranted because theyhave used data that is thus tainted by the test set.To help prevent any such criticism from being lev-eled at a particular publication, we recommend thanin any set of published results, the exact data usedfor training and validation be specified.
The train-ing data provided on our website may also be con-sidered ?safe?
and useful for making comparisonsacross sites.4 Baseline Results4.1 A Simple 4-gram modelAs a sanity check we constructed a very simple N-gram model as follows: given a test sentence (withthe position of the focus word known), the score forthat sentence was initialized to zero, and then incre-33mented by one for each bigram match, by two foreach trigram match, and by three for each 4-grammatch, where a match means that the N-gram inthe test sentence containing the focus word occursat least once in the background data.
This simplemethod achieved 34% correct (compared to 20% byrandom choice) on the test set.4.2 Smoothed N-gram modelAs a somewhat more sophisticated baseline, we usethe CMU language modeling toolkit 1 to build a 4-gram language model using Good-Turing smooth-ing.
We kept all bigrams and trigrams occurringin the data, as well as four-grams occurring at leasttwice.
We used a vocabulary of the 126k words thatoccurred five or more times, resulting in a total of26M N-grams.
Sentences were ordered according totheir probability according to the language model:P (w1 .
.
.
wN ).
This improved by 5% absolute onthe simple baseline to achieve 39% correct.4.3 Latent Semantic Analysis SimilarityAs a final benchmark, we present scores for a novelmethod based on latent semantic analysis.
In thisapproach, we treated each sentence in the trainingdata as a ?document?
and performed latent semanticanalysis (Deerwester et al, 1990) to obtain a 300dimensional vector representation of each word inthe vocabulary.
Denoting two words by their vectorsx,y, their similarity is defined as the cosine of theangle between them:sim(x,y) =x ?
y?
x ??
y ?.To decide which option to select, we computed theaverage similarity to every other word in the sen-tence, and then output the word with the greatestoverall similarity.
This results in our best baselineperformance, at 49% correct.4.4 Benchmark SummaryTable 1 summarizes our benchmark study.
First, forreference, we had an unaffiliated human answer arandom subset of 100 questions.
Ninety-one per-cent were answered correctly, showing that scoresin the range of 90% are reasonable to expect.
Sec-ondly, we tested the performance of the same model1http://www.speech.cs.cmu.edu/SLM/toolkit.htmlMethod % Correct (N=1040)Human 91Generating Model 31Smoothed 3-gram 36Smoothed 4-gram 39Positional combination 43Simple 4-gram 34Average LSA Similarity 49Table 1: Summary of Benchmarks(Model M) that was used to generate the data.
Be-cause this model output alternates that it assignshigh-probability, there is a bias against it, and itscored 31%.
Smoothed 3 and 4-gram models builtwith the CMU toolkit achieved 36 to 39 percent.
Re-call that the sampling process introduced some biasinto the word scores at specific positions relative tothe focus word.
Exploiting the negative bias inducedon the immediately following word, and combin-ing it with the score of the word two positions inthe future, we were able to obtain 43%.
The sim-ple 4-gram model described earlier did somewhatworse than the other N-gram language models, andthe LSA similarity model did best with 49%.
Asa further check on this data, we have run the sametests on 108 sentence completion questions from apractice SAT exam (Princeton Review, 11 PracticeTests for the SAT & PSAT, 2011 Edition).
To trainlanguage models for the SAT question task, we used1.2 billion words of Los Angeles Times data takenfrom the years 1985 through 2002.
Results for theSAT data are similar, with N-gram language modelsscoring 42-44% depending on vocabulary size andsmoothing, and LSA similarity attaining 46%.These results indicate that the ?Holmes?
sentencecompletion set is indeed a challenging problem, andhas a level of difficulty roughly comparable to thatof SAT questions.
Simple models based on N-gramstatistics do quite poorly, and even a relatively so-phisticated semantic-coherence model struggles tobeat the 50% mark.5 Related WorkThe past work which is most similar to ours is de-rived from the lexical substitution track of SemEval-2007 (McCarthy and Navigli, 2007).
In this task,the challenge is to find a replacement for a word or34phrase removed from a sentence.
In contrast to ourSAT-inspired task, the original answer is indicated.For example, one might be asked to find replace-ments for match in ?After the match, replace any re-maining fluid deficit to prevent problems of chronicdehydration throughout the tournament.?
Scoringis done by comparing a system?s results with thoseproduced by a group of human annotators (not un-like the use of multiple translations in machine trans-lation).
Several forms of scoring are defined us-ing formulae which make the results impossible tocompare with correct/incorrect multiple choice scor-ing.
Under the provided scoring metrics, two con-sistently high-performing systems in the SemEval2007 evaluations are the KU (Yuret, 2007) and UNT(Hassan et al, 2007) systems.
These operate in twophases: first they find a set of potential replacementwords, and then they rank them.
The KU systemuses just an N-gram language model to do this rank-ing.
The UNT system uses a large variety of infor-mation sources, each with a different weight.
A lan-guage model is used, and this receives the highestweight.
N-gram statistics were also very effective -according to one of the scoring paradigms - in (Giu-liano et al, 2007); as a separate entry, this paper fur-ther explored the use of Latent Semantic Analysisto measure the degree of similarity between a poten-tial replacement and its context, but the results werepoorer than others.
Since the original word providesa strong hint as to the possible meanings of the re-placements, we hypothesize that N-gram statisticsare largely able to resolve the remaining ambigui-ties, thus accounting for the good performance ofthese methods on this task.
The Holmes data doesnot have this property and thus may be more chal-lenging.ESL synonym questions were studied by Turney(2001), and subsequently considered by numerousresearch groups including Terra and Clarke (2003)and Pado and Lapata (2007).
These questions areeasier than the SemEval task because in addition tothe original word and the sentence context, the listof options is provided.
For example, one might beasked to identify a replacement for ?rusty?
in ?A[rusty] nail is not as strong as a clean, new one.
(corroded; black; dirty; painted).?
Jarmasz andSzpakowicz (2003) used a sophisticated thesaurus-based method and achieved state-of-the art perfor-mance on the ESL synonyms task, which is 82%.Again the Holmes data does not have the propertythat the intended meaning is signaled by providingthe original word, thus adding extra challenge.Although it was not developed for this task, webelieve the recurrent language modeling work ofMikolov (2010; 2011b; 2011a) is also quite rel-evant.
In this work, a recurrent neural net lan-guage model is used to achieve state-of-the-art per-formance in perplexity and speech recognition er-ror rates.
Critically, the recurrent neural net doesnot maintain a fixed N-gram context, and its hid-den layer has the potential to model overall sen-tence meaning and long-span coherence.
While the-oretical results (Bengio et al, 1994) indicate thatextremely long-range phenomena are hard to learnwith a recurrent neural network, in practice the spanof usual sentences may be manageable.
Recursiveneural networks (Socher et al, 2011) offer similaradvantages, without the theoretical limitations.
Bothoffer promising avenues of research.6 ConclusionIn this paper we have described a new, publiclyavailable, corpus of sentence-completion questions.Whereas for many traditional language modelingtasks, N-gram models provide state-of-the-art per-formance, and may even be fully adequate, this taskis designed to be insoluble with local models.
Be-cause the task now allows us to measure progressin an area where N-gram models do poorly, we ex-pect it to stimulate research in fundamentally newand more powerful language modeling methods.ReferencesYoshua Bengio, Patrice Simard, and Paolo Frasconi.1994.
Learning long-term dependencies with gradi-ent descent is difficult.
IEEE Transactions on NeuralNetworks, 5(2):157 ?166.Ciprian Chelba and Frederick Jelinek.
1998.
Exploit-ing syntactic structure for language modeling.
In Pro-ceedings of the 36th Annual Meeting of the Associationfor Computational Linguistics and 17th InternationalConference on Computational Linguistics - Volume 1,ACL ?98, pages 225?231, Stroudsburg, PA, USA.
As-sociation for Computational Linguistics.Ciprian Chelba and Frederick Jelinek.
2000.
Structured35language modeling.
Computer Speech and Language,14(4):283 ?
332.Stanley F. Chen and Joshua Goodman.
1999.
An empir-ical study of smoothing techniques for language mod-eling.
Computer Speech and Language, 13(4):359 ?393.S.
Chen.
2009a.
Performance prediction for exponentiallanguage models.
In NAACL-HLT.S.
Chen.
2009b.
Shrinking exponential language models.In NAACL-HLT.S.
Deerwester, S.T.
Dumais, G.W.
Furnas, T.K.
Landauer,and R. Harshman.
1990.
Indexing by latent semanticanalysis.
Journal of the American Society for Informa-tion Science, 41(96).Educational-Testing-Service.
2011.https://satonlinecourse.collegeboard.com/sr/ digi-tal assets/assessment/pdf/0833a611-0a43-10c2-0148-cc8c0087fb06-f.pdf.Claudio Giuliano, Alfio Gliozzo, and Carlo Strapparava.2007.
Fbk-irst: Lexical substitution task exploitingdomain and syntagmatic coherence.
In Proceedingsof the 4th International Workshop on Semantic Evalu-ations, SemEval ?07, pages 145?148, Stroudsburg, PA,USA.
Association for Computational Linguistics.Samer Hassan, Andras Csomai, Carmen Banea, RaviSinha, and Rada Mihalcea.
2007.
Unt: Subfinder:Combining knowledge sources for automatic lexicalsubstitution.
In Proceedings of the 4th InternationalWorkshop on Semantic Evaluations, SemEval ?07,pages 410?413, Stroudsburg, PA, USA.
Associationfor Computational Linguistics.Sanjeev Khudanpur and Jun Wu.
2000.
Maximumentropy techniques for exploiting syntactic, semanticand collocational dependencies in language modeling.Computer Speech and Language, 14(4):355 ?
372.R.
Kneser and H. Ney.
1995.
Improved backing-offfor m-gram language modeling.
In Proceedings ofICASSP.Jarmasz M. and Szpakowicz S. 2003.
Roget?s thesaurusand semantic similarity.
In Recent Advances in Natu-ral Language Processing (RANLP).Diana McCarthy and Roberto Navigli.
2007.
Semeval-2007 task 10: English lexical substitution task.
In Pro-ceedings of the 4th International Workshop on Seman-tic Evaluations (SemEval-2007), pages 48?53.Tomas Mikolov, Martin Karafiat, Jan Cernocky, and San-jeev Khudanpur.
2010.
Recurrent neural networkbased language model.
In Proceedings of Interspeech2010.Tomas Mikolov, Anoop Deoras, Stefan Kombrink, LukasBurget, and Jan Cernocky.
2011a.
Empirical evalua-tion and combination of advanced language modelingtechniques.
In Proceedings of Interspeech 2011.Tomas Mikolov, Stefan Kombrink, Lukas Burget, JanCernocky, and Sanjeev Khudanpur.
2011b.
Ex-tensions of recurrent neural network based languagemodel.
In Proceedings of ICASSP 2011.Sebastian Pado and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Compu-tational Linguistics, 33 (2), pages 161?199.Harry Printz and Peder A. Olsen.
2002.
Theory and prac-tice of acoustic confusability.
Computer Speech andLanguage, 16(1):131 ?
164.Ronald Rosenfeld, Stanley F. Chen, and Xiaojin Zhu.2001.
Whole-sentence exponential language models:a vehicle for linguistic-statistical integration.
Com-puter Speech and Language, 15(1):55 ?
73.R.
Rosenfeld.
1997.
A whole sentence maximum en-tropy language model.
In Proceedings ASRU.Holger Schwenk and Jean-Luc Gauvain.
2002.
Connec-tionist language modeling for large vocabulary contin-uous speech recognition.
In Proceedings of ICASSP.Holger Schwenk.
2007.
Continuous space languagemodels.
Computer Speech and Language, 21(3):492?
518.Claude E. Shannon and Warren Weaver.
1949.
TheMathematical Theory of Communication.
Universityof Illinois Press.Richard Socher, Cliff Chiung-Yu Lin, Andrew Y. Ng,and Christopher D. Manning.
2011.
Parsing naturalscenes and natural language with recursive neural net-works.
In Proceedings of the 2011 International Con-ference on Machine Learning (ICML-2011).E.
Terra and C. Clarke.
2003.
Frequency estimates forstatistical word similarity measures.
In Conferenceof the North American Chapter of the Association forComputational Linguistics (NAACL).Peter D. Turney.
2001.
Mining the web for synonyms:PMI-IR versus LSA on TOEFL.
In European Confer-ence on Machine Learning (ECML).Jun Wu and Sanjeev Khudanpur.
1999.
Combining non-local, syntactic and n-gram dependencies in languagemodeling.
In Proceedings of Eurospeech.Kenji Yamada and Kevin Knight.
2001.
A syntax-based statistical translation model.
In Proceedings ofthe 39th Annual Meeting on Association for Computa-tional Linguistics, ACL ?01, pages 523?530, Strouds-burg, PA, USA.
Association for Computational Lin-guistics.Deniz Yuret.
2007.
Ku: word sense disambiguationby substitution.
In Proceedings of the 4th Interna-tional Workshop on Semantic Evaluations, SemEval?07, pages 207?213, Stroudsburg, PA, USA.
Associ-ation for Computational Linguistics.36
