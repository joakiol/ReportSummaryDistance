Proceedings of the 2009 Workshop on Multiword Expressions, ACL-IJCNLP 2009, pages 17?22,Suntec, Singapore, 6 August 2009. c?2009 ACL and AFNLPVerb Noun Construction MWE Token Supervised ClassificationMona T. DiabCenter for Computational Learning SystemsColumbia Universitymdiab@ccls.columbia.eduPravin BhutadaComputer Science DepartmentColumbia Universitypb2351@columbia.eduAbstractWe address the problem of classifying multi-word expression tokens in running text.
Wefocus our study on Verb-Noun Constructions(VNC) that vary in their idiomaticity depend-ing on context.
VNC tokens are classified aseither idiomatic or literal.
We present a super-vised learning approach to the problem.
We ex-periment with different features.
Our approachyields the best results to date on MWE clas-sification combining different linguistically mo-tivated features, the overall performance yieldsan F-measure of 84.58% corresponding to an F-measure of 89.96% for idiomaticity identificationand classification and 62.03% for literal identifi-cation and classification.1 IntroductionIn the literature in general a multiword expression(MWE) refers to a multiword unit or a colloca-tion of words that co-occur together statisticallymore than chance.
A MWE is a cover term fordifferent types of collocations which vary in theirtransparency and fixedness.
MWEs are pervasivein natural language, especially in web based textsand speech genres.
Identifying MWEs and under-standing their meaning is essential to language un-derstanding, hence they are of crucial importancefor any Natural Language Processing (NLP) appli-cations that aim at handling robust language mean-ing and use.
In fact, the seminal paper (Sag et al,2002) refers to this problem as a key issue for thedevelopment of high-quality NLP applications.For our purposes, a MWE is defined as a collo-cation of words that refers to a single concept, forexample - kick the bucket, spill the beans, make adecision, etc.
An MWE typically has an idiosyn-cratic meaning that is more or different from themeaning of its component words.
AnMWEmean-ing is transparent, i.e.
predictable, in as muchas the component words in the expression relaythe meaning portended by the speaker composi-tionally.
Accordingly, MWEs vary in their de-gree of meaning compositionality; composition-ality is correlated with the level of idiomaticity.An MWE is compositional if the meaning of anMWE as a unit can be predicted from the mean-ing of its component words such as in make adecision meaning to decide.
If we conceive ofidiomaticity as being a continuum, the more id-iomatic an expression, the less transparent and themore non-compositional it is.
Some MWEs aremore predictable than others, for instance, kick thebucket, when used idiomatically to mean to die,has nothing in common with the literal meaningof either kick or bucket, however, make a decisionis very clearly related to to decide.
Both of theseexpressions are considered MWEs but have vary-ing degrees of compositionality and predictability.Both of these expressions belong to a class of id-iomatic MWEs known as verb noun constructions(VNC).
The first VNC kick the bucket is a non-decomposable VNC MWE, the latter make a deci-sion is a decomposable VNC MWE.
These typesof constructions are the object of our study.To date, most research has addressed the prob-lem of MWE type classification for VNC expres-sions in English (Melamed, 1997; Lin, 1999;Baldwin et al, 2003; na Villada Moiro?n andTiedemann, 2006; Fazly and Stevenson, 2007;Van de Cruys and Villada Moiro?n, 2007; Mc-Carthy et al, 2007), not token classification.
Forexample: he spilt the beans on the kitchen counteris most likely a literal usage.
This is given away bythe use of the prepositional phrase on the kitchencounter, as it is plausable that beans could haveliterally been spilt on a location such as a kitchencounter.
Most previous research would classifyspilt the beans as idiomatic irrespective of con-textual usage.
In a recent study by (Cook et al,2008) of 53 idiom MWE types used in differentcontexts, the authors concluded that almost half ofthem had clear literal meaning and over 40% oftheir usages in text were actually literal.
Thus, itwould be important for an NLP application suchas machine translation, for example, when givena new VNC MWE token, to be able to determinewhether it is used idiomatically or not as it couldpotentially have detrimental effects on the qualityof the translation.17In this paper, we address the problem of MWEclassification for verb-noun (VNC) token con-structions in running text.
We investigate the bi-nary classification of an unseen VNC token ex-pression as being either Idiomatic (IDM) or Lit-eral (LIT).
An IDM expression is certainly anMWE, however, the converse is not necessarilytrue.
To date most approaches to the problem ofidiomaticity classification on the token level havebeen unsupervised (Birke and Sarkar, 2006; Diaband Krishna, 2009b; Diab and Krishna, 2009a;Sporleder and Li, 2009).
In this study we carryout a supervised learning investigation using sup-port vector machines that uses some of the featureswhich have been shown to help in unsupervisedapproaches to the problem.This paper is organized as follows: In Section2 we describe our understanding of the variousclasses of MWEs in general.
Section 3 is a sum-mary of previous related research.
Section 4 de-scribes our approach.
In Section 5 we present thedetails of our experiments.
We discuss the resultsin Section 6.
Finally, we conclude in Section 7.2 Multi-word ExpressionsMWEs are typically not productive, though theyallow for inflectional variation (Sag et al, 2002).They have been conventionalized due to persis-tent use.
MWEs can be classified based on theirsemantic types as follows.
Idiomatic: This cat-egory includes expressions that are semanticallynon-compositional, fixed expressions such as king-dom come, ad hoc, non-fixed expressions suchas break new ground, speak of the devil.
TheVNCs which we are focusing on in this paper fallinto this category.
Semi-idiomatic: This classincludes expressions that seem semantically non-compositional, yet their semantics are more or lesstransparent.
This category consists of Light VerbConstructions (LVC) such as make a living andVerb Particle Constructions (VPC) such as write-up, call-up.
Non-Idiomatic: This category in-cludes expressions that are semantically compo-sitional such as prime minister, proper nouns suchas New York Yankees and collocations such as ma-chine translation.
These expressions are statisti-cally idiosyncratic.
For instance, traffic light isthe most likely lexicalization of the concept andwould occur more often in text than, say, trafficregulator or vehicle light.3 Related WorkSeveral researchers have addressed the problem ofMWE classification (Baldwin et al, 2003; Katzand Giesbrecht, 2006; Schone and Juraksfy, 2001;Hashimoto et al, 2006; Hashimoto and Kawa-hara, 2008).
The majority of the proposed researchhas been using unsupervised approaches and haveaddressed the problem of MWE type classifica-tion irrespective of usage in context (Fazly andStevenson, 2007; Cook et al, 2007).
We areaware of two supervised approaches to the prob-lem: work by (Katz and Giesbrecht, 2006) andwork by (Hashimoto and Kawahara, 2008).In Katz and Giesbrecht (2006) (KG06) the au-thors carried out a vector similarity comparisonbetween the context of an MWE and that of theconstituent words using LSA to determine if theexpression is idiomatic or not.
The KG06 is sim-ilar in intuition to work proposed by (Fazly andStevenson, 2007), however the latter work was un-supervised.
KG06 experimented with a tiny dataset of only 108 sentences corresponding to oneMWE idiomatic expression.Hashimoto and Kawahara (2008) (HK08) is thefirst large scale study to our knowledge that ad-dressed token classification into idiomatic versusliteral for Japanese MWEs of all types.
They ap-ply a supervised learning framework using sup-port vector machines based on TinySVM with aquadratic kernel.
They annotate a web based cor-pus for training data.
They identify 101 idiomtypes each with a corresponding 1000 examples,hence they had a corpus of 102K sentences of an-notated data for their experiments.
They exper-iment with 90 idiom types only for which theyhad more than 50 examples.
They use two typesof features: word sense disambiguation (WSD)features and idiom features.
The WSD featurescomprised some basic syntactic features such asPOS, lemma information, token n-gram features,in addition to hypernymy information on words aswell as domain information.
For the idiom fea-tures they were mostly inflectional features suchas voice, negativity, modality, in addition to adja-cency and adnominal features.
They report resultsin terms of accuracy and rate of error reduction.Their overall accuracy is of 89.25% using all thefeatures.4 Our ApproachWe apply a supervised learning framework tothe problem of both identifying and classifying aMWE expression token in context.
We specificallyfocus on VNC MWE expressions.
We use the an-notated data by (Cook et al, 2008).
We adopt achunking approach to the problem using an InsideOutside Beginning (IOB) tagging framework forperforming the identification of MWE VNC to-kens and classifying them as idiomatic or literalin context.
For chunk tagging, we use the Yam-18Cha sequence labeling system.1 YamCha is basedon Support Vector Machines technology using de-gree 2 polynomial kernels.We label each sentence with standard IOB tags.Since this is a binary classification task, we have 5different tags: B-L (Beginning of a literal chunk),I-L (Inside of a literal chunk), B-I (Beginning anIdiomatic chunk), I-I (Inside an Idiomatic chunk),O (Outside a chunk).
As an example a sentencesuch as John kicked the bucket last Friday will beannotated as follows: John O, kicked B-I, the I-I,bucket I-I, last O, Friday O.
We experiment withsome basic features and some more linguisticallymotivated ones.We experiment with different window sizes forcontext ranging from ?/+1 to ?/+5 tokens be-fore and after the token of interest.
We also em-ploy linguistic features such as character n-gramfeatures, namely last 3 characters of a token, asa means of indirectly capturing the word inflec-tional and derivational morphology (NGRAM).Other features include: Part-of-Speech (POS)tags, lemma form (LEMMA) or the citation formof the word, and named entity (NE) information.The latter feature is shown to help in the unsuper-vised setting in recent work (Diab and Krishna,2009b; Diab and Krishna, 2009a).
In general allthe linguistic features are represented as separatefeature sets explicitly modeled in the input data.Hence, if we are modeling the POS tag feature forour running example the training data would beannotated as follows: {John NN O, kicked VBDB-I, the Det I-I, bucket NN I-I, last ADV O, FridayNN O }.
Likewise adding the NGRAM featurewould be represented as follows: {John NN ohnO, kicked VBD ked B-I, the Det the I-I, bucket NNket I-I, last ADV ast O, Friday NN day O.}
and soon.With the NE feature, we followed the same rep-resentation as the other features as a separate col-umn as expressed above, referred to as NamedEntity Separate (NES).
For named entity recogni-tion (NER) we use the BBN Identifinder softwarewhich identifies 19 NE tags.2 We have two set-tings for NES: one with the full 19 tags explic-itly identified (NES-Full) and the other where wehave a binary feature indicating whether a wordis a NE or not (NES-Bin).
Moreover, we addedanother experimental condition where we changedthe words?
representation in the input to their NEclass, Named Entity InText (NEI).
For example forthe NEI condition, our running example is repre-sented as follows: {PER NN ohn O, kicked VBDked B-I, the Det the I-I, bucket NN ket I-I, last ADV1http://www.tado-chasen.com/yamcha2http://www.bbn.com/identifinderast O, DAY NN day O}, where John is replaced bythe NE ?PER?
.5 Experiments and Results5.1 DataWe use the manually annotated standard dataset identified in (Cook et al, 2008).
This datacomprises 2920 unique VNC-Token expressionsdrawn from the entire British National Corpus(BNC).3 The BNC contains 100M words of multi-ple genres including written text and transcribedspeech.
In this set, VNC token expressions aremanually annotated as idiomatic, literal or un-known.
We exclude those annotated as unknownand those pertaining to the Speech part of thedata leaving us with a total of 2432 sentences cor-responding to 53 VNC MWE types.
This datahas 2571 annotations,4 corresponding to 2020 Id-iomatic tokens and 551 literal ones.
Since the dataset is relatively small we carry out 5-fold cross val-idation experiments.
The results we report are av-eraged over the 5 folds per condition.
We splitthe data into 80% for training, 10% for testing and10% for development.
The data used is the tok-enized version of the BNC.5.2 Evaluation MetricsWe use F?=1 (F-measure) as the harmonic meanbetween (P)recision and (R)ecall, as well as accu-racy to report the results.5 We report the resultsseparately for the two classes IDM and LIT aver-aged over the 5 folds of the TEST data set.5.3 ResultsWe present the results for the different featuressets and their combination.
We also present resultson a simple most frequent tag baseline (FREQ) aswell as a baseline of using no features, just thetokenized words (TOK).
The baseline is basicallytagging all identified VNC tokens in the data set asidiomatic.
It is worth noting that the baseline hasthe advantage of gold identification of MWE VNCtoken expressions.
In our experimental conditions,identification of a potential VNC MWE is part ofwhat is discovered automatically, hence our sys-tem is penalized for identifying other VNC MWE3http://www.natcorp.ox.ac.uk/4A sentence can have more than one MWE expressionhence the number of annotations exceeds the number of sen-tences.5We do not think that accuracy should be reported in gen-eral since it is an inflated result as it is not a measure of error.All words identified as O factor into the accuracy which re-sults in exaggerated values for accuracy.
We report it onlysince it the metric used by previous work.19tokens that are not in the original data set.6In Table 2 we present the results yielded per fea-ture and per condition.
We experimented with dif-ferent context sizes initially to decide on the opti-mal window size for our learning framework, re-sults are presented in Table 1.
Then once that isdetermined, we proceed to add features.Noting that a window size of ?/+3 yields thebest results, we proceed to use that as our contextsize for the following experimental conditions.
Wewill not include accuracy since it above 96% for allour experimental conditions.All the results yielded by our experiments out-perform the baseline FREQ.
The simple tokenizedwords baseline (TOK) with no added features witha context size of ?/+3 shows a significant im-provement over the very basic baseline FREQwithan overall F measure of 77.04%.Adding lemma information or POS or NGRAMfeatures all independently contribute to a bettersolution, however combining the three featuresyields a significant boost in performance over theTOK baseline of 2.67% absolute F points in over-all performance.Confirming previous observations in the liter-ature, the overall best results are obtained byusing NE features.
The NEI condition yieldsslightly better results than the NES conditionsin the case when no other features are beingused.
NES-Full significantly outperforms NES-Bin when used alone especially on literal classi-fication yielding the highest results on this classof phenomena across the board.
However whencombined with other features, NES-Bin fares bet-ter than NES-Full as we observe slightly less per-formance when comparing NES-Full+L+N+P andNES-Bin+L+N+P.Combining NEI+L+N+P yields the highest re-sults with an overall F measure of 84.58% a sig-nificant improvement over both baselines and overthe condition that does not exploit NE features,L+N+P.
Using NEI may be considered a formof dimensionality reduction hence the significantcontribution to performance.6 DiscussionThe overall results strongly suggest that using lin-guistically interesting features explicitly has a pos-itive impact on performance.
NE features helpthe most and combining them with other features6We could have easily identified all VNC syntactic con-figurations corresponding to verb object as a potential MWEVNC assuming that they are literal by default.
This wouldhave boosted our literal score baseline, however, for this in-vestigation, we decided to strictly work with the gold stan-dard data set exclusively.yields the best results.
In general performanceon the classification and identification of idiomaticexpressions yielded much better results.
This maybe due to the fact that the data has a lot more id-iomatic token examples for training.
Also we notethat precision scores are significantly higher thanrecall scores especially with performance on lit-eral token instance classification.
This might be anindication that identifying when an MWE is usedliterally is a difficult task.We analyzed some of the errors yielded in ourbest condition NEI+L+N+P.
The biggest errors area result of identifying other VNC constructionsnot annotated in the training and test data as VNCMWEs.
However, we also see errors of confusingidiomatic cases with literal ones 23 times, and theopposite 4 times.Some of the errors where the VNC should havebeen classified as literal however the system clas-sified them as idiomatic are kick heel, find feet,make top.
Cases of idiomatic expressions erro-neously classified as literal are for MWE types hitthe road, blow trumpet, blow whistle, bit a wall.The system is able to identify new VNC MWEconstructions.
For instance in the sentence On theother hand Pinkie seemed to have lost his head toa certain extent perhaps some prospects of mak-ing his mark by bringing in something novel inthe way of business, the first MWE lost his headis annotated in the training data, however makinghis mark is newly identified as idiomatic in thiscontext.Also the system identified hit the post as aliteral MWE VNC token in As the ball hit thepost the referee blew the whistle, where blew thewhistle is a literal VNC in this context and it iden-tified hit the post as another literal VNC.7 ConclusionIn this study, we explore a set of features that con-tribute to VNC token expression binary supervisedclassification.
The use of NER significantly im-proves the performance of the system.
Using NERas a means of dimensionality reduction yields thebest results.
We achieve a state of the art perfor-mance of an overall F measure of 84.58%.
In thefuture we are looking at ways of adding more so-phisticated syntactic and semantic features fromWSD.
Given the fact that we were able to get moreinteresting VNC data automatically, we are cur-rently looking into adding the new data to the an-notated pool after manual checking.20IDM-F LIT-F Overall F Overall Acc.
?/+1 77.93 48.57 71.78 96.22?/+2 85.38 55.61 79.71 97.06?/+3 86.99 55.68 81.25 96.93?/+4 86.22 55.81 80.75 97.06?/+5 83.38 50 77.63 96.61Table 1: Results in %s of varying context window sizeIDM-P IDM-R IDM-F LIT-P LIT-R LIT-F Overall FFREQ 70.02 89.16 78.44 0 0 0 69.68TOK 81.78 83.33 82.55 71.79 43.75 54.37 77.04(L)EMMA 83.1 84.29 83.69 69.77 46.88 56.07 78.11(N)GRAM 83.17 82.38 82.78 70 43.75 53.85 77.01(P)OS 83.33 83.33 83.33 77.78 43.75 56.00 78.08L+N+P 86.95 83.33 85.38 72.22 45.61 55.91 79.71NES-Full 85.2 87.93 86.55 79.07 58.62 67.33 82.77NES-Bin 84.97 82.41 83.67 73.49 52.59 61.31 79.15NEI 89.92 85.18 87.48 81.33 52.59 63.87 82.82NES-Full+L+N+P 89.89 84.92 87.34 76.32 50 60.42 81.99NES-Bin+L+N+P 90.86 84.92 87.79 76.32 50 60.42 82.33NEI+L+N+P 91.35 88.42 89.86 81.69 50 62.03 84.58Table 2: Final results in %s averaged over 5 folds of test data using different features and their combina-tions8 AcknowledgementThe first author was partially funded by DARPAGALE andMADCAT projects.
The authors wouldlike to acknowledge the useful comments by twoanonymous reviewers who helped in making thispublication more concise and better presented.ReferencesTimothy Baldwin, Collin Bannard, Takakki Tanaka,and Dominic Widdows.
2003.
An empirical modelof multiword expression decomposability.
In Pro-ceedings of the ACL 2003 workshop on Multiwordexpressions, pages 89?96, Morristown, NJ, USA.J.
Birke and A. Sarkar.
2006.
A clustering approach fornearly unsupervised recognition of nonliteral lan-guage.
In Proceedings of EACL, volume 6, pages329?336.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2007.
Pulling their weight: Exploiting syntacticforms for the automatic identification of idiomaticexpressions in context.
In Proceedings of the Work-shop on A Broader Perspective on Multiword Ex-pressions, pages 41?48, Prague, Czech Republic,June.
Association for Computational Linguistics.Paul Cook, Afsaneh Fazly, and Suzanne Stevenson.2008.
The VNC-Tokens Dataset.
In Proceedings ofthe LREC Workshop on Towards a Shared Task forMultiword Expressions (MWE 2008), Marrakech,Morocco, June.Mona Diab and Madhav Krishna.
2009a.
Handlingsparsity for verb noun MWE token classification.
InProceedings of the Workshop on Geometrical Mod-els of Natural Language Semantics, pages 96?103,Athens, Greece, March.
Association for Computa-tional Linguistics.Mona Diab and Madhav Krishna.
2009b.
Unsuper-vised classification for vnc multiword expressionstokens.
In CICLING.Afsaneh Fazly and Suzanne Stevenson.
2007.
Dis-tinguishing subtypes of multiword expressions us-ing linguistically-motivated statistical measures.
InProceedings of the Workshop on A Broader Perspec-tive on Multiword Expressions, pages 9?16, Prague,Czech Republic, June.
Association for Computa-tional Linguistics.Chikara Hashimoto and Daisuke Kawahara.
2008.Construction of an idiom corpus and its applica-tion to idiom identification based on WSD incor-porating idiom-specific features.
In Proceedings ofthe 2008 Conference on Empirical Methods in Nat-ural Language Processing, pages 992?1001, Hon-olulu, Hawaii, October.
Association for Computa-tional Linguistics.Chikara Hashimoto, Satoshi Sato, and Takehito Utsuro.2006.
Japanese idiom recognition: Drawing a linebetween literal and idiomatic meanings.
In Proceed-ings of the COLING/ACL 2006 Main Conference21Poster Sessions, pages 353?360, Sydney, Australia,July.
Association for Computational Linguistics.Graham Katz and Eugenie Giesbrecht.
2006.
Au-tomatic identification of non-compositional multi-word expressions using latent semantic analysis.
InProceedings of the Workshop on Multiword Expres-sions: Identifying and Exploiting Underlying Prop-erties, pages 12?19, Sydney, Australia, July.
Asso-ciation for Computational Linguistics.Dekang Lin.
1999.
Automatic identification of non-compositional phrases.
In Proceedings of ACL-99,pages 317?324, Univeristy of Maryland, CollegePark, Maryland, USA.Diana McCarthy, Sriram Venkatapathy, and AravindJoshi.
2007.
Detecting compositionality of verb-object combinations using selectional preferences.In Proceedings of the 2007 Joint Conference onEmpirical Methods in Natural Language Process-ing and Computational Natural Language Learning(EMNLP-CoNLL), pages 369?379, Prague, CzechRepublic, June.
Association for Computational Lin-guistics.Dan I. Melamed.
1997.
Automatic discovery of non-compositional compounds in parallel data.
In Pro-ceedings of the 2nd Conference on Empirical Meth-ods in Natural Language Processing (EMNLP?97),pages 97?108, Providence, RI, USA, August.Bego na Villada Moiro?n and Jo?rg Tiedemann.
2006.Identifying idiomatic expressions using automaticword-alignment.
In Proceedings of the EACL-06Workshop on Multiword Expressions in a Multilin-gual Context, pages 33?40, Morristown, NJ, USA.Ivan A.
Sag, Timothy Baldwin, Francis Bond, Ann A.Copestake, and Dan Flickinger.
2002.
Multiwordexpressions: A pain in the neck for nlp.
In Pro-ceedings of the Third International Conference onComputational Linguistics and Intelligent Text Pro-cessing, pages 1?15, London, UK.
Springer-Verlag.Patrick Schone and Daniel Juraksfy.
2001.
Isknowledge-free induction of multiword unit dictio-nary headwords a solved problem?
In Proceedingsof Empirical Methods in Natural Language Process-ing, pages 100?108, Pittsburg, PA, USA.C.
Sporleder and L. Li.
2009.
Unsupervised Recog-nition of Literal and Non-Literal Use of IdiomaticExpressions.
In Proceedings of the 12th Conferenceof the European Chapter of the ACL (EACL 2009),pages 754?762.
Association for Computational Lin-guistics.Tim Van de Cruys and Begon?a Villada Moiro?n.
2007.Semantics-based multiword expression extraction.In Proceedings of the Workshop on A Broader Per-spective on Multiword Expressions, pages 25?32,Prague, Czech Republic, June.
Association for Com-putational Linguistics.22
