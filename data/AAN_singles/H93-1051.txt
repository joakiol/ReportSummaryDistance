CORPUS-BASED STAT IST ICAL  SENSE RESOLUTIONClaudia Leacock, 1 Geoffrey Towell, 2 Ellen Voorhees 21Princeton University, Cognitive Science Laboratory, Princeton, New Jersey 085422Siemens Corporate Research, Inc., Princeton, New Jersey 08540ABSTRACTThe three corpus-based statistical sense resolution methodsstudied here attempt o infer the correct sense of a polyse-mous word by using knowledge about patterns of word co-occurrences.
The techniques were based on Bayesian decisiontheory, neural networks, and content vectors as used in in-formation retrieval.
To understand these methods better, weposed s very specific problem: given a set of contexts, eachcontaining the noun line in a known sense, construct a classi-fier that selects the correct sense of line for new contexts.
Tosee how the degree of polysemy affects performance, resultsfrom three- and slx-sense tasks are compared.The results demonstrate hat each of the techniques i able todistinguish six senses of line with an accuracy greater than70%.
Furthermore, the response patterns of the classifiersare, for the most part, statistically indistinguishable fromone another.
Comparison of the two tasks suggests that thedegree of difficulty involved in resolving individual senses isa greater performance factor than the degree of polysemy.1.
INTRODUCTIONThe goal of this study is to systematical ly explore theeffects of such variables as the number of  senses perword and the number of training examples per sense oncorpus-based statistical sense resolution methods.
To en-able us to study the effects of  the number of word senses,we selected the highly polysemous noun line, which has25 senses in WordNet.
1Automat ic  sense resolution systems need to resolvehighly polysemous words.
As Zipf \[2\] pointed out in1945, frequently occurring words tend to be polysemous.The words encountered in a given text will have fargreater polysemy than one would assume by simply tak-ing the overall percentage of po\]ysemous words in thelanguage.
Even though 86% of the nouns in WordNethave a single sense, the mean number of WordNet sensesper word for the one hundred most frequently occurringnouns in the Brown Corpus is 5.15, with only eight wordshaving a single sense.1WordNet is a lexical database developed by George Miller andhis colleagues at Princeton Urdversity.\[l\]2.
PREVIOUS WORKYarowsky \[3\] compared the Bayesian statistical methodwith the published results of other corpus-based statisti-cal models.
Although direct comparison was not possibledue to the differences in corpora and evaluation criteria,he minimizes these differences by using the same words,with the same definition of sense.
He argues, convinc-ingly, that the Bayesian model is as good as or betterthan the costlier methods.As a pilot for the present study, a two-sense distinctiontask for line was run using the content vector and neuralnetwork classifiers, achieving reater than 90~ accuracy.A three-sense distinction task was then run, which is re-ported in Voorhees, st. al.
\[4\], and discussed in Section 5.3.
METHODOLOGYThe training and testing contexts were taken from the .1987-89 Wall Street Journal corpus and from the APHBcorpus.
2 Sentences containing '\[L1\]ine(s)' were extractedand manual ly assigned a single sense from WordNet.Sentences containing proper names such as ' Japan AirLines' were removed from the set of sentences.
Sentencescontaining collocations that  have a single sense in Word-Net, such as product line and line of products, were alsoexcluded since the collocations are not ambiguous.Typically, experiments have used a fixed number ofwords or characters on either side of the target as thecontext.
In this experiment, we used linguistic units -sentences - instead.
Since the target word is often usedanaphorical ly to refer back to the previous sentence, wechose to use two-sentence contexts: the sentence con-taining line and the preceding sentence.
However, if thesentence containing line is the first sentence in the ar-ticle, then the context consists of one sentence.
I f  thepreceding sentence also contains line in the same sense,then an additional preceding sentence is added to thecontext, creating contexts three or more sentences long.
:ZThe 25 million word corpus, obtained from the AmericanPrinting House for the Blind, is archlved at 1BM's T.J. WatsonResearch Center; it consists of stories and articles from books andgeneral circulation magazines.260The average size of the training and testing contexts is44.5 words.The sense resolution task used the following six sensesof the noun line:1. a product: 'a  new line of workstations'2.
a formation of people or things: 'stand in line'3.
spoken or written tezt: 'a  line from Shakespeare'4.
a thin, flexible object; cord: 'a nylon line'5.
an abstract division: 'a line between good and evil'6.
a telephone connection: 'the line went dead'The classifiers were run three times each on randomlyselected training sets.
The set of contexts for each sensewas randomly permuted, with each permutation corre-sponding to one trial.
For each trial, the first 200 con-texts of each sense were selected as training contexts.The next 149 contexts were selected as test contexts.The remaining contexts were not used in that trial.
The200 training contexts for each sense were combined toform a final training set (called the 200 training set) ofsize 1200.
The final test set contained the 149 test con-texts from each sense, for a total of 894 contexts.To test the effect that the number of training exampleshas on classifier performance, smaller training sets wereextracted from the 200 training set.
The first 50 and100 contexts for each sense were used to build the newtraining sets.
The same set of 894 test contexts wereused with each of the training sets in a given trial.
Eachof the classifiers used the same training and test contextswithin the same trial, but processed the text differentlyaccording to the needs of the method.4.
THE CLASS IF IERSThe only information used by the three classifiers is co-occurrence of character strings in the contexts.
Theyuse no other cues, such as syntactic tags or word order.Nor do they require any augmentation of the trainingcontexts that is not fully automatic.4 .1 .
A Bayes ian  ApproachThe Bayesian classifier, developed by Gale, Church andYarowsky \[5\], uses Bayes' decision theory for weightingtokens that co-occur with each sense of a polysemoustarget.
Their work is inspired by Mosteller and Wallace\[6\], who applied Bayes' theorem to the problem of au-thor discrimination.
The main component of the model,a token, was defined as any character string: a word,number, symbol, punctuation or any combination.
Theentire token is significant, so inflected forms of a baseword (wait vs. waiting) and mixed case strings (Bushvs.
bush) are distinct tokens.
Associated with each to-ken is a set of saliences, one for each sense, calculatedfrom the training data.
The salience of a token for agiven sense is Pr(tolzenlsense)/Pr(token ).
The weightof a token for a given sense is the log of its salience.To select the sense of the target word in a (test) con-text, the classifier computes the sum of the tokcns'weights over all tokens in the context for each sense,and selects the sense with the largest sum.
In thecase of author identification, Mosteller and Wallacebuilt their models using high frequency function words.With sense resolution, the salient tokens include contentwords, which have much lower frequencies of occurrence.Gale, et.
al.
devised a method for estimating the requiredprobabilities using sparse training data, since the max-imum likelihood estimate (MLE) of a probability - thenumber of times a token appears in a set of contextsdivided by the total number of tokens in the set of con-texts - is a poor estimate of the true probability.
In par-ticular, many tokens in the test contexts do not appearin any training context, or appear only once or twice.
Inthe former case, the MLE is zero, obviously smaller thanthe true probability; in the latter case, the MLE is muchlarger than the true probability.
Gale, et.
al.
adjust theirestimates for new or infrequent words by interpolatingbetween local and global estimates of the probability.The Bayesian classifier experiments were performed byKenneth Church of AT&T Bell Laboratories.
In theseexperiments, two-sentence contexts are used in place ofa fixed-sized window of ?50 tokens surrounding the tar-get word that Gale, et.
al.
find optimal, s resulting in asmaller amount of context used to estimate the proba-bilities.4 .2 .
Content  VectorsThe content vector approach to sense resolution is moti-vated by the vector-space model of information retrievalsystems \[8\], where each concept in a corpus defines anaxis of the vector space, and a text in the corpus is rep-resented as a point in this space.
The concepts in acorpus are usually defined as the set of word stems thatappear in the corpus (e.g., the strings computer(s), com-puting, computation(al), etc.
are conflated to the conceptcomput) minus stopwords, a set of about 570 very highfrequency words that includes function words (e.g., the,by, 7/ou, that, who, etc.)
and content words (e.g., be, say,etc.).
The similarity between two texts is computed asa function of the vectors representing the two texts.SWhereas current  research tends  to conf irm the hypothes is  thathumans  need a narrow window of ::I::2 words for sense resolut ion \[7\],Gale, et.
al.
have found much larger window sizes are better  forthe Bayesian c\]assliler, p resumably  because so much informat ion(e.g., word order and  syntax)  is thrown away.261Product Formation TextBayesian Vector Network Bayesian Vector Network Bayesian Vector  NetworkChryslerworkstationsDigitalintroducedmodelsIBMCompaqsellagreementcomputerscomputibmproduccorpsalemodelseUintroducbrandmainframecomputsellminicomputmodelintroducextendacquirlaunchcontinuquakCordBayesian Vector NetworknightcheckoutwaitgasolineoutsidewaitingfoodhourslongdriverwaitlongcheckoutparkmrairportshopcountpeoplcanadwaitlongstandcheckoutparkhourformshortcustomshopBidenadBushopeningfamousDolespeechDukakisfunnyspeechesspeechwritmrbushadspeakreaddukakbidenpoemfamiliarwritadremembdelivfamespeakfunnymoviereadfishfishingbowdeckseaboatwaterclothesfastenedshipfishboatwarhookwashfloatmendivecagerodhapfishwashpullboatropebreakhookexerciscryDivision PhoneBayesian Vector Network Bayesian Vector NetworkdrawfineblurcrosswalknarrowmrtreadfactionthindrawprivhugblurcrossfinethinfunctgeniusnarrowphonestollpornBellsouthgabtelephoneBellbillionPacificcallstelephonphoncallaccessdialgabbellservictollpornblurredwalkingcrossedethicsnarrowfineclassbetweenwalkdrawtelephonphondeadcheerhearhendersonminutcallbillsilentTable 1: The ten most heavily weighted tokens for each sense of line for the Bayesian, content vector and neuralnetwork classifiers.For the sense resolution problem, each sense is repre-sented by a single vector constructed from the trainingcontexts for that sense.
A vector in the space definedby the training contexts is also constructed for each testcontext.
To select a sense for a test context, the innerproduct between its vector and each of the sense vectorsis computed, and the sense whose inner product is thelargest is chosen.The components of the vectors are weighted to reflectthe relative importance of the concepts in the text.
Theweighting method was designed to favor concepts thatoccur frequently in exactly one sense.
The weight of aconcept c is computed as follows:Let n, : number of times c occurs in sense sp = n , /  ~senses  n,d : difference between the two largest n,(if difference is 0, d is set to 1)thenw,  = p*min(n , ,d )For example, if a concept occurs 6 times in the trainingcontexts of sense 1, and zero times in the other five setsof contexts, then its weights in the six vectors are (6, 0,0, 0, 0, 0).
However, a concept hat appears 10, 4, 7 ,0, 1, and 2 times in the respective senses, has weightsof (1.25, .5, .88, 0, .04, .17), reflecting the fact that it isnot as good an indicator for any sense.
This weightingmethod is the most effective among several variants thatwere tried.We also experimented with keeping all words in the con-tent vectors, but performance degraded, probably be-cause the weighting function does not handle very highfrequency words well.
This is evident in Table 1, where'mr' is highly weighted for three different senses.4 .3 .
Neura l  NetworkThe neural network approach \[9\] casts sense resolution asa supervised learning paradigm.
Pairs of \[input features,desired response\] arc presented to a learning program.The program's task is to devise some method for us-ing the input features to partition the training contextsinto non-overlapping sets corresponding to the desiredresponses.
This is achieved by adjusting link weights sothat the output unit representing the desired responsehas a larger activation than any other output unit.262Each context is translated into a bit-vector.
As with thecontent vector approach, suffixes are removed to con-flate related word forms to a common stem, and stop-words and punctuation axe removed.
Each concept hatappears at least twice in the entire training set is as-signed to a bit-vector position.
The resulting vector hasones in positions corresponding to concepts in the con-text and zeros otherwise.
This procedure creates vectorswith more than 4000 positions.
The vectors are, how-ever, extremely sparse; on average they contain slightlymore than 17 concepts.Networks are trained until the output of the unit cor-responding to the desired response is greater than theoutput of any other unit for every training example.
Fortesting, the classification determined by the network isgiven by the unit with the largest output.
Weights in aneural network link vector may be either positive or neg-ative, thereby allowing it to accumulate vidence bothfor and against a sense.The result of training a network until all examples axeclassified correctly is that infrequent okens can acquiredisproportionate importance.
For example, the context'Fine,' Henderson said, aimiably \[sic\].
'Can 7/ou get hinton the liner' clearly uses line in the phone sense.
How-ever, the only non-stopwords that are infrequent in othersenses are 'henderson' and 'aimiably'; and, due to itsmisspelling, the latter is conflated to 'aim'.
The net-work must raise the weight of 'henderson' so that it issufficient o give phone the largest output.
As a result,'henderson' appears in Table 1, in spite of its infrequencyin the training corpus.To determine a good topology for the network, variousnetwork topologies were explored: networks with from0 to 100 hidden units arranged in a single hidden layer;networks with multiple layers of hidden units; and net-works with a single layer of hidden units in which theoutput units were connected to both the hidden and in-put units.
In all cases, the network configuration withno hidden units was either superior or statistically in-distinguishable from the more complex networks.
As nonetwork topology was significantly better than one withno hidden units, all data reported here are derived fromsuch networks.5.
RESULTS AND DISCUSSIONAll of the classifiers performed best with the largest num-ber (200) of training contexts.
The percent correct re-sults reported below are averaged over the three trialswith 200 training contexts.
The Bayesian classifier av-eraged 71~ correct answers, the content vector classifieraveraged 72%, and the neural network classifier averaged76%.
None of these differences are statlstlcally signifi-cant due to the limited sample size of three trials.The results reported below are taken from trial A with200 training contexts.
Confusion matrices of this trialare given in Tables 2 - 4.
4 The diagonals how the num-ber of correct classifications for each sense, and the off-diagonal elements how classification errors.
For exam-ple, the entry containing 5 in the bottom row of Table 2means that 5 contexts whose correct sense is the productsense were classified as the phone sense.Ten heavily weighted tokens for each sense for each clas-sifter appear in Table 1.
The words on the list seem,for the most part, indicative of the target sense.
How-ever, there are some consistent differences among themethods.
For example, whereas the Bayesian method issensitive to proper nouns, the neural network appears tohave no such preference.To test the hypothesis that the methods have differentresponse patterns, we performed the X 2 test for corre-lated proportions.
This test measures how consistentlythe methods treat individual test contexts by determin-ing whether the classifiers are making the same classifica-tion errors in each of the senses.
For each sense, the testcompares the off-diagonal elements of a matrix whosecolumns contain the responses of one classifier and therows show a second classifier's responses in the same testset.
This process constructs a square matrix whose di-agonal elements contain the number of test contexts onwhich the two methods agree.The results of the X ~ test for a three-sense resolutiontask (product,/orraation a d tezt), s indicate that the re-sponse pattern of the content vector classifier is verysignificantly different from the patterns of both theBayesian and neural network classifiers, but the Bayesianresponse pattern is significantly different from the neuralnetwork pattern for only the product sense.
In the six-sense disambiguation task, the X 2 results indicate thatthe Bayesian and neural network classifiers' responsepatterns are not significantly different for any sense.
Theneural network and Bayesian classifiers' response pat-terns are significantly different from the content vectorclassifier only in the formation and tezt senses.
There-fore, with the addition of three senses, the classifiers'response patterns appear to be converging.The pilot two-sense distinction task (between productand formation) yielded over 90% correct answers.
In thethree-sense distinction task, the three classifiers had a4The numbers in the con/union matr ix  in Table 4 are averagesover ten rune with randomly initialiBed networks.STraining and test sets for these senses are identical to those inthe slx-sense resolution task.263Correct SenseClassifiedSenseProductFormationTextCordDivisionPhoneProduct Formation Text Cord Division Phone120 7 4 2 4 59 97 19 6 14 115 26 93 6 20 112 10 11 129 5 108 8 21 5 103 35 1 1 1 3 109Table 2: Confusion matr ix for Bayesian classifier (columns how the correct sense, rows the selected sense).Correct SenseClassifiedSenseProductFormationTextCordDivisionPhoneProduct Formation Text Cord Division Phone139 33 32 5 17 142 88 "15 12 8 53 7 71 3 8 60 7 7 120 2 50 9 12 4 108 05 5 12 5 6 119Table 3: Confusion matr ix for content vector classifier (columns how the correct sense, rows the selected sense).Correct SenseClassifiedSenseProductFormationTextCordDivisionPhoneProduct Formation Text Cord Division Phone122 11 4 1 3 64 90 17 9 8 29 14 83 4 10 72 11 13 125 3 34 13 16 4 121 18 10 16 6 4 130Table 4: Confusion matr ix for neural network classifier (columns how the correct sense, rows the selected sense)imean of 76% correct, 6 yielding a sharp degradation withthe addition of a third sense.
Therefore, we hypothesizeddegree of polysemy to be a major factor for performance.We were surprised to find that in the six-sense task, allthree classifiers degraded only slightly from the three-sense task, with a mean of 73% correct.
Although theaddition of three new senses to the task caused consistentdegradation, the degradation is relatively slight.
Hence,we conclude that some senses are harder to resolve thanothers, and it appears that overall accuracy is a functionof the difficulty of the sense rather than being strictly afunction of the number of senses.
The hardest sense tolearn, for all three classifiers, was tezt, followed by for-mation~ To test the validity of this conclusion, furthertests need to be run.SThe Bayesian classifier averaged 76~ correct answers, the con-tent vector classifier averaged 73%, and the neural networks 79%.I f  statistical classifiers are to be part of higher-levelNLP tasks, characteristics other than overall accuracyare important.
Collecting training contexts is by far themost time-consuming part of the entire process.
Untiltraining-context acquisition is fully automated, classi-fiers requiring smaller training sets are preferred.
Figure1 shows that the content vector classifier has a flatterlearning curve between 50 and 200 training contexts thanthe neural network and Bayesian classifiers, suggestingthat the latter two require more (or larger) training con-texts.
Ease and efficiency of use is also a factor.
Thethree classifiers are roughly comparable in this regard,although the neural network classifier is the most expen-sive to train.26480-070.cP.CL6O: : Neural networks--c = Content vectors: Bayesian classifier5'0 160 150 260Number of Training Examples in Each CategoryFigure 1: Learning curves.6.
CONCLUSIONThe convergence of the response patterns for the threemethods suggests that each of the classifiers is extractingas much data as is available in word counts from trainingcontexts.
If this is the case, any technique that uses onlyword counts will not be significantly more accurate thanthe techniques tested here.Although the degree of polysemy does affect the diffi-culty of the sense resolution task, a greater factor of per-formance is the difficulty of resolving individual senses.Using hindsight, it is obvious that the tezt sense is hardfor these statistical methods to learn because one cantalk or write about anything.
In effect, all words be-tween a pair of quotation marks are noise (unless line iswithin the quotes).
In the three-sense task, the Bayesianclassifier did best on the tezt sense, perhaps because ithad open and closed quotes as important okens.
Thisadvantage was lost in the six-sense task because quo-tation marks also appear in the contexts of the phonesense.
It is not immediately obvious why the formationsense should be hard.
From inspection of the contexts, itappears that the crucial information is close to the word,and context hat is more than a few words away is noise.These corpus-based statistical techniques use an impov-erished representation of the training contexts: simplecounts of tokens appearing within two sentences.
Webelieve significant increases in resolution accuracy willnot be possible unless other information, such as wordorder or syntactic information, is incorporated into thetechniques.ment, and Slavs Katz of IBM's T.J. Watson ResearchCenter for generously supplying line contexts from theAPHB corpus.
We are indebted to George A. Miller forsuggesting this line of research.Re ferences1.
Miller, G. A.
(ed.
), WordNet: An on-line lexicaldatabase.
International Journal of Lexicography (specialissue), 3(4):235-312, 1990.2.
Zipf, G. K., The meaning-fxequency relationship ofwords.
Journal of General Psychology, 3:251-256, 1945.3.
Yarowsky, D., Word-sense disambiguation using statisti-cal models of Roget's categories trained on large corpora,COLING-9~, 1992.4.
Voorhees, E. M., Leacock C., and Towell, G., Learningcontext o disambiguate word senses.
Proceedings ofthe3rd Computational Learning Theory and Natural Learn-ing Systems Conference, 199~, MIT Press (to appear).Also available as a Siemens technical report.5.
Gale, W., Church, K. W., and Yarowsky, D., A methodfor disambiguating word senses in a large corpus.
Sta-tistical Research Report 104, AT&T Bell Laboratories,1992.6.
Mosteller F. and Wallace, D., Inference and DisputedAuthorship: The Federalist.
Addison-Wessley, Reading,MA, 1964.7.
Choueka Y. and Lusignan, S., Disambiguation by shortcontexts.
Computers and the Humanities, 19:147-157,1985.8.
Salton, G., Wong, A., and Yang, C. S., A vector spacemodel for automatic indexing.
Communications of theACM, 18(11):613-620, 1975.9.
Rumelhart, D. E., Hinton, G. E., and Williams, R J.,Learning internal representations by error propagation.in Rumelhart, D. E. and McCleUand, I. L.
(eds.
), Par-allel Distributed Processing: Ezplorations in the Mi-crostructure of Cognition, Volume 1: Foundations.
MITPress, Cambridge, MA, 1986, pp.
318-363.ACKNOWLEDGMENTSThis work was supported in part by Grant No.
N00014-91-1634 from the Defense Advanced Research ProjectsAgency, Information and Technology Office, by the Of-fice of Naval Research, and by the James S. McDonnellFoundation.
We thank Kenneth Church of AT&T BellLaboratories for running the Bayesian classifier experi-265
