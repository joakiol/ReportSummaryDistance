Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 199?202,Prague, June 2007. c?2007 Association for Computational LinguisticsJHU1 : An Unsupervised Approach to Person Name Disambiguationusing Web SnippetsDelip Rao Nikesh Garera David YarowskyDept.
of Computer ScienceJohns Hopkins UniversityBaltimore, MD 21218{delip, ngarera, yarowsky}@cs.jhu.eduAbstractThis paper presents an approach to personname disambiguation using K-means clus-tering on rich-feature-enhanced documentvectors, augmented with additional web-extracted snippets surrounding the polyse-mous names to facilitate term bridging.
Thisyields a significant F-measure improvementon the shared task training data set.
The pa-per also illustrates the significant divergencebetween the properties of the training andtest data in this shared task, substantiallyskewing results.
Our system optimized onF0.2 rather than F0.5 would have achievedtop performance in the shared task.1 IntroductionBeing able to automatically distinguish betweenJohn Doe, the musician, and John Doe, the actor, onthe Web is a task of significant importance with ap-plications in IR and other information managementtasks.
Mann and Yarowsky (2004) used bigograph-ical data annotated with named entitities and per-form fusion of extracted information across multipledocuments.
Bekkerman and McCallum (2005) stud-ied the problem in a social network setting exploit-ing link topology to disambiguate namesakes.
Al-Kamha and Embley (2004) used a combination ofattributes (like zipcodes, state, etc.
), links, and pagesimilarity to derive the name clusters while Wan et.al.
(2005) used lexical features and named entities.2 ApproachesOur framework focuses on the K-means clusteringmodel using both bag of words as features and vari-ous augumented feature sets.
We experimented withseveral similarity functions and chose Pearson?s cor-relation coefficient1 as the distance measure for clus-tering.
The weights for the features were set to theterm frequency of their respective words in the doc-ument.22.1 Submitted system: Clustering using WebSnippetsWe queried the Google search engine with thetarget person names and extracted up to the topone thousand results.
For each result we alsoextracted the snippet associated with it.
An exampleis shown below in Figure 2.1.
As can be seen theFigure 1: Google snippet for ?Dekang Lin?snippets contain high quality, low noise features thatcould be used to improve the performance of thesystem.
Each snippet was treated as a document and1This performs better than the standard measures like Eu-clidean and Cosine with K-means clustering on this data.2We found that using TF weights instead of TF-IDF weightsgives a better performance on this task.199clustered along with the supplied documents.
Thisprocess is illustrated in Figure 2.
The followingexample illustrates how these web snippets canimprove performance by lexical transitivity.
Inthis hypothetical example, a short test documentcontains a Canadian postal code (T6G 2H1) notfound in any of the training documents.
However,there may exist an additional web page not in thetraining or test data which contains both this termand also overlap with other terms in the training data(e.g.
492-9920), serving as an effective transitivebridge between the two.Training Document 1 492-9920, not(T6G 2H1)Web Snippet 2 both 492-9920, T6G 2H1Test Document 3 T6G 2H1, not(492-9920)Thus K-means clustering is likely to cluster thethree documents above together while without thistransitive bridge the association between trainingand test documents is much less strong.
The finalclustering of the test data is simply a projection withthe training documents and web snippets removed.Projection of test documentsInitial clusters of web snippets + test documentsWeb snippet documentTest documentFigure 2: Clustering using Web Snippets2.2 BaselinesIn this section we describe several trivial baselines:1.
Singletons: A clustering where each clusterhas only one document hence number of clus-ters is same as the number of documents.2.
One Cluster: A clustering with only one clus-ter containing all documents.3.
Random: A clustering scheme which parti-tions the documents uniformly at random intoK clusters, where the value of K were the op-timal K on the training and test data.These results are summarized in Table 1.
Note thatall average F-scores mentioned in this table and therest of the paper are microaverages obtained by av-eraging the purity and invese purity over all namesand then calculating the F-score.Train TestBaseline F0.2 F0.5 F0.2 F0.5Singletons .676 .511 .843 .730One Cluster .688 .638 .378 .327Random .556 .493 .801 .668Table 1: Baseline performance2.3 K-means on Bag of Words modelThe standard unaugumented Bag of Words modelachieves F0.5 of 0.666 on training data, as shownin Table 2.2.4 Part of speech tag featuresWe then consider only terms that are nouns (NN,NNP) and adjectives (JJ) with the intuition thatmost of the content bearing words and descriptivewords that disambiguate a person would fall in theseclasses.
The result then improves to 0.67 on thetraining data.2.5 Rich featuresAnother variant of this system, that we call Rich-Feats, gives preferential weighting to terms that areimmediately around all variants of the person namein question, place names, occupation names, andtitles.
For marking up place names, occupationnames, and titles we used gazetteer3 lookup with-out explicit named entity disambiguation.
The key-words that appeared in the HTML tag <META ..>were also given higher weights.
This resulted in anF0.5 of 0.664.2.6 Snippets from the WebThe addition of web snippets as described in Sec-tion 2.1 yeilds a significant F0.5 improvement to0.72.3Totalling 19646 terms, gathered from publicly available re-sources on the web.
Further details are available on request.2002.7 Snippets and Rich featuresThis is a combination of the models mentioned inSections 2.5 and 2.6.
This model combination re-sulted in a slight degradation of performance oversnippets by themselves on the training data but aslight improvement on test data.Model K F0.2 F0.5Vanilla BOW 10% 0.702 0.666BOW + PoS 10% 0.706 0.670BOW + RichFeats 10% 0.700 0.664Snippets 10 0.721 0.718Snippets + RichFeats 10 0.714 0.712Table 2: Performance on Training Data3 Selection of ParametersThe main parameter for K-means clustering ischoosing the number of clusters, K. We optimizedK over the training data varying K from 10%,20%,?
?
?,100% of the number of documents as wellas varying absolute K values from 10, 20, ?
?
?
to 100documents.4 The evaluation score of F-measure canbe highly sensitive to this parameter K, as shownin Table 3.
The value of K that gives the best F-measure on training set using vanilla bag of words(BOW) model is K = 10%, however we see in Ta-ble 3 that this value of K actually performs muchworse on the test data as compared to other K val-ues.4 Training/Test discrepancy andre-evaluation using cross validation ontest dataTable 4 compares cluster statistics between the train-ing and test data.
This data was derived from Artileset.
al (2007).
The large difference between aver-age number of clusters in training and test sets in-dicates that the parameter K, optimized on trainingset cannot be transferred to test set as these two setsbelong to a very different distribution.
This can beemprically seen in Table 3 where applying the bestK on training results in a significant performance4We discard the training and test documents that have no textcontent, thus the absolute value K = 10 and percentage value K= 10% can result in different K?s, even if name had originally100 documents to begin with.drop on test set given this divergence when param-eters are optimized for F0.5 (although performancedoes transfer well when parameters are optimized onF0.2).
This was observed in our primary evaluationsystem which was optimized for F0.5 and resulted ina low official score of F0.5 = .53 and F0.2 = .65.Train TestK F0.2 F0.5 F0.2 F0.510% .702 .666 .527 .60020% .716 .644 .617 .63030% .724 .631 .683 .67640% .724 .618 .728 .70550% .732 .614 .762 .72460% .731 .601 .798 .74770% .730 .593 .832 .76680% .732 .586 .855 .77390% .714 .558 .861 .764100% .670 .502 .843 .730Table 3: Selecting the optimal parameter on trainingdata and application to test dataThus an interesting question is to measure per-formance when parameters are chosen on data shar-ing the distributional character of the test data ratherthan the highly divergent training set.
To do this, weused a standard 2-fold cross validation to estimateclustering parameters from a held-out, alternate-halfportion of the test data5, which more fairly repre-sents the character of the other half of the test datathan does the very different training data.
We di-vide the test set into two equal halves (taking firstfifteen names alphabetically in one set and the restin another).
We optimize K on the first half, teston the other half and vice versa.
We report the twoK-values and their corresponding F-measures in Ta-ble 5 and we also report the average in order to com-pare it with the results on the test set obtained usingK optimized on training.
Further, we also reportwhat would be oracle best K, that is, if we optimizeK on the entire test data 6.
We can see in Table 5that how optimizing K on a devlopment set with5This also prevents overfitting as the two halves for trainingand testing are disjoint.6By oracle best K we mean the K obtained by optimizingover the entire test data.
Note that, the oracle best K is justfor comparison because it would be unfair to claim results byoptimizing K on the entire test set, all our claimed results fordifferent models are based on 2-fold cross validation.201same distribution as test set can give us F-measurein the range of 77%, a significant increase as com-pared to the F-measure obtained by optimizing K ongiven training data.
Further, Table 5, also indicatesresults by a custom clustering method, that takes thebest K-means clustering using vanilla bag of wordsmodel, retains the largest cluster and splits all theother clusters into singleton clusters.
This methodgives an improved 2-fold F-measure score over thesimple bag of words model, implying that most ofthe namesakes in test data have one (or few) domi-nant cluster and a lot of singleton clusters.
Table 6shows a full enumeration of model variance underthis cross validated test evaluation.
POS and Rich-Feats yield small gains, and a best F0.5 performanceof .776.Data set cluster size # of clustersMean Variance Mean VarianceTrain 5.4 144.0 10.8 146.3Test 3.1 26.5 45.9 574.1Table 4: Cluster statistics from the test and trainingdataData set K F0.2 F0.5F0.5 Best K on train 10% .702 .666F0.2 Best K on train 10 .707 .663Best K on train 10% .527 .560applied to test 10 .540 .5712Fold on Test 80 .847 .74880% .862 .793.854* .771*2Fold on Single 80 .847 .749Largest Cluster 80 .866 .795.856* .772*Oracle on Test 80 .858 .774Table 5: Comparision of training and test results us-ing Vanilla Bag-of-words model.
The values indi-cated with * represent the average value.5 ConclusionWe presented a K-means clustering approach for thetask of person name disambiguation using severalaugmented feature sets including HTML meta fea-tures, part-of-speech-filtered features, and inclusionof additional web snippets extracted from Googleto facilitate term bridging.
The latter showed sig-nificant empirical gains on the training data.
BestModel K F0.2 F0.5Vanilla BOW 80/ .847/.862 .749/.79380% Avg = .854 Avg = .771BOW + PoS 80%/ .844/.865 .749/.79580% Avg = .854 Avg = .772BOW 80%/ .847/.868 .754/.798RichFeats 80% Avg = .858 Avg = .776Snippets 50%/ .842/.875 .746/.80050% Avg = .859 Avg = .773Snippets + 40%/ .836/.874 .750/.798RichFeats 50% Avg = .855 Avg = .774Table 6: Performance on 2Fold Test Dataperformance on test data, when parameters are op-timized for F0.2 on training (Table 3), yielded a topperforming F0.2 of .855 on test data (and F0.5=.773on test data).
We also explored the striking discrep-ancy between training and test data characteristicsand showed how optimizing the clustering param-eters on given training data does not transfer wellto the divergent test data.
To control for similartraining and test distributional characteristics, we re-evaluated our test results estimating clustering pa-rameters from alternate held-out portions of the testset.
Our models achieved cross validated F0.5 of .77-.78 on test data for all feature combinations, furthershowing the broad strong performance of these tech-niques.ReferencesReema Al-Kamha and David W. Embley.
2004.
Groupingsearch-engine returned citations for person-name queries.
InProceedings of the 6th annual ACM international workshopon Web information and data management, pages 96?103.Javier Artiles, Julio Gonzalo, and Felisa Verdejo.
2007.
Eval-uation: Establishing a benchmark for the web people searchtask.
In Proceedings of Semeval 2007, Association for Com-putational Linguistics.Ron Bekkerman and Andrew McCallum.
2005.
Disambiguat-ing web appearances of people in a social network.
In Pro-ceedings of the 14th international conference on World WideWeb, pages 463?470.Gideon S. Mann and David Yarowsky.
2004.
Unsupervisedpersonal name disambiguation.
In Proceedings of the sev-enth conference on Natural language learning (CONLL),pages 33?40.Xiaojun Wan, Jianfeng Gao, Mu Li, and Binggong Ding.
2005.Person resolution in person search results: Webhawk.
InProceedings of the 14th ACM international conference onInformation and knowledge management, pages 163?170.202
