Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36?44,Boulder, Colorado, June 2009. c?2009 Association for Computational LinguisticsEvaluating Automation Strategies in Language DocumentationAlexis Palmer, Taesun Moon, and Jason BaldridgeDepartment of LinguisticsThe University of Texas at AustinAustin, TX 78712{alexispalmer,tsmoon,jbaldrid}@mail.utexas.eduAbstractThis paper presents pilot work integrating ma-chine labeling and active learning with humanannotation of data for the language documen-tation task of creating interlinearized glosstext (IGT) for the Mayan language Uspanteko.The practical goal is to produce a totally an-notated corpus that is as accurate as possiblegiven limited time for manual annotation.
Wedescribe ongoing pilot studies which examinethe influence of three main factors on reduc-ing the time spent to annotate IGT: sugges-tions from a machine labeler, sample selectionmethods, and annotator expertise.1 IntroductionLanguages are dying at the rate of two each month.By the end of this century, half of the approxi-mately 6000 extant spoken languages will cease tobe transmitted effectively from one generation ofspeakers to the next (Crystal, 2000).
Under thisimmense time pressure, documentary linguists seekto preserve a record of endangered languages whilethere are still communities of speakers to work with.Many language documentation projects target lan-guages about which our general linguistic knowl-edge is nonexistent or much less than for morewidely-spoken languages.
The vast majority of theseare individual or small-group endeavors on smallbudgets with little or no institutional guidance bythe greater documentary linguistic community.
Thefocus in such projects is often first on collection ofdata (documentation), with a following stage of lin-guistic analysis and description.
A key part of theanalysis process, detailed linguistic annotation of therecorded texts, is a time-consuming and tedious taskusually occurring late in the project, if it occurs atall.Text annotation typically involves producing in-terlinearized glossed text (IGT), labeling for mor-phology, parts-of-speech, etc., which greatly facil-itates further exploration and analysis of the lan-guage.
The following is IGT for the phrase xelchli from the Mayan language Uspanteko:1(1) x-COM-elsalir-ch-DIRliDEMSpanish: ?Salio entonces.?
English:?Then he left.
?The levels of analysis include morpheme segmenta-tion, transliteration of stems, and labeling of stemsand morphemes with tags, some corresponding toparts-of-speech and others to semantic distinctions.There is no single standard format for IGT.
TheIGT systems developed by documentation projectstend to be idiosyncratic: they may be linguisticallywell-motivated and intuitive, but they are unlikely tobe compatible or interchangeable with systems de-veloped by other projects.
They may lack internalconsistency as well.
Nonetheless, IGT in a read-ily accessible format is an important resource thatcan be used fruitfully by linguists to examine hy-potheses on novel data (e.g.
Xia and Lewis (2007;2008), Lewis and Xia (2008)).
Furthermore, it canbe used by educators and language activists to createcurriculum material for mother language educationand promote the survival of the language.Despite the urgent need for such resources, IGTannotations are time consuming to create entirely byhand, and both human and financial resources areextremely limited in this domain.
Thus, language1KEY: COM=completive aspect, DEM=demonstrative,DIR=directional36documentation presents an interesting test case andan ideal context for use of machine labeling and ac-tive learning.
This paper describes a series of ex-periments designed to assess this promise in a re-alistic documentation context: creation of IGT forthe Mayan language Uspanteko.
We systematicallycompare varying degrees of machine involvement inthe development of IGT, from minimally involvedsituations where examples for tagging are selectedsequentially to active learning situations where themachine learner selects samples for human taggingand suggests labels.
We also discuss the challengesfaced by linguists in having to learn, transcribe, ana-lyze, and annotate a language almost simultaneouslyand discuss whether machine involvement reducesor compounds those challenges.In the experiments, two documentary linguists an-notate IGT for Uspanteko texts using different lev-els of support from a machine learned classifier.
Weconsider the interaction of three main conditions: (1)sequential, random, or uncertainty sampling for re-questing labels from an annotator, (2) suggestions orno suggestions from a machine labeler, and (3) ex-pert versus non-expert annotator.
All annotator deci-sions are timed, enabling the actual time cost of an-notation to be measured within the context of eachcondition.
This paper describes the Uspanteko dataset we adapted for the experiments, expands on thechoices described above, and reports on preliminaryresults from our ongoing annotation experiments.2 Data: Uspanteko IGTThis section describes the Uspanteko corpus usedfor the experiments, our clean-up of the corpus, andthe specific task?labeling part-of-speech and glosstags?addressed by the experiments.2.1 OKMA Uspanteko corpusOur primary dataset is a corpus of texts (Pixabaj etal., 2007) in the Mayan language Uspanteko thatwere collected, transcribed, translated (into Span-ish) and annotated as part of the OKMA languagedocumentation project.2 Uspanteko, a member ofthe K?ichee?
branch of the Mayan language family,is spoken by approximately 1320 people in centralGuatemala (Richards, 2003).2http://www.okma.orgThe corpus contains 67 texts, 32 of them glossed.Four textual genres are represented in the glossedportion of the corpus: oral histories (five texts) usu-ally have to do with the history of the village and thecommunity, personal experience texts (five texts) re-count events from the lives of individual people inthe community, and stories (twenty texts) are pri-marily folk stories and children?s stories.
The corpusalso contains one recipe and one advice text in whicha speaker discusses what the community should bedoing to better preserve and protect the environment.The transcriptions are based on spoken data, withattendant dysfluencies, repetitions, false starts, andincomplete sentences.
Of the 284,455 words, 74,298are segmented and glossed.
This is a small datasetby computational linguistics standards but ratherlarge for a documentation project.2.2 Interlinearized Glossed TextOnce recordings have been made, the next tasks aretypically to produce translations and transcription ofthe audio.
Transcription is a complex and difficultprocess, often involving the development of an or-thography for the language in parallel.
The productof the transcription is raw text like the Uspantekosample shown below (text 068, clauses 283-287):Non li in yolow rk?il kita?tinch?ab?ex laj inyolj iin, si no kelaj yolj jqaaj tinch?ab?ej i non qe lixk?am rib?
chuwe, non qe li lajorinon li iin yolow rk?ilaq.3Working with the transcription, the translation, andany previously-attained knowledge about the lan-guage, the linguist next makes decisions about thedivision of words into morphemes and the contribu-tions made by individual morphemes to the meaningof the word or of the sentence.
IGT efficiently bringstogether and presents all of this information.In the traditional four-line IGT format, mor-phemes appear on one line and glosses for thosemorphemes on the next.
The gloss line includes bothlabels for grammatical morphemes (e.g.
PL or COM)and translations of stems (e.g.
salir or ropa).
Seethe following example from Uspanteko:43Spanish: Solo asi yo aprendi con e?l.
No le hable en elidioma mio.
Si no que en el idioma su papa?
le hablo.
Y solo asime fui acostumbrando.
Solo asi ahora yo platico con ellos.4KEY: E1S=singular first person ergative, INC=incompletive,PART=particle, PREP=preposition, PRON=pronoun, NEG=negation,37(2) Kita?
tinch?ab?ej laj inyolj iin(3) kita?NEGPARTt-in-ch?abe-jINC-E1S-hablar-SCTAM-PERS-VT-SUFlajPREPPREPin-yoljE1S-idiomaPERS-SiinyoPRON?No le hablo en mi idioma.?
(?I don?t speak to him in my language.?
)Most commonly, IGT is presented in a four-tierformat.
The first tier (2) is the raw, unannotatedtext.
The second (first line of (3)) is the same textwith each word morphologically segmented.
Thethird tier (second line of (3)), the gloss line, is acombination of Spanish translations of the Uspan-teko stems and gloss tags representing the grammat-ical information encoded by affixes and stand-alonemorphemes.
The fourth tier (fourth line of (3)) is atranslation in the target language of documentation.Some interlinear texts include other project-defined tiers.
OKMA uses a fifth tier (third line of(3)), described as the word-class line.
This line isa mix of traditional POS tags, positional labels (e.g.suffix, prefix), and broader linguistic categories likeTAM for tense-aspect-mood.2.3 Cleaning up the OKMA annotationsThe OKMA annotations were created using Shoe-box,5 a standard tool used by documentary linguistsfor lexicon management and IGT creation.
To de-velop a corpus suitable for these studies, it was nec-essary to put considerable effort into normalizingthe original OKMA source annotations.
Varied lev-els of linguistic training of the original annotatorsled to many inconsistencies in the original annota-tions.
Also, Shoebox (first developed in 1987) usesa custom, pre-XML whitespace delimited data for-mat, making normalization especially challenging.Finally, not all of the texts are fully annotated.
Al-most half of the 67 texts are just transcriptions, sev-eral texts are translated but not further analyzed, andseveral others are only partially annotated at textlevel, clause level, word level, or morpheme level.
Itwas thus necessary to identify complete texts for usein our experiments.
Some missing labels in nearly-complete texts were filled in by the expert annotator.A challenge for representing IGT in a machine-readable format is maintaining the links betweenS=sustantivo (noun), SC=category suffix, SUF=suffix,TAM=tense/aspect/mood, VT=transitive verb5http://www.sil.org/computing/shoebox/the source text morphemes in the second tier andthe morpheme-by-morpheme glosses in the thirdtier.
The standard Shoebox output format, for ex-ample, enforces these links through management ofthe number of spaces between items in the output.To address this, we converted the cleaned annota-tions into IGT-XML (Palmer and Erk, 2007) withhelp from the Shoebox/Toolbox interfaces providedin the Natural Language Toolkit (Robinson et al,2007).
Automating the transformation from Shoe-box format to IGT-XML?s hierarchical format re-quired cleaning up tier-to-tier alignment and check-ing segmentation in some cases where morphemesand glosses were misaligned, as in (5) below.6(4) Non li in yolow rk?il(5) NonDEMDEMliDEMDEMinyoPRONyolowplaticarVIr-k?ilAPSUFE3s.-SRPERS SREL?Solo asi yo aprendi con e?l.
?Here, the number of elements in the morpheme tier(first line of (5)) does not match the number of el-ements in the gloss tier (second line of (5)).
Theproblem is a misanalysis of yolow: it should besegmented yol-ow with the gloss platicar-AP.Automating this transformation has the advantage ofidentifying such inconsistencies and errors.There also were many low-level issues that hadto be handled, such as checking and enforcing con-sistency of tags.
For example, the tag E3s.
in thegloss tier of (5) is a typo; the correct tag is E3S.
Theannotation tool used in these studies does not allowsuch inconsistencies to occur.2.4 Target labelsThere are two main tasks in producing IGT: wordsegmentation (determination of stems and affixes)and glossing each segment.
Stems and affixes eachget a different type of gloss: the gloss of a stem istypically its translation whereas the gloss of an affixis a label indicating its grammatical role.
The addi-tional word-class line provides part-of-speech infor-mation for the stems, such as VT for salir.Complete prediction of segmentation, gloss trans-lations and labels is our ultimate goal for aiding IGT6KEY: AP=antipassive, DEM=demonstrative, E3S=singular thirdperson ergative, PERS=person marking, SR/SREL=relational noun,VI=intransitive verb38creation with automation.
Here, we study the poten-tial for improving annotation efficiency for the morelimited task of predicting the gloss label for each af-fix and the part-of-speech label for each stem.
Thus,the experiments aim to produce a single label foreach morpheme.
We assume that words have beenpre-segmented and we ignore the gloss translations.The target representation in these studies is an ad-ditional tier which combines gloss labels for affixesand stand-alone morphemes with part-of-speech la-bels for stems.
Example (6) repeats the clause in (4),adding this new combined tier.
Stem labels are givenin bold text, and affix labels in plain text.
(6) Non li in yolow rk?il(7) NonDEMliDEMinPRONyol-owVI-APr-k?ilE3S-SR?Solo asi yo aprendi con e?l.
?A simple procedure was used to create the new tier.For each morpheme, if a gloss label (such as DEMor E3S) appears on the gloss line (second line of(3)), we select that label.
If what appears is a stemtranslation, we instead select the part-of-speech la-bel from the next tier down (third line of (3)).In the entire corpus, sixty-nine different labelsappear in this combined tier.
The following tableshows the five most common part-of-speech labels(left) and the five most common gloss labels (right).The most common label, S, accounts for 11.3% ofthe tokens in the corpus.S noun 7167 E3S sg.3p.
ergative 3433ADV adverb 6646 INC incompletive 2835VT trans.
verb 5122 COM completive 2586VI intrans.
verb 3638 PL plural 1905PART particle 3443 SREL relational noun 18813 Integrated annotation and automationThe experimental framework described in this sec-tion is designed to model and evaluate real-time inte-gration of human annotation, active learning strate-gies, and output from machine-learned classifiers.The task is annotation of morpheme-segmented textsfrom a language documentation project (sec.
2).3.1 Tools and resourcesIntegrating automated support and human annota-tion in this context requires careful coordination ofthree components: 1) presenting examples to the an-notator and storing the annotations, 2) training andevaluation of tagging models using data labeled bythe annotator, and 3) selecting new examples for an-notation.
The processes are managed and coordi-nated using the OpenNLP IGT Editor.7 The anno-tation component of the tool, and in particular theuser interface, is built on the Interlinear Text Editor(Lowe et al, 2004).For tagging we use a strong but simple standardclassifier.
There certainly are many other modelingstrategies that could be used, for example a condi-tional random field (as in Settles and Craven (2008)),or a model that deals differently with POS labels andmorpheme gloss labels.
Nonetheless, a documen-tary linguistics project would be most likely to use astraightforward, off-the-shelf labeler, and our focusis on exploring different annotation approaches in arealistic documentation setting rather than buildingan optimal classifier.
To that end, we use a standardmaximum entropy classifier which predicts the labelfor a morpheme based on the morpheme itself plusa window of two morphemes before and after.
Stan-dard features used in part-of-speech taggers are ex-tracted from the morpheme to help with predictinglabels for previously unseen stems and morphemes.3.2 Annotators and annotation proceduresA practical goal of these studies is to explore bestpractices for using automated support to create fully-annotated texts of the highest quality possible withinfixed resource limits.
For producing IGT, one of themost valuable resources is the time of a linguist withlanguage-specific expertise.
Documentary projectsmay also (or instead) have access to a trained lin-guist without prior experience in the language.
Wecompare results from two annotators with differentlevels of exposure to the language.
Both are trainedlinguists who specialize in language documentationand have extensive field experience.8The first, henceforth referred to as the expertannotator, has worked extensively on Uspanteko,including writing a grammar of the language and7http://igt.sourceforge.net/8It should be noted that these are pilot studies.
With justtwo annotators, the annotation comparisons are suggestive butnot conclusive.
Even so, this scenario accurately reflects theresource limitations encountered in documentation projects.39contributing to the publication of an Uspanteko-Spanish dictionary (A?ngel Vicente Me?ndez, 2007).She is a native speaker of K?ichee?, a closely-relatedMayan language.
The second annotator, the non-expert annotator, is a doctoral student in languagedocumentation with no prior experience with Us-panteko and only limited previous knowledge ofMayan languages.
Throughout the annotation pro-cess, the non-expert annotator relied heavily on theUspanteko-Spanish dictionary.
Both annotators arefluent speakers of Spanish, the target translation andglossing language for the OKMA texts.In many annotation projects, labeling of trainingdata is done with reference to a detailed annotationmanual.
In the language documentation context, amore usual situation is for the annotator(s) to workfrom a set of agreed-upon conventions but withoutstrict annotation guidelines.
This is not because doc-umentary linguists lack motivation or discipline butsimply because many aspects of the language are un-known and the analysis is constantly changing.In the absence of explicit written annotationguidelines, we use an annotation training process forthe annotators to learn the OKMA annotation con-ventions.
Two seed sets of ten clauses each were se-lected to be used both for human annotation trainingand for initial classifier training.
The first ten clausesof the first text in the training data were used to seedmodel training for the sequential selection cases (see3.4).
The second set of ten were randomly selectedfrom the entire corpus and used to seed model train-ing for both random and uncertainty sampling.These twenty clauses were used to provide initialguidance to the annotators.
With the aid of a list ofpossible labels and the grammatical categories theycorrespond to, each annotator was asked to label theseed clauses, and these labels were compared to thegold standard labels.
Annotators were told whichlabels were correct and which were incorrect, andthe process was repeated until all morphemes werecorrectly labeled.
In some cases during this trainingphase, the correct label for a morpheme was sup-plied to the annotator after several incorrect guesses.3.3 Suggesting labelsWe consider two situations with respect to the con-tribution of the classifier: a suggest condition inwhich the labels predicted by the machine learnerare shown to the annotator as she begins labeling aselected clause, and a no-suggest condition in whichthe annotator does not see the predicted labels.In the suggest cases, the annotator is shown the la-bel assigned the greatest likelihood by the tagger aswell as a list of several highly-likely labels, rankedaccording to likelihood.
To be included on this list,a label must be assigned a probability greater thanhalf that of the most-likely label.
In the no-suggestcases, the annotator has access to a list of the la-bels previously seen in the training data for a givenmorpheme, ranked in order of frequency of occur-rence with the morpheme in question; this is similarto the input an annotator gets while glossing texts inShoebox/Toolbox.
Specifically, Shoebox/Toolboxpresents previously seen glosses and labels for agiven morpheme in alphabetic order.3.4 Sample selectionWe consider three methods of selecting examplesfor annotation?sequential (seq), random (rand), anduncertainty sampling (al)?and the performance ofeach method in both the suggest and the no-suggestsetups.
For uncertainty sampling, we measure un-certainty of a clause as the average entropy per mor-pheme (i.e., per labeling decision).3.5 Measuring annotation costNot all examples take the same amount of effort toannotate.
Even so, the bulk of the literature on activelearning assumes some sort of unit cost to determinethe effectiveness of different sample selection strate-gies.
Examples of unit cost measurements includethe number of documents in text classification, thenumber of sentences in part-of-speech tagging (Set-tles and Craven, 2008), or the number of constituentsin parsing (Hwa, 2000).
These measures are conve-nient for performing active learning simulations, butawareness has grown that they are not truly repre-sentative measures of the actual cost of annotation(Haertel et al, 2008a; Settles et al, 2008), with Ngaiand Yarowsky (2000) being an early exception to theunit-cost approach.
Also, Baldridge and Osborne(2004) use discriminants in parse selection, whichare annotation decisions that they later showed cor-relate with timing information (Baldridge and Os-borne, 2008).The cost of annotation ultimately comes down to40money.
Since annotator pay may be variable but will(under standard assumptions) be constant for a givenannotator, the best approximation of likely cost sav-ings is to measure the time taken to annotate underdifferent levels of automated support.
This is es-pecially important in sample selection and its inter-action with automated suggestions: active learningseeks to find more informative examples, and thesewill most likely involve more difficult decisions, de-creasing annotation quality and/or increasing anno-tation time (Hachey et al, 2005).
Thus, we measurecost in terms of the time taken by each annotator oneach example.
This allows us to measure the actualtime taken to produce a given labeled data set, andthus compare the effectiveness of different levels ofautomated support plus their interaction with anno-tators of different levels of expertise.Recent work shows that paying attention to pre-dicted annotation cost in sample selection itself canincrease the effectiveness of active learning (Settleset al, 2008; Haertel et al, 2008b).
Though we havenot explored cost-sensitive selection here, the sce-nario described here is an appropriate test ground forit: in fact, the results of our experiments, reported inthe next section, provide strong evidence for a realnatural language annotation task that active learningselection with cost-sensitivity is indeed sub-optimal.4 DiscussionThis section presents and discusses preliminary re-sults from the ongoing annotation experiments.
TheUspanteko corpus was split into training, develop-ment, and held-out test sets, roughly 50%, 25%,and 25%.
Specifically, the training set of 21 textscontains 38802 words, the development set of 5texts contains 16792 words, and the held-out testset, 6 texts, contains 18704 words.
These are smalldatasets, but the size is realistic for computationalwork on endangered languages.When measuring the performance of annotators,factors like fatigue, frustration, and especially theannotator?s learning process must be considered.Annotators improve as they see more examples (es-pecially the non-expert annotator).
To minimize theimpact of the annotator?s learning process on the re-sults, annotation is done in rounds.
Each round con-sists of ten clauses from each of the six experimental0 10 20 30 40 50010203040Number of Annotation RoundsSecondsper MorphemeNon?expertExpertFigure 1: Average annotation time (in seconds per mor-pheme) over annotation rounds, averaged over all six con-ditions for each annotator.cases for each annotator.
The newly-labeled clausesare then added to the labeled training data, and a newtagging model is trained on the updated training setand evaluated on the development set.
Both annota-tors have completed fifty-one rounds of annotationso far, labeling 510 clauses for each of the six ex-perimental conditions.
The average number of mor-phemes labeled is 3059 per case.
Because the anno-tation experiments are ongoing, we discuss results interms of the trends seen thus far.4.1 Annotator speedThe expert annotator showed a small increase inspeed after an initial familiarization period, and thenon-expert showed a dramatic increase.
Figure 1plots the number of seconds taken per morphemeover the course of annotation, averaged over all sixconditions for each annotator.
The slowest, fastest,and mean rates, in seconds per morpheme, for theexpert annotator were 12.60, 1.89, and 4.14, respec-tively.
For the non-expert, they were 59.71, 1.90,and 8.03.4.2 Accuracy of model on held-out dataTable 1 provides several measures of the currentstate of annotation in all 12 conditions after 51rounds of annotation.
The sixth column, labeled41Anno Suggest Select Time (sec) #Morphs Model Accuracy Total Accuracy of AnnotationNonExp N Seq 23739.79 3314 63.28 63.92NonExp N Rand 22721.11 2911 68.36 68.69NonExp N AL 23755.71 2911 68.26 67.84NonExp Y Seq 21514.05 2887 66.55 66.89NonExp Y Rand 22189.68 3002 68.41 68.73NonExp Y AL 25731.57 2750 67.63 67.30Exp N Seq 11862.39 3354 61.15 61.88Exp N Rand 11665.10 3043 64.60 64.91Exp N AL 13894.14 3379 66.74 66.47Exp Y Seq 11758.74 2892 61.12 61.48Exp Y Rand 11426.85 2979 60.13 60.57Exp Y AL 16253.40 3296 63.30 63.15Table 1: After 51 rounds of annotation: ModelAcc=accuracy on development set, TotalAnnoAcc=accuracy of fully-labeled corpusModelAcc, shows the accuracy of models on thedevelopment data.
This represents a unit cost as-sumption at the clause level: measured this way, theresults would suggest that the non-expert was bestserved by random selection, with no effect from ma-chine suggestions.
For the expert, they suggest ac-tive learning without suggestions is best, and thatsuggestions actually hurt effectiveness.4.3 Accuracy of fully-labeled corpusWe are particularly concerned with the question ofhow to develop a fully-labeled corpus with the high-est level of accuracy, given a finite set of resources.Thus, we combine the portion of the training set la-beled by the human annotator with the results of tag-ging the remainder of the training set with the modeltrained on those annotations.
The rightmost columnof Table 1, labeled Total Accuracy of Annotation,shows the accuracy of the fully labeled training set(part human, part machine labels) after 51 rounds.These accuracies parallel the model accuracies: ran-dom selection is best for the non-expert annotator,and uncertainty selection is best for the expert.Since this tagging task involves labeling mor-phemes, a clause cost assumption is not ideal?e.g.,active learning tends to select longer clauses andthereby obtains more labels.
To reflect this, a sub-clause cost can help: here we use the number ofmorphemes annotated.
The column labeled Tokensin Table 2 shows the total accuracy achieved in eachcondition when human annotation ceases at 2750morphemes.
The figure in parentheses is the cumu-lative annotation time at the morpheme cut-off point.Here, the non-expert does best: he took great carewith the annotations and was clearly not tempted toAnno Suggest Select Time Tokens (time)(11427 sec) (2750 morphs)NonExp N Seq 55.01 59.80 (21678 secs)NonExp N Rand 59.95 68.68 (22069 secs)NonExp N AL 59.86 67.70 (22879 secs)NonExp Y Seq 60.27 66.79 (21053 secs)NonExp Y Rand 62.96 68.38 (21194 secs)NonExp Y AL 59.18 67.30 (25732 secs)Exp N Seq 61.21 59.18 (10110 secs)Exp N Rand 64.92 64.42 (10683 secs)Exp N AL 65.72 65.74 (11826 secs)Exp Y Seq 61.47 61.47 (11436 secs)Exp Y Rand 60.57 61.16 (10934 secs)Exp Y AL 61.54 62.87 (13957 secs)Table 2: For given cost, accuracy of fully-labeled corpus.accept erroneous suggestions from the machine la-beler.
In contrast, the expert does seem to have ac-cepted many bad machine suggestions.Morpheme unit cost is more fine-grained thanclause-level cost, but it hides the fact that the ex-pert annotator needed far less time to produce a cor-pus of higher overall labeled quality than the non-expert.
This can be seen in the Time column ofTable 2, which gives the total annotation accuracywhen 11427 seconds are alloted for human label-ing.
The expert annotator achieved the highest accu-racy for total labeling of the training set using activelearning without machine label suggestions.
Activelearning helps the non-expert as well, but his bestcondition is random selection with machine labels.4.4 Annotator accuracy by roundActive learning clearly selects harder examples thathurt the non-expert?s performance.
To see thisclearly, we measured the accuracy of the annotators?labels for each round of each experimental setup,420 10 20 30 40 505060708090100Number of Annotation RoundsSingle Round AccuracySuggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential0 10 20 30 40 505060708090100Number of Annotation RoundsSingle Round AccuracySuggest + UncertaintySuggest + RandomSuggest + SequentialNo?suggest + UncertaintyNo?suggest + RandomNo?suggest + Sequential(a) (b)Figure 2: Single round accuracy per round for each experiment type by: (a) non-expert annotator, (b) expert annotatorgiven in Fig.
2.
It is not clear at this stage whetherthe tag suggestions by the machine labeler are help-ful to human annotation.
It is useful to compare thecases where the machine learner is not involved inexample selection (i.e.
random and sequential) touncertainty sampling, which does involve the ma-chine learner.
One thing that is apparent is that whenactive learning is used to select samples for annota-tion, both the expert and non-expert annotator havea harder time providing correct tags.
A point of con-trast between the expert and non-expert is that thenon-expert generally outperforms the expert on labelaccuracy in the non-active learning scenarios.
Thenon-expert was very careful with his labeling deci-sions, but also much slower than the expert.
In theend, speedier annotation rates allowed the expert an-notator to achieve higher accuracies in less time.5 ConclusionWe have described a set of ongoing pilot experi-ments designed to test the utility of machine label-ing and active learning in the context of documen-tary linguistics.
The production of IGT is a realisticannotation scenario which desperately needs label-ing efficiency improvements.
Our preliminary re-sults suggest that both machine labeling and activelearning can increase the effectiveness of annotators,but they interact quite strongly with the expertise ofthe annotators.
In particular, though active learn-ing works well with the expert annotator, for a non-expert annotator it seems that random selection isa better choice.
However, we stress that our anno-tation experiments are ongoing.
Active learning isoften less effective early in the learning curve, es-pecially when automated label suggestions are pro-vided, because the model is not yet accurate enoughto select truly useful examples, nor to suggest labelsfor them reliably (Baldridge and Osborne, 2004).Thus, we expect automation via uncertainty sam-pling and/or suggestion may gather momentum andoutpace random selection and/or no suggestions bywider margins as annotation continues.AcknowledgmentsThis work is funded by NSF grant BCS 06651988?Reducing Annotation Effort in the Documentationof Languages using Machine Learning and ActiveLearning.?
Thanks to Katrin Erk, Nora England,Michel Jacobson, and Tony Woodbury; and to anno-tators Telma Kaan Pixabaj and Eric Campbell.
Fi-nally, thanks to the anonymous reviewers for valu-able feedback.43ReferencesMiguel A?ngel Vicente Me?ndez.
2007.
Diccionario bil-ingu?e Uspanteko-Espan?ol.
Cholaj Tzijb?al li Uspan-teko.
Okma y Cholsamaj, Guatemala.Jason Baldridge and Miles Osborne.
2004.
Active learn-ing and the total cost of annotation.
In Proceedings ofEmpirical Approaches to Natural Language Process-ing (EMNLP).Jason Baldridge and Miles Osborne.
2008.
Active learn-ing and logarithmic opinion pools for HPSG parse se-lection.
Natural Language Engineering, 14(2):199?222.David Crystal.
2000.
Language Death.
Cambridge Uni-versity Press, Cambridge.Ben Hachey, Beatrice Alex, and Markus Becker.
2005.Investigating the effects of selective sampling on theannotation task.
In Proceedings of the 9th Conferenceon Computational Natural Language Learning, AnnArbor, MI.Robbie Haertel, Eric Ringger, Kevin Seppi, James Car-roll, and McClanahan Peter.
2008a.
Assessing thecosts of sampling methods in active learning for anno-tation.
In Proceedings of ACL-08: HLT, Short Papers,pages 65?68, Columbus, Ohio, June.
Association forComputational Linguistics.Robbie A. Haertel, Kevin D. Seppi, Eric K. Ringger, andJames L. Carroll.
2008b.
Return on investment foractive learning.
In Proceedings of the NIPS Workshopon Cost-Sensitive Learning.
ACL Press.Rebecca Hwa.
2000.
Sample selection for statisticalgrammar induction.
In Proceedings of the 2000 JointSIGDAT Conference on EMNLP and VLC, pages 45?52, Hong Kong, China, October.William Lewis and Fei Xia.
2008.
Automatically iden-tifying computationally relevant typological features.In Proceedings of IJCNLP-2008, Hyderabad, India.John Lowe, Michel Jacobson, and Boyd Michailovsky.2004.
Interlinear text editor demonstration and projetarchivage progress report.
In 4th EMELD workshopon Linguistic Databases and Best Practice, Detroit,MI.Grace Ngai and David Yarowsky.
2000.
Rule Writing orAnnotation: Cost-efficient Resource Usage for BaseNoun Phrase Chunking.
In Proceedings of the 38thAnnual Meeting of the Association for ComputationalLinguistics, pages 117?125, Hong Kong.Alexis Palmer and Katrin Erk.
2007.
IGT-XML: AnXML format for interlinearized glossed text.
In Pro-ceedings of the Linguistic Annotation Workshop (LAW-07), ACL07, Prague.Telma Can Pixabaj, Miguel Angel Vicente Me?ndez,Mar?
?a Vicente Me?ndez, and Oswaldo Ajcot Damia?n.2007.
Text collections in Four Mayan Languages.Archived in The Archive of the Indigenous Languagesof Latin America.Michael Richards.
2003.
Atlas lingu??
?stico de Guatemala.Servipresna, S.A., Guatemala.Stuart Robinson, Greg Aumann, and Steven Bird.
2007.Managing fieldwork data with Toolbox and the Natu-ral Language Toolki t. Language Documentation andConservation, 1:44?57.Burr Settles and Mark Craven.
2008.
An analysis ofactive learning strategies for sequence labeling tasks.In Proceedings of the 2008 Conference on Empiri-cal Methods in Natural Language Processing, pages1070?1079, Honolulu, Hawaii, October.
Associationfor Computational Linguistics.Burr Settles, Mark Craven, and Lewis Friedland.
2008.Active learning with real annotation costs.
In Proceed-ings of the NIPS Workshop on Cost-Sensitive Learn-ing, pages 1069?1078.
ACL Press.Fei Xia and William Lewis.
2007.
Multilingual struc-tural projection across interlinear text.
In Proceedingsof HLT/NAACL 2007, Rochester, NY.Fei Xia and William Lewis.
2008.
Repurposing theoreti-cal linguistic data for tool development antd search.
InProceedings of IJCNLP-2008, Hyderabad, India.44
