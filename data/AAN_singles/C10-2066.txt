Coling 2010: Poster Volume, pages 579?587,Beijing, August 2010Automatic Allocation of Training Data for Rapid Prototypingof Speech Understanding based on Multiple Model CombinationKazunori Komatani?
Masaki Katsumaru?
Mikio Nakano?Kotaro Funakoshi?
Tetsuya Ogata?
Hiroshi G.
Okuno??
Graduate School of Informatics, Kyoto University{komatani,katumaru,ogata,okuno}@kuis.kyoto-u.ac.jp?
Honda Research Institute Japan Co., Ltd.{nakano,funakoshi}@jp.honda-ri.comAbstractThe optimal choice of speech understand-ing method depends on the amount oftraining data available in rapid prototyp-ing.
A statistical method is ultimatelychosen, but it is not clear at which pointin the increase in training data a statisti-cal method become effective.
Our frame-work combines multiple automatic speechrecognition (ASR) and language under-standing (LU) modules to provide a setof speech understanding results and se-lects the best result among them.
Theissue is how to allocate training data tostatistical modules and the selection mod-ule in order to avoid overfitting in trainingand obtain better performance.
This paperpresents an automatic training data alloca-tion method that is based on the changein the coefficients of the logistic regres-sion functions used in the selection mod-ule.
Experimental evaluation showed thatour allocation method outperformed base-line methods that use a single ASR mod-ule and a single LU module at every pointwhile training data increase.1 IntroductionSpeech understanding in spoken dialogue systemsis the process of extracting a semantic represen-tation from a user?s speech.
That is, it consistsof automatic speech recognition (ASR) and lan-guage understanding (LU).
Because vocabulariesand language expressions depend on individualsystems, it needs to be constructed for each sys-tem, and accordingly, training data are requiredfor each.
To collect more real training data, whichwill lead to higher performance, it is more desir-able to use a prototype system than that based onthe Wizard-of-Oz (WoZ) method where real ASRerrors cannot be observed, and to use a more ac-curate speech understanding module.
That is, inthe bootstrapping phase, spoken dialogue systemsneed to operate before sufficient real data havebeen collected.We have been addressing the issue of rapid pro-totyping on the basis of the ?Multiple Languagemodel for ASR and Multiple language Under-standing (MLMU)?
framework (Katsumaru et al,2009).
In MLMU, the most reliable speech un-derstanding result is selected from candidates pro-duced by various combinations of multiple ASRand LU modules using hand-crafted grammar andstatistical models.
A grammar-based method isstill effective at an early stage of system devel-opment because it does not require training data;Schapire et al (2005) also incorporated human-crafted prior knowledge into their boosting al-gorithm.
By combining multiple understandingmodules, complementary results can be obtainedby different kinds of ASR and LU modules.We propose a novel method to allocate avail-able training data to statistical modules when theamount of training data increases.
The trainingdata need to be allocated adaptively because thereare several modules to be trained, and they wouldcause overfitting without data allocation.
Thereare speech understanding modules that have lan-guage models (LMs) for ASR and LU models579(LUMs), and a selection module that selects themost reliable speech understanding result frommultiple candidates in the MLMU framework.When the amount of available training data issmall, and an LUM and the selection module aretrained on the same data set, they are trained un-der a closed-set condition, and thus the trainingdata for the selection module include too manycorrect understanding results.
In such cases, thedata need to be divided into subdata sets to avoidoverfitting.
On the other hand, when the amountof available training data is large, so that overfit-ting does not occur, all available data should beused to train each statistical module to prepare asmuch training data as possible.We therefore develop a method for switchingdata allocation policies.
More specifically, twopoints are automatically determined at which sta-tistical modules with more parameters start to betrained.
As a result, better overall performanceis achieved at every point while the amount oftraining data increases, compared with all combi-nations of a single ASR module and a single LUmodule.2 Related WorkIt is important to consider the amount of availabletraining data when designing a speech understand-ing module.
Many statistical LU methods havebeen studied, e.g., (Wang and Acero, 2006; Jeongand Lee, 2006; Raymond and Riccardi, 2007;Hahn et al, 2008; Dinarelli et al, 2009).
Theygenerally outperform grammar-based LU meth-ods when a sufficient amount of training data isavailable; but sufficient training data are not nec-essarily available during rapid prototyping.
Sev-eral LU methods were constructed using a smallamount of training data (Fukubayashi et al, 2008;Dinarelli et al, 2009).
Fukubayashi et al (2008)constructed an LU method based on the weightedfinite state transducer (WFST), in which fillertransitions accepting arbitrary inputs and transi-tion weights were added to a hand-crafted FST.This method is placed between a grammar-basedmethod and a statistical method because a sta-tistically selected weighting scheme is appliedto a hand-crafted grammar model.
Therefore,the amount of training data can be smaller com-pared with general statistical LU methods, but thismethod does not outperform them when plenty oftraining data are available.
Dinarelli et al (2009)used a generative model for which overfitting isless prone to occur than discriminative modelswhen the amount of training data is small, butthey did not use a grammar-based model, which isexpected to achieve reasonable performance evenwhen the amount of training data is very small.Raymond et al (2007) compared the perfor-mances of statistical LU methods for variousamounts of training data.
They used a statis-tical finite-state transducer (SFST) as a genera-tive model and a support vector machine (SVM)and conditional random fields (CRF) as discrim-inative models.
The generative model was moreeffective when the amount of data was small,and the discriminative models were more effec-tive when it was large.
This shows that the perfor-mance of an LUmethod depends on the amount oftraining data available, and therefore, LU meth-ods need to be switched automatically.
Wang etal.
(2002) developed a two-stage speech under-standing method by applying statistical methodsfirst and then grammatical rules.
They also ex-amined the performance of the statistical methodsat their first stage for various amounts of train-ing data and confirmed that the performance is notvery high when a small amount of data is used.Schapire et al (2005) showed that accuracyof call classification in spoken dialogue systemsimproved by incorporating hand-crafted priorknowledge into their boosting algorithm.
Theiridea is the same as ours in that they improve thesystem?s performance by using hand-crafted hu-man knowledge while only a small amount oftraining data is available.
We furthermore solvethe data allocation problem because there are mul-tiple statistical models to be trained in speechunderstanding, while their call classification hasonly one statistical model.3 MLMU FrameworkMLMU is the framework for selecting the mostreliable speech understanding result from multi-ple speech understanding modules (Katsumaru etal., 2009).
In this paper, we furthermore adapt theselection module to the amount of available train-580LU model#1Languagemodel #1LUmodulesASRmodulesResult:1CMNCMMNCM?iiCMmaxargN1Selection moduleLUresultsASRresultsUtteranceASR: automatic speech recognitionLU: language understandingCM: confidence measureM ?M ?Speech understandingLanguagemodel #2Languagemodel #NLU model#2LU model#MLogisticregression #1Logisticregression #NLogisticregression #Figure 1: Overview of speech understanding framework MLMUing data.
More specifically, the allocation policyof training data is changed and thus appropriateLMs and LUMs are selected as its result.An overview of MLMU is shown in Figure 1.MLMU uses multiple LMs for ASR and multi-ple LUMs and selects the most reliable speech un-derstanding result from all combinations of them.We denote a speech understanding module as SUi(i = 1, .
.
.
, n).
Its result is a semantic representa-tion consisting of a set of concepts.
The concept iseither a semantic slot and its value or an utterancetype.
Note that n = N ?
M , when N LMs andM LUMs are used.
The confidence measure perutterance for a result of i-th speech understandingmodule SUi is denoted as CMi.
The speech un-derstanding result having the highest confidencemeasure is selected as the final result for the ut-terance.
That is, the result is the output of SUmwhere m = argmaxi CMi.The confidence measure is calculated by logis-tic regression based on the features of each speechunderstanding result.
A logistic regression func-tion is constructed for each speech understandingmodule SUi:CMi =11 + e?
(ai1Fi1+...+ai7Fi7+bi) .
(1)Parameters ai1, .
.
.
, ai7 and bi are determined byusing training data.
In the training phase, teachersignal 1 is given when a speech understanding re-sult is completely correct; that is, when no error iscontained in the result.
Otherwise, 0 is given.
Weuse seven features, Fi1, Fi2, .
.
.
, Fi7, as indepen-dent variables.
Each feature value is normalizedTable 1: Features of speech understanding resultobtained from SUiFi1: Acoustic score normalized by utterance lengthFi2: Difference between Fi1 and normalized acousticscores of verification ASRFi3: Average concept CM in understanding resultFi4: Minimum concept CM in understanding resultFi5: Number of concepts in understanding resultFi6: Whether any understanding result is obtainedFi7: Whether understanding result is yes/noCM: confidence measureso as to make its mean zero and its variance one.The features used are listed in Table 1.
Com-pared with those used in our previous paper (Kat-sumaru et al, 2009), we deleted ones that werehighly correlated with other features and addedones regarding content of the speech understand-ing results.
Features Fi1 and Fi2 are obtainedfrom an ASR result.
Another ASR with a gen-eral large vocabulary LM is executed for verifyingthe i-th ASR result.
Fi2 is the difference betweenits score and Fi1 (Komatani et al, 2007).
Thesetwo features represent the reliability of the ASRresult.
Fi3 and Fi4 are calculated for each conceptin the LU result on the basis of the posterior prob-ability of the 10-best ASR candidates (Komataniand Kawahara, 2000).
Fi5 is the number of con-cepts in the LU result.
This feature is effective be-cause the LU results of lengthy utterances tend tobe erroneous in a grammar-based LU.
Fi6 repre-sents the case when an ASR result is not acceptedby the subsequent LU module.
In such cases, nospeech understanding result is obtained, which is581U1: It is June ninth.ASR result:- grammar ?It is June ninth.
?- N-gram ?It is June noon and?LU result:- grammar + FST ?month:6 day:9 type:refer-time?- N-gram + WFST ?month:6 type:refer-time?U2: I will borrow it on twentieth.
(Underlined part is out-of-grammar.
)ASR result:- grammar ?Around two pm on twentieth.
?- N-gram ?Around two at ten on twentieth.
?LU result:- grammar + FST ?day:20 hour:14 type:refer-time?- N-gram + WFST ?day:20 type:refer-time?Combination of LM and LUM is denoted as ?LM+LUM?.Figure 2: Example of speech understanding re-sults in MLMU frameworkregarded as an error.
Fi7 is added because affirma-tive and negative responses, typically ?Yes?
and?No?, tend to be correctly recognized and under-stood.Figure 2 depicts an example when multipleASRs based on LMs and multiple LUs are used.In short, the correct speech understanding result isobtained from a different combination of LMs andLUMs.4 Automatic Allocation of Training DataUsing Change in CoefficientsThe training data need to be allocated to thespeech understanding modules (i.e., statistical LMand statistical LUM) and the selection module.
Ifmore data are allocated to the ASR and LU mod-ules, the performances of these modules are im-proved, but the overall performance is degradedbecause of the low performance of the selectionmodule.
On the other hand, even if more trainingdata are allocated to the selection module, the per-formance of each ASR and LU module remainslow.4.1 Allocation PolicyWe focus on the convergence of the logistic re-gression functions when the amount of trainingdata increases.
The convergence is defined asthe change in their coefficients, which will appearlater as Equation 2, and determines two points1.
All data are used totrain selection modules2.
Data are allocated to SUand selection modules3.
Data arenot dividedNo NoYesYesSelection modulefirst converges?No over-fittingoccurs?Amount of training data increasesSU: speech understandingFigure 3: Flowchart of data allocationduring the increase in training data, and thus threephases are defined.
The flowchart of data alloca-tion is depicted in Figure 3.
The three phases areexplained below.In the first phase, the first priority is given tothe selection module.
This is because the lo-gistic regression functions used in the selectionmodule converge with relatively less training datathan those in the statistical ASR and LU mod-ules for speech understanding; there are eight pa-rameters for each logistic regression function asshown in Equation 1, far fewer than for other sta-tistical models such as N-gram and CRF.
The out-put from a speech understanding module that em-ploys grammar-based LM and LUM would be themost reliable in many cases because its perfor-mance is better than that of other statistical mod-ules when a very small amount of training data isavailable.
As a result, equivalent or better perfor-mance would be achieved than methods using asingle ASR module and a single LU module.In the second phase, the training data are alsoallocated to the speech understanding modules af-ter the selection module converges.
This aimsto improve the performance of the speech under-standing modules by allocating as much trainingdata to them as possible.
The amount of train-ing data is fixed in this phase to the amount al-located to the selection module determined in thefirst phase.
The remaining data are used to trainthe speech understanding modules.When the performances of all the speech under-standing modules stabilize, the allocation phaseproceeds to the third one.
After this point, wehypothesize that overfitting does not occur in thisphase because plenty of training data are avail-able.
All available data are used to train all mod-582ules without dividing the data in this phase.4.2 Determining When to Switch AllocationPoliciesAutomatic switching from one phase to the nextrequires the determination of two points in thenumber of training utterances: when the selec-tion module first converges (konlysel) and whenthe speech understanding modules all become sta-ble (knodiv).
These points are determined by fo-cusing on the changes in the coefficients of thelogistic regression functions when the number ofutterances used as training data increases.
We ob-serve the sum of the changes in the coefficients ofthe functions and then identify the points at whichthe changes converge.
The points are determinedindividually by the following algorithm.Step 1 Construct two logistic regression func-tions for speech understanding module SUiby using k and (k + ?k) utterances out ofkmax utterances, where kmax is the amountof training data available.Step 2 Calculate the change in coefficients fromthe two logistic regression functions by?i(k) =?j|aij(k + ?k) ?
aij(k)|+|bi(k + ?k) ?
bi(k)|, (2)where aij(k) and bi(k) denote the param-eters of the logistic regression functions,shown in Equation 1, for speech understand-ing module SUi, when k utterances are usedto train the functions.Step 3 If ?i(k) becomes smaller than threshold?, consider that the training of the functionshas converged, and record this k as the pointof convergence.
If not, return to Step 1 afterk ?
k + ?k.The ?k is the minimum unit of training data con-taining various utterances.
We set it as the numberof utterances in one dialogue session, whose aver-age was 17.
Threshold ?
was set to 8, which corre-sponds to the number of parameters in the logisticregression functions.
No experiments were con-ducted to determine if better performance couldbe achieved with other choices of ?1.The first point, konlysel, is determined using thespeech understanding module that uses no trainingdata.
Specifically, we used ?grammar+FST?
asmethod SUi.
Here, ?LM+LUM?
denotes a com-bination of LM for ASR and LUM.
If the func-tion converges at k utterances, we set konlysel tok and fix the k utterances as training data used bythe selection module.
The remaining (kmax ?
k)utterances are allocated to the speech understand-ing modules, that is, the LMs and LUMs.
Notethat if k becomes equal to kmax before ?i con-verges, all training data are allocated to the selec-tion module; that is, no data are allocated to theLMs and LUMs.
In this case, no output is ob-tained from statistical speech understanding mod-ules, and only outputs from the grammar-basedmodules are used.The second point, knodiv , is determined on thebasis of the speech understanding module thatneeds the largest amount of data for training.
Theamount of data needed depends on the number ofparameters.
Specifically, we used ?N-gram+CRF?as SUi in Equation 2.
If the function converges,we hypothesize that the performance of all thespeech understanding modules stabilize and thusoverfitting does not occur.
We then stop the divi-sion of training data, and use all available data totrain the statistical modules.5 Experimental Evaluation5.1 Target Data and ImplementationWe used a data set previously collected throughactual dialogues with a rent-a-car reservation sys-tem (Nakano et al, 2007) with 39 participants.Each participant performed 8 dialogue sessions,and 5900 utterances were collected in total.
Outof these utterances, we used 5240 for which theautomatic voice activity detection (VAD) resultsagreed with manual annotation.
We divided theutterances into two sets: 2121 with 16 participantsas training data and 3119 with 23 participants asthe test data.1We do not think the value is very critical after seeing theresults shown in Figure 4.583We constructed another rent-a-car reservationsystem to evaluate our allocation method.
Thesystem included two language models (LMs)and four language understanding models (LUMs).That is, eight speech understanding results in totalwere obtained.
The two LMs were a grammar-based LM (?grammar?, hereafter) and a domain-specific statistical LM (?N-gram?).
The grammarmodel was described by hand to be equivalent tothe FST model used in LU.
The N-gram modelwas a class 3-gram and was trained on a tran-scription of the available training data.
The vo-cabulary size was 281 for the grammar model and420 for the N-gram model when all the trainingdata were used.
The ASR accuracies of the gram-mar and N-gram models were 67.8% and 90.5%for the training data and 66.3% and 85.0% for thetest data when all the training data were used.
Weused Julius (ver.
4.1.2) as the speech recognizerand a gender-independent phonetic-tied mixturemodel as the acoustic model (Kawahara et al,2004).
We also used a domain-independent statis-tical LM with a vocabulary size of 60250, whichwas trained on Web documents (Kawahara et al,2004), as the verification model.The four LUMs were a finite-state transducer(FST) model, a weighted FST (WFST) model,a keyphrase-extractor (Extractor) model, and aconditional random fields (CRF) model.
In theFST-based LUM, the FST was constructed byhand.
The WFST-based LUM is based on themethod developed by Fukubayashi et al (2008).The WFSTs were constructed by using the MITFST Toolkit (Hetherington, 2004).
The weight-ing scheme used for the test data was selected byusing training data (Fukubayashi et al, 2008).
Inthe extractor-based LUM, as many parts as pos-sible in the ASR result were simply transformedinto concepts.
As the CRF-based LUM, we usedopen-source software, CRF++2, to construct theLUM.
As its features, we use a word in the ASRresult, its first character, its last character, and theASR confidence of the word.
Its parameters wereestimated by using training data.The metric used for speech understanding per-formance was concept understanding accuracy,2http://crfpp.sourceforge.net/Table 2: Absolute degradation in oracle accuracywhen each module was removedCase (A) (B)With all modules (%) 86.6 90.1w/o grammar ASR -12.0 -1.1w/o N-gram ASR -6.1 -7.7w/o FST LUM -0.4 0.0w/o WFST LUM -1.2 -0.5w/o Extractor LUM -0.1 0.0w/o CRF LUM -0.6 -3.7(w/o FST & Extractor LUMs) -1.0 -0.1(A): 141 utterances with 1 participant(B): 2121 utterances with 16 participantsdefined as1 ?
SUB + INS + DELno.
of concepts in correct results,where SUB, INS, and DEL denote the numbers ofsubstitution, insertion, and deletion errors.5.2 Effectiveness of Using Multiple LMs andLUMsWe investigated how much the performance of ourframework degraded when one ASR or LU mod-ule was removed.
We used the oracle accuracies,i.e., when the most appropriate result was selectedby hand.
The result reveals the contribution ofeach ASR and LU module to the performance ofthe framework.
A module is regarded as more im-portant when the accuracy is degraded more whenit is removed than when another one is removed.Two cases (A) and (B) were defined: when theamount of available training data was (A) smalland (B) large.
We used 141 utterances with 1 par-ticipant for case (A) and 2121 utterances with 16participants for case (B).
The results are shown inTable 2.When a small amount of training data wasavailable (case (A)), the accuracy was degraded by12.0 points when the grammar-based ASRmodulewas removed and 6.1 points when the N-gram-based ASR module was removed.
The accuracywas thus degraded substantially when either ASRmodule was removed.
This indicates that the twoASR modules work complementarily.5840204060801001201401601802000 100 200 300 400 500ChangesincoefficientsNumber of training utterances available(a) grammar+FST0204060801001201401601802000 100 200 300 400 500ChangesincoefficientsNumber of training utterances available(b) N-gram+CRFFigure 4: Change in the sum of coefficients ?i when amount of training data increases (?LM+LUM?denotes combination of LM and LUM)On the other hand, when a large amount oftraining data was available (case (B)), the ac-curacy was degraded by 1.1 points when thegrammar-based ASR was removed.
This meansthat it became less important when there areplenty of training data because the coverage of theN-gram-based ASR became wider.
In short, espe-cially when the amount of training data is smaller,speech understanding modules based on a hand-crafted grammar are more important because ofthe low performance of statistical modules.Concerning the LUMs, the accuracy was de-graded when any of the LUM modules was re-moved when a small amount of training data wasavailable.
When a large amount of training datawas available, the module based on CRF in par-ticular became more important.5.3 Results and Evaluation of AutomaticAllocationFigure 4 shows the change in the sum of the co-efficients, ?i, with the increase in the amount oftraining data.
In Figure 4(a), the change was verylarge while the amount of training data was small,and decreased dramatically and converged aroundone hundred utterances.
By applying ?
(=8) to ?i,we set 111 utterances as the first point, konlysel,up to which all the training data are allocated tothe selection module, as described in Section 4.1.Similarly, from the results shown in Figure 4(b),we set 207 utterances as the second point, knodiv,from which the training data are not divided.To evaluate our method for allocating training556065707580859050 100 200 400 800 1600Conceptunderstandingaccuracy[%]Number of training utterances availableOur methodNa?ve allocationNo divisionFigure 5: Results of allocation methodsdata, we compared it with two baseline methods:?
No-division method: All data available ateach point were used to train both the speechunderstanding modules and the selectionmodule.
That is, the same data set was usedto train them.?
Naive-allocation method: Training dataavailable at each point were allocated equallyto the speech understanding modules and theselection module.As shown in Figure 5, our method had the bestconcept understanding accuracy when the amountof training data was small, that is, up to about278 utterances.
This indicates that our method forallocating the available training data is effectivewhen the amount of training data is small.This result is explained more specifically by us-585Table 3: Concept understanding accuracy for 141utterancesAccuracy (%)Our method 77.9Naive allocation 73.5No division 74.1ing the case in which 141 utterances were used asthe training data.
111 (= konlysel) were secured totrain the selection module and 30 utterances wereallocated to train the speech understanding mod-ules.
As shown in Table 3, the accuracy with ourmethod was 3.8 points higher than that with theno-division baseline method.
This was achievedby avoiding the overfitting of the logistic regres-sion functions; i.e., the data input to the functionsbecame similar to the test data due to allocation,so the concept understanding accuracy for the testset was improved.
The accuracy with our methodwas 4.4 points higher than that with the naive al-location baseline method.
This was because theamount of training data allocated to the selectionmodule was less than our method, and accordinglythe selection module was not trained sufficiently.5.4 Comparison with methods using a singleASR and a single LUFigure 6 plots concept understanding accuracywith our method against baseline methods usinga single ASR module and a single LU module forvarious amounts of training data.
Each module forcomparison was constructed by using all availabletraining data at each point while training data in-creased; i.e., the same condition as our method.The accuracies of only three speech understand-ing modules are shown in the figure, out of theeight obtained by combining two LMs for ASRand four LUMs.
These three are the ones with thehighest accuracies while the amount of trainingdata increased.
Our method switched the alloca-tion phase at 111 and 207 utterances, as describedin Section 5.3.Our method performed equivalently or betterthan all baseline methods even when only a smallamount of training data was available.
As a result,our method outperformed all the baseline methods5560657075808550 100 200 400 800 1600Conceptunderstandingaccuracy[%]Number of training utterances availableour methodgrammar+FSTN-gram+WFSTN-gram+CRFFigure 6: Comparison with baseline methods us-ing single speech understandingat every point while training data increase.6 ConclusionWe developed a method to automatically allo-cate training data to statistical modules so as toavoid performance degradation caused by overfit-ting.
Experimental evaluation showed that speechunderstanding accuracies achieved by our methodwere equivalent or better than the baseline meth-ods based on all combinations of a single ASRmodule and a single LU module at every pointwhile training data increase.
This includes a casewhen a very small amount of training data is avail-able.
We also showed empirically that the trainingdata should be allocated while an amount of train-ing data is not sufficient.
Our method allocatedavailable training data on the basis of our alloca-tion policy described in Section 4.1, and outper-formed the two baselines where the training datawere equivalently allocated and not allocated.When plenty of training data were available,there was no difference between our method andthe speech understanding method that requires themost training data, i.e., N-gram+CRF, as shown inFigure 6.
It is possible that our method combin-ing multiple speech understanding modules wouldoutperform it as Schapire et al (2005) reported.In their data, there were some examples that onlya hand-crafted rules can parse.
Including such atask as more complicated language understandinggrammar is required, verification of our method inother tasks is one of the future works.586ReferencesDinarelli, Marco, Alessandro Moschitti, and GiuseppeRiccardi.
2009.
Re-Ranking Models for SpokenLanguage Understanding.
In Proc.
European Chap-ter of the Association for Computational Linguistics(EACL), pages 202?210.Fukubayashi, Yuichiro, Kazunori Komatani, MikioNakano, Kotaro Funakoshi, Hiroshi Tsujino, Tet-suya Ogata, and Hiroshi G. Okuno.
2008.
Rapidprototyping of robust language understanding mod-ules for spoken dialogue systems.
In Proc.
Interna-tional Joint Conference on Natural Language Pro-cessing (IJCNLP), pages 210?216.Hahn, Stefan, Patrick Lehnen, and Hermann Ney.2008.
System Combination for Spoken LanguageUnderstanding.
In Proc.
Annual Conference of theInternational Speech Communication Association(INTERSPEECH), pages 236?239.Hetherington, Lee.
2004.
The MIT Finite-State Trans-ducer Toolkit for Speech and Language Processing.In Proc.
Int?l Conf.
Spoken Language Processing(ICSLP), pages 2609?2612.Jeong, Minwoo and Gary Geunbae Lee.
2006.
Ex-ploiting non-local features for spoken language un-derstanding.
In Proc.
COLING/ACL 2006 MainConference Poster Sessions, pages 412?419.Katsumaru, Masaki, Mikio Nakano, Kazunori Ko-matani, Kotaro Funakoshi, Tetsuya Ogata, and Hi-roshi G. Okuno.
2009.
Improving speech un-derstanding accuracy with limited training data us-ing multiple language models and multiple under-standing models.
In Proc.
Annual Conference ofthe International Speech Communication Associa-tion (INTERSPEECH), pages 2735?2738.Kawahara, Tatsuya, Akinobu Lee, Kazuya Takeda,Katsunobu Itou, and Kiyohiro Shikano.
2004.
Re-cent progress of open-source LVCSR engine Juliusand Japanese model repository.
In Proc.
Int?l Conf.Spoken Language Processing (ICSLP), pages 3069?3072.Komatani, Kazunori and Tatsuya Kawahara.
2000.Flexible mixed-initiative dialogue management us-ing concept-level confidence measures of speechrecognizer output.
In Proc.
Int?l Conf.
Computa-tional Linguistics (COLING), pages 467?473.Komatani, Kazunori, Yuichiro Fukubayashi, TetsuyaOgata, and Hiroshi G. Okuno.
2007.
Introducingutterance verification in spoken dialogue system toimprove dynamic help generation for novice users.In Proc.
8th SIGdial Workshop on Discourse andDialogue, pages 202?205.Nakano, Mikio, Yuka Nagano, Kotaro Funakoshi,Toshihiko Ito, Kenji Araki, Yuji Hasegawa, and Hi-roshi Tsujino.
2007.
Analysis of user reactions toturn-taking failures in spoken dialogue systems.
InProc.
8th SIGdial Workshop on Discourse and Dia-logue, pages 120?123.Raymond, Christian and Giuseppe Riccardi.
2007.Generative and Discriminative Algorithms for Spo-ken Language Understanding.
In Proc.
AnnualConference of the International Speech Communi-cation Association (INTERSPEECH), pages 1605?1608.Shapire, Robert E., Marie Rochery, Mazin Rahim, andNarendra Gupta.
2005.
Boosting with prior knowl-edge for call classification.
IEEE Trans.
on Speechand Audio Processing, 13(2):174?181.Wang, Ye-Yi and Alex Acero.
2006.
Discrimina-tive models for spoken language understanding.
InProc.
Int?l Conf.
Spoken Language Processing (IN-TERSPEECH), pages 2426?2429.Wang, Ye-Yi, Alex Acero, Ciprian Chelba, BrendanFrey, and Leon Wong.
2002.
Combination of Sta-tistical and Rule-based Approaches for Spoken Lan-guage Understanding.
In Proc.
Int?l Conf.
SpokenLanguage Processing (ICSLP), pages 609?612.587
