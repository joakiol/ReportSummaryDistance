Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1417?1421,Seattle, Washington, USA, 18-21 October 2013. c?2013 Association for Computational LinguisticsAutomatic Idiom Identification in WiktionaryGrace Muzny and Luke ZettlemoyerComputer Science & EngineeringUniversity of WashingtonSeattle, WA 98195{muznyg,lsz}@cs.washington.eduAbstractOnline resources, such as Wiktionary, providean accurate but incomplete source of idiomaticphrases.
In this paper, we study the problemof automatically identifying idiomatic dictio-nary entries with such resources.
We trainan idiom classifier on a newly gathered cor-pus of over 60,000 Wiktionary multi-worddefinitions, incorporating features that modelwhether phrase meanings are constructedcompositionally.
Experiments demonstratethat the learned classifier can provide highquality idiom labels, more than doubling thenumber of idiomatic entries from 7,764 to18,155 at precision levels of over 65%.
Thesegains also translate to idiom detection in sen-tences, by simply using known word sensedisambiguation algorithms to match phrasesto their definitions.
In a set of Wiktionary def-inition example sentences, the more completeset of idioms boosts detection recall by over28 percentage points.1 IntroductionIdiomatic language is common and provides uniquechallenges for language understanding systems.
Forexample, a diamond in the rough can be the literalunpolished object or a crude but lovable person.
Un-derstanding such distinctions is important for manyapplications, including parsing (Sag et al 2002) andmachine translation (Shutova et al 2012).We use Wiktionary as a large, but incomplete, ref-erence for idiomatic entries; individual entries canbe marked as idiomatic but, in practice, most arenot.
Using these incomplete annotations as super-vision, we train a binary Perceptron classifier foridentifying idiomatic dictionary entries.
We intro-duce new lexical and graph-based features that useWordNet and Wiktionary to compute semantic re-latedness.
This allows us to learn, for example, thatthe words in the phrase diamond in the rough aremore closely related to the words in its literal defi-nition than the idiomatic one.
Experiments demon-strate that the classifier achieves precision of over65% at recall over 52% and that, when used to fill inmissing Wiktionary idiom labels, it more than dou-bles the number of idioms from 7,764 to 18,155.These gains also translate to idiom detection insentences, by simply using the Lesk word sensedisambiguation (WSD) algorithm (1986) to matchphrases to their definitions.
This approach allowsfor scalable detection with no restrictions on the syn-tactic structure or context of the target phrase.
In aset of Wiktionary definition example sentences, themore complete set of idioms boosts detection recallby over 28 percentage points.2 Related WorkTo the best of our knowledge, this work representsthe first attempt to identify dictionary entries as id-iomatic and the first to reduce idiom detection toidentification via a dictionary.Previous idiom detection systems fall in oneof two paradigms: phrase classification, where aphrase p is always idiomatic or literal, e.g.
(Gedigianet al 2006; Shutova et al 2010), or token classifi-cation, where each occurrence of a phrase p can beidiomatic or literal, e.g.
(Katz and Giesbrecht, 2006;1417Birke and Sarkar, 2006; Li and Sporleder, 2009).Most previous idiom detection systems have focusedon specific syntactic constructions.
For instance,Shutova et al(2010) consider subject/verb (cam-paign surged) and verb/direct-object idioms (stir ex-citement) while Fazly and Stevenson (2006), Cooket al(2007), and Diab and Bhutada (2009) de-tect verb/noun idioms (blow smoke).
Fothergill andBaldwin (2012) are syntactically unconstrained, butonly study Japanese idioms.
Although we focus onidentifying idiomatic dictionary entries, one advan-tage of our approach is that it enables syntacticallyunconstrained token-level detection for any phrasein the dictionary.3 Formal Problem DefinitionsIdentification For identification, we assume dataof the form {(?pi, di?, yi) : i = 1 .
.
.
n} wherepi is the phrase associated with definition di andyi ?
{literal, idiomatic}.
For example, this wouldinclude both the literal pair ?
?leave for dead?, ?Toabandon a person or other living creature that is in-jured or otherwise incapacitated, assuming that thedeath of the one abandoned will soon follow.??
andthe idiomatic pair ?
?leave for dead?, ?To disregardor bypass as unimportant.?
?.
Given ?pi, di?, we aimto predict yi.Detection To evaluate identification in the con-text of detection, we assume data {(?pi, ei?, yi) :i = 1 .
.
.
n}.
Here, pi is the phrase in exam-ple sentence ei whose idiomatic status is labeledyi ?
{idiomatic, literal}.
One such idiomatic pairis ?
?heart to heart?, ?They sat down and had along overdue heart to heart about the future of theirrelationship.??.
Given ?pi, ei?, we again aim to pre-dict yi.4 DataWe gathered phrases, definitions, and example sen-tences from the English-language Wiktionary dumpfrom November 13th, 2012.1Identification Phrase, definition pairs ?p, d?
weregathered with the following restrictions: the title ofthe Wiktionary entry must be English, p must com-posed of two or more words w, and ?p, d?must be in1We used the Java Wiktionary Library (Zesch et al 2008).Data Set Literal Idiomatic TotalAll 56,037 7,764 63,801Train 47,633 6,600 54,233Unannotated Dev 2,801 388 3,189Annotated Dev 2,212 958 3,170Unannotated Test 5,603 776 6,379Annotated Test 4,510 1,834 6,344Figure 1: Number of dictionary entries with each classfor the Wiktionary identification data.Data Set Literal Idiomatic TotalDev 171 330 501Test 360 695 1055Figure 2: Number of sentences of each class for the Wik-tionary detection data.its base form?senses that are not defined as a dif-ferent tense of a phrase?e.g.
the pair ?
?weapons ofmass destruction?, ?Plural form of weapon of massdestruction?
?was removed while the pair ?
?weaponof mass destruction?, ?A chemical, biological, radio-logical, nuclear or other weapon that ...
??
was kept.Each pair ?p, d?
was assigned label y accordingto the idiom labels in Wiktionary, producing theTrain, Unannotated Dev, and Unannotated Test datasets.
In practice, this produces a noisy assignmentbecause a majority of the idiomatic senses are notmarked.
The development and test sets were anno-tated to correct these potential omissions.
Annota-tors used the definition of an idiom as a ?phrase witha non-compositional meaning?
to produce the An-notated Dev and Annotated Test data sets.
Figure 1presents the data statistics.We measured inter-annotator agreement on 1,000examples.
Two annotators marked each dictionaryentry as literal, idiomatic, or indeterminable.
Lessthan one half of one percent could not be deter-mined2?the computed kappa was 81.85.
Giventhis high level of agreement, the rest of the datawere only labeled by a single annotator, follow-ing the methodology used with the VNC-TokensDataset (Cook et al 2008).Detection For detection, we gathered the examplesentences provided, when available, for each defi-nition used in our annotated identification data sets.These sentences provide a clean source of develop-2The indeterminable pairs were omitted from the data.1418ment and test data containing idiomatic and literalphrase usages.
In all, there were over 1,300 uniquephrases, half of which had more than one possibledictionary definition in Wiktionary.
Figure 2 pro-vides the complete statistics.5 Identification ModelFor identification, we use a linear model that pre-dicts class y?
?
{literal, idiomatic} for an input pair?p, d?
with phrase p and definition d. We assign theclass:y?
= argmaxy?
?
?
(p, d, y)given features ?
(p, d, y) ?
Rn with associated pa-rameters ?
?
Rn.Learning In this work, we use the averaged Per-ceptron algorithm (Freund and Schapire, 1999) toperform learning, which was optimized in terms ofiterations T , bounded by range [1, 100], by maxi-mizing F-measure on the development set.The models described correspond to the featuresthey use.
All models are trained on the same, unan-notated training data.Features The features that were developed fallinto two categories: lexical and graph-based fea-tures.
The lexical features were motivated by theintuition that literal phrases are more likely to haveclosely related words in d to those in p because lit-eral phrases do not break the principle of compo-sitionality.
All words compared are stemmed ver-sions.
Let count(w, t) = number of times word wappears in text t.?
synonym overlap: Let S be the set of syn-onyms as defined in Wiktionary for all wordsin p. Then, we define the synonym overlap =1|S|?s?S count(s, d).?
antonym overlap: Let A be the set of antonymsas defined in Wiktionary for all words inp.
Then, we define the antonym overlap =1|A|?a?A count(a, d).?
average number of capitals:3 The value ofnumber of capital letters in pnumber of words in p .3In practice, this feature identifies most proper nouns.Graph-based features use the graph structure ofWordNet 3.0 to calculate path distances.
Letdistance(w, v, rel, n) be the minimum distance vialinks of type rel in WordNet from a word w to aword v, up to a threshold max integer value n, and 0otherwise.
The features compute:?
closest synonym:minw?p,v?ddistance(w, v, synonym, 5)?
closest antonym:4minw?p,v?ddistance(w, v, antonym, 5)?
average synonym distance:1|p|?w?p,v?ddistance(w, v, synonym, 5)?
average hyponym:1|p|?w?p,v?ddistance(w, v, hyponym, 5)?
synsets connected by an antonym: This feature in-dicates whether the following is true.
The set ofsynsets Synp, all synsets from all words in p, andthe set of synsets Synd, all synsets from all wordsin d, are connected by a shared antonym.
This fea-ture follows an approach described by Budanitskyet al(2006).6 ExperimentsWe report identification and detection results, vary-ing the data labeling and choice of feature sets.6.1 IdentificationRandom Baseline We use a proportionally ran-dom baseline for the identification task that classi-fies according to the proportion of literal definitionsseen in the training data.Results Figure 3 provides the results for the base-line, the full approach, and variations with subsetsof the features.
Results are reported for the origi-nal, unannotated test set, and the same test exampleswith corrected idiom labels.
All models increased4The first relation expanded was the antonym relation.
Allsubsequent expansions were via synonym relations.1419Data Set Model Rec.
Prec.
F1Unannotated Lexical 85.8 21.9 34.9Graph 62.4 26.6 37.3Lexical+Graph 70.5 28.1 40.1Baseline 12.2 11.9 12.0Annotated Lexical 81.2 49.3 61.4Graph 64.3 51.3 57.1Lexical+Graph 75.0 52.9 62.0Baseline 29.5 12.5 17.6Figure 3: Results for idiomatic definition identification.Figure 4: Precision and recall with varied features on theannotated test set.over their corresponding baselines by more than 22points and both feature families contributed.5Figure 4 shows the complete precision, recallcurve.
We selected our operating point to optimizeF-measure, but we see that the graph features per-form well across all recall levels and that adding thelexical features provides consistent improvement inprecision.
However, other points are possible, es-pecially when aiming for high precision to extendthe labels in Wiktionary.
For example, the original7,764 entries can be extended to 18,155 at 65% pre-cision, 9,594 at 80%, or 27,779 at 52.9%.Finally, Figures 5 and 6 present qualitative results,including newly discovered idioms and high scoringfalse identifications.
Analysis reveals where our sys-tem has room to improve?errors most often occurwith phrases that are specific to a certain field, such5We also ran ablations demonstrating that removing eachfeature from the Lexical+Graph model hurt performance, butomit the detailed results for space.Phrase Definitionfeel free You have my permission.live down To get used to something shameful.nail down To make something(e.g.
a decision or plan) firm or certain.make after To chase.get out To say something with difficulty.good riddance A welcome departure.to bad rubbishas all hell To a great extent or degree; very.roll around To happen, occur, take place.Figure 5: Newly discovered idioms.Phrase Definitionput asunder To sunder; disjoin; separate;disunite; divorce; annul; dissolve.add up To take a sum.peel off To remove (an outer layer orcovering, such as clothing).straighten up To become straight, or straighter.wild potato The edible root of this plant.shallow embedding The act of representing one logicor language with another byproviding a syntactic translation.Figure 6: High scoring false identifications.as sports or mathematics, and with phrases whosewords also appear in their definitions.6.2 DetectionApproach We use the Lesk (1986) algorithm toperform WSD, matching an input phrase p from sen-tence e to the definition d in Wiktionary that definesthe sense p is being used in.
The final classification yis then assigned to ?p, d?
by the identification model.Results Figure 7 shows detection results.
Thebaseline for this experiment is a model that assignsthe default labels within Wiktionary to the disam-biguated definition.
The Annotated model is theLexical+Graph model shown in Figure 3 evaluatedon the annotated data.
The +Default setting aug-ments the identification model by labeling the ?p, e?as idiomatic if either the model or the original labelwithin Wiktionary identifies it as such.7 ConclusionsWe presented a supervised approach to classifyingdefinitions as idiomatic or literal that more than dou-1420Model Rec.
Prec.
F1Default 60.5 1 75.4Annotated 78.3 76.7 77.5Annotated+Default 89.2 79.0 83.8Figure 7: Detection results.bles the number of marked idioms in Wiktionary,even when training on incomplete data.
When com-bined with the Lesk word sense algorithm, this ap-proach provides a complete idiom detector for anyphrase in the dictionary.We expect that semi-supervised learning tech-niques could better recover the missing labels andboost overall performance.
We also think it shouldbe possible to scale the detection approach, perhapswith automatic dictionary definition discovery, andevaluate it on more varied sentence types.AcknowledgmentsThe research was supported in part by the Na-tional Science Foundation (IIS-1115966) and aMary Gates Research Scholarship.
The authorsthank Nicholas FitzGerald, Sarah Vieweg, and MarkYatskar for helpful discussions and feedback.ReferencesJ.
Birke and A. Sarkar.
2006.
A clustering approachfor nearly unsupervised recognition of nonliteral lan-guage.
In Proceedings of the Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics.A.
Budanitsky and G. Hirst.
2006.
Evaluating wordnet-based measures of lexical semantic relatedness.
Com-putational Linguistics, 32(1):13?47.P.
Cook, A. Fazly, and S. Stevenson.
2007.
Pulling theirweight: Exploiting syntactic forms for the automaticidentification of idiomatic expressions in context.
InProceedings of the workshop on a broader perspectiveon multiword expressions.P.
Cook, A. Fazly, and S. Stevenson.
2008.
Thevnc-tokens dataset.
In Proceedings of the LanguageResources and Evaluation Conference Workshop To-wards a Shared Task for Multiword Expressions.M.
Diab and P. Bhutada.
2009.
Verb noun constructionmwe token supervised classification.
In Proceedingsof the Workshop on Multiword Expressions: Identifica-tion, Interpretation, Disambiguation and Applications.A.
Fazly and S. Stevenson.
2006.
Automatically con-structing a lexicon of verb phrase idiomatic combina-tions.
In Proceedings of the Conference of the Eu-ropean Chapter of the Association for ComputationalLinguistics.R.
Fothergill and T. Baldwin.
2012.
Combining re-sources for mwe-token classification.
In Proceedingsof the First Joint Conference on Lexical and Compu-tational Semantics-Volume 1: Proceedings of the mainconference and the shared task, and Volume 2: Pro-ceedings of the Sixth International Workshop on Se-mantic Evaluation.Y.
Freund and R.E.
Schapire.
1999.
Large margin clas-sification using the perceptron algorithm.
Machinelearning, 37(3):277?296.M.
Gedigian, J. Bryant, S. Narayanan, and B. Ciric.2006.
Catching metaphors.
In Proceedings of theThird Workshop on Scalable Natural Language Un-derstanding.G.
Katz and E. Giesbrecht.
2006.
Automatic identi-fication of non-compositional multi-word expressionsusing latent semantic analysis.
In Proceedings of theWorkshop on Multiword Expressions: Identifying andExploiting Underlying Properties.M.
Lesk.
1986.
Automatic sense disambiguation usingmachine readable dictionaries: How to tell a pine conefrom an ice cream cone.
In Proceedings of SpecialInterest Group on the Design of Communication.L.
Li and C. Sporleder.
2009.
Classifier combination forcontextual idiom detection without labelled data.
InProceedings of the Conference on Empirical Methodsin Natural Language Processing.I.
Sag, T. Baldwin, F. Bond, A. Copestake, andD.
Flickinger.
2002.
Multiword expressions: A painin the neck for nlp.
In Computational Linguistics andIntelligent Text Processing.
Springer.E.
Shutova, L. Sun, and A. Korhonen.
2010.
Metaphoridentification using verb and noun clustering.
In Pro-ceedings of the International Conference on Computa-tional Linguistics.E.
Shutova, S. Teufel, and A. Korhonen.
2012.
Statisti-cal metaphor processing.
Computational Linguistics,39(2):301?353.T.
Zesch, C. Mu?ller, and I. Gurevych.
2008.
Extractinglexical semantic knowledge from wikipedia and wik-tionary.
In Proceedings of the International Confer-ence on Language Resources and Evaluation.1421
