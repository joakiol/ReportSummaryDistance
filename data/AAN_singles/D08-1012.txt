Proceedings of the 2008 Conference on Empirical Methods in Natural Language Processing, pages 108?116,Honolulu, October 2008. c?2008 Association for Computational LinguisticsCoarse-to-Fine Syntactic Machine Translationusing Language ProjectionsSlav Petrov Aria Haghighi Dan KleinComputer Science Division, EECS DepartmentUniversity of California at BerkeleyBerkeley, CA 94720{petrov, aria42, klein}@eecs.berkeley.eduAbstractThe intersection of tree transducer-basedtranslation models with n-gram languagemodels results in huge dynamic programs formachine translation decoding.
We propose amultipass, coarse-to-fine approach in whichthe language model complexity is incremen-tally introduced.
In contrast to previous order-based bigram-to-trigram approaches, we fo-cus on encoding-based methods, which usea clustered encoding of the target language.Across various encoding schemes, and formultiple language pairs, we show speed-ups ofup to 50 times over single-pass decoding whileimproving BLEU score.
Moreover, our entiredecoding cascade for trigram language modelsis faster than the corresponding bigram passalone of a bigram-to-trigram decoder.1 IntroductionIn the absence of an n-gram language model, decod-ing a synchronous CFG translation model is veryefficient, requiring only a variant of the CKY al-gorithm.
As in monolingual parsing, dynamic pro-gramming items are simply indexed by a source lan-guage span and a syntactic label.
Complexity ariseswhen n-gram language model scoring is added, be-cause items must now be distinguished by their ini-tial and final few target language words for purposesof later combination.
This lexically exploded searchspace is a root cause of inefficiency in decoding, andseveral methods have been suggested to combat it.The approach most relevant to the current work isZhang and Gildea (2008), which begins with an ini-tial bigram pass and uses the resulting chart to guidea final trigram pass.
Substantial speed-ups are ob-tained, but computation is still dominated by the ini-tial bigram pass.
The key challenge is that unigrammodels are too poor to prune well, but bigram mod-els are already huge.
In short, the problem is thatthere are too many words in the target language.
Inthis paper, we propose a new, coarse-to-fine, mul-tipass approach which allows much greater speed-ups by translating into abstracted languages.
Thatis, rather than beginning with a low-order model ofa still-large language, we exploit language projec-tions, hierarchical clusterings of the target language,to effectively reduce the size of the target language.In this way, initial passes can be very quick, withcomplexity phased in gradually.Central to coarse-to-fine language projection isthe construction of sequences of word clusterings(see Figure 1).
The clusterings are deterministicmappings from words to clusters, with the propertythat each clustering refines the previous one.
Thereare many choice points in this process, includinghow these clusterings are obtained and how muchrefinement is optimal for each pass.
We demon-strate that likelihood-based hierarchical EM train-ing (Petrov et al, 2006) and cluster-based languagemodeling methods (Goodman, 2001) are superiorto both rank-based and random-projection methods.In addition, we demonstrate that more than twopasses are beneficial and show that our computa-tion is equally distributed over all passes.
In ourexperiments, passes with less than 16-cluster lan-guage models are most advantageous, and even asingle pass with just two word clusters can reducedecoding time greatly.108To follow related work and to focus on the effectsof the language model, we present translation re-sults under an inversion transduction grammar (ITG)translation model (Wu, 1997) trained on the Eu-roparl corpus (Koehn, 2005), described in detail inSection 3, and using a trigram language model.
Weshow that, on a range of languages, our coarse-to-fine decoding approach greatly outperforms base-line beam pruning and bigram-to-trigram pruning ontime-to-BLEU plots, reducing decoding times by upto a factor of 50 compared to single pass decoding.In addition, coarse-to-fine decoding increases BLEUscores by up to 0.4 points.
This increase is a mixtureof improved search and subtly advantageous coarse-to-fine effects which are further discussed below.2 Coarse-to-Fine DecodingIn coarse-to-fine decoding, we create a series of ini-tially simple but increasingly complex search prob-lems.
We then use the solutions of the simpler prob-lems to prune the search spaces for more complexmodels, reducing the total computational cost.2.1 Related WorkTaken broadly, the coarse-to-fine approach is notnew to machine translation (MT) or even syntacticMT.
Many common decoder precomputations canbe seen as coarse-to-fine methods, including the A*-like forward estimates used in the Moses decoder(Koehn et al, 2007).
In an ITG framework likeours, Zhang and Gildea (2008) consider an approachin which the results of a bigram pass are used asan A* heuristic to guide a trigram pass.
In theirtwo-pass approach, the coarse bigram pass becomescomputationally dominant.
Our work differs in twoways.
First, we use posterior pruning rather thanA* search.
Unlike A* search, posterior pruningallows multipass methods.
Not only are posteriorpruning methods simpler (for example, there is noneed to have complex multipart bounds), but theycan be much more effective.
For example, in mono-lingual parsing, posterior pruning methods (Good-man, 1997; Charniak et al, 2006; Petrov and Klein,2007) have led to greater speedups than their morecautious A* analogues (Klein and Manning, 2003;Haghighi et al, 2007), though at the cost of guaran-teed optimality.LMOrderBits in language modelthe,report-NP-these,states1pi232 3the-NP-states0-NP-1 01-NP-10 010-NP-1000,1-NP-0,1 01,10-NP-00,10 010,100-NP-000,100......?Figure 2: Possible state projections pi for the target nounphrase ?the report for these states?
using the clustersfrom Figure 1.
The number of bits used to encode the tar-get language vocabulary is varied along the x-axis.
Thelanguage model order is varied along the y-axis.Second, we focus on an orthogonal axis of ab-straction: the size of the target language.
The in-troduction of abstract languages gives better controlover the granularity of the search space and providesa richer set of intermediate problems, allowing usto adapt the level of refinement of the intermediate,coarse passes to minimize total computation.Beyond coarse-to-fine approaches, other relatedapproaches have also been demonstrated for syntac-tic MT.
For example, Venugopal et al (2007) con-siders a greedy first pass with a full model followedby a second pass which bounds search to a regionnear the greedy results.
Huang and Chiang (2007)searches with the full model, but makes assumptionsabout the the amount of reordering the languagemodel can trigger in order to limit exploration.2.2 Language Model ProjectionsWhen decoding in a syntactic translation model withan n-gram language model, search states are spec-ified by a grammar nonterminal X as well as thethe n-1 left-most target side words ln?1, .
.
.
, l1 andright-most target side words r1, .
.
.
, rn?1 of the gen-erated hypothesis.
We denote the resulting lexical-ized state as ln?1, .
.
.
, l1-X-r1, .
.
.
, rn?1.
Assum-ing a vocabulary V and grammar symbol set G, thestate space size is up to |V |2(n?1)|G|, which is im-mense for a large vocabulary when n > 1.
Weconsider two ways to reduce the size of this searchspace.
First, we can reduce the order of the lan-guage model.
Second, we can reduce the numberof words in the vocabulary.
Both can be thoughtof as projections of the search space to smaller ab-109theseonewetheytheathatforstatesreportoftoalsobeenwillmust010001000 001010 011 100 101 110 1111011Figure 1: An example of hierarchical clustering of target language vocabulary (see Section 4).
Even with a smallnumber of clusters our divisive HMM clustering (Section 4.3) captures sensible syntactico-semantic classes.stracted spaces.
Figure 2 illustrates those two or-thogonal axes of abstraction.Order-based projections are simple.
As shownin Figure 2, they simply strip off the appropriatewords from each state, collapsing dynamic program-ming items which are identical from the standpointof their left-to-right combination in the lower or-der language model.
However, having only order-based projections is very limiting.
Zhang and Gildea(2008) found that their computation was dominatedby their bigram pass.
The only lower-order passpossible uses a unigram model, which provides noinformation about the interaction of the languagemodel and translation model reorderings.
We there-fore propose encoding-based projections.
Theseprojections reduce the size of the target language vo-cabulary by deterministically projecting each targetlanguage word to a word cluster.
This projection ex-tends to the whole search state in the obvious way:assuming a bigram language model, the state l-X-rprojects to c(l)-X-c(r), where c(?)
is the determin-istic word-to-cluster mapping.In our multipass approach, we will want a se-quence c1 .
.
.
cn of such projections.
This requires ahierarchical clustering of the target words, as shownin Figure 1.
Each word?s cluster membership can berepresented by an n-bit binary string.
Each prefix oflength k declares that word?s cluster assignment atthe k-bit level.
As we vary k, we obtain a sequenceof projections ck(?
), each one mapping words to amore refined clustering.
When performing inferencein a k-bit projection, we replace the detailed originallanguage model over words with a coarse languagemodel LMk over the k-bit word clusters.
In addition,we replace the phrase table with a projected phrasetable, which further increases the speed of projectedpasses.
In Section 4, we describe the various clus-tering schemes explored, as well as how the coarseLMk are estimated.2.3 Multipass DecodingUnlike previous work, where the state space existsonly at two levels of abstraction (i.e.
bigram and tri-gram), we have multiple levels to choose from (Fig-ure 2).
Because we use both encoding-based andorder-based projections, our options form a latticeof coarser state spaces, varying from extremely sim-ple (a bigram model with just two word clusters) tonearly the full space (a trigram model with 10 bits or1024 word clusters).We use this lattice to perform a series of coarsepasses with increasing complexity.
More formally,we decode a source sentence multiple times, in asequence of state spaces S0, S1, .
.
.
, Sn=S, whereeach Si is a refinement of Si?1 in either languagemodel order, language encoding size, or both.
Thestate spaces Si and Sj (i < j) are related to eachother via a projection operator pij?i(?)
which mapsrefined states deterministically to coarser states.We start by decoding an input x in the simpleststate space S0.
In particular, we compute the chartof the posterior distributions p0(s) = P (s|x) for allstates s ?
S0.
These posteriors will be used to prunethe search space S1 of the following pass.
States swhose posterior falls below a threshold t trigger theremoval of all more refined states s?
in the subse-quent pass (see Figure 3).
This technique is poste-rior pruning, and is different from A* methods intwo main ways.
First, it can be iterated in a multi-pass setting, and, second, it is generally more effi-1100-X-011-X-10 10-X-11 11-X-1100-X-11 10-X-1011-X-01 01-X-1010-X-00 11-X-00 10-X-0100-X-00 01-X-00 00-X-011-X-0 0-X-1 1-X-12-Bit Pass1-Bit Pass< t ?
< t ?
< t ?
< t ?
< t ?
< t ?
< t ?
< t ?< t ?< t ?
< t ?
< t ?01-X-1100-X-1001-X-01Figure 3: Example of state pruning in coarse-to-fine decoding using the language encoding projection (see Section 2.2).During the coarse one-bit word cluster pass, two of the four possible states are pruned.
Every extension of the prunedone-bit states (indicated by the grey shading) are not explored during the two-bit word cluster pass.cient with a potential cost of increased search errors(see Section 2.1 for more discussion).Looking at Figure 2, multipass coarse-to-fine de-coding can be visualized as a walk from a coarsepoint somewhere in the lower left to the most re-fined point in the upper right of the grid.
Manycoarse-to-fine schedules are possible.
In practice,we might start decoding with a 1-bit word bigrampass, followed by an 3-bit word bigram pass, fol-lowed by a 5-bit word trigram pass and so on (seeSection 5.3 for an empirical investigation).
In termsif time, we show that coarse-to-fine gives substantialspeed-ups.
There is of course an additional mem-ory requirement, but it is negligible.
As we will seein our experiments (Section 5) the largest gains canbe obtained with extremely coarse language mod-els.
In particular, the largest coarse model we use inour best multipass decoder uses a 4-bit encoding andhence has only 16 distinct words (or at most 4096trigrams).3 Inversion Transduction GrammarsWhile our approach applies in principle to a vari-ety of machine translation systems (phrase-based orsyntactic), we will use the inversion transductiongrammar (ITG) approach of Wu (1997) to facili-tate comparison with previous work (Zens and Ney,2003; Zhang and Gildea, 2008) as well as to focus onlanguage model complexity.
ITGs are a subclass ofsynchronous context-free grammars (SCFGs) wherethere are only three kinds of rules.
Preterminal unaryproductions produce terminal strings on both sides(words or phrases): X ?
e/f .
Binary in-order pro-ductions combine two phrases monotonically (X ?
[Y Z]).
Finally, binary inverted productions invertthe order of their children (X ?
?Y Z?).
These pro-ductions are associated with rewrite weights in thestandard way.Without a language model, SCFG decoding is justlike (monolingual) CFG parsing.
The dynamic pro-gramming states are specified by iXj , where ?i, j?
isa source sentence span and X is a nonterminal.
Theonly difference is that whenever we apply a CFGproduction on the source side, we need to remem-ber the corresponding synchronous production onthe target side and store the best obtainable transla-tion via a backpointer.
See Wu (1996) or Melamed(2004) for a detailed exposition.Once we integrate an n-gram language model, thestate space becomes lexicalized and combining dy-namic programming items becomes more difficult.Each state is now parametrized by the initial andfinal n?1 words in the target language hypothesis:ln?1, ..., l1-iXj-r1, ..., rn?1.
Whenever we combinetwo dynamic programming items, we need to scorethe fluency of their concatentation by incorporat-ing the score of any language model features whichcross the target side boundaries of the two concate-nated items (Chiang, 2005).
Decoding with an in-tegrated language model is computationally expen-sive for two reasons: (1) the need to keep track ofa large number of lexicalized hypotheses for eachsource span, and (2) the need to frequently query thelarge language model for each hypothesis combina-tion.Multipass coarse-to-fine decoding can alleviateboth computational issues.
We start by decodingin an extremely coarse bigram search space, wherethere are very few possible translations.
We com-pute standard inside/outside probabilities (iS/oS),as follows.
Consider the application of non-invertedbinary rule: we combine two items lb-iBk-rb andlc-kCj-rc spanning ?i, k?
and ?k, j?
respectively toform a larger item lb-iAj-rc, spanning ?i, j?.
The111lb-iAj -rc lb-iBk-rb lc-kCj-rcrclb+lb rc+=iS(lb-iAj -rc) += iS(lb-iBk-rb) ?
iS(lc-kCj-rc)LM(rb, lc) ?p(X?
[Y Z]) ?lcrbFigure 4: Monotonic combination of two hypotheses dur-ing the inside pass involves scoring the fluency of the con-catenation with the language model.inside score of the new item is incremented by:iS(lb-iAj-rc) += p(X ?
[Y Z]) ?
iS(lb-iBk-rb) ?iS(lc-kCj-rc) ?
LM(rb, lc)This process is also illustrated in Figure 4.
Ofcourse, we also loop over the split point k and ap-ply the other two rule types (inverted concatenation,terminal generation).
We omit those cases from thisexposition, as well as the update for the outside pass;they are standard and similar.
Once we have com-puted the inside and outside scores, we compute pos-terior probabilities for all items:p(la-iAj-ra) =iS(la-iAj-ra)oS(la-iAj-ra)iS(root)where iS(root) is sum of all translations?
scores.States with low posteriors are then pruned away.We proceed to compute inside/outside score in thenext, more refined search space, using the projec-tions pii?i?1 to map between states in Si and Si?1.In each pass, we skip all items whose projection intothe previous stage had a probability below a stage-specific threshold.
This process is illustrated in Fig-ure 3.
When we reach the most refined search spaceS?, we do not prune, but rather extract the Viterbiderivation instead.14 Learning Coarse LanguagesCentral to our encoding-based projections (see Sec-tion 2.2) are hierarchical clusterings of the tar-get language vocabulary.
In the present work,these clusterings are each k-bit encodings and yieldsequences of coarse language models LMk andphrasetables PTk.1Other final decoding strategies are possible, of course, in-cluding variational methods and minimum-risk methods (Zhangand Gildea, 2008).Given a hierarchical clustering, we estimate thecorresponding LMk from a corpus obtained by re-placing each token in a target language corpus withthe appropriate word cluster.
As with our originalrefined language model, we estimate each coarselanguage model using the SRILM toolkit (Stolcke,2002).
The phrasetables PTk are similarly estimatedby replacing the words on the target side of eachphrase pair with the corresponding cluster.
This pro-cedure can potentially map two distinct phrase pairsto the same coarse translation.
In such cases we keeponly one coarse phrase pair and sum the scores of thecolliding originals.There are many possible schemes for creating hi-erarchical clusterings.
Here, we consider several di-visive clustering methods, where coarse word clus-ters are recursively split into smaller subclusters.4.1 Random projectionsThe simplest approach to splitting a cluster is to ran-domly assign each word type to one of two new sub-clusters.
Random projections have been shown to bea good and computationally inexpensive dimension-ality reduction technique, especially for high dimen-sional data (Bingham andMannila, 2001).
Althoughour best performance does not come from randomprojections, we still obtain substantial speed-upsover a single pass fine decoder when using randomprojections in coarse passes.4.2 Frequency clusteringIn frequency clustering, we allocate words to clus-ters by frequency.
At each level, the most frequentwords go into one cluster and the rarest words gointo another one.
Concretely, we sort the words ina given cluster by frequency and split the cluster sothat the two halves have equal token mass.
This ap-proach can be seen as a radically simplified versionof Brown et al (1992).
It can, and does, result inhighly imbalanced cluster hierarchies.4.3 HMM clusteringAn approach found to be effective by Petrov andKlein (2007) for coarse-to-fine parsing is to uselikelihood-based hierarchical EM training.
Weadopt this approach here by identifying each clus-ter with a latent state in an HMM and determiniz-ing the emissions so that each word type is emitted11201000020000300004000050000600007000080000900000  1  2  3  4  5  6  7  8  9  10PerplexityNumber of bits in coarse language modelHMMJClusterFrequencyRandomFigure 5: Results of coarse language model perplexityexperiment (see Section 4.5).
HMM and JClustering havelower perplexity than frequency and random clusteringfor all number of bits in the language encoding.by only one state.
When splitting a cluster s intos1 and s2, we initially clone and mildly perturb itscorresponding state.
We then use EM to learn pa-rameters, which splits the state, and determinize theresult.
Specifically, each word w is assigned to s1 ifP (w|s1) > P (w|s2) and s2 otherwise.
Because ofthis determinization after each round of EM, a wordin one cluster will be allocated to exactly one of thatcluster?s children.
This process not only guaranteesthat the clusters are hierarchical, it also avoids thestate drift discussed by Petrov and Klein (2007).
Be-cause the emissions are sparse, learning is very effi-cient.
An example of some of the words associatedwith early splits can be seen in Figure 1.4.4 JClusterGoodman (2001) presents a clustering schemewhich aims to minimize the entropy of a word givena cluster.
This is accomplished by incrementallyswapping words between clusters to locally mini-mize entropy.2 This clustering algorithm was devel-oped with a slightly different application in mind,but fits very well into our framework, because thehierarchical clusters it produces are trained to maxi-mize predictive likelihood.4.5 Clustering ResultsWe applied the above clustering algorithms to ourmonolingual language model data to obtain hierar-2The software for this clustering technique is available athttp://research.microsoft.com/?joshuago/.2828.228.428.628.82929.229.4100  1000  10000  100000BLEUTotal time in secondsHMMJClusterFrequenceRandomSingle pass (no clustering)Figure 6: Coarse-to-fine decoding with HMM or JClus-tering coarse language models reduce decoding timeswhile increasing accuracy.chical clusters.
We then trained coarse languagemodels of varying granularity and evaluated them ona held-out set.
To measure the quality of the coarselanguage models we use perplexity (exponentiatedcross-entropy).3 Figure 5 shows that HMM clus-tering and JClustering have lower perplexity thanfrequency and random based clustering for all com-plexities.
In the next section we will present a set ofmachine translation experiments using these coarselanguage models; the clusterings with better per-plexities generally produce better decoders.5 ExperimentsWe ran our experiments on the Europarl corpus(Koehn, 2005) and show results on Spanish, Frenchand German to English translation.
We used thesetup and preprocessing steps detailed in the 2008Workshop on Statistical Machine Translation.4 Ourbaseline decoder uses an ITG with an integrated tri-gram language model.
Phrase translation parame-ters are learned from parallel corpora with approx-imately 8.5 million words for each of the languagepairs.
The English language model is trained on theentire corpus of English parliamentary proceedingsprovided with the Europarl distribution.
We reportresults on the 2000 development test set sentencesof length up to 126 words (average length was 30words).3We assumed that each cluster had a uniform distributionover all the words in that cluster.4See http://www.statmt.org/wmt08 for details.1130501001502002503001-2-3-f1-3-f2-3-f1-f2-f3-f4-ffTotal timeinminutesLanguage model bits for coarse passesfine4 bits3 bits2 bits1 bitFigure 7: Many passes with extremely simple languagemodels produce the highest speed-ups.Our ITG translation model is broadly competitivewith state-of-the-art phrase-based-models trained onthe same data.
For example, on the Europarl devel-opment test set, we fall short of Moses (Koehn et al,2007) by less than one BLEU point.
On Spanish-English we get 29.47 BLEU (compared to Moses?s30.40), on French-English 29.34 (vs. 29.95), and23.80 (vs. 24.64) on German-English.
These differ-ences can be attributed primarily to the substantiallyricher distortion model used by Moses.The multipass coarse-to-fine architecture that wehave introduced presents many choice points.
Inthe following, we investigate various axes individu-ally.
We present our findings as BLEU-to-time plots,where the tradeoffs were generated by varying thecomplexity and the number of coarse passes, as wellas the pruning thresholds and beam sizes.
Unlessotherwise noted, the experiments are on Spanish-English using trigram language models.
Whendifferent decoder settings are applied to the samemodel, MERT weights (Och, 2003) from the unpro-jected single pass setup are used and are kept con-stant across runs.
In particular, the same MERTweights are used for all coarse passes; note that thisslightly disadvantages the multipass runs, which useMERT weights optimized for the single pass de-coder.5.1 ClusteringIn section Section 4, HMM clustering and JCluster-ing gave lower perplexities than frequency and ran-dom clustering when using the same number of bitsfor encoding the language model.
To test how these2828.228.428.628.82929.229.429.6100  1000  10000  100000BLEUTotal time in secondsEncoding+OrderOrderEncodingSingle passFigure 8: A combination of order-based and encoding-based coarse-to-fine decoding yields the best results.models perform at pruning, we ran our decoder sev-eral times, varying only the clustering source.
Ineach case, we used a 2-bit trigram model as a sin-gle coarse pass, followed by a fine output pass.
Fig-ure 6 shows that we can obtain significant improve-ments over the single-pass baseline regardless of theclustering.
To no great surprise, HMM clusteringand JClustering yield better results, giving a 30-foldspeed-up at the same accuracy, or improvements ofabout 0.3 BLEU when given the same time as thesingle pass decoder.
We discuss this increase in ac-curacy over the baseline in Section 5.5.
Since theperformance differences between those two cluster-ing algorithms are negligible, we will use the sim-pler HMM clustering in all subsequent experiments.5.2 SpacingGiven a hierarchy of coarse language models, alltrigam for the moment, we need to decide on thenumber of passes and the granularity of the coarselanguage models used in each pass.
Figure 7 showshow decoding time varies for different multipassschemes to achieve the same translation quality.A single coarse pass with a 4-bit language modelcuts decoding time almost in half.
However, onecan further cut decoding time by starting with evencoarser language models.
In fact, the best resultsare achieved by decoding in sequence with 1-, 2-and 3-bit language models before running the finalfine trigram pass.
Interestingly, in this setting, eachpass takes about the same amount of time.
A simi-lar observation was reported in the parsing literature,where coarse-to-fine inference with multiple passes11428 28.228.4 28.628.8 2929.2 29.429.6100  1000  10000BLEUTotal time in secondsSpanishCoarse-To-FineFine Baseline  2828.228.428.628.82929.229.4100  1000  10000BLEUTotal time in secondsFrenchCoarse-To-FineFine Baseline  2222.52323.524100  1000  10000BLEUTotal time in secondsGermanCoarse-To-FineFine BaselineFigure 9: Coarse-to-fine decoding is faster than single pass decoding with a trigram language model and leads to betterBLEU scores on all language pairs and for all parameter settings.of roughly equal complexity produces tremendousspeed-ups (Petrov and Klein, 2007).5.3 Encoding vs. OrderAs described in Section 2, the language model com-plexity can be reduced either by decreasing the vo-cabulary size (encoding-based projection) or by low-ering the language model order from trigram to bi-gram (order-based projection).
Figure 7 shows thatboth approaches alone yield comparable improve-ments over the single pass baseline.
Fortunately,the two approaches are complimentary, allowing usto obtain further improvements by combining both.We found it best to first do a series of coarse bigrampasses, followed by a fine bigram pass, followed bya fine trigram pass.5.4 Final ResultsFigure 9 compares our multipass coarse-to-fine de-coder using language refinement to single pass de-coding on three different languages.
On each lan-guage we get significant improvements in terms ofefficiency as well as accuracy.
Overall, we canachieve up to 50-fold speed-ups at the same accu-racy, or alternatively, improvements of 0.4 BLEUpoints over the best single pass run.In absolute terms, our decoder translates on aver-age about two Spanish sentences per second at thehighest accuracy setting.5 This compares favorablyto the Moses decoder (Koehn et al, 2007), whichtakes almost three seconds per sentence.5Of course, the time for an average sentence is much lower,since long sentences dominate the overall translation time.5.5 Search Error AnalysisIn multipass coarse-to-fine decoding, we noticedthat in addition to computational savings, BLEUscores tend to improve.
A first hypothesis isthat coarse-to-fine decoding simply improves searchquality, where fewer good items fall off the beamcompared to a simple fine pass.
However, this hy-pothesis turns out to be incorrect.
Table 1 showsthe percentage of test sentences for which the BLEUscore or log-likelihood changes when we switchfrom single pass decoding to coarse-to-fine multi-pass decoding.
Only about 30% of the sentencesget translated in the same way (if much faster) withcoarse-to-fine decoding.
For the rest, coarse-to-finedecoding mostly finds translations with lower likeli-hood, but higher BLEU score, than single pass de-coding.6 An increase of the underlying objectives ofinterest when pruning despite an increase in model-score search errors has also been observed in mono-lingual coarse-to-fine syntactic parsing (Charniak etal., 1998; Petrov and Klein, 2007).
This effect maybe because coarse-to-fine approximates certain min-imum Bayes risk objective.
It may also be an effectof model intersection between the various passes?models.
In any case, both possibilities are often per-fectly desirable.
It is also worth noting that the num-ber of search errors incurred in the coarse-to-fineapproach can be dramatically reduced (at the costof decoding time) by increasing the pruning thresh-olds.
However, the fortuitous nature of coarse-to-fine search errors seems to be a substantial and de-sirable effect.6We compared the influence of multipass decoding on theTM score and the LM score; both decrease.115LL> = <BLEU > 3.6% - 26.3%= 1.5% 29.6 % 12.9 %< 2.2% - 24.1%Table 1: Percentage of sentences for which the BLEUscore/log-likelihood improves/drops during coarse-to-fine decoding (compared to single pass decoding).6 ConclusionsWe have presented a coarse-to-fine syntactic de-coder which utilizes a novel encoding-based lan-guage projection in conjunction with order-basedprojections to achieve substantial speed-ups.
Un-like A* methods, a posterior pruning approach al-lows multiple passes, which we found to be verybeneficial for total decoding time.
When aggres-sively pruned, coarse-to-fine decoding can incur ad-ditional search errors, but we found those errors tobe fortuitous more often than harmful.
Our frame-work applies equally well to other translation sys-tems, though of course interesting new challengesarise when, for example, the underlying SCFGs be-come more complex.ReferencesE.
Bingham and H.i Mannila.
2001.
Random projectionin dimensionality reduction: applications to image andtext data.
In KDD ?01.P.
Brown, V. Della Pietra, P. deSouza, J. Lai, and R. Mer-cer.
1992.
Class-based n-gram models of natural lan-guage.
Computational Linguistics.E.
Charniak, S. Goldwater, and M. Johnson.
1998.
Edge-based best-first chart parsing.
6th Workshop on VeryLarge Corpora.E.
Charniak, M. Johnson, D. McClosky, et al 2006.Multi-level coarse-to-fine PCFG Parsing.
In HLT-NAACL ?06.D.
Chiang.
2005.
A hierarchical phrase-based model forstatistical machine translation.
In ACL ?05.J.
Goodman.
1997.
Global thresholding and multiple-pass parsing.
In EMNLP ?97.J.
Goodman.
2001.
A bit of progress in language model-ing.
Technical report, Microsoft Research.A.
Haghighi, J. DeNero, and D. Klein.
2007.
A* searchvia approximate factoring.
In NAACL ?07.L.
Huang and D. Chiang.
2007.
Forest rescoring: Fasterdecoding with integrated language models.
In ACL?07.D.
Klein and C. Manning.
2003.
A* parsing: fast exactviterbi parse selection.
In NAACL ?03.P.
Koehn, H. Hoang, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In ACL ?07.P.
Koehn.
2005.
Europarl: A parallel corpus for statisti-cal machine translation.
In MT Summit.I.
D. Melamed.
2004.
Statistical machine translation byparsing.
In ACL ?04.F.
Och.
2003.
Minimum error rate training in statisticalmachine translation.
In ACL ?03.S.
Petrov and D. Klein.
2007.
Improved inference forunlexicalized parsing.
In HLT-NAACL ?07.S.
Petrov, L. Barrett, R. Thibaux, and D. Klein.
2006.Learning accurate, compact, and interpretable tree an-notation.
In ACL ?06.A.
Stolcke.
2002.
SRILM ?
an extensible language mod-eling toolkit.
In ICSLP ?02.A.
Venugopal, A. Zollmann, and S. Vogel.
2007.
An ef-ficient two-pass approach to synchronous-CFG drivenstatistical MT.
In HLT-NAACL ?07.D.
Wu.
1996.
A polynomial-time algorithm for statisti-cal machine translation.
In ACL ?96.D.
Wu.
1997.
Stochastic inversion transduction gram-mars and bilingual parsing of parallel corpora.
InComputational Linguistics.R.
Zens and H. Ney.
2003.
A comparative study on re-ordering constraints in statistical machine translation.In ACL ?03.H.
Zhang and D. Gildea.
2008.
Efficient multi-passdecoding for synchronous context free grammars.
InACL ?08.116
