Word Sense Induction using Cluster EnsembleBichuan  Zhang, Jiashen SunLingjia Deng, Yun Huang, Jianri Li,Zhongwan Liu, Pujun ZuoCenter of Intelligence Science andTechnologySchool of ComputerBeijing University of Posts and Telecommunications, Beijing, 100876 ChinaAbstractIn this paper, we describe the implementationof an unsupervised learning method forChinese word sense induction inCIPS-SIGHAN-2010 bakeoff.
We presentthree individual clustering algorithms and theensemble of them, and discuss in particulardifferent approaches to represent text andselect features.
Our main system based oncluster ensemble achieves 79.33% in F-score,the best result of this WSI task.
Ourexperiments also demonstrate the versatilityand effectiveness of the proposed model ondata sparseness problems.1 IntroductionWord Sense Induction (WSI) is a particular taskof computational linguistics which consists inautomatically discovering the correct sense foreach instance of a given ambiguous word(Pinto , 2007).
This problem is closely related toWord Sense Disambiguation (WSD), however,in WSD the aim is to tag each ambiguous wordin a text with one of the senses known as prior,whereas in WSI the aim is to induce the differentsenses of that word.The object of the sense induction task ofCIPS-SIGHAN-2010 was to cluster 5,000instances of 100 different words into senses orclasses.
The task data consisted of thecombination of the test and training data (minusthe sense tags) from the Chinese lexical sampletask.
Each instance is a context of severalsentences which contains an occurrence of agiven word that serves as the target of senseinduction.The accuracy of the corpus-based algorithmsfor WSD is usually proportional to the amount ofhand-tagged data available, but the constructionof that kind of training data is often difficult forreal applications.
WSI overcomes this drawbackby using clustering algorithms which do not needtraining data in order to determine the possiblesense for a given ambiguous word.This paper describes an ensemble-basedunsupervised system for induction andclassification.
Given a set of data to be classified,the system clusters the data by individual clusters,then operates cluster ensemble to ensure theresult to be robust and accurate accordingly.The paper is organized as follows.
Section 2gives an description of the general framework ofour system.
Sections 3 and 4 present in moredetail the implementation of feature set andcluster algorithms used for the task, respectively.Section 5 presents the results obtained, andSection 6 draws conclusions and someinteresting future work.2 Methodology in Sense Induction TaskSense induction is typically treated as anunsupervised clustering problem.
The input tothe clustering algorithm are instances of theambiguous word with their accompanyingcontexts (represented by co-occurrence vectors)and the output is a grouping of these instancesinto classes corresponding to the induced senses.In other words, contexts that are groupedtogether in the same class represent a specificword sense.In this task, an instance to be clustered isrepresented as a bag of tokens or characters thatco?occur with the target word.
To exploit thediversity of features, besides the co?occurrencematrix, we invoke the n-gram such as bi-gramsthat occur in the contexts.
For assigning a weightfor each term in each instance, a number ofalternatives to tf-idf and entropy have beeninvestigated.This representation raises one severeproblem: the high dimensionality of the featurespace and the inherent data sparseness.Obviously, a single document has a sparse vectorover the set of all terms.
The performance ofclustering algorithms will decline dramaticallydue to the problems of high dimensionality anddata sparseness.
Therefore it is highly desirableto reduce the feature space dimensionality.
Weused two techniques to deal with this problem:feature selection and feature combination.Feature selection is a process that chooses asubset from the original feature set according tosome criterion.
The selected feature retainsoriginal physical meaning and provides a betterunderstanding for the data and learning process.Depending on whether the class labelinformation is required, feature selection can beeither unsupervised or supervised.
For WSIshould be an unsupervised fashion, thecorrelation of each feature with the class label iscomputed by distance, information dependence,or consistency measures.Feature combination is a process thatcombines multiple complementary features basedon different aspects extracted at the selectionstep, and forms a new set of features.The methods mentioned above are notdirectly targeted to clustering instances; in thispaper we introduce three cluster algorithms: (a)EM algorithms (Dempster et al, 1977;McLachlan and Krishnan, 1997), (b) K-means(MacQueen, 1967), and (c) LAC (LocallyAdaptive Clustering) (Domeniconi et al, 2004),and one cluster ensemble method to incorporatethree results together to represent the targetpatterns and conduct sense clustering.We conduct multiple experiments to assessdifferent methods for feature selection andfeature combination on real unsupervised WSIproblems, and make analysis through three facets:(a) to what extent feature selection can improvethe clustering quality, (b) how much width of thesmallest window that contains all theco?occurrence context can be reduced withoutlosing useful information in text clustering, and(c) what index weighting methods should beapplied to sense clustering.
Besides the featureexploitation, we studied in more detail theperformance of cluster ensemble method.3 Feature Extraction3.1 PreprocessingEach training or test instance for WSI taskcontains up to a few sentences as the surroundingcontext of the target word w, and the number ofthe sense of w is provided.
We assume that thesurrounding context of a target w is informativeto determine the sense of it.
Therefore a streamof induction methods can be designed byexploiting the context features for WSI.In our experiment, we consider both tokens(after word segmentation) and characters(without word segmentation) in the surroundingcontext of target word w as discriminativefeatures, and these tokens or characters can be indifferent sentences from instances of w. Tokensin the list of stop words and tokens with only onecharacter (such as punctuation symbols) areremoved from the feature sets.
All remainingterms are gathered to constitute the feature spaceof w.Since the long dependency property, the wordsense could be relying on the context far awayfrom it.
From this point, it seems that morefeatures will bring more accurate induction, andall linguistic cues should be incorporated into themodel.
However, more features are involved,more serious sparseness happens.
Therefore, it isimportant to find a sound trade-off between thescale and the representativeness of features.
Weuse the sample data provided by theCIPS-SIGHAN as a development data to find agenetic parameter to confine the context scale.Let ?
be the width of the smallest window in aninstance d that contains terms near the targetword, measured in the number of words in thewindow.
In cases where the terms in the windowdo not contain all of the informative terms, wecan set ?
to be some enormous number (?
< thelength of sentence).
Such proximity-weightedscoring functions are a departure from purecosine similarity and closer to the ?softconjunctive?
semantics.Token or character is the most straightforwardbasic term to be used to represent an instance.For WSI, in many cases a term is a meaningfulunit with little ambiguity even withoutconsidering context.
In this case the bag-of-termsrepresentation is in fact a bag-of-words, thereforeN-gram model can be used to exploit suchmeaningful units.
An n-gram is a sequence of nconsecutive characters (or tokens) in an instance.The advantages of n-grams are: they arelanguage independent, robust against errors ininstance, and they capture information aboutphrases.
We performed experiments to show thatfor WSI, n-gram features perform significantlybetter than the flat features.There exists many approaches to weightfeatures in text computing (Aas and Eikvil, 1999).A simple approach is TF (term frequency) usingthe frequency of the word in the document.
Theschemes take into account the frequency of theword throughout all documents in the collection.A well known variant of TF measure is TF-IDFweighting which assigns the weight to word i indocument k in proportion to the number ofoccurrences of the word in the document, and ininverse proportion to the number of documentsin the collection for which the word occurs atleast once.
*log( )ik ikiNa fn=Another approach is Entropy weighting,Entropy weighting is based on informationtheoretic ideas and is the most sophisticatedweighting scheme.
It has been proved moreeffective than word frequency weighting in textrepresenting.
In the entropy weighting scheme,the weight for word  in document  is givenby ai kik.11log( 1.0)* 1 log( )log( )Nij ijik ikj i if fa fN n n=?
?= + +?
??
??
???
??
??
?Re-parameterization is the process ofconstructing new features as combinations ortransformations of the original features.
Weinvestigated Latent Semantic Indexing (LSI)method in our research and produce aterm-document matrix for each target word.
LSIis based on the assumption that there is someunderlying or latent structure in the pattern ofword usage across documents, and that statisticaltechniques can be used to estimate this structure.However, it is against the primitive goal of theLSI weighting that LSI performs slightly poorercompared with the TF, TF-IDF and entropy.
Themost likely reason may is that the feature spacewe construct is far from high-dimension, whilefeature the LSI omitted may be of help forspecific sense induction.3.2 Feature SelectionA simple features election method used here isfrequency thresholding.
Instance frequency is thenumber of instance to be clustered in which aterm occurs.
We compute the instance frequencyfor each unique term in the training corpus andremove from the feature space those terms whoseinstance frequency was less than somepredetermined threshold (in our experiment, thethreshold is 5).
The basic assumption is that rareterms are either non-informative for categoryprediction, or not influential in globalperformance.
The assumption of instancefrequency threshold is more straightforward thatof LSI, and in either case, removal of rare termsreduces the dimensionality of the feature space.Improvement in cluster accuracy is also possibleif rare terms happen to be noise terms.Frequency threshold is the simplest techniquefor feature space reduction.
It easily scales tosparse data, with a computational complexityapproximately linear in the number of trainingdocuments.
However, it is usually considered anad hoc approach to improve efficiency, not aprincipled criterion for selecting predictivefeatures.
Also, frequency threshold is typicallynot used for aggressive term removal because ofa widely received assumption in informationretrieval.
That is, low instance frequency termsare assumed to be relatively informative andtherefore should not be removed aggressively.We will re-examine this assumption with respectto WSI tasks in experiments.Information gain (IG) is another featurefelection can be easily applied to clustering andfrequently employed as a term-goodnesscriterion in the field of machine learning.
Itmeasures the number of bits of informationobtained for cluster prediction by knowing thepresence or absence of a term in an instance.Since WSI should be conducted in anunsupervised fashion, that is, the labels are notprovided, the IG method can not be directly usedfor WSI task.
But IG can be used to find whichkind of features we consider in Section 3.1 aremost informative feature among all the featureset.
We take the training samples as thedevelopment data to seek for the cues of mostinformative feature.
For each unique term wecompute the information gain and selecte fromthe feature space those terms whose informationgain is more than some predetermined threshold.The computation includes the estimation of theconditional probabilities of a cluster given a termand the entropy computations in the definition.mi 1IG(t) = - ( ) log ( )i ip c p c=?mi 1( ) ( | t) log ( | t)i ip t p c p c=+ ?mi 1( ) ( | ) log ( | )i ip t p c t p c t=+ ?where t is the token under consideration, ci isthe corresponding cluster.This definition is moregeneral than the one employed in binaryclassification models.
We use the more generalform because WSI task have a feature sparseproblem, and we need to measure the goodnessof a feature selection method globally withrespect to all clusters on average.3.3 Feature combinationCombining all features selected by differentfeature set can improve the performance of aWSI system.
In the selection step, we find thefeature that best distinguishes the sense classes,and iteratively search additional features whichin combination with other chosen featuresimprove word sense class discrimination.
Thisprocess stops once the maximal evaluationcriterion is achieved.We are trying to disply an empiricalcomparison of representative featurecombination methods.
We hold that particularcluster support specific datasets; a test with suchcombination of cluster algorithm and feature setmay wrongly show a high accuracy rate unless avariety of clusterers are chosen and manystatistically different feature sets are used.
Also,as different feature selection methods have adifferent bias in selecting features, similar to thatof different clusterers, it is not fair to use certaincombinations of methods and clusterers, and tryto generalize from the results that some featureselection methods are better than others withoutconsidering the clusterer.This problem is challenging because theinstances belonging to the same sense classusually have high intraclass variability.
Toovercome the problem of variability, one strategyis to design feature combination method whichare highly invariant to the variations presentwithin the sense classes.
Invariance is animprovement, but it is clear that none of thefeature combination method will have the samediscriminative power for all clusterers.For example, features based on global windowmight perform well when instances are shot,whereas a feature weighting method for this taskshould be invariant to the all the WSI corpus.Therefore it is widely accepted that, instead ofusing a single feature type for all target words itis better to adaptively combine a set of diverseand complementary features.
In our experiment,we use several combination of features inmultiple views, that is, uni-gram/bi-gram,global/window, and tfidf/entropy ?
in order todiscriminate each combination best from allother clusters.4 ClusterThere are two main issues in designing clusterensembles: (a) the design of the individual?clusterers?
so that they form potentially anaccurate ensemble, and (b) the way the outputsof the clusterers are combined to obtain the finalpartition, called the consensus function.
In someensemble design methods the two issues aremerged into a single design procedure, e.g.,when one clusterer is added at a time and theoverall partition is updated accordingly (calledthe direct or greedy approach).In this task we consider the two tasksseparately, and investigate three powerful clustermethods and corresponding consensus functions.4.1 EM algorithmExpectation-maximization algorithm, or EMalgorithm (Dempster et al, 1977; McLachlan andKrishnan, 1997) is an elegant and powerfulmethod for finding maximum likelihoodsolutions for models with latent variables.Given a joint distribution  (X, Z | )p ?
overobserved variables X and latent variables Z,governed by parameters ?
, the goal is tomaximize the likelihood function (X | )p ?with respect to?
.1.
Choose an initial setting for theparametersold?
;2.
E step Evaluate ; (Z | X, )oldp ?3.
M step Evaluate new?
given by; = argmax ( , )new old??
?
?
?where ( , ) (Z|X, ) ln (X,Z| )old oldzp p?
?
?
?
?=?4.
Check for convergence of either the loglikelihood or the parameter values.
If theconvergence criterion is not satisfied, then letold new?
?
?and return to step 2.4.2  K-meansK-means clustering (MacQueen, 1967) is amethod commonly used to automaticallypartition a data set into k groups.
It proceeds byselecting k initial cluster centers and theniteratively refining them as follows:1.
Each instance d is assigned to its closestcluster center.i2.
Each cluster center C is updated to be themean of its constituent instances.jThe algorithm converges when there is nofurther change in assignment of instances toclusters.
In this work, we initialize the clustersusing instances chosen at random from the dataset.
The data sets we used are composed ofnumeric feature, for numeric features, we use aEuclidean distance metric.4.3  LACDomeniconi et al(2004) proposed an LocallyAdaptive Clustering algorithm (LAC), whichdiscovers clusters in subspaces spanned bydifferent combinations of dimensions via localweightings of features.
Dimensions along whichdata are loosely correlated receive a small weight,which has the effect of elongating distancesalong that dimension.
Features along which dataare strongly correlated receive a large weight,which has the effect of constricting distancesalong that dimension.
Thus the learned weightsperform a directional local reshaping of distanceswhich allows a better separation of clusters, andtherefore the discovery of different patterns indifferent subspaces of the original input space.The clustering result of LAC depends ontwo input parameters.
The first one is common toall clustering algorithms: the number of clustersk to be discovered in the data.
The second one(called h) controls the strength of the incentive tocluster on more features.
The setting of h isparticularly difficult, since no domain knowledgefor its tuning is likely to be available.
Thus, itwould be convenient if the clustering processautomatically determined the relevant subspaces.4.4 Cluster EnsembleCluster ensembles offer a solution to challengesinherent to clustering arising from its ill-posednature.
Cluster ensembles can provide robust andstable solutions by leveraging the consensusacross multiple clustering results, whileaveraging out emergent spurious structures thatarise due to the various biases to which eachparticipating algorithm is tuned.Kuncheva et al (2006) has shown Clusterensembles to be a robust and accurate alternativeto single clustering runs.
In the work ofKuncheva et al (2006), 24 methods fordesigning cluster ensembles are compared using24 data sets, both artificial and real.
Bothdiversity within the ensemble and accuracy ofthe individual clusterers are important factors,although not straightforwardly related to theensemble accuracy.The consensus function aggregates the outputsof the Individual clusterers into a single partition.Many consensus functions use the consensusmatrix obtained from the adjacency matrices ofthe individual clusterers.
Let N be the number ofobjects in the data set.
The adjacency matrix forclusterer  is an N by N matrix with entry k( , ) 1i j =  if objects i  and j  are placed in thesame cluster by clusterer , and ( ,k ) 0i j = ,otherwise.
The overall consensus matrix, M, isthe average of the adjacency matrices of theclusterers.
Its entry gives the proportion ofclusterers which put and( , )i ji j  in the same cluster.Here the overall consensus matrix, M, can beinterpreted as similarity between the objects orthe ?data?.
It appears that the clear winner in theconsensus function ?competition?
is using theconsensus matrix as data.
Therefore, theconsensus functions used in the WSI task invokethe approach whereby the consensus matrix M isused as data (features).
Each object isrepresented by N features, i.e., the j -the featurefor object  is the ( ,  entry of M. i )i jThen we use Group-average agglomerativeclustering (GAAC) to be the consensus functionsclustering the M matrix.5 AnalysisFirst, we conducte an ideal case experiment onthe training samples provided by CIPS-SIGHAN2010, to see whether good terms can help senseclustering.
Specifically, we applied supervisedfeature selection methods to choose the bestfeature combinations driven by performanceimproving on the training features.
Then, weexecuted the word sense induction task usingfeatures under the prefered feature combinationsand compare the various clustering results outputby three individual cluster.We then designe cluster ensemble methodwith results on three clusters, distributed as Mdata consensus matrix.5.1 Soundex for FeatureWe apply feature selection and featurecombination to instances in the preprocessing ofK-means, EM and LAC.
The effectiveness of acombination method is evaluated using theperformance of the cluster algorithm on thepreprocessed WSI.
We use the standarddefinition of recall and precision as F-score(Zhao and Karypis, 2005) to evaluate theclustering result.As described in Section 3, selection methodsare included in this study, each of which uses aterm-goodness criterion threshold to achieve adesired degree from the full feature set of WSIcorpus.Table 2 shows The F-score figures for thedifferent combinations of knowledge sources andlearning algorithms for the training data set.
Thefeature columns correspond to:(i) tfidf: tf-idf weighting(ii) entro: Entropy weighting(iii) bi: bi-gram representation(iv) uni: uni-gram representation(v) global: using all the terms in theinstance(vi) winXX: using only terms in thesurrounding context, and the width ofthe window is the figure followed by.As shown in Table 2, the best averagedF-score for WSI (without combination) isobtained by global_entro by maintaining a veryconsistent result for three cluster algorithm.
Thatis, the feature weighting method will dominateFeature k-means LAC EM averagecombine_uni_bi_entro_8:2 0.817375775 0.819315654 0.811188742 0.81596combine_uni_bi_entro_9:1 0.812858111 0.817265352 0.81510355 0.815075combine_uni_bi_entro_7:3 0.805319576 0.817909374 0.819887132 0.814372combine_uni_bi_entro_1:1 0.810324177 0.81397143 0.812962625 0.812419combine_uni_bi_entro_6:4 0.806647971 0.815069965 0.810440791 0.811945combine_uni_bi_entro_1:9 0.810576944 0.811287122 0.813785918 0.811883combine_uni_bi_entro_4:6 0.810475113 0.810512846 0.811584054 0.810857combine_uni_bi_entro_3:7 0.809265111 0.811142052 0.811340668 0.810582combine_uni_bi_entro_2:8 0.811090379 0.804433939 0.813767918 0.809764uni_global_entro 0.765063808 0.75954835 0.746212504 0.756942uni_global_tfidf 0.765011785 0.757537564 0.745006996 0.755852uni_win30_tfidf 0.764949578 0.757424304 0.744497086 0.755624uni_win40_tfidf 0.764772672 0.755702292 0.744319609 0.754932uni_win30_entro 0.764286757 0.755514592 0.742825875 0.754209uni_win40_tfidf 0.763994795 0.75954835 0.742747114 0.75543bi_global_entro 0.740026161 0.731310077 0.71651859 0.729285bi_global_tfidf 0.739555095 0.731264758 0.716031966 0.728951bi_win30_entro 0.737209909 0.729711844 0.714498518 0.72714bi_win40_entro 0.715230191 0.713987571 0.699644178 0.709621bi_win40_tfidf 0.714031488 0.710282928 0.697201196 0.707172bi_win30_ tfidf 0.740026161 0.731310077 0.71651859 0.729285Table 1: Feature selection for our system.the F-score.
On the other hand, we shouldcombine uni_global_entro and bi_global_entro toimprove the cluster performance:(vii) combine: combining all two feature(uni and bi) with the at the rate ofthe ratio followed by.From these figures, we found the followingpoints.
First, feature selection can improve theclustering performance when a certain terms arecombined.
For example, any feature combinationmethods can achieve about 5% improvement.Second, as can be seen from Table 1, the bestperformances yielded at the combination ratio of8:2.
As can be seen, when more bi-gram termsare added, the performances of combinationmethods drop obviously.
In order to find out thereason, we compared the terms selected atdifferent ratio.
After analysis, we found thatChinese word senses have their owncharacteristics, unigram language model issuitable for WSI in Chinese; also, in WSI task,informative term may be in the entire instancebut not appear closest to the target word, thelanguage model and the width of window ismuch more important than the feature weightingfor feature selection.
Since entropy weightingperform better than tf-idf weighting, tf-idfweighting can be removed with an improvementin clustering performance on the training dataset.Hence, it is obvious that combination methodsare much better than single feature set whenprocessing WSI, and we chosecombine_uni_bi_entro_8:2, i.e., the top 80%uni-gram features and top 20% features as thefinal clustering features.5.2 The cluster ensemblesAs described in Section 5.1, we use two languagemodels (uni-gram and bi-gram), 4 types of thecontext window (20, 30, 40 and global) and 2feature weighting methods (tf-idf and entropy),also, 10 combined feature set and 3 clusteralgorithm is introduced; in the other word, wehave at least 78 result, that is 78 consensusmatrix interpreted as ?data?
to be aggregated.Thus we can evaluate the statistical significanceof the difference among any ensemble methodson any cluster result set.To compare all ensemble methods, we groupthe result sets (out of 78) into different featurerepresentation scheme.
Significant difference fora given feature representation methods, theensemble result is observed to check weathercluster ensembles can be more accurate thansingle feature set and to find out which methodappears to be the best choice for the WSI task.Table 2 shows the ensembles examined in ourexperiment.
The feature columns correspond todifferent group of result set, for example, bi_tfidfindicates bi-gram model and tf-idf featureweighting methods are selected, all the 3 clusterresults on win20, win30, win 40 and globalfeature sets (12 consensus matrix) are aggregated;complex_entro indicates that all the featurerepresentation methods selecting entropyweighting are chosen.Results show that the best performance is thegroup in which all the outputs of all theclusterers are combined (the top row in Table 2).Feature F1-score Scalecomplex 0.827566232 78complex_entro 0.823006644 24complex_nocomb 0.822970703 48complex_global 0.821960768 12uni_complex 0.821931155 24uni_ entro 0.821931155 15uni_global 0.821817211 6complex_combine 0.819456935 30uni_ tfidf 0.811631894 12complex_tfidf 0.806807226 24complex_entro 0.806063712 24bi_complex 0.801211134 24bi_entro 0.794939656 12bi_global 0.788673134 6bi_tfidf 0.788170215 12Table 2: Ensemble designs sorted by the totalindex of performance5.3 CIPS-SIGHAN WSI PerformanceThe goal of this task is to promote the exchangeof ideas among participants and improve theperformance of Chinese WSI systems.
The inputconsists of 100 target words, each target wordhaving a set of contexts where the word appears.The goal is to automatically induce the senseseach word has, and cluster the contextsaccordingly.
The evaluation measures providedis F-Score measure.
In order to improve theoverall performance, we used two techniques:feature combination and Cluster Ensemble.We chose combinomg global size of window,entropy weighting, uni-garm and bi-gram at theratio of 8:2 as the final feature extraction method.Three powerful cluster algorithms, EM, K-meansand LAC recieve these features as input, and inour main system all the outputs of all theclusterers are combined to process clusterensemble.
In Table 3 we show four resultsobtained by three individual clusters and oneensemble of them.Our main system has outperformed the othersystems achieving 79.33%.
Performance forLAC is 78.95%, 0.4% lower the best system.
ForEM our F-sore is 78.55%, which is around 0.8%lower than the best system, the similar result iaalso observed for K-means.
The results of oursystem are ranked in the top 4 place andobviously better the other systems.Name F1-score RankBUPT_mainsys 0.7933 1BUPT_LAC 0.7895 2BUPT_EM  0.7855 3BUPT_kmeans 0.7849 4Table 3: Evaluation (F-score performance)6 ConclusionsIn this paper, we described the implementation ofour systems that participated in word senseinduction task at CIPS-SIGHAN-2010 bakeoff.Our ensemble model achieved 79.33% in F-score,78.95% for LAC, 78.55% for EM and 78.49%for K-means.
The result proved that our systemhad the ability to fully exploit the informativefeature in senses and the ensemble clustersenhance this advantage.One direction of future work is to exploit moresemantic cues for word sense distribution.Furthermore, in order to represent the shortcontext of the target word, we should investigatemore powerful model and external knowledge toexpand its linguistic environments.AcknowledgementThis research has been partially supported by theNational Science Foundation of China (NO.NSFC90920006).
We also thank Xiaojie Wang,Caixia Yuan and Huixing Jiang for usefuldiscussion of this work.ReferencesD.
Pinto, P. Rosso, and H. Jim?enez-Salazar.
UPV-SI:Word sense induction using self term expansion.
InProc.
of the 4th International Workshop onSemantic Evaluations - SemEval 2007.
Associationfor Computational Linguistics, 2007. pp.
430-433.Yiming Yang and Jan O. Pedersen.
A ComparativeStudy on Feature Selection in Text Categorization.In Proceedings of the 14th InternationalConference on Machine Learning (ICML), 1997.pp.
412-420.Salton, Gerard, and Chris Buckley.
1987.
Termweighting approaches in automatic text retrieval.Technical report, Cornell University, Ithaca, NY,USA.S.
Dumais, Improving the retrieval of informationfrom external sources, Behavior Research Methods,Instruments, & Computers, 1991, 23:229-236.M.
W. Berry, S. T. Dumais, and G. W. O'Brien, Usinglinear algebra for intelligent information retrieval,SIAM Rev., 1995, 37:573-595MacQueen, J.
B.
(1967).
Some methods forclassification and analysis of multivariateobservations.
Proceedings of the Fifth Symposiumon Math, Statistics, and Probability, Berkeley, CA:University of California Press.
pp.
281-297.Dempster,A.P., Laird,N.M and Rubin,D.B.
(1977)Maximum likelihood from incomplete data via theEM algorithm.
J. Roy.
Statist.
Soc.
B, 39, 1-38.MCLACHLAN, G., AND KRISHNAN, T. 1997.
TheEM algorithm and extensions.
Wiley series inprobability and statistics.
JohnWiley & Sons.Y.
Zhao and G. Karypis.
2002.
Evaluation ofhierarchical clustering algorithms for documentdatasets.
In Proceedings of the 11th Conference ofInformation and Knowledge Management (CIKM),pp.
515-524.K.
Aas and L. Eikvil.
Text categorisation: A survey.Technical Report 941, Norwegian ComputingCenter, June 1999.C.
Domeniconi, D. Papadopoulos, D. Gunopulos, andS.
Ma.
Subspace clustering of high dimensionaldata.
SIAM International Conference on DataMining, 2004.Kuncheva, L.I., Hadjitodorov, S.T., and Todorova,L.P.
Experimental Comparison of ClusterEnsemble Methods, The 9th InternationalConference on Information Fusion, 2006.
