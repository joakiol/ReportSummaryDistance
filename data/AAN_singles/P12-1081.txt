Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 768?776,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAttacking Parsing Bottlenecks with Unlabeled Data and RelevantFactorizationsEmily PitlerComputer and Information ScienceUniversity of PennsylvaniaPhiladelphia, PA 19104epitler@seas.upenn.eduAbstractPrepositions and conjunctions are two ofthe largest remaining bottlenecks in parsing.Across various existing parsers, these twocategories have the lowest accuracies, andmistakes made have consequences for down-stream applications.
Prepositions and con-junctions are often assumed to depend on lex-ical dependencies for correct resolution.
Aslexical statistics based on the training set onlyare sparse, unlabeled data can help amelio-rate this sparsity problem.
By including un-labeled data features into a factorization ofthe problem which matches the representationof prepositions and conjunctions, we achievea new state-of-the-art for English dependen-cies with 93.55% correct attachments on thecurrent standard.
Furthermore, conjunctionsare attached with an accuracy of 90.8%, andprepositions with an accuracy of 87.4%.1 IntroductionPrepositions and conjunctions are two large remain-ing bottlenecks in parsing.
Across various exist-ing parsers, these two categories have the lowestaccuracies, and mistakes made on these have con-sequences for downstream applications.
Machinetranslation is sensitive to parsing errors involvingprepositions and conjunctions, because in some lan-guages different attachment decisions in the parseof the source language sentence produce differ-ent translations.
Preposition attachment mistakesare particularly bad when translating into Japanese(Schwartz et al, 2003) which uses a different post-position for different attachments; conjunction mis-takes can cause word ordering mistakes when trans-lating into Chinese (Huang, 1983).Prepositions and conjunctions are often assumedto depend on lexical dependencies for correct resolu-tion (Jurafsky and Martin, 2008).
However, lexicalstatistics based on the training set only are typicallysparse and have only a small effect on overall pars-ing performance (Gildea, 2001).
Unlabeled data canhelp ameliorate this sparsity problem.
Backing offto cluster membership features (Koo et al, 2008) orby using association statistics from a larger corpus,such as the web (Bansal and Klein, 2011; Zhou etal., 2011), have both improved parsing.Unlabeled data has been shown to improve the ac-curacy of conjunctions within complex noun phrases(Pitler et al, 2010; Bergsma et al, 2011).
How-ever, it has so far been less effective within fullparsing ?
while first-order web-scale counts notice-ably improved overall parsing in Bansal and Klein(2011), the accuracy on conjunctions actually de-creased when the web-scale features were added(Table 4 in that paper).In this paper we show that unlabeled data can helpprepositions and conjunctions, provided that the de-pendency representation is compatible with how theparsing problem is decomposed for learning and in-ference.
By incorporating unlabeled data into factor-izations which capture the relevant dependencies forprepositions and conjunctions, we produce a parserfor English which has an unlabeled attachment ac-curacy of 93.5%, over an 18% reduction in errorover the best previously published parser (Bansaland Klein, 2011) on the current standard for depen-dency parsing.
The best model for conjunctions at-768taches them with 90.8% accuracy (42.5% reductionin error over MSTParser), and the best model forprepositions with 87.4% accuracy (18.2% reductionin error over MSTParser).We describe the dependency representations ofprepositions and conjunctions in Section 2.
We dis-cuss the implications of these representations forhow learning and inference for parsing are decom-posed (Section 3) and how unlabeled data may beused (Section 4).
We then present experiments ex-ploring the connection between representation, fac-torization, and unlabeled data in Sections 5 and 6.2 Dependency RepresentationsA dependency tree is a rooted, directed tree (or ar-borescence), in which the vertices are the words inthe sentence plus an artificial root node, and eachedge (h,m) represents a directed dependency rela-tion from the head h to the modifier m. Through-out this section, we will use Y to denote a particularparse tree, and (h,m) ?
Y to denote a particularedge in Y .The Wall Street Journal Penn Treebank (PTB)(Marcus et al, 1993) contains parsed constituencytrees (where each sentence is represented as acontext-free-grammar derivation).
Dependencyparsing requires a conversion from these con-stituency trees to dependency trees.
The Tree-bank constituency trees left noun phrases (NPs)flat, although there have been subsequent projectswhich annotate the internal structure of noun phrases(Vadas and Curran, 2007; Weischedel et al, 2011).The presence or absence of these noun phrase in-ternal annotations interacts with constituency-to-dependency conversion program in ways which haveeffects on conjunctions and prepositions.We consider two such mapping regimes here:1.
PTB trees ?
Penn2Malt1 ?
Dependencies2.
PTB trees patched with NP-internal annota-tions (Vadas and Curran, 2007) ?
penncon-verter2 ?
Dependencies1http://w3.msi.vxu.se/?nivre/research/Penn2Malt.html2Johansson and Nugues (2007) http://nlp.cs.lth.se/software/treebank_converter/Regime (1) is very commonly done in paperswhich report dependency parsing experiments (e.g.,(McDonald and Pereira, 2006; Nivre et al, 2007;Zhang and Clark, 2008; Huang and Sagae, 2010;Koo and Collins, 2010)).
Penn2Malt uses the headfinding table from Yamada and Matsumoto (2003).Regime (2) is based on the recommendations ofthe two converter tools; as of the date of this writing,the Penn2Malt website says: ?Penn2Malt has beensuperseded by the more sophisticated pennconverter,which we strongly recommend?.
The pennconverterwebsite ?strongly recommends?
patching the Tree-bank with the NP annotations of Vadas and Curran(2007).
A version of pennconverter was used to pre-pare the data for the CoNLL Shared Tasks of 2007-2009, so the trees produced by Regime 2 are similar(but not identical)3 to these shared tasks.
As far aswe are aware, Bansal and Klein (2011) is the onlypublished work which uses both steps in Regime (2).The dependency representations produced byRegime 2 are designed to be more useful for ex-tracting semantics (Johansson and Nugues, 2007).The parsing attachment accuracy of MALTPARSER(Nivre et al, 2007) was lower using pennconverterthan Penn2Malt, but using the output of MALT-PARSER under the new format parses produces amuch better semantic role labeler than using its out-put with Penn2Malt (Johansson and Nugues, 2007).Figures 1 and 2 show how conjunctions andprepositions, respectively, are represented after thetwo different conversion processes.
These differ-ences are not rare?70.7% of conjunctions and 5.2%of prepositions in the development set have a differ-ent parent under the two conversion types.
Theserepresentational differences have serious implica-tions for how well various factorizations will be ableto capture these two phenomena.3 Implications of Representations on theScope of FactorizationParsing requires a) learning to score potential parsetrees, and b) given a particular scoring function,finding the highest scoring tree according to thatfunction.
The number of potential trees for a sen-3The CoNLL data does not include the NP annotations; itdoes include annotations of named entities (Weischedel andBrunstein, 2005) so had some internal NP edges.769Conversion 1 Conversion 2Committeethe HouseWaysand Means(a)Committeethe HouseWaysandMeans(b)debtnotes and other(c)notesanddebtother(d)sellor merge 600 by(e)sellormerge600 by(f)Figure 1: Examples of conjunctions: the House Waysand Means Committee, notes and other debt, and sell ormerge 600 by.
The conjunction is bolded, the left con-junct (in the linear order of the sentence) is underlined,and the right conjunct is italicized.tence is exponential, so parsing is made tractable bydecomposing the problem into a set of local sub-structures which can be combined using dynamicprogramming.
Four possible factorizations are: sin-gle edges (edge-based), pairs of edges which sharea parent (siblings), pairs of edges where the childof one is the parent of the other (grandparents), andtriples of edges where the child of one is the parentof two others (grandparent+sibling).
In this section,we discuss these factorizations and their relevanceto conjunction and preposition representations.3.1 Edge-based ScoringOne possible factorization corresponds to first-orderparsing, in which the score of a parse tree Y decom-poses completely across the edges in the tree:S(Y ) =?
(h,m)?YS(h,m) (1)Conversion 1 Conversion 2planinlaw(a)planinlaw(b)yesterdayopening oftradinghere(c)openingoftradinghere yesterday(d)whoseplansforissues(e)planswhoseforissues(f)Figure 2: Examples of prepositions: plan in the S&Lbailout law, opening of trading here yesterday, and whoseplans for major rights issues.
The preposition is boldedand the (semantic) head is underlined.Conjunctions: Under Conversion 1, we can seethree different representations of conjunctions inFigures 1(a), 1(c), and 1(e).
Under edge-based scor-ing, the conjunction would be scored along with nei-ther of its conjuncts in 1(a).
In Figure 1(c), the con-junction is scored along with its right conjunct only;in figure 1(e) along with its left conjunct only.
Theinconsistency here is likely to make learning moredifficult, as what is learned is split across these threecases.
Furthermore, the conjunction is connectedwith an edge to either zero or one of its two argu-ments; at least one of the arguments is completelyignored in terms of scoring the conjunction.In Figures 1(c) and 1(e), the words being con-joined are connected to each other by an edge.
Thisoverloads the meaning of an edge; an edge indicatesboth a head-modifier relationship and a conjunctionrelationship.
For example, compare the two naturalphrases dogs and cats and really nice.
dogs and catsare a good pair to conjoin, but cats is not a goodmodifier for dogs, so there is a tension when scoringan edge like (dogs, cats): it should get a high score770when actually indicating a conjunction and low oth-erwise.
(nice, really) shows the opposite pattern?really is a good modifier for nice, but nice and re-ally are not two words which should be conjoined.This may be partially compensated for by includingfeatures about the surrounding words (McDonald etal., 2005), but any feature templates which would beidentical across the two contexts will be in tension.In Figures 1(b), 1(d) and 1(f), the conjunction par-ticipates in a directed edge with each of the con-juncts.
Thus, in edge-based scoring, at least underConversion 2 neither of the conjuncts is being ig-nored; however, the factorization scores each edgeindependently, so how compatible these two con-juncts are with each other cannot be included in thescoring of a tree.Prepositions: For all of the examples in Figure 2,there is a directed edge from the head of the phrasethat the preposition modifies to the preposition.
Dif-ferences in head finding rules account for the dif-ferences in preposition representations.
In the sec-ond example, the first conversion scheme choosesyesterday as the head of the overall NP, resulting inthe edge yesterday?
of, while the second conver-sion scheme ignores temporal phrases when findingthe head, resulting in the more semantically mean-ingful opening?of.
Similarly, in the third example,the preposition for attaches to the pronoun whose inthe first conversion scheme, while it attaches to thenoun plans in the second.With edge-based scoring, the object is not acces-sible when scoring where the preposition should at-tach, and PP-attachment is known to depend on theobject of the preposition (Hindle and Rooth, 1993).3.2 Sibling ScoringAnother alternative factorization is to score sib-lings as well as parent-child edges (McDonald andPereira, 2006).
Scores decompose as:S(Y ) =????
(h,m, s) (h,m) ?
Y, (h, s) ?
Y,(m, s) ?
Sib(Y )??
?S(h,m, s) (2)where Sib(Y ) is the set containing ordered and ad-jacent sibling pairs in Y : if (m, s) ?
Sib(Y ), theremust exist a shared parent h such that (h,m) ?
Yand (h, s) ?
Y , m and s must be on the same sideof h, m must be closer to h than s in the linear orderof the sentence, and there must not exist any otherchildren of h in between m and s.Under this factorization, two of the three ex-amples in Conversion 1 (and none of the exam-ples in Conversion 2) in Figure 1 now include theconjunction and both conjuncts in the same score(Figures 1(c) and 1(e)).
The scoring for head-modifier dependencies and conjunction dependen-cies are again being overloaded: (debt, notes, and)and (debt, and, other) are both sibling parts in Fig-ure 1(c), yet only one of them represents a conjunc-tion.
The position of the conjunction in the siblingis not enough to determine whether one is scoring atrue conjunction relation or just the conjunction anda different sibling; in 1(c) the conjunction is on theright of its sibling argument, while in 1(e) the con-junction is on the left.For none of the other preposition or conjunc-tion examples does a sibling factorization bringmore of the arguments into the scope of what isscored along with the preposition/conjunction.
Sib-ling scoring may have some benefit in that preposi-tions/conjunctions should have only one argument,so for prepositions (under both conversions) andconjunctions (under Conversion 2), the model canlearn to disprefer the existence of any siblings andthus enforce choosing a single child.3.3 Grandparent ScoringAnother alternative over pairs of edges scores grand-parents instead of siblings, with factorization:S(Y ) =?
{(h,m, c) (h,m) ?
Y, (m, c) ?
Y}S(h,m, c) (3)Under Conversion 2, we would expect this fac-torization to perform much better on conjunctionsand prepositions than edge-based or sibling-basedfactorizations.
Both conjunctions and prepositionsare consistently represented by exactly one grand-parent relation (with one relevant argument as thegrandparent, the preposition/conjunction as the par-ent, and the other argument as the child), so this isthe first factorization that has allowed the compati-bility of the two arguments to affect the attachmentof the preposition/conjunction.Under Conversion 1, this factorization is particu-larly appropriate for prepositions, but would be un-likely to help conjunctions, which have no children.7713.4 Grandparent-Sibling ScoringA further widening of the factorization takes grand-parents and siblings simultaneously:S(Y ) =????
(g, h,m, s) (g, h) ?
Y, (h,m) ?
Y,(h, s) ?
Y, (m, s) ?
Sib(Y )??
?S(g, h,m, s) (4)For projective parsing, dynamic programming forthis factorization was derived in Koo and Collins(2010) (Model 1 in that paper), and for non-projective parsing, dual decomposition was used forthis factorization in Koo et al (2010).This factorization should combine all the ben-efits of the sibling and grandparent factorizationsdescribed above?for Conversion 1, sibling scoringmay help conjunctions and grandparent scoring mayhelp prepositions, and for Conversion 2, grandparentscoring should help both, while sibling scoring mayor may not add some additional gains.4 Using Unlabeled Data EffectivelyAssociations from unlabeled data have the poten-tial to improve both conjunctions and prepositions.We predict that web counts which include both con-juncts (for conjunctions), or which include both theattachment site and the object of a preposition (forprepositions) will lead to the largest improvements.For the phrase dogs and cats, edge-based countswould measure the associations between dogs andand, and and and cats, but never any web countsthat include both dogs and cats.
For the phrase atespaghetti with a fork, edge-based scoring would notuse any web counts involving both ate and fork.We use associations rather than raw counts.
Thephrases trading and transacting versus trading andwhat provide an example of the difference betweenassociations and counts.
The phrase trading andwhat has a higher count than the phrase trading andtransacting, but trading and transacting are morehighly associated.
In this paper, we use point-wisemutual information (PMI) to measure the strength ofassociations of words participating in potential con-junctions or prepositions.4 For three words h, m, c,this is calculated with:PMI(h,m, c) = logP (h .
* m .
* c)P (h)P (m)P (c)(5)4PMI can be unreliable when frequency counts are small(Church and Hanks, 1990), however the data used was thresh-olded, so all counts used are at least 10.The probabilities are estimated using web-scalen-gram counts, which are looked up using thetools and web-scale n-grams described in Lin et al(2010).
Defining the joint probability using wild-cards (rather than the exact sequence h m c) iscrucially important, as determiners, adjectives, andother words may naturally intervene between thewords of interest.Approaches which cluster words (i.e., Koo etal.
(2008)) are also designed to identify wordswhich are semantically related.
As manually labeledparsed data is sparse, this may help generalize acrosssimilar words.
However, if edges are not connectedto the semantic head, cluster-based methods may beless effective.
For example, the choice of yesterdayas the head of opening of trading here yesterday inFigure 2(c) or whose in 2(e) may make cluster-basedfeatures less useful than if the semantic heads werechosen (opening and plans, respectively).5 ExperimentsThe previous section motivated the use of unlabeleddata for attaching prepositions and conjunctions.
Wehave also hypothesized that these features will bemost effective when the data representation and thelearning representation both capture relevant prop-erties of prepositions and conjunctions.
We predictthat Conversion 2 and a factorization which includesgrand-parent scoring will achieve the highest perfor-mance.
In this section, we investigate the impactof unlabeled data on parsing accuracy using the twoconversions and using each of the factorizations de-scribed in Section 3.1-3.4.5.1 Unlabeled Data Feature SetClusters: We replicate the cluster-based featuresfrom Koo et al (2008), which includes features overall edges (h,m), grand-parent triples (h,m, c), andparent sibling triples (h,m, s).
The features wereall derived from the publicly available clusters pro-duced by running the Brown clustering algorithm(Brown et al, 1992) over the BLLIP corpus with thePenn Treebank sentences excluded.5Preposition and conjunction-inspired features(motivated by Section 4) are described below:5people.csail.mit.edu/maestro/papers/bllip-clusters.gz772Web Counts: For each set of words of interest, wecompute the PMI between the words, and then in-clude binary features for whether the mutual infor-mation is undefined, if it is negative, and whether itis greater than each positive integer.For conjunctions, we only do this for triples ofboth conjunct and the conjunction (and if the con-junction is and or or and the two potential conjunctsare the same coarse grained part-of-speech).
Forprepositions, we consider only cases in which theparent is a noun or a verb and the child is a noun(this corresponds to the cases considered by Hindleand Rooth (1993) and others).
Prepositions use as-sociation features to score both the triple (parent,preposition, child) and all pairs within that triple.The counts features are not used if all the words in-volved are stopwords.
For the scope of this paper weuse only the above counts related to prepositions andconjunctions.5.2 ParserWe use the Model 1 version of dpo3, a state-of-the-art third-order dependency parser (Koo and Collins,2010))6.
We augment the feature set used with theweb-counts-based features relevant to prepositionsand conjunctions and the cluster-based features.
Theonly other change to the parser?s existing feature setwas the addition of binary features for the part-of-speech tag of the child of the root node, alone andconjoined with the tags of its children.
For furtherdetails about the parser, see Koo and Collins (2010).5.3 Experimental Set-upTraining was done on Section 2-21 of the PennTreebank.
Section 22 was used for development,and Section 23 for test.
We use automatic part-of-speech tags for both training and testing (Rat-naparkhi, 1996).
The set of potential edges waspruned using the marginals produced by a first-orderparser trained using exponentiated gradient descent(Collins et al, 2008) as in Koo and Collins (2010).We train the full parser for 15 iterations of averagedperceptron training (Collins, 2002), choose the itera-tion with the best unlabeled attachment score (UAS)on the development set, and apply the model afterthat iteration to the test set.6http://groups.csail.mit.edu/nlp/dpo3/We also ran MSTParser (McDonald and Pereira,2006), the Berkeley constituency parser (Petrov andKlein, 2007), and the unmodified dpo3 Model 1(Koo and Collins, 2010) using Conversion 2 (thecurrent recommendations) for comparison.
Sincethe converted Penn Treebank now contains a fewnon-projective sentences, we ran both the projectiveand non-projective versions of the second order (sib-ling) MSTParser.
The Berkeley parser was trainedon the constituency trees of the PTB patched withVadas and Curran (2007), and then the predictedparses were converted using pennconverter.6 Results and DiscussionTable 1 shows the unlabeled attachment scores,complete sentence exact match accuracies, and theaccuracies of conjunctions and prepositions underConversion 2.7 The incorporation of the unlabeleddata features (clusters and web counts) into the dpo3parser yields a significantly better parser than dpo3alone (93.54 UAS versus 93.21)8, and is more thana 1.5% improvement over MSTParser.6.1 Impact of FactorizationIn all four metrics (attachment of all non-punctuation tokens, sentence accuracy, prepositions,and conjunctions), there is no significant differencebetween the version of the parser which uses thegrandparent and sibling factorization (Grand+Sib)and the version which uses just the grandparent fac-torization (Grand).
A parser which uses only grand-parents (referred to as Model 0 in Koo and Collins(2010)) may therefore be preferable, as it containsfar fewer parameters than a third-order parser.While the grandparent factorization and the sib-ling factorization (Sib) are both ?second-order?parsers, scoring up to two edges (involving threewords) simultaneously, their results are quite dif-ferent, with the sibling factorization scoring muchworse.
This is particularly notable in the conjunc-tion case, where the sibling model is over 5% abso-lute worse in accuracy than the grandparent model.7As is standard for English dependency parsing, five punc-tuation symbols :, ,, ?, ?, and .
are excluded from the results(Yamada and Matsumoto, 2003).8If the (deprecated) Conversion 1 is used, the new featuresimprove the UAS of dpo3 from 93.04 to 93.51.773Model UAS Exact Match Conjunctions PrepositionsMSTParser (proj) 91.96 38.9 84.0 84.2MSTParser (non-proj) 91.98 38.7 83.8 84.6Berkeley (converted) 90.98 36.0 85.6 84.3dpo3 (Grand+Sib) 93.21 44.8 89.6 86.9dpo3+Unlabeled (Edges) 93.12 43.6 85.3 87.0dpo3+Unlabeled (Sib) 93.15 43.7 85.5 86.8dpo3+Unlabeled (Grand) 93.55 46.1 90.6 87.5dpo3+Unlabeled (Grand+Sib) 93.54 46.0 90.8 87.4- Clusters 93.10 45.0 90.5 87.5- Prep,Conj Counts 93.52 45.8 89.9 87.1Table 1: Test set accuracies under Conversion 2 of unlabeled attachment scores, complete sentence exact match accu-racies, conjunction accuracy, and preposition accuracy.
Bolded items are the best in each column, or not significantlydifferent from the best in that column (sign test, p < .05).6.2 Impact of Unlabeled DataThe unlabeled data features improved the alreadystate-of-the-art dpo3 parser in UAS, complete sen-tence accuracy, conjunctions, and prepositions.However, because the sample sizes are much smallerfor the latter three cases, only the UAS improvementis statistically significant.9 Overall, the results in Ta-ble 1 show that while the inclusion of unlabeled dataimproves parser performance, increasing the size offactorization matters even more.
Ablation experi-ments showed that cluster features have a larger im-pact on overall UAS, while count features have alarger impact on prepositions and conjunctions.6.3 Comparison with Other ParsersThe resulting dpo3+Unlabeled parser is significantlybetter than both versions of MSTParser and theBerkeley parser converted to dependencies across allfour evaluations.
dpo3+Unlabeled has an UAS 1.5%higher than MSTParser, which has an UAS 1.0%higher than the converted constituency parser.
TheMSTParser uses sibling scoring, so it is unsurpris-ing that it performs less well on the new conversion.While the converted constituency parser is notas good on dependencies as MSTParser overall,note that it is over a percent and a half better thanMSTParser on attaching conjunctions (85.6% versus84.0%).
Conjunction scope may benefit from paral-lelism and higher-level structure, which is easily ac-cessible when joining two matching non-terminals9There are 52,308 non-punctuation tokens in the test set,compared with 2416 sentences, 1373 conjunctions, and 5854prepositions.in a context-free grammar, but much harder todetermine in the local views of graph-based de-pendency parsers.
The dependencies arising fromthe Berkeley constituency trees have higher con-junction accuracies than either the edge-based orsibling-based dpo3+Unlabeled parser.
However,once grandparents are included in the factorization,the dpo3+Unlabeled is significantly better at attach-ing conjunctions than the constituency parser, at-taching conjunctions with an accuracy over 90%.Therefore, some of the disadvantages of dependencyparsing compared with constituency parsing can becompensated for with larger factorizations.ConjunctionsConversion 1 Conversion 2Scoring (deprecated)Edge 86.3 85.3Sib 87.8 85.5Grand 87.2 90.6Grand+Sib 88.3 90.8Table 2: Unlabeled attachment accuracy for conjunc-tions.
Bolded items are the best in each column, or notsignificantly different (sign test, p < .05).6.4 Impact of Data RepresentationTables 2 and 3 show the results of thedpo3+Unlabeled parser for conjunctions andprepositions, respectively, under the two differentconversions.
The data representation has an impacton which factorizations perform best.
UnderConversion 1, conjunctions are more accurate undera sibling parser than a grandparent parser, while the774PrepositionsConversion 1 Conversion 2Scoring (deprecated)Edge 87.4 87.0Sib 87.5 86.8Grand 87.9 87.5Grand+Sib 88.4 87.4Table 3: Unlabeled attachment accuracy for prepositions.Bolded items are the best in each column, or not signifi-cantly different (sign test, p < .05).pattern is reversed for Conversion 2.Conjunctions show a much stronger need forhigher order factorizations than prepositions do.This is not too surprising, as prepositions have moreof a selectional preference than conjunctions, andso the preposition itself is more informative aboutwhere it should attach.
While prepositions do im-prove with larger factorizations, the improvementbeyond edge-based is not significant for Conversion2.
One hypothesis for why Conversion 1 shows moreof an improvement is that the wider scope leads tothe semantic head being included; in Conversion2, the semantic head is chosen as the parent of thepreposition, so the wider scope is less necessary.6.5 Preposition Error AnalysisPrepositions are still the largest source of errors inthe dpo3+Unlabeled parser.
We therefore analyzethe errors made on the development set to determinewhether the difficult remaining cases for parsers cor-respond to the Hindle and Rooth (1993) style PP-attachment classification task.
In the PP-attachmentclassification task, the two choices for where thepreposition attaches are the previous verb or the pre-vious noun, and the preposition itself has a noun ob-ject.
The ones that do attach to the preceeding nounor verb (not necessarily the preceeding word) andhave a noun object (2323 prepositions) are attachedby the dpo3+Unlabeled grandparent-scoring parserwith 92.4% accuracy, while those that do not fit thatcategorization (1703 prepositions) have the correctparent only 82.7% of the time.Local attachments are more accurate ?
preposi-tions are attached with 94.8% accuracy if the correctparent is the immediately preceeding word (2364cases) and only 79.1% accuracy if it is not (1662cases).
The preference is not necessarily for lowattachments though: the prepositions whose parentis not the preceeding word are attached more accu-rately if the parent is the root word (usually corre-sponding to the main verb) of the sentence (90.8%,587 cases) than if the parent is lower in the tree(72.7%, 1075 cases).7 ConclusionFeatures derived from unlabeled data (clusters andweb counts) significantly improve a state-of-the-artdependency parser for English.
We showed howwell various factorizations are able to take advantageof these unlabeled data features, focusing our anal-ysis on conjunctions and prepositions.
Includinggrandparents in the factorization increases the accu-racy of conjunctions over 5% absolute over edge-based or sibling-based scoring.
The representationof the data is extremely important for how the prob-lem should be factored?under the old Penn2Malt de-pendency representation, a sibling parser was moreaccurate than a grandparent parser.
As some impor-tant relationships were represented as siblings andsome as grandparents, there was a need to developthird-order parsers which could exploit both simul-taneously (Koo and Collins, 2010).
Under the newpennconverter standard, a grandparent parser is sig-nificantly better than a sibling parser, and there is nosignificant improvement when including both.AcknowledgmentsI would like to thank Terry Koo for making the dpo3parser publically available and for his help with us-ing the parser.
I would also like to thank Mitch Mar-cus and Kenneth Church for useful discussions.
Thismaterial is based upon work supported under a Na-tional Science Foundation Graduate Research Fel-lowship.ReferencesM.
Bansal and D. Klein.
2011.
Web-scale features forfull-scale parsing.
In Proceedings of ACL, pages 693?702.S.
Bergsma, D. Yarowsky, and K. Church.
2011.
Usinglarge monolingual and bilingual corpora to improvecoordination disambiguation.
In Proceedings of ACL,pages 1346?1355.775P.F.
Brown, P.V.
Desouza, R.L.
Mercer, V.J.D.
Pietra, andJ.C.
Lai.
1992.
Class-based n-gram models of naturallanguage.
Computational linguistics, 18(4):467?479.K.W.
Church and P. Hanks.
1990.
Word associationnorms, mutual information, and lexicography.
Com-putational linguistics, 16(1):22?29.M.
Collins, A. Globerson, T. Koo, X. Carreras, and P.L.Bartlett.
2008.
Exponentiated gradient algorithmsfor conditional random fields and max-margin markovnetworks.
The Journal of Machine Learning Research,9:1775?1822.Michael Collins.
2002.
Discriminative training meth-ods for hidden Markov models: theory and experi-ments with perceptron algorithms.
In Proceedings ofEMNLP, pages 1?8.D.
Gildea.
2001.
Corpus variation and parser perfor-mance.
In Proceedings of EMNLP, pages 167?202.D.
Hindle and M. Rooth.
1993.
Structural ambigu-ity and lexical relations.
Computational Linguistics,19(1):103?120.L.
Huang and K. Sagae.
2010.
Dynamic programmingfor linear-time incremental parsing.
In Proceedings ofACL, pages 1077?1086.X.
Huang.
1983.
Dealing with conjunctions in a machinetranslation environment.
In Proceedings of EACL,pages 81?85.R.
Johansson and P. Nugues.
2007.
Extendedconstituent-to-dependency conversion for English.
InProc.
of the 16th Nordic Conference on ComputationalLinguistics (NODALIDA), pages 105?112.D.
Jurafsky and J.H.
Martin.
2008.
Speech and languageprocessing: an introduction to natural language pro-cessing, computational linguistics and speech recogni-tion.
Prentice Hall.T.
Koo and M. Collins.
2010.
Efficient third-order de-pendency parsers.
In Proceedings of ACL, pages 1?11.T.
Koo, X. Carreras, and M. Collins.
2008.
Simplesemi-supervised dependency parsing.
In Proceedingsof ACL, pages 595?603.T.
Koo, A.M.
Rush, M. Collins, T. Jaakkola, and D. Son-tag.
2010.
Dual decomposition for parsing with non-projective head automata.
In Proceedings of EMNLP,pages 1288?1298.D.
Lin, K. Church, H. Ji, S. Sekine, D. Yarowsky,S.
Bergsma, K. Patil, E. Pitler, R. Lathbury, V. Rao,et al 2010.
New tools for web-scale n-grams.
In Pro-ceedings of LREC.M.P.
Marcus, M.A.
Marcinkiewicz, and B. Santorini.1993.
Building a large annotated corpus of En-glish: The Penn Treebank.
Computational Linguistics,19(2):313?330.R.
McDonald and F. Pereira.
2006.
Online learning ofapproximate dependency parsing algorithms.
In Pro-ceedings of EACL, pages 81?88.R.
McDonald, K. Crammer, and F. Pereira.
2005.
On-line large-margin training of dependency parsers.
InProceedings of ACL, pages 91?98.J.
Nivre, J.
Hall, J. Nilsson, A. Chanev, G. Eryigit,S.
Ku?bler, S. Marinov, and E. Marsi.
2007.
Malt-parser: A language-independent system for data-driven dependency parsing.
Natural Language Engi-neering, 13(2):95?135.Slav Petrov and Dan Klein.
2007.
Improved inferencefor unlexicalized parsing.
In Proceedings of NAACL,pages 404?411.E.
Pitler, S. Bergsma, D. Lin, and K. Church.
2010.
Us-ing web-scale n-grams to improve base np parsing per-formance.
In Proceedings of the 23rd InternationalConference on Computational Linguistics, pages 886?894.A.
Ratnaparkhi.
1996.
A maximum entropy model forpart-of-speech tagging.
In Proceedings of EMNLP,pages 133?142.L.
Schwartz, T. Aikawa, and C. Quirk.
2003.
Disam-biguation of English PP attachment using multilingualaligned data.
In Proceedings of MT Summit IX.D.
Vadas and J. Curran.
2007.
Adding noun phrase struc-ture to the Penn Treebank.
In ACL, pages 240?247.R.
Weischedel and A. Brunstein.
2005.
BBN pronouncoreference and entity type corpus.
Linguistic DataConsortium, Philadelphia.R.
Weischedel, M. Palmer, M. Marcus, E. Hovy, S. Prad-han, L. Ramshaw, N. Xue, A. Taylor, J. Kaufman,M.
Franchini, et al 2011.
Ontonotes release 4.0.LDC2011T03, Philadelphia, Penn.
: Linguistic DataConsortium.H.
Yamada and Y. Matsumoto.
2003.
Statistical depen-dency analysis with support vector machines.
In Pro-ceedings of International Workshop of Parsing Tech-nologies, pages 195?206.Y.
Zhang and S. Clark.
2008.
A tale of two parsers: in-vestigating and combining graph-based and transition-based dependency parsing using beam-search.
In Pro-ceedings of EMNLP, pages 562?571.G.
Zhou, J. Zhao, K. Liu, and L. Cai.
2011.
Exploitingweb-derived selectional preference to improve statisti-cal dependency parsing.
In Proceedings of ACL, pages1556?1565.776
