Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 1?8,Sydney, July 2006. c?2006 Association for Computational LinguisticsUnsupervised Discovery of a Statistical Verb LexiconTrond Grenager and Christopher D. ManningComputer Science DepartmentStanford UniversityStanford, CA 94305{grenager, manning}@cs.stanford.eduAbstractThis paper demonstrates how unsupervised tech-niques can be used to learn models of deep linguis-tic structure.
Determining the semantic roles of averb?s dependents is an important step in naturallanguage understanding.
We present a method forlearning models of verb argument patterns directlyfrom unannotated text.
The learned models are sim-ilar to existing verb lexicons such as VerbNet andPropBank, but additionally include statistics aboutthe linkings used by each verb.
The method isbased on a structured probabilistic model of the do-main, and unsupervised learning is performed withthe EM algorithm.
The learned models can alsobe used discriminatively as semantic role labelers,and when evaluated relative to the PropBank anno-tation, the best learned model reduces 28% of theerror between an informed baseline and an oracleupper bound.1 IntroductionAn important source of ambiguity that must beresolved by any natural language understandingsystem is the mapping between syntactic depen-dents of a predicate and the semantic roles1 thatthey each express.
The ambiguity stems from thefact that each predicate can allow several alternatemappings, or linkings,2 between its semantic rolesand their syntactic realization.
For example, theverb increase can be used in two ways:(1) The Fed increased interest rates.
(2) Interest rates increased yesterday.The instances have apparently similar surface syn-tax: they both have a subject and a noun phrasedirectly following the verb.
However, while thesubject of increase expresses the agent role in thefirst, it instead expresses the patient role in the sec-ond.
Pairs of linkings such as this allowed by asingle predicate are often called diathesis alterna-tions (Levin, 1993).The current state-of-the-art approach to resolv-ing this ambiguity is to use discriminative classi-fiers, trained on hand-tagged data, to classify the1Also called thematic roles, theta roles, or deep cases.2Sometimes called frames.semantic role of each dependent (Gildea and Juraf-sky, 2002; Pradhan et al, 2005; Punyakanok et al,2005).
A drawback of this approach is that evena relatively large training corpus exhibits consid-erable sparsity of evidence.
The two main hand-tagged corpora are PropBank (Palmer et al, 2003)and FrameNet (Baker et al, 1998), the former ofwhich currently has broader coverage.
However,even PropBank, which is based on the 1M wordWSJ section of the Penn Treebank, is insufficientin quantity and genre to exhibit many things.
Aperfectly common verb like flap occurs only twice,across all morphological forms.
The first exampleis an adjectival use (flapping wings), and the sec-ond is a rare intransitive use with an agent argu-ment and a path (ducks flapping over Washington).From this data, one cannot learn the basic alterna-tion pattern for flap: the bird flapped its wings vs.the wings flapped.We propose to address the challenge of datasparsity by learning models of verb behavior di-rectly from raw unannotated text, of which thereis plenty.
This has the added advantage of be-ing easily extendible to novel text genres and lan-guages, and the possibility of shedding light onthe question of human language acquisition.
Themodels learned by our unsupervised approach pro-vide a new broad-coverage lexical resource whichgives statistics about verb behavior, informationthat may prove useful in other language process-ing tasks, such as parsing.
Moreover, they may beused discriminatively to label novel verb instancesfor semantic role.
Thus we evaluate them both interms of the verb alternations that they learn andtheir accuracy as semantic role labelers.This work bears some similarity to the sub-stantial literature on automatic subcategorizationframe acquisition (see, e.g., Manning (1993),Briscoe and Carroll (1997), and Korhonen(2002)).
However, that research is focused on ac-quiring verbs?
syntactic behavior, and we are fo-cused on the acquisition of verbs?
linking behav-ior.
More relevant is the work of McCarthy and1Relation Descriptionsubj NP preceding verbnp#n NP in the nth position following verbnp NP that is not the subject andnot immediately following verbcl#n Complement clausein the nth position following verbcl Complement clausenot immediately following verbxcl#n Complement clause without subjectin the nth position following verbxcl Complement clause without subjectnot immediately following verbacomp#n Adjectival complementin the nth position following verbacomp Adjectival complementnot immediately following verbprep x Prepositional modifierwith preposition xadvmod Adverbial modifieradvcl Adverbial clauseTable 1: The set of syntactic relations we use, where n ?
{1, 2, 3} and x is a preposition.Korhonen (1998), which used a statistical modelto identify verb alternations, relying on an existingtaxonomy of possible alternations, as well as La-pata (1999), which searched a large corpus to findevidence of two particular verb alternations.
Therehas also been some work on both clustering andsupervised classification of verbs based on theiralternation behavior (Stevenson and Merlo, 1999;Schulte im Walde, 2000; Merlo and Stevenson,2001).
Finally, Swier and Stevenson (2004) per-form unsupervised semantic role labeling by usinghand-crafted verb lexicons to replace supervisedsemantic role training data.
However, we believethis is the first system to simultaneously discoververb roles and verb linking patterns from unsuper-vised data using a unified probabilistic model.2 Learning SettingOur goal is to learn a model which relates a verb,its semantic roles, and their possible syntactic re-alizations.
As is the case with most semantic rolelabeling research, we do not attempt to model thesyntax itself, and instead assume the existence of asyntactic parse of the sentence.
The parse may befrom a human annotator, where available, or froman automatic parser.
We can easily run our systemon completely unannotated text by first runningan automatic tokenizer, part-of-speech tagger, andparser to turn the text into tokenized, tagged sen-tences with associated parse trees.In order to keep the model simple, and indepen-dent of any particular choice of syntactic represen-tation, we use an abstract representation of syn-Sentence: A deeper market plunge today couldgive them their first test.Verb: giveSyntactic Semantic HeadRelation Role Wordsubj ARG0 plunge/NNnp ARGM today/NNnp#1 ARG2 they/PRPnp#2 ARG1 test/NNv = give` = {ARG0 ?
subj, ARG1 ?
np#2ARG2 ?
np#1}o = [(ARG0, subj), (ARGM, ?
),(ARG2, np#1), (ARG1, np#2)](g1, r1, w1) = (subj,ARG0, plunge/NN)(g2, r2, w2) = (np,ARG0, today/NN)(g3, r3, w3) = (np#1, ARG2, they/PRP )(g4, r4, w4) = (np#2, ARG1, test/NN)Figure 1: An example sentence taken from the Penn Treebank(wsj 2417), the verb instance extracted from it, and the valuesof the model variables for this instance.
The semantic roleslisted are taken from the PropBank annotation, but are notobserved in the unsupervised training method.tax.
We define a small set of syntactic relations,listed in Table 1, each of which describes a possi-ble syntactic relationship between the verb and adependent.
Our goal was to choose a set that pro-vides sufficient syntactic information for the se-mantic role decision, while remaining accuratelycomputable from any reasonable parse tree usingsimple deterministic rules.
Our set does not in-clude the relations direct object or indirect object,since this distinction can not be made determin-istically on the basis of syntactic structure alone;instead, we opted to number the noun phrase (np),complement clause (cl, xcl), and adjectival com-plements (acomp) appearing in an unbroken se-quence directly after the verb, since this is suffi-cient to capture the necessary syntactic informa-tion.
The syntactic relations used in our experi-ments are computed from the typed dependenciesreturned by the Stanford Parser (Klein and Man-ning, 2003).We also must choose a representation for se-mantic roles.
We allow each verb a small fixednumber of roles, in the manner similar to Prop-Bank?s ARG0 .
.
.
ARG5.
We also designate asingle adjunct role which is shared by all verbs,similar to PropBank?s ARGM role.
We say ?sim-ilar?
because our system never observes the Prop-Bank roles (or any human annotated semanticroles) and so cannot possibly use the same names.Our system assigns arbitrary integer names to theroles it discovers, just as clustering systems give21 ?
j ?
Mv`ogj rj wjFigure 2: A graphical representation of the verb linkingmodel, with example values for each variable.
The rectangleis a plate, indicating that the model contains multiple copiesof the variables shown within it: in this case, one for eachdependent j. Variables observed during learning are shaded.arbitrary names to the clusters they discover.3Given these definitions, we convert our parsedcorpora into a simple format: a set of verb in-stances, each of which represents an occurrenceof a verb in a sentence.
A verb instance consists ofthe base form (lemma) of the observed verb, andfor each dependent of the verb, the dependent?ssyntactic relation and head word (represented asthe base form with part of speech information).
Anexample Penn Treebank sentence, and the verb in-stances extracted from it, are given in Figure 1.3 Probabilistic ModelOur learning method is based on a structured prob-abilistic model of the domain.
A graphical repre-sentation of the model is shown in Figure 2.
Themodel encodes a joint probability distribution overthe elements of a single verb instance, includingthe verb type, the particular linking, and for eachdependent of the verb, its syntactic relation to theverb, semantic role, and head word.We begin by describing the generative processto which our model corresponds, using as our run-ning example the instance of the verb give shownin Figure 1.
We begin by generating the verblemma v, in this case give.
Conditioned on the3In practice, while our system is not guaranteed to chooserole names that are consistent with PropBank, it often doesanyway, which is a consequence of the constrained form ofthe linking model.choice of verb give, we next generate a linking`, which defines both the set of core semanticroles to be expressed, as well as the syntactic re-lations that express them.
In our example, wesample the ditransitive linking ` = {ARG0 ?subj,ARG1 ?
np#2, ARG2 ?
np#1}.
Con-ditioned on this choice of linking, we next gen-erate an ordered linking o, giving a final positionin the dependent list for each role and relation inthe linking `, while also optionally inserting oneor more adjunct roles.
In our example, we gener-ate the vector o = [(ARG0, subj), (ARGM, ?
),(ARG2, np#1), (ARG1, np#2)].
In doing sowe?ve specified positions for ARG0, ARG1, andARG2 and added one adjunct role ARGM in thesecond position.
Note that the length of the or-dered linking o is equal to the total number of de-pendents M of the verb instance.
Now we iteratethrough each of the dependents 1 ?
j ?
M , gen-erating each in turn.
For the core arguments, thesemantic role rj and syntactic relation gj are com-pletely determined by the ordered linking o, so itremains only to sample the syntactic relation forthe adjunct role: here we sample g2 = np.
Wefinish by sampling the head word of each depen-dent, conditioned on the semantic role of that de-pendent.
In this example, we generate the headwords w1 = plunge/NN , w2 = today/NN ,w3 = they/NN , and w4 = test/NN .Before defining the model more formally, wepause to justify some of the choices made in de-signing the model.
First, we chose to distinguishbetween a verb?s core arguments and its adjuncts.While core arguments must be associated with asemantic role that is verb specific (such as the pa-tient role of increase: the rates in our example),adjuncts are generated by a role that is verb inde-pendent (such as the time of a generic event: lastmonth in our example).
Linkings include map-pings only for the core semantic roles, resulting ina small, focused set of possible linkings for eachverb.
A consequence of this choice is that we in-troduce uncertainty between the choice of linkingand its realization in the dependent list, which werepresent with ordered linking variable o.4We now present the model formally as a fac-tored joint probability distribution.
We factor thejoint probability distribution into a product of the4An alternative modeling choice would have been to add astate variable to each dependent, indicating which of the rolesin the linking have been ?used up?
by previous dependents.3probabilities of each instance:P(D) =N?i=1P(vi, `i, oi,gi, ri,wi)where we assume there are N instances, and wehave used the vector notation g to indicate the vec-tor of variables gj for all values of j (and similarlyfor r and w).
We then factor the probability ofeach instance using the independencies shown inFigure 2 as follows:P(v, `, o,g, r,w) =P(v)P(`|v)P(o|`)M?j=1P(gj |o)P(rj |o)P(wj |rj)where we have assumed that there are M depen-dents of this instance.
The verb v is always ob-served in our data, so we don?t need to defineP(v).
The probability of generating the linkinggiven the verb P(`|v) is a multinomial over pos-sible linkings.5 Next, the probability of a partic-ular ordering of the linking P(o|`) is determinedonly by the number of adjunct dependents that areadded to o.
One pays a constant penalty for eachadjunct that is added to the dependent list, but oth-erwise all orderings of the roles are equally likely.Formally, the ordering o is distributed accordingto the geometric distribution of the difference be-tween its length and the length of `, with constantparameter ?.6 Next, P(gj |o) and P(rj|o) are com-pletely deterministic for core roles: the syntacticrelation and semantic role for position j are speci-fied in the ordering o.
For adjunct roles, we gener-ate gj from a multinomial over syntactic relations.Finally, the word given the role P(wj |rj) is dis-tributed as a multinomial over words.To allow for labeling elements of verb instances(verb types, syntactic relations, and head words) attest time that were unobserved in the training set,we must smooth our learned distributions.
We useBayesian smoothing: all of the learned distribu-tions are multinomials, so we add psuedocounts, ageneralization of the well-known add-one smooth-ing technique.
Formally, this corresponds to aBayesian model in which the parameters of thesemultinomial distributions are themselves random5The way in which we estimate this multinomial fromdata is more complex, and is described in the next section.6While this may seem simplistic, recall that all of the im-portant ordering information is captured by the syntactic re-lations.Role Linking OperationsARG0 Add ARG0 to subjARG1 No operationAdd ARG1 to np#1Add ARG1 to cl#1Add ARG1 to xcl#1Add ARG1 to acomp#1Add ARG1 to subj, replacing ARG0ARG2 No operationAdd ARG2 to prep x, ?xAdd ARG2 to np#1, shifting ARG1 to np#2Add ARG2 to np#1, shifting ARG1 to prep withARG3 No operationAdd ARG3 to prep x, ?xAdd ARG3 to cl#n, 1 < n < 3ARG4 No operationAdd ARG4 to prep x, ?xTable 2: The set of linking construction operations.
To con-struct a linking, select one operation from each list.variables, distributed according to a Dirichlet dis-tribution.73.1 Linking ModelThe most straightforward choice of a distributionfor P(`|v) would be a multinomial over all pos-sible linkings.
There are two problems with thissimple implementation, both stemming from thefact that the space of possible linkings is large(there are O(|G+1||R|), where G is the set of syn-tactic relations and R is the set of semantic roles).First, most learning algorithms become intractablewhen they are required to represent uncertaintyover such a large space.
Second, the large spaceof linkings yields a large space of possible mod-els, making learning more difficult.As a consequence, we have two objectives whendesigning P(`|v): (1) constrain the set of linkingsfor each verb to a set of tractable size which arelinguistically plausible, and (2) facilitate the con-struction of a structured prior distribution over thisset, which gives higher weight to linkings that areknown to be more common.
Our solution is tomodel the derivation of each linking as a sequenceof construction operations, an idea which is sim-ilar in spirit to that used by Eisner (2001).
Eachoperation adds a new role to the linking, possiblyreplacing or displacing one of the existing roles.The complete list of linking operations is given inTable 2.
To build a linking we select one opera-tion from each list; the presence of a no-operationfor each role means that a linking doesn?t have toinclude all roles.
Note that this linking derivationprocess is not shown in Figure 2, since it is possi-7For a more detailed presentation of Bayesian methods,see Gelman et al (2003).4ble to compile the resulting distribution over link-ings into the simpler multinomial P(`|v).More formally, we factor P(`|v) as follows,where c is the vector of construction operationsused to build `:P(`|v) =?cP(`|c)P(c|v)=?c|R|?i=1P(ci|v)Note that in the second step we drop the termP(`|c) since it is always 1 (a sequence of opera-tions leads deterministically to a linking).Given this derivation process, it is easy to cre-ated a structured prior: we just place pseudocountson the operations that are likely a priori acrossall verbs.
We place high pseudocounts on theno-operations (which preserve simple intransitiveand transitive structure) and low pseudocounts onall the rest.
Note that the use of this structuredprior has another desired side effect: it breaks thesymmetry of the role names (because some link-ings more likely than others) which encourages themodel to adhere to canonical role naming conven-tions, at least for commonly occurring roles likeARG0 and ARG1.The design of the linking model does incorpo-rate prior knowledge about the structure of verblinkings and diathesis alternations.
Indeed, thelinking model provides a weak form of Univer-sal Grammar, encoding the kinds of linking pat-terns that are known to occur in human languages.While not fully developed as a model of cross-linguistic verb argument realization, the model isnot very English specific.
It provides a not-very-constrained theory of alternations that capturescommon cross-linguistic patterns.
Finally, thoughwe do encode knowledge in the form of the modelstructure and associated prior distributions, notethat we do not provide any verb-specific knowl-edge; this is left to the learning algorithm.4 LearningOur goal in learning is to find parameter settings ofour model which are likely given the data.
Using?
to represent the vector of all model parameters,if our data were fully observed, we could expressour learning problem as??
= argmax?P(?|D) = argmax?N?i=1P(di; ?
)= argmax?N?i=1P(vi, `i, oi,gi, ri,wi; ?
)Because of the factorization of the joint distri-bution, this learning task would be trivial, com-putable in closed form from relative frequencycounts.
Unfortunately, in our training set the vari-ables `, o and r are hidden (not observed), leavingus with a much harder optimization problem:??
= argmax?N?i=0P(vi,gi,wi; ?
)= argmax?N?i=0?`i,oi,riP(vi, `i, oi,gi, ri,wi; ?
)In other words, we want model parameters whichmaximize the expected likelihood of the observeddata, where the expectation is taken over thehidden variables for each instance.
Althoughit is intractable to find exact solutions to opti-mization problems of this form, the Expectation-Maximization (EM) algorithm is a greedy searchprocedure over the parameter space which is guar-anteed to increase the expected likelihood, andthus find a local maximum of the function.While the M-step is clearly trivial, the E-stepat first looks more complex: there are three hid-den variables for each instance, `, o, and r, each ofwhich can take an exponential number of values.Note however, that conditioned on the observedset of syntactic relations g, the variables ` and oare completely determined by a choice of roles rfor each dependent.
So to represent uncertaintyover these variables, we need only to represent adistribution over possible role vectors r. Thoughin the worst case the set of possible role vectors isstill exponential, we only need role vectors that areconsistent with both the observed list of syntacticrelations and a linking that can be generated bythe construction operations.
Empirically the num-ber of linkings is small (less than 50) for each ofthe observed instances in our data sets.Then for each instance we construct a condi-tional probability distribution over this set, which5is computable in terms of the model parameters:P(r, `r, or, |v,g,w) ?P(`r|v)P(or|`r)M?j=1P(gj |or)P(rj |or)P(wj |rj)We have denoted as `r and or the values of ` ando that are determined by each choice of r.To make EM work, there are a few additionalsubtleties.
First, because EM is a hill-climbing al-gorithm, we must initialize it to a point in parame-ter space with slope (and without symmetries).
Wedo so by adding a small amount of noise: for eachdependent of each verb, we add a fractional countof 10?6 to the word distribution of a semantic roleselected at random.
Second, we must choose whento stop EM: we run until the relative change in datalog likelihood is less than 10?4.A separate but important question is how wellEM works for finding ?good?
models in the spaceof possible parameter settings.
?Good?
models areones which list linkings for each verb that corre-spond to linguists?
judgments about verb linkingbehavior.
Recall that EM is guaranteed only tofind a local maximum of the data likelihood func-tion.
There are two reasons why a particular maxi-mum might not be a ?good?
model.
First, becauseit is a greedy procedure, EM might get stuck in lo-cal maxima, and be unable to find other points inthe space that have much higher data likelihood.We take the traditional approach to this problem,which is to use random restarts; however empir-ically there is very little variance over runs.
Adeeper problem is that data likelihood may not cor-respond well to a linguist?s assessment of modelquality.
As evidence that this is not the case, wehave observed a strong correlation between datalog likelihood and labeling accuracy.5 Datasets and EvaluationWe train our models with verb instances ex-tracted from three parsed corpora: (1) the WallStreet Journal section of the Penn Treebank (PTB),which was parsed by human annotators (Marcus etal., 1993), (2) the Brown Laboratory for Linguis-tic Information Processing corpus of Wall StreetJournal text (BLLIP), which was parsed automat-ically by the Charniak parser (Charniak, 2000),and (3) the Gigaword corpus of raw newswire text(GW), which we parsed ourselves with the Stan-ford parser.
In all cases, when training a model,Coarse Roles Core RolesSec.
23 P R F1 P R F1ID Only .957 .802 .873 .944 .843 .891CL OnlyBaseline .856 .856 .856 .975 .820 .886PTB Tr.
.889 .889 .889 .928 .898 .9111000 Tr.
.897 .897 .897 .947 .898 .920ID+CLBaseline .819 .686 .747 .920 .691 .789PTB Tr.
.851 .712 .776 .876 .757 .8121000 Tr.
.859 .719 .783 .894 .757 .820Sec.
24 P R F1 P R F1ID Only .954 .788 .863 .941 .825 .879CL OnlyBaseline .844 .844 .844 .980 .810 .882PTB Tr.
.893 .893 .893 .940 .903 .9201000 Tr.
.899 .899 .899 .956 .898 .925ID+CLBaseline .804 .665 .729 .922 .668 .775PTB Tr.
.852 .704 .771 .885 .745 .8091000 Tr.
.858 .709 .776 .900 .741 .813Table 3: Summary of results on labeling verb instancesin PropBank Section 23 and Section 24 for semantic role.Learned results are averaged over 5 runs.we specify a set of target verb types (e.g., the onesin the test set), and build a training set by adding afixed number of instances of each verb type fromthe PTB, BLLIP, and GW data sets, in that order.For the semantic role labeling evaluation, weuse our system to label the dependents of unseenverb instances for semantic role.
We use the sen-tences in PTB section 23 for testing, and PTB sec-tion 24 for development.
The development setconsists of 2507 verb instances and 833 differentverb types, and the test set consists of 4269 verbinstances and 1099 different verb types.
Free pa-rameters were tuned on the development set, andthe test set was only used for final experiments.Because we do not observe the gold standardsemantic roles at training time, we must choosean alignment between the guessed labels and thegold labels.
We do so optimistically, by choos-ing the gold label for each guessed label whichmaximizes the number of correct guesses.
This isa well known approach to evaluation in unsuper-vised learning: when it is used to compute accu-racy, the resulting metric is sometimes called clus-ter purity.
While this amounts to ?peeking?
at theanswers before evaluation, the amount of humanknowledge that is given to the system is small: itcorresponds to the effort required to hand assign a?name?
to each label that the system proposes.As is customary, we divide the problem intotwo subtasks: identification (ID) and classifica-tion (CL).
In the identification task, we identifythe set of constituents which fill some role for a6                                                                  fffi fl ffi   !
"Figure 3: Test set F1 as a function of training set size.target verb: in our system we use simple rulesto extract dependents of the target verb and theirgrammatical relations.
In the classification task,the identified constituents are labeled for their se-mantic role by the learned probabilistic model.
Wereport results on two variants of the basic classifi-cation task: coarse roles, in which all of the ad-junct roles are collapsed to a single ARGM role(Toutanova, 2005), and core roles, in which weevaluate performance on the core semantic rolesonly (thus collapsing the ARGM and unlabeledcategories).
We do not report results on the allroles task, since our current model does not distin-guish between different types of adjunct roles.
Foreach task we report precision, recall, and F1.6 ResultsThe semantic role labeling results are summarizedin Table 3.
Our performance on the identificationtask is high precision but low recall, as one wouldexpect from a rule-based system.
The recall er-rors stem from constituents which are consideredto fill roles by PropBank, but which are not identi-fied as dependents by the extraction rules (such asthose external to the verb phrase).
The precisionerrors stem from dependents which are found bythe rules, but are not marked by PropBank (suchas the expletive ?it?
).In the classification task, we compare our sys-tem to an informed baseline, which is computedby labeling each dependent with a role that is a de-terministic function of its syntactic relation.
Thesyntactic relation subj is assumed to be ARG0,and the syntactic relations np#1, cl#1, xcl#1, andacomp#1 are mapped to role ARG1, and all otherdependents are mapped to ARGM .Our best system, trained with 1000 verb in-stances per verb type (where available), gets an F1of 0.897 on the coarse roles classification task onVerb Learned Linkings(4 F1)give .57 {0=subj,1=np#2,2=np#1}(+.436) .24 {0=subj,1=np#1}.13 {0=subj,1=np#1,2=to}work .45 {0=subj}(+.206) .09 {0=subj,2=with}.09 {0=subj,2=for}.09 {0=subj,2=on}pay .47 {0=subj,1=np#1}(+.178) .21 {0=subj,1=np#1,2=for}.10 {0=subj}.07 {0=subj,1=np#2,2=np#1}look .28 {0=subj}(+.170) .18 {0=subj,2=at}.16 {0=subj,2=for}rise .25 {0=subj,1=np#1,2=to}(+.160) .17 {0=subj,1=np#1}.14 {0=subj,2=to}.12 {0=subj,1=np#1,2=to,3=from}Table 4: Learned linking models for the most improved verbs.To conserve space, ARG0 is abbreviated as 0, and prep to isabbreviated as to.the test set (or 0.783 on the combined identifica-tion and classification task), compared with an F1of 0.856 for the baseline (or 0.747 on the com-bined task), thus reducing 28.5% of the relativeerror.
Similarly, this system reduces 35% of theerror on the coarse roles task on development set.To get a better sense of what is and is not be-ing learned by the model, we compare the perfor-mance of individual verbs in both the baseline sys-tem and our best learned system.
For this analysis,we have restricted focus to verbs for which thereare at least 10 evaluation examples, to yield a re-liable estimate of performance.
Of these, 27 verbshave increased F1 measure, 17 are unchanged, and8 verbs have decreased F1.
We show learned link-ings for the 5 verbs which are most and least im-proved in Tables 4 and 5.The improvement in the verb give comes fromthe model?s learning the ditransitive alternation.The improvements in work, pay, and look stemfrom the model?s recognition that the oblique de-pendents are generated by a core semantic role.Unfortunately, in some cases it lumps differentroles together, so the gains are not as large asthey could be.
The reason for this conservatismis the relatively high level of smoothing in theword distribution relative to the linking distribu-tion.
These smoothing parameters, set to opti-mize performance on the development set, preventerrors of spurious role formation on other verbs.The improvement in the verb rise stems from themodel correctly assigning separate roles each forthe amount risen, the source, and the destination.7Verb Learned Linkings(4 F1)help .52 {0=subj,1=cl#1}(?.039) .25 {0=subj,1=xcl#1}.16 {0=subj,1=np#1}follow .81 {0=subj,1=np#1}(?.056) .13 {0=subj,1=cl#1}make .64 {0=subj,1=np#1}(?.133) .23 {0=subj,1=cl#1}leave .57 {0=subj,1=np#1}(?.138) .18 {0=subj}.12 {0=subj,1=cl#1}close .24 {0=subj,2=in,3=at}(?.400) .18 {0=subj,3=at}.11 {0=subj,2=in}.10 {0=subj,1=np#1,2=in,3=at}Table 5: Learned linking models for the least improved verbs.To conserve space, ARG0 is abbreviated as 0, and prep to isabbreviated as to.The poor performance on the verb close stemsfrom its idiosyncratic usage in the WSJ corpus;a typical use is In national trading, SFE sharesclosed yesterday at 31.25 cents a share, up 6.25cents (wsj 0229).
Our unsupervised system findsthat the best explanation of this frequent use pat-tern is to give special roles to the temporal (yes-terday), locative (at 31.25 cents), and manner (intrading) modifiers, none of which are recognizedas roles by PropBank.
The decrease in perfor-mance on leave stems from its inability to distin-guish between its two common senses (left Marywith the gift vs. left Mary alone), and the factthat PropBank tags Mary as ARG1 in the first in-stance, but ARG2 (beneficiary) in the second.
Theerrors in make and help result from the fact that ina phrase like make them unhappy the Penn Tree-bank chooses to wrap them unhappy in a singleS, so that our rules show only a single dependentfollowing the verb: a complement clause (cl#1)with head word unhappy.
Unfortunately, our sys-tem calls this clause ARG1 (omplement clausesfollowing the verb are usually ARG1), but Prop-Bank calls it ARG2.
The errors in the verb followalso stem from a sense confusion: the second fol-lowed the first vs. he followed the principles.7 ConclusionWe have demonstrated that it is possible to learn astatistical model of verb semantic argument struc-ture directly from unannotated text.
More workneeds to be done to resolve particular classes oferrors; for example, the one reported above for theverb work.
It is perhaps understandable that thedependents occurring in the obliques with and forare put in the same role (the head words should re-fer to people), but it is harder to accept that depen-dents occurring in the oblique on are also groupedinto the same role (the head words of these shouldrefer to tasks).
It seems plausible that measures tocombat word sparsity might help to differentiatethese roles: backing-off to word classes, or evenjust training with much more data.
Nevertheless,semantic role labeling performance improvementsdemonstrate that on average the technique is learn-ing verb linking models that are correct.ReferencesC.
F. Baker, C. J. Fillmore, and J.
B. Lowe.
1998.
The Berke-ley FrameNet project.
In ACL 1998, pages 86?90.T.
Briscoe and J. Carroll.
1997.
Automatic extraction ofsubcategorization from corpora.
In Applied NLP 1997,pages 356?363.E.
Charniak.
2000.
A maximum entropy inspired parser.
InNAACL 2002.J.
M. Eisner.
2001.
Smoothing a probabilistic lexicon via syn-tactic transformations.
Ph.D. thesis, University of Penn-sylvania.A.
Gelman, J.
B. Carlin, H. S. Stern, and Donald D. B. Rubin.2003.
Bayesian Data Analysis.
Chapman & Hall.D.
Gildea and D. Jurafsky.
2002.
Automatic labeling of se-mantic roles.
Computational Linguistics, 28.D.
Klein and C. Manning.
2003.
Accurate unlexicalized pars-ing.
In ACL 2003.A.
Korhonen.
2002.
Subcategorization acquisition.
Ph.D.thesis, University of Cambridge.M.
Lapata.
1999.
Acquiring lexical generalizations fromcorpora: A case study for diathesis alternations.
In ACL1999, pages 397?404.B.
Levin.
1993.
English Verb Classes and Alternations.
Uni-versity of Chicago Press.C.
D. Manning.
1993.
Automatic acquisition of a large sub-categorization dictionary.
In ACL 1993, pages 235?242.M.
P. Marcus, B. Santorini, and M. A. Marcinkiewicz.
1993.Building a large annotated corpus of English: the PennTreebank.
Computational Linguistics, 19:313?330.C.
McCarthy and A. Korhonen.
1998.
Detecting verbal par-ticipation in diathesis alternations.
In ACL 1998, pages1493?1495.P.
Merlo and S. Stevenson.
2001.
Automatic verb classifi-cation based on statistical distributions of argument struc-ture.
Computational Linguistics, 27(3):373?408.M.
Palmer, D. Gildea, and P. Kingsbury.
2003.
The proposi-tion bank: An annotated corpus of semantic roles.
Com-putational Linguistics.S.
Pradhan, W. Ward, K. Hacioglu, J. Martin, and D. Juraf-sky.
2005.
Semantic role labeling using different syntacticviews.
In ACL 2005.V.
Punyakanok, D. Roth, and W. Yih.
2005.
Generalizedinference with multiple semantic role labeling systemsshared task paper.
In CoNLL 2005.S.
Schulte im Walde.
2000.
Clustering verbs automaticallyaccording to their alternation behavior.
In ACL 2000,pages 747?753.S.
Stevenson and P. Merlo.
1999.
Automatic verb classifica-tion using distributions of grammatical features.
In EACL1999, pages 45?52.R.
S. Swier and S. Stevenson.
2004.
Unsupervised semanticrole labeling.
In EMNLP 2004.K.
Toutanova.
2005.
Effective statistical models for syntac-tic and semantic disambiguation.
Ph.D. thesis, StanfordUniversity.8
