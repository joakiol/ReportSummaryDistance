On Parsing Strategies and Closure'Kenneth ChurchMITCambridge.
MA 02139This paper proposes a welcome hypothesis: a computationallysimple device z is sufficient for processing natural language.Traditionally it has been argued that processing naturallanguage syntax requires very powerful machinery.
Manyengineers have come to this rather grim conclusion; almost allworking parers are actually Turing Machines (TM), Forexample, Woods believed that a parser should have TMcomplexity and specifically designed his Augmented TransitionNetworks (ATNs) to be Turing Equivalent.
(1) "It is well known (cf.
\[Chomsky64\]) that the strictcontext-free grammar model is not an adequatemechanism for characterizing the subtleties ofnatural anguages."
\[WoodsTO\]If the problem is really as hard as it appears, then the onlysolution is to grin and bear it.
Our own position is that parsingacceptable sentences i simpler because there are constraints onhuman performance that drastically reduce the computationalcomplexity.
Although Woods correctly observes thatcompetence models are very complex, this observation may notapply directly to a performance problem such as parsing)The claim is that performance limitations actually reduceparsing complexity.
This suggests two interesting questions: (a)How is the performance model constrained so as to reduce itscomplexit?, and (b) How can the constrained performancemodel naturally approximate competence idealizations?1.
The FS HypothesisWe assume a severe processing limitation on available short termmemory (5TM), as commonly suggested in the psycholinguisticliterature (\[Frazier79\], \[Frazier and Fodor?9\].
\[Cowper76\],\[Kimball73, 75\]).
Technically a machine with limited memoryis a finite state machine (FSM) which has very good complexitybounds compared to a TM.How does this assumption interact with competence?
It isplausible for there to be a rule of competence (call itCcomplex) which cannot be processed with limited memory.What does this say about the psychological reality of Ccomplex?What does this imply about the FS hypothesis?When discussing certain performance issues (e.g.
center-embedding).
4 it will be most useful to view the processor as aFSM; on the other hand, competence phenomena(e.g.
subjacency) suggest a more abstract point of view.
It willbe assumed that there is ultimately a single processing machinewith its multiple characterizations (the ideal and the realcomponents).
The processor does not literally apply ideal rulesof competence for lack of ideal TM resources, but rather, itresorts to more realistic approximations.
Exactly where theidealizations call for inordinate resources, we should expect tofind empirical discrepancies between competence andperformance.A F5 processor is unable to parse complex sentences eventhough they may be grammatical.
We claim these complexsentences are unacceptable.
Which constructions are inprinciple beyond the capabilities of a finite state machine?Chomsky and Bar-Hillel independently showed that (arbitrarilydeep) center-embedded structures require unbounded memory\[Chomsky59a, b\] \[Bar-Hillelbl\] \[Langendoen75\].
As predicted,arbitrarily center-embedded sentences are unacceptable, even atrelatively shallow depths.
(2) ;g\[The man \[who the boy \[who the studentsrecognized\] pointed out\] is a friend of mine.\](3) ~\ [The  rat \[the cat \[the dog chased\] bit\] ate thecheese.\]A memory limitation provides a very attractive account of thecenter-embedding phenomena (in the limit)J1.
I would like to thank Peter Szolovits, Mitch Marcus, Bill Martin, BobBerwick, Joan Bresnan, Jon Alien, Ramesh Patil, Bill $wartout, JayKeyser.
Ken Wexler, Howard L&,nik, Dave McDonald, Per-KristianHalvorsen, and countless others for many useful comments,2.
Throughout this work, the complexity notion will be u=md in iucomputational sense as a measure of time and space resources requiredby an optimal processor.
The term will not he used in the linguisticsense (the .~ite of the grammar itself).
In general, one can trade one offfor the other, which leads to conslderable confusion.
The site of aprogram (linguistic compiexhy) is typically inversely related to thepower of ttle interpreter (computational complexily).3.
A ha.~i~ mark (~) is used to indicate that a sentence is unacceptable;,an asterisk (=) is used in the traditional fashion to denoteungrammaficality.
Grammaticality is associated with competence(post-theoretic), where&,~ acceptability is a matter of performance(empirical).
(4) "This fact \[that deeply center-embedded sentencesare unacceptable\], and this alone, follows from theassumption of finiteness of memory (which no one,surely, has ever questioned)."
\[Chomskybl, pp.
127\]What other phenomena follow from a memory limitation?Center-embedding is the most striking example, but it is norunique.
There have been many refutations of FS competence4.
A center-embedded sentence contains an embedded clausesurrounded by \]exical material from the higher claus:.
\[sx \[s - \ ]  Y\]' whereboth x and y contain lexical material.5.
A complexity argumem of this sort does not distinguish between adepth of three or a depth of four.
It would require considerablepsychological experimentation to di~over the precise limitations,107models: each one illustrates the point: computationally complexstructures are unacceptable.
Lasnik's noncoreference rule\[Lasnik76\] is another source of evidence.
The rule observes tllattwo noun phrases in a particular structural configuration arenoncoreferential.
(5) The Noncoreference Rule: Given two noun phrasesNP 1.
NP 2 in a sentence, if NP 1 precedes andcommands NP 2 and NP 2 is not a pronoun, thenNP1 and NP 2 are noncoreferentiaLIt appears t o be impossible to apply Lasnik's rule with onlyfinite memory.
The rule becomes harder and harder to enforceas more and more names are mentioned.
As the memoryrequirements grow, the performance model is less and less likelyto establish the noncoreferential link.
In (6).
the co-indeaednoun phrases cannot be coreferential.
At the depth increases.the noncoreferential judgments become less and less sharp, eventhough (6)-(8) are all equally ungrammatical(65 *~Did you hear that John i told the teacher John ithrew the first punch.
(7) *?
?Did you hear that John i told the teacher that Billsaid John i threw the first punch.
(85 *?Did you hear that John i told the teacher that Billsaid that Sam thought John i threw the first punch.Ideal rules of competence do not (and should not) specify realprocessing limitations (e.g.
limited memory); these are mattersof performance.
(65-(8) do not refute Lasnik's rule in any way;they merely point out thal its performance r alization has someimportant empirical differences from Lasnik's idealization.Notice that movement phenomena can cross unboundeddistances without degrading acceptability.
Compare this withthe center-embedding examples previously discussed.
We claimthat center-embedding demands unbounded resources whereasmovement has a bounded cost (in the wont case).
6 It ispossible for a machine to process unbounded movement withvery limited resources.
7 This shows that movement phenomena(unlike center-embedding) can be implemented in aperformance model without approximation.
(9) There seems likely to seem likely ... to be a problem.
(10) What did Bob say that Bill said that ... John liked?It is a positive result when performance and competence happento converge, as in the movement case.
Convergence nablesperformance to apply competence rules without approximation.However.
there is no logical necessity that performance and6.
The claim is that movement will never consume more than abounded cost: the cost is independent of the length of the sentence.Some movement .~entences may be ea.
'~ier than others (subject vs. objectrelatives).
See (Church80\] for more di~ussion.7.
In fact, the human processor may not be optimal The functionalargument ob~erve~ that an optimal proce~r could process unboundedmovement with bounded resources.
This should encourage furtherinvestigation, but it alone is not sufficient evidence that the humanprocesr.or has optimal properties.competence will ultimately converge in every area.
The FShypothesis, if correct, would necessitate compromising manycompetence idealizations.2.
The Proposed Model: YAPMost psycholinguists believe there is a natural mapping from thecomplex competence model onto the finite performance world.This hypothesis is intuitively attractive, even though there is nological reason that it need be the case.
s Unfortunately, the~ychoi inguist ic literature does not precisely describe themapping.
We have implemented a parser (YAP) which behaveslike a complex competence model on acceptable 9 cases, but failsto pane more difficult unacceptable sentences.
Thisperformance model looks very similar to the more complexcompetence machine on acceptable sentences even though it"happens" to run in severely limited memory.
Since it is aminimal augmentation of existing psychological and linguisticwork, it will hopefully preserve 1heir accomplishments, and inaddition, achieve computational dvantages.The  basic design of YAP  is similar to Marcus' Parsifal\[Marcus79\], with the additional limitation on memory.
Hisparser, like most stack machine parsers, will occasionally fill thestack with structures it no longer needs, consuming unboundedmemory.
To achieve the finite memory limitation, it must beguaranteed that this never happens on acceptable structures.That is, there must be a procedure (like a garbage collector) forcleaning out the stack so that acceptable sentences can beparsed without causing a stack overflow.
Everything on thestack should be there for a reason; in Marcus' machine it ispossible to have something on the stack which cannot bereferenced again.
Equipped with its garbage collector, YAPruns on a bounded stack even though it is approximating amuch more complicated machine (e.g.
a PDA).
l?
The claim isthat YAP can parse acceptable sentences with limited memory,although there may be certain unacceptable s ntences that willcause YAP to overflow its stack.3.
Marcus' Determinism HypothesisThe memory constraint becomes particularly interesting when itis combined with a control constraint such as Marcus'Detfrminism Hvvothesis \[Marcus79\].
The DeterminismHypothesis claims that once the processor is committed to aparticular path, it is extremely difficult to select an alternative.For example, most readers will misinterpret he underlinedportions of (11)-(135 and then have considerable difficultycontinuing.
\]=or this reason, these unacceptable s ntences areoften called Qarden Paths (GP).
The memory limitation alonefails to predict the unacceptability of (115-(I 3) since GPs don't8.
Chomsky and Lasnik (per~naI communication) have each suggestedthat the competence model might generate a non-computable ,..eL If thiswere indeed the c&~e, it would seem unlikely that there could be amapping onto tile finite performance world.9.
Acceptability is a formal term: see footnote 3.10.
A push down automata (PDA) is a formalization of stack machines.108center-embed very deeply.
Determinism offers an additionalconstraint on memory allocation which provides an account forthe data.
(11) ~T_.~h horse raced past the barn fell.
(12) ~ John  .lifted a hundred pound bags.
(1 3) HI told the boy the doR bit Sue would help him.At first we believed the memory constraint alone wouldsubsume Marcus' hypothesis as well as providing an explanationof the center-embedding phenomena.
Since all FSMs have adeterministic realization, tl it was originally supposed that thememory limitation guaranteed that the parser is deterministic(or equivalent to one that is).
Although the argument istheoretically sound, it is mistaken) ~ The deterministicrealization may have many more states than the correspondingnon-deterministic FSM.
These extra states would enable themachine to parse GPs by delaying the critical decision) 3 Inspirit, Marcus' Determinism Hypothesis excludes encodingnon-determinism by exploding the state space in this way.
Thisamounts to an exponential reduction in the size of the statespace, which is an interesting claim, not subsumed by FS (whichonly requires the state space to be finite).By assumption, the garbage collection procedure must act"deterministically"; it cannot backup or undo previous decisions.Consequently, the machine will not only reject deeplycenter-embedded sentences but it will also reject sentences suchas (14) where the heuristic garbage collector makes a mistake(takes a garden path).
(14) .if:Harold heard \[that John told the teacher \[that Billsaid that Sam thought that Mike threw the firstpunch\] yesterday\].YAP is essentially a stack machine parser like Marcus' Parsifalwith the additional bound on stack depth.
There will be agarbage collector to remove finished phrases from the stack sothe space can be recycled.
The garbage collector will have todecide when a phrase is finished (closed).4.
Closure SpecificationsAssume that the stack depth should be correlated to the depthof center-embedding.
It is up to the garbage collector to closephrases and remove them from the stack, so onlycenter-embedded phrases will be left on the stack.
The garbagecollector could err in either of two directions; it could be overlyuthless,  cleaning out a node (phrase) which will later turn outto be useful, or it could be overly conservative, allowing itslimited memory to be congested with unnecessary information.In either case.
the parser will run into trouble, finding the, I.
A non-deterministic FSM with n states is equivalent to anotherdeterministic FSM with 2 a states.12.
l am indebted to Ken Wexier for pointing this out.13.
The exploded states encode disjunctive alternatives.
Intuitively,GPs mgge.~t that it im't possible to delay the critical decision: themachine has to decide which way to proceed.sentence unacceptable.
We have defined the two types oferrors below.
(15) Premature Closure: The garbage collectorprematurely removes phrases that turn out to benecessary.
(16) Ineffective Closure: The garbage collector does notremove enough phrases, eventually overflowing thelimited memory.There are two garbage collection (closure) proceduresmentioned in the psycholinguistic literature: KimbaU's earlyclosure \[Kimball73.
75\] and Frazier's late closure \[Frazier79\].We will argue that Kimball's procedure is too ruthless, closingphrases too soon, whereas Frazier's procedure is tooconservative, wasting memory.
Admittedly it is easier tocriticize than to offer constructive solutions.
We will developsome tests for evaluating solutions, and then propose our ownsomewhat ad hoc compromise which should perform better thaneither of the two extremes, early closure and late closure, but itwill hardly be the final word.
The closure puzzle is extremelydifficult, but also crucial to understanding the seeminglyidiosyncratic parsing behavior that people exhibit.5.
Kimball's Early ClosureThe bracketed interpretations of (17)-(19) are unacceptableeven though they are grammatical.
Presumably, the rootmatrix"* was "closed off" before the final phrase, so that thealternative attachment was never considered.
(17) ~:Joe figured \[that Susan wanted to take the train toNew York\] out.
(18) H I  met \[the boy whom Sam took to the park\]'sfriend.
(19) ~The girl i applied for the jobs \[that was attractive\]i.Closure blocks high attachments in sentences like (17)-(19) byremoving the root node from memory long before the lastphrase is parsed.
For example, it would close the root clausejust before that in (21) and who in (22) because the nodes\[comp that\] and \[comp who\] are not immediate constituents ofthe root.
And hence, it shouldn't be possible to attach anythingdirectly to the root after that and who.
js(20) Kimball's Early Closure: A phrase is closed as soonas possible, i.e., unless the next node parsed is animmediate constituent of that phrase.
\[Kimball73\](21) \[s Tom saidis- that Bill had taken the cleaning out ...(22) \[s Joe looked the friendis- who had smashed his new car ... up14.
A matrix is roughly equivalent o a phra.,e or a clause.
A matrix isa frame wifl~ slots for a mother and several daughters.
The root matrix isthe highest clause.\[5, Kimbali's closure is premature in these examples ince it is po~ibieto interpret yesterday attaching high as in: Tom said\[that Bill had takenthe c/caning out\] yesterday.109This model inherently assumes that memory is costly andpresumably fairly limited.
Otherwise.
there wouldn't be amotivation for closing off phrases.Although Kimball's strategy strongly supports our own position.it isn't completely correct.
The general idea that phrases areunavailable is probably right, but the precise formulation makesan incorrect prediction.
If the upper matrix is really closed off,then it shouldn't be possible to attach anything to it.
Yet(23)-(24) form a minimal pair where the final constituentattaches low in one case.
as Kimball would predict, but high inthe other, thus providing a counter-example to Kimball'sstrategy.
(23) I called \[the guy who smashed my brand new carup\].
(low attachment)(24) I called \[the guy who smashed my brand new car\] arotten driver.
(high attachment)Kimball would probably not interpret his closure strategy asliterally as we have.
Unfortunately computer modeh arebrutally literal.
Although there is considerable content toKimball's proposal (closing before memory overflow,), theprecise formulation has some flaws.
We will reformulate thebasic notion along with some ideas proposed by Frazier.6.
Frazier's Late ClosureSuppose that the upper matrix is not closed off.
as Kimballsuggested, but rather, temporarily out of view.
Imagine thatonly the lowest matrix is available at any given moment, andthat the higher matrices are stacked up.
The decision thenbecomes whether to attach to the current matrix or to c.l.gse itoff.
making the next higher matrix available.
The strategyattaches as low as possible; it will attach high if all the lowerattachments are impossible.
Kimhall's strategy, on the otherhand.
prevents higher attachments by closing off the highermatrices as soon as possible.
In (23).
according to Frazier's lateclosure, up can attach t~ to the lower matrix, so it does; whereasin (24).
a rotten driver cannot attach low.
so the lower matrix isclosed off.
allowing the next higher attachment.
Frazier callsthis strategy late cto~ure because lower nodes (matrices) areclosed as late as possible, after all the lower attachments havebeen tried.
She contrasts her approach with Kimball's earlyclosure, where :~e higher matrices are closed very early, beforethe lower matrices are done.
j7(25) Late Closure: When possible, attach incomingmaterial into the clause or phrase currently beingparsed.Unfortunately.
it seems that Frazier's late closure is tooconservative, allowing nodes to remain open too long.congest ing valuable stack space.
Without any form of earlyclosure, right branching structures uch as (26) and (27) are areal problem; the machine will eventually flU up with unfinishedmatrices, unable to close anything because it hasn't reached thebottom right-most clause.
Perhaps Kimball's suggestion ispremature, but Frazier's is ineffective.
Our compromise willaugment Frazier's strategy to enable higher clause, to closeearlier under marked conditions (which cover the rightbranching case).
(26) This is the dog that chased the cat that ran after therat that ate the cheese that you left in the trap thatMary bought at the store that ...(27) I consider every candidate likely to be consideredcapable of being considered somewhat less thanhonest toward the people who ...Our argument is like all complexity arguments; it coasiden thelimiting behavior as the number of clauses increase.
Certainlythere are numerous other factors which decide borderline cares(3-deep center.embedded clauses for example), some of whichFrazier and Fodor have discussed.
We have specifically avoidedborderline cases because judgments are so difficult and variable;the limiting behavior is much sharper.
In these limiting case,,though, there can be no doubt that memory limitations arerelevant o parsing strategies.
In particular, alternatives cannotexplain why there are no acceptable sentences with 20 deepcenter-embedded clauses.
The only reason is that memory islimited; see \[Chomsky59a.b\].
\[Bar-Hillel6l\] and \[Langendnen75\]for the mathematical rgument.7.
A CompromiseAfter criticizing early closure for being too early and lateclosure for being too late.
we promised that ~e would provideyet another "improvement".
Our suggestion is similar to lateclosure, except that we allow one case of early closure (theA-over-A early closure principle), to clear out stack space in theright recursive case.
I~ The A-over-A early closure principle issimilar to Kimball's early closure principle except that it wait,for two nodes, not just one.
For example in (28).
our principlewould close \[I that Bill raid $2\] just before the that in S 3whereas Kimball's scheme would close it just before the that inS 2 .16.
Deczding whether a node ca__nq or cannot attach is a difficultquestion which must be addressed.
YAP uses the functional .~tructure\[Bre.
'man (to appear)\] and the phrase structure rules.
For now we willhave to appeai to the reader's intuitions.|7, Frazier'.s strategy will attach to the lower matrix even when thefinal particle is required by the higher ciau.,.e &, in: ?!
looked the guy whosmashed my car ,40. or ?Put the block which is on the box on the tabl?~ig.
Earl)' closure is similar to a compil"  optimization called tailrecursion, which converts right recursive exp,'essions into iterative ones,thus optimizing stack u~ge.
Compilers would perform the optimizationonly when the structure is known to be right recursive: the A..over-Aclo.,,ure principle is somewhat heuristic since the structure may turn outto be center-embedded.110(28) John said \[I that Bill said \[2 that Sam said \[3 that?
Jack ...(29) The A-over-A early closure principle: Given twophrases in the same category (noun phrase, verbphrase, clause, etc.
), the higher closes when both areeligible for Kimball closure.
That is.
(1) both nodesare in ~he same category, (2) the next node parsed isnot an immediate constituent of either phrase, and(3) the mother and all obligatory daughters havebeen attached to both nodes.This principle, which is more aggressive th.qn late closure,enables the parser to process unbounded right recursion within abounded stack by constantly closing off.
However, it is notnearly as ruthless as Kimball's early closure, because it waits fortwo nodes, not just one.
which will hopefully alleviate theproblems that Frazier observed with Kimball's strategy.There are some questions about the borderline cases wherejudgments are extremely variable.
Although the A-over.Aclosure principle makes very sharp distinctions, the borderlineare often questionable, ~ See \[Cowper76\] for an amazingcollection of subtle judgments that confound every proposal yetmade.
However, we think that the A-over-A notion is a step inthe right direction: it has the desired limiting behavior, althoughthe borderline cases are not yet understood.
We are stillexperimenting with the YAP system, looking for a morecomplete solution to the closure puzzle.In conclusion, we have argued that a memory limitation iscritical to reducing performance model complexity.
Although itis difficult to discover the exact memory allocation procedure, itseems that the closure phenomenon offers an interesting set ofevidence.
There are basically two extreme closure models inthe literature.
Kimball's early and Frazier's late closure.
Wehave argued for a compromise position: Kimball's position is toorestrictive (rejects too many sentences) and Frazier's position istoo expensive (requires too much memory for right branching).We have propo~d our own compromise, the A-over-A closureprinciple, which shares many advantages of both previousproposals without some of the attendant disadvantages.
Ourprinciple is not without its own problems; it seems that there isconsiderable work to be done.By incorporating this compromise, YAP is able to cover a widerrange of phenomena :?
than Parsifal while adhering to a finitestate memory constraint.
YAP provides empirical evidence thatit is possible to build a FS performance device whichapproximates a more complicated competence model in the easyacceptable cases, but fails on certain unacceptable constructionssuch as closure violations and deeply center embeddedsentences.
In short, a finite state memory limitation simplifiesthe parsing task.8.
ReferencesBar-Hillel.
Perles, M., and Shamir, E., On Formal Properties ofSimple Phrase Structure Grammars, reprinted in Readings inMathematical Psychology, 1961.Chomsky.
Three models for the description of language, I.R.E.Transactions on Information Theory.
voL IT-2, Proceedings ofthe symposium on information theory.
1956.Chomsky.
On Certain Formal Properties of Grammars,Information and Control, vol 2. pp.
137-167.
1959a.Chomsky, A Arose on Phrase Structure Grammars, Informationand Control, vol 2, pp.
393-395, 1959b.Chomsky.
On the Notion "Rule of Grammar'; (1961 ), reprintedin J. Fodor and J. Katz.
ads., pp 119-136, 19~.Chomsky.
A Transformational Approach to Syntax, in Fodorand Katz.
eds., 1964.Cowper.
Elizabeth A..
Constraints on Sentence Complexity: AModel for Syntactic Processing.
PhD Thesis, Brown University,1976.Church, Kenneth W.. On Memory Limitations in NaturalLanguage Processing.
Masters Thesis in progress, 1980.Frazier.
Lyn, On Comprehending Sentences: Syntactic ParsingStrategies.
PhD Thesis.
University of Massachusetts, IndianaUniversity Linguistics Club, 1979.Frazier, Lyn & Fodor.
Janet D..
The Sausage machine: A NewTwo-Stage Parsing Model Cognition.
1979.Kimball.
John.
Seven Principles of Surface Structure Parsing inNatural Language.
Cognition 2:1, pp 15-47, 1973.Kimball.
Predictive Analysis and Over-the-Top Parsing, inSyntax arrd Symantics IV, Kimball editor, 1975.Langendoen.
Finite-State Parsing of Phrase-StructureLanguages and the Status of Readjustment Rules in Grammar,Linguistic Inquiry Volume VI Number 4, Fall 1975.Lasnik.
H..
Remarks on Co-reference, Linguistic Analysis.Volume 2.
Number 1.
1976.Marcus.
Mitchell.
A Theory of Syntactic Recognition forNatural Language, MIT Press, 1979.Woods, William, Transition Network Grammars for NaturalLanguage Analysis.
CACM.
Oct. 1970.19.
\[n particular, the A-over-A ear|y closure principle does notaccount for preferences in sentences like: \[ said that you did it yesterdaybecause there are only two clau.~es.
Our principle only addresses thelimhing cases.
We believe there is another related mechanism (likeFrazier's Minimal Attachment) to account for the preferred lowattachments.
See \[Church80\].20.
T~e A-over-A principle is useful for thinking about conjunction.111
