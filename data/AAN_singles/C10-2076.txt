Coling 2010: Poster Volume, pages 665?673,Beijing, August 2010Combining Constituent and Dependency Syntactic Views forChinese Semantic Role LabelingShiqi Li1, Qin Lu2, Tiejun Zhao1, Pengyuan Liu3 and Hanjing Li11School of Computer Science and Technology,Harbin Institute of Technology{sqli,tjzhao,hjlee}@mtlab.hit.edu.cn2Department of Computing,The Hong Kong Polytechnic Universitycsluqin@comp.polyu.edu.hk3Institute of Computational Linguistics,Peking Universityliupengyuan@pku.edu.cnAbstractThis paper presents a novel feature-based semantic role labeling (SRL)method which uses both constituentand dependency syntactic views.
Com-paring to the traditional SRL methodrelying on only one syntactic view, themethod has a much richer set of syn-tactic features.
First we select severalimportant constituent-based and de-pendency-based features from existingstudies as basic features.
Then, we pro-pose a statistical method to select dis-criminative combined features whichare composed by the basic features.SRL is achieved by using the SVMclassifier with both the basic featuresand the combined features.
Experimen-tal results on Chinese Proposition Bank(CPB) show that the method outper-forms the traditional constituent-basedor dependency-based SRL methods.1 IntroductionSemantic role labeling (SRL) is a major me-thod in current semantic analysis which is im-portant to NLP applications.
The SRL task isto identify semantic roles (or arguments) ofeach predicate and then label them with theirfunctional tags, such as 'Arg0' and 'ArgM' inPropBank (Palmer et al, 2005), or 'Agent' and'Patient' in FrameNet (Baker et al, 1998).The significance of syntactic analysis inSRL has been proven by (Gildea and Palmer,2002; Punyakanok et al, 2005), and syntacticparsing has been applied by almost all currentstudies.
In terms of syntactic representations,the SRL approaches are mainly divided intothree categories: constituent-based, chunk-based and dependency-based.
Constituent-based SRL has been studied intensively withsatisfactory results.
Chunk-based SRL hasbeen found to be less effective than the con-stituent-based by (Punyakanok et al, 2005).
Inrecent years, the dependency-based SRL hasbeen greatly promoted by the CoNLL sharedtasks on semantic parsing (Hajic et al, 2009).However, there is not much research on com-bined use of different syntactic views (Pradhanet al, 2005), on the feature level of SRL.This paper introduces a novel method forChinese SRL utilizing both constituent-basedand dependency-based features.
The methodtakes constituent as the basic unit of argumentand adopts the labeling of PropBank.
It followsthe prevalent feature-based SRL methods tofirst turn predicate-argument pairs into flatstructures by well-defined linguistic features,and then uses machine learning methods topredict the semantic labels.
The method alsoinvolves two classification phases: semanticrole identification (SRI) and semantic roleclassification (SRC).
In addition, a heuristic-based pruning preprocessing (Xue and Palmer,2004) is used to filter out a lot of apparentlyinappropriate constituents at the beginning.665And it has been widely reported that, in fea-ture-based SRL, the performance can be im-proved by adding several combined featureseach of which is composed by two single fea-tures (Xue and Palmer, 2004; Toutanova et al,2005; Zhao et al, 2009).
Thus, in this work,we exploit combined use of both constituent-based and dependency-based features in addi-tion to using features of singular types of syn-tactic view.
We propose a statistical method toselect effective combined features using bothconstituent-based and dependency-based fea-tures to make full use of two syntactic views.2 Related WorkIn recent years, many advances have beenmade on SRL using singular syntactic view,such as constituent (Gildea and Jurafsky, 2002;Xue and Palmer, 2004; Surdeanu et al, 2007),dependency (Hacioglu, 2004; Johansson andNugues, 2008; Zhao et al, 2009), and CCG(Chen and Rambow, 2003; Boxwell et al2009).
However, there are few studies on theuse of multiple syntactic views.
We brieflyreview the relevant studies of SRL usingmultiple syntactic views as follows.Pradhan et al (2005) built three semanticrole labelers using constituent, dependency andchunk syntactic views, and then heuristicallycombined them at the output level.
The methodwas further improved in Pradhan et al (2008)which trains two semantic role labelers forconstituents and dependency separately, andthen uses the output of the two systems as ad-ditional features in another labeler using chunkparsing.
The result shows an improvement toeach labeler alone.
A possible reason for theimprovement is that the errors caused by dif-ferent syntactic parsers are compensated.
Yet,the features of different syntactic views canhardly complement each other in labeling.
Andthe complexity of using multiple syntacticparsers is extremely high.
Hacioglu (2004)proposed a SRL method to combine constitu-ent and dependency syntactic views where thedependency parses are ob-tained through auto-matic mapping of constitu-ent parses.
It usesthe constituent parses to get candidates andthen, the dependency parses to label them.Boxwell et al (2009) proposed a SRL me-thod using features of three syntactic views:CCG, CFG and dependency.
It primarily usesCCG-based features associated with 4 CFG-based and 2 dependency-based features.
Thecombination of these syntactic views leads to asubstantial performance improvement.
Nguyenet al (2009) proposed a composite kernelbased on both constituent and dependency syn-tactic views and achieved a significant im-provement in a relation extraction application.3Compared to related work, the proposed me-thod integrates the constituent and dependencyviews in a collaborative manner.
First, we de-fine a basic feature set containing featuresfrom constituent and dependency syntacticviews.
Then, to make better use of two syntac-tic views, we introduce a statistical method toselect effective combined features from thebasic feature set.
Finally we use both the basicfeatures and the combined features to identifyand label arguments.
One of the drawbacks ofthe related work is the considerable complexitycaused by multiple syntactic parsing processes.In our method, the cost of syntactic parsingwill increase only slightly as we derive de-pendency parsing from constituent parsing us-ing a constituent-to-dependency converter in-stead of using an additional dependency parser.In our method, the feature set used for SRLconsists of two parts: the basic feature set andthe combined feature set built upon the basicfeature set.
The basic feature set can be furtherdivided into constituent-based features anddependency-based features.
Constituent fea-tures focus on hierarchical relations betweenmulti-word constituents whereas dependencyfeatures focus on dependencies between indi-vidual words, as shown in Figure 1.
Take thepredicate '??'
(increased) as an example, inFigure 1(a), the NP constituent '?????
'(China's position) is labeled as 'Arg0'.
The ar-gument and the predicate are connected by thepath of node types: 'NP-IP-VP-VP'.
But inFigure 1(b), the individual word '??'
(posi-tion) is labeled as 'Arg0'.
And the connectionbetween the argument and the predicate is onlyone edge with the relation 'nsubj', which ismore explicit than the path in the constituentstructure.
So the two syntactic views can com-plement each other on different linguistic units.Design Principle and Basic Features6663.1 Constituent-Based FeaturesAs a prevalent syntactic feature set for SRL,constituent-based features have beenextensively studied by many researchers.
Inthis work, we simply take 26 constituent-basedfeatures tested by existing studies, and add 8new features define by us.
Firstly, the 26constituent-based features used by others are:y The seven "standard" features: predicate (c1),path (c2), phrase type (c3), position (c4),voice (c5), head word (c6) and predicatesubcategorization (c7) features proposed by(Gildea and Jurafsky, 2002).y Syntactic frame (c8) feature from (Xue andPalmer, 2004).y Head word POS (c9), partial path (c10),first/last word in constituent (c11/c12),first/last POS in constituent (c13/c14),left/right sibling constituent (c15/c16),left/right sibling head (c17/c18), left/rightsibling POS (c19/c20), constituent tree dis-tance (c21) and temporal cue words (c22)features from (Pradhan et al, 2004).y Predicate POS (c23), argument's parentconstituent (c24), argument's parent con-stituent head (c25) and argument's parentconstituent POS (c26) inspired by (Pradhanet al, 2004).Secondly, the 8 new features that we defineare (we take the 'Arg0' node in Figure 1(a) asthe example to illustrate them):y Locational cue words (c27): a binary featureindicating whether the constituent containslocation cue words, similar to the temporalcue words (c22).
This feature is defined todistinguish the arguments with the 'ArgM-LOC' type from others.y POS pattern of argument's children (c28):the left-to-right chain of the POS tags of theargument's children, e.g.
'NR-DEG-NN'.y Phrase type pattern of argument's children(c29): the left-to-right chain of the phrasetype labels of the argument's children, simi-lar with the POS pattern of argument's chil-dren (c28), e.g.
'DNP-NP'.y Type of LCA and left child (c30): The phrasetype of the Lowest Common Ancestor (LCA)combined with its left child, e.g.
'IP-NP'.y Type of LCA and right child (c31): Thephrase type of the LCA combined with itsright child, e.g.
'IP-VP'.Three features: bag of words of path (c32),bag of words of POS pattern (c33) and bag ofwords of type pattern (c34), for generalizingthree sparse features: path (c2), POS pattern ofargument's children (c28) and phrase type pat-tern of argument's children (c29) by the bag-of-words representation.3.2 Dependency-Based FeaturesThe dependency parse can effectively repre-sent the head-dependent relationship betweenwords, yet, it lacks constituent information.
Ifwe want to label constituents using depend-ency-based features, we should firstly mapeach constituent to one or more appropriatewords in the dependency tree.
In this paper, weuse the head word of a constituent to representthe constituent in the dependency parses.The selection method of dependency-basedfeatures is similar to the method of constitu-ent-based features.
The 35 selected dependen-cy-based features include:y Predicate/Argument relation type (d1/d2),relation path (d3), POS pattern of predi-cate?s children (d4) and relation pattern ofpredicate?s children (d5) features from (Ha-cioglu, 2004).y Child relation set (d6), child POS set (d7),predicate/argument parent word (d8/d9),predicate/argument parent POS (d10/d11),left/right word (d12/d13), left/right POS(d14/d15), left/right relation (d16/d17),left/right sibling word (d18/d19), left/rightsibling POS (d20/d21) and left/right siblingrelation (d22/d23) features as described in(Johansson and Nugues, 2008).y Dep-exists (d24) and dep-type (d25) featuresfrom (Boxwell et al, 2009).y POS path (d26), POS path length (d27), RELpath length (d28) from (Che et al, 2008).y High/low support verb (d29/d30), high/lowsupport noun (d31/d32) features from (Zhaoet al, 2009).y  LCA?s word/POS/relation (d33/d34/d35)inspired by (Toutanova et al, 2005).To maintain the consistency between twosyntactic views, the dependency parses aregenerated by a constituent-to-dependency con-verter (Marneffe et al, 2006), which is suitablefor semantic analysis as it retrieves the seman-tic head rather than the general syntactic head,using a set of modified Bikel's head rules.6674 Selection of Combined FeaturesThe combined features, each of which consistsof two different basic features, have proven tobe positive for SRL.
Several combined featureshave been widely used in SRL, such as 'predi-cate+head word' and 'position+voice'.
But toour knowledge, there is no prior report aboutthe selection method of combined features forSRL.
The common entropy-based criteria areinvalid here because the combined featuresalways take lots of distinct values.
And thegreedy method is too complicated to be practi-cal due to the large number of combinations.In this paper, we define two statistical crite-ria to efficiently estimate the classification per-formance of each combined feature on the cor-pus.
Inspired by Fisher Linear DiscriminantAnalysis (FLDA) (Fisher, 1938) in which theseparation of two classes is defined as the ratioof the variance between the classes to the vari-ance within the classes, namely larger ratio canlead to better separation between two classes,and the discriminant plane can be achieved bymaximizing the separation.
Therefore, in thispaper, we adopt the ratio of inter-class distanceto intra-class distance to measure to what ex-tent a combined feature can partition the data.Initially, the feature set contains only the Nbasic features.
We construct one combinedfeature abf  at each iteration by combining twobasic features af  and bf , where , [1, ]a b N?and a b?
.
We push abf  into the feature set andtake it as the 1N + th feature.
Then, all thetraining instances are represented by featurevectors using the new feature set, and we thenquantize the feature vectors of positive andnegative data orderly to keep their intrinsicstatistical difference.
If the training dataset isdenoted as :{ , }pos negD D D , then the separationcriterion, namely the ratio of inter-class to in-tra-class distance for feature if  can be given as( ) ( , )( , )ifif pos negipos negInterDist D Dg fIntraDist D D=(1)where the inter-class and the intra-class dis-tance between posD  and negD  for feature if  arespecified by (2) and (3), respectively.
( )2( , ) ( ) ( )i i if pos neg f pos f negInterDist D D Mean D Mean D= ?
(2)2 2( , ) ( ) ( )f fi iif pos neg pos negIntraDist D D S D S D= + (3)( )ifMean D  in (2) and ( )ifS D  in (3) repre-sents the sample mean and the correspondingsample standard deviation of feature if  indataset D  as given in (4) and (5).
( )( )| |ix Dfx iMean DD?=?, [1, 1]i N?
+  (4)( )2( ) ( )( )iifx DfMean D x iS DN?
?=?, [1, 1]i N?
+(5)Essentially, the inter-class distance reflectsthe distance between the center of positive da-taset and the center of negative dataset, and theintra-class distance indicates the intensity of allinstances relative to the corresponding center.Therefore, larger ratio will lead to a better par-tition for a feature, as has been pointed out byFLDA.
In order to compare the ratio betweendifferent combined features, we further stan-dardize the value of ( )ig f  by computing its z-score ( )iZ f  which indicates how many stan-dard deviations between a sample and its mean,as given in (6).
( ) ( )( ) i iiGg f g fZ fS?=  (6)where ( )ig f  represents the sample mean asgiven in (7), and GS  represents the samplestandard deviation of the sequence ( )ig fwhere i  ranges from 1 to N+1 as given in (8).11( )( )1Niiig fg fN+== +?, [1, 1]i N?
+(7)121( ( ) ( ))Ni iiGg f g fSN+=?=?, [1, 1]i N?
+(8)After figuring out the ( )aZ f  and ( )bZ f  forthe basic feature af  and bf , and ( )abZ f  for thecombined feature abf  by (6), we define theother criterion, namely the improvement( )abI f  of the combined feature, as the smallerdifference between the z-score of the combined668feature and its two corresponding basic fea-tures as given in (9).
( )( ) ( ) Max ( ),  ( )ab ab a bI f Z f Z f Z f= ?
(9)Finally, the combined feature with a nega-tive ( )abI f  value is eliminated.
Then, we willrank the combined features in terms of their z-score, and use the top N of them for later clas-sification.
The selection method based on thetwo criteria can effectively filter out combinedfeatures whose means have no significant dif-ference between positive and negative data,and hence retain the potentially useful com-bined features for the separation.
Meanwhile, ithas a relatively fast speed when dealing with alarge number of features in comparison to thegreedy method due to its simplicity.5 Performance Evaluation5.1 Experimental SettingIn our experiments, we adopt the three-stepstrategy proposed by (Xue and Palmer, 2004).First, argument candidates are generated fromthe input constituent parse tree using the preva-lent heuristic-based pruning algorithm in (Xueand Palmer, 2004).
Then, each predicate-argument pair is converted to a flat featurestructure by which the similarity between twoinstances can be easily measured.
Finally weemploy the Support Vector Machines (SVM)classifier to identify and classify the arguments.It is noteworthy that we use the same basicfeatures, but different combined features forthe identification and classification of argu-ments.
We present the result comparison be-tween using gold-standard parsing and auto-matic parsing, and also offer an analysis of thecontribution of the combined features.To evaluate the proposed method and com-pare it with others, we use the most commonlyused corpus in Chinese SRL, Chinese Proposi-tion Bank (CPB) version 1.0, as the dataset.The CPB corpus contains 760 documents,10,364 sentences, 37,183 target predicates and88,134 arguments.
In this paper, we focus onsix main types of semantic roles: Arg0, Arg1,Arg2, ArgM-ADV, ArgM-LOC and ArgM-TMP.
The number of semantic roles of the sixtypes accounted for 95% of all the semanticroles in CPB.
For SRC, we use the one-versus-all approach, in which six SVMs will betrained to separate each semantic type from theremaining types.
We divide the corpus intothree parts: the first 99 documents(chtb_001.fid to chtb_099.fid) serve as the testdata, the last 32 documents (chtb_900.fid tochtb_931.fid) serve as the development dataand the left 629 documents (chtb_100.fid tochtb_899.fid) serve as the training data.We use the SVM-Light Toolkit version 6.02(Joachims, 1999) for the implementation ofSVM, and use the Stanford Parser version 1.6(Levy and Manning, 2003) as the constituentparser and the constituent-to-dependency con-verter.
In classifications, we employ the linearkernel for SVM and set the regularization pa-rameter to the default value which is the recip-rocal of the average Euclidean norm of trainingdata.
The performance metrics are: accuracy(A), precision (P), recall (R) and F-score (F).5.2 Combined Feature SelectionFirst, we select the combined features for clas-sifications of SRI and SRC using the methoddescribed in Section 4 on the training data withgold-standard parse trees.
Due to the limit ofthis paper, we only list the top-10 combinedfeatures for SRI and SRC for the 6 differenttypes, as shown in Table 1 in which each com-bined feature is expressed by the IDs of its twobasic features with a plus sign between them.Rank SRI ARG0 ARG1 ARG2 ADV LOC TMP1 c1+c6 c1+c6 c1+c6 c1+c6 c1+c6 c5+c27 c1+c62 c1+d3 c32+c30 c30+d31 c1+d1 c30+d27 c9+d17 c22+c273 d25+d14 c7+c6 c30+d32 c1+c7 c30+d28 c9+d13 c7+c64 c4+d25 c1+c2 c5+c30 c7+c6 c1+c11 c9+c2 d26+d275 d25+d22 c1+c12 c30+d24 c1+c5 c24+d33 c23+c27 d26+d286 d25+d20 c23+c6 c30+c21 c1+c23 c30+d25 c9+c20 c23+d267 d25+d21 c1+c3 c5+c4 c23+c6 c24+d9 c14+c32 c5+d268 d25+d18 c10+d35 c1+c10 c1+c3 c27+c2 c14+c10 d26+d319 d25+d19 c10+d1 c30+d10 c5+c6 c22+c2 c9+c26 d26+d3210 d25+d35 c10+d28 c4+c6 c1+d5 c24+d13 c14+c2 c23+c6Table 1.
Top-10 combined features for SRI andSRC ranked by z-scoreTable 1 shows that the commonly usedcombined features, such as 'predicate+headword' (c1+c6) and 'position+voice' (c4+c5)proposed by (Xue and Palmer, 2004) are alsoincluded.
In particular, the 'predicate+headword' feature takes first place in all semantic669categories except LOC, in which the combina-tion of the new feature 'locational cue words'(c27) and the 'voice (c5)' feature performs thebest.
The results also show that the most fre-quently occurred basic features in the com-bined set are 'predicate' (c1), 'head word' (c6),'type of LCA and left child' (c30), 'dep-type'(d25) and 'POS path' (d26).
These basic fea-tures should be more discriminative whencombined with others.
Additionally, we findsome other latent effective combined features,such as 'predicate subcategorization+headword' (c7+c6), 'predicate POS+head word'(c23+c6) and 'predicate+phrase type' (c1+c3),whose performance will be further validatedand analyzed later in this section.
It is obviousthat the obtained combined features for SRIand SRC are different, and the obtained com-bined features for each type are also differentas our selection method is based on positiveand negative data which are completely differ-ent for each argument type.
In SRI phase, wewill use the combined features for all the sixsemantic types (after removing duplicates).Then, we evaluate the performance of SRLbased on the top-N combined features.
Thepreliminary evaluation on the development setsuggests that the performance becomes stablewhen N exceeds 20.
Therefore, we vary thevalue of N to 5, 10 and 20 in the experimentsto evaluate the performance of combined fea-tures.
Corresponding to the three different val-ues of N, we finally obtained 28, 60 and 114combined features for the SRL, respectively.5.3 SRL Using Gold ParsesTo illustrate each component of the method,we constructed 6 SRL systems using 6 differ-ent feature sets: 'Constituent Only' (CO) - usesthe constituent-based features, as presented inSection 3.1; 'Dependency Only' (DO) - usesthe dependency-based features, as presented inSection 3.2; 'CD' - uses both the constituent-based features and the dependency-based fea-tures, but no combined features; 'CD+Top5' -obtained by adding the top-5 combined fea-tures to the 'CD' system; and similarly for the'CD+Top10' and the 'CD+Top20' systems.
And'CO' serves as the baseline in our experiments.First, we evaluate the performance of SRIusing the held-out test set with gold-standardconstituent parse trees.
The corresponding de-pendency parse trees are automatically gener-ated by the constituent-to-dependency con-verter included in the Stanford Parser.
Thetesting results of the six systems on the SRIphase are shown in Table 2.System A (%) P (%) R (%) F (%)CO 97.87 97.04 97.30 97.17DO 92.76 92.90 84.19 88.33CD 97.98 97.44 97.25 97.34CD+Top5 98.12 97.56 97.58 97.57CD+Top10 98.15 97.61 97.62 97.61CD+Top20 98.18 97.68 97.64 97.66Table 2.
Results of SRI using gold parsesIt can be seen from Table 2 that 'CD' and'CD+Top20' give only slightly improvementover 'CO' by less than 1% point.
In other words,feature combinations do not seem to be veryeffective for SRI.
Then we label all recognizedconstituents in the SRI phase with one of thesix semantic role types.
Table 3 displays the F-score of each semantic type and the overallSRC on the test set with gold-standard parses.System Arg0 Arg1 Arg2 ADV LOC TMP ALLCO 92.40 90.57 59.98 96.25 86.80 98.14 91.23DO 90.70 88.22 56.95 94.54 81.23 97.37 89.14CD 92.85 91.29 63.35 96.55 87.55 98.32 91.86CD+Top5 93.96 92.79 73.48 97.13 88.63 98.31 93.22*1CD+Top10 94.15 93.23 74.18 97.42 87.17 98.57 93.41*CD+Top20 94.10 93.19 75.13 97.23 88.05 98.48 93.46*Table 3.
Results of SRC using gold parsesTable 3 shows that the proposed methodperforms much better in SRC.
It improves theconstituent-based method by more than 2% inSRC.
The effectiveness of combined featurescan also be clearly seen because the overall F-scores of the three systems using combinedfeatures all exceed 93%, significant greaterthan the systems using singular features.
Theimprovement is noticeable for all semantic roletypes except the 'TMP' type.
It means that thedependency parses cannot provide additionalinformation to the labeling of this type.
Theresults of Table 2 and Table 3 together show1 The F-score value with an asterisk (*) indicatesthat there is a statistically significant differencebetween this system and the baseline ('CO') usingthe chi-square test (p<0.05).670that our method using combined features caneffectively improve the performance of SRLon the SRC phases, when using gold parses.5.4 SRL Using Automatic ParsesTo measure the performance of the algorithmin practical conditions, we replicate the aboveexperiments using Stanford Parser on the rawtexts of the test set, without segmentation orPOS tagging.
The dependency parses are alsogenerated from the automatic constituentparses, as described in Section 5.3.
The resultsare shown in Table 4.System A (%) P (%) R (%) F (%)CO 71.54 68.72 70.62 69.66DO 68.86 65.06 60.68 62.79CD 73.53 70.63 72.75 71.67*CD+Top5  73.62 70.69 72.98 71.82*CD+Top10 73.65 70.71 73.08 71.88*CD+Top20 73.67 70.70 73.16 71.91*Table 4.
Results of SRI using automatic parsesTable 4 shows that the proposed method isalso effective when using automatic parsesdespite the dramatic decrease in F-scores incomparison to using gold-standard parses.
Thedecline is mainly caused by the heuristic-basedpruning strategy in which a number of real ar-guments are pruned when using the constituentparses with errors.
Further analysis shows that,in SRI using gold parses, the ratio of incor-rectly pruned arguments to the total is less than2%, but the ratio jumps to 17% when usingautomatic parses.
Next, on the basis of the SRIresults, we test the performance of SRC usingthe automatic parses, as shown in Table 5.System Arg0 Arg1 Arg2 ADV LOC TMP ALLCO 89.20 88.90 54.47 93.93 81.80 94.38 88.24DO 88.79 89.32 50.21 91.27 78.26 93.86 87.63CD 89.75 89.87 57.71 95.28 84.22 94.71 89.16*CD+Top5 90.75 90.97 65.64 95.53 84.45 94.45 90.16*CD+Top10 90.96 91.37 67.25 95.31 84.49 94.61 90.45*CD+Top20 90.94 91.29 67.42 95.22 84.39 94.65 90.42*Table 5.
Results of SRC using auto parsesTable 5 shows only a slight decline in com-parison with the result of using gold-standardparses, and it maintains the same trend of per-formance for each semantic role in the Table 3,which proves the validity of the proposed me-thod when using automatic parses.
Table 6shows the F-score of the overall SRL on boththe gold-standard and the automatic parse data.System Gold Parse (F%) Auto Parse (F%)CO 89.29 63.13DO 82.69 60.34CD 90.01 65.56*CD+Top5 91.47* 66.37*CD+Top10 91.68* 66.61*CD+Top20 91.76* 66.61*Table 6.
Results of overall SRLTable 6 shows that the F-score of the'CD+Top20' surpasses that of the 'CO' systemby more than 2% on the gold parses, and morethan 3% on the automatic parse.
In other words,the method using constituent and dependencysyntactic views performs even more effectivefor the automatic parses.
The last three rows ofTable 6 shows that the top-10 combined fea-tures perform better than the top-5 features byadding 32 more features, but the top-20 com-bined features obtain similar results to the top-10 features by adding 54 more features.
It sug-gests that only several salient combined fea-tures can actually improve the performance.5.5 Combined Feature PerformanceTo evaluate the performance of each combinedfeature to identify the salient combined fea-tures for SRL, we rank the 60 combined fea-tures used by the 'CD+Top10' system on thetest data with gold-standard parses, accordingto the F-score improvement achieved by eachcombined feature.
Here we list the top 20 ofthem which are shown in Table 7.Rank Feature ?
F(%) Rank Feature ?
F(%)1 c1+c6 0.611 11 c10+d1 0.4132 c1+c10 0.593 12 c5+d26 0.4043 c4+c6 0.557 13 c24+d9 0.3954 c9+c20 0.503 14 d25+d35 0.3955 c23+c6 0.494 15 c30+d24 0.3776 c1+c3 0.458 16 c9+c26 0.3777 c9+d13 0.449 17 c10+d28 0.3688 c14+c10 0.431 18 c30+d29 0.3659 c1+c5 0.422 19 c30+d30 0.36110 c24+d33 0.413 20 c7+c6 0.361Table 7.
Top-20 combined featuresAs can be seen from Table 7, a half of com-bined features are composed by constituent671features only, and the other half contain at leastone dependency-based feature.
This indicatesthat dependency features can be helpful to con-struct combined features for SRL.
Throughanalyzing the performance of each combinedfeatures, we have obtained some new and ef-fective combined features which were not rec-ognized before, such as 'predicate+partialpath' (c1+c10), 'position+head word' (c4+c6),'Head word POS+right sibling POS' (c9+c20).Observation from these combined featuressuggests that not all combined features arecomposed by two significant basic features.Some not significant ones, such as 'partialpath' (c10) and 'Head word POS' (c9) can alsoproduce salient combined features.Furthermore, we find that the relative orderof the combined features in Table 7 is not ex-actly consistent with their orders in Table 1.The inconsistency indicates that the estimationcriteria used for combined features selection isnot perfect.
In estimation, the effect of com-bined features is evaluated simply based on thedistance between the positive and the negativedataset by considering the efficiency.
But inpractice, the effects of them are determinedthrough one-by-one classification.5.6 Comparison to Other WorkFinally, we compare the proposed method withother four representative Chinese SRL systems.First, the 'Xue1' system (Xue and Palmer, 2005)is a typical feature-based system using 9 basicfeatures, 2 combined features and the Maxi-mum Entropy (ME) classifier.
Second, the 'Liu'system (Liu et al 2007) which uses 19 basicfeatures, 10 combined features and also theME classifier.
Third, the 'Che' (Che, 2008) sys-tem use a hybrid convolution tree kernel todirectly measure the similarity between twoconstituent structures.
Fourth, the 'Xue2' sys-tem described in (Xue, 2008), which is similarto 'Xue1' on basic framework, but using a newfeature set.
The 'Xue2' system evaluates theSRL of the verbal predicates and the nominal-ized predicates separately, and offers no con-solidated evaluation in (Xue, 2008).
So in thecomparison, we refer to its performance on theverbal predicates and the nominalized predi-cates as 'Xue21' and 'Xue22'.All the four systems mentioned above usethe constituent as the labeling unit and use theCPB corpus as the data set, the same as ourmethod.
And we use the same training and testdata splits as in the 'Xue1' and 'Che' systems.Table 8 shows the comparison results in termsof F-score on both gold parses and auto parses.System Gold Parse (F%) Auto Parse (F%)Xue22 69.6 57.3Xue1 91.3 61.3Liu 91.31 ?Che 91.67 65.42Ours 91.76 66.61Xue21 92.0 66.8Table 8.
Comparison to other workTable 8 shows that our method performsbetter than the 'Xue1', 'Liu' and 'Che' systemson both gold parses and automatic parses.
It isonly slightly worse than the 'Xue21', namely theverbal predicates part of the 'Xue2' system.
Butfor the other part of the 'Xue2' system for thenominalized predicates, namely the 'Xue22', ourmethod performs much better than it.
The re-sults further verify the validity of the method.6 ConclusionsThis paper presents a novel feature-based SRLapproach for Chinese.
Compared to the tradi-tional feature-based methods, the method caneffectively integrate the constituent and thedependency syntactic views at the feature level.The method provides an effective way to con-nect two syntactic views by a statistical selec-tion method of combined features to substan-tially improve the feature-based SRL method.The complexity of the method will not increasesignificantly compared to the method usingone syntactic view as we use a constituent-to-dependency conversion rather than additionaldependency parsing.
The effectiveness of themethod has been proven by the experiments onCPB using SVM classifier with linear kernel.AcknowledgmentsThis work is supported by the Key Program ofNational Natural Science Foundation of Chinaunder Grant No.
60736014, the Key Project ofthe National High Technology Research andDevelopment Program of China under GrantNo.
2006AA010108, and the Hong Kong Poly-technic University under Grant No.
G-U297and G-U596.672ReferencesCollins F. Baker, Charles J. Fillmore, and John B.Lowe.
1998.
The Berkeley FrameNet Project.Proceedings of Coling-ACL-1998.Stephen A. Boxwell, Dennis Mehay, and ChrisBrew.
2009.
Brutus: A Semantic Role LabelingSystem Incorporating CCG, CFG, and Depend-ency Features.
Proceedings of ACL-2009.Wanxiang Che.
2008.
Kernel-based Semantic RoleLabeling.
Ph.D. Thesis.
Harbin Institute ofTechnology, Harbin, China.John Chen and Owen Rambow.
2003.
Use of DeepLinguistic Features for the Recognition and La-beling of Semantic Arguments.
Proceedings ofEMNLP-2003.Weiwei Ding and Baobao Chang.
2008.
ImprovingChinese Semantic Role Classification with Hier-archical Feature Selection Strategy.
Proceedingsof EMNLP-2008.Ronald A. Fisher.
1938.
The Statistical Utilizationof Multiple Measurements.
Annals of Eugenics,8:376-386.Daniel Gildea and Daniel Jurafsky.
2002.
Auto-matic Labeling of Semantic Roles.
Computa-tional Linguistics, 28(3):245-288.Daniel Gildea and Martha Palmer.
2002.
The Ne-cessity of Syntactic Parsing for Predicate Argu-ment Recognition.
Proceedings of ACL-2002.Kadri Hacioglu.
2004.
Semantic Role LabelingUsing Dependency Trees.
Proceedings of COL-ING-2004.Jan Hajic, Massimiliano Ciaramita, Richard Jo-hansson, et al The CoNLL-2009 Shared Task:Syntactic and Semantic Dependencies in Multi-ple Languages.
Proceedings of CoNLL-2009.Thorsten Joachims.
1999.
Making large-Scale SVMLearning Practical.
Advances in Kernel Methods.Support Vector Learning, B. Sch?lkopf and C.Burges and A. Smola (ed), MIT Press.Richard Johansson and Pierre Nugues.
2008.
De-pendency-based Semantic Role Labeling ofPropBank.
Proceedings of EMNLP-2008.Roger Levy and Christopher D. Manning.
2003.
Isit harder to parse Chinese, or the Chinese Tree-bank.
Proceedings of ACL-2003.Huaijun Liu, Wanxiang Che, and Ting Liu.
2007.Feature Engineering for Chinese Semantic RoleLabeling.
Journal of Chinese Information Proc-essing, 21(2):79-85.Marie-Catherine de Marneffe, Bill MacCartney,and Christopher D. Manning.
2006.
GeneratingTyped Dependency Parses from Phrase StructureParses.
Proceedings of LREC-2006.Truc-Vien T. Nguyen, Alessandro Moschitti, andGiuseppe Riccardi.
2009.
Convolution Kernelson Constituent, Dependency and SequentialStructures for Relation Extraction.
Proceedingsof EMNLP-2009.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The proposition Bank: An Annotated Cor-pus of Semantic Roles.
Computational Linguis-tics, 31(1):71-106Sameer Pradhan, Wayne Waed, Kadri Haciolgu,and James H. Martin.
2004.
Shallow SemanticParsing using Support Vector Machines.
Pro-ceedings of HLT/NAACL-2004Sameer Pradhan, Wayne Ward, Kadri Hacioglu,James H. Martin, and Daniel Jurafsky.
2005.Semantic Role Labeling Using Different Syntac-tic Views.
Proceedings of ACL-2005.Sameer Pradhan, Wayne Ward, and James H. Mar-tin.
2008.
Towards Robust Semantic Role Label-ing.
Computational Linguistics, 34(2): 289-310.Vasin Punyakanok, Dan Roth, Wentau Yih.
2005.The Necessity of Syntactic Parsing for SemanticRole Labeling.
Proceedings of IJCAI-2005.Mihai Surdeanu, Lluis Marquez, Xavier Carreras,and Pere R. Comas.
2007.
Combination Strate-gies for Semantic Role Labeling.
Journal ofArtificial Intelligence Research, 29:105-151.Kristina Toutanova, Aria Haghighi, and Christo-pher D. Manning.
2005.
Joint learning improvessemantic role labeling.
Proceedings of ACL-2005.Nianwen Xue and Martha Palmer.
2004.
Calibrat-ing Features for Semantic Role Labeling.
Pro-ceedings of EMNLP-2004.Nianwen Xue and Martha Palmer.
2005 Automaticsemantic role labeling for Chinese verbs.
Pro-ceedings of IJCAI-2005.Nianwen Xue.
2008.
Labeling Chinese Predicateswith Semantic Roles.
Computational Linguistics,34(2):225-255.Hai Zhao, Wenliang Chen, and Chunyu Kit.
2009.Semantic Dependency Parsing of NomBank andPropBank: An Efficient Integrated Approach viaa Large-scale Feature Selection.
Proceedings ofEMNLP-2009.673
