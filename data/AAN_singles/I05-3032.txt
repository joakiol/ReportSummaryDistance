Chinese Word Segmentation in ICT-NLPShuangLong LiInstitute of Computing Technology,BeijingUniversity of Science and Technology Beijing,Beijingslli@ict.ac.cnAbstractChinese word segmentation is alwaysmuch accounted of in ICT-NLP.
In thisbakeoff, two different systems in ICT-NLP participated.
The one isSYSTEM_#1 evaluated in three tracks-- PK-close, MSR-close and MSR-open,and SYSTEM_#2 PK-open.
Throughthis bakeoff , the development of Chi-nese segmentation is learned and theproblems are found in our systems.1 System DescriptionTwo different systems in ICT-NLP participatedthe second bakeoff.1.1 SYSTEM_#1The SYSTEM_#1 is implemented mainlybased on the log-linear model CRFs(ConditionalRandom Fields).
CRFs are arbitrary undirectedgraphical models trained to maximize the condi-tional probability of the desired outputs giventhe corresponding inputs.
We cast the segmenta-tion as one of sequence tagging.The conditional probability for the tag se-quence given a input Chinese sentenceis defined by a linear-chain CRFwith parameters1 2 ... nT t t t1 2 ... nC c c c^ `1 2 ... mO O O O  to be111( | ) exp ( , , , )nm m i ii mcP T C f t t C iZO/ ?
?
?????
?Where cZ is the per-input normalization thatmakes the probability of all state sequences sumto one.
is a feature function whichis can be any real number.1( , , , )m i if t t C iThe most probable tag sequence for an inputC,* arg max ( | )TT P T/ Cis determined using the Viterbi algorithm, An N-best list of tagging sequences is obtained usingmodi-fied Viterbi algorithm.Six tags according to the different positionsof one character in a word are used in this model,such as #START(beginning of one sentence),B(beginning of one word), M(middle of oneword), E(end of one word), and #END(end ofone sentence).The feature templates used in this model arelisted in Table 1.Description Featurecurrent state ,i it ccurrent & previous states 1, ,i it t c icurrent & two previous states 2 1, , ,i i it t t c  istate transitions 1,i it tsecond previous character2,i it c previous character 1,i it c next character 1,i it c second next character 2,i it c previous two characters 2 1, ,i i it c c next two characters 1 2, ,i i it c c previous current & next character 1 1, , ,i i i it c c t current and previous character 1, ,i i it c ccurrent and next character 1, ,i i it c c character types of current & twoprevious & two next characters2 11 2, , ,,i i i ii it T T TT T  ,Table 1.
Feature templates used in SYSTEM_#1x Any Arabia number like ??,?,???
isreplaced by mark #N;x Some punctuation like ??
,?
,???
isreplaced by mark #C;187x Any Chinese letter like ?a,?,b,???
isreplaced by mark #L;x All the characters are classified to be 7types, such as : number, letter, time-suffix,etc.In this model, the parameters are estimatedby one Perceptron Algorithm.
The parameterscan also be trained through the maximum en-tropy learning algorithms like GIS or IIS, butperformed poorly in our tests.1.2 SYSTEM_#2The SYSTEM_#2 is mainly a HHMM-basedChinese segmentation system which has partici-pated the 1st bakeoff in 2003.
And the improve-ment has been made by some post processes.The system structure is shown in Figure 1Figure 1 .
SYSTEM_#2 structureDetails of the post-processing are listed be-low.x TBL(Transformation Based Learning) isapplied for adaptation  to different seg-mentation standards.x The repeating strings extracted in the testdata using Accessor Variety correct theinconsistent segmentation.x A filtering approach to find the unknownwords in the single character strings isused.2 TrackHere ,we introduce the operation of the differenttracks.
Table 2 gives the results of the tracks.Tracks SYSTEM_#1 SYSTEM_#2PK-closeP: 0.942R: 0.938F: 0.940--PK-open --P: 0.961R: 0.944F: 0.952MSR-closeP: 0.942R: 0.948F: 0.945--MSR-openP: 0.933R: 0.916F: 0.924--Table 2 .
The results of the tracks2.1 Closed TracksOnly the SYSTEM_#1 participated theclosed tracks(PK and MSR) becauseSYSTEM_#2 must utilize the POS informationin the entity recognition process.All the features information used in theSYSTEM_#1 were trained automatic for onetime.We split the training randomly to two parts?80% used for training and 20% used for estimat-ing the parameters.2.2 Open TracksWe participated two open tracks--SYSTEM_#1 MSR and SYSTEM_#2 PK.For SYSTEM_#1, an external dictionarywhich contains 140,332 entries with POS is used.However, because of the standard conflict, theopen result is weaker than the closed result.For SYSTEM_#2, the system is trained onsix-months tagged news corpus of People Daily1998 and an external dictionary which contains22,858 entries with POS.3 ConclusionThe result in this bakeoff is  not so satisfied asthere are also many problems in our systems.Through this bakeoff, we learn more about thethe development of Chinese segmentation.
Sothe future research is needed to improve ourwork.SentenceClass-based HMMRoleTag-basedEntity RecognitionClass-based HMMPost-processingResult188
