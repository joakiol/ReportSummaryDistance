Proceedings of the 53rd Annual Meeting of the Association for Computational Linguisticsand the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 231?236,Beijing, China, July 26-31, 2015.c?2015 Association for Computational LinguisticsGrounding Semantics in Olfactory PerceptionDouwe Kiela, Luana Bulat and Stephen ClarkComputer LaboratoryUniversity of Cambridgedouwe.kiela,ltf24,stephen.clark@cl.cam.ac.ukAbstractMulti-modal semantics has relied on fea-ture norms or raw image data for per-ceptual input.
In this paper we examinegrounding semantic representations in ol-factory (smell) data, through the construc-tion of a novel bag of chemical compoundsmodel.
We use standard evaluations formulti-modal semantics, including measur-ing conceptual similarity and cross-modalzero-shot learning.
To our knowledge, thisis the first work to evaluate semantic sim-ilarity on representations grounded in ol-factory data.1 IntroductionDistributional semantics represents the meaningsof words as vectors in a ?semantic space?, rely-ing on the distributional hypothesis: the idea thatwords that occur in similar contexts tend to havesimilar meanings (Turney and Pantel, 2010; Clark,2015).
Although these models have been success-ful, the fact that the meaning of a word is repre-sented as a distribution over other words impliesthey suffer from the grounding problem (Harnad,1990); i.e.
they do not account for the fact thathuman semantic knowledge is grounded in phys-ical reality and sensori-motor experience (Louw-erse, 2008).Multi-modal semantics attempts to address thisissue and there has been a surge of recentwork on perceptually grounded semantic models.These models learn semantic representations fromboth textual and perceptual input and outperformlanguage-only models on a range of tasks, includ-ing modelling semantic similarity and relatedness,and predicting compositionality (Silberer and La-pata, 2012; Roller and Schulte im Walde, 2013;Bruni et al, 2014).
Perceptual information is ob-tained from either feature norms (Silberer and La-pata, 2012; Roller and Schulte im Walde, 2013;Hill and Korhonen, 2014) or raw data sources suchas images (Feng and Lapata, 2010; Leong and Mi-halcea, 2011; Bruni et al, 2014; Kiela and Bottou,2014).
The former are elicited from human anno-tators and thus tend to be limited in scope and ex-pensive to obtain.
The latter approach has the ad-vantage that images are widely available and easyto obtain, which, combined with the ready avail-ability of computer vision methods, has led to rawvisual information becoming the de-facto percep-tual modality in multi-modal models.However, if our objective is to ground seman-tic representations in perceptual information, whystop at image data?
The meaning of lavender isprobably more grounded in its smell than in thevisual properties of the flower that produces it.Olfactory (smell) perception is of particular in-terest for grounded semantics because it is muchmore primitive compared to the other perceptualmodalities (Carmichael et al, 1994; Krusemark etal., 2013).
As a result, natural language speak-ers might take aspects of olfactory perception ?forgranted?, which would imply that text is a rel-atively poor source of such perceptual informa-tion.
A multi-modal approach would overcomethis problem, and might prove useful in, for ex-ample, metaphor interpretation (the sweet smell ofsuccess; rotten politics) and cognitive modelling,as well as in real-world applications such as au-tomatically retrieving smells or even producingsmell descriptions.
Here, we explore groundingsemantic representations in olfactory perception.We obtain olfactory representations by con-structing a novel bag of chemical compounds(BoCC) model.
Following previous work in multi-modal semantics, we evaluate on well known con-ceptual similarity and relatedness tasks and onzero-shot learning through induced cross-modalmappings.
To our knowledge this is the firstwork to explore using olfactory perceptual data forgrounding linguistic semantic models.231Olfactory-Relevant ExamplesMEN sim SimLex-999 simbakery bread 0.96 steak meat 0.75grass lawn 0.96 flower violet 0.70dog terrier 0.90 tree maple 0.55bacon meat 0.88 grass moss 0.50oak wood 0.84 beach sea 0.47daisy violet 0.76 cereal wheat 0.38daffodil rose 0.74 bread flour 0.33Table 1: Examples of pairs in the evaluationdatasets where olfactory information is relevant,together with the gold-standard similarity score.2 TasksFollowing previous work in grounded semantics,we evaluate performance on two tasks: conceptualsimilarity and cross-modal zero-shot learning.2.1 Conceptual similarityWe evaluate the performance of olfactory multi-modal representations on two well-known similar-ity datasets: SimLex-999 (Hill et al, 2014) and theMEN test collection (Bruni et al, 2014).
Thesedatasets consist of concept pairs together with ahuman-annotated similarity score.
Model perfor-mance is evaluated using the Spearman ?scorre-lation between the ranking produced by the cosineof the model-derived vectors and that produced bythe gold-standard similarity scores.Evidence suggests that the inclusion of visualrepresentations only improves performance forcertain concepts, and that in some cases the in-troduction of visual information is detrimental toperformance on similarity and relatedness tasks(Kiela et al, 2014).
The same is likely to be truefor other perceptual modalities: in the case of acomparison such as lily-rose, the olfactory modal-ity certainly is meaningful, while this is probablynot the case for skateboard-swimsuit.
Some exam-ples of relevant pairs can be found in Table 1.Hence, we had two annotators rate the twodatasets according to whether smell is relevant tothe pairwise comparison.
The annotation criterionwas as follows: if both concepts in a pairwise com-parison have a distinctive associated smell, thenthe comparison is relevant to the olfactory modal-ity.
Only if both annotators agree is the com-parison deemed olfactory-relevant.
This annota-tion leads to a total of four evaluation sets: theMEN test collection MEN (3000 pairs) and itsolfactory-relevant subset OMEN (311 pairs); andthe SimLex-999 dataset SLex (999 pairs) and itsolfactory-relevant subset OSLex (65 pairs).
Theinter-annotator agreement on the olfactory rele-vance judgments was high (?
= 0.94 for the MENtest collection and ?
= 0.96 for SimLex-999).12.2 Cross-modal zero-shot learningCross-modal semantics, instead of being con-cerned with improving semantic representationsthrough grounding, focuses on the problem of ref-erence.
Using, for instance, mappings betweenvisual and textual space, the objective is to learnwhich words refer to which objects (Lazaridou etal., 2014).
This problem is very much related tothe object recognition task in computer vision, butinstead of using just visual data and labels, thesecross-modal models also utilize textual informa-tion (Socher et al, 2014; Frome et al, 2013).
Thisapproach allows for zero-shot learning, where themodel can predict how an object relates to otherconcepts just from seeing an image of the object,but without ever having seen the object previously(Lazaridou et al, 2014).We evaluate cross-modal zero-shot learningperformance through the average percentage cor-rect at N (P@N), which measures how many of thetest instances were ranked within the top N high-est ranked nearest neighbors.
A chance baseline isobtained by randomly ranking a concept?s nearestneighbors.
We use partial least squares regression(PLSR) to induce cross-modal mappings from thelinguistic to the olfactory space and vice versa.2Due to the nature of the olfactory data source(see Section 3), it is not possible to build olfac-tory representations for all concepts in the test sets.However, cross-modal mappings yield an addi-tional benefit: since linguistic representations havefull coverage over the datasets, we can projectfrom linguistic space to perceptual space to alsoobtain full coverage for the perceptual modalities.This technique has been used to increase coveragefor feature norms (Fagarasan et al, 2015).
Con-sequently, we are in a position to compare percep-tual spaces directly to each other, and to linguistic1To facilitate further work in multi-modal semantics be-yond vision, our code and data have been made publiclyavailable at http://www.cl.cam.ac.uk/?dk427/aroma.html.2To avoid introducing another parameter, we set the num-ber of latent variables in the cross-modal PLSR map to a thirdof the number of dimensions of the perceptual representation.232Chemical CompoundPhenethylacetateIsoamylbutyrateAnisylbutyrateMyrceneSyringaldehydeMelon 3 3Pineapple 3 3Licorice 3Anise 3 3SmelllabelBeer 3 3Table 2: A BoCC model.space, over the entire dataset, as well as on the rel-evant olfactory subsets.
When projecting into sucha space and reporting results, the model is prefixedwith an arrow (?)
in the corresponding table.3 Olfactory PerceptionThe Sigma-Aldrich Fine Chemicals flavors andfragrances catalog3(henceforth SAFC) is one ofthe largest publicly accessible databases of se-mantic odor profiles that is used extensively infragrance research (Zarzo and Stanton, 2006).It contains organoleptic labels and the chemicalcompounds?or more accurately the perfume rawmaterials (PRMs)?that produce them.
By auto-matically scraping the catalog we obtained a totalof 137 organoleptic smell labels from SAFC, witha total of 11,152 associated PRMs.
We also exper-imented with Flavornet4and the LRI and odourdatabase5, but found that the data from these weremore noisy and generally of lower quality.For each of the smell labels in SAFC we countthe co-occurrences of associated chemical com-pounds, yielding a bag of chemical compounds(BoCC) model.
Table 2 shows an example sub-space of this model.
Although the SAFC cata-log is considered sufficiently comprehensive forfragrance research (Zarzo and Stanton, 2006), thefact that PRMs usually occur only once per smelllabel means that the representations are rathersparse.
Hence, we apply dimensionality reduc-tion to the original representation to get denser3http://www.sigmaaldrich.com/industries/flavors-and-fragrances.html4http://www.flavornet.org5http://www.odour.org.ukFigure 1: Performance of olfactory representa-tions when using SVD to reduce the number ofdimensions.Dataset Linguistic BoCC-Raw BoCC-SVDOMEN (35) 0.40 0.42 0.53Table 3: Comparison of olfactory representationson the covered OMEN dataset.vectors.
We call the model without any dimen-sionality reduction BOCC-RAW and use singu-lar value decomposition (SVD) to create an ad-ditional BOCC-SVD model with reduced dimen-sionality.
Positive pointwise mutual information(PPMI) weighting is applied to the raw space be-fore performing dimensionality reduction.The number of dimensions in human olfactoryspace is a hotly debated topic in the olfactorychemical sciences (Buck and Axel, 1991; Zarzoand Stanton, 2006).
Recent studies involvingmulti-dimensional scaling on the SAFC catalogrevealed approximately 32 dimensions in olfactoryperception space (Mamlouk et al, 2003; Mamloukand Martinetz, 2004).
We examine this findingby evaluating the Spearman ?scorrelation on thepairs of OMEN that occur in the SAFC database(35 pairs).
The coverage on SimLex was not suffi-cient to also try that dataset (only 5 pairs).
Figure1 shows the results.
It turns out that the best olfac-tory representations are obtained with 30 dimen-sions.
In other words, our findings appear to cor-roborate recent evidence suggesting that olfactoryspace (at least when using SAFC as a data source)is best modeled using around 30 dimensions.3.1 Linguistic representationsFor the linguistic representations we use the con-tinuous vector representations from the log-linearskip-gram model of Mikolov et al (2013), specif-ically the 300-dimensional vector representationstrained on part of the Google News dataset (about100 billion words) that have been released on the233MEN OMEN SLex OSLexLinguistic 0.78 0.38 0.44 0.30?BoCC-Raw 0.38 0.36 0.19 0.23?BoCC-SVD 0.46 0.51 0.23 0.48Multi-modal 0.69 0.53 0.40 0.49Table 4: Comparison of linguistic, olfactory andmulti-modal representations.Mapping P@1 P@5 P@20 P@50Chance 0.0 3.76 13.53 36.09Olfactory ?
Ling.
1.51 8.33 24.24 47.73Ling.
?
Olfactory 4.55 15.15 43.18 67.42Table 5: Zero-shot learning performance forBoCC-SVD.Word2vec website.63.2 Conceptual SimilarityResults on the 35 covered pairs of OMEN for thetwo BoCC models are reported in Table 3.
Ol-factory representations outperform linguistic rep-resentations on this subset.
In fact, linguistic rep-resentations perform poorly compared to their per-formance on the whole of MEN.
The SVD modelperforms best, improving on the linguistic and rawmodels with a 33% and 26% relative increase inperformance, respectively.We use a cross-modal PLSR map, trained onall available organoleptic labels in SAFC, to ex-tend coverage and allow for a direct compari-son between linguistic representations and cross-modally projected olfactory representations on theentire datasets and relevant subsets.
The resultsare shown in Table 4.
As might be expected, lin-guistic performs better than olfactory on the fulldatasets.
On the olfactory-relevant subsets, how-ever, the projected BOCC-SVD model outper-forms linguistic for both datasets.
Performance in-creases even further when the two representationsare combined into a multi-modal representation byconcatenating the L2-normalized linguistic and ol-factory (?BOCC-SVD) vectors.3.3 Zero-shot learningWe learn a cross-modal mapping between the twospaces and evaluate zero-shot learning.
We use all137 labels in the SAFC database that have corre-sponding linguistic vectors for the training data.6https://code.google.com/p/word2vec/apple bacon brandy cashewpear smoky rum hazelnutbanana roasted whiskey peanutmelon coffee wine-like almondapricot mesquite grape hawthornepineapple mossy fleshy jamchocolate lemon cheese caramelcocoa citrus grassy nuttysweet geranium butter roastedcoffee grapefruit oily maplelicorice tart creamy butterscotchroasted floral coconut coffeeTable 6: Example nearest neighbors for BoCC-SVD representations.For each term, we train the map on all other la-bels and measure whether the correct instance isranked within the top N neighbors.
We use theBOCC-SVD model for the olfactory space, sinceit performed best on the conceptual similarity task.Table 5 shows the results.
It appears that mappinglinguistic to olfactory is easier than mapping olfac-tory to linguistic, which may be explained by thedifferent number of dimensions in the two spaces.One could say that it is easier to find the chemicalcomposition of a ?smelly?
word from its linguisticrepresentation, than it is to linguistically representor describe a chemical composition.3.4 Qualitative analysisWe also examined the BoCC representations qual-itatively.
As Table 6 shows, the nearest neigh-bors are remarkably semantically coherent.
Thenearest neighbors for bacon and cheese, for ex-ample, accurately sum up how one might describethose smells.
The model also groups together nutsand fruits, and expresses well what chocolate andcaramel smell (or taste) like.4 ConclusionsWe have studied grounding semantic representa-tions in raw olfactory perceptual information.
Weused a bag of chemical compounds model to ob-tain olfactory representations and evaluated onconceptual similarity and cross-modal zero-shotlearning, with good results.
It is possible that theolfactory modality is well-suited to other forms ofevaluation, but in this initial work we chose to fol-low standard practice in multi-modal semantics toallow for a direct comparison.234This work opens up interesting possibilities inanalyzing smell and even taste.
It could be appliedin a variety of settings beyond semantic similarity,from chemical information retrieval to metaphorinterpretation to cognitive modelling.
A specula-tive blue-sky application based on this, and othermulti-modal models, would be an NLG applica-tion describing a wine based on its chemical com-position, and perhaps other information such as itscolor and country of origin.AcknowledgementsDK is supported by EPSRC grant EP/I037512/1.LB is supported by an EPSRC Doctoral Train-ing Grant.
SC is supported by ERC Start-ing Grant DisCoTex (306920) and EPSRC grantEP/I037512/1.
We thank the anonymous review-ers for their helpful comments and Flaviu Bulatfor providing useful feedback.ReferencesElia Bruni, Nam-Khanh Tran, and Marco Baroni.2014.
Multimodal distributional semantics.
Journalof Artifical Intelligence Research, 49:1?47.Linda Buck and Richard Axel.
1991.
A novel multi-gene family may encode odorant receptors: a molec-ular basis for odor recognition.
Cell, 65(1):175?187.S.
Thomas Carmichael, M.-C. Clugnet, and Joseph L.Price.
1994.
Central olfactory connections in themacaque monkey.
Journal of Comparative Neurol-ogy, 346(3):403?434.Stephen Clark.
2015.
Vector Space Models of LexicalMeaning.
In Shalom Lappin and Chris Fox, editors,Handbook of Contemporary Semantics, chapter 16.Wiley-Blackwell, Oxford.Luana Fagarasan, Eva Maria Vecchi, and StephenClark.
2015.
From distributional semantics to fea-ture norms: grounding semantic models in humanperceptual data.
In Proceedings of the 11th Inter-national Conference on Computational Semantics(IWCS 2015), pages 52?57, London, UK.Yansong Feng and Mirella Lapata.
2010.
Visual infor-mation in semantic representation.
In Proceedingsof NAACL, pages 91?99.Andrea Frome, Gregory S. Corrado, Jonathon Shlens,Samy Bengio, Jeffrey Dean, Marc?Aurelio Ranzato,and Tomas Mikolov.
2013.
DeViSE: A DeepVisual-Semantic Embedding Model.
In Proceedingsof NIPS, pages 2121?2129.Stevan Harnad.
1990.
The symbol grounding problem.Physica D, 42:335?346.Felix Hill and Anna Korhonen.
2014.
Learning ab-stract concept embeddings from multi-modal data:Since you probably can?t see what I mean.
In Pro-ceedings of EMNLP, pages 255?265.Felix Hill, Roi Reichart, and Anna Korhonen.2014.
SimLex-999: Evaluating semantic mod-els with (genuine) similarity estimation.
CoRR,abs/1408.3456.Douwe Kiela and L?eon Bottou.
2014.
Learning imageembeddings using convolutional neural networks forimproved multi-modal semantics.
In Proceedings ofEMNLP, pages 36?45.Douwe Kiela, Felix Hill, Anna Korhonen, and StephenClark.
2014.
Improving multi-modal representa-tions using image dispersion: Why less is sometimesmore.
In Proceedings of ACL, pages 835?841.Elizabeth A Krusemark, Lucas R Novak, Darren RGitelman, and Wen Li.
2013.
When the sense ofsmell meets emotion: anxiety-state-dependent olfac-tory processing and neural circuitry adaptation.
TheJournal of Neuroscience, 33(39):15324?15332.Angeliki Lazaridou, Elia Bruni, and Marco Baroni.2014.
Is this a wampimuk?
Cross-modal map-ping between distributional semantics and the visualworld.
In Proceedings of ACL, pages 1403?1414.Chee Wee Leong and Rada Mihalcea.
2011.
Goingbeyond text: A hybrid image-text approach for mea-suring word relatedness.
In Proceedings of IJCNLP,pages 1403?1407.Max M. Louwerse.
2008.
Symbol interdependency insymbolic and embodied cognition.
Topics in Cogni-tive Science, 59(1):617?645.Amir Madany Mamlouk and Thomas Martinetz.
2004.On the dimensions of the olfactory perception space.Neurocomputing, 58:1019?1025.Amir Madany Mamlouk, Christine Chee-Ruiter, Ul-rich G Hofmann, and James M Bower.
2003.
Quan-tifying olfactory perception: Mapping olfactory per-ception space by using multidimensional scaling andself-organizing maps.
Neurocomputing, 52:591?597.Tomas Mikolov, Kai Chen, Greg Corrado, and JeffreyDean.
2013.
Efficient estimation of word repre-sentations in vector space.
In Proceedings of ICLR,Scottsdale, Arizona, USA.Stephen Roller and Sabine Schulte im Walde.
2013.A multimodal LDA model integrating textual, cog-nitive and visual modalities.
In Proceedings ofEMNLP, pages 1146?1157.Carina Silberer and Mirella Lapata.
2012.
Groundedmodels of semantic representation.
In Proceedingsof EMNLP, pages 1423?1433.235Richard Socher, Andrej Karpathy, Quoc V. Le, Christo-pher D. Manning, and Andrew Y. Ng.
2014.Grounded compositional semantics for finding anddescribing images with sentences.
Transactions ofACL, 2:207?218.Peter D. Turney and Patrick Pantel.
2010.
FromFrequency to Meaning: vector space models of se-mantics.
Journal of Artifical Intelligence Research,37(1):141?188, January.Manuel Zarzo and David T. Stanton.
2006.
Identi-fication of latent variables in a semantic odor pro-file database using principal component analysis.Chemical Senses, 31(8):713?724.236
