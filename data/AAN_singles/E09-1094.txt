Proceedings of the 12th Conference of the European Chapter of the ACL, pages 826?834,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsA robust and extensible exemplar-based model of thematic fitBram Vandekerckhovea, Dominiek Sandraa, Walter DaelemansbaCenter for Psycholinguistics, bCenter for Dutch Language and Speech (CNTS)University of AntwerpAntwerp, Belgium{bram.vandekerckhove,dominiek.sandra,walter.daelemans}@ua.ac.beAbstractThis paper presents a new, exemplar-basedmodel of thematic fit.
In contrast to pre-vious models, it does not approximatethematic fit as argument plausibility or?fit with verb selectional preferences?, butdirectly as semantic role plausibility fora verb-argument pair, through similarity-based generalization from previously seenverb-argument pairs.
This makes themodel very robust for data sparsity.
Weargue that the model is easily extensible toa model of semantic role ambiguity reso-lution during online sentence comprehen-sion.The model is evaluated on human seman-tic role plausibility judgments.
Its predic-tions correlate significantly with the hu-man judgments.
It rivals two state-of-the-art models of thematic fit and exceeds theirperformance on previously unseen or low-frequency items.1 IntroductionThematic fit (or semantic role plausibility) is theplausibility of a noun phrase referent playing aspecific semantic role (like agent or patient) inthe event denoted by a verbal predicate, e.g.
theplausibility that a judge sentences someone (whichmakes the judge the agent of the sentencing event)or that a judge is sentenced him- or herself (whichmakes the judge the patient).
Thematic fit has beenan important concept in psycholinguistics as a pre-dictor variable in models of human sentence com-prehension, either to discriminate between pos-sible structural analyses during initial processingin constraint-based models (see MacDonald andSeidenberg (2006) for a recent overview), or af-ter initial syntactic processing in modular models(e.g.
Frazier (1987)).
In fact, thematic fit is at thecore of the most-studied of all structural ambiguityphenomena, the ambiguity between a main clauseor a reduced relative clause interpretation of an NPverb-ed sequence (the MV/RR ambiguity), whichis essentially a semantic role ambiguity.
If thetemporarily ambiguous sentence The judge sen-tenced ... is continued as a main clause (e.g.
Thejudge sentenced him to 10 years in prison), thenoun phrase the judge would be the agent of theverb sentenced, while it would be the patient ofsentenced in a reduced relative clause continuation(e.g.
The judge sentenced to 4 years in prison forindecent exposure could also lose his state pen-sion).
Apart from its importance in psycholinguis-tics, the concept of thematic fit is also relevant forcomputational linguistics in general (see Pado?
etal.
(2007) for some examples).A number of models that try to capture hu-man thematic fit preferences have been developedin recent years (Resnik, 1996; Pado?
et al, 2006;Pado?
et al, 2007).
These previous approaches relyon the linguistic notion of verb selectional pref-erences.
The plausibility that an argument playsa specific semantic role in the event denoted bya verb?in other words, that a verb, role and argu-ment occur together?is predicted by how well theargument head fits the restrictions that the verb im-poses on the argument candidates for the semanticrole slot under consideration (e.g.
eat prefers ed-ible arguments to fill its patient slot).
Therefore,what these models capture is actually not seman-tic role plausibility, but argument plausibility.The model presented here takes a different ap-proach.
Instead of predicting the plausibility of anargument given a verb-role pair (e.g.
the plausi-bility of judge given sentence-patient), it predictsthe plausibility of a semantic role given a verb-argument pair (e.g.
the plausibility of patient givensentence-judge), through similarity-based general-ization from previously seen verb-argument pairs.In the context of modeling thematic fit as a con-826straint in the resolution of sentence-level ambigu-ity problems like the MV/RR ambiguity, predict-ing role fit instead of argument fit seems to be themost straightforward approach.
After all, whenthematic fit is approached in this way, the modeldirectly captures the semantic role ambiguity thatis at stake during the analysis of sentences that aretemporarily ambiguous between a main clause anda reduced relative interpretation.
This means thatour model of thematic fit should be very easy toextend into a full-blown model of the resolutionof any sentence-level ambiguity that crucially re-volves around a semantic role ambiguity.
In ad-dition, the fact that it generalizes from previouslyseen verb-argument pairs, based on their similarityto the target pair, should make it more robust thanprevious approaches.The remainder of the paper is organized as fol-lows: in the next section, we briefly discuss twostate-of-the-art thematic fit models, the perfor-mance of which will be compared to that of ourmodel.
Section 3 introduces three different instan-tiations of our model.
The evaluation of the modeland the comparison of its performance with that ofthe models discussed in Section 2 is presented inSection 4.
Section 5 ties everything together withsome general conclusions.2 Previous modelsIn this section of the paper, we look at two state-of-the-art models of thematic fit, developed by Pado?et al (2006) and Pado?
et al (2007).
We willnot discuss the selectional preferences model ofResnik (1996), but for a comparison between theResnik model and the Pado?
models, see Pado?
et al(2007).2.1 Pado?
et al (2006)In their model of thematic fit, Pado?
et al (2006)use FrameNet thematic roles (Fillmore et al,2003) to approximate semantic roles.
The the-matic fit of a verb-role-argument triple (v, r, a) isgiven by the joint probability of the role r, the ar-gument headword a, the verb sense vs, and thegrammatical function gf of a:Plausibilityv,r,a = P (vs, r, a, gf) (1)Since computing this joint probability from cor-pus co-occurrence frequencies is problematic dueto an obvious sparse data issue, the term isdecomposed into several subterms, including aterm P (a|vs, gf, r) that captures selectional pref-erences.
Good-Turing and class-based smoothingare used to further alleviate the remaining sparsedata problem, but because of the fact that themodel can only make predictions for verbs that oc-cur in the small FrameNet corpus, for a large num-ber of verbs, it cannot provide any output.
For theverbs that do occur in the training corpus, how-ever, the model?s predictions correlate very wellwith human plausibility ratings.2.2 Pado?
et al (2007)The model of Pado?
et al (2007) does not use se-mantically annotated resources, but approximatesthe agent and patient relations with the syntac-tic subject and object relations, respectively.
Theplausibility of a verb-role-argument triple (v, r, a)is found by calculating the weighted mean seman-tic similarity of the argument headword a to allheadwords that have previously been seen togetherwith the verb-role pair (v, r), as shown in Equa-tion 2.
The prediction is that high semantic sim-ilarity of a target headword a to seen headwordsfor a given (v, r) tuple corresponds to high the-matic fit of the (v, r, a) tuple, while low similarityimplies low thematic fit.Plausibilityv,r,a =?a??Seenr(v)w(a?)?
sim(a, a?)|Seenr(v)|(2)w(a?)
is the weighting factor.
Pado?
et al (2007)used the frequency of the previously seen ar-gument headwords as weights.
Similarity be-tween headwords was defined as the cosine be-tween so-called ?dependency vector?
representa-tions of these headwords (Pado?
and Lapata, 2007).These vectors are constructed from the frequencycounts with which the target items occur at oneend of specific paths in a corpus of syntactic de-pendency trees.
The argument headword vectorsPado?
et al (2007) used in their experiments con-sisted of 2000 features, representing the most fre-quent (head, subject) and (head, object) pairs inthe British National Corpus (BNC).
The feature-values of the headword vectors were the log-likelihoods of the headwords occurring at the de-pendent end of these (relation, head) pairs (soeither as subjects or objects of the heads).
Themodel?s performance approaches that of the Pado?et al (2006) model on the correlation of its predic-tions with human ratings, and it attains higher cov-827erage (it can provide plausibility values for a largerproportion of the test items), since the model onlyrequires that the verb occurs with subject and ob-ject arguments in the training corpus, and that thetarget argument headwords occur in the trainingdata frequently enough to attain reliable depen-dency vectors.3 Exemplar-based modeling of thematicfitExemplar-based models of cognition (also knownas Memory-Based Learning or instance/case-based reasoning/learning models) (Fix andHodges, 1951; Cover and Hart, 1967; Daelemansand van den Bosch, 2005) are classificationmodels that extrapolate their behavior from storedrepresentations of earlier experiences to newsituations, based on the similarity of the old andthe new situation.
These models keep a databaseof stored exemplars and refer to that database toguide their behavior in new situations.
Modelscan extrapolate from only one similar memoryexemplar, a group of similar exemplars (a nearestneighbor set), or even the whole exemplar mem-ory, using some decay function to give less weightto less similar exemplars.Applied to our model of thematic fit, this meansthat the model should have a database in which se-mantic representations of verb-argument pairs arestored together with the semantic roles of the ar-guments.
The plausibility of a semantic role givena new verb-argument pair is then determined bythe support for that role among the verb-argumentpairs in memory that are semantically most similarto the target pair.An immediately obvious advantage of this ap-proach should be its potential robustness for datasparsity, since similarity-based smoothing is an in-trinsic part of the model.
Even if neither the verbnor the argument of a verb-argument pair occurin the exemplar memory, role plausibilities can bepredicted, as long as the similarity of the target ex-emplar?s semantic representation with the seman-tic representations in the exemplar memory can becalculated.
An additional advantage of similarity-based smoothing is that it does not involve the es-timation of an exponential number of smoothingparameters, as is the case for backed-off smooth-ing methods (Zavrel and Daelemans, 1997).For this study, we will implement three differentkinds of exemplar-based models.
The first modelis a basic k-Nearest Neighbor (k-NN) model.
Inthis model, the plausibility rating for a semanticrole given a verb-argument pair is simply deter-mined by the (relative) frequency with which thatsemantic role is assigned to the k verb-argumentpairs that are nearest (i.e.
most similar) to the tar-get verb-argument pair (these exemplars constitutethe nearest neighbor set).
The second model addsa decay function to this simple k-NN model, sothat not only the role frequency, but also the ab-solute semantic distance between the target itemand the neighbors in the nearest neighbor set de-termine the plausibility rating.
In the third model,a normalization factor ensures that distance of theexemplars in the nearest neighbor set to the targetitem determines their weight in the calculation ofthe plausibility rating while factoring out an effectof absolute distance.The semantic distance between two verb-argument exemplars is determined by the seman-tic distance between the verbs and between thenouns.
In all models described below, the distancebetween two exemplars i and j (dij) is given bythe sum of the weighted distances (?)
between thesemantic representations of the exemplars?
nouns(n) and verbs (v):dij = wv ?
?
(vi, vj) + wn ?
?
(ni, nj) (3)We are not theoretically committed to any spe-cific semantic representation or similarity metricfor the computation of ?
(vi, vj) and ?
(ni, nj).
Theonly requirement is that they should be able to dis-tinguish nouns that typically occur in the samecontexts, but in different roles (like writer andbook), which probably excludes all vector-basedapproaches that do not take into account syntacticinformation (see also Pado?
et al (2007)).In the next three sections, each of the threeexemplar-based models is discussed in more de-tail.3.1 A basic k-NN modelThe most basic of all exemplar-based models is ak-NN model in which the preference strength of aclass upon presentation of a stimulus is simply therelative frequency of that class among the nearestneighbors of the stimulus.
In the context of the-matic fit, this means that the preference strength(PS) for a semantic role response J given a verb-argument stimulus i is found by summing the fre-quencies of all exemplars with semantic role J828Verb Noun Role Ratingsentence judge agent 6.9sentence judge patient 1.3sentence criminal agent 1.3sentence criminal patient 6.7Table 1: Example mean thematic fit ratings fromMcRae et al (1998)among the k nearest neighbors of i (CkJ ) and di-viding this by the total number of exemplars inthe k-nearest neighbor set, with k (the number ofnearest neighbors taken into consideration) beinga free parameter:PS(RJ |Si) =?j?CkJf(j)?l?Ck f(l)(4)We will call this model the k-NN frequency model(henceforth kNNf).3.2 A distance decay modelThe kNNf model uses the similarity between thetarget exemplar and the memory exemplars onlyto determine which items belong to the nearestneighbor set.
Whether these nearest neighbors arevery similar or only slightly similar to the targetexemplar, or whether there are some very similaritems but also some very dissimilar items amongthose neighbors does not have any influence onthe class?s preference strength; only relative fre-quency within the nearest neighbor set counts.Only relying on the relative frequency of se-mantic roles within the nearest neighbor set to pre-dict their plausibilities might indeed be a reason-able approach to modeling thematic fit in a lot ofcases.
Being a good agent for a given verb of-ten entails being a bad patient for that same verb(or even in general), and the other way around.For example, judge is a very plausible agent ofthe verb sentence, while at the same time it is arather unlikely patient of the same verb, while itis exactly the other way around for criminal, asthe mean participant ratings (on a 7-point scale)in Table 1 show (these were taken from McRaeet al (1998)).
The relative frequencies of theagent and patient roles in the nearest neighbor setcould in theory perfectly explain these ratings: ahigh relative frequency of the agent role amongthe nearest neighbors of the verb-argument pair(sentence, judge) should correspond to a highrating for the role, and implies low relative fre-quencies for other roles such as the patient role,which means the patient role should receive a lowrating.
For (sentence, criminal) this works inexactly the opposite way.Solely relying on the the relative semantic rolefrequencies in the nearest neighbor set might notalways work, though, since it implies that plausi-bility ratings for different roles are always com-pletely dependent on and therefore perfectly pre-dictable from each other: high plausibility for acertain semantic role given a verb-argument pairalways implies low plausibility for the other rolesin the nearest neighbor set, and low plausibility forone semantic role invariably means higher plausi-bility for the other ones.
However, nouns can alsobe more or less equally good as agents and patientsfor a given verb?one is hopefully as likely to behelped by a friend as to help a friend oneself?or equally bad?houses only kill in horror movies,and ?to kill a house?
can only be made sense of in ametaphorical way.
Therefore, we also implement amodel that takes distance into account for its plau-sibility ratings.
The basic idea is that a seman-tic role will receive a lower rating as the nearestneighbors supporting that role become less simi-lar to the target item.
The plausibility rating fora semantic role given a verb-argument pair in thismodel is a joint function of:1. the frequency with which the role occurs inthe set of memory exemplars that are seman-tically most similar to the target pair2.
the target pairs similarity to those exemplarsWe will call this model the Distance Decay model(henceforth DD).Formally, the preference strength (PS) for a se-mantic role J (RJ ) given a verb-argument tuple i(Si) is found by summing the distance-weightedfrequency of all exemplars with semantic role J inthe nearest neighbor set (CkJ ):PS(RJ |Si) =?j?CkJf(j)?
?j (5)The weight of an exemplar j (?j) is given by anexponential decay function, taken from Shepard(1987), over the distance between that exemplarand the target exemplar i (dij):?j = e??
?dij (6)829In Equation 6, the free parameter ?
determines therate of decay over dij .
Higher values of ?
result ina faster drop in similarity as dij increases.3.3 A normalized distance decay modelIn Equation 5, we do not include a denominatorthat sums over the similarity strengths of all ex-emplars in the nearest neighbor set, because wewant to keep the absolute effect of distance intothe formula, so as to be able to accurately pre-dict the bad fit of both the agent and patient rolesfor verb-argument pairs like (kill, house) or thegood fit of both agent and patient roles for a pairlike (help, friend).
To find out whether a non-normalized model is indeed a better predictor ofthematic fit than a normalized model, we alsorun experiments with a normalized version of themodel presented in Section 3.2:PS(RJ |Ti) =?j?CkJf(j)?
?j?l?Ck f(l)?
?l(7)Someone familiar with the literature on humancategorization behavior might recognize Equation7; this model is actually simply a GeneralizedContext Model (GCM) (Nosofsky, 1986), with the?context?
being restricted to the k nearest neigh-bors of the target item.
Therefore, we will refer tothis model using the shorthand kGCM.4 Evaluation4.1 The task: predicting human plausibilityjudgmentsThe model is evaluated by comparing its predic-tions to thematic fit or semantic role plausibilityjudgments from two rating experiments with hu-man subjects.
In these tasks, participants had torate the plausibility of verb-role-argument tripleson a scale from 1 to 7.
They were asked ques-tions like How common is it for a judge to sen-tence someone?, in which judge is the agent, orHow common is it for a judge to be sentenced?, inwhich judge is the patient.
The prediction is thatmodel preference strengths of semantic roles givenspecific verb-argument pairs should correlate pos-itively with participant ratings for the correspond-ing verb-role-argument triples.4.2 Training the modelIn exemplar-based models, training the modelsimply amounts to storing exemplars in memory.Our model uses an exemplar memory that consistsof 133566 verb-role-noun triples extracted fromthe Wall Street Journal and Brown parts of thePenn Treebank (Marcus et al, 1993).
These werefirst annotated with semantic roles using a state-of-the-art semantic role labeling system (Koomenet al, 2005).Semantic roles are approximated by PropBankargument roles (Palmer et al, 2005).
These con-sist of a limited set of numbered roles that are usedfor all verbs but are defined on a verb-by-verb ba-sis.
This contrasts with FrameNet roles, which aresense-specific.
Hence PropBank roles provide ashallower level of semantic role annotation.
Theyalso do not refer consistently to the same semanticroles over different verbs, although the A0 and A1roles in the majority of cases do correspond to theagent and patient roles, respectively.
The A2 rolerefers to a third participant involved in the event,but the label can stand for several types of seman-tic roles, such as beneficiary or recipient.
To createthe exemplar memory, all lemmatized verb-noun-role triples that contained the A0, A1, or A2 roleswere extracted.4.3 Testing the modelTo obtain the semantic distances between nounsand verbs for the calculation of the distance be-tween exemplars (see Equation 3), we make useof a thesaurus compiled by Lin (1998), whichlists the 200 nearest neighbors for a large num-ber of English noun and verb lemmas, togetherwith their similarity values.
This resource wascreated by computing the similarity between worddependency vectors that are composed of fre-quency counts of (head, relation, dependent)triples (dependency triples) in a 64-million wordparsed corpus.
To compute these similarities, aninformation-theoretic similarity metric was used.The basic idea of this metric is that the similaritybetween two words is the amount of informationcontained in the commonality between the twowords, i.e.
the frequency counts of the dependencytriples that occur in the descriptions of both words,divided by the amount of information in the de-scriptions of the words, i.e.
the frequency countsof the dependency triples that occur in either ofthe two words.
See Lin (1998) for details.
Thesesimilarity values were transformed into distancesby subtracting them from the maximum similarityvalue 1.Gain Ratio is used to determine the weights of830the nouns and verbs in the distance calculation.Gain Ratio is a normalization of Information Gain,an information-theoretic measure that quantifieshow informative a feature is in the prediction of aclass label; in this case how informative in generalnouns or verbs are when one has to predict a se-mantic role.
Based on our exemplar memory, theGain Ratio values and so the feature weights are0.0402 for the verbs, and 0.0333 for the nouns.The model predictions are evaluated againsttwo data sets of human semantic role plausibil-ity ratings for verb-role-noun triples (McRae et al,1998; Pado?
et al, 2006).
These data sets were cho-sen because they are the same data sets that wereoriginally used in the evaluation of the two othermodels discussed in sections 2.1 and 2.2.The first data set, from McRae et al (1998),consists of semantic role plausibility ratings for 40verbs, each coupled with both a good agent and agood patient, which were presented to the raters inboth roles.
This means there are 40?
2?
2 = 160items in total.
We divide this data set in the same60-item development and 100-item test sets thatwere used by Pado?
et al (2006) and Pado?
et al(2007) for the evaluation of their models.For most of the McRae items, being a goodagent for a given verb also entails being a bad pa-tient for that same verb, and the other way around.This leads us to predict that on this data set thekNNf model (see section 3.1) and the kGCM (seesection 3.3) should perform no worse than the DDmodel (see section 3.2).The second data set is taken from Pado?
et al(2006) and consists of 414 verb-role-noun triples.Agent and patient ratings are more evenly dis-tributed, so we predict that a model that exclu-sively relies on the relative role frequencies in thenearest neighbor sets of these items might not cap-ture as much variability as a model that takes dis-tance into account to weight the exemplars.
There-fore, we expect the DD model to do better than thekNNf model on this data set.
We randomly dividethe data set in a 276-item development set, and a138-items test set.Because of the non-normal distribution of thetest data, we use Spearman?s rank correlation testto measure the correlation strength between theplausibility ratings predicted by the model and thehuman ratings.
To estimate whether the strengthwith which the predictions of the different mod-els correlate with the human judgments differssignificantly between the models, we use an ap-proximate test statistic described in Raghunathan(2003).
This test statistic is robust for sample sizedifferences, which is necessary in this case giventhe fact that the models differ in their coverage.We will refer to this statistic as the Q-statistic.Experiments on the development sets are runto find optimal values per model for two param-eters: k, the number of nearest neighbors that aretaken into account for the construction of the near-est neighbor set, and ?
(for the DD and kGCMmodels), the rate of decay over distance (see Equa-tion 6).4.4 Results4.4.1 McRae dataResults on the McRae test set are summarized inTable 2.
The first three rows contain the resultsfor the exemplar-based models.
The last two rowsshow the results of the two previous models forcomparison.
The values for k and ?
that werefound to be optimal in the experiments on the de-velopment set are specified where applicable.The predictions of all three exemplar-basedmodels correlate significantly with the human rat-ings, with the DD model doing somewhat bet-ter than the kNNf model and the kGCM model,although these differences are not significant(Q(0.28) = 0.134, p = 2.8?10?1 andQ(0.28) =0.116, p = 2.9?10?1, respectively).
Coverage ofthe exemplar-based models is very high.When we compare the results of the exemplar-based models with those of the Pado?
models, wefind that the predictions of the DD model correlatesignificantly stronger with the human ratings thanthe predictions of the Pado?
et al (2007) model,Q(0.98) = 4.398, p = 3.5 ?
10?2.
The DDmodel also matches the high performance of thePado?
et al (2006) model.
Actually, the correlationstrength of the DD predictions with the human rat-ings is higher, but that difference is not significant,Q(0.93) = 0.285, p = 5.6 ?
10?1.
However, theDD model has a much higher coverage than themodel of Pado?
et al (2006), ?2(1, N = 100) =44.5, p = 2.5?
10?11.4.4.2 Pado?
dataTable 3 summarizes the results for the Pado?data set.
We find that the predictions of allthree exemplar-based models correlate signifi-cantly with the human ratings, and that there are831Model k ?
Coverage ?
pkNNf 9 - 96% .407 p = 3.9?
10?5DD 11 5 96% .488 p = 4.6?
10?7kGCM 9 21 96% .397 p = 6.2?
10?5Pado?
et al (2006) - - 56% .415 p = 1.5?
10?3Pado?
et al (2007) - - 91% .218 p = 3.8?
10?2Table 2: Results for the McRae data.Model k ?
Coverage ?
pkNNf 12 - 97% .521 p = 1.1?
10?10DD 8 21 97% .523 p = 9.1?
10?11kGCM 10 25 97% .512 p = 2.7?
10?10Pado?
et al (2006) - - 96% .514 p = 2.9?
10?10Pado?
et al (2007) - - 98% .506 p = 3.7?
10?10Table 3: Results for the Pado?
data.no significant differences between the three modelinstantiations.
Coverage is again very high.There are no significant performance differ-ences between the exemplar-based models and thePado?
models.
Correlation strengths and coverageare more or less the same for all models.4.5 DiscussionIn general, we find that our exemplar-based, se-mantic role predicting approach attains a verygood fit with the human semantic role plausibil-ity ratings from both the McRae and the Pado?
dataset.
Moreover, because of the fact that generaliza-tion is determined by similarity-based extrapola-tion from verb-noun pairs, the high correlations ofthe model?s predictions with the human ratings areaccompanied by a very high coverage.As concerns the comparison with the models ofPado?
et al (2006) and Pado?
et al (2007) on thePado?
data, we can be brief: the exemplar-basedmodels?
performance matches that of the Pado?models, and basically all models perform equallywell, both on correlation strength and coverage.However, there is a striking discrepancy be-tween the performance of the Pado?
models andthe DD model on the McRae data sets.
We findthat the DD model performs well for both correla-tion strength and coverage, as opposed to the Pado?models, both of which score less well on one orthe other of these two dimensions.
Although themodel of Pado?
et al (2006) attains a good fit on theMcRae data, its coverage is very low.
This is espe-cially problematic considering the fact that it is ex-actly this type of test items that is used in the kindof sentence comprehension experiments for whichthese thematic fit models should help explain theresults.
The model of Pado?
et al (2007) succeedsin boosting coverage, but at the expense of corre-lation strength, which is reduced to approximatelyhalf the correlation strength attained by the Pado?et al (2006) model.The model of Pado?
et al (2006) requires thetest verbs and their senses to be attested in theFrameNet corpus to be able to make its predic-tions.
However, only 64 of the 100 test items inthe McRae data set contain verbs that are attestedin the FrameNet corpus, 8 of which involve anunattested verb sense.
On the other hand, the onlyrequirement for the exemplar-based model to beable to make its predictions is that the similaritiesbetween the verbs and the nouns in the target ex-emplars and the memory exemplars can be com-puted.
In our case, this means that the verbs andnouns need to have entries in the thesaurus we use(see Section 4.3).
In the McRae data set, this is thecase for all verbs, and for 48 out of the 50 nouns.This explains the large difference in coverage be-tween the DD model and the model of Pado?
et al(2006).Pado?
et al (2007) attribute the poorer correla-832tion of their 2007 model with the human ratingsin the McRae data set to the much lower frequen-cies of the nouns in that data set as compared tothe frequencies of the nouns in the Pado?
data set.That is probably also the explanation for the dif-ference in correlation strength between our modeland the model of Pado?
et al (2007).
Both modelsuse similarity-based smoothing to compensate forlow-frequency target items, but the generalizationproblem caused by low frequency nouns is allevi-ated in our model by the fact that the model notonly generalizes over nouns, but also over verbs.Since the model can base its generalizations onverb-noun pairs that contain the noun of the tar-get pair coupled to a verb that is different from theverb in the target pair, the neighbor set that it gen-eralizes from can contain a larger number of ex-emplars with nouns that are identical to the nounin the target pair.
The model of Pado?
et al (2007)has no access to nouns that are not coupled to thetarget verb in the training corpus.In Section 3, we predicted that the kNNf andthe kGCM should perform equally well as the DDmodel on the McRae data set, because of the bal-anced nature of that data set (all nouns are eithergood agents and bad patients, or the other wayaround), but that the DD model should do betteron the less balanced Pado?
data set.
This predic-tion is not borne out by the results, since the DDmodel does not perform significantly better on ei-ther of the data sets, although on both data setsit achieves the highest correlation strength of allthree models.
However, what we see is that theperformance difference between the DD model onthe one hand and the kNNf model and kGCM onthe other hand is larger on the McRae data thanon the Pado?
data, which is exactly the opposite ofwhat we predicted.
The fact that the differencesare not significant makes us hesitant to draw anyconclusions from this finding, though.5 ConclusionWe presented an exemplar-based model of the-matic fit that is founded on the idea that seman-tic role plausibility can be predicted by similarity-based generalization over verb-argument pairs.
Incontrast to previous models, this model does notimplement semantic role plausibility as ?fit withverb selectional preferences?, but directly capturesthe semantic role ambiguity problem comprehen-ders have to solve when confronted with sentencesthat contain structural ambiguities like the MV/RRambiguity, namely deciding which semantic role anoun has in the event denoted by the verb.
There-fore, the model should be easily extensible to-wards a complete model of any sentence-level am-biguity that revolves around a semantic role ambi-guity.We have shown that our model can account verywell for human semantic role plausibility judg-ments, attaining both high correlations with hu-man ratings and high coverage overall, and im-proving on two state-of-the-art models, the per-formance of which deteriorates when there is asmall overlap between the verbs in the trainingcorpus and in the test data, or when the test nounshave low frequencies in the training corpus.
Wesuggest that this improvement is due to the factthat our model applies similarity-based smoothingover both nouns and verbs.
Generally, one cansay that the exemplar-based model?s architecturemakes it very robust for data sparsity.We also found that a non-normalized versionof our model that takes distance into accountto weight the memory exemplars seems to per-form somewhat better than a simple nearest neigh-bor model or a normalized distance decay model.However, these performance differences are notstatistically significant, and we did not find thepredicted advantage of the non-normalized dis-tance decay model on the Pado?
data set.In future work, we will test our claim ofstraightforward extensibility of the model by in-deed extending our model to account for readingtime patterns in the online processing of sentencesexemplifying temporary semantic role ambigui-ties, more specifically the MV/RR ambiguity.
An-other avenue for future research is to see how ourapproach to thematic fit can be used to augmentexisting semantic role labeling systems.AcknowledgmentsThis work was supported by a grant from theResearch Foundation ?
Flanders (FWO).
We aregrateful to Ken McRae and Ulrike Pado?
for mak-ing their datasets available, Dekang Lin for thethesaurus, and the people of the Cognitive Com-putation Group at UIUC for their SRL system.ReferencesThomas M. Cover and Peter E. Hart.
1967.
Nearestneighbor pattern classification.
IEEE Transactions833on Information Theory, 13(1):21?27.Walter Daelemans and Antal van den Bosch.
2005.Memory-based language processing.
CambridgeUniversity Press, Cambridge.Charles J. Fillmore, Christopher R. Johnson, andMiriam R. L. Petruck.
2003.
Background toFrameNet.
International Journal of Lexicography,16:235?250.Evelyn Fix and Joseph L. Hodges.
1951.
Discrimina-tory analysis?nonparametric discrimination: con-sistency properties.
Technical Report Project 21-49-004, Report No.
4, USAF School of AviationMedicine, Randolp Field, TX.Lyn Frazier.
1987.
Sentence processing: A tutorial re-view.
In Max Coltheart, editor, Attention and Per-formance XII: The Psychology of Reading, pages559?586.
Erlbaum, Hillsdale, NJ.Peter Koomen, Vasin Punyakanok, Dan Roth, andWen-tau Yih.
2005.
Generalized inference withmultiple semantic role labeling systems.
In IdoDagan and Daniel Gildea, editors, Proceedings ofthe Ninth Conference on Computational NaturalLanguage Learning (CoNLL-2005), pages 181?184.Association for Computational Linguistics, Morris-town, NJ.Dekang Lin.
1998.
Automatic retrieval and cluster-ing of similar words.
In Christian Boitet and PeteWhitelock, editors, Proceedings of the 17th Inter-national Conference on Computational Linguistics,pages 768?774.
Association for Computational Lin-guistics, Morristown, NJ.Maryellen C. MacDonald and Mark S. Seidenberg.2006.
Constraint satisfaction accounts of lexicaland sentence comprehension.
In Matthew J. Traxlerand Morton A. Gernsbacher, editors, Handbook ofPsycholinguistics (Second Edition), pages 581?611.Academic Press, London.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the Penn Treebank.
Compu-tational Linguistics, 19(2):313?330.Ken McRae, Michael J. Spivey-Knowlton, andMichael K. Tanenhaus.
1998.
Modeling the influ-ence of thematic fit (and other constraints) in on-linesentence comprehension.
Journal of Memory andLanguage, 38(3):283?312.Robert M. Nosofsky.
1986.
Attention, similar-ity, and the identification-categorization relation-ship.
Journal of Experimental Psychology-General,115(1):39?57.Sebastian Pado?
and Mirella Lapata.
2007.Dependency-based construction of semantic spacemodels.
Computational Linguistics, 33(2):161?199.Ulrike Pado?, Frank Keller, and Matthew Crocker.2006.
Combining syntax and thematic fit in a prob-abilistic model of sentence processing.
In Ron Sunand Naomi Miyake, editors, Proceedings of the 28thAnnual Conference of the Cognitive Science Society,pages 657?662.
Cognitive Science Society, Austin,TX.Sebastian Pado?, Ulrike Pado?, and Katrin Erk.
2007.Flexible, corpus-based modelling of human plau-sibility judgements.
In Jason Eisner, editor, Pro-ceedings of the 2007 Joint Conference on EmpiricalMethods in Natural Language Processing and Com-putational Natural Language Learning (EMNLP-CoNLL), pages 400?409.
Association for Computa-tional Linguistics, Morristown, NJ.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An annotated cor-pus of semantic roles.
Computational Linguistics,31(1):71?106.Trivellore Raghunathan.
2003.
An approximate testfor homogeneity of correlated correlation coeffi-cients.
Quality and Quantity, 4(1):99?110.Philip Resnik.
1996.
Selectional constraints: aninformation-theoretic model and its computationalrealization.
Cognition, 61(1-2):127?159.Roger N. Shepard.
1987.
Toward a universal law ofgeneralization for psychological science.
Science,237(4820):1317?1323.Jakub Zavrel and Walter Daelemans.
1997.
Memory-based learning: Using similarity for smoothing.In Philip R. Cohen and Wolfgang Wahlster, edi-tors, Proceedings of the 35th Annual Meeting of theAssociation for Computational Linguistics, pages436?443.
Association for Computational Linguis-tics, Morristown, NJ.834
