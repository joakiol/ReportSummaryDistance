Proc.
of 5th Workshop on Cognitive Aspects of Computational Language Learning (CogACLL) @ EACL 2014, pages 30?37,Gothenburg, Sweden, April 26 2014.c?2014 Association for Computational LinguisticsA multimodal corpus for the evaluation of computational models for(grounded) language acquisitionJudith Gaspersa, Maximilian Panznera, Andre Lemmeb, Philipp Cimianoa,Katharina J. Rohlfingc, Sebastian WredebaSemantic Computing Group, CITEC, Bielefeld University, Germany{jgaspers|mpanzner|cimiano}@cit-ec.uni-bielefeld.debResearch Institute for Cognition and Robotics (CoR-Lab), Bielefeld University, Germany{alemme|swrede}@cor-lab.uni-bielefeld.decEmergentist Semantics Group, CITEC, Bielefeld University, Germanykjr@uni-bielefeld.deAbstractThis paper describes the design and ac-quisition of a German multimodal cor-pus for the development and evaluation ofcomputational models for (grounded) lan-guage acquisition and algorithms enablingcorresponding capabilities in robots.
Thecorpus contains parallel data from multi-ple speakers/actors, including speech, vi-sual data from different perspectives andbody posture data.
The corpus is designedto support the development and evalua-tion of models learning rather complexgrounded linguistic structures, e.g.
syn-tactic patterns, from sub-symbolic input.It provides moreover a valuable resourcefor evaluating algorithms addressing sev-eral other learning processes, e.g.
conceptformation or acquisition of manipulationskills.
The corpus will be made availableto the public.1 IntroductionChildren acquire linguistic structures through ex-posure to (spoken) language in a rich context andenvironment.
The semantics of language may belearned by establishing connections between lin-guistic structures and corresponding structures inthe environment, i.e.
in different domains suchas the visual one (Harnad, 1990).
Both with re-spect to modeling language acquisition in chil-dren and with respect to enabling correspondinglanguage acquisition capabilities in robots, whichmay ideally be also grounded in their environment,it is hence of great interest to explore i) how lin-guistic structures of different levels of complexity,e.g.
words or grammatical phrases, can be derivedfrom speech input, ii) how structured representa-tions for entities observed in the environment canbe derived, e.g.
how concepts and structured rep-resentations of actions can be formed, and iii) howconnections can be established between structuredrepresentations derived from different domains.
Inorder to gain insights concerning the mechanismsat play during language acquisition (LA), whichenable children to solve these learning tasks, mod-els are needed which ideally cover several learningtasks.
For instance, they may cover the acquisitionof both words and grammatical rules as well as theacquisition of their grounded meanings.
Comple-mentarily, data resources are needed which enablethe design and evaluation of these models by pro-viding suitable parallel data.Aiming to provide a basis for the developmentand evaluation of LA models addressing the ac-quisition of rather complex and grounded linguis-tic structures, i.e.
syntactic patterns, from sub-symbolic input, we designed a German multi-modal input corpus.
The corpus consists of data ofmultiple speakers/actors who performed actions infront of a robot and described these actions whileexecuting them.
Subjects were recorded, i.e.
par-allel data of speech, stereo vision (including theview-perspective of the ?infant?/robot) and bodypostures were gathered.
The resulting data henceallow grounding of linguistic structures in bothvision and body postures.
Among others, learn-ing processes that may be evaluated using the cor-pus include: acquisition of several linguistic struc-tures, acquisition of visual structures, concept for-mation, acquisition of generalized patterns whichabstract over different speakers and actors, estab-lishment of correspondences between structures30from different domains, acquisition of manipula-tion skills, and development of appropriate modelsfor the representations of actions.This paper is organized as follows.
Next, wewill provide background information concerningcomputational models of LA.
In Section 3, wewill then describe the corpus design and acqui-sition, including the desired properties of thecollected data, corresponding experimental set-tings and technical implementation.
We will thenpresent the resulting data set and subsequentlyconclude..2 BackgroundTo date, several models addressing LA learningtasks have been proposed and evaluated using dif-ferent copora.
Yet, these models typically focus ona subset or certain aspects of the LA learning tasksmentioned in the previous section, often assumingother learning tasks, e.g.
those of lower complex-ity, as already solved by the learner.
For instance,models addressing the acquisition of grammaticalconstructions and their meaning (Kwiatkowski etal., 2012; Alishahi and Stevenson, 2008; Gaspersand Cimiano, in press; Chang and Maia, 2001)typically learn from symbolic input.
In particu-lar, assuming that the child is already able to seg-ment a speech signal into a stream of words and toextract structured representations from the visualcontext, such models typically explore learningfrom sequences of words and symbolic descrip-tions of the non-linguistic context.
Models ad-dressing the acquisition of word-like units directlyfrom a speech signal (R?as?anen, 2011; R?as?anen etal., 2009) have also been explored.
These, how-ever, typically do not address learning of morecomplex linguistic structures/constructions.Taken together, lexical acquisition from speechand syntactic acquisition have been mainly stud-ied independently of each other, often assumingthat syntactic acquisition follows from knowledgeof words.
However, learning processes might ac-tually be interleaved, and top-down learning pro-cesses may play an important role in LA.
Forinstance, with respect to computational learn-ing from symbolic input, it has been shown thatknowledge of syntax can facilitate word learning(Yu, 2006).
Children may, for instance, also makeuse of syntactic cues during speech segmentationand/or word learning, but models addressing lexi-cal acquisition from speech have to date mainly ig-nored syntax (R?as?anen, 2012).
Models addressingthe acquisition of syntactic patterns directly fromspeech provide a basis for exploring to what extentlearning mechanisms might be interleaved in earlyLA.
Moreover, they allow to investigate the pos-sible role of several top-down learning processeswhich have to date been little explored.Several corpora comprising interactions of chil-dren with their caregivers have been collected.
Alarge such resource is the CHILDES data base(MacWhinney, 2000), which contains transcribedspeech.
Data from CHILDES have been oftenused to evaluate models learning from symbolicinput, in particular models for syntactic acquisi-tion from sequences of words; additional accom-panying symbolic context representations havebeen often created (semi?)automatically.
More-over, multimodal corpora containing caregiver-child interactions have been recorded and anno-tated (Bj?orkenstam and Wirn, 2013; Yu et al.,2008), thus also allowing to study the role of socialinteraction and extra-linguistic cues in languagelearning.
By contrast, in this work we aim to pro-vide a basis for developing and evaluating modelswhich address the acquisition of syntactic patternsfrom speech.
Hence, allowing to derive general-ized patterns, linguistic units as well as the objectsand actions they refer to have to re-appear in thedata several times.
Thus, in line with the CARE-GIVER corpus (Altosaar et al., 2010) we did notrecord caregiver-child interactions but attemptedto approximate speech used by caregivers with re-spect to the learning task(s) at hand.
However,the focus of the CAREGIVER corpus is on mod-els learning word-like units from speech.
Thus,a number of keywords were spoken in differentcarrier sentences; speech is accompanied by onlylimited non-linguistic context information in thecorpus.
In contrast to CAREGIVER, we did notrestrict language use directly and recorded paral-lel context information from different modalities,focusing not only on the acquisition of word-likeunits from speech and word-to-object mapping butmoreover on the acquisition of simple syntacticpatterns and mapping language to actions.3 Corpus design and acquisitionIn this section, we will first describe the desiredproperties of the corpus.
Subsequently, we willpresent the corresponding experimental settings,used stimuli and procedure, the technical imple-31mentation of the robot behavior and the data ac-quisition as well as the resulting corpus.3.1 Desired propertiesOur goal was to design a corpus comprising multi-modal data which supports the evaluation of com-putational models addressing several LA learn-ing tasks, and in particular the acquisition ofgrounded syntactic patterns from sub-symbolic in-put only as well as the development of compo-nents supporting the acquisition of language byrobots.
Thus, the main focus was to design thecorpus in such a way that the data acquisitionscenario was simplified enough to allow solvingthe task of learning grounded syntactic patternsfrom sub-symbolic input with the resulting dataset (which of course contains much less data whencompared to the innumerable natural language ex-amples children receive when acquiring languageover several years).
In particular, since the ac-quisition of rather complex structures should beenabled using sub-symbolic information, several(repeated) examples for contained structures wereneeded, allowing the formation of generalized rep-resentations.
Thus, we opted for a rather sim-ple scenario.
Specifically, the following propertieswere taken into account:?
Rather few objects and actions were includedthat could moreover be differentiated rathereasily from a visual point of view.
How-ever, in order to reflect differences betweenactions, these differed i) with respect tothe number of their referents as well as ii)with respect to their specificity to certainobjects.
In particular, we included actionswhich could be performed on different sub-sets of the objects, ranging from specificityto one certain object to being executed withall of the objects.?
Objects and actions reappeared several times,yielding several examples for each of them.Repeated appearance is an essential aspect,since the formation of generalized represen-tations starting from continuous input re-quires several observations in order to allowabstraction over observed examples/differentactors and speakers.?
The scenario was designed such that it en-couraged human subjects to use rather simplesyntactic patterns/short sentences.
Yet, lan-guage use was in principle unrestricted in or-der to acquire rather natural data and to cap-ture speaker-dependent differences.
This alsoreflects the input children receive in that par-ents use rather simple language when talkingto children.?
Data were gathered from several human sub-jects in order to allow for the evaluation ofgeneralization over different speakers (withdifferent acoustic properties and differentlanguage use, e.g.
different words for ob-jects, different syntactic patterns with differ-ent complexity, etc) as well as over differentactors in case of actions, since children inter-act with different people and are able to solvethis task.
Moreover, generalization to dif-ferent speakers/actors is also important withrespect to learning in artificial agents whichshould preferably not be operable by a singleperson only.?
Parallel data were gathered in which ob-jects and actions were explicitly named whenthey were used.
This is an important as-pect because the corpus should allow learn-ing connections between vision, i.e.
objectsand actions, and speech (segments) referringto these objects/actions, i.e.
(sequences ofwords) and syntactic patterns.
It reflects theinput children receive in that caregivers alsoexplain/show objects directly to their chil-dren and may show them how to use ob-jects/perform actions in front of them (Rolfet al., 2009; Schillingmann et al., 2009).We opted for the collection of parallel data con-cerning vision and body postures for human tutors.Hence, the corpus allows grounding of linguisticstructures in both vision and body postures.
In-cluding body postures moreover allows the eval-uation of algorithms showing manipulation skillswhich is of interest with respect to learning inrobots.We used stereo vision to allow computationallearners to reliably track object movement and in-teraction using both visual and depth information.With respect to vision, four cameras with two dif-ferent perspectives were used: two static externalcameras as well as the robot?s two internal movingcameras.
The latter basically mimics the ?infant?view, i.e.
while the external cameras were static,32the robot moved its eyes (and thus the cameras)and focused on the tutor?s hand performing the ac-tions, thus reflecting how a child may focus her/hisattention to the important aspects of a scene/a per-formance of her/his caregiver.3.2 ParticipantsA total of 27 adult human subjects participated indata collection (7 male, 20 female, mean age: 26).Subjects were paid for their participation.3.3 Experimental settingHuman subjects performed pre-defined actionsand simultaneously described their performancesin front of the robot iCub (Metta et al., 2008); Fig.1 depicts a human subject interacting with iCub.While interacting with iCub, human subjects?
be-Figure 1: A human subject interacting with iCub.havior was recorded.
In particular, the followingdata were recorded simultaneously:?
Speech/Audio (via a headset microphone)?
Vision/Video, static perspective (via twocameras, allowing for stereo vision)?
iCub-Vision/Video, iCub?s (attentive) per-spective (via iCub?s two internal cameras,again allowing for stereo vision)?
Body postures (via a Kinect).An experimental sketch showing the experimentalsetting including the positions of the human sub-ject and iCub, as well as camera and Kinect po-sitions, is illustrated in Fig.
2.
As can be seen,the human subject was placed directly opposite toiCub.
The two external cameras and the Kincetwere placed slightly sloped opposite to the sub-ject.
Subjects were instructed about which actionsshould be performed via a computer screen whichwas operated by an experimentator.In order to encourage subjects to perform the tu-toring task rather naturally, i.e.
just like they wereFigure 2: Experimental sketch.interacting with a human (child), iCub providedfeedback (Nagai and Rohlfing, 2009; Fischer etal., 2011).
In particular, a gazing behavior wasimplemented to make the robot appear attentivelyfollowing the tutoring.3.4 StimuliData were gathered in the framework of a toycooking scenario.
In particular, subjects preparedseveral dishes in front of iCub using toy objects.Specifically, 21 toy objects were chosen such thatFigure 3: Utilized objects.they were rather easy to differentiate with respectto color and/or form.
The chosen objects were:pizza, pita bread, plate, bowl, spaghetti, pepper,vinegar, red pepper, lettuce leafs, tomato, onion,cucumber, cheese, toast, salami, chillies, egg, an-chovy, cutting board, knife, and mushrooms.
Theobjects are depicted in Fig.
3.
Moreover, six dif-ferent actions were chosen which could be exe-cuted using these objects.
Again, the goal wasto support rather easy identification visually (withrespect to their trajectories).
The chosen actionswere: showing an object, cutting an object (eggor tomato) into two pieces (with knife), placingan object onto another one (plate, pizza, cuttingboard, toast), putting an object into another one(bowl, pita bread), pour vinegar, and strew pep-per.
Thus, most actions were object-specific to a33certain degree, i.e.
they were to be executed witha certain subset of the objects each.
The show ac-tion was to be executed using each of the objects.Furthermore, 20 different dishes, i.e.
preparationprocesses each consisting of a sequence of actions,were created (four dishes including salad, pizza,pita bread, spaghetti and sandwich/toast, respec-tively).
This was done in order to gather ratherfluent/consistent courses of action and rather flu-ent communication in case of descriptions.
For in-stance, one sequence for preparing a salad startedas follows: showing bowl, showing lettuce leafs,putting lettuce leafs into bowl, showing cuttingboard, showing knife, showing tomato, puttingtomato onto cutting board, cutting tomato into twopieces, putting tomato pieces into bowl, etc.3.5 ProcedureSubjects first prepared one dish while not beingrecorded in order to get familiar with the task.They were instructed to perform presented actionsand to describe their performance simultaneously.Moreover, they were asked to name objects andactions explicitly, since a goal of the corpus is toallow learning connections between speech, visionand body postures.
Subjects were not asked touse particular words or phrases, but were free tomake own choices.
For instance, when being ex-posed to a picture of the pita bread, they were sup-posed to explicitly name the pita bread.
Yet, theywere free to choose a suitable word (or sequenceof words), e.g.
?Pita?, ?Pitatasche?, ?Teigtasche?,?D?onertasche?, ?Brottasche?, etc.Actions to be performed were presented to thesubjects via a computer screen; either one actionwas presented or ?
in most cases ?
two actionswere presented at once to be executed one afteranother.
In most cases two actions were presentedin order to gain more fluent communications andcourses of action.
In no case more than two ac-tions were presented together because we wantedsubjects to focus on performance and not on re-membering a certain course of action.
Actionswere presented only in the form of pictures in or-der to elicit rather natural language use.
In par-ticular, as mentioned previously, subjects couldchoose freely how to name objects and actions.
Anexample for a screen/picture showing two actionsto be performed one after another is presented inFig.
4.
An experimentator operated the screen,i.e.
guided the subjects through the sequences ofFigure 4: Example screen showing the actionsshow red pepper and put red pepper into bowl.actions.
Subjects participated for approximatelyone hour; only subject?s actual performances wererecorded, yielding approximately 20?30 minutesof usable material per subject.3.6 Robot behaviorAs mentioned previously, a gazing behavior wasimplemented to make the robot appear attentivelyfollowing the tutoring.
In particular, the robot?sgaze followed a subject?s presentation of an actionby gazing at her/his right wrist.
At times whensubjects did not move their hands (to present ac-tions) the robot was looking around, i.e.
it gazedat random targets.
In the following, the implemen-tation of the robot behavior will be described inmore detail.The experimental setup shown in Fig.
2 allows thesystem to observe a person in front of the robotiCub.
While the presentation task was performedby the person, the robot was supposed to gaze atthe right wrist of this person.
Via the Kinect datait was possible to acquire the body posture of therobot?s interaction partner.
We extracted the loca-tion of the wrist and represented the Cartesian po-sition in the coordinate system of the robot.
Thisposition was then used as the target to generatethe head and eye movements.
The movement wasexecuted by the iKinGaze module available in theiCub software repository (Pattacini, 2010).Next to this ?tracking?
behavior of the robot wealso used a ?background?
behavior.
The ?back-ground?
behavior then drew randomly new tar-gets xtarg(in meter) from the uniform distribution?
?
[?1.5,?1, 5] ?
[?0.2, 0.2] ?
[0.2, 0, 4] infront of the robot.
After convergence to the tar-get the behavior waited for t = 3 seconds beforea new target was drawn.
The switch from ?back-ground?
behavior to ?tracking?
behavior was trig-gered if new targets arrived from the Kinect-basedtracking component.
This behavior stayed active34as long as targets were received.
If no targets werearriving during t = 2 sec.
after the gazing con-verged on the last target, the ?background?
behav-ior took over.
Due to the difference in distancebetween targets, the motion duration was differentas well.
Therefore, time delays were added to thetarget generation, which resulted in a more naturalbehavior of the robot gazing.4 Acquired dataIn order to record synchronized data from theexternal sensors, the robot system and the ex-perimental control software, we utilized a ded-icated framework for the acquisition of multi-modal human-robot interaction data sets (Wienkeet al., 2012).
The framework and the underly-ing technology (Wienke and Wrede, 2011) allowsto directly capture the network communication ofrobotics software components.
Through this ap-proach, system-internal data from the iCub such asits proprioception and stereo cameras images canbe synchronously captured and transformed intoan RETF1-described log-file format with explicittime and type information.
Moreover, additionalrecording devices such as the Kinect sensors, theexternal pair of stereo cams or the audio inputfrom a close-talk microphone are captured directlywith this system and stored persistently.
An exam-ple of the acquired parallel data is provided by Fig.5 while Table 6 summarizes the technical aspectsof the acquired data.The applied framework also supports the auto-matic export and conversion of synchronized partsof the multimodal data set to common formatsused by other 3rd party tools such as the annota-tion tool ELAN (Sloetjes and Wittenburg, 2008)used for ground truth annotation of the acquiredcorpus.
In this experiment, we additionally cap-tured the logical state of the experiment controlsoftware which allowed us to efficiently post-process the raw data and, e.g., automatically pro-vide cropped video files containing only single ut-terances.
A logical state corresponds to the imageseen at the screen by a human subject at a certaintime, showing the action(s) to be performed.The acquired corpus contains in total 11.45hours / approx.
2.3 TB of multimodal input datarecorded in 27 trials.
Each trial was recorded inabout 1 hour of wallclock time and cropped to 20?30 minutes of effective parallel data.
While in 51Robot Engineering Task-Force, cf.
http://retf.info/a)b)c) d)Figure 5: Example of acquired parallel data com-prising a) visual data from two static cameras, b)visual data from two cameras contained in therobot?s eyes, c) audio and d) body posture datarecorded by the Kinect.
In this example the sub-ject is preparing a sandwich, and currently strew-ing pepper onto it.cases not all of the parallel data streams are avail-able due to difficulties with the robot and the wire-less microphones, we decided to leave this datain the corpus to evaluate machine learning pro-cesses addressing learning from one or a subset ofthe modalities only, e.g.
blind segmentation of aspeech stream.From the data logs, we exported audio (in AACformat) and the 4 synchronized video (with H.264encoding) files (MP4 container format) for eachtrial with an additional ELAN project file for an-notation.
This annotation is currently carried out;a screenshot of acquired data and correspondingannotations in ELAN is depicted in Fig.
7.
Itcomprises annotation of errors, as well as start-ing and end points for both presented actions andspoken utterances.
In particular, in case of speechword transcriptions are added, while in case of vi-sion actions are annotated in the form of predicatelogic formulas.
Hence, once the corpus is prepro-cessed, it is also suitable for the evaluation of mod-els learning from symbolic input with respect todata from one or more domains.
For instance, onecould explore the acquisition of syntactic patternsfrom speech by providing parallel visual context35# Device Description Data type Frequency Dimension Throughput1 Cam 1 Scene video rst.vision.Image ?
30 Hz 640?
480?
3 ?
28 MB/s2 Cam 2 Scene video rst.vision.Image ?
30 Hz 640?
480?
3 ?
28 MB/s3 Mic 1 Speech rst.audition.SoundChunk ?
50 kHz 1-2 ?
0.5 MB/s4 iCub Cam 1 Ego left bottle/yarp::sig::Image ?
30 Hz 320?
240?
3 ?
7 MB/s5 iCub Cam 2 Ego right bottle/yarp::sig::Image ?
30 Hz 320?
240?
3 ?
7 MB/s6 Kinect Body posture TrackedPosture3DFloat2?
30 Hz 36 ?
6 kB/s7 Control Logical state string ?
0.05 Hz - ?
5 B/sFigure 6: Description of acquired data streams, type specifications, average frequency, data dimensionand throughput as measured during recording.information either in sub-symbolic form or in theform of predicate logic formulas.Figure 7: Example of acquired data and corre-sponding annotations in ELAN.Word transcriptions for utterances for the wholedata set are not yet available.
According to the ex-perimentators?
impressions, most subjects indeedused, as desired, rather short sentences.
Further-more, a few subjects tried to vary their linguis-tic descriptions, i.e.
to use different sentencesfor each description.
Thus, the corpus appearsto cover not only several examples of rather sim-ple linguistic constructions with variations acrossspeakers, but moreover input examples with arather large degree of linguistic variation for a sin-gle speaker, hence providing examples of morechallenging data.We will make the corpus available to the publiconce post-processing is completely finished.5 ConclusionIn this paper, we have described the design andacquisition of a German multimodal data set forthe development and evaluation of grounded lan-guage acquisition models and algorithms enablingcorresponding abilities in robots.
The corpus con-tains parallel data including speech, visual datafrom four different cameras with different per-spectives and body posture data from multiplespeakers/actors.
Among others, learning pro-cesses that may be evaluated using the corpus in-clude: acquisition of several linguistic structures,acquisition of visual structures, concept forma-tion, acquisition of generalized patterns which ab-stract over different speakers and actors, establish-ment of correspondences between structures fromdifferent domains and acquisition of manipulationskills.AcknowledgmentsWe are deeply grateful to Jan Moringen, MichaelG?otting and Stefan Kr?uger for providing techni-cal support.
We wish to thank Luci Filinger,Christina Lehwalder, Anne Nemeth and FrederikeStrunz for support in data collection and annota-tion.
This work has been funded by the GermanResearch Foundation DFG within the Collabora-tive Research Center 673 Alignment in Communi-cation and the Center of Excellence Cognitive In-teraction Technology.
Andre Lemme is funded byFP7 under GA. No.
248311-AMARSi.ReferencesAfra Alishahi and Suzanne Stevenson.
2008.
A com-putational model of early argument structure acqui-sition.
Cognitive Science, 32(5):789?834.Toomas Altosaar, Louis ten Bosch, Guillaume Aimetti,Christos Koniaris, Kris Demuynck, and Henkvan den Heuvel.
2010.
A speech corpus for mod-eling language acquisition: Caregiver.
In Proceed-36ings of the International Conference on LanguageResources and Evaluation.Kristina Nilsson Bj?orkenstam and Mats Wirn.
2013.Multimodal annotation of parent-child interactionin a free-play setting.
In Proceedings of the Thir-teenth International Conference on Intelligent Vir-tual Agents.Nancy C. Chang and Tiago V. Maia.
2001.
Learn-ing grammatical constructions.
In Proceedings ofthe 23rd Cognitive Science Society Conference.Kerstin Fischer, Kilian Foth, Katharina J. Rohlfing,and Britta Wrede.
2011.
Mindful tutors: Linguis-tic choice and action demonstration in speech to in-fants and to a simulated robot.
Interaction Studies,12(1):134?161.Judith Gaspers and Philipp Cimiano.
in press.
Acomputational model for the item-based induction ofconstruction networks.
Cognitive Science.Stevan Harnad.
1990.
The symbol grounding problem.Physica D: Nonlinear Phenomena, 42(1-3):335?346.Tom Kwiatkowski, Sharon Goldwater, Luke Zettle-moyer, and Mark Steedman.
2012.
A probabilis-tic model of syntactic and semantic acquisition fromchild-directed utterances and their meanings.
InProceedings of the 13th Conference of the EuropeanChapter of the Association for Computational Lin-guistics.Brian MacWhinney.
2000.
The CHILDES Project:Tools for analyzing talk.
Mahwah, NJ.Giorgio Metta, Giulio Sandini, David Vernon, LorenzoNatale, and Francesco Nori.
2008.
The iCub hu-manoid robot: an open platform for research in em-bodied cognition.
In Proceedings of the 8th Work-shop on Performance Metrics for Intelligent Sys-tems, pages 50?56, New York, NY.
ACM.Yukie Nagai and Katharina J. Rohlfing.
2009.
Com-putational analysis of motionese toward scaffoldingrobot action learning.
IEEE Transactions on Au-tonomous Mental Development, 1:44?54.Ugo Pattacini.
2010.
Modular Cartesian Controllersfor Humanoid Robots: Design and Implementationon the iCub.
Ph.D. thesis, RBCS, Istituto Italiano diTecnologia, Genova.Okko R?as?anen, Unto K. Laine, and Toomas Altosaar.2009.
Computational language acquisition by sta-tistical bottom-up processing.
In Proceedings Inter-speech.Okko R?as?anen.
2011.
A computational model of wordsegmentation from continuous speech using transi-tional probabilities of atomic acoustic events.
Cog-nition, 120:149176.Okko R?as?anen.
2012.
Computational modeling ofphonetic and lexical learning in early language ac-quisition: existing models and future directions.Speech Communication, 54:975?997.Matthias Rolf, Marc Hanheide, and Katharina J. Rohlf-ing.
2009.
Attention via synchrony.
making use ofmultimodal cues in social learning.
IEEE Transac-tions on Autonomous Mental Development, 1:55?67.Lars Schillingmann, Britta Wrede, and Katharina J.Rohlfing.
2009.
A computational model of acous-tic packaging.
IEEE Transactions on AutonomousMental Development, 1:226?237.Han Sloetjes and Peter Wittenburg.
2008.
Annota-tion by category: Elan and iso dcr.
In Proceed-ings of the International Conference on LanguageResources and Evaluation.Johannes Wienke and Sebastian Wrede.
2011.
A Mid-dleware for Collaborative Research in Experimen-tal Robotics.
In IEEE/SICE International Sympo-sium on System Integration (SII2011), Kyoto, Japan.IEEE.Johannes Wienke, David Klotz, and Sebastian Wrede.2012.
A Framework for the Acquisition of Multi-modal Human-Robot Interaction Data Sets with aWhole-System Perspective.
In LREC Workshop onMultimodal Corpora for Machine Learning: Howshould multimodal corpora deal with the situation?,Istanbul, Turkey.Chen Yu, Linda B. Smith, and Alfredo F. Pereira.
2008.Grounding word learning in multimodal sensorimo-tor interaction.
In Proceedings of the 30th AnnualConference of the Cognitive Science Society.Chen Yu.
2006.
Learning syntax-semantics mappingsto bootstrap word learning.
In Proceedings of the28th Annual Conference of the Cognitive ScienceSociety (2006) Key: citeulike:5276016.37
