A Robust and Hybrid Deep-Linguistic Theory Applied toLarge-Scale ParsingGerold Schneider, James Dowdall, Fabio RinaldiInstitute of Computational Linguistics, University of Zurich{gschneid,rinaldi}@ifi.unizh.ch, j.m.dowdall@sussex.ac.ukAbstractModern statistical parsers are robust and quitefast, but their output is relatively shallow whencompared to formal grammar parsers.
We sug-gest to extend statistical approaches to a moredeep-linguistic analysis while at the same timekeeping the speed and low complexity of a sta-tistical parser.
The resulting parsing architec-ture suggested, implemented and evaluated hereis highly robust and hybrid on a number oflevels, combining statistical and rule-based ap-proaches, constituency and dependency gram-mar, shallow and deep processing, full and near-full parsing.
With its parsing speed of about300,000 words per hour and state-of-the-art per-formance the parser is reliable for a number oflarge-scale applications discussed in the article.1 IntroductionRobustness in Computational Linguistics hasbeen recently recognized as a central issue forthe design of reliable, large-scale Natural Lan-guage Processing (NLP) systems.
While thehighest possible linguistic coverage is desirable,speed and robustness are equally important inpractical applications.Formal Grammar Parser have carefullycrafted grammars written by professional lin-guists.
In addition to expressing local relations,i.e.
relations between a mother and a directdaughter node, a number of non-local relations,i.e.
relations involving more than two genera-tions, are also modeled.
An example of a non-local relation is the subject control relation inthe sentence John wants to leave, where John isnot only the explicit subject of want, but equallythe implicit subject of leave.
A parser that failsto recognize control subjects misses importantinformation, quantitatively about 3 % of all sub-jects.But unrestricted real-world texts still pose aproblem to NLP systems that are based on For-mal Grammars.
Few hand-crafted, deep linguis-tic grammars achieve the coverage and robust-ness needed to parse large corpora (see (Riezleret al, 2002) for an exception, and (Burke et al,2004; Hockenmaier and Steedman, 2002) for ap-proaches extracting formal grammars from theTreebank), and speed remains a serious chal-lenge.
The typical problems can be grouped asfollows.Grammar complexity Fully comprehensivegrammars are difficult to maintain and consid-erably increase parsing complexity.
Note thatstatistical parsers can equally suffer from thisproblem, see e.g.
(Kaplan et al, 2004).Parsing complexity Typical formal gram-mar parser complexity is much higher thanthe O(n3) for CFG (Eisner, 1997).
The com-plexity of some formal grammars is still un-known.
For Tree-Adjoining Grammars (TAG)it is O(n7) or O(n8) depending on the im-plementation (Eisner, 2000).
(Sarkar et al,2000) state that the theoretical bound of worsttime complexity for Head-Driven Phrase Struc-ture Grammar (HPSG) parsing is exponential.Parsing algorithms able to treat completely un-restricted long-distance dependencies are NP-complete (Neuhaus and Bro?ker, 1997).Ranking Returning all syntactically possibleanalyses for a sentence is not really what is ex-pected of a syntactic analyzer if it should be ofpractical use, since for a human there is usuallyonly one ?correct?
interpretation.
A clear in-dication of preference, by means of ranking theanalyses in a preference order is needed.Pruning In order to keep search spaces man-ageable it is in fact necessary to discard uncon-vincing alternatives already during the parsingprocess.
In a statistical parser, the ranking ofintermediate structures occurs naturally, whilea rule-based system has to rely on ad hoc heuris-tics.
With a beam search in a parse-time prun-ing system, which means that the total numberof alternatives kept is constant from a certainsearch complexity onwards, real-world parsingtime can be reduced to near-linear.
If one wereto assume a constantly full beam, or uses anoracle (Nivre, 2004) it is linear in practice.A number of robust statistical parsers thatoffer solutions to these problems have now be-come available (Charniak, 2000; Collins, 1999;Henderson, 2003), but they typically produceCFG constituency data as output, trees that donot express long-distance dependencies.Although grammatical function and emptynodes annotation expressing long-distance de-pendencies are provided in Treebanks such asthe Penn Treebank (Marcus et al, 1993), moststatistical Treebank trained parsers fully orlargely ignore them1, which entails two prob-lems: first, the training cannot profit from valu-able annotation data.
Second, the extractionof long-distance dependencies (LDD) and themapping to shallow semantic representations isnot always possible from the output of theseparsers.
This limitation is aggravated by alack of co-indexation information and parsingerrors across an LDD.
In fact, some syntac-tic relations cannot be recovered on configura-tional grounds only.
For these reasons, (John-son, 2002) provocatively refers to them as ?half-grammars?.The paper is organized as follows.
We first ex-plore a deep-linguistic grammar theory for En-glish that is inherently designed to be robustby extending the low processing complexity andthe robustness of statistical approaches to amore deep-linguistic level, by making carefuluse of underspecification, grammar compressiontechniques and using a grammar that directlydelivers simple predicate-argument structures.This allow us to use a context-free grammarat parse-time while successfully treating long-distance dependencies using low-complexity ap-proaches before and after parsing.
Our ap-proach is to use finite-state approximationsof long-distance dependencies, as they are de-scribed in (Schneider, 2003a) for DependencyGrammar (DG) and (Cahill et al, 2004) forLexical Functional Grammar (LFG).
(Dienesand Dubey, 2003) show that finite-state pre-processing modules can successfully deal withLDDs.
Our approach is similar in also amount-ing to a preprocessing recognition of LDDs.Then we show that the implementation(Pro3Gres) profits from hybridness and is fast1(Collins, 1999) Model 2 uses some of the functionallabels, and Model 3 some long-distance dependenciesand robust enough to do large-scale parsing oftotally unrestricted texts and give an overviewof its applications.
To conclude, two evaluationsare given.2 A Robust Deep-Linguistic TheoryGenerally, a linguistic analysis model aims atcomplete and correct analysis, which meansthat the mapping between the text data and itssyntactic and semantic analysis is sound (themodel extracts correct readings) and complete(the model deals with all language phenomena).In practice, however, both goals cannot be to-tally reached.
The main obstacle for soundnessis the all-pervasive characteristic of natural lan-guage to be ambiguous, where ambiguities canoften only be resolved with world knowledge.Statistical disambiguation such as (Collinsand Brooks, 1995) for PP-attachment or(Collins, 1997; Charniak, 2000) for generativeparsing greatly improve disambiguation, but asthey model by imitation instead of by under-standing, complete soundness has to remain elu-sive.As for completeness, already early ?na?
?ve?statistical approaches have shown that the prob-lem of grammar size is not solved but even ag-gravated by a naive probabilistic parser imple-mentation, in which e.g.
all CFG rules permit-ted in the Penn Treebank are extracted.
Fromhis 300,000 words training part of the PennTreebank (Charniak, 1996) obtains more than10,000 CFG rules, of which only about 3,000occur more than once.
It is therefore necessaryto either discard infrequent rules, do manualediting, use a different rule format such as indi-vidual dependencies (Collins, 1996) or gain fulllinguistic control and insight by using a hand-written grammar ?
each of which sacrifices totalcompleteness.2.1 Near-full ParsingThe approach we have chosen is to use amanually-developed wide-coverage tag sequencegrammar (Abney, 1995; Briscoe and Carroll,2002), and to exclude or restrict rare, markedand error-prone phenomena.
For example,while it is generally possible for nouns to bemodified by more than one PP, only nouns seenin the Treebank with several PPs are allowed tohave several PPs.
Or, while it is generally possi-ble for a subject to occur to the immediate rightof a verb (said she), this is only allowed for verbsseen with a subject to the right in the train-ing corpus, typically verbs of utterance, andonly in a comma-delimited or sentence-final con-text.
This entails that the parser profits froma lean grammar but finds a complete structurespanning the entire sentence in the majority ofreal-world sentences and needs to resorts to col-lecting partial parses in the remaining minority.Starting from the most probable longest span,recursively the most probable longest span toleft and right is searched.Near-full parsing only leads to a very smallloss.
If an analysis consists of two partial parses,on the dependency relation level only the one,usually high-level relation between the headsof the two partial parses remains unexpressed.The risk of returning ?garden path?, locallycorrect but globally wrong, analyses diminisheswith increasing span length.2.2 Functional Dependency GrammarWe follow the broad architecture suggested by(Abney, 1995) which naturally integrates chunk-ing and dependency parsing and has provento be practical, fast and robust (Collins, 1996;Basili and Zanzotto, 2002).
Tagging and chunk-ing are very robust, finite-state approaches,parsing then only occurs between heads ofchunks.2 The perspicuous rules of a hand-written dependency grammar build up the pos-sible syntactic structures, which are ranked andpruned by calculating lexical attachment proba-bilities for the majortiy of the dependency rela-tions used in the grammar.
The grammar con-tains around 1000 rules containing the depen-dent?s and the head?s tag, the direction of thedependency, lexical information for closed classwords, and context restrictions3.
Context re-strictions express e.g.
that only a verb whichhas an object in its context is allowed to attacha secondary object.Our approach can be seen as an extension of(Collins and Brooks, 1995) from PP-attachmentto most dependency relations.
Training datais a partial mapping of the Penn Treebank todeep-linguistic dependency structures, similarto (Basili et al, 1998).Robustness also depends on the grammarformalism.
While many formalisms fail to2Practical experiments using a toy NP and verb-group grammar have shown that parsing between headsof chunks only is about four times faster than parsingbetween every word, i.e.
without chunking.3the number of rules is high because of tag combina-torics leading to many almost identical rules.
A subjectrelations is e.g.
possible between the 6 verb tags and the4 noun tagsproject when subcategorized arguments cannotbe found, in a grammar like DG, in which maxi-mal projections and terminal nodes are isomor-phic, projection can never fail.In classical DG, only content words can beheads, and there is no distinction between syn-tactic and semantic dependency ?
semantic de-pendency is used as far as possible.
These as-sumptions entail that there are no functionaland no empty nodes, which means that low com-plexity O(n3) algorithms such as CYK, which isused here, can be employed.The classical dependency grammar distinc-tion between ordre line?aire and ordre struc-tural, basically an immediate dominance / linearprecedence distinction (ID/LP) also has the ad-vantage that a number of phenomena classicallyassumed to involve long-distance dependencies,fronted or inversed constituents, can be treatedlocally.
They only need rules that allow an in-version of the ?canonical?
dependency directionunder well-defined conditions.
As for fronted el-ements, since DG does not distinguish betweenexternal and internal arguments, front positionsare always locally available to the verb.2.3 Underspecification andDisambiguationThe cheapest approach to dealing with the all-pervasive NL ambiguity is to underspecifiy ev-erything, which leads to a sound and completemapping, but one that is content-free and ab-surd.
But in few, carefully selected areas wheredistinctions do not matter for the task at hand,where the disambiguation task is particularlyunreliable, or where inter-annotator agreementis very low, underspecification can serve as atool to greatly facilitate linguistic analysis.
Forexample, intra-base NP ambiguities, such asquantifier scope ambiguities do not matter fora parser like ours aiming at predicate-argumentstructure, and are thus not attempted to an-alyze.
There is one part-of-speech distinctionwhere inter-annotator agreement is quite lowand the performance of taggers generally verypoor: the distinction between verbal particlesand prepositions.
We currently leave the dis-tinction underspecified, but a statistical disam-biguator is being developed.Conversely, the Penn Treebank annotation issometimes not specific enough.
The parser dis-tinguishes between the reading of the tag IN asa complementizer or as a preposition, and dis-ambiguates commas as far as it can, betweenapposition, subordination and conjunction.Some typical tagging errors can be robustlycorrected by the hand-written grammar.
Forexample, the distinction between verb pasttense VBD and participle VBN is unreliable,but can usually be disambiguated in the pars-ing process by leaving this tag distinction un-derspecified for a number of constructions.2.4 Long-distance DependenciesLong-distance dependencies exponentiallyincrease parsing complexity (Neuhaus andBro?ker, 1997).
We therefore use an approachthat preprocesses, post-processes and partlyunderspecifies them, allowing us to use acontext-free grammar at parse time.In detail, (1) before the parsing we modeldedicated patterns across several levels of con-stituency subtrees partly leading to dedicated,compressed and fully local dependency rela-tions, (2) we use statistical lexicalized post-processing, and (3) we rely on traditional De-pendency Grammar assumptions (section 2.2).2.4.1 Pre-processing(Johnson, 2002) presents a pattern-matching al-gorithm for post-processing the output of sta-tistical parsers to add empty nodes to theirparse trees.
While encouraging results are re-ported for perfect parses, performance dropsconsiderably when using trees produced by astatistical parser.
?If the parser makes a sin-gle parsing error anywhere in the tree fragmentmatched by the pattern, the pattern will nolonger match.
This is not unlikely since the sta-tistical model used by the parser does not modelthese larger tree fragments.
It suggests that onemight improve performance by integrating pars-ing, empty node recovery and antecedent find-ing in a single system ... ?
(Johnson, 2002).We have applied structural patterns to thePenn Treebank, where like in perfect parses pre-cision and recall are high, and where in addi-tion functional labels and empty nodes are avail-able, so that patterns similar to Johnson?s but?
like (Jijkoun, 2003) ?
relying on functionallabels and empty nodes reach precision close to100%.
Unlike in Johnson, also patterns for localdependencies are used; non-local patterns sim-ply stretch across more subtree-levels.
We usethe extracted lexical counts as lexical frequencytraining material.
Every dependency relationhas a group of structural extraction patternsassociated with it.
This amounts to a partialmapping of the Penn Treebank to FunctionalRelation Label Exampleverb?subject subj he sleepsverb?first object obj sees itverb?second object obj2 gave (her) kissesverb?adjunct adj ate yesterdayverb?subord.
clause sentobj saw (they) cameverb?prep.
phrase pobj slept in bednoun?prep.
phrase modpp draft of papernoun?participle modpart report writtenverb?complementizer compl to eat applesnoun?preposition prep to the houseTable 1: The most important dependency typesused by the parser?NP-SBJ-X@nounVP@Vpassive verbNP-NONE-*-X?NP-SBJ-X@nounVP@Vcontrol-verbSNP-SBJ-NONE-*-XFigure 1: The extaction patterns for passivesubjects (top) and subject control (bottom)DG (Hajic?, 1998), (Tapanainen and Ja?rvinen,1997).
Table 1 gives an overview of the mostimportant dependencies.The subj relation, for example, has the headof an arbitrarily nested NP with the functionaltag SBJ as dependent, and the head of an ar-bitrarily nested VP as head for all active verbs.In passive verbs, however, a movement involv-ing an empty constituent is assumed, which cor-responds to the extraction pattern in figure 1,where VP@ is an arbitrarily nested VP, and NP-SBJ-X@ the arbitrarily nested surface subjectand X the co-indexed, moved element.
Move-ments are generally supposed to be of arbitrarylength, but a closer investigation reveals thatthis type of movement is fixed.The same argument can be made for otherrelations, for example control structures, whichhave the extraction pattern shown in figure 1.Grammatical role labels, empty node labels andtree configurations spanning several local sub-trees are used as integral part of some of thepatterns.
This leads to much flatter trees, astypical for DG, which has the advantages that(1) it helps to alleviate sparse data by map-ping nested structures that express the samedependency relation, (2) the costly overhead fordealing with unbounded dependencies can belargely avoided, (3) it is ensured that the lex-ical information that matters is available in onecentral place, allowing the parser to take onewell-informed decision instead of several brittledecisions plagued by sparseness, which greatlyreduces complexity and the risk of errors (John-son, 2002).
Collapsing deeply nested structuresinto a single dependency relation is less complexbut has the same effect as carefully selectingwhat goes in to the parse history in history-based approaches.
?Much of the interestingwork is determining what goes into [the history]H(c)?
(Charniak, 2000).
(Schneider, 2003a) shows that the vast ma-jority of LDDs can be treated in this way,essentially compressing non-local subtrees intodedicated relations even before grammar writ-ing starts.
The compressed trees correspondto a simple LFG f-structure.
The trees ob-tained from parsing can be decompressed intotraditional constituency trees including emptynodes and co-indexation, or into shallow seman-tic structures such as Minimal Logical Forms(MLF) (Rinaldi et al, 2004b; Schneider et al,2000; Schwitter et al, 1999).
This approachleaves LDDs underspecified, but recoverable,and makes no claims as to whether empty nodesat an automonous syntactic level exist or not.2.4.2 Post-ProcessingAfter parsing, shared constituents can be ex-tracted again.
The parser explicitly does thisfor control, raising and semi-auxiliary relations,because the grammar does not distinguish be-tween subordinating clauses with and withoutcontrol.
A probability model based on the verbsemantics is invoked if a subordinate clausewithout overt subject is seen, in order to decidewhether the matrix clause subject or object isshared.2.4.3 What do we lose?Among the 10 most frequent types of emptynodes, which cover more than 60,000 of the64,000 empty nodes in the Penn treebank, thereare only two problematic LDD types: WHTraces and indexed gerunds.WH traces Only 113 of the 10,659 WHNPantecedents in the Penn Treebank are actuallyquestion pronouns.
The vast majority, over9,000, are relative pronouns.
For them, an in-version of the direction of the relation they haveto the verb is allowed if the relative pronounFigure 2: Pro3Gres flowchartprecedes the subject.
This method succeeds inmost cases, but linguistic non-standard assump-tions need to be made for stranded prepositions.Only non-subject WH-question pronouns andsupport verbs need to be treated as ?real?non-local dependencies.
In question sentences,before the main parsing is started, the sup-port verb is attached to any lonely participlechunk in the sentence, and the WH-pronounpre-parses with any verb.Indexed Gerunds Unlike in control, rais-ing and semi-auxiliary constructions, the an-tecedent of an indexed gerund cannot be es-tablished easily.
The fact that almost half ofthe gerunds are non-indexed in the Penn Tree-bank indicates that information about the un-expressed participant is rather semantic thansyntactic in nature, much like in pronoun res-olution.
Currently, the parser does not try todecide whether the target gerund is an indexedor non-indexed gerund nor does it try to find theidentity of the lacking participant in the lattercase.
This is an important reason why recallvalues for the subject and object relations arelower than the precision values.3 Robustness ?in the small?In addition to a robust deep-linguistic design(robustness ?in the large?, section 2), the im-plemented parser, Pro3Gres, uses a number ofpractical robust approaches ?in the small?
ateach processing level, such as relying on finite-state tagging and chunking or collecting par-tial parses if no complete analysis can be found,or using incrementally more aggressive pruningtechniques in very long sentences.
During theparsing process, only a certain number of alter-natives for each possible span are kept.
Experi-ments have shown that using a fixed number ora number dependent on the parsing complex-ity in terms of global chart entries lead to verysimilar results.
Using reasonable beam sizes in-creases parsing speed by an order of magnitudewhile hardly affecting parser performance.
Forthe fixed number model, performance starts tocollapse only when less than 4 alternatives perspan are kept.When a certain complexity has been reached(currently 1000 chart entries), only reductionsabove a certain probability threshold are per-missible.
The threshold starts very low, butis a function of the total number of chart en-tries.
This entails that even sentences withhundreds of words can be parsed quickly, butit is not aimed at finding complete parses forthem, rather a graceful degradation of perfor-mance (Menzel, 1995) is intended.4 A hybrid approach on many levelsPro3Gres profits from being hybrid on manylevels.
Hybridness means that the most robustapproach can be chosen for each task and eachprocessing level.statistical vs. rule-based the most obviousway in which Pro3Gres is a hybrid (Schneider,2003b).
Unlike formal grammars to which post-hoc statistical disambiguators can be added,Pro3Gres has been designed to be hybrid, care-fully distinguishing between tasks that can bestbe solved by finite-state methods, rule-basedmethods and statistical methods.
While e.g.grammar writing is easy for a linguist, and anaive Treebank grammar suffers from similarcomplexity problems as a comprehensive for-mal grammar, the scope of application and theamount of ambiguity a rule creates is often be-yond our imagination and best handled by astatistical system.shallow vs. deep the designing philosophyfor Pro3Gres has been to stay as shallow as pos-sible to obtain reliable results at each level.Treebank constituency vs. DG the obser-vation that a DG that expresses grammaticalrelations is more informative, but also more in-tuitive to interpret for a non-expert, and thatFunctional DG can avoid a number of LDDtypes has made DG the formalism of our choice.For lexicalizing the grammar, a partial mappingfrom the largest manually annotated corpusavailable, the Penn Treebank, was necessary, ex-hibiting a number of mapping challenges.history-based vs. mapping-basedPro3Gres is not a parse-history-based ap-proach.
Instead of manually selecting whatgoes into the history, as is usually done (see(Henderson, 2003) for an exception), we man-ually select how to linguistically meaningfullymap Treebank structures onto dependency re-lations by the use of mapping patterns adaptedfrom (Johnson, 2002).probabilistic vs. statistical Pro3Gres isnot a probabilistic system in the sense of aPCFG.
From a practical viewpoint, knowing theprobability of a certain rule expansion per seis of little interest.
Pro3Gres models decisionprobabilities, the probability of a parse is un-derstood to be the product of all the decisionprobabilities taken during the derivation.local subtress vs. DOP psycholinguisticexperiments and Data-Oriented Parsing (DOP)(Bod et al, 2003) suggest that people storesubtrees of various sizes, from two-word frag-ments to entire sentences.
But (Goodman,2003) suggests that the large number of sub-trees can be reduced to a compact grammar thatmakes DOP parsing computationally tractable.In Pro3Gres, a subset of non-local fragmentswhich, based on linguistic intuition are espe-cially important, are used.generative vs. structure-generating DGgenerally, although generative in the sense thatconnected complete structures are generated, isnot generative in the sense that it is alwaysguaranteed to terminate if used for random gen-eration of language.
Since a complete or partialhierarchical structure that follows CFG assump-tions due to the employed grammar is built upfor each sentence.
Pro3Gres?
constraint to alloweach complement dependency type only onceper verb can be seen as a way of rendering itgenerative in practice.syntax vs. semantics instead of usinga back-off to tags (Collins, 1999), semanticclasses, Wordnet for nouns and Levin classesfor verbs, are used, in the hope that they bettermanage better to express selectional restrictionsthan tags.
Practical experiments have shown,however, that, in accordance to (Gildea, 2001)on head-lexicalisation, there is almost no in-crease in performance.5 Applications and EvaluationPro3Gres is currently being applied in a Ques-tion Answering system specifically targeted atFigure 3: Dependency Tree output of the SWI Prolog graphical implementation of the parsertechnical domains (Rinaldi et al, 2004b).
Oneof the main advantages of a dependency-basedparser such as Pro3Gres over other parsing ap-proaches is that a mapping from the syntacticlayer to a semantic layer (meaning representa-tion) is partly simplified (Molla?
et al, 2000; Ri-naldi et al, 2002).The original version of the QA system usedthe Link Grammar (LG) parser (Sleator andTemperley, 1993), which however had a numberof significant shortcomings.
In particular theset of the dependency relations used in LG isvery idiosyncratic, which makes any syntactic-semantic mapping created for LG necessarilyunportable and difficult to extend and maintain.A recent line of research concerns applicationsfor the Semantic Web.
The documents avail-able in the World Wide Web are mostly writtenin natural language.
As such, they are under-standable only to humans.
One of the directionsof Semantic Web research is about adding alayer to the documents that somehow formalizestheir content, making it understandable also tosoftware agents.
Such Semantic Web annota-tions can be seen as a way to mark explicitlythe meaning of certain parts of the documents.The dependency relations provided by a parsersuch as Pro3Gres, combined with domain spe-cific axioms, allow the creation of (some of) thesemantic annotations, as described in (Rinaldiet al, 2003; Kaljurand et al, 2004).The modified QA system (using Pro3Gres) isbeing exploited in the area of ?Life Sciences?, forapplications concerning Knowledge Discoveryover Medline abstracts (Rinaldi et al, 2004a;Dowdall et al, 2004).
We illustrate some of thedifferences between general-purpose parsing andthe parsing of highly technical texts like Med-line and give two evaluations.5.1 General unrestricted textsWe first report an evaluation on sentences froman open domain, which gives a good impressionof the performance of the parser on general, un-restricted text.In traditional constituency approaches,parser evaluation is done in terms of the corre-spondence of the bracketting between the goldstandard and the parser output.
(Lin, 1995;Carroll et al, 1999) suggest evaluating on thelinguistically more meaningful level of syntacticrelations.
Two evaluations on the syntacticrelation level are reported in the following.First, a general-purpose evaluation using ahand-compiled gold standard corpus (Carrollet al, 1999), which contains the grammaticalrelation data of 500 random sentences from theSusanne corpus.The performance, shown in table 2, is, accord-ing to (Preiss, 2003), similar to a large selectionof statistical parsers and a grammatical relationfinder.
Relations involving long-distance depen-dencies form part of these relations.
In order tomeasure specifically their performance, a selec-tion of them is also given: WH-Subject (WHS),WH-Object (WHO), passive Subject (PSubj),control Subject (CSubj), and the anaphor of therelative clause pronoun (RclSubjA).5.2 Parsing highly technical languageWhile measuring general parsing performance isfundamental in the development of any parsingsystem there is a danger of fostering domain de-pendence in concentrating on a single domain.In order to answer how the parser performsover domains markedly different to the trainingcorpus , the parser has been applied to the GE-NIA corpus (Kim et al, 2003), 2000 MEDLINEabstracts of more than 400,000 words describingthe results of Biomedical research.Average sentence length is 27 words, the lan-Percentage Values for some relations, general, on Carroll corpus only LDD-involvingSubject Object noun-PP verb-PP subord.
cl.
WHS WHO PSubj CSubj RclSubjAPrecision 91 89 73 74 68 92 60 n/a 80 89Recall 81 83 67 83 n/a 90 86 83 n/a 63Table 2: Results of evaluating the parser output on Carroll?s test suite on subject, object, PP-attachment and clause subordination relations, and a selective evaluation of 5 relations involvinglong-distance dependencies (LDD)Percentage Values for some relations, general, on the GENIA corpusSubject Object noun-PP verb-PP subord.
clausePrecision 90 94 83 82 71Recall 86 95 82 84 75Table 3: Results of evaluating 100 random sentences from the terminology-annotated GENIAcorpus, on subject, object, PP-attachment and clause subordination relationsguage is very technical and extremely domain-specific.
But the most striking characteristicof this domain is the frequency of MultiWordTerms (MWT) which are known to cause seri-ous problems for NLP systems (Sag et al, 2002),(Dowdall et al, 2003).
The token to chunk ra-tio: NPs = 2.3 , VPs = 1.3 (number of tokensdivided by the number of chunks) is unusuallyhigh.The GENIA corpus does not include any syn-tactic annotation (making standard evaluationmore difficult) but approx.
100, 000 MWTs areannotated and assigned a semantic type fromthe GENIA ontology.This novel parsing application is designed todetermine how parsing performance interactswith MWT recognition as well as the applica-bility and possible improvements to the proba-blistic model over this domain, to test the hy-pothesis if terminology is the key to a successfulparsing system.
We do not discard this infor-mation, thus simulating a situation in which anear-perfect terminology-recognition tool is atone?s disposal.
MWT are regarded as chunks,the parsing thus takes place between betweenthe heads of MWT, words and chunks.100 random sentences from the GENIA cor-pus have been manually annotated for this eval-uation and compared to the parser output.
De-spite the extreme complexity and technical lan-guage, parsing performance under these condi-tions is considerably better than on the Carrollcorpus when using automated chunking, as ta-ble 3 reveals.It is worth noting that 10 of the 17 subjectprecision errors (out of 171 subjects) are ?hard?cases involving long-distance dependencies (1control, 4 relative pronouns) and 5 verb groupchunking errors.
Equally interesting, 2 of the 4object recall errors (out of 79 objects) are dueto 1 mistagging and 1 mischunking.In practice, MWT extraction is still not au-tomated to the level of chunking or Name En-tity recognition simulated in this experiment(for a comprehensive review of the state-of-the-art see (Castellv et al, 2001)).
This is, ina large part, due to the lack of definitive or-thographic, morphological and syntactic char-acteristics to differentiante between MWTs andcanonical phrases.
So MWT extraction remainsa semi-automated task performed in cycles withthe result of each cycle requiring manual valida-tion.
The return for this time consuming activ-ity are the characteristics of MWTs which canbe use to fine tune the algorithms during thenext extraction cycle.6 ConclusionWe have suggested a robust, deep-linguisticgrammar theory delivering grammatical rela-tion structures as output, which are closer topredicate-argument structures than pure con-stituency structures, and more informative ifnon-local dependencies are involved.
We havepresented an implementation of the theory thatis used for large-scale parsing.
An evaluationat the grammatical relation level shows that itsperformance is state-of-the-art.ReferencesSteven Abney.
1995.
Chunks and dependencies:Bringing processing evidence to bear on syntax.In Jennifer Cole, Georgia Green, and Jerry Mor-gan, editors, Computational Linguistics and theFoundations of Linguistic Theory, pages 145?164.CSLI.Roberto Basili and Fabio Massimo Zanzotto.
2002.Parsing engineering and empirical robustness.Journal of Natural Language Engineering, 8/2-3.Roberto Basili, Maria Teresa Pazienza, andFabio Massimo Zanzotto.
1998.
Evaluating a ro-bust parser for Italian language.
In Proceedings ofEvaluations of Parsing Systems Workshop, heldjointly with 1st LREC, Granada,Spain.Rens Bod, Remko Scha, and Khalil Sima?an, editors.2003.
Data-Oriented Parsing.
Center for theStudy of Language and Information, Studies inComputational Linguistics (CSLI-SCL).
ChicagoUniversity Press.Ted Briscoe and John Carroll.
2002.
Robust accu-rate statistical annotation of general text.
In Pro-ceedings of the 3rd International Conference onLanguage Resources and Evaluation, pages 1499?1504, Las Palmas, Gran Canaria.M.
Burke, A. Cahill, R. O?Donovan, J. van Gen-abith, and A.
Way.
2004.
Treebank-based ac-quisistion of wide-coverage, probabilistic LFG re-sources: Project overview, results and evalua-tion.
In The First International Joint Confer-ence on Natural Language Processing (IJCNLP-04), Workshop ?Beyond shallow analyses - For-malisms and statistical modeling for deep analy-ses?, Sanya City, Hainan Island, China.Aoife Cahill, Michael Burke, Ruth O?Donovan,Josef van Genabith, and Andy Way.
2004.Long-distance dependency resolution in automat-ically acquired wide-coverage PCFG-based LFGapproximations.
In Proceedings of ACL-2004,Barcelona, Spain.John Carroll, Guido Minnen, and Ted Briscoe.1999.
Corpus annotation for parser evaluation.In Proceedings of the EACL-99 Post-ConferenceWorkshop on Linguistically Interpreted Corpora,Bergen, Norway.M.
Teresa Cabre?
Castellv, Rosa Estopa?, andJordi Vivaldi Palatresi, 2001.
Recent Advances inComputational Terminology, chapter Automaticterm detection: A review of current systems,pages 53?87.
John Benjamins.Eugene Charniak.
1996.
Tree-bank grammar.
Tech-nical Report Technical Report CS-96-02, Depart-ment of Computer Science, Brown University.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of the NorthAmerican Chapter of the ACL, pages 132?139.Michael Collins and James Brooks.
1995.
Preposi-tional attachment through a backed-off model.
InProceedings of the Third Workshop on Very LargeCorpora, Cambridge, MA.Michael Collins.
1996.
A new statistical parserbased on bigram lexical dependencies.
In Proceed-ings of the Thirty-Fourth Annual Meeting of theAssociation for Computational Linguistics, pages184?191, Philadelphia.Michael Collins.
1997.
Three generative, lexicalisedmodels for statistical parsing.
In Proc.
of the 35thAnnual Meeting of the ACL, pages 16?23, Madrid,Spain.Michael Collins.
1999.
Head-Driven Statistical Mod-els for Natural Language Parsing.
Ph.D. thesis,University of Pennsylvania, Philadelphia, PA.Pter Dienes and Amit Dubey.
2003.
Antecedentrecovery: Experiments with a trace tagger.
InProceedings of the 2003 Conference on Empir-ical Methods in Natural Language Processing(EMNLP), Sapporo, Japan.James Dowdall, Fabio Rinaldi, Fidelia Ibekwe-Sanjuan, and Eric SanJuan.
2003.
Complexstructuring of term variants for question answer-ing.
In Proceedings of the ACL workshop on Mul-tiWord Expressions: Analysis, Acquisition andTreatment, Sapporo, Japan, July.James Dowdall, Fabio Rinaldi, Andreas Persidis,Kaarel Kaljurand, Gerold Schneider, and MichaelHess.
2004.
Terminology expansion and re-lation identification between genes and path-ways.
In Workshop on Terminology, Ontologyand Knowledge Representation.
Universite JeanMoulin (Lyon 3).Jason Eisner.
1997.
Bilexical grammars and a cubic-time probabilistic parser.
In Proceedings of the5th International Workshop on Parsing Technolo-gies, pages 54?65, MIT, Cambridge, MA, Septem-ber.Jason Eisner.
2000.
Bilexical grammars and theircubic-time parsing algorithms.
In Harry Bunt andAnton Nijholt, editors, Advances in Probabilis-tic and Other Parsing Technologies.
Kluwer Aca-demic Publishers.Daniel Gildea.
2001.
Corpus variation and parserperformance.
In Proceedings of the 2001 Confer-ence on Empirical Methods in Natural LanguageProcessing (EMNLP), pages 167?202, Pittsburgh,PA.Joshua Goodman.
2003.
Efficient parsing of DOPwith PCFG-reductions.
In Bod et al (Bod et al,2003).Jan Hajic?.
1998.
Building a syntactically annotatedcorpus: The Prague Dependency Treebank.
InEva Hajic?ova?, editor, Issues of Valency and Mean-ing.
Studies in Honor of Jarmila Panevova?, pages106?132.
Karolinum, Charles University Press,Prague.James Henderson.
2003.
Inducing history repre-sentations for broad coverage statistical parsing.In Proceedings of HLT-NAACL 2003, Edmonton,Canada.Julia Hockenmaier and Mark Steedman.
2002.
Gen-erative models for statistical parsing with combi-natory categorial grammar.
In Proceedings of 40thAnnual Meeting of the Association for Computa-tional Linguistics, Philadelphia.Valentin Jijkoun.
2003.
Finding non-local depen-dencies: beyond pattern matching.
In Proceedingsof the ACL 03 Student Workshop, Budapest.Mark Johnson.
2002.
A simple pattern-matchingalgorithm for recovering empty nodes and theirantecedents.
In Proceedings of the 40th Meetingof the ACL, University of Pennsylvania, Philadel-phia.Kaarel Kaljurand, Fabio Rinaldi, James Dowdall,and Michael Hess.
2004.
Exploiting language re-sources for semantic web annotations.
In Proceed-ings of LREC 2004, Lisbon, May 24-30. acceptedfor publication.Ron Kaplan, Stefan Riezler, Tracy H. King, JohnT.
Maxwell III, Alex Vasserman, and RichardCrouch.
2004.
Speed and accuracy in shallowand deep stochastic parsing.
In Proceedings ofHLT/NAACL 2004, Boston, MA.J.D.
Kim, T. Ohta, Y. Tateisi, and J. Tsujii.
2003.Genia corpus - a semantically annotated corpusfor bio-textmining.
Bioinformatics, 19(1):i180?i182.Dekang Lin.
1995.
A dependency-based method forevaluating broad-coverage parsers.
In Proceedingsof IJCAI-95, Montreal.Mitch Marcus, Beatrice Santorini, and M.A.Marcinkiewicz.
1993.
Building a large annotatedcorpus of English: the Penn Treebank.
Computa-tional Linguistics, 19:313?330.Wolfgang Menzel.
1995.
Robust processing of natu-ral language.
Lecture Notes in Computer Science,981:19?34.Diego Molla?, Gerold Schneider, Rolf Schwitter, andMichael Hess.
2000.
Answer Extraction using aDependency Grammar in ExtrAns.
TraitementAutomatique de Langues (T.A.L.
), Special Issueon Dependency Grammar, 41(1):127?156.Peter Neuhaus and Norbert Bro?ker.
1997.
The com-plexity of recognition of linguistically adequatedependency grammars.
In Proceedings of the 35thACL and 8th EACL, pages 337?343, Madrid,Spain.Joakim Nivre.
2004.
Inductive dependency parsing.In Proceedings of Promote IT, Karlstad Univer-sity.Judita Preiss.
2003.
Using grammatical relations tocompare parsers.
In Proc.
of EACL 03, Budapest,Hungary.Stefan Riezler, Tracy H. King, Ronald M. Kaplan,Richard Crouch, John T. Maxwell, and MarkJohnson.
2002.
Parsing the Wall Street Journalusing a Lexical-Functional Grammar and discrim-inative estimation techniques.
In Proc.
of the 40thAnnual Meeting of the Association for Computa-tional Linguistics (ACL?02), Philadephia, PA.Fabio Rinaldi, James Dowdall, Michael Hess, DiegoMolla?, and Rolf Schwitter.
2002.
Towards AnswerExtraction: an application to Technical Domains.In ECAI2002, European Conference on ArtificialIntelligence, Lyon, pages 460?464.Fabio Rinaldi, Kaarel Kaljurand, James Dowdall,and Michael Hess.
2003.
Breaking the deadlock.In ODBASE 2003 (International Conference onOntologies, Databases and Applications of Seman-tics) Catania, Italy., volume 2889 of Lecture Notesin CS.
Springer Verlag.Fabio Rinaldi, James Dowdall, Gerold Schneider,and Andreas Persidis.
2004a.
Answering Ques-tions in the Genomics Domain.
In ACL 2004Workshop on Question Answering in restricteddomains, Barcelona, Spain, 21?26 July.Fabio Rinaldi, Michael Hess, James Dowdall, DiegoMolla?, and Rolf Schwitter.
2004b.
Question an-swering in terminology-rich technical domains.
InMark Maybury, editor, New Directions in Ques-tion Answering.
MIT/AAAI Press.Ivan A.
Sag, Timothy Baldwin, Francis Bond, AnnCopestake, and Dan Flickinger.
2002.
MultiwordExpressions: a Pain in the Neck for NLP.
In Pro-ceedings of the Third International Conference,CICLing 2002, pages 1?15, Mexico City, Febru-rary.Anoop Sarkar, Fei Xia, and Aravind Joshi.
2000.Some experiments on indicators of parsing com-plexity for lexicalized grammars.
In Proc.
ofCOLING.Gerold Schneider, Diego Molla` Aliod, and MichaelHess.
2000.
Inkrementelle minimale logische for-men fr die antwortextraktion.
In Proceedings of34th Linguistic Colloquium, September 1999, Uni-versity of Mainz, FASK.Gerold Schneider.
2003a.
Extracting and usingtrace-free Functional Dependencies from the PennTreebank to reduce parsing complexity.
In Pro-ceedings of Treebanks and Linguistic Theories(TLT) 2003, Va?xjo?, Sweden.Gerold Schneider.
2003b.
A low-complexity, broad-coverage probabilistic dependency parser for En-glish.
In Proceedings of HLT-NAACL 2003 Stu-dent session, Edmonton, Canada.Rolf Schwitter, Diego Molla?
Aliod, and MichaelHess.
1999.
ExtrAns - answer extraction fromtechnical documents by minimal logical formsand selective highlighting.
In Proceedings of TheThird International Tbilisi Symposium on Lan-guage, Logic and Computation, Batumi, Georgia.Daniel D. Sleator and Davy Temperley.
1993.
Pars-ing English with a link grammar.
In Proc.
ThirdInternational Workshop on Parsing Technologies,pages 277?292.Pasi Tapanainen and Timo Ja?rvinen.
1997.
A non-projective dependency parser.
In Proceedings ofthe 5th Conference on Applied Natural LanguageProcessing, pages 64?71.
Association for Compu-tational Linguistics.
