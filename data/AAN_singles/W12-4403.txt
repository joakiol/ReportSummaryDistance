Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 21?29,Jeju, Republic of Korea, 8-14 July 2012. c?2012 Association for Computational LinguisticsAccurate Unsupervised Joint Named-Entity Extraction from UnalignedParallel TextRobert MunroDepartment of LinguisticsStanford UniversityStanford, CA 94305rmunro@stanford.eduChristopher D. ManningDepartment of Computer ScienceStanford UniversityStanford, CA 94305manning@stanford.eduAbstractWe present a new approach to named-entityrecognition that jointly learns to identifynamed-entities in parallel text.
The sys-tem generates seed candidates through local,cross-language edit likelihood and then boot-straps to make broad predictions across bothlanguages, optimizing combined contextual,word-shape and alignment models.
It is com-pletely unsupervised, with no manually la-beled items, no external resources, only us-ing parallel text that does not need to be eas-ily alignable.
The results are strong, withF > 0.85 for purely unsupervised named-entity recognition across languages, comparedto just F = 0.35 on the same data for su-pervised cross-domain named-entity recogni-tion within a language.
A combination of un-supervised and supervised methods increasesthe accuracy to F = 0.88.
We conclude thatwe have found a viable new strategy for unsu-pervised named-entity recognition across low-resource languages and for domain-adaptationwithin high-resource languages.1 IntroductionAt first pass, our approach sounds like it shouldn?twork, as ?unsupervised?
tasks significantly under-perform their supervised equivalents and for mostcross-linguistic tasks ?unaligned?
will mean ?unus-able?.
However, even among very loosely alignedmultilingual text it is easy to see why named-entitiesare different: they are the least likely words/phrasesto change form in translation.
We can see this in thefollowing example which shows the named-entitiesin both a Kre`yol message and its English translation:Lopital Sacre-Coeur ki nan vil Milot, 14km nan sid vil Okap, pre pou li resevwamoun malad e lap mande pou moun kimalad yo ale la.Sacre-Coeur Hospital which located inthis village Milot 14 km south of Oakpis ready to receive those who are injured.Therefore, we are asking those who aresick to report to that hospital.The example is taken from the parallel corpus ofEnglish and Haitian Kre`yol text messages used inthe 2010 Shared Task for the Workshop on MachineTranslation (Callison-Burch et al, 2011), which isthe corpus used for evaluation in this paper.The similarities in the named-entities across thetranslation are clear, as should be the intuition forhow we can leverage these for named-entity ex-traction.
Phrases with the least edit distance be-tween the two languages, such as ?Lopital Sacre-Coeur?, ?Milot?, and ?Okap?, can be treated ashigh-probability named-entity candidates, and thena model can be bootstrapped that exploits predictivefeatures, such as word shape (e.g.
: more frequentcapitalization) and contextual cues such as the pre-ceding ?vil?
in two cases above.However, the problem of identifying entities inthis way is non-trivial due to a number of complicat-ing factors.
The inexact translation repeats the non-entity ?hospital?
which limits machine-translation-style alignments and has an equal edit-distance withthe entity ?Loptial?.
The entity ?Hospital?
and ?Lo-pital?
are not an exact match and are not perfectlyaligned, changing position within the phrase.
The21capitalization of entities is not always so consistent(here and in short-message communications moregenerally).
A typographic error in the translationwrites ?Okap?
as ?Oakp?.
?Okap?
is itself slang for?Cap-Ha??tien?
and other messages translated this lo-cation across the different spellings (?Cap-Haitien?,?Cap Haitien?, ?Kap?, ?Kapayisyen?, etc.
), which in-creases the edit distance.
There are few resourcesfor Haitian Kre`yol such as gazatteers of place names(except at the Department/major Town/City level ?at the time these messages were sent, Google Mapsand Open Street Map listed only a handful of loca-tions in Haiti, and such resources tend not to includeslang terms).
Finally, what was one sentence in theoriginal message is split into two in the translation.As Kay points out, most parallel texts shouldn?tbe alignable, as different contexts mean differenttranslation strategies, most of which will not resultin usable input for machine translation (Kay, 2006).This is true of the corpus used here ?
the translationswere made for quick understanding by aid workers,explaining much of the above: it was clearer to breakthe translation into two sentences; it reduced ambi-guity to repeat ?hospital?
rather than leave it under-specified; the typo simply didn?t matter.
We con-firmed the ?unalignability?
of this corpus using theGIZA++ aligner in the Moses toolkit (Koehn et al,2007); by noting Microsoft Research?s work on thesame data where they needed to carefully retranslatethe messages for training (Lewis, 2010); and fromcorrespondence with participants in the 2011 Work-shop on Machine Translation who reported the needfor substantial preprocessing and mixed results.We do not rule out the alignability of the corpusaltogether ?
the system presented here could even beused to create better alignment models ?
noting onlythat it is rare that translations can be used straight-of-the-box, while in our case we can still make use ofthis data.
Even with perfect alignment, the accuracyfor named-entity extraction in Haitian Kre`yol couldonly be as accurate as that for English, which in thiscase was F = 0.336 with a supervised model, soalignment is therefore only part of the problem.For the same reasons, we are deliberately omittinganother important aspect of cross-linguistic named-entity recognition: transliteration.
Latin Scriptmay be wide-spread, especially for low resourcelanguages where it is the most common script fortranscribing previously non-written languages, butsome of the most widely spoken languages includethose that use Arabic, Bengali, Cyrillic, Devanagari(Hindi) and Hanzi (Chinese) scripts, and the meth-ods proposed here would be even richer if they couldalso identify named entities across scripts.
A firstpass on cross-script data looks like it is possible toapply our methods across scripts, especially becausethe seeds only need to be drawn from the most con-fident matches and across scripts there seem to besome named entities that are more easier to translit-erate than others (which is not surprising, of course ?most cross-linguistic tasks are heterogeneous in thisway).
However, with a few notable exceptions likeTao et al (2006), transliteration is typically a super-vised task.
As with machine translation it is likelythat the methods used here could aid transliteration,providing predictions that can be used within a fi-nal, supervised transliteration model (much like thesemi-supervised model proposed later on).11.1 The limitations of edit-distance andsupervised approachesDespite the intuition that named-entities are lesslikely to change form across translations, it is clearlyonly a weak trend.
Even if we assume oracle knowl-edge of entities in English (that is, imagining thatwe have perfect named-entity-recognition for En-glish), by mapping the lowest edit-distance phrasein the parallel Kre`yol message to each entity we canonly identify an entity with about 61%, accuracy.Without oracle knowledge ?
training on an existingEnglish NER corpora, tagging the English transla-tions, and mapping via edit distance ?
identifies anentity with only around 15% accuracy.
This is notparticularly useful and we could probably achievethe same results with naive techniques like cross-linguistic gazetteers.Edit distance and cross-linguistic supervisednamed-entity recognition are not, therefore, partic-ularly useful as standalone strategies.
However, weare able to use aspects of both in an unsupervisedapproach.1On a more practical level, we also note that this year?sshared task for the Named Entity Workshop is on translitera-tion.
With the leading researchers in the field currently tacklingthe transliteration problem, it is likely that any methods we pre-sented here would soon be outdated.22In this paper we focus on named-entity identifica-tion, only briefly touching on named-entity classifi-cation (distinguishing between types of entities), pri-marily because the named-entity identification com-ponent of our system is more novel and thereforedeserves greater attention.We use 3,000 messages in Haitian Kre`yoland their English translations, with named-entitiestagged in an evaluation set of 1,000 of the messages.To keep the task as unsupervised a possible, the sys-tem was designed and parameters were set withoutobserving the actual tags.1.2 Strategy and potential applicationsOur approach is two-step for pairs of low resourcelanguages, and three-step for pairs of languageswhere one has named-entity resources:1.
Generate seeds by calculating the edit like-lihood deviation.
For all cross-languagepairs of messages, extract the cross-languageword/phrase pairs with the highest edit like-lihood, normalized for length.
Calculate theintramessage deviation of this edit likelihoodfrom the mean pair-wise likelihood from allcandidate pairs within the message.
Acrossall messages, generate seeds by selecting theword/phrase pairs with the highest and lowestintramessage edit likelihood deviation.2.
Learn context, word-shape and alignment mod-els.
Using the seeds from Step 1, learn mod-els over the context, word-shape and align-ment properties (but not edit distance).
Applythe models to all candidate pairs.
Because wehave the candidate alignments between the lan-guages, we can also jointly learn to identifynamed-entities by leveraging the context andword-shape features in the parallel text, in com-bination with the alignment predictions.3.
Learn weighted models over the context, word-shape, alignment and supervised predictions(with high-resource languages only).
Using theseeds from Step 1 and predictions from Step 2,learn models over the broader features and su-pervised predictions from a model in the high-resource language, applying the models to allcandidate pairs.The results are very strong, with F > 0.85for purely unsupervised named-entity recognitionacross languages.
This is compared to just F = 0.35for supervised approaches across domains within alanguage (MUC/CoNLL-trained English applied tothe English translations of the messages).The combined unsupervised/supervised methodsincrease the accuracy to F = 0.88.
Inter-annotatoragreement is around 0.95, so this may be close to thebest possible result.This leads us to conclude that cross-linguistic un-supervised named-entity recognition, even when notalignable via machine-translation methods, is a pow-erful, scalable technique for named-entity recogni-tion in low resource languages.The potential applications of are broad.
There aresome 5,000 languages in the connected world, mostof which will have no resources other than loosetranslations, so there is great application potential.For high-resource languages, the results here indi-cate that the technique can be used to increase ac-curacy in cross-domain named-entity recognition, aconsistent problem across even closely-related do-mains.
For the specific corpus used there is alsodirect practical value ?
the messages include highvolumes of time-critical requests for aid, citing lo-cations that did not appear on any map in a languagewith few resources.2 STEP 1: Establish Edit LikelihoodDeviationAs we state in the introduction, we cannot simplytag in English and then find the least-edit distanceword/phrase in the parallel Kre`yol.We evaluated several different edit distance func-tions, including the well-known Levenshtein andslightly more complex Jaro-Winkler measures.
Wealso extended the Levenshtein measure by reducingthe edit penalty for pairs of letters of phonetic relat-edness, such as ?c?
and ?k?, following the subwordmodeling work of Munro and Manning on this cor-pus and previous subword modeling for short mes-sages (Munro, 2011; Munro and Manning, 2010).22We also attempted a more sophisticated approach to learn-ing weights for edits by extracting edit probabilities from the fi-nal model.
This also made little improvement, but it could havesimply been the result data-sparseness over only 3000 pairs ofentities, so no strong conclusions can be drawn.23The more sophisticated edit distance functionsgave more accurate predictions (which is unsurpris-ing), but the advantages were lost in the followingstep when calculating the deviation from the norm,with all approaches producing more or less the sameseeds.
Rather than the String Similarity Estimate be-ing the key factor, we conclude that our novel treat-ment of edit distance (calculating the local devia-tion) is the critical factor in generating seeds for themodel.All else being equal, then, we report results fromthe simplest approach to edit distance, normalizingLevenshtein?s measure, LEV () by length to a 0-1 scale.
Candidate words/phrases were limited toa maximum of four words, delimited by space orpunctuation, simply to cap the cost of the LEV ().Given a string S in message M , MS and and its can-didate pair M ?S?
, and a length function LEN(), thisgives us SSE(MS ,M ?S?)
=1?
(2(LEV (MS ,M?S?))
+ 1LEN(MS) + LEN(M ?S?)
+ 1The +1 smoothing is to avoid too much variationat smaller lengths, which is fairly common practicein subword models looking at morphological varia-tion (Tchoukalov et al, 2010).The String Similarity Estimate is a global measurethat is not sensitive to the contexts of the given pairs.Suppose a sentence wasn?t a translation, but simplya repetition, or that much of the translation was adirect (non-translated) quote of the original.
Bothoccur in the data we used.We propose, then, that the best candidate seedsfor named-entities are those that display the highestlikelihood relative to the other candidate pairs withinthe same pairs of messages.
In other words, whenthere are two phrases with very little edit distance,but when there is very high cross-language edit dis-tance between the contexts of the phrases.
We definethis as Edit Likelihood Deviation, ELD().There are many ways to calculating deviation.Again, to keep it as simple as possible we report re-sults using the most well-known deviation metric, z-scores.
Given average and standard deviation func-tions AV () and SD(), gives ELD(MS ,M ?S?)
=(SSE(MS ,M ?S?
))?AV (SSE(M0?n,M ?0?m))SD(SSE(M0?n,M ?0?m))Figure 1: A comparison of the different approaches togenerating seeds from edit distance.
The comparisonshows that local deviation, the novel method introducedin this paper, is the most successful.
With about 10%of the most confident entity candidates by Edit Likeli-hood Deviation or Weighted Deviation Estimate, there isgreater than 95% precision, giving a clean enough divi-sion of the data to seed a model.At this point, we have the global string similarity ofeach candidate entity pair across languages, SSE(),and the local string similarity deviation of each can-didate pair, ELD().A combination was also explored that combinedthe two, creating an equally weighted product ofSSE and ELD(), Weighted Deviation Estimate,WDE() (equation omitted for space).
As Figure1 shows, there is only a slight improvement fromthe combination of the two, showing that Edit Like-lihood Deviation, the novel approach here, con-tributes the most to identifying candidate seeds.We can calculate the first accuracies here by as-suming that the best candidate in each message pairwas an entity.
All results also summarized at the endof the paper:Precision Recall F-valueKre`yol: 0.619 0.619 0.619English: 0.633 0.633 0.633The results are reasonably strong for methods thatmade few assumptions about the data and were notoptimized, with errors in a little under half the pre-dictions.While the different equations are monotonicallydistributed within each pair of messages, the esti-24mates between messages now take into account bothlocal and global edit likelihoods, allowing us to rankthe candidates by WDE and sample the most likelyand least likely.
Here, we simply took the top andbottom 5%.33 STEP 2: Learn joint alignment andword-shape models using the likelihoodestimates as seeds.Taking the seeds from Step 1, we can then treat themas training items in a linear model.We used the Stanford Maximum Entropy Classi-fier.
Model-choice is only important in that a dis-criminative learner is required.
The 5% ?non-entity?pairs were still the highest String Similarity for theirparticular message/translation, but simply did notdeviate greatly from the average within that mes-sage/translation.
Therefore, we are explicitly target-ing the border between entities and non-entities inthe high String Similarity part of the vector space.This sampling strategy would not work for a gener-ative learner.For the same reason, though, we do not includeraw edit distance or the String Similarity Estimateamong the features.
If we did, then the model willsimply relearn and overfit this bias and give all theweight to edit distance.We build the model on features that include con-text (the entity itself and surrounding words), word-shape features (capitalization, punctuation, segmen-tation, and numerical patterns), and alignment (ab-solute and relative character offsets between the can-didates in the messages and translation).
For word-shape features, we used a simple representation thatconverted all sequences of capitalized letters, lower-case letters, and non-letter characters into ?C?, ?c?and ?n?, respectively.
Therefore, ?Port-au-Prince?,?Port au Prince?
and ?Port.a.Prons?
would all getthe same word-shape feature, ?CcncnCc?.
We al-3There are clearly many more parameters and variants ofequations that could be explored.
As an unsupervised approach,it is by conscious choice that only the most well-known equa-tions are used and tunable parameters are set at sensible defaults(like the equal weights here).
This is to keep the experiments ascleanly ?unsupervised?
as possible, and to demonstrate that theaccurate results here are not simply a quirk of a particular equa-tion, but a broadly applicable approach to generating seeds bylocal deviation estimates.Figure 2: Comparing the predictions for the String Sim-ilarity for the same candidates, to the jointly-learnedmodel.
(Coding scheme: tp = true-positive, etc.)
Thedistribution shows that while String Similarity correlateswith named-entities, it is not a clean division.
Note espe-cially the mass of true-negatives in the bottom-right cor-ner of the graph.
These would be a relatively high vol-ume of false-positives for String Similarity alone, but themodel that bootstraps knowledge of context, word-shapeand alignment has little trouble distinguishing them andcorrectly assigning them zero-probably of being an entity.lowed the model to also find character-ngrams overthese shapes to capture features which would rep-resent characteristics like ?is-capitalized?, ?contains-internal-capital?, and ?is-multiword-phrase?.As a relatively small set of features, we alsomodel the intersection of each of them.
This al-lows the model to learn, for example, that words thatare perfectly aligned, but are both all lower-case, areweighted 0.06 more likely as a non-entity.
Despitethe simplicity and low number of features, this is afairly powerful concept to model.As with all unsupervised methods that bootstrappredictions through seeded data, the success relieson a representative feature space to avoid learningonly one part of the problem.
The results are strong:Precision Recall F-valueKre`yol: 0.907 0.687 0.781English: 0.932 0.766 0.840There is a reasonably high precision-recall ratiowhich is typical of unsupervised learning that learnsa model on seeded data, but the results are still25strong for both Kre`yol and English, indicating thatthe seeding method in Step 1 did, in fact, producecandidates that occurred in broad range of contexts,overcoming one of the limits of gazetteer-based ap-proaches.Perhaps the most obvious extension is to jointlylearn the models on both languages, using the candi-date alignment models in combination with the con-texts in both the original text and the translation:Precision Recall F-valueKre`yol: 0.904 0.794 0.846English: 0.915 0.813 0.861This improves the results for both, especially theKre`yol which can now take advantage of the moreconsistent capitalization and spelling in the Englishtranslations.For many supervised learners, 0.846 would be astrong result.
Here, we are able to get this in HatianKre`yol using only unsupervised methods and a fewthousand loosely translated sentences.4 STEP 3: Learning weighted models overthe context, word-shape, alignment andsupervised predictions (withhigh-resource languages)The natural extension to the supervised comparisonis to combine the methods.
We included the StanfordNER predictions in the features for the final model,allowing the bootstrapped model to arrive at the op-timal weights to apply to the supervised predictionsin the given context.From the perspective of supervised NER, thiscan be thought of as leveraging unsupervised align-ment models for domain-adaptation.
The StanfordNER predictions were added as features in the finalmodel, directly for the English phrases and acrossthe candidate alignments for the Kre`yol phrases.Taken alone, the unsupervised strategies clearlyimprove the results, but for someone coming from asupervised learning background in NER (which willbe most NER researchers) this should provide an in-tuition as to exactly how good.
We cannot comparethe Kre`yol as there is no supervised NER corpus forKre`yol, and our labeled evaluation data is too smallto train on.
However, we can compare the Englishresults to near state-of-the-art NER taggers.We compared our system to the predictions madeby the Stanford NER parser trained on MUC andCoNLL data (Sang, 2002; Sang and De Meulder,2003):Precision Recall F-valueEnglish: 0.915 0.206 0.336The low cross-domain result is expected, but 0.336for supervised cross-domain predictions within alanguage is much less than 0.861 for unsupervisedcross-language predictions.
This clearly shows thatthe methods and evaluation used here really dodemonstrate a new strategy for NER.
It also showsthat domain-specificity might be even be more im-portant than language-specificity when we can boot-strap our knowledge of context.4Combining the two approaches, we get the mostaccurate results:Precision Recall F-valueKre`yol: 0.838 0.902 0.869English: 0.846 0.916 0.880Even though English is a high-resource language,this is still a very good result for cross-domain adap-tation, with F > 0.5 improvement over the super-vised model alone.
It is clear that this strategy couldbe used for domain adaptation more broadly wher-ever loose translations exists.While not as big a gain in accuracy as the previ-ous steps, the F > 0.02 gain is still significant.
Al-though untested here, it is easy to imagine that with asmall amount of labeled data or improved gazetteersthe supervised approach should further.
About 10%of the error can be attributed to capitalization, too,which is a slight bias against the MUC/CoNLLtrained data where the capitalization of named enti-ties was consistent.
A realistic deployment approachwould be to create an initial model using the unsu-pervised methods described in this paper and then tofurther bootstrap the accuracy through supervised la-beling.
This particular approach to semi-supervisedlearning is outside the scope of this paper.4For the edge cases and entity boundary errors, we alwaysgave the benefit of the doubt to the Stanford NER tagger.264.1 Distinguishing Types of EntityNER often distinguishes types of Entities (eg: Peo-ple, Locations, Organizations); a frequent subtasksometimes called named-entity discrimination ornamed-entity classification.
We discuss this briefly.By seeding the data with the Stanford NER pre-dictions for ?Person?, ?Location?, and ?Organization?and learning a three-way distinction within the enti-ties, we saw that it wasn?t a difficult problem for thisparticular corpus.
The main potential complicationwas between organizations and locations (especiallyfor radio stations) but there were relatively few or-ganizations in the data so the micro-fvalue wouldchange very little.
No doubt, in other texts the lo-cation/organization division would compose a big-ger part of the problem.
These observations aboutdistinguishing NERs are consistent with the knownproblems in NER more broadly.
The Stanford NERonly made predictions for 114 of the entities thatwere confidently mapped to their Kre`yol counter-parts in Step 1:Precision Recall F-valueEnglish: 0.512 0.782 0.619To exploit any signal here, let alne a respectableF = 0.619 is a good result, but clearly more im-provements are possible.5 AnalysisThe results presented in the paper are summarizedin Table 1.
Taken together, they make it clear thatthis is a very promising new method for named-entity recognition in low resources languages, andfor domain-adaptation in high-resource languages.Analysis of the consistent errors shows severalclear patterns.
Products like ?aquatab?
were a com-mon false positive, although a product could be anamed-entity in certain coding schemas.
Dates, fig-ures and currency (?250gd?)
were also frequent falsepositives, but would be reasonably easy to filter asthey follow predictable patterns.Some cognates and borrowings alsomade it through as false-positives: ?antibi-otics?/?antibiotik?, ?drinking water?/?drinking wa-ter?, ?medicine?/?medicament?, ?vitamin c?/?vitaminec?, ?cyber?/?cyber?, ?radio?/?radyo?, although ?cybercafe?
almost always referred to a specific locationand ?radio?
was often part of an organization name,?radio enspirasyon?.The false-negatives were almost all very low-frequency words or high-frequency words that weremore commonly used as non-entities.
This is con-sistent with named-entity recognition more broadly.6 Background and Related WorkWe were surprised that no one had previously re-ported looked at leveraging cross-linguistic named-entity recognition in this way.
Perhaps previousresearchers had found (like us) that edit distancealone was not particularly useful in cross-linguisticnamed-entity recognition, and therefore not pursuedit.
While the approach is novel, the general observa-tion that named-entities change form less than otherwords cross-linguistically is one of the oldest in lan-guage studies.
Shakespeare?s ?River Avon?
simplymeans ?River River?, as ?Avon?
is, literally, ?River?in the pre-English Celtic language of the region.For parallel short-message corpora, named-entityrecognition is completely unresearched, but there isgrowing work in classification (Munro and Man-ning, 2010; Munro, 2011) and translation (Lewis,2010), the latter two using the same corpus as here.Past ?Multilingual Named-Entity Recognition?systems meant training the same supervised systemon different languages, which was the focus of thepast CoNLL tasks.
While the goal of these systemswas the same as ours ?
broad cross-linguistic cov-erage for named-entity recognition ?
this is not thesame ?cross-linguistic?
as the one employed here.More closely related to our work, Steinbergerand Pouliquen have found cross-linguistic named-entity recognition to be possible by aligning textsat the granularity of news stories (Steinberger andPouliquen, 2007), but using a supervised approachfor the first pass and focusing on transliteration.
Inother related work, the 2007 NIST REFLEX eval-uation (Song and Strassel, 2008), tasked partici-pants with using alignment models to map named-entities between English, Arabic, and Chinese data.They found that relying on alignment models alonewas very poor, even among these high-resource lan-guages, although it was a relatively small corpus(about 1,000 aligned entities).
The focus was more27on transliteration ?
an important aspect of translationthat we simply aren?t addressing here.Most earlier work used a tagger in one languagein combination with machine translation-style align-ments models.
Among these, Huang et al is themost closely related to our work as they are translat-ing rare named-entities, and are therefore in a similarlow-resource context (Huang et al, 2004).
As withthe NIST project, most work building on Huang etal.
has been in transliteration.Although not cross-linguistic, Piskorski et al?swork on NER for inflectional languages (2009) alsorelied on the similarities in edit distance between theintra-language variation of names.In gazetteer-related work, Wang et al and otherssince, have looked at edit distance within a language,modeling the distance between observed words andlists of entities (Wang et al, 2009).
Similarly, thereis a cluster of slightly older work on unsupervisedentity detection, also within one language (Pedersenet al, 2006; Nadeau et al, 2006), but all relying onweb-scale quantities of unlabeled data.While the implementation is not related, it isalso worth highlighting Lin et al?s very recent workon unsupervised language-independent name trans-lation the mines data from Wikipedia ?infoboxes?,(Lin et al, 2011) however the infoboxes give a fairlyand highly structured resource, that might be consid-ered more supervised than not.In alignment work, the foundational work isYarowsky et al?s induction of projections acrossaligned corpora (Yarowsky et al, 2001), most suc-cessfully adapted to cross-linguistic syntactic pars-ing (Hwa et al, 2005).
The machine translation sys-tems used named-entity recognition are too many tolist here, but as we say, the system we present couldaid translation considerably, especially in the con-text of low resources languages and humanitariancontexts, a recent focus in the field (Callison-Burchet al, 2011; Lewis et al, 2011).7 ConclusionsWe have presented a promising a new strategyfor named-entity recognition from unaligned paral-lel corpora, finding that unsupervised named-entityrecognition across languages can be bootstrappedfrom calculating the local edit distance deviation be-Unsupervised Precision Recall F-valueEdit likelihood deviationKre`yol: 0.619 0.619 0.619English: 0.633 0.633 0.633Language-specific modelsKre`yol: 0.907 0.687 0.781English: 0.932 0.766 0.840Jointly-learned modelsKre`yol: 0.904 0.794 0.846English: 0.915 0.813 0.861SupervisedEnglish: 0.915 0.206 0.336Semi-supervisedIdentificationKre`yol: 0.838 0.902 0.869English: 0.846 0.916 0.880Classification (micro-F)English: 0.512 0.782 0.619Table 1: A summary of the results presented in this papershowing promising new methods for unsupervised andsemi-supervised named-entity recognition.tween candidate entities.
Purely unsupervised ap-proaches are able to identify named entities withF = 0.846 accuracy for Kre`yol and F = 0.861 forEnglish, leveraging the candidate alignments for im-proved accuracy in both cases.
Combined with su-pervised learning, the accuracy rises to F = 0.869and F = 0.880 respectively, which is approachingthe level of accuracy achieved by in-domain super-vised systems.
It is rare for unsupervised systemsto be competitive with supervised approaches as ac-curacy is usually lost for coverage, but here it lookslike the method can be effective for both.There is the potential to apply this system to alarge number of natural language processing prob-lems, and to extend the system in a number of di-rections.
Each of the three steps has parametersthat could be optimized, especially in combinationwith supervised approaches.
The linguistic natureof the language pairs might also influence the effec-tiveness.
The results here are therefore the first pre-sentation of a new strategy ?
one that will hopefullylead to more research in extracting rich informationfrom a diverse range of low-resource languages.28ReferencesC.. Callison-Burch, P. Koehn, C. Monz, and Zaidan.
O.2011.
Findings of the 2011 workshop on statisticalmachine translation.
In Proceedings of the Sixth Work-shop on Statistical Machine Translation.F.
Huang, S. Vogel, and A. Waibel.
2004.
Improvingnamed entity translation combining phonetic and se-mantic similarities.
In Proc.
of HLT-NAACL, pages281?288.R.
Hwa, P. Resnik, A. Weinberg, C. Cabezas, and O. Ko-lak.
2005.
Bootstrapping parsers via syntactic projec-tion across parallel texts.
Natural language engineer-ing, 11(3):311?325.M.
Kay.
2006.
Translation, Meaning and Refer-ence.
Intelligent Linguistic Architectures: Variationson Themes by Ronald M. Kaplan, page 3.P.
Koehn, H. Hoang, A. Birch, C. Callison-Burch,M.
Federico, N. Bertoldi, B. Cowan, W. Shen,C.
Moran, R. Zens, et al 2007.
Moses: Open sourcetoolkit for statistical machine translation.
In Proceed-ings of the 45th Annual Meeting of the ACL on Inter-active Poster and Demonstration Sessions, pages 177?180.
Association for Computational Linguistics.W.
Lewis, R. Munro, and S. Vogel.
2011.
Crisis mt:Developing a cookbook for mt in crisis situations.
InProceedings of the Sixth Workshop on Statistical Ma-chine Translation.W.
Lewis.
2010.
Haitian Creole: How to Build and Shipan MT Engine from Scratch in 4 days, 17 hours, & 30minutes.
In 14th Annual Conference of the EuropeanAssociation for Machine Translation.W.P.
Lin, M. Snover, and H. Ji.
2011.
Unsuper-vised Language-Independent Name Translation Min-ing from Wikipedia Infoboxes.
Proceedings of theEMNLP Workshop on Unsupervised Learning in NLP,page 43.R.
Munro and C.D.
Manning.
2010.
Subword varia-tion in text message classification.
In Proceedings ofthe Annual Conference of the North American Chap-ter of the Association for Computational Linguistics(NAACL 2010).R.
Munro.
2011.
Subword and spatiotemporal mod-els for identifying actionable information in haitiankreyol.
In Fifteenth Conference on ComputationalNatural Language Learning (CoNLL 2011).D.
Nadeau, P. Turney, and S. Matwin.
2006.
Unsuper-vised named-entity recognition: Generating gazetteersand resolving ambiguity.
Advances in Artificial Intel-ligence, pages 266?277.T.
Pedersen, A. Kulkarni, R. Angheluta, Z. Kozareva, andT.
Solorio.
2006.
An unsupervised language inde-pendent method of name discrimination using secondorder co-occurrence features.
Computational Linguis-tics and Intelligent Text Processing, pages 208?222.J.
Piskorski, K. Wieloch, and M. Sydow.
2009.
Onknowledge-poor methods for person name matchingand lemmatization for highly inflectional languages.Information retrieval, 12(3):275?299.E.F Tjong Kim Sang and F. De Meulder.
2003.
Intro-duction to the CoNLL-2003 shared task: Language-independent named entity recognition.
In Proceedingsof the seventh conference on Natural language learn-ing at HLT-NAACL 2003-Volume 4, pages 142?147.Association for Computational Linguistics.E.F.
Tjong Kim Sang.
2002.
Introduction to the CoNLL-2002 shared task: language-independent named entityrecognition, proceedings of the 6th conference on nat-ural language learning.
August, 31:1?4.Z.
Song and S. Strassel.
2008.
Entity translation andalignment in the ACE-07 ET task.
Proceedings ofLREC-2008.R.
Steinberger and B. Pouliquen.
2007.
Cross-lingualnamed entity recognition.
Lingvistic?
Investigationes,30(1):135?162.T.
Tao, S.Y.
Yoon, A. Fister, R. Sproat, and C.X.
Zhai.2006.
Unsupervised named entity transliteration usingtemporal and phonetic correlation.
In Proceedings ofthe 2006 Conference on Empirical Methods in NaturalLanguage Processing, pages 250?257.
Association forComputational Linguistics.T.
Tchoukalov, C. Monson, and B. Roark.
2010.
Mor-phological analysis by multiple sequence alignment.Multilingual Information Access Evaluation I.
Text Re-trieval Experiments, pages 666?673.W.
Wang, C. Xiao, X. Lin, and C. Zhang.
2009.
Effi-cient approximate entity extraction with edit distanceconstraints.
In Proceedings of the 35th SIGMOD in-ternational conference on Management of data, pages759?770.
ACM.D.
Yarowsky, G. Ngai, and R. Wicentowski.
2001.
In-ducing multilingual text analysis tools via robust pro-jection across aligned corpora.
In Proceedings ofthe first international conference on Human languagetechnology research, pages 1?8.
Association for Com-putational Linguistics.29
