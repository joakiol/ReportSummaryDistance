Proceedings of the Seventeenth Conference on Computational Natural Language Learning, pages 143?152,Sofia, Bulgaria, August 8-9 2013. c?2013 Association for Computational LinguisticsTowards Robust Linguistic Analysis Using OntoNotesSameer Pradhan1, Alessandro Moschitti2,3, Nianwen Xue4, Hwee Tou Ng5Anders Bjo?rkelund6, Olga Uryupina2, Yuchen Zhang4 and Zhi Zhong51 Boston Childrens Hospital and Harvard Medical School, Boston, MA 02115, USA2 University of Trento, University of Trento, 38123 Povo (TN), Italy3 QCRI, Qatar Foundation, 5825 Doha, Qatar4 Brandeis University, Brandeis University, Waltham, MA 02453, USA5 National University of Singapore, Singapore, 1174176 University of Stuttgart, 70174 Stuttgart, GermanyAbstractLarge-scale linguistically annotated cor-pora have played a crucial role in advanc-ing the state of the art of key natural lan-guage technologies such as syntactic, se-mantic and discourse analyzers, and theyserve as training data as well as evaluationbenchmarks.
Up till now, however, mostof the evaluation has been done on mono-lithic corpora such as the Penn Treebank,the Proposition Bank.
As a result, it is stillunclear how the state-of-the-art analyzersperform in general on data from a vari-ety of genres or domains.
The completionof the OntoNotes corpus, a large-scale,multi-genre, multilingual corpus manuallyannotated with syntactic, semantic anddiscourse information, makes it possibleto perform such an evaluation.
This paperpresents an analysis of the performance ofpublicly available, state-of-the-art tools onall layers and languages in the OntoNotesv5.0 corpus.
This should set the bench-mark for future development of variousNLP components in syntax and semantics,and possibly encourage research towardsan integrated system that makes use of thevarious layers jointly to improve overallperformance.1 IntroductionRoughly a million words of text from the WallStreet Journal newswire (WSJ), circa 1989, hashad a significant impact on research in the lan-guage processing community ?
especially thosein the area of syntax and (shallow) semantics, thereason for this being the seminal impact of thePenn Treebank project which first selected this textfor annotation.
Taking advantage of a solid syn-tactic foundation, later researchers who wanted toannotate semantic phenomena on a relatively largescale, also used it as the basis of their annota-tion.
For example the Proposition Bank (Palmer etal., 2005), BBN Name Entity and Pronoun coref-erence corpus (Weischedel and Brunstein, 2005),the Penn Discourse Treebank (Prasad et al 2008),and many other annotation projects, all annotatethe same underlying body of text.
It was also con-verted to dependency structures and other syntac-tic formalisms such as CCG (Hockenmaier andSteedman, 2002) and LTAG (Shen et al 2008),thereby creating an even bigger impact throughthese additional syntactic resources.
The most re-cent one of these efforts is the OntoNotes corpus(Weischedel et al 2011).
However, unlike theprevious extensions of the Treebank, in additionto using roughly a third of the same WSJ subcor-pus, OntoNotes also added several other genres,and covers two other languages ?
Chinese andArabic: portions of the Chinese Treebank (Xue etal., 2005) and the Arabic Treebank (Maamouri andBies, 2004) have been used to sample the genre oftext that they represent.One of the current hurdles in language process-ing is the problem of domain, or genre adaptation.Although genre or domain are popular terms, theirdefinitions are still vague.
In OntoNotes, ?genre?means a type of source ?
newswire (NW), broad-cast news (BN), broadcast conversation (BC), mag-azine (MZ), telephone conversation (TC), web data(WB) or pivot text (PT).
Changes in the entity andevent profiles across source types, and even in thesame source over a time duration, as explicitly ex-pressed by surface lexical forms, usually accountfor a lot of the decrease in performance of mod-els trained on one source and tested on another,usually because these are the salient cues that arerelied upon by statistical models.Large-scale corpora annotated with multiplelayers of linguistic information exist in variouslanguages, but they typically consist of a singlesource or collection.
The Brown corpus, whichconsists of multiple genres, have been usually usedto investigate issues of genres of sensitivity, but itis relatively small and does not include any infor-1A portion of the English data in the OntoNotes corpusis a selected set of sentences that were annotated for parseand word sense information.
These sentences are present in adocument of their own, and so the documents for parse layersfor English are inflated by about 3655 documents and for theword sense are inflated by about 8797 documents.143Language Parse Proposition Sense Name CoreferenceDocuments Words Documents Verb Prop.
Noun Prop.
Documents Verb Sense Noun Sense Documents Words Documents WordsEnglish 7,9671 2.6M 6,124 300K 18K 12K 173K 120K 3,637 2.0M 2,384(3493) 1.7MChinese 2002 1.0M 1861 148K 7K 1573 83K 1K 1,911 988K 1,729(2,280) 950KArabic 599 402K 599 30K - 310 4.3K 8.7K 446 298K 447(447) 300KTable 1: Coverage for each layer in the OntoNotes v5.0 corpus, by number of documents, words, andsome other attributes.
The numbers in parenthesis are the total number of parts in the documents.mal genres such as web data.
Very seldom has itbeen the case that the exact same phenomena havebeen annotated on a broad cross-section of thesame language before OntoNotes.
The OntoNotescorpus thus provides an opportunity for studyingthe genre effect on different syntactic, semanticand discourse analyzers.Parts of the OntoNotes Corpus have been usedfor various shared tasks organized by the languageprocessing community.
The word sense layer wasthe subject of prediction in two SemEval-2007tasks, and the coreference layer was the subjectof prediction in the SemEval-20102 (Recasens etal., 2010), CoNLL-2011 and 2012 shared tasks(Pradhan et al 2011; Pradhan et al 2012).
TheCoNLL-2012 shared task provided predicted in-formation to the participants, however, that did notinclude a few layers such as the named entitiesfor Chinese and Arabic, propositions for Arabic,and for better comparison of the English data withthe CoNLL-2011 task, a smaller OntoNotes v4.0portion of the English parse and propositions wasused for training.This paper is a first attempt at presenting a co-herent high-level picture of the performance ofvarious publicly available state-of-the-art tools onall the layers of OntoNotes in all three languages,so as to pave the way for further explorations inthe area of syntax and semantics processing.The possible avenues for exploratory studieson various fronts are enormous.
However, givenspace considerations, in this paper, we will re-strict our presentation of the performance on alllayers of annotation in the data by using a strat-ified cross-section of the corpus for training, de-velopment, and testing.
The paper is organizedas follows: Section 2 gives an overview of theOntoNotes corpus.
Section 3 explains the param-eters of the evaluation and the various underlyingassumptions.
Section 4 presents the experimentalresults and discussion, and Section 5 concludes thepaper.2 OntoNotes CorpusThe OntoNotes project has created a large-scalecorpus of accurate and integrated annotation of2A small portion 125K words in English was used for thisevaluation.multiple layers of syntactic, semantic and dis-course information in text.
The English lan-guage portion comprises roughly 1.7M words andChinese language portion comprises roughly 1Mwords of newswire, magazine articles, broadcastnews, broadcast conversations, web data and con-versational speech data3.
The Arabic portion issmaller, comprising 300K words of newswire ar-ticles.
This rich, integrated annotation coveringmany layers aims at facilitating the developmentof richer, cross-layer models and enabling bet-ter automatic semantic analysis.
The corpus istagged with syntactic trees, propositions for mostverb and some noun instances, partial verb andnoun word senses, coreference, and named enti-ties.
Table 1 gives an overview of the number ofdocuments that have been annotated in the entireOntoNotes corpus.2.1 Layers of AnnotationThis section provides a very concise overview ofthe various layers of annotations in OntoNotes.For a more detailed description, the reader is re-ferred to (Weischedel et al 2011) and the docu-mentation accompanying the v5.04 release.2.1.1 SyntaxThis represents the layer of syntactic annotationbased on revised guidelines for the Penn Tree-bank (Marcus et al 1993; Babko-Malaya et al2006), the Chinese Treebank (Xue et al 2005)and the Arabic Treebank (Maamouri and Bies,2004).
There were two updates made to the parsetrees as part of the OntoNotes project: i) the in-troduction of NML phrases, in the English portion,to mark nominal sub-constituents of flat NPs thatdo not follow the default right-branching structure,and ii) re-tokenization of hyphenated tokens intomultiple tokens in English and Chinese.
The Ara-bic Treebank on the other hand was also signifi-cantly revised in an effort to increase consistency.2.1.2 Word SenseCoarse-grained word senses are tagged for themost frequent polysemous verbs and nouns, in or-3These numbers are for the portion that has all layers ofannotations.
The word count for each layer is mentioned inTable 14For all the layers of data used in this study, theOntoNotes v4.99 pre-release that was used for the CoNLL-2012 shared task is identical to the v5.0 release.144der to maximize token coverage.
The word sensegranularity is tailored to achieve very high inter-annotator agreement as demonstrated by Palmer etal.
(2007).
These senses are defined in the senseinventory files.
In the case of English and Arabiclanguages, the sense-inventories (and frame files)are defined separately for each part of speech thatis realized by the lemma in the text.
For Chinese,however the sense inventories (and frame files) aredefined per lemma ?
independent of the part ofspeech realized in the text.2.1.3 PropositionThe propositions in OntoNotes are PropBank-stylesemantic roles for English, Chinese and Arabic.Most English verbs and few nouns were anno-tated using the revised guidelines for the EnglishPropBank (Babko-Malaya et al 2006) as part ofthe OntoNotes effort.
Some enhancements weremade to the English PropBank and Treebank tomake them synchronize better with each other:one of the outcomes of this effort was that twotypes of LINKs that represent pragmatic coref-erence (LINK-PCR) and selectional preferences(LINK-SLC) were added to the original PropBank(Palmer et al 2005).
More details can be found inthe addendum to the PropBank guidelines5 in theOntoNotes v5.0 release.
A part of speech agnosticChinese PropBank (Xue and Palmer, 2009) guide-lines were used to annotate most frequent lem-mas in Chinese.
Many verbs and some nouns andadjectives were annotated using the revised Ara-bic PropBank guidelines (Palmer et al 2008; Za-ghouani et al 2010).2.1.4 Named EntitiesThe corpus was tagged with a set of 18 well-defined proper named entity types that have beentested extensively for inter-annotator agreementby Weischedel and Burnstein (2005).2.1.5 CoreferenceThis layer captures general anaphoric corefer-ence that covers entities and events not limitedto noun phrases or a limited set of entity types(Pradhan et al 2007).
It considers all pronouns(PRP, PRP$), noun phrases (NP) and heads of verbphrases (VP) as potential mentions.
Unlike En-glish, Chinese and Arabic have dropped subjectsand objects which were also considered duringcoreference annotation6.
The mentions formed bythese dropped pronouns total roughly about 11%for both Chinese and Arabic.
Coreference is theonly document-level phenomenon in OntoNotes.Some of the documents in the corpus ?
especiallythe ones in the broadcast conversation, web data,5doc/propbank/english-propbank.pdf6As we will see later these are not used during the task.and telephone conversation genre ?
are very longwhich prohibited efficient annotation in their en-tirety.
These are split into smaller parts, and eachpart is considered a separate document for the sakeof coreference evaluation.3 Evaluation SettingGiven the scope of the corpus and the multitude ofsettings one can run evaluations, we had to restrictthis study to a relatively focused subset.
There hasalready been evidence of models trained on WSJdoing poorly on non-WSJ data on parses (Gildea,2001; McClosky et al 2006), semantic role label-ing (Carreras and Ma`rquez, 2005; Pradhan et al2008), word sense (Escudero et al 2000; ?
), andnamed entities.
The phenomenon of coreference issomewhat of an outlier.
The winning system in theCoNLL-2011 shared task was one that was com-pletely rule-based and not directly trained on theOntoNotes corpus.
Given this overwhelming evi-dence, we decided not to focus on potentially com-plex cross-genre evaluations.
Instead, we decidedon evaluating the performance on each layer of an-notation using an appropriately selected, stratifiedtraining, development and test set, so as to facili-tate future studies.3.1 Training, Development and TestPartitionsIn this section we will have a brief discussionon the logic behind the partitioning of the datainto training, development and test sets.
Beforewe do that, it would help to know that given therange and peculiarities of the layers of annota-tion and presence of various resource and techni-cal constraints, not all the documents in the cor-pus are annotated with all the layers of informa-tion, and token-centric phenomena (such as wordsense and propositions of predicates) were not an-notated with 100% coverage.
Most of the propo-sition annotation in English and Arabic is for theverb predicates, with a few nouns annotated inEnglish and some adjectives in Arabic.
In Chi-nese, the selection is part of speech agnostic, and isbased on the lemmas that can be considered predi-cates.
Some documents in the corpora are actuallysnippets from larger documents, and have been an-notated for a combination of parse, propositions,word sense and names, but not coreference.
If oneconsiders each layer independently, then an idealpartitioning scheme would create a separate parti-tion for each layer such that it maximizes the num-ber of examples that can be extracted for that layerfrom the corpus.
The upside is that one wouldget as much data there is to train and estimate theperformance of each layer across the entire cor-pus.
The downside is that this might cover vari-145ous cross sections of the documents in the corpus,and would not provide a clean picture when look-ing at the collective performance for all the lay-ers.
The documents that are annotated with coref-erence correspond to the intersection of all anno-tations.
These are the documents that have alsobeen annotated with all the other layers of infor-mation.
The amount of data we can get togetherin such a test set is big enough to be represen-tative.
Therefore, we decided that it would beideal to choose a portion of these documents asthe test collection for all layers.
An additional ad-vantage is that it is the exact same test set usedin the CoNLL-2012 shared task, and so in a wayis already a standard.
On the training and devel-opment side however, one can still imagine usingall possible information for training models for aparticular layer, and that is what we decided todo.
The training and development data is gener-ated by providing all documents with all availablelayers of annotation for input, however, the testset is generated by providing as input to the algo-rithm the set of documents in the corpus that havebeen annotated for coreference.
This algorithmtries to reuse previously established partitions forEnglish, i.e., the WSJ portion.
Unfortunately, inthe case of Chinese and Arabic, either the histor-ical partitions were not in the selection used forOntoNotes, or were partially overlapping with theones created using this scheme, and/or had a verysmall portion of OntoNotes covered in the test set.Therefore, we decided to create a fresh partitionfor the Chinese and Arabic data.
Note, however,that the these test sets also match the ones usedin the CoNLL-2012 evaluation.
The algorithm forselecting the training, development and test parti-tions is described on the CoNLL-2012 shared taskwebpage, along with the list of training, develop-ment, and test document IDs7.3.2 AssumptionsNext we had to decide on a set of assumptionsto use while designing the experiments to mea-sure the automatic prediction accuracy for each ofthe layers.
Since some of these decisions affectmore than one layer of annotation, we will de-scribe these in this section instead of in the sectionwhere we discuss the experiment with a particularlayer of annotation.7http://conll.cemantix.org/2012/download/ids/For each language there are two sub-directories ?
?all?contains more general lists which include documentsthat had at least one of the layers of annotation, and?coref?
contains the lists that include documents thathave coreference annotation.
The former were used togenerate training, development, test sets for layers otherthan coreference, and the latter was used to generatetraining/development/test sets for the coreference layerused in the CoNLL-2012 shared task.Word Segmentation The three languages thatwe are evaluating are from quite different lan-guage families.
Arabic has a complex morphol-ogy, English has limited morphology, whereasChinese has very little morphology.
English wordsegmentation amounts to rule-based tokenization,and is close to perfect.
In the case of Chinese andArabic, although the tokenization/segmentation isnot as good as English, the accuracies are in thehigh 90s.
Given this we decided to use gold,Treebank segmentation for all languages.
In thecase of Chinese, the words themselves are lem-mas, whereas in English they can be predictedwith very high accuracy.
For Arabic, by defaultwritten text is unvocalised, and lemmatization is acomplex process which we considered out of thescope of this study, so we decided to use correct,gold standard lemmas, along with the correct vo-calized version of the tokens.Traces and Function Tags Treebank traceshave hardly played a role in the mainstream parserand semantic role labeling evaluation.
Functiontags also have received similar treatment in theparsing community, and though they are impor-tant, there is also a significant information overlapbetween them and the proposition structure pro-vided by the PropBank layer.
Whereas in English,most traces represent syntactic phenomena suchas movement and raising, in Chinese and Arabic,they can also represent dropped subjects/objects.These subset of traces directly affect the corefer-ence layer, since, unlike English, traces in Chineseand Arabic (*pro* and * respectively) are legit-imate targets of mentions and are considered forcoreference annotation in OntoNotes.
Recoveringtraces in text is a hard problem, and the most re-cently reported numbers in literature for Chineseare around a F-score of 50 (Yang and Xue, 2010;Cai et al 2011).
For Arabic there have not beenmuch studies on recovering these.
A study byGabbard (2010) shows that these can be recoveredwith an F-score of 55 with automatic parses androughly 65 using gold parses.
Considering the lowlevel of prediction accuracy of these tokens, andtheir relative low frequency, we decided to con-sider predicting traces in trees out of the scope ofthis study.
In other words, we removed the man-ually identified traces and function tags from theTreebanks across all three languages, in all thethree ?
training, development and test partitions.This meant removing any and all dependent an-notation in layers such as PropBank and Coref-erence.
In the case of PropBank these are theargument bearing traces, whereas in coreferencethese are the mentions formed by these elided sub-jects/objects.146Disfluencies One thing that needs to be dealtwith in conversational data is the presence of dis-fluencies (restarts, etc.).
In the English parses ofthe OntoNotes, disfluencies are marked using aspecial EDITED8 phrase tag ?
as was the case forthe Switchboard Treebank.
Computing the accu-racy of identifying disfluencies is also out of thescope of this study.
Given the frequency of dis-fluencies and the performance with which one canidentify them automatically,9 a probable process-ing pipeline would filter them out before parsing.We decided to remove them using oracle infor-mation available in the English Treebank, and thecoreference chains were remapped to trees with-out disfluencies.
Owing to various technical con-straints, we decided to retain the disfluencies in theChinese data.Spoken Genre Given the scope of this study, wemake another significant assumption.
For the spo-ken genres ?
BC, BN and TC ?
we use the manualtranscriptions rather than the output of a speechrecognizer, as would be the case in real world.
Theperformance on various layers for these genreswould therefore be artificially inflated, and shouldbe taken into account while analyzing results.
Notmany studies have previously reported on syntac-tic and semantic analysis for spoken genre.
Favreet al(2010) report the performance on the Englishsubset of an earlier version of OntoNotes.Discourse The corpus contains information onthe speaker for broadcast communication, conver-sation, telephone conversation and writer for theweb data.
This information provides an importantclue for correctly linking anaphoric pronouns withthe right antecedents.
This information could beautomatically deduced, but is also not within thescope of our study.
Therefore, we decided to pro-vide gold, instead of predicted, data both duringtraining and testing.
Table 2 lists the status of thelayers.4 ExperimentsIn this section, we will report on the experimentscarried out using all available data in the train-ing set for training models for a particular layer,and using the CoNLL-2012 test set as the test set.8There is another phrase type ?
EMBED in the telephoneconversation genre which is similar to the EDITED phrasetype, and sometimes identifies insertions, but sometimes con-tains logical continuation of phrases by different speakers, sowe decided not to remove that from the data.9A study by Charniak and Johnson (2001) shows that onecan identify and remove edits from transcribed conversationalspeech with an F-score of about 78, with roughly 95 precisionand 67 recall.10The predicted part of speech for Arabic are a mappeddown version of the richer gold version present in the Tree-bankLayer English Chinese ArabicSegmentation ?
?
?Lemma ?
?
?Parse ?
?
?10Proposition ?
?
?Predicate Frame ?
?
?Word Sense ?
?
?Name Entities ?
?
?Coreference ?
?
?Speaker ?
?
?Number ?
?
?Gender ?
?
?Table 2: Status of layers used during predictionof other layers.
A ???
indicates gold annotation,a ???
indicates predicted, a ???
indicates an ab-sence of the predicted layer, and a ???
indicatesthat the layer is not applicable to the language.The predicted annotation layers input to down-stream models were automatically annotated byusing NLP processors learned with n-cross foldvalidation on the training data.
This way, the nchunks of training data are annotated avoiding de-pendencies with the data used for training the NLPprocessors.4.1 SyntaxPredicted parse trees for English were producedusing the Charniak parser11 (Charniak and John-son, 2005).
Some additional tag types used inthe OntoNotes trees were added to the parser?stagset, including the nominal (NML) tag, and therules used to determine head words were extendedcorrespondingly.
Chinese and Arabic parses weregenerated using the Berkeley parser (Petrov andKlein, 2007).
In the case of Arabic, the pars-ing community uses a mapping from rich Arabicpart of speech tags to Penn-style part of speechtags.
We used the mapping that is included withthe Arabic Treebank.
The predicted parses forthe training portion of the data were generated us-ing 10-fold (5-folds for Arabic) cross-validation.For testing, we used a model trained on the entiretraining portion.
Table 3 shows the precision, re-call and F1-scores of the re-trained parsers on theCoNLL-2012 test along with the part of speech ac-curacies (POS) using the standard evalb scorer.The performance on the PT genre for English isthe highest among other English genres.
This ispossibly because of the professional, clean trans-lations of the underlying text, and are mostlyshorter sentences.
The MZ genre and the NW bothof which contain well edited text, share similarscores.
There is a few points gap between theseand the other genres.
As for Chinese, the per-formance on MZ is the highest followed by BN.Surprisingly, the WB genre has a similar score andthe others are close behind except for TC.
As ex-pected, the Arabic parser performance is the low-11http://bllip.cs.brown.edu/download/reranking-parserAug06.tar.gz147All SentencesN POS P R FEnglish BC 2,211 97.33 86.36 86.11 86.23BN 1,357 97.32 87.61 87.03 87.32MZ 780 96.58 89.90 89.49 89.70NW 2,327 97.15 87.68 87.25 87.47TC 1,366 96.11 85.09 84.13 84.60WB 1,787 96.03 85.46 85.26 85.36PT 1,869 98.77 95.29 94.66 94.98Overall 11,697 97.09 88.08 87.65 87.87Chinese BC 885 94.79 80.17 79.35 79.76BN 929 93.85 83.49 80.13 81.78MZ 451 97.06 88.48 83.85 86.10NW 481 94.07 82.26 77.28 79.69TC 968 92.22 71.90 69.19 70.52WB 758 92.37 82.57 78.92 80.70Overall 4,472 94.12 82.23 78.93 80.55Arabic NW 1,003 94.12 74.71 75.67 75.19Table 3: Parser performance on the CoNLL-2012test set.est among the three languages.4.2 Word SenseWe used the IMS12 (It Makes Sense) (Zhong andNg, 2010) word sense tagger.
IMS was trained onall the word sense data that is present in the train-ing portion of the OntoNotes corpus using cross-validated predictions on the input layers similarto the proposition tagger.
During testing, for En-glish and Arabic, IMS must first use the auto-matic POS information to identify the nouns andverbs in the test data, and then assign senses tothe automatically identified nouns and verbs.
Inthe case of Arabic, IMS uses gold lemmas.
Sinceautomatic POS tagging is not perfect, IMS doesnot always output a sense to all word tokens thatneed to be sense tagged due to wrongly predictedPOS tags.
As such, recall is not the same as pre-cision on the English and Arabic test data.
ForChinese the measure of performance is just theaccuracy since the senses are defined per lemmarather than per part of speech.
Since we providegold word segmentation, IMS attempts to sensetag all correctly segmented Chinese words, so re-call and precision are the same and so is the F1-score.
Table 4 shows the performance of this clas-sifier aggregated over both the verbs and nounsin the CoNLL-2012 test set and an overall scoresplit by nouns and verbs for English and Ara-bic.
For both nouns and verbs in English, theF1-score is over 80%.
The performance on En-glish nouns is slightly higher than English verbs.Comparing to the other two languages, the perfor-mance on Arabic is relatively lower, especially theperformance on Arabic verbs, whose F1-score isless than 70%.
For English, genres PT and TC,and for Chinese genres TC and WB, no gold stan-dard senses were available, and so their accuraciescould not be computed.
Previously, Zhong et al(2008) reported the word sense performance onthe Wall Street Journal portion of an earlier ver-12http://www.comp.nus.edu.sg/?nlp/sw/IMS v0.9.2.1.tar.gzPerformanceP R F AEnglish BC 81.2 81.3 81.2 -BN 82.0 81.5 81.7 -MZ 79.1 78.8 79.0 -NW 85.7 85.7 85.7 -WB 77.5 77.6 77.5 -Overall 82.5 82.5 82.5 -Nouns 83.4 83.1 83.2 -Verbs 81.8 81.9 81.8 -Chinese BC - - - 80.5BN - - - 85.4MZ - - - 82.4NW - - - 89.1Overall - - - 84.3Arabic NW 75.9 75.2 75.6 -Nouns 79.2 77.7 78.4 -Verbs 68.8 69.5 69.1 -Table 4: Word sense performance on the CoNLL-2012 test set.sion of OntoNotes, but the results are not directlycomparable.4.3 PropositionThe revised PropBank has introduced two newlinks ?
LINK-SLC and LINK-PCR.
Since the com-munity is not used to the new PropBank represen-tation which (i) relies heavily on the trace struc-ture in the Treebank and (ii) we decided to ex-clude, we unfold the LINKs back to their originalrepresentation as in the PropBank 1.0 release.
Weused ASSERT15 (Pradhan et al 2005) to predictthe propositional structure for English.
We madea small modification to ASSERT, and replacedthe TinySVM classifier with a CRF16 to speedup training the model on all the data.
The Chi-nese propositional structure was predicted with theChinese semantic role labeler described in (Xue,2008), retrained on the OntoNotes v5.0 data.
TheArabic propositional structure was predicted us-ing the system described in Diab et al(2008).
(Diab et al 2008) Table 5 shows the detailed per-14The Frame ID column indicates the F-score for Englishand Arabic, and accuracy for Chinese for the same reasons asword sense.15http://cemantix.org/assert.html16http://leon.bottou.org/projects/sgdFrame Total Total % Perfect Argument ID + ClassID Sent.
Prop.
Prop.
P R FEnglish BC 93.2 1994 5806 52.89 80.76 69.69 74.82BN 92.7 1218 4166 54.78 80.22 69.36 74.40MZ 90.8 740 2655 50.77 79.13 67.78 73.02NW 92.8 2122 6930 46.45 79.80 66.80 72.72TC 91.8 837 1718 49.94 79.85 72.35 75.91WB 90.7 1139 2751 42.86 80.51 69.06 74.35PT 96.6 1208 2849 67.53 89.35 84.43 86.82Overall 92.8 9,261 26,882 51.66 81.30 70.53 75.53Chinese BC 87.7 885 2,323 31.34 53.92 68.60 60.38BN 93.3 929 4,419 35.44 64.34 66.05 65.18MZ 92.3 451 2,620 31.68 65.04 65.40 65.22NW 96.6 481 2,210 27.33 69.28 55.74 61.78TC 82.2 968 1,622 32.74 48.70 59.12 53.41WB 87.8 758 1,761 35.21 62.35 68.87 65.45Overall 90.9 4,472 14,955 32.62 61.26 64.48 62.83Arabic NW 85.6 1,003 2337 24.18 52.99 45.03 48.68Table 5: Proposition and frameset disambiguationperformance14 in the CoNLL-2012 test set.148formance numbers17.
The CoNLL-2005 scorer18was used to compute the scores.
At first glance,the performance on the English newswire genre ismuch lower than what has been reported for WSJSection 23.
This could be attributed to several fac-tors: i) the newswire in OntoNotes not only con-tains WSJ data, but also Xinhua news, and someother newswire evaluation data, ii) The WSJ train-ing and test portions in OntoNotes are a subset ofthe standard ones that have been used to reportperformance earlier; iii) the PropBank guidelineswere significantly revised during the OntoNotesproject in order to synchronize well with the Tree-bank, and finally iv) it includes propositions forbe verbs missing from the original PropBank.
Itlooks like the newly added Pivot Text data (com-prised of the New Testament) shows very goodperformance.
The Chinese and Arabic19 accuracyis much worse.
In addition to automatically pre-dicting the arguments, we also trained the IMSsystem to tag PropBank frameset IDs.Language Genre Entity PerformanceCount P R FEnglish BC 1671 80.17 77.20 78.66BN 2180 88.95 85.69 87.29MZ 1161 82.74 82.17 82.45NW 4679 86.79 84.25 85.50TC 362 74.09 61.60 67.27WB 1133 77.72 68.05 72.56Overall 11186 84.04 80.86 82.42Chinese BC 667 72.49 58.47 64.73BN 3158 82.17 71.50 76.46NW 1453 86.11 76.39 80.96MZ 1043 65.16 56.66 60.62TC 200 48.00 60.00 53.33WB 886 80.60 51.13 62.57Overall 7407 78.20 66.45 71.85Arabic NW 2550 74.53 62.55 68.02Table 6: Performance of the named entity recog-nizer on the CoNLL-2012 test set.4.4 Named EntitiesWe retrained the Stanford named entity recog-nizer20 (Finkel et al 2005) on the OntoNotes data.Table 6 shows the performance details for all thelanguages across all 18 name types broken downby genre.
In English, BN has the highest perfor-mance followed by the NW genre.
There is a sig-nificant drop from those and the TC and WB genre.Somewhat similar trend is observed in the Chi-nese data, with Arabic having the lowest scores.Since the Pivot Text portion (PT) of OntoNoteswas not tagged with names, we could not com-pute the accuracy for that cross-section of the data.Previously Finkel and Manning (2009) performed17The number of sentences in this table are a subset of theones in the table showing parser performance, since these arethe sentences for which at least one predicate has been taggedwith its arguments18http://www.lsi.upc.es/?srlconll/srl-eval.pl19The system could not not use the morphology features inDiab et al(2008).20http://nlp.stanford.edu/software/CRF-NER.shtmla joint estimation of named entity and parsing.However, it was on an earlier version of the En-glish portion of OntoNotes using a different cross-section for training and testing and therefore is notdirectly comparable.4.5 CoreferenceThe task is to automatically identify mentions ofentities and events in text and to link the corefer-ring mentions together to form entity/event chains.The coreference decisions are made using auto-matically predicted information on other structuraland semantic layers including the parses, seman-tic roles, word senses, and named entities thatwere produced in the earlier sections.
Each docu-ment part from the documents that were split intomultiple parts during coreference annotation weretreated as separate document.We used the number and gender predictionsgenerated by Bergsma and Lin (2006).
Unfortu-nately neither Arabic, nor Chinese have compara-ble data available.
Chinese, in particular, does nothave number or gender inflections for nouns, but(Baran and Xue, 2011) look at a way to infer suchinformation.We trained the Bjo?rkelund and Farkas (2012)coreference system21 which uses a combination oftwo pair-wise resolvers, the first is an incremen-tal chain-based resolution algorithm (Bjo?rkelundand Farkas, 2012), and the second is a best-firstresolver (Ng and Cardie, 2002).
The two resolversare combined by stacking, i.e., the output of thefirst resolver is used as features in the second one.The system uses a large feature set tailored foreach language which, in addition to classic coref-erence features, includes both lexical and syntacticinformation.Recently, it was discovered that there is pos-sibly a bug in the official scorer used for theCoNLL 2011/2012 and the SemEval 2010 corefer-ence tasks.
This relates to the mis-implementationof the method proposed by (Cai and Strube, 2010)for scoring predicted mentions.
This issue has alsobeen recently reported in Recasens et al (2013).As of this writing, the BCUBED metric has beenfixed, and the correctness of the CEAFm, CEAFeand BLANC metrics is being verified.
We willbe updating the CoNLL shared task webpages22with more detailed information and also releasethe patched scripts as soon as they are available.We will also re-generate the scores for previousshared tasks, and the coreference layer in this pa-per and make them available along with the mod-els and system outputs for other layers.
Table7 shows the performance of the system on the21http://www.ims.uni-stuttgart.de/?anders/coref.html22http://conll.cemantix.org149CoNLL-2012 test set, broken down by genre.
Thesame metrics that were used for the CoNLL-2012shared task are computed, with the CONLL col-umn being the official CONLL measure.Language Genre MD MUC BCUBED CEAFm CEAFe BLANC CONLLPREDICTED MENTIONSEnglish BC 73.43 63.92 61.98 54.82 42.68 73.04 56.19BN 73.49 63.92 65.85 58.93 48.14 72.74 59.30MZ 71.86 64.94 71.38 64.03 50.68 78.87 62.33NW 68.54 60.20 65.11 57.54 45.10 73.72 56.80PT 86.95 79.09 68.33 65.52 50.83 77.74 66.08TC 80.81 76.78 71.35 65.41 45.44 82.45 64.52WB 74.43 66.86 61.43 54.76 42.05 73.54 56.78Overall 75.38 67.58 65.78 59.20 45.87 75.8 59.74Chinese BC 68.02 59.6 59.44 53.12 40.77 73.63 53.27BN 68.57 61.34 67.83 60.90 48.10 77.39 59.09MZ 55.55 48.89 58.83 55.63 46.04 74.25 51.25NW 89.19 80.71 73.64 76.30 70.89 82.56 75.08TC 77.72 73.59 71.65 64.30 48.52 83.14 64.59WB 72.61 65.79 62.32 56.71 43.67 77.45 57.26Overall 66.37 58.61 66.56 59.01 48.19 76.07 57.79Arabic NW 60.55 47.82 61.16 53.42 44.30 69.63 51.09GOLD MENTIONSEnglish BC 85.63 76.09 68.70 61.73 49.87 76.24 64.89BN 82.11 73.56 71.52 63.67 52.29 75.70 65.79MZ 85.65 77.73 78.82 72.75 60.09 83.88 72.21NW 80.68 73.52 73.08 65.63 51.96 81.06 66.19PT 93.20 85.72 73.25 70.76 58.81 79.78 72.59TC 90.68 86.83 78.94 73.87 56.26 85.82 74.01WB 88.12 80.61 69.86 63.45 51.13 76.48 67.20Overall 86.16 78.7 72.67 66.32 53.23 79.22 68.2Chinese BC 84.88 76.34 69.89 62.02 49.29 76.89 65.17BN 80.97 74.89 76.88 68.91 55.56 81.94 69.11MZ 78.85 73.06 70.15 61.68 46.86 78.78 63.36NW 93.23 86.54 86.70 80.60 76.60 85.75 83.28TC 92.91 88.31 84.51 79.49 63.87 90.04 78.90WB 85.87 77.61 69.24 60.71 47.47 77.67 64.77Overall 83.47 76.85 76.30 68.30 56.61 81.56 69.92Arabic NW 76.43 60.81 67.29 59.50 49.32 74.61 59.14Table 7: Performance of the coreference systemon the CoNLL-2012 test set.The varying results across genres mostly meetour expectations.
In English, the system does beston TC and the PT genres.
The text in the TC setoften involve long chains where the speakers re-fer to themselves which, given speaker informa-tion, is fairly easy to resolve.
The PT sectionincludes many references to god (e.g.
god andthe lord) which the lexicalized resolver is quitegood at picking up during training.
The more dif-ficult genres consist of texts where references tomany entities are interleaved in the discourse andis as such harder to resolve correctly.
For Chi-nese the numbers on the TC genre are also quitegood, and the explanation above also holds here?
many mentions refer to either of the speak-ers.
For Chinese the NW section displays by farthe highest scores, however, and the reason forthis is not clear to us.
Not surprisingly, restrictingthe set of mentions only to gold mentions givesa large boost across all genres and all languages.This shows that mention detection (MD) and sin-gleton detection (which is not part of the annota-tion) remain a big source of errors for the coref-erence resolver.
For these experiments we useda combination of training and development datafor training ?
following the CoNLL-2012 sharedtask specification.
Leaving out the developmentset has a very negligible effect on the CoNLL-score for all the languages (English: 0.14; Chi-nese 0.06; Arabic: 0.40 F-score respectively).
Theeffect on Arabic is the most (0.40 F-score) mostlikely because of its much smaller size.
To gaugethe performance improvement between 2011 and2012 shared tasks, we performed a clean com-parison of over the best performing system andan earlier version of this system (Bjo?rkelund andNugues, 2011) on the CoNLL 2011 test set us-ing the CoNLL 2011 train and development setfor training.
The current system has a CoNLLscore of 60.09 (64.92+69.84+45.513 )23 as opposed tothe 54.53 reported in bjo?rkelund (Bjo?rkelund andNugues, 2011), and the 57.79 reported for the bestperforming system of CoNLL-2011.
One caveatis that these score comparison are done using theearlier version (v4) of the CoNLL scorer.
Nev-ertheless, it is encouraging to see that within ashort span of a year, there has been significantimprovement in system performance ?
partiallyowing to cross-pollination of research generatedthrough the shared tasks.5 ConclusionIn this paper we reported work on finding a rea-sonable training, development and test split forthe various layers of annotation in the OntoNotesv5.0 corpus, which consists of multiple genres inthree typologically very different languages.
Wealso presented the performance of publicly avail-able, state-of-the-art algorithms on all the differentlayers of the corpus for the different languages.The trained models as well as their output willbe made publicly available24 to serve as bench-marks for language processing community.
Train-ing so many different NLP components is verytime-consuming, thus, we hope the work reportedhere has lifted the burden of having to create rea-sonable baselines for researchers who wish to usethis corpus to evaluate their systems.
We createdjust one data split in training, development and testset, covering a collection of genres for each layerof annotation in each language in order to keep theworkload manageable However, the results do notdiscriminate the performance on individual gen-res: we believe such a setup is still a more realisticgauge for the performance of the state-of-the-artNLP components than a monolithic corpus suchas the Wall Street Journal section of the Penn Tree-bank.
It can be used as a starting point for devel-oping the next generation of NLP components thatare more robust and perform well on a multitudeof genres for a variety of different languages.23(MUC + BCUBED + CEAFe)/324http://cemantix.org1506 AcknowledgmentsWe gratefully acknowledge the support of theDefense Advanced Research Projects Agency(DARPA/IPTO) under the GALE program,DARPA/CMO Contract No.
HR0011-06-C-0022for sponsoring the creation of the OntoNotescorpus.
This work was partially supportedby grants R01LM10090 and U54LM008748from the National Library Of Medicine, andR01GM090187 from the National Institutes ofGeneral Medical Sciences.
We are indebted toSlav Petrov for helping us to retrain his syntacticparser for Arabic.
Alessandro Moschitti andOlga Uryupina have been partially funded bythe European Community?s Seventh FrameworkProgramme (FP7/2007-2013) under the grantnumber 288024 (LIMOSINE).
The contentis solely the responsibility of the authors anddoes not necessarily represent the official viewsof the National Institutes of Health.
NianwenXue and Yuchen Zhang are supported in partby the DAPRA via contract HR0011-11-C-0145entitled ?Linguistic Resources for MultilingualProcessing.
?ReferencesOlga Babko-Malaya, Ann Bies, Ann Taylor, Szuting Yi,Martha Palmer, Mitch Marcus, Seth Kulick, and LibinShen.
2006.
Issues in synchronizing the English treebankand propbank.
In Workshop on Frontiers in LinguisticallyAnnotated Corpora 2006, July.Elizabeth Baran and Nianwen Xue.
2011.
Singular or plural?exploiting parallel corpora for Chinese number prediction.In Proceedings of Machine Translation Summit XIII, Xia-men, China.Shane Bergsma and Dekang Lin.
2006.
Bootstrapping path-based pronoun resolution.
In Proceedings of the 21st In-ternational Conference on Computational Linguistics and44th Annual Meeting of the Association for Computa-tional Linguistics, pages 33?40, Sydney, Australia, July.Anders Bjo?rkelund and Richa?rd Farkas.
2012.
Data-drivenmultilingual coreference resolution using resolver stack-ing.
In Joint Conference on EMNLP and CoNLL - SharedTask, pages 49?55, Jeju Island, Korea, July.
Associationfor Computational Linguistics.Anders Bjo?rkelund and Pierre Nugues.
2011.
Exploring lex-icalized features for coreference resolution.
In Proceed-ings of the Fifteenth Conference on Computational Natu-ral Language Learning: Shared Task, pages 45?50, Port-land, Oregon, USA, June.
Association for ComputationalLinguistics.Jie Cai and Michael Strube.
2010.
Evaluation metrics forend-to-end coreference resolution systems.
In Proceed-ings of the 11th Annual Meeting of the Special InterestGroup on Discourse and Dialogue, SIGDIAL ?10, pages28?36.Shu Cai, David Chiang, and Yoav Goldberg.
2011.Language-independent parsing with empty elements.
InProceedings of the 49th Annual Meeting of the Associationfor Computational Linguistics: Human Language Tech-nologies, pages 212?216, Portland, Oregon, USA, June.Association for Computational Linguistics.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduction tothe CoNLL-2005 shared task: Semantic role labeling.
InProceedings of the Ninth Conference on ComputationalNatural Language Learning (CoNLL), Ann Arbor, MI,June.Eugene Charniak and Mark Johnson.
2001.
Edit detectionand parsing for transcribed speech.
In Proceedings of theSecond Meeting of the North American Chapter of the As-sociation for Computational Linguistics, June.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-finen-best parsing and maxent discriminative reranking.
InProceedings of the 43rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL), Ann Arbor, MI,June.Mona Diab, Alessandro Moschitti, and Daniele Pighin.
2008.Semantic role labeling systems for Arabic using kernelmethods.
In Proceedings of ACL-08: HLT, pages 798?806, Columbus, Ohio, June.
Association for Computa-tional Linguistics.Gerard Escudero, Lluis Marquez, and German Rigau.
2000.An empirical study of the domain dependence of super-vised word disambiguation systems.
In 2000 Joint SIG-DAT Conference on Empirical Methods in Natural Lan-guage Processing and Very Large Corpora, pages 172?180, Hong Kong, China, October.
Association for Com-putational Linguistics.Benoit Favre, Bernd Bohnet, and D. Hakkani-Tur.
2010.Evaluation of semantic role labeling and dependencyparsing of automatic speech recognition output.
InProceedings of 2010 IEEE International Conference onAcoustics, Speech and Signal Processing (ICASSP), page5342?5345.Jenny Rose Finkel and Christopher D. Manning.
2009.
Jointparsing and named entity recognition.
In Proceedings ofHuman Language Technologies: The 2009 Annual Con-ference of the North American Chapter of the Associationfor Computational Linguistics, pages 326?334, Boulder,Colorado, June.
Association for Computational Linguis-tics.Jenny Rose Finkel, Trond Grenager, and Christopher Man-ning.
2005.
Incorporating non-local information into in-formation extraction systems by Gibbs sampling.
In Pro-ceedings of the 43rd Annual Meeting of the Associationfor Computational Linguistics, page 363?370.Ryan Gabbard.
2010.
Null Element Restoration.
Ph.D. the-sis, University of Pennsylvania.Daniel Gildea.
2001.
Corpus variation and parser perfor-mance.
In 2001 Conference on Empirical Methods in Nat-ural Language Processing (EMNLP), Pittsburgh, PA.Julia Hockenmaier and Mark Steedman.
2002.
Acquir-ing compact lexicalized grammars from a cleaner tree-bank.
In Proceedings of the Third LREC Conference, page1974?1981.Mohamed Maamouri and Ann Bies.
2004.
Developing anArabic treebank: Methods, guidelines, procedures, andtools.
In Ali Farghaly and Karine Megerdoomian, edi-tors, COLING 2004 Computational Approaches to ArabicScript-based Languages, pages 2?9, Geneva, Switzerland,August 28th.
COLING.Mitchell P. Marcus, Beatrice Santorini, and Mary AnnMarcinkiewicz.
1993.
Building a large annotated corpusof English: The Penn treebank.
Computational Linguis-tics, 19(2):313?330, June.David McClosky, Eugene Charniak, and Mark Johnson.2006.
Effective self-training for parsing.
In Proceedingsof the Human Language Technology Conference/NorthAmerican Chapter of the Association for ComputationalLinguistics (HLT/NAACL), New York City, NY, June.151Vincent Ng and Claire Cardie.
2002.
Improving machinelearning approaches to coreference resolution.
In Pro-ceedings of the Association for Computational Linguistics(ACL-02), pages 104?111.Martha Palmer, Daniel Gildea, and Paul Kingsbury.
2005.The Proposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Martha Palmer, Hoa Trang Dang, and Christiane Fellbaum.2007.
Making fine-grained and coarse-grained sense dis-tinctions, both manually and automatically.
Journal ofNatural Language Engineering, 13(2).Martha Palmer, Olga Babko-Malaya, Ann Bies, Mona Diab,Mohammed Maamouri, Aous Mansouri, and Wajdi Za-ghouani.
2008.
A pilot Arabic propbank.
In Proceedingsof the International Conference on Language Resourcesand Evaluation (LREC), Marrakech, Morocco, May 28-30.Slav Petrov and Dan Klein.
2007.
Improved inferencing forunlexicalized parsing.
In Proc of HLT-NAACL.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler, WayneWard, James Martin, and Dan Jurafsky.
2005.
Supportvector learning for semantic argument classification.
Ma-chine Learning, 60(1):11?39.Sameer Pradhan, Lance Ramshaw, Ralph Weischedel, Jes-sica MacBride, and Linnea Micciulla.
2007.
Unre-stricted coreference: Indentifying entities and events inOntoNotes.
In Proceedings of the IEEE InternationalConference on Semantic Computing (ICSC), September17-19.Sameer Pradhan, Wayne Ward, and James H. Martin.
2008.Towards robust semantic role labeling.
ComputationalLinguistics Special Issue on Semantic Role Labeling,34(2).Sameer Pradhan, Lance Ramshaw, Mitchell Marcus, MarthaPalmer, Ralph Weischedel, and Nianwen Xue.
2011.CoNLL-2011 shared task: Modeling unrestricted corefer-ence in OntoNotes.
In Proceedings of the Fifteenth Con-ference on Computational Natural Language Learning:Shared Task, pages 1?27, Portland, Oregon, USA, June.Association for Computational Linguistics.Sameer Pradhan, Alessandro Moschitti, Nianwen Xue, OlgaUryupina, and Yuchen Zhang.
2012.
CoNLL-2012 sharedtask: Modeling multilingual unrestricted coreference inOntoNotes.
In Joint Conference on EMNLP and CoNLL -Shared Task, pages 1?40, Jeju Island, Korea, July.
Associ-ation for Computational Linguistics.Rashmi Prasad, Nikhil Dinesh, Alan Lee, Eleni Miltsakaki,Livio Robaldo, Aravind Joshi, and Bonnie Webber.
2008.The Penn discourse treebank 2.0.
In Proceedings of theSixth International Conference on Language Resourcesand Evaluation (LREC?08), Marrakech, Morocco, May.Marta Recasens, Llu?
?s Ma`rquez, Emili Sapena, M.
Anto`niaMart?
?, Mariona Taule?, Ve?ronique Hoste, Massimo Poesio,and Yannick Versley.
2010.
Semeval-2010 task 1: Coref-erence resolution in multiple languages.
In Proceedings ofthe 5th International Workshop on Semantic Evaluation,pages 1?8, Uppsala, Sweden, July.Marta Recasens, Marie-Catherine de Marneffe, and Christo-pher Potts.
2013.
The life and death of discourse enti-ties: Identifying singleton mentions.
In Proceedings ofthe 2013 Conference of the North American Chapter ofthe Association for Computational Linguistics: HumanLanguage Technologies, pages 627?633, Atlanta, Geor-gia, June.
Association for Computational Linguistics.Libin Shen, Lucas Champollion, and Aravind K. Joshi.
2008.LTAG-spinal and the treebank.
Language Resources andEvaluation, 42(1):1?19, March.Ralph Weischedel and Ada Brunstein.
2005.
BBN pro-noun coreference and entity type corpus LDC catalog no.:LDC2005T33.
BBN Technologies.Ralph Weischedel, Eduard Hovy, Mitchell Marcus, MarthaPalmer, Robert Belvin, Sameer Pradhan, Lance Ramshaw,and Nianwen Xue.
2011.
OntoNotes: A large train-ing corpus for enhanced processing.
In Joseph Olive,Caitlin Christianson, and John McCary, editors, Hand-book of Natural Language Processing and MachineTranslation: DARPA Global Autonomous Language Ex-ploitation.
Springer.Nianwen Xue and Martha Palmer.
2009.
Adding semanticroles to the Chinese Treebank.
Natural Language Engi-neering, 15(1):143?172.Nianwen Xue, Fei Xia, Fu dong Chiou, and Martha Palmer.2005.
The Penn Chinese TreeBank: phrase structure an-notation of a large corpus.
Natural Language Engineer-ing, 11(2):207?238.Nianwen Xue.
2008.
Labeling Chinese predicates with se-mantic roles.
Computational Linguistics, 34(2):225?255.Yaqin Yang and Nianwen Xue.
2010.
Chasing the ghost:recovering empty categories in the Chinese treebank.In Proceedings of the 23rd International Conference onComputational Linguistics (COLING), Beijing, China.Wajdi Zaghouani, Mona Diab, Aous Mansouri, Sameer Prad-han, and Martha Palmer.
2010.
The revised Arabic prop-bank.
In Proceedings of the Fourth Linguistic AnnotationWorkshop, pages 222?226, Uppsala, Sweden, July.Zhi Zhong and Hwee Tou Ng.
2010.
It makes sense: A wide-coverage word sense disambiguation system for free text.In Proceedings of the ACL 2010 System Demonstrations,pages 78?83, Uppsala, Sweden.Zhi Zhong, Hwee Tou Ng, and Yee Seng Chan.
2008.Word sense disambiguation using OntoNotes: An empiri-cal study.
In Proceedings of the Conference on EmpiricalMethods in Natural Language Processing, pages 1002?1010.152
