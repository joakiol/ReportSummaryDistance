Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 541?550,Austin, Texas, November 1-5, 2016. c?2016 Association for Computational LinguisticsSolving Verbal Questions in IQ Test by Knowledge-Powered WordEmbeddingHuazheng WangUniversity of Virginiahw7ww@virginia.eduFei TianMicrosoft Researchfetia@microsoft.comBin GaoMicrosoftbingao@microsoft.comChengjieren ZhuUniversity of California, San Diegochz191@ucsd.eduJiang BianYidian Inc.jiang.bian.prc@gmail.comTie-Yan LiuMicrosoft Researchtyliu@microsoft.comAbstractVerbal comprehension questions appear veryfrequently in Intelligence Quotient (IQ) tests,which measure human?s verbal ability includ-ing the understanding of the words with mul-tiple senses, the synonyms and antonyms, andthe analogies among words.
In this work,we explore whether such tests can be solvedautomatically by the deep learning technolo-gies for text data.
We found that the taskwas quite challenging, and simply applyingexisting technologies like word embeddingcould not achieve a good performance, dueto the multiple senses of words and the com-plex relations among words.
To tackle thesechallenges, we propose a novel framework toautomatically solve the verbal IQ questionsby leveraging improved word embedding byjointly considering the multi-sense nature ofwords and the relational information amongwords.
Experimental results have shown thatthe proposed framework can not only outper-form existing methods for solving verbal com-prehension questions but also exceed the aver-age performance of the Amazon MechanicalTurk workers involved in the study.1 IntroductionThe Intelligence Quotient (IQ) test (Stern, 1914)is a test of intelligence designed to formally studythe success of an individual in adapting to a spe-cific situation under certain conditions.
CommonIQ tests measure various types of abilities such asverbal, mathematical, logical, and reasoning skills.These tests have been widely used in the study ofpsychology, education, and career development.
Inthe community of artificial intelligence, agents havebeen invented to fulfill many interesting and chal-lenging tasks like face recognition, speech recogni-tion, handwriting recognition, and question answer-ing.
However, as far as we know, there are very lim-ited studies of developing an agent to solve IQ tests,which in some sense is more challenging, since evencommon human beings do not always succeed onsuch tests.
Considering that IQ test scores have beenwidely considered as a measure of intelligence, wethink it is worth further investigating whether we candevelop an agent that can solve IQ test questions.The commonly used IQ tests contain several typesof questions like verbal, mathematical, logical, andpicture questions, among which a large proportion(near 40%) are verbal questions (Carter, 2005).
Therecent progress on deep learning for natural lan-guage processing (NLP), such as word embeddingtechnologies, has advanced the ability of machines(or AI agents) to understand the meaning of wordsand the relations among words.
This inspires usto solve the verbal questions in IQ tests by lever-aging the word embedding technologies.
However,our attempts show that a straightforward applica-tion of word embedding does not result in satisfac-tory performances.
This is actually understandable.Standard word embedding technologies learn oneembedding vector for each word based on the co-occurrence information in a text corpus.
However,verbal comprehension questions in IQ tests usuallyconsider the multiple senses of a word (and often fo-cus on the rare senses), and the complex relationsamong (polysemous) words.
This has clearly ex-ceeded the capability of standard word embedding541technologies.To tackle the aforementioned challenges, we pro-pose a novel framework that consists of three com-ponents.First, we build a classifier to recognize the spe-cific type (e.g., analogy, classification, synonym,and antonym) of verbal questions.
For differenttypes of questions, different kinds of relationshipsneed to be considered and the solvers could havedifferent forms.
Therefore, with an effective ques-tion type classifier, we may solve the questions in adivide-and-conquer manner.Second, we obtain distributed representations ofwords and relations by leveraging a novel word em-bedding method that considers the multi-sense na-ture of words and the relational knowledge amongwords (or their senses) contained in dictionaries.
Inparticular, for each polysemous word, we retrieveits number of senses from a dictionary, and con-duct clustering on all its context windows in thecorpus.
Then we attach the example sentences forevery sense in the dictionary to the clusters, suchthat we can tag the polysemous word in each con-text window with a specific word sense.
On topof this, instead of learning one embedding vectorfor each word, we learn one vector for each pairof word-sense.
Furthermore, in addition to learningthe embedding vectors for words, we also learn theembedding vectors for relations (e.g., synonym andantonym) at the same time, by incorporating rela-tional knowledge into the objective function of theword embedding learning algorithm.
That is, thelearning of word-sense representations and relationrepresentations interacts with each other, such thatthe relational knowledge obtained from dictionariesis effectively incorporated.Third, for each type of question, we propose aspecific solver based on the obtained distributedword-sense representations and relation represen-tations.
For example, for analogy questions, wefind the answer by minimizing the distance betweenword-sense pairs in the question and the word-sensepairs in the candidate answers.We have conducted experiments using a com-bined IQ test set to test the performance of our pro-posed framework.
The experimental results showthat our method can outperform several baselinemethods for verbal comprehension questions on IQtests.
We further deliver the questions in the testset to human beings through Amazon MechanicalTurk1.
The average performance of the human be-ings is even a little lower than that of our proposedmethod.2 Related Work2.1 Verbal Questions in IQ TestIn common IQ tests, a large proportion of ques-tions are verbal comprehension questions, whichplay an important role in deciding the final IQscores.
For example, in Wechsler Adult IntelligenceScale (Wechsler, 2008), which is among the most fa-mous IQ test systems, the full-scale IQ is calculatedfrom two IQ scores: Verbal IQ and Performance IQ,and around 40% of questions in a typical test are ver-bal comprehension questions.
Verbal questions cantest not only the verbal ability (e.g., understandingpolysemy of a word), but also the reasoning abil-ity and induction ability of an individual.
Accord-ing to previous studies (Carter, 2005), verbal ques-tions mainly have the types elaborated in Table 1,in which the correct answers are highlighted in boldfont.Analogy-I questions usually take the form ?A isto B as C is to ??.
One needs to choose a wordD from a given list of candidate words to form ananalogical relation between pair (A, B) and pair (C,D).
Such questions test the ability of identifyingan implicit relation from word pair (A, B) and ap-ply it to compose word pair (C, D).
Note that theAnalogy-I questions are also used as a major eval-uation task in the word2vec models (Mikolov et al,2013).
Analogy-II questions require two words tobe identified from two given lists in order to form ananalogical relation like ?A is to ?
as C is to ??.
Suchquestions are a bit more difficult than the Analogy-I questions since the analogical relation cannot beobserved directly from the questions, but need to besearched for in the word pair combinations from thecandidate answers.
Classification questions requireone to identify the word that is different (or dissim-ilar) from others in a given word list.
Such ques-tions are also known as odd-one-out, which havebeen studied in (Pinte?r et al, 2012).
Classificationquestions test the ability to summarize the majority1http://www.mturk.com/542Type ExampleAnalogy-I Isotherm is to temperature as isobar is to?
(i) atmosphere, (ii) wind, (iii) pressure, (iv) latitude, (v) current.Analogy-II Identify two words (one from each set of brackets) that form a connection (analogy)when paired with the words in capitals: CHAPTER (book, verse, read), ACT (stage, audience, play).Classification Which is the odd one out?
(i) calm, (ii) quiet, (iii) relaxed, (iv) serene, (v) unruffled.Synonym Which word is closest to IRRATIONAL?
(i)intransigent, (ii) irredeemable, (iii) unsafe, (iv) lost, (v) nonsensical.Antonym Which word is most opposite to MUSICAL?
(i) discordant, (ii) loud, (iii) lyrical, (iv) verbal, (v) euphonious.Table 1: Types of verbal questions.sense of the words and identify the outlier.
Synonymquestions require one to pick one word out of a list ofwords such that it has the closest meaning to a givenword.
Synonym questions test the ability of identi-fying all senses of the candidate words and selectingthe correct sense that can form a synonymous rela-tion to the given word.
Antonym questions requireone to pick one word out of a list of words suchthat it has the opposite meaning to a given word.Antonym questions test the ability of identifying allsenses of the candidate words and selecting the cor-rect sense that can form an antonymous relation tothe given word.
(Turney, 2008; Turney, 2011) stud-ied the analogy, synonym and antonym problem us-ing a supervised classification approach.Although there are some efforts to solve math-ematical, logical, and picture questions in IQtest (Sanghi and Dowe, 2003; Strannegard et al,2012; Kushmany et al, 2014; Seo et al, 2014; Hos-seini et al, 2014; Weston et al, 2015), there havebeen very few efforts to develop automatic methodsto solve verbal questions.2.2 Deep Learning for Text MiningBuilding distributed word representations (Bengio etal., 2003), a.k.a.
word embeddings, has attractedincreasing attention in the area of machine learn-ing.
Different from conventional one-hot represen-tations of studies or distributional word representa-tions based on co-occurrence matrix between wordssuch as LSA (Dumais et al, 1988) and LDA (Bleiet al, 2003), distributed word representations areusually low-dimensional dense vectors trained withneural networks by maximizing the likelihood of atext corpus.
Recently, a series of works applied deeplearning techniques to learn high-quality word rep-resentations (Collobert and Weston, 2008; Mikolovet al, 2013; Pennington et al, 2014).Nevertheless, since the above works learn wordrepresentations mainly based on the word co-occurrence information, it is quite difficult to obtainhigh quality embeddings for those words with verylittle context information; on the other hand, a largeamount of noisy or biased context could give rise toineffective word embeddings.
Therefore, it is neces-sary to introduce extra knowledge into the learningprocess to regularize the quality of word embedding.Some efforts have paid attention to learn word em-bedding in order to address knowledge base comple-tion and enhancement (Bordes et al, 2011; Socheret al, 2013; Weston et al, 2013a), and some otherefforts have tried to leverage knowledge to enhanceword representations (Luong et al, 2013; Weston etal., 2013b; Fried and Duh, 2014; Celikyilmaz et al,2015).
Moreover, all the above models assume thatone word has only one embedding no matter whetherthe word is polysemous or not, which might causesome confusion for the polysemous words.
To solvethe problem, there are several efforts like (Huanget al, 2012; Tian et al, 2014; Neelakantan et al,2014).
However, these models do not leverage anyextra knowledge (e.g., relational knowledge) to en-hance word representations.3 Solving Verbal QuestionsIn this section, we introduce our proposed frame-work to solve the verbal questions, which consistsof the following three components.3.1 Classification of Question TypesThe first component of the framework is a questionclassifier, which identifies different types of verbalquestions.
Since different types of questions havetheir unique ways of expression, the classificationtask is relatively easy, and we therefore take a simpleapproach to fulfill the task.
Specifically, we regardeach verbal question as a short document and usethe TF?IDF features to build its representation.
Thenwe train an SVM classifier with linear kernel on aportion of labeled question data, and apply it to other543questions.
The question labels include Analogy-I,Analogy-II, Classification, Synonym, and Antonym.We use the one-vs-rest training strategy to obtain alinear SVM classifier for each question type.3.2 Embedding of Word-Senses and RelationsThe second component of our framework leveragesdeep learning technologies to learn distributed rep-resentations for words (i.e.
word embedding).
Notethat in the context of verbal question answering, wehave some specific requirements on this learningprocess.
Verbal questions in IQ tests usually con-sider the multiple senses of a word (and focus onthe rare senses), and the complex relations among(polysemous) words, such as synonym and antonymrelation.
Figure 1 shows an example of the multi-sense of words and the relations among word senses.We can see that irrational has three senses.
Its firstsense has an antonym relation with the second senseof rational, while its second sense has a synonymrelation with nonsensical and an antonym relationwith the first sense of rational.The above challenge has exceeded the capabilityof standard word embedding technologies.
To ad-dress this problem, we propose a novel approachthat considers the multi-sense nature of words andintegrate the relational knowledge among words (ortheir senses) into the learning process.
In particu-lar, our approach consists of two steps.
The firststep aims at labeling a word in the text corpus withits specific sense, and the second step employs boththe labeled text corpus and the relational knowledgecontained in dictionaries to simultaneously learnembeddings for both word-sense pairs and relations.3.2.1 Multi-Sense IdentificationFirst, we learn a single-sense word embedding byusing the skip-gram method in word2vec (Mikolovet al, 2013).Second, we gather the context windows of all oc-currences of a word used in the skip-gram model,and represent each context by a weighted averageof the pre-learned embedding vectors of the con-text words.
We use TF?IDF to define the weight-ing function, where we regard each context win-dow of the word as a short document to calcu-late the document frequency.
Specifically, for aword w0, each of its context window can be de-noted by (w?N , ?
?
?
, w0, ?
?
?
, wN ).
Then we repre-sent the window by calculating the weighted averageof the pre-learned embedding vectors of the contextwords as ?
= 12N?Ni=?N,i6=0 gwivwi ,where gwi isthe TF?IDF score of wi, and vwi is the pre-learnedembedding vector of wi.
After that, for each word,we use spherical k-means to cluster all its contextrepresentations, where cluster number k is set as thenumber of senses of this word in the online dictio-nary.Third, we match each cluster to the correspond-ing sense in the dictionary.
On one hand, we repre-sent each cluster by the average embedding vectorof all those context windows included in the clus-ter.
For example, suppose word w0 has k senses andthus it has k clusters of context windows, we de-note the average embedding vectors for these clus-ters as ?
?1, ?
?
?
, ??k.
On the other hand, since the on-line dictionary uses some descriptions and examplesentences to interpret each word sense, we can rep-resent each word sense by the average embeddingof those words including its description words andthe words in the corresponding example sentences.Here, we assume the representation vectors (basedon the online dictionary) for the k senses of w0 are?1, ?
?
?
, ?k.
After that, we consecutively match eachcluster to its closest word sense in terms of the dis-tance computed in the word embedding space:(??i?
, ?j?)
= argmini,j=1,???
,kd(?
?i, ?j), (1)where d(?, ?)
calculates the Euclidean distance and(??i?
, ?j?)
is the first matched pair of window clusterand word sense.
Here, we simply take a greedy strat-egy.
That is, we remove ??i?
and ?j?
from the clustervector set and the sense vector set, and recursivelyrun (1) to find the next matched pair till all the pairsare found.
Finally, each word occurrence in the cor-pus is relabeled by its associated word sense, whichwill be used to learn the embeddings for word-sensepairs in the next step.3.2.2 Co-Learning Word-Sense PairRepresentations and RelationRepresentationsAfter relabeling the text corpus, different occur-rences of a polysemous word may correspond toits different senses, or more accurately word-sensepairs.
We then learn the embeddings for word-544Irrational (sense 1)adj.
without power to reasonIrrational (sense 2)adj.
unreasonableIrrational (sense 3)n. real number that cannot be expressed as the quotient of two integersnonsensical (sense 1)adj.
foolish or absurdAbsurd (sense 1)adj.
against reason or common senseAbsurd (sense 2)adj.
funny because clearly unsuitable, foolish, false, or impossibleRational (sense 1)adj.
sensibleRational (sense 2)adj.
able to reason Synonym relationAntonym relationFigure 1: An example on the multi-sense of words and the relations between word senses.sense pairs and relations (obtained from dictionar-ies, such as synonym and antonym) simultaneously,by integrating relational knowledge into the objec-tive function of the word embedding learning modellike skip-gram.
We propose to use a function Er asdescribed below to capture the relational knowledge.Specifically, the existing relational knowledgeextracted from dictionaries, such as synonym,antonym, etc., can be naturally represented in theform of a triplet (head, relation, tail) (denoted by(hi, r, tj) ?
S, where S is the set of relationalknowledge), which consists of two word-sense pairs(i.e.
word h with its i-th sense and word t with itsj-th sense), h, t ?
W (W is the set of words) and arelationship r ?
R (R is the set of relationships).
Tolearn the relation representations, we make an as-sumption that relationships between words can beinterpreted as translation operations and they can berepresented by vectors.
The principle in this modelis that if the relationship (hi, r, tj) exists, the repre-sentation of the word-sense pair tj should be closeto that of hi plus the representation vector of the re-lationship r, i.e.
hi + r; otherwise, hi + r shouldbe far away from tj .
Note that this model learnsword-sense pair representations and relation repre-sentations in a unified continuous embedding space.According to the above principle, we define Er asa margin-based regularization function over the setof relational knowledge S,Er =?(hi,r,tj)?S(h?,r,t?)?S?(hi,r,tj)[?
+ d(hi + r, tj)?
d(h?+ r, t?
)]+.Here [X]+ = max(X, 0), ?
> 0 is a margin hyper-parameter, and d(?, ?)
is the Euclidean distance be-tween two words in the embedding space.The set ofcorrupted triplets S?
(h,r,t) is defined as S?
(hi,r,tj) ={(h?
, r, t)}?
{(h, r, t?
)},which is constructed fromS by replacing either the head word-sense pair or thetail word-sense pair by another randomly selectedword with its randomly selected sense.To avoid the trivial solution that simply increasesthe norms of representation vectors, we use an ad-ditional soft norm constraint on the relation repre-sentations as ri = 2?
(xi) ?
1, where ?(?)
is thesigmoid function ?
(xi) = 1/(1 + e?xi), ri is thei-th dimension of relation vector r, and xi is a latentvariable, which guarantees that every dimension ofthe relation representation vector is within the range(?1, 1).By combining the skip-gram objective functionand the regularization function derived from rela-tional knowledge, we get the combined objectiveJr = ?Er ?
L that incorporates relational knowl-edge into the word-sense pair embedding calcula-tion process, where ?
is the combination coefficient.Our goal is to minimize Jr, which can be optimizedusing back propagation neural networks.
Figure 2shows the structure of the proposed model.By usingthis model, we can obtain the distributed representa-tions for both word-sense pairs and relations simul-taneously.3.3 Solvers for Each Type of Questions3.3.1 Analogy-IFor the Analogy-I questions like ?A is to B as Cis to ?
?, we answer them by optimizing:D = argmaxib,ia,ic,id?
;D?
?Tcos(v(B,ib) ?
v(A,ia) + v(C,ic), v(D?,id?
))(2)545Embedding ofsoftmax softmax softmax softmax???
?Embedding of Embedding ofLoss of relationFigure 2: The structure of the proposed model.where T contains all the candidate answers, cosmeans cosine similarity, and ib, ia, ic, id?
are the in-dexes for the word senses of B,A,C,D?
respec-tively.
Finally D is selected as the answer.3.3.2 Analogy-IIAs the form of the Analogy-II questions is like?A is to ?
as C is to ??
with two lists of candidateanswers, we can apply an optimization method asbelow to select the best (B,D) pair,argmaxib?
,ia,ic,id?
;B??T1,D??T2cos(v(B?,ib? )
?
v(A,ia) + v(C,ic), v(D?,id?
)),(3)where T1, T2 are two lists of candidate words.
Thuswe get the answers B and D that can form an ana-logical relation between word pair (A, B) and wordpair (C,D) under a certain specific word sense com-bination.3.3.3 ClassificationFor the Classification questions, we leverage theproperty that words with similar co-occurrence in-formation are distributed close to each other inthe embedding space.
The candidate word thatis not similar to others does not have similar co-occurrence information to other words in the train-ing corpus, and thus this word should be far awayfrom other words in the word embedding space.Therefore we first calculate a group of mean vec-tors miw1 ,???
,iwN of all the candidate words withany possible word senses as below miw1 ,???
,iwN =1N?wj?T v(wj ,iwj ), where T is the set of candidatewords, N is the capacity of T , wj is a word in T ;iwj (j = 1, ?
?
?
, N ; iwj = 1, ?
?
?
, kwj ) is the indexfor the word senses of wj , and kwj (j = 1, ?
?
?
, N)is the number of word senses of wj .
Therefore, thenumber of the mean vectors is M = ?Nj=1 kwj .As both N and kwj are very small, the computationcost is acceptable.
Then, we choose the word withsuch a sense that its closest sense to the correspond-ing mean vector is the largest among the candidatewords as the answer, i.e.,w = argmaxwj?Tminiwj ;l=1,???
,Md(v(wj ,iwj ),ml).
(4)3.3.4 SynonymFor the Synonym questions, we empirically ex-plored two solvers.
For the first solver, we alsoleverage the property that words with similar co-occurrence information are located closely in theword embedding space.
Therefore, given the ques-tion word wq and the candidate words wi, we canfind the answer by solving:w = argminiwq ,iwj ;wj?Td(v(wj ,iwj ), v(wq,iwq )), (5)where T is the set of candidate words.
The sec-ond solver is based on the minimization objectiveof the translation distance between entities in the re-lational knowledge model (2).
Specifically, we cal-culate the offset vector between the embedding ofquestion word wq and each word wj in the candi-date list.
Then, we set the answer w as the candidateword with which the offset is the closest to the rep-resentation vector of the synonym relation rs, i.e.,w = argminiwq ,iwj ;wj?T?
?|v(wj ,iwj ) ?
v(wq,iwq )| ?
rs??.
(6)In practice, we found the second solver performsbetter (the results are listed in Section 4).
For ourbaseline embedding model skip-gram, since it doesnot assume the relation representations explicitly,we use the first solver for it.3.3.5 AntonymSimilar to solving the Synonym questions, we ex-plored two solvers for Antonym questions as well.That is, the first solver (7) is based on the smalloffset distance between semantically close wordswhereas the second solver (8) leverages the trans-lation distance between two words?
offset and theembedding vector of the antonym relation.
The firstsolver is based on the fact that since an antonym and546its original word have similar co-occurrence infor-mation from which the embedding vectors are de-rived, the embedding vectors of both words withantonym relation will still lie closely in the embed-ding space.w = argminiwq ,iwj ;wj?Td(v(wj ,iwj ), v(wq,iwq )), (7)w = argminiwq ,iwj ;wj?T?
?|v(wj ,iwj ) ?
v(wq,iwq )| ?
ra?
?, (8)Here T is the set of candidate words and ra is therepresentation vector of the antonym relation.
Againwe found that the second solver performs better.Similarly, for skip-gram, the first solver is applied.4 ExperimentsWe conduct experiments to examine whether ourproposed framework can achieve satisfying resultson verbal comprehension questions.4.1 Data Collection4.1.1 Training Set for Word EmbeddingWe trained word embeddings on a publicly avail-able text corpus named wiki20142, which is a largetext snapshot from Wikipedia.
After being pre-processed by removing all the html meta-data andreplacing the digit numbers by English words, thefinal training corpus contains more than 3.4 billionword tokens, and the number of unique words, i.e.the vocabulary size, is about 2 million.4.1.2 IQ Test SetAccording to our study, there is no online datasetspecifically released for verbal comprehension ques-tions, although there are many online IQ tests forusers to play with.
In addition, most of the on-line tests only calculate the final IQ scores but donot provide the correct answers.
Therefore, we onlyuse the online questions to train the verbal questionclassifier described in Section 3.1.
Specifically, wemanually collected and labeled 30 verbal questionsfrom the online IQ test Websites3 for each of thefive types (i.e.
Analogy-I, Analogy-II, Classifica-tion, Synonym, and Antonym) and trained an one-2http://en.wikipedia.org/wiki/Wikipedia:Database_download3http://wechsleradultintelligencescale.com/vs-rest SVM classifier for each type.
The total accu-racy on the training set itself is 95.0%.
The classifierwas then applied in the test set below.We collected a set of verbal comprehension ques-tions associated with correct answers from pub-lished IQ test books, such as (Carter, 2005; Carter,2007; Pape, 1993; Ken Russell, 2002), and we usedthis collection as the test set to evaluate the ef-fectiveness of our new framework.
In total, thistest set contains 232 questions with the correspond-ing answers.4 The number of each question type(i.e., Analogy-I, Analogy-II, Classification, Syn-onym, Antonym) are respectively 50, 29, 53, 51, 49.4.2 Compared MethodsIn our experiments, we compare our new relationknowledge powered model to several baselines.Random Guess Model (RG).
Random guess isthe most straightforward way for an agent to solvequestions.
In our experiments, we used a randomguess agent which would select an answer randomlyregardless of what the question was.
To measure theperformance of random guess, we ran each task for5 times and calculated the average accuracy.Human Performance (HP).
Since IQ tests aredesigned to evaluate human intelligence, it is quitenatural to leverage human performance as a base-line.
To collect human answers on the test ques-tions, we delivered them to human beings throughAmazon Mechanical Turk (AMT), a crowd-sourcingInternet marketplace that allows people to partici-pate in Human Intelligence Tasks.
In our study, wepublished five AMT jobs, one job corresponding toone specific question type.
The jobs were deliv-ered to 200 people.
To control the quality of thecollected results, we used several strategies: (i) weimposed high restrictions on the workers by requir-ing all the workers to be native English speakers inNorth America and to be AMT Masters (who havedemonstrated high accuracy on previous tasks onAMT marketplace); (ii) we recruited a large numberof workers in order to guarantee the statistical confi-dence in their performances; (iii) we tracked theirage distribution and education background, which4It can be downloaded from https://www.dropbox.com/s/o0very1gwv3mrt5/VerbalQuestions.zip?dl=0.547are very similar to those of the overall populationin the U.S.Latent Dirichlet Allocation Model (LDA).
Thisbaseline model leveraged one of the most commonclassical distributional word representations, i.e.
La-tent Dirichlet Allocation (LDA) (Blei et al, 2003).In particular, we trained word representations usingLDA on wiki2014 with the topic number 1000.Skip-Gram Model (SG).
In this baseline, weapplied the word embedding trained by skip-gram (Mikolov et al, 2013) (denoted by SG-1) onwiki2014.
In particular, we set the window size as 5,the embedding dimension as 500, the negative sam-pling count as 3, and the epoch number as 3.
In ad-dition, we also employed a pre-trained word embed-ding by Google5 with the dimension of 300 (denotedby SG-2).Glove.
Another powerful word embeddingmodel (Pennington et al, 2014).
Glove configura-tions are the same as those in running SG-1.Multi-Sense Model (MS).
In this baseline, weapplied the multi-sense word embedding modelsproposed in (Huang et al, 2012; Tian et al, 2014;Neelakantan et al, 2014) (denoted by MS-1, MS-2and MS-3 respectively).
For MS-1, we directly usedthe published multi-sense word embedding vectorsby the authors6, in which they set 10 senses for thetop 5% most frequent words.
For MS-2 and MS-3, we get the embedding vectors by usingf the re-leased codes from the authors using the same con-figurations as MS-1.Relation Knowledge Powered Model (RK).This is our proposed method in Section 3.
In par-ticular, when learning the embedding on wiki2014,we set the window size as 5, the embedding dimen-sion as 500, the negative sampling count as 3, andthe epoch number as 3.
We adopted the online Long-man Dictionary as the dictionary used in multi-senseclustering.
We used a public relation knowledge set,WordRep (Gao et al, 2014), for relation training.4.3 Experimental Results4.3.1 Accuracy of Question ClassifierWe applied the question classifier trained in Sec-tion 4.1.2 on the test set, and got the total accuracy5https://code.google.com/p/word2vec/6http://ai.stanford.edu/?ehhuang/93.1%.
For RG and HP, the question classifier wasnot needed.
For other methods, the wrongly classi-fied questions were also sent to the correspondingwrong solver to find an answer.
If the solver re-turned an empty result (which was usually causedby invalid input format, e.g., an Analogy-II questionwas wrongly input to the Classification solver), wewould randomly select an answer.4.3.2 Overall AccuracyTable 2 demonstrates the accuracy of answeringverbal questions by using all the approaches men-tioned in Section 4.2.
The numbers for all the mod-els are mean values from five repeated runs.
Fromthis table, we observe: (i) RK can achieve the bestoverall accuracy than all the other methods.
In par-ticular, RK can raise the overall accuracy by about4.63% over HP7.
(ii) RK is empirically superior tothe skip-gram models SG-1/SG-2 and Glove.
Ac-cording to our understanding, the improvement ofRK over SG-1/SG-2/Glove comes from two aspects:multi-sense and relational knowledge.
Note that theperformance difference between MS-1/MS-2/MS-3and SG-1/SG-2/Glove is not significant, showingthat simply changing single-sense word embeddingto multi-sense word embedding does not bring toomuch benefit.
One reason is that the rare word-senses do not have enough training data (contextualinformation) to produce high-quality word embed-ding.
By further introducing the relational knowl-edge among word-senses, the training for rare word-senses will be linked to the training of their relatedword-senses.
As a result, the embedding quality ofthe rare word-senses will be improved.
(iii) RK isempirically superior than the two multi-sense algo-rithms MS-1, MS-2 and MS-3, demonstrating theeffectiveness brought by adopting fewer model pa-rameters and using an online dictionary in buildingthe multi-sense embedding model.These results are quite impressive, indicating thepotential of using machines to comprehend humanknowledge and even achieve a comparable level ofhuman intelligence.4.3.3 Accuracy on Different Question TypesTable 2 reports the accuracy of answering varioustypes of verbal questions by each method.
From the7With the t-test score p = 0.036.548Analogy-I Analogy-II Classification Synonym Antonym TotalRG 24.60 11.72 20.75 19.27 23.13 20.51LDA 28.00 13.79 39.62 27.45 30.61 29.31HP 45.87 34.37 47.23 50.38 53.30 46.23SGSG-1 38.00 24.14 37.74 45.10 40.82 38.36SG-2 38.00 20.69 39.62 47.06 44.90 39.66Glove 45.09 24.14 32.08 47.06 40.82 39.03MSMS-1 36.36 19.05 41.30 50.00 36.59 38.67MS-2 40.00 20.69 41.51 49.02 40.82 40.09MS-3 17.65 20.69 47.17 47.06 30.61 36.73RK 48.00 34.48 52.83 60.78 51.02 50.86Table 2: Accuracy of different methods among different human groups.table, we can observe that the SG and MS modelscan achieve competitive accuracy on certain ques-tion types (like Synonym) compared with HP.
Afterincorporating knowledge into learning word embed-ding, our RK model can improve the accuracy overall question types.
Moreover, the table shows thatRK can result in a big improvement over HP on thequestion types of Synonym and Classification.To sum up, the experimental results have demon-strated the effectiveness of the proposed RK modelcompared with several baseline methods.
Althoughthe test set is not large, the generalization of RK toother test sets should not be a concern due to the un-supervised nature of our model.5 ConclusionsWe investigated how to automatically solve verbalcomprehension questions in IQ Tests by using wordembedding techniques.
In particular, we proposeda three-step framework: (i) to recognize the spe-cific type of a verbal comprehension question bya classifier, (ii) to leverage a novel deep learningmodel to co-learn the representations of both word-sense pairs and relations among words (or theirsenses), (iii) to design dedicated solvers, based onthe obtained word-sense pair representations and re-lation representations, for addressing each type ofquestions.
Experimental results have demonstratedthat this novel framework can achieve better per-formance than existing methods for solving verbalcomprehension questions and even exceed the aver-age performance of the Amazon Mechanical Turkworkers involved in the experiments.ReferencesYoshua Bengio, Re?jean Ducharme, Pascal Vincent, andChristian Jauvin.
2003.
A neural probabilistic lan-guage model.
Journal of Machine Learning Research,3:1137?1155.David M Blei, Andrew Y Ng, and Michael I Jordan.2003.
Latent dirichlet alocation.
the Journal of ma-chine Learning research, 3:993?1022.Antoine Bordes, Jason Weston, Ronan Collobert, YoshuaBengio, et al 2011.
Learning structured embeddingsof knowledge bases.
In AAAI.Philip Carter.
2005.
The complete book of intelligencetests.
John Wiley & Sons Ltd.Philip Carter.
2007.
The Ultimate IQ Test Book: 1,000Practice Test Questions to Boost Your Brain Power.Kogan Page Publishers.Asli Celikyilmaz, Dilek Hakkani-Tur, Panupong Pasu-pat, and Ruhi Sarikaya.
2015.
Enriching word em-beddings using knowledge graph for semantic taggingin conversational dialog systems.
In Proceedings ofAAAI.Ronan Collobert and Jason Weston.
2008.
A unified ar-chitecture for natural language processing: Deep neu-ral networks with multitask learning.
In Proceedingsof ICML, pages 160?167.
ACM.Susan T Dumais, George W Furnas, Thomas K Landauer,Scott Deerwester, and Richard Harshman.
1988.
Us-ing latent semantic analysis to improve access to tex-tual information.
In Proceedings of SIGCHI.Daniel Fried and Kevin Duh.
2014.
Incorporating bothdistributional and relational semantics in word repre-sentations.
CoRR, abs/1412.4369.Bin Gao, Jiang Bian, and Tie-Yan Liu.
2014.
Wordrep:A benchmark for research on learning word represen-tations.
arXiv preprint arXiv:1407.1640.Mohammad Javad Hosseini, Hannaneh Hajishirzi, OrenEtzioni, and Nate Kushman.
2014.
Learning to solvearithmetic word problems with verb categorization.
InProceedings of EMNLP, pages 523?533.549Eric H Huang, Richard Socher, Christopher D Manning,and Andrew Y Ng.
2012.
Improving word representa-tions via global context and multiple word prototypes.In Association for Computational Linguistics (ACL),pages 873?882.Philip Carter Ken Russell.
2002.
The Times Book of IQTests.
Kogan Page Limited.Nate Kushmany, Yoav Artziz, Luke Zettlemoyerz, andRegina Barzilayy.
2014.
Learning to automaticallysolve algebra word problems.
In Proceedings of ACL.Minh-Thang Luong, Richard Socher, and Christopher DManning.
2013.
Better word representations withrecursive neural networks for morphology.
CoNLL-2013, 104.Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Cor-rado, and Jeff Dean.
2013.
Distributed representa-tions of words and phrases and their compositionality.In NIPS, pages 3111?3119.Arvind Neelakantan, Jeevan Shankar, Alexandre Pas-sos, and Andrew McCallum.
2014.
Efficient non-parametric estimation of multiple embeddings perword in vector space.
In Proceedings of EMNLP,pages 1059?1069, Doha, Qatar, October.
Associationfor Computational Linguistics.Dan Pape.
1993.
The Original Cambridge Self ScoringIQ Test.
The Magni Group, Inc.Jeffrey Pennington, Richard Socher, and Christopher DManning.
2014.
Glove: Global vectors for word rep-resentation.
Proceedings of EMNLP, 12:1532?1543.Bala?zs Pinte?r, Gyula Vo?ro?s, Zolta?n Szabo?, and Andra?sLo?rincz.
2012.
Automated word puzzle generationvia topic dictionaries.
CoRR, abs/1206.0377.Pritika Sanghi and David Dowe.
2003.
A computer pro-gram capable of passing i.q.
tests.
In Proceedings ofthe Joint International Conference on Cognitive Sci-ence.Min Joon Seo, Hannaneh Hajishirzi, Ali Farhadi, andOren Etzioni.
2014.
Diagram understanding in geom-etry questions.
In Proceedings of the Twenty-EighthAAAI Conference on Artificial Intelligence, July 27 -31, 2014, Que?bec City, Que?bec, Canada., pages 2831?2838.Richard Socher, Danqi Chen, Christopher D Manning,and Andrew Ng.
2013.
Reasoning with neural tensornetworks for knowledge base completion.
In NIPS,pages 926?934.William Stern.
1914.
The Psychological Methods ofTesting Intelligence.
Warwick & York.Claes Strannegard, Mehrdad Amirghasemi, and SimonUlfsbacker.
2012.
An anthropomorphic method fornumber sequence problems.
Cognitive Systems Re-search.Fei Tian, Hanjun Dai, Jiang Bian, Bin Gao, Rui Zhang,Enhong Chen, and Tie-Yan Liu.
2014.
A probabilisticmodel for learning multi-prototype word embeddings.In Proceedings of COLING.Peter D Turney.
2008.
A uniform approach to analogies,synonyms, antonyms, and associations.
In Proceed-ings of the Coling 2008, pages 905?912.Peter D Turney.
2011.
Analogy perception appliedto seven tests of word comprehension.
Journal ofExperimental & Theoretical Artificial Intelligence,23(3):343?362.David Wechsler.
2008.
Wechsler adult intelligencescale?fourth edition (wais?iv).
San Antonio, TX: NCSPearson.Jason Weston, Antoine Bordes, Oksana Yakhnenko, andNicolas Usunier.
2013a.
Connecting language andknowledge bases with embedding models for relationextraction.
arXiv preprint arXiv:1307.7973.Jason Weston, Antoine Bordes, Oksana Yakhnenko, andNicolas Usunier.
2013b.
Connecting language andknowledge bases with embedding models for relationextraction.
In Proceedings of EMNLP.Jason Weston, Antoine Bordes, Sumit Chopra, andTomas Mikolov.
2015.
Towards ai-complete ques-tion answering: a set of prerequisite toy tasks.
arXivpreprint arXiv:1502.05698.550
