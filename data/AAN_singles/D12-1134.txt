Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational NaturalLanguage Learning, pages 1466?1477, Jeju Island, Korea, 12?14 July 2012. c?2012 Association for Computational LinguisticsIdentifying Event-related Bursts via Social Media ActivitiesWayne Xin Zhao?, Baihan Shu?, Jing Jiang?, Yang Song?, Hongfei Yan??
and Xiaoming Li?
?School of Electronics Engineering and Computer Science, Peking University?School of Information Systems, Singapore Management University{batmanfly,baihan.shu,yhf1029}@gmail.com,ysong@pku.edu.cnjingjiang@smu.edu.sg, lxm@pku.edu.cnAbstractActivities on social media increase at a dra-matic rate.
When an external event happens,there is a surge in the degree of activities re-lated to the event.
These activities may betemporally correlated with one another, butthey may also capture different aspects of anevent and therefore exhibit different burstypatterns.
In this paper, we propose to iden-tify event-related bursts via social media activ-ities.
We study how to correlate multiple typesof activities to derive a global bursty pattern.To model smoothness of one state sequence,we propose a novel function which can cap-ture the state context.
The experiments on alarge Twitter dataset shows our methods arevery effective.1 IntroductionOnline social networks (e.g., Twitter, Facebook,Myspace) significantly influence the way we live.Activities on social media increase at a dramaticrate.
Millions of users engage in a diverse rangeof routine activities on social media such as postingblog messages, images, videos or status messages,as well as interacting with items generated by oth-ers such as forwarding messages.
When an eventinteresting to a certain group of individuals takesplace, there is usually a surge in the degree of ac-tivities related to the event (e.g., a sudden explosionof tweets).
Since social media activities may indi-cate the happenings of external events, can we lever-age on the rich social media activities to help iden-tify meaningful external events?
This is the researchproblem we study in this paper.
By external events,we refer to real-world events that happen external tothe online space.
?Corresponding author.2 4 6 8 10020406080100120140Time indexall?tweetsretweetsurl?embeddedtweetsNoise(a) Query=?Amazon.
?2 4 6 8 10050100150Time indexall?tweetsretweetsurl?embeddedtweetsNoise(b) Query=?Eclipse?.Figure 1: The amount of activities within a 10-hourwindow for two queries.
Three types of activitiesare considered: (1) posting a tweet (upward triangle),(2) retweet (downward triangle), (3) posting a URL-embedded tweet (excluding retweet) (filled circle).
Asexplained in Table 1, both bursts above are noisy.Mining events from text streams is usuallyachieved by detecting bursty patterns (Swan and Al-lan, 2000; Kleinberg, 2003; Fung et al 2005).
How-ever, previous work has mostly focused on tradi-tional text streams such as scientific publications andnews articles.
There is still a lack of systematic in-vestigations into the problem of identifying event-related bursty patterns via social media activities.There are at least two basic characteristics of socialmedia that make the problem more interesting andchallenging.First, social media involve various types of activ-ities taking place in real time.
These activities maybe temporally correlated with one another, but theymay also capture different aspects of an event andtherefore exhibit different bursty patterns.
Most ofprevious methods (Swan and Allan, 2000; Klein-berg, 2003; Fung et al 2005) deal with a single typeof textual activities.
When applied to social media,they oversimplify the complex nature of online so-1466Bursty Activity Time # in Sr # in Su # in St Noisy?Sr,St 23:00?23:59, Nov. 23, 2009 108 5 147 YSee Fig.
1(b) [Query=eclipse] major bursty reason: The tweet from Robert Pattinson ?
@twilight: from rob cont .- i hope you are looking forward to eclipse as much as i am .?
has been retweeted many times.Su,St 07:00?07:59, Jul.
25, 2009 6 122 133 YSee Fig.
1(a) [Query=Amazon] major bursty reason: Advertisement tweets like ?
@fitnessjunkies amazon.com deals: http://tinyurl.com/lakz3h.?
have been posted many times.St,Su,Sr 09:00?9:59, Oct. 9, 2009 1562 423 2848 N[Query=Nobel] major bursty reason: The news ?Obama won Nobel Peace Prize?
flood Twitter.Table 1: Examples of bursts.
The first two bursts are judged as noise since they do not correspond to any meaningfulexternal events.
In fact, the reasons why a burst appears in social media can be quite diverse.
In this paper, we onlyfocus on event-related bursts.
St denotes posting a tweet, Su denotes posting a url-embedded tweet, and Sr denotesretweet.cial activities, and therefore they may not be wellsuitable to social media.
Let us consider a moti-vating example.
Figure 1 shows the change of theamount of activities of three different types over a10-hour time window for two queries.
If we consideronly the total number of tweets, we can see that forboth queries there is a burst.
However, neither of thetwo bursts corresponds to a real-world event.
Thefirst burst was caused by the broadcast of an adver-tisement from several Twitter bots, and the secondburst was caused by numerous retweets of a statusupdate of a movie star1.
The detailed explanationsof why the two bursts are noisy are also shown inTable 1.
On the other hand, interestingly, we can seethat not all the activity streams display noisy burstypatterns at the same time.
It indicates that we maymake use of multiple views of different activitiesto detect event-related bursts.
The intuition is thatusing multiple types of activities may help learn abetter global picture of event-related bursty patterns.Learning may also be more resistant to noisy bursts.Second, in social media, burst detection is chal-lenged by irregular, unpredictable and spuriousnoisy bursts.
To overcome this challenge, a reason-able assumption is that a burst corresponding to areal event should not fluctuate too much within arelatively short time window.
To illustrate it, wepresent an example in Figure 2, in which we firstuse a simple threshold method to detect bursts andthen analyze the effect of local smoothness.
In par-ticular, if the amount of activities at a certain timeis above a pre-defined threshold, we set its state to1, which indicates a bursty state.
Otherwise, we setthe state to 0.
Figure 2(a) shows that for the query?Eclipse,?
with a threshold of 50, the state sequencefor the time window we consider is ?0000100000.
?1The reasons for these bursts were revealed by manuallychecking the tweets during the corresponding periods.1 2 3 4 5 6 7 8 9 10050100150 Correct0000000000Threshold0000100000(a) Query=?Eclipse?.1 2 3 4 5 6 7 8 9 10050010001500200025003000 Correct0000011111Threshold0000011111(b) Query=?Nobel?.Figure 2: Analysis of the effect of local smoothness onthreshold method.
It shows two examples of thresholdmethods for burst detection in a 10-hour window.
Thered line denotes the bursty threshold.
If the number ofactivities is above the threshold in one time interval, thestate of this time interval is judge as bursty.
Detailed de-scriptions of these cases are shown in Table 1.Although there is a burst in this sequence, its dura-tion is very short.
In fact, this is the first exampleshown in Table 1, which is a noisy burst.
In con-trast, in Figure 2(b), the state sequence for the query?Nobel?
is ?0000011111,?
in which the longer andsmoother burst corresponds to a true event.
A goodfunction for evaluating the smoothness of a state se-quence should be able to discriminate these casesand model the context of state sequences effectively.With its unique characteristics and challenges,there is an emergent need to deeply study the prob-lem of event-related burst detection via social me-dia activities.
In this paper, we conduct a system-atic investigation on this problem.
We formulatethis problem as burst detection from time series ofsocial media activities.
We develop an optimiza-tion model to learn bursty patterns based on multipletypes of activities.
We propose to detect bursts byconsidering both local state smoothness and correla-tion across multiple streams.
We define a function to1467quantitatively measure local smoothness of one sin-gle state sequence.
We systematically evaluate threetypes of activities for burst detection on a large Twit-ter dataset and analyze different properties of thesethree streams for burst detection.2 Problem DefinitionBefore formally introducing our problems, we firstdefine some basic concepts.Activity: An activity refers to some type of actionthat users perform when they are interested in sometopic or event.Activity Stream: An activity stream of lengthN and type m is a sequence of numbers(nm1 , nm2 , ..., nmN ), where each nmi denotes theamount of activities of type m that occur during theith time interval.Query: A queryQ is a sequence of terms q1, ..., q|Q|which can represent the information needs of users.For example, an example query related to PresidentObama is ?barack obama.
?Event-related Burst: Given a query Q, an event-related burst is defined as a period [ts, te] in whichsome event related with Q takes place, where ts andte are the start timestamp and end timestamp of theevent period respectively.
During the event periodthe amount of activities is significantly higher thanaverage.Based on these definitions, our task is to try toidentify event-related bursts via multiple social me-dia activity streams.3 Identifying Event-related Bursts fromSocial MediaIn this section, we discuss how to identify event-related bursts via social media activities.
For-mally, given a query Q, we first build M ac-tivity streams related with Q on T timestamps:{(nm1 , ..., nmT )}Mm=1.
The definition of activity in ourmethods is very general; it includes various types ofsocial media activities, including textual and non-textual activities, e.g., a click on a shared photo anda link formation between two users.Given the input, we try to infer a state sequenceover these T timestamps: z = (z1, ..., zT ), wherezi is 1 or 0.
1 indicates a time point within a burstwhile 0 indicates a non-bursty time point.3.1 Modeling a Single Activity Stream3.1.1 Generation functionIn probability theory and statistics, the Poissondistribution2 is a discrete probability distributionthat can measure the probability of a given numberof ?activities?
occurring in a fixed time interval.
Weuse the Poisson distribution to study the probabilityof observing the number of social media activities,and we treat one hour as one time interval in thispaper.Homogeneous Poisson Distribution The genera-tive probability of the ith number in one activitystream of type m is defined as f(nmi , i, zmi ) =(?zmi)nmi exp(?
?zmi)nmi !, where ?0 is the (normal) expec-tation of the number of activities in one time inter-val.
If one state is bursty, it would emit activitieswith a faster rate and result in a larger expectation?1.
We can set ?1 = ?0 ?
?, where ?
> 1.Heterogeneous Poisson Distribution The two-statemachine in (Kleinberg, 2003) used two global refer-ences for all the time intervals: one for bursty andthe other for non-bursty.
In our experiments, we ob-serve temporal patterns of user behaviors, i.e., activ-ities in some hours are significantly more than thosein the others.
Instead of using fixed global rates ?0and ?1, we try to model temporal patterns of userbehaviors by parameterizing ?(?)
with the time in-dex.
By following (Ihler et al 2006), we use aset of hour-specific rates {?1,h}24h=1 and {?0,h}24h=1.3Given a time index h, we set ?0,h to be the expecta-tion of the number of activities in hth time intervalevery day, then we have ?1,h = ?0,h ?
?.
In thispaper, ?
is empirally set as 1.5.3.1.2 Smoothness of a State SequenceFor burst detection, the major aim is to identifysteady and meaningful bursts and to discard tran-sient and spurious bursts.
Given a state sequencez1z2...zT , to quantitatively measure the smoothnessand compactness of it, we introduce some measures.One simple method is to count the number ofchange in the state sequence.
Formally, we use thefollowing formula:g1(z) = T ?T?1?i=1I(zi 6= zi+1), (1)2http://en.wikipedia.org/wiki/Poisson distribution3We can also make the rates both day-specific and hour-specific, i.e., {?(?),d,h}h?{1,...,24},d?
{1,...,7}.1468where T is length of the state sequence and I(?
)is an indicator function which returns 1 only if thestatement is true.
Let us take the state sequence?0000100000?
(shown in Figure 2(a)) as an exampleto see how g1 works.
State changes 0pos=4 ?
1pos=5and 1pos=5 ?
0pos=6 each incur a cost of 1, there-fore g1(0000100000) = 10 ?
2 = 8.
Similarly, wecan get g1(0000000000) = 10.
There is a cost dif-ference between these two sequences, i.e., ?g1 = 2.Kleinberg (2003) uses state transition probabilitiesto model the smoothness of state sequences.
Withsimple derivations, we can show that Kleinberg?smodel essentially also uses a cost function that islinear in terms of the number of state changes in asequence, and therefore similar to g1.In social media, very short noisy bursts like?0000100000?
are very frequent.
To discard suchnoises, we may multiply g1 by a big cost factor topunish short-term fluctuations.
However, it is notsensitive to the state context4 and may affect thedetection of meaningful bursts.
For example, statechange 0pos=4 ?
1pos=5 in ?0000111100?
wouldreceive the same cost as that of 0pos=4 ?
1pos=5in ?0000100000?
although the later is more like anoise.To better measure the smoothness of a state se-quence , we propose a novel context-sensitive func-tion, which sums the square of the length of the max-imum subsequences in which all states are the same.Formally, we haveg2(z1, z2, ..., zT ) =?si<ei(ei ?
si + 1)2, (2)where si and ei are the start index and end in-dex of the ith subsequence respectively.
To define?maximum?, we have the constraints zsi = zsi+1 =... = zei , zsi?1 6= zsi , zei 6= zei+1.
For example,g2(0000000000)= 102 = 100, g2(0000100000)=42 + 12 + 52 = 42, we can see that ?g2 =100 ?
42 = 58, which is significantly larger than?g1(= 2).
g2 rewards the continunity of state se-quences while punish the fluctuating changes, and itis context-sensitive.
State change 0pos=4 ?
1pos=5in ?0000111100?
receives a cost of 4,5 which is4Here context refers to the window of hidden state se-quences.5Indeed, g2 is not designed for a single state change but forthe overall smoothness patterns, so we choose a referring se-quence generated by making the corresponding state negative tocompute the cost, i.e., |g2(0000011110)?g2(0000001110)| =4.much smaller than that of 0pos=4 ?
1pos=5 in?0000100000?.
g2 is also sensitive to the po-sition of state changes, e.g., g2(0000100000) 6=g2(0100000000).3.2 Burst Detection from a Single ActivityStreamGiven an activity stream (nm1 , ..., nmT ), we wouldlike to infer a state sequence over these T times-tamps, i.e., to find out the most possible state se-quence z = (zm1 , ..., zmT ) based on the data, wherezmi = 1 or 0.
We formulate this problem asan optimization problem.
The cost of a state se-quence includes two parts: generation of activitiesand smoothness of the state sequence.
The objectivefunction is to find a state sequence which incurs theminimum cost.
Formally, we define the total costfunction asCost(z) = ?T?i=1log f(nmi , i, zmi )?
??
?generating cost+(?
?
(zm1 , ..., zmT ) ?
?1)?
??
?smoothness cost,(3)where ?1 > 0 is a scaling factor which balance thesetwo parts.
?(?)
function is the smoothness function,and we can set it as either g1(?)
or g2(?
).To seek the optimal state sequence, we can min-imize Equation 3.
However, exact inference is harddue to the exponential search space.
Instead of ex-amining the smoothness of the whole state sequence,we propose to measure the smoothness of all the L-length subsequences, so called ?local smoothness?.The assumption is that the states in a relatively shorttime window should not change too much.
The newobjective function is defined asCost(z) = ?T?i=1log f(nmi , i, zmi ) (4)?(?i?L?
(zmi , ..., zmi+L?1))?
?1.The objective function in Equation 4 can besolved efficiently by a dynamic programming algo-rithm shown in Algorithm 1.
The time complexityof this algorithm is O(T ?
2L).
Note that the meth-ods we present in Equation 4 and Algorithm 1 arequite general.
They are independent of the concreteforms of f(?)
and ?(?
), which leaves room for flexi-ble adaptation or extension in specific tasks.
In pre-vious methods (Kleinberg, 2003), L is often fixed as14692.
Indeed, as shown in Figure 2, in some cases, wemay need a longer window to infer the global pat-terns.
In our model, L can be tuned based on realdatasets.
We can seek a trade-off between efficiencyand length of context windows.Algorithm 1: Dynamic Programming for Equation 4.d[i][s][zi...zi?L+1] denotes the minimum cost of the first1i timestamps with the state subsequence: zi...zi?L+1 andzi = s;set d[0][?][?]
= 0;2set c1[i] = log f(nmi , i, zmi );3set c2[i] = ?
(zi, ..., zi?L+1);4b, b?
: previous and current state window are represented as5L-bit binary numbers;for i = 1 to T do6for s = 0 to 1 do7for b = 0 to 2L ?
1 do8b?
= (b << 1|s)&(1 << L?
1);9d[i][s][b?]??10min(d[i][s][b?
], d[i?1][s][b]+c1[i]+c2[i]);end11end12end133.3 Correlating Multiple Activity StreamsIn this section, we discuss how to correlate multi-ple activity streams to learn a global bursty patterns.The hidden state sequences corresponding to theseactivity streams are not fully independent.
An ex-ternal event may intricate surges in multiple activitystreams simultaneously.We propose to correlate multiple activity streamsin an optimization model.
The idea is that activ-ity streams related with one query might be depen-dent, i.e., the states of multiple activity streams onthe same timestamp tend to be the same6; if not,it would incur a cost.
To implement this idea, wedevelop an optimization model.
For convenience,we call the states of each activity stream as ?localstates?
while the overall states learnt from multipleactivity streams as ?global states?.The idea is that although various activity streamsare different in the scale of frequencies, they tend toshare similar trend patterns.
We incorporate the cor-relation between local states on the same timestamp.6In our experiments, we compute the cross correlation be-tween different streams with a lag factor ?, we find the crosscorrelation achieves maximum consistantly when ?
= 0.Formally, we haveCost(Z) =M?m=1{?T?i=1log f(nmi , i, zmi )??i?L?
(zmi , ..., zmi+L?1) ?
?1}+T?i=1?m1,m2I(zm1i 6= zm2i ) ?
?2, (5)where I(?)
is indicator function, and ?2 is the costwhen a pair of states are different across multiplestreams on the same timestamp.The objective function in Equation 5 can besolved by a dynamic programming algorithm pre-sented in Algorithm 2.
The time complexity of thisalgorithm is O(T ?
2M ?L+M ).
Generally, L can beset as one small value, e.g., L =2 to 6, and we canselect just a few representative activity streams, i.e.,M =2 to 6.
In this case, the algorithm can be effi-cient.Algorithm 2: Dynamic Programming for Equation 5.d[i][z1i ...z1i?L+1; ...; zMi ...zMi?L+1] denotes the minimum1cost of the first i timestamps with the local statesubsequence zmi ...zmi?L+1 in the mth stream;set d[0][...] = 0;2bl, bl?
: previous and current state windows represented as3M ?
L-bit binary numbers;c[i, bl, bl?]
denotes all the cost in the tth timestamp;4for i = 1 to T do5for bl = 0 to 2M?L ?
1 do6deriving current local state sequences bl?from bl;7d[i][b?l]??8min(d[i][bl?
], d[i?
1][bl] + c[i, bl, bl?
]);end9end10Given M types of activity streams, we can getM (local) state sequences {(zm1 , ..., zmT )}Mm=1.
Thenext question is how to learn a global state sequence(zG1 , ..., zGT ) based on local state sequences.
Here wegive a few options:CONJUNCT: we set a global state zi as bursty ifall local states are bursty, i.e., zGi = ?Mm=1zmi .DISJUNCT: we set a global state zi as bursty ifone of the local states is bursty, i.e., zGi = ?Mm=1zmi .BELIEF: we set a global state zi as the most con-fident local state, i.e., zGi = argmaxmbelief(zmi ).The belief(?)
function can be defined as the ratio be-tween generating costs from states zmi and 1 ?
zmi :belief(zmi ) =f(nmi ,i,zmi )f(nmi ,i,1?zmi ).1470Table 2: Basic statistics of our golden test collection.# of queries 17Aver.
# of event-related bursts per query 19Min.
bursty interval 3 hoursMax.
bursty interval 163 hoursAver.
bursty interval 17.8 hoursL2G: we treat the states of one local stream as theglobal states.4 Experiments4.1 Construction of Test CollectionWe test our algorithms on a large Twitter dataset,which contains about 200 million tweets and rangesfrom July, 2009 to December 2009.
We manuallyconstructed a list of 17 queries that have high vol-umes of relevant tweets during this period.
Thesequeries have a very broad coverage of topics.
Exam-ple queries are ?Barack Obama?, ?Apple?, ?Earth-quake?, ?F1?
and ?Nobel Prize?.
For each query, weinvite two senior graduate students to manually iden-tify their golden bursty intervals, and each bursty in-terval is represented as a pair of timestamps in termsof hours.
Specifically, to generate the golden stan-dard, given a query, the judges first manually gen-erate a candidate list of external events7; then foreach event, they look into the tweets within the cor-responding period and check whether there is a surgeon the frequency of tweets.
If so, the judges fur-ther determine the start timepoint and end timepointof it.
If there is a conflict, a third judge will makethe final decision.
We used Cohen?s kappa coeffi-cient to measure the agreement of between the firsttwo judges, which turned out to be 0.67, indicating agood level of agreement8.
We present basic statisticsof the test collection in Table 2.4.2 Evaluation MetricsBefore introducing our evaluation metrics, we firstdefine the Bursty Interval Overlap Ratio (BIOR)BIOR(f,X ) =?f ?
?X ?l(f, f?
)L(f),f is a bursty interval, ?l(f, f ?)
is the length ofoverlap between f ?
and f , L(f) is the length of7We refer to some gold news resources, e.g., Google Newsand Yahoo!
News.8http://en.wikipedia.org/wiki/Cohen?s kappaFigure 3: Examples to illustrate BIOR.
X0, X1and X2 are three sets of bursty intervals.
X0and X2 consist of one interval, and X1 consists oftwo intervals.
BIOR(f,X0)=1, BIOR(f,X1)=0.5 andBIOR(f,X2)=0.5.bursty period of f .
X is a set of bursty intervals,BIOR measures the proportion of the timestamps inf which are covered by one of bursty intervals inX .
We use BIOR to measure partial match of inter-vals, because a system may not return all the exactbursty intervals9.
We show some examples of BIORin Figure 3.We use modified Precision, Recall and F as ba-sic measures.
Given one query, P, R and F can bedefined as followsR =?f?B I(1|Mf |BIOR(f,M) > 0.5)|B|,P =1|M|?f ?
?M(BIOR(f ?,B)),F =2?
P ?RP + R,where M is the set of bursty intervals identifiedby one candidate method, B is the set of bursty in-tervals in golden standards, and Mf is the set of in-tervals which overlap with f in M. We incorporatethe factor 1|Mf | in Recall to penalize the incontin-uous coverage of the golden interval, and we alsorequire that the overlap ratio with penalized factoris higher than a threshold of 0.5.
Given two sets ofbursty intervals which have the same value of BIOR,we prefer the one with fewer intervals.
In Figure 3,we can easily derive X1 and X2 have the same value9A simple evaluation method is that we label each one hourtime slot as being part of a burst or not and compare with thegold standard.
However, in our experiments, we find that somemethods tend to break one meaningful burst into small parts andeasier to be affected by small fluctuations although they mayhave a good coverage of bursty points.
This is why we adopt adifferent evaluation approach.1471Table 3: Average cross-correlation between differentstreams.St Sr SuSt 1 0.830235 0.851514Sr 0.830235 1 0.59905Su 0.851514 0.59905 1of BIOR, when computing Recall, we prefer X2 toX1 since X2 consists of only one complete inter-val whileX1 consists of two inconsecutive intervals.I(?)
is an indicator function which returns 1 only ifthe statement if true.
In our experiments, we use theaverage of R, P and F over all test queries.4.3 Experiment SetupSelecting activity streamsWe consider three types of activity streams inTwitter: 1) posting a tweet, denoted as St; 2) for-warding a tweet (retweet), denoted as Sr; 3) post-ing a URL-embedded tweet, denoted as Su.
It isnatural to test the performance of St in discover-ing bursty patterns, while Su and Sr measure theinfluence of external events on users in Twitter intwo different aspects.
Sr: An important conventionin Twitter is the ?retweeting?
mechanism, throughwhich users can actively spread the news or relatedinformation; Su: Another characteristic of Twitter isthat the length of tweets is limited to 140 characters,which constrains the capacity of information.
Usersoften embed a URL link in the tweets to help othersknow more about the corresponding information.We compute the average cross correlation be-tween different activity streams for these 17 queriesin our test collection, and we summarize the resultsin Table 3.
We can see that both Sr and Su have ahigh correlation with St, and Sr has a relatively lowcorrelation with Su.
10Methods for comparisonsS(?
): using Equation 4 and considers a single ac-tivity stream, namely St, Su and Sr.MBurst(?
): using Equation 5 and considers mul-tiple activity streams.To compare our methods with previous methods,we adopt the following baselines:StateMachine: This is the method proposedin (Kleinberg, 2003).
We use heterogeneous Poisson10We also consider the frequencies of unique users by hours,however, we find it has a extremely high correlation coefficientwith St, about 0.99, so we do not incorporate it.function as generating functions instead of binomialfunction Cnk because sometimes it is difficult to getthe exact total number n in social media.Threshold: If we find that the count in one timeinterval is higher than a predefined threshold, it istreated as a burst.
The threshold is set as 1.5 timesof the average number.PeakFinding: This is the method proposedin (Marcus et al 2011), which aims to automaticallydiscover peaks from tweets.Binomial: This is the method proposed in (Fung etal., 2007a), which uses a cumulative binomial distri-bution with a base probability estimated by remov-ing abnormal frequencies.As for multiple-stream burst detection, to the bestof our knowledge, the only existing work is pro-posed by (Yao et al 2010), which is supervised andrequires a considerable amount of training time, sowe do not compare our work with it.
We compareour method with the following heuristic baselines:SimpleConjunct: we first find the optimal state se-quences for each single activity stream.
We then de-rive a global state sequence by taking the conjunc-tion of all local states.SimpleDisjunct: we first find the optimal state se-quences for each single activity stream, and then wederive a global state sequence by take the disjunctionof all local states.Another possible baseline is that we first mergeall the activities, then apply the single-stream algo-rithm.
However, in our data set, we find that thenumber of activities in St is significantly larger thanthat of the two types.
St dominantly determines thefinal performance, so we do not incorporate it hereas a comparison.4.4 Experimental ResultsPreliminary results on a single streamWe first examine the performance of our proposedmethod on a single stream.
Note that, our methodin Equation 4 has two merits: 1) the length of lo-cal window can be tuned on different datasets; 2) anovel state smoothness function is adopted.We set the ?
function in Equation 4 respectivelyas g1 and g2, and apply our proposed methods tothree streams (St,Sr,Su) mentioned above.
Notethat, when L = 2 and ?
= g1, our method becomesthe algorithm in (Kleinberg, 2003).
We tune the pa-rameter ?1 in Equation 4 from 2 to 20 with a step of2.
We record the best F performance and compute1472the corresponding standard deviation.
In Table 5, wecan observe that 1) streams St and Sr perform betterthan Su; 2) the length of local window significantlyaffects the performance; 3) g2 is much better than g1in our proposed burst detection algorithm; 4) gen-erally speaking, a longer window size (L = 3, 4)performs better than the most common used size 2in (Kleinberg, 2003).We can see that our proposed method is more ef-fective than the other baselines.
The major reason isthat none of these methods consider state smooth-ness in a systematic way.
In our preliminary ex-periments, we find that these baselines usually out-put a lot of bursts, most of which are broken mean-ingful bursts.
To overcome this, baseline methodStateMachine (g1 + L = 2) requires larger ?
and?1, which may discard relatively small meaningfulbursts; while our proposed single stream method(g2 + L = 3, 4) tends to identify steady and con-secutive bursts through the help of longer contextwindow and context sensitive smoothness functiong2, it is more suitable to be applied to social mediafor burst detection.Compared with the other baselines, (Kleinberg,2003) is still one good and robust baseline since itmodels the state smoothness partially.
These prelim-inary findings indicate that state smoothness is veryimportant for burst detection, and the length of statecontext window will affect the performance signifi-cantly.To get a deep analysis of the performance of dif-ferent streams, we set up three classes, and eachclass corresponds to a single stream.
Since for eachquery, we can obtain multiple results in different ac-tivity streams, we further categorize the 17 anno-tated queries to the stream which leads to the opti-mal performance on that query.
Interestingly, we cansee: 1) the url stream gives better performance onqueries about big companies because users in Twit-ter usually talk about the release of new productsor important evolutionary news via url-embeddedtweets; 2) the retweet stream gives better perfor-mance on queries which correspond to unexpectedor significant events, e.g., diasters.
It is consistentwith our intuitions that users in Twitter do activelyspread such information.
Combining previous anal-ysis of Table 5, overall we find the retweet stream ismore capable to identify bursts which correspond tosignificant events.Table 4: Categorization of 17 queries according to theoptimal performance.Streams Queriesurl Apple,Microsoft,Nokia, climateretweet bomb,crash,earthquake,typhoon,F1,Google,Olympicsall tweet Amazon, eclipse, Lakers,NASA, Nobel Prize, Barack ObamaTable 5: Performance (average F) on a single stream.????
indicates that the improvement our proposed single-stream methodg2,L=4 over all the other baselines is ac-cepted at the confidence level of 0.95, i.e., StateMachine,PeakingFinding, Binomial and Threshold.?
L St Sr Su4 0.545/0.015 0.543/0.037 0.451/0.036g2 3 0.536/0.013 0.549?
?/0.019 0.464/0.0252 0.468/0.055 0.542/0.071 0.455/0.0454 0.513/0.059 0.546/0.058 0.465/0.047g1 3 0.469/0.055 0.542/0.071 0.455/0.0452 0.396/0.043 0.489/0.074 0.374/0.035StateMachine 0.396 0.489 0.374PeakFinding 0.410 0.356 0.302Binomial 0.315 0.420 0.341Threshold 0.195 0.181 0.175Preliminary results on multiple streamsAfter examining the basic results on a singlestream, we continue to evaluate the performance ofour proposed models on multiple activity streams.For MBurst in Equation 5, we have three parame-ters to set, namely L, ?1 and ?2.
We do a grid searchfor both ?1 and ?2 from 1 to 12 with a step of 1, andwe also examine the performance when L = 2, 3, 4.We can see that MBurst has four candidate meth-ods to derive global states from local states; for L2G,we use the states of St as the final states, and we em-pirically find that it performs best compared with theother two streams in L2G.Recall that our proposed single-stream methodis better than all the other single-stream baselines,so here single-best denotes our method in Equa-tion 4 (?
= g2, L = 4) on Sr. For SimpleConjunctand SimpleDisjunct, we first find the optimal statesequences for each single activity stream using ourproposed method in Equation 4 (?
= g2, L = 4),and then we derive a global state sequence by takethe conjunction or disjunction of all local states re-spectively.Besides the best performance, we further computethe average of the top 10 results of each methodby tuning parameters to check the average perfor-1473Table 6: Performance (average F) on multiple streams.???
indicates that the improvement our proposedmultiple-stream method over our proposed single-streammethod at the confidence level of 0.9 in terms of averageperformance.Methods best averagesingle-best (g2 + Sr) 0.549 0.526SimpleConjunct 0.548 -SimpleDisjunct 0.465 -MBurst+CONJUNCTr,t,u 0.555 0.548MBurst+DISJUNCTr,t,u 0.576 0.570?MBurst+BELIEFr,t,u 0.568 0.561MBurst+L2Gr,t,u(t) 0.574 0.567MBurst+L2Gr,t,u(r) 0.560 0.558mance.
The average performance can show the sta-bility of models in some degree.
If one model out-puts the maximum in a very limited set of parame-ters, it may not work well in real data, especially insocial media.In Table 6, we can seeMBurst+DISJUNCTr,t,ugives the best performance.
MBurst performsconsistently better than single-best which is a verystrong single-stream method, especially for averageperformance.
MBurst+DISJUNCTr,t,u has an im-provement of average performance over single-bestby 8.4%.
And simply combining three differentstreams may hit results (SimpleConjunct and Sim-pleDisjunct).
It indicates that MBurst is more sta-ble and shows a higher performance.For different methods to derive global bursty pat-terns, we can see that MBurst+DISJUNCT per-forms best while MBurst+CONJUNCT performsworst.
Interestingly, however, SimpleConjunct isbetter than SimpleDisjunct, the major reason is thatMBurst performs a local-state correlation of mul-tiple activity streams to correct possible noisy fluc-tuations from single streams before the conjunctionor disjunction of local states.
After such correlation,the performance of each activity stream should im-prove.
To see this, we present the optimal results of asingle stream without/with local-state correlation inTable 7.
Local-state correlation significantly booststhe performance of a single stream.
Indeed, we findthat the step of local-state correlation is more impor-tant for our multiple stream algorithm than the stepof how to derive global states based on local states.We test our MBurst algorithm with the setting:T = 4416, L = 4 and M = 3, and for all the testTable 7: Comparison between the optimal results of asingle stream with/without local-state correlation.all retweet retweet urlwithout 0.536 0.549 0.464with 0.574 0.560 0.5472 4 6 8 10 120.540.550.560.570.580.590.6?1AverageFMBurst+orsingle?best(a) ?2 = 4, varying ?1.2 4 6 8 10 120.540.550.560.570.580.590.6?2AverageFMBurst+orsingle?best(b) ?1 = 11, varying ?2.Figure 4: Parameter sensitivity of MBurst + DIS-JUNCT.queries, our algorithm can respond in 2 seconds 11,which is efficient to be deployed in social media.Parameter sensitivityWe have shown the performance of different pa-rameter settings for single stream algorithm in Ta-ble 5.
Next, we check parameter sensitivity inMBurst.
In our experiments, we find a longer lo-cal window (L = 3, 4) is better than L = 2, sowe first set L = 4, then we select parameter set-tings of ?2 = 4 and ?1 = 11, which give best per-formance for MBurst+DISJUNCT.
We vary onewith the other fixed to see how one single parame-ter affects the performance.
The results are shown inFigure 4, and we can see MBurst+DISJUNCT isconsistently better than single-best.5 Related WorkOur work is related to burst detection from textstreams.
Pioneered by the automaton model pro-posed in (Kleinberg, 2003), many techniques havebeen proposed for burst detection such as the ?2-test based method (Swan and Allan, 2000), theparameter-free method (Fung et al 2005) and mov-ing average method (Vlachos et al 2004).
Our workis related to the applications of these burst detectionalgorithms for event detection (He et al 2007; Funget al 2007b; Shan et al 2012; Zhao et al 2012).11All experiments are tested in a Mac PC, 2.4GHz Intel Core2 Duo.1474Some recent work try to identify hot trends (Math-ioudakis and Koudas, 2010; Zubiaga et al 2011;Budak et al 2011; Naaman et al 2011) or makeuse of the burstiness (Sakaki et al 2010; Aramkiet al 2011; Marcus et al 2011) in social media.However, few of these methods consider modelingthe local smoothness of one state sequence in a sys-tematic way and often use a fixed window length of2.Little work considers making use of differenttypes of social media activities for burst detection.
(Yao et al 2010; Kotov et al 2011; Wang et al2007; Wang et al 2009) conducted some prelim-inary studies of mining correlated bursty patternsfrom multiple sources.
However, they either highlyrelies on high-quality training datasets or require aconsiderable amount of training time.
Online socialactivities are dynamic, with a large number of newitems generated continuously.
In such a dynamicsetting, burst detection algorithms should effectivelycollect evidence, efficiently adjust prediction modelsand respond to the users as social media activitiesevolve.
Therefore it is not suitable to deploy suchalgorithms in social media.Our work is also similar to studies which aimto mine and leverage knowledge from social me-dia (Mathioudakis et al 2010; Ruiz et al 2012;Morales et al 2012).
We share the common pointwith these studies that we try to utilize the under-lying rich knowledge in social media, while our fo-cus of this work is quite different from theirs, i.e., toidentify event-related bursts.Another line of related research is Twitter relatedstudies (Kwak et al 2010; Sakaki et al 2010).
Ourproposed methods can provide event-related burstsfor downstream applications.6 ConclusionIn this paper, we propose to identify event-relatedbursts via social media activities.
We propose oneoptimization model to correlate multiple activitystreams to learn the bursty patterns.
To better mea-sure local smoothness of the state sequence, we pro-pose a novel state cost function.
We test our meth-ods in a large Twitter dataset.
The experiment re-sults show that our methods are both effective andefficient.
Our work can provide a preliminary un-derstanding of the correlation between the happen-ings of events and the degree of online social mediaactivities.Finally, we present a few promising directionswhich may potentially improve or enrich currentwork.1) Variable-length context.
In this paper, L is apre-determined parameter which controls the size ofcontext window.
It cannot be modified when the al-gorithm runs.
A large L will significantly increasesthe algorithm complexity, and we may not need alarge L for all the states in a Markov chain.
Thisproblem can be addressed by using the variable-length hidden Markov model (Wang et al 2006),which is able to learn the ?minimum?
context lengthfor accurately determining each state.2) Incorporation of more useful features.
Ourcurrent model mainly considers temporal variationsof streaming data and searches the surge patterns ex-isting in it.
In some cases, simple frequency infor-mation may not be capable to identify all the mean-ingful bursts.
It can be potentially useful to leverageup more features to help filter out noisy bursts, e.g.,semantic information (Zhao et al 2010).3) Modeling multi-modality data.
We have ex-amined our multi-stream algorithm by using threedifferent activity streams.
These streams are textual-based.
It will be interesting to check our algorithm inmulti-modality data streams.
E.g., in Facebook, wemay collect a stream consisting of the daily frequen-cies of photo sharing and another stream consistingof the daily frequencies of text status updates.4) Evaluation of the identified bursts.
In mostof previous work, they seldom construct a gold stan-dard for quantitative test, instead they qualitativelyevaluate their methods.
In our work, we invite hu-man judges to generate the gold standard.
It is time-consuming, and the bias from human judges cannotbe completely eliminated although more judges canbe invited.
A possible evaluation method is to exam-ine the identified bursts in downstream applications,e.g., event detection.AcknowledgementThis work is partially supported by NSFC Grant61073082, 60933004 and 70903008.
Xin Zhao issupported by Google PhD Fellowship (China).
Wethank the insightful comments from Junjie Yao andthe anonymous reviewers.1475ReferencesEiji Aramki, Sachiko Maskawa, and Mizuki Morita.2011.
Twitter catches the flu: Detecting influenza epi-demics using twitter.
In Proceedings of the 2011 Con-ference on Empirical Methods in Natural LanguageProcessing, pages 1568?1576, Edinburgh, Scotland,UK., July.
Association for Computational Linguistics.Ceren Budak, Divyakant Agrawal, and Amr El Abbadi.2011.
Structural trend analysis for online social net-works.
Proc.
VLDB Endow., 4, July.Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Philip S. Yu,and Hongjun Lu.
2005.
Parameter free bursty eventsdetection in text streams.
In VLDB.Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, andPhilip S. Yu.
2007a.
Time-dependent event hierar-chy construction.
In Proceedings of the 13th ACMSIGKDD international conference on Knowledge dis-covery and data mining, KDD ?07.Gabriel Pui Cheong Fung, Jeffrey Xu Yu, Huan Liu, andPhilip S. Yu.
2007b.
Time-dependent event hierarchyconstruction.
In SIGKDD.Qi He, Kuiyu Chang, and Ee-Peng Lim.
2007.
Analyz-ing feature trajectories for event detection.
In SIGIR.Alexander Ihler, Jon Hutchins, and Padhraic Smyth.2006.
Adaptive event detection with time-varyingpoisson processes.
In Proceedings of the 12th ACMSIGKDD international conference on Knowledge dis-covery and data mining, KDD, pages 207?216, NewYork, NY, USA.
ACM.J.
Kleinberg.
2003.
Bursty and hierarchical structure instreams.
Data Mining and Knowledge Discovery.Alexander Kotov, ChengXiang Zhai, and Richard Sproat.2011.
Mining named entities with temporally corre-lated bursts from multilingual web news streams.
InProceedings of the fourth ACM international confer-ence on Web search and data mining, WSDM, pages237?246.Haewoon Kwak, Changhyun Lee, Hosung Park, and SueMoon.
2010.
What is Twitter, a social network or anews media?
In WWW ?10: Proceedings of the 19thinternational conference on World wide web, pages591?600.Adam Marcus, Michael S. Bernstein, Osama Badar,David R. Karger, Samuel Madden, and Robert C.Miller.
2011.
Twitinfo: aggregating and visualizingmicroblogs for event exploration.
In Proceedings ofthe 2011 annual conference on Human factors in com-puting systems, CHI ?11.Michael Mathioudakis and Nick Koudas.
2010.
Twit-termonitor: trend detection over the twitter stream.In Proceedings of the 2010 international conferenceon Management of data, SIGMOD ?10, pages 1155?1158.Michael Mathioudakis, Nick Koudas, and Peter Marbach.2010.
Early online identification of attention gather-ing items in social media.
In Proceedings of the thirdACM international conference on Web search and datamining, WSDM ?10, pages 301?310, New York, NY,USA.
ACM.Gianmarco De Francisci Morales, Aristides Gionis, andClaudio Lucchese.
2012.
From chatter to headlines:harnessing the real-time web for personalized newsrecommendation.
In WSDM, pages 153?162.Mor Naaman, Hila Becker, and Luis Gravano.
2011.
Hipand trendy: Characterizing emerging trends on twitter.JASIST, 62(5):902?918.Eduardo J. Ruiz, Vagelis Hristidis, Carlos Castillo, Aris-tides Gionis, and Alejandro Jaimes.
2012.
Correlat-ing financial time series with micro-blogging activity.pages 513?522.Takeshi Sakaki, Makoto Okazaki, and Yutaka Matsuo.2010.
Earthquake shakes twitter users: real-time eventdetection by social sensors.
WWW, pages 851?860,New York, NY, USA.
ACM.Dongdong Shan, Wayne Xin Zhao, Rishan Chen, ShuBaihan, Hongfei Yan, and Xiaoming Li.
2012.Eventsearch: A system for event discovery and re-trieval on multi-type historical data.
In KDD?12, De-mostration.Russell Swan and James Allan.
2000.
Automatic gener-ation of overview timelines.
In SIGIR.Michail Vlachos, Christopher Meek, Zografoula Vagena,and Dimitrios Gunopulos.
2004.
Identifying similari-ties, periodicities and bursts for online search queries.In SIGMOD.Yi Wang, Lizhu Zhou, Jianhua Feng, JianyongWang, andZhi-Qiang Liu.
2006.
Mining complex time-seriesdata by learning markovian models.
In Proceedingsof the Sixth International Conference on Data Min-ing, ICDM, pages 1136?1140, Washington, DC, USA.IEEE Computer Society.Xuanhui Wang, ChengXiang Zhai, Xiao Hu, and RichardSproat.
2007.
Mining correlated bursty topic pat-terns from coordinated text streams.
In Proceedingsof the 13th ACM SIGKDD international conference onKnowledge discovery and data mining.Xiang Wang, Kai Zhang, Xiaoming Jin, and Dou Shen.2009.
Mining common topics from multiple asyn-chronous text streams.
In Proceedings of the SecondACM International Conference on Web Search andData Mining, WSDM, pages 192?201.Junjie Yao, Bin Cui, Yuxin Huang, and Xin Jin.
2010.Temporal and social context based burst detectionfrom folksonomies.
In AAAI.Wayne Xin Zhao, Jing Jiang, Jing He, Dongdong Shan,Hongfei Yan, and Xiaoming Li.
2010.
Context mod-eling for ranking and tagging bursty features in text1476streams.
In Proceedings of the 19th ACM interna-tional conference on Information and knowledge man-agement, CIKM ?10.Wayne Xin Zhao, Rishan Chen, Kai Fan, Hongfei Yan,and Xiaoming Li.
2012.
A novel burst-based textrepresentation model for scalable event detection.
InACL?12.Arkaitz Zubiaga, Damiano Spina, V?
?ctor Fresno, andRaquel Mart??nez.
2011.
Classifying trending topics:a typology of conversation triggers on twitter.
In Pro-ceedings of the 20th ACM international conference onInformation and knowledge management, CIKM.1477
