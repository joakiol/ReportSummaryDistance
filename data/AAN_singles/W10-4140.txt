A domain adaption Word SegmenterFor Sighan Bakeoff 2010Guo jiangInstitute of IntelligentInformation Processing,Beijing Information Science &Technology University,Beijing, China, 100192Guojiang132@gmail.comSu WenjieInstitute of IntelligentInformation Processing,Beijing Information Science &Technology University,Beijing, China, 100192dev.sunflower@gmail.comYangsen ZhangInstitute of IntelligentInformation Processing,Beijing Information Science &Technology University,Beijing, China, 100192zhangyangsen@163.comAbstractWe present a Chinese word segmentationsystem which ran on the closed track of thesimplified Chinese Word Segmentation taskof CIPS-SIGHAN-CLP 2010 bakeoffs.
Oursegmenter was built using a HMM.
To fulfillthe cross-domain segmentation task, we usesemi-supervised machine learning method toget the HMM model.
Finally we get themean result of four domains: P=0.719,R=0.721 IntroductionThe 2010 Sighan Bakeoff included two typesof evaluations:(1) Closed training:  In the closed trainingevaluation, participants can only use dataprovided by organizers to train their systemsspecifically, the following data resources andsoftware tools are not permitted to be used inthe training:1) Unspecified corpus;2) Unspecified dictionary, word list orcharacter list: include the dictionariesof named entity, character lists forspecific type of Chinese named entities,idiom dictionaries, semantic lexicons,etc.3) Human-encoded rule bases;4) Unspecified software tools, includeword segmenters, part-of-speechtaggers, or parsers which are trainedusing unspecified data resources.The character type information to distinguishthe following four character types can beused in training: Chinese characters, Englishletters, digits and punctuations.
(2) Open training:  In the open trainingevaluation, participants can use any languageresource, including the training data providedby organizersWe prefer character-based Tagging thandictionary based word segmentation in closedtraining, for we can only use the provide traincorpus and scale of the corpus is not largeenough.
If we select dictionary based methodwe will encounter the out-of-vocabularyproblem.
But in character-based Taggingmethod we can yield a better performancethan the dictionary based method for suchproblem.2 AlgorithmEver before 2002 almost all word segmentmethod is based on dictionary.
In SIGHAN2003 bakeoff, a character-based Taggingmethod was proposed and since then thecharacter-based Tagging method becamemore and more popular.
HMM (HiddenMarkov Model) has been used extensively inspeech recognition, pos tagging and get goodgrades.
So we chose HMM as our machinelearning method to fulfill our task.We formally define the elements of an HMM,and explain how the model generates anobservation sequence.An HMM is characterized by the following:1) N, the number of states in the model.
wedenote the individual states ass={s?, s?,?
, s?
},and the state at time t asq?2) M, the number of distinct observationsymbols per state.
we denote theindividual symbols as v={v?, v?, .
.
, v?
}This work was supported by the national natural science foundation of China (60873013)?Beijing natural science foundation (KZ200811232019); The OpenProject Program of the Key Laboratory of Computational Linguistics (Peking University), Ministry of Education;Funding Project for Academic Human ResourcesDevel-opment in Institutions of Higher Learning Under the Juris-diction of Beijing Municipality (PHR201007131)3) The state transition probabilitydistribution A= { a?)
} where a?)
=P[q???
?s)|q??s?
], 1<i,j<N.4) The observation symbol probabilitydistribution in state j, B={b)?k?
}, whereb)?k??P?v?
at t|q?
?s)?5) The initial state distribution ???
?where ??
?P?q?
?s?
?Graph1For convenience, we use the compactnotation??
?A,B, ?)
to indicate the completeparameter set of the model.There are three basic problems for HMM, forproblem 1 we use forward-backwardalgorithm, for problem 2 we use Viterbialgorithm, for problem 3 we useBaum-Welch algorithm.To application HMM to our task we definethe HMM five factors as blow:1) We define the whole labels set as Q={B,M, E, S}, B represents word?s begin, Mrepresents word?s middle, E representsword?s end and S represents single word.2) We define all Unicode characters as O3) We define A={a?
)}, where a?
)=P[priortoken=s?|posterior label =s)]4) We define B={ b)?k?
}, whereb)?k?=P[current character= v?
|currentlabel =s)]5) We define a sentence as a train sample.So ?={sentences start with s, s ?
Q}.Through the design we transform thecharacter-based tagging problem to HMMproblem 2.
So we can solve this problemwith Viterbi algorithm.3  ExperimentWe use HMM to establish the WordSegment prototype system and make use ofthe Labeled supplied by the ChineseAcademy of  Sciences to train the HMMand get the model parameters which will beused for the next iterative scaling.
After that,we can get a system based on HMM model.Then, with the help of the gotten system, weprocess the unlabeled corpus.
Once it isfinished, we should add the processed corpusto the labeled corpus and get a larger corpuswith which we can retrain the HMM.
Allthese steps have been done according fourtest corpuses: literature, computer, medicine,finance.
In the table, R indicates the recallrate, P indicates the precision rate, F1indicates the macro average, OOV Rindicates the out-of-vocabulary (OOV) rate,OOV RR indicates the out-of-vocabulary(OOV) self repair rate, IV RR indicates theout-of-vocabulary (OOV) self repair rate.
Inorder to more easily view data, we havepresented the Graph2.From the table and graph, we can see that thefinance corpus has a better result, thecomputer corpus don't show a good result forthe R, P, F1.
Generally speaking, this resultis a reflection for the difference between thedictionary based Tagging method andcharacter-based Tagging method.
Afterrecheck our corpus, we can find that thereare more technical terms in the computercorpus than finance corpus.
The explanationfor the result is that if the system encounter atechnical terms, the character-based Taggingmethod will have a bad performance.
In suchsituation, dictionary based Tagging methodmay have a better performance.
For the OOVR and OOV RR, the system has a not badperformance.
Table I and Graph2 show thedetailed experimental data.The results of four test corpus as follow:Type R P F1 OOV ROOVRRIVRRliterature0.6950.7440.7190.0690.3810.719Computer0.7130.6410.6750.1520.2570.795medicine0.7350.74 0.7380.11 0.3780.779finance0.7360.7520.7440.0870.23 0.784Table1Graph24 ConclusionOur system used a HMM andsemi-supervised learning for domainadapting.
Our final system achieved aP=0.719, R=0.72.
There exist two ways toimprove our system performance one isinstead our model of CRF, the other ischange another way to use the unlabeled data.Because the inherent shortage of HMM wecould not get a precise model, and the waywe use the unlabeled data can import err tolabeled data.ReferencesLawrence R. Rabiner.
1989,2.
A Tutorial onHidden Markov Models and SelectedApplications in Speech Recognition.
IEEE,VOL.77,No.2,pp:257-286.Huang Changning, HaoHai.
2007.
Ten Years ofChinese word segmentation.
Vol.
21, No.
3.JOURNAL OF CHINESE INFORMATIONPROCESSINGHuihsin Tseng, Pichuan Chang, Galen Andrew,Daniel Jurafsky, Christopher Manning.
AConditional Random Field Word Segmenter forSighan Bakeoff 2005.Blum A, MITCHELL T. Combining labeled andunlabeled data with co-training Proceeding ofthe 11thAnnual conference on ComputationalLearning Theory.Holmes, W., Russell, M., 1995b.
Speechrecognition using a linear dynamic segmentalHMM.
In: Internat.
Conf.
on Acoust.
SpeechSignal Process.
1995, Detroit, MI.
