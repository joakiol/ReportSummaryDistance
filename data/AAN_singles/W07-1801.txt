Proceedings of SPEECHGRAM 2007, pages 1?8,Prague, Czech Republic, June 2007. c?2007 Association for Computational LinguisticsSpeech Recognition Grammar Compilation in Grammatical FrameworkBjo?rn BringertDepartment of Computer Science and EngineeringChalmers University of Technology and Go?teborg UniversitySE-412 96 Go?teborg, Swedenbringert@cs.chalmers.seAbstractThis paper describes how grammar-basedlanguage models for speech recognition sys-tems can be generated from GrammaticalFramework (GF) grammars.
Context-freegrammars and finite-state models can begenerated in several formats: GSL, SRGS,JSGF, and HTK SLF.
In addition, semanticinterpretation code can be embedded in thegenerated context-free grammars.
This en-ables rapid development of portable, multi-lingual and easily modifiable speech recog-nition applications.1 IntroductionSpeech recognition grammars are used for guid-ing speech recognizers in many applications.
How-ever, there are a number of problems associatedwith writing grammars in the low-level, system-specific formats required by speech recognizers.This work addresses these problems by generat-ing speech recognition grammars and semantic in-terpretation components from grammars written inGrammatical Framework (GF), a high-level, type-theoretical grammar formalism.
Compared to exist-ing work on compiling unification grammars, suchas Regulus (Rayner et al, 2006), our work uses atype-theoretical grammar formalism with a focus onmultilinguality and modular grammar development,and supports multiple speech recognition grammarformalisms, including finite-state models.We first outline some existing problems in the de-velopment and maintenance of speech recognitiongrammars, and describe how our work attempts toaddress these problems.
In the following two sec-tions we introduce speech recognition grammars andGrammatical Framework.
The bulk of the paperthen describes how we generate context-free speechrecognition grammars, finite-state language modelsand semantic interpretation code from GF gram-mars.
We conclude by giving references to a numberof experimental dialogue systems which already useour grammar compiler for generating speech recog-nition grammars.Expressivity Speech recognition grammars arewritten in simple formalisms which do not havethe powerful constructs of high-level grammar for-malisms.
This makes speech recognition grammarwriting labor-intensive and error prone, especiallyfor languages with more inflection and agreementthan English.This is solved by using a high-level grammar for-malism with powerful constructs and a grammarlibrary which implements the domain-independentlinguistic details.Duplicated work When speech recognition gram-mars are written directly in the low-level format re-quired by the speech recognizer, other parts of thesystem, such as semantic interpretation components,must often be constructed separately.This duplicated work can be avoided by gener-ating all the components from a single declarativesource, such as a GF grammar.Consistency Because of the lack of abstractionmechanisms and consistency checks, it is difficult1to modify a system which uses hand-written speechrecognition grammars.
The problem is multipliedwhen the system is multilingual.
The developerhas to modify the speech recognition grammar andthe semantic interpretation component manually foreach language.
A simple change may require touch-ing many parts of the grammar, and there are no au-tomatic consistency checks.The strong typing of the GF language enforcesconsistency between the semantics and the concreterepresentation in each language.Localization With hand-written grammars, it isabout as difficult to add support for a new languageas it is to write the grammar and semantic interpre-tation for the first language.GF?s support for multilingual grammars and thecommon interface implemented by all grammars inthe GF resource grammar library makes it easier totranslate a grammar to a new language.Portability A grammar in any given speech recog-nition grammar format cannot be used with a speechrecognizer which uses another format.In our approach, a GF grammar is used as thecanonical representation which the developer workswith, and speech recognition grammars in many for-mats can be generated automatically from this rep-resentation.2 Speech Recognition GrammarsTo achieve acceptable accuracy, speech recognitionsoftware is guided by a language model which de-fines the language which can be recognized.
A lan-guage model may also assign different probabilitiesto different strings in the language.
A languagemodel can either be a statistical language model(SLM), such as an n-gram model, or a grammar-based language model, for example a context-freegrammar (CFG) or a finite-state automaton (FSA).In this paper, we use the term speech recogni-tion grammar (SRG) to refer to all grammar-basedlanguage models, including context-free grammars,regular grammars and finite-state automata.3 Grammatical FrameworkGrammatical Framework (GF) (Ranta, 2004) is agrammar formalism based on constructive type the-ory.
In GF, an abstract syntax defines a seman-tic representation.
A concrete syntax declares howterms in an abstract syntax are linearized, that is,how they are mapped to concrete representations.GF grammars can be made multilingual by havingmultiple concrete syntaxes for a single abstract syn-tax.3.1 The Resource Grammar LibraryThe GF Resource Grammar Library (Ranta et al,2006) currently implements the morphological andsyntactic details of 10 languages.
This library is in-tended to make it possible to write grammars with-out caring about the linguistic details of particularlanguages.
It is inspired by library-based softwareengineering, where complex functionality is imple-mented in reusable software libraries with simple in-terfaces.The resource grammar library is used throughGF?s facility for grammar composition, where theabstract syntax of one grammar is used in the imple-mentation of the concrete syntax of another gram-mar.
Thus, an application grammar writer who usesa resource grammar uses its abstract syntax termsto implement the linearizations in the applicationgrammar.The resource grammars for the different lan-guages implement a common interface, i.e.
theyall have a common abstract syntax.
This meansthat grammars which are implemented using re-source grammars can be easily localized to otherlanguages.
Localization normally consists of trans-lating the application-specific lexical items, and ad-justing any linearizations which turn out to be uni-diomatic in the language in question.
For example,when the GoTGoDiS (Ericsson et al, 2006) appli-cation was localized to Finnish, only 3 out of 180linearization rules had to be changed.3.2 An Example GF GrammarFigure 1 contains a small example GF abstract syn-tax.
Figure 2 defines an English concrete syntaxfor it, using the resource grammar library.
We willuse this grammar when we show examples of speechrecognition grammar generation later.In the abstract syntax, cat judgements introducesyntactic categories, and fun judgements declareconstructors in those categories.
For example, the2abstract Food = {cat Order; Items; Item;Number;Size;fun order : Items?
Order;and : Items?
Items?
Items;items : Item?
Number?
Size?
Items;pizza,beer : Item;one, two :Number;small, large :Size;}Figure 1: Food.gf: A GF abstract syntax module.concrete FoodEng of Food = open English in {flags startcat = Order;lincat Order = Utt; Items = NP;Item = CN;Number = Det;Size = AP;lin order x = mkUtt x;and x y = mkNP and Conj x y;items x n s = mkNP n (mkCN s x);pizza = mkCN (regN ?pizza?
);beer = mkCN (regN ?beer?
);one = mkDet one Quant;two = mkDet n2;small = mkAP (regA ?small?
);large = mkAP (regA ?large?
);}Figure 2: FoodEng.gf: English concrete syntax forthe abstract syntax in Figure 1.items constructor makes an Items term from an Item,aNumber and a Size.
The term items pizza two smallis an example of a term in this abstract syntax.In the concrete syntax, a lincat judgement de-clares the type of the concrete terms generated fromthe abstract syntax terms in a given category.
Thelinearization of each constructor is declared with alin judgement.
In the concrete syntax in Figure 2,library functions from the English resource gram-mar are used for the linearizations, but it is also pos-sible to write concrete syntax terms directly.
Thelinearization of the term items pizza two small is{s= ?two small pizzas?
}, a record containing a sin-gle string field.By changing the imports and the four lexicalitems, this grammar can be translated to any otherlanguage for which there is a resource grammar.For example, in the German version, we replace(regN ?beer?)
with (reg2N ?Bier?
?Biere?
neuter)and so on.
The functions regN and reg2N implementparadigms for regular English and German nouns,respectively.
This replacement can be formalizedusing GF?s parameterized modules, which lets onewrite a common implementation that can be instan-tiated with the language-specific parts.
Note that theapplication grammar does not deal with details suchas agreement, as this is taken care of by the resourcegrammar.4 Generating Context-free Grammars4.1 AlgorithmGF grammars are converted to context-free speechrecognition grammars in a number of steps.
Anoverview of the compilation pipeline is show in Fig-ure 3.
The figure also includes compilation to finite-state automata, as described in Section 5.
Each stepof the compilation is described in more detail in thesections below.Conversion to CFG The GF grammar is firstconverted into a context-free grammar annotatedwith functions and profiles, as described byLjunglo?f (2004).Cycle elimination All directly and indirectlycyclic productions are removed, since they cannot behandled gracefully by the subsequent left-recursionelimination.
Such productions do not contribute tothe coverage to the grammar, only to the set of pos-sible semantic results.Bottom-up filtering Productions whose right-hand sides use categories for which there are no pro-ductions are removed, since these will never matchany input.Top-down filtering Only productions for cate-gories which can be reached from the start categoryare kept.
This is mainly used to remove parts of thegrammar which are unused because of the choiceof start category.
One example where this is usefulis when a speech recognition grammar is generatedfrom a multimodal grammar (Bringert et al, 2005).In this case, the start category is different from thestart category used by the parser, in that its lineariza-tion only contains the speech component of the in-3GF grammarCFG conversionCycle eliminationBottom-up filteringTop-down filteringLeft-recursioneliminationIdentical categoryeliminationEBNF compactionSRGS/JSGF/GSLRegularapproximationFSA compilationMinimizationSLFFigure 3: Grammar compilation pipeline.put.
Top-down filtering then has the effect of ex-cluding the non-speech modalities from the speechrecognition grammar.The bottom-up and top-down filtering steps are it-erated until a fixed point is reached, since both thesesteps may produce new filtering opportunities.Left-recursion elimination All direct and indi-rect left-recursion is removed using the LCLR trans-form described by Moore (2000).
We have modi-fied the LCLR transform to avoid adding productionswhich use a category A?X when there are no pro-ductions for A?X .Identical category elimination In this step, thecategories are grouped into equivalence classes bytheir right-hand sides and semantic annotations.
Thecategories A1 .
.
.An in each class are replaced by asingle category A1+.
.
.+An throughout the grammar,discarding any duplicate productions.
This has theeffect of replacing all categories which have identi-cal sets of productions with a single category.
Con-crete syntax parameters which do not affect inflec-tion is one source of such redundancy; the LCLRtransform is another.EBNF compaction The resulting context-freegrammar is compacted into an Extended Backus-Naur Form (EBNF) representation.
This reduces thesize and improves the readability of the final gram-mar.
The compaction is done by, for each cate-gory, grouping all the productions which have thesame semantic interpretation, and the same sequenceof non-terminals on their right-hand sides, ignoringany terminals.
The productions in each group aremerged into one EBNF production, where the ter-minal sequences between the non-terminals are con-verted to regular expressions which are the unions ofthe original terminal sequences.
These regular ex-pressions are then minimized.Conversion to output format The resulting non-left-recursive grammar is converted to SRGS, JSGFor Nuance GSL format.A fragment of a SRGS ABNF grammar generatedfrom the GF grammar in Figure 2 is shown below.The left-recursive and rule was removed from thegrammar before compilation, as the left-recursionelimination step makes it difficult to read the gen-erated grammar.
The fragment shown here is for thesingular part of the items rule.$FE1 = $FE6 $FE9 $FE4;$FE6 = one;$FE9 = large | small;$FE4 = beer | pizza;The corresponding fragment generated from theGerman version of the grammar is more complex,since the numeral and the adjective must agree withthe gender of the noun.$FG1 = $FG10 $FG13 $FG6 | $FG9 $FG12 $FG4;$FG9 = eine; $FG10 = ein;$FG12 = gro?e | kleine;$FG13 = gro?es | kleines;$FG4 = Pizza; $FG6 = Bier;4.2 DiscussionThe generated grammar is an overgenerating ap-proximation of the original GF grammar.
This isinevitable, since the GF formalism is stronger than4context-free grammars, for example through its sup-port for reduplication.
GF?s support for dependentlytyped and higher-order abstract syntax is also notyet carried over to the generated speech recogni-tion grammars.
This could be handled in a subse-quent semantic interpretation step.
However, thatrequires that the speech recognizer considers mul-tiple hypotheses, since some may be discarded bythe semantic interpretation.
Currently, if the abstractsyntax types are only dependent on finite types, thegrammar can be expanded to remove the dependen-cies.
This appears to be sufficient for many realisticapplications.In some cases, empty productions in the gener-ated grammar could cause problems for the cycleand left-recursion elimination, though we have yetto encounter this in practice.
Empty productions canbe removed by transforming the grammar, thoughthis has not yet been implemented.For some grammars, the initial CFG generationcan generate a very large number of productions.While the resulting speech recognition grammarsare of a reasonable size, the large intermediate gram-mars can cause memory problems.
Further opti-mization is needed to address this problem.5 Finite-State Models5.1 AlgorithmSome speech recognition systems use finite-state au-tomata rather than context-free grammars as lan-guage models.
GF grammars can be compiled tofinite-state automata using the procedure shown inFigure 3.
The initial part of the compilation toa finite-state model is shared with the context-freeSRG compilation, and is described in Section 4.Regular approximation The context-free gram-mar is approximated with a regular grammar, us-ing the algorithm described by Mohri and Neder-hof (2001).Compilation to finite-state automata The reg-ular grammar is transformed into a set of non-deterministic finite automata (NFA) using a modi-fied version of the make fa algorithm described byNederhof (2000).
For realistic grammars, applyingthe original make fa algorithm to the whole gram-mar generates a very large automaton, since a copyof the sub-automaton corresponding to a given cate-gory is made for every use of the category.Instead, one automaton is generated for each cat-egory in the regular grammar.
All categories whichare not in the same mutually recursive set as thecategory for which the automaton is generated aretreated as terminal symbols.
This results in a setof automata with edges labeled with either terminalsymbols or the names of other automata.If desired, the set of automata can be con-verted into a single automaton by substituting eachcategory-labeled edge with a copy of the corre-sponding automaton.
Note that this always termi-nates, since the sub-automata do not have edges la-beled with the categories from the same mutually re-cursive set.Minimization Each of the automata is turned intoa minimal deterministic finite automaton (DFA) byusing Brzozowski?s (1962) algorithm, which min-imizes the automaton by performing two deter-minizations and reversals.Conversion to output format The resulting finiteautomaton can be output in HTK Standard LatticeFormat (SLF).
SLF supports sub-lattices, which al-lows us to convert our set of automata directly into aset of lattices.
Since SLF uses labeled nodes, ratherthan labeled edges, we move the labels to the nodes.This is done by first introducing a new labeled nodefor each edge, and then eliminating all internal un-labeled nodes.
Figure 4 shows the SLF model gen-erated from the example grammar.
For clarity, thesub-lattices have been inlined.and onetwopizzasbeerspizzabeersmalllargesmalllargeENDSTARTFigure 4: SLF model generated from the grammarin Figure 2.55.2 DiscussionFinite-state models are even more restrictive thancontext-free grammars.
This problem is handledby approximating the context-free grammar withan overgenerating finite-state automaton.
This maylead to failure in a subsequent parsing step, which,as in the context-free case, is acceptable if the rec-ognizer can return all hypotheses.6 Semantic InterpretationSemantic interpretation can be done as a separateparsing step after speech recognition, or it can bedone with semantic information embedded in thespeech recognition grammar.
The latter approach re-sembles the semantic actions used by parser genera-tors for programming languages.
One formalism forsemantic interpretation is the proposed Semantic In-terpretation for Speech Recognition (SISR) standard.SISR tags are pieces of ECMAScript code embed-ded in the speech recognition grammar.6.1 AlgorithmThe GF system can include SISR tags when gen-erating speech recognitions grammars in SRGSand JSGF format.
The SISR tags are generatedfrom the semantic information in the annotatedCFG (Ljunglo?f, 2004).
The result of the semanticinterpretation is an abstract syntax term.The left-recursion elimination step makes itsomewhat challenging to produce correct abstractsyntax trees.
We have extended Moore?s (2000)LCLR transform to preserve the semantic interpreta-tion.
The LCLR transform introduces new categoriesof the form A?X where X is a proper left corner ofa category A.
The new category A?X can be under-stood as ?the category A, but missing an initial X?.Thus the semantic interpretation for a production inA?X is the semantic interpretation for the original A-production, abstracted (in the ?-calculus sense) overthe semantic interpretation of the missing X .
Con-versely, where-ever a category A?X is used, its re-sult is applied to the interpretation of the occurrenceof X .6.2 DiscussionAs discussed in Section 4.2, the semantic interpre-tation code could be used to implement the non-context-free features of GF, but this is not yet done.The slot-filling mechanism in the GSL formatcould also be used to build semantic representations,by returning program code which can then be ex-ecuted.
The UNIANCE grammar compiler (Bos,2002) uses that approach.7 Related Work7.1 Unification Grammar CompilationCompilation of unification grammars to speechrecognition grammars is well described in the liter-ature (Moore, 1999; Dowding et al, 2001).
Regu-lus (Rayner et al, 2006) is perhaps the most ambi-tious such system.
Like GF, Regulus uses a generalgrammar for each language, which is specialized to adomain-specific one.
Ljunglo?f (Ljunglo?f, 2007b) re-lates GF and Regulus by showing how to convert GFgrammars to Regulus grammars.
We carry composi-tional semantic interpretation through left-recursionelimination using the same idea as the UNIANCEgrammar compiler (Bos, 2002), though our versionhandles both direct and indirect left-recursion.The main difference between our work and theexisting compilers is that we work with type-theoretical grammars rather than unification gram-mars.
While the existing work focuses on GSLas the output language, we also support a numberof other formats, including finite-state models.
Byusing the GF resource grammars, speech recogni-tion language models can be produced for more lan-guages than with previous systems.
One shortcom-ing of our system is that it does not yet have supportfor weighted grammars.7.2 Generating SLMs from GF GrammarsJonson (2006) has shown that in addition to gener-ating grammar-based language models, GF can beused to build statistical language models (SLMs).
Itwas found that compared to our grammar-based ap-proach, use of generated SLMs improved the recog-nition performance for out-of-grammar utterancessignificantly.8 ResultsSpeech recognition grammars generated from GFgrammars have already been used in a number ofresearch dialogue systems.6GOTTIS (Bringert et al, 2005; Ericsson et al,2006), an experimental multimodal and multilingualdialogue system for public transportation queries,uses GF grammars for parsing multimodal input.For speech recognition, it uses GSL grammars gen-erated from the speech modality part of the GFgrammars.DJ-GoDiS, GoDiS-deLUX, and GoTGoDiS (Er-icsson et al, 2006) are three applications which useGF grammars for speech recognition and parsingtogether with the GoDiS implementation of issue-based dialogue management (Larsson, 2002).
GoT-GoDiS has been translated to 7 languages using theGF resource grammar library, with each new transla-tion taking less than one day (Ericsson et al, 2006).The DICO (Villing and Larsson, 2006) dialoguesystem for trucks has recently been modified touse GF grammars for speech recognition and pars-ing (Ljunglo?f, 2007a).DUDE (Lemon and Liu, 2006) and its extensionREALL-DUDE (Lemon et al, 2006b) are environ-ments where non-experts can develop dialogue sys-tems based on Business Process Models describingthe applications.
From keywords, prompts and an-swer sets defined by the developer, the system gen-erates a GF grammar.
This grammar is used for pars-ing input, and for generating a language model inSLF or GSL format.The Voice Programming system by Georgila andLemon (Georgila and Lemon, 2006; Lemon et al,2006a) uses an SLF language model generated froma GF grammar.Perera and Ranta (2007) have studied how GFgrammars can be used for localization of dialoguesystems.
A GF grammar was developed and local-ized to 4 other languages in significantly less timethan an equivalent GSL grammar.
They also foundthe GSL grammar generated by GF to be muchsmaller than the hand-written GSL grammar.9 ConclusionsWe have shown how GF grammars can be compiledto several common speech recognition grammar for-mats.
This has helped decrease development time,improve modifiability, aid localization and enableportability in a number of experimental dialoguesystems.Several systems developed in the TALK andDICO projects use the same GF grammars forspeech recognition, parsing and multimodal fu-sion (Ericsson et al, 2006).
Using the same gram-mar for multiple system components reduces devel-opment andmodification costs, and makes it easierto maintain consistency within the system.The feasibility of rapid localization of dialoguesystems which use GF grammars has been demon-strated in the GoTGoDiS (Ericsson et al, 2006) sys-tem, and in experiments by Perera and Ranta (2007).Using speech recognition grammars generated byGF makes it easy to support different speech rec-ognizers.
For example, by using the GF grammarcompiler, the DUDE (Lemon and Liu, 2006) systemcan support both the ATK and Nuance recognizers.Implementations of the methods described in thispaper are freely available as part of the GF distribu-tion1.AcknowledgmentsAarne Ranta, Peter Ljunglo?f, Rebecca Jonson,David Hjelm, Ann-Charlotte Forslund, Ha?kan Bur-den, Xingkun Liu, Oliver Lemon, and the anony-mous referees have contributed valuable commentson the grammar compiler implementation and/orthis article.
We would like to thank Nuance Com-munications, Inc., OptimSys, s.r.o., and Opera Soft-ware ASA for software licenses and technical sup-port.
The code in this paper has been typeset usinglhs2TeX, with help fromAndres Lo?h.
This work hasbeen partly funded by the EU TALK project, IST-507802.ReferencesJohan Bos.
2002.
Compilation of unification grammarswith compositional semantics to speech recognitionpackages.
In Proceedings of the 19th internationalconference on Computational linguistics, pages 1?7,Morristown, NJ, USA.
Association for ComputationalLinguistics.Bjo?rn Bringert, Robin Cooper, Peter Ljunglo?f, and AarneRanta.
2005.
Multimodal Dialogue System Gram-mars.
In Proceedings of DIALOR?05, Ninth Workshopon the Semantics and Pragmatics of Dialogue, pages53?60.1http://www.cs.chalmers.se/?aarne/GF/7Janusz A. Brzozowski.
1962.
Canonical regular expres-sions and minimal state graphs for definite events.
InMathematical theory of Automata, Volume 12 of MRISymposia Series, pages 529?561.
Polytechnic Press,Polytechnic Institute of Brooklyn, N.Y.John Dowding, Beth A. Hockey, Jean M. Gawron, andChristopher Culy.
2001.
Practical issues in compil-ing typed unification grammars for speech recognition.In ACL ?01: Proceedings of the 39th Annual Meetingon Association for Computational Linguistics, pages164?171, Morristown, NJ, USA.
Association for Com-putational Linguistics.Stina Ericsson, Gabriel Amores, Bjo?rn Bringert, Ha?kanBurden, Ann C. Forslund, David Hjelm, Rebecca Jon-son, Staffan Larsson, Peter Ljunglo?f, Pilar Mancho?n,David Milward, Guillermo Pe?rez, and Mikael Sandin.2006.
Software illustrating a unified approach to mul-timodality and multilinguality in the in-home domain.Technical Report 1.6, TALK Project.Kallirroi Georgila and Oliver Lemon.
2006.
Program-ming by Voice: enhancing adaptivity and robustnessof spoken dialogue systems.
In BRANDIAL?06, Pro-ceedings of the 10th Workshop on the Semantics andPragmatics of Dialogue, pages 199?200.Rebecca Jonson.
2006.
Generating Statistical Lan-guage Models from Interpretation Grammars in Dia-logue Systems.
In Proceedings of EACL?06.Staffan Larsson.
2002.
Issue-based Dialogue Manage-ment.
Ph.D. thesis, Go?teborg University.Oliver Lemon and Xingkun Liu.
2006.
DUDE: aDialogue and Understanding Development Environ-ment, mapping Business Process Models to Informa-tion State Update dialogue systems.
In EACL 2006,11st Conference of the European Chapter of the Asso-ciation for Computational Linguistics.Oliver Lemon, Kallirroi Georgila, David Milward, andTommy Herbert.
2006a.
Programming Devices andServices.
Technical Report 2.3, TALK Project.Oliver Lemon, Xingkun Liu, Daniel Shapiro, and CarlTollander.
2006b.
Hierarchical Reinforcement Learn-ing of Dialogue Policies in a development environmentfor dialogue systems: REALL-DUDE.
In BRAN-DIAL?06, Proceedings of the 10th Workshop on the Se-mantics and Pragmatics of Dialogue, pages 185?186,September.Peter Ljunglo?f.
2004.
Expressivity and Complexity ofthe Grammatical Framework.
Ph.D. thesis, Go?teborgUniversity, Go?teborg, Sweden.Peter Ljunglo?f.
2007a.
Personal communication, March.Peter Ljunglo?f.
2007b.
Converting Grammatical Frame-work to Regulus.Mehryar Mohri and Mark J. Nederhof.
2001.
Regu-lar Approximation of Context-Free Grammars throughTransformation.
In Jean C. Junqua and Gertjan vanNoord, editors, Robustness in Language and SpeechTechnology, pages 153?163.
Kluwer Academic Pub-lishers, Dordrecht.Robert C. Moore.
1999.
Using Natural-LanguageKnowledge Sources in Speech Recognition.
In K. M.Ponting, editor, Computational Models of Speech Pat-tern Processing, pages 304?327.
Springer.Robert C. Moore.
2000.
Removing left recursion fromcontext-free grammars.
In Proceedings of the firstconference on North American chapter of the Associ-ation for Computational Linguistics, pages 249?255,San Francisco, CA, USA.
Morgan Kaufmann Publish-ers Inc.Mark J. Nederhof.
2000.
Regular Approximation ofCFLs: A Grammatical View.
In Harry Bunt and AntonNijholt, editors, Advances in Probabilistic and otherParsing Technologies, pages 221?241.
Kluwer Aca-demic Publishers.Nadine Perera and Aarne Ranta.
2007.
An Experiment inDialogue System Localization with the GF ResourceGrammar Library.Aarne Ranta, Ali El Dada, and Janna Khegai.
2006.
TheGF Resource Grammar Library, June.Aarne Ranta.
2004.
Grammatical Framework: A Type-Theoretical Grammar Formalism.
Journal of Func-tional Programming, 14(2):145?189, March.Manny Rayner, Beth A. Hockey, and Pierrette Bouil-lon.
2006.
Putting Linguistics into Speech Recogni-tion: The Regulus Grammar Compiler.
CSLI Publica-tions, Ventura Hall, Stanford University, Stanford, CA94305, USA, July.Jessica Villing and Staffan Larsson.
2006.
Dico: AMultimodal Menu-based In-vehicle Dialogue System.In BRANDIAL?06, Proceedings of the 10th Workshopon the Semantics and Pragmatics of Dialogue, pages187?188.8
