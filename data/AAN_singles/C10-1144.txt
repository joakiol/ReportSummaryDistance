Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1281?1289,Beijing, August 2010Forest-guided Supertagger TrainingYao-zhong Zhang ?
Takuya Matsuzaki ??
Department of Computer Science, University of Tokyo?
School of Computer Science, University of Manchester?National Centre for Text Mining{yaozhong.zhang, matuzaki, tsujii}@is.s.u-tokyo.ac.jpJun?ichi Tsujii??
?AbstractSupertagging is an important techniquefor deep syntactic analysis.
A super-tagger is usually trained independentlyof the parser using a sequence labelingmethod.
This presents an inconsistenttraining objective between the supertaggerand the parser.
In this paper, we pro-pose a forest-guided supertagger trainingmethod to alleviate this problem by incor-porating global grammar constraints intothe supertagging process using a CFG-filter.
It also provides an approach tomake the supertagger and the parser moretightly integrated.
The experiment showsthat using the forest-guided trained super-tagger, the parser got an absolute 0.68%improvement from baseline in F-scorefor predicate-argument relation recogni-tion accuracy and achieved a competi-tive result of 89.31% with a faster pars-ing speed, compared to a state-of-the-artHPSG parser.1 IntroductionDeep syntactic analysis by lexicalized grammarparsing, which provides linguistic-rich informa-tion for many NLP tasks, has recently receivedmore and more attention from the NLP commu-nity.
To use a deep parser in real large-scale ap-plications, speed is an important issue to take intoconsideration.
Supertagging is one of the speed-up technique widely used for lexicalized grammarparsing.
A supertagger is used to limit the numberof plausible lexical entries fed to the parser, thiscan greatly reduce the search space for the parser.Supertagging was first proposed for LexicalizedTree Adjoining Grammar (LTAG) (Bangalore andJoshi, 1999), and then successfully applied toCombinatory Categorial Grammar (CCG) (Clark,2002) and Head-driven Phrase Structure Gram-mar (HPSG) (Ninomiya et al, 2006).
In addi-tion, supertags can also be used for other NLPtasks besides parsing, such as semantic role label-ing (Chen and Rambow, 2003) and machine trans-lation (Birch et al, 2007; Hassan et al, 2007) toutilize syntactic information in the supertags.In lexicalized grammar parsing, supertagging isusually treated as a sequence labeling task inde-pendently trained from the parser.
Previous re-search (Clark, 2002) showed that even a point-wise classifier not considering context edge fea-tures is effective when used as a supertagger.
Tomake up for the insufficient accuracy as a single-tagger, more than one supertag prediction is re-served and the parser takes the burden of resolvingthe rest of the supertag ambiguities.A non-trivial problem raised by the separatetraining of the supertagger is that the predictionscore provided by the supertagger might not besuitable for direct use in the parsing process, sincea separately trained supertagger that does not takeinto account grammar constraints has a trainingobjective which is inconsistent with the parser.Although the scores provided by the supertaggercan be ignored (e.g., in some CCG parsers), thismay also discard some useful information for ef-fective beam search and accurate disambiguation.Based on this observation, we assume thatconsidering global grammar constraints duringthe supertagger training process would make thesupertagger and the parser more tightly integrated.1281In this paper, we propose an on-line forest-guidedtraining method for a supertagger to make thetraining objective of a supertagger more closelyrelated to the parsing task.
We implemented thismethod on a large-scale HPSG grammar.
Weused a CFG grammar to approximate the originalHPSG grammar in the supertagging stage and ap-plied best-first search to select grammar-satisfyingsupertag sequences for the parameter updating.The experiments showed that the HPSG parser isimproved by considering structure constraints inthe supertagging training process.
For the stan-dard test set (Penn Treebank Section 23), we ac-complished an absolute 0.68% improvement frombaseline in F-score for predicate-argument rela-tion recognition and got a competitive result of89.31% with a faster parsing speed, compared toa state-of-the-art HPSG parser.The remainder of the paper is organized asfollows: in section 2 we provide the necessarybackground regarding HPSG parsing.
In section3, we introduce the on-line forest-guided super-tagger training method.
Section 4 shows the ex-periment results and the related analysis.
Section5 compares the proposed approach with relatedwork and section 6 presents our conclusions andfuture work.2 Background2.1 Statistical HPSG ParsingHPSG (Pollard and Sag, 1994) is a lexicalistgrammar framework.
In HPSG, a large numberof lexical entries are used to express word-specificcharacteristics, while only a small number of ruleschemata are used to describe general construc-tion rules.
Typed feature structures named ?signs?are used to represent both lexical entries andphrasal constituents.
A classic efficient statisti-cal HPSG parsing process is depicted in Figure 1.Given a word and part-of-speech sequence (w, p)as input, the first step (called ?supertagging?)
inHPSG parsing is to assign possible lexical entries.In practice, for each word, more than one super-tag is reserved for the parser.
Then, the parsersearches the given lexical entry space to constructa HPSG tree using the rule schemata to com-bine possible signs.
Constituent-based methodsand transition-based methods can be used for treestructure disambiguation.
This parsing frameworkusing supertagging is also used in other lexical-ized grammars, such as LTAG and CCG.2.2 HPSG SupertaggingLike other lexicalized grammar, the lexical en-tries defined in HPSG are referred to as ?super-tags?.
For example, the word ?like?
is assigneda lexical entry for transitive verbs in non-3rd per-son present form, which indicates that the headsyntactic category of ?like?
is verb and it hasan NP subject and an NP complement.
Withsuch fine-grained grammatical type distinctions,the number of supertags is very large.
Comparedto the 45 part-of-speech (POS) tags defined in thePennTreebank, the HPSG grammar we used con-tains 2,308 supertags.
The large number and thecomplexity of the supertags makes supertaggingharder than the POS tagging task.Supertagging can be formulated as a sequencelabeling task.
Here, we follow the definition ofCollins?
perceptron (Collins, 2002).
The train-ing objective of supertagging is to learn the map-ping from a POS-tagged word sentence w =(w1/p1, ..., wn/pn) to a sequence of supertagss = (s1, ..., sn).
We use function GEN(w)to indicate all candidates of supertag sequencesgiven input w. Feature function ?
maps a sam-ple (w, s) to a point in the feature space Rd.
?
isthe vector of feature weights.
Given an input w,the most plausible supertag sequence is found bythe prediction function defined as follows:F (w) = argmaxs?GEN(w)?
?
?
(w, s) (1)2.3 CFG-filteringCFG-filtering (Kiefer and Krieger, 2000) is a tech-nique to find a superset of (packed) HPSG parsetrees that satisfy the constraints in a grammar.
ACFG that approximates the original HPSG gram-mar is used for efficiently finding such trees with-out doing full-fledged HPSG parsing that is com-putationally demanding because the schema ap-plication involves unification operations amonglarge feature structures (signs).
The number ofpossible signs is infinite in general and hence1282Figure 1: HPSG parsing for the sentence ?They like coffee.
?some features (e.g., the number agreement fea-ture) are ignored in the approximating CFG so thatthe set of possible signs can be approximated bya finite set of non-terminal symbols in the CFG.By this construction, some illegal trees may beincluded in the set of trees licensed by the ap-proximating CFG, but none of the well-formedtrees (i.e., those satisfying all constraints in thegrammar) are excluded by the approximation.
Weuse the algorithm described by Kiefer and Krieger(2000) to obtain the approximating CFG for theoriginal HPSG.
The technical details regardingthe algorithm can be found in Kiefer and Krieger(2000).3 Forest-guided Training forSupertagging3.1 MotivationIn lexicalized grammar parsing, a parser aims tofind the most plausible syntactic structure for agiven sentence based on the supertagging results.One efficient parsing approach is to use predic-tion scores provided by the supertagger.
Usu-ally, the supertagger is trained separately from thestructure disambiguation in a later stage.
Thispipeline parsing strategy poses a potential prob-lem in that the training objective of a supertaggercan deviate from the final parser, if the globalgrammar constraints are not considered.
For ex-ample, the supertag predictions for some wordscan contribute to high supertagging accuracy, butcause the parser to fail.
Therefore, considering theglobal grammar constraints in the supertaggingtraining stage can make the supertagger and theAlgorithm 1: Forest-guided supertagger trainingInput: Training Sample (wi, si)i=1,...,N ,Number of iterations T1: ?
?
(0, ..., 0), ?sum ?
(0, ..., 0)2: for iterNum?
1 to T do3: for i ?
1 to N do4: Generate supertag lattice usingthe point-wise classifier with current ?5: Select s?i from the latticewhich can construct a treewith largest sequence score6: if( No s?i satisfied grammar constraints)s?i ?
argmaxs?GEN(wi) ?i ?
?
(wi, si)7: if s?i "= si then8: ?i+1 ?
?i + ?
(wi, si)?
?
(wi, s?i)9: ?sum ?
?sum + ?i+1Return: ?sum/NTparser more tightly related, which will contributetowards the performance of the parser.3.2 Training AlgorithmBased on the motivation above, we proposea forest-guided supertagger training method tomake the supertagger more tightly integrated withthe parser.
This method is based on the averagedperceptron training algorithm.
The training pro-cess is given in Algorithm 1.The most important difference of the proposedalgorithm compared to the traditional supertaggertraining method is that the current best-scoredsupertag sequence is searched only within thespace of the supertag sequences that are allowedby the grammar.
As for whether the grammar1283constraints are satisfied, we judge it by whethera possible syntactic tree can be constructed usingthe given supertag sequence.
We do not requirethe constructed syntactic tree to be identical to thegold tree in the corpus.
For this reason we call it?forest-guided?.In the forest-guided training of the supertagger,an approximating CFG is used to filter out thesupertag sequences from which no well-formedtree can be built.
It is implemented as a best-firstCFG parser wherein the score of a constituent isthe score of the supertag (sub-)sequence on thefringe of the constituent, which is calculated us-ing the current value of the parameters.
Note thatthe best-first parser can find the best-scored super-tag sequence very efficiently given proper scoringfor the candidate supertag set for each token; thisis actually the case in the course of training exceptfor the initial phase of the training, wherein the pa-rameter values are not well-tuned.
The efficiencyis due to the sparseness of the approximating CFG(i.e., the production rule set includes only a tinyfraction of the possible parent-children combina-tions of symbols) and highest-scored supertags of-ten have a well-formed tree on top of them.As is clear from the above description, the useof CFG-filter in the forest-guided training of thesupertagger is not essential but is only a subsidiarytechnique to make the training faster.
The im-provement by the forest-guided training shouldhowever depend on whether the CFG approxi-mation is reasonably tight or not.
Actually, wemanaged to obtain a manageable size out of aCFG grammar, which includes 80 thousand non-terminal symbols and 10 million rules, by elimi-nating only a small number of features (semantics,case and number agreement, and fine distinctionsin nouns, adjectives and complementizers).
Wethus believe that the approximation is fairly tight.This training algorithm can also be explainedin a search-based learning framework (Hal Daume?III and Daniel Marcu, 2005).
In this framework,the objective of learning is to optimize the ?
forthe enqueue function to make the good hypothe-ses rank high in the search queue.
The rank scorer consists of two components: path score g andheuristic score h. In the forest-guided trainingmethod, r can be rewritten as follows:r = g + h= ?
?
?
(x, y?)
+ [Tree(y?)]
?
Penalty (2)The heuristic part h checks whether the super-tag candidate sequence satisfies the grammar con-straints: if no CFG tree can be constructed, -?penalty is imposed to the candidate sequence inthe forest-guided training method.4 ExperimentsWe mainly evaluated the proposed forest-guidedsupertagger training method on HPSG parsing.Supertagging accuracy1 using different trainingmethods was also investigated.4.1 Corpus DescriptionThe HPSG grammar used in the experiments isEnju version 2.32.
It is semi-automatically con-verted from the WSJ portion of PennTreebank(Miyao, 2006).
The grammar consists of 2,308supertags in total.
Sections 02-21 were used totrain different supertagging models and the HPSGparser.
Section 22 and section 23 were used asthe development set and the test set respectively.We evaluated the HPSG parser performance by la-beled precision (LP) and labeled recall (LR) ofpredicate-argument relations of the parser?s out-put as in previous works (Miyao, 2005).
All ex-periments were conducted on an AMD Opteron2.4GHz server.Template Type TemplateWord wi,wi?1,wi+1,wi?1&wi, wi&wi+1POS pi, pi?1, pi?2, pi+1,pi+2, pi?1&pi, pi?2&pi?1,pi?1&pi+1, pi&pi+1,pi+1&pi+2Word-POS pi?1&wi, pi&wi, pi+1&wiTable 1: Feature templates used for supertaggingmodels.1?UNK?
supertags are ignored in evaluation as in previ-ous works.2http://www-tsujii.is.s.u-tokyo.ac.jp/enju/index.html12844.2 Baseline Models and SettingsWe used a point-wise averaged perceptron (PW)to train a baseline supertagger.
Point-wise classi-fiers have been reported to be very effective andwith competitive results for the supertagging task(Clark, 2002; Zhang et al, 2009).
The number oftraining iterations was set to 5.
The features usedin the supertaggers are described in Table 1.
Forcomparison, these features are identical to the fea-tures used in the previous works (Matsuzaki et al,2007; Ninomiya et al, 2007).
To make the train-ing efficient, we set the default chart size limit forthe forest-guided supertagger training to be 20kby tuning it on the development set.We combined the supertagger trained underforest-guidance with a supertagging-based HPSGparser (Matsuzaki et al, 2007) and evaluated thecontribution of the improved supertagger train-ing procedure for the final HPSG parsing by theaccuracy of the predicate-argument relations out-put of the parser.
The parser crucially dependson the supertagger?s performance in that it out-puts the first well-formed tree successfully con-structed on the highest scored supertag sequence.The highest-scored supertag sequences are enu-merated one by one in descending order in re-gards to their score.
The enumeration is actu-ally implemented as n-best parsing on the super-tag candidates using an approximating CFG.
TheHPSG tree construction on a supertag sequence isdone using a shift-reduce style parsing algorithmequipped with a classifier-based action selectionmechanism.The automatically assigned POS tags weregiven by a maximum entropy tagger with roughly97% accuracy.4.3 Supertagging ResultsAlthough we mainly focused on improving the fi-nal HPSG parsing performance through the im-proved supertagger training, it is also very inter-esting to investigate the supertagger performanceusing different training methods.
To evaluate theforest-guided training method for a supertagger,we also need to incorporate structure constraintsin the test stage.
To make fair comparisons,for the averaged perceptron trained supertaggerwe also add structure constraints in its testing.Model Name Acc%FT+CFG 92.77auto-POS PW+CFG 92.47PW 91.14ME 91.45FT+CFG 93.98gold-POS PW+CFG 93.70PW 92.48ME 92.78Table 2: Supertagging results in section 23.
?FT?represents the forest-guided trained supertagger.?PW?
is the baseline average perceptron trainedsupertagger.
?ME?
is the supertagger trained byusing the maximum entropy method.
?+CFG?
in-dicates the use of the CFG-filter for the super-tagger results.
The accuracy of automatically as-signed POS tags in this section is 97.39%.For simplicity, throughout this paper, we call theforest-guided trained supertagger ?FT?
in short,while the ?PW?
is used to represent the base-line point-wise averaged perceptron supertagger.?ME?
is the re-implemented maximum entropysupertagger described in Matsuzaki et al (2007).For the PW supertagger, the performance wasroughly 0.3% below the ME supertagger.
Simi-lar results were reported by Zhang et al (2009),which used a Bayes point machine to reduce thegap between the averaged perceptron supertaggerand the maximum entropy supertagger.
Althoughwe expected the ME supertagger using CFG-filterto give better results than the PW supertagger, im-plementing forest-guided supertagger training ina maximum entropy framework is different andmore sophisticated than the current on-line train-ing method.
Considering that the performance ofthe PW supertagger and the ME supertagger wereat a similar level, we chose the PW supertagger asour baseline.We used a CFG-filter to incorporate globalgrammar constraints into both the training andthe testing phase.
Compared to the PW super-tagger, the PW+CFG supertagger incorporatedglobal grammar constraints only in the test phase,while for the FT+CFG supertagger, the globalgrammar constraints were incorporated both in1285!!!!!!!!!!!!!!!
!Training MethodIter NUM1 2 3 4 5 Total TimeFT 6684s 4189s 3524s 3285s 3086s ?
5.8hPW 99s 116s 117s 117s 117s ?
10 minME / ?
3hTable 3: Supertagger training time on section 02-21.
?FT?
and ?PW?
represent forest-guided trainingand point-wise averaged perceptron training separately.
?ME?
is the point-wise maximum entropytraining reported in Matsuzaki et al (2007).the training and the testing stage.
The super-tagging accuracy for different models is shownin Table 2.
Firstly, incorporating grammar con-straints only in the testing phase (PW+CFG) gavean absolute 1.22% (gold POS) and 1.33% (autoPOS) increase in F-score compared to the PWsupertagger.
Secondly, incorporating grammarconstraints into both the training and the testingstage (FT+CFG) gave an additional 0.28% (goldPOS) and 0.3% (auto POS) improvement over thePW+CFG supertagger with p-values 0.0018 (goldPOS) and 0.0016 (auto POS).This also indicates that the supertagger and theparser are closely related to each other.
The orig-inal motivation for supertagging is using simplemodels to resolve lexical ambiguities, which canefficiently reduce the search space of the parser.A better supertagger can contribute to more ef-ficient and more accurate lexicalized grammarparsing.
Actually, a supertagger can act as acoarse parser for the whole parsing process aswell, as long as the coarse parser is efficient.
Sincesupertag disambiguation is highly constrained bythe grammar, incorporating grammar constraintsinto supertagging (including training and testing)by using the CFG-filter can further improve thesupertagging performance, as shown in Table 2.As for the supertagger training time, incorpo-rating grammar constraints inevitably increasesthe training time.
As shown in Table 3, the to-tal training time of forest-guided training (defaultsettings, with chart size limited to 20k) was about5.8 hours.
For each iteration of the FT model,we find that the training time gradually decreaseswith each successive iteration.
This hints that wecan do better model initialization to further reducethe training time.4.4 HPSG Parsing ResultsWe evaluated the HPSG parsers using differentsupertagger training methods.
For the baselineHPSG parser, a CFG-filter is already incorporatedto accelerate the parsing process.
In the follow-ing experiments, we fed the parser all the possi-ble supertag candidates with the prediction scoresgenerated by the supertaggers.
We controlled theupper bound of the chart size in the CFG-filter tomake the parser more efficient.Table 4 shows the results of the different pars-ing models.
We first compared the baselineparsers using different supertaggers.
The forest-guided supertagger improved the final FT parser?sF-score by 0.68% (statistically significant) overthe PW parser using the PW supertagger, whichdid not consider global grammar constraints dur-ing the supertagger training process.
The parsingtime of the FT parser was very close to that of thePW parser (108s vs. 106s), which was also ef-ficient.
The result empirically reflects that incor-porating the global grammar constraints into thesupertagger training process can refine supertagpredicting scores, which become more consistentand compatible with the parser.We also compared our results with a state-of-the-art HPSG parser using the same grammar.Enju (Miyao, 2005; Ninomiya et al, 2007) isa log-linear model based HPSG parser, whichuses a maximum entropy model for the struc-ture disambiguation.
In contrast to our baselineparser, full HPSG grammar is directly used withCKY algorithm in the parsing stage.
As for theparsing performance, our baseline PW parser us-ing the PW supertagger was 0.23% below theEnju parser.
However, by using the forest-guidedtrained supertagger, our improved FT parser per-1286Parser UP UR LP LR F-score Time ?FT Parser 92.28 92.14 89.38 89.23 89.31 108sPW Parser 91.88 91.63 88.75 88.51 88.63 106sEnju 2.3 92.26 92.21 88.89 88.84 88.86 775sTable 4: Parser performance on Section 23.
?FT Parser?
represents baseline parser which uses forest-guided trained supertagger.
?PW Parser?
represents the baseline parser which uses the point-wise av-eraged perceptron trained supertagger.
(?)
The time is the total time of both supertagging and parsingand it was calculated on all 2291 sentences of the Section 23.8585.58686.58787.58888.5890.5k1k 1.5k2k 2.5k3k 3.5k4k 10k15k20kHPSGParsingF-scoreChart size limit in the parsingParser using the PW supertaggerParser using the 10k-train FT supertaggerParser using the 20k-train FT supertaggerFigure 2: The F-score of the HPSG parsers on sec-tion 22 using different settings for the chart sizelimit in supertagger training and parsing.formed 0.45% better than the Enju parser (defaultsettings) in F-score.
In addition, our shift-reducestyle parser was faster than the Enju parser.Beam size plays an important role for theforest-guided supertagger training method, since alarger beam size reduces the possibility of searcherrors.
Precisely speaking, we control the beamsize by limiting the number of edges in the chartin both the forest-guided supertagger training pro-cess and the final parsing.
Figure 2 shows the re-sults of setting different limits for the chart sizeduring supertagger training and parsing on the de-velopment set.
The X-axis represents the chartsize limitation for the parsing.
?10k-train?
rep-resents the chart size to be limited to 10k dur-ing FT supertagger training phase.
A similarrepresentation is used for ?20k-train?.
There isno tree structure search process for the baselinePW supertagger.
We evaluated the F-score of theparsers using different supertaggers.
As shown inFigure 2, when the chart size of the parser wasmore than 10k, the benefit of using forest-guidedsupertaggers were obvious (around an absolute0.5% improvement in F-score, compared to theparser using the baseline PW supertagger).
Theperformance of the parser using ?10k-train?
FTsupertagger was already approaching to that of theparser using ?20k-train?
FT supertagger.
Whenthe chart size of the parser was less than 2000, theforest-guided supertaggers were not work.
Simi-lar to the results showed in previous research (HalDaume?
III and Daniel Marcu, 2005), it is better touse the same chart size limit in the forest-guidedsupertagger training and the final parsing.5 Related WorkSince the supertagging technique is well knownto drastically improve the parsing speed and ac-curacy, there is work concerned with tightly in-tegrating a supertagger with a lexicalized gram-mar parser.
Clark and Curran (2004) investigateda multi-tagger supertagging technique for CCG.Based on the multi-tagging technique, supertaggerand parser are tightly coupled, in the sense that theparser requests more supertags if it fails.
They(Clark and Curran, 2007) also used the percep-tron algorithm to train a CCG parser.
Differ-ent from their work, we focused on improvingthe performance of the deep parser by refiningthe training method for supertagging.
Ninomiyaet al (2007) used the supertagging probabili-ties as a reference distribution for the log-linearmodel for HPSG, which aimed to consistentlyintegrate supertagging into probabilistic HPSGparsing.
Prins et al (2001) trained a POS-tagger on an automatic parser-generated lexicalentry corpus as a filter for Dutch HPSG parsingto improve the parsing speed and accuracy.1287The existing work most similar to ours isBoullier (2003).
He presented a non-statisticalparsing-based supertagger for LTAG.
Similar tohis method, we used a CFG to approximate theoriginal lexicalized grammar.
The main differencebetween these two methods is that we considerthe grammar constraints in the training phase ofthe supertagger, not only in the supertagging testphase and our main objective is to improve theperformance of the final parser.6 Conclusions and Future WorkIn this paper, based on the observation that su-pertaggers are commonly trained separately fromlexicalized parsers without global grammar con-straints, we proposed a forest-guided supertaggertraining method to integrate supertagging moretightly with deep parsing.
We applied this methodto HPSG parsing and made further significant im-provement for both supertagging (0.28%) and theHPSG parsing (0.68%) compared to the baseline.The improved parser also achieved a competitiveresult (89.31%) with a faster parsing speed, com-pared to a state-of-the-art HPSG parser.For future work, we will try to weight the for-est trees for the supertagger training and extendthis method to other lexicalized grammars, suchas LTAG and CCG.AcknowledgmentsWe are grateful to the anonymous reviewers fortheir valuable comments.
We also thank GoranTopic and Pontus Stenetorp for their help proof-reading this paper.
The first author was sup-ported by The University of Tokyo Fellowship(UT-Fellowship).
This work was partially sup-ported by Grant-in-Aid for Specially PromotedResearch (MEXT, Japan).ReferencesBangalore, Srinivas and Aravind K. Joshi.
1999.Supertagging: An approach to almost parsing.Computational Linguistics, 25:237?265.Birch, Alexandra, Miles Osborne, and Philipp Koehn.2007.
CCG supertags in factored statistical machinetranslation.
In Proceedings of the Second Workshopon Statistical Machine Translation, pages 9?16.Boullier, P. 2003.
Supertagging: A non-statisticalparsing-based approach.
In In Proceedings IWPT-2003, volume 3, pages 55?65.Chen, John and Owen Rambow.
2003.
Use of deeplinguistic features for the recognition and labelingof semantic arguments.
In Proceedings of EMNLP-2003, pages 41?48.Clark, Stephen and James R. Curran.
2004.
Theimportance of supertagging for wide-coverage ccgparsing.
In Proceedings of COLING-04, pages 282?288.Clark, S. and J.R. Curran.
2007.
Perceptron train-ing for a wide-coverage lexicalized-grammar parser.In Proceedings of the Workshop on Deep LinguisticProcessing, pages 9?16.Clark, Stephen.
2002.
Supertagging for combinatorycategorial grammar.
In Proceedings of the 6th In-ternational Workshop on Tree Adjoining Grammarsand Related Frameworks (TAG+ 6), pages 19?24.Collins, M. 2002.
Discriminative training meth-ods for hidden markov models: Theory and exper-iments with perceptron algorithms.
In Proceedingsof EMNLP-2002, pages 1?8.Hal Daume?
III and Daniel Marcu.
2005.
Learningas search optimization: Approximate large marginmethods for structured prediction.
In InternationalConference on Machine Learning (ICML), pages169?176.Hassan, Hany, Mary Hearne, and Andy Way.
2007.Supertagged phrase-based statistical machine trans-lation.
In Proceedings of ACL-2007, pages 288?295.Kiefer, Bernd and Hans-Ulrich Krieger.
2000.
Acontext-free approximation of head-driven phrasestructure grammar.
In Proceedings of IWPT-2000,pages 135?146.Matsuzaki, Takuya, Yusuke Miyao, and Jun?ichi Tsu-jii.
2007.
Efficient HPSG Parsing with Super-tagging and CFG-filtering.
In Proceedings ofIJCAI-07, pages 1671?1676.Miyao, Yusuke.
2005.
Probabilistic disambiguationmodels for wide-coverage HPSG parsing.
In Pro-ceedings of the 43rd AnnualMeeting on Associationfor Computational Linguistics, pages 83?90.Miyao, Yusuke.
2006.
From Linguistic Theory to Syn-tactic Analysis: Corpus-Oriented Grammar Devel-opment and Feature Forest Model.
Ph.D. Disserta-tion, The University of Tokyo.1288Ninomiya, Takashi, Yoshimasa Tsuruoka, TakuyaMatsuzaki, and Yusuke Miyao.
2006.
Extremelylexicalized models for accurate and fast HPSG pars-ing.
In Proceedings of EMNLP-2006, pages 155?163.Ninomiya, T., T. Matsuzaki, Y. Miyao, and J. Tsujii.2007.
A log-linear model with an n-gram referencedistribution for accurate HPSG parsing.
In Proceed-ings of IWPT-2007, pages 60?68.Pollard, Carl and Ivan A.
Sag.
1994.
Head-drivenPhrase Structure Grammar.
University of Chicago/ CSLI.Prins, R. and G. Van Noord.
2001.
UnsupervisedPos-Tagging Improves Parsing Accuracy And Pars-ing Efficiency.
In Proceedings of IWPT-2001, pages154?165.Zhang, Yao-zhong, Takuya Matsuzaki, and Jun?ichiTsujii.
2009.
HPSG Supertagging: A Sequence La-beling View.
In Proceedings of IWPT-2009, pages210?213.1289
