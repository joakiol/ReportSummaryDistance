Proceedings of ACL-08: HLT, Short Papers (Companion Volume), pages 161?164,Columbus, Ohio, USA, June 2008. c?2008 Association for Computational LinguisticsPartial Matching Strategy for Phrase-based Statistical Machine TranslationZhongjun He1,2 and Qun Liu1 and Shouxun Lin11Key Laboratory of Intelligent Information ProcessingInstitute of Computing TechnologyChinese Academy of SciencesBeijing, 100190, China2Graduate University of Chinese Academy of SciencesBeijing, 100049, China{zjhe,liuqun,sxlin}@ict.ac.cnAbstractThis paper presents a partial matching strat-egy for phrase-based statistical machine trans-lation (PBSMT).
Source phrases which do notappear in the training corpus can be trans-lated by word substitution according to par-tially matched phrases.
The advantage of thismethod is that it can alleviate the data sparse-ness problem if the amount of bilingual corpusis limited.
We incorporate our approach intothe state-of-the-art PBSMT system Moses andachieve statistically significant improvementson both small and large corpora.1 IntroductionCurrently, most of the phrase-based statistical ma-chine translation (PBSMT) models (Marcu andWong, 2002; Koehn et al, 2003) adopt full matchingstrategy for phrase translation, which means that aphrase pair (f?
, e?)
can be used for translating a sourcephrase f?
, only if f?
= f?
.
Due to lack of generaliza-tion ability, the full matching strategy has some lim-itations.
On one hand, the data sparseness problemis serious, especially when the amount of the bilin-gual data is limited.
On the other hand, for a certainsource text, the phrase table is redundant since mostof the bilingual phrases cannot be fully matched.In this paper, we address the problem of trans-lation of unseen phrases, the source phrases thatare not observed in the training corpus.
Thealignment template model (Och and Ney, 2004)enhanced phrasal generalizations by using wordsclasses rather than the words themselves.
But thephrases are overly generalized.
The hierarchicalphrase-based model (Chiang, 2005) used hierar-chical phrase pairs to strengthen the generalizationability of phrases and allow long distance reorder-ings.
However, the huge grammar table greatly in-creases computational complexity.
Callison-Burchet al (2006) used paraphrases of the trainig corpusfor translating unseen phrases.
But they only foundand used the semantically similar phrases.
Anothermethod is to use multi-parallel corpora (Cohn andLapata, 2007; Utiyama and Isahara, 2007) to im-prove phrase coverage and translation quality.This paper presents a partial matching strategy fortranslating unseen phrases.
When encountering un-seen phrases in a source sentence, we search par-tially matched phrase pairs from the phrase table.Then we keep the translations of the matched partand translate the unmatched part by word substitu-tion.
The advantage of our approach is that we alle-viate the data sparseness problem without increasingthe amount of bilingual corpus.
Moreover, the par-tially matched phrases are not necessarily synony-mous.
We incorporate the partial matching methodinto the state-of-the-art PBSMT system, Moses.
Ex-periments show that, our approach achieves statis-tically significant improvements not only on smallcorpus, but also on large corpus.2 Partial Matching for PBSMT2.1 Partial MatchingWe use matching similarity to measure how well thesource phrases match each other.
Given two sourcephrases f?J1 and f?
?J1 , the matching similarity is com-puted as:161?/P {I/N <?/N u?/V ?
?/Nissued warning to the American people?/P /N <?/N ?5/V ?/Nbring advantage to the Taiwan peopleFigure 1: An example of partially matched phrases withthe same POS sequence and word alignment.SIM(f?J1 , f?
?J1 ) =?Jj=1 ?
(fj , f ?j)J (1)where,?
(f, f ?)
={1 if f = f ?0 otherwise (2)Therefore, partial matching takes full matching(SIM(f?
, f?)
= 1.0) as a special case.
Note that inorder to improve search efficiency, we only considerthe partially matched phrases with the same length.In our experiments, we use a matching threshold?
to tune the precision of partial matching.
Lowthreshold indicates high coverage of unseen phrases,but will suffer from much noise.
In order to alleviatethis problem, we search partially matched phrasesunder the constraint that they must have the sameparts-of-speech (POS) sequence.
See Figure 1 forillustration.
Although the matching similarity of thetwo phrases is only 0.2, as they have the same POSsequence, the word alignments are the same.
There-fore, the lower source phrase can be translated ac-cording to the upper phrase pair with correct wordreordering.
Furthermore, this constraint can sharplydecrease the computational complexity since thereis no need to search the whole phrase table.2.2 Translating Unseen PhrasesWe translate an unseen phrase fJ1 according to thepartially matched phrase pair (f ?J1 , e?I1, a?)
as follows:1.
Compare each word between fJ1 and f ?J1 to getthe position set of the different words: P ={j|fj 6= f ?j , j = 1, 2, .
.
.
, J};2.
Remove f ?j from f ?J1 and e?aj from e?I1, wherej ?
P ;3.
Find the translation e for fj(j ?
P ) from thephrase table and put it into the position aj ine?I1 according to the word alignment a?.u?U-?Iu?-??.
?arrived in Prague last eveningu-?arrived inarrived in Thailand yesterdayFigure 2: An example of phrase translation.Figure 2 shows an example.
In fact, we create atranslation template dynamically in step 2:?u X1 -?
X2, arrived in X2 X1?
(3)Here, on the source side, each of the non-terminalX corresponds to a single source word.
In addition,the removed sub-phrase pairs should be consistentwith the word alignment matrix.Following conventional PBSMT models, we use4 features to measure phrase translation quality: thetranslation weights p(f?
|e?)
and p(e?|f?
), the lexicalweights pw(f?
|e?)
and pw(e?|f?).
The new constructedphrase pairs keep the translation weights of their?parent?
phrase pair.
The lexical weights are com-puted by word substitution.
Suppose S{(f ?, e?)}
isthe pair set in (f?
?,e??,a?)
which replaced by S{(f, e)}to create the new phrase pair (f?
,e?,a?
), the lexicalweight is computed as:pw(f?
|e?, a?)=pw(f?
?|e?
?, a?)
??
(f,e)?S{(f,e)} pw(f |e)?
(f ?,e?
)?S{(f ?,e?)}
pw(f ?|e?
)(4)Therefore, the newly constructed phrase pairs can beused for decoding as they have already existed in thephrase table.2.3 Incorporating Partial Matching into thePBSMT ModelIn this paper, we incorporate the partial matchingstrategy into the state-of-the-art PBSMT system,Moses1.
Given a source sentence, Moses firstlyuses the full matching strategy to search all possi-ble translation options from the phrase table, andthen uses a beam-search algorithm for decoding.1http://www.statmt.org/moses/162Therefore, we do incorporation by performing par-tial matching for phrase translation before decod-ing.
The advantage is that the main search algorithmneed not be changed.For a source phrase f?
, we search partiallymatched phrase pair (f?
?, e?
?, a?)
from the phrase table.If SIM(f?
, f?
?
)=1.0, which means f?
is observed inthe training corpus, thus e??
can be directly stored as atranslation option.
However, if ?
?
SIM(f?
, f?
?)
<1.0, we construct translations for f?
according to Sec-tion 2.2.
Then the newly constructed translations arestored as translation options.Moses uses translation weights and lexicalweights to measure the quality of a phrase transla-tion pair.
For partial matching, besides these fea-tures, we add matching similarity SIM(f?
, f?
?)
as anew feature.
For a source phrase, we select top Ntranslations for decoding.
In Moses, N is set by thepruning parameter ttable-limit.3 ExperimentsWe carry out experiments on Chinese-to-Englishtranslation on two tasks: Small-scale task, the train-ing corpus consists of 30k sentence pairs (840K +950K words); Large-scale task, the training cor-pus consists of 2.54M sentence pairs (68M + 74Mwords).
The 2002 NIST MT evaluation test data isused as the development set and the 2005 NIST MTtest data is the test set.
The baseline system we usedfor comparison is the state-of-the-art PBSMT sys-tem, Moses.We use the ICTCLAS toolkit2 to perform Chineseword segmentation and POS tagging.
The trainingscript of Moses is used to train the bilingual corpus.We set the maximum length of the source phraseto 7, and record word alignment information in thephrase table.
For the language model, we use theSRI Language Modeling Toolkit (Stolcke, 2002) totrain a 4-gram model on the Xinhua portion of theGigaword corpus.To run the decoder, we set ttable-limit=20,distortion-limit=6, stack=100.
The translation qual-ity is evaluated by BLEU-4 (case-sensitive).
We per-form minimum-error-rate training (Och, 2003) totune the feature weights of the translation model tomaximize the BLEU score on development set.2http://www.nlp.org.cn/project/project.php?proj id=6?
1.0 0.7 0.5 0.3 0.1BLEU 24.44 24.43 24.86 25.31 25.13Table 1: Effect of matching threshold on BLEU score.3.1 Small-scale TaskTable 1 shows the effect of matching threshold ontranslation quality.
The baseline uses full matching(?=1.0) for phrase translation and achieves a BLEUscore of 24.44.
With the decrease of the matchingthreshold, the BLEU scores increase.
when ?=0.3,the system obtains the highest BLEU score of 25.31,which achieves an absolute improvement of 0.87over the baseline.
However, if the threshold con-tinue decreasing, the BLEU score decreases.
Thereason is that low threshold increases noise for par-tial matching.The effect of matching threshold on the coverageof n-gram phrases is shown in Figure 3.
When us-ing full matching (?=1.0), long phrases (length?3)face a serious data sparseness problem.
With the de-crease of the threshold, the coverage increases.01020304050607080901001  2  3  4  5  6  7coverageratioonthe test setphrase length?=1.0?=0.7?=0.5?=0.3?=0.1Figure 3: Effect of matching threshold on the coverage ofn-gram phrases.Table 2 shows the phrase number of 1-best out-put under ?=1.0 and ?=0.3.
When ?=1.0, the longphrases (length?3) only account for 2.9% of the to-tal phrases.
When ?=0.3, the number increases to10.7%.
Moreover, the total phrase of ?=0.3 is lessthan that of ?=1.0, since source text is segmentedinto more long phrases under partial matching, andmost of the long phrases are translated from partiallymatched phrases (the row 0.3?
SIM <1.0).3.2 Large-scale TaskFor this task, the BLEU score of the baseline is30.45.
However, for partial matching method with163Phrase Length 1 2 3 4 5 6 7 total?=1.0 19485 4416 615 87 12 2 1 24618SIM=1.0 14750 2977 387 48 10 1 0?=0.3 0.3?
SIM <1.0 0 1196 1398 306 93 17 12 21195Table 2: Phrase number of 1-best output.
?=1.0 means full matching.
For ?=0.3, SIM=1.0 means full matching,0.3 ?
SIM < 1.0 means partial matching.
?=0.53, the BLEU score is 30.96, achieving an ab-solute improvement of 0.51.
Using Zhang?s signif-icant tester (Zhang et al, 2004), both the improve-ments on the two tasks are statistically significant atp < 0.05.The improvement on large-scale task is less thanthat on small-scale task since larger corpus relievesdata sparseness.
However, the partial matching ap-proach can also improve translation quality by usinglong phrases.
For example, the segmentation andtranslation for the Chinese sentence ???L??????
are as follows:Full matching:?
| ?L?
|?
| |??
|?long term | economic output | , but | the | trend | willPartial matching:?
| ?L????
|?but | the long-term trend of economic output | willHere the source phrase ?
?L ?
 ?
???
cannot be fully matched.
Thus the decoderbreaks it into 4 short phrases, but performs an in-correct reordering.
Using partial matching, the longphrase is translated correctly since it can partiallymatched the phrase pair ?
?Lu7,??
?the inevitable trend of economic development?.3.3 ConclusionThis paper presents a partial matching strategy forphrase-based statistical machine translation.
Phraseswhich are not observed in the training corpus canbe translated according to partially matched phrasesby word substitution.
Our method can relieve datasparseness problem without increasing the amountof the corpus.
Experiments show that our approachachieves statistically significant improvements overthe state-of-the-art PBSMT system Moses.In future, we will study sophisticated partialmatching methods, since current constraints are ex-cessively strict.
Moreover, we will study the effect3Due to time limit, we do not tune the threshold for large-scale task.of word alignment on partial matching, which mayaffect word substitution and reordering.AcknowledgmentsWe would like to thank Yajuan Lv and Yang Liufor their valuable suggestions.
This work was sup-ported by the National Natural Science Foundationof China (NO.
60573188 and 60736014), and theHigh Technology Research and Development Pro-gram of China (NO.
2006AA010108).ReferencesC.
Callison-Burch, P. Koehn, and M. Osborne.
2006.Improved statistical machine translation using para-phrases.
In Proc.
of NAACL06, pages 17?24.D.
Chiang.
2005.
A hierarchical phrase-based modelfor statistical machine translation.
In Proc.
of ACL05,pages 263?270.T.
Cohn and M. Lapata.
2007.
Machine translation bytriangulation: Making effective use of multi-parallelcorpora.
In Proc.
of ACL07, pages 728?735.P.
Koehn, F. J. Och, and D. Marcu.
2003.
Statisticalphrase-based translation.
In Proc.
of HLT-NAACL03,pages 127?133.D.
Marcu and W. Wong.
2002.
A phrasebased jointprobabilitymodel for statistical machine translation.
InProc.
of EMNLP02, pages 133?139.F.
J. Och and H. Ney.
2004.
The alignment templateapproach to statistical machine translation.
Computa-tional Linguistics, 30:417?449.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL03, pages 160?167.A.
Stolcke.
2002.
Srilm ?
an extensible language model-ing toolkit.
In Proc.
of ICSLP02, pages 901?904.M.
Utiyama and H. Isahara.
2007.
A comparison of pivotmethods for phrase-based statistical machine transla-tion.
In Proc.
of NAACL-HLT07, pages 484?491.Y.
Zhang, S. Vogel, and A. Waibel.
2004.
Interpretingbleu/nist scores: How much improvement do we needto have a better system?
In Proc.
of LREC04, pages2051?2054.164
