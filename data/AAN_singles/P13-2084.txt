Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 474?478,Sofia, Bulgaria, August 4-9 2013. c?2013 Association for Computational LinguisticsText Classification from Positive and Unlabeled Data using MisclassifiedData CorrectionFumiyo Fukumoto and Yoshimi Suzuki and Suguru MatsuyoshiInterdisciplinary Graduate School of Medicine and EngineeringUniversity of Yamanashi, Kofu, 400-8511, JAPAN{fukumoto,ysuzuki,sugurum}@yamanashi.ac.jpAbstractThis paper addresses the problem of deal-ing with a collection of labeled trainingdocuments, especially annotating negativetraining documents and presents a methodof text classification from positive and un-labeled data.
We applied an error detec-tion and correction technique to the re-sults of positive and negative documentsclassified by the Support Vector Machines(SVM).
The results using Reuters docu-ments showed that the method was compa-rable to the current state-of-the-art biased-SVM method as the F-score obtained byour method was 0.627 and biased-SVMwas 0.614.1 IntroductionText classification using machine learning (ML)techniques with a small number of labeled data hasbecome more important with the rapid increase involume of online documents.
Quite a lot of learn-ing techniques e.g., semi-supervised learning, self-training, and active learning have been proposed.Blum et al proposed a semi-supervised learn-ing approach called the Graph Mincut algorithmwhich uses a small number of positive and nega-tive examples and assigns values to unlabeled ex-amples in a way that optimizes consistency in anearest-neighbor sense (Blum et al, 2001).
Cabr-era et al described a method for self-training textcategorization using the Web as the corpus (Cabr-era et al, 2009).
The method extracts unlabeleddocuments automatically from the Web and ap-plies an enriched self-training for constructing theclassifier.Several authors have attempted to improve clas-sification accuracy using only positive and unla-beled data (Yu et al, 2002; Ho et al, 2011).
Liuet al proposed a method called biased-SVM thatuses soft-margin SVM as the underlying classi-fiers (Liu et al, 2003).
Elkan and Noto proposeda theoretically justified method (Elkan and Noto,2008).
They showed that under the assumptionthat the labeled documents are selected randomlyfrom the positive documents, a classifier trained onpositive and unlabeled documents predicts proba-bilities that differ by only a constant factor fromthe true conditional probabilities of being positive.They reported that the results were comparable tothe current state-of-the-art biased SVM method.The methods of Liu et al and Elkan et al modela region containing most of the available positivedata.
However, these methods are sensitive to theparameter values, especially the small size of la-beled data presents special difficulties in tuningthe parameters to produce optimal results.In this paper, we propose a method for elimi-nating the need for manually collecting trainingdocuments, especially annotating negative train-ing documents based on supervised ML tech-niques.
Our goal is to eliminate the need for manu-ally collecting training documents, and hopefullyachieve classification accuracy from positive andunlabeled data as high as that from labeled posi-tive and labeled negative data.
Like much previouswork on semi-supervised ML, we apply SVM tothe positive and unlabeled data, and add the classi-fication results to the training data.
The differenceis that before adding the classification results, weapplied the MisClassified data Detection and Cor-rection (MCDC) technique to the results of SVMlearning in order to improve classification accu-racy obtained by the final classifiers.2 Framework of the SystemThe MCDC method involves category error cor-rection, i.e., correction of misclassified candidates,while there are several strategies for automati-cally detecting lexical/syntactic errors in corpora(Abney et al, 1999; Eskin, 2000; Dickinson and474trainingUPP1 N1N1trainingSVMMCDCN1RC1U?N1CP1CN1SVMMCDC?
Final resultsSVMtrainingselectionclassificationPCPN1RC1N1RC2N1CNMCDCFigure 1: Overview of the systemMeurers., 2005; Boyd et al, 2008) or categoricaldata errors (Akoglu et al, 2013).
The method firstdetects error candidates.
As error candidates, wefocus on support vectors (SVs) extracted from thetraining documents by SVM.
Training by SVM isperformed to find the optimal hyperplane consist-ing of SVs, and only the SVs affect the perfor-mance.
Thus, if some training document reducesthe overall performance of text classification be-cause of an outlier, we can assume that the docu-ment is a SV.Figure 1 illustrates our system.
First, we ran-domly select documents from unlabeled data (U )where the number of documents is equal to that ofthe initial positive training documents (P1).
We setthese selected documents to negative training doc-uments (N1), and apply SVM to learn classifiers.Next, we apply the MCDC technique to the re-sults of SVM learning.
For the result of correction(RC1)1, we train SVM classifiers, and classify theremaining unlabeled data (U \ N1).
For the re-sult of classification, we randomly select positive(CP1) and negative (CN1) documents classifiedby SVM and add to the SVM training data (RC1).We re-train SVM classifiers with the training doc-uments, and apply the MCDC.
The procedure isrepeated until there are no unlabeled documentsjudged to be either positive or negative.
Finally,the test data are classified using the final classi-fiers.
In the following subsections, we present theMCDC procedure shown in Figure 2.
It consistsof three steps: extraction of misclassified candi-dates, estimation of error reduction, and correctionof misclassified candidates.1The manually annotated positive examples are not cor-rected.Extractionof miss-classifiedcandidatesTraining data DtestlearningD ?SV (Support vectors)Estimation oferror reductionclassificationSV label?NB labelD ?Error candidatesCorrection ofmisclassified candidatesD1D2Final resultsError candidatesSVM NBLoss functionJudgment using loss valuesFigure 2: The MCDC procedure2.1 Extraction of misclassified candidatesLet D be a set of training documents and xk ?
{x1, x2, ?
?
?, xm} be a SV of negative or positivedocuments obtained by SVM.We remove ?mk=1xkfrom the training documents D. The resultingD \ ?mk=1xk is used for training Naive Bayes(NB) (McCallum, 2001), leading to a classifica-tion model.
This classification model is tested oneach xk, and assigns a positive or negative label.If the label is different from that assigned to xk,we declare xk an error candidate.2.2 Estimation of error reductionWe detect misclassified data from the extractedcandidates by estimating error reduction.
The es-timation of error reduction is often used in ac-tive learning.
The earliest work is the method ofRoy and McCallum (Roy and McCallum, 2001).They proposed a method that directly optimizesexpected future error by log-loss or 0-1 loss, usingthe entropy of the posterior class distribution ona sample of unlabeled documents.
We used theirmethod to detect misclassified data.
Specifically,we estimated future error rate by log-loss function.It uses the entropy of the posterior class distribu-tion on a sample of the unlabeled documents.
Aloss function is defined by Eq (1).EP?D2?
(xk,yk) = ?1| X |?x?X?y?YP (y|x)?
log(P?D2?(xk,yk)(y|x)).
(1)Eq (1) denotes the expected error of the learner.P (y | x) denotes the true distribution of out-put classes y ?
Y given inputs x. X denotes a475set of test documents.
P?D2?
(xk,yk)(y | x) showsthe learner?s prediction, and D2 denotes the train-ing documents D except for the error candidates?lk=1xk.
If the value of Eq (1) is sufficientlysmall, the learner?s prediction is close to the trueoutput distribution.We used bagging to reduce variance of P (y | x)as it is unknown for each test document x. Moreprecisely, from the training documents D, a dif-ferent training set consisting of positive and nega-tive documents is created2.
The learner then cre-ates a new classifier from the training documents.The procedure is repeated m times3, and the finalclass posterior for an instance is taken to be the un-weighted average of the class posteriori for each ofthe classifiers.2.3 Correction of misclassified candidatesFor each error candidate xk, we calculated the ex-pected error of the learner, EP?D2?
(xk,yk old) andEP?D2?
(xk,yk new) by using Eq (1).
Here, yk oldrefers to the original label assigned to xk, andyk new is the resulting category label estimated byNB classifiers.
If the value of the latter is smallerthan that of the former, we declare the documentxk to be misclassified, i.e., the label yk old is anerror, and its true label is yk new.
Otherwise, thelabel of xk is yk old.3 Experiments3.1 Experimental setupWe chose the 1996 Reuters data (Reuters, 2000)for evaluation.
After eliminating unlabeled doc-uments, we divided these into three.
The data(20,000 documents) extracted from 20 Aug to 19Sept is used as training data indicating positiveand unlabeled documents.
We set the range of ?from 0.1 to 0.9 to create a wide range of scenar-ios, where ?
refers to the ratio of documents fromthe positive class first selected from a fold as thepositive set.
The rest of the positive and negativedocuments are used as unlabeled data.
We usedcategories assigned to more than 100 documentsin the training data as it is necessary to examinea wide range of ?
values.
These categories are 88in all.
The data from 20 Sept to 19 Nov is used2We set the number of negative documents extracted ran-domly from the unlabeled documents to the same number ofpositive training documents.3We set the number of m to 100 in the experiments.as a test set X, to estimate true output distribu-tion.
The remaining data consisting 607,259 from20 Nov 1996 to 19 Aug 1997 is used as a test datafor text classification.
We obtained a vocabularyof 320,935 unique words after eliminating wordswhich occur only once, stemming by a part-of-speech tagger (Schmid, 1995), and stop word re-moval.
The number of categories per documents is3.21 on average.
We used the SVM-Light package(Joachims, 1998)4.
We used a linear kernel and setall parameters to their default values.We compared our method, MCDC with threebaselines: (1) SVM, (2) Positive Example-BasedLearning (PEBL) proposed by (Yu et al, 2002),and (3) biased-SVM (Liu et al, 2003).
We chosePEBL because the convergence procedure is verysimilar to our framework.
Biased-SVM is thestate-of-the-art SVM method, and often used forcomparison (Elkan and Noto, 2008).
To makecomparisons fair, all methods were based on a lin-ear kernel.
We randomly selected 1,000 positiveand 1,000 negative documents classified by SVMand added to the SVM training data in each itera-tion5.
For biased-SVM, we used training data andclassified test documents directly.
We empiricallyselected values of two parameters, ?c?
(trade-offbetween training error and margin) and ?j?, i.e.,cost (cost-factor, by which training errors on posi-tive examples) that optimized the F-score obtainedby classification of test documents.The positive training data in SVM are assignedto the target category.
The negative training dataare the remaining data except for the documentsthat were assigned to the target category, i.e., thisis the ideal method as we used all the training datawith positive/negative labeled documents.
Thenumber of positive training data in other threemethods depends on the value of ?, and the restof the positive and negative documents were usedas unlabeled data.3.2 Text classificationClassification results for 88 categories are shownin Figure 3.
Figure 3 shows micro-averaged F-score against the ?
value.
As expected, the re-sults obtained by SVM were the best among all?
values.
However, this is the ideal methodthat requires 20,000 documents labeled posi-tive/negative, while other methods including our4http://svmlight.joachims.org5We set the number of documents up to 1,000.476SVM PEBL Biased-SVM MCDCLevel (# of Cat) Cat F Cat F (Iter) Cat F (Iter) Cat F (Iter)Best GSPO .955 GSPO .802 (26) CCAT .939 GSPO .946 (9)Top (22) Worst GODD .099 GODD .079 (6) GODD .038 GODD .104 (4)Avg .800 .475 (19) .593 .619 (8)Best M14 .870 E71 .848 (7) M14 .869 M14 .875 (9)Second (32) Worst C16 .297 E14 .161 (14) C16 .148 C16 .150 (3)Avg .667 .383 (22) .588 .593 (7)Best M141 .878 C174 .792 (27) M141 .887 M141 .885 (8)Third (33) Worst G152 .102 C331 .179 (16) G155 .130 C331 .142 (6)Avg .717 .313 (18) .518 .557 (8)Fourth (1) ?
C1511 .738 C1511 .481 (16) C1511 .737 C1511 .719 (4)Micro Avg F-score .718 .428 (19) .614 .627 (8)Table 1: Classification performance (?
= 0.7)0.20.30.40.50.60.70.80.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9F-scoreDelta ValueSVMPEBLBiased-SVMMCDCFigure 3: F-score against the value of ?method used only positive and unlabeled docu-ments.
Overall performance obtained by MCDCwas better for those obtained by PEBL and biased-SVM methods in all ?
values, especially when thepositive set was small, e.g., ?
= 0.3, the improve-ment of MCDC over biased-SVM and PEBL wassignificant.Table 1 shows the results obtained by eachmethod with a ?
value of 0.7.
?Level?
indi-cates each level of the hierarchy and the numbersin parentheses refer to the number of categories.?Best?
and ?Worst?
refer to the best and the low-est F-scores in each level of a hierarchy, respec-tively.
?Iter?
in PEBL indicates the number of it-erations until the number of negative documentsis zero in the convergence procedure.
Similarly,?Iter?
in the MCDC indicates the number of it-erations until no unlabeled documents are judgedto be either positive or negative.
As can be seenclearly from Table 1, the results with MCDC werebetter than those obtained by PEBL in each levelof the hierarchy.
Similarly, the results were bet-?
SV Ec Err CorrectPrec Rec F0.3 227,547 54,943 79,329 .693 .649 .6700.7 141,087 34,944 42,385 .712 .673 .692Table 2: Miss-classified data correction resultster than those of biased-SVM except for the fourthlevel, ?C1511?
(Annual results).
The average num-bers of iterations with MCDC and PEBL were 8and 19 times, respectively.
In biased-SVM, it isnecessary to run SVM many times, as we searched?c?
and ?j?.
In contrast, MCDC does not requiresuch parameter tuning.3.3 Correction of misclassified candidatesOur goal is to achieve classification accuracy fromonly positive documents and unlabeled data ashigh as that from labeled positive and negativedata.
We thus applied a miss-classified data de-tection and correction technique for the classifica-tion results obtained by SVM.
Therefore, it is im-portant to examine the accuracy of miss-classifiedcorrection.
Table 2 shows detection and correctionperformance against all categories.
?SV?
showsthe total number of SVs in 88 categories in all iter-ations.
?Ec?
refers to the total number of extractederror candidates.
?Err?
denotes the number of doc-uments classified incorrectly by SVM and addedto the training data, i.e., the number of documentsthat should be assigned correctly by the correctionprocedure.
?Prec?
and ?Rec?
show the precisionand recall of correction, respectively.Table 2 shows that precision was better than re-call with both ?
values, as the precision obtainedby ?
value = 0.3 and 0.7 were 4.4% and 3.9%improvement against recall values, respectively.These observations indicated that the error candi-dates extracted by our method were appropriately477corrected.
In contrast, there were still other doc-uments that were miss-classified but not extractedas error candidates.
We extracted error candidatesusing the results of SVM and NB classifiers.
En-semble of other techniques such as boosting andkNN for further efficacy gains seems promising totry with our method.4 ConclusionThe research described in this paper involved textclassification using positive and unlabeled data.Miss-classified data detection and correction tech-nique was incorporated in the existing classifica-tion technique.
The results using the 1996 Reuterscorpora showed that the method was comparableto the current state-of-the-art biased-SVM methodas the F-score obtained by our method was 0.627and biased-SVM was 0.614.
Future work will in-clude feature reduction and investigation of otherclassification algorithms to obtain further advan-tages in efficiency and efficacy in manipulatingreal-world large corpora.ReferencesS.
Abney, R. E. Schapire, and Y.
Singer.
1999.
Boost-ing Applied to Tagging and PP Attachment.
In Proc.of the Joint SIGDAT Conference on EMNLP andVery Large Corpora, pages 38?45.L.
Akoglu, H. Tong, J. Vreeken, and C. Faloutsos.2013.
Fast and Reliable Anomaly Detection in Cate-gorical Data.
In Proc.
of the CIKM, pages 415?424.A.
Blum, J. Lafferty, M. Rwebangira, and R. Reddy.2001.
Learning from Labeled and Unlabeled Datausing Graph Mincuts.
In Proc.
of the 18th ICML,pages 19?26.A.
Boyd, M. Dickinson, and D. Meurers.
2008.
OnDetecting Errors in Dependency Treebanks.
Re-search on Language and Computation, 6(2):113?137.R.
G. Cabrera, M. M. Gomez, P. Rosso, and L. V.Pineda.
2009.
Using the Web as Corpus forSelf-Training Text Categorization.
Information Re-trieval, 12(3):400?415.M.
Dickinson and W. D. Meurers.
2005.
DetectingErrors in Discontinuous Structural Annotation.
InProc.
of the ACL?05, pages 322?329.C.
Elkan and K. Noto.
2008.
Learning Classifiers fromOnly Positive and Unlabeled Data.
In Proc.
of theKDD?08, pages 213?220.E.
Eskin.
2000.
Detectiong Errors within a Corpus us-ing Anomaly Detection.
In Proc.
of the 6th ANLPConference and the 1st Meeting of the NAACL,pages 148?153.C.
H. Ho, M. H. Tsai, and C. J. Lin.
2011.
ActiveLearning and Experimental Design with SVMs.
InProc.
of the JMLRWorkshop on Active Learning andExperimental Design, pages 71?84.T.
Joachims.
1998.
SVM Light Support Vector Ma-chine.
In Dept.
of Computer Science Cornell Uni-versity.B.
Liu, Y. Dai, X. Li, W. S. Lee, and P. S. Yu.
2003.Building Text Classifiers using Positive and Unla-beled Examples.
In Proc.
of the ICDM?03, pages179?188.A.
K. McCallum.
2001.
Multi-label Text Classifica-tion with a Mixture Model Trained by EM.
In Re-vised Version of Paper Appearing in AAAI?99 Work-shop on Text Learning, pages 135?168.Reuters.
2000.
Reuters Corpus Volume1 English Lan-guage.
1996-08-20 to 1997-08-19 Release Date2000-11-03 Format Version 1.N.
Roy and A. K. McCallum.
2001.
Toward OptimalActive Learning through Sampling Estimation of Er-ror Reduction.
In Proc.
of the 18th ICML, pages441?448.H.
Schmid.
1995.
Improvements in Part-of-SpeechTagging with an Application to German.
In Proc.
ofthe EACL SIGDAT Workshop, pages 47?50.H.
Yu, H. Han, and K. C-C. Chang.
2002.
PEBL: Pos-itive Example based Learning for Web Page Classi-fication using SVM.
In Proc.
of the ACM SpecialInterest Group on Knowledge Discovery and DataMining, pages 239?248.478
