Proceedings of COLING 2014, the 25th International Conference on Computational Linguistics: Technical Papers,pages 709?720, Dublin, Ireland, August 23-29 2014.Improving distributional thesauri by exploring the graph of neighborsVincent ClaveauIRISA - CNRSCampus de Beaulieu35042 Rennes, Francevincent.claveau@irisa.frEwa KijakIRISA - Univ.
of Rennes 1Campus de Beaulieu35042 Rennes, Franceewa.kijak@irisa.frOlivier FerretCEA, LISTLVIC91191 Gif-sur-Yvette, Franceolivier.ferret@cea.frAbstractIn this paper, we address the issue of building and improving a distributional thesaurus.
Wefirst show that existing tools from the information retrieval domain can be directly used in orderto build a thesaurus with state-of-the-art performance.
Secondly, we focus more specificallyon improving the obtained thesaurus, seen as a graph of k-nearest neighbors.
By exploitinginformation about the neighborhood contained in this graph, we propose several contributions.1) We show how the lists of neighbors can be globally improved by examining the reciprocity ofthe neighboring relation, that is, the fact that a word can be close of another and vice-versa.
2)We also propose a method to associate a confidence score to any lists of nearest neighbors (i.e.any entry of the thesaurus).
3) Last, we demonstrate how these confidence scores can be usedto reorder the closest neighbors of a word.
These different contributions are validated throughexperiments and offer significant improvement over the state-of-the-art.1 IntroductionDistributional thesauri are useful for many NLP tasks and their construction is an issue widely discussedfor several years (Grefenstette, 1994).
However this is still a very active research field, maintained bythe increasingly large number of available corpus and by many applications.
These thesauri associateeach of their entry with a list of words that are desired semantically close to the entry.
This notion ofproximity varies (synonymy, other paradigmatic relations, syntagmatic relations (Budanitsky and Hirst,2006; Adam et al., 2013, for a discussion)), but the methods used for the automatic construction ofthesauri are often shared.
For the most part, these methods rely on the distributional hypothesis of (Firth,1957): each word is characterized by the set of contexts in which it appears, and the semantic proximityof two words can be inferred from the proximity of their contexts.
This hypothesis has been implementedin different ways, and several propositions to improve the results have been explored (see next sectionfor a state of the art).The work presented in this article are part of this framework.
We propose several contributions onthe creation of these distributional thesauri and their improvement.
We first show that models frominformation retrieval (IR) can provide information on semantic relationships, and are thus adapted to thetask of creating these thesauri.
In addition, they offer very competitive results compared to the state ofthe art, while enjoying existing tools (Section 3).The most important part of our work then focuses on the exploitation of such semantic neighborhoodrelations.
The IR models indeed provide lists ordering all words by decreasing similarity, that form agraph of nearest neighbors.
We propose to take advantage of some of the neighborhood informationcontained in this graph and we derive three contributions.1) We globally improve neighbor lists by taking into account the reciprocity of the neighborhood rela-tionship, that is to say the fact that a word is a close neighbor of another and vice versa (Section 4).2) We also propose a method that associates each neighbor list (i.e.
each entry of the thesaurus built) witha confidence score (Section 5).
This method uses the nearest neighbor graph to estimate the probabilitiesthat a given word is the i-th neighbor of another word.This work is licenced under a Creative Commons Attribution 4.0 International License.
Page numbers and proceedings footerare added by the organizers.
License details: http://creativecommons.org/licenses/by/4.0/7093) Finally, on the basis of this work, we show how to use this confidence score and these probabilitiesto reorder the list of nearest neighbors (Section 6).
To achieve this goal, we model the reranking as anoptimization problem of assignments, solved by the Hungarian algorithm (Kuhn and Yaw, 1955).2 Related workThe notion of distributional thesaurus, as it was initially defined by Grefenstette (1994), followed by Lin(1998a) and Curran and Moens (2002), is not often considered specifically, probably because of its stronglink with the notion of semantic similarity.
As a consequence, the improvement of distributional thesaurihas been first a side effect of the improvement of the distributional similarity measures used for theirbuilding and more precisely, of the distributional data they rely on.
Both the nature of the constituents ofdistributional contexts and their weighting have been considered in this regard.
Concerning their weight-ing, Broda et al.
(2009) proposed to turn the weights of context constituents into ranks to make them lessdependent on a specific weighting function while Zhitomirsky-Geffet and Dagan (2009), extended by Ya-mamoto and Asakura (2010), defined a bootstrapping method for modifying the weights of constituentsin the distributional context of a word according to the similarity with its semantic neighbors.The nature of distributional contexts has been first considered through the distinction between window-based and syntactic co-occurrents (Grefenstette, 1994; Curran and Moens, 2002).
However, most of thework related to this issue has focused on the fact that the ?traditional?
representation of distributionalcontexts is very sparse and redundant, as illustrated by Hagiwara et al.
(2006).
Hence, several meth-ods for dimension reduction were tested in this context: from Latent Semantic Analysis (Landauer andDumais, 1997), extended for syntactic co-occurrents (Pad?o and Lapata, 2007), to Random Indexing(Sahlgren, 2001), Non-negative Matrix Factorization (Van de Cruys, 2010) and more recently, lexicalrepresentations learnt by neural networks (Huang et al., 2012; Mikolov et al., 2013).The work we present in this article follows a different perspective as our objective is to improve anexisting distributional thesaurus by relying on its structure through a reranking of its neighbors.
Such ap-proach was adopted to some extent by Zhitomirsky-Geffet and Dagan (2009) as it exploited the neighborsof an entry in an initial thesaurus for reweighting its distributional representation and finally, rerankingits neighbors.
Ferret (2013) proposed a more indirect method in which the reranking is based on thedowngrading of the neighbors that are detected as not similar to their entry through a pseudo word sensedisambiguation task: such detection occurs if a certain proportion of the occurrences of a neighbor arenot tagged as the entry.
Finally, the closest work to ours is (Ferret, 2012), which selects in an unsu-pervised way a set of examples of semantically similar words from an initial thesaurus for training aclassifier whose decision function is used for reranking the neighbors of each entry.
Its unsupervisedselection of examples is more precisely based on the symmetry of semantic similarity relations.As Ferret (2012), our work exploits a certain kind of symmetry in the relation of distributional neigh-borhood between words but extends it to a larger scale by considering the initial thesaurus as a k-nearestneighbor graph and using the relations in this graph for reranking the neighbors of each entry, similarlyto Pedronette et al.
(2014) in the context of image retrieval.3 IR models for building distributional thesauri3.1 PrinciplesAs mentioned in the state of the art, distributional approaches aim to calculate similarities betweentextual representations of word contexts.
Methods to calculate similarities from IR seem then relevantfor this problem.
For a given word, the set of contexts of all its occurrences is considered as a document.The proximity between two words is then measured on their contexts by a similarity function from IR.This idea has many links with the work from the state of the art, but seems relatively unexplored, withthe exeption of (Vechtomova and Robertson, 2012) in the specific context of similar named entities.
Itoffers the advantage of being easily implementable because of the numerous IR tools available.
Someadaptations are of course required.
In contrast to IR, the stop words are kept as well as their positionsrelative to the considered occurrence.
Lemmatization instead of stemming is performed.
For example, inthe excerpt: ?...
all forms of restrictions on freedom of expression, threats ...?, the indexing terms restriction-2,710on-1, of+1, expression+2 are added to the context of freedom noted C(freedom).
The whole set of collectedcontexts for a word is used as a query in order to find its distributional neighbors.
According to an IRsimilarity measure, the nearest words of this query (those whose contexts are closest) are returned.We tested some of the most classical similarity measures used in IR: Hellinger (Escoffier, 1978;Domeng`es and Volle, 1979), TF-IDF/cosinus, and Okapi-BM-25 (Robertson et al., 1998).
The lastmodel can be seen as a variation of TF-IDF that better takes into account the difference between doc-ument sizes.
This point is of importance since in our case the documents (namely the set of contextsof a word) are actually of very variable sizes, due to the very variable number of occurrences of eachword.
The Okapi-BM25 similarity between a word wi(C(wi) being the query), and wj(C(wj) being adocument), is given in Eqn 1.similarity(wi, wj) =?t?C(wi)(k3+ 1) ?
qtfk3+ qtf?tf ?
(k1+ 1)tf + k1?
(1?
b+ b ?dl(C(wj))dlavg)?
logn?
df(t) + 0.5df(t) + 0.5(1)qtf is the number of occurrences of the word t in the query (C(wi)), dl is the size of C(wj)), dlavgthe average size of all contexts, n is the number of documents (that means in our case the number ofconsidered words/thesaurus entries).
df(t) is the number of contexts (C(?))
containing t. Finally, k1, k3and b are some constants, with default values k1= 2, k3= 1000 and b = 0.75.
Details of these classicalIR models are not given here but can be found in (Manning et al., 2008).In the following experiments, the context of an occurrence is defined by the two words before andafter the occurrence, and we also use an adjusted version of Okapi-BM25 similarity that enhances theinfluence of the document size and gives more importance to the most discriminating context words bysetting b = 1 and putting the IDF squared to give more importance to the most discriminating contextwords.3.2 Experimental setupFor the sake of comparison, we use in our experiments the data and baselines provided by Ferret (2013).The corpus used to build the distributional thesaurus is AQUAINT-2.
It is a collection of articles frompress containing about 380 million words.
The thesaurus entries are all the nouns in the corpus witha frequency > 10.
That represents 25,000 entries (i.e.
unique nouns), denoted by n in the remaining.The corpus is labeled in parts of speech by TreeTagger (Schmid, 1994).
In this way, we can identify thenames that form the thesaurus entries and thus compare to existing work.
However this information isnot used to build the thesaurus, ensuring the portability of the method to other languages, similarly to(Freitag et al., 2005).To evaluate the built thesauri, WordNet 3.0 synonyms (Miller, 1990) and Moby (Ward, 1996) are usedas references, either separately, or jointly.
These two resources exhibit quite different and complementarycharacteristics: on the one hand, WordNet indicates strong paradigmatic links between words (synonymsor quasi-synonyms).
On the other hand, Moby groups words sharing more extended syntagmatic andparadigmatic relations, including synonymy, hyper/hypo-nymy, meronymy, but also many more complextypes such as the composition of co-hyponymy and hyponymy (abolition ?
annulment, cataclysm ?debacle) or hypernymy and co-hyponymy (abyss ?
rift, algorithm ?
routine).
As a result, WordNetprovides lists of 3 neighbors on average for the 10,473 names of the corpus it covers, while Mobyprovides lists of 50 neighbors on average for 9,216 names.
When combined, the two resources provide areference of 38 neighbors on average for 12,243 names.
It is this combination of WordNet and Moby thatwill be used as the main reference in all evaluations of this article.
Some results restricted to WordNet orMoby only as reference are also given in some cases to illustrate the impact of our methods on semanticsimlarity versus semantic relatedness relations.Through this intrinsic evaluation framework, the semantic neighbors of about half of the entries ofour thesauri are evaluated, which can be considered as a very large evaluation set compared to classicalbenchmarks such as WordSim 353 for instance (Gabrilovich and Markovitch, 2007).
This kind of intrin-sic evaluation is of course limited by the relations that are present in the resources used as gold standards,often restricted to ?classical?
relation types such as synonymy or hypernymy.
In our case, this limitation711Reference Method MAP R-Prec P@1 P@5 P@10 P@50 P@100WordNet + MobyFerret 2013 base 5.6 7.7 22.5 14.1 10.8 5.3 3.8Ferret 2013 best rerank 6.1 8.4 24.8 15.4 11.7 5.7 3.8Hellinger 2.45 2.89 9.73 6.28 5.31 4.12 3.30TF-IDF 5.40 7.28 21.73 13.74 9.59 5.17 3.49Okapi-BM25 6.72 8.41 24.82 14.65 10.85 5.16 3.66Okapi-BM25 ajusted 8.97 10.94 31.05 18.44 13.76 6.46 4.54Ferret 2014 synt 7.9 10.7 29.4 18.9 14.6 7.3 5.2WordNetFerret 2013 base 9.8 8.2 11.7 5.1 3.4 1.1 0.7Ferret 2013 best rerank 10.7 9.1 12.8 5.6 3.7 1.2 0.7Okapi-BM25 ajusted 14.17 12.22 16.97 7.10 4.47 1.41 0.84Ferret 2014 synt 13.3 11.5 15.6 6.9 4.5 1.5 0.9MobyFerret 2013 base 3.2 6.7 24.1 16.4 13.0 6.6 4.8Ferret 2013 best rerank 3.5 7.2 26.5 17.9 14.0 6.9 4.8Okapi-BM25 ajusted 5.69 9.14 32.18 21.37 16.42 8.02 5.69Ferret 2014 synt 4.8 9.4 30.6 21.7 17.3 8.9 6.5Table 1: Performance of IR models for distributional thesaurus building with the references WordNet,Moby and WordNet+Mobyholds true for WordNet?s synonyms but can be considered as far less restrictive for the related words ofMoby, due to the diversity of their underlying relation types.3.3 ResultsFor a given name, our approach by IR models returns a list of names ordered by decreasing similarity.This list is compared to the reference one by computing several classical measures (expressed in % in thefollowing): the precision after k first names, denoted P@k, the Mean Average Precision (MAP) whichis the mean of the average precision scores for each query after a reference synonym is found, and theR-precision (precision atR-th position in the ranking of results, whereR is the number of relevant namesfor the query).Table 1 indicates the performance of different models of IR similarities.
For purposes of comparison,we show the results obtained under the same conditions by Ferret (2013), with both a state of the artapproach based on using cosine similarity over pointwise mutual information between contexts (referredas base in the table), and an improved version by learning as described in section 2 (referred as bestrerank).
We also give the results on the same corpus on an approach based on syntactic co-occurrents(Ferret, 2014 in press), extracted with the Minipar syntactic parser as in (Lin, 1998b).In these early results, it is worth noting that some IR similarities are quite inefficient, including theTF alone or Hellinger similarity.
This is hardly surprising since these similarities use very basic weightsthat do not enhance the discriminative contexts of words.
The similarities that include a notion of IDFget better results in this.
Okapi BM25-based similarities offer good results.
The standard Okapi versionyields performance similar to the state of the art, and the adjusted version even widely outperforms thetwo systems from Ferret (2013), in particular in terms of overall quality (measured by the MAP).
More-over, the results of this adjusted version are comparable to those obtained with syntactic co-occurrentswhile it only exploits window-based co-occurents, known to give usually worst results than syntacticco-occurrents, without even lemmatization.
This latest version of the system serves as reference for therest of this article.4 Reciprocity in the graph of k-NNComputing all the similarities between all pairs of words produces a weighted graph of neighbors: Eachword is connected with certain strength to the n other words.
The results above do not reflect thisstructure.
The following sections aim to examine how take advantage of the neighborhood relationsembedded in this graph.
It must be first noted that some of the IR similarity measures we used are notsymmetric, including Okapi-BM25.
The similarity between a word wi, used as query, and another wordwjdoes not give the same value as the similarity between the query wjand wi.
Apart from that, even if712the similarity measure it-self is symmetric, nearest neighbor relationships are not.It seems then reasonable to assume that the reciprocity between two adjacent words (each belonging tothe k nearest neighbors of the other) is a sign of confidence on the proximity between these words.
Usingthis information to improve the previous results is discussed in this section.
In the following, ?wi(wj)denotes the rank of the word wjin the list of neighbors of wi.
?wi(wj) thus varies from 1 to n.4.1 Distributional neighborhood graphReciprocal relationship in distributional neighborhood has already been discussed and used in somework (Ferret, 2013) on distributional semantic, or more generally, on nearest neighbors graphs (Pe-dronette et al., 2014).
In these papers, the reciprocity was considered for giving a new similarity score ina simple way.
For a word wiand its neighbor wj, the maximal or the minimal rank between ?wi(wj) and?wj(wi) is taken as the new rank.
These two operators have too severe effects as only one rank is takeninto consideration to decide the final score.
This leads to highly degraded performance as shown later.Many other aggregation operators have however been proposed in other contexts with a behavior maybe more appropriate to the task, including fuzzy logic (Detyniecki, 2000).
These operators carry somesemantic that allow to comprehend their behavior, such as T-norms (fuzzy logic AND) and S-norms (orT-conorms, fuzzy OR).In this section, we test some of these operators without claiming to be exhaustive.
These are definedon [0, 1]2, 1 being the certainty.
They are used to generate a new similarity score according to:scorewi(wj) = Aggreg(1?
?wi(wj)/n, 1?
?wj(wi)/n) (2)where Aggreg is an aggregation operator.
The new scores are then used to produce a new list of nearestneighbors of wi(the higher the score, the greater proximity is proven).
We thus perceive the semanticassociated with these operators.
For example, if the aggregation function is max, we get the expectedbehavior of the fuzzy OR associated with this S-norm: wjwill be ranked very close to wiin the new listif wjwas close to wior if wiwas close to wj.
For the T-norm min, this happens if wjis close to wiandwiis close to wj.4.2 ResultsBesides the min and max aggregation operators, Figure 1 reports the results obtained with the followingT-norms (or T-norm families dependent on a parameter ?)
used as aggregation function Aggreg:TProb(x, y) = x ?
yTLukasiewicz(x, y) = max(x+ y ?
1, 0)THamacher(x, y) =x?y?+(1??)?
(x+y?x?y); ?
?
0TYager(x, y) = max(0, 1???(1?
x)?+ (1?
y)?)
; ?
> 0We also tested the standard related S-norms, obtained by generalization of the De Morgan?s law:S(x, y) = 1 ?
T (1 ?
x, 1 ?
y).
For the T-norm families dependent on a parameter, we varied thisparameter value in a systematic way.
The results reported correspond to the parameter values that maxi-mize the MAP.All these operators get very different results.
Some operators, such as min, max, Lukasiewicz, andothers for some ?, induce a threshold effect which degrades the performance: they return a default valuegenerating too much ex aequo among the neighbors, for some values of ?wi(wj) and ?wj(wi).
T-norms,focusing on pairs of words symmetrically close to each other, are too restrictive.
This is consistent withthe conclusions of the work cited: if the reciprocity condition is applied too strictly, it does not improvethe nearest neighbor lists over all the words.
In contrast, S-norms seem better able to take advantageof the ranking.
The improvements are modest in terms of overall quality (MAP), but important at someranks (e.g.
P@10).Finally, it is important to note that these results depend heavily on the resource used as reference.We tested the aggregation rank with SHamacher, ?
= 0.95, on Moby and WordNet references separately.Results are given in Table 2.
Because Wordnet is based on a synonymy relationship strong enough (andtherefore reciprocal), the performance gains on WordNet are much higher than on Moby.713Figure 1: Performance of reciprocal rank aggregation, on the reference WordNet+MobyReference MAP R-Prec P@1 P@5 P@10 P@50 P@100WordNet 9.30 (+3.75) 11.06 (+2.03) 30.42 (-2.53) 19.29 (+4.58) 14.71 (+6.92) 7.09 (+9.78) 4.86 (+7.07)+ MobyWordNet 15.05 (+6.23) 12.81 (+4.81) 17.55 (+3.41) 7.96 (+12.16) 5.07 (+13.30) 1.63 (+15.69) 0.94 (+12.23)Moby 5.90 (+3.65) 11.86 (+4.14) 31.77 (-1.27) 21.65 (+1.34) 17.0 (+3.53) 8.42 (+5.01) 5.92 (+4.12)Table 2: Performance and gains (%) of reciprocal rank aggregation, relatively to adjusted Okapi-BM25,on the references WordNet and Moby taken separately, with aggregation operator SHamacher, ?
= 0.955 Confidence estimation for a distributional neighborhood listIn the previous section, the rank ofwiin the list of neighbors ofwjis used to improve the ranking ofwjinthe list of neighbors of wi.
We can also be interested in a more general way to the relative positions of wiand wjin all neighbor lists of all the words.
Thereby, we expect to derive a more complete information.As a first step, we define a confidence criterion associated with each list of nearest neighbors, only basedon the neighborhood graph.5.1 PrincipleWe make the following assumption: the nearest neighbor list of a word w is probably of good quality ifthe distance (in terms of rank) between w and each of its neighbors wi, denoted ?
(w,wi), is consistentwith the distance observed between these same words (w, wi) in other lists.
The intuition here is thatwords supposed to be close should also be found close to the same other words.
If k nearest neighborsof w have this property, then we attribute a high confidence to the neighbor list of w.Formally, we define the confidence of the k-nearest neighbor list of w by:Q(w) =?{wi|?w(wi)?k}p(?
(w,wi) = ?w(wi)) (3)where p(?
(w,wi) = ?w(wi)) is the probability that wiis the ?w(wi)-th neighbor of w. The problem isthen to estimate the probability distribution p(?
(w,wi)) for each pair of words (w,wi).
To achieve thisgoal, we use the Parzen windows which is a method for nonparametric density estimation.
We describebelow how this classic method (Parzen, 1962; Wasserman, 2005) is applied in our context.5.2 Parzen-window density estimationLet xab= ?
(wa, wb) be the distance (in terms of ranks) between two words waand wbin a list of neigh-bors of any given word.
Considering the n words of the thesaurus, we have a sample of n realizationsassumed iid: (x1ab, x2ab, ... , xnab), which are the observed distances between waet wbin each (complete)neighbor list of each word.
These counts can be represented by an histogram as illustrated in Figure 2 (a).Using the Parzen window technique, we can then estimate the probability density of xabwith a kernel714(a) (b)Figure 2: (a) Example of two distributions of distances xaband xacbetween a word waand two of itsneighbors wband wc, represented as histograms (blue and red) (b) Same distributions represented bydensities estimated with the Parzen-windows method.density estimator with Eqn 4 where h is a smoothing parameter called the bandwidth, andK(?)
is a kernelthat we choose Gaussian.
The resulting density is illustrated in Figure 2 (b).p?h(xab) =1nhn?i=1K(xab?
xiabh)with K(u) =1?2pie?u22(4)Thus, the resulting probability is a mixture of Gaussians centered on each xiab.
These methods areknown to be sensitive to the bandwidth h, which controls the regularity of the estimation.
The problem ofchoosing h is crucial in density estimation and was widely discussed in the literature.
We use Silverman?srule of thumb (Silverman, 1986, page 48, eqn (3.31)) to set its value.
Under the assumption of normalityof the underlying distribution, this rule provides a simple way to calculate the optimal parameter h whenGaussian functions are used to approximate univariate data (Eqn 5 where ??
is the standard deviation ofthe samples, and q3?
q1is the interquartile range).
?h = 0.9 min(??,q3?
q11.34) n?15(5)Once these probabilities have been estimated on each of the k-nearest neighbors ofw, we can calculatethe confidence score Q(w).
The complexity of this estimation for all neighbor lists is O(k ?
n2).5.3 Using the confidence scoreThe expected benefit of using the confidence score is to have an a priori indication on the quality of aneighbor list for a given word.
Such a score may thus be useful for many applications using thesauriproduced by our approach (e.g.
for expanding queries in information retrieval tasks).
An evaluationof the confidence score through such applications would certainly be the most suitable, but beyond thescope of this article.
We use default direct assessment towards the MAP: we measure the correlationbetween MAP and the confidence score, the idea being that an entry with a neighbor list of low qualitymatches an entry with low MAP.
We use Spearman?s correlation ?
and the Kendall?s rank correlationcoefficient ?
, which do not make any assumption about linearity and compare only the order of wordsclassified according to their MAP with the order according to their confidence score.
The results of thesecoefficients are given in Table 3, along with p-value of the associated test of significance.
A coefficient715Correlation coefficient value statistical significanceKendall ?
0.37 p < 10?64Spearman ?
0.51 p < 10?64Table 3: Correlation coefficient values between the MAP and the confidence score, and their statisticalsignificance (p-value).Figure 3: Average MAP computed on words with a confidence score lower than a threshold q (x-axis,log-scale), and cumulative proportion of concerned words.value of 1 indicates a perfect correlation, 0 no correlation and -1 an inverse correlation.
A low p-value,for example < 0.05, indicates a statistically significant result.
The confidence scores are obtained withk = 20.
Other experiments, not reported here, show that this parameter k has little influence on thecorrelation, for values between 5 and 100.These measures attest to some statistically significant correlation between our confidence score and theMAP, however this correlation is imperfect and non-linear.
We compute the average MAP on neighborlists with a confidence score lower than a threshold q.
Figure 3 represents the average MAP (y-axis) infunction of the threshold q (x-axis).
It shows that the confidence score is still a good indicator of quality,as the MAP decreases with the confidence score.The confidence score can be used to improve the performance of aggregation techniques presented inSection 4 by integrating it in the final score:scorewi(wj) = Q(wj) ?
Aggreg(1?
?wi(wj)/n, 1?
?wj(wi)/n) (6)As shown in Table 4, using this information allows even greater gains than those reported in the previoussection (a Wilcoxon test (p < 0.05) (Hull, 1993) is performed to ensure that the differences are statisti-cally significant; non-significant ones are shown in italics).
In the next section, we propose another useof the confidence scores to improve results more specifically on the head of the lists, that is to say on theneighbors judged closest.Method MAP R-Prec P@1 P@5 P@10 P@50 P@100SHamacher?
= 0.95 9.61 (+7.20) 11.59 (+5.85) 30.86 (-0.53) 19.52 (+5.83) 14.76 (+7.24) 7.03 (+8.88) 4.93 (+8.67)Table 4: Performance gains (%) by reciprocal rank aggregation using the confidence score, on Word-Net+Moby reference.716Target MAP R-Prec P@1 P@5 P@10all words 9.16 (+2.17) 11.24 (+2.76) 30.73 (-1.02) 19.30 (+4.64) 14.37 (+4.44)the third of words with the lowest Q(w) 9.55 (+6.44) 11.81 (+7.99) 31.85 (+2.56) 20.43 (+10.81) 15.46 (+12.37)Table 5: Performance gains (%) of reranking with the Hungarian algorithm.6 Local rerankingThe previous method gives an overall score to the list, but one can also make use of the individualranking probabilities p(?
(wi, wj)), estimated according to the method of Parzen windows.
For a givenword w, we have for each of its neighbors wjthe probability of his current rank: p(?
(w,wj)) = ?w(wj).For a given neighbor wj, we can also calculate the probability of any other rank ?
: p(?
(w,wj)) = ?with ?
= 1, 2, ...
In this section, we propose to rely on these more local information to improve theperformance by reranking the k-nearest neighbors.6.1 Reranking by the Hungarian algorithmA simple approach would be to reorder the list based on this criterion, from the most probable neighborsto the least ones.
But ranking probability estimation for each word is imperfect, and such a rerankingstrongly degrades the results.
We therefore propose instead a method to rerank the k-nearest neighborson a more local and controlled manner: a word that was not originally in the k-nearest neighbors can notbecome a k-nearest neighbor, and a word can not be reranked too far from its original rank.Our problem is expressed by the following matrix Mprofit.
The rows correspond to words in theiroriginal ranks (denoted w1to wk), the columns to new ranks ?
at which these words can be assigned,and matrix values are the probabilities of each word wjto appear at rank ?
.
Given these probabilities,the goal is to find the most likely permutation of the k-nearest neighbors.Mprofit=???p(?
(w,w1) = 1) ?
?
?
p(?
(w,w1) = k).........p(?
(w,wk) = 1) ?
?
?
p(?
(w,wk) = k)??
?As pointed out, we want to avoid that an initially very close neighbor was moved far away and viceversa.
This constraint is added by multiplying the matrixMprofitby a penalty matrixMpenalty(see below)with the Hadamard product (element by element matrix product, denoted ?).Mpenalty=?????1k?1k?
?
?
0k?1k1 ?
?
?1k.........01k?
?
?
1????
?We then face a combinatorial optimization problem which can be solved in polynomial time by theHungarian method (Kuhn and Yaw, 1955, for a description of the algorithm) on the matrix of assignmentcostsMprofit?Mpenalty.
This algorithm was originally proposed to optimize the assignment of workers(in our case, the neighbors) on tasks (in our case, ranks), according to the profit generated by eachworker for each task (in our case, the probability that a neighbor stands at a given rank).
The resultof this algorithm therefore indicates a new rank for each word.
The algorithm converges to an optimalsolution with a complexity O(k3) (for reranking the k-nearest neighbors).6.2 ResultsTable 5 presents the performance achieved by our local reranking method compared to the adjustedOkapi-BM25 reference using the same experimental conditions as above.
As before, the consideredneighborhood is set to k = 20.
Precisions beyond this threshold are unchanged and thus not reported.We test the effectiveness of the local reranking on all neighbor lists and on a third of lists with the lowestquality scores.It appears that the reranking on the whole lists does not provide a real gain.
However, the gain issubstantial on the lists with low confidence score.
Moreover, unlike the experiments of section 4, these717gains apply by construction to the heads of lists, which are most likely to be used in practice.
Thisdifference between results on the whole set of words and on those with the lowest confidence scores canbe explained in two ways.
First, the lists with the highest confidence scores correspond largely to thelists with the best MAP, as expected and illustrated in Figure 3.
This therefore suggests a priori littleroom for improvement.
Second, regardless of MAP, we can also assume that these lists already have anoptimum arrangement of individual probabilities that explains the high confidence score.
The rerankingthus concerns only few neighbors.7 Conclusion and future workThe different contributions proposed in this article do not place themselves all at the same level.
Thethesaurus construction using tools from the IR is not a major conceptual innovation, but this approachseems curiously unexplored although it provides very competitive results while requiring minimum im-plementations through existing tools from IR.The various propositions exploiting the neighborhood graph to improve the thesaurus are part of amore original approach where the whole thesaurus is considered.
We have specifically examined theaspects of reciprocity and distance, in terms of rank, between two words to offer several contributions.The improvements obtained by aggregation over all neighbors or by the local reranking from confidencescores validate our approach.
It should be noted that the gains are small in absolute terms, but, comparedto those observed in the field, correspond to significant improvements.The various aspects of this work open up many prospects of research.
For example, many otheraggregate functions in addition to those tested in section 4 exist in the literature.
Some may even offer thepossibility of integrating the confidence score associated with each neighbor, as Choquet?s or Sugeno?sintegrals (Detyniecki, 2000).
More generally, it would be interesting to iteratively use improvementsof neighbor lists to update the confidence scores, etc., in the spirit for example of what is proposedby Pedronette et al.
(2014).
A detailled analysis of the impact of these techniques according to the type ofsemantic relation is still to be performed.
Beyond the distributional thesauri construction, the proposedmethods to compute confidence scores or reorder lists of neighbors can be applied to other problemswhere the k-nearest neighbor graphs of are built.
Also note that we have only considered a small part ofthe information carried by the neighborhood graph.
We focused on the aspects of reciprocity, but takinginto account other aspects of the graph (in particular the transitivity, or more generally its topology),could lead to further improvements.ReferencesCl?ementine Adam, C?ecile Fabre, and Philippe Muller.
2013.?Evaluer et am?eliorer une ressource distributionnelle :protocole d?annotation de liens s?emantiques en contexte.
TAL, 54(1):71?97.Bartosz Broda, Maciej Piasecki, and Stan Szpakowicz.
2009.
Rank-Based Transformation in Measuring SemanticRelatedness.
In 22ndCanadian Conference on Artificial Intelligence, pages 187?190.Alexander Budanitsky and Graeme Hirst.
2006.
Evaluating WordNet-based measures of lexical semantic related-ness.
Computational Linguistics, 32(1):13?47.James R. Curran and Marc Moens.
2002.
Improvements in automatic thesaurus extraction.
In Workshop of theACL Special Interest Group on the Lexicon (SIGLEX), pages 59?66, Philadelphia, USA.Marcin Detyniecki.
2000.
Mathematical aggregation operators and their application to video querying.
Ph.D.thesis, Universit?e de Paris 6.Dominique Domeng`es and Michel Volle.
1979.
Analyse factorielle sph?erique : une exploration.
Annales del?INSEE, 35:3?83.Bernard Escoffier.
1978.
Analyse factorielle et distances r?epondant au principe d?
?equivalence distributionnelle.Revue de statistique appliqu?ee, 26(4):29?37.Olivier Ferret.
2012.
Combining bootstrapping and feature selection for improving a distributional thesaurus.
In20thEuropean Conference on Artificial Intelligence (ECAI 2012), pages 336?341, Montpellier, France.718Olivier Ferret.
2013.
Identifying bad semantic neighbors for improving distributional thesauri.
In 51stAnnualMeeting of the Association for Computational Linguistics (ACL 2013), pages 561?571, Sofia, Bulgaria.Olivier Ferret.
2014 (in press).
Typing relations in distributional thesauri.
In N. Gala, R. Rapp, and G. Bel, editors,Advances in Language Production, Cognition and the Lexicon.
Springer.John R. Firth, 1957.
Studies in Linguistic Analysis, chapter A synopsis of linguistic theory 1930-1955, pages 1?32.Blackwell, Oxford.Dayne Freitag, Matthias Blume, John Byrnes, Edmond Chow, Sadik Kapadia, Richard Rohwer, and ZhiqiangWang.
2005.
New experiments in distributional representations of synonymy.
In Ninth Conference on Compu-tational Natural Language Learning (CoNLL), pages 25?32, Ann Arbor, Michigan, USA.Evgeniy Gabrilovich and Shaul Markovitch.
2007.
Computing semantic relatedness using wikipedia-based ex-plicit semantic analysis.
In 20thInternational Joint Conference on Artificial Intelligence (IJCAI 2007), pages6?12.Gregory Grefenstette.
1994.
Explorations in automatic thesaurus discovery.
Kluwer Academic Publishers.Masato Hagiwara, Yasuhiro Ogawa, and Katsuhiko Toyama.
2006.
Selection of effective contextual informa-tion for automatic synonym acquisition.
In 21stInternational Conference on Computational Linguistics and44thAnnual Meeting of the Association for Computational Linguistics (COLING-ACL 2006), pages 353?360,Sydney, Australia.Eric H. Huang, Richard Socher, Christopher D. Manning, and Andrew Y. Ng.
2012.
Improving word repre-sentations via global context and multiple word prototypes.
In 50th Annual Meeting of the Association forComputational Linguistics (ACL?12), pages 873?882.David Hull.
1993.
Using Statistical Testing in the Evaluation of Retrieval Experiments.
In Proc.
of the 16thAnnual ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR?93, Pittsburgh,?Etats-Unis.Harold W. Kuhn and Bryn Yaw.
1955.
The Hungarian method for the assignment problem.
Naval ResearchLogistic Quarterly, 2:83?97.Thomas K. Landauer and Susan T. Dumais.
1997.
A solution to Plato?s problem: the latent semantic analysistheory of acquisition, induction, and representation of knowledge.
Psychological review, 104(2):211?240.Dekang Lin.
1998a.
Automatic retrieval and clustering of similar words.
In 17thInternational Conference onComputational Linguistics and 36thAnnual Meeting of the Association for Computational Linguistics (ACL-COLING?98), pages 768?774, Montr?eal, Canada.Dekang Lin.
1998b.
Automatic retrieval and clustering of similar words.
In 17thInternational Conference onComputational Linguistics and 36thAnnual Meeting of the Association for Computational Linguistics (ACL-COLING?98), pages 768?774, Montr?eal, Canada.Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch?utze.
2008.
Introduction to Information Retrieval.Cambridge University Press.Tomas Mikolov, Wen-tau Yih, and Geoffrey Zweig.
2013.
Linguistic regularities in continuous space word repre-sentations.
In 2013 Conference of the North American Chapter of the Association for Computational Linguis-tics: Human Language Technologies (NAACL HLT 2013), pages 746?751, Atlanta, Georgia.George A. Miller.
1990.
WordNet: An On-Line Lexical Database.
International Journal of Lexicography, 3(4).Sebastian Pad?o and Mirella Lapata.
2007.
Dependency-based construction of semantic space models.
Computa-tional Linguistics, 33(2):161?199.Emanuel Parzen.
1962.
On estimation of a probability density function and mode.
Ann.
Math.
Stat., 33:1065?1076.Daniel Carlos Guimar?aes Pedronette, Ot?avio Augusto Bizetto Penatti, and Ricardo da Silva Torres.
2014.
Unsu-pervised manifold learning using reciprocal knn graphs in image re-ranking and rank aggregation tasks.
ImageVision Computing, 32(2):120?130.Stephen E. Robertson, Steve Walker, and Micheline Hancock-Beaulieu.
1998.
Okapi at TREC-7: Automatic AdHoc, Filtering, VLC and Interactive.
In Proc.
of the 7thText Retrieval Conference, TREC-7, pages 199?210.719Magnus Sahlgren.
2001.
Vector-based semantic analysis: Representing word meanings based on random labels.In ESSLLI 2001 Workshop on Semantic Knowledge Acquisition and Categorisation, Helsinki, Finland.Helmut Schmid.
1994.
Probabilistic part-of-speech tagging using decision trees.
In International Conference onNew Methods in Language Processing.Bernard W. Silverman.
1986.
Density estimation for statistics and data analysis.
Monographs on statistics andapplied probability.
Chapman and Hall Boca Raton, London, Glasgow, Weinheim.Tim Van de Cruys.
2010.
Mining for Meaning.
The Extraction of Lexico-semantic Knowledge from Text.
Ph.D.thesis, University of Groningen, The Netherlands.Olga Vechtomova and Stephen E. Robertson.
2012.
A domain-independent approach to finding related entities.Information Processing and Management, 48(4):654?670.Grady Ward.
1996.
Moby thesaurus.
Moby Project.Larry Wasserman.
2005.
All of Statistics: A Concise Course in Statistical Inference.
Springer Texts in Statistics.Kazuhide Yamamoto and Takeshi Asakura.
2010.
Even unassociated features can improve lexical distributionalsimilarity.
In Second Workshop on NLP Challenges in the Information Explosion Era (NLPIX 2010), pages32?39, Beijing, China.Maayan Zhitomirsky-Geffet and Ido Dagan.
2009.
Bootstrapping Distributional Feature Vector Quality.
Compu-tational Linguistics, 35(3):435?461.720
