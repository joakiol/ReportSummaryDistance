Second Joint Conference on Lexical and Computational Semantics (*SEM), Volume 2: Seventh International Workshop on SemanticEvaluation (SemEval 2013), pages 328?332, Atlanta, Georgia, June 14-15, 2013. c?2013 Association for Computational LinguisticsGU-MLT-LT: Sentiment Analysis of Short Messages usingLinguistic Features and Stochastic Gradient DescentTobias Gu?ntherUniversity of GothenburgOlof Wijksgatan 641255 Go?teborg, Swedenemail@tobias.ioLenz FurrerUniversity of ZurichBinzmu?hlestrasse 148050 Zu?rich, Switzerlandlenz.furrer@gmail.comAbstractThis paper describes the details of our systemsubmitted to the SemEval-2013 shared task onsentiment analysis in Twitter.
Our approach topredicting the sentiment of Tweets and SMSis based on supervised machine learning tech-niques and task-specific feature engineering.We used a linear classifier trained by stochas-tic gradient descent with hinge loss and elas-tic net regularization to make our predictions,which were ranked first or second in three ofthe four experimental conditions of the sharedtask.
Furthermore, our system makes use ofsocial media specific text preprocessing andlinguistically motivated features, such as wordstems, word clusters and negation handling.1 IntroductionSentiment analysis, also known as opinion min-ing, is a research field in the area of text min-ing and natural language processing, which inves-tigates the automated detection of opinions in lan-guage.
In written text, an opinion is a person?s atti-tude towards some topic, pronounced by verbal (e.g.choice of words, rhetorical figures) or non-verbalmeans (e.g.
emoticons, emphatic spelling).
Moreformally, Liu (2012) defines an opinion as the quin-tuple (ei, aij , sijkl, hk, tl) where ?ei is the name ofan entity, aij is an aspect of ei, sijkl is the sentimenton aspect aij of entity ei, hk is the opinion holder,and tl is the time when the opinion is expressed byhk.
The sentiment sijkl is positive, negative, or neu-tral, or expressed with different strength/intensitylevels [...].
When an opinion is on the entity itselfas a whole, the special aspect GENERAL is usedto denote it.
[...] ei and aij together represent theopinion target?
(Liu, 2012).With the massively growing importance of socialmedia in everyday life, being able to automaticallyfind and classify attitudes in written text allows forestimating the mood of a large group of people, e.g.towards a certain event, service, product, matter offact or the like.
As working with the very short andinformal texts typical for social networks poses chal-lenges not encountered in more traditional text gen-res, the International Workshop on Semantic Evalu-ation (SemEval) 2013 has a shared task on sentimentanalysis in microblogging texts, which is detailed inWilson et al(2013).
The task requires sentimentanalysis of Twitter1 and SMS messages and com-prises two subtasks, one of which deals with deter-mining the sentiment of a given message fragmentdepending on its context (Task A) and one on over-all message polarity classification (Task B).We treat both tasks as document-level senti-ment classification tasks, which we define ac-cording to Liu (2012) as determining the opinion( ,GENERAL, s, , ) of a given message, wheres ?
{positive, negative, neutral} and ?the entity e,opinion holder h, and time of opinion t are assumedknown or irrelevant?
(Liu, 2012).
For Task A weonly consider the marked fraction of the message tobe given.This introduction is followed by sections dis-cussing related work (2), details of our system (3),experiments (4) and results and conclusion (5).1a popular microblogging service on the Internet, seehttp://twitter.com3282 Related WorkPrevious approaches to sentiment analysis of mi-croblogging texts make use of a wide range of fea-tures, including unigrams, n-grams, part-of-speechtags and polarity values from (usually hand-crafted)sentiment lexicons.
O?Connor et al(2010) exam-ine tweets concerned with the 2009 US presiden-tial elections, relying solely on the occurrence ofwords from a sentiment lexicon.
Nielsen (2011) in-vestigates the impact of including internet slang andobscene language when building a sentiment lexi-con.
Barbosa and Feng (2010) make use of threedifferent sentiment detection websites to label Twit-ter data, while Davidov et al(2010), Kouloumpis etal.
(2011) and Pak and Paroubek (2010) use Twit-ter hashtags and emoticons as labels.
Speriosu etal.
(2011) propagate information from seed labelsalong a linked structure that includes Twitter?s fol-lower graph, and Saif et al(2012) specifically ad-dress the data-sparsity problem by using semanticsmoothing and topic extraction.3 System DescriptionIn this section we present the details of our senti-ment analysis system, which was implemented inthe Python programming language and is publiclyavailable online.2 We used the same preprocessing,feature extraction and learning algorithm for bothsubtasks, only the hyperparameters of the machinelearning algorithm were adjusted to the respectivedataset.3.1 PreprocessingTokenization of the messages was done using a sim-ple regular expression, which matches either URLs,alphanumeric character sequences (plus apostrophe)or non-alphanumeric non-whitespace character se-quences.
This way punctuation sequences likeemoticons are preserved, while still being separatedfrom words in case of missing whitespace.
The samehappens to hashtags, so ?#liiike:)?
gets separatedinto the three tokens #, liiike and :), which canthen be processed separately or as n-grams.
Whilethis strategy performed reasonably well for us, moresophisticated tokenizers for social media messages2http://tobias.io/semevaltweetthat handle more special cases like emoticons in-cluding letters are thinkable.To address the large variety in spelling typical forsocial networks we store three different variants ofeach token:a) The raw token found in the messageb) A normalized version, in which all charactersare converted to lowercase and all digits to 0c) A collapsed version, in which all adjacent du-plicate characters are removed from the nor-malized version, if it is not present in anEnglish word list.
That way ?school?
stays?school?, but ?liiike?
gets converted to ?like?.3.2 FeaturesWe explored a wide variety of linguistic and lexicalfeatures.
In our final submission we used the follow-ing set of features for each message:?
The normalized tokens [boolean]?
The stems of the collapsed tokens, which werecomputed using the Porter stemming algo-rithm (Porter, 1980) implemented in the PythonNatural Language Toolkit (Bird et al 2009).[boolean]?
The word cluster IDs of raw, normalized andcollapsed tokens.
The clusters were obtainedvia unsupervised Brown clustering (Brown etal., 1992) of 56,345,753 Tweets by Owoputiet al(2013) and are available on the web.3[boolean]?
The accumulated (summed) positive and accu-mulated negative SentiWordNet scores (Bac-cianella et al 2010) of all synsets matching thecollapsed token strings.
[continuous]Furthermore, the normalized tokens and stemswere marked with a special negation prefix, if theyoccurred after a negation word or word cluster ofnegation words.
If a punctuation token occurred be-fore the end of the message the marking was discon-tinued at that point.3http://www.ark.cs.cmu.edu/TweetNLP3293.3 Machine Learning MethodsFor the classification of the messages into the posi-tive, negative and neutral classes we use three linearmodels, which were trained in an one-vs.-all man-ner.
At prediction time we simply choose the labelwith the highest score.
All training was done us-ing the open-source machine learning toolkit scikit-learn,4 which provides a consistent Python API tofast implementations of various machine learning al-gorithms (Pedregosa et al 2011).The linear models were trained using stochasticgradient descent (SGD), which is a gradient de-scent optimization method that minimizes a givenloss function.
The term ?stochastic?
refers to thefact that the weights of the model are updated foreach training example, which is an approximation ofbatch gradient descent, in which all training exam-ples are considered to make a single step.
This waySGD is very fast to train, which was important to usto be able to rapidly evaluate different feature com-binations and hyperparameter settings using cross-validation.Algorithm 1 Stochastic gradient descent with hingeloss and elastic net regularization1: t?
1/(?
?
)2: u?
03: Initialize wj and qj with 0 for all j4: for epoch to NITER do5: for i to NSAMPLES do6: s?
wTx(i)7: ?
?
1/(?
t)8: c?
CLASSWEIGHT(y(i))9: u?
u+ ((1?
?)
?
?
)10: for j to NFEATURES do11: ?`?wj ?
{?y(i)x(i)j if y(i)s < 10 otherwise12: wj ?
(1?
?
?
?)
wj ?
?
c ?`?wj13: z ?
wj14: if wj > 0 then15: wj ?
max(0, wj ?
(u+ qj))16: else if wj < 0 then17: wj ?
min(0, wj + (u?
qj))18: qj ?
qj + (wj ?
z)19: t?
t+ 14Version 0.13.1, http://scikit-learn.orgHyperparameter Task A Task BNITER 1000 1000CLASSWEIGHT(y(i)) 1 auto5?
0.0001 0.001?
0.15 0.15Table 1: Hyperparameters used for final model trainingThe loss function we used was hinge loss, whichis a large-margin loss function known for its use insupport vector machines.
To avoid overfitting thetraining set we employed elastic net regularization,which is a combination of L1 and L2 regularization.A simplified version of the SGD learning proce-dure implemented in scikit-learn is shown in Algo-rithm 1, where w is the weight vector of the model,x(i) the feature vector of sample i, y(i) ?
{?1,+1}the ground truth label of sample i, ?
the learningrate, ?
the regularization factor and ?
the elasticnet mixing parameter.
Be aware that we did notpick samples at random or shuffle the data, whichis crucial in case of training data which is sortedby classes.
The initial learning rate is set heuris-tically and updated following Shalev-Shwartz et al(2007).6 The way of applying the L1 penalty (lines13 to 18) is published as ?cumulative L1 penalty?
inTsuruoka et al(2009).
The final settings for the hy-perparameters were determined by running a cross-validated grid search on the combined training anddevelopment sets and can be found in Table 1.4 ExperimentsFor our experiments and the final model trainingwe used the combined training and development setof the shared task.
For Task A we removed mes-sages labeled ?objective?
prior to training, while wemerged them into the ?neutral?
class for Task B.This left us with 9419 training samples (5855 pos-itive, 457 neutral, 3107 negative) for Task A and10368 training samples (3855 positive, 4889 neutral,1624 negative) for Task B.
As the shared task wasevaluated on average F1 of the positive and negativeclass, disregarding the neutral class, we also provideour results in these measures in the following.5inversely proportional to class frequency6This is achieved by choosing ?optimal?
as setting for thelearning rate for scikit-learn?s SGDClassifier.330Negative Positive Avg.Prec Rec Prec Rec F1ALL 53.86 62.68 77.88 68.95 65.54-stem -0.38 -1.10 -0.07 -0.08 -0.385-wc -0.74 -0.30 +0.13 -2.05 -0.835-swn -0.15 -0.73 -0.27 +0.10 -0.23-neg +0.04 -0.92 -1.06 +0.44 -0.30bow -4.03 -7.01 -0.44 -3.68 -3.83Table 2: Feature ablation study (Task B)During the process of preparing our submissionwe used 10-fold cross-validation to evaluate differ-ent combinations of features, machine learning algo-rithms and their hyperparameter settings.
While wefound the features described in section 3.2 to be use-ful, we did not find further improvement by usingn-grams and part-of-speech tags, despite using theTwitter-specific part-of-speech tagger by Owoputi etal.
(2013).
Table 2 shows a cross-validated ablationstudy on the features, removing one group of fea-tures at a time to see their contribution to the model.Using only normalized tokens is referred to as bag-of-words (bow).
One can see that word clusters arethe most important for our model, causing the high-est overall loss in F1 performance when being re-moved.
Nevertheless, all other features contribute tothe performance of the model as well.Further improvement can be made by carefullypicking a machine learning algorithm and tuning itshyperparameters.
For this task we found linear mod-els to perform better than other classification meth-ods such as naive bayes, decision tree / random for-est and k-nearest neighbor.
Figure 1 shows thatmodels trained with the method described in sec-tion 3.3 (marked SGD) clearly outperforms mod-els trained with the popular perceptron algorithm(which could be described as stochastic gradient de-scent with zero-one loss, no regularization and con-stant learning rate, marked PER) with increasingtraining set size.
The values were obtained by train-ing on different portions of the training set of TaskB and testing on the previously unseen Task B Twit-ter test set (3813 samples).
Starting from a cer-tain amount of available training data, the choice ofthe training algorithm becomes even more importantthan the choice of features.lllll404550556065Training samples usedAverageF 1500 1000 2500 5000 7500lll lllSGD ALLSGD BOWPER ALLPER BOWFigure 1: Effect of training set size on different classifiers5 Results and ConclusionThe results of our submission for the four hidden testsets of the shared task can be found in Table 3.
Giventhe relatively small deviation from the results of thecross-validation on combined training and develop-ment set and the good ranks obtained in the sharedtask system ranking, we conclude that the methodfor sentiment analysis in microblogging messagespresented in this paper yields competitive results.We showed that the performance for this task canbe improved by using linguistically motivated fea-tures as well as carefully choosing a learning algo-rithm and its hyperparameter settings.Task Prec Rec F1 (Rank)A SMS 86.09 91.01 88.37 (1)A Twitter 85.06 85.43 85.19 (7)B SMS 55.83 72.55 62.15 (2)B Twitter 70.21 61.49 65.27 (2)Table 3: Final results of our submissionAcknowledgmentsWe would like to thank the organizers of the sharedtask for their effort, Peter Prettenhofer for his helpwith getting to the bottom of the SGD implementa-tion in scikit-learn and Richard Johansson as well asthe anonymous reviewers for their helpful commentson the paper.331ReferencesStefano Baccianella, Andrea Esuli, and Fabrizio Sebas-tiani.
2010.
Sentiwordnet 3.0: An enhanced lexicalresource for sentiment analysis and opinion mining.In Proceedings of the 7th conference on InternationalLanguage Resources and Evaluation (LREC?10), Val-letta, Malta.Luciano Barbosa and Junlan Feng.
2010.
Robust Sen-timent Detection on Twitter from Biased and NoisyData.
In COLING (Posters), pages 36?44.Steven Bird, Ewan Klein, and Edward Loper.
2009.
Nat-ural Language Processing with Python.
O?Reilly Me-dia.Peter F Brown, Peter V Desouza, Robert L Mercer, Vin-cent J Della Pietra, and Jenifer C Lai.
1992.
Class-based n-gram models of natural language.
Computa-tional linguistics, 18(4):467?479.Dmitry Davidov, Oren Tsur, and Ari Rappoport.
2010.Enhanced Sentiment Learning Using Twitter Hashtagsand Smileys.
In COLING (Posters), pages 241?249.Efthymios Kouloumpis, Theresa Wilson, and JohannaMoore.
2011.
Twitter Sentiment Analysis: The Goodthe Bad and the OMG!
In Fifth International AAAIConference on Weblogs and Social Media, ICWSM.Bing Liu.
2012.
Sentiment Analysis and Opinion Min-ing.
Synthesis Lectures on Human Language Tech-nologies.
Morgan & Claypool Publishers.Finn A?rup Nielsen.
2011.
A new ANEW: Evaluation ofa word list for sentiment analysis in microblogs.
InProceedings of the ESWC2011 Workshop on ?MakingSense of Microposts?
: Big things come in small pack-ages, volume 718 of CEUR Workshop Proceedings,pages 93?98.Brendan O?Connor, Ramnath Balasubramanyan,Bryan R. Routledge, and Noah A. Smith.
2010.
FromTweets to Polls: Linking Text Sentiment to PublicOpinion Time Series.
In Fourth International AAAIConference on Weblogs and Social Media, ICWSM.Olutobi Owoputi, Brendan O?Connor, Chris Dyer, KevinGimpel, Nathan Schneider, and Noah A Smith.
2013.Improved part-of-speech tagging for online conver-sational text with word clusters.
In Proceedings ofNAACL 2013.Alexander Pak and Patrick Paroubek.
2010.
Twitter asa Corpus for Sentiment Analysis and Opinion Min-ing.
In Proceedings of the 7th conference on Inter-national Language Resources and Evaluation, volume2010, pages 1320?1326.Fabian Pedregosa, Gae?l Varoquaux, Alexandre Gram-fort, Vincent Michel, Bertrand Thirion, Olivier Grisel,Mathieu Blondel, Peter Prettenhofer, Ron Weiss, Vin-cent Dubourg, Jake Vanderplas, Alexandre Passos,David Cournapeau, Matthieu Brucher, Matthieu Per-rot, and E?douard Duchesnay.
2011.
Scikit-learn: Ma-chine learning in Python.
Journal of Machine Learn-ing Research, 12:2825?2830.Martin F Porter.
1980.
An algorithm for suffix stripping.Program: electronic library and information systems,14(3):130?137.Hassan Saif, Yulan He, and Harith Alani.
2012.
Alle-viating data sparsity for twitter sentiment analysis.
InProceedings of the 2nd Workshop on Making Sense ofMicroposts.Shai Shalev-Shwartz, Yoram Singer, and Nathan Srebro.2007.
Pegasos: Primal estimated sub-gradient solverfor svm.
In Proceedings of the 24th international con-ference on Machine learning, pages 807?814.
ACM.Michael Speriosu, Nikita Sudan, Sid Upadhyay, and Ja-son Baldridge.
2011.
Twitter Polarity Classificationwith Label Propagation over Lexical Links and theFollower Graph.
In Proceedings of the 2011 Confer-ence on Empirical Methods in Natural Language Pro-cessing, EMNLP ?11, page 53?63.Yoshimasa Tsuruoka, Jun?ichi Tsujii, and Sophia Ana-niadou.
2009.
Stochastic gradient descent trainingfor l1-regularized log-linear models with cumulativepenalty.
In Proceedings of the Joint Conference of the47th Annual Meeting of the ACL and the 4th Interna-tional Joint Conference on Natural Language Process-ing of the AFNLP: Volume 1-Volume 1, pages 477?485.
Association for Computational Linguistics.Theresa Wilson, Zornitsa Kozareva, Preslav Nakov, AlanRitter, Sara Rosenthal, and Veselin Stoyanov.
2013.Semeval-2013 task 2: Sentiment analysis in twitter.In Proceedings of the 7th International Workshop onSemantic Evaluation.
Association for ComputationalLinguistics.332
