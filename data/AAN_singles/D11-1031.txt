Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 333?343,Edinburgh, Scotland, UK, July 27?31, 2011. c?2011 Association for Computational LinguisticsTraining a Log-Linear Parser with Loss Functions via Softmax-MarginMichael AuliSchool of InformaticsUniversity of Edinburghm.auli@sms.ed.ac.ukAdam LopezHLTCOEJohns Hopkins Universityalopez@cs.jhu.eduAbstractLog-linear parsing models are often trainedby optimizing likelihood, but we would preferto optimise for a task-specific metric like F-measure.
Softmax-margin is a convex objec-tive for such models that minimises a boundon expected risk for a given loss function, butits na?
?ve application requires the loss to de-compose over the predicted structure, whichis not true of F-measure.
We use softmax-margin to optimise a log-linear CCG parser fora variety of loss functions, and demonstratea novel dynamic programming algorithm thatenables us to use it with F-measure, lead-ing to substantial gains in accuracy on CCG-Bank.
When we embed our loss-trained parserinto a larger model that includes supertaggingfeatures incorporated via belief propagation,we obtain further improvements and achievea labelled/unlabelled dependency F-measureof 89.3%/94.0% on gold part-of-speech tags,and 87.2%/92.8% on automatic part-of-speechtags, the best reported results for this task.1 IntroductionParsing models based on Conditional RandomFields (CRFs; Lafferty et al, 2001) have been verysuccessful (Clark and Curran, 2007; Finkel et al,2008).
In practice, they are usually trained by max-imising the conditional log-likelihood (CLL) of thetraining data.
However, it is widely appreciated thatoptimizing for task-specific metrics often leads tobetter performance on those tasks (Goodman, 1996;Och, 2003).An especially attractive means of accomplishingthis for CRFs is the softmax-margin (SMM) ob-jective (Sha and Saul, 2006; Povey and Woodland,2008; Gimpel and Smith, 2010a) (?2).
In addition toretaining a probabilistic interpretation and optimiz-ing towards a loss function, it is also convex, mak-ing it straightforward to optimise.
Gimpel and Smith(2010a) show that it can be easily implemented witha simple change to standard likelihood-based train-ing, provided that the loss function decomposes overthe predicted structure.Unfortunately, the widely-used F-measure met-ric does not decompose over parses.
To solve this,we introduce a novel dynamic programming algo-rithm that enables us to compute the exact quanti-ties needed under the softmax-margin objective us-ing F-measure as a loss (?3).
We experiment withthis and several other metrics, including precision,recall, and decomposable approximations thereof.Our ability to optimise towards exact metrics en-ables us to verify the effectiveness of more effi-cient approximations.
We test the training proce-dures on the state-of-the-art Combinatory CategorialGrammar (CCG; Steedman 2000) parser of Clarkand Curran (2007), obtaining substantial improve-ments under a variety of conditions.
We then embedthis model into a more accurate model that incor-porates additional supertagging features via loopybelief propagation.
The improvements are additive,obtaining the best reported results on this task (?4).2 Softmax-Margin TrainingThe softmax-margin objective modifies the standardlikelihood objective for CRF training by reweighting333each possible outcome of a training input accordingto its risk, which is simply the loss incurred on a par-ticular example.
This is done by incorporating theloss function directly into the linear scoring functionof an individual example.Formally, we are given m training pairs(x(1), y(1))...(x(m), y(m)), where each x(i) ?
X isdrawn from the set of possible inputs, and eachy(i) ?
Y(x(i)) is drawn from a set of possibleinstance-specific outputs.
We want to learn the Kparameters ?
of a log-linear model, where each ?k ??
is the weight of an associated feature hk(x, y).Function f(x, y) maps input/output pairs to the vec-tor h1(x, y)...hK(x, y), and our log-linear model as-signs probabilities in the usual way.p(y|x) = exp{?Tf(x, y)}?y?
?Y(x) exp{?Tf(x, y?
)}(1)The conditional log-likelihood objective function isgiven by Eq.
2 (Figure 1).
Now consider a function`(y, y?)
that returns the loss incurred by choosing tooutput y?
when the correct output is y.
The softmax-margin objective simply modifies the unnormalised,unexponentiated score ?Tf(x, y?)
by adding `(y, y?
)to it.
This yields the objective function (Eq.
3) andgradient computation (Eq.
4) shown in Figure 1.This straightforward extension has several desir-able properties.
In addition to having a probabilis-tic interpretation, it is related to maximum marginand minimum-risk frameworks, it can be shown tominimise a bound on expected risk, and it is convex(Gimpel and Smith, 2010b).We can also see from Eq.
4 that the only differ-ence from standard CLL training is that we mustcompute feature expectations with respect to thecost-augmented scoring function.
As Gimpel andSmith (2010a) discuss, if the loss function decom-poses over the predicted structure, we can treat itsdecomposed elements as unweighted features thatfire on the corresponding structures, and computeexpectations in the normal way.
In the case ofour parser, where we compute expectations usingthe inside-outside algorithm, a loss function decom-poses if it decomposes over spans or productions ofa CKY chart.3 Loss Functions for ParsingIdeally, we would like to optimise our parser towardsa task-based evaluation.
Our CCG parser is evalu-ated on labeled, directed dependency recovery us-ing F-measure (Clark and Hockenmaier, 2002).
Un-der this evaluation we will represent output y?
andground truth y as variable-sized sets of dependen-cies.
We can then compute precision P (y, y?
), recallR(y, y?
), and F-measure F1(y, y?
).P (y, y?)
= |y ?
y?||y?| (5)R(y, y?)
= |y ?
y?||y| (6)F1(y, y?)
=2PRP +R =2|y ?
y?||y|+ |y?| (7)These metrics are positively correlated with perfor-mance ?
they are gain functions.
To incorporatethem in the softmax-margin framework we reformu-late them as loss functions by subtracting from one.3.1 Computing F-Measure-AugmentedExpectations at the Sentence LevelUnfortunately, none of these metrics decomposeover parses.
However, the individual statistics thatare used to compute them do decompose, a fact wewill exploit to devise an algorithm that computes thenecessary expectations.
Note that since y is fixed,F1 is a function of two integers: |y ?
y?|, represent-ing the number of correct dependencies in y?
; and|y?|, representing the total number of dependenciesin y?, which we will denote as n and d, respectively.1Each pair ?n, d?
leads to a different value of F1.
Im-portantly, both n and d decompose over parses.The key idea will be to treat F1 as a non-local fea-ture of the parse, dependent on values n and d.2 Tocompute expectations we split each span in an oth-erwise usual inside-outside computation by all pairs?n, d?
incident at that span.Formally, our goal will be to compute expecta-tions over the sentence a1...aL.
In order to abstractaway from the particulars of CCG we present the al-gorithm in relatively familiar terms as a variant of1For numerator and denominator.2This is essentially the same trick used in the oracle F-measurealgorithm of Huang (2008), and indeed our algorithm is a sum-product variant of that max-product algorithm.334min?m?i=1???
?Tf(x(i), y(i)) + log?y?Y(x(i))exp{?Tf(x(i), y)}??
(2)min?m?i=1???
?Tf(x(i), y(i)) + log?y?Y(x(i))exp{?Tf(x(i), y) + `(y(i), y)}??
(3)???k=m?i=1??
?hk(x(i), y(i)) +?y?Y(x(i))exp{?Tf(x(i), y) + `(y(i), y)}?y?
?Y(x(i)) exp{?Tf(x(i), y?)
+ `(y(i), y?
)}hk(x(i), y)??
(4)Figure 1: Conditional log-likelihood (Eq.
2), Softmax-margin objective (Eq.
3) and gradient (Eq.
4).the classic inside-outside algorithm (Baker, 1979).We use the notation a : A for lexical entries andBC ?
A to indicate that categories B and C com-bine to form category A via forward or backwardcomposition or application.3 The weight of a ruleis denoted with w. The classic algorithm associatesinside score I(Ai,j) and outside score O(Ai,j) withcategory A spanning sentence positions i through j,computed via the following recursions.I(Ai,i+1) =w(ai+1 : A)I(Ai,j) =?k,B,CI(Bi,k)I(Ck,j)w(BC ?
A)I(GOAL) =I(S0,L)O(GOAL) =1O(Ai,j) =?k,B,CO(Ci,k)I(Bj,k)w(AB ?
C)+?k,B,CO(Ck,j)I(Bk,i)w(BA?
C)The expectation of A spanning positions i through jis then I(Ai,j)O(Ai,j)/I(GOAL).Our algorithm extends these computations tostate-split itemsAi,j,n,d.4 Using functions n+(?)
andd+(?)
to respectively represent the number of cor-rect and total dependencies introduced by a parsingaction, we present our algorithm in Fig.
3.
The fi-nal inside equation and initial outside equation in-corporate the loss function for all derivations hav-ing a particular F-score, enabling us to obtain the3These correspond respectively to unary rules A ?
a and bi-nary rules A ?
BC in a Chomsky normal form grammar.4Here we use state-splitting to refer to splitting an item Ai,j intomany items Ai,j,n,d, one for each ?n, d?
pair.desired expectations.
A simple modification of thegoal equations enables us to optimise precision, re-call or a weighted F-measure.To analyze the complexity of this algorithm, wemust ask: how many pairs ?n, d?
can be incident ateach span?
A CCG parser does not necessarily re-turn one dependency per word (see Figure 2 for anexample), so d is not necessarily equal to the sen-tence length L as it might be in many dependencyparsers, though it is still bounded by O(L).
How-ever, this behavior is sufficiently uncommon that weexpect all parses of a sentence, good or bad, to haveclose to L dependencies, and hence we expect therange of d to be constant on average.
Furthermore,n will be bounded from below by zero and fromabove by min(|y|, |y?|).
Hence the set of all possi-ble F-measures for all possible parses is bounded byO(L2), but on average it should be closer to O(L).Following McAllester (1999), we can see from in-spection of the free variables in Fig.
3 that the algo-rithm requires worst-case O(L7) and average-caseO(L5) time complexity, and worse-case O(L4) andaverage-case O(L3) space complexity.Note finally that while this algorithm computesexact sentence-level expectations, it is approximateat the corpus level, since F-measure does not decom-pose over sentences.
We give the extension to exactcorpus-level expectations in Appendix A.3.2 Approximate Loss FunctionsWe will also consider approximate but more effi-cient alternatives to our exact algorithms.
The ideais to use cost functions which only utilise statistics335I(Ai,i+1,n,d) = w(ai+1 : A) iffn = n+(ai+1 : A), d = d+(ai+1 : A)I(Ai,j,n,d) =?k,B,C?{n?,n??:n?+n??+n+(BC?A)=n},{d?,d??:d?+d??+d+(BC?A)=d}I(Bi,k,n?,d?)I(Ck,j,n??,d??
)w(BC ?
A)I(GOAL) =?n,dI(S0,L,n,d)(1?
2nd+ |y|)O(S0,N,n,d) =(1?
2nd+ |y|)O(Ai,j,n,d) =?k,B,C?{n?,n??:n??n???n+(AB?C)=n},{d?,d??:d??d???d+(AB?C)=d}O(Ci,k,n?,d?)I(Bj,k,n??,d??
)w(AB ?
C)+?k,B,C?{n?,n??:n??n???n+(BA?C)=n},{d?,d??:d??d???d+(BA?C)=d}O(Ck,j,n?,d?)I(Bk,i,n??,d??)w(BA?
C)Figure 3: State-split inside and outside recursions for computing softmax-margin with F-measure.Figure 2: Example of flexible dependency realisation inCCG: Our parser (Clark and Curran, 2007) creates de-pendencies arising from coordination once all conjunctsare found and treats ?and?
as the syntactic head of coor-dinations.
The coordination rule (?)
does not yet estab-lish the dependency ?and - pears?
(dotted line); it is thebackward application (<) in the larger span, ?apples andpears?, that establishes it, together with ?and - pears?.CCG also deals with unbounded dependencies which po-tentially lead to more dependencies than words (Steed-man, 2000); in this example a unification mechanism cre-ates the dependencies ?likes - apples?
and ?likes - pears?in the forward application (>).
For further examples anda more detailed explanation of the mechanism as used inthe C&C parser refer to Clark et al (2002).available within the current local structure, similar tothose used by Taskar et al (2004) for tracking con-stituent errors in a context-free parser.
We designthree simple losses to approximate precision, recalland F-measure on CCG dependency structures.Let T (y) be the set of parsing actions requiredto build parse y.
Our decomposable approximationto precision simply counts the number of incorrectdependencies using the local dependency counts,n+(?)
and d+(?
).DecP (y) =?t?T (y)d+(t)?
n+(t) (8)To compute our approximation to recall we requirethe number of gold dependencies, c+(?
), whichshould have been introduced by a particular parsingaction.
A gold dependency is due to be recoveredby a parsing action if its head lies within one childspan and its dependent within the other.
This yields adecomposed approximation to recall that counts thenumber of missed dependencies.DecR(y) =?t?T (y)c+(t)?
n+(t) (9)336Unfortunately, the flexible handling of dependenciesin CCG complicates our formulation of c+, render-ing it slightly more approximate.
The unificationmechanism of CCG sometimes causes dependenciesto be realised later in the derivation, at a point whenboth the head and the dependent are in the samespan, violating the assumption used to compute c+(see again Figure 2).
Exceptions like this can causemismatches between n+ and c+.
We set c+ = n+whenever c+ < n+ to account for these occasionaldiscrepancies.Finally, we obtain a decomposable approximationto F-measure.DecF1(y) = DecP (y) +DecR(y) (10)4 ExperimentsParsing Strategy.
CCG parsers use a pipeline strat-egy: we first multitag each word of the sentence witha small subset of its possible lexical categories us-ing a supertagger, a sequence model over these cat-egories (Bangalore and Joshi, 1999; Clark, 2002).Then we parse the sentence under the requirementthat the lexical categories are fixed to those preferredby the supertagger.
In our experiments we used twovariants on this strategy.First is the adaptive supertagging (AST) approachof Clark and Curran (2004).
It is based on a stepfunction over supertagger beam widths, relaxing thepruning threshold for lexical categories only if theparser fails to find an analysis.
The process eithersucceeds and returns a parse after some iteration orgives up after a predefined number of iterations.
AsClark and Curran (2004) show, most sentences canbe parsed with very tight beams.Reverse adaptive supertagging is a much less ag-gressive method that seeks only to make sentencesparsable when they otherwise would not be due to animpractically large search space.
Reverse AST startswith a wide beam, narrowing it at each iteration onlyif a maximum chart size is exceeded.
Table 1 showsbeam settings for both strategies.Adaptive supertagging aims for speed via pruningwhile the reverse strategy aims for accuracy by ex-posing the parser to a larger search space.
AlthoughClark and Curran (2007) found no actual improve-ments from the latter strategy, we will show thatwith our softmax-margin-trained models it can havea substantial effect.Parser.
We use the C&C parser (Clark and Cur-ran, 2007) and its supertagger (Clark, 2002).
Ourbaseline is the hybrid model of Clark and Curran(2007), which contains features over both normal-form derivations and CCG dependencies.
The parserrelies solely on the supertagger for pruning, usingexact CKY for search over the pruned space.
Train-ing requires calculation of feature expectations overpacked charts of derivations.
For training, we lim-ited the number of items in this chart to 0.3 million,and for testing, 1 million.
We also used a more per-missive training supertagger beam (Table 2) than inprevious work (Clark and Curran, 2007).
Modelswere trained with the parser?s L-BFGS trainer.Evaluation.
We evaluated on CCGbank (Hocken-maier and Steedman, 2007), a right-most normal-form CCG version of the Penn Treebank.
We usesections 02-21 (39603 sentences) for training, sec-tion 00 (1913 sentences) for development and sec-tion 23 (2407 sentences) for testing.
We supplygold-standard part-of-speech tags to the parsers.
Weevaluate on labelled and unlabelled predicate argu-ment structure recovery and supertag accuracy.4.1 Training with Maximum F-measure ParsesSo far we discussed how to optimise towards task-specific metrics via changing the training objective.In our first experiment we change the data on whichwe optimise CLL.
This is a kind of simple base-line to our later experiments, attempting to achievethe same effect by simpler means.
Specifically, weuse the algorithm of Huang (2008) to generate or-acle F-measure parses for each sentence.
Updatingtowards these oracle parses corrects the reachabil-ity problem in standard CLL training.
Since the su-pertagger is used to prune the training forests, thecorrect parse is sometimes pruned away ?
reducingdata utilisation to 91%.
Clark and Curran (2007)correct for this by adding the gold tags to the parserinput.
While this increases data utilisation, it bi-ases the model by training in an idealised setting notavailable at test time.
Using oracle parses correctsthis bias while permitting 99% data utilisation.
Thelabelled F-score of the oracle parses lies at 98.1%.Though we expected that this might result in someimprovement, results (Table 3) show that this has no337Condition Parameter Iteration 1 2 3 4 5AST?
(beam width) 0.075 0.03 0.01 0.005 0.001k (dictionary cutoff) 20 20 20 20 150Reverse?
0.001 0.005 0.01 0.03 0.075k 150 20 20 20 20Table 1: Beam step function used for standard (AST) and less aggressive (Reverse) AST throughout our experiments.Parameter ?
is a beam threshold while k bounds the number of lexical categories considered for each word.Condition Parameter Iteration 1 2 3 4 5 6 7Training ?
0.001 0.001 0.0045 0.0055 0.01 0.05 0.1k 150 20 20 20 20 20 20C&C ?07 ?
0.0045 0.0055 0.01 0.05 0.1k 20 20 20 20 20Table 2: Beam step functions used for training: The first row shows the large scale settings used for most experimentsand the standard C&C settings.
(cf.
Table 1)LF LP LR UF UP UR Data Util (%)Baseline 87.40 87.85 86.95 93.11 93.59 92.63 91%Max-F Parses 87.46 87.95 86.98 93.09 93.61 92.57 99%CCGbank+Max-F 87.45 87.96 86.94 93.09 93.63 92.55 99%Table 3: Performance on section 00 of CCGbank when comparing models trained with treebank-parses (Baseline)and maximum F-score parses (Max-F) using adaptive supertagging as well as a combination of CCGbank and Max-Fparses.
Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).effect.
However, it does serve as a useful baseline.4.2 Training with the Exact AlgorithmWe first tested our assumptions about the feasibil-ity of training with our exact algorithm by measur-ing the amount of state-splitting.
Figure 4 plots theaverage number of splits per span against the rela-tive span-frequency; this is based on a typical set oftraining forests containing over 600 million states.The number of splits increases exponentially withspan size but equally so decreases the number ofspans with many splits.
Hence the small number ofstates with a high number of splits is balanced by alarge number of spans with only a few splits: Thehighest number of splits per span observed with oursettings was 4888 but we find that the average num-ber of splits lies at 44.
Encouragingly, this enablesexperimentation in all but very large scale settings.Figure 5 shows the distribution of n and d pairsacross all split-states in the training corpus; since0%?2%?4%?6%?8%?10%?12%?1?10?100?1000?1?
11?
21?
31?
41?
51?
61?
71?%?of?total?spans?Average?number?of?splits?span?length?Average?number?of?splits?Percentage?of?total?spans?Figure 4: Average number of state-splits per span lengthas introduced by a sentence-level F-measure loss func-tion.
The statistics are averaged over the training forestsgenerated using the settings described in ?4.n, the number of correct dependencies, over d, thenumber of all recovered dependencies, is precision,the graph shows that only a minority of states haveeither very high or very low precision.
The rangeof values suggests that the softmax-margin criterion338will have an opportunity to substantially modify theexpectations, hopefully to good effect.!"#!"$!"%!"&!"'!"!"
#!"
$!"
%!"
&!"
'!"
(!"
)!"!"#$%&'()'*(&&%*+',%-%,%!*.%/'0!1'!"#$%&'()'233',%-%!,%!*.%/'0,1'!*'!!!!!!"
'!!!!!!*#!!!!!!!"
#!!!!!!!*#'!!!!!!"
#'!!!!!!*$!!!!!!!
"Figure 5: Distribution of states with d dependencies ofwhich n are correct in the training forests.We next turn to the question of optimization withthese algorithms.
Due to the significant computa-tional requirements, we used the computationallyless intensive normal-form model of Clark and Cur-ran (2007) as well as their more restrictive trainingbeam settings (Table 2).
We train on all sentences ofthe training set as above and test with AST.In order to provide greater control over the influ-ence of the loss function, we introduce a multiplier?
, which simply amends the second term of the ob-jective function (3) to:log?y?Y (xi)exp{?T f(xi, y) + ?
?
`(yi, y)}Figure 6 plots performance of the exact loss func-tions across different settings of ?
on various evalu-ation criteria, for models restricted to at most 3000items per chart at training time to allow rapid ex-perimentation with a wide parameter set.
Even inthis constrained setting, it is encouraging to see thateach loss function performs best on the criteria it op-timises.
The precision-trained parser also does verywell on F-measure; this is because the parser has atendency to perform better in terms of precision thanrecall.4.3 Exact vs.
Approximate Loss FunctionsWith these results in mind, we conducted a compar-ison of parsers trained using our exact and approxi-mate loss functions.
Table 4 compares their perfor-mance head to head when restricting training chartsizes to 100,000 items per sentence, the largest set-ting our computing resources allowed us to experi-ment with.
The results confirm that the loss-trainedmodels improve over a likelihood-trained baseline,and furthermore that the exact loss functions seemto have the best performance.
However, the approx-imations are extremely competitive with their exactcounterparts.
Because they are also efficient, thismakes them attractive for larger-scale experiments.Training time increases by an order of magnitudewith exact loss functions despite increased theoreti-cal complexity (?3.1); there is no significant changewith approximate loss functions.Table 5 shows performance of the approximatelosses with the large scale settings initially outlined(?4).
One striking result is that the softmax-margintrained models coax more accurate parses from thelarger search space, in contrast to the likelihood-trained models.
Our best loss model improves thelabelled F-measure by over 0.8%.4.4 Combination with Integrated Parsing andSupertaggingAs a final experiment, we embed our loss-trainedmodel into an integrated model that incorporatesMarkov features over supertags into the parsingmodel (Auli and Lopez, 2011).
These features haveserious implications on search: even allowing for theobservation of Fowler and Penn (2010) that our CCGis weakly context-free, the search problem is equiva-lent to finding the optimal derivation in the weightedintersection of a regular and context-free language(Bar-Hillel et al, 1964), making search very expen-sive.
Therefore parsing with this model requires ap-proximations.To experiment with this combined model we useloopy belief propagation (LBP; Pearl et al, 1985),previously applied to dependency parsing by Smithand Eisner (2008).
A more detailed account of itsapplication to our combined model can be found in(2011), but we sketch the idea here.
We construct agraphical model with two factors: one is a distribu-339!
"# !$#!%# !&#'()'(*+)'(*,)'(*-)'(*.
)'(*/),) -) .)
/) () +0)!"#$%%$&'()*$"+,-$'.
",'1"234563) 7+)4822)9:3%52586)4822) ;3%"44)4822)'/*<)'(*+)'(*-)'(*/)'(*=)'(*<)'=*+),) -) .)
/) () +0)!"#$%%$&'/-$01+123'.
",'1"234563) 7+)4822)9:3%52586)4822) ;3%"44)4822)'/*/)'/*=)'/*<)'(*+)'(*-)'(*/),) -) .)
/) () +0)!"#$%%$&'4$0"%%'.
",'1"234563) 7+)4822)9:3%52586)4822) ;3%"44)4822)<-*=)<-*=/)<-*')<-*'/)<-*<)<-*</),) -) .)
/) () +0)5,6$-7"88138'900,-"0:'.
",'1"234563) 7+)4822)9:3%52586)4822) ;3%"44)4822)Figure 6: Performance of exact cost functions optimizing F-measure, precision and recall in terms of (a) labelledF-measure, (b) precision, (c) recall and (d) supertag accuracy across various settings of ?
on the development set.section 00 (dev) section 23 (test)LF LP LR UF UP UR LF LP LR UF UP URCLL 86.76 87.16 86.36 92.73 93.16 92.30 87.46 87.80 87.12 92.85 93.22 92.49DecP 87.18 87.93 86.44 92.93 93.73 92.14 87.75 88.34 87.17 93.04 93.66 92.43DecR 87.31 87.55 87.07 93.00 93.26 92.75 87.57 87.71 87.42 92.92 93.07 92.76DecF1 87.27 87.78 86.77 93.04 93.58 92.50 87.69 88.10 87.28 93.04 93.48 92.61P 87.25 87.85 86.66 92.99 93.63 92.36 87.76 88.23 87.30 93.06 93.55 92.57R 87.34 87.51 87.16 92.98 93.17 92.80 87.57 87.62 87.51 92.92 92.98 92.86F1 87.34 87.74 86.94 93.05 93.47 92.62 87.71 88.01 87.41 93.02 93.34 92.70Table 4: Performance of exact and approximate loss functions against conditional log-likelihood (CLL): decomposableprecision (DecP), recall (DecR) and F-measure (DecF1) versus exact precision (P), recall (R) and F-measure (F1).Evaluation is based on labelled and unlabelled F-measure (LF/UF), precision (LP/UP) and recall (LR/UR).340section 00 (dev) section 23 (test)AST Reverse AST ReverseLF UF ST LF UF ST LF UF ST LF UF STCLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01DecP 87.35 92.99 94.25 87.75 93.25 94.22 88.10 93.26 94.51 88.51 93.50 94.39DecR 87.48 93.00 94.34 87.70 93.16 94.30 87.66 92.83 94.38 87.77 92.91 94.22DecF1 87.67 93.23 94.39 88.12 93.52 94.46 88.09 93.28 94.50 88.58 93.57 94.53Table 5: Performance of decomposed loss functions in large-scale training setting.
Evaluation is based on labelled andunlabelled F-measure (LF/UF) and supertag accuracy (ST).tion over supertag variables defined by a supertag-ging model, and the other is a distribution over thesevariables and a set of span variables defined by ourparsing model.5 The factors communicate by pass-ing messages across the shared supertag variablesthat correspond to their marginal distributions overthose variables.
Hence, to compute approximate ex-pectations across the entire model, we run forward-backward to obtain posterior supertag assignments.These marginals are passed as inside values to theinside-outside algorithm, which returns a new setof posteriors.
The new posteriors are incorporatedinto a new iteration of forward-backward, and thealgorithm iterates until convergence, or until a fixednumber of iterations is reached ?
we found that asingle iteration is sufficient, corresponding to a trun-cated version of the algorithm in which posteriorsare simply passed from the supertagger to the parser.To decode, we use the posteriors in a minimum-riskparsing algorithm (Goodman, 1996).Our baseline models are trained separately as be-fore and combined at test time.
For softmax-margin,we combine a parsing model trained with F1 anda supertagger trained with Hamming loss.
Table 6shows the results: we observe a gain of up to 1.5%in labelled F1 and 0.9% in unlabelled F1 on the testset.
The loss functions prove their robustness by im-proving the more accurate combined models up to0.4% in labelled F1.
Table 7 shows results with au-tomatic part-of-speech tags and a direct comparisonwith the Petrov parser trained on CCGbank (Fowlerand Penn, 2010) which we outpeform on all metrics.5These complex factors resemble those of Smith and Eisner(2008) and Dreyer and Eisner (2009); they can be thought ofas case-factor diagrams (McAllester et al, 2008)5 Conclusion and Future WorkThe softmax-margin criterion is a simple and effec-tive approach to training log-linear parsers.
We haveshown that it is possible to compute exact sentence-level losses under standard parsing metrics, not onlyapproximations (Taskar et al, 2004).
This enablesus to show the effectiveness of these approxima-tions, and it turns out that they are excellent sub-stitutes for exact loss functions.
Indeed, the approxi-mate losses are as easy to use as standard conditionallog-likelihood.Empirically, softmax-margin training improvesparsing performance across the board, beating thestate-of-the-art CCG parsing model of Clark andCurran (2007) by up to 0.8% labelled F-measure.It also proves robust, improving a stronger base-line based on a combined parsing and supertaggingmodel.
Our final result of 89.3%/94.0% labelledand unlabelled F-measure is the best result reportedfor CCG parsing accuracy, beating the original C&Cbaseline by up to 1.5%.In future work we plan to scale our exact lossfunctions to larger settings and to explore trainingwith loss functions within loopy belief propagation.Although we have focused on CCG parsing in thiswork, we expect our methods to be equally appli-cable to parsing with other grammar formalisms in-cluding context-free grammar or LTAG.AcknowledgementsWe would like to thank Stephen Clark, Chris-tos Christodoulopoulos, Mark Granroth-Wilding,Gholamreza Haffari, Alexandre Klementiev, TomKwiatkowski, Kira Mourao, Matt Post, and MarkSteedman for helpful discussion related to thiswork and comments on previous drafts, and the341section 00 (dev) section 23 (test)AST Reverse AST ReverseLF UF ST LF UF ST LF UF ST LF UF STCLL 87.38 93.08 94.21 87.36 93.13 93.99 87.73 93.09 94.33 87.65 93.06 94.01BP 87.67 93.26 94.43 88.35 93.72 94.73 88.25 93.33 94.60 88.86 93.75 94.84+DecF1 87.90 93.40 94.52 88.58 93.88 94.79 88.32 93.32 94.66 89.15 93.89 94.98+SA 87.73 93.28 94.49 88.40 93.71 94.75 88.47 93.48 94.71 89.25 93.98 95.01Table 6: Performance of combined parsing and supertagging with belief propagation (BP); using decomposed-F1 asparser-loss function and supertag-accuracy (SA) as loss in the supertagger.section 00 (dev) section 23 (test)LF LP LR UF UP UR LF LP LR UF UP URCLL 85.53 85.73 85.33 91.99 92.20 91.77 85.74 85.90 85.58 91.92 92.09 91.75Petrov I-5 85.79 86.09 85.50 92.44 92.76 92.13 86.01 86.29 85.73 92.34 92.64 92.04BP 86.45 86.75 86.17 92.60 92.92 92.29 86.84 87.08 86.61 92.57 92.82 92.32+DecF1 86.73 87.07 86.39 92.79 93.16 92.43 87.08 87.37 86.78 92.68 93.00 92.37+SA 86.51 86.86 86.16 92.60 92.98 92.23 87.20 87.50 86.90 92.76 93.08 92.44Table 7: Results on automatically assigned POS tags.
Petrov I-5 is based on the parser output of Fowler and Penn(2010); evaluation is based on sentences for which all parsers returned an analysis.anonymous reviewers for helpful comments.
Wealso acknowledge funding from EPSRC grantEP/P504171/1 (Auli); and the resources provided bythe Edinburgh Compute and Data Facility.A Computing F-Measure-AugmentedExpectations at the Corpus LevelTo compute exact corpus-level expectations for softmax-margin using F-measure, we add an additional transitionbefore reaching the GOAL item in our original program.To reach it, we must parse every sentence in the corpus,associating statistics of aggregate ?n, d?
pairs for the en-tire training set in intermediate symbols ?(1)...?
(m) withthe following inside recursions.I(?
(1)n,d) = I(S(1)0,|x(1)|,n,d)I(?
(`)n,d) =?n?,n??:n?+n??=nI(?(`?1)n?,d?
)I(S(`)0,N,n??,d??
)I(GOAL) =?n,dI(?
(m)n,d )(1?
2nd+ |y|)Outside recursions follow straightforwardly.
Implemen-tation of this algorithm would require substantial dis-tributed computation or external data structures, so wedid not attempt it.ReferencesM.
Auli and A. Lopez.
2011.
A Comparison of LoopyBelief Propagation and Dual Decomposition for Inte-grated CCG Supertagging and Parsing.
In Proc.
ofACL, June.J.
K. Baker.
1979.
Trainable grammars for speech recog-nition.
Journal of the Acoustical Society of America,65.S.
Bangalore and A. K. Joshi.
1999.
Supertagging: AnApproach to Almost Parsing.
Computational Linguis-tics, 25(2):238?265, June.Y.
Bar-Hillel, M. Perles, and E. Shamir.
1964.
On formalproperties of simple phrase structure grammars.
InLanguage and Information: Selected Essays on theirTheory and Application, pages 116?150.S.
Clark and J. R. Curran.
2004.
The importance of su-pertagging for wide-coverage CCG parsing.
In COL-ING, Morristown, NJ, USA.S.
Clark and J. R. Curran.
2007.
Wide-Coverage Ef-ficient Statistical Parsing with CCG and Log-LinearModels.
Computational Linguistics, 33(4):493?552.S.
Clark and J. Hockenmaier.
2002.
Evaluating a Wide-Coverage CCG Parser.
In Proceedings of the LREC2002 Beyond Parseval Workshop, pages 60?66, LasPalmas, Spain.S.
Clark, J. Hockenmaier, and M. Steedman.
2002.Building deep dependency structures with a wide-coverage CCG parser.
In Proc.
of ACL.342S.
Clark.
2002.
Supertagging for Combinatory Catego-rial Grammar.
In TAG+6.M.
Dreyer and J. Eisner.
2009.
Graphical models overmultiple strings.
In Proc.
of EMNLP.J.
R. Finkel, A. Kleeman, and C. D. Manning.
2008.Feature-based, conditional random field parsing.
InProceedings of ACL-HLT.T.
A. D. Fowler and G. Penn.
2010.
Accurate context-free parsing with combinatory categorial grammar.
InProc.
of ACL.K.
Gimpel and N. A. Smith.
2010a.
Softmax-marginCRFs: training log-linear models with cost functions.In HLT ?10: The 2010 Annual Conference of the NorthAmerican Chapter of the Association for Computa-tional Linguistics.K.
Gimpel and N. A. Smith.
2010b.
Softmax-margintraining for structured log-linear models.
TechnicalReport CMU-LTI-10-008, Carnegie Mellon Univer-sity.J.
Goodman.
1996.
Parsing algorithms and metrics.
InProc.
of ACL, pages 177?183, Jun.J.
Hockenmaier and M. Steedman.
2007.
CCGbank:A corpus of CCG derivations and dependency struc-tures extracted from the Penn Treebank.
Computa-tional Linguistics, 33(3):355?396.L.
Huang.
2008.
Forest Reranking: Discriminative pars-ing with Non-Local Features.
In Proceedings of ACL-08: HLT.J.
Lafferty, A. McCallum, and F. Pereira.
2001.
Con-ditional random fields: Probabilistic models for seg-menting and labeling sequence data.
In Proc.
of ICML,pages 282?289.D.
McAllester, M. Collins, and F. Pereira.
2008.
Case-factor diagrams for structured probabilistic modeling.Journal of Computer and System Sciences, 74(1):84?96.D.
McAllester.
1999.
On the complexity analysis ofstatic analyses.
In Proc.
of Static Analysis Symposium,volume 1694/1999 of LNCS.
Springer Verlag.F.
J. Och.
2003.
Minimum error rate training in statisticalmachine translation.
In Proc.
of ACL, Jul.J.
Pearl.
1988.
Probabilistic Reasoning in IntelligentSystems: Networks of Plausible Inference.
MorganKaufmann.D.
Povey and P. Woodland.
2008.
Minimum phone er-ror and I-smoothing for improved discrimative train-ing.
In Proc.
of ICASSP.F.
Sha and L. K. Saul.
2006.
Large margin hiddenMarkov models for automatic speech recognition.
InProc.
of NIPS.D.
A. Smith and J. Eisner.
2008.
Dependency parsing bybelief propagation.
In Proc.
of EMNLP.M.
Steedman.
2000.
The syntactic process.
MIT Press,Cambridge, MA.B.
Taskar, D. Klein, M. Collins, D. Koller, and C. Man-ning.
2004.
Max-margin parsing.
In Proc.
of EMNLP,pages 1?8, Jul.343
