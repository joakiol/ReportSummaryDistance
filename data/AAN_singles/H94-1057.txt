One DODer's View of ARPA Spoken Language DirectionsCalvin OianoDepartment of DefenseAttn.
: R523Fort George G. Meade, MD 20755colano@charm.isi.edu1.
IntroductionDOD support for ARPA/HLT speech researchstems from the belief that large vocabularycontinuous speech recognition and speechunderstanding will have advantages inportability, and in the variety of applicationssustainable from a single, basic speech system.With some imagination one can envisionapplications in such remotely related areas asspeaker identification and languageidentification.changes in a problem over time.
Once the initialsystem is trained, an adequate port to a newproblem should be possible with limitedtraining.
Improved performance should beachievable over time by bootstrappingtechniques that require minimum userinteraction.However, even ff everyone accepted these goalsthere would still be sources of conflict.
The firstarea of potential conflict I will consider is therelative emphasis of research versus technologytransfer.Like many of you, I feel that some usefulapplications can be found for current systems,but many potential applications will fail theusefulness test.
Those that pass will requireconsiderable algorithmic tuning.
So, how dowe expand the range of applications, whilesimultaneously making life easier for the personwho must develop applications?
Of course,ARPA has been working toward this byintroducing stress testing, unconstrainedvocabulary, multi-lingual speech processing,contrastive testing, etc.
But we can do more.
Ishare a number of opinions with my colleaguesin DOD about current, and potentially newARPA research directions.
Perhaps surfacingthese perspectives will stimulate discussion andhave some impact.If we could agree on the goal of ARPA speechresearch, agreeing on the research directionsmight be easier.
For me this goal is a simplemotherhood statement.
I would like to have acompetent large vocabulary continuous peechrecognition engine.To achieve that competence we will need betterbasic performance that is robust to changes inproblem and environment.
Preferably the systemwould be capable of assisting in tracking2.
Tech Transfer vs ResearchAs we have long recognized, there are severalways to improve performance on a task.
One isby improving the underlying science andalgorithms.
Another is taking advantage ofnatural structural constraints that are commonto many tasks.
But equally important is tailoringthe system to use constraints and biases uniqueto the specific task.
To clarify this view,consider an application like the ATIS speechunderstanding task: Finding features thatprovide a better performing talker independentsystem is an algorithmic improvement to theacoustic recognizer.
Using knowledge of theprior discourse and likely new queries to guidethe acoustic recognizer is a natural stn~cturalconstraint.
Designing a talker specific systembecause only a small set of talkers will use thesystem is tailoring based upon task specificbiases.I am deeply interested in task specific tailoring,and I don't care whether the information thatguides the process comes from a speech or sometotally unique task specific bias.To advance speech research we work on generalproblems, like ATIS and WSJ, which must be297constructed with extraordinary caution to avoidbias.
But, I love biases, as long as we knowabout them and can take advantage of them.And my experience is that most problems havebiases that may be exploited once they areknown.
Perhaps ome of our work should attacknatural or real tasks with a no-holds-hatredapproach.Does this mean that speech researchers shouldgive up their careers and become systemdevelopers?
- Certainly not.
But, at this pointin time, throwing a speech algorithm over thefence to the system developer does not work.Some percentage of the speech knowledgeableworkers will either have to become systemdevelopers or must establish a rapport with thesystem developer that is a top priority of theirwork.
In the current state of development, onlyby tailoring the algorithm to the constraints ofthe problem will speech technology become costeffective.soon?
I'm skeptical.
Considerable work hasbeen done to improve underlying speechprocessing systems by using natural constraintssuch as statistical grammars.
We need to domore.
I do not know whether systems with littleor no senmntic or higher level knowledge canever give acceptable performance on a realapplication like WSJ.
Worse than that, I don'tknow how to measure the performance limitsimposed by this handicap.The alternative is not to live with the handicap,but to begin to introduce higher levelknowledge, perhaps in a fragmentary manner.Indeed, some researchers are working toconstrain lower level processes by usingstructure based on higher level considerations.Since I believe that this will prove to be veryimportant, I would like to encourage this work.In addition, we should give thought o whetherthere are better architectures for applying theseconstraints.It is my belief that working on more realisticproblems, and being forced to think more aboutthe technology transfer issues will producesystems are more capable of taking advantage ofthe specifics of a new task.
Such a systemwould be architecturally different from a systemdesigned to work as the general continuousspeech transcription system or the generaldatabase interface tool.
I believe that inaddition we would further our understanding ofour speech recognition systems and of human -computer speech communication.3.
Natura l  Const ra in tsWe need to make money with our technology,but I have already expressed the belief that thenumber of applications we can expect success ontoday is limited.
What are the limitations ofcurrent system performance?
Can we takeadvantage of the natural structures in speech toimprove performance?Consider the speech tasks worked on under theARPA HLT program.
The Wall Street Journaltranscription task can be useful in advancinglarge vocabulary speech recognition, but willviable applications of this technology follow4.
Improving the Basic SystemSince everyone is continually trying to improvetheir system, you might think that there wouldbe little new to surface here.
But this is not thecase.
There hasn't been enough time, enoughmoney or enough manpower to do as thorough ajob as we would like.Improving the understanding of our algorithmsis always a useful activity.
I strongly suspectthat many of the research elements present heredo not understand the relative contributions anddependencies intrinsic to each of their systemcomponents as well as they believe they do.Discovering these dependencies is anevolutionary process.
In most cases, manydiagnostic tests that could be run to gain insightinto the system have never been done.
- At aminimum, they have never been reported.One aspect of system performance that is notoften measured is consistency across talkers,channel environments, etc.
Although we look atchanges in word recognition performance, howoften do we measure the consistency of ourrecognizers at the subword level?298We can learn more from our experiments andour data bases, if we are wilting to make theeffort.
Them arc uncontrolled variables, theeffect of which could be measured.
Forinstance, we could use channel simulation togauge the effect of channel differences.
Asanother example, consider the WSJ task.
Thereare great disparities of performance from talkertO talker.
If we reused our test data andrestructured our test to reflect this "greatdivide", we would gain new insight.
- Arethere other ideas for getting more milk fromthese data?We should explore new testing paradigms.
Rwould be useful to know how our systemsdiffered from human performance.
It should bepossible to do psychophysical measurements onour systems and compare them to humanpsychophysics.
As one simple experiment wecould compare human and system performanceusing diagnostic rhyme, or nonsense syllabletests.
This can be looked on as a contrastivetest of human versus acoustic recognizerperformance with higher level knowledgedenied both humans and machines.A more dramatic experimental change would beto run our systems with an "infinite" corpus ofdata.
That is, we would devote a portion of ourenergy to testing our systems on a continuing,day to day basis on a realistic problem.
Wewould get experience with how problemswith time, and undoubtedly improve portability.This would also be a good scenario forhaving the speech systems perform erA'lainlevels of self diagnosis.
The machine could tellus when discontinuities in the data occurred,and could be structured toassist in learning thenecessary repairs.
One advantage of this testingparadigm is that we would test our systems onorders of magnitude more data than we do in thenormal static test mode, thereby obtaining moreexposure to low probability events that could besaved for further study and system updating.In the infinite data paradigm our view oftraining would change drastically.
We would beblessed by having much more data available fortraining, and cursed by having less informationabout he data.
From my point of view, it wouldbe extremely beneficial to see the ingeniousmechanisms that would evolve to cope with andtake advantage ofthis situation.5.
Concluding RemarkI would like to hear a serious discussion of whatwe might do differently and what the benefitswould be.
A related issue that should beseriously addressed is what can be done toencourage more diversity of approach, more risktaking, and, consequently, more innovation.299
