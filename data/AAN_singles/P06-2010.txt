Proceedings of the COLING/ACL 2006 Main Conference Poster Sessions, pages 73?80,Sydney, July 2006. c?2006 Association for Computational LinguisticsA Hybrid Convolution Tree Kernel for Semantic Role LabelingWanxiang CheHarbin Inst.
of Tech.Harbin, China, 150001car@ir.hit.edu.cnMin ZhangInst.
for Infocomm ResearchSingapore, 119613mzhang@i2r.a-star.edu.sgTing Liu, Sheng LiHarbin Inst.
of Tech.Harbin, China, 150001{tliu, ls}@ir.hit.edu.cnAbstractA hybrid convolution tree kernel is pro-posed in this paper to effectively modelsyntactic structures for semantic role la-beling (SRL).
The hybrid kernel consistsof two individual convolution kernels: aPath kernel, which captures predicate-argument link features, and a ConstituentStructure kernel, which captures the syn-tactic structure features of arguments.Evaluation on the datasets of CoNLL-2005 SRL shared task shows that thenovel hybrid convolution tree kernel out-performs the previous tree kernels.
Wealso combine our new hybrid tree ker-nel based method with the standard richflat feature based method.
The experi-mental results show that the combinationalmethod can get better performance thaneach of them individually.1 IntroductionIn the last few years there has been increasing in-terest in Semantic Role Labeling (SRL).
It is cur-rently a well defined task with a substantial bodyof work and comparative evaluation.
Given a sen-tence, the task consists of analyzing the proposi-tions expressed by some target verbs and someconstituents of the sentence.
In particular, for eachtarget verb (predicate) all the constituents in thesentence which fill a semantic role (argument) ofthe verb have to be recognized.Figure 1 shows an example of a semantic rolelabeling annotation in PropBank (Palmer et al,2005).
The PropBank defines 6 main arguments,Arg0 is the Agent, Arg1 is Patient, etc.
ArgM-may indicate adjunct arguments, such as Locative,Temporal.Many researchers (Gildea and Jurafsky, 2002;Pradhan et al, 2005a) use feature-based methods      Figure 1: Semantic role labeling in a phrase struc-ture syntactic tree representationfor argument identification and classification inbuilding SRL systems and participating in eval-uations, such as Senseval-3 1, CoNLL-2004 and2005 shared tasks: SRL (Carreras and Ma`rquez,2004; Carreras and Ma`rquez, 2005), where aflat feature vector is usually used to represent apredicate-argument structure.
However, it?s hardfor this kind of representation method to explicitlydescribe syntactic structure information by a vec-tor of flat features.
As an alternative, convolutiontree kernel methods (Collins and Duffy, 2001)provide an elegant kernel-based solution to im-plicitly explore tree structure features by directlycomputing the similarity between two trees.
Inaddition, some machine learning algorithms withdual form, such as Perceptron and Support VectorMachines (SVM) (Cristianini and Shawe-Taylor,2000), which do not need know the exact presen-tation of objects and only need compute their ker-nel functions during the process of learning andprediction.
They can be well used as learning al-gorithms in the kernel-based methods.
They arenamed kernel machines.In this paper, we decompose the Moschitti(2004)?s predicate-argument feature (PAF) kernelinto a Path kernel and a Constituent Structure ker-1http://www.cs.unt.edu/?rada/senseval/senseval3/73nel, and then compose them into a hybrid con-volution tree kernel.
Our hybrid kernel methodusing Voted Perceptron kernel machine outper-forms the PAF kernel in the development sets ofCoNLL-2005 SRL shared task.
In addition, the fi-nal composing kernel between hybrid convolutiontree kernel and standard features?
polynomial ker-nel outperforms each of them individually.The remainder of the paper is organized as fol-lows: In Section 2 we review the previous work.In Section 3 we illustrate the state of the artfeature-based method for SRL.
Section 4 discussesour method.
Section 5 shows the experimental re-sults.
We conclude our work in Section 6.2 Related WorkAutomatic semantic role labeling was first intro-duced by Gildea and Jurafsky (2002).
They useda linear interpolation method and extract featuresfrom a parse tree to identify and classify the con-stituents in the FrameNet (Baker et al, 1998) withsyntactic parsing results.
Here, the basic featuresinclude Phrase Type, Parse Tree Path, Position.Most of the following works focused on featureengineering (Xue and Palmer, 2004; Jiang et al,2005) and machine learning models (Nielsen andPradhan, 2004; Pradhan et al, 2005a).
Someother works paid much attention to the robust SRL(Pradhan et al, 2005b) and post inference (Pun-yakanok et al, 2004).These feature-based methods are considered asthe state of the art method for SRL and achievedmuch success.
However, as we know, the standardflat features are less effective to model the syntac-tic structured information.
It is sensitive to smallchanges of the syntactic structure features.
Thiscan give rise to a data sparseness problem and pre-vent the learning algorithms from generalizing un-seen data well.As an alternative to the standard feature-basedmethods, kernel-based methods have been pro-posed to implicitly explore features in a high-dimension space by directly calculating the sim-ilarity between two objects using kernel function.In particular, the kernel methods could be effectivein reducing the burden of feature engineering forstructured objects in NLP problems.
This is be-cause a kernel can measure the similarity betweentwo discrete structured objects directly using theoriginal representation of the objects instead of ex-plicitly enumerating their features.Many kernel functions have been proposed inmachine learning community and have been ap-plied to NLP study.
In particular, Haussler (1999)and Watkins (1999) proposed the best-known con-volution kernels for a discrete structure.
In thecontext of convolution kernels, more and morekernels for restricted syntaxes or specific do-mains, such as string kernel for text categoriza-tion (Lodhi et al, 2002), tree kernel for syntacticparsing (Collins and Duffy, 2001), kernel for re-lation extraction (Zelenko et al, 2003; Culottaand Sorensen, 2004) are proposed and exploredin NLP domain.
Of special interest here, Mos-chitti (2004) proposed Predicate Argument Fea-ture (PAF) kernel under the framework of convo-lution tree kernel for SRL.
In this paper, we fol-low the same framework and design a novel hybridconvolution kernel for SRL.3 Feature-based methods for SRLUsually feature-based methods refer to the meth-ods which use the flat features to represent in-stances.
At present, most of the successful SRLsystems use this method.
Their features are usu-ally extended from Gildea and Jurafsky (2002)?swork, which uses flat information derived froma parse tree.
According to the literature, weselect the Constituent, Predicate, and Predicate-Constituent related features shown in Table 1.Feature DescriptionConstituent related featuresPhrase Type syntactic category of the constituentHead Word head word of the constituentLast Word last word of the constituentFirst Word first word of the constituentNamed Entity named entity type of the constituent?s head wordPOS part of speech of the constituentPrevious Word sequence previous word of the constituentNext Word sequence next word of the constituentPredicate related featuresPredicate predicate lemmaVoice grammatical voice of the predicate, either active or passiveSubCat Sub-category of the predicate?s parent nodePredicate POS part of speech of the predicateSuffix suffix of the predicatePredicate-Constituent related featuresPath parse tree path from the predicate to the constituentPosition the relative position of the constituent and the predicate, before or afterPath Length the nodes number on the parse tree pathPartial Path some part on the parse tree pathClause Layers the clause layers from the constituent to the predicateTable 1: Standard flat featuresHowever, to find relevant features is, as usual,a complex task.
In addition, according to the de-scription of the standard features, we can see thatthe syntactic features, such as Path, Path Length,bulk large among all features.
On the other hand,the previous researches (Gildea and Palmer, 2002;Punyakanok et al, 2005) have also recognized the74      Figure 2: Predicate Argument Feature spacenecessity of syntactic parsing for semantic role la-beling.
However, the standard flat features cannotmodel the syntactic information well.
A predicate-argument pair has two different Path features evenif their paths differ only for a node in the parsetree.
This data sparseness problem prevents thelearning algorithms from generalizing unseen datawell.
In order to address this problem, one methodis to list all sub-structures of the parse tree.
How-ever, both space complexity and time complexityare too high for the algorithm to be realized.4 Hybrid Convolution Tree Kernels forSRLIn this section, we introduce the previous ker-nel method for SRL in Subsection 4.1, discussour method in Subsection 4.2 and compare ourmethod with previous work in Subsection 4.3.4.1 Convolution Tree Kernels for SRLMoschitti (2004) proposed to apply convolutiontree kernels (Collins and Duffy, 2001) to SRL.He selected portions of syntactic parse trees,which include salient sub-structures of predicate-arguments, to define convolution kernels for thetask of predicate argument classification.
This por-tions selection method of syntactic parse trees isnamed as predicate-arguments feature (PAF) ker-nel.
Figure 2 illustrates the PAF kernel featurespace of the predicate buy and the argument Arg1in the circled sub-structure.The kind of convolution tree kernel is similar toCollins and Duffy (2001)?s tree kernel except thesub-structure selection strategy.
Moschitti (2004)only selected the relative portion between a predi-cate and an argument.Given a tree portion instance defined above, wedesign a convolution tree kernel in a way similarto the parse tree kernel (Collins and Duffy, 2001).Firstly, a parse tree T can be represented by a vec-tor of integer counts of each sub-tree type (regard-less of its ancestors):?
(T ) = (# of sub-trees of type 1, .
.
.
,# of sub-trees of type i, .
.
.
,# of sub-trees of type n)This results in a very high dimension since thenumber of different subtrees is exponential to thetree?s size.
Thus it is computationally infeasibleto use the feature vector ?
(T ) directly.
To solvethis problem, we introduce the tree kernel functionwhich is able to calculate the dot product betweenthe above high-dimension vectors efficiently.
Thekernel function is defined as following:K(T1, T2) = ??(T1),?(T2)?
=?i ?i(T1), ?i(T2)=?n1?N1?n2?N2?i Ii(n1) ?
Ii(n2)where N1 and N2 are the sets of all nodes intrees T1 and T2, respectively, and Ii(n) is the in-dicator function whose value is 1 if and only ifthere is a sub-tree of type i rooted at node n and0 otherwise.
Collins and Duffy (2001) show thatK(T1, T2) is an instance of convolution kernelsover tree structures, which can be computed inO(|N1| ?
|N2|) by the following recursive defi-nitions (Let ?
(n1, n2) =?i Ii(n1) ?
Ii(n2)):(1) if the children of n1 and n2 are different then?
(n1, n2) = 0;(2) else if their children are the same and they areleaves, then ?
(n1, n2) = ?
;(3) else ?
(n1, n2) = ?
?nc(n1)j=1 (1 +?
(ch(n1, j), ch(n2, j)))where nc(n1) is the number of the children ofn1, ch(n, j) is the jth child of node n and ?
(0 <?
< 1) is the decay factor in order to make thekernel value less variable with respect to the treesizes.4.2 Hybrid Convolution Tree KernelsIn the PAF kernel, the feature spaces are consid-ered as an integral portion which includes a pred-icate and one of its arguments.
We note that thePAF feature consists of two kinds of features: oneis the so-called parse tree Path feature and anotherone is the so-called Constituent Structure feature.These two kinds of feature spaces represent dif-ferent information.
The Path feature describes the75      Figure 3: Path and Constituent Structure featurespaceslinking information between a predicate and its ar-guments while the Constituent Structure featurecaptures the syntactic structure information of anargument.
We believe that it is more reasonableto capture the two different kinds of features sepa-rately since they contribute to SRL in different fea-ture spaces and it is better to give different weightsto fuse them.
Therefore, we propose two convo-lution kernels to capture the two features, respec-tively and combine them into one hybrid convolu-tion kernel for SRL.
Figure 3 is an example to il-lustrate the two feature spaces, where the Path fea-ture space is circled by solid curves and the Con-stituent Structure feature spaces is circled by dot-ted curves.
We name them Path kernel and Con-stituent Structure kernel respectively.Figure 4 illustrates an example of the distinc-tion between the PAF kernel and our kernel.
Inthe PAF kernel, the tree structures are equal whenconsidering constitutes NP and PRP, as shown inFigure 4(a).
However, the two constituents playdifferent roles in the sentence and should not belooked as equal.
Figure 4(b) shows the comput-ing example with our kernel.
During computingthe hybrid convolution tree kernel, the NP?PRPsubstructure is not computed.
Therefore, the twotrees are distinguished correctly.On the other hand, the constituent structure fea-ture space reserves the most part in the traditionalPAF feature space usually.
Then the ConstituentStructure kernel plays the main role in PAF kernelcomputation, as shown in Figure 5.
Here, believesis a predicate and A1 is a long sub-sentence.
Ac-cording to our experimental results in Section 5.2,we can see that the Constituent Structure kerneldoes not perform well.
Affected by this, the PAFkernel cannot perform well, either.
However, inour hybrid method, we can adjust the compromise  	  	(a) PAF Kernel   (b) Hybrid Convolution Tree KernelFigure 4: Comparison between PAF and HybridConvolution Tree KernelsFigure 5: An example of Semantic Role Labelingof the Path feature and the Constituent Structurefeature by tuning their weights to get an optimalresult.Having defined two convolution tree kernels,the Path kernel Kpath and the Constituent Struc-ture kernel Kcs, we can define a new kernel tocompose and extend the individual kernels.
Ac-cording to Joachims et al (2001), the kernel func-tion set is closed under linear combination.
Itmeans that the following Khybrid is a valid kernelif Kpath and Kcs are both valid.Khybrid = ?Kpath + (1?
?
)Kcs (1)where 0 ?
?
?
1.According to the definitions of the Path and theConstituent Structure kernels, each kernel is ex-plicit.
They can be viewed as a matching of fea-76tures.
Since the features are enumerable on thegiven data, the kernels are all valid.
Therefore, thenew kernel Khybrid is valid.
We name the new ker-nel hybrid convolution tree kernel, Khybrid.Since the size of a parse tree is not con-stant, we normalize K(T1, T2) by dividing it by?K(T1, T1) ?K(T2, T2)4.3 Comparison with Previous WorkIt would be interesting to investigate the differ-ences between our method and the feature-basedmethods.
The basic difference between them liesin the instance representation (parse tree vs. fea-ture vector) and the similarity calculation mecha-nism (kernel function vs. dot-product).
The maindifference between them is that they belong to dif-ferent feature spaces.
In the kernel methods, weimplicitly represent a parse tree by a vector of in-teger counts of each sub-tree type.
That is to say,we consider all the sub-tree types and their occur-ring frequencies.
In this way, on the one hand,the predicate-argument related features, such asPath, Position, in the flat feature set are embed-ded in the Path feature space.
Additionally, thePredicate, Predicate POS features are embeddedin the Path feature space, too.
The constituent re-lated features, such as Phrase Type, Head Word,Last Word, and POS, are embedded in the Con-stituent Structure feature space.
On the other hand,the other features in the flat feature set, such asNamed Entity, Previous, and Next Word, Voice,SubCat, Suffix, are not contained in our hybridconvolution tree kernel.
From the syntactic view-point, the tree representation in our feature spaceis more robust than the Parse Tree Path feature inthe flat feature set since the Path feature is sensi-tive to small changes of the parse trees and it alsodoes not maintain the hierarchical information ofa parse tree.It is also worth comparing our method withthe previous kernels.
Our method is similar tothe Moschitti (2004)?s predicate-argument feature(PAF) kernel.
However, we differentiate the Pathfeature and the Constituent Structure feature in ourhybrid kernel in order to more effectively capturethe syntactic structure information for SRL.
In ad-dition Moschitti (2004) only study the task of ar-gument classification while in our experiment, wereport the experimental results on both identifica-tion and classification.5 Experiments and DiscussionThe aim of our experiments is to verify the effec-tiveness of our hybrid convolution tree kernel andand its combination with the standard flat features.5.1 Experimental Setting5.1.1 CorpusWe use the benchmark corpus provided byCoNLL-2005 SRL shared task (Carreras andMa`rquez, 2005) provided corpus as our training,development, and test sets.
The data consist ofsections of the Wall Street Journal (WSJ) part ofthe Penn TreeBank (Marcus et al, 1993), withinformation on predicate-argument structures ex-tracted from the PropBank corpus (Palmer et al,2005).
We followed the standard partition usedin syntactic parsing: sections 02-21 for training,section 24 for development, and section 23 fortest.
In addition, the test set of the shared taskincludes three sections of the Brown corpus.
Ta-ble 2 provides counts of sentences, tokens, anno-tated propositions, and arguments in the four datasets.Train Devel tWSJ tBrownSentences 39,832 1,346 2,416 426Tokens 950,028 32,853 56,684 7,159Propositions 90,750 3,248 5,267 804Arguments 239,858 8,346 14,077 2,177Table 2: Counts on the data setThe preprocessing modules used in CONLL-2005 include an SVM based POS tagger (Gime?nezand Ma`rquez, 2003), Charniak (2000)?s full syn-tactic parser, and Chieu and Ng (2003)?s NamedEntity recognizer.5.1.2 EvaluationThe system is evaluated with respect toprecision, recall, and F?=1 of the predicted ar-guments.
Precision (p) is the proportion of ar-guments predicted by a system which are cor-rect.
Recall (r) is the proportion of correct ar-guments which are predicted by a system.
F?=1computes the harmonic mean of precision andrecall, which is the final measure to evaluate theperformances of systems.
It is formulated as:F?=1 = 2pr/(p + r).
srl-eval.pl2 is the officialprogram of the CoNLL-2005 SRL shared task toevaluate a system performance.2http://www.lsi.upc.edu/?srlconll/srl-eval.pl775.1.3 SRL StrategiesWe use constituents as the labeling units to formthe labeled arguments.
In order to speed up thelearning process, we use a four-stage learning ar-chitecture:Stage 1: To save time, we use a pruningstage (Xue and Palmer, 2004) to filter out theconstituents that are clearly not semantic ar-guments to the predicate.Stage 2: We then identify the candidates derivedfrom Stage 1 as either arguments or non-arguments.Stage 3: A multi-category classifier is used toclassify the constituents that are labeled as ar-guments in Stage 2 into one of the argumentclasses plus NULL.Stage 4: A rule-based post-processing stage (Liuet al, 2005) is used to handle some un-matched arguments with constituents, such asAM-MOD, AM-NEG.5.1.4 ClassifierWe use the Voted Perceptron (Freund andSchapire, 1998) algorithm as the kernel machine.The performance of the Voted Perceptron is closeto, but not as good as, the performance of SVM onthe same problem, while saving computation timeand programming effort significantly.
SVM is tooslow to finish our experiments for tuning parame-ters.The Voted Perceptron is a binary classifier.
Inorder to handle multi-classification problems, weadopt the one vs. others strategy and select theone with the largest margin as the final output.
Thetraining parameters are chosen using developmentdata.
After 5 iteration numbers, the best perfor-mance is achieved.
In addition, Moschitti (2004)?sTree Kernel Tool is used to compute the tree kernelfunction.5.2 Experimental ResultsIn order to speed up the training process, in thefollowing experiments, we ONLY use WSJ sec-tions 02-05 as training data.
The same as Mos-chitti (2004), we also set the ?
= 0.4 in the com-putation of convolution tree kernels.In order to study the impact of ?
in hybrid con-volution tree kernel in Eq.
1, we only use the hy-brid kernel between Kpath and Kcs.
The perfor-mance curve on development set changing with ?is shown in Figure 6.Figure 6: The performance curve changing with ?The performance curve shows that when ?
=0.5, the hybrid convolution tree kernel gets thebest performance.
Either the Path kernel (?
= 1,F?=1 = 61.26) or the Constituent Structure kernel(?
= 0, F?=1 = 54.91) cannot perform better thanthe hybrid one.
It suggests that the two individualkernels are complementary to each other.
In ad-dition, the Path kernel performs much better thanthe Constituent Structure kernel.
It indicates thatthe predicate-constituent related features are moreeffective than the constituent features for SRL.Table 3 compares the performance comparisonamong our Hybrid convolution tree kernel, Mos-chitti (2004)?s PAF kernel, standard flat featureswith Linear kernels, and Poly kernel (d = 2).
Wecan see that our hybrid convolution tree kernel out-performs the PAF kernel.
It empirically demon-strates that the weight linear combination in ourhybrid kernel is more effective than PAF kernel forSRL.However, our hybrid kernel still performs worsethan the standard feature based system.
This issimple because our kernel only use the syntac-tic structure information while the feature-basedmethod use a large number of hand-craft diversefeatures, from word, POS, syntax and semantics,NER, etc.
The standard features with polynomialkernel gets the best performance.
The reason isthat the arbitrary binary combination among fea-tures implicated by the polynomial kernel is usefulto SRL.
We believe that combining the two meth-ods can perform better.In order to make full use of the syntacticinformation and the standard flat features, wepresent a composite kernel between hybrid kernel(Khybrid) and standard features with polynomial78Hybrid PAF Linear PolyDevel 66.01 64.38 68.71 70.25Table 3: Performance (F?=1) comparison amongvarious kernelskernel (Kpoly):Kcomp = ?Khybrid + (1?
?
)Kpoly (2)where 0 ?
?
?
1.The performance curve changing with ?
in Eq.
2on development set is shown in Figure 7.Figure 7: The performance curve changing with ?We can see that when ?
= 0.5, the systemachieves the best performance and F?=1 = 70.78.It?s statistically significant improvement (?2 testwith p = 0.1) than only using the standard featureswith the polynomial kernel (?
= 0, F?=1 = 70.25)and much higher than only using the hybrid con-volution tree kernel (?
= 1, F?=1 = 66.01).The main reason is that the convolution tree ker-nel can represent more general syntactic featuresthan standard flat features, and the standard flatfeatures include the features that the convolutiontree kernel cannot represent, such as Voice, Sub-Cat.
The two kind features are complementary toeach other.Finally, we train the composite method usingthe above setting (Eq.
2 with when ?
= 0.5) on theentire training set.
The final performance is shownin Table 4.6 Conclusions and Future WorkIn this paper we proposed the hybrid convolu-tion kernel to model syntactic structure informa-tion for SRL.
Different from the previous convo-lution tree kernel based methods, our contributionPrecision Recall F?=1Development 80.71% 68.49% 74.10Test WSJ 82.46% 70.65% 76.10Test Brown 73.39% 57.01% 64.17Test WSJ Precision Recall F?=1Overall 82.46% 70.65% 76.10A0 87.97% 82.49% 85.14A1 80.51% 71.69% 75.84A2 75.79% 52.16% 61.79A3 80.85% 43.93% 56.93A4 83.56% 59.80% 69.71A5 100.00% 20.00% 33.33AM-ADV 66.27% 43.87% 52.79AM-CAU 68.89% 42.47% 52.54AM-DIR 56.82% 29.41% 38.76AM-DIS 79.02% 75.31% 77.12AM-EXT 73.68% 43.75% 54.90AM-LOC 72.83% 50.96% 59.97AM-MNR 68.54% 42.44% 52.42AM-MOD 98.52% 96.37% 97.43AM-NEG 97.79% 96.09% 96.93AM-PNC 49.32% 31.30% 38.30AM-TMP 82.15% 68.17% 74.51R-A0 86.28% 87.05% 86.67R-A1 80.00% 74.36% 77.08R-A2 100.00% 31.25% 47.62R-AM-CAU 100.00% 50.00% 66.67R-AM-EXT 50.00% 100.00% 66.67R-AM-LOC 92.31% 57.14% 70.59R-AM-MNR 20.00% 16.67% 18.18R-AM-TMP 68.75% 63.46% 66.00V 98.65% 98.65% 98.65Table 4: Overall results (top) and detailed resultson the WSJ test (bottom).is that we distinguish between the Path and theConstituent Structure feature spaces.
Evaluationon the datasets of CoNLL-2005 SRL shared task,shows that our novel hybrid convolution tree ker-nel outperforms the PAF kernel method.
Althoughthe hybrid kernel base method is not as good asthe standard rich flat feature based methods, it canimprove the state of the art feature-based methodsby implicating the more generalizing syntactic in-formation.Kernel-based methods provide a good frame-work to use some features which are difficult tomodel in the standard flat feature based methods.For example the semantic similarity of words canbe used in kernels well.
We can use general pur-pose corpus to create clusters of similar words oruse available resources like WordNet.
We can alsouse the hybrid kernel method into other tasks, suchas relation extraction in the future.79AcknowledgementsThe authors would like to thank the reviewers fortheir helpful comments and Shiqi Zhao, YanyanZhao for their suggestions and useful discussions.This work was supported by National NaturalScience Foundation of China (NSFC) via grant60435020, 60575042, and 60503072.ReferencesCollin F. Baker, Charles J. Fillmore, and John B. Lowe.1998.
The Berkeley FrameNet project.
In Proceed-ings of the ACL-Coling-1998, pages 86?90.Xavier Carreras and Llu?
?s Ma`rquez.
2004.
Introduc-tion to the CoNLL-2004 shared task: Semantic rolelabeling.
In Proceedings of CoNLL-2004, pages 89?97.Xavier Carreras and Llu?
?s Ma`rquez.
2005.
Introduc-tion to the CoNLL-2005 shared task: Semantic rolelabeling.
In Proceedings of CoNLL-2005, pages152?164.Eugene Charniak.
2000.
A maximum-entropy-inspired parser.
In Proceedings of NAACL-2000.Hai Leong Chieu and Hwee Tou Ng.
2003.
Named en-tity recognition with a maximum entropy approach.In Proceedings of CoNLL-2003, pages 160?163.Michael Collins and Nigel Duffy.
2001.
Convolu-tion kernels for natural language.
In Proceedingsof NIPS-2001.Nello Cristianini and John Shawe-Taylor.
2000.
An In-troduction to Support Vector Machines.
CambridgeUniversity Press, Cambirdge University.Aron Culotta and Jeffrey Sorensen.
2004.
Dependencytree kernels for relation extraction.
In Proceedingsof ACL-2004, pages 423?429.Yoav Freund and Robert E. Schapire.
1998.
Largemargin classification using the perceptron algorithm.In Computational Learning Theory, pages 209?217.Daniel Gildea and Daniel Jurafsky.
2002.
Automaticlabeling of semantic roles.
Computational Linguis-tics, 28(3):245?288.Daniel Gildea and Martha Palmer.
2002.
The necessityof parsing for predicate argument recognition.
InProceedings of ACL-2002, pages 239?246.Jesu?s Gime?nez and Llu?
?s Ma`rquez.
2003.
Fast andaccurate part-of-speech tagging: The svm approachrevisited.
In Proceedings of RANLP-2003.David Haussler.
1999.
Convolution kernels on dis-crete structures.
Technical Report UCSC-CRL-99-10, July.Zheng Ping Jiang, Jia Li, and Hwee Tou Ng.
2005.
Se-mantic argument classification exploiting argumentinterdependence.
In Proceedings of IJCAI-2005.Thorsten Joachims, Nello Cristianini, and John Shawe-Taylor.
2001.
Composite kernels for hypertext cat-egorisation.
In Proceedings of ICML-2001, pages250?257.Ting Liu, Wanxiang Che, Sheng Li, Yuxuan Hu, andHuaijun Liu.
2005.
Semantic role labeling systemusing maximum entropy classifier.
In Proceedingsof CoNLL-2005, pages 189?192.Huma Lodhi, Craig Saunders, John Shawe-Taylor,Nello Cristianini, and Chris Watkins.
2002.
Textclassification using string kernels.
Journal of Ma-chine Learning Research, 2:419?444.Mitchell P. Marcus, Mary Ann Marcinkiewicz, andBeatrice Santorini.
1993.
Building a large anno-tated corpus of english: the penn treebank.
Compu-tational Linguistics, 19(2):313?330.Alessandro Moschitti.
2004.
A study on convolutionkernels for shallow statistic parsing.
In Proceedingsof ACL-2004, pages 335?342.Rodney D. Nielsen and Sameer Pradhan.
2004.
Mix-ing weak learners in semantic parsing.
In Proceed-ings of EMNLP-2004.Martha Palmer, Dan Gildea, and Paul Kingsbury.
2005.The proposition bank: An annotated corpus of se-mantic roles.
Computational Linguistics, 31(1).Sameer Pradhan, Kadri Hacioglu, Valeri Krugler,Wayne Ward, James H. Martin, and Daniel Juraf-sky.
2005a.
Support vector learning for semanticargument classification.
Machine Learning Journal.Sameer Pradhan, Wayne Ward, Kadri Hacioglu, JamesMartin, and Daniel Jurafsky.
2005b.
Semantic rolelabeling using different syntactic views.
In Proceed-ings of ACL-2005, pages 581?588.Vasin Punyakanok, Dan Roth, Wen-tau Yih, and DavZimak.
2004.
Semantic role labeling via integerlinear programming inference.
In Proceedings ofColing-2004, pages 1346?1352.Vasin Punyakanok, Dan Roth, and Wen tau Yih.
2005.The necessity of syntactic parsing for semantic rolelabeling.
In Proceedings of IJCAI-2005, pages1117?1123.Chris Watkins.
1999.
Dynamic alignment kernels.Technical Report CSD-TR-98-11, Jan.Nianwen Xue and Martha Palmer.
2004.
Calibratingfeatures for semantic role labeling.
In Proceedingsof EMNLP 2004.Dmitry Zelenko, Chinatsu Aone, and AnthonyRichardella.
2003.
Kernel methods for relation ex-traction.
Journal of Machine Learning Research,3:1083?1106.80
