Coling 2008: Proceedings of the workshop on Human Judgements in Computational Linguistics, pages 33?41Manchester, August 2008Polysemy in verbs: systematic relations between sensesand their effect on annotationAnna Rumshisky?Dept.
of Computer ScienceBrandeis UniversityWaltham, MA USAarum@cs.brandeis.eduOlga Batiukova???Dept.
of Spanish PhilologyMadrid Autonomous UniversityMadrid, Spainvolha.batsiukova@uam.esAbstractSense inventories for polysemous predicatesare often comprised by a number of relatedsenses.
In this paper, we examine differenttypes of relations within sense inventories andgive a qualitative analysis of the effects theyhave on decisions made by the annotators andannotator error.
We also discuss some commontraps and pitfalls in design of sense inventories.We use the data set developed specifically forthe task of annotating sense distinctions depen-dent predominantly on semantics of the argu-ments and only to a lesser extent on syntacticframe.1 IntroductionLexical ambiguity is pervasive in natural language, andits resolution has been used to improve performance ofa number of natural language processing (NLP) appli-cations, such as statistical machine translation (Chanet al, 2007; Carpuat and Wu, 2007), cross-languageinformation retrieval and question answering (Resnik,2006).
Sense differentiation for the predicates dependson a number of factors, including syntactic frame, se-mantics of the arguments and adjuncts, contextual cluesfrom the wider context, text domain identification, etc.Preparing sense-tagged data for training and evalua-tion of word sense disambiguation (WSD) systems in-volves two stages: (1) creating a sense inventory and(2) applying it in annotation.
Creating sense invento-ries for polysemous words is a task that is notoriouslydifficult to formalize.
For polysemous verbs especially,constellations of related meanings make this task evenmore difficult.
In lexicography, ?lumping and splitting?senses during dictionary construction ?
i.e.
decidingwhen to describe a set of usages as a separate sense?
is a well-known problem (Hanks and Pustejovsky,c?
2008.
Licensed under the Creative CommonsAttribution-Noncommercial-Share Alike 3.0 Unported li-cense (http://creativecommons.org/licenses/by-nc-sa/3.0/).Some rights reserved.2005; Kilgarriff, 1997).
It is often resolved on an ad-hoc basis, resulting in numerous cases of ?overlappingsenses?, i.e.
instances when the same occurrence mayfall under more than one sense category simultaneously.This problem has also been the subject of extensivestudy in lexical semantics, addressing questions suchas when the context selects a distinct sense and whenit merely modulates the meaning, what is the regularrelationship between related senses, and what composi-tional processes are involved in sense selection (Puste-jovsky, 1995; Cruse, 1995; Apresjan, 1973).
A num-ber of syntactic and semantic tests are traditionally ap-plied for sense identification, such as examining syn-onym series, compatible syntactic environments, coor-dination tests such as cross-understanding or zeugmatest (Cruse, 2000).
None of these tests are conclu-sive and normally a combination of factors is used.At the recent Senseval competitions (Mihalcea et al,2004; Snyder and Palmer, 2004; Preiss and Yarowsky,2001), the choice of sense inventories frequently pre-sented problems, spurring the efforts to create coarser-grained sense inventories (Hovy et al, 2006; Palmer etal., 2007; Navigli, 2006).Part of the reason for such difficulties in establish-ing a set of senses available to a lexical item is thatthe meaning of a polysemous verb is often determinedin composition and depends to the same extent on se-mantics of the particular arguments as it does on thebase meaning of the verb itself.
A number of system-atic relations often holds between different senses of apolysemous verb.
Depending on the kind of ambiguityinvolved in each case, some senses are easier to dis-tinguish than others.
Sense-tagged data (e.g.
SemCor(Landes et al, 1998), PropBank (Palmer et al, 2005),OntoNotes (Hovy et al, 2006)) typically provides noway to differentiate between sense distinctions moti-vated by different factors.
Treating different disam-biguation factors separately would allow one to exam-ine the contribution of each factor, as well as the successof a given algorithm in identifying the correspondingsenses.Within the scope of a sentence, syntactic frame andsemantics of the arguments are most prominent in sense33disambiguation.
The latter is often more subtle andhence complex.
Our goal in the present study was to tar-get sense distinctions motivated strongly or exclusivelyby differences in argument semantics.
We base thepresent discussion on the sense-tagged data set we de-veloped for 20 polysemous verbs.
We argue below thatcases which can not be reliably disambiguated by hu-mans introduce noise into the data and therefore shouldbe kept out, a principle adhered to in the design of thisdata set.The choice of argument semantics as the target dis-ambiguation factor was motivated by several consider-ations.
In automatic sense detection systems, argumentsemantics is often represented using external resourcessuch as thesauri or shallow ontologies.
Sense inductionsystems using distributional information often do nottake into account the possible implications of inducedword clusters for sense disambiguation.
Our goal wasto analyze differences in argument semantics that con-tribute to disambiguation.In this paper, we discuss different kinds of systematicrelations observed between senses of polysemous pred-icates and examine the effects they have on decisionsmade by the annotators.
We also examine sense in-ventories for other factors that influence inter-annotatoragreement rates and lead to annotation error.
In Section2, we discuss some of the factors that influence com-pilation of sense inventories and the methodology in-volved.
In Section 3, we describe briefly the data setand the annotation task.
In Sections 4 and 5, we discussthe relations observed between different senses withinsense inventories in our data set, their effect on deci-sions made by the annotators, and the related annotationerrors.2 Defining A Sense InventorySeveral current resource-oriented projects undertake toformalize the procedure of identifying a word sense.FrameNet (Ruppenhofer et al, 2006) attempts to orga-nize lexical information in terms of script-like semanticframes, with semantic and syntactic combinatorial pos-sibilities specified for each frame-evoking lexical unit(word/sense pairing).
Semantics of the arguments isrepresented by Fillmore?s case roles (frame elements)which are derived on ad-hoc basis for each frame.In OntoNotes project, annotators use small-scale cor-pus analysis to create sense inventories derived bygrouping together WordNet senses.
The procedure isrestricted to maintain 90% inter-annotator agreement(Hovy et al, 2006).Corpus Pattern Analysis (CPA) (Hanks and Puste-jovsky, 2005; Pustejovsky et al, 2004) attempts to cat-alog prototypical norms of usage for individual words,specifying them in terms of context patterns.
As a cor-pus analysis technique, CPA has its origins in the anal-ysis of large corpora for lexicographic purposes, of thekind that was used for compiling the Cobuild dictionary(Sinclair and Hanks, 1987).
Each pattern gives a com-bination of surface textual clues and argument specifi-cations.
A lexicographer creates a set of patterns bysorting a concordance for the target predicate accordingto the context features.
In the present study, we use amodification of the CPA technique in the way explainedin Section 3.In CPA, syntactic and textual clues include argu-ment structure and minor syntactic categories such aslocatives and adjuncts; collocates from wider context;subphrasal cues such as genitives, partitives, bare plu-ral/determiner, infinitivals, negatives, etc.
Semanticsof the arguments is represented either through a set ofshallow semantic types corresponding to basic seman-tic features (e.g.
Person, Location, PhysObj, Abstract,Event, etc.)
or extensionally through lexical sets, whichare effectively collections of lexical items.1Several CPA patterns may correspond to a singlesense.
The patterns vary in syntactic structure or the en-coding of semantic roles relative to the described event.For example, for the verb treat, DOCTOR treating PA-TIENT and DOCTOR treating DISEASE both correspondto the medical sense of treat.
Knowing which seman-tic role is expressed by a particular argument is oftenuseful for performing inference.
For instance, treatinga disease eliminates the disease, but not the patient.
Inthe present annotation task, each pattern is viewed assense in construction and labeled as a separate sense.In the rest of the paper, we will use the term ?sense?
torefer also to such microsenses.For the cases where sense differentiation dependsstrongly on differences in semantics of the arguments,several factors further complicate creating a sense in-ventory.
Prototypicality as a general principle of cat-egory organization seems to play an important role indefining both the boundaries of senses and the corre-sponding argument groupings.
The same sense of thepredicate is often activated by a number of semanticallydiverse arguments.
Such argument sets are frequentlyorganized around a core of typical members that area ?good fit?
with respect to semantic requirements ofthe corresponding sense of the target.
The relevant se-mantic feature is prominent for them, while other, moreperipheral members of the argument set, merely allowthe relevant interpretation (see Rumshisky (2008) fordiscussion).
For example, the verb absorb has a senseinvolving absorbing a substance, and the typical mem-bers of the corresponding argument set would be actualsubstances, such as oil, oxygen, water, air, salt, etc.
Butgoodness, dirt, flavor, moisture would also activate thesame sense.Each decision to split a sense and make another cat-egory is to a certain extent an arbitrary decision.
Forexample, for the verb absorb, one can separate absorb-ing a substance (oil, oxygen, water, air, salt) from ab-sorbing energy (radiation, heat, sound, energy).
Thelatter sense may or may not be separated from absorb-1See Rumshisky et al (2006) and Pustejovsky et al (2004)for more detail.34ing impact (blow, shock, stress).
But it is a marked con-tinuum, i.e.
certain points in the continuum are moreprominent, with necessity of a given concept reflectedin the frequency of use.When several senses are postulated based on argu-ment distinctions, there are almost always boundarycases that can be seen to belong to both categories.Consider, for example, two senses defined for the verblaunch and the corresponding direct objects in (1):(1) a. Physically propel an object into the air or watermissile, rocket, torpedo, satellite, shuttle, craftb.
Begin or initiate an endeavorcampaign, initiative, investigation, expedition, drive,competition, crusade, attack, assault, inquiryThe senses seem to be very clearly separated, yet ex-amples like launch a ship clearly fall on the bound-ary: while ships are physical objects propelled into wa-ter, launching a ship can be virtually synonymous withlaunching an expedition.Similarly, for the verb conclude, two senses belowwhich are linked to nominal complements are clearlyseparated:(2) a. finishmeeting, debate, investigation, visit, tour, discussion;letter, chapter, novelb.
reach an agreementtreaty, agreement, deal, contract, truce, alliance,ceasefire, saleHowever, conclude negotiations is clearly a boundarycase where both interpretations are equally possible(negotiations may be concluded without reaching anagreement).
In fact, the two annotators chose differentsenses for this example:2(3) We were able to operate under a lease agreement untilpurchase negotiations were concluded.annoA: finishannoB: reach an agreementIn many cases, postulating a separate sense for a co-herent set of nominal complements is not justified, asthere are regular semantic processes that allow the com-plements to satisfy selectional requirements of the verb.For example, the verb conclude, in the finish sense ac-cepts EVENT complements.
Therefore, nouns such asletter, chapter, novel in (2) must be coerced into eventscorresponding to the activity that typically brings themabout, that is, re-interpreted as events of writing (theirAgentive quale, cf.
Pustejovsky (1995)).
Similarly, theverb deny in the first sense (state or maintain that some-thing is untrue) accepts PROPOSITION complements:(4) a. state or maintain that something is untrueallegations, reports, rumour; significance, impor-tance, difference; attack, assault, involvementb.
refuse to grant somethingaccess, visa, approval, funding, license2All examples are taken from the annotated data set.In some cases, sentence structure was slightly modified forbrevity.Event nouns such as attack and assault are coerced intoa propositional reading, as are relational nouns such assignificance and importance.Interestingly, as we have noted before (Rumshiskyet al, 2006), each predicate imposes its own gradationwith respect to prototypicality of elements of the ar-gument set.
As a result, even though basic semantictypes such as PHYSOBJ, ANIMATE, EVENT, are useduniformly by many predicates, argument sets, while se-mantically similar, typically differ between predicates.For example, fall in the subject position and cut in thedirect object position select for things that can be de-creased:(5) a. cut (dobj): reduce or lessenprice, inflation, profits, cost, emission, spending,deficit, wages overhead, production, consumption,fees, staffb.
fall (subj): decreaseprice, inflation, profits, attendance, turnover, temper-ature, membership, import, demand, levelWhile there is a clear commonality between these argu-ment sets, the overlap is only partial.
To give anotherexample, consider INFORMATION-selecting predicatesexplain (subj), grasp (dobj) and know (dobj).
The nounsbook and note occur in the subject position of explain;answer occurs both as the subject of explain and directobject of know; however, grasp accepts neither of thesenouns as direct object.
Thus, the actual selectional be-havior of the predicates does not seem to be well de-scribed in terms of a fixed set of types, which is whatis typically assumed by many ontologies used in auto-matic WSD.3 Task DescriptionWe were interested specifically in those cases wheredisambiguation needs to be made without relying onsyntactic frame, and the main source of disambiguationis semantics of the arguments.
Such cases are harderto identify formally in the development of sense inven-tories and harder for the annotators to determine.
Forexample, phrasal verbs or idiomatic constructions thathelp identify a particular sense were intentionally ex-cluded from our data set.
Thus, for the verb cut, one ofthe senses involves cutting out a shape or a form (e.g.cut a suit), but the sentences with the correspondingphrasal form cut out were thrown out.Even so, syntactic clues that contribute to disam-biguation in some cases overrule the interpretation sug-gested by the argument.
For example, for the verb deny,in deny the attack, the direct object strongly suggestsa propositional interpretation for deny (that the attackdidn?t happen).
However, the use of ditransitive con-struction (indicated in the example below by the pastparticiple) overrules this interpretation, and we get therefuse to grant sense:(6) Astorre, denied his attack, had stayed in camp, uneasilybrooding.35In fact, during the actual annotation, one of the anno-tators did not recognize the use of past participle, anderroneously assigned the state or maintain something tobe untrue sense to this sentence.3.1 Data setThe data set was developed using the British NationalCorpus (BNC), which is more balanced than the morecommonly annotated Wall Street Journal data.
We se-lected 20 polysemous verbs with sense distinctions thatwere judged to depend for disambiguation on seman-tics of the argument in several argument positions, in-cluding direct object (dobj), subject (subj), or indirectobject within a prepositional phrase governed by with(iobj with):dobj: absorb, acquire, admit, assume, claim, conclude,cut, deny, dictate, drive, edit, enjoy, fire, grasp, know,launchsubj: explain, fall, leadiobj with: meetWe used the Sketch Engine (Kilgarriff et al, 2004)both to select the verbs and to aid the creation of thesense inventories.
The Sketch Engine is a lexicographictool that lists collocates that co-occur with a given targetword in the specified grammatical relation.
The collo-cates are sorted by their association score with the tar-get.A set of senses was created for each verb using amodification of the CPA technique (Pustejovsky et al,2004).
A set of complements was examined in theSketch Engine.
If a clear division was observed be-tween semantically different groups of collocates in acertain argument position, the verb was selected.
Forsemantically distinct groups of collocates, a separatesense was added to the sense inventory for the target.For example, for the verb acquire, a separate sense wasadded for each of the following sets of direct objects:(7) a.
Take on certain characteristicsshape, meaning, color, form, dimension, reality, sig-nificance, identity, appearance, characteristic, flavorb.
Purchase or become the owner of propertyland, stock, business, property, wealth, subsidiary, es-tate, stakeThe sense inventory for each verb was cross-checkedagainst several resources, including WordNet, Prop-Bank, Merriam-Webster and Oxford English dictionar-ies, and existing correspondences in FrameNet (Rup-penhofer et al, 2006; Hiroaki, 2003), OntoNotes (Hovyet al, 2006),3 and CPA patterns (Hanks and Puste-jovsky, 2005; Rumshisky and Pustejovsky, 2006; Puste-jovsky et al, 2004).We performed test annotation on 100 instances, withthe sense inventory additionally modified upon exam-ining the results of the annotation.
This sense inven-tory was provided to two annotators, along with 2003Sense inventories released for the 65 verbs made avail-able for SemEval-2007.sentences for each verb.
Each sentence was pre-parsedwith RASP (Briscoe and Carroll, 2002), and the headof the target argument phrase was identified.
Misparseswere manually corrected in post-processing.3.2 Defining the task for the annotatorsData set creation for a WSD task is notoriously hard (cf.Palmer et al (2007)), as the annotators are frequentlyforced to perform disambiguation on sentences whereno disambiguation can really be performed.
This is thecase, for example, for overlapping senses, where morethan one sense is activated simultaneously (Rumshisky,2008; Pustejovsky and Boguraev, 1993).
The goal wasto create, for each target word, a set of instances wherehumans had no trouble disambiguating between differ-ent senses.Two undergraduate linguistics majors served as an-notators.
The annotators were instructed to mark eachsentence with the most fitting sense.
The annotatorswere allowed to mark the sentence as ?N/A?
and wereinstructed to do so if (i) the sense inventory was missingthe relevant sense, (ii) more than one sense seemed tofit, or (iii) the sense was impossible to determine fromthe context.With respect to metaphoric senses, instructions wereto throw out cases of creative use where the interpreta-tion was difficult or not immediately clear.
The caseswhere the target grammatical relation was actually ab-sent from the sentence also had to be marked as ?N/A?(e.g.
for fire, sentences without direct object, e.g.
astolen car was fired upon).
The annotators were alsoinstructed to mark idiomatic expressions and phrasalverbs as ?N/A?, e.g.
for the verb fall: fall from favor,fall through, fall in, fall back, fall silent, fall short, fallin love.Disagreements between the annotators were resolvedin adjudication by the co-authors.
The average inter-annotator agreement (ITA) for our data set was com-puted as a macro-average of the percentage of instancesthat were annotated with the same sense by both anno-tators to the total number of instances retained in thedata set for each verb.
The instances that were markedas ?N/A?
by one of the annotators (or thrown out duringthe adjudication) were not included in the computation.The ITA value for our data set was 95%.
However, aswe will see below, the ITA values do not always reflectthe actual accuracy of annotation, due to some commonproblems with sense inventories.3.3 Glossing a senseA very common problem with glossing a sense in-volves the situation where a sense inventory includestwo senses one of which is an extension of the other.The derived sense may be related to the primary sensethrough metaphor, and this often results in the for-mer taking on a semantically less specific interpreta-tion.
The problem with creating glosses in this situa-tion is that the words used may have sense distinctions36parallel to the ones in the target verb being described.This leaves the annotators free to choose either sense.This seems to be the case, for example, with OntoNotessense inventory for fire, where ignite or become ignitedis the gloss under which very divergent examples aregrouped: oil fired the furnace (literal, primary sense)and curiosity fired my imagination (metaphoric exten-sion).
Clearly, annotators were having a problem withthis sense due to the fact that the verb ignite has sensedistinctions which are based on the same metaphor (fire= inspire) and therefore are very similar to those of theverb fire.In case of semantic underspecification, annotatorsmay be left free to choose the more generic sense,which contaminates the data set while not being re-flected in the inter-annotator agreement values.
For ex-ample, in our sense inventory for acquire, the gloss foracquire a new customer has to be very generic.
Weused the gloss ?become associated with something, of-ten newly brought into being?.
However, that led theannotators to overuse this gloss and select this sense incases where a more specific gloss was more appropri-ate:4(8) By this treaty, Russia acquired a Black Sea coastline.annoA: become associated with something, often newlybrought into beingannoB: become associated with something, ...correct: purchase or become the owner of propertyFor a more detailed analysis of this phenomenon, seeSection 5.4 Relations Between SensesIn this section, we discuss linguistic processes underly-ing relations between senses within a single sense in-ventory.
We believe that a detailed analysis of theseprocesses should help to account for the annotator?sability to perform disambiguation.
Some sense distinc-tions appear more striking to the annotators, dependingon the type of relation involved.In line with existing approaches to sense relations,we will look at both the linguistic structures involvedin sense modification and the productive processes act-ing on linguistic structures.
For the purposes of ourpresent discussion, we interpret the literal (physical, di-rect) senses to be primary, with respect to more abstractor metaphorical senses.4.1 Argument structure alternationsSome of the most striking differences between thesenses are related to the argument structure alternations:1.
Different case roles (frame elements) may be ex-pressed in the same argument position (in this case, di-rect object), corresponding to different perspectives onthe same event.
For example, direct object position ofthe verb drive may be filled by VEHICLE, DISTANCE,4We will refer to annotators A and B as annoA and annoB.or PHYSOBJ giving rise to three distinct senses: (i) op-erate a vehicle controlling its motion, (ii) travel in a ve-hicle a certain distance, and (iii) transport something orsomeone.
Similarly, for the verb fire, PROJECTILE orWEAPON in direct object position give rise to two re-lated senses: (i) shoot, discharge a weapon, (ii) shoot,propel a projectile.2.
The distinction between propositional and non-propositional complements, as for the verbs admit anddeny in (9) and (10):(9) a. admit defeat, inconsistency, offense(acknowledge the truth or reality of )b. admit patients, students(grant entry or allow into a community)(10) a. deny reports, importance, allegations(state or maintain to be untrue)b. deny visa, access(refuse to grant)3.
There is a mutual dependency between subcate-gorization features of the complements in different ar-gument positions.
For example, the [+animate] subjectmay combine with specific complements not availablefor [?animate], as for the two senses of acquire: (i)learn and (ii) take on certain characteristics.
CompareNPsubj[-animate] acquire NPdobj(language, man-ners, knowledge, skill) vs. NPsubj[?animate] acquireNPdobj(importance, significance).
Similarly, for ab-sorb, compare NPsubj[?animate] absorb NPdobj(sub-stance) and NPsubj[+animate] absorb NPdobj(skill,information).
Note that, as one would expect, such de-pendencies are inevitable even despite the fact that ourdata set was developed specifically to target sense dis-tinctions dependent on a single argument position.4.2 Event structure modificationEvent structure modifications (i.e.
operations affectingaspectual properties of the predicate) are another sourceof sense differentiation.
Two cases appear most promi-nent:1.
The event structure is modified along with thecharacteristics of the arguments.
For example, for en-joy, compare enjoy skiing, vacation (DYNAMIC EVENT)with enjoying a status (STATE).
Similarly, for lead,compare a person leads smb somewhere (PROCESS) vs.a road (PATH) leads somewhere (STATE); for explain,compare something or somebody explains smth (= clar-ifies, describes, makes comprehensible, PROCESS) vs.something [?inanimate, +abstract] explains something(= is a reason for something, STATE); for fall, comparePHYSOBJ falls (TRANSITION or ACCOMPLISHMENT)vs. a case falls into a certain category (STATE).2.
The aspectual nature of the predicate is the onlysemantically relevant feature that remains unchangedafter consecutive sense modifications.
For example, theingressive meaning of ?beginning something?
is pre-served in shifting from the physical sense of the verblaunch in launch a missile to launch a campaign andlaunch a product.374.3 Lexical semantic featuresSense distinctions often involve deeper semantic char-acteristics of the verbs which could be accounted for bymeans of lexical semantic features such as qualia struc-ture roles in Generative Lexicon (Pustejovsky, 1995):51.
Consider how the meaning component ?mannerof motion?
(typically associated with the agentive role)gets transformed in the different senses of drive.
It isobviously present in the physical uses of drive (suchas operate a vehicle, transport something or somebody,etc.
), but is completely lost in motivate the progressof (as in drive the economy, drive the market forward,etc.).
The value of the agentive role of drive becomesunderspecified or semantically weak, so that the overallmeaning of drive is transformed to cause something tomove.2.
Information about semantic type contained inqualia structure allows apparently diverse elements toactivate the same sense of the verb.
For instance, theverb absorb in the sense learn or incorporate skill orinformation occurs with direct objects such as values,atmosphere, information, idea, words, lesson, attitudes,culture.
The requisite semantic component is realizeddifferently for each of these words.
Some of them arecomplex types6 with INFORMATION as one of the con-stituent types: words (ACOUSTIC/VISUAL ENTITY ?INFO), lesson (EVENT ?
INFO).
Others, such as idea,are polysemous, with one of the senses being INFOR-MATION.
Cases like culture and values are more diffi-cult, but since they refer to knowledge, the INFORMA-TION component is clearly present.
Consequently, theannotators are able to identify the corresponding senseof absorb with a high degree of agreement.4.4 Metaphor and metonymyThe processes causing the mentioned meaning trans-formations in our corpus often involve metaphor andmetonymy.
Below are some of the conventionalized ex-tensions with metaphorical flavor:(11) a. grasp object vs. grasp meaningb.
launch object vs. launch an event (campaign, as-sault) or launch a product (newspaper, collection)c. meet with a person vs. meet with success, resistanced.
lead somebody somewhere vs. lead to a consequenceNote that these metaphorical extensions involve ab-stract or continuous objects (meaning, assault, success,consequence), which in turn cause event structure mod-ifications (lead as a process vs. lead as a state).
Thus,the processes and structures we are dealing with areclearly interrelated.The metonymical process can be exemplified by editas make changes to the text and as supervise publica-5We will use the terminology from Generative Lexicon(Pustejovsky, 1995; Pustejovsky, 2007) to discuss lexical se-mantic properties, such as qualia roles, complex and func-tional types, and so on.6Complex type is a term used for concepts that inherentlyrefer to more than one semantic type.tion, which are in a clear contiguity relationship.One of the effects of the metaphorization and pro-gressive emptying of the primary (physical, concrete)senses is the distinction between generic and specificsenses.
For example, compare acquire land, business(specific sense) to acquire an infection, a boyfriend, afollowing, which refers to some extremely light genericassociation.
Similar process is observed for the seman-tically weak sense of fall, be associated with or get as-signed to a person or location or for event to fall onto atime:(12) Birthdays, lunches, celebrations fall on a certain date ortimeStress or emphasis fall on a given topic or a syllableResponsibility, luck, suspicion fall on or to a personThe specificity often involves specialization within acertain domain:(13) a. conclude as finish vs. conclude as reach an agree-ment (Law, Politics)b. fire as shoot a weapon or a projectile vs. fire as kickor pass an object of play in sports (Sport)Thus, when concluding a pact or an agreement, a cer-tain EVENT is also being finished (negotiation of thatagreement), necessarily with a positive outcome.In the following section, we will try to show how dif-ferent kinds of relations between senses influence dis-ambiguation carried out by the annotators.
In particular,we look at different sources of disagreement and anno-tator error as determined in adjudication.5 Analysis of Annotation DecisionsAs we have seen above, in many cases disambigua-tion is impossible due to the nature of compositional-ity.
Also, as there are no clear answers to a number ofquestions concerning sense identification, the annota-tors deal with sense inventories that are imperfect.
Re-sults of the disambiguation task carried out by the an-notators reflect all these defects.In cases when a specific meaning from the data setis not included into the sense inventory (e.g.
due to itslow frequency or extreme fine-grainedness) the annota-tors may use a more general meaning or pick the clos-est meaning available.
For example, within the senseinventory for fire, there was no separate gloss for fire anengine.
Annotator A in our experiment chose the clos-est specific meaning available, and Annotator B markedit with a more generic sense:(14) Engineers successfully fired thrusters to boost the re-search satellite to an altitude of 507 km.annoA: shoot, propel a projectileannoB: apply fire toAs mentioned in Section 3.3, even when the appropriatespecific sense is available, annotators frequently chosethe more generic sense in its place, as in (15), (16) and(17), and also in (8).38(15) Several referrals fell into this category.annoA: be associated with or get assigned to a personor location or for event to fall onto a timeannoB: be categorized as or fall into a range(16) The terrible silence had fallen.annoA: be associated with or get assigned to a personor location or for event to fall onto a timeannoB: for a state (such as darkness or silence) to come,to commence(17) He acquired a taste for performing in public.annoA: become associated with something, oftennewly brought into beingannoB: become associated with something, ...correct: learnNote that in (8) this decision was probably motivated bythe annotators?
uncertainty about the semantic ascrip-tion of the relevant argument (coastline is not a proto-typical owned property).
The generic sense seems to bethe safest option to take for the annotators, as comparedto taking a chance with a specific meaning.
Due to itslow degree of semantic specification, the generic senseis potentially able to embrace almost every possible use.This is not a desirable outcome because the genericsenses are introduced in the inventory to account onlyfor semantically underspecified cases.
For instance, be-come associated with something, often newly broughtinto being is appropriate for acquire a grandchild, butnot for acquire a taste or acquire a proficiency.Remarkable variation is also observed with respect tonon-literal uses as discussed in Section 4.4.
For exam-ple, in (18) and (19) abstract NPs panic and imbalanceof forces are equated with energy or impact by one an-notator and with substance by the other.
(18) Her panic was absorbed by his warmth.annoA: absorb energy or impactannoB: absorb substance(19) Alternatively, imbalance of forces can be absorbed intothe body.annoA: absorb energy or impactannoB: absorb substanceIn some cases, the literal and the metaphoric sensesare activated simultaneously resulting in ambiguity (cf.Cruse (2000)):(20) For over 300 years this waterfall has provided the en-ergy to drive the wheels of industry.annoA: motivate the progress ofannoB: provide power for or physically move a mech-anism(21) But fashion changed and the short skirt fell ?
literally ?from favour and started skimming the ankles.annoA: lose power or suffer a defeatannoB: N/A(22) She was delighted when the story of Hank fell into herlap.annoA: be associated with or get assigned to a personor location or for event to fall onto a timeannoB: physically drop; move or extend downwardImpact of subcategorization features on disam-biguation (cf.
Section 4.1 para 3) is illustrated in (23).
(23) The reggae tourist can easily absorb the current reggaevibe.annoA: absorb energy or impactannoB: learn or incorporate skill or informationBoth interpretations chosen here (absorb energy or im-pact and learn or incorporate skill or information) werepossible due to the animacy of the subject, which acti-vates two different subcategorization frames and subse-quently two different senses.Typically, cases where semantic type of the relevantarguments (cf.
Section 4.3 para 2) is not clear result inannotator disagreement:(24) The AAA launched education programs.annoA: begin or initiate an endeavor (EVENT)annoB: begin to produce or distribute; start a company(PRODUCT)(25) France plans to launch a remote-sensing vehicle calledSpot.annoA: physically propel into the air, water or space(PHYSOBJ)annoB: begin to produce or distribute; start a company(PRODUCT)The two cases above are interesting in that both pro-gram and vehicle are ambiguous and can be analyzedsemantically as members of different semantic classes.This is what the annotators in fact do, and as a result,ascribe them to different senses.
Program can be cate-gorized as EVENT (?series of steps?)
or as INTELLEC-TUAL ACTIVITY PRODUCT (?document or system ofprojects?).
It is a complex type, i.e.
it is an inherentlypolysemous word that represents at least two differentsemantic types.
Vehicle, in turn, is a functional type:on the one hand, it represents an entity with certain for-mal properties (PHYSOBJ interpretation), on the otherhand, it is an artifact, with a prominent practical pur-pose (PRODUCT interpretation).In fact, most problems the annotators had with thetask are due to the inherent semantic complexity ofwords such as vehicle and program in (24) and (25) andto the existence of boundary cases, where the relevantnoun does not properly belong to one or another seman-tic category.
This is the case with panic, imbalance orreggae vibe in (18), (19), and (23), and also with tasteand coastline in (17) and (7).In some of these cases, other contextual clues maycome into play and tip the balance in favor of one or an-other sense.
Note that disambiguation was influencedby a wider context even despite the intentionally re-strictive task design (targeting a particular syntactic re-lation for each verb).
For instance, in (26), domain-specific clues referring to war or military conflict (suchas rebel control) could have motivated Annotator B?sdecision to ascribe it to the sense lose power or suffera defeat (even though a road is not typically an entitythat can lose power), while the other annotator chose amore generic meaning:39(26) The road fell into rebel control.annoA: be associated with or get assigned to a personor location or for event to fall onto a timeannoB: lose power or suffer a defeatOther pragmatic and discourse-oriented clues playeda role, in particular, positive and negative connotationof the senses and the relevant arguments, as well asthe temporal organization of discourse.
For example, in(27) and (28), positive or neutral interpretation of waveof immigrants and change could have led to the choiceof take in or assimilate and learn or incorporate skill orinformation senses, while the negatively-colored inter-pretation might explain the choice of the bear the costof sense.
(27) ..help absorb the latest wave of immigrants.annoA: bear the cost of; take on an expenseannoB: take in or assimilate, making part of a whole ora group(28) For senior management an important lesson was thetrade unions?
capacity to absorb change and to becomeits agents.annoA: learn or incorporate skill or informationannoB: bear the cost of; take on an expenseTemporal organization of a broader discourse is an-other important factor.
For example, for the verb claim,the senses claim the truth of and claim property you areentitled to have different presuppositions with respectto preexistence of the thing claimed.
In (28), due to theabsence of a broader context, the annotators chose twodifferent temporal reference interpretations.
For Anno-tator B, success was something that has happened al-ready, while for A this was not clear (success mighthave been achieved or not):(29) One area where the government can claim some successinvolves debt repayment.annoA: come in possession of or claim property you areentitled toannoB: claim the truth of6 ConclusionWe have given a brief overview of different types ofsense relations commonly found in polysemous predi-cates and analyzed their effect on different aspects ofthe annotation task, including sense inventory designand execution of the WSD annotation.The present analysis suggests that theoretical toolsmust be refined and further developed in order to givean adequate account to the sense modifications found inreal corpus data.
To this end, broader contextual cluesand discourse-oriented clues need to be included in theanalysis.Semantically annotated corpora are routinely devel-oped for the training and testing of automatic sensedetection and induction algorithms.
But they do nottypically provide a way to distinguish between differ-ent kinds of ambiguities.
Consequently, it is difficultto perform adequate error analysis for different sensedetection systems.
Appropriate semantic annotationthat would allow one to determine which sense dis-tinctions can be detected better by automatic systemsdoes not need to be highly specific and unnecessarilycomplex, but requires development of robust general-izations about sense relations.One obvious conclusion is that data sets need to beexplicitly restricted to the instances where humans haveno trouble disambiguating between different senses.Thus, prototypical cases can be accounted for reliably,ensuring the clarity of annotated sense distinctions.
Atface value, imposing such restrictions may appear tonegatively influence usability of the resulting data setin particular applications requiring WSD, such as ma-chine translation or information retrieval.
However, thisdecision impacts most strongly those boundary caseswhich are not reliably disambiguated by human anno-tators, and which rather introduce noise into the dataset.AcknowledgmentsThis work was supported in part by NSF CRI grant toBrandeis University.
The work of O. Batiukova is sup-ported by postdoctoral grant of the Ministry of Educa-tion of Spain and Madrid Autonomous University.ReferencesApresjan, Ju.
1973.
Regular polysemy.
Linguistics,142(5):5?32.Briscoe, T. and J. Carroll.
2002.
Robust accurate sta-tistical annotation of general text.
Proceedings ofthe Third International Conference on Language Re-sources and Evaluation (LREC 2002), Las Palmas,Canary Islands, May 2002, pages 1499?1504.Carpuat, M. and D. Wu.
2007.
Improving statisticalmachine translation using word sense disambigua-tion.
In Proc.
of EMNLP-CoNLL, pages 61?72.Chan, Y. S., H. T. Ng, and D. Chiang.
2007.
Wordsense disambiguation improves statistical machinetranslation.
In Proc.
of ACL, pages 33?40, Prague,Czech Republic, June.Cruse, D. A.
1995.
Polysemy and related phenom-ena from a cognitive linguistic viewpoint.
In Dizier,Patrick St. and Evelyne Viegas, editors, Computa-tional Lexical Semantics, pages 33?49.
CambridgeUniversity Press, Cambridge, England.Cruse, D. A.
2000.
Meaning in Language, an Intro-duction to Semantics and Pragmatics.
Oxford Uni-versity Press, Oxford, United Kingdom.Hanks, P. and J. Pustejovsky.
2005.
A patterndictionary for natural language processing.
RevueFranc?aise de Linguistique Applique?e.Hiroaki, S. 2003.
FrameSQL: A software tool forFrameNet.
In Proceedigns of ASIALEX ?03, pages251?258, Tokyo, Japan.
Asian Association of Lexi-cography.40Hovy, E., M. Marcus, M. Palmer, L. Ramshaw, andR.
Weischedel.
2006.
OntoNotes: The 90% solu-tion.
In Proceedings of the Human Language Tech-nology Conference of the NAACL, Companion Vol-ume: Short Papers, pages 57?60, New York City,USA, June.
Association for Computational Linguis-tics.Kilgarriff, A., P. Rychly, P. Smrz, and D. Tugwell.2004.
The Sketch Engine.
Proceedings of Euralex,Lorient, France, pages 105?116.Kilgarriff, A.
1997.
I don?t believe in word senses.Computers and the Humanities, 31:91?113.Landes, S., C. Leacock, and R.I. Tengi.
1998.
Build-ing semantic concordances.
In Fellbaum, C., editor,Wordnet: an electronic lexical database.
MIT Press,Cambridge (Mass.
).Mihalcea, R., T. Chklovski, and A. Kilgarriff.
2004.The Senseval-3 English lexical sample task.
In Mi-halcea, Rada and Phil Edmonds, editors, Senseval-3:Third International Workshop on the Evaluation ofSystems for the Semantic Analysis of Text, pages 25?28, Barcelona, Spain, July.
Association for Compu-tational Linguistics.Navigli, R. 2006.
Meaningful clustering of senseshelps boost word sense disambiguation performance.In Proceedings of the 21st International Conferenceon Computational Linguistics and 44th Annual Meet-ing of the Association for Computational Linguistics,pages 105?112, Sydney, Australia, July.
Associationfor Computational Linguistics.Palmer, M., D. Gildea, and P. Kingsbury.
2005.
TheProposition Bank: An annotated corpus of semanticroles.
Computational Linguistics, 31(1):71?106.Palmer, M., H. Dang, and C. Fellbaum.
2007.
Makingfine-grained and coarse-grained sense distinctions,both manually and automatically.
Journal of Natu-ral Language Engineering.Preiss, J and D. Yarowsky, editors.
2001.
Proceedingsof the Second Int.
Workshop on Evaluating WSD Sys-tems (Senseval 2).
ACL2002/EACL2001.Pustejovsky, J. and B. Boguraev.
1993.
Lexical knowl-edge representation and natural language processing.Artif.
Intell., 63(1-2):193?223.Pustejovsky, J., P. Hanks, and A. Rumshisky.
2004.Automated Induction of Sense in Context.
In COL-ING 2004, Geneva, Switzerland, pages 924?931.Pustejovsky, J.
1995.
Generative Lexicon.
Cambridge(Mass.
): MIT Press.Pustejovsky, J.
2007.
Type Theory and Lexical De-composition.
In Bouillon, P. and C. Lee, editors,Trends in Generative Lexicon Theory.
Kluwer Pub-lishers (in press).Resnik, P. 2006.
Word sense disambiguation in NLPapplications.
In Agirre, E. and P. Edmonds, editors,Word Sense Disambiguation: Algorithms and Appli-cations.
Springer.Rumshisky, A. and J. Pustejovsky.
2006.
Induc-ing sense-discriminating context patterns from sense-tagged corpora.
In LREC 2006, Genoa, Italy.Rumshisky, A., P. Hanks, C. Havasi, and J. Pustejovsky.2006.
Constructing a corpus-based ontology usingmodel bias.
In The 19th International FLAIRS Con-ference, FLAIRS 2006, Melbourne Beach, Florida,USA.Rumshisky, A.
2008.
Resolving polysemy in verbs:Contextualized distributional approach to argumentsemantics.
Distributional Models of the Lexicon inLinguistics and Cognitive Science, special issue ofItalian Journal of Linguistics / Rivista di Linguistica.forthcoming.Ruppenhofer, J., M. Ellsworth, M. Petruck, C. Johnson,and J. Scheffczyk.
2006.
FrameNet II: ExtendedTheory and Practice.Sinclair, J. and P. Hanks.
1987.
The Collins CobuildEnglish Language Dictionary.
HarperCollins, 4thedition (2003) edition.
Published as Collins CobuildAdvanced Learner?s English Dictionary.Snyder, B. and M. Palmer.
2004.
The english all-wordstask.
In Mihalcea, Rada and Phil Edmonds, edi-tors, Senseval-3: Third International Workshop onthe Evaluation of Systems for the Semantic Analysisof Text, pages 41?43, Barcelona, Spain, July.
Associ-ation for Computational Linguistics.41
