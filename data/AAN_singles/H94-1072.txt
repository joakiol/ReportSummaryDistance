DOCUMENT REPRESENTATION INNATURAL LANGUAGE TEXT RETRIEVALTomek StrzalkowskiCourant Institute of Mathematical SciencesABSTRACTIn information retrieval, the content of a document may berepresented as a collection of terms: words, stems, phrases, orother units derived or inferred from the text of the document.These terms are usually weighted to indicate their importancewithin the document which can then be viewed as a vector in a N-dimensional space.
In this paper we demonstrate that a proper termweighting is at least as important as their selection, and that dif-ferent types of terms (e.g., words, phrases, names), and termsderived by different means (e.g., statistical, linguistic) must betreated ifferently for a maximum benefit in rel~ieval.
We reportsome observations made during and after the second TextREtrieval Conference (TREC-2).
11.
INTRODUCTIONThe task of information retrieval is to extract relevant documentsfrom a large collection of documents in response to user queries.When the documents contain primarily unrestricted text (e.g.,newspaper articles, legal documents, etc.)
the relevance of a docu-ment is established through 'full-text' retrieval.
This has been usu-ally accomplished by identifying key terms in the documents (theprocess known as 'indexing') which could then be matched againstterms in queries \[2\].
The effectiveness of any such term-basedapproach is directly related to the accuracy with which a set ofterms represents he content of a document, as well as how well itcontrasts a given document with respect o other documents.
Inother words, we are looking for a representation R such that forany text items D1 and D2, R(D1) = R(D2) iff meaning(D1) =meaning(D2), at an appropriate level of abstraction (which maydepend on the types and character of anticipated queries).The simplest word-based representations of content are usuallyinadequate since single words are rarely specific enough for accu-rate discrimination, and their grouping is often accidental.
A bettermethod is to identify groups of words that create meaningfulphrases, especially if these phrases denote important concepts inthe database domain.
For example, joint venture is an importantterm in the Wall Street Journal (WSJ henceforth) database, whileneither joint nor venture are important by themselves.
In fact, in a800+ MBytes database, both joint and venture would often bedropped from the list of terms by the system because their inverteddocument frequency (idJ) weights were too low.
In large databasesl See \[1\] for a detailed introduction to TREC.New York University715 Broadway, rm.
704New York, NY 10003tomek@cs.nyu.educomprising hundreds of thousands of documents he use of phrasalterms is not just desirable, it becomes necessary.An accurate syntactic analysis is an essential prerequisite for selec-tion of phrasal terms.
Various statistical methods, e.g., based onword co-occurrences and mutual information, as well as partialparsing techniques, are prone to high error rates (sometimes ashigh as 50%), turning out many unwanted associations.
Thereforea good, fast parser is necessary, but it is by no means sufficient.While syntactic phrases are often better indicators of content han'statistical phrases' - -  where words are grouped solely on thebasis of physical proximity, e.g., "college junior" is not the sameas "junior college" - -  the creation of compound terms makes theterm matching process more complex since in addition to the usualproblems of synonymy and subsumption, one must deal with theirstructure (e.g., "college junior" is the same as "junior in college").For all kinds of terms that can be assigned to the representation fa document, e.g., words, syntactic phrases, fixed phrases, andproper names, various levels of "regularization" are needed toassure that syntactic or lexical variations of input do not obscureunderlying semantic uniformity.
Without actually doing semanticanalysis, this kind of normalization can be achieved through thefollowing processes: 2(1) morphological stemming: e.g., retrieving is reduced toretriev;(2) lexicon-based word normalization: e.g., retrieval isreduced to retrieve;(3) operator-argument representation f phrases: e.g., informa-tion retrieval, retrieving of information, and retrieverelevant information are all assigned the same representa-tion, retrieve+information;(4) context-based term clustering into synonymy classes andsubsumption hierarchies: e.g., takeover is a kind ofacquisition (in business), and Fortran is a programminglanguage.In traditional full-text indexing, terms are selected from amongwords and stems and weighted according to their frequencies anddistribution among documents.
The introduction of terms whichare derived primarily by linguistic means into the representation fdocuments changes the balance of frequency-based weighting andtherefore calls for more complex term weighting schemes than2 An alternative, but less efficient method is to generate all variants(lexical, syntactic, etc.)
of words/phrases in the queries \[31.364those devised and tested on single-word representations.
The stan-dard ff.idf scheme (term frequency times inverted ocument fre-quency), for example, weights terms proportionately to their globalscores (idf) and their in-document frequencies (tO, usually normal-ized by document length.
It is appropriate when most uses a termare explicit, that is, appropriate words actually occur in text.
This,however, is frequently not the case with proper names or phrasesas various anaphorrs can be used to create implicit termoccurrences.2.
OVERALL DESIGNWe have established the general architecture of a NLP-IR system,depicted schematically below, in which an advanced NLP moduleis inserted between the textual input (new documents, user queries)and the database search engine (in our case, NIST's PRISE sys-tem\[4\]).
This design has already shown some promise in produc-ing a better performance than the base statistical system \[5,6,7\]..text NLP repres, databasNLP: ~ \ ] ~In our system the database t xt is first processed with a sequenceof programs that include a part-of-speech tagger, a lexicon-basedmorphological stemmer and a fast syntactic parser (TTP).
3 Subse-quently certain types of phrases are extracted from the parse treesand used as compound indexing terms in addition to single-wordterms.
The extracted phrases are statistically analyzed as syntacticcontexts in order to discover a variety of similarity links betweensmaller subphrases and words occurring in them.
A further filter-ing process maps these similarity links onto semantic relations(generalization, specialization, synonymy, etc.)
after which theyare used to transform auser's request into a search query.The user's natural language request is also parsed, and all indexingterms occurring in it are identified.
Certain highly ambiguous, usu-ally single-word terms may be dropped, provided that they alsooccur as elements in some compound terms.
For example,"natural" may be deleted from a query already containing "naturallanguage" because "natural" occurs in many unrelated contexts:"natural number", "natural logarithm", "natural approach", etc.
Atthe same time, other terms may be added, namely those which arelinked to some query term through admissible similarity relations.For example, "unlawful activity" is added to a query (TREC topic055) containing the compound term "illegal activity" via asynonymy link between "illegal" and "unlawful".One of the observations made during the course of TREC-2 was tonote that removing low-quality terms from the queries is at least asimportant (and often more so) as adding synonyms and specializa-tions.
In some instances (e.g., routing runs) low-quality terms hadto be removed (or inhibited) before similar terms could be added tothe query or else the effect of query expansion was all but drownedout by the increased noise.3 For a description f TTP parser, efer to \[8,9\].After the final query is constructed, the database search follows,and a ranked list of documents i  returned.
It should be noted thatall the processing steps, those performed by the backbone system,and those performed by the natural anguage processing com-ponents, are fully automated, and no human intervention rmanualencoding isrequired.3.
SELECTING PHRASAL TERMSSyntactic phrases extracted from the parse structures arerepresented as head-modifier pairs.
The head in such a pair is acentral element of a phrase (main verb, main noun, etc.
), while themodifier is one of the adjuncts or arguments of the head.
In theTREC experiments reported here we extracted head-modifier wordpairs only, i.e., nested pairs were not used even though this waswarranted by the size of the database.
4Figure 1 shows all stages of the initial inguistic analysis of a sam-ple sentence from the WSJ database.
The reader may note that theparser's output is a predicate-argument structure centered aroundthe main elements of various phrases.
For example, BE is the mainpredicate (modified by HAVE) with 2 arguments ( ubject, object)and 2 adjuncts (adv, sub ord).
INVADE is the predicate in thesubordinate clause with 2 arguments ( ubject, object).
The subjectof BE is a noun phrase with PRESIDENT as the head element, womodifiers (FORMER, SOVIET) and a determiner (THE).
Fromthis structure, we extract head-modifier pairs that become candi-dates for compound terms.
In general, the following types of pairsare considered: (1) a head noun of a noun phrase and its left adjec-tive or noun adjunct, (2) a head noun and the head of its rightadjunct, (3) the main verb of a clause and the head of its objectphrase, and (4) the head of the subject phrase and the main verb.These types of pairs account for most of the syntactic variants forrelating two words (or simple phrases) into pairs carrying compati-ble semantic ontent.
For example, the pair retrieve+informationwill be extracted from any of the following fragments: informationretrieval system; retrieval of information from databases; andinformation that can be retrieved by a user-controlled interactivesearch process.
5 We also attempted to identify and remove anyterms which were explicitly negated in order to prevent matchesagainst their positive counterparts, either in the database or in thequeries.One difficulty in obtaining head-modifier pairs of highest accuracyis the notorious ambiguity of nominal compounds.
The pMr extrac-tor looks at the distribution statistics of the compound terms todecide whether the association between any two words (nouns andadjectives) in a noun phrase is both syntactically valid and semant-ically significant.
For example, we may accept language+naturaland processing+language from natural language processing ascorrect, however, case+trading would make a mediocre termwhen extracted from insider trading case.
On the other hand, it isimportant o extract trading+insider to be able to match4 Even with 2-word phrases, compound terms accounted for nearly88% of all index entries, in other words, including 2-word phrases in-creased the index size approximately 8 times.s Longer phrases or nested pairs may be more appropriate in somecases, e.g., when former Soviet president is broken into former presidentand Soviet president, we get something potentially quite different fromwhat he original phrase refers to, and this may have a negative effect on re-trieval precision.365INPUT SENTENCEThe former Soviet president has been a local hero ever since a Russian tankinvaded Wisconsin.TAGG ED SENTENCEThe/dt former/jj Sovieffjj president/nn has/vbz beenlvbn aldt loealljjhero/nn ever/rb since~in aldt Russian/jj tank/nn invadedlvbd Wisconsin/ripTAGGED & STEMMED SENTENCEthe~dr former/jj soviet/jj president/nn have/vbz be/vbn a/dt localljj hero/nnever/rb since~in aldt russian/jj tanklnn invade/vbd wisconsin/np ./perPARSED SENTENCE\[assertlIpeff \[HAVEII \[Iverb \[BEll\[subject\[rip \[n PRESIDENT\] \[t_pos THEIIadj \[FORMER\]I \[adJ \[SOVlETIlII\[object\[np In HEROI It..pos AI \[adj \[LOCAL\]I\]I\[adv EVER\]\[sub_oral \[SINCEI\[verb \[INVADEll\[subject \[np In TANK\] It_pos A\] \[adj \[RUSSIAN\]Ill\[object \[np \[name \[WISCONSIN\]IlllIIlllEXTRACTED TERMS & WEIGHTSpresident 2.623519 soviet 5.416102president+former 14.594883 hero 7.896426invade 8.435012 tank 6.848128tank+russian 16 .030809 mssian 7.383342president+soviet 11.556747 hero+local 14.314775tank+invade 17.402237 wisconsin 7.785689Figure 1.
Stages of sentence processing.documents containing phrases insider trading sanctions act orinsider trading activity.
In addition, phrases with a significantnumber of occurrences across different documents, including thosefor which no clear disambiguation i to paks can be obtained, areincluded as a third level of index (beside single-word terms, andpairs).
64.
TERM WEIGHTING ISSUESFinding a proper term weighting scheme is critical in term-basedretrieval since the rank of a document is determined by the weightsof the terms it shares with the query.
One popular term weightingscheme, known as tf.idf, weights terms proportionately to theirinverted ocument frequency scores and to their in-document fre-quencies (tf).
The in-document frequency factor is usually nor-realized by the document length, that is, it is more significant for aterm to occur in a short lO0-word abstract, than in a 5000-wordarticle.
76 Longer phrases were not used in TREC-2.7 This is not always tree, for example when all occurrences ofa termare concentrated in a single section or a paragraph rather than spreadaround the article.
See the following section for more discussion.In our official TREC runs we used the normalized tf.idf weightsfor all terms alike: single 'ordinary-word' terms, proper names, aswell as phrasal terms consisting of 2 or more words.
8 Wheneverphrases were included in the term set of a document, he length ofthis document was increased accordingly.
This had the effect ofdecreasing tf factors for 'regular' single word terms.A standard tf.idf weighting scheme may be inappropriate formixed term sets, consisting of ordinary concepts, proper names,and phrases, because:(1) It favors terms that occur fairly frequently in a document,which supports only general-type queries (e.g., "all youknow about 'star wars'").
Such queries were not typical inTREC.
(2) It attaches low weights to infrequent, highly specific terms,such as names and phrases, whose only occurrences in adocument are often decisive for relevance.
Note that suchterms cannot be reliably distinguished using their distribu-tion in the database as the sole factor, and therefore syntac-tic and lexical information is required.
(3) It does not address the problem of inter-term dependenciesarising when phrasal terms and their component single-word terms are all included in a document representation,i.e., launch+satellite and satellite are not independent, andit is unclear whether they should be counted as two terms.In our post-TREC-2 experiments we considered (1) and (2) only.We noted that linguistic phrases, that is, phrases derived from textthrough primarily linguistic means, display a markedly differentstatistical behaviour than 'statistical phrases', i.e., those obtainedusing frequency-based or probabilistic formulas such as MutualInformation \[ i i \ ] .
For example, while statistical phrases with fewoccurrences in the corpus could be dismissed as insignificant or'noise', infrequent linguistic phrases may in fact turn out to bequite important if only we could count all their implicitoccurrences, e.g., as anaphors.Rather than trying to resolve anaphoric references, we changed theweighting scheme so that the phrases (but not the names, which wedid not distinguish in TREC-2) were more heavily weighted bytheir idf scores while the in-document frequency scores werereplaced by logarithms multiplied by sufficiently large constants.In addition, the top N highest-idf matching terms (simple or com-pound) were counted more toward the document score than theremaining terms.Schematically, these new weights for phrasal and highly specificterms are obtained using the following formula, while weights formost of the single-word terms remain unchanged:weight (Ti)=( C 1 *log (tf )+C 2" ~(N, i) )*idfIn the above, ~(N,i) is 1 for i <N and is 0 otherwise.
9Table 1 illustrates the effect of differential weighting of phrasalterms using topic 101 and a relevant document (WSJ870226-0091)s Specifically, the system used Inc-ntc combination f weights whichis already one of the most effective options of ff.idf; see \[ 10\] for details.9 The selection of a weighting formula was partly constrained by thefact that document-length-normalized ff weights were precomputed attheindexing stage and could not be altered without re-indexing of the entiredatabase.
The intuitive interpretation f the 0~(N,i) facctoris given in the fol-lowing section.366as an example.
Note that while most of the affected terms havetheir weights increased, sometimes ubstantially, for some (e.g.,space+base) the weight actually decreases.
Table 2 shows howranks of the relevant documents change when phrasal terms areused with the new weighting scheme.
Changing the weightingscheme for compound terms has led to an overall increase of preci-sion of more than 20% over our official TREC-2 ad-hoc results.Table 3 summarizes tatistics of the runs for queries 101-150against he WSJ database, both with new weighting scheme andwith the standard tf.idf weighting.5.
'HOT SPOT' RETRIEVALAnother difficulty with frequency-based term weighting ariseswhen a long document needs to be retrieved on the basis of a fewTopic 101 matches WSJ870226-0091duplicate terms not shownTERM TF.IDF NEW WEIGHTsdi 1750 1750efis 3175 3175star 1072 1072wars 1670 1670laser 1456 1456weapon 1639 1639missile 872 872space+base 2641 2105interceptor 2075 2075exoatmospheric 1879 3480system+defense 2846 2219reentry+vehicle 1879 3480initiative+defense 1646 2032system+interceptor 2526 3118DOC RANK 30 10Table 1.
The effect of differential term weighting.DOC ID OLD RANK NEW RANKWSJ891004-0119WSJ891005-0005WSJ890918-0173WSJ880608-0121WSJ870723-0064WSJ870213-0053WSJ891009-0009WSJ890920-0115WSJ891009-0188WSJ880609-0061WSJ870601-0075WSJ890928-0184WSJ891005-0001WSJ871028-0059WSJ880705-01947152148103539735312840283183971457812182646505261729395Table 2.
Rank changes for relevant documents for Topic 104 when phrasalterms are used in retrieval.short relevant passages.
If the bulk of the document is not directlyrelevant o the query, then there is a strong possibility that thedocument will score low in the final ranking, despite some stronglyrelevant material in it.
This problem can be dealt with by subdi-viding long documents at paragraph breaks, or into approximatelyequal length fragments and indexing the database with respect othese (e.g., \[12\]).
While such approaches are effective, they alsotend to be costly because of increased index size and more compli-cated access methods.Efficiency considerations have led us to investigate an alternativeapproach to the hot spot retrieval which would not require re-indexing of the existing database or any changes in documentaccess.
In our approach, the maximum number of terms on which aquery is permitted to match a document is limited to N highestweight erms, where N can be the same for all queries or may varyfrom one query to another.
Note that this is not the same as simplytaking the N top terms from each query.
Rather, for each documentfor which there are M matching terms with the query, onlymin(M,N) of them, namely those which have highest weights, willbe considered when computing the document score.
Moreover,only the global importance weights for terms are considered (suchas idf), while local in-document frequency (eg., t o is suppressedby either taking a log or replacing it with a constant.
The effect ofthis 'hot spot' retrieval is shown in Table 4 in the ranking ofrelevant documents within the top 30 retrieved ocuments for topic72.The final ranking is obtained by adding the scores of documents in'regular' tf.idf ranking and in the hot-spot ranking..
While someof the recall may be sacrificed ('hot spot' retrieval has often lowerrecall than full query retrieval, and this becomes the lower boundon recall for the combined ranking) the combined ranking preci-sion has been consistently better than in either of the original rank-ings: an average improvement is 10-12% above the tf.idf run preci-sion (which is often the stronger of the two).
The 'hot spot'weighting is represented with the (x factor in the term weightingformula given in the previous ection.6.
CONCLUSIONSWe presented some detail of our natural anguage informationretrieval system consisting of an advanced NLP module and a'pure' statistical core engine.
While many problems remain to beresolved, including the question of adequacy of term-basedrepresentation f document content, we attempted to demonstratethat the architecture described here is nonetheless viable.
Wedemonstrated that natural language processing can now be done ona fairly large scale and that its speed and robustness can matchthose of traditional statistical programs uch as key-word indexingor statistical phrase extraction.
We suggest moreover that whenproperly used natural language processing can be very effective inimproving retrieval precision.
In particular, we show that in term-based document representation, term weighting is at least asimportant as their selection.
In order to achieve optimal perfor-mance terms obtained primarily through the linguistic analysismust be weighted differently than those obtained through tradi-tional frequency-based methods.On the other hand, we must be aware of the limits of NLP techno-logies at our disposal.
While part-of-speech tagging, lexicon-basedstemming, and parsing can be done on large amounts of text (hun-dreds of millions of words and more), other, more advanced367R.
I - ,  I n ir2 I I con +, pRetRelRelRet%chgTot number of does over all queries50000 49876 49999 500003929 3929 3929 39293129 3274 3332 3401+4.6 +6.4 1+8.7Recall0.00 0.70640.10 0.53160.20 10.4533i0.30 ~ 0.37670.40 0.33290.50 0.28400.60 0.23980.70 0.19460.80 0.14600.90 0.08081.00 0.0125(inteqa) Precision Averages0.75280.55670.47210.40600.36170.31350.27030.22310.16670.09150.01540.7469 0.80630.5726 0.61980.4970 0.55660.4193 0.47860.3747 0.42570.3271 !
0.38280.2783 0.33800.2267 0.28170.1670 0.21640.0959 0.14710.0168 0.0474Average precision over all tel docsAvg 0.2881 0.3111 0.3210 0.3759%chg +8.0 +I 1.4 +30.5Precision at5 does10 does15 does20 does30 does100 does200 does500 does1000 does0.50800.46800.44400.43100.3887/0.28400.20090.10750.06260.53600.48800.46930.43900.40670.30940.21390.11370.06550.56000.50200.47730.45600.41000.30840.21560.11620.06660.60400.55800.52530.49800.46070.33460.23250.12290.0680R-Precision (after Rel)Exact 0.3076 0.3320 0.3455 0.3950%chg +8.0 +12.3 +28.4Table 3.
Run statistics for ad-hoc queries 101-150 against WSJ databasewith 1000 does per query: (1) con1 - single-word terms only; (2) nyuir2 -the official TREC-2 run including phrases with standard ff.idf weighting;(3) con2 - single-word terms only with low weight erms removed;,and (4)con2+nlp  - single-word terms and phrases with the new weighting scheme.In all cases documents preprroeessed with the lexicon-based suffix-trimmer.DOCUMENTID \[RANK \[SCOREFul l  ~.
id f  retr ieval - words  and  phrasesWSJ901228-0063 2 15957WSJ910619-0153 3 15843WSJ910322-0041 4 \] 15063WSJ880118-0090 7 13816WSJ910102-0058 11 12803WSJ870324-0083 12 12720WSJ910916-O109 17 11014WSJ910208-OI91 18 10912WSJ871013-0105 19 10745WSJ910419-0071 21 10540WSJ901227-0001 27 9928WSJ900904-0093 28 9685WSJ910215-0054 30 9609Hot-spot  idf -dominated with N=20WSJ910916-0109WSJ910322-0041WSJ920226-0151WSJ901228-0063WSJ901227/-0001WSJ870324-0083WSJ880127-0086!
WSJ910227-0107WSJ901227-0005iWSJ900524-0125WSJ880118-0090WSJ911218-0028WSJ910719-0067124611121314485159616711822118221001699178704870487/04757167546754675467546754Merged rankings - new weightsWSJ910322-0041 1 15975WSJ901228-0063 2 15060WSJ910916-0109 3 13951WSJ910619-0153 4 12745WSJ870324-0083 6 12577WSJ880118-0090 9 11732WSJ920226-0151 11 11518WSJ910102-0058 13 11225WSJ901227-0001 16 11181WSJ880127-0086 18 10871WSJ910227-0107 23 9821WSJ910419-0071 24 9811WSJ871006-0091 37 8768Table 4.
Ranks of the relevant documents in hot-spot retrieval and mergedranking for Topic 72.processing involving conceptual structuring, logical forms, etc., isstill beyond reach, computationally.
It may be assumed that thesesuper-advanced techniques will prove even more effective, sincethey address the problem of representation-level limits; howeverthe experimental evidence is sparse and necessarily limited torather small scale tests (e.g., \[ 13\]).368AcknowledgementsWe would like to thank Donna Harman of NIST for making herPRISE system available to us.
We would also like to thank RalphWeischedel and Heidi Fox of BBN for providing and assisting inthe use of the part of speech tagger.
Thanks to Ralph Grishman forhis comments on an earlier version of this paper.
This paper isbased upon work supported by the Advanced Research ProjectAgency under Contract N00014-90-J-1851 from the Office ofNaval Research, under Contract N00600-88-D-3717 from PRCInc., and the National Science Foundation under Grant IRI-93-02615.
We also acknowledge support from the Canadian Institutefor Robotics and Intelligent Systems (IRIS).References1.
Harman, Donna (ed.).
1993.
First Text REtrieval Conference.NIST special publication 500-207.2.
Salton, Gerard.
1989.
Automatic Text Processing: thetransformation, analysis, and retrieval of information by com-puter.
Addison-Wesley, Reading, MA.3.
Sparck Jones, K. and J. I. Tait.
1984.
"Automatic .searchterm variant generation."
Journal of Documentation, 40(1),pp.
50-66.4.
Harman, Donna and Gerald Candela.
1989.
"RetrievingRecords from a Gigabyte of text on a Minicomputer UsingStatistical Ranking."
Journal of the American Society forInformation Science, 41(8), pp.
581-589.5.
Strzalkowski, Tomek.
1993.
"Natural Language Processingin Large-Scale Text Retrieval Tasks."
Proceedings of theFirst Text REtrieval Conference (TREC-1), NIST SpecialPublication 500-207, pp.
173-187.6.
Strzalkowski, Tomek.
1993.
"Robust Text Processing inAutomated Information Retrieval."
Proc.
of ACL-sponsoredworkshop on Very Large Corpora.
Ohio State Univ.Columbus, June 22.7.
Strzalkowski, Tomek and Barbara Vauthey.
1992.
"Informa-tion Retrieval Using Robust Natural Language Processing."Proc.
of the 30th ACL Meeting, Newark, DE, June-July.
pp.104-111.8.
Strzalkowski, Tomek.
1992.
'"vrP: A Fast and RobusrParserfor Natural Language."
Proceedings of the 14th InternationalConference on Computational Linguistics (COLING),Nantes, France, July 1992. pp.
198-204.9.
Strzalkowski, Tomek, and Peter Scheyen.
1993.
"An Evalua-tion of 'VFP Parser: a preliminary report."
Proceedings ofInternational Workshop on Parsing Technologies (IWPT-93),Tilburg, Netherlands and Durbuy, Belgium, August 10-13.10.
Buckley, Chris.
1993.
"The Importance of Proper WeightingMethods."
Human Language Technology, Proceed'nags ofthe workshop, Princeton, NJ.
Morgan-Kaufmann, pp.
349-352.11.
Lewis, David D. and W. Bruce Croft.
1990.
"Term Cluster-ing of Syntactic Phrases".
Proceedings of ACM SIGIR-90,pp.
385-405.12.
Kwok, K.L., L. Papadopoulos and Kathy Y.Y.
Kwan.
1993.
"Retrieval Experiments with a Large Collection usingPIRCS."
Proceedings of TREC-1 conference, NIST special13.publication 500-207, pp.
153-172.Mauldin, Michael.
1991.
"Retrieval Performance in Ferret:A Conceptual Information Relaieval System" Proceedings ofACM SIGIR-91, pp.
347-355.369
