Proceedings of the 12th Conference of the European Chapter of the ACL, pages 683?691,Athens, Greece, 30 March ?
3 April 2009. c?2009 Association for Computational LinguisticsNatural Language Generation as Planning Under Uncertainty for SpokenDialogue SystemsVerena RieserSchool of InformaticsUniversity of Edinburghvrieser@inf.ed.ac.ukOliver LemonSchool of InformaticsUniversity of Edinburgholemon@inf.ed.ac.ukAbstractWe present and evaluate a new model forNatural Language Generation (NLG) inSpoken Dialogue Systems, based on statis-tical planning, given noisy feedback fromthe current generation context (e.g.
a userand a surface realiser).
We study its use ina standard NLG problem: how to presentinformation (in this case a set of search re-sults) to users, given the complex trade-offs between utterance length, amount ofinformation conveyed, and cognitive load.We set these trade-offs by analysing exist-ing MATCH data.
We then train a NLG pol-icy using Reinforcement Learning (RL),which adapts its behaviour to noisy feed-back from the current generation context.This policy is compared to several base-lines derived from previous work in thisarea.
The learned policy significantly out-performs all the prior approaches.1 IntroductionNatural language allows us to achieve the samecommunicative goal (?what to say?)
using manydifferent expressions (?how to say it?).
In a Spo-ken Dialogue System (SDS), an abstract commu-nicative goal (CG) can be generated in many dif-ferent ways.
For example, the CG to presentdatabase results to the user can be realized as asummary (Polifroni and Walker, 2008; DembergandMoore, 2006), or by comparing items (Walkeret al, 2004), or by picking one item and recom-mending it to the user (Young et al, 2007).Previous work has shown that it is useful toadapt the generated output to certain features ofthe dialogue context, for example user prefer-ences, e.g.
(Walker et al, 2004; Demberg andMoore, 2006), user knowledge, e.g.
(Janarthanamand Lemon, 2008), or predicted TTS quality, e.g.
(Nakatsu and White, 2006).In extending this previous work we treat NLGas a statistical sequential planning problem, anal-ogously to current statistical approaches to Dia-logue Management (DM), e.g.
(Singh et al, 2002;Henderson et al, 2008; Rieser and Lemon, 2008a)and ?conversation as action under uncertainty?
(Paek and Horvitz, 2000).
In NLG we havesimilar trade-offs and unpredictability as in DM,and in some systems the content planning and DMtasks are overlapping.
Clearly, very long systemutterances with many actions in them are to beavoided, because users may become confused orimpatient, but each individual NLG action willconvey some (potentially) useful information tothe user.
There is therefore an optimization prob-lem to be solved.
Moreover, the user judgementsor next (most likely) action after each NLG actionare unpredictable, and the behaviour of the surfacerealizer may also be variable (see Section 6.2).NLG could therefore fruitfully be approachedas a sequential statistical planning task, wherethere are trade-offs and decisions to make, such aswhether to choose another NLG action (and whichone to choose) or to instead stop generating.
Re-inforcement Learning (RL) allows us to optimizesuch trade-offs in the presence of uncertainty, i.e.the chances of achieving a better state, while en-gaging in the risk of choosing another action.In this paper we present and evaluate a newmodel for NLG in Spoken Dialogue Systems asplanning under uncertainty.
In Section 2 we arguefor applying RL to NLG problems and explain theoverall framework.
In Section 3 we discuss chal-lenges for NLG for Information Presentation.
InSection 4 we present results from our analysis ofthe MATCH corpus (Walker et al, 2004).
In Sec-tion 5 we present a detailed example of our pro-posed NLG method.
In Section 6 we report onexperimental results using this framework for ex-ploring Information Presentation policies.
In Sec-tion 7 we conclude and discuss future directions.6832 NLG as planning under uncertaintyWe adopt the general framework of NLG as plan-ning under uncertainty (see (Lemon, 2008) for theinitial version of this approach).
Some aspects ofNLG have been treated as planning, e.g.
(Kollerand Stone, 2007; Koller and Petrick, 2008), butnever before as statistical planning.NLG actions take place in a stochastic environ-ment, for example consisting of a user, a realizer,and a TTS system, where the individual NLG ac-tions have uncertain effects on the environment.For example, presenting differing numbers of at-tributes to the user, and making the user more orless likely to choose an item, as shown by (Rieserand Lemon, 2008b) for multimodal interaction.Most SDS employ fixed template-based gener-ation.
Our goal, however, is to employ a stochas-tic realizer for SDS, see for example (Stent et al,2004).
This will introduce additional noise, whichhigher level NLG decisions will need to reactto.
In our framework, the NLG component mustachieve a high-level Communicative Goal fromthe Dialogue Manager (e.g.
to present a numberof items) through planning a sequence of lower-level generation steps or actions, for example firstto summarize all the items and then to recommendthe highest ranking one.
Each such action has un-predictable effects due to the stochastic realizer.For example the realizer might employ 6 attributeswhen recommending item i4, but it might use only2 (e.g.
price and cuisine for restaurants), depend-ing on its own processing constraints (see e.g.
therealizer used to collect the MATCH project data).Likewise, the user may be likely to choose an itemafter hearing a summary, or they may wish to hearmore.
Generating appropriate language in context(e.g.
attributes presented so far) thus has the fol-lowing important features in general:?
NLG is goal driven behaviour?
NLG must plan a sequence of actions?
each action changes the environment state orcontext?
the effect of each action is uncertain.These facts make it clear that the problem ofplanning how to generate an utterance falls nat-urally into the class of statistical planning prob-lems, rather than rule-based approaches such as(Moore et al, 2004; Walker et al, 2004), or super-vised learning as explored in previous work, suchas classifier learning and re-ranking, e.g.
(Stent etal., 2004; Oh and Rudnicky, 2002).
Supervisedapproaches involve the ranking of a set of com-pleted plans/utterances and as such cannot adaptonline to the context or the user.
ReinforcementLearning (RL) provides a principled, data-drivenoptimisation framework for our type of planningproblem (Sutton and Barto, 1998).3 The Information Presentation ProblemWe will tackle the well-studied problem of Infor-mation Presentation in NLG to show the benefitsof this approach.
The task here is to find the bestway to present a set of search results to a user(e.g.
some restaurants meeting a certain set of con-straints).
This is a task common to much priorwork in NLG, e.g.
(Walker et al, 2004; Dembergand Moore, 2006; Polifroni and Walker, 2008).In this problem, there there are many decisionsavailable for exploration.
For instance, which pre-sentation strategy to apply (NLG strategy selec-tion), how many attributes of each item to present(attribute selection), how to rank the items and at-tributes according to different models of user pref-erences (attribute ordering), how many (specific)items to tell them about (conciseness), how manysentences to use when doing so (syntactic plan-ning), and which words to use (lexical choice) etc.All these parameters (and potentially many more)can be varied, and ideally, jointly optimised basedon user judgements.We had two corpora available to study some ofthe regions of this decision space.
We utilise theMATCH corpus (Walker et al, 2004) to extract anevaluation function (also known as ?reward func-tion?)
for RL.
Furthermore, we utilise the SPaRKycorpus (Stent et al, 2004) to build a high qualitystochastic realizer.
Both corpora contain data from?overhearer?
experiments targeted to InformationPresentation in dialogues in the restaurant domain.While we are ultimately interested in how hearersengaged in dialogues judge different InformationPresentations, results from overhearers are still di-rectly relevant to the task.4 MATCH corpus analysisThe MATCH project made two data sets available,see (Stent et al, 2002) and (Whittaker et al, 2003),which we combine to define an evaluation functionfor different Information Presentation strategies.684strategy example av.#attr av.#sentenceSUMMARY ?The 4 restaurants differ in food quality, and cost.?
(#attr = 2,#sentence = 1)2.07?.63 1.56?.5COMPARE ?Among the selected restaurants, the following offerexceptional overall value.
Aureole?s price is 71 dol-lars.
It has superb food quality, superb service andsuperb decor.
Daniel?s price is 82 dollars.
It has su-perb food quality, superb service and superb decor.?
(#attr = 4,#sentence = 5)3.2?1.5 5.5?3.11RECOMMEND ?Le Madeleine has the best overall value among theselected restaurants.
Le Madeleine?s price is 40 dol-lars and It has very good food quality.
It?s in Mid-town West.
?
(#attr = 3,#sentence = 3)2.4?.7 3.5?.53Table 1: NLG strategies present in the MATCH corpus with average no.
attributes and sentences as foundin the data.The first data set, see (Stent et al, 2002), com-prises 1024 ratings by 16 subjects (where we onlyuse the speech-based half, n = 512) on the follow-ing presentation strategies: RECOMMEND, COM-PARE, SUMMARY.
These strategies are realizedusing templates as in Table 2, and varying num-bers of attributes.
In this study the users rate theindividual presentation strategies as significantlydifferent (F (2) = 1361, p < .001).
We find thatSUMMARY is rated significantly worse (p = .05with Bonferroni correction) than RECOMMENDand COMPARE, which are rated as equally good.This suggests that one should never generatea SUMMARY.
However, SUMMARY has differentqualities from COMPARE and RECOMMEND, asit gives users a general overview of the domain,and probably helps the user to feel more confi-dent when choosing an item, especially when theyare unfamiliar with the domain, as shown by (Po-lifroni and Walker, 2008).In order to further describe the strategies, we ex-tracted different surface features as present in thedata (e.g.
number of attributes realised, number ofsentences, number of words, number of databaseitems talked about, etc.)
and performed a step-wise linear regression to find the features whichwere important to the overhearers (following thePARADISE framework (Walker et al, 2000)).
Wediscovered a trade-off between the length of the ut-terance (#sentence) and the number of attributesrealised (#attr), i.e.
its informativeness, whereoverhearers like to hear as many attributes as pos-sible in the most concise way, as indicated bythe regression model shown in Equation 1 (R2 =.34).
1score = .775?#attr + (?.301)?#sentence;(1)The second MATCH data set, see (Whittaker etal., 2003), comprises 1224 ratings by 17 subjectson the NLG strategies RECOMMEND and COM-PARE.
The strategies realise varying numbers ofattributes according to different ?conciseness?
val-ues: concise (1 or 2 attributes), average (3or 4), and verbose (4,5, or 6).
Overhearersrate all conciseness levels as significantly different(F (2) = 198.3, p < .001), with verbose ratedhighest and concise rated lowest, supportingour findings in the first data set.
However, the rela-tion between number of attributes and user ratingsis not strictly linear: ratings drop for #attr = 6.This suggests that there is an upper limit on howmany attributes users like to hear.
We expect thisto be especially true for real users engaged in ac-tual dialogue interaction, see (Winterboer et al,2007).
We therefore include ?cognitive load?
as avariable when training the policy (see Section 6).In addition to the trade-off between length andinformativeness for single NLG strategies, we areinterested whether this trade-off will also hold forgenerating sequences of NLG actions.
(Whittakeret al, 2002), for example, generate a combinedstrategy where first a SUMMARY is used to de-scribe the retrieved subset and then they RECOM-MEND one specific item/restaurant.
For example?The 4 restaurants are all French, but differ in1For comparison: (Walker et al, 2000) report on R2 be-tween .4 and .5 on a slightly lager data set.685Figure 1: Possible NLG policies (X=stop generation)food quality, and cost.
Le Madeleine has the bestoverall value among the selected restaurants.
LeMadeleine?s price is 40 dollars and It has verygood food quality.
It?s in Midtown West.
?We therefore extend the set of possible strate-gies present in the data for exploration: we allowordered combinations of the strategies, assumingthat only COMPARE or RECOMMEND can follow aSUMMARY, and that only RECOMMEND can fol-low COMPARE, resulting in 7 possible actions:1.
RECOMMEND2.
COMPARE3.
SUMMARY4.
COMPARE+RECOMMEND5.
SUMMARY+RECOMMEND6.
SUMMARY+COMPARE7.
SUMMARY+COMPARE+RECOMMENDWe then analytically solved the regressionmodel in Equation 1 for the 7 possible strategiesusing average values from the MATCH data.
This issolved by a system of linear inequalities.
Accord-ing to this model, the best ranking strategy is todo all the presentation strategies in one sequence,i.e.
SUMMARY+COMPARE+RECOMMEND.
How-ever, this analytic solution assumes a ?one-shot?generation strategy where there is no intermediatefeedback from the environment: users are simplystatic overhearers (they cannot ?barge-in?
for ex-ample), there is no variation in the behaviour of thesurface realizer, i.e.
one would use fixed templatesas in MATCH, and the user has unlimited cogni-tive capabilities.
These assumptions are not real-istic, and must be relaxed.
In the next Section wedescribe a worked through example of the overallframework.5 Method: the RL-NLG modelFor the reasons discussed above, we treat theNLG module as a statistical planner, operat-ing in a stochastic environment, and optimiseit using Reinforcement Learning.
The in-put to the module is a Communicative Goalsupplied by the Dialogue Manager.
The CGconsists of a Dialogue Act to be generated,for example present items(i1, i2, i5, i8),and a System Goal (SysGoal) which isthe desired user reaction, e.g.
to make theuser choose one of the presented items(user choose one of(i1, i2, i5, i8)).
TheRL-NLG module must plan a sequence of lower-level NLG actions that achieve the goal (at lowestcost) in the current context.
The context consistsof a user (who may remain silent, supply moreconstraints, choose an item, or quit), and variationfrom the sentence realizer described above.Now let us walk-through one simple ut-terance plan as carried out by this model,as shown in Table 2.
Here, we startwith the CG present items(i1, i2, i5, i8)&user choose one of(i1, i2, i5, i8) from thesystem?s DM.
This initialises the NLG state (init).The policy chooses the action SUMMARY and thistransitions us to state s1, where we observe that4 attributes and 1 sentence have been generated,and the user is predicted to remain silent.
In thisstate, the current NLG policy is to RECOMMENDthe top ranked item (i5, for this user), which takesus to state s2, where 8 attributes have been gener-ated in a total of 4 sentences, and the user choosesan item.
The policy holds that in states like s2 the686init s1 s2summarise recommendendstopatts=4user=silent atts=8user=chooseENVIRONMENT:ACTIONS:GOALRewardFigure 2: Example RL-NLG action sequence for Table 4State Action State change/effectinit SysGoal: present items(i1, i2, i5, i8)& user choose one of(i1, i2, i5, i8) initialise states1 RL-NLG: SUMMARY(i1, i2, i5, i8) att=4, sent=1, user=silents2 RL-NLG: RECOMMEND(i5) att=8, sent=4, user=choose(i5)end RL-NLG: stop calculate RewardTable 2: Example utterance planning sequence for Figure 2best thing to do is ?stop?
and pass the turn to theuser.
This takes us to the state end, where the totalreward of this action sequence is computed (seeSection 6.3), and used to update the NLG policyin each of the visited state-action pairs via back-propagation.6 ExperimentsWe now report on a proof-of-concept study wherewe train our policy in a simulated learning envi-ronment based on the results from the MATCH cor-pus analysis in Section 4.
Simulation-based RLallows to explore unseen actions which are not inthe data, and thus less initial data is needed (Rieserand Lemon, 2008b).
Note, that we cannot directlylearn from the MATCH data, as therefore we wouldneed data from an interactive dialogue.
We arecurrently collecting such data in a Wizard-of-Ozexperiment.6.1 User simulationUser simulations are commonly used to trainstrategies for Dialogue Management, see for ex-ample (Young et al, 2007).
A user simulation forNLG is very similar, in that it is a predictive modelof the most likely next user act.
However, this useract does not actually change the overall dialoguestate (e.g.
by filling slots) but it only changes thegenerator state.
In other words, the NLG user sim-ulation tells us what the user is most likely to donext, if we were to stop generating now.
It alsotells us the probability whether the user choosesto ?barge-in?
after a system NLG action (by eitherchoosing an item or providing more information).The user simulation for this study is a simplebi-gram model, which relates the number of at-tributes presented to the next likely user actions,see Table 3.
The user can either follow the goalprovided by the DM (SysGoal), for examplechoosing an item.
The user can also do some-thing else (userElse), e.g.
providing anotherconstraint, or the user can quit (userQuit).For simplification, we discretise the number ofattributes into concise-average-verbose,reflecting the conciseness values from the MATCHdata, as described in Section 4.
In addition, weassume that the user?s cognitive abilities are lim-ited (?cognitive load?
), based on the results fromthe second MATCH data set in Section 4.
Once thenumber of attributes is more than the ?magic num-ber 7?
(reflecting psychological results on short-term memory) (Baddeley, 2001)) the user is morelikely to become confused and quit.The probabilities in Table 3 are currently man-ually set heuristics.
We are currently conducting aWizard-of-Oz study in order to learn these proba-687bilities (and other user parameters) from real data.SysGoal userElse userQuitconcise 20.0 60.0 20.0average 60.0 20.0 20.0verbose 20.0 20.0 60.0Table 3: NLG bi-gram user simulation6.2 Realizer modelThe sequential NLG model assumes a realizer,which updates the context after each generationstep (i.e.
after each single action).
We estimatethe realiser?s parameters from the mean values wefound in the MATCH data (see Table 1).
For thisstudy we first (randomly) vary the number of at-tributes, whereas the number of sentences is fixed(see Table 4).
In current work we replace the re-alizer model with an implemented generator thatreplicates the variation found in the SPaRKy real-izer (Stent et al, 2004).#attr #sentenceSUMMARY 1 or 2 2COMPARE 3 or 4 6RECOMMEND 2 or 3 3Table 4: Realizer parameters6.3 Reward functionThe reward function defines the final goal of theutterance generation sequence.
In this experimentthe reward is a function of the various data-driventrade-offs as identified in the data analysis in Sec-tion 4: utterance length and number of providedattributes, as weighted by the regression modelin Equation 1, as well as the next predicted useraction.
Since we currently only have overhearerdata, we manually estimate the reward for thenext most likely user act, to supplement the data-driven model.
If in the end state the next mostlikely user act is userQuit, the learner gets apenalty of ?100, userElse receives 0 reward,and SysGoal gains +100 reward.
Again, thesehand coded scores need to be refined by a moretargeted data collection, but the other componentsof the reward function are data-driven.Note that RL learns to ?make compromises?with respect to the different trade-offs.
For ex-ample, the user is less likely to choose an itemif there are more than 7 attributes, but the real-izer can generate 9 attributes.
However, in somecontexts it might be desirable to generate all 9 at-tributes, e.g.
if the generated utterance is short.Threshold-based approaches, in contrast, cannot(easily) reason with respect to the current content.6.4 State and Action SpaceWe now formulate the problem as a Markov De-cision Process (MDP), relating states to actions.Each state-action pair is associated with a transi-tion probability, which is the probability of mov-ing from state s at time t to state s?
at time t+1 af-ter having performed action awhen in state s. Thistransition probability is computed by the environ-ment model (i.e.
user and realizer), and explic-itly captures noise/uncertainty in the environment.This is a major difference to other non-statisticalplanning approaches.
Each transition is also as-sociated with a reinforcement signal (or reward)rt+1 describing how good the result of action awas when performed in state s.The state space comprises 9 binary features rep-resenting the number of attributes, 2 binary fea-tures representing the predicted user?s next ac-tion to follow the system goal or quit, as well asa discrete feature reflecting the number of sen-tences generated so far, as shown in Figure 3.This results in 211 ?
6 = 12, 288 distinct genera-tion states.
We trained the policy using the wellknown SARSA algorithm, using linear function ap-proximation (Sutton and Barto, 1998).
The policywas trained for 3600 simulated NLG sequences.In future work we plan to learn lower level deci-sions, such as lexical adaptation based on the vo-cabulary used by the user.6.5 BaselinesWe derive the baseline policies from Informa-tion Presentation strategies as deployed by cur-rent dialogue systems.
In total we utilise 7 differ-ent baselines (B1-B7), which correspond to singlebranches in our policy space (see Figure 1):B1: RECOMMEND only, e.g.
(Young et al, 2007)B2: COMPARE only, e.g.
(Henderson et al, 2008)B3: SUMMARY only, e.g.
(Polifroni and Walker,2008)B4: SUMMARY followed by RECOMMEND, e.g.
(Whittaker et al, 2002)B5: Randomly choosing between COMPARE andRECOMMEND, e.g.
(Walker et al, 2007)688??????????action:?????SUMMARYCOMPARERECOMMENDend?????state:?????????
?attributes |1 |-|9 |:{0,1}sentence:{1-11}userGoal:{0,1}userQuit:{0,1}???????????????????
?Figure 3: State-Action space for RL-NLGB6: Randomly choosing between all 7 outputsB7: Always generating whole sequence, i.e.SUMMARY+COMPARE+RECOMMEND, assuggested by the analytic solution (seeSection 4).6.6 ResultsWe analyse the test runs (n=200) using an ANOVAwith a PostHoc T-Test (with Bonferroni correc-tion).
RL significantly (p < .001) outperforms allbaselines in terms of final reward, see Table 5.
RLis the only policy which significantly improves thenext most likely user action by adapting to featuresin the current context.
In contrast to conventionalapproaches, RL learns to ?control?
its environmentaccording to the estimated transition probabilitiesand the associated rewards.The learnt policy can be described as follows:It either starts with SUMMARY or COMPARE af-ter the init state, i.e.
it learnt to never start with aRECOMMEND.
It stops generating after COMPAREif the userGoal is (probably) reached (e.g.
theuser is most likely to choose an item in the nextturn, which depends on the number of attributesgenerated), otherwise it goes on and generates aRECOMMEND.
If it starts with SUMMARY, it al-ways generates a COMPARE afterwards.
Again, itstops if the userGoal is (probably) reached, oth-erwise it generates the full sequence (which corre-sponds to the analytic solution B7).The analytic solution B7 performs second best,and significantly outperforms all the other base-lines (p < .01).
Still, it is significantly worse(p < .001) than the learnt policy as this ?one-shot-strategy?
cannot robustly and dynamically adopt tonoise or changes in the environment.In general, generating sequences of NLG ac-tions rates higher than generating single actionsonly: B4 and B6 rate directly after RL and B7,while B1, B2, B3, B5 are all equally bad givenour data-driven definition of reward and environ-ment.
Furthermore, the simulated environmentallows us to replicate the results in the MATCHcorpus (see Section 4) when only comparing sin-gle strategies: SUMMARY performs significantlyworse, while RECOMMEND and COMPARE per-form equally well.policy reward (?std)B1 99.1 (?129.6)B2 90.9 (?142.2)B3 65.5 (?137.3)B4 176.0 (?154.1)B5 95.9 (?144.9)B6 168.8 (?165.3)B7 229.3 (?157.1)RL 310.8 (?136.1)Table 5: Evaluation Results (p < .001 )7 ConclusionWe presented and evaluated a new model for Nat-ural Language Generation (NLG) in Spoken Dia-logue Systems, based on statistical planning.
Aftermotivating and presenting the model, we studiedits use in Information Presentation.We derived a data-driven model predictingusers?
judgements to different information presen-tation actions (reward function), via a regressionanalysis on MATCH data.
We used this regressionmodel to set weights in a reward function for Re-inforcement Learning, and so optimize a context-adaptive presentation policy.
The learnt policy wascompared to several baselines derived from previ-ous work in this area, where the learnt policy sig-nificantly outperforms all the baselines.There are many possible extensions to thismodel, e.g.
using the same techniques to jointlyoptimise choosing the number of attributes, aggre-gation, word choice, referring expressions, and soon, in a hierarchical manner.689We are currently collecting data in targetedWizard-of-Oz experiments, to derive a fully data-driven training environment and test the learntpolicy with real users, following (Rieser andLemon, 2008b).
The trained NLG strategywill also be integrated in an end-to-end statis-tical system within the CLASSiC project (www.classic-project.org).AcknowledgmentsThe research leading to these results has receivedfunding from the European Community?s Sev-enth Framework Programme (FP7/2007-2013) un-der grant agreement no.
216594 (CLASSiC projectproject: www.classic-project.org) andfrom the EPSRC project no.
EP/E019501/1.ReferencesA.
Baddeley.
2001.
Working memory and language:an overview.
Journal of Communication Disorder,36(3):189?208.Vera Demberg and Johanna D. Moore.
2006.
Infor-mation presentation in spoken dialogue systems.
InProceedings of EACL.James Henderson, Oliver Lemon, and KallirroiGeorgila.
2008.
Hybrid reinforcement / supervisedlearning of dialogue policies from fixed datasets.Computational Linguistics (to appear).Srinivasan Janarthanam and Oliver Lemon.
2008.
Usersimulations for online adaptation and knowledge-alignment in Troubleshooting dialogue systems.
InProc.
of SEMdial.Alexander Koller and Ronald Petrick.
2008.
Experi-ences with planning for natural language generation.In ICAPS.Alexander Koller and Matthew Stone.
2007.
Sentencegeneration as planning.
In Proceedings of ACL.Oliver Lemon.
2008.
Adaptive Natural LanguageGeneration in Dialogue using Reinforcement Learn-ing.
In Proceedings of SEMdial.Johanna Moore, Mary Ellen Foster, Oliver Lemon, andMichael White.
2004.
Generating tailored, com-parative descriptions in spoken dialogue.
In Proc.FLAIRS.Crystal Nakatsu and Michael White.
2006.
Learningto say it well: Reranking realizations by predictedsynthesis quality.
In Proceedings of ACL.Alice Oh and Alexander Rudnicky.
2002.
Stochasticnatural language generation for spoken dialog sys-tems.
Computer, Speech & Language, 16(3/4):387?407.Tim Paek and Eric Horvitz.
2000.
Conversation asaction under uncertainty.
In Proc.
of the 16th Con-ference on Uncertainty in Artificial Intelligence.Joseph Polifroni and Marilyn Walker.
2008.
Inten-sional Summaries as Cooperative Responses in Di-alogue Automation and Evaluation.
In Proceedingsof ACL.Verena Rieser and Oliver Lemon.
2008a.
Does thislist contain what you were searching for?
Learn-ing adaptive dialogue strategies for Interactive Ques-tion Answering.
J.
Natural Language Engineering,15(1):55?72.Verena Rieser and Oliver Lemon.
2008b.
Learn-ing Effective Multimodal Dialogue Strategies fromWizard-of-Oz data: Bootstrapping and Evaluation.In Proceedings of ACL.S.
Singh, D. Litman, M. Kearns, and M. Walker.
2002.Optimizing dialogue management with Reinforce-ment Learning: Experiments with the NJFun sys-tem.
JAIR, 16:105?133.Amanda Stent, Marilyn Walker, Steve Whittaker, andPreetam Maloor.
2002.
User-tailored generation forspoken dialogue: an experiment.
In In Proc.
of IC-SLP.Amanda Stent, Rashmi Prasad, and Marilyn Walker.2004.
Trainable sentence planning for complex in-formation presentation in spoken dialog systems.
InAssociation for Computational Linguistics.R.
Sutton and A. Barto.
1998.
Reinforcement Learn-ing.
MIT Press.Marilyn A. Walker, Candace A. Kamm, and Diane J.Litman.
2000.
Towards developing general mod-els of usability with PARADISE.
Natural LanguageEngineering, 6(3).Marilyn Walker, S. Whittaker, A. Stent, P. Maloor,J.
Moore, M. Johnston, and G. Vasireddy.
2004.User tailored generation in the match multimodal di-alogue system.
Cognitive Science, 28:811?840.MarilynWalker, Amanda Stent, Franc?ois Mairesse, andRashmi Prasad.
2007.
Individual and domain adap-tation in sentence planning for dialogue.
Journal ofArtificial Intelligence Research (JAIR), 30:413?456.Steve Whittaker, Marilyn Walker, and Johanna Moore.2002.
Fish or Fowl: A Wizard of Oz evaluationof dialogue strategies in the restaurant domain.
InProc.
of the International Conference on LanguageResources and Evaluation (LREC).Stephen Whittaker, Marilyn Walker, and Preetam Mal-oor.
2003.
Should i tell all?
an experiment onconciseness in spoken dialogue.
In Proc.
EuropeanConference on Speech Processing (EUROSPEECH).690Andi Winterboer, Jiang Hu, Johanna D. Moore, andClifford Nass.
2007.
The influence of user tailoringand cognitive load on user performance in spokendialogue systems.
In Proc.
of the 10th InternationalConference of Spoken Language Processing (Inter-speech/ICSLP).SJ Young, J Schatzmann, K Weilhammer, and H Ye.2007.
The Hidden Information State Approach toDialog Management.
In ICASSP 2007.691
