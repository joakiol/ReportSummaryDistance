Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing (EMNLP 2006), pages 138?145,Sydney, July 2006. c?2006 Association for Computational LinguisticsSemantic Role Labeling of NomBank: A Maximum Entropy ApproachZheng Ping Jiang and Hwee Tou NgDepartment of Computer ScienceNational University of Singapore3 Science Drive 2, Singapore 117543{jiangzp, nght}@comp.nus.edu.sgAbstractThis paper describes our attempt atNomBank-based automatic Semantic RoleLabeling (SRL).
NomBank is a project atNew York University to annotate the ar-gument structures for common nouns inthe Penn Treebank II corpus.
We treatthe NomBank SRL task as a classifica-tion problem and explore the possibilityof adapting features previously shown use-ful in PropBank-based SRL systems.
Var-ious NomBank-specific features are ex-plored.
On test section 23, our best sys-tem achieves F1 score of 72.73 (69.14)when correct (automatic) syntactic parsetrees are used.
To our knowledge, thisis the first reported automatic NomBankSRL system.1 IntroductionAutomatic Semantic Role Labeling (SRL) sys-tems, made possible by the availability of Prop-Bank (Kingsbury and Palmer, 2003; Palmer etal., 2005), and encouraged by evaluation ef-forts in (Carreras and Marquez, 2005; Litkowski,2004), have been shown to accurately determinethe argument structure of verb predicates.A successful PropBank-based SRL systemwould correctly determine that ?Ben Bernanke?is the subject (labeled as ARG0 in PropBank) ofpredicate ?replace?, and ?Greenspan?
is the object(labeled as ARG1):?
Ben Bernanke replaced Greenspan as Fedchair.?
Greenspan was replaced by Ben Bernanke asFed chair.The recent release of NomBank (Meyers et al,2004c; Meyers et al, 2004b), a databank that an-notates argument structure for instances of com-mon nouns in the Penn Treebank II corpus, madeit possible to develop automatic SRL systems thatanalyze the argument structures of noun predi-cates.Given the following two noun phrases and onesentence, a successful NomBank-based SRL sys-tem should label ?Ben Bernanke?
as the subject(ARG0) and ?Greenspan?
as the object (ARG1)of the noun predicate ?replacement?.?
Greenspan?s replacement Ben Bernanke?
Ben Bernanke?s replacement of Greenspan?
Ben Bernanke was nominated as Greenspan?sreplacement.The ability to automatically analyze the argu-ment structures of verb and noun predicates wouldgreatly facilitate NLP tasks like question answer-ing, information extraction, etc.This paper focuses on our efforts at buildingan accurate automatic NomBank-based SRL sys-tem.
We study how techniques used in buildingPropBank SRL system can be transferred to de-veloping NomBank SRL system.
We also makeNomBank-specific enhancements to our baselinesystem.
Our implemented SRL system and exper-iments are based on the September 2005 release ofNomBank (NomBank.0.8).The rest of this paper is organized as follows:Section 2 gives an overview of NomBank, Sec-tion 3 introduces the Maximum Entropy classifica-tion model, Section 4 introduces our features andfeature selection strategy, Section 5 explains theexperimental setup and presents the experimen-tal results, Section 6 compares NomBank SRL to138SHHHHHHNP(ARG0) HHNNPBenNNPBernankeVPHHHHHVBDwasVPHHHHVBN(Support)nominatedPPHHHHINasNPHHHHNP(ARG1) PPPGreenspan ?sNNpredicatereplacementFigure 1: A sample sentence and its parse tree la-beled in the style of NomBankPropBank SRL and discusses possible future re-search directions.2 Overview of NomBankThe NomBank (Meyers et al, 2004c; Meyerset al, 2004b) annotation project originated fromthe NOMLEX (Macleod et al, 1997; Macleod etal., 1998) nominalization lexicon developed underthe New York University Proteus Project.
NOM-LEX lists 1,000 nominalizations and the corre-spondences between their arguments and the ar-guments of their verb counterparts.
NomBankframes combine various lexical resources (Meyerset al, 2004a), including an extended NOMLEXand PropBank frames, and form the basis for anno-tating the argument structures of common nouns.Similar to PropBank, NomBank annotation ismade on the Penn TreeBank II (PTB II) corpus.For each common noun in PTB II that takes argu-ments, its core arguments are labeled with ARG0,ARG1, etc, and modifying arguments are labeledwith ARGM-LOC to denote location, ARGM-MNR to denote manner, etc.
Annotations aremade on PTB II parse tree nodes, and argumentboundaries align with the span of parse tree nodes.A sample sentence and its parse tree labeledin the style of NomBank is shown in Figure 1.For the nominal predicate ?replacement?, ?BenBernanke?
is labeled as ARG0 and ?Greenspan?s?
is labeled as ARG1.
There is also the speciallabel ?Support?
on ?nominated?
which introduces?Ben Bernanke?
as an argument of ?replacement?.The support construct will be explained in detail inSection 4.2.3.We are not aware of any NomBank-based auto-matic SRL systems.
The work in (Pradhan et al,2004) experimented with an automatic SRL sys-tem developed using a relatively small set of man-ually selected nominalizations from FrameNet andPenn Chinese TreeBank.
The SRL accuracy oftheir system is not directly comparable to ours.3 Model training and testingWe treat the NomBank-based SRL task as a clas-sification problem and divide it into two phases:argument identification and argument classifica-tion.
During the argument identification phase,each parse tree node is marked as either argumentor non-argument.
Each node marked as argumentis then labeled with a specific class during theargument classification phase.
The identificationmodel is a binary classifier , while the classifica-tion model is a multi-class classifier.Opennlp maxent1, an implementation of Maxi-mum Entropy (ME) modeling, is used as the clas-sification tool.
Since its introduction to the NaturalLanguage Processing (NLP) community (Bergeret al, 1996), ME-based classifiers have beenshown to be effective in various NLP tasks.
MEmodeling is based on the insight that the bestmodel is consistent with the set of constraints im-posed and otherwise as uniform as possible.
MEmodels the probability of label l given input x asin Equation 1. fi(l, x) is a feature function thatmaps label l and input x to either 0 or 1, while thesummation is over all n feature functions and with?i as the weight parameter for each feature func-tion fi(l, x).
Zx is a normalization factor.
In theidentification model, label l corresponds to either?argument?
or ?non-argument?, and in the classi-fication model, label l corresponds to one of thespecific NomBank argument classes.
The classifi-cation output is the label l with the highest condi-tional probability p(l|x).p(l|x) = exp(?ni=1 ?ifi(l, x))Zx (1)To train the ME-based identification model,training data is gathered by treating each parse treenode that is an argument as a positive example andthe rest as negative examples.
Classification train-ing data is generated from argument nodes only.During testing, the algorithm of enforcing non-overlapping arguments by (Toutanova et al, 2005)is used.
The algorithm maximizes the log-probability of the entire NomBank labeled parse1http://maxent.sourceforge.net/139tree.
Specifically, assuming we only have twoclasses ?ARG?
and ?NONE?, the log-probabilityof a NomBank labeled parse tree is defined byEquation 2.Max(T ) = max{NONE(T ) +?
(Max(child))ARG(T ) +?
(NONETree(child))(2)Max(T ) is the maximum log-probability of atree T , NONE(T ) and ARG(T ) are respectivelythe log-probability of assigning label ?NONE?and ?ARG?
by our argument identification modelto tree node T , child ranges through each ofT ?s children, and NONETree(child) is the log-probability of each node that is dominated by nodechild being labeled as ?NONE?.
Details are pre-sented in Algorithm 1.Algorithm 1 Maximizing the probability of anSRL treeInput p{syntactic parse tree}Input m{argument identification model, assigns each con-stituent in the parse tree log likelihood of being a semanticargument}Output score{maximum log likelihood of the parse tree pwith arguments identified using model m}MLParse(p, m)if parse p is a leaf node thenreturn max(Score(p,m,ARG), Score(p,m,NONE))elseMLscore = 0for each node ci in Children(p) doMLscore += MLParse(ci,m)end forNONEscore = 0for each node ci in Children(p) doNONEscore += NONETree(ci,m)end forreturn max(Score(p,m,NONE)+MLscore,Score(p,m,ARG)+NONEscore)end ifNONETree(p,m)NONEscore = Score(p,m,NONE)if parse p is a leaf node thenreturn NONEscoreelsefor each node ci in Children(p) doNONEscore += NONETree(ci,m)end forreturn NONEscoreend ifSubroutine:Children(p) returns the list of children nodes of p.Score(p,m, state) returns the log likelihood assigned bymodel m, for parse p with state.
state is either ARG orNONE.NomBank sections 02-21 are used as trainingdata, section 24 and 23 are used as developmentand test data, respectively.3.1 Training data preprocessingUnlike PropBank annotation which does not con-tain overlapping arguments (in the form of parsetree nodes domination) and does not allow pred-icates to be dominated by arguments, NomBankannotation in the September 2005 release containssuch cases.
In NomBank sections 02-21, about0.6% of the argument nodes dominate some otherargument nodes or the predicate.
To simplify ourtask, during training example generation, we ig-nore arguments that dominate the predicate.
Wealso ignore arguments that are dominated by otherarguments, so that when argument domination oc-curs, only the argument with the largest word spanis kept.
We do not perform similar pruning on thetest data.4 Features and feature selection4.1 Baseline NomBank SRL featuresTable 1 lists the baseline features we adapted fromprevious PropBank-based SRL systems (Pradhanet al, 2005; Xue and Palmer, 2004).
For easeof description, related features are grouped, witha specific individual feature given individual ref-erence name.
For example, feature b11FW inthe group b11 denotes the first word spanned bythe constituent and b13LH denotes the left sis-ter?s head word.
We also experimented with vari-ous feature combinations, inspired by the featuresused in (Xue and Palmer, 2004).
These are listedas features b31 to b34 in Table 1.Suppose the current constituent under identifi-cation or classification is ?NP-Ben Bernanke?
inFigure 1.
The instantiations of the baseline fea-tures in Table 1 for this example are presented inTable 2.
The symbol ?NULL?
is used to denotefeatures that fail to instantiate.4.2 NomBank-specific features4.2.1 NomBank predicate morphology andclassThe ?NomBank-morph?
dictionary provided bythe current NomBank release maps the base formof a noun to various morphological forms.
Be-sides singular-plural noun form mapping, it alsomaps base nouns to hyphenated and compoundnouns.
For example, ?healthcare?
and ?medical-care?
both map to ?care?.
For NomBank SRL fea-140Baseline Features (Pradhan et al, 2005)b1 predicate: stemmed nounb2 subcat: grammar rule that expands the predicate?sparentb3 phrase type: syntactic category of the constituentb4 head word: syntactic head of the constituentb5 path: syntactic path from the constituent to thepredicateb6 position: to the left or right of the predicateb11 first or last word/POS spanned by the constituent(b11FW, b11LW, b11FP, b11LP)b12 phrase type of the left or right sister (b12L, b12R)b13 left or right sister?s head word/POS (b13LH,b13LP, b13RH, b13RP)b14 phrase type of parentb15 parent?s head word or its POS (b15H, b15P)b16 head word of the constituent if its parent has phrasetype PPb17 head word or POS tag of the rightmost NP node, ifthe constituent is PP (b17H, b17P)b18 phrase type appended with the length of pathb19 temporal keyword, e.g., ?Monday?b20 partial path from the constituent to the lowest com-mon ancestor with the predicateb21 projected path from the constituent to the highestNP dominating the predicateBaseline Combined Features (Xue and Palmer, 2004)b31 b1 & b3b32 b1 & b4b33 b1 & b5b34 b1 & b6Table 1: Baseline features for NomBank SRLtures, we use this set of more specific mappingsto replace the morphological mappings based onWordNet.
Specifically, we replace feature b1 inTable 1 with feature a1 in Table 3.The current NomBank release also containsthe ?NOMLEX-PLUS?
dictionary, which con-tains the class of nominal predicates according totheir origin and the roles they play.
For exam-ple, ?employment?
originates from the verb ?em-ploy?
and is classified as ?VERB-NOM?, whilethe nouns ?employer?
and ?employee?
are classi-fied as ?SUBJECT?
and ?OBJECT?
respectively.Other classes include ?ADJ-NOM?
for nominal-ization of adjectives and ?NOM-REL?
for rela-tional nouns.
The class of a nominal predicate isvery indicative of the role of its arguments.
Wewould expect a ?VERB-NOM?
predicate to takeboth ARG0 and ARG1, while an ?OBJECT?
pred-icate to take only ARG0.
We incorporated theclass of nominal predicates as additional featuresin our NomBank SRL system.
We add feature a2in Table 3 to use this information.Baseline Features (Pradhan et al, 2005)b1 replacementb2 NP ?
NP NNb3 NPb4 Bernankeb5 NP?S?VP?VP?PP?NP?NNb6 leftb11 Ben, Bernanke, NNP, NNPb12 NULL, VPb13 NULL, NULL, was, VBDb14 Sb15 was, VBDb16 NULLb17 NULL, NULLb18 NP-7b19 NULLb20 NP?Sb21 NP?S?VP?VP?PP?NPBaseline Combined Features (Xue and Palmer, 2004)b31 replacement & NPb32 replacement & Bernankeb33 replacement & NP?S?VP?VP?PP?NP?NNb34 replacement & leftTable 2: Baseline feature instantiations, assumingthe current constituent is ?NP-Ben Bernanke?
inFigure 1.Additional Features Based on NomBanka1 Nombank morphed noun stema2 Nombank nominal classa3 identical to predicate?a4 a DEFREL noun?a5 whether under the noun phrase headed by the pred-icatea6 whether the noun phrase headed by the predicateis dominated by a VP node or has neighboring VPnodesa7 whether there is a verb between the constituent andthe predicateAdditional Combined Featuresa11 a1 & a2a12 a1 & a3a13 a1 & a5a14 a3 & a4a15 a1 & a6a16 a1 & a7Additional Features of Neighboring Argumentsn1 for each argument already classified, b3-b4-b5-b6-r, where r is the argument class, otherwise b3-b4-b5-b6n2 backoff version of n1, b3-b6-r or b3-b6Table 3: Additional NomBank-specific featuresfor NomBank SRL4.2.2 DEFREL relational noun predicateAbout 14% of the argument node instances inNomBank sections 02-21 are identical to theirnominal predicate nodes.
Most of these nominalpredicates are DEFREL relational nouns (Mey-ers et al, 2004c).
Examples of DEFREL rela-tional nouns include ?employee?, ?participant?,141and ?husband?, where the nominal predicate itselftakes part as an implied argument.We include in our classification features an indi-cator of whether the argument coincides with thenominal predicate.
We also include a feature test-ing if the argument is one of the DEFREL nounswe extracted from NomBank training sections 02-21.
These two features correspond to a3 and a4 inTable 3.4.2.3 Support verbStatistics show that almost 60% of the argu-ments of nominal predicates occur locally insidethe noun phrase headed by the nominal pred-icate.
For the cases where an argument ap-pears outside the local noun phrase, over half ofthese arguments are introduced by support verbs.Consider our example ?Ben Bernanke was nomi-nated as Greenspan?s replacement.
?, the argument?Ben Bernanke?
is introduced by the support verb?nominate?.
The arguments introduced by sup-port verbs can appear syntactically distant fromthe nominal predicate.To capture the location of arguments and theexistence of support verbs, we add features in-dicating whether the argument is under the nounphrase headed by the predicate, whether the nounphrase headed by the predicate is dominated bya VP phrase or has neighboring VP phrases, andwhether there is a verb between the argument andthe predicate.
These are represented as featuresa5, a6, and a7 in Table 3.
Feature a7 was also pro-posed by the system in (Pradhan et al, 2004).We also experimented with various featurecombinations, inspired by the features usedin (Xue and Palmer, 2004).
These are listed asfeatures a11 to a16 in Table 3.4.2.4 Neighboring argumentsThe research of (Jiang et al, 2005; Toutanova etal., 2005) has shown the importance of capturinginformation of the global argument frame in orderto correctly classify the local argument.We make use of the features {b3,b4,b5,b6} ofthe neighboring arguments as defined in Table 1.Arguments are classified from left to right in thetextual order they appear.
For arguments that arealready labeled, we also add their argument classr.
Specifically, for each argument to the left of thecurrent argument, we have a feature b3-b4-b5-b6-r. For each argument to the right of the currentargument, the feature is defined as b3-b4-b5-b6.We extract features in a window of size 7, centeredat the current argument.
We also add a backoffversion (b3-b6-r or b3-b6) of this specific feature.These additional features are shown as n1 and n2in Table 3.Suppose the current constituent under identi-fication or classification is ?NP-Ben Bernanke?.The instantiations of the additional features in Ta-ble 3 are listed in Table 4.Additional Features based on NomBanka1 replacementa2 VERB-NOMa3 noa4 noa5 noa6 yesa7 yesAdditional Combined Featuresa11 replacement & VERB-NOMa12 replacement & noa13 replacement & noa14 no & noa15 replacement & yesa16 replacement & yesAdditional Features of Neighboring Argumentsn1 NP-Greenspan-NP?NP?NN-leftn2 NP-leftTable 4: Additional feature instantiations, assum-ing the current constituent is ?NP-Ben Bernanke?in Figure 1.4.3 Feature selectionFeatures used by our SRL system are automati-cally extracted from PTB II parse trees manuallylabeled in NomBank.
Features from Table 1 andTable 3 are selected empirically and incremen-tally according to their contribution to test accu-racy on the development section 24.
The featureselection process stops when adding any of theremaining features fails to improve the SRL ac-curacy on development section 24.
We start theselection process with the basic set of features{b1,b2,b3,b4,b5,b6}.
The detailed feature selec-tion algorithm is presented in Algorithm 2.Features for argument identification and argu-ment classification are independently selected.
Toselect the features for argument classification, weassume that all arguments have been correctlyidentified.After performing greedy feature selection, thebaseline set of features selected for identificationis {b1-b6, b11FW, b11LW, b12L, b13RH, b13RP,b14, b15H, b18, b20, b32-b34}, and the baseline142Algorithm 2 Greedy feature selectionInput Fcandidate{set of all candidate features}Output Fselect{set of selected features}Output Mselect{selected model}Initialize:Fselect = {b1, b2, b3, b4, b5, b6}Fcandidate = AllFeatures?
FselectMselect = Train(Fselect)Eselect = Evaluate(Mselect, DevData)loopfor each feature fi in Fcandidate doFi = Fselect?
fiMi = Train(Fi)Ei = Evaluate(Mi, DevData)end forEmax = Max(Ei)if Emax > Eselect thenFselect = Fselect?
fmaxMselect = MmaxEselect = EmaxFcandidate = Fcandidate ?
fmaxend ifif Fcandidate == ?
or Emax ?
Eselect thenreturn Fselect,Mselectend ifend loopSubroutine:Evaluate(Model,Data) returns the accuracy score byevaluating Model on Data.Train(FeatureSet) returns maxent model trained on thegiven feature set.set of features selected for classification is {b1-b6,b11, b12, b13LH, b13LP, b13RP, b14, b15, b16,b17P, b20, b31-b34}.
Note that features in {b19,b21} are not selected.
For the additional featuresin Table 3, greedy feature selection chose {a1, a5,a6, a11, a12, a14} for the identification model and{a1, a3, a6, a11, a14, a16, n1, n2} for the classifi-cation model.5 Experimental results5.1 Scores on development section 24After applying the feature selection algorithm inSection 4.3, the SRL F1 scores on developmentsection 24 are presented in Table 5.
We sepa-rately present the F1 score for identification-onlyand classification-only model.
We also apply theclassification model on the output of the identifica-tion phase (which may contain erroneously identi-fied arguments in general) to obtain the combinedaccuracy.
During the identification-only and com-bined identification and classification testing, thetree log-probability maximization algorithm basedon Equation 2 (and its extension to multi-classes)is used.
During the classification-only testing, weidentification classification combinedbaseline 80.32 84.86 69.70additional 80.55 87.31 70.12Table 5: NomBank SRL F1 scores on develop-ment section 24, based on correct parse treesidentification classification combinedbaseline 82.33 85.85 72.20additional 82.50 87.80 72.73Table 6: NomBank SRL F1 scores on test section23, based on correct parse treesclassify each correctly identified argument usingthe classification ME model.
The ?baseline?
rowlists the F1 scores when only the baseline fea-tures are used, and the ?additional?
row lists theF1 scores when additional features are added tothe baseline features.5.2 Testing on section 23The identification and classification models basedon the chosen features in Section 4.3 are then ap-plied to test section 23.
The resulting F1 scoresare listed in Table 6.
Using additional features, theidentification-only, classification-only, and com-bined F1 scores are 82.50, 87.80, and 72.73, re-spectively.Performing chi-square test at the level of sig-nificance 0.05, we found that the improvement ofthe classification model using additional featurescompared to using just the baseline features is sta-tistically significant, while the corresponding im-provements due to additional features for the iden-tification model and the combined model are notstatistically significant.The improved classification accuracy due to theuse of additional features does not contribute anysignificant improvement to the combined identifi-cation and classification SRL accuracy.
This is dueto the noisy arguments identified by the inadequateidentification model, since the accurate determi-nation of the additional features (such as those ofneighboring arguments) depends critically on anaccurate identification model.5.3 Using automatic syntactic parse treesSo far we have assumed the availability of cor-rect syntactic parse trees during model trainingand testing.
We relax this assumption by usingthe re-ranking parser presented in (Charniak and143Johnson, 2005) to automatically generate the syn-tactic parse trees for both training and test data.The F1 scores of our best NomBank SRL sys-tem, when applied to automatic syntactic parsetrees, are 66.77 for development section 24 and69.14 for test section 23.
These F1 scores are forcombined identification and classification, withthe use of additional features.
Comparing thesescores with those in Table 5 and Table 6, the usageof automatic parse trees lowers the F1 accuracy bymore than 3%.
The decrease in accuracy is ex-pected, due to the noise introduced by automaticsyntactic parsing.6 Discussion and future work6.1 Comparison of the composition ofPropBank and NomBankCounting the number of annotated predicates, thesize of the September 2005 release of NomBank(NomBank.0.8) is about 83% of PropBank release1.
Preliminary consistency tests reported in (Mey-ers et al, 2004c) shows that NomBank?s inter-annotator agreement rate is about 85% for corearguments and lower for adjunct arguments.
Theinter-annotator agreement for PropBank reportedin (Palmer et al, 2005) is above 0.9 in terms of theKappa statistic (Sidney and Castellan Jr., 1988).While the two agreement measures are not di-rectly comparable, the current NomBank.0.8 re-lease documentation indicates that only 32 of themost frequently occurring nouns in PTB II havebeen adjudicated.We believe the smaller size of NomBank.0.8and the potential noise contained in the current re-lease of the NomBank data may partly explain ourlower SRL accuracy on NomBank, especially inthe argument identification phase, as compared tothe published accuracies of PropBank-based SRLsystems.6.2 Difficulties in NomBank SRLThe argument structure of nominalization phrasesis less fixed (i.e., more flexible) than the argumentstructure of verbs.
Consider again the examplegiven in the introduction, we find the followingflexibility in forming grammatical NomBank ar-gument structures for ?replacement?:?
The positions of the arguments are flexi-ble, so that ?Greenspan?s replacement BenBernanke?, ?Ben Bernanke?s replacement ofGreenspan?
are both grammatical.?
Arguments can be optional, so that?Greenspan?s replacement will assumethe post soon?, ?The replacement BenBernanke will assume the post soon?, and?The replacement will assume the post soon?are all grammatical.With the verb predicate ?replace?, except for?Greenspan was replaced?, there is no freedom offorming phrases like ?Ben Bernanke replaces?
orsimply ?replaces?
without supplying the necessaryarguments to complete the grammatical structure.We believe the flexible argument structure ofNomBank noun predicates contributes to the lowerautomatic SRL accuracy as compared to that of thePropBank SRL task.6.3 Integrating PropBank and NomBankSRLWork in (Pustejovsky et al, 2005) discussed thepossibility of merging various Treebank annota-tion efforts including PropBank, NomBank, andothers.
Future work involves studying waysof concurrently producing automatic PropBankand NomBank SRL, and improving the accuracyby exploiting the inter-relationship between verbpredicate-argument and noun predicate-argumentstructures.Besides the obvious correspondence between averb and its nominalizations, e.g., ?replace?
and?replacement?, there is also correspondence be-tween verb predicates in PropBank and supportverbs in NomBank.
Statistics from NomBank sec-tions 02-21 show that 86% of the support verbs inNomBank are also predicate verbs in PropBank.When they coincide, they share 18,250 argumentsof which 63% have the same argument class inPropBank and NomBank.Possible integration approaches include:?
Using PropBank data as augmentation toNomBank training data.?
Using re-ranking techniques (Collins, 2000)to jointly improve PropBank and NomBankSRL accuracy.7 ConclusionWe have successfully developed a statisticalNomBank-based SRL system.
Features that werepreviously shown to be effective in PropBank SRLare carefully selected and adapted for NomBankSRL.
We also proposed new features to address144the special predicate-argument structure in Nom-Bank data.
To our knowledge, we presented thefirst result in statistical NomBank SRL.ReferencesAdam Berger, Stephen Della Pietra, and Vincent DellaPietra.
1996.
A Maximum Entropy Approach toNatural Language Processing.
Computational Lin-guistics.Xavier Carreras and Lluis Marquez.
2005.
Intro-duction to the CoNLL-2005 Shared Task: SemanticRole Labeling.
In Proceedings of CoNLL-2005.Eugene Charniak and Mark Johnson.
2005.
Coarse-to-fine n-Best Parsing and MaxEnt DiscriminativeReranking.
In Proceedings of ACL-2005.Michael Collins.
2000.
Discriminative Reranking forNatural Language Parsing.
In Proceedings of ICML2000.Zheng Ping Jiang, Jia Li, and Hwee Tou Ng.
2005.Semantic Argument Classification Exploiting Argu-ment Interdependence.
In Proceedings of IJCAI2005.Paul Kingsbury and Martha Palmer.
2003.
PropBank:the Next Level of TreeBank.
In Proceedings of Tree-banks and Lexical Theories.Kenneth C. Litkowski.
2004.
SENSEVAL-3 Task: Au-tomatic Labeling of Semantic Roles.
In Proceedingsof Senseval-3: The Third International Workshop onthe Evaluation of Systems for the Semantic Analysisof Text.Catherine Macleod, Adam Meyers, Ralph Grishman,Leslie Barrett, and Ruth Reeves.
1997.
Designing aDictionary of Derived Nominals.
In Proceedings ofRecent Advances in Natural Language Processing.Catherine Macleod, Ralph Grishman, Adam Meyers,Leslie Barrett, and Ruth Reeves.
1998.
NOMLEX:A Lexicon of Nominalizations.
In Proceedings ofEURALEX?98.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, and BrianYoung.
2004a.
The Cross-Breeding of Dictionar-ies.
In Proceedings of LREC-2004.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004b.
Annotating Noun Ar-gument Structure for NomBank.
In Proceedings ofLREC-2004.Adam Meyers, Ruth Reeves, Catherine Macleod,Rachel Szekely, Veronika Zielinska, Brian Young,and Ralph Grishman.
2004c.
The NomBankProject: An Interim Report.
In Proceedings of HLT-NAACL 2004 Workshop on Frontiers in Corpus An-notation.Martha Palmer, Daniel Gildea, and Paul Kingsbury.2005.
The Proposition Bank: An Annotated Corpusof Semantic Roles.
Computational Linguistics.Sameer S. Pradhan, Honglin Sun, Wayne Ward,James H. Martin, and Dan Jurafsky.
2004.
ParsingArguments of Nominalizations in English and Chi-nese.
In Proceedings of HLT/NAACL 2004.Sameer Pradhan, Kadri Hacioglu, Valerie Krugler,Wayne Ward, James H. Martin, and Daniel Juraf-sky.
2005.
Support Vector Learning for SemanticArgument Classification.
Machine Learning.James Pustejovsky, Adam Meyers, Martha Palmer, andMassimo Poesio.
2005.
Merging PropBank, Nom-Bank, TimeBank, Penn Discourse Treebank andCoreference.
In ACL 2005 Workshop on Frontiersin Corpus Annotations II: Pie in the Sky.Siegel Sidney and N. John Castellan Jr. 1988.
Non-parametric Statistics for the Behavioral Sciences.McGraw-Hill, New York.Kristina Toutanova, Aria Haghighi, and Christopher D.Manning.
2005.
Joint Learning Improves SemanticRole Labeling.
In Proceedings of ACL 2005.Nianwen Xue and Martha Palmer.
2004.
CalibratingFeatures for Semantic Role Labeling.
In Proceed-ings of EMNLP-2004.145
