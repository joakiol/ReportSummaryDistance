Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics?System Demonstrations, pages 49?54,Berlin, Germany, August 7-12, 2016.c?2016 Association for Computational LinguisticsTMop: a Tool for Unsupervised Translation Memory CleaningMasoud Jalili Sabet(1), Matteo Negri(2), Marco Turchi(2),Jos?e G. C. de Souza(2), Marcello Federico(2)(1)School of Electrical and Computer Engineering, University of Tehran, Iran(2)Fondazione Bruno Kessler, Trento, Italyjalili.masoud@ut.ac.ir{negri,turchi,desouza,federico}@fbk.euAbstractWe present TMop, the first open-sourcetool for automatic Translation Memory(TM) cleaning.
The tool implements afully unsupervised approach to the task,which allows spotting unreliable transla-tion units (sentence pairs in different lan-guages, which are supposed to be trans-lations of each other) without requiringlabeled training data.
TMop includes ahighly configurable and extensible set offilters capturing different aspects of trans-lation quality.
It has been evaluated ona test set composed of 1,000 translationunits (TUs) randomly extracted from theEnglish-Italian version of MyMemory, alarge-scale public TM.
Results indicate itseffectiveness in automatic removing ?bad?TUs, with comparable performance to astate-of-the-art supervised method (76.3vs.
77.7 balanced accuracy).1 IntroductionComputer-assisted translation (CAT) refers to aframework in which the work of human translatorsis supported by machines.
Its advantages, espe-cially in terms of productivity and translation con-sistency, have motivated huge investments botheconomic (by the translation industry) and intel-lectual (by the research community).
Indeed, thehigh market potential of solutions geared to speedup the translation process and reduce its costs hasattracted increasing interest from both sides.Advanced CAT tools currently integrate thestrengths of two complementary technologies:translation memories (TM - a high-precisionmechanism for storing and retrieving previouslytranslated segments) and machine translation (MT- a high-recall technology for translating unseensegments).
The success of the integration has de-termined the quick growth of market shares thatare held by CAT, as opposed to fully manual trans-lation that became a niche of the global transla-tion market.
However, differently from MT thatis constantly improving and reducing the distancefrom human translation, core TM technology hasslightly changed over the years.
This is in contrastwith the fact that TMs are still more widely usedthan MT, especially in domains featuring high textrepetitiveness (e.g.
software manuals).Translation memories have a long tradition inCAT, with a first proposal dating back to (Arth-ern, 1979).
They consist of databases that storepreviously translated segments, together with thecorresponding source text.
Such (source, target)pairs, whose granularity can range from the phraselevel to the sentence or even the paragraph level,are called translation units (TUs).
When workingwith a CAT tool, each time a segment of a docu-ment to be translated matches with the source sideof a TU, the corresponding target is proposed asa suggestion to the user.
The user can also storeeach translated (source, target) pair in the TM forfuture use, thus increasing the size and the cover-age of the TM.
Due to such constant growth, inwhich they evolve over time incorporating usersstyle and terminology, the so-called private TMsrepresent an invaluable asset for individual trans-lators and translation companies.
Collaboratively-created public TMs grow in a less controlled waybut still remain a practical resource for the transla-tors?
community at large.The usefulness of TM suggestions mainly de-pends on two factors: the matching process andthe quality of the TU.
To increase recall, the re-trieval is based on computing a ?fuzzy match?score.
Depending on how the matching is per-formed, its output can be a mix of perfect and par-tial matches requiring variable amounts of correc-49tions by the user.
For this reason, most prior workson TM technology focused on improving this as-pect (Gupta et al, 2014; Bloodgood and Strauss,2014; Vanallemeersch and Vandeghinste, 2015;Chatzitheodoroou, 2015; Gupta et al, 2015).The other relevant factor, TU quality, relates tothe reliability of the target translations.
Indeed,a perfectly matching source text associated to awrong translation would make the correspondingsuggestion useless or, even worse, an obstacle toproductivity.
On this aspect, prior research is lim-ited to the work proposed in (Barbu, 2015), whichso far represents the only attempt to automaticallyspot false translations in the bi-segments of a TM.However, casting the problem as a supervised bi-nary classification task, this approach highly de-pends on the availability of labelled training data.Our work goes beyond the initial effort of Barbu(2015) in two ways.
First, we propose a config-urable and extensible open source frameworkfor TM cleaning.
In this way, we address the de-mand of easy-to-use TM management tools whosedevelopment is out of the reach of individual trans-lators and translation companies.
Such demand isnot only justified by productivity reasons (removebad suggestions as a cause of slow production),but also for usability reasons.
Loading, searchingand editing a TM are indeed time-consuming andresource-demanding operations.
In case of verylarge databases (up to millions of TUs) the accu-rate removal of useless units can significantly in-crease usability.
Though paid, the few existingtools that incorporate some data cleaning meth-ods (e.g.
Apsic X-Bench1) only implement verysimple syntactic checks (e.g.
repetitions, open-ing/closing tags consistency).
These are insuffi-cient to capture the variety of errors that can be en-countered in a TM (especially in the public ones).Second, our approach to TM cleaning is fullyunsupervised.
This is to cope with the lack of la-belled training data which, due to the high acqui-sition costs, represents a bottleneck rendering su-pervised solutions unpractical.
It is worth remark-ing that also current approaches to tasks closely re-lated to TM cleaning (e.g.
MT quality estimation(Mehdad et al, 2012; C. de Souza et al, 2014))suffer from the same problem.
Besides not beingcustomised for the specificities of the TM clean-ing scenario (their usefulness for the task shouldbe demonstrated), their dependence on labelled1http://www.xbench.net/training data is a strong requirement from the TMcleaning application perspective.2 The TM cleaning taskThe identification of ?bad?
TUs is a multifacetedproblem.
First, it deals with the recognition of avariety of errors.
These include:?
Surface errors, such as opening/closing tagsinconsistencies and empty or suspiciouslylong/short translations;?
Language inconsistencies, for instance due tothe inversion between the source and targetlanguages;?
Translation fluency issues, such as typos andgrammatical errors (e.g.
morpho-syntacticdisagreements, wrong word ordering);?
Translation adequacy issues, such as the pres-ence of untranslated terms, wrong lexicalchoices or more complex phenomena (e.g.negation and quantification errors) for whicha syntactically correct target can be a seman-tically poor translation of the source segment.The severity of the errors is another aspect totake into account.
Deciding if a given error makesa TU useless is often difficult even for humans.For instance, judging about the usefulness of a TUwhose target side has missing/extra words wouldbe a highly subjective task.2For this reason, iden-tifying ?bad?
TUs with an automatic approachopens a number of problems related to: i) definingwhen a given issue becomes a real error (e.g.
theratio of acceptable missing words), ii) combiningpotentially contradictory evidence (e.g.
syntacticand semantic issues), and iii) making these actionseasily customisable by different users having dif-ferent needs, experience and quality standards.What action to take when one or more errorsare identified in a TU is also important.
Ideally,a TM cleaning tool should allow users either tosimply flag problematic TUs (leaving the final de-cision to a human judgment), or to automaticallyremove them without further human intervention.Finally, two critical aspects are the externalknowledge and resources required by the TM-cleaning process.
On one side, collecting evidence2Likely, the perceived severity of a missing word out of nperfectly translated terms will be inversely proportional to n.50for each TU can involve processing steps that ac-cess external data and tools.
On the other side,decision making can require variable amounts oflabelled training data (i.e.
positive/negative exam-ples of ?good?/?bad?
TUs).
For both tasks, therecourse to external support can be an advantagein terms of performance due to the possibility toget informed judgments taken from models trainedin a supervised fashion.
At the same time, it canbe a limitation in terms of usability and portabil-ity across languages.
When available, external re-sources and tools (e.g.
syntactic/semantic parsers)can indeed be too slow to process huge amountsof data.
Most importantly, labelled training dataare usually difficult to acquire.
In case of need, aTM cleaning tool should hence minimise the de-pendence of its performance from the availabilityof external resources.All these aspects were considered in the designof TMop, whose capability to cope with a vari-ety of errors, customise its actions based on theirseverity and avoid the recourse to external knowl-edge/resources are described in the next section.3 The TMop frameworkTMop (Translation Memory open-source purifier)is an open-source TM cleaning software writtenin Python.
It consists of three parts: core, filtersand policy managers.
The core, the main part ofthe software, manages the workflow between fil-ters, policy managers and input/output files.
Thefilters (?3.2) are responsible for detecting ?bad?TUs.
Each of them can detect a specific type ofproblems (e.g.
formatting, fluency, adequacy) andwill emit an accept or reject judgment for eachTU.
Policy managers (?3.3) collect the individualresults from each filter and take a final decision foreach TM entry based on different possible strate-gies.
Filters, policies and basic parameters can beset by means of a configuration file, which wasstructured by keeping ease of use and flexibility asthe main design criteria.TMop implements a fully unsupervised ap-proach to TM cleaning.
The accept/reject criteriaare learned from the TM itself and no training dataare required to inform the process.3Nevertheless,the filters?
output could be also used to instantiatefeature vectors in any supervised learning scenariosupported by training data.3The tool has been recently used also in the unsupervisedapproach by Jalili Sabet et al (2016).StartInitialize Other FiltersFor # of iterationsLearningFor each TUProcess TUFinalize a full scanFinalize LearningFor each TUDecisionDecide on TUFor each TUCollect Filters?
DecisionsApply PolicyWrite to FilesEndFilter iPolicy ManagerFigure 1: TMop workflow3.1 WorkflowThe input file of TMop is a TM represented as atext file containing one TU per line in the form(ID, source, target).
The output consists of sev-eral files, the most important of which are the ac-cept and reject files containing the TUs identifiedas ?good?/?bad?, in the same format of the input.As depicted in Figure 1, TMop filters operate intwo steps.
In the first one, the learning step,each filter i iterates over the TM or a subset of itto gather the basic statistics needed to define itsaccept/reject criteria.
For instance, by computingmean and standard deviation values for a given in-dicator (e.g.
sentence length ratio, proportion ofaligned words), quantiles or std counts in case ofnormal value distributions will be used as deci-sion boundaries.
Then, in the decision step,each filter uses the gathered information to decideabout each TU.
At the end of this process, for each51TU the policy manager collects all the decisionstaken by the filters and applies the policy set by theuser in the configuration file to assign an accept orreject judgment.
The final labels, the TUs and thefilters outputs are saved in different files.3.2 FiltersOur filters capture different aspects of the similar-ity between the source and the target of a TU.
Thefull set consists of 23 filters, which are organizedin four groups.Basic filters (8 in total).
This group (B) ex-tends the filters proposed by Barbu (2015) andsubstantially covers those offered by commercialTM cleaning tools.
They capture translation qual-ity by looking at surface aspects, such as the pos-sible mismatches in the number of dates, numbers,URLs, XML tags, ref and image tags present in thesource and target segments.
Other filters model thesimilarity between source and target by computingthe direct and inverse ratio between the number ofcharacters and words, as well as the average wordlength in the two segments.
Finally, two filterslook for uncommon character or word repetitions.Language identification filter (1).
This filter(LI) exploits the Langid tool (Lui and Baldwin,2012) to verify the consistency between the sourceand target languages of a TU and those indicatedin the TM.
Though simple, it is quite effectivesince often the two languages are inverted or evencompletely different from the expected ones.QE-derived filters (9).
This group (QE) con-tains filters borrowed from the closely-related taskof MT quality estimation, in which the complex-ity of the source, the fluency of the target and theadequacy between source and target are modeledas quality indicators.
Focusing on the adequacyaspect, we exploit a subset of the features pro-posed by C. de Souza et al (2013).
They use wordalignment information to link source and targetwords and capture the quantity of meaning pre-served by the translation.
For each segment of aTU, word alignment information is used to calcu-late: i) the proportion of aligned and unalignedword n-grams (n=1,2), ii) the ratio between thelongest aligned/unaligned word sequence and thelength of the segment, iii) the average length ofthe aligned/unaligned word sequences, and iv) theposition of the first/last unaligned word, normal-ized by the length of the segment.
Word alignmentmodels can be trained on the whole TM with oneof the many existing word aligners.
For instance,the results of WE filters reported in ?4 were ob-tained using MGIZA++ (Gao and Vogel, 2008).Word embedding filters (5).
Cross-lingualword embeddings provide a common vector rep-resentation for words in different languages andallow looking at the source and target segments atthe same time.
In TMop, they are computed us-ing the method proposed in (S?gaard et al, 2015)but, instead of considering bilingual documentsas atomic concepts to bridge the two languages,they exploit the TUs contained in the TM itself.Given a TU and a 100-dimensional vector repre-sentation of each word in the source and targetsegments, this group of filters (WE) includes: i)the cosine similarity between the source and tar-get segment vectors obtained by averaging (or us-ing the median) the source and target word vec-tors; ii) the average embedding alignment scoreobtained by computing the cosine similarity be-tween each source word and all the target wordsand averaging over the largest cosine score of eachsource word; iii) the average cosine similarity be-tween source/target word alignments; iv) a scorethat merges features (ii) and (iii) by complement-ing word alignments (also in this case obtained us-ing MGIZA++) with the alignments obtained fromword embedding and averaging all the alignmentweights.3.3 PoliciesDecision policies allow TMop combining the out-put of the active filters into a final decision for eachTU.
Simple decision-making strategies can con-sider the number of accept and reject judgments,but more complex methods can be easily imple-mented by the user (both filters and policy man-agers can be easily modified and extended by ex-ploiting well-documented abstract base classes).TMop currently implements three policies:OneNo, 20%No and MajorityVoting.
The first onecopies a TU in the reject file if at least one filterrejects it.
The second and the third policy take thisdecision only if at least twenty or fifty percent ofthe filters reject the TU respectively.These three policies reflect different TM clean-ing strategies.
The first one is a very aggressive(recall-oriented) solution that tends to flag moreTUs as ?bad?.
The third one is a more conser-vative (precision-oriented) solution, as it requires52at least half of the judgments to be negative forpushing a TU in the reject file.
Depending on theuser needs and the overall quality of the TM, thechoice of the policy will allow keeping under con-trol the number of false positives (?bad?
TUs ac-cepted) and false negatives (?good?
TUs rejected).4 BenchmarkingWe test TMop on the English-Italian version ofMyMemory,4one of the world?s largest collabo-rative public TMs.
This dump contains about 11MTUs coming from heterogeneous sources: aggre-gated private TMs, either provided by translatorsor automatically extracted from the web/corpora,as well as anonymous contributions of (source,target) bi-segments.
Its uncontrolled sources callfor accurate cleaning methods (e.g.
to make itmore accurate, smaller and manageable).From the TM we randomly extracted a subsetof 1M TUs to compute the statistics of each filterand a collection of 2,500 TUs manually annotatedwith binary labels.
Data annotation was done bytwo Italian native speakers properly trained withthe same guidelines prepared by the TM owner forperiodic manual revisions.
After agreement com-putation (Cohen?s kappa is 0.78), a reconciliationended up with about 65% positive and 35% nega-tive examples.
This pool is randomly split in twoparts.
One (1,000 instances) is used as test setfor our evaluation.
The other (1,500 instances) isused to replicate the supervised approach of Barbu(2015), which leverages human-labelled data totrain an SVM binary classifier.
We use it as aterm of comparison to assess the performance ofthe different groups of filters.To handle the imbalanced (65%-35%) data dis-tribution, and equally reward the correct classifi-cation on both classes, we evaluate performancein terms of balanced accuracy (BA), computed asthe average of the accuracies on the two classes(Brodersen et al, 2010).In Table 1, different combinations of the fourgroups of filters are shown with results aggregatedwith the 20%No policy, which, on this data, re-sults to be the best performing policy among theones implemented in TMop.
Based on the statis-tics collected in the learning phase of eachfilter, the accept/reject criterion applied in theseexperiments considers as ?good?
all the TUs for4http://mymemory.translated.netFilters BA?
(Barbu, 2015) 77.7B 52.8LI 69.0QE 71.2WE 65.0B + LI 55.4B + QE 70.1B + WE 68.7QE + LI 71.7QE + WE 67.9LI + WE 68.1B + QE + LI 72.9B + WE + LI 70.3B + QE + WE 73.3B + QE + LI + WE 76.3Table 1: Balanced accuracy of different filter combinationson a 1,000 TU, EN-IT test set.
B=Basic, LI=language identi-fication, QE=quality estimation, WE=word embedding.which the filter value is below one standard devia-tion from the mean and ?bad?
otherwise.Looking at the results, it is worth noting that theLI, QE and WE groups, both alone and in combi-nation, outperform the basic filters (B), which sub-stantially represent those implemented by com-mercial tools.
Although relying on an externalcomponent (the word aligner), QE filters producethe best performance in isolation, showing thatword alignment information is a good indicator oftranslation quality.
The results obtained by com-bining the different groups confirm their comple-mentarity.
In particular, when using all the groups,the performance is close to the results achieved bythe supervised method by Barbu (2015), which re-lies on human-labelled data (76.3 vs. 77.7).The choice of which filter combination to usestrongly depends on the application scenario andit is often a trade-off.
A first important aspectconcerns the type of user.
When the expertise totrain a word aligner is not available, combining B,WE and LI is the best solution, though it comesat the cost of lower accuracy.
Another aspect isthe processing time that the user can afford.
TMcleaning is an operation conceived to be performedonce in a while (possibly overnight), once the TMhas grown enough to justify a new sanity check.However, although it does not require real-timeprocessing, the size of the TM can motivate theselection of faster filter combinations.
An analy-sis of the efficiency of the four groups, made by53counting the number of processed TUs per sec-ond,5indicates that B and QE are the fastest filters(processing on average ?2,000 TUs/sec.).
The LIfilter is slower, processing ?300 TUs per second,while the large number of times the cosine similar-ity score is computed does not allow the WE filterto process more than 50 TUs per second.5 ConclusionWe presented TMop, the first open-source toolfor automatic Translation Memory (TM) clean-ing.
We summarised its design criteria, work-flow and main components, also reporting someefficiency and performance indicators.
TMop isimplemented in Python and can be downloaded,together with complete documentation, fromhttps://github.com/hlt-mt/TMOP.
Itslicense is FreeBSD, a very open permissive non-copyleft license, compatible with the GNU GPLand with any use, including commercial.AcknowledgmentsThis work has been partially supported by the EC-funded project ModernMT (H2020 grant agree-ment no.
645487).
The work carried out atFBK by Masoud Jalili Sabet was sponsored bythe EAMT summer internships 2015 program andsupported by Prof. Heshaam Faili (University ofTehran).
The authors would also like to thankTranslated for providing a dump of MyMemory.ReferencesPeter Arthern.
1979.
Machine Translation andComputerized Terminology Systems: a Translator?sViewpoint.
In Translating and the computer.
Proc.of a seminar, pages 77?108, London, UK.Eduard Barbu.
2015.
Spotting False Translation Seg-ments in Translation Memories.
In Proc.
of theWorkshop Natural Language Processing for Trans-lation Memories, pages 9?16, Hissar, Bulgaria.Michael Bloodgood and Benjamin Strauss.
2014.Translation Memory Retrieval Methods.
In Proc.
ofthe 14th Conference of the EACL, pages 202?210,Gothenburg, Sweden.Kay Henning Brodersen, Cheng Soon Ong, Klaas EnnoStephan, and Joachim M. Buhmann.
2010.
The Bal-anced Accuracy and Its Posterior Distribution.
InProc.
of the 2010 20th International Conference onPattern Recognition, ICPR ?10, pages 3121?3124.5Experiments were run with a PC with an Intel Core i5M540 @ 2.53GHz and 6 GB RAM.Jos?e G. C. de Souza, Christian Buck, Marco Turchi,and Matteo Negri.
2013.
FBK-UEdin Participationto the WMT13 Quality Estimation Shared Task.
InProc.
of the Eighth Workshop on Statistical MachineTranslation, pages 352?358, Sofia, Bulgaria.
Asso-ciation for Computational Linguistics.Jos?e G. C. de Souza, Jes?us Gonz?alez-Rubio, Chris-tian Buck, Marco Turchi, and Matteo Negri.
2014.FBK-UPV-UEdin Participation in the WMT14Quality Estimation Shared-task.
In Proc.
of theNinth Workshop on Statistical Machine Translation,pages 322?328, Baltimore, Maryland, USA.Konstantinos Chatzitheodoroou.
2015.
ImprovingTranslation Memory Fuzzy Matching by Paraphras-ing.
In Proc.
of the Workshop Natural LanguageProcessing for Translation Memories, pages 24?30,Hissar, Bulgaria.Qin Gao and Stephan Vogel.
2008.
Parallel Implemen-tations of Word Alignment Tool.
In In Proc.
of theACL 2008 Software Engineering, Testing, and Qual-ity Assurance Workshop.Rohit Gupta, Hanna Bechara, and Constantin Orasan.2014.
Intelligent Translation Memory Matching andRetrieval Metric Exploiting Linguistic Technology.In Proc.
of Translating and the Computer: Vol.
36.,pages 86?89.Rohit Gupta, Constantin Orasan, Marcos Zampieri,Mihaela Vela, and Josef Van Genabith.
2015.
CanTranslation Memories afford not to use paraphras-ing?
In Proc.
of the 18th Annual Conference ofthe European Association for Machine Translation,pages 35?42, Antalya, Turkey.Masoud Jalili Sabet, Matteo Negri, Marco Turchi, andEduard Barbu.
2016.
An Unsupervised Method forAutomatic Translation Memory Cleaning.
In Proc.of the 54th Annual Meeting of the Association forComputational Linguistics, Berlin, Germany.Marco Lui and Timothy Baldwin.
2012. langid.py: AnOff-the-shelf Language Identification Tool.
In Proc.of the ACL 2012 system demonstrations, pages 25?30.
Association for Computational Linguistics.Yashar Mehdad, Matteo Negri, and Marcello Federico.2012.
Match without a Referee: Evaluating MTAdequacy without Reference Translations.
In Proc.of the Machine Translation Workshop (WMT2012),pages 171?180, Montr?eal, Canada.Anders S?gaard,?Zeljko Agi?c, H?ector Mart?
?nez Alonso,Barbara Plank, Bernd Bohnet, and Anders Jo-hannsen.
2015.
Inverted indexing for cross-lingualNLP.
In The 53rd Annual Meeting of the Associa-tion for Computational Linguistics (ACL 2015).Tom Vanallemeersch and Vincent Vandeghinste.
2015.Assessing Linguistically Aware Fuzzy Matching inTranslation Memories.
In Proc.
of the 18th AnnualConference of the European Association for Ma-chine Translation, pages 153?160, Antalya, Turkey.54
